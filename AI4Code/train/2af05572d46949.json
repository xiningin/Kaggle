{"cell_type":{"2de6de60":"code","f70aa7fd":"code","8e1173c3":"code","5470c350":"code","03a74e4a":"code","2f759a9d":"code","a526c6ce":"code","26023f34":"code","3bf6fed8":"code","973c19bc":"code","8d5b559d":"code","1bf60779":"code","6eeaace9":"code","a87801fd":"code","716cd963":"code","a8874ae4":"code","24e5c4d6":"code","a5debf9c":"code","ac78f4fe":"code","e5aed452":"code","86673a01":"code","810789c1":"code","822ebd77":"code","cbc6b369":"code","7cf3b3a2":"code","64da7dff":"code","4ded1c61":"code","e24ec5d2":"code","bbec16cf":"code","6e37fdae":"code","e2af58aa":"code","189e4b21":"markdown","c4b4f170":"markdown","fb8d7182":"markdown","4e1c478a":"markdown","35d727ba":"markdown","0b356be0":"markdown","34063189":"markdown","baa8dc52":"markdown","b8ea2a84":"markdown","f4f5ae01":"markdown","1535e6cc":"markdown","f0210caf":"markdown","6fefea44":"markdown","f4de0418":"markdown","fec69782":"markdown","9898ae91":"markdown","688e5c31":"markdown","e9a57743":"markdown","49fb09b8":"markdown","a4d49f11":"markdown","3833723e":"markdown","25b7ceef":"markdown","bc3ae5ba":"markdown","941541ef":"markdown","988f0492":"markdown","cccb4b46":"markdown","e36162e8":"markdown"},"source":{"2de6de60":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f70aa7fd":"# main libraries\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\n\nfrom tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropout,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nprint('version of tensorflow : ',tf.__version__)\n\n# supporting libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold","8e1173c3":"# for confusion matrix plotting\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import multilabel_confusion_matrix,confusion_matrix","5470c350":"train=pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')","03a74e4a":"train.head()","2f759a9d":"train.shape","a526c6ce":"test=pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv')","26023f34":"test.head()","3bf6fed8":"test.shape","973c19bc":"X_train=train.drop('label',1)\ny_train=train['label']","8d5b559d":"X_train=X_train.to_numpy()","1bf60779":"X_train=X_train.reshape(60000,28,28,1)","6eeaace9":"X_test=test.drop('label',1)\ny_test=test['label']","a87801fd":"X_test=X_test.to_numpy()\nX_test=X_test.reshape(10000,28,28,1)","716cd963":"y_train=y_train.to_numpy()\ny_test=y_test.to_numpy()","a8874ae4":"plt.imshow(X_train[0].reshape(28,28))","24e5c4d6":"labels=['T-shirt\/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']","a5debf9c":"labels[y_train[0]]","ac78f4fe":"X_train=X_train\/255.0\nX_test=X_test\/255.0","e5aed452":"model=Sequential()\nmodel.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=X_train[0].shape))  #in first layer we have to feed the input shape.\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.25))           # dropout reduces the number of functional neurons.\n\nmodel.add(Flatten())                   # after convnet layers we have to flatten the model to create actual prediction.\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10,activation='softmax'))          # In this step we have used a softmax function to predict classes.","86673a01":"model.summary()","810789c1":"model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])","822ebd77":"hist=model.fit(X_train,y_train,batch_size=128,epochs=10,validation_data=(X_test,y_test),verbose=1)   \n# validation data can indicate overfitting or underfitting of the model.","cbc6b369":"plt.title('Model Accuracy')\nplt.plot(range(1,11),hist.history['accuracy'])\nplt.plot(range(1,11),hist.history['val_accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('accuracy')\nplt.legend(labels=['train','validation'])\nplt.show()\n\nplt.title('Model Loss')\nplt.plot(range(1,11),hist.history['loss'])\nplt.plot(range(1,11),hist.history['val_loss'])\nplt.xlabel('Epoch')\nplt.ylabel('loss')\nplt.legend(labels=['train','validation'])\nplt.show()","7cf3b3a2":"y_pr=model.predict(X_test)","64da7dff":"y_pr","4ded1c61":"y_pred=[]\nfor i in range(len(y_pr)):\n  y_pred.append(np.argmax(y_pr[i]))","e24ec5d2":"for i in range(5):\n    print(y_pred[i])","bbec16cf":"for i in range(5):\n  print('prediction of the image : ',labels[y_pred[i]])\n  print('actual image :')\n  plt.imshow(X_test[i].reshape(28,28))\n  plt.show()","6e37fdae":"mat=confusion_matrix(y_test,y_pred)\nmat","e2af58aa":"plot_confusion_matrix(mat,figsize=(9,9),colorbar=True)","189e4b21":"At last but not the least you can visit my other works on [github](https:\/\/github.com\/sagnik1511?tab=repositories) , [Kaggle](https:\/\/www.kaggle.com\/sagnik1511)","c4b4f170":"Revealing file directories","fb8d7182":"## Model generation\n\nAs we are making a CNN model for image classification so, we are using **2D ConvNet**","4e1c478a":"We have created this array for better understanding .\n\nYou can use the word **decoder** of it.","35d727ba":"After plotting the confusion matrixes we can see that the model is well tuned for image classification of those fashion types.Moreover *Tshirt\/top* quite resembles with *shirt* what actually should happen.","0b356be0":"## Preprocessing","34063189":"The total deep learning or machhine learning project can be done in following steps :\n\n         1. Importing required libraries.\n         2. Reading the data and primary visualization\n         3. Preprocessing\n         4. Creating X and y\n         5. Model Generation\n         6. Training \/ Fitting model with data\n         7. Model evaluation\n         8. Creating prediction data\n         9. Accuracy checking \n                           ","baa8dc52":"Now we have achieved to the most interesting part of the whole project.\n\nWe are storing the whole prediction in **hist** for model evaluation.","b8ea2a84":"## Libraries","f4f5ae01":"As we preprocessed the data we have to normalize the values of the train and test for better prediction.","1535e6cc":"as we can see the prediction of each image is sequence of values. So wee need to use argmax for getting the actual prediction.","f0210caf":"## Accuracy checking","6fefea44":"# THE END","f4de0418":"As we have created our model we can see the summary of it.","fec69782":"So, the predictions are quite good. So, the model is quite effective to the dataset.","9898ae91":"# Fashion Recognizer with CNN using tensorflow","688e5c31":"**Just for visualization :** we can see the first image and and check it's original label.\n(As the data is now in 4-d form we have to reshape it for visualization.","e9a57743":"So, now after getting the prediction we can check if it is really giving accurate predictions or not.","49fb09b8":"## Reading the data and primary visualization","a4d49f11":"## Creating prediction data","3833723e":"We are compiling our model with *adam* optimizer and as this is a multiclass classification we are using the *sparse categorical cross entropy* as loss function.","25b7ceef":"As we have taken the train and test data in dataframe we have to change them into numpy ndarray for fitting in tensorflow framework.\n\nThe data is not shaped properly to feed through the CNN layers. So we also have to reshape the data.\n\nBefore that we have to split the X and Y of the datas.","bc3ae5ba":"After visualizaing the accuracy and loss digram of both train and validation data we can see the model is slightly overfitted. and it is visible that the best accuracy for training and validation data is on **9th epoch**.","941541ef":"## Model Evaluation","988f0492":"### Hurrah ! We've completed a whole image classification project using CNN model with tensorflow framework :)\n\n#### UPVOTE if you like this kernel :)","cccb4b46":"***THANK YOU***","e36162e8":"So, the model fitting took almost 30 minutes. and after that we can see the we have reached a train accuracy of 94% and validation accuracy of 93%.\n\n"}}