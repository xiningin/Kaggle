{"cell_type":{"21a433c5":"code","c5731f38":"code","14252459":"code","891a88d8":"code","774ee64d":"code","06e903cf":"code","5578b9c5":"code","75d30142":"code","e0bc443e":"code","cb5b0bb0":"code","814271e3":"code","c432d37a":"code","36e17faf":"markdown","c432d50f":"markdown","7ce3640a":"markdown","b88f7088":"markdown"},"source":{"21a433c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c5731f38":"import plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score, classification_report","14252459":"train_dir = '..\/input\/blood-cells\/dataset2-master\/dataset2-master\/images\/TRAIN'\ntest_dir = '..\/input\/blood-cells\/dataset2-master\/dataset2-master\/images\/TEST'","891a88d8":"# Create generators\n\ntrain_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function = tf.keras.applications.mobilenet_v2.preprocess_input , \n    validation_split= 0.2\n)\n\ntest_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function = tf.keras.applications.mobilenet_v2.preprocess_input , \n    validation_split= 0.2\n)","774ee64d":"# Flow image data \n\ntrain_images = train_gen.flow_from_directory(\n    directory = train_dir , target_size = (224,224) , color_mode = 'rgb' , \n    class_mode = 'categorical' , batch_size = 32 , shuffle= True , seed = 42,\n    subset = 'training'\n)\n\nval_images = train_gen.flow_from_directory(\n    directory = train_dir , target_size = (224,224) , color_mode = 'rgb' , \n    class_mode = 'categorical' , batch_size = 32 , shuffle= True , seed = 42,\n    subset = 'validation'\n)\n\ntest_images = test_gen.flow_from_directory(\n    directory = test_dir , target_size = (224,224) , color_mode = 'rgb' , \n    class_mode = 'categorical' , batch_size = 32 , shuffle= False , seed = 42\n)","06e903cf":"pretrained_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)\n\npretrained_model.trainable = False","5578b9c5":"inputs = pretrained_model.input\nx = tf.keras.layers.Dense(128,activation = 'relu')(pretrained_model.output)\n\noutputs = tf.keras.layers.Dense(4,activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs = inputs , outputs = outputs)\n\nmodel.compile(\n    optimizer = 'adam' , \n    loss = 'categorical_crossentropy' , \n    metrics = ['accuracy']\n)\nprint(model.summary())","75d30142":"# Training \n\nhistory = model.fit(\n    train_images , \n    validation_data = val_images ,\n    epochs = 100 , \n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor = 'val_loss' , \n            patience = 3,\n            restore_best_weights = True\n        )\n    ]\n    \n)","e0bc443e":"fig = px.line(\n    history.history,\n    y = ['loss' , 'val_loss'] , \n    labels = {'index':'Epoch' , 'value':'Loss'},\n    title = 'Training and Validation Loss Over Time'\n    \n)\n\nfig.show()","cb5b0bb0":"CLASS_NAMES = list(train_images.class_indices.keys())","814271e3":"predictions = np.argmax(model.predict(test_images) , axis=1)\n\nacc = accuracy_score(test_images.labels , predictions)\n\ncm = tf.math.confusion_matrix(test_images.labels , predictions)\nclr = classification_report(test_images.labels , predictions , target_names = CLASS_NAMES)\n\nprint(\"Test Accuracy: {:.3f}%\".format(acc * 100))\n\nplt.figure(figsize=(8, 8))\nsns.heatmap(cm, annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)\nplt.xticks(ticks= np.arange(4) + 0.5, labels=CLASS_NAMES)\nplt.yticks(ticks= np.arange(4) + 0.5, labels=CLASS_NAMES)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\nprint(\"Classification Report:\\n----------------------\\n\", clr)","c432d37a":"val_images = train_gen.flow_from_directory(\n    directory=train_dir,\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=False,\n    seed=42,\n    subset='validation'\n)\n\n\npredictions = np.argmax(model.predict(val_images), axis=1)\n\nacc = accuracy_score(val_images.labels, predictions)\ncm = tf.math.confusion_matrix(val_images.labels, predictions)\nclr = classification_report(val_images.labels, predictions, target_names=CLASS_NAMES)\n\nprint(\"Validation Accuracy: {:.3f}%\".format(acc * 100))\n\nplt.figure(figsize=(8, 8))\nsns.heatmap(cm, annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)\nplt.xticks(ticks= np.arange(4) + 0.5, labels=CLASS_NAMES)\nplt.yticks(ticks= np.arange(4) + 0.5, labels=CLASS_NAMES)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\nprint(\"Classification Report:\\n----------------------\\n\", clr)\n","36e17faf":"## Loading Image Data","c432d50f":"## Build Classification Model","7ce3640a":"## Build Pretrained Model","b88f7088":"## Results"}}