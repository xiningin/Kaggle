{"cell_type":{"8074fc38":"code","805b1b83":"code","27aeccbc":"code","ff0c7c19":"code","2a655805":"code","1b4a91b3":"code","53455990":"code","b73bdb72":"code","ea2c02e8":"code","79cf480e":"code","4bb38a9f":"code","0c403586":"code","a57bffa9":"code","8d477c46":"code","a3272e70":"code","319426a6":"code","b1e91d40":"code","db568de8":"code","08ca5dfb":"code","8f0b60a4":"code","bfc5ee97":"code","9ee1f373":"code","8bd78a18":"code","d0e3e0cc":"code","b87dea03":"code","4f0e913d":"code","a80737ef":"code","f1433d01":"code","75699628":"code","1a588331":"code","47c0dfdc":"code","8c2ae3e3":"code","a4f35662":"code","b4ab9ee6":"code","3a3a7bba":"code","a2b480ee":"code","29c1ec09":"code","c2a2f39c":"code","c77f2054":"code","1db987aa":"code","c9c4f637":"code","2c656838":"code","41fd5245":"code","3717ab36":"code","e96dbec1":"code","d708522c":"code","c3cfc648":"code","b5b33b97":"code","ae2453a0":"code","ddc95a04":"code","95037aae":"code","ec5f3a2b":"code","f0dcedd2":"code","99fddb98":"code","cff74a13":"code","32006544":"code","16621d38":"code","fbffba96":"markdown","02740fb8":"markdown","21c2f1b7":"markdown","b195a13c":"markdown","d80de4b2":"markdown","897b9c56":"markdown","6a0e0e9f":"markdown","68041f01":"markdown","de957be9":"markdown","7a892c50":"markdown","2eb26d2f":"markdown","ac7173a2":"markdown","fd0cfb27":"markdown","7246740e":"markdown","d0b70649":"markdown","4e40d71e":"markdown","88c57552":"markdown","12f69cf5":"markdown","37e87e48":"markdown","fc0b5fc4":"markdown","fb0cf9de":"markdown","96ff6186":"markdown","57c49c53":"markdown","40482782":"markdown","0e0c7f71":"markdown","f81f02f7":"markdown","2f254145":"markdown","6b29de58":"markdown","85d353c5":"markdown","a604f36e":"markdown","4a846c36":"markdown","eb7ce554":"markdown","f88425b0":"markdown","e0ff2524":"markdown","a560450d":"markdown","d2248842":"markdown","5a4c7037":"markdown","320f9b5f":"markdown","459aa936":"markdown","798232d1":"markdown","e8ae69f2":"markdown","66a599ee":"markdown","26653091":"markdown","78fae64d":"markdown","684b5f73":"markdown","2255f109":"markdown","3edceb3d":"markdown","1dc30d54":"markdown","6a992e7f":"markdown","e12537bb":"markdown","96f46ad9":"markdown","b1bcfe60":"markdown","94bdc498":"markdown","e6738cd5":"markdown","32f04963":"markdown","639d9d00":"markdown","0d2eaae6":"markdown","2268d64f":"markdown"},"source":{"8074fc38":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nsns.set()\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import train_test_split\n\nfrom scipy.stats import boxcox\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n","805b1b83":"original = pd.read_csv(\"..\/input\/en.openfoodfacts.org.products.tsv\",\n                       delimiter='\\t',\n                       encoding='utf-8')\noriginal.head()","27aeccbc":"original.shape","ff0c7c19":"original.product_name = original.product_name.astype(str)","2a655805":"def get_outliers(log_prob, treshold):\n    epsilon = np.quantile(log_prob, treshold)\n    outliers = np.where(log_prob <= epsilon, 1, 0)\n    return outliers \n\ndef make_word_cloud(data, cluster, subplotax, title):\n    words = data[data.cluster==cluster][\"product\"].apply(lambda l: l.lower().split())\n    cluster_words=words.apply(pd.Series).stack().reset_index(drop=True)\n\n    text = \" \".join(w for w in cluster_words)\n\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud(max_font_size=30, max_words=30,\n                          background_color=\"white\", colormap=\"YlGnBu\").generate(text)\n\n    # Display the generated image:\n    \n    subplotax.imshow(wordcloud, interpolation='bilinear')\n    subplotax.axis(\"off\")\n    subplotax.set_title(title)\n    return subplotax\n\ndef split_data_by_nan(cutoff):\n    cols_of_interest = nan_values[nan_values <= cutoff].index\n    data = original[cols_of_interest]\n    return data.copy()\n\ndef transform(data, feature, constant, lam):\n    scaler = MinMaxScaler((0,5))\n    scaled_feature = scaler.fit_transform(data[feature].values.copy().reshape(-1,1))\n    data[\"boxcox_\" + feature] = boxcox(scaled_feature + constant, lam)\n    scaler = StandardScaler()\n    data[\"transformed_\" + feature] = scaler.fit_transform(data[\"boxcox_\" + feature].values.reshape(-1,1))\n    return data\n\n\nclass TransformParameter:\n \n    def __init__(self, const, lam, color=\"Dodgerblue\"):\n        self.const = const\n        self.lam = lam\n        self.color = color","1b4a91b3":"nutrition_table_cols = [\"energy_100g\",\n                        \"fat_100g\",\n                        \"carbohydrates_100g\",\n                        \"sugars_100g\",\n                        \"proteins_100g\",\n                        \"salt_100g\"]","53455990":"nutrition_table = original[nutrition_table_cols].copy()\n\nnutrition_table[\"isempty\"] = np.where(nutrition_table.isnull().sum(axis=1) >= 1, 1, 0)\npercentage = nutrition_table.isempty.value_counts()[1] \/ nutrition_table.shape[0] * 100\nprint(\"Percentage of incomplete tables: \" + str(percentage))\n\nnutrition_table = nutrition_table[nutrition_table.isempty==0].copy()\nnutrition_table.isnull().sum()\n\nnutrition_table.drop(\"isempty\", inplace=True,axis=1)\nnutrition_table.dropna(axis = 0, how = \"any\", inplace=True)","b73bdb72":"nutrition_table.shape[0]","ea2c02e8":"nutrition_table[\"g_sum\"] = nutrition_table.fat_100g + nutrition_table.carbohydrates_100g + nutrition_table.proteins_100g + nutrition_table.salt_100g\nnutrition_table[\"g_sum\"] = round(nutrition_table.g_sum)\nnutrition_table[\"other_carbs\"] = nutrition_table.carbohydrates_100g - nutrition_table.sugars_100g\n\nnutrition_table[\"reconstructed_energy\"] = nutrition_table.fat_100g * 37 + (nutrition_table.proteins_100g + nutrition_table.carbohydrates_100g)* 17","79cf480e":"nutrition_table.columns","4bb38a9f":"meta = pd.DataFrame(index=nutrition_table.index)\nfor col in nutrition_table.columns:\n    meta[\"zero_\" + col] = np.where(nutrition_table[col] == 0, 1, 0)\nmeta[\"contains_zero\"] = np.where(meta.sum(axis=1) > 0,1,0 )","0c403586":"for col in nutrition_table.columns:\n    if col not in [\"energy_100g\", \"reconstructed_energy\"]:\n        nutrition_table = nutrition_table.loc[nutrition_table[col] <= 100]\n    nutrition_table = nutrition_table.loc[nutrition_table[col] >= 0]\n\nnutrition_table = nutrition_table.loc[nutrition_table.energy_100g <= 3700]\nnutrition_table = nutrition_table.loc[nutrition_table.carbohydrates_100g >= nutrition_table.sugars_100g]\nnutrition_table = nutrition_table.loc[nutrition_table.g_sum <= 100]","a57bffa9":"nutrition_table.head()","8d477c46":"to_knead = np.arange(0,10,0.1)\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\n\nk = ax[0].plot(to_knead, to_knead, color=\"Red\", label=\"to_knead\")\ne1 = ax[0].plot(to_knead, boxcox(to_knead + 0.01, 1.3), color=\"darkblue\", label=\"0.7\")\ne2 = ax[0].plot(to_knead, boxcox(to_knead + 0.01, 0.7), color=\"mediumblue\", label=\"0.7\")\ne3 = ax[0].plot(to_knead, boxcox(to_knead + 0.01, 0.5), color=\"royalblue\", label=\"0.5\")\ne4 = ax[0].plot(to_knead, boxcox(to_knead + 0.01, 0.01), color=\"cornflowerblue\", label=\"0.01\")\nax[0].set_title(\"Boxcox-Knead with different powers $\\lambda$\")\nhandles=[k, e1, e2, e3, e4]\nax[0].set_xlabel(\"Value\")\nax[0].set_ylabel(\"Transformed Value\")\n#ax[0].legend(handles)\n\nk = ax[1].plot(to_knead, to_knead, color=\"Red\", label=\"to_knead\")\ne1 = ax[1].plot(to_knead, boxcox(to_knead + 0.01, 0.5), color=\"darkblue\", label=\"0.01\")\ne2 = ax[1].plot(to_knead, boxcox(to_knead + 0.5, 0.5), color=\"mediumblue\", label=\"0.5\")\ne3 = ax[1].plot(to_knead, boxcox(to_knead + 1, 0.5), color=\"royalblue\", label=\"1\")\ne4 = ax[1].plot(to_knead, boxcox(to_knead + 1.5, 0.5), color=\"cornflowerblue\", label=\"1.5\")\nax[1].set_title(\"Boxcox-Knead with different constants $c$\")\nax[1].set_xlabel(\"Value\")\nax[1].set_ylabel(\"Transformed Value\")\nhandles=[k, e1, e2, e3, e4]\n#ax[0].legend(handles)","a3272e70":"constants = {\"carbohydrates_100g\": TransformParameter(const=0.01, lam=0.9, color=\"Green\"),\n            \"fat_100g\" : TransformParameter(const=0.1, lam=0.05, color=\"orange\"),\n            \"proteins_100g\": TransformParameter(const=0.1, lam=0.1, color=\"Red\"),\n             \"sugars_100g\" : TransformParameter(const=0.07, lam=0.03, color=\"Fuchsia\"),\n            \"other_carbs\" : TransformParameter(const=0.05, lam=0.3, color=\"maroon\"),\n            \"salt_100g\" : TransformParameter(const=0.01, lam=0.005, color=\"Dodgerblue\"),\n            \"energy_100g\" : TransformParameter(const=0.3, lam=0.7, color=\"Purple\"),\n            \"reconstructed_energy\" : TransformParameter(const=0.3, lam=0.7, color=\"mediumvioletred\"),\n            \"g_sum\": TransformParameter(const=0.1, lam=1.2, color=\"turquoise\")}","319426a6":"for key in constants.keys():\n    transform(data=nutrition_table,\n              feature=key,\n              constant=constants[key].const,\n              lam=constants[key].lam)","b1e91d40":"fig, ax = plt.subplots(9,3,figsize=(25,40))\nn = 0\nfor feature in constants.keys():\n    sns.distplot(nutrition_table[feature],\n                 ax=ax[n,0],\n                 color=constants[feature].color)\n    sns.distplot(nutrition_table[nutrition_table[feature]>0][\"boxcox_\" + feature],\n                 ax=ax[n,1],\n                 color=constants[feature].color)\n    sns.distplot(nutrition_table[\"transformed_\" + feature],\n                 ax=ax[n,2],\n                 color=constants[feature].color)\n    n+=1\n","db568de8":"nutrition_table, test_table = train_test_split(nutrition_table, test_size=0.3, random_state=0)","08ca5dfb":"features = [\"transformed_carbohydrates_100g\",\n            \"transformed_fat_100g\",\n            \"transformed_proteins_100g\",\n            \"transformed_sugars_100g\",\n            \"transformed_salt_100g\",\n            \"transformed_other_carbs\",\n            \"transformed_energy_100g\",\n            \"transformed_reconstructed_energy\",\n            \"transformed_g_sum\"]","8f0b60a4":"X_train = nutrition_table[features].values\nX_test = test_table[features].values","bfc5ee97":"print(\"Number of train samples: \" + str(X_train.shape[0]))","9ee1f373":"# ONLY uncomment, if you have at least an hour time to wait for computation! \n# scores = []\n# bic = []\n# aic = []\n# to_try = range(11,81,10)\n# for components in to_try:\n    # model = GaussianMixture(n_components=components, covariance_type=\"full\", random_state=1)\n    # model.fit(X_train)\n    # scores.append(model.score(X_test))\n    # bic.append(model.bic(X_test))\n    # aic.append(model.aic(X_test))\n\n# fig, ax = plt.subplots(1,3,figsize=(25,5))\n# ax[0].plot(to_try, scores)\n# ax[1].plot(to_try, bic)\n# ax[2].plot(to_try, aic)\n# ax[0].set_xlabel(\"Number of components\")\n# ax[0].set_ylabel(\"Log-Likelihood\")\n# ax[1].set_xlabel(\"Number of components\")\n# ax[1].set_title(\"BIC\")\n# ax[1].set_ylabel(\"Penalized Log-Likelihood\")\n\n# ax[2].set_ylabel(\"Penalized Log-Likelihood\")\n# ax[2].set_xlabel(\"Number of components\")\n# ax[2].set_title(\"AIC\")","8bd78a18":"components = 20\n\nmodel = GaussianMixture(n_components=components,\n                        covariance_type=\"full\",\n                        random_state=1,\n                        n_init=1,\n                        max_iter=200,\n                        init_params=\"kmeans\")\nmodel.fit(X_train)\nprint(\"Model converged: \" + str(model.converged_))\n\nnutrition_table[\"cluster\"] = model.predict(X_train)\ntest_table[\"cluster\"] = model.predict(X_test)","d0e3e0cc":"#plot_features = [\"transformed_carbohydrates_100g\", \"transformed_proteins_100g\", \"transformed_fat_100g\"]\nplot_features = [\"carbohydrates_100g\", \"proteins_100g\", \"fat_100g\"]","b87dea03":"N = 10000\n\ntrace1 = go.Scatter3d(\n    x=nutrition_table[plot_features[0]].values[0:N], \n    y=nutrition_table[plot_features[1]].values[0:N],\n    z=nutrition_table[plot_features[2]].values[0:N],\n    mode='markers',\n    marker=dict(\n        color=nutrition_table.cluster.values,\n        colorscale = \"Jet\",\n        opacity=0.3,\n        size=2\n    )\n)\n\nfigure_data = [trace1]\nlayout = go.Layout(\n    title = 'Clustering results',\n    scene = dict(\n        xaxis = dict(title=plot_features[0]),\n        yaxis = dict(title=plot_features[1]),\n        zaxis = dict(title=plot_features[2]),\n    ),\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    ),\n    showlegend=True\n)\n\nfig = go.Figure(data=figure_data, layout=layout)\npy.iplot(fig, filename='simple-3d-scatter')","4f0e913d":"cluster_count = nutrition_table.cluster.value_counts()\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.barplot(x=cluster_count.index,\n            y=cluster_count.values \/ nutrition_table.shape[0] * 100,\n            order=cluster_count.index, ax=ax[0])\nax[0].set_xlabel(\"Cluster Number\")\nax[0].set_ylabel(\"Percentage of data\")\nsns.distplot(cluster_count.values \/ nutrition_table.shape[0] * 100, bins=10, ax=ax[1])\nax[1].set_xlabel(\"Data coverage in %\")\nax[1].set_ylabel(\"Density\")","a80737ef":"results = nutrition_table.copy()\nresults[\"product\"] = original.loc[nutrition_table.index, \"product_name\"]\n\nfig, ax = plt.subplots(10,2, figsize=(20,50))\nfor m in range(10):\n    for n in range(2):\n        cluster = m*2+ n\n        title = \"Cluster \" + str(cluster) \n        make_word_cloud(results, cluster, ax[m,n], title)","f1433d01":"cluster_stats = pd.DataFrame(data=model.means_)\nsns.set(style=\"white\")\ncorr = cluster_stats.transpose().corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corr, mask=mask, cmap=\"coolwarm\",\n            vmin=-1, vmax=1, square=True, center=0,\n            linewidths=.5, cbar_kws={\"shrink\": .5},\n           annot=True)\nax.set_xlabel(\"Cluster\")\nax.set_ylabel(\"Cluster\")\nax.set_title(\"Cluster center correlation\")","75699628":"nutrients = [\"fat_100g\",\n             \"proteins_100g\",\n             \"carbohydrates_100g\",\n             \"sugars_100g\", \n             \"other_carbs\",\n             \"salt_100g\",\n             \"g_sum\"]\nenergies = [\"energy_100g\", \"reconstructed_energy\"]\n\ntransformed_nutrients = [\"transformed_\" + nutrient for nutrient in nutrients]\ntransformed_energies = [\"transformed_\" + energy for energy in energies]","1a588331":"def make_violin(subax, cluster, nutrients):\n    pos = np.arange(1, len(nutrients)+1)\n    part = subax.violinplot(\n            nutrition_table[nutrition_table.cluster==cluster]\n                [nutrients].values,\n            showmeans=True,\n            showextrema=False)\n    subax.set_title(\"Feature distributions of cluster: \" + str(cluster), size = 20)\n    subax.set_xticks(pos)\n    subax.set_xticklabels(nutrients)\n    set_color(part, len(nutrients))\n    return subax\n\ndef set_color(axes, num_colors):\n    cm = plt.cm.get_cmap('RdYlBu_r')\n    NUM_COLORS=num_colors\n    for n in range(len(axes[\"bodies\"])):\n        pc = axes[\"bodies\"][n]\n        pc.set_facecolor(cm(1.*n\/NUM_COLORS))\n        pc.set_edgecolor('black')\n    return axes","47c0dfdc":"cluster_pair = [5, 12]","8c2ae3e3":"sns.set()\n\nfig, ax = plt.subplots(2,1,figsize=(20,11))\npair1 = make_violin(ax[0], cluster_pair[0], energies)\npair2 = make_violin(ax[1], cluster_pair[1], energies)","a4f35662":"plt.figure(figsize=(10,10))\nq75 = nutrition_table.groupby(\"cluster\")[transformed_nutrients].quantile(0.75)\nq25 = nutrition_table.groupby(\"cluster\")[transformed_nutrients].quantile(0.25)\niqr = q75-q25\nsns.heatmap(iqr.transpose(), cmap=\"YlGnBu\")","b4ab9ee6":"plt.figure(figsize=(10,2))\nq75 = nutrition_table.groupby(\"cluster\")[[\"transformed_energy_100g\",\n                                          \"transformed_reconstructed_energy\"]].quantile(0.75)\nq25 = nutrition_table.groupby(\"cluster\")[[\"transformed_energy_100g\",\n                                          \"transformed_reconstructed_energy\"]].quantile(0.25)\niqr = q75-q25\nsns.heatmap(iqr.transpose(), cmap=\"YlGnBu\")","3a3a7bba":"species_probas = model.predict_proba(X_train)\nbest_species_idx = np.argmax(species_probas,axis=1)\n\ncolor = np.zeros(best_species_idx.shape[0])\nfor n in range(len(color)):\n    color[n] = np.round(species_probas[n,best_species_idx[n]], 4)\nnutrition_table[\"certainty\"] = color","a2b480ee":"fig, ax = plt.subplots(1,2, figsize = (20,5))\nsns.distplot(nutrition_table.certainty, color = \"red\", ax = ax[0],kde = False)\nsns.distplot(nutrition_table.certainty[nutrition_table.certainty < 0.95], color = \"orange\", kde = False)\nax[0].set_title(\"Distribution of certainty\")\nax[1].set_title(\"certainty < 0.95\")","29c1ec09":"plt.figure(figsize=(20,5))\nsns.violinplot(x=\"cluster\", y=\"certainty\", data=nutrition_table)","c2a2f39c":"cluster_pair=[9, 16]","c77f2054":"fig, ax = plt.subplots(2,2, figsize = (20,10))\nmake_word_cloud(results[nutrition_table.certainty == 1.],\n                cluster_pair[0],\n                ax[0,0],\n                title = \"cluster:\" + str(cluster_pair[0]) + \" \/\/ \" + \"certainty == 1.0\")\nmake_word_cloud(results[nutrition_table.certainty < 1.],\n                cluster_pair[0],\n                ax[0,1],\n                title = \"cluster:\" + str(cluster_pair[0]) + \" \/\/ \" + \"certainty < 1.0\")\n\nmake_word_cloud(results[nutrition_table.certainty == 1.],\n                cluster_pair[1],\n                ax[1,0],\n                title = \"cluster:\" + str(cluster_pair[1]) + \" \/\/ \" + \"certainty == 1.0\")\nmake_word_cloud(results[nutrition_table.certainty < 1.],\n                cluster_pair[1],\n                ax[1,1],\n                title = \"cluster:\" + str(cluster_pair[1]) + \" \/\/ \" + \"certainty < 1.0\")","1db987aa":"nutrition_table[\"alternative_cluster\"] = np.argsort(species_probas, axis=1)[:,-2]\nuncertain_table = nutrition_table[nutrition_table.certainty < 0.95]","c9c4f637":"competition = np.round(100 * uncertain_table.groupby(\n    \"cluster\").alternative_cluster.value_counts() \/ uncertain_table.groupby(\n    \"cluster\").alternative_cluster.count())\ncompetition = competition.unstack()\ncompetition.fillna(0, inplace=True)","2c656838":"plt.figure(figsize=(20,10))\nsns.heatmap(competition, cmap=\"Greens\", annot=True, fmt=\"g\")","41fd5245":"trace2 = go.Scatter3d(\n    x=nutrition_table[plot_features[0]].values[0:N], \n    y=nutrition_table[plot_features[1]].values[0:N],\n    z=nutrition_table[plot_features[2]].values[0:N],\n    mode='markers',\n    marker=dict(\n        color= color,\n        colorscale = \"Blackbody\",\n        opacity=0.5,\n        size=2\n    )\n)\n\nfigure_data = [trace2]\nlayout = go.Layout(\n    title = 'Certainty of Clustering',\n    scene = dict(\n        xaxis = dict(title= plot_features[0]),\n        yaxis = dict(title= plot_features[1]),\n        zaxis = dict(title= plot_features[2]),\n    ),\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    ),\n    showlegend=True\n)\n\nfig = go.Figure(data=figure_data, layout=layout)\npy.iplot(fig, filename='simple-3d-scatter')","3717ab36":"log_prob = model.score_samples(X_train)","e96dbec1":"your_choice = 0.07","d708522c":"plt.figure(figsize=(20,5))\n\nsns.distplot(log_prob, kde=False, bins=100, color=\"Red\")\ng1 = plt.axvline(np.quantile(log_prob, 0.25), color=\"Green\", label=\"Q_25\")\ng2 = plt.axvline(np.quantile(log_prob, 0.5), color=\"Blue\", label=\"Q_50 - Median\")\ng3 = plt.axvline(np.quantile(log_prob, 0.75), color=\"Green\", label=\"Q_75\")\ng4 = plt.axvline(np.quantile(log_prob, your_choice), color=\"Purple\", label=\"Q_ %i\" % (int(your_choice*100)))\nhandles = [g1, g2, g3, g4]\nplt.xlabel(\"log-probabilities of the data spots\")\nplt.xlim((-50,0))\nplt.ylabel(\"frequency\")\nplt.legend(handles) ","c3cfc648":"outliers = get_outliers(log_prob, your_choice)\nnutrition_table[\"anomaly\"] = outliers","b5b33b97":"anomalies_per_cluster = nutrition_table.groupby(\"cluster\").anomaly.value_counts() \/ nutrition_table.groupby(\"cluster\").anomaly.count()\nreset = anomalies_per_cluster.reset_index(level=\"cluster\")\npercentage_anomalies = reset.loc[1].set_index(\"cluster\", drop=True)\n\nplt.figure(figsize=(20,5))\nsns.barplot(x=percentage_anomalies.index, y=percentage_anomalies.anomaly.values * 100)\nplt.ylabel(\"Percentage of Anomalies\")","ae2453a0":"plt.figure(figsize=(20,5))\nsns.boxplot(x=\"cluster\", y=\"certainty\", data=nutrition_table[nutrition_table.anomaly==1])\nplt.title(\"Certainty of anomalies\")","ddc95a04":"cluster=5\n\nfeatures = [\"energy_100g\", \"reconstructed_energy\"]","95037aae":"fig, ax = plt.subplots(1,2, figsize = (20,10))\nax[0].scatter(nutrition_table[nutrition_table.cluster == cluster][features[0]].values,\n              nutrition_table[nutrition_table.cluster == cluster][features[1]].values,\n              c = nutrition_table[nutrition_table.cluster == cluster].anomaly.values)\nax[1].scatter(nutrition_table[nutrition_table.cluster == cluster][features[0]].values,\n              nutrition_table[nutrition_table.cluster == cluster][features[1]].values,\n              c = nutrition_table[nutrition_table.cluster == cluster].certainty.values)\nax[0].set_xlabel(features[0])\nax[0].set_ylabel(features[1])\nax[1].set_xlabel(features[0])\nax[1].set_ylabel(features[1])","ec5f3a2b":"fig, ax = plt.subplots(2,2, figsize=(20,10))\nmake_word_cloud(results[nutrition_table.anomaly==1], 1, ax[0,0], \"Anomalies in Bean Cluster 1\")\nmake_word_cloud(results[nutrition_table.anomaly==1], 13, ax[0,1], \"Anomalies in Bread Cluster 13\")\nmake_word_cloud(results[nutrition_table.anomaly==1], 9, ax[1,0], \"Anomalies in Pasta Cluster 9\")\nmake_word_cloud(results[nutrition_table.anomaly==1], 16, ax[1,1], \"Anomalies in Grain Cluster 16\")","f0dcedd2":"fig, ax = plt.subplots(1,1,figsize=(15,5))\nmake_word_cloud(results,\n                6,\n                ax,\n                title = \"Anomalistic garbage cluster 6\")","99fddb98":"cols_to_drop = [col for col in nutrition_table.columns if \"transformed_\" in col]\ncols_to_drop.extend([col for col in nutrition_table.columns if \"boxcox_\" in col])","cff74a13":"results.drop(cols_to_drop, axis=1).loc[(nutrition_table.cluster==6) & (nutrition_table.anomaly==1)].head(10)","32006544":"results.drop(cols_to_drop, axis=1).loc[(nutrition_table.cluster==6) & (nutrition_table.anomaly==0)].head(10)","16621d38":"trace2 = go.Scatter3d(\n    x=nutrition_table[plot_features[0]].values[0:N], \n    y=nutrition_table[plot_features[1]].values[0:N],\n    z=nutrition_table[plot_features[2]].values[0:N],\n    mode='markers',\n    marker=dict(\n        color=nutrition_table.anomaly.values,\n        colorscale = \"Portland\",\n        opacity=0.2,\n        size=1.5\n    )\n)\n\nfigure_data = [trace2]\nlayout = go.Layout(\n    title = 'Log3D of Proteins, Fat and Carbohydrates',\n    scene = dict(\n        xaxis = dict(title=plot_features[0]),\n        yaxis = dict(title=plot_features[1]),\n        zaxis = dict(title=plot_features[2]),\n    ),\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    ),\n    showlegend=True\n)\n\nfig = go.Figure(data=figure_data, layout=layout)\npy.iplot(fig, filename='simple-3d-scatter')","fbffba96":"### Searching for the optimal amount of clusters\n\nWe are faced with the challange to find the optimal number of components (clusters) for our gaussian mixture model. This is the second difficult part of our analysis. If we would chose to less components we would obtain clusters of mixed product types and far too broad ranged features per gaussian. For example, it does not make sense for one cluster to occupy the whole span from 0 to 100g of carbohydrates. In contrast if we chose to many components, we may find some fine-grained, special clusters of seldom products but we may also find separate clusters of the same product type. To understand what a more theoretical approach would advice us we ran GMM for different amount of components and took a look at three criteria: \n\n* The log-likelihood on the test data \n* The Bayesian Information Criterion (penalizes the log-likelihood with model complexity and number of data)\n* The Akaike Information Criterion (only penailizes the log-likelihood with model complexity)\n","02740fb8":"We found that some clusters have a very high percentage of anomalies or are completely occupied by them. We have already seen that cluster 6 has the tendency to hold all outliers. Consequently it's not strange that it mainly consists of data spots in low dense regions. Let's try to find out, how certain the cluster assignment of anomalies is and how they look like in some example clusters.","21c2f1b7":"### Take-Away\n\n* Cool! That's great! Data spots of high certainty are even closer in the product name\/type. \n* In contrast those with less certainty are often different in the product type. \n* The pasta and grain clusters for example show some overlaps in the uncertain data spots. There are some cereals in the pasta cluster that our model is uncertain about that are maybe under the hood of the grain cluster as well with a second peak of resposibility there. These would be products \"inbetween\". \n\nWe should gain a deeper understanding of competing clusters by looking at the second highest responsibility per data spot and cluster. Let's visualize this competitor distribution with a heatmap. ","b195a13c":"### Take-Away\n\n* That's again cool! Our pasta cluster 9 for example has in around 78 % of uncertain cases a high responsibility for the grain cluster 16. The other major uncertainties of the pasta cluster are assigned to our mixed type - outlier - cluster 6. \n* In contrast our grain cluster 16 shows only low percentage in uncertain cases for the pasta cluster, namely 11 %. Consequently we can't conclude that if two cluster compete that they do it in both directions!\n* This heatmap is really an advantage if you like to know which clusters or groups are somehow interacting.\n* One interesting competing cluster is the creamy, cheesy cluster 4. Many clusters like the meat cluster 10, the cheese cluster 14 and the virgin oil and sauces cluster 17 are in love with it. If you compare the features in the violinplot above, we can see that the cluster 4 covers a broad range of proteins, fat and salt. This causes the high tendency of second highest resposibilites.","d80de4b2":"Looking at the power parameter $\\lambda$ of the boxcox transformation you can see that we are able to compress outliers more and more by chosing lower values of $\\lambda$. In addition we obtain a \"negative\" stretching of low original values lower than one. If we fix $\\lambda =0.5$ and vary the constant $c$ we can observe only slight differences of the high values compression. On the other hand we can see that the stretching of low values is even stronger of constants c close to zero. \n\n#### Take-away (not that simple but still good)\n\n* If you like to compress outliers - chose a low $\\lambda$\n* If you like to stretch low values - chose a low $c$ close to zero.\n\n\n### Transform! \n\nOk, let's go! If you like, fork and chose other values. You will see that the resulting clusters highly depend on these hyperparameters!","897b9c56":"### Certainty 3D Scatterplot\n\nJust to obtain an impression how uncertainty of cluster assignments are looking in the feature space:","6a0e0e9f":"# Loading packages and data <a class=\"anchor\" id=\"load\"><\/a>\n\nBefore we can start we need to import some packages and the data of course:","68041f01":"**PS: If you like to fork and play with the results, don't plotly the 3D scatterplots due to instability of the kernel!  ;-) **\n\n# Data Science Trainee Program\n\nThis kernel is part of Leas trainee program in data science & machine learning. During this program she studied what is meant by machine learning and how can a model find pattern in data. To obtain a realistic impression of the power and difficulties of this hot field of our time we decided to chose a powerful model - namely the Gaussian Mixture Model - and real (& dirty) data of openfoodfacts.  During this kernel we discovered the various applications and the structure of Gaussian Mixture like clustering with clusters of different densities and sizes, anomaly detection and uncertainty analysis. We examined the importance of feature preprocessing like scaling and transformation and the difficulty of setting hyperparameters like the number of clusters\/components of our model.\n\nWe collaborated equally on this notebook and if you like our analysis, you can make us very happy with an upvote! :-)\nIf you have further questions, feedback or ideas, don't hesitate to leave a comment.\n\n\n## Can we cluster products of the same type? \n\nIn this Kernel we want have a closer look at the OpenFoodFacts data. Everyone who already used the OpenFoodFacts app knows that it is complicated to type in all queried information and you also have to put high effort into this procedure as well. This leads to wrong entries, regardless of whether it happened on purpose or just because someone made a mistake. By using the app you can also realise that there is no check or test for the user inputs. For example you can even type in negative amounts of fat if you want to. Because of these circumstances we can assume that there are a lot of mistakes in the OpenFoodFacts data.\n\nThese mistakes are not advantageous for the app if it wants to tell its users the important information about a product easily and fast, what definetly is one of the aims of the app. So it seems like a huge problem that the app is full of these mistakes and can't prevent them.\n\nWe now want to look closer at the given data and analyze possible false data. Besides the obvious mistakes like negative or too high entries we also want to use the Gaussian Mixture Model to find anomalies depending on the natural structure of the given data. \n\nIn addition to that we will also use the Gaussian Mixture Model to see if there are any natural structures in our data. This would be really helpful to classify products without entering lots of different categories like you have to do now while adding a product to the database. The app could also control itself by suggesting a category out of the given entries and then asking the user if the product really belongs to this category. \n\nAfter this analysis is done we will also discuss why the Gaussian Mixture Model was used and evaluate our results. Then we are also going to give an outlook for the possible future of the app.\n\n## Table of contents\n\n1. [Loading packages and data](#load) (complete)\n2. [Helper methods](#helper) \n3. [The Gaussian Mixture Model](#gmm)\n4. [Excluding missing values](#missing) (complete)\n5. [Feature engineering](#featureE) (complete)\n6. [Eliminating obvious error sources](#errors) (complete)\n7. [Feature scaling and transformation](#preprocessing) (complete)\n8. [Generate test data](#test) \n9. [Training](#training) (complete)\n10. [Clustering of product types](#clustering) (complete)\n11. [Certainty Analysis](#uncertainty) (complete)\n12. [Anomaly detection](#anomalies) (close to complete)\n13. [Conclusion](#conclusion) (close to complete)\n14. [Outlook](#outlook)\n\n","de957be9":"### Take-away\n\n* First of all: YES! We can find hidden product categories. That's amazing! No one told Gaussian Mixture something about product names or types. Only dealing with nutrition table information it was able to group similar products. We can find verious interesting clusters of chocolate, pasta, ice cream, cheese, yoghurts, juice, grains, sauces, meat, water and nuts.\n* On the goarse-grained view our clustering was very successful. But if you take a closer look you may wonder why some clusters hold similar products or seem to be of a mixed type like cluster 6.\n\nFor this reason, some more question arise:\n1. How sure is our model about our cluster assignments?\n2. How valid is the data in context of app user errors given a cluster?","7a892c50":"# Helper methods <a class=\"anchor\" id=\"helper\"><\/a>\n\nWe will use some helper methods that we collect in this chapter to reduce amount of code during analysis:\n\n* get_outliers\n* make_word_cloud\n* split_data_by_nan\n* scale_and_log","2eb26d2f":"### Visualizing anomalies","ac7173a2":"# Outlook <a class=\"anchor\" id=\"outlook\"><\/a>","fd0cfb27":"### Take-Away\n\n* We can find some very interesting patterns: In cluster 5 (water), 6 (mixed outlier cluster), 12 (sweets & sauces) and 17 (oils and cream dressings) the model is very certain in its cluster assignments in most of the anomaly cases! \n* In contrast cluster 3 (chocolate & cookies) and 1 (potatoes and beans) the model is very uncertain (below 0.6) about the cluster assignments of the majority of its anomalies. \n* Looking at all clusters we can see that most of them spread widely in their certainties.","7246740e":"### Take-away\n\n* We can see that most of our clusters cover around  5 % of our data. \n* Nonetheless there exist some clusters with verly low or very high coverage.\n\nWe should try to find out which kind of products and feature ranges belong to these clusters. This way we should try to find out if our clusters are inhomogenous or if we should use more components for our mixture model.","d0b70649":"## How much data is covered per cluster?","4e40d71e":"## What can we tell about the anomaly cluster 6?\n\nWe have found one special cluster that holds many high outlier products. Let's try to understand this special kind of cluster:","88c57552":"We can't find an obvious pattern!","12f69cf5":"Many of them look far better than before but the problem of setting optimal hyperparameters is still very difficult.","37e87e48":"Scrolling through the features we can observe that we have a lot of missing values in this data.","fc0b5fc4":"# Conclusion <a class=\"anchor\" id=\"conclusion\"><\/a>\n\n* The Gaussian Mixture Model is able to cluster the data in a useful way depending on their nutrition-tables. We have found various type of clusters that hold differnt product categories.\n* The model can even tell us how certain it is about the clustering assignment. \n* The probabilistic nature of the model can tell us something about competing clusters as well. They could be a good choice for some products as well. Sometimes the competitors are neighborhood clusters but we found some attractions towards a mixed outlier cluster as well.\n* We can also find anomalies (both rare and wrong entries).\n* There is also an anomaly-cluster - that holds many outliers, wrong entries and seldom products.\n* Besides these benefits the model needs much predatory work (cleaning, transforming, scaling the data)\n* It is also really depending on the decisions we make (e.g. the transformation-features and the number of components (cluster)). Consequently hyperparameter selection is the most crucial part.\n* But the model seems to be worth this work! It has much more to offer than a bunch of other clustering algorithms like K-Means for example.","fb0cf9de":"# Generate test and validation data <a class=\"anchor\" id=\"test\"><\/a>\n\n* Find the optimal number of clusters (test)\n* Show outlier\/anomalie detection on unseen data with outliers! ","96ff6186":"# Feature engineering <a class=\"anchor\" id=\"featureE\"><\/a>\n\nBefore we start our analysis we have to make some more adjustments to our dataset. First of all we want to add some more features that could be helpful for clustering our data.\n\nTherefore we add the feature **g_sum** which represents the **rounded sum of the fat-, carbohydrates-, proteins- and salt-values** in our data. By doing that we can easily see if there are some products with false entries.\n\nFurthermore we add the feature **other_carbs**  which includes the value of **all carbs that are not sugars**. Because of that our model can see the correlation between carbohydrates and sugars.\n\nThe last feature we want to add is **reconstructed_engery**. It calculates the energy value of a product **based on energy values of the features fat, carbohydrates and proteins**. We can compare this feature to the amount of energy that is given in our dataset to see if there possibly are some wrong entries.","57c49c53":"## How can we define at which density it's anomal?\n\nThe scoring of data points only yields sample log-likelihoods. No one tells us, if the sample is anomalous or not and we must define a criteria or some other strategy to proceed. In this case we decided to use a threshold that is based on the p-quartile of your choice. We set it by looking at the distribution over all sample-log-likelihoods. At the point were the distribution shows a sharp increase we set its value. This way we made sure that all spots in low dense regions are called an anomaly.   ","40482782":"Obviously our model is very certain about its cluster assignments for the majority of the data spots. Looking at samples with centainty below 95 % we can see a sharp break at 0.5. Consequently there are only very few samples that our model is really not sure to which cluster they should belong. We shall check whether these points could be outliers or anomalies. ","0e0c7f71":"For this step we only take into account uncertain data spots with a certainty below 95 %. This way we can clearly see which is a competing cluster for those samples. ","f81f02f7":"### Take-Away\n\n* By looking at the most common product names of anomalies per cluster we can see different kinds:\n    *  We can find **outlier products like blackeye peas, pumpernickel, puffed grain, products without gluten**\n    * But we can also find **seldom products** that do not fit to their clusters like **sugarfree products** in pasta cluster for example\n* Besides these outliers there are probably user errors of the data type-in input process as well. We don't know! ","2f254145":"## How are the nutrients of different clusters distributed? ","6b29de58":"## How similar are different clusters?","85d353c5":"## Are there some clusters with high uncertainty?\n\nPerhaps we have some cluster with more uncertain assigments than others. And perhaps similar clusters are both more uncertain. Let's find it out!","a604f36e":"# Certainty Analysis <a class=\"anchor\" id=\"uncertainty\"><\/a>\n\nWouldn't it be nice to find out what kind of products are somehow inbetween of two or more clusters? \n\nWe are lucky! Our Gaussian Mixture Model is a probabilistic model that can tell us, how certain it was during its cluster assignments. Intuitively one would say: \"Let's take a look at $\\pi_{k} N(\\mu_{k}, \\Sigma_{k})$ per cluster component. But as we want a probability, we need to normalize over all components of our model. Looking at the E-Step, we can see that this are our converged responsibilities! :-) During the E-Step our model calculates how responsible some cluster $k$ is for some data spot $n$. Consequently we can try to understand for which kind of products or model is not certain about in its cluster assignment. This is a great advantage of Gaussian Mixture compared to non-probabilistic models like K-Means.","4a846c36":"## How do anomalies of normal clusters look like?","eb7ce554":"### Visual comparison\n\nLet's have a look at our transformed features:","f88425b0":"# Training <a class=\"anchor\" id=\"training\"><\/a>","e0ff2524":"### Take-away\n\n* Many clusters do not correlate or show anti-correlation. This is what we like to have as we want them do be dissimilar in the feature space. This way we can make sure that they can cover different type of products. \n* Unfortunately we also find some high correlating clusters that have nearby cluster centers like the pasta cluster 9 and the whole grain cluster 16. In this case it seems to make sense. Even though these kind of products are very similar, pasta seems to be different from whole grain. That's cool! But we should try to understand in the feature space what makes this difference.","a560450d":"### Take-Away\n\n* Comparing two clusters of our choice we can obtain an deeper understanding why similar clusters are still different. They are often very close in all features in the transformed feature space, sometimes the feature distributions overlap. But even if they are very close together small and tiny shifts in some features make the difference. \n* In the case of the pasta and grain cluster we can see that pasta has a little lower fat and higher protein than the whole grain cluster. In contrast, the grain cluster holds more products with verious amounts of sugars and salt.\n* Looking at the mixed type cluster 6 we can see that it covers outliers of each feature distribution. You might wonder and ask yourself why we still see broad distributions on low values as well but this can be explained when you keep in mind, that an outlier might only occur in one or few features per product whereas all others behave normal. ","d2248842":"# Anomaly detection <a class=\"anchor\" id=\"anomalies\"><\/a>\n\nThe last and perhaps the most important part of the app is anomaly detection. But first of all, what is meant by an anomaly in the view of a gaussian mixture model? Before we start, we should be certain about the answer. Let's recap the formulation of the Gaussian Mixture Model:\n\n$$ p(x) = \\sum_{k=1}^{K} \\pi_{k} \\cdot N(x|\\mu_{k}, \\Sigma_{k})$$\n\nThis sum yields a so called probability density function. If you are not very familiar with measure theory you are in good company. If we assume that all data spots are drawn randomly from that density distribution and finally take the natural log, we would end up with the following formula:\n\n$$ \\ln p(x) = \\sum_{n=1}^{N} ln \\sum_{k=1}^{K} \\pi_{k} \\cdot N(x_{n}|\\mu_{k}, \\Sigma_{k}) $$\n\nThe stuff we need to score each data spot is now given by:\n\n$$ \\ln p_{n}(x) = \\ln \\sum_{k=1}^{K} \\pi_{k} \\cdot N(x_{n}|\\mu_{k}, \\Sigma_{k}) $$\n\nThis is what we obtain by calling model_score_samples of scikit-learns gaussian mixture model. But what does it tell us? We think that it is a measure how dense the region is where data spot $x_{n}$ is located. Consequently in low dense regions of our probability density function this would yield a low number. Let's go further - what can be detected as anomaly in low dense regions?\n\n* outliers that could be errors\n* seldom products\n\nWe should keep in mind that there is one kind of error we will not detect as anomaly: Imagine a user typed in nutrition table information that contains flips between features or a wrong product name. This kind of data spot can be located in dense regions and perfectly match with some cluster. Consequently we would not detect it as an anomaly and we can't find it as an error! ","5a4c7037":"After dropping all incomplete nutrition tables, we are left with the following total number of samples:","320f9b5f":"## Can we reveal the hidden product type?","459aa936":"To obtain a first impression of the similarity of our clusters we can look at the correlation of the cluster center:","798232d1":"# The Gaussian Mixture Model <a class=\"anchor\" id=\"gmm\"><\/a>","e8ae69f2":"Let's see how many products have NaN-entries in at least one of the features below. After that we are going to drop these products so our dataset doesn't have missing values anymore.","66a599ee":"## How certain is our model about its anomalies in cluster assignment ?","26653091":"# Clustering of product types <a class=\"anchor\" id=\"clustering\"><\/a>","78fae64d":"Fitting with 20 components seemed to be a good choice looking at cluster center correlations later in the analysis. Select 3 features of your choice and take look at these interesting clusters:\n\n* carbohydrates_100g\n* fat_100g\n* proteins_100g\n* salt_100g\n* other_carbs\n* sugars_100g\n* energy_100g\n* reconstructed_energy\n* g_sum\n\nYou can select the transformed features as well with \"transformed_\" + your_feature!\n\nFor the plot we only used 40000 data spots for stability reasons of plotly.","684b5f73":"## How dense are our clusters?","2255f109":"## How certain are our cluster assignments?","3edceb3d":"## How do certain and uncertain product names look like?\n\nLooking at the distributions of cluster assignment certainty we can't find interesting relationships. Probably the product name is more informative to find interesting patterns:","1dc30d54":"**Peek at the data**","6a992e7f":"For us a value of 7 % anomalies in the data seemed to be suitable. Try another value if you like ;-) !","e12537bb":"#### Take-away\n\n* The pure log-likelihood as well as the AIC and BIC tell us: \"the more clusters, the better!\"\n* But we know we should be very careful. Products of the same kind could spread widely in their nutrition features according to many differences in the ingredients. In addition we are challanged by dirty and real data. User can make various of different errors like flipping nutrients while typing in the nutrition table or setting commas in numerical values on the wrong place. Even mistakes on purpose with wrong entries are possible. \n\nConsequently we will try another approach as well to select a sufficent number of clusters by looking at the correlation of features between different clusters.","96f46ad9":"As already explained we will use the transformed and scaled features during the learning procedure to improve the update of the M-Step. A difficult and not obvious task is to chose the right amount of components of our mixture model. If you uncomment the code part \"searching for the optimal amount of clusters\" you can see that we tried to estimate the optimal number of components on test data.","b1bcfe60":"# Eliminating obvious error sources <a class=\"anchor\" id=\"errors\"><\/a>\n\nNow that we implemented our new features we also want to exclude obvious wrong entries, so that we delete all products with:\n\n* a feature (except for the energy-ones) higher than 100g\n* a feature with a negative entry\n* an energy-amount of more than 3700kJ (the maximum amount of energy a product can have; in this case it would conists of 100% fat)\n* more sugars than carbohydrates\n* g_sum higher than 100g","94bdc498":"# Excluding missing values <a class=\"anchor\" id=\"missing\"><\/a>\n\nTo cluster products we like to focus on the nutrition table as this information is often typed in by the user and will be automatically read in by deep learning networks in the future. Consequently we will only use these features:","e6738cd5":"First of all, what is the idea of a gaussian mixture model? \n\n\n\n$$ p(x) = \\sum_{k=1}^{K} \\pi_{k} \\cdot N(x|\\mu_{k}, \\Sigma_{k}) $$\n\n\n#### E-Step:  \n\nCalculate how responsible a cluster $z_{k}$ is for one data spot $x_{n}$.\n\n$$\\gamma_{nk}  = \\frac {\\pi_{k} \\cdot N(x|\\mu_{k}, \\Sigma_{k})} {\\sum_{j=1}^{K} \\pi_{j} \\cdot N(x|\\mu_{j}, \\Sigma_{j})}$$\n\n\n\n#### M-Step:\n\n$$\\mu_{k} = \\frac{1}{N_{k}} \\sum_{n=1}^{N} \\gamma_{nk} x_{n} $$\n\n$$\\Sigma_{k} =  \\frac{1}{N_{k}}  \\sum_{n=1}^{N}  \\gamma_{nk} (x_{n} - \\mu_{k}) (x_{n} - \\mu_{k})^{T}$$ \n\nOhOhOh! What do we see there? :-)\n\nWe have to be very careful with outliers as our training procedure depends on the features $x_{n}$ themselves!! High outlier values shift the means and covariances of clusters to higher values as well! That motivates extensive data preprocessing for training data!\n","32f04963":"### Take-away\n\n* Every cluser belongs to a component that has its own covariance matrice. As we allowed full matrices during training we can see clusters of various sizes and densities. This is one advantage of using the gaussian mixture model.\n* However some less dense clusters in boundary regions show some outliers even though they have a compact and dense base structure. In these cases we are curious if the model is centrain about its cluster assignment or if it detected those points as anomalies. \n* If you compare the scattered data of original and transformed features you can see that our preprocessing was able to expand the room the data occupies. This way we uncovered substructures that would have been hidden using the skewed original feature distributions. \n* We can find a cluster that somehow looks like protective shield. It seems to hold all outliers in the data and its cluster center may be located somewhere in the middle of all data. That is very interesting! We haven't expected that!","639d9d00":"# Feature scaling and transformation <a class=\"anchor\" id=\"preprocessing\"><\/a>\n\nAfter these steps our dataset is cleaned and optimized for our analysis, but because we want to use the Gaussian Mixture Model we still have to scale and transform the data a bit. Looking closer on how our model learns, we can see two problems in the M-Step:\n\n$$\\mu_{k} = \\frac{1}{N_{k}} \\sum_{n=1}^{N} \\gamma_{nk} x_{n} $$\n\n$$\\Sigma_{k} =  \\frac{1}{N_{k}}  \\sum_{n=1}^{N}  \\gamma_{nk} (x_{n} - \\mu_{k}) (x_{n} - \\mu_{k})^{T}$$ \n\n### Why should we transform features?\n\nThe cluster center $\\mu_{k}$ and the covariance matrices $\\Sigma_{k}$ are influenced by the feature values $x_{n}$ themselves. As we have skewed feature distributions with high outliers the cluster center are shifted towards higher values during each update step. This is caused by the weighted mean. Even though the responsibilities as weights are able to soften the effect of outliers, they can still contribute very much. In this case our model builds clusters during learning that try to match the structure of outliers. The cluster center would not explain the majority of its cluster members and could be even located in regions with no data spots at all. To solve this problem we transform our features using boxcox transformation. Doing so we like to expand the majority of data points to uncover hidden substructures and compress the outliers.\n\nEven this sounds easy, this is the most toughest part of all! \n\n### Why should we scale them?\nEven with transformed features there is still a problem left: So far all our features have positive values and the responsibilities are positive as well as they are probabilities between 0 and 1.  This will still cause a shift in the center and covariance updates as the assignments will be lead by the higher values. To correct this we want to shift our transformed feature distributions such that it is evenly distributed around 0. For this purpose we subtract the mean and scale to unit variance. \n\n### How does to boxcox knead our features?\nUsing the boxcox transformation we are able to knead the feature distributions as we like them to be. But this this great power comes also great responsibility ;-) . Consequently we should try to understand what boxcox can do with our values!","0d2eaae6":"## Is the percentage of anomalies dependent on the cluster?","2268d64f":"## Which cluster was the second choice of certainty per cluster?"}}