{"cell_type":{"ed0c1116":"code","da30106d":"code","05d5ad79":"code","5f908732":"code","9ac7f628":"code","e122066e":"code","5aa959d5":"code","6fe5df14":"code","bea8033c":"code","c694f30d":"code","4adeb212":"code","b88d0190":"code","62733520":"code","229164f8":"code","146b31af":"code","0a52ba9c":"code","82f18d5e":"code","663da710":"code","7ba648b6":"code","34665e5c":"code","c51bfaa6":"code","1fd22dd4":"code","4d547169":"code","e6b8ac2e":"code","de953169":"code","21c8dc63":"code","7d6d7151":"code","33cafa2e":"code","954399a7":"code","0bbcb33a":"code","2cdacb73":"code","341d5f90":"code","c010ebe2":"code","4e8443ea":"code","0d8ab01d":"code","ff421058":"code","1d18b136":"markdown","893d9ec2":"markdown"},"source":{"ed0c1116":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da30106d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb","05d5ad79":"covid_df = pd.read_csv('..\/input\/covid19-sentiments\/COVID-19_Sentiments.csv')\ncovid_df.head()","5f908732":"covid_df.shape","9ac7f628":"if covid_df.duplicated().any():\n    print('Duplicates Found')\nelse:    \n    print('No Duplicates Found')","e122066e":"covid_df = covid_df.drop_duplicates().reset_index(drop=True)","5aa959d5":"covid_df.shape","6fe5df14":"covid_df.isna().sum()","bea8033c":"covid_df.Sentiments.fillna(covid_df['Sentiments'].mean(),inplace=True)","c694f30d":"covid_df.isna().sum()","4adeb212":"covid_df.head()","b88d0190":"covid_df.Sentiments.value_counts()","62733520":"def response(x):\n    if x > 0:\n        return 1\n    elif x < 0:\n        return -1\n    else :\n        return 0\ncovid_df['Sentiments'] = covid_df['Sentiments'].apply(response)\n","229164f8":"covid_df.Sentiments.value_counts()","146b31af":"covid_df.head(10)","0a52ba9c":"covid_df.drop(['Text_Id', 'Date', 'Location'], axis =1, inplace = True)\n","82f18d5e":"covid_df = covid_df[covid_df.Sentiments != 0]","663da710":"covid_df.head()","7ba648b6":"import string\nstring.punctuation\n\ndef remove_punctuation(text):\n    no_punct = [words for words in text if words not in string.punctuation]\n    words_wo_punct = ''.join(no_punct)\n    return words_wo_punct\ncovid_df[\"Text_wo_punct\"] = covid_df['Text'].apply(lambda x: remove_punctuation(x))\ncovid_df.head()\n    ","34665e5c":"from nltk.tokenize import RegexpTokenizer\nimport re\ntokenizer = RegexpTokenizer(r'\\w+') \ncovid_df['Text_wo_punct'] = covid_df['Text_wo_punct'].apply(lambda x: tokenizer.tokenize(x.lower()))\ncovid_df.head()","c51bfaa6":"from nltk.corpus import stopwords\nstop = list(stopwords.words('english'))\nremove = ['i','a','d','o','y']\nstop = [ele for ele in stop if ele not in remove ]\nprint(len(stop))\n\ndef remove_stopwords(text):\n    words = [words for words in text if words not in stop]\n    return words\ncovid_df['Text_wo_punct_wo_sw'] = covid_df['Text_wo_punct'].apply(lambda x: remove_stopwords(x))\ncovid_df.head()","1fd22dd4":"from nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\ndef lemmatise(text):\n    lem_text = ' '.join([lemm.lemmatize(words) for words in text])\n    return lem_text\ncovid_df['Final_Text'] = covid_df['Text_wo_punct_wo_sw'].apply(lambda x: lemmatise(x))\ncovid_df","4d547169":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(covid_df.Final_Text, covid_df.Sentiments, test_size = 0.3, random_state = 1)","e6b8ac2e":"good = x_train[y_train[y_train == 1].index]\n#neutral = x_train[y_train[y_train == 0].index]\nbad = x_train[y_train[y_train == -1].index]","de953169":"from wordcloud import WordCloud\n\n\nplt.figure(figsize = (20,20))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800).generate(\" \".join(good))\n\nplt.imshow(wc,interpolation = 'bilinear')","21c8dc63":"#plt.figure(figsize = (20,20))\n#wc = WordCloud(min_font_size = 3, max_words = 3000, width =1600, height = 800).generate(\" \".join(neutral))\n#plt.imshow(wc,interpolation = 'bilinear')","7d6d7151":"plt.figure(figsize = (20,20))\nwc = WordCloud(min_font_size = 3, max_words = 3000, width = 1600, height = 800).generate(\" \".join(bad))\nplt.imshow(wc, interpolation = 'bilinear')","33cafa2e":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","954399a7":"from sklearn.feature_extraction.text import TfidfVectorizer\nobj = TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\nobj_xtrain = obj.fit_transform(x_train)\nobj_xtest = obj.transform(x_test)\nprint(obj_xtrain.shape)\nprint(obj_xtest.shape)\n","0bbcb33a":"from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=0)\nlr_tfidf=lr.fit(obj_xtrain,y_train)","2cdacb73":"#lr_tfidf_predict = lr.predict(obj_xtest)\nprint(lr.score(obj_xtest,y_test))","341d5f90":"#lr_tfidf_score=accuracy_score(y_test,lr_tfidf_predict)\n#print(lr_tfidf_score)","c010ebe2":"from sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(obj_xtrain, y_train)","4e8443ea":"#mnb_tfidf_predict = classifier.predict(obj_xtest)\nprint(classifier.score(obj_xtest, y_test))\n","0d8ab01d":"#mnb_tfidf_score = accuracy_score(y_test,mnb_tfidf_predict)\n#print(mnb_tfidf_score)","ff421058":"from sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC(C=0.5, random_state=42)\nlinear_svc.fit(obj_xtrain, y_train)\n\nprint(linear_svc.score(obj_xtest,y_test))","1d18b136":"Data Cleaning","893d9ec2":"**Training Model for prediction **"}}