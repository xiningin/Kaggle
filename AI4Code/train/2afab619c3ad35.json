{"cell_type":{"5c950c80":"code","5cc375fc":"code","92c6d60f":"code","f7e8d576":"code","4ff03631":"code","d3e195fc":"code","a5c784b5":"code","85ddb77a":"code","9bf18c11":"code","80d7884c":"code","dc5426f5":"code","e268a51c":"code","c048d27b":"code","7e6348e2":"markdown","e6b7ec35":"markdown","eb67b94a":"markdown","c0f2ae37":"markdown","bc0054da":"markdown","c991ca90":"markdown","e9b646fa":"markdown","2919f694":"markdown","77d0a25a":"markdown","60c70bd8":"markdown","33747b6b":"markdown","3203fb79":"markdown","c9a11c1d":"markdown","3e525e03":"markdown"},"source":{"5c950c80":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Used to change filepaths\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport IPython\nfrom IPython.display import display\nfrom PIL import Image\n%matplotlib inline\nimport os\nprint(os.listdir(\"..\/input\"))","5cc375fc":"IPython.display.Image(filename='..\/input\/flowers\/flowers\/rose\/12202373204_34fb07205b.jpg') ","92c6d60f":"IPython.display.Image(filename='..\/input\/flowers\/flowers\/sunflower\/1008566138_6927679c8a.jpg') ","f7e8d576":"# generate test_data.\ntest_data=np.random.beta(1, 1, size=(120, 120, 3))\n\n# display the test_data\nplt.imshow(test_data)","4ff03631":"# open the image\nimg = Image.open('..\/input\/flowers\/flowers\/rose\/14510185271_b5d75dd98e_n.jpg')\n# Get the image size\nimg_size = img.size\n\nprint(\"The image size is: {}\".format(img_size))\n\n# Just having the image as the last line in the cell will display it in the notebook\nimg","d3e195fc":"# Crop the image to 25, 25, 75, 75\nimg_cropped = img.crop([25,25,75,75])\ndisplay(img_cropped)\n\n# rotate the image by 45 degrees\nimg_rotated = img.rotate(45,expand=25)\ndisplay(img_rotated)\n\n# flip the image left to right\nimg_flipped = img.transpose(Image.FLIP_LEFT_RIGHT) \ndisplay(img_flipped)","a5c784b5":"# Turn our image object into a NumPy array\nimg_data = np.array(img)\n\n# get the shape of the resulting array\nimg_data_shape = img_data.shape\n\nprint(\"Our NumPy array has the shape: {}\".format(img_data_shape))\n\n# plot the data with `imshow` \nplt.imshow(img_data)\nplt.show()\n\n# plot the red channel\nplt.imshow(img_data[:,:,0], cmap=plt.cm.Reds_r)\nplt.show()\n\n# plot the green channel\nplt.imshow(img_data[:,:,1], cmap=plt.cm.Greens_r)\nplt.show()\n\n# plot the blue channel\nplt.imshow(img_data[:,:,2], cmap=plt.cm.Blues_r)\nplt.show()","85ddb77a":"def plot_kde(channel, color):\n    \"\"\" Plots a kernel density estimate for the given data.\n        \n        `channel` must be a 2d array\n        `color` must be a color string, e.g. 'r', 'g', or 'b'\n    \"\"\"\n    data = channel.flatten()\n    return pd.Series(data).plot.density(c=color)\n\n# create the list of channels\nchannels = ['r','g','b']\n    \ndef plot_rgb(image_data):\n    # use enumerate to loop over colors and indexes\n    for ix, color in enumerate(channels):\n        plt.imshow(image_data[:,:,ix])\n        plt.show()\n    \nplot_rgb(img_data)","9bf18c11":"# load rose\nrose =Image.open('..\/input\/flowers\/flowers\/rose\/16001846141_393fdb887e_n.jpg')\n# display the rose image\ndisplay(rose)\n\n# NumPy array of the rose image data\nrose_data=np.array(rose)\n# plot the rgb densities for the rose image\nplot_rgb(rose_data)","80d7884c":"# load sunflower\nsunflower =Image.open('..\/input\/flowers\/flowers\/sunflower\/1044296388_912143e1d4.jpg')\n# display the sunflower image\ndisplay(sunflower)\n# NumPy array of the sunflower image data\nsunflower_data=np.array(sunflower)\n# plot the rgb densities for the sunflower image\nplot_rgb(sunflower_data)","dc5426f5":"# convert rose to grayscale\nrose_bw = rose.convert(\"L\")\ndisplay(rose_bw)\n\n# convert the image to a NumPy array\nrose_bw_arr = np.array(rose_bw)\n\n# get the shape of the resulting array\nrose_bw_arr_shape = rose_bw_arr.shape\nprint(\"Our NumPy array has the shape: {}\".format(rose_bw_arr_shape))\n\n# plot the array using matplotlib\nplt.imshow(rose_bw_arr, cmap=plt.cm.gray)\nplt.show()\n\n# plot the kde of the new black and white array\nplot_kde(rose_bw_arr, 'k')","e268a51c":"# flip the image left-right with transpose\nrose_bw_flip = rose.transpose(Image.FLIP_LEFT_RIGHT)\n\n# show the flipped image\ndisplay(rose_bw_flip)\n\n# save the flipped image\nrose_bw_flip.save(\"bw_flipped.jpg\")\n\n# create higher contrast by reducing range\nrose_hc_arr = np.maximum(rose_bw_arr, 100)\n\n# show the higher contrast version\nplt.imshow(rose_hc_arr, cmap=plt.cm.gray)\n\n# convert the NumPy array of high contrast to an Image\nrose_bw_hc = Image.fromarray(rose_hc_arr,\"L\")\n\n# save the high contrast version\nrose_bw_hc.save(\"bw_hc.jpg\")","c048d27b":"# take only four image from sunflower\nimage_paths = ['..\/input\/flowers\/flowers\/sunflower\/1022552036_67d33d5bd8_n.jpg',\n               '..\/input\/flowers\/flowers\/sunflower\/14121915990_4b76718077_m.jpg',\n               '..\/input\/flowers\/flowers\/sunflower\/1043442695_4556c4c13d_n.jpg',\n               '..\/input\/flowers\/flowers\/sunflower\/14472246629_72373111e6_m.jpg']\n\ndef process_image(path):\n    img = Image.open(path)\n\n    # create paths to save files to\n    bw_path = \"bw_{}.jpg\".format(path.stem)\n    rcz_path = \"rcz_{}.jpg\".format(path.stem)\n\n    print(\"Creating grayscale version of {} and saving to {}.\".format(path, bw_path))\n    bw = img.convert(\"L\").save(bw_path)\n    print(\"Creating rotated, cropped, and zoomed version of {} and saving to {}.\".format(path, bw_path))\n    rcz = img.rotate(45).crop([25, 25, 75, 75]).resize((100, 100)).save(rcz_path)\n\n# for loop over image paths\nfor img_path in image_paths:\n    process_image(Path(img_path))","7e6348e2":"## Images as arrays of data\nWhat is an image? So far, PIL has handled loading images and displaying them. However, if we're going to use images as data, we need to understand what that data looks like.\n\nMost image formats have three color <a href=\"https:\/\/en.wikipedia.org\/wiki\/RGB_color_model\">\"channels\": red, green, and blue<\/a> (some images also have a fourth channel called \"alpha\" that controls transparency). For each pixel in an image, there is a value for every channel.<\/p>\n![RGB Colors](https:\/\/lh3.googleusercontent.com\/JYhIGVIvnCU_txibv3XBIpI1PMcLZuijPn-_o65uzL8IyfZsl5jKI6-VeLpwHXcGxcQayPmMOhpueslHtwvJQvcdu8V_gQLal-VfAQ4SKXtDddZ9nhwnrZDYUsaGbe2Lzqngq24qGuKQMObRTgllmRTHePYhS7_qb5Xa4Kw73pWu2SeiOWkuDw0ZVlOohbWzkRlbTjHUCPHpYKEZfg7OAIPX3RdUhBl4DmjdMipr1-Kf9esqX8zwNTHSXjD3mI0uJ4XvtsK6GzBcBBMh93lO90J7uzSesXhSVN3H11wdl2thbxUm_YYvEGphHfDZyyBANQ9hmrHLb3P6fePxaOLFkroqs7qMmsYMPaaqSOHVeF1WnncH2KUY6XuNYf4fbCvywmeUP2FfOJH1UpF1zipTm699bxhCV1inG-yPdabQIrczF50O3BRDLMBE5uWM3nAcAdGxBy0Ea0XUV9-yq-UhJFz70hhl8XbH_ZSr1Y5pnsmUEdwlzu5GVIC9J4HVDMwHQuNA7SnNzmPZuE0O_x8X6c0yvy582Q5yaq5Wwf8XkGVGCE3nwosK_yaslgSegB2Qw_ISJEkpZM6-VRRpjHwZOFrhVArGf9yHKbP9KcLPoz1vXwhZ8UKKQP7aTqNpZvmh-Lis8UY3r8FxuI8Myuvo3EawZbEV5Oo=s400-no)\nThe way this is represented as data is as a three-dimensional matrix. The width of the matrix is the width of the image, the height of the matrix is the height of the image, and the depth of the matrix is the number of channels. So, as we saw, the height and width of our image are both 100 pixels. This means that the underlying data is a matrix with the dimensions <code>100x100x3<\/code>.<\/p>","e6b7ec35":"## Save your work!\nWe've been talking this whole time about making changes to images and the manipulations that might be useful as part of a machine learning pipeline. To use these images in the future, we'll have to save our work after we've made changes.\n\nNow, we'll make a couple changes to the <code>Image<\/code> object from Pillow and save that. We'll flip the image left-to-right, just as we did with the color version. Then, we'll change the NumPy version of the data by clipping it. Using the <code>np.maximum<\/code> function, we can take any number in the array smaller than <code>100<\/code> and replace it with <code>100<\/code>. Because this reduces the range of values, it will increase the <a href=\"https:\/\/en.wikipedia.org\/wiki\/Contrast_(vision)\">contrast of the image<\/a>. We'll then convert that back to an <code>Image<\/code> and save the result.","eb67b94a":"The question at hand is: can a machine identify a flower as a rose or a sunflower? These flowers have different appearances,but given the variety of backgrounds, positions, and image resolutions it can be a challenge for machines to tell them apart.\n\nBeing able to identify flowers types from images is a task that ultimately would allow researchers to more quickly and effectively collect field data.","c0f2ae37":"## Explore the color channels\nColor channels can help provide more information about an image. A picture of the ocean will be more blue, whereas a picture of a field will be more green. This kind of information can be useful when building models or examining the differences between images.\n\nWe'll look at the <a href=\"https:\/\/en.wikipedia.org\/wiki\/Kernel_density_estimation\">kernel density estimate<\/a> for each of the color channels on the same plot so that we can understand how they differ.\n\nWhen we make this plot, we'll see that a shape that appears further to the right means more of that color, whereas further to the left means less of that color.","bc0054da":"## Rose and Sunflower(ii)\nNow let's look at the sunflower.","c991ca90":"## Rose and Sunflower(i)\nNow we'll look at two different images and some of the differences between them. The first image is of a rose, and the second image is of a sunflower.\n<p>First, let's look at the rose.<\/p>","e9b646fa":"## Make a pipeline\nNow it's time to create an image processing pipeline. We have all the tools in our toolbox to load images, transform them, and save the results.\n\nIn this pipeline we will do the following:\n<ul>\n<li>Load the image with <code>Image.open<\/code> and create paths to save our images to<\/li>\n<li>Convert the image to grayscale<\/li>\n<li>Save the grayscale image<\/li>\n<li>Rotate, crop, and zoom in on the image and save the new image<\/li>\n<\/ul>","2919f694":"## Simplify, simplify, simplify\nWhile sometimes color information is useful, other times it can be distracting. In this examples where we are looking at flowers, the flowers are very similar colors.so let's convert these images to <a href=\"https:\/\/en.wikipedia.org\/wiki\/Grayscale\">black-and-white, or \"grayscale.\"<\/a>\n\nGrayscale is just one of the <a href=\"https:\/\/pillow.readthedocs.io\/en\/5.0.0\/handbook\/concepts.html#modes\">modes that Pillow supports<\/a>. Switching between modes is done with the <code>.convert()<\/code> method, which is passed a string for the new mode.\n\nBecause we change the number of color \"channels,\" the shape of our array changes with this change. It also will be interesting to look at how the KDE of the grayscale version compares to the RGB version above.","77d0a25a":"<em>A rose.<\/em>","60c70bd8":"This notebook walks through loading and processing images. After loading and processing these images, they will be ready for building models that can automatically detect rose and sunflowers.","33747b6b":"## Image manipulation with PIL\nPillow has a number of common image manipulation tasks built into the library. For example, one may want to resize an image so that the file size is smaller. Or, perhaps, convert an image to black-and-white instead of color. Operations that Pillow provides include:\n<ul>\n<li>resizing<\/li>\n<li>cropping<\/li>\n<li>rotating<\/li>\n<li>flipping<\/li>\n<li>converting to greyscale (or other <a href=\"https:\/\/pillow.readthedocs.io\/en\/5.1.x\/handbook\/concepts.html#concept-modes\">color modes<\/a>)<\/li>\n<\/ul>\n\nOften, these kinds of manipulations are part of the pipeline for turning a small number of images into more images to create training data for machine learning algorithms. This technique is called <a href=\"http:\/\/cs231n.stanford.edu\/reports\/2017\/pdfs\/300.pdf\">data augmentation<\/a>, and it is a common technique for image classification.\n<p>We'll try a couple of these operations and look at the results.<\/p>","3203fb79":"## Opening images with PIL\nNow that we have all of our imports ready, it is time to work with some real images.\n\nPillow is a very flexible image loading and manipulation library. It works with many different image formats, for example, <code>.png<\/code>, <code>.jpg<\/code>, <code>.gif<\/code> and more. For most image data, one can work with images using the Pillow library (which is imported as <code>PIL<\/code>).<\/p>\n<p>Now we want to load an image, display it in the notebook, and print out the dimensions of the image. By dimensions, we mean the width of the image and the height of the image. These are measured in pixels. The documentation for <a href=\"https:\/\/pillow.readthedocs.io\/en\/5.1.x\/reference\/Image.html\">Image<\/a> in Pillow gives a comprehensive view of what this object can do.<\/p>","c9a11c1d":"## Import Python libraries","3e525e03":"\n<em>A sunflower.<\/em>"}}