{"cell_type":{"2791f09f":"code","03a503cd":"code","cc591fb0":"code","36f1f927":"code","53587b12":"code","faba5a6c":"code","30fd673d":"code","a52e15ef":"code","8abb663d":"code","63c3f5d7":"markdown","5ab30428":"markdown","86376705":"markdown","291e95b3":"markdown","058c251a":"markdown","017e8bbd":"markdown"},"source":{"2791f09f":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import RandomizedSearchCV\nimport lightgbm\nimport matplotlib.pyplot as plt","03a503cd":"train = pd.read_csv(\"..\/input\/learntogether\/train.csv\")\ntest = pd.read_csv(\"..\/input\/learntogether\/test.csv\")\n\ntrain = train.drop([\"Id\"], axis = 1)\n\ntest_ids = test[\"Id\"]\ntest = test.drop([\"Id\"], axis = 1)","cc591fb0":"from sklearn.ensemble import RandomForestClassifier\ndef random_lgbm(X,y,n_iter=250):\n    n_estimators = [int(x) for x in np.linspace(start = 5, stop = 1000, num = 100)]\n    random_grid={\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ],\n    \"n_estimators\": n_estimators}\n    random_grid = {\n    'learning_rate': [0.005, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ],\n        \"max_depth\"        : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n    'n_estimators': n_estimators,\n    'num_leaves': [6,8,12,16],\n    'boosting_type' : ['gbdt'],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n    'objective' : ['multiclass'],\n     \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n    'random_state' : [501], # Updated from 'seed'\n    'colsample_bytree' : [0.3, 0.4, 0.65, 0.66],\n    'subsample' : [0.7,0.75],\n    'reg_alpha' : [1,1.2],\n    'reg_lambda' : [1,1.2,1.4],\n    }\n    lgbm_model =  lightgbm.LGBMClassifier(n_jobs=-1)\n    lgbm_random = RandomizedSearchCV(estimator = lgbm_model, param_distributions = random_grid, n_iter = n_iter, cv = 5, verbose=1, random_state=111, n_jobs = -1, scoring = {'NLL':'neg_log_loss', 'Accuracy':'accuracy'}, refit='NLL', return_train_score=False)\n    # Fit the random search model\n    lgbm_random.fit(X, y)\n    return lgbm_random.best_estimator_, lgbm_random.cv_results_\n\ndef random_RF(X,y,n_iter):\n    # Number of trees in random forest\n    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n    # Number of features to consider at every split\n    max_features = ['auto', 'sqrt']\n    # Maximum number of levels in tree\n    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n    max_depth.append(None)\n    # Minimum number of samples required to split a node\n    min_samples_split = [2, 5, 10]\n    # Minimum number of samples required at each leaf node\n    min_samples_leaf = [1, 2, 4]\n    # Method of selecting samples for training each tree\n    bootstrap = [True, False]\n    # Create the random grid\n    random_grid = {'n_estimators': n_estimators,\n                   'max_features': max_features,\n                   'max_depth': max_depth,\n                   'min_samples_split': min_samples_split,\n                   'min_samples_leaf': min_samples_leaf,\n                   'bootstrap': bootstrap}\n\n    rfc = RandomForestClassifier(n_jobs=-1)\n    rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = n_iter, cv = 5, verbose=1, random_state=111, n_jobs = -1, scoring = {'NLL':'neg_log_loss', 'Accuracy':'accuracy'}, refit='NLL')\n\n    # Fit the random search model\n    rf_random.fit(X, y)\n    return rf_random.best_estimator_, rf_random.cv_results_","36f1f927":"n_iter=1\nX,y=train.drop(['Cover_Type'], axis=1), train['Cover_Type']\nm,s = X.mean(0), X.std(0)\ns[s==0]=1\nX = (X-m)\/s\ntrained_model, results=random_lgbm(X,y,n_iter=n_iter)","53587b12":"plt.figure(figsize=(13, 13))\nplt.title(\"RandomSearchCV evaluating using multiple scorers simultaneously\",\n          fontsize=16)\n\nplt.xlabel(\"iters\")\nplt.ylabel(\"Score\")\n\nax = plt.gca()\n#ax.set_xlim(0, 402)\nax.set_ylim(0.2, 1.2)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.arange(0,n_iter)\n\nscoring={'NLL':'neg_log_loss', 'Accuracy':'accuracy'}\nfor scorer, color in zip(sorted(scoring, reverse=True), ['g', 'k']):\n    sample = 'test'\n    if scorer == 'NLL':\n        sample_score_mean = -1 * results['mean_%s_%s' % (sample, scorer)]\n    else:\n        sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n    sample_score_std = results['std_%s_%s' % (sample, scorer)]\n    ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                    sample_score_mean + sample_score_std,\n                    alpha=0.1 if sample == 'test' else 0, color=color)\n    ax.plot(X_axis, sample_score_mean,  color=color,\n            alpha=0.4,\n            label=\"%s (%s)\" % (scorer, sample))\n    \n    if scorer=='NLL':\n        best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n        best_score = -1*results['mean_test_%s' % scorer][best_index]\n    if scorer=='Accuracy':\n        best_score = results['mean_test_%s' % scorer][best_index]\n        \n        \n\n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    if scorer==\"NLL\":\n        ax.annotate(\"%0.2f\" % best_score,\n                   (X_axis[best_index]+0.05, best_score + 0.005))\n    else:\n        ax.annotate(\"%0.2f\" % best_score,\n                    (X_axis[best_index]+0.05, best_score - 0.01))\n\nplt.legend(loc=\"best\")\nplt.grid(False)\nplt.show()","faba5a6c":"trained_model2, results=random_RF(X,y,n_iter=n_iter)","30fd673d":"plt.figure(figsize=(13, 13))\nplt.title(\"RandomSearchCV evaluating using multiple scorers simultaneously\",\n          fontsize=16)\n\nplt.xlabel(\"iters\")\nplt.ylabel(\"Score\")\n\nax = plt.gca()\n#ax.set_xlim(0, 402)\nax.set_ylim(0.2, 1.2)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.arange(0,n_iter)\n\nscoring={'NLL':'neg_log_loss', 'Accuracy':'accuracy'}\nfor scorer, color in zip(sorted(scoring, reverse=True), ['g', 'k']):\n    sample = 'test'\n    if scorer == 'NLL':\n        sample_score_mean = -1 * results['mean_%s_%s' % (sample, scorer)]\n    else:\n        sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n    sample_score_std = results['std_%s_%s' % (sample, scorer)]\n    ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                    sample_score_mean + sample_score_std,\n                    alpha=0.1 if sample == 'test' else 0, color=color)\n    ax.plot(X_axis, sample_score_mean,  color=color,\n            alpha=0.4,\n            label=\"%s (%s)\" % (scorer, sample))\n    \n    if scorer=='NLL':\n        best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n        best_score = -1*results['mean_test_%s' % scorer][best_index]\n    if scorer=='Accuracy':\n        best_score = results['mean_test_%s' % scorer][best_index]\n        \n        \n\n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    if scorer==\"NLL\":\n        ax.annotate(\"%0.2f\" % best_score,\n                   (X_axis[best_index]+0.05, best_score + 0.005))\n    else:\n        ax.annotate(\"%0.2f\" % best_score,\n                    (X_axis[best_index]+0.05, best_score - 0.01))\n\nplt.legend(loc=\"best\")\nplt.grid(False)\nplt.show()","a52e15ef":"test = (test - m)\/s\ntest_pred = trained_model.predict(test)\ntest_pred_rf = trained_model2.predict(test)","8abb663d":"# Save test predictions to file\noutput = pd.DataFrame({'Id': test_ids,\n                       'Cover_Type': test_pred})\noutput.to_csv('lgbm_submission.csv', index=False)\n# Save test predictions to file\noutput = pd.DataFrame({'Id': test_ids,\n                       'Cover_Type': test_pred_rf})\noutput.to_csv('rf_submission.csv', index=False)","63c3f5d7":"# LGBM: random search","5ab30428":"We are going to use LightGBM. It is a state-of-the-art method of tree ensembling together with XGBOOST or Random Forest. XGBOOST is considered to be the best method for classification task (specially if there are categorical features) but some people consider LightGBM better. In this problem we going to use LightGBM because we have a great amount of train data so XGBOOST will be so slow and almost will never end while with LGBM we can train a good model within few minutes.\n\nWe will do hyper-parameter tuning. We create a function called random_lgbm which returns the best estimator and the metrics resulted of the cross-validation. The method used for looking for the best params is random search. We will define a searching space where we expect that the best params will be and randomly we will train so estimators as n_iter. The best thing could be a Grid Search where we try all the parameters of the space but computionally costs so much and with a random search we can find good params with not so much steps.","86376705":"In this notebook, we will train a state-of-the-art LGBM model with hyperparameter tuning (using random search) with no preprocessing of data.","291e95b3":"We read the data and drop the columns of ID since it is an identifier. It will not provide any information about the target variable but noise.","058c251a":"# Submission file","017e8bbd":"# Import Data"}}