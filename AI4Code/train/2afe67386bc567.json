{"cell_type":{"8e09d4f0":"code","a03c509d":"code","d8569a2c":"code","38d69090":"code","a3f72bd8":"code","f8f2e6f5":"code","c1fe00b6":"code","263bbb94":"code","debb5d59":"code","5b727670":"code","722cdc2a":"code","63a1c356":"code","bdffebfe":"code","cb07693b":"code","82dcd067":"code","a1bc9fb5":"code","59808ffe":"code","e9a064ec":"code","a0a63b80":"code","36960ed0":"code","cd134bcf":"code","6800a05a":"code","f4e324fa":"code","2c01e937":"code","327534f8":"code","1752f2f9":"code","80dd85b5":"code","d9b6288d":"code","db697225":"code","09a5dd61":"code","aec6d8d9":"code","e1fffd20":"code","22fd2bf4":"code","efe9e292":"code","8bf88d7c":"code","11aba430":"code","7e0f5174":"code","70e3c48a":"code","961fa81c":"code","78586442":"code","dc696a6d":"code","5f9996f8":"code","9762403d":"code","c7e32864":"code","dc830b2f":"code","dffeebca":"markdown","c640cccf":"markdown"},"source":{"8e09d4f0":"import pandas as pd \nimport numpy as np \nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.linear_model  import Ridge,Lasso,RidgeCV, LassoCV, ElasticNet, ElasticNetCV, LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_boston\nimport statsmodels.api as sm \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","a03c509d":"boston = load_boston()","d8569a2c":"print(boston.DESCR)","38d69090":"boston.data.shape","a3f72bd8":"boston.target.shape","f8f2e6f5":"cols = boston.feature_names","c1fe00b6":"boston_df = pd.DataFrame(boston.data, columns=cols)","263bbb94":"boston_df.head()","debb5d59":"boston_target = pd.DataFrame(boston.target, columns = ['Target'])","5b727670":"boston_target.head()","722cdc2a":"df = pd.concat([boston_df,boston_target], axis=1)","63a1c356":"df.head()","bdffebfe":"df.describe(include='all')","cb07693b":"df.isna().sum()","82dcd067":"# No null values","a1bc9fb5":"# let's see how data is distributed for every column\nplt.figure(figsize=(20,25), facecolor='white')\nplotnumber = 1\n\nfor column in df:\n    if plotnumber<=16 :\n        ax = plt.subplot(4,4,plotnumber)\n        if column!='CHAS':\n            sns.distplot(df[column])\n            plt.xlabel(column,fontsize=20)\n        #plt.ylabel('Salary',fontsize=20)\n    plotnumber+=1\nplt.tight_layout()","59808ffe":"# Dropping columns with skewness\nboston_df = boston_df.drop(columns=['CRIM','ZN','CHAS','B','RAD'])","e9a064ec":"boston_df.head()","a0a63b80":"plt.figure(figsize=(20,30), facecolor='white')\nplotnumber = 1\n\nfor column in boston_df:\n    if plotnumber<=15 :\n        ax = plt.subplot(5,3,plotnumber)\n        plt.scatter(boston_df[column],boston_target)\n        plt.xlabel(column,fontsize=20)\n        plt.ylabel('Target',fontsize=20)\n    plotnumber+=1\nplt.tight_layout()","36960ed0":"# Removoing columns that don't show a linear relationship with target","cd134bcf":"boston_df = boston_df.drop(columns=['INDUS','AGE','DIS','TAX'])","6800a05a":"scaler =StandardScaler()\n\nX_scaled = scaler.fit_transform(boston_df)","f4e324fa":"X_scaled.shape","2c01e937":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvariables = X_scaled\n\n# we create a new data frame which will include all the VIFs\n# note that each variable has its own variance inflation factor as this measure is variable specific (not model specific)\n# we do not include categorical values for mulitcollinearity as they do not provide much information as numerical ones do\nvif = pd.DataFrame()\n\n# here we make use of the variance_inflation_factor, which will basically output the respective VIFs \nvif[\"VIF\"] = [variance_inflation_factor(variables, i) for i in range(variables.shape[1])]\n# Finally, I like to include names so it is easier to explore the result\nvif[\"Features\"] = boston_df.columns","327534f8":"vif","1752f2f9":"# We don't find any VIF greater than 5","80dd85b5":"x_train,x_test,y_train,y_test = train_test_split(X_scaled,boston_target,test_size = 0.25,random_state=355)","d9b6288d":"x_train","db697225":"x_train.shape","09a5dd61":"x_test.shape","aec6d8d9":"y_train","e1fffd20":"regression = LinearRegression()\nregression.fit(x_train,y_train)","22fd2bf4":"regression.score(x_train,y_train)","efe9e292":"# For adjusted R-Squared let's create a function\ndef adj_r2(x,y):\n    r2 = regression.score(x,y)\n    n = x.shape[0]\n    p = x.shape[1]\n    adjusted_r2 = 1-(1-r2)*(n-1)\/(n-p-1)\n    return adjusted_r2","8bf88d7c":"adj_r2(x_train,y_train)","11aba430":"# We have a r2 value of 67% and adjusted r2 value of 66%","7e0f5174":"regression.score(x_test,y_test)","70e3c48a":"adj_r2(x_test,y_test)","961fa81c":"# For test data we have a r2 value of 69% and adjusted r2 value of 68%. So the model is not overfitting.","78586442":"# Lasso Regularization\n# LassoCV will return best alpha and coefficients after performing 10 cross validations\nlasscv = LassoCV(alphas = None,cv =10, max_iter = 100000, normalize = True)\nlasscv.fit(x_train, y_train)","dc696a6d":"# best alpha parameter\nalpha = lasscv.alpha_\nalpha","5f9996f8":"#now that we have best parameter, let's use Lasso regression and see how well our data has fitted before\n\nlasso_reg = Lasso(alpha)\nlasso_reg.fit(x_train, y_train)","9762403d":"lasso_reg.score(x_test, y_test)","c7e32864":"import pickle","dc830b2f":"# saving the model to the local file system\nfilename = 'finalized_model.pickle'\npickle.dump(regression, open(filename, 'wb'))","dffeebca":"Check for multicollinearity","c640cccf":"our r2_score for test data (69.08%) comes same as before using regularization. So, it is fair to say our OLS model did not overfit the data."}}