{"cell_type":{"35426d20":"code","d404276a":"code","c817cc68":"code","5f5fa1bb":"code","91be0c27":"code","8ca016bd":"code","ba827e56":"code","8e020f81":"code","8741cc9e":"code","c128d5d5":"code","633201d6":"code","65751ec3":"code","a849650c":"code","914808a7":"code","11b36554":"code","2fe382fc":"code","01f8072a":"code","61834a14":"code","9ae043b7":"code","57c8be8b":"code","9114300f":"code","2c31a5e0":"code","ec121b4a":"code","f2a28669":"code","60f5a1f7":"code","e93d109d":"code","7e199c75":"code","0edd9365":"code","1e0dea65":"code","6f4aeec9":"code","af2781e1":"code","d5f6febb":"code","82f15eca":"code","77112859":"code","e1e21395":"code","8a88f0fb":"markdown","bc282f78":"markdown","c5fa589e":"markdown","f47f33d8":"markdown","dd7e56f7":"markdown","3e61f857":"markdown","def375fc":"markdown","e31568d1":"markdown","e7fea179":"markdown","dd533851":"markdown","172344de":"markdown","7e3993ff":"markdown","f966edd3":"markdown","7eeae467":"markdown","438332fa":"markdown","76ab7c65":"markdown","6c31e55e":"markdown","88ef6dc6":"markdown","1ec4a0ab":"markdown","7c39ba1d":"markdown","b23f5d0c":"markdown","a3615677":"markdown","986c17a9":"markdown","191f921a":"markdown","046ccc30":"markdown","b958344a":"markdown"},"source":{"35426d20":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport copy\nimport statsmodels\n\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go","d404276a":"# Paths\npath_train = '\/kaggle\/input\/tabular-playground-series-jul-2021\/train.csv'\npath_test = '\/kaggle\/input\/tabular-playground-series-jul-2021\/test.csv'\npath_sub = '\/kaggle\/input\/tabular-playground-series-jul-2021\/sample_submission.csv'\n\n# Seed for reproducibility\nSEED = 100","c817cc68":"# Reading the original training and testing sets\ntrain = pd.read_csv(path_train)\ntest = pd.read_csv(path_test)\n\n# Saving the indexes to separate variables for future reference\ntrain_index = pd.to_datetime(train['date_time'])\ntest_index = pd.to_datetime(test['date_time'])\n\n# First data check\nprint(f'Train shape: {train.shape}')\ntrain.head()","5f5fa1bb":"print(f'Test shape: {test.shape}')\ntest.head()","91be0c27":"# Let's start with a brief description of the data\ntrain.describe()","8ca016bd":"train['date_time'] = pd.to_datetime(train['date_time'])\nprint(f'Training set goes from {train[\"date_time\"].min()} to {train[\"date_time\"].max()} - for a total of {train[\"date_time\"].max() - train[\"date_time\"].min()}.\\n')\n\ntest['date_time'] = pd.to_datetime(test['date_time'])\nprint(f'Test set goes from {test[\"date_time\"].min()} to {test[\"date_time\"].max()} - for a total of {test[\"date_time\"].max() - test[\"date_time\"].min()}.')","ba827e56":"fig = go.Figure()\n\nfig.add_trace(\n      go.Scatter(x=train.date_time, \n                 y=train.target_carbon_monoxide, \n                 mode = 'lines', \n                 line = {'color':'darkgoldenrod', 'width' : 1},\n                 name=\"Carbon Monoxide\")\n)\n\nfig.update_layout(\n    title=\"Carbon Monoxide over time\"\n)\nfig.show()","8e020f81":"fig = go.Figure()\n\nfig.add_trace(\n      go.Scatter(x=train.date_time, \n                 y=train.target_benzene, \n                 mode = 'lines', \n                 line = {'color':'darkgoldenrod', 'width' : 1},\n                 name=\"Benzene\")\n)\n\nfig.update_layout(\n    title=\"Benzene over time\"\n)\nfig.show()","8741cc9e":"fig = go.Figure()\n\nfig.add_trace(\n      go.Scatter(x=train.date_time, \n                 y=train.target_nitrogen_oxides, \n                 mode = 'lines', \n                 line = {'color':'darkgoldenrod', 'width' : 1},\n                 name=\"Nitrogen_oxides\")\n)\n\nfig.update_layout(\n    title=\"Nitrogen Oxides over time\"\n)\nfig.show()","c128d5d5":"from statsmodels.graphics.tsaplots import plot_acf\n\n# Autocorrelation plots to understand significance between lags\nplot_acf(train.target_carbon_monoxide, title=\"ACF - Carbon Monoxide\", lags=96)\nplt.show()\n\nplot_acf(train.target_benzene, title=\"ACF - Benzene\", lags=96)\nplt.show()\n\nplot_acf(train.target_nitrogen_oxides, title=\"ACF - Nitrogen Oxides\", lags=96)\nplt.show()","633201d6":"# PACF plots \nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n# Autocorrelation plots to understand significance between lags\nplot_pacf(train.target_carbon_monoxide, title=\"PACF - Carbon Monoxide\", lags=48)\nplt.show()\n\nplot_pacf(train.target_benzene, title=\"PACF - Benzene\", lags=48)\nplt.show()\n\nplot_pacf(train.target_nitrogen_oxides, title=\"PACF - Nitrogen Oxides\", lags=48)\nplt.show()","65751ec3":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# class to add engineered features, will be used as part of a pipeline for rapid testing\nclass FeatureCreator(BaseEstimator, TransformerMixin):\n    def __init__(self, add_day_of_week=True, add_is_weekend=True,add_time_of_day=True, add_is_daylight=False, add_is_dark=True):\n        self.add_day_of_week = add_day_of_week\n        self.add_is_weekend = add_is_weekend\n        self.add_time_of_day = add_time_of_day\n        self.add_is_daylight = add_is_daylight\n        self.add_is_dark = add_is_dark \n        \n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        \n        X['date_time'] = pd.to_datetime(X['date_time'])\n        X['day_of_week'] = X['date_time'].dt.dayofweek\n        X['time_of_day'] = X['date_time'].dt.hour\n\n        if self.add_is_weekend:\n            X.loc[(X['day_of_week'] == 5)|(X['day_of_week'] == 6), 'is_weekend'] = 1\n        if self.add_is_daylight:\n            X.loc[(X['time_of_day']>6) & (X['time_of_day']<19), 'is_daylight'] = 1\n        if self.add_is_dark:\n            X.loc[(X['time_of_day']>18) | (X['time_of_day']<7), 'is_dark'] = 1\n        \n        X.fillna(0, inplace=True)\n        \n        if not self.add_day_of_week:\n            X.drop(columns='day_of_week', inplace=True)\n        if not self.add_time_of_day:\n            X.drop(columns='time_of_day', inplace=True)\n            \n        X.set_index('date_time', inplace=True)\n                    \n        return X","a849650c":"%pip install pmdarima","914808a7":"# Modeling helper functions\nfrom pmdarima.arima import auto_arima\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n\n# Helper functions\ndef plot_real_vs_preds(y_test_:pd.DataFrame, preds:np.ndarray, target):\n    df_preds = pd.DataFrame(preds, index=y_test_.index)\n    plt.figure(figsize=(17,5))\n    plt.plot(y_test_.index, y_test_, 'b-', label='Test')\n    plt.plot(y_test_.index, df_preds.values, 'g--', label='Predictions')\n    plt.legend()\n    plt.grid()\n    plt.title(f'Real data vs. predictions - {target}')\n    plt.show()\n    \ndef calc_rmsle(real, preds, print_result=False):\n    # try:\n    rmsle = np.sqrt(mean_squared_log_error(real, preds))\n    if print_result:\n        print(f'RMSLE: {rmsle}')\n    else:\n        return rmsle\n          \n    #except ValueError:\n    #    print(f'Negative values in predictions, error cannot be calculated.')\n    \ndef time_cross_validation(splits, train:pd.DataFrame, target:pd.DataFrame, model_):\n    \n    time_kfold = TimeSeriesSplit(n_splits=splits)\n\n    # Loop over the splits\n    for idx_fold, (train_ix, val_ix )in enumerate(time_kfold.split(train)):\n\n        # Folds, training and validation sets\n        X_train, X_val = train.iloc[train_ix], train.iloc[val_ix]\n        y_train, y_val = target.iloc[train_ix], target.iloc[val_ix]\n\n        model_ = model_.fit(y_train, X_train)\n        preds = model_.predict(n_periods=val_ix.shape[0], exogenous=X_val)\n        \n        try:\n            rmsle = calc_rmsle(y_val, preds)\n            print(f'Fold {idx_fold} -> RMSLE: {rmsle}')\n            \n        except ValueError:\n            print(f'Fold {idx_fold} -> Negative values in predictions, error cannot be calculated.')\n            \ndef fit_predict_auto_arima(y_train, X_train, X_test):\n\n    model = auto_arima(y=y_train, X=X_train, stepwise=True)\n    # model = SARIMAX(y_train, X_train, (3,1,2))\n    print(model.summary())\n    return model, model.predict(n_periods=X_test.shape[0], exogenous=X_test)\n\ndef fit_predict_sarima_model(X_test, model):\n\n    # model = auto_arima(y=y_train, X=X_train, stepwise=True)\n    fitted_model = model.fit()\n    print(type(fitted_model))\n    print(fitted_model.summary())\n    return fitted_model, fitted_model.predict(end=X_test.shape[0] - 1, exogenous=X_test)","11b36554":"feat_creator = FeatureCreator()\n# train.reset_index(inplace=True)\n# train = feat_creator.transform(train)","2fe382fc":"# First training, let's get a model using the auto_arima function to start testing\ndf_train = train.drop(columns=['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'])\ndf_train.set_index('date_time', inplace=True)\ndf_target_cox = train[['target_carbon_monoxide']]\ndf_target_cox.set_index(train_index, inplace=True)","01f8072a":"# Split for validation, no cross validation yet as we're getting a starter model\nX_train, X_test, y_train_cox, y_test_cox = train_test_split(df_train, df_target_cox, test_size=0.2,\n                                                            random_state=SEED, shuffle=False)","61834a14":"%%time\n# the auto_arima function finds a good fit to the data using several statistical tests, it's a good way to start probing models\n# best_model_cox, preds_cox = fit_predict_auto_arima(y_train_cox, X_train, X_test)","9ae043b7":"best_model_cox = SARIMAX(y_train_cox, X_train, (3,1,2), freq='H')\nfitted_model_cox, preds_cox = fit_predict_sarima_model(X_test, best_model_cox)","57c8be8b":"plot_real_vs_preds(y_test_cox, preds_cox, 'Carbon Monoxide')","9114300f":"print('Carbon Monoxide')\nrmsle_cox = calc_rmsle(y_test_cox, preds_cox, print_result=True)","2c31a5e0":"%%time\nfrom pmdarima.arima import auto_arima\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_log_error\n\nsplits = 3\nprint('Carbon Monoxide\\n')\n# time_cross_validation(splits, df_train, df_target_cox, best_model_cox)","ec121b4a":"df_target_ben = train[['target_benzene']]\ndf_target_ben.set_index(train_index, inplace=True)\n\n# Split for validation, no cross validation yet as we're getting a starter model\nX_train, X_test, y_train_ben, y_test_ben = train_test_split(df_train, df_target_ben, test_size=0.2,\n                                                            random_state=SEED, shuffle=False)","f2a28669":"%%time\n# the auto_arima function finds a good fit to the data using several statistical tests, it's a good way to start probing models\n# best_model_ben, preds_ben = fit_predict_auto_arima(y_train_ben, X_train, X_test)","60f5a1f7":"best_model_ben = SARIMAX(y_train_ben, X_train, (2,1,2))\nfitted_model_ben, preds_ben = fit_predict_sarima_model(X_test, best_model_ben)","e93d109d":"plot_real_vs_preds(y_test_ben, preds_ben, 'Benzene')","7e199c75":"print('Benzene')\n# rmsle_ben = calc_rmsle(y_test_ben, preds_ben, print_result=True)","0edd9365":"%%time\nsplits = 3\nprint('Benzene\\n')\n# time_cross_validation(splits, df_train, df_target_ben, best_model_ben)","1e0dea65":"df_target_nox = train[['target_nitrogen_oxides']]\ndf_target_nox.set_index(train_index, inplace=True)\n\n# Split for validation, no cross validation yet as we're getting a starter model\nX_train, X_test, y_train_nox, y_test_nox = train_test_split(df_train, df_target_nox, test_size=0.2,\n                                                            random_state=SEED, shuffle=False)","6f4aeec9":"%%time\n# the auto_arima function finds a good fit to the data using several statistical tests, it's a good way to start probing models\n# best_model_nox, preds_nox = fit_predict_auto_arima(y_train_nox, X_train, X_test)","af2781e1":"best_model_nox = SARIMAX(y_train_nox, X_train, (1,1,1))\nfitted_model_nox, preds_nox = fit_predict_sarima_model(X_test, best_model_nox)","d5f6febb":"plot_real_vs_preds(y_test_nox, preds_nox, 'Nitrogen Oxides')","82f15eca":"print('Nitrogen Oxides')\n# rmsle_nox = calc_rmsle(y_test_nox, preds_nox, print_result=True)","77112859":"%%time\nsplits = 3\nprint('Nitrogen Oxides\\n')\n# time_cross_validation(splits, df_train, df_target_nox, best_model_nox)","e1e21395":"test.set_index('date_time', inplace=True)\npreds_cox = fitted_model_cox.predict(end=test.shape[0]-1, exogenous=test)\npreds_ben = fitted_model_ben.predict(end=test.shape[0]-1, exogenous=test)\npreds_nox = fitted_model_nox.predict(end=test.shape[0]-1, exogenous=test)\n\nsub = pd.DataFrame(np.c_[preds_cox, preds_ben, preds_nox], index=test_index).reset_index()\nsub.columns = ['date_time', 'target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']\nsub.to_csv('submission.csv', index=False)","8a88f0fb":"**Partial Autocorrelation plots (PACF)**\n\nPACF plots are a great complement to ACF plots, they're one for the other! \n\nPACF plots explain the *partial correlations* between the lags. A partial correlation tells us the amount of correlation between two variables that is NOT explained by their mutual correlations with other variables. In this case, the other variables are the lower-order lags. So the partial correlation tells us the amount of correlation between our current observation and a time lag, that is NOT explained by lower-order time-lags. \n\nTake a look at the ACF and the PACF for Carbon Monoxide. In the ACF pretty much every higher order correlation was significant up until the 30th lag. The PACF tells us a different part of the story, it looks like from lag 4-~18, the higher order lags are explained by the lower order ones. So in reality lags 1-3 are the most important here, autocorrelations for the higher order lags (4-18) were propagated by the lower order lags (1-3). \n\nIt also looks like we have a similar story for Benzene and Nitrogen Oxides).\n\nThese highly positive low-order correlations tell us how a good model could look like, but I'll go into further detail later.\n\nNote: take into account the ACF and PACF should be used after differencing the series, in this case we're assuming the data is already stationary. If it had a clear trend or seasonality, you should differentiate before doing any autocorrelation analysis.","bc282f78":"**Autocorrelation Plots (ACF)**\n\nIn simple words, an **autocorrelation** in a time-series is the relationship between a data point in some time and another data point in the past. For example, if we have daily data, and we know our present observation is correlated with yesterday's observation, then we know that yesterday can be used to explained today! \n\nSo we're trying to understand the relationship between one observation and observations in the past as a function of the lag between them.\n\nIn this case, our observations are hourly, so one time lag would be a difference of one hour.\n\nLet's look at the plots above:\n\n**Carbon Monoxide**\n\nAs we can see, the y axis marks the correlation value (0 to 1) and the x-axis marks the time lags. We can understand from this graph that the past few lags are heavily correlated with our current observation. So the past few hours are correlated with the current hour. Also, every ~24 lags we get a high correlation with the current observation, which means emissions are cyclical, the time of day does have an influence. \n\nHowever, look how the peaks get smaller every 24 hours. That means that every day that goes by, the current observations are less and less correlated with the past observations. \n\nFinally, take a look at the small shaded area above and below the x-axis, this tells us which of the lags have a *statistically significant correlation* with our current observation. The lags that are outside the range are statistically significant (according to our confidence interval). In this case all lags before ~30 hours seem significant, however we will try to verify this in the next step.\n\n**Benzene and Nitrogen Oxides**\n\nBenzene has a very similar behavior to Carbon Monoxide, while Nitrogen Oxides shows higher correlations overall, with the same trend we observed in the first two plots. It seems a lot of the time-lags are significant, this could, however, be a TRAP! Let's introduce PACF plots.\n\n","c5fa589e":"**Nitrogen Oxides** \n\nNitrogen Oxide readings have a big change after september 2010, the range becomes a lot wider, with max values hitting 1400, where before the max values were around ~600. This could cause us some problems because we have a split of different behaviors, so we don't really know what's \"normal\". Could be a case of seasonality but we can't really confirm it without a few more years of data.","f47f33d8":"#### Section 3.1: Carbon Monoxide Model","dd7e56f7":"### Section 4: Model tuning and Feature Engineering\n\n--- Work in progress ---","3e61f857":"#### Section 3.3: Nitrogen Oxides Model","def375fc":"Take a look at the model summary. SARIMAX is the model architecture, the S means it's a Seasonal ARIMA model, but in this case we didn't add any seasonal terms, so it's the architecture we saw before. The X means we added exogenous features to the model (temperature, sensors, humidity).\n\nNow, we saw a cut-off in the autocorrelation significance at about 4 lags, so it makes sense that the best model found has 3 orders of autocorrelation.  \n\nLet's plot the predictions against the real values to get an estimate of performance.","e31568d1":"**ARIMA architecture**\n\nARIMA stands for AutoRegressive Integrated Moving Average, it has three components: autocorrelation (AR), moving average(MA) and differentiation. I'll brief each of the components:\n\nAutocorrelation (AR) = basically modeling a variable against itself, using past observations to model new observations.\nMoving Average (MA) = instead of using past observations of the variable, moving averages use past forecast errors.\nDifferencing = it's the degree of differencing applied to a time-series to make it stationary. A stationary time-series is one in which any of its fragments has equal properties. For example: time-series with trends or seasonality are not stationary. Before using ARIMA, you must ensure your time-series is stationary, this can sometimes be solved by differentiating the series.\n\nARIMA also has a notation we should be aware about: ARIMA (p,d,q). The three letters represent the orders of the autocorrelation, differencing and moving average parts respectively. For example, an ARIMA(1,0,0) model is strictly an autocorrelation model.\n\nThe orders must be integers. In simple words, the order represents the degree to which we apply each component, for example: if we had ARIMA (5,0,0), we would be adding 5 terms to the autocorrelation equation, meaning we would be considering 5 time lags.\nIn the case of differencing, the order determines the rounds of differencing applied.\n\nRemember the ACF-PACF plots? We saw the 3-5 lags had high correlation with our current observation. In general,a positive correlation in lag-1 in the PACF tells us we could benefit from adding terms to the AR part of the model, so ARIMA (n,0,0).\n\nOn the other hand, negative correlations in lag-1 means we could benefit from adding MA terms to the model. \n\nThis rules only tell us the baseline, we would benefit from experimenting by adding or subtracting terms.\n","e7fea179":"### Section 5: Submission","dd533851":"### Section 3: Modeling\n\n","172344de":"### References and future reading:\n\nChapter 8 on ARIMA Models of Rob Hyndman's and George Athanasopoulos' excellent book on forecasting called Forecasting (Principles and practice): https:\/\/otexts.com\/fpp2\/arima.html\n\nA gentle introduction to autocorrelation and partial autocorrelation by Jason Brownlee: https:\/\/machinelearningmastery.com\/gentle-introduction-autocorrelation-partial-autocorrelation\/\n\n","7e3993ff":"#### Time-Series Cross Validation\nNow let's do some Cross Validation to get an idea of how our model would perform.\n\nBecause we're using a time-series approach to forecasting, we can't use traditional Cross Validation methods, because they usually rely on shuffling the training set to get the different folds. In this case, we need to maintain the order of the observations, because we can't forecast the past with the future! \n\nHere's an image that explains the CV approach I will use, it's called Time Series Cross Validation:\n\n![image.png](attachment:7bff32f6-f5bf-4f21-b502-0c8dad4e2bd8.png)\n\nSource: https:\/\/medium.com\/@soumyachess1496\/cross-validation-in-time-series-566ae4981ce4 \n\nThat way, we always ensure we're training on past data and evaluating on future data, which makes sense for autocorrelation models and forecasting in general.","f966edd3":"Finally, the submission for the competition. These models aren't competitive as is, but we could apply some feature engineering to boost performance, also maybe some model tuning couldn't hurt. Those are the next steps!","7eeae467":"As you can probably see from the plot, there are some big differences when the real emissions are really low. This could be random noise, which would be good for the model, but it's possible that's not the case.\n\nLet's go ahead and calculate the RMSLE.","438332fa":"**Takeaways:**\n* No missing values (woo!).\n* The features are on very different scale. Sensors have similar range but deg_C and humidities don't.\n* As we're going to use some traditional forecasting models, we will do some plots to understand the data better and tune our models accordingly.","76ab7c65":"# Learning from the past \n## ARIMA models with exogenous variables\n\nIn this notebook, I will try to forecast the three targets using ARIMA models. ARIMA is a traditional time-series forecasting model that learns by looking for **autocorrelations** in the data, meaning it looks at past data to predict the future. In this case, we would look at previous values of Carbon Monoxide, Benzene and Nitrous Oxides to try to predict their future values. \nARIMA models can sometimes be improved by adding exogenous variables to the mix, so features that are external to the original target time-series. In this case, the exogenous variables are the sensors, the temperature, the humidity and other features we manage to engineer.\n\n**Note:** some of the sections are works in progress, I'll be adding to them in the next few days! Be sure to give any feedback, I'd appreciate it!","6c31e55e":"### Section 1: Loading required packages and reading the data","88ef6dc6":"### Section 2: Exploratory Data Analysis (EDA) \nWe start with a high-level feature exploration, then we dig deeper into the targets. ","1ec4a0ab":"The auto_arima function finds a good fit to the data using several statistical tests, it's a good way to start probing models.\n\nReference: https:\/\/alkaline-ml.com\/pmdarima\/modules\/generated\/pmdarima.arima.auto_arima.html\n","7c39ba1d":"**Benzene**\n\nThere are some clear cases of readings marking 0, April 9, June 19, August 25, December 14 (hover over the data points to find these dates). These are the examples of what could be bad data.","b23f5d0c":"Data ranges from ~March 2010 to ~January 2011.\n\n**Carbon Monoxide**\n* In general, looks stationary, but we could apply a certain degree of differentiation.\n* Some people have found what looks like bad data (outliers, bad reads) that could impact model performance.\n* We will do a first analysis with all the data.","a3615677":"### Feature engineering \n\n* This a class that can be used to add new features.\n* New features are: \n* day_of_week:  (monday, tuesday, etc.)\n* is_weekend: boolean value, True if day_of_week is saturday or sunday\n* time_of_day: hour\n* is_daylight: boolean, True if time_of_day between 6 and 19\n* is_dark: boolean, True if time_of_day between 18 and 7","986c17a9":"Interestingly enough, some predictions are negative for the first folds, which means we can't calculate the RMSLE (the competition metric).","191f921a":"THE RMSLE is not that good compared to other models, however, we haven't applied any feature engineering, and we haven't tuned the model.\n\nOther models for reference:\nCatBoost with feature engineering by Tsai Yi Lin: https:\/\/www.kaggle.com\/andy6804tw\/catboost-13feature-cross-validation\n\nXGBoost: https:\/\/www.kaggle.com\/mehrankazeminia\/1-tps-jul-21-xgboost-leaveonegroupout","046ccc30":"Looks like we have less than a year of data. Let's plot some graphs.\n\nThanks to @melanie7744 for the plots: https:\/\/www.kaggle.com\/melanie7744\/tps7-interactive-eda-with-plotly","b958344a":"#### Section 3.2: Benzene Model"}}