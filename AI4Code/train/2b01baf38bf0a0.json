{"cell_type":{"cb6daa77":"code","94a55bb5":"code","0f2a76c8":"code","76a25bcd":"code","c19197e1":"code","b7f25f5b":"code","9222e357":"code","01305e33":"code","ee0da45d":"code","9122c557":"code","3f3f3823":"code","3b9c1a4c":"code","3a8f0287":"code","dc762305":"code","d10ebb35":"code","c9372764":"code","6190d663":"code","f59644ad":"code","6d1c547e":"code","ecf5b657":"code","47a796d9":"code","43b68206":"markdown","0d5ab784":"markdown","fcc3d44c":"markdown","08dc1af6":"markdown","d845423c":"markdown","23544b86":"markdown","254df1fd":"markdown","af6df894":"markdown","cd2e53ad":"markdown","217f1c95":"markdown"},"source":{"cb6daa77":"# Import Libraries\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport pickle","94a55bb5":"# Load Dataset\ntext=pd.read_csv(\"..\/input\/mbti_1.csv\" ,index_col='type')\nprint(text.shape)\nprint(text[0:5])\nprint(text.iloc[2])","0f2a76c8":"from sklearn.preprocessing import LabelBinarizer\n\n# One hot encode labels\nlabels=text.index.tolist()\nencoder=LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\nlabels=encoder.fit_transform(labels)\nlabels=np.array(labels)\nprint(labels[50:55])","76a25bcd":"mbti_dict={0:'ENFJ',1:'ENFP',2:'ENTJ',3:'ENTP',4:'ESFJ',5:'ESFP',6:'ESTJ',7:'ESTP',8:'INFJ',9:'INFP',10:'INTJ',11:'INTP',12:'ISFJ',13:'ISFP',14:'ISFP',15:'ISTP'}","c19197e1":"import re\n\n# Function to clean data ... will be useful later\ndef post_cleaner(post):\n    \"\"\"cleans individual posts`.\n    Args:\n        post-string\n    Returns:\n         cleaned up post`.\n    \"\"\"\n    # Covert all uppercase characters to lower case\n    post = post.lower() \n    \n    # Remove |||\n    post=post.replace('|||',\"\") \n\n    # Remove URLs, links etc\n    post = re.sub(r'''(?i)\\b((?:https?:\/\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))''', '', post, flags=re.MULTILINE) \n    # This would have removed most of the links but probably not all \n\n    # Remove puntuations \n    puncs1=['@','#','$','%','^','&','*','(',')','-','_','+','=','{','}','[',']','|','\\\\','\"',\"'\",';',':','<','>','\/']\n    for punc in puncs1:\n        post=post.replace(punc,'') \n\n    puncs2=[',','.','?','!','\\n']\n    for punc in puncs2:\n        post=post.replace(punc,' ') \n    # Remove extra white spaces\n    post=re.sub( '\\s+', ' ', post ).strip()\n    return post","b7f25f5b":"# Clean up posts\n# Covert pandas dataframe object to list. I prefer using lists for prepocessing. \nposts=text.posts.tolist()\nposts=[post_cleaner(post) for post in posts]","9222e357":"# Count total words\nfrom collections import Counter\n\nword_count=Counter()\nfor post in posts:\n    word_count.update(post.split(\" \"))","01305e33":"# Size of the vocabulary available to the RNN\nvocab_len=len(word_count)\nprint(vocab_len)\n\nprint(len(posts[0]))","ee0da45d":"# Create a look up table \nvocab = sorted(word_count, key=word_count.get, reverse=True)\n# Create your dictionary that maps vocab words to integers here\nvocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n\nposts_ints=[]\nfor post in posts:\n    posts_ints.append([vocab_to_int[word] for word in post.split()])\n\nprint(posts_ints[0])\nprint(len(posts_ints[0]))","9122c557":"posts_lens = Counter([len(x) for x in posts])\nprint(\"Zero-length reviews: {}\".format(posts_lens[0]))\nprint(\"Maximum review length: {}\".format(max(posts_lens)))\nprint(\"Minimum review length: {}\".format(min(posts_lens)))\n\nseq_len = 500\nfeatures=np.zeros((len(posts_ints),seq_len),dtype=int)\nfor i, row in enumerate(posts_ints):\n    features[i, -len(row):] = np.array(row)[:seq_len]\nprint(features[:10])","3f3f3823":"# Split data into training, test and validation\n\nsplit_frac = 0.8\n\nnum_ele=int(split_frac*len(features))\nrem_ele=len(features)-num_ele\ntrain_x, val_x = features[:num_ele],features[num_ele:int(rem_ele\/2)+num_ele]\ntrain_y, val_y = labels[:num_ele],labels[num_ele:int(rem_ele\/2)+num_ele]\n\ntest_x =features[num_ele+int(rem_ele\/2):]\ntest_y = labels[num_ele+int(rem_ele\/2):]\n\nprint(\"\\t\\t\\tFeature Shapes:\")\nprint(\"Train set: \\t\\t{}\".format(train_x.shape), \n      \"\\nValidation set: \\t{}\".format(val_x.shape),\n      \"\\nTest set: \\t\\t{}\".format(test_x.shape))","3b9c1a4c":"lstm_size = 256\nlstm_layers = 1\nbatch_size = 256\nlearning_rate = 0.01\nembed_dim=250","3a8f0287":"n_words = len(vocab_to_int) + 1 # Adding 1 because we use 0's for padding, dictionary started at 1\n\n# Create the graph object\ngraph = tf.Graph()\n# Add nodes to the graph\nwith graph.as_default():\n    input_data = tf.placeholder(tf.int32, [None, None], name='inputs')\n    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n    keep_prob = tf.placeholder(tf.float32, name='keep_prob')","dc762305":"# Embedding\nwith graph.as_default():\n    embedding= tf.Variable(tf.random_uniform(shape=(n_words,embed_dim),minval=-1,maxval=1))\n    embed=tf.nn.embedding_lookup(embedding,input_data)\n    print(embed.shape)","d10ebb35":"# LSTM cell\nwith graph.as_default():\n    # basic LSTM cell\n    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    \n    # Add dropout to the cell\n    drop = tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n    \n    # Stack up multiple LSTM layers, for deep learning\n    cell = tf.contrib.rnn.MultiRNNCell([drop]* lstm_layers)\n    \n    # Getting an initial state of all zeros\n    initial_state = cell.zero_state(batch_size, tf.float32)","c9372764":"with graph.as_default():\n    outputs,final_state=tf.nn.dynamic_rnn(cell,embed,dtype=tf.float32 )","6190d663":"with graph.as_default():\n    \n    pre = tf.layers.dense(outputs[:,-1], 16, activation=tf.nn.relu)\n    predictions=tf.layers.dense(pre, 16, activation=tf.nn.softmax)\n    \n    cost = tf.losses.mean_squared_error(labels_, predictions)\n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)","f59644ad":"with graph.as_default():\n    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))","6d1c547e":"def get_batches(x, y, batch_size=100):    \n    n_batches = len(x)\/\/batch_size\n    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n    for ii in range(0, len(x), batch_size):\n        yield x[ii:ii+batch_size], y[ii:ii+batch_size]","ecf5b657":"epochs = 3\n\nwith graph.as_default():\n    saver = tf.train.Saver()\n\nwith tf.Session(graph=graph) as sess:\n    sess.run(tf.global_variables_initializer())\n    iteration = 1\n    for e in range(epochs):\n        state = sess.run(initial_state)\n        \n        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n            feed = {input_data: x,\n                    labels_: y,\n                    keep_prob: 1.0,\n                    initial_state: state}\n            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n            \n            if iteration%5==0:\n                print(\"Epoch: {}\/{}\".format(e, epochs),\n                      \"Iteration: {}\".format(iteration),\n                      \"Train loss: {:.3f}\".format(loss))\n\n            if iteration%25==0:\n                val_acc = []\n                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n                for x, y in get_batches(val_x, val_y, batch_size):\n                    feed = {input_data: x,\n                            labels_: y,\n                            keep_prob: 1,\n                            initial_state: val_state}\n                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n                    val_acc.append(batch_acc)\n                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n            iteration +=1\n    saver.save(sess, \"checkpoints\/mbti.ckpt\")","47a796d9":"test_acc = []\nwith tf.Session(graph=graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n        feed = {input_data: x,\n                labels_: y,\n                keep_prob: 1,\n                initial_state: test_state}\n        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n        test_acc.append(batch_acc)\n    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))","43b68206":"## Testing","0d5ab784":"# MBTI (Myers-Briggs Type Indicator) RNN","fcc3d44c":"## The RNN","08dc1af6":"### Convert words to integers","d845423c":"## Preprocessing labels\nThe neural letwork cannot understand string labels, so we one-hot-encode them using sklearn.preprocessing.LabelBinarizer. I'm displaying the first few labels to see if everything's okay.","23544b86":"### Preparing tranining, test and validation datasets","254df1fd":"## Load dataset\n\nThe dataset is a 'csv' file, so we'll use pandas to load it. We shall print the shape and the first few entries of the dataset to understand what we're working with. Accordingly, we need to choose what strategy to use to clean the data.","af6df894":"## Training","cd2e53ad":" ### Make posts uniform\nWe can see that the lengths of the posts aren't uniform, so we'll limit number of words in each post to 1000.For posts with less than 1000 words, we'll pad with zeros.","217f1c95":"### Preprocessing posts\n\nWe can see that the posts are very noisy, so they need to be cleaned. For this I'm doing the following:\n\n1. Converting all letters to lowercase.\n2. Remove '|||'\n3. Removing punctuation.\n4. Removing URLs, links etc..\n5. Convert words to integers\n\nWe'll leave unicode emojis alone."}}