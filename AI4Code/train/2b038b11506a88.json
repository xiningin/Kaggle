{"cell_type":{"b9373837":"code","8c09f8e7":"code","e253d386":"code","662d2da2":"code","32688317":"code","6658b025":"code","431dd738":"code","157211e4":"code","2e04c658":"code","2e2df7a2":"code","1539c906":"code","586eea1c":"code","0a2ddb52":"code","ab80dafb":"code","ab17743d":"code","8b68f3b7":"code","566ac907":"code","0ed7cf15":"code","fb8bb48f":"code","3006286a":"code","624aaef9":"markdown","f1b74400":"markdown","b3d4580e":"markdown","22658269":"markdown","cdc4f4c2":"markdown","47120696":"markdown","642358c9":"markdown","5d4d62e4":"markdown","f030e496":"markdown"},"source":{"b9373837":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c09f8e7":"df = pd.read_csv(\"\/kaggle\/input\/beginners-classification-dataset\/classification.csv\")","e253d386":"df.head() # only three variables. \ndf.info() ","662d2da2":"df.shape # 297 rows, 3 columns (small size)","32688317":"df.isnull().sum() # no missing","6658b025":"df[df.duplicated()] # no duplicated values. ","431dd738":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)","157211e4":"px.histogram(df, x=\"age\", color=\"success\", marginal = \"box\")","2e04c658":"px.histogram(df, x=\"interest\", color=\"success\", marginal='box')","2e2df7a2":"from sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\ndf = shuffle(df)\nX = df.drop('success', axis=1).values\ny = df['success'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)","1539c906":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, auc, roc_curve, accuracy_score\n\n# make get_score function\ndef get_scores(y, y_pred):\n    data={'Accuracy': np.round(accuracy_score(y, y_pred),2),\n    'Precision':np.round(precision_score(y, y_pred),2),\n    'Recall':np.round(recall_score(y, y_pred),2),\n    'F1':np.round(f1_score(y, y_pred),2),\n    'ROC AUC':np.round(roc_auc_score(y, y_pred),2)}\n    scores_df = pd.Series(data).to_frame(' ').transpose()\n    return scores_df","586eea1c":"model1 = LogisticRegression()\npred = cross_val_predict(model1, X_train, y_train, cv=5)\nget_scores(y_train, pred)","0a2ddb52":"import xgboost as xgb\nfrom xgboost import plot_importance\nfrom xgboost import XGBClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import GridSearchCV\n","ab80dafb":"xgb_model = xgb.XGBClassifier()\n\nparam = {'nthread':[4], \n         'objective':['binary:logistic'],\n         'learning_rate': [0.03, 0.05, 0.07], \n         'max_depth': [6, 8, 10],\n         'min_child_weight': [7, 9, 11],\n         'subsample': [0.5, 0.7, 0.9],\n         'colsample_bytree': [0.7, 0.9, 1.0],\n         'n_estimators': [100],\n         'seed': [122]}\nclf = GridSearchCV(xgb_model, param_grid = param, scoring = 'accuracy', refit = True, verbose = 0)\n","ab17743d":"xgbfit = clf.fit(X_train, y_train)","8b68f3b7":"clf.best_params_","566ac907":"pred2 = clf.predict(X_train)","0ed7cf15":"get_scores(y_train, pred2) # great result. ","fb8bb48f":"# logistic regression\nmodel1 = LogisticRegression().fit(X_train, y_train)\npred = model1.predict(X_test)\nget_scores(y_test, pred)","3006286a":"# XGBoost\npred2 = clf.predict(X_test)\nget_scores(y_test, pred2)","624aaef9":"Let's test the result using test dataset. ","f1b74400":"I would practice XGBoost with new data. \nData source: https:\/\/www.kaggle.com\/sveneschlbeck\/beginners-classification-dataset","b3d4580e":"### 2. XGBoost","22658269":"When I test the result, the recall rate of tuned XGBoost was much worser than basic logistic regression. \nOther indices were slightly better with tuned XGBoost. ","cdc4f4c2":"Interest seems to be more influential factor! ","47120696":"### 0. Split data into train & test. \n- considering the small datasize, I divided data with the ratio of 8:2 (8 for train, 2 for test). ","642358c9":"### 1. Logistic Regression\nLet's use logistic regression as the baseline. ","5d4d62e4":"Interestingly, as one is younger, success rate seems to be lower. ","f030e496":"For logistic regression, accuracy = 0.88, precision=0.92, recall=0.86, F1=0.89, roc-auc=0.88. "}}