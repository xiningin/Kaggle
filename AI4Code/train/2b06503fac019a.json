{"cell_type":{"920737e9":"code","6d160945":"code","901243e6":"code","11025a50":"code","f4beebfc":"code","d49fed54":"code","6facf5ae":"code","ce3b762a":"code","6faa7883":"code","1acaeff2":"code","5672124a":"code","447d9105":"code","3c870777":"code","010df5bb":"code","f2a95433":"code","e7aadbb1":"code","06af3eab":"code","3bffecf3":"code","29d8f67c":"code","e5bae8a9":"code","833bdffe":"code","eef16afc":"code","811c9462":"code","a002e80e":"code","23f400f2":"markdown","ac987f5b":"markdown","d80bd0e5":"markdown","989a1e73":"markdown","afff425e":"markdown","3a710f1a":"markdown","a35fafa2":"markdown","eedf49ac":"markdown","2dd7353d":"markdown","1b23d308":"markdown","62778343":"markdown","019f3036":"markdown","3a0bd75c":"markdown","27ce513d":"markdown","9bd199ed":"markdown"},"source":{"920737e9":"import numpy as np\nimport pandas as pd\n\n# load the data\ntrain_set = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_set = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","6d160945":"train_set.info()","901243e6":"# split the training data into features and targets\ntrain_text = train_set['text']\ntrain_target = train_set['target']","11025a50":"train_text.head()","f4beebfc":"train_text.tail()","d49fed54":"def standardize_text(x):\n    \"\"\"\n    x: a Pandas Series\n    \"\"\"\n    \n    x = x.str.replace(r\"http\\S+\", \"\")\n    x = x.str.replace(r\"http\", \"\")\n    x = x.str.replace(r\"@\\S+\", \"\")\n    x = x.str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n\\ ]\", \"\")\n    x = x.str.replace(r\"@\", \"at\")\n    x = x.str.lower()\n    return x\n\ntrain_text = standardize_text(train_text)\ntrain_text.head()","6facf5ae":"train_target.value_counts()","ce3b762a":"from nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r\"\\w+\")\n\ndef tokenize_text(x):\n    \"\"\"\n    x: a Pandas Series\n    \"\"\"\n    \n    return x.apply(tokenizer.tokenize)\n\ntrain_tokens = tokenize_text(train_text)\ntrain_tokens.head()","6faa7883":"from sklearn.feature_extraction.text import CountVectorizer\n\n# convert to bag of words\ncount_vectorizer = CountVectorizer()\nx_train_full = count_vectorizer.fit_transform(train_text.to_list())\ny_train_full = np.array(train_target)","1acaeff2":"from sklearn.decomposition import TruncatedSVD\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport matplotlib.patches as mpatches\n\ndef plot_lsa(x, y):\n    \"\"\"\n    Latent Sentiment Analysis\n    \"\"\"\n    svd_decomposer = TruncatedSVD(2)\n    x_svd = svd_decomposer.fit_transform(x)\n    x_svd.shape\n\n    plt.style.use('ggplot')\n    plt.figure(figsize=(10, 10))\n    colors = ['blue','red']\n    plt.scatter(x_svd[:,0], x_svd[:,1], s=8, alpha=.8, c=y, cmap=matplotlib.colors.ListedColormap(colors))\n    red_patch = mpatches.Patch(color='blue', label='Irrelevant')\n    green_patch = mpatches.Patch(color='red', label='Disaster')\n    plt.legend(handles=[red_patch, green_patch], prop={'size': 30})\n    \nplot_lsa(x_train_full, y_train_full)\nplt.show()","5672124a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# split the data into training and validation set\nx_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full, test_size=0.2, stratify=y_train_full,\n                                                      random_state=42)\n\n# fit a logistic regression model on the training set\nlogreg_clf = LogisticRegression()\nlogreg_clf.fit(x_train, y_train)\n\n# make predictions on the validation set (for later)\ny_pred_valid = logreg_clf.predict(x_valid)\n\n# evaluate the model's accuracy on the validation set\nprint(f\"Logistic regression on bag-of-words model accuracy: {logreg_clf.score(x_valid, y_valid):.4f}\")","447d9105":"from sklearn.model_selection import cross_val_score\n\ncv_scores = cross_val_score(logreg_clf, x_train_full, y_train_full, cv=5, scoring='accuracy')\nprint(f\"Logistic regression on bag-of-words model accuracy -> mean: {cv_scores.mean():.4f}, std. dev: {cv_scores.std():.4f}\")","3c870777":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nimport seaborn as sn\n\ndef plot_confusion_matrix(y_true, y_pred):\n    plt.figure(figsize=(6, 5))\n    cm = confusion_matrix(y_true, y_pred)\n    ax = sn.heatmap(cm, annot=True, fmt=\"d\", xticklabels=['Irrelevant', 'Disaster'], yticklabels=['Irrelevant', 'Disaster'])\n    ax.set(xlabel=\"Predicted label\", ylabel=\"True label\")\n    \n    s =  f\"Precision: {precision_score(y_true, y_pred):.4f}\\n\"\n    s += f\"Recall: {recall_score(y_true, y_pred):.4f}\\n\"\n    s += f\"F1-Score: {f1_score(y_true, y_pred):.4f}\"\n    ax.text(0, 2.6, s, fontdict={\"fontsize\": 12})\n    plt.plot()\n    \nplot_confusion_matrix(y_valid, y_pred_valid)","010df5bb":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer()\nx_train_full = tfidf_vectorizer.fit_transform(train_text.to_list())\ny_train_full = np.array(train_target)\n\n# LSA plot\nplot_lsa(x_train_full, y_train_full)\nplt.show()","f2a95433":"# split the data into training and validation set\nx_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full, test_size=0.2, stratify=y_train_full,\n                                                      random_state=42)\n\n# fit a logistic regression model on the training set\nlogreg_clf = LogisticRegression()\nlogreg_clf.fit(x_train, y_train)\n\n# make predictions on the validation set (for later)\ny_pred_valid = logreg_clf.predict(x_valid)\n\n# evaluate the model's accuracy on the validation set\nprint(f\"Logistic regression on TF-IDF model accuracy: {logreg_clf.score(x_valid, y_valid):.4f}\")","e7aadbb1":"cv_scores = cross_val_score(logreg_clf, x_train_full, y_train_full, cv=5, scoring='accuracy')\nprint(f\"Logistic regression on TF-IDF model accuracy -> mean: {cv_scores.mean():.4f}, std. dev: {cv_scores.std():.4f}\")","06af3eab":"plot_confusion_matrix(y_valid, y_pred_valid)","3bffecf3":"from gensim.models import Word2Vec\n\n# create and train the word2vec model\nw2v = Word2Vec(train_tokens, size=150, min_count=1)\nw2v.train(train_tokens, total_examples=len(train_tokens), epochs=30)","29d8f67c":"print(w2v)\nprint(w2v.wv['deeds'].shape)","e5bae8a9":"def avg_word2vec(tokens):\n    \"\"\"\n    tokens should be the list of string tokens in the sentence\n    \"\"\"\n    \n    return w2v.wv[tokens].mean(axis=0)\n\nprint(avg_word2vec(train_tokens[0]).shape)","833bdffe":"x_train_full = np.array(train_tokens.apply(avg_word2vec).tolist())\ny_train_full = train_target\n\n# LSA plot\nplot_lsa(x_train_full, y_train_full)\nplt.show()","eef16afc":"# split the data into training and validation set\nx_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full, test_size=0.2, stratify=y_train_full,\n                                                      random_state=42)\n\n# fit a logistic regression model on the training set\nlogreg_clf = LogisticRegression(max_iter=1000)\nlogreg_clf.fit(x_train, y_train)\n\n# make predictions on the validation set (for later)\ny_pred_valid = logreg_clf.predict(x_valid)\n\n# evaluate the model's accuracy on the validation set\nprint(f\"Logistic regression on Word2Vec model accuracy: {logreg_clf.score(x_valid, y_valid):.4f}\")","811c9462":"cv_scores = cross_val_score(logreg_clf, x_train_full, y_train_full, cv=5, scoring='accuracy')\nprint(f\"Logistic regression on Word2Vec model accuracy -> mean: {cv_scores.mean():.4f}, std. dev: {cv_scores.std():.4f}\")","a002e80e":"plot_confusion_matrix(y_valid, y_pred_valid)","23f400f2":"Just to be sure, I will also try K-fold cross validation.","ac987f5b":"## TF-IDF Model","d80bd0e5":"For some reason there seems to be a significant difference between using K-fold cross validation and using a single validation set.","989a1e73":"## Bag of Words Model","afff425e":"Precision increased for this model while recall decreased. For this task recall is more important because we need to reduce false negatives as much as possible. For this we can use ROC and precision-vs-recall plots to choose a better threshold but I will leave that after I find better models.","3a710f1a":"## Word Embeddings (Word2Vec) Model","a35fafa2":"I will stick to using the text only as a predictor for this notebook.","eedf49ac":"The word2vec model seems to separate the 2 labels even better.","2dd7353d":"The TF-IDF model seems to separate the 2 labels much better.","1b23d308":"## Cleaning and Preparing the Data","62778343":"In the future I might consider lemmatization and converting misspelled or alternatively spelled words to a single representation.","019f3036":"The TF-IDF-based logistic regression model has a higher accuracy.","3a0bd75c":"This representation doesn't look very cleany separated.","27ce513d":"The word2vec model seems the best so far in terms of accuracy.","9bd199ed":"The precision and recall are both worse for the word2vec model though."}}