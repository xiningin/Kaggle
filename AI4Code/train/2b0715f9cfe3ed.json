{"cell_type":{"a03ff58c":"code","bc4fa35e":"code","1f958a96":"code","faf495fe":"code","c5ee44ee":"code","5b0293cd":"code","78f8f0d2":"code","2858f6de":"code","bd0673ac":"code","8355d169":"code","3b0da16a":"code","7d6148d6":"code","5b2b5bf7":"code","10338efa":"code","102dc162":"code","e0baf8c5":"code","628ac739":"code","1ed62254":"code","9a480427":"code","9a4df2a4":"code","7061c292":"code","b3126f96":"code","7a620c71":"code","fd71d5c2":"code","17a3538e":"code","fa78c1a4":"code","e6a7d61f":"code","49dfee7f":"code","84f80e37":"code","4aaf8cfb":"code","5a6c2093":"code","243f126e":"code","d17eebc5":"code","706950bb":"code","90efb09b":"code","26a501ad":"code","52a0be60":"code","79f9f238":"code","482119f8":"code","5a60123f":"code","45c38218":"code","80c09548":"code","0bf7b03d":"code","eee0abf3":"code","ff67a6d4":"code","5d7e0396":"code","230c8aba":"code","14fbc554":"code","bd34cd46":"code","4159413c":"code","3ff26449":"markdown","94e5c17b":"markdown","4291c782":"markdown","6c02a979":"markdown","fbf1f683":"markdown","4332b287":"markdown","2e388681":"markdown","6a30de4e":"markdown","3a994c8e":"markdown","ec663666":"markdown","74c58048":"markdown","5bbb2fac":"markdown","9c385867":"markdown","b41fe7ab":"markdown","f847fd9c":"markdown","80a98e6f":"markdown","2844e357":"markdown","be389460":"markdown","00fce1dc":"markdown","f9438030":"markdown","62dfc433":"markdown","69e6f9e2":"markdown","4f9b5c45":"markdown","84379fa5":"markdown","1a6a33f6":"markdown","c58805b7":"markdown","51a5aa21":"markdown","54400d38":"markdown","e2b2893b":"markdown","2ad66381":"markdown","148246d0":"markdown","2ce90d1f":"markdown","a51888f6":"markdown","fd4f6e71":"markdown","d40890b9":"markdown","28917bdb":"markdown","9a04b9ea":"markdown","ea98466c":"markdown","13654a8c":"markdown","c4f7a5e2":"markdown","200ae92e":"markdown"},"source":{"a03ff58c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport nltk, os, re, string\nfrom nltk.corpus import stopwords\nimport string\nprint(os.listdir('..\/input\/'))","bc4fa35e":"train = pd.read_excel('..\/input\/machinehacknewscategory\/Data_Train.xlsx')\ntrain.head()","1f958a96":"sns.countplot(train.SECTION)","faf495fe":"## A. TOTAL NUMBER OF WORDS USED \ntrain['nb_words'] = train.STORY.apply(lambda x: len(x.split()))\n\n## B. TOTAL NUMBER OF UNIQUE WORDS USED\ntrain['nb_unique_words'] = train.STORY.apply(lambda x: len(set(x.split())))\n\n## C. TOTAL NUMBER OF CHARACTERS USED\ntrain['nb_char'] = train.STORY.apply(lambda x: len(x))","c5ee44ee":"## D. TOTAL NUMBER OF PUNCTUATION USED\ndef punct(text):\n    return(len([w for w in text.split() if w in list(string.punctuation)]))\ntrain['nb_punct'] = train.STORY.apply(lambda x: punct(x))\n\n## E. TOTAL NUMBER OF STOPWORDS USED\nstopword = stopwords.words('english')\ndef stop(text):\n    return(len([w for w in text.split() if w in stopword]))\ntrain['nb_stopwords'] = train.STORY.apply(lambda x: stop(x))\n\n## F. TOTAL NUMBER OF TITLE WORDS USED\ndef title(text):\n    return(len([w for w in text.split() if w.istitle()]))\ntrain['nb_title_case'] = train.STORY.apply(lambda x: title(x))\n\n## G. AVERAGE LENGTH OF WORDS\ndef length(text):\n    return(np.mean([len(w) for w in text.split()]))\ntrain['avg_len_word'] = train.STORY.apply(lambda x: length(x))","5b0293cd":"## H. NUMBER OF MOST FREQUENT TERMS\ntoken = nltk.word_tokenize(''.join(train.STORY))\nfrequent = nltk.FreqDist(token)\nfrequent.most_common(15)","78f8f0d2":"## REMOVING PUNCTUATION AND STOPWORDS FROM MOST FREQUENT WORDS\nfor sym in string.punctuation:\n    del frequent[sym]\nfor word in stopword:\n    del frequent[word]\nfrequent.most_common(15)","2858f6de":"%%time\n## I. NUMBER OF WORDS CONTAIN OUT OF MOST COMMON 100 WORDS \nfreq_words = list(dict(frequent.most_common()[:100]).keys())\ndef freq(text):\n    return(len([w for w in text.split() if w in freq_words]))\ntrain['nb_freq_words'] = train.STORY.apply(lambda x: freq(x))","bd0673ac":"%%time\n## J. AVERAGE OF FREQ TERMS WITH TOTAL WORDS USED\nfreq_words = list(dict(frequent.most_common()[:100]).keys())\ndef freq(text):\n    return(len([w for w in text.split() if w in freq_words])\/len(text.split()))\ntrain['avg_freq_word']= train.STORY.apply(lambda x: freq(x))","8355d169":"train_label = train.SECTION\ntrain_backup = train\ntrain = train.drop(columns=['SECTION','STORY'])\ntrain.head(1)","3b0da16a":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, accuracy_score, log_loss\nimport xgboost as xgb\nimport lightgbm as lgb","7d6148d6":"## Helper function to train and plot LGB and XGB models\nparam_xgb = {}\nparam_xgb['objective'] = 'multi:softprob'\nparam_xgb['num_class'] = 4\nparam_xgb['learning_rate'] = 0.1\nparam_xgb['seed'] = 666\nparam_xgb['eval_metric'] = 'mlogloss'\n\nparam_lgb = {}\nparam_lgb['objective'] = 'multiclass'\nparam_lgb['num_classes'] = 4\nparam_lgb['learning_rate'] = 0.1\nparam_lgb['seed'] = 666\nparam_lgb['metric'] = 'multi_logloss'\n\ndef lgb_xgb_helper(train, train_label ,name):\n    cv = []\n    pred_based_on_cv = pd.DataFrame(data = np.zeros(shape = (train.shape[0], 4)))\n    kfold = KFold(n_splits=5, shuffle= True, random_state=2019)\n    for t_index, v_index in kfold.split(train_label.ravel()):\n        xtrain, ytrain = train.loc[t_index,:], train_label[t_index]\n        xtest, ytest = train.loc[v_index,:], train_label[v_index]\n        if (name == 'xgb'):\n            trainset = xgb.DMatrix(xtrain, label=ytrain)\n            testset = xgb.DMatrix(xtest, label=ytest)\n            model = xgb.train(list(param_xgb.items()), trainset, evals=[(trainset,'train'), (testset,'test')], \n                             num_boost_round = 5000, early_stopping_rounds = 200, verbose_eval= 200)\n            pred_based_on_cv.loc[v_index,:] = model.predict(testset, ntree_limit = model.best_ntree_limit)\n        else :\n            trainset = lgb.Dataset(xtrain, label=ytrain)\n            testset = lgb.Dataset(xtest, label=ytest, reference=trainset)\n            model = lgb.train(param_lgb, trainset, valid_sets= testset ,\n                             num_boost_round= 5000, early_stopping_rounds = 200,  verbose_eval= 200)\n            pred_based_on_cv.loc[v_index,:] = model.predict(xtest, best_iteration = model.best_iteration)\n    cv.append(log_loss(ytest, pred_based_on_cv.loc[v_index,:]))\n    return(np.mean(cv), pred_based_on_cv, model)\n\ndef lgb_xgb_plotting(cv, pred, label, model, name=None):\n    fig, ax = plt.subplots(1,2,figsize=(18,5))\n    print(\"CV score : %s\" %cv)\n    sns.heatmap(confusion_matrix(label, np.argmax(pred.values, axis=1)), annot=True, ax= ax[0])\n    ax[0].set_title(\"Accuracy : %s\" % accuracy_score(np.argmax(pred.values, axis=1), train_label))\n    name.plot_importance(model, ax= ax[1])\n    plt.title(\"Feature Importance\")\n    return(accuracy_score(np.argmax(pred.values, axis=1), train_label), cv)","5b2b5bf7":"# Let's check data distribution\nplt.subplots(3,3, figsize = (18,10))\ni = 1\nfor col in train.columns :\n    plt.subplot(3,3,i)\n    sns.distplot(train[col])\n    i = i+1","10338efa":"std_scaler = StandardScaler()\ntrain = pd.DataFrame(std_scaler.fit_transform(train), columns = train.columns)","102dc162":"# TRAIN LGB MODEL ON META FEATURES\ncv, pred, model = lgb_xgb_helper(train, train_label, 'lgb')","e0baf8c5":"# PLOTTING LGB MODEL CONFUSION MATRIX AND FEATURE IMPORTANCE\nmeta_acc_lgb, meta_cv_lgb = lgb_xgb_plotting(cv , pred, train_label, model, lgb)","628ac739":"# TRAIN XGB MODEL ON META FEATURES\ncv, pred, model = lgb_xgb_helper(train, train_label, 'xgb')","1ed62254":"# PLOTTING XGB MODEL CONFUSION MATRIX AND FEATURE IMPORTANCE\nmeta_acc_xgb, meta_cv_xgb = lgb_xgb_plotting(cv, pred, train_label, model, xgb)","9a480427":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.naive_bayes import MultinomialNB","9a4df2a4":"def helper(train, train_label ,model):\n    cv = []\n    pred_based_on_cv = pd.DataFrame(data = np.zeros(shape = (train.shape[0], 4)))\n    kfold = KFold(n_splits=5, shuffle= True, random_state= 2019)\n    for t_index, v_index in kfold.split(train_label.ravel()):\n        xtrain, ytrain = train[t_index,:], train_label[t_index]\n        xtest, ytest = train[v_index,:], train_label[v_index]\n        \n        model.fit(xtrain, ytrain)\n        pred_based_on_cv.loc[v_index,:] = model.predict_proba(xtest)\n        cv.append(log_loss(ytest, pred_based_on_cv.loc[v_index,:]))\n    return(np.mean(cv), pred_based_on_cv)\n\ndef plotting_helper(cv, pred, label, name=None):\n    print(\"CV score : %s\" %cv)\n    plt.figure(figsize = (9,5))\n    sns.heatmap(confusion_matrix(label, np.argmax(pred.values, axis=1)), annot=True)\n    plt.title(\"Accuracy : %s\" % accuracy_score(np.argmax(pred.values, axis=1), train_label))\n    return(accuracy_score(np.argmax(pred.values, axis=1), train_label), cv)","7061c292":"# COUNT VECTORIZATION USING WORD AS LOWEST LEVEL\ncount_vec = CountVectorizer(ngram_range=(1,3), stop_words='english')\ncount_vec.fit(train_backup['STORY'].values.tolist())\ntrain_count_vec = count_vec.transform(train_backup['STORY'].values.tolist())\ntrain_count_vec.shape","b3126f96":"cv, pred = helper(train_count_vec, train_label, MultinomialNB())\ncount_acc_mnb, count_cv_mnb = plotting_helper(cv, pred, train_label)","7a620c71":"# REDUCING DIMENSION OF SPARSE MATRIX TO 20 COMPONENTS\nsvd = TruncatedSVD(n_components=20)\nsvd.fit_transform(train_count_vec)\ntrain_count_vec_svd = svd.transform(train_count_vec)\ntrain_count_vec_svd.shape","fd71d5c2":"# # TRAIN XGB MODEL ON TEXT BASED FEATURES\ncv, pred, model = lgb_xgb_helper(pd.DataFrame(train_count_vec_svd), train_label, 'xgb')","17a3538e":"# PLOTTING XGB MODEL CONFUSION MATRIX AND FEATURE IMPORTANCE\ncount_acc_xgb, count_cv_xgb = lgb_xgb_plotting(cv, pred, train_label, model, xgb)","fa78c1a4":"%%time\n# # TRAIN LGB MODEL ON TEXT BASED FEATURES\ncv, pred, model = lgb_xgb_helper(pd.DataFrame(train_count_vec_svd), train_label, 'lgb')","e6a7d61f":"# PLOTTING LGB MODEL CONFUSION MATRIX AND FEATURE IMPORTANCE\ncount_acc_lgb, count_cv_lgb = lgb_xgb_plotting(cv, pred, train_label, model, lgb)","49dfee7f":"count_vec_char = CountVectorizer(ngram_range = (1,5), analyzer='char', stop_words='english')\ncount_vec_char.fit(train_backup.STORY.values.tolist())\ntrain_count_vec_char = count_vec_char.transform(train_backup.STORY.values.tolist())","84f80e37":"cv, pred = helper(train_count_vec_char, train_label, MultinomialNB())\ncount_acc_mnb_char, count_cv_mnb_char = plotting_helper(cv, pred, train_label)","4aaf8cfb":"## COMPARISON OF MODELS TILL NOW\nperformance_accuracy = pd.DataFrame({'Model': ['Meta_LGB', 'Meta_XGB', 'Count_LGB', 'Count_XGB', 'Count_MNB', 'Count_MNB_Char'], \n              'Accuracy': [meta_acc_lgb, meta_acc_xgb, count_acc_lgb, count_acc_xgb, count_acc_mnb, count_acc_mnb_char]})\n\nperformance_logloss = pd.DataFrame({'Model': ['Meta_LGB', 'Meta_XGB', 'Count_LGB', 'Count_XGB', 'Count_MNB','Count_MNB_Char'], \n              'logloss': [meta_cv_lgb, meta_cv_xgb, count_cv_lgb, count_cv_xgb, count_cv_mnb, count_cv_mnb_char]})\nfig, ax = plt.subplots(1,2,figsize=(18,6))\nsns.barplot(x = 'Model', y = 'Accuracy', data = performance_accuracy , ax= ax[0])\nsns.barplot(x = 'Model', y = 'logloss', data = performance_logloss , ax= ax[1])","5a6c2093":"tfidf_vec = TfidfVectorizer(ngram_range = (1,3), stop_words= 'english')\ntfidf_vec.fit(train_backup.STORY.values.tolist())\ntrain_tfidf_vec = tfidf_vec.transform(train_backup['STORY'].values.tolist())","243f126e":"cv, pred = helper(train_tfidf_vec, train_label, MultinomialNB())\ntfidf_acc_mnb, tfidf_cv_mnb = plotting_helper(cv, pred, train_label)","d17eebc5":"train_tfidf_vec_svd = svd.fit_transform(train_tfidf_vec)\n# TRAIN XGB MODEL ON TEXT BASED FEATURES\ncv, pred, model = lgb_xgb_helper(pd.DataFrame(train_tfidf_vec_svd), train_label, 'xgb')","706950bb":"# PLOTTING XGB MODEL CONFUSION MATRIX AND FEATURE IMPORTANCE\ntfidf_acc_xgb, tfidf_cv_xgb = lgb_xgb_plotting(cv, pred, train_label, model, xgb)","90efb09b":"%%time\n# # TRAIN LGB MODEL ON TEXT BASED FEATURES\ncv, pred, model = lgb_xgb_helper(pd.DataFrame(train_tfidf_vec_svd), train_label, 'lgb')","26a501ad":"# PLOTTING LGB MODEL CONFUSION MATRIX AND FEATURE IMPORTANCE\ntfidf_acc_lgb, tfidf_cv_lgb = lgb_xgb_plotting(cv, pred, train_label, model, lgb)","52a0be60":"tfidf_vec_char = TfidfVectorizer(ngram_range = (1,5), stop_words= 'english', analyzer='char')\ntfidf_vec_char.fit(train_backup.STORY.values.tolist())\ntrain_tfidf_vec_char = tfidf_vec_char.transform(train_backup['STORY'].values.tolist())","79f9f238":"cv, pred = helper(train_tfidf_vec_char, train_label, MultinomialNB())\ntfidf_acc_mnb_char, tfidf_cv_mnb_char = plotting_helper(cv, pred, train_label)","482119f8":"performance_accuracy = pd.concat([performance_accuracy, pd.DataFrame({'Model': ['Tfidf_MNB', 'Tfidf_XGB', 'Tfidf_LGB','Tfidf_MNB_Char'], \n                                          'Accuracy': [tfidf_acc_mnb, tfidf_acc_xgb, tfidf_acc_lgb,tfidf_acc_mnb_char]})], axis=0)\n\nperformance_logloss = pd.concat([performance_logloss, pd.DataFrame({'Model': ['Tfidf_MNB', 'Tfidf_XGB', 'Tfidf_LGB','Tfidf_MNB_Char'], \n                                          'logloss': [tfidf_cv_mnb, tfidf_cv_xgb, tfidf_cv_lgb, tfidf_acc_mnb_char]})], axis=0)","5a60123f":"fig,ax= plt.subplots(2,1, figsize = (16,8))\nsns.barplot('Model', 'Accuracy', data= performance_accuracy.sort_values(by = 'Accuracy', ascending=False), ax= ax[1], palette= 'Set3')\nsns.barplot('Model', 'logloss', data= performance_logloss.sort_values(by = 'logloss'), ax= ax[0], palette= 'Set2')\nplt.xticks(rotation= 90)","45c38218":"train = pd.concat([pd.DataFrame(train_backup['STORY']), pd.DataFrame(train_backup['SECTION'])], axis = 1)\ntrain.head()","80c09548":"%%time\n# STEPS TAKEN FROM SRK NOTEBOOK\n# LOWER CASE ALL CHARACTERS \ntrain.STORY = train.STORY.apply(lambda x: x.lower())\n\n## LEMMATIZATION\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet as wn\nlemmatizer = WordNetLemmatizer()\n\ndef lem(text):\n    pos_dict = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}\n    return(' '.join([lemmatizer.lemmatize(w,pos_dict.get(t, wn.NOUN)) for w,t in nltk.pos_tag(text.split())]))\n\ntrain.STORY = train.STORY.apply(lambda x: lem(x))\n\n# REMOVING PUNCTUATION\ndef cleaner(text):\n    return(text.translate(str.maketrans('','', string.punctuation)))\ntrain.STORY = train.STORY.apply(lambda x: cleaner(x))\n\n# REMOVING STOPWORDS\nst_words = stopwords.words()\ndef stopword(text):\n    return(' '.join([w for w in text.split() if w not in st_words ]))\ntrain.STORY = train.STORY.apply(lambda x: stopword(x))","0bf7b03d":"train.head()","eee0abf3":"from keras.layers import Input, LSTM, Bidirectional, SpatialDropout1D, Dropout, Flatten, Dense, Embedding, BatchNormalization\nfrom keras.models import Model\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical","ff67a6d4":"train_label = to_categorical(train.SECTION, 4)\ntrain_label.shape","5d7e0396":"max_words = 40000\nmax_len = 256\n\ntok = Tokenizer(lower=True, char_level=False)\ntok.fit_on_texts(train.STORY)\nsequence = tok.texts_to_sequences(train.STORY)\nsequence = pad_sequences(padding='post', sequences=sequence, maxlen= max_len)\nsequence.shape","230c8aba":"def modeling():\n    inp = Input(shape=(max_len,))\n    x = Embedding(max_words, 300 ,input_length = max_len)(inp)\n    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n    x = BatchNormalization()(x)\n    x = SpatialDropout1D(0.5)(x)\n    x = Flatten()(x)\n    x = Dense(128, activation= 'relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(4, activation='softmax')(x)\n    model = Model(inputs = inp, outputs = x)\n    return(model)","14fbc554":"model = modeling()\nmodel.compile(optimizer = 'adam', metrics = ['accuracy'], loss= 'categorical_crossentropy')\nmodel.summary()","bd34cd46":"lr = ReduceLROnPlateau(monitor='valid_set', factor=0.002, min_lr=0.00001)\nmodel.fit(sequence, train_label, validation_split=0.30, callbacks=[lr], batch_size=64, epochs=10)","4159413c":"performance_WO = pd.DataFrame({'epoch': model.history.epoch, 'loss': model.history.history['loss'], \n              'val_loss': model.history.history['val_loss'], 'acc': model.history.history['accuracy'],\n             'val_acc': model.history.history['val_accuracy']})\n\nfig, ax = plt.subplots(2,1, figsize=(12,6), sharex = True)  \nsns.lineplot(x= 'epoch', y= 'loss', data = performance_WO, ax= ax[0])\nsns.lineplot(x= 'epoch', y= 'val_loss', data = performance_WO, ax = ax[0])\nax[0].legend(['TRAINING LOSS', 'VALIDATION LOSS'])\nax[0].set_ylabel('Loss')\n\n  \nsns.lineplot(x= 'epoch', y= 'acc', data = performance_WO, ax= ax[1])\nsns.lineplot(x= 'epoch', y= 'val_acc', data = performance_WO, ax = ax[1])\nax[1].legend(['TRAINING ACCURACY', 'VALIDATION ACCURACY'])\nax[1].set_ylabel('Accuracy')","3ff26449":"## <a> D. TFIDF VECTORIZATION <\/a>","94e5c17b":"### Also check whether DL approach can beats ML aprroach or NOT","4291c782":"Based on above model we can say nearly around **epcoh 5~6** we are getting an accuracy of around **96 ~ 97%**. Which is still less than the accuracy which we got for **CountVectorization with MultinomialNB()**. <br><br>\nBut if we do better text cleaning then it may perform better than **CountVectorization**.","6c02a979":"# 1. Meta features\nIn meta features we try to generate some basic features which we can use as an input to our Machine learning model. \n<br>This is the simplest and fastest way to make a model without much of the hassle. <br>\n","fbf1f683":"Till now we can see clearly that **Multinomial naive bayes** performed better than **XGB and LGB model**. As it is utilizing all words where **XGB and LGB model** can't be train on such large sparse matrix. <br><br>Therefore reduction of dimension was required. But this also leads to loss of information about out text. <br> <br>\nAs a result **XGB and LGB lagging behind Multinomial naive bayes**\n<br>\n<br>\n\nWith **logloss** ,**LGB and XGB** worked way better than **MultinomialNB** ","4332b287":"With XGB and LGB our accuracy decreased a lot. This is because of dimension reduction as we lost lot of information that we have generated through Count Vectorization part. ","2e388681":"Pretty Bad here :( <br>\nMeta features performance are very bad. Let's try **Kaggle's most popular model: XGB**","6a30de4e":"Most of the columns are **positive skewed**. We can use log transformation but let's ignore this for now. <br>Lets use **StandardScaler** before using this to build our model so that each columns are aligned to Standard deviation as 1 and mean as 0","3a994c8e":"Purpose of this notebook is give a **brief idea on Machine learning and Deep learning implementation on any NLP.** As we all know that there are various different approaches to solve any NLP problem. For eg: Frequency based model like Tfidf and various Prediction based model like Word2Vec. <br>  \n\n**NLP Begineer's** people like me faces **lots and lots of doubt** to understand. As for same problem some people are using ML techniques whereas others are using DL approach using **pretrained vectors**. It gave me tough time for Begineers understand all things. <br>\n\nTherefore i decided to make a kernel on **NLP with all approaches to a single problem** <br>\n**A Big thanks to GrandMaster [SRK](https:\/\/www.kaggle.com\/sudalairajkumar) **<br>\nYour kernel helped a lot to get overall idea \n\n## Give your Upvotes and comment down your suggestion so that we together improve this kernel :)\n","ec663666":"# 2. Text based feature","74c58048":"I have divided this Notebook into two part. <br>\n<br>\nPart 1 : This part will talk about various **ML approaches to NLP **problems.<br> <br> Part 2: This will give a brief idea on **DL approaches to NLP **problems\n<br>\n<br>\nLet's get started ...","5bbb2fac":"Let's check target distribution","9c385867":"Till now with all **ML** approaches we got around **97% accuracy ** + logloss of ~ 0.17**. Now in next version we will see **Deep learning** approaches to **NLP** problems. Let's see how pretrained, trained model perform.","b41fe7ab":"# SIMPLE APPROACH TO ANY NLP PROBLEM","f847fd9c":"We have reduce shape from **(7628, 729975) to (7628, 20)**. Let's see how LGB and XGB perform on this reduced matrix","80a98e6f":"In this we basically use word\/letter Frequencies in order to determine their relavance. Before getting started let's import some libraries and also create some helper function to train and plot model.","2844e357":"![](https:\/\/miro.medium.com\/max\/770\/1*yfr3m-JjwQLucxIsThIJGQ.jpeg)","be389460":"Let's import some libraries for ML model and also create some helper function to train and plot graph as per our requirement. ","00fce1dc":"## Next plan to include in this notebook :\n#### 1. DL approach (with pretrained model like Word2Vec, Glove, Fasttext)","f9438030":"## <a> a. Basic feature extraction<\/a>","62dfc433":"## <a> E. TFIDF + DIMENSION REDUCTION","69e6f9e2":"## <a> A. COUNT VECTORIZATION :- <\/a>","4f9b5c45":"TFIDF didn't performed well. Let's check TFIDF with Boosting models","84379fa5":"# Part 2 : DL Approach","1a6a33f6":"We have 4 classes of qualitative targets. It is little bit **imbalanced** but let's ignore this for now","c58805b7":"# Part 1 : ML approaches","51a5aa21":"## <a> B. COUNT VECTORIZATION + DIMENSION REDUCTION :-<\/a>","54400d38":"Here we can clearly see simple CountVectorization performed way better than any Meta based model. **We got around 97 %** accuracy and Our confusion matrix also looks great.<br>\n<br>\n<br>\nSince CountVectorized will contain a sparse matrix. and if we use that matrix for our LGB, XGB model then it will takes lot's of time so better way is to use Dimension reduction technique and then apply XGB\/LGB","e2b2893b":"## <a> C. COUNT VECTORIZATION USING CHAR NGRAM :-<\/a>","2ad66381":"We got lot's of meta features about our Text field. <br><br>Now we will  this basic features in our ML model and will check **model performance**. First Let's check our dataframe and also label and do preprocessing.","148246d0":"# Part 1 : Machine learning approches \n**1. Meta features**\n<br>\n <a> a. Basic feature extract<\/a>\n\n**2. Text based features**\n<br>\n <a> a. Count Vectorization (word_based)<\/a>\n<br>\n <a> b. Count Vectorization (word_based + Dimension Reduction)<\/a>\n<br>\n <a> c. Count Vectorization (char_based)<\/a>\n<br>\n <a> d. Tfidf Vectorization (word_based)<\/a>\n<br>\n <a> e. Tfidf Vectorization (word_based + Dimension Reduction)<\/a>\n<br>\n <a> f.  Tfidf Vectorization (char_based)<\/a>","2ce90d1f":"Based on **Machine Learning approches** we found **MultinomialNB** accuracy better than other. But only when we use it with **Count Vectrorization** ","a51888f6":"If we look into **logloss** we can clearly see **LGB** logloss smaller than any other **model** <br>\nBut if we use it with **Count Vectorization. ** ","fd4f6e71":"# Part 2 : Deep learning approches\n**1. Without Using Pretrained Vector**\n<br>\n<br>\n**2. With Pretrained Vector**\n<br>\n<a> a. Word2Vec<\/a>\n<br>\n<a> b. Glove<\/a>\n<br>\n<a> c. Fastext<\/a>\n ","d40890b9":"## <a> F. TFIDF CHAR BASED <\/a>","28917bdb":"## <a> Data Cleaning<\/a>","9a04b9ea":"After applying Count Vectorization over all text we got sparse matrix of size **(7628, 729975)**. This is **huge matrix**","ea98466c":"First we have to clean our data as we don't want unwanted words to be part of our vocabulary ","13654a8c":"## <a>1. Without using Pretrained Vectors<\/a>","c4f7a5e2":"# Comment down if you have any query\/suggestion. <br> Don't forget to Upvote it :)","200ae92e":"XGB also perform very bad. Let's move to Text based features and using that we will try to build our Model"}}