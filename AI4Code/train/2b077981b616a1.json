{"cell_type":{"ce13e772":"code","69f12115":"code","9a389bde":"code","537582b6":"code","1937328e":"code","68928e2b":"code","ec45a598":"code","d91dc770":"code","70cac58f":"code","2f39a88f":"code","acbaa5fa":"code","6ee504aa":"code","4b6b706a":"code","7d001c4d":"code","5265e7b7":"code","7354d630":"code","79596f3e":"code","2a0180f9":"code","197d618f":"code","b70e1789":"code","d5b884dd":"code","64d53d61":"code","6972ca6c":"code","30bf70eb":"code","2f3bf801":"code","6ecc4005":"code","2b8b3473":"markdown","54e91101":"markdown","d2fae0bb":"markdown","f86324c7":"markdown","1eb55997":"markdown","40a01b31":"markdown","4083e055":"markdown","33819e1b":"markdown","538a9b0f":"markdown","95c554dc":"markdown","ab954d7d":"markdown","d77cbca1":"markdown","8a8a1482":"markdown","ec585e00":"markdown","d0121001":"markdown","ed783315":"markdown","6e002e27":"markdown","a5f1f035":"markdown","655105da":"markdown","1c06789d":"markdown","971f5129":"markdown","b4f881e8":"markdown","0e9844ee":"markdown","2e2ddb1c":"markdown","0a73f769":"markdown","ad528eab":"markdown","f6bdead2":"markdown","f4c1dc96":"markdown","cffbe0b6":"markdown","294ac282":"markdown","d7bf8101":"markdown"},"source":{"ce13e772":"import random\nimport os\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn.metrics\nimport keras\n\nprint(np.__version__)\nprint(pd.__version__)\nprint(matplotlib.__version__)\nprint(sklearn.__version__)\nprint(sns.__version__)\nprint(keras.__version__)","69f12115":"from IPython.display import YouTubeVideo\nYouTubeVideo(\"4Lo3tcrz8U0\")","9a389bde":"YouTubeVideo(\"OmmEXUnL4_8\")","537582b6":"YouTubeVideo(\"L0Q6cboXyLY\")","1937328e":"technocast_PATH = '\/kaggle\/input\/real-life-industrial-dataset-of-casting-product\/casting_data\/casting_data\/'\n\ntechnocast_train_path = technocast_PATH + 'train\/'\ntechnocast_test_path = technocast_PATH + 'test\/'","68928e2b":"dir1 = technocast_train_path+'\/ok_front\/'\ndir2 = technocast_train_path+'\/def_front\/'\n\nimg1 = plt.imread(dir1+random.choice(os.listdir(dir1)))\nimg2 = plt.imread(dir2+random.choice(os.listdir(dir2)))\n\nfig, ax = plt.subplots(1,2)\n\nax[0].imshow(img1)\nax[0].axis('off')\nax[0].set_title('ok')\n\nax[1].imshow(img2)\nax[1].axis('off');\nax[1].set_title('defective');","ec45a598":"def make_generators():\n\n    datagen = keras.preprocessing.image.ImageDataGenerator(rescale = 1\/255,\n                                                           validation_split = 0.1)\n    \n    train_generator = datagen.flow_from_directory(directory = technocast_train_path, \n                                                  batch_size = 32,\n                                                  target_size = (300, 300),\n                                                  color_mode = \"grayscale\",\n                                                  class_mode = \"binary\",\n                                                  classes = {\"ok_front\": 0, \"def_front\": 1},\n                                                  shuffle = True,\n                                                  #seed = 0,\n                                                  subset = \"training\")\n\n    validation_generator = datagen.flow_from_directory(directory = technocast_train_path,\n                                                       batch_size = 32,\n                                                       target_size = (300, 300),\n                                                       color_mode = \"grayscale\",\n                                                       class_mode = \"binary\",\n                                                       classes = {\"ok_front\": 0, \"def_front\": 1},\n                                                       shuffle = True,\n                                                       #seed = 0,\n                                                       subset = \"validation\")\n    \n    return train_generator, validation_generator","d91dc770":"def visualizeImageBatch(datagen, title):\n    \n    '''\n    Adapted from:\n    https:\/\/www.kaggle.com\/tomythoven\/casting-inspection-with-data-augmentation-cnn\n    '''\n    \n    mapping_class = {0: \"0 (ok)\", 1: \"1 (defective)\"}\n    \n    images, labels = next(iter(datagen))\n    images = images.reshape(32, *(300,300))\n    \n    fig, axes = plt.subplots(4, 8, figsize=(13,6.5))\n\n    for ax, img, label in zip(axes.flat, images, labels):\n        ax.imshow(img, cmap = \"gray\")\n        ax.axis(\"off\")\n        ax.set_title(mapping_class[label], size = 12)\n\n    fig.tight_layout()\n    fig.suptitle(title, size = 16, y = 1.05)","70cac58f":"train_generator, validation_generator = make_generators()\n\nvisualizeImageBatch(train_generator, 'Training manibatch example')","2f39a88f":"test_datagen = keras.preprocessing.image.ImageDataGenerator(rescale = 1\/255)\n\nn_test = sum([len(files) for r, d, files in os.walk(technocast_test_path)])\n\ntest_generator = test_datagen.flow_from_directory(directory = technocast_test_path,\n                                                  batch_size = n_test,\n                                                  target_size = (300, 300),\n                                                  color_mode = \"grayscale\",\n                                                  class_mode = \"binary\",\n                                                  classes = {\"ok_front\": 0, \"def_front\": 1},\n                                                  shuffle = False)","acbaa5fa":"fig, ax = plt.subplots(1,3,figsize=(10,3))\n\nsns.countplot(train_generator.classes,ax=ax[0])\nsns.countplot(validation_generator.classes,ax=ax[1])\nsns.countplot(test_generator.classes,ax=ax[2])\n\nax[0].set_title('Training')\nax[1].set_title('Validation')\nax[2].set_title('Test')\n\nfig.suptitle('Proportion of classes (normal\/defectives)')\nfig.tight_layout(rect=[0, 0.03, 1, 0.92]);","6ee504aa":"def make_cnn():\n\n    m = keras.models.Sequential([\n\n    keras.layers.Conv2D(filters=16, kernel_size=(7,7), strides = 2, activation=\"relu\", \n                        padding=\"same\", input_shape=(300, 300, 1)),\n    keras.layers.MaxPooling2D(pool_size = (2,2), strides = 2),\n        \n    keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n    keras.layers.MaxPooling2D(pool_size = (2,2), strides = 2),\n                \n    keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n    keras.layers.MaxPooling2D(pool_size = (2,2), strides = 2),\n                \n    keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n    keras.layers.MaxPooling2D(pool_size = (2,2), strides = 2),\n        \n    keras.layers.Conv2D(filters=256, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n    keras.layers.MaxPooling2D(pool_size = (2,2), strides = 2),\n                \n    keras.layers.Flatten(),\n    keras.layers.Dense(units=64, activation=\"relu\"),\n\n    keras.layers.Dropout(0.2),\n\n    keras.layers.Dense(units=1, activation=\"sigmoid\")\n\n    ])\n\n    m.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return m","4b6b706a":"make_cnn().summary()","7d001c4d":"early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-6, patience=5)","5265e7b7":"%%time\n\n# number of nets in the ensemble\nn_models = 10\n\n# lists that will store models and result histories\nm = [0]*n_models\nH = [0]*n_models\n\nfor i in range(n_models):\n\n    print(f'Net {i+1}')\n    print('----------------------------------------')\n    \n    # generating different training\/validation sets for each train\n    train_generator, validation_generator = make_generators()\n    \n    # generating the model\n    m[i] = make_cnn()\n    \n    # checkpoint to save the best model for the network\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"technocast_cnn_{i+1}.hdf5\",\n                                                 save_best_only = True,\n                                                 monitor = \"val_loss\")\n    \n    print('Training...')\n    \n    # training the network\n    H[i] = m[i].fit(train_generator, validation_data = validation_generator, \n                    epochs = 30, callbacks=[early_stop,checkpoint], verbose=0)\n    \n    # printing results after the training end\n    epoch_min = pd.DataFrame(H[i].history).idxmin(axis=0)['val_loss']\n    print(f\"Epochs: {len(H[i].history['loss'])}\") \n    print(f\"loss: {H[i].history['loss'][epoch_min]:.4}, accuracy: {H[i].history['accuracy'][epoch_min]:.4}\")\n    print(f\"val_loss: {H[i].history['val_loss'][epoch_min]:.4}, val_accuracy: {H[i].history['val_accuracy'][epoch_min]:.4}\")\n    print('----------------------------------------\\n')","7354d630":"fig, ax = plt.subplots(2,5, figsize=(20,5))\n\nfor i in range(n_models):\n    \n    pd.DataFrame(H[i].history).plot(ax=ax.ravel()[i], legend = True if i==0 else False);\n    plt.xlabel('epoch');","79596f3e":"best_models = [keras.models.load_model(f\"technocast_cnn_{i+1}.hdf5\") for i in range(n_models)]","2a0180f9":"def ensemble_prediction(generator):\n    \n    y_probs = [best_models[i].predict(generator).squeeze() for i in range(len(best_models))]\n    return np.mean(np.array(y_probs), axis=0)","197d618f":"y_probs = ensemble_prediction(test_generator)","b70e1789":"y = test_generator.classes\ny_pred = y_probs>0.5\n\nprint(f'Accuracy:{sklearn.metrics.accuracy_score(y, y_pred):.4}')\nprint('------------------------------')\nprint('Confusion matrix:')\nprint(sklearn.metrics.confusion_matrix(y, y_pred))","d5b884dd":"print(f'Precision: {sklearn.metrics.precision_score(y, y_pred):.4}')\nprint(f'Recall: {sklearn.metrics.recall_score(y, y_pred):.4}')\nprint(f'F1: {sklearn.metrics.f1_score(y, y_pred):.4}')","64d53d61":"prob_x = np.linspace(0.01,0.99)\n\nrecall = [0]*len(prob_x)\nf1 = [0]*len(prob_x)\nprecision = [0]*len(prob_x)\n\nfor i in range(len(prob_x)):\n    y_pred_rp_curve = y_probs>prob_x[i]\n    recall[i] = sklearn.metrics.recall_score(y, y_pred_rp_curve)\n    f1[i] = sklearn.metrics.f1_score(y, y_pred_rp_curve)\n    precision[i] = sklearn.metrics.precision_score(y, y_pred_rp_curve)\n\nplt.plot(prob_x, recall,'.--')\nplt.plot(prob_x, f1,'*-')\nplt.plot(prob_x, precision,'.-')\n\nplt.xlabel('Cut-off probability')\nplt.legend(['recall','$F_1$','precision']);","6972ca6c":"direc = technocast_test_path\nimgs = [plt.imread(direc+file) for file in test_generator.filenames]\n\nfig, ax = plt.subplots(715\/\/13,13, figsize=(18,100))\n\nfor i in range(715):\n    \n    ax.ravel()[i].imshow(imgs[i])\n    ax.ravel()[i].axis('off');\n    \n    color = ('black' if ((test_generator.labels[i]==0 and y_probs[i]<0.5)) or \n                        (test_generator.labels[i]==1 and y_probs[i]>=0.5) \n             else 'red')\n    \n    ax.ravel()[i].set_title(f'{test_generator.labels[i]}\\n {y_probs[i]:.2}', color=color)","30bf70eb":"'''\nIn case the model reaches 100% accuracy, this cell will not run\n'''\n\nif sklearn.metrics.accuracy_score(y, y_pred)<1:\n\n    direc = technocast_test_path\n\n    # selecting images with wrong predictions\n\n    wrong_positions = np.where(y!=y_pred)[0]\n    wrong_files = [test_generator.filenames[i] for i in wrong_positions]\n\n    imgs = [plt.imread(direc+file) for file in wrong_files]\n\n    # probabilities for images with wrong predictions\n    y_probs_wrong = ensemble_prediction((next(test_generator)[0][wrong_positions]))\n\n    fig, ax = plt.subplots(1,len(wrong_positions), figsize = (20,5))\n\n    # transforming ax, y_probs_wrong and wrong_positions objects into containers if they are not\n    # so that we can enter the loop below even when there is only one wrong position\n    ax = np.array([ax]) if not hasattr(type(ax), '__iter__') else ax\n    y_probs_wrong = [y_probs_wrong] if not hasattr(type(y_probs_wrong), '__iter__') else y_probs_wrong\n    wrong_positions = [wrong_positions] if not hasattr(type(wrong_positions), '__iter__')  else wrong_positions\n\n    # plotting!\n\n    for i in range(len(wrong_positions)):\n\n        ax[i].imshow(imgs[i])\n        ax[i].axis('off');\n        ax[i].set_title(f'{test_generator.labels[wrong_positions[i]]}\\n {y_probs_wrong[i]:.2}')\n        ax[i].text(2, 15, str(wrong_positions[i]))\n\n    fig.suptitle('Sample(s) incorrectly classified', fontsize=18)\n    fig.tight_layout(rect=[0, 0.03, 1, 0.9]);","2f3bf801":"'''\nIn case the model reaches 100% accuracy, this cell will not run\n'''\nif sklearn.metrics.accuracy_score(y, y_pred)<1:\n    \n    direc = technocast_test_path\n\n    # selecting images with predictions away from 0 and 1\n\n    mask = np.logical_and(y_probs > 0.2, y_probs < 0.8)\n\n    unusual_prob_positions = np.where(mask)[0]\n    unusual_prob_files = [test_generator.filenames[i] for i in unusual_prob_positions]\n\n    imgs = [plt.imread(direc+file) for file in unusual_prob_files]\n\n    # predictions themselves\n    y_probs_unusual = ensemble_prediction(next(test_generator)[0][unusual_prob_positions])\n\n    # generating the plot window\n\n    n_columns = 5\n    n_lines = max(1,max(1,len(unusual_prob_positions)\/\/n_columns+1))\n\n    fig, ax = plt.subplots(n_lines,n_columns, \n                           figsize = (20,4*n_lines))\n\n    # turning all axes off before entering the loop\n    [ax.ravel()[i].axis('off') for i in range(len(ax.ravel()))]\n\n    # plotting!\n\n    for i in range(len(unusual_prob_positions)):\n\n        ax.ravel()[i].imshow(imgs[i])\n\n        color = ('black' if ((test_generator.labels[unusual_prob_positions[i]]==0 and y_probs_unusual[i]<0.5)) or \n                            (test_generator.labels[unusual_prob_positions[i]]==1 and y_probs_unusual[i]>=0.5) \n                 else 'red')\n\n        ax.ravel()[i].set_title(f'{test_generator.labels[unusual_prob_positions[i]]}\\n {y_probs_unusual[i]:.2}',\n                               color = color)\n\n        ax.ravel()[i].text(2, 15, str(unusual_prob_positions[i]))\n\n    suptitle1 = f'{len(unusual_prob_positions)} samples to be sent for human review ({(len(unusual_prob_positions)\/715*100):.2}% of the total)'\n    suptitle2 = f'of which {np.in1d(unusual_prob_positions, wrong_positions).sum()} classified incorrectly.'\n\n    fig.suptitle(suptitle1+'\\n'+suptitle2,fontsize=18);\n\n    fig.tight_layout(rect=[0, 0.03, 1, 0.9])","6ecc4005":"YouTubeVideo(\"cd-VSQeiPSM\")","2b8b3473":"Samples with incorrect classifications mostly have predictions close to $0.5$. This fact can be used to build a strategy for reviewing the model's results. Predictions whose probabilities are very far from $0$ or $1$ (that is, in which the model is not very confident about the belonging to any of the classes) can be forwarded for human verification.\n\nIn the cell below, we select the samples that result in probabilities that are distant from $0$ or $1$ ('how far' can be an adjustable parameter):","54e91101":"# Convolutional networks\n\n[Convolutional networks](https:\/\/en.wikipedia.org\/wiki\/Convolutional_neural_network) are sparsely connected networks, where each neuron *does not* connect to all neurons in the previous layer. It is the opposite of densely connected networks, such as [MLP](https:\/\/en.wikipedia.org\/wiki\/Multilayer_perceptron).\n\nDensely connected networks can only learn global patterns, that is, patterns that involve all points present in a given sample or observation. Convolutional networks, as we will see, are able to learn *local patterns* and recognize these patterns in *any position* of some other sample. In the case of images, these local patterns may correspond to borders, curves, etc.\n\nThroughout the network structure, as we advance into deeper layers, simple local patterns are aggregated and become more and more complex. It is a learning structure analogous to that observed in the [visual cortex](https:\/\/en.wikipedia.org\/wiki\/Visual_cortex), the brain portion responsible for the processing of visual information. That is why convolutional networks are so used to process images.\n\nMathematically, the network learns these local patterns through the [convolution](https:\/\/en.wikipedia.org\/wiki\/Convolution) operation, explained below.\n\n## Operation of the convolutional network\n\n* To feed an image to the network, it is necessary to represent it as a tensor. For example, the following is a representation of puppy image as a three-dimensional tensor of pixels:\n\n<img src = \"https:\/\/media.springernature.com\/original\/springer-static\/image\/chp%3A10.1007%2F978-3-030-47994-7_16\/MediaObjects\/472738_1_En_16_Fig2_HTML.png\" width = \"600\" height = \"600\" \/>\n\n* In this case, the resulting tensor has a 15x20x3 shape (that is, there are 15 rows, 20 columns and 3 color channels).\n\n* Then the image goes through a *convolutional layer*. In this layer, the convolution operation is applied:\n\n<img src = \"https:\/\/media.springernature.com\/original\/springer-static\/image\/chp%3A10.1007%2F978-3-030-47994-7_16\/MediaObjects\/472738_1_En_16_Fig3_HTML.png\" width = \"500\" height = \"500\" \/>\n\n* In the convolution operation, a *filter* (also called a *kernel*), usually 3x3 (the most common), 5x5 or 7x7, runs through the pixels of an image computing the dot product at each step and storing the results in a matrix called *feature map*.\n\n* The feature map gets its name because it is the structure responsible for recognizing the various patterns (features) related to the image. The nature of the recognized pattern depends on the structure of the filter.\n\n* When the image has several channels, the feature map resulting from the filter is the sum of the feature maps corresponding to each channel.\n\n* The convolutional layer can have several filters, each giving rise to a feature map.\n\n* Each element of a feature map corresponds to a neuron. The neurons in the same feature map share the same parameters (weights).\n\n* In general, the matrix that constitutes each filter of a given layer is a parameter of the network, that is, it is adjusted together with the weights in order to result in the optimal learning. The number of filters in a given layer is a hyperparameter (must be determined a priori).\n\n* Multiple feature maps can be fed to a new convolutional layer, resulting in new maps, as illustrated:\n\n<img src = \"https:\/\/www.mdpi.com\/sensors\/sensors-19-01693\/article_deploy\/html\/images\/sensors-19-01693-g002-550.jpg\" width = \"400\" height = \"400 \"\/>\n\n* Each neuron in an output feature map connects to only a portion of the neurons in the input maps. The portion of the previous layer that connects to a neuron is called its *receptive field*. The following figure illustrates well the concept of a neuron's receptive field:\n\n<img src = \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/8\/85\/Convolution_arithmetic_-_Full_padding_no_strides_transposed.gif\" width = \"200\" height = \"200\" \/>\n\n* The convolution operation makes the output map smaller in shape than the input map. In order to make the shape the same, you can use *padding* strategies. In the following strategy, known as *zero padding* or *same padding*, zeros are added to the edges of the input map so that the output map has the same shape as the input. It is a way to preserve information from the edges along the network.\n\n<img src = \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/8\/80\/Convolution_arithmetic_-_Same_padding_no_strides_transposed.gif\" width = \"200\" height = \"200\" \/>\n\n* If it is desired to generate output maps smaller than the input ones, *strides* can be used. In the following figure, the map is calculated with a stride of 2:\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/c\/c1\/Convolution_arithmetic_-_Dilation.gif\" width=\"200\" height=\"200\"\/>\n\n* Another strategy for generating output maps smaller than those of the input is the *expansion*, proposed by [YU and KOLTUN (2015)](https:\/\/arxiv.org\/abs\/1511.07122) and illustrated in the following figure. The advantage of dilation in relation to the stride strategy is the greater coverage of the receptive field of each neuron.\n\n<img src = \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/c\/c1\/Convolution_arithmetic_-_Dilation.gif\" width = \"200\" height = \"200\" \/>\n\n* In addition to the convolutional layer, it is common to add *pooling* layers to the networks. Like convolutional layers, the neurons in pooling layers also have receptive fields, but there are no weights and the operation performed is not the convolution, but some aggregation operation, such as the average or the choice of the maximum value (the latter being the most used):\n\n<img src=\"https:\/\/www.researchgate.net\/publication\/333593451\/figure\/fig2\/AS:765890261966848@1559613876098\/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max.png\n\" width=\"350\" height=\"350\"\/>\n\n* In general, receptive pooling fields have shape 2x2 and stride 2, as shown above.\n\n* The pooling layers reduce the number of network parameters, attenuating overfitting and computational cost. Furthermore, as information is aggregated from larger windows into smaller windows, the receptive fields of deeper layers, even though small, have information related to the entire original image. This makes it possible the recognition of global and more complex\/abstract patterns in the deep layers.\n\n* The following figure illustrates the typical architecture of a convolutional network:\n\n<img src=\"https:\/\/www.researchgate.net\/publication\/336805909\/figure\/fig1\/AS:817888827023360@1572011300751\/Schematic-diagram-of-a-basic-convolutional-neural-network-CNN-architecture-26.ppm\" width=\"500\" height=\"500\"\/>\n\n* The first part of the network performs *feature extraction*, that is, it identifies the various patterns present in the image. Typically, several pairs of convolution\/pooling layers are used, which defines the depth of the network.\n\n* The second part consists of a dense layer (fully connected, as in the MLP network) and is responsible for carrying out the classification.\n\n* Some excellent pages for interactive views of convolutional networks are [this](https:\/\/www.cs.ryerson.ca\/~aharley\/vis\/conv\/) and [this](https:\/\/poloclub.github.io\/cnn-explainer\/).\n\nNow we are going to implement an ensemble of convolutional networks in Keras.","d2fae0bb":"Now let's create the test image generator, defined a bit differently:","f86324c7":"# Analysis of results","1eb55997":"It is noticed that almost all of the predictions are close to 0 or 1, which indicates great certainty of the ensemble model regarding the classes of most images.\n\nLooking only at the images classified incorrectly:","40a01b31":"Loading the best settings for each model (saved by the `model_checkpoint` callback):","4083e055":"The arguments of the `flow_from_directory` method specify that minibatches will be generated containing 32 grayscale images  (that is, with only 1 color channel) of shape 300x300. The classification problem is binary; the normal (`ok_front`) and defective (`def_front`) classes were assigned labels 0 and 1, respectively. The data will be shuffled and the seed of randomness has been specified as 0.\n\nAt this point, it is convenient to take a look at an example of a minibatch. For this, we will use the function `visualizeImageBatch`:","33819e1b":"Only a small fraction of the samples is in the range considered suspect (in the case above, 0.2 <p<0.8).\n\nThe use of an ensemble of nets increases the overall accuracy (reducing the samples in this range) and also helps to bring the averages of uncertain predictions close to 0.5 (which tends to bring images with incorrect classifications to that range). It is a favorable scenario for the implementation of a human revision strategy, resulting in a detection system that can reach 100% accuracy with low additional cost of [person-hour](https:\/\/en.wikipedia.org\/wiki\/Man-hour).\n\nThe review strategy is particularly important in this process because even low error rates (less than 1%) can result in considerable economic losses, since the cost of a single false positive or negative can be high (as discussed [here](https:\/\/www.kaggle.com\/ravirajsinh45\/real-life-industrial-dataset-of-casting-product\/discussion\/129717)).\n\nIf you run this notebook, you will probably get different results from the ones presented here, mainly regarding the number of incorrect classifications in the test set (which varies from 0 to 4, in general). But, in particular, I verified two consistent points throughout all the executions I made:\n\n* the accuracy is higher than the recall over most of the cutoff probability range;\n* the probabilities of the incorrectly classified samples are close to 0.5 (generally below, due to the recall being lower than precision).\n\nPerhaps another strategy or architecture would lead to more stable results. You can try :)","538a9b0f":"The cut-off probability, therefore, can be a parameter defined according to the company's interest:\n\n* If you want to minimize the loss caused by the waste of good pieces (ie, reduce the risk of false positives), you must select a cut that results in maximum precision.\n* If you want to avoid sending defective parts to the customer as much as possible (that is, to reduce the risk of false negatives), you must select a cut that results in maximum recall.\n* If you want a good compromise between the two scenarios described, you can select a cut that results in a maximum of $F_1$.\n\nIn the case of adjustment of this parameter, it is recommended to make the choice based on the validation data and not on the test data, so that we can use the test data to verify the suitability of the choice.\n\nWe have just understood the importance of the network's output probability for the classification process. In the next cells, this importance will become even clearer through a detailed analysis of the test set.\n\nIn the following figure, we depict all the 715 samples from the test set together with, respectively, the true class and the probability predicted by the model. In the case of mistaken predictions, the headings are in red.","95c554dc":"Let's check the newtork's structure:","ab954d7d":"Each network provides a probability for each image (that is, a number between $0$ and $1$). The closer the probability is to $0$, the more the network believes that the image belongs to class $0$; the closer to $1$, the more it believes the class is $1$.\n\nIn the function above, the probability of the ensemble is calculated as the average of the probabilities provided by each network.\n\nIn the next cell, we apply the function to calculate the probabilities for the test set:","d77cbca1":"As usual, the feature maps sizes decrease and the number of filters increases as we advance in depth. Note that the first receptive field is larger than the others (7x7), which can be convenient to reduce the size of the image without losing a lot of information (and since there are generally only a maximum of 3 channels in the third dimension of the input, the operation is not so expensive).\n\nBelow we define a [callback](https:\/\/keras.io\/api\/callbacks\/) for early stop. *Callbacks* are objects that perform actions during training. In the case below, the `early_stop` callback, defined from the [EarlyStopping](https:\/\/keras.io\/api\/callbacks\/early_stopping\/) class, will monitor the loss of validation at each epoch and, if the decrease in last 5 epochs is no more than $10^{- 6}$, training will end.","8a8a1482":"Plotting the learning curves:","ec585e00":"# Ending\n\nThank you for reading! If you know portuguese, there is a lecture about this application in the following video:","d0121001":"### The cost of error is very high. Let's call humans to *eliminate* it, but as humans are also expensive, let's use AI to make them as few as possible!!!\n\nIn this notebook, we will propose an artificial+human intelligence strategy to achieve 100% accuracy in the classification of normal and defective pieces generated by a casting process. \n\nThe AI system will consist of an ensemble of convolutional neural networks. The use of an ensemble is absolutely necessary for the strategy to work, as we will discuss below.\n\nIn this way, we can achieve 100% classification accuracy using the minimum person-hour possible. This is important because, in this problem, the cost of a false positive or negative is very high. Therefore, pure AI systems, even with more than 99% accuracy, could be unacceptable in a real-world business environment.\n\nThe strategy was thought of as a solution for the issue proposed in [this discussion](https:\/\/www.kaggle.com\/ravirajsinh45\/real-life-industrial-dataset-of-casting-product\/discussion\/129717).\n\nThis text can be used as an introduction to computer vision and convolutional networks in the context of a problem with more practical appeal than the classical MNIST, for example.\n\nIf you find it useful, please upvote :)","ed783315":"The dataset is organized in two directories, according to the structure:\n\n    casting_data\n    \u251c\u2500\u2500\u2500test\n    \u2502   \u251c\u2500\u2500\u2500def_front\n    \u2502   \u2514\u2500\u2500\u2500ok_front\n    \u2514\u2500\u2500\u2500train\n        \u251c\u2500\u2500\u2500def_front\n        \u2514\u2500\u2500\u2500ok_front\n        \nThis structure is convenient because, at the time of reading by `keras`, the classes ` def_front` and `ok_front` are recognized according to the directories names.","6e002e27":"The proportions seem consistent between sets. We could try to correct the prevalence of defective part images over normal ones, but as the difference is not so great, it will not be done.\n\nOk!! We already have the objects that will generate our training, validation and test data. Now let's proceed to the modeling phase.","a5f1f035":"We can train now. In the cell below, the various networks are trained in sequence. Note the presence of another callback, the `checkpoint`, instantiated from the [ModelCheckpoint](https:\/\/keras.io\/api\/callbacks\/model_checkpoint\/) class and whose function is to save the best performing model (measured in relation to the loss of validation). This is useful because the result of last epoch is not necessarily the best.","655105da":"# The process and the data\n\nThe [dataset](https:\/\/www.kaggle.com\/ravirajsinh45\/real-life-industrial-dataset-of-casting-product), provided by [Pilot Technocast](http:\/\/www.pilottechnocast.com\/), consists of images of regular and defective pieces from a casting process. In this process, the molten metallic material is fed into a mold and, during cooling, solidifies into the desired shape, as shown below:","1c06789d":"# Generating data with Keras\n\n\n* Image files must be converted to tensors to be fed into neural networks (we will learn more about tensors below).\n* We will use the class [ImageDataGenerator](https:\/\/keras.io\/api\/preprocessing\/image\/#imagedatagenerator-class) (available in the module [keras.preprocessing.image](https:\/\/keras.io\/api\/preprocessing\/image)) to generate the input tensors from the files availables in the `technocast_train_path` and` technocast_test_path` folders.\n\n* This generation will take place in real time during training: at each iteration, a minibatch of tensors will be provided to the model so that a learning step can be carried out.\n\nIn the function below, we instantiate an object named `datagen` from the` ImageDataGenerator` class, specifying that the pixels will be normalized to the 0-1 range (by dividing by 255) and 10% of the data will be reserved for validation. After that, we use the `flow_from_directory` method to effectively create the objects that will generate the training and validation minibatches.","971f5129":"Inspecting a manibatch:","b4f881e8":"Two other metrics suitable for this problem are [$P$ (precision) and $R$ (recall)](https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall). Denoting the negative class as being from normal samples (without defects) and the positive class as being from samples with defects, we define:\n\n$$\nP = \\frac {VP} {VP + FP}\n$$\n\n$$\nR = \\frac {VP} {VP + FN}\n$$\n\nwhere $ P$ is the number of true positives, $FP$ is the number of false positives and $FN$ is the number of false negatives. In essence:\n\n* Accuracy refers to the proportion of pieces detected as defective that is correct.\n* Revocation refers to the proportion of defective pieces that are correctly detected.\n\nAccuracy is a measure of the accuracy of detections and is all about *quality*; recall is a measure of completeness of detections and has to do with *quantity*. From the point of view of [hypothesis testing](https:\/\/en.wikipedia.org\/wiki\/Statistical_hypothesis_testing), precision is associated with type I errors and the recall is associated with type II errors.\n\nAn also widely used metric is [$F_1$](https:\/\/en.wikipedia.org\/wiki\/F1_score), the harmonic average between precision and recall:\n\n$$F_1 = \\frac{2}{1 \/ P + 1 \/ R}$$\n\nIn the next cell, we calculate the accuracy, recall and $F_1$ of our model:","0e9844ee":"# Generating an ensemble of networks\n\nThe strategy proposed here will be to create multiple networks and combine the various results in a single prediction. In general, creating a model consisting of multiple submodels is a procedure known as [ensembling](https:\/\/en.wikipedia.org\/wiki\/Ensemble_learning).\n\nThe motivation for using ensembling is the same as in the [random forest method](https:\/\/en.wikipedia.org\/wiki\/Random_forest): to reduce the generalization error. Statistics states that the greater the independence between the models, the greater the reduction in error. The differences between the networks in this case arise from the randomness of i) separation between training\/validation and ii) initialization of weights.\n\nThe following function returns a compiled convolutional network:","2e2ddb1c":"Defining a function to return the ensemble's prediction:","0a73f769":"[Defects](https:\/\/en.wikipedia.org\/wiki\/Casting_defect) can be of different natures: gas porosity, shrinkage defects, mould material defects, pouring metal defects, metallurgical defects, etc. The following video, for example, provides tips to avoid the porosity problem:","ad528eab":"There is a trade-off between precision and recall: when we increase one, the other tends to decrease. We can verify this trade-off by plotting the precision and recall curves as a function of the cut:","f6bdead2":"We specified the minibatch size as the total size of the test set, since the test prediction will be made with all the data at once, after training ends.\n\nIt is useful to check the proportions of classes in the different sets:","f4c1dc96":"Our models will need to distinguish these two types of images.","cffbe0b6":"Using `matplotlib` to take a look at two random pieces (normal and defective):","294ac282":"To compute classification metrics, it is necessary to define which range of probabilities is associated with each class. In other words, it is necessary to choose the cut-off value that separates each of the two classes. The most intuitive way to do this is as follows:\n\n* Class $0$: $ p \\leq 0.5 $\n* Class $1$: $ p> 0.5 $.\n\nAlthough $0.5$ is the most common cut, we can vary this value in order to manipulate the proportions between [type I and type II errors](https:\/\/en.wikipedia.org\/wiki\/Type_I_and_type_II_errors), as we will discuss in more detail later.\n\nCalculating the metrics:","d7bf8101":"The pieces in question are [submersible pumps](https:\/\/en.wikipedia.org\/wiki\/Submersible_pump) impellers, whose operation is illustrated in the following video:"}}