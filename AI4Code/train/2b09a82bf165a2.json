{"cell_type":{"906005cd":"code","0be917f2":"code","c243c9ca":"code","3de36a40":"code","b1f2a5f9":"code","541b02b9":"code","773846e9":"code","9c70db5e":"code","ffa799f9":"code","ea540731":"code","6b462ab0":"code","19678373":"code","5e874e28":"code","eab0df26":"code","6a8ffed0":"code","aa56ebce":"code","310c9a14":"code","f4da16f8":"code","fbe6a5cc":"code","07d22b0d":"code","f930cfe4":"code","b31cae49":"code","83c34846":"code","f743dca7":"code","fcaa368e":"code","1de1206c":"code","a25e89b2":"code","d5e0861b":"code","3f830f9d":"code","235739f0":"code","d4b35485":"code","43b7e1f5":"code","1b22fa02":"markdown","a80d046c":"markdown","ea184847":"markdown","2e835d33":"markdown","49041894":"markdown","48b662f3":"markdown","4c58138d":"markdown","d9b06fc7":"markdown","b461f918":"markdown","4be2ff23":"markdown","1f385ccb":"markdown","73d0172d":"markdown","4f4e9724":"markdown","9b9389a5":"markdown","ccbdf0f0":"markdown","f46d2432":"markdown","14373925":"markdown","c7aeccec":"markdown","d08cdd90":"markdown","fe3f08ce":"markdown","691f23ec":"markdown"},"source":{"906005cd":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport cufflinks\ncufflinks.go_offline()\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nsns.set()\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0be917f2":"df=pd.read_csv('\/kaggle\/input\/heart-disease\/heart.csv')","c243c9ca":"df.head(15).style.background_gradient(cmap=\"Wistia\",text_color_threshold=0.01)","3de36a40":"df.info()","b1f2a5f9":"df.describe().drop(['25%','50%','75%'])","541b02b9":"plt.figure(figsize=(15,9))\nsns.heatmap(df.isnull(),cmap='viridis',yticklabels=False,cbar=False);\nplt.title('Heatmap of NaN Variables')\nplt.xlabel('Columns');","773846e9":"plt.figure(figsize=(15,9))\ngp=sns.countplot(data=df,x='target',hue='sex',palette='Set1')\nfor p in gp.patches:\n    value=p.get_height() \n    if value <0:\n        continue\n    x = p.get_x()+.18\n    y = p.get_y() + p.get_height() - 10\n    gp.text((x), (y), int(value), fontsize=12,bbox=dict(facecolor='#ccddee', edgecolor='black', boxstyle='round', linewidth=0.65))\nplt.xlabel('Diagnoses - 1:Yes - 0:No')\nplt.ylabel('Counts')\nplt.legend(title='Sex',loc='upper left' ,labels=['Female', 'Male'])\nplt.title('Diagnoses by Sex',fontsize=15);","9c70db5e":"plt.figure(figsize=(15,9))\nsns.heatmap(df.corr(),cmap='viridis',annot=True,linewidths=.75,linecolor='black');","ffa799f9":"plt.figure(figsize=(15,9))\nsns.lmplot(data=df,x='age',y='chol',hue='sex',col='target',markers='x',palette='Set1',height=6,aspect=1.5);","ea540731":"plt.figure(figsize=(15,9))\ngp=sns.histplot(data=df,x='thalach',hue='target',bins=30,kde=True,stat='count',palette='inferno')\nfor p in gp.patches:\n    value=p.get_height() \n    if value <1:\n        continue\n    x = p.get_x()+1.5\n    y = p.get_y() + p.get_height()+.25\n    gp.text((x), (y), int(value), fontsize=9,bbox=dict(facecolor='#ccddee', edgecolor='black', boxstyle='round', linewidth=0.65))\n# plt.legend(labels=['True', 'False'])\nplt.title('Diagnoses with Max Heart Rate',fontsize=15)\nplt.xlabel('Max Heart rate Achieved');","6b462ab0":"plt.figure(figsize=(15,9))\nsns.kdeplot(data=df,x='trestbps',y='oldpeak',shade=True,cmap='viridis',cbar=True)\nplt.xlabel('Blood Pressure')\nplt.ylabel('ST Depression');","19678373":"gp=sns.jointplot(x='trestbps',y='chol',data=df,height=7,ratio=5,kind='hex',color='#4CB391')\ngp.set_axis_labels('Blood Pressure', 'Cholesterol', fontsize=12);","5e874e28":"from sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB,BernoulliNB\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import classification_report,confusion_matrix","eab0df26":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()","6a8ffed0":"X,y=df.drop(['target'],axis=1),df['target']","aa56ebce":"sc.fit(X)\ndf_sc=pd.DataFrame(sc.transform(X),columns=df.columns[:-1])\ndf_sc['target']=df['target']","310c9a14":"df_sc","f4da16f8":"X=df_sc.drop('target',axis=1)\ny=df_sc['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)","fbe6a5cc":"X_train.shape,y_train.shape,X_test.shape,y_test.shape","07d22b0d":"classifiers=[LogisticRegression(),\n             KNeighborsClassifier(),\n             SVC(kernel='rbf'),\n             SVC(kernel='linear'),\n             GaussianNB(),\n             BernoulliNB(),\n             GaussianProcessClassifier(),\n             DecisionTreeClassifier(),\n             RandomForestClassifier(),\n             AdaBoostClassifier(),\n             QuadraticDiscriminantAnalysis()]","f930cfe4":"\ndef predict(clf_list,score_list):\n    for i in clf_list:\n        i.fit(X_train,y_train)\n        print('                ',i)\n        print('Score =',i.score(X_test,y_test))\n        print(confusion_matrix(y_test,i.predict(X_test)))\n        print(classification_report(y_test,i.predict(X_test)))\n        score_list.append(i.score(X_test,y_test))\n        print('*'*80)\n    return score_list","b31cae49":"default_scores=[]\nds=predict(classifiers,default_scores)\nds","83c34846":"classifiers=[LogisticRegression(),KNeighborsClassifier(),SVC(),SVC(),GaussianProcessClassifier(),DecisionTreeClassifier(),RandomForestClassifier()]\nparams=[{'penalty':('l1', 'l2', 'elasticnet', 'none'),'C':np.append(np.arange(0.01,1,0.03),[2,3,4,5]),'solver' :('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'),'multi_class' : ('auto', 'ovr', 'multinomial')},\n       {'n_neighbors':range(1,51),'weights' : ('uniform', 'distance'),'algorithm' : ('auto', 'ball_tree', 'kd_tree', 'brute'),'metric':('minkowski','manhattan','chebyshev','euclidean')},\n       {'kernel':('rbf','rbf'),'C':np.append(np.arange(0.01,1,0.03),[2,3,4,5]),'gamma' : ('scale', 'auto')},\n       {'kernel':('linear','linear'),'C':np.append(np.arange(0.01,1,0.03),[2,3,4,5]),'gamma' : ('scale', 'auto')},\n       {'max_iter_predict':np.arange(10,201),'multi_class':('one_vs_rest', 'one_vs_one')},\n       {'criterion' : ('gini', 'entropy'),'max_depth':range(1,10),'splitter':('best','random'),'min_samples_split':np.arange(1,25)},\n       {'criterion' : ('gini', 'entropy'),'max_depth':range(1,10),'n_estimators':[5,10,15,25,50,75,100,125,150]}]","f743dca7":"alt_params=[]\nfor clf,param in zip(classifiers,params):\n    grid=GridSearchCV(clf,param)\n    grid.fit(X_train,y_train)\n    print(clf,' = ',grid.best_params_) \n    alt_params.append(grid.best_params_)\n    #it may take a while","fcaa368e":"alt_params","1de1206c":"classifiers=[LogisticRegression(C= alt_params[0]['C'], multi_class= alt_params[0]['multi_class'], penalty= alt_params[0]['penalty'], solver= alt_params[0]['solver']),\n             KNeighborsClassifier(algorithm= alt_params[1]['algorithm'], metric= alt_params[1]['metric'], n_neighbors= alt_params[1]['n_neighbors'], weights= alt_params[1]['weights']),\n             SVC(C= alt_params[2]['C'], gamma= alt_params[2]['gamma'], kernel= alt_params[2]['kernel']),\n             SVC(C= alt_params[3]['C'], gamma= alt_params[3]['gamma'], kernel= alt_params[3]['kernel']),\n             GaussianNB(),\n             BernoulliNB(),\n             GaussianProcessClassifier(max_iter_predict=alt_params[4]['max_iter_predict'],multi_class=alt_params[4]['multi_class']),\n             DecisionTreeClassifier(criterion= alt_params[5]['criterion'], max_depth= alt_params[5]['max_depth'],splitter=alt_params[5]['splitter'],min_samples_split=alt_params[5]['min_samples_split']),\n             RandomForestClassifier(criterion= alt_params[6]['criterion'], max_depth= alt_params[6]['max_depth'], n_estimators= alt_params[6]['n_estimators']),\n             AdaBoostClassifier(),\n             QuadraticDiscriminantAnalysis()]","a25e89b2":"best_scores=[]\nbs=predict(classifiers,best_scores)\nbs","d5e0861b":"error_rate=[]\nfor i in np.arange(50):\n    i+=1\n    knn_model=KNeighborsClassifier(n_neighbors=i)\n    knn_model.fit(X_train,y_train)\n    y_pred_i=knn_model.predict(X_test)\n    error_rate.append(np.mean(y_pred_i !=y_test))","3f830f9d":"import plotly.express as px\n\nfig = px.line(pd.DataFrame(error_rate),markers=True,title='Error Rate')\n\nfig.show()","235739f0":"classifiers=['LogisticRegression',\n             'KNeighborsClassifier',\n             'SVC(rbf)',\n             'SVC(linear)',\n             'GaussianNB',\n             'BernoulliNB',\n             'GaussianProcessClassifier',\n             'DecisionTreeClassifier',\n             'RandomForestClassifier',\n             'AdaBoostClassifier',\n             'QuadraticDiscriminantAnalysis']\ndf2=pd.DataFrame({'Default Params Score':ds,'Alternate Params Score':bs},index=classifiers)","d4b35485":"df2","43b7e1f5":"\nfig, axes = plt.subplots(ncols=1,nrows=2,figsize=(15,12))\n\n\nsns.barplot(y=df2.index,x='Default Params Score',data=df2*100,color='#ff1010',alpha=.75,ax=axes[0])\nsns.barplot(y=df2.index,x='Alternate Params Score',data=df2*100,color='#1010ff',alpha=.75,ax=axes[1]);\nfor i in range (len(axes)):\n    for p in axes[i].patches:\n        x = p.get_x() + p.get_width()+.5\n        y = p.get_y() + p.get_height()-.3\n        value = int(p.get_width())\n        axes[i].text(x, y, value, ha=\"left\",fontsize=10,bbox=dict(facecolor='#ccddee', edgecolor='black',boxstyle='circle', linewidth=0.45))\n\n","1b22fa02":"### Classification with Default Params","a80d046c":"### Bar Plot of Classifiers Score","ea184847":"### Lm Plot of Age and Cholesterol","2e835d33":"##### **Note that the parameters coming from gridsearch is not always best. Sometimes they predict worse than default parameters.**\n##### **It depends on method, data, random state etc.**","49041894":"### Joint Plot of Chol - Blood Pressure","48b662f3":"# Classification","4c58138d":"### KNN n_neighbors errors","d9b06fc7":"### Classification with GridSearch Params","b461f918":"# Get Info","4be2ff23":"# Comparing Classifiers with Default and Alternate Params","1f385ccb":"### Finding Alternate Params with GridSearch","73d0172d":"### Countplot of Diagnoses with Heart Rate","4f4e9724":"### Heatmap plot of Dataset","9b9389a5":"### Kde Plot of ST Depression - Blood Pressure","ccbdf0f0":"### Check for Null Data","f46d2432":"### Train Test Split","14373925":"### Standardize the Variables","c7aeccec":"#### Imports","d08cdd90":"# Exploratory Data Analysis","fe3f08ce":"**Age**: Age<br>\n**Sex**: Sex (1 = male; 0 = female)<br>\n**ChestPain**: Chest pain (typical, asymptotic, nonanginal, nontypical)<br>\n**RestBP**: Resting blood pressure<br>\n**Chol**: Serum cholestoral in mg\/dl<br>\n**Fbs**: Fasting blood sugar > 120 mg\/dl (1 = true; 0 = false)<br>\n**RestECG**: Resting electrocardiographic results<br>\n**MaxHR**: Maximum heart rate achieved<br>\n**ExAng**: Exercise induced angina (1 = yes; 0 = no)<br>\n**Oldpeak**: ST depression induced by exercise relative to rest<br>\n**Slope**: Slope of the peak exercise ST segment<br>\n**Ca**: Number of major vessels colored by flourosopy (0 - 3)<br>\n**Thal**: (3 = normal; 6 = fixed defect; 7 = reversable defect)<br>\n**Target**: AHD - Diagnosis of heart disease (1 = yes; 0 = no)<br>","691f23ec":"#### Imports"}}