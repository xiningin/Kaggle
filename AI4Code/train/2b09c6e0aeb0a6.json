{"cell_type":{"0b2b5a88":"code","1311e866":"code","f6e66fb0":"code","fe1aaeb3":"code","ae9871ac":"code","d6b9033a":"code","19014d46":"code","d58cfb48":"code","34d1718a":"code","e245551e":"code","9aaac005":"code","859adb21":"code","38a8f692":"code","41f84dfd":"code","ce1bc66f":"code","a138e725":"code","5b3e7e14":"code","af5564e1":"code","2061e4a3":"code","ef61cd3f":"code","cb48f622":"code","ad983c19":"code","3019585e":"code","0250ef58":"code","056fd727":"code","ac057a03":"code","40b5b549":"code","69c30423":"code","dd568f8b":"code","94ecb304":"code","020f3050":"code","e4fd4cd7":"code","d433b4bb":"code","4678ef1e":"markdown","2f4cd60f":"markdown","c608a886":"markdown","2c93e7b9":"markdown","b94480ba":"markdown","f0835c46":"markdown","601c827a":"markdown","f7a2e3f8":"markdown","00a8b4d3":"markdown","3fc6b816":"markdown","30f6bf2a":"markdown","cbd6168d":"markdown","c1fb9f76":"markdown","2281d09e":"markdown","dd2b995f":"markdown","df2e1945":"markdown","e5b02e16":"markdown","8c729e4e":"markdown","cf42f44f":"markdown","d598f8b6":"markdown","78abd7ee":"markdown","09f59116":"markdown","35c8bf44":"markdown","93623d3d":"markdown","48ed75d5":"markdown","36ff4b18":"markdown","47d66efe":"markdown","4e68aad5":"markdown","8e80a37b":"markdown","62f2661b":"markdown","e1b450b4":"markdown","b00807ca":"markdown","9e7c23ce":"markdown","ea2fae4c":"markdown","740469dd":"markdown","cb900901":"markdown","f8ddcf19":"markdown","6a660a47":"markdown"},"source":{"0b2b5a88":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly_express as px\n# import plotly.plotly as py\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\nimport os\nprint(os.listdir(\"..\/input\"))","1311e866":"NUM_MODELS = 2\nBATCH_SIZE = 512\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nMAX_LEN = 220","f6e66fb0":"print(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\"))","fe1aaeb3":"train_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\nsample_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv')","ae9871ac":"train_df.head(2)\n","d6b9033a":"test_df.head(2)","19014d46":"sample_df.head(2)","d58cfb48":"IDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\nTEXT_COLUMN = 'comment_text'\nTARGET_COLUMN = 'target'","34d1718a":"x_train = train_df[TEXT_COLUMN].astype(str)\ny_train = train_df[TARGET_COLUMN].values\ny_aux_train = train_df[AUX_COLUMNS].values\nx_test = test_df[TEXT_COLUMN].astype(str)\n\nfor column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n    train_df[column] = np.where(train_df[column] >= 0.5, True, False)","e245551e":"x_train.head()","9aaac005":"y_train[:5]","859adb21":"train_df.head(5)","38a8f692":"from keras.preprocessing import text\nCHARS_TO_REMOVE = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014'\n\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(list(x_train) + list(x_test))","41f84dfd":"x_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)","ce1bc66f":"print(x_train[0])","a138e725":"from keras.preprocessing import sequence\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\nprint(x_train[0])","5b3e7e14":"sample_weights = np.ones(len(x_train), dtype=np.float32)","af5564e1":"sample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\n\n\"\"\"\nFor reminder on identity columns:\nIDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\n\"\"\"","2061e4a3":"data = [go.Histogram(x=sample_weights)]\nlayout = {'title': 'Distribution of weights after adding identity_columns'}\niplot({'data':data, 'layout':layout})","ef61cd3f":"sample_weights += train_df[TARGET_COLUMN] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\nsample_weights += (~train_df[TARGET_COLUMN]) * train_df[IDENTITY_COLUMNS].sum(axis=1) * 5\nsample_weights \/= sample_weights.mean()","cb48f622":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix","ad983c19":"path = '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\nwith open(path) as f:\n    i = 0\n    for line in f:\n        output1 = line.strip().split(' ')\n        print(output1)\n        print(type(output1))\n        print('=====')\n        i += 1\n        if i == 5:\n            break              ","3019585e":"EMBEDDING_FILES = [\n    '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec',\n    '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\n]\nembedding_file1 = '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\nembedding_file2 = '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'","0250ef58":"# embedding_index = load_embeddings(path)\n# embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 300))\n# for word, i in tokenizer.word_index.items():\n#     try:\n#         embedding_matrix[i] = embedding_index[word]\n#     except KeyError:\n#         pass  ","056fd727":"print(embedding_matrix[:2])","ac057a03":"embedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)","40b5b549":"\"\"\"\nReminder: y_aux_train = train_df[AUX_COLUMNS].values\n\"\"\"\nnum_aux_targets = y_aux_train.shape[-1]\nprint(num_aux_targets)","69c30423":"\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 4","dd568f8b":"def build_model(embedding_matrix, num_aux_targets):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model","94ecb304":"model = build_model(embedding_matrix, y_aux_train.shape[-1])","020f3050":"model.summary()","e4fd4cd7":"# checkpoint_predictions = []\n# weights = []\n# EPOCHS = 4\n# NUM_MODELS = 2\n# BATCH_SIZE = 512\n\n# for model_idx in range(NUM_MODELS):  # Not sure why we use this, since model_idx is never used below\n#     model = build_model(embedding_matrix, y_aux_train.shape[-1])\n#     for global_epoch in range(EPOCHS):\n#         model.fit(\n#             x_train,\n#             [y_train, y_aux_train],\n#             batch_size=BATCH_SIZE,\n#             epochs=1,\n#             verbose=2,\n#             sample_weight=[sample_weights.values, np.ones_like(sample_weights)],\n#             callbacks=[\n#                 LearningRateScheduler(lambda _: 1e-3 * (0.55 ** global_epoch))\n#             ]\n#         )\n#         checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n#         weights.append(2 ** global_epoch)","d433b4bb":"# predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n\n# submission = pd.DataFrame.from_dict({\n#     'id': test_df.id,\n#     'prediction': predictions\n# })\n# submission.to_csv('submission.csv', index=False)","4678ef1e":"# Embedding Matrix","2f4cd60f":"Let's check out what it looks like","c608a886":"### Explanation\nThe weighting steps below are to account for Bias AUC metrics presented in the evaluation. To recap, it consisted of 3 criteria:\n\n**Subgroup AUC:** Here, we restrict the data set to only the examples that mention the specific identity subgroup. A low value in this metric means the model does a poor job of distinguishing between toxic and non-toxic comments that mention the identity.\n\n**BPSN (Background Positive, Subgroup Negative) AUC:** Here, we restrict the test set to the non-toxic examples that mention the identity and the toxic examples that do not. A low value in this metric means that the model confuses non-toxic examples that mention the identity with toxic examples that do not, likely meaning that the model predicts higher toxicity scores than it should for non-toxic examples mentioning the identity.\n\n**BNSP (Background Negative, Subgroup Positive) AUC: **Here, we restrict the test set to the toxic examples that mention the identity and the non-toxic examples that do not. A low value here means that the model confuses toxic examples that mention the identity with non-toxic examples that do not, likely meaning that the model predicts lower toxicity scores than it should for toxic examples mentioning the identity.","2c93e7b9":"As you can see, this example has relatively few word compared to the longest text. Therefore, we see a lot of zero padding being added in the front.","b94480ba":"Then we apply tokenizer to transform X_train and X_test dataframes into token sequences","f0835c46":"# Weightings to account for the bias metricss","601c827a":"# Now since the model is built, we'll run the predictions","f7a2e3f8":"Let's see how the load_embeddings function work:","00a8b4d3":"Initialize weighting","3fc6b816":"# Final Notes\n\nThis Kernel is still WIP\nI commented out the training and prediction steps to cut down on running time, since the updates after the previous commit is just the markdown","30f6bf2a":"#### Explanation: Multiple LSTM Units\nHere is a good explanation about having multiple LSTM units https:\/\/stackoverflow.com\/questions\/44273249\/in-keras-what-exactly-am-i-configuring-when-i-create-a-stateful-lstm-layer-wi\n\n![Image](https:\/\/i.stack.imgur.com\/xLZCK.png)\n\n![Image2](https:\/\/i.stack.imgur.com\/PQs02.png)\n\n","cbd6168d":"First, we want to get the embedding from pre-trained model. The output is embedding index, which maps a word with its embedding vector.\n\nAfterwards, we want to get our tokenizer word index from our training data corpus, and then for each word \/ word index, we'd map its embedding vector\n\nThe steps here also similar to steps outlined here: https:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/","c1fb9f76":"# Now, build model stuffs","2281d09e":"Now, print example of two words and show its embedding vector","dd2b995f":"Here is another good guide: https:\/\/medium.com\/jatana\/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f","df2e1945":"If you try to look up weights parameter in Keras.Layers.Embedding documentation, you would not find it there. It is because weights and trainable are inherited from its Layers base class","e5b02e16":"We'll create embedding index from each file separately to have better visibility","8c729e4e":"### Now that we've done Embedding Matrix building manually, let's just rerun the provided function","cf42f44f":"# Model Architecture Overview\n![Image](https:\/\/i.ibb.co\/XYjvJJ8\/Screen-Hunter-3119.jpg)\n![Image](https:\/\/i.ibb.co\/RpNyPWb\/Screen-Hunter-3118.jpg)","d598f8b6":"Except for the first line, each line is a list of 301 elements. The first element is the word, and then the next 300 is the vector. So the function above will create dictionary that map each word to the vector.\n\nThe get_coef function reads 301 elements as input to the function, and will output a tupple of (word, 300-D array), which will get dictionarized","78abd7ee":"# Pre-work: Variables","09f59116":"# Pre-work - Working with label columns","35c8bf44":"Let's start with the weighting on identity_columns and show the histogram","93623d3d":"After each epoch run, we will run and save prediction based on the latest model. In the next section, we will make our prediction based on weighted average of each checkpoint prediction","48ed75d5":"## Text Pre-processing","36ff4b18":"## First, create 3 custom functions","47d66efe":"# Imports","4e68aad5":"Then we do padding so that each row will have the same length of tokens. We'll also print out one example","8e80a37b":"Other weightings that we'll apply:\n* Adding weights when the comment is abusive and not mentioning identity\n* Adding 5x weights when the comment is not abusive, but an innocent mention of identity","62f2661b":"First, we fit the tokenizer on X train and test. The text to be fitted by tokenizer must be in form of list, so we convert the original DataFrame format into list first","e1b450b4":"Instead of using model from final epoch, we will use weighted mean of prediction from each epoch.  The weighting is based on 2^epoch, which doubles the weight for each epoch step","b00807ca":"# Overview\n\nThis notebook will run through step-by-step and provide explanatory notes on the following notebook:\n* Basic LSTM: https:\/\/www.kaggle.com\/thousandvoices\/simple-lstm\n\nAlso leveraging this EDA to have better understanding of the data: https:\/\/www.kaggle.com\/ekhtiar\/unintended-eda-with-tutorial-notes","9e7c23ce":"Manually create the embedding matrix","ea2fae4c":"### From Embedding Index + Word Index --> Embedding Matrix","740469dd":"# Pre-work: Basic EDA","cb900901":"### Spatial Dropout\n\nGood explanation taken from stackoverflow https:\/\/stackoverflow.com\/questions\/50393666\/how-to-understand-spatialdropout1d-and-when-to-use-it\n\n>  In order to understand SpatialDropout1D, you should get used to the notion of the noise shape. In plain vanilla dropout, each element is kept or dropped independently. For example, if the tensor is [2, 2, 2], each of 8 elements can be zeroed out depending on random coin flip (with certain \"heads\" probability); in total, there will be 8 independent coin flips and any number of values may become zero, from 0 to 8.\n\n> Sometimes there is a need to do more than that. For example, one may need to drop the whole slice along 0 axis. The noise_shape in this case is [1, 2, 2] and the dropout involves only 4 independent random coin flips. The first component will either be kept together or be dropped together. The number of zeroed elements can be 0, 2, 4, 6 or 8. It cannot be 1 or 5.\n\n> You may want to do this to account for adjacent pixels correlation, especially in the early convolutional layers. Effectively, you want to prevent co-adaptation of pixels with its neighbors across the feature maps, and make them learn as if no other feature maps exist.\n\nAnother simpler one from https:\/\/datascience.stackexchange.com\/questions\/38519\/what-does-spatialdropout1d-do-to-output-of-embedding-in-keras:\n\n> Basically, it removes all the pixel in a row from all channels. eg: take [[1,1,1], [2,4,5]], there are 3 points with values in 2 channels, by doing SpatialDropout1D it zeros an entire row ie all attributes of a point is set to 0; like [[1,1,0], [2,4,0]]\n\n> The intuition behind this is in many cases for an image the adjacent pixels are correlated, so hiding one of them is not helping much, rather hiding entire row, that's gotta make a difference\n","f8ddcf19":"Overview of model Summary\n> ![Image](https:\/\/i.ibb.co\/RpNyPWb\/Screen-Hunter-3118.jpg)","6a660a47":"## Pre-work: Get your pre-trained NLP model for embedding first\nClick on \"+ Add Dataset\" on the top right corner, and add these two datasets:\n* https:\/\/www.kaggle.com\/takuok\/glove840b300dtxt\n* https:\/\/www.kaggle.com\/yekenot\/fasttext-crawl-300d-2m\n\n\n"}}