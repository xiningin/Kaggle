{"cell_type":{"b7de684a":"code","f609a0df":"code","538f0d4c":"code","56222e7a":"code","857f3e58":"code","45a4c7d3":"code","56a48f99":"code","9b01a5f7":"code","3b15734f":"code","952e214c":"code","07d77c84":"code","00d5c6d1":"code","a74e3869":"code","d7dd500d":"code","f244cca1":"code","8cf9b8ff":"code","965a5f42":"code","1364e897":"code","3ee8cd19":"code","d9ff0006":"code","03d92f5c":"code","f31da4b5":"code","b6f9050f":"code","1bec3b81":"code","42dddd11":"code","358999fe":"code","d83c7ff7":"code","c3ca5d11":"code","880488b6":"code","2f18a80d":"markdown","2395d223":"markdown","babc36b7":"markdown","650ee51e":"markdown","84a5ad6a":"markdown","23d47ff9":"markdown","8be36199":"markdown","294db535":"markdown"},"source":{"b7de684a":"!pip install seaborn --upgrade\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import confusion_matrix, classification_report","f609a0df":"data = pd.read_csv('..\/input\/forest-cover-type-dataset\/covtype.csv')","538f0d4c":"data","56222e7a":"# Start class labels from 0 rather than 1\ndata['Cover_Type'] = data['Cover_Type'] - 1","857f3e58":"data['Cover_Type'].value_counts()","45a4c7d3":"cmap = sns.color_palette('Set2', as_cmap=True)(np.arange(7))\n\nplt.figure(figsize=(8, 8))\nplt.pie(\n    data['Cover_Type'].value_counts().values,\n    colors=cmap,\n    labels=data['Cover_Type'].value_counts().keys(),\n    autopct='%.1f%%'\n)\nplt.title(\"Class Distribution\")\nplt.show()","56a48f99":"def split_and_scale(df):\n    df = df.copy()\n    \n    # Split df into X and y\n    y = df['Cover_Type'].copy()\n    X = df.drop('Cover_Type', axis=1).copy()\n    \n    # Train-test-split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    \n    X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n    \n    return X_train, X_test, y_train, y_test","9b01a5f7":"def evaluate_model(model, class_balance, X_test, y_test):\n    \n    model_acc = model.score(X_test, y_test)\n    print(\"Accuracy ({}): {:.2f}%\".format(class_balance, model_acc * 100))\n    \n    y_pred = model.predict(X_test)\n    \n    cm = confusion_matrix(y_test, y_pred)\n    clr = classification_report(y_test, y_pred)\n    \n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt='g', vmin=0, cbar=False, cmap='Blues')\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n    \n    print(\"Classification Report:\\n----------------------\\n\", clr)","3b15734f":"imbalanced_data = data.copy()\n\nX_train, X_test, y_train, y_test = split_and_scale(imbalanced_data)","952e214c":"model1 = LogisticRegression()\nmodel1.fit(X_train, y_train)","07d77c84":"evaluate_model(model1, \"Imbalanced\", X_test, y_test)","00d5c6d1":"undersampled_data = data.copy()","a74e3869":"undersampled_data['Cover_Type'].value_counts()","d7dd500d":"min_class_size = np.min(undersampled_data['Cover_Type'].value_counts().values)\n\nprint(\"Size of smallest class:\", min_class_size)","f244cca1":"# Undersampling the majority classes\nclass_subsets = [undersampled_data.query(\"Cover_Type == \" + str(i)) for i in range(7)]\n\nfor i in range(7):\n    class_subsets[i] = class_subsets[i].sample(min_class_size, replace=False, random_state=123)\n\nundersampled_data = pd.concat(class_subsets, axis=0).sample(frac=1.0, random_state=123).reset_index(drop=True)","8cf9b8ff":"undersampled_data","965a5f42":"undersampled_data['Cover_Type'].value_counts()","1364e897":"X_train, X_test, y_train, y_test = split_and_scale(undersampled_data)","3ee8cd19":"model2 = LogisticRegression()\nmodel2.fit(X_train, y_train)","d9ff0006":"evaluate_model(model2, \"Undersampling\", X_test, y_test)","03d92f5c":"oversampled_data = data.copy()","f31da4b5":"oversampled_data['Cover_Type'].value_counts()","b6f9050f":"max_class_size = np.max(oversampled_data['Cover_Type'].value_counts().values)\n\nprint(\"Size of largest class:\", max_class_size)","1bec3b81":"# Oversampling the minority classes\nclass_subsets = [oversampled_data.query(\"Cover_Type == \" + str(i)) for i in range(7)]\n\nfor i in range(7):\n    class_subsets[i] = class_subsets[i].sample(max_class_size, replace=True, random_state=123)\n\noversampled_data = pd.concat(class_subsets, axis=0).sample(frac=1.0, random_state=123).reset_index(drop=True)","42dddd11":"oversampled_data","358999fe":"oversampled_data['Cover_Type'].value_counts()","d83c7ff7":"X_train, X_test, y_train, y_test = split_and_scale(oversampled_data)","c3ca5d11":"model3 = LogisticRegression()\nmodel3.fit(X_train, y_train)","880488b6":"evaluate_model(model3, \"Oversampling\", X_test, y_test)","2f18a80d":"# Task for Today  \n\n***\n\n## Forest Cover Type Prediction  \n\nGiven *data about trees and forests*, let's try to predict the **cover type** of a given forest.  \n  \nWe will use a logistic regression model to make our predictions, but first we have to deal with the imbalanced classes.","2395d223":"# Getting Started","babc36b7":"# Training (Oversampling)","650ee51e":"# Imbalanced Data: Class Distribution","84a5ad6a":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/y70bUYIPe2A","23d47ff9":"# Training (Imbalanced)","8be36199":"# Training (Undersampling)","294db535":"# Some Helper"}}