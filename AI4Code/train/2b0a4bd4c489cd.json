{"cell_type":{"737790d0":"code","ea31739c":"code","29ef4603":"code","6774caa8":"code","41307aef":"code","77e73568":"code","7d262259":"code","fe328fbf":"code","c11e9264":"code","8294c1ca":"code","8eae3417":"code","629e94f3":"code","349e05dd":"code","929f454e":"code","34856831":"code","04b9cc3d":"code","f925867a":"code","74b74d67":"code","7bd4d51a":"code","de029dce":"code","540de2ab":"code","9c2d4518":"code","0c39ab21":"code","f6b9fee8":"code","79e9d7fb":"code","88b2aa3d":"code","69e4d0f7":"code","610fa6cf":"code","37d8a3d0":"code","5b128af1":"code","e14f59a4":"code","be70daa6":"code","07609f30":"code","fb7ff36a":"code","7f4cd472":"code","3f128a0a":"code","26bc5096":"code","92d3dfa2":"code","e22fb434":"code","2a633dff":"code","25ff3f5f":"code","3d71c7b6":"code","1eb79f0e":"markdown","39f43c04":"markdown"},"source":{"737790d0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# list all directory \nimport os\nprint(os.listdir(\"..\/input\"))","ea31739c":"#load train and test \ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","29ef4603":"#check sample data, there are lot of NaNs in many variables\ntrain.head(10)","6774caa8":"#get training data set dimensions\nprint(\"train data shape\", train.shape)\n#get testing data set dimensions\nprint(\"test data shape\", test.shape)","41307aef":"#list of columns \nprint(\"\\ncolumn in training data set\\n\\n\",train.columns.values)\nprint(\"\\ncolumn in testing data set\\n\\n\",test.columns.values)","77e73568":"print(\"extra columns found in training dataset\", set(train.columns.values)-set(test.columns.values))","7d262259":"#check  summary of dependent variable\ntrain.SalePrice.describe()","fe328fbf":"\nimport matplotlib.pyplot as plt \nplt.style.use(style='ggplot')\nplt.rcParams['figure.figsize']=(10,6)\n#check skewness of sale price \nprint(\"Skewness : \", train.SalePrice.skew())\nplt.hist(train.SalePrice, color='blue')\nplt.show()\n#distribution is a bit skewed to the left ","c11e9264":"#outlier detection \nimport seaborn as sns\nsns.boxplot(x=train['SalePrice'])\ntrain[train[\"SalePrice\"]>450000].head(30).reset_index() ","8294c1ca":"train.head()","8eae3417":"#treat outliers \ntrain['SalePrice'] = np.where(train['SalePrice']>450000 , 450000 ,train['SalePrice'])","629e94f3":"#change sale price to natural log \nprint(\"Skewness after log: \", np.log(train.SalePrice).skew())\nplt.hist(np.log(train.SalePrice), color='blue')\nplt.show()\ntarget= np.log(train.SalePrice)\n#natural log transformation of target var changes it to normal distribution ","349e05dd":"#find numeric features \nnum_features = train.select_dtypes(include=[np.number])\n#check data types of these \nnum_features.dtypes","929f454e":"#check correlation of numeric variables \ncorr = num_features.corr()\n#top 5 highly correlated vars \nprint(corr['SalePrice'].sort_values(ascending=False)[:5],'\\n')\n#bottom 5 highly correlated vars \nprint(corr['SalePrice'].sort_values(ascending=False)[-5:],'\\n')","34856831":"#check unique values of feature OverallQual\ntrain.OverallQual.unique()","04b9cc3d":"#check first overall quality variable with SalePirce\nqual_pivot = train.pivot_table(index='OverallQual', \n                               values='SalePrice', \n                               aggfunc=np.mean)\ndisplay(qual_pivot)","f925867a":"#create pivot for overall quality \nqual_pivot.plot(kind='bar', color='green')\nplt.xlabel('Overall Quality')\nplt.ylabel('Mean Sale Price')\nplt.xticks(rotation=0)\nplt.show()","74b74d67":"#create pivot for Gr Living area \nplt.scatter(x=train['GrLivArea'], y=target)\nplt.ylabel('Sale Price')\nplt.xlabel('Above grade(ground) living area square feet')\nplt.show()","7bd4d51a":"plt.scatter(x=train['GarageArea'], y=target)\nplt.ylabel('Sale Price')\nplt.xlabel('Garage Area')\nplt.show()","de029dce":"train = train[train['GarageArea'] < 1150]\nplt.scatter(x=train['GarageArea'], y =np.log(train.SalePrice))\nplt.xlim(-200,1600) #adjusting to same scale \nplt.ylabel('Sale Price')\nplt.xlabel('Garage Area')\nplt.show()","540de2ab":"nuls = pd.DataFrame(train.isnull().sum().sort_values(ascending =False)[:25])\nnuls.columns = ['Null Count']\nnuls.index.name = 'Feature'\nnuls","9c2d4518":"print (\"Unique values are:\", train.MiscFeature.unique())","0c39ab21":"catgr  = train.select_dtypes(exclude=[np.number])\ncatgr.describe()","f6b9fee8":"print('originals')\nprint(train.Street.value_counts(),\"\\n\")","79e9d7fb":"train['enc_street'] = pd.get_dummies(train.Street, drop_first=True)\ntest['enc_street'] = pd.get_dummies(train.Street, drop_first=True)","88b2aa3d":"print(\"Encoded:\")\nprint(train.enc_street.value_counts())","69e4d0f7":"# One more variable, Garage car capacity\ntrain.GarageCars.value_counts().plot(kind='bar', color='green')\nplt.xlabel('Garage Car Capacity')\nplt.ylabel('Counts')\nplt.xticks(rotation=0)\nplt.show()","610fa6cf":"condition_pivot = train.pivot_table(index='SaleCondition', values='SalePrice', aggfunc=np.median)\ncondition_pivot.plot(kind='bar', color='skyblue')\nplt.xlabel('Sale Condition')\nplt.ylabel('Median Sale Price')\nplt.xticks(rotation=0)\nplt.show()\n#encoding steps\ndef encode_condition(x) : \n    return 1 if x =='Partial' else 0\ntrain['enc_condition'] = train.SaleCondition.apply(encode_condition)\ntest['enc_condition'] = test.SaleCondition.apply(encode_condition)","37d8a3d0":"condition_pivot = train.pivot_table(index='enc_condition', values='SalePrice', aggfunc=np.median)\ncondition_pivot.plot(kind='bar', color='gray')\nplt.xlabel('Encoded Sale Condition')\nplt.ylabel('Median Sale Price')\nplt.xticks(rotation=0)\nplt.show()","5b128af1":"#update missing values \ntrain = train.fillna(train.mean())\ntest = test.fillna(test.mean())","e14f59a4":"#interpolate missing values \ndt = train.select_dtypes(include=[np.number]).interpolate().dropna()\n#check if all cols have zero null values \nsum(dt.isnull().sum()!=0)","be70daa6":"#change y to natural log \ny = np.log(train.SalePrice)\n#drop original dependent var and id \nX = dt.drop(['Id','SalePrice'], axis=1)","07609f30":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)","fb7ff36a":"#Hyper paramteter tuning  example \nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n#Hyper parameter tuning example \ngbm = xgb.XGBRegressor()\nreg_cv = GridSearchCV(gbm, {\"colsample_bytree\":[1.0],\"min_child_weight\":[1.0,1.2]\n                            ,'max_depth': [3,4,6], 'n_estimators': [500,1000]}, verbose=1)\nreg_cv.fit(X_train,y_train)\nreg_cv.best_params_\n","7f4cd472":"#Hyper paramteter tuning  example \nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n#Hyper parameter tuning example \ngbm = xgb.XGBRegressor()\nreg_cv = GridSearchCV(gbm, {\"colsample_bytree\":[1.0],\"min_child_weight\":[1.0,1.2]\n                            ,'max_depth': [3,4,6], 'n_estimators': [500,1000]}, verbose=1)\nreg_cv.fit(X_train,y_train)\nreg_cv.best_params_\n","3f128a0a":"###########\ngbm = xgb.XGBRegressor(**reg_cv.best_params_)\ngbm.fit(X_train,y_train)\n##############\nsubmit= pd.DataFrame()\nsubmit['Id'] = test.Id\ntest_features = test.select_dtypes(include=[np.number]).drop(['Id'], axis=1).interpolate()\npreds = gbm.predict(test_features)\nfinal_preds = np.exp(preds)\nprint('Original preds :\\t', preds[:5])\nprint('Final preds :\\t', final_preds[:5])\nsubmit['SalePrice'] = final_preds\n#final submission  \nsubmit.to_csv('xgb_hyper_param_subm.csv', index=False)\nprint('XGB submission using hyper param tuning code  created')","26bc5096":"#1. linear regression \nfrom sklearn import linear_model\nlr = linear_model.LinearRegression()\nmodel = lr.fit(X_train, y_train)\n#r square \nprint(\"R-Square : \" ,model.score(X_test,y_test))\n#rmse \npreds = model.predict(X_test)\nfrom sklearn.metrics import mean_squared_error\nprint ('RMSE: ', mean_squared_error(y_test, preds))","92d3dfa2":"#Adding simple XGB output and test \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom xgboost import XGBRegressor\n#\ndef xgb_regressor(learn_rate):\n    #instance of XGB regressor \n    xgbmodel = XGBRegressor(n_estimators=1000, learning_rate=learn_rate)\n    xgbmodel.fit(X_train, y_train, verbose=False)\n    # make predictions\n    predictions = xgbmodel.predict(X_test)\n    from sklearn.metrics import mean_absolute_error\n    print('{:^20}'.format('Learning Rate:')+ '{:^5}'.format(str(learn_rate)) +'{:^5}'.format(\"\\tMAE: \")+'{:<20}'.format(str(mean_absolute_error( y_test,predictions))) +'{:^5}'.format(\"\\tRMSE: \")+'{:<20}'.format(str(mean_squared_error( y_test,predictions)) +'{:^5}'.format(\"\\tR^2: \")+'{:<20}'.format(xgbmodel.score(X_test,y_test)))  )\n    \nxgb_regressor(0.04) #experimented with .03 -.09, .04 looks best \n\n#using best learning rate xgb_regressor(0.04) and updating same code for submission \ndef xgb_regressor_updated(learn_rate):\n    #instance of XGB regressor \n    print('***********Final run with best learning rate*************')\n    xgbmodel = XGBRegressor(n_estimators=1000, learning_rate=learn_rate)\n    xgbmodel.fit(X_train, y_train, verbose=False)\n    # make predictions\n    predictions = xgbmodel.predict(X_test)\n    from sklearn.metrics import mean_absolute_error\n    print('{:^20}'.format('Learning Rate:')+ '{:^5}'.format(str(learn_rate)) +'{:^5}'.format(\"\\tMAE: \")+'{:<20}'.format(str(mean_absolute_error( y_test,predictions))) +'{:^5}'.format(\"\\tRMSE: \")+'{:<20}'.format(str(mean_squared_error( y_test,predictions)) +'{:^5}'.format(\"\\tR^2: \")+'{:<20}'.format(xgbmodel.score(X_test,y_test)))  )\n    #test \n    submit= pd.DataFrame()\n    submit['Id'] = test.Id\n    test_features = test.select_dtypes(include=[np.number]).drop(['Id'], axis=1).interpolate()\n    preds = xgbmodel.predict(test_features)\n    final_preds = np.exp(preds)\n    print('Original preds :\\t', preds[:5])\n    print('Final preds :\\t', final_preds[:5])\n    submit['SalePrice'] = final_preds\n    #final submission  \n    submit.to_csv('xgb_submit.csv', index=False)\n    print('XGB submission file created')\n\n#test and create xgb submission \nxgb_regressor_updated(0.04)","e22fb434":"plt.scatter(preds, y_test, alpha=.75, color='g')\nplt.xlabel('predicted price')\nplt.ylabel('actual sale price ')\nplt.title('Linear regression ')\nplt.show()","2a633dff":"#Regularization \nfor i in range (-3, 3):\n    alpha = 10**i\n    rm = linear_model.Ridge(alpha=alpha)\n    ridge_model = rm.fit(X_train, y_train)\n    preds_ridge = ridge_model.predict(X_test)\n    plt.scatter(preds_ridge, y_test, alpha=.75, color='g')\n    plt.xlabel('Predicted Price')\n    plt.ylabel('Actual Price')\n    plt.title('Ridge Regularization with alpha = {}'.format(alpha))\n    overlay = 'R^2 is: {}\\nRMSE is: {}'.format(ridge_model.score(X_test, y_test),\n                                               mean_squared_error(y_test, preds_ridge))\n    plt.annotate(s=overlay,xy=(12.1,10.6),size='x-large')\n    plt.show()","25ff3f5f":"submit= pd.DataFrame()\nsubmit['Id'] = test.Id\n#select features \ntest_features = test.select_dtypes(include=[np.number]).drop(['Id'], axis=1).interpolate()\npreds = model.predict(test_features)\n#unlog\/exp the prediction  \nfinal_preds = np.exp(preds)\nprint('Original preds :\\n', preds[:5])\nprint('Final preds :\\n', final_preds[:5])\nsubmit['SalePrice'] = final_preds\n#final submission  \nsubmit.to_csv('test_submit.csv', index=False)","3d71c7b6":"#variables not used \nprint(\"Vars not used : \\n\", set(test.columns.values)-set(X.columns.values))","1eb79f0e":"<h1 style=\"color:red;text-align:center\">Basic Tutorial<\/h1>","39f43c04":"\n<h5> Conclusion:\n    XGB performs better than linear and ridge \n    <br><br>latest: Updating the outliers in dependent variable improved the score marginally <\/h5>"}}