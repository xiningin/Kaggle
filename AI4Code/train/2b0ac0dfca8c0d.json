{"cell_type":{"88193523":"code","988d6b4a":"code","9ad86787":"code","2ef8cbd1":"code","5b056f16":"code","851ba828":"code","712984a5":"code","4c39ccf0":"code","63e8fded":"code","6ef31262":"code","b9c2c2f9":"code","d4b5fe3a":"code","cd41c67b":"code","916ead55":"code","5970f561":"markdown","4255fa32":"markdown","4818b4fc":"markdown","673b79e3":"markdown","a265ac19":"markdown","77acb8fa":"markdown","414c98e7":"markdown","4c0940ef":"markdown","c95bf59f":"markdown","e18ada40":"markdown","0275d43e":"markdown"},"source":{"88193523":"!\/opt\/conda\/bin\/python3.7 -m pip install --upgrade pip\n! pip install -q efficientnet","988d6b4a":"#-------------------\n# importing libraries\n#-------------------\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport efficientnet.tfkeras as efn\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nfrom kaggle_datasets import KaggleDatasets\n\n\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport shutil\nimport csv\n\nimport matplotlib.pyplot as plt\nimport PIL","9ad86787":"AUTO = tf.data.experimental.AUTOTUNE\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","2ef8cbd1":"GCS_DS_PATH = KaggleDatasets().get_gcs_path(\"siim-covid19-dataset-256px-jpg\")\nprint(GCS_DS_PATH)\n\nTRAIN_PATH = GCS_DS_PATH + \"\/256px\/train\/train\/\"\nTEST_PATH = GCS_DS_PATH + \"\/256px\/test\/test\/\"\n\nNUM_CLASSES = 4\nHEIGHT,WIDTH = 256,256\nCHANNELS = 3\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nSEED = 143\n\n\nclasses_dict = {\n    \"Negative for Pneumonia\" : 0,\n    \"Typical Appearance\" : 1,\n    \"Indeterminate Appearance\" : 2,\n    \"Atypical Appearance\"  : 3\n}\n\nstudy_df = pd.read_csv(\"..\/input\/siim-covid19-detection\/train_study_level.csv\")\ntrain_df = pd.read_csv('..\/input\/siim-covid19-dataset-256px-jpg\/train.csv')\ntrain_df = train_df[[\"ImageInstanceUID\",\"StudyInstanceUID\",\"label_id\",\"study_label\"]]","5b056f16":"train_df.head(3)","851ba828":"study_df.head(3)","712984a5":"def process_img(filepath,label):\n    image = tf.io.read_file(filepath)\n    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n    image = tf.image.convert_image_dtype(image, tf.float32) \n    #image = tf.image.resize(image, [HEIGHT,WIDTH])\n    return image,label\n\n\ndef data_augment(image, label):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n            \n    # Flips\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n        \n    # Rotates\n    if p_rotate > .75:\n        image = tf.image.rot90(image, k=3) \n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k=2) \n    elif p_rotate > .25:\n        image = tf.image.rot90(image, k=1) \n        \n    \n    if p_pixel_1 >= .4:\n        image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n    if p_pixel_2 >= .4:\n        image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n    if p_pixel_3 >= .4:\n        image = tf.image.random_brightness(image, max_delta=.1)\n        \n    \n    if p_crop > .7:\n        if p_crop > .9:\n            image = tf.image.central_crop(image, central_fraction=.7)\n        elif p_crop > .8:\n            image = tf.image.central_crop(image, central_fraction=.8)\n        else:\n            image = tf.image.central_crop(image, central_fraction=.9)\n    elif p_crop > .4:\n        crop_size = tf.random.uniform([], int(HEIGHT*.8), HEIGHT, dtype=tf.int32)\n        image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS])\n    \n    image = tf.image.resize(image, [HEIGHT,WIDTH])\n    return image,label\n\ndef get_dataset(filenames,labels, training=True):\n    dataset = tf.data.Dataset.from_tensor_slices((filenames,labels))\n    dataset = dataset.map(process_img,num_parallel_calls=AUTO)\n    dataset = dataset.map(data_augment,num_parallel_calls=AUTO)\n    dataset = dataset.cache()\n    dataset = dataset.repeat()\n    if training:\n        dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset","4c39ccf0":"def create_model():\n    \n    pretrained = efn.EfficientNetB4(include_top=False, weights='noisy-student',input_shape=[HEIGHT,WIDTH, 3])\n            \n    x = pretrained.output\n    x = tf.keras.layers.GlobalAveragePooling2D() (x)\n    outputs = tf.keras.layers.Dense(NUM_CLASSES,activation=\"softmax\", dtype='float32')(x)\n        \n    model = tf.keras.Model(pretrained.input, outputs)\n    return model\n\nmodel = create_model()\n#model.summary()","63e8fded":"#chxnet model\n\nwith strategy.scope():\n    model = tf.keras.applications.DenseNet121(weights= \"imagenet\",\n                                    include_top=False,\n                                    input_shape=(HEIGHT,WIDTH,CHANNELS), pooling=\"avg\")\n    predictions = tf.keras.layers.Dense(14, activation='sigmoid', name='predictions')(model.output)\n\n    model = tf.keras.Model(inputs=model.input, outputs=predictions)   \n    model.load_weights(\"..\/input\/pneumonia-classification-challenge\/pretrained.h5\")\n    model = tf.keras.Model(model.input, model.layers[-2].output) \n    x = tf.keras.layers.Dense(512, activation = \"relu\")(model.output)\n    x = tf.keras.layers.Dropout(0.3)(x)\n    x = tf.keras.layers.Dense(128, activation = \"relu\")(x)\n    x = tf.keras.layers.Dense(64, activation = \"relu\")(x)\n    outputs = tf.keras.layers.Dense(NUM_CLASSES, activation = \"softmax\", dtype = tf.float32)(x)\n\n    chxnet = tf.keras.Model(model.input,outputs)\n\n    for layer in chxnet.layers[:-15]:\n        layer.trainble = False","6ef31262":"import tensorflow_addons as tfa\n\ndef compile_model(model, lr=0.001):\n    \n    optimizer = tf.keras.optimizers.Adam(lr=lr)\n    \n    loss = tf.keras.losses.CategoricalCrossentropy()\n   \n    metrics = [\n       tfa.metrics.F1Score(num_classes = NUM_CLASSES,average = \"macro\", name = \"f1_score\"),\n       tf.keras.metrics.CategoricalAccuracy(name='acc')\n    ]\n\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n    return model","b9c2c2f9":"METRIC = \"val_acc\"\n\ndef create_callbacks(kfold,metric = METRIC):\n    \n    cpk_path = f'.\/best_model_{kfold}.h5'\n    \n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath=cpk_path,\n        monitor= metric,\n        mode='max',\n        save_best_only=True,\n        verbose=1,\n    )\n\n    reducelr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor= metric,\n        mode='max',\n        factor=0.1,\n        patience=3,\n        verbose=0\n    )\n\n    earlystop = tf.keras.callbacks.EarlyStopping(\n        monitor= metric,\n        mode='max',\n        patience=10, \n        verbose=1\n    )\n    \n    callbacks = [checkpoint, reducelr, earlystop]         \n    \n    return callbacks","d4b5fe3a":"files_ls = tf.io.gfile.glob(TRAIN_PATH + \"*.jpg\" )\nfiles_df = pd.DataFrame(files_ls, columns = [\"filepath\"])\n\nlabels = np.zeros((len(files_ls),NUM_CLASSES))\ntmp_labels = np.zeros((len(files_ls)))\n\ndef get_id(filepath):\n    tmp = filepath.split(\"\/\")[-1]\n    tmp = tmp.split(\".\")[0]\n    tmp = tmp.split(\"_\")[-1]\n    return tmp\n\nfor i in range(len(files_ls)):\n    image_id = get_id(files_ls[i])\n    label_id = train_df[train_df[\"ImageInstanceUID\"] == image_id][\"study_label\"]\n    labels[i][label_id] = 1\n    tmp_labels[i] = label_id\n    \nprint(\"Labels shape: \",labels.shape)","cd41c67b":"EPOCHS = 30\nVERBOSE = 1\nN_SPLITS = 5\n\nkfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nhistory = {}\n\n\nfor fold,(tID,vID) in enumerate(kfold.split(files_ls,tmp_labels)):\n    tFiles, tLabels = list(files_df.iloc[tID][\"filepath\"]) , labels[tID]\n    vFiles, vLabels = list(files_df.iloc[vID][\"filepath\"]) , labels[vID]\n    print(\"Number of Training Images: \",len(tID))\n    print(\"Number of Validation Images: \",len(vID))\n    \n    STEPS_PER_EPOCH  = len(tID)\/\/BATCH_SIZE\n    VALID_STEPS = len(vID)\/\/BATCH_SIZE\n    \n    tf.keras.backend.clear_session()\n    \n    train_ds = get_dataset(tFiles,tLabels, training = True)\n    val_ds = get_dataset(vFiles, vLabels, training = False)\n    \n    with strategy.scope():\n        #model = create_model()\n        model = chxnet\n        model = compile_model(model, lr=0.0001)\n        callbacks = create_callbacks(kfold = fold)\n    \n        print(\"------------------Fold - \",fold+1,\" --------------------------\")\n        history[fold] = model.fit(\n                            train_ds,\n                            epochs=EPOCHS,\n                            callbacks=callbacks,\n                            validation_data = val_ds,\n                            verbose=VERBOSE,\n                            steps_per_epoch = STEPS_PER_EPOCH,\n                            validation_steps=VALID_STEPS\n                           )","916ead55":"plt.figure(figsize=(8*N_SPLITS,24))\n\nfor i in range(N_SPLITS):\n    acc = history[i].history['acc']\n    val_acc = history[i].history['val_acc']\n    f1 = history[i].history['f1_score']\n    val_f1 = history[i].history['val_f1_score']\n    loss = history[i].history['loss']\n    val_loss = history[i].history['val_loss']\n    epochs_range = range(len(history[i].history['val_loss'])) \n    \n    plt.subplot(N_SPLITS, 3,i*3+1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation  Accuracy')\n    plt.legend(loc='lower right')\n    plt.title(f'FOLD:{str(i)} Training and Validation  Accuracy')\n    \n    plt.subplot(N_SPLITS, 3,i*3+2)\n    plt.plot(epochs_range, f1, label='Training F1 score')\n    plt.plot(epochs_range, val_f1, label='Validation  F1 score')\n    plt.legend(loc='lower right')\n    plt.title(f'FOLD:{str(i)} Training and Validation  F1 score')\n    \n    plt.subplot(N_SPLITS, 3, i*3+3)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title(f'FOLD:{str(i)} Training and Validation Loss')\n\nplt.show()","5970f561":"<h2 style = \"text-align :center; font-family:verdana; color:black; background-image: url(https:\/\/i.pinimg.com\/originals\/dd\/f9\/97\/ddf997d65d8b92a3d7e085d6bf1cc484.jpg); \">Utility Functions for Data Preprocessing<\/h2>","4255fa32":"<h2 style = \"text-align :center; font-family:verdana; color:black; background-image: url(https:\/\/i.pinimg.com\/originals\/dd\/f9\/97\/ddf997d65d8b92a3d7e085d6bf1cc484.jpg); \">Importing Dependencies<\/h2>","4818b4fc":"<h2 style = \"text-align :center; font-family:verdana; color:black; background-image: url(https:\/\/i.pinimg.com\/originals\/dd\/f9\/97\/ddf997d65d8b92a3d7e085d6bf1cc484.jpg); \">Training<\/h2>","673b79e3":"<h2 style = \"text-align :center; font-family:verdana; color:black; background-image: url(https:\/\/i.pinimg.com\/originals\/dd\/f9\/97\/ddf997d65d8b92a3d7e085d6bf1cc484.jpg); \">Model Function<\/h2>","a265ac19":"<h2 style = \"text-align :center; font-family:verdana; color:red; background-image: url(https:\/\/hookagency.com\/wp-content\/uploads\/2015\/11\/miracle-grow-light-green-gradient.jpg); \">Install Requirements<\/h2>","77acb8fa":"<h2 style = \"text-align :center; font-family:verdana; color:black; background-image: url(https:\/\/i.pinimg.com\/originals\/dd\/f9\/97\/ddf997d65d8b92a3d7e085d6bf1cc484.jpg); \">History Plotting<\/h2>","414c98e7":"<h1 style = \"text-align :center; color:white; background-image:url(https:\/\/cdn.pixabay.com\/photo\/2014\/06\/16\/23\/40\/blue-370128__340.png)\"> About Competition <\/h1>","4c0940ef":"<h2 style = \"text-align :center; font-family:verdana; color:red; background-image: url(https:\/\/hookagency.com\/wp-content\/uploads\/2015\/11\/miracle-grow-light-green-gradient.jpg); \">Checking TPU access<\/h2>","c95bf59f":"<h2 style = \"text-align :center; font-family:verdana; color:black; background-image: url(https:\/\/i.pinimg.com\/originals\/dd\/f9\/97\/ddf997d65d8b92a3d7e085d6bf1cc484.jpg); \">Model Callbacks<\/h2>","e18ada40":"<h2 style = \"text-align :center; font-family:verdana; color:black; background-image: url(https:\/\/i.pinimg.com\/originals\/dd\/f9\/97\/ddf997d65d8b92a3d7e085d6bf1cc484.jpg); \">Compiling the Model<\/h2>","0275d43e":"<h2 style=\"border-style: outset;border-color: red;text-align: center;\">SIIM-FISABIO-RSNA COVID-19 Study Level Predictions<\/h2>\n\n<img src=\"https:\/\/content.presspage.com\/uploads\/2110\/gettyimages-1214942330.jpg\" height=\"500\" width=\"500\" style=\"display: block;margin-left: auto;margin-right: auto;\"> \n\n<h2 style=\"text-align: center;border-style: double;text-align: center;border-color: red; \">About SIIM<\/h2>\n<img src=\"https:\/\/siim.org\/resource\/resmgr\/SIIM_logo-600x315.png\" width=\"200\" style=\"display: block;margin-left: auto;margin-right: auto;\">\n<p> <b>Society for Imaging Informatics in Medicine<\/b> (<a href=\"https:\/\/siim.org\/\">SIIM<\/a>) is the leading healthcare professional organization for those interested in the current and future use of informatics in medical imaging. The society's mission is to advance medical imaging informatics across the enterprise through education, research, and innovation in a multi-disciplinary community.<\/p>\n\n<h3 style = \"text-align :center; color:red; background-color: yellow; \">This is my first Data Visualization Notebook. Kindly comment if there are any mistakes \ud83d\ude42<\/h3>\n\n<a href = \"https:\/\/www.kaggle.com\/shanmukh05\/siim-covid19-dataset-256px-jpg\" style=\"font-weight:'bold'; color:blue; font-family:monospace; \"><h3>My Dataset<\/h3><\/a>\n\n<a href = \"https:\/\/www.kaggle.com\/shanmukh05\/siim-covid-19-data-preparation-for-detectron2\" style=\"font-weight:'bold'; color:blue; font-family:monospace; \"><h3>My Data Preparation Notebook<\/h3><\/a> \n\n<a href = \"https:\/\/www.kaggle.com\/shanmukh05\/siim-covid-19-yolo-v5-image-level-predictions\/output\" style=\"font-weight:'bold'; color:blue; font-family:monospace; \"><h3>YOLO v5 Image Level Training Notebook<\/h3><\/a> \n\n<a href = \"https:\/\/www.kaggle.com\/shanmukh05\/siim-covid-19-detection-detectron2-training\" style=\"font-weight:'bold'; color:blue; font-family:monospace; \"><h3>My Training Notebook<\/h3><\/a> \nTo be updated"}}