{"cell_type":{"9d0c732b":"code","82454808":"code","59a05e63":"code","8833cd27":"code","9c29b5f1":"code","0d3b6b40":"code","9fa76416":"code","2f4e4df2":"code","18980c37":"code","f5f7f7a8":"code","289336fb":"code","86f6449d":"code","ac6f59c0":"code","192d3c8c":"code","0587c3b2":"code","f5f14366":"code","8e477dfb":"code","8ea2aa1a":"code","0ad161dc":"code","e0265cb2":"code","a1bbb9f8":"code","9f15d3a5":"code","c8ea34b6":"code","878cb404":"code","beeb5024":"code","d6b8752c":"code","94ed505b":"code","e9954116":"code","bfd1154a":"markdown","77ad11f8":"markdown","b96a7e71":"markdown","126b4e94":"markdown","b5008516":"markdown","4adc9c2d":"markdown","4a480e73":"markdown","820f70ba":"markdown","68c6be90":"markdown","58920ff9":"markdown","fd48da49":"markdown","6b947a3b":"markdown","050878e6":"markdown","318d81c4":"markdown","30f79557":"markdown","d3ad20b2":"markdown","8b9f935e":"markdown","8261f262":"markdown","8436d64b":"markdown","858365a2":"markdown","1321eb68":"markdown","daf3831a":"markdown","b24afbb5":"markdown","28534816":"markdown","5df65bca":"markdown","7f2b34ee":"markdown","9bab3809":"markdown","ef9264eb":"markdown","5bee8b36":"markdown","29ad92f3":"markdown","9761b0eb":"markdown","ac560e37":"markdown","31363c35":"markdown","68b402ad":"markdown","7c086afc":"markdown","e6e76c86":"markdown","972a7517":"markdown","2db013d4":"markdown","c4dcfa41":"markdown","02c99cc0":"markdown"},"source":{"9d0c732b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #Visualization\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","82454808":"dataset = pd.read_csv('..\/input\/train.csv')\nX = dataset.iloc[:, 1:].values\ny = dataset.iloc[:, 0].values","59a05e63":"import seaborn as sns\nsns.countplot(y)","8833cd27":"# Let's see some sample images\nfig = plt.figure(figsize=(25,4))\nfig.subplots_adjust(hspace=0.5)\nfor i,index in enumerate(np.random.randint(0,100,10)):\n    ax = fig.add_subplot(2,5,i+1)\n    ax.imshow(X[index].reshape(28,28), cmap='gray')\n    ax.set_title(\"Label= {}\".format(y[index]), fontsize = 20)\n    ax.axis('off')\nplt.show()","9c29b5f1":"# Check IF some Feature variables are NaN\nnp.unique(np.isnan(X))[0]","0d3b6b40":"# Check IF some Target Variables are NaN\nnp.unique(np.isnan(y))[0]","9fa76416":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)","2f4e4df2":"# Normalization\nX_train = X_train \/ 255.0\nX_test = X_test \/ 255.0","18980c37":"import torch\nimport torch.utils.data\nfrom torch.autograd import Variable","f5f7f7a8":"'''Create tensors for our train and test set. \nAs you remember we need variable to accumulate gradients. \nTherefore first we create tensor, then we will create variable '''\n# Numpy to Tensor Conversion (Train Set)\nX_train = torch.from_numpy(X_train)\ny_train = torch.from_numpy(y_train)\n\n# Numpy to Tensor Conversion (Train Set)\nX_test = torch.from_numpy(X_test)\ny_test = torch.from_numpy(y_test)","289336fb":"# Make torch datasets from train and test sets\ntrain = torch.utils.data.TensorDataset(X_train,y_train)\ntest = torch.utils.data.TensorDataset(X_test,y_test)\n\n# Create train and test data loaders\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = 64, shuffle = True)\ntest_loader = torch.utils.data.DataLoader(test, batch_size = 64, shuffle = True)","86f6449d":"import torch.nn as nn\nimport torch.nn.functional as F","ac6f59c0":"class ANN(nn.Module):\n    def __init__(self, input_dim = 784, output_dim = 10):\n        super(ANN, self).__init__()\n    \n        # Input Layer (784) -> 784\n        self.fc1 = nn.Linear(input_dim, 784)\n        # 784 -> 128\n        self.fc2 = nn.Linear(784, 128)\n        # 128 -> 128\n        self.fc3 = nn.Linear(128, 128)\n        # 128 -> 64\n        self.fc4 = nn.Linear(128, 64)\n        # 64 -> 64\n        self.fc5 = nn.Linear(64, 64)\n        # 64 -> 32\n        self.fc6 = nn.Linear(64, 32)\n        # 32 -> 32\n        self.fc7 = nn.Linear(32, 32)\n        # 32 -> output layer(10)\n        self.output_layer = nn.Linear(32,10)\n        # Dropout Layer (20%) to reduce overfitting\n        self.dropout = nn.Dropout(0.2)\n    \n    # Feed Forward Function\n    def forward(self, x):\n        \n        # flatten image input\n        x = x.view(-1, 28 * 28)\n        \n        # Add ReLU activation function to each layer\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        # Add dropout layer\n        x = self.dropout(x)\n        x = F.relu(self.fc4(x))\n        x = F.relu(self.fc5(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc6(x))\n        x = F.relu(self.fc7(x))\n        x = self.dropout(x)\n        # Don't add any ReLU activation function to Last Output Layer\n        x = self.output_layer(x)\n        \n        # Return the created model\n        return x","192d3c8c":"# Create the Neural Network Model\nmodel = ANN(input_dim = 784, output_dim = 10)\n# Print its architecture\nprint(model)","0587c3b2":"import torch.optim as optim\n# specify loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# specify optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay= 1e-6, momentum = 0.9,nesterov = True)","f5f14366":"# Define epochs (between 20-50)\nepochs = 30\n\n# initialize tracker for minimum validation loss\nvalid_loss_min = np.Inf # set initial \"min\" to infinity\n\n# Some lists to keep track of loss and accuracy during each epoch\nepoch_list = []\ntrain_loss_list = []\nval_loss_list = []\ntrain_acc_list = []\nval_acc_list = []\n\n\n\n# Start epochs\nfor epoch in range(epochs):\n    # monitor training loss\n    train_loss = 0.0\n    val_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    # Set the training mode ON -> Activate Dropout Layers\n    model.train() # prepare model for training\n    # Calculate Accuracy         \n    correct = 0\n    total = 0\n    \n    # Load Train Images with Labels(Targets)\n    for data, target in train_loader:\n        \n        # Convert our images and labels to Variables to accumulate Gradients\n        data = Variable(data).float()\n        target = Variable(target).type(torch.LongTensor)\n        \n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        \n        # Calculate Training Accuracy \n        predicted = torch.max(output.data, 1)[1]        \n        # Total number of labels\n        total += len(target)\n        # Total correct predictions\n        correct += (predicted == target).sum()\n        \n        # calculate the loss\n        loss = loss_fn(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update running training loss\n        train_loss += loss.item()*data.size(0)\n    \n    # calculate average training loss over an epoch\n    train_loss = train_loss\/len(train_loader.dataset)\n    \n    # Avg Accuracy\n    accuracy = 100 * correct \/ float(total)\n    \n    # Put them in their list\n    train_acc_list.append(accuracy)\n    train_loss_list.append(train_loss)\n    \n        \n    # Implement Validation like K-fold Cross-validation \n    # Set Evaluation Mode ON -> Turn Off Dropout\n    model.eval() # Required for Evaluation\/Test\n\n    # Calculate Test\/Validation Accuracy         \n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n\n            # Convert our images and labels to Variables to accumulate Gradients\n            data = Variable(data).float()\n            target = Variable(target).type(torch.LongTensor)\n\n            # Predict Output\n            output = model(data)\n\n            # Calculate Loss\n            loss = loss_fn(output, target)\n            val_loss += loss.item()*data.size(0)\n            # Get predictions from the maximum value\n            predicted = torch.max(output.data, 1)[1]\n\n            # Total number of labels\n            total += len(target)\n\n            # Total correct predictions\n            correct += (predicted == target).sum()\n    \n    # calculate average training loss and accuracy over an epoch\n    val_loss = val_loss\/len(test_loader.dataset)\n    accuracy = 100 * correct\/ float(total)\n    \n    # Put them in their list\n    val_acc_list.append(accuracy)\n    val_loss_list.append(val_loss)\n    \n    # Print the Epoch and Training Loss Details with Validation Accuracy   \n    print('Epoch: {} \\tTraining Loss: {:.4f}\\t Val. acc: {:.2f}%'.format(\n        epoch+1, \n        train_loss,\n        accuracy\n        ))\n    # save model if validation loss has decreased\n    if val_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        val_loss))\n        torch.save(model.state_dict(), 'model.pt')\n        valid_loss_min = val_loss\n    # Move to next epoch\n    epoch_list.append(epoch + 1)","8e477dfb":"model.load_state_dict(torch.load('model.pt'))","8ea2aa1a":"plt.plot(epoch_list,train_loss_list)\nplt.plot(val_loss_list)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss vs Number of Epochs\")\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.show()","0ad161dc":"plt.plot(epoch_list,train_acc_list)\nplt.plot(val_acc_list)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Training Accuracy\")\nplt.title(\"Accuracy vs Number of Epochs\")\nplt.legend(['Train', 'Test'], loc='best')\nplt.show()","e0265cb2":"val_acc = sum(val_acc_list[20:]).item()\/10\nprint(\"Test Accuracy of model = {} %\".format(val_acc))","a1bbb9f8":"kaggle_test_set = pd.read_csv('..\/input\/test.csv')\n\n# Convert it to numpy array and Normalize it\nkaggle_test_set = kaggle_test_set.values\/255.0","9f15d3a5":"kaggle_test_set = Variable(torch.from_numpy(kaggle_test_set)).float()","c8ea34b6":"# Predicted Labels will be stored here\nresults = []\n\n# Set Evaluation Mode ON -> Turn Off Dropout\nmodel.eval() # Required for Evaluation\/Test\n\nwith torch.no_grad():\n    for image in kaggle_test_set:\n        output = model(image)\n        pred = torch.max(output.data, 1)[1]\n        results.append(pred[0].numpy())","878cb404":"# Convert List to Numpy Array\nresults = np.array(results)","beeb5024":"# Plot using Matplotlib\nfig = plt.figure(figsize=(25,4))\nfig.subplots_adjust(hspace=0.5)\nfor i,index in enumerate(np.random.randint(0,100,10)):\n    ax = fig.add_subplot(2,5,i+1)\n    ax.imshow(kaggle_test_set[index].reshape(28,28), cmap='gray')\n    ax.set_title(\"Label= {}\".format(results[index]), fontsize = 20)\n    ax.axis('off')\nplt.show()","d6b8752c":"results = pd.Series(results,name=\"Label\")","94ed505b":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"submission.csv\",index=False)","e9954116":"submission.head()","bfd1154a":"**Convert our Results Numpy Array to Pandas Series**","77ad11f8":"# 4.  Performance Evaluation","b96a7e71":"Define any desired architecture with feed-forward function","126b4e94":"### Import required libraries","b5008516":"### Import Training data as Numpy array","4adc9c2d":"### Splitting Dataset into Training set and Test set","4a480e73":"### Architect our Neural Network","820f70ba":"### Create the Model","68c6be90":"## Key Features Of PyTorch\n* HYBRID FRONT-END\n* DISTRIBUTED TRAINING\n* PYTHON-FIRST\n* ADVANCED TOOLS & LIBRARIES\n* NATIVE ONNX SUPPORT  (Open Neural Network Exchange Support) \n* VERY FAST PRODUCTION READY DEPLOYMENT WITH C++\n\nReference: [click here](https:\/\/pytorch.org\/features)","58920ff9":"Since all our target classes are well-balanced we can move to our next step.","fd48da49":"### Visualize Training Stats","6b947a3b":"### Define Loss Function and Optimizer\nHere we've used Cross Entropy Loss Function and SGD optimizer. Feel free to experiment with other hyperparameters.","050878e6":"### Build Train and Test Data loaders","318d81c4":"### Validation\/Test Accuracy\nHere we're taking average of last 10 validation accuracies","30f79557":"## What is PyTorch?\n> A NEW WAY FROM RESEARCH TO PRODUCTION\n\nPyTorch is an open source deep learning platform that provides a seamless path from research prototyping to production deployment.","d3ad20b2":"### Visualiza some Test Images and their Predicted Labels","8b9f935e":"### Predict Labels\/Targets for Test Images","8261f262":"**From the visualization we can see that our model performs really Good !!!**","8436d64b":"# Introduction Artificial Neural Networks using PyTorch\n\n*15 November 2018*  \n\n#### ***[Soumya Ranjan Behera](https:\/\/www.linkedin.com\/in\/soumya044)***\n\n### In this Kernel I have demonstrated a beginner's approach of implementing an  Artificial Neural Network (ANN) using [PyTorch](https:\/\/pytorch.org\/) to classify the digits into their respective categories which have scored **97.7 %** Accuracy in the Digit Recognizer Competition.\n\n### Goals of this Kenel:  \n* To demonstrate a new Deep Learning framework **[PyTorch](https:\/\/pytorch.org\/)**\n* To provide a basic implementation of Artificial Neural Network (ANN)\n* A beginner friendly kernel to show a procedure to compete in Kaggle Digit Recognizer Competition","858365a2":"Since no NULL or NaN values are there, we're good to go!","1321eb68":"### Check for Null Values","daf3831a":"# Thank You  \n\nIf you liked this kernel please **Upvote**. Don't forget to drop a comment or suggestion.  \n\n### *Soumya Ranjan Behera*\nLet's stay Connected! [LinkedIn](https:\/\/www.linkedin.com\/in\/soumya044)  \n\n**Happy Coding !**","b24afbb5":"**Note:** In this kernel I have provided only the implementation of ANN using PyTorch. For basics of ANN or PyTorch please refer to this free course [Intro to Deep Learning using PyTorch](https:\/\/in.udacity.com\/course\/deep-learning-pytorch--ud188) ","28534816":"## Dataset Overview\n ``` \n ..\/input\/\n     |_ train.csv  \n     |_ test.csv  \n     |_ sample_submission.csv```  \n     \n* ```train.csv``` contains 42k samples of 28x28 digit images with their labels\n* ```test.csv``` contains 28k samples of 28x28 digit images without labels\n* We've to predict the labels for the ```test``` samples and submit them in a csv file as shown  in ```sample_submission.csv```\n","5df65bca":"# 3. Train our Model with simultaneous Validation","7f2b34ee":"### Normalization\nThe goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information. Here we're dividing all the pixel values with 255.0 to make all pixel values lie in between 0 and 1.","9bab3809":"### Visualization of our Data (Sample Images)","ef9264eb":"**Avg. Accuracy VS Number of Epochs Graph**","5bee8b36":"# 2. Building our ANN using PyTorch","29ad92f3":"## Load the Model with the Lowest Validation Loss","9761b0eb":"**Add an ' ImageId ' Column and Save as CSV file**","ac560e37":"# 5. Prepare Final Submission","31363c35":"### Let's make our Final Submission CSV file","68b402ad":"**Just Check our format to ensure correctness**","7c086afc":"# 1. Prepare our Data\n\n### Import Numpy, Pandas and Matplotlib","e6e76c86":"### Convert Numpy Arrays to Tensors \nHere we have to convert our train and test data sets (which are in numpy array format) to tensors. Because PyTorch only takes tensors as input.","972a7517":"### Import test data","2db013d4":"### Convert our Test Data to Tensor Variable","c4dcfa41":"### Let's see the distribution of data","02c99cc0":"**Average Loss VS Number of Epochs Graph**"}}