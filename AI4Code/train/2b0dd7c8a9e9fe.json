{"cell_type":{"4bc2ea8d":"code","544a5b0e":"code","e77a329a":"code","c5988664":"code","22f343cd":"code","b728984b":"code","510a2373":"code","4390deba":"code","8d4fd58e":"code","2e46779f":"code","99f9243b":"markdown","ece20410":"markdown","4d2c0828":"markdown","3810931a":"markdown","61406154":"markdown","3d8cdd03":"markdown"},"source":{"4bc2ea8d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","544a5b0e":"# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Set a random seed\nimport random\nrandom.seed(42)","e77a329a":"# Load the dataset\ndata = pd.read_csv('..\/input\/titantic-data\/titanic_data.csv')\n\n# Print the first few entries of the Titanic data\ndata.head()","c5988664":"# Store 'Survived' feature into a new variable and remove it from the dataset\noutcomes = data['Survived']\nfeatures = data.drop('Survived', axis=1)","22f343cd":"features_encoded = pd.get_dummies(features)","b728984b":"features_encoded = features_encoded.fillna(0.0)\nfeatures_encoded.head()","510a2373":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features_encoded, outcomes, test_size=0.2, random_state=42)","4390deba":"# Import the classifier from sklearn\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Define the classifier, and fit it to the data\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)","8d4fd58e":"# Making Predictions\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# Calculate the accuracy\nfrom sklearn.metrics import accuracy_score\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint('The training accuracy is', train_accuracy)\nprint('The test accuracy is', test_accuracy)","2e46779f":"from sklearn.model_selection import GridSearchCV\n\nmodel = DecisionTreeClassifier(random_state=42)\n\nparams = {\n    'max_depth': range(2,11),\n    'min_samples_leaf': range(2,11),\n    'min_samples_split': range(2,11)\n}\n\ngrid_search = GridSearchCV(model, params)\ngrid_search.fit(X_train, y_train)\n\nprint(grid_search.best_params_)\n\n# Make predictions\n# predict() on GridSearchCV picks the best model\ny_train_pred = grid_search.predict(X_train)\ny_test_pred = grid_search.predict(X_test)\n\n# Calculate the accuracy\nfrom sklearn.metrics import accuracy_score\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint('The training accuracy is', train_accuracy)\nprint('The test accuracy is', test_accuracy)","99f9243b":"# Testing the model\nWe will calculate the accuracy over both training and testing set.","ece20410":"# Training the Model\n\n First, I'll split the data into training and testing sets.\n Then I'll train the model on the training set.","4d2c0828":"# Titanic Survival Exploration with Decision Trees\n![ship-3401500_640.jpg](attachment:ship-3401500_640.jpg)\n- **Survived**: Outcome of survival (0 = No; 1 = Yes)\n- **Pclass**: Socio-economic class (1 = Upper class; 2 = Middle class; 3 = Lower class)\n- **Name**: Name of passenger\n- **Sex**: Sex of the passenger\n- **Age**: Age of the passenger (Some entries contain `NaN`)\n- **SibSp**: Number of siblings and spouses of the passenger aboard\n- **Parch**: Number of parents and children of the passenger aboard\n- **Ticket**: Ticket number of the passenger\n- **Fare**: Fare paid by the passenger\n- **Cabin** Cabin number of the passenger (Some entries contain `NaN`)\n- **Embarked**: Port of embarkation of the passenger (C = Cherbourg; Q = Queenstown; S = Southampton)\n","3810931a":"From this, we can see that we are overfitting as we have high training and low testing accuracy.","61406154":"Since we're interested in the outcome of survival for each passenger or crew member, we can remove the **Survived** feature from this dataset and store it as its own separate variable `outcomes`. We will use these outcomes as our prediction targets.  \nRun the code cell below to remove **Survived** as a feature of the dataset and store it in `outcomes`.","3d8cdd03":"# Preprocessing the data - one-hot encoding\nWe will use one-hot encoding technique and convert categorical into continous**"}}