{"cell_type":{"2175ecb8":"code","25d4317f":"code","95a51022":"code","afb0bea5":"code","ae826f61":"code","2f65a30f":"code","fe922938":"code","d1f44e8a":"code","ce7d524f":"code","08212aa4":"code","751ec474":"code","14b25259":"code","0387efcf":"code","7bdc236f":"code","53d33a7c":"code","f016b472":"code","961d9473":"markdown","789e45c6":"markdown","1b4ef55a":"markdown","c9de8d74":"markdown","a8a66678":"markdown","947edf21":"markdown","0ab5fbe9":"markdown","568cda58":"markdown","538a141b":"markdown","77809e0b":"markdown","5624dd54":"markdown","0aa75cb7":"markdown","17e4cb21":"markdown"},"source":{"2175ecb8":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm_notebook\nimport pickle\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom tqdm import tqdm_notebook\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","25d4317f":"import warnings\nwarnings.filterwarnings(\"ignore\")","95a51022":"train_data = pd.read_csv('..\/input\/train_features.csv', index_col='match_id_hash')\ntest_data = pd.read_csv('..\/input\/test_features.csv', index_col='match_id_hash')\ny_train = pd.read_csv('..\/input\/train_targets.csv', index_col='match_id_hash')['radiant_win'].map({True: 1, False:0})","afb0bea5":"train_data.head()","ae826f61":"def hero_dammies(X_train, X_test, let):\n    r_cols = [let +'%s_hero_id' %i for i in range(1, 6)]\n    X = pd.concat([X_train, X_test])\n    X['herois'+ let] = X.apply(lambda row: ' '.join(row.loc[r_cols].map(int).map(str)), axis=1)\n    cvv = CountVectorizer()\n    heroes = pd.DataFrame(cvv.fit_transform(X['herois'+let]).todense(), columns=cvv.get_feature_names(), index=X.index)\n    return heroes.loc[X_train.index], heroes.loc[X_test.index]","2f65a30f":"train_r, test_r = hero_dammies(train_data, test_data, 'r')\ntrain_d, test_d = hero_dammies(train_data, test_data, 'd')","fe922938":"X_train = train_data[['game_time', 'game_mode', 'lobby_type', 'objectives_len', 'chat_len']].copy()\n\nmean_cols = ['kills','deaths', 'assists','denies','gold', 'lh','xp','health','max_health','max_mana', 'level', 'stuns','creeps_stacked', 'camps_stacked', 'rune_pickups', 'firstblood_claimed','teamfight_participation',\n'towers_killed', 'roshans_killed', 'obs_placed', 'sen_placed']\n\nfor col in tqdm_notebook(mean_cols):\n    for let in ['r','d']:\n        full_cols = [x+col for x in [let +'%s_' %i for i in range(1, 6)]]\n        X_train[let+col+'_mean'] = train_data[full_cols].apply(np.mean, axis=1)\n        X_train[let+col+'_max'] = train_data[full_cols].apply(np.max, axis=1)\n        X_train[let+col+'_min'] = train_data[full_cols].apply(np.min, axis=1)\n\nX_test = test_data[['game_time', 'game_mode', 'lobby_type', 'objectives_len', 'chat_len']].copy()\nfor col in tqdm_notebook(mean_cols):\n    for let in ['r','d']:\n        full_cols = [x+col for x in [let +'%s_' %i for i in range(1, 6)]]\n        X_test[let+col+'_mean'] = test_data[full_cols].apply(np.mean, axis=1)\n        X_test[let+col+'_max'] = test_data[full_cols].apply(np.max, axis=1)\n        X_test[let+col+'_min'] = test_data[full_cols].apply(np.min, axis=1)","d1f44e8a":"X_train = X_train.join(train_r, rsuffix='_r').join(train_d, rsuffix='_d')\n\nX_test = X_test.join(test_r, rsuffix='_r').join(test_d, rsuffix='_d')","ce7d524f":"import lightgbm\nfrom bayes_opt import BayesianOptimization","08212aa4":"def lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples, min_data_in_leaf):\n    params = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\", \n    'is_unbalance': True,\n    \"num_leaves\" : int(num_leaves),\n    \"max_depth\" : int(max_depth),\n    \"lambda_l2\" : lambda_l2,\n    \"lambda_l1\" : lambda_l1,\n    \"num_threads\" : 20,\n    \"min_child_samples\" : int(min_child_samples),\n    'min_data_in_leaf': int(min_data_in_leaf),\n    \"learning_rate\" : 0.03,\n    \"subsample_freq\" : 5,\n    \"bagging_seed\" : 42,\n    \"verbosity\" : -1\n    }\n    \n    lgtrain = lightgbm.Dataset(X_train, y_train,categorical_feature=categorical_features)\n    cv_result = lightgbm.cv(params,\n                       lgtrain,\n                       10000,\n                       early_stopping_rounds=300,\n                       stratified=True,\n                       nfold=5)\n    return cv_result['auc-mean'][-1]\n\ndef lgb_train(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples, min_data_in_leaf):\n    params = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\", \n    'is_unbalance': True,\n    \"num_leaves\" : int(num_leaves),\n    \"max_depth\" : int(max_depth),\n    \"lambda_l2\" : lambda_l2,\n    \"lambda_l1\" : lambda_l1,\n    \"num_threads\" : 20,\n    \"min_child_samples\" : int(min_child_samples),\n    'min_data_in_leaf': int(min_data_in_leaf),\n    \"learning_rate\" : 0.03,\n    \"subsample_freq\" : 5,\n    \"bagging_seed\" : 42,\n    \"verbosity\" : -1\n    }\n    t_x,v_x,t_y,v_y = train_test_split(X_train, y_train,test_size=0.2)\n    lgtrain = lightgbm.Dataset(t_x, t_y,categorical_feature=categorical_features)\n    lgvalid = lightgbm.Dataset(v_x, v_y,categorical_feature=categorical_features)\n    model = lightgbm.train(params, lgtrain, 4000, valid_sets=[lgvalid], early_stopping_rounds=400, verbose_eval=200)\n    pred_test_y = model.predict(X_test, num_iteration=model.best_iteration)\n    return pred_test_y, model\n    \ndef param_tuning(init_points,num_iter,**args):\n    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (15, 200),\n                                                'max_depth': (5, 63),\n                                                'lambda_l2': (0.0, 5),\n                                                'lambda_l1': (0.0, 5),\n                                                'min_child_samples': (50, 5000),\n                                                'min_data_in_leaf': (50, 300)\n                                                })\n\n    lgbBO.maximize(init_points=init_points, n_iter=num_iter,**args)\n    return lgbBO","751ec474":"categorical_features= ['lobby_type', 'game_mode']","14b25259":"result = param_tuning(10,50)","0387efcf":"params = result.max['params']\nparams","7bdc236f":"pred_test_y1, _ = lgb_train(**params)\npred_test_y2, _ = lgb_train(**params)\npred_test_y3, model = lgb_train(**params)\ny_pred = (pred_test_y1 + pred_test_y2 + pred_test_y3)\/3","53d33a7c":"feature_importance = pd.DataFrame({'feature': X_train.columns, 'importance':model.feature_importance()}).sort_values('importance', ascending=False)[:100]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=feature_importance.importance, y=feature_importance.feature);","f016b472":"df_submission_extended = pd.DataFrame(\n    {'radiant_win_prob': y_pred}, \n    index=test_data.index)\ndf_submission_extended.to_csv('submission.csv')","961d9473":"The main thing is gold ;)","789e45c6":"We run learning for 3 times and take the mean value of predictions.","1b4ef55a":"We have fields for each hero. Hmmm. I want to aggregate them","c9de8d74":"This is the function to make one_hot_encoding for heroes in whole team","a8a66678":"Let's do it! We get heroes for Radiant and Dire team in separated tables","947edf21":"It is just a beginning! ","0ab5fbe9":"### Make team features","568cda58":"Now we want to find a way to count aggregate features for teams. Let's try to find the way to do this. I think it looks like good to use mean, max and min for each parameter.","538a141b":"This notebook is about two things:<br>\n1. We have different fields for ordered list of characters. In fact, the order os heroes is not importent. So I want to make features for team, deleting the order\n2. I will tune parameters for LightGBM with the help of baesian optimisation. Very pretty way\n\nThere is no real feature engeneering here. It is coming soon...","77809e0b":"We can find the way to choose parameters in docs:\n\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html\n\nThanks to this kernel: https:\/\/www.kaggle.com\/qwe1398775315\/eda-lgbm-bayesianoptimization","5624dd54":"### Parameters tuning for LightGBM","0aa75cb7":"### Load the data","17e4cb21":"Last step - we gather all this data to dataset"}}