{"cell_type":{"56122746":"code","44212528":"code","8dad99e3":"code","9d7f2e58":"code","7ec6f86c":"code","6bfd3708":"code","f53138a3":"code","a1282e1f":"code","a3bb5134":"code","b88c3ffb":"code","c21233a8":"code","b9395ac9":"code","e940a297":"code","56a57ebc":"code","05b2660d":"code","90e0ae1a":"code","18866b13":"code","64efc0c4":"code","822162fd":"code","084895ff":"code","f7551fc2":"code","9b03b5a7":"code","1e13d5a4":"code","087236b1":"code","c27a3903":"code","e5d614f0":"code","7de3cb37":"code","b91e7e93":"code","ac759820":"code","d76c9510":"code","ebf2faf3":"code","cdf03d02":"code","2a903d73":"code","fded56d3":"code","3a7ffe38":"code","eaac4d7c":"code","8300b834":"code","5f6a73ac":"code","89f4928b":"code","71990f7e":"code","3d84bfdd":"code","379b4619":"code","d61344ce":"code","f72a4cff":"code","e5506e6b":"code","48a690d9":"code","fb0588e2":"code","3ffae53c":"code","7dba09e8":"code","10bd7901":"code","a7b62a7f":"code","b99c8abb":"code","c9c81f9d":"code","3a9cc8ff":"markdown","12d28c19":"markdown","dd61a90a":"markdown","5e11c005":"markdown","348d3262":"markdown","71d833b6":"markdown","e807a388":"markdown","66162313":"markdown","bdd34487":"markdown","495c79d1":"markdown","4da1d026":"markdown","fb5fd1f5":"markdown","069f2fde":"markdown","e651d45a":"markdown","a4f4fe11":"markdown","f9cc2510":"markdown","f0e8ff26":"markdown","edefcaf9":"markdown","b9eb0710":"markdown","ced37ffc":"markdown","41c62a88":"markdown","1edb8752":"markdown","80c5e549":"markdown","68b65044":"markdown","dd84636d":"markdown"},"source":{"56122746":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","44212528":"# Carregando os dados do desafio\ntrain = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/test.csv')\n\nprint('Tamanho dos datasets Train e Test:')\nprint(' - Train - Linhas:', train.shape[0],'Colunas:', train.shape[1],'\\n - Test - Linhas:', test.shape[0],'Colunas:', test.shape[1])","8dad99e3":"# Juntando os dataframes - train e test\ndf_all = train.append(test)\n\nprint('Informa\u00e7\u00f5es do novo dataset:\\n df_all - Linhas:', df_all.shape[0],'Colunas:', df_all.shape[1])","9d7f2e58":"# Quais colunas do dataframe s\u00e3o do tipo object\n# Importante passo para saber se alguma coluna tem dados (linhas) \n#    com tipo diferente\ndf_all.select_dtypes('object').head()","7ec6f86c":"# Analisando os dados da coluna edjefa\ndf_all['edjefa'].value_counts()","6bfd3708":"# Analisando os dados da coluna edjefe\ndf_all['edjefe'].value_counts()","f53138a3":"# Analisando os dados da coluna dependency\ndf_all['dependency'].value_counts()","a1282e1f":"# Fun\u00e7\u00e3o para excecutar troca dos valores yes e no, para 1 e 0, respectivamente\nmapeamento = {'yes': 1, 'no': 0}","a3bb5134":"# Transforma\u00e7\u00e3o dos dados das colunas edjefa e edjefe\ndf_all['edjefa'] = df_all['edjefa'].replace(mapeamento).astype(int)\ndf_all['edjefe'] = df_all['edjefe'].replace(mapeamento).astype(int)","b88c3ffb":"# Quais colunas do dataframe s\u00e3o do tipo object\ndf_all.select_dtypes('object').head()","c21233a8":"# Olhando a coluna dependency\ndf_all['dependency'].value_counts()","b9395ac9":"# Verificando as colunas dependency e SQBdependency, pois uma \u00e9 baseada na outra.\ndf_all[['dependency','SQBdependency']]","e940a297":"# Preenchimento dos valores de dependency \n#  com os valores da raiz quadrada de SQBdependency\ndf_all['dependency'] = np.sqrt(df_all['SQBdependency'])","56a57ebc":"# Quais colunas do dataframe s\u00e3o do tipo object\ndf_all.select_dtypes('object').head()","05b2660d":"# Verificando os valores nulos\ndf_all.isnull().sum().sort_values()","90e0ae1a":"# Tratamento dos valores nulos nas vari\u00e1veis: v2a1, v18q1 e rez_esc\n#df_all['v2a1'].fillna(0, inplace=True)\n#df_all['v18q1'].fillna(0, inplace=True)\n#df_all['rez_esc'].fillna(0, inplace=True)\ndf_all['v2a1'] = df_all['v2a1'].fillna(value=df_all['tipovivi3'])\ndf_all['v18q1'] = df_all['v18q1'].fillna(value=df_all['v18q'])\ndf_all['rez_esc'].fillna(0, inplace=True)","18866b13":"# Verificando os valores nulos restantes\ndf_all.isnull().sum().sort_values()","64efc0c4":"# Prenchendo com 0 todos os valores nulos\ndf_all['meaneduc'].fillna(0, inplace=True)\ndf_all['SQBmeaned'].fillna(0, inplace=True)","822162fd":"df_all['Target'].fillna(-1, inplace=True)","084895ff":"# Carregando as bibliotecas utilizadas para os modelos\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, f1_score, make_scorer","f7551fc2":"# Separando as colunas para treinamento\nfeats = [c for c in df_all.columns if c not in ['Id', 'idhogar', 'Target']]","9b03b5a7":"# Separar os dataframes\ntrain, test = df_all[df_all['Target'] != -1], df_all[df_all['Target'] == -1]","1e13d5a4":"# Instanciando RadomForestClassifier\nrf = RandomForestClassifier(n_jobs=-1, n_estimators=200, oob_score=True, random_state=42)","087236b1":"# Treinando o modelo\nrf.fit(train[feats], train['Target'])","c27a3903":"# Prever o Target de teste usando o modelo treinado\ntest['Target'] = rf.predict(test[feats]).astype(int)","e5d614f0":"# Vamos verificar as previs\u00f5es\ntest['Target'].value_counts(normalize=True)","7de3cb37":"# Criando o arquivo para submiss\u00e3o\n#test[['Id', 'Target']].to_csv('submission.csv', index=False)","b91e7e93":"# Feature Engineering\n\n# Vamos criar novas colunas para valores percapita\ndf_all['hsize-pc'] = df_all['hhsize'] \/ df_all['tamviv']\ndf_all['phone-pc'] = df_all['qmobilephone'] \/ df_all['tamviv']\ndf_all['tablets-pc'] = df_all['v18q1'] \/ df_all['tamviv']\ndf_all['rooms-pc'] = df_all['rooms'] \/ df_all['tamviv']\ndf_all['rent-pc'] = df_all['v2a1'] \/ df_all['tamviv']","ac759820":"# Separar os dataframes\ntrain, test = df_all[df_all['Target'] != -1], df_all[df_all['Target'] == -1]","d76c9510":"# Utilizando matriz de correspond\u00eancia para verificar se algumas vari\u00e1veis podem ser retiradas do modelo\n\n# Create correlation matrix\ncorr = df_all.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.975\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.975)]\n\nprint(f'There are {len(to_drop)} correlated columns to remove.')\nprint(to_drop)","ebf2faf3":"# Removendo algumas colunas que tem basicamente o mesmo valor para o modelo e\/ou tenha alta correla\u00e7\u00e3o\n# r4t3, tamviv, tamhog, hhsize = hogar_total\n# v18q, mobilephone = v18q1, qmobilephone\n# v14a = saniatrio1\n# male oposto do female. Retirando female.\n# area1 oposto da area2\n\nretira_cols = ['agesq', 'area2', 'hogar_total', 'male', 'public', 'r4t3', 'tamhog', 'tamviv', \n               'hhsize', 'v18q', 'v14a', 'mobilephone', 'female']\n\n\nparentesco_cols = [colunas for colunas in train.columns.tolist() if 'parentesco' in colunas and 'parentesco1' not in colunas]\n\nretira_cols.extend(parentesco_cols)\n\ntrain = train.drop(retira_cols, axis=1)\ntest = test.drop(retira_cols, axis=1)\ndf_all = df_all.drop(retira_cols, axis=1)","cdf03d02":"# Features para treinamento\nfeats = [c for c in df_all.columns if c not in ['Id', 'idhogar', 'Target']]","2a903d73":"# Limitando o treinamento ao chefe da familia\n\n# Criando um novo dataframe para treinar\nheads = train[train['parentesco1'] == 1]","fded56d3":"# Criando um novo modelo\nrf2 = RandomForestClassifier(n_jobs=-1, n_estimators=200, oob_score=True, random_state=42)","3a7ffe38":"# Treinando o modelo\nrf2.fit(train[feats], train['Target'])","eaac4d7c":"# Prever o Target de teste usando o modelo treinado\ntest['Target'] = rf2.predict(test[feats]).astype(int)","8300b834":"test['Target'].value_counts(normalize=True)","5f6a73ac":"# Criando o arquivo para submiss\u00e3o\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","89f4928b":"# Juntando as abordagens\nheads2 = train[train['parentesco1'] == 1]","71990f7e":"feats = [c for c in df_all.columns if c not in ['Id', 'idhogar', 'Target']]","3d84bfdd":"# Criando um novo modelo\nrf3 = RandomForestClassifier(n_jobs=-1, n_estimators=200, oob_score=True, random_state=42)","379b4619":"# Treinando o modelo\nrf3.fit(heads2[feats], heads2['Target'])","d61344ce":"# Prevendo usando o modelo treinado\ntest['Target'] = rf3.predict(test[feats]).astype(int)","f72a4cff":"# Criando o arquivo para submiss\u00e3o\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","e5506e6b":"# Feature Importance\npd.Series(rf3.feature_importances_, index=feats).sort_values().plot.barh(figsize = (20, 40));","48a690d9":"# Copiando do campe\u00e3o\nrf4 = RandomForestClassifier(max_depth=None, random_state=42, n_jobs=4, n_estimators=700,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2,\n                            verbose=0, class_weight='balanced')","fb0588e2":"# Treinando o modelo\nrf4.fit(heads2[feats], heads2['Target'])","3ffae53c":"# Prevendo usando o modelo treinado\ntest['Target'] = rf4.predict(test[feats]).astype(int)","7dba09e8":"# Feature Importance\npd.Series(rf4.feature_importances_, index=feats).sort_values().plot.barh(figsize = (20, 40));","10bd7901":"# Criando o arquivo para submiss\u00e3o\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","a7b62a7f":"# Divis\u00e3o dos datasets para poder fazer a matriz de confus\u00e3o.\n# Obtens\u00e3o do f1_score tamb\u00e9m\n\nimport lightgbm as lgb\n\ntrain_y = train['Target']\n\nX_train, X_test, y_train, y_test = train_test_split(train[feats], train_y, test_size=0.2, random_state=42)\n\nF1_scorer = make_scorer(f1_score, greater_is_better=True, average='macro')\n\ngbm = lgb.LGBMClassifier(boosting_type='dart', objective='multiclassova', class_weight='balanced', random_state=0)\n\ngbm.fit(X_train, y_train)","b99c8abb":"import lightgbm as lgb\n\ntrain_y = train.Target\n\nX_train, X_test, y_train, y_test = train_test_split(train[feats], train_y, test_size=0.2, random_state=42)\n\nF1_scorer = make_scorer(f1_score, greater_is_better=True, average='macro')\n\ngbm = lgb.LGBMClassifier(boosting_type='dart', objective='multiclassova', class_weight='balanced', random_state=0)\n\ngbm.fit(X_train, y_train)","c9c81f9d":"# Matriz de confus\u00e3o\ny_test_pred = gbm.predict(X_test)\ncm = confusion_matrix(y_test, y_test_pred)\nf1 = f1_score(y_test, y_test_pred, average='macro')\nprint(\"confusion matrix: \\n\", cm)\nprint(\"macro F1 score: \\n\", f1)","3a9cc8ff":"Ap\u00f3s execu\u00e7\u00e3o, espera que apenas a coluna dependency ainda seja object","12d28c19":"Como temos poucos dados e iremos atribuir 0 para as colunas meaneduc e SQBmeaned","dd61a90a":"# Tratamento dos dados","5e11c005":"Passo 1. Verifica\u00e7\u00e3o do dataset para validar se os dados inseridos s\u00e3o os dados esperados pelo dataset. Caso seja esperado dado n\u00famerico e o campo for object, esses registros precisam ser tratados.","348d3262":"Para altera\u00e7\u00e3o dos registros, ser\u00e1 utilizada a fun\u00e7\u00e3o abaixo, onde:\n* yes ser\u00e1 1\n* no ser\u00e1 0","71d833b6":"# Modelo 2","e807a388":"Ap\u00f3s an\u00e1lise dos dados das colunas edjefa, edjefe e dependency, temos:\n\nPara a coluna edjefa, temos 214: yes e 22075: no\n\nPara a coluna edjefe, temos 416: yes e 12818: no\n\nPara a coluna dependecy, temos 7580: yes e 6036: no\n","66162313":"# Modelo 4 - Melhor","bdd34487":"# Importa\u00e7\u00f5es iniciais\n","495c79d1":"# Modelo 1","4da1d026":"Como a vari\u00e1vel dependecy possui uma vari\u00e1vel SQBdependency com os dados 'Ao Quadrado', iremos trat\u00e1-la com os dados da raiz quadrada de SQBdependency","fb5fd1f5":"# Bibliotecas","069f2fde":"Com a execu\u00e7\u00e3o anterior podemos verificar que as vari\u00e1veis edjefa e edjefe n\u00e3o s\u00e3o mais do tipo object, agora apenas coluna dependecy possui dados desse tipo yes\/no e precisam ser tratados.","e651d45a":"Agora n\u00e3o \u00e9 esperado mais colunas do tipo object.","a4f4fe11":"Registros tratados, vamos tratar os outros dados que ainda possuem nulo","f9cc2510":"# Conclus\u00e3o:\n\nEsse notebook \u00e9 resultado da mat\u00e9ria de Data Mining da p\u00f3s gradua\u00e7\u00e3o em Ci\u00eancia de Dados do IESB - Bras\u00edlia.\n\nO notebook do professor Marcos https:\/\/www.kaggle.com\/marcosvafg\/iesb-miner-ii-aula-05-random-forest?scriptVersionId=29607563 foi utilizado como base para esse trabalho.\n\nNo decorer da execu\u00e7\u00e3o do modelo tamb\u00e9m foram utilizados notebooks submetidos na competi\u00e7\u00e3o que utilizei aqui tamb\u00e9m.\n\nO trabalho foi conclu\u00eddo com um valor de 0.44109 para o melhor modelo, o modelo 4. Utilizando RandomForestClassifier com os par\u00e2metros utilizados pelo campe\u00e3o da competi\u00e7\u00e3o. Sendo necess\u00e1rio um grande trabalho de verifica\u00e7\u00e3o das vari\u00e1veis e de qual modelo seria melhor. Outras valida\u00e7\u00f5es tamb\u00e9m foram utilizadas de alguns c\u00f3digos no kaggle, e est\u00e3o documentados nesse arquivo.\n\n# Desafios e problemas encontrados\n\nA cada teste realizado o valor diminuia consideravelmente mesmo tomando a\u00e7\u00f5es que achava que seria melhor. Enfim, em alguns momentos parece que nada fluiria.\n\nTentei utilizar o plot_confusion_matrix para melhorar a visualiza\u00e7\u00e3o mas n\u00e3o foi poss\u00edvel. Acredito que a vers\u00e3o no kaggle estava diferente da necess\u00e1ria.","f0e8ff26":"Passos para cria\u00e7\u00e3o da Matriz de Confus\u00e3o. C\u00f3digo foi verificado no kaggle https:\/\/www.kaggle.com\/kuriyaman1002\/reduce-features-140-85-keeping-f1-score\/execution","edefcaf9":"\n\nComo pode ser visualizado na sa\u00edda acima, as colunas dependency, edjefa e edjefe s\u00e3o do tipo object. Nas instru\u00e7\u00f5es do desafio informava o seguinte:\n\n* dependency, Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)\/(number of member of household between 19 and 64)\n* edjefe, years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n* edjefa, years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\nOu seja, valores n\u00famericos, ent\u00e3o nesse caso precisam ser tratados.\n","b9eb0710":"Os valores da vari\u00e1vel Target ser\u00e3o tratados com -1 para realizar o split do dataset","ced37ffc":"Ap\u00f3s tratamento dos registros das colunas objects, vamos verificar se temos valores nulos no dataset","41c62a88":"Temos algumas colunas como dados nulos, vamos tratar primeiramente as que mais tem dados, pois sa\u00edda acima est\u00e1 ordenada. S\u00e3o elas: v2a1, v18q1 e rez_esc.","1edb8752":"# Modelo 3","80c5e549":"# Trabalho Final - Data Mining II - IESB\n\nEsse projeto foi realizado como entrega final da mat\u00e9ria de Data Mining II da P\u00f3s Gradua\u00e7\u00e3o de Ci\u00eancia de Dados no IESB.\nPara execu\u00e7\u00e3o do projeto foi realizada a competi\u00e7\u00e3o Costa Rican Household Poverty Level Prediction - https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\n    \nComo base do projeto foi utilizado o notebook disponibilizado pelo professor Marcos https:\/\/www.kaggle.com\/marcosvafg\/iesb-miner-ii-aula-05-random-forest e o objetivo era ter uma submiss\u00e3o maior do que 0.43719.\n\nFoi obtido um valor de 0.44109 na competi\u00e7\u00e3o, atigindo o objetivo prim\u00e1rio da mat\u00e9ria.","68b65044":"\nPara isso utilizaremos matrix de correla\u00e7\u00e3o e tamb\u00e9m verifica\u00e7\u00e3o no dataset para vari\u00e1veis que sejam basicamente a mesma coisa. O c\u00f3digo abaixo foi retirado do kaggle https:\/\/www.kaggle.com\/willkoehrsen\/a-complete-introduction-and-walkthrough para verificar a correla\u00e7\u00e3o das vari\u00e1veis. Alguns ajustes foram realizados.\n\nPara realizar a correla\u00e7\u00e3o foram utilizados:\n\nnp.triu: Upper triangle of an array. Return a copy of a matrix with the elements below the k-th diagonal zeroed.\n\nnp.ones: Return a new array of given shape and type, filled with ones.\n\nPara que a colune fosse considerada correlacionada e pudesse ser retirada, foi utlizado o par\u00e2metro de 0.975, fazendo o usu do valor absoluto da coluna (abs).\n","dd84636d":"\n\nAp\u00f3s primeira submiss\u00e3o ap\u00f3s limpeza da base. Tivemos um resultado de 0.43777. Agora iremos fazer novas limpezas para verificar se o valor pode ser melhorado.\n"}}