{"cell_type":{"ca4f5ad6":"code","ec87c624":"code","f0fb0bce":"code","6d42fbbb":"code","04c3820f":"code","13732477":"code","109569ca":"code","610df057":"code","02e43ab9":"code","fa70dfb0":"code","d45b7242":"code","b4059e65":"code","0fb0e869":"code","958b3d13":"code","14a4b266":"code","8a4c9963":"code","70aade69":"code","0e985c08":"code","7fa90d9c":"code","bf1e247f":"code","a76fa6b3":"code","79f70f77":"code","7f203d41":"code","0f5ebea2":"code","ce6bdafd":"code","28b7a6ee":"code","88c60af7":"code","3ce0f023":"code","9a294195":"code","8025f1cf":"code","9b63988d":"code","bb4b3bf2":"code","7882a814":"code","4a2bc470":"code","81a943ff":"code","005738d6":"code","54cc3846":"markdown","be94df50":"markdown","32d86bdf":"markdown","c40dd4dd":"markdown","a7aa2a9d":"markdown","4db1f673":"markdown","310edacf":"markdown","1db0c166":"markdown","0bcd2d1b":"markdown","2b4a31eb":"markdown","e5be0b49":"markdown","7d1e6ccf":"markdown","141119a2":"markdown","c2834227":"markdown","bbd2bfcb":"markdown","2f1234bf":"markdown","603cbcd6":"markdown","c6358ddf":"markdown","c37b945a":"markdown","6f2d7340":"markdown","3c59ee00":"markdown","696e9804":"markdown","673b5db3":"markdown","caa6df0f":"markdown","00b12532":"markdown"},"source":{"ca4f5ad6":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm, skew, kurtosis\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport os\nprint(os.listdir('..\/input'))","ec87c624":"df_train = pd.read_csv('..\/input\/train.csv')","f0fb0bce":"df_train.info()","6d42fbbb":"df_train.head(20)","04c3820f":"df_train.columns","13732477":"#check if there any zero in minimal of the price\ndf_train['SalePrice'].describe()","109569ca":"#https:\/\/seaborn.pydata.org\/tutorial\/distributions.html#plotting-univariate-distributions\nsns.distplot(df_train['SalePrice'], bins=20, rug=True);","610df057":"#Skew = ambience distributions data (0=evenly distributed)\n#Kurt = to check the outlier data (3=standart value)\nprint(\"Skewness: %f\" %df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" %df_train['SalePrice'].kurt())","02e43ab9":"#Annotation Heatmap https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\n\nHeatmap_Annotation = df_train.corr()\nf, ax = plt.subplots(figsize=(9, 6))\nsns.heatmap(Heatmap_Annotation, \n            vmax=.8, square=True)","fa70dfb0":"#Diagonal Correlation https:\/\/seaborn.pydata.org\/examples\/many_pairwise_correlations.html\n\nsns.set(style='white')\nDiagonal_Corr = df_train.corr()\nmask = np.zeros_like(Diagonal_Corr, dtype=np.bool)\nmask[np.tril_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11,9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(Diagonal_Corr, mask=mask, cmap=cmap,\n           vmax=.8, center=0,square=True,\n           linewidths=5, cbar_kws={\"shrink\":.5})","d45b7242":"#SalePrice Correlation Matrix\nk=10\nsns.set(font_scale=1.25)\ncorrmat=df_train.corr()\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.heatmap(cm, annot=True, square=True,\n           fmt='.2f', annot_kws={'size':10},\n           yticklabels=cols.values,\n           xticklabels=cols.values)\n","b4059e65":"#Scatterplot https:\/\/seaborn.pydata.org\/tutorial\/regression.html\ncols = ['SalePrice', 'OverallQual','GrLivArea',\n       'GarageCars', 'GarageArea', 'TotalBsmtSF',\n       '1stFlrSF', 'FullBath', 'TotRmsAbvGrd',\n       'YearBuilt']\nsns.pairplot(df_train[cols], size =5)","0fb0e869":"#missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = ((df_train.isnull().sum()\/df_train.isnull().count())*100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","958b3d13":"f, ax = plt.subplots(figsize=(15,12))\nplt.xticks(rotation='90')\nsns.barplot(x=total.index, y=total)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values',fontsize=15)\nplt.title('Percent missing data by feature',fontsize=15)","14a4b266":"#drop columns\/keys that have more than 50% of null values\ndf_train = df_train.drop((missing_data[missing_data['Percent'] > 50 ]).index,1)\ndf_train.isnull().sum().sort_values(ascending=False) #check","8a4c9963":"#FireplaceQu : data description says Null means \"no fireplace\"\ndf_train['FireplaceQu'] = df_train['FireplaceQu'].fillna('None')","70aade69":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ndf_train[\"LotFrontage\"] = df_train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","0e985c08":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', \n            'BsmtFinType1', 'BsmtFinType2',\n            'GarageType', 'GarageFinish', 'GarageQual', \n            'GarageCond'):\n    df_train[col] = df_train[col].fillna('None')","7fa90d9c":"#GarageYrBlt replacing missing data with 0\ndf_train['GarageYrBlt'] = df_train['GarageYrBlt'].fillna(0)","bf1e247f":"df_train[\"MasVnrType\"] = df_train[\"MasVnrType\"].fillna(\"None\")\ndf_train[\"MasVnrArea\"] = df_train[\"MasVnrArea\"].fillna(0)","a76fa6b3":"#Electrical : It has one NA value. \n#Since this feature has mostly 'SBrkr', we can set that for the missing value.\ndf_train['Electrical'] = df_train['Electrical'].fillna(df_train['Electrical'].mode()[0])","79f70f77":"df_train.isnull().sum().sort_values(ascending=False) #check","7f203d41":"#SalePrice Correlation Matrix\nk=10\nsns.set(font_scale=1.5)\ncorrmat=df_train.corr()\ncols = corrmat.nsmallest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.heatmap(cm, annot=True, square=True,\n           fmt='.2f', annot_kws={'size':10},\n           yticklabels=cols.values,\n           xticklabels=cols.values)\n\n\n","0f5ebea2":"#deleting uncorrelate colomns\nUncor = ['EnclosedPorch', 'OverallCond', \n        'YrSold', 'LowQualFinSF', 'Id', \n         'MiscVal', 'BsmtHalfBath', 'BsmtFinSF2']\ndf_train.drop(Uncor, axis=1, inplace=True)\ndf_train.info()","ce6bdafd":"#More features engineering\n#Transforming some numerical variables that are really\ndf_train['MSSubClass'] = df_train['MSSubClass'].astype(str)\ndf_train['MoSold'] = df_train['MoSold'].astype(str)","28b7a6ee":"# Adding total sqfootage feature \ndf_train['TotalSF'] = df_train['TotalBsmtSF'] + df_train['1stFlrSF'] + df_train['2ndFlrSF']","88c60af7":"#Univariate analysis\n#Detect and exclude outlier in numeric dtype\n#low 0.05 and high 0.90 quantile\nfrom pandas.api.types import is_numeric_dtype\ndef remove_outlier(df_train):\n    low = .05\n    high = .90\n    quant_df = df_train.quantile([low, high])\n    for name in list(df_train.columns):\n        if is_numeric_dtype(df_train[name]):\n            df_train = df_train[(df_train[name] > quant_df.loc[low, name]) & (df_train[name] < quant_df.loc[high, name])]\n    return df_train\n\nremove_outlier(df_train).head()","3ce0f023":"#check the standardizing data\nfor name in list(df_train.columns):\n    if is_numeric_dtype(df_train[name]):\n        saleprice_scaled = StandardScaler().fit_transform(df_train[name][:,np.newaxis]);\n        low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:5]\n        high_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-5:]\n        print('outer range (low) of the distribution:',name)\n        print(low_range)\n        print('\\nouter range (high) of the distribution:',name)\n        print(high_range)","9a294195":"#Bivariate\/Multivariate outlier checking with scatter plot\nfor name in list(df_train.columns):\n    if is_numeric_dtype(df_train[name]):\n        data = pd.concat([df_train['SalePrice'], df_train[name]], axis=1)\n        data.plot.scatter(x=name, y='SalePrice', ylim=(0,800000))","8025f1cf":"#Dropping the outlier\n#Only on the Feature that perform linear regression dot in the scatter plot\ndf_train = df_train.drop(df_train[df_train['LotFrontage'] > 300].index)\ndf_train = df_train.drop(df_train[df_train['LotArea'] > 60000].index)\ndf_train = df_train.drop(df_train[(df_train['OverallQual'] > 9) & (df_train['SalePrice'] < 200000)].index)\ndf_train = df_train.drop(df_train[df_train['MasVnrArea'] > 1500].index)\ndf_train = df_train.drop(df_train[df_train['TotalBsmtSF'] > 3000].index)\ndf_train = df_train.drop(df_train[df_train['1stFlrSF'] > 2500].index)\ndf_train = df_train.drop(df_train[df_train['BsmtFullBath'] > 2.5].index)\ndf_train = df_train.drop(df_train[df_train['HalfBath'] > 1.5].index)\ndf_train = df_train.drop(df_train[df_train['BedroomAbvGr'] > 4].index)\ndf_train = df_train.drop(df_train[df_train['TotRmsAbvGrd'] > 13].index)\ndf_train = df_train.drop(df_train[df_train['Fireplaces'] > 2.5].index)\ndf_train = df_train.drop(df_train[df_train['GarageCars'] > 3].index)\ndf_train = df_train.drop(df_train[df_train['GarageArea'] >= 1250].index)\n","9b63988d":"#skewed features\nnumeric_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\n# Check the skew of all numerical features\nskewed_feats = df_train[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(20)","bb4b3bf2":"sns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","7882a814":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #df_train[feat] += 1\n    df_train [feat] = boxcox1p(df_train[feat], lam)\n    \n#df_train[skewed_features] = np.log1p(df_train[skewed_features])","4a2bc470":"#check\nsns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","81a943ff":"from sklearn.preprocessing import LabelEncoder\ncolomns = df_train.dtypes[df_train.dtypes == \"object\"].index\n# process columns, apply LabelEncoder to categorical features\nfor name in colomns:\n    lbl = LabelEncoder() \n    lbl.fit(list(df_train[name].values)) \n    df_train[name] = lbl.transform(list(df_train[name].values))\n\n# shape        \nprint('Shape of df_train: {}'.format(df_train.shape))","005738d6":"#Dummy categorical features\ndf_train = pd.get_dummies(df_train)\nprint(df_train.shape)\ndf_train.head(20) #please compare the data after engineering and before engineering","54cc3846":"**Simple Tutorial for Seaborn : \nData Visualization**\n\nWhen dealing with a set of data, often the first thing you\u2019ll want to do is get a sense for how the variables are distributed. \n","be94df50":"**Special Thanks** to Kaggle Team, Github Team, Wikipedia, and StackOverflow Team  for abundant source data that make this important notebook happen.\n\n-Comprehensive Data Exploration with Python by Pedro Marcelino\n\n-Stacked Regressions to predict House Prices by Serigne\n\nThis notebook is Data preprosesing unit for Supervised learning, csv file, multiple colum\/multivaried column.\n\nBy using this overflow of Data Engineering notebook, it will make it simpler for the Machine Learning Professional to have a good source of Data. By following this notebook's overflow you can analysize any data source easily.\n\nUsing data from House Prices: Advanced Regression Techniques, Kaggle competition.\n\n","32d86bdf":"**Motivation**\n\nFeature engineering is an essential part of building any intelligent system. Even though you have a lot of newer methodologies coming in like deep learning and meta-heuristics which aid in automated machine learning, each problem is domain specific and better features (suited to the problem) is often the deciding factor of the performance of your system. Feature Engineering is an art as well as a science and this is the reason Data Scientists often spend 70% of their time in the data preparation phase before modeling. Let\u2019s look at a few quotes relevant to feature engineering from several renowned people in the world of Data Science.\n\n> \u201cComing up with features is difficult, time-consuming, requires expert knowledge. \u2018Applied machine learning\u2019 is basically feature engineering.\u201d*\n\u2014 Prof. Andrew Ng.\n\nThis basically reinforces what we mentioned earlier about data scientists spending close to 80% of their time in engineering features which is a difficult and time-consuming process, requiring both domain knowledge and mathematical computations.\n\n>\u201cFeature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\u201d\n>\u2014 Dr. Jason Brownlee\n\nThis gives us an idea about feature engineering being the process of transforming data into features to act as inputs for machine learning models such that good quality features help in improving the overall model performance. Features are also very much dependent on the underlying problem. Thus, even though the machine learning task might be same in different scenarios, like classification of emails into spam and non-spam or classifying handwritten digits, the features extracted in each scenario will be very different from the other.\n\n\nProf. Pedro Domingos from the University of Washington, in his paper titled, \u201cA Few Useful Things to Know about Machine Learning\u201d tells us the following.\n\n> \u201cAt the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.\u201d\n> \u2014 Prof. Pedro Domingos\n\n\nThe final quote which should motivate you about feature engineering is from renowned Kaggler, Xavier Conort. Most of you already know that tough real-world machine learning problems are often posted on Kaggle regularly which is usually open to everyone.\n\n> \u201cThe algorithms we used are very standard for Kagglers. \u2026We spent most of our efforts in feature engineering. \u2026 We were also very careful to discard features likely to expose us to the risk of over-fitting our model.\u201d\n> \u2014 Xavier Conort","c40dd4dd":"**More Features Engineering**\n\nSince area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house","a7aa2a9d":"We already did some data cleaning and discovered a lot about 'SalePrice'. Now it's time to go deep and understand how 'SalePrice' complies with the statistical assumptions that enables us to apply multivariate techniques.\n\nAccording to Hair et al. (2013), four assumptions should be tested:\n\n**Normality** \n\nWhen we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.\n\n\nNormalization technique : \n1. Log Transformation\n2. Sqrt Transformation\n3. Box-Cox Transformation\nWhat is a Box Cox Transformation?\nA Box Cox transformation is a way to transform non-normal dependent variables into a normal shape. Normality is an important assumption for many statistical techniques; if your data isn\u2019t normal, applying a Box-Cox means that you are able to run a broader number of tests.\n\nThe Box Cox transformation is named after statisticians George Box and Sir David Roxbee Cox who collaborated on a 1964 paper and developed the technique.\n\n**Homoscedasticity** \n\nI just hope I wrote it right. Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' (Hair et al., 2013). Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.\n\n**Linearity**\n\nThe most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.\n\n**Absence of correlated errors**\n\nCorrelated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.\n\n","4db1f673":"**Label Encoding**\n\n\nIn many practical Data Science activities, the data set will contain categorical variables. These variables are typically stored as text values which represent various traits. Some examples include color (\u201cRed\u201d, \u201cYellow\u201d, \u201cBlue\u201d), size (\u201cSmall\u201d, \u201cMedium\u201d, \u201cLarge\u201d) or geographic designations (State or Country). Regardless of what the value is used for, the challenge is determining how to use this data in the analysis. Many machine learning algorithms can support categorical values without further manipulation but there are many more algorithms that do not. Therefore, the analyst is faced with the challenge of figuring out how to turn these text attributes into numerical values for further processing.\n\nAs with many other aspects of the Data Science world, there is no single answer on how to approach this problem. Each approach has trade-offs and has potential impact on the outcome of the analysis. Fortunately, the python tools of pandas and scikit-learn provide several approaches that can be applied to transform the categorical data into suitable numeric values.  http:\/\/pbpython.com\/categorical-encoding.html","310edacf":"Important step as Data Scientist is to work with Data.\nBelow is the step by step to work with CSV data:\n\n1.  Understand the problem. \n     We'll look at each variable and do a philosophical analysis about their meaning and importance for this problem.\n     \n2. Univariable study. \n    We'll just focus on the dependent variable ('SalePrice') and try to know a little bit more about it.\n    \n3. Multivariate study. \n    We'll try to understand how the dependent variable and independent variables relate.\n    \n4. Feature Engineering \n    We'll clean the dataset and handle the missing data, outliers and categorical variables.\n   \n5. Test assumptions. \n    We'll check if our data meets the assumptions required by most multivariate techniques.","1db0c166":"**Feature Selection with Correlation**\n\n\n**Correlation** quantifies the degree to which two variables are related. Correlation does not fit a line through the data points. You simply are computing a correlation coefficient (r) that tells you how much one variable tends to change when the other one does. When r is 0.0, there is no relationship. When r is positive, there is a trend that one variable goes up as the other one goes up. When r is negative, there is a trend that one variable goes up as the other one goes down.\n**Linear regression** finds the best line that predicts Y from X.  Correlation does not fit a line\nhttps:\/\/en.wikipedia.org\/wiki\/Pearson_correlation_coefficient\n\n\n1. Correlation matrix (heatmap style).\n2. 'SalePrice' correlation matrix (zoomed heatmap style).\n3. Scatter plots between the most correlated variables ","0bcd2d1b":"MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.","2b4a31eb":"![![a.png](attachment:a.png)]\n","e5be0b49":"Although we already know some of the main figures, this mega scatter plot gives us a reasonable idea about variables relationships.\n\n\nOne of the figures we may find interesting is the one between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area (unless you're trying to buy a bunker).\n\n\nThe plot concerning 'SalePrice' and 'YearBuilt' can also make us think. In the bottom of the 'dots cloud', we see what almost appears to be a shy exponential function (be creative). We can also see this same tendency in the upper limit of the 'dots cloud' (be even more creative). Also, notice how the set of dots regarding the last years tend to stay above this limit (I just wanted to say that prices are increasing faster now).\n","7d1e6ccf":"**Meet the Outlier**\n\nIn statistics, an outlier is an observation point that is distant from other observations.\nThe above definition suggests that outlier is something which is separate\/different from the crowd.\n\nThere are two types of analysis we will follow to find the outliers- Uni-variate(one variable outlier analysis) and Multi-variate(two or more variable outlier analysis).","141119a2":"**The Curse of Dimensionality**\n\nToo many features can be a problem that will lead to sparse data. And have to be remember that every feature is a new dimension -equal- more compute resources. \n\nMuch of feature engineering is selecting the features most relevant to the problem at hand, this is often where domain knowledge comes into play.\n\nUnsupervised dimensionality reduction techniques can also be employed to distill many features into fewer features :\nUsing Principal Component Analysis (PCA) AND\/OR K-Means.","c2834227":"**SalePrice Correlations**\n\nOverallQual, TotalBsmtSF, 1stFlrSF, GrLivArea, GarageCars, GarageArea. These are our first six highest corr from the Heatmap that satisfied saleprice correlations","bbd2bfcb":"**Handling Unbalanced data**\n\nLarge discrepancy between 'positive' and 'negative' caces. Example Fraud Detection. Fraud is rare and most rows will be not fraud. Don't let the terminology confuse you, \"positive\" doesn't mean good, it means the thin you are testing for is what happened, if your machine learning is made to detect fraud, the fraud is the positive case. Mainly a problem with Neural Network.\n\n1.Oversampling : duplicate samples from the minority class, can be done at random.\n\n2.Undersampling : instead of creating more positive samples, remove negative ones but throwing away data is usually not the right answer, unless you are specifically trying to avoid big data scalling issues.\n\n3.SMOTE (Synthetic Minority Over-Sampling Techniqe) : Run K-nearest neighbors of each sample minority class, and create new sample for the KNN result(mean of the neighbors). Both generates new samples and undersamples majority class. Generally better than just oversampling\n\n4.Adjusting Thresholds : When making predictions about a classification (fraud\/not fraud), you have some sort of threshold of probability at which point you'll flag something as the positive case(fraud). If you have too many false positive, one way to fix that is to simply increase that threshold, guaranteed to reduce false positives but could result in more false negative.","2f1234bf":"**Feature Enginering**\n\nLet's analyse this to understand how to handle the missing data.\n\nWe'll consider that when more than 50% of the data is missing, we should delete the corresponding variable and pretend it never existed. This means that we will not try any trick to fill the missing data in these cases. According to this, there is a set of variables (e.g. 'PoolQC', 'MiscFeature', 'Alley', etc.) that we should delete. The point is: will we miss this data? I don't think so. None of these variables seem to be very important, since most of them are not aspects in which we think about when buying a house (maybe that's the reason why data is missing?). Moreover, looking closer at the variables, we could say that variables like 'PoolQC', 'MiscFeature' , 'FireplaceQu' and 'Fence are strong candidates for outliers and bias so we'll be happy to delete them.\n\nThe features engeneering is rather parsimonious (at least compared to some others great scripts) . \n\n**Imputing missing values** by proceeding sequentially through the data\n\n**Transforming **some numerical variables that seem really categorical\n\n**Label Encoding** some categorical variables that may contain information in their ordering set\n\n**Box Cox Transformation** of skewed features (instead of log-transformation) : This gave me a slightly better result both on leaderboard and cross-validation.\n\n**Getting dummy variables** for categorical features.","603cbcd6":"**Missing data**\n\nImportant questions when thinking about missing data:\n\nHow prevalent is the missing data?\nIs missing data random or does it have a pattern?\nThe answer to these questions is important for practical reasons because missing data can imply a reduction of the sample size. This can prevent us from proceeding with the analysis. Moreover, from a substantive perspective, we need to ensure that the missing data process is not biased and hidding an inconvenient truth.\n\n**How to solve imputing Missing Data**\n\n1.Mean Replacement : replace missing values with the mean value from the rest of the column(not rows, a column represents a single feature, it only make sense to take the mean from other samples of the same feature). Median are better choice than Mean when outliers are present.\n\n2.Dropping : best approach if not many rows contain missing data.\n\n3.Inputing Missing Data with Machine Learning model\n\n-KNN : Find K nearest rows and average their values. Assumes numerical data, not categorical. There are ways to handle categorical data (Hamming distance), but categorical data is probably better served by Deep Learning and MICE Regression\n\n-Deep Learning : Build a machine learning model to impute data for your machine learning model, works well for categorical data, but its need more compute resources.\n\n-MICE Regression : Find linear or non-linear relationship between missing feature and other features, it's the most advanced technique MICE (Multiple Imputation by Chained Equation)\n\n4. Just Get More data ","c6358ddf":"**Steps that're not Feature Engineering**\n\n-Initial data collection\n\n-Creating the target variable\n\n-Removing duplicates, handling missing values, fixing mislabeled classes -it's data cleaning\n\n-Scaling or normalization\n\n-Feature Selection\n\n**Feature Engineering**\n\n-Extract more new gold features, remove irrelevant or noisy features\n  Simpler models with better results\n\n-Key Elements\n  \n  1.Target Transformation\n\n2.Feature Encoding\n \n 3.Feature Extraction\n  \n**TARGET TRANSFORMATION**\nPredictor\/Response Variable Transformation\n\n-Use it when variable showed a skewed distribution make the residuals more close to 'normal distribution(bell curve)\n\n**FEATURE ENCODING**\nTurn categorical features into numeric features to provide more fine-grained information\n\n-Help explicity capture non-linear relationships and interactions between the values of features\n\n-Most ML tools only accept numbers as their input: xgboost, gbm, gimnet, libsvm, liblinear,etc.\n\n*1.Label Encoding*\nInterpret the categories as ordered integers(mostly wrong)\n\nPython scikit-learn:LabelEncoder\n\nOK for tree-based methods\n\n*2.One Hot Encoding*\nTransform categories into individual binary (0 or 1)\n\nPython scikit-learn: DictVectorizer, OneHotEncoder\n\nOK for K-means, Linear, NNs, etc\n\n*3.Frequency Encoding*\nEncoding of categorical levels of features to values between 0 and 1 based on their relative frequency\n\n*4.Target Mean Encoding*\nInstead of dummy encoding of categorical variables and increasing the number of features we can encodee each level as the mean of the response.\nAlso instead of just by mean, it is better to calculate weighted average of the overall mean of the training set and the mean of the level.\nThe weight are based on the frequency of the levels i.e. if a category only appears a few times in the dataset then its encoded value will be close to the overall mean instead of the mean of that level.\n\nTo avoid overfitting in Target Mean Encoding we could use leave-one-out schema\n\n*5.Weight of Evidence*\n\n*6.Weight of Evidence and Information Value*\n\n\n**Feature Encoding-for Numerical Value**\n\n-Binning using quantiles(population of the same size in each bin) or histograms(bins of same size).\n  Replace with bin's mean or median\n  -Treat bin id as a category level and use any categorical encoding schema\n  \n -Dimensionality reduction techniques-SVD and PCA\n \n -Clustering and using cluster IDs or\/and distances to cluster centers as new features","c37b945a":"**Most common causes of outliers on a data set:**\n- Data entry errors (human errors)\n- Measurement errors (instrument errors)\n- Experimental errors (data extraction or experiment planning\/executing errors)\n- Intentional (dummy outliers made to test detection methods)\n- Data processing errors (data manipulation or data set unintended mutations)\n- Sampling errors (extracting or mixing data from wrong or various sources)\n- Natural (not an error, novelties in data)\n\nAlso, when starting an outlier detection quest you have to answer two important questions about your dataset:\n\n- Which and how many features am I taking into account to detect outliers ? (univariate \/ multivariate)\n- Can I assume a distribution(s) of values for my selected features? (parametric \/ non-parametric)\n\n\nSome of the most popular methods for outlier detection are:\n\n* Z-Score or Extreme Value Analysis (parametric)\n* Probabilistic and Statistical Modeling (parametric)\n* Linear Regression Models (PCA, LMS)\n* Proximity Based Models (non-parametric)\n* Information Theory Models\n* High Dimensional Outlier Detection Methods (high dimensional sparse data)\n* Best Fit for most cases are using Random Cut Forest","6f2d7340":"The primary concern here is to establish a threshold that defines an observation as an outlier.\n\nHow 'df_train'  looks with her new clothes:\nprimary concern of possible outlier : Total1SF and BsmtFinSF1 ,we should be careful with those 11 something values.","3c59ee00":"BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement.\n\nGarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None\n\n","696e9804":"**What is Feature Engineering ?**\n\nApplying your knowledge of the data and the model you are using, to create better feature to train your model with.\n\n-Which features should I use?\n\n-Do I need to transform these features in some way?\n\n-How do I handle missing data?\n\n-Should I create new features from the existing ones?\n\n**You can't just throw in raw data and expect good result.**\n\n\"This is the art of Machine Learning ; where expertise is applied ; Applied Machine Learning is basically Feature Engineering \" - Andrew Ng\n","673b5db3":"**Why Data pre processing ??**\n**Why I found is hard to have complete set workflow of Feature Data Engineering ??**\n\nData preprocessing is an important step in the data mining process. The phrase \"garbage in, garbage out\" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values (e.g., Income: \u2212100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), missing values, etc. Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running an analysis.[1] Often, data preprocessing is the most important phase of a machine learning project, especially in computational biology.[2]\n\nIf there is much irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase is more difficult. Data preparation and filtering steps can take considerable amount of processing time. Data preprocessing includes cleaning, Instance selection, normalization, transformation, feature extraction and selection, etc. The product of data preprocessing is the final training set. ","caa6df0f":"**More Feature Engineering ** \n\nReduce size of the matrix to save cost of computation by deleting features\/colomns that less than 1 % have correlation with SalePrice","00b12532":"\nIn order to understand our data, we can look at each variable and try to understand their meaning and relevance to this problem. I know this is time-consuming, but it will give us the flavour of our dataset.\n\nIn order to have some discipline in our analysis, we can create an Excel spreadsheet with the following columns:\n\n**Variable** - Variable name.\n\n**Type** - Identification of the variables' type. There are two possible values for this field: 'numerical' or 'categorical'. By 'numerical' we mean variables for which the values are numbers, and by 'categorical' we mean variables for which the values are categories.\n\n**Segment** - Identification of the variables' segment. We can define three possible segments: building, space or location. When we say 'building', we mean a variable that relates to the physical characteristics of the building (e.g. 'OverallQual'). When we say 'space', we mean a variable that reports space properties of the house (e.g. 'TotalBsmtSF'). Finally, when we say a 'location', we mean a variable that gives information about the place where the house is located (e.g. 'Neighborhood').\n\n**Expectation **- Our expectation about the variable influence in 'SalePrice'. We can use a categorical scale with 'High', 'Medium' and 'Low' as possible values.\nConclusion - Our conclusions about the importance of the variable, after we give a quick look at the data. We can keep with the same categorical scale as in 'Expectation'.\n\n**Comments** - Any general comments that occured to us.\nWhile 'Type' and 'Segment' is just for possible future reference, the column 'Expectation' is important because it will help us develop a 'sixth sense'. To fill this column, we should read the description of all the variables and, one by one, ask ourselves:\n"}}