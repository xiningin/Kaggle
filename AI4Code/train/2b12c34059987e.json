{"cell_type":{"ed15f81e":"code","d051c7b0":"code","31f09eea":"code","c1918ed5":"code","a22a2481":"code","e22701f6":"code","fd718be2":"code","f51e2e55":"code","24bfed35":"code","65c703e9":"code","05af62bc":"code","c90a49e3":"code","454d39de":"code","c1505209":"code","43e6d2d6":"code","623bcd9b":"code","6958b9cf":"code","d6296ea6":"code","b75a4a8c":"code","0307cd1c":"code","b2531416":"code","73b0fb7b":"code","4d755ff7":"code","2c231b6c":"code","93bfd242":"code","4a7ebf19":"code","c84f041e":"code","216552db":"code","47deb127":"code","f4124607":"code","f0e72fd5":"code","c1f97783":"code","312b30db":"code","6e97d9b2":"code","65b9ab8e":"code","fd6c09e3":"code","d676e4e9":"code","2dd1e31e":"code","c6d80f3e":"code","79114cc5":"markdown","4688ffb4":"markdown","3f9bdf41":"markdown","58dcd888":"markdown","ccecabb2":"markdown","86a0b1a2":"markdown","cd7a0a89":"markdown","857c53ac":"markdown","cc246ac5":"markdown","0361d4f1":"markdown","0fbd0532":"markdown","3afdf609":"markdown","963e81f2":"markdown","5db5b5b0":"markdown","117c30a8":"markdown","aa1fd6df":"markdown","8c28af1e":"markdown","beaf0274":"markdown","88b29f40":"markdown","fa785d59":"markdown","fb08d7c7":"markdown","ea37c984":"markdown","ec77f72e":"markdown","0da0b46d":"markdown","da8f04fe":"markdown","5cab0713":"markdown","619cf208":"markdown","ffd207c1":"markdown"},"source":{"ed15f81e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d051c7b0":"df = pd.read_csv(\"..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")\ndf.head()","31f09eea":"df.shape","c1918ed5":"df.columns","a22a2481":"#there are 8 object data that needs to be converted\ndf.info()","e22701f6":"df.describe()","fd718be2":"df.describe(include ='object')","f51e2e55":"df['status'].value_counts()","24bfed35":"df = df.drop('sl_no',axis=1)\ndf['salary'] = df['salary'].fillna(0)\ndf.head()","65c703e9":"plt.style.use('ggplot')\nplt.figure(figsize=(20,25))\nplt.subplot(4,2,1)\ndf['gender'].value_counts().plot(kind='bar',title='gender')\nplt.subplot(4,2,2)\ndf['ssc_b'].value_counts().plot(kind='bar',title='ssc_b')\nplt.subplot(4,2,3)\ndf['hsc_b'].value_counts().plot(kind='bar',title='hsc_b')\nplt.subplot(4,2,4)\ndf['hsc_s'].value_counts().plot(kind='bar',title='hsc_s')\nplt.subplot(4,2,5)\ndf['degree_t'].value_counts().plot(kind='bar',title='degree_t')\nplt.subplot(4,2,6)\ndf['workex'].value_counts().plot(kind='bar',title='workex')\nplt.subplot(4,2,7)\ndf['specialisation'].value_counts().plot(kind='bar',title='specialisation')\nplt.subplot(4,2,8)\ndf['status'].value_counts().plot(kind='bar',title='status')\nplt.show()","05af62bc":"plt.figure(figsize=(20,20))\nplt.subplot(5,2,1)\nsns.scatterplot(data=df, x=\"ssc_p\", y=\"salary\", hue=\"status\")\nplt.subplot(5,2,2)\nsns.scatterplot(data=df, x=\"hsc_p\", y=\"salary\", hue=\"status\")\nplt.subplot(5,2,3)\nsns.scatterplot(data=df, x=\"degree_p\", y=\"salary\", hue=\"status\")\nplt.subplot(5,2,4)\nsns.scatterplot(data=df, x=\"etest_p\", y=\"salary\", hue=\"status\")\nplt.subplot(5,2,5)\nsns.scatterplot(data=df, x=\"mba_p\", y=\"salary\", hue=\"status\")\nplt.show()","c90a49e3":"# using label encoder beacuse the below columns are ordinal attributes\n# replaces the attributes with 0,1 and 2 in alphabetically appearing columns \nfrom sklearn.preprocessing import LabelEncoder\ncols = ['gender', 'ssc_b', 'hsc_b','hsc_s', 'degree_t','workex','specialisation', 'status']\nle = LabelEncoder()\ndf[cols] = df[cols].apply(le.fit_transform)\ndf.head()","454d39de":"# extracting independent variable\nX = df.iloc[:,:-1].values\n# extracting dependent variable\nY = df.iloc[:,-1].values\nprint(X.shape)\nprint(Y.shape)","c1505209":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.1, random_state = 100)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","43e6d2d6":"X_mean = X_train.mean(axis=0)\nX_std = X_train.std(axis=0)\n\nX_train = (X_train-X_mean)\/X_std\n\nX_test = (X_test-X_mean)\/X_std\n\nprint(X_train.shape, X_test.shape)","623bcd9b":"import sklearn\nimport numpy as np\nimport pandas as pd\nfrom math import sqrt\nfrom xgboost import XGBRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import f1_score, r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold","6958b9cf":"l_reg = LinearRegression()\nl_reg.fit(X_train,Y_train)\n\nprint(\"Train accuracy:\", (l_reg.score(X_train,Y_train))*100)\nprint(\"Test accuracy:\", (l_reg.score(X_test,Y_test))*100)","d6296ea6":"print(l_reg.coef_)\nprint(\"B0 =\",l_reg.intercept_)","b75a4a8c":"Y_pred = l_reg.predict(X_test)\nprint(Y_test.shape, Y_pred.shape)","0307cd1c":"r2_l = r2_score(Y_test, Y_pred)*100\nrms_l = sqrt(mean_squared_error(Y_test, Y_pred))\nmae_l = mean_absolute_error(Y_test, Y_pred)\nprint(f\"R^2 score of model is {r2_l} %\")\nprint(f\"Root mean squared error is {rms_l}\")\nprint(f\"Mean absolute error is {mae_l}\")","b2531416":"plt.style.use('fivethirtyeight') \nplt.figure(figsize=(10,6))\nplt.scatter(l_reg.predict(X_train), l_reg.predict(X_train)-Y_train,\n            color = \"green\", s = 10, label = 'LR Train data') \nplt.scatter(l_reg.predict(X_test), l_reg.predict(X_test)-Y_test,\n            color = \"blue\", s = 10, label = 'LR Test data') \nplt.hlines(y = 0, xmin = 0, xmax = 950000, linewidth = 2) \nplt.legend(loc = 'upper right') \nplt.title(\"Residual errors\") \nplt.xlabel(\"Salary\")\nplt.ylabel(\"Error\")\nplt.show() ","73b0fb7b":"d_reg = DecisionTreeRegressor()\nd_reg.fit(X_train,Y_train)\n\nprint(\"Train accuracy:\", (d_reg.score(X_train,Y_train))*100)\nprint(\"Test accuracy:\", (d_reg.score(X_test,Y_test))*100)","4d755ff7":"Y_pred = d_reg.predict(X_test)\nprint(Y_test.shape, Y_pred.shape)","2c231b6c":"r2_d = r2_score(Y_test, Y_pred)*100\nrms_d = sqrt(mean_squared_error(Y_test, Y_pred))\nmae_d = mean_absolute_error(Y_test, Y_pred)\nprint(f\"R^2 score of model is {r2_d} %\")\nprint(f\"Root mean squared error is {rms_d}\")\nprint(f\"Mean absolute error is {mae_d}\")","93bfd242":"r_reg = RandomForestRegressor()\nr_reg.fit(X_train,Y_train)\n\nprint(\"Training accuracy:\",(r_reg.score(X_train,Y_train))*100)\nprint(\"Test accuracy:\",(r_reg.score(X_test,Y_test))*100)","4a7ebf19":"Y_pred = r_reg.predict(X_test)\nprint(Y_test.shape, Y_pred.shape)","c84f041e":"r2_r = r2_score(Y_test, Y_pred)*100\nrms_r = sqrt(mean_squared_error(Y_test, Y_pred))\nmae_r = mean_absolute_error(Y_test, Y_pred)\nprint(f\"R^2 score of model is {r2_r} %\")\nprint(f\"Root mean squared error is {rms_r}\")\nprint(f\"Mean absolute error is {mae_r}\")","216552db":"x_reg = XGBRegressor()\nx_reg.fit(X_train,Y_train)\n\nprint(\"Training accuracy:\",(x_reg.score(X_train,Y_train))*100)\nprint(\"Test accuracy:\",(x_reg.score(X_test,Y_test))*100)","47deb127":"Y_pred = x_reg.predict(X_test)\nprint(Y_test.shape, Y_pred.shape)","f4124607":"r2_x = r2_score(Y_test, Y_pred)*100\nrms_x = sqrt(mean_squared_error(Y_test, Y_pred))\nmae_x = mean_absolute_error(Y_test, Y_pred)\nprint(f\"R^2 score of model is {r2_x} %\")\nprint(f\"Root mean squared error is {rms_x}\")\nprint(f\"Mean absolute error is {mae_x}\")","f0e72fd5":"models = pd.DataFrame({\n    'Algorithm': ['Linear Regression','Decision Tree Regressor', \n             'Random Forest Regressor',  'XGBoost Regressor'],\n    'R^2 Score': [ r2_l, r2_d, r2_r, r2_x],\n    'RMS Score' : [rms_l, rms_d, rms_r, rms_x],\n    'MAE Score' : [mae_l, mae_d, mae_r, mae_x]\n})\n\nmodels.sort_values(by = ['R^2 Score', 'RMS Score', 'MAE Score'], ascending = True)","c1f97783":"plt.style.use('fivethirtyeight') \nplt.figure(figsize=(12,8))\nplt.scatter(l_reg.predict(X_train), l_reg.predict(X_train)-Y_train,\n            color = \"green\", s = 20, label = 'LR Train data') \nplt.scatter(l_reg.predict(X_test), l_reg.predict(X_test)-Y_test,\n            color = \"blue\", s = 20, label = 'LR Test data') \nplt.scatter(d_reg.predict(X_train), d_reg.predict(X_train)-Y_train,\n            color = \"red\", s = 20, label = 'DT R Train data') \nplt.scatter(d_reg.predict(X_test), d_reg.predict(X_test)-Y_test,\n            color = \"black\", s = 20, label = 'DT R Test data') \nplt.scatter(r_reg.predict(X_train), r_reg.predict(X_train)-Y_train,\n            color = \"yellow\", s = 20, label = 'RF R Train data') \nplt.scatter(r_reg.predict(X_test), r_reg.predict(X_test)-Y_test,\n            color = \"pink\", s = 20, label = 'RF R Test data') \nplt.scatter(x_reg.predict(X_train), x_reg.predict(X_train)-Y_train,\n            color = \"orange\", s = 20, label = 'XGB R Train data') \nplt.scatter(x_reg.predict(X_test), x_reg.predict(X_test)-Y_test,\n            color = \"white\", s = 20, label = 'XGB R Test data') \nplt.hlines(y = 0, xmin = 0, xmax = 950000, linewidth = 2) \nplt.legend(loc = 'upper right') \nplt.title(\"Residual errors\") \nplt.xlabel(\"Salary\")\nplt.ylabel(\"Error\")\nplt.show() ","312b30db":"plt.figure(figsize=(12,8))\nsns.barplot(x='Algorithm',y='R^2 Score',data=models)\nplt.show()","6e97d9b2":"# prepare the cross-validation procedure\ncv = KFold(n_splits=10, random_state=100, shuffle=True)\n# create model\nl_reg = LinearRegression()\nl_reg.fit(X_train, Y_train)\n# evaluate model\nscores = cross_val_score(l_reg, X_train, Y_train, scoring='r2', cv=cv)\nprint(f'Score Array list: {scores}') \nprint('\\n')\n# report performance\nY_pred = l_reg.predict(X_test)\nr2_l = sklearn.metrics.r2_score(Y_test, Y_pred)\nprint(f'R^2 Score: {r2_l}')","65b9ab8e":"# prepare the cross-validation procedure\ncv = KFold(n_splits=10, random_state=100, shuffle=True)\n# create model\nd_reg = DecisionTreeRegressor()\nd_reg.fit(X_train, Y_train)\n# evaluate model\nscores = cross_val_score(d_reg, X_train, Y_train, scoring='r2', cv=cv)\nprint(f'Score Array list: {scores}') \nprint('\\n')\n# report performance\nY_pred = d_reg.predict(X_test)\nr2_d = sklearn.metrics.r2_score(Y_test, Y_pred)\nprint(f'R^2 Score: {r2_d}')","fd6c09e3":"# prepare the cross-validation procedure\ncv = KFold(n_splits=10, random_state=100, shuffle=True)\n# create model\nr_reg = RandomForestRegressor()\nr_reg.fit(X_train, Y_train)\n# evaluate model\nscores = cross_val_score(r_reg, X_train, Y_train, scoring='r2', cv=cv)\nprint(f'Score Array list: {scores}') \nprint('\\n')\n# report performance\nY_pred = r_reg.predict(X_test)\nr2_r = sklearn.metrics.r2_score(Y_test, Y_pred)\nprint(f'R^2 Score: {r2_r}')","d676e4e9":"# prepare the cross-validation procedure\ncv = KFold(n_splits=10, random_state=100, shuffle=True)\n# create model\nx_reg = XGBRegressor()\nx_reg.fit(X_train, Y_train)\n# evaluate model\nscores = cross_val_score(x_reg, X_train, Y_train, scoring='r2', cv=cv)\nprint(f'Score Array list: {scores}') \nprint('\\n')\n# report performance\nY_pred = x_reg.predict(X_test)\nr2_x = sklearn.metrics.r2_score(Y_test, Y_pred)\nprint(f'R^2 Score: {r2_x}')","2dd1e31e":"models = pd.DataFrame({\n    'Algorithm': ['Linear Regression','Decision Tree Regressor', \n             'Random Forest Regressor',  'XGBoost Regressor'],\n    'R^2 Score': [ r2_l, r2_d, r2_r, r2_x],\n    })\n\nmodels.sort_values(by = ['R^2 Score'], ascending = True)","c6d80f3e":"plt.figure(figsize=(12,8))\nsns.barplot(x='Algorithm',y='R^2 Score',data=models)\nplt.show()","79114cc5":"## **Plotting the Graph**","4688ffb4":"# **Cross-Validation of Models using K-fold CV**","3f9bdf41":"# **Data Visualization**","58dcd888":"### **Cross-Validation of Random Forest Model**","ccecabb2":"## **Plotting the Residual Chart**","86a0b1a2":"## **Linear Regression**","cd7a0a89":"## **Extract the independent (input) and dependent (output) variable**","857c53ac":"# **Training Model using Regression algorithms**\n\n* ### **Linear Regressor**\n* ### **Decision Tree Regressor**\n* ### **Random Forest Regressor**\n* ### **XGBoost Regressor**","cc246ac5":"## **Splitting the dataset into the Training and Testing sets**","0361d4f1":"# **Data Description**","0fbd0532":"# **Descriptive Data Analysis**","3afdf609":"## **Importing Dataset**","963e81f2":"## **Random Forest Regressor**","5db5b5b0":"* **sl_no:** Serial Number\n* **gender:** Gender- Male='M',Female='F'\n* **ssc_p:** Secondary Education percentage- 10th Grade\n* **ssc_b:** Board of Education- Central\/ Others\n* **hsc_p:** Higher Secondary Education percentage- 12th Grade\n* **hsc_b:** Board of Education- Central\/ Others\n* **hsc_s:** Specialization in Higher Secondary Education\n* **degree_p:** Degree Percentage\n* **degree_t:** Under Graduation(Degree type)- Field of degree education\n* **workex:** Work Experience\n* **etest_p:** Employability test percentage ( conducted by college)\n* **specialisation:** Post Graduation(MBA)- Specialization\n* **mba_p:** MBA percentage\n* **status:** Status of placement- Placed\/Not placed\n* **salary:** Salary offered by corporate to candidates","117c30a8":"## **Importing Libraries**","aa1fd6df":"## **XGBoost Regressor**","8c28af1e":"### **Cross-Validation of XGBoost Regressor Model**","beaf0274":"## **Visualizing the results**","88b29f40":"### **Cross-Validation of Decision Tree Regressor Model**","fa785d59":"## **Normalization of Dataset**","fb08d7c7":"### **Cross-Validation of Linear Regression Model**","ea37c984":"## **Importing Libraries**","ec77f72e":"## **Decision Tree Regressor**","0da0b46d":"**The goal is to predict the salary of employees using regression models**","da8f04fe":"## **Analysing the cross-validation of models**","5cab0713":"## **Evaluation Table**","619cf208":"# **Campus Recruitment | EDA and Regression Models**","ffd207c1":"# **Data Preprocessing**\n\n## **Encoding of categorical values using Label Encoder**"}}