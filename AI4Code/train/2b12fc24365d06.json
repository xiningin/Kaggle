{"cell_type":{"a85b3e03":"code","c829756e":"code","b7474257":"code","4e3f766c":"code","e2a3ab3b":"code","c185e563":"code","a43ef665":"code","581b2d23":"code","1b195a4f":"code","992673c4":"code","59bc9226":"code","8ab84da8":"code","a2664e10":"code","36470c29":"code","c6367b3e":"code","2707083f":"code","ff7b2f41":"code","a8bd81ee":"code","ee4e1240":"code","0e77a14e":"code","0fdfb4de":"code","4e81ccd6":"code","5d2e5452":"code","5f8c6812":"code","19d54831":"code","3fb1cebf":"code","72d7e747":"code","990c9c84":"code","142412b5":"code","a32212ca":"code","0c87873d":"code","716bb63b":"code","cb77f63a":"code","5734b47a":"code","79ec3976":"code","a365bdb8":"code","e0392a8a":"code","7d3222c5":"code","b4d2a4d0":"code","c5b74a5c":"code","5387b236":"code","385989f0":"code","1a44fb27":"code","e75c1f5e":"code","3634509f":"code","45ed6773":"code","81a517b7":"code","8f61f5ad":"code","fbff528d":"code","65755e4a":"code","f46ee51c":"code","a0ba8086":"code","88f75412":"code","65e63f52":"code","6d62ce4d":"code","c4c85d13":"code","ffe673a3":"code","3a14e503":"markdown","763a3844":"markdown","12cadedf":"markdown","90cc1c04":"markdown","6bd1bf1e":"markdown","29db2044":"markdown","3f425486":"markdown","cc2d960d":"markdown","98b20bc8":"markdown","4adf49e7":"markdown","3a82c583":"markdown","ff0aeb07":"markdown","a2d07532":"markdown","3b12d96c":"markdown","1cee5642":"markdown","41a25306":"markdown","583e11b0":"markdown","6f022a87":"markdown","3ce7b980":"markdown","8a66f350":"markdown","49c683da":"markdown","c22bed6d":"markdown","cef563ba":"markdown","289ce893":"markdown","7d6b361b":"markdown","ccb2bdfb":"markdown","86c45b81":"markdown"},"source":{"a85b3e03":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport warnings\nimport os \nwarnings.filterwarnings(\"ignore\")\nimport datetime\n","c829756e":"data=pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\n","b7474257":"data.head()      #displaying the head of dataset they gives the 1st to 5 rows of the data","4e3f766c":"data.describe()      #description of dataset ","e2a3ab3b":"data.info()","c185e563":"data.shape       #569 rows and 33 columns","a43ef665":"data.columns     #displaying the columns of dataset","581b2d23":"data.value_counts","1b195a4f":"data.dtypes","992673c4":"data.isnull().sum()","59bc9226":"data.drop('Unnamed: 32', axis = 1, inplace = True)\n","8ab84da8":"data","a2664e10":"data.corr()","36470c29":"plt.figure(figsize=(18,9))\nsns.heatmap(data.corr(),annot = True, cmap =\"Accent_r\")\n\n\n\n","c6367b3e":"sns.barplot(x=\"id\", y=\"diagnosis\",data=data[160:190])\nplt.title(\"Id vs Diagnosis\",fontsize=15)\nplt.xlabel(\"Id\")\nplt.ylabel(\"Diagonis\")\nplt.show()\nplt.style.use(\"ggplot\")\n","2707083f":"sns.barplot(x=\"radius_mean\", y=\"texture_mean\", data=data[170:180])\nplt.title(\"Radius Mean vs Texture Mean\",fontsize=15)\nplt.xlabel(\"Radius Mean\")\nplt.ylabel(\"Texture Mean\")\nplt.show()\nplt.style.use(\"ggplot\")\n","ff7b2f41":" \nmean_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\nsns.pairplot(data[mean_col],hue = 'diagnosis', palette='Accent')\n","a8bd81ee":"sns.violinplot(x=\"smoothness_mean\",y=\"perimeter_mean\",data=data)","ee4e1240":"plt.figure(figsize=(14,7))\nsns.lineplot(x = \"concavity_mean\",y = \"concave points_mean\",data = data[0:400], color='green')\nplt.title(\"Concavity Mean vs Concave Mean\")\nplt.xlabel(\"Concavity Mean\")\nplt.ylabel(\"Concave Points\")\nplt.show()\n\n","0e77a14e":"worst_col = ['diagnosis','radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\n\nsns.pairplot(data[worst_col],hue = 'diagnosis', palette=\"CMRmap\")","0fdfb4de":"# Getting Features\n\nx = data.drop(columns = 'diagnosis')\n\n# Getting Predicting Value\ny = data['diagnosis']\n","4e81ccd6":"\n#train_test_splitting of the dataset\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)\n","5d2e5452":"print(len(x_train))\n","5f8c6812":"print(len(x_test))","19d54831":"print(len(y_train))","3fb1cebf":"print(len(y_test))","72d7e747":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(x_train,y_train)                         \n","990c9c84":"y_pred=reg.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",reg.score(x_train,y_train)*100)\n\n\n","142412b5":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata\n\n\n\n\n","a32212ca":"print(accuracy_score(y_test,y_pred)*100)","0c87873d":"from sklearn.model_selection import GridSearchCV\nparam = {\n         'penalty':['l1','l2'],\n         'C':[0.001, 0.01, 0.1, 1, 10, 20,100, 1000]\n}\nlr= LogisticRegression(penalty='l1')\ncv=GridSearchCV(reg,param,cv=5,n_jobs=-1)\ncv.fit(x_train,y_train)\ncv.predict(x_test)\n","716bb63b":"print(\"Best CV score\", cv.best_score_*100)","cb77f63a":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=6, random_state=123)\n\ndtree.fit(x_train,y_train)\n\n#y_pred = dtree.predict(x_test)\n","5734b47a":"y_pred=dtree.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",dtree.score(x_train,y_train)*100)\n\n","79ec3976":"print(accuracy_score(y_test,y_pred)*100)","a365bdb8":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nrfc.fit(x_train,y_train)\n\n","e0392a8a":"y_pred=rfc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",rfc.score(x_train,y_train)*100)\n","7d3222c5":"print(accuracy_score(y_test,y_pred)*100)","b4d2a4d0":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=7)\n\nknn.fit(x_train,y_train)\n","c5b74a5c":"y_pred=knn.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",knn.score(x_train,y_train)*100)\nprint(knn.score(x_test,y_test))\n","5387b236":"print(accuracy_score(y_test,y_pred)*100)\n","385989f0":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\n","1a44fb27":"y_pred=svc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",svc.score(x_train,y_train)*100)\nprint(svc.score(x_test,y_test))\n","e75c1f5e":"print(\"Training Score: \",svc.score(x_train,y_train)*100)","3634509f":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = None)\nadb.fit(x_train,y_train)\n\n\n\n\n\n","45ed6773":"y_pred=adb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",adb.score(x_train,y_train)*100)","81a517b7":"print(accuracy_score(y_test,y_pred)*100)","8f61f5ad":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\ngbc.fit(x_train,y_train)\n","fbff528d":"y_pred=gbc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",gbc.score(x_train,y_train)*100)\nprint(gbc.score(x_test,y_test))\n","65755e4a":"print(accuracy_score(y_test,y_pred)*100)","f46ee51c":"from xgboost import XGBClassifier\n\nxgb =XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxgb.fit(x_train, y_train)\n","a0ba8086":"y_pred=xgb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",xgb.score(x_train,y_train)*100)\nprint(xgb.score(x_test,y_test))\n","88f75412":"print(\"Training Score: \",xgb.score(x_train,y_train)*100)","65e63f52":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","6d62ce4d":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)","c4c85d13":"y_pred=gnb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\nprint(\"Training Score: \",gnb.score(x_train,y_train)*100)\nprint(gnb.score(x_test,y_test))\n","ffe673a3":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","3a14e503":"**So we get a accuracy score of 63.29 % using Naive Bayes**","763a3844":"# LOADING THE DATASET","12cadedf":"# 2. DECISION TREE CLASSIFIER","90cc1c04":"**Ada Boost Classifier got the highest accuracy**","6bd1bf1e":"**So now we conclude the accuracy of different models:**\n\n**1. AdaBoost Classifier = 98.24 %**\n\n**2. XGB Classifier= 97.84 %**\n\n**3. Random Forest Classifier =96.57 %**\n\n**4. Gradient Boosting Classifier= 95.66%**\n\n**5. Decision Tree Classifier= 94.78 %**\n\n**6. K Neighbours Classifier= 70.18 %**\n\n**7. SVC = 63.80 %**\n\n**8. Naiye Bayes= 63.30 %**\n\n**9. Logistic Regression = 58.82%**\n","29db2044":"# 4. KNeighborsClassifier\n\n","3f425486":"**So we get a accuracy score of 96.49 % using Random Forest Classifier**","cc2d960d":"**So we get a accuracy score of 97.80 % using  XGBClassifier**","98b20bc8":"**So we get a accuracy score of 98.24 % using AdaBoostClassifier**","4adf49e7":"**So we get a accuracy score of 95.61 % using GradientBoostingClassifier**","3a82c583":"# VISUALIZING THE DATA","ff0aeb07":"**So we get a accuracy score of 58.7 % using logistic regression**","a2d07532":"# 9. Naive Bayes","3b12d96c":"**So we have to drop the Unnamed: 32 coulumn which contains NaN values**","1cee5642":"**So we get a accuracy score of 94.73 % using Decision Tree Classifier**","41a25306":"**So we get a accuracy score of 70.17 % using KNeighborsClassifier**","583e11b0":"# 1. Logistic Regression","6f022a87":"# If you liked this notebook, please UPVOTE it.","3ce7b980":"# 6. AdaBoostClassifier","8a66f350":"# 3. Random Forest Classifier","49c683da":"**So we get a accuracy score of 63.7 % using SVC**","c22bed6d":"# 8. XGBClassifier","cef563ba":"# MODELS","289ce893":"# IMPORTING THE LIBRARIES","7d6b361b":"#  7. Gradient Boosting Classifier","ccb2bdfb":"# 5. SVC","86c45b81":"# TRAINING AND TESTING DATA"}}