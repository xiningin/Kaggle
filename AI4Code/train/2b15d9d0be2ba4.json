{"cell_type":{"360978d9":"code","eeb76497":"code","564ad210":"code","6eccc5d3":"code","40ff6ea7":"code","ee2a6061":"code","55ccca72":"code","0c8eb8cf":"code","68abfcc7":"code","b2decd31":"code","e24fcb48":"code","ab9d9dcc":"code","c72fb96d":"code","a9162feb":"code","efc53e17":"code","68a8e98c":"code","a9ebad8b":"code","38582be7":"code","92c86a28":"code","2a7b604a":"code","cdf3c404":"code","3ce68b42":"code","fcded72e":"code","d63529ca":"code","9086564b":"code","00edb161":"code","a1710803":"code","4df0bf2f":"code","de87cad7":"code","5178ad3a":"code","0e228dc2":"code","b5bb63d9":"code","b7a610f8":"code","b4ac4ed2":"code","9e41963a":"code","d7ea0163":"code","9229f2c9":"code","6c05b764":"code","e51793c5":"code","24748a46":"code","120b9da5":"code","4f9ba042":"code","9a2ca30e":"code","c5e095c4":"code","5f0bc846":"code","bd1e5e40":"code","fdd1db14":"code","e9d9f2f1":"code","70265888":"code","3512df3b":"code","86fa122e":"code","e885cfd0":"code","d9bacb2e":"code","8c60ad16":"code","c8bc78c7":"code","4bfda071":"code","773d9e9a":"code","ddbaae19":"code","913d2117":"code","0b5dbcaf":"code","49b092a0":"code","00b1ee67":"code","14d9caff":"code","815a4ab9":"code","547f4f0e":"code","f305356d":"code","d51a4213":"code","4be9a7b3":"code","6c996224":"code","62ac7a71":"code","0fdcbebc":"code","e9a73bc5":"code","1612bd55":"code","8c14bdb9":"code","47cb6653":"code","fb821f2e":"code","2e7e9e01":"code","84d660ca":"code","29cc9a63":"code","f6829820":"code","ec6ffd07":"code","3a0f9ca4":"code","10547e3c":"code","7c1cdbf1":"code","adddb4c2":"code","8678896e":"code","4b321054":"code","5e409f70":"code","7f598f42":"code","7e5c391c":"code","065f0587":"code","9108303a":"code","4e137700":"code","1168057f":"code","26d15cc9":"code","ab16a5c6":"code","f9a9326c":"code","284f96dc":"code","f216d938":"code","50661f6f":"code","1ea4b374":"code","182e25ca":"code","ed26ed2b":"code","fcb9e2a2":"code","4e5c6595":"code","3c77f897":"code","5556ef09":"code","451285df":"code","a67f5f65":"code","3fe6041b":"markdown","a06c8a9d":"markdown","a7f0c0f3":"markdown","b41c25c1":"markdown","63b2b59f":"markdown","13fdbeda":"markdown","66c79954":"markdown","783a3880":"markdown","f3db6148":"markdown","8ca5af23":"markdown","e6c5dcfb":"markdown","e470be71":"markdown","0a81d17f":"markdown","8526a4b2":"markdown","b188fc08":"markdown","791ccd28":"markdown","b8a520d7":"markdown","1cda1ac6":"markdown","f88991f3":"markdown","82946a6e":"markdown","5d041c1b":"markdown","f67ff55c":"markdown","b855b8ef":"markdown","3fdf6d3e":"markdown","04a5f88d":"markdown","1835201f":"markdown","7a65945c":"markdown","521abb07":"markdown","733ebc40":"markdown","3e9c106a":"markdown","dbf7338a":"markdown","554274ca":"markdown","e50cae21":"markdown","e835f9f2":"markdown","b11020b2":"markdown","3ea64f3e":"markdown","212e4879":"markdown","263d89e0":"markdown","0dd3035f":"markdown","c21812f1":"markdown","1438e888":"markdown","b117123f":"markdown","2e5581aa":"markdown","4bb5fc2c":"markdown","8ddc65f4":"markdown","44f236d0":"markdown","edb5e113":"markdown","df295f98":"markdown","5e7c01ac":"markdown","c59d9075":"markdown","f3e33e31":"markdown","03f9994e":"markdown","3ce3c0a6":"markdown","f63cc36c":"markdown","08baaef4":"markdown","6c7eb8a0":"markdown","40fb4bee":"markdown","03a3e782":"markdown","545c50e5":"markdown","3a86e9c8":"markdown","76f6d9f6":"markdown","bb71bc35":"markdown","7705dd6a":"markdown","d851ef5e":"markdown","40fec8c5":"markdown","bb5aa988":"markdown","35dc01b5":"markdown","0cac3ebf":"markdown","d82cfa5e":"markdown","a41805c1":"markdown","1de3574a":"markdown","9dfdc72a":"markdown","6870613f":"markdown","352d9477":"markdown","6e63dec8":"markdown","a456ddc5":"markdown","a5f69b28":"markdown","13eeff4e":"markdown","b817763d":"markdown","1cdf2628":"markdown","392620cb":"markdown","99953269":"markdown","02240b96":"markdown","cbd6a3cd":"markdown","3e418f70":"markdown","121955d7":"markdown","512a153c":"markdown","facfb0b6":"markdown","e9c007ce":"markdown","adfb8ee4":"markdown","c2a01dbd":"markdown","990e8854":"markdown","110ed37b":"markdown","75469e06":"markdown","c8aa5a10":"markdown","377927e5":"markdown","36ec260e":"markdown","b3d96494":"markdown","d5819818":"markdown","d8bf26e5":"markdown","84421309":"markdown","5501bfd9":"markdown"},"source":{"360978d9":"# Imports and reading data\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom IPython.display import display\npd.options.display.max_columns = None","eeb76497":"train=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","564ad210":"train.head()","6eccc5d3":"train.info()\n","40ff6ea7":"train.describe()","ee2a6061":"train[(train['LotArea']>11601)&(train['LotArea']<=215245)]['LotArea'].sort_values(ascending =False).head(20)","55ccca72":"train['MiscVal'].sort_values(ascending =False).head(20)","0c8eb8cf":"# Creating list for numeric values.\nfrom pandas.api.types import is_numeric_dtype\nnumcollist=[col for col in train.drop('Id', axis=1).columns if is_numeric_dtype(train[col]) ]","68abfcc7":"for col in numcollist:\n    print(col, train[col].nunique())","b2decd31":"# Creating list for ordinal and categorical variables\n\nordlist=[col for col in numcollist if train[col].nunique() < 16 ]\nordlist.append('YearBuilt')\nordlist.append('YearRemodAdd')\nordlist.append('GarageYrBlt')\nfor col in ordlist:\n    numcollist.remove(col)\ncateglist=[col for col in train.drop('Id', axis=1).columns if not is_numeric_dtype(train[col])]\n\ncateglist=categlist+ ordlist\n\n","e24fcb48":"train[categlist].isnull().sum()","ab9d9dcc":"# Missing values\ntrain[numcollist].isnull().sum()","c72fb96d":"#zero values\nfor col in numcollist:\n    print(col,train[train[col]==0][col].count())\n\n    ","a9162feb":"#Histogram of SalePrice to see impact of zero values in 'BsmtSF1'\ntrain[train['BsmtFinSF1']!=0]['SalePrice'].plot.hist( bins=30, color= 'blue', label='SalePrice at non-zero BsmtSF1')\nplt.axvline(train[train['BsmtFinSF1']!=0]['SalePrice'].mean(),color= 'blue', label='Mean Sale Price for Non zero BsmtSF1')\ntrain[train['BsmtFinSF1']==0]['SalePrice'].plot.hist( bins=30, color='yellow',alpha=.6, label='SalePrice at zero BsmtSF1')\nplt.axvline(train[train['BsmtFinSF1']==0]['SalePrice'].mean(),color= 'yellow', label='Mean Sale Price for zero BsmtSF1')\nplt.xlabel('SalePrice')\nplt.title('Saleprice variation at zero values of BsmtFinSF1')\nplt.legend()","efc53e17":"#Histogram of SalePrice to see impact of zero values in 'BsmtFinSF2'\ntrain[train['BsmtFinSF2']!=0]['SalePrice'].plot.hist( bins=30, color= 'blue',label='SalePrice at non-zero BsmtFinSF2')\nplt.axvline(train[train['BsmtFinSF2']!=0]['SalePrice'].mean(),color= 'blue', label='Mean Sale Price for Non zero BsmFintSF2')\ntrain[train['BsmtFinSF2']==0]['SalePrice'].plot.hist( bins=30, color='yellow',alpha=.6, label='SalePrice at zero BsmtFinSF2')\nplt.axvline(train[train['BsmtFinSF2']==0]['SalePrice'].mean(),color= 'yellow', label='Mean Sale Price for zero BsmtFinSF2')\nplt.xlabel('SalePrice')\nplt.title('Saleprice variation at zero values of BsmtFinSF2')\nplt.legend()","68a8e98c":"#Histogram of SalePrice to see impact of zero values in 'OpenPorchSF'\ntrain[train['OpenPorchSF']!=0]['SalePrice'].plot.hist( bins=30, color= 'blue')\nplt.axvline(train[train['OpenPorchSF']!=0]['SalePrice'].mean(),color= 'blue', label='Mean Sale Price for Non zero OpenPorchSF')\ntrain[train['OpenPorchSF']==0]['SalePrice'].plot.hist( bins=30, color='yellow',alpha=.6)\nplt.axvline(train[train['OpenPorchSF']==0]['SalePrice'].mean(),color= 'yellow', label='Mean Sale Price for zero OpenPorchSF')\nplt.xlabel('SalePrice')\nplt.title('Saleprice variation at zero values of OpenPorchSF')\nplt.legend()","a9ebad8b":"# We derive zscore and plot its histogram.\ntrain['zscore']=(train['SalePrice']-train['SalePrice'].mean())\/train['SalePrice'].std()\nplt.hist(train['zscore'], bins=30)\nplt.axvline(x=3, color = 'red', label='zscore of 3')\nplt.xlabel('Zscore')\nplt.ylabel('Frequency')\nplt.title('Zscore of SalePrice')\nplt.legend()\n\n","38582be7":"\npoints=len(train[train['zscore'] >=3])\nprint('{} of SalePrice points are outliers'.format(points))\n\ntrain=train.drop('zscore', axis=1)","92c86a28":"#Calculate correlation coefficients of variables eith SalePrice.\ntrain[numcollist].corr()['SalePrice'].sort_values(ascending=False).head()","2a7b604a":"# Select GrLivArea and plot scatter plot\nplt.scatter(x='GrLivArea', y='SalePrice' ,data=train)\nplt.xlabel('GrliveArea')\nplt.ylabel('SalePrice')\nplt.title('Relation between SalePrice and GrLivArea')","cdf3c404":"# Sample of normal distribution\nnormal = np.random.normal(np.mean(train['SalePrice']), np.std(train['SalePrice']), size=len(train))\n#Distribution plot of SalePrice alongside kde plot of a normal distribution. \nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nsns.distplot(train['SalePrice'], norm_hist=False, kde = True, bins=50)\nsns.kdeplot(normal, color='orange', label='normal')\nplt.axvline(train['SalePrice'].mean(), color='lightgreen', linewidth=3, label='Mean')\nplt.axvline(np.percentile(train['SalePrice'],25), color='brown', linewidth=3, label='Quartiles')\nplt.axvline(np.percentile(train['SalePrice'],75), color='brown', linewidth=3)\nplt.xlabel('SalePrice',fontsize=14)\nplt.ylabel('Frequency',fontsize=14)\nplt.title('Histogram of Saleprice',fontsize=14)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.legend(fontsize=14)\n\n#Plotting cumulative distribution of Sale Price and cumulative normal distribution for comparison.\nplt.subplot(1,2,2)\nplt.hist(normal, cumulative=True, density=True, color='grey',histtype='stepfilled',rwidth=None, bins=50, label='Normal cumulative distribution')\nplt.hist(train['SalePrice'], cumulative=True, density=True, color='lightgreen',histtype='stepfilled',rwidth=None,alpha=.5, bins=50, label='Cumulative Frequency of Sale Price')\nplt.legend(loc='upper left', fontsize=14)\nplt.axvline(np.percentile(normal, 75), color='grey' ,label='75th point in normal distrubution')\nplt.axvline(np.percentile(train['SalePrice'], 75), color='green',label='75th point in Sale price distribution')\nplt.xlabel('Sale Price',fontsize=14)\nplt.ylabel('Cumulative frequency of Sale Price',fontsize=14)\nplt.title('Cumulative Distribution Comparison',fontsize=14)\nplt.legend(loc='center',fontsize=14)\n","3ce68b42":"# We create a sample of lognormal distribution\nlognormal = np.random.normal(np.mean(np.log(train['SalePrice'])), np.std(np.log(train['SalePrice'])), size=len(train))\n# Distribution plot of SalePrice alongside kde plot of a lognormal distribution. \nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nsns.distplot(np.log(train['SalePrice']), norm_hist=False, kde = True, bins=50, label='Sale Price')\nsns.kdeplot(lognormal, color='orange', label='lognormal')\nplt.axvline(np.log(train['SalePrice']).mean(), color='lightgreen', linewidth=3, label='Mean Sale Price')\nplt.axvline(np.percentile(np.log(train['SalePrice']),25), color='brown', linewidth=3, label='Quartiles')\nplt.axvline(np.percentile(np.log(train['SalePrice']),75), color='brown', linewidth=3)\nplt.xlabel('SalePrice',fontsize=14)\nplt.ylabel('Frequency',fontsize=14)\nplt.title('Histogram of Saleprice',fontsize=14)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.legend(fontsize=14)\n\n#Plotting cumulative distribution of Sale Price and cumulative lognormal distribution for comparison.\nplt.subplot(1,2,2)\n#plt.figure(figsize=(5,5))\nplt.hist(lognormal, cumulative=True, density=True, color='grey',histtype='stepfilled',rwidth=None, bins=50, label='Cumulative lognormal distribution')\nplt.hist(np.log(train['SalePrice']), cumulative=True, density=True, color='lightgreen',histtype='stepfilled',rwidth=None,alpha=.5, bins=50, label='Cumulative Frequency of Sale Price')\nplt.legend(loc='center right', fontsize=14)\nplt.xlabel('Sale Price',fontsize=14)\nplt.ylabel('Cumulative frequency of Sale Price',fontsize=14)\nplt.title('Cumulative Distribution Comparison',fontsize=14)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)","fcded72e":"def plot_variable(col):\n    plt.figure(figsize=(11,5))\n    plt.subplot(1,2,1)\n    sns.distplot(train[col], norm_hist=False, kde = True, bins=50)\n    plt.axvline(train[col].mean(), color='lightgreen', linewidth=3, label='Mean')\n    plt.axvline(np.percentile(train[col],25), color='brown', linewidth=3, label='Quartiles')\n    plt.axvline(np.percentile(train[col],75), color='brown', linewidth=3)\n    plt.xlabel(col,fontsize=14)\n    plt.ylabel('Frequency',fontsize=14)\n    plt.title('Histogram of ' +col,fontsize=14)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.legend(fontsize=14)\n     \n    plt.subplot(1,2,2)    \n    sns.boxplot(y=train[col])\n    \n    ","d63529ca":"plot_variable('TotalBsmtSF')","9086564b":"plot_variable('GrLivArea')","00edb161":"plot_variable('1stFlrSF')","a1710803":"plot_variable('2ndFlrSF')","4df0bf2f":"plot_variable('GarageArea')","de87cad7":"plot_variable('BsmtFinSF1')","5178ad3a":"plot_variable('MasVnrArea')","0e228dc2":"plot_variable('LotFrontage')","b5bb63d9":"plot_variable('LotArea')","b7a610f8":"plot_variable('BsmtFinSF1')","b4ac4ed2":"plot_variable('BsmtUnfSF')","9e41963a":"plot_variable('WoodDeckSF')","d7ea0163":"plot_variable('OpenPorchSF')","9229f2c9":"outliers2=train[(train['BsmtFinSF1']>5000)  | (train['LotFrontage']>300) |  (train['TotalBsmtSF']>5000) | (train['1stFlrSF']>5000) ].index.to_list()","6c05b764":"# Calculating correlation coefficients between SalePrice and Numerical independent variables\ntrain[numcollist].corr()['SalePrice'].sort_values(ascending=False)","e51793c5":"#Standardising dataframe.\nscaleddf=(train[numcollist]-train[numcollist].mean())\/train[numcollist].std()\n","24748a46":"# Regression Scatter plots of 'Saleprice with numerical independant variables from Standardised dataframe using PairGrid'\ng = sns.PairGrid(data=scaleddf, x_vars=numcollist[0:6],  y_vars='SalePrice')\ng.map(sns.regplot)\nplt.xlim((-1,10))\nplt.ylim((-3,8))\n","120b9da5":"g = sns.PairGrid(data=scaleddf, x_vars=numcollist[6:12],  y_vars='SalePrice')\ng.map(sns.regplot)\nplt.xlim((-1,10))\nplt.ylim((-3,8))","4f9ba042":"g = sns.PairGrid(data=scaleddf, x_vars=numcollist[12:18],  y_vars='SalePrice')\ng.map(sns.regplot)\nplt.xlim((-1,10))\nplt.ylim((-3,8))","9a2ca30e":"# Plotting heatmap\nplt.figure(figsize=(18,18))\nsns.heatmap(train[numcollist].corr(),cmap='coolwarm',annot=True)\nplt.title('Correlation Matrix of Numeric variables',fontsize=12)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)","c5e095c4":"# Plotting clustermap\nplt.figure(figsize=(18,18))\nsns.clustermap(train[numcollist].corr(), cmap='rainbow')\nplt.title('Correlation Clustermap of Numeric variables',fontsize=12)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n","5f0bc846":"# Plotting regression plot of GrLivArea and SalePrice.\nsns.regplot('GrLivArea', 'SalePrice', data=train)\nplt.xlabel('GrLivArea',fontsize=14)\nplt.ylabel('SalePrice',fontsize=14)\nplt.title('Regression plot of GrLivArea and SalePrice',fontsize=14)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\n","bd1e5e40":"# Creating new variable\ntrain['newsqvar']=train['TotalBsmtSF']+train['1stFlrSF']+train['GrLivArea']\n# Regression plot of SalePrice with new variable post log transformation\nsns.regplot(np.log(train['newsqvar']), np.log(train['SalePrice']), data=train)\nplt.xlabel('newsqvar',fontsize=14)\nplt.ylabel('SalePrice',fontsize=14)\nplt.title('Regression plot of SalePrice with new variable post log transformation',fontsize=14)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\n","fdd1db14":"#Calculating correlation of newsqvar with SalePrice\ntrain[['SalePrice','newsqvar']].corr()","e9d9f2f1":"# The features need to be adjusted as follows\ndroplist=['TotalBsmtSF','1stFlrSF','GrLivArea']\ntrain=train.drop(droplist, axis=1)\nfor col in droplist:\n    numcollist.remove(col)\nnumcollist.append('newsqvar')\n","70265888":"#Plotting histogram\nplt.figure(figsize=(10,6))\nplt.subplot(1,2,1)\nsns.distplot(train['newsqvar'], norm_hist=False, kde = True, bins=50)\nplt.xlabel('newsqvar',fontsize=12)\nplt.ylabel('Frequency',fontsize=12)\nplt.title('Histogram of Newsqvar',fontsize=12)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.subplot(1,2,2)\nsns.distplot(np.log(train['newsqvar']), norm_hist=False, kde = True, bins=50)\nplt.ylabel('Frequency')\nplt.xlabel('newsqvar',fontsize=12)\nplt.ylabel('Frequency',fontsize=12)\nplt.title('Histogram of Newsqvar',fontsize=12)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n","3512df3b":"def plot_catvariable(col):\n    plt.figure(figsize=(12,8))\n    sns.swarmplot( col,  'SalePrice', data=train)\n    plt.xlabel(col,fontsize=12)\n    plt.ylabel('SalePrice',fontsize=12)\n    plt.title('Swarm Plot of SalePrice vs '+col,fontsize=12)\n    plt.xticks(rotation=60,fontsize=12)\n    plt.yticks(fontsize=12)\n    ","86fa122e":"plot_catvariable('Neighborhood')","e885cfd0":"plot_catvariable('ExterQual')","d9bacb2e":"plot_catvariable('ExterCond')","8c60ad16":"plot_catvariable('Foundation')","c8bc78c7":"plot_catvariable('BsmtQual')","4bfda071":"plot_catvariable('HeatingQC')","773d9e9a":"plot_catvariable('CentralAir')","ddbaae19":"plot_catvariable('BsmtFinType1')","913d2117":"plot_catvariable('KitchenQual')","0b5dbcaf":"plot_catvariable('FullBath')","49b092a0":"plot_catvariable('BedroomAbvGr')","00b1ee67":"plot_catvariable('TotRmsAbvGrd')","14d9caff":"# Creating new variables out of numeric variables that reflect absence or presence of zero.\nfor col in numcollist:\n    if 0 in train[col].values:\n        #print(col)\n        train['zero'+col]=np.where(train[col]==0,1,0)\n        categlist.append('zero'+col)\n# Nan Imputation\ntrain[numcollist]=train[numcollist].replace(0, np.nan).interpolate(kind='linear',limit_direction='both')\ntrain[categlist]=train[categlist].fillna('Absent')","815a4ab9":"# Function for assigning numeric labels for categorical variables\ndef train_category_conversion(col):\n    a=train.groupby(col)['SalePrice'].mean()\n    a=round((a\/10000),2)\n    index=a.index.values.tolist()\n    weight=a.values.tolist()\n    zipped=list(zip(index,weight))\n    dict1=dict(zipped)\n    train[col]=train[col].map(dict1)\n\n# Function for labelling categorical variables with numeric values for test data.\ndef test_category_conversion(col):\n    a=test.groupby(col)['SalePrice'].mean()\n    a=round((a\/10000),2)\n    index=a.index.values.tolist()\n    weight=a.values.tolist()\n    zipped=list(zip(index,weight))\n    dict1=dict(zipped)\n    test[col]=test[col].map(dict1)\n","547f4f0e":"# Passing function to provide numeric labels to categorical variables\nfor col in categlist:\n    train_category_conversion(col)","f305356d":"# We create Lasso Regression  object with tuned alpha and fit the data. The data is converted to its logarithmic equivalent.\nfrom sklearn.linear_model import Lasso\nX=np.log(train.drop(['SalePrice', 'Id'], axis=1))\ny=np.log(train['SalePrice'])\nlasso=Lasso(alpha=.001 , random_state=40)\nlasso.fit(X,y)\n\n","d51a4213":"# Now we create a dataframe of coefficients and view the top 20 of them\nlasso_features=pd.DataFrame({'features': X.columns, 'coefficients': lasso.coef_}).sort_values(by='coefficients', ascending=False)\nlasso_features.head(37)\n\n","4be9a7b3":"# We create a list of features whise coefficients are non zero.\nfeaturecols=lasso_features[lasso_features['coefficients']!=0]['features'].values.tolist()\nlen(featurecols)","6c996224":"plt.figure(figsize=(12,12))\nax=sns.barplot(x='coefficients',y='features', data=lasso_features.head(20) )\nax.set_xlabel('coefficients', fontsize=14)\nax.set_ylabel('features', fontsize=14)\nax.tick_params(axis=\"y\", labelsize=18)\nax.set_title('Lasso Features- Highest coefficients', fontsize=14)\n","62ac7a71":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols","0fdcbebc":"for col  in featurecols:\n    print(col,' + ', end='')","e9a73bc5":"#Creating a dataframe olslogtrain\nlogtrain=np.log(train)\nolslogtrain=logtrain\nolslogtrain.rename(columns={\"1stFlrSF\": \"FirstFlrSF\", \"2ndFlrSF\": \"SecondFlrSf\",\"3SsnPorch\": \"ThreeSsnPorch\" }, inplace=True)","1612bd55":"# Fitting ols model to data\nolsmodel1=ols('SalePrice ~  MSZoning  + LotArea  + Neighborhood  + Condition1  + OverallQual    + YearRemodAdd       + BsmtExposure    + BsmtUnfSF  + HeatingQC  + CentralAir    + FullBath  + HalfBath  + BedroomAbvGr  + KitchenQual  + TotRmsAbvGrd  + Functional      + GarageCars    + WoodDeckSF    + SaleCondition  + newsqvar ',olslogtrain).fit()\nprint(olsmodel1.summary())","8c14bdb9":"# The list of features are reduced on the basis of above findings\ndropcols=['MSSubClass','MasVnrArea' , 'LotFrontage','YearBuilt','Exterior1st','BsmtQual','BsmtFinSF1','2ndFlrSF' ,'Fireplaces','FireplaceQu','GarageYrBlt','GarageFinish','GarageQual','GarageCond' ,'EnclosedPorch' ,'ScreenPorch' ,'MiscVal']  \nfor col in dropcols:\n    featurecols.remove(col)\n","47cb6653":"# Adjusting Lasso_features dataframe to view only statistically significant features.\nlasso_features=lasso_features[lasso_features['coefficients']!=0]\nlasso_features=lasso_features.set_index('features')\nlasso_features=lasso_features.drop(dropcols, axis=0)\nlasso_features=lasso_features.reset_index()\n# Let's plot the top 20 statistically significant features as per their coefficients.\nplt.figure(figsize=(12,12))\nax=sns.barplot(x='coefficients',y='features', data=lasso_features.head(20) )\nax.set_xlabel('Coefficients', fontsize=14)\nax.set_ylabel('Features', fontsize=14)\nax.tick_params(axis=\"y\", labelsize=18)\nax.set_title('Features - statistically significant', fontsize=14)\n","fb821f2e":"featurecols","2e7e9e01":"# Deleting outliers.  \n\ntrain_lm=train.drop(outliers2, axis=0)\n","84d660ca":"# Creating a log object and transforming Features and target variable\nfrom sklearn.preprocessing import FunctionTransformer\nlogfunc=FunctionTransformer(np.log, inverse_func=np.exp)\nlogX_lm=logfunc.fit(train_lm[featurecols].values)\nlogy_lm=logfunc.fit(train_lm['SalePrice'].values)\nX_train_lm=logX_lm.transform(train_lm[featurecols].values)\ny_train_lm=logy_lm.transform(train_lm['SalePrice'].values)","29cc9a63":"# with dropped outliers\nlogX_netrf=logfunc.fit(train.drop(outliers2).drop(['SalePrice','Id'], axis=1).values)\nlogy_netrf=logfunc.fit(train.drop(outliers2)['SalePrice'].values)\nX_train_netrf=logX_netrf.transform(train.drop(outliers2).drop(['SalePrice','Id'], axis=1).values)\ny_train_netrf=logy_netrf.transform(train.drop(outliers2)['SalePrice'].values)\n","f6829820":"#Reading Test data and merging it with sample_submission \ntest=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf2=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ntest=pd.merge(test,df2, on='Id')\n","ec6ffd07":"# Creating New variable'newsqvar' and amending the dataframe\ntest['newsqvar']=test['TotalBsmtSF']+test['1stFlrSF']+test['GrLivArea']\ndroplist=['TotalBsmtSF','1stFlrSF','GrLivArea']\ntest=test.drop(droplist, axis=1)","3a0f9ca4":"# Creating new variables out of numeric variables that reflect absence or presence of zero.\nfor col in numcollist:\n    if 0 in test[col].values:\n        test['zero'+col]=np.where(test[col]==0,1,0)\n","10547e3c":"# Nan Imputation \ntest[numcollist]=test[numcollist].replace(0, np.nan).interpolate(kind='linear',limit_direction='both')\ntest[categlist]=test[categlist].fillna('Absent')\n","7c1cdbf1":"# Perform numeric labelling of categorical variables for test data\n\nfor col in categlist:\n    test_category_conversion(col)","adddb4c2":"# Create features and target set from test data\nX_test_lm=logX_lm.transform(test[featurecols].values)\ny_test_lm=logy_lm.transform(test['SalePrice'].values)","8678896e":"# with dropped outliers\n\nX_test_netrf=logX_netrf.transform(test.drop(['SalePrice','Id'], axis=1).values)\ny_test_netrf=logy_netrf.transform(test['SalePrice'].values)","4b321054":"# Score function parameter\nfrom sklearn.metrics import mean_squared_error, make_scorer\nmean_sq_err=make_scorer(mean_squared_error)","5e409f70":"# Create and fit Linear Regression object and compute cross validated score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nlm1=LinearRegression()\nlm1.fit(X_train_lm,y_train_lm)\ncv_results=cross_val_score(lm1, X_train_lm, y_train_lm, scoring=mean_sq_err, cv=5)\nprint('The cross validated RMSE of Linear Regression Model- lm1, on train set is: ',round(np.sqrt(np.mean(cv_results)),3))","7f598f42":"prediction=lm1.predict(X_train_lm)\nprint('The RMSE i.e root mean squared error on Train set is : ', round(np.sqrt(mean_squared_error(y_train_lm, prediction)),3))\nprediction=lm1.predict(X_test_lm)\nprint('The RMSE i.e root mean squared error on Test Set is: ', round(np.sqrt(mean_squared_error(y_test_lm, prediction)),3))","7e5c391c":"# We perform Grid serach cv for hyperparameter tuning\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid={'alpha':[0,.1,.3,.5,1,10,50]}\nridg_cv=GridSearchCV(Ridge(),param_grid,scoring=mean_sq_err, cv=5 )\nridg_cv.fit(X_train_lm, y_train_lm)\n\nprint(' Optimum value of alpha for Ridge Regression Model is: ' , ridg_cv.best_params_)\nprint(' The cross validated RMSE for Ridge Regression Model, on train set, using optimum value for alpha is :', np.sqrt(ridg_cv.best_score_).round(3))","065f0587":"# We create Ridge Regression object using optimum value for alpha and fit on train set.\n\nridg=Ridge(alpha =50)\nridg.fit(X_train_lm, y_train_lm)\n\nprediction=ridg.predict(X_train_lm)\nprint('The RMSE for Ridge Regression Model on train set is :  ', round(np.sqrt(mean_squared_error(y_train_lm, prediction)),3))\n\nprediction=ridg.predict(X_test_lm)\nprint('The RMSE for Ridge Regression Model on test set is :  ', round(np.sqrt(mean_squared_error(y_test_lm, prediction)),3))\n","9108303a":"# Let's apply SVM to the dataset. Here we use default parameters.","4e137700":"from sklearn.svm import SVR\nsvm_reg=SVR()","1168057f":"svm_reg.get_params","26d15cc9":"svm_reg.fit(X_train_netrf, y_train_netrf)\nprediction= svm_reg.predict(X_train_netrf)\n\ncv_results=cross_val_score(svm_reg, X_train_netrf, y_train_netrf, scoring=mean_sq_err, cv=5)\nprint('The cross validated for SVR model on Train set is: ',round(np.sqrt(np.mean(cv_results)),3))\nprint('The RMSE for SVR model on train set is :  ', round(np.sqrt(mean_squared_error(y_train_netrf, prediction)),3))\nprediction= svm_reg.predict(X_test_netrf)\nprint('The RMSE for SVR model on test set is :  ', round(np.sqrt(mean_squared_error(y_test_netrf, prediction)),3))","ab16a5c6":"# Finding the optimum hyperparameters\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\nparam_grid={'max_features':[5,10,15,25], 'max_samples':[200, 300, 500, 700, 1000, 1168],'n_estimators':[100, 500, 1000,1500],'min_samples_split':[2,6,8,10,12,15]}\nmean_sq_err=make_scorer(mean_squared_error)\n\nrf_cv=RandomizedSearchCV(RandomForestRegressor(),param_grid,scoring=mean_sq_err, cv=5, random_state=42 )\nrf_cv.fit(X_train_netrf,y_train_netrf)\n\nprint(' Optimum values of hyperparameters for RandomForest model are: ' , rf_cv.best_params_)\nprint(' The cross validated score on train set for RandomForest model is : ' , np.sqrt(rf_cv.best_score_).round(3))\n","f9a9326c":"from sklearn.ensemble import RandomForestRegressor","284f96dc":"# Create and fit Random Forest with optimum hyperparameters\nrf =RandomForestRegressor(n_estimators=500,min_samples_split= 6,max_samples=300,max_features=5, random_state=42)\nrf.fit(X_train_netrf, y_train_netrf)\n\nprediction=rf.predict(X_train_netrf)\nprint('The RMSE for Random Forest Model on train set is :  ', round(np.sqrt(mean_squared_error(y_train_netrf, prediction)),3))\n\nprediction=rf.predict(X_test_netrf)\nprint('The RMSE for Random Forest Model on test set is :  ', round(np.sqrt(mean_squared_error(y_test_netrf, prediction)),3))\n\nfinal_prediction=prediction","f216d938":"# Create dataframe of Feature importances and list the top 20 features\nfeature_list=train.drop(['SalePrice', 'Id'], axis=1).columns\ndf=pd.DataFrame({'Feature_name': feature_list, 'Feature_Importance':rf.feature_importances_.round(2) })\ndf=df.sort_values(by='Feature_Importance', ascending=False).head(20)","50661f6f":"plt.figure(figsize=(12,12))\nax=sns.barplot(x='Feature_Importance',y='Feature_name',data=df.head(20))\nax.set_xlabel('Feature_Importance', fontsize=14)\nax.set_ylabel('Feature_name', fontsize=14)\nax.tick_params(axis=\"y\", labelsize=18)\nax.set_title('Important Features- Random Forest Model', fontsize=14)\n","1ea4b374":"# Finding the optimum hyperparameters\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\nparam_grid={'min_samples_leaf':[2,5,10,15],'n_estimators':[100,200, 500],'min_samples_split':[2,6,8,10,12,15], 'learning_rate':np.linspace(.05,.15,5), 'max_depth': [2,4,6,8,10,12,15], 'max_features':[5,10,15] }    \nmean_sq_err=make_scorer(mean_squared_error)\n\ngb_cv=RandomizedSearchCV(GradientBoostingRegressor(),param_grid,scoring=mean_sq_err, cv=5, random_state=3 )\ngb_cv.fit(X_train_netrf,y_train_netrf)\n\nprint(' Optimum values of hyperparameters for Gradient Boosting model are: ' , gb_cv.best_params_)\nprint('The cross validated score for Gradient Boosting model on train set is', np.sqrt(gb_cv.best_score_).round(3))","182e25ca":"from sklearn.ensemble import GradientBoostingRegressor","ed26ed2b":"# Create and fit Gradient Boosting Regressor with optimum hyperparameters\ngb=GradientBoostingRegressor(n_estimators= 100, min_samples_split= 10, min_samples_leaf=10, max_depth= 2, learning_rate= 0.1, max_features=5, random_state=3)\ngb.fit(X_train_netrf, y_train_netrf)\nprediction=gb.predict(X_train_netrf)\nprint('The RMSE for Gradient Boosting model on train set is :  ', round(np.sqrt(mean_squared_error(y_train_netrf, prediction)),3))\nprediction=gb.predict(X_test_netrf)\nprint('The RMSE for Gradient Boosting model on test set is :  ', round(np.sqrt(mean_squared_error(y_test_netrf, prediction)),3))\n","fcb9e2a2":"submission=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","4e5c6595":"saleprice=logy_netrf.inverse_transform(final_prediction)","3c77f897":"np.sqrt(mean_squared_error(np.log(submission['SalePrice'].values[1000:1400]), np.log(saleprice)[1000:1400]))","5556ef09":"np.sqrt(mean_squared_error(submission['SalePrice'].values, saleprice))","451285df":"submission['SalePrice']=saleprice","a67f5f65":"submission.to_csv('sample_submission1.csv', index=False)","3fe6041b":"![image.png](attachment:image.png)","a06c8a9d":"__Observations on correlation__:\nThere are lot of variables which have a correlation with each other.\n1. TotalBsmtSF nad 1stFlrSF have a high correlation with each other i.e .88.\n2. GrLivArea and  TotalBsmtSF have correlation of .61.\n3. There is multicolinearity between variables- MasVnrArea, TotalBsmtSF, 1stFlrSF, GrLivArea and GarageArea","a7f0c0f3":"__Linear regression__ is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables).\nLinear regression models are often fitted using the least squares approach.\n\n__Ordinary Least Squares:__\nWhen we have more than one input we can use Ordinary Least Squares to estimate the values of the coefficients.\nThe Ordinary Least Squares procedure seeks to minimize the sum of the squared residuals. This means that given a regression line through the data we calculate the distance from each data point to the regression line, square it, and sum all of the squared errors together. This is the quantity that ordinary least squares seeks to minimize.\nThis approach treats the data as a matrix and uses linear algebra operations to estimate the optimal values for the coefficients.\n\nThere are extensions of the training of the linear model called *regularization methods*. These seek to both minimize the sum of the squared error of the model on the training data (using ordinary least squares) but also to reduce the complexity of the model (like the number or absolute size of the sum of all coefficients in the model).\n\nTwo popular examples of regularization procedures for linear regression are:\n\n__Lasso Regression:__ where Ordinary Least Squares is modified to also minimize the absolute sum of the coefficients (called L1 regularization).\n\n__Ridge Regression:__ where Ordinary Least Squares is modified to also minimize the squared absolute sum of the coefficients (called L2 regularization).\nThese methods are effective to use when there is collinearity in your input values and ordinary least squares would overfit the training data.\n\nNow that you know some techniques to learn the coefficients in a linear regression model, let\u2019s look at how we can use these models to make predictions on housing data.\n","b41c25c1":"The average Saleprice is differentiated in between categories.The count of Ex category is highest. This category has high SalePrice points as well.","63b2b59f":"# MODELS","13fdbeda":"From the plot above, we can identify the residual points above .25 and below -.25 as outliers. The same standard will be applied to test data.","66c79954":"# HOUSING PRICES: Advanced Regression Techniques","783a3880":"## Outliers","f3db6148":"## Creating X (Features) and y (target variable) ","8ca5af23":"### Log Transformation of numeric variables\nIf we transform these variables to its logarithmic equivalent, they will be closer to a normal distribution and have reduced skewness. The influence of the high value outliers will also be reduced","e6c5dcfb":"## Missing Values","e470be71":"The year related columns : 'YearBuilt','YearRemodAdd','GarageYrBlt' , have no significance in numerical calculations. However it is important as it arranges data as per time to reflect trends. Here we have included year column in categorical list for conversion to represent a numeric value. This numeric value itself captures the trend.  ","0a81d17f":"The average SalePrice in N category is lower and the counts in this category is far lower.","8526a4b2":"### Missing values and zero values: Numeric columns","b188fc08":"## Log transformation of Target Variable 'SalePrice'","791ccd28":"![image.png](attachment:image.png)\n","b8a520d7":"__Observations__\nThe distribution of 'SalePrice' is very close to lognormal distribution.\nThe tails are matching, though 'SalePrice' appears to be bimodal.\nThe the range above upper quartile has normalised to quiet an extent.\nThe cumulative distribution of Saleprice is very close to cumulative lognormal distribution.","1cda1ac6":"## Lasso regularisation for feature selection.\nNow we have a train set which is cleaned and transformed. We need to select few relevant features. This helps in reducing cost. For this we will apply Lasso Regularisation.\n\nLasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.\nIt forces the sum of the absolute value of the regression coefficients to be less than a fixed value, which forces certain coefficients to be set to zero, effectively choosing a simpler model that does not include those coefficients.","f88991f3":"The categories Gd, TA and Ex have differentiated SalePrice range and Average price with Ex having the highest Average SalePrice and Gd having the highest counts. Few outliers are there in category Ex.","82946a6e":"This variable has different Sale Price range and average SalePrice in 3 categories- 1,2, and 3, with 3 having the highest Avg SalePrice followed by category 2. Counts are highest for category 1 followed by category 2.","5d041c1b":"## 'SalePrice' Relationship with Independent categorical variables","f67ff55c":"## Creating X (Features) and y (target variable)  for test set.\nWe transform the Test data to its logarithmic equivalent using the log transformation object created. Before this step the outliers need to be taken care of. For identifying outliers this we have used the same criteria as we used for train data.","b855b8ef":"The SalePrice range in the 3 categories- Gd,TA and Ex differs. The counts of Gd category is the highest, followed by TA. The average SalePrice of Ex category is highest, followed by Gd category, and also shows some outliers.","3fdf6d3e":"\nFirst let's create a Linear Regression Object and fit to train data.\nThe score for evaluating performance is Root Mean Square Error between log of Sale Price and log of Predictions.\nWe will use 5 fold cross validation on train set.","04a5f88d":"The range in SalePrice and average SalePrice is differentiated in the 3 categories-PConc, CBlock, and BrkTil. The counts vary, with PConc count leading and followed by CBlock. Very high SalePrice points are seen in category PConc.","1835201f":"Lets take a look at the distribution of 'SalePrice' to identify outliers.","7a65945c":"So as seen above, there are unique values far less than the number of observations. These are ordinal variables.There is no point in doing any numeric calculations with them so we place all variables with categories less than 16 to ordinal list. For the time being columns representing year is kept seperate.","521abb07":"__Analysis of GrLivArea:__\n\nWe have seen earlier that some variables have high correlation with each other and linear relation with SalePrice is also similar. \n\nOn this basis we create a new variable- 'newsqvar' which is a combination of 'TotalBsmtSF','1stFlrSF' and 'GrLivArea'. \nLet's take the log value of this variable and see the rgression plot with SalePrice.","733ebc40":"__Observations__\n\n1. TotalBsmtSF and FirstFlrSF have a correlation of .6 with SalePrice though relationship is somewhat linear. Variance in SalePrice increases  as these independent variable increases , for eg we see a range from 100000 to 400000 with both these variables.Both these variables impact the average SalePrice. There is an outlier in both cases.\n\n2. The variance in SalePrice is non constant with SecondFlrSF with a weak linear relationship. It has a .3 correlation with SalePrice.\n\n3. GrLivArea and GarageArea have .7 and .58 correlation with SalePrice. \nSalePrice variance increases with higher values of GrLivArea and eventually spreads out of the group at few very high values. \nSalePrice has a constant variance,  with GarageArea and here too it has outliers.\nGrLivArea shows some linearity with SalePrice.","3e9c106a":"The categories in this variable are differentiated in terms of range in Saleprice and Average Saleprice.","dbf7338a":"We convert the SalePrice to its log equivlent, repeat the plots and see whether it conforms to normal distribution.","554274ca":"This variable has a right skewed distribution with a significating outlier","e50cae21":"__Observations:__\n\n1.The distribution is not normal. Mean Saleprice and median Saleprice, both have lower probabilities of occurence than the mode.\n2. Distribution of SalePrice is leptokurtic. An example of a leptokurtic distribution is the Laplace distribution, which has tails that asymptotically approach zero more slowly than a Gaussian.\n3. The distribution is right skewed. The range of upper 25% of data is around 60000 which is 3 times more that the Interquartile range.\n4. Mean Saleprice is not a good representation and there are quite a number of outliers.\n5. The cumulative distribution comparison makes it very clear that SalePrice does not conform to a normal pattern. There is also greater chance of SalePrice in range 100000 to 300000 occuring in comparison to what a normal distribution should have. We can also see that the range of upper 25% of Sale Price is abnormally greater than normal pattern.","e835f9f2":"### Independent variable GrLivArea and its transformation","b11020b2":"### Missing values of categorical variables","3ea64f3e":"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\nRandom forest is a supervised learning algorithm. The \"forest\" it builds, is an ensemble of decision trees, usually trained with the \u201cbagging\u201d method. The general idea of the bagging method is that a combination of learning models increases the overall result.\n\nAnother great quality of the random forest algorithm is that it is very easy to measure the relative importance of each feature on the prediction. Sklearn provides a great tool for this that measures a feature's importance by looking at how much the tree nodes that use that feature reduce impurity across all trees in the forest. It computes this score automatically for each feature after training and scales the results so the sum of all importance is equal to one.\n\nLets use this model to train and predict housing prices.\n","212e4879":"# DATA WRANGLING \n \nIn this section we will observe the data and datatypes. We also look at the Nan values and Outliers and how to deal with it.","263d89e0":"__Great! This new variable has a strong linear relation with SalePrice and a constant variance as well. The correlation also has improved from .70 to .76\nWe can also view this transformation in a histogram.__","0dd3035f":"## Observation","c21812f1":"## Finding features which are statistically significant.\nWe have 37 features which are suitable for the model. However we can further reduce this number by selecting the features which are statistically significant. This helps in reducing costs and building a model that would be accurate most of the time when applied on unseen data. We can import Ols from stats model which gives us the p values of features.","1438e888":"Here we creat seperate columns indicating absence or presence of a numerical featrue.\nThis code will be passed after further Analysis.\n\nfor col in numcollist:\n\n    if 0 in train[col].values:\n        train['zero'+col]=np.where(train[col]==0,1,0)\n        categlist.append('zero'+col)\n        \n        \n        \n","b117123f":"## RIDGE REGRESSION MODEL","2e5581aa":"__Observations:__\n1. Features that have p values greater than or equal to .05 have been seperated.\n2. All remaining features above have p_values lesser than .05 and hence are statisticaly significant.\n3. R^2 is 89% which shows how much variance in data is explained by model. The higher this value, the better predictor the model is.\n4. P value of F-statistic is 0, which shows that the fitted model is statistically significant.","4bb5fc2c":"We need to set the feature and target variable so that models can be trained on them.","8ddc65f4":"In LotArea, as seen above, there are many high values. This does not look like an error, it can be an outlier.","44f236d0":"We classify data types into 2 classes i.e numeric and categoric. We can assign the variables to seperate lists. We also need to identify ordinal variables and decide which class we can place them in.","edb5e113":"In the first figure, we can see that the distribution is not normal as it is right-skewed and has infrequent high-value outliers. The boxplot shows few observations in the outlier range with and a single observation taking the maximum value of 6000. Almost 50% of the range is represented by high value points. ","df295f98":"This variable has many missing values and has maximum observations in the outlier section. ","5e7c01ac":"### Problem\n\nPrediction of sale prices:\n\nLet\u2019s say that a buyer is interested in purchasing a house. He has an estimate of the price of the house and has an offer in mind. The price estimation might have been based on few factors or external sources such as real estate agencies. The problem for the buyer is knowing the exact amount for the purchase price of the house. For a real estate company, which can also pose as a buyer or broker, the problem is to negotiate for the best deal. This dataset has several factors. It becomes crucial to know the levers that drive the price and develop a model to predict them with best accuracy.\n\nAs reiterated by my mentor and industry experts,__we spend atleast 80% time in wrangling__. Here I have elaborated the steps for cleaning data and handling minute details such as zero values.\n\nOne of the essential requirements before training models is doing a __Exploratory data Analysis__. Indeed, there are a lot of helpful insights that can be drawn by simply exploring all aspects in data:- target variable, independent variables and their relationships.\n\n### Process\n\nData cleaning to handle Nan values and outliers.\n\nData observation to understand the scope challenges, estimators data types and central tendencies.\n\nData visualisation to look at the distribution and other aspects of features and target variable.\n\nExploratory Data Analysis to understand the correlations of features among itself and with the target variable.\n\nFit the Machine Learning Models and predict prices.\n\n### Metric\n\nModel will be evaluated on Root mean squared error(RMSE) between logarithm of predicted value and logarithm of sale price.\n\n\n### Score\n\nThe score varies from __.083 to .138__ with different models. The Least score is with __Random Forest Regression__ model which is .083  followed by Gradient Boosting Regressor which is .091. Ridge Model and SVR have also done done a good job.","c59d9075":"__What is Support Vector Machine?__\n\u201cSupport Vector Machine\u201d (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well.\n\nSupport Vector Regression (SVR) uses the same principle as SVM, but for regression problems.\n\n![image.png](attachment:image.png)","f3e33e31":"__Observations__:\nWoodDeckSF and OpenPorchSF have .3 correlation with SalePrice. Again variance in SalePrice is not constanct. SalePrice has outliers in both cases. There is no linear relationship. ","03f9994e":"Average SalePrice is much lower where OpenPorchSF is not present","3ce3c0a6":"This variable has a right-skewed distribution and with outliers in the top 50% of its range. ","f63cc36c":"We will look at the following:\n1. Distribution plot of SalePrice alongside kde plot of a normal distribution. \n2. Develop a cumulative distribution of Sale Price alongside cumulative normal distribution.\n","08baaef4":"This variable has maximum counts spread between categories 2,3,4 wit 3 being highest in count. Spread in Saleprice and Average SalePrice varies slightly between these categories.","6c7eb8a0":"## LINEAR REGRESSION MODEL","40fb4bee":"Similar to TotalBsmtSF, 1stFlrSF has a right-skewed distribution and with outliers in the top 50% of its range. .","03a3e782":"\nNan Imputation method for numeric columns: The following nan imputation will be done after the variable transformations.\n    \n    \ntrain[numcollist]=train[numcollist].replace(0, np.nan).interpolate(kind='linear',limit_direction='both')","545c50e5":"A first look of the data sample..","3a86e9c8":" The Average SalePrice and range differs in the 3 categories GLQ, ALQ and Unf, with GLQ leading and having almost equal counts with Unf. GLQ, ALQ and Unf category also have presence of outliers.","76f6d9f6":"The categories TA and Gd almost have a equal counts. The average SalePrice of Gd is higher than TA. There are high prized houses in Gd and Ex category, which look like outliers.","bb71bc35":"Imputing Nan:\nNan Imputation menthod is given below, which will be passed after further analysis.\n\n\ntrain[categlist]=train[categlist].fillna('Absent')","7705dd6a":"The distribution of this variable is right-skewed with high value outliers. ","d851ef5e":"Observations:\n1. There are 22 values above zscore of 3.\n2. Before taking a decision to remove or transform these outliers we looked at the linear relationship between Sale Price and GrLivArea.\n3. Out of these 22 values most of the values are following a linear trend with GrLiveArea.But these values are high. If we normalize the data, this could be taken care of. However 2 points at bottom right are not following the trend. So these 2 points are outliers.","40fec8c5":"This variable has a right-skewed distribution with many high value points, lying significantly far from its upper quartile. ","bb5aa988":"### Transforming labels of categorical variables.\nBasis our observations we find that there are categories, sensitive to Average Saleprice. For incluing these variables in the prediction models we need to assign them numerical options. Here, have assigned the labels based on average SalePrice.\nThere are also other options available with OneHotEcoder and LabelEncoder for this job.","35dc01b5":"## GRADIENT BOOSING REGRESSOR\n\n\n__\"Boosting\"__ in machine learning is a way of combining multiple simple models into a single composite model. This is also why boosting is known as an additive model, since simple models (also known as weak learners) are added one at a time, while keeping existing trees in the model unchanged. As we combine more and more simple models, the complete final model becomes a stronger predictor. The term \"gradient\" in \"gradient boosting\" comes from the fact that the algorithm uses gradient descent to minimize the loss.\n\nWhen gradient boost is used to predict a continuous value \u2013 like age, weight, or cost \u2013 we're using gradient boost for regression. \n\n__Decision trees are used as the weak learners in gradient boosting.__ Decision Tree solves the problem of machine learning by transforming the data into tree representation. Each internal node of the tree representation denotes an attribute and each leaf node denotes a class label. The loss function is generally the squared error (particularly for regression problems). The loss function needs to be differentiable.\n\n Gradient boosting Regression calculates the difference between the current prediction and the known correct target value.\nThis difference is called residual. After that Gradient boosting Regression trains a weak model that maps features to that residual. This residual predicted by a weak model is added to the existing model input and thus this process nudges the model towards the correct target. Repeating this step again and again improves the overall model prediction.\n\nAlso it should be noted that Gradient boosting regression is used to predict continuous values like house price , while Gradient Boosting Classification is used for predicting classes like whether a patient has a particular disease or not.\n\n\n*The high level steps that we follow to implement Gradient Boosting Regression is as below:*\n\nSelect a weak learner\nUse an additive model\nDefine a loss function\nMinimize the loss function\n","0cac3ebf":"Let's further take a look at how SalePrice points behave with a correlated variable.","d82cfa5e":"There are 1460 observations. There are variables of int,float and object dtypes.","a41805c1":"This variable also has a  right-skewed distribution. The boxplot shows a single point of value 350 in comparison to the 75th obsevation of around 110. ","1de3574a":"The variable is right skewed and has very high value outliers. Almost 70% of the range is on its right tail.The boxplot shows that almost 70% of observations are outliers. ","9dfdc72a":"![image.png](attachment:image.png)","6870613f":"This variable has a bimodal frequency and is right skewed with high value outliers. The boxplot shows that the outlier points are many to deal with.","352d9477":"In MiscVal, these top 2 values seem to be outiers. ","6e63dec8":"In this part of the Capstone Project, we will train the dataset with suitable Machine Learning Models on train set and predict the target variable i.e Sale Price for Test set.  ","a456ddc5":"Average SalePrice is slightly more where BsmtSF2 is not present","a5f69b28":"# EXPLORATORY DATA ANALYSIS OF AMES HOUSING DATASET\n\nIn this notebook, we will analyse the target variable and its relationship with independant variables. This will help in selecting predictive variables. ","13eeff4e":"![image.png](attachment:image.png)","b817763d":"### Models\n\nModels used to train and predict the prices are:\n    Linear Regression\n    Ridge Regression\n    Random Forest\n    SVM-Regressor\n    Random Forest Regressor\n    Gradient Boosting Regressor","1cdf2628":"This variable has some high value outliers grabbing almost 66% of the range.The boxplot shows that these observations are few with a single highest point of around 6000, far from the crowd. The log transformation has reduced the influence of outliers.","392620cb":"Let's create a Ridge Regression Object and fit to train data. The score for evaluating performance is Root Mean Square Error between log of Sale Price and log of Predictions. First we will use Grid Search cross validation on train set in order to tune alpha.\n\n","99953269":"Note: There are certain types of column names that are not accepted by ols model. Hence only for the purpose of this model, I am creating a seperate dataframe with some columns renamed.","02240b96":"## Classifying dtypes","cbd6a3cd":"For the Nan values we create a seperate category 'Absent'.This would help to capture any new information such as average Sale Price in case a feature is absent. Variables like PoolQC, Fence, MiscFeature and Alley have a very high Nan. These may not be useful at all. However, we cannot make similar assumptions about population data. So we retain them for whatever little information they may provide.","3e418f70":"Now how do we treat the Nan and zero values. \nObservation:\nNan values are not very high in numbers.As far as zero values are concerned, mean SalePrice differs when a numerical feature is not present in some variables.  \n1. We create a seperate variable for each numerical column and label the absence of feature i.e 0 value as 1, and presence of a feature as 0. \n2. We replace zero's with Nan and interpolate the variable by linear method using the interpolate method.","121955d7":"## Target Variable 'SalePrice'","512a153c":"## 'SalePrice' Relationship with Independent numerical variables","facfb0b6":"### TEST DATA","e9c007ce":"The following observations can be made:\n1. Mean of target variable, SalePrice is 180921.\n2. Median of target variable, SalePrice is 163000. So without looking at a histogram we can say that it is right-skewed.\n3. Considering that the 75th observation is 214000, the maximum SalePrice 755000 is abnormally high and an outlier.\n4. These observations can be made with other variables as well.\n5. Potential outliers are there in data..eg. LotArea has a unusually high value of 215245, which has pushed its mean up. We need to further check this variable. Same is te case with MiscVal.\n6. There are many 0 values in some columns.","adfb8ee4":"__Observations:__\n1. SalePrice has non constant variance with Variables LotFrontage, LotArea, MasVnrArea and BsmtFinSF1. Linear relationship cannot be clearly established. \n\n2. BsmtUnfSF  and BsmtFinSF2 both are weakly correlated with SalePrice.\n\n3. SalePrice shows weak  linear relationship with  BsmtFinSF1  and has non constant variance.","c2a01dbd":"Average 'SalePrice' is lesser where BsmtFinSF1 is not present","990e8854":"Category TA dominates in counts and has a higher average SalePrice, compared to Gd. Both reflect few outlier points","110ed37b":"Neighborhood: This variable has 25 categories. Overall it looks like each category has differentiated range in SalePrice.Most categories also have an almost equally distributed counts. Few categories look similar in spread and average SalePrice like OldTown, BrkSide and Edwards. Overall, the outliers are few except in NoRidge and NridgHT, where there are high priced houses.","75469e06":"This variable is again right-skewed. The boxplot also shows several outlier points.Log -transformation removed the skewness but shows some infrequent low value outliers","c8aa5a10":"## RANDOM FOREST","377927e5":"This variable has a right-skewed distribution with many high value points, lying significantly far from its upper quartile. ","36ec260e":"# IN-DEPTH ANALYSIS(MACHINE LEARNING)","b3d96494":"## SEARCH VECTOR MACHINE -REGRESSION","d5819818":"We can get a glance of the data and note the following:\n1. Column names and their dtypes.\n2. There are numeric values and categorical values.","d8bf26e5":"More than the problem of Nan, we have a problem of zero values. This could mean that a feature is not present. Let us take a look at the impact of a numerical feature being absent, on 'SalePrice' variable by plotting the histogram of 'Saleprice'. We are doing this so that we can understand the impact of zero values on our target variable.","84421309":"Note: We will scale the dataframe using formula (x- x.mean())\/x.std(). We are doing this for a comparative view of numeric variables in scatter plots.","5501bfd9":"Lets see how these variables are correlated wth each other.."}}