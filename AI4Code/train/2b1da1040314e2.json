{"cell_type":{"4190a31f":"code","46a5fcb9":"code","505d0468":"code","f3f3f84e":"code","01b2226e":"code","108f37e8":"code","a7b8ed17":"code","3fe61310":"code","f8110bcf":"code","acfb91ea":"markdown","03375d60":"markdown","d5a383d4":"markdown","f5c6ecdb":"markdown","99d69b6d":"markdown","10534e0a":"markdown","c83d410a":"markdown","ae8636df":"markdown","10813727":"markdown","0771635f":"markdown"},"source":{"4190a31f":"import pandas as pd\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn\nfrom sklearn import set_config\nfrom sklearn.pipeline import Pipeline\n\nimport warnings\nwarnings.simplefilter(action='ignore')","46a5fcb9":"# df_loaded\ndf_loaded = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\n\n# test_df\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")\n\n# sample_submission\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")","505d0468":"df_loaded.head(5)","f3f3f84e":"transformers = []\n\nnumerical_pipeline = Pipeline(steps=[\n    (\"converter\", FunctionTransformer(lambda df: df.apply(pd.to_numeric, errors=\"coerce\"))),\n    (\"imputer\", SimpleImputer(strategy=\"mean\"))\n])\n\ntransformers.append((\"numerical\", numerical_pipeline, [\"f0\", \"f1\", \"f10\", \"f11\", \"f12\", \"f13\", \"f14\", \"f15\", \"f16\", \"f17\", \"f18\", \"f19\", \"f2\", \"f20\", \"f21\", \"f22\", \"f23\", \"f24\", \"f25\", \"f26\", \"f27\", \"f28\", \"f29\", \"f3\", \"f30\", \"f31\", \"f32\", \"f33\", \"f34\", \"f35\", \"f36\", \"f37\", \"f38\", \"f39\", \"f4\", \"f40\", \"f41\", \"f42\", \"f43\", \"f44\", \"f45\", \"f46\", \"f47\", \"f48\", \"f49\", \"f5\", \"f50\", \"f51\", \"f52\", \"f53\", \"f54\", \"f55\", \"f56\", \"f57\", \"f58\", \"f59\", \"f6\", \"f60\", \"f61\", \"f62\", \"f63\", \"f64\", \"f65\", \"f66\", \"f67\", \"f68\", \"f69\", \"f7\", \"f70\", \"f71\", \"f72\", \"f73\", \"f74\", \"f75\", \"f76\", \"f77\", \"f78\", \"f79\", \"f8\", \"f80\", \"f81\", \"f82\", \"f83\", \"f84\", \"f85\", \"f86\", \"f87\", \"f88\", \"f89\", \"f9\", \"f90\", \"f91\", \"f92\", \"f93\", \"f94\", \"f95\", \"f96\", \"f97\", \"f98\", \"f99\", \"id\"]))","01b2226e":"preprocessor = ColumnTransformer(transformers, remainder=\"passthrough\", sparse_threshold=0)","108f37e8":"standardizer = StandardScaler()","a7b8ed17":"target_col = \"target\"\nsplit_X = df_loaded.drop([target_col], axis=1)\nsplit_y = df_loaded[target_col]\n\nX_train, X_val, y_train, y_val = train_test_split(split_X, split_y, random_state=43, stratify=split_y)","3fe61310":"set_config(display=\"diagram\")\n\nsklr_classifier = LogisticRegression(\n  C=0.00030024581542297343,\n  penalty=\"l2\",\n  random_state=102376075,\n)\n\nmodel = Pipeline([\n    (\"preprocessor\", preprocessor),\n    (\"standardizer\", standardizer),\n    (\"classifier\", sklr_classifier),\n])\n\nmodel\n\nmodel.fit(X_train, y_train)","f8110bcf":"model_pred = model.predict(test_df)\nsample_submission['target'] = model_pred\nsample_submission.to_csv('.\/submission.csv',index=False)","acfb91ea":"<a id=\"t0.\"><\/a>\n## Intoduction\n* Starter notebook to help Kagglers to get going\n* This was generated using Databricks AutoML \n* Using a simple Logistic Regression model. Logistic Regression is a supervised machine learning classification algorithm that is used to predict the probability of a categorical dependent variable.","03375d60":"<a id=\"t3.\"><\/a>\n## 3. Preview data","d5a383d4":"<a id=\"t4.\"><\/a>\n## 4. Pipeline\n### Below is a pipeline that imputes the missing values in numeric data, scales them, and fits an Logistic Regression model. \n* Numerical columns\n\n* Missing values for numerical columns are imputed with mean.","f5c6ecdb":"<a id=\"t5.\"><\/a>\n## 5. Feature standardization\n### Scale all feature columns to be centered around zero with unit variance.","99d69b6d":"<a id=\"t8.\"><\/a>\n## 8. Prediction","10534e0a":"<a id=\"t6.\"><\/a>\n## 6. Training - Validation Split\n### Split the input data into training and validation data","c83d410a":"<a id=\"t1.\"><\/a>\n## 1. Import libraries","ae8636df":"<a id=\"t2.\"><\/a>\n## 2. Load datasets","10813727":"## Notes \n\n#### Thank you for reading my notebook. Please upvote if this was helpful in any way. If you have any comments\/feedback\/suggestions, please feel free to comment.","0771635f":"<a id=\"t7.\"><\/a>\n## 7. Train classification model"}}