{"cell_type":{"33e43bc4":"code","a7fd4c80":"code","eed8908a":"code","84c91e35":"code","f68a96a8":"code","fa550907":"code","de0fecd6":"code","f4ac6fcd":"code","f221bd94":"code","520b0911":"code","22dcdc8e":"code","89c3f3a2":"code","a03843c0":"code","8c5b4f6b":"code","42dfd07a":"code","dd758e1c":"code","93d83393":"code","6fe9824f":"code","d3b79ab3":"code","8588e9d3":"markdown","13e13a33":"markdown","3147c57c":"markdown","4bc916f6":"markdown","b02ea422":"markdown","eb72df35":"markdown","bc407f34":"markdown","2ffd783d":"markdown","71168b4a":"markdown","4d9c1689":"markdown"},"source":{"33e43bc4":"#Importing all the necessary libraries and data set\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a7fd4c80":"#display the data\ndata = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndata.head()","eed8908a":"#checking 'NaN' values in the data\ndata.isnull().sum()","84c91e35":"#describing data to view various statstical variables\ndata.describe()","f68a96a8":"#Plot a count plot\nsns.countplot(data = data, x = \"Outcome\",hue = \"Outcome\")\nplt.title(\"WomenDiabetesData\")","fa550907":"#Plot Box plot\nfig, ax = plt.subplots(figsize = (5,5))\nsns.boxplot(data = data, y = \"Pregnancies\", x = \"Outcome\", hue = \"Outcome\")\nplt.title(\"Pregnancies\")","de0fecd6":"#Plot a line plot\nsns.lineplot(data = data, x = 'Age', y = 'BloodPressure', hue = 'Age')\nplt.title(\"Age and BP\")","f4ac6fcd":"#Plotting histograms for all features in the data set\nfor i in data.columns:\n    plt.figsize=(5,5)\n    plt.hist(data[i])\n    plt.title(i)\n    plt.show()","f221bd94":"#Plot scatter plot for all features in the data set\nsns.pairplot(data = data, hue = \"Outcome\")","520b0911":"#Plot heatmap and show case correlation of each features\ncorr_mat = data.corr()\nplt.figure(figsize=(12,10))\nsns.heatmap(corr_mat, annot = True, cmap = \"coolwarm\")\nplt.show()","22dcdc8e":"#Assigning X and Y\nX = data[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']]\nY = data.Outcome","89c3f3a2":"#Assigning training and test data\nfrom sklearn.model_selection import train_test_split\nX_train_orig, X_test, Y_train,Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)","a03843c0":"#Feature Scaling the data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(X_train_orig)\nX_train = sc.transform(X_train_orig)\nX_test = sc.transform(X_test)","8c5b4f6b":"from sklearn.linear_model import LogisticRegression\nlr_classifier = LogisticRegression()\nlr_classifier.fit(X_train, Y_train)\nY_pred = lr_classifier.predict(X_test)\nprint(\"Accuracy of the model:\",accuracy_score(Y_test, Y_pred))\nprint(classification_report(Y_test, Y_pred))","42dfd07a":"from sklearn.naive_bayes import GaussianNB\ngnb_classifier = GaussianNB()\ngnb_classifier.fit(X_train, Y_train)\nY_pred = gnb_classifier.predict(X_test)\nprint(\"Accuracy of the model:\",accuracy_score(Y_test, Y_pred))\nprint(classification_report(Y_test, Y_pred))","dd758e1c":"from sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier()\nrf_classifier.fit(X_train, Y_train)\nY_pred = rf_classifier.predict(X_test)\nprint(\"Accuracy of the model:\",accuracy_score(Y_test, Y_pred))\nprint(classification_report(Y_test, Y_pred))","93d83393":"from sklearn.svm import SVC\nsvm_classifier = SVC()\nsvm_classifier.fit(X_train, Y_train)\nY_pred = svm_classifier.predict(X_test)\nprint(\"Accuracy of the model:\",accuracy_score(Y_test, Y_pred))\nprint(classification_report(Y_test, Y_pred))","6fe9824f":"import tensorflow as tf \nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n#define keras model\nmodel = Sequential()\nmodel.add(Dense(12, input_dim = 8, activation = 'relu'))\nmodel.add(Dense(8, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\n#compile the keras model\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\n\n#fit the model\nmodel.summary()\nmodel.fit(X_train, Y_train, epochs = 150, batch_size = 10)","d3b79ab3":"#Predict the model\n_, accuracy = model.evaluate(X_train, Y_train, verbose = 0)\nprint(\"Accuracy of the model:\", accuracy)","8588e9d3":"# Random Forest Classifier","13e13a33":"## Correlation of each features","3147c57c":"**We can conclude that SVM and Logistic Regression model give the best results in terms of recall. \nIn any sort of medical diagnosis it is important to consider recall value and from the above results we conclude the same.**","4bc916f6":"# Support Vector Machine","b02ea422":"# Neural Network Model","eb72df35":"## Standard Scaling\n","bc407f34":"**The Pima Indian Diabetes dataset consists of women who are diagnosed with and without diabetes along with various features represented as columns that play an important role in the diagnosis. This notebook shows a comparative study between various classifier models used to perform Binary Classification on this data set. In the end we conclude which classifier model gives us the best result.","2ffd783d":"# Data Visualization","71168b4a":"# Naive Bayes Classifier","4d9c1689":"# Logistic Regression Model"}}