{"cell_type":{"4fb91129":"code","02ad8d39":"code","e330aab9":"code","54d910f5":"code","814c5cb8":"code","f9412f31":"code","f176bfde":"code","b26d2ebc":"code","b9675685":"code","7c568f05":"code","624976df":"code","c1819633":"code","02ac3a66":"code","27300779":"code","3c06741e":"code","400c6068":"code","46ad513f":"code","7e3ff56a":"code","b14a623d":"code","8054cd40":"code","f3a7f80b":"code","c2c3e275":"code","7991006c":"code","1f87196c":"code","87076994":"code","debca816":"code","f209cda5":"code","3fcebef7":"code","ab798c58":"code","2faf6d4f":"code","e2e48a48":"code","e915d066":"code","5d09a5f1":"code","4327463f":"code","355aaca6":"code","a88057fb":"code","f3fe0098":"code","61129c02":"code","56e986dc":"code","be5a4fd8":"code","a23b5423":"code","7488e41e":"code","390982b0":"code","0356a61d":"code","06cbec1b":"code","3d85dda5":"code","cc7f7d34":"code","623cc7dc":"code","6c4bbaa0":"code","4df238cc":"code","0d321824":"code","9d0574e7":"code","882bb2db":"code","0c885677":"code","10086760":"markdown","8493ea70":"markdown","852eca18":"markdown","bec500ec":"markdown","86120cec":"markdown","c1b3c101":"markdown","cbc212ff":"markdown","1a641316":"markdown","4361e5b3":"markdown","ea824a93":"markdown","f2fcc423":"markdown","8456517a":"markdown","dbdc9e93":"markdown","b114a99e":"markdown","4d7cd6fc":"markdown","476d0432":"markdown","8c3a1954":"markdown"},"source":{"4fb91129":"LABEL_SMOOTHING=0.001\n\nSEEDS = [120,2524]\nSPLITS = 7\nBATCH_SIZE = 64\nEPOCHS = 60\n\nVARIANCE_THRESHOLD = True\n\nADD_NON_TRAIN = True\nADD_SEEDS = SEEDS\nADD_SPLITS = SPLITS\nADD_BATCH_SIZE = BATCH_SIZE\n\nADD_EPOCHS = EPOCHS\n\nRUN_SNN = True\nSNN_SEEDS = SEEDS + [42]\nSNN_SPLITS = SPLITS\nSNN_BATCH_SIZE = BATCH_SIZE\nSNN_EPOCHS = EPOCHS\n\nRUN_NN = True\nNN_SEEDS = SEEDS \nNN_SPLITS = SPLITS\nNN_BATCH_SIZE = BATCH_SIZE\nNN_EPOCHS = EPOCHS\n\nRUN_NN2 = True\nNN2_SEEDS = SEEDS\nNN2_SPLITS = SPLITS\nNN2_BATCH_SIZE = BATCH_SIZE\nNN2_EPOCHS = EPOCHS\n\nRUN_RNN = True\nRNN_SEEDS = SEEDS\nRNN_SPLITS = SPLITS\nRNN_BATCH_SIZE = 128\nRNN_EPOCHS = EPOCHS\n\nRUN_TABNET = True\nTAB_SEEDS = SEEDS\nTAB_SPLITS = SPLITS\nTAB_BATCH_SIZE = BATCH_SIZE\nTAB_EPOCHS = EPOCHS\n\nRUN_STACKING = True\nSTK_SEEDS = SEEDS + [42]\nSTK_SPLITS = SPLITS \nSTK_EPOCHS = EPOCHS\nSTK_BATCH_SIZE = BATCH_SIZE\n\nRE_RUN_NN = True\nRE_NN_SEEDS = SEEDS","02ad8d39":"import pandas as pd\nimport numpy as np\nimport random\nimport os\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping","e330aab9":"import sys\nsys.path.append('..\/input\/interactivestratification\/iterative-stratification-master')\nsys.path.append('..\/input\/pytorchtabnet\/tabnet-develop')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom pytorch_tabnet import tab_network","54d910f5":"from sklearn.metrics import log_loss\nimport torch\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    torch.manual_seed(seed)  \n    session_conf = tf.compat.v1.ConfigProto(\n        intra_op_parallelism_threads=1,\n        inter_op_parallelism_threads=1\n    )\n    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n    tf.compat.v1.keras.backend.set_session(sess)\n\ndef swish(x):\n    return x * K.sigmoid(x)\n\ndef show_metrics(valid_preds, show_each=True):\n    metrics = []\n    for _target in valid_preds.columns:\n      logloss = log_loss(train_targets_scored.iloc[:,1:].loc[:, _target], valid_preds.loc[:, _target])\n      metrics.append(logloss)\n      if show_each:\n        print(f'column: {_target}, log loss: {logloss}')\n    print(f'OOF Metric: {np.mean(metrics)}')\n\n    return metrics","814c5cb8":"train_features = pd.read_csv(f'..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv(f'..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(f'..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv(f'..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv(f'..\/input\/lish-moa\/sample_submission.csv')","f9412f31":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","f176bfde":"_X_train = train_features.copy()\n_X_test = test_features.copy()\n_y_train = train_targets_scored.copy()","b26d2ebc":"from sklearn.feature_selection import VarianceThreshold\n\nif VARIANCE_THRESHOLD:\n    vt_cols = _X_train.loc[:,GENES+CELLS].columns\n\n    vh = VarianceThreshold(0.8)\n    train_trans = pd.DataFrame(vh.fit_transform(_X_train.loc[:,GENES+CELLS]))\n    vt_cols = vt_cols[vh.get_support()]\n\n\n    vt_cols_leaky = _X_train.loc[:,GENES+CELLS].columns\n\n    vh_leaky = VarianceThreshold(0.8)\n    train_trans = pd.DataFrame(vh_leaky.fit_transform(pd.concat([_X_train.loc[:,GENES+CELLS], _X_test.loc[:,GENES+CELLS]])))\n    vt_cols_leaky = vt_cols_leaky[vh_leaky.get_support()]","b9675685":"def deal_with_cp_type(_X_train, _X_test):\n    train_trt_index = _X_train.cp_type == 'trt_cp'\n    test_trt_index = _X_test.cp_type == 'trt_cp'\n\n    _X_train = _X_train[train_trt_index]\n    _X_test = _X_test\n    del _X_train['cp_type']\n    del _X_test['cp_type']\n\n    return _X_train, _X_test, train_trt_index, test_trt_index\n\n_X_train, _X_test, train_trt_index, test_trt_index = deal_with_cp_type(_X_train, _X_test)\n\n\ndef deal_with_cp_dose(_X_train, _X_test):\n    _X_train = pd.concat([_X_train, pd.get_dummies(_X_train.cp_dose)], axis=1)\n    del _X_train['cp_dose']\n    _X_test = pd.concat([_X_test, pd.get_dummies(_X_test.cp_dose)], axis=1)\n    del _X_test['cp_dose']\n\n    return _X_train, _X_test\n\n_X_train, _X_test = deal_with_cp_dose(_X_train, _X_test)\n\n\ndef deal_with_sig_id(_X_train, _X_test):\n    del _X_train['sig_id']\n    del _X_test['sig_id']\n\n    return _X_train, _X_test\n\n_X_train, _X_test = deal_with_sig_id(_X_train, _X_test)\n\n\ndef deal_with_y(_y_train, train_trt_index):\n    _y_train = _y_train[train_trt_index]\n    del _y_train['sig_id']\n\n    return _y_train\n\n_y_train = deal_with_y(_y_train, train_trt_index)\n\n# I guess 'D1' should be eliminated but anyway, I was using it. \n\nBASE_COLS = ['cp_time', 'D1', 'D2']\n\n_X_train_dae = _X_train.copy()\n_X_test_dae = _X_test.copy()","7c568f05":"_X_train = _X_train.reset_index()","624976df":"non_scored_ones = pd.DataFrame()\nnon_scored_ones['col_name'] = ''\nnon_scored_ones['item_counts'] = 0\n\nfor col in train_targets_nonscored.columns[1:]:\n\n    item_counts = len(train_targets_nonscored[train_targets_nonscored[col] == 1])\n\n    non_scored_ones = non_scored_ones.append({'col_name':col, 'item_counts':item_counts}, ignore_index=True)\n\nnon_scored_target_cols = non_scored_ones[non_scored_ones.item_counts > 10].col_name\n\ny_non_train = train_targets_nonscored[non_scored_target_cols]\ny_non_train = y_non_train[train_trt_index].reset_index(drop=True)","c1819633":"from sklearn.decomposition import PCA\n\ndef yield_pca(_X_train, _X_test, prefix, decomp_cols, comp=50, random_state=42):\n    pca = PCA(n_components=comp, random_state=random_state)\n    pca.fit(pd.concat([_X_train.loc[:,decomp_cols], _X_test.loc[:,decomp_cols]]))\n    _X_train_PCA = pca.transform(_X_train[decomp_cols])\n    _X_test_PCA = pca.transform(_X_test[decomp_cols])\n\n    _X_train_PCA = pd.DataFrame(_X_train_PCA, columns=[f'pca_{prefix}-{i}' for i in range(comp)])\n    _X_test_PCA = pd.DataFrame(_X_test_PCA, columns=[f'pca_{prefix}-{i}' for i in range(comp)])\n\n    return _X_train_PCA, _X_test_PCA\n\n_X_train_G_PCA, _X_test_G_PCA = yield_pca(_X_train, _X_test, 'G', GENES, 90)\n_X_train_C_PCA, _X_test_C_PCA = yield_pca(_X_train, _X_test, 'C', CELLS, 27)\n_X_train_G_PCA_Dense, _X_test_G_PCA_Dense = yield_pca(_X_train, _X_test, 'Gd', GENES, 8)\n_X_train_C_PCA_Dense, _X_test_C_PCA_Dense = yield_pca(_X_train, _X_test, 'Cd', CELLS, 1)\n\n_X_train = pd.concat([_X_train, _X_train_G_PCA], axis=1)\n_X_train = pd.concat([_X_train, _X_train_C_PCA], axis=1)\n_X_train = pd.concat([_X_train, _X_train_G_PCA_Dense], axis=1)\n_X_train = pd.concat([_X_train, _X_train_C_PCA_Dense], axis=1)\n\n_X_test = pd.concat([_X_test, _X_test_G_PCA], axis=1)\n_X_test = pd.concat([_X_test, _X_test_C_PCA], axis=1)\n_X_test = pd.concat([_X_test, _X_test_G_PCA_Dense], axis=1)\n_X_test = pd.concat([_X_test, _X_test_C_PCA_Dense], axis=1)\n\n# cols\nPCA_G = [col for col in _X_train.columns if col.startswith('pca_G-')] + [col for col in _X_train.columns if col.startswith('pca_Gd-')]\nPCA_C = [col for col in _X_train.columns if col.startswith('pca_C-')] + [col for col in _X_train.columns if col.startswith('pca_Cd-')]","02ac3a66":"def make_feature(data, target_feats, create, prefix):\n    target_data = data.loc[:,target_feats]\n    invoker = getattr(target_data, create)\n    return pd.DataFrame(invoker(axis=1), columns=[f'{prefix}_{create}'])\n\nfor method in ['sum', 'mean', 'std', 'skew', 'kurt', 'median']: # min max\n    _X_train = pd.concat([_X_train, make_feature(_X_train, GENES+CELLS, method, 'gce')], axis = 1)\n    _X_train = pd.concat([_X_train, make_feature(_X_train, GENES, method, 'ge')], axis = 1)\n\n    _X_test = pd.concat([_X_test, make_feature(_X_test, GENES+CELLS, method, 'gce')], axis = 1)\n    _X_test = pd.concat([_X_test, make_feature(_X_test, GENES, method, 'ge')], axis = 1)\n\nGC_EX = [col for col in _X_train.columns if col.startswith('gce_')]# + [col for col in _X_train.columns if col.startswith('gce_')]\nG_EX = [col for col in _X_train.columns if col.startswith('ge_')]# + [col for col in _X_train.columns if col.startswith('ge_')]","27300779":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\n\ntrain_col_names = _X_train.columns\ntest_col_names = _X_test.columns\n\nTRANSFORM_TARGET_COLS = BASE_COLS + GENES + CELLS + PCA_G + PCA_C + G_EX + GC_EX\n\n\ndef rank_gauss(_X_train, _X_test, cols=TRANSFORM_TARGET_COLS, random_state=42):\n    qt = QuantileTransformer(random_state=random_state, output_distribution='normal')\n    qt.fit(pd.concat([_X_train[cols], _X_test[cols]]))\n    _X_train[cols] = qt.transform(_X_train[cols])\n    _X_test[cols] = qt.transform(_X_test[cols])\n\n    return _X_train, _X_test\n\n\ndef standard_scaler(_X_train, _X_test, cols=TRANSFORM_TARGET_COLS):\n    ss = StandardScaler()\n    ss.fit(pd.concat([_X_train[cols], _X_test[cols]]))\n    _X_train[cols] = ss.transform(_X_train[cols])\n    _X_test[cols] = ss.transform(_X_test[cols])\n\n    return _X_train, _X_test\n\n_X_train, _X_test = rank_gauss(_X_train, _X_test)\n_X_train, _X_test = standard_scaler(_X_train, _X_test)\n\n_X_train.columns = train_col_names\n_X_test.columns = test_col_names","3c06741e":"if VARIANCE_THRESHOLD:\n    vt_cols = vt_cols.values.tolist() + BASE_COLS + PCA_G + PCA_C + GC_EX + G_EX\n\n    _X_train_nl = pd.concat([_X_train.iloc[:,:1], _X_train.loc[:,vt_cols]], axis=1)\n    _X_test_nl = _X_test.loc[:,vt_cols]\n    \n        \n    vt_cols_leaky = vt_cols_leaky.values.tolist() + BASE_COLS + PCA_G + PCA_C + GC_EX + G_EX\n\n    _X_train = pd.concat([_X_train.iloc[:,:1], _X_train.loc[:,vt_cols_leaky]], axis=1)\n    _X_test = _X_test.loc[:,vt_cols_leaky]\n\n    GENES = [col for col in _X_train.columns if col.startswith('g-')]\n    CELLS = [col for col in _X_train.columns if col.startswith('c-')]","400c6068":"_X_train = _X_train.set_index('index')\nX_train = _X_train.copy()\nX_test = _X_test.copy()\n\n_X_train_nl = _X_train_nl.set_index('index')\nX_train_nl = _X_train_nl.copy()\nX_test_nl = _X_test_nl.copy()\n\ny_train = _y_train.copy()","46ad513f":"import time\n\ndef train_and_predict(name, model_func, _X_i_train, _y_i_train, _X_i_test, orig_targets, result_template,\n                      seeds, splits, epochs, batch_size, shuffle_rows=False, pick_col_size=800, \n                      do_show_metrics=True, show_each_metrics=True):\n    st = time.time()\n\n    is_list = isinstance(_X_i_train, list)\n\n    val_result = orig_targets.copy()\n    val_result.loc[:, :] = 0\n\n    sub_result = result_template.copy()\n    sub_result.loc[:, 1:] = 0\n\n    for h, seed in enumerate(seeds):\n\n        seed_everything(seed)\n\n        for i, (train_idx, valid_idx) in enumerate(MultilabelStratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n                                        .split(_y_i_train, _y_i_train)):\n            print(f'Fold {i+1}')\n\n            if is_list:\n                _X_train = [_X_i_train[0].loc[:,:].values[train_idx], _X_i_train[1].loc[:,:].values[train_idx]]\n                _X_valid = [_X_i_train[0].loc[:,:].values[valid_idx], _X_i_train[1].loc[:,:].values[valid_idx]]\n            else:\n                _X_train = _X_i_train.loc[:,:].values[train_idx]\n                _X_valid = _X_i_train.loc[:,:].values[valid_idx]\n\n            _y_train = _y_i_train.values[train_idx]\n            _y_valid = _y_i_train.values[valid_idx]\n\n            if is_list:\n                model = model_func(len(_X_i_train[0].columns), len(_X_i_train[1].columns))\n            else:\n                model = model_func(len(_X_i_train.columns))\n            \n            model.fit(_X_train, _y_train,\n                    validation_data=(_X_valid, _y_valid),\n                    epochs=epochs, batch_size=batch_size,\n                    callbacks=[\n                        ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min')\n                        , ModelCheckpoint(f'{name}_{seed}_{i}.hdf5', monitor = 'val_logloss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n                        , EarlyStopping(monitor = 'val_logloss', min_delta = 1e-4, patience = 5, mode = 'min', baseline = None, restore_best_weights = True)\n                    ], verbose=2)\n        \n            model.load_weights(f'{name}_{seed}_{i}.hdf5')\n            val_result.iloc[_y_i_train.iloc[valid_idx,:].index.values, :] += model.predict(_X_valid)\n\n            if is_list:\n                sub_result.loc[test_trt_index, sub_result.columns[1:]] += model.predict([_X_i_test[0].loc[test_trt_index, :], _X_i_test[1].loc[test_trt_index, :]])\n            else:\n                sub_result.loc[test_trt_index, sub_result.columns[1:]] += model.predict(_X_i_test.loc[test_trt_index, :])\n\n            print('')\n\n        tmp_result = val_result.copy()\n        tmp_result.iloc[:,1:] = val_result.iloc[:,1:] \/ (h + 1)\n        print(f' ---- seed:{seed}, ensemble:{h + 1}')\n        if do_show_metrics:\n            _ = show_metrics(tmp_result, show_each=False)\n\n    val_result.iloc[:,1:] = val_result.iloc[:,1:] \/ len(seeds)\n    if do_show_metrics:\n        metrics = show_metrics(val_result, show_each=show_each_metrics)\n    else:\n        metrics = None\n\n    sub_result.iloc[:, 1:] = sub_result.iloc[:, 1:] \/ (len(seeds) * splits)\n\n    print(f' elapsed: {time.time() - st}')\n\n    return sub_result, val_result, metrics","7e3ff56a":"p_min = 0.001\np_max = 0.999\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))","b14a623d":"output_bias=tf.keras.initializers.Constant(-np.log(y_train.mean(axis=0).to_numpy()))","8054cd40":"def create_add_model(input_dim):\n    print(f'the input dim is {input_dim}')\n\n    model = M.Sequential()\n    model.add(L.Input(input_dim))\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.5))\n    model.add(WeightNormalization(L.Dense(input_dim, activation='elu')))\n\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.4))\n    model.add(WeightNormalization(L.Dense(512, activation='selu')))\n\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.5))\n    model.add(WeightNormalization(L.Dense(y_non_train.shape[1], activation='sigmoid',\n                                          bias_initializer=tf.keras.initializers.Constant(-np.log(y_non_train.mean(axis=0).to_numpy())\n                                          ))))\n\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(lr=1e-3), sync_period=10),\n                loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING), metrics=logloss)\n\n    return model","f3a7f80b":"non_scored_template = sample_submission.copy()\nnon_scored_template = pd.DataFrame(non_scored_template.pop('sig_id'))\nfor col in non_scored_target_cols:\n    non_scored_template[col] = 0.","c2c3e275":"if ADD_NON_TRAIN:\n    add_result, add_valid_preds, add_metrics = train_and_predict('ADD', create_add_model, X_train, y_non_train, X_test,\n                                                                 train_targets_nonscored[non_scored_target_cols].loc[train_trt_index,:], non_scored_template,\n                                                                  ADD_SEEDS, ADD_SPLITS, ADD_EPOCHS, ADD_BATCH_SIZE, shuffle_rows=False, do_show_metrics=False, show_each_metrics=False)","7991006c":"if ADD_NON_TRAIN:\n    X_non_train = add_valid_preds.copy()\n    X_non_test = add_result.iloc[:,1:].copy()\n    cols = X_non_train.columns\n    \n    X_non_train, X_non_test = rank_gauss(X_non_train, X_non_test, cols)\n    X_non_train, X_non_test = standard_scaler(X_non_train, X_non_test, cols)","1f87196c":"def create_autoencoder(input_dim):\n    input_vector = L.Input(shape=(input_dim,))\n    encoded = L.Dense(1024, activation='elu')(input_vector)\n    encoded = L.Dense(2048, activation='elu')(encoded)\n    decoded = L.Dense(1024, activation='elu')(encoded)\n    decoded = L.Dense(input_dim, activation='elu')(decoded)\n    \n    autoencoder = M.Model(input_vector, decoded)\n    autoencoder.compile(optimizer=tf.optimizers.Adam(lr=0.0012, amsgrad=True), loss='mse')\n    \n    return autoencoder","87076994":"mu, sigma = 0, 0.1\n\nseed_everything(128)\n\ndae_train = pd.concat([_X_train_dae, _X_test_dae])\n\nnoise = np.random.normal(mu, sigma, [dae_train.shape[0], dae_train.shape[1]]) \nnoised_train = dae_train + noise\n\nautoencoder = create_autoencoder(dae_train.shape[1])\n\nautoencoder.fit(noised_train, dae_train,\n                epochs=500,\n                batch_size=128,\n                callbacks=[\n                    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience = 3, verbose=1, min_delta=1e-4, mode='min')\n                    , ModelCheckpoint(f'dae.hdf5', monitor = 'val_loss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n                    , EarlyStopping(monitor = 'val_loss', min_delta = 1e-4, patience = 8, mode = 'min', baseline = None, restore_best_weights = True)],\n                shuffle=True,\n                validation_split=0.2)\n\nautoencoder.load_weights(f'dae.hdf5')\n#138\/138 [==============================] - 1s 10ms\/step - loss: 0.6584 - val_loss: 0.6722\n#138\/138 [==============================] - 1s 10ms\/step - loss: 0.6576 - val_loss: 0.6714","debca816":"#163\/163 [==============================] - 1s 6ms\/step - loss: 0.6668 - val_loss: 0.6222\nencoder = M.Model(autoencoder.input, autoencoder.layers[2].output)\n__X_train_dae = pd.DataFrame(encoder.predict(_X_train_dae))\n__X_test_dae = pd.DataFrame(encoder.predict(_X_test_dae))","f209cda5":"vh_dae = VarianceThreshold(0.2)\nvh_dae.fit_transform(pd.concat([__X_train_dae, __X_test_dae]))\ndae_cols = __X_train_dae.columns[vh_dae.get_support()]\nprint(len(dae_cols))\n\nX_train_dae = __X_train_dae.loc[:, dae_cols]\nX_test_dae = __X_test_dae.loc[:, dae_cols]","3fcebef7":"X_train_dae = pd.concat([X_train_dae, X_non_train.reset_index(drop=True)], axis=1)\nX_test_dae = pd.concat([X_test_dae, X_non_test.reset_index(drop=True)], axis=1)","ab798c58":"_X_train_G_PCA_dae, _X_test_G_PCA_dae = yield_pca(_X_train, _X_test, 'Gd', GENES, 90, 128)\n_X_train_C_PCA_dae, _X_test_C_PCA_dae = yield_pca(_X_train, _X_test, 'Cd', CELLS, 27, 128)\n\n_X_train_G_PCA_Dense_dae, _X_test_G_PCA_Dense_dae = yield_pca(_X_train, _X_test, 'Gd', GENES, 8, 128)\n_X_train_C_PCA_Dense_dae, _X_test_C_PCA_Dense_dae = yield_pca(_X_train, _X_test, 'Cd', CELLS, 1, 128)\n\nX_train_dae = pd.concat([X_train_dae, _X_train_G_PCA_dae, _X_train_C_PCA_dae, _X_train_G_PCA_Dense, _X_train_C_PCA_Dense_dae], axis=1)\nX_test_dae = pd.concat([X_test_dae, _X_test_G_PCA_dae, _X_test_C_PCA_dae, _X_test_G_PCA_Dense, _X_test_C_PCA_Dense_dae], axis=1)\n\nX_train_dae, X_test_dae = rank_gauss(X_train_dae, X_test_dae, X_train_dae.columns, random_state=128)\nX_train_dae, X_test_dae = standard_scaler(X_train_dae, X_test_dae, X_train_dae.columns)","2faf6d4f":"def create_simple_nn(input_dim):\n    print(f'the input dim is {input_dim}')\n\n    model = M.Sequential()\n    model.add(L.Input(input_dim))\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.5))\n    model.add(WeightNormalization(L.Dense(input_dim, activation='selu')))\n\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.5))\n    model.add(WeightNormalization(L.Dense(512, activation='swish')))\n\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.5))\n    model.add(WeightNormalization(L.Dense(206, activation='sigmoid', bias_initializer=output_bias)))\n\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(lr=0.01), sync_period=10),\n                loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING), metrics=logloss)\n\n    return model","e2e48a48":"if RUN_SNN:\n    snn_result, snn_valid_preds, snn_metrics = train_and_predict('SNN', create_simple_nn, X_train_dae, _y_train, X_test_dae,\n                                                                 train_targets_scored.iloc[:,1:], sample_submission,\n                                                                 SNN_SEEDS, SNN_SPLITS, SNN_EPOCHS, SNN_BATCH_SIZE, shuffle_rows=False)","e915d066":"_X_train = pd.concat([X_train, X_non_train], axis=1)\n_X_test = pd.concat([X_test, X_non_test], axis=1)","5d09a5f1":"sep_cols = BASE_COLS + GENES #+ CELLS\n_X_train_1 = _X_train[sep_cols]\n_X_train_2 = _X_train.drop(sep_cols,axis=1)\n\n_X_test_1 = _X_test[sep_cols]\n_X_test_2 = _X_test.drop(sep_cols,axis=1)\n\n_X_train = [_X_train_1, _X_train_2]\n_X_test = [_X_test_1, _X_test_2]","4327463f":"def create_res_model(n_features, n_features_2):\n    print(f'the input dim is {n_features}, {n_features_2}')\n\n    input_1 = L.Input(shape = (n_features,), name = 'Input1')\n    input_2 = L.Input(shape = (n_features_2,), name = 'Input2')\n\n    head_1 = M.Sequential([\n        L.BatchNormalization(),\n        L.Dropout(0.3),\n        L.Dense(512, activation='elu'), \n        L.BatchNormalization(),\n        L.Dropout(0.5),\n        L.Dense(256, activation='elu')\n        ],name='Head1') \n\n    input_3 = head_1(input_1)\n    input_3_concat = L.Concatenate()([input_2, input_3])\n\n    head_2 = M.Sequential([\n        L.BatchNormalization(),\n        L.Dropout(0.3),\n        L.Dense(n_features_2, activation='relu'),\n        L.BatchNormalization(),\n        L.Dropout(0.5),\n        L.Dense(n_features_2, activation='elu'),\n        L.BatchNormalization(),\n        L.Dropout(0.5),\n        L.Dense(256, activation='relu'),\n        L.BatchNormalization(),\n        L.Dropout(0.5),\n        L.Dense(256, activation='selu')\n        ],name='Head2')\n\n    input_4 = head_2(input_3_concat)\n    input_4_avg = L.Average()([input_3, input_4]) \n\n    head_3 = M.Sequential([\n        L.BatchNormalization(),\n        L.Dropout(0.3),\n        L.Dense(256, activation='swish'),\n        L.BatchNormalization(),\n        L.Dense(256, activation='selu'),\n        L.BatchNormalization(),\n        L.Dense(206, activation='sigmoid')\n        ],name='Head3')\n\n    output = head_3(input_4_avg)\n\n\n    model = M.Model(inputs = [input_1, input_2], outputs = output)\n    model.compile(optimizer=tf.optimizers.Adam(lr=0.002),\n                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING), metrics=logloss)\n\n    return model","355aaca6":"if RUN_NN:\n    nn_result, nn_valid_preds, nn_metrics = train_and_predict('RES', create_res_model, _X_train, y_train, _X_test,\n                                                              train_targets_scored.iloc[:,1:], sample_submission,\n                                                              NN_SEEDS, NN_SPLITS, NN_EPOCHS, NN_BATCH_SIZE, shuffle_rows=False)","a88057fb":"if ADD_NON_TRAIN:\n    _X_train = []\n    _X_train.append(X_train.copy())\n    _X_train.append(X_non_train.copy())\n\n    _X_test = []\n    _X_test.append(X_test.copy())\n    _X_test.append(X_non_test.copy())\n\n# extract_cells = set(CELLS) - set(['c-22'])\n\n# _X_train[0] = _X_train[0].drop(extract_cells, axis=1)\n# _X_test[0] = _X_test[0].drop(extract_cells, axis=1)\n\n_X_train[1] = pd.concat([_X_train[1], _X_train[0][CELLS]], axis=1)\n_X_test[1] = pd.concat([_X_test[1], _X_test[0][CELLS]], axis=1)\n_X_train[0] = _X_train[0].drop(CELLS, axis=1)\n_X_test[0] = _X_test[0].drop(CELLS, axis=1)","f3fe0098":"def create_twohead_model(input_dim1, input_dim2):\n    print(f'the input dim is {input_dim1}, {input_dim2}')\n\n    input_1 = L.Input(input_dim1)\n    x1 = L.BatchNormalization()(input_1)\n    x1 = L.Dropout(0.5)(x1)\n    o1 = L.Dense(input_dim1, activation='elu')(x1)\n\n    input_2 = L.Input(input_dim2)\n    x2 = L.BatchNormalization()(input_2)\n    x2 = L.Dropout(0.5)(x2)\n    o2 = L.Dense(input_dim2, activation='elu')(x2)\n\n    x = L.Concatenate()([o1, o2])\n\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(0.4)(x)\n    x = L.Dense(512, activation='selu')(x)\n    \n    x = L.BatchNormalization()(x)\n    x = L.Dropout(0.5)(x)\n    output = L.Dense(206, activation='sigmoid')(x)\n\n    model = M.Model([input_1, input_2], output)\n\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(lr=0.01), sync_period=10),\n                loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING), metrics=logloss)\n    \n    return model","61129c02":"if RUN_NN2:\n    nn2_result, nn2_valid_preds, nn2_metrics = train_and_predict('NN2', create_twohead_model, _X_train, y_train, _X_test,\n                                                                 train_targets_scored.iloc[:,1:], sample_submission,\n                                                                 NN2_SEEDS, NN2_SPLITS, NN2_EPOCHS, NN2_BATCH_SIZE, shuffle_rows=False)","56e986dc":"top_feats = [   0,    1,    2,    4,    6,    9,   11,   12,   13,   15,   16,\n         17,   19,   20,   21,   22,   24,   25,   26,   27,   28,   29,\n         31,   33,   36,   37,   39,   40,   41,   42,   44,   45,   47,\n         48,   49,   50,   51,   52,   53,   54,   55,   57,   58,   60,\n         61,   65,   67,   68,   69,   71,   72,   73,   75,   76,   78,\n         79,   80,   81,   82,   83,   85,   86,   87,   88,   89,   91,\n         92,   93,   94,   96,   98,   99,  100,  101,  102,  103,  104,\n        107,  108,  109,  110,  111,  112,  113,  114,  115,  117,  118,\n        119,  120,  122,  123,  124,  125,  126,  128,  129,  130,  132,\n        133,  134,  135,  136,  137,  140,  141,  142,  143,  144,  146,\n        148,  149,  150,  151,  152,  153,  154,  155,  158,  159,  161,\n        162,  164,  165,  166,  167,  168,  169,  170,  171,  172,  173,\n        175,  178,  181,  183,  184,  185,  186,  187,  189,  190,  191,\n        193,  197,  198,  200,  201,  202,  203,  204,  205,  206,  209,\n        210,  211,  213,  214,  216,  218,  219,  220,  221,  223,  225,\n        226,  227,  229,  230,  231,  232,  233,  236,  237,  239,  241,\n        242,  243,  244,  245,  246,  247,  248,  249,  250,  251,  254,\n        255,  256,  257,  258,  261,  262,  263,  265,  266,  268,  270,\n        271,  272,  273,  275,  276,  277,  279,  280,  281,  283,  284,\n        288,  289,  291,  294,  297,  298,  299,  300,  302,  303,  304,\n        305,  306,  309,  310,  311,  312,  314,  315,  316,  317,  319,\n        321,  322,  323,  324,  325,  326,  329,  330,  332,  336,  337,\n        339,  340,  341,  342,  343,  344,  347,  348,  349,  351,  354,\n        355,  356,  357,  358,  359,  360,  361,  362,  365,  367,  369,\n        370,  371,  373,  374,  375,  376,  378,  379,  380,  383,  384,\n        385,  388,  390,  392,  395,  397,  400,  401,  403,  404,  406,\n        407,  408,  410,  414,  415,  416,  418,  419,  420,  422,  423,\n        424,  425,  426,  427,  428,  430,  431,  433,  434,  436,  438,\n        439,  441,  443,  446,  447,  448,  450,  451,  452,  454,  455,\n        457,  461,  462,  463,  464,  466,  468,  469,  470,  473,  474,\n        475,  477,  478,  479,  481,  482,  483,  484,  487,  488,  489,\n        492,  495,  497,  501,  502,  503,  505,  506,  508,  509,  510,\n        511,  512,  513,  514,  516,  517,  519,  522,  523,  524,  525,\n        527,  528,  529,  531,  538,  543,  546,  548,  549,  550,  552,\n        553,  554,  555,  558,  559,  560,  561,  562,  563,  564,  565,\n        567,  568,  569,  572,  573,  574,  575,  576,  579,  580,  583,\n        585,  587,  588,  591,  592,  595,  596,  597,  599,  600,  603,\n        610,  611,  612,  613,  614,  617,  619,  620,  621,  623,  624,\n        625,  627,  628,  629,  632,  634,  635,  636,  637,  638,  639,\n        640,  642,  643,  644,  645,  646,  647,  649,  650,  651,  654,\n        656,  659,  660,  661,  662,  663,  665,  669,  671,  672,  673,\n        674,  676,  677,  678,  680,  681,  682,  685,  686,  687,  689,\n        692,  694,  695,  696,  697,  698,  699,  701,  702,  704,  705,\n        706,  708,  709,  710,  714,  716,  717,  718,  722,  724,  725,\n        729,  732,  733,  734,  735,  736,  737,  738,  742,  743,  748,\n        749,  751,  753,  754,  755,  756,  757,  759,  761,  763,  764,\n        765,  767,  768,  771,  772,  773,  774,  775,  776,  778,  779,\n        781,  782,  784,  786,  788,  791,  792,  793,  794,  800,  801,\n        802,  805,  807,  814,  815,  816,  820,  821,  822,  823,  824,\n        825,  828,  829,  830,  831,  832,  833,  834,  835,  836,  837,\n        838,  839,  840,  841,  842,  843,  844,  846,  847,  848,  849,\n        850,  851,  852,  853,  854,  855,  856,  857,  858,  859,  860,\n        861,  863,  865,  866,  867,  868,  869,  870,  871,  872,  873,\n        876,  877,  878,  879,  880,  881,  882,  883,  884,  886,  887,\n        889,  890,  892,  893,  894,  895,  896,  897,  898,  899,  901,\n        902,  904,  905,  906,  909,  910,  911,  912,  913,  914,  915,\n        916,  917,  918,  919,  920,  921,  922,  923,  924,  925,  926,\n        927,  928,  929,  930,  931,  932,  933,  934,  938,  941,  942,\n        944,  948,  949,  950,  951,  954,  955,  956,  957,  958,  959,\n        960,  961,  962,  963,  964,  965,  966,  967,  968,  969,  970,\n        971,  972,  974,  976,  980,  981,  985,  987,  988,  989,  990,\n        991,  993,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003,\n       1004, 1005, 1006, 1010, 1011, 1012, 1013, 1014, 1016, 1017, 1018,\n       1019, 1020, 1021, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1031,\n       1032, 1034, 1035, 1037, 1038, 1040, 1041, 1042, 1046, 1047, 1048,\n       1049, 1050, 1051, 1053, 1055, 1056, 1057, 1058, 1059, 1060, 1062,\n       1064, 1065, 1066, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1076,\n       1078, 1079, 1080, 1082, 1083, 1084, 1085, 1086, 1087, 1089, 1092,\n       1093, 1094, 1095, 1096, 1097, 1099, 1100, 1101, 1102, 1103, 1104,\n       1105, 1106, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116,\n       1117, 1118, 1119, 1120, 1121, 1122, 1125, 1126, 1127, 1128, 1131,\n       1132, 1134, 1136, 1137, 1138, 1140, 1141]","be5a4fd8":"X_train = pd.concat([X_train_nl, X_non_train], axis=1)\nX_test = pd.concat([X_test_nl, X_non_test], axis=1)","a23b5423":"X_train_rnn = X_train.iloc[:, top_feats]\nX_test_rnn = X_test.iloc[:, top_feats]","7488e41e":"def create_rnn_model(input_dim):\n    print(f'the input dim is {input_dim}')\n\n    model = M.Sequential()\n    model.add(L.Input(input_dim))\n    model.add(L.Reshape((1, input_dim)))\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.3))\n    model.add(L.GRU(1024, dropout = 0.5, recurrent_dropout=0.3, return_sequences = True, activation='elu'))\n    model.add(L.GRU(1024, dropout = 0.5, recurrent_dropout=0.5, activation='selu'))\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.5))\n    model.add(WeightNormalization(L.Dense(206, activation='sigmoid', bias_initializer=output_bias)))\n    \n    model.compile(optimizer = tfa.optimizers.Lookahead(tf.optimizers.Adam(0.0015), sync_period = 2), \n                    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING), metrics=logloss)\n    \n    return model","390982b0":"if RUN_RNN:\n    rnn_result, rnn_valid_preds, rnn_metrics = train_and_predict('RNN', create_rnn_model, X_train_rnn, y_train, X_test_rnn, \n                                                                 train_targets_scored.iloc[:,1:], sample_submission,\n                                                                 RNN_SEEDS, RNN_SPLITS, RNN_EPOCHS, RNN_BATCH_SIZE)","0356a61d":"#from pytorch_tabnet.tab_model import TabModel\nimport torch\nimport numpy as np\nfrom scipy.sparse import csc_matrix\nimport time\nfrom abc import abstractmethod\nfrom pytorch_tabnet import tab_network\nfrom pytorch_tabnet.multiclass_utils import unique_labels\nfrom sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\nfrom torch.nn.utils import clip_grad_norm_\nfrom pytorch_tabnet.utils import (PredictDataset,\n                                  create_dataloaders,\n                                  create_explain_matrix)\nfrom sklearn.base import BaseEstimator\nfrom torch.utils.data import DataLoader\nfrom copy import deepcopy\nimport io\nimport json\nfrom pathlib import Path\nimport shutil\nimport zipfile\n\nclass TabModel(BaseEstimator):\n    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n                 lambda_sparse=1e-3, seed=0,\n                 clip_value=1, verbose=1,\n                 optimizer_fn=torch.optim.Adam,\n                 optimizer_params=dict(lr=2e-2),\n                 scheduler_params=None, scheduler_fn=None,\n                 mask_type=\"sparsemax\",\n                 input_dim=None, output_dim=None,\n                 device_name='auto'):\n        \"\"\" Class for TabNet model\n        Parameters\n        ----------\n            device_name: str\n                'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n        \"\"\"\n\n        self.n_d = n_d\n        self.n_a = n_a\n        self.n_steps = n_steps\n        self.gamma = gamma\n        self.cat_idxs = cat_idxs\n        self.cat_dims = cat_dims\n        self.cat_emb_dim = cat_emb_dim\n        self.n_independent = n_independent\n        self.n_shared = n_shared\n        self.epsilon = epsilon\n        self.momentum = momentum\n        self.lambda_sparse = lambda_sparse\n        self.clip_value = clip_value\n        self.verbose = verbose\n        self.optimizer_fn = optimizer_fn\n        self.optimizer_params = optimizer_params\n        self.device_name = device_name\n        self.scheduler_params = scheduler_params\n        self.scheduler_fn = scheduler_fn\n        self.mask_type = mask_type\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        self.batch_size = 1024\n\n        self.seed = seed\n        torch.manual_seed(self.seed)\n        # Defining device\n        if device_name == 'auto':\n            if torch.cuda.is_available():\n                device_name = 'cuda'\n            else:\n                device_name = 'cpu'\n        self.device = torch.device(device_name)\n        print(f\"Device used : {self.device}\")\n\n    @abstractmethod\n    def construct_loaders(self, X_train, y_train, X_valid, y_valid,\n                          weights, batch_size, num_workers, drop_last):\n        \"\"\"\n        Returns\n        -------\n        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n            Training and validation dataloaders\n        -------\n        \"\"\"\n        raise NotImplementedError('users must define construct_loaders to use this base class')\n\n    def init_network(\n                     self,\n                     input_dim,\n                     output_dim,\n                     n_d,\n                     n_a,\n                     n_steps,\n                     gamma,\n                     cat_idxs,\n                     cat_dims,\n                     cat_emb_dim,\n                     n_independent,\n                     n_shared,\n                     epsilon,\n                     virtual_batch_size,\n                     momentum,\n                     device_name,\n                     mask_type,\n                     ):\n        self.network = tab_network.TabNet(\n            input_dim,\n            output_dim,\n            n_d=n_d,\n            n_a=n_a,\n            n_steps=n_steps,\n            gamma=gamma,\n            cat_idxs=cat_idxs,\n            cat_dims=cat_dims,\n            cat_emb_dim=cat_emb_dim,\n            n_independent=n_independent,\n            n_shared=n_shared,\n            epsilon=epsilon,\n            virtual_batch_size=virtual_batch_size,\n            momentum=momentum,\n            device_name=device_name,\n            mask_type=mask_type).to(self.device)\n\n        self.reducing_matrix = create_explain_matrix(\n            self.network.input_dim,\n            self.network.cat_emb_dim,\n            self.network.cat_idxs,\n            self.network.post_embed_dim)\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, loss_fn=None,\n            weights=0, max_epochs=100, patience=10, batch_size=1024,\n            virtual_batch_size=128, num_workers=0, drop_last=False):\n        \"\"\"Train a neural network stored in self.network\n        Using train_dataloader for training data and\n        valid_dataloader for validation.\n        Parameters\n        ----------\n            X_train: np.ndarray\n                Train set\n            y_train : np.array\n                Train targets\n            X_train: np.ndarray\n                Train set\n            y_train : np.array\n                Train targets\n            weights : bool or dictionnary\n                0 for no balancing\n                1 for automated balancing\n                dict for custom weights per class\n            max_epochs : int\n                Maximum number of epochs during training\n            patience : int\n                Number of consecutive non improving epoch before early stopping\n            batch_size : int\n                Training batch size\n            virtual_batch_size : int\n                Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n            num_workers : int\n                Number of workers used in torch.utils.data.DataLoader\n            drop_last : bool\n                Whether to drop last batch during training\n        \"\"\"\n        # update model name\n\n        self.update_fit_params(X_train, y_train, X_valid, y_valid, loss_fn,\n                               weights, max_epochs, patience, batch_size,\n                               virtual_batch_size, num_workers, drop_last)\n\n        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n                                                                    y_train,\n                                                                    X_valid,\n                                                                    y_valid,\n                                                                    self.updated_weights,\n                                                                    self.batch_size,\n                                                                    self.num_workers,\n                                                                    self.drop_last)\n\n        self.init_network(\n            input_dim=self.input_dim,\n            output_dim=self.output_dim,\n            n_d=self.n_d,\n            n_a=self.n_a,\n            n_steps=self.n_steps,\n            gamma=self.gamma,\n            cat_idxs=self.cat_idxs,\n            cat_dims=self.cat_dims,\n            cat_emb_dim=self.cat_emb_dim,\n            n_independent=self.n_independent,\n            n_shared=self.n_shared,\n            epsilon=self.epsilon,\n            virtual_batch_size=self.virtual_batch_size,\n            momentum=self.momentum,\n            device_name=self.device_name,\n            mask_type=self.mask_type\n        )\n\n        self.optimizer = self.optimizer_fn(self.network.parameters(),\n                                           **self.optimizer_params)\n\n        if self.scheduler_fn:\n            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n        else:\n            self.scheduler = None\n\n        self.losses_train = []\n        self.losses_valid = []\n        self.learning_rates = []\n        self.metrics_train = []\n        self.metrics_valid = []\n\n        if self.verbose > 0:\n            print(\"Will train until validation stopping metric\",\n                  f\"hasn't improved in {self.patience} rounds.\")\n            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n            print('---------------------------------------')\n            print(msg_epoch)\n\n        total_time = 0\n        while (self.epoch < self.max_epochs and\n               self.patience_counter < self.patience):\n            starting_time = time.time()\n            # updates learning rate history\n            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n\n            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n\n            # leaving it here, may be used for callbacks later\n            self.losses_train.append(fit_metrics['train']['loss_avg'])\n            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n\n            stopping_loss = fit_metrics['valid']['stopping_loss']\n            if stopping_loss < self.best_cost:\n                self.best_cost = stopping_loss\n                self.patience_counter = 0\n                # Saving model\n                self.best_network = deepcopy(self.network)\n                has_improved = True\n            else:\n                self.patience_counter += 1\n                has_improved=False\n            self.epoch += 1\n            total_time += time.time() - starting_time\n            if self.verbose > 0:\n                if self.epoch % self.verbose == 0:\n                    separator = \"|\"\n                    msg_epoch = f\"| {self.epoch:<5} | \"\n                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n                    msg_epoch += f' {separator:<2} '\n                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n                    msg_epoch += f' {separator:<2} '\n                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n                    msg_epoch += f\" {has_improved}\"\n                    print(msg_epoch)\n\n        if self.verbose > 0:\n            if self.patience_counter == self.patience:\n                print(f\"Early stopping occured at epoch {self.epoch}\")\n            print(f\"Training done in {total_time:.3f} seconds.\")\n            print('---------------------------------------')\n\n        self.history = {\"train\": {\"loss\": self.losses_train,\n                                  \"metric\": self.metrics_train,\n                                  \"lr\": self.learning_rates},\n                        \"valid\": {\"loss\": self.losses_valid,\n                                  \"metric\": self.metrics_valid}}\n        # load best models post training\n        self.load_best_model()\n\n        # compute feature importance once the best model is defined\n        self._compute_feature_importances(train_dataloader)\n\n    def save_model(self, path):\n        \"\"\"\n        Saving model with two distinct files.\n        \"\"\"\n        saved_params = {}\n        for key, val in self.get_params().items():\n            if isinstance(val, type):\n                # Don't save torch specific params\n                continue\n            else:\n                saved_params[key] = val\n\n        # Create folder\n        Path(path).mkdir(parents=True, exist_ok=True)\n\n        # Save models params\n        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n            json.dump(saved_params, f)\n\n        # Save state_dict\n        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n        shutil.make_archive(path, 'zip', path)\n        shutil.rmtree(path)\n        print(f\"Successfully saved model at {path}.zip\")\n        return f\"{path}.zip\"\n\n    def load_model(self, filepath):\n\n        try:\n            with zipfile.ZipFile(filepath) as z:\n                with z.open(\"model_params.json\") as f:\n                    loaded_params = json.load(f)\n                with z.open(\"network.pt\") as f:\n                    try:\n                        saved_state_dict = torch.load(f)\n                    except io.UnsupportedOperation:\n                        # In Python <3.7, the returned file object is not seekable (which at least\n                        # some versions of PyTorch require) - so we'll try buffering it in to a\n                        # BytesIO instead:\n                        saved_state_dict = torch.load(io.BytesIO(f.read()))\n        except KeyError:\n            raise KeyError(\"Your zip file is missing at least one component\")\n\n        self.__init__(**loaded_params)\n\n        self.init_network(\n            input_dim=self.input_dim,\n            output_dim=self.output_dim,\n            n_d=self.n_d,\n            n_a=self.n_a,\n            n_steps=self.n_steps,\n            gamma=self.gamma,\n            cat_idxs=self.cat_idxs,\n            cat_dims=self.cat_dims,\n            cat_emb_dim=self.cat_emb_dim,\n            n_independent=self.n_independent,\n            n_shared=self.n_shared,\n            epsilon=self.epsilon,\n            virtual_batch_size=1024,\n            momentum=self.momentum,\n            device_name=self.device_name,\n            mask_type=self.mask_type\n        )\n        self.network.load_state_dict(saved_state_dict)\n        self.network.eval()\n        return\n\n    def fit_epoch(self, train_dataloader, valid_dataloader):\n        \"\"\"\n        Evaluates and updates network for one epoch.\n        Parameters\n        ----------\n            train_dataloader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n            valid_dataloader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with valid set\n        \"\"\"\n        train_metrics = self.train_epoch(train_dataloader)\n        valid_metrics = self.predict_epoch(valid_dataloader)\n\n        fit_metrics = {'train': train_metrics,\n                       'valid': valid_metrics}\n\n        return fit_metrics\n\n    @abstractmethod\n    def train_epoch(self, train_loader):\n        \"\"\"\n        Trains one epoch of the network in self.network\n        Parameters\n        ----------\n            train_loader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n        \"\"\"\n        raise NotImplementedError('users must define train_epoch to use this base class')\n\n    @abstractmethod\n    def train_batch(self, data, targets):\n        \"\"\"\n        Trains one batch of data\n        Parameters\n        ----------\n            data: a :tensor: `torch.tensor`\n                Input data\n            target: a :tensor: `torch.tensor`\n                Target data\n        \"\"\"\n        raise NotImplementedError('users must define train_batch to use this base class')\n\n    @abstractmethod\n    def predict_epoch(self, loader):\n        \"\"\"\n        Validates one epoch of the network in self.network\n        Parameters\n        ----------\n            loader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with validation set\n        \"\"\"\n        raise NotImplementedError('users must define predict_epoch to use this base class')\n\n    @abstractmethod\n    def predict_batch(self, data, targets):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            batch_outs: dict\n        \"\"\"\n        raise NotImplementedError('users must define predict_batch to use this base class')\n\n    def load_best_model(self):\n        if self.best_network is not None:\n            self.network = self.best_network\n\n    @abstractmethod\n    def predict(self, X):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            predictions: np.array\n                Predictions of the regression problem or the last class\n        \"\"\"\n        raise NotImplementedError('users must define predict to use this base class')\n\n    def explain(self, X):\n        \"\"\"\n        Return local explanation\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            M_explain: matrix\n                Importance per sample, per columns.\n            masks: matrix\n                Sparse matrix showing attention masks used by network.\n        \"\"\"\n        self.network.eval()\n\n        dataloader = DataLoader(PredictDataset(X),\n                                batch_size=self.batch_size, shuffle=False)\n\n        for batch_nb, data in enumerate(dataloader):\n            data = data.to(self.device).float()\n\n            M_explain, masks = self.network.forward_masks(data)\n            for key, value in masks.items():\n                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n                                            self.reducing_matrix)\n\n            if batch_nb == 0:\n                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n                                             self.reducing_matrix)\n                res_masks = masks\n            else:\n                res_explain = np.vstack([res_explain,\n                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n                                                        self.reducing_matrix)])\n                for key, value in masks.items():\n                    res_masks[key] = np.vstack([res_masks[key], value])\n        return res_explain, res_masks\n\n    def _compute_feature_importances(self, loader):\n        self.network.eval()\n        feature_importances_ = np.zeros((self.network.post_embed_dim))\n        for data, targets in loader:\n            data = data.to(self.device).float()\n            M_explain, masks = self.network.forward_masks(data)\n            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n\n        feature_importances_ = csc_matrix.dot(feature_importances_,\n                                              self.reducing_matrix)\n        self.feature_importances_ = feature_importances_ \/ np.sum(feature_importances_)\n        \n        \nclass TabNetRegressor(TabModel):\n\n    def construct_loaders(self, X_train, y_train, X_valid, y_valid, weights,\n                          batch_size, num_workers, drop_last):\n        \"\"\"\n        Returns\n        -------\n        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n            Training and validation dataloaders\n        -------\n        \"\"\"\n        if isinstance(weights, int):\n            if weights == 1:\n                raise ValueError(\"Please provide a list of weights for regression.\")\n        if isinstance(weights, dict):\n            raise ValueError(\"Please provide a list of weights for regression.\")\n\n        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n                                                                y_train,\n                                                                X_valid,\n                                                                y_valid,\n                                                                weights,\n                                                                batch_size,\n                                                                num_workers,\n                                                                drop_last)\n        return train_dataloader, valid_dataloader\n\n    def update_fit_params(self, X_train, y_train, X_valid, y_valid, loss_fn,\n                          weights, max_epochs, patience,\n                          batch_size, virtual_batch_size, num_workers, drop_last):\n\n        if loss_fn is None:\n            self.loss_fn = torch.nn.functional.mse_loss\n        else:\n            self.loss_fn = loss_fn\n\n        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n        self.input_dim = X_train.shape[1]\n\n        if len(y_train.shape) == 1:\n            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n                                if doing single regression.\"\"\")\n        assert y_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n        self.output_dim = y_train.shape[1]\n\n        self.updated_weights = weights\n\n        self.max_epochs = max_epochs\n        self.patience = patience\n        self.batch_size = batch_size\n        self.virtual_batch_size = virtual_batch_size\n        # Initialize counters and histories.\n        self.patience_counter = 0\n        self.epoch = 0\n        self.best_cost = np.inf\n        self.num_workers = num_workers\n        self.drop_last = drop_last\n\n    def train_epoch(self, train_loader):\n        \"\"\"\n        Trains one epoch of the network in self.network\n        Parameters\n        ----------\n            train_loader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n        \"\"\"\n\n        self.network.train()\n        y_preds = []\n        ys = []\n        total_loss = 0\n\n        for data, targets in train_loader:\n            batch_outs = self.train_batch(data, targets)\n            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n            total_loss += batch_outs[\"loss\"]\n\n        y_preds = np.vstack(y_preds)\n        ys = np.vstack(ys)\n\n        #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n        stopping_loss = self.log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n        total_loss = total_loss \/ len(train_loader)\n        epoch_metrics = {'loss_avg': total_loss,\n                         'stopping_loss': total_loss,\n                         }\n\n        if self.scheduler is not None:\n            self.scheduler.step()\n        return epoch_metrics\n\n    def train_batch(self, data, targets):\n        \"\"\"\n        Trains one batch of data\n        Parameters\n        ----------\n            data: a :tensor: `torch.tensor`\n                Input data\n            target: a :tensor: `torch.tensor`\n                Target data\n        \"\"\"\n        self.network.train()\n        data = data.to(self.device).float()\n\n        targets = targets.to(self.device).float()\n        self.optimizer.zero_grad()\n\n        output, M_loss = self.network(data)\n\n        y_smo = targets.float() * (1 - LABEL_SMOOTHING) + 0.5 * LABEL_SMOOTHING\n        loss = self.loss_fn(output, y_smo)\n        \n        loss -= self.lambda_sparse*M_loss\n\n        loss.backward()\n        if self.clip_value:\n            clip_grad_norm_(self.network.parameters(), self.clip_value)\n        self.optimizer.step()\n\n        loss_value = loss.item()\n        batch_outs = {'loss': loss_value,\n                      'y_preds': output,\n                      'y': targets}\n        return batch_outs\n\n\n    def log_loss_multi(self, y_true, y_pred):\n        M = y_true.shape[1]\n        results = np.zeros(M)\n        for i in range(M):\n            results[i] = self.log_loss_score(y_true[:,i], y_pred[:,i])\n        return results.mean()\n\n    def log_loss_score(self, actual, predicted,  eps=1e-15):\n\n            \"\"\"\n            :param predicted:   The predicted probabilities as floats between 0-1\n            :param actual:      The binary labels. Either 0 or 1.\n            :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n            :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n            \"\"\"\n\n            \n            p1 = actual * np.log(predicted+eps)\n            p0 = (1-actual) * np.log(1-predicted+eps)\n            loss = p0 + p1\n\n            return -loss.mean()\n\n    def predict_epoch(self, loader):\n        \"\"\"\n        Validates one epoch of the network in self.network\n        Parameters\n        ----------\n            loader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with validation set\n        \"\"\"\n        y_preds = []\n        ys = []\n        self.network.eval()\n        total_loss = 0\n\n        for data, targets in loader:\n            batch_outs = self.predict_batch(data, targets)\n            total_loss += batch_outs[\"loss\"]\n            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n\n        y_preds = np.vstack(y_preds)\n        ys = np.vstack(ys)\n\n        stopping_loss = self.log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n\n        total_loss = total_loss \/ len(loader)\n        epoch_metrics = {'total_loss': total_loss,\n                         'stopping_loss': stopping_loss}\n\n        return epoch_metrics\n\n    def predict_batch(self, data, targets):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            batch_outs: dict\n        \"\"\"\n        self.network.eval()\n        data = data.to(self.device).float()\n        targets = targets.to(self.device).float()\n\n        output, M_loss = self.network(data)\n       \n        y_smo = targets.float() * (1 - LABEL_SMOOTHING) + 0.5 * LABEL_SMOOTHING\n        loss = self.loss_fn(output, targets)\n        #print(self.loss_fn, loss)\n        loss -= self.lambda_sparse*M_loss\n        #print(loss)\n        loss_value = loss.item()\n        batch_outs = {'loss': loss_value,\n                      'y_preds': output,\n                      'y': targets}\n        return batch_outs\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            predictions: np.array\n                Predictions of the regression problem\n        \"\"\"\n        self.network.eval()\n        dataloader = DataLoader(PredictDataset(X),\n                                batch_size=self.batch_size, shuffle=False)\n\n        results = []\n        for batch_nb, data in enumerate(dataloader):\n            data = data.to(self.device).float()\n\n            output, M_loss = self.network(data)\n            predictions = output.cpu().detach().numpy()\n            results.append(predictions)\n        res = np.vstack(results)\n        return res ","06cbec1b":"import time\nimport torch\n\ndef train_and_predict(name, _X_i_train, _y_i_train, _X_i_test, seeds, splits, epochs, batch_size, shuffle_rows=False, pick_col_size=800):\n    st = time.time()\n\n    val_result = train_targets_scored.iloc[:,1:].copy()\n    val_result.loc[:, :] = 0\n\n    sub_result = sample_submission.copy()\n    sub_result.loc[:, 1:] = 0\n\n    all_cols = _X_i_train.columns.values\n\n    for h, seed in enumerate(seeds):\n\n        seed_everything(seed)\n\n        for i, (train_idx, valid_idx) in enumerate(MultilabelStratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n                                      .split(_y_i_train, _y_i_train)):\n            print(f'Fold {i+1}')\n\n            if shuffle_rows:\n                seed_everything(seed + i)\n                target_cols = np.random.choice(all_cols, size=pick_col_size)\n                seed_everything(seed)\n            else:\n                target_cols = all_cols\n\n            _X_train = torch.as_tensor(_X_i_train.loc[:,target_cols].values[train_idx])\n            _y_train = torch.as_tensor(_y_i_train.values[train_idx])\n            _X_valid = torch.as_tensor(_X_i_train.loc[:,target_cols].values[valid_idx])\n            _y_valid = torch.as_tensor(_y_i_train.values[valid_idx])\n\n            ## model\n            model = TabNetRegressor(n_d=24, n_a=24, n_steps=1, gamma=1.3, lambda_sparse=0, \n                                    #cat_dims=cfg.cat_dims, cat_emb_dim=cfg.cat_emb_dim, cat_idxs=cfg.cats_idx, \n                                    optimizer_fn=torch.optim.Adam,\n                                    optimizer_params=dict(lr=2e-2, weight_decay=1e-5), mask_type='entmax', \n                                    device_name=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n                                    scheduler_params=dict(milestones=[100,150], gamma=0.9), scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n            #'sparsemax'\n            model.fit(X_train=_X_train, y_train=_y_train, X_valid=_X_valid, y_valid=_y_valid, max_epochs=epochs, patience=20, batch_size=1024, virtual_batch_size=batch_size,\n                      num_workers=0, drop_last=False, loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n            model.load_best_model()\n            \n            preds = model.predict(_X_valid)\n            preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n            val_result.iloc[y_train.iloc[valid_idx,:].index.values, :] += preds\n\n            preds = model.predict(torch.as_tensor(_X_i_test.loc[test_trt_index, target_cols].values))\n            preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n            sub_result.loc[test_trt_index, sub_result.columns[1:]] += preds\n\n            print('')\n\n        tmp_result = val_result.copy()\n        tmp_result.iloc[:,1:] = val_result.iloc[:,1:] \/ (h + 1)\n        print(f' ---- seed:{seed}, ensemble:{h + 1}')\n        _ = show_metrics(tmp_result, show_each=False)\n\n    val_result.iloc[:,1:] = val_result.iloc[:,1:] \/ len(seeds)\n    metrics = show_metrics(val_result)\n\n    sub_result.iloc[:, 1:] = sub_result.iloc[:, 1:] \/ (len(seeds) * splits)\n\n    print(f' elapsed: {time.time() - st}')\n\n    return sub_result, val_result, metrics","3d85dda5":"if RUN_TABNET:\n    tab_result, tab_valid_preds, tab_metrics = train_and_predict('TAB', X_train, y_train, X_test, \n                                                TAB_SEEDS, TAB_SPLITS, TAB_EPOCHS, TAB_BATCH_SIZE)","cc7f7d34":"if not RE_RUN_NN:\n    ens_result.to_csv('submission.csv', index=False)","623cc7dc":"import time\n\ndef re_train_and_predict(name, model_func, _X_i_train, _y_i_train, _X_i_test, seeds, splits, epochs, batch_size, shuffle_rows=False, pick_col_size=800):\n  st = time.time()\n\n  val_result = train_targets_scored.loc[:,_y_i_train.columns].copy()\n  val_result.loc[:, :] = 0\n\n  sub_result = sample_submission.loc[:, _y_i_train.columns].copy()\n  #sub_result = sub_result.loc[:, _y_i_train.columns]\n  sub_result.loc[:, :] = 0\n\n  all_cols = _X_i_train.columns.values\n  #target_cols = all_cols\n\n  for h, seed in enumerate(seeds):\n\n      seed_everything(seed)\n\n      for i, (train_idx, valid_idx) in enumerate(MultilabelStratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n                                      .split(_y_i_train, _y_i_train)):\n          print(f'Fold {i+1}')\n\n          if shuffle_rows:\n            seed_everything(seed + i)\n            target_cols = np.random.choice(all_cols, size=pick_col_size)\n            seed_everything(seed)\n          else:\n            target_cols = all_cols\n\n          _X_train = _X_i_train.loc[:,target_cols].values[train_idx]\n          _y_train = _y_i_train.values[train_idx]\n          _X_valid = _X_i_train.loc[:,target_cols].values[valid_idx]\n          _y_valid = _y_i_train.values[valid_idx]\n\n          model = model_func(len(target_cols), len(_y_i_train.columns))\n          \n          model.fit(_X_train, _y_train,\n                  validation_data=(_X_valid, _y_valid),\n                  epochs=epochs, batch_size=batch_size,\n                  callbacks=[\n                      ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min')\n                      , ModelCheckpoint(f'{name}_{seed}_{i}.hdf5', monitor = 'val_logloss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n                      , EarlyStopping(monitor = 'val_logloss', min_delta = 1e-4, patience = 5, mode = 'min', baseline = None, restore_best_weights = True)\n                  ], verbose=2)\n      \n          model.load_weights(f'{name}_{seed}_{i}.hdf5')\n          val_result.iloc[y_train.iloc[valid_idx,:].index.values, :] += model.predict(_X_valid)\n          sub_result.loc[test_trt_index, :] += model.predict(_X_i_test.loc[test_trt_index, target_cols])\n\n          print('')\n\n      tmp_result = val_result.copy()\n      tmp_result.iloc[:,1:] = val_result.iloc[:,1:] \/ (h + 1)\n      print(f' ---- seed:{seed}, ensemble:{h + 1}')\n      _ = show_metrics(tmp_result, show_each=False)\n\n  val_result.iloc[:,1:] = val_result.iloc[:,1:] \/ len(seeds)\n  metrics = show_metrics(val_result)\n\n  sub_result.iloc[:, :] = sub_result.iloc[:, :] \/ (len(seeds) * splits)\n\n  print(f' elapsed: {time.time() - st}')\n\n  return sub_result, val_result, metrics","6c4bbaa0":"X_stacking_train = pd.concat([nn_valid_preds, rnn_valid_preds], axis=1, ignore_index=True)\nX_stacking_train = pd.concat([X_stacking_train, tab_valid_preds], axis=1, ignore_index=True)\nX_stacking_train = pd.concat([X_stacking_train, nn2_valid_preds], axis=1, ignore_index=True)\nX_stacking_train = pd.concat([X_stacking_train, snn_valid_preds], axis=1, ignore_index=True)\nX_stacking_train = X_stacking_train[train_trt_index]\n\nX_stacking_test = pd.concat([nn_result.iloc[:, 1:], rnn_result.iloc[:, 1:]], axis=1, ignore_index=True)\nX_stacking_test = pd.concat([X_stacking_test, tab_result.iloc[:, 1:]], axis=1, ignore_index=True)\nX_stacking_test = pd.concat([X_stacking_test, nn2_result.iloc[:, 1:]], axis=1, ignore_index=True)\nX_stacking_test = pd.concat([X_stacking_test, snn_result.iloc[:, 1:]], axis=1, ignore_index=True)\nX_stacking_test = X_stacking_test","4df238cc":"def create_stacking_model(input_dim, output_dim):\n    print(f'the input dim is {input_dim}')\n\n    model = M.Sequential()\n    model.add(L.Input(input_dim))\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.3))\n    model.add(WeightNormalization(L.Dense(input_dim, activation='elu')))\n\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.5))\n    model.add(WeightNormalization(L.Dense(input_dim, activation='swish')))\n\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.5))\n    model.add(WeightNormalization(L.Dense(output_dim, activation='sigmoid', bias_initializer=output_bias)))\n\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(lr=1e-3), sync_period=10),\n                    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING), metrics=logloss)\n\n    return model","0d321824":"if RUN_STACKING:\n    st_re_result, st_re_valid_preds, st_re_metrics = re_train_and_predict('ST_NN', create_stacking_model, \n                                                                         X_stacking_train, y_train, X_stacking_test, \n                                                                         STK_SEEDS, STK_SPLITS, STK_EPOCHS, STK_BATCH_SIZE)","9d0574e7":"# I used this to find optimum ratio of blending.\nimport pickle\nwith open('nn_valid_preds.pkl', 'wb') as p:\n  pickle.dump(nn_valid_preds , p)\nwith open('nn2_valid_preds.pkl', 'wb') as p:\n  pickle.dump(nn2_valid_preds , p)\nwith open('rnn_valid_preds.pkl', 'wb') as p:\n  pickle.dump(rnn_valid_preds , p)\nwith open('tab_valid_preds.pkl', 'wb') as p:\n  pickle.dump(tab_valid_preds , p)\nwith open('st_re_valid_preds.pkl', 'wb') as p:\n  pickle.dump(st_re_valid_preds , p)\nwith open('snn_valid_preds.pkl', 'wb') as p:\n  pickle.dump(snn_valid_preds , p)","882bb2db":"ratio = [0.05, 0.05, 0.1, 0.25, 0.35, 0.2]\n\nens_valid_preds = nn_valid_preds * ratio[0] + nn2_valid_preds * ratio[1] + rnn_valid_preds * ratio[2] + tab_valid_preds * ratio[3] + \\\n                  st_re_valid_preds * ratio[4] + snn_valid_preds * ratio[5]\n_ = show_metrics(ens_valid_preds, show_each=False)","0c885677":"ens_result = nn_result.iloc[:, 1:] * ratio[0] + nn2_result.iloc[:, 1:] * ratio[1] + rnn_result.iloc[:, 1:] * ratio[2] + \\\n             tab_result.iloc[:, 1:] * ratio[3] + st_re_result * ratio[4] + snn_result.iloc[:, 1:] * ratio[5]\nens_result = pd.concat([nn_result.iloc[:,0], ens_result], axis=1)\n\nens_result.to_csv('submission.csv', index=False)","10086760":"## Blending\nJust blend all of them.<br>\nThe ratio was found manually. Seeing the OOF score.<br>\nActually this ratio wasn't found in this notebook.<br>\nIt was found when I use the new CV method.<br>\nI swapped this old CV method to the new CV method and found the optimum blending ratio then applied it here.<br>\n<br>\nThere's one note.<br>\nI found that the optimum ratio is different if I apply different number of seeds or different number of folds. <br>\nTo make the experiments faster, I was using 1 seed and 5 fold to find this optimum blending ratio. <br>\nBut one day, I found 'pickle' the library. Pickle allow me to save the 'fully run' version of prediction results. Then I can try finding the optimum ratio. <br>","8493ea70":"## Shallow NN with DenoisingAutoEncoder\nFor this model for the starter, make 2048 features using denoising auto encoder. <br>\nThen reduce the features using Variance Threshold. 2048 is too much too handle. <br>\nThe reason why I made 2048 features is, even 2048 is too much, it seemed that it succeeded to express denoising better than other number of units. <br>\nThen make PCA feats, apply rank gauss and standard scaler, then make pediction.","852eca18":"## ResNet\nActually I don't know whether it still can be called as ResNet or not. I made many changes for the model.","bec500ec":"## Other things to consider\n- Validation<br>\n  I used MultilabelStratifiedKFold for the final submission as it gave me a better LB all the time. <br>\n  Also in my case, the drug id based CV didn't correlate to LB that much either. So I decided to use it. <br>\n  Well, I know validation strategy is quite important, but I really don't know what was the right validation scheme. <br>\n  I'm looking forward to see others solutions.<br>\n- Hyper parameter tuning<br>\n  I spent most of the time to tune each models. <br>\n  I don't have any knowledge in MoA. Even after I read many explanations, I didn't know how to utilize these...<br>\n  I tuned models seeing OOF score. I didn't know the CV works or not, but it seems it worked after all. ...maybe just luckey. <br>","86120cec":"## Options\nI used basically 2 seeds, 7 splits for each models to train and infer. <br>\nI provide specific seeds but it doesn't matter. I was just not comfortable with using 1, 2, 3 kind of consective numbers.<br>\nDAE and Stacking has 3 seeds. I wanted to exploit the limitation, 2 hours of running time. <br>\nI added seeds as much as I can. <br>\n<br>\nAbout batch size, I selected basically 64 for all the models. Only RNN uses 128, as it is very slow. I needed to make it faster even though it hurts the score a bit.<br>","c1b3c101":"So this is it.<br>\n<br>\nThank you for reading my notebook.<br>\nI think there might be naive thinkings or codes.<br>\nIf you can leave some feedback, I reeeeeally appreciate it!!<br>\n<br>\nThank you soooooo much!!","cbc212ff":"## TabNet\nSo many people are using it. No explanation is needed, right? <br>\nI put label smoothing in TabNet as well, of course.","1a641316":"## RNN\nFor RNN, I didn't use leaky feats. <br>\nI found that feeding some specific features gives me a good result. <br>\nOn the other hand, if I apply Variance Threshold using test data as well, the extracted features are different. And I don't know how many features are selected when I use private test set. <br>\nSo for this RNN, I choose to use just train set to feed.<br>\nThese features are selected from the code [in this notebook](https:\/\/www.kaggle.com\/simakov\/keras-multilabel-neural-network-v1-2). <br>\n<br>\nInterestingly, GRU gave me better result and faster than LSTM in this case.","4361e5b3":"## Stacking\nGather all predictions from these 5 models and predict again. <br>\nThis stacking alone didn't offer me a better LB.","ea824a93":"## Prediction Flow Summary\n1. Make prediction with 5 models\n 1. Simple NN with DenoisingAutoEncoder\n 1. ResNet\n 1. 2 Headed NN ( I don't know how it should be called\n 1. RNN\n 1. TabNet\n1. Stack all of them\n1. Blend all these '6' models\n\nThe \"magic\" part was in blending. I blended all results including stacking result. <br>\nEven though all these 6 models gave me the LB score 0.0185-ish, this blending gave me the LB score of 0.0182-ish.<br>\nI've never seen someone did this before, that's why I was not confident at all.<br>\nIntuitively, it offers much 'overfit' to CV. Please correct me if I'm wrong.<br>\n<br>\nAnd if this method has some kind of specific name, please make a comment.<br>\n<br>\n\n![image.png](attachment:image.png)","f2fcc423":"## Make Extra Features from Non-Scored Targets\nTo use non scored targets, I made a model and created non scored targets for test set. <br>","8456517a":"## The strategy\n**Overfit to private.**<br>\n<br>\nSome of the method done in this notebook won't work in the real problem. Like using test set for PCA or Variance Threshold. <br>\nBut for this competition, I used them on purpose, hoping it works for private LB. <br>\nI wasn't sure it works or not at all. <br>","dbdc9e93":"## Acknowledgement\nI learned a lot from public notebooks and discussions.<br><br>\nMy solution is hugely influenced by them. <br>\nPlease do upvote them, all of them are super! <br>\n- Notebooks\n  - [MoA Stacked TabNet Baseline [Tensorflow 2.0]](https:\/\/www.kaggle.com\/gogo827jz\/moa-stacked-tabnet-baseline-tensorflow-2-0)\n  - [MoA LSTM \/ Pure Transformer [fast and NOT bad]](https:\/\/www.kaggle.com\/gogo827jz\/moa-lstm-pure-transformer-fast-and-not-bad)\n  - [MoA: Multi Input ResNet Model](https:\/\/www.kaggle.com\/rahulsd91\/moa-multi-input-resnet-model)\n  - [Keras autoencoder (DAE) + neural network starter](https:\/\/www.kaggle.com\/isaienkov\/keras-autoencoder-dae-neural-network-starter)\n  - [MoA - Tensorflow Fast Convergence](https:\/\/www.kaggle.com\/tolgadincer\/moa-tensorflow-fast-convergence)\n  - [keras Multilabel Neural Network v1.2](https:\/\/www.kaggle.com\/simakov\/keras-multilabel-neural-network-v1-2).\n\nI also have to mention some discussions which helped me much.\n- Discussions\n  - [After label smoothing, my CV matches my LB. Is this a coincidence?](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/185593)<br>\n    As everybody notices, this label smoothing offered us big jump.\n  - [A method to smart-initialize the final layers of NNs](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/185161)<br>\n    It allows all of my models converge much faster. As I included all the training part in this notebook, making them faster is one of my challenge.\n  - [A Tip on Parameter Tuning for Beginners](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/190838)<br>\n    Thanks to his tip, I can make much experiments faster.\n    \nAnd everybody participated in this competition. <br>\nI learned reeeeally a lot! <br>","b114a99e":"## Two Headed NN\nI don't know how to call it either. Kindly point me out if there's name for it. <br>\nFor this NN, I separated the training data into 2 parts, cells and the others. <br>","4d7cd6fc":"### Thank you for opening this notebook.\nI am going to explain what I did in this competition through this 3 months. <br>\nYes, I was in this competition since the beginning. <br>\nThis is the first competition I was fully committed.<br>\n<br>\nI did everything on this one notebook. Training and inference.<br>\nSo if you see this notebook, you will know everything what I did.<br>\n<br>\nAs I am just a ML beginner, I really appreciate your feedbacks! Please don't hesitate pointing out any falters or things to be improved you found.<br>\nThank you!<br>","476d0432":"## Feature Engineering \n- Variance Threshold<br>\nThere are 2 Variance Thresholds, leaky and non-leaky. Meaning of 'leaky' here is, using test set or not. I intentionally included test set, expecting overfit to private. <br>\nLeaky Variance Threshold is applied to all the training and test sets except for RNN. Only RNN uses non-leaky Variance Threshold. I will explain the reason in the RNN section.\n- PCA features<br>\nLeaky again, I used test set as well.<br>\nI made 2 types of PCA features. More n_comp feats and less n_comp feats. For genes and cells respectivelly. \n- Statistical features<br>\nI made these features for the following features, genes + cells and genes only.<br>\n'sum', 'mean', 'std', 'skew', 'kurt', 'median'\n- Rank Gauss, Standard Scaler <br>\nI applied both of them to the features.\n- Non scored targets<br>\nI used the non scored train targets to include the training set. <br>\nThe way I used this is, as additional features.<br>\nSome features in the non scored targets don't have much positive values. Some of them have no positive values. So I omitted these features. <br>\nTo use this features, as the this non scored targets are not provided for test set, I had to create the features for test set. <br>\nTo do this, I made a model to create non scored targets for test set. <br>\nThe details are explained in the \"Make Extra Features from Non-Scored Targets\" part.","8c3a1954":"### The result\n- public : 0.01816 : 24th\n- private: 0.01605 : 14th"}}