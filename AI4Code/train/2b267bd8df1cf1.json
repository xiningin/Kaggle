{"cell_type":{"1579182b":"code","9ac81309":"code","e6fe9758":"code","68884ca1":"code","6210e1de":"code","054fdedb":"code","45969b0b":"code","9e0a3e24":"code","59adadc9":"code","15fa2b74":"code","bcbe2a88":"code","05b7fffd":"code","44475a5f":"code","ff3099f9":"code","9f56063f":"code","7b142464":"code","57f12443":"code","043e2346":"code","969b29ee":"code","682eae02":"code","cba3fb96":"code","2b6f1f16":"code","25351f61":"code","46ad4158":"code","3c740358":"code","defe6670":"code","d71d1cab":"code","956586e7":"code","8cd5f344":"code","360606aa":"code","ce0cb756":"code","67fc4590":"code","3975f897":"code","9cd053a2":"code","502571e1":"markdown","e84b0851":"markdown","286a0715":"markdown","9ec8bca9":"markdown","2ae14294":"markdown","f4089138":"markdown","2c0abfca":"markdown","7ecabfa0":"markdown","f16402c2":"markdown","9199f86d":"markdown","2a56ba56":"markdown","bebbbac4":"markdown"},"source":{"1579182b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9ac81309":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e6fe9758":"# Load Data\ndf = pd.read_csv('..\/input\/train.tsv', delimiter='\\t')\npd.set_option('display.max_colwidth', -1)\ndf.head()","68884ca1":"df.tail()","6210e1de":"df.info()","054fdedb":"df.shape","45969b0b":"df.describe()","9e0a3e24":"sns.countplot(df[\"Sentiment\"]) #Examining_Sentiment","59adadc9":"df[\"text_length\"] = df[\"Phrase\"].apply(len)","15fa2b74":"df[[\"Sentiment\",\"text_length\",\"Phrase\"]].head()","bcbe2a88":"df[\"text_length\"].describe()","05b7fffd":"df[\"text_length\"].hist(bins=50)","44475a5f":"g = sns.FacetGrid(df,col = \"Sentiment\")\ng.map(plt.hist,\"text_length\")\n","ff3099f9":"sns.boxenplot(x=\"Sentiment\",y=\"text_length\",data=df, palette=\"rainbow\")","9f56063f":"sns.heatmap(df[[\"Sentiment\",\"text_length\"]].corr(),annot=True,cmap=\"cool\",fmt = \"g\")","7b142464":"#word cloud\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud","57f12443":"text = df[\"Phrase\"].to_string()\nwordcloud = WordCloud(relative_scaling=0.5 , background_color='white',stopwords=set(stopwords.words('english'))).generate(text)\nplt.figure(figsize=(12,12))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","043e2346":"from keras.utils import to_categorical\nX = df[\"Phrase\"]\ny = to_categorical(df[\"Sentiment\"])\nnum_classes = df[\"Sentiment\"].nunique()\ny\n","969b29ee":"# setting seed to have identical result in future run for comparisons\nseed = 42\nnp.random.seed(seed)","682eae02":"from sklearn.model_selection import train_test_split\nX_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , random_state = seed)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","cba3fb96":"from keras.preprocessing.text import Tokenizer\nmax_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","2b6f1f16":"totalNumWords = [len(one_review) for one_review in X_train]\nplt.hist(totalNumWords,bins=50)\nplt.show()","25351f61":"X_train[6]","46ad4158":"from keras.preprocessing import sequence\nmax_words = max(totalNumWords)\nX_train = sequence.pad_sequences(X_train , maxlen = max_words)\nX_test = sequence.pad_sequences(X_test , maxlen = max_words)\nprint(X_train.shape,X_test.shape)","3c740358":"X_train[6]","defe6670":"import keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Conv1D,MaxPooling1D\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report","d71d1cab":"batch_size = 128\nepochs = 2","956586e7":"def get_model(max_features , embed_dim):\n    np.random.seed(seed)\n    K.clear_session()\n    model = Sequential()\n    model.add(Embedding(max_features , embed_dim , input_length=X_train.shape[1]))\n    model.add(LSTM(100 , dropout=0.2 , recurrent_dropout=0.2))\n    model.add(Dense(num_classes , activation='softmax'))\n    model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'])\n    print(model.summary())\n    return model\n    ","8cd5f344":"def get_cnn_lstm_model(max_features, embed_dim):\n    np.random.seed(seed)\n    K.clear_session()\n    model = Sequential()\n    model.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\n    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))    \n    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    print(model.summary())\n    return model","360606aa":"def model_train(model):\n    #training the model\n    model_history = model.fit(X_train , y_train , validation_data = (X_test , y_test), \n                              epochs = epochs ,batch_size= batch_size,verbose = 2)\n    #plotting train history\n    plot_model_history(model_history)","ce0cb756":"def plot_model_history(model_history):\n    fig , axs = plt.subplots( 1 , 2 , figsize=(15,5))\n    \n    #Summarize history for accuracy\n    \n    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n    \n    axs[0].set_title(\"Model Accuracy\")\n    axs[0].set_ylabel(\"Accuracy\")\n    axs[0].set_xlabel(\"Epoch\")\n    \n    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])\/10)\n    \n    axs[0].legend(['train', 'val'], loc='best')\n    \n    #Summarize history for loss\n    \n    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n    \n    axs[1].set_title(\"Model Loss\")\n    axs[1].set_ylabel(\"Loss\")\n    axs[1].set_xlabel(\"Epoch\")\n    \n    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])\/10)\n    \n    axs[1].legend(['train', 'val'], loc='best')\n    \n    plt.show()\n    \n    ","67fc4590":"def model_evaluate():\n    #predict classes with test set\n    y_pred_test = model.predict_classes(X_test , batch_size = batch_size, verbose =0)\n    print(\"Predicted \", y_pred_test[:50])\n    print(\"True \" , np.argmax(y_test[:50],axis = 1))\n    print('Accuracy:\\t{:0.1f}%'.format(accuracy_score(np.argmax(y_test,axis = 1),y_pred_test)*100))\n    \n    #Classification Report\n    print(\"\\n\")\n    print(classification_report(np.argmax(y_test, axis =1),y_pred_test))\n    \n    #Confusion Matrix\n    confmat = confusion_matrix(np.argmax(y_test , axis = 1), y_pred_test)\n    fig , ax = plt.subplots(figsize=(4,4))\n    ax.matshow(confmat , cmap =plt.cm.Blues , alpha = 0.3)\n    \n    for i in range(confmat.shape[0]):\n        for j in range(confmat.shape[1]):\n            ax.text( x = j , y = i , s =confmat[i,j] , va = 'center' , ha = 'center')\n    \n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.tight_layout()","3975f897":"max_features = 20000\nembed_dim =100\nmodel = get_cnn_lstm_model(max_features,embed_dim)\nmodel_train(model)","9cd053a2":"model_evaluate()","502571e1":"# LSTM MODEL","e84b0851":"# Train Test Split\n* test_size is how much do you subset the training data into a validation set","286a0715":"# Encode Categorical Variable","9ec8bca9":"# Loading Data","2ae14294":" # LSTM","f4089138":"# Tokenize text","2c0abfca":"# Importing Required Libraries","7ecabfa0":"# Exploratory Data Analysis","f16402c2":"# CNN - LSTM MODEL","9199f86d":"# Train The Model","2a56ba56":"# Evaluate Model With Test Set","bebbbac4":"**Importing Libraries**"}}