{"cell_type":{"0f92273e":"code","98b95b29":"code","5c00980a":"code","ed22c61a":"code","d1f20b35":"code","b7976675":"code","bdebf6a9":"code","a540ee0d":"code","2ade5862":"code","f2fcfed9":"markdown","fd2f722c":"markdown","61cf2491":"markdown","cb925bd4":"markdown","15b8ec21":"markdown","3500b6d6":"markdown","595f2b97":"markdown","4b9580bf":"markdown","a346be27":"markdown","25cb7393":"markdown","dfb3846a":"markdown"},"source":{"0f92273e":"import numpy as np\nnp.random.seed(42)\nX = np.random.randint(1,50, size=30) #30 random integers between 1 and 50\nerrors = np.random.normal(scale=20,size=30) #the error term for each observation\nY = 17.5 + 4.2 * X + errors","98b95b29":"import matplotlib.pyplot as plt\nplt.scatter(X, Y)\nplt.xlabel(\"X values\")\nplt.ylabel(\"Y values\")\nplt.show()","5c00980a":"X_mean = np.mean(X)\nY_mean = np.mean(Y)\nmean_deviances_X = X - X_mean\nmean_deviances_Y = Y - Y_mean\nnumerator = np.sum(mean_deviances_X * mean_deviances_Y)\ndenominator = np.sum(mean_deviances_X**2)\nB1 = numerator \/ denominator\nprint(f\"The estimation for B1 is {B1:.2f}, for a real value of 4.2\")","ed22c61a":"B0 = Y_mean -  B1 * X_mean\nprint(f\"The estimation for B0 is {B0:.2f}, for a real value of 17.5\")","d1f20b35":"plt.scatter(X, Y)\nline_x = np.linspace(0,50,100) #Generates 100 points equally distributed between 0 and 20\nline_y = B1*line_x+B0\nplt.plot(line_x, line_y)\nplt.xlabel(\"X values\")\nplt.ylabel(\"Y values\")\nplt.show()","b7976675":"y_estimated = B0 + B1 * X\n\nresiduals = Y - y_estimated\nsquared_residuals = residuals ** 2\nsigma_squared = np.sum(squared_residuals) \/ (len(X)-2)\n\nX_mean_squared = X_mean**2\nse_B0_squared = sigma_squared * ((1\/len(X)) + (X_mean_squared\/denominator)) \n\nse_B0 = np.sqrt(se_B0_squared)\n\nprint(f'The standard error of B0 is {se_B0:.2f}')","bdebf6a9":"se_B1_squared = sigma_squared \/ denominator\nse_B1 = np.sqrt(se_B1_squared)\nprint(f'The standard error of B1 is {se_B1:.2f}')","a540ee0d":"low_b0_interval = B0 - 2*se_B0\nhigh_b0_interval = B0 + 2*se_B0\nlow_b1_interval = B1 - 2*se_B1\nhigh_b1_interval = B1 + 2*se_B1\nprint(f'With 95% probability, the true value of B0 is between {low_b0_interval:.2f} and {high_b0_interval:.2f}')\nprint(f'With 95% probability, the true value of B1 is between {low_b1_interval:.2f} and {high_b1_interval:.2f}')","2ade5862":"t_statistic = B1 \/ se_B1\nprint(f'The t-statistic is {t_statistic:.2f}')","f2fcfed9":"Now we can estimate B<sub>0<\/sub>:","fd2f722c":"# Simple linear regression","61cf2491":"In simple linear regression, we will fit the data as a linear relationship between a single predictor X and the outcome variable Y. This model could be useful for both trying to predict the value of Y given the value of X or for analyzing if the value of X has a real influence over the value of Y.\n<br>This is the formula of simple linear regresion:<\/br>\n<center>\n    Y \u2248 B<sub>0<\/sub> + B<sub>1<\/sub> X\n<\/center>\nwhere:<ul>\n    <li>Y is the the estimation for the dependent variable <\/li>\n    <li>B0 is the the intercept, the value of Y when X equals 0<\/li>\n    <li>B1 is the coefficient of X, the value that determines the influence of X over Y <\/li>\n    <li>X is the observed value of the independent variable <\/li>\n    <\/ul>\nSo, the model calculates B0 and B1 to explain Y values in functions of X values.\n<br><br> We are going to work with randomly generated datasets but, in order to get a better understanding, you can think of X and Y as real world problems, like the price of an apartment(Y) as a function of square footage (X) or the relation between salary (Y) and years of experience (X).\n<br><br>\nIn these real cases, we don't know the real relationship in the data (we cannot get all the prices and footage of all the apartments) but we can estimate it with a sample. We are going to work in the same way, estimating the coefficients from the sample data.\n<br><br>\nLet's imagine a situation where the real relationship between X and Y is as follows:\n<br>\n<br>\n<center>\nY = 17.5 + 4.2 X + \u2208\n<\/center>\n<br>\nSo, real B0 is 17.5 and real B1 is 4.2. The error term (\u2208) is independent of X and has 0 mean.\n<br><br>We are going to generate a sample data of 30 observations (n).\n","cb925bd4":"Finally, a hipothesys test is performed to check if the coefficient is statistically significant difference from 0. This is specially interesting for the B1 coefficient, because it shows if it's a relationship between X and Y. We set two hipothesys:\n<center>\n    ${H}_{0}: {B}_{1} = 0$ \n<\/center><br>\n<center>\n    ${H}_{0}: {B}_{1} \\neq 0$ \n<\/center><br>\nWe are going to perform a test to see if we can reject the null hipothesys of no relationship between X and Y. The t-statistic gives us the result:\n<center>\n    $t = \\frac{\\hat{B}_{1} - 0}{SE(\\hat{B}_{1})}$ \n<\/center><br>","15b8ec21":"We can clearly see the positive relationship between X and Y, but we want to estimate the best linear estimation of Y based on X. So, we want to draw a line to estimate the values of Y with the best estimation possible, in other words, with the minimum error.<br><br> How we measure the error?<br><br> The prediction error (also called residual) is the distance between the predicted value (B<sub>0<\/sub> + B<sub>1<\/sub> \\*X) and the true value (Y). We should use the squared distance in order to avoid compensating negative and positive deviations. So we need to minimize the Residual Sum of Squares (RSS):\n<br>\n<br>\n<center>\n    RSS = (Y<sub>1<\/sub> - B<sub>0<\/sub> - B<sub>1<\/sub> *X<sub>1<\/sub>)<sup>2<\/sup> + (Y<sub>2<\/sub> - B<sub>0<\/sub> - B<sub>1<\/sub> *X<sub>2<\/sub>)<sup>2<\/sup> + ... + (Y<sub>n<\/sub> - B<sub>0<\/sub> - B<sub>1<\/sub> *X<sub>n<\/sub>)<sup>2<\/sup> \n<\/center>\n<br> This is why this method is called least squares.\n<br>Minimizing this function let us estimate B<sub>0<\/sub> and B<sub>1<\/sub>:\n<center>\n    $\\hat{B}_{1} = \\frac{\\sum_{i = 1}^{n} (x_{i}-\\overline{x})(y_{i}-\\overline{y})}{\\sum_{i = 1}^{n} (x_{i}-\\overline{x})^{2}}$\n<br> <br>\n$\\hat{B}_{0} = \\overline{y} - \\hat{B}_{1}\\overline{x}$  \n<\/center>\nFor the estimation of B<sub>1<\/sub> we need:<ul>\n    <li> the deviation of X from its mean multiplied by the deviation of Y from its mean for each observation <\/li>\n    <li> the deviation of X from its mean squared<\/li>","3500b6d6":"We have generated the X and Y observed data, the only data we have in real problems, where we don't know the real relationship and we will try to estimate it. <br>We can plot the data we have:","595f2b97":"We have the line that estimates the real relationship but this estimation was based only on the observed data. If we collect data again, we can get different observation and the new data would change the estimated equation, so we can ask ourselves, how reliable is this estimation?\n<br>\nTo answer this question, we calculate the standard error of B<sub>0<\/sub> and B<sub>1<\/sub>:\n<center>\n    $SE(\\hat{B}_{0})^{2} = \\sigma^{2}\\left[\\frac{1}{n}+\\frac{\\overline{x}^{2}}{\\sum_{i = 1}^{n}(x_{i}-\\overline{x})^{2}}\\right]$    \n<br><br>\n$SE(\\hat{B}_{1})^{2} = \\frac{\\sigma^{2}}{\\sum_{i = 1}^{n}(x_{i}-\\overline{x})^{2}}$ \n<\/center>\nBeing:\n<br>\n$\\sigma^{2} = \\frac{\\sum_{i = 1}^{n}(y_{i}-\\hat{y})^{2}}{n-2}$","4b9580bf":"Standard error of coefficients are useful for:<ul>\n    <li>calculate the confidence interval of the coefficient<\/li>\n    <li>check if the coefficient is stastistically significant different from 0<\/li><\/ul>\n<br><br>A certain % confidence intervale is a range that with a % probability contains the true value of the coefficiente. 95% confidence interval is the most commonly used. The value we need to perform this calculation comes from the t-distribution with n-2 degrees of freedom, and this value with a n close to 30 or larger is very close to 2. So, for the sake of simplicity, we will assume 2, and we will calculate the 95% confidence interval as:\n<center>\n    $\\left[\\hat{B}_{0}-2SE({B}_{0}), \\hat{B}_{0}+2SE({B}_{0})\\right]$\n<\/center><br>\n<center>\n    $\\left[\\hat{B}_{1}-2SE({B}_{1}), \\hat{B}_{1}+2SE({B}_{1})\\right]$\n<\/center>","a346be27":"We have the equation of the line:\n<center>\nY \u2248 15.28 + 4.18 X\n<\/center>\nAnd we can plot it:","25cb7393":"From the t-statistic, we should compute the probability of rejecting the null hipothesys by chance. That's what the p-value measures. For a p-value of 5% or 1% (the most commonly used) the corresponding t-statistics are 2 and 2.75 respectively. We got a t-statitic far higher than these values, so we can say that we reject the null hipothesys, there is a relationship between X and Y and the probability of getting this conclusion by chance is far less than 1%.","dfb3846a":"We have the line that estimates the real relationship but this estimation was based only on the observed data. If we collect data again, we can get different observation and the new data would change the estimated equation, so we can ask ourselves, how reliable is this estimation?\n<br>\nTo answer this question, we calculate the standard error of B<sub>0<\/sub> and B<sub>1<\/sub>:\n<center>\n    $SE(\\hat{B}_{0})^{2} = \\sigma^{2}\\left[\\frac{1}{n}+\\frac{\\overline{x}^{2}}{\\sum_{i = 1}^{n}(x_{i}-\\overline{x})^{2}}\\right]$    \n<br><br>\n$SE(\\hat{B}_{1})^{2} = \\frac{\\sigma^{2}}{\\sum_{i = 1}^{n}(x_{i}-\\overline{x})^{2}}$ \n"}}