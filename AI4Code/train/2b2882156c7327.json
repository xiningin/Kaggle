{"cell_type":{"14bb4646":"code","f7a90bde":"code","975ab46e":"code","5f2ae06e":"code","95fd88b4":"markdown","f516a560":"markdown","839870b7":"markdown","e5f405f0":"markdown","0400fcf8":"markdown"},"source":{"14bb4646":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pandas.api.types import is_string_dtype\nimport plotly.express as px\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\n\nprint(\"Dataset analysis fake news\")\nprint(\"Link - https:\/\/www.kaggle.com\/clmentbisaillon\/fake-and-real-news-dataset\")","f7a90bde":"df_fake_news = pd.read_csv(r\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")\ntokenizer = TweetTokenizer()\ntweet_tokens =[]\n\n#tokenize, remove stopwords, non-alphabetic words, lowercase\ndef preprocess(textstring):\n   stops =  set(stopwords.words('english'))\n   tokens = tokenizer.tokenize(textstring)\n   return [token.lower() for token in tokens if token.isalpha()\n          and token not in stops]\n\n\n\nfor text in df_fake_news['text'].values:\n    temp=preprocess(text)\n   ## print(temp)\n    tweet_tokens.extend(temp)\n\n##print(tweet_tokens)","975ab46e":"from wordcloud import WordCloud\n#convert list to string and generate\nunique_string=(\" \").join(tweet_tokens)\ntweet_word_cloud = WordCloud(width=500,height=350).generate(unique_string)\nplt.imshow(tweet_word_cloud)\nplt.axis(\"off\")\nplt.show()","5f2ae06e":"import spacy\nnlp = spacy.load(\"en_core_web_lg\")\nnlp.max_length = 39167694\ntweet_tokens_dup_removed = list(set(tweet_tokens))\n##split the list into 10 lists\nsplit_list = np.array_split(tweet_tokens_dup_removed,10)\nnamedEnt = []\nfor i in range(0,9) :\n    tmpList = split_list[i]\n    tmpList_str = \" \".join(tmpList)\n    doc = nlp(tmpList_str)\n    for ent in doc.ents :\n       namedEnt.extend(ent)\n\nprint(namedEnt)","95fd88b4":"The tokens visualisation using word-cloud . This will give a glimpse into the\ntarget of fake news be it nouns or verbs used in tweets","f516a560":"Management decision problem - Take corrective action in  order to curtail the impact of fake news.\n\nAnalytics Research Problem - Identify the targets of fake news and report to management so that they\ncan narrow down the scope .\n\nResearch Objective\n  1. Pre-process data.  2. Identify named entity about which fake news is spread.  3. Visualise","839870b7":"Below code can be used for summarisation if you use gensim older version. The latest 4.0.1 version gensim summarize package is removed\nimport sys\n!{sys.executable} -m pip install -U gensim\ndf_trump = df_fake_news[df_fake_news['text'].str.contains(\"trump\")]\ntrump_list = df_trump[\"text\"].tolist()\n##create text corpus only of tweets mentioning trump\ntrump_txt = \" \".join(trump_list)\n\n## summarise tweets\nfrom gensim.summarization import summarize\nprint(summarize(trump_txt))","e5f405f0":"Analysis Summary - As evident from word cloud target of fake news conversation is Donald Trump highlighted.\nThe campaign managers for Trump can use this analyis to further monitor and take strategic decisions to\nturn public sentiment towards Trump positively.\n\nSummarization of the conversations about Trump","0400fcf8":"As evident from wordcloud both entities and verbs\/topics are being seen here.\nPull  out the entities being target of fake news conversation"}}