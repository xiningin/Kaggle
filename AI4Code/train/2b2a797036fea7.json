{"cell_type":{"da01d980":"code","33110f0e":"code","2fea6504":"code","5af1b385":"code","885b4d2d":"code","60c07997":"code","324cebee":"code","9bdfa81f":"code","c35e50f6":"code","a6e21322":"code","8755f801":"code","1b88b493":"code","be6de4f2":"code","37d3dab7":"code","4be0d87b":"code","18e4a62d":"code","46aa8077":"code","638f2f7f":"code","4acf8772":"code","385da28f":"code","c074e959":"code","87bce3e4":"code","4a4d4932":"code","1c1f1e9c":"code","954f0253":"code","afe0f31b":"code","3b6313ea":"code","881bb738":"code","3ed8b043":"code","b1c5e961":"code","dcf24d5e":"code","bc04279d":"code","e1904245":"code","0696a19a":"code","c89daa7b":"code","5c5e7cdb":"code","347169c2":"code","8552a305":"code","5ac30b53":"code","145c7b68":"code","be412244":"code","62ff5fb0":"code","e4635004":"code","93bc4e42":"code","389a20c8":"code","89142134":"code","daf14821":"code","3d004cb0":"code","da53f5c3":"code","87a526fb":"code","11602b3e":"code","3b4abb00":"code","3428b6f5":"markdown","537cd8dc":"markdown","a0d85673":"markdown","ad81bc98":"markdown","1abb7ae5":"markdown","560d2f84":"markdown","9dab2447":"markdown","088163a4":"markdown","35f0ee05":"markdown","559a73f9":"markdown","1c71586e":"markdown"},"source":{"da01d980":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","33110f0e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn import preprocessing \nfrom category_encoders import *\nfrom sklearn.preprocessing import LabelEncoder\n%matplotlib inline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets, linear_model, metrics\nfrom sklearn.metrics import  confusion_matrix\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix","2fea6504":"df = pd.read_csv('..\/input\/projectgamepersona\/persona.csv')\ndf","5af1b385":"# Exploratory Data Analysis\ndef libraries():\n    global pd,np\n    import pandas as pd\n    import numpy as np\ndef load():\n    global df\n    df=pd.read_csv('..\/input\/projectgamepersona\/persona.csv')\n    \ndef top_rows(value):\n    print('\\033[1m'+ 'displaying the', value, 'rows from top'+'\\033[0m')\n    a=df.head(value)\n    print(a,'\\n')\n    \ndef bottom_rows(value):\n    print('\\033[1m'+'displaying the', value, 'rows from bottom'+'\\033[0m')\n    b=df.tail(value)\n    print(b,'\\n')\n    \ndef rows_columns():\n    print('\\033[1m'+'Shape of the Data set'+'\\033[0m')\n    c=df.shape\n    print(c,'\\n')\n    \ndef col_names():\n    print('\\033[1m'+'Column Names in the Data set'+'\\033[0m')\n    d=df.columns\n    print(d,'\\n')\n    \ndef information():\n    print('\\033[1m'+'Quick Overview of DataSet(info)'+'\\033[0m')\n    e = df.info()\n    print(e,'\\n')\n\ndef sizee():\n    print('\\033[1m'+'No.of Elements in the DataSet'+'\\033[0m')\n    f = df.size\n    print(f,'\\n')\n\ndef ndimension():\n    print('\\033[1m'+'Dimensions in your dataframe'+'\\033[0m')\n    g = df.ndim\n    print(g,'\\n')\n    \ndef stats_summary():\n    print('\\033[1m'+'Staistical Summary of DataSet'+'\\033[0m')\n    h = df.describe()\n    print(h,'\\n')\n    \ndef null_values():\n    print('\\033[1m'+'Number of Missing values in each column'+'\\033[0m')\n    i = df.isnull().sum()\n    print(i,'\\n')\n    \ndef n_unique():\n    print('\\033[1m'+'Number of unique elements'+'\\033[0m')\n    j = df.nunique()\n    print(j,'\\n')\n    \ndef memory_use():\n    print('\\033[1m'+'Memory used by all colomns in bytes'+'\\033[0m')\n    k = df.memory_usage()\n    print(k,'\\n')\n    \ndef is_na(value):\n    print('\\033[1m'+'Dataframe filled with boolean values with true indicating missing values'+'\\033[0m')\n    l = df.isna().head(value)\n    print(l,'\\n')\n    \ndef duplicate():\n    print('\\033[1m'+'Boolean Series denoting duplicate rows'+'\\033[0m')\n    m = df.duplicated().sum()\n    print(m,'\\n')\n    \ndef valuecounts():\n    print('\\033[1m'+'Series containing count of unique values'+'\\033[0m')\n    n = df.value_counts()\n    print(n,'\\n')\n\ndef datatypes():\n    print('\\033[1m'+'Datatype of each column'+'\\033[0m')\n    o = df.dtypes\n    print(o,'\\n')\n    \ndef correlation():\n    print('\\033[1m'+'Correalation between all columns in DataFrame'+'\\033[0m')\n    p = df.corr()\n    print(p,'\\n')\n    \ndef nonnull_count():\n    print('\\033[1m'+'Count of non-null values'+'\\033[0m')\n    q = df.count()\n    print(q,'\\n')\n    \ndef eda():\n    load()\n    datatypes()\n    top_rows(5)\n    bottom_rows(5)\n    rows_columns()\n    col_names()\n    information()\n    sizee()\n    ndimension()\n    stats_summary()\n    null_values()\n    n_unique()\n    memory_use()\n    is_na(5)\n    nonnull_count()\n    duplicate()\n    valuecounts()\n    correlation()\n    \n    \n    \n        \ndef stats_u(data,col):\n    if data[col].dtype == \"float64\":\n        print(col,\"has Quantitative data\")\n        mean_value=data[col].mean()\n        print('mean of',col,'column',mean_value)\n        max_value = data[col].max()\n        print('Maximum value of',col,'column',max_value)\n        min_value = data[col].min()\n        print('Minimum value of',col,'column',min_value)\n        median_value = data[col].median(skipna = True)\n        print('median of',col,'column',median_value)\n        std_value = data[col].std()\n        print('standard deviation of',col,'column',std_value)\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        print('quartile 1 of',col,'column is',q1)\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        print('quartile 2 of',col,'column is',q2)\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        print('quartile 3 of',col,'column is',q3)\n        q4 = data[col].quantile(1,interpolation='nearest')\n        print('quartile 4 of',col,'column is',q4)\n        IQR = q3 -q1\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        print('Lower Limit Point:',LLP)\n        print('Upper Limit Point:',ULP)\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers\")\n        else:\n            print(\"There are outliers\")\n            print(data[data[col]<LLP][col])\n            print(data[data[col]>ULP][col])\n            \n    elif data[col].dtype == \"int64\":\n        print(col,\"has Quantitative data\")\n        mean_value=data[col].mean()\n        print('mean of',col,'column',mean_value)\n        median_value = data[col].median(skipna = True)\n        print('median of',col,'column',median_value)\n        std_value = data[col].std()\n        print('standard deviation of',col,'column',std_value)\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        print('quartile 1 of',col,'column is',q1)\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        print('quartile 2 of',col,'column is',q2)\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        print('quartile 3 of',col,'column is',q3)\n        q4 = data[col].quantile(1,interpolation='nearest')\n        print('quartile 4 of',col,'column is',q4)\n        IQR = q3 -q1\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        print('Lower Limit Point:',LLP)\n        print('Upper Limit Point:',ULP)\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers\")\n        else:\n            print(\"There are outliers\")\n            print(\"Outliers are:\")\n            print(data[data[col]<LLP][col])\n            print(data[data[col]>ULP][col])\n    else:\n        print(col,'has Qualitative Data')\n        z = df[col].mode()\n        print('mode of',col,'column:\\n',z)\n        print('Count of mode is:\\n',df[col].value_counts())\n        print('Unique strings in',col,'are',data[col].nunique())\n        if(data[col].nunique() == 1):\n            print(col,'has same string')\n        elif(data[col].nunique() == 2):\n            print(col,'has binary strings')\n        else:\n            print(col,'has multi stings')\n\n\nlibraries()\neda()\n\nprint(\"----------------------------------------------------------------------------------------------------------------------\")\nprint('\\033[1m'+'Summary Of DataSet'+'\\033[0m')\nprint('\\033[1m'+'DataTypes in the DataSet:\\n'+'\\033[0m',df.dtypes)\nprint('\\033[1m'+'Columns in DataSet:'+'\\033[0m',df.columns)\nprint('\\033[1m'+'Shape of DataSet:'+'\\033[0m',df.shape)\nprint('\\033[1m'+'Size of DataSet:'+'\\033[0m',df.size)\nprint('\\033[1m'+'Dimension of DataSet:'+'\\033[0m',df.ndim)\nprint('\\033[1m'+'Total Memory used in DataSet:'+'\\033[0m',df.memory_usage().sum())\nprint('\\033[1m'+'Total Number of missing values in DataSet:'+'\\033[0m',df.isnull().sum().sum())\nprint('\\033[1m'+'Total Number of Unique values in DataSet:'+'\\033[0m',df.nunique().sum())\nprint('\\033[1m'+'Total Number of non null values in DataSet:'+'\\033[0m',df.count().sum())\nprint('\\033[1m'+'Total Number of duplicate rows in DataSet:'+'\\033[0m',df.duplicated().sum())\nprint(\"----------------------------------------------------------------------------------------------------------------------\")\nprint('\\033[1m'+'Summary Of Each Colomn'+'\\033[0m')\nprint(\"\\n\")\ncols=df.columns\ncols\nfor i in cols:\n    print('\\033[1m'+i+'\\033[0m')\n    stats_u(df,i)\n    print(\"\\n\")\n            \n","885b4d2d":"df.head()","60c07997":"df.tail()","324cebee":"df.shape","9bdfa81f":"df.size","c35e50f6":"df.dtypes","a6e21322":"df.columns","8755f801":"df.info()","1b88b493":"df.describe()","be6de4f2":"df.corr()","37d3dab7":"df.skew()","4be0d87b":"df.duplicated().sum()","18e4a62d":"df.isnull().sum()","46aa8077":"! pip install Autoviz","638f2f7f":"! pip install Autoviz","4acf8772":"! pip install xlrd","385da28f":"from autoviz.AutoViz_Class import AutoViz_Class\nAV = AutoViz_Class()\ndf_av = AV.AutoViz('..\/input\/projectgamepersona\/persona.csv')","c074e959":"df['SOURCE'].value_counts()","87bce3e4":"sns.countplot(x = 'SOURCE',data = df)\nplt.show()","4a4d4932":"df['SEX'].value_counts()","1c1f1e9c":"sns.countplot(x = 'SEX',data = df)\nplt.show()","954f0253":"df['COUNTRY'].value_counts()","afe0f31b":"sns.countplot(x = 'COUNTRY',data = df)\nplt.show()","3b6313ea":"# 3 categorical columns\nobj = []\nfor i in df.columns:\n    if df[i].dtypes == 'object':\n        obj.append(i)\nobj#categorical column name","881bb738":"for i in obj:\n    data=df.copy()\n    data.groupby(i)['PRICE'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('PRICE')\n    plt.title(i)\n    plt.show()","3ed8b043":"for i in obj:\n    data=df.copy()\n    data.groupby(i)['AGE'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('AGE')\n    plt.title(i)\n    plt.show()","b1c5e961":"num = []\nfor i in df.columns:\n    if i not in obj:\n        num.append(i)\nnum","dcf24d5e":"for i in range(len(obj)):\n    x='SOURCE'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","bc04279d":"for i in range(len(obj)):\n    x='SEX'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","e1904245":"for i in range(len(obj)):\n    x='COUNTRY'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","0696a19a":"for i in range(len(obj)):\n    x='AGE'\n    for j in range(len(num)):\n        if num[j] != x:\n            sns.scatterplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","c89daa7b":"for i in range(len(obj)):\n    x='PRICE'\n    for j in range(len(num)):\n        if num[j] != x:\n            sns.scatterplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","5c5e7cdb":"plt.figure(figsize=(6,8))\nx = df.drop(obj,axis = 1)\nfor i in x.columns:\n    sns.histplot(x[i],kde = True)\n    plt.show()","347169c2":"x = df.drop(obj,axis = 1)\nfor i in x.columns:\n    sns.boxplot(x = i, data = x,color = 'yellowgreen')   \n    plt.xlabel(i)\n    plt.show()","8552a305":"x = df.drop(obj,axis = 1)\nfor i in x.columns:\n    sns.violinplot(x = i, data = x,color = 'yellowgreen')   \n    plt.xlabel(i)\n    plt.show()","5ac30b53":"corrmat = df.corr()\ntop_corr_features = corrmat.index\n\nplt.figure(figsize=(20,15))\n\n#plot heat map\ng=sns.heatmap(df.corr(),annot=True,cmap=\"RdYlGn\")","145c7b68":"sns.pairplot(df)","be412244":"def count_outliers(data,col):\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        q4 = data[col].quantile(1,interpolation='nearest')\n        IQR = q3 -q1\n        global LLP\n        global ULP\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers in\",i)\n        else:\n            print(\"There are outliers in\",i)\n            x = data[data[col]<LLP][col].size\n            y = data[data[col]>ULP][col].size\n            a.append(i)\n            print('Count of outliers are:',x+y)\nglobal a\na = []\nfor i in x.columns:\n    count_outliers(df,i)","62ff5fb0":"df1=pd.get_dummies(data=df,columns=['SEX','COUNTRY'],drop_first=True)\ndf1","e4635004":"from sklearn import preprocessing\nlabel_encoder = preprocessing.LabelEncoder() \nc1 = 'SOURCE'\ndf1[c1]= label_encoder.fit_transform(df1[c1]) \ndf1[c1].unique()\ndf1\n# label encoding target column source","93bc4e42":"scaler = StandardScaler()\nscaler.fit(df1.drop('SOURCE',axis = 1))","389a20c8":"scaled_features = scaler.transform(df1.drop('SOURCE',axis = 1))\ndf_feat = pd.DataFrame(scaled_features,columns = ['PRICE', 'AGE', 'SEX_male', 'COUNTRY_can', 'COUNTRY_deu',\n       'COUNTRY_fra', 'COUNTRY_tur', 'COUNTRY_usa'])\ndf_feat.head()","89142134":"X = df_feat\ny = df['COUNTRY']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train,y_train)","daf14821":"pred = knn.predict(X_test)\npred","3d004cb0":"print(confusion_matrix(y_test,pred))","da53f5c3":"print(classification_report(y_test,pred,zero_division=0))","87a526fb":"error_rate= []\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","11602b3e":"plt.figure(figsize = (10,6))\nplt.plot(range(1,40),error_rate,color = 'blue',linestyle = '--',marker = 'o',markerfacecolor='red',markersize = 10)\nplt.title('Error Rate vs K')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","3b4abb00":"print(metrics.accuracy_score(y_test, pred))","3428b6f5":"# Exploratory Data Analysis using bulitin function(AUTOMATION)","537cd8dc":"# Exploratory Data Analysis","a0d85673":"# Data Preprocessing","ad81bc98":"# Feature Scaling","1abb7ae5":"# Feature Selection","560d2f84":"# Count of Outliers","9dab2447":"# Importing Libraries","088163a4":"# Prediction of SOURCE uisng KNN","35f0ee05":"# Data Visualisation","559a73f9":"# Loading DataSet","1c71586e":"## UPVOTE IF U LIKE"}}