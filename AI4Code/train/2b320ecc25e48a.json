{"cell_type":{"7bdc6306":"code","04c9bec6":"code","c9c7c2fb":"code","3914b54b":"code","fb8b4661":"code","46faab9b":"code","4eb52401":"code","a7336d55":"code","0d60deed":"code","5d3e8053":"code","fdf56d31":"code","923df16c":"code","2d5017dd":"code","2e8b462d":"code","91b0d036":"code","bcf6633e":"code","d046ba56":"code","e486a5ae":"code","18a74bfe":"code","b0e9faa4":"code","04b30052":"code","b8eb71e1":"code","f278e679":"code","06ad3554":"code","6122526e":"code","120e2b5e":"code","bdc489b1":"code","34ca9e08":"code","f8073e85":"code","068608ff":"code","c9da44a0":"code","3bac7512":"code","a1cf54c2":"code","b13a4856":"code","634b17d2":"code","33a2a413":"code","a42517b3":"code","de413f6a":"code","6af16758":"code","5884a7d1":"code","a642d764":"code","945998a6":"markdown","c1cff0dd":"markdown","b896cadf":"markdown","464ad651":"markdown","6fa66631":"markdown","a696f7b7":"markdown","a7f0a1bc":"markdown","d0f9e837":"markdown","fc6170c5":"markdown","75457f76":"markdown","26c4150d":"markdown","159848af":"markdown","625507b6":"markdown","19747a98":"markdown","0d265fdc":"markdown","ea008795":"markdown","afacc93b":"markdown","a7c7c4be":"markdown","615cf839":"markdown","3f3b3133":"markdown"},"source":{"7bdc6306":"import math\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm \nfrom IPython.display import clear_output\n\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split","04c9bec6":"# Loading datasets\n\n# Contains date features, like week of year and holidays\ncalendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv', index_col='d')\n\n# Contains item sales by day, the target features \nsales = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\n\n# Contais item sell prices by departments\nsell = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\n","c9c7c2fb":"# Days in past to retrive data for model training \nLAST_N_DAYS = 365 * 2","3914b54b":"# CALCULATE THE SAMPLE SIZE\n# Original code from Marek Madejski in his github repository: https:\/\/github.com\/veekaybee\/data\/blob\/master\/samplesize.py\n# Extracted from his article for sample size: http:\/\/veekaybee.github.io\/2015\/08\/04\/how-big-of-a-sample-size-do-you-need\/ \ndef get_sample_size(population_size, confidence_level, confidence_interval):\n    # SUPPORTED CONFIDENCE LEVELS: 50%, 68%, 90%, 95%, and 99%\n    confidence_level_constant = [50,.67], [68,.99], [90,1.64], [95,1.96], [99,2.57]\n    \n    Z = 0.0\n    p = 0.5\n    e = confidence_interval\/100.0\n    N = population_size\n    n_0 = 0.0\n    n = 0.0\n\n    # LOOP THROUGH SUPPORTED CONFIDENCE LEVELS AND FIND THE NUM STD\n    # DEVIATIONS FOR THAT CONFIDENCE LEVEL\n    for i in confidence_level_constant:\n        if i[0] == confidence_level:\n            Z = i[1]\n\n    if Z == 0.0:\n        return -1\n\n    # CALC SAMPLE SIZE\n    n_0 = ((Z**2) * p * (1-p)) \/ (e**2)\n\n    # ADJUST SAMPLE SIZE FOR FINITE POPULATION\n    n = n_0 \/ (1 + ((n_0 - 1) \/ float(N)) )\n\n    return int(math.ceil(n)) # THE SAMPLE SIZE","fb8b4661":"# Calculating the sample size\npopulation = len(sales)\nconfidence_level = 99.0\nmargin_error_acceptable = 1\n\nsample_size = get_sample_size(population, confidence_level, margin_error_acceptable)\n\nprint(f\"Sample size needed: {sample_size}\")","46faab9b":"sales.store_id.value_counts()","4eb52401":"stores_quantity = len(sales.store_id.unique())\n\nstore_sample_size = round(sample_size \/ stores_quantity)\n\nprint(f\"Each store sample size will have {store_sample_size} items\")","a7336d55":"def get_item_samples_by_store(store_id, sample_size):\n        \n    temp = sales.query(f\"store_id == '{store_id}'\").copy()\n    temp['index'] = temp.index\n    \n    # List that will containing the sample indexes\n    items_sample_indexes = []\n    \n    for sample in range(sample_size):\n        row = random.randint(0, len(temp)-1)\n\n        item_index = temp.iloc[row]['index']\n\n        items_sample_indexes.append(item_index)\n        \n        # Drops the sample drawn \n        temp.drop(item_index, axis=0, inplace=True)\n            \n    return items_sample_indexes","0d60deed":"stores = sales.store_id.unique()\n\nsamples_by_store = {}\n\n# Progress bar params\ntotal = len(stores)\ndesc = 'Drawing samples by store'\n    \nfor store in tqdm(stores, total=total, desc=desc):\n    items_index_list = get_item_samples_by_store(store, store_sample_size)\n    samples_by_store.update( {store : items_index_list} )","5d3e8053":"# Preparing feature name cols\n\ninitial_day = 1914 - LAST_N_DAYS\n\ntrain_indexes = [\n    f\"d_{x}\"\n    for x in range(initial_day, 1914)\n]\n\ntarget_indexes = [\n    f\"d_{x}\"\n    for x in range(1914, 1970)\n]\n\nsales_train_features = list(sales.columns[1:6])\nsales_train_features","fdf56d31":"# Dataframes to get features and targets data\n\n# Contains historical sales for each day and item\ndf_targets = sales[train_indexes].copy()\ndf_targets","923df16c":"# Contains store and item names\ndf_features = sales[sales_train_features].copy()\ndf_features","2d5017dd":"# Calendar (empty) dataframe for prediction\nlast = len(target_indexes) - 1\ncalendar_predict = calendar.loc[target_indexes[0]:target_indexes[last]]\ncalendar_predict","2e8b462d":"# Filtering calendar dataset for data from LAST_N_DAYS\nlast = len(train_indexes) - 1\ncalendar = calendar.loc[train_indexes[0]:train_indexes[last]]\ncalendar","91b0d036":"# Historical sell data\nsell = sell.query(f\"wm_yr_wk >= {min(calendar.wm_yr_wk)}\")\nsell","bcf6633e":"# Relevant features in calendar\ncalendar_cols = [\n    'wm_yr_wk',\n    'wday',\n    'month',\n    'year',\n    'event_type_1',\n    'event_type_2'\n]\n\ncalendar = calendar[calendar_cols].copy()\ncalendar_predict = calendar_predict[calendar_cols].copy()\n\ncalendar","d046ba56":"from sklearn.preprocessing import LabelEncoder\n\nfeatures_encoder = LabelEncoder()\ncalendar_encoder = LabelEncoder()\n\n# Encoding sales features\ndf_features_encoded = df_features.apply(features_encoder.fit_transform)\n\n# Encoding calendar features\ncalendar_cols_encode = calendar.select_dtypes('object').columns\n\ncalendar[calendar_cols_encode] = calendar[calendar_cols_encode].fillna('-')\ncalendar[calendar_cols_encode] = calendar[calendar_cols_encode].apply(calendar_encoder.fit_transform)\n\ncalendar_predict[calendar_cols_encode] = calendar_predict[calendar_cols_encode].fillna('-')\ncalendar_predict[calendar_cols_encode] = calendar_predict[calendar_cols_encode].apply(calendar_encoder.fit_transform) \n\ncalendar","e486a5ae":"# Sells features\nsells_encoder = LabelEncoder()\n\nsells_encode_cols = ['store_id', 'item_id']\nsells_features = ['wm_yr_wk', 'sell_price']\n\nsell_encoded = sell[sells_encode_cols].apply(sells_encoder.fit_transform) \nsell_encoded[sells_features] = sell[sells_features]\n\nsell_encoded","18a74bfe":"# Multidimension dataframes (for each store)\ntrain_data = []\n\n# Just print processing stores rows\ncurrent_store = 0\ntotal_stores = len(samples_by_store)\n\n    \nfor stores_samples in samples_by_store.values():\n    current_store += 1\n    for index in tqdm(stores_samples, total=len(stores_samples), desc=f'Creating mutidimensional dataframe {current_store}\/{total_stores}'):\n        # Transposing targets for each item\n        # New datafame in multidimension dataframe\n        tmp_train_data = pd.DataFrame()\n        \n        # Calendar features\n        for col in calendar_cols:\n            tmp_train_data[col] = np.array(calendar[col])\n\n        # Unique id to match sell price in sell dataframe\n        item_id = df_features.loc[index, 'item_id']\n        store_id = df_features.loc[index, 'store_id']\n\n        # Sales features\n        for feature in sales_train_features:\n            tmp_train_data[feature] = df_features_encoded.loc[index, feature]\n\n        # Input sell price    \n        min_week = min(tmp_train_data.wm_yr_wk)\n\n        sell_cols = ['wm_yr_wk', 'item_id', 'store_id', 'sell_price']\n        sell_item = sell_encoded.query(f\"item_id == '{item_id}' & store_id == '{store_id}' & wm_yr_wk >= {min_week}\")[sell_cols]\n\n        tmp_train_data['sell_price'] = tmp_train_data.merge(sell_item, on='wm_yr_wk', how='left')['sell_price']\n\n        # No sell price indicates no sell made\n        tmp_train_data['sell_price'].fillna(0, inplace=True)\n\n        # Target transpose from row to column\n        tmp_train_data['target'] = np.array(df_targets.iloc[index].values)\n        \n        train_data.append(tmp_train_data)\n        \n    clear_output(wait=True)","b0e9faa4":"X = pd.DataFrame()\ny = np.array([])\n\nfor dataset in train_data:\n    X = X.append(dataset.loc[:, dataset.columns != 'target'])\n    y = np.append(y, dataset['target'].to_numpy())","04b30052":"# year col is not necessary for predicting\n# causes a worst prediction\nX.drop('year', axis=1, inplace=True) ","b8eb71e1":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)","f278e679":"# Params for evaluation\nparams_xgb = {\n    \"objective\": \"reg:squarederror\",\n    'max_depth': 50, \n    'min_child_weight': 1,    \n    'learning_rate': 0.05,\n    'alpha': 10,\n    'gamma': 10\n}\n\nfit_params = {\n    'early_stopping_rounds': 30,\n    'eval_metric': 'mae',\n    'eval_set': [[X_test, y_test]]\n}","06ad3554":"def get_xg_boost_mae(hyper_params, fit_params, X, y):\n    model_xgb = XGBRegressor(\n        n_estimators=1000,\n        objective=hyper_params['objective'],\n        max_depth=hyper_params['max_depth'],\n        min_child_weight=hyper_params['min_child_weight'],\n        alpha=hyper_params['alpha'],\n        gamma=hyper_params['gamma'],\n        n_jobs=4\n    )\n    return cross_val_score(\n        model_xgb, \n        X, y, \n        cv=5, \n        scoring='neg_mean_absolute_error', \n        fit_params=fit_params\n    ).mean()","6122526e":"mae = get_xg_boost_mae(params_xgb, fit_params, X_train, y_train)","120e2b5e":"mae * -1","bdc489b1":"def set_xgb_model():\n    return XGBRegressor(\n        n_estimators=1000,\n        objective=params_xgb['objective'],\n        max_depth=params_xgb['max_depth'],\n        min_child_weight=params_xgb['min_child_weight'],\n        alpha=params_xgb['alpha'],\n        gamma=params_xgb['gamma'],\n        early_stopping_rounds=fit_params['early_stopping_rounds'],\n        learning_rate=params_xgb['learning_rate'], \n        n_jobs=4  \n    )","34ca9e08":"# Preparing dataset for training model\ntrain_data = X.copy()\ntrain_data['target'] = y","f8073e85":"# Training a model for each store\nmodels = []\n\nfor store in tqdm(X.store_id.unique(), total=len(X.store_id.unique()), desc='Fitting a model for each store...'):\n    models.append(set_xgb_model())\n    \n    # X_store: contains data only for the store model\n    X_store = train_data.loc[train_data.store_id == store].copy() \n    \n    # Important: drop target for X training\n    X_store.drop('target', axis=1, inplace=True)\n     \n    y_store = train_data.loc[train_data.store_id == store]['target']\n    \n    models[store].fit(X_store, y_store)   ","068608ff":"# Creating dataframe to with features to predict sales \ndf_predict = pd.DataFrame()\n\nsell_cols = ['wm_yr_wk', 'item_id', 'store_id', 'sell_price']\n\nfor index in tqdm(sales.index, total=len(sales), desc=f'Creating predict dataframe'):\n    # Transposing targets for each item\n    # New datafame in multidimension dataframe\n    tmp_df = pd.DataFrame()\n\n    # Calendar features\n    for col in calendar_cols:\n        tmp_df[col] = np.array(calendar_predict[col])\n\n    # Sales features\n    for feature in sales_train_features:\n        tmp_df[feature] = df_features_encoded.loc[index, feature]\n\n    # Input sell price\n    min_week = min(tmp_df.wm_yr_wk)\n    \n    sell_item = sell_encoded.query(f\"item_id == '{item_id}' & store_id == '{store_id}' & wm_yr_wk >= {min_week}\")[sell_cols]\n\n    tmp_df['sell_price'] = tmp_df.merge(sell_item, on='wm_yr_wk', how='left')['sell_price']\n\n    # No sell price indicates no sell made\n    tmp_df['sell_price'].fillna(0, inplace=True)\n \n    df_predict = df_predict.append(tmp_df)  \n\nclear_output(wait=True)","c9da44a0":"# Analyzing MAE for different hyper params\n# predict with year column causes a worst prediction\n# So, let's drop it\ndf_predict.drop('year', axis=1, inplace=True) ","3bac7512":"# Creating target column\ndf_predict['predictions'] = None","a1cf54c2":"# Making predictions for items by each store model\nfor store in tqdm(X.store_id.unique(), total=len(X.store_id.unique()), desc='Making predictions for each store...'):\n    \n    # Getting a dataset contaning a single store\n    to_predict = df_predict.loc[df_predict.store_id == store].copy()\n    \n    # Drop null predictions column for this dataset to be predicted \n    to_predict.drop('predictions', axis=1, inplace=True)\n\n    # Making predictions using the specific store model    \n    predictions = np.round(models[store].predict(to_predict))\n\n    # Assigning the predictions in result dataset    \n    df_predict.loc[df_predict.store_id == store, 'predictions'] = np.absolute(predictions)","b13a4856":"# item_id decoded\ndf_predict['item_id'] = sells_encoder.inverse_transform(df_predict['item_id'])\ndf_predict.head() ","634b17d2":"# This inverse_transform for store_id is not working, so its made manually\nreverse_encode_dict = {\n    key : value\n    for key, value in zip(df_features_encoded.store_id.unique(), df_features.store_id.unique())\n} \n\ndf_predict.replace({\"store_id\": reverse_encode_dict}, inplace=True) ","33a2a413":"# Preparing item name for submission dataset\ndf_predict['item_validation'] = df_predict.apply(lambda x: x.item_id + '_' + x.store_id , axis=1)\ndf_predict","a42517b3":"# Preparing submission dataset\ndf_predict_submission = df_predict[['item_validation', 'predictions']]","de413f6a":"submission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')","6af16758":"for item in tqdm(df_predict.item_validation.unique(), total=len(df_predict.item_validation.unique()), desc='Preparing prediction dataset...'):\n    \n    item_predictions = df_predict.query(f'item_validation == \"{item}\"')['predictions']\n\n    submission.loc[submission.id == item+'_validation', 'F1':'F28'] = item_predictions[:28].ravel()\n    submission.loc[submission.id == item+'_evaluation', 'F1':'F28'] = item_predictions[28:].ravel()\n    \n    # Removing already proccessed predictions to optimize execution \n    df_predict = df_predict.iloc[28:]","5884a7d1":"submission.to_csv('submission.csv')  ","a642d764":"submission","945998a6":"Making cross validation","c1cff0dd":"### Check items by store\nTo define the sample data, we will divide it proportionally according to the quantity of items per store.","b896cadf":"### Defining `X` and `y` data","464ad651":"Calculating the sample size needed to make predictions. At this point in competiton, to improve my leaderboard, I make aggressive parameters.\n\nIn real life (and avoid overfitting too) the good confidence level is about 95 and margin error acceptable is about 2.","6fa66631":"## Predict\n\nFirst of all, lets prepare the predict dataframe","a696f7b7":"Drawing samples for each store\n","a7f0a1bc":"## Sample from population\nThe dataset are huge*. There are over 30k of items and each of them have historical sales data over `LAST_N_DAYS` days. \n\nLets use a sample of data sets for predictions. \n\n\\* *In this study, its about 22 milion rows of data (30490 items x 730 days)* ","d0f9e837":"### Splitting train and test data","fc6170c5":"### Model evaluation\n\nThe best model prediction was got using `XGBoost`. The evaluation was made by tries with different hyper params, and comparing to `RandomForest` model too.\n\nI tried to run a RandomGridSearch to find the best params, but this runs hours by all night along and in the end, causes a memory exception, so that's very difficult to use in this case. ","75457f76":"### Transposing data from `sell` and `target` dataframes\nData transposing for preparing datasets to made sales predictions","26c4150d":"Well, we have the same quantitiy of items in each store. So, we must divide the sample equally for each store.","159848af":"Reversing encoded features","625507b6":"### Parameters","19747a98":"## Preprocessing data\n\nCleaning dataframes to preprocessing","0d265fdc":"## Objective\n\nThe detailed competition objective and rules are described in [M5 Forecasting - Accuracy](https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy) competition page.\n\nThe summary is that: given a retail product and stores historical sales by day dataset, the goal is forecast the sales by day over 30k products for next 27 days.","ea008795":"### Train\n\nXGBoost model with best hyper params","afacc93b":"Making predictions","a7c7c4be":"### Notes\n- `d_` columns are the goal columns in past to use in model.\n- The predictions are about columns `d_1914 - d_1941` ","615cf839":"### Encoding string features ","3f3b3133":"### Split the store samples"}}