{"cell_type":{"cef2003f":"code","54d9ee2d":"code","02e1817a":"code","df08c870":"code","dc42c6c7":"code","f4b62a34":"code","d543bdc0":"code","1bb9b519":"code","96e43dc9":"code","375ba420":"code","50084e06":"code","b70d0660":"code","ec0ca43e":"code","b5a8bb9b":"code","dbdac201":"code","d4b95e41":"code","e8299357":"code","3cdd323d":"code","f3aa290a":"code","06396ade":"code","9eed9c35":"code","49861202":"code","c3f5d844":"code","308567de":"code","b8665e50":"code","a0f33e5f":"code","19e7027d":"code","a1c50e03":"code","9a38b494":"code","ace73178":"code","283e3ab2":"code","f4f956f7":"code","75c9db55":"code","6b3e50ba":"code","12732473":"code","9c3f1e46":"code","0a09fe50":"code","e4f0abab":"code","016ade2e":"code","8bca4a70":"code","953ca8c2":"code","034608aa":"code","dd2dd8dc":"code","9297ee03":"code","044f76c5":"code","efec900e":"code","00825f35":"code","71815b62":"code","f76d0649":"markdown","7840dadb":"markdown","33f6bdda":"markdown","fa0b21d8":"markdown"},"source":{"cef2003f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","54d9ee2d":"import matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport seaborn as sns\nimport warnings\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold   #For K-fold cross validation\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz\nfrom sklearn.metrics import mean_squared_error as mse, mean_squared_log_error as msle, r2_score as r2\nfrom sklearn.model_selection import cross_val_score\n","02e1817a":"train = pd.read_csv(\"\/kaggle\/input\/mengary-revenue-prediction\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/mengary-revenue-prediction\/test.csv\")\nsub = pd.read_csv(\"\/kaggle\/input\/mengary-revenue-prediction\/sampleSolution.csv\")","df08c870":"print(train.shape)","dc42c6c7":"train.isnull().sum()","f4b62a34":"#train = train.dropna()\ntrain = train.drop_duplicates()","d543bdc0":"print(train.shape)","1bb9b519":"train.head()","96e43dc9":"cat_col = train.select_dtypes(include=['object']).columns\nnum_col = train.select_dtypes(exclude=['object']).columns","375ba420":"#ax = sns.pairplot(train[num_col])","50084e06":"sns.boxplot(x='price',data=train)","b70d0660":"sns.boxplot(x='price',data=test)","ec0ca43e":"#train = train.drop(train[(train['price']>15000)].index)","b5a8bb9b":"sns.boxplot(x='profit',data=train)","dbdac201":"#train = train.drop(train[(train['profit']>4000)].index)\n#train = train.drop(train[(train['profit']<-2000)].index)","d4b95e41":"from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(standardize=False)\n\ntrain['profit'] = pt.fit_transform(train['profit'].values.reshape(-1,1))","e8299357":"# Merging Train and Test Data\ndf = pd.concat([train,test], ignore_index = True, sort = False)","3cdd323d":"df.head()","f3aa290a":"df.tail()","06396ade":"df1 = df.groupby(['class','sub-class'])['profit'].mean()","9eed9c35":"df1=df1.reset_index()\ndf1=df1.rename(columns={'profit':'class_mean_profit'})\n\ndf1.head()","49861202":"#df = pd.merge(df, df1,  how='left', left_on=['class','sub-class'], right_on = ['class','sub-class'])","c3f5d844":"df.head()","308567de":"df['delivery type'] = df['delivery type'].fillna('Standard Class')","b8665e50":"df['discount_price'] = df['discount'] * df['price']\ndf['MRP'] = df['discount_price'] + df['price']","a0f33e5f":"pt1 = PowerTransformer()\n\ndf['price'] = pt1.fit_transform(df['price'].values.reshape(-1,1))\ndf['discount_price'] = pt1.fit_transform(df['discount_price'].values.reshape(-1,1))\ndf['MRP'] = pt1.fit_transform(df['MRP'].values.reshape(-1,1))","19e7027d":"df['delivery date'].unique()","a1c50e03":"df['placement date'] = pd.to_datetime(df['placement date'], format='%Y-%m-%d')\ndf['delivery date'] = pd.to_datetime(df['delivery date'],  format='%Y-%d-%m')\n\ndf['scheduleDiff'] = (df['delivery date'] - df['placement date']).map(lambda x:str(x).split()[0])\ndf['scheduleDiff'] = pd.to_numeric(df['scheduleDiff'])","9a38b494":"df['location'] = df['location'].replace( {'Central':0 , 'South':1, 'West':2, 'East':3} )\ndf['class'] = df['class'].replace( {'kariox':0, 'qexty':1, 'fynota':2} )\ndf['segment'] = df['segment'].replace( {'Consumer':0, 'Corporate':1, 'Home Office':2} )\ndf['delivery type'] = df['delivery type'].replace( {'Standard Class':0, 'First Class':1, 'Second Class':2, 'Same Day':3} )","ace73178":"df['sub-class'] = df['sub-class'].factorize()[0]\ndf['departure city'] = df['departure city'].factorize()[0]\ndf['departure state'] = df['departure state'].factorize()[0]\ndf['delivery date'] = df['delivery date'].factorize()[0]\ndf['placement date'] = df['placement date'].factorize()[0]","283e3ab2":"df['isProfit'] = np.where(df['discount'] >= 0.2, 1, 0)","f4f956f7":"temp = df.groupby(['sub-class']).agg({'discount':['count','mean','sum'],\n                                      'price':['count','mean','sum'],\n                                      'no of items':['count','mean','sum'],\n                                      'MRP':['count','mean','sum'],\n                                      'discount_price':['count','mean','sum'],\n                                      'departure city':['count','nunique'],\n                                      'location':['count'],\n                                      'segment':['count'],\n                                      'departure state':['count','nunique'],\n                                      'address code':['count','nunique'],\n                                      })\ntemp.columns = ['_'.join(x) for x in temp.columns]\ndf = pd.merge(df,temp,on=['sub-class'],how='left')","75c9db55":"df.head()","6b3e50ba":"import seaborn as sns\nplt.subplots(figsize = (24,24))\nsns.heatmap(df.corr(),annot=True)","12732473":"df.drop(columns=['id','RID','class'],inplace=True)","9c3f1e46":"Target = 'profit'\n\ntrain_df = df[~df.profit.isna()]\ntest_df = df[df.profit.isna()]\n\nfeatures = [c for c in train_df.columns if c not in [Target]]","0a09fe50":"print(train_df.shape, test_df.shape)","e4f0abab":"train_df.head()","016ade2e":"from sklearn.metrics import mean_squared_error, mean_squared_log_error,mean_absolute_error\n\nfrom sklearn import metrics\ndef rmsle(y_true, y_pred):\n  return np.sqrt(mean_squared_log_error(y_true, y_pred))\n\ndef av_metric(y_true, y_pred):\n  results = metrics.r2_score(y_true, y_pred)\n  #results = 100*max(0, 1 -mean_squared_log_error(y_true, y_pred))\n  return results\n\ndef av_metric1(y_true, y_pred):\n  results = mean_absolute_error(y_true, y_pred)\n  #results = 100*max(0, 1 -mean_squared_log_error(y_true, y_pred))\n  return results*100","8bca4a70":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\n\ndef run_clf_kfold(clf, train, test, features):\n\n  N_SPLITS = 5\n\n  oofs = np.zeros(len(train))\n  preds = np.zeros((len(test)))\n\n  target = train[Target]\n\n  folds = StratifiedKFold(n_splits = N_SPLITS)\n  stratified_target = pd.qcut(train[Target], 10, labels = False, duplicates='drop')\n\n  feature_importances = pd.DataFrame()\n\n  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, stratified_target)):\n    print(f'\\n------------- Fold {fold_ + 1} -------------')\n\n    ############# Get train, validation and test sets along with targets ################\n  \n    ### Training Set\n    X_trn, y_trn = train[features].iloc[trn_idx], target.iloc[trn_idx]\n\n    ### Validation Set\n    X_val, y_val = train[features].iloc[val_idx], target.iloc[val_idx]\n\n    ### Test Set\n    X_test = test[features]\n\n    ############# Scaling Data ################\n    scaler = StandardScaler()\n    _ = scaler.fit(X_trn)\n\n    X_trn = scaler.transform(X_trn)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n\n\n    ############# Fitting and Predicting ################\n\n    _ = clf.fit(X_trn, y_trn)\n\n    fold_importance = pd.DataFrame({'fold': fold_ + 1, 'feature': features, 'importance': clf.feature_importances_})\n    feature_importances = pd.concat([feature_importances, fold_importance], axis=0)\n\n    ### Instead of directly predicting the classes we will obtain the probability of positive class.\n    preds_val = clf.predict(X_val)\n    preds_test = clf.predict(X_test)\n\n    fold_score = av_metric(y_val, preds_val)\n    print(f'\\nAV metric score for validation set is {fold_score}')\n\n    oofs[val_idx] = preds_val\n    preds += preds_test \/ N_SPLITS\n\n\n  oofs_score = av_metric(target, oofs)\n  oofs_score1 = av_metric1(target, oofs)\n\n  print(f'\\n\\nAV metric for oofs is {oofs_score}')\n  print(f'\\n\\nAV metric for oofs is {oofs_score1}')\n\n  feature_importances = feature_importances.reset_index(drop = True)\n  fi = feature_importances.groupby('feature')['importance'].mean().sort_values(ascending = False)[:20][::-1]\n  fi.plot(kind = 'barh', figsize=(20, 10))\n\n  return oofs, preds,fi\n","953ca8c2":"clf = ExtraTreesRegressor(n_estimators =700, max_depth =100)\n\ndtet_oofs, test_pred, feature_importance = run_clf_kfold(clf, train_df, test_df, features)","034608aa":"test_pred","dd2dd8dc":"sub['profit'] = test_pred\nsub['profit'] = pt.inverse_transform(sub['profit'].values.reshape(-1,1))\nsub.to_csv(\"final.csv\",index=False)","9297ee03":"sub.head()","044f76c5":"clf1 = ExtraTreesRegressor(n_estimators =800, max_depth =100)\n\ndtet_oofs, test_pred, feature_importance = run_clf_kfold(clf1, train_df, test_df, features)","efec900e":"xgb = XGBRegressor(n_estimators=800)\n\ndtet_oofs, test_pred, feature_importance = run_clf_kfold(xgb, train_df, test_df, features)","00825f35":"!pip install catboost","71815b62":"from catboost import CatBoostRegressor\nm = CatBoostRegressor(n_estimators=5000, random_state=1994, eval_metric='R2', learning_rate=0.03, max_depth=5)\n\n\ndtet_oofs, test_pred, feature_importance = run_clf_kfold(m, train_df, test_df, features)\n","f76d0649":"**CatBoost**","7840dadb":"**Model Testing**","33f6bdda":"**Extra Trees Classifier**","fa0b21d8":"**XGBoost**"}}