{"cell_type":{"a2b938fd":"code","80e29422":"code","5fc00cbd":"code","2d2f74a4":"code","e86d4f83":"code","fccb5324":"code","08be43f2":"code","93f8789e":"code","ebaf3bda":"code","0aa0f1df":"code","56b4f250":"code","6df9a4fb":"code","ce1bde3a":"code","22853055":"code","2171e4e4":"code","138b7883":"code","80d21adb":"code","e65bde69":"code","296dc9a0":"code","ea270a61":"code","ac7a997d":"code","235eaf44":"code","292e5fff":"code","4fa3e8f6":"code","6d0fa627":"code","4b3b660a":"code","ae088075":"code","88a5fb3e":"markdown","0eb81ddb":"markdown","99da2548":"markdown","9063e444":"markdown","ebd12314":"markdown","2d516342":"markdown","26c61e4d":"markdown","6ff34454":"markdown","a558421c":"markdown","774bc857":"markdown","04d76fbb":"markdown","f7b05cc2":"markdown"},"source":{"a2b938fd":"# imports\r\nfrom sklearn.datasets import load_digits\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn.decomposition import PCA\r\nimport pandas as pd\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\r\nfrom statsmodels.tools.tools import add_constant\r\nimport numpy as np\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\nimport math\r\n\r\nimport warnings\r\nwarnings.filterwarnings('ignore')","80e29422":"# load data\r\ndata = load_digits(as_frame=False) # load data and create a dataframe in .frame\r\ndata.DESCR","5fc00cbd":"# data as dataframe\r\n# data_df = data.frame\r\ndata_df = pd.DataFrame(data.data, columns=data.feature_names)\r\ntarget_series = pd.Series(data.target)\r\ndata_df = pd.concat([data_df, pd.DataFrame({'target':data.target})], axis=1)\r\nprint('data shape', data_df.shape)\r\ndata_df.head()","2d2f74a4":"# show Images from data\r\nfig = plt.figure(figsize=(6, 6))  # figure size in inches\r\nfig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\r\n\r\nfor i in range(64):\r\n    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\r\n    ax.imshow(data.images[i], cmap=plt.cm.binary, interpolation='nearest')\r\n    # label the image with the target value\r\n    ax.text(0, 7, str(data.target[i]))","e86d4f83":"def plot_digits(data, expected_target=None, predicted_target=None, observations_to_plot=64, figsize=6):\r\n    '''\r\n    show inages from data by readng pixel columns\r\n    inputs:\r\n    data -> Input data\r\n    expected_target -> labelled target\r\n    predicted_target -> predicted values\r\n    observations_to_plot -> count of observations to plot from expected\/predicted\r\n    '''\r\n    fig = plt.figure(figsize=(figsize, figsize))  # figure size in inches\r\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\r\n\r\n    # plot equal dimentions (a x a)..so for 100 observations_to_plot, we plot 10 x 10\r\n    # for 101 ,we plot 11 x 11\r\n    decimal, integer = math.modf(np.sqrt(observations_to_plot))\r\n    entries = integer if decimal == 0 else (integer+1)\r\n        \r\n    for i in range(observations_to_plot):\r\n        ax = fig.add_subplot(entries, entries, i + 1, xticks=[], yticks=[])\r\n        ax.imshow(data[i].reshape(8,8), cmap=plt.cm.binary, interpolation='nearest')\r\n        # label the image with the target value\r\n        if (expected_target is not None) and (predicted_target is None):\r\n            ax.text(0, 7, str(expected_target[i]))\r\n        elif (expected_target is not None) and (predicted_target is not None):\r\n            if predicted_target[i] == expected_target[i]:\r\n                ax.text(0, 7, str(predicted_target[i]), color='green')\r\n            else:\r\n                ax.text(0, 7, str(predicted_target[i]), color='red')\r\n\r\nplot_digits(data.data, data.target)","fccb5324":"pca = PCA(n_components=2)\r\nproj = pca.fit_transform(data.data)\r\n# plot thesse 2 PC's\r\nfig ,ax = plt.subplots(1,1, figsize=(15,10))\r\nplt.scatter(proj[:, 0], proj[:,1], c=data.target, edgecolor=None, alpha=0.9, cmap = plt.get_cmap('Set1', 10))\r\nplt.xlabel('PC1')\r\nplt.ylabel('PC2')\r\nplt.colorbar()","08be43f2":"pca_transformed_dim_2 = pca.inverse_transform(proj)\r\nplot_digits(pca_transformed_dim_2, data.target)","93f8789e":"#scree plot to find those optimum components required to plot digits within reasonable acceptance range\r\nsklearn_pca = PCA(n_components=data.data.shape[1]) # for 64 features there can be 64 PC's ...take all 64 PC's\r\nsklearn_pca.fit(data.data)\r\nplt.rcParams[\"figure.figsize\"] = (15,7)\r\n\r\nplt.subplot(1,2,1)\r\nPC_values = np.arange(sklearn_pca.n_components_) + 1\r\nplt.plot(PC_values, sklearn_pca.explained_variance_ratio_, 'ro-', linewidth=2)\r\nplt.title('Scree plot')\r\nplt.xlabel('Number of Principle components')\r\nplt.ylabel('Proportion of variance explained \\n(eigenvalues)')\r\n\r\nplt.subplot(1,2,2)\r\nplt.plot(np.cumsum(sklearn_pca.explained_variance_ratio_))\r\nplt.title('Scree plot')\r\nplt.xlabel('Number of Principle components')\r\nplt.ylabel('Proportion of variance explained \\n(eigenvalues)')\r\n\r\nplt.show()","ebaf3bda":"sklearn_pca = PCA(n_components=10)\r\nproj = sklearn_pca.fit_transform(data.data)\r\npca_transformed_dim_10 = sklearn_pca.inverse_transform(proj)\r\nplot_digits(pca_transformed_dim_10, data.target)","0aa0f1df":"sklearn_pca = PCA(n_components=15)\r\nproj = sklearn_pca.fit_transform(data.data)\r\npca_transformed_dim_10 = sklearn_pca.inverse_transform(proj)\r\nplot_digits(pca_transformed_dim_10, data.target)","56b4f250":"sklearn_pca = PCA(n_components=20)\r\nproj = sklearn_pca.fit_transform(data.data)\r\npca_transformed_dim_10 = sklearn_pca.inverse_transform(proj)\r\nplot_digits(pca_transformed_dim_10, data.target)","6df9a4fb":"# feature\r\nx = data_df.iloc[:,0:-1]\r\nprint('x.shape', x.shape)\r\nx.head()","ce1bde3a":"# target\r\ny = data_df.iloc[:, -1]\r\nprint('y.shape:', y.shape)\r\ny.head()","22853055":"#split train test\r\nx_train,x_test, y_train,y_test = train_test_split(x, y, test_size=0.2, random_state=10)\r\nprint(x_train.shape, x_test.shape)","2171e4e4":"# run logistic regression without PCA\r\nlr = LogisticRegression(penalty='l2', solver='liblinear')\r\nlr.fit(x_train, y_train)\r\ny_predict = lr.predict(x_test)\r\nprint(np.round(accuracy_score(y_predict, y_test) *100,2), '%')","138b7883":"# PCA treatment\r\nsklearn_pca = PCA(n_components=2)\r\nsklearn_pca.fit(x_train)\r\nx_train_transformed = sklearn_pca.transform(x_train)\r\nx_test_transformed = sklearn_pca.transform(x_test)\r\nprint(x_train_transformed.shape, x_train.shape)\r\nprint(x_test_transformed.shape, x_test.shape)","80d21adb":"# results of PCA treatment\r\npc_nos= np.arange(0, len(sklearn_pca.explained_variance_ratio_))+1\r\nprint('Total variance explained : ', np.round(sklearn_pca.explained_variance_ratio_.sum()*100, 2), '%')\r\nprint(\"Total no of PC's: \", sklearn_pca.n_components_)\r\ndf_PC = pd.DataFrame(data={'PC{}'.format(i): np.round(val*100, 2) for i,val in zip(pc_nos, sklearn_pca.explained_variance_ratio_)}, index=[0])\r\ndisplay(df_PC)\r\ndf_PC.T.plot(kind='bar', title=\"Principal component's explained variance\", ylabel='Explained Variance', xlabel='Principal components', legend= False)","e65bde69":"# plot thesse 2 PC's\r\nfig ,ax = plt.subplots(1,1, figsize=(15,10))\r\nplt.scatter(x_train_transformed[:, 0], x_train_transformed[:,1], c=y_train, edgecolor=None, alpha=0.9, cmap = plt.get_cmap('Set1', 10))\r\nplt.xlabel('PC1')\r\nplt.ylabel('PC2')\r\nplt.colorbar()","296dc9a0":"# run logistic regression with PCA treatment\r\nlr = LogisticRegression(penalty='l2', solver='liblinear')\r\nlr.fit(x_train_transformed, y_train)\r\ny_predict = lr.predict(x_test_transformed)\r\nprint('Accuracy: ',np.round(accuracy_score(y_predict, y_test) *100,2),'%')\r\n\r\n# plot the test digits comparing the predictions (red color text in bottom left corner indicates we were wrong in predicting those entries)\r\nx_test_detransformed = sklearn_pca.inverse_transform(x_test_transformed)\r\nplot_digits(x_test_detransformed, y_test.to_list(), y_predict, x_test_detransformed.shape[0], 15)\r\n","ea270a61":"# PCA treatment\r\nsklearn_pca = PCA(n_components=0.95)\r\nsklearn_pca.fit(x_train)\r\nx_train_transformed = sklearn_pca.transform(x_train)\r\nx_test_transformed = sklearn_pca.transform(x_test)\r\nprint(x_train_transformed.shape, x_train.shape) # Dimentions reduced to 28 for 95% variance\r\nprint(x_test_transformed.shape, x_test.shape) # Dimentions reduced to 28 for 95% variance","ac7a997d":"# results of PCA treatment\r\npc_nos= np.arange(0, len(sklearn_pca.explained_variance_ratio_))+1\r\nprint('Total variance explained : ', np.round(sklearn_pca.explained_variance_ratio_.sum()*100, 2), '%')\r\nprint(\"Total no of PC's: \", sklearn_pca.n_components_)\r\ndf_PC = pd.DataFrame(data={'PC{}'.format(i): np.round(val*100, 2) for i,val in zip(pc_nos, sklearn_pca.explained_variance_ratio_)}, index=[0])\r\ndisplay(df_PC)\r\ndf_PC.T.plot(kind='bar', title=\"Principal component's explained variance\", ylabel='Explained Variance', xlabel='Principal components', legend= False)","235eaf44":"# run ligistic regression with PCA treatment\r\nlr = LogisticRegression(penalty='l2', solver='liblinear')\r\nlr.fit(x_train_transformed, y_train)\r\ny_predict = lr.predict(x_test_transformed)\r\nprint(np.round(accuracy_score(y_predict, y_test) *100,2),'%')\r\n\r\n# plot the test digits comparing the predictions (red color text in bottom left corner indicates we were wrong in predicting those entries)\r\nx_test_detransformed = sklearn_pca.inverse_transform(x_test_transformed)\r\nplot_digits(x_test_detransformed, y_test.to_list(), y_predict, x_test_detransformed.shape[0], 15)","292e5fff":"# PCA treatment with all features\r\nsklearn_pca = PCA(n_components=x_train.shape[1])\r\nsklearn_pca.fit(x_train)\r\nx_train_transformed = sklearn_pca.transform(x_train)\r\nx_test_transformed = sklearn_pca.transform(x_test)\r\nprint(x_train_transformed.shape, x_train.shape)\r\nprint(x_test_transformed.shape, x_test.shape)","4fa3e8f6":"#scree plot\r\nplt.rcParams[\"figure.figsize\"] = (15,7)\r\n\r\nplt.subplot(1,2,1)\r\nPC_values = np.arange(sklearn_pca.n_components_) + 1\r\nplt.plot(PC_values, sklearn_pca.explained_variance_ratio_, 'ro-', linewidth=2)\r\nplt.title('Scree plot')\r\nplt.xlabel('Number of Principle components')\r\nplt.ylabel('Proportion of variance explained \\n(eigenvalues)')\r\n\r\nplt.subplot(1,2,2)\r\nplt.plot(np.cumsum(sklearn_pca.explained_variance_ratio_))\r\nplt.title('Scree plot')\r\nplt.xlabel('Number of Principle components')\r\nplt.ylabel('Proportion of variance explained \\n(eigenvalues)')\r\n\r\nplt.show()","6d0fa627":"# PCA treatment\r\nsklearn_pca = PCA(n_components=10)\r\nsklearn_pca.fit(x_train)\r\nx_train_transformed = sklearn_pca.transform(x_train)\r\nx_test_transformed = sklearn_pca.transform(x_test)\r\nprint(x_train_transformed.shape, x_train.shape)\r\nprint(x_test_transformed.shape, x_test.shape)","4b3b660a":"# results of PCA treatment\r\npc_nos= np.arange(0, len(sklearn_pca.explained_variance_ratio_))+1\r\nprint('Total variance explained : ', np.round(sklearn_pca.explained_variance_ratio_.sum()*100, 2), '%')\r\nprint(\"Total no of PC's: \", sklearn_pca.n_components_)\r\ndf_PC = pd.DataFrame(data={'PC{}'.format(i): np.round(val*100, 2) for i,val in zip(pc_nos, sklearn_pca.explained_variance_ratio_)}, index=[0])\r\ndisplay(df_PC)\r\ndf_PC.T.plot(kind='bar', title=\"Principal component's explained variance\", ylabel='Explained Variance', xlabel='Principal components', legend= False)","ae088075":"# run ligistic regression with PCA treatment\r\nlr = LogisticRegression(penalty='l2', solver='liblinear')\r\nlr.fit(x_train_transformed, y_train)\r\ny_predict = lr.predict(x_test_transformed)\r\nprint(np.round(accuracy_score(y_predict, y_test) *100,2),'%')\r\n\r\n# plot the test digits comparing the predictions (red color text in bottom left corner indicates we were wrong in predicting those entries)\r\nx_test_detransformed = sklearn_pca.inverse_transform(x_test_transformed)\r\nplot_digits(x_test_detransformed, y_test.to_list(), y_predict, x_test_detransformed.shape[0], 15)","88a5fb3e":"***\r\n## While still keeping the accuracy 93.33 % (close to 95.33 without PCS treatment), dimentionality of data is reduced from 64 to 10\r\n***","0eb81ddb":"***\n## Above score is almost same without PCA treatment \n## Find the optimum components that explain max variance using scree plot\n***","99da2548":"***\n## PCA Treatment\n## Lets reduce the dimention from 64 (there are 64 features) to just 2 dimentions (accuracy comes around to just ~60%)\n***","9063e444":"***\r\n## Every row (excluding target) represents a digit whereas target specifies what that digit is \r\n## There are 1797 digits in dataset each with already specified target\r\n***","ebd12314":"***\r\n## From above plots its clear max variation is explained by 10 PC's\r\n## Check with both 10 ,15 and  20 PC's\r\n***","2d516342":"***\n## Regression with PCA (there are 64 features so 64 PC)\n## n_components = 0.95 tells to select PC's such that total variance explained by them be greated than 95%\n***","26c61e4d":"***\n## Max variance can be seen till 10 PC's (apprx 75 %), after that the variance is not that much\n## Choosing n_components=10, PCA and regression\n***","6ff34454":"***\r\n## Code build on Python 3.7.11 (64 bit) Windows in VS Code 1.60.2\r\n## If you want to try your hands on VS code for Python, get it done on [this](www.google.com).\r\n***","a558421c":"***\r\n## Visualize Digits dataset\r\n***","774bc857":"***\n## Regression without PCA\n***","04d76fbb":"***\r\n## Lets do regression to see how much of PC's reduction costs us (10 PC's)\r\n***","f7b05cc2":"***\r\n## REduce dimention to just 2 Principle components\r\n## with only 2 dimentions digits are even more washed out ->\r\n***"}}