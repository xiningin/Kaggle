{"cell_type":{"37121912":"code","fbe27a4c":"code","e9da940d":"code","d8b32902":"code","c6912a10":"code","6660a4af":"code","9387a987":"code","104852ef":"code","08e51408":"code","8ebc5424":"code","22c6c3f7":"code","46fe5f60":"code","2ba1f94b":"code","49092ba4":"code","aea10920":"code","55653788":"code","eb325577":"code","6db51901":"code","60b634bb":"code","64cbdccc":"code","489bb192":"markdown","21797510":"markdown","318e84be":"markdown","1de5f650":"markdown","494c766f":"markdown","a5138afc":"markdown","1a591800":"markdown","9748e3bb":"markdown","490a581a":"markdown","c9d85611":"markdown","a50119a7":"markdown","73806126":"markdown","912a982f":"markdown","b0820e65":"markdown","c900579c":"markdown"},"source":{"37121912":"!git clone https:\/\/github.com\/bayartsogt-ya\/mlub-muis-soril.git\n%cd \/kaggle\/working\/mlub-muis-soril","fbe27a4c":"import gc\nimport os\nimport json\nimport time\nimport types\nimport argparse\nimport shutil\nimport subprocess\nimport warnings\nimport collections\nfrom glob import glob\n\nfrom tqdm.auto import tqdm\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom huggingface_hub.hf_api import HfFolder, HfApi\nfrom huggingface_hub.repository import Repository\n\n# local imports\nfrom dataset import CustomDataset, truncate_text\nfrom models import MLUBModel\nfrom train_utils import compute_metrics, get_kfold\nfrom optimizers import create_optimizer_roberta_large\nfrom preprocessing import preprocess_consecutive_item_error, replace_labeling_error\n\nwarnings.filterwarnings(\"ignore\")","e9da940d":"print(\"loading data\")\nBASE_DIR = \"\/kaggle\/input\/muis-challenge\"\ndf_submission = pd.read_csv(f\"{BASE_DIR}\/submission.csv\")\ndf_train = pd.read_csv(f\"{BASE_DIR}\/train.csv\")\ndf_test = pd.read_csv(f\"{BASE_DIR}\/test.csv\")\ndf_synset_meaning = pd.read_csv(f\"{BASE_DIR}\/synset_meaning.csv\")\ndf_synset_meaning[\"word_len\"] = df_synset_meaning.word.apply(len)\nprint(df_train.shape, df_test.shape, df_submission.shape, df_synset_meaning.shape)","d8b32902":"df_train.loc[:, \"text\"] = df_train.loc[:, \"text\"].str.replace(\"r\", \"\")","c6912a10":"df_train = preprocess_consecutive_item_error(df_train)\ndf_test = preprocess_consecutive_item_error(df_test)\ndf_train.shape, df_test.shape","6660a4af":"for a in [\n    [df_train, 101, \"\u0443\u043b\u0430\u0430\u043d,\u043c\u04e9\u043d\u0433\u04e9\", \"\u0443\u043b\u0430\u0430\u043d, \u043c\u04e9\u043d\u0433\u04e9\"],\n    [df_train, 666, \"\u0445\u0443\u0433\u0430\u0446\u0430\u0430\u043d\u044b \u0448\u0438\u0439\u0434\u0432\u044d\u0440\u043b\u044d\u0445 \u04af\u0435#0000000667 \u0448\u0430\u0442\u0430\u043d\u0434\", \"\u0445\u0443\u0433\u0430\u0446\u0430\u0430\u043d\u044b \u0448\u0438\u0439\u0434\u0432\u044d\u0440\u043b\u044d\u0445 \u04af\u0435 \u0448\u0430\u0442\u0430\u043d\u0434#0000000667\"],\n    [df_train, 2878, \"\u04af\u04af\u0434\u043d\u044d\u044d\u0441\u0441\u0443\u043c\", \"\u04af\u04af\u0434\u043d\u044d\u044d\u0441 \u0441\u0443\u043c\"],\n    [df_train, 8547, \"\u0433\u044d\u0440\u0438\u0439\u043d\u0431\u0430\u0440\u0430\u0430\", \"\u0433\u044d\u0440\u0438\u0439\u043d \u0431\u0430\u0440\u0430\u0430\"],\n    [df_train, 8714, \"Cp\", \"\u0441\u0430\u0440\"],\n    [df_train, 8715, \"Cp\", \"\u0441\u0430\u0440\"],\n]:\n    replace_labeling_error(*a)\n    \nfor a in [\n    [df_test, 485, \"\u0445\u0438\u0439\u0445 \u04af\u0435#0000009206 \u0448\u0430\u0442\u0430\u043d\u0434\", \"\u0445\u0438\u0439\u0445 \u04af\u0435 \u0448\u0430\u0442\u0430\u043d\u0434#0000009206\"],\n    [df_test, 1764, \"\u0442\u0430\u0432\u0438\u0440\u0447,\u043d\u04af\u0434#0000010485\", \"\u0442\u0430\u0432\u0438\u0440\u0447, \u043d\u04af\u0434#0000010485\"],\n    [df_test, 2353, \"\u0442\u04e9\u0432\u0431\u0430\u0439\u0440#0000011074\", \"\u0442\u04e9\u0432 \u0431\u0430\u0439\u0440#0000011074\"],\n    [df_test, 2961, \"\u0443\u043b\u0441c\u0442\u04e9\u0440\u0438\u0439\u043d#0000011682\", \"\u0443\u043b\u0441 \u0442\u04e9\u0440\u0438\u0439\u043d#0000011682\"],\n    [df_test, 13964, \"\u0433\u044d\u0440\u0431\u0430\u0440\u0430\u0430\u0433\u0430\u0430#0000022685\", \"\u0433\u044d\u0440 \u0431\u0430\u0440\u0430\u0430\u0433\u0430\u0430#0000022685\"],\n    [df_test, 10690, \"\u0442\u04e9\u0432\u0442\u044d\u0439\u0442\u04e9\u0440\u0438\u0439\u043d#0000019411\", \"\u0442\u04e9\u0432\u0442\u044d\u0439 \u0442\u04e9\u0440\u0438\u0439\u043d#0000019411\"],\n    [df_test, 6635, \"\u043d\u04af\u04af\u0440 \u0446\u0430\u0440\u0430\u0439\u0433#0000015356\", \"\u043d\u04af\u04af\u0440#0000015356 \u0446\u0430\u0440\u0430\u0439\u0433\"],\n]:\n    replace_labeling_error(*a)","9387a987":"from types import SimpleNamespace\nargs = SimpleNamespace(\n    model_name_or_path=\"tugstugi\/bert-large-mongolian-uncased\",\n    base_model_dropout=0.3,\n    linear_dropout=0.3,\n    use_fast_tokenizer = False,\n    roberta_large_optimizer = True,\n    truncate_length = 5,\n    max_len = 35,\n    batch_size = 128,\n    learning_rate = 3e-5,\n    warmup_steps = 100,\n    num_epochs = 5, # Resource limit \u044d\u044d\u0441 \u0448\u0430\u043b\u0442\u0433\u0430\u0430\u043b\u0430\u043d \u0431\u0430\u0433\u0430\u0430\u0440 \u043e\u043d\u043e\u043e\u0445 \u0445\u044d\u0440\u044d\u0433\u0442\u044d\u0439\n    num_folds = 5,\n    seed = 42\n)","104852ef":"output_dir = \"\/kaggle\/working\/mlub-\" + \\\n            f\"{args.model_name_or_path.split('\/')[1].replace('mongolian-','')}-\" + \\\n            f\"tr{args.truncate_length}\" + \\\n            (f\"do{int(args.base_model_dropout*100)}\" if args.base_model_dropout else \"\") + \\\n            f\"ep{args.num_epochs}\"\n\nprint(\"Hyperparameters Table:\")\ndf_args = pd.DataFrame()\ndf_args[\"key\"] = list(vars(args).keys())\ndf_args[\"value\"] = list(vars(args).values())\ndf_args","08e51408":"os.makedirs(output_dir, exist_ok=True)\nwith open(f\"{output_dir}\/training_arguments.json\", \"w\") as writer:\n    json.dump(vars(args), writer, indent=4)\n    print(f\"[success] wrote training args to {output_dir}\")","8ebc5424":"# ----------------------------- TRAINING ARGUMENTS --------------------------------\ntraining_args = TrainingArguments(\n    output_dir=\"\/this_will_be_replaced\", # this will be replaced\n    per_device_train_batch_size=args.batch_size,\n    per_device_eval_batch_size=args.batch_size*4,\n    dataloader_num_workers=2,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_accuracy\",\n    greater_is_better=True,\n    fp16=False,\n    learning_rate=args.learning_rate,\n    warmup_steps=args.warmup_steps,\n    num_train_epochs=args.num_epochs,\n    save_total_limit=1, # because of kaggle storage limit\n    report_to = 'none',\n    log_level=\"warning\"\n)","22c6c3f7":"# ----------------------------- PREPROCESS --------------------------------\ndict_synset_meaning = collections.defaultdict(dict)\nfor row in df_synset_meaning.itertuples():\n    dict_synset_meaning[row.word][row.synset_id] = row.meaning.lower()\nsynset_id2word = {row.synset_id:row.word for row in df_synset_meaning.itertuples()}\nunique_synset = set(df_synset_meaning.word.unique().tolist())\n\nids = sorted(df_synset_meaning.synset_id.unique().tolist())\nindex2id, id2index = {i:id for i, id in enumerate(ids)}, {id:i for i, id in enumerate(ids)}\nnum_labels = len(index2id)","46fe5f60":"df_train[\"synset_index\"] = df_train.synset_id.map(id2index)\ndf_train = get_kfold(df_train, num_folds=args.num_folds, random_state=args.seed)","2ba1f94b":"# truncate text\ndf_train = truncate_text(df_train, effective_len=args.truncate_length)\ndf_test = truncate_text(df_test, effective_len=args.truncate_length)","49092ba4":"# ----------------------------- TOKENIZER --------------------------------\ntokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=args.use_fast_tokenizer)\ntokenizer.save_pretrained(output_dir)\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nprint(f\"TOKENIZERS_PARALLELISM = {os.environ['TOKENIZERS_PARALLELISM']}\")","aea10920":"df_test[\"synset_index\"] = 0\ntest_dataset = CustomDataset(df_test, tokenizer, max_len=args.max_len)","55653788":"print(\"Before:\", df_test.iloc[6].text)\nprint(\"After: \", df_test.iloc[6].text_truncated)","eb325577":"for fold in range(args.num_folds):\n    train = df_train.query(\"fold!=@fold\")\n    valid = df_train.query(\"fold==@fold\")\n\n    train_dataset = CustomDataset(train, tokenizer, max_len=args.max_len)\n    valid_dataset = CustomDataset(valid, tokenizer, max_len=args.max_len)\n\n    training_args.output_dir = f\"\/tmp\/{output_dir}\/fold_{fold}\"\n\n    model = MLUBModel(\n        args.model_name_or_path,\n        num_labels=num_labels,\n        base_model_dropout=args.base_model_dropout,\n        linear_dropout=args.linear_dropout,\n    )\n    optimizer = None\n    if args.roberta_large_optimizer:\n        optimizer = create_optimizer_roberta_large(model=model, learning_rate=args.learning_rate)\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        compute_metrics=compute_metrics,\n        train_dataset=train_dataset,\n        eval_dataset=valid_dataset,\n        optimizers=(optimizer, None)\n    )\n\n    train_output = trainer.train()\n    eval_result = trainer.evaluate(valid_dataset)\n    print(\"fold\", fold, \"=>\", eval_result)\n    \n    torch.save(trainer.model.state_dict(), f\"{output_dir}\/model_{fold}.bin\")\n    shutil.rmtree(f\"\/tmp\/{output_dir}\/fold_{fold}\"); print(f\"Removed \/tmp\/{output_dir}\/fold_{fold}\")","6db51901":"from torch.utils.data import DataLoader\n\n# result writer\nresult_writer = open(f\"{output_dir}\/README.md\", \"a\")\nresult_writer.write(\"|fold|accuracy|\\n\")\nresult_writer.write(\"|-|-|\\n\")\ntest_dataset = CustomDataset(df_test, tokenizer, max_len=args.max_len)\ntest_loader = DataLoader(test_dataset, batch_size=args.batch_size)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"using\", device)\n\nmodel = MLUBModel(\n    args.model_name_or_path,\n    num_labels=num_labels,\n    inference=True\n)\nmodel.to(device)\nmodel.eval()\n\nppredictions = []\noof = np.zeros((df_train.shape[0], num_labels))\nfor fold in range(args.num_folds):\n    model.load_state_dict(torch.load( f\"{output_dir}\/model_{fold}.bin\"))\n\n    valid = df_train.query(\"fold==@fold\")\n    valid_dataset = CustomDataset(valid, tokenizer, max_len=args.max_len)\n    valid_dataloader = DataLoader(valid_dataset, batch_size=args.batch_size)\n    \n    pred = np.zeros((0, num_labels))\n    for batch in tqdm(valid_dataloader, total=len(valid_dataloader)):\n        batch = {k:v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            output = model(**batch)\n            pred = np.concatenate([pred, output.logits.detach().cpu().numpy()])\n    print(\"fold\", fold, accuracy_score(valid.synset_index.values, pred.argmax(1)))\n    result_writer.write(f\"| fold {fold} |  {accuracy_score(valid.synset_index.values, pred.argmax(1))} |\\n\")\n\n    oof[valid.index] = pred\n    pred = np.zeros((0, num_labels))\n    for batch in tqdm(test_loader, total=len(test_loader)):\n        batch = {k:v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            output = model(**batch)\n            pred = np.concatenate([pred, output.logits.detach().cpu().numpy()])\n    print(pred.shape)\n    ppredictions.append(pred)\n\ndf_train[\"pred_synset_index\"] = np.array(oof.argmax(1), dtype=np.int32)\ndf_train[\"pred_synset_id\"] = df_train.pred_synset_index.map(index2id)\ndf_train.to_csv(f\"{output_dir}\/oof.csv\", index=False)\nprint(\"************\")\noof_accuracy = accuracy_score(df_train.synset_index.values, oof.argmax(1))\nprint(\"OOF Acc:\", oof_accuracy)\nresult_writer.write(f\"| OOF Acc |  {oof_accuracy} |\")\nresult_writer.close()\n\n# ------------ save logits to disk ------------\nnp.save(f\"{output_dir}\/oof.npy\", oof)\nnp.save(f\"{output_dir}\/predictions.npy\", np.array(ppredictions))","60b634bb":"df_submission[\"synset_index\"] = np.array(ppredictions).mean(0).argmax(-1)\ndf_submission.loc[:, \"synset_id\"] = df_submission.loc[:,\"synset_index\"].map(index2id)\ndf_submission.drop(\"synset_index\", axis=1).to_csv(f\"{output_dir}\/submission.csv\", index=False)\ndf_submission.head(2)","64cbdccc":"%cd \/kaggle\/working\n%rm -rf mlub-muis-soril\n!pwd\n%ls $output_dir","489bb192":"5-fold [Stratified Cross Validation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html) \u0430\u0448\u0438\u0433\u043b\u0430\u0436 \u04af\u0440 \u0434\u04af\u043d\u0433\u044d\u044d offline \u0434\u04af\u043d\u044d\u0445 \u0431\u043e\u043b\u043d\u043e.\n","21797510":"Multi-class training \u0445\u0438\u0439\u0445 \u0433\u044d\u0436 \u0431\u0430\u0439\u0433\u0430\u0430 \u0442\u0443\u043b label-\u0430\u0430 0-indexed \u0431\u0430\u0439\u0445\u0430\u0430\u0440 \u0442\u043e\u0445\u0438\u0440\u0443\u0443\u043b\u0430\u0445:","318e84be":"\u041e\u0434\u043e\u043e \u0442\u0435\u043a\u0441\u0442 \u0434\u0430\u0442\u0430\u0433\u0430\u0430 tokenize \u0445\u0438\u0439\u0435. \u042d\u043d\u0434 `CustomDataset` class \u043d\u044c \u044d\u043d\u044d\u0445\u04af\u04af \u04af\u0439\u043b\u0434\u0438\u0439\u0433 wrap \u0445\u0438\u0439\u0441\u044d\u043d \u0431\u043e\u043b\u043d\u043e.","1de5f650":"3. \u0426\u04e9\u04e9\u0445\u04e9\u043d \u0437\u0430\u0441\u0430\u0436 \u0447\u0430\u0434\u0430\u0445\u0430\u0430\u0440\u0433\u04af\u0439 edge case-\u04af\u04af\u0434\u0438\u0439\u0433 \u044d\u043d\u0434 \u0434\u04af\u0440\u043c\u044d\u044d\u0440 \u04e9\u04e9\u0440\u0447\u043b\u04e9\u0432.","494c766f":"## \u0421\u0443\u0440\u0433\u0430\u043b\u0442 \u044d\u0445\u043b\u04af\u04af\u043b\u044d\u0445\n- **`[Model]`** \n    - Backbone \u043c\u043e\u0434\u0435\u043b\u0438\u0439\u0433 \u043e\u043b\u043e\u043d \u044f\u043d\u0437\u0430\u0430\u0440 \u0441\u043e\u043b\u044c\u0441\u043e\u043d \u0431\u0430 \u043e\u043b\u043e\u043d \u0442\u0443\u0440\u0448\u0438\u043b\u0442\u044b\u043d \u044d\u0446\u044d\u0441\u0442 BERT-\u0438\u0439\u0433 \u0441\u043e\u043d\u0433\u043e\u0432. [\u0434\u044d\u043b\u0433\u044d\u0440\u044d\u043d\u0433\u04af\u0439](https:\/\/huggingface.co\/tugstugi\/bert-large-mongolian-uncased)\n    - \u0428\u0443\u0443\u0434 pooler_output \u0430\u0448\u0438\u0433\u043b\u0430\u0445\u044b\u043d \u043e\u0440\u043e\u043d\u0434 \u0437\u04e9\u0432\u0445\u04e9\u043d synset word \u0438\u0439\u043d embedding-\u0438\u0439\u0433 \u0446\u0430\u0430\u0448 \u0441\u0443\u0440\u0433\u0430\u0432. [\u0434\u044d\u043b\u0433\u044d\u0440\u044d\u043d\u0433\u04af\u0439](https:\/\/github.com\/bayartsogt-ya\/mlub-muis-soril\/blob\/main\/models.py#L36)\n- **`[Optimizer]`** BERT-Large model \u04af\u04af\u0434 converge \u0445\u0438\u0439\u0445\u0433\u04af\u0439 \u0431\u0430\u0439\u0445 \u04af\u0435\u0434 higher level layer \u04af\u04af\u0434 \u0434\u044d\u044d\u0440\u0445 learning rate \u04af\u04af\u0434\u0438\u0439\u0433 \u0431\u0430\u0433\u0430\u0430\u0440 \u043e\u043d\u043e\u043e\u0445 \u0430\u0440\u0433\u0430 \u0445\u044d\u0440\u044d\u0433\u043b\u044d\u0432. [\u0434\u044d\u043b\u0433\u044d\u0440\u044d\u043d\u0433\u04af\u0439](https:\/\/github.com\/bayartsogt-ya\/mlub-muis-soril\/blob\/main\/optimizers.py#L3)\n    - *BERT-base \u043c\u043e\u0434\u0435\u043b\u044c \u0438\u0439\u043c trick \u0430\u0448\u0438\u0433\u043b\u0430\u0445 \u0448\u0430\u0430\u0440\u0434\u043b\u0430\u0433\u0430\u0433\u04af\u0439 \u0431\u043e\u043b\u043d\u043e.\n- **`[Learning Rate Scheduler]`** HuggingFace Trainer-\u0438\u0439\u043d default-\u0433 (`torch.optim.lr_scheduler.LambdaLR`) \u0448\u0443\u0443\u0434 \u0430\u0448\u0438\u0433\u043b\u0430\u0432. [\u0434\u044d\u043b\u0433\u044d\u0440\u044d\u043d\u0433\u04af\u0439](https:\/\/huggingface.co\/transformers\/main_classes\/trainer.html#id1)","a5138afc":"## \u04e8\u0433\u04e9\u0433\u0434\u04e9\u043b \u0431\u043e\u043b\u043e\u0432\u0441\u0440\u0443\u0443\u043b\u0430\u043b\u0442 (Preprocess)\n\n\u041d\u0430\u0440\u0438\u0439\u043d \u043a\u043e\u0434\u0442\u043e\u0439 \u0434\u0430\u0440\u0430\u0430\u0445 [\u043b\u0438\u043d\u043a\u044d\u044d\u0440](https:\/\/github.com\/bayartsogt-ya\/mlub-muis-soril\/blob\/main\/preprocessing.py) \u0442\u0430\u043d\u0438\u043b\u0446\u0430\u043d\u0430 \u0443\u0443!","1a591800":"## \u0413\u0430\u0440\u0430\u0430\u0441 \u04e9\u0433\u04e9\u0433\u0434\u04e9\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u04af\u04af\u0434 (Hyperparameters)\nbase \u043c\u043e\u0434\u0435\u043b\u0438\u043e\u0440 BERT-\u0438\u0439\u0433 \u0441\u043e\u043d\u0433\u043e\u0436 \u0430\u0448\u0438\u0433\u043b\u0430\u0432. [bert \u043c\u043e\u0434\u0435\u043b\u04af\u04af\u0434](https:\/\/huggingface.co\/tugstugi) [bert-large-mongolian-uncased](https:\/\/huggingface.co\/tugstugi\/bert-large-mongolian-uncased)\n\n\u042d\u043d\u044d\u0445\u04af\u04af \u0445\u044d\u0441\u044d\u0433 `num_epochs=5` \u0433\u044d\u0436 \u0442\u043e\u0445\u0438\u0440\u0443\u0443\u043b\u0436 \u04e9\u0433\u0441\u04e9\u043d \u0431\u0430\u0439\u0433\u0430\u0430 \u0431\u0430 `num_epochs=25` \u04af\u0435\u0434 `+0.97` \u04af\u0440 \u0434\u04af\u043d \u0433\u0430\u0440\u0433\u0430\u0436 \u0430\u0432\u0447 \u0447\u0430\u0434\u043d\u0430. \n\nKaggle notebook-\u0438\u0439\u043d disk limit 73.1GB \u0431\u0430\u0439\u0434\u0433\u0438\u0439\u0433 \u0430\u043d\u0445\u0430\u0430\u0440\u043d\u0430 \u0443\u0443!","9748e3bb":"\u041e\u0440\u043e\u043b\u0442\u044b\u043d \u0434\u0430\u0442\u0430\u0433\u0430\u0430\u0440\u0430\u0430 \u0431\u04af\u0445 \u0442\u0435\u043a\u0442\u0438\u0439\u0433 \u0441\u043e\u043d\u0433\u043e\u0445\u044b\u043d \u043e\u0440\u043e\u043d\u0434 `#`-\u044b\u0433 \u043e\u0440\u043e\u043b\u0446\u0443\u0443\u043b\u0430\u043d \u0442\u0430\u0441\u043b\u0430\u0445 \u0437\u0430\u043c\u0430\u0430\u0440 \u0431\u043e\u0433\u0438\u043d\u043e \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u0431\u0438\u0439 \u0431\u043e\u043b\u0433\u043e\u043d\u043e\n\u0416\u0438\u0448\u044d\u044d \u043d\u044c \u0434\u043e\u043e\u0440 \u04e9\u0433\u04af\u04af\u043b\u0431\u044d\u0440 \u0431\u0430\u0439\u0445\u0430\u0434:\n```\n\u042d\u0440\u0442\u043d\u0438\u0439 \u0445\u04af\u043d\u0438\u0439 \u0441\u044d\u0442\u0433\u044d\u0445\u04af\u0439, \u043e\u0440\u0447\u0438\u043d \u0442\u043e\u0439\u0440\u043d\u044b\u0445\u043e\u043e \u044e\u043c\u044b\u0433 ,\u0431\u0430\u0439\u0433\u0430\u043b\u0438\u0439\u043d \u0442\u04af\u043c\u044d\u043d \u04af\u0437\u044d\u0433\u0434\u043b\u0438\u0439\u0433 \u0445\u044d\u0440\u0445\u044d\u043d \u04af\u0437\u044d\u0436 \u044e\u0443 \u0433\u044d\u0436 \u0442\u0430\u0439\u043b\u0431\u0430\u0440\u043b\u0430\u0436 \u0431\u0430\u0439\u0441\u0430\u043d \u0437\u044d\u0440\u0433\u0438\u0439\u0433 \u0445\u04af\u043d \u0442\u04e9\u0440\u04e9\u043b\u0445\u0442\u043d\u0438\u0439 \u0445\u0430\u043c\u0433\u0438\u0439\u043d \u044d\u0440\u0442 \u0431\u0430\u043b\u0440\u044b\u043d \u0448\u0430\u0448\u0438\u043d \u0431\u043e\u043b\u043e\u0445 \u0431\u04e9\u04e9\u0433\u0438\u0439\u043d \u0448\u0430\u0448\u043d\u044b \u0451\u0441 \u0437\u0430\u043d\u0448\u0438\u043b, \u0448\u04af\u0442\u043b\u044d\u0433 \u0442\u0430\u0445\u0438\u043b\u0433\u0430 \u0437\u044d\u0440\u0433\u0438\u0439\u043d \u0437\u04af\u0439\u043b \u043d\u044c \u043d\u0430\u0434\u0430\u0434 \u0438\u0445 \u0441\u043e\u043d\u0438\u043d \u043e\u043b\u043e\u043d \u044e\u043c\u043d\u044b \u0442\u04af\u043b\u0445\u04af\u04af\u0440 \u043d\u044c \u0431\u043e\u043b\u0436, \u0430\u0440\u0434\u044b\u043d \u0430\u043c\u0430\u043d \u0437\u043e\u0445\u0438\u043e\u043b, \u0431\u0438\u043b\u0438\u0433\u0437\u04af\u0439 \u0437\u044d\u0440\u0433\u0438\u0439\u043d \u043e\u043b\u043e\u043d \u0431\u0430\u0440\u0438\u043c\u0442 \u0431\u0430\u0439\u0434\u043b\u044b\u043d \u0443\u0447\u0440\u044b \u043d\u044c \u0442\u043e\u0434\u043e\u0440\u0445\u043e\u0439 \u0431\u043e\u043b\u0433\u043e\u0436, \u0430\u043c\u0430\u043d#0000006740 \u0437\u043e\u0445\u0438\u043e\u043b \u0443\u0433\u0441\u0430\u0430\u0442\u043d\u044b \u0441\u0443\u0434\u043b\u0430\u043b, \u0430\u0440\u0434\u044b\u043d \u0431\u0438\u043b\u0438\u0433\u0437\u04af\u0439, \u0445\u044d\u043b\u043d\u0438\u0439 \u0448\u0438\u043d\u0436\u043b\u044d\u043b \u0437\u044d\u0440\u0433\u0438\u0439\u0433 \u0442\u0443\u0441\u0433\u0430\u0439\u043b\u0430\u043d \u0445\u044d\u043b\u043d\u0438\u0439 \u0442\u0443\u0445\u0430\u0439 \u0442\u04af\u04af\u043d \u0434\u043e\u0442\u0440\u043e\u043e \u043c\u043e\u043d\u0433\u043e\u043b \u0445\u044d\u043b\u0438\u0439\u0433 \u0433\u043e\u043b \u0431\u043e\u043b\u0433\u043e\u043d , \u043c\u043e\u043d\u0433\u043e\u043b\u0442\u043e\u0439 \u0442\u04e9\u0440\u04e9\u043b \u0442\u04e9\u0441 \u0442\u04af\u0440\u044d\u0433 \u0445\u044d\u043b\u0442\u044d\u043d, \u0445\u0430\u043c\u043d\u0438\u0433\u0430\u043d \u0445\u044d\u043b\u0442\u044d\u043d \u0436\u0438\u0447, \u044d\u043b, \u043c\u043e\u043d\u0433\u043e\u043b, \u0442\u04af\u0440\u044d\u0433, \u0445\u0430\u043c\u043d\u0438\u0433\u0430\u043d \u0433\u0443\u0440\u0432\u0430\u043d \u0445\u044d\u043b \u0431\u0443\u044e\u0443 \u0430\u043b\u0442\u0430\u0439 \u043e\u0432\u043e\u0433 \u0445\u044d\u043b \u0433\u044d\u0434\u0433\u0438\u0439\u043d\u0445\u043d\u0438\u0439 \u0445\u044d\u043b \u0443\u0433\u0441\u0430\u0430\u0442\u043d\u044b \u0441\u0443\u0434\u043b\u0430\u043b, \u0430\u0440\u0434\u044b\u043d \u0431\u0438\u043b\u0438\u0433\u0437\u04af\u0439\u0433, \u0443\u0440\u0430\u043b\u044b\u043d \u043e\u0432\u043e\u0433 \u0445\u044d\u043b\u0442\u044d\u043d \u0433\u044d\u0434\u044d\u0433 \u043c\u0430\u0436\u0430\u0440, \u0444\u0438\u043d, \u044d\u0441\u0442\u0438, \u0431\u043e\u043b\u043e\u043d \u0441\u043b\u0430\u0432, \u0433\u0435\u0440\u043c\u0430\u043d \u0443\u0433\u0441\u0430\u0430\u0442\u043d\u044b \u043c\u04e9\u043d \u044d\u043b \u0437\u04af\u0439\u043b\u0442\u044d\u0439 \u0445\u044d\u043b\u0437\u04af\u043d \u0448\u0438\u043d\u0436\u043b\u044d\u043b\u0438\u0439\u043d \u04af\u04af\u0434\u043d\u044d\u044d\u0441 \u0433\u043e\u043b\u043b\u043e\u0436 \u0441\u0443\u0434\u0430\u043b\u0441\u043d\u0430\u0430\u0440\u0430\u0430 \u0436\u0438\u0447 \u043c\u043e\u043d\u0433\u043e\u043b\u044b\u043d \u0430\u043c\u0430\u043d \u0437\u043e\u0445\u0438\u043e\u043b, \u0442\u0443\u0443\u043b\u044c\u0441, \u0431\u04e9\u04e9\u0433\u0438\u0439\u043d \u0434\u0443\u0443\u0434\u043b\u0430\u0433\u0430, \u0437\u0430\u043d \u04af\u0439\u043b \u0442\u044d\u0440\u0433\u04af\u04af\u0442\u043d\u0438\u0439\u0433 1927 \u043e\u043d\u043e\u043e\u0441 <<\u0417\u0430\u0430\u043d \u0417\u0430\u043b\u0443\u0443\u0434\u0430\u0439\u0433 >> \u0431\u0438\u0447\u0438\u0436 \u044d\u0445\u044d\u043b\u0441\u044d\u043d 1962 \u043e\u043d \u0445\u04af\u0440\u0442\u044d\u043b \u043e\u0440\u043e\u043b\u0434\u0441\u043e\u043e\u0440 \u043c\u043e\u043d\u0433\u043e\u043b\u044b\u043d \u0445\u044d\u043b \u0441\u043e\u0451\u043b\u044b\u043d \u0442\u0443\u0445\u0430\u0439 \u0441\u0443\u0434\u0430\u043b\u0433\u0430\u0430 \u0445\u0438\u0439\u0436, \u0446\u0443\u0433\u043b\u0443\u0443\u043b\u0441\u0430\u043d \u0445\u044d\u0440\u044d\u0433\u043b\u044d\u0433\u0434\u044d\u0445\u04af\u04af\u043d \u043c\u0438\u043d\u044c <<\u0417\u0430\u0430\u043d \u0417\u0430\u043b\u0443\u0443\u0434\u0430\u0439 >> \u0433\u044d\u0434\u044d\u0433 \u0431\u0430\u043b\u0430\u0440 \u044d\u0440\u0442\u043d\u0438\u0439 \u0442\u04af\u04af\u0445\u044d\u043d \u0440\u043e\u043c\u0430\u043d\u044b\u0433 \u0431\u0438\u0447\u0438\u0445\u044d\u0434 \u043d\u044d\u043d \u0438\u0445 \u0442\u0443\u0441 \u0431\u043e\u043b\u0441\u043e\u043d \u0431\u0438\u043b\u044d\u044d\n```\n\u0434\u043e\u043e\u0440\u0445 \u0431\u0430\u0439\u0434\u043b\u0430\u0430\u0440 \u0431\u043e\u0433\u0438\u043d\u043e \u0431\u043e\u043b\u0433\u043e\u043d\u043e\n```\n\u0442\u043e\u0434\u043e\u0440\u0445\u043e\u0439 \u0431\u043e\u043b\u0433\u043e\u0436, \u0430\u043c\u0430\u043d#0000006740 \u0437\u043e\u0445\u0438\u043e\u043b \u0443\u0433\u0441\u0430\u0430\u0442\u043d\u044b\n```","490a581a":"## \u04ae\u0440 \u0434\u04af\u043d\u0433\u044d\u044d \u0438\u043b\u0433\u044d\u044d\u0445 (Submission)","c9d85611":"1. \u0410\u043d\u0433\u043b\u0438 \u04af\u0441\u044d\u0433 \u0445\u043e\u043b\u0438\u043b\u0434\u0441\u043e\u043d \u043d\u044d\u0433 \u0442\u043e\u0445\u0438\u043e\u043b\u0434\u044b\u0433 \u0437\u0430\u0441\u0430\u0445\n```\n\u0448\u043e\u0440\u043e\u043d\u0433\u043e\u043e\u0441 \u0433\u0430\u0440\u043c\u0430\u0433\u0446#0000005434 \u044d\u0440\u04af\u04af\u043br \u0430\u0433\u0430\u0430\u0440 \u04e9\u04e9\u0434\u04e9\u04e9\u0441 \u043d\u044c \u0443\u0433\u0442\u0430\u043d\n\u044d\u0440\u04af\u04af\u043br => \u044d\u0440\u04af\u04af\u043b\n```","a50119a7":"## \u041e\u0440\u0447\u043d\u043e\u043e \u0431\u044d\u043b\u0434\u044d\u0445 (Prepare environment)\n- Repo\n- Libraries\n- Data\n\n### Clone repo","73806126":"## MLUB \u041c\u0423\u0418\u0421 \u0421\u043e\u0440\u0438\u043b - 1-\u0440 \u0431\u0430\u0439\u0440\u043d\u044b \u0448\u0438\u0439\u0434\u044d\u043b (\u0421\u0443\u0440\u0433\u0430\u043b\u0442)\n\n- \u0413\u04af\u0439\u0446\u044d\u0442\u0433\u044d\u0441\u044d\u043d \u042f.\u0411\u0430\u044f\u0440\u0446\u043e\u0433\u0442 [kaggle](https:\/\/www.kaggle.com\/bayartsogtya) - [github](https:\/\/github.com\/bayartsogt-ya) - [twitter](https:\/\/twitter.com\/_tsogoo_) - [linkedin](https:\/\/www.linkedin.com\/in\/bayartsogt-yadamsuren\/)\n- \u042d\u043d\u044d\u0445\u04af\u04af notebook \u043d\u044c \u0437\u04e9\u0432\u0445\u04e9\u043d \u0441\u0443\u0440\u0433\u0430\u043b\u0442\u044b\u043d \u0434\u0430\u0440\u0430\u0430\u043b\u043b\u044b\u0433 \u0445\u0430\u0440\u0443\u0443\u043b\u0441\u0430\u043d \u0431\u043e\u043b\u043d\u043e. \n- \u0425\u0430\u043c\u0433\u0438\u0439\u043d \u0441\u0430\u0439\u043d \u04af\u0440 \u0434\u04af\u043d\u0433 \u0445\u0430\u0440\u0443\u0443\u043b\u0441\u0430\u043d (inference) notebook-\u0438\u0439\u0433 [\u044d\u043d\u044d \u043b\u0438\u043d\u043a\u044d\u044d\u0440](https:\/\/www.kaggle.com\/bayartsogtya\/mlub-muis-soril-best-single-model-private-0-9725?scriptVersionId=74875736) \u043e\u0440\u0436 \u04af\u0437\u043d\u044d \u04af\u04af!\n- \u0422\u044d\u043c\u0446\u044d\u044d\u043d\u0438\u0439 \u0448\u0438\u0439\u0434\u044d\u043b \u0434\u0430\u0440\u0430\u0430\u0445 \u04af\u043d\u0434\u0441\u044d\u043d \u0445\u044d\u0441\u0433\u04af\u04af\u0434\u044d\u044d\u0441 \u0442\u043e\u0433\u0442\u043e\u0445 \u0431\u043e\u043b\u043d\u043e.\n    0. \u041e\u0440\u0447\u043d\u043e\u043e \u0431\u044d\u043b\u0434\u044d\u0445\n    1. \u04e8\u0433\u04e9\u0433\u0434\u04e9\u043b \u0431\u043e\u043b\u043e\u0432\u0441\u0440\u0443\u0443\u043b\u0430\u043b\u0442\n    2. \u0413\u0430\u0440\u0430\u0430\u0441 \u04e9\u0433\u04e9\u0433\u0434\u04e9\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u04af\u04af\u0434\n    3. \u0421\u0443\u0440\u0433\u0430\u043b\u0442 \u044d\u0445\u043b\u04af\u04af\u043b\u044d\u0445\n    4. \u0411\u0430\u0442\u0430\u043b\u0433\u0430\u0430\u0436\u0443\u0443\u043b\u0430\u043b\u0442\n    5. \u04ae\u0440 \u0434\u04af\u043d\u0433\u044d\u044d \u0438\u043b\u0433\u044d\u044d\u0445\n- \u0414\u0430\u0440\u0430\u0430\u0445 \u0430\u0436\u043b\u0443\u0443\u0434 \u0431\u0430\u0439\u0433\u0430\u0430\u0433\u04af\u0439 \u0431\u043e\u043b \u044d\u043d\u044d\u0445\u04af\u04af \u04af\u0440 \u0434\u04af\u043d\u0433 \u0433\u0430\u0440\u0433\u0430\u0445 \u0431\u043e\u043b\u043e\u043c\u0436\u0433\u04af\u0439 \u0431\u0430\u0439\u043b\u0430\u0430. \u0411\u0430\u044f\u0440\u043b\u0430\u043b\u0430\u0430!\n    * https:\/\/www.kaggle.com\/c\/muis-challenge\n    * https:\/\/huggingface.co\/tugstugi\n    * https:\/\/huggingface.co\/tugstugi\/bert-large-mongolian-uncased\n    * https:\/\/huggingface.co\/transformers\/main_classes\/trainer.html\n    * https:\/\/github.com\/bayartsogt-ya\/mlub-muis-soril","912a982f":"## \u0411\u0430\u0442\u0430\u043b\u0433\u0430\u0430\u0436\u0443\u0443\u043b\u0430\u0445 (Validation)","b0820e65":"2. \u0414\u0430\u0440\u0430\u0430\u043b\u0430\u043b\u0441\u0430\u043d label-\u04af\u04af\u0434\u044d\u044d\u0441 \u04af\u04af\u0441\u0441\u044d\u043d \u0430\u043b\u0434\u0430\u0430\u0433 \u0437\u0430\u0441\u0430\u0445 (reverse-engineering)\n\n![Reverse Engineering](https:\/\/raw.githubusercontent.com\/bayartsogt-ya\/mlub-muis-soril\/main\/images\/ss0.png)","c900579c":"### Load Data"}}