{"cell_type":{"b365b0f1":"code","868d5ddf":"code","95a34809":"code","90e9a997":"code","dbf009da":"code","9e8a7670":"code","8801fce2":"code","48f1f4c7":"code","57a3c43f":"code","2ed8e41f":"code","a994c6b3":"code","ad2f540e":"code","bf0d48b6":"code","31536951":"code","928bea96":"code","6290a73c":"code","6260d153":"code","ca384d8b":"code","12350125":"code","d83bce4b":"code","5ffc9d76":"code","82bc8eac":"code","d2415133":"code","662b68ef":"code","525e0e99":"markdown","62d417fa":"markdown","7fa334b6":"markdown","b74d2256":"markdown","53458aaa":"markdown","a62c168d":"markdown","bd9785b8":"markdown","c053cf39":"markdown","19073bec":"markdown","8ef3294a":"markdown","a3b5996a":"markdown","6586406c":"markdown","b00a8f70":"markdown","c616ff44":"markdown","025d6e46":"markdown","a2b2ab9f":"markdown","2cdc4d9a":"markdown","b3ee6cce":"markdown","b629a575":"markdown","39110fb2":"markdown","de2294a0":"markdown","c990cd96":"markdown","4e80b5c6":"markdown","2b018b29":"markdown","8cb059fe":"markdown","543f19e1":"markdown","ef85ee07":"markdown"},"source":{"b365b0f1":"import numpy as np\nimport pandas as pd\nfrom pandas.io.parsers import read_csv\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nfrom imblearn.over_sampling import SMOTE                   # For Oversampling\n#from outliers import smirnov_grubbs as grubbs\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.neural_network import *\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.svm import SVC\n\n\nimport matplotlib.pyplot as plt\n%pylab inline\nimport seaborn as sns","868d5ddf":"dataset = read_csv('..\/input\/caravan-insurance-challenge.csv')\n\n\nvar=16 \n\nprint(dataset.describe())\nprint('Variables selected :  ', list(dataset.columns.values[[3,10,16,25,29,31,33,40,41,42,43,44,47,59,61,68]]))\n\nselected = dataset.columns.values[[3,10,16,25,29,31,33,40,41,42,43,44,47,59,61,68]]\n\nX = (dataset[dataset.columns[[3,10,16,25,29,31,33,40,41,42,43,44,47,59,61,68]]].values)\n\n\n\n# Normalization - Using MinMax Scaler\nmin_max_scaler = preprocessing.MinMaxScaler()\nX = min_max_scaler.fit_transform(X)\n\ny = np.vstack(dataset['CARAVAN'].values)\n\nprint('\\n')\nprint('X and y Input Data:   ', X.shape, y.shape)\n\n\nX_train_original, X_test2, y_train_original, y_test2 = train_test_split(X, y, test_size=0.3,\n                                                                        random_state=42)\n\nprint('Training Set Shape:   ', X_train_original.shape, y_train_original.shape)\n\nX_val, X_test, y_val, y_test = train_test_split(X_test2, y_test2, test_size=0.33,random_state=42)\n# Used Seed in Partitioning so that Test Set remains same for every Run\n\nprint('Validation Set Shape: ', X_val.shape,y_val.shape)\nprint('Test Set Shape:       ', X_test.shape, y_test.shape)","95a34809":"#for i in range(var):\n#    print((grubbs.test(X_train[:,i], alpha=0.025).reshape(-1)).shape)","90e9a997":"doOversampling = True\n\nif doOversampling:\n# Apply regular SMOTE\n    sm = SMOTE(kind='regular')\n    X_train, y_train = sm.fit_sample(X_train_original, y_train_original)\n    print('Training Set Shape after oversampling:   ', X_train.shape, y_train.shape)\n    print(pd.crosstab(y_train,y_train))\nelse:\n    X_train = X_train_original\n    y_train = y_train_original","dbf009da":"# Plot the feature importances of the forest\n'''\nplt.figure(figsize=(6 * 2, 2.4 * int(var\/2+.5)))\nplt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\nplt.title(\"Variable Co-relation with Outcome\",size=20)\nfor i in range(var):\n    plt.subplot(8, 2, i+1)\n    plt.title(selected[i], size=9,color='darkslateblue',fontweight='bold')\n    plt.scatter(range(len(X)),X[:,i], s=40, marker= 'o',c=((y[:,0:1])+20).reshape(-1), alpha=0.5)\n    plt.yticks()\n    plt.xticks()\nplt.show()\n'''","9e8a7670":"doPCA = False\n\nif doPCA:\n    pca = PCA(svd_solver='randomized',n_components=10,random_state=42).fit(X_train)\n\n    X_train = pca.transform(X_train)\n    X_val = pca.transform(X_val)\n    #print(pca.components_)\n    #print(pca.explained_variance_)\n    #print(pca.explained_variance_ratio_) \n    #print(pca.mean_)\n    print(pca.n_components_)\n    print(pca.noise_variance_)\n    plt.figure(1, figsize=(8, 4.5))\n    plt.clf()\n    plt.axes([.2, .2, .7, .7])\n    plt.plot(pca.explained_variance_, linewidth=2)\n    plt.axis('tight')\n    plt.xlabel('n_components')\n    plt.ylabel('explained_variance_')\n    plt.show()\nelse:\n    X_train = X_train\n    X_val = X_val  ","8801fce2":"Final_Run = True          # Will Not Process Test Set if value is False","48f1f4c7":"clf_DT = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=10, \n                                min_samples_split=2, min_samples_leaf=1, \n                                min_weight_fraction_leaf=0.0, max_features=None, \n                                max_leaf_nodes=None, min_impurity_split=1e-07)\nclf_DT.fit(X_train, y_train)\ny_pred_DT = clf_DT.predict(X_val)","57a3c43f":"clf_NB = BernoulliNB()\nclf_NB.fit(X_train, y_train)\ny_pred_NB = clf_NB.predict(X_val)\n#print(clf_NB.predict_proba(X_val))","2ed8e41f":"MLPClassifier(activation='relu', alpha=1e-05,\n       batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,\n       epsilon=1e-08, hidden_layer_sizes=(64), learning_rate='constant',\n       learning_rate_init=0.001, max_iter=2000, momentum=0.9,\n       nesterovs_momentum=True, power_t=0.5, random_state=42, shuffle=True,\n       tol=0.001, validation_fraction=0.1, verbose=True,\n       warm_start=False)\nclf_MLP = MLPClassifier(alpha=1e-05, hidden_layer_sizes=(64))\n\nclf_MLP.fit(X_train, y_train)\ny_pred_MLP = clf_MLP.predict(X_val)","a994c6b3":"#clf_Log = LogisticRegression(solver='sag', max_iter=1000, random_state=42,verbose=2)\nclf_Log = LogisticRegression(solver='liblinear', max_iter=1000, \n                             random_state=42,verbose=2,class_weight='balanced')\n\nclf_Log.fit(X_train, y_train)\ny_pred_Log = clf_Log.predict(X_val)\nprint(clf_Log.coef_)\nprint(clf_Log.intercept_)","ad2f540e":"clf_RF = RandomForestClassifier(n_estimators=500, criterion='gini', max_depth=15,\n                                min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n                                max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, \n                                bootstrap=True, oob_score=False, n_jobs=1, \n                                random_state=42, verbose=1, warm_start=False, class_weight=None)\nclf_RF.fit(X_train, y_train)\ny_pred_RF = clf_RF.predict(X_val)","bf0d48b6":"clf_AdaB = AdaBoostClassifier(n_estimators=100)\nclf_AdaB.fit(X_train, y_train)\ny_pred_AdaB = clf_AdaB.predict(X_val)","31536951":"clf_GB = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\nclf_GB.fit(X_train, y_train)\ny_pred_GB = clf_GB.predict(X_val)","928bea96":"clf_ET = ExtraTreesClassifier(n_estimators=250, random_state=42)\nclf_ET.fit(X_train, y_train)\ny_pred_ET = clf_ET.predict(X_val)","6290a73c":"clf_SVM = SVC(C=10, class_weight='balanced', gamma='auto', kernel='rbf',\n              max_iter=-1, probability=True, random_state=42, verbose=True)\nclf_SVM.fit(X_train, y_train)\ny_pred_SVM = clf_SVM.predict(X_val)","6260d153":"y_val = y_val.reshape(-1)","ca384d8b":"print('       Accuracy of Models       ')\nprint('--------------------------------')\nprint('Decision Tree           '+\"{:.2f}\".format(accuracy_score(y_val, y_pred_DT)*100)+'%')\nprint('Naive Bayes             '+\"{:.2f}\".format(accuracy_score(y_val, y_pred_NB)*100)+'%')\nprint('Neural Network          '+\"{:.2f}\".format(accuracy_score(y_val, y_pred_MLP)*100)+'%')\nprint('Logistic Regression     '+\"{:.2f}\".format(accuracy_score(y_val, y_pred_Log)*100)+'%')\nprint('Random Forest           '+\"{:.2f}\".format(accuracy_score(y_val, y_pred_RF)*100)+'%')\nprint('AdaBoost                '+\"{:.2f}\".format(accuracy_score(y_val, y_pred_AdaB)*100)+'%')\nprint('GradientBoost           '+\"{:.2f}\".format(accuracy_score(y_val, y_pred_GB)*100)+'%')\nprint('Extra Tree              '+\"{:.2f}\".format(accuracy_score(y_val, y_pred_ET)*100)+'%')\nprint('Support Vector Machine  '+\"{:.2f}\".format(accuracy_score(y_val, y_pred_SVM)*100)+'%')","12350125":"print('Decision Tree  ')\ncm_DT = confusion_matrix(y_val,y_pred_DT)\nprint(cm_DT)\nprint('\\n')\n\nprint('Naive Bayes  ')\ncm_NB = confusion_matrix(y_val,y_pred_NB)\nprint(cm_NB)\nprint('\\n')\n\nprint('Neural Network  ')\ncm_MLP = confusion_matrix(y_val,y_pred_MLP)\nprint(cm_MLP)\nprint('\\n')\n\nprint('Logistic Regression  ')\ncm_Log = confusion_matrix(y_val,y_pred_Log)\nprint(cm_Log)\nprint('\\n')\n\nprint('Random Forest  ')\ncm_RF = confusion_matrix(y_val,y_pred_RF)\nprint(cm_RF)\nprint('\\n')\n\nprint('AdaBoost  ')\ncm_AdaB = confusion_matrix(y_val,y_pred_AdaB)\nprint(cm_AdaB)\nprint('\\n')\n\nprint('GradientBoost  ')\ncm_GB = confusion_matrix(y_val,y_pred_GB)\nprint(cm_GB)\nprint('\\n')\n\nprint('Extra Tree  ')\ncm_ET = confusion_matrix(y_val,y_pred_ET)\nprint(cm_ET)\nprint('\\n')\n\nprint('SVM  ')\ncm_SVM = confusion_matrix(y_val,y_pred_SVM)\nprint(cm_SVM)","d83bce4b":"if Final_Run:\n    if doPCA:\n        X_test = pca.transform(X_test)\n        X_train_original = pca.transform(X_train_original)\n    y_test = y_test.reshape(-1)\n    y_train_original = y_train_original.reshape(-1)\n    \n    y_pred_train_DT = clf_DT.predict(X_train_original)\n    y_pred_train_NB = clf_NB.predict(X_train_original)\n    y_pred_train_MLP = clf_MLP.predict(X_train_original)\n    y_pred_train_Log = clf_Log.predict(X_train_original)\n    y_pred_test_DT = clf_DT.predict(X_test)\n    y_pred_test_NB = clf_NB.predict(X_test)\n    y_pred_test_MLP = clf_MLP.predict(X_test)\n    y_pred_test_Log = clf_Log.predict(X_test)\n    cm_DT_train = confusion_matrix(y_train_original,y_pred_train_DT)\n    cm_NB_train = confusion_matrix(y_train_original,y_pred_train_NB)\n    cm_MLP_train = confusion_matrix(y_train_original,y_pred_train_MLP)\n    cm_Log_train = confusion_matrix(y_train_original,y_pred_train_Log)\n    cm_DT_test = confusion_matrix(y_test,y_pred_test_DT)\n    cm_NB_test = confusion_matrix(y_test,y_pred_test_NB)\n    cm_MLP_test = confusion_matrix(y_test,y_pred_test_MLP)\n    cm_Log_test = confusion_matrix(y_test,y_pred_test_Log)\n    \n    print('Decision Tree Classification Matrix  ')\n    print('Training')\n    print(cm_DT_train)\n    print('Validation')\n    print(cm_DT)\n    print('Test')\n    print(cm_DT_test)\n    print('\\n')\n\n    print('Naive Bayes Classification Matrix ')\n    print('Training')\n    print(cm_NB_train)\n    print('Validation')\n    print(cm_NB)\n    print('Test')\n    print(cm_NB_test)\n    print('\\n')\n\n    print('Neural Network Classification Matrix ')\n    print('Training')\n    print(cm_MLP_train)\n    print('Validation')\n    print(cm_MLP)\n    print('Test')\n    print(cm_MLP_test)\n    print('\\n')\n\n    print('Logistic Regression Classification Matrix ')\n    print('Training')\n    print(cm_Log_train)\n    print('Validation')\n    print(cm_Log)\n    print('Test')\n    print(cm_Log_test)\n    print('\\n')","5ffc9d76":"clf = clf_NB","82bc8eac":"importances_RF = clf_RF.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_RF.estimators_],\n             axis=0)\nindices1 = np.argsort(importances_RF[0:var])[::-1]\n\nindices = indices1[0:var]\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(var):\n    print(\"%d. %s (%f)\" % (f + 1, (dataset.columns.values[[3,10,16,25,29,31,33,40,41,42,43,44,47,59,61,68]]).reshape(-1)[indices[f]], importances_RF[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure(figsize=(14, 3))\nplt.title(\"Most Important Features - Random Forest\",size=20)\nplt.bar(range(var), importances_RF[indices],\n       color=\"#aa6d0a\", yerr=std[indices], align=\"center\")\nplt.yticks(size=14)\nplt.xticks(range(var), (dataset.columns.values[[3,10,16,25,29,31,33,40,41,42,43,44,47,59,61,68]]).reshape(-1)[indices],rotation='vertical',size=12,color=\"#201506\")\nplt.xlim([-1, var])\nplt.show()","d2415133":"FN_amount = -9950\nTP_amount = 9950\nTN_amount = 0\nFP_amount = -50\n\nif Final_Run:\n    print('Compare Profit from Models - Test Set')\n    print('-------------------------------------')\n\n    Profit_DT     = (cm_DT_test[0][0]*TN_amount + cm_DT_test[1][0]*FN_amount + cm_DT_test[0][1]*FP_amount +\n                   cm_DT_test[1][1]*TP_amount)\n    print('Decision Tree Profit(Rs):        ' + str(Profit_DT))\n\n    Profit_NB     = (cm_NB_test[0][0]*TN_amount + cm_NB_test[1][0]*FN_amount + cm_NB_test[0][1]*FP_amount + \n                  cm_NB_test[1][1]*TP_amount)\n    print('Naive Bayes Profit(Rs):          ' + str(Profit_NB))\n\n    Profit_MLP    = (cm_MLP_test[0][0]*TN_amount + cm_MLP_test[1][0]*FN_amount + cm_MLP_test[0][1]*FP_amount + \n                  cm_MLP_test[1][1]*TP_amount)\n    print('Neural Network Profit(Rs):       ' + str(Profit_MLP))\n\n    Profit_Log    = (cm_Log_test[0][0]*TN_amount + cm_Log_test[1][0]*FN_amount + cm_Log_test[0][1]*FP_amount + \n                  cm_Log_test[1][1]*TP_amount)\n    print('Logistic Regression Profit(Rs):  ' + str(Profit_Log))\n    \n    \n    y_pred_test = clf.predict(X_test)\n    print('\\n\\nBest Model Accuracy on Test Set: '+\"{:.2f}\".format(accuracy_score(y_test, y_pred_test)*100)+'%')\n    print('\\nConfusion Matrix on Test Set  ')\n    cm = confusion_matrix(y_test,y_pred_test)\n    print(cm)\n    print('\\n')\n    Profit = cm[0][0]*TN_amount + cm[1][0]*FN_amount + cm[0][1]*FP_amount + cm[1][1]*TP_amount\n    print('Profit(Rs) for Test Set:        ' + str(Profit))\n    \n    # Checked Actual Positive and Negative Class in Test Set\n    Max_Profit = 146*TN_amount + 0*FN_amount + 0*FP_amount + 4*TP_amount\n    print('Max_Profit = ' + str(Max_Profit))\n    \n    print('\\n')\n    print('Test Set Profit % w.r.t Maximum Profit: ' + \"{:.2f}\".format(float(Profit)\/Max_Profit*100)+'%')\n    print('\\n')\n    print('Final Model')\n    print('-----------')\n    print(str(clf))","662b68ef":"print(str(clf_DT));print('\\n')\nprint(str(clf_NB));print('\\n')\nprint(str(clf_MLP));print('\\n')\nprint(str(clf_Log));print('\\n')\nprint(str(clf_RF));print('\\n')\nprint(str(clf_AdaB));print('\\n')\nprint(str(clf_GB));print('\\n')\nprint(str(clf_ET));print('\\n')\nprint(str(clf_SVM));print('\\n')","525e0e99":"###### Execute only on Final Run","62d417fa":"###### Oversampling of underrepresented class","7fa334b6":"###### Logistic Regression Classifier","b74d2256":"###### Neural Network Classifier","53458aaa":"###### Feature Reduction thru PCA - Not used in final phase","a62c168d":"###### Flag for Final Run","bd9785b8":"### Pre-Processing","c053cf39":"###### Outlier Detection","19073bec":"### Test Set Results","8ef3294a":"###### Compare Models on Training, Validation and Test Set Results","a3b5996a":"###### Extra Tree Classifier","6586406c":"###### Read and Partition Data","b00a8f70":"###### Random Forest Classifier","c616ff44":"###### Compare Accuracy of Models on Validation Set","025d6e46":"###### Choose Final Model","a2b2ab9f":"###### Scatterplot for Variable Selection","2cdc4d9a":"Choosing Decision Tree Model as Final Model due to Accuracy + Simplicity","b3ee6cce":"###### AdaBoost Classifier","b629a575":"###### Naive Bayes Classifier","39110fb2":"###### Import Libraries","de2294a0":"### Model Performance Comparison","c990cd96":"###### Decision Tree Classifier","4e80b5c6":"### Build Models","2b018b29":"###### Gradient Boost Classifier","8cb059fe":"###### Execute only on Final Run","543f19e1":"###### Print Confusion Matrix for all Models","ef85ee07":"###### SVM Classifier"}}