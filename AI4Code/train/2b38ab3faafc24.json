{"cell_type":{"2962c2b7":"code","3c672cc9":"code","88001a58":"code","fb8adbbd":"code","0a7c7d96":"code","40fb95f1":"code","11c32240":"code","f9480cd0":"code","cfb91645":"code","bb361b02":"code","4e40924c":"code","b77db031":"code","b8a58394":"code","020ee788":"code","3df5eab2":"code","78a3bd77":"code","2ae2d3aa":"code","2795b3ee":"code","ab3a82b5":"code","64f087d4":"code","31cb69b3":"code","19f2846d":"code","6a13d4c9":"code","c0ef4cb8":"code","901d66ca":"code","63467b5a":"code","15c48982":"code","c9afb76e":"code","9867565c":"code","bff65962":"code","b029416a":"code","4df15e11":"markdown","9997c95b":"markdown","e9b2d122":"markdown","c72c0b7f":"markdown","ffa35243":"markdown","83de4543":"markdown","d0088da6":"markdown","05a26b45":"markdown","4d5c5cf1":"markdown","787a72ad":"markdown","06f5d2f8":"markdown","15016291":"markdown","0f0e0300":"markdown","ff91a2c0":"markdown","6e8d468e":"markdown"},"source":{"2962c2b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3c672cc9":"df = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.isna().sum()","88001a58":"# handling missing values\ndf['bmi'].fillna(df['bmi'].mean(), inplace=True)\ndf.isnull().sum()","fb8adbbd":"sns.countplot(x= 'gender', hue='stroke', data=df)\nplt.show()","0a7c7d96":"sns.displot(df['age'], bins=10, kde=True)\nplt.show()","40fb95f1":"sns.set_theme(style=\"darkgrid\")\nsns.countplot(data=df, x=\"ever_married\")\nplt.show()","11c32240":"sns.countplot(data=df, x=\"work_type\")\nplt.show()","f9480cd0":"sns.countplot(data=df, x=\"work_type\", hue='gender')\nplt.show()","cfb91645":"sns.countplot(data=df, x=\"smoking_status\")\nplt.xticks(rotation=90)\nplt.show()","bb361b02":"fig = plt.figure(figsize=(7,7))\nsns.displot(df.bmi, color=\"orange\", label=\"bmi\", kde=True)\nplt.legend()\nplt.show()","4e40924c":"plt.figure(figsize=(12,10))\n\nsns.distplot(df[df.stroke == 0][\"age\"], color=\"green\")\nsns.distplot(df[df.stroke != 0][\"age\"], color=\"red\")\n\nplt.title(\"No strock Vs Stroke By BMI\", fontsize=15)\nplt.xlim([10,100])\nplt.show()","b77db031":"# Import LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# Instantiate LabelEncoder\nle=LabelEncoder()\n\n# Iterate over all the values of each column and extract their dtypes\nfor col in df.columns:\n    # Compare if the dtype is object\n    if df[col].dtypes=='object':\n    # Use LabelEncoder to do the numeric transformation\n        df[col]=le.fit_transform(df[col])","b8a58394":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop(['id', 'stroke'], axis=1)\ny = df.stroke\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","020ee788":"labels = df.stroke.value_counts(sort=True).index\nsizes = df.stroke.value_counts(sort=True)\n\ncolors=[\"lightblue\", \"red\"]\nexplode=(0.05,0)\nplt.figure(figsize=(7,7))\nplt.pie(sizes, \n        explode=explode, \n        labels=labels,\n        colors=colors,\n        autopct=\"%1.1f%%\",\n        shadow=True,\n        startangle=90)\n\nplt.title(\"Stroke Percent\")\nplt.show()","3df5eab2":"from imblearn.over_sampling import SMOTE\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))","78a3bd77":"from sklearn.model_selection import GridSearchCV\n\nknn = KNeighborsClassifier()\n\npara = {'n_neighbors':range(0,10)}\n\ngrid_cv = GridSearchCV(knn, para, cv=5)\n\ngrid_cv.fit(X_train_res, y_train_res)","2ae2d3aa":"import matplotlib.pyplot as plt\n\nbest = {}\nfor i in range(1, 50):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train_res, y_train_res)\n    y_pred = knn.predict(X_test)\n    acc = knn.score(X_test, y_test)\n    #print(i,':',acc)\n    best[i] = round(acc, 3)\n    \nplt.plot(best.keys(), best.values())\nplt.xticks([i for i in range(0, 100, 5)])\nplt.grid(True)\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()","2795b3ee":"best_model = grid_cv.best_estimator_\nbest_model.fit(X_train_res, y_train_res)\ny_pred = best_model.predict(X_test)","ab3a82b5":"best_model.score(X_test, y_test)","64f087d4":"from sklearn.metrics import classification_report, accuracy_score\n\nprint(classification_report(y_test, y_pred))\nprint(accuracy_score(y_pred, y_test))","31cb69b3":"df[df['stroke'] == 1].iloc[10].tolist()\n\nbest_model.predict([[0.0, 81.0, 1.0, 0.0, 1.0, 2.0, 0.0, 80.43, 29.7, 2.0]])","19f2846d":"# Import PCA\nfrom sklearn.decomposition import PCA\n\n# Create PCA instance: model\nmodel = PCA()\n\n# Apply the fit_transform method of model to grains: pca_features\npca_features = model.fit_transform(X_train_res)\n\n# Assign 0th column of pca_features: xs\nxs = pca_features[:,0]\n\n# Assign 1st column of pca_features: ys\nys = pca_features[:,1]\n\n# Scatter plot xs vs ys\nplt.scatter(xs, ys)\nplt.axis('equal')\nplt.show()","6a13d4c9":"# Perform the necessary imports\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport matplotlib.pyplot as plt\n\n# Create scaler: scaler\nscaler = StandardScaler()\n\n# Create a PCA instance: pca\npca = PCA()\n\n# Create pipeline: pipeline\npipeline = make_pipeline(scaler, pca)\n\n# Fit the pipeline to 'samples'\npipeline.fit(X_train_res)\n\n# Plot the explained variances\nfeatures = range(pca.n_components_)\nplt.bar(features, pca.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.xticks(features)\nplt.show()","c0ef4cb8":"dtc = DecisionTreeClassifier()\ndepth = np.arange(1,30)\nleaves = [1,2,4,5,10,20,30,40,80,100]\nparam_grid =[{'max_depth':depth,\n             'min_samples_leaf':leaves}]\ngrid_search = GridSearchCV(estimator = dtc,param_grid = param_grid,\n                           scoring='roc_auc',cv=10)\ngrid_search = grid_search.fit(X_train_res,y_train_res)","901d66ca":"dt = grid_search.best_estimator_\ny_pred = dt.predict(X_train_res)","63467b5a":"grid_search.best_params_","15c48982":"from sklearn.metrics import roc_auc_score\n\ny_pred_proba = dt.predict_proba(X_test)[:,1]\n\n# Compute test_roc_auc\ntest_roc_auc = roc_auc_score(y_test, y_pred_proba)\n\n# Print test_roc_auc\nprint('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))","c9afb76e":"accuracy_score(y_pred, y_train_res)","9867565c":"dt.predict([[0.0, 81.0, 1.0, 0.0, 1.0, 2.0, 0.0, 80.43, 29.7, 2.0]])","bff65962":"# Set seed for reproducibility\nSEED=1\n\n# Instantiate lr\nlr = LogisticRegression(random_state=SEED)\n\n# Instantiate knn\nKNN = KNeighborsClassifier\nknn = KNN(n_neighbors=1)\n\n# Instantiate dt\ndt = DecisionTreeClassifier(max_depth=27, min_samples_leaf=5, random_state=SEED)\n\n# Define the list classifiers\nclassifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]\n\n# Iterate over the pre-defined list of classifiers\nfor clf_name, clf in classifiers:    \n \n    # Fit clf to the training set\n    clf.fit(X_train_res, y_train_res)    \n   \n    # Predict y_pred\n    y_pred = clf.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_pred, y_test) \n   \n    # Evaluate clf's accuracy on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy))","b029416a":"# Import VotingClassifier from sklearn.ensemble\nfrom sklearn.ensemble import VotingClassifier\n\n# Instantiate a VotingClassifier vc\nvc = VotingClassifier(estimators=classifiers)     \n\n# Fit vc to the training set\nvc.fit(X_train_res, y_train_res)   \n\n# Evaluate the test set predictions\ny_pred = vc.predict(X_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_pred, y_test)\nprint('Voting Classifier: {:.3f}'.format(accuracy))","4df15e11":"## VotingClassifier","9997c95b":"## Dimension reduction (if necessary)","e9b2d122":"##### 40s-60s have more chances","c72c0b7f":"##### shocking, peple who never smoked have more chances","ffa35243":"\n## Preprocessing for ML","83de4543":"# DecisionTrees","d0088da6":"## Searching right hyperparameters","05a26b45":"# Classifier","4d5c5cf1":"##### Rather than first, every other feature have similar importance. So I don't think its necessary","787a72ad":"##### As you can see, percentage of people with no stroke is low and this will affect our model","06f5d2f8":"##### Female have more chances of getting a stroke","15016291":"##### Accuracy is 95% and thats impressive","0f0e0300":"##### Private sector is more vernuable, makes sence","ff91a2c0":"## Fitting best model","6e8d468e":"# Comparing Models and Choosing best one"}}