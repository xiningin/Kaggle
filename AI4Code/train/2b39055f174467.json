{"cell_type":{"20875882":"code","2c68bdf6":"code","28023bd3":"code","36ddcb55":"code","ee31aa7b":"code","44c4ce2c":"code","cfa839ec":"code","b6bbdfd5":"code","c35d7184":"code","f0feea6a":"code","046833b3":"code","849bf39b":"code","433767e0":"code","66a4b9f3":"code","8a71b0d0":"code","e386c6de":"code","37d7a455":"code","0f9e11cb":"code","a16f9844":"code","363e73f1":"code","0e84281e":"code","35af05c6":"code","65075372":"code","a5ff89e8":"code","1b3f6c99":"code","81ea4a1b":"code","17bd8446":"code","54f91e29":"code","dbf1ef5d":"code","a93b80cf":"code","57dd18de":"code","f8c9ba31":"code","1f8b584d":"code","062bb0d5":"code","4c0dc285":"code","069bf466":"code","75c2a804":"code","124d6e64":"code","adbd4b23":"code","dab9da80":"code","905e5a8f":"code","f33c2de3":"code","07281643":"code","d6be3c03":"code","dcbb12d5":"markdown","3f50ad9f":"markdown","70bedf7a":"markdown","0de9b105":"markdown","0deaebc5":"markdown","e79088d1":"markdown","08eacc26":"markdown","f5f58b17":"markdown","43a83dca":"markdown","2cf8aef4":"markdown","f9f7026a":"markdown","5b954c46":"markdown","997e6b97":"markdown","65f83b48":"markdown","6dc6e23c":"markdown","d8b8387c":"markdown","530f2b53":"markdown"},"source":{"20875882":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2c68bdf6":"#import required library\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","28023bd3":"df = pd.read_csv('..\/input\/openintro-possum\/possum.csv')\ndf","36ddcb55":"df.info()\n#some null values","ee31aa7b":"#categorical columns\n\ncat = ['sex','Pop','site']\n\nfor col in cat:\n    print(f'In {col}: {df[col].unique()}')","44c4ce2c":"fig,ax=plt.subplots(3, figsize=(10,10))\nax=ax.ravel()\n\nfor index, col in enumerate(cat):\n    sns.boxplot(x='age',y=col,data=df, ax=ax[index])","cfa839ec":"df['site'] = df['site'].apply(lambda x:str(x))\ndf.info() #change site into string","b6bbdfd5":"fig,ax=plt.subplots(figsize=(15,15))\nsns.heatmap(df.corr(),annot=True)\nplt.show()\n#to a limited degree, body dimensions are to some degree correlated with age","c35d7184":"num = ['hdlngth',\n 'skullw',\n 'totlngth',\n 'taill',\n 'footlgth',\n 'earconch',\n 'eye',\n 'chest',\n 'belly']\n\n#numerical columns EDA\n\nfig,ax=plt.subplots(3,3, figsize=(10,10),constrained_layout=True)\nax=ax.ravel()\n\nfor index, col in enumerate(num):\n    sns.histplot(x=col,data=df,ax=ax[index],\n               kde=True)\n    ax[index].set_title(f'Skewness:{df[col].skew(axis = 0)}')\n    \n#Some regression techniques don't work well skewed data, so we are doing this to detech is that's the case","f0feea6a":"num = ['hdlngth',\n 'skullw',\n 'totlngth',\n 'taill',\n 'footlgth',\n 'earconch',\n 'eye',\n 'chest',\n 'belly']\n\n#numerical columns EDA\n\nfig,ax=plt.subplots(3,3, figsize=(10,10),constrained_layout=True)\nax=ax.ravel()\n\nfor index, col in enumerate(num):\n    log = (f'{col}_log')\n    df[log] = df[col].apply(lambda x:np.log(x+1))\n    sns.histplot(x=f'{col}_log',data=df,ax=ax[index],\n               kde=True)\n    ax[index].set_title(f'Skewness:{df[log].skew(axis = 0)}')\n    \n","046833b3":"#target variable\nsns.histplot(x='age',data=df,kde=True)\nplt.title(f'Skewness:{df.age.skew(axis = 0)}')\nplt.show()","849bf39b":"#log transform\ndf['age_log'] = df['age'].apply(lambda x:np.log(x+1))\nsns.histplot(x='age_log',data=df,kde=True)\nplt.title(f'Skewness:{df.age_log.skew(axis = 0)}')\nplt.show()","433767e0":"df.head()","66a4b9f3":"#handling missing date - dropping them since only a few rows are missing\ndf.dropna(axis=0,inplace=True)\ndf.info()","8a71b0d0":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error","e386c6de":"df.columns","37d7a455":"#different column variations\n\nX_log = ['site', 'Pop', 'sex','hdlngth_log','skullw_log', 'totlngth_log', 'taill_log', 'footlgth_log',\n         'earconch_log', 'eye_log', 'chest_log', 'belly_log']\n\nX_regular = ['site', 'Pop', 'sex', 'hdlngth', 'skullw', 'totlngth', 'taill',\n       'footlgth', 'earconch', 'eye', 'chest', 'belly']\n\nX_mix = ['skullw_log', 'taill_log', 'earconch_log', 'eye_log', 'chest_log','totlngth', 'taill',\n       'footlgth','site', 'Pop', 'sex', 'belly']\n\ny_regular = 'age'\ny_log = 'age_log'","0f9e11cb":"#base case\nX = df[X_regular]\ny = df[y_regular]","a16f9844":"X = pd.get_dummies(X)\nX","363e73f1":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, random_state=42)","0e84281e":"lr = LinearRegression()","35af05c6":"lr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nprint(f'RMSE:{np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age:{df.age.std()}')","65075372":"#log\nX = df[X_log].copy()\ny = df[y_log].copy()\n\nX = pd.get_dummies(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, random_state=42)\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n\ny_test = np.exp(y_test)-1\ny_pred = np.exp(y_pred)-1\n\nprint(f'RMSE:{np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age:{df.age.std()}')","a5ff89e8":"#mix\nX = df[X_mix].copy()\ny = df[y_log].copy()\n\nX = pd.get_dummies(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, random_state=42)\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n\ny_test = np.exp(y_test)-1\ny_pred = np.exp(y_pred)-1\n\nprint(f'RMSE:{np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age:{df.age.std()}')","1b3f6c99":"#mix v2\n\nX = df[X_mix].copy()\ny = df[y_regular].copy()\n\nX = pd.get_dummies(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, random_state=42)\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n\nprint(f'RMSE:{np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age:{df.age.std()}')","81ea4a1b":"#recall that distribution of 'footlgth' & 'earconch' are like 2 bell curves joined together, let's see if binning helps with performance","17bd8446":"num #columns of numerical values","54f91e29":"#'footlgth', 'earconch' columns to be binned\nX=df[X_regular].copy()\ny=df[y_regular].copy()","dbf1ef5d":"#base case of 2 bins per column\n\nto_bin = ['footlgth', 'earconch']\n\nfor col in to_bin:\n    bins = np.linspace(X[col].min(),X[col].max(),2)\n    foot_bin = np.digitize(X[col], bins=bins)\n    X[f'{col}_binned'] = foot_bin\n    X[f'{col}_binned'] = X[f'{col}_binned'].apply(lambda x:str(x))\n    \n    ","a93b80cf":"X = pd.get_dummies(X)\nX","57dd18de":"X.drop(to_bin,axis=1,inplace=True) #dropping original columns\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, random_state=42)\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n\nprint(f'RMSE:{np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age:{df.age.std()}')","f8c9ba31":"bin_num = [2,3,4,5,6,7,8,9,10,11,12,13,14,15] #testing number of bins\nto_bin = ['footlgth', 'earconch'] #columns to bin\n\nfor bin_n in bin_num:\n    \n    X=df[X_regular].copy()\n    y=df[y_regular].copy()\n\n    for col in to_bin:\n        \n        bins = np.linspace(X[col].min(),X[col].max(),bin_n)\n        foot_bin = np.digitize(X[col], bins=bins)\n        X[f'{col}_binned'] = foot_bin\n        X[f'{col}_binned'] = X[f'{col}_binned'].apply(lambda x:str(x))\n        \n        \n    X.drop(to_bin,axis=1,inplace=True) #dropping original columns\n    X = pd.get_dummies(X)\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, random_state=42)\n    lr = LinearRegression()\n\n    lr.fit(X_train,y_train)\n    y_pred = lr.predict(X_test)\n\n    print(f'At bin = {bin_n}, RMSE:{np.sqrt(mean_squared_error(y_test, y_pred))}')\n    print(f'Standard Deviation of Age:{df.age.std()}\\n')\n    \n#default RMSE:1.8887116003974755\n#seems at bin = 4, we have the most improvements","1f8b584d":"#adding back\n\nto_bin = ['footlgth', 'earconch']\n\n\nX=df[X_regular].copy()\ny=df[y_regular].copy()\n\n\nfor col in to_bin:\n\n    bins = np.linspace(X[col].min(),X[col].max(),4)\n    foot_bin = np.digitize(X[col], bins=bins)\n    X[f'{col}_binned'] = foot_bin\n    X[f'{col}_binned'] = X[f'{col}_binned'].apply(lambda x:str(x))\n\nX = pd.get_dummies(X)\n\n#don't drop to_bin this time\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                test_size=0.33, random_state=42)\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n\nprint(f'RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age: {df.age.std()}\\n')\n\n#seems to perform worse","062bb0d5":"#product of the bin dummies and the original data \n\nto_bin = ['footlgth', 'earconch']\n\n\nX=df[X_regular].copy()\ny=df[y_regular].copy()\n\n\nfor col in to_bin:\n\n    bins = np.linspace(X[col].min(),X[col].max(),4)\n    foot_bin = np.digitize(X[col], bins=bins)\n    X[f'{col}_binned'] = foot_bin\n    X[f'{col}_binned'] = X[f'{col}_binned'].apply(lambda x:str(x))\n\nX = pd.get_dummies(X)\nX[['footlgth_binned_1', 'footlgth_binned_2', 'footlgth_binned_3',\n    'footlgth_binned_4', 'earconch_binned_1', 'earconch_binned_2',\n    'earconch_binned_3', 'earconch_binned_4']]","4c0dc285":"dummy = [['footlgth_binned_1', 'footlgth_binned_2', 'footlgth_binned_3',\n    'footlgth_binned_4'], ['earconch_binned_1', 'earconch_binned_2',\n    'earconch_binned_3', 'earconch_binned_4']]\n\noriginal = ['footlgth', 'earconch']\n\nfor o in range(0,len(original)):\n    for d in range(0,len(dummy[o])):\n        col_name = f'{original[o]}*{[dummy[o][d]]}'\n        X[col_name] = (X[original[o]]*X[dummy[o][d]])","069bf466":"X.drop(original,axis=1,inplace=True)\nX","75c2a804":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                test_size=0.33, random_state=42)\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n\nprint(f'RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age: {df.age.std()}\\n')\n\n","124d6e64":"#given what we've learned so far, use bin 4 with dropping original data going forward\n\nto_bin = ['footlgth', 'earconch']\n\n\nX=df[X_regular].copy()\ny=df[y_regular].copy()\n\n\nfor col in to_bin:\n\n    bins = np.linspace(X[col].min(),X[col].max(),4)\n    foot_bin = np.digitize(X[col], bins=bins)\n    X[f'{col}_binned'] = foot_bin\n    X[f'{col}_binned'] = X[f'{col}_binned'].apply(lambda x:str(x))\n    \nX.drop(to_bin,axis=1,inplace=True)","adbd4b23":"from sklearn.preprocessing import PolynomialFeatures\n\n#baseline = x^(2)\npoly = PolynomialFeatures(degree=2, include_bias=False)","dab9da80":"X","905e5a8f":"to_transform = ['hdlngth',\n 'skullw',\n 'totlngth',\n 'taill',\n 'eye',\n 'chest',\n 'belly']\n\npoly.fit(X[to_transform])\nX_poly = pd.DataFrame(poly.transform(X[to_transform]),\n                      columns=poly.get_feature_names(X[to_transform].columns))\nX_poly","f33c2de3":"# adding back categorical data from before\n\nadd_back = ['site','Pop','sex','footlgth_binned','earconch_binned']\nX_poly[add_back] = X[add_back]\nX_poly","07281643":"X_poly = pd.get_dummies(X_poly) \n\nX_train, X_test, y_train, y_test = train_test_split(X_poly, y, \n                                                test_size=0.33, random_state=42)\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n\nprint(f'RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age: {df.age.std()}\\n')","d6be3c03":"degrees = [2,3,4,5,6,7,8,9,10]\n\nto_transform = ['hdlngth',\n                'skullw',\n                'totlngth',\n                'taill',\n                'eye',\n                'chest',\n                'belly']\n\nadd_back = ['site','Pop','sex','footlgth_binned','earconch_binned']\n\nX_preprocessed = X.copy() #with the bins included\n\nfor d in degrees:\n    \n    X_preprocessed = X.copy() #with the bins included\n    \n    poly = PolynomialFeatures(degree=d, include_bias=False)\n    \n    poly.fit(X_preprocessed[to_transform])\n    \n    X_poly = pd.DataFrame(poly.transform(X_preprocessed[to_transform]),\n                          columns=poly.get_feature_names(X_preprocessed[to_transform].columns))\n    \n    X_poly = pd.get_dummies(X_poly) \n\n    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, \n                                                    test_size=0.33, random_state=42)\n    lr = LinearRegression()\n\n    lr.fit(X_train,y_train)\n    y_pred = lr.predict(X_test)\n\n    print(f'At degree = {d}, RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}')\n    print(f'Standard Deviation of Age: {df.age.std()}\\n')","dcbb12d5":"## Handling missing data","3f50ad9f":"## Binning conclusion\n\n- Recall base-case default RMSE = 1.8887116003974755\n\n- At bin = 4, we have the most improvements\n\n- Binning is a powerful tool to improve performance of Linear Regression","70bedf7a":"## Polynomial Findings\n\n- In this case, adding features that scale up the original data by degrees do not seem to help with performance.  However, that may not be the case for other dataset","0de9b105":"## Distribution, skewness & log-transform\n\n- Some regression techniques don't work well skewed data, so we are doing this to detech is that's the case.  Therefore, we will inspect each numerical columns and see if log-transform is appropriate\n\n- We can also experiment with log-transforming the outomce (age)","0deaebc5":"## Basic EDA\n\n- See age distribution per values in each categorical column\n- See correlation of different numerical columns with age column","e79088d1":"## Trying binning\n\n- recall that distribution of 'footlgth' & 'earconch' are like 2 bell curves joined together, let's see if binning helps with performance","08eacc26":"## Binning and interactions\n\n- In the above, we dropped the original data when we binned their columns\n\n- However, we can include the original data back in after getting dummies\n\n- Without adding back, we predict a value for each bin.  However, we may also want to capture the slope for each bin\n\n- Besides adding back, we can compute a product of the bin dummies and the original data so we can capture a unique slope for each of the bin","f5f58b17":"## Categorical findings\n\n- female has higher median age than males\n\n- Vic has more variations in age\n\n- site 2 has significantly lower median age than other sites","43a83dca":"## Base-case binning results\n\n- Base case of 2 bins do not provide much difference in performance\n\n- Let's see if more bins can help.","2cf8aef4":"## log vs not-log X\n\n- So far, in this dataset, log does not help at all.  However, that may not be the case in other dataset.  \n\n- Data science, IMO, is all about trial and error.  Considering the computing resources required by Linear Regression, feel free to recommend or try more combinations!","f9f7026a":"## Log-transform findings\n\n- Skewness improves the best for skullw, taill, earconch, eye, chest\n\n- We can inverse this later post-prediction with **np.exp(x) - 1**","5b954c46":"## Numerical findings\n\n- to a limited degree, body dimensions are to some degree correlated with age","997e6b97":"## Base Case results\n\n- RMSE = 1.89\n\n- We will explore how each strategy worsens or improves the model","65f83b48":"## All about Linear Regression\n\nIn the book \"Introduction to Machine Learning\", there are many techniques to improve a basic Linear Regression technique, including the following:\n\n- comparing results with just log, no log and mix of log and no log on Linear Regression\n\n- trying binning and discretization\n\n- trying polynomial features\n\nIn the next section of the notebook, we will try each of the strategy and see how the Linear Regression performs","6dc6e23c":"## Polynomial Features\n\n- we can also use polynomial to expand the features we have.  Let's see if that improves the performance","d8b8387c":"## Interaction Findings\n\n- Just adding back does not improve performance, worsens it in fact\n\n- However, using a product of original data and dummy variables do improve upon baseline model.  However, not to the degree of just binning","530f2b53":"## Conclusion\n\n**In this notebook, we have done:**\n\n- Implementining a basic Linear Regression to predict Age of possums\n\n- Using binning, binning-interaction and polynomials techniques to improve upon a basic Linear Regression technique\n\n- We learned that binning is a powerful technique to boost performance for Linear Regression in this dataset\n\n- Most of the techniques in the notebook have been inspired by the book *\"Introduction to Machine Learning\" by Andreas C. M\u00fcller and Sarah Guido* "}}