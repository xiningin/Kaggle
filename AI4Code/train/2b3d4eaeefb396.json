{"cell_type":{"ec275e37":"code","63b3a4ad":"code","8c18dafd":"code","e976e495":"code","9f7b9227":"code","48fcf770":"code","a058e2a5":"code","f3e1d8c8":"code","9711de8d":"code","a8062f01":"code","a6ac6cba":"code","f189d92b":"code","ef343fda":"code","8bc7b4e4":"code","bac3c17b":"code","39ffdb0b":"code","b4819033":"code","5b4b05c0":"code","0f33a87c":"code","ddf88e79":"code","cf3bb44b":"code","5ddec3ac":"code","eaaa5e02":"code","7146ff5f":"code","f8d9914a":"code","6c50e4af":"code","04f90ca5":"code","5e3cee12":"code","17b99256":"code","536cb2e0":"code","a4a1f3f8":"code","45c2f3a2":"code","68fcb724":"code","4e44bc11":"markdown","7247a93d":"markdown","cc1f4e59":"markdown","3ec83cd1":"markdown","7c6c0260":"markdown","26207bd7":"markdown","a627faae":"markdown","cac8c439":"markdown"},"source":{"ec275e37":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, make_scorer, precision_score, recall_score \nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\nfrom IPython.display import HTML\nimport base64\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\ndef prepocess(data):\n    pass\n","63b3a4ad":"def create_download_link(df, title = \"Download CSV file\", filename = \"submission.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n","8c18dafd":"# Any results you write to the current directory are saved as output.\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.head()","e976e495":"test_df = pd.read_csv('..\/input\/test.csv')","9f7b9227":"train_df.Embarked.describe()","48fcf770":"train_df.Sex.describe()","a058e2a5":"train_df['EstAge'] = train_df.Age.map(lambda x: x if x >= 1.0 else x * 100)\ntest_df['EstAge'] = test_df.Age.map(lambda x: x if x >= 1.0 else x * 100)","f3e1d8c8":"train_df.Age.describe()\ntest_df.Age.describe()","9711de8d":"train_df.EstAge.describe()\ntest_df.EstAge.describe()","a8062f01":"def calc_corr(df):\n    train_corr = df.corr()\n\n    mask = np.zeros_like(train_corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(11, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(train_corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .5})","a6ac6cba":"calc_corr(train_df)","f189d92b":"sex_category_series = pd.get_dummies(train_df.Sex)\ntrain_df['female'] = sex_category_series['female']","ef343fda":"test_sex_category_series = pd.get_dummies(test_df.Sex)\ntest_df['female'] = test_sex_category_series['female']","8bc7b4e4":"embarked_category_series = pd.get_dummies(train_df.Embarked)\ntrain_df[['C', 'Q', 'S']] =embarked_category_series[['C', 'Q', 'S']]","bac3c17b":"test_embarked_category_series = pd.get_dummies(test_df.Embarked)\ntest_df[['C', 'Q', 'S']] = test_embarked_category_series[['C', 'Q', 'S']]","39ffdb0b":"del train_df['Sex']\ndel train_df['Age']\ndel train_df['Embarked']","b4819033":"calc_corr(train_df)","5b4b05c0":"def get_features_from_df(df, target):\n    temp = df.fillna(0)\n    return temp[target], temp[['Survived']]","0f33a87c":"def get_features_from_test_df(df, target):\n    return df[target].fillna(0)","ddf88e79":"features, labels = get_features_from_df(train_df, ['Pclass', 'female', 'EstAge'])\ntest_features = get_features_from_test_df(test_df, ['Pclass', 'female', 'EstAge'])","cf3bb44b":"scaler = StandardScaler()\nfeatures = scaler.fit_transform(features)\ntest_features = scaler.transform(test_features)","5ddec3ac":"X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=42)","eaaa5e02":"# Hyperparameter tuning","7146ff5f":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 5)]\n# Number of features to consider at every split\nmax_features = [\"auto\", \"sqrt\", \"log2\"]\n# Criterion\ncriterions = [\"gini\", \"entropy\"]\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 60, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 3, 5, 10, 15, 30]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 3, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Warm start\nwarm_start = [True, False]\n# Create the random grid\nrandom_grid = {\"n_estimators\": n_estimators,\n               \"max_features\": max_features,\n               \"criterion\": criterions,\n               \"max_depth\": max_depth,\n               \"min_samples_split\": min_samples_split,\n               \"min_samples_leaf\": min_samples_leaf,\n               \"warm_start\": warm_start,\n               \"bootstrap\": bootstrap}","f8d9914a":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nclf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 150, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\ntrained_model = rf_random.fit(X_train, y_train)","6c50e4af":"predictions_train = trained_model.predict(X_train)\npredictions_test = trained_model.predict(X_test)","04f90ca5":"acc_train = accuracy_score(predictions_train, y_train)\nacc_test = accuracy_score(predictions_test, y_test)\n\nprec_train = precision_score(predictions_train, y_train)\nprec_test = precision_score(predictions_test, y_test)\n\nrecall_train = recall_score(predictions_train, y_train)\nrecall_test = recall_score(predictions_test, y_test)","5e3cee12":"print(f\"Train Accuracy: {acc_train}\")\nprint(f\"Test Accuracy: {acc_test}\")","17b99256":"print(f\"Train Precision: {prec_train}\")\nprint(f\"Test Precision: {prec_test}\")","536cb2e0":"print(f\"Train Recall: {recall_train}\")\nprint(f\"Test Recall: {recall_test}\")","a4a1f3f8":"predictions = trained_model.predict(test_features)","45c2f3a2":"submission = pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':predictions})\n\n#Visualize the first 5 rows\nsubmission.head()","68fcb724":"create_download_link(submission)","4e44bc11":"# Normalization","7247a93d":"Eliminate categorical features\n","cc1f4e59":"# Load data","3ec83cd1":"# Hyperparameter tuning","7c6c0260":"# Delete unnecessary features","26207bd7":"# Eliminate fractional data","a627faae":"No significant correlation so do some preparation","cac8c439":"# Check correlations"}}