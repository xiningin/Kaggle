{"cell_type":{"9de47506":"code","31f7715b":"code","4820529a":"code","1ea792c5":"code","ecdb754e":"code","e984141b":"code","0f121a35":"code","79fa87f1":"code","6619dd4c":"code","f045073d":"code","13f97a84":"code","fbbe4437":"code","4eb945e0":"code","0145f437":"code","65cefb5b":"markdown","69dd2381":"markdown","a6f9f15a":"markdown","f53b49fe":"markdown","3daf12cd":"markdown","39920104":"markdown","0eacd744":"markdown","e824b60e":"markdown","75ab6309":"markdown","d6a5cac5":"markdown","dccee121":"markdown","cfffc47a":"markdown","2fc6be49":"markdown","73169ac7":"markdown","b0385fdc":"markdown","9b6d1104":"markdown","29dcf0c9":"markdown","812d7784":"markdown"},"source":{"9de47506":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport tensorflow as tf \nimport tensorflow_datasets as tfds\nimport pathlib\nimport os\nfrom tensorflow.keras.preprocessing import image\nfrom contextlib import suppress","31f7715b":"data_path = pathlib.Path(r\"..\/input\/animals10\/raw-img\")","4820529a":"batch_size = 64\nimg_height = 224\nimg_width = 224\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntraining_data_gen = ImageDataGenerator(rotation_range=40,\n                                       shear_range=0.2,\n                                       zoom_range=0.2,\n                                       horizontal_flip=True,\n                                       vertical_flip=True,\n                                       rescale=1\/255.0,\n                                       validation_split=0.2)\n\n\ntrain_generator = training_data_gen.flow_from_directory(data_path,\n                                                        target_size=(img_height, img_width),\n                                                        batch_size=128,\n                                                        shuffle=True,\n                                                        class_mode='categorical',\n                                                        subset='training') \n\nvalidation_generator = training_data_gen.flow_from_directory(data_path, \n                                                             target_size=(img_height, img_width),\n                                                             batch_size=64,\n                                                             shuffle=True,\n                                                             class_mode='categorical',\n                                                             subset='validation') ","1ea792c5":"model = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\nmodel.add(tf.keras.layers.MaxPooling2D(2))\n\n\nmodel.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(tf.keras.layers.MaxPooling2D(2))\n\n\nmodel.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(tf.keras.layers.MaxPooling2D(2))\n\n\nmodel.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(tf.keras.layers.MaxPooling2D(2))\n\n\n\n\nmodel.add(tf.keras.layers.Flatten())\n\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.4))\n\n\n\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))","ecdb754e":"model.summary()","e984141b":"model.compile(loss='categorical_crossentropy',\n             optimizer='rmsprop',\n             metrics=['acc'])","0f121a35":"history = model.fit(train_generator, steps_per_epoch=163,\n                   validation_data=validation_generator, \n                   validation_steps=81, epochs=10)","79fa87f1":"for_example = r\"..\/input\/animals10\/raw-img\/gatto\/112.jpeg\"\n#take image path and load the image\nimg = image.load_img(for_example, target_size=(224, 224))\n#trasform the image to array\nimg_tensor = image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis=0)\n#scaling the image\nimg_tensor \/= 255.0\n\n# model that takes the input image and return the activations for the\n# conv and pool layers in the trained model \nlayer_outputs = [layer.output for layer in model.layers[:9]]\nactivation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)\n    \nactivation = activation_model.predict(img_tensor)\n\nfirst_layer_act = activation[0]\nplt.imshow(first_layer_act[0, :, :, 5], aspect='auto', cmap='viridis')","6619dd4c":"#examples to show the response map for\ncat_img_path = r\"..\/input\/animals10\/raw-img\/gatto\/150.jpeg\"\ndog_img_path = r\"..\/input\/animals10\/raw-img\/cane\/OIP-_t431jQrfdq6YUVpvifqpQEsEs.jpeg\"\ncheap_img_path = r\"..\/input\/animals10\/raw-img\/pecora\/ea31b40921f51c22d2524518b7444f92e37fe5d404b0144390f8c078a0eabd_640.jpg\"\nspider_img_path = r\"..\/input\/animals10\/raw-img\/ragno\/e831b0092bf1053ed1584d05fb1d4e9fe777ead218ac104497f5c97ca5ecb5b1_640.jpg\"","f045073d":"from tensorflow.keras.preprocessing import image\n\n\ndef visualize (image_path):\n    \n    #take image path and load the image\n    img = image.load_img(image_path, target_size=(224, 224))\n    #trasform the image to array\n    img_tensor = image.img_to_array(img)\n    img_tensor = np.expand_dims(img_tensor, axis=0)\n    #scaling the image\n    img_tensor \/= 255.0\n\n    # model that takes the input image and return the activations for the\n    # conv and pool layers in the trained model \n    layer_outputs = [layer.output for layer in model.layers[:9]]\n    activation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)\n    \n    activation = activation_model.predict(img_tensor)\n    \n    layer_names = []\n\n    for layer in model.layers[:9]:\n        layer_names.append(layer.name)\n\n    images_per_row = 16\n\n    for layer_name, layer_activation in zip(layer_names, activation):\n        n_features = layer_activation.shape[-1]\n\n        size = layer_activation.shape[1]\n\n        n_cols = n_features \/\/ images_per_row\n        display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n        for col in range(n_cols):\n            for row in range(images_per_row):\n                channel_image = layer_activation[0,\n                                                :, :,\n                                                col * images_per_row + row]\n\n                channel_image -= channel_image.mean()\n                channel_image \/= channel_image.std()\n                channel_image *= 64\n                channel_image += 128\n                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n                display_grid[col * size : (col + 1) * size, \n                            row * size : (row + 1) * size] = channel_image\n\n        scale = 1. \/ size\n        plt.figure(figsize=(scale * display_grid.shape[1],\n                            scale * display_grid.shape[0]))\n        #display_grid.astype(np.uint8)\n        #print(display_grid.shape())\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='viridis')    ","13f97a84":"with suppress(Exception):\n    visualize(cat_img_path)","fbbe4437":"with suppress(Exception):\n    visualize(dog_img_path)","4eb945e0":"with suppress(Exception):\n    visualize(cheap_img_path)","0145f437":"with suppress(Exception):\n    visualize(spider_img_path)","65cefb5b":"## 3.3 visualize the activations for an input cheap image ","69dd2381":"**Thank you for reading, I hope you enjoyed and benefited from it.** <br>\n \n**If you have any questions or notes please leave it in the commont section.** <br>\n\n**If you like this notebook please press upvote and thanks again.** <br>","a6f9f15a":"# 3- Visualizing intermediate activations","f53b49fe":"in iorder to extract the feature maps, we will creat a model that take batches of input images and return the \nactivations of all convolutional and pooling layers. \n\nwe will see that each channel in the feature maps will encode a specific detector.","3daf12cd":"# 4- Thank you ","39920104":"Neural networks are black boxes where we can not extract the learning represntations in a form that the human can understand, \nbut it in not true for ConvNets, Where we can for example visualize intermediate convnet outputs, convnet filters, and heatmaps of class activation in an image. ","0eacd744":"The above image is the Sixth  (since we start counting at 0) feature map from the first Conv layer","e824b60e":"## 3.1 visualize the activations for an input image cat","75ab6309":"Visualizing intermediate convnet outputs is the process of displaying the feature maps that output by various convolution and pooling layers in the netweork given an input. ","d6a5cac5":"# 1- ImageDataGenerator","dccee121":"**If you see an error forget about it and you can see the feature maps under the error**","cfffc47a":"# 2- Building and training the model","2fc6be49":"We will start by making our image data generator for the training, validation, test sets then structuring a ConvNet, it's a simple and traditional network, series of Conolutional and pooling layers then at the top we add Dense layers (The classifier)","73169ac7":"## 3.2 visualize the activations for an input dog image ","b0385fdc":"**Note:** blank means that the pattern that encoded by the filter is not found in the this image","9b6d1104":"The network act like **Information distillation pipeline** where irrelevant information is filtered out and the useful information \nis refined. As we can see as we go deeper in the network activations become less visuallyinterpretable. ","29dcf0c9":"**Have you ever asked about the responce map what looks like?**\n\nAfter the training of the Convolutional network, if we pass an image to it we can extract the feature maps that produced\nby the proceesing of this image using network layers. \n\nEach layer contain learnable filters that trained to detect certain featire from the input of the layer. The layer may contain \nany number of filters 32, 64, ... each of them detect certain feature and produce a feature map, so a layer that have \n64 filter will produce 64 feature map. \n\nin this notebook we will see the feature maps produced by each convolutional and pooling layer in a simple network we will train. \n\n\n**The code referance: Deep Learning with Python by Fran\u00e7ois Chollet**","812d7784":"## 3.4 visualize the activations for an input spider image "}}