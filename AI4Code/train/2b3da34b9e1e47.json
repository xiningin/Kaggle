{"cell_type":{"6916132d":"code","5598fcd5":"code","64842cef":"code","0053abc1":"code","d29ef456":"markdown","ca3e4254":"markdown","7c38398a":"markdown","02fd207b":"markdown","74d6d1bf":"markdown","29732ded":"markdown","d4db5dcb":"markdown","6c60e3d3":"markdown","fc770651":"markdown","b30eb077":"markdown","8dc3c10e":"markdown","b68212f5":"markdown","a52618ac":"markdown","80c6451c":"markdown","99e74c0f":"markdown","2a283fb0":"markdown"},"source":{"6916132d":"# Import Dependencies\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets,linear_model,metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the Boston dataset\n\nboston=datasets.load_boston()\n\nX=boston.data\ny=boston.target","5598fcd5":"# splitting X and y into training and testing sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,\n                                                    random_state=1)\n\n# Create linear regression objest\n\nlin_reg=linear_model.LinearRegression()\nlin_reg.fit(X_train,y_train)","64842cef":"# Predict values for X_test data\npredicted = lin_reg.predict(X_test)\n\n# Regression coefficients\nprint('Coefficients are:\\n',lin_reg.coef_)\n\n# Intecept\nprint('\\nIntercept : ',lin_reg.intercept_)\n\n# variance score: 1 means perfect prediction\nprint('Variance score: ',lin_reg.score(X_test, y_test))\n\n# Mean Squared Error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_test, predicted))","0053abc1":"# Original data of X_test\nexpected = y_test\n\n# Plot a graph for expected and predicted values\n\nplt.title('ActualPrice Vs PredictedPrice (BOSTON Housing Dataset)')\nplt.scatter(expected,predicted,c='b',marker='.',s=36)\nplt.plot([0, 50], [0, 50], 'r')\nplt.xlabel('Actual Price(1000$)')\nplt.ylabel('Predicted Price(1000$)')\nplt.show()","d29ef456":"<h1 style=\"font-size:250%; font-family:cursive; color:darkorange;\"><b>LIST OF CONCEPTS:<\/b><\/h1>\n<ul>\n    <li style=\"font-size:180%; font-family:cursive;\">What is Regression Analysis?<\/li>\n    <li style=\"font-size:180%; font-family:cursive;\">Linear \/ Multiple Regression<\/li>\n    <li style=\"font-size:180%; font-family:cursive;\">Implementation using Python<\/li>\n    <li style=\"font-size:180%; font-family:cursive;\">R2 & Adjusted R2<\/li>\n    <li style=\"font-size:180%; font-family:cursive;\">Multi-Colinearity<\/li>\n    <li style=\"font-size:180%; font-family:cursive;\">Regularization<\/li>\n    <li style=\"font-size:180%; font-family:cursive;\">Ridge & Lasso<\/li>\n<\/ul>","ca3e4254":"<center><h1 style=\"font-size:300%; font-family:cursive; background:darkorange; padding:10px; color:white; border-radius: 30px 30px;\">\ud83d\udcc9 REGRESSION \ud83d\udcc8<\/h1><\/center>\n<br>\n<center><h1 style=\"font-size:200%; font-family:cursive;\">\ud83c\udd95: Updated Version<\/h1><\/center>","7c38398a":" <center><h1 style=\"font-size:250%; font-family:cursive; color:darkorange; border:solid; border-radius:10px 10px; padding:13px;\"><b>2. Linear \/ Multiple Regression + Implementation<\/b><\/h1><\/center>","02fd207b":"----","74d6d1bf":"<p style=\"font-size:150%; font-family:cursive;\">Origin of the word: The word multi-collinearity consists of two words:Multi, meaning multiple, and Collinear, meaning being linearly dependent on each other.<\/p>\n\n<p style=\"font-size:150%; font-family:cursive; text-align:center;\">For e.g., Let\u2019s consider this equation  \ud835\udc4e+\ud835\udc4f=1=>\ud835\udc4f=1\u2212\ud835\udc4e <\/p>\n<p style=\"font-size:150%; font-family:cursive;\">It means that \u2018b\u2019 can be represented in terms of \u2018a\u2019 i.e., if the value of \u2018a\u2019 changes, automatically the value of \u2018b\u2019 will also change. This equation denotes a simple linear relationship among two variables.<\/p>\n\n<h1 style=\"font-size:180%; font-family:cursive; color:#ff6666;\">4.1 Why Should We Care About Multi-Collinearity?<\/h1>\n<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">The coefficients in a Linear Regression model represent the extent of change in Y when a certain x (amongst X1,X2,X3\u2026) is changed keeping others constant. But, if x1 and x2 are dependent, then this assumption itself is wrong that we are changing one variable keeping others constant as the dependent variable will also be changed. It means that our model itself becomes a bit flawed.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">We have a redundancy in our model as two variables (or more than two) are trying to convey the same information.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">As the extent of the collinearity increases, there is a chance that we might produce an overfitted model. An overfitted model works well with the test data but its accuracy fluctuates when exposed to other data sets.<\/li>\n<\/ul>\n<h1 style=\"font-size:180%; font-family:cursive; color:#ff6666;\">4.2 Detection<\/h1>\n<ul>\n    <li style=\"font-size:150%; font-family:cursive; color:#ff6666;\">4.2.1 Correlation & Pair Plots<\/li>\n    <li style=\"font-size:150%; font-family:cursive; color:#ff6666;\">4.2.2 Variance Inflation Factor<\/li>\n<\/ul>","29732ded":" <center><h1 style=\"font-size:250%; font-family:cursive; color:darkorange; border:solid; border-radius:10px 10px; padding:13px;\"><b>4. Multi-Collinearity<\/b><\/h1><\/center>","d4db5dcb":"-------------","6c60e3d3":" <center><h1 style=\"font-size:250%; font-family:cursive; color:darkorange; border:solid; border-radius:10px 10px; padding:13px;\"><b>5. Regularization<\/b><\/h1><\/center>","fc770651":" <center><h1 style=\"font-size:250%; font-family:cursive; color:darkorange; border:solid; border-radius:10px 10px; padding:13px;\"><b>1. What is Regression Analysis?<\/b><\/h1><\/center>","b30eb077":"<h1 style=\"font-size:200%; font-family:cursive; color:#ff6666;\"><b>2.1 LINEAR REGRESSION:<\/b><\/h1> \n<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">Linear Regression is one of the most fundamental and widely known Machine Learning Algorithms which people start with. Building blocks of a Linear Regression Model are:<\/li>\n<br>\n<ol>\n    <li style=\"font-size:150%; font-family:cursive;\">Discreet\/continuous independent variables<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">A best-fit regression line<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">Continuous dependent variable<\/li>\n<\/ol>\n<br>\n    <li style=\"font-size:150%; font-family:cursive;\"> A Linear Regression model predicts the dependent variable using a regression line based on the independent variables.<\/li>\n    <br>\n    <center><img src=\"https:\/\/www.mssqltips.com\/tutorialimages\/9062_machine-learning-lifecycle.004.png\"><\/center>\n<\/ul>\n<br>\n<h1 style=\"font-size:180%; font-family:cursive; color:#ff6666;\">Let\u2019s see the underlying assumptions:<\/h1> \n<ol>\n    <li style=\"font-size:150%; font-family:cursive;\">The regression model is linear in terms of coefficients and error term.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">The mean of the residuals is zero.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">The error terms are not correlated with each other, i.e. given an error value; we cannot predict the next error value.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">The independent variables(x) are uncorrelated with the residual term, also termed as <b>exogeneity<\/b>. This, in layman term, generalises that in no way should the error term be predicted given the value of independent variables.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">The error terms have a constant variance, i.e. <b>homoscedasticity<\/b>.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">No Multicollinearity, i.e. no independent variables should be correlated with each other or affect one another. If there is multicollinearity, the precision of prediction by the OLS model decreases.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">The error terms are normally distributed.<\/li>\n<\/ol>\n<br>\n<h1 style=\"font-size:200%; font-family:cursive; color:#ff6666;\"><b>2.2 MULTIPLE LINEAR REGRESSION<\/b><\/h1>\n<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">This is similar to simple linear regression, but there is more than one independent variable. Every value of the indepen dent variable x is associated with a value of the dependent variable y.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">The goal of multiple linear regression (MLR) is to model the linear relationship between the explanatory (independent) variables and response (dependent) variable.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">In essence, multiple regression is the extension of ordinary least-squares (OLS) regression because it involves more than one explanatory variable.<\/li>\n    <center><img src=\"https:\/\/miro.medium.com\/max\/414\/1*Ko7YDmTa_TctiL2Fkm-kGQ.png\"><\/center>\n<\/ul>\n<h1 style=\"font-size:180%; font-family:cursive; color:#ff6666;\">The multiple regression model is based on the following assumptions:<\/h1> \n<ol>\n    <li style=\"font-size:150%; font-family:cursive;\">There is a linear relationship between the dependent variables and the independent variables.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">yi observations are selected independently and randomly from the population.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">Residuals should be normally distributed with a mean of 0 and variance \u03c3.<\/li>\n<\/ol>\n<h1 style=\"font-size:200%; font-family:cursive; color:#ff6666;\"><b>2.3 How Do you Know this is the best fit line?<\/b><\/h1>\n<p style=\"font-size:150%; font-family:cursive;\">The best fit line is obtained by minimizing the residual. Residual is the distance between the actual Y and the predicted Y, as shown below:<\/p>\n<center><h1 style=\"font-size:150%; font-family:cursive; color:#ff6666;\">Residual for a point in the data is the difference between the actual value and the value predicted by our linear regression model.<\/h1><\/center>\n<br>\n<center><img src=\"https:\/\/i0.wp.com\/statisticsbyjim.com\/wp-content\/uploads\/2017\/03\/residuals-300x186.png?resize=300%2C186&is-pending-load=1\"><\/center>\n<h1 style=\"font-size:200%; font-family:cursive; color:#ff6666;\"><b>2.4 How Well Does the Model Fit the data?<\/b><\/h1>\n<p style=\"font-size:150%; font-family:cursive;\">One of the most generic way to evaluate the fit of a linear model is by computing the R-squared value \/ Adjusted R2 value.<\/p>\n\n<h1 style=\"font-size:180%; font-family:cursive; color:#ff6666;\">2.4.1 R-Squared Value<\/h1>\n<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">R-squared statistic or coefficient of determination is a scale invariant statistic that gives the proportion of variation in target variable explained by the linear regression model.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">It always takes on a value between 0 and 1.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">In simple words, it represents how much of our data is being explained by our model.<\/li>\n<\/ul>\n<p style=\"font-size:150%; font-family:cursive;\">For example,  \ud835\udc452  statistic = 0.75, it says that our model fits 75 % of the total data set. Similarly, if it is 0, it means none of the data points is being explained and a value of 1 represents 100% data explanation.<\/p>\n<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">Mathematically  \ud835\udc452  statistic is calculated as :<\/li>\n    <br>\n    <center><img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/07\/R2-decrease.png\"><\/center>\n    <li style=\"font-size:150%; font-family:cursive;\">TSS : Total variation in target variable is the sum of squares of the difference between the actual values and their mean.<\/li>\n    <br>\n    <center><img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/07\/TSSchange.png\"><\/center>\n    <br>\n    <li style=\"font-size:150%; font-family:cursive;\">RSS : Itis the residual(error) term we have been talking about so far.<\/li>\n<\/ul>\n<br>\n<h1 style=\"font-size:180%; font-family:cursive; color:#ff6666;\">2.4.2 Adjusted R-Squared Value<\/h1>\n<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">The Adjusted R-squared takes into account the number of independent variables used for predicting the target variable. In doing so, we can determine whether adding new variables to the model actually increases the model fit.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">In simple words : As we increase the number of independent variables in our equation, the R2 increases as well. But that doesn\u2019t mean that the new independent variables have any correlation with the output variable. In other words, even with the addition of new features in our model, it is not necessary that our model will yield better results but R2 value will increase. To rectify this problem, we use Adjusted R2 value which penalises excessive use of such features which do not correlate with the output data. <\/li>\n<\/ul>\n<p style=\"font-size:150%; font-family:cursive;\">Let\u2019s have a look at the formula for adjusted R-squared to better understand its working.<\/p>\n<center><img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/07\/edit.png\"><\/center>","8dc3c10e":"<div>\n    <p style=\"font-size:150%; font-family:cursive;\">Regression analysis is a fundamental concept in the field of machine learning. It falls under supervised learning wherein the algorithm is trained with both input features and output labels. It helps in establishing a relationship among the variables by estimating how one variable affects the other.<\/p>\n    <h1 style=\"font-size:150%; font-family:cursive; color:#ff6666;\"><b>REGRESSION IN MACHINE LEARNING<\/b><\/h1>\n    <p style=\"font-size:150%; font-family:cursive;\">Regression in machine learning consists of mathematical methods that allow data scientists to predict a continuous outcome (y) based on the value of one or more predictor variables (x). Linear regression is probably the most popular form of regression analysis because of its ease-of-use in predicting and forecasting.<\/p>\n<\/div>","b68212f5":" <center><h1 style=\"font-size:250%; font-family:cursive; color:darkorange; border:solid; border-radius:10px 10px; padding:13px;\"><b>3. Implementation<\/b><\/h1><\/center>","a52618ac":"<center><h1 style=\"font-size:300%; font-family:cursive; color:green;\">To Be Continued...<\/h1><\/center>","80c6451c":"-----","99e74c0f":"--------------","2a283fb0":"----------"}}