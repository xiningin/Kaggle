{"cell_type":{"d0b5c47f":"code","42491fe5":"code","9ecfec09":"code","1952c863":"code","d2102347":"code","7f51d74f":"code","2271fad3":"code","243be924":"code","b9f45619":"code","3a187ac5":"markdown","20ee57c3":"markdown","783b7f93":"markdown","255bb9b6":"markdown","260656ff":"markdown","0475100b":"markdown","ffb916b8":"markdown"},"source":{"d0b5c47f":"import numpy as np\nimport pandas as pd\nimport math\nimport time\nimport pickle\nimport argparse\nimport sklearn.preprocessing\nimport torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import KFold\n\ndebug = False\n\ndef set_seed(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\nset_seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","42491fe5":"def create_features(df):\n    df = df.copy()\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n\n    df['u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n    df['u_in_lag4'] = df['u_in'].shift(4).fillna(0)\n\n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df = pd.get_dummies(df)\n\n    g = df.groupby('breath_id')['u_in']\n    df['ewm_u_in_mean'] = g.ewm(halflife=10).mean()\\\n                           .reset_index(level=0, drop=True)\n    df['ewm_u_in_std'] = g.ewm(halflife=10).std()\\\n                          .reset_index(level=0, drop=True)\n    df['ewm_u_in_corr'] = g.ewm(halflife=10).corr()\\\n                           .reset_index(level=0, drop=True)\n\n    df['rolling_10_mean'] = g.rolling(window=10, min_periods=1).mean()\\\n                             .reset_index(level=0, drop=True)\n    df['rolling_10_max'] = g.rolling(window=10, min_periods=1).max()\\\n                            .reset_index(level=0, drop=True)\n    df['rolling_10_std'] = g.rolling(window=10, min_periods=1).std()\\\n                            .reset_index(level=0, drop=True)\n\n    df['expand_mean'] = g.expanding(2).mean()\\\n                         .reset_index(level=0, drop=True)\n    df['expand_max'] = g.expanding(2).max()\\\n                        .reset_index(level=0, drop=True)\n    df['expand_std'] = g.expanding(2).std()\\\n                        .reset_index(level=0, drop=True)\n    df = df.fillna(0)\n\n    df.drop(['id', 'breath_id'], axis=1, inplace=True)\n    if 'pressure' in df.columns:\n        df.drop('pressure', axis=1, inplace=True)\n\n    return df\n\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, X, y, w):\n        if y is None:\n            y = np.zeros(len(X), dtype=np.float32)\n\n        self.X = X.astype(np.float32)\n        self.y = y.astype(np.float32)\n        self.w = w.astype(np.float32)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, i):\n        return self.X[i], self.y[i], self.w[i]","9ecfec09":"n = 100*1024 if debug else None\n\ndi = '\/kaggle\/input\/ventilator-pressure-prediction\/'\ntrain = pd.read_csv(di + 'train.csv', nrows=n)\ntest = pd.read_csv(di + 'test.csv', nrows=n)\nsubmit = pd.read_csv(di + 'sample_submission.csv', nrows=n)\n\nfeatures = create_features(train)\nrs = sklearn.preprocessing.RobustScaler()\nfeatures = rs.fit_transform(features)  # => np.ndarray\n\nX_all = features.reshape(-1, 80, features.shape[-1])\ny_all = train.pressure.values.reshape(-1, 80)\nw_all = 1 - train.u_out.values.reshape(-1, 80)  # weights for the score, but not used in this notebook\n\ninput_size = X_all.shape[2]\n\nprint(len(X_all))","1952c863":"class Model(nn.Module):\n    def __init__(self, input_size):\n        hidden = [400, 300, 200, 100]\n        super().__init__()\n        self.lstm1 = nn.LSTM(input_size, hidden[0],\n                             batch_first=True, bidirectional=True)\n        self.lstm2 = nn.LSTM(2 * hidden[0], hidden[1],\n                             batch_first=True, bidirectional=True)\n        self.lstm3 = nn.LSTM(2 * hidden[1], hidden[2],\n                             batch_first=True, bidirectional=True)\n        self.lstm4 = nn.LSTM(2 * hidden[2], hidden[3],\n                             batch_first=True, bidirectional=True)\n        self.fc1 = nn.Linear(2 * hidden[3], 50)\n        self.selu = nn.SELU()\n        self.fc2 = nn.Linear(50, 1)\n        self._reinitialize()\n\n    def _reinitialize(self):\n        \"\"\"\n        Tensorflow\/Keras-like initialization\n        \"\"\"\n        for name, p in self.named_parameters():\n            if 'lstm' in name:\n                if 'weight_ih' in name:\n                    nn.init.xavier_uniform_(p.data)\n                elif 'weight_hh' in name:\n                    nn.init.orthogonal_(p.data)\n                elif 'bias_ih' in name:\n                    p.data.fill_(0)\n                    # Set forget-gate bias to 1\n                    n = p.size(0)\n                    p.data[(n \/\/ 4):(n \/\/ 2)].fill_(1)\n                elif 'bias_hh' in name:\n                    p.data.fill_(0)\n            elif 'fc' in name:\n                if 'weight' in name:\n                    nn.init.xavier_uniform_(p.data)\n                elif 'bias' in name:\n                    p.data.fill_(0)\n\n    def forward(self, x):\n        x, _ = self.lstm1(x)\n        x, _ = self.lstm2(x)\n        x, _ = self.lstm3(x)\n        x, _ = self.lstm4(x)\n        x = self.fc1(x)\n        x = self.selu(x)\n        x = self.fc2(x)\n\n        return x","d2102347":"model = Model(input_size)\nfor name, p in model.named_parameters():\n    print('%-32s %s' % (name, tuple(p.shape)))","7f51d74f":"criterion = torch.nn.L1Loss()\n\ndef evaluate(model, loader_val):\n    tb = time.time()\n    was_training = model.training\n    model.eval()\n\n    loss_sum = 0\n    score_sum = 0\n    n_sum = 0\n    y_pred_all = []\n\n    for ibatch, (x, y, w) in enumerate(loader_val):\n        n = y.size(0)\n        x = x.to(device)\n        y = y.to(device)\n        w = w.to(device)\n\n        with torch.no_grad():\n            y_pred = model(x).squeeze()\n\n        loss = criterion(y_pred, y)\n\n        n_sum += n\n        loss_sum += n*loss.item()\n        \n        y_pred_all.append(y_pred.cpu().detach().numpy())\n\n    loss_val = loss_sum \/ n_sum\n\n    model.train(was_training)\n\n    d = {'loss': loss_val,\n         'time': time.time() - tb,\n         'y_pred': np.concatenate(y_pred_all, axis=0)}\n\n    return d","2271fad3":"nfold = 5\nkfold = KFold(n_splits=nfold, shuffle=True, random_state=228)\nepochs = 2 if debug else 300\nlr = 1e-3\nbatch_size = 1024\nmax_grad_norm = 1000\nlog = {}\n\nfor ifold, (idx_train, idx_val) in enumerate(kfold.split(X_all)):\n    print('Fold %d' % ifold)\n    tb = time.time()\n    model = Model(input_size)\n    model.to(device)\n    model.train()\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=10)\n\n    X_train = X_all[idx_train]\n    y_train = y_all[idx_train]\n    w_train = w_all[idx_train]\n    X_val = X_all[idx_val]\n    y_val = y_all[idx_val]\n    w_val = w_all[idx_val]\n\n    dataset_train = Dataset(X_train, y_train, w_train)\n    dataset_val = Dataset(X_val, y_val, w_val)\n    loader_train = torch.utils.data.DataLoader(dataset_train, shuffle=True,\n                         batch_size=batch_size, drop_last=True)\n    loader_val = torch.utils.data.DataLoader(dataset_val, shuffle=False,\n                         batch_size=batch_size, drop_last=False)\n\n    losses_train = []\n    losses_val = []\n    lrs = []\n    time_val = 0\n    best_score = np.inf\n   \n    print('epoch loss_train loss_val lr time')\n    for iepoch in range(epochs):\n        loss_train = 0\n        n_sum = 0\n        \n        for ibatch, (x, y, w) in enumerate(loader_train):\n            n = y.size(0)\n            x = x.to(device)\n            y = y.to(device)\n\n            optimizer.zero_grad()\n\n            y_pred = model(x).squeeze()\n\n            loss = criterion(y_pred, y)\n            loss_train += n*loss.item()\n            n_sum += n\n\n            loss.backward()\n            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\n            optimizer.step()\n\n        val = evaluate(model, loader_val)\n        loss_val = val['loss']\n        time_val += val['time']\n\n        losses_train.append(loss_train \/ n_sum)\n        losses_val.append(val['loss'])\n        lrs.append(optimizer.param_groups[0]['lr'])\n\n        print('%3d %9.6f %9.6f %7.3e %7.1f %6.1f' %\n              (iepoch + 1,\n               losses_train[-1], losses_val[-1], \n               lrs[-1], time.time() - tb, time_val))\n\n        scheduler.step(losses_val[-1])\n\n\n    ofilename = 'model%d.pth' % ifold\n    torch.save(model.state_dict(), ofilename)\n    print(ofilename, 'written')\n\n    log['fold%d' % ifold] = {\n        'loss_train': np.array(losses_train),\n        'loss_val': np.array(losses_val),\n        'learning_rate': np.array(lrs),\n        'y_pred': val['y_pred'],\n        'idx': idx_val\n    }\n    \n    if ifold >= 1: # due to time limit\n        break\n","243be924":"print('Fold loss_train loss_val best loss_val')\nfor ifold in range(2):\n    d = log['fold%d' % ifold]\n    print('%4d %9.6f %9.6f %9.6f' % (ifold, d['loss_train'][-1], d['loss_val'][-1], np.min(d['loss_val'])))","b9f45619":"features = create_features(test)\nfeatures = rs.transform(features)\n\nX_test = features.reshape(-1, 80, features.shape[-1])\ny_test = np.zeros(len(features)).reshape(-1, 80)\nw_test = 1 - test.u_out.values.reshape(-1, 80)\n\ndataset_test = Dataset(X_test, y_test, w_test)\nloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size)\n\ny_pred_folds = np.zeros((len(test), 5), dtype=np.float32)\nfor ifold in range(5):\n    model = Model(input_size)\n    model.to(device)\n    filename = '\/kaggle\/input\/pytorchlstmwithtensorflowlikeinitialization\/' \\\n               'model%d.pth' % ifold\n    model.load_state_dict(torch.load(filename, map_location=device))\n    model.eval()\n    \n    y_preds = []\n    for x, y, _ in loader_test:\n        x = x.to(device)\n        with torch.no_grad():\n            y_pred = model(x).squeeze()\n\n        y_preds.append(y_pred.cpu().numpy())\n    \n    y_preds = np.concatenate(y_preds, axis=0)\n    y_pred_folds[:, ifold] = y_preds.flatten()\n\nsubmit.pressure = np.mean(y_pred_folds, axis=1)\nsubmit.to_csv('submission.csv', index=False)\nprint('submission.csv written')","3a187ac5":"I trained 5 folds locally,\n\n```\nepoch loss_train loss_val\n0.119303  0.184425 \n0.105525  0.184154\n0.109591  0.179805\n0.127961  0.191654\n0.141102  0.202042\n```\n\nThe original TensorFlow scores at the end are,\n\n```\nloss val_loss (epoch 300)\n0.1351 0.1902\n0.1365 0.1897\n0.1292 0.1972\n0.1221 0.1970\n0.1276 0.1976\n```\n\nI am satisfied with the similarity.\n\nNote that the loss here is the overall MAE including the expiratory phase, which is not the evaluation metric.","20ee57c3":"# Predict and submit","783b7f93":"Minor differences from the original, with no reason.\n\n* Learning rate scheduler is ReduceLROnPlateau, which is used in [other notebooks](https:\/\/www.kaggle.com\/tenffe\/finetune-of-tensorflow-bidirectional-lstm);\n* I have not implemented early stopping;\n* random seeds other than kfold are not fixed in the original. Mine is not strictly deterministic either.\n\nThere are more features, loss function, or better aggrigation of nfold predictions, and so on, in public notebooks, but my goal here is to reproduce score as good as TensorFlow using same model and features.","255bb9b6":"# Training","260656ff":"# PyTorch LSTM with TensorFlow-like initialization\n\nMy purpose for this notebook is to reproduce,\n\nDmitry Uarov: https:\/\/www.kaggle.com\/dmitryuarov\/ventilator-pressure-eda-lstm-0-189\/notebook\n\nwith PyTorch (public LB 0.189). This sounds easy, but not always. I first got a significantly worse score ~ 0.3 using the same features and the model. Since I have heard that the weight initializations are different between PyTorch and TensorFlow, I am trying to make them as similar as I can in this notebook.\n\nThe weight initializations are as follows, according to the official documents\n([Keras](https:\/\/keras.io\/api\/layers\/recurrent_layers\/lstm\/), \n[PyTorch](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.LSTM.html)):\n\n| Parameter | TensorFlow\/Keras | PyTorch |\n| ---       | --- | --- |\n| weight_ih | xavier uniform | uniform \u221ahidden_size |\n| weight_hh | orthogonal     | same as above                    |\n| bias      | 1 for forget gate, 0 other wise | same as above    |\n| linear    | xavier uniform | uniform \u221ainput_size |\n\nI wrote `_reinitialize()` in class `Model`, which is the main content of this notebook.\n\nFor me, using Xavier uniform for the fully connected (linear) after the LSTM was most important (which \nlooks least important to me, though). TensorFlow initialization scheme for LSTM helped, too.\n\nRemaining uncertainties:\n\n* One LSTM weight is actually 4 matrices packed in one tensor. Should I initialize 4 matrices separately?\n* Two biases bi and bh in Pytorch LSTM seem redundant. For the forget gate, I only set one of them to 1 because I saw somewhere that Keras have only one bias, but I am not sure.\n\nChange log\n* Version 3: Public score is computed from 5 models (5 folds) trained locally. It was from 2 folds trained in notebook in version 2.","0475100b":"# Model","ffb916b8":"## Features and Dataset\n\nFrom: https:\/\/www.kaggle.com\/dmitryuarov\/ventilator-pressure-eda-lstm-0-189\/notebook"}}