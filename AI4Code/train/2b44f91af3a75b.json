{"cell_type":{"adf180d8":"code","1821e12b":"code","4f988dec":"code","024f8de9":"code","35693b48":"code","0dbfe40d":"code","f7e61002":"code","6a388d1d":"code","7cca3256":"code","531e86fa":"markdown","fd7b6a76":"markdown","d90ab6bf":"markdown","cef0fa45":"markdown"},"source":{"adf180d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1821e12b":"import pandas as pd\nfrom numpy import mean, std\nfrom matplotlib import pyplot as plt\n\n# input_file = os.listdir(\"..\/input\")[0]\n# df = pd.read_csv(input_file)\ndf = pd.read_csv(\"..\/input\/wdbc.csv\")\ndf","4f988dec":"def normalize_by_column(df): \n    for i in list(df):\n        df[i] = list( (df[i].values - mean(df[i].values) ) \/ std(df[i].values) )\n    return df\n\ndf_copy = df.copy()\ndf_for_analyses = df.drop(['id', 'diagnosis_numeric', 'diagnosis'], axis=1)\ndf_for_analyses_normed = normalize_by_column(df_for_analyses)\narray_for_analyses = df_for_analyses_normed.values\ndiagnosis_numeric = list(df['diagnosis_numeric'].values)","024f8de9":"# Run PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(array_for_analyses)\nP = pca.transform(array_for_analyses)\n\ndiagnosis_pos = [val for ind_, val in enumerate(P) if diagnosis_numeric[ind_]==1]\ndiagnosis_neg = [val for ind_, val in enumerate(P) if diagnosis_numeric[ind_]==0]\n\nplt.figure(figsize=(4,3))\nplt.scatter(list(zip(*diagnosis_pos))[0], list(zip(*diagnosis_pos))[1], label=\"malignant\", facecolors='none', color=\"red\", s=3)\nplt.scatter(list(zip(*diagnosis_neg))[0], list(zip(*diagnosis_neg))[1], label=\"benign\", facecolors='none', color=\"green\", s=3)\nplt.xticks(fontsize=8); plt.yticks(fontsize=8)\nplt.legend(); plt.title(\"PC1 vs PC2\"); plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\nplt.tight_layout()","35693b48":"import umap\nreducer = umap.UMAP(random_state=42, n_neighbors=5)\nembedding = reducer.fit_transform(df_for_analyses)\n\ndiagnosis_pos_umap = [val for ind_, val in enumerate(embedding) if diagnosis_numeric[ind_]==1]\ndiagnosis_neg_umap = [val for ind_, val in enumerate(embedding) if diagnosis_numeric[ind_]==0]\n\nplt.figure(figsize=(4,3))\nplt.scatter(list(zip(*diagnosis_pos_umap))[0], list(zip(*diagnosis_pos_umap))[1], label=\"malignant\", facecolors='none', color=\"red\", s=3)\nplt.scatter(list(zip(*diagnosis_neg_umap))[0], list(zip(*diagnosis_neg_umap))[1], label=\"benign\", facecolors='none', color=\"green\", s=3)\nplt.xticks(fontsize=8); plt.yticks(fontsize=8)\nplt.legend(); plt.title(\"UMAP1 vs UMAP2\"); plt.xlabel(\"UMAP1\"); plt.ylabel(\"UMAP2\")\nplt.tight_layout()","0dbfe40d":"# Run T-SNE\n\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, perplexity=5, learning_rate=100, n_iter=10000, n_iter_without_progress=1000)\ntsne_embedding = tsne.fit_transform(array_for_analyses)\n\ndiagnosis_pos_tsne = [val for ind_, val in enumerate(tsne_embedding) if diagnosis_numeric[ind_]==1]\ndiagnosis_neg_tsne = [val for ind_, val in enumerate(tsne_embedding) if diagnosis_numeric[ind_]==0]\n\nplt.figure(figsize=(4,3))\nplt.scatter(list(zip(*diagnosis_pos_tsne))[0], list(zip(*diagnosis_pos_tsne))[1], label=\"malignant\", facecolors='none', color=\"red\", s=3)\nplt.scatter(list(zip(*diagnosis_neg_tsne))[0], list(zip(*diagnosis_neg_tsne))[1], label=\"benign\", facecolors='none', color=\"green\", s=3)\nplt.xticks(fontsize=8); plt.yticks(fontsize=8)\nplt.legend(); plt.title(\"TSNE1 vs TSNE2\"); plt.xlabel(\"TSNE1\"); plt.ylabel(\"TSNE2\")\nplt.tight_layout()","f7e61002":"from sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.metrics import roc_curve, auc\nfrom numpy import mean, median, array\nfrom random import shuffle\n\nn_split=3\n\ndef svm_roc_auc(x_train, x_test, y_train, y_test):\n\tkernel='linear'\n\tclf = svm.SVC(kernel=kernel, C=1, probability=True)\n\t#auc_list=[]\n\t#x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=1.\/n_split, stratify=Y)\n\tfitted_clf = clf.fit(x_train, y_train)\n\ty_score = fitted_clf.predict_proba(x_test)[:, 1]\n\tfpr, tpr, _ = roc_curve(y_test, y_score)\n\tauc_roc = auc(fpr, tpr)\n\treturn fpr, tpr, auc_roc\n\ndef plot_roc(fpr, tpr, auc_roc, title=\"\"):\n\tplt.figure(figsize=(4,3))\n\tplt.plot(fpr, tpr, color='darkorange', lw=2, label='auc = %0.2f)' % auc_roc)\n\tplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--'); plt.legend()\n\tplt.title(title, fontsize=8)\n\tplt.xlabel('FPR', fontsize=8); plt.ylabel('TPR', fontsize=8)\n\tplt.tight_layout()\n\nx_train_P, x_test_P, y_train_P, y_test_P = train_test_split(P, diagnosis_numeric, test_size=1.\/n_split, stratify=diagnosis_numeric)\nfpr_P, tpr_P, auc_roc_P = svm_roc_auc(x_train_P, x_test_P, y_train_P, y_test_P)\n# Randomize labels to get auc. This shows if we can get similar auc with random labeling\ndiagnosis_numeric_random = array(diagnosis_numeric).copy()\nshuffle(diagnosis_numeric_random)\nx_train_P_r, x_test_P_r, y_train_P_r, y_test_P_r = train_test_split(P, diagnosis_numeric_random, test_size=1.\/n_split, stratify=diagnosis_numeric)\nfpr_P_r, tpr_P_r, auc_roc_P_r = svm_roc_auc(x_train_P_r, x_test_P_r, y_train_P_r, y_test_P_r)\n#\nplot_roc(fpr_P, tpr_P, auc_roc_P, title=\"ROC on PCA reduced data\")\nplot_roc(fpr_P_r, tpr_P_r, auc_roc_P_r, title=\"ROC (rand labels) on PCA reduced data\")","6a388d1d":"# Repeat with 2000 random train_tests_splits and find the mean and median auc\n\nn_repeats=2000\nauc_list_P=[]\nfor i in range(n_repeats):\n\tx_train_P, x_test_P, y_train_P, y_test_P = train_test_split(P, diagnosis_numeric, test_size=1.\/n_split, stratify=diagnosis_numeric)\n\tfpr_P, tpr_P, auc_roc_P = svm_roc_auc(x_train_P, x_test_P, y_train_P, y_test_P)\n\tauc_list_P.append(auc_roc_P)\n\n\"%.2f\" % median(auc_list_P)\n'0.99'\n\n# D the same for the randomized label set\nauc_list_P_random=[]\nfor i in range(n_repeats):\n\tx_train_P_r, x_test_P_r, y_train_P_r, y_test_P_r = train_test_split(P, diagnosis_numeric_random, test_size=1.\/n_split, stratify=diagnosis_numeric_random)\n\tfpr_P_r, tpr_P_r, auc_roc_P_r = svm_roc_auc(x_train_P_r, x_test_P_r, y_train_P_r, y_test_P_r)\n\tauc_list_P_random.append(auc_roc_P_r)\n\n\"%.2f\" % median(auc_list_P_random)\n'0.50'\n\nplt.figure(figsize=(4,3)); plt.title(\"auc distribution n=2000\", fontsize=8)\nplt.hist(auc_list_P_random, bins=50, label=\"random labeling\", density=True)\nplt.hist(auc_list_P, bins=50, label=\"labeling from data\", density=True); plt.legend()\nplt.tight_layout()","7cca3256":"df_copy_for_analyses = df_copy.drop(['id', 'diagnosis', 'diagnosis_numeric'], axis=1)\nnumber_of_features_selected = len(list(df_copy_for_analyses))\nnumber_of_rows=10; number_of_columns=3\n\nplt.figure(figsize=(30,50))\nfor i in range(1, number_of_features_selected+1):\n\tplt.subplot(10,3,i)\n\tcolor_by = list(df_copy_for_analyses)[i-1] # Recall we start i with 1\n\t_=plt.scatter(list(zip(*P))[0], list(zip(*P))[1], label=\"malignant\", facecolors='none', c=df_copy[color_by].values, cmap='PuBuGn')\n\t_=plt.xticks(fontsize=4); plt.yticks(fontsize=4)\n\tplt.title(color_by, fontsize=15)\n\tcbar=plt.colorbar()\n\tcbar.ax.tick_params(labelsize=6)\n\tplt.subplots_adjust(hspace=0.9)","531e86fa":"This data is described by several features. Let us run a few dimensionality reduction techniques on this data and see how well it is able to classify it based on input labels","fd7b6a76":"Here we see how each feature relates to the input labels according to PCA embedding. It also makes sense to see that larger tumor radius is correlated with malignant status and smaller radius with benign. Or that the tumor texture bears no correlation to the status. We can do the same with T-SNE and UMAP generated embedding above.","d90ab6bf":"We trained an svm classifer on PCA reduced data and confirmed it's robustness by calculating auc for random combinations of train and test split. We can do the same for T-SNE and UMAP reduced data.\n\n\nNow we can look at individual features and how they are represented on the PCA distribution","cef0fa45":"Dimensionality reduced data does a pretty good job overall in separating the two kinds of data.We can train svm classifier on this reduced data (say the PCA reduced data)"}}