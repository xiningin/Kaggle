{"cell_type":{"fd492e3d":"code","5490dc55":"code","1f1ea769":"code","00574c45":"code","e2d9f219":"code","9206a2e1":"code","c4dc81d8":"code","e079b4c6":"code","9ff13c13":"code","4818c466":"code","228c31d6":"code","d2affaa3":"code","4de10ff0":"code","190d7c7c":"code","101f7121":"code","175d78bf":"code","89df8bcd":"code","2b7c37dc":"code","62c0d78f":"code","84e21d6a":"markdown","557bee1a":"markdown","4e8b5e8f":"markdown","8afd67cd":"markdown","f227b0b5":"markdown","d0ab3ff8":"markdown","6688d005":"markdown","f3521ee7":"markdown","48120849":"markdown","a3f79660":"markdown","67abad5a":"markdown","6cbca37a":"markdown","9765b789":"markdown"},"source":{"fd492e3d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5490dc55":"import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.layers.experimental.preprocessing import RandomFlip\nfrom tensorflow.keras.layers.experimental.preprocessing import RandomRotation\nimport keras_tuner as kt\nfrom keras_tuner import HyperParameters as hp\n","1f1ea769":"dataset_url = '\/kaggle\/input\/arthropod-taxonomy-orders-object-detection-dataset\/ArTaxOr'\nimage_size = (128, 128)\nbatch_size = 32\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    dataset_url,\n    validation_split=0.1,\n    subset=\"training\",\n    seed=1337,\n    image_size=image_size,\n    batch_size=batch_size,\n)\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    dataset_url,\n    validation_split=0.1,\n    subset=\"validation\",\n    seed=1337,\n    image_size=image_size,\n    batch_size=batch_size,\n)\n\nnum_elements = tf.data.experimental.cardinality(val_ds).numpy()\nclass_names = train_ds.class_names\nprint(\"num_elements-> \",num_elements)\nprint(\"class_names-> \",class_names)","00574c45":"#data = tf.keras.preprocessing.image_dataset_from_directory(dataset_url,labels = 'inferred',image_size=(128,128),label_mode='int',class_names = None,shuffle = True,color_mode = 'rgb',seed = 120 )\ndef get_dataset_partitions_tf(data,data_size , val_split=0.5,test_split=0.5, shuffle=False):\n    assert (val_split + test_split) == 1\n    \n    if shuffle:\n        # Specify seed to always have the same split distribution between runs\n        data = data.shuffle(shuffle_size, seed=12)\n    \n    val_size = int(val_split * data_size)\n    test_size = int(test_split * data_size)\n    \n    train_ds = data.take(val_size)\n    test_ds = data.skip(val_size).take(test_size)\n    \n    return val_ds,test_ds\n\n#data = preprocess_image_data(data)\nval_ds, test_ds = get_dataset_partitions_tf(val_ds,num_elements)\nfor image,label in test_ds:\n    print(image.shape)\n    print(label.shape)\n    break","e2d9f219":"plt.figure(figsize=(10, 10))\nfor images, labels in test_ds.take(1):\n    print(\"Image shape: \", images.numpy().shape)\n    print(\"Label: \", labels.numpy().shape)\n    for i in range(0,4):\n        ax = plt.subplot(2, 2, i + 1) \n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","9206a2e1":"data_spec = train_ds.element_spec\ntemp_file_path = '\/kaggle\/output\/data'\ntrain_data_path = os.path.join(temp_file_path,\"train_data\")\nprint(\"Saving the dataset to a temp location: train_ds\")\ntf.data.experimental.save(train_ds,train_data_path)\nval_data_path = os.path.join(temp_file_path,\"val_data\")\nprint(\"Saving the dataset to a temp location: val_ds\")\ntf.data.experimental.save(val_ds,val_data_path)\ntest_data_path = os.path.join(temp_file_path,\"test_data\")\nprint(\"Saving the dataset to a temp location: test_ds\")\ntf.data.experimental.save(test_ds,test_data_path)\n","c4dc81d8":"def data_test(data):\n    for image,label in data:\n        print(\"Image and Label shape in\",data)\n        print(image.shape)\n        print(label.shape)\n        break\n        \ntrain_data_imp = tf.data.experimental.load(train_data_path,data_spec)\ndata_test(train_data_imp)\nval_data_imp = tf.data.experimental.load(val_data_path,data_spec)\ndata_test(val_data_imp)\ntest_data_imp = tf.data.experimental.load(test_data_path,data_spec)\ndata_test(test_data_imp)","e079b4c6":"data_augmentation = keras.Sequential(\n    [\n        RandomFlip(\"horizontal\"),\n        RandomRotation(0.1),\n    ]\n)\n\ndef make_model(input_shape,num_classes):\n    inputs = keras.Input(shape = input_shape)\n    x = data_augmentation(inputs)\n    \n    x = preprocessing.Rescaling(1.0\/255)(x)\n    x = layers.Conv2D(32,3,strides=2,padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    \n    block_activation = x\n    \n    for size in [128,256,512,728]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n        # Project residual\n        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n            block_activation\n        )\n        x = layers.add([x, residual])  # Add back residual\n        block_activation = x  # Set aside next residual\n\n    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    x = layers.GlobalAveragePooling2D()(x)\n    num_classes == 7\n    activation = \"softmax\"\n    units = 7\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(units, activation=activation)(x)\n    return keras.Model(inputs, outputs)\n\n\nmodel = make_model(input_shape=image_size + (3,), num_classes=7)\nkeras.utils.plot_model(model, show_shapes=True)\n        \n","9ff13c13":"epochs = 10\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.h5\"),\n]\nadam_opt = tf.keras.optimizers.Adam(1e-3)\nmodel.compile(\n  optimizer=adam_opt,\n  loss='sparse_categorical_crossentropy',\n  metrics=['accuracy'])\n\n#model.fit(train_data_imp,epochs = epochs, callbacks = callbacks,validation_data = val_data_imp)","4818c466":"#model.evaluate(test_data_imp)","228c31d6":"!pip install -q -U keras-tuner","d2affaa3":"def make_model_with_hp(hp):\n    input_shape=image_size+(3,)\n    num_classes = 7\n    inputs = keras.Input(shape = input_shape)\n    x = data_augmentation(inputs)\n    \n    x = preprocessing.Rescaling(1.0\/255)(x)\n    x = layers.Conv2D(32,3,strides=2,padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    \n    block_activation = x\n    hp_units = hp.Choice(name = 'units',values=[32,128,256,512])\n    x = layers.Activation(\"relu\")(x)\n    x = layers.SeparableConv2D(hp_units, 3, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    x = layers.SeparableConv2D(hp_units, 3, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n    # Project residual\n    residual = layers.Conv2D(hp_units, 1, strides=2, padding=\"same\")(\n        block_activation\n    )\n    x = layers.add([x, residual])  # Add back residual\n    block_activation = x  # Set aside next residual\n\n    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    x = layers.GlobalAveragePooling2D()(x)\n    num_classes == 7\n    activation = \"softmax\"\n    units = 7\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(units, activation=activation)(x)\n    model = keras.Model(inputs, outputs)\n    \n    \n    hp_learning_rate = hp.Choice('learning_rate',values=[1e-1,1e-2,1e-3])\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate = hp_learning_rate),\n        loss = keras.losses.SparseCategoricalCrossentropy(from_logits =True),\n        metrics = ['accuracy']\n    )\n    \n    return model\n","4de10ff0":"tuner = kt.Hyperband(make_model_with_hp,\n                     objective = 'val_accuracy',\n                     max_epochs = 5,\n                     factor = 3,\n                     directory = 'my_dir',\n                     project_name = 'Anthropoly_Obj_Det'\n                    )\nstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\nkeras.utils.plot_model(model,show_shapes =True)","190d7c7c":"tuner.search(train_data_imp,epochs = epochs, callbacks = callbacks,validation_data = val_data_imp)\n","101f7121":"best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]","175d78bf":"model = tuner.hypermodel.build(best_hps)\nlogs = model.fit(train_data_imp,epochs = epochs, callbacks = callbacks,validation_data = val_data_imp)","89df8bcd":"val_acc_per_epoch = logs.history['val_accuracy']\nbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch))+1","2b7c37dc":"hypermodel = tuner.hypermodel.build(best_hps)\nhypermodel.fit(train_data_imp,epochs = best_epoch, callbacks = callbacks,validation_data = val_data_imp)","62c0d78f":"result = hypermodel.evaluate(test_data_imp)\n","84e21d6a":"### Train Test Validation Split of data ","557bee1a":"### Train the Model with Best Parameters","4e8b5e8f":"# Anthropod Object Detection Model using the Tensorflow\n## Importing the data\n* Used the tensorflow preprocessing methods to import the data \n* Found 15376 files belonging to 7 classes.\n* Using 12301 files for training.\n* Found 15376 files belonging to 7 classes.\n* Using 3075 files for validation.\n* Data belongs to the classes ->['Araneae', 'Coleoptera', 'Diptera', 'Hemiptera', 'Hymenoptera', 'Lepidoptera', 'Odonata']\n* image_dataset_from_directory() search for all images in the directory and import it.\n* While importing the images gets resized to the specified format i.e(128,128) in batches\n* When same seed is used we can divide the data directly into train and validataion set\n## Visualizing the data\n* Using matplot lib, picked a single batch of data and processed it to create a subplot(4,4) \n## Saving the preprocessed data to a location to enable the GPU \n* the dataset of format tf.data can be saved to a location using tf.data.experimental.save() function\n* Saved data can only be retrieved from tf.data.experimental.load() function so the dataset structure can be maintained\n* \n","8afd67cd":"### Evaluating the Model","f227b0b5":"### Importing the data from the saved Files(To Improve the Performance)","d0ab3ff8":"### Parameter Search","6688d005":"### Compiling and Training the Model","f3521ee7":"### Hyper Parameter Tuning - Keras Tuner","48120849":"### Defining the model (Conv2D)","a3f79660":"## Importing the images ","67abad5a":"### Saving the image dataset to improve the performance","6cbca37a":"### Defining a model with Hyper Parameters","9765b789":"### Visualizing the input Data"}}