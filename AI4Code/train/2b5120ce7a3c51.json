{"cell_type":{"e1ee2396":"code","26ad167e":"code","7656b48f":"code","ca0c2751":"code","b45e11fb":"code","9d648b93":"code","7292690e":"code","7ab59fff":"code","700681e6":"code","006b67a7":"code","b481b0a3":"code","22701168":"code","fe985707":"code","4fc07243":"code","d90078c8":"code","daa3fa24":"code","bba03da4":"code","86cb34a9":"code","7b440c25":"code","13ab834c":"code","e31d8fb2":"code","c2916a42":"code","dbe2ab41":"code","1a741771":"code","9d9a5b81":"code","86618834":"code","cc5887de":"code","88056ed4":"code","2d38fd40":"code","38bc3ded":"code","cfc94621":"code","87057cb3":"code","2854ed08":"code","5a30061a":"code","9b364769":"code","fb64e0f3":"code","097de013":"code","7af0ec14":"code","e4105d81":"code","af5d6729":"markdown","be40af40":"markdown","85aa1778":"markdown","5a54b382":"markdown","f7d1f412":"markdown","eb0c75b9":"markdown","5dfbb72f":"markdown","3691179e":"markdown","c66e4d6c":"markdown","f9a63fab":"markdown","cf1ddc0c":"markdown","368edb7d":"markdown","b664a198":"markdown","2c463c94":"markdown","44ac0c43":"markdown","6b87a93a":"markdown","a6ccc023":"markdown","516e725a":"markdown","3c4ce95a":"markdown","f5f17035":"markdown","e1c91ff3":"markdown","07575093":"markdown","d82b1344":"markdown","dc90a17e":"markdown","be6f9190":"markdown","ea72d450":"markdown","4833d6f3":"markdown"},"source":{"e1ee2396":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport time\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","26ad167e":"train = pd.read_csv('..\/input\/liverpool-ion-switching\/train.csv')\ntest = pd.read_csv('..\/input\/liverpool-ion-switching\/test.csv')","7656b48f":"train.head()","ca0c2751":"test.head()","b45e11fb":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(15,5))\nplt.title('Train Set: Signal over Time')\nplt.plot(train.time,train.signal)\nplt.figure(figsize=(15,5))\nplt.title('Train Set: Open Channels over Time')\nplt.plot(train['time'],train['open_channels'])\nplt.figure(figsize=(15,5))\nplt.title('Test Set: Signal over Time')\nplt.plot(test['time'],test['signal'])\nplt.show()","9d648b93":"def add_batch(data, batch_size):\n    c = 'batch_' + str(batch_size)\n    data[c] = 0\n    ci = data.columns.get_loc(c)\n    n = int(data.shape[0] \/ batch_size)\n    print('Batch size:', batch_size, 'Column name:', c, 'Number of batches:', n)\n    for i in range(0, n):\n        data.iloc[i * batch_size: batch_size * (i + 1), ci] = i\n        \nfor batch_size in [500000, 50000]:\n    add_batch(train, batch_size)\n    add_batch(test, batch_size)","7292690e":"batch = train[train['batch_50000']==10]\nplt.figure(figsize=(15,5))\nplt.plot(batch.time,batch.signal)\nplt.figure(figsize=(15,5))\nplt.plot(batch.time,batch.open_channels)\nplt.show()","7ab59fff":"from scipy import signal\n\ntrain_detrend = train.copy()\ntest_detrend = test.copy()","700681e6":"train_batch1_coords = [[50.000,60.0000]]       # start and stop times of trend in this format [[start, stop], [start stop]]\ntrain_batch1 = train[train['batch_500000']==1] # isolating overall batch of where linear trend was found\nbaseline = abs(min(train_batch1['signal']))    # establishing baseline value, by taking absolute value of overall batch's minimal value.\n#print(baseline)\n\nfor i in range(0,len(train_batch1_coords)):    # iterate through start\/stop coordinates\n    s_index = train.index.get_loc(train.index[train['time'] == train_batch1_coords[i][0]][0]) # get index of where start time is in training set\n    #print(s_index)\n    f_index = train.index.get_loc(train.index[train['time'] == train_batch1_coords[i][1]][0]) # get index of where stop time is in training set\n    #print(f_index)\n    lin_batch = train[s_index:f_index]                                                        # get slice of df from start to stop index\n    detrend_lin = signal.detrend(lin_batch['signal'])                                         # detrend that slice's signal and store result\n    offset = abs(min(detrend_lin)) - baseline                                                 # calculate the offset\n    train_detrend.loc[train.index[s_index:f_index], 'signal'] = detrend_lin + (offset)        # replace signal in new df in the respective indices.\n    ","006b67a7":"plt.figure(figsize=(15,5))\nplt.plot(train_detrend.time,train_detrend.signal)\nplt.show()","b481b0a3":"test_batch0 = test[test['batch_500000']==0]\ntest_batch0_coords = [[500.0001,510.0000],[510.0001,520.0000],[540.0000,550.0000]]\n\nsb_index = test_batch0.index.get_loc(test_batch0.index[test_batch0['time'] == 530.0001][0]) # get start index of the \"baseline\" slice we identify\nfb_index = test_batch0.index.get_loc(test_batch0.index[test_batch0['time'] == 539.9999][0]) # get stop index of the \"baseline\" slice we identify\nbaseline_batch = test_batch0[sb_index:fb_index]                                             \nbaseline = abs(min(baseline_batch['signal']))                                               # get minimal value of the \"baselines\" slice\n\nfor i in range(0,len(test_batch0_coords)):\n    s_index = test.index.get_loc(test.index[test['time'] == test_batch0_coords[i][0]][0])\n    #print(s_index)\n    f_index = test.index.get_loc(test.index[test['time'] == test_batch0_coords[i][1]][0])\n    #print(f_index)\n    lin_batch = test[s_index:f_index]\n    detrend_lin = signal.detrend(lin_batch['signal'])\n    offset = abs(min(detrend_lin)) - baseline\n    test_detrend.loc[test.index[s_index:f_index], 'signal'] = detrend_lin + (offset)\n    ","22701168":"test_batch1 = test[test['batch_500000']==1]\ntest_batch1_coords = [[560.0000, 569.9999], [570.0000, 580.0000],[580.0001,590.0000]]\n\nsb_index = test_batch1.index.get_loc(test_batch1.index[test_batch1['time'] == 590.0001][0])\nfb_index = test_batch1.index.get_loc(test_batch1.index[test_batch1['time'] == 599.9999][0])\nbaseline_batch = test_batch1[sb_index:fb_index]\nbaseline = abs(min(baseline_batch['signal']))\n\nfor i in range(0,len(test_batch1_coords)):\n    s_index = test.index.get_loc(test.index[test['time'] == test_batch1_coords[i][0]][0])\n    #print(s_index)\n    f_index = test.index.get_loc(test.index[test['time'] == test_batch1_coords[i][1]][0])\n    #print(f_index)\n    lin_batch = test[s_index:f_index]\n    detrend_lin = signal.detrend(lin_batch['signal'])\n    offset = abs(min(detrend_lin)) - baseline\n    #print(offset)\n    test_detrend.loc[test.index[s_index:f_index], 'signal'] = detrend_lin + offset","fe985707":"plt.figure(figsize=(15,5))\nplt.plot(test_detrend.time,test_detrend.signal)\nplt.show()","4fc07243":"def remove_poly_trend(x, y):\n    model = np.polyfit(x, y, 4)\n    predicted = np.polyval(model, x)\n    \n    detrended = y - predicted \n    \n    #print(detrended)\n    return detrended","d90078c8":"sb_index = train_detrend.index.get_loc(train_detrend.index[train_detrend['time'] == 190.0000][0]) # start index of relatively flat slice of dataset\nfb_index = train_detrend.index.get_loc(train_detrend.index[train_detrend['time'] == 199.9999][0]) # stop index of relatively flat slice of dataset\nbaseline_batch = train_detrend[sb_index:fb_index]\nbaseline = abs(min(baseline_batch['signal']))                                                     #base line value\n\nn_train = int(train_detrend.shape[0] \/ 500000)\n\nfor i in range(6, n_train):\n    batch = train[train['batch_500000']==i]\n    detrend_poly = remove_poly_trend(batch['time'], batch['signal'])\n    offset = abs(min(detrend_poly)) - baseline\n    min_index = train_detrend.index.get_loc(train_detrend.index[train_detrend['time'] == min(batch['time'])][0])\n    #print(min_index)\n    max_index = train_detrend.index.get_loc(train_detrend.index[train_detrend['time'] == max(batch['time'])][0])\n    #print(max_index)\n    train_detrend.loc[train_detrend.index[min_index : max_index], 'signal'] = detrend_poly + offset\n    #print(detrend_poly)","daa3fa24":"plt.figure(figsize=(15,5))\nplt.plot(train_detrend.time,train_detrend.signal)\nplt.show()","bba03da4":"sb_index = train_detrend.index.get_loc(train_detrend.index[train_detrend['time'] == 340.0000][0])\nfb_index = train_detrend.index.get_loc(train_detrend.index[train_detrend['time'] == 349.9999][0])\nbaseline_batch = train_detrend[sb_index:fb_index]\nbaseline = abs(min(baseline_batch['signal']))\n\nsm_index = train_detrend.index.get_loc(train_detrend.index[train_detrend['time'] == 350.0000][0])\nfm_index = train_detrend.index.get_loc(train_detrend.index[train_detrend['time'] == 359.9999][0])\nmisalign_batch = train_detrend[sm_index:fm_index]\nmisalign = abs(min(misalign_batch['signal']))\n\nbatch = train_detrend[train_detrend['batch_500000']==7]\n\noffset = misalign - baseline\nmin_index = train_detrend.index.get_loc(train_detrend.index[train_detrend['time'] == min(batch['time'])][0])\nmax_index = train_detrend.index.get_loc(train_detrend.index[train_detrend['time'] == max(batch['time'])][0])\ntrain_detrend.loc[train_detrend.index[min_index : max_index], 'signal'] = train_detrend.loc[train_detrend.index[min_index : max_index], 'signal'] + offset\n#print(detrend_poly)","86cb34a9":"plt.figure(figsize=(15,5))\nplt.plot(train_detrend.time,train_detrend.signal)\nplt.show()","7b440c25":"sb_index = test_detrend.index.get_loc(test_detrend.index[test_detrend['time'] == 660.0000][0])\nfb_index = test_detrend.index.get_loc(test_detrend.index[test_detrend['time'] == 664.9999][0])\nbaseline_batch = test_detrend[sb_index:fb_index]\nbaseline = abs(min(baseline_batch['signal']))\n\nbatch = test[test['batch_500000']==2]\ndetrend_poly = remove_poly_trend(batch['time'], batch['signal'])\noffset = abs(min(detrend_poly)) - baseline\nmin_index = test_detrend.index.get_loc(test_detrend.index[test_detrend['time'] == min(batch['time'])][0])\n#print(min_index)\nmax_index = test_detrend.index.get_loc(test_detrend.index[test_detrend['time'] == max(batch['time'])][0])\n#print(max_index)\ntest_detrend.loc[test_detrend.index[min_index : max_index], 'signal'] = detrend_poly + offset\n#print(detrend_poly)\n","13ab834c":"plt.figure(figsize=(15,5))\nplt.plot(test_detrend.time,test_detrend.signal)\nplt.show()","e31d8fb2":"import gc\n\ndel train\ndel test\n\ngc.collect()","c2916a42":"window_sizes = [10, 50, 100]\nwindow_sizes1 = [1000, 2500, 5000]","dbe2ab41":"n_train = int(train_detrend.shape[0] \/ 500000)\n\nfor i in range(0, n_train):\n    batch = train_detrend[train_detrend['batch_500000']==i]\n    \n    min_index = train_detrend.index.get_loc(train_detrend.index[train_detrend['time'] == min(batch['time'])][0])\n    max_index = train_detrend.index.get_loc(train_detrend.index[train_detrend['time'] == max(batch['time'])][0])\n\n    for window in window_sizes:\n        train_detrend.loc[train_detrend.index[min_index : max_index], \"rolling_mean_\" + str(window)] = train_detrend.loc[train_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).mean()\n        train_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_std_\" + str(window)] = train_detrend.loc[train_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).std()\n        train_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_var_\" + str(window)] = train_detrend.loc[train_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).var()\n        train_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_min_\" + str(window)] = train_detrend.loc[train_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).min()\n        train_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_max_\" + str(window)] = train_detrend.loc[train_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).max()\n        train_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_kurtosis_\" + str(window)] = train_detrend.loc[train_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).kurt()\n        train_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_covariance_\" + str(window)] = train_detrend.loc[train_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).cov()\n       \n    for window in window_sizes1:\n        train_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_25_quartile_\" + str(window)] = train_detrend.loc[train_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).quantile(0.25)\n        train_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_50_quartile\" + str(window)] = train_detrend.loc[train_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).quantile(0.5)\n        train_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_75_quartile_\" + str(window)] = train_detrend.loc[train_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).quantile(0.75)\n        train_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_90_quartile_\" + str(window)] = train_detrend.loc[train_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).quantile(0.9)\n        \ntrain_detrend.fillna(0, inplace=True)\n\ntrain_detrend.head()","1a741771":"n_test = int(test_detrend.shape[0] \/ 500000)\n\nfor i in range(0, n_test):\n    batch = test_detrend[test_detrend['batch_500000']==i]\n    \n    min_index = test_detrend.index.get_loc(test_detrend.index[test_detrend['time'] == min(batch['time'])][0])\n    max_index = test_detrend.index.get_loc(test_detrend.index[test_detrend['time'] == max(batch['time'])][0])\n\n    for window in window_sizes:\n        test_detrend.loc[test_detrend.index[min_index : max_index], \"rolling_mean_\" + str(window)] = test_detrend.loc[test_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).mean()\n        test_detrend.loc[test_detrend.index[min_index : max_index],\"rolling_std_\" + str(window)] = test_detrend.loc[test_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).std()\n        test_detrend.loc[test_detrend.index[min_index : max_index],\"rolling_var_\" + str(window)] = test_detrend.loc[test_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).var()\n        test_detrend.loc[test_detrend.index[min_index : max_index],\"rolling_min_\" + str(window)] = test_detrend.loc[test_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).min()\n        test_detrend.loc[test_detrend.index[min_index : max_index],\"rolling_max_\" + str(window)] = test_detrend.loc[test_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).max()\n        test_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_kurtosis_\" + str(window)] = test_detrend.loc[test_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).kurt()\n        test_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_covariance_\" + str(window)] = test_detrend.loc[test_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).cov()\n        \n        \n    for window in window_sizes1:\n        test_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_25_quartile_\" + str(window)] = test_detrend.loc[test_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).quantile(0.25)\n        test_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_50_quartile_\" + str(window)] = test_detrend.loc[test_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).quantile(0.5)\n        test_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_75_quartile_\" + str(window)] = test_detrend.loc[test_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).quantile(0.75)\n        test_detrend.loc[train_detrend.index[min_index : max_index],\"rolling_90_quartile_\" + str(window)] = test_detrend.loc[test_detrend.index[min_index : max_index], 'signal'].rolling(window=window, min_periods=1).quantile(0.9)\n        \n\ntest_detrend.fillna(0, inplace=True)\n\ntest_detrend.head()","9d9a5b81":"train_detrend = train_detrend.drop(columns=['batch_50000', 'batch_500000'])\ntest_detrend = test_detrend.drop(columns=['batch_50000', 'batch_500000'])","86618834":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        if col != 'time':\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","cc5887de":"train_detrend=reduce_mem_usage(train_detrend)\ntest_detrend=reduce_mem_usage(test_detrend)","88056ed4":"def features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index \/\/ 25_000\n    df['batch_index'] = df.index  - (df.batch * 25_000)\n    df['batch_slices'] = df['batch_index']  \/\/ 2500\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in ['batch','batch_slices2']:\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal'].mean()\n        d['median'+c] = df.groupby([c])['signal'].median()\n        d['max'+c] = df.groupby([c])['signal'].max()\n        d['min'+c] = df.groupby([c])['signal'].min()\n        d['std'+c] = df.groupby([c])['signal'].std()\n        d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n        d['range'+c] = d['max'+c] - d['min'+c]\n        d['maxtomin'+c] = d['max'+c] \/ d['min'+c]\n        d['abs_avg'+c] = (d['abs_min'+c] + d['abs_max'+c]) \/ 2\n            \n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n\n    # add shifts_1\n    df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==24999].index:\n        df['signal_shift_-1'][i] = np.nan\n    \n    df = df.drop(columns=['batch', 'batch_index', 'batch_slices', 'batch_slices2'])\n\n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels'] and 'quartile' not in c1 and 'kurtosis' not in c1 and 'covariance' not in c1 ]:\n        df[c+'_msignal'] = df[c] - df['signal']\n        \n    df = df.replace([np.inf, -np.inf], np.nan)    \n    df.fillna(0, inplace=True)\n    gc.collect()\n    return df\n\ntrain_detrend = features(train_detrend)\ntest_detrend = features(test_detrend)","2d38fd40":"train_detrend=reduce_mem_usage(train_detrend)\ntest_detrend=reduce_mem_usage(test_detrend)","38bc3ded":"train_final = train_detrend.drop(columns=['time'])\ntest_final = test_detrend.drop(columns=['time'])\n\n#train_final = train_detrend.copy()\n#test_final = train_detrend.copy()","cfc94621":"from sklearn.model_selection import train_test_split\n\nseed_random = 316\n\ny_train = train_final['open_channels'].copy()\nx_train = train_final.drop(['open_channels'], axis=1)\n\nx_test = test_final.copy()\n\nx_t_train, x_t_val, y_t_train, y_t_val = train_test_split(x_train, y_train, test_size=0.25, random_state=seed_random)","87057cb3":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\n\nencoder = LabelEncoder()\nencoder = encoder.fit(y_t_train)\n\ny_train_econded = encoder.transform(y_t_train)\ny_val_econded = encoder.transform(y_t_val)\n\ny_train_dummy = np_utils.to_categorical(y_train_econded)\ny_val_dummy = np_utils.to_categorical(y_val_econded)","2854ed08":"from keras.models import Sequential\nfrom keras.optimizers import Adam, Nadam\nfrom keras.layers import Dense, Dropout\n\ninput_size = len(x_t_train.columns)\n\ndeep_model = Sequential()\ndeep_model.add(Dense(180, input_dim=input_size, kernel_initializer='glorot_uniform', activation='softplus'))\n#deep_model.add(Dropout(0.2))\ndeep_model.add(Dense(80, kernel_initializer='glorot_uniform', activation='softplus'))\ndeep_model.add(Dense(30,kernel_initializer='glorot_uniform', activation='softplus'))\ndeep_model.add(Dense(24,kernel_initializer='glorot_uniform', activation='softplus'))\n#deep_model.add(Dense(18,kernel_initializer='glorot_uniform', activation='softplus'))\ndeep_model.add(Dense(11, kernel_initializer='glorot_uniform', activation='softmax'))\n\ndeep_model.compile(loss='categorical_crossentropy', \n                   optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=True),\n                   metrics=['accuracy'])","5a30061a":"deep_model.fit(x_t_train, y_train_dummy, \n               epochs=50, \n               batch_size=2500,\n               validation_data=(x_t_val, y_val_dummy))","9b364769":"deep_test_pred = deep_model.predict_classes(x_test)\ndeep_test_pred_decoded = encoder.inverse_transform(deep_test_pred)\n","fb64e0f3":"deep_val_pred = deep_model.predict_classes(x_t_val)\ndeep_val_pred_decoded = encoder.inverse_transform(deep_val_pred)","097de013":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix, f1_score, mean_absolute_error, make_scorer \n\n# Showing Confusion Matrix\n# Thanks to https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title):\n    figsize=(14,14)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","7af0ec14":"plot_cm(y_t_val, deep_val_pred_decoded, 'Confusion matrix for the ANN predictions on validation set')\nf1_score(y_t_val, deep_val_pred_decoded, average = 'macro')","e4105d81":"submission = test_detrend.filter(['time'], axis=1)\nsubmission.reset_index(drop=True, inplace=True)\nsubmission['open_channels'] = deep_test_pred_decoded\nsubmission.to_csv('submission.csv', index=False, float_format='%.4f')\n\nsubmission.head()","af5d6729":"## Batching\n\nFrom an overview of the data, it can be easy to assume that it is one continous time-series dataset. This, however, is not the case! Thankfully the University of Liverpool organizers clarified [here](https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/data) that the data is composed of a number of batches, containing 500,000 rows each!\n\nErgo, we should first discretize our datasets by batch so that whatever manipulations we perform on them can be batch-specific, making said manipulations batch specific without fear of being affected by what would be considered *outlier* behavior from other batches. This accomplish this we use a slightly modified piece of code from this notebook: [Ion Switching: FE + LGB](https:\/\/www.kaggle.com\/pavelvpster\/ion-switching-fe-lgb).\n\nSpecifically, we create two columns:\n\n* Batch_500000 : Which identifies which overall batch, of 500,000 rows, the row belongs to.\n* Batch_50000  : Which groups rows into a smaller batches of 50,000 rows. We'll mainly use this as a way to reference a finer view into our dataset.","be40af40":"Finally, now that we have detrended training and testing sets that we will use moving forward, we free up some memory by deleting the original sets.","85aa1778":"We establish and compile our model.","5a54b382":"So it seems we really only have up to three base features to work with: Time, Signal Magnitude, and Number of Open Channels. \n\nLet's visualize these features to get a better understanding of their relationship.","f7d1f412":"From above, although the graph is quite crowded, we can see how the number of open channels there are is directly related to magnitude of the signal. \n\nWe also see that there is a significant number of sections  of the signal feature, in both training and testing sets, with linear and parabolic drifts. We definitely want to remove these drifts, especially when we see these drifts do not relate to the number of the open channels available, would most likely cause inaccurate model learning and subsequently lead to poor model performance.\n\nWith these preliminary observations, we have an idea of what preprocessing needs to be done to get our data into a more workable form.","eb0c75b9":"Everything looks good!","5dfbb72f":"## Parabolic Detrending\n\nTo remove the parabolic trends in our data, we take a similar approach to how we remove the linear trends with the addition of a custom function that maps the polynomial shape of our input data. We also notice that when doing this detrending, the resultant signal is misaligned from its original position; To get around this, we find the necessary offset in a similar way we did for removing the linear trend from the test set above, except we select a *baseline* slice of relatively flat signal from the overall dataset, instead of the batch.\n\nWe observe from our testing and training sets that wherever there is a parabolic trend, that trend is found across the entirety of an overall batch; This helps simplify our detrending a bit.","3691179e":"We take the opportunity to get a finer view of our data by visualizing a batch below:","c66e4d6c":"### Training Set\n\nFrom a review of our training set batches and sub-batches (not shown), we see that the only linear trend exists in *Batch 1*, specifically from the 50.0000 to 60.0000 second marks. We use the following code to remove the linear trend from this segment of our data, and we will use a modified version of it for subsequent needs.\n\nI found that using *signal.detrend* on its own was not sufficient, as the resultant detrended data had an offset that misaligned its signal from the rest of the batch's signal. To get around this, we use a naive approach to get a *baseline* of the batch by taking the absolute minimum value of the overall batch, subtracting it from the absolute minimum of the detrended signal, and adding the difference to the detrended signal as an offset.","f9a63fab":"Now that we've made the most use out of our batch identifiers and the fact they will interfere with subsequent feature engineering, we drop them.","cf1ddc0c":"# **Introduction**\n\nIn this notebook, I aim to add some variety to the competition by offering some relatively simple methods of preprocessing the data, doing some feature engineering on it, and training a model that yields great performance (without needing a significant amount of time to train)!","368edb7d":"Everything looks good!","b664a198":"After run the features function, we reduce its memory. Good thing we did, as we managed to reduce the space taken up significantly.","2c463c94":"### Testing Set\nWe repeat the parabolic detrending on the test set, where we only find it in *Batch 2*.","44ac0c43":"For the rest of the feature engineering, we use a bit of code from the notebook: [Ion Switching - Advanced FE, LGB, XGB, ConfMatrix](https:\/\/www.kaggle.com\/vbmokin\/ion-switching-advanced-fe-lgb-xgb-confmatrix). \n\nAs this code does some significant work, we first introduce a function that will reduce the amount of memory our resultant dataframes allowing us to do more with our notebook.","6b87a93a":"## Linear Detrending\n\nIn this preprocessing section, we will remove the linear trends found in both our training and testing sets. First, we will import the *signal* library from *Scipy* and create copies of our training and testing sets, which we will use to store our detrended results.","a6ccc023":"# Model Building\n\nNow that we have fully preprocessed training and testing sets, we are able to move onto model building. First, we make copies of said sets without the *time* feature, as we'd like have our model train and predict without it.\n\nWe hold onto our detrended sets in case we hope to do more work on them before this section in future iterations of this notebook.","516e725a":"Train it on our training and validation data.","3c4ce95a":"First, let's read in our datasets and see what we are dealing with.","f5f17035":"As we will be attempting to perform categorical classification on how many open channels exist at a given signal, we need to encode our target labels before providing them to our model. We accomplish this below:","e1c91ff3":"We then split these sets into *X (training\/eval)* features and *Y (label)* features. We also take 25% of the training dataset to use as our validation set, to use during model training to ensure our model is acheived its best possible performance.","07575093":"We also use the features function from the above [notebook](https:\/\/www.kaggle.com\/vbmokin\/ion-switching-advanced-fe-lgb-xgb-confmatrix) to generate the following features:\n* The mean of a batch of 25,000 and 2,500.\n* The median of a batch of 25,000 and 2,500.\n* The max of a batch of 25,000 and 2,500.\n* The min of a batch of 25,000 and 2,500.\n* The standard of a batch of 25,000 and 2,500.\n* The absolute change of a batch of 25,000 and 2,500.\n* The absolute max of a batch of 25,000 and 2,500.\n* The absolute min of a batch of 25,000 and 2,500.\n* The range of a batch of 25,000 and 2,500.\n* The ratio of a batch of 25,000 and 2,500.\n* The average of a batch of 25,000 and 2,500.\n* The shift features of a batch of 25,000 and 2,500.\n* Features of all prior mentioned features with the signal added and substracted from them.","d82b1344":"# **Preprocessing**","dc90a17e":"# Feature Engineering\nWe found that using just *signal* feature was enough to train an an accurate model. It is necessary for us to do some feature engineering to extract some more information out of our datasets.\n\nFirst, we extract a few rolling variables for each batch in our training and testing sets. We use window sizes of 10 and 50 to extract the rolling mean, standard, variance, minimum and maximum for each discrete batch of 500,000 rows; Again, this is to ensure the rolling features are not affected by other batches, as this data is not truely continuous.","be6f9190":"We see that for *Batch 7*, our signal is misaligned because our prior algorithm used the center stub to generate the offset. We manually adjust it with the following code below:","ea72d450":" ### Testing Set\n \nWe now repeat what we did, to detrend our training set, on our testing set. From our overview (not shown), we found that multiple overall batches of the testing set contain linear trends, sometimes with multiple unique trends.\n\nThe main modifications we make are:\n* adding more start\/stop times\n* modifying our code to get the baseline value from a specific slice of the batch, instead of the overall \n ","4833d6f3":"### Training Set"}}