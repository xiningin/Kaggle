{"cell_type":{"1b085382":"code","efd077bb":"code","44f1c9b5":"code","ae2d6f6d":"code","4dc9cafb":"code","c8d4a71e":"code","81366d39":"code","42987314":"code","d3c7165b":"code","c2a4305e":"code","5cf75db1":"code","1e3de385":"code","7997f95e":"code","052429f9":"code","21c233b2":"code","e416bd0a":"code","4dc10b57":"code","359882d8":"code","24a3b691":"code","f329fdc4":"code","a155d894":"code","fbeec5e6":"code","11534843":"code","766ad39c":"code","67c14699":"code","ccd1e301":"code","b6b3106e":"code","7d79c39b":"code","789703a2":"code","99b243b9":"code","87a507d3":"code","390ead33":"code","b29c96f4":"code","8b95c360":"code","40b5509e":"code","1701f38f":"code","13e0fbb0":"code","1d06b3d7":"code","6f26b0bc":"code","7d92dd7c":"code","e17b27a2":"code","486c7aa6":"code","89107c47":"code","8b0fe86e":"code","3a3e66f9":"code","9a1463c7":"code","4e8ea619":"code","eef916fc":"code","3fa39ef5":"code","370cf212":"code","08013337":"code","2c001fc1":"code","52ac2118":"code","0a47f169":"code","9bf744ff":"code","a704afde":"code","ea29813e":"code","3413977a":"code","068c3119":"code","291bb936":"code","f73b207c":"code","a79fa4c4":"code","a9b73fff":"code","8bd609d0":"code","cbd611d0":"code","cd626b5b":"code","97988dbe":"code","9b4d58d8":"code","73cc9571":"code","0cf5f2de":"code","aec1f6ff":"code","4d1bb9ef":"code","2f85fb88":"code","d01f8946":"code","bb13d577":"code","6c4de795":"code","5c8cc192":"code","70339eab":"code","5842718a":"code","8ac94f53":"code","6c1b3a04":"code","79a97ff7":"code","1498ec8e":"code","2429f4ae":"code","1ce6c939":"markdown","637e326b":"markdown","78fc483e":"markdown","3dd85e71":"markdown","ecc22e9a":"markdown","4f77ff36":"markdown","2afd7dd4":"markdown","0742a10d":"markdown","48d747c5":"markdown","cb43e77d":"markdown","097c6604":"markdown","d0300611":"markdown","3f16e9e4":"markdown","9d36466e":"markdown","4385906d":"markdown","b3ad4fcf":"markdown","2f51593f":"markdown","34563a12":"markdown","f5b255df":"markdown","bcab461d":"markdown","5d935771":"markdown","3d9741fb":"markdown","e641c340":"markdown","559c3c08":"markdown","737ae735":"markdown","6a242cd7":"markdown","8389cc1d":"markdown","4fdb9508":"markdown","ae809fb6":"markdown","015cc761":"markdown","edf76b42":"markdown","3f918125":"markdown","da20ca51":"markdown","4715b915":"markdown","b2107449":"markdown","7c8d7924":"markdown","c72b8cae":"markdown","183efe2e":"markdown","b7b24f0c":"markdown","68242413":"markdown","db6fb382":"markdown","ec5dd896":"markdown","b95e3c13":"markdown","a30471eb":"markdown","ddefc0d1":"markdown","8054a7a0":"markdown","3c40801a":"markdown","5c9d603d":"markdown","774a2ddd":"markdown","fef53959":"markdown","923a2411":"markdown","5c470f15":"markdown","b80251f3":"markdown","13e90d5a":"markdown","7858f7a5":"markdown","8d1848b6":"markdown","1407ff2a":"markdown","bab6e1d2":"markdown","25a44bc8":"markdown","ea58780a":"markdown","197c5efd":"markdown","606480a6":"markdown","f7cd4155":"markdown","6fc89b05":"markdown"},"source":{"1b085382":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime as dt","efd077bb":"price1999 = pd.read_csv('..\/input\/singapore-hdb-flat-resale-prices-19902020\/resale-flat-prices-based-on-approval-date-1990-1999.csv')\nprice2012 = pd.read_csv('..\/input\/singapore-hdb-flat-resale-prices-19902020\/resale-flat-prices-based-on-approval-date-2000-feb-2012.csv')\nprice2014 = pd.read_csv('..\/input\/singapore-hdb-flat-resale-prices-19902020\/resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.csv')\nprice2016 = pd.read_csv('..\/input\/singapore-hdb-flat-resale-prices-19902020\/resale-flat-prices-based-on-registration-date-from-jan-2015-to-dec-2016.csv')\nprice2017 = pd.read_csv('..\/input\/singapore-hdb-flat-resale-prices-19902020\/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv')\ncpi = pd.read_csv('..\/input\/flat-amenities\/CPI.csv')","44f1c9b5":"price1999.head()","ae2d6f6d":"price2012.head()","4dc9cafb":"price2014.head()","c8d4a71e":"price2016.head()","81366d39":"# Merge dfs\nprices = pd.concat([price1999, price2012, price2014], sort=False)\nprices = pd.concat([prices, price2016, price2017], axis=0, ignore_index=True, sort=False)\n\nprices['month'] = pd.to_datetime(prices['month']) # to datetime\n\nprices.info()","42987314":"prices[~prices.isnull().any(axis=1)]['month'].dt.year.unique()","d3c7165b":"# Clean flat type\nprices['flat_type'] = prices['flat_type'].str.replace('MULTI-GENERATION', 'MULTI GENERATION')\nprices['flat_type'].unique()","c2a4305e":"# Rename flat model duplicates\nreplace_values = {'NEW GENERATION':'New Generation', 'SIMPLIFIED':'Simplified', 'STANDARD':'Standard', 'MODEL A-MAISONETTE':'Maisonette', 'MULTI GENERATION':'Multi Generation', 'IMPROVED-MAISONETTE':'Executive Maisonette', 'Improved-Maisonette':'Executive Maisonette', 'Premium Maisonette':'Executive Maisonette', '2-ROOM':'2-room', 'MODEL A':'Model A', 'MAISONETTE':'Maisonette', 'Model A-Maisonette':'Maisonette', 'IMPROVED':'Improved', 'TERRACE':'Terrace', 'PREMIUM APARTMENT':'Premium Apartment', 'Premium Apartment Loft':'Premium Apartment', 'APARTMENT':'Apartment', 'Type S1':'Type S1S2', 'Type S2':'Type S1S2'}\n\nprices = prices.replace({'flat_model': replace_values})\n\nprices['flat_model'].value_counts()\n\n","5cf75db1":"prices['storey_range'].unique()","1e3de385":"prices['town'].unique()","7997f95e":"plt.hist(prices['floor_area_sqm'], bins=50, edgecolor='black')\nplt.title('Distribution of HDB Floor Area')\nplt.show()\ndisplay(prices[prices['floor_area_sqm'] > 200]['flat_model'].value_counts())","052429f9":"bins = prices['lease_commence_date'].max() - prices['lease_commence_date'].min()\nplt.hist(prices['lease_commence_date'], bins=bins, edgecolor='black')\nplt.title('Distribution of Lease Commence Year')\nplt.show()","21c233b2":"# Compute Resale Price Adjusted for Inflation Using Consumer Price Index for Housing & Utilities\n# https:\/\/www.singstat.gov.sg\/find-data\/search-by-theme\/economy\/prices-and-price-indices\/latest-data\ncpi['month'] = pd.to_datetime(cpi['month'], format='%Y %b') # to datetime\nprices = prices.merge(cpi, on='month', how='left') \n# https:\/\/people.duke.edu\/~rnau\/411infla.htm\nprices['real_price'] = (prices['resale_price'] \/ prices['cpi']) * 100 ","e416bd0a":"# Plot Median Resale Prices Over the Years\n\n# Unadjusted\nfig = plt.figure(figsize=(14,4.5))\nfig.suptitle('Median HDB Resale Prices Over the Years', fontsize=18)\nax1 = fig.add_subplot(121)\nprices.groupby('month')[['resale_price']].median().plot(ax=ax1, color='#00cef6', legend=None)\nax1.set_xlabel('Date'), ax1.set_ylabel('Resale Price in SGD ($)'), ax1.set_ylim(0, 500000), ax1.set_title('Unadjusted for Inflation', size=15)\n\n# Adjusted\n# https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/04.09-text-and-annotation.html\nax2 = fig.add_subplot(122)\nprices.groupby('month')[['real_price']].median().plot(ax=ax2, color='#3c78d8', legend=None)\nax2.set_xlabel('Date'), ax2.set_ylabel('Resale Price in SGD ($)'), ax2.set_ylim(0, 500000), ax2.set_title('Adjusted for Inflation to 2019 SGD',size=15)\nax2.annotate('1997 Asian Financial Crisis\\nMedian: $403,766', xy=('1997-05-01',380000), xycoords='data', \n    bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"none\", ec=\"#28324a\"), xytext=(50,-140), textcoords='offset points', ha='center',\n    arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"angle,angleA=0,angleB=90,rad=20\"))\nax2.annotate('2013 Cooling Measures\\nMedian: $401,887', xy=('2013-07-01',380000), xycoords='data', \n    bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"none\", ec=\"#28324a\"), xytext=(0,-90), textcoords='offset points', ha='center',\n    arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"angle,angleA=0,angleB=90,rad=20\"))  \nplt.tight_layout(rect=[0, 0, 0.9, 0.9]) \n# for ax, color in zip([ax1, ax2], ['#3c78d8', '#3c78d8']):\n#     plt.setp(tuple(ax.spines.values()), color=color)\n#     plt.setp([ax.get_xticklines(), ax.get_yticklines()], color=color)\nplt.show()\n#prices.set_index('month').loc['1997']['real_price'].median()","4dc10b57":"# Convert remaining_lease to number of years\ndef getYears(text):\n    if isinstance(text, str):\n        yearmonth = [int(s) for s in text.split() if s.isdigit()]\n        if len(yearmonth) > 1: # if there's year and month\n            years = yearmonth[0] + (yearmonth[1]\/12)\n        else: # if only year\n            years = yearmonth[0]\n        return years\n    else: # if int\n        return text\n\nprices['remaining_lease'] = prices['remaining_lease'].apply(lambda x: getYears(x))\n\n","359882d8":"prices.info()","24a3b691":"bins = prices['remaining_lease'].max() - prices['remaining_lease'].min()\nplt.hist(prices['remaining_lease'], bins=int(bins), edgecolor='black')\nplt.title('Distribution of Remaining Lease for 2016-2020 Data')\nplt.show()","f329fdc4":"prices.head()","a155d894":"## Waffle chart for flat type - number of rooms\n!pip install pywaffle\nfrom pywaffle import  Waffle\n\nflattype = dict(prices['flat_type'].value_counts()\/len(prices)*100)\nflattype1519 = dict(prices.set_index('month')['2015':'2019'].reset_index()['flat_type'].value_counts()\/len(prices.set_index('month')['2015':'2019'].reset_index())*100)\n\nplt.figure(figsize=(10,5),\n    FigureClass=Waffle, \n    plots={\n        '211': {\n            'values': flattype,\n            'legend': {'loc': 'upper left', 'bbox_to_anchor': (1.05, 1), 'fontsize':11},\n            'title': {'label': 'Proportion of HDB Flat Types (All Years)', 'loc': 'left', 'fontsize':16}\n        },\n        '212': {\n            'values': flattype1519,\n            'legend': {'loc': 'upper left', 'bbox_to_anchor': (1.05, 1), 'fontsize':11},\n            'title': {'label': 'Proportion of HDB Flat Types (2015-2019)', 'loc': 'left', 'fontsize':16}            \n        },\n    },\n    rows=5, \n    colors=[\"#1f77b4\", \"#ff7f0e\", \"green\", 'purple', 'black', 'yellow', 'brown'],\n    #colors=[\"#3c78d8\", \"#00cef6\", \"#aff000\", '#28324a', 'black', 'yellow', 'brown'],\n    icons='home', \n    font_size=22, \n    icon_legend=True)\n    \nplt.show()","fbeec5e6":"flattype = ['3 ROOM','4 ROOM','5 ROOM','EXECUTIVE']\nprices1519 = prices.set_index('month').sort_index().loc['2015-01':'2019-12']\nprices1519 = prices1519[prices1519['flat_type'].isin(flattype)][['flat_type','real_price']].reset_index()\nprices1519['flat_type_year'] = prices1519['flat_type'] + ' - ' + prices1519['month'].apply(lambda x: str(x)[:4])\nprices1519","11534843":"# ridgeline plot for looking at distribution of flat types by year\n!pip install joypy\nimport joypy\nfig, axes = joypy.joyplot(prices1519, by=\"flat_type_year\", column=\"real_price\",figsize=(9,7),\n             linewidth=0.05,overlap=1.5,alpha=0.8,colormap=plt.cm.get_cmap('tab20',4))\naxes[-1].set_xlim([0,1200000])\naxes[-1].set_xticklabels(['0', '200k', '400k', '600k', '800k', '1000k', '1200k']) \nplt.xlabel('Resale Price SGD ($)', fontsize=14)\nfig.show()","766ad39c":"## 2015 to 2019\nprices['year'] = pd.DatetimeIndex(prices['month']).year # extract out year\nprices1519 = prices[prices['year'].isin([2015,2016,2017,2018,2019])].groupby(['town'], as_index=False).agg({'real_price': 'median'}).sort_values('real_price', ascending=True).reset_index(drop=True)\nprices1519['real_price'] = round(prices1519['real_price']\/1000)\nprices1519['color'] = ['#f8766d'] + ['#3c78d8']*(len(prices1519)-2) + ['#00ba38']\n\n# 4-room\nprices1519_4room = prices[(prices['flat_type'].isin(['4 ROOM'])) & (prices['year'].isin([2015,2016,2017,2018,2019]))].groupby(['town'], as_index=False).agg({'real_price': 'median'}).sort_values('real_price', ascending=True).reset_index(drop=True)\nprices1519_4room['real_price'] = round(prices1519_4room['real_price']\/1000)\nprices1519_4room['color'] = ['#f8766d','#f8766d'] + ['#3c78d8']*(len(prices1519_4room)-3) + ['#00ba38']\n\n## 1997 vs 2019\n# all room type\nprices9719 = prices[prices['year'].isin([1997,2019])].groupby(['town','year'], as_index=False).agg({'real_price': 'median'})\nprices9719['change'] = prices9719.groupby('town')['real_price'].apply(lambda x: x.pct_change()*100)\nprices9719 = prices9719[prices9719['change'].notnull()] \nprices9719 = prices9719.sort_values('change', ascending=True).reset_index(drop=True).reset_index()\nprices9719['color'] = prices9719['change'].apply(lambda x: '#00ba38' if x > 0 else '#f8766d')\n\n# 4-room\nprices9719_4room = prices[(prices['flat_type'].isin(['4 ROOM']) & prices['year'].isin([1997,2019]))].groupby(['town','year'], as_index=False).agg({'real_price': 'median'})\nprices9719_4room['change'] = prices9719_4room.groupby('town')['real_price'].apply(lambda x: x.pct_change()*100)\nprices9719_4room = prices9719_4room[prices9719_4room.change.notnull()]\nprices9719_4room = prices9719_4room.sort_values('change', ascending=True).reset_index(drop=True).reset_index()\nprices9719_4room['color'] = prices9719_4room['change'].apply(lambda x: '#00ba38' if x > 0 else '#f8766d')\n\n## 2018 vs 2019\n# all room type\nprices1819 = prices[prices['year'].isin([2018,2019])].groupby(['town','year'], as_index=False).agg({'real_price': 'median'})\nprices1819['change'] = prices1819.groupby('town')['real_price'].apply(lambda x: x.pct_change()*100)\nprices1819 = prices1819[prices1819['change'].notnull()] \nprices1819 = prices1819.sort_values('change', ascending=True).reset_index(drop=True).reset_index()\nprices1819['color'] = prices1819['change'].apply(lambda x: '#00ba38' if x > 0 else '#f8766d')\n\n# 4-room\nprices1819_4room = prices[(prices['flat_type'].isin(['4 ROOM']) & prices['year'].isin([2018,2019]))].groupby(['town','year'], as_index=False).agg({'real_price': 'median'})\nprices1819_4room['change'] = prices1819_4room.groupby('town')['real_price'].apply(lambda x: x.pct_change()*100)\nprices1819_4room = prices1819_4room[prices1819_4room.change.notnull()]\nprices1819_4room = prices1819_4room.sort_values('change', ascending=True).reset_index(drop=True).reset_index()\nprices1819_4room['color'] = prices1819_4room['change'].apply(lambda x: '#00ba38' if x > 0 else '#f8766d')\n","67c14699":"# Function for lollipop charts\ndef loll_plot(df, x, y, subtitle, xlabel, xlim):\n    plt.rc('axes', axisbelow=True)\n    plt.grid(linestyle='--', alpha=0.4)\n    plt.hlines(y=df.index, xmin=0, xmax=df[x], color=df.color, linewidth=1)\n    plt.scatter(df[x], df.index, color=df.color, s=300)\n    for i, txt in enumerate(df[x]):\n        plt.annotate(str(round(txt)), (txt, i), color='white', fontsize=9, ha='center', va='center')\n    plt.annotate(subtitle, xy=(1, 0), xycoords='axes fraction', fontsize=20,\n                    xytext=(-5, 5), textcoords='offset points',\n                    ha='right', va='bottom')\n    plt.yticks(df.index, df[y]); plt.xticks(fontsize=12); plt.xlim(xlim)\n    plt.xlabel(xlabel, fontsize=14)\n\n","ccd1e301":"fig = plt.figure(figsize=(12,7))\n\nax1 = plt.subplot(121)\nloll_plot(prices1519, 'real_price', 'town', 'All Room Types', 'Resale Price (SGD)', [50,800])\nax1.set_xticklabels(['{:,.0f}'.format(x) + 'K' for x in ax1.get_xticks()])\nax1.yaxis.set_ticks_position('none') \n\nax2 = plt.subplot(122)\nloll_plot(prices1519_4room, 'real_price', 'town', '4-Room', 'Resale Price (SGD)', [50,800])\nax2.set_xticklabels(['{:,.0f}'.format(x) + 'K' for x in ax2.get_xticks()])\nax2.yaxis.set_ticks_position('none') \n\nfig.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\nplt.suptitle('2015 to 2019, Median Price of Flats', fontsize=16)\nplt.show()","b6b3106e":"fig = plt.figure(figsize=(12,7))\n\nax1 = plt.subplot(121)\nloll_plot(prices9719, 'change', 'town', 'All Room Types', 'Percent Change (%)', [-40,125])\n\nax2 = plt.subplot(122)\nloll_plot(prices9719_4room, 'change', 'town', '4-Room', 'Percent Change (%)', [-40,125])\n\nfig.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\nplt.suptitle('1997 vs 2019, Median Price of Flats', fontsize=16)\nplt.show()","7d79c39b":"fig = plt.figure(figsize=(12,7))\n\nax1 = plt.subplot(121)\nloll_plot(prices1819, 'change', 'town', 'All Room Types', 'Percent Change (%)', [-30,20])\n\nax2 = plt.subplot(122)\nloll_plot(prices1819_4room, 'change', 'town', '4-Room', 'Percent Change (%)', [-30,20])\n\nfig.tight_layout(pad=0.5, rect=[0, 0, 0.9, 0.9])\nplt.suptitle('2018 vs 2019, Median Price of Flats', fontsize=16)\nplt.show()","789703a2":"prices[prices['year'].isin([2018,2019])].groupby('town')['lease_commence_date'].median().sort_values()","99b243b9":"storey.head()","87a507d3":"storey","390ead33":"fig = plt.figure(figsize=(12,4))\n\n# Storey Prices\nax1 = plt.subplot(121)\nstorey = prices.groupby('storey_range')['real_price'].median().reset_index().sort_values(by='storey_range')\nstorey['storey_rank'] = storey['storey_range'].astype('category').cat.codes # label encode\na=sns.scatterplot(x=storey['storey_rank'], y=storey['real_price'], size=storey['storey_rank'].astype('int')*30, color='#00994d', edgecolors='w', alpha=0.5, legend=False, ax=ax1)\nylabels = ['{:,.0f}'.format(x) + 'K' for x in a.get_yticks()\/1000]\nax1.set_yticklabels(ylabels)\nax1.set_xticklabels(pd.Series(['']).append(storey.iloc[[0,5,10,15,20,24],0]))\nax1.set_ylim([280000,1100000]), ax1.set_ylabel('Resale Price SGD ($)', size=15), ax1.set_xlabel('Storey', size=15)\nax1.set_title('All Years', size=15)\n\n# Floor Area Prices\nax2 = plt.subplot(122)\nstorey2 = prices[prices['year'].isin([2015,2016,2017,2018,2019])].groupby('storey_range')['real_price'].median().reset_index().sort_values(by='storey_range')\nstorey2['storey_rank'] = storey2['storey_range'].astype('category').cat.codes\n\n# Bubble chart\nb=sns.scatterplot(x=storey2['storey_rank'], y=storey2['real_price'], size=storey2['storey_rank'].astype('int')*30, color='#00994d', edgecolors='w', alpha=0.5, legend=False, ax=ax2)\nylabels = ['{:,.0f}'.format(x) + 'K' for x in ax2.get_yticks()\/1000]\nax2.set_yticklabels(ylabels); ax2.set_ylabel('')\nax2.set_xticks([0,4,8,12,16])\nax2.set_xticklabels(storey2.iloc[[0,4,8,12,16],0])\nax2.set_ylim([280000,1100000]), ax2.set_xlabel('Storey', size=15)\nax2.set_title('2015 to 2019', size=15)\n\nplt.show()\n","b29c96f4":"# Floor Area Prices\narea = prices[prices['year'].isin([2015,2016,2017,2018,2019])]\np=sns.regplot(x='floor_area_sqm', y='real_price', data=area, scatter_kws={\"s\": 1, 'alpha':0.5})\nylabels = ['{:,.0f}'.format(x) + 'K' for x in p.get_yticks()\/1000]\np.set_yticklabels(ylabels)\np.set_ylabel('Resale Price SGD ($)', size=15)\np.set_xlabel('Floor Area (Square Meters)', size=15)\nplt.show()","8b95c360":"display(area[area['floor_area_sqm'] > 200])","40b5509e":"import re\n\n# Block Number Prices\nget_num = lambda x: int(re.findall(\"\\d+\", x)[0])\nprices['blocknum'] = prices['block'].apply(get_num) # get only digits from block number\ntmp = prices[prices['blocknum'] > 99] # get only blocks that use 3-digit numbering system\ntmp = tmp.groupby('blocknum')['real_price'].median().reset_index()\n\n# Scatterplots\nfig = plt.figure(figsize=(12,4))\n\nax1 = plt.subplot(121)\na=sns.scatterplot(x=tmp['blocknum'].apply(lambda x: int(str(x)[0])), y=tmp['real_price'], color='#ff9933', edgecolors='w', alpha=0.9)\nylabels = ['{:,.0f}'.format(x) + 'K' for x in a.get_yticks()\/1000]\nax1.set_yticklabels(ylabels)\nax1.set_ylabel('Resale Price SGD ($)', size=15), ax1.set_xlabel('Neighbourhood Number', size=15)\n\nax2 = plt.subplot(122)\nb=sns.scatterplot(x=tmp['blocknum'].apply(lambda x: int(str(x)[1:])), y=tmp['real_price'], edgecolors='w', alpha=0.9)\nax2.set_yticklabels(ylabels)\nax2.set_ylabel('', size=15)\nax2.set_xlabel('Block Number', size=15)\n\nplt.show()","1701f38f":"# Violin plots for price distribution of each flat model\n\nfig = plt.figure(figsize=(12,7))\np=sns.violinplot(x='flat_model', y='real_price', data=prices, width=1,\n                order=prices.groupby('flat_model')['real_price'].median().sort_values().reset_index()['flat_model'].tolist())\np.set_xticklabels(p.get_xticklabels(), rotation=30, ha='right'), p.set_xlabel('Flat Models', size=15)\nylabels = ['{:,.0f}'.format(x) + 'K' for x in p.get_yticks()\/1000]\np.set_yticklabels(ylabels)\np.set_ylabel('Resale Price SGD ($)', size=15)\nplt.show()\n","13e0fbb0":"# ridgeline plot\n\n# tmp = prices.set_index('flat_model')\n# tmp = tmp.loc[prices.groupby('flat_model')['real_price'].median().sort_values().reset_index()['flat_model'].tolist()].reset_index().groupby(\"flat_model\", sort=False)\n# fig, axes = joypy.joyplot(tmp, by=\"flat_model\", column=\"real_price\",figsize=(12,5),\n#              linewidth=0.05,overlap=1.5,alpha=0.8,colormap=plt.cm.get_cmap('tab20',16))\n# axes[-1].set_xlim([-50000,1400000])\n# axes[-1].set_xticklabels(['0', '200k', '400k', '600k', '800k', '1000k', '1200k', '1400k']) \n# plt.xlabel('Resale Price SGD ($)', fontsize=14)\n# fig.show()","1d06b3d7":"# Boxplot for each year of lease commence date\n\nfig = plt.figure(figsize=(7,9))\np=sns.boxplot(y='lease_commence_date', x='real_price', data=prices, width=1, orient='h', flierprops = dict(markerfacecolor = 'red', markersize = 0.1, linestyle='none'), linewidth=0.4)\np.set_xlabel('Resale Price SGD ($)', size=15), p.set_ylabel('Lease Commence Year', size=15)\nxlabels = ['{:,.0f}'.format(x) + 'K' for x in p.get_xticks()\/1000]\np.set_xticklabels(xlabels)\np.set_title('Resale Price By Lease Commence Year', size=15)\nplt.show()","6f26b0bc":"tmp = prices[prices['year'].isin([2015,2016,2017,2018,2019])]\nfig, axes = joypy.joyplot(tmp, by=\"lease_commence_date\", column=\"real_price\",figsize=(6,10),\n             linewidth=1,overlap=5,alpha=0.8,colormap=plt.cm.get_cmap('tab20',16))\naxes[-1].set_xlim([-50000,1400000])\naxes[-1].set_xticklabels(['0', '200k', '400k', '600k', '800k', '1000k', '1200k', '1400k']) \nplt.xlabel('Resale Price SGD ($)', fontsize=14)\nfig.show()","7d92dd7c":"flat_amenities = pd.read_csv('..\/input\/flat-amenities\/flat_amenities.csv')\n\n# merge amenities data to flat data\nprices1519 = prices[prices['year'].isin([2015,2016,2017,2018,2019])]\nprices1519['flat'] = prices['block'] + ' ' + prices['street_name']\nprices1519 = prices1519.merge(flat_amenities, on='flat', how='left')\n\n# reduce number of class of town to regions\nd_region = {'ANG MO KIO':'North East', 'BEDOK':'East', 'BISHAN':'Central', 'BUKIT BATOK':'West', 'BUKIT MERAH':'Central',\n       'BUKIT PANJANG':'West', 'BUKIT TIMAH':'Central', 'CENTRAL AREA':'Central', 'CHOA CHU KANG':'West',\n       'CLEMENTI':'West', 'GEYLANG':'Central', 'HOUGANG':'North East', 'JURONG EAST':'West', 'JURONG WEST':'West',\n       'KALLANG\/WHAMPOA':'Central', 'MARINE PARADE':'Central', 'PASIR RIS':'East', 'PUNGGOL':'North East',\n       'QUEENSTOWN':'Central', 'SEMBAWANG':'North', 'SENGKANG':'North East', 'SERANGOON':'North East', 'TAMPINES':'East',\n       'TOA PAYOH':'Central', 'WOODLANDS':'North', 'YISHUN':'North'}\nprices1519['region'] = prices1519['town'].map(d_region)\ncolors = {'North East':'Purple', 'East':'Green', 'Central':'Brown', 'West':'Red', 'North':'Orange'}","e17b27a2":"# get median info of each town\ntmp = prices1519.groupby('town')[['dist_dhoby','school_dist','num_school_2km','hawker_dist','num_hawker_2km','park_dist','num_park_2km','mall_dist','num_mall_2km','mrt_dist','num_mrt_2km','supermarket_dist','num_supermarket_2km','real_price']].median().reset_index()\ntmp['region'] = tmp['town'].map(d_region)\n\n# Scatterplot with names of towns\nfig, ax = plt.subplots(figsize=(8,5))\ngrouped = tmp.groupby('region')\nfor key, group in grouped:\n    group.plot(ax=ax, kind='scatter', x='dist_dhoby', y='real_price', label=key, color=colors[key], s=60)\nb, a = np.polyfit(tmp['dist_dhoby'], tmp['real_price'], 1)\nax.plot(tmp['dist_dhoby'], a + b* tmp['dist_dhoby'], '-')  \nax.set_xlim([0,20]), ax.set_xlabel('Distance from Dhoby Ghaut MRT (km)', size=15)\nylabels = ['{:,.0f}'.format(x) + 'K' for x in ax.get_yticks()\/1000]\nax.set_yticklabels(ylabels), ax.set_ylabel('Resale Price SGD ($)', size=15)\nfor i, txt in enumerate(tmp['town']):\n    ax.annotate(txt, (tmp['dist_dhoby'][i]+0.3, tmp['real_price'][i]), size=8, alpha=0.9)\n\nplt.show()","486c7aa6":"prices1519.groupby('region')['real_price'].median()","89107c47":"# scatterplot for median price of each town against nearest distance from each amenity\n\np=sns.pairplot(tmp, x_vars=[\"school_dist\", \"hawker_dist\", \"park_dist\", \"mall_dist\", \"mrt_dist\", \"supermarket_dist\"], y_vars=[\"real_price\"], height=3, aspect=1, kind=\"reg\", plot_kws=dict(scatter_kws=dict(s=40)))\naxes=p.axes\nylabels = ['{:,.0f}'.format(x) + 'K' for x in axes[0,0].get_yticks()\/1000]\naxes[0,0].set_yticklabels(ylabels), axes[0,0].set_ylabel('Resale Price SGD ($)', size=10)\naxes[0,0].set_xlabel('Distance From School (km)', size=10), axes[0,1].set_xlabel('Distance From Hawker (km)', size=10)\naxes[0,2].set_xlabel('Distance From Park (km)', size=10), axes[0,3].set_xlabel('Distance From Mall (km)', size=10)\naxes[0,4].set_xlabel('Distance From MRT (km)', size=10), axes[0,5].set_xlabel('Distance From Supermarket (km)', size=10)\nplt.suptitle('Resale Price (Median of Each Town) VS Distance from Nearest Amenities (Median of Each Town)')\nplt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\nplt.show()","8b0fe86e":"# scatterplot for price of each flat against nearest distance from each amenity\n\np=sns.pairplot(prices1519[prices1519['school_dist']<3], x_vars=[\"school_dist\", \"hawker_dist\", \"park_dist\", \"mall_dist\", \"mrt_dist\", \"supermarket_dist\"], y_vars=[\"real_price\"], height=3, aspect=1, kind=\"reg\", plot_kws=dict(scatter_kws=dict(s=0.5,alpha=0.3), line_kws=dict(color='#ff7f0e'))) # remove outliers (>3km)\naxes=p.axes\nylabels = ['{:,.0f}'.format(x) + 'K' for x in axes[0,0].get_yticks()\/1000]\naxes[0,0].set_yticklabels(ylabels), axes[0,0].set_ylabel('Resale Price SGD ($)', size=10)\naxes[0,0].set_xlabel('Distance From School (km)', size=10), axes[0,1].set_xlabel('Distance From Hawker (km)', size=10)\naxes[0,2].set_xlabel('Distance From Park (km)', size=10), axes[0,3].set_xlabel('Distance From Mall (km)', size=10)\naxes[0,4].set_xlabel('Distance From MRT (km)', size=10), axes[0,5].set_xlabel('Distance From Supermarket (km)', size=10)\nplt.suptitle('Resale Price VS Distance from Nearest Amenities')\nplt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\nplt.show()","3a3e66f9":"# scatterplot for median price of each town against number of amenities\n\np=sns.pairplot(tmp, x_vars=[\"num_school_2km\", \"num_hawker_2km\", \"num_park_2km\", \"num_mall_2km\", \"num_mrt_2km\", \"num_supermarket_2km\"], y_vars=[\"real_price\"], height=3, aspect=1, kind=\"reg\", plot_kws=dict(scatter_kws=dict(s=40)))\naxes=p.axes\nylabels = ['{:,.0f}'.format(x) + 'K' for x in axes[0,0].get_yticks()\/1000]\naxes[0,0].set_yticklabels(ylabels), axes[0,0].set_ylabel('Resale Price SGD ($)', size=10)\naxes[0,0].set_xlabel('Number of Schools', size=10), axes[0,1].set_xlabel('Number of Hawkers', size=10)\naxes[0,2].set_xlabel('Number of Parks', size=10), axes[0,3].set_xlabel('Number of Malls', size=10)\naxes[0,4].set_xlabel('Number of MRTs', size=10), axes[0,5].set_xlabel('Number of Supermarkets', size=10)\nplt.suptitle('Resale Price (Median of Each Town) VS Number of Amenities in 2km Radius (Median of Each Town)')\nplt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\nplt.show()","9a1463c7":"# scatterplot for price of each flat against number of amenities\n\np=sns.pairplot(prices1519, x_vars=[\"num_school_2km\", \"num_hawker_2km\", \"num_park_2km\", \"num_mall_2km\", \"num_mrt_2km\", \"num_supermarket_2km\"], y_vars=[\"real_price\"], height=3, aspect=1, kind=\"reg\", plot_kws=dict(scatter_kws=dict(s=0.5,alpha=0.3), line_kws=dict(color='#ff7f0e')))\naxes=p.axes\nylabels = ['{:,.0f}'.format(x) + 'K' for x in axes[0,0].get_yticks()\/1000]\naxes[0,0].set_yticklabels(ylabels), axes[0,0].set_ylabel('Resale Price SGD ($)', size=10)\naxes[0,0].set_xlabel('Number of Schools', size=10), axes[0,1].set_xlabel('Number of Hawkers', size=10)\naxes[0,2].set_xlabel('Number of Parks', size=10), axes[0,3].set_xlabel('Number of Malls', size=10)\naxes[0,4].set_xlabel('Number of MRTs', size=10), axes[0,5].set_xlabel('Number of Supermarkets', size=10)\nplt.suptitle('Resale Price VS Number of Amenities in 2km Radius')\nplt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\nplt.show()","4e8ea619":"# clear unused variables\ndel(price1999, price2012, price2014, price2016, price2017, prices1819, prices1819_4room, prices9719, prices9719_4room,\n    storey, storey2, tmp, xlabels, ylabels, p, grouped, flattype, flat_amenities, cpi, ax, ax1, ax2, area)\nimport gc\ngc.collect()","eef916fc":"df = prices1519[['town', 'flat_type', 'storey_range', 'floor_area_sqm', 'flat_model', 'lease_commence_date', 'year', 'school_dist', 'num_school_2km', 'hawker_dist', 'num_hawker_2km', 'park_dist', 'num_park_2km', 'mall_dist', 'num_mall_2km', 'mrt_dist', 'num_mrt_2km', 'supermarket_dist', 'num_supermarket_2km', 'dist_dhoby', 'region', 'real_price']]\n\n# function for replacing NAs with median of the town\ndef replace_NA_median(df, columns):\n    for c in columns:      \n        df[c] = df.groupby(\"town\").transform(lambda x: x.fillna(x.median()))[c]\n    return df\n\ndf = replace_NA_median(df, ['school_dist', 'num_school_2km', 'hawker_dist',\n       'num_hawker_2km', 'park_dist', 'num_park_2km', 'mall_dist',\n       'num_mall_2km', 'mrt_dist', 'num_mrt_2km', 'supermarket_dist',\n       'num_supermarket_2km', 'dist_dhoby'])\ndf.info()","3fa39ef5":"# Correlation heatmap\nfig = plt.figure(figsize=(10,10))\nax = sns.heatmap(df.select_dtypes(include=['int64','float64']).corr(), annot = True, fmt='.2g', \n    vmin=-1, vmax=1, center= 0, cmap= 'coolwarm_r', linecolor='black', linewidth=1, annot_kws={\"size\": 7})\n#ax.set_ylim(0 ,5)\nplt.xticks(rotation=45, ha='right')\nplt.title('Correlation Heatmap')\nfig.show()","370cf212":"# Multicollinearity\n# Import library for VIF\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['tolerance'] = 1\/vif.VIF\n    vif['meanVIF'] = vif.VIF.mean()\n\n    return(vif)\n\ncalc_vif(df.drop('real_price',axis=1).select_dtypes(include=['int64','float64']))","08013337":"calc_vif(df.drop(['real_price','num_supermarket_2km','year','num_school_2km','dist_dhoby'],axis=1).select_dtypes(include=['int64','float64']))","2c001fc1":"# drop columns\nlr_df = df.drop(['num_supermarket_2km','year','num_school_2km','dist_dhoby'], axis=1)","52ac2118":"# Plot distribution for each continuous variable\nlr_df.hist(bins=50, figsize=(15, 10), grid=False, edgecolor='black')\nplt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\nplt.show()","0a47f169":"# plot qqplot before and after log transformation\n\nfrom statsmodels.api import qqplot\nfig, ((ax1,ax2), (ax3,ax4)) = plt.subplots(2,2,figsize=(5,5))\n\nax1.hist(lr_df['real_price'], bins=50, edgecolor='black')\nqqplot(lr_df['real_price'], line='s', ax=ax2)\nax3.hist(np.log(lr_df['real_price']), bins=50, edgecolor='black')\nqqplot(np.log(lr_df['real_price']), line='s', ax=ax4)\nplt.suptitle('Real Price Normality Before (Top) & After (Bottom) Logging')\nplt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\nplt.show()","9bf744ff":"# Frequency plots for catergorical features\nfig = plt.figure(figsize=(30,5))\nfor count, col in enumerate(df.select_dtypes(include=['category','object']).columns):\n    fig.add_subplot(1,5,count+1)\n    df[col].value_counts().plot.barh()\n    plt.title(col)\n    plt.yticks(rotation=45)\n    plt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])","a704afde":"# label encode storeys\ndf = df.sort_values(by='storey_range')\ndf['storey_range'] = df['storey_range'].astype('category').cat.codes # label encode\nlr_df = lr_df.sort_values(by='storey_range')\nlr_df['storey_range'] = lr_df['storey_range'].astype('category').cat.codes # label encode\n\n# remove flat types with very few cases\ndf = df[~df['flat_type'].isin(['MULTI GENERATION', '1 ROOM'])]\nlr_df = lr_df[~lr_df['flat_type'].isin(['MULTI GENERATION', '1 ROOM'])]\n\n# Re-categorize flat model to reduce num classes\nreplace_values = {'Executive Maisonette':'Maisonette', 'Terrace':'Special', 'Adjoined flat':'Special', \n                    'Type S1S2':'Special', 'DBSS':'Special', 'Model A2':'Model A', 'Premium Apartment':'Apartment', 'Improved':'Standard', 'Simplified':'Model A', '2-room':'Standard'}\ndf = df.replace({'flat_model': replace_values})\nlr_df = lr_df.replace({'flat_model': replace_values})\n\n# Label encode flat type\nreplace_values = {'2 ROOM':0, '3 ROOM':1, '4 ROOM':2, '5 ROOM':3, 'EXECUTIVE':4}\ndf = df.replace({'flat_type': replace_values})\nlr_df = lr_df.replace({'flat_type': replace_values})\n\ndf = df.reset_index(drop=True)\ndisplay(df['flat_model'].value_counts())\nlr_df = lr_df.reset_index(drop=True)\ndisplay(lr_df['flat_model'].value_counts())","ea29813e":"display(lr_df.head())","3413977a":"## dummy encoding\ndf = pd.get_dummies(df, columns=['region'], prefix=['region'], drop_first=True) # central is baseline\ndf = pd.get_dummies(df, columns=['flat_model'], prefix=['model'])\ndf= df.drop('model_Standard',axis=1) # remove standard, setting it as the baseline\nlr_df = pd.get_dummies(lr_df, columns=['region'], prefix=['region'], drop_first=True) # central is baseline\nlr_df = pd.get_dummies(lr_df, columns=['flat_model'], prefix=['model'])\nlr_df= lr_df.drop('model_Standard',axis=1) # remove standard, setting it as the baseline\n","068c3119":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# fit to continuous columns and transform\nscaled_columns = ['floor_area_sqm','lease_commence_date','school_dist','hawker_dist','num_hawker_2km','park_dist',\n                    'num_park_2km', 'mall_dist', 'num_mall_2km', 'mrt_dist', 'num_mrt_2km', 'supermarket_dist']\nscaler.fit(lr_df[scaled_columns])\nscaled_columns = pd.DataFrame(scaler.transform(lr_df[scaled_columns]), index=lr_df.index, columns=scaled_columns)\n\n# separate unscaled features\nunscaled_columns = lr_df.drop(scaled_columns, axis=1)\n\n# concatenate scaled and unscaled features\nlr_df = pd.concat([scaled_columns,unscaled_columns], axis=1)\n","291bb936":"display(lr_df.head())","f73b207c":"from yellowbrick.regressor import CooksDistance\n\nlr_y = lr_df[['real_price']]\nlr_X = lr_df.drop(['real_price','town'], axis=1)\n\nyy = np.log(lr_y)['real_price']\nXX = lr_X.values\n\nvisualizer = CooksDistance()\nvisualizer.fit(XX, yy)\n#visualizer.show()\nplt.show()","a79fa4c4":"from yellowbrick.regressor import ResidualsPlot\nfrom sklearn.linear_model import LinearRegression\n\n# visualize residuals before outlier removal\nmodel = LinearRegression()\nvisualizer_residuals = ResidualsPlot(model)\nvisualizer_residuals.fit(XX, yy)\n#visualizer_residuals.show()\nplt.show()","a9b73fff":"# remove outliers\ni_less_influential = (visualizer.distance_ <= visualizer.influence_threshold_)\nX_li, y_li = XX[i_less_influential], yy[i_less_influential]\nlr_X, lr_y = lr_X[i_less_influential], lr_y[i_less_influential]\n\n# visualize residuals after outliers removal\nmodel = LinearRegression()\nvisualizer_residuals = ResidualsPlot(model)\nvisualizer_residuals.fit(X_li, y_li)\n#visualizer_residuals.show()\nplt.show()","8bd609d0":"from sklearn.linear_model import LinearRegression\n\n# sklearn method, which doesn't give much additional info\n\nlin_reg = LinearRegression()\nlin_reg.fit(lr_X, np.log(lr_y))\n\nprint(f'Coefficients: {lin_reg.coef_}')\nprint(f'Intercept: {lin_reg.intercept_}')\nprint(f'R^2 score: {lin_reg.score(lr_X, np.log(lr_y))}')","cbd611d0":"# statsmodel method, which gives more info\nimport statsmodels.api as sm\n# alternate way using statistical formula, which does not require dummy coding manually\n# https:\/\/stackoverflow.com\/questions\/50733014\/linear-regression-with-dummy-categorical-variables\n# https:\/\/stackoverflow.com\/questions\/34007308\/linear-regression-analysis-with-string-categorical-features-variables\n\nX_constant = sm.add_constant(lr_X)\nlin_reg = sm.OLS(np.log(lr_y),X_constant).fit()\nlin_reg.summary()","cd626b5b":"# scatterplot of y (observed) and yhat (predicted)\n\nplt.style.use('default')\nfig = plt.figure(figsize=(5,3))\nax = sns.scatterplot(x=np.log(lr_y)['real_price'], y=lin_reg.predict(), edgecolors='w', alpha=0.9, s=8)\nax.set_xlabel('Observed')#, ax.set_xticklabels(['{:,.0f}'.format(x) + 'K' for x in ax.get_xticks()\/1000])\nax.set_ylabel('Predicted')#, ax.set_yticklabels(['{:,.0f}'.format(x) + 'K' for x in ax.get_yticks()\/1000])\nax.annotate('Adjusted R\\u00b2: ' + str(format(round(lin_reg.rsquared_adj,2),'.2f')), xy=(0, 1), xytext=(25, -25),\n    xycoords='axes fraction', textcoords='offset points', fontsize=12)\nplt.show()","97988dbe":"# Homoscedasticity and Normality of Residuals\npred = lin_reg.predict()\nresids = lin_reg.resid\nresids_studentized = lin_reg.get_influence().resid_studentized_internal\n\nfig = plt.figure(figsize=(10,3))\n\nax1 = plt.subplot(121)\nsns.scatterplot(x=pred, y=resids_studentized, edgecolors='w', alpha=0.9, s=8)\nax1.set_xlabel('Predicted Values')\nax1.set_ylabel('Studentized Residuals')\n\nax2 = plt.subplot(122)\nsns.distplot(resids_studentized, norm_hist=True, hist_kws=dict(edgecolor='w'))\nax2.set_xlabel('Studentized Residual')\n\nplt.show()","9b4d58d8":"# Feature Importance\n\nlr_results = pd.read_html(lin_reg.summary().tables[1].as_html(),header=0,index_col=0)[0]\ncoefs = lr_results[['coef']][1:].reset_index().rename(columns={'index':'feature'})\ncoefs['feature_importance'] = np.abs(coefs['coef'])\ncoefs = coefs.sort_values('feature_importance').reset_index(drop=True)\ncoefs['color'] = coefs['coef'].apply(lambda x: '#66ff8c' if x>0 else '#ff8c66')\ncoefs.plot.barh(x='feature',y='feature_importance',color=coefs['color'],figsize=(4,5))\ncolors = {'Positive':'#66ff8c', 'Negative':'#ff8c66'}         \nlabels = list(colors.keys())\nhandles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\nlegend = plt.legend(handles, labels, title='Relationship', fontsize = '15')\nplt.setp(legend.get_title(),fontsize='17')\nplt.xlabel('Standardized Coefficients', size=15), plt.ylabel('Features', size=15)\nplt.ylim([-1,23])\nplt.title('Linear Regression Feature Importance', size=15)\nplt.show()","73cc9571":"from sklearn.model_selection import train_test_split\n\n# Train Test Split\ny = df[['real_price']]\nX = df.drop(['real_price','town', 'year'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1, shuffle=True, random_state=0)\nprint('Shape of X_train:', X_train.shape)\nprint('Shape of X_test:', X_test.shape)\nprint('Shape of y_train:', y_train.shape)\nprint('Shape of y_test:', y_test.shape)","0cf5f2de":"X_test.columns","aec1f6ff":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom scipy.stats import spearmanr, pearsonr\n\n# Validation using out-of-bag method\nrf = RandomForestRegressor(n_estimators=100, oob_score=True, random_state=0)\nrf.fit(X_train, y_train)\npredicted_train = rf.predict(X_train)\n\nprint(f'Out-of-bag R\\u00b2 score estimate: {rf.oob_score_:>5.3}')","4d1bb9ef":"# predict and get evaluation metrics on test set\npredicted_test = rf.predict(X_test)\noob_test_score = r2_score(y_test['real_price'], predicted_test)\nspearman = spearmanr(y_test['real_price'], predicted_test)\npearson = pearsonr(y_test['real_price'], predicted_test)\noob_mae = mean_absolute_error(y_test['real_price'], predicted_test)\n\nprint(f'Test data R\\u00b2 score: {oob_test_score:>5.3}')\nprint(f'Test data Spearman correlation: {spearman[0]:.3}')\nprint(f'Test data Pearson correlation: {pearson[0]:.3}')\nprint(f'Test data Mean Absolute Error: {round(oob_mae)}')","2f85fb88":"from sklearn.model_selection import GridSearchCV\n\n# validation by k-fold cross validation with grid search for best hyperparameters\n# hyperparameter values shown below are the tuned final values\nparam_grid = {\n    'max_features': ['auto'], # max number of features considered for splitting a node\n    'max_depth': [20], # max number of levels in each decision tree\n    'min_samples_split': [15], # min number of data points placed in a node before the node is split\n    'min_samples_leaf': [2]} # min number of data points allowed in a leaf node\nrfr =GridSearchCV(RandomForestRegressor(n_estimators = 500, n_jobs=-1, random_state=0),\n                        param_grid, cv=10, scoring='r2', return_train_score=True)\nrfr.fit(X_train,y_train)\nprint(\"Best parameters set found on Cross Validation:\\n\\n\", rfr.best_params_)\nprint(\"\\nCross Validation R\\u00b2 score:\\n\\n\", rfr.best_score_.round(3))","d01f8946":"# predict and get evaluation metrics for test set\n\ncv_predicted_test = rfr.predict(X_test)\n\ncv_test_score = r2_score(y_test['real_price'], cv_predicted_test)\nspearman = spearmanr(y_test['real_price'], cv_predicted_test)\npearson = pearsonr(y_test['real_price'], cv_predicted_test)\ncv_mae = mean_absolute_error(y_test['real_price'], cv_predicted_test)\n\nprint(f'Test data R\\u00b2 score: {cv_test_score:>5.3}')\nprint(f'Test data Spearman correlation: {spearman[0]:.3}')\nprint(f'Test data Pearson correlation: {pearson[0]:.3}')\nprint(f'Test data Mean Absolute Error: {round(cv_mae)}')","bb13d577":"# scatterplots of y (observed) and yhat (predicted)\n\nfig = plt.figure(figsize=(13,4))\n\nax1 = plt.subplot(121)\nax1 = sns.scatterplot(x=y_test['real_price'], y=predicted_test, edgecolors='w', alpha=0.9, s=8)\nax1.set_xlabel('Observed'), ax1.set_xticklabels(['{:,.0f}'.format(x) + 'K' for x in ax1.get_xticks()\/1000])\nax1.set_ylabel('Predicted'), ax1.set_yticklabels(['{:,.0f}'.format(x) + 'K' for x in ax1.get_yticks()\/1000])\nax1.annotate('Test R\\u00b2: ' + str(round(oob_test_score,3)) + '\\nTest MAE: ' + str(round(oob_mae)), xy=(0, 1), xytext=(25, -35),\n    xycoords='axes fraction', textcoords='offset points', fontsize=12)\nax1.set_title('Tuned Using Out-Of-Bag')\n\nax2 = plt.subplot(122)\nax2 = sns.scatterplot(x=y_test['real_price'], y=cv_predicted_test, edgecolors='w', alpha=0.9, s=8)\nax2.set_xlabel('Observed'), ax2.set_xticklabels(['{:,.0f}'.format(x) + 'K' for x in ax2.get_xticks()\/1000])\nax2.set_ylabel('Predicted'), ax2.set_yticklabels(['{:,.0f}'.format(x) + 'K' for x in ax2.get_yticks()\/1000])\nax2.annotate('Test R\\u00b2: ' + str(round(cv_test_score,3)) + '\\nTest MAE: ' + str(round(cv_mae)), xy=(0, 1), xytext=(25, -35),\n    xycoords='axes fraction', textcoords='offset points', fontsize=12)\nax2.set_title('Tuned Using Cross Validation')\nplt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\nplt.show()","6c4de795":"fig = plt.figure(figsize=(14,7))\n\nax1 = plt.subplot(121)\nfeat_imp = pd.DataFrame({'Features': X_train.columns, 'Feature Importance': rf.feature_importances_}).sort_values('Feature Importance', ascending=False)\nsns.barplot(y='Features', x='Feature Importance', data=feat_imp)\n#plt.xticks(rotation=45, ha='right')\nax1.set_title('OOB Feature Importance', size=15)\n\nax2 = plt.subplot(122)\nfeat_imp = pd.DataFrame({'Features': X_train.columns, 'Feature Importance': rfr.best_estimator_.feature_importances_}).sort_values('Feature Importance', ascending=False)\nsns.barplot(y='Features', x='Feature Importance', data=feat_imp)\nax2.set_title('CV Feature Importance', size=15)\n\nplt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\nfig.show()","5c8cc192":"import shap\nshap.initjs()\n\nexplainer = shap.TreeExplainer(rfr.best_estimator_)\nshap_values = explainer.shap_values(X_test.iloc[[16]])\nshap.force_plot(explainer.expected_value[0], shap_values[0], X_test.iloc[[16]])","70339eab":"explainer = shap.TreeExplainer(rfr.best_estimator_)\nshap_values = explainer.shap_values(X_test.iloc[[5]])\nshap.force_plot(explainer.expected_value[0], shap_values[0], X_test.iloc[[5]])","5842718a":"explainer = shap.TreeExplainer(rfr.best_estimator_)\nshap_values = explainer.shap_values(X_test.iloc[[1]])\nshap.force_plot(explainer.expected_value[0], shap_values[0], X_test.iloc[[1]])","8ac94f53":"explainer = shap.TreeExplainer(rfr.best_estimator_)\nshap_values = explainer.shap_values(X_test.iloc[[100]])\nshap.force_plot(explainer.expected_value[0], shap_values[0], X_test.iloc[[100]])","6c1b3a04":"explainer = shap.TreeExplainer(rfr.best_estimator_)\nshap_values = explainer.shap_values(X_test.iloc[[172]])\nshap.force_plot(explainer.expected_value[0], shap_values[0], X_test.iloc[[172]])","79a97ff7":"explainer = shap.TreeExplainer(rfr.best_estimator_)\nshap_values = explainer.shap_values(X_test.iloc[[10084]])\nshap.force_plot(explainer.expected_value[0], shap_values[0], X_test.iloc[[10084]])","1498ec8e":"print(\"Flat Type Encoding = 2 ROOM:0, 3 ROOM:1, 4 ROOM:2, 5 ROOM:3, EXECUTIVE:4\")","2429f4ae":"# Save model\n\n# import pickle\nimport joblib\n\n# pickle.dump(rfr.best_estimator_, open('hdb_prices_rf_model.sav', 'wb'))\n#joblib.dump(rfr.best_estimator_, 'rf_compressed.pkl', compress=3) # smaller size","1ce6c939":"The floor area outliers mostly belong to special HDBs that are larger than the standard ones. So they might not be outliers from a multivariate perspective.","637e326b":"There are not many 1 Room, 2 Room and Multi Generation flats, so they will be removed for looking at flat types.","78fc483e":"The selected features are able to account for 90% of the variance in HDB resale prices.","3dd85e71":"Punggol has its own LRT and Central Area has lots of stations and malls nearby.","ecc22e9a":"The special models like the Type S1S2 (The Pinnacle@Duxton) and Terrace tend to fetch higher prices while the older models from the 1900s tend to go lower.","4f77ff36":"<a id='RF'><\/a>\n# 6. Random Forest","2afd7dd4":"<a id='Inflation'><\/a>\n### 2.1. Inflation Adjustment Using CPI","0742a10d":"<a id='Multicollinearity'><\/a>\n## 4.2. Multicollinearity\n\nMulticollinearity occurs when two or more independent variables are highly correlated with one another in a regression model. Multicollinearity can be a problem in a regression model because we would not be able to distinguish between the individual effects of the independent variables on the dependent variable.\n\nIf our goal is one of classification or prediction and not to look at the importance or contribution of each feature, we do not need to deal with multicollinearity as it does not affect the overall prediction or goodness of fit. It just affects the p-value and the coefficients. But since we are interested in feature importance by looking at the coefficients of the linear regression model, we have to focus on it.\n\n* If the **largest VIF** is greater than 10 then there is cause for concern (Bowerman & O\u2019Connell, 1990; Myers, 1990)\n* If the **average VIF** is substantially greater than 1 then the regression may be biased (Bowerman & O\u2019Connell, 1990).\n* **Tolerance** below 0.1 indicates a serious problem.\n* **Tolerance** below 0.2 indicates a potential problem (Menard, 1995).","48d747c5":"`Region` and `flat_model` were dummy encoded, with **Central** region and **Standard** model selected as the baseline to which other classes are compared to.","cb43e77d":"Surprisingly, this follows a linear relationship, with higher storeys being sold at a higher price.","097c6604":"Random Forest appears to be performing better than linear regression in terms of R square.","d0300611":"<a id='BlockNum'><\/a>\n## 3.5. By Block Number\n\n3 digit system was introduced in the 1970s, with the 1st digit representing a neighbourhood in a town. So for e.g., AMK neighbourhood 1 starts with 101, \nand AMK neighbourhood 2 starts with 201. So first digit was separated from last 2 digits and plotted separately\n","3f16e9e4":"<a id='Model'><\/a>\n## 3.6. By Flat Model","9d36466e":"# Drivers of HDB Resale Price and Prediction<br>\nTeYang Lau<br>\nCreated: 4\/11\/2020<br>\nLast update: 18\/1\/2021<br>\n\n<img src = 'https:\/\/bn1301files.storage.live.com\/y4mQZSEVK9waxO8abyRRTLfnn-MHfTRWx41j7ip1BmXxVSGCVbkN1vEw1kLLjfX6-K_NBNmWq5CtEcmAWPEfewlfl3B886qDfkZk_3Tqzt81grkmjW0n2w7Py6Cg5ImMfPwJHIQwF8iHgnKM0V7H4CIApUZ2OHUNMY2HnNzj-XkoZVBdjKZuwFXakFwSz211_Lx?width=2047&height=1154&cropmode=none' width=900>\n<br><br>\n\n## Github Repo\n\nThe repo for the entire project can be found [here](https:\/\/github.com\/teyang-lau\/HDB_Resale_Prices). Please consider starring it if you enjoy this notebook! \n\n## Project Goals \n\n1. Start a end-to-end project, from scraping data, to cleaning, modelling, and deploying the model\n2. To **identify** the drivers of HDB resale prices in Singapore.\n3. To **scrape** and **engineer** additional features from online public datasets that might also influence resale prices\n4. To **deploy** the model onto a web app, allowing for HDB resale prices prediction for different HDB features\n\n<br>\n\n## About the Data\nThe HDB resale price data was downloaded from [Data.gov.sg](https:\/\/data.gov.sg\/dataset\/resale-flat-prices), containing ~800k resale transactions from 1990 to 2020.\n<br><br>\n\n## What's in this notebook:\n1. [Data Loading](#Data_loading)<br>\n\n2. [Data Cleaning](#Clean)<br>\n    2.1 [Inflation Adjustment Using CPI](#Inflation) <br>\n\n3. [Exploratory Data Analysis](#EDA)<br>\n    3.1. [By Flat Type](#Flat_type)<br>\n    3.2. [By Town](#Town)<br>\n    3.3. [By Storeys](#Storey)<br>\n    3.4. [By Floor Area](#Area)<br>\n    3.5. [By Block Number](#BlockNum)<br>\n    3.6. [By Flat Model](#Model)<br>\n    3.7. [By Lease Commence Date](#Lease)<br>\n    3.8. [By Distance to Nearest Amenities](#Amenities_Dist)<br>\n    3.9. [By Number of Amenities in 2km Radius](#Num_Amenities)<br>\n4. [Data Preparation](#Prep)<br>\n    4.1. [Missing Values](#Missing)<br>\n    4.2. [Multicollinearity](#Multicollinearity)<br>\n    4.3. [Normality](#Normality)<br>\n    4.4. [Label & Dummy Encoding](#Encode)<br>\n    4.5. [Feature Scaling](#Scaling)<br>\n    4.6. [Outlier Detection](#Outlier)<br>\n    \n5. [Linear Regression](#LR)<br>\n    5.1. [Model Building & Results](#Results)<br>\n    5.2. [Homoscedasticity and Normality of Residuals](#Homoscedasticity)<br>\n    5.3. [Feature Importance](#LR_Importance)<br>\n\n6. [Random Forest](#RF)<br>\n    6.1. [Out-Of-Bag](#RF_OOB)<br>\n    6.2. [K-fold Cross Validation](#RF_CV)<br>\n    6.3. [Feature Importance](#RF_Importance)<br>\n    6.4. [SHAP Values](#SHAP)<br>\n\n7. [Conclusion](#Conclusion)<br>","4385906d":"Not all the variables follow a normal distribution, and most of the distances variables have some outliers. For these outliers, its better to use a *multivariable approach like mahalanobis distance* to see if they are outliers instead of using a univariate approach. If needed, some of these variables can be transformed to reduce the skewness.","b3ad4fcf":"Homoscedaticity appears to be satisfied. The residuals are normally distributed around 0, satisfying the linearity and normality assumptions of the linear model.","2f51593f":"<a id='Area'><\/a>\n## 3.4. By Floor Area","34563a12":"<a id='Homoscedasticity'><\/a>\n## 5.2. Homoscedasticity and Normality of Residuals","f5b255df":"<a id='Normality'><\/a>\n## 4.3. Normality","bcab461d":"`Region` might be a better choice instead of `town` since `town` has lots of classes and one-hot encoding it might lead to a very sparse matrix. For `flat_type`, we can remove **Multi Generation** and **1 Room** since there are not many instances of them. `storey_range` will be label encoded according to their levels while `flat_model` should be further grouped to reduce the number of classes.","5d935771":"There is definitely multicollinearity in our continuous features. Even after removing several features, the highest VIF is still around 50 (`lease_commence_date`) and `floor_area_sqm` is above 10 as well. I chose to left them in as I believe that they are really important contributors to resale prices. One way is to refit another model without them and compare the output.","3d9741fb":"Queenstown appears to have one of the highest increases in resale price, which could be due to it being developed over the past 2 decades.","e641c340":"<a id='Outlier'><\/a>\n## 4.6. Outlier Detection\n\nUsing Cook's Distance as Mahalanobis Distance required too much memory for the large dataset. The threshold for Cook's Distance used here is 4\/n.","559c3c08":"<a id='Data_loading'><\/a>\n# 1. Data Loading","737ae735":"`remaining_lease` has lots of NAs. They are only available after 2015 sales onwards.","6a242cd7":"<a id='EDA'><\/a>\n# 3. Exploratory Data Analysis\n\n<a id='Flat_type'><\/a>\n## 3.1. By Flat Type","8389cc1d":"After removing outliers (~5000, 5.24%), the homoscedaticity became better. The residuals are normally distributed around 0, satisfying the linearity and normality assumptions of the linear model.","4fdb9508":"The feature importance are abit different from the linear regression model. `Floor area` and `lease commence date` are still one of the main drivers of resale prices. However, features like `distance from Dhoby Ghaut MRT` and `flat type` also appears to be good drivers.\n\nTree-based models seem to give lower importance to categorical values. This is due to the importance score being a measure of how often the feature was selected for splitting and how much gain in purity was achieved as a result of the selection.","ae809fb6":"Since the sample is huge, most variables\/features will be significant although they might not be practically significant.","015cc761":"<a id='LR_Importance'><\/a>\n## 5.3. Feature Importance","edf76b42":"### Flats with predicted medium resale price","3f918125":"<a id='Clean'><\/a>\n# 2. Data Cleaning","da20ca51":"<a id='LR'><\/a>\n# 5. Linear Regression\n\n<a id='LR_Results'><\/a>\n## 5.1. Model Building & Results","4715b915":"<a id='SHAP'><\/a>\n## 6.4. SHAP Values\n\nWe can also look at feature importance using SHAP (SHapley Additive exPlanations) values first proposed by [Lundberg and Lee (2006)](https:\/\/papers.nips.cc\/paper\/7062-a-unified-approach-to-interpreting-model-predictions.pdf) for model interpretability of any machine learning model. SHAP values have a few advantages:\n\n1. Directionality \u2014 Unlike the feature importance from random forest, SHAP values allows us to see the importance of a feature and the direction in which it influences the outcome \n2. Global interpretability \u2014 the collective SHAP values can show how much each predictor contributes to the entire dataset (this is not shown here as it takes a long time for a large dataset)\n3. Local interpretability \u2014 each observation gets its own SHAP values, allowing us to identify which features are more important for each observation\n4. SHAP values can be calculated for any tree-based model, which other methods are not able to do\n\nBelow shows the contributors to resale price for an example of low, middle and high resale price flats.","b2107449":"<a id='Missing'><\/a>\n## 4.1. Missing Values\n\nReplace missing distance values with median of the town. Only Kallang\/Whampoa has missing data, so the function below will replace them with the median of the Kallang\/Whampoa distance variables.","7c8d7924":"Marine Parage is far from MRT station.","c72b8cae":"Block number doesn't seem to influence prices.","183efe2e":"<a id='Amenities_Dist'><\/a>\n## 3.8. By Distance to Nearest Amenities\n\nThe names of schools, supermarkets, hawkers, shopping malls, parks and MRTs were downloaded\/scraped from [Data.gov.sg](https:\/\/data.gov.sg\/) and Wikipedia and fed through a function that uses [OneMap.sg](https:\/\/www.onemap.sg\/main\/v2\/) api to get their coordinates (latitude and longitude). These coordinates were then fed through other functions which uses geopy package to get the distance between locations. By doing this, the nearest distance of each amenity from each house can be computed, as well as the number of each amenity within a 2km radius of each flat.\n\nThe script for this can be found [here](https:\/\/github.com\/teyang-lau\/HDB_Resale_Prices\/blob\/main\/get_coordinates.ipynb).\n\nSome of Kallang\/Whampoa has no distance due to search error on OneMap.sg.","b7b24f0c":"Those cases on top right of the chart consists of flats that are either Terrace or Executive Maisonette, which is not surprising.","68242413":"<a id='Num_Amenities'><\/a>\n## 3.9. By Number of Amenities in 2km Radius","db6fb382":"<a id='Lease'><\/a>\n## 3.7. By Lease Commence Date","ec5dd896":"We will fit the linear regression first and come back to check on the **normality of the residuals** as well as **homoscedasticity**.","b95e3c13":"### Flats with predicted low resale price","a30471eb":"<font size=\"+3\" color=\"steelblue\"><b>My other works<\/b><\/font><br>\n\n<div class=\"row\">\n    \n  <div class=\"col-sm-4\">\n    <div class=\"card\">\n      <div class=\"card-body\" style=\"width: 20rem;\">\n         <h5 class=\"card-title\"><u>Pneumonia Detection with PyTorch<\/u><\/h5>\n         <img style='height:170' src=\"https:\/\/raw.githubusercontent.com\/teyang-lau\/Pneumonia_Detection\/master\/Pictures\/train_grid.png\" class=\"card-img-top\" alt=\"...\"><br>\n         <p class=\"card-text\">Pneumonia Detection using Transfer Learning via ResNet in PyTorch<\/p>\n         <a href=\"https:\/\/www.kaggle.com\/teyang\/pneumonia-detection-resnets-pytorch\" class=\"btn btn-primary\" style=\"color:white;\">Go to Post<\/a>\n      <\/div>\n    <\/div>\n  <\/div>   \n    \n  <div class=\"col-sm-4\">\n    <div class=\"card\">\n      <div class=\"card-body\" style=\"width: 20rem;\" style='background:red'>\n        <h5 class=\"card-title\"><u>Transformer is All You Need for Disaster Tweets? <\/u><\/h5>\n        <img style='width:250px' src=\"https:\/\/bn1301files.storage.live.com\/y4miItelaS-PGrdZrtHBBmQsPvQ8kv1csnkuYMYpwTlo5r4ayFzZ-LngiqzvwfZhNFMkPPH1NCliUr6r6SEe0cJgqA3a1xHkDVlyOQD5amu1z_tKzTte8I9C7tJYjHj_ETqZXLh7ZVrW4AD_S3shi0tn9FmnJuEd9Ej2nRXYb1FS72mGoGszHN6wrenHBtUygDk?width=521&height=329&cropmode=none\" class=\"card-img-top\" alt=\"...\"><br>\n        <p class=\"card-text\">Classify real disaster tweets using LSTM and BERT.<\/p>\n        <a href=\"https:\/\/www.kaggle.com\/teyang\/transformer-is-all-you-need-for-disaster-tweets\" class=\"btn btn-primary\" style=\"color:white;\">Go to Post<\/a>\n      <\/div>\n    <\/div>\n  <\/div>\n    \n  <div class=\"col-sm-4\">\n    <div class=\"card\">\n      <div class=\"card-body\" style=\"width: 20rem;\">\n         <h5 class=\"card-title\"><u>Covid-19 & Google Trends<\/u><\/h5>\n         <img style='height:135px' src=\"https:\/\/miro.medium.com\/max\/821\/1*Fi6masemXJT3Q8YWekQCDQ.png\" class=\"card-img-top\" alt=\"...\"><br><br>\n         <p class=\"card-text\">Covid-19-Google Trend Analysis and Data Vizualization<\/p>\n         <a href=\"https:\/\/www.kaggle.com\/teyang\/covid-19-google-trends-auto-arima-forecasting\" class=\"btn btn-primary\" style=\"color:white;\">Go to Post<\/a>\n      <\/div>\n    <\/div>\n  <\/div>  ","ddefc0d1":"<a id='RF_Importance'><\/a>\n## 6.3. Feature Importance","8054a7a0":"<a id='RF_OOB'><\/a>\n## 6.1. Out-Of-Bag","3c40801a":"We don't see much changes within each flat type through the last 5 years. The only consistent pattern is that prices become higher as flats have more rooms, which is unsurprising.","5c9d603d":"### Flats with predicted high resale price","774a2ddd":"### Before we begin, feel free to try out the deployment of this project into an interactive web app hosted on Streamlit Sharing [here](https:\/\/share.streamlit.io\/teyang-lau\/hdb_resale_prices\/main\/predict_hdb_prices_streamlit.py)!\n\n<br>\n\n[<img src = 'https:\/\/media.giphy.com\/media\/HGqtMxvdgzEr6Gb1IW\/source.gif' width=900>](https:\/\/share.streamlit.io\/teyang-lau\/hdb_resale_prices\/main\/predict_hdb_prices_streamlit.py)","fef53959":"<a id='Encode'><\/a>\n## 4.4. Label & Dummy Encoding","923a2411":"The changes are not very large from 2018 to 2019, although prices for Toa Payoh has dropped and Central Area 4-room flats have also dropped by 20%. Could it be because these areas have older HDB meaning their lease are now shorter? As shown below, it seems that places like Punggol, and Sengkang, tend to have later lease commence date, as they were developed later, which might have led to their slight increase in prices, while places like Toa Payoh and Central Area, tend to have older lease commence date.","5c470f15":"Surprisingly, the relationship with school is not as strong as expected, while distance from hawker is more pronounced from the median plots.","b80251f3":"<a id='Storey'><\/a>\n## 3.3. By Storeys","13e90d5a":"**Types of Flat Models:**\n\n[**Standard:**](https:\/\/www.teoalida.com\/singapore\/hdbfloorplans\/) (1\/2\/3\/4\/5-room). 1960s HDB. Had WC and shower in same room. 5-room Standard were introduced in 1974. <br>\n[**Improved:**](https:\/\/www.teoalida.com\/singapore\/hdbfloorplans\/) (1\/2\/3\/4\/5-room). Introduced in 1966, the 3\/4-room having separate WC and shower, they also featured void decks. 5-room Improved were introduced in 1974. <br>\n[**New Generation:**](https:\/\/www.teoalida.com\/singapore\/hdbfloorplans\/) Started first in 1975, New Generation flats can be 3-Room (67 \/ 82 sqm) or 4-Room (92 sqm), featuring en-suite toilet for master bedroom, with pedestal type Water Closet, plus store room. <br>\n[**Model A:**](https:\/\/www.teoalida.com\/singapore\/hdbfloorplans\/) Introduced in 1981: 3-Room (75 sqm), 4-Room (105 sqm), 5-Room (135 sqm), 5-Room Maisonette (139 sqm) <br>\n[**Model A2:**](https:\/\/www.teoalida.com\/singapore\/hdbfloorplans\/) Smaller units of Model A. e.g., 4-Room Model A2 (90 sqm) <br>\n[**Simplified:**](https:\/\/www.teoalida.com\/singapore\/hdbfloorplans\/) Introduced in 1984: 3-Room (64 sqm), 4-Room (84 sqm) <br>\n[**Multi Generation:**](https:\/\/blog.carousell.com\/property\/hdb-flat-types-singapore\/) 3Gen flats designed to meet the needs of multi-generation families. <br>\n[**Maisonette:**](https:\/\/sg.finance.yahoo.com\/news\/different-types-hdb-houses-call-020000642.html#:~:text=Model%20A%20Maisonettes%20are%20HDB,ft%20to%201%2C551%20sq%20ft.) AKA Model A Maisonette \u2014 2 storeys HDB flat <br>\n[**Premium Apartment:**](https:\/\/www.teoalida.com\/singapore\/hdbfloorplans\/) Introduced somewhere during 1990s, featuring better quality finishes, you get them in ready-to-move condition, with flooring, kitchen cabinets, built-in wardrobes <br>\n[**Executive Maisonette:**](https:\/\/sg.finance.yahoo.com\/news\/different-types-hdb-houses-call-020000642.html#:~:text=Model%20A%20Maisonettes%20are%20HDB,ft%20to%201%2C551%20sq%20ft.) More premium version of Model A Maisonettes. These units are no longer being built after being replaced by the Executive Condominium (EC) scheme in 1995 <br>\n[**Executive Apartment:**](https:\/\/www.teoalida.com\/singapore\/hdbfloorplans\/) Executive Apartment \/ Maisonette (146-150 sqm) were introduced in 1983 and replaced 5-Room Model A flats, in addition of the 3-bedroom and separate living\/dining found in 5A flats, EA and EM feature an utility\/maid room. 80% of Executive units were Maisonettes and 20% were Apartments. <br>\n[**DBBS:**](https:\/\/www.propertyguru.com.sg\/property-guides\/dbss-singapore-17893) public apartments built under the HDB's short-lived Design, Build and Sell Scheme (DBSS) from 2005 to 2012. They are a unique (and premium) breed of HDB flats in Singapore, which are built by private developers. High Prices. Quite similiar to Executive Condominium except DBBS is like a premium HDB without facilities of private condos <br>\n[**Adjoined Flat:**](https:\/\/blog.carousell.com\/property\/hdb-flat-types-singapore\/) Large HDB flats which are combined from 2 HDB flats <br>\n[**Terrace:**](https:\/\/blog.carousell.com\/property\/hdb-flat-types-singapore\/) HDB terrace flats built before HDB, without realizing Singapore's land constraint. Discontinued <br>\n[**Type S1S2:**](http:\/\/getforme.com\/previous2004\/previous290504_hdblaunchesthepinnancleatduxton.htm)  apartments at The Pinnacle@Duxton are classified as \"S\" or Special apartments in view of its historical significance and award-winning design. For application of HDB policies, S1 and S2 apartments will be treated as 4-room and 5-room flats respectively <br>\n[**2-room:**](https:\/\/blog.carousell.com\/property\/hdb-flat-types-singapore\/) Most likely refers to 2-room flexi where there is 1 bedroom and 1 common area <br>","7858f7a5":"Following the collapse of the thai Baht in July 1997, housing prices in Singapore continue to fall and only started gradually increasing again around 2004. In 2013, it experienced a decline due to 'Propery Market Cooling Measures', such as the Additional Buyer's Stamp Duty (ABSD), Loan-to-Value (LTV) Ratio, and Total Debt Servicing Ratio (TDSR). Refer [here](https:\/\/www.srx.com.sg\/cooling-measures) for more information.","8d1848b6":"`Storey_range` and `flat_type` were label encoded since they are ordinal.","1407ff2a":"<a id='RF_CV'><\/a>\n## 6.2. K-fold Cross Validation","bab6e1d2":"<a id='Scaling'><\/a>\n## 4.5. Feature Scaling\n\nScaling is only done for linear regression. Tree-based models do not require scaling as it does not affect performance.","25a44bc8":"<a id='Prep'><\/a>\n# 4. Data Preparation\n\nFocus on only data from 2015 to 2019","ea58780a":"It seems that `region` is the feature that drives resale prices the most. The Central region was used as the baseline against which all other regions are compared to. This means that all the other regions tend to be sold lower as compared to the Central area. `Floor area`, `lease commence date` and **special flat models** are also positive drivers of resale prices while distance to nearest hawker center is a negative driver.\n\nLet's look at whether a non-linear model will show consistent drivers for resale prices.","197c5efd":"Relationship is negative, with flats that are further away from Dhoby Ghaut MRT (Central), having lower resale prices.","606480a6":"<a id='Town'><\/a>\n## 3.2. By Town","f7cd4155":"<a id='Conclusion'><\/a>\n# 7. Conclusion\n\nIn this project, linear regression and random forest were used to looked at the drivers of HDB resale prices. Linear regression is powerful because it allows one to interpret the results of the model by looking at its coefficients for every feature. However, it assumes a linear relationship between the features and the outcome, which isn't always the case in real life. It also tends to suffer from bias due to its **parametric** nature. Conversely, **non-parametric** methods do not assume any function or shape, and random forest is a powerful non-linear machine learning model which uses **bootstrap aggregating (bagging)** and **ensembling** methods. A single decision tree has high variance as it tends to overfit to the data. Through bagging and ensembling, it is able to reduce the variance of each tree by combining them. \n\nLooking at the output of the models, linear regression showed that `regions`, `floor area`, `flat model`, `lease commencement date` and `distance from hawker` are the top 5 drivers of HDB prices. However, random forest gave a slightly different result. `floor area`, and `lease commencement date` and `distance from hawker` still in the top 5 while `distance from Dhoby Ghaut MRT` and `flat type` has also came up on top. This happens as tree-based models tend to give lower importance to categorical variables (`region` and `flat model`) due to the way it computes importance.\n\nNevertheless, the **size of the flat**, **lease date**, and certain aspects of the **location** appear to be consistently the most important drivers of HDB resale prices.\n\n### Things to improve on:\n* Remove `lease commencement date` from the linear regression model and rerun it again as it still has a high value of VIF, which could lead to a poor estimation of the coefficients of the model. Also, can try out different removal of features to see if VIF is still high, and then rerun the linear regression to see which set of features is the best. This will allow us to see whether features that are important in the random forest (`distance from dhoby`) are important using linear regression as well.\n* Some of the amenities might not have been built when the flat were sold. For example, many of the stations in Downtown line (e.g., Tampines West, Bedok North, Fort Canning) were only opened in 2017 and so flats sold in 2015 to 2016 were not affected by them. A better way is to take this into account when computing the nearest distance to amenities and the number of amenities. Although the difference might be small and I don't think it will make a huge difference, it is definitely a cleaner approach. \n* Account for **number of BTOs** offered each year\n* Compute **travelling time** instead of euclidean distance of flats from certain locations\n* Try out different ways to recategorize flat models, as the current way was arbitrary, and see if that changes the results of the models.\n* There are many other factors when considering the price of a HDB, some of which are harder to procure, some easier. The condition\/cleaniness of the block, the neighbours, the position in which the flat faces, the availability of Built-to-Order (BTO) flats, changes in government policies, are all examples of important factors to consider\n\n**If you like this notebook, please give me an upvote and\/or check out my other work!**","6fc89b05":"Here, I used a ratio of **9:1** for the train and test set and used both **Out-Of-Bag** and **K-fold Cross Validation** as validation methods."}}