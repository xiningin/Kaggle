{"cell_type":{"4506fbf8":"code","ddebe1f6":"code","569a46d0":"code","4a017a57":"code","32353c3c":"code","d0fb643c":"code","c7422428":"code","d29c8a02":"code","b39019d5":"code","702ef164":"code","7f161315":"code","82ef37eb":"code","264c1404":"code","6bd09239":"code","11573bf8":"code","129d82f2":"code","8251e479":"code","9bae2a7e":"code","e713170d":"code","0233daa8":"code","c12c1ff0":"markdown","91a5deff":"markdown","24ce5d77":"markdown","4e38fc31":"markdown","d9f7d7af":"markdown","e7e4eed9":"markdown","3ccbe5d9":"markdown","b2400c36":"markdown","630cdecb":"markdown","c0b316e2":"markdown","057ea9a7":"markdown","e319dbbc":"markdown","0ed232a0":"markdown","a2ee5ffc":"markdown","7a13d0be":"markdown","c0ef111d":"markdown","42927e2e":"markdown","0b3f57b3":"markdown"},"source":{"4506fbf8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport json\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom nltk.corpus import stopwords\nfrom unidecode import unidecode\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom wordcloud import WordCloud\n\nSTOPWORDS = set(stopwords.words('english'))\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Model\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Input\n\nimport re\n\nimport seaborn as sns","ddebe1f6":"# reading csv files and train & test file paths\ntrain_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\nsample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","569a46d0":"train_df.head()","4a017a57":"train_df.info()","32353c3c":"for col in train_df.columns:\n    print(f'We have {len(list(set(train_df[col].values)))} unique values in {col}')","d0fb643c":"# This method is taken from https:\/\/www.kaggle.com\/prashansdixit\/coleridge-initiative-eda-baseline-model#2.-Data-Exploration%F0%9F%94%8D\n# So please give them an upvote if you find this part useful!\n\ndef read_append_return(filename, train_files_path=train_files_path, output='text'):\n    \"\"\"\n    Function to read json file and then return the text data from them and append to the dataframe\n    \"\"\"\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data\n    \ntrain_df['text'] = train_df['Id'].apply(read_append_return)","c7422428":"display(train_df['cleaned_label'].value_counts())\nplt.figure(figsize=(40, 30))\nsns.set(font_scale = 2)\nsns.histplot(train_df['cleaned_label'])\nplt.xticks(rotation = 90)\nplt.show();","d29c8a02":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n\ndef preprocess_text(text):\n    REPLACE_BY_SPACE_RE = re.compile(r'[(){}\\[\\]\\|@,;]')\n    \n    # Lowercase text\n    text = text.lower()\n    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n    text = text.replace('\/', ' \/ ')\n    \n    text = unidecode(text).lower()\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n    \n    return text","b39019d5":"train_df['clean_pub_title'] = train_df['pub_title'].progress_apply(clean_text)\n# Note: I have commented out the preprocessing here because it was slowing the notebook down, so for publishing sake I have left this commented out.\ntrain_df['clean_text'] = train_df['text'].progress_apply(clean_text)","702ef164":"# train_df.to_csv('clean_train.csv', index=False)","7f161315":"# Get word counts and unique word counts\ntrain_df['length'] = train_df['clean_text'].apply(lambda x: len(x.split(' ')))\ntrain_df['unique_word_count'] = train_df['clean_text'].apply(lambda x: len(set(x.split(' '))))","82ef37eb":"# Distribution plots\ndisplay(train_df['length'].describe())\nplt.figure(figsize=(40, 30))\nax = sns.distplot(train_df['length'].values)\nplt.show();\n\ndisplay(train_df['unique_word_count'].describe())\nplt.figure(figsize=(40, 30))\nax = sns.distplot(train_df['unique_word_count'].values)\nplt.show();","264c1404":"# Define a function to plot word cloud\ndef plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(40, 30))\n    # Display image\n    plt.imshow(wordcloud) \n    # No axis details\n    plt.axis(\"off\");","6bd09239":"# Generate word cloud\nwordcloud = WordCloud(width = 3000, \n                      height = 2000, \n                      random_state=1, \n                      # background_color='salmon', \n                      colormap='Pastel1', \n                      collocations=False, \n                      stopwords = STOPWORDS).generate(' '.join(train_df['clean_pub_title'].values))\n# Plot\nplot_cloud(wordcloud)\n","11573bf8":"# Generate word cloud\nwordcloud = WordCloud(width = 3000, \n                      height = 2000, \n                      random_state=1, \n                      # background_color='salmon', \n                      colormap='Pastel1', \n                      collocations=False, \n                      stopwords = STOPWORDS).generate(' '.join(train_df['clean_text'].sample(4000).values))\n# Plot\nplot_cloud(wordcloud)","129d82f2":"def tokenize(text, MAX_NB_WORDS=1000, MAX_SEQUENCE_LENGTH=25, tokenizer=None):\n    #text = text.apply(preprocess_text)\n\n    if tokenizer is None:\n        tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters=r'!\"#$%&()*+,-.:;<=>?@[\\]^_`{|}~', lower=True)\n        tokenizer.fit_on_texts(text.values)\n\n    # word_index = tokenizer.word_index\n\n    X = tokenizer.texts_to_sequences(text.values)\n    X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n\n    return X, tokenizer\n\n\ndef encode_labels(data, target_columns):\n    \"\"\"\n        Inputs:\n            data: All the data we are provided\n            target_columns: List of all the columns that are the targets for our model to predict\n        Outputs:\n            y: One-hot encoding of the true labels for each sample\n    \"\"\"\n    y = {}\n    y_dummy_columns = {}\n    for col in target_columns:\n        dummies = pd.get_dummies(data[col])\n        y_dummy_columns[col] = dummies.columns\n        y[col] = dummies.values\n\n    return y, y_dummy_columns\n\n\ndef prepare_data(df, feature_column, target_columns, MAX_SEQUENCE_LENGTH=25):\n    X, tokenizer = tokenize(df[feature_column], MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH, MAX_NB_WORDS=1000)\n\n    Y, Y_dummy_columns = encode_labels(df, target_columns)\n\n    return X, Y, Y_dummy_columns, tokenizer\n","8251e479":"def train_test_split(X, Y, test_size=0.1):\n    # Create a mask for the train test split\n    np.random.seed(123)\n    mask = np.random.rand(X.shape[0]) < (1 - test_size)\n\n    # Split the data into train and test\n    X_train = X[mask]\n    X_test = X[~mask]\n\n    y_train = {}\n    y_test = {}\n    for key, value in Y.items():\n        y_train[key] = value[mask]\n        y_test[key] = value[~mask]\n\n    return X_train, X_test, y_train, y_test, mask","9bae2a7e":"MAX_NB_WORDS = 3000  # The maximum number of words to be used. (most frequent)\nMAX_SEQUENCE_LENGTH = 25    # Max number of words in each pieice of text\nEMBEDDING_DIM = 300\n\n# Randomly order the samples\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\n\ntarget_columns = ['cleaned_label']\n\n# Get our inputs and outputs from the data\nX, Y, Y_dummy_columns, tokenizer = prepare_data(train_df, 'clean_text', target_columns, MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH)\n\n# Split the inputs and outputs into our train and test sets\nX_train, X_test, y_train, y_test, _ = train_test_split(X, Y, test_size=0.2)\n\ninput_length = X_train.shape[1]\noutput_length = y_train[next(iter(y_train))].shape[1]\noutput_name = list(y_train.keys())[0]\n\ninp = Input(shape=(input_length,))\nx = Embedding(MAX_NB_WORDS, EMBEDDING_DIM)(inp)\nx = SpatialDropout1D(0.2)(x)\nx = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(x)\noutput = Dense(output_length, activation='softmax', name=output_name)(x)\n\nmodel = Model(inp, output)\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\ncallbacks = [EarlyStopping(\n    monitor='val_loss', patience=3, min_delta=0.0001)]\n\nmodel.fit(X_train, y_train['cleaned_label'], epochs=100,\n          batch_size=500,\n          validation_split=0.1, callbacks=callbacks)","e713170d":"# Evaluate it using the metric that they use in this dataset\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","0233daa8":"preds = model.predict(X_test)\n\nscores = []\nfor i in range(preds.shape[0]):\n    pred = Y_dummy_columns['cleaned_label'].tolist()[np.argmax(preds[i])] \n    true = Y_dummy_columns['cleaned_label'].tolist()[np.argmax(y_test['cleaned_label'][i])]\n    scores.append(jaccard(pred, true))\n\nprint(f'Score: {np.mean(scores)}')","c12c1ff0":"## Word Clouds","91a5deff":"The above analysis shows us that:\n* We have some duplicate ID values\n* We have some duplicate pub_title values\n* We can also see that we have 130 uniqe dataset labels","24ce5d77":"# <p style=\"background-color:#F8C1EE; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px;\">Please <u>upvote<\/u> if you find this notebook useful or interesting, I really appreciate the encouragement. Thanks!<\/p>","4e38fc31":"###\u00a0Train","d9f7d7af":"# <p style=\"background-color:#F8C1EE; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">Coleridge Initiative EDA \ud83d\udd0d and LSTM Model \ud83d\udcc8<\/p>","e7e4eed9":"# Closing Remarks\n\nThis is a work in progress and I will be making regular updates to the models to improve their performance.\n\nMy next steps will be:\n* Extract words unique to the papers and feed this into the model\n* Train the LSTM on the text field.\n* Hyperparameter tuning of the LSTM model","3ccbe5d9":"### Evaluate","b2400c36":"### Target Distribution\n\nWe will look at the distribution of the target field's values.","630cdecb":"### \"Pub Title\" Field Word Cloud","c0b316e2":"The above cells shows, as we'd expected, that there is a large imbalance in the distribution of the words where some words appear in almost every single paper. This suggests that some use of TF-IDF might help with the classification.","057ea9a7":"We can see from the above that there is a significant imbalance in the classes, which we should addressed before running any models.","e319dbbc":"# LSTM Model\n\nNormally, I wouldn't dive straight into a LSTM model but I have experience building these and I know they can perform well for these types of tasks.","0ed232a0":"# Text Preprocessing and Exploration\n\nWe will now preprocess the text and explore the results.","a2ee5ffc":"Here we can see that we have no null values to remove as part of the preprocessing.","7a13d0be":"The above score is obviously low, but I believe that with some simple tuning, and bring in the additional features we should be able to increase this quite easily. ","c0ef111d":"# Tabular Exploration","42927e2e":"The above word clouds show us that there is still come cleaning required for the two fields but in general, there are some words appearing in these clouds that could be good indicators. ","0b3f57b3":"### \"Text\" Field Word Cloud"}}