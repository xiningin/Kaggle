{"cell_type":{"5aa7a80b":"code","6252edfc":"code","5333f8d8":"code","e513db39":"code","f0b20ab4":"code","5ac34204":"code","ae3bc655":"code","b9907538":"code","4817b3ce":"code","cbb79d21":"code","bf075c9e":"code","8e6a56c8":"code","6358904c":"code","ae08fa77":"code","795487db":"code","505f7984":"code","cbe48084":"code","7948ce9d":"code","555ce3a9":"code","d77c2511":"code","c9ba4dfa":"code","e30dbbb1":"code","3f227039":"code","c7330a5c":"markdown","dc55122b":"markdown","7bf0c28a":"markdown","e0c665ef":"markdown","870ae0c1":"markdown","f8a8a8a8":"markdown","0ab10b67":"markdown","842ecb7f":"markdown","25bbaff9":"markdown","593bce2d":"markdown","b60cdce2":"markdown","ee2be293":"markdown","412adb53":"markdown"},"source":{"5aa7a80b":"#libraries\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport glob\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.utils import shuffle\nimport albumentations as A","6252edfc":"file_path = '..\/input\/african-wildlife\/zebra'\n#I will resize all pictures\nnew_image_size = 256","5333f8d8":"images = {}\nsizes = {}\nboxes = {}","e513db39":"#load images\nfor image_file_path in glob.glob(file_path+'\/*.jpg'):\n    image = load_img(image_file_path, target_size = (new_image_size, new_image_size))\n    pixels = img_to_array(image)\/255.0\n    image_id = os.path.basename(image_file_path)\n    image_id= image_id[:3]\n    images[image_id] = pixels","f0b20ab4":"#load sizes\nfor image_file_path in glob.glob(file_path+'\/*.jpg'):\n    image = load_img(image_file_path)\n    width, height = image.size\n    image_id = os.path.basename(image_file_path)\n    image_id= image_id[:3]\n    sizes[image_id] = width, height","5ac34204":"#load boxes\nfor bboxes_file_path in glob.glob(file_path+'\/*.txt'):\n    bbox =  pd.read_csv(bboxes_file_path, delimiter = ' ', header = None).values\n    bbox = bbox[1:]\n    bboxes_id = os.path.basename(bboxes_file_path)\n    bboxes_id= bboxes_id[:3]\n    boxes[bboxes_id] = bbox","ae3bc655":"image_ids = list(images.keys())\nimage_ids = np.array(image_ids, dtype = 'object')","b9907538":"#Split dictionaries into train and val sets.\ntrain_images = {key:images[key] for key in image_ids[0:300]}\ntrain_boxes = {key:new_boxes[key] for key in image_ids[0:300]}\n\nval_images = {key:images[key] for key in image_ids[300:377]}\nval_boxes = {key:new_boxes[key] for key in image_ids[300:377]}","4817b3ce":"train_image_ids = image_ids[0:300]\nval_image_ids = image_ids[300:377]","cbb79d21":"import matplotlib.pyplot as plt\nimport cv2\n%matplotlib inline\n\n#We have Yolo box format in the text file which has normed x_center, y_center, width and height\n#In order to visualise it we need to convert it to absolute values.\ndef rescale_to_abs(X_center_norm, y_center_norm, width_norm, height_norm, image_width, image_height):\n    X_abs = X_center_norm*image_width\n    y_abs = y_center_norm*image_height\n    width_abs = width_norm*image_width\n    height_abs = height_norm*image_height\n    return (X_abs, y_abs, width_abs, height_abs)\n    \ndef draw_picture_with_bboxes(boxes, images, sizes, image_id):\n    image_path = '..\/input\/african-wildlife\/zebra\/'+image_id+'.jpg'\n    image =img_to_array(load_img(image_path))\/255.0\n    for i in range(len(boxes[image_id])):\n        \n        X_center_norm, y_center_norm, width_norm, height_norm = boxes[image_id][i][1:]\n        image_width, image_height = sizes[image_id]\n        \n        X_abs, y_abs, width_abs, height_abs = rescale_to_abs(X_center_norm,\\\n                y_center_norm, width_norm, height_norm, image_width, image_height)\n        \n        #now we need to convert it to two points: top left and bottom right\n        x0, y0 = round(X_abs-width_abs\/2), round(y_abs-height_abs\/2)\n        x1, y1 = round(X_abs + width_abs\/2), round(y_abs + height_abs\/2)\n        \n        cv2.rectangle(image, (x0, y0), (x1, y1),(0,255,0),3)\n        \n    plt.imshow(image)","bf075c9e":"draw_picture_with_bboxes(boxes, images, sizes, '036')","8e6a56c8":"class custom_data_generator(tf.keras.utils.Sequence):\n    def __init__(self, images, boxes, new_image_size, image_ids, augment, batch_size=1, cell_size = 8, shuffle = True):\n        self.boxes = boxes\n        self.augment = augment\n        self.images = images\n        self.list_IDs = image_ids\n        self.batch_size = batch_size\n        self.cell_size = cell_size\n        self.new_image_size = new_image_size\n        self.amount_of_cells = int(self.new_image_size\/self.cell_size)\n        self.shuffle = shuffle\n        \n        self.train_augmentations = A.Compose([\n        A.OneOf([\n            A.HueSaturationValue(),\n            A.RandomBrightnessContrast()\n        ], p=0.6),\n        A.OneOf([\n            A.GaussNoise(),\n            A.GlassBlur(),\n        ], p=0.5),\n        A.Cutout(\n            num_holes=2, \n            max_h_size=4, \n            max_w_size=8, \n            fill_value=0, \n            p=0.5)],  bbox_params={'format': 'yolo', 'label_fields': ['labels']})\n        self.on_epoch_end()\n    \n    def __len__(self):\n        return int(np.floor(len(self.images)\/self.batch_size))\n    \n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.list_IDs))\n        \n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n            \n    def __getitem__(self, index):\n        #we need to slice indexes for one batch\n        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n        batch_indexes = [self.list_IDs[i] for i in indexes]\n        X, y = self.__data_generation(batch_indexes)\n\n        return X, y\n        \n        \n    def __data_generation(self, batch_indexes):\n        #the y_matrix is a gridcell table for every picture\n        #in the end we sum them all in y[] for one batch\n        y_matrix = np.zeros((32, 32, 10))\n        y = []\n        X = []\n        batch_boxes = {}\n        batch_images = {}\n        \n        \n        if self.augment == True:\n            for index in batch_indexes:\n                image = self.images[index]\n                boxes = self.boxes[index]\n                labels = np.ones(len(boxes))\n                augmented_data = self.train_augmentations(image=image, bboxes=boxes, labels = labels)\n                #in both cases we need to normalize the data\n                batch_images[index] = np.array(augmented_data['image'])\/255.0\n                batch_boxes[index] = np.array(augmented_data['bboxes'])\n         \n        if self.augment == False:\n            for index in batch_indexes:\n                batch_boxes[index] = self.boxes[index] \n                batch_images[index] = self.images[index]\/255.0\n            \n            \n            \n        for one_image in batch_boxes:\n            X.append(batch_images[one_image])\n            #iterate though all boxes in one picture\n            for one_box in batch_boxes[one_image]:\n                    \n                    x_norm, y_norm, width_norm, height_norm = one_box\n                    \n                    #check which cell they belong to\n                    cell_x = int(x_norm*self.amount_of_cells) \n                    cell_y = int(y_norm*self.amount_of_cells) \n                    \n                    #now rescale thoose coords to a given cell\n                    x_cell = self.amount_of_cells * x_norm - cell_x\n                    y_cell = self.amount_of_cells * y_norm - cell_y\n                    width_cell = width_norm*self.cell_size\n                    height_cell = height_norm*self.cell_size\n                    \n                    #now append information to the y_matrix\n                    #first, we check wheather this anchor box already has an object\n                    if y_matrix[cell_x][cell_y][0]==0:\n                        y_matrix[cell_x][cell_y][0]=1\n                        y_matrix[cell_x][cell_y][1:5]=x_cell, y_cell, width_cell, height_cell\n                    #maybe it does(for example, if there are two objects in one cell)\n                    #so we check if there is an object in another anchor box\n                    elif y_matrix[cell_x][cell_y][5]==0:\n                        y_matrix[cell_x][cell_y][5]=1\n                        y_matrix[cell_x][cell_y][6:10]=x_cell, y_cell, width_cell, height_cell\n                    \n                    else:\n                        break\n            y.append(y_matrix)   \n                    \n        return np.array(X), np.array(y)\n","6358904c":"train_generator = custom_data_generator(train_images, train_boxes, new_image_size, train_image_ids, True)","ae08fa77":"val_generator = custom_data_generator(val_images, val_boxes, new_image_size, val_image_ids, True)","795487db":"#reminder:\n#our vector will look like this: [p1, x1, y1, w1, h1, p2, x2, y2, w2, h2]\n#we have two anchor boxes and two bounding boxes for each cell\ndef custom_loss_function(y_true, y_pred):\n    alpha = 0.00001\n    \n    #(X, Y) LOSS\n    \n    xy_pred = tf.concat([y_pred[..., 1:3], y_pred[..., 6:8]], axis = 0)\n    xy_true = tf.concat([y_pred[..., 1:3], y_pred[..., 6:8]], axis = 0)   \n        \n    xy_loss = tf.keras.losses.MSE(xy_true, xy_pred)\n    \n    #(W, H) LOSS\n    \n    #because width and height are naturally bigger than coordinates the paper suggests\n    #to use their squares to stabilize the loss function\n    \n    def sqrt_func(x):\n        return tf.vectorized_map(tf.sqrt, x)\n    \n    wh_pred = tf.concat([sqrt_func(y_pred[..., 3:5] + alpha), sqrt_func(y_pred[..., 8:10] + alpha)], axis = 0)\n    wh_true = tf.concat([sqrt_func(y_true[..., 3:5]+ alpha), sqrt_func(y_true[..., 8:10] + alpha)], axis = 0)\n          \n\n    wh_loss = tf.keras.losses.MSE(wh_true, wh_pred)\n\n    #PROBABILITY LOSS\n    \n    prob_pred = tf.concat([y_pred[..., 0], y_pred[..., 5]], axis = 0)\n    prob_true = tf.concat([y_true[..., 0], y_true[..., 5]], axis = 0)\n        \n    #it is better to use Binary Crossentropy if we have only two options\n    binary_crossentropy = tf.keras.losses.BinaryCrossentropy(\n        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE\n    )\n    prob_loss = binary_crossentropy(prob_true, prob_pred)\n\n    #we cannot use if statements in loss functions because they are not differentiable\n    def mask(y_true):\n        first_mask = tf.where(y_true[...,0] == 0, 0.5, 5.0)\n        second_mask = tf.where(y_true[...,5] == 0, 0.5, 5.0)\n    \n        bboxes_mask = tf.concat([first_mask,second_mask],axis=0)\n    \n        return bboxes_mask\n    \n    mask = mask(y_true)\n    \n    xy_loss = xy_loss * mask\n    wh_loss = wh_loss * mask\n    prob_loss = prob_loss*mask\n    \n    return prob_loss + xy_loss + wh_loss","505f7984":"x_input = tf.keras.Input(shape=(256,256,3))\n\nx = tf.keras.layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same')(x_input)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx_shortcut = x\n\nfor i in range(1):\n    x = tf.keras.layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Add()([x_shortcut, x])\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x_shortcut = x\n\n\nx = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx_shortcut = x\n\nfor i in range(2):\n    x = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Add()([x_shortcut, x])\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x_shortcut = x\n\nx = tf.keras.layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx_shortcut = x\n\nfor i in range(8):\n    x = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Add()([x_shortcut, x])\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x_shortcut = x\n\nx = tf.keras.layers.Conv2D(512, (3, 3), strides=(2, 2), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx_shortcut = x\n\nfor i in range(8):\n    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Add()([x_shortcut, x])\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x_shortcut = x\n\n\nx = tf.keras.layers.Conv2D(1024, (3, 3), strides=(2, 2), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx_shortcut = x\n\nfor i in range(4):\n    x = tf.keras.layers.Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Conv2D(1024, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Add()([x_shortcut, x])\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x_shortcut = x\n\nx = tf.keras.layers.Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx = tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\npredictions = tf.keras.layers.Conv2D(10, (1, 1), strides=(1, 1), activation='sigmoid')(x)\n\nmodel = tf.keras.Model(inputs=x_input, outputs=predictions)\n","cbe48084":"#we can check the output shape using model.summary()\nmodel.summary()","7948ce9d":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(), \n    loss=custom_loss_function\n)","555ce3a9":"history = model.fit_generator(train_generator, epochs=50)","d77c2511":"predictions = model.predict(val_generator)","c9ba4dfa":"#this function works with all the boxes from a single image\ndef descale_the_boxes(predictions):\n    for x_cell in range(predictions.shape[0]):\n        for y_cell in range(predictions.shape[1]):\n            boxes = predictions[x_cell, y_cell, :]\n            \n            first_box_height = boxes[3]*8\n            first_box_width = boxes[4]*8\n            \n            first_box_x_normed = (boxes[1]+x_cell)\/256\n            first_box_y_normed = (boxes[2]+y_cell)\/256\n            \n            second_box_height = boxes[8]*8\n            second_box_width = boxes[9]*8\n            \n            second_box_x_normed = (boxes[6]+x_cell)\/256\n            second_box_y_normed = (boxes[7]+y_cell)\/256\n            \n            predictions[x_cell, y_cell, :] = [predictions[x_cell, y_cell, 0], first_box_x_normed, \n                                              first_box_y_normed, first_box_height, first_box_width,\n                                             predictions[x_cell, y_cell, 5], second_box_x_normed, \n                                              second_box_y_normed, second_box_height, second_box_width]\n            return predictions","e30dbbb1":"#intersection over union \ndef IOU(box1, box2):\n    #now we have [x_center, y_center, w, h] format\n    #we need to change it to [x1, y1, x2, y2] \n    \n    x_cen_1, y_cen_1, w_1, h_1 =box1\n    #x_tl = x top left corner, x_br = x bottom right corner\n    x_tl_1 = x_cen_1 - w_1\/2\n    y_tl_1 = y_cen_1 - h_1\/2\n    x_br_1 = x_cen_1 + w_1\/2\n    y_br_1 = y_cen_1 + h_1\/2\n    \n    x_cen_2, y_cen_2, w_2, h_2 =box2\n    #x_tl = x top left corner, x_br = x bottom right corner\n    x_tl_2 = x_cen_2 - w_2\/2\n    y_tl_2 = y_cen_2 - h_2\/2\n    x_br_2 = x_cen_2 + w_2\/2\n    y_br_2 = y_cen_2 + h_2\/2\n    \n    #now we calculate intersection\n    x_tl_inter = max(x_tl_1, x_tl_2)\n    y_tl_inter = max(y_tl_1, y_tl_2)\n    x_br_inter = min(x_br_1, x_br_2)\n    y_br_inter = min(y_br_1, y_br_2)\n    \n    #intercection_area\n    inter_area = np.abs((x_br_inter - x_tl_inter))*np.abs((y_br_inter - y_tl_inter))\n    \n    #union area\n    box1_area = np.abs((x_br_1 - x_tl_1))*np.abs((y_br_1 - y_tl_1))\n    \n    box2_area = np.abs((x_br_2 - x_tl_2))*np.abs((y_br_2 - y_tl_2))\n    \n    union_area = box1_area+box2_area - inter_area\n    \n    return union_area\/inter_area\n\ndef non_max_suppression(bboxes, iou_threshold, threshold):\n\n    #we only take the boxes with probability theshold higher than a certain level\n    bboxes = [box for box in bboxes if box[1] > threshold]\n    #sort the boxes by thier probability \n    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n    bboxes_after_nms = []\n\n    \n    while bboxes:\n        #take the box with the highest prob, and exclude it from boxes\n        chosen_box = bboxes.pop(0)\n        \n        #now we calculate the IOU of this box and every other box in boxes\n        #if the IOU between out chosen_box and other box is too big\n        #(which means that these boxes intersect with each other too much)\n        #we exclude this box from boxes\n        bboxes = [\n            box\n            for box in bboxes\n            if IOU(chosen_box[1:],\n                 box[1:])\n            < iou_threshold\n        ]\n\n        bboxes_after_nms.append(chosen_box)\n\n    return bboxes_after_nms","3f227039":"predictions_after_nms = []\nfor one_picture_boxes in predictions:\n\n    boxes = descale_the_boxes(one_picture_boxes)\n    \n    first_boxes = boxes[..., :5].reshape(32*32, 5).tolist()\n    second_boxes = boxes[..., 5:].reshape(32*32, 5).tolist()\n    \n    boxes = first_boxes + second_boxes\n    \n    boxes_after_nms = non_max_suppression(boxes, iou_threshold, threshold)\n    predictions_after_nms.append(boxes_after_nms)","c7330a5c":"We will also be using data augmentation, specifically the albumentations library.","dc55122b":"This part is still in the making. ","7bf0c28a":"# **5.** Creating a custom loss function\n\nI will be implementing the loss function from the YoloV1 paper, and try to follow it as close as possible(except for the class part). \n\n![image.png](attachment:3ef801c6-6f99-4bae-95c7-9b355960fd2f.png)","e0c665ef":"![image.png](attachment:04888dc0-3b5d-4385-97ee-7e4a329e1022.png)","870ae0c1":"# **6.** Creating a model\nI have chosen YoloV3 model architecture with residual blocks. Residual blocks are used to avoid vanishing gradients. An example of this can be seen in the picture below. \n\nI changed the stride in some Conv2d layers so the output shape would match y_matrix of the boxes. \n","f8a8a8a8":"# 7. Sorting the boxes\nOut of all the boxes we created during training(and that would be 32*32 for every image) we need to choose our final ones. \nTo do this we use Non Max Suppression.","0ab10b67":"The functions for finding IOUs and implementing NMS:","842ecb7f":"# **3.**  Visualise the images with bboxes","25bbaff9":"# **1.** General theory\n\nIn this notebook I will be implementing a Yolo algorithm with a few changes, mainly due to the fact that I will be working with only one class(zebras). I plan to update this kernel to include all classes or create a new one. \n\nThe Yolo algorithm has a few main steps:\n\n**1.** First, we divide our picture into gridcells.\n\n![yolo_dog_grid.jpg](attachment:f2a4c432-b409-43e9-8690-6474cc143a0d.jpg)\n\n**2.** During data generation, every gridcell will be assigned one(in my implementation two) bounding boxes. We determine which cell has which bounding box by checking which cell has its center.\nWhy did I decide to have two bounding boxes? Only one box per cell means that if our cell has two objects we can only write in one object. A lot of zebras stand very close together in our training dataset so one-box approach could cause some errors.\n![rsz_611254307b3c1.jpg](attachment:057c5792-f8a5-41b0-bd13-83c8dc22bba7.jpg)\n\nThis way every gridcell will have a following vector:  [p1, x1, y2, w1, h1, p2, x2, y2, w2, h2].  Here p1 and p2 are probabilities that the object is in the cell. In y_true probability can be either one or zero.\n\n**3.**  During training every cell will predict two bounding boxes. \n\nNow we need to get rid of the extra bounding boxes. We do this in two steps. First: we get rid of boxes with a low confidence score(i.e. low probability of a box in them). Second: we use non_max_suppression.\n\nWhat is NMS? First: we calculate Intersection Over Union of every box with every other box. IOU is (area of box intersection)\/(area of box union). If IOU is 0 we take both boxes because that means that they do not intersect(describe two different objects). But if IOU is higher than a certain threshold we take the bounding box with a higher confidence score.\n\n![rsz_611255a57ef20.jpg](attachment:1dde7c05-62bf-47f6-b95d-f6027568b1c1.jpg)\n\nIt is also important to note that for multiclass detection NMS performs for every class separately.\n","593bce2d":"# **4.** Creating a custom data generator\nCreating a custom data generator in tensorflow has a few nuances:\n1. it needs to inherit tf.keras.utils.Sequence\n2. it needs to have the following methods(aside from init):\n* 'len' to define the number of batches\n* 'on_epoch_end' to perform operations after each epoch(update the indexes and shuffle)\n* 'data_generation' to generate data \n* 'getitem' to generate one batch","b60cdce2":"# **2.** Data Loading\n\nI will store pictures, boxes and picture sizes in three separate dictionaries with image_ids as keys.","ee2be293":"But before we do this we need to descale the boxes. Right now x, y, width and height are all scaled to a single cell and we need to make them normed again. ","412adb53":"# **0.** Intro\n\nThis notebook is still in the process of making and I will be thankful for any comments and feedback. I will try to explain my understanding and implementation of the YOLO algorithm as well as I can. "}}