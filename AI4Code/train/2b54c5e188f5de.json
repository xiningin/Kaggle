{"cell_type":{"a59d4561":"code","4602a8f1":"code","02752733":"code","5590fb56":"code","43561b0d":"code","ce0a7bf3":"code","ba03806b":"code","73490b45":"code","2d7bf8a9":"code","92f41571":"code","8667fbbc":"code","29b0cad7":"code","8a895443":"code","62a85c67":"code","17d9d6de":"code","e063834a":"code","7b8d4800":"code","9f4b8533":"code","bbcad1bd":"code","75f8f24c":"code","23b02410":"code","3a9d14ec":"code","6d048c91":"code","c76ee5b9":"code","bfc29dec":"code","bff62667":"code","c52a8c65":"code","561537a7":"code","3f612f03":"code","ed3979a2":"code","9909f9ee":"code","2c312b58":"code","1254aefb":"code","b2c75f26":"code","980507f3":"code","3826b0d5":"code","914b79bc":"code","2904ac7f":"code","c766cd1a":"code","f3265462":"code","8b454593":"code","755bc290":"code","52157fd9":"code","5fae9442":"code","74f4efc4":"code","b256ea1c":"code","fef5a45d":"markdown","a00df332":"markdown","120e43ad":"markdown","ca00079f":"markdown","a3ac36bd":"markdown","8ac3d932":"markdown","7b646e31":"markdown","bc503405":"markdown","49fa60f8":"markdown","ca3d578b":"markdown","5d39f57e":"markdown","de0376fd":"markdown","e0ef276f":"markdown","788c1030":"markdown","f2b5dcca":"markdown","9aba2cd6":"markdown","bf90e876":"markdown","6448c35e":"markdown","034fa089":"markdown","ade792bc":"markdown","7b8b42f6":"markdown","82b8d4d3":"markdown","6c5f163e":"markdown","ceffa523":"markdown","9f0a044a":"markdown","f31ea05e":"markdown","6a958601":"markdown","8e84813c":"markdown","356ef866":"markdown"},"source":{"a59d4561":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score , confusion_matrix, f1_score\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nimport xgboost as xgboost\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Visualization\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\nimport seaborn as sns\n\n# Plotly\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True) # #do not miss this line\nimport plotly as py\nimport plotly.graph_objs as go\n\nfrom wordcloud import WordCloud,STOPWORDS\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4602a8f1":"data_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndata_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","02752733":"display(data_train.head())\ndisplay(data_test.head())\ndisplay(submission.head())","5590fb56":"data_train.info()","43561b0d":"data_train.columns","ce0a7bf3":"print(\"Train set contains {} rows and {} cols\".format(data_train.shape[0],data_train.shape[1]))\nprint(\"Test set contains {} rows and {} cols\".format(data_test.shape[0],data_test.shape[1]))","ba03806b":"data_train.location.value_counts()","73490b45":"data_train.keyword.value_counts()","2d7bf8a9":"# Missing values in train set\ndata_train.isnull().sum()","92f41571":"# Missing values in test set\ndata_test.isnull().sum()","8667fbbc":"train = data_train[['text', 'target']]\ntrain.head()","29b0cad7":"train.target.value_counts()","8a895443":"test = data_test[['text']]\ntest.head()","62a85c67":"def clean_tweets(text):\n    text = re.sub('https?:\/\/[A-Za-z0-9.\/]*','', text) # Remove https..(URL)\n    text = re.sub('[0-9]*','', text) # Removed digits\n    text = re.sub('RT @[\\w]*:','', text) # Removed RT \n    text = re.sub('@[A-Za-z0-9]+', '', text) # Removed @mention\n    text = re.sub('&amp; ','',text) # Removed &(and) \n    return text\n\ndef remove_punctuations(text):\n    text = ' '.join([i for i in text if i not in frozenset(string.punctuation)])\n    return text\n\nstop = stopwords.words('english')\nstop_list = ['u','\u00fb_']\nfor i in range(len(stop_list)):\n    stop.append(stop_list[i])\n\ndef remove_stopword(text):\n    words = [w for w in text if w not in stop]\n    return words","17d9d6de":"train['cleaned_text'] = train['text'].apply(clean_tweets)\ntrain['cleaned_text'] = train['cleaned_text'].apply(lambda x: x.lower()) \ntokenizer = RegexpTokenizer(r'\\w+')\ntrain['cleaned_text'] = train['cleaned_text'].apply(lambda x: tokenizer.tokenize(x)) # word tokenize\ntrain['cleaned_text'] = train['cleaned_text'].apply(remove_stopword) \ntrain['cleaned_text'] = train['cleaned_text'].apply(remove_punctuations) \ntrain.head()","e063834a":"test['cleaned_text'] = test['text'].apply(clean_tweets)\ntest['cleaned_text'] = test['cleaned_text'].apply(lambda x: x.lower()) \ntokenizer = RegexpTokenizer(r'\\w+')\ntest['cleaned_text'] = test['cleaned_text'].apply(lambda x: tokenizer.tokenize(x)) # word tokenize\ntest['cleaned_text'] = test['cleaned_text'].apply(remove_stopword) \ntest['cleaned_text'] = test['cleaned_text'].apply(remove_punctuations) \ntest.head()","7b8d4800":"w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]","9f4b8533":"train['cleaned_text'] = train['cleaned_text'].apply(lemmatize_text)\ntrain['cleaned_text'] = train['cleaned_text'].apply(remove_punctuations) \ntrain.head()","bbcad1bd":"test['cleaned_text'] = test['cleaned_text'].apply(lemmatize_text)\ntest['cleaned_text'] = test['cleaned_text'].apply(remove_punctuations) \ntest.head()","75f8f24c":"fig, ax = plt.subplots(figsize=(5, 4), subplot_kw=dict(aspect=\"equal\"))\nlabels=['Not-Disaster', 'Disaster']\nwedges, texts, autotexts = ax.pie(data_train.target.value_counts(),autopct=\"%1.2f%%\", colors=['#66b3ff','#cc1d00'], \n                                            explode = (0,0.07), startangle=90,\n                                            textprops={'fontsize': 15, 'color':'#f5f5f5'})\nplt.title('The Target Distribution', fontsize=16, weight=\"bold\")\nax.legend(wedges, labels,\n          title=\"Ingredients\",\n          loc=\"center left\",\n          bbox_to_anchor=(1.2, 0, 0, 1))\n\nplt.setp(autotexts, weight=\"bold\")\nplt.show()","23b02410":"fig = go.Figure([go.Bar(x=['Disaster', 'Not-Disaster'], \n                        y=[len(data_train[data_train['target']== 1]),len(data_train[data_train['target']== 0])])])\nfig.update_traces(marker_color='indianred', marker_line_color='rgb(58,48,107)',\n                  marker_line_width=1.5, opacity=0.7)\nfig.update_layout(title_text='The Target Distribution',autosize=False,width=400,height=500)\nfig.show()","3a9d14ec":"disaster = train[train['target']==1]['cleaned_text']\nnon_disaster = train[train['target']==0]['cleaned_text']","6d048c91":"print('Disaster Tweet: {} \\nNot-Disaster Tweet: {}'.format(disaster.values[2],non_disaster.values[2]))","c76ee5b9":"from PIL import Image\npath = '..\/input\/twitter-logo\/twitter_logo.png'\nmask = np.array(Image.open(path).convert('L'))\nmask.shape","bfc29dec":"def grey_color_func(word, font_size, position, orientation, **kwargs):\n    return \"hsl(0, 0%%, %d%%)\" % np.random.randint(60, 100)\n\nfig, (plt1, plt2) = plt.subplots(1, 2, figsize=[14, 6])\nwordcloud = WordCloud(\n                        background_color='#123456',\n                        random_state = 42,\n                        max_words= 50,\n                        mask=mask,\n                        contour_width=1,\n                        contour_color=\"#b5b5b5\"\n                     ).generate(''.join(disaster))\n\nplt1.imshow(wordcloud.recolor(color_func=grey_color_func,random_state=3), interpolation=\"bilinear\")\nplt1.axis(\"off\")\nplt1.set_title('Disaster Tweets',fontsize=30);\n\nwordcloud = WordCloud(\n                        background_color='#123456',\n                        random_state = 42,\n                        max_words= 50,\n                        mask=mask,\n                        contour_width=1,\n                        contour_color=\"#b5b5b5\"\n                     ).generate(''.join(non_disaster))\n\nplt2.imshow(wordcloud.recolor(color_func=grey_color_func,random_state=3), interpolation=\"bilinear\")\nplt2.axis(\"off\")\nplt2.set_title('Not-Disaster Tweets',fontsize=30);","bff62667":"max_features=300\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words=stop)\ntrain_vectors = count_vectorizer.fit_transform(train['text']).toarray()","c52a8c65":"print('In Train Set, the most common {} words:\\n{} '.format(max_features,count_vectorizer.get_feature_names()))","561537a7":"# Term Frequency\ntf = (train.cleaned_text).apply(lambda x : pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index()","3f612f03":"tf.columns = ['words','frequence']\ntf.head()","ed3979a2":"tfreq = tf[tf['frequence']>100.0]\nplt.subplots(figsize = (18,5))\nchart = sns.barplot(x=tfreq.words, y=tfreq.frequence, palette=sns.color_palette(\"coolwarm\",7), edgecolor=\".3\")\nchart.set_xticklabels(chart.get_xticklabels(), rotation=75)\nchart.set_title('Frequencies of the Most Common Words');","9909f9ee":"tf_idf_ngram = TfidfVectorizer(ngram_range=(1,2))\ntf_idf_ngram.fit(train.cleaned_text)\nx_train_tf_bigram = tf_idf_ngram.transform(train.cleaned_text) #.todense()\nx_test_tf_bigram = tf_idf_ngram.transform(test.cleaned_text)","2c312b58":"print(x_train_tf_bigram.shape,x_test_tf_bigram.shape)","1254aefb":"tf_idf_ngram.get_feature_names()[:5]","b2c75f26":"X = x_train_tf_bigram\ny = train.target.values\n\n# Train-Test Splitting\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint('Train Data splitted successfully')\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","980507f3":"df_accuracy = pd.DataFrame(columns=[\"Model\",\"Accuracy\",\"F1_score\"])","3826b0d5":"# Fitting Train set\nclf_LR = LogisticRegression(C=2,dual=True, solver='liblinear',random_state=0)\nclf_LR.fit(X_train,y_train)\n\n# Predicting \ny_pred_LR = clf_LR.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_LR) * 100\nf1score = f1_score(y_test, y_pred_LR) * 100\nprint(\"Logistic Regression Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"Logistic Regression F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'LogisticRegression','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)","914b79bc":"# Fitting Train set\nclf_NB = MultinomialNB()\nclf_NB.fit(X_train,y_train)\n\n# Predicting \ny_pred_NB = clf_NB.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_NB) * 100\nf1score = f1_score(y_test, y_pred_NB) * 100\nprint(\"MultinomialNB Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"MultinomialNB F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'NaiveBayes','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)","2904ac7f":"# Fitting Train set\nclf_KNN = KNeighborsClassifier(n_neighbors = 7,weights = 'distance')\nclf_KNN.fit(X_train, y_train)\n\n# Predicting \ny_pred_KNN = clf_KNN.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_KNN) * 100\nf1score = f1_score(y_test, y_pred_KNN) * 100\nprint(\"K-Nearest Neighbors Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"K-Nearest Neighbors F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'K-NearestNeighbors','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)","c766cd1a":"# Fitting Train set\nclf_RF = RandomForestClassifier(random_state=0)\nclf_RF.fit(X_train,y_train) \n\n# Predicting \ny_pred_RF = clf_RF.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_RF) * 100\nf1score = f1_score(y_test, y_pred_RF) * 100\nprint(\"Random Forest Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"Random Forest F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'RandomForest','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)","f3265462":"# Fitting Train set\nclf_DT = DecisionTreeClassifier(criterion= 'entropy', random_state=0)\nclf_DT.fit(X_train,y_train) \n\n# Predicting \ny_pred_DT = clf_DT.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_DT) * 100\nf1score = f1_score(y_test, y_pred_DT) * 100\nprint(\"Decision Tree Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"Decision Tree F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'DecisionTree','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)","8b454593":"# Fitting Train set\nclf_GB = GradientBoostingClassifier(n_estimators=400, learning_rate=0.05, max_depth=20, random_state=0)\nclf_GB.fit(X_train,y_train)\n\n# Predicting \ny_pred_GB = clf_GB.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_GB) * 100\nf1score = f1_score(y_test, y_pred_GB) * 100\nprint(\"Gradient Boosting Classifier Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"Gradient Boosting Classifier F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'GradientBoostingClassifier','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)","755bc290":"# Fitting Train set\nclf_XGB = xgboost.XGBClassifier(n_estimators=400, random_state=0, learning_rate=0.05, booster=\"gbtree\",\n                                n_jobs=-1, max_depth=20)\nclf_XGB.fit(X_train,y_train)\n# Predicting \ny_pred_XGB = clf_XGB.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_XGB) * 100\nf1score = f1_score(y_test, y_pred_XGB) * 100\nprint(\"XGBOOST Classifier Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"XGBOOST Classifier F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'XGBOOSTClassifier','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)","52157fd9":"# Fitting Train set\nclf_LGB = LGBMClassifier(n_estimators=1300, learning_rate=0.05, random_state=0, max_depth=20, n_jobs=-1)\nclf_LGB.fit(X_train,y_train)\n\n# Predicting \ny_pred_LGB = clf_LGB.predict(X_test)\n\n# Calculating Model Accuracy and F1_score\naccuracy = accuracy_score(y_test, y_pred_LGB) * 100\nf1score = f1_score(y_test, y_pred_LGB) * 100\nprint(\"LightGB Classifier Accuracy: {0:.3f} %\".format(accuracy))\nprint(\"LightGB Classifier F1 Score: {0:.3f} %\".format(f1score))\ndf_accuracy = df_accuracy.append({'Model':'LightGBClassifier','Accuracy':accuracy, 'F1_score': f1score},ignore_index=True)","5fae9442":"# Accuracy and F1-score Comparison of Models\ntrace1=go.Bar(\n                x=df_accuracy.Model,\n                y=df_accuracy.Accuracy,\n                name=\"Accuracy\",\n                marker=dict(color = 'rgba(50, 240,120, 0.7)',\n                           line=dict(color='rgb(0,0,0)',width=1.9)),\n                text='Accuracy')\ntrace2=go.Bar(\n                x=df_accuracy.Model,\n                y=df_accuracy.F1_score,\n                name=\"F1-score\",\n                marker=dict(color = 'rgba(240,120,10 , 0.7)', \n                           line=dict(color='rgb(0,0,0)',width=1.9)),\n                text='F1-score')\n\nedit_df=[trace1,trace2]\nlayout=go.Layout(barmode=\"group\", xaxis_tickangle=-60, title=\"Accuracy and F1-score of Models\")\nfig=dict(data=edit_df,layout=layout)\niplot(fig)","74f4efc4":"submission['target'] = clf_XGB.predict(x_test_tf_bigram)\nsubmission['target']","b256ea1c":"submission_final= submission[['id','target']]\nsubmission_final.to_csv('submission.csv',index=False)","fef5a45d":"### LightGB Classifier Model","a00df332":"### Gradient Boosting Classifier Model","120e43ad":"## <span style='font-weight:bold;color:#561225'>7. Text Classification Models<\/span>","ca00079f":"### Random Forest Model","a3ac36bd":"## <span style='font-weight:bold;color:#561225'>9. References<\/span>\n\n* https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n* https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp-a-general-intro\n* https:\/\/www.kaggle.com\/kushbhatnagar\/disaster-tweets-eda-nlp-classifier-models\/notebook\n* https:\/\/www.kaggle.com\/elcaiseri\/nlp-the-simplest-way","8ac3d932":"### Logistic Regression Model","7b646e31":"We see that class 0 (Not-Disaster Tweets) is more than class 1 (Disaster Tweets).","bc503405":"### Bag of Words","49fa60f8":"## <span style='font-weight:bold;color:#561225'>2.Read Datas<\/span>","ca3d578b":"## <span style='font-weight:bold;color:#561225'>8. Prediction and Submission<\/span>","5d39f57e":"### Lemmatization\n\n<p><strong style='font-weight:bold;color:#561225'>Stemming,<\/strong> refers to reducing a word to its root form. \n\n<p><strong style='font-weight:bold;color:#561225'>Lemmatization,<\/strong> on the other hand, takes into consideration the morphological analysis of the words. <br>\n\nStemming technique only looks at the form of the word whereas lemmatization technique looks at the meaning of the word.\n\n![](https:\/\/qph.fs.quoracdn.net\/main-qimg-cd7f4bafaa42639deb999b1580bea69f)\n    \nLet's use lemmatization technique.","de0376fd":"## <span style='font-weight:bold;color:#561225'>1.Import Libraries<\/span>","e0ef276f":"### Naive Bayes Model","788c1030":"In this kernel, 8 machine learning algorithms were used.As result;\n* The accuracy and the f1-score of Decision Tree Algorithm is lower than other algorithms.\n* The F1-score of the XGBOOST Algorithm is the highest compared to the others.","f2b5dcca":"### Missing Value","9aba2cd6":"There are many missing values in the location column in both train and test sets.","bf90e876":"## <span style='font-weight:bold;color:#561225'>5.Visualization<\/span>","6448c35e":"## <span style='font-weight:bold;color:#561225'>3.Data Analysis<\/span>","034fa089":"## <span style='font-weight:bold;color:#561225'>4.Data Pre-Processing<\/span>\n\nTweets are not syntactically well constructed. So preprocess will be applied.\n\n*  Convert all letters in tweets to lower case,\n*  Tokenization (disassembling according to desired features)\n*  Remove punctuation marks in the text,\n*  Removing Stopwords (commonly used words: the, at, and\u2026)\n*  Removing URLs, mentions and usernames,\n*  Removing numerical expressions,","ade792bc":"## <span style='font-weight:bold;color:#561225'>6.Converting Tokens to a Vector<\/span>","7b8b42f6":"<p style='font-weight:bold;color:#123456'><i>I hope you find this kernel useful. If you like it please do an upvote.<\/i><p> ","82b8d4d3":"### K-Nearest Neighbors Model","6c5f163e":"### Columns\n<p><strong style='font-weight:bold;color:#561225'>id<\/strong>: a unique identifier for each tweet<\/p>\n<p><strong style='font-weight:bold;color:#561225'>text <\/strong>: the text of the tweet <\/p>\n<p><strong style='font-weight:bold;color:#561225'>location <\/strong>: the location the tweet was sent from (may be blank)<\/p>\n<p><strong style='font-weight:bold;color:#561225'>keyword <\/strong>: a particular keyword from the tweet (may be blank)<\/p>\n<p><strong style='font-weight:bold;color:#561225'>target <\/strong>: in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)<\/p>","ceffa523":"### WordCloud","9f0a044a":"### Decision Tree Model","f31ea05e":"## <span style='font-weight:bold;color:#560010'>INTRODUCTION<\/span>\n\nThis notebook is a beginner kernel about NLP. For submission will evaluate different models based on Accuracy and F1-Score.","6a958601":"### TF-IDF\n\n<i style='font-weight:bold;color:#561225'>TF: <\/i>Term Frequency, which measures how frequently a term occurs in a document.<br>\n* TF(t) = (Number of times term t appears in a document) \/ (Total number of terms in the document). <br>\n\n<i style='font-weight:bold;color:#561225'>IDF: <\/i>Inverse Document Frequency, which measures how important a term is.<br>\n* IDF(t) = log_e(Total number of documents \/ Number of documents with term t in it).","8e84813c":"### The Target Distribution.\n\nWe will see the target distribution with visualization","356ef866":"### XGBOOST Classifier Model"}}