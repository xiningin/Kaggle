{"cell_type":{"bbb573df":"code","121af7af":"code","e8a466bd":"code","8d5e4b26":"code","78cfce47":"code","1c292d6b":"code","39630d26":"code","dca46c79":"code","e3ee82dc":"code","f92a1796":"code","4fc04114":"code","c3c1b284":"code","6b5780bb":"code","b040f82b":"code","879d9326":"code","2a2551cc":"code","58351ce5":"code","55b2d256":"code","023cf98a":"code","51e46a87":"code","c069fe84":"code","7c770256":"code","66be370d":"code","a6f2a727":"code","5a3b88d5":"code","04fafca8":"code","ac7e2ac0":"code","bc05cee2":"code","abbf3a4c":"code","2e95479d":"code","45bc1d63":"code","05484508":"markdown","f8b63896":"markdown","709cdfbd":"markdown","9fc698d9":"markdown","7ff6f29f":"markdown","d5aacee7":"markdown","b7ef39f0":"markdown","370a5462":"markdown","c0bb4f28":"markdown","c96871a2":"markdown","287f003a":"markdown","d994dd65":"markdown","e1e758e1":"markdown","6e5306c1":"markdown","da3994cb":"markdown","1580663a":"markdown","6f5b0b71":"markdown","47b4f766":"markdown"},"source":{"bbb573df":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')\npd.options.display.max_columns = 150","121af7af":"data = pd.read_csv('..\/input\/diamonds\/diamonds.csv')\nprint(data.shape)\ndata.head()","e8a466bd":"data.drop('Unnamed: 0', axis=1, inplace=True)","8d5e4b26":"data.describe()","78cfce47":"data = data[(data['x'] > 0) & (data['y'] > 0) & (data['z'] > 0)].reset_index(drop=True)\nprint(len(data))","1c292d6b":"fig = plt.figure(figsize=(20, 6))\nsns.heatmap(data.isnull(), yticklabels=False, cbar=False)","39630d26":"fig = plt.figure(figsize=(20, 6))\nsns.distplot(data['price'], kde=False)","dca46c79":"for col in ['cut', 'color', 'clarity']:\n    fig, ax =plt.subplots(1, 2, figsize=(20, 6))\n    fig.suptitle(col, fontsize=18)\n    data[col].value_counts().plot.pie(ax=ax[0], autopct=\"%1.1f%%\")\n    ax[0].legend()\n    for val in data[col].unique():\n        sns.distplot(data[data[col] == val]['price'], ax=ax[1], label=val, kde=False)\n    ax[1].legend()\n    plt.show()","e3ee82dc":"for col in ['carat', 'depth', 'table', 'x', 'y', 'z']:\n    fig, ax =plt.subplots(1, 2, figsize=(20, 6))\n    fig.suptitle(col, fontsize=18)\n    sns.distplot(data[col], ax=ax[0], kde=False)\n    data[[col]+['price']].plot.scatter(x=col, y='price', ax=ax[1])\n    plt.show()","f92a1796":"sns.catplot(data=data, x='clarity', hue='cut', y='price', kind='point', aspect=3)","4fc04114":"sns.catplot(data=data, x='color', hue='clarity', y='price', kind='point', aspect=3)","c3c1b284":"sns.catplot(data=data, x='color', hue='cut', y='price', kind='point', aspect=3)","6b5780bb":"df = pd.DataFrame()\ndf['Volume'] = data[['x', 'y', 'z']].apply(lambda row: row['x'] * row['y'] * row['z'], axis=1)\ndf['Mass'] = data['carat']\ndf['Density'] = df['Mass'] \/ df['Volume']\ndf['Price'] = data['price']","b040f82b":"df.head()","879d9326":"df.describe()","2a2551cc":"for col in ['Volume', 'Density']:\n    fig, ax =plt.subplots(1, 2, figsize=(20, 6))\n    fig.suptitle(col, fontsize=18)\n    sns.distplot(df[col], ax=ax[0], kde=False)\n    df[[col]+['Price']].plot.scatter(x=col, y='Price', ax=ax[1])\n    plt.show()","58351ce5":"fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n\ndf.plot.scatter(x='Density', y='Price', ax=ax[0])\ndf.plot.scatter(x='Mass', y='Price', ax=ax[1])\ndf.plot.scatter(x='Volume', y='Price', ax=ax[2])","55b2d256":"sns.heatmap(df.corr(), annot=True, center=0, cmap='RdYlGn')","023cf98a":"X = data.drop(['price'], axis=1)\ny = data['price']","51e46a87":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.pipeline import make_pipeline","c069fe84":"def get_column_names(feature_name, columns):\n    val = feature_name.split('_')[1]\n    col_idx = int(feature_name.split('_')[0][1:])\n    return f'{columns[col_idx]}_{val}'\n\nclass Preprocessor():\n    \n    def __init__(self, return_df=True):\n        self.return_df = return_df\n        \n        self.impute_median = SimpleImputer(strategy='median')\n        self.impute_const = SimpleImputer(strategy='constant')\n        self.ss = StandardScaler()\n        self.ohe = OneHotEncoder(handle_unknown='ignore')\n        \n        self.num_cols = make_column_selector(dtype_include='number')\n        self.cat_cols = make_column_selector(dtype_exclude='number')\n        \n        self.preprocessor = make_column_transformer(\n            (make_pipeline(self.impute_median, self.ss), self.num_cols),\n            (make_pipeline(self.impute_const, self.ohe), self.cat_cols),\n        )\n        \n    def fit(self, X):\n        return self.preprocessor.fit(X)\n        \n    def transform(self, X):\n        Xtransformed = self.preprocessor.transform(X)\n        try:\n            Xtransformed = Xtransformed.todense()\n        except:\n            pass\n        if self.return_df:\n            return pd.DataFrame(\n                Xtransformed,\n                columns=self.num_cols(X)+list(map(\n                    lambda x: get_column_names(x, self.cat_cols(X)),\n                    self.preprocessor.transformers_[1][1][1].get_feature_names()\n                ))\n            )\n        return X\n        \n    def fit_transform(self, X):\n        self.fit(X)\n        return self.transform(X)","7c770256":"X = Preprocessor().fit_transform(X)\nprint(X.shape)\nX.head()","66be370d":"features = X.columns\nX = X.values\ny= y.values","a6f2a727":"from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.metrics import mean_squared_error","5a3b88d5":"kf = KFold(random_state=19, shuffle=True)","04fafca8":"%%time\nr2scores = []\nrmse = []\nfor train_index, test_index in kf.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    model = LinearRegression().fit(X_train, y_train)\n    r2scores.append(model.score(X_test, y_test))\n    rmse.append(np.sqrt(mean_squared_error(y_test, model.predict(X_test))))\n    \nprint('Mean r2 score', np.mean(r2scores))\nprint('Mean rmse', np.mean(rmse))","ac7e2ac0":"%%time\nr2scores = []\nrmse = []\nfor train_index, test_index in kf.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    model = DecisionTreeRegressor(random_state=19).fit(X_train, y_train)\n    r2scores.append(model.score(X_test, y_test))\n    rmse.append(np.sqrt(mean_squared_error(y_test, model.predict(X_test))))\n    \nprint('Mean r2 score', np.mean(r2scores))\nprint('Mean rmse', np.mean(rmse))","bc05cee2":"from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRFRegressor, XGBRegressor\nfrom lightgbm import LGBMRegressor\n\ntrees = [\n    ('Random Forest', RandomForestRegressor), ('Extra Trees', ExtraTreesRegressor), ('LightGBM', LGBMRegressor),\n    ('Gradient Boosting', GradientBoostingRegressor), ('XGBoost', XGBRegressor), ('XGBoostRF', XGBRFRegressor),\n]","abbf3a4c":"%%time\nfor name, algo in trees:\n    r2scores = []\n    rmse = []\n    for train_index, test_index in kf.split(X, y):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        model = algo(random_state=19).fit(X_train, y_train)\n        r2scores.append(model.score(X_test, y_test))\n        rmse.append(np.sqrt(mean_squared_error(y_test, model.predict(X_test))))\n\n    print(name)\n    print('Mean r2 score', np.mean(r2scores))\n    print('Mean rmse', np.mean(rmse))\n    print()","2e95479d":"%%time\nr2scores = []\nrmse = []\nfor train_index, test_index in kf.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    model = LGBMRegressor(random_state=19).fit(X_train, y_train)\n    r2scores.append(model.score(X_test, y_test))\n    rmse.append(np.sqrt(mean_squared_error(y_test, model.predict(X_test))))\n    \nprint('Mean r2 score', np.mean(r2scores))\nprint('Mean rmse', np.mean(rmse))","45bc1d63":"fig, ax = plt.subplots(figsize=(20, 6))\nfig.suptitle('Feature Importance')\npd.Series(model.feature_importances_, index=features).sort_values(ascending=False).plot.bar(ax=ax)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))","05484508":"# 'Deciding' the Diamond Prices","f8b63896":"**The key deciding factors for the price of a diamond are:**\n\n* Its weight (carat)\n* Its Dimensions (y, z, depth, x, table)","709cdfbd":"## Feature Analysis (Univariate and Bivariate)\n\n### Categorical Features","9fc698d9":"### Numerical Features","7ff6f29f":"### Multivariate Analysis","d5aacee7":"### Mass, Volume and Density","b7ef39f0":"# Exploring About Diamonds","370a5462":"**Density is almost constant, and mass and volume are highly correlated**","c0bb4f28":"## Linear Regression Baseline","c96871a2":"**LightGBM works best with this data, Lets explore it further**","287f003a":"## Exploring the data","d994dd65":"# Diamonds \n\n![img](https:\/\/miro.medium.com\/max\/4000\/0*6WLqebrITTPNHwu7.gif)\n\nDiamond is one of the best-known and most sought-after gemstones. They have been used as decorative items since ancient times. \nThe hardness of diamond and its high dispersion of light\u2014giving the diamond its characteristic \"fire\" make it useful for industrial applications and desirable as jewellery. Diamonds are such a highly traded commodity that multiple organizations have been created for grading and certifying them based on the \"four Cs\", which are color, cut, clarity, and carat. Other characteristics, such as presence or lack of fluorescence, also affect the desirability and thus the value of a diamond used for jewelry. ","e1e758e1":"## Data Preprocessing\n\n* Missing Treatment (Not Required in this case)\n    * Numerical data: Median Imputation \n    * Categorical data: Constant Imputation \n* Preprocessing:    \n    * Numerical data: Scaling \n    * Categorical data: One Hot Encoding     ","6e5306c1":"## Characterstics of Diamond\n\nColumn | Description\n:---|:---\nprice | Price in US dollars\ncarat | The mass of a diamond. One carat is defined as 200 milligrams. The price per carat increases with carat weight, since larger diamonds are both rarer and more desirable for use as gemstones, but it does not increase linearly with increasing size. Instead, there are sharp jumps around milestone carat weights\ncut quality of the cut | The cut of a diamond describes the manner in which a diamond has been shaped and polished from its beginning form as a rough stone to its final gem proportions. The cut of a diamond describes the quality of workmanship and the angles to which a diamond is cut, (Fair, Good, Very Good, Premium, Ideal)\ncolor | Diamond colour, from J (worst) to D (best). The finest quality as per color grading is totally colorless, which is graded as D color diamond across the globe. The next grade has a very slight trace of color, These are graded as E color or F color diamonds. Diamonds which show very little traces of color are graded as G or H color diamonds. Slightly colored diamonds are graded as I or J or K color.\nclarity | Clarity is a measure of internal defects of a diamond called inclusions. Inclusions may be crystals of a foreign material or another diamond crystal, or structural imperfections such as tiny cracks that can appear whitish or cloudy. The number, size, color, relative location, orientation, and visibility of inclusions can all affect the relative clarity of a diamond (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\nx | Length of the diamond in mm\ny | Width of the diamond in mm\nz | Depth of the diamond in mm\ndepth | Total depth percentage = z \/ mean(x, y) = 2 * z \/ (x + y) \ntable | Width of top of diamond relative to widest point\n\n**Measurements of a Diamond**\n\n![img](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcTSvEgoy9XcJ3itQ4B6i0IJTqwO4KR8zZ2dKQ&usqp=CAU)\n\n**Color Grading Scale**\n\n![img](https:\/\/i.pinimg.com\/originals\/23\/c4\/6a\/23c46a456f285489c5893ab719cb611f.jpg)\n\n\n**Clarity Grading**\n![img](https:\/\/edipson.com\/wp-content\/uploads\/2019\/07\/clarity-chart.jpg)","da3994cb":"## Decision Tree","1580663a":"**Checking Missing Data**\n___","6f5b0b71":"**The minimum values of x, y, z are 0 which is not possible, hence removing those data**","47b4f766":"**Price Distribution**\n___"}}