{"cell_type":{"8ae02519":"code","7fb0a3a3":"code","ff092305":"code","a274a81d":"code","13cbb7db":"code","22369d91":"code","efc3e14d":"code","8bdda661":"code","fd838083":"code","3e949291":"code","a9d3ed91":"code","275fbc37":"code","6259889a":"code","42a8f328":"code","6883a7ba":"code","0b833e67":"code","f5f9dbaf":"code","cc1062bf":"code","815dd9dd":"code","6b983046":"code","28abd07d":"code","f100bd10":"code","e2683fc8":"code","ddd1a89d":"code","4715abda":"code","ec0ac2d5":"code","2d41e6ca":"code","dd054cc4":"code","6de0e9e1":"code","84af0d67":"code","697600aa":"code","63923706":"code","307da5fc":"code","091d6f16":"code","d79ffab2":"code","fc85124b":"code","d80a1249":"code","bc1deb7d":"code","9b73c8d7":"code","e6b57267":"code","443f108f":"code","e47b9d62":"code","69ae41b9":"code","5ffb6b9e":"code","c602dc03":"code","804f27a9":"code","25fd2f8f":"code","198b176b":"code","fbbf34f2":"code","ddbe249f":"code","b446ad7a":"code","0040663c":"code","34e279e5":"code","3ae17baf":"code","9fdcba66":"code","fa9536bb":"code","c9a15d70":"code","d7fce5b7":"markdown","2d554813":"markdown","aa16a3de":"markdown","722a3fd1":"markdown","b4cfd9ad":"markdown","dfd001ac":"markdown","1df3f898":"markdown","7ad1d51c":"markdown","9621a6bf":"markdown","4e0b28d5":"markdown","3467ee40":"markdown","ac9a713b":"markdown","a4e178ed":"markdown","0dc5ad33":"markdown","7094d222":"markdown","bd94a919":"markdown","fe1ac4fb":"markdown","7df8b2bf":"markdown","6c0d7faf":"markdown","f210bea2":"markdown","a8332d38":"markdown","0fec5daf":"markdown","bd83c0fe":"markdown","c39748d3":"markdown","1b49b059":"markdown","1e647a51":"markdown","d73f2c55":"markdown","c7ee2368":"markdown","a5aae58d":"markdown","7573c2ba":"markdown","9345e32a":"markdown","b3c1c6be":"markdown","023fc87d":"markdown","33b84fcb":"markdown","24311068":"markdown","cc51728b":"markdown","3e6acaaf":"markdown","597cdf55":"markdown","3aa5e196":"markdown","1c8f74d1":"markdown","1b5a1306":"markdown","1c90bc95":"markdown","f86b0c0c":"markdown","86b00138":"markdown","6fe7c262":"markdown","2f534f42":"markdown","e10c225c":"markdown","659cfd92":"markdown","58a62272":"markdown","1fe31da3":"markdown","ad7d3726":"markdown"},"source":{"8ae02519":"# Import libraries\n\n# data analysis, array processing\nimport numpy as np\nimport pandas as pd\n\n# visualization\nfrom matplotlib import pyplot as plt, rc, cm, lines\nimport seaborn as sns\n\n# data transform\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import TSNE\n\n# machine learning\nfrom sklearn.model_selection import train_test_split, \\\n    StratifiedKFold, GridSearchCV, cross_val_predict\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, \\\n    RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nimport xgboost as XGB\n\n# machine learning metrics\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, \\\n    precision_score, recall_score, f1_score, classification_report, roc_curve\n\n# statistical metrics\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\nfrom scipy import stats","7fb0a3a3":"# I don't want this class in the middle of work because it's too large. So I import it from github repository.\nimport requests\nimport os\n# get content by URL and decode as utf-8\nurl='https:\/\/github.com\/RuslanOraev\/handmade-data-science\/raw\/main\/my-own-library\/ensemble_stacking.py'\ncontent=requests.get(url).content\ntext=content.decode('utf-8')\n# write content in a file\nwith open ('ensemble_stacking.py', 'w', encoding=\"utf-8\") as line:\n    line.write(text)\n\n# import Stacking from a file\nfrom ensemble_stacking import Stacking\n# remove temporary file\nos.remove('ensemble_stacking.py')","ff092305":"# matplotlib parameters\nrc('font', size=14)\nrc('figure', figsize=(10,8))","a274a81d":"# max enable number of displaying columns\npd.options.display.max_columns = 25","13cbb7db":"# load data from a github repository using url\nurl = 'https:\/\/github.com\/RuslanOraev\/handmade-data-science\/raw\/main\/datasets\/telecom_users.csv'\ndf_raw = pd.read_csv(url).rename(columns={'Unnamed: 0': 'old_index'})\ndf_raw.head()","22369d91":"# info about dataset\ndf_raw.info()","efc3e14d":"# drop unnecessary columns\ndf_raw = df_raw.drop(['old_index', 'customerID'], axis=1)","8bdda661":"# get columns of dataset and print unique sorted values. Returns None\ndef print_unique(*args):\n    for column in args:\n        print(f'{column}: {df_raw[column].sort_values().unique()}')","fd838083":"# print all the unique sorted values of each column\nprint_unique(*df_raw.columns.to_list())","3e949291":"# search for the rows with a gap in \"TotalCharges\"\ndf_raw[df_raw['TotalCharges'] == ' ']","a9d3ed91":"# Ignore clients with no Total Charges\ndf_raw = df_raw[~(df_raw['TotalCharges'] == ' ')]\n# set index from 0 to number of objects\ndf_raw.index = range(len(df_raw))\n# set float type in \"TotalCharges\" column\ndf_raw['TotalCharges'] = df_raw['TotalCharges'].astype(float)","275fbc37":"# histograms of churn share customers to remaining customers\n\n# labelsize\nrc('axes', labelsize = 18)\n\n# row and col lenghts\ncol_len = 4\nrow_len = round(len(df_raw.drop('Churn', axis=1).columns)\/col_len)\n\n# building histplots\nfig, ax = plt.subplots(int(np.ceil(row_len)), col_len, figsize=(22, 20))\nfor i, col in enumerate(df_raw.drop('Churn', axis=1).columns):\n    row_idx = int(i \/\/ col_len)\n    col_idx = int(i - (col_len * (i \/\/ col_len)))\n    ax_curr = ax[row_idx, col_idx]\n    hist = sns.histplot(data=df_raw, x=col, hue='Churn', palette='seismic', alpha=.7, shrink=.9, multiple=\"fill\", ax=ax_curr, legend=False)\n    if col == 'PaymentMethod':\n        ax_curr.tick_params(axis='x', rotation=-45)\n    if row_idx == 0 and col_idx == col_len-1:\n        ax_curr.legend(labels=['Yes', 'No'], title='Churn', bbox_to_anchor=(1, 1))\n        \nax[0, 1].title._text = '\\n\\n'\nfig.suptitle('Customer churn share distribution by different features', size=24)\nplt.tight_layout()\nrc('axes', labelsize = 14)","6259889a":"# Match features to numbers\ndf = df_raw.copy()\n\ndf.loc[df['gender'] == 'Male', 'gender'] = 1\ndf.loc[df['gender'] == 'Female', 'gender'] = 0\n\ndf.loc[df['Partner'] == 'Yes', 'Partner'] = 1\ndf.loc[df['Partner'] == 'No', 'Partner'] = 0\n\ndf.loc[df['Dependents'] == 'Yes', 'Dependents'] = 1\ndf.loc[df['Dependents'] == 'No', 'Dependents'] = 0\n\ndf.loc[df['PhoneService'] == 'Yes', 'PhoneService'] = 1\ndf.loc[df['PhoneService'] == 'No', 'PhoneService'] = 0\n\ndf.loc[df['MultipleLines'] == 'Yes', 'MultipleLines'] = 1\ndf.loc[df['MultipleLines'] == 'No', 'MultipleLines'] = 0\ndf.loc[df['MultipleLines'] == 'No phone service', 'MultipleLines'] = 2\n\ndf.loc[df['InternetService'] == 'DSL', 'InternetService'] = 1\ndf.loc[df['InternetService'] == 'No', 'InternetService'] = 0\ndf.loc[df['InternetService'] == 'Fiber optic', 'InternetService'] = 2\n\ndf.loc[df['OnlineSecurity'] == 'Yes', 'OnlineSecurity'] = 1\ndf.loc[df['OnlineSecurity'] == 'No', 'OnlineSecurity'] = 0\ndf.loc[df['OnlineSecurity'] == 'No internet service', 'OnlineSecurity'] = 2\n\ndf.loc[df['OnlineBackup'] == 'Yes', 'OnlineBackup'] = 1\ndf.loc[df['OnlineBackup'] == 'No', 'OnlineBackup'] = 0\ndf.loc[df['OnlineBackup'] == 'No internet service', 'OnlineBackup'] = 2\n\ndf.loc[df['DeviceProtection'] == 'Yes', 'DeviceProtection'] = 1\ndf.loc[df['DeviceProtection'] == 'No', 'DeviceProtection'] = 0\ndf.loc[df['DeviceProtection'] == 'No internet service', 'DeviceProtection'] = 2\n\ndf.loc[df['TechSupport'] == 'Yes', 'TechSupport'] = 1\ndf.loc[df['TechSupport'] == 'No', 'TechSupport'] = 0\ndf.loc[df['TechSupport'] == 'No internet service', 'TechSupport'] = 2\n\ndf.loc[df['StreamingTV'] == 'Yes', 'StreamingTV'] = 1\ndf.loc[df['StreamingTV'] == 'No', 'StreamingTV'] = 0\ndf.loc[df['StreamingTV'] == 'No internet service', 'StreamingTV'] = 2\n\ndf.loc[df['StreamingMovies'] == 'Yes', 'StreamingMovies'] = 1\ndf.loc[df['StreamingMovies'] == 'No', 'StreamingMovies'] = 0\ndf.loc[df['StreamingMovies'] == 'No internet service', 'StreamingMovies'] = 2\n\ndf.loc[df['Contract'] == 'Two year', 'Contract'] = 2\ndf.loc[df['Contract'] == 'One year', 'Contract'] = 1\ndf.loc[df['Contract'] == 'Month-to-month', 'Contract'] = 0\n\ndf.loc[df['PaperlessBilling'] == 'Yes', 'PaperlessBilling'] = 1\ndf.loc[df['PaperlessBilling'] == 'No', 'PaperlessBilling'] = 0\n\ndf.loc[df['PaymentMethod'] == 'Electronic check', 'PaymentMethod'] = 3\ndf.loc[df['PaymentMethod'] == 'Mailed check', 'PaymentMethod'] = 2\ndf.loc[df['PaymentMethod'] == 'Bank transfer (automatic)', 'PaymentMethod'] = 1\ndf.loc[df['PaymentMethod'] == 'Credit card (automatic)', 'PaymentMethod'] = 0\n\ndf.loc[df['Churn'] == 'Yes', 'Churn'] = 1\ndf.loc[df['Churn'] == 'No', 'Churn'] = 0\n\n# change dtypes of columns from object to int\ndf.loc[:, 'gender':'PaymentMethod'], df['Churn'] = df.loc[:, 'gender':'PaymentMethod'].astype(int), df['Churn'].astype(int)","42a8f328":"# new values of columns\ndf.head(3)","6883a7ba":"# statistical metrics\ndesc = df.describe().T\ndesc['mode'] = df.apply(lambda x: x.value_counts()[x.value_counts().values == x.value_counts().max()].index).T.rename(columns={0: 'mode'})\ndesc['mode_freq'] = [len(df[df[col] == mode]) for col, mode in zip(desc.index, desc['mode'])]\ndesc['unique'] = df.apply(lambda x: len(x.unique()), axis=0)\ndesc['kurtosis'] = kurtosis(df)\ndesc['skew'] = skew(df)\ndesc","0b833e67":"# groupby the customers\n\n# you may change it and watch the share of departed and remaining customers before and after this mark\ntenure_mark = 12\n\n# calc the number of the whole years or months\nif tenure_mark % 12 == 0:\n    mark_name = f'{tenure_mark \/\/ 12} year' + bool(tenure_mark \/\/ 24) * 's'\nelse:\n    mark_name = f'{tenure_mark} months'\n    \n# calculating the number of departed and remaining customers before and after the tenure_mark\ndf_year = df.copy()\ndf_year['tenure'] = df_year['tenure'].apply(lambda x: f'more than {mark_name}' if x > tenure_mark else f'less than {mark_name}')\ndf_year['Churn'] = df_year['Churn'].apply(lambda x: 'churn' if x == 1 else 'remain')\ndf_year_actual = df_year.groupby(['Churn', 'tenure']).size()\ndf_year_share = df_year_actual.to_frame().apply(lambda x: 100 * x \/ x.sum())\nratio = df_year_actual.groupby(level=0).sum()\n\n# plot a pie\nexplode = [0.1, 0.1, 0, 0] \nplt.figure(figsize=(6,6),facecolor='lavender')\nplt.pie(df_year_share.values.ravel(), labels=[': '.join(token) for token in df_year_share.index], autopct='%1.1f%%', shadow=True, startangle=0, \n        explode=explode, radius=1, colors=['darkred', 'lightcoral', 'lightgreen', 'darkgreen'])\n\n# plot formatting\nplt.title(f'Share of departed and remaining customers\\nThe ratio is {ratio.churn}:{ratio.remain} ppl\\n', size=18)\nplt.legend(labels=df_year_actual, bbox_to_anchor=(1.3, 0.1), title='count of customers')\nplt.show()","f5f9dbaf":"# whisker plots for features by churn\n\n# labelsize\nrc('axes', labelsize = 20)\n\ncol_len = len(df.drop('Churn', axis=1).columns)\/2\nfig, ax = plt.subplots(2, int(np.ceil(col_len)), figsize=(26, 8))\nfor i, col in enumerate(df.drop('Churn', axis=1).columns):\n    sns.boxplot(data=df, x='Churn', y=col, ax=ax[int(i \/\/ col_len), int(i - (col_len * (i \/\/ col_len)))], whis=1.5, palette='seismic')\n\n# plot formatting\nax[0, 1].title._text = '\\n\\n'\nfig.suptitle('Box plots for different features of departed and remaining customers', size=24)\nplt.tight_layout()\nrc('axes', labelsize = 14)","cc1062bf":"# mask to create one cornered correlation\nmask = np.zeros_like(df.corr())\nmask[np.triu_indices_from(mask)] = True","815dd9dd":"# heatmap of correlation between features\nplt.figure(figsize=(16,12))\nsns.heatmap(df.corr(), vmin=-1, vmax=1, center=0, annot=True, fmt='.2f', cmap='RdBu_r', mask=mask)\nplt.title('Correlation coefficients between features')\nplt.show()","6b983046":"# strongest feature-target correlation coeffs\ntarget_corr = df.corrwith(df['Churn'])\nmost_corr_columns = target_corr[target_corr.abs() >= 0.3].drop('Churn')\nmost_corr_columns.sort_values()","28abd07d":"# paired scatter and kde plots between features\n\n# axes label size\nrc('axes', labelsize = 28)\n\n# the most corr columns and columns with the charges\ndf_most_corr_charges = df[most_corr_columns.index]\ndf_most_corr_charges[['MonthlyCharges', 'TotalCharges', 'Churn']] = df[['MonthlyCharges', 'TotalCharges', 'Churn']]\n\n# lower corner - paired kde plot, diag - kde plot, upper corner - scatter plot\ng = sns.PairGrid(df_most_corr_charges, hue='Churn', palette='seismic', diag_sharey=False, height=3.5, aspect=1.8)\ng.map_upper(sns.scatterplot, s=70)\ng.map_diag(sns.kdeplot, shade=True)\ng.map_lower(sns.kdeplot)\n\n# plot formatting\ng.add_legend(fontsize=22, title_fontsize=22)\ng.legend.set_bbox_to_anchor([1.05, 0.5])\ng.axes[0][0].set_title('\\n\\n\\n')\ng.fig.suptitle('Customer churn distribution by different paired features', size=44)\nplt.tight_layout()\n\n# back to 14 size of axes labels\nrc('axes', labelsize = 14)","f100bd10":"# ceil every value in pd.Interval\ndef IntervalIndex_ceil(data):\n    intervals = []\n    for interval in data:\n        interval = interval.left, interval.right\n\n        interval_rnd = np.ceil(interval).astype(int)\n        interval_rnd = pd.Interval(*interval_rnd, closed=data.closed)\n        intervals.append(interval_rnd)\n    return pd.IntervalIndex(intervals, closed=data.closed)","e2683fc8":"# assign new column \"bin\" with n binned intervals. Values are being taken from pandas index.\ndef auto_bin(self, df, bins_num=10):\n    self.df_bin = df.copy()\n    min_val = df.index.min()\n    max_val = df.index.max()\n    bins = pd.interval_range(min_val-1, max_val, bins_num)\n    bins = IntervalIndex_ceil(bins)\n    self.df_bin['bin'] = pd.DataFrame(pd.cut(df.index, bins)).values\n    \n    return self","ddd1a89d":"# split months into periods of 7 months\ndf['tenure_bin'] = auto_bin(auto_bin, df[['tenure']].set_index('tenure')).df_bin.bin.values","4715abda":"# mean of MonthlyCharges in each period for departed and remaining clients\nfd = sns.FacetGrid(df.sort_values('tenure_bin'), size=6, aspect=1.6)\nfd.map(sns.pointplot, 'tenure_bin', 'MonthlyCharges', 'Churn', palette='seismic').add_legend(title='Churn')\n\n# plot formatting\nplt.title('Mean of monthly charges of departed and remaining customers by tenure\\n')\nplt.show()","ec0ac2d5":"# monthly charges departed and remaining customers within different price segments\ndis = sns.displot(data=df, x='MonthlyCharges', hue='InternetService', col='Churn', kind='hist', palette='seismic', height=6)\nfiber_charg_min = df[df.InternetService == 2].MonthlyCharges.min()\ndsl_charg_max = df[df.InternetService == 1].MonthlyCharges.max()\n\n# annotations\ndis.axes[0][0].annotate(f'Fiber min charges: {fiber_charg_min}', [fiber_charg_min-5, 300], rotation=90)\ndis.axes[0][0].annotate(f'DSL max charges: {dsl_charg_max}', [dsl_charg_max-5, 300], rotation=90)\n\n# min and max of overlap interval\ndis.axes[0][0].axvline(fiber_charg_min, color='red')\ndis.axes[0][0].axvline(dsl_charg_max, color='red')\n\n# intervals with the highest share of churn\ndis.axes[0][1].axvline(18, color='b')\ndis.axes[0][1].axvline(25, color='b')\ndis.axes[0][1].axvspan(18, 25, color='b', alpha=0.15)\n\ndis.axes[0][1].axvline(42, color='black')\ndis.axes[0][1].axvline(60, color='black')\ndis.axes[0][1].axvspan(42, 60, color='black', alpha=0.15)\n\ndis.axes[0][1].axvline(66, color='r')\ndis.axes[0][1].axvline(107, color='r')\ndis.axes[0][1].axvspan(66, 107, color='r', alpha=0.15)\n\n# plot formatting\ndis.legend.set_bbox_to_anchor([1.1, 0.5])\ndis.axes[0][0].title._text = '\\n\\n' + dis.axes[0][0].title._text\ndis.fig.suptitle('Customer churn distribution by monthly charges with the different internet services')\ndis.fig.tight_layout()","2d41e6ca":"# mean of MonthlyCharges in each period for departed and remaining clients within price segments\nfd = sns.FacetGrid(df.sort_values('tenure_bin'), row='InternetService', size=6.5, aspect=1.6)\nfd.map(sns.pointplot, 'tenure_bin', 'MonthlyCharges', 'Churn', palette='seismic').add_legend(title='Churn')\n# plot formatting\nfd.fig.suptitle('Mean of monthly charges of departed and \\nremaining customers by tenure with the different internet services\\n\\n', size=20)\nfd.fig.tight_layout()","dd054cc4":"# monthly charges departed and remaining customers within different price segments before certain month\n\n# before this month including himself\ntenure = 1\n\n# displot\ndis = sns.displot(data=df[df.tenure <= tenure], x='MonthlyCharges', hue='InternetService', col='Churn', kind='hist', palette='seismic', bins=19)\n\n# plot formatting\ndis.axes[0][0].title._text = '\\n\\n' + dis.axes[0][0].title._text\ndis.fig.suptitle(f'Customer churn distribution by monthly charges with the different internet services\\ntenure <= {tenure}')\ndis.tight_layout()","6de0e9e1":"# creating new \"IS*MC\" feature which is multiplying of InternetService and MonthlyCharges columns\ndf['IS*MC'] = df['InternetService'].map({0: 1, 1:2, 2:3}) * df['MonthlyCharges']","84af0d67":"# IS*MC departed and remaining customers within different price segments\ndis = sns.displot(data=df, x='IS*MC', hue='InternetService', col='Churn', kind='hist', palette='seismic', bins=19)\n\n# plot formatting\ndis.axes[0][0].title._text = '\\n\\n' + dis.axes[0][0].title._text\ndis.fig.suptitle(f'Customer churn distribution by IS*MC (InterneService * MonthlyCharges)')\ndis.tight_layout()","697600aa":"# share of departed customers in each period with different contract types\nfd = sns.FacetGrid(df.sort_values('tenure_bin'), size=6, aspect=1.6)\nfd.map(sns.pointplot, 'tenure_bin', 'Churn', 'Contract', palette='magma').add_legend(title='Contract')\n\n# plot formatting\nfd.axes[0][0].title._text = '\\n\\n' + fd.axes[0][0].title._text\nplt.title('Churn share by tenure with the different contract types\\n')\nplt.show()","63923706":"# the change of share of departing customers with the time running\ndis = sns.displot(data=df, x='tenure', hue='Contract', col='Churn', kind='hist', palette='seismic', multiple='stack', bins=8)\n\n# plot formatting\ndis.axes[0][0].title._text = '\\n\\n' + dis.axes[0][0].title._text\ndis.fig.suptitle('Customer churn distribution by tenure with the different contract types')\ndis.tight_layout()","307da5fc":"# the share of customers with different combinations of InternetService and Contract\nfd = sns.FacetGrid(df, col='Contract', row='InternetService', size=3, aspect=1.6, sharey=False)\nfd.map(sns.kdeplot, 'tenure', 'MonthlyCharges', hue=df['Churn'], palette='seismic', legend=True)\n\n# plot formatting\nhandles = lines.Line2D([None], [None], color='blue'),\\\n    lines.Line2D([None], [None], color='red')\nfd.axes[1][2].legend(handles, [0, 1], title='Churn', bbox_to_anchor=(1.2, 1))\nfd.axes[0][0].title._text = '\\n\\n'\nfd.fig.suptitle('Kdeplot of customer churn distribution by tenure with the different contract types and internet services')\nfd.fig.tight_layout()","091d6f16":"# plotting mean of Churn by a feature\ndef plot_mean(df, *cols, col_func='Churn'):\n    for col in cols:\n        plt.plot(df.groupby(col)[col_func].mean().index, \n                 df.groupby(col)[col_func].mean(), marker='s')\n\ncols = ['Contract', 'OnlineSecurity', 'TechSupport', 'InternetService']\nplot_mean(df, *cols)\n\n# plot formatting\nplt.title('Customer churn by service features and contract type')\nplt.xlabel('Churn')\nplt.ylabel('feature value')\nplt.legend(cols, bbox_to_anchor=(1, 1), title='Column')\nplt.show()","d79ffab2":"# Churn distribution at different value combinations of three features\ndis = sns.displot(data=df, x='tenure', hue='Churn',  row='InternetService', col='Contract', kind='hist', palette='seismic')\n\n# plot formatting\ndis.axes[0][0].title._text = '\\n\\n\\n' + dis.axes[0][0].title._text\ndis.fig.suptitle('Histogram of customer churn distribution by tenure with the different contract types and internet services')\ndis.tight_layout()","fc85124b":"# visual definition of the group with a high probability to have a churn.\n\n# labelsize\nrc('axes', labelsize = 18)\n\n# FacetGrid of scatter plots\nfd = sns.FacetGrid(df,  hue='Churn', col='Contract', row='OnlineSecurity', size=4.5, aspect=1.6, palette='seismic', sharey=False)\nfd.map(plt.scatter, \"tenure\", \"IS*MC\", s=20).add_legend()\n\n# plot formatting\nfd.legend.set_bbox_to_anchor([1.02, 0.5])\nfd.axes[0][0].title._text = '\\n\\n\\n' + fd.axes[0][0].title._text\nfd.fig.suptitle('Scatterplot of customer churn distribution by tenure and IS*MC with the different contract types and online security services', size=22)\nfd.fig.tight_layout()\nrc('axes', labelsize = 14)","d80a1249":"# rules to apply to dataset to reveal the group with a high probability to have a churn\nrules = ((df['Contract'] == 0) & (df['IS*MC'] > 200) & (df['tenure'] < 55) ) | \\\n           ((df['Contract'] == 0) & (df['IS*MC'] < 150) & (df['IS*MC'] > 40) & (df['tenure'] < 7) ) | \\\n           ((df['Contract'] == 0) & (df['IS*MC'] < 22) & (df['tenure'] < 2))\n\n# applying the classification rules to dataset\ndf_rules = df[rules]\n\n# count the precision, recall and f1 score\nrules_cnt = df_rules.Churn.value_counts().sort_index(ascending=False)\na = (rules_cnt[1] + len(df[df.Churn == 0]) - rules_cnt[0]) \/ len(df)\np = rules_cnt[1]\/(rules_cnt[1] + rules_cnt[0])\nr = rules_cnt[1] \/ len(df[df.Churn == 1])\nf1_score_ = 2*p*r \/ (p + r)\nprint(f'accuracy: {a}\\nprecision: {p}\\nrecall: {r}\\nf1_score: {f1_score_}')","bc1deb7d":"# animation of applying the rules. Process of revealing the group with a high probability to have a churn\nimport plotly.graph_objects as go\n\n# frame 1 - initial dataset\nframe_1 = go.Frame(data=go.Scatter(x=df['tenure'], y=df['MonthlyCharges'], marker=dict(color=df['Churn'])), \n                     layout=go.Layout(title_text=\"Applying the classification rules\", yaxis_title=\"MonthlyCharges\"))\n\n# frame 2 - create new feature IS*MC\nframe_2 = go.Frame(data=go.Scatter(x=df['tenure'], y=df['IS*MC'], marker=dict(color=df['Churn'])), layout=go.Layout(yaxis_title=\"IS*MC\"))\n\n# frame 3 - customers who have month-to-month contract only\ndf_3 = df[df['Contract'] == 0]\nframe_3= go.Frame(data=go.Scatter(x=df_3['tenure'], y=df_3['IS*MC'], marker=dict(color=df_3['Churn'])))\n\n# frame 4 - customers who have month-to-month contract and certain IS*MC borders\ndf_4 = df[((df['Contract'] == 0) & (df['IS*MC'] > 200)) | \\\n           ((df['Contract'] == 0) & (df['IS*MC'] < 150) & (df['IS*MC'] > 40)) | \\\n           ((df['Contract'] == 0) & (df['IS*MC'] < 22))]\nframe_4= go.Frame(data=go.Scatter(x=df_4['tenure'], y=df_4['IS*MC'], marker=dict(color=df_4['Churn'])))\n\n# frame 5 - all the rules have been applied\ndf_5 = df[rules]\nframe_5= go.Frame(data=go.Scatter(x=df_5['tenure'], y=df_5['IS*MC'], marker=dict(color=df_5['Churn'])), \n                     layout=go.Layout(title_text=\"Target group (high prob to have a churn)\"))\n\n# creating layout and button\nfig = go.Figure(\n    data=[go.Scatter(x=df['tenure'], y=df['MonthlyCharges'],\n                     mode='markers', name='0', showlegend=True, \n                     marker=dict(color=df['Churn'], colorscale='RdBu_r')),\n          \n         go.Scatter(x=[None], y=[None], mode='markers',\n                       marker=dict(color='darkred'),\n                       showlegend=True, name='1')],\n    layout=go.Layout(\n        width=600, height=400,\n        xaxis=dict(range=[0, 80], autorange=False),\n        yaxis=dict(range=[0, 350], autorange=False),\n        title=\"Scatterplot of customer churn distribution\",\n        title_x=0.5,\n        updatemenus=[dict(\n            type=\"buttons\",\n            y=-0.1,\n            x=0.,\n            font=dict(size=24),\n            buttons=[dict(label=\"Play\", method=\"animate\", args=[None])])]\n    ),\n    frames=[frame_1, frame_2, frame_3, frame_4, frame_5]\n)\n\n# plot formatting\nfig.update_layout(\n    xaxis_title=\"tenure\",\n    yaxis_title=\"MonthlyCharges\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=14,\n        color=\"Black\"\n    ),\n    legend=dict(\n        title='Churn',\n        yanchor=\"top\",\n        y=0.99,\n        xanchor=\"left\",\n        x=1\n    )\n)\n\nfig.show()","9b73c8d7":"# data transform, split to training and testing datasets and calc the feature importances\nX_raw = df.drop(['Churn', 'IS*MC', 'tenure_bin'], axis=1)\ny = df.loc[:, 'Churn']\n# xgboost classifier object definition\nxgb_raw = XGBClassifier(objective='binary:logistic', eval_metric='logloss').fit(X_raw, y)\n# RandomForest classifier object definition\nrf_raw = RandomForestClassifier(random_state=0).fit(X_raw, y)\n# AdaBoost classifier object definition\nada_raw = AdaBoostClassifier(random_state=0).fit(X_raw, y)\n# GradientBoosting classifier object definition\ngb_raw = GradientBoostingClassifier(random_state=0).fit(X_raw, y)\n# ExtraTrees classifier object definition\net_raw = ExtraTreesClassifier(random_state=0).fit(X_raw, y)\n\n# mean of the feature importances of the models\nfeat_imp = np.c_[xgb_raw.feature_importances_, \n                 rf_raw.feature_importances_, \n                 ada_raw.feature_importances_, \n                 gb_raw.feature_importances_, \n                 et_raw.feature_importances_].mean(axis=1)","e6b57267":"# mean of the feature importances of the models\nplt.figure(figsize=(20,6))\nax = plt.axes()\nscatter = plt.scatter(x=X_raw.columns, y=feat_imp, c=feat_imp, cmap=cm.viridis, s=6000*feat_imp, marker='o')\n\n# plot formatting\nplt.title('Mean of the feature importances of the models\\n')\nax.tick_params(axis='x', rotation=90)\nplt.ylabel('Importance (gain)')\nplt.xlabel('Feature')\nplt.axis(ymax=ax.get_ylim()[1] * 1.05)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.grid(alpha=.2, lw=2)\nplt.colorbar()\nplt.show()","443f108f":"# split dataset to train and test data\n\n# get the most useful features\nX = df[['Contract',  'tenure', 'IS*MC', 'TotalCharges']]#[['Contract', 'tenure', 'InternetService', 'MonthlyCharges', 'TotalCharges']]\ny = df['Churn']\n\n# split to training and testing datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2, stratify=y)","e47b9d62":"# initial parameters of models\n# I've already put the best parameters after grid searching in initial parameters\nxgb_params = {'objective': 'binary:logistic',\n              'eval_metric': 'logloss',\n              'use_label_encoder': False,\n              'random_state': 0,\n              'learning_rate': 0.30000000000000004,\n              'max_depth': 1,\n              'n_estimators': 60,\n              'scale_pos_weight': 2.1282657452870217}\n\nada_params = {'random_state': 0, 'learning_rate': 0.1, 'n_estimators': 110}\n\ngb_params = {'loss': 'exponential',\n             'random_state': 0,\n             'learning_rate': 0.1,\n             'max_depth': 1,\n             'n_estimators': 110}\n\nrf_params = {'random_state': 0,\n             'max_depth': 5,\n             'n_estimators': 110,\n             'class_weight': {1: 2.4000000000000004}}\net_params = {'random_state': 0,\n             'max_depth': 7,\n             'n_estimators': 160,\n             'class_weight': {1: 2.700000000000001}}\ndt_params = {'random_state': 0, 'max_depth': 4, 'class_weight': {1: 2.1000000000000005}}\ngnb_params = {}\nlr_params = {'random_state': 0}\n\nmeta_params = {'use_label_encoder': False,\n               'random_state': 0,\n               'objective': 'binary:logistic',\n               'eval_metric': 'logloss',\n               'learning_rate': 0.5000000000000001,\n               'max_depth': 2,\n               'n_estimators': 10,\n               'scale_pos_weight': 1.627497334631252}","69ae41b9":"# the models definition\nxgb = XGBClassifier(**xgb_params)\nada = AdaBoostClassifier(**ada_params)\ngb = GradientBoostingClassifier(**gb_params)\nrf = RandomForestClassifier(**rf_params)\net = ExtraTreesClassifier(**et_params)\ndt = DecisionTreeClassifier(**dt_params)\ngnb = GaussianNB(**gnb_params)\nlr = LogisticRegression(**lr_params)","5ffb6b9e":"# the folds of data for cross-validation or cross-prediction\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)","c602dc03":"# if ON then launch grid searching otherwise not\nGRID_SEARCH_MODE = 'OFF'","804f27a9":"# setting grid search parameters\n\nif GRID_SEARCH_MODE == 'ON':\n    \n    # grid params\n    scale_pos_weight = y_train[y_train==0].size \/ y_train[y_train==1].size\n    xgb_params_grid = {'n_estimators': np.arange(10, 170, 50), \n                       'max_depth': np.arange(1, 5, 1),\n                       'scale_pos_weight': scale_pos_weight\/np.arange(0.9, 1.8, 0.2),\n                       'learning_rate': np.arange(0.1, 1, 0.2)\n                      }\n\n    ada_params_grid = {'n_estimators': np.arange(10, 220, 50),\n                      'learning_rate': np.arange(0.1, 1, 0.2)\n                      }\n\n    gb_params_grid = {\n                      'n_estimators': np.arange(10, 220, 50),\n                      'learning_rate': np.arange(0.1, 1, 0.2),\n                      'max_depth': np.arange(1, 8, 1)\n                     }\n\n    rf_params_grid = {'n_estimators': np.arange(10, 170, 50),\n                      'max_depth': np.arange(1, 8, 1),\n                      'class_weight': [{1: weight} for weight in np.arange(2, 4, 0.1)] + ['balanced']\n                     }\n\n    et_params_grid = {'n_estimators': np.arange(10, 170, 50),\n                      'max_depth': np.arange(1, 8, 1),\n                      'class_weight': [{1: weight} for weight in np.arange(1.5, 3, 0.1)] + ['balanced']\n                     }\n\n    dt_params_grid = {'max_depth': np.arange(1, 8, 1),\n                      'class_weight': [{1: weight} for weight in np.arange(1.4, 3, 0.1)] + ['balanced']}\n    gnb_params_grid = {}\n    lr_params_grid = {'C': np.arange(0.1, 1.1, 0.1),\n                      'class_weight': [{1: weight} for weight in np.arange(2, 3.5, 0.1)] + ['balanced']\n                     }\n\n    # count how many cycles of grid searching ahead\n    def grid_cycles(*params_grid): \n        all_cycles = 0\n        for i, model_param_grid in enumerate(params_grid):\n            if len(model_param_grid):\n                model_cycles = 1\n            else:\n                model_cycles = 0\n            for label in model_param_grid:\n                model_cycles *= len(model_param_grid[label])\n            print(f'{i} model cycles: {model_cycles}')\n            all_cycles += model_cycles\n        print(f'cycles: {all_cycles}')\n\n    grid_cycles(xgb_params_grid, ada_params_grid, gb_params_grid, rf_params_grid, et_params_grid, dt_params_grid, gnb_params_grid, lr_params_grid)","25fd2f8f":"# Grid Search of the models parameters \n\nif GRID_SEARCH_MODE == 'ON':\n\n    # the GridSearchCV objects with a grid-params definition\n    xgb_grid = GridSearchCV(xgb, xgb_params_grid, 'roc_auc', cv=skf)\n    ada_grid = GridSearchCV(ada, ada_params_grid, 'roc_auc', cv=skf)\n    gb_grid = GridSearchCV(gb, gb_params_grid, 'roc_auc', cv=skf)\n    rf_grid = GridSearchCV(rf, rf_params_grid, 'roc_auc', cv=skf)\n    et_grid = GridSearchCV(et, et_params_grid, 'roc_auc', cv=skf)\n    dt_grid = GridSearchCV(dt, dt_params_grid, 'roc_auc', cv=skf)\n    gnb_grid = GridSearchCV(gnb, gnb_params_grid, 'roc_auc', cv=skf)\n    lr_grid = GridSearchCV(lr, lr_params_grid, 'roc_auc', cv=skf)\n\n    # fitting GridSearchCV objects to find the best params for the models\n    xgb_grid.fit(X_train, y_train)\n    ada_grid.fit(X_train, y_train)\n    gb_grid.fit(X_train, y_train)\n    rf_grid.fit(X_train, y_train)\n    et_grid.fit(X_train, y_train)\n    dt_grid.fit(X_train, y_train)\n    gnb_grid.fit(X_train, y_train)\n    lr_grid.fit(X_train, y_train)\n\n    # cross-val scores of the models with the best params\n    print('scores of the models with the best params: \\n',\n          xgb_grid.cv_results_['mean_test_score'].max(),\n          ada_grid.cv_results_['mean_test_score'].max(),\n          gb_grid.cv_results_['mean_test_score'].max(),\n          rf_grid.cv_results_['mean_test_score'].max(),\n          et_grid.cv_results_['mean_test_score'].max(),\n          dt_grid.cv_results_['mean_test_score'].max(),\n          gnb_grid.cv_results_['mean_test_score'].max(),\n          lr_grid.cv_results_['mean_test_score'].max()\n         )\n\n    # combination initial params with the best gridded params\n    xgb_params.update(xgb_grid.best_params_)\n    ada_params.update(ada_grid.best_params_)\n    gb_params.update(gb_grid.best_params_)\n    rf_params.update(rf_grid.best_params_)\n    et_params.update(et_grid.best_params_)\n    dt_params.update(dt_grid.best_params_)\n    gnb_params.update(gnb_grid.best_params_)\n    lr_params.update(lr_grid.best_params_)\n\n    # new exemplars of classes for models with the combination of the best and initial params\n    xgb = XGBClassifier(**xgb_params)\n    ada = AdaBoostClassifier(**ada_params)\n    gb = GradientBoostingClassifier(**gb_params)\n    rf = RandomForestClassifier(**rf_params)\n    et = ExtraTreesClassifier(**et_params)\n    dt = DecisionTreeClassifier(**dt_params)\n    gnb = GaussianNB(**gnb_params)\n    lr = LogisticRegression(**lr_params)","198b176b":"# lists of the models for stacking and their labels, meta alrorithm definition\n\n# models list\nmodels = [xgb, ada, gb, rf, et, dt, gnb, lr]\n\n# metrics list\nmetrics = [roc_auc_score, confusion_matrix, \n    classification_report]\n\n# models labels (for score)\nmodel_labels = ['XGBClassifier', 'AdaBoostClassifier', \n                'GradientBoostingClassifier', 'RandomForestClassifier', \n                'ExtraTreesClassifier', 'DecisionTreeClassifier', \n                'GaussianNB', 'LogisticRegression']\n\n# meta alg\nmeta_alg = XGBClassifier(**meta_params)","fbbf34f2":"# stacking algorithm definition\nstc = Stacking(models, meta_alg, metrics=metrics, meta_weight=1, base_weight=1)\n# fit the base models\nstc.fit_base(X_train, y_train, fit_params={ada: {'sample_weight':y_train.apply(lambda x: 1 if x==0 else 1.3)},\n                                           gb: {'sample_weight':y_train.apply(lambda x: 1 if x==0 else 1.3)}})","ddbe249f":"# grid search of the meta algorithm's params\n\nif GRID_SEARCH_MODE == 'ON':\n\n    # get the base predicts\n    X_train_base = stc.predict_base(X_train)\n\n    # fitting grid search of meta algorithm\n    meta_grid = GridSearchCV(meta_alg, xgb_params_grid, 'roc_auc', cv=skf)\n    meta_grid.fit(X_train_base, y_train)\n\n    # updating meta algorithm params\n    meta_params.update(meta_grid.best_params_)\n    meta_alg = XGBClassifier(**meta_params)","b446ad7a":"# fit meta algorithm\nstc.meta_alg = meta_alg\nstc.fit_meta(X_train, y_train)","0040663c":"# get base models scorev\nstc_base_score = stc.score_base(X_train, y_train, model_labels, print_scores=True)","34e279e5":"# get base models scorev\nstc_base_score = stc.score_base(X_test, y_test, model_labels, print_scores=True)","3ae17baf":"# get stacking score\nstc_meta_score = stc.score(X_train, y_train, print_scores=True)","9fdcba66":"# get stacking score\nstc_meta_score = stc.score(X_test, y_test, print_scores=True)","fa9536bb":"# ROC curve by test data\nfpr, tpr, _ = roc_curve(y_test.values, stc.predict_proba(X_test)[:,1])\nauc_ = roc_auc_score(y_test, stc.predict_proba(X_test)[:, 1])\nax = plt.axes()\nplt.plot(fpr, tpr, 'g-', lw=3, label=f'AUC: {auc_:.3f}')\nplt.plot([0, 1], [0, 1], 'r-.')\n\n# plot formatting\nplt.legend(loc='center right')\nplt.title('Receiver operating characteristic\\n')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.axis(ymax=ax.get_ylim()[1] * 1.05)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.grid(alpha=.7, lw=2)\nplt.show()","c9a15d70":"# feature importances of the models\nfeat_imp = np.c_[xgb.feature_importances_, \n                 ada.feature_importances_,\n                 gb.feature_importances_, \n                 rf.feature_importances_,  \n                 et.feature_importances_].mean(axis=1)\nplt.figure(figsize=(12,6))\nax = plt.axes()\nscatter = plt.scatter(x=X.columns, y=feat_imp, c=feat_imp, cmap=cm.viridis, s=4000*feat_imp, marker='o')\n\n# plot formatting\nplt.title('Mean of the feature importances of the models\\n')\nplt.ylabel('Importance (gain)')\nplt.xlabel('Feature')\nplt.axis(ymax=ax.get_ylim()[1] * 1.05)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.grid(alpha=.3, lw=2)\nplt.colorbar()\nplt.show()","d7fce5b7":"So using this common classification rules the best we can do is to designate the group with 75% of departed customers but we will have to cover almost the same number of remaining customers and mark them as the potential leaving customers too. The vast majority of this group is the customers who signed month-to-month contract and using fiber optic internet service.","2d554813":"**Share of churn before and after a year**\n* More than a half of leaving customers leave the company in the first year, this is very important time for client to get used to the company;\n* The vast majority of remaining customers are already more than a year a part of the company.","aa16a3de":"**Lines with a gap in the \"TotalCharges\" column**\n* All the clients with a tenure same to 0 have a gap in \"TotalCharges\" column. This clients also have a Churn same to 0;\n* All these customers just signed their contracts. This is a start of their first month. Until this they did not paid even once;\n* We can't judge yet will they depart or remain after 1st month. I will ignore them in further;\n* If that group had have the departed customers, it would worth to analyze them but it's not.","722a3fd1":"### Train data","b4cfd9ad":"## Hypotheses justification","dfd001ac":"* Contract is definitely most important feature;\n* When using well-fitted (generalized) models TotalCharges already seems like more useless feature.","1df3f898":"# 2. Dependencies research and form the hypotheses","7ad1d51c":"**Selection system**<br>\nFor a better understanding, let's turn the choice of maximizing precision or recall in answer to the question which of the following is more important in this case:\n* prediction reliability (precision).\n* the number of positive outcomes found among their total number (recall).\n\nTo answer, it necessary to ask the following two questions: \n1. What are the consequences in this case of low precision?<br>\nThat is, what will happen if you say about a positive outcome, but in fact a negative one will happen?\n2. What are the consequences in this case of low recall?<br>\nThat is, what will happen if not all positive outcomes are marked as positive?\n\nWhen answering which question the consequences turned out to be worse, that metric should be maximized.. <br>","9621a6bf":"**Justification of using the models**<br>\nDeparted and remaining clients cannot be separated precisely, we can only to designate the group with a high probability to have a churn. So there is no clear clusters. This cause we may be absolutely sure k neighbors classifier will be useless here. Several general classification rules usual make it possible to designate this group, so all the models using decision tree with a small depth may be useful. Use gradient descent just to get min of  Logistic regression and gaussian naive bayes may also set one generalized rule to separate departed and remaining customers. The most important thing is to select parameters of the models so as to make them generalized otherwise it'll be too easy to make them overfitted under training data.<br><br>\n**Models list**\n* XGBClassifier\n* AdaBoostClassifier\n* GradientBoostingClassifier\n* RandomForestClassifier\n* ExtraTreesClassifier\n* DecisionTreeClassifier\n* GaussianNB\n* LogisticRegression\n\n**Features**<br>\nThis work the most important features will be used only:\n* Contract\n* TotalCharges\n* tenure\n* IS*MC","4e0b28d5":"**Feature importances**<br><br>\nThere are 5 the most important features. The results are very similar to those which have been found during the EDA.\n* Contract;\n* TotalCharges;\n* tenure;\n\n* $\\text{IS*MC} \n\\begin{cases} \n\\text{MonthlyCharges} \\\\\n\\text{InternetService}  \n\\end{cases}$","3467ee40":"In the first months, MonthlyCharges does not affect to the share of leaving customers, because the distribution there is even, but when the tenure becomes more than 30 and monthly charges more than 60 (in average), it starts to affect to the share of leaving customers, even those who have a two year contract start to leave.","ac9a713b":"# 3. Models building to predict a churn","a4e178ed":"### Test data","0dc5ad33":"## Base models","7094d222":"**Third hypothes - BUSTED**\n* The share of those who leave the company after many months becomes fewer because by that time almost no one left of customers who have signed a month-to-month contract - and they are exactly who are leaving the company. So it's happening not because the customers getting used to the company - just almost all the clients with the month-to-month contract are already departed;\n* The customers with the very begin are already sure about want they remain for a long time (and they are signing two or one year contract) or to take a closer look (and they are signing month-to-month contract). The latter, almost all leave the company within 4 years;\n* The monthly charges inevitably grows within the price segment, and when it becomes more than the critical mark (depending on the Internet service and the contract), those who have a one and two year contracts starting to leave the company too. This may be a problem very soon, but there are very few of them at the moment;\n* The most resistant customers are those with one year and two year contracts and no internet connection. No wonder, because the price for their segment hardly grows over time.\n\n**Alternative conclusion**<br>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0In the first 4 years almost all the customers who had a month-to-month contract leave. After this moment, people with a one-year and two-year contracts start to leave, and soon this may become a problem.","bd94a919":"## Stacking","fe1ac4fb":"**Second hypothes - APPROVED** <br>\n* Combined feature has been created clearly with no overlapping;\n* One combined feature instead of two allows to increase the performance and separate departed and remaining customers using fewer rules.","7df8b2bf":"**Churn distribution conclusions**\n* Seniors (Senior Citizen==1) more likely to have a churn;\n* Customers who have no partner (Parnter==No) have more chances to have a churn. It may be bound to lonely ppl moving new house more often;\n* If client has no dependents (Dependents==No) the probability to have a churn is twice higher. There may be a same reason;\n* More than a half of departed customers leaving the company a the first months (tenure<\\?);\n* About 50% of customers who have an optic cable (InternetService==\"Fiber optic\") more likely to have a churn.\n* When a client has an ability to connect different services but they are not connected (OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies == No) he more likely to have a churn;\n* Short term contract (Contract==Month-to-month) makes customers to have a churn more often;\n* Clients using a Paperless Billing (PaperlessBilling==Yes) more likely to have a churn. Maybe interface isn't friendly enough or something;\n* People using an Electronic check (PaymentMethod=\"Electronic check\") more likely to have a churn;\n* Departed clients have an average of Monthly Charges more than the remained clients;\n* As the Total Charges becomes higher - share of departed customers becomes lower. It's simply a reflection of how much time the client has spent with the company.","6c0d7faf":"**What do we know (about searching of leaving customers)**\n* We may use \"IS*MC\" column to take into consideration InternetService and MonthlyCharges together with no without quality degradation;\n* The rule Contract=0 gives the highest precision and recall - it should be used;\n* Tenure affects hard to the share of leaving customers who signed Contract=0 (month-to-month);\n* We may also use the features OnlineSecurity=0 and TechSupport=0 to get a better precision but it may to decrease a recall.","f210bea2":"**Churn has been happen (1 - Churn, 0 - Remain).** <br>\n* At low precision many customers who would remain will be marked as potential departed customes;\n <br><br>Potential consequences:\n    - a waste of resources on the customers retaining who would have remained;\n    - a drop in profits because an increase in proceeds from retained customers may not cover the cost of retaining them.<br><br>\n\n* At low recall many customers who would leave will be marked as potential remained customers.\n <br><br>Potential consequences:\n    - a drop in profits. Lost customers - lost proceeds;\n    - waste of resources to attracting new clients which may to leave too. Easier way is to retain the old customers.<br><br>\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0As we can see the company may lose the profit in both cases and it's hard to say yet which way implies more losing because we still don't know the capabilities of the model. A way out is to try to keep maximum of both a precision and a recall. At the start we can say that all the customers are potential departed, in this case a recall will be 100% but a precision will be about 26% which is so low. So we have to try to increase a precision as maximum as it possible so as not to lose much of a recall.<br><br>\n**There is no sense to get maximum of precision or recall - it's bad in both cases. It necessary to achieve maximum when considering these metrics together - f score.**","a8332d38":"**Feature importance**\n* The most useful features are Contract, tenure, OnlineSecurity, TechSupport and InternetService;\n* Columns MonthlyCharges and TotalCharges may also be useful.","0fec5daf":"## Identifying the feature importances through EDA","bd83c0fe":"The models are well-fitted and they have good generalize as the results of metrics for train and test data are almost the same.","c39748d3":"Consider this distributions using scatter plot.","1b49b059":"**Monthly Charges within the price segments**\n* Monthly charges getting higher over the time within each segment but for those who have no internet service this effect is weak;\n* In each period, the average monthly charges is almost the same for both of the departed and the remaining customers. It means that the share of departed customers doesn't depend on MonthlyCharges within of one price segment. Only very transition from one price segment to another affects to the share of departed customers. In other words, the price of the segment as a whole has an impact.","1e647a51":"**Typos and errors**\n* Column \"TotalCharges\" should be a float type and not the object including strings; \n* Column \"TotalCharges\" has a strings with a gap. It has to be fixed to convert that column to float type.","d73f2c55":"### Train data","c7ee2368":"<font size=4>**Let's try to reveal more complicated rules (combinations of more than two features), which will allow to separate 1 and 0 classes from each other better.**","a5aae58d":"**The influence of time**<br>\n* Over the time, customers with a month-to-month contract leave the company less and less, but the real reason is that there almost no one left to depart from this group;\n* Clients with a one and two year contract behave another, over the time the share of departing customers becomes higher but their quantity still very small compared with the remaining customers;\n* After a long time (tenure > 60) the vast majority of remaining customets have two year contracts;\n* The main task is to not lose the customers who have month-to-month contract. The most efficient way to do it is to try to lead them to sign two-year contract otherwise they will run away anyway.","7573c2ba":"You may find the entire stacking class here - [STACKING](https:\/\/github.com\/RuslanOraev\/handmade-data-science\/blob\/main\/my-own-library\/ensemble_stacking.py)","9345e32a":"**Separating**\n* Customers who are paying more than 60 (67.75 actually) are mosly those who are using a Fiber optic (InternetService==2) but there is an interval from 67 to 95 when it isn't - it's the  clients who are using DSL;\n* Because of this interval we CAN NOT just say that \"vast majority of leaving customers are the people who are paying more than 67 every month\" because in this case we will cover many of the remaining customers who use DSL **BUT** if we say \"Departed customers are the people who are using Fiber optic\" it will be a better way to separate leaving and remaining clients;\n* To get maximum of precision it is necesssary to consider highlighted areas at the second figure (Churn = 1). To do this, it necessary to separate fiber optic (InternetService==2) and DSL (InternetService==1) gropus using MonthlyCharges (the interval from 67 to 95 prevents to do it). If multiply MonthlyCharges and InternetService it looks like it will work.\n\n**Price segments**\n* So we have 3 price segments: *low-cost* with no Internet (InternetService==0), *medium-cost* with DSL (InternetService==1) and *high-cost* with fiber optic (InternetService==2);\n* As the price segment becomes higher the share of departed customers becomes higher too. Due to the fact that the most of people leave the company from the high-cost segment being dissatisfied by him, departed customers pay more per month in average.\n\n**First hypothes - APPROVED** <br>\n* As we can see it's really the vast majority of people leaving the company with the monthly charges higher than 67.75 and almost all of them have InternetService with a fiber optic cable (InternetService==2);\n* High-cost segment gives the best precision and recall when looking for departed (Churn = 1) customers.","b3c1c6be":"**Based on \"Churn\" distributions let's match string values to numbers - put the biggest and smallest shares on the opposite sides but still trying to save the common sence (all the \"No\" values is 0, \"Yes\" is 1 and so on). By this way we can see a correlation between features and target better in further.**\n\n&nbsp;&nbsp;&nbsp;&nbsp; `customerID` \u2013 client's id<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `gender` \u2013 sex (male - 1\/female - 0)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `SeniorCitizen` \u2013 customer is senior citizen (1, 0)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `Partner` \u2013 married (Yes - 1, No - 0)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `Dependents` \u2013 does the client have dependents (Yes - 1, No - 0)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `tenure` \u2013 how many months a person has been a customer of the company<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `PhoneService` \u2013 telephone service has been activated (Yes - 1, No - 0)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `MultipleLines` \u2013 multiple phone lines connected (Yes - 1, No - 0, No phone service - 2)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `InternetService` \u2013 type of internet connection (DSL - 1, Fiber optic - 2, No - 0)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `OnlineSecurity` \u2013 the online security service has been activated (Yes - 1, No - 0, No internet service - 2)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `OnlineBackup` \u2013 the online backup service is connected (Yes - 1, No - 0, No internet service - 2)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `DeviceProtection` \u2013 the client have equipment insurance (Yes - 1, No - 0, No internet service - 2)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `TechSupport` \u2013 the technical support service is connected (Yes - 1, No - 0, No internet service - 2)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `StreamingTV` \u2013 streaming TV service is connected (Yes - 1, No - 0, No internet service - 2)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `StreamingMovies` \u2013 the streaming cinema service activated (Yes - 1, No - 0, No internet service - 2)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `Contract` \u2013 client's contract type (Month-to-month - 0, One year - 1, Two year - 2)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `PaperlessBilling` \u2013 the client use paperless billing (Yes - 1, No - 0)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `PaymentMethod` \u2013 payment method (Electronic check - 3, Mailed check - 2, Bank transfer (automatic) - 1, Credit card (automatic) - 0)<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `MonthlyCharges` \u2013 monthly payment at the moment<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `TotalCharges` \u2013 the total amount that the client paid for the services for the entire time<br>\n&nbsp;&nbsp;&nbsp;&nbsp; `Churn` \u2013 there has been a churn (Yes - 1 or No - 0)<br>","023fc87d":"# 1. Data description","33b84fcb":"You may find the entire stacking class here - [STACKING](https:\/\/github.com\/RuslanOraev\/handmade-data-science\/blob\/main\/my-own-library\/ensemble_stacking.py)","24311068":"# Conclusions\n* The most important thought is what we cannot define the classification rules to completely separate leaving and remaining customers. All that remains is to identify a group with a high probability of churn - \"target group\";\n    \n    * Target group contains about 75% of all the customers who leaving the company;\n\n    * Precision is about 60%;\n\n    * ROC-AUC is about 82%;\n\n* There may be 4 features used instead of 19 with no quality degradation of the models which gives good performance.\n    * Contract;\n    * TotalCharges;\n    * tenure;\n    * $\\text{IS*MC} \n    \\begin{cases} \n    \\text{MonthlyCharges} \\\\\n    \\text{InternetService}  \n    \\end{cases}$\n<br><br>\n* The vast majority of leaving customers have a month-to-month contract (88% of all departed customers). About 60% of them (exactly among those who leaving the company) leaving the company in the first year. In about 4 years almost all of them (94%) leave. After 5 years the share of remaining customers with a month-to-month contract is about 7% as at the first year this value is 92%;\n    \n* As the time goes on, fewer customers leave, but the reason is described in the paragraph above - in 4 years almost everyone who had a monthly contract left but since this moment those who have one-year and two-year contract start to leave too and the every month the number of them growing up (but for this moment this number very small).\n    \n* About 70% of leaving customers who have month-to-month contract have an internet service with a fiber optic cable - \"high-cost segment\". They are paying much more than those who have DSL or have no internet service at all;\n    \n* Everybody who sign one-year or two-year contract pretty sure they whant to be client of the company. The task is to try to transfer as much as possible the clients to one-year or two-year contract until they leave. Probably there a sense to apply some discount for a few first month for those who have month-to-month contract and using internet service with a fiber optic cable if they will move to one-year or two-year contract;\n\n* As monthly charges getting higher - the customers more likely to have a churn but it is the price segment that exactly is important here (depends on Internet service). Within the price segments monthly charges does not affect to the share of leaving customers.\n    * InternetService = 'Fiber optic' - high-cost segment;\n    * InternetService = 'DSL' - medium-cost segment;\n    * InternetService = 'No' - low-cost segmet;\n\n**Additional data**<br>\nI would request for an additional information.\n* Records of every client for every month to consider the problem in the dynamics;\n* Feature \"number of reports to technical support\" could be useful;\n* Feature \"monthly traffic\" could be useful;","cc51728b":"This overlap in interval from 67 to 95 occurs because over time people pay more and those who have DSL Internet with the large tenure pay the same as those who have just connected a fiber optic. So if we look at the figure above with a low tenure we can see that overlap had almost vanished. Plus, the average monthly charges of those who use DSL with a large tenure is comparable to those who use a fiber optic and have just connected, which proves the above mentioned once again.\n\n**Feature engineering**<br>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Instead of using many rules like \"if (InternetService==2 & MonthlyCharges > 67 & MonthlyCharges < 107) | (InternetService==1...\" there is the way to use MonthlyCharges and InternetService as one feature by multiplying them if we change InternetService values from 0, 1, and 2 to 1,2 and 3 to not loose the information.","3e6acaaf":"**Separating**<br>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Now we can select specific intervals of the price segments in a simpler way with no side effects (like overlapping that makes precision worse).","597cdf55":"**Correlation conclusions**\n* It is clear from the graph that most of the customers leaving the company are concentrated where the values of the columns \"Contract\", \"TechSupport\", \"OnlineSecurity\" take zero values (No), as well as where \"tenure\" does not exceed 12 months, and \"InternetService\" is equal to two (Fiber optic);\n* In no case of considering pairs of features it is possible to separate one class from another precisely. The most successful pairs of features, at certain values, only equalize the number of objects of classes 1 and 0, which will have a bad effect on the accuracy of the model;\n* The zero value of the \"Contract\" column allows you to cover the largest number of objects of the first class, in the future it is necessary to push from this;\n* The best combinations of pairs, at first glance, are Contract-tenure, tenure-InternetService, Contract-InternetService, Contract-OnlineSecurity, Contract-TechSupport, InternetService-MonthlyCharges;\n* It is necessary to identify only the most general rules (a small depth for training models) for separating 1 and 0 classes, otherwise, having piled dozens of conditions, you can greatly get overfitted.\n\n## Hypotheses\n\n**Main hypotheses**\n1. Too high charges (MonthlyCharges>60) for those customers who are using optic (InternetService==\"Fiber optic\"==2) cable making them leave (about 40% clients that are using optic cable had departed);\n2. MonthlyCharges and InternetService features may be combined;\n3. As the time running customers getting used to the company more and more and the monthly charges doesn't already affect so much to the people who have impressive tenure (tenure>?);\n4. Services connection like TechSupport and OnlineSecurity and signing one year or two year contracts are just reflection of solid customer's intention to remain in the company. Customers who signed two year contracts (Contract=\"Two year\") more like to have remained but we still have to lead him to sign it using some influence levers (discounts or something). So if the client choosing Month-to-month (Contract=\"Month-to-month\") contract - he is not sure yet wants he to leave or to remain.\n\n**Reasons for forming hypotheses**\n1. InternetService affects to MonthlyCharges so hard that the most majority of ppl who leaving the company have a fiber optic cable and they are paying much more money in average than those who have the DSL or who have't internet service at all;\n2.  Correlation coeff between MonthlyCharges and InternetService features is 0.9;\n3. If you look at the graph tenure-MonthlyCharges you will see that the customer's charges as more higher as tenure more higher but the share of people who are leaving the company getting lower;\n4. Customers who have some service (TechSupport or OnlineSecurity) connected or signed one\/two year contract more likely to have remained. As an example, when InternetService is connected it leads the customer to leave the company, so in that case connected service is the reason of churn.","3aa5e196":"### Test data","1c8f74d1":"**Feature importance**\n* The feature is more useful than distribution of departed and remaining clients is more different;\n* The features like \"gender\", \"Partner\", \"Multiple Lines\" and \"Paperless Billing\" where the distributions for remaining and churn clients are the same are affectless to the target feature.","1b5a1306":"# Coursework ML Beginner\n<br>\n\n**NB!** The code should be with comments for each logical block of the code. Otherwise, the work will **not** be accepted. <br><br>\nCompleting coursework may require the use of additional libraries. Data preprocessing may be required.\n\n<br><br>\n**Work description:**\n&nbsp;&nbsp;&nbsp;&nbsp;\n<br><br>\n&nbsp;&nbsp;&nbsp;&nbsp; Any business wants to maximize the number of customers. To achieve this goal, it is important not only to try to attract new ones, but also to retain existing ones. Retaining a client will cost the company less than attracting a new one. In addition, a new client may be weakly interested in business services and it will be difficult to work with him, while old clients already have the necessary data on interaction with the service.\n <br>\n\n&nbsp;&nbsp;&nbsp;&nbsp; Accordingly, predicting the churn, we can react in time and try to keep the client who wants to leave. Based on the data about the services that the client uses, we can make him a special offer, trying to change his decision to leave the operator. This will make the task of retention easier to implement than the task of attracting new users, about which we do not know anything yet.<br>\n\n&nbsp;&nbsp;&nbsp;&nbsp; You are provided with a dataset from a telecommunications company. The data contains information about almost six thousand users, their demographic characteristics, the services they use, the duration of using the operator's services, the method of payment, and the amount of payment.\n<br>\n\n&nbsp;&nbsp;&nbsp;&nbsp; The task is to analyze the data and predict the churn of users (identify people who will and will not renew the contract). The work should include the following mandatory items:\n1. Data description * (with the calculation of basic statistics) *,\n2. Dependencies research and form the hypotheses,\n3. Models building to predict a churn * (with justification of the choice of a particular model) * based on tested hypotheses and identified relationships,\n4. Models quality comparison.\n\nIn the second section, there must be a justification of hypotheses, a detailed description of the identified relationships, as well as their visualization.\n<br>In the fourth, the general conclusions of the work should be formulated additionally.\n\n<br><br>\n\n_________\n\n[Codebook](#Codebook) <br>\n[1. Data description](#1.-Data-description)<br>\n[2. Dependencies research and form the hypotheses](#2.-Dependencies-research-and-form-the-hypotheses)<br>\n[3. Models building to predict a churn](#3.-Models-building-to-predict-a-churn)<br>\n[4. Models quality comparison](#4.-Models-quality-comparison) <br>","1c90bc95":"# 4. Models quality comparison","f86b0c0c":"## Precision\/recall maximization","86b00138":"**The influence of price**<br>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0The departed customers really paying more than the remaining customers (for departed customers a mean of MonthlyCharges always higher than 60) .","6fe7c262":"Before we start we have to choose what do we want to maximize - precision or recall.","2f534f42":"## Identifying the feature importances using Ensemble Methods","e10c225c":"**Customer's intentions**\n* If you look at the InternetService line, you can see that it is the connected service (DSL and Fiber optic) that makes people leave - the reason is in the service itself (quality or cost), and when the service is disabled, fewer people leave;\n* Unlike the InternetService line, in all other cases, it is the lack of online security, tech support service or longer-term contract is the reason for leaving - that is, there are no problems with the service itself and this is cannot be the reason of churn. It means that those who connect one of this services (or conclude a longer-term contract) are simply tuned in to a longer-term cooperation - this shows the client's intention to stay with the company longer;\n* The unsurest customers (those who we are searching for) are those who have a month-to-month contract, and not a single service is connected, provided they have the Internet. \n\n**Fourth hypothes - APPROVED**\n* The customers who have one or two year contracts (Contract==1 | Contract==2), connected tech support (TechSupport==1) and online security(OnlineSecurity==1) services are more sure about they want to a long-term cooperation with the company;\n* This can be used to improve the precision of searching for departed customers, so the highest value may be achieved when Contract==0 & OnlineSecurity==0 & TechSupport==0 & InternetService==2;\n* Those who have TechSupport==2 and OnlineSecurity=2 (no internet service) satisfied with the pricing policy, so the share of departed customers there is so little.","659cfd92":"* There is 5976 customers;\n* 1587 (26.5%) customers are already departed;\n* 4389 are remaining customers.","58a62272":"**Info**\n* There's no any null values;\n* customerID and old_index should be dropped - they can not affect to the target feature.","1fe31da3":"Let's check all the unique values of columns is correct and figure it out if there a reason to fix it.","ad7d3726":"**Feature importances**\n* The most important feature is \"Contract\";\n* The second feature on importance is \"InternetService\" which may be combined with \"MonthlyCharges\" to become \"IS*MC\" feature;\n* The third feature on importance is \"tenure\";\n* Using OnlineSecurity and TechSupport features already looks like unnecessary;\n* We may use more features and rules to but this is overfitting.\n\n**The classification rules to define the group with a high probability to have a churn**<br>\n\u00a0\u00a0\u00a0\u00a0There is the optimum common rules we can use to define the group with a high probability of churn.\n* Contract==0 & IS*MC > 200 & tenure < 55;\n* Contract==0 & (40 < IS*MC < 150) & tenure < 7;\n* Contract==0 & IS*MC < 22 & tenure < 2."}}