{"cell_type":{"2a5e2f45":"code","739098c3":"code","22af9f68":"code","dc924f5b":"code","fdd60922":"code","44c906d5":"code","7d3705f2":"code","dd48cf52":"code","f1b1ab38":"code","12f272f9":"code","39d71c3e":"code","73cd0b9b":"code","384ad8f1":"code","9c0cd452":"code","e8ef3eb4":"code","669d4946":"code","15c022b3":"code","f5c4f10d":"code","46a5f7b7":"code","85b88d91":"code","5db0d74b":"code","c4b8128c":"code","7f63b9df":"code","74af939c":"code","d0d62f1a":"code","c7cedf5e":"code","99cc1d3c":"code","d4503171":"code","1d09bf28":"code","28a4dfe4":"code","fd225e15":"code","3bff7b99":"code","558890b5":"code","6ba7d21c":"markdown","1d084dff":"markdown","c22697d2":"markdown","a424d970":"markdown","5676382f":"markdown","9a9dd6d4":"markdown","9cf5d69a":"markdown"},"source":{"2a5e2f45":"from tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import metrics\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import models\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\nWIDE = (12,6)\nWIDER = (16,8)\nLONG = (8, 12)\nLONGER = (8, 18)\nplt.rcParams['figure.figsize'] = WIDER\n\n%matplotlib inline","739098c3":"df = pd.read_csv('..\/input\/consumer-reviews-of-amazon-products\/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv')","22af9f68":"df.head()","dc924f5b":"df.columns","fdd60922":"df_new = df[[ 'name', 'brand',\n       'categories', 'primaryCategories', \n        'reviews.doRecommend',\n       'reviews.numHelpful', 'reviews.rating', \n       'reviews.text', 'reviews.title', 'reviews.username']]","44c906d5":"df_new.head(3)","7d3705f2":"df_reviews = df[[ 'reviews.rating','reviews.text', 'reviews.title',]]","dd48cf52":"\ndf_reviews.head(3)","f1b1ab38":"df_reviews.shape","12f272f9":"df_classify = df_reviews[df_reviews[\"reviews.rating\"].notnull()]\ndf_classify[\"sentiment\"] = df_classify[\"reviews.rating\"] >= 4\ndf_classify[\"sentiment\"] = df_classify[\"sentiment\"].replace([True , False] , [\"Postive\" , \"Negative\"])\n\n# Lets count positive and negative review\ndf_classify[\"sentiment\"].value_counts().plot.bar()","39d71c3e":"# Cleaning the texts\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []","73cd0b9b":"for i in range(0, 5000):\n    review = re.sub('[^a-zA-Z]', ' ', df_reviews['reviews.text'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus.append(review)","384ad8f1":"corpus=pd.DataFrame(corpus, columns=['Reviews']) \ncorpus.head()","9c0cd452":"result=corpus.join(df_reviews[['reviews.rating']])\nresult.head()","e8ef3eb4":"def formatt(x):\n    if x < 4:\n        return 0\n    if x >= 4:\n        return 1\nvfunc = np.vectorize(formatt)\n\nresult['sentiment'] = result['reviews.rating'].map(vfunc)","669d4946":"result.head(3)","15c022b3":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\ntfidf.fit(result['Reviews'])","f5c4f10d":"from sklearn.model_selection import train_test_split\nX = tfidf.transform(result['Reviews'])\ny = result['sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\nprint(X_train.shape,y_train.shape)\nprint(X_test.shape,y_test.shape)","46a5f7b7":"prediction =  {}","85b88d91":"from sklearn.naive_bayes import BernoulliNB\nmodel1 = BernoulliNB().fit(X_train , y_train)\ny_pred_bernoulli = model1.predict_proba(X_test)\n\nprediction['BernoulliNB'] = model1.predict_proba(X_test)[:,1]\nprint(\"BernoulliNB Accuracy : {}\".format(model1.score(X_test , y_test)))\n","5db0d74b":"from sklearn.naive_bayes import MultinomialNB\nmodel2 = MultinomialNB().fit(X_train , y_train)\ny_pred_multinomial = model2.predict_proba(X_test)\n\nprediction['Multinomial'] = model2.predict_proba(X_test)[:,1]\nprint(\"Multinomial Accuracy : {}\".format(model2.score(X_test , y_test)))\n","c4b8128c":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(solver='lbfgs', multi_class='auto' , max_iter=4000)\nlogistic = logreg.fit(X_train , y_train)\nprediction['LogisticRegression'] = logreg.predict_proba(X_test)[:,1]\n#y_pred_logistic = logreg.decision_function(X_test)\ny_pred_logistic = logreg.predict(X_test)\nprint(\"Logistic Regression Accuracy : {}\".format(logreg.score(X_test , y_test)))","7f63b9df":"from sklearn.svm import SVC\nsvcreg = SVC(kernel = 'rbf', random_state = 4)\nsvc = svcreg.fit(X_train , y_train)\nprediction['SVC'] = logreg.predict_proba(X_test)[:,1]\n\ny_pred_svm = svcreg.decision_function(X_test)\n\nprint(\"SVC Accuracy : {}\".format(svcreg.score(X_test , y_test)))","74af939c":"from sklearn.metrics import roc_curve,auc\ncolors_counter = 0\ncolors_code = ['b', 'g', 'y', 'm', 'k']\nfor model, predicted in prediction.items():\n    fpr, tpr, thresholds = roc_curve(y_test, predicted)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, colors_code[colors_counter], label='%s: AUC %0.2f'% (model,roc_auc))\n    colors_counter += 1\n\nplt.rcParams['figure.figsize'] = WIDER\nplt.title('Classifiers Comparaison With ROC')\nplt.legend(loc='lower right')\n#plt.plot([0,1],[0,1],'r--')\n#plt.xlim([-0.1,1.2])\n#plt.ylim([-0.1,1.2])\nplt.xlabel('False Positive Rate -->')\nplt.ylabel('True Positive Rate -->')\nplt.show()","d0d62f1a":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\ntfidf.fit(result['Reviews'])","c7cedf5e":"\nfrom wordcloud import STOPWORDS\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nstopwords = set(STOPWORDS)\nstopwords.remove(\"not\")\n\ncount_vect = CountVectorizer(min_df=2 ,stop_words=stopwords , ngram_range=(1,2))\ntfidf_transformer = TfidfTransformer()\n\ndf_cv = count_vect.fit_transform(result[\"Reviews\"])        \n\ndf_tf = tfidf_transformer.fit_transform(df_cv)","99cc1d3c":"from sklearn.feature_extraction.text import CountVectorizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nimport re\n\nmax_fatures = 30000\ntokenizer = Tokenizer(nb_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(result['Reviews'].values)\nX1 = tokenizer.texts_to_sequences(result['Reviews'].values)\nX1 = pad_sequences(X1)\n\nY1 = pd.get_dummies(result['reviews.rating']).values\nX1_train, X1_test, Y1_train, Y1_test = train_test_split(X1,Y1, random_state = 42)\nprint(X1_train.shape,Y1_train.shape)\nprint(X1_test.shape,Y1_test.shape)\n\nembed_dim = 150\nlstm_out = 200\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X1.shape[1]))\nmodel.add(LSTM(lstm_out))\nmodel.add(Dense(5,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","d4503171":"batch_size = 512","1d09bf28":"model.fit(X1_train, Y1_train, epochs = 50, batch_size=batch_size, validation_split=0.3 , verbose = 2)","28a4dfe4":"history = model.history","fd225e15":"score,acc = model.evaluate(X1_test, Y1_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","3bff7b99":"print(history.history.keys())","558890b5":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","6ba7d21c":"### Create TF-IDF","1d084dff":"## Text Cleaning or Preprocessing","c22697d2":"## Plot ROC and compare AUC","a424d970":"## Applying Deeplearning LSTM","5676382f":"## Read Data","9a9dd6d4":"## Applying SkLearn Algorithems","9cf5d69a":"## Classifying text as Postive and Negative"}}