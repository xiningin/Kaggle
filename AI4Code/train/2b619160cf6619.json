{"cell_type":{"c2051fea":"code","9726a6e7":"code","d5e68279":"code","d51a04d5":"code","87c2b8ae":"code","2fed81b5":"code","12fdaf67":"code","39e8eb69":"code","69cd1b2b":"code","a9200a4f":"code","8dd39a24":"code","d4887722":"code","db4c5592":"code","59ae3b92":"code","2c8790f8":"code","c5a0b6ce":"code","eb476658":"code","77880435":"code","fccee6f8":"code","e49bc91e":"code","bf726c24":"code","791f4490":"code","f7d92f2f":"code","fa3b5ad5":"code","a645e6ac":"code","597bdd5e":"code","d79dab93":"markdown","c0c1e115":"markdown","6adb1440":"markdown","f717a1d9":"markdown","0470b915":"markdown","b5d6299b":"markdown","8b50a90f":"markdown","39a9c81f":"markdown","8a6f2fc4":"markdown","b7536768":"markdown","f318cbc5":"markdown","437ab5c5":"markdown","47150778":"markdown","741289a9":"markdown","5a913f6b":"markdown","c8648715":"markdown"},"source":{"c2051fea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# Set random state for reproducibility\nnp.random.seed(42)\n\nimport os\n# Any results you write to the current directory are saved as output.\n# Look at data directory:\nprint(os.listdir('\/kaggle\/input\/bank-marketing-prediction\/'))","9726a6e7":"df_train = pd.read_csv('\/kaggle\/input\/bank-marketing-prediction\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/bank-marketing-prediction\/test.csv')","d5e68279":"df_train.info()","d51a04d5":"df_test.info()","87c2b8ae":"import seaborn as sns","2fed81b5":"sns.countplot('y', data=df_train);","12fdaf67":"sns.boxplot(y='age', x='y', data=df_train);","39e8eb69":"sns.boxplot(y='duration', x='y', data=df_train);","69cd1b2b":"sns.jointplot(x='age', y='duration', data=df_train[(df_train['duration'] < 1000) & (df_train['age'] < 60)], kind='hex');","a9200a4f":"sns.barplot(y='y', x='month', data=df_train);","8dd39a24":"# List of features that I want to remove\nremove_features = ['SampleId', 'y']#, 'day', 'month']","d4887722":"# Get the list of categorical features\ncategorical_features = list(df_train.dtypes[df_train.dtypes == 'object'].index)\ncategorical_features = [c for c in categorical_features if not c in remove_features]\n\ncategorical_features","db4c5592":"# Get the list of numerical features\nnumerical_features = list(df_train.dtypes[df_train.dtypes != 'object'].index)\nnumerical_features = [c for c in numerical_features if not c in remove_features]\n\nnumerical_features","59ae3b92":"# Plot the distributions of all numerical features\nsns.pairplot(data=df_train[:500], vars=numerical_features, hue='y', diag_kind='hist');","2c8790f8":"import matplotlib.pyplot as plt\n\n# Plot the distributions of all numerical features and their logarithms\nfor col in numerical_features:\n    # Create 2 subplots\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    # Plot distribution of the parameter\n    sns.distplot(df_train[col], kde=False, ax=axes[0])\n    # Take the logarithm of the parameter\n    log_col = np.log(df_train[col] - df_train[col].min() + 1)\n    log_col.name = 'log({})'.format(col)\n    # Plot distribution of the log of the parameter\n    sns.distplot(log_col, kde=False, ax=axes[1])\n    plt.show()","c5a0b6ce":"# Plot all the same distributions using boxplots\nfor col in numerical_features:\n    # Create 2 subplots\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    # Plot distribution of the parameter\n    sns.boxplot(y=col, x='y', data=df_train, ax=axes[0])\n    # Take the logarithm of the parameter\n    log_col = np.log(df_train[col] - df_train[col].min() + 1)\n    log_col.name = 'log({})'.format(col)\n    # Plot distribution of the log of the parameter\n    sns.boxplot(y=log_col, x='y', data=df_train, ax=axes[1])\n    plt.show()","eb476658":"columns_log = ['balance', 'duration', 'campaign', 'pdays', 'previous']","77880435":"# Drop columns we don't need\nX_train = df_train.drop(remove_features, axis=1)\ny_train = df_train['y']\n\nX_test = df_test.drop(remove_features[:-1], axis=1) # not including 'y'\n\nX_train.head()","fccee6f8":"# One-hot encoding of the categorical features\n\n# Merge train and test parts\nX = pd.concat([X_train, X_test], axis=0)\nX = pd.get_dummies(X, categorical_features, drop_first=False)\n\n# Split them again\nX_train = X.iloc[:len(X_train)]\nX_test = X.iloc[len(X_train):]\n\nX_train.head()","e49bc91e":"# Logarithm of some values\nfor col in columns_log:\n    for X in [X_train, X_test]:\n        X[col + '_log'] = np.log(X[col] - X_train[col].min() + 1)\n# Drop old values\nfor col in columns_log:\n    for X in [X_train, X_test]:\n        X.drop(col, axis=1, inplace=True)","bf726c24":"# Plot the correlation matrix between features\nplt.figure(figsize=(10, 10))\nsns.heatmap(X_train.corr());","791f4490":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_validate\n\n# Scale features\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train_sc = scaler.transform(X_train)\nX_test_sc = scaler.transform(X_test)\n\n# Create model\nlr = LogisticRegression()\n\n# Measure model performance\nlr_f1 = cross_validate(lr, X_train_sc, y_train, cv=5, scoring='f1')['test_score'].mean()\nprint('f1 score: {:.03f}'.format(lr_f1))\n\n# Fit linear model\nlr.fit(X_train_sc, y_train)\n\npredictions_lr = lr.predict(X_test_sc)\npredictions_lr.shape","f7d92f2f":"# Make submission table\nsub_lr = pd.read_csv('\/kaggle\/input\/bank-marketing-prediction\/sample_submission.csv')\nsub_lr['y'] = predictions_lr\n\n# Save as file\nsub_lr.to_csv('submission_lr.csv', index=False)\n\nsub_lr.head()","fa3b5ad5":"from lightgbm import LGBMClassifier\n\n# Create model\nlgb = LGBMClassifier()\n# I use default parameters. Better hyperparameters can be found using GridSearchCV, RandomSearchCV or bayes optimisation\n\n# Measure model performance\nlgb_f1 = cross_validate(lgb, X_train, y_train, cv=5, scoring='f1')['test_score'].mean()\nprint('f1 score: {:.03f}'.format(lgb_f1))\n\n# Fit gradient boosting model\nlgb.fit(X_train, y_train)\n\npredictions_lgb = lgb.predict(X_test)\npredictions_lgb.shape","a645e6ac":"from lightgbm import plot_importance\n\nplot_importance(lgb, max_num_features=-1, height=0.5, grid=False, figsize=(5,15));","597bdd5e":"# Make submission table\nsub_lgb = pd.read_csv('\/kaggle\/input\/bank-marketing-prediction\/sample_submission.csv')\nsub_lgb['y'] = predictions_lgb\n\n# Save as file\nsub_lgb.to_csv('submission_lgb.csv', index=False)\n\nsub_lgb.head()","d79dab93":"## Load data","c0c1e115":"We can see that sometimes log of a value has much smoother distribution that the value itself","6adb1440":"## Conclusion\nThis is a baseline notebook. It shows very basic manipulations with the data and model training.  \nIt is not the best possible solution to the problem, but is a good starting point.  \n### How to improve the results?\nThere are many more things to do to beat this baseline:  \n1. Try different hyperparameters to LightGBM. Try using `GridSearchCV` or `RandomSearchCV` or bayesian search\n2. Try different models\n3. I didn't use features `day` and `month`\n4. Feature engineering: create new features from the existing ones\n5. Try model stacking, if you've done everything else\n\nBut be careful:\n\n1. Do not everfit on the public test set, when playing with parameters. Final private scores may be different. Use cross-validation.\n2. Do not overfit on cross-validation. Sometimes run your solutions with different random seeds\n3. Be **very careful** with *target encoding*. Better not use it.","f717a1d9":"We can clearly see that LightGBM gives better results than a linear model.\n\nLet's look at the feature importances:","0470b915":"There is no missing data. Good for us!","b5d6299b":"### 1. Logistic regression","8b50a90f":"Prety much depends on the duration of the last contact!","39a9c81f":"## Model training","8a6f2fc4":"This is how age and the duration of the last contact are correlated","b7536768":"Target variable does not depend on age very much","f318cbc5":"## Feature preparation\nLet's do one-hot encoding","437ab5c5":"Not sure how the month of the last contact influences the result. I will delete this feature for now","47150778":"## EDA (Exploratory Data Analysis)\nLet's look at some distributions","741289a9":"Duration of the marketing company seems to be the most important factor.  \nThis makes sense.  \nBut sometimes high feature importance means that the model is overfitted on that feature, so be cautious.","5a913f6b":"### 2. LGBM\nLight Gradient Boosting","c8648715":"### Apply transformations\nNow let's apply all these transformations:"}}