{"cell_type":{"07d25444":"code","d74a3b81":"code","484d0779":"code","0540ccf4":"code","0b74ceae":"code","02712466":"code","d62c9197":"code","baa605e8":"code","6de35c4b":"code","d4ec2547":"code","ae445166":"code","86f14492":"code","a3bff63c":"code","bbbd88f3":"code","f87e90ed":"code","b7c5f0f1":"code","a4847d71":"code","aedd20de":"code","b05e3989":"code","255feaf7":"code","7d1b1b8b":"code","000e0e8d":"code","6317d8b5":"code","76bbbf44":"code","00f8261f":"code","3cfa755e":"code","ec8e7dcb":"code","3674067a":"code","fe71acce":"code","42c86eed":"code","36269e2c":"code","0491c78c":"code","227e830e":"code","b183c381":"code","8b7e451d":"code","30872380":"code","9b30e8ae":"code","c3772ac1":"markdown","3e0f2202":"markdown","0f04df56":"markdown","d69da89c":"markdown","6e509693":"markdown","a8204a11":"markdown","e2ee2644":"markdown","6b2907f8":"markdown","6f276793":"markdown","d6215446":"markdown","a8c4a450":"markdown","561ec5a6":"markdown","15011e58":"markdown","3d736210":"markdown","d9e8369e":"markdown","56cae33b":"markdown","67d4dfa5":"markdown","604bd096":"markdown","240b13c0":"markdown","a8ab2213":"markdown","dc2b9e6f":"markdown","0c84d78a":"markdown","54a19718":"markdown","ac85d004":"markdown","54235393":"markdown","1beeaa65":"markdown","94213d04":"markdown","fd20f695":"markdown","5b7697b7":"markdown","631857f0":"markdown","f1662a0a":"markdown","612b63e9":"markdown","a8c25f90":"markdown","54fe613f":"markdown"},"source":{"07d25444":"import warnings\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly import figure_factory as ff, graph_objects as go, express as px\nfrom sklearn.metrics import (\n    f1_score,\n)\nfrom plotly.subplots import make_subplots\nimport math\nimport numpy as np\nfrom yellowbrick.classifier import ConfusionMatrix, ClassificationReport, ClassBalance\n\nwarnings.filterwarnings(\"ignore\")\n\ndf_train = pd.read_csv(\"..\/input\/porto-seguro-data-challenge\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/porto-seguro-data-challenge\/test.csv\")\nmetadata = pd.read_csv(\"..\/input\/porto-seguro-data-challenge\/metadata.csv\")\nprint(df_train.shape)","d74a3b81":"def plot_hists(df, labels):\n    row = 1\n    col = 1\n    num_graphs = len(labels)\n    rows = math.ceil(num_graphs \/ 2)\n    fig = make_subplots(rows=rows, cols=2, subplot_titles=labels)\n\n    index = []\n    for row in range(1, rows + 1):\n        for col in range(1, 3):\n            index.append({\"row\": row, \"col\": col})\n\n    graphs = []\n    pos_g = 0\n    for label in labels:\n        local_data = df[label].value_counts()\n        x = list(local_data.index)\n        y = list(local_data)\n        fig.add_trace(\n            go.Histogram(x=df[label]),\n            row=index[pos_g][\"row\"],\n            col=index[pos_g][\"col\"],\n        )\n        pos_g = pos_g + 1\n\n    fig.update_layout(\n        autosize=False,\n        width=800,\n        showlegend=False,\n        height=300 * rows,\n        margin=dict(l=50, r=50, b=100, t=100, pad=4),\n    )\n\n    fig.show()\n","484d0779":"df_train.info()","0540ccf4":"def verify_null_values(df):\n    df[df < 0.0] = np.nan\n    null_values = pd.DataFrame(df.isnull().sum())\n    null_values.columns = [\"count\"]\n    null_values[\"NAN%\"] = (null_values \/ df.shape[0]) * 100\n    return null_values.sort_values(\"count\", ascending=False)[:10]\n\n\nverify_null_values(df_train)","0b74ceae":"y = df_train[\"y\"]\nvisualizer = ClassBalance(labels=[\"Negativo 0\", \"Positivo 1\"])\nvisualizer.fit(y)\n_ = visualizer.show()","02712466":"metadata[\"Variavel tipo\"].value_counts()","d62c9197":"def get_feature_names(fill):\n    return [\n        *metadata[metadata[\"Variavel tipo\"] == fill][\"Variavel cod\"].values,\n    ]\n\n\nnumerical_features = get_feature_names(\"Quantitativo continua\")\n\ncategorical_features = get_feature_names(\"Qualitativo nominal\")\ncategorical_features.remove(\"id\")\n\ndiscrete_features = get_feature_names(\"Quantitativo discreto\")\ndiscrete_features.remove(\"y\")\n\nordinal_features = get_feature_names(\"Qualitativo ordinal\")","baa605e8":"def verify_unique_values(df, cols):\n    aux = df[cols].describe().T\n    aux[\"unique_values\"] = [len(df[col].unique()) for col in cols]\n    aux[\"unique_values %\"] = [\n        (len(df[col].unique()) \/ df.shape[0]) * 100 for col in cols\n    ]\n    ## 0 means low variance (few categories) 1 means high variance (many categories)\n    return aux[[\"count\", \"min\", \"max\", \"unique_values\", \"unique_values %\"]].sort_values(\n        \"unique_values %\", ascending=False\n    )\n\n\nverify_unique_values(df_train, categorical_features)","6de35c4b":"for col in categorical_features:\n    df_train[col] = df_train[col].astype(str)\n    df_test[col] = df_test[col].astype(str)\n\ncat_descibre = df_train[categorical_features].describe()\ncat_descibre","d4ec2547":"high_cat = cat_descibre.T.sort_values(\"unique\", ascending=False)[:15]\nhigh_cat_col = list(high_cat[:10].index.values)\nhigh_cat","ae445166":"corr = df_train[high_cat_col + [\"y\"]].dropna()\nfor col in high_cat_col:\n    corr[col] = corr[col].astype(float)\ncorr = corr.corr()\n\nf, ax = plt.subplots(figsize=(15, 15))\nax = sns.heatmap(\n    corr,\n    vmin=-1,\n    vmax=1,\n    center=0,\n    annot=True,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True,\n    linewidths=0.5,\n)\n_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment=\"right\")\n_ = ax.set_title(\n    \"Correlation between categorical columns which too many unique values and target.\"\n)","86f14492":"for col in high_cat_col:\n    numerical_features.append(col)\n    categorical_features.remove(col)","a3bff63c":"corr = df_train[categorical_features + [\"y\"]].dropna()\nfor col in categorical_features:\n    corr[col] = corr[col].astype(float)\ncorr = corr.corr()[\"y\"].reset_index()\ncorr.columns = [\"feature\", \"corr_value\"]\n\n\nf, ax = plt.subplots(figsize=(15, 6))\n_ = sns.barplot(\n    x=\"feature\",\n    y=\"corr_value\",\n    data=corr[:-1],\n    palette=\"rocket\",\n    label=\"corr_value\",\n    color=\"y\",\n)\n_ = ax.set_title(\"Correlation between target and categorical columns.\")","bbbd88f3":"low_cat_col = df_train[categorical_features].describe()\nlow_cat_col","f87e90ed":"def group_less_freq_cat(df, col, limit):\n    value_counts_local = df[col].value_counts()\n    col_size = len(df[col])\n    list_values = value_counts_local[(value_counts_local \/ col_size < limit)].index\n    return df[col].replace(list_values, np.nan)","b7c5f0f1":"for col in categorical_features:\n    df_train[col] = group_less_freq_cat(df_train, col, 0.01)\n    df_train[col] = df_train[col].astype(str)","a4847d71":"df_train[categorical_features].describe()","aedd20de":"numerical_features = discrete_features + numerical_features + ordinal_features\n\nfor col in numerical_features:\n    df_train[col] = df_train[col].astype(float)\n    df_test[col] = df_test[col].astype(float)\n\nnum_describe = df_train[numerical_features].describe()\nnum_describe","b05e3989":"ax = sns.boxplot(x=\"y\", y=\"var55\", data=df_train)\n_ = ax.set_title(\"Boxplot for column: var55 with outliers.\")","255feaf7":"def identify_outlier(data, cut_lim=3):\n    data_mean, data_std = np.mean(data), np.std(data)\n    cut_off = data_std * cut_lim\n    lower, upper = data_mean - cut_off, data_mean + cut_off\n\n    def aux(val):\n        if val < lower or val > upper:\n            return np.nan\n        else:\n            return val\n\n    return data.apply(aux)\n\n\nfor col in numerical_features:\n    df_train[col] = identify_outlier(df_train[col], cut_lim=2)","7d1b1b8b":"ax = sns.boxplot(x=\"y\", y=\"var55\", data=df_train)\n_ = ax.set_title(\"Boxplot for column: var55 withOUT outliers.\")","000e0e8d":"from scipy.stats import kstest, norm\n\nks_statistic, p_value = kstest(df_train[\"var4\"].dropna(), \"norm\")\n\nprint(ks_statistic, p_value)","6317d8b5":"df_train[ordinal_features]","76bbbf44":"for col in ordinal_features:\n    describe = df_train[col].describe()\n\n    def quantile_value(value):\n        if value < describe[\"25%\"]:\n            return 0\n        elif value < describe[\"50%\"]:\n            return 1\n        elif value < describe[\"75%\"]:\n            return 2\n        else:\n            return 3\n\n    df_train[col] = df_train[col].apply(quantile_value)\n    df_test[col] = df_test[col].apply(quantile_value)\n\ndf_train[ordinal_features].describe().T","00f8261f":"plot_hists(df_train, ordinal_features)","3cfa755e":"df_train[discrete_features].describe().T","ec8e7dcb":"for col in discrete_features:\n    describe = df_train[col].describe()\n\n    def quantile_value(value):\n        if value < describe[\"25%\"]:\n            return 0\n        elif value < describe[\"50%\"]:\n            return 1\n        elif value < describe[\"75%\"]:\n            return 2\n        else:\n            return 3\n\n    df_train[col] = df_train[col].apply(quantile_value)\n    df_test[col] = df_test[col].apply(quantile_value)","3674067a":"plot_hists(df_train, discrete_features)","fe71acce":"from sklearn.model_selection import train_test_split\n\nstd_cols = numerical_features + ordinal_features\n\nfor col in std_cols:\n    df_train[col] = df_train[col].astype(float)\n    df_test[col] = df_test[col].astype(float)\n\none_hot_cols = categorical_features + discrete_features\n\nfor col in categorical_features:\n    df_train[col] = df_train[col].astype(str)\n    df_test[col] = df_test[col].astype(str)\n\n\nx_train, x_test, y_train, y_test = train_test_split(\n    df_train.drop(\"y\", axis=1),\n    df_train.y,\n    test_size=0.2,\n    random_state=0,\n)","42c86eed":"from sklearn.impute import SimpleImputer\n\nmedian_imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\nconstant_imp = SimpleImputer(missing_values=np.nan, strategy=\"constant\")\n\nx_train[one_hot_cols] = constant_imp.fit_transform(x_train[one_hot_cols])\nx_test[one_hot_cols] = constant_imp.transform(x_test[one_hot_cols])\ndf_test[one_hot_cols] = constant_imp.transform(df_test[one_hot_cols])\n\n\nx_train[std_cols] = median_imp.fit_transform(x_train[std_cols])\nx_test[std_cols] = median_imp.transform(x_test[std_cols])\ndf_test[std_cols] = median_imp.transform(df_test[std_cols])","36269e2c":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\nfrom sklearn.metrics import f1_score\nfrom imblearn.over_sampling import ADASYN\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"std\", StandardScaler(), std_cols),\n        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"), one_hot_cols),\n    ]\n)\n\n# Append classifier to preprocessing pipeline.\n# Now we have a full prediction pipeline.\npipe = Pipeline(steps=[(\"preprocessor\", preprocessor)])\nx_train = pipe.fit_transform(x_train, y_train)\n\nx_test = pipe.transform(x_test)\ndf_test = pipe.transform(df_test)\n\nprint(\"x_train\", x_train.shape)\nprint(\"x_test\", x_test.shape)","0491c78c":"from imblearn.over_sampling import ADASYN\n\nada = ADASYN()\nx_train, y_train = ada.fit_resample(x_train, y_train)\nprint(\"x_train\", x_train.shape)\nprint(\"y_train\", y_train.mean())","227e830e":"x_train = pd.DataFrame(x_train.toarray())\ny_train = pd.DataFrame(y_train).reset_index(drop=True)\ny_test = pd.DataFrame(y_test).reset_index(drop=True)\nx_test = pd.DataFrame(x_test.toarray())\ndf_test = pd.DataFrame(df_test.toarray())","b183c381":"from lightgbm import LGBMClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, StackingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\nlgbm = LGBMClassifier(\n    subsample=0.8,\n    reg_lambda=10,\n    reg_alpha=3,\n    objective=\"binary\",\n    num_leaves=12,\n    n_estimators=90,\n    max_depth=25,\n    learning_rate=0.1,\n    feature_fraction=0.5,\n    colsample_bytree=0.75,\n    boosting_type=\"gbdt\",\n    bagging_fraction=0.9,\n)\n\ngbc = GradientBoostingClassifier(\n    ccp_alpha=0.0,\n    criterion=\"friedman_mse\",\n    init=None,\n    learning_rate=0.1,\n    loss=\"deviance\",\n    max_depth=3,\n    max_features=None,\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    min_samples_leaf=1,\n    min_samples_split=2,\n    min_weight_fraction_leaf=0.0,\n    n_estimators=100,\n    n_iter_no_change=None,\n    presort=\"deprecated\",\n    random_state=1234,\n    subsample=1.0,\n    tol=0.0001,\n    validation_fraction=0.1,\n    verbose=0,\n    warm_start=False,\n)\n\nneigh = KNeighborsClassifier(n_neighbors=10)\n\ngnb = GaussianNB(var_smoothing=7.171717e-04)","8b7e451d":"from sklearn.model_selection import KFold\n\nestimators = [\n    (\"lgbm\", lgbm),\n    (\"gnb\", gnb),\n    (\"neigh\", neigh),\n    (\"gbc\", gbc),\n]\n\nkf = KFold(n_splits=5, random_state=0)\nclf = StackingClassifier(estimators=estimators, final_estimator=gnb, cv=kf)\nclf.fit(x_train, y_train)\nclf.score(x_test, y_test)","30872380":"visualizer = ClassificationReport(clf, classes=[0, 1], support=True)\nvisualizer.score(x_test, y_test)\nvisualizer.show()\ncm = ConfusionMatrix(clf, classes=[0, 1])\ncm.score(x_test, y_test)\n_ = cm.show()","9b30e8ae":"print(\"The F1 score for both classes:\", f1_score(clf.predict(x_test), y_test))","c3772ac1":"#### Removing outliers","3e0f2202":"### Check class balance","0f04df56":"Importing the libs and datasets...","d69da89c":"Training...","6e509693":"### Reducing number of categories\n\nA function that will help us identify the values with low frequency","a8204a11":"##### Normal number of categories -> ['var1', 'var2', 'var7', 'var8', 'var9', 'var10', 'var14', 'var15', 'var16', 'var17', 'var18', 'var20', 'var22', 'var23', 'var28', 'var29', 'var30', 'var31', 'var33', 'var34', 'var36', 'var37', 'var38', 'var39', 'var41']\n","e2ee2644":"### Discrete Features","6b2907f8":"### Categorical features\n- 35 columns;","6f276793":"# Kaggle Competition - Porto Seguro Data Challenge\nPT-BR -> Este Notebook foi desenvolvidor por Marco Carujo e Paulo Jarbas para a competi\u00e7\u00e3o do Kaggle - Porto Seguro Data Challenge 2021.\n\nEN -> This Notebook was developed by Marco Carujo and Paulo Jarbas for the Kaggle competition - Porto Seguro Data Challenge 2021.","d6215446":"The features with just one value will be removed soon during the \"feature selection\" process.","a8c4a450":"** some categorical columns look more numerical than categorical.","561ec5a6":"### Selected Features","15011e58":"# Dataset\n\n### Features\nStarting the features analysis.\n\n- We have the data type for each column (Categorical, Numerical, Discrete and Ordinal).\n- There is no description in terms of meaning or represetation for each column. ","3d736210":"Now, it's time to visualise the final results...","d9e8369e":"To visualize the results.","56cae33b":"### Start the feature analyse and split features per type\nFor this task, we can use the `metadata.csv` to understand more about our dataset.","67d4dfa5":"**var65** and **var66** have 86% of nan values","604bd096":"Fixing the unbalanced problem..","240b13c0":"Apply the function","a8ab2213":"Creating new catories...","dc2b9e6f":"Final describe without negative values.","0c84d78a":"Using **var4** as numeric feature.","54a19718":"### Esemble ","ac85d004":"# Modeling","54235393":"Function that will indentfy outliers and replace by nan.","1beeaa65":"Stack of [estimators](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.StackingClassifier.html) with a final classifier.\n\nStacked generalization consists in stacking the output of individual estimators and use a classifier to compute the final prediction. Stacking allows using the strength of each individual estimator by using their output as input of a final estimator.","94213d04":"Just a function to print histograms for each column.","fd20f695":"It's an unbalanced classification problem and that will require some treatment to avoid bias for the further steps.","5b7697b7":"##### Too many unique values - > ['var4', 'var11', 'var5', 'var12', 'var3', 'var6', 'var21', 'var13','var19', 'var35']\nMaybe a few of them could be used as numerical features, we can check this possibility by analysing the correlation.","631857f0":"Ploting the columns...","f1662a0a":"### Numerical features\n- 43 columns;","612b63e9":"For this problem we have too many columns, however, the feature selection will be done after the data preprocessing step.","a8c25f90":"### Ordinal Features\n- 4 columns.","54fe613f":"### Impute missing values"}}