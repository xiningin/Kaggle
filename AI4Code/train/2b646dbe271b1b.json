{"cell_type":{"ca5c0c53":"code","d952ba83":"code","d2a331cb":"code","892d19ba":"code","de66ac42":"code","5d37d0d4":"code","43dfe9dd":"code","a29deec1":"code","679d6d6e":"code","1f9b44f4":"code","df484a93":"code","88f51581":"code","3970bf45":"code","dc892e81":"code","ffcc8b11":"code","caf83606":"code","e61bcdc0":"code","7831bbb9":"code","ac78a52b":"code","4961e00c":"code","edd9a973":"code","6a1c53b6":"markdown","e1fee469":"markdown","4994f935":"markdown","44e99eab":"markdown","fe680156":"markdown","fc76a18c":"markdown","4219d866":"markdown","56767955":"markdown"},"source":{"ca5c0c53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d952ba83":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpb\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import skew\nimport category_encoders as ce\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n%matplotlib inline\nsns.set_theme()","d2a331cb":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","892d19ba":"train.head()","de66ac42":"fig, ax = plt.subplots(figsize=(10,8))\nsns.heatmap(train.isnull(), ax=ax)","5d37d0d4":"print(\"Train: \", train.shape)\nprint(\"Test: \", test.shape)","43dfe9dd":"fig, ax = plt.subplots(figsize=(8,6))\nmissing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar(ax=ax)","a29deec1":"train.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], inplace=True, axis=1)\ntest.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], inplace=True, axis=1)","679d6d6e":"corr = train.corr()\nfig, ax = plt.subplots(figsize=(14,8))\nsns.heatmap(corr)","1f9b44f4":"target = ['SalePrice']\ncat_features = train.drop(columns=['Id', 'SalePrice']).select_dtypes(include='object').columns.tolist()\nnum_features = train.drop(columns=['Id', 'SalePrice']).select_dtypes(include=np.number).columns.tolist()\nall_features = cat_features + num_features","df484a93":"mpb.rcParams['figure.figsize'] = (15.0, 6.0)\nprices = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log(price + 1)\":np.log1p(train['SalePrice'])})\nprices.hist()","88f51581":"train.SalePrice = np.log(train.SalePrice)","3970bf45":"# Pipeline for categorical features\ncat_tfms = Pipeline(steps=[\n    ('cat_ordenc', ce.OrdinalEncoder(return_df=True, handle_unknown='value', handle_missing='value'))\n])\n\n# Pipeline for numerical features\nnum_tfms = Pipeline(steps=[\n    ('num_imputer',  SimpleImputer(missing_values=np.nan, strategy='median'))\n])\n\nfeatures = ColumnTransformer(transformers=[\n    ('cat_tfms', cat_tfms, cat_features),\n    ('num_tfms', num_tfms, num_features)\n], remainder='passthrough')","dc892e81":"X = train[all_features]\ny = train.SalePrice\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True,random_state=42)\nX_train_tf = pd.DataFrame(features.fit_transform(X_train), columns=all_features)\nX_test_tf = pd.DataFrame(features.fit_transform(X_test), columns=all_features)\ntest_tf = test[all_features]\ntest_tf = pd.DataFrame(features.transform(test_tf), columns=all_features)\nenc_map = dict()\nfor feat in cat_features: enc_map[feat] = dict(zip(X_train[feat], X_train_tf[feat]))","ffcc8b11":"print(\"X_train shape: \", X_train_tf.shape)\nprint(\"test shape:\", test_tf.shape)","caf83606":"rf = RandomForestRegressor(\n    n_estimators=50, max_depth=None, min_samples_leaf=1, min_samples_split=2,\n    max_features=.7, max_samples=None, n_jobs=-1, random_state=42)","e61bcdc0":"rf.fit(X_train_tf, y_train)","7831bbb9":"y_preds = rf.predict(X_test_tf)","ac78a52b":"rmse = mean_squared_error(y_test, y_preds, squared=False)\nrmse","4961e00c":"preds = rf.predict(test_tf)","edd9a973":"submission = pd.DataFrame({\n    'Id': np.asarray(test.Id), \n    'SalePrice': preds.astype(int)\n})\nsubmission.to_csv('my_submission.csv', index=False)","6a1c53b6":"# Data Preprocessing","e1fee469":"We have created target and feature variables and separately stored the categorical and numerical features in a list","4994f935":"# Model Training","44e99eab":"We can see that a lot of features are highly correlated. This features can be removed when using high computational model to increase speed such as XgBoost but for now we will keep it","fe680156":"## Correlation Matrix","fc76a18c":"# Submission","4219d866":"Alley, FireplaceQu, PoolQc, Fence, MiscFeature and some of the features with most missing values. Hence, it is worth while to remove this features from the training data","56767955":"### Log Transform the data"}}