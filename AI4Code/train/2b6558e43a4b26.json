{"cell_type":{"cae811b3":"code","97e3109a":"code","e5ca7be3":"code","1e22602c":"code","c31cd63a":"code","40a19a11":"code","0d8e0134":"code","8e7c31e7":"code","9a2cf316":"code","4cdc618a":"code","23717ad2":"code","0ee9c89d":"code","ef02aa45":"code","194ab10a":"code","7b818050":"code","3861322c":"code","d0fb1a16":"code","db04b889":"code","7164062e":"code","8bac1f24":"code","1d84f874":"code","4d719b28":"code","273870de":"code","ba7397f5":"code","fa3eeb62":"code","e5b9d042":"code","d1d0f4a8":"code","0074fcda":"code","84ce02b1":"code","38a0c722":"code","7a70e309":"code","7ea59322":"code","c747c4d7":"code","4ca361c6":"code","6b7f8ffb":"code","4736a69e":"code","16589968":"code","bdbcdc24":"code","5b7b72eb":"code","e5103661":"code","ec582c5e":"code","ef470395":"code","84b42784":"code","d311d2c3":"code","55e80195":"code","691d7bb6":"code","6c58576e":"code","7fac65b8":"code","28b01ebd":"code","7e30d7b5":"code","5cab833d":"code","7333400c":"code","28387ea3":"code","6b3f4c86":"code","25793182":"code","9d028275":"code","6e189581":"markdown","4826fe8f":"markdown","fbcadd5d":"markdown"},"source":{"cae811b3":"#!python --version\n\n#!pip3 uninstall keras --yes\n\n#!pip3 uninstall tensorflow --yes\n#!pip3 uninstall tensorflow-gpu --yes\n#!pip3 install tensorflow-gpu\n\n#!pip3 install keras","97e3109a":"import IPython.display as ipd\nimport numpy as np\nimport pandas as pd\nimport librosa\nfrom librosa import display\nimport matplotlib.pyplot as plt\n\nfrom scipy.io import wavfile as wav\n\nfrom sklearn import metrics \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split \n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical","e5ca7be3":"import tensorflow as tf\nimport keras","1e22602c":"import os\n\n#Copy the path and read_csv, for example:\ndf = pd.read_csv(\"\/kaggle\/input\/urbansounds\/UrbanSound8K\/metadata\/UrbanSound8K.csv\")\n#For the shape of the extracted features\nn_mfcc = 40","c31cd63a":"labels = list(df['class'].unique())# Let's grab a single audio file from each class\nfiles = dict()\nfor i in range(len(labels)):\n    tmp = df[df['class'] == labels[i]][:1].reset_index()\n    path = '\/kaggle\/input\/urbansounds\/UrbanSound8K\/audio\/fold{}\/{}'.format(tmp['fold'][0], tmp['slice_file_name'][0])\n    files[labels[i]] = path","40a19a11":"fig = plt.figure(figsize=(15,15))\n# Log graphic of waveforms to Comet\n#experiment.log_image('class_examples.png')\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i, label in enumerate(labels):\n    fn = files[label]\n    fig.add_subplot(5, 2, i+1)\n    plt.title(label)\n    data, sample_rate = librosa.load(fn)\n    librosa.display.waveplot(data, sr= sample_rate)\nplt.savefig('class_examples.png')","0d8e0134":"fn = '\/kaggle\/input\/urbansounds\/UrbanSound8K\/audio\/fold1\/191431-9-0-66.wav'\nlibrosa_audio, librosa_sample_rate = librosa.load(fn)\nscipy_sample_rate, scipy_audio = wav.read(fn)\nprint(\"Original sample rate: {}\".format(scipy_sample_rate))\nprint(\"Librosa sample rate: {}\".format(librosa_sample_rate))","8e7c31e7":"print('Original audio file min~max range: {} to {}'.format(np.min(scipy_audio),np.max(scipy_audio)))\nprint('Librosa audio file min~max range: {0:.2f} to {0:.2f}'.format(np.min(librosa_audio), np.max(librosa_audio)))","9a2cf316":"plt.figure(figsize=(12, 4))\nplt.plot(scipy_audio)\nplt.savefig('original_audio.png')","4cdc618a":"# Librosa: mono track\nplt.figure(figsize=(12,4))\nplt.plot(librosa_audio)\nplt.savefig('librosa_audio.png')","23717ad2":"mfccs = librosa.feature.mfcc(y=librosa_audio, sr=librosa_sample_rate, n_mfcc = n_mfcc)","0ee9c89d":"print(mfccs.shape)","ef02aa45":"plt.figure(figsize=(8,8))\nlibrosa.display.specshow(mfccs, sr=librosa_sample_rate, x_axis='time')\nplt.savefig('MFCCs.png')","194ab10a":"print(keras.__version__)\nprint(tf.__version__)","7b818050":"#sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))","3861322c":"from tensorflow.python.client import device_lib\n\nprint(device_lib.list_local_devices())","d0fb1a16":"tf.debugging.set_log_device_placement(False)\n\n","db04b889":"with tf.device('\/gpu:0'):\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n    c = tf.matmul(a, b)\n\nprint(c)","7164062e":"print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\ntf.debugging.set_log_device_placement(False)\n","8bac1f24":"def extract_features(file_name):\n    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n    mfccs_processed = np.mean(mfccs.T,axis=0)\n    return mfccs_processed","1d84f874":"features = []\n#this may take some time to process\n# Iterate through each sound file and extract the features \nfulldatasetpath = '\/kaggle\/input\/urbansounds\/UrbanSound8K\/audio\/'\nfor index, row in df.iterrows():\n    slice_file_name = 'fold'+str(row[\"fold\"])+'\/'+str(row[\"slice_file_name\"])\n    file_name = os.path.join(os.path.abspath(fulldatasetpath), slice_file_name)\n    #file_name = '\/kaggle\/input\/urbansounds\/UrbanSound8K\/audio\/fold{}\/{}'.format(row['fold'],row['slive_file_name'])\n    #  path = '\/kaggle\/input\/urbansounds\/UrbanSound8K\/audio\/fold{}\/{}'.format(tmp['fold'][0], tmp['slice_file_name'][0])\n    class_label = row[\"class\"]\n    data = extract_features(file_name)\n    #appending the fold+#+file_name , for later reference into the specific files, and be able to acess them\n    features.append([slice_file_name,data, class_label])\n# Convert into a Panda dataframe \nfeaturesdf = pd.DataFrame(features, columns=['file_name','feature','class_label'])","4d719b28":"featuresdf.head()","273870de":"print(featuresdf.class_label.unique())\nlen(featuresdf.class_label.unique())","ba7397f5":"featuresdf.iloc[0]['feature']","fa3eeb62":"#Number of rows\/soundfiles in the dataframe\nlen(featuresdf)","e5b9d042":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n# Convert features and corresponding classification labels into numpy arrays\nX = np.array(featuresdf.feature.tolist())\ny = np.array(featuresdf.class_label.tolist())\n# Encode the classification labels\nle = LabelEncoder()\nyy = to_categorical(le.fit_transform(y))\nz = np.array(featuresdf.file_name.tolist())","d1d0f4a8":"x = X.astype(float)\n#print(X)","0074fcda":"# split the dataset \nfrom sklearn.model_selection import train_test_split \n#train is 0.8 of total\n#stratify makes sure that we have an equal destribution of the differnt labels, ie. so all the dogs arent in one set\nx_train, x_test, y_train, y_test, z_train, z_test = train_test_split(X, yy, z, test_size=0.2, random_state = 127  , shuffle=True,stratify=yy)\n#validation size 10% of test corresponding to 0.1 of total\n#test is 0.1 of total\nx_test, x_val, y_test, y_val, z_test, z_val = train_test_split(x_test, y_test,z_test, test_size=0.5, random_state = 2)","84ce02b1":"import sys\nimport numpy\nnumpy.set_printoptions(threshold=10)","38a0c722":"#print(x_train.astype(float))","7a70e309":"#Number of labels indiacating the classes that we want to classify\n#With a softmax activation layer, consider \nnum_labels = yy.shape[1]\ndef build_model_graph(input_shape=(n_mfcc,)):\n    model = Sequential()\n    model.add(Dense(256))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(256))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_labels))\n    model.add(Activation('softmax'))\n    # Compile the model\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')    \n    return model\nmodel = build_model_graph()","7ea59322":"# Display model architecture summary \n#model.summary()\n# Calculate pre-training accuracy \nscore = model.evaluate(x_test, y_test, verbose=0)\naccuracy = 100*score[1]\nmodel.summary()","c747c4d7":"print(\"Pre-training accuracy: %.4f%%\" % accuracy)","4ca361c6":"class CustomLearningRateScheduler(keras.callbacks.Callback):\n    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n\n  Arguments:\n      schedule: a function that takes an epoch index\n          (integer, indexed from 0) and current learning rate\n          as inputs and returns a new learning rate as output (float).\n  \"\"\"\n\n    def __init__(self, schedule):\n        super(CustomLearningRateScheduler, self).__init__()\n        self.schedule = schedule\n\n    def on_epoch_begin(self, epoch, logs=None):\n        if not hasattr(self.model.optimizer, \"lr\"):\n            raise ValueError('Optimizer must have a \"lr\" attribute.')\n        # Get the current learning rate from model's optimizer.\n        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n        # Call schedule function to get the scheduled learning rate.\n        scheduled_lr = self.schedule(epoch, lr)\n        # Set the value back to the optimizer before this epoch starts\n        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))\n\n\nLR_SCHEDULE = [\n    # (epoch to start, learning rate) tuples\n    (3, 0.05),\n    (6, 0.01),\n    (9, 0.005),\n    (12, 0.001),\n    (100, 0.0001),\n]\n\n\ndef lr_schedule(epoch, lr):\n    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n        return lr\n    for i in range(len(LR_SCHEDULE)):\n        if epoch == LR_SCHEDULE[i][0]:\n            return LR_SCHEDULE[i][1]\n    return lr","6b7f8ffb":"class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n\n  Arguments:\n      patience: Number of epochs to wait after min has been hit. After this\n      number of no improvement, training stops.\n  \"\"\"\n\n    def __init__(self, patience=0):\n        super(EarlyStoppingAtMinLoss, self).__init__()\n        self.patience = patience\n        # best_weights to store the weights at which the minimum loss occurs.\n        self.best_weights = None\n\n    def on_train_begin(self, logs=None):\n        # The number of epoch it has waited when loss is no longer minimum.\n        self.wait = 0\n        # The epoch the training stops at.\n        self.stopped_epoch = 0\n        # Initialize the best as infinity.\n        self.best = np.Inf\n\n    def on_epoch_end(self, epoch, logs=None):\n        current = logs.get(\"loss\")\n        if np.less(current, self.best):\n            self.best = current\n            self.wait = 0\n            # Record the best weights if current results is better (less).\n            self.best_weights = self.model.get_weights()\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.stopped_epoch = epoch\n                self.model.stop_training = True\n                print(\"Restoring model weights from the end of the best epoch.\")\n                self.model.set_weights(self.best_weights)\n\n    def on_train_end(self, logs=None):\n        if self.stopped_epoch > 0:\n            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n","4736a69e":"from keras.callbacks import ModelCheckpoint \nfrom datetime import datetime \nimport tensorflow as tf\n\nnum_epochs = 1000\nnum_batch_size = 256 #256\n\n#log_dir = \"logs\/fit\/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\nmy_callbacks = [\n    #patience is how long the it will keep training after it has gotten a worse accuracy, if no better result has been found in the meantime\n    EarlyStoppingAtMinLoss(patience=200),\n    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n    CustomLearningRateScheduler(lr_schedule)\n  #  tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1),\n]\n#Set verbose to 1 if information about the tranining procedure is desired\nhistory = model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_val, y_val), verbose=0,callbacks=my_callbacks)\n\n","16589968":"from keras.models import load_model\n\ntime_now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\nmodel.save('my_model.h5'+time_now)  # creates a HDF5 file 'my_model.h5'\n\n\n# returns a compiled model\n# identical to the previous one\n#model = load_model('my_model.h5')\n","bdbcdc24":"#del model  # deletes the existing model","5b7b72eb":"print(history.history.keys())","e5103661":"# Check out our train accuracy and validation accuracy over epochs.\ntrain_accuracy = history.history['accuracy']\ntrain_loss = history.history['loss']\n\nval_accuracy = history.history['val_accuracy']\nval_loss = history.history['val_loss']\n\n\n\n# create figure and axis objects with subplots()\nfig,ax = plt.subplots(figsize=(30,15))\n\n\n# Generate line plot of training, testing loss over epochs.\nax.set_xlabel('Epoch', fontsize = 18,rotation=0)\nax.set_ylabel('Categorical Crossentropy', fontsize = 18)\nax.plot(train_accuracy, label='Training Accuracy', color='#185fad')\nax.plot(val_accuracy, label='Validation Accuracy', color='orange')\nax.legend(fontsize = 18,loc=10)\n\n# Set title\n\nplt.title('Training and Validation Accuracy by Epoch', fontsize = 25)\n\nticks=[]\nfor i in range(0,11):\n    var = num_epochs\/10\n    ticks.append(var*i)\n\n\n# twin object for two different y-axis on the sample plot\nax2=ax.twinx()\n# make a plot with different y-axis using second axis object\nax2.plot(train_loss, label='Training Loss', color='purple')\nax2.plot(train_accuracy, label='Validation Loss', color='red')\nax2.set_ylabel('Loss',fontsize=18)\n\n\n\"\"\"\n'best' \t0\n'upper right' \t1\n'upper left' \t2\n'lower left' \t3\n'lower right' \t4\n'right' \t5\n'center left' \t6\n'center right' \t7\n'lower center' \t8\n'upper center' \t9\n'center' \t10\n\"\"\"\n\nax2.legend(fontsize = 18,loc=5)\nplt.xticks(ticks=ticks,labels=ticks,rotation=0)\n\n\n# Tweak spacing to prevent clipping of tick-labels\nplt.subplots_adjust(bottom=0.15)\nplt.grid()\nplt.show()\nplt.savefig('training_loss_graph.png');\n\n","ec582c5e":"# Evaluating the model on the training and testing set\ntrain_score = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Training Accuracy: {0:.3%}\".format(train_score[1]))\ntest_score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Testing Accuracy: {0:.3%}\".format(test_score[1]))","ef470395":"# We get our predictions from the test data\npreds = np.argmax(model.predict(x_test),axis=1)","84b42784":"print(preds)","d311d2c3":"# We transform back our predictions to the strings of what they represent\ninvpreds = le.inverse_transform(preds)\nprint(invpreds)","55e80195":"#See how the data of ytest looks like\nprint(y_test)","691d7bb6":"#Converting the class labels into a single value, from the 1\/0 on the index that they would otherwise represent\ndarray_y_test = np.argmax(y_test,axis=1) \n#print(darray_y_test)","6c58576e":"#Converting back to the strings with the use of the encoder\ninvy_test = le.inverse_transform(darray_y_test)","7fac65b8":"#Print how they look on string form\n#print(invy_test)","28b01ebd":"# Create new data frame with the features\ndfx = pd.DataFrame(x_test)\n# Create data frame with the class test labels\ndfy = pd.DataFrame(invy_test)\n#create data frame with the filenames\ndfz = pd.DataFrame(z_test)\n#Addeing the real values of the test class labels\ndfx['class_labels'] = dfy\n#Adding the predictions of the class labels\ndfx['preds'] = invpreds\n#adding filenames to the df\ndfx['file_name'] = dfz","7e30d7b5":"#Print the whole dataframe with all the features and the test values predicted and the real ones\ndfx.head()","5cab833d":"# Checking which predictions I got wrong, printing the rows of the unequal predictions\ndfx[dfx['class_labels'] != dfx['preds']]","7333400c":"#Extracting the pairs that are not equal\ndf_unequal = dfx[dfx['class_labels'] != dfx['preds']]\n\n\npd.crosstab(df_unequal.preds, df_unequal.class_labels)","28387ea3":"#Play a sound from the dataset\nfrom IPython.display import Audio\nimport IPython.display as ipd\nfrom IPython.display import clear_output\nimport sys\nsound_file = '\/kaggle\/input\/urbansounds\/UrbanSound8K\/audio\/'\nfor index, row in df_unequal.iterrows():\n    slice_name = row['file_name']\n    \n    name= sound_file+slice_name\n    \n    print(\"Name of file: {} , Predicted to be: {} , Real Value: {} \".format(slice_name , row['preds'] ,row['class_labels']))\n    \n    \n    input(ipd.display(ipd.Audio(name,autoplay=True)))\n    #ipd.set_matplotlib_close(close=True)\n    #sys.stdout.flush()\n    clear_output(wait=True)\n    \n","6b3f4c86":"#This file is in folder 4. Jackhammer, car_horn example\nAudio('\/kaggle\/input\/urbansounds\/UrbanSound8K\/audio\/fold4\/7389-1-0-4.wav')","25793182":" #This file is in folder 4. Does not sound like street_music nor children playing, \n    #but it does sound like streenoise although this is not a label\n    #It probably says children_playing because there are people talking\nAudio('\/kaggle\/input\/urbansounds\/UrbanSound8K\/audio\/fold4\/194458-9-1-75.wav')\n    ","9d028275":"#creating a dataframe for comparison between the pairs that are not equal for comparison \n#This is to test whether they have some similar features and in this case which\ndf_numerical = pd.DataFrame(x_test)\ndf_preds = pd.DataFrame(preds)\ndf_numerical['preds'] = df_preds\ndf_classlabels = pd.DataFrame(darray_y_test)\ndf_numerical['class_labels'] = df_classlabels\n\n#Extracting the pairs that are not equal\ndf_numerical = df_numerical[df_numerical['class_labels'] != df_numerical['preds']]\n\n\npd.crosstab(df_numerical.preds, df_numerical.class_labels)","6e189581":"\n# Now that we have evaluated the model, lets take a close look at the dataset and see which files it was unable to predict # ","4826fe8f":"*High correlation between the results of children_playing and dog_barking, aswell as gun_shot*","fbcadd5d":"One instance in particular classified to be car_horn, and predicted to be a jackhammer. But the car_horn is in the background, and the jackhammer is in the foreground. So I think the classifier actually does a better job here than the human person who classified the sound bite. fold4\/7389-1-0-4.wav. \n* Some Sound bites are so low amplitude that is not possible to distinguish what the sound actually is"}}