{"cell_type":{"90b1172e":"code","42cb5b36":"code","7c7bcffc":"code","dfb31a89":"code","577bfadb":"code","dc92f2a4":"code","639becd5":"code","62f4d7f2":"code","75289593":"code","169c0a1a":"code","43800db1":"code","a7a99b1e":"code","718f12af":"code","2ca1399c":"code","81fe6fad":"code","5aef468d":"code","4d7fa887":"code","0ce8b61d":"code","aaeac438":"code","0130a776":"code","1b51354d":"code","fe4a3542":"code","1e92dc96":"code","14b4379d":"code","f86d69aa":"code","69b2b29a":"code","fe325129":"code","4122488e":"code","7b116f5f":"code","8226ddde":"code","54eed6b4":"code","ece43127":"code","7cb51e18":"code","cb30ccd1":"code","1161fc40":"code","d1d018b7":"code","9b856392":"markdown","164c31e4":"markdown","4efd2097":"markdown","dbff00d1":"markdown","2aadfdce":"markdown","1615f211":"markdown","552afa48":"markdown","ea08189a":"markdown","a94c41a6":"markdown","d5913b1c":"markdown","bfb89666":"markdown","2c49a591":"markdown","3fd7495e":"markdown","68f8c093":"markdown","11892ff0":"markdown","087eda86":"markdown","e198072f":"markdown","83989365":"markdown","a9b71a19":"markdown","8de87ed1":"markdown","07e47f1c":"markdown","752e282e":"markdown"},"source":{"90b1172e":"class Config:\n    '''general'''\n    debug = False\n    seed = 42\n    num_workers = 4\n    use_tpu = False\n    \n    '''data'''\n    batch_size = 64\n    target_col = \"Pawpularity\"\n    \n    '''model'''\n    # 'regnety_002', 'efficientnet_b3', 'efficientnet_b0', \n    # 'vit_base_patch16_224', 'tf_efficientnet_b4_ns'\n    model_name = 'tf_efficientnet_b3'\n    # number of predictors\n    targets = 1\n    # regressor features, number of useful features in meta data\n    # 12 means using all columns except id and target column\n    num_features = 12\n    # input image size send to network can be like 256, 512, 768, 1028\n    input_size = 260\n    # freeze backbone network\n    freeze_backbone = True\n    \n    '''training'''\n    n_fold = 5\n    trn_folds = [0, 1, 2, 3, 4]\n    epochs = 1000\n    print_freq = 10\n    lr = 1e-4\n    train = True\n    # for fp16 training change it to 16\n    precision = 16\n    patience = 5\n    \n    '''gradients'''\n    #adamw', 'adam'\n    optimizer = 'adamw'\n    weight_decay = 1e-6\n    gradient_accumulation_steps= 1\n    max_grad_norm = 1000\n    \n    '''lr scheduler'''\n    #'ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts'\n    lr_scheduler = 'CosineAnnealingLR'\n    factor = 0.2 # ReduceLROnPlateau\n    patience = 4 # ReduceLROnPlateau\n    eps = 1e-6 # ReduceLROnPlateau\n    T_max = 15 # CosineAnnealingLR\n    T_0 = 10 # CosineAnnealingWarmRestarts\n    min_lr = 1e-6\n    warmup_epochs = 0\n    multiplier = 10\n    \n    '''loggers -> weights and bias'''\n    loggers = True\n    project = \"pet-pawpularity\"\n    group = \"gpu-training\"","42cb5b36":"import subprocess\nif Config.use_tpu:\n    if not subprocess.check_call(\"curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\", shell=True):\n        print(\"setting up python xla\")\n    else:\n        print(\"cannot setup pytorch xla\")\n    if not subprocess.check_call(\"python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev\", shell=True):\n        print(\"successfuly installed xla and its dependencies\")\n    else:\n        print(\"cannot install xla and dependencies\")","7c7bcffc":"! pip install -qq ..\/input\/timm-pytorch-image-models\/pytorch-image-models-master","dfb31a89":"import os\nimport gc\nimport random\nimport time\nimport math\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom tqdm.auto import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport pytorch_lightning as pl\nimport torchmetrics as metrics\nimport timm\n\nfrom kaggle_secrets import UserSecretsClient\nfrom kaggle_datasets import KaggleDatasets\n\nimport sklearn.model_selection as ms\n\nimport wandb\n\nimport warnings\nwarnings.filterwarnings('ignore')","577bfadb":"# before running add your api secret inside Add-ons > Secrets\nos.environ[\"WANDB_SILENT\"] = \"true\"\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_api\")\nwandb.login(key=secret_value_0)","dc92f2a4":"train_csv = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\")\ntest_csv = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")\ntrain_data_path = \"..\/input\/petfinder-pawpularity-score\/train\"\ntest_data_path = \"..\/input\/petfinder-pawpularity-score\/test\"","639becd5":"train_csv[\"path\"] = train_csv[\"Id\"].apply(lambda x: os.path.join(train_data_path, x+\".jpg\"))\ntest_csv[\"path\"] = test_csv[\"Id\"].apply(lambda x: os.path.join(test_data_path, x+\".jpg\"))","62f4d7f2":"train_csv.head()","75289593":"test_csv.head()","169c0a1a":"# if debug mode use only 1000 samples for testing only make debug=False for actual training\nif Config.debug:\n    print(\"debug mode on\")\n    Config.epochs = 2\n    Config.print_freq = 100\n    Config.trn_folds = [0]\n    Config.loggers = False\n    #train_csv = train_csv.sample(n=1000, random_state=Config.seed).reset_index(drop=True)\nelse:\n    Config.loggers = True\n    print(\"debug mode off\")","43800db1":"pl.seed_everything(Config.seed)","a7a99b1e":"len(train_csv)","718f12af":"train_csv.shape","2ca1399c":"len(test_csv)","81fe6fad":"test_csv.shape","5aef468d":"train_csv.columns","4d7fa887":"train_csv.isnull().sum()","0ce8b61d":"test_csv.isnull().sum()","aaeac438":"for col in train_csv.columns:\n    if col not in [\"Id\", Config.target_col, \"path\"]:\n        print(train_csv[col].value_counts())","0130a776":"max_paws = train_csv[Config.target_col].max()\nprint(max_paws)","1b51354d":"class CrossValidation:\n    def __init__(self, df, shuffle,random_state=None):\n        self.df = df\n        self.random_state = random_state\n        self.shuffle = shuffle\n        if shuffle is True:\n            self.df = df.sample(frac=1,\n                random_state=self.random_state).reset_index(drop=True)\n        else:\n            self.random_state=None\n\n    def hold_out_split(self,percent,stratify=None):\n        if stratify is not None:\n            y = self.df[stratify]\n            train,val = ms.train_test_split(self.df, test_size=percent\/100,\n                stratify=y, random_state=self.random_state)\n            return train,val\n        size = len(self.df) - int(len(self.df)*(percent\/100))\n        train = self.df.iloc[:size,:]\n        val = self.df.iloc[size:,:]\n        return train, val\n\n    def kfold_split(self, splits, stratify=None):\n        if stratify is not None:\n            kf = ms.StratifiedKFold(n_splits=splits, \n                                    shuffle=self.shuffle,\n                                    random_state=self.random_state\n                                    )\n            y = self.df[stratify]\n            for train, val in kf.split(X=self.df,y=y):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t, v\n        else:\n            kf = ms.KFold(n_splits=splits, shuffle=self.shuffle,\n                random_state=self.random_state)\n            for train, val in kf.split(X=self.df):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t, v","fe4a3542":"class PawpularityDataset(torch.utils.data.Dataset):\n    def __init__(self, csv_file, augmentations=None, test=False):\n        super(PawpularityDataset, self).__init__()\n        self.csv = csv_file\n        self.test = test\n        self.augs = augmentations\n        self.length = len(self.csv)\n    \n    def __len__(self):\n        return self.length\n    \n    def __getitem__(self, idx):\n        path = self.csv.iloc[idx][\"path\"]\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.augs is not None:\n            img = self.augs(image=img)['image']\n        else:\n            img = torch.from_numpy(img).float()\n            img = img.permute(2, 0, 1)\n        meta = torch.from_numpy(self.csv.iloc[idx, 1:13].values.astype(np.float32))\n        if self.test:\n            return img, meta\n        label = torch.tensor(self.csv.iloc[idx][Config.target_col], dtype=torch.float)\/max_paws\n        return img, meta, label","1e92dc96":"class ImageAugmentations:\n    '''\n        image_size: resize image to -> (width, height)\n        train_augs: include augmentations like random crop, rotation etc training if false then return\n                    only resize image as pytorch tensor\n    '''\n    def __init__(self, image_size, apply_augs=False):\n        self.image_size = image_size\n        self.apply_augs = apply_augs\n        \n    def train_augs(self):\n        if self.apply_augs:\n            return A.Compose([A.Resize(self.image_size, self.image_size),\n                              A.HorizontalFlip(p=.5),\n                              A.ChannelShuffle(p=.1),\n                              A.ColorJitter(brightness=0.1,\n                                            hue=0.1,\n                                            saturation=0.1,\n                                            contrast=0.1,\n                                            p=.2),\n                              A.RandomGamma(p=.1),\n                              A.Sharpen(p=.1),\n                              A.Cutout(p=.2),\n                              # imagenet normalization\n                              A.Normalize(mean=[0.485, 0.456, 0.406],\n                                          std=[0.229, 0.224, 0.225],\n                                          max_pixel_value=255.0,\n                                          p=1.0),\n                              ToTensorV2()])\n        return A.Compose([A.Resize(self.image_size, self.image_size),\n                          A.Normalize(mean=[0.485, 0.456, 0.406],\n                                      std=[0.229, 0.224, 0.225],\n                                      max_pixel_value=255.0,\n                                      p=1.0),\n                          ToTensorV2()])\n    \n    def valid_augs(self):\n        return A.Compose([A.Resize(self.image_size, self.image_size),\n                          A.Normalize(mean=[0.485, 0.456, 0.406],\n                                      std=[0.229, 0.224, 0.225],\n                                      max_pixel_value=255.0,\n                                      p=1.0),\n                          ToTensorV2()])\n        ","14b4379d":"augs = ImageAugmentations(Config.input_size, apply_augs=True)\nds = PawpularityDataset(train_csv, augmentations=augs.train_augs())\nloader = torch.utils.data.DataLoader(ds,\n                                     shuffle=True,\n                                     batch_size=16)\nimages, meta, labels = next(iter(loader))\nplt.figure(figsize=(15, 15))\nprint(torch.max(images))\nprint(torch.min(images))\nfor step, (image, label) in enumerate(zip(images, labels)):\n    plt.subplot(4, 4, step+1)\n    plt.imshow(image.permute(1, 2, 0).numpy())\n    plt.axis('off')\n    plt.title(f'Pawpularity: {label:.2f}')","f86d69aa":"class PawpularityModel(nn.Module):\n    def __init__(self, cfg, pretrained=False):\n        super(PawpularityModel, self).__init__()\n        self.cfg = cfg\n        timm_model = timm.create_model(self.cfg.model_name, \n                                       pretrained=pretrained, \n                                       in_chans=3)\n        \n        if self.cfg.freeze_backbone:\n            modules = []\n            for module in timm_model.children():\n                for param in module.parameters():\n                    param.requires_grad = False\n                modules.append(module)\n        else:\n            modules = list(timm_model.children())\n        \n        self.classifier = modules[-1]\n        cnn_out_features = self.classifier.in_features\n        classifier_out_features = self.classifier.out_features\n        \n        self.cnn = nn.Sequential(*modules[:-1])\n        self.dropout = nn.Dropout(0.2)\n        \n        self.meta_reg = nn.Linear(cfg.num_features, cfg.num_features)\n        num_features = classifier_out_features + cfg.num_features + cnn_out_features\n        self.regressor_1 = nn.Sequential(nn.Linear(num_features, 128),\n                                         nn.SiLU(),\n                                         nn.Dropout(0.2),\n                                         nn.Linear(128, 64),\n                                         nn.SiLU(),\n                                         nn.Dropout(0.2),\n                                         nn.Linear(64, 32),\n                                         nn.SiLU(),\n                                         nn.Dropout(0.2),\n                                         nn.Linear(32, 16),\n                                         nn.SiLU(),\n                                         nn.Dropout(0.2),\n                                         nn.Linear(16, cfg.targets))\n        num_features = cnn_out_features + cfg.num_features\n        self.regressor_2 = nn.Sequential(nn.Linear(num_features, 64),\n                                         nn.SiLU(),\n                                         nn.Dropout(0.2),\n                                         nn.Linear(64, cfg.targets))\n        \n    def forward(self, img, meta):\n        meta = self.meta_reg(meta)\n        cnn_features = self.cnn(img)\n        if self.cfg.freeze_backbone:\n            classes = self.classifier(cnn_features)\n            cnn_features = self.dropout(cnn_features)\n            x = torch.cat((cnn_features, meta, classes), dim=1)\n            x = self.dropout(x)\n            return self.regressor_1(x)\n        cnn_features = self.dropout(cnn_features)\n        x = torch.cat((cnn_features, meta), dim=1)\n        x = self.dropout(x)\n        return self.regressor_2(x)","69b2b29a":"class LitPawpularity(pl.LightningModule):\n    def __init__(self, cfg, model, fold):\n        super(LitPawpularity, self).__init__()\n        self.cfg = cfg\n        self.model = model\n        self.criterion = nn.BCEWithLogitsLoss()\n        self.rmse = metrics.MeanSquaredError(squared=False)\n        self.fold = fold\n        \n    def forward(self, img, meta):\n        return self.model(img, meta)\n\n    def configure_optimizers(self):\n        self.optimizer = self.__get_optimizer()\n        self.lr_scheduler = self.__get_lr_scheduler()\n        return {'optimizer': self.optimizer, 'lr_scheduler': self.lr_scheduler}\n    \n    def training_step(self, batch, batch_idx):\n        img, meta, y = batch\n        y_hat = self(img, meta)\n        loss = self.criterion(y_hat.view(-1), y)\n        rmse = self.__calc_rmse(y_hat.view(-1), y)\n        logs = {'train_loss': loss, \n                'train_rmse': rmse,\n                'lr': self.optimizer.param_groups[0]['lr']}\n        self.log_dict(\n            logs,\n            on_step=True, on_epoch=True, prog_bar=True, logger=True\n        )\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        img, meta, y = batch\n        y_hat = self(img, meta)\n        loss = self.criterion(y_hat.view(-1), y)\n        rmse = self.__calc_rmse(y_hat.view(-1), y)\n        logs = {'val_loss': loss, \n                'val_rmse': rmse}\n        self.log_dict(\n            logs,\n            on_step=True, on_epoch=True, prog_bar=True, logger=True\n        )\n        return logs\n    \n    def validation_epoch_end(self, outputs):\n        rmse = torch.stack([x['val_rmse'] for x in outputs]).mean()\n        loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        print(f'Epoch {self.current_epoch} : Fold {self.fold} -> loss: {loss}\\t rmse: {rmse}')\n        return outputs\n    \n    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n        img, meta, _ = batch\n        out = self(img, meta).flatten()\n        return F.sigmoid(out)\n\n    def __get_optimizer(self):\n        optimizer = None\n        if self.cfg.optimizer == 'adam':\n            optimizer = torch.optim.Adam(self.parameters(), lr=self.cfg.lr, \n                                         weight_decay=self.cfg.weight_decay, \n                                         amsgrad=False)\n        elif self.cfg.optimizer == 'adamw':\n            optimizer = torch.optim.AdamW(self.parameters(), lr=self.cfg.lr, \n                                          weight_decay=self.cfg.weight_decay)\n        return optimizer\n\n    def __get_lr_scheduler(self):\n        scheduler = None\n        if self.cfg.lr_scheduler=='ReduceLROnPlateau':\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', \n                                                                   factor=self.cfg.factor, \n                                                                   patience=self.cfg.patience, \n                                                                   eps=self.cfg.eps)\n        elif self.cfg.lr_scheduler=='CosineAnnealingLR':\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, \n                                                                   T_max=self.cfg.T_max, \n                                                                   eta_min=self.cfg.min_lr, \n                                                                   last_epoch=-1)\n        elif self.cfg.lr_scheduler=='CosineAnnealingWarmRestarts':\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer,\n                                                                             T_0=self.cfg.T_0,\n                                                                             eta_min=self.cfg.min_lr,\n                                                                             last_epoch=-1)\n        return scheduler\n    \n    def __calc_rmse(self, preds, targets):\n        preds = F.sigmoid(preds)\n        return self.rmse(preds, targets)","fe325129":"class PawpularityDataModule(pl.LightningDataModule):\n    def __init__(self, \n                 train_files, \n                 val_files,\n                 batch_size: int = 32,\n                 image_size: int = Config.input_size,\n                 apply_augmentations: bool = False):\n        super(PawpularityDataModule, self).__init__()\n        self.train_files = train_files\n        self.val_files = val_files\n        self.batch_size = batch_size\n        self.augs = ImageAugmentations(image_size, apply_augs=apply_augmentations)\n\n    def setup(self, stage=None):\n        self.train_ds = PawpularityDataset(self.train_files,\n                                           augmentations=self.augs.train_augs(),\n                                           test=False)\n        self.val_ds = PawpularityDataset(self.val_files,\n                                         augmentations=self.augs.valid_augs(),\n                                         test=False)\n\n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(self.train_ds,\n                                           shuffle=True,\n                                           num_workers=Config.num_workers, \n                                           pin_memory=True,\n                                           drop_last=True,\n                                           batch_size=self.batch_size)\n\n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(self.val_ds,\n                                           shuffle=False,\n                                           num_workers=Config.num_workers, \n                                           pin_memory=True,\n                                           drop_last=False,\n                                           batch_size=self.batch_size*2)","4122488e":"def model_checkpointing_callbacks(fold):\n    return [pl.callbacks.ModelCheckpoint(monitor='val_loss',\n                                         dirpath=\".\/\",\n                                         save_top_k=1, \n                                         save_last=False, \n                                         save_weights_only=True, \n                                         filename=f'best\/{Config.model_name}_best_loss_fold{fold}', \n                                         verbose=True, \n                                         mode='min'),\n            pl.callbacks.ModelCheckpoint(monitor='val_rmse',\n                                         dirpath=\".\/\",\n                                         save_top_k=1, \n                                         save_last=False, \n                                         save_weights_only=True, \n                                         filename=f'best\/{Config.model_name}_best_rmse_fold{fold}', \n                                         verbose=True,\n                                         mode='min')]","7b116f5f":"def wandb_logger_init(name, job_type, **kwargs):\n    return pl.loggers.WandbLogger(project=Config.project,\n                                  name=name,\n                                  group=Config.group, \n                                  job_type=job_type,\n                                  config = {k:v for k,v in Config.__dict__.items() if not k.startswith(\"__\")},\n                                  **kwargs)","8226ddde":"def get_loggers(fold):\n    if Config.loggers:\n        return [wandb_logger_init(name=f\"{Config.model_name}_fold{fold}\", job_type=\"training\"),\n                pl.loggers.CSVLogger(\"logs\", name=f\"{Config.model_name}_fold{fold}\")]\n    return None","54eed6b4":"if Config.train:\n    cv = CrossValidation(train_csv, shuffle=True, random_state=Config.seed)\n    for fold, (train_, val_) in enumerate(cv.kfold_split(splits=Config.n_fold)):\n        if fold in Config.trn_folds:\n            print(f\"{'='*10} Fold {fold} {'='*10}\")\n            datamodule = PawpularityDataModule(train_, \n                                               val_,\n                                               batch_size=Config.batch_size,\n                                               apply_augmentations=True)\n            datamodule.setup()\n            loggers = get_loggers(fold)\n            callbacks = [pl.callbacks.EarlyStopping(monitor='val_loss', \n                                                    patience=Config.patience, \n                                                    mode='min')]\n            model = PawpularityModel(Config, pretrained=True)\n            checkpoint_callbacks = model_checkpointing_callbacks(fold)\n            lit = LitPawpularity(Config, model, fold=fold)\n            trainer_params = {\"max_epochs\": Config.epochs,\n                              \"accumulate_grad_batches\": Config.gradient_accumulation_steps,\n                              \"precision\": Config.precision,\n                              \"callbacks\": callbacks+checkpoint_callbacks,\n                              \"logger\": loggers,\n                              \"progress_bar_refresh_rate\": Config.print_freq}\n            if Config.use_tpu:\n                trainer_params[\"tpu_cores\"] = 8\n            else:\n                trainer_params[\"gpus\"] = -1\n            trainer = pl.Trainer(**trainer_params)\n            # Train the model\n            trainer.fit(lit, datamodule)\n            # Close wandb run\n            wandb.finish()","ece43127":"def get_weights_path(fold, mode):\n    return os.path.join(\"best\", f\"{Config.model_name}_best_{mode}_fold{fold}.ckpt\")","7cb51e18":"def get_fold_predictions(fold, data):\n    print(\"=\"*10)\n    print(\"Predictions using Fold: \", fold)\n    print(\"=\"*10)\n    augs = ImageAugmentations(Config.input_size, apply_augs=False)\n    ds = PawpularityDataset(data,\n                            augmentations=augs.valid_augs(),\n                            test=False)\n    weights = [get_weights_path(fold, \"loss\"),\n               get_weights_path(fold, \"rmse\")]\n    preds = []\n    for weight in weights:\n        print(\"Using weights: \", weight)\n        loader = torch.utils.data.DataLoader(ds,\n                                             shuffle=False,\n                                             num_workers=Config.num_workers, \n                                             pin_memory=True,\n                                             drop_last=False,\n                                             batch_size=Config.batch_size*2)\n        model = PawpularityModel(Config, pretrained=False)\n        lit = LitPawpularity.load_from_checkpoint(weight, cfg=Config, model=model, fold=fold)\n        trainer_params = {}\n        if Config.use_tpu:\n            trainer_params[\"tpu_cores\"] = 8\n        else:\n            trainer_params[\"gpus\"] = -1\n        trainer = pl.Trainer(**trainer_params)\n        predictions = trainer.predict(lit, loader)\n        predictions = torch.cat([x for x in predictions]).detach().cpu().numpy()\n        preds.append(predictions)\n    preds = np.mean(np.column_stack(preds), axis=1)\n    return preds","cb30ccd1":"oof_df = pd.DataFrame()\ncv = CrossValidation(train_csv, shuffle=True, random_state=Config.seed)\nfor fold, (_, val_) in enumerate(cv.kfold_split(splits=Config.n_fold)):\n    if fold in Config.trn_folds:\n        _oof_df = pd.DataFrame()\n        _oof_df[\"Id\"] = val_[\"Id\"]\n        _oof_df[\"Pawpularity\"] = val_[\"Pawpularity\"]\n        _oof_df[\"Predictions\"] = get_fold_predictions(fold, val_) * max_paws\n        oof_df = pd.concat([oof_df, _oof_df])\noof_df.to_csv(f\"oof_{Config.model_name}.csv\", index=False)","1161fc40":"cv_rmse = metrics.functional.mean_squared_error(\n    torch.from_numpy(oof_df[\"Pawpularity\"].values), \n    torch.from_numpy(oof_df[\"Predictions\"].values),\n    squared=False)\nprint(\"CV Score RMSE: \", cv_rmse.numpy())","d1d018b7":"oof = pd.read_csv(f\"oof_{Config.model_name}.csv\")\noof.head()","9b856392":"### What's New\n- Training with GPU and TPU\n- Saving OOF files for later blending\n- Removing unnecessary code sections that are used for inference only, that will be part of inference notebook only\n- model used : tf_efficientnet_b2 + meta\n- added functionality to freeze backbone also through config\n- if pretrained backbone is freeze we can use class information we get from model's classfier as extra data.\n- make logger to pause when we are in debug mode for testing code.","164c31e4":"### Data Augmentation helpers","4efd2097":"# Loading Data","dbff00d1":"### Lightning data module","2aadfdce":"# Model defining and lightning functionality\n\n- Model takes images and meta data as input\n- using a pretrained cnn (we can finetune or freeze depending on situtation) and extract image features\n- if network backbone is freezed we can pass images features to its trained classifier and get the class which network thinks present in image\n- combine all features ie. image features + meta_features + classes(if using freezed backbone)\n- pass these features to a regressor that gives pawpularity output","1615f211":"# \ud83d\udc15\ud83d\udc15\ud83d\udc15\ud83d\udc15\ud83d\udc15\ud83e\uddae\ud83e\uddae\ud83e\uddae\ud83e\uddae\ud83e\uddae\ud83d\udc04\ud83d\udc04\ud83d\udc04\ud83d\udc04\ud83d\udc04\ud83d\udc04","552afa48":"### Dataset loader helper","ea08189a":"# PAWPULARITY PETFINDER\n\n![Photo by Eddie Galaxy from Pexels](https:\/\/images.pexels.com\/photos\/3628100\/pexels-photo-3628100.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=650&w=940)\n\n> Saving one animal won't change the world, but it will change the world for that one animal.","a94c41a6":"### Model checkpointing","d5913b1c":"# Configurations","bfb89666":"### plotting images from dataset","2c49a591":"## Objective\n**Predict engagement with a pet's profile based on the photograph for that profile**\n\nTo predict engagement we have to predict pawpularity score, which is derived from each pet profile's page view statistics at the listing pages, using an algorithm that normalizes the traffic data across different pages, platforms (web & mobile) and various metrics.\n\n## Evaluation\n**RMSE (Root Mean Square Error)**\n\n$$ \\Large RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n\n## Internet Disabled\nHere since we are using wandb and pretrained model we have to enable internet, for submission we will create a new notebook with intenet disabled. Here is an inference [notebook](https:\/\/www.kaggle.com\/tarunbisht11\/find-a-pet-with-lightning-speed-inference).","3fd7495e":"### Training Loggers","68f8c093":"# Basic EDA","11892ff0":"# Saving OOF Predictions ","087eda86":"### Cross validation folds creation validation helper","e198072f":"# Installing dependencies","83989365":"# Training","a9b71a19":"### Installing\n- tpu pytorch if using tpus\n- timm vision models library","8de87ed1":"# Define Helpers","07e47f1c":"### Lightning Module","752e282e":"### wandb login"}}