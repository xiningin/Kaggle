{"cell_type":{"032e1ec1":"code","15de03b6":"code","bd5689f8":"code","573b914f":"code","61353a19":"markdown","48e7d405":"markdown","405995ca":"markdown"},"source":{"032e1ec1":"#\n# Generate dummy data.\n#\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\n\n# Generate dummy data\nimport numpy as np\nimport pandas as pd\n\nX_train = np.random.random((1000, 3))\ny_train = pd.get_dummies(np.argmax(X_train[:, :3], axis=1)).values\nX_test = np.random.random((100, 3))\ny_test = pd.get_dummies(np.argmax(X_test[:, :3], axis=1)).values","15de03b6":"#\n# Build a KerasClassifier wrapper object.\n# I had trouble getting the callable class approach to work. The method approach seems to be pretty universial anyway.\n#\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n# Doesn't work?\n# class TwoLayerFeedForward:\n#     def __call__():\n#         clf = Sequential()\n#         clf.add(Dense(9, activation='relu', input_dim=3))\n#         clf.add(Dense(9, activation='relu'))\n#         clf.add(Dense(3, activation='softmax'))\n#         clf.compile(loss='categorical_crossentropy', optimizer=SGD())\n#         return clf\n\ndef twoLayerFeedForward():\n    clf = Sequential()\n    clf.add(Dense(9, activation='relu', input_dim=3))\n    clf.add(Dense(9, activation='relu'))\n    clf.add(Dense(3, activation='softmax'))\n    clf.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=[\"accuracy\"])\n    return clf\n\n\n# clf = KerasClassifier(TwoLayerFeedForward(), epochs=100, batch_size=500, verbose=0)\nclf = KerasClassifier(twoLayerFeedForward, epochs=100, batch_size=500, verbose=0)","bd5689f8":"from sklearn.model_selection import StratifiedKFold, cross_val_score\n\ntrans = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\nimport pandas as pd\n\n# Keras classifiers work with one hot encoded categorical columns (e.g. [[1 0 0], [0 1 0], ...]).\n# StratifiedKFold works with categorical encoded columns (e.g. [1 2 3 1 ...]).\n# This requires juggling the representation at shuffle time versus at runtime.\nscores = []\nfor train_idx, test_idx in trans.split(X_train, y_train.argmax(axis=1)):\n    X_cv, y_cv = X_train[train_idx], pd.get_dummies(y_train.argmax(axis=1)[train_idx]).values\n    clf.fit(X_cv, y_cv)\n    scores.append(clf.score(X_cv, y_cv))","573b914f":"scores","61353a19":"## Conclusion\n\nAs you can see the accuracy scores that we achieve vary widely between consequetive runs, indicating that our model has not yet found enough signal in the dataset (we should increase the number of epochs, increase the learning rate, or decrease the batch size, or all of the above).\n\nI couldn't get the callable class approach to work, unfortunately. I didn't want to get bogged down digging too deep, but in poking around online I noticed that every example I see uses a factory function to build the Keras classifier...\n\nThere are still some awkward edges around the interaction of `keras` and `sklearn`. In this example we see that we have to perform representational transformations on the target columns from one-hot to a categorical encoding to one-hot encoding again. So it seems that whilst having this pipeline code helps a lot, there's still some glue code that you have to write yourself!\n\n[Wrapping Keras learners in scikit-learn pipelines](https:\/\/www.kaggle.com\/residentmario\/pipelines-with-linux-gamers) seems like a good way to go for production development environments.\n\nYou can use Keras from within a `scikit-learn` pipeline as a grid search target: https:\/\/machinelearningmastery.com\/grid-search-hyperparameters-deep-learning-models-python-keras\/.","48e7d405":"# Using keras models with scikit-learn pipelines\n\n## Discussion\n\n`sklearn` is Python's general purpose machine learning library, and it features a lot of utilities not just for building learners but for pipelining and structuring them as well. `keras` models don't work with `sklearn` out of the box, but they can be made compatible quite easily. To be compatible with `sklearn` utilities on a basic level a learner need only be a class object with `fit`, `predict`, and `score` methods (and optionally a `predict_proba`), so you can write a quick object wrapper that delegates these methods on a `keras` object.\n\nHowever this is unnecessary because `keras` comes with a wrapper built in. This is described in the `keras` docs [here](https:\/\/keras.io\/scikit-learn-api\/).\n\nThere are two wrappers, one for classifiers and one for regressors. The signatures are `keras.wrappers.scikit_learn.KerasClassifier(build_fn=None, **sk_params)` and `keras.wrappers.scikit_learn.KerasRegressor(build_fn=None, **sk_params)` respectively.\n\nThe `build_fn` parameter should be given a factory function (or a functional object, e.g. an object that defines a `__call__()` method) that returns the model.\n\nThe `sk_params`parameter can be used to pass parameters to the `fit`, `predict`, `predict_proba`, and `score` methods, which are the aforementioned \"standard interface\" methods for a scikit-learn compatible predictor. These methods by default will take on the values that you set for them inside of the factory function, but this parameter does is it allows you to change them manually at call time. To change parameters unambiguously, use a dictionary whose first-level keys is the methods whose parameters are being modified. To change parameters all at once, pass a top-level key-value pair; this will be passed down to all of these methods.","405995ca":"## Demonstration\n\nA quick demonstration of this wrapper follows. In this code, we will perform cross validation on the Keras model accuracy using the `StatifiedKFold` method in the `sklearn` library."}}