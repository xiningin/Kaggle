{"cell_type":{"92734cbb":"code","ac323fb6":"code","6b323ca9":"code","11d092dc":"code","885e0fe2":"code","ce77c3a8":"code","24b9d891":"code","e1d004f7":"code","33bf1ea5":"code","40127f8b":"code","b78a62b5":"code","8e004cec":"code","251b6bce":"code","5339fcb7":"code","7cf5b607":"code","a4abfe08":"code","0d3e194d":"code","6e9f125d":"code","311f6fc0":"code","037f7b2e":"code","e365bafe":"code","cb08b15a":"code","aab8522d":"code","725f3f41":"code","0f8daaf3":"code","7db57d5e":"code","f04b4d61":"code","758c790f":"code","05240cf2":"code","4fb9b33d":"code","b4e75d2a":"code","fc406306":"code","71587a5a":"code","9436f3b9":"code","51c4a329":"code","adc8488d":"code","b4daa9b0":"code","e415db8f":"code","b22b7dca":"code","85e5f594":"code","cc246e12":"code","a371482f":"code","6f2654ba":"code","04b7a146":"code","8c246bd9":"code","ab797873":"code","722200d8":"code","5a14bf5d":"code","f03fcf71":"code","d1749cfa":"code","d299d4d2":"code","08fdbc12":"code","3493b9af":"code","2e21ac88":"code","99e02161":"code","218b89dd":"code","76b88184":"code","0af0f160":"code","a75697a6":"code","9181fde5":"markdown","ab0e881f":"markdown","672940e5":"markdown","4cfe93e2":"markdown","d4d2f852":"markdown","361b9815":"markdown","19c4e1ba":"markdown","8e3847ac":"markdown","441fa737":"markdown","3feed237":"markdown","64fdc97c":"markdown","b77a7ba8":"markdown","2c0d6b9a":"markdown","f684d1db":"markdown","64aba795":"markdown","69cb8484":"markdown","863ad248":"markdown","29f6729a":"markdown","9e1c1ee9":"markdown","4432d96b":"markdown","34144fcb":"markdown","0a53570b":"markdown","fa22a2af":"markdown","3d53e4e3":"markdown","2e6fe9d6":"markdown","46f902c5":"markdown"},"source":{"92734cbb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ac323fb6":"import matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport seaborn as sns\nimport itertools\n%matplotlib inline\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport math\nplt.style.use(\"seaborn-whitegrid\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import r2_score,mean_squared_error,mean_squared_log_error\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom catboost import CatBoostRegressor, Pool, cv\nimport catboost as cbr\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy import stats\nfrom scipy.stats import skew\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6b323ca9":"#Reading the file\nraw_file = pd.read_csv(\"\/kaggle\/input\/seoul-bike-rental-ai-pro-iti\/train.csv\")\nfile = raw_file.copy()\ndebug=0  # (=0) To Skip Visualization during debugging to consume time, else (=1) To Enable Visualization plot.\nfile.columns","11d092dc":"file.describe()","885e0fe2":"file","ce77c3a8":"#Null Checking\nfile.isnull().sum(axis=0)","24b9d891":"#duplicates Checking\nfile.duplicated().sum()","e1d004f7":"# check when the not a workingday there is no rental of bike\n#But found there exist renting\nz=file[file['Functioning Day']=='No']\nz['y'].shape\nprint(z['y'].unique())","33bf1ea5":"# Data Correlation\nif debug != 0:\n    corr = file[['Temperature(\ufffdC)','Humidity(%)', 'Wind speed (m\/s)','Visibility (10m)', 'Dew point temperature(\ufffdC)', 'Solar Radiation (MJ\/m2)', 'Rainfall(mm)', 'Snowfall (cm)','y']].corr()\n    f,axes = plt.subplots(1,1,figsize = (10,10))\n    sns.heatmap(corr,square=True,annot = True,linewidth = .5,center = 2,ax = axes)","40127f8b":"#right Skewed Label\nif debug != 0:\n    _=sns.histplot(file['y'])\n    _=plt.title(\"Visualizing Target column\")\n    _=plt.xlabel(\"Rented bikes\")\n    _=plt.ylabel(\"Frequency\")","b78a62b5":"#Data Distributions\nif debug != 0:\n    plt.figure(figsize=(25, 25))\n    for i, col in enumerate(list(file.columns)):\n        plt.subplot(7, 4, i+1)\n        sns.histplot(file[col], kde=True, bins=10)","8e004cec":"#drawFeatures_VS_y\nif debug != 0:\n    plt.figure(figsize=(25, 25))\n    for i, col in enumerate(list(file.columns)):\n        plt.subplot(7, 4, i+1)\n        col_rental = file.groupby(col,as_index=False)['y'].mean()\n        sns.scatterplot(data = col_rental,x=col,y='y')","251b6bce":"# see the mean of label to every unique value of each column\n#may be helpful to know most important features and for featur engineerng and encoding\ndef insights(df):\n    for col in df.columns:\n        if col=='y':\n            continue\n        else:\n            display(df[[col, 'y']].groupby([col], as_index=False).mean().sort_values(by='y', ascending=False).T)\n            \ninsights(file)","5339fcb7":"#Calc the skeweness of each continous feature\n\ndef calc_skew(df):\n    print(\"\\nIF THE DATA IS HIGHLY SKEWED IF SKWENESS  > 1 OR < -1 \\n\")\n    for col in df.loc[:, df.dtypes != np.object ]:\n        print(\"the skewness of \",col,\"is :\",df[col].skew())\n\ncalc_skew(file)","7cf5b607":"file.columns","a4abfe08":"file.describe().T","0d3e194d":"X = file.copy()\nX = X[['y',  'Hour', 'Temperature(\ufffdC)', 'Humidity(%)',\n       'Wind speed (m\/s)', 'Visibility (10m)', 'Dew point temperature(\ufffdC)',\n       'Solar Radiation (MJ\/m2)', 'Rainfall(mm)', 'Snowfall (cm)', 'Seasons',\n       'Holiday', 'Functioning Day',\n      ]]\ny = X.pop('y')\n\n# Label encoding for categoricals\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = X.dtypes == int","6e9f125d":"discrete_features","311f6fc0":"def make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\n# mi_scores[::3]  # show a few features with their MI scores","037f7b2e":"mi_scores","e365bafe":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)","cb08b15a":"#Reading the file again to provide any mistakes above\nraw_file = pd.read_csv(\"\/kaggle\/input\/seoul-bike-rental-ai-pro-iti\/train.csv\")\nfile = raw_file.copy()","aab8522d":"def encodingHoliday(dataset, cols):\n    for col_name in cols:\n        dataset[col_name] = dataset[col_name].replace({'Holiday': 1, 'No Holiday': 0})\n    return dataset\n\ndef encodingFunctioningDay(dataset, cols):\n    for col_name in cols:\n        dataset[col_name] = dataset[col_name].replace({'Yes': 1, 'No': 0})\n    return dataset\n\ndef encodingSeasons(dataset, cols):\n    for col_name in cols:\n        dataset[col_name] = dataset[col_name].replace({'Winter': 1, 'Spring': 2, 'Summer': 3, 'Autumn': 4})\n    return dataset\n\ndef encoding_label_day_night(dataset, cols):\n    for col_name in cols:\n        dataset[col_name] = dataset[col_name].replace({'Day': 1, 'Night': 0})\n    return dataset\n\ndef encodingTempLevels(dataset, cols):\n    for col_name in cols:\n        dataset[col_name] = dataset[col_name].replace({'02_Medium_Temp': 0, '01_Low_Temp': 1, '03_High_Temp': 2})\n    return dataset\n\ndef encodingWindSpeedLevels(dataset, cols):\n    for col_name in cols:\n        dataset[col_name] = dataset[col_name].replace({'02_Medium_Speed': 0, '01_Low_Speed': 1, '03_High_Speed': 2})\n    return dataset\n\ndef encodingWeekDay(dataset, cols):\n    for col_name in cols:\n        dataset[col_name] = dataset[col_name].replace({'Sunday': 6, 'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5})\n    return dataset\n\ndef encoding_rush_hours(dataset, cols):\n    for col_name in cols:\n        dataset[col_name] = dataset[col_name].replace({0 : 2, 1: 2, 2: 1, 3: 1, 4: 0, 5: 0,\\\n                                                       6: 1, 7: 3, 8: 5, 9: 3, 10: 2, 11: 3,\\\n                                                       12: 4, 13: 4, 14: 4, 15: 4, 16: 5, 17: 6,\\\n                                                       18: 7, 19: 6, 20:6, 21: 6, 22: 5, 23:3})\n    return dataset\n\ndef encoding_Peak_hours(dataset, cols):\n    for col_name in cols:\n        dataset[col_name] = dataset[col_name].replace({4 : 0 , 5 : 1 , 3 : 2 , 6 : 3 , 2 : 4 , 1 : 5 , 10 : 6 , 0 : 7 , 11 : 8 , 7 : 9 , 9 : 10 , 23 : 11 , 12 : 12 , 13 : 13 , 14 : 14 , 15 : 15 , 22 : 16 , 16 : 17 , 8 : 18 , 21 : 19 , 20 : 20 , 17 : 21 , 19 : 22 , 18 : 23 })\n    return dataset\n\ndef normalize_column(file,column):\n    return MinMaxScaler().fit_transform(np.array(file[column]).reshape(-1,1))\n\ndef removeOutliers(df,outliersColomns):\n    z_scores = stats. zscore(df[outliersColomns]) \n    abs_z_scores = np. abs(z_scores)\n    filtered_entries = (abs_z_scores < 3). all(axis=1)\n    new_df = df[filtered_entries]\n    return new_df\n\ndef removeDuplicate(df_):\n    return (df_.drop_duplicates( keep = False, inplace = True))\n     \ndef oneHotEncoding(df):\n    return pd.get_dummies(df, drop_first=True)\n\ndef meanEncodingFeature(df_source , df_target , feature_name, label_):\n    Mean_encoded_subject = df_source.groupby([feature_name])[label_].mean().to_dict()\n    df_target[feature_name] = df_target[feature_name].map(Mean_encoded_subject)\n    return df_target\n\ndef getMeanEncodingDict( df_source , feature_name, label_):\n    return (df_source.groupby([feature_name])[label_].mean().to_dict())\n\ndef encodingFeatureWithDict(df_target , feature_name, map_dict):\n    #data[num] if num in data else data[min(data.keys(), key=lambda k: abs(k-num))]\n    #print('Keys are ',map_dict.keys())\n    #df_array= df_target[feature_name].as\n    for i in range(len(df_target[feature_name])):\n        #print('encoding ',feature_name,' at i=',i,' with value ',df_target[feature_name].array[i])\n        if df_target[feature_name].array[i] not in map_dict.keys():\n            df_target[feature_name].array[i] = min(map_dict.keys(), key=lambda k: abs(k-df_target[feature_name].array[i]))\n    df_target[feature_name] = df_target[feature_name].map(map_dict)\n    return df_target\n\ndef sortDF(df_,col_name):\n    df_= df_.sort_values(by=col_name,ascending=True)\n    return df_\n\ndef swapDFRows(df_,row_index_1,row_index_2):\n    temp = df_.iloc[row_index_1].copy()\n    df_.iloc[row_index_1] = df_.iloc[row_index_2]\n    df_.iloc[row_index_2] = temp\n    return df_\n\ndef sortPredictionsAsNormalDistribution(df_source,col_name):\n    sortedPredictions = sortDF(df_source,col_name)\n    left_ = 0\n    right_ = sortedPredictions.y.count()-1\n    middle_ = int(sortedPredictions.y.count()\/2)\n    if (right_%2==0):\n        right_=right_-1\n    df_left = pd.DataFrame(columns = sortedPredictions.columns)\n    df_right = pd.DataFrame(columns = sortedPredictions.columns)\n    for i in range(middle_+2):\n        #print('i= ',i,' ,left = ',left_,' ,right = ',right_)\n        if left_ < (sortedPredictions.y.count()-1) :\n            #print(df_left)\n            df_left = df_left.append(sortedPredictions.iloc[left_])\n        if right_ > 0:\n            df_right = df_right.append(sortedPredictions.iloc[right_])\n        left_ = left_ + 2\n        right_ = right_ - 2\n    #print(df_left)\n    sortedPredictions = df_left.append(df_right)\n    return sortedPredictions","725f3f41":"# Transform data\ndef transformation(df,columns,func):\n    for col in columns:\n        df[col]=func(df[col])\n    return df\n\n#Demo of function params\n#transformation(test_df,['Temperature(\ufffdC)','Hour'],np.log1p)\n\n# Adding Day Month Year to data frame\ndef add_day_month_year(df):\n    df['Year'] =  pd.DatetimeIndex(df['Date']).year\n    df['Month'] =  pd.DatetimeIndex(df['Date']).month\n    df['day'] =  pd.DatetimeIndex(df['Date']).day\n    df['weekday'] =  pd.DatetimeIndex(df['Date']).dayofweek\n    df['weekofyear']= pd.DatetimeIndex(df['Date']).weekofyear\n    df['dayofyear']= pd.DatetimeIndex(df['Date']).dayofyear\n    #df['Hour']= pd.DatetimeIndex(df['datetime']).hour\n    return df\n\n#Dropping Outliers beyond 99 Percentile\ndef dropOutliersBeyond99(df):\n    cnt=df['y'].values\n    q99=np.percentile(cnt,[99])\n    df=df[df['y']<q99[0]]\n    return df\n\n#evaluation matrix\ndef rmsle(y_pred,y_true):\n    y_pred = np.expm1(y_pred)\n    y_true = np.expm1(y_true)\n    log1=np.log(y_pred + 1)\n    log2=np.log(y_true + 1)\n    se = (log1 - log2) ** 2 \n    mse=np.mean(se)\n    return np.sqrt(mse)\n\nfrom sklearn.metrics import make_scorer\nmyScorer = make_scorer(rmsle, greater_is_better=False)\n\n# PCA\ndef transform_with_pca(df, columns):\n    pca = PCA(n_components=0.9)\n    # Find new components\n    pca.fit(train_df[colums_for_pca])\n    # No. of PCA Components\n    # print ('Variance: ', pca.n_components)\n    # print ('No. of components to keep: ', pca.n_components_)\n    transformed_data = pca.transform(df[columns])\n    tcols = []\n    for i in range(pca.n_components_):       \n        tcols.append('component_' + str(i))\n    # print ('components:',tcols)\n    df_transformed = pd.DataFrame(transformed_data, columns=tcols)\n    for col in df_transformed.columns:\n        df[col] = df_transformed[col]\n    df=df.drop(columns, axis=1)\n    return df\n","0f8daaf3":"meanEncodersMaps = {}\ndef PrepareData(rawData,isTest=False):\n    test_=rawData.copy()\n    \n    test_['Day'] = pd.DatetimeIndex(test_['Date']).day\n    test_['Month'] = pd.DatetimeIndex(test_['Date']).month\n    test_['Year'] = pd.DatetimeIndex(test_['Date']).year\n    test_['Date']= pd.to_datetime(test_['Date'],format=\"%d\/%m\/%Y\")\n    \n    test_['WeekDay'] = test_[\"Date\"].dt.day_name()\n\n    test_['label_day_night'] = test_['Hour'].apply(lambda x : 'Night' if (x >20 or x<5) else( 'Day'))\n    test_['RushHours'] = test_['Hour'].apply(lambda x : 1 if ( (x >=7 and x<=9) or (x >=17 and x<=22)) else( 0))\n    test_['Temp_levels'] = test_['Temperature(\ufffdC)'].apply(lambda x : '01_Low_Temp' if (x<10) \\\n                                                        else('02_Medium_Temp' if (x >=10 and x<=35) else( '03_High_Temp')))\n    test_['DewLevels']=test_['Dew point temperature(\ufffdC)'].apply(lambda x : 4 if (x>=10 and x<=20) else (2 if(x>=-10 and x<0) else (1 if (x<-10) else (3))))\n    test_['WindSpeed_levels'] = test_['Wind speed (m\/s)'].apply(lambda x : '01_Low_Speed' if (x<4)\\\n                                                              else('02_Medium_Speed' if (x >=4 and x<=6) else( '03_High_Speed')))\n    test_['Rainfall_Levels'] = test_['Rainfall(mm)'].apply(lambda x : 0 if (x>=4) else (0.5 if (x>0 and x<4) else (1)))\n    test_['Snowfall_Levels'] = test_['Snowfall (cm)'].apply(lambda x : 0 if (x>0) else (1))\n    test_['SnowOrRain'] = test_['Rainfall(mm)']+test_['Snowfall (cm)']\n    test_['Is_Winter'] = test_['Seasons'].apply(lambda x : 1 if (x=='Winter') else (0))\n    test_['Holiday'] = test_['Holiday'].replace({'Holiday': 1, 'No Holiday': 0})\n    test_['RadiationLevels']=test_['Solar Radiation (MJ\/m2)'].apply(lambda x : 4 if (x>=0 and x<1.25) else (3 if(x>=1.25 and x<=2) else (1 if (x>2 and x<=2.5) else (2))))\n    test_['DaysOFF'] = test_['WeekDay'].apply(lambda x : 1 if ((x == 'Saturday') or (x == 'Sunday')) else(0))\n    holiay_features = [\"Holiday\", \"DaysOFF\"]\n    test_[\"AllHolidays\"] = test_[holiay_features].sum(axis=1)\n    test_['AllHolidays'] = test_['AllHolidays'].replace({2: 1})\n    test_['VisibilityLevels']=test_['Visibility (10m)'].apply(lambda x : 4 if (x>=1000 and x<1500) else (3 if(x>=1500) else (2 if (x>=500 and x<1000) else (1))))\n    test_['HumidityLevels']=test_['Humidity(%)'].apply(lambda x : 3 if (x>=40 and x<=60) else (2 if(x>=20 and x<40) else (2 if (x>60 and x<=80) else (1))))\n    test_['InverseVisibility'] = test_['Visibility (10m)'].apply(lambda x : 1\/x if(x!=0) else(1))\n    test_['BinarySolar'] = test_['Solar Radiation (MJ\/m2)'].apply(lambda x : 0 if(x==0) else(1))\n    test_['Rainfall(mm)_'] = test_['Rainfall(mm)'].apply(lambda x : math.ceil(x) if (x<35) else (35))\n    test_['Snowfall (cm)_'] = test_['Snowfall (cm)'].apply(lambda x : math.ceil(x) if (x<6) else (6))\n    \n    test_ = test_.drop(columns = ['Date','DaysOFF',])\n\n    test_ = encodingFunctioningDay(test_, ['Functioning Day'])\n    test_ = encodingWeekDay(test_, ['WeekDay'])\n#     test_ = encodingHoliday(test_, ['Holiday'])\n    test_ = encodingTempLevels(test_, ['Temp_levels']) \n    test_ = encodingWindSpeedLevels(test_, ['WindSpeed_levels'])\n    test_ = encodingSeasons(test_, ['Seasons']) \n    test_ = encoding_label_day_night(test_, ['label_day_night']) \n        \n    global meanEncodersMaps\n    MeanEncodedColomns = [ 'Day','Wind speed (m\/s)','RushHours',]\n    outliersColomns = ['y','Solar Radiation (MJ\/m2)',]\n    if isTest == False:\n        test_ = removeOutliers(test_,outliersColomns)\n        test_ = test_[test_['Functioning Day']>0]\n        test_['log_y'] = test_['y'].apply(lambda x : np.log(x+1))\n        test_['10x_y'] = test_['y'].apply(lambda x : int(x\/10))\n        for col_name in test_.columns:\n            if col_name in MeanEncodedColomns:\n                meanEncodersMaps[col_name]= getMeanEncodingDict(test_ , col_name, 'y')\n\n    for col_name in test_.columns:\n        if col_name in MeanEncodedColomns:\n            test_ = encodingFeatureWithDict(test_ , col_name, meanEncodersMaps[col_name])\n            \n    ScaledFeatures=[]\n    for col_name in test_.columns:\n        if col_name in ScaledFeatures:\n            test_[col_name]=scale_column(test_,col_name)\n    categorical_colomns= ['Hour', 'Seasons','Day', 'Month', 'label_day_night',\n       'RushHours', 'Temp_levels', 'DewLevels', 'WindSpeed_levels',\n       'Rainfall_Levels', 'Snowfall_Levels', 'Is_Winter',\n       'RadiationLevels', 'AllHolidays', 'VisibilityLevels']\n#Can't be used with XGBoost       \n    # for col_ in categorical_colomns:\n    #   test_[col_] = test_[col_].astype('category')\n    return test_\n  \nfile = PrepareData(raw_file)\n\nfile.sample(1)","7db57d5e":"print(file.shape)\nprint(file.columns)","f04b4d61":"# Selecting specific columns for model:\n\n# Uninformative colomns but have reduced the error : Visibility (10m)', 'VisibilityLevels', 'Wind speed (m\/s)','Solar Radiation (MJ\/m2)'\n\nused_colomns = ['AllHolidays', 'Dew point temperature(\ufffdC)', 'Hour', 'Humidity(%)', 'HumidityLevels', 'Month'\n                , 'Rainfall(mm)', 'Rainfall_Levels', 'RushHours', 'Seasons', 'Snowfall (cm)_', 'Solar Radiation (MJ\/m2)'\n                , 'Temperature(\ufffdC)', 'Visibility (10m)', 'VisibilityLevels', 'Wind speed (m\/s)', 'Year', 'label_day_night']","758c790f":"def ModelPredictionsPostProcessing(predictions_t,train_x,Param = [890, 1000, 200, 1000]):\n  #This function do preprossing on the model predictions to decrease the errors of the model\n    predictions_ = predictions_t.copy()\n    for i in range(len(predictions_)):\n      # Remove Negative Values and Set prediction to 0 if not a Functioning Day\n        predictions_[i] = max(0,predictions_[i]) * train_x['Functioning Day'].array[i]\n        if (train_x['Snowfall_Levels'].array[i] == 0) :\n          #if there is Snowfall limit predictions to max expected according to data analysis\n          predictions_[i] = min(Param[0],predictions_[i])\n        if  (train_x['Rainfall_Levels'].array[i] == 0):\n          #if there is High Rainfall limit predictions to max expected according to data analysis\n          predictions_[i] = min(Param[1],predictions_[i])\n        if  (train_x['Rainfall_Levels'].array[i] == 0.5):\n          #if there is Medium Rainfall limit predictions to max expected according to data analysis\n          predictions_[i] = min(Param[2],predictions_[i])\n        if  (train_x['Is_Winter'].array[i] == 1):\n          #In Winter Season limit predictions to max expected according to data analysis\n          predictions_[i] = min(Param[3],predictions_[i])\n    return predictions_","05240cf2":"# PCA\n\n# file_pca = file.copy()\n# features = file_pca.columns[2:96] #grab all numeric columns of interest\n# z = StandardScaler()\n# file_pca[features] = z.fit_transform(file_pca[features])\n\n# pca = PCA(random_state=100)\n# pca.fit(file_pca[features])\n# pca.explained_variance_ratio_ #tune pca\n\n# data = pca.transform(file_pca[features])\n# df1 = pd.DataFrame(data[:,0:50])\n# df2 = pd.DataFrame(data[:, 0:30])\n\n# #modeMod1\n# lr1 = CatBoostRegressor(random_seed=100,  verbose=False)\n# lr1.fit(file_pca[features], file_pca[\"y\"])\n# print(\"all data: \", lr1.score(file_pca[features], file_pca[\"y\"]))\n\n# #modeMod1\n# lr2 = CatBoostRegressor(random_seed=100,  verbose=False)\n# lr2.fit(df1, file_pca[\"y\"])\n# print(\"50 PCs:   \", lr2.score(df1, file_pca[\"y\"]))\n\n# #modeMod1\n# lr3 = CatBoostRegressor(random_seed=100,   verbose=False)\n# lr3.fit(df2, file_pca[\"y\"])\n# print(\"30 PCs:    \", lr3.score(df2, file_pca[\"y\"]))","4fb9b33d":"#  Lazy Predicror for best model:\n\n# from lazypredict.Supervised import LazyRegressor\n# from sklearn.utils import shuffle\n# # from sklearn import datasets\n\n# data = file.drop(columns=['ID','y'])\n# target = file['y']\n\n# # boston = datasets.load_boston() #boston.data, boston.target\n# X, y = shuffle(data, target, random_state=13)\n# X = X.astype(np.float32)\n\n# offset = int(X.shape[0] * 0.9)\n\n# X_train, y_train = X[:offset], y[:offset]\n# X_test, y_test = X[offset:], y[offset:]\n\n# reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n# models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n\n# print(models)","b4e75d2a":"train_df, val_df = train_test_split(file, test_size=0.20, random_state=3) \n\nx_train = train_df.drop(columns=['ID','y'])\ny_train = train_df['log_y']\n\nx_val = val_df.drop(columns=['ID','y'])\ny_val = val_df['log_y']\n\nx_train_ = x_train.drop(columns=['Functioning Day'])\nx_val_ = x_val.drop(columns=['Functioning Day'])","fc406306":"# randomForestAlgo = RandomForestRegressor(n_estimators = 100, \n#          max_depth = 12,\n#          min_samples_split=2,\n#          min_samples_leaf=2,\n#          bootstrap = True,\n#          random_state=5,\n#          verbose=0,)\n# randomForestAlgo.fit(x_train,y_train)\n# predictionsRFR = randomForestAlgo.predict(x_val)\n# predictionsRFR_T = randomForestAlgo.predict(x_train)\n\n# #remove negative values from target\n# Limit_Predictions(predictionsRFR,x_val)\n# Limit_Predictions(predictionsRFR_T,x_train)\n   \n# print(\"RMSLE for Validate= \", np.sqrt(mean_squared_log_error( y_val, predictionsRFR )))\n# print(\"RMSLE for train= \", np.sqrt(mean_squared_log_error( y_train, predictionsRFR_T )))\n# # get importance\n# importance = randomForestAlgo.feature_importances_\n# # summarize feature importance\n# for i,v in enumerate(importance):\n#     print('Feature ',i,': %s, Score: %.5f' % (x_train.columns[i],v))\n# # plot feature importance\n# #plt.bar(x_train_.columns.values, importance,width=0.8,)\n# plt.bar([x for x in range(len(importance))], importance,width=0.8)\n# plt.show()","71587a5a":"# CatBoost Regressor:\n\n#  Identify columns with categorical features\n# categorical_features_indices = np.where(x_train_.dtypes != np.float)[0]  \n\n# #  Pooling data\n# #train_data = Pool(x_train, y_train, categorical_features_indices)\n\n# # specify the training parameters ,depth=15,n_estimators=80,l2_leaf_reg=0.01 ,eval_metric=LoglossMetric(), loss_function='Logloss',reg_lambda=6\n# #loss_function : string, [default='RMSE']'RMSE','MAE','Quantile:alpha=value','LogLinQuantile:alpha=value','Poisson','MAPE','Lq:q=value','SurvivalAft:dist=value;scale=value\n# #(random_seed = i+200, gradient_iterations = i+1 ,leaf_estimation_method ='Newton', learning_rate=0.057, l2_leaf_reg = 23, depth=6, od_pval=0.0000001, iterations = 877, loss_function='Logloss')\n# CBR_Model = CatBoostRegressor(learning_rate=0.001,depth=15,l2_leaf_reg=0.001,random_seed=100, iterations=6000, loss_function='Lq:q=2')#, eval_metric = 'RMSE'\n\n# # train the model\n# CBR_Model.fit(    \n# #    train_data,  #   Tried this with pooled data defined above\n#      x_train_,\n#      y_train,\n#      cat_features=categorical_features_indices,\n#      eval_set=[(x_train_, y_train), (x_val_, y_val)],\n#      early_stopping_rounds=100,\n#      verbose=False)\n\n# predictionsCB = CBR_Model.predict(x_val_)\n# predictionsCB_T = CBR_Model.predict(x_train_)\n\n# # remove negative values from target\n# predictionsCB=Limit_Predictions(predictionsCB,x_val)\n# predictionsCB_T=Limit_Predictions(predictionsCB_T,x_train)\n# print(\"RMSLE for Validate= \", np.sqrt(mean_squared_log_error( y_val, predictionsCB )))\n# print(\"RMSLE for train= \", np.sqrt(mean_squared_log_error( y_train, predictionsCB_T )))\n\n#plot train and validation losses\n# results = CBR_Model.evals_result_\n# plt.plot(range(len(results['validation_0']['RMSE'])), results['validation_0']['RMSE'])\n# plt.plot(range(len(results['validation_1']['RMSE'])), results['validation_1']['RMSE'])\n# plt.legend(['Train Error', 'Val Error'])\n# plt.show()\n","9436f3b9":"# k-fold and XGBoost Regressor: \"Should use GPU Accelerator\" with 'tree_method': \"gpu_hist\" option\n\ndef kfold_xgb(train, x_test, target, seed=2020):\n    kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n    paras = {\n        'objective': 'count:poisson',\n        'learning_rate': 0.008,\n        'lambda': 0.003261847937677995,\n        'alpha': 0.041719982269195065, \n        'colsample_bytree': 0.5, \n        'subsample': 1.0,\n        'max_depth': 6,\n        'min_child_weight': 97,\n        'random_state': 24,\n        'eval_metric': 'rmse',\n        # 'tree_method': \"gpu_hist\",\n    }\n    \n    y_sub = 0  \n    feature_importance_df = pd.DataFrame()  \n    \n    for fold, (train_index, val_index) in tqdm(enumerate(kf.split(train, target))):\n        X_train, X_val = train.iloc[train_index], train.iloc[val_index]\n        y_train, y_val = target.iloc[train_index], target.iloc[val_index]\n        data_train = xgb.DMatrix(X_train, y_train)\n        data_val = xgb.DMatrix(X_val, y_val)\n        watchlist = [(data_train,'train'),(data_val,'val')]\n        evals_result={}\n        model = xgb.train(dict(paras),data_train, evals = watchlist, num_boost_round=60000, evals_result=evals_result , early_stopping_rounds=100, verbose_eval=False)\n        \n        \n       # test---output\n        data_test = xgb.DMatrix(x_test)  \n        y_sub = model.predict(data_test)\n        \n\n        # features importance\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = model.get_score().keys()\n        fold_importance_df[\"importance\"] = model.get_score().values()  # weight\n        # fold_importance_df[\"importance\"] = model.feature_importance(importance_type = 'gain')  # gain\n\n        fold_importance_df[\"fold\"] = fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n                            \n    return y_sub, feature_importance_df, model, evals_result","51c4a329":"#XGBoost implementation\npredictionsXGB, feature_importance, XGB_Model ,evals_result = kfold_xgb(x_train[used_colomns], x_val[used_colomns], y_train)\npredictionsXGB = ModelPredictionsPostProcessing(predictionsXGB,x_val)\n\nprint(\"RMSLE for Validate = \", np.sqrt(mean_squared_log_error( y_val, predictionsXGB )))\n\nresults = evals_result\nplt.plot(range(len(results['train']['rmse'])), results['train']['rmse'])\nplt.plot(range(len(results['val']['rmse'])), results['val']['rmse'])\nplt.legend(['Train Error', 'Val Error'])\nplt.show()","adc8488d":"feature_importance=feature_importance.sort_values(by='importance',ascending=False)\nsns.catplot('feature', 'importance', data=feature_importance, height=5,aspect=5) ","b4daa9b0":"# Extra Trees Regressor\n\n# extraTreesAlgo = ExtraTreesRegressor(n_estimators = 80, \n#          max_depth = 15,\n#          random_state=3,\n#          min_samples_split=2,\n#          min_samples_leaf=2,\n#          bootstrap = True)\n\n# extraTreesAlgo.fit(x_train_,y_train)\n\n# predictionsET = extraTreesAlgo.predict(x_val_)\n# predictionsET_T = extraTreesAlgo.predict(x_train_)\n\n# #remove negative values from target\n# Limit_Predictions(predictionsET,x_val)\n# Limit_Predictions(predictionsET_T,x_train)  \n    \n# print(\"RMSLE for Validate= \", np.sqrt(mean_squared_log_error( y_val, predictionsET )))\n# print(\"RMSLE for train= \", np.sqrt(mean_squared_log_error( y_train, predictionsET_T )))\n\n# # summarize feature importance\n# importance = extraTreesAlgo.feature_importances_\n# for i,v in enumerate(importance):\n#     print('Feature: %s, Score: %.5f' % (x_train_.columns[i],v))\n# # plot feature importance\n# plt.bar([x for x in range(len(importance))], importance)\n# plt.show()","e415db8f":"# Hist Grad Boost Regressor:\n# from sklearn.experimental    import enable_hist_gradient_boosting\n# from sklearn.ensemble        import HistGradientBoostingRegressor\n# %%time\n\n# kf = KFold(n_splits=10, random_state=42, shuffle=True)\n\n# predictions_array = []\n# CV_score_array    = []\n\n# for train_index, test_index in kf.split(X):\n    \n#     X_train, X_valid = X[train_index], X[test_index]\n#     y_train, y_valid = y[train_index], y[test_index]\n    \n#     regressor =  HistGradientBoostingRegressor()\n#     regressor.fit(X_train, y_train)\n    \n#     predictions_array.append(regressor.predict(X_test))\n#     CV_score_array.append(mean_absolute_error(y_valid,regressor.predict(X_valid)))    \n\n# predictions = np.mean(predictions_array,axis=0)","b22b7dca":"# histogram-based gradient boosting for regression in scikit-learn\n# from numpy import mean\n# from numpy import std\n# from sklearn.datasets import make_regression\n# from sklearn.experimental import enable_hist_gradient_boosting\n# from sklearn.ensemble import HistGradientBoostingRegressor\n# from sklearn.model_selection import cross_val_score\n# from sklearn.model_selection import RepeatedKFold\n# from matplotlib import pyplot\n# # define dataset\n# X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n# # evaluate the model\n# model = HistGradientBoostingRegressor()\n# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n# print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n# # fit the model on the whole dataset\n# model = HistGradientBoostingRegressor()\n# model.fit(X, y)\n# # make a single prediction\n# row = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\n# yhat = model.predict(row)\n# print('Prediction: %.3f' % yhat[0])","85e5f594":"\n# RMSE_RF =[]\n# for randomx in range(2,20):\n#     train_df, val_df = train_test_split(file, test_size=0.20, random_state=randomx) \n#     x_train = train_df.drop(columns=['ID','y'])\n#     y_train = train_df['y']\n\n#     x_val = val_df.drop(columns=['ID','y'])\n#     y_val = val_df['y']\n#     randomForestAlgo = RandomForestRegressor()\n#     param = {'n_estimators' : [80], \n#              'max_depth' : [12],\n#              'min_samples_split':[2],\n#              'min_samples_leaf':[2],\n#              'bootstrap' : [True]\n#             }\n#     gridSearch_RandomForest=GridSearchCV(randomForestAlgo,param,scoring='r2',cv=5)\n#     gridSearch_RandomForest.fit(x_train,y_train)\n#     predictionsRFR = gridSearch_RandomForest.predict(x_val)\n\n#     #remove negative values from target\n#     for i in range(len(predictionsRFR)):\n#         predictionsRFR[i] = max(0,predictionsRFR[i]) * x_val['Functioning Day'].array[i]\n#     RMSE_RF.append(np.sqrt(mean_squared_log_error( y_val, predictionsRFR )))\n\n# df = pd.DataFrame()\n# df['RMSE_RF_df'] = RMSE_RF\n# print(\"RMSLE max = \",df['RMSE_RF_df'].max()*100,'%'  )\n# print(\"RMSLE min = \",df['RMSE_RF_df'].min()*100,'%'  )\n# print(\"RMSLE diff= \",(df['RMSE_RF_df'].max()-df['RMSE_RF_df'].min())*100,'%' )\n# print(\"RMSLE mean = \",df['RMSE_RF_df'].mean()*100,'%'  )\n# df","cc246e12":"# gridSearch_RandomForest.get_params(all)","a371482f":"# test for RandomForestRegressor best parametes\n\n# for depth in range(2,100):\n#         param = {'n_estimators' : [depth], \n#          'max_depth' : [12],\n#          'min_samples_split':[5],\n#          'min_samples_leaf':[2],\n#          'bootstrap' : [True]\n#         }\n#         gridSearch_RandomForest=GridSearchCV(randomForestAlgo,param,scoring='r2',cv=5)\n#         gridSearch_RandomForest.fit(x_train,y_train)\n#         predictionsRFR = gridSearch_RandomForest.predict(x_val)\n#         for i in range(len(predictionsRFR)):\n#             predictionsRFR[i] = max(0,predictionsRFR[i]) * x_val['Functioning Day'].array[i]\n#         print(\"max_depth= \",depth,\" RMSLE= \", np.sqrt(mean_squared_log_error( y_val, predictionsRFR )))\n        ","6f2654ba":"# remove test file from working directory\n#os.remove('.\/test.csv')","04b7a146":"# #LinearReggression Model:\n\n# # Using polynomial LinearRegression on the dataset\n# reg = LinearRegression().fit(x_train, y_train)\n# predictionsLiR = reg.predict(x_val)\n\n# #remove negative values from target\n# for i in range(len(predictionsLiR)):\n#     predictionsLiR[i] = max(0,predictionsLiR[i])\n    \n# #RMSLE\n# print(\"RMSLE= \", np.sqrt(mean_squared_log_error( y_val, predictionsLiR )))\n# # get importance\n# importance = reg.coef_\n# # summarize feature importance\n# for i,v in enumerate(importance):\n#     print('Feature: %s, Score: %.5f' % (x_train.columns[i],v))\n# # plot feature importance\n# plt.bar([x for x in range(len(importance))], importance)\n# plt.show()","8c246bd9":"# test for SVR best parametes\n\n# for depth in [7000]:\n#         param = {'C' : [depth]}\n#         gridSearchSVR=GridSearchCV(svr_Model, param, scoring='r2', cv=5)\n#         gridSearchSVR.fit(x_train, y_train)\n#         predictionsSVR = gridSearchSVR.predict(x_val)\n#         for i in range(len(predictionsSVR)):\n#             predictionsSVR[i] = max(0,predictionsSVR[i])\n#         print(\"max_depth= \",depth,\"RMSLE= \", np.sqrt(mean_squared_log_error( y_val, predictionsSVR )))\n","ab797873":"# # SVR regression model:\n\n# svr_Model=SVR(kernel='rbf',C= 7000)\n# #'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'\n# svr_Model.fit(x_train, y_train)\n\n# predictionsSVR = svr_Model.predict(x_val)\n# predictionsSVR_T = svr_Model.predict(x_train)\n\n# #remove negative values from target\n# for i in range(len(predictionsSVR)):\n#     predictionsSVR[i] = max(0,predictionsSVR[i]) * x_val['Functioning Day'].array[i]\n# for i in range(len(predictionsSVR_T)):\n#     predictionsSVR_T[i] = max(0,predictionsSVR_T[i]) * x_train['Functioning Day'].array[i]    \n    \n# print(\"RMSLE for Validate= \", np.sqrt(mean_squared_log_error( y_val, predictionsSVR )))\n# print(\"RMSLE for train= \", np.sqrt(mean_squared_log_error( y_train, predictionsSVR_T )))\n","722200d8":"# # xgboost for feature importance on a regression problem\n# from xgboost import XGBRegressor\n# # define the model\n# XGB_model = XGBRegressor()\n# # fit the model\n# XGB_model.fit(x_train, y_train)\n\n# predictionsXGB = XGB_model.predict(x_val)\n# predictionsXGB_T = XGB_model.predict(x_train)\n\n# #remove negative values from target\n# for i in range(len(predictionsXGB)):\n#     predictionsXGB[i] = max(0,predictionsXGB[i]) * x_val['Functioning Day'].array[i]\n# for i in range(len(predictionsXGB_T)):\n#     predictionsXGB_T[i] = max(0,predictionsXGB_T[i]) * x_train['Functioning Day'].array[i]    \n    \n# print(\"RMSLE for Validate= \", np.sqrt(mean_squared_log_error( y_val, predictionsXGB )))\n# print(\"RMSLE for train= \", np.sqrt(mean_squared_log_error( y_train, predictionsXGB_T )))\n\n# # get importance\n# importance = XGB_model.feature_importances_\n# # summarize feature importance\n# for i,v in enumerate(importance):\n#     print('Feature: %s, Score: %.5f' % (x_train.columns[i],v))\n# # plot feature importance\n# plt.bar([x for x in range(len(importance))], importance)\n# plt.show()","5a14bf5d":"# test for Ridge best parametes\n\n# for depth in range(50,150,1):\n#         ridge=Ridge()\n#         parameters={'alpha':[depth]}\n#         gridSearchRidge=GridSearchCV(ridge, parameters, scoring='r2', cv=3)\n#         gridSearchRidge.fit(x_train,y_train)\n#         predictionsRg = gridSearchRidge.predict(x_val)\n#         #remove negative values from target\n#         for i in range(len(predictionsRg)):\n#             predictionsRg[i] = max(0,predictionsRg[i])\n#         print(\"max_depth= \",depth,\"RMSLE= \", np.sqrt(mean_squared_log_error( y_val, predictionsRg )))\n        ","f03fcf71":"# Ridge regression model:\n\n# ridge=Ridge()\n# parameters={'alpha':[100]}\n# gridSearchRidge=GridSearchCV(ridge, parameters, scoring='r2', cv=3)\n# gridSearchRidge.fit(x_train,y_train)\n\n# predictionsRg = gridSearchRidge.predict(x_val)\n\n# #remove negative values from target\n# for i in range(len(predictionsRg)):\n#     predictionsRg[i] = max(0,predictionsRg[i])\n# print(\"RMSLE= \", np.sqrt(mean_squared_log_error( y_val, predictionsRg )))\n","d1749cfa":"# Decision Tree Regressor\n\n# decisionTree = DecisionTreeRegressor()\n\n# param = {'max_depth' : [10]}\n\n# gridSearch_decisionTree=GridSearchCV(decisionTree,param,scoring='r2',cv=6)\n# gridSearch_decisionTree.fit(x_train,y_train)\n\n# predictionsDT = gridSearch_decisionTree.predict(x_val)\n# predictionsDT_T = gridSearch_decisionTree.predict(x_train)\n\n# #remove negative values from target\n# for i in range(len(predictionsDT)):\n#     predictionsDT[i] = max(0,predictionsDT[i]) * x_val['Functioning Day'].array[i]\n# for i in range(len(predictionsDT_T)):\n#     predictionsDT_T[i] = max(0,predictionsDT_T[i]) * x_train['Functioning Day'].array[i]    \n    \n# print(\"RMSLE for Validate= \", np.sqrt(mean_squared_log_error( y_val, predictionsDT )))\n# print(\"RMSLE for train= \", np.sqrt(mean_squared_log_error( y_train, predictionsDT_T )))\n","d299d4d2":"# # decision tree for feature importance on a regression problem\n# from sklearn.tree import DecisionTreeRegressor\n\n# # define the model\n# model = DecisionTreeRegressor()\n# # fit the model\n# model.fit(x_train,y_train)\n# # get importance\n# importance = model.feature_importances_\n# # summarize feature importance\n# for i,v in enumerate(importance):\n#     print('Feature: %s, Score: %.5f' % (x_train.columns[i],v))\n# # plot feature importance\n# plt.bar([x for x in range(len(importance))], importance)\n# plt.show()","08fdbc12":"# Comparing Model's RMSLE Scores:\n\n# df = pd.DataFrame()\n# df['Y'] = y_val\n# df['RFC'] = predictionsRFC\n# df['LiR'] = predictionsLiR\n# #df['LoR'] = predictionsLoR\n# df['RFR'] = predictionsRFR\n# #df['SVR'] = predictionsSVR\n# # Ensemble methods: regression\n# df['Merged'] = (predictionsRFR + predictionsSVR)\/2\n# df\n#print(\"RMSLE= \", np.sqrt(mean_squared_log_error( y_val, ((predictionsRFR + predictionsET )\/2) )))\n\n#Since we know that the output is never less than 1 we replace all negative values with 1 before appending in pre to calculate error.\n#plt = sns.residplot(x = predictionsRFR, y = y_val, lowess = True,color = 'r')","3493b9af":"# Train with ALL DATA to increase accuracy after it has been validated.\n\n# file = sortPredictionsAsNormalDistribution(file,\"y\")\nx_train = file.drop(columns=['ID','y'])\ny_train = file['log_y']\n\nx_train_ = x_train.drop(columns=['Functioning Day'])\nx_val_ = x_val.drop(columns=['Functioning Day'])","2e21ac88":"raw_test = pd.read_csv('\/kaggle\/input\/seoul-bike-rental-ai-pro-iti\/test.csv')","99e02161":"test=raw_test\ntest","218b89dd":"test.describe()","76b88184":"test = PrepareData(raw_test,True)\ntest.sample(10)","0af0f160":"\nX_test = test\nX_test = X_test.drop(columns=['ID'])\n# Train the model with all the data and predict the test file.\ny_test_predicted_XGB,feature_importance,XGB_Model ,evals_result = kfold_xgb(x_train[used_colomns], X_test[used_colomns], y_train)\n\ny_test_predicted_XGB = ModelPredictionsPostProcessing(y_test_predicted_XGB, X_test)\n\ntest['y'] =  np.round(np.exp(y_test_predicted_XGB) - 1)\ntest","a75697a6":"test[['ID', 'y']].to_csv('\/kaggle\/working\/submission.csv', index=False)","9181fde5":"## \u26a1  Model Training","ab0e881f":"## \u26a1 Data visualization:","672940e5":"# \u26a1 Importing libraries\n","4cfe93e2":"## \u26a1 Data Preparation","d4d2f852":"## 9- Decission Tree Regressor:","361b9815":"## 8- XGboost without kfold:","19c4e1ba":"# \u26a1 Reading the test file","8e3847ac":"## 2- CatBoost Regressor","441fa737":"### Comparing models predictions:","3feed237":"## 5- Hist Grad Boost Regressor:","64fdc97c":"## \u26a1 EDA","b77a7ba8":"# \u26a1 ML Regressor Models:","2c0d6b9a":"## \u26a1 Splitting data into training and validation sets","f684d1db":"## 9- Ridge Regressor:","64aba795":"## 3- k-fold and XGBoost Regressor","69cb8484":"## 1- Random Forest Regressor","863ad248":"# \u26a1 Generating Submission CSV output file:","29f6729a":"# \u26a1 Read The Train Data ","9e1c1ee9":"## \u26a1 Mutual Information for Feature Selection","4432d96b":"### Grid Search:","34144fcb":"## \u26a1 Function of all preparing data","0a53570b":"## 7- SVR reggressor:","fa22a2af":"## 4- Extra Trees Regressor","3d53e4e3":"## 6- Linear Regressor:","2e6fe9d6":"### \u26a1 All functions ","46f902c5":"## \u26a1 Test set preprocessing:"}}