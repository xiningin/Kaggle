{"cell_type":{"3777ab9b":"code","0c5be4c2":"code","e0032e1a":"code","03672d0f":"code","16c18bf1":"code","333db24d":"code","42a2def3":"code","8dcba730":"code","b70ab72e":"code","f3871ac1":"code","d517baab":"code","a226f15a":"code","95585c94":"code","2c591590":"code","d854daf6":"code","9b443d53":"code","3c1f98b8":"code","ccb1a842":"code","fb2559f4":"code","7680ab20":"code","aa5f0698":"code","721c403a":"code","53e29835":"code","569451ff":"code","a9332da4":"code","d204c1ae":"code","90f07e7e":"code","1891bac9":"code","f99de15e":"code","473f5a2b":"code","89e33ae4":"code","70b67bc9":"code","c9ca9649":"code","45425ee0":"code","dbc0756a":"code","cfb1b1ed":"code","0236998e":"code","6509330c":"code","5f1acb77":"code","67de8c96":"code","eb5cff76":"code","3e58b7ad":"code","a7f8aa2a":"code","7770ec57":"code","1b8934d9":"code","c33fda3f":"code","65ad4eac":"code","4779ca49":"code","aa528f58":"code","51d4fd77":"code","486cd27b":"markdown","51f91194":"markdown","5edd8b0a":"markdown","6b3101ae":"markdown","24eef2c0":"markdown","a4317036":"markdown","8c0e5ea3":"markdown","31f2df1a":"markdown","c62d2a8d":"markdown","604aeec8":"markdown","7904e526":"markdown"},"source":{"3777ab9b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0c5be4c2":"import pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# I'm usng four base model and one meta model\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn import model_selection\n","e0032e1a":"train = pd.read_csv(r'..\/input\/titanic\/train.csv')\ntest = pd.read_csv(r'..\/input\/titanic\/test.csv')\n","03672d0f":"\"\"\"lets store our Passenger Id for easy access and future use\"\"\"\nPassengerId = test['PassengerId']","16c18bf1":"train.head(4)","333db24d":"test.head()","42a2def3":"train.info()","8dcba730":"test.info()","b70ab72e":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","f3871ac1":"sns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='viridis')","d517baab":"full_data = [train, test]","a226f15a":"# Feature engineering steps taken from Sina\n#1. Pclass\nprint (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())","95585c94":"#2. Sex\nprint (train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean())","2c591590":"#With the number of siblings\/spouse and the number of children\/parents we can create new feature called Family Size.\n#3. SibSp and ParCh\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\nprint (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())","d854daf6":"#it seems has a good effect on our prediction but let's go further and categorize people to check whether they are alone in this ship or not.\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\nprint (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())","9b443d53":"#the embarked feature has some missing value. and we try to fill those with the most occurred value ( 'S' ).\n#4. Embarked\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\nprint (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())","3c1f98b8":"#Fare also has some missing value and we will replace it with the median. then we categorize it into 4 ranges.\n#5. Fare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\nprint (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())","ccb1a842":"#we have plenty of missing values in this feature. # generate random numbers between (mean - std) and (mean + std). then we categorize age into 5 range.\n#6. Age\nfor dataset in full_data:\n    age_avg \t   = dataset['Age'].mean()\n    age_std \t   = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    \n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n    \ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n\nprint (train[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())","fb2559f4":"#inside this feature we can find the title of people\n#7. Title\ndef get_title(name):\n\ttitle_search = re.search(' ([A-Za-z]+)\\.', name)\n\t# If the title exists, extract and return it.\n\tif title_search:\n\t\treturn title_search.group(1)\n\treturn \"\"\n\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n\nprint(pd.crosstab(train['Title'], train['Sex']))","7680ab20":"for dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nprint (train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())","aa5f0698":"train['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)","721c403a":"for dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t= 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age']  = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4","53e29835":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)","569451ff":"train.head(4)","a9332da4":"test.head()","d204c1ae":"#Pearson Correlation Heatmap\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","90f07e7e":"g = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","1891bac9":"# this variables will help simplify the process and easy usage\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = model_selection.StratifiedKFold(n_splits = NFOLDS, random_state=SEED)\n","f99de15e":"# Class to extend the Sklearn classifier\nclass SklearnClassifierSolver(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n        return self.clf.fit(x,y).feature_importances_\n    \n# Class to extend XGboost classifer","473f5a2b":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","89e33ae4":"# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }","70b67bc9":"# Create 5 objects that represent our 4 models\nrf = SklearnClassifierSolver(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnClassifierSolver(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnClassifierSolver(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnClassifierSolver(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnClassifierSolver(clf=SVC, seed=SEED, params=svc_params)","c9ca9649":"# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data","45425ee0":"# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")","dbc0756a":"rf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)","cfb1b1ed":"rf_features = list(rf_feature)\net_features = list(et_feature)\nada_features = list(ada_feature)\ngb_features = list(rf_feature)","0236998e":"cols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })","6509330c":"# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Extra Trees  feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Extra Trees  feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Extra Trees Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['AdaBoost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['AdaBoost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'AdaBoost Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Gradient Boost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Gradient Boost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Gradient Boosting Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","5f1acb77":"# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(7)","67de8c96":"y = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='bar-direct-labels')","eb5cff76":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()","3e58b7ad":"data = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')","a7f8aa2a":"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)","7770ec57":"gbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)","1b8934d9":"# Generate Submission File \nStackingEnsembleSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingEnsembleSubmission.to_csv(\"StackingEnsembleSubmission.csv\", index=False)","c33fda3f":"StackingEnsembleSubmission.head()","65ad4eac":"gender_submission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\ngender_submission.to_csv(\"gender_submission.csv\", index=False)","4779ca49":"gender_submission.head()","aa528f58":"Ism = pd.read_csv(r'.\/StackingEnsembleSubmission.csv')","51d4fd77":"Ism.head()","486cd27b":"\"\"\"Feature Engineering for both train and test datasets\"\"\"\n\n\"\"\" I also borrowed heavily from Sinu's very comprehensive and well-organized notebook on feature engineering. Kindly check out his work:\n\n'https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier' \"\"\"","51f91194":"\"\"\" Visualizing our important features with plots, to see if there is any need to add or drop a feature\"\"\"","5edd8b0a":"\"\"\" Data Visualization\"\"\"","6b3101ae":"So a looping in a name and cabin variable to drive process improvements and enhance model performance","24eef2c0":"\"\"\"Importing major libraries and modules\"\"\"\n\n\"\"\"I already have SVC as my best model. However, i intend to improve the model by stacking\"\"\"","a4317036":"\"\"\"I will take care of the missing values in 'Age', 'Cabin', 'Fare', and 'Embarked'. \"\"\"\n\n\"\"\" And then see how each feature affects the target variable 'Survived'. \"\"\"","8c0e5ea3":"\"\"\" Features Selection\"\"\"","31f2df1a":"\"\"\"Load our train and test datasets\"\"\"\n\n\"\"\"The materials in this note book borrows a lot from Arthurtok script. However, I had to introduce new concepts to suite the upgraded packages and modules. Please check out his script here:\n\n'https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python' \"\"\"\n\n","c62d2a8d":"\"\"\"Ensembling and Stacking model\"\"\"","604aeec8":"\"\"\" Data Cleaning\"\"\"","7904e526":"\"\"\" Working on the base model\"\"\"\n\n\"\"\"So now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\nRandom Forest classifier, Extra Trees classifier, AdaBoost classifer, Gradient Boosting classifer, Support Vector Machine\"\"\""}}