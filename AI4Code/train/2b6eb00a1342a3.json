{"cell_type":{"e6708fd1":"code","3ce1647e":"code","f7ea20a8":"code","f859a425":"code","d697f131":"code","f1af7664":"code","cad5b3a9":"code","5f95605f":"code","1077ce31":"code","0719833c":"code","513c01a4":"code","4baf6fee":"code","3f14a469":"code","183324d7":"code","835e4a09":"code","6eb521d3":"code","63a66f3a":"code","cef56e46":"code","88631ac4":"code","1b7d3bad":"code","30674789":"code","0ef3801f":"code","81114a8f":"code","50d6c86a":"code","dc49bcd1":"code","a8f8b0b4":"code","397aee61":"code","53d0af1b":"code","1a98e07f":"code","2d667627":"code","00f6bfbf":"code","d7aa8fdd":"code","dc5dc7eb":"code","aec68f0f":"code","2f495530":"code","4d3c9c0d":"code","3f7e5627":"code","2a4d77e7":"code","a82d69de":"code","0e6b4b25":"code","1009531e":"code","78b12e91":"code","33055e08":"code","d678bcfc":"code","8831c850":"code","25113ddd":"code","a363670f":"code","09056854":"code","5ece3a98":"code","9cc74d99":"code","2d4c039e":"code","c9bf9bba":"code","baeac48b":"code","033512f8":"code","6dd0c7fc":"code","eb8cb9c2":"code","f3c663b4":"code","ac35b137":"code","ae2fba9b":"code","fb5d80fe":"code","4c11d006":"code","3022bfe8":"code","fa01a747":"code","64743720":"code","57d0a955":"code","8e74c358":"code","e846220e":"markdown","feeb11af":"markdown","21930871":"markdown","f5cbc57e":"markdown","d8ee65c4":"markdown","f721aa2b":"markdown"},"source":{"e6708fd1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3ce1647e":"%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport datetime as dt\n\nimport nltk\nfrom tqdm import tqdm\nimport lightgbm as lgb\nimport re\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import KFold\n\n# \uc2dc\uac01\ud654 \uc0ac\uc804 \uc900\ube44\nsns.set_context(\"talk\")\nsns.set_style(\"white\")\n\n# \ud55c\uae00 \uc0ac\uc6a9 \uc900\ube44\nplt.rcParams['font.family']='NanumGothic'\nplt.rcParams['axes.unicode_minus'] = False\n# fig, ax = plt.subplots(figsize=(20, 9))","f7ea20a8":"# train \ub370\uc774\ud130\ntrain_err_data = pd.read_csv('\/kaggle\/input\/dacon-lg\/train_err_data.csv')\ntrain_quality_data = pd.read_csv('\/kaggle\/input\/dacon-lg\/train_quality_data.csv')\ntrain_problem_data = pd.read_csv('\/kaggle\/input\/dacon-lg\/\/train_problem_data.csv')\n\ntrain_problem_data.columns = ['user_id', 'pb_time']\n\n# test \ub370\uc774\ud130\ntest_err_data = pd.read_csv('\/kaggle\/input\/dacon-lg\/test_err_data.csv')\ntest_quality_data = pd.read_csv('\/kaggle\/input\/dacon-lg\/test_quality_data.csv')\n\nsample_submssion = pd.read_csv('\/kaggle\/input\/dacon-lg\/sample_submission.csv')","f859a425":"train_problem_data.groupby('user_id')['pb_time'].nunique().reset_index()","d697f131":"# \ud544\uc694\ud55c \ud568\uc218 \uc815\uc758\ndef make_datetime(x):\n    # string \ud0c0\uc785\uc758 Time column\uc744 datetime \ud0c0\uc785\uc73c\ub85c \ubcc0\uacbd\n    x     = str(x)\n    year  = int(x[:4])\n    month = int(x[4:6])\n    day   = int(x[6:8])\n    #hour  = int(x[8:10])\n    #mim  = int(x[10:12])\n    #sec  = int(x[12:])\n    return dt.datetime(year, month, day)#, hour)","f1af7664":"train_problem_data['pb_datetime'] = train_problem_data['pb_time'].apply(make_datetime)\ntrain_err_data['datetime'] = train_err_data['time'].apply(make_datetime)","cad5b3a9":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n","5f95605f":"train_err_data = reduce_mem_usage(train_err_data)","1077ce31":"# train_err_data['year_date'] = train_err_data.loc[:,'time'].str[:8]\n# train_err_data['year_date_hour'] = train_err_data.loc[:,'time'].str[:10]\n# train_err_data['hour'] = train_err_data.loc[:,'time'].str[8:10]\n# train_err_data['minute'] = train_err_data.loc[:,'time'].str[10:12]\n\n# test_err_data['year_date'] = test_err_data.loc[:,'time'].str[:8]\n# test_err_data['hour'] = test_err_data.loc[:,'time'].str[8:10]\n# test_err_data['minute'] = test_err_data.loc[:,'time'].str[10:12]","0719833c":"print('model_nm', train_err_data.model_nm.nunique())\nprint('fwver', train_err_data.fwver.nunique())\nprint('errtype', train_err_data.errtype.nunique())\nprint('errcode', train_err_data.errcode.nunique())","513c01a4":"# \uc720\uc800\ub2f9 \ud3c9\uade0 \uba87\ud68c? \uc5c4\uccad \ub9ce\ub124..?\ntrain_err_data.groupby('user_id')['time'].nunique().plot()","4baf6fee":"# \ubaa8\ub378 \ub118\ubc84\uac00 \uc62c\ub77c\uac00\uba74?\ntrain_err_data.groupby(['model_nm']).agg(['nunique', 'count'])","3f14a469":"# \uc5b4\ub290 \uc2dc\uac04\ub300\uc5d0 \uc5d0\ub7ec\uac00 \ub9ce\uc744\uae4c?\ntrain_err_data.groupby(['datetime'])['user_id'].agg(['count']).plot()\n\ntest_err_data.groupby(['datetime'])['user_id'].agg(['count']).plot()","183324d7":"t = train_err_data.groupby(['datetime', 'errtype'])['user_id'].agg(['count']).reset_index()\nt = t.pivot(index = 'datetime', columns = 'errtype', values = 'count')","835e4a09":"# \uc77c\uc790\ubcc4 \uc5d0\ub7ec\ud0c0\uc785\uc740, \ud56d\uc0c1 \uc77c\uc815\ud55c \ube44\uc728\uc774 \uc788\ub294\uac83 \uac19\ub2e4\nt.plot(figsize=(20,9))","6eb521d3":"# \uc77c\ubcc4 \uc5d0\ub7ec\ud0c0\uc785 \ube44\uc728 \ubcf4\uae30 --> \uc77c\ubcc4 \uc77c\uc815\ud55c \ube44\uc728 \ud655\uc778\ndf_pct = (train_err_data.groupby(['year_date','errtype'])['time'].count()\/train_err_data.groupby(['year_date'])['time'].count())\ndf_pct.unstack().plot.bar(stacked=True, figsize = (20, 9))\ndel df_pct","63a66f3a":"# 1, 6\uc2dc\uc5d0 \uc65c \uac00\ub77c\uc549\uc9c0?\ntrain_err_data.groupby(['hour'])['user_id'].agg(['count']).plot()","cef56e46":"# \uc77c\ubd80 \ud38c\uc6e8\uc5b4\uc5d0\uc11c \uc5d0\ub7ec\uac00 \uc2ec\uac01\ud558\uac8c \ubc1c\uc0dd\ud558\ub294\uac83 \uac19\ub2e4.\n# --> \uc55e \ub450\uc790\ub9ac\ub85c \uad6c\ubd84\ud574 \ucd94\uc774 \ubcf4\uae30\nt = train_err_data.groupby(['datetime', 'fwver'])['time'].agg(['count']).reset_index().pivot('year_date', 'fwver', 'count').fillna(0)\nt.plot()","88631ac4":"# 4\ubc88\uc774 \uac00\uc7a5 \ub9ce\uc740 \uc5d0\ub7ec \ud69f\uc218\ub97c \ubcf4\uc778\ub2e4\ntrain_err_data.loc[train_err_data.fwver=='8.5.3', 'fwver'] = '08.5.3'\ntrain_err_data.loc[:, 'fwver_ver'] = train_err_data.fwver.str[:2]\nt = train_err_data.groupby(['datetime', 'fwver_ver'])['time'].agg(['count']).reset_index().pivot('year_date', 'fwver_ver', 'count').fillna(0)\nt.plot()","1b7d3bad":"t = train_err_data.groupby(['user_id'])['year_date'].agg(['max', 'min'])\nt = (t.loc[:,'max'].astype('int') - t.loc[:,'min'].astype('int'))#.reset_index(name = 'diff')\nt.head()","30674789":"# \uc720\uc800\ubcc4 \uc77c \ud3c9\uade0 \uc5d0\ub7ec \ubc1c\uc0dd\ud69f\uc218\nuser_avg_err = train_err_data.groupby(['user_id'])['time'].agg(['count'])\nuser_avg_err = user_avg_err.groupby(['user_id'])['count'].sum()\/(t+1)\nuser_avg_err = user_avg_err.reset_index()\nuser_avg_err.head()","0ef3801f":"# \uc720\uc800\ubcc4 \uc2e0\uace0 \ud69f\uc218\nuser_problem_cnt = train_problem_data.groupby('user_id').count().reset_index()\nuser_problem_cnt.head()","81114a8f":"t = pd.merge(user_avg_err, user_problem_cnt, how = 'left', on = 'user_id').fillna(0)\nt.head()","50d6c86a":"sns.violinplot(x=\"time\", y=\"avg_cnt\", data=t.loc[t.avg_cnt < 400])","dc49bcd1":"t2 = t.loc[t.avg_cnt < 100]\nairlines = t2['time'].unique().tolist()\nfor i, airline in enumerate(airlines):\n  sns.distplot(t2[t2['time'] == airline]['avg_cnt'], label=airline, bins=int(180\/5), \n                    hist=False, kde_kws={'linewidth': 2, 'shade':True})","a8f8b0b4":"t = train_err_data.groupby(['user_id', 'year_date'])['time'].count().reset_index()\nt['rn'] = t.groupby(['user_id']).cumcount() + 1\nt = t.groupby('user_id')['time'].agg(['max', 'min']).reset_index()\nt['diff'] = t['max'] \/ t['min']\nt.head()","397aee61":"temp = pd.merge(t, user_problem_cnt, how='left', on='user_id').fillna(0)\ntemp.head()","53d0af1b":"g=sns.distplot(temp[(temp.time == 0)&(temp['diff'] < 300)]['diff'], color=\"blue\", label=\"0 time\")\nsns.distplot(temp[(temp.time == 1)&(temp['diff'] < 300)]['diff'], color=\"red\", label=\"1 time\")\nsns.distplot(temp[(temp.time == 2)&(temp['diff'] < 300)]['diff'], color=\"yellow\", label=\"2 time\")\nsns.distplot(temp[(temp.time == 3)&(temp['diff'] < 300)]['diff'], color=\"green\", label=\"3 time\")\nsns.distplot(temp[(temp.time == 4)&(temp['diff'] < 300)]['diff'], color=\"k\", label=\"4 time\")\n\ng.set(xlim=(-10, 100))\n\nplt.legend(title=\"dd\")\n\nplt.show()","1a98e07f":"tt = train_err_data.groupby(['user_id', 'year_date'])['time'].count().reset_index()\ntt.head()","2d667627":"t = train_err_data.loc[train_err_data.user_id.isin(train_problem_data.user_id.unique())]\nt = pd.merge(t, train_problem_data, how ='left', on='user_id')\nt = t.loc[t.time < t.pb_time]\nt","00f6bfbf":"err_report_time = (t.groupby('user_id')['pb_time'].max() - t.groupby('user_id')['time'].max())\/6000","d7aa8fdd":"err_report_time.plot()","dc5dc7eb":"t['Rank'] = t.groupby(by=['user_id'])['time'].transform(lambda x: x.rank(method=\"first\", ascending=False))\nlast_err = t.loc[t.Rank == 1, ].groupby('errtype')['user_id'].count().sort_values()\nplt.bar(last_err.index, last_err)","aec68f0f":"last_err.tail()","2f495530":"last_err2 = t.loc[t.Rank < 3 , ].groupby('errtype')['user_id'].count().sort_values()\nplt.bar(last_err2.index, last_err2)","4d3c9c0d":"train_user_id_max = 24999\ntrain_user_id_min = 10000\ntrain_user_number = 15000\n\ndef make_gram_to_pd(n, t):\n    gram_2 = pd.Series(nltk.ngrams(t.errtype, n)).value_counts()\n    return gram_2.sort_values().index[-1]","3f7e5627":"result = pd.DataFrame(index = range(train_user_id_min, train_user_id_max+1))\nresult['gram_2'] = ''\nresult['gram_3'] = ''\nresult['gram_4'] = ''\nresult['gram_5'] = ''\n\nfor i in tqdm(range(train_user_id_min, train_user_id_max+1)): #range(train_user_id_min, train_user_id_max+1):\n    t = train_err_data.loc[train_err_data.user_id==i, ]\n    len_t = len(t)\n    if len_t > 1:\n        result.loc[i, 'gram_2'] = make_gram_to_pd(n=2, t = t)\n    if len_t > 2:    \n        result.loc[i, 'gram_3'] = make_gram_to_pd(n=3, t = t)\n    if len_t > 3: \n        result.loc[i, 'gram_4'] = make_gram_to_pd(n=4, t = t)\n    if len_t > 4:    \n        result.loc[i, 'gram_5'] = make_gram_to_pd(n=5, t = t)\n        \nresult = result.reset_index()\nresult.columns = ['user_id', 'gram_2','gram_3','gram_4','gram_5']","2a4d77e7":"pb_data = train_problem_data.groupby('user_id')['pb_datetime'].min().reset_index()\nt = pd.merge(result, pb_data, how='left', on='user_id')\nt['y'] = t.pb_datetime.notnull().astype('int')\nt = t.drop('pb_datetime', axis = 1)","a82d69de":"gram2 = t.groupby('gram_2')['y'].agg(['mean', 'count']).sort_values('count', ascending = False)\ngram2 = gram2.reset_index()\ngram2 = gram2.query('mean > 0.5 and count > 100')\ngram2['cumsum'] = gram2['count'].cumsum()\ngram2","0e6b4b25":"gram3 = t.groupby('gram_3')['y'].agg(['mean', 'count']).sort_values('count', ascending = False)\ngram3 = gram3.reset_index()\n# gram3 = gram3.query('mean > 0.5 and count > 100')\ngram3['cumsum'] = gram3['count'].cumsum()\ngram3","1009531e":"gram4 = t.groupby('gram_4')['y'].agg(['mean', 'count']).sort_values('count', ascending = False)\ngram4 = gram4.reset_index()\ngram4 = gram4.query('mean > 0.5 and count > 100')\ngram4['cumsum'] = gram4['count'].cumsum()\ngram4","78b12e91":"gram5 = t.groupby('gram_5')['y'].agg(['mean', 'count']).sort_values('count', ascending = False)\ngram5 = gram5.reset_index()\n# gram5 = gram5.query('mean > 0.5 and count > 100')\ngram5['cumsum'] = gram5['count'].cumsum()\ngram5","33055e08":"problem = np.zeros(15000)\n# error\uc640 \ub3d9\uc77c\ud55c \ubc29\ubc95\uc73c\ub85c person_idx - 10000 \uc704\uce58\uc5d0 \n# person_idx\uc758 problem\uc774 \ud55c \ubc88\uc774\ub77c\ub3c4 \ubc1c\uc0dd\ud588\ub2e4\uba74 1\n# \uc5c6\ub2e4\uba74 0\nproblem[train_problem_data.user_id.unique()-10000] = 1 \nproblem.shape","d678bcfc":"id_error = train_err_data[['user_id','errtype']].values\nerror = np.zeros((train_user_number,42))\n\nfor person_idx, err in tqdm(id_error):\n    # person_idx - train_user_id_min \uc704\uce58\uc5d0 person_idx, errtype\uc5d0 \ud574\ub2f9\ud558\ub294 error\uac12\uc744 +1\n    error[person_idx - train_user_id_min,err - 1] += 1","8831c850":"error2 = pd.DataFrame(error)\nerror2['user_id'] = t.user_id\nerror2 = pd.merge(error2, t.loc[t.gram_2==(16, 31),['user_id', 'y']], on='user_id', how ='left').rename(columns={'y': 'gram_2'}).fillna(0)\nerror2 = pd.merge(error2, t.loc[t.gram_3==(15, 16, 31),['user_id', 'y']], on='user_id', how ='left').rename(columns={'y': 'gram_3'}).fillna(0)\nerror2 = pd.merge(error2, t.loc[t.gram_4==(15, 16, 31, 31),['user_id', 'y']], on='user_id', how ='left').rename(columns={'y': 'gram_4'}).fillna(0)\nerror2 = pd.merge(error2, t.loc[t.gram_5==(31, 15, 16, 31, 31),['user_id', 'y']], on='user_id', how ='left').rename(columns={'y': 'gram_5'}).fillna(0)","25113ddd":"train_x = error2.drop(['user_id' ], axis = 1)#,'gram_2','gram_3', 'gram_4', 'gram_5'\ntrain_y = problem#t['y']#\n# del error, problem\nprint(train_x.shape)\nprint(train_y.shape)","a363670f":"# Train\n#-------------------------------------------------------------------------------------\n# validation auc score\ub97c \ud655\uc778\ud558\uae30 \uc704\ud574 \uc815\uc758\ndef f_pr_auc(probas_pred, y_true):\n    labels=y_true.get_label()\n    p, r, _ = precision_recall_curve(labels, probas_pred)\n    score=auc(r,p) \n    return \"pr_auc\", score, True\n#-------------------------------------------------------------------------------------\nmodels     = []\nrecalls    = []\nprecisions = []\nauc_scores   = []\nthreshold = 0.5\n# \ud30c\ub77c\ubbf8\ud130 \uc124\uc815\nparams =      {\n                'boosting_type' : 'gbdt',\n                'objective'     : 'binary',\n                'metric'        : 'auc',\n                'seed': 1015\n                }\n#-------------------------------------------------------------------------------------\n# 5 Kfold cross validation\nk_fold = KFold(n_splits=5, shuffle=True, random_state=42)\nfor train_idx, val_idx in k_fold.split(train_x):\n\n    # split train, validation set\n    X = train_x.loc[train_idx]\n    y = train_y[train_idx]\n    valid_x = train_x.loc[val_idx]\n    valid_y = train_y[val_idx]\n\n    d_train= lgb.Dataset(X, y)\n    d_val  = lgb.Dataset(valid_x, valid_y)\n    \n    #run traning\n    model = lgb.train(\n                        params,\n                        train_set       = d_train,\n                        num_boost_round = 1000,\n                        valid_sets      = d_val,\n                        feval           = f_pr_auc,\n                        verbose_eval    = 20, \n                        early_stopping_rounds = 3\n                       )\n    \n    # cal valid prediction\n    valid_prob = model.predict(valid_x)\n    valid_pred = np.where(valid_prob > threshold, 1, 0)\n    \n    # cal scores\n    recall    = recall_score(    valid_y, valid_pred)\n    precision = precision_score( valid_y, valid_pred)\n    auc_score = roc_auc_score(   valid_y, valid_prob)\n\n    # append scores\n    models.append(model)\n    recalls.append(recall)\n    precisions.append(precision)\n    auc_scores.append(auc_score)\n\n    print('==========================================================')","09056854":"print(np.mean(auc_scores))","5ece3a98":"# \ub370\uc774\ud130 \uc124\uba85\uc744 \ud655\uc778\ud558\uba74\n# test \ub370\uc774\ud130\ub294 ueser_id\uac00 30000\ubd80\ud130 44998\uae4c\uc9c0 \ucd1d 14999\uac1c\uac00 \uc874\uc7ac.\ntest_user_id_max = 44998\ntest_user_id_min = 30000\ntest_user_number = 14999","9cc74d99":"id_error = test_err_data[['user_id','errtype']].values\ntest_x = np.zeros((test_user_number,42))\nfor person_idx, err in tqdm(id_error):\n    # person_idx - test_user_id_min \uc704\uce58\uc5d0 person_idx, errtype\uc5d0 \ud574\ub2f9\ud558\ub294 error\uac12\uc744 +1\n    test_x[person_idx - test_user_id_min,err - 1] += 1\ntest_x = test_x.reshape(test_x.shape[0],-1)\nprint(test_x.shape)","2d4c039e":"result_test = pd.DataFrame(index = range(test_user_id_min, test_user_id_max+1))\nresult_test['gram_2'] = ''\nresult_test['gram_3'] = ''\nresult_test['gram_4'] = ''\nresult_test['gram_5'] = ''\n\nfor i in tqdm(range(test_user_id_min, test_user_id_max+1)): #range(train_user_id_min, train_user_id_max+1):\n    t = test_err_data.loc[test_err_data.user_id==i, ]\n    len_t = len(t)\n    if len_t > 1:\n        result_test.loc[i, 'gram_2'] = make_gram_to_pd(n=2, t = t)\n    if len_t > 2:    \n        result_test.loc[i, 'gram_3'] = make_gram_to_pd(n=3, t = t)\n    if len_t > 3: \n        result_test.loc[i, 'gram_4'] = make_gram_to_pd(n=4, t = t)\n    if len_t > 4:    \n        result_test.loc[i, 'gram_5'] = make_gram_to_pd(n=5, t = t)\n        \nresult_test = result_test.reset_index()\nresult_test.columns = ['user_id', 'gram_2','gram_3','gram_4','gram_5']","c9bf9bba":"error2_test = pd.DataFrame(test_x)\nerror2_test['user_id'] = result_test.user_id\nerror2_test = pd.merge(error2_test, result_test.loc[result_test.gram_2==(16, 31),['user_id', 'gram_2']], on='user_id', how ='left')#\nerror2_test['gram_2'] = error2_test.gram_2.notnull().astype(int)\n\nerror2_test = pd.merge(error2_test, result_test.loc[result_test.gram_3==(15, 16, 31),['user_id', 'gram_3']], on='user_id', how ='left')#\nerror2_test['gram_3'] = error2_test.gram_3.notnull().astype(int)\n\nerror2_test = pd.merge(error2_test, result_test.loc[result_test.gram_4==(15, 16, 31, 31),['user_id', 'gram_4']], on='user_id', how ='left')#\nerror2_test['gram_4'] = error2_test.gram_4.notnull().astype(int)\n\nerror2_test = pd.merge(error2_test, result_test.loc[result_test.gram_5==(31, 15, 16, 31, 31),['user_id', 'gram_5']], on='user_id', how ='left')#\nerror2_test['gram_5'] = error2_test.gram_5.notnull().astype(int)\n","baeac48b":"error2_test = error2_test.drop(['user_id'], axis = 1)#drop('user_id', axis = 1) # ,'gram_2','gram_3', 'gram_4', 'gram_5' ","033512f8":"pred_y_list = []\nfor model in models:\n    pred_y = model.predict(error2_test)\n    pred_y_list.append(pred_y.reshape(-1,1))\n    \npred_ensemble = np.mean(pred_y_list, axis = 0)","6dd0c7fc":"sample_submssion = pd.read_csv('\/kaggle\/input\/dacon-lg\/sample_submission.csv')","eb8cb9c2":"sample_submssion['problem'] = pred_ensemble.reshape(-1)\nsample_submssion.to_csv(\"ngram2.csv\", index = False)\nsample_submssion","f3c663b4":"lgb.plot_importance(models[4])","ac35b137":"train_err_data","ae2fba9b":"t = train_err_data.groupby(['user_id'])['fwver'].apply(lambda x: x.nunique())-1\nt = t.reset_index()\nt2 = train_problem_data.groupby('user_id')['pb_time'].nunique().reset_index()\nt2['category'] = (t2['pb_time']>0 ).astype(int)\nt3 = pd.merge(t2[['user_id', 'category']],t, how ='left', on='user_id')\nt3","fb5d80fe":"t3['fwver'].sum()","4c11d006":"# \ucffc\ub9b4\ud2f0 \ub85c\uadf8\uac00 \uc788\ub294 \uc720\uc800\ub4e4 \uc911, \uba87\uba85\uc774\ub098 \uc2e0\uace0\ud588\uc744\uae4c?\nq = set(train_quality_data['user_id'].unique())\np = len(train_problem_data['user_id'].unique())\nt = q.intersection(set(train_problem_data['user_id'].unique())) \n\nprint(f\"\uc804\uccb4 \uc2e0\uace0\uc790 {p}\uba85 \uc911, {len(t)}\uba85\uc774 \ud004\ub9ac\ud2f0 \ud655\uc778\uc744 \ud568\")\nprint(f\"\ud004\ub9ac\ud2f0 \ub85c\uadf8\uac00 \uc788\ub294 \uc720\uc800\ub294 \ucd1d {len(q)} \uba85 \")","3022bfe8":"# \ud004\ub9ac\ud2f0 \ub85c\uadf8\uc5d0 \ubcc0\ub3d9\uc774 \uc0dd\uae34 \uc720\uc800\ub294?\ncols = train_quality_data.columns\ncols = [x for x in cols if 'qu' in x]","fa01a747":"def how_about_nunique(x):\n    return x.nunique()\n\ndef how_about_unique(x):\n    unique_items = np.unique(x.values)\n    return len(unique_items)","64743720":"# train_quality_data.loc[train_quality_data.user_id==23664, ].groupby(['user_id', 'time'])[cols].apply(how_about_nunique)-1\n\nt = train_quality_data.groupby(['user_id', 'time'])[cols].apply(how_about_nunique)-1","57d0a955":"# t = train_quality_data.loc[train_quality_data.user_id==23664, ].groupby(['user_id', 'time'])[cols].apply(how_about_nunique)-1\nt = t.reset_index()\nt","8e74c358":"t.groupby('user_id')[cols].sum().reset_index()","e846220e":"# \uc2e0\uace0\uc804 \ub9c8\uc9c0\ub9c9 \uc5d0\ub7ec\ub294 \uba87\ubd84\uc804\uc5d0 \ubc1c\uc0dd\ud588\uc744\uae4c?","feeb11af":"* \uc5d0\ub7ec \ud0c0\uc785\ubcc4 \ud004\ub9ac\ud2f0 \uc0c1\ud0dc \ud655\uc778\n* \uc720\uc800\ubcc4 \uc804\uc77c\ub300\ube44 \uc5d0\ub7ec\uc728 \uc99d\uac00\ucd94\uc774","21930871":"# \ud38c\uc6cc\uc5d0 \uc5c5\ub370\uc774\ud2b8\uac00 \uc77c\uc5b4\ub09c \uace0\uac1d\uc778\uc9c0?\n* \uc5c5\uadf8\ub808\uc774\ub4dc? \ub2e4\uc6b4\uadf8\ub808\uc774\ub4dc\n","f5cbc57e":"# \ubcf4\uace0 \uc804, \ub9c8\uc9c0\ub9c9 \uc5d0\ub7ec\ucf54\ub4dc\ub294?","d8ee65c4":"# \uc5d0\ub7ec \ubc1c\uc0dd\uc5d0\ub294 \uc5b4\ub5a4 \ud328\ud134\uc774 \uc788\uc9c0 \uc54a\uc744\uae4c?","f721aa2b":"# Quaily EDA\n\n* 2\uc2dc\uac04\ub9c8\ub2e4 10\ubd84\uac04\uaca9\uc73c\ub85c \ub85c\uadf8\uc804\uc1a1\n* \ub85c\uadf8 \uc804\uc1a1\uc740 \uc0ac\uc6a9\uc790\uac00 \uc120\ud0dd\ud55c\ub2e4\ub294 \uc810\n--> \ud004\ub9ac\ud2f0 \ub85c\uadf8\uc5d0 \ubcc0\ub3d9\uc774 \uc788\ub294 \uace0\uac1d\uc774 \uc788\ub294\uc9c0 \ud655\uc778"}}