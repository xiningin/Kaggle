{"cell_type":{"bb8277dd":"code","ba2d10a9":"code","55eb1c60":"code","6bbadde0":"code","27c8efa6":"code","ed52fadb":"code","890b4cfd":"code","9369e681":"code","856ed906":"code","1ce25396":"code","a3960b1c":"code","22c3d7ff":"code","119329e5":"code","35518863":"code","e3ca5272":"code","80c198ac":"code","17ffee53":"code","23fd0436":"code","dea5ce3e":"code","16af09e7":"code","71bb453a":"code","d361572f":"markdown","5b43b3d6":"markdown","27d4fbb6":"markdown","f34f53e0":"markdown","3d6c6272":"markdown","3c80ef61":"markdown","a45af5db":"markdown","0837a3ad":"markdown","2e75707d":"markdown","417169f2":"markdown"},"source":{"bb8277dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ba2d10a9":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nimport tensorflow.compat.v1 as tf\ntf.compat.v1.disable_v2_behavior()","55eb1c60":"train_data = pd.read_csv('\/kaggle\/input\/sign-language-mnist\/sign_mnist_train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/sign-language-mnist\/sign_mnist_test.csv')","6bbadde0":"train_data.head()","27c8efa6":"train_data.shape, test_data.shape","ed52fadb":"train_data.head()","890b4cfd":"plt.figure(figsize=(15,10))\nsns.countplot(train_data['label'])","9369e681":"f, ax = plt.subplots(2,5) \nf.set_size_inches(10, 10)\nk = 0\nfor i in range(2):\n    for j in range(5):\n        ax[i,j].imshow(x_train[k].reshape(28, 28) , cmap = \"gray\")\n        k += 1\n    plt.tight_layout()","856ed906":"encoder = OneHotEncoder() # encoding target variable i.e. label\n\nx_train = (train_data.iloc[:, 1:]\/255).values # normalizing train images to standard 0-1 pixel values.\ny_train = encoder.fit_transform(train_data['label'].values.reshape(-1,1)).toarray()\n\nx_test = (test_data.iloc[:, 1:]\/255).values # normalizing test images to standard 0-1 pixel values.\ny_test = encoder.fit_transform(test_data['label'].values.reshape(-1,1)).toarray()","1ce25396":"x_train.shape, y_train.shape, x_test.shape, y_test.shape","a3960b1c":"input_width = 28\ninput_height = 28\ninput_channel = 1\ninput_pixels = 784\n\nn_conv1 = 64\nn_conv2 = 128\nstride_conv1 = 1\nstride_conv2 = 1\nfilter1_k = 5\nfilter2_k = 5\nmaxpool1_k = 2\nmaxpool2_k = 2\n\nn_hidden = 1024\nn_out = 24\n\ninput_size_to_hidden_layer = ((input_width\/\/(maxpool1_k*maxpool2_k)) * (input_height\/\/(maxpool1_k*maxpool2_k)) * n_conv2)","22c3d7ff":"weights = {\n    'wc1' : tf.Variable(tf.random_normal([filter1_k, filter1_k, input_channel, n_conv1])), # weight corresponding to convolutional layer1.\n    'wc2' : tf.Variable(tf.random_normal([filter2_k, filter2_k, n_conv1, n_conv2])), # weight corresponding to convolutional layer2.\n    'wh' : tf.Variable(tf.random_normal([input_size_to_hidden_layer, n_hidden])),  # weight corresponding to hidden layer.\n    'wo' : tf.Variable(tf.random_normal([n_hidden, n_out])) # weight corresponding to output layer.\n}\n\nbiases = {\n    'bc1' : tf.Variable(tf.random_normal([n_conv1])), # biases corresponding to convolutional layer1.\n    'bc2' : tf.Variable(tf.random_normal([n_conv2])), # biases corresponding to convolutional layer2.\n    'bh' : tf.Variable(tf.random_normal([n_hidden])), # biases corresponding to hidden layer.\n    'bo' : tf.Variable(tf.random_normal([n_out])) # biases corresponding to output layer.\n}","119329e5":"# function to get the output from a convolutional layer.\ndef conv(x, weights, bias, stride = 1):\n    output = tf.nn.conv2d(x, weights, padding='SAME', strides=[1, stride, stride, 1])\n    output = tf.nn.bias_add(output, bias)\n    output = tf.nn.relu(output) # applying activation function.\n    return output","35518863":"# function which return output of pooling layer used to decrease image size so that we have to train less weights and biases.\ndef maxpooling(x, k):\n    return tf.nn.max_pool(x, padding='SAME', ksize=[1, k, k, 1], strides=[1, k, k, 1])","e3ca5272":"def forward_propagation(x, weights, biases):\n    x = tf.reshape(x, shape = [-1, input_width, input_height, input_channel])\n    \n    conv1 = conv(x, weights['wc1'], biases['bc1'], stride_conv1)\n    conv1_pool = maxpooling(conv1, maxpool1_k)\n    \n    conv2 = conv(conv1_pool, weights['wc2'], biases['bc2'], stride_conv2)\n    conv2_pool = maxpooling(conv2, maxpool2_k)\n    \n    hidden_layer_input = tf.reshape(conv2_pool, shape = [-1, input_size_to_hidden_layer])\n    hidden_layer_output = tf.nn.relu(tf.add(tf.matmul(hidden_layer_input, weights['wh']), biases['bh']))\n    \n    output = tf.add(tf.matmul(hidden_layer_output, weights['wo']), biases['bo'])\n    return output","80c198ac":"X = tf.placeholder(tf.float32, [ None, input_pixels], name='x')\nY = tf.placeholder(tf.int32, [ None, n_out], name='y')\npred = forward_propagation(X, weights, biases)","17ffee53":"cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=Y))\n\n# using adam optimizer on the cost.\noptimizer = tf.train.AdamOptimizer(learning_rate=0.011)\noptimize = optimizer.minimize(cost)","23fd0436":"# creating a new session of tensorflow.\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())","dea5ce3e":"batch_size=64\na = 0\nfor i in range(10):\n    num_batches = int(len(x_train)\/batch_size)\n    total_cost = 0\n    for j in range(num_batches):\n        batch_x = x_train[a: a+batch_size]\n        batch_y = y_train[a: a+batch_size]\n        c, _ = sess.run([cost, optimize], feed_dict={X:batch_x, Y:batch_y})\n        total_cost += c\n        a += batch_size\n    a = 0\n    print('total cost at',i+1,'iteration:',total_cost)","16af09e7":"# testing model with training data.\npredictions = tf.argmax(pred, axis=1)\ncorrect_labels = tf.argmax(Y, axis=1)\naccuracy = tf.equal(predictions, correct_labels)\npredictions, labels, accuracy = sess.run([predictions, correct_labels, accuracy], feed_dict={X:x_train, Y:y_train})\naccuracy.sum()\/len(x_train)","71bb453a":"# testing model with testing data.\npredictions = tf.argmax(pred, axis=1)\ncorrect_labels = tf.argmax(Y, axis=1)\naccuracy = tf.equal(predictions, correct_labels)\npredictions, labels, accuracy = sess.run([predictions, correct_labels, accuracy], feed_dict={X:x_test, Y:y_test})\naccuracy.sum()\/len(x_test)","d361572f":"# **Data Preprocessing**","5b43b3d6":"# **Batch Gradient Descent To Train**","27d4fbb6":"# **Data Visualisation**","f34f53e0":"# **Testing Our Model**","3d6c6272":"**Previewing Few Images**","3c80ef61":"# **Initializing Weights And Units In Layers**","a45af5db":"# **Defining Variables**","0837a3ad":"Classes that we have to predict are pretty balanced in data.","2e75707d":"# **Forward Propagation**","417169f2":"# **Loading Data**"}}