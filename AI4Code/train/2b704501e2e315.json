{"cell_type":{"9314951f":"code","92b14285":"code","4dc0a2e8":"code","27a12743":"code","0fba1bab":"code","dcd17530":"code","6af8e149":"code","e20c2a5d":"code","5e8e7267":"code","e74fc539":"code","5055c767":"code","b3144304":"code","9ebc9604":"code","ace92538":"code","47eb1f11":"code","89fe1f3d":"code","678db213":"code","ad0baae9":"code","919cd439":"code","3334360f":"code","d9728e7e":"code","bb077425":"code","8141fa88":"code","e71fbee8":"code","a2430087":"code","22b148d2":"code","93aef80d":"code","462dccd8":"code","21f76bf2":"code","2ce2f2af":"code","e1e484e9":"code","c50db211":"code","0b072bd0":"code","deb1f282":"code","71117e8e":"code","f2796f72":"code","8b270d6c":"code","bee60baf":"code","dadab443":"code","f7f05414":"code","a5ed2d1d":"code","fae1f7cc":"markdown","1b3fc437":"markdown","c0ff1620":"markdown","9f1a9009":"markdown","674822a2":"markdown","429cd6bc":"markdown","5a9211db":"markdown","7a03dbc2":"markdown","9efdb670":"markdown","cd13be9b":"markdown","425449f5":"markdown","55c081a5":"markdown","7f06105d":"markdown","d6bec32b":"markdown","cdf3bb3f":"markdown","7f0628a8":"markdown","f9ddd198":"markdown","3f0c1187":"markdown","00666ce4":"markdown","ab5caac8":"markdown","8e166afa":"markdown","39d657b2":"markdown","aa7849c8":"markdown","9ab12740":"markdown","e6b9dfe3":"markdown"},"source":{"9314951f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n%matplotlib inline","92b14285":"import os # accessing directory structure\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4dc0a2e8":"# Load data from Excel csv sheet\ndf = pd.read_csv('..\/input\/iris-dirty\/iris_data_corrupted.csv', delimiter = ',',\n                 names=['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'species-id', 'species'])\nprint(df)\n# show 10 out of 151 records for 4 attributes and 1 target class and its derivable attribute ID\n\n# link to Flower csv file, extracted as at 31 Oct 2020 : https:\/\/drive.google.com\/file\/d\/1uM0orcGAIeOZAElZqsCwhukcbHGrWCgJ\/view?usp=sharing\n# link to Kaggle data source : https:\/\/www.kaggle.com\/larsgrygosch\/iris-dirty","27a12743":"# return the object type, which is dataframe\ntype(df)\n\n# dataframes allow a wider variety of methods for data analysis","0fba1bab":"# 'sepal-width' is inconsistent, as measurements are in mm instead of cm\n\n# replace \" mm\" with \"\"\ndf['sepal-width'] = df['sepal-width'].str.replace(\" mm\", \"\")\n\n# change attribute type from string to numerical\ndf['sepal-width'] = pd.to_numeric(df['sepal-width'])\n\n# change measurements to cm by dividing by 10\ndf['sepal-width'] = df['sepal-width'].div(10)\nprint(df)","dcd17530":"# display unique class labels of Flower species\ndf['species'].unique()\n# identified 1 inconsistent value for species \"setosa\" and \"setsoa\"","6af8e149":"# replace all 'setsoa' with 'setosa'\ndf['species'] = df['species'].replace('setsoa', 'setosa')\ndf['species'].unique()","e20c2a5d":"# identify impossible values and outliers using boxplot\ndf.boxplot(rot=0, boxprops=dict(color='blue'))\nplt.title(\"Box Plot of Flower Data\") # title of plot\nplt.suptitle(\"\")\nplt.xlabel(\"Attribute\") # x axis label\nplt.ylabel(\"Measurements (cm)\") # y axis label\nplt.show()\n\n# outliers identified for 'sepal-width' and 'petal-length'","5e8e7267":"# smooth outliers for 'sepal-width' using winsorization technique\n# replace outlier with maximum or minimum non-outlier \n\n# compute interquartile range (IQR)\nIQR = df['sepal-width'].quantile(0.75) - df['sepal-width'].quantile(0.25)\n\n# compute maximum and minimum non-outlier value\nminAllowed = df['sepal-width'].quantile(0.25)-1.5*IQR\nmaxAllowed = df['sepal-width'].quantile(0.75)+1.5*IQR\n\n# replace outlier values\nfor i in range(len(df['sepal-width'])): \n    if df['sepal-width'][i] < minAllowed:\n       df['sepal-width'] = df['sepal-width'].replace(df['sepal-width'][i], minAllowed)\n    elif df['sepal-width'][i] > maxAllowed:\n       df['sepal-width'] = df['sepal-width'].replace(df['sepal-width'][i], maxAllowed)\n    else: continue","e74fc539":"# compute range of values for 'petal-length'\nmin_val = min(df['petal-length'])\nmax_val = max(df['petal-length'])\nrange_data = (min_val, max_val)\nprint(range_data)\n\n# assumed wrong entry of 103 as 10.3","5055c767":"# smooth outlier for 'petal-length' by replacing with the value of 10.3\ndf['petal-length'] = df['petal-length'].replace(max(df['petal-length']), 10.3)","b3144304":"# confirm smoothed outliers using boxplot\ndf.boxplot(rot=0, boxprops=dict(color='blue'))\nplt.title(\"Box Plot of Flower Data\") # title of plot\nplt.suptitle(\"\")\nplt.xlabel(\"Attribute\") # x axis label\nplt.ylabel(\"Measurements (cm)\") # y axis label\nplt.show()","9ebc9604":"# detect duplicated records\ndf[df.duplicated(subset=None, keep=False)]","ace92538":"# drop the duplicated records, retain only one copy for each\ndf = pd.DataFrame.drop_duplicates(df)\nprint(df)","47eb1f11":"# display the number of entries, the number and names of the column attributes, the data type and\n    # digit placings, and the memory space used\ndf.info()\n\n# output :\n# 1 target object class\n# 4 attributes of continuous numerical feature data type with 64 digit placings\n# 149 records with one missing value\n# memory space usage is at least 6kb\n\n# identified 1 missing value for attribute 'petal-width'","89fe1f3d":"# fill in missing value with the mean value of 'petal-width'\ndf['petal-width'] = df['petal-width'].fillna(df['petal-width'].mean())\ndf.info()","678db213":"# summary statistics of the attributes, including measures of central tendency and \n    # measures of dispersion\ndf.describe() \n\n# the range is appropriate and small, data transformation is not needed","ad0baae9":"# display unique names of Flower species\ndf['species'].unique()","919cd439":"# display unique class ID of Flower species\ndf['species-id'].unique()\n# identified redundant attribute 'species-id' as derivable data from target class 'species'","3334360f":"# drop 'species-id' for data reduction\ndf.drop('species-id', axis=1, inplace=True)\nprint(df)","d9728e7e":"# compare linear relationships between attributes using correlation coefficient generated using\n    # correlation matrix\ncorrMatrix = df.corr()\n\n# only remove redundant features with high intercorrelations of above absolute value of 0.95\n    # since chosen KNN, Decision Tree, and Naive Bayes models are immune to multicollinearity\n# note that those above absolute value of 0.5 is considered highly correlated\ncor_features = corrMatrix[:]\nrelevant_features = cor_features[abs(cor_features)<0.95]\nrelevant_features","bb077425":"# import searborn library for more variety of data visualisation using fewer syntax and \n# interesting default themes\nimport seaborn as sns \n\n# visualise pairs plot or scatterplot matrix in relation to species\ng = sns.pairplot(df, hue='species')\ng = g.map_upper(plt.scatter)\ng = g.map_lower(sns.kdeplot)\n\n# relatively clear class-attribute relationship","8141fa88":"# start exploratory data analysis (EDA) to summarise main characteristics\n\n# display the number of entries, the number and names of the column attributes, the data type and\n    # digit placings, and the memory space used\ndf.info()\n\n# output :\n# 1 target object class\n# 4 columns\/attributes of continuous numerical feature data type with 64 digit placings\n# 149 rows\/records with no missing values\n# memory space usage is at least 6kb\n\n# labeled data will use supervised learning techniques","e71fbee8":"# list and count the class names and their frequency\nfrom collections import Counter\ncount = Counter(df['species'])\nprint(count.items())","a2430087":"# summary statistics of the attributes, including measures of central tendency and \n    # measures of dispersion\ndf.describe() ","22b148d2":"# density plotting to visualise data frequency for overall data distribution for three features \n# that are correlated to the target feature of Flower species\nfrom seaborn import distplot\n\n# features for density plotting\nfor i in ('sepal-length', 'sepal-width', 'petal-length', 'petal-width'):\n    plt.figure()\n    \n    # compare distribution of different Flower species by using different RGB hex codes to \n    # differentiate the species by colour\n    distplot(df[i][df['species']=='virginica'], color='#990099')\n    distplot(df[i][df['species']=='versicolor'], color='#990033')\n    distplot(df[i][df['species']=='setosa'], color='#FF0000')\n    \n    # plot distribution of data, set graph title, and specify a dashed line style and transparency\n    df[i].plot.kde(title=i.title(), linestyle='dashed', alpha=0.7)\n    \n    # display legend by passing location argument (to top right corner) and a list of legend texts\n    plt.legend(loc='upper right', labels=['Distribution of Data','Virginica','Versicolor', 'Setosa'])\n    \n    # set x axis limit\n    plt.xlim(0, )\n\n    plt.show()\n    \n# ranking of correlation to Flower species in decreasing order : \n# petal-width, petal-length, sepal-length, sepal-width\n\n# approximate normal data distribution for sepal length and width, since it is approximately\n    # symmetric, unimodal, asymptotic, and the mean, median, and mode are similar\n# slightly larger sepal lengths from Setosa to Versicolor to Virginica\n# Setosa has slightly larger sepal widths than the other two species\n\n# bimodal or multimodel data distribution for petal length and width, since there are two \n    # distinct peaks (local maxima)\n# larger petal lengths and widths from Setosa to Versicolor to Virginica","93aef80d":"# classify and model the data using k-Nearest Neighbour (KNN), Decision Tree (DT), and Naive Bayes (NB)\n    # machine learning algorithms\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport math\n\n# split dataset into attributes and labels\nX = df.iloc[:, :-1].values # the attributes\ny = df.iloc[:, 4].values # the labels\n\n# choose appropriate range of training set proportions\nt = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n\n# plot decision tree based on information gain\nDT = DecisionTreeClassifier(splitter='best', criterion='entropy')\n\n# use Gaussian method to support continuous data values\nNB = GaussianNB()\n\n# choose recommended optimal number of clusters of sqrt(number of records)\nKNN = KNeighborsClassifier(n_neighbors = math.ceil(math.sqrt(151)))\n\n# find best training set proportion for the chosen models\nplt.figure()\nfor s in t:\n    scores = []\n    for i in range(1,1000):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-s, random_state=111)\n        DT.fit(X_train, y_train) # consider DT scores\n        scores.append(DT.score(X_test, y_test))\n        NB.fit(X_train, y_train) # consider NB scores\n        scores.append(NB.score(X_test, y_test))\n        KNN.fit(X_train, y_train) # consider KNN scores\n        scores.append(KNN.score(X_test, y_test))\n    plt.plot(s, np.mean(scores), 'bo')\nplt.xlabel('Training Set Proportion') # x axis label\nplt.ylabel('Accuracy'); # y axis label","462dccd8":"# choose train test splits from original dataset as 80% train data and 20% test data for highest accuracy\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=111)\n\n# find optimal k number of clusters\nk_range = range(1,20)\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train)\n    scores.append(knn.score(X_test, y_test))\nplt.figure()\nplt.xlabel('k') # x axis label\nplt.ylabel('Accuracy') # y axis label\nplt.scatter(k_range, scores) # scatter plot\nplt.xticks([0,5,10,15,20]);","21f76bf2":"# using k-Nearest Neighbour (KNN) classifier\n# choose 7 as the optimal number of clusters\nclassifierKNN = KNeighborsClassifier(n_neighbors=7)\nclassifierKNN.fit(X_train, y_train)\n\n# using Euclidean distance metric\nclassifierKNN.effective_metric_","2ce2f2af":"# using Naive Bayes (NB) classifier\nclassifierNB = GaussianNB()\nclassifierNB.fit(X_train, y_train)\n\n# show prior probability of each class\nclassifierNB.class_prior_","e1e484e9":"# using Decision Tree (DT) classifier\nclassifierDT = DecisionTreeClassifier(splitter='best', criterion='entropy')\nclassifierDT.fit(X_train, y_train)\n\n# plot decison tree\nfrom sklearn import tree\nfig = plt.figure(figsize=(55,20))\nfn = ['sepal-length','sepal-width','petal-length','petal-width']\nDT = tree.plot_tree(classifierDT,\n                    feature_names=fn,  \n                    class_names=y,\n                    filled=True)\n# outputs 9 extracted rules","c50db211":"# identifies the important features\nclassifierDT.feature_importances_","0b072bd0":"# number of records in training set\nlen(X_train)","deb1f282":"# count each species in training set\ncount = Counter(y_train)\nprint(count.items())","71117e8e":"# number of records in test set\nlen(X_test)","f2796f72":"# use the chosen three models to make predictions on test data\ny_predKNN = classifierKNN.predict(X_test)\ny_predDT = classifierDT.predict(X_test)\ny_predNB = classifierDT.predict(X_test)","8b270d6c":"# for k-Nearest Neighbours model\n# using confusion matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_predKNN))\nprint(classification_report(y_test, y_predKNN))\n\n# using accuracy performance metric\nfrom sklearn import metrics\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_predKNN))","bee60baf":"# for Naive Bayes model\n# using confusion matrix\nprint(confusion_matrix(y_test, y_predNB))\nprint(classification_report(y_test, y_predNB))\n\n# using accuracy performance metric\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_predNB))","dadab443":"# for Decision Tree model\n# using confusion matrix\nprint(confusion_matrix(y_test, y_predDT))\nprint(classification_report(y_test, y_predDT))\n\n# using accuracy performance metric\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_predDT))","f7f05414":"# new data\nnewdata = [[1, 2, 3, 3]]\n\n# compute probabilities of assigning to each of the three classes of species\nprobaKNN = classifierKNN.predict_proba(newdata)\nprobaKNN.round(4) # round probabilities to four decimal places, if applicable","a5ed2d1d":"# make prediction of class label\npredKNN = classifierKNN.predict(newdata)\npredKNN","fae1f7cc":"### Answer for Question 3","1b3fc437":"### Answer for Question 2","c0ff1620":"### Question 1\nFrame the problem: Who is your client? What exactly is the client asking you to solve? How can you translate their ambiguous request into a concrete, well-defined problem?","9f1a9009":"### Answer for Question 4","674822a2":"# Flower Classification","429cd6bc":"### Question 7\nInterpreting Data:\nWe are at the final and most crucial step of a data science project, interpreting models and data. The predictive power of a model lies in its ability to generalise. How do we explain a model depends on its ability to generalise unseen future data.","5a9211db":"### Answer for Question 7","7a03dbc2":"### Question 4\nExplore Data:\nOnce your data is ready to be used, and right before you jump into AI and Machine Learning, you will have to examine the data.\n","9efdb670":"### Answer for Question 1","cd13be9b":"Conclusion : \n\nExploratory data analysis (EDA) is helpful in performing initial investigations on data before formal modeling and graphical representations, in order to discover patterns, look over assumptions, and test hypothesis.\n\nThe flower dataset contains 149 records for 4 predictors or independent or explanatory attributes, and an attribute for class label. The predictors all have continuous numerical data values and no missing data values. One of the main methods used to view these information is info(). \n\nThe predictors include 'sepal-length' which is sepal length in centimeter (cm) measurements, while 'sepal-width' is sepal width in cm. The attribute 'petal-length' is petal length in cm, while 'petal-width' is petal width in cm. There are 3 classes of flower species with qualitative discrete data values, where \u201csetosa\u201d has 50 instances, \u201cvirginica\u201d has 49, and \u201cversicolor\u201d has 50. This clearly illustrates that data available for the target class of \"virginica\" has slightly less proportion than that for the other two classes, which will be taken note of for further data visualisations and analysis later on. Since the data is labeled, supervised machine learning methods will be used to model the data.\n\ndescribe() is used to obtain summary statistics including measures of central tendency such as mean and median, and measures of dispersion such as standard deviation, which are useful in providing a quick and simple description of the dataset and its characteristics. Overall, \u2018petal-width\u2019 has the smallest values, followed by \u2018sepal-width\u2019, \u2018petal-length\u2019, then \u2018sepal-length\u2019 which is the largest.","425449f5":"# Thank you !","55c081a5":"### Answer for Question 6","7f06105d":"Conclusion : \n\nThe machine Learning algorithms of KNN, decision tree, and Na\u00efve Bayes, are chosen to fit to the dataset.\n\nThe dataset is split into two separate sets - the training set and test set. They both consist of the same attributes, but not the same attribute values. The training set is used to train and construct the classification models. The test set is used to predict the classifications of the \u201cnew\u201d unbiased data that were not used to train the model, before evaluating the model performance based on the performance metrics of accuracy, precision, recall, and F1-score of those classifications. \n\nThe target labels have uneven distribution, where \u201cversicolor\u201d has 38 instances, \u201cvirginica\u201d has 40, and \u201csetosa\u201d has 41. In order to ensure that the training and test sets are unbiased and representative of the three classes, the list of random numbers starting from the random selected position of 111 is used to perform random splitting. An accuracy graph is plotted to find the most accurate training set proportion, after taking the chosen Decision Tree, Naive Bayes, and KNN model scores into consideration. According to the graph, the training subset should take up 80% of the dataset which is 119 instances, whereas the test subset will take up 20% which is 30 instances.\n\nFor the KNN model, the optimal value of k number of nearest neighbours is found by plotting an accuracy graph, which identifies the optimal value of k as 7 in order to obtain 100% accuracy. This is done to obtain a k value that is large enough to minimise error rate and sensitivity to noise, but not too large such that the boundaries are over-smoothed or overfitted with points from the other classes. The chosen k value is also appropriate since 7 is not a multiple of the 3 classes, which is a requirement when selecting k value. The KNN model parameters are thus the value of k of 7, and Euclidean distance metric to compute the distance between data points. The output is the assigned class membership based on the majority vote on the data point\u2019s k number of neighbors.\n\nGaussian Na\u00efve Bayes is suitable for continuous data types, and the prior probabilities and likelihoods are computed in order to predict the posterior probability of a data point belonging to each of the three classes. The outputs are the aforementioned posterior probabilities, and the assigned class membership is selected as the class with the highest posterior probability.\n\nDecision tree is constructed based on parameters of best split strategy, and the criterion of entropy which utilises information gain to iteratively select the next node according to higher feature importance to optimise the quality of splits. The outputs are the classification rules. These are determined by the flow sequence from the root node and the corresponding branches to the internal or decision nodes, then stopping when the leaf node representing the class label is reached. A total of nine rules were extracted from the decision tree.","d6bec32b":"Conclusion : \n\nThe Flower species dataset originates from an online Kaggle data source. Version 1 of the dataset was used, and it was recently uploaded and updated by Lars Grygosch. The link to the Excel csv file that was extracted as at 31 October 2020 for the purpose of this assignment project is <https:\/\/drive.google.com\/file\/d\/1uM0orcGAIeOZAElZqsCwhukcbHGrWCgJ\/view?usp=sharing>. The dataframe format type will allow the use of a wider variety of syntax and methods for future data analysis, including describe() and info().\n\nThe dataset contains 151 rows\/records and 6 columns\/attributes, consisting of 4 independent numerical attributes as the predictors, and 1 target class label and 1 of its derivable attribute. \n\nThe attribute 'sepal-length' is sepal length in centimeter (cm) measurements, while 'sepal-width' is sepal width in millimeters (mm). The attribute 'petal-length' is petal length in cm, while 'petal-width' is petal width in cm. All of these have a quantitative discrete data type limited to one decimal place.\n\nThe dataset has 3 classes of Flower species - Setosa, Versicolor, and Virginica. These species names are represented by 'species', while 'species-id' is derived from it. The ID assigns a unique identification for each of the three 'species', where a value of 0 represents \"setosa\", 1 represents \"versicolor\", and 2 represents \"virginica\". The data type for 'species' is qualitative discrete, while that of 'species-id' is quantitative discrete.","cdf3bb3f":"Conclusion : \n\nThe main problem that is to be solved by the data science task must be properly framed, in terms of client's goals, background information, and purpose of task. This allows the data science task to be understood and explored to better inform the decision-making process of the possible range of approaches and solutions to the problem.\n\nThe client is a biologist, and her goal is to obtain a classification model on flower species. The problem is that she does not have any experience in data science or in constructing machine learning models. The background information is that, without accurate and reliable classifications, rare and expensive flower species may be sold at a lower price than its actual market value, and the florists that she plans on helping may end up making a loss instead. The aim of this assignment project is to design, develop, evaluate, and deploy an accurate classification model to help predict the flower species.","7f0628a8":"### Question 5\nModel Data:\nThis is the stage where most people consider interesting. As many people call it \u201cwhere the magic happens\u201d.","f9ddd198":"Conclusion : \n\nThe findings from this assignment project is that the ranking of correlation to the Flower species is, in decreasing order, petal-width, petal-length, sepal-length, then sepal-width. The interpretation is that petal-width is the best predictor of flower species, out of the four predictors.\n\nOther findings include the pattern of larger petal lengths and widths, as well as slightly larger sepal lengths, from Setosa to Versicolor to Virginica flower species. Setosa flower speciesalso has slightly larger sepal widths than the other two species. The interpretation is that Setosa flower heads are larger than the others, while that of Virginica is the smallest.\n\nThe KNN model with parameters of Euclidean distance metric and using 7 as the value of k, is chosen as the final model to predict flower species since it has the best performance metrics of 100% for all four metrics of accuracy, precision, recall, and F1-score. This strong prediction power was evaluated using the confusion matrix in relation to the testing set, in order to effectively test the model's ability to generalise unseen future data. On the other hand, the KNN model parameters were found to be most accurate after plotting accuracy graphs and choosing the parameter value that is able to provide the highest predictive accuracy. \n\nThe KNN model is now ready to be deployed to predict new value instances. To do so, a data frame is created to describe the characteristics of a number of flowers based on its sepal length and width as well as petal length and width. These new data instances will be passed to the KNN model classifier to predict its class label. A similar example is demonstrated above, using sepal length of 1, sepal width of 2, petal length of 3, and petal width of 3. The predicted class for the specified example is assigned as \"versicolor\", as its probability is the highest among that for the other classes of species. It is also safe to intepret this result as having 100% accuracy, precision, recall, and F1-score, based on the model's performance metrics.\n\nPossible improvements can be to include other strong predictors of flower species, such as leaf color, length and width. For instance, Setosa has medium green leaves with an average of 45cm in length and 1.5cm in width. Versicolor has blue-green leaves with an average of 46cm in length and 2.5cm in width. Virginica has bright green leaves with an average of 61cm in length and 2cm in width. Therefore, these three predictors are very relevant and can be useful to include in the model design.","3f0c1187":"For this assignment project, the client is a biologist that specialises in analysing the morphologic variations of different species of flowers. She wants to obtain an accurate classification model to help local florists to classify various flower species, but does not possess any data science background or knowledge on proper data collection and data analysis techniques. As a start, she collected 151 samples on the length and width of the flower sepals and petals for three popular Iris flower species. Version 1 of the dataset used was recently uploaded and updated by Lars Grygosch in Kaggle. \n\nThe Iris flower genus has close to 300 species of flowering plants with showy flowers in a rainbow of colours. The worth of each flower species usually depends on their colours, size, and seasonal changes, amongst other factors. These features differ for individual species, which makes classification important for biologists and florists alike. For instance, a wrong classification of a rare species as a more common species will cause a loss in monetary terms, and vice versa.\n\nThe purpose of this assignment project is to design, develop, evaluate, and deploy an accurate classification model to help predict the flower species. The primary focus is on size-related factors, which are the predictors \u2018sepal-length\u2019, \u2018sepal-width\u2019, 'petal-length', and \u2018petal-width\u2019, in order to assess and distinguish flower species. \n\nThis assignment will extract relevant, representative, and sufficient case study data from a reputable and reliable online source. Appropriate preprocessing adjustments and data exploration will be performed on the dataset to ensure reliable model outcomes and outputs. For the data mining and modelling process, the popular classifier models of k-Nearest Neighbours, Decision Tree, and Naive Bayes will be fitted, analysed, and evaluated in terms of the performance metrics of accuracy, precision, recall, and F1-score in predicting the classifications of species. All significant interpretations and observations will be noted and considered for future improvements.","00666ce4":"### Question 3\nScrub Data:\nAfter obtaining the data, the next immediate thing to do is scrubbing data. This process is for us to \u201cclean\u201d and to filter the data. Remember the \u201cgarbage in, garbage out\u201d philosophy, if the data is unfiltered and irrelevant, the results of the analysis will not mean anything.","ab5caac8":"Conclusion : \n\nData preprocessing has four main stages \u2013 data cleaning, data integration, data transformation, and data reduction. \n\nData cleaning will filter and handle dirty data to ensure quality data and quality mining results. In this case, there are noises of impossible and extreme values and outliers, and missing values. The errors are inconsistent data. \n\nInconsistency in 'sepal-width' mm measurements are converted to cm, for comparable numerical format and data type. Next, all instances of \u201csetsoa\u201d in 'species' are replaced with \u201csetosa\u201d, since the inconsistent values actually that have the same meaning.\n\nWinsorisation method is chosen to handle outliers, where outlier values are replaced with the minimum or maximum non-outlier value identified using the interquartile range (IQR) method. The acceptable value range is [Q1-1.5IQR, Q3+1.5IQR], where Q1 is the first quartile of 25 percentile, Q3 is the third quartile of 75 percentile, and IQR is (Q3 \u2013 Q1).\n\nThe impossible and extreme value of 103cm of petal length is assumed as an incorrect data entry with actual value of 10.3. It is identified as it differs from the mean attribute value by a comparatively large margin.\n\nTwo duplicated rows or records are dropped from the dataset, as this redundancy will cause overfitting of the chosen models.\n\nFinally, the last data cleaning task is to replacea the missing value of petal width with the mean value of petal width. This is the least biased method which preserves the variance and will not create random data, as the missing value is a very small number of 1 record which is only around 0.0067% of the total number of records.\n\nData integration is not needed, since only one dataset is used with no schema integrations, and thus no discernable entity identification issues or data value conflicts.\n\nData transformation will check overall range of values for the entire dataset. It is found that all values already fall under an acceptable small range of [0.1, 10], so there is no need for data transformation to scale the values into a comparable range for easy visualisations and modelling.\n\nData reduction involves dropping \u2018species-id\u2019 through attribute dimensionality reduction, since it a redundant derivable attribute with 100% correlation to \u2018species\u2019.\n\nA correlation matrix is used to list all the correlation coefficients in order to identify multicollinearity, in other words high intercorrelation above an absolute value of 0.5 between the predictors. For a pair of predictors with multicollinearity, one of them will be dropped since it would be redudant to include both of them. Another reason is to prevent model overfitting.\n\nThe correlation will compare and describe the linear connection and relationship between pairs of features, through the type of correlation and its strength. A positive correlation indicates that both features will change their values in the same direction, while a negative correlation indicates that both features will change their values in opposite directions. The larger the correlation strength, the stronger the connection and relationship. \n\nHowever, Decision Tree, KNN, and Naive Bayes models are chosen as most appropriate classification models, and they are all immune to multicollinearity. The first two are non-parametric models - Decision Tree only examines one of the features at a time during the splitting process, while KNN examines features all together. Na\u00efve Bayes assumes all features are conditionally independent. Due to these reasons, the only predictors that are considered to be dropped will be if their intercorrelations are above 0.95 and thus almost perfect descriptions of each other. It would be redundant to include both of them. Therefore, no attributes were removed as to not lose relevant information and degrade the overall model. \n\nPairs plot or scatterplot matrix are used to identify and remove attributes with weak class-attribute relationship. This is of most use and interest for lassification purposes. Scatter plots on the upper triangle will visualize the relationships between two variables. Kernel density estimate (KDE) plots, which will be discussed in further detail in the next assignment section, will illustrate the univariate distribution of a single variable in relation to the target variable. 2-D kernel density plots on the lower triangle will illustrate the density of single variable in relation to the target variable. All of them were analysed, and it was found that all features have acceptable class-attribute relationship with relatively distinguishable class boundaries as well as acceptable degree of overlapping or overplotting areas. Therefore, no attributes were removed as they are all able to allow relatively accurate predictions for classification purposes.","8e166afa":"Conclusion : \n\nThe model performance is evaluated by using the test set of 30 records to predict the classifications of these \u201cnew\u201d unbiased data that were not used to train the model. The confusion matrix is then used to determine the performance metrics of accuracy, precision, recall, and F1-score, based on those classifications. The supports are 9 instances for \"setosa\", 12 for \"versicolor\", and 9 for \"virginica\".\n\nKNN model has the best performance metrics of 100% for all four metrics of accuracy, precision, recall, and F1-score. The Decision Tree model and Na\u00efve Bayes model both have the same accuracy of 96.6667%, and 97% for all three of precision, recall, and F1-score. For all three chosen models, the species that was best classified is \"setosa\", followed by \"versicolor\", then \"virginica\" has the worst predictions out of all the species.\n\nAccuracy indicates the overall proportion of correct predictions for all the three classes. \n\nRecall indicates the proportion of correct predictions for each individual class, out of the corresponding actual class. In other words, the proportion of all actual classes that were predicted correctly. \n\nPrecision indicates the proportion of correct predictions for each individual class, out of the corresponding predicted class. In other words, the proportion of all predicted classes were actually predicted correctly.\n\nF1-score, also known as F-score or F-measure, is used to make precision and recall comparable in cases where they are both important, by measuring their harmonic mean. This allows it to consider both metrics and punish extreme values more heavily. F-score is more useful for biased datasets that are common in real-life scenarios. This occurs when the counts of FP and FN are very different, but they must still be properly considered since they are crucial conditions in the prediction. Therefore, F-score will compute the overall quality of translations produced by the chosen machine learning engine.\n\nIn conclusion, the KNN model using Euclidean distance metric and using 7 as the value of k, is chosen as the final model for the prediction of flower species.","39d657b2":"### Answer for Question 5","aa7849c8":"Conclusion : \n\nAnalysis is mostly focused on the relationship between the various flower features and the target feature which is flower species. This is because the classification purpose will be mostly interested in these types of correlation and their strengths, in order for accurate predictions. \n\nmatplotlib.pyplot graphics library is imported for the visualisations of figures. It is convenient as it has good reproducibility of scientific figures, for example when regenerating a figure using updated data, appearance, latex labels and texts, and aspects such as orientation. %matplotlib inline is used to configure the output of the figures to be embedded in the Jupyter notebook file, instead of opening a new window each time.\n\nData visualisation will include suitable graphs that are descriptive and comparative to effectively communicate both abstract and concrete ideas. Histogram density plots are chosen to illustrate the overall data distribution, as well as the data distributions of flower species based on the predictor features. Seaborn is used for a terser Application Programming Interface (API). \n\nKernel density estimate (KDE) plots, using the kde() method, will visualise the overall distribution through a continuous probability density curve. The distplot() method generates three histogram density plots and their continuous probability density curves for the three target feature values in the same figure space, and clearly differentiates them by specifying different RGB hex colour codes in its parameters. The xlim() method restricts plot range to 0 and above for meaningful visualisations, since it is impossible for the predictor features to have a negative value.\n\nAll the overall KDE distribution curves have negligible kurtosis and skewness, since the data instances available for each of the three classes are approximately the same. Petal lengths and widths have bimodal or multimodel data distribution as there are two distinct peaks also known as local maxima. Sepal lengths and widths have approximate normal data distribution as the curves are approximately symmetric, unimodal, asymptotic, and their mean, median, and mode are similar.\n\nThe histogram density plots and their respective highest point in the curves show the pattern of larger petal lengths and widths as well as slightly larger sepal lengths from Setosa to Versicolor to Virginica flower species. In addition, Setosa flower species has slightly larger sepal widths than the other two species.\n\nThe difference in the overlapping or overplotting histogram density plots clearly shows that the target class of strongly correlated features can be predicted more easily, and more useful meaning can be extracted. Its data points are less scattered and thus have less overlapping or overplotting areas, which means that they better follow their respective common relationship or pattern. It can thus be seen that the ranking of correlation to flower species, in decreasing order, is 'petal-width', 'petal-length', 'sepal-length', and 'sepal-width'. Petal width is thus the best predictor of flower species in this case.","9ab12740":"### Question 2\nObtain Data: \nThe first step of a data science project is to obtain the necessary data that we need from available data sources.","e6b9dfe3":"### Question 6\nEvaluation of the model:\nThe final model (or maybe the best 2-3 models) will then be put through the validation process. In this process, we will test the model using a completely new data set i.e. data that was not used to build the model. This process ensures that that our model is generalisable to new data and not just a useful model for the specific data used earlier. On a technical level, this is called \u2018avoiding over fitting\u2019.\n\n "}}