{"cell_type":{"3a51233b":"code","be9f4dd4":"code","eaf052a8":"code","88e0018c":"code","1b5e2060":"code","ff6dca1f":"code","c043fc0d":"code","e388c835":"code","de3cca9e":"code","05b1b0e4":"code","c213b9b4":"code","1a3cb8da":"code","14b815ae":"code","7ffd4faa":"code","20b06a94":"code","60a543b7":"code","35c1362d":"code","fa59df9d":"code","c8af8161":"code","37ebef38":"code","afae17a8":"code","6e18685a":"code","2520bff7":"code","0116a765":"code","19a7203f":"code","d96a4cf9":"code","bd3be297":"code","45cd43f7":"code","11960d97":"code","dcdb4cd1":"code","5d938018":"code","d47926b9":"code","38f6db6a":"code","39565276":"code","5beb33fb":"code","672a15ab":"code","1fee21ba":"code","1e82d57f":"code","c6f67856":"code","cc137ec5":"code","6a7cdaea":"code","7e06bd88":"code","a61634c8":"code","c6662bc2":"code","8fad0ed4":"code","f6b28b3b":"code","91b6febd":"code","1649fe7d":"code","d05be108":"code","4c82a8ea":"markdown","8a0170cd":"markdown","0dd4b5c5":"markdown","fa30480f":"markdown","f15dcf2f":"markdown","32e15d0b":"markdown","2405ac58":"markdown","1cc7c252":"markdown","27e0fb6f":"markdown","3cd4eaa6":"markdown"},"source":{"3a51233b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","be9f4dd4":"#The dimensionality reduction requires a large amount of calculation and runs slowly.but it works on reduce feature.\n#PCA compresses existing features, and the reduced-dimensional features are not any original features, \n#but new features combined in some way. Therefore, the dimensionality reduction algorithm represented by PCA is a kind of feature creation\n\n#pca\n\n\"\"\"\ncalss sklearn.decomposition.PCA(n_components=None,copy=True,whiten=False,scd_solver='auto',tol=0.0,iterated_power='auto',random_state=None)\n\n\"\"\"\n","eaf052a8":"import matplotlib .pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA","88e0018c":"iris=load_iris()\ny=iris.target\nx=iris.data","1b5e2060":"# 2-dimensional array, 4-dimensional matrix\nx.shape","ff6dca1f":"#pca\npca=PCA(n_components=2)\npca=pca.fit(x)#Fitting the model\nx_dr=pca.transform(x)#acquire new matrix\nx_dr.shape\n# x_dr=PCA(2).fit_transform(x)","c043fc0d":"x_dr[y==0,0]# acquire x1,when y==0 ## x_dr[:,0]","e388c835":"plt.figure()\nplt.scatter(x_dr[y==0,0],x_dr[y==0,1],c='red',label=iris.target_names[0])\nplt.scatter(x_dr[y==1,0],x_dr[y==1,1],c='black',label=iris.target_names[1])\nplt.scatter(x_dr[y==2,0],x_dr[y==2,1],c='orange',label=iris.target_names[2])\nplt.legend()\nplt.title('PCA of IRIS dataset')\nplt.show()","de3cca9e":"pca.explained_variance_ #Check the amount of information (explainable variance) of each new feature after dimensionality reduction","05b1b0e4":"#Interpretable variance contribution rate\npca.explained_variance_ratio_ #Check the percentage of information occupied by each new feature vector to the metadata information after dimensionality reduction","c213b9b4":"pca.explained_variance_ratio_.sum() # new 2 features contain 4 feature information","1a3cb8da":"import numpy as np\nnp.cumsum(pca.explained_variance_ratio_) # Cumulative result","14b815ae":"pca_line=PCA().fit(x)\nplt.plot([1,2,3,4],np.cumsum(pca_line.explained_variance_ratio_))\nplt.xticks([1,2,3,4])#Restrict the axes to be integers\nplt.xlabel('number of components after dimension reduction')\nplt.ylabel('cumculative explained variance')\nplt.show()","7ffd4faa":"# Choose the most suitable n_component by itself ,but it takes time\npca_mle=PCA(n_components='mle').fit(x)\nx_mle=pca_mle.transform(x)\npca_mle.explained_variance_ratio_.sum()","20b06a94":"# Select hyperparameters according to the proportion of information\npca_f=PCA(n_components=0.97,svd_solver='full')# if data size is too large to calculate, please use svd_solver='randomized'\npca_f=pca_f.fit(x)\nx_f=pca_f.transform(x)\npca_f.explained_variance_ratio_","60a543b7":"from sklearn.datasets import fetch_lfw_people # Face recognition data","35c1362d":"# import data\nfaces=fetch_lfw_people(min_faces_per_person=60)","fa59df9d":"faces.data.shape","c8af8161":"faces.images.shape\n# 1348 is the number of image matrix\n# 62 is the row number of image matrix\n# 47 is the columns number of image matrix","37ebef38":"X=faces.data # estimator expected <=2","afae17a8":"#plot image pic\nfig,axes=plt.subplots(3,8\n                      ,figsize=(8,4)\n                      ,subplot_kw={\"xticks\":[],\"yticks\":[]}# not show the axis\n                     )\n#axes[0][0].imshow(faces.images[0,:,:]) # the image pic of index 0\nfor i,ax in enumerate(axes.flat):\n    ax.imshow(faces.images[i,:,:],cmap=\"gray\")","6e18685a":"# Modeling dimensionality reduction\npca=PCA(150).fit(X)\nv=pca.components_\nv.shape","2520bff7":"fig,axes=plt.subplots(3,8\n                      ,figsize=(8,4)\n                      ,subplot_kw={\"xticks\":[],\"yticks\":[]}# not show the axis\n                     )\nfor i,ax in enumerate(axes.flat):\n    ax.imshow(v[i,:].reshape(62,47),cmap=\"gray\")","0116a765":"faces=fetch_lfw_people(min_faces_per_person=60)\nfaces.images.shape\n\nx=faces.data","19a7203f":"faces.data.shape","d96a4cf9":"pca=PCA(150)\nx_dr=pca.fit_transform(x)\nx_dr.shape","bd3be297":"x_inverse=pca.inverse_transform(x_dr)\nx_inverse.shape","45cd43f7":"# We compare the original image with the restored image after dimensionality reduction\nfig,ax=plt.subplots(2,10,figsize=(10,2.5)\n                   ,subplot_kw={\"xticks\":[],\"yticks\":[]}\n                   )\nfor i in range(10):\n    ax[0,i].imshow(faces.images[i,:,:],cmap='binary_r')\n    ax[1,i].imshow(x_inverse[i].reshape(62,47),cmap='binary_r')\nax","11960d97":"from sklearn.datasets import load_digits\ndigts=load_digits()\ndigts.data.shape","dcdb4cd1":"digts.data","5d938018":"digts.images.shape","d47926b9":"set(digts.target.tolist())","38f6db6a":"def plot_digts(data):\n    # The structure of data must be (m, n) and n must be divided into (8, 8)\n    fig,axes=plt.subplots(4,10,figsize=(10,4)\n                   ,subplot_kw={\"xticks\":[],\"yticks\":[]}\n                   )\n    for i,ax in enumerate(axes.flat):\n        ax.imshow(data[i].reshape(8,8),cmap='binary')","39565276":"plot_digts(digts.data)","5beb33fb":"# add noise\nnp.random.RandomState(42)\nnoisy=np.random.normal(digts.data,2) #Sampling from the digit data set,and the selected samples meet the normal distribution ,variance=2\nplot_digts(noisy)","672a15ab":"# Noise reduction\npca=PCA(0.5,svd_solver='full').fit(noisy)# The data after dimensionality reduction contains more than 50% of the original data information\nx_dr=pca.transform(noisy)\nx_dr.shape","1fee21ba":"without_noisy=pca.inverse_transform(x_dr)\nwithout_noisy.shape","1e82d57f":"plot_digts(without_noisy)","c6f67856":"from sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np","cc137ec5":"data=pd.read_csv('..\/input\/mnist-digit-recognizer\/train.csv')\ndata.shape","6a7cdaea":"X=data.iloc[:,1:]\ny=data.iloc[:,0]\nprint(X.shape,y.shape)","7e06bd88":"pca_line=PCA().fit(X)\nplt.figure(figsize=[20,5])\nplt.plot(np.cumsum(pca_line.explained_variance_ratio_))\nplt.xlabel('number of components after dimension reduction')\nplt.ylabel('cumulative explained variance ratio')\nplt.show()\n# so 50 features contain almost 80% information","a61634c8":"score=[]\nfor i in range(1,101,10):\n    x_dr=PCA(i).fit_transform(X)\n    once=cross_val_score(RFC(n_estimators=10,random_state=0)\n                        ,x_dr,y,cv=5).mean()\n    score.append(once)\nplt.figure(figsize=[20,5])\nplt.plot(range(1,101,10),score)\nplt.show()\n# when n_compoment==20 we can accquire over 0.9 score","c6662bc2":"score=[]\nfor i in range(10,25):\n    x_dr=PCA(i).fit_transform(X)\n    once=cross_val_score(RFC(n_estimators=10,random_state=0)\n                        ,x_dr,y,cv=5).mean()\n    score.append(once)\nplt.figure(figsize=[20,5])\nplt.plot(range(10,25),score)\nplt.show()\n# when n_compoment==21, we got the max score","8fad0ed4":"x_dr=PCA(21).fit_transform(X)\ncross_val_score(RFC(n_estimators=10,random_state=0),x_dr,y,cv=5).mean()","f6b28b3b":"# import n_estimators to 100 ,to see whether the score improve\ncross_val_score(RFC(n_estimators=100,random_state=0),x_dr,y,cv=5).mean()","91b6febd":"# Test on KNN model\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\ncross_val_score(KNN(),x_dr,y,cv=5).mean()","1649fe7d":"# find the best k of KNN\nscore=[]\nfor i in range(10):\n   # x_dr=PCA(21).fit_transform(X)\n    once=cross_val_score(KNN(i+1),x_dr,y,cv=5).mean()\n    score.append(once)\nplt.figure(figsize=[20,5])\nplt.plot(range(10),score)\nplt.show()\n# we can find when k==3 ,we can get best score","d05be108":"%%timeit\ncross_val_score(KNN(3),x_dr,y,cv=5).mean()","4c82a8ea":"## 'mle'","8a0170cd":"* After face recognition of ID card information being scanned, a lot of information has been stored.\n* However,we can only extract less information  by dimensionality reduction,\n* and then a small amount of information will be used for comparison with real people,\n* Even so it can also have a high matching accuracy","0dd4b5c5":"#### learning curve  after dimensionality reduction","fa30480f":"#### Draw the cumulative variance contribution rate curve, find the best dimension range after dimensionality reduction","f15dcf2f":"# inverse_transform","32e15d0b":"### SVD and\uff08Principal component analysis\uff09 PCA belongs to the entry algorithm in matrix algorithm\n*PCA uses information measurement indicators. The larger the variance of the sample, the more information the feature carries","2405ac58":"# Use PCA for noise filtering","1cc7c252":"#### import data","27e0fb6f":"* Because the data after dimensionality reduction is upgraded again, \n  it cannot make up for the information lost in the process of dimensionality reduction, so the image is vague","3cd4eaa6":"# record_newcase"}}