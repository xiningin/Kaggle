{"cell_type":{"12a73ada":"code","a15b4657":"code","070ebca0":"code","665b5037":"code","e3c45380":"code","fb7bffda":"code","5c14a2ff":"code","569729a6":"code","a2bd1b4c":"markdown","e41362d7":"markdown","cbe7e740":"markdown","ed61e0c4":"markdown","4e056e38":"markdown","18975584":"markdown","2e836a27":"markdown","878cb542":"markdown","35abf643":"markdown","8f6c6a08":"markdown"},"source":{"12a73ada":"!git clone https:\/\/github.com\/huckiyang\/speech_quantum_dl\n%cd speech_quantum_dl","a15b4657":"!pip install PennyLane\n!pip install qiskit","070ebca0":"## get mel (*.npy converted from datasets\/*\/*.wav)\n#!python main_qsr.py --mel 1 --quanv 1 --dataroot '\/kaggle\/input\/google-speech-commands'\n\n## get mel (data_quantum\/*.npy)\n!python main_qsr.py","665b5037":"!ls -l checkpoints","e3c45380":"!python cam_sp.py","fb7bffda":"from IPython.display import Image\nImage('images\/cam_sp_0.png')","5c14a2ff":"%cd data_quantum\/asr_set\n!bash unzip.sh\n%cd ..\/..","569729a6":"!python qsr_ctc_wer.py","a2bd1b4c":"### QCNN U-Net Bi-LSTM Attention Model\n![](https:\/\/github.com\/huckiyang\/speech_quantum_dl\/raw\/main\/images\/QCNN_Sys_ASR.png)","e41362d7":"# CTC model with Word Error Rate(WER) evaluation","cbe7e740":"### For GPU, helper_q_tool.py uncomment line #13 ","ed61e0c4":"## Paper: [Decentralizing Feature Extraction with Quantum Convolutional Neural Network for Automatic Speech Recognition](https:\/\/arxiv.org\/abs\/2010.13309)","4e056e38":"![image.png](attachment:image.png)","18975584":"## Dataset: Google Speech Commands dataset v0.0.1","2e836a27":"## Train Model","878cb542":"### saved checkpoints","35abf643":"## Repro [Github](https:\/\/github.com\/huckiyang\/speech_quantum_dl)","8f6c6a08":"# Neural Saliency by Class Activation Mapping (CAM)"}}