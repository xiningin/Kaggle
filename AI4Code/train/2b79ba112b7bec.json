{"cell_type":{"26a0aefc":"code","3d6265f2":"code","380ac150":"code","c14707fa":"code","8453fee9":"code","b7dbef3f":"code","c90dd88e":"code","e824445c":"code","be289f90":"code","36d36f0b":"code","5f2b1bf5":"code","5fbf3d12":"code","3f011667":"code","73d9a4a2":"code","29f85381":"code","db5f7b8e":"code","8e6de210":"code","86920169":"code","bdc21868":"code","c801c18f":"code","cb731ca0":"code","60c28011":"code","fe3b4b8a":"code","3698bddc":"code","d290b024":"code","60e7219c":"code","9025db53":"code","10ced725":"code","9fdd545a":"code","de4c04a2":"code","09b6b8a4":"code","caa93643":"code","21a4d52e":"code","a79d9016":"code","0f1c3cc4":"code","433f4754":"code","8886a92b":"code","9ac70200":"code","b51e4e92":"code","f9ae0888":"code","663b0829":"code","4e8e53b0":"code","d9a733df":"code","01e26bcc":"code","c97ca55b":"code","288e7139":"code","8ca84e34":"code","7dce9315":"code","94ec9e85":"code","3285d313":"code","e32a97e1":"code","0cdfbaee":"code","b867dfcf":"code","a05efe69":"code","2267c117":"code","3a375f79":"code","274cd6b1":"code","7c1aecc2":"code","893f099f":"code","6d38086c":"code","a45217d8":"code","e67f41cb":"code","9473ecf0":"code","25e862c5":"code","b8931063":"code","c3c379a7":"code","b716a2e1":"code","6e3104a0":"code","a62e35b9":"code","171774e6":"code","c2241792":"code","1438aef3":"code","d29c4026":"code","91498376":"code","f9ad3933":"code","ea42458e":"code","b1e85a80":"code","0a375167":"code","469cc499":"code","da1a1a42":"markdown","01491457":"markdown","6a95e4b1":"markdown","61d69720":"markdown","cd02aabe":"markdown","517a5156":"markdown","53d67744":"markdown","2d34d749":"markdown","f920ebfc":"markdown","eed9d873":"markdown","cca16323":"markdown","93762bee":"markdown","ac948ba2":"markdown","625686e8":"markdown","ae8c60c8":"markdown","f3bfd8fc":"markdown","76b6a586":"markdown","88699e39":"markdown","57838c9f":"markdown","1c2fb8d5":"markdown","cf2fa428":"markdown","2d516ba8":"markdown","5b1c0a23":"markdown","7ce1a4b4":"markdown","04611aeb":"markdown","1ceb8a2c":"markdown","eba12b50":"markdown","0b533be5":"markdown","ee3c813a":"markdown","0d573349":"markdown","224b5056":"markdown","46ac8424":"markdown","2b46629e":"markdown","9a2e5958":"markdown","81b5a33f":"markdown","c9df9f50":"markdown","78130d99":"markdown","e1d88d60":"markdown","9d33ff5b":"markdown","9b676a11":"markdown","c1ee25e6":"markdown","42850a83":"markdown","1a2beb5e":"markdown","9b7e76c9":"markdown","479569a5":"markdown","0ad50ba0":"markdown","b33f9f2f":"markdown","0c36d54d":"markdown","0ac9f402":"markdown","28a21f87":"markdown","f6c9922d":"markdown","03ffa1ef":"markdown","7f7172c7":"markdown","1a26744a":"markdown","9c0e0fb9":"markdown","29fef801":"markdown","9adca071":"markdown","5067143d":"markdown","8305e066":"markdown","4bb1d3c8":"markdown","3cf5dcad":"markdown","e614842b":"markdown","fa10145b":"markdown","d598df0c":"markdown","f4bea13e":"markdown","e41c3dbd":"markdown","3dbe0d88":"markdown","98d6677e":"markdown","cdc19b49":"markdown","b3977619":"markdown","291b494c":"markdown","d3ab1198":"markdown","000cce54":"markdown","11f7517d":"markdown","1fa6a3e5":"markdown"},"source":{"26a0aefc":"import numpy as np\nimport pandas as pd\nimport pickle\n\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.max_open_warning': 0})\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')","3d6265f2":"%%time\ntrain_pickle_file = '\/kaggle\/input\/pickling\/train.csv.pandas.pickle'\ntrain_data = pickle.load(open(train_pickle_file, 'rb'))\ntrain_data.info()","380ac150":"train_data['action'] = (train_data['resp'].values > 0).astype('int')\ntrain_data['profit'] = train_data['resp'].multiply(train_data['weight'])","c14707fa":"plt.plot(train_data.groupby('date').feature_41.nunique())","8453fee9":"nb_opportunities = train_data.query('date==0').feature_41.shape[0]\ncount_f41 = train_data.query('date==0').feature_41.value_counts()\nnb_stock = count_f41.values.shape[0]\nprint('number of opportunities: ' + str(nb_opportunities))\nprint('number of stocks: ' + str(nb_stock))\nprint('number of stocks accounting for at least 50% of trades: ' + str(np.argmax(count_f41.cumsum().values\/nb_opportunities > 0.5)))\nplt.plot(count_f41.cumsum().values\/nb_opportunities)","b7dbef3f":"nth_day = 0\ntrain_data_fd = train_data.query('date=='+str(nth_day)).copy()\n\nnth_most_common = 0\nvalue = train_data_fd.feature_41.value_counts().index[nth_most_common]\ndf_md = train_data_fd[train_data_fd.feature_41 == value]","c90dd88e":"def plot_cols(data, columns):\n    for i in columns:\n        fig, ax = plt.subplots()\n        colors = {0:'red', 1:'blue'}\n        markers = {-1:'x', 1:'o'}\n\n        x = data.index\n        y = data[i]\n        c = data['action'].map(colors)\n        m = data['feature_0'].map(markers)\n\n        unique_markers = set(m)  # or yo can use: np.unique(m)\n\n        for um in unique_markers:\n            mask = m == um \n            # mask is now an array of booleans that can be used for indexing  \n            ax.scatter(x[mask], y[mask], c=c[mask], marker=um)\n            ax.set_title(str(i))\n\n        plt.show()","e824445c":"#plot_cols(df_md, df_md.columns)","be289f90":"plot_cols(df_md, ['date','weight','resp_1','resp_2','resp_3','resp_4','resp','ts_id','action','profit'])","36d36f0b":"df_md_f0 = df_md.multiply(df_md['feature_0'], axis=0)\ndf_md_f0['action'] = df_md['action']\ndf_md_f0['ts_id'] = df_md['ts_id']\ndf_md_f0['feature_0'] = df_md['feature_0']\ndf_md_f0['weight'] = df_md['weight']\ndf_md_f0['profit'] = df_md['profit']","5f2b1bf5":"plot_cols(df_md_f0, ['resp_1','resp_2','resp_3','resp_4','resp'])","5fbf3d12":"plot_cols(df_md, ['feature_1','feature_2'])","3f011667":"def plot_scatter(data, columns1, columns2):\n\n    fig, ax = plt.subplots()\n    colors = {0:'red', 1:'blue'}\n    markers = {-1:'x', 1:'o'}\n\n    x = data[columns1]\n    y = data[columns2]\n    c = data['action'].map(colors)\n    m = data['feature_0'].map(markers)\n\n    unique_markers = set(m)  # or yo can use: np.unique(m)\n\n    for um in unique_markers:\n        mask = m == um \n        # mask is now an array of booleans that can be used for indexing  \n        ax.scatter(x[mask], y[mask], c=c[mask], marker=um)\n        ax.set_title(columns2 + ' v.s. ' + columns1)\n\n    plt.show()","73d9a4a2":"plot_scatter(df_md, 'feature_1', 'feature_2')","29f85381":"plt.plot((df_md['feature_2'].divide(df_md['feature_1']).clip(-5,5)))","db5f7b8e":"plt.plot((df_md['feature_2'].divide(df_md['feature_1']+1e-5).clip(-5,5)))","8e6de210":"min_f = 3\nmax_f = 8\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md, f)","86920169":"min_f = 3\nmax_f = 8\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md_f0, f)","bdc21868":"plot_scatter(df_md, 'feature_3', 'feature_4')\nplot_scatter(df_md, 'feature_3', 'feature_5')\nplot_scatter(df_md, 'feature_3', 'feature_6')","c801c18f":"plot_scatter(df_md_f0, 'feature_3', 'feature_4')\nplot_scatter(df_md_f0, 'feature_3', 'feature_5')\nplot_scatter(df_md_f0, 'feature_3', 'feature_6')","cb731ca0":"plot_scatter(df_md, 'feature_7', 'feature_8')\nplot_scatter(df_md_f0, 'feature_7', 'feature_8')","60c28011":"min_f = 9\nmax_f = 16\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md, f)","fe3b4b8a":"plot_scatter(df_md, 'feature_9', 'feature_10')\nplot_scatter(df_md, 'feature_11', 'feature_12')\nplot_scatter(df_md, 'feature_13', 'feature_14')\nplot_scatter(df_md, 'feature_15', 'feature_16')","3698bddc":"min_f = 17\nmax_f = 38\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md, f)","d290b024":"min_f = 17\nmax_f = 38\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md_f0, f)","60e7219c":"for i in range(17, 39, 2):\n    plot_scatter(df_md, 'feature_'+str(i), 'feature_'+str(i+1))","9025db53":"min_f = 39\nmax_f = 40\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md, f)","10ced725":"min_f = 39\nmax_f = 40\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md_f0, f)","9fdd545a":" plot_scatter(df_md, 'feature_39', 'feature_40')","de4c04a2":"plot_scatter(df_md, 'feature_39', 'feature_3')\nplot_scatter(df_md, 'feature_40', 'feature_3')","09b6b8a4":"min_f = 41\nmax_f = 45\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md, f)","caa93643":"min_f = 46\nmax_f = 54\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md, f)","21a4d52e":"plot_scatter(df_md, 'feature_46', 'feature_47')\nplot_scatter(df_md, 'feature_46', 'feature_48')\nplot_scatter(df_md, 'feature_46', 'feature_49')\nplot_scatter(df_md, 'feature_46', 'feature_50')","a79d9016":" plot_scatter(df_md, 'feature_50', 'feature_51')","0f1c3cc4":"df_md['feature_51-50']=df_md['feature_51']-df_md['feature_50']\nplot_cols(df_md, ['feature_51-50'])","433f4754":" plot_scatter(df_md, 'feature_46', 'feature_52')","8886a92b":"plot_scatter(df_md, 'feature_46', 'feature_53')\nplot_scatter(df_md, 'feature_53', 'feature_54')","9ac70200":"plot_cols(df_md,['feature_55'])","b51e4e92":"min_f = 56\nmax_f = 59\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md, f)","f9ae0888":"plot_scatter(df_md, 'feature_55', 'feature_56')\nplot_scatter(df_md, 'feature_55', 'feature_57')\nplot_scatter(df_md, 'feature_55', 'feature_58')\nplot_scatter(df_md, 'feature_55', 'feature_59')","663b0829":"plt.scatter(df_md['feature_55'],df_md['feature_55'].rolling(10).mean(), c = df_md['action'].map({0:'red', 1:'blue'}))","4e8e53b0":"min_f = 60\nmax_f = 63\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md, f)","d9a733df":"df_md['feature_60_jump'] = df_md['feature_60'].diff()\nplot_cols(df_md, ['feature_60_jump'])","01e26bcc":"plot_scatter(df_md, 'feature_60', 'feature_61')\nplot_scatter(df_md, 'feature_62', 'feature_63')","c97ca55b":"df_md['feature_61d60']=df_md['feature_61'].divide(df_md['feature_60'])\nplot_cols(df_md, ['feature_61d60'])","288e7139":"df_md['feature_63d62']=df_md['feature_63'].divide(df_md['feature_62'])\nplot_cols(df_md, ['feature_63d62'])","8ca84e34":"plot_cols(df_md,['feature_64'])","7dce9315":"plot_scatter(df_md, 'feature_64', 'feature_60')\nplot_scatter(df_md, 'feature_64', 'feature_61')\nplot_scatter(df_md, 'feature_64', 'feature_62')\nplot_scatter(df_md, 'feature_64', 'feature_63')","94ec9e85":"plot_cols(df_md, ['feature_64','feature_67','feature_68'])","3285d313":"df_md['feature_68-64']=df_md['feature_68']-df_md['feature_64']\nplot_cols(df_md, ['feature_68-64'])","e32a97e1":"min_f = 65\nmax_f = 66\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md, f)","0cdfbaee":"plot_scatter(df_md, 'feature_66', 'feature_65')","b867dfcf":"df_md['feature_66d65']=df_md['feature_66'].divide(df_md['feature_65'])\nplot_cols(df_md, ['feature_66d65'])","a05efe69":"df_md['feature_64-60']=df_md['feature_64']-df_md['feature_60']\nplot_cols(df_md, ['feature_64-60'])\nplot_cols(df_md, ['feature_65'])","2267c117":"plot_scatter(df_md, 'feature_64-60', 'feature_65')","3a375f79":"min_f = 69\nmax_f = 71\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md, f)","274cd6b1":"min_f = 53\nmax_f = 54\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md, f)","7c1aecc2":"plot_scatter(df_md, 'feature_53', 'feature_69')\nplot_scatter(df_md, 'feature_53', 'feature_70')\nplot_scatter(df_md, 'feature_53', 'feature_71')\n\nplot_scatter(df_md, 'feature_54', 'feature_69')\nplot_scatter(df_md, 'feature_54', 'feature_70')\nplot_scatter(df_md, 'feature_54', 'feature_71')","893f099f":"df_md['feature_71-70']=df_md['feature_71']-df_md['feature_70']\nplot_cols(df_md, ['feature_71-70'])","6d38086c":"plt.scatter(df_md['feature_71-70'],df_md['feature_69'], c = df_md['action'].map({0:'red', 1:'blue'}))","a45217d8":"df_md['feature_71+70']=df_md['feature_71']+df_md['feature_70']\nplot_cols(df_md, ['feature_71+70'])","e67f41cb":"plt.scatter(df_md['feature_71+70'],df_md['feature_54'], c = df_md['action'].map({0:'red', 1:'blue'}))","9473ecf0":"min_f = 72\nmax_f = 77\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md, f)","25e862c5":"plot_cols(df_md, ['feature_77','feature_72','feature_76','feature_75','feature_74','feature_73'])","b8931063":"a = (df_md['action'] -0.5)*2\n\ndf_md_a = df_md.multiply(a, axis=0)\ndf_md_a['action'] = df_md['action']\ndf_md_a['ts_id'] = df_md['ts_id']\ndf_md_a['feature_0'] = df_md['feature_0']\ndf_md_a['weight'] = df_md['weight']\ndf_md_a['profit'] = df_md['profit']\n\nplot_cols(df_md_a, ['feature_77','feature_72','feature_76','feature_75','feature_74','feature_73'])","c3c379a7":"plot_scatter(df_md, 'feature_72', 'feature_77')\nplot_scatter(df_md, 'feature_73', 'feature_76')","b716a2e1":"min_f = 72\nmax_f = 77\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md, f)","6e3104a0":"min_f = 78\nmax_f = 83\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md, f)","a62e35b9":"for i in range(72,78):\n    plot_scatter(df_md, 'feature_'+str(i), 'feature_'+str(i+6))","171774e6":"plot_cols(df_md, ['feature_72','feature_78','feature_84','feature_90','feature_102','feature_108','feature_114','feature_120','feature_121'])","c2241792":"for i in range(72,114,6):\n     plot_scatter(df_md, 'feature_'+str(i), 'feature_'+str(i+6))","1438aef3":"min_f = 120\nmax_f = 129\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md, f)","d29c4026":"min_f = 120\nmax_f = 129\n\nf = ['feature_'+str(i) for i in range(min_f,max_f+1)]\n\nplot_cols(df_md_f0, f)","91498376":"plot_scatter(df_md, 'feature_120', 'feature_121')\ndf_md['feature_121-120']=df_md['feature_121']-df_md['feature_120']\nplot_cols(df_md, ['feature_121-120'])","f9ad3933":"plot_scatter(df_md, 'feature_122', 'feature_123')\ndf_md['feature_123-122']=df_md['feature_123']-df_md['feature_122']\nplot_cols(df_md, ['feature_123-122'])","ea42458e":"plot_scatter(df_md, 'feature_124', 'feature_125')\ndf_md['feature_125-124']=df_md['feature_125']-df_md['feature_124']\nplot_cols(df_md, ['feature_125-124'])","b1e85a80":"plot_scatter(df_md, 'feature_126', 'feature_127')\ndf_md['feature_127-126']=df_md['feature_127']-df_md['feature_126']\nplot_cols(df_md, ['feature_127-126'])","0a375167":"plot_scatter(df_md, 'feature_128', 'feature_129')\ndf_md['feature_129-128']=df_md['feature_129']-df_md['feature_128']\nplot_cols(df_md, ['feature_129-128'])","469cc499":"plt.scatter(df_md['feature_129-128'],df_md['feature_127-126'], c = df_md['action'].map({0:'red', 1:'blue'}))","da1a1a42":"More surprinsingly they seems to be really close to something we already saw : feature 3 to 6 :","01491457":"<a id='features_39_40'><\/a>\n### Feature 39 to 40\n\ntwo odds features that don't resemble previous or next ones. We just got ou of feature_6 tag.","6a95e4b1":"The features are related, the difference (or ratio) is unclear, but seems to be interesting for feature 60-61, a bit less for feature 62-63 as it only take two values.\nMaybe they should be investgated trough mutliple days.","61d69720":"pretty much the same thing if you ask me. Main point is that we might use some time series tools here.\n\nLet's have a look at corresponding values (feature number +6) :","cd02aabe":"(This doesn't seems to hold at all when changing day or stock wtf ?)","517a5156":"It seems a bit bad because we have very small values around 0 that create spikes (or maybe I ma doing the division wrong).\nIn this discussion https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/214321 LinDada proposes the following modification :","53d67744":"Main difference seems to be a single trade... what would that mean ?","2d34d749":"There are two notable execeptions :\n- Features 51 looks like the others but drunk. I suspect that feature 51 is feature 50 multiplied by something. Would that something make sense ?\n- feature 52 look like symmetrized. Can we find its relationship to others ?","f920ebfc":"So we have an alternating pattern of straight line and more chaotic ones, depending on tags. I think we need to investigate, notably if those pattern hold for other days \/ stocks.","eed9d873":"# Loading data \n\nLoading a pickle file. Check this notebook [pickling](https:\/\/www.kaggle.com\/quillio\/pickling) if you haven't pickled your data set yet. Check this notebook [one liner to halve your memory usage](https:\/\/www.kaggle.com\/jorijnsmit\/one-liner-to-halve-your-memory-usage) if you want to reduce memory usage before pickling.","cca16323":"## Features\n\n- [Main features](#Main_features)\n- [Features 1-2](#features_1_2)\n- [Features 3-8](#features_3_8)\n- [Features 9-16](#features_9_16)\n- [Features 17-38](#features_17_38)\n- [Features 39-40](#features_39_40)\n- [Features 41-45](#features_41_45)\n- [Features 46-54](#features_46_54)\n- [Features 55-59](#features_55_59)\n- [Features 60-68](#features_60_68)\n- [Features 69-71](#features_69_71)\n- [Features 72-119](#features_72_119)\n- [Features 120-129](#features_120_129)","93762bee":"They are definitely related. It seems that only one point (in blue this time) is outside from the line. What could that be ?","ac948ba2":"Looks like we could do time series here. Very similar features both in terms of normal feature and features multiplied by f0 : ","625686e8":"Oddly symetrical with respect to feature 0.","ae8c60c8":"Those feature look quite similar. May be with some exception for feature 51 - 52. Let's have a look at them.","f3bfd8fc":"(Note : as of now the relationship appears to be mixed when changing day : when using day one instead, it seems that the role of some features are inverted between which ones are the 'stepwise' and which one are the 'minutes').","76b6a586":"Nothing too surprising here. But already see that feature_0 have a lot of importance for the problem. so we define a second dataframe multiplied by feature 0 (except for the main data so as not to get weird results).","88699e39":"# Isolate the most common underlying of the first day\nYou can esaily change the date and the number of the stock if you want to participate in the exploration effort. ","57838c9f":"<a id='number_trades'><\/a>\n# Study the number of trades a day\nFor each date, isolate the number of unique values for feature 41.","1c2fb8d5":"is it related to feature 60 - feature 63 ? I am under the impression that the first group of point trought the feature 64 correspond to the one in feature 62-63.","cf2fa428":"That look like different time series starting from 77 to 72 and others... maybe if we shuffle a bit the order ?","2d516ba8":"May be that how they relate to feature 53 and 54 ?","5b1c0a23":"<a id='features_72_119'><\/a>\n### Feature 72 to 119\nThat a good amount of features. From tag it seems that there is a 6 features repeating pattern. We will try to investigate that pattern on feature 72 to 77 and apply it to other features.","7ce1a4b4":"<a id='features_41_45'><\/a>\n### feature 41 to 45\nThose feature are mostly constant over the day. Those are the ones that are used to find stock intraday.\nThere is not much to do with those intra-day. But there are still some questions to be answered imo.\nNotably, if there are some discrepancies between them. Or maybe their ratio ?","04611aeb":"Some plotting function that will make our life easier. The color show if we need to take action or not, the marker reflect feature 0.","1ceb8a2c":"<a id='features_60_68'><\/a>\n### Feature 60 to 68\nThings are getting weirder with feature 60 to 63. They have this stepwise aspect. Their proximity with feature 64 made me wonder if they are time related but I can't see anything obvious. Notably because their behavior seems to change when stocks are changing.\nFeature 64 is believed to be time. Let's see if we can see something more.","eba12b50":"There seems to be some link between feature 53, 54 and the previous ones. 53 and 54 oblivously have a link between them.","0b533be5":"There seems to be some pattern but not sure what.","ee3c813a":"Maybe feature 60-63 are group of time, I suspect they indicate fairly different market conditions such as pre-market (feature 62-63). Maybe feature 60-61 are the complete day cycle in term of market phase like [pre market](https:\/\/www.investopedia.com\/terms\/p\/premarket.asp) \/[after hours trading](https:\/\/www.investopedia.com\/terms\/a\/afterhourstrading.asp) \/[extended session](https:\/\/www.investopedia.com\/terms\/e\/extended_trading.asp).","0d573349":"the difference is not clear ... yet ?","224b5056":"<a id='Main_features'><\/a>\n# Main features","46ac8424":"# Loading base packages\n\nNothing fancy here : just basic packages and removal of annoying warnings","2b46629e":"Not very clear what we are dealing with. But as feature_55 is a time series. I suspect that the pattern we can see in feature 57, 58 of trails are indicative of some sort of lag. See below a lagged feature 55 against itself that exhibit similar trails than the plot of 55 v.s. 57. The main problem is that the lag used is probably expressed in real time and not in ticks. I haven't found a proper way to lag variables acocunting for real time (feature 64 ?). Feel free to comment if you know something that work properly.","9a2e5958":"<a id='Conclusion'><\/a>\n### Conclusion\n\nCongrats for reaching the end of the notebook ! \n\nThere seems to be a lot of patterns in the data. If we undertsand them we might be able to build better features, then get better scores. \nI am specifficaly intrigued by the multiples 'lines' we can see above. How would they matter ? are the difference between consecutives values informatives ? or are they some artefact of an anonymisation process ? It seems that when changing the day \/ stock we get different lines. What info lies in the evolution of the form of the lines trough the days ?\n\nI think we should probably start by looking at other days and stocks. Feel free to pick a day at random, some common stock and look if the relationships holds.","81b5a33f":"# Building Target\n\nNothing fancy here. I try to keep the raw result (resp * weight) as it might be interesting.","c9df9f50":"<a id='features_46_54'><\/a>\n### Feature 46 to 54","78130d99":"They appear to have very 'symmetric' behavior towards features 0 (look at dots versus crosses). So we can have a look at the features multiplied by feature_0.","e1d88d60":"pretty much symmetric in f_0:","9d33ff5b":"The other link we have are trough tag 0-5, basically feature 72 is linked with features 72,78,84,90,96,102,108,114. We might even add 120 and 121 that do not follow the previsou 6 patter but are on a 12 patterns trough tag 0-5.","9b676a11":"No luck this time.\nNote : these realtionships appears to be changing heavily over stocks \/ days. ","c1ee25e6":"Not sure what we can see from the scatter plots, but we can make one observation about range of feature 69,70,71 (basically the range of 69 is twice of those of 70-71), so I'll try for the sum and difference:","42850a83":"there even seems to be some pattern when comparing the difference :","1a2beb5e":"So we have approximately 700 unique underlyings a day. It is interesting to note that on a given day, the repartition is usually quite unbalanced: on day 0, 82 stocks make for half the opportunities.","9b7e76c9":"So uh, just like that a well behaved time series ?\nThere seems to be some link with feature 56 to 59, that appears to be some sort of times series too.","479569a5":"<a id='features_3_8'><\/a>\n### Feature 3 to 8","0ad50ba0":"<a id='features_9_16'><\/a>\n### Feature 9 to 16","b33f9f2f":"Not sure what to do with that. ratios ?","0c36d54d":"There seems to be more to it than a single trade ...\nNow into the very weird feature 65 and 66 :","0ac9f402":"not surprisingly they are similare to one another :","28a21f87":"63\/62 do not appear interesting, but 61\/60 appears to be. \n\n\nNote : this happens to depend on the day. For day 1 instead of day 0 the role of 60 and 61 are inversed with 62 and 63.","f6c9922d":"No obvious symmetry overall. ","03ffa1ef":"That definitely look like feature 65, but the relationship doesn't seems obvious as of now :","7f7172c7":"<a id='features_120_129'><\/a>\n### feature 120 to 129\nFinally some more interesting thing to appears are features at the end.\n\nNaturally they are linked to one another. And their difference patterns appears to be interesting.","1a26744a":"So not really usefull intra-day. We might want to keep an eye on some discrepencies. ","9c0e0fb9":"It seems to be a time series features that slowly delves into chaos. It seems a bit curious because it en up being quite symmetric.","29fef801":"hu. weird. Maybe the ratio would be a good feature.","9adca071":"It seems that feature 65 - 66 are time modulo something, like removing the hour from the time of the day ? It seems weird that it take negative values. It feels like we removed something constant by parts from feature 64. Feature 60 maybe ?","5067143d":"yay ! back to weird lines we don't really know how to deal with.","8305e066":"<a id='features_17_38'><\/a>\n### Feature 17 to 38","4bb1d3c8":"No obvious symettry here, but they all look pretty similar :","3cf5dcad":"Very much symmetric trough feature 0 :","e614842b":"<a id='features_55_59'><\/a>\n### Feature 55 - 59\nThings start to get weird white feature 55.","fa10145b":"Seems like some smoothing or time difference. Pretty unconclusive if you ask me. Maybe we should look intra tags 24-28. That is we have tag 27 that group features 72 to 83. So le's look at feature 78 to 83 and compare it to 72 to 77. ","d598df0c":"They look oddly similar. Let's see what a scatter plot look like. (defining a fonction might be useful here)","f4bea13e":"Definitely weird. Maybe they are indicating periods of the day. I suspect we might be interested in jumps.","e41c3dbd":"We can see (between index) 1500 and 1750 that some jump are not hard jumps. Definitely need to be investigated.","3dbe0d88":"<a id='features_64'><\/a>\n### feature 64 \nFinally one that is pretty clear and somewhat well behaved.","98d6677e":"Yeah there is probably some link, we are definitely onto something, but a bit diffcult to interpret. now the sum : ","cdc19b49":"<a id='features_69_71'><\/a>\n### Feature 69,70,71\nNow we are encountering some features that are hard to interpret. Those features make me especially curious as most features appears by group of 2, not of 3. Feature 69,70,71 appears to be linked to feature 53 and 54 trough their tag. Let's investigate that.","b3977619":"<a id='features_1_2'><\/a>\n### Feature 1 and 2","291b494c":"There is definitely some link, but it is unclear what : ","d3ab1198":"Still not sure what to do with those relationships.","000cce54":"Feature 65 to 68 also seems related to time. Relationship between 67, 68 are pretty direct to 64. Now I wonder what would be the difference:","11f7517d":"Already seems like a significant improvement and a step towards the usage of time series.","1fa6a3e5":"# Intraday feature exploration - feature engineering\n\nAs discussed [here](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/207709), feature 41-45, being constant for each 'stock' trough the day can help make the links between opportunities that regard the same stock appears. (We don't know if they are actually stocks but it's probably clearer than 'underlyings'). Isolating one stock in one given day we can give a hard look at all the available features, without being parasited by the other stocks.\n\nThere seems to be multiple main results in this notebook:\n- Once you isolate one stock some time series start to appear, allowing us to use a whole lot of new tools\n- Most features seems to be redundant in a way or another at the daily x stock level (It is still a bit unclear as to why they are not at a higher level)\n- Some features exhibit very weird patterns so that we should be able to devellop some original engineered features\n\nThe main drawback of this approach is that we have 500 days and around 700 'stocks' a day. So the conclusions we might draw from one stock over one day might not hold, especially if we do our exploration work on underlyings with higher number of opportunities... This notebooks might require a lot of additional work to explore the data set. (Well to be honest this is the main reason why I share the notebook: I can't possibly explore everything by myself, so I hope some of you will try it on other stocks \/ days and share their findings).\n\nOnce you have read the notebook feel free to fork it and run it for another stock or another day and share your results if they are significantly different. And if you don't want to, that's ok as long as you don't forget to upvote \ud83d\ude1c. If you still have time and want to go deeper you might want to check my other notebooks (If you want to go further, you can check my other works (about [Running algos for fast inference](https:\/\/www.kaggle.com\/lucasmorin\/running-algos-fe-for-fast-inference),[Target Engineering](https:\/\/www.kaggle.com\/lucasmorin\/target-engineering-patterns-denoising), and [using yfinance to download financial data in Ptyhon](https:\/\/www.kaggle.com\/lucasmorin\/downloading-market-data)). Feel free to upvote \/ share those too.\nLucas\n\nBest,\nLucas"}}