{"cell_type":{"760256d3":"code","7e4f00be":"code","b50e8f10":"code","b416bdbe":"code","479ccba9":"code","581aec0a":"code","2d220af8":"code","3107c57d":"code","490c5ebd":"code","90347267":"code","4107f962":"code","6cd7dd7e":"code","1750d54b":"code","c80dca81":"code","7f51cda5":"code","f8bc4926":"code","ae2fb24d":"code","48573b3b":"code","5026d194":"code","be103d4e":"code","24a5e558":"code","23270f6f":"code","410efd49":"code","4b4e6f5c":"code","104b5e3c":"code","2541588c":"code","81bc8727":"code","e4ff0558":"code","a733fc13":"code","bedebf2d":"code","e83ad1f4":"code","1ce5d315":"code","0a49c34f":"code","37e5864c":"code","db5b849c":"code","f7f6b487":"code","0dce41f1":"code","cb2e0bc5":"code","8f6b2211":"code","351e0b54":"code","8a1e1b10":"code","eef220ab":"code","8e44d545":"code","5170b031":"code","09fc12b3":"code","f7e20559":"code","b839a68b":"code","dd063deb":"code","40ff5239":"code","fd608134":"code","9e812050":"code","52f8e7b1":"code","c1040db1":"code","0f5514eb":"code","51b37c13":"code","1d05ed81":"code","818ceb88":"code","44839398":"code","1cfbf416":"code","2edc1fe2":"code","648e2ecc":"markdown","8b3045c7":"markdown","d7e7f392":"markdown","6955e1ed":"markdown","df8b1f08":"markdown","9d7cf0bc":"markdown","eaf63903":"markdown","c0769bc9":"markdown","20debe8c":"markdown","a2658f01":"markdown","54c97e67":"markdown","cd647faa":"markdown","d5a3cf7e":"markdown","93a00186":"markdown","bc429597":"markdown","c58e6c26":"markdown","1e1d6ff8":"markdown","3eaea11d":"markdown","835e5d1f":"markdown","24496cc3":"markdown","8d0974b1":"markdown","f2898fec":"markdown","d46fe6ef":"markdown","6e8e689d":"markdown","0187a599":"markdown","58834274":"markdown","0e242c46":"markdown","f284ba5d":"markdown","fd677243":"markdown","81dfed70":"markdown","4efdcc14":"markdown","50e0ec86":"markdown","588c15ef":"markdown","56a73c0d":"markdown"},"source":{"760256d3":"# Importing necessary Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import style","7e4f00be":"#Reading Dataset\ndf= pd.read_csv('..\/input\/online-shoppers-intention\/online_shoppers_intention.csv')\ndf.head()","b50e8f10":"#Checking datatypes\ndf.dtypes","b416bdbe":"#Checking null values\ndf.isnull().any()","479ccba9":"df_final=df.dropna() #Removing Null items from out dataset\n\nrw,col=df_final.shape \n\nprint(f\"We have final dataset with {rw} rows and {col} columns.\")","581aec0a":"revenue= df_final['Revenue'].value_counts()\n#style.use('classic')\nsns.set_style(\"darkgrid\")\nsns.set_context(\"talk\")\nplt.figure(figsize=(8,8))\nx=revenue.index\ny=revenue.values\noutside=(0,0.1)\nplt.pie(y,labels=x,autopct=\"%1.1f%%\",startangle=90,explode=outside,shadow=True)\nplt.title('Revenue')\nplt.legend(loc='upper right')","2d220af8":"# People visiting on Weekends and Visitor Types\n#Weekends\nplt.figure(figsize=(10,8))\nplt.subplot(2,1,1)\nsns.countplot(x=df_final['Weekend'],palette='twilight')\n\n# Visitor Types\nplt.figure(figsize=(10,8))\nplt.subplot(2,1,2)\nsns.countplot(x=df_final['VisitorType'])","3107c57d":"month_wise_Count= df_final['Month'].value_counts().sort_values(ascending=False)\nmonth_wise_Count","490c5ebd":"# Plotting Months\n\nsns.set_style(\"ticks\")\nsns.set_context(\"notebook\")\nplt.figure(figsize=(10,8))\nx=month_wise_Count.index\ny=month_wise_Count.values\noutside=(0.05,0,0,0,0,0,0,0,0,0)\nplt.pie(y,labels=x,autopct=\"%1.1f%%\",startangle=90,explode=outside,pctdistance=0.8,shadow=True,labeldistance=1.1)\nplt.title('Month-wise Customers')\nplt.legend(loc='best')","90347267":"# Plotting OperatingSystems,Browser,Region and TrafficType\n\nplt.figure(figsize=(10,9))\n\n#Operating Systems\nplt.subplot(2,2,1)\nsns.countplot(x=df_final['OperatingSystems'],palette='twilight')\n\n# Browser\nplt.subplot(2,2,2)\nsns.countplot(x=df_final['Browser'],palette=\"cubehelix\")\n\n# Regions\nplt.subplot(2,2,3)\nsns.countplot(x=df_final['Region'],palette=\"winter\")\n\n# Regions\nplt.subplot(2,2,4)\nsns.countplot(x=df_final['TrafficType'],palette=\"RdGy\")","4107f962":"# Weekends\/VisitorType vs Revenue\nsns.set_style(\"ticks\")\nsns.set_context(\"talk\")\ndf1 = pd.crosstab(df_final['Weekend'], df_final['Revenue'])\ndf1.plot(kind='bar',stacked=True,colormap='RdBu')\nplt.title('Weekend vs Revenue', fontsize = 30)","6cd7dd7e":"# Visitor Types\nsns.set_style(\"ticks\")\nsns.set_context(\"notebook\")\ndf2 = pd.crosstab(df_final['VisitorType'], df_final['Revenue'])\ndf2.plot(kind='bar',stacked=True)\nplt.title('Visitor Type vs Revenue', fontsize = 25)","1750d54b":"# Traffic Type vs Revenue\nsns.set_style(\"ticks\")\nsns.set_context(\"talk\")\n#plt.figure(figsize=(5,5))\ndf3 = pd.crosstab(df_final['TrafficType'], df_final['Revenue'])\ndf3.plot(kind='bar',stacked=True,colormap=\"Wistia\")\nplt.title('Traffic Type vs Revenue', fontsize = 25)","c80dca81":"# Region vs Revenue\nsns.set_style(\"ticks\")\nsns.set_context(\"talk\")\n#plt.figure(figsize=(5,5))\ndf4 = pd.crosstab(df_final['Region'], df_final['Revenue'])\ndf4.plot(kind='bar',stacked=True,colormap=\"nipy_spectral\")\nplt.title('Region Type vs Revenue', fontsize = 25)","7f51cda5":"# Browser vs Revenue\nsns.set_style(\"ticks\")\nsns.set_context(\"talk\")\n#plt.figure(figsize=(5,5))\ndf4 = pd.crosstab(df_final['Browser'], df_final['Revenue'])\ndf4.plot(kind='bar',stacked=True,colormap=\"Wistia\")\nplt.title('Browser vs Revenue', fontsize = 25)","f8bc4926":"# Operating System vs Revenue\nsns.set_style(\"ticks\")\nsns.set_context(\"talk\")\n#plt.figure(figsize=(5,5))\ndf5 = pd.crosstab(df_final['OperatingSystems'], df_final['Revenue'])\ndf5.plot(kind='bar',stacked=True,colormap=\"Set1\")\nplt.title('Operating System vs Revenue', fontsize = 25)","ae2fb24d":"df_final.columns","48573b3b":"sns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\nplt.figure(figsize=(10,8))\ndistcols=df_final[['Administrative', 'Administrative_Duration', 'Informational','Informational_Duration', 'ProductRelated', 'ProductRelated_Duration','BounceRates', 'ExitRates', 'PageValues', 'SpecialDay','Revenue']]\nsns.heatmap(distcols.corr(),cmap=\"icefire\")","5026d194":"# Distribution of columns w.r.t Revenue\nsns.set_style(\"whitegrid\")\nsns.set_context(\"paper\")\nplt.figure(figsize=(9,9))\ndistcols1=df_final[['ProductRelated', 'ProductRelated_Duration','BounceRates','ExitRates','Revenue']]\nsns.pairplot(distcols1,hue='Revenue')","be103d4e":"# Checking distribution of Sepcial Days\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\nplt.figure(figsize=(7,5))\nsns.distplot(df_final['SpecialDay'],kde=False,bins=15)","24a5e558":"df_final.dtypes","23270f6f":"#Converting Object datatypes into numeric to make machine readable\nfrom sklearn.preprocessing import LabelEncoder\nlbenc=LabelEncoder()\ndf_final['Month']= lbenc.fit_transform(df_final['Month'])\ndf_final['VisitorType']= lbenc.fit_transform(df_final['VisitorType'])","410efd49":"#df_final.dtypes","4b4e6f5c":"x=df_final.loc[:,df_final.columns!='Revenue'] #Independent Variables\nx\ny=df_final.loc[:,'Revenue'] #Dependent Variable","104b5e3c":"# Train_test Data Split\nfrom sklearn.model_selection import train_test_split\nx_trn,x_tst,y_trn,y_tst= train_test_split(x,y,test_size=.20,random_state=1)\nprint(\"x_trn\",x_trn)\nprint(\"\\n x_tst\",x_tst)\nprint(\"\\n \\n y_trn\",y_trn)\nprint(\"\\n y_tst\",y_tst)","2541588c":"#Applying Logistic Regression Algorithm\nfrom sklearn.linear_model import LogisticRegression\nlgr= LogisticRegression()\n\nlgr.fit(x_trn,y_trn) #Training\n\npredic_y1= lgr.predict(x_tst) #Predicting y values of x test based on training\npredic_y1\n\n# Checking accuracy\nfrom sklearn.metrics import accuracy_score\nlogistic_accuracy=accuracy_score(y_tst,predic_y1)*100\nprint(f'\\n Accuracy of Logistic Regression {logistic_accuracy}%.')","81bc8727":"# checking no. of wrong results\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_tst,predic_y1))","e4ff0558":"from sklearn.neighbors import KNeighborsClassifier\nkn=KNeighborsClassifier()\nkn.fit(x_trn,y_trn)\npredic_y2=kn.predict(x_tst)#Output Prediction\npredic_y2\n\n#Checking Accuracy score\nKNN_accuracy=accuracy_score(y_tst,predic_y2)*100\nKNN_accuracy\n\nprint(f'Accuracy of KNN algorithm is {KNN_accuracy}%.')\n\n#Accurate values\nconfusion_matrix(y_tst,predic_y2)","a733fc13":"#Applying Algorithm\nfrom sklearn.svm import SVC\nsvc=SVC()\nsvc.fit(x_trn,y_trn)\n\npredic_y3=svc.predict(x_tst)\nprint(\"Predicting output y:\",predic_y3)\n\n#Checking accuracy\nfrom sklearn.metrics import accuracy_score\nSVM_accuracy=accuracy_score(y_tst,predic_y3)*100\nprint(f'Accuracy score with dafault parameters of SVC is {SVM_accuracy}%.')","bedebf2d":"# rbf kernel with C and gamma values\nfrom sklearn.svm import SVC\nsvc1=SVC(kernel=\"rbf\",C=1.0,gamma=0.1,random_state=3)\nsvc1.fit(x_trn,y_trn)\npredict_y6=svc1.predict(x_tst)\nprint(\"Output Prediction\",predict_y6)\n\n#Checking accuracy\nSVM_accuracy_rbf= accuracy_score(y_tst,predict_y6)*100\nprint(f'Accuracy score with rbf kernelC,gamma values is {SVM_accuracy_rbf}%.')","e83ad1f4":"# Sigmoid kernel\nsvc2=SVC(kernel='sigmoid')\nsvc2.fit(x_trn,y_trn)\npredic_y9=svc2.predict(x_tst)\nprint(\"Output Prediction\",predic_y9)\n\n#Checking accuracy\nSVM_accuracy_sig= accuracy_score(y_tst,predic_y9)*100\nprint(f'Accuracy score with Sigmoid kernel and C,gamma values:{SVM_accuracy_sig}%.')","1ce5d315":"# Checking best SVM Algorithm\nSVM_Algorithms = [\"SVM with Default Parameters\",\"SVM with rbf Kernel\",\"SVM with Sigmoid Kernel\"]\nAccuracy_Score=[SVM_accuracy,SVM_accuracy_rbf,SVM_accuracy_sig]\n\nBest_SVM_Algorithm= pd.DataFrame({\" SVM Algorithm\":SVM_Algorithms,\"Accuracy Score\":Accuracy_Score})\nBest_SVM_Algorithm","0a49c34f":"from sklearn.tree import DecisionTreeClassifier\ndt= DecisionTreeClassifier(criterion='entropy',max_depth=17,random_state=3)\nprint(\"Decision Tree with parameters: Criteria-Entropy\\n\",dt.fit(x_trn,y_trn))\n\npredic_y10= dt.predict(x_tst)\npredic_y10\n\n#Accuracy Check\nfrom sklearn.metrics import accuracy_score\nDT_accuracy= accuracy_score(y_tst,predic_y10)*100\nprint(f'\\n Accuracy Score with Decision Tree Algoritm is {DT_accuracy}%.')","37e5864c":"x.columns","db5b849c":"#Plotting tree\nfrom sklearn import tree\nfrom sklearn.tree import export_graphviz \ndata_feature= ['Administrative', 'Administrative_Duration', 'Informational','Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',\n'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay', 'Month','OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType',\n'Weekend']\nfrom graphviz import Source\nfrom IPython.display import SVG\nfrom IPython.display import display\n\ngraph = Source(tree.export_graphviz(dt,out_file=None, feature_names=data_feature, filled = True,rounded=True))\ndisplay(SVG(graph.pipe(format='svg')))","f7f6b487":"# Applying Algorithm\nfrom sklearn.ensemble import RandomForestClassifier\nrnd_clf= RandomForestClassifier(n_estimators=10, random_state=3, max_depth=10, criterion = 'entropy')\nrnd_clf.fit(x_trn,y_trn)\n\n#Predicting Output\npredic_y11= rnd_clf.predict(x_tst)\npredic_y11\n\n#Checking accuracy\nRNDFrst_accuracy= accuracy_score(y_tst,predic_y11)*100\nprint(f'\\n Accuracy Score with Decision Tree Algoritm is {RNDFrst_accuracy}%.')","0dce41f1":"#Plotting Random Forest Tree\nestimators=rnd_clf.estimators_[5] # gives 5 decision trees\ndata_feature= ['Administrative', 'Administrative_Duration', 'Informational','Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',\n'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay', 'Month','OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType',\n'Weekend']\nfrom sklearn import tree\nfrom graphviz import Source\nfrom IPython.display import SVG  #SVG format\nfrom IPython.display import display \n\ngraph = Source(tree.export_graphviz(estimators, out_file=None,feature_names=data_feature,filled = True))\ndisplay(SVG(graph.pipe(format='svg')))","cb2e0bc5":"from sklearn.naive_bayes import GaussianNB\ngauss= GaussianNB()\ngauss.fit(x_trn,y_trn)\n\npredict_y12= gauss.predict(x_tst) \npredict_y12\n\n#Checking Accuracy\nGaussNB_accuracy= accuracy_score(y_tst,predict_y12)*100\nprint(f'\\n Accuracy Score with Decision Tree Algoritm is {GaussNB_accuracy}%.')","8f6b2211":"#Let us compare accuracy of each Algrithm\nAlgorithms = [\"Logistic Regression\",\" K-NN Algorithm \",\"SVM \",\"Decision Tree\",\"Random Forest\",\"Gaussian NB\"]\nAccuracy_Score=[logistic_accuracy,KNN_accuracy,SVM_accuracy_rbf,DT_accuracy,RNDFrst_accuracy,GaussNB_accuracy]\n\n#Checking Accuracies of various Algorithms\nAlgorithms_Accuracy= pd.DataFrame({\" Algorithms\":Algorithms,\"Accuracy Score\":Accuracy_Score})\nAlgorithms_Accuracy","351e0b54":"Algorithms_Accuracy['Accuracy Score'].mean()","8a1e1b10":"from sklearn.preprocessing import MinMaxScaler\nminmax_sclr= MinMaxScaler()\nx_trn1= minmax_sclr.fit_transform(x_trn) #Scaling x-train and test data\nx_tst1=minmax_sclr.transform(x_tst)","eef220ab":"#Applying Logistic Regression Algorithm\nlgr_m= LogisticRegression()\nlgr_m.fit(x_trn1,y_trn) #Training\n\n#Predicting output\npredic_y1_m= lgr_m.predict(x_tst1) #Predicting y values of x test based on training\npredic_y1_m\n\n# Checking accuracy\nlogistic_accuracy_m=accuracy_score(y_tst,predic_y1_m)*100\nprint(f'\\n Accuracy of Logistic Regression using Min-Max Scaling is {logistic_accuracy_m}%.')\n\n# checking no. of wrong results\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_tst,predic_y1_m))","8e44d545":"# K-NN algorithm\nkn_m=KNeighborsClassifier()\nkn_m.fit(x_trn1,y_trn)\npredic_y2_m=kn_m.predict(x_tst1)#Output Prediction\npredic_y2_m\n\n#Checking Accuracy score\nKNN_accuracy_m=accuracy_score(y_tst,predic_y2_m)*100\nKNN_accuracy_m\n\nprint(f'Accuracy of KNN algorithm using Min-Max Scaler is {KNN_accuracy_m}%.')\n\n#Accurate values\nconfusion_matrix(y_tst,predic_y2_m)","5170b031":"# SVM algorithm with rbf kernel(C and gamma values)\nsvc1_m=SVC(kernel=\"rbf\",C=1.0,gamma=0.1,random_state=1)\nsvc1_m.fit(x_trn1,y_trn)\npredict_y6_m=svc1_m.predict(x_tst1)\nprint(\"Output Prediction\",predict_y6_m)\n\n#Checking accuracy\nSVM_accuracy_rbf_m= accuracy_score(y_tst,predict_y6_m)*100\nprint(f'Accuracy score of SVM with rbf kernel(C,gamma values)is {SVM_accuracy_rbf_m}%.')","09fc12b3":"#Decision Tree Algorithm\ndt_m= DecisionTreeClassifier(criterion='entropy',max_depth=17,random_state=3)\nprint(\"Decision Tree with parameters: Criteria-Entropy\\n\",dt_m.fit(x_trn1,y_trn))\n\npredic_y10_m= dt_m.predict(x_tst1)\npredic_y10_m\n\n#Accuracy Check\nDT_accuracy_m= accuracy_score(y_tst,predic_y10_m)*100\nprint(f'\\n Accuracy Score with Decision Tree Algoritm using Min-Max Scaler is {DT_accuracy_m}%.')","f7e20559":"# Applying Random Forest Algorithm\nrnd_clf_m= RandomForestClassifier(n_estimators=10, random_state=3, max_depth=17, criterion = 'entropy')\nrnd_clf_m.fit(x_trn1,y_trn)\n\n#Predicting Output\npredic_y11_m= rnd_clf_m.predict(x_tst1)\npredic_y11_m\n\n#Checking accuracy\nRNDFrst_accuracy_m= accuracy_score(y_tst,predic_y11_m)*100\nprint(f'\\n Accuracy Score with Decision Tree Algoritm with Min-Max Scaler is {RNDFrst_accuracy_m}%.')","b839a68b":"# Gaussian Naive Bayes' Algorithm\ngauss_m= GaussianNB()\ngauss_m.fit(x_trn1,y_trn)\n\npredict_y12_m= gauss_m.predict(x_tst1) \npredict_y12_m\n\n#Checking Accuracy\nGaussNB_accuracy_m= accuracy_score(y_tst,predict_y12_m)*100\nprint(f'\\n Accuracy Score with Decision Tree Algoritm is {GaussNB_accuracy_m}%.')","dd063deb":"#Comparing accuracy of each Algorithm\nAlgorithms1 = [\"Logistic Regression\",\" K-NN Algorithm \",\"SVM \",\"Decision Tree\",\"Random Forest\",\"Gaussian NB\"]\nAccuracy_Score1=[logistic_accuracy_m,KNN_accuracy_m,SVM_accuracy_rbf_m,DT_accuracy_m,RNDFrst_accuracy_m,GaussNB_accuracy_m]\n\n#Checking Accuracies of various Algorithms\nAlgorithms_Accuracy1= pd.DataFrame({\" Algorithms\":Algorithms1,\"Accuracy Score\":Accuracy_Score1})\nAlgorithms_Accuracy1\n\nAlgorithms_Accuracy1['Accuracy Score'].mean()","40ff5239":"#Standard Scaling\nfrom sklearn.preprocessing import StandardScaler\nstd_sclr = StandardScaler()\nx_trn2 = std_sclr.fit_transform(x_trn)\nx_tst2 = std_sclr.transform(x_tst)\n\n#print(\"x_trn2:\\n \",x_trn2)\n#print(\"x_tst2:\\n \",x_tst2)\n\n#Dimenstionality Reduction Using PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=10)\nx_trn_P=pca.fit_transform(x_trn2)\nx_tst_P= pca.transform(x_tst2)\nprint(\"Old dimension of training dataset:\",x_trn2.shape)\nprint(\"Reduced Dimension of training datset:\",x_trn_P.shape)","fd608134":"print(pca.explained_variance_ratio_)","9e812050":"#Applying Logistic Regression Algorithm\nlgr_p= LogisticRegression()\nlgr_p.fit(x_trn_P,y_trn) #Training\n\n#Predicting output\npredic_y1_P= lgr_p.predict(x_tst_P) #Predicting y values of x test based on training\npredic_y1_P\n\n# Checking accuracy\nlogistic_accuracy_P=accuracy_score(y_tst,predic_y1_P)*100\nprint(f'\\n Accuracy of Logistic Regression using PCA is {logistic_accuracy_P}%.')\n\n# checking no. of wrong results\nprint(confusion_matrix(y_tst,predic_y1_P))","52f8e7b1":"# K-NN algorithm\nkn_p=KNeighborsClassifier()\nkn_p.fit(x_trn_P,y_trn)\npredic_y2_P=kn_p.predict(x_tst_P)#Output Prediction\npredic_y2_P\n\n#Checking Accuracy score\nKNN_accuracy_P=accuracy_score(y_tst,predic_y2_P)*100\nKNN_accuracy_P\n\nprint(f'Accuracy of KNN algorithm using PCA is {KNN_accuracy_P}%.')\n\n#Accurate values\nconfusion_matrix(y_tst,predic_y2_P)","c1040db1":"# SVM algorithm with rbf kernel(C and gamma values)\nsvc1_p=SVC(kernel=\"rbf\",C=1.0,gamma=0.1,random_state=2)\nsvc1_p.fit(x_trn_P,y_trn)\npredict_y6_P=svc1_p.predict(x_tst_P)\nprint(\"Output Prediction\",predict_y6_P)\n\n#Checking accuracy\nSVM_accuracy_rbf_P= accuracy_score(y_tst,predict_y6_P)*100\nprint(f'\\n Accuracy score of SVM with rbf kernel(C,gamma values) using PCA is {SVM_accuracy_rbf_P}%.')","0f5514eb":"#Decision Tree Algorithm\ndt_p= DecisionTreeClassifier(criterion='entropy',max_depth=17,random_state=3)\nprint(\"Decision Tree with parameters: Criteria-Entropy\\n\",dt_p.fit(x_trn_P,y_trn))\n\npredic_y10_P= dt_p.predict(x_tst_P)\npredic_y10_P\n\n#Accuracy Check\nDT_accuracy_P= accuracy_score(y_tst,predic_y10_P)*100\nprint(f'\\n Accuracy Score with Decision Tree Algoritm using PCA is {DT_accuracy_P}%.')","51b37c13":"# Applying Random Forest Algorithm\nrnd_clf_p= RandomForestClassifier(n_estimators=10, random_state=3, max_depth=17, criterion = 'entropy')\nrnd_clf_p.fit(x_trn_P,y_trn)\n\n#Predicting Output\npredic_y11_P= rnd_clf_p.predict(x_tst_P)\npredic_y11_P\n\n#Checking accuracy\nRNDFrst_accuracy_P= accuracy_score(y_tst,predic_y11_P)*100\nprint(f'\\n Accuracy Score with Decision Tree Algoritm with PCA is {RNDFrst_accuracy_P}%.')","1d05ed81":"# Gaussian Naive Bayes' Algorithm\ngauss_p= GaussianNB()\ngauss_p.fit(x_trn_P,y_trn)\n\npredict_y12_P= gauss_p.predict(x_tst_P) \npredict_y12_P\n\n#Checking Accuracy\nGaussNB_accuracy_P= accuracy_score(y_tst,predict_y12_P)*100\nprint(f'\\n Accuracy Score with Decision Tree Algoritm using PCA is {GaussNB_accuracy_P}%.')","818ceb88":"#Comparing accuracy of each Algorithm using PCA\nAlgorithms2 = [\"Logistic Regression\",\" K-NN Algorithm \",\"SVM \",\"Decision Tree\",\"Random Forest\",\"Gaussian NB\"]\nAccuracy_Score2=[logistic_accuracy_P,KNN_accuracy_P,SVM_accuracy_rbf_P,DT_accuracy_P,RNDFrst_accuracy_P,GaussNB_accuracy_P]\n\n#Checking Accuracies of various Algorithms using PCA\nAlgorithms_Accuracy2= pd.DataFrame({\" Algorithms\":Algorithms2,\"Accuracy Score\":Accuracy_Score2})\nAlgorithms_Accuracy2","44839398":"Algorithms_Accuracy2['Accuracy Score'].mean()","1cfbf416":"#Let us combine all results in one Dataframe and plot in a line graph for best understanding.\nAlgo_vs_Accuracy= pd.DataFrame({'Algorithm': Algorithms,'Accuracy score(without Scaling)':Accuracy_Score,'Accuracy score(Min-Max Scaling)':Accuracy_Score1,'Accuracy score(using PCA)':Accuracy_Score2})\nAlgo_vs_Accuracy","2edc1fe2":"#Plotting the results\nfrom matplotlib import style\nstyle.use('seaborn-deep')\nsns.set_context(\"talk\")\nplt.figure(figsize=(22,10))\nplt.plot(Algorithms,Accuracy_Score,'yo--',label='Without Scaling',linewidth=4, markersize=12)\nplt.plot(Algorithms,Accuracy_Score1,'ro-',label='Using Min-Max Scaling',linewidth=4, markersize=12)\nplt.plot(Algorithms,Accuracy_Score2,'bo-',label='Using PCA',linewidth=4, markersize=12)\nplt.legend()\nplt.title(\"Algorithms vs Accuracy Scores\",fontsize=25)","648e2ecc":"Here we can see the count of every feature related to cutomers.","8b3045c7":"Clearly, we can see the Decision Tree algorithm is giving highest Accuracy of 89% for our datset. ","d7e7f392":"## Data Analysis and Visualization","6955e1ed":"It means Revenue is not much higher during Sepcial days.","df8b1f08":"27% customers are visiting in May, 24% in Nov anf 15% in March. So there is no specific month when customers visit the sites mostly.","9d7cf0bc":"## Gaussian Naive Bayes' Algorithm","eaf63903":"## KNN Algorithm","c0769bc9":"## Standard Scaling and Dimension Reduction through PCA","20debe8c":"## SVM Algorithm","a2658f01":"## Applying Algorithms for Revenue Prediction\nHere we will apply categorial algorithms to solve the problems and find out the best suited one from their accuracies.","54c97e67":"More revenue is recieved from Traffic type-2.","cd647faa":"We can observe some points here like:\n* We are getting highest accuracy from Random Forest Algorithm( with and without data scaling).\n* The average algorithm accuracy is 85% for Min-Max and PCA and 86% without using Scaling.\n* Logistic and KNN  algorithms are having almost same accuracies in all scenarios(without scaling,Min-Max and PCA).","d5a3cf7e":"Most of the revenue obtained from weekdays.","93a00186":"It means we have more visitors on week days as compared to weekends. Further, Returning visitors are more than that of new.","bc429597":"## Let us try Scaling and reducing Dimensions  of our input data through PCA and checking if there is any impact on the algorithm prediction.","c58e6c26":"Here we can see the less the Exit rates, bounce rate, the more the revenue is. Similarly, revenue increase with the higher Product related value.","1e1d6ff8":"Here we can getting same accuracy from SVM algorithm with default parameters and with 'rbf' kernel. so we can consider 83.48% as our final SVM algorithm accuracy.\n.","3eaea11d":"This can be seen clearly the columns:'ProductRelated', 'ProductRelated_Duration,'Bounce Rates','ExitRates' are strongly connected with revenue.Let us check each one of these saparetly.","835e5d1f":"### About Dataset \n The dataset consists of feature vectors belonging to 12,330 sessions. The dataset was formed so that each session  would belong to a different user.This file consists of various Information related to customer behavior in online shopping websites.\n \nThe dataset consists of 10 numerical and 8 categorical attributes. The **'Revenue'** attribute can be used as the class  label.\n\n#### Dataset Features\n\n- **Administrative, Administrative Duration, Informational, Informational Duration, Product Related** and **Product Related Duration** represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories.The **Bounce Rate, Exit Rate** and **Page Value** features represent the metrics measured by \"Google Analytics\" for each page in the e-commerce site. \n\n- **Bounce Rate**- feature for a web page refers to the percentage of visitors who enter the site from that page and then leave   (\"bounce\") without triggering any other requests to the analytics server during that session.\n- **Exit Rate**- feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session. \n- **Page Value**- feature represents the average value for a web page that a user visited before completing an e-commerce transaction.\n- **Special Day**- feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother\u2019s Day, Valentine's Day).The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina\u2019s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8.\n- The dataset also includes operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.","24496cc3":"## Decision Tree Algorithm","8d0974b1":"Only 15% of total customers visiting the shopping sites are giving final revenues.","f2898fec":"People logging on from Browser-2 are giving more.","d46fe6ef":"More people  are logging on from Operating system type-2  and are giving more revenues.","6e8e689d":"## Random Forest Algorithm","0187a599":"This means the Reuturning visitor are giving more revenue to the shopping site.","58834274":"### Min-Max Scaling","0e242c46":"Here, again we are getting maximum accuracy from Random Forest Algorithm.","f284ba5d":"### Logistic Algorithm","fd677243":"Most revenue is obtained from customers logged on from region-1.","81dfed70":"Let us check various data features  of the dataset and analyse their impact on revenue.","4efdcc14":"### Now let us analyze these categorial feautres first such as:\n#### Month,OperatingSystems,Browser,Region,\tTrafficType,VisitorType and Weekend\t.","50e0ec86":"#### Let us check  whether these feautures are related to site's revenue or not.","588c15ef":"### Now let us plot numerical attributes and check relationship with Revenue.","56a73c0d":"* Here, we are getting almost same accuracy from Logistic,KNN,SVM and Random forest Algorithms.\n* Further, accuracy scores are higher as compared to Min-Max Scaling."}}