{"cell_type":{"e5678873":"code","fb704c2f":"code","3f948df0":"code","ca36608c":"code","312040c6":"code","afabd607":"code","2ecf0214":"code","1970abe9":"code","0d0296db":"code","ab6c02d0":"code","2426a73e":"code","50d2d427":"code","53a8dbb1":"code","ccc529d7":"code","c9131010":"code","8b41d006":"code","f9290787":"code","b2c64fa4":"code","0b41f342":"code","8db9809a":"code","3807a415":"code","0aefd3eb":"code","c377f3f2":"code","b35ffaa7":"code","b8eb76c5":"code","8b2cbde2":"code","f7001956":"code","d1eef4db":"code","a933ab8b":"code","0359b54d":"code","fc5b6e20":"code","f70f6ade":"code","f8b04306":"code","e5847552":"code","35bd5e67":"code","3a06b52e":"code","a2e39a3c":"code","66fd1c85":"code","b8a2c33c":"code","7d798cfa":"code","34f86190":"code","b6286e6f":"code","0ddef901":"code","4b87bf84":"code","33253c80":"code","eb084ad7":"code","d0d84bbc":"code","c1240f04":"code","3d33fd36":"code","564f9d13":"code","54728e44":"code","fdedaabd":"code","68631554":"markdown","bfcc910b":"markdown","57fc3dca":"markdown","d52c5ef4":"markdown","c153ec56":"markdown","fa4c4c4f":"markdown","75be278b":"markdown","7c5d9372":"markdown","f378355c":"markdown","f77454e0":"markdown","1e40e8c5":"markdown","6845e63a":"markdown","3433e99a":"markdown","4f27d6bc":"markdown","cb1bd329":"markdown","41f2432d":"markdown","cdab7fcc":"markdown"},"source":{"e5678873":"import torch\nfrom sklearn.model_selection import train_test_split\n\nX = torch.rand(10, 3) # 10 data points, with 3 dimensions\/features each.\nY = torch.rand(10)    # 10 corresponding y values for the data points.","fb704c2f":"print(X.shape)\nX","3f948df0":"print(Y.shape) \nY","ca36608c":"test_size = 0.2 # If we want to hold-out 20% of our data as test-set\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)","312040c6":"print(X_train.shape)  # 8 data points with 3 dimensions each\nprint(Y_train.shape)  # 8 y-values corresponding to the 8 data points","afabd607":"print(X_test.shape)  # 2 data points with 3 dimensions each\nprint(Y_test.shape)  # 2 y-values corresponding to the 2 data points","2ecf0214":"# To get the same 20% validation proportion as the test size, \n# you can use this idiom to get the valid_size\nvalid_size = 0.2 \/ (1 - test_size) \n# We split the validation set from the training set.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=valid_size)","1970abe9":"print(X_train.shape)  # 6 data points with 3 dimensions each\nprint(Y_train.shape)  # 6 y-values corresponding to the 6 data points","0d0296db":"print(X_valid.shape)  # 2 data points with 3 dimensions each\nprint(Y_valid.shape)  # 2 y-values corresponding to the 2 data points","ab6c02d0":"def train_valid_test_split(X, Y, valid_size, test_size, random_state=42):\n    \"\"\"\n    Extending sklearn's train_test_split and using only floating point test_size\n    https:\/\/scikit-learn.org\/0.16\/modules\/generated\/sklearn.cross_validation.train_test_split.html\n    \"\"\"\n    assert valid_size < 1 and test_size < 1 \n    X_train_valid, X_test, Y_train_valid, Y_test =  train_test_split(X, Y, test_size=test_size, random_state=random_state)\n    _valid_size = valid_size \/ (1 - test_size)\n    X_train, X_valid, Y_train, Y_valid =  train_test_split(X_train_valid, Y_train_valid, \n                                                           test_size=_valid_size, random_state=random_state)\n    return X_train, X_valid, X_test, Y_train, Y_valid, Y_test","2426a73e":"X = torch.rand(100, 3) # 100 data points, with 3 dimensions\/features each.\nY = torch.rand(100)    # 100 corresponding y values for the data points.\n\nX_train, X_valid, X_test, Y_train, Y_valid, Y_test = train_valid_test_split(X, Y, valid_size=0.30, test_size=0.20)","50d2d427":"print('Training set:\\t',   X_train.shape, Y_train.shape)\nprint('Validation set:\\t', X_valid.shape, Y_valid.shape)\nprint('Test set:\\t',       X_test.shape, Y_test.shape)","53a8dbb1":"from tqdm import tqdm\n\nimport torch\nfrom torch import nn, optim","ccc529d7":"import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsns.set_style(\"darkgrid\")\nsns.set(rc={'figure.figsize':(15, 10)})","c9131010":"# Here's some torch randomization magic to fix the outputs so that \n# we'll see the same outputs no matter how we run this notebook.\n\n# For CPU version.\ntorch.backends.mkl.deterministic = True\n# For CUDA version\n##torch.backends.cuda.deterministic = True\n\ntorch.manual_seed(42)","8b41d006":"# Create some random data.\nX = torch.rand(100, 3) # 100 data points, with 3 dimensions\/features each.\nY = torch.rand(100)    # 100 corresponding y values for the data points.\n\nX_train, X_valid, X_test, Y_train, Y_valid, Y_test = train_valid_test_split(X, Y, valid_size=0.20, test_size=0.10)\n","f9290787":"print('Training set:\\t',   X_train.shape, Y_train.shape)\nprint('Validation set:\\t', X_valid.shape, Y_valid.shape)\nprint('Test set:\\t',       X_test.shape, Y_test.shape)","b2c64fa4":"num_data, input_dim = X_train.shape\nnum_data, output_dim = Y_train.unsqueeze(1).shape\n\nprint('No. of training data:', num_data)\nprint('Input dimension:', input_dim)\nprint('Output dimension:', output_dim)","0b41f342":"# Step 1: Initialization. \n# Note: When using PyTorch a lot of the manual weights\n#       initialization is done automatically when we define\n#       the model (aka architecture)\nmodel = nn.Sequential(\n            nn.Linear(input_dim, output_dim), \n            nn.Sigmoid())\ncriterion = nn.MSELoss() \nlearning_rate = 1.0\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\nnum_epochs = 200\n\nlosses = []\n\nfor i in tqdm(range(num_epochs)):\n    # Reset the gradient after every epoch. \n    optimizer.zero_grad() \n    # Step 2: Foward Propagation\n    predictions = model(X_train)\n    \n    # Step 3: Back Propagation \n    # Calculate the cost between the predictions and the truth.\n    loss_this_epoch = criterion(predictions, Y_train)\n    # Note: The neat thing about PyTorch is it does the \n    #       auto-gradient computation, no more manually defining\n    #       derivative of functions and manually propagating\n    #       the errors layer by layer.\n    loss_this_epoch.backward()\n    \n    # Step 4: Optimizer take a step. \n    # Note: Previously, we have to manually update the \n    #       weights of each layer individually according to the\n    #       learning rate and the layer delta. \n    #       PyTorch does that automatically =)\n    optimizer.step()\n    \n    # Log the loss value as we proceed through the epochs.\n    losses.append(loss_this_epoch.data.item())\n    \n# Visualize the losses\nplt.plot(losses)\nplt.show()","8db9809a":"# Create some random data.\nX = torch.rand(100, 3) # 100 data points, with 3 dimensions\/features each.\nY = torch.rand(100)    # 100 corresponding y values for the data points.\n\nX_train, X_valid, X_test, Y_train, Y_valid, Y_test = train_valid_test_split(X, Y, valid_size=0.20, test_size=0.10)\n","3807a415":"num_data, input_dim = X_train.shape\nnum_data, output_dim = Y_train.unsqueeze(1).shape\n\nprint('No. of training data:', num_data)\nprint('Input dimension:', input_dim)\nprint('Output dimension:', output_dim)","0aefd3eb":"# Step 1: Initialization. \n# Note: When using PyTorch a lot of the manual weights\n#       initialization is done automatically when we define\n#       the model (aka architecture)\nmodel = nn.Sequential(\n            nn.Linear(input_dim, output_dim), \n            nn.Sigmoid())\ncriterion = nn.MSELoss() \nlearning_rate = 1.0\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\nnum_epochs = 250\n\ntraining_losses, validation_losses = [], []","c377f3f2":"for i in tqdm(range(num_epochs)):\n    # Reset the gradient after every epoch. \n    optimizer.zero_grad() \n    # Step 2: Foward Propagation\n    predictions = model(X_train)\n    \n    # Step 3: Back Propagation \n    # Calculate the cost between the predictions and the truth.\n    loss_this_epoch = criterion(predictions, Y_train)\n    # Note: The neat thing about PyTorch is it does the \n    #       auto-gradient computation, no more manually defining\n    #       derivative of functions and manually propagating\n    #       the errors layer by layer.\n    loss_this_epoch.backward()\n    \n    # Step 4: Optimizer take a step. \n    # Note: Previously, we have to manually update the \n    #       weights of each layer individually according to the\n    #       learning rate and the layer delta. \n    #       PyTorch does that automatically =)\n    optimizer.step()  # The model has been updated in this epoch.\n    \n    # Log the loss value as we proceed through the epochs.\n    training_losses.append(loss_this_epoch.data.item())\n    # Bonus: On top of just logging the training loss. \n    # First, use the new model (after optimizer updates the weights)\n    # And put it through a forward propagation without gradient.\n    with torch.no_grad():\n        valid_predictions = model(X_valid)\n        valid_loss = criterion(valid_predictions, Y_valid)\n        validation_losses.append(valid_loss)\n\n# Visualize the losses\nplt.plot(training_losses, label='Train loss')\nplt.plot(validation_losses, label='Valid loss')\nplt.legend(loc='upper right')\nplt.show()","b35ffaa7":"def repeat_experiments(X_train, Y_train, X_valid, Y_valid):\n    \n    # Step 1: Initialization. \n    # Note: When using PyTorch a lot of the manual weights\n    #       initialization is done automatically when we define\n    #       the model (aka architecture)\n    model = nn.Sequential(\n                nn.Linear(input_dim, output_dim), \n                nn.Sigmoid())\n    criterion = nn.MSELoss() \n    learning_rate = 1.0\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n    num_epochs = 250\n\n    training_losses, validation_losses = [], []\n    \n    for i in tqdm(range(num_epochs)):\n        # Reset the gradient after every epoch. \n        optimizer.zero_grad() \n        # Step 2: Foward Propagation\n        predictions = model(X_train)\n\n        # Step 3: Back Propagation \n        # Calculate the cost between the predictions and the truth.\n        loss_this_epoch = criterion(predictions, Y_train)\n        # Note: The neat thing about PyTorch is it does the \n        #       auto-gradient computation, no more manually defining\n        #       derivative of functions and manually propagating\n        #       the errors layer by layer.\n        loss_this_epoch.backward()\n\n        # Step 4: Optimizer take a step. \n        # Note: Previously, we have to manually update the \n        #       weights of each layer individually according to the\n        #       learning rate and the layer delta. \n        #       PyTorch does that automatically =)\n        optimizer.step()\n\n        # Log the loss value as we proceed through the epochs.\n        training_losses.append(loss_this_epoch.data.item())\n        # Bonus: On top of just logging the training loss. \n        # First, use the new model (after optimizer updates the weights)\n        # And put it through a forward propagation without gradient.\n        with torch.no_grad():\n            valid_predictions = model(X_valid)\n            valid_loss = criterion(valid_predictions, Y_valid)\n            validation_losses.append(valid_loss)\n        \n    # Visualize the losses\n    plt.plot(training_losses, label='Train loss')\n    plt.plot(validation_losses, label='Valid loss')\n    plt.legend(loc='upper right')\n    plt.show()\n\n","b8eb76c5":"# Create some random data.\nX = torch.rand(100, 3) # 100 data points, with 3 dimensions\/features each.\nY = torch.rand(100)    # 100 corresponding y values for the data points.\n\nX_train, X_valid, X_test, Y_train, Y_valid, Y_test = train_valid_test_split(X, Y, valid_size=0.20, test_size=0.10)\n","8b2cbde2":"for i in range(3):\n    torch.manual_seed(i)\n    repeat_experiments(X_train, Y_train, X_valid, Y_valid)","f7001956":"# IPython candies...\nfrom IPython.display import Image\nfrom IPython.core.display import HTML\n\nfrom IPython.display import clear_output","d1eef4db":"from collections import namedtuple\n\nimport numpy as np\nfrom tqdm import tqdm\n\nimport pandas as pd\n\nfrom gensim.corpora import Dictionary\n\nimport torch\nfrom torch import nn, optim, tensor, autograd\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","a933ab8b":"from nltk import word_tokenize, sent_tokenize \n\nimport os\nimport requests\nimport io #codecs\n\n\n# Text version of https:\/\/kilgarriff.co.uk\/Publications\/2005-K-lineer.pdf\nif os.path.isfile('language-never-random.txt'):\n    with io.open('language-never-random.txt', encoding='utf8') as fin:\n        text = fin.read()\nelse:\n    url = \"https:\/\/gist.githubusercontent.com\/alvations\/53b01e4076573fea47c6057120bb017a\/raw\/b01ff96a5f76848450e648f35da6497ca9454e4a\/language-never-random.txt\"\n    text = requests.get(url).content.decode('utf8')\n    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n        fout.write(text)\n        \n# Tokenize the text.\ntokenized_text = [list(map(str.lower, word_tokenize(sent))) \n                  for sent in sent_tokenize(text)]","0359b54d":"class KilgariffDataset(nn.Module):\n    def __init__(self, texts):\n        self.texts = texts\n        \n        # Initialize the vocab \n        special_tokens = {'<pad>': 0, '<unk>':1, '<s>':2, '<\/s>':3}\n        self.vocab = Dictionary(texts)\n        self.vocab.patch_with_special_tokens(special_tokens)\n        \n        # Keep track of the vocab size.\n        self.vocab_size = len(self.vocab)\n        \n        # Keep track of how many data points.\n        self._len = len(texts)\n        \n        # Find the longest text in the data.\n        self.max_len = max(len(txt) for txt in texts) \n        \n    def __getitem__(self, index):\n        vectorized_sent = self.vectorize(self.texts[index])\n        x_len = len(vectorized_sent)\n        # To pad the sentence:\n        # Pad left = 0; Pad right = max_len - len of sent.\n        pad_dim = (0, self.max_len - len(vectorized_sent))\n        vectorized_sent = F.pad(vectorized_sent, pad_dim, 'constant')\n        return {'x':vectorized_sent[:-1], \n                'y':vectorized_sent[1:], \n                'x_len':x_len}\n    \n    def __len__(self):\n        return self._len\n    \n    def vectorize(self, tokens, start_idx=2, end_idx=3):\n        \"\"\"\n        :param tokens: Tokens that should be vectorized. \n        :type tokens: list(str)\n        \"\"\"\n        # See https:\/\/radimrehurek.com\/gensim\/corpora\/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n        # Lets just cast list of indices into torch tensors directly =)\n        \n        vectorized_sent = [start_idx] + self.vocab.doc2idx(tokens) + [end_idx]\n        return torch.tensor(vectorized_sent)\n    \n    def unvectorize(self, indices):\n        \"\"\"\n        :param indices: Converts the indices back to tokens.\n        :type tokens: list(int)\n        \"\"\"\n        return [self.vocab[i] for i in indices]","fc5b6e20":"# Initialize the dataset object.\nnum_sents = len(tokenized_text)\n\n# 20% of test.\nstart_of_test_split = -1 * int(0.2 * num_sents)\n# 20% of valid.\nstart_of_valid_split = -1 * int(0.4 * num_sents)\n\ntrain_text = tokenized_text[:start_of_valid_split]\nvalid_text = tokenized_text[start_of_valid_split:start_of_test_split]\ntest_text = tokenized_text[start_of_test_split:]\n\ntrain_dataset = KilgariffDataset(train_text)\nvalid_dataset = KilgariffDataset(valid_text)\ntest_dataset = KilgariffDataset(test_text)\n\n# Another way to split the dataset is using \n# https:\/\/pytorch.org\/docs\/master\/data.html#torch.utils.data.SubsetRandomSampler\n\n# When training, take a batch of size 15.\nbatch_size = 15\ntrain_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n\n# When validating and testing, lets do the whole validation set as a batch\n# When validate\/test set is small, just fit in the whole validation\/test set as a one batch.\nvalid_dataloader = DataLoader(dataset=valid_dataset, batch_size=len(valid_dataset), shuffle=True) \ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset), shuffle=True)","f70f6ade":"class LanguageModel(nn.Module):\n    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers, dropout=0.8):\n        super(LanguageModel, self).__init__()\n\n        # Initialize the embedding layer with the \n        # - size of input (i.e. no. of words in input vocab)\n        # - no. of hidden nodes in the embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        \n        # Initialize the GRU with the \n        # - size of the input (i.e. embedding layer)\n        # - size of the hidden layer \n        self.gru = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True)\n        \n        # Initialize the \"classifier\" layer to map the RNN outputs\n        # to the vocabulary. Remember we need to -1 because the \n        # vectorized sentence we left out one token for both x and y:\n        # - size of hidden_size of the GRU output.\n        # - size of vocabulary\n        self.classifier = nn.Linear(hidden_size, vocab_size)\n        \n        # Global dropout.\n        self.dropout = dropout\n        \n    def forward(self, inputs, use_softmax=False, hidden=None):\n        # Look up for the embeddings for the input word indices.\n        embedded = self.embedding(inputs)\n        ##embedded = F.dropout(embedded, self.dropout)\n        # Put the embedded inputs into the GRU.\n        output, hidden = self.gru(embedded, hidden)\n        \n        # Matrix manipulation magic.\n        batch_size, sequence_len, hidden_size = output.shape\n        # Technically, linear layer takes a 2-D matrix as input, so more manipulation...\n        output = output.contiguous().view(batch_size * sequence_len, hidden_size)\n        # Apply dropout.\n        output = F.dropout(output, self.dropout)\n        # Put it through the classifier\n        # And reshape it to [batch_size x sequence_len x vocab_size]\n        output = self.classifier(output).view(batch_size, sequence_len, -1)\n        \n        return (F.softmax(output,dim=2), hidden) if use_softmax else (output, hidden)\n        ","f8b04306":"\n# Training routine.\ndef train(num_epochs, train_dataloader, valid_dataloader, model, criterion, optimizer):\n    training_losses, validation_losses = [], []\n    plt.ion()\n    for _e in range(num_epochs):\n        for batch in tqdm(train_dataloader):\n            # Zero gradient.\n            optimizer.zero_grad()\n            x = batch['x'].to(device)\n            x_len = batch['x_len'].to(device)\n            y = batch['y'].to(device)\n            # Feed forward. \n            output, hidden = model(x, use_softmax=False)\n            # Compute loss:\n            # Shape of the `output` is [batch_size x sequence_len x vocab_size]\n            # Shape of `y` is [batch_size x sequence_len]\n            # CrossEntropyLoss expects `output` to be [batch_size x vocab_size x sequence_len]\n            _, prediction = torch.max(output, dim=2)\n            loss = criterion(output.permute(0, 2, 1), y)\n            loss.backward()\n            optimizer.step()\n            training_losses.append(loss.float().data)\n            \n            with torch.no_grad():\n                valid_x = next(iter(valid_dataloader))['x'].to(device)\n                valid_y = next(iter(valid_dataloader))['y'].to(device)\n                # Forward propagation on validation set. \n                # Set the model to eval() mode, no gradient, no dropout.\n                model.eval()\n                output, hidden = model(valid_x, use_softmax=False)\n                valid_loss = criterion(output.permute(0, 2, 1), valid_y)\n                validation_losses.append(valid_loss.float().data)\n                # Rest the model to train() mode.\n                model.train()\n                \n        # Visualize the losses\n        clear_output(wait=True)\n        plt.plot(training_losses, label='Train loss')\n        plt.plot(validation_losses, label='Valid loss')\n        plt.legend(loc='upper right')\n        plt.pause(0.05)\n","e5847552":"def initialize_data_model_optim_loss(hyperparams):\n    # Loss function.\n    criterion = hyperparams.loss_func(ignore_index=train_dataset.vocab.token2id['<pad>'], \n                                      reduction='mean')\n\n    # Model.\n    model = LanguageModel(len(train_dataset.vocab), hyperparams.embed_size, \n                      hyperparams.hidden_size, hyperparams.num_layers, dropout=hyperparams.dropout).to(device)\n\n    # Optimizer.\n    optimizer = hyperparams.optimizer(model.parameters(), lr=hyperparams.learning_rate)\n    \n    return model, optimizer, criterion","35bd5e67":"# Set some hyperparameters.\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n_hyper = ['embed_size', 'hidden_size', 'num_layers',\n          'loss_func', 'learning_rate', 'optimizer', 'dropout']\nHyperparams = namedtuple('Hyperparams', _hyper)\n\n\nhyperparams = Hyperparams(embed_size=250, hidden_size=250, num_layers=1,\n                          loss_func=nn.CrossEntropyLoss,\n                          learning_rate=0.03, optimizer=optim.Adam, dropout=0.5)","3a06b52e":"# Initialize the dataset object.\nnum_sents = len(tokenized_text)\n\n# 20% of test.\nstart_of_test_split = -1 * int(0.2 * num_sents)\n# 20% of valid.\nstart_of_valid_split = -1 * int(0.4 * num_sents)\n\ntrain_text = tokenized_text[:start_of_valid_split]\nvalid_text = tokenized_text[start_of_valid_split:start_of_test_split]\ntest_text = tokenized_text[start_of_test_split:]\n\ntrain_dataset = KilgariffDataset(train_text)\nvalid_dataset = KilgariffDataset(valid_text)\ntest_dataset = KilgariffDataset(test_text)\n\n# When training, take a batch of size 50.\nbatch_size = 100\ntrain_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n\n# When validating and testing, lets do the whole validation set as a batch\nvalid_dataloader = DataLoader(dataset=valid_dataset, batch_size=len(valid_dataset), shuffle=True)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset), shuffle=True)","a2e39a3c":"# Experiment 1. \n\nhyperparams = Hyperparams(embed_size=128, hidden_size=128, num_layers=3,\n                          loss_func=nn.CrossEntropyLoss,\n                          learning_rate=3e-5, optimizer=optim.Adam, dropout=0.5)\n\nmodel, optimizer, criterion = initialize_data_model_optim_loss(hyperparams)\n\ntrain(150, train_dataloader, valid_dataloader, model, criterion, optimizer)","66fd1c85":"model # 140th epoch\n\ntorch.save(model, '140-epoch.pth')","b8a2c33c":"test_x = next(iter(test_dataloader))['x'].to(device)\ntest_y = next(iter(test_dataloader))['y'].to(device)\n# Forward propagation on validation set. \n# Set the model to eval() mode, no gradient, no dropout.\nmodel.eval()\noutput, hidden = model(test_x, use_softmax=False)\nprint(criterion(output.permute(0, 2, 1), test_y)) # This is the cross-entropy loss score!! Perplexity = **** crossentropy =)\n\n\n\n","7d798cfa":"def generate_example(model, temperature=1.0, max_len=100, hidden_state=None):\n    start_token, start_idx = '<s>', 2\n    # Start state.\n    inputs = torch.tensor(train_dataset.vocab.token2id[start_token]).unsqueeze(0).unsqueeze(0).to(device)\n\n    sentence = [start_token]\n    i = 0\n    while i < max_len and sentence[-1] not in ['<\/s>', '<pad>']:\n        i += 1\n        \n        embedded = model.embedding(inputs)\n        output, hidden_state = model.gru(embedded, hidden_state)\n\n        batch_size, sequence_len, hidden_size = output.shape\n        output = output.contiguous().view(batch_size * sequence_len, hidden_size)    \n        output = model.classifier(output).view(batch_size, sequence_len, -1).squeeze(0)\n        #_, prediction = torch.max(F.softmax(output, dim=2), dim=2)\n        \n        word_weights = output.div(temperature).exp().cpu()\n        if len(word_weights.shape) > 1:\n            word_weights = word_weights[-1] # Pick the last word.    \n        word_idx = torch.multinomial(word_weights, 1).view(-1)\n        \n        sentence.append(train_dataset.vocab[int(word_idx)])\n        \n        inputs = tensor([train_dataset.vocab.token2id[word] for word in sentence]).unsqueeze(0).to(device)\n    print(' '.join(sentence))","34f86190":"generate_example(model, max_len=20) # 140th epoch","b6286e6f":"hyperparams = Hyperparams(embed_size=64, hidden_size=64, num_layers=3,\n                          loss_func=nn.CrossEntropyLoss,\n                          learning_rate=3e-5, optimizer=optim.Adam, dropout=0.5)\n\nmodel, optimizer, criterion = initialize_data_model_optim_loss(hyperparams)\n\ntrain(150, train_dataloader, valid_dataloader, model, criterion, optimizer)","0ddef901":"train(50, train_dataloader, valid_dataloader, model, criterion, optimizer)","4b87bf84":"train(50, train_dataloader, valid_dataloader, model, criterion, optimizer)","33253c80":"hyperparams = Hyperparams(embed_size=64, hidden_size=64, num_layers=6,\n                          loss_func=nn.CrossEntropyLoss,\n                          learning_rate=3e-3, optimizer=optim.Adam, dropout=0.7)\n\nmodel, optimizer, criterion = initialize_data_model_optim_loss(hyperparams)\n\ntrain(150, train_dataloader, valid_dataloader, model, criterion, optimizer)","eb084ad7":"hyperparams = Hyperparams(embed_size=64, hidden_size=64, num_layers=2,\n                          loss_func=nn.CrossEntropyLoss,\n                          learning_rate=0.0003, optimizer=optim.Adam, dropout=0.5)\n\nmodel, optimizer, criterion = initialize_data_model_optim_loss(hyperparams)\n\ntrain(150, train_dataloader, valid_dataloader, model, criterion, optimizer)","d0d84bbc":"hyperparams = Hyperparams(embed_size=64, hidden_size=64, num_layers=2,\n                          loss_func=nn.CrossEntropyLoss,\n                          learning_rate=0.00003, optimizer=optim.Adam, dropout=0.8)\n\nmodel, optimizer, criterion = initialize_data_model_optim_loss(hyperparams)\n\ntrain(150, train_dataloader, valid_dataloader, model, criterion, optimizer)","c1240f04":"# When training, take a batch of size 50.\nbatch_size = 200\ntrain_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n\n# When validating and testing, lets do the whole validation set as a batch\nvalid_dataloader = DataLoader(dataset=valid_dataset, batch_size=len(valid_dataset), shuffle=True)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset), shuffle=True)","3d33fd36":"len(train_dataset)","564f9d13":"hyperparams = Hyperparams(embed_size=64, hidden_size=64, num_layers=3,\n                          loss_func=nn.CrossEntropyLoss,\n                          learning_rate=3e-5, optimizer=optim.Adam, dropout=0.7)\n\nmodel, optimizer, criterion = initialize_data_model_optim_loss(hyperparams)\n\ntrain(150, train_dataloader, valid_dataloader, model, criterion, optimizer)","54728e44":"train(150, train_dataloader, valid_dataloader, model, criterion, optimizer)","fdedaabd":"len(train)","68631554":"# Hey, all 3 models are reporting different numbers!! \n\nYes, now you see that different random initialization leads to similar but different models. ","bfcc910b":"# Hold-Out Evaluation\n\nThe most typical evaluation method is to hold out: \n\n - a test set that the model will ***NEVER*** see during the training phase. \n - a validation set that the model will use to check against certain metric but never use to train the model\n - a training set that is the only subset of the data that the model will use to train (i.e. update the parameters of the models)\n\n<img src=\"https:\/\/ibin.co\/5G1RQyn3rEJj.png\" width=\"600\">","57fc3dca":"### Jedi Cut-away\n\n**Yoda:** Some sort of reality check with unseen data, we need. (i.e. data not used to train the model \/ update the parameters of the model)\n\n**Count Dooku:** Ah, the validation set!!\n\n#### \\[10 seconds later\\]\n\n# \uc7a0\uae50\ub9cc... (wait a minute) \n\n\n**Count Dooku:** You haven't taught us how to use the validation splits!! \n\n**Yoda:** But forward propagation and loss computation learnt, you have. \n\n**Obi-Wan:** I don't believe you.\n\n**Count Dooku:**  Master Kenobi, you disappoint me. Yoda holds you in such high esteem. Surely you can do better!\n\n#### \\[And Dooku started coding, as follows...\\]","d52c5ef4":"# Heh? How would you get the `validation_losses`?","c153ec56":"\n# \uc5b4\ub5a1\ud574?! (What should I do?!)\n\n### That's a whole lot of information but you still haven't %^&*-ing tell me how to use validation set on NLP task!!! \n\nAlright, alright. Lets do the hold-out validation set on language modelling. We'll re-use the code from [GRU Language Model](https:\/\/www.kaggle.com\/alvations\/gru-language-model) exercise.","fa4c4c4f":"### Then we iterate through different random seeds with the same X-Y pairs.","75be278b":"# Introduction\n\n> ***\"Your eyes can deceive you. Don't trust them.\"*** <br>\n> \\- Obi-Wan Kenobi, Star Wars: Episode IV - A New Hope (1977)\n\nThis notebook introduces the notion of data splits that are usually used in Natural Language Processing (NLP). \n\n\n\n","7c5d9372":"# Recall that, previously, we roll out our own torch Dataset object.","f378355c":"# \ub098 \uae30\uc5ed\ud574? (Remember me?) Simple GRU Langauge Model","f77454e0":"# Previously, we will just use the full `tokenized_text` to create the dataset.\n\n# What if we split it into train \/ valid \/ test sets?\n\nSimplest way to split the data is to split by sentence =)\n","1e40e8c5":"## Putting everything together.\n\nWe can formalize the train\/valid\/test splits with a `train_valid_test_split` function.\n    ","6845e63a":"# Okay, okay, so you manage to get a validation loss, but the plot is useless!! \n\nActually, the plot is rather informative. Remember when we initialize the model, the linear layer parameters are \"randomly\" initialized. \n\n```python\nmodel = nn.Sequential(\n            nn.Linear(input_dim, output_dim), \n            nn.Sigmoid())\n```\n\nIt's not exactly random but its [Kaiming initialization (aka. He initialization)](https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/nn\/modules\/linear.py#L33) a robust initialization techniques that is known to train models well from the [He et al. (2015)](https:\/\/arxiv.org\/abs\/1502.01852)\n\nSo what the plot is saying: \n\n - when the model was initialized, it's actually doing pretty well in guessing the Y values given the X, credits most probably goes to Kaiming init.\n - as the model trains, the models fits the training data better and gets worse at predicting in the validation data\n - so the model cannot **generalize**\n \nIt's not bad that the model can automatically learn the mappings of randomly created X to Ys. \n\n","3433e99a":"# Lets get some text data!","4f27d6bc":"### But what happens if we don't fix the random seed for torch?\n\nFirst lets encapsulate the experiments.","cb1bd329":"## Wow, we trained a model with 99.933% accuracy to guess a randomly generated Ys!!! \n\n### What black magic is this? \n\n#### Does that mean a randomly generator can be automatically learned from a single layer perceptron?!\n\n##### Did we just crack the [random problem](https:\/\/www.wired.com\/story\/quantum-mechanics-could-solve-cryptographys-random-number-problem\/)?! \n\n\n> ***It's a trap!*** <br>\n> \\- Admiral Akbar, Star Wars: Episode VI - Return of the Jedi (1983)\n\nDon't be fooled. Of course the loss is getting lower as we iterate through the same 70 data points in the training dataset, the model will automatically learn the best parameters\/weights to multiply the inputs `X`s such that the prediction is getting closer to the truth value `Y`s with every epoch. \n","41f2432d":"# Now, apply what you learnt on validation loss to the model training routine!\n\n\nWe can re-use what we learn from the random number model =)\n\n```\n# Bonus: On top of just logging the training loss. \n# First, use the new model (after optimizer updates the weights)\n# And put it through a forward propagation without gradient.\nwith torch.no_grad():\n    valid_predictions = model(X_valid)\n    valid_loss = criterion(valid_predictions, Y_valid)\n    validation_losses.append(valid_loss)\n\n\n```\n\n","cdab7fcc":"## Can we train a model to learn to map the randomly created Xs and Ys?\n\nLets borrow the simple perceptron code from our [Hello World](https:\/\/github.com\/alvations\/tsundoku\/blob\/master\/completed\/Session%201%20-%20Hello%20DL%20World.ipynb) exercise =)"}}