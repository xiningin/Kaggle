{"cell_type":{"338b6ba4":"code","41fcabb9":"code","abca2f12":"code","255837b7":"code","1ad338f6":"code","6c62b531":"code","8c88b531":"code","dcb065af":"code","5e7398a8":"code","5b31d996":"code","2a78ed2b":"code","26b43699":"code","2e43e201":"code","85f7d8a8":"markdown","828c555e":"markdown"},"source":{"338b6ba4":"import pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm_notebook\n\nimport zipfile\nfrom subprocess import check_output","41fcabb9":"print(check_output([\"ls\", \"..\/input\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\"]).decode(\"utf8\"))","abca2f12":"# This will not working because of limitation\n# zip_path = '..\/input\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\/train.zip'\n# with zipfile.ZipFile(zip_path,\"r\") as z:\n#     z.extractall('')","255837b7":"# Getting list of files in zip file\nzip_path = '..\/input\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\/train.zip'\nwith zipfile.ZipFile(zip_path,\"r\") as z:\n    file_list = z.namelist()\n\n# #Filtering csv files\n# file_list = [file_name for file_name in file_list if '.csv' in file_name ]\n# print(file_list[:10]) ","1ad338f6":"#Extract Alice log\nwith zipfile.ZipFile(zip_path,\"r\") as z:\n    z.extract('train\/Alice_log.csv')\n    \n#Extract first 100 files of other users\nLIMIT = 100\nstep = 0\nwith zipfile.ZipFile(zip_path,\"r\") as z:\n    for file in z.namelist():        \n        if file.startswith('train\/other_user_logs\/'):\n            z.extract(file)\n            step += 1\n        if step == LIMIT:\n            break\n            \n            ","6c62b531":"print(check_output([\"ls\", \"train\"]).decode(\"utf8\"))","8c88b531":"# If you run this notebok locally and you have already unzipped folder manually, specify your paths here\nalice_path = 'train\/Alice_log.csv'\nother_user_path = 'train\/other_user_logs'","dcb065af":"def prepare_data(path, target):\n    raw_df = pd.read_csv(path)\n    \n    site_names = ['site{}'.format(i) for i in range(1, 11)]\n    time_names = ['time{}'.format(i) for i in range(1, 11)]\n    feature_names = [None] * (2 * len(site_names))\n    feature_names[1::2] = time_names\n    feature_names[0::2] = site_names\n    \n    # prepare 30-min steps\n    raw_df['timestamp'] = pd.to_datetime(raw_df['timestamp'])\n    time_diff = raw_df['timestamp'].diff().astype(int)\n    time_diff = np.where(time_diff < 0, np.nan, time_diff)\n    raw_df['min_diff'] = time_diff\/(1e9*60) # nanoseconds to minutes \n    raw_df['min_diff'].fillna(0, inplace = True)\n    raw_df['min_cumsum'] = raw_df['min_diff'].cumsum()\n    raw_df['step'] = (raw_df['min_cumsum']\/\/30).astype(int)\n    \n    step_list = raw_df['step'].unique()\n    \n    stacking_list = []\n    for step in step_list:\n        temp_part = raw_df[raw_df['step'] == step][['timestamp', 'site']].to_numpy()\n        \n        # infill matrix by NaN`s\n        if temp_part.shape[0]%10 != 0:\n            temp_padding = np.full((10 - temp_part.shape[0]%10, 2), np.nan)\n            temp_part = np.vstack([temp_part, temp_padding])\n\n        # https:\/\/stackoverflow.com\/questions\/3678869\/pythonic-way-to-combine-two-lists-in-an-alternating-fashion\n        temp_combine = [None] * (2 * len(temp_part))\n        temp_combine[::2] = temp_part[:, 1]\n        temp_combine[1::2] = temp_part[:, 0]\n\n        temp_result = np.array(temp_combine).reshape((-1, 20))\n        stacking_list.append(temp_result)\n\n    data = np.vstack(stacking_list)\n    \n    df = pd.DataFrame(data, columns = feature_names)\n    df['target'] = np.full(df.shape[0], target)\n    \n    return df  ","5e7398a8":"%%time\nalice_df = prepare_data(alice_path, target = 1)","5b31d996":"files_list = sorted([file for file in os.listdir(other_user_path) if 'csv' in file])\nother_users_df = pd.DataFrame(columns = alice_df.columns)\n\n\nfor file_name in tqdm_notebook(files_list):\n    temp_df = prepare_data(os.path.join(other_user_path, file_name), target = 0)\n    other_users_df = pd.concat([other_users_df, temp_df])\n    \nother_users_df.reset_index(drop = True, inplace = True)\n    ","2a78ed2b":"result_df = pd.concat([alice_df, other_users_df]).sort_values('time1')\n\nresult_df.reset_index(drop = True, inplace = True)\nresult_df['session_id'] = result_df.index.to_numpy() + 1","26b43699":"#reorder columns\ncolumns = result_df.columns.to_list()\nresult_df = result_df[['session_id'] + columns[:-1]]\nresult_df.head()","2e43e201":"# result_df.to_csv('Data\/additional_train_data.csv')","85f7d8a8":"Kaggle has a strong limitation of output files amount (500). So, in this notebook I will use only first 100 of unzipped files. \nIf you want to use this kernel on all data, run it locally. In this case skip cells with zipfile and manually specify pathways.","828c555e":"How to extract data from zipped files on kaggle: <br>\nhttps:\/\/www.kaggle.com\/mchirico\/how-to-read-datasets"}}