{"cell_type":{"9059f6c6":"code","acc1e19a":"code","a51bd3af":"code","57fa17d8":"code","00ab04cb":"code","f99bd826":"code","5508cbe7":"code","69a4d584":"code","9247fa3a":"code","88467f79":"code","601eb428":"code","0490b159":"code","c42f8791":"code","bb9a1282":"code","31750631":"code","0c0be872":"code","6298a5ae":"code","3fbbb008":"code","1de19fc3":"code","4d52a1b1":"code","958df6a9":"code","0eb58822":"code","cc666327":"code","37218712":"code","21afc3da":"code","59c84ad9":"code","13fa510c":"markdown"},"source":{"9059f6c6":"%cd \/kaggle\/working","acc1e19a":"! wget http:\/\/images.cocodataset.org\/zips\/val2017.zip","a51bd3af":"!unzip val2017.zip","57fa17d8":"import os\nimport tensorflow as tf\nimport numpy as np\nimport PIL\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport tensorflow\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import add,Dense\nfrom tensorflow.keras.layers import UpSampling2D\nfrom tensorflow.keras.layers import LeakyReLU, PReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Activation,Flatten\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow.keras.backend as K\nfrom IPython.display import display\nfrom skimage.transform import rescale, resize\n","00ab04cb":"hr_shape=(384,384,3)\nlr_shape=(hr_shape[0]\/4,hr_shape[1]\/4,hr_shape[2])","f99bd826":"## Loading Data ##\n\ndef load_data(directory):\n  images = []\n  count = 0\n  for img in os.listdir(directory):\n    \n    if(count==700):\n      break\n\n    img = PIL.Image.open(os.path.join(directory,img)).convert('RGB')\n    img_resized=np.array(img.resize((hr_shape[0],hr_shape[1]),PIL.Image.BICUBIC))\n    images.append(img_resized)\n    count+=1\n  \n  return images\n\ndirectory = \"\/kaggle\/working\/val2017\"\ndata = load_data(directory)\n                                                # data is a list containing path of 5k images\nX_train = data[0:500]                           # used 1000 images for training\nX_test = data[500:700]                        # used 280 images for validation or testing\n\n","5508cbe7":"print(X_train[270].shape)","69a4d584":"\n# Takes list of images and provide HR images in form of numpy array\ndef high_res_images(images):\n    HR_images = np.array(images)\n    return HR_images                              # Shape (np_of_images, H, W, no_of_channels)\n\n# Takes list of images and provide LR images in form of numpy array\n# i.e., Downsampling function\n# use downscale = 4\n    \ndef low_res_images(images_real , downscale):\n\n    images = []\n    for img in  range(len(images_real)):\n        img1 = PIL.Image.fromarray(images_real[img])\n        img2 = img1.resize((images_real[img].shape[0]\/\/downscale,images_real[img].shape[1]\/\/downscale),PIL.Image.BICUBIC)\n\n        images.append(np.array(img2))\n\n    images_lr = np.array(images)\n    return images_lr\n\ndef normalize_img(input_data):\n  return (input_data.astype(np.float32) - 127.5)\/127.5 \n\ndef denormalize(input_data):\n    input_data = (input_data + 1) * 127.5\n    return input_data.astype(np.uint8) \n\n","9247fa3a":"print(len(X_train))","88467f79":"print(len(X_test))","601eb428":"X_train_hr = high_res_images(X_train)\nX_train_hr = normalize_img(X_train_hr)\n\nX_train_lr = low_res_images(X_train, 4)\nX_train_lr = normalize_img(X_train_lr)             ### Need some work to denormalize and converting it back to unit8 before imshow()\n\n\nX_test_hr = high_res_images(X_test)\nX_test_hr = normalize_img(X_test_hr)\n\nX_test_lr = low_res_images(X_test, 4)\nX_test_lr = normalize_img(X_test_lr)","0490b159":"###Upsample function(changing from H * W * C to H * W * 4C then to 2H * 2W * C using pixelshuffling)\ndef upsample(model,filter_size,no_of_channels,strides):\n  #scaling factor=2\n  scale=2\n  no_of_filters=no_of_channels *(scale ** 2)\n  model=Conv2D(filters=no_of_filters,kernel_size=filter_size,strides=strides,padding='same')(model)\n  model=UpSampling2D(size=scale)(model)\n  model=PReLU()(model)\n  return model","c42f8791":"\ndef residual_block(model, kernel_size, no_of_filters, strides):\n    \n    gen = model\n    model = Conv2D(filters = no_of_filters, kernel_size = kernel_size, strides = strides, padding = \"same\")(model)\n    model = BatchNormalization(momentum = 0.5)(model)\n    \n    # Using Parametric ReLU\n    model = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(model)\n    model = Conv2D(filters = no_of_filters, kernel_size = kernel_size, strides = strides, padding = \"same\")(model)\n    model = BatchNormalization(momentum = 0.5)(model)\n        \n    model = add([gen, model])\n    \n    return model\n","bb9a1282":"# Using Functional API of Keras\n\ndef generator_network(gen_input):\n\n  gen_input = Input(shape = gen_input)\n     \n  model = Conv2D(filters = 64, kernel_size = 9, strides = 1, padding = \"same\")(gen_input)\n  model = PReLU(shared_axes=[1,2])(model)               # each filter only has one set of parameters\n  model_part1 = model\n        \n  # Using 16 Residual Blocks\n  for i in range(16):\n      model = residual_block(model, 3, 64, 1)\n      # 16 residual blocks with skip connections \n\n  model = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = \"same\")(model)\n  model = BatchNormalization(momentum = 0.5)(model)\n  model = add([model_part1, model])                      \n  #  Element wise of model_part1 and model after 16 residual blocks\n     \n\n  for i in range(2):\n      model = upsample(model, 3, 64, 1)  #no of channels=64  \n     \n  model = Conv2D(filters = 3, kernel_size = 9, strides = 1, padding = \"same\")(model)\n  \n  model = Activation('tanh')(model)                  # tanh activation in last layer\n    \n  gen_model = Model(inputs = gen_input, outputs = model)     # specifying the input and output to the model\n  \n  return gen_model","31750631":"def conv_disc_block(model,filters,filter_size,strides):\n  model=Conv2D(filters=filters,kernel_size=filter_size,strides=strides,padding='same')(model)\n  model = BatchNormalization(momentum = 0.5)(model)\n  model=LeakyReLU(alpha=0.1)(model)\n  return model","0c0be872":"def discriminator_network(image_shape):\n  disc_input=Input(shape = image_shape)\n  #convolution layer(k3n64s1)\n  model=Conv2D(filters = 64,kernel_size = 3,strides=1,padding='same' )(disc_input)\n  #Activation-leaky relu\n  model=LeakyReLU(alpha=0.1)(model)\n  \n  #discriminator block (k3n64s2)\n  model=conv_disc_block(model,64,3,2)\n  #discriminator block (k3n128s1)\n  model=conv_disc_block(model,128,3,1)\n  #discriminator block (k3n128s2)\n  model=conv_disc_block(model,128,3,2)\n  #discriminator block (k3n256s1)\n  model=conv_disc_block(model,256,3,1)\n  #discriminator block (k3n256s2)\n  model=conv_disc_block(model,256,3,2)\n  #discriminator block (k3n512s1)\n  model=conv_disc_block(model,512,3,1)\n  #discriminator block (k3n512s2)\n  model=conv_disc_block(model,512,3,2)\n\n  #for dense layer input should be column vector\/flatten\n  model=Flatten()(model)\n  #dense layer with 1024 nodes\n  model=Dense(1024)(model)\n  #Activation-leaky relu\n  model=LeakyReLU(alpha=0.1)(model)\n\n  #dense layer with 1 nodes\n  model=Dense(1)(model)\n  #Activation-sigmoid\n  model=Activation('sigmoid')(model)\n  disc_model=Model(inputs=disc_input,outputs=model)\n  return disc_model","6298a5ae":"# To use mean, square we need to use keras.backend\n# For content loss, compare the results which the vgg19 provides which feeding the y_true and y_pred\n\ndef content_loss(image_shape):\n  def loss( y_true, y_pred):\n      vgg19 = VGG19(include_top=False, weights='imagenet', input_shape=image_shape)\n      vgg19.trainable = False\n      for layer in vgg19.layers:\n          layer.trainable = False\n      model = Model(inputs=vgg19.input, outputs=vgg19.get_layer('block5_conv4').output)\n      model.trainable = False\n      return K.mean(K.square(model(y_true) - model(y_pred)))\n  return loss  ","3fbbb008":"def gan_network(generator_model,discriminator_model,shape):\n  discriminator_model.trainable = False\n  gan_input=Input(shape=shape)\n  print(\"input\")\n  SR=generator_model(gan_input)\n  print(\"SR\")\n  gan_output=discriminator_model(SR)\n  print(\"gan_output\")\n  model=Model(inputs=gan_input,outputs=[SR,gan_output])\n  return model","1de19fc3":"import os\n\nos.makedirs('\/kaggle\/working\/Super-Resolve', exist_ok = True)","4d52a1b1":"\n\ndef plot_generated_images(epoch,generator, examples=3 , dim=(1, 3), figsize=(15, 5)):  \n    \n    rand_nums = np.random.randint(0, X_test_hr.shape[0], size=examples)\n    image_batch_hr = denormalize(X_test_hr[rand_nums])\n    image_batch_lr = X_test_lr[rand_nums]\n    gen_img = generator.predict(image_batch_lr)\n    generated_image = denormalize(gen_img)\n    image_batch_lr = denormalize(image_batch_lr)\n    \n    \n    plt.figure(figsize=figsize)\n    \n    plt.subplot(dim[0], dim[1], 1)\n    plt.imshow(image_batch_lr[1], interpolation='nearest')\n    plt.axis('off')\n        \n    plt.subplot(dim[0], dim[1], 2)\n    plt.imshow(generated_image[1], interpolation='nearest')\n    plt.axis('off')\n    \n    plt.subplot(dim[0], dim[1], 3)\n    plt.imshow(image_batch_hr[1], interpolation='nearest')\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('\/kaggle\/working\/Super-Resolve\/gan_generated_image_epoch_%d.png' % epoch)\n ","958df6a9":"np.random.seed(10)","0eb58822":"hr_shape","cc666327":"from tqdm import tqdm\n\ndef train_model(batch_size,epochs):\n  no_of_batches=X_train_hr.shape[0]\/\/batch_size\n  adam = Adam(lr=0.0001 ,beta_1=0.9 ,beta_2=0.999, epsilon=1e-08 )\n  discriminator_model = discriminator_network(hr_shape)\n  generator_model = generator_network(lr_shape)\n  generator_model.compile(loss=content_loss(hr_shape), optimizer=adam)\n  discriminator_model.compile(loss='binary_crossentropy',optimizer=adam)\n  \n  gan_model=gan_network(generator_model,discriminator_model,lr_shape)\n  discriminator_model.trainable = False\n  gan_model.compile(loss=[content_loss(hr_shape),'binary_crossentropy'],loss_weights=[1.0,1e-3],optimizer=adam)\n\n\n  for i in range(0,epochs):\n    print(\"\\nEpoch    : \"+ str(i))\n\n    for j in range(no_of_batches):\n\n      print(\"Batch    : \"+str(j))\n\n      rand_nums = np.random.randint(0, X_train_hr.shape[0], size=batch_size)\n      \n      image_batch_hr = X_train_hr[rand_nums]\n      image_batch_lr = X_train_lr[rand_nums]\n\n      batch_gen_sr = generator_model.predict(image_batch_lr)\n      real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2      ## Here we use concept of label smoothing\n      fake_data_Y = np.random.random_sample(batch_size)*0.2\n\n      discriminator_model.trainable = True\n      d_loss_real = discriminator_model.train_on_batch(image_batch_hr, real_data_Y)\n      d_loss_fake = discriminator_model.train_on_batch(batch_gen_sr, fake_data_Y)\n      d_loss = 0.5*np.add(d_loss_fake, d_loss_real)\n\n      discriminator_model.trainable = False     \n      gan_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n      loss_gan = gan_model.train_on_batch(image_batch_lr, [image_batch_hr,gan_data_Y])\n\n\n    print(\"discriminator_loss : %f\" % d_loss)\n    print(\"gan_loss :\", loss_gan)\n    loss_gan = str(loss_gan)\n    \n    loss_file = open( '\/kaggle\/working\/losses.txt' , 'a')\n    loss_file.write('epoch%d : gan_loss = %s ; discriminator_loss = %f\\n' %(i, loss_gan, d_loss) )\n    loss_file.close()\n\n    plot_generated_images(i, generator_model)\n\n    generator_model.save('\/kaggle\/working\/Super-Resolve\/gen_model.h5')\n    discriminator_model.save('\/kaggle\/working\/Super-Resolve\/dis_model.h5')","37218712":"tf.config.experimental_run_functions_eagerly(True)","21afc3da":"lr_shape = tuple(map(int, lr_shape))","59c84ad9":"# Not Enough RAM to train on kaggle, but I have successfully trained it on colab\n# To train with batch size 4 and epochs 50\n\n# train_model(4, 50)","13fa510c":"**Super-Resolving an Image by a factor of 4**"}}