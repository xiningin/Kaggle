{"cell_type":{"4ada1545":"code","52146505":"code","210463f2":"code","b10c1e4d":"code","f7f4bf75":"code","9ca6dac9":"code","84841577":"code","23f89acb":"code","931832e0":"code","68b685db":"code","80d72c3d":"code","8329e108":"code","59f291bb":"code","e0d5e945":"code","8f45a74a":"code","5da06948":"code","1f397f05":"code","23a7db72":"code","8de07638":"code","320d33ab":"code","e0c8eb32":"code","1856bece":"code","a6ca5251":"code","5b8793e9":"code","246d7a73":"code","4951ca0a":"code","88dd75d0":"code","6b54cfeb":"code","775594bd":"code","735687c2":"code","2d0b688c":"code","f5b2a89c":"code","248c875b":"code","c8edc2cd":"code","4462d090":"code","e794f935":"code","d98d863c":"code","d68c3e6e":"code","ac5f6e20":"code","866df4c6":"code","f03db682":"code","13f37af1":"markdown","404885ee":"markdown","7cff39e2":"markdown","67bd8eb8":"markdown","31cffacb":"markdown","9a1b93c5":"markdown","27109255":"markdown","391b8ea3":"markdown","868f5e13":"markdown","c15e0c6c":"markdown","6ad95f41":"markdown","8b056254":"markdown","91db7b26":"markdown","d30d5c37":"markdown","9f10283d":"markdown","fdbb1323":"markdown","5812f82a":"markdown","8dbad6c7":"markdown","a8b972e3":"markdown","4d29a0ab":"markdown"},"source":{"4ada1545":"# we will verify that GPU is enabled for this notebook\n# following should print: CUDA is available!  Training on GPU ...\n# \n# if it prints otherwise, then you need to enable GPU: \n# from Menu > Runtime > Change Runtime Type > Hardware Accelerator > GPU\n\nimport torch\nimport numpy as np\n\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","52146505":"from google.colab import drive\ndrive.mount('\/content\/gdrive\/')","210463f2":"!ls ","b10c1e4d":"!unzip -q  \/content\/gdrive\/MyDrive\/journey-springfield.zip -d dataset","f7f4bf75":"!nvidia-smi\nimport torch\ntorch.cuda.is_available()","9ca6dac9":"import pickle\nimport numpy as np\nfrom skimage import io\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom PIL import Image\nfrom pathlib import Path\n\nfrom torchvision import transforms\nfrom multiprocessing.pool import ThreadPool\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\nfrom matplotlib import colors, pyplot as plt\n%matplotlib inline\n\n# \u0432 sklearn \u043d\u0435 \u0432\u0441\u0435 \u0433\u043b\u0430\u0434\u043a\u043e, \u0447\u0442\u043e\u0431\u044b \u0432 colab \u0443\u0434\u043e\u0431\u043d\u043e \u0432\u044b\u0432\u043e\u0434\u0438\u0442\u044c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \n# \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0433\u043d\u043e\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c warnings\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\n","84841577":"# \u0440\u0430\u0437\u043d\u044b\u0435 \u0440\u0435\u0436\u0438\u043c\u044b \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \nDATA_MODES = ['train', 'val', 'test']\n# \u0432\u0441\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0431\u0443\u0434\u0443\u0442 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u043a \u0440\u0430\u0437\u043c\u0435\u0440\u0443 224x224 px\nRESCALE_SIZE = 224\n# \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u043c \u043d\u0430 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0435\nDEVICE = torch.device(\"cuda\")","23f89acb":"class SimpsonsDataset(Dataset):\n    \"\"\"\n    \u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0441 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u043c\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u0430\u0440\u0430\u043b\u0435\u043b\u044c\u043d\u043e \u043f\u043e\u0434\u0433\u0440\u0443\u0436\u0430\u0435\u0442 \u0438\u0445 \u0438\u0437 \u043f\u0430\u043f\u043e\u043a\n    \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442 \u0441\u043a\u0430\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0438 \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u0432 \u0442\u043e\u0440\u0447\u0435\u0432\u044b\u0435 \u0442\u0435\u043d\u0437\u043e\u0440\u044b\n    \"\"\"\n    def __init__(self, files, mode):\n        super().__init__()\n        # \u0441\u043f\u0438\u0441\u043e\u043a \u0444\u0430\u0439\u043b\u043e\u0432 \u0434\u043b\u044f \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438\n        self.files = sorted(files)\n        # \u0440\u0435\u0436\u0438\u043c \u0440\u0430\u0431\u043e\u0442\u044b\n        self.mode = mode\n\n        if self.mode not in DATA_MODES:\n            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n            raise NameError\n\n        self.len_ = len(self.files)\n     \n        self.label_encoder = LabelEncoder()\n\n        if self.mode != 'test':\n            self.labels = [path.parent.name for path in self.files]\n            self.label_encoder.fit(self.labels)\n\n            with open('label_encoder.pkl', 'wb') as le_dump_file:\n                  pickle.dump(self.label_encoder, le_dump_file)\n                      \n    def __len__(self):\n        return self.len_\n      \n    def load_sample(self, file):\n        image = Image.open(file)\n        image.load()\n        return image\n  \n    def __getitem__(self, index):\n        # \u0434\u043b\u044f \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u044b PyTorch \u0438 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0432\u0445\u043e\u0434\u0430\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n        ])\n        data_transforms = transforms.Compose([\n        transforms.ToTensor(),\n        #transforms.RandomGrayscale(p=0.1),\n        #transforms.RandomAffine(degrees = 20),\n        #transforms.RandomPerspective(),\n        #transforms.RandomErasing(),     \n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        transforms.RandomVerticalFlip(p=0.5),\n        transforms.RandomHorizontalFlip(p=0.5),])\n        x = self.load_sample(self.files[index])\n        x = self._prepare_sample(x)\n        x = np.array(x \/ 255, dtype='float32')\n        if self.mode == 'train':\n          x = data_transforms(x)\n        else:\n          x = transform(x)\n        if self.mode == 'test':\n            return x\n        else:\n            label = self.labels[index]\n            label_id = self.label_encoder.transform([label])\n            y = label_id.item()\n            return x, y\n        \n    def _prepare_sample(self, image):\n        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n        return np.array(image)","931832e0":"def imshow(inp, title=None, plt_ax=plt, default=False):\n    \"\"\"Imshow \u0434\u043b\u044f \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt_ax.imshow(inp)\n    if title is not None:\n        plt_ax.set_title(title)\n    plt_ax.grid(False)","68b685db":"TRAIN_DIR = Path('\/content\/dataset\/train\/simpsons_dataset')\nTEST_DIR = Path('\/content\/dataset\/testset\/testset')\n\ntrain_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\ntest_files = sorted(list(TEST_DIR.rglob('*.jpg')))\ntrain_val_labels = [path.parent.name for path in train_val_files]","80d72c3d":"\nTRAIN_DIR = Path(r'..\/input\/journey-springfield\/train\/simpsons_dataset')\nTEST_DIR = Path(r'..\/input\/journey-springfield\/testset\/testset')\n\ntrain_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\ntest_files = sorted(list(TEST_DIR.rglob('*.jpg')))\ntrain_val_labels = [path.parent.name for path in train_val_files]","8329e108":"train_val_files = []\nd ={}\nfor i in train_val_labels:\n  if i in d:\n    d[i] += 1\n  else:\n    d[i] = 1\n\n\nfor i in d:\n  f_path = Path(f'..\/input\/journey-springfield\/train\/simpsons_dataset\/{i}')\n  train_val_files.extend(list(f_path.rglob('*.jpg'))* (2246\/\/d[i]))\n\nlen(train_val_files)","59f291bb":"42 * 2246","e0d5e945":"from sklearn.model_selection import train_test_split\n\ntrain_val_labels = [path.parent.name for path in train_val_files]\ntrain_files, val_files = train_test_split(train_val_files, test_size=0.4, \\\n                                          stratify=train_val_labels,shuffle = True)","8f45a74a":"val_dataset = SimpsonsDataset(val_files, mode='val')","5da06948":"fig, ax = plt.subplots(nrows=4, ncols=4,figsize=(18, 18), \\\n                        sharey=True, sharex=True)\nfor fig_x in ax.flatten():\n    random_characters = int(np.random.uniform(0,1000))\n    im_val, label = val_dataset[random_characters]\n    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n    imshow(im_val.data.cpu(), \\\n          title=img_label,plt_ax=fig_x)","1f397f05":"# \u041e\u0447\u0435\u043d\u044c \u043f\u0440\u043e\u0441\u0442\u0430\u044f \u0441\u0435\u0442\u044c\nclass SimpleCnn(nn.Module):\n  \n    def __init__(self, n_classes):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=96, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        \n        self.out = nn.Linear(96 * 5 * 5, n_classes)\n  \n  \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n\n        x = x.view(x.size(0), -1)\n        logits = self.out(x)\n        return logits","23a7db72":"def fit_epoch(model, train_loader, criterion, optimizer):\n    running_loss = 0.0\n    running_corrects = 0\n    processed_data = 0\n  \n    for inputs, labels in train_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        preds = torch.argmax(outputs, 1)\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_data += inputs.size(0)\n              \n    train_loss = running_loss \/ processed_data\n    train_acc = running_corrects.cpu().numpy() \/ processed_data\n    return train_loss, train_acc","8de07638":"def eval_epoch(model, val_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    running_corrects = 0\n    processed_size = 0\n\n    for inputs, labels in val_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n\n        with torch.set_grad_enabled(False):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            preds = torch.argmax(outputs, 1)\n\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_size += inputs.size(0)\n    val_loss = running_loss \/ processed_size\n    val_acc = running_corrects.double() \/ processed_size\n    return val_loss, val_acc","320d33ab":"def train(train_files, val_files, model, epochs, batch_size):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n        opt = torch.optim.AdamW(model.parameters())\n        criterion = nn.CrossEntropyLoss()\n\n        for epoch in range(epochs):\n            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n            print(\"loss\", train_loss)\n            \n            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n            history.append((train_loss, train_acc, val_loss, val_acc))\n            \n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n            \n    return history","e0c8eb32":"def predict(model, test_loader):\n    with torch.no_grad():\n        logits = []\n    \n        for inputs in test_loader:\n            inputs = inputs.to(DEVICE)\n            model.eval()\n            outputs = model(inputs).cpu()\n            logits.append(outputs)\n            \n    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n    return probs","1856bece":"import torchvision.models as models\nn_classes = len(np.unique(train_val_labels))\nsimple_cnn = models.resnet50(pretrained=True)\n#simple_cnn = models.resnet152(pretrained=True)\n#simple_cnn = models.alexnet(pretrained=True)\n#simple_cnn = models.vgg19_bn(pretrained=True)\nfor param in simple_cnn.parameters():\n    param.requires_grad = False\n#num_ftrs = simple_cnn.classifier[6].in_features\nnum_ftrs = simple_cnn.fc.in_features\nsimple_cnn.fc = nn.Linear(num_ftrs,n_classes)\n#simple_cnn.classifier[6] = nn.Linear(num_ftrs, n_classes)","a6ca5251":"n_classes = len(np.unique(train_val_labels))\nsimple_cnn = SimpleCnn(n_classes).to(DEVICE)\n#simple_cnn = simple_cnn.to(DEVICE)\nprint(\"we will classify :{}\".format(n_classes))\nprint(simple_cnn)","5b8793e9":"if val_dataset is None:\n    val_dataset = SimpsonsDataset(val_files, mode='val')\n    \ntrain_dataset = SimpsonsDataset(train_files, mode='train')","246d7a73":"history = train(train_dataset, val_dataset, model=simple_cnn, epochs=30, batch_size=256)","4951ca0a":"torch.save(simple_cnn,'\/content\/gdrive\/MyDrive\/Kaggle_simpsons\/res50.pth')\ntorch.save(simple_cnn.state_dict(), '\/content\/gdrive\/MyDrive\/Kaggle_simpsons\/res501.pth')","88dd75d0":"!home","6b54cfeb":"loss, acc, val_loss, val_acc = zip(*history)","775594bd":"plt.figure(figsize=(15, 9))\nplt.plot(loss, label=\"train_loss\")\nplt.plot(val_loss, label=\"val_loss\")\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()","735687c2":"def predict_one_sample(model, inputs, device=DEVICE):\n    \"\"\"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435, \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0439 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\"\"\"\n    with torch.no_grad():\n        inputs = inputs.to(device)\n        model.eval()\n        logit = model(inputs).cpu()\n        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n    return probs","2d0b688c":"random_characters = int(np.random.uniform(0,1000))\nex_img, true_label = val_dataset[random_characters]\nprobs_im = predict_one_sample(simple_cnn, ex_img.unsqueeze(0))","f5b2a89c":"idxs = list(map(int, np.random.uniform(0,1000, 20)))\nimgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n\nprobs_ims = predict(simple_cnn, imgs)","248c875b":"label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))","c8edc2cd":"y_pred = np.argmax(probs_ims,-1)\n\nactual_labels = [val_dataset[id][1] for id in idxs]\n\npreds_class = [label_encoder.classes_[i] for i in y_pred]","4462d090":"from sklearn.metrics import f1_score\n\nf1_score(actual_labels, y_pred, average='micro')\n","e794f935":"import matplotlib.patches as patches\nfrom matplotlib.font_manager import FontProperties\n\nfig, ax = plt.subplots(nrows=3, ncols=3,figsize=(12, 12), \\\n                        sharey=True, sharex=True)\nfor fig_x in ax.flatten():\n    random_characters = int(np.random.uniform(0,1000))\n    im_val, label = val_dataset[random_characters]\n    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n    \n    \n\n    imshow(im_val.data.cpu(), \\\n          title=img_label,plt_ax=fig_x)\n    \n    actual_text = \"Actual : {}\".format(img_label)\n            \n    fig_x.add_patch(patches.Rectangle((0, 53),86,35,color='white'))\n    font0 = FontProperties()\n    font = font0.copy()\n    font.set_family(\"fantasy\")\n    prob_pred = predict_one_sample(simple_cnn, im_val.unsqueeze(0))\n    predicted_proba = np.max(prob_pred)*100\n    y_pred = np.argmax(prob_pred)\n    \n    predicted_label = label_encoder.classes_[y_pred]\n    predicted_label = predicted_label[:len(predicted_label)\/\/2] + '\\n' + predicted_label[len(predicted_label)\/\/2:]\n    predicted_text = \"{} : {:.0f}%\".format(predicted_label,predicted_proba)\n            \n    fig_x.text(1, 59, predicted_text , horizontalalignment='left', fontproperties=font,\n                    verticalalignment='top',fontsize=8, color='black',fontweight='bold')","d98d863c":"test_dataset = SimpsonsDataset(test_files, mode=\"test\")\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\nprobs = predict(simple_cnn, test_loader)\n\n\npreds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\ntest_filenames = [path.name for path in test_dataset.files]\n","d68c3e6e":"! ls ","ac5f6e20":"import pandas as pd\nmy_submit = pd.read_csv(\"\/content\/dataset\/sample_submission.csv\")\nmy_submit = pd.DataFrame({'Id': test_filenames, 'Expected': preds})\nmy_submit.head()","866df4c6":"# TODO : \u0441\u0434\u0435\u043b\u0430\u0439\u0442\u0435 \u0441\u0430\u0431\u043c\u0438\u0442 (\u044d\u0442\u043e \u0432\u0430\u0436\u043d\u043e, \u0435\u0441\u043b\u0438 \u0412\u044b \u043d\u0435 \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0435\u0441\u044c, \u043d\u043e \u0434\u043e\u0448\u043b\u0438 \u0434\u043e \u044d\u0442\u043e\u0439 \u044f\u0447\u0435\u0439\u043a\u0438, \u0442\u043e \u0441\u043e\u043e\u0431\u0449\u0438\u0442\u0435 \u0432 \u0447\u0430\u0442 \u0438 \u0412\u0430\u043c \u043f\u043e\u043c\u043e\u0433\u0443\u0442)","f03db682":"my_submit.to_csv('\/content\/dataset\/simple_cnn_baseline_my.csv', index=False)","13f37af1":"\u041d\u0438\u0436\u0435 \u043c\u044b \u0438\u0441\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432\u0440\u0430\u043f\u043f\u0435\u0440 \u043d\u0430\u0434 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u043c \u0434\u043b\u044f \u0443\u0434\u043e\u0431\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u044b. \u0412\u0430\u043c \u0441\u0442\u043e\u0438\u0442 \u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c, \u0447\u0442\u043e \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u0441 LabelEncoder \u0438  \u0441 torch.Transformation. \n\nToTensor \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u0442  PIL Image \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \u0432 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0435 [0, 255] (\u043a\u0430\u043a \u0432\u0441\u0435 \u043f\u0438\u043a\u0441\u0435\u043b\u0438) \u0432 FloatTensor \u0440\u0430\u0437\u043c\u0435\u0440\u0430 (C x H x W) [0,1] , \u0437\u0430\u0442\u0435\u043c \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0441\u044f \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435:\n$input = \\frac{input - \\mu}{\\text{standard deviation}} $, <br>       \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u044b - \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0438 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u043f\u043e \u043a\u0430\u043d\u0430\u043b\u0430\u043c \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 ImageNet\n\n\n\u0421\u0442\u043e\u0438\u0442 \u0442\u0430\u043a\u0436\u0435 \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c, \u0447\u0442\u043e \u043c\u044b \u043f\u0435\u0440\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u043c\u0435\u0442\u043e\u0434 __getitem__ \u0434\u043b\u044f \u0443\u0434\u043e\u0431\u0441\u0442\u0432\u0430 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0434\u0430\u043d\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u043e\u0439 \u0434\u0430\u043d\u043d\u044b\u0445.\n \u0422\u0430\u043a\u0436\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f LabelEncoder \u0434\u043b\u044f \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u044b\u0445 \u043c\u0435\u0442\u043e\u043a \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 id \u0438 \u043e\u0431\u0440\u0430\u0442\u043d\u043e. \u0412 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0443\u043a\u0430\u0437\u0430\u043d\u043e, \u0447\u0442\u043e \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0440\u0430\u0437\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430, \u0442\u0430\u043a \u043a\u0430\u043a \u0431\u0440\u0430\u043b\u0438\u0441\u044c \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0441 \u0432\u0438\u0434\u0435\u043e, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0441\u043b\u0435\u0434\u0443\u0435\u043c \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u0438\u0445 \u043a \u043e\u0434\u043d\u043e\u043c\u0443 \u0440\u0430\u0437\u043c\u0435\u0440 (\u044d\u0442\u043e \u0434\u0435\u043b\u0430\u0435\u0442 \u043c\u0435\u0442\u043e\u0434  _prepare_sample) ","404885ee":"### \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0435\u0439","7cff39e2":"![alt text](https:\/\/i.redd.it\/nuaphfioz0211.jpg)","67bd8eb8":"## \u041f\u0440\u0438\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435?\n\n\u0410 \u0442\u0435\u043f\u0435\u0440\u044c \u0441\u0430\u043c\u043e\u0435 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e\u0435, \u043c\u044b \u0441\u0434\u0435\u043b\u0430\u043b\u0438 \u043f\u0440\u043e\u0441\u0442\u0435\u043d\u044c\u043a\u0443\u044e \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u0443\u044e \u0441\u0435\u0442\u044c \u0438 \u0441\u043c\u043e\u0433\u043b\u0438 \u043e\u0442\u043f\u0440\u0430\u0432\u0438\u0442\u044c \u0441\u0430\u0431\u043c\u0438\u0442, \u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0439\u0441\u044f \u0441\u043a\u043e\u0440 \u043d\u0430\u0441 \u044f\u0432\u043d\u043e \u043d\u0435 \u0443\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u0435\u0442. \u041d\u0430\u0434\u043e \u0441 \u044d\u0442\u0438\u043c \u0447\u0442\u043e-\u0442\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c. \n\n\u041d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u0440\u043e\u0447\u043d\u044b\u0439\u0445 \u0443\u043b\u0443\u0447\u0448\u0435\u0439\u043d\u0438 \u0434\u043b\u044f \u043d\u0430\u0448\u0435\u0439 \u0441\u0435\u0442\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0430\u0432\u0435\u0440\u043d\u044f\u043a\u0430 \u043f\u0440\u0438\u0448\u043b\u0438 \u0412\u0430\u043c \u0432 \u0433\u043e\u043b\u043e\u0432\u0443: \n\n\n*   \u0423\u0447\u0438\u043c \u0434\u043e\u043b\u044c\u0448\u0435 \u0438 \u0438\u0437\u043c\u0435\u043d\u044f\u0435\u043c \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0441\u0435\u0442\u0438\n*  learning rate, batch size, \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0438 \u0432\u043e\u0442 \u044d\u0442\u043e \u0432\u0441\u0451\n*   \u041a\u0442\u043e \u0436\u0435 \u0442\u0430\u043a \u0441\u0442\u0440\u043e\u0438\u0442 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0435 \u0441\u0435\u0442\u0438? \u0410 \u0433\u0434\u0435 \u043f\u0443\u043b\u0438\u043d\u0433\u0438 \u0438 \u0431\u0430\u0442\u0447 \u043d\u043e\u0440\u043c\u044b? \u041d\u0430\u0434\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c\n*  \u041d\u0443 \u0440\u0430\u0437\u0432\u0435 \u0410\u0434\u0430\u043c \u043d\u0430\u0448\u0435 \u0432\u0441\u0435? [adamW](https:\/\/www.fast.ai\/2018\/07\/02\/adam-weight-decay\/) \u0434\u043b\u044f \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0430, [\u0441\u0442\u0430\u0442\u0435\u0439\u043a\u0430 \u0434\u043b\u044f \u043b\u044e\u0431\u0438\u0442\u0435\u043b\u0435\u0439](https:\/\/openreview.net\/pdf?id=ryQu7f-RZ) (\u043e\u0447\u0435\u043d\u044c \u0445\u043e\u0440\u043e\u0448\u0438\u0439 \u0430\u043d\u0430\u043b\u0438\u0437), [\u043d\u0430\u0448\u0438 ](https:\/\/github.com\/MichaelKonobeev\/adashift\/) \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b \u0434\u043b\u044f \u0437\u0430\u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u043d\u043d\u044b\u0445.\n\n* \u041d\u0443 \u0440\u0430\u0437\u0432\u0435 \u044d\u0442\u043e deep learning? \u0412\u043e\u0442 ResNet \u0438 Inception, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0436\u043d\u043e \u0437\u0430\u0444\u0430\u0439\u043d\u0442\u044c\u044e\u043d\u0438\u0442\u044c \u043f\u043e\u0434 \u043d\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435, \u0432\u043e\u0442 \u044d\u0442\u043e \u044f \u043f\u043e\u043d\u0438\u043c\u0430\u044e (\u043c\u043e\u0436\u043d\u043e \u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u0432 \u043a\u043e\u043b\u0430\u0431\u0435, \u0430 \u043c\u043e\u0436\u043d\u043e \u0438 [\u0433\u043e\u0442\u043e\u0432\u044b\u0435](https:\/\/github.com\/Cadene\/pretrained-models.pytorch) \u0441\u043a\u0430\u0447\u0430\u0442\u044c).\n\n* \u0414\u0430\u043d\u043d\u044b\u0445 \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u043c\u043d\u043e\u0433\u043e, \u043c\u043e\u0436\u043d\u043e \u0438\u0445 \u0430\u0443\u0433\u0443\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438  \u0434\u043e\u0443\u0447\u0438\u0442\u0438\u0442\u044c\u0441\u044f \u043d\u0430 \u043d\u043e\u0432\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 ( \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0443\u0436\u0435 \u0431\u0443\u0434\u0435\u0442 \u0441\u043e\u0441\u0442\u043e\u044f\u0442\u044c \u0438\u0437, \u043a\u0430\u043a  \u043f\u0440\u0438\u043c\u0435\u0440 \u0430\u0443\u0433\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438, \u043f\u0435\u0440\u0435\u0432\u0435\u0440\u043d\u0443\u0442\u044b\u0445 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439)\n\n* \u0421\u0442\u043e\u0438\u0442 \u043f\u043e\u0434\u0443\u043c\u0430\u0442\u044c \u043e\u0431 \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u044f\u0445\n\n\n\u041d\u0430\u0434\u0435\u044e\u0441\u044c, \u0447\u0442\u043e \u0443 \u0412\u0430\u0441 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u0441\u044f!\n\n![alt text](https:\/\/pbs.twimg.com\/profile_images\/798904974986113024\/adcQiVdV.jpg)\n","31cffacb":"\u0417\u0430\u043f\u0443\u0441\u0442\u0438\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0441\u0435\u0442\u0438.","9a1b93c5":"\u0412 \u043d\u0430\u0448\u0435\u043c \u0442\u0435\u0441\u0442\u0435 \u0431\u0443\u0434\u0435\u0442 990 \u043a\u0430\u0440\u0442\u043d\u043e\u043a, \u0434\u043b\u044f \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0432\u0430\u043c \u0431\u0443\u0434\u0435\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u043a\u043b\u0430\u0441\u0441.","27109255":"\u0425\u043e\u0440\u043e\u0448\u043e \u0431\u044b \u043f\u043e\u043d\u044f\u0442\u044c, \u043a\u0430\u043a \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0441\u0430\u0431\u043c\u0438\u0442. \n\u0423 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u0441\u0435\u0442\u044c \u0438 \u043c\u0435\u0442\u043e\u0434\u044b eval \u0443 \u043d\u0435\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u044e\u0442 \u043f\u0435\u0440\u0435\u0432\u0435\u0441\u0442\u0438 \u0441\u0435\u0442\u044c \u0432 \u0440\u0435\u0436\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f. \u0421\u0442\u043e\u0438\u0442 \u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c, \u0447\u0442\u043e \u0443 \u043d\u0430\u0448\u0435\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u043c \u0441\u043b\u043e\u0435 \u0441\u0442\u043e\u0438\u0442 softmax, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0432\u0435\u043a\u0442\u043e\u0440 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439  \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u043e\u0431\u044a\u0435\u043a\u0442 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0441\u044f \u043a \u0442\u043e\u043c\u0443 \u0438\u043b\u0438 \u0438\u043d\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443. \u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u044d\u0442\u0438\u043c.","391b8ea3":"\u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043d\u0430\u0448\u0438\u0445 \u0433\u0435\u0440\u043e\u0435\u0432 \u0432\u043d\u0443\u0442\u0440\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430.","868f5e13":"https:\/\/jhui.github.io\/2018\/02\/09\/PyTorch-Data-loading-preprocess_torchvision\/\n","c15e0c6c":"### Submit \u043d\u0430 Kaggle","6ad95f41":"\u041c\u043e\u0436\u0435\u0442\u0435 \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0432\u0430\u0448\u0438 \u043b\u044e\u0431\u0438\u043c\u044b\u0435 \u0441\u0446\u0435\u043d\u044b \u0438 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438\u0445. (\u0432\u0435\u0441\u0435\u043b\u044b\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043c\u043e\u0436\u043d\u043e \u043a\u0438\u0434\u0430\u0442\u044c \u0432 \u0447\u0430\u0442)","8b056254":"![alt text](https:\/\/www.indiewire.com\/wp-content\/uploads\/2014\/08\/the-simpsons.jpg)","91db7b26":"### \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438\n\n\u0417\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c \u0431\u0443\u0434\u0435\u0442 \u0432\u0430\u0448\u0438\u043c \u043c\u0438\u043d\u0438-\u0437\u0430\u0434\u0430\u043d\u0438\u0435\u043c \u043d\u0430 \u043f\u0435\u0440\u0432\u0443\u044e \u043d\u0435\u0434\u0435\u043b\u044e, \u0447\u0442\u043e\u0431\u044b \u0431\u044b\u043b\u043e \u043f\u0440\u043e\u0449\u0435 \u0443\u0447\u0430\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u0432 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0438.\n\n\u0414\u0430\u043d\u043d\u0430\u044f \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430 \u0431\u0443\u0434\u0435\u0442 \u043e\u0447\u0435\u043d\u044c \u043f\u0440\u043e\u0441\u0442\u043e\u0439 \u0438 \u043d\u0443\u0436\u043d\u0430 \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c \u0431\u0430\u0437\u043e\u0432\u043e\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043f\u0440\u043e\u0441\u0442\u0435\u043d\u044c\u043a\u0438\u0439 \u0441\u0430\u0431\u043c\u0438\u0442 \u043d\u0430 Kaggle\n\n<!-- \u0417\u0434\u0435\u0441\u044c \u0432\u0430\u043c \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f \u0434\u043e\u043f\u0438\u0441\u0430\u0442\u044c \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u0443\u044e \u0441\u0435\u0442\u044c \u0433\u043b\u0443\u0431\u0438\u043d\u044b 4\/5.  -->\n\n*\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0441\u043b\u043e\u0435\u0432*:\n\n\n\n1. \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0432\u0445\u043e\u0434\u0430: 3x224x224 \n2.\u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u0441\u043b\u0435 \u0441\u043b\u043e\u044f:  8x111x111\n3. 16x54x54\n4. 32x26x26\n5. 64x12x12\n6. \u0432\u044b\u0445\u043e\u0434: 96x5x5\n","d30d5c37":"\n\n## **\u0424\u0438\u0437\u0442\u0435\u0445-\u0428\u043a\u043e\u043b\u0430 \u041f\u0440\u0438\u043a\u043b\u0430\u0434\u043d\u043e\u0439 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0438 \u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u043a\u0438 (\u0424\u041f\u041c\u0418) \u041c\u0424\u0422\u0418**","9f10283d":"### \u041d\u0443 \u0438 \u0447\u0442\u043e \u0442\u0435\u043f\u0435\u0440\u044c \u0441\u043e \u0432\u0441\u0435\u043c \u044d\u0442\u0438\u043c \u0434\u0435\u043b\u0430\u0442\u044c?","fdbb1323":"\u0421\u0434\u0435\u043b\u0430\u0435\u043c \u043a\u043b\u0430\u0441\u0441\u043d\u0443\u044e \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e,  \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u0435\u0442\u044c \u0443\u0432\u0435\u0440\u0435\u043d\u0430 \u0432 \u0441\u0432\u043e\u0438\u0445 \u043e\u0442\u0432\u0435\u0442\u0430\u0445. \u041c\u043e\u0436\u0435\u0442\u0435 \u0438\u0441\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u044d\u0442\u043e, \u0447\u0442\u043e\u0431\u044b \u043e\u0442\u043b\u0430\u0436\u0438\u0432\u0430\u0442\u044c \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0432\u044b\u0432\u043e\u0434\u0430.","5812f82a":"\u041e\u0431\u0440\u0430\u0442\u0438\u0442\u0435 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435, \u0447\u0442\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0430, \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0432 \u043a\u043e\u043d\u043a\u0443\u0440\u0441\u0435 --- f1-score. \u0412\u044b\u0447\u0438\u0441\u043b\u0438\u043c \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u043c\u0435\u0442\u0440\u0438\u043a\u0443 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435.","8dbad6c7":"# \u041f\u0443\u0442\u0435\u0448\u0435\u0441\u0442\u0432\u0438\u0435 \u043f\u043e \u0421\u043f\u0440\u0438\u043d\u0433\u0444\u0438\u043b\u0434\u0443.\n\n\n\u0421\u0435\u0433\u043e\u0434\u043d\u044f \u0432\u0430\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u043e\u0438\u0442\u044c \u043f\u043e\u043c\u043e\u0447\u044c \u0442\u0435\u043b\u0435\u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 FOX  \u0432 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0438\u0445 \u043a\u043e\u043d\u0442\u0435\u043d\u0442\u0430. \u041a\u0430\u043a \u0432\u044b \u0437\u043d\u0430\u0435\u0442\u0435 \u0441\u0435\u0440\u0438\u0430\u043b \u0421\u0438\u043c\u0441\u043e\u043d\u044b \u0438\u0434\u0435\u0442 \u043d\u0430 \u0442\u0435\u043b\u0435\u044d\u043a\u0440\u0430\u043d\u0430\u0445 \u0431\u043e\u043b\u0435\u0435 25 \u043b\u0435\u0442 \u0438 \u0437\u0430 \u044d\u0442\u043e \u0432\u0440\u0435\u043c\u044f \u0441\u043a\u043e\u043f\u0438\u043b\u043e\u0441\u044c \u043e\u0447\u0435\u043d\u044c \u043c\u043d\u043e\u0433\u043e \u0432\u0438\u0434\u0435\u043e \u043c\u0430\u0442\u0435\u0440\u0438\u0430\u043b\u0430. \u041f\u0435\u0440\u0441\u043e\u043e\u043d\u0430\u0436\u0438 \u043c\u0435\u043d\u044f\u043b\u0438\u0441\u044c \u0432\u043c\u0435\u0441\u0442\u0435 \u0441 \u0438\u0437\u043c\u0435\u043d\u044f\u044e\u0449\u0438\u043c\u0438\u0441\u044f \u0433\u0440\u0430\u0444\u0438\u0447\u0435\u0441\u043a\u0438\u043c\u0438 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u044f\u043c\u0438   \u0438 \u0413\u043e\u043c\u0435\u0440 2018 \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u043f\u043e\u0445\u043e\u0436 \u043d\u0430 \u0413\u043e\u043c\u0435\u0440\u0430 1989. \u041d\u0430\u0448\u0435\u0439 \u0437\u0430\u0434\u0430\u0447\u0435\u0439 \u0431\u0443\u0434\u0435\u0442 \u043d\u0430\u0443\u0447\u0438\u0442\u044c\u0441\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0435\u0440\u0441\u043e\u043d\u0430\u0436\u0435\u0439 \u043f\u0440\u043e\u0436\u0438\u0432\u0430\u044e\u0449\u0438\u0445 \u0432 \u0421\u043f\u0440\u0438\u043d\u0433\u0444\u0438\u043b\u0434\u0435. \u0414\u0443\u043c\u0430\u044e, \u0447\u0442\u043e \u043d\u0435\u0442 \u0441\u043c\u044b\u0441\u043b\u0430 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u043d\u0438\u0445 \u0432 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438.\n\n\n\n ![alt text](https:\/\/vignette.wikia.nocookie.net\/simpsons\/images\/5\/5a\/Spider_fat_piglet.png\/revision\/latest\/scale-to-width-down\/640?cb=20111118140828)\n\n","a8b972e3":"\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u043a\u0440\u0438\u0432\u044b\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f","4d29a0ab":"\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u043d\u0430\u0439\u0442\u0438 \u0442\u0435 \u043a\u043b\u0430\u0441\u0441\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0435\u0442\u044c \u043d\u0435 \u0441\u043c\u043e\u0433\u043b\u0430 \u0440\u0430\u0441\u0441\u043f\u043e\u0437\u043d\u0430\u0442\u044c. \u0418\u0437\u0443\u0447\u0438\u0442\u0435 \u0434\u0430\u043d\u043d\u0443\u044e \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0443, \u044d\u0442\u043e \u043f\u043e\u043d\u0430\u0434\u043e\u0431\u0438\u0442\u0441\u044f \u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u043c."}}