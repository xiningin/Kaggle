{"cell_type":{"a430d93e":"code","f434efb8":"code","1964d88b":"code","956d496d":"code","6e24ac18":"code","26692eb8":"code","a84cb3bc":"code","55cfcd2d":"code","be4bfa75":"code","4e0ebe84":"code","5e74bb25":"code","a3652bff":"code","97455d9c":"code","6d48b72a":"code","0b6619ea":"code","cacc42f3":"code","51a38540":"code","eddff44d":"code","b849f1c4":"code","72333dcc":"code","68ec82ec":"code","a595972c":"code","e774dde5":"code","75bea95f":"code","d2e8e66c":"code","5f6763a2":"code","57af4c86":"code","6584f943":"code","be95472f":"code","9a5b4aaf":"code","820031ca":"code","ff88863c":"code","a2e29d1d":"code","e105e938":"code","c092b3e7":"code","32c1306a":"code","cb264a52":"code","efa7df05":"code","078fb383":"code","db834fcc":"code","9f01ac75":"code","27b4d578":"code","c5477646":"code","120574ca":"code","6df87e81":"code","f19586c5":"code","b11f30b7":"code","543d55a3":"code","f60f509e":"code","1dd50494":"code","713237b7":"code","35686574":"code","d1575e2b":"code","bd7724fe":"code","e30b0c85":"code","1c1bcb2d":"code","09e29f24":"code","46298d47":"code","4a39b48d":"markdown","6d4c4a0e":"markdown","462f3988":"markdown","d9910521":"markdown","6475748a":"markdown","cbc1bd8f":"markdown","3046971e":"markdown","20690fb9":"markdown","9b5e038f":"markdown","2055d30c":"markdown","9ebff6e7":"markdown","cbf6827d":"markdown","7e8f12f9":"markdown","ea8b19db":"markdown","1b4d379d":"markdown","339c83b7":"markdown"},"source":{"a430d93e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f434efb8":"import pandas as pd\nimport matplotlib.pyplot as plt\n% matplotlib inline","1964d88b":"train=pd.read_csv('..\/input\/X_train.csv')\ntest=pd.read_csv('..\/input\/X_test.csv')\ny=pd.read_csv('..\/input\/y_train.csv')","956d496d":"train.head()","6e24ac18":"print('No. of series in training data: ',train['series_id'].nunique())\nprint('No. of series in testing data: ',test['series_id'].nunique())","26692eb8":"train.groupby('series_id')['measurement_number'].count()","a84cb3bc":"(len(train)==128*3810)","55cfcd2d":"test.groupby('series_id')['measurement_number'].count()","be4bfa75":"(len(test)==3816*128)","4e0ebe84":"test.head()","5e74bb25":"y.head()","a3652bff":"print(len(y)); print(y['surface'].unique()); y['surface'].nunique()","97455d9c":"y['surface'].value_counts().reset_index().plot(x='index',y='surface',kind='bar')","6d48b72a":"def plot_series_distribution(series):\n    df_train=train[train['series_id']==series]\n    df_test=test[test['series_id']==series]\n    plt.figure(figsize=(30,15))\n    for i,col in enumerate(df_train.columns[3:]):\n        plt.subplot(3,4,i+1)\n        df_train[col].hist(bins=100,color='blue')\n        df_test[col].hist(bins=100,color='red')\n        plt.title(col)","0b6619ea":"plot_series_distribution(1)","cacc42f3":"def plot_series(series):\n    df_train=train[train['series_id']==series]\n    df_test=test[test['series_id']==series]\n    plt.figure(figsize=(30,15))\n    for i,col in enumerate(df_train.columns[3:]):\n        plt.subplot(3,4,i+1)\n        df_train[col].plot(color='blue')\n        #df_test[col].hist(color='red')\n        plt.title(col)\n       ","51a38540":"plot_series(0)","eddff44d":"train_df=train[['series_id']].drop_duplicates().reset_index(drop=True)","b849f1c4":"\ntest_df=test[['series_id']].drop_duplicates().reset_index(drop=True)","72333dcc":"import numpy as np\ndef new_features(df,tf):\n    for i,col in enumerate(df.columns[3:]):\n        tf[col+'_mean']=df.groupby('series_id')[col].mean()\n        tf[col+'_std']=df.groupby('series_id')[col].std()\n        tf[col+'_max']=df.groupby('series_id')[col].max()\n        tf[col+'_min']=df.groupby('series_id')[col].min()\n        tf[col + '_max_to_min'] = tf[col + '_max'] \/ tf[col + '_min']\n        tf[col+'_abs_max']=df.groupby('series_id')[col].apply(lambda x: np.max(np.abs(x)))\n        tf[col+'_abs_min']=df.groupby('series_id')[col].apply(lambda x: np.min(np.abs(x)))\n        tf[col+'_mad']=df.groupby('series_id')[col].mad()\n        tf[col+'_kurtosis']=df.groupby('series_id')[col].apply(lambda x: x.kurtosis())\n        tf[col+'_skew']=df.groupby('series_id')[col].skew()\n        tf[col+'_median']=df.groupby('series_id')[col].median()\n        tf[col+'_rolling_avg_10']=df.groupby('series_id')[col].rolling(10).mean().mean(skipna=True)\n        tf[col+'_rolling_avg_10']=df.groupby('series_id')[col].rolling(10).std().mean(skipna=True)\n    return tf","68ec82ec":"train_df=new_features(train,train_df)","a595972c":"test_df=new_features(test,test_df)","e774dde5":"test_df.head()","75bea95f":"new_train=train_df.copy()\nnew_test=test_df.copy()","d2e8e66c":"!pip install fastai==0.7.0","5f6763a2":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline","57af4c86":"from fastai.imports import *\nfrom fastai.structured import *\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics","6584f943":"new_train['surface']=y['surface']","be95472f":"train_cats(new_train)","9a5b4aaf":"df_train,y,nas=proc_df(new_train,'surface')\ndf_test,_,nas=proc_df(new_test,na_dict=nas)\ndf_train,y,nas=proc_df(new_train,'surface',na_dict=nas)","820031ca":"def rmse(x,y):\n    return np.sqrt(((x-y)**2).mean())\ndef print_score(m):\n    res=[rmse(m.predict(X_train),y_train),rmse(m.predict(X_valid),y_valid),m.score(X_train,y_train),m.score(X_valid,y_valid)]\n    if hasattr(m,'oob_score_'):\n        res.append(m.oob_score_)\n    print(res)","ff88863c":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(df_train, y, test_size = 0.2, random_state = 42)","a2e29d1d":"m=RandomForestClassifier(n_estimators=120,min_samples_leaf=5,max_features=0.5,oob_score=True,n_jobs=-1)\nm.fit(X_train,y_train)\npred=m.predict(X_valid)\nprint_score(m)","e105e938":"m=RandomForestClassifier(n_estimators=200,min_samples_leaf=5,max_features=0.5,oob_score=True,n_jobs=-1)\nm.fit(X_train,y_train)\npred=m.predict(X_valid)\n#print(accuracy_score(y_valid,pred))\nprint_score(m)","c092b3e7":"plt.figure(figsize=(16,16))\nfi=rf_feat_importance(m,df_train)\nfi.plot(kind='bar',x='cols',y='imp')\ndf_k=fi[fi['imp']>=0.005]['cols'] # We'll keep features having feature importance value greater than 0.005","32c1306a":"len(df_k)","cb264a52":"df_keep=df_train[df_k]\ndf_keep_test=df_test[df_k]","efa7df05":"X_train, X_valid, y_train, y_valid = train_test_split(df_keep, y, test_size = 0.2, random_state = 42)\nX_train.shape","078fb383":"m=RandomForestClassifier(n_estimators=200,min_samples_leaf=5,max_features=0.5,oob_score=True,n_jobs=-1)\nm.fit(X_train,y_train)\npred=m.predict(X_valid)\nprint_score(m)","db834fcc":"from scipy.cluster import hierarchy as hc","9f01ac75":"corr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,16))\ndendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=16)\nplt.show()","27b4d578":"def get_oob(df):\n    m = RandomForestClassifier(n_estimators=200, min_samples_leaf=5, max_features=0.5, n_jobs=-1, oob_score=True)\n    X_train, X_valid, y_train, y_valid = train_test_split(df_keep, y, test_size = 0.2, random_state = 42)\n    m.fit(X_train, y_train)\n    return m.oob_score_","c5477646":"get_oob(df_keep)","120574ca":"a=['orientation_Z_mean','orientation_Z_median','orientation_Z_min','orientation_Z_max','orientation_Y_mean','orientation_Y_median','orientation_Y_min','orientation_Y_max','orientation_X_mean','orientation_X_median','orientation_X_min','orientation_X_max','orientation_W_mean','orientation_W_median','orientation_W_min','orientation_W_max']","6df87e81":"for c in a:\n    print(c,get_oob(df_keep.drop(c,axis=1)))","f19586c5":"to_drop=['orientation_Z_mean','orientation_Z_median','orientation_Y_mean','orientation_Y_median','orientation_X_mean','orientation_X_median','orientation_W_mean','orientation_W_median']","b11f30b7":"get_oob(df_keep.drop(to_drop, axis=1))","543d55a3":"df_keep.drop(to_drop, axis=1, inplace=True)\ndf_keep_test.drop(to_drop,axis=1,inplace=True)","f60f509e":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=546789)\nsub_preds_rf = np.zeros((df_keep_test.shape[0], 9))\noof_preds_rf = np.zeros((df_keep.shape[0]))\nscore = 0\nfor i, (train_index, test_index) in enumerate(folds.split(df_keep, y)):\n    print('-'*20, i, '-'*20)\n    \n    clf =  RandomForestClassifier(n_estimators = 200, n_jobs = -1)\n    clf.fit(df_keep.iloc[train_index], y[train_index])\n    oof_preds_rf[test_index] = clf.predict(df_keep.iloc[test_index])\n    sub_preds_rf += clf.predict_proba(df_keep_test) \/ folds.n_splits\n    score += clf.score(df_keep.iloc[test_index], y[test_index])\n    print('score ', clf.score(df_keep.iloc[test_index], y[test_index]))\n    ","1dd50494":"sub_preds_rf","713237b7":"sub_preds_rf.argmax(axis=1)","35686574":"new_train['surface'].cat.codes.unique()","d1575e2b":"new_train['surface'].cat.categories","bd7724fe":"s={0:'carpet',1:'concrete',2:'fine_concrete',3:'hard_tiles',4:'hard_tiles_large_space',5:'soft_pvc',6:'soft_tiles',7:'tiled',8:'wood'}\n","e30b0c85":"df_sub=pd.DataFrame({'series_id':df_test['series_id'],'predictions':sub_preds_rf.argmax(axis=1)})","1c1bcb2d":"df_sub['surface']=df_sub['predictions'].apply(lambda x: s[x])","09e29f24":"df_sub.drop(['predictions'],axis=1,inplace=True)","46298d47":"df_sub.to_csv('submission2_cv.csv',index=False)","4a39b48d":"# Feature Engineering","6d4c4a0e":"Function for Calculating RMSE and R2 score","462f3988":"Public Leaderboard Score: 0.6521 Private Leaderboard Score 0.5367","d9910521":"Saving Submission File","6475748a":"Each Series in test data also has 128 measurement values","cbc1bd8f":"The scores are not affected much by dropping those features","3046971e":"# Feature Importance","20690fb9":"The features in 'a' are correlated. Hence we will drop each feature one by one and check whether the score is affected by dropping them.","9b5e038f":"There are 54 features having importance value greater than 0.005","2055d30c":"# Removing Redundant features","9ebff6e7":"Each series has 128 measurement values in training data","cbf6827d":"# References\n1. fastai course by Jeremy Howard\n\n2. https:\/\/www.kaggle.com\/prashantkikani\/help-humanity-by-helping-robots\n\n3. https:\/\/www.kaggle.com\/artgor\/where-do-the-robots-drive","7e8f12f9":"# Cross Validation ","ea8b19db":"# Data Exploration","1b4d379d":"The score does not change on dropping the features in to_drop list.","339c83b7":"train_cats() function converts categorical features into codes. In the present dataset the 'surface' column is a categorical feature"}}