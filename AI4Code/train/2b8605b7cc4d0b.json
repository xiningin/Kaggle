{"cell_type":{"db4759f2":"code","f8a6e2fc":"code","5f214653":"code","0d09b3e1":"code","56758963":"code","97f491a4":"code","d9179735":"code","4bf24ed6":"code","305f84fb":"code","0d771548":"code","e2cec393":"code","29d6df20":"code","59e8e276":"code","95029895":"code","1ec6bf51":"code","740c8354":"code","d2ce07c2":"code","56d3a825":"code","7ddd741f":"code","376b0048":"code","5904b840":"code","55626eeb":"code","ade0e735":"code","6edeb6c5":"code","4d0f159e":"code","cc2fd57e":"code","318b3ba2":"code","47d2d6a6":"code","0824140f":"code","6b734e4a":"code","e0206170":"code","24c3c583":"code","fe6bb22e":"markdown","7ed7bd5b":"markdown","c5fd3056":"markdown","e22719d4":"markdown","a893f95e":"markdown","6db39a59":"markdown","d22d0492":"markdown","291ebe1a":"markdown","d5b8f488":"markdown","e67ccf1e":"markdown","1947005a":"markdown","deae1018":"markdown"},"source":{"db4759f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f8a6e2fc":"df=pd.read_csv(\"\/kaggle\/input\/fake-news\/fake_train.csv\")\ndf.head()","5f214653":"df.info()","0d09b3e1":"df.isnull().sum()","56758963":"df=df.dropna()","97f491a4":"#Splitting the dataset as:\n#independent feature\nX=df.iloc[:,:-1]\n#dependant feature\ny=df.iloc[:,-1]","d9179735":"X.shape","4bf24ed6":"y.shape","305f84fb":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding\n#pad_sequence is used to make sure that input length of every sentence is same\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense","0d771548":"#Setting the vocabulary size\nvoc_size=5000","e2cec393":"#Creating a copy so that any changes in copy does not affect the original data\nmessages = X.copy()","29d6df20":"messages.shape","59e8e276":"#resetting the index as we have dropped the nan values\nmessages.reset_index(inplace=True)","95029895":"import nltk\nimport re\nfrom nltk.corpus import stopwords","1ec6bf51":"#Stopwords are the words that are not importat in a sentence\nnltk.download(\"stopwords\")","740c8354":"#Data Preprocessing\nfrom nltk.stem.porter import PorterStemmer\nps=PorterStemmer()\ncorpus=[]\nfor i in range(len(messages)):\n    #subtituting everythin other than letter \"a-z\" and \"A-Z\" with a space\n    review=re.sub(\"[^a-zA-Z] \",\" \",messages[\"title\"][i])\n    #converting all the words in lower case\n    review=review.lower()\n    review=review.split()\n    \n    review=[ps.stem(word) for word in review if not word in stopwords.words(\"english\")]\n    review=\" \".join(review)\n    corpus.append(review)","d2ce07c2":"corpus","56d3a825":"#we will do one hot encoding for the corpus. It is alloting every word an index according to the vocabulary size\nonehot=[one_hot(words,voc_size) for words in corpus]\nonehot","7ddd741f":"sent_len=20\n#if length of sentence is not 20 than it will ad 0 in front of sentence such that length becomes 20\nembedded_docs=pad_sequences(onehot,padding=\"pre\",maxlen=sent_len)\nembedded_docs","376b0048":"#taking the first sentece \nembedded_docs[0]","5904b840":"len(embedded_docs)","55626eeb":"embedding_vector_features=40\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_len))\nmodel.add(LSTM(100))\nmodel.add(Dense(1,activation=\"sigmoid\"))\nmodel.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])","ade0e735":"model.summary()","6edeb6c5":"len(embedded_docs),y.shape","4d0f159e":"#Creating new independent and dependent variables\nimport numpy as np\nX_final=np.array(embedded_docs)\ny_final=np.array(y)","cc2fd57e":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X_final,y_final,test_size=0.3,random_state=0)","318b3ba2":"TF_FORCE_GPU_ALLOW_GROWTH=True ","47d2d6a6":"model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=5,batch_size=64)","0824140f":"y_pred=model.predict_classes(X_test)","6b734e4a":"from sklearn.metrics import confusion_matrix","e0206170":"confusion_matrix(y_test,y_pred)","24c3c583":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","fe6bb22e":"## Accuracy","7ed7bd5b":"## Data Preprocessing","c5fd3056":"**We got an accuarcy of 91% which is pretty decent**","e22719d4":"## Splitting the Dataset into Train and Test set","a893f95e":"## IMPORTING LIBRARIES FOR MODEL CREATION","6db39a59":"## Importing Dataset","d22d0492":"## Creating Confusion Matrix","291ebe1a":"## One-Hot Represetation","d5b8f488":"## Model Creation","e67ccf1e":"## Embedding Representatio","1947005a":"## Model Training","deae1018":"## REMOVING NULL VALUES"}}