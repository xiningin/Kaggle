{"cell_type":{"c930ecd5":"code","82d12e1f":"code","ce99400f":"code","e1d58907":"code","baaa296b":"code","83a3170a":"code","8490ae67":"code","358ae584":"code","f6f59e44":"code","a25af7d2":"code","d03ed8ea":"code","0e9d52c8":"code","9adaa474":"code","31af136d":"code","13b27f57":"code","15e56941":"code","01c9c188":"code","9828c944":"code","9206de4e":"code","a71f8b43":"code","f47083dd":"markdown","5b4f8f54":"markdown","594b3152":"markdown","4aa2fcea":"markdown","9be290a0":"markdown","bdf22d5a":"markdown","34af3f78":"markdown","c6fc39f4":"markdown","ae1825d1":"markdown","38ad484e":"markdown","4770f1da":"markdown","499b59fb":"markdown","4254d2ae":"markdown","e4471798":"markdown","fabe14b1":"markdown","6c67acf8":"markdown","f23edb3b":"markdown","84ff9573":"markdown","88a9fec9":"markdown","856c5c08":"markdown","dd2df669":"markdown","2860eb40":"markdown","418510cf":"markdown","2bfe3094":"markdown"},"source":{"c930ecd5":"import tensorflow as tf\nimport numpy as np\nimport os ","82d12e1f":"text = open(\"..\/input\/war-and-peace-project-gutenberg\/war_peace_plain.txt\",\"rb\").read().decode(\"utf-8\")\nprint(text[:1000])","ce99400f":"print(\"There are {} char in the text\".format(len(text)))","e1d58907":"vocab = sorted(set(text))\nprint(\"There are {} unique char in the text\".format(len(vocab)))","baaa296b":"char2idx = {u:i for i,u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = [char2idx[char] for char in text]\n\nprint(text_as_int[:20])","83a3170a":"char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\nfor index in char_dataset.take(10):\n    if idx2char[index.numpy()] == \" \":\n        print(\"SPACE\",index.numpy())\n    elif idx2char[index.numpy()] == \"\\n\":\n        print(\"\\\\n\",index.numpy())\n    else:\n        print(idx2char[index.numpy()],index.numpy())\n    ","8490ae67":"seq_length = 100\nsequences = char_dataset.batch(seq_length+1,drop_remainder=True)\n\nfor item in sequences.take(1):\n    \n    print(item,end=\"\\n\")\n    print(\"\\n\")\n    print(idx2char[item],end=\"\\n\")\n    print(\"\".join(idx2char[item]))","358ae584":"def split_input_target(chunk):\n    \"\"\"\n    A function that converts sequences into input_text and target_text\n    \"\"\"\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text,target_text\n\ndataset = sequences.map(split_input_target)\n\nfor input_example,target_example in dataset.take(1):\n    \n    print(\"Input Example\")\n    print(\"\".join(idx2char[input_example.numpy()]))\n    print(\"\\n\")\n    print(\"Target Example\")\n    print(\"\".join(idx2char[target_example.numpy()]))\n\n","f6f59e44":"BATCH_SIZE = 128\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)\n\ndataset\n","a25af7d2":"# loading everything\nfrom tensorflow.python.keras.layers import Dense,CuDNNGRU,Embedding\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint","d03ed8ea":"# Determining parameters\n\nvocab_size = len(vocab)\n\n# Embedding dimension, size of each word vector\nembedding_dim = 256\n\n# Unit number of each GRU\nrnn_units = 1024","0e9d52c8":"def build_model(vocab_size,embedding_dim,rnn_units,batch_size):\n    \n    model = Sequential()\n    \n    model.add(Embedding(vocab_size,\n                       embedding_dim,\n                       batch_input_shape=[batch_size,None]))\n    \n    model.add(CuDNNGRU(rnn_units,\n                      return_sequences=True,\n                      stateful=True,\n                      recurrent_initializer=\"glorot_uniform\"))\n    \n    model.add(CuDNNGRU(rnn_units,\n                      return_sequences=True,\n                      stateful=True,\n                      recurrent_initializer=\"glorot_uniform\"))\n    \n    model.add(CuDNNGRU(rnn_units,\n                      return_sequences=True,\n                      stateful=True,\n                      recurrent_initializer=\"glorot_uniform\"))\n    \n    model.add(Dense(vocab_size))\n    \n    return model\n","9adaa474":"model = build_model(vocab_size,\n                    embedding_dim,\n                    rnn_units,\n                   BATCH_SIZE)\n\nmodel.summary()","31af136d":"def loss(labels,logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels,logits,from_logits=True)","13b27f57":"model.compile(optimizer=\"rmsprop\",loss=loss)\nmodel.summary()","15e56941":"# Directory where the checkpoints will be saved\n\n# Creating a folder\nos.mkdir('.\/training_checkpoints')\n\ncheckpoint_dir = '.\/training_checkpoints'\n# Name of the checkpoint files\n\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","01c9c188":"EPOCHS = 10\nmodel.fit(dataset,epochs=EPOCHS,callbacks=[checkpoint_callback])","9828c944":"model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))","9206de4e":"def generateText(model,start_string,char_size,temp=1.0):\n    \n    # Length of the text (as char)\n    num_generate = char_size\n    \n    # Converting start string to numbers\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval,0)\n    \n    text_generated = []\n    \n    # Low temperature results in more predictable text.\n    # Higher temperature results in more surprising text.\n    temperature = temp\n    \n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        \n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a categorical distribution to predict the character returned by the model\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # Pass the predicted character as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(idx2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))","a71f8b43":"print(generateText(model,start_string=\"CHAPTER 1 \\n\",char_size=1000))","f47083dd":"# Data Processing\n\nIn this section I am going to prepare the dataset in order to use in our future deep learning model.","5b4f8f54":"I've sliced the dataset by each char so, each iteration is a character from the text.","594b3152":"* Now let's define a function in order to generate texts.","4aa2fcea":"First I defined a function that converts each chunk into input_text and target_text. In char based models, I generally use this method. \n\nAfter that,I applied this function to every chunk.","9be290a0":"* Text looks good, it does not look meaningful but nice anyway. \n* If you read War and Peace, you will see the characters in this text. Rostov, Natasha and Pierre are characters in the book.\n* And I realised one more thing. In book, place names are in Russian and the model confused when it wants to create a place name. ","bdf22d5a":"# Sequence Model\n\nIn this section I am going to build a seqeunce model in order to generate texts. In text generating, we use RNNs. There are many types of RNNs, like Simple RNN, LSTM and GRU.\n\nSimple RNN is the most primitive version of RNN. It has problems, so we don't use it. In this kernel I am going to use GRU as RNN layer. You can also try LSTM. Both are good RNNs.\n\nBefore building the model. Let's take a look at the layers that we will use.\n\n### Layers\n1. Embedding Layers: In order to create word vectors, we will use this layer.\n1. GRU1 : It is the first RNN layer of the model\n1. GRU2: It is the second RNN layer of the model\n1. GRU3: It is the third RNN layer of the model\n1. Dense: It is the output layer of the model.\n\nNow that we know our layers. Let's build and compile the model.\n","34af3f78":"* I give an index to each unique char. In order to give indexes to chars I've used enumerate function of Python.","c6fc39f4":"* Our function is ready, now let's create our model.","ae1825d1":"Now, I will define the loss and after that I will compile the model.","38ad484e":"Our model has compiled, now there is only one step before fitting the model. Preparing the callback.","4770f1da":"# Conclusion\n\nThanks for your attention, if you have questions in your mind, please ask. I will answer your questions as much as I can.","499b59fb":"## Batching\n\nIn this section I am going to split dataset into batches. But before batching, I will shuffle the dataset.","4254d2ae":"* Now I am gonna define a function that helps us to build our model.","e4471798":"# Importing Libraries and The Data\n\nIn this section I am going to import the libraries that I will use.","fabe14b1":"# Data Overview\n\nIn this section I am going to take a look at the data.","6c67acf8":"# Evaluating Model\n\nIn this section I am going to use the trained model. I'll start with rebuilding the model.","f23edb3b":"# Fitting Model\n\nIn this section we will fix the model using our prepared dataset. ","84ff9573":"## Loading Model\nIn this section I am going to rebuild model, you know our model takes 64 batches but we want to give only batch. In order to do this, I am going to rebuild model.","88a9fec9":"# Starter: Text Generation with Keras\n\nHello people, welcome to this kernel. In this kernel I am gonna generate text using RNNs. I will explain everything step-by-step. Before starting, let's take a look at our content.\n\n# Notebook Content\n1. Importing Libraries and The Data\n1. Data Overview\n1. Data Processing\n    * Char Dataset\n    * Sequence Dataset\n    * Splitting Input and Target\n    * Batching\n1. Modeling\n    * Sequence Model\n    * Loss Function and Compiling The Model\n    * Fitting Model\n1. Evaluating\n    * Loading Model\n    * Text Generating Function\n1. Conclusion","856c5c08":"## Loss Function and Compiling The Model","dd2df669":"* Let's compile the model.","2860eb40":"* Finally we came to the funniest stage. Let's create some texts.","418510cf":"* We will use sparse categorical cross entropy, here is the formula:\n\n![image-6.png](attachment:image-6.png)","2bfe3094":"Now, I've created sequences. Each sequences contains 100 chars. I'will use these sequences in order to create input and target."}}