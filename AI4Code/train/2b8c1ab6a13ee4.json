{"cell_type":{"a0f8b6bd":"code","22aa569f":"code","24681fef":"code","5fa3897e":"code","0118d9e6":"code","f8f7c6bb":"code","876114b2":"code","d7aa77a8":"code","9a954ca8":"code","ecc54e61":"code","896de537":"code","c5bab2cd":"code","1d13c229":"code","6d68bb2b":"code","83a366e6":"code","dbb8312f":"code","1172fae3":"code","916c9590":"code","c126ba26":"code","b058e04e":"code","64667319":"code","0b73623d":"code","8642a282":"code","a1b02718":"code","87603ef2":"code","8db1c2d0":"code","fa0a6947":"code","784665db":"markdown","fd66d895":"markdown","8d09fc3d":"markdown","62fd9d38":"markdown"},"source":{"a0f8b6bd":"import pandas as pd\nimport numpy as np","22aa569f":"train_path = '..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv'\ntest_path = '..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv'","24681fef":"#load file\ntrain = pd.read_csv(train_path, encoding='latin-1')\ntest = pd.read_csv(test_path, encoding='latin-1')\nprint(f'training set size: {train.shape}, test set size: {test.shape}')","5fa3897e":"train['category'] = 'train'\ntest['category'] = 'test'\ndata = pd.concat([train, test])\ndata.shape","0118d9e6":"data.head()","f8f7c6bb":"data.info()","876114b2":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport wordcloud","d7aa77a8":"data['Sentiment'].value_counts().plot(kind='bar', figsize=(15,7))\nplt.xlabel('Sentiment')\nplt.ylabel('Number of tweets');","9a954ca8":"import nltk\n#nltk.download('stopwords')\n#nltk.download('wordnet')\nstopwords = nltk.corpus.stopwords.words('english')\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\nlemmatizer = nltk.stem.WordNetLemmatizer()","ecc54e61":"import re\n\ndef clean_tweet(text):\n    text = re.sub(r'http\\S+', ' ', text)\n    text = re.sub(r'@\\w+', ' ', text)\n    text = re.sub(r'#\\w+', ' ', text)\n    text = re.sub(r'\\d+', ' ', text)\n    text = re.sub('r<.*?>', ' ', text)\n    text = ' '.join(text.split()) #remove duplicate space\n    return ' '.join([lemmatizer.lemmatize(w) for w in tokenizer.tokenize(text.lower()) if w not in stopwords])","896de537":"data['OriginalTweet'] = data['OriginalTweet'].apply(clean_tweet)","c5bab2cd":"# conver all tweets into a single string to generate wordcloud\ntweets_str = ' '.join([i for i in data['OriginalTweet']])","1d13c229":"wc = wordcloud.WordCloud(stopwords=stopwords)\nwc.generate(tweets_str)\ndisplay(wc.to_image())","6d68bb2b":"covid_pos = 0\ncovid_neg = 0\ncovid_expos = 0\ncovid_exneg = 0\ncovid_neu = 0\nfor tweet, sentiment in np.nditer([data['OriginalTweet'], data['Sentiment']], flags=['refs_ok']):\n    if re.search(r'\\w*corona\\w*|\\w*covid\\w*', str(tweet)) and sentiment=='Positive':\n        covid_pos+=1\n    elif re.search(r'\\w*corona\\w*|\\w*covid\\w*', str(tweet)) and sentiment=='Negative':\n        covid_neg+=1\n    elif re.search(r'\\w*corona\\w*|\\w*covid\\w*', str(tweet)) and sentiment=='Neutral':\n        covid_neu += 1\n    elif re.search(r'\\w*corona\\w*|\\w*covid\\w*', str(tweet)) and sentiment=='Extremely Positive':\n        covid_expos += 1\n    elif re.search(r'\\w*corona\\w*|\\w*covid\\w*', str(tweet)) and sentiment=='Extremely Negative':\n        covid_exneg +=1\ncovid_pos, covid_neg, covid_expos, covid_exneg, covid_neu","83a366e6":"fig = plt.figure(figsize=(10,7))\nplt.bar(\n    ['Positive', 'Negative', 'Neutral', 'Extremely Positive', 'Extremely Negative'],\n    height=[covid_pos, covid_neg, covid_neu, covid_exneg, covid_expos]\n)\nplt.title('Frequencies of tweets sentiments that contains the word corona or covid');","dbb8312f":"data.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1, inplace=True)\nn_data = data #keep data for lstm model\nd = {'Neutral':0, 'Extremely Positive':1, 'Extremely Negative':-1, 'Positive':1,\n     'Negative':-1\n    }\ndata['Sentiment'] = data['Sentiment'].map(d)\ndata.head()","1172fae3":"data.isna().sum()","916c9590":"lr_train = data[data['category']=='train'].drop('category', axis=1)\nlr_test = data[data['category']=='test'].drop('category', axis=1)\nX_train = lr_train['OriginalTweet']\ny_train = lr_train['Sentiment']\nX_test = lr_test['OriginalTweet']\ny_test = lr_test['Sentiment']","c126ba26":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report","b058e04e":"count_vector = CountVectorizer(stop_words='english')","64667319":"x_train = count_vector.fit_transform(X_train)\nprint(f'number of unique words: {len(count_vector.get_feature_names())}')\nprint(x_train.shape)","0b73623d":"np.max(x_train), np.min(x_train)","8642a282":"x_test = count_vector.transform(X_test)","a1b02718":"lreg = LogisticRegression(C=1, max_iter=1000, random_state=3)","87603ef2":"lreg.fit(x_train, y_train)","8db1c2d0":"print(f'Accuracy on training set: {lreg.score(x_train, y_train)}')\nprint(f'Accuracy on test set: {lreg.score(x_test, y_test)}')","fa0a6947":"print(classification_report(y_test, lreg.predict(x_test)))","784665db":"#### from the chart above, since the frequency of the word covid or corona is fairly distributed, I won't bother adding it to the stopwords. Unlike in some nlp task where the frequency of some key words is not fairly distributed which in turn affect the prediction of the sentiments as the prediction will be biased towards the sentiment with the highest frequency.","fd66d895":"# EDA","8d09fc3d":"# Logistic Regression","62fd9d38":"#### having issue using fastai lstm learner...\n### my whole work can be viwed here:https:\/\/colab.research.google.com\/drive\/1MXntVq8_B-d76mLc550PxsTL5P4WlhNe?usp=sharing"}}