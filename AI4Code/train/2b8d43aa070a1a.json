{"cell_type":{"99fb2d7c":"code","4525ad21":"code","1cedf0db":"code","556ae1cc":"code","29a2a89f":"code","96107ab9":"code","5362ae31":"code","99b30989":"code","bb374d22":"code","3c3d4bbe":"code","c81d3fea":"code","639e033e":"code","5d6402d6":"code","9dc55059":"code","c2168d7c":"code","3f9b81bf":"code","40ae9603":"code","de8d2777":"code","5120184e":"code","37f23f08":"code","8a8ba60e":"code","1885930c":"code","b90971e3":"markdown","e1f8930c":"markdown","1555fd85":"markdown","c5c98ab7":"markdown","c1aeba38":"markdown","bd6965f9":"markdown","e71d4749":"markdown"},"source":{"99fb2d7c":"import pandas as pd\nimport numpy as np\nimport json\nimport tensorflow.keras.layers as L\nimport keras.backend as K\nimport tensorflow as tf\nimport plotly.express as px\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.cluster import KMeans\nimport os\nfrom collections import Counter as count\n#import cond_rnn\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\ndef allocate_gpu_memory(gpu_number=0):\n    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n\n    if physical_devices:\n        try:\n            print(\"Found {} GPU(s)\".format(len(physical_devices)))\n            tf.config.set_visible_devices(physical_devices[gpu_number], 'GPU')\n            tf.config.experimental.set_memory_growth(physical_devices[gpu_number], True)\n            print(\"#{} GPU memory is allocated\".format(gpu_number))\n        except RuntimeError as e:\n            print(e)\n    else:\n        print(\"Not enough GPU hardware devices available\")\nallocate_gpu_memory()\n\nVer='GRU_LSTM1'\ndebug = False","4525ad21":"def gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))\n\ndef lstm_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))\n\ndef build_model(seq_len=107, pred_len=68, dropout=0.5, embed_dim=100, hidden_dim=256, type=0):\n    inputs1 = L.Input(shape=(seq_len, 6)) #[(None, 107, 7)]\n    inputs2 = L.Input(shape = (1,18)) ###Number 12 needs to be changed according to the feature engineering\n    inputs3 = L.Dense(6, activation = 'relu')(inputs2)\n    inputs = 0.97*inputs1 + 0.03*inputs3\n    \n    # split categorical and numerical features and concatenate them later.\n    categorical_feat_dim = 3\n    categorical_fea = inputs[:, :, :categorical_feat_dim] #(Tens [(None, 107, 4)]\n    numerical_fea = inputs[:, :, 3:] #(Te [(None, 107, 3)]\n\n    embed = L.Embedding(input_dim=len(token2int), output_dim=embed_dim)(categorical_fea) #(None, 107, 4, 100)\n    reshaped = tf.reshape(embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3])) #(TensorFlow [(None, 107, 400)]\n    reshaped = L.concatenate([reshaped, numerical_fea], axis=2) #(None, 107, 403)\n    \n    if type == 0:\n        hidden = gru_layer(hidden_dim, dropout)(reshaped)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n    elif type == 1:\n        hidden = lstm_layer(hidden_dim, dropout)(reshaped)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n    elif type == 2:\n        hidden = gru_layer(hidden_dim, dropout)(reshaped)\n        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n    elif type == 3:\n        hidden = lstm_layer(hidden_dim, dropout)(reshaped) #(None, 107, 512)\n        hidden = lstm_layer(hidden_dim, dropout)(hidden) #(None, 107, 512)\n    \n    truncated = hidden[:, :pred_len] #(Te [(None, 68, 512)]\n    #out1 = L.Dense(128, activation='relu')(truncated) #(None, 68, 5) ###the number 15 becomes kind of a hyperparameter here\n    #out2 = L.concatenate([inputs1, truncated])\n    out = L.Dense(5, activation = 'linear')(truncated)\n    model = tf.keras.Model(inputs=[inputs1, inputs2], outputs=out)\n    model.compile(tf.keras.optimizers.Adam(), loss=mcrmse)\n    return model","1cedf0db":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSXPK')}\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n#pair_dict = {'A':'K','U':'K','G':'P','C':'P'}\n\ndef preprocess_inputs(df):\n    #df['pairs'] = df[['sequence']].applymap(lambda seq: ('').join([pair_dict[x] for x in seq]))\n    cols=['sequence', 'structure', 'predicted_loop_type']\n    base_fea = np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n    bpps_sum_fea = np.array(df['bpps_sum'].to_list())[:,:,np.newaxis]\n    bpps_max_fea = np.array(df['bpps_max'].to_list())[:,:,np.newaxis]\n    bpps_nb_fea = np.array(df['bpps_nb'].to_list())[:,:,np.newaxis]\n    return np.concatenate([base_fea,bpps_sum_fea,bpps_max_fea,bpps_nb_fea], 2)\n\ndef rmse(y_actual, y_pred):\n    mse = tf.keras.losses.mean_squared_error(y_actual, y_pred)\n    return K.sqrt(mse)\n\ndef mcrmse(y_actual, y_pred, num_scored=len(pred_cols)):\n    score = 0\n    for i in range(num_scored):\n        score += rmse(y_actual[:, :, i], y_pred[:, :, i]) \/ num_scored\n    return score","556ae1cc":"train = pd.read_json('..\/input\/stanford-covid-vaccine\/train.json', lines=True)\ntest = pd.read_json('..\/input\/stanford-covid-vaccine\/test.json', lines=True)","29a2a89f":"def process_inputs_2(df):\n    df1 = df.copy()\n    df2 = df.copy()\n    df3 = df.copy()\n    df4 = df.copy()\n    df5 = df.copy()\n    from collections import Counter as count\n    bases = []\n    for j in range(len(df1)):\n        counts = dict(count(df1.iloc[j]['sequence']))\n        bases.append((\n            counts['A'] \/ df1.iloc[j]['seq_length'],\n            counts['G'] \/ df1.iloc[j]['seq_length'],\n            counts['C'] \/ df1.iloc[j]['seq_length'],\n            counts['U'] \/ df1.iloc[j]['seq_length']\n        ))\n\n    bases = pd.DataFrame(bases, columns=['A_percent', 'G_percent', 'C_percent', 'U_percent'])\n    del df1\n    \n    pairs = []\n    all_partners = []\n    for j in range(len(df2)):\n        partners = [-1 for i in range(130)]\n        pairs_dict = {}\n        queue = []\n        for i in range(0, len(df2.iloc[j]['structure'])):\n            if df2.iloc[j]['structure'][i] == '(':\n                queue.append(i)\n            if df2.iloc[j]['structure'][i] == ')':\n                first = queue.pop()\n                try:\n                    pairs_dict[(df2.iloc[j]['sequence'][first], df2.iloc[j]['sequence'][i])] += 1\n                except:\n                    pairs_dict[(df2.iloc[j]['sequence'][first], df2.iloc[j]['sequence'][i])] = 1\n\n                partners[first] = i\n                partners[i] = first\n\n        all_partners.append(partners)\n\n        pairs_num = 0\n        pairs_unique = [('U', 'G'), ('C', 'G'), ('U', 'A'), ('G', 'C'), ('A', 'U'), ('G', 'U')]\n        for item in pairs_dict:\n            pairs_num += pairs_dict[item]\n        add_tuple = list()\n        for item in pairs_unique:\n            try:\n                add_tuple.append(pairs_dict[item]\/pairs_num)\n            except:\n                add_tuple.append(0)\n        pairs.append(add_tuple)\n\n    pairs = pd.DataFrame(pairs, columns=['U-G', 'C-G', 'U-A', 'G-C', 'A-U', 'G-U'])\n    del df2\n    \n    pairs_rate = []\n    for j in range(len(df3)):\n        res = dict(count(df3.iloc[j]['structure']))\n        pairs_rate.append(res['('] \/ (df3.iloc[j]['seq_length']\/2))\n\n    pairs_rate = pd.DataFrame(pairs_rate, columns=['pairs_rate'])\n    del df3\n    \n    loops = []\n    for j in range(len(df4)):\n        counts = dict(count(df4.iloc[j]['predicted_loop_type']))\n        available = ['E', 'S', 'H', 'B', 'X', 'I', 'M']\n        row = []\n        for item in available:\n            try:\n                row.append(counts[item] \/ df4.iloc[j]['seq_length'])\n            except:\n                row.append(0)\n        loops.append(row)\n\n    loops = pd.DataFrame(loops, columns=available)\n    del df4\n    \n    return pd.concat([df5, bases, pairs, loops, pairs_rate], axis=1)","96107ab9":"train = process_inputs_2(train)\ntrain.head()","5362ae31":"test = process_inputs_2(test)\ntest.head()","99b30989":"# additional features\n\ndef read_bpps_sum(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").sum(axis=1))\n    return bpps_arr\n\ndef read_bpps_max(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").max(axis=1))\n    return bpps_arr\n\ndef read_bpps_min(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").max(axis=1))\n    return bpps_arr\n\ndef read_bpps_nb(df):\n    # normalized non-zero number\n    # from https:\/\/www.kaggle.com\/symyksr\/openvaccine-deepergcn \n    bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n    bpps_nb_std = 0.08914   # std of bpps_nb across all training data\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps = np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\")\n        bpps_nb = (bpps > 0).sum(axis=0) \/ bpps.shape[0]\n        bpps_nb = (bpps_nb - bpps_nb_mean) \/ bpps_nb_std\n        bpps_arr.append(bpps_nb)\n    return bpps_arr \n\n\ntrain['bpps_sum'] = read_bpps_sum(train)\ntest['bpps_sum'] = read_bpps_sum(test)\ntrain['bpps_max'] = read_bpps_max(train)\ntest['bpps_max'] = read_bpps_max(test)\ntrain['bpps_nb'] = read_bpps_nb(train)\ntest['bpps_nb'] = read_bpps_nb(test)","bb374d22":"# clustering for GroupKFold\n# expecting more accurate CV by putting similar RNAs into the same fold.\nkmeans_model = KMeans(n_clusters=100, random_state=110).fit(preprocess_inputs(train)[:,:,0])\ntrain['cluster_id'] = kmeans_model.labels_","3c3d4bbe":"aug_df = pd.read_csv('..\/input\/augmented-data-covax-1\/aug_data1.csv')\ndisplay(aug_df.head())","c81d3fea":"def aug_data(df):\n    target_df = df.copy()\n    new_df = aug_df[aug_df['id'].isin(target_df['id'])]\n                         \n    del target_df['structure']\n    del target_df['predicted_loop_type']\n    new_df = new_df.merge(target_df, on=['id','sequence'], how='left')\n\n    df['cnt'] = df['id'].map(new_df[['id','cnt']].set_index('id').to_dict()['cnt'])\n    df['log_gamma'] = 100\n    df['score'] = 1.0\n    df = df.append(new_df[df.columns]) #each id now has 2 rows\n    return df\ntrain = aug_data(train)\ntest = aug_data(test)","639e033e":"if debug:\n    train = train[:200]\n    test = test[:200]","5d6402d6":"def preprocess_ns(df, pred_len = 1):\n    ns_columns = ['A_percent',\n       'G_percent', 'C_percent', 'U_percent', 'U-G', 'C-G', 'U-A', 'G-C',\n       'A-U', 'G-U', 'E', 'S', 'H', 'B', 'X', 'I', 'M', 'pairs_rate']\n    z = np.array(df[ns_columns])\n    b = np.repeat(z[:, np.newaxis,:], pred_len, axis=1)\n    return b","9dc55059":"preprocess_ns(train).shape","c2168d7c":"model = build_model()\nmodel.summary()","3f9b81bf":"def train_and_predict(type = 0, FOLD_N = 5):\n    \n    gkf = GroupKFold(n_splits=FOLD_N)\n\n    public_df = test.query(\"seq_length == 107\").copy()\n    private_df = test.query(\"seq_length == 130\").copy()\n\n    public_inputs1 = preprocess_inputs(public_df)\n    public_inputs2 = preprocess_ns(public_df)\n    private_inputs1 = preprocess_inputs(private_df)\n    private_inputs2 = preprocess_ns(private_df)\n\n\n    holdouts = []\n    holdout_preds = []\n\n    for cv, (tr_idx, vl_idx) in enumerate(gkf.split(train,  train['reactivity'], train['cluster_id'])):\n        trn = train.iloc[tr_idx]\n        x_trn1 = preprocess_inputs(trn)\n        x_trn2 = preprocess_ns(trn)        \n        y_trn = np.array(trn[pred_cols].values.tolist()).transpose((0, 2, 1))\n        w_trn = np.log(trn.signal_to_noise+1.1)\/2.5\n\n        val = train.iloc[vl_idx]\n        x_val_all1 = preprocess_inputs(val)\n        x_val_all2 = preprocess_ns(val)\n        val = val[val.SN_filter == 1]\n        x_val1 = preprocess_inputs(val)\n        x_val2 = preprocess_ns(val)\n        y_val = np.array(val[pred_cols].values.tolist()).transpose((0, 2, 1))\n\n        model = build_model(type=type)\n        model_short = build_model(seq_len=107, pred_len=107,type=type)\n        model_long = build_model(seq_len=130, pred_len=130,type=type)\n\n        history = model.fit(\n            [x_trn1, x_trn2], y_trn,\n            validation_data = ([x_val1, x_val2], y_val),\n            batch_size=64,\n            epochs=65,\n            sample_weight=w_trn,\n            callbacks=[\n                tf.keras.callbacks.ReduceLROnPlateau(),\n                tf.keras.callbacks.ModelCheckpoint(f'model{Ver}_cv{cv}.h5')\n            ]\n        )\n\n        fig = px.line(\n            history.history, y=['loss', 'val_loss'], \n            labels={'index': 'epoch', 'value': 'Mean Squared Error'}, \n            title='Training History')\n        fig.show()\n\n        model.load_weights(f'model{Ver}_cv{cv}.h5')\n        model_short.load_weights(f'model{Ver}_cv{cv}.h5')\n        model_long.load_weights(f'model{Ver}_cv{cv}.h5')\n\n        holdouts.append(train.iloc[vl_idx])\n        holdout_preds.append(model.predict([x_val_all1,x_val_all2]))\n        if cv == 0:\n            public_preds = model_short.predict([public_inputs1, public_inputs2])\/FOLD_N\n            private_preds = model_long.predict([private_inputs1, private_inputs2])\/FOLD_N\n        else:\n            public_preds += model_short.predict([public_inputs1, public_inputs2])\/FOLD_N\n            private_preds += model_long.predict([private_inputs1, private_inputs2])\/FOLD_N\n    return holdouts, holdout_preds, public_df, public_preds, private_df, private_preds","40ae9603":"val_df, val_preds, test_df, test_preds = [], [], [], []\nif debug:\n    nmodel = 1\nelse:\n    nmodel = 4\nfor i in range(nmodel):\n    holdouts, holdout_preds, public_df, public_preds, private_df, private_preds = train_and_predict(i)\n    val_df += holdouts\n    val_preds += holdout_preds\n    test_df.append(public_df)\n    test_df.append(private_df)\n    test_preds.append(public_preds)\n    test_preds.append(private_preds)","de8d2777":"preds_ls = []\nfor df, preds in zip(test_df, test_preds):\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n        preds_ls.append(single_df)\npreds_df = pd.concat(preds_ls).groupby('id_seqpos').mean().reset_index()\n# .mean() is for\n# 1, Predictions from multiple models\n# 2, TTA (augmented test data)\n\npreds_ls = []\nfor df, preds in zip(val_df, val_preds):\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n        single_df['SN_filter'] = df[df['id'] == uid].SN_filter.values[0]\n        preds_ls.append(single_df)\nholdouts_df = pd.concat(preds_ls).groupby('id_seqpos').mean().reset_index()","5120184e":"submission = preds_df[['id_seqpos', 'reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']]\nsubmission.to_csv(f'submission.csv', index=False)\nprint(f'wrote to submission.csv')","37f23f08":"def print_mse(prd):\n    val = pd.read_json('..\/input\/stanford-covid-vaccine\/train.json', lines=True)\n\n    val_data = []\n    for mol_id in val['id'].unique():\n        sample_data = val.loc[val['id'] == mol_id]\n        sample_seq_length = sample_data.seq_length.values[0]\n        for i in range(68):\n            sample_dict = {\n                           'id_seqpos' : sample_data['id'].values[0] + '_' + str(i),\n                           'reactivity_gt' : sample_data['reactivity'].values[0][i],\n                           'deg_Mg_pH10_gt' : sample_data['deg_Mg_pH10'].values[0][i],\n                           'deg_Mg_50C_gt' : sample_data['deg_Mg_50C'].values[0][i],\n                           }\n            val_data.append(sample_dict)\n    val_data = pd.DataFrame(val_data)\n    val_data = val_data.merge(prd, on='id_seqpos')\n\n    rmses = []\n    mses = []\n    for col in ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']:\n        rmse = ((val_data[col] - val_data[col+'_gt']) ** 2).mean() ** .5\n        mse = ((val_data[col] - val_data[col+'_gt']) ** 2).mean()\n        rmses.append(rmse)\n        mses.append(mse)\n        print(col, rmse, mse)\n    print(np.mean(rmses), np.mean(mses))","8a8ba60e":"print_mse(holdouts_df)","1885930c":"print_mse(holdouts_df[holdouts_df.SN_filter == 1])","b90971e3":"This notebook will demonstrate feature engineering and augmentation for the GRU\/LSTM model.\n\nGRU\/LSTM part is mainly based on [OpenVaccine: Simple GRU Model](https:\/\/www.kaggle.com\/xhlulu\/openvaccine-simple-gru-model).\n","e1f8930c":"## Validation","1555fd85":"## Load and preprocess data","c5c98ab7":"This file was created using ARNIE, ViennaRNA and bpRNA in the following way.\n\nGet candidate structures with different gamma values. \nSee last cell of [How to Use ARNIE on Kaggle Notebook](https:\/\/www.kaggle.com\/its7171\/how-to-use-arnie-on-kaggle-notebook).\n\nRemove the same as the original structure.\n\nGet a structure with the largest score for each sequence.\n\nGet the predicted_loop_type from the sequence and structure.\nSee [How to Generate predicted_loop_type](https:\/\/www.kaggle.com\/its7171\/how-to-generate-predicted-loop-type).","c1aeba38":"## post process","bd6965f9":"## Build and train model","e71d4749":"## Data augmentation for training and TTA(test)"}}