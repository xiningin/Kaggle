{"cell_type":{"bf274a1c":"code","b530600d":"code","43033468":"code","78da350e":"code","f06ad849":"code","c0b1b75e":"code","6ad697d4":"code","a2ee5190":"code","6f44f379":"code","cc65c612":"code","82a1cd0a":"code","65bc8db1":"code","b279d214":"code","40492938":"code","015c18b3":"code","9636da0a":"code","353f8c3c":"code","013af004":"code","ae004373":"code","264a02d9":"code","c98f9a7b":"code","cdd802d6":"code","e6c2a4d7":"code","61737483":"code","79c1b7dc":"code","e0dc3a03":"code","e1fc8807":"code","6d4e12f5":"code","894094cb":"code","5b17530c":"markdown","4064cdac":"markdown","93f738fb":"markdown","a85dd30a":"markdown","236b4c32":"markdown","43308fd5":"markdown","cabaa6d8":"markdown","4e78c933":"markdown","d491a276":"markdown","f79b7602":"markdown","0b3e86e0":"markdown","0a07d29e":"markdown","d48b80ee":"markdown"},"source":{"bf274a1c":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd \nimport numpy as np\nfrom numpy import mean\nfrom numpy import std\nimport feature_engine as ft\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scikitplot as skplt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.feature_selection import VarianceThreshold\nfrom feature_engine.encoding import OneHotEncoder\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import classification_report\n\nimport imblearn\nfrom imblearn.over_sampling import SMOTE","b530600d":"data = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","43033468":"data.head()","78da350e":"data.info()","f06ad849":"data.describe()","c0b1b75e":"data.nunique()","6ad697d4":"X = data.drop(labels = ['output'], axis = 1)\ny = data['output']","a2ee5190":"categorical = [feature for feature in X.columns if X[feature].nunique() < 6]\ncontinous = [feature for feature in X.columns if feature not in categorical and X[feature].dtype != 'object']","6f44f379":"print('Catgorical variable are ' , categorical, ',total they are', len(categorical))\nprint('Continous variables are ', continous, ',total they are ', len(continous))","cc65c612":"X[categorical] = X[categorical].astype('object')","82a1cd0a":"X_train, X_test , y_train, y_test = train_test_split(X,y, test_size = 0.1, stratify = y, random_state = 42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","65bc8db1":"oht = OneHotEncoder(top_categories = None,\n              variables = categorical,\n              drop_last = False\n              \n             )\noht.fit(X_train)\nX_train = oht.transform(X_train)\nX_test = oht.transform(X_test)","b279d214":"X_train.shape, X_test.shape","40492938":"sns.countplot(y_train)\nplt.title('Before SMOTE')\nplt.show()","015c18b3":"smote = SMOTE(random_state= 42)\nx_train ,Y_train = smote.fit_resample(X_train, y_train)\nsns.countplot(Y_train)\nplt.title('After Smote')\nplt.show()","9636da0a":"clf = RandomForestClassifier()\n\nparam_grid = { \"criterion\" : [\"gini\", \"entropy\"], \n              \"min_samples_leaf\" : [1, 5, 10], \n           \"min_samples_split\" : [2, 4, 10, 12, 16],\n              \"n_estimators\": [50, 100, 400, 700, 1000],\n              \"max_depth\" : [None, 5, 10 ,20],\n             \"max_features\" : ['auto', 'sqrt', 'log2'],\n             \"bootstrap\" : [True, False]\n}\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1, verbose= 2)\n\ngs.fit(x_train, Y_train)\n\n\n","353f8c3c":"#Model = gs.best_estimator_\nModel = RandomForestClassifier(max_depth=5, max_features='log2', min_samples_leaf=5,\n                       n_estimators=50)\nModel.fit(x_train, Y_train)","013af004":"Model.score(x_train, Y_train)","ae004373":"roc_auc_score(y_test, Model.predict_proba(X_test)[:, 1])","264a02d9":"y_pred = Model.predict_proba(X_test)\n\n","c98f9a7b":"skplt.metrics.plot_roc_curve(y_test, y_pred)\nplt.show()\n","cdd802d6":"importance  = Model.feature_importances_\nfeatures = pd.Series(importance)\nfeatures.index = X_train.columns\nfeatures.sort_values(ascending = False, inplace = True)\nfeatures.plot.bar(orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2, )\nplt.show()","e6c2a4d7":"features = list (features.index)","61737483":"## Making with 1 feature at first\nmodel_one_feature = RandomForestClassifier(max_depth=5, max_features='log2', min_samples_leaf=5,\n                       n_estimators=50)\n\nmodel_one_feature.fit(X_train[features[0]].to_frame(), y_train)\n\ny_pred = model_one_feature.predict_proba(X_test[features[0]].to_frame())[:,1]\n\nroc_first = roc_auc_score(y_test, y_pred)\nprint('Test one feature  ROC AUC=%f' % (roc_first))\n","79c1b7dc":"tol = 0.00001\nfeature_to_keep = [features[0]]\n\nfor feature in features[1:]:\n    \n    model_int = RandomForestClassifier(max_depth=5, max_features='log2', min_samples_leaf=5,\n                       n_estimators=50)\n    \n    model_int.fit(X_train[feature_to_keep + [feature]], y_train)\n    \n    y_pred_test = model_int.predict_proba(\n        X_test[feature_to_keep + [feature] ])[:, 1]\n    \n    roc_int = roc_auc_score(y_test, y_pred_test)\n    \n    diff_roc = roc_int - roc_first\n    \n    if diff_roc >= tol:\n        roc_first = roc_int\n        feature_to_keep.append(feature)\n        print('FEATURE ADDED: ->', feature)\n    else:\n        print('REMOVED :->', feature)\n    \nprint(len(feature_to_keep))","e0dc3a03":"print(feature_to_keep)","e1fc8807":"model_final =  RandomForestClassifier(max_depth=5, max_features='log2', min_samples_leaf=5,\n                       n_estimators=50, random_state= 42)\nmodel_final.fit(X_train[feature_to_keep], y_train)\ny_pred_test = model_final.predict_proba(X_test[feature_to_keep])[:,1]\n\nroc_final = roc_auc_score(y_test, y_pred_test)\nprint('Test selected features ROC AUC=%f' % (roc_final))\n","6d4e12f5":"importance  = model_final.feature_importances_\nfeat = pd.Series(importance * 100)\nfeat.index = X_train[feature_to_keep].columns\nfeat.sort_values(ascending = False, inplace = True)\nfeat.plot.bar(orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2, )\nplt.show()","894094cb":"y_pred_tested = model_final.predict(X_test[feature_to_keep])\nprint(classification_report(y_test, y_pred_tested))","5b17530c":"**Dropping the output column**","4064cdac":"### Grid Search to find the best parameters","93f738fb":"**Now we are balanced**","a85dd30a":"**We are using test size of 0.1, which means more data for our train set**","236b4c32":"**Now assigning the Data types**","43308fd5":"**Dividing into continous and categorical features, we have set the uniqueness threshold to 6 for that**","cabaa6d8":"**let see which can be divided to continous and categorical**","4e78c933":"**There is a Class imbalence of 20 samples we will use Smote to balance that avoid biasness**","d491a276":"**Since all of our categroical features are not ordinal we will be using one hot encoder**","f79b7602":"**Grid search takes time, so we will not be running here but have stored its results**","0b3e86e0":"### Using only six features out of 30 gives us improved performace","0a07d29e":"### Using Recursive feature addition to make efficient Model with minimum features ","d48b80ee":"**We have over 12 features and one output, all of which have no missing values**"}}