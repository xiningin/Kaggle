{"cell_type":{"9f99f290":"code","5eeed9e1":"code","4daac329":"code","e679ccb3":"code","823f7514":"code","ff425535":"code","6f76ee3c":"code","3f3e1023":"code","e5ec72f5":"code","ce2d221e":"code","91cab2f5":"code","b197dd13":"code","466eaa53":"code","b5b7200b":"code","63fe5898":"code","fd806d78":"code","701a93ae":"code","f1eb7ca5":"code","212a9b03":"code","e1a83276":"code","99fcc744":"code","83691cb3":"code","b18f6d22":"code","22bfb3fc":"code","d1b6cb4a":"code","56c97904":"code","d296bf74":"markdown","23edc637":"markdown","f15df683":"markdown","78905d8f":"markdown","5d74048a":"markdown","95407b57":"markdown","73cb4574":"markdown","496ad227":"markdown","cc4fa284":"markdown","885c369f":"markdown","b26621d2":"markdown","f4f24396":"markdown","f866dc91":"markdown","3208f650":"markdown","250172c4":"markdown"},"source":{"9f99f290":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5eeed9e1":"### Import libraries ###\nimport matplotlib.pyplot as plt\nimport seaborn as sns","4daac329":"# Use Jupyter Widges Package - https:\/\/ipywidgets.readthedocs.io\/en\/stable\/index.html \nfrom __future__ import print_function\nfrom ipywidgets import interact, interactive, fixed, interact_manual, Layout\nimport ipywidgets as widgets\nstyle = {'description_width': 'initial'}\n\npd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 300)","e679ccb3":"icu_data_path_training = \"\/kaggle\/input\/widsdatathon2021\/TrainingWiDS2021.csv\"\nicu_data_path_test = \"\/kaggle\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\"\ndata_dictionary_path = \"\/kaggle\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv\"\n\nicu_df_training = pd.read_csv(icu_data_path_training, index_col=0) ### index_col ###\nicu_df_test = pd.read_csv(icu_data_path_test, index_col=0)\ndata_dictionary_df = pd.read_csv(data_dictionary_path, index_col=0)","823f7514":"search_column_name = 'Variable Name'\n# You can also use:'Variable Name','Category','Data Type','Unit of Measure' for search_column_name\n\n### Print the selected data rows ###\ndef f(x):\n    return data_dictionary_df[data_dictionary_df[search_column_name].isin(list(x))]\n\n### Multiple selection widgets ###\nwidget_variable=widgets.SelectMultiple(\n    options=data_dictionary_df[search_column_name].unique(),\n    layout=Layout(width='35%', height='150px'),\n    description=search_column_name, \n    style = style\n)\ninteract(f, x=widget_variable);","ff425535":"### Have a brief look at training dataset ###\nprint(icu_df_training.shape) #dataset number of rows and columns\n# icu_df_training.head(10)  # sample of 10 rows\n# icu_df_training.describe()","6f76ee3c":"### Have a brief look at test dataset ###\nprint(icu_df_test.shape) #dataset number of rows and columns\n# icu_df_test.head(10)  # sample of 10 rows\n# icu_df_test.describe()","3f3e1023":"cat_variable = \"ethnicity\"\n\nfig, ax  = plt.subplots(figsize=(8, 6))\nfig.suptitle('Distribution of %s' %cat_variable, size = 15)\nlabels = list(icu_df_training[cat_variable].dropna().unique()) # Pay attention to dropna()\nexplode = (0.05, 0.05, 0.05, 0.05, 0.2, 0.3)\nsizes = icu_df_training[cat_variable].value_counts()\nax.pie(sizes, explode=explode, startangle=60, labels=labels,autopct='%1.0f%%', pctdistance=0.9) #explode=explode\nax.add_artist(plt.Circle((0,0),0.4,fc='white'))\nplt.tight_layout()\nplt.show()","e5ec72f5":"### hist plot categorical features ###\ndef count_plot(var, icu_df_training):\n    plt.figure(figsize = (10,8))\n    ax = sns.countplot(y = var, data = icu_df_training)\n    plt.title(var, size = 15)\n\ndef inter_cat_plot(x):\n    return count_plot(x, icu_df_training)\n\n### Multiple selection widgets ###\nwidget_cat_plot=widgets.Dropdown(\n    options=data_dictionary_df[data_dictionary_df['Data Type'] =='string']['Variable Name'],\n    value='hospital_admit_source',\n    description=\"Numeric Variable:\", \n    style = style\n)\ninteract(inter_cat_plot, x=widget_cat_plot);","ce2d221e":"!pip install pywaffle","91cab2f5":"from pywaffle import Waffle\n\n### hist plot binary features ###\ndef binary_plot(var, icu_df_training):\n    binary_fig=plt.figure(\n        figsize=(10, 8),\n        FigureClass=Waffle,\n        rows=7,\n        columns=15,\n        icons='child',\n        values=dict(icu_df_training[var].value_counts()),\n        font_size=30,\n#         rounding_rule='floor',\n        legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)}\n    )\n\ndef inter_binary_plot(x):\n    return binary_plot(x, icu_df_training)\n\n### Multiple selection widgets ###\nwidget_binary_plot=widgets.Dropdown(\n    options=data_dictionary_df[data_dictionary_df['Data Type'] =='binary']['Variable Name'],\n    description=\"Binary Variable:\", \n    style = style\n)\ninteract(inter_binary_plot, x=widget_binary_plot);","b197dd13":"def hist(x,title):\n    plt.figure(figsize = (10,8))\n    ax = sns.distplot(x, kde=False);\n    values = np.array([rec.get_height() for rec in ax.patches])\n    norm = plt.Normalize(values.min(), values.max())\n    colors = plt.cm.jet(norm(values))\n    for rec, col in zip(ax.patches, colors):\n        rec.set_color(col)\n    plt.title(title, size = 15)","466eaa53":"### hist plot numeric features ###\ndef inter_num_plot(x):\n    return hist(icu_df_training[x],'Distribution of %s' %x)\n\n### Multiple selection widgets ###\nwidget_numeric_plot=widgets.Dropdown(\n    options=data_dictionary_df[data_dictionary_df['Data Type'] =='numeric']['Variable Name'],\n    description=\"Numeric Variable:\", \n    style = style\n)\ninteract(inter_num_plot, x=widget_numeric_plot);","b5b7200b":"part_of_df = icu_df_training[data_dictionary_df[data_dictionary_df['Data Type'] =='numeric']['Variable Name'][0:30]]\n\ndef corr_plot(input_df):\n    # Compute a correlation matrix and plot it into a heatmap\n    corr_df = input_df.corr()\n    corr_mat = corr_df[corr_df.columns[::-1]].stack().reset_index(name=\"correlation\")\n\n    # Draw each cell as a scatter point with varying size and color\n    sns.set_style(\"whitegrid\")\n    g = sns.relplot(\n        data=corr_mat,\n        x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\",\n        palette=\"vlag\", edgecolor=\".7\",\n        height=10, sizes=(50, 250), hue_norm=(-0.5, 1),size_norm=(-0.5, 1), \n    )\n\n    # Tweak the figure to finalize\n    g.set(xlabel=\"\", ylabel=\"\", aspect=\"equal\")\n    g.despine(left=True, bottom=True)\n    g.ax.margins(.02)\n    for label in g.ax.get_xticklabels():\n        label.set_rotation(90)\n\ncorr_plot(part_of_df)","63fe5898":"### check if there is any missing value in the dataset ###\ndef check_missing(df, col):\n    missing  = 0\n    misVariables = []\n    CheckNull = df.isnull().sum()\n    for var in range(0, len(CheckNull)):\n        misVariables.append([col[var], CheckNull[var], round(CheckNull[var]\/len(df),3)])\n        missing = missing + 1\n\n    if missing == 0:\n        print('Dataset is complete with no blanks.')\n    else:\n        df_misVariables = pd.DataFrame.from_records(misVariables)\n        df_misVariables.columns = ['Variable', 'Missing', 'Percentage']\n        s = df_misVariables.sort_values(by=['Percentage'], ascending=False).style.bar(subset=['Percentage'], color='#d65f5f')\n        display(s)\n    return df_misVariables\n        \nranked_df_missing_value = check_missing(icu_df_training, icu_df_training.columns)","fd806d78":"### Take all variables which has less than 5% missing values ###\n\ndf_training_partial = icu_df_training[ranked_df_missing_value[ranked_df_missing_value['Percentage']<0.05]['Variable']]\ndf_test_partial = icu_df_test[ranked_df_missing_value[ranked_df_missing_value['Percentage']<0.05]['Variable'][:-1]]\n\n### Remove the rows with missing values ###\n### Please note how many rows you have excluded ###\ndf_training_partial = df_training_partial.dropna().reset_index(drop=True)\n\nprint(\"Original training dataset (rows):\", len(icu_df_training))\nprint(\"After removing missing (rows):\", len(df_training_partial))\nprint(\"Original test dataset (rows):\", len(icu_df_test))\n","701a93ae":"train_test.reindex","f1eb7ca5":"df_training_partial['label'] = 1\ndf_test_partial['label'] = 0\n\ntrain_test = pd.concat([df_training_partial, df_test_partial]).reset_index(drop=True)\n## impute \ncategorical_cols = ['icu_id', 'ethnicity', 'gender',\n                'icu_admit_source', 'icu_stay_type', 'icu_type',\n                'aids', 'cirrhosis', 'hepatic_failure','ethnicity',\n                'apache_2_diagnosis', 'apache_3j_diagnosis']\n\n\nbinary_cols = ['elective_surgery','apache_post_operative','arf_apache',\n            'intubated_apache','ventilated_apache','aids','cirrhosis',\n            'gcs_eyes_apache','gcs_verbal_apache', 'gcs_motor_apache' ,\n            'gcs_unable_apache','hepatic_failure','readmission_status',\n            'immunosuppression','leukemia','lymphoma','solid_tumor_with_metastasis']\n\n\nfor i, v in enumerate(binary_cols):\n    train_test[v] = train_test[v].fillna(10)\n\n# filling categorical columns with median value\nfor i, v in enumerate(categorical_cols):\n    train_test[v] = train_test[v].fillna(train_test[v].value_counts().index[0])\n\nfeatures = train_test.columns\nnumeric_cols = [col for col in features if col not in categorical_cols and col not in binary_cols]\n\nfor i, v in enumerate([numeric_cols]):\n    train_test[v] = train_test[v].apply(lambda x: x.fillna(x.mean()))\n    \nprint(\"The missing values in the train dataset are:\", train_test.isnull().sum().sum()) ","212a9b03":"int_str_variables = data_dictionary_df[data_dictionary_df['Data Type'].isin([\"integer\",\"string\"])][\"Variable Name\"]\nfor col in df_training_partial.columns:\n    if col in list(int_str_variables):\n        print(col)","e1a83276":"### A basic and simple encoding: One-hot Encoding ###\n# https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#encoding-categorical-features #\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# drop_enc = OneHotEncoder(drop='first').fit(df_training_partial[[\"ethnicity\", \"gender\", \"icu_admit_source\"]])\n# # drop_enc.categories_\n# enc_training_features = pd.DataFrame(drop_enc.transform(df_training_partial[[\"ethnicity\", \"gender\", \"icu_admit_source\"]]).toarray())\n# enc_test_features = pd.DataFrame(drop_enc.transform(df_test_partial[[\"ethnicity\", \"gender\", \"icu_admit_source\"]]).toarray())\n\ncategorical_cols = [\"ethnicity\", \"gender\", \"icu_admit_source\"]\ndrop_enc = OneHotEncoder(drop='first').fit(train_test[categorical_cols])\n# # drop_enc.categories_\nenc_features = pd.DataFrame(drop_enc.transform(train_test[categorical_cols]).toarray())\n","99fcc744":"drop_cat_feature = train_test.drop([\"ethnicity\", \"gender\", \"icu_admit_source\",\"icu_stay_type\", \n                                                                   \"icu_type\",\"encounter_id\", \"hospital_id\", \"icu_id\"], axis=1)\ntrain_test_encoded = pd.concat([drop_cat_feature,enc_features.reindex(drop_cat_feature.index)],axis=1) ","83691cb3":"### Will introduce in Day-2 ###\n# Lets first use all 60+ features we have now #\n\ndef feature_imp(df_com, target):\n    from sklearn.tree import ExtraTreeRegressor\n    from collections import Counter\n    # Build a forest and compute the feature importances\n    forest = ExtraTreeRegressor(splitter='best', max_leaf_nodes=500)\n\n    forest.fit(df_com, target)\n    importances = forest.feature_importances_\n    # std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n    indices = np.argsort(importances)[::-1]\n\n#     # Print the feature ranking\n#     print(\"Feature ranking:\")\n\n#     for f in range(df_com.shape[1]):\n#         print(\"%d. feature %d %s (%f)\" % (f + 1, indices[f], df_com.columns[indices[f]], importances[indices[f]]))\n\n#     # Plot the feature importances of the forest\n#     plt.figure()\n#     plt.title(\"Feature importances\")\n#     plt.bar(range(df_com.shape[1]), importances[indices],\n#            color=\"r\", align=\"center\")\n#     plt.xticks(range(df_com.shape[1]), indices)\n#     plt.xlim([-1, df_com.shape[1]])\n#     plt.show()\n    \n    return indices\nindices = feature_imp(train_test_encoded.drop(['diabetes_mellitus','label'],axis=1), train_test_encoded['diabetes_mellitus'])","b18f6d22":"## Very simple Decision Tree Classifier ##\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrain_dataset = train_test_encoded[train_test_encoded['label']==1]\n\ndf_training_partial_encoded_target = train_dataset['diabetes_mellitus']\ndf_training_partial_encoded_features = train_dataset[train_dataset.columns[indices[0:29]]]#df_training_partial_encoded.drop(['diabetes_mellitus'], axis=1)\n\n# clf = DecisionTreeClassifier(random_state=2021, class_weight='balanced')\nclf = RandomForestClassifier(class_weight=\"balanced_subsample\", max_depth=None, random_state=None)\ncross_val_score(clf, df_training_partial_encoded_features, df_training_partial_encoded_target, cv=10)","22bfb3fc":"clf.fit(df_training_partial_encoded_features, df_training_partial_encoded_target)\n\ntest_dataset = train_test_encoded[train_test_encoded['label']==0].drop(['diabetes_mellitus', 'label'],axis=1)\ndf_test_partial_encoded_features = test_dataset[test_dataset.columns[indices[0:29]]]\n\npred = clf.predict_proba(df_test_partial_encoded_features)[:, 1]","d1b6cb4a":"pd.Series(pred).plot.kde()","56c97904":"icu_df_test['label'] = 'test'\nicu_df_test['diabetes_mellitus'] = pred\nicu_df_test[[\"encounter_id\", \"diabetes_mellitus\"]].to_csv(\"submission_WiDS_csun.csv\", index=False)","d296bf74":"## Model training [Challenge4 Dive into details on Day 2]##\n1. Challenge-4.1 Imbalanced target feature (patient without diabetes >> patient with diabetes)\n2. Challenge-4.2 Model selection (select the optimal model based on your dataset and prediction)","23edc637":"> ***Why is it important to identify patient with diabetes in the ICU***\n\nMany studies have observed that patients who do not know they have diabetes when hospitalized in an ICU have greater complications during their stay and poorer outcomes than patients without diabetes [(Ref)](https:\/\/news.emory.edu\/stories\/2015\/12\/coopersmith_unknown_diabetes_in_ICU_study\/index.html). In the recent COVID-19 pandemic, people with diabetes have a high risk to require ICU admission. Probably they are not exposed to higher risk of being infected, however, in the case, particularly when the metabolic control is not sufficient, they are more prone to serious complications and to die. Therefore, it\u2019s critical for medical professionals to identify diabetes so that a patient can be appropriately treated.\n","f15df683":"# Plot binary features #","78905d8f":"# Plotting catogrical variables\/features\/attributes #\nVisualization was inspired by notebook: <[WiDS Datathon 2021: RAPIDS+XGB](http:\/\/www.kaggle.com\/ruchi798\/wids-datathon-2021-rapids-xgb\/notebook)> ","5d74048a":"## Feature Selection [Challenge3 Dive into details on Day 2]##","95407b57":"## Encoding [Challenge2 Dive into details on Day 2]##\nWhy we need to encode features?\nThere are many ways to encode the features... \nFigure out the differences between them, and how to choose the optimal one for this dataset, and why?","73cb4574":"# Load training\/test data","496ad227":"# WiDS Maastricht Datathon Training Session Notebook #\nThis jupyter notebook is prepared for [WiDS Maastricht Datathon](https:\/\/www.maastrichtuniversity.nl\/events\/wids-datathon-maastricht-2021) training session on 5-6th Feb 2021. The goal is to provide background knowledge and guidence for participants to start the data challenge. To get more materials and tutorials, please visit our [github repository](https:\/\/github.com\/MaastrichtU-IDS\/kaggle-datathon-instructions).\n\nThis notebook is delivered by [Chang Sun](https:\/\/www.linkedin.com\/in\/chang-sun-maastricht\/), [Andreea Grigorio](https:\/\/www.linkedin.com\/in\/andreea-grigoriu\/), [Yenisel Plasencia Cala\u00f1a](https:\/\/www.linkedin.com\/in\/yenisel-plasencia-cala%C3%B1a-phd-10144190\/).","cc4fa284":"> ***About Diabetes***\n\nDiabetes is a disease in which your blood glucose, or blood sugar, levels are too high. Glucose comes from the foods you eat. Insulin is a hormone that helps the glucose get into your cells to give them energy. With type 1 diabetes, your body does not make insulin. With type 2 diabetes, the more common type, your body does not make or use insulin well.\n\nIn this dataset, the target feature - diabetes_mellitus is a binary featuers (yes or no). However, in fact, there are three main types of diabetes mellitus so that patients might have very different symptoms but all labeled as diabetes patients (in this dataset). \n\n<img src=\"https:\/\/www.who.int\/images\/default-source\/departments\/ncds\/diabetes\/whd2016-diabetes-infographic-v2-page-1.png?sfvrsn=a5638d91_2\" width=\"500\">\n","885c369f":"# Background #\n\n> ***What is ICU and why prediction models are needed in the ICU?***\n\nThe ICU is designed to care for patients who are seriously injured, have a critical or life-threatening illness, or have undergone a major surgical procedure, therefore requiring 24-hour care and monitoring. Medical professionals in the ICU usually need to make high-stakes, difficult, emotive decisions with limited patient information, high uncertainty about outcomes and extreme pressure. It is dramatically challenging for medical professionals to integrate all the dynamic measurements from the devices in a short time and combine their experience and beliefs to make a quick decision without knowing the medical history of the patient. Using machine learning models that are trianed on preivous ICU data will allow medical professionals to combine the temporal predictions with their existing beliefs to facilitate decision making.\n\nThe following figure presents how machine learning models can be trained on previous ICU data and and applied to new individual patiens. This figure is from this [paper](https:\/\/www.thelancet.com\/journals\/landig\/article\/PIIS2589-7500(20)30018-2\/fulltext) published in 2020. Highly recommend our participants to read!\n\n<img src=\"https:\/\/els-jbs-prod-cdn.jbs.elsevierhealth.com\/cms\/asset\/3318d4c4-2ecd-4d9c-9f1d-2eb877f3f634\/gr1.jpg\" width=\"600\">","b26621d2":"### Plotting numerical variables\/features\/attributes ###","f4f24396":"## Check missing values in each variable","f866dc91":"## Correlation Matrix ","3208f650":"## Handling missing values [Challenge1 Dive into details on Day 2] ##\nThere are many ways to handle missing values... ...\nPlease note that you need to do the same for training and test dataset","250172c4":"![](http:\/\/)**\"icu_admit_type\" in data dictionary file, but not in the training files.**"}}