{"cell_type":{"088a577c":"code","c9fe81a8":"code","9e271b65":"code","1cf8fc15":"code","5387d671":"code","826e7fce":"code","6dd81ace":"code","53133b5a":"code","d710b119":"code","cc29efc2":"code","7a2185d7":"code","8c5003d7":"code","f260e98b":"code","a68184b5":"code","30e203ab":"code","ca81a886":"code","64cd0cdd":"code","f5b11d54":"code","4425795b":"code","3b3a17de":"code","75039b18":"code","80999978":"code","5f64cba7":"code","eedcadec":"code","28083e6e":"code","3a858567":"code","e81d8237":"code","fc75ffe2":"code","9163b8b8":"code","72a4c6b1":"code","b0281e66":"code","73954ade":"code","e7699367":"code","1a9df4b2":"code","8887ba64":"code","cdf9c8ec":"code","59aed295":"code","6d349488":"code","7affd477":"code","8a56d87d":"code","3792b292":"code","31ec252e":"code","719c8574":"code","546dfc5e":"code","c7e5a6b4":"code","35f92b19":"code","d7fbf3ee":"code","b26cc312":"code","0b53cab8":"code","784f972b":"code","78d66352":"code","22b3b545":"code","beaa0ad4":"code","ce61fb81":"code","11f41843":"code","de1ed500":"code","e9ef4721":"code","546bd950":"code","af58e5a1":"code","5391a019":"code","4dd15fba":"code","0e882645":"code","fb29c331":"code","bcdb7266":"code","595a6c2e":"code","daf01b77":"code","e450f1eb":"code","b3b1a891":"markdown","229f9901":"markdown","2c34f83c":"markdown","3414f892":"markdown","4b35d4bb":"markdown","6e68cd42":"markdown","17476eed":"markdown","1c8da885":"markdown","ebe756ef":"markdown","fe031de6":"markdown","ff0ceff6":"markdown","b52f897a":"markdown","8ad9806f":"markdown","a6c5ed89":"markdown","dfeaf8e3":"markdown","58b2e0e5":"markdown","e7e517d5":"markdown","dd142c4d":"markdown","9e34179d":"markdown","f2a38993":"markdown","0e479013":"markdown","21db44e7":"markdown","9760e6b5":"markdown","57990774":"markdown","91595eee":"markdown","47686790":"markdown","097285c3":"markdown","5d7b2818":"markdown","45b05c00":"markdown","ff6c425c":"markdown","ccf1cb3d":"markdown"},"source":{"088a577c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n#plot libaries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA#, FastICA\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n\n# data mining libaries\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA#, FastICA\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV, learning_curve\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\nfrom imblearn.pipeline import make_pipeline, Pipeline\nfrom imblearn.over_sampling import SMOTE\n\nSEED = 10 # specify seed for reproducable results","c9fe81a8":"RANDOM_FOREST_PARAMS = {\n    'clf__max_depth': [25, 50, 75],\n    'clf__max_features': [\"sqrt\"], # just sqrt is used because values of log2 and sqrt are very similar for our number of features (10-19) \n    'clf__criterion': ['gini', 'entropy'],\n    'clf__n_estimators': [100, 300, 500, 1000]\n}\n\nDECISION_TREE_PARAMS = {\n    'clf__max_depth': [25, 50, 75],\n    'clf__max_features': [\"sqrt\"], # just sqrt is used because values of log2 and sqrt are very similar for our number of features (10-19)\n    'clf__criterion': ['gini', 'entropy'],\n    'clf__min_samples_split': [6, 10, 14],\n}\n\nLOGISTIC_REGRESSION_PARAMS = {\n    'clf__solver': ['liblinear'],\n    'clf__C': [0.1, 1, 10],\n    'clf__penalty': ['l2', 'l1']\n}\n\nKNN_PARAMS = {\n    'clf__n_neighbors': [5, 15, 25, 35, 45, 55, 65],\n    'clf__weights': ['uniform', 'distance'],\n    'clf__p': [1, 2, 10]\n}\n\nKNN_PARAMS_UNIFORM = {\n    'clf__n_neighbors': [5, 15, 25, 35, 45, 55, 65],\n    'clf__weights': ['uniform'],\n    'clf__p': [1, 2, 10]\n}\n\nSVM_PARAMS = [\n{\n    'clf__kernel': ['linear'],\n    'clf__C': [0.1, 1, 10],\n}, \n{\n    'clf__kernel': ['rbf'],\n    'clf__C': [0.01, 0.1, 1, 10, 100],\n    'clf__gamma': [0.01, 0.1, 1, 10, 100],\n}]","9e271b65":"data=pd.read_csv('\/kaggle\/input\/wine-data\/wine.data',header=None)\ndata.head(3)","1cf8fc15":"data.columns  = ['Class_Id'\n                 ,'Alcohol'\n                 ,'Malic_Acid'\n                 ,'Ash'\n                 ,'Alcalinity_Ash'\n                 ,'Magnesium'\n                 ,'Total_Phenols'\n                 ,'Flavanoids'\n                 ,'NonFlavanoid_Phenols'\n                 ,'Proanthocyanins'\n                 ,'Color_Intensity'\n                 ,'Hue'\n                 ,'OD280_OD315_DWines'\n                 ,'Proline'\n                ]\n#data.columns = columns_names\n\ndata.head(3)","5387d671":"data.isnull().sum()","826e7fce":"data.info() # prints out a basic information about the data.","6dd81ace":"# This method prints us some summary statistics for each column in our data.\ndata.describe()","53133b5a":"import plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.offline import init_notebook_mode\n#import plotly.express as px\ninit_notebook_mode(connected=True) # to show plots in notebook\n# online plotly\nfrom plotly.offline import plot, iplot\n#plotly.tools.set_credentials_file(username='XXXXXXXXXXXXXX', api_key='XXXXXXXXXXXXXX')\n\n# do not show any warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\ncolors = plotly.colors.DEFAULT_PLOTLY_COLORS\nclass_dict = {1: \"class 1\", 2: \"class 2\",3:\"class 3\"}","d710b119":"y=data[\"Class_Id\"].value_counts()\ndata_bar = [go.Bar(x=[class_dict[x] for x in y.index], \n                y=y.values,\n                marker=dict(color=colors[:len(y.index)])\n               )]\nlayout = go.Layout(\n    title='Class Distribution',\n    autosize=False,\n    width=400,\n    height=400,\n    yaxis=dict(\n        title='#Samples',\n    ),\n)\nfig = go.Figure(data=data_bar, layout= layout)\n#fig.show()\niplot(fig, filename='basic-bar3')","cc29efc2":"import plotly.graph_objects as go\n\ndata_3=[go.Pie(\n    labels=[class_dict[x] for x in y.index],\n    values=y.values,\n    marker=dict(colors=colors[:len(y.index)])\n    \n)]\n\nlayout_percent = go.Layout(\n    title='Percentage class Distribution',\n    autosize=False,\n    width=400,\n    height=400,\n)\nfig=go.Figure(data=data_3,layout=layout_percent)\nfig.show()\n#iplot(fig, filename='new')","7a2185d7":"pre_data = data.copy()\npre_data.head(3)","8c5003d7":"#class_1 = data[\"Class_Id\"].value_counts()\nclass_1 = pre_data[pre_data[\"Class_Id\"] == 1]\nclass_2 = pre_data[pre_data[\"Class_Id\"] == 2]\nclass_3 = pre_data[pre_data[\"Class_Id\"] == 3]\ndef create_box_class_1_trace(col, visible=False):\n    return go.Box(\n        y=class_1[col],\n        name='class 1',\n        marker = dict(color = colors[1]),\n        visible=visible,\n    )\n\ndef create_box_class_2_trace(col, visible=False):\n    return go.Box(\n        y=class_2[col],\n        name='class 2',\n        marker = dict(color = colors[2]),\n        visible = visible,\n    )\n\ndef create_box_class_3_trace(col, visible=False):\n    return go.Box(\n        y=class_3[col],\n        name='class 3',\n        marker = dict(color = colors[3]),\n        visible = visible,\n    )\nfeatures_not_for_hist = [\"Class_Id\"]\nfeatures_for_hist = [x for x in pre_data.columns if x not in features_not_for_hist]\n# remove features with too less distinct values (e.g. binary features), because boxplot does not make any sense for them\nfeatures_for_box = [col for col in features_for_hist if len(class_1[col].unique())>5 or len(class_2[col].unique())>5 or len(class_1[col].unique())>5]\n#features_for_box = [col for col in features_for_hist]\n\nactive_idx = 0\nbox_traces_class_1 = [(create_box_class_1_trace(col) if i != active_idx else create_box_class_1_trace(col, visible=True)) for i, col in enumerate(features_for_box)]\nbox_traces_class_2 = [(create_box_class_2_trace(col) if i != active_idx else create_box_class_2_trace(col, visible=True)) for i, col in enumerate(features_for_box)]\nbox_traces_class_3 = [(create_box_class_3_trace(col) if i != active_idx else create_box_class_3_trace(col, visible=True)) for i, col in enumerate(features_for_box)]\n\ndata_box = box_traces_class_1 + box_traces_class_2 + box_traces_class_3\n\nn_features = len(features_for_box)\nsteps = []\nfor i in range(n_features):\n    step = dict(\n        method = 'restyle',  \n        args = ['visible', [False] * len(data_box)],\n        label = features_for_box[i],\n    )\n    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n    step['args'][1][i + n_features] = True # Toggle i'th trace to \"visible\"\n    step['args'][1][i + n_features + n_features] = True # Toggle i'th trace to \"visible\"\n    steps.append(step)\n\nsliders = [dict(\n    active = active_idx,\n    currentvalue = dict(\n        prefix = \"Feature: \", \n        xanchor= 'center',\n    ),\n    pad = {\"t\": 50},\n    steps = steps,\n    len=1,\n)]\n\nlayout = dict(\n    sliders=sliders,\n    yaxis=dict(\n        title='value',\n        automargin=True,\n    ),\n    legend=dict(\n        x=0,\n        y=1,\n    ),\n)\n\nfig = dict(data=data_box, layout=layout)\n#fig.show()\niplot(fig, filename='box_slider')","f260e98b":"corr = pre_data.corr()\ntrace = go.Heatmap(z=corr.values.tolist(), x=corr.columns, y=corr.columns )\ndata_heatmap=[trace]\nlayout = go.Layout(\n    title='Heatmap of pairwise correlation of the columns',\n    autosize=False,\n    width=850,\n    height=700,\n    yaxis=go.layout.YAxis(automargin=True),\n    xaxis=dict(tickangle=40),\n    margin=go.layout.Margin(l=0, r=200, b=200, t=80),\n\n)\n\n\nfig = go.Figure(data=data_heatmap, layout=layout)\niplot(fig, filename='labelled-heatmap1')","a68184b5":"from scipy.cluster import hierarchy as hc\nX = np.random.rand(10, 10)\nnames = pre_data.columns\ninverse_correlation = 1 - abs(pre_data.corr())\nfig = ff.create_dendrogram(inverse_correlation.values, orientation='left', labels=names, colorscale=colors, linkagefun=lambda x: hc.linkage(x, 'average'))\nfig['layout'].update(dict(\n    title=\"Dendrogram of the features according to correlation\",\n    width=800, \n    height=600,\n    margin=go.layout.Margin(l=180, r=50),\n    xaxis=dict(\n        title='distance',\n    ),\n    yaxis=dict(\n        title='features',\n        automargin=True,\n    ),\n))\niplot(fig, filename='dendrogram_corr_clustering')","30e203ab":"import heapq\n\nprint('Absolute overall correlations')\nprint('-' * 30)\ncorr_abs_sum = corr[corr.columns].abs().sum()\nprint(corr_abs_sum, '\\n')\n\nprint('Weakest correlations')\nprint('-' * 20)\nprint(corr_abs_sum.nsmallest(3),'\\n')\n\nprint('Heighest correlations')\nprint('-' * 20)\nprint(corr_abs_sum.nlargest(4))","ca81a886":"# splitting the dataset into feature vectors and the target variable\ndata_y = pre_data[\"Class_Id\"]\ndata_X = pre_data.drop([\"Class_Id\"], axis=1)\ndata_X.head(3)","64cd0cdd":"# normalize the dataset (note: for decision tree\/random forest it would not be needed)\ndata_X_normed = (data_X - data_X.mean()) \/ data_X.std()\ndata_X_normed.head(3)","f5b11d54":"# calculate the principal components\npca = PCA(random_state=SEED)\ndata_X_pca = pca.fit_transform(data_X_normed)","4425795b":"tot = sum(pca.explained_variance_) # total explained variance of all principal components\nvar_exp = [(i \/ tot) * 100 for i in sorted(pca.explained_variance_, reverse=True)] # individual explained variance\ncum_var_exp = np.cumsum(var_exp) # cumulative explained variance","3b3a17de":"trace_cum_var_exp = go.Bar(\n    x=list(range(1, len(cum_var_exp) + 1)), \n    y=var_exp,\n    name=\"individual explained variance\",\n)\ntrace_ind_var_exp = go.Scatter(\n    x=list(range(1, len(cum_var_exp) + 1)),\n    y=cum_var_exp,\n    mode='lines+markers',\n    name=\"cumulative explained variance\",\n    line=dict(\n        shape='hv',\n    ))\ndata_pca = [trace_cum_var_exp, trace_ind_var_exp]\nlayout = go.Layout(\n    title='Individual and Cumulative Explained Variance',\n    autosize=True,\n    yaxis=dict(\n        title='percentage of explained variance',\n    ),\n    xaxis=dict(\n        title=\"principal components\",\n        dtick=1,\n    ),\n    legend=dict(\n        x=0,\n        y=1,\n    ),\n)\nfig = go.Figure(data=data_pca, layout=layout)\niplot(fig, filename='basic-bar')","75039b18":"n_components = 7\ndata_X_reduced = np.dot(data_X_normed.values, pca.components_[:n_components,:].T)\ndata_X_reduced = pd.DataFrame(data_X_reduced, columns=[\"PC#%d\" % (x + 1) for x in range(n_components)])\ndata_X_reduced.head(3)","80999978":"# prints the best grid search scores along with their parameters.\ndef print_best_grid_search_scores_with_params(grid_search, n=5):\n    if not hasattr(grid_search, 'best_score_'):\n        raise KeyError('grid_search is not fitted.')\n    print(\"Best grid scores on validation set:\")\n    indexes = np.argsort(grid_search.cv_results_['mean_test_score'])[::-1][:n]\n    means = grid_search.cv_results_['mean_test_score'][indexes]\n    stds = grid_search.cv_results_['std_test_score'][indexes]\n    params = np.array(grid_search.cv_results_['params'])[indexes]\n    for mean, std, params in zip(means, stds, params):\n        print(\"%0.3f (+\/-%0.03f) for %r\" % (mean, std * 2, params))","5f64cba7":"def do_gridsearch_with_cv(clf, params, X_train, y_train, cv, smote=None):\n\n    if smote is None:\n        pipeline = Pipeline([('clf', clf)])\n    else:\n        pipeline = Pipeline([('sm', sm), ('clf', clf)])\n    scorer = sklearn.metrics.make_scorer(sklearn.metrics.f1_score, average='weighted') \n    gs = GridSearchCV(pipeline, params, cv=kf, n_jobs=-1, scoring=scorer, return_train_score=True)\n    gs.fit(X_train, y_train)\n    return gs\n\ndef score_on_test_set(clfs, datasets):\n    scores = []\n    for c, (X_test, y_test) in zip(clfs, datasets):\n        scores.append(c.score(X_test, y_test))\n    return scores","eedcadec":"#features closely related\ncorrelated_features = [\"Flavanoids\",\"Total_Phenols\",\"OD280_OD315_DWines\"]","28083e6e":"# split data into train and test set in proportion 4:1 for all differntly preprocessed datasets\n#full train dataset\nX_train, X_test, y_train, y_test = train_test_split(data_X_normed, data_y, test_size=0.2, random_state=SEED)\n#removing the most correlated features\ncols_without_duplicate = [x for x in data_X_normed.columns if x not in correlated_features]\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(data_X_normed[cols_without_duplicate], data_y, test_size=0.2, random_state=SEED)\n#using pca\nX_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(data_X_reduced, data_y, test_size=0.2, random_state=SEED)\n","3a858567":"print(\"Full train dataset:\", X_train.shape)\nprint(\"Train dataset with reduced features\", X_train_red.shape)\nprint(\"Train dataset using the first 7 Principal Components\", X_train_pca.shape)","e81d8237":"sm = SMOTE(random_state=SEED)\nkf = StratifiedKFold(n_splits=5, random_state=SEED)\nclf_rf = RandomForestClassifier(random_state=SEED)\nclf_balanced = RandomForestClassifier(random_state=SEED, class_weight=\"balanced\")","fc75ffe2":"%%time\nimport sklearn\ngs_full = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train, y_train, kf, smote=None)\ngs_red = do_gridsearch_with_cv(clf_rf,RANDOM_FOREST_PARAMS, X_train_red, y_train_red, kf, smote=None)\ngs_pca = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train_pca, y_train_pca, kf, smote=None)\ngss_raw = [gs_full, gs_red, gs_pca]","9163b8b8":"test_results_raw = score_on_test_set(gss_raw, [(X_test, y_test),(X_test_red, y_test_red), (X_test_pca, y_test_pca)])","72a4c6b1":"%%time\ngs_full_balanced = do_gridsearch_with_cv(clf_balanced, RANDOM_FOREST_PARAMS, X_train, y_train, kf, smote=None)\ngs_red_balanced = do_gridsearch_with_cv(clf_balanced, RANDOM_FOREST_PARAMS, X_train_red, y_train_red, kf, smote=None)\ngs_pca_balanced = do_gridsearch_with_cv(clf_balanced, RANDOM_FOREST_PARAMS, X_train_pca, y_train_pca, kf, smote=None)\ngss_balanced_weights = [gs_full_balanced,gs_red_balanced, gs_pca_balanced]","b0281e66":"test_results_balanced_weights = score_on_test_set(gss_balanced_weights, [(X_test, y_test),(X_test_red, y_test_red), (X_test_pca, y_test_pca)])","73954ade":"%%time\ngs_full_smote = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train, y_train, kf, smote=sm)\ngs_red_smote = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train_red, y_train_red, kf, smote=sm)\ngs_pca_smote = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train_pca, y_train_pca, kf, smote=sm)\ngss_smote = [gs_full_smote,gs_red_smote, gs_pca_smote]","e7699367":"\ntest_results_smote = score_on_test_set(gss_smote, [(X_test, y_test),(X_test_red,y_test_red), (X_test_pca, y_test_pca)])","1a9df4b2":"dataset_strings = [\"full dataset\",\"dataset with Reduced\",\"dataset with first 7 principal components\"]\nmethod_strings = [\"without any balancing\", \"using balanced class weights\", \"using SMOTE\"]\n\nresult_strings = dict()\nfor ms, results in zip(method_strings, [test_results_raw, test_results_balanced_weights, test_results_smote]):\n    for ds, res in zip(dataset_strings, results):\n        string = \"%.3f\" % res + \"     \" + ds + \" \" + ms\n        result_strings[string] = res\n        2\nresult_strings = sorted(result_strings.items(), key=lambda kv: kv[1], reverse=True)\nprint(\"F1 score  dataset and method\")\nprint(\"-\"* 30)\nfor k,_ in result_strings:\n    print(k)","8887ba64":"def get_color_with_opacity(color, opacity):\n    return \"rgba(\" + color[4:-1] + \", %.2f)\" % opacity\n\n# partially based on https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html\ndef plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    scorer = sklearn.metrics.make_scorer(sklearn.metrics.f1_score, average = 'weighted')\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scorer, random_state=SEED)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    trace1 = go.Scatter(\n        x=train_sizes, \n        y=train_scores_mean - train_scores_std, \n        showlegend=False,\n        mode=\"lines\",\n        name=\"\",\n        hoverlabel = dict(\n            namelength=20\n        ),\n        line = dict(\n            width = 0.1,\n            color = get_color_with_opacity(colors[0], 0.4),\n        ),\n    )\n    trace2 = go.Scatter(\n        x=train_sizes, \n        y=train_scores_mean + train_scores_std, \n        showlegend=False,\n        fill=\"tonexty\",\n        mode=\"lines\",\n        name=\"\",\n        hoverlabel = dict(\n            namelength=20\n        ),\n        line = dict(\n            width = 0.1,\n            color = get_color_with_opacity(colors[0], 0.4),\n        ),\n    )\n    trace3 = go.Scatter(\n        x=train_sizes, \n        y=train_scores_mean, \n        showlegend=True,\n        name=\"Train score\",\n        line = dict(\n            color = colors[0],\n        ),\n    )\n    \n    trace4 = go.Scatter(\n        x=train_sizes, \n        y=test_scores_mean - test_scores_std, \n        showlegend=False,\n        mode=\"lines\",\n        name=\"\",\n        hoverlabel = dict(\n            namelength=20\n        ),\n        line = dict(\n            width = 0.1,\n            color = get_color_with_opacity(colors[1], 0.4),\n        ),\n    )\n    trace5 = go.Scatter(\n        x=train_sizes, \n        y=test_scores_mean + test_scores_std, \n        showlegend=False,\n        fill=\"tonexty\",\n        mode=\"lines\",\n        name=\"\",\n        hoverlabel = dict(\n            namelength=20\n        ),\n        line = dict(\n            width = 0.1,\n            color = get_color_with_opacity(colors[1], 0.4),\n        ),\n    )\n    trace6 = go.Scatter(\n        x=train_sizes, \n        y=test_scores_mean, \n        showlegend=True,\n        name=\"Test score\",\n        line = dict(\n            color = colors[1],\n        ),\n    )\n    \n    data_scatter = [trace1, trace2, trace3, trace4,trace5, trace6]\n    layout = go.Layout(\n        title=title,\n        autosize=True,\n        yaxis=dict(\n            title='F1 Score',\n        ),\n        xaxis=dict(\n            title=\"#Training samples\",\n        ),\n        legend=dict(\n            x=0.8,\n            y=0,\n        ),\n    )\n    fig = go.Figure(data=data_scatter, layout=layout)\n    return iplot(fig, filename=title)","cdf9c8ec":"def plot_feature_importance(feature_importance, title):\n    trace1 = go.Bar(\n        x=feature_importance[:, 0],\n        y=feature_importance[:, 1],\n        marker = dict(color = colors[0]),\n        name='feature importance'\n    )\n    data = [trace1]\n    layout = go.Layout(\n        title=title,\n        autosize=True,\n        margin=go.layout.Margin(l=50, r=100, b=150),\n        xaxis=dict(\n            title='feature',\n            tickangle=30\n        ),\n        yaxis=dict(\n            title='feature importance',\n            automargin=True,\n        ),\n    )\n    fig = go.Figure(data=data, layout=layout)\n    return iplot(fig, filename=title)","59aed295":"%%time\nclf_lr = LogisticRegression(random_state=SEED)\ngs_lr = do_gridsearch_with_cv(clf_lr, LOGISTIC_REGRESSION_PARAMS, X_train, y_train, kf, smote=sm)","6d349488":"print_best_grid_search_scores_with_params(gs_lr)","7affd477":"gs_lr_score = gs_lr.score(X_test, y_test)\ny_pred_lr = gs_lr.predict(X_test)\ncm_lr = confusion_matrix(y_test, y_pred_lr)\ncm_lr = cm_lr.astype('float') \/ cm_lr.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix","8a56d87d":"cm_df = pd.DataFrame(cm_lr.round(3), index=[\"actual class 1\", \"actual class 2\",\"actual class 3\"], columns=[\"predicted class 1\", \"predicted class 2\", \"predicted class 3\"])\ncm_df","3792b292":"plot_learning_curve(gs_lr.best_estimator_, \"Learning Curve of Logistic Regression\", X_train, y_train, cv=5)","31ec252e":"%%time\nclf_knn = KNeighborsClassifier()\ngs_knn = do_gridsearch_with_cv(clf_knn, KNN_PARAMS, X_train, y_train, kf, smote=sm)","719c8574":"print_best_grid_search_scores_with_params(gs_knn)","546dfc5e":"\ngs_knn_score = gs_knn.score(X_test, y_test)\ny_pred_knn = gs_knn.predict(X_test)\ncm_knn = confusion_matrix(y_test, y_pred_knn)\ncm_knn = cm_knn.astype('float') \/ cm_knn.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix","c7e5a6b4":"cm_df = pd.DataFrame(cm_knn.round(3), index=[\"actual class 1\", \"actual class 2\",\"actual class 3\"], columns=[\"predicted class 1\", \"predicted class 2\", \"predicted class 3\"])\ncm_df","35f92b19":"plot_learning_curve(gs_knn.best_estimator_, \"Learning Curve of KNN\", X_train, y_train, cv=5)","d7fbf3ee":"%%time\nclf_svm = svm.SVC(random_state=SEED, probability=True)\ngs_svm = do_gridsearch_with_cv(clf_svm, SVM_PARAMS, X_train, y_train, kf, smote=sm)","b26cc312":"print_best_grid_search_scores_with_params(gs_svm)","0b53cab8":"gs_svm_score = gs_svm.score(X_test, y_test)\ny_pred_svm = gs_svm.predict(X_test)\ncm_svm = confusion_matrix(y_test, y_pred_svm)\ncm_svm = cm_svm.astype('float') \/ cm_svm.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix","784f972b":"cm_df = pd.DataFrame(cm_svm.round(3), index=[\"actual class 1\", \"actual class 2\",\"actual class 3\"], columns=[\"predicted class 1\", \"predicted class 2\", \"predicted class 3\"])\ncm_df","78d66352":"plot_learning_curve(gs_svm.best_estimator_, \"Learning Curve of SVM\", X_train, y_train, cv=5)","22b3b545":"%%time\nclf_dt = DecisionTreeClassifier(random_state=SEED)\ngs_dt = do_gridsearch_with_cv(clf_dt, DECISION_TREE_PARAMS, X_train, y_train, kf, smote=sm)","beaa0ad4":"print_best_grid_search_scores_with_params(gs_dt)","ce61fb81":"gs_dt_score = gs_dt.score(X_test, y_test)\ny_pred_dt = gs_dt.predict(X_test)\ncm_dt = confusion_matrix(y_test, y_pred_dt)\ncm_dt = cm_dt.astype('float') \/ cm_dt.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix","11f41843":"cm_df = pd.DataFrame(cm_dt.round(3), index=[\"actual class 1\", \"actual class 2\",\"actual class 3\"], columns=[\"predicted class 1\", \"predicted class 2\", \"predicted class 3\"])\ncm_df","de1ed500":"feature_importance = np.array(sorted(zip(X_train.columns, gs_dt.best_estimator_.named_steps['clf'].feature_importances_), key=lambda x: x[1], reverse=True))\nplot_feature_importance(feature_importance, \"Feature importance in the decision tree\")","e9ef4721":"\nplot_learning_curve(gs_dt.best_estimator_, \"Learning Curve of the Decision Tree\", X_train, y_train, cv=5)","546bd950":"%%time\nclf_rf = RandomForestClassifier(random_state=SEED)\ngs_rf = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train, y_train, kf, smote=sm)","af58e5a1":"print_best_grid_search_scores_with_params(gs_rf)","5391a019":"gs_rf_score = gs_rf.score(X_test, y_test)\ny_pred_rf = gs_rf.predict(X_test)\ncm_rf = confusion_matrix(y_test, y_pred_rf)\ncm_rf = cm_rf.astype('float') \/ cm_rf.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix","4dd15fba":"cm_df = pd.DataFrame(cm_rf.round(3), index=[\"actual class 1\", \"actual class 2\",\"actual class 3\"], columns=[\"predicted class 1\", \"predicted class 2\", \"predicted class 3\"])\ncm_df","0e882645":"feature_importance_rf = np.array(sorted(zip(X_train.columns, gs_rf.best_estimator_.named_steps['clf'].feature_importances_), key=lambda x: x[1], reverse=True))\nplot_feature_importance(feature_importance_rf, \"Feature importance in the Random Forest\")","fb29c331":"plot_learning_curve(gs_dt.best_estimator_, \"Learning Curve of the Random Forest\", X_train, y_train, cv=5)","bcdb7266":"# code partially from https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html\ndef plot_roc_curve(classifiers, legend, title, X_test, y_test):\n    trace1 = go.Scatter(\n        x=[0, 1], \n        y=[0, 1], \n        showlegend=False,\n        mode=\"lines\",\n        name=\"\",\n        line = dict(\n            color = colors[0],\n        ),\n    )\n    \n    data = [trace1]\n    aucs = []\n    #accuracys = []\n    for clf, string, c in zip(classifiers, legend, colors[1:]):\n        y_test_roc = np.array([([1,2,3] if y else [3,2,1]) for y in y_test])\n        y_score = clf.predict_proba(X_test)\n        \n        # Compute ROC curve and ROC area for each class\n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n        for i in range(2):\n            fpr[i], tpr[i], _ = roc_curve(y_test_roc[:, i], y_score[:, i],pos_label=2)\n            roc_auc[i] = auc(fpr[i], tpr[i])\n            #roc_auc[i] = accuracy_score(fpr[i],tpr[i])\n        # Compute micro-average ROC curve and ROC area\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_roc.ravel(), y_score.ravel(),pos_label=2)\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n        #roc_auc[\"micro\"] = accuracy_score(fpr[\"micro\"], tpr[\"micro\"])\n        \n        aucs.append(roc_auc['micro'])\n        \n       # accuracys.append(roc_auc['micro'])\n\n        trace = go.Scatter(\n            x=fpr['micro'], \n            y=tpr['micro'], \n            showlegend=True,\n            mode=\"lines\",\n            name=string + \" (area = %0.2f)\" % roc_auc['micro'],\n            hoverlabel = dict(\n                namelength=30\n            ),\n            line = dict(\n                color = c,\n            ),\n        )\n        data.append(trace)\n\n    layout = go.Layout(\n        title=title,\n        autosize=False,\n        width=550,\n        height=550,\n        yaxis=dict(\n            title='True Positive Rate',\n        ),\n        xaxis=dict(\n            title=\"False Positive Rate\",\n        ),\n        legend=dict(\n            x=0.4,\n            y=0.06,\n        ),\n    )\n    fig = go.Figure(data=data, layout=layout)\n    return aucs, iplot(fig, filename=title)","595a6c2e":"classifiers = [gs_lr, gs_knn, gs_svm, gs_dt, gs_rf]\nclassifier_names = [\"Logistic Regression\", \"KNN\", \"SVM\", \"Decision Tree\", \"Random Forest\"]\nauc_scores, roc_plot = plot_roc_curve(classifiers, classifier_names, \"ROC curve\", X_test, y_test)\nroc_plot","daf01b77":"accs = []\nrecalls = []\nprecision = []\nresults_table = pd.DataFrame(columns=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"auc\"])\nfor (i, clf), name, auc in zip(enumerate(classifiers), classifier_names, auc_scores):\n    y_pred = clf.predict(X_test)\n    row = []\n    row.append(accuracy_score(y_test, y_pred))\n    row.append(precision_score(y_test, y_pred,average='weighted'))\n    row.append(recall_score(y_test, y_pred,average='weighted'))\n    row.append(f1_score(y_test, y_pred,average='weighted'))\n    row.append(auc)\n    row = [\"%.3f\" % r for r in row]\n    results_table.loc[name] = row","e450f1eb":"results_table","b3b1a891":"\n# **K-Nearest Neighbors**\nThe K-Nearest Neighbors algorithm (KNN) is a non-parametric method, which considers the K closest training examples to the point of interest for predicting its class. This is done by a simple majority vote over the K closest points.\n\nThe hyperparameters of KNN include the following ones, which can be passed to the KNeighborsClassifier of sklearn.neighbors:\n\nn_neighbors: corresponds to K, the number of nearest neighbors considered for the prediction (default=5)\nweights:\nif uniform, then all neighbors have the same weight for the voting (default)\nif distance, then the votes of the neighbors are weighted by the inverse of the distance for the voting\np: the power parameter for the Minkowski metric (default=2)","229f9901":"**Without additional balancing techniques**\n\nWe train the Random Forest on the full dataset, on the dataset with reduced features and on the dataset transformed using the first seven principal components.","2c34f83c":" **Correlation Between features**","3414f892":"**Comparison of differently processed Dataset**","4b35d4bb":"On the basis of f1 score for the datasets we can say that using full dataset in all the three techniques gives the best results. As a conclusion we will use the full dataset and apply SMOTE. ","6e68cd42":"We apply PCA to the dataset in order to reduce the number of features","17476eed":"**Check Missing Values**","1c8da885":" **Comparison of differently preprocessed datasets for classification**\n \nIn this chapter we apply one classifier to the different version of the dataset. The Random Forest classifier is used because it is considered as a great baseline model for most applications. It is described in more detail in the next chapter. The following different versions of the dataset are investigated:\n\n* full dataset\n* dataset with variables reduced by the clustering according to the correlation\n* dataset reduced by considering the first seven principal components after applying PCA\n\nFurthermore, we apply two different methods to deal with the unbalanced target variable. First, we try adjusting the weights for the penalization when the classifier makes a mistake in the training and then we try to oversample the data using the Synthetic Minority Over-sampling Technique.\n\nFor the evaluation of the classifiers on the different datasets, a hold-out test set is used, which has 20% of all the data. To account for the class imbalance of our target variable, we use the f1-score as our main evaluation metric. We define:\n\nTP = #samples for which the prediction is positive and the true label is positive\nFP = #samples for which the prediction is positive but the true label is negative\nTN = #samples for which the prediction is negative and the true label is negative\nFN = #samples for which the prediction is negative but the true label is positive\nThen we define the following:\n\n$\\text{precision} = \\frac{TP}{TP + FP} \\;\\;\\; \\text{and} \\;\\;\\; \\text{recall} = \\frac{TP}{TP + FN}$\n\nThen the f1-score is given by the following equation:\n\n$F_{1}=2\\,\\frac{\\text{precision}\\, \\times \\,\\text{recall}}{\\text{precision} \\, +\\, \\text{recall}}$\n\nEvery classifier has a set of hyperparameters, which can be tuned by training the classifier with different values for these hyperparameters and selecting the classifier with the best score. In order to estimate the performance of a classifier in a more reliable way, k-fold cross validation (CV) is used. In k-fold CV, the training set is divided into a k subsets. Then we train k times our classifier on different unions of k-1 subsets and calculate its score on the subset which was not used for training. Then the final score is calculated by averaging the score of each iteration. In detail, let $C_1, C_2, ... C_k$ be the indices of the samples in each of the $K$ parts of the dataset and let $n_k$ be the number of observations in part $k$. Then the score from the cross validation is computed as follows:\n\n$\\text{Score}_{CV(K)} = \\sum_{k=1}^{K} \\frac{n_k}{n}\\text{Score}_k$.\n\nIn our hyper parameter tuning the Score is the f1-Score defined above.\n\nTo do this analysis, we use the sklearn.model_selection.GridSearchCV object, to which we pass a classifier, a dictionary of hyperparameters with values and a $k$-fold object. For a good trade-off between runtime and accuracy of the score we choose $k=5$, so the classifiers are trained on 80% of the train data in each iteration.","ebe756ef":"Dendogram","fe031de6":"**Using Synthetic Minority Over-sampling Technique\u00b6**\n\nThe Synthetic Minority Over-sampling Technique (SMOTE) algorithm applies KNN approach where it selects one of the k nearest neighbors and computes the vector between the original point and the selected neighbor. The difference is multiplied by random number between (0, 1) and it is added back to original point to obtain the new synthetic point. Geometrically, the synthetic point is somewhere on the line between the original point and its neighbor.\n\nIn the following, we use the SMOTE implementation SMOTE of the imblearn.over_sampling library. Furthermore, we create a Pipeline of applying Smote and then training the classifier, so that it is executed in every fold of x-fold cross validation.","ff0ceff6":"# **Data Exploration**","b52f897a":" **GridsearchCV for Hyperparameter Tuning**","8ad9806f":"The confusion matrix shows that the SVM classifier has a clear bias towards predicting class 3.","a6c5ed89":"**Box Plot**","dfeaf8e3":"# **Classification**","58b2e0e5":"# **Feature Extraction:-**\n","e7e517d5":"**Class Distribution**","dd142c4d":"**Box plot for each features**","9e34179d":"** Train Test Split**","f2a38993":"# Introduction\n\nThis report analyzes wine dataset available to the public through \\href{http:\/\/archive.ics.uci.edu\/ml\/index.php}{UCI Machine Learning Repository} which is a collection of databases, domain theories, and data generators that are used by the machine learning community for the empirical analysis of machine learning algorithms.\nThe dataset can be found here: \\href{http:\/\/archive.ics.uci.edu\/ml\/datasets\/Wine}{Wine Data}.\n\n\n**State of art**\n\nThis dataset is the result of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n\n\nDataset consists of 178 Italian wine samples. Each sample has 14 features: one is the cultivar's identifier, and the others are chemical attributes. The cultivar is a categorical variable that can take only 3 values: (Region) 1, (Region) 2, and (Region) 3. The chemical attributes are all numerical and continuous. This is a list of the 14 variables:\n\nCultivar's Identifier:- \n<br\/> Class Id\n\nThe Attributes are:-\n*  Alcohol\n*  Malic acid\n*  Ash\n*  Alcalinity of ash\n* Magnesium\n* Total phenols\n* Flavanoids\n* Nonflavanoid phenols\n* Proanthocyanins\n* Color intensity\n* Hue\n* OD280\/OD315 of diluted wines\n* Proline","0e479013":"**Heatmap**\nTo visualize these correlations we use a heatmap plot, in which high correlations are coloured more to the yellow and lower ones more to the voilet","21db44e7":"**Percentage Class Distribution**","9760e6b5":"# **Conclusion\u00b6**\nTo conclude the performance of the different classification model, we consider several evalutation metric in addition to the in chapter 4 defined precision, recall and f1-score. Further, let TP, FP, TN, FN be defined as in chapter 4.\n\nAccuracy\nThe accuracy is the percentage of samples classified correctly: $\\text{accuracy} = \\frac{TP + TN}{TP + FP + TN + FN}$.\n\nArea Under the Receiver Operating Characteristic curve (AUC)\nTo introduce this concept, we define the following two metrics:\n\nTrue positive rate (TPR): this is the same as the recall: $FPR = \\text{recall} = \\frac{TP}{FN + TP}$\nFalse positive rate (FPR): this corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points: $FPR = \\frac{FP}{TN + FP}$\nTo plot the Receiver Operating Characteristic (ROC) curve we choose a number of different classification thresholds and compute the TPR and the FPR. So the curve shows the trade-off between these two. To combine the TPR and the FPR into one evaluation metric the area under the ROC curve (AUC) is computed.\n\nThe following plot shows the ROC curves of the classifiers trained in the previous chapters:","57990774":"# **Support-Vector Machine**\nA linear Support-Vector Machine (SVM) finds the optimal hyperplane between the points of two classes such that the distance of the nearest points to the decision boundary is maximized. This distance is called margin.\n\nIf the data set is not linearly separable, we can map the samples ${\\bf x}$ into a feature space of higher dimensions: ${\\bf x} \\longrightarrow \\phi({\\bf x})$ in which the classes can be linearly separated. This results in a non-linear decision boundary in the original dimensions.\n\nAs the vectors ${\\bf x}_i$ appear only in inner products in both the decision function and the learning law, the mapping function $\\phi({\\bf x})$ does not need to be explicitly specified. Instead, we define a so-called kernel function:\n\n$ K({\\bf x}_1,{\\bf x}_2)=\\phi({\\bf x}_1)^T\\phi({\\bf x}_2)$.\n\nIn the gridsearch we consider the following two kernels:\n\nlinear kernel: $ K({\\bf x}_1,{\\bf x}_2) = {\\bf x}_1 \\cdot {\\bf x}_2$\nradial basis function: $ K({\\bf x}_1,{\\bf x}_2) = exp(-\\gamma({\\Vert {\\bf x}_1 - {\\bf x}_2 \\Vert}^2))$\nThe hyperparameters of a SVM include the following ones, which can be passed to the SVC of sklearn.svm:\n\nC: the inverse of the regularization strength (default=1.0)\nkernel: the kernel used (default='rbf')\ngamma: The higher the gamma value it tries to exactly fit the training data set (default='auto_deprecated'","91595eee":"\nIn the following we plot the Scree Plot to determine how many components we use.","47686790":"\nIn the following, the normalized confusion matrix is shown:","097285c3":"\n# Random Forest\nA random forest is an ensemble model that fits a number of decision tree classifiers on various sub-samples of the dataset which are created by the use of bootstrapping. In the inference stage it uses a majority vote over all trees to obtain the prediction. This improves the predictive accuracy and controls over-fitting.\n\nThe hyperparameters of a random forest include the following ones, which can be passed to the RandomForestClassifier of sklearn.ensemble:\n\nn_estimators: the number of trees\ncriterion: the criterion which decides the feature and the value at the split (default='gini')\nmax_depth: the maximum depth of each tree (default=None)\nmin_samples_split: the minimum number of samples in a node to be considered for further splitting (default=2)\nmax_features: the number of features which are considered for a split (default='sqrt')","5d7b2818":"**Using class weights in the loss function**\n\nThe sklearn library offers for all parametric classifiers a parameter class_weight, which can be set to \"balanced\". Then, mistakes are weighted inversely proportional to the class frequencies. This means that mistakes for the minority class are penalized more than mistakes made for the majority class.","45b05c00":"\n# **Logistic Regression**\n\nWe define $x_i$ as the $n$-dimensional feature vector of a given sample and $\\beta_{0},\\,\\boldsymbol{\\beta} = (\\beta_{1}, ..., \\beta_{n})^T$ as the model parameters. Then the logistic regression model is defined as:\n\n$P(Y=1 \\vert x_i)= \\frac{\\text{exp}(\\beta_{0} + x_i^T\\boldsymbol{\\beta} )}{1+\\text{exp}(\\beta_{0} + x_i^T\\boldsymbol{\\beta} )} = \\frac{1}{1+\\text{exp}(-(\\beta_{0} + x_i^T\\boldsymbol{\\beta} ))}$\n\nThe hyperparameters of a logistic regression include the following ones, which can be passed to the LogisticRegression of sklearn.linear_model:\n\npenalty: the norm used for penalization (default='l2')\nC: the inverse of the regularization strength (default=1.0)","ff6c425c":"# Decision Tree\nA decision tree for classification consists of several splits, which determine for a input sample, the predicted class, which is a leaf node in the tree. The construction of the decision trees is done with a greedy algorithm, because the theoretical minimum of function exists but it is NP-hard to determine it, because number of partitions has a factorial growth. Specifically, a greedy top-down approach is used which chooses a variable at each step that best splits the set of items. For measuring the \"best\" different metrics can be used, which generally measure the homogeneity of the target variable within the subsets. For this analysis we consider the following two metrics:\n\nGini impurity: Let $j$ be the number of classes and $p_i$ the fraction of items of class $i$ in a subset $p$, for $i \\in \\{1,2,..., j\\}$. Then the gini impurity is defined as follows: $\\;I_G(p) = 1- \\sum_{i=1}^j {p_i}^2$.\n\nInformation gain: It measures the reduction in entropy when applying the split. The entropy is defined as $H(t) = - \\sum_{i=1}^j p_i\\, \\text{log}_2\\,p_i$. Then we define the information gain to split $n$ samples in parent node $p$ into $k$ partitions, where $n_i$ is the number of samples in partition $i$ as $IG = H(p) - \\sum_{i = 1}^k \\frac{n_i}{n} H(i)$.\n\nThe hyperparameters of a Decision Tree include the following ones, which can be passed to the DecisionTreeClassifier of sklearn.tree:\n\ncriterion: the criterion which decides the feature and the value at the split (default='gini')\nmax_depth: the maximum depth of each tree (default=None)\nmin_samples_split: the minimum number of samples in a node to be considered for further splitting (default=2)","ccf1cb3d":" **Statistical overview of Data**"}}