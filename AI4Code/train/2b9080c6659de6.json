{"cell_type":{"81ff4a32":"code","ca6cf32a":"code","01977535":"code","a49eacd8":"code","4c174003":"code","d9136783":"code","d13172a8":"code","c207f932":"code","c97a6776":"code","93723cb4":"code","fece3c6c":"code","affb7396":"code","2cb42c67":"code","5abce339":"code","b35cad70":"code","a8a3a5c5":"code","03a7c91a":"code","f208a3e8":"code","9fa7eadf":"code","eb2463ec":"code","59dac1aa":"code","657c81ea":"code","0cfd73fd":"code","bca12422":"markdown","192202c5":"markdown","f3d3f219":"markdown","bca5b1b5":"markdown","773dcaec":"markdown","f8933938":"markdown","06e824f9":"markdown","b9ba5b23":"markdown","844ea377":"markdown","a9b8246d":"markdown","432d80b7":"markdown","f7af507d":"markdown","5709a716":"markdown","3e28ce12":"markdown","81e79375":"markdown","dc556761":"markdown","e13e48b7":"markdown"},"source":{"81ff4a32":"import pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk import word_tokenize, FreqDist","ca6cf32a":"df = pd.read_csv('..\/input\/Womens Clothing E-Commerce Reviews.csv')\ndf.head()","01977535":"df = df.replace(np.nan, \"\")\ndf.head()","a49eacd8":"df.shape","4c174003":"df.dtypes","d9136783":"df.describe()","d13172a8":"tokens = word_tokenize(df['Review Text'].to_string())","c207f932":"len(tokens)","c97a6776":"len(set(tokens))","93723cb4":"fd = nltk.FreqDist(tokens)\nfd.plot(30)","fece3c6c":"# WNlemma = nltk.WordNetLemmatizer()\n\n# def pre_process(text):\n#     text = text.lower()\n#     tokens = nltk.word_tokenize(text)\n#     tokens = [t for t in tokens if len(t) > 2]\n#     tokens = [WNlemma.lemmatize(t) for t in tokens]\n#     text_after_process = \" \".join(tokens)\n#     return text_after_process","affb7396":"WNlemma = nltk.WordNetLemmatizer()\nfrom nltk.corpus import wordnet\nfrom nltk import pos_tag\n\ndef get_wordnet_pos(treebank_tag):\n    if treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\ndef pre_process_with_pos_tag(text):\n    text = text.lower()\n    tokens = nltk.word_tokenize(text)\n    tokens = [t for t in tokens if len(t) > 2]\n    tokens = [WNlemma.lemmatize(t, get_wordnet_pos(pos_tag(word_tokenize(t))[0][1])) for t in tokens]\n    text_after_process = \" \".join(tokens)\n    return text_after_process","2cb42c67":"# review_text_processed = df['Review Text'].apply(pre_process)\nreview_text_processed = df['Review Text'].apply(pre_process_with_pos_tag)\nreview_text_processed","5abce339":"review_text_processed_df = pd.DataFrame(\n    {\n        'review_text_processed':review_text_processed,\n        'recommended':df['Recommended IND']\n    },\n    columns = ['review_text_processed','recommended']\n)\nreview_text_processed_df.head()","b35cad70":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(review_text_processed_df.review_text_processed,\n                                                    review_text_processed_df.recommended,\n                                                    test_size = 0.2,\n                                                    random_state = 5205\n                                                   )","a8a3a5c5":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\n\nX_train_counts = count_vect.fit_transform(X_train)\nX_train_counts.shape","03a7c91a":"dtm = pd.DataFrame(X_train_counts.toarray().transpose(), index=count_vect.get_feature_names())\ndtm = dtm.transpose()\ndtm.head()","f208a3e8":"from sklearn.feature_extraction.text import TfidfTransformer\ntf_transformer = TfidfTransformer().fit(X_train_counts)\nX_train_tf = tf_transformer.transform(X_train_counts)\nX_train_tf.shape","9fa7eadf":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\n\nnb_clf = Pipeline([('vect', CountVectorizer()),\n                   ('tfidf', TfidfTransformer()),\n                   ('clf', MultinomialNB())\n                  ])\nnb_clf.fit(X_train, y_train)\nnb_predicted = nb_clf.predict(X_test)\n\nprint(metrics.confusion_matrix(y_test, nb_predicted))\nprint(np.mean(nb_predicted==y_test))\nprint(metrics.classification_report(y_test, nb_predicted))","eb2463ec":"from sklearn import tree\ndt_clf = Pipeline([('vect', CountVectorizer()),\n                   ('tfidf', TfidfTransformer()),\n                   ('clf', tree.DecisionTreeClassifier())\n                  ])\ndt_clf.fit(X_train, y_train)\ndt_predicted = dt_clf.predict(X_test)\n\nprint(metrics.confusion_matrix(y_test, dt_predicted))\nprint(np.mean(dt_predicted==y_test))\nprint(metrics.classification_report(y_test, dt_predicted))","59dac1aa":"from sklearn import svm\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\n\nsvm_clf = Pipeline([('vect', CountVectorizer()),\n                    ('tfidf', TfidfTransformer()),\n                    ('clf', svm.LinearSVC(C=1.0))\n                   ])\nsvm_clf.fit(X_train, y_train)\nsvm_predicted = svm_clf.predict(X_test)\n\nprint(metrics.confusion_matrix(y_test, svm_predicted))\nprint(np.mean(svm_predicted==y_test))\nprint(metrics.classification_report(y_test, svm_predicted))","657c81ea":"from sklearn.linear_model import LogisticRegression\nlr_clf = Pipeline([('vect', CountVectorizer()),\n                   ('tfidf', TfidfTransformer()),\n                   ('clf', LogisticRegression())\n                  ])\nlr_clf.fit(X_train, y_train)\nlr_predicted = lr_clf.predict(X_test)\n\nprint(metrics.confusion_matrix(y_test, lr_predicted))\nprint(np.mean(lr_predicted==y_test))\nprint(metrics.classification_report(y_test, lr_predicted))","0cfd73fd":"lr_clf = LogisticRegression()\n# lr_clf.fit(dtm, y_train)\nlr_clf.fit(X_train_tf, y_train)\n\nlr_clf_coef = (\n    pd.DataFrame(lr_clf.coef_[0], index=dtm.columns)\n    .rename(columns={0:'Coefficient'})\n    .sort_values(by='Coefficient', ascending=False)\n)\n\nlr_clf_coef.head()","bca12422":"There were total of 281,146 tokens, of which 31,291 were unique tokens.","192202c5":"## 1) Data Understanding & Data Preparation","f3d3f219":"## 5b) Decision tree model","bca5b1b5":"## 3) Splitting to train & test sets","773dcaec":"## 5d) Logistic regression model","f8933938":"## 2b) Pre-process 2 function","06e824f9":"## 2a) Pre-process 1 function","b9ba5b23":"Experimented with 4 classification algorithms - (i) naive bayes, (ii) decision tree, (iii) support vector machine, (iv) logistic regression. Metrics used to evaluate the models are - (i) classification accuracy, (ii) F1 score.","844ea377":"Comparing the 4 classification models, SVM and logistic regression models had better F1 scores as compared with naive bayes and decision tree models.","a9b8246d":"## 4b) Term frequency-inverse document frequency document-term matrix","432d80b7":"## 5c) Support vector machine model","f7af507d":"To predict recommended or not based on review text, we first need to pre-process the text before inputting it into the various classification algorithms. There were 2 pre-processing steps attempted.\n\nPre-process 1\n1. Lowercase the text\n2. Tokenize the text\n3. Remove the tokens if it is less than 3 characters\n4. Lemmatize the tokens (default option used - only tokens with part-of-speech = 'noun' were lemmatized)\n\nPre-process 2\n1. Lowercase the text\n2. Tokenize the text\n3. Remove the tokens if it is less than 3 characters\n4. Lemmatize the tokens (tokens with part-of-speech = 'noun', 'verb', 'adjective', 'adverb' were lemmatized)","5709a716":"After the pre-processing steps, split the dataset into 80% train and 20% test.","3e28ce12":"## 4a) Term Frequency document-term matrix","81e79375":"There were 2 ways we can construct the document-term matrix - (i) term frequency, (ii) term frequency-inverse document frequency (tf-idf). Term frequency is just the raw count of a term in a document. Tf-idf increases proportionally to the number of times a word appears in the document and is offset by number of documents in the corpus that contain the word.","dc556761":"Objective of this analysis was to find out if lemmatizing 'verb', 'adjective', and 'adverb' to their root words will improve the classification accuracy of predicting recommended or not based on review text.","e13e48b7":"## 5a) Naive bayes model"}}