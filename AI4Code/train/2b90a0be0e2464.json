{"cell_type":{"e3f37144":"code","1e4b0e1a":"code","fafe8c27":"code","3d3f0113":"code","9f721f15":"code","0d064167":"code","9b30c2d0":"code","a0623026":"code","38b819a1":"code","18b9802e":"code","b5636915":"code","9c768160":"code","86fcb573":"code","48a033ce":"code","93698f09":"code","2373eef3":"code","75032d28":"code","83454dad":"code","fd427552":"code","0fcfb7a9":"code","5f41d6ad":"code","643bd63e":"code","5e810eea":"code","f31d41c4":"code","235feb65":"code","c83e6240":"code","0b7656bf":"code","6b1fb9c3":"code","7ff79fff":"code","0c621cc4":"code","d2f935f3":"code","a7582136":"code","f60da92c":"code","aa4c06b6":"code","b352b68b":"code","12c2e8d0":"code","10c67ce4":"code","754253da":"code","3a837f47":"code","3ab0212c":"code","e6752a10":"code","c5393312":"code","aaa12998":"code","58d077d8":"code","fb7ab679":"code","6751e07d":"code","5e4b8666":"code","e88b7bbe":"code","11745320":"code","1c8d6aec":"code","d12b334c":"code","25baad9b":"code","0c2fc6fa":"code","d51aaf31":"code","c95ee75a":"code","c807346c":"code","73ff335d":"code","d375b85d":"markdown","28592c1a":"markdown","7f3219f0":"markdown","436c0dc3":"markdown","ce68fb23":"markdown","b9cc4495":"markdown","89c15eb7":"markdown","26c3c584":"markdown","f1729c2a":"markdown","42954df5":"markdown","6d5d53e8":"markdown","83e3f646":"markdown","e1eca30c":"markdown","0dca89ca":"markdown","f7482b4f":"markdown","66dbb736":"markdown","a7ed0b0b":"markdown","5880c7c8":"markdown","24932c16":"markdown","ada556ec":"markdown","0f060869":"markdown","8d854aa3":"markdown","2bcd3da8":"markdown","14377761":"markdown","d01f23b6":"markdown","4eb27292":"markdown","bac8c173":"markdown","e9f869ee":"markdown","6ec04b40":"markdown","15cb651f":"markdown","72e11ee6":"markdown","cfa02ff3":"markdown","8ac87301":"markdown","42adbee4":"markdown","4d878f8a":"markdown","befc8bc2":"markdown","ac169b5f":"markdown","243497fd":"markdown","8d708c21":"markdown","c74db16b":"markdown","59f5a12c":"markdown","c4b85608":"markdown","abb30856":"markdown","9c23baaa":"markdown","f68195c4":"markdown","1e291f77":"markdown","80fef9c6":"markdown","4e8f9f3f":"markdown","644d5a3a":"markdown","72c763f2":"markdown","b1b5ef10":"markdown","9e30b31c":"markdown","defa3762":"markdown","22b44d0e":"markdown","dd61c875":"markdown","6d021372":"markdown","680556c3":"markdown","645e0e94":"markdown","4d7bfd1a":"markdown","e776867a":"markdown","45698ddf":"markdown","f155b881":"markdown","6d92df57":"markdown","ac9dfc7e":"markdown","1b246417":"markdown","26272a91":"markdown","79a47e1a":"markdown","40fa3471":"markdown","80db6aa9":"markdown","32ee2e19":"markdown","7fc78f5f":"markdown","1555f40e":"markdown","91b4b4a7":"markdown","f2f8171b":"markdown"},"source":{"e3f37144":"import numpy as np\nimport pandas as pd\n#Visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nstyle.use('fivethirtyeight') # use style fivethirtyeight\nimport seaborn as sns\nfrom matplotlib import rcParams\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n# Scaling\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n# Selection\nfrom scipy.stats import chi2_contingency\n# Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split\n# Algorithm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n# Evaluation metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\nfrom sklearn.metrics import plot_confusion_matrix\n# Hyperparameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","1e4b0e1a":"df = pd.read_csv('\/kaggle\/input\/customer-analytics\/Train.csv')","fafe8c27":"df.rename(columns={'Reached.on.Time_Y.N':'is_late'}, inplace=True)\ndf.head()","3d3f0113":"df.shape","9f721f15":"df.columns","0d064167":"df.columns = df.columns.str.lower()","9b30c2d0":"df.info()","a0623026":"# Categorical data\ncategorical = ['warehouse_block','mode_of_shipment','product_importance', 'gender', 'is_late', 'customer_rating']\n# Numerical data\nnumeric = ['customer_care_calls', 'cost_of_the_product', 'prior_purchases', 'discount_offered', 'weight_in_gms']","38b819a1":"df_dt = df.copy()","18b9802e":"df_dt.isna().values.any() # Missing value detection","b5636915":"df_dt.isna().sum()  # Calculate missing values","9c768160":"# Select all duplicate rows based on all columns\ndf_dt[df_dt.duplicated(keep=False)] ","86fcb573":"# Select all duplicate rows based on selected column\ndf_dt[df_dt.duplicated(subset=['id'],keep=False)] # Display all duplicated rows based on column 'id'","48a033ce":"# Identify using boxplot\nplt.figure(figsize=(20,8))\nfor i in range(0,len(numeric)):\n    plt.subplot(1, len(numeric), i+1)\n    sns.boxplot(y=df_dt[numeric[i]], color='orange')\n    plt.tight_layout()","93698f09":"# Identify outlier using IQR\nfor col in numeric:\n    \n    # Menghitung nilai IQR\n    Q1 = df_dt[col].quantile(0.25)\n    Q3 = df_dt[col].quantile(0.75)\n    IQR = Q3 - Q1\n    \n    # Define value \n    nilai_min = df_dt[col].min()\n    nilai_max = df_dt[col].max()\n    lower_lim = Q1 - (1.5*IQR)\n    upper_lim = Q3 + (1.5*IQR)\n    \n    # Identify low outlier\n\n    if (nilai_min < lower_lim):\n        print('Low outlier is found in column',col,'<', lower_lim,'\\n')\n        #display total low outlier \n        print('Total of Low Outlier in column',col, ':', len(list(df_dt[df_dt[col] < lower_lim].index)),'\\n')\n    elif (nilai_max > upper_lim):\n        print('High outlier is found in column',col,'>', upper_lim,'\\n')\n        #display total high outlier \n        print('Total of High Outlier in column',col, ':', len(list(df_dt[df_dt[col] > upper_lim].index)),'\\n')\n        \n    else:\n        print('Outlier is not found in column',col,'\\n')\n        ","2373eef3":"# We handle outlier with replace the value with upper_bound or lower_bound \nfor col in ['prior_purchases', 'discount_offered']:\n    # Initiate Q1\n    Q1 = df_dt[col].quantile(0.25)\n    # Initiate Q3\n    Q3 = df_dt[col].quantile(0.75)\n    # Initiate IQR\n    IQR = Q3 - Q1\n    # Initiate lower_bound & upper_bound \n    lower_bound = Q1 - (IQR * 1.5)\n    upper_bound = Q3 + (IQR * 1.5)\n\n    # Filtering outlier & replace with upper_bound or lower_bound \n    df_dt[col] = np.where(df_dt[col] >= upper_bound,\n                         upper_bound, df_dt[col])\n    df_dt[col] = np.where(df_dt[col] <= lower_bound,\n                         lower_bound, df_dt[col])","75032d28":"# Visualize after handle outlier\nfor i in range(0, len(['prior_purchases', 'discount_offered'])):\n               plt.subplot(1,2, i+1)\n               sns.boxplot(y= df_dt[col], color = 'orange', orient = 'v');\n               plt.tight_layout();","83454dad":"# Check data distribution\nplt.figure(figsize=(20,5))\nfor i in range(0,len(numeric)):\n    plt.subplot(1, len(numeric), i+1)\n    sns.distplot(df_dt[numeric[i]], color='orange')\n    plt.tight_layout()","fd427552":"# Apply log transformation\nfor col in numeric:\n    df_dt[col] = (df_dt[col]+1).apply(np.log)","0fcfb7a9":"# Visualize after log transformation\nplt.figure(figsize=(20,5))\nfor i in range(0,len(numeric)):\n    plt.subplot(1, len(numeric), i+1)\n    sns.distplot(df_dt[numeric[i]], color='orange')\n    plt.tight_layout()","5f41d6ad":"# Apply standardization\nfor col in numeric:\n    df_dt[col]= StandardScaler().fit_transform(df_dt[col].values.reshape(len(df_dt), 1))","643bd63e":"df_dt.describe()","5e810eea":"# Selection for categorial feature\n# Import module\nfrom scipy.stats import chi2_contingency\n\ncategory = ['warehouse_block','mode_of_shipment','product_importance', \n            'gender','customer_rating']\nchi2_check = []\n# Iteration\nfor col in category:\n    # If pvalue < 0.05 \n    if chi2_contingency(pd.crosstab(df_dt['is_late'], df_dt[col]))[1] < 0.05 :\n        chi2_check.append('Reject Null Hypothesis')\n    # If pvalue > 0.05\n    else :\n        chi2_check.append('Fail to Reject Null Hypothesis')\n        \n# Make the result into dataframe\nres = pd.DataFrame(data = [category, chi2_check]).T\n# Rename columns\nres.columns = ['Column', 'Hypothesis']\nres","f31d41c4":"# Adjusted P-Value use the Bonferroni-adjusted method\n\n# Initiate empty dictionary\ncheck = {}\n# Iteration for product_importance column\nfor i in res[res['Hypothesis'] == 'Reject Null Hypothesis']['Column']:\n    # One hot encoding product_importance column\n    dummies = pd.get_dummies(df_dt[i])\n    # Initiate Bonferroni-adjusted formula\n    bon_p_value = 0.05\/df_dt[i].nunique()\n    for series in dummies:\n        if chi2_contingency(pd.crosstab(df_dt['is_late'], dummies[series]))[1] < bon_p_value:\n            check['{}-{}'.format(i, series)] = 'Reject Null Hypothesis'\n        else :\n            check['{}-{}'.format(i, series)] = 'Fail to Reject Null Hypothesis'\n# Make the result into dataframe\nres_chi_ph = pd.DataFrame(data=[check.keys(), check.values()]).T\n# Rename the columns\nres_chi_ph.columns = ['Pair', 'Hypothesis']\nres_chi_ph","235feb65":"# one hot encoding feature product_importance and keep high category\nonehots = pd.get_dummies(df_dt['product_importance'], prefix = 'product_importance')\ndf_dt = df_dt.join(onehots)\n# drop all categorical columns & 'id, except product_importance_high\ndf_dt.drop(columns=['warehouse_block','gender','mode_of_shipment',\n                   'product_importance', 'product_importance_low',\n                   'product_importance_medium','id'], inplace = True)\n# check dataframe after encoding\ndf_dt.info()","c83e6240":"# Copy dataset\ndf_eda = df.copy()","0b7656bf":"delay = pd.DataFrame(df_eda.groupby(['is_late'])['id'].count()\/len(df_eda)).reset_index()\nplt.pie(delay['id'],labels=delay['is_late'],autopct='%.2f%%');","6b1fb9c3":"# for categorical column\nfor col in categorical:\n    print('Value count kolom', col, ':')\n    print(df_eda[col].value_counts())\n    print()","7ff79fff":"# Plot categorical columns\nfor col in categorical:\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(141);\n    sns.countplot(df_eda[col], palette = 'colorblind', orient='v');\n    plt.title('Countplot')\n    plt.tight_layout();\n    \n    \n    plt.subplot(143);\n    df_eda[col].value_counts().plot.pie(autopct='%1.2f%%');\n    plt.title('Pie chart')  ","0c621cc4":"df_eda[numeric].describe()","d2f935f3":"plt.figure(figsize=(7,6));\nsns.heatmap(df_eda.corr(), annot = True, fmt = '.2f', cmap = 'Reds');","a7582136":"plt.figure(figsize=(5, 5));\np = sns.pairplot(df_eda, markers = '+', diag_kind = 'kde', hue = 'is_late');\n# title\nnew_title = 'Shipment ?'\np._legend.set_title(new_title);\n\n# replace labels\nnew_labels = ['on_time', 'late']\nfor t, l in zip(p._legend.texts, new_labels): t.set_text(l);","f60da92c":"fig = plt.figure(figsize=(15, 15))\nfor i in range(0, len(numeric)): # untuk setiap kolom numerik\n    ax = fig.add_subplot(2, 3, i+1) # kita set posisi catplot\/stripplotnya di layout\n    sns.stripplot(ax=ax, data=df_eda, x='warehouse_block', y=numeric[i]) # gambar catplot\/stripplotnya\n    plt.tight_layout() # layout\n\n    if i != 0: plt.close(i+1) # close figure \n\n    plt.close(len(numeric) + 1) # close figure ","aa4c06b6":"fig = plt.figure(figsize=(15, 15))\nfor i in range(0, len(numeric)): # untuk setiap kolom numerik\n    ax = fig.add_subplot(3, 3, i+1) # kita set posisi catplot\/stripplotnya di layout\n    sns.stripplot(ax=ax, data=df_eda, x='mode_of_shipment', y=numeric[i]) # gambar catplot\/stripplotnya\n    plt.tight_layout() # layout\n\n    if i != 0: plt.close(i+1) # close figure \n\n    plt.close(len(numeric) + 1) # close figure ","b352b68b":"fig = plt.figure(figsize=(15, 15))\nfor i in range(0, len(numeric)): # untuk setiap kolom numerik\n    ax = fig.add_subplot(2, 3, i+1) # kita set posisi catplot\/stripplotnya di layout\n    sns.stripplot(ax=ax, data=df_eda, x='product_importance', y=numeric[i]) # gambar catplot\/stripplotnya\n    plt.tight_layout() # layout\n\n    if i != 0: plt.close(i+1) # close figure \n\n    plt.close(len(numeric) + 1) # close figure ","12c2e8d0":"fig = plt.figure(figsize=(15, 15))\nfor i in range(0, len(numeric)): # untuk setiap kolom numerik\n    ax = fig.add_subplot(3, 3, i+1) # kita set posisi catplot\/stripplotnya di layout\n    sns.stripplot(ax=ax, data=df_eda, x='gender', y=numeric[i]) # gambar catplot\/stripplotnya\n    plt.tight_layout() # layout\n\n    if i != 0: plt.close(i+1) # close figure \n\n    plt.close(len(numeric) + 1) # close figure ","10c67ce4":"i=1\nplt.figure(figsize=(15,10))\nfor col in ['mode_of_shipment', 'warehouse_block', 'product_importance']:\n    plt.subplot(2,2,i)\n    sns.countplot(df_eda[col], hue=df_eda['gender'], palette=\"ch:.25\")\n    i+=1","754253da":"i=1\nplt.figure(figsize=(15,10))\nfor col in ['mode_of_shipment', 'warehouse_block']:\n    plt.subplot(2,2,i)\n    sns.countplot(df_eda[col], hue=df_eda['product_importance'], palette=\"ch:.25\")\n    i+=1","3a837f47":"sns.catplot(x=\"warehouse_block\", kind=\"count\", hue='mode_of_shipment',\n            palette=\"ch:.25\", data=df_eda);","3ab0212c":"i=1\nplt.figure(figsize=(15,10))\nfor col in ['mode_of_shipment', 'warehouse_block', 'product_importance',\n            'gender','customer_rating']:\n    plt.subplot(2,3,i)\n    sns.countplot(df_eda[col], hue=df_eda['is_late'], palette=\"ch:.25\")\n    i+=1\n    plt.legend(['on_time','late']);","e6752a10":"# Inititate feature & target\nX = df_dt.drop(columns = 'is_late')\ny = df_dt['is_late']","c5393312":"# Split Train & Test Data\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30, random_state=42)","aaa12998":"# Create function to fit model & model evaluation\ndef fit_evaluation(Model, Xtrain, ytrain, Xtest, ytest):\n    model = Model # initiate model\n    model.fit(Xtrain, ytrain) # fit the model\n    y_pred = model.predict(Xtest)\n    y_pred_train = model.predict(Xtrain)\n    train_score = model.score(Xtrain, ytrain) # Train Accuracy\n    test_score = model.score(Xtest, ytest)    # Test Accuracy\n    fpr, tpr, thresholds = roc_curve(ytest, y_pred, pos_label=1)\n    AUC = auc(fpr, tpr) # AUC\n    return round(train_score,2), round(test_score,2), round(precision_score(ytest, y_pred),2), \\\n           round(recall_score(ytrain, y_pred_train),2),round(recall_score(ytest, y_pred),2), \\\n           round(f1_score(ytest, y_pred),2), round(AUC,2)","58d077d8":"# Inititate algorithm\nlr = LogisticRegression(random_state=42)\ndt = DecisionTreeClassifier(random_state=42)\nrf = RandomForestClassifier(random_state=42)\nknn = KNeighborsClassifier(n_neighbors=5)\nsvc = SVC(random_state=42)\nxgb = XGBClassifier(random_state=42, eval_metric = 'logloss')\n\n# Create function to make the result as dataframe \ndef model_comparison_default(X,y):  \n    \n    # Logistic Regression\n    lr_train_score, lr_test_score, lr_pr, lrtr_re, lrte_re, lr_f1, lr_auc = fit_evaluation(\n        lr, Xtrain, ytrain, Xtest, ytest)\n    # Decision Tree\n    dt_train_score, dt_test_score, dt_pr, dttr_re, dtte_re, dt_f1, dt_auc = fit_evaluation(\n        dt, Xtrain, ytrain, Xtest, ytest)\n    # Random Forest\n    rf_train_score, rf_test_score, rf_pr, rftr_re, rfte_re, rf_f1, rf_auc = fit_evaluation(\n        rf, Xtrain, ytrain, Xtest, ytest)\n    # KNN\n    knn_train_score, knn_test_score, knn_pr, knntr_re, knnte_re, knn_f1, knn_auc = fit_evaluation(\n        knn, Xtrain, ytrain, Xtest, ytest)\n    # SVC\n    svc_train_score, svc_test_score, svc_pr, svctr_re, svcte_re, svc_f1, svc_auc = fit_evaluation(\n        svc, Xtrain, ytrain, Xtest, ytest)\n    # XGBoost\n    xgb_train_score, xgb_test_score, xgb_pr, xgbtr_re, xgbte_re, xgb_f1, xgb_auc = fit_evaluation(\n        xgb, Xtrain, ytrain, Xtest, ytest)\n    \n    \n    models = ['Logistic Regression','Decision Tree','Random Forest',\n             'KNN','SVC','XGBoost']\n    train_score = [lr_train_score, dt_train_score, rf_train_score, \n                   knn_train_score, svc_train_score, xgb_train_score]\n    test_score = [lr_test_score, dt_test_score, rf_test_score,\n                  knn_test_score, svc_test_score, xgb_test_score]\n    precision = [lr_pr, dt_pr, rf_pr, knn_pr, svc_pr, xgb_pr]\n    recall_train = [lrtr_re, dttr_re, rftr_re, knntr_re, svctr_re, xgbtr_re]\n    recall_test = [lrte_re, dtte_re, rfte_re, knnte_re, svcte_re, xgbte_re]\n    f1 = [lr_f1, dt_f1, rf_f1, knn_f1, svc_f1, xgb_f1]\n    auc = [lr_auc, dt_auc, rf_auc, knn_auc, svc_auc, xgb_auc]\n    \n    model_comparison = pd.DataFrame(data=[models, train_score, test_score, \n                                          precision, recall_train, recall_test,\n                                          f1,auc]).T.rename({0: 'Model',\n                                                             1: 'Accuracy_Train',\n                                                             2: 'Accuracy_Test',\n                                                             3: 'Precision',\n                                                             4: 'Recall_Train',\n                                                             5: 'Recall_Test',\n                                                             6: 'F1 Score',\n                                                             7: 'AUC'\n                                                                                  }, axis=1)\n    \n    return model_comparison","fb7ab679":"model_comparison_default(X,y)","6751e07d":"# List Hyperparameters yang akan diuji\npenalty = ['l2','l1','elasticnet']\nC = [0.0001, 0.001, 0.002] # Inverse of regularization strength; smaller values specify stronger regularization.\nhyperparameters = dict(penalty=penalty, C=C)\n\n# Inisiasi model\nlogres = LogisticRegression(random_state=42) # Init Logres dengan Gridsearch, cross validation = 5\nmodel = RandomizedSearchCV(logres, hyperparameters, cv=5, random_state=42,  scoring='recall')\n\n# Fitting Model & Evaluation\nmodel.fit(Xtrain, ytrain)\ny_pred = model.predict(Xtest)\nmodel.best_estimator_","5e4b8666":"# Let's do hyperparameter tuning using RandomizesearchCV\n\n# Hyperparameter lists to be tested\nmax_depth = list(range(1,10)) \nmin_samples_split = list(range(5,10)) \nmin_samples_leaf = list(range(5,15)) \nmax_features = ['auto', 'sqrt', 'log2'] \ncriterion = ['gini','entropy']\nsplitter = ['best','random']\n\n# Initiate hyperparameters\nhyperparameters = dict(max_depth=max_depth, \n                       min_samples_split=min_samples_split, \n                       min_samples_leaf=min_samples_leaf,\n                       max_features=max_features,\n                       criterion = criterion,\n                       splitter = splitter)\n\n# Initiate model\ndt_tun = DecisionTreeClassifier(random_state=42)\nmodel = RandomizedSearchCV(dt_tun, hyperparameters, cv=10, scoring='recall',random_state=42) \nmodel.fit(Xtrain, ytrain)\ny_pred_tun = model.predict(Xtest)\nmodel.best_estimator_","e88b7bbe":"# Initiate hyperparameters\nparams = {'max_depth':[50],'n_estimators':[100,150], \n          'criterion':['gini', 'entropy']}\n# Initiate model\nmodel = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n                     param_grid=params,scoring='recall', cv=5)\n# Fit model\nmodel.fit(Xtrain,ytrain)\ny_pred = model.predict(Xtest)\n# Get best estimator\nmodel.best_estimator_","11745320":"#List Hyperparameters that we want to tune.\nleaf_size = list(range(1,50))\nn_neighbors = list(range(1,30))\np=[1,2]\n\n#Convert to dictionary\nhyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n\n#Create new KNN object\nKNN_2 = KNeighborsClassifier()\n\n#Use RandomizedSearchCV\nclf = RandomizedSearchCV(KNN_2, hyperparameters, cv=10, scoring = 'recall')\n\n#Fit the model\nbest_model = clf.fit(X,y)\n# Get best estimator\nclf.best_estimator_","1c8d6aec":"# Hyperparameter lists to be tested\nkernel = ['linear', 'poly', 'rbf', 'sigmoid']\nC = [0.0001, 0.001, 0.002] \ngamma = ['scale', 'auto']\n\n#Convert to dictionary\nhyperparameters = dict(kernel=kernel, C=C, gamma=gamma)\n\n# Initiate model\nsvc = SVC(random_state=42) \nmodel = RandomizedSearchCV(svc, hyperparameters, cv=5, random_state=42, \n                           scoring='recall')\n\n# Fitting Model & Evaluation\nmodel.fit(Xtrain, ytrain)\ny_pred = model.predict(Xtest)\nmodel.best_estimator_","d12b334c":"# Hyper Parameter Optimization\n\nhyperparameters={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]    \n}\n# Initiate model\nxg = XGBClassifier(random_state=42,eval_metric = 'logloss')\nxg_tuned = RandomizedSearchCV(xg, hyperparameters, cv=5, random_state=42, scoring='recall')\nxg_tuned.fit(Xtrain,ytrain)\n\n# Predict & Evaluation\ny_pred = xg_tuned.predict(Xtest)#Check performa dari model\nxg_tuned.best_estimator_","25baad9b":"# Inititate best estimator\nlr_tune = LogisticRegression(C=0.0001, random_state=42)\ndt_tune = DecisionTreeClassifier(criterion='entropy', max_depth=4, max_features='sqrt',\n                       min_samples_leaf=12, min_samples_split=6,\n                       random_state=42)\nrf_tune = RandomForestClassifier(max_depth=50, random_state=42)\nknn_tune = KNeighborsClassifier(algorithm='auto', leaf_size=1, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n                     weights='uniform')\nsvc_tune = SVC(C=0.0001, kernel='linear', random_state=42)\nxgb_tune = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.3, eval_metric='logloss',\n              gamma=0.4, gpu_id=-1, importance_type='gain',\n              interaction_constraints='', learning_rate=0.15, max_delta_step=0,\n              max_depth=15, min_child_weight=1,\n              monotone_constraints='()', n_estimators=100, n_jobs=4,\n              num_parallel_tree=1, random_state=42, reg_alpha=0, reg_lambda=1,\n              scale_pos_weight=1, subsample=1, tree_method='exact',\n              validate_parameters=1, verbosity=None)\n\n# Create function to make the result as dataframe \ndef model_comparison_tuned(X,y):  \n    \n    # Logistic Regression\n    lr_train_score, lr_test_score, lr_pr, lrtr_re, lrte_re, lr_f1, lr_auc = fit_evaluation(\n        lr_tune, Xtrain, ytrain, Xtest, ytest)\n    # Decision Tree\n    dt_train_score, dt_test_score, dt_pr, dttr_re, dtte_re, dt_f1, dt_auc = fit_evaluation(\n        dt_tune, Xtrain, ytrain, Xtest, ytest)\n    # Random Forest\n    rf_train_score, rf_test_score, rf_pr, rftr_re, rfte_re, rf_f1, rf_auc = fit_evaluation(\n        rf_tune, Xtrain, ytrain, Xtest, ytest)\n    # KNN\n    knn_train_score, knn_test_score, knn_pr, knntr_re, knnte_re, knn_f1, knn_auc = fit_evaluation(\n        knn_tune, Xtrain, ytrain, Xtest, ytest)\n    # SVC\n    svc_train_score, svc_test_score, svc_pr, svctr_re, svcte_re, svc_f1, svc_auc = fit_evaluation(\n        svc_tune, Xtrain, ytrain, Xtest, ytest)\n    # XGBoost\n    xgb_train_score, xgb_test_score, xgb_pr, xgbtr_re, xgbte_re, xgb_f1, xgb_auc = fit_evaluation(\n        xgb_tune, Xtrain, ytrain, Xtest, ytest)\n    \n    \n    models = ['Logistic Regression','Decision Tree','Random Forest',\n             'KNN','SVC','XGBoost']\n    train_score = [lr_train_score, dt_train_score, rf_train_score, \n                   knn_train_score, svc_train_score, xgb_train_score]\n    test_score = [lr_test_score, dt_test_score, rf_test_score,\n                  knn_test_score, svc_test_score, xgb_test_score]\n    precision = [lr_pr, dt_pr, rf_pr, knn_pr, svc_pr, xgb_pr]\n    recall_train = [lrtr_re, dttr_re, rftr_re, knntr_re, svctr_re, xgbtr_re]\n    recall_test = [lrte_re, dtte_re, rfte_re, knnte_re, svcte_re, xgbte_re]\n    f1 = [lr_f1, dt_f1, rf_f1, knn_f1, svc_f1, xgb_f1]\n    auc = [lr_auc, dt_auc, rf_auc, knn_auc, svc_auc, xgb_auc]\n    \n    model_comparison = pd.DataFrame(data=[models, train_score, test_score, \n                                          precision, recall_train, recall_test,\n                                          f1,auc]).T.rename({0: 'Model',\n                                                             1: 'Accuracy_Train',\n                                                             2: 'Accuracy_Test',\n                                                             3: 'Precision',\n                                                             4: 'Recall_Train',\n                                                             5: 'Recall_Test',\n                                                             6: 'F1 Score',\n                                                             7: 'AUC'\n                                                                                  }, axis=1)\n    \n    return model_comparison","0c2fc6fa":"model_comparison_tuned(X,y)","d51aaf31":"from sklearn.metrics import plot_confusion_matrix\n\ndef confusion_matrix(Model, Xtrain, ytrain, Xtest, ytest):\n    model = Model\n    plot_confusion_matrix(model.fit(Xtrain, ytrain), Xtest, ytest,\n                     display_labels=['on_time','late'], cmap = 'Oranges', \n                     values_format='.0f')  \n    plt.grid(False)\n    plt.show()  ","c95ee75a":"# After hyperparameter tuning\nconfusion_matrix(dt_tune, Xtrain, ytrain, Xtest, ytest)","c807346c":"# Before hyperparameter tuning\nconfusion_matrix(dt, Xtrain, ytrain, Xtest, ytest)","73ff335d":"feat_importances = pd.Series(dt_tune.feature_importances_, index=X.columns)\nax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))\nax.invert_yaxis()\n\nplt.xlabel('Score');\nplt.ylabel('Feature');\nplt.title('Feature Importance Score');","d375b85d":"Based on the *Correlation heatmap* above :  \n1. Target *is_late* has a moderate positive correlation with *discount_offered* & weak negative correlation with *weight_in_gms*.\n2. Feature *customer_care_calss* has a weak positive correlation with *cost_of_the_product* and negative correlation with *weight_in_gms*.\n3. Feature *discount_offered* has a moderate negative correlation with *weight_in_gms*.\n","28592c1a":"**Summary :**  \n- Mostly high & low priority parcels used ship.\n\n","7f3219f0":"#### Tuned Parameter","436c0dc3":"**Numeric - Mode of Shipment**","ce68fb23":"### Get dataset information","b9cc4495":"We found outliers in `discount_offered` & `prior_purchases` with almost 30% of data.","89c15eb7":"#### Feature Transformation : Log transform","26c3c584":"**Summary :**  \nSome parcels with the weight more than 6 Kg are stored in warehouse_block B, D, & F.\n\n\n\n","f1729c2a":"## Data Cleansing & Feature Engineering","42954df5":"From the result above, `product_importance` with **high** category has a correlation with our target.","6d5d53e8":"# Data Pre-processing","83e3f646":"Because of the target's name is too long, so we simplify the name to ease the next step.","e1eca30c":"### Target Visualization","0dca89ca":"#### Random Forest","f7482b4f":"### Descriptive Statistic","66dbb736":"##### Based on target 'is_late'","a7ed0b0b":"The class of target looks balance.","5880c7c8":"#### Categorical - Categorical","24932c16":"#### Identify missing values","ada556ec":"**Recommendation for E-Commerce :**\n- The operation team should add more manpower when there is a sale program, especially for the discount more than 10% and the parcel weight is 1 - 4 Kg.\n- The parcel should not be centralized in the warehouse block F, so that the handling is not too crowded which can cause the late shipment.\n- Adding more features can imporve model performance, such as delivery time estimation, delivery date, customer address, and courier.\n","0f060869":"**Summary :**\n- Distribution of **customer_care_calls**, **Customer_rating**, **Cost_of_the_Product**, **Prior_purchases** look normal, beacuse the mean and the median are close, while  **discount_offered** and **weight_in_grams** are indicated skewed.\n\n\n","8d854aa3":"### Feature Importance","2bcd3da8":"**Numeric - Gender**","14377761":"### Import file","d01f23b6":"##### Based on Product Importance","4eb27292":"#### Default Parameter","bac8c173":"##### Based on Gender","e9f869ee":"#### Stripplot","6ec04b40":"#### XG Boost","15cb651f":"**Summary :**  \n- `gender` doesn't seem to have a correlation with the target.\n","72e11ee6":"### Rename column target","cfa02ff3":"**Based on the information above :**\n1. Dataframe has 10999 rows and 12 columns.\n2. No missing values are found.\n3. There are only 2 data types, integer and object.\n4. Classification target `is_late` and others we call features.","8ac87301":"**Numeric - Warehouse block**","42adbee4":"#### Feature Encoding : One hot encoding","4d878f8a":"We didn't remove the outliers, but replacing with upper bound and lower bound. And we can see in the visualization above, there is no outliers detected.","befc8bc2":"### Separate feature & target column","ac169b5f":"**Summary :**\n- The shipment tends to deliver on time when the discount under 10% and weight is 1 - 2 Kg & 4 - 6 Kg.\n- Most of products that cost around $300 delivered on time, others late.\n- Majority customers who get the late parcels are more likely to call customer care, no matter how often they called.","243497fd":"Decision Tree algorithm with hyper-parameter tuning has a good balance between its score, also neither underfitting nor overfitting.","8d708c21":"##### Warehouse block - Mode of Shipment","c74db16b":"### Get the shape of dataset","59f5a12c":"### Get list of columns","c4b85608":"**Summary :**  \n- Most of parcels are stored in warehouse_block F.\n- The ship contributes the most late delivery.\n- Most of parcels in all shipment priority are delivered late.","abb30856":"### Hyperparameter","9c23baaa":"#### Feature Selection : Chi squared method","f68195c4":"#### Numeric values","1e291f77":"# E-Commerce Shipping Dataset : Data Pre-Processing\n\n### Product Shipment Delivered on time or not ? To Meet E-Commerce Customer Demand\nSource of dataset : https:\/\/www.kaggle.com\/prachi13\/customer-analytics\n\nThe data contains the following information:  \n1. **ID** : ID Number of Customers.\n2. **Warehouse block** : The Company have big Warehouse which is divided in to block such as A,B,C,D,E.\n3. **Mode of shipment** :The Company Ships the products in multiple way such as Ship, Flight and Road.\n4. **Customer care calls** : The number of calls made from enquiry for enquiry of the shipment.\n5. **Customer rating** : The company has rated from every customer. 1 is the lowest (Worst), 5 is the highest (Best).\n6. **Cost of the product** : Cost of the Product in US Dollars.\n7. **Prior purchases** : The Number of Prior Purchase.\n8. **Product importance** : The company has categorized the product in the various parameter such as low, medium, high.\n9. **Gender** : Male and Female.\n10. **Discount offered** : Discount offered on that specific product. \n11. **Weight in gms** : It is the weight in grams.\n12. **Reached on time** : It is the target variable, where 1 Indicates that the\nproduct has NOT reached on time and 0 indicates it has reached on time.\n","80fef9c6":"Luckily, there is no duplicated value in the dataframe.","4e8f9f3f":"#### Decision Tree","644d5a3a":"### Fit & Evaluation Model","72c763f2":"### Split train & test data","b1b5ef10":"#### SVC","9e30b31c":"#### Identify duplicated values","defa3762":"**Summary :**  \n- Total parcels of female customers in the warehouse_block are more dominant than male customers, except in warehouse_block B.","22b44d0e":"Just for making sure that no missing values are found.","dd61c875":"#### Correlation Heatmap","6d021372":"#### KNN","680556c3":"# Machine Learning Modelling & Evaluation","645e0e94":"**Numeric - Product Importance**","4d7bfd1a":"**Summary :**\n- **Warehouse_Block** has 5 unique values and dominated with `Warehouse_block_f`.\n- **Mode_of_Shipment** has 3 unique values and mostly used ship.\n- **Product_importance** has 3 unique values and mostly priority of products are low.\n- Female customers are often shopping than male.\n\n\n\n","e776867a":"#### Reload dataset","45698ddf":"## Exploratory Data Analysis (EDA)","f155b881":"### Import library","6d92df57":"### Confusion matrix","ac9dfc7e":"## Load & Describe Data","1b246417":"#### Feature Scaling : Standardization","26272a91":"#### Pairplot","79a47e1a":"### Separate numeric & categorical column","40fa3471":"### Change all column names to lower case","80db6aa9":"#### Logistic Regression","32ee2e19":"**Summary :**  \nSome parcels with the weight more than 6 Kg are delivered with `ship` & `road`. The ship can afford to carry the big parcels.\n\n","7fc78f5f":"From the result above, only **Logistic Regression and SVC which are neither overfitting nor underfiting**. Logistic Regression has the highest recall. Let's see with tuned parameter.","1555f40e":"**Summary :**  \n- Some parcels with the weight more than 6 Kg are delivered in `medium` priority.\n- Mostly cost of the products which are delivered in high priority are between 130 - 280 dollar.\n\n","91b4b4a7":"#### Categorical values","f2f8171b":"#### Identify outliers"}}