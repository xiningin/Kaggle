{"cell_type":{"ca4643b6":"code","6eb629b1":"code","e2abd6be":"code","229a3d2c":"code","ffd1fc74":"code","6f121a41":"code","78d6624a":"code","0ff6fa3d":"code","85f42626":"code","2178c034":"code","1ff60f76":"code","140ca326":"code","6c577d06":"code","1e7333b4":"code","4d93e441":"code","d163c902":"code","25266b5d":"code","8b3de495":"code","7570c33b":"code","60f44a70":"code","3ddebe43":"code","41f134c9":"code","c3204e78":"code","f83a93e2":"code","a401b94b":"code","76d95c64":"code","fd500332":"code","b944b9f7":"code","0633ccb4":"code","c8430a7e":"code","38aeb813":"code","1963f55b":"code","ae7298ed":"code","82b7e856":"code","960d524f":"code","4cef9d8e":"code","9a64ca09":"code","4eadfd4e":"code","370b2a5d":"code","16d1c392":"code","0f7d9fab":"code","ee286cac":"code","555e3c5e":"code","6accf85b":"code","61ae13a9":"code","1c6cfb5a":"code","487262c4":"code","55516c99":"code","42ac977f":"code","a620c7f3":"code","c5b62404":"code","710776a3":"code","396836a9":"code","2109b739":"code","48a31b64":"code","7482234a":"code","1351a83e":"code","08ccf04e":"code","3cc92101":"code","a1dc66f0":"code","8d8dde1d":"code","52a5964d":"code","69955196":"code","f74ac3cc":"code","1de68bab":"code","570351ff":"code","d98b11d4":"code","568bdd5b":"code","a1908ee3":"code","26918b90":"code","f245e383":"markdown","c01390f7":"markdown","87f8ef6a":"markdown","a2ff9347":"markdown","58766ebd":"markdown","8f051fce":"markdown","ad4c71a8":"markdown","9ffd7431":"markdown","e8f13850":"markdown","2f905ecc":"markdown","9f4408b4":"markdown","9acd9914":"markdown","77069e1e":"markdown","775de6c1":"markdown","09a740a9":"markdown","6959f6b1":"markdown","6e1dca69":"markdown","ea0d2ba8":"markdown","a3497202":"markdown","2f500660":"markdown","87b077d5":"markdown","1cde431a":"markdown","f6c3cdbb":"markdown","1ef7ccf8":"markdown","03434ade":"markdown","a4e7fc02":"markdown","7930a303":"markdown","c20a9c21":"markdown","93e3074b":"markdown","60f8dd3c":"markdown","4fa7b0b1":"markdown","ead652c7":"markdown","90b05930":"markdown","0872235c":"markdown","35b3d78f":"markdown","4871eb59":"markdown"},"source":{"ca4643b6":"# Importing the libraries\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Importing the training dataset\ntrain = pd.read_csv(\"..\/input\/train.csv\")\n\n# Viewing the number of rows and columns in the training dataset\ntrain_shape = train.shape\nprint(train_shape)","6eb629b1":"# Importing the testing dataset\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\n# Viewing the number of rows and columns in the training dataset\ntest_shape = test.shape\nprint(test_shape)","e2abd6be":"train.info()","229a3d2c":"# The first 5 rows of the training dataset are below:\n# Index in python starts with 0.\ntrain.head()","ffd1fc74":"# We can use DataFrame.pivot_table() to easily do this\n# Importing the library for plotting\nimport matplotlib.pyplot as plt\n# Calling the pivot_table() function for Sex\nsex_pivot = train.pivot_table(index = \"Sex\", values = \"Survived\")\nsex_pivot.plot.bar()\nplt.show()","6f121a41":"# Calling the dataframe.pivot_table() function for Pclass\npclass_pivot = train.pivot_table(index = \"Pclass\", values = \"Survived\")\npclass_pivot.plot.bar()\nplt.show()","78d6624a":"# Let's take a look at the Age column using Series.describe()\ntrain[\"Age\"].describe()","0ff6fa3d":"# Contains the details of the passengers who survived\nsurvived = train[train[\"Survived\"] == 1]\nsurvived[\"Age\"].plot.hist(alpha=0.5, color=\"red\", bins=50)","85f42626":"# Contains the details fo the passengers who died\ndied = train[train[\"Survived\"] == 0]\ndied[\"Age\"].plot.hist(alpha=0.5, color=\"blue\", bins=50)","2178c034":"# Viewing them combined\nsurvived[\"Age\"].plot.hist(alpha=0.5, color=\"red\", bins=50)\ndied[\"Age\"].plot.hist(alpha=0.5, color=\"blue\", bins=50)\nplt.legend([\"Survived\",\"Died\"])\nplt.show()","1ff60f76":"# Create a function to process the Age column to different categories\ndef process_age(df, cut_points, label_names):\n    # use the pandas.fillna() method to fill all of the missing values with -0.5\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    # cuts the Age column using pandas.cut()\n    df[\"Age_categories\"] = pd.cut(df[\"Age\"], cut_points, labels=label_names)\n    return df\n\n# Cut the Age column into seven segments: Missing, from -1 to 0 Infant, from 0 to 5 Child, from 5 to 12 Teenager, from 12 to 18 \n# Young Adult, from 18 to 35 Adult, from 35 to 60 Senior, from 60 to 100\ncut_points = [-1, 0, 5, 12, 18, 35, 60, 100]\nlabel_names = [\"Missing\", \"Infant\", \"Child\", \"Teenager\", \"Young Adult\", \"Adult\", \"Senior\"]\n\ntrain = process_age(train, cut_points, label_names)\ntest = process_age(test, cut_points, label_names)    ","140ca326":"# Use the pivot_tables() function to plot with Age_categories column\nage_categories_pivot = train.pivot_table(index=\"Age_categories\", values = \"Survived\")\nage_categories_pivot.plot.bar()\nplt.show()","6c577d06":"# value_counts() function is used to get the count of occurence unique values present in the column of the dataset.\ntrain[\"Pclass\"].value_counts()","1e7333b4":"# pandas.get_dummies() function will generate columns for us.\ndef create_dummies(df, column_name):\n    dummies = pd.get_dummies(df[column_name], prefix=column_name)\n    df = pd.concat([df,dummies], axis=1)\n    return df\n\ntrain = create_dummies(train, \"Pclass\")\ntest = create_dummies(test, \"Pclass\")\n\ntrain.head()","4d93e441":"# Similarly for Sex & Age Categories Column\ntrain = create_dummies(train, \"Sex\")\ntest = create_dummies(test, \"Sex\")\n\ntrain = create_dummies(train, \"Age_categories\")\ntest = create_dummies(test, \"Age_categories\")\n\ntrain.head()","d163c902":"# Calling the pivot_table() function for Embarked\nembarked_pivot = train.pivot_table(index=\"Embarked\", values=\"Survived\")\nembarked_pivot.plot.bar()\nplt.show()","25266b5d":"# Calling the pivot_table() function for SibSp\nSibSp_pivot = train.pivot_table(index=\"SibSp\", values=\"Survived\")\nSibSp_pivot.plot.bar()\nplt.show()","8b3de495":"# Calling the pivot_table() function for Parch\nparch_pivot = train.pivot_table(index=\"Parch\", values=\"Survived\")\nparch_pivot.plot.bar()\nplt.show()","7570c33b":"train.columns","60f44a70":"# View the details of the other columns\ncolumns = [\"SibSp\", \"Parch\", \"Fare\", \"Cabin\", \"Embarked\"]\ntrain[columns].describe(include=\"all\", percentiles=[])","3ddebe43":"# the preprocessing.minmax_scale() function allows us to quickly and easily rescale our data\nfrom sklearn.preprocessing import minmax_scale\n# rescale the SibSp, Parch, and Fare columns\n# Added 2 backets to make it a dataframe. Otherwise you will get a type error stating cannot iterate over 0-d array.\ntrain[\"SibSp_scaled\"] = minmax_scale(train[[\"SibSp\"]])\ntrain[\"Parch_scaled\"] = minmax_scale(train[[\"Parch\"]])\ntrain[\"Fare_scaled\"] = minmax_scale(train[[\"Fare\"]])\ntrain.head()\n","41f134c9":"# Checking the details with the test data\ntest[columns].describe(include=\"all\",percentiles=[])","c3204e78":"# Fare column has a missing value, we will replace the missing value with the mean\ntest[\"Fare\"] = test[\"Fare\"].fillna(train[\"Fare\"].mean())\ntest[\"Fare\"].count()","f83a93e2":"# Applying the same rescaling to the test dataset \ntest[\"SibSp_scaled\"] = minmax_scale(test[[\"SibSp\"]])\ntest[\"Parch_scaled\"] = minmax_scale(test[[\"Parch\"]])\ntest[\"Fare_scaled\"] = minmax_scale(test[[\"Fare\"]])\ntest.head()","a401b94b":"# Analyzing & Fixing the Embarked Column\ntrain[columns].describe(include=\"all\", percentiles=[])","76d95c64":"# We have 2 missing values in the training data of the Embarked column \n# S is the most common value occuring 644 time. So we will replace the missing value with S.\ntrain[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\ntrain[\"Embarked\"].describe()","fd500332":"# Checking with the testing dataset\ntest[columns].describe(include=\"all\", percentiles=[])","b944b9f7":"test.shape\n# We have no missing value for Embarked in the test data","0633ccb4":"# Creating dummy columns for Embarked columns\ntrain = create_dummies(train, \"Embarked\")\ntest = create_dummies(test, \"Embarked\")\ntrain.head()","c8430a7e":"test.head()","38aeb813":"# Defining all the featured columns\nlrColumns = ['Age_categories_Missing','Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Teenager',\n       'Age_categories_Young Adult', 'Age_categories_Adult',\n       'Age_categories_Senior', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n       'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n       'SibSp_scaled', 'Parch_scaled', 'Fare_scaled']\n\nprint(lrColumns)","1963f55b":"# Applying Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogisticRegression = LogisticRegression()\nlogisticRegression.fit(train[lrColumns], train[\"Survived\"])\ncoefficients = logisticRegression.coef_\nprint(coefficients)","ae7298ed":"feature_importance = pd.Series(coefficients[0], index=lrColumns)\nprint(feature_importance)","82b7e856":"# Plotting as a horizontal Bar chart\nfeature_importance.plot.barh()\nplt.show()","960d524f":"ordered_feature_importance = feature_importance.abs().sort_values()\nordered_feature_importance.plot.barh()\nplt.show()","4cef9d8e":"# We'll train a model with the top 8 scores\npredictors = ['Age_categories_Infant', 'SibSp_scaled', 'Sex_female', 'Sex_male',\n       'Pclass_1', 'Pclass_3', 'Age_categories_Senior', 'Parch_scaled']\n\nlr = LogisticRegression()\nlr.fit(train[predictors], train[\"Survived\"])\npredictions = lr.predict(test[predictors])\nprint(predictions)","9a64ca09":"# Calculating the accuracy using the k-fold cross validation method with k=10\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(lr, train[predictors], train[\"Survived\"], cv=10)\nprint(scores)","4eadfd4e":"# Taking the mean of all the scores\naccuracy = scores.mean()\nprint(accuracy)","370b2a5d":"# # Submitting the result\n# submission = pd.DataFrame({\n#         \"PassengerId\": test[\"PassengerId\"],\n#         \"Survived\": predictions\n#     })\n# submission.to_csv('submission1.csv', index=False)","16d1c392":"print(train[\"Fare\"])","0f7d9fab":"import numpy as np\nnp.histogram(train[\"Fare\"])","ee286cac":"# Creating a frequency table\nfrom collections import Counter\nfare_count = Counter(train[\"Fare\"])\n# fare_labels, fare_values = zip(*Counter(train[\"Fare\"]).items())\n# print(fare_labels)\nprint(fare_count)","555e3c5e":"plt.hist(train[\"Fare\"], bins = range(150))\nplt.show()","6accf85b":"# print(fare_values)","61ae13a9":"# indexes = np.arange(len(fare_labels))\n# print(indexes)","1c6cfb5a":"# Plotting a bar chart\n# width = 6\n# plt.bar(indexes, fare_values, width)\n# plt.show()","487262c4":"# Plotting a histogram\n# plt.hist(fare_values, bins=10)\n# plt.show()","55516c99":"import seaborn as sns\nsns.distplot(train[\"Fare\"])","42ac977f":"survived = train[train[\"Survived\"]==1]\ndied = train[train[\"Survived\"]==0]\nsurvived[\"Fare\"].plot.hist(alpha=0.5, color = \"red\", bins=range(150))\ndied[\"Fare\"].plot.hist(alpha=0.5, color = \"blue\", bins=range(150))\nplt.legend(\"Survived\",\"died\")\nplt.show()","a620c7f3":"# Creating functions as created for Age to perform binning on the Fare column\ndef process_fare(df, cut_points, label_names):\n    df[\"Fare_categories\"] = pd.cut(df[\"Fare\"], cut_points, labels = label_names)\n    return df\n\nfare_cut_points = [0, 12, 50, 100, 1000]\nfare_label_names = [\"0-12\", \"12-50\", \"50-100\", \"100+\"]\n\nprocess_fare(train, fare_cut_points, fare_label_names)\nprint(train[\"Fare_categories\"])","c5b62404":"# for the test dataset\nprocess_fare(test, fare_cut_points, fare_label_names)\nprint(test[\"Fare_categories\"])","710776a3":"# Calling the create_dummies function to convert the categories into columns\ntrain = create_dummies(train, \"Fare_categories\")\ntrain.head()","396836a9":"test = create_dummies(test, \"Fare_categories\")\ntest.head()","2109b739":"train[[\"Name\",\"Cabin\"]].head(10)","48a31b64":"train.head()[\"Cabin\"]","7482234a":"train.head()[\"Cabin\"].str[0]","1351a83e":"# Creating a new column Cabin_type to store these values and fill all the missing values with unknown\ntrain[\"Cabin_type\"] = train[\"Cabin\"].str[0]\ntrain[\"Cabin_type\"] = train[\"Cabin_type\"].fillna(\"Unknown\")\ntrain[\"Cabin_type\"].head()","08ccf04e":"# Doing the same for test dataset\ntest[\"Cabin_type\"] = test[\"Cabin\"].str[0]\ntest[\"Cabin_type\"] = test[\"Cabin_type\"].fillna(\"Unknown\")\ntest[\"Cabin_type\"].head()","3cc92101":"titles = {\n    \"Mr\" :         \"Mr\",\n    \"Mme\":         \"Mrs\",\n    \"Ms\":          \"Mrs\",\n    \"Mrs\" :        \"Mrs\",\n    \"Master\" :     \"Master\",\n    \"Mlle\":        \"Miss\",\n    \"Miss\" :       \"Miss\",\n    \"Capt\":        \"Officer\",\n    \"Col\":         \"Officer\",\n    \"Major\":       \"Officer\",\n    \"Dr\":          \"Officer\",\n    \"Rev\":         \"Officer\",\n    \"Jonkheer\":    \"Royalty\",\n    \"Don\":         \"Royalty\",\n    \"Sir\" :        \"Royalty\",\n    \"Countess\":    \"Royalty\",\n    \"Dona\":        \"Royalty\",\n    \"Lady\" :       \"Royalty\"\n}","a1dc66f0":"extracted_titles = train[\"Name\"].str.extract(' ([A-Za-z]+)\\.',expand=False)\ntrain[\"Title\"] = extracted_titles.map(titles)\ntrain[\"Title\"].head()","8d8dde1d":"extracted_titles = test[\"Name\"].str.extract(' ([A-Za-z]+)\\.',expand=False)\ntest[\"Title\"] = extracted_titles.map(titles)\ntest[\"Title\"].head()","52a5964d":"# Using the create dummies function to convert these values into categories\nfor column in [\"Title\",\"Cabin_type\"]:\n    train = create_dummies(train,column)\n    test = create_dummies(test,column)\ntrain.head()","69955196":"test.head()","f74ac3cc":"import seaborn as sns\ncorrelations = train.corr()\nsns.heatmap(correlations)\nplt.show()","1de68bab":"# custom function to set the style for heatmap\ndef plot_correlation_heatmap(df):\n    corr = df.corr()\n    sns.set(style=\"white\")\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    f, ax = plt.subplots(figsize=(11, 9))\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n    plt.show()\n    \n# Columns to use    \nheatmap_columns = ['Age_categories_Missing', 'Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Teenager',\n       'Age_categories_Young Adult', 'Age_categories_Adult',\n       'Age_categories_Senior', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n       'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n       'SibSp_scaled', 'Parch_scaled', 'Fare_categories_0-12',\n       'Fare_categories_12-50','Fare_categories_50-100', 'Fare_categories_100+',\n       'Title_Master', 'Title_Miss', 'Title_Mr','Title_Mrs', 'Title_Officer',\n       'Title_Royalty', 'Cabin_type_A','Cabin_type_B', 'Cabin_type_C', 'Cabin_type_D',\n       'Cabin_type_E','Cabin_type_F', 'Cabin_type_G', 'Cabin_type_T', 'Cabin_type_Unknown']\n\nplot_correlation_heatmap(train[heatmap_columns])","570351ff":"from sklearn.feature_selection import RFECV\n\npredictor_columns = ['Age_categories_Missing', 'Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Young Adult',\n       'Age_categories_Adult', 'Age_categories_Senior', 'Pclass_1', 'Pclass_3',\n       'Embarked_C', 'Embarked_Q', 'Embarked_S', 'SibSp_scaled',\n       'Parch_scaled', 'Fare_categories_0-12', 'Fare_categories_50-100',\n       'Fare_categories_100+', 'Title_Miss', 'Title_Mr', 'Title_Mrs',\n       'Title_Officer', 'Title_Royalty', 'Cabin_type_B', 'Cabin_type_C',\n       'Cabin_type_D', 'Cabin_type_E', 'Cabin_type_F', 'Cabin_type_G',\n       'Cabin_type_T', 'Cabin_type_Unknown']\n\nall_X = train[predictor_columns]\nall_y = train[\"Survived\"]\n\nlr = LogisticRegression()\nselector = RFECV(lr, cv = 10)\nselector.fit(all_X, all_y)\n\noptimized_predictors = all_X.columns[selector.support_]\nprint(optimized_predictors)","d98b11d4":"all_X = train[optimized_predictors]\nall_y = train[\"Survived\"]\n\nlr = LogisticRegression()\nlr.fit(all_X, all_y)\nscores = cross_val_score(lr,all_X,all_y, cv=10)\nprint(scores)","568bdd5b":"accuracy = scores.mean()\nprint(accuracy)","a1908ee3":"predictions = lr.predict(test[optimized_predictors])","26918b90":"# Submitting the result\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": predictions\n    })\nsubmission.to_csv('submission1.csv', index=False)","f245e383":"Of these,* SibSp, Parch and Fare * look to be standard numeric columns with no missing values. Cabin has values for only 204 of the 891 rows, and even then most of the values are unique, so for now we will leave this column also. Embarked looks to be a standard categorical column with 3 unique values, much like PClass was, except that there are two missing values. We can easily fill these two missing values with the most common value, \"S\" which occurs 644 times.\n\nLooking at our numeric columns, we can see a big difference between the range of each. SibSp has values between 0-8, Parch between 0-6, and Fare is on a dramatically different scale, with values ranging from 0-512. In order to make sure these values are equally weighted within our model, we'll need to rescale the data.\n\nRescaling simply stretches or shrinks the data as needed to be on the same scale, in our case between 0 and 1.","c01390f7":"In order to select the best-performing features, we need a way to measure which of our features are relevant to our outcome - in this case, the survival of each passenger. One effective way is by training a logistic regression model using all of our features, and then looking at the coefficients of each feature.\n\nThe scikit-learn LogisticRegression class has an attribute in which coefficients are stored after the model is fit, **LogisticRegression.coef_**. We first need to train our model, after which we can access this attribute.","87f8ef6a":"The type of machine learning we will be doing is called **classification**, because when we make predictions we are classifying each passenger as survived or not. More specifically, we are performing **binary classification**, which means that there are only two different states we are classifying.","a2ff9347":"While the class of each passenger certainly has some sort of ordered relationship, the relationship between each class is not the same as the relationship between the numbers 1, 2, and 3. For instance, class 2 isn't \"worth\" double what class 1 is, and class 3 isn't \"worth\" triple what class 1 is.\n\nIn order to remove this relationship, we can create dummy columns for each unique value in Pclass.","58766ebd":"**The dataset has below columns :**\n* PassengerID - A column added by Kaggle to identify each row and make submissions easier\n* Survived - Whether the passenger survived or not and the value we are predicting (0=No, 1=Yes)\n* Pclass - The class of the ticket the passenger purchased (1=1st, 2=2nd, 3=3rd)\n* Sex - The passenger's sex\n* Age - The passenger's age in years\n* SibSp - The number of siblings or spouses the passenger had aboard the Titanic\n* Parch - The number of parents or children the passenger had aboard the Titanic\n* Ticket - The passenger's ticket number\n* Fare - The fare the passenger paid\n* Cabin - The passenger's cabin number\n* Embarked - The port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton)","8f051fce":"*Hey guys, thank you for visiting my kernel.\nThis kernel is for all the beginners  and machine learning enthusiasts who face the difficulty and want to learn how to get started with the dataset. \nIf you are an expert reading this and would like to give your inputs. Please comment the same below. We really appreciate your guidance.* ","ad4c71a8":"In any machine learning exercise, thinking about the topic you are predicting is very important. We call this step acquiring domain knowledge, and it's one of the most important determinants for success in machine learning.\n\nIn this case, understanding the Titanic disaster and specifically what variables might affect the outcome of survival is important. Anyone who has watched the movie Titanic would remember that women and children were given preference to lifeboats (as they were in real life). You would also remember the vast class disparity of the passengers.\n\nThis indicates that *Age*, *Sex*, and *PClass* may be good predictors of survival. We'll start by exploring Sex and Pclass by visualizing the data.","9ffd7431":"While in isolation the cabin number of each passenger will be reasonably unique to each, we can see that the format of the cabin numbers is one letter followed by two numbers. It seems like the letter is representative of the type of cabin, which could be useful data for us. We can use the pandas Series.str accessor and then subset the first character using brackets:","e8f13850":"The *Sex* and *PClass* columns are what we call **categorical** features. That means that the values represented a few separate options (for instance, whether the passenger was male or female).","2f905ecc":"In this competition, we have a data set of different information about passengers onboard the Titanic, and we see if we can use that information to predict whether those people survived or not.","9f4408b4":"The relationship here is not simple, but we can see that in some age ranges more passengers survived - where the red bars are higher than the blue bars.\n\nIn order for this to be useful to our machine learning model, we can separate this continuous feature into a categorical feature by dividing it into ranges. We can use the *pandas.cut()* function to help us out.\n\nThe pandas.cut() function has two required parameters - the column we wish to cut, and a list of numbers which define the boundaries of our cuts. We are also going to use the optional parameter labels, which takes a list of labels for the resultant bins. This will make it easier for us to understand our results.\n\nBefore we modify this column, we have to be aware of two things. Firstly, any change we make to the train data, we also need to make to the test data, otherwise we will be unable to use our model to make predictions for our submissions. Secondly, we need to remember to handle the missing values we observed above.","9acd9914":"Before we build our model, we need to prepare these columns for machine learning. Most machine learning algorithms can't understand text labels, so we have to convert our values into numbers.\n\nAdditionally, we need to be careful that we don't imply any numeric relationship where there isn't one. If we think of the values in the Pclass column, we know they are 1, 2, and 3.","77069e1e":"Collinearity can happen in other places, too. A common way to spot collinearity is to plot correlations between each pair of variables in a heatmap. ","775de6c1":"Looking at the values, it looks like we can separate the feature into four bins to capture some patterns from the data:\n\n* 0-12\n* 12-50\n* 50-100\n* 100+","09a740a9":"Looking at the Name column, There is a title like 'Mr' or 'Mrs' within each, as well as some less common titles, like the 'Countess'.  By spending some time researching the different titles, we can categorize these into the below categories:","6959f6b1":"We can use the Series.str.extract method and a regular expression to extract the title from each name and then use the Series.map() method and a predefined dictionary to simplify the titles.","6e1dca69":"We now have 34 possible feature columns we can use to train our model. One thing to be aware of as you start to add more features is a concept called **collinearity**. Collinearity occurs where more than one feature contains data that are similar.\n\nThe effect of collinearity is that your model will overfit - you may get great results on your test data set, but then the model performs worse on unseen data (like the test set).\n\nOne easy way to understand collinearity is with a simple binary variable like the **Sex** column in our dataset. Every passenger in our data is categorized as either male or female, so 'not male' is exactly the same as 'female'.\n\nAs a result, when we created our two dummy columns from the categorical Sex column, we've actually created two columns with identical data in them. This will happen whenever we create dummy columns, and is called the **dummy variable trap**. The easy solution is to choose one column to drop any time you make dummy columns.","ea0d2ba8":"You can see that a passenger belonging to class 1 has a better chance of surviving in comparison to class 2 & 3 passenger.","a3497202":"We can immediately see that females survived in much higher proportions than males did.\n\nLet's do the same with the Pclass column.","2f500660":"***Happy Learning :D***","87b077d5":"Another way to engineer features is by extracting data from text columns. Earlier, we decided that the **Name** and **Cabin** columns weren't useful by themselves, but what if there is some data there we could extract? Let's take a look at a random sample of rows from those two columns:","1cde431a":"The *Age* column contains numbers ranging from *0.42* to *80.0* (If you look at Kaggle's data page, it informs us that Age is fractional if the passenger is less than one). The other thing to note here is that there are 714 values in this column, fewer than the 891 rows we discovered that the train data set had earlier in this mission which indicates we have some missing values.","f6c3cdbb":"The training set contains data we can use to train our model. It has a number of feature columns which contain various descriptive data, as well as a column of the target values we are trying to predict:  ***Survival***","1ef7ccf8":"It is very difficult to make inferences from this heatmap.\n\nLets create a custom function for the same.","03434ade":"In an earlier step, we manually used the logit coefficients to select the most relevant features. An alternate method is to use one of scikit-learn's inbuilt feature selection classes. We will be using the feature_selection.**RFECV** class which performs recursive feature elimination with cross-validation.\n\n*The RFECV class starts by training a model using all of your features and scores it using cross validation. It then uses the logit coefficients to eliminate the least important feature, and trains and scores a new model. At the end, the class looks at all the scores, and selects the set of features which scored highest.*\n\nLike the LogisticRegression class, RFECV must first be instantiated and then fit. The first parameter when creating the RFECV object must be an estimator, and we need to use the cv parameter to specific the number of folds for cross-validation.","a4e7fc02":"The testing set contains all of the same feature columns, but is missing the target value column. Additionally, the testing set usually has fewer observations (rows) than the training set.","7930a303":"**Visualizations**\n\nThere are a lot of different ways to visualize the data but lets start with the simple ones.\n\nBecause the *Survived* column contains 0 if the passenger did not survive and 1 if they did, we can segment our data by sex and calculate the mean of this column.","c20a9c21":"The plot we generated shows a range of both positive and negative values. Whether the value is positive or negative isn't as important in this case, relative to the magnitude of the value. If you think about it, this makes sense. A feature that indicates strongly whether a passenger died is just as useful as a feature that indicates strongly that a passenger survived, given they are mutually exclusive outcomes.\n\nTo make things easier to interpret, we'll alter the plot to show all positive values, and have sorted the bars in order of size:","93e3074b":"**Getting Started **","60f8dd3c":"All of this means that the *Age* column needs to be treated slightly differently, as this is a continuous numerical column. One way to look at *distribution of values in a continuous numerical set* is to use **histograms** . We can create two histograms to compare visually the those that survived vs those who died across different age ranges:","4fa7b0b1":"The RFECV() selector returned only four columns.\n\nLet's train a model using cross validation using these columns and check the score.","ead652c7":"**Importing the dataset**","90b05930":"We can see that there is a high correlation between **Sex_female\/Sex_male** and **Title_Miss\/Title_Mr\/Title_Mrs.**\n\nWe will remove the columns Sex_female and Sex_male since the title data may be more nuanced.\n\nApart from that, we should remove one of each of our dummy variables to reduce the collinearity in each. We'll remove:\n\n* Pclass_2\n* Age_categories_Teenager\n* Fare_categories_12-50\n* Title_Master\n* Cabin_type_A","0872235c":"The coef() method returns a NumPy array of coefficients, in the same order as the features that were used to fit the model. To make these easier to interpret, we can convert the coefficients to a pandas series, adding the column names as the index:","35b3d78f":"A lot of the gains in accuracy in machine learning come from Feature Engineering. Feature engineering is the practice of creating new features from your existing data.\n\nOne common way to engineer a feature is using a technique called **binning**. Binning is when you take a continuous feature, like the fare a passenger paid for their ticket, and separate it out into several ranges (or 'bins'), turning it into a categorical variable.\n\nThis can be useful when there are patterns in the data that are non-linear and you're using a linear model (like logistic regression). We actually used binning when we dealt with the Age column, although we didn't use the term.","4871eb59":"This is just the 1st submission, many more to come."}}