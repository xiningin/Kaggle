{"cell_type":{"c23fad9c":"code","61bc5c00":"code","30017013":"code","5d3e8cd2":"code","6a0e4702":"code","aa2e3f05":"code","ffbbad53":"code","568c33d0":"code","cd236ab5":"code","690f0a55":"code","fa04bacd":"code","f4edef42":"code","89645ac1":"code","ee59c834":"code","a4572c72":"code","4324406d":"code","36c9ceb2":"code","83aa64b0":"code","ff144175":"code","26497f2d":"code","7370fa54":"code","a06dc1e3":"code","32e2606c":"code","560b3c06":"code","0c6b994b":"code","0d2edf8a":"code","40286626":"code","3a8667e2":"code","5148f746":"code","4ca9a795":"code","d3ee81cb":"code","18308a87":"code","dca5d569":"code","28d826b3":"code","66ceff13":"markdown","0f7c2cc3":"markdown","23dbaaf1":"markdown","61ab9dff":"markdown","c7b0ff1c":"markdown","7d68b220":"markdown","65e6e91a":"markdown","818a2116":"markdown","f66403d6":"markdown","1c0badef":"markdown","feddf74a":"markdown","873f53bc":"markdown","737e7202":"markdown","f9493f2b":"markdown","f69702d3":"markdown","c7f63f55":"markdown","f9445bb2":"markdown","74a5cc59":"markdown","6a654c46":"markdown","8d99c091":"markdown","03edafbf":"markdown","d0897070":"markdown","224e079f":"markdown","fe5d786a":"markdown","d2b41bb7":"markdown","a9b893cb":"markdown","52e62fda":"markdown","08cb80df":"markdown","c2bd62c5":"markdown","fd7333ba":"markdown","b69c7dfe":"markdown","d41b551b":"markdown"},"source":{"c23fad9c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.offline as py\ncolor = sns.color_palette()\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\nimport plotly.tools as tls\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSMALL_SIZE = 10\nMEDIUM_SIZE = 12\n\nplt.rc('font', size=SMALL_SIZE)\nplt.rc('axes', titlesize=MEDIUM_SIZE)\nplt.rc('axes', labelsize=MEDIUM_SIZE)\nplt.rcParams['figure.dpi']=150\n# Any results you write to the current directory are saved as output.","61bc5c00":"df=pd.read_csv('..\/input\/Skyserver_SQL2_27_2018 6_51_39 PM.csv')\ndf.head()","30017013":"df.describe()","5d3e8cd2":"df.info()","6a0e4702":"columns = df.columns\npercent_missing = df.isnull().sum() * 100 \/ len(df)\nmissing_value_df = pd.DataFrame({'column_name': columns,\n                                 'percent_missing': percent_missing})\nmissing_value_df.sort_values('percent_missing')","aa2e3f05":"sns.heatmap(df.corr())\ndf.corr()","ffbbad53":"df.drop(['specobjid','fiberid'],axis=1,inplace=True)","568c33d0":"cnt_srs = df['class'].value_counts()\ntrace = go.Bar(\n    y=cnt_srs.index[::-1],\n    x=cnt_srs.values[::-1],\n    orientation = 'h',\n    marker=dict(\n        color=cnt_srs.values[::-1],\n        colorscale = 'Blues',\n        reversescale = True\n    ),\n)\n\nlayout = dict(\n    title='Class distribution',\n    )\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"Ratings\")","cd236ab5":"fig, axes = plt.subplots(nrows=1, ncols=3,figsize=(16, 4))\nax = sns.distplot(df[df['class']=='STAR'].redshift, bins = 30, ax = axes[0], kde = False)\nax.set_title('Star')\nax = sns.distplot(df[df['class']=='GALAXY'].redshift, bins = 30, ax = axes[1], kde = False)\nax.set_title('Galaxy')\nax = sns.distplot(df[df['class']=='QSO'].redshift, bins = 30, ax = axes[2], kde = False)\nax = ax.set_title('QSO')","690f0a55":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(16, 4))\nax = sns.lvplot(x=df['class'], y=df['dec'], palette='coolwarm')\nax.set_title('dec')\n","fa04bacd":"di={'STAR':1,'GALAXY':2,'QSO':3}\ndf.replace({'class':di},inplace=True)\n\ny=df['class']\ndf.drop(['objid','class'],axis=1,inplace=True)","f4edef42":"dx=df[['ra','dec','u','g','r','i','z','run','rerun','camcol','field','redshift','plate','mjd']]\nfor i in dx.columns:\n    plt.figure(figsize=(12,8))\n    sns.boxplot(y=i, data=df)\n    plt.ylabel(i+'Distribution', fontsize=12)\n    plt.title(i+\"Distribution\", fontsize=14)\n    plt.xticks(rotation='vertical')\n    plt.show()","89645ac1":"import umap\n\nembedding = umap.UMAP(n_neighbors=5,\n                      min_dist=0.3,\n                      metric='correlation').fit_transform(df.iloc[:20000, 1:])\n\nplt.figure(figsize=(12,12))\nplt.scatter(embedding[:20000, 0], embedding[:20000, 1], \n            c=df.iloc[:20000, 0], \n            edgecolor='none', \n            alpha=0.80, \n            s=10)\nplt.axis('off');","ee59c834":"df.head(1)","a4572c72":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nsdss = scaler.fit_transform(df)","4324406d":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df, \n                                                    y, test_size=0.33)","36c9ceb2":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\npreds = knn.predict(X_test)\nacc_knn = (preds == y_test).sum().astype(float) \/ len(preds)*100\nprint(\"Accuracy of KNN: \", acc_knn)","83aa64b0":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} \n\ngrid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\n\ngrid.fit(X_train,y_train)","ff144175":"grid.best_params_","26497f2d":"grid.best_estimator_","7370fa54":"grid_predictions = grid.predict(X_test)","a06dc1e3":"acc_gv_rbf = (grid_predictions == y_test).sum().astype(float) \/ len(grid_predictions)*100\nprint(\"Accuracy of KNN: \", acc_gv_rbf)","32e2606c":"'''param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['linear']} \n\ngrid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\n\ngrid.fit(X_train,y_train)'''","560b3c06":"from sklearn.naive_bayes import GaussianNB\n\ngnb=GaussianNB()\ngnb.fit(X_train,y_train)\npreds2=gnb.predict(X_test)\nacc_gnb=(preds2==y_test).sum().astype(float)\/len(preds)*100\nprint(\"Accuracy of Naive Bayes: \",acc_gnb)","0c6b994b":"from sklearn.ensemble import RandomForestClassifier\n\nrf=RandomForestClassifier()\nrf.fit(X_train,y_train)\npreds3=rf.predict(X_test)\nacc_rf=(preds3==y_test).sum().astype(float)\/len(preds)*100\nprint(\"Accuracy of Random Forest Classifier: \",acc_rf)","0d2edf8a":"import xgboost as xgb\n\nxgb=xgb.XGBClassifier()\nxgb.fit(X_train,y_train)\npreds4=xgb.predict(X_test)\nacc_xgb=(preds4==y_test).sum().astype(float)\/len(preds)*100\nprint(\"Accuracy of XGBoost Classifier: \",acc_xgb)","40286626":"import lightgbm as lgb\n\nlgb=lgb.LGBMClassifier()\nlgb.fit(X_train,y_train)\npreds5=lgb.predict(X_test)\nacc_lgb=(preds5==y_test).sum().astype(float)\/len(preds)*100\nprint(\"Accuracy of LightGBM Classifier: \",acc_lgb)","3a8667e2":"trace1 = go.Bar(\n    x=['KNN','Naive Bayes','Random Forest','XGBoost','LightGBM'],\n    y=[acc_knn,acc_gnb,acc_rf,acc_xgb,acc_lgb],\n    name = 'Accuracy Comparisons of the 4 algorithms',\n        marker=dict(\n        color=cnt_srs.values,\n        colorscale = 'Picnic'\n    ),\n)\n\nlayout = go.Layout(\n    title='Accuracy Score Ratio'\n)\n\ndata = [trace1]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"Ratio\")","5148f746":"from sklearn.cross_validation import *\nfrom sklearn.grid_search import GridSearchCV\n\nparameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['multi:softmax'],\n              'learning_rate': [0.05], #so called `eta` value\n              'max_depth': [6],\n              'min_child_weight': [11],\n              'silent': [1],\n              'subsample': [0.8],\n              'colsample_bytree': [0.7],\n              'n_estimators': [1000], #number of trees, change it to 1000 for better results\n              'missing':[-999],\n              'seed': [1337]}\n\n\nclf = GridSearchCV(xgb, parameters, n_jobs=5, \n                   cv=StratifiedKFold(y_train, n_folds=5, shuffle=True),\n                   verbose=2, refit=True)\n\nclf.fit(X_train, y_train)\npreds6=clf.predict(X_test)\n\nacc_xgbpt=(preds6==y_test).sum().astype(float)\/len(preds)*100\nprint(\"Accuracy of XGBoost Classifier after parameter tuning: \",acc_xgbpt)","4ca9a795":"print(\"Accuracy decreased by =\",(acc_xgb-acc_xgbpt),\"% after parameter tuning with GridSearchCV\")","d3ee81cb":"from keras.utils import to_categorical\n\ny_train=to_categorical(y_train)\ny_test=to_categorical(y_test)","18308a87":"from keras.models import Sequential\nfrom keras.layers import Dense,Dropout\n\nmodel=Sequential()\nmodel.add(Dense(50, activation = \"relu\", input_shape=(14, )))\n# Hidden - Layers\nmodel.add(Dropout(0.3, noise_shape=None, seed=None))\nmodel.add(Dense(50, activation = \"relu\"))\nmodel.add(Dropout(0.2, noise_shape=None, seed=None))\nmodel.add(Dense(50, activation = \"relu\"))\n# Output- Layer\nmodel.add(Dense(4, activation = \"softmax\"))\nmodel.summary()","dca5d569":"model.compile(\n optimizer = \"RMSProp\",\n loss = \"categorical_crossentropy\",\n metrics = [\"accuracy\"]\n)","28d826b3":"results = model.fit(\n X_train, y_train,\n epochs= 10,\n batch_size = 32,\n validation_data = (X_test, y_test)\n)","66ceff13":"We will use GridSearcgCV method for each kernel and check which is giving the highest accuracy.\n\nKernels to be tried are :\n* Radial Basis Function or  RBF Kernel or Gaussian Kernel (widely used, because of non-linear data)\n* Polynomial Kernel \n* Linear Kenel (works best on linear data)","0f7c2cc3":"### XGBoost with Parameter Tuning","23dbaaf1":"Linear Kernel was taking lots of time, that's why had to stop it.","61ab9dff":"#### Random Forest Classifier","c7b0ff1c":"But we will drop the ID columns","7d68b220":"#### Lets see if there are any null values in our dataset","65e6e91a":"So we see for some there are very few or no outliers.","818a2116":"So we don't have any missing values\n\n#### Next we will see if there are any highly correlated columns and drop it accordingly","f66403d6":"### So lets check out the number of each classes","1c0badef":"### SVM","feddf74a":"#### So it seems that in this case XGBoost with default parameters worked well","873f53bc":"#### I'm not going for SVM. I'm also not trying Neural Networks because data is very less. So we will put up a comparison of the scores of the different algorithms.","737e7202":"#### The redshift can be an estimate(!) for the distance from the earth to a object in space.","f9493f2b":"\n#### Not much of a difference between XGBoost and LightGBM. Lets try out XGBoost with some parameter tuning.\n--------------------------------------------------------------------","f69702d3":"So we see that **QSO** is relatively very less in no. We will do some sampling techniques before we fit it into a model.\n\nLets take a look at the redshift.","c7f63f55":"#### Naive Bayes","f9445bb2":"#### KNN","74a5cc59":"--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n### About the notebook\n\nSo here we are going to have some Explaratory Data Analysis along with creating a model which predicts whether it is a galaxy or a star or Quasars with a pretty good accuracy.\n\n\nSo lets get started","6a654c46":"#### Linear Kernel with GridSearchCV","8d99c091":"# Sloan Digital Sky Survey Classification\n\n## Classification of Galaxies, Stars and Quasars based on the RD14 from the SDSS\n","03edafbf":"### Dimensional Reduction with U-Map\n\nSo it is probably the best dimensional reduction technique that I've seen. Its much faster and much more better than t-SNE or PCA as fas as I've read.","d0897070":"#### XGBoost","224e079f":"#### The dimensions have been reduced and we can visualize the different transformed components. There is very less correlation between the transformed variables. We can see that the correlation between the components obtained from UMAP is quite les. Hence, UMAP tends to give better results. ","fe5d786a":"### So now lets go into feature engg. and removing outliers\n\n#### One of the most popular methods to remove outliers is the boxplot method which we are going to try out first.\n\nBefore we proceed we are goint to separate the \"class\" and drop the \"objid\".","d2b41bb7":"## Lets begin with Data Analysis","a9b893cb":"### Comparison of the Accuracy scores of all the algorithms","52e62fda":"#### LightGBM","08cb80df":"#### Lets do some Feature Scaling before we apply SVM and KNN, which will be the first algorithms to be applied.","c2bd62c5":"Lets check the difference in the accuracy scores","fd7333ba":"#### So we see because of the less amount of data, neural networks couldn't perform well.","b69c7dfe":"##### **Next we come to the Letter value plot. The Letter value (LV) Plot show us an estimate of the distribution of the data. It shows boxes which relate to the amount of values within the range of values inside the box.**","d41b551b":"#### RBF Kernel with GridSearchCV"}}