{"cell_type":{"ac555969":"code","aded582d":"code","6d6ec38f":"code","d9ed113f":"code","5741ce64":"code","c268858f":"code","0d1d7690":"code","9077c23a":"code","6547847b":"code","08b84c27":"code","495a0847":"code","e65f2298":"code","26abccf3":"code","cd6c363c":"code","ca0b4ebe":"code","92c6f8ec":"code","3f6a0a2b":"code","4b8c9457":"code","b6802801":"code","3b1cb2a4":"code","fb3d01d1":"code","236d9233":"code","08b0dce3":"markdown","b2eb44b7":"markdown","39f6e024":"markdown","0fb29171":"markdown"},"source":{"ac555969":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(\"hi\",os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","aded582d":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, RepeatedKFold,StratifiedKFold, train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_squared_log_error\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","6d6ec38f":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nsns.set()\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport statsmodels.api as sm\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\n# import the_module_that_warns\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom fbprophet import Prophet\n\n\n## for Deep-learing:\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD,Adadelta,Adam,RMSprop \nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nimport itertools\nfrom keras.layers import LSTM\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import Dropout","d9ed113f":"def poly(feat,_df,degree=0):\n    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n    from sklearn.preprocessing import RobustScaler, PolynomialFeatures, StandardScaler, LabelEncoder\n    from sklearn.linear_model import SGDRegressor,LinearRegression,Ridge,Lasso,ElasticNet\n\n    y = _df[_df[label_col]>0][label_col]\n    X = _df.loc[_df[label_col]>0, feat] \n    #print(X.shape,y.shape)\n   \n    # Initializatin of regression models\n    regr = LinearRegression()\n    regr = regr.fit(X, y)\n    y_lin_fit = regr.predict(X)\n    linear_r2 = r2_score(y, regr.predict(X))\n    \n    # create polynomial features\n    quadratic = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n    cubic = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n    fourth = PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)\n    fifth = PolynomialFeatures(degree=5, interaction_only=False, include_bias=False)\n    X_quad = quadratic.fit_transform(X)\n    X_cubic = cubic.fit_transform(X)\n    X_fourth = fourth.fit_transform(X)\n    X_fifth = fifth.fit_transform(X)\n\n    if degree==3:\n        res = X_cubic\n        Degreedpoly=cubic\n        target_feature_names = [feat.replace(' ','_') for feat in Degreedpoly.get_feature_names(X.columns)]\n        output_df = pd.DataFrame(res, columns = target_feature_names,  index=X.index).iloc[:, len(X.columns):]\n        return output_df\n    # quadratic fit\n    regr = regr.fit(X_quad, y)\n    y_quad_fit = regr.predict(quadratic.fit_transform(X))\n    quadratic_r2 = r2_score(y, y_quad_fit)\n    \n    # cubic fit\n    regr = regr.fit(X_cubic, y)\n    y_cubic_fit = regr.predict(cubic.fit_transform(X))\n    cubic_r2 = r2_score(y, y_cubic_fit)\n\n    # Fourth fit\n    regr = regr.fit(X_fourth, y)\n    y_fourth_fit = regr.predict(fourth.fit_transform(X))\n    four_r2 = r2_score(y, y_fourth_fit)\n\n    # Fifth fit\n    regr = regr.fit(X_fifth, y)\n    y_fifth_fit = regr.predict(fifth.fit_transform(X))\n    five_r2 = r2_score(y, y_fifth_fit)\n    \n\n    \n    if len(feat)==1:\n        fig = plt.figure(figsize=(30,10))\n        # Plot lowest Polynomials\n        fig1 = fig.add_subplot(121)\n        plt.scatter(X[feat], y, label='training points', color='lightgray')\n        plt.plot(X[feat], y_lin_fit, label='linear (d=1), $R^2=%.3f$' % linear_r2, color='blue', lw=0.5, linestyle=':')\n        plt.plot(X[feat], y_quad_fit, label='quadratic (d=2), $R^2=%.3f$' % quadratic_r2, color='red', lw=0.5, linestyle='-')\n        plt.plot(X[feat], y_cubic_fit, label='cubic (d=3), $R^2=%.3f$' % cubic_r2,  color='green', lw=0.5, linestyle='--')\n\n        plt.xlabel(feat)\n        plt.ylabel('Sale Price')\n        plt.legend(loc='upper left')\n\n        # Plot higest Polynomials\n        fig2 = fig.add_subplot(122)\n        plt.scatter(X[feat], y, label='training points', color='lightgray')\n        plt.plot(X[feat], y_lin_fit, label='linear (d=1), $R^2=%.3f$' % linear_r2, color='blue', lw=2, linestyle=':')\n        plt.plot(X[feat], y_fifth_fit, label='Fifth (d=5), $R^2=%.3f$' % five_r2, color='yellow', lw=2, linestyle='-')\n        plt.plot(X[feat], y_fifth_fit, label='Fourth (d=4), $R^2=%.3f$' % four_r2, color='red', lw=2, linestyle=':')\n\n        plt.xlabel(feat)\n        plt.ylabel('Sale Price')\n        plt.legend(loc='upper left')\n    else:\n        # Plot initialisation\n        fig = plt.figure(figsize=(20,10))\n        ax = fig.add_subplot(121, projection='3d')\n        ax.scatter(X.iloc[:, 0], X.iloc[:, 1], y, s=40)\n        return\n        # make lines of the regressors:\n        plt.plot(X.iloc[:, 0], X.iloc[:, 1], y_lin_fit, label='linear (d=1), $R^2=%.3f$' % linear_r2, \n                 color='blue', lw=2, linestyle=':')\n        plt.plot(X.iloc[:, 0], X.iloc[:, 1], y_quad_fit, label='quadratic (d=2), $R^2=%.3f$' % quadratic_r2, \n                 color='red', lw=0.5, linestyle='-')\n        plt.plot(X.iloc[:, 0], X.iloc[:, 1], y_cubic_fit, label='cubic (d=3), $R^2=%.3f$' % cubic_r2, \n                 color='green', lw=0.5, linestyle='--')\n        # label the axes\n        ax.set_xlabel(X.columns[0])\n        ax.set_ylabel(X.columns[1])\n        ax.set_zlabel('Sales Price')\n        ax.set_title(\"Poly up to 3 degree\")\n        plt.legend(loc='upper left')\n\n    plt.tight_layout()\n    plt.show()\n","5741ce64":"def evaluateColumn_Polynomial(_feature,_label_col,_df,_degree=0,_numCol=3):\n    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n    from sklearn.preprocessing import RobustScaler, PolynomialFeatures, StandardScaler, LabelEncoder\n    from sklearn.linear_model import SGDRegressor,LinearRegression,Ridge,Lasso,ElasticNet\n   \n    # Initializatin of regression models\n    regr = lgb1#LinearRegression()\n    def extractData(feat):\n        #print(\"ddd\")\n        y = _df[_df[_label_col]>0][_label_col]\n        X = _df.loc[_df[_label_col]>0, [feat]]\n        return (X,y)\n    \n    def getFeatureR2(_model,feat):\n        X,y=extractData(feat)\n        _model = _model.fit(X, y)\n        y_lin_fit = _model.predict(X)\n        #print(r2_score(y, y_lin_fit))\n        return r2_score(y, y_lin_fit)\n\n\n    def getPolyDF(_model,_degree,_feat):\n        X,y=extractData(_feat)\n        _polyType = PolynomialFeatures(degree=_degree, interaction_only=False, include_bias=False)\n        _fittedPoly = _polyType.fit_transform(_df[[feat]])\n        target_feature_names = [feature.replace(' ','_') for feature in _polyType.get_feature_names(feat)]\n        output_df = pd.DataFrame(_fittedPoly, columns = target_feature_names,  index=_df.index).iloc[:,0:]\n        \n        _fitted_model = _model.fit(output_df, y)\n        _polyPrediction = _model.predict(output_df)\n        poly_r2 = r2_score(y, _polyPrediction)\n        linear_r2 =getFeatureR2(regr,feat).round(4)\n        return ( linear_r2.round(4),poly_r2.round(4),output_df.shape[1])\n    _cols=['colname','col_R2_val1','col_poly_R2_val2','diff(val2,val1)','colCount']\n    FeaturePoly_r2DF=pd.DataFrame(columns=_cols)\n\n    for feat in _feature:\n            r2VAL1,r2VAL2,cnt= getPolyDF(regr,_degree,feat)\n            #print(feat,r2VAL1,r2VAL2,r2VAL2-r2VAL1)\n            _ser1={'colname':feat,'col_R2_val1':r2VAL1,'col_poly_R2_val2':r2VAL2,'diff(val2,val1)':r2VAL2-r2VAL1,'colCount':cnt}\n            FeaturePoly_r2DF=FeaturePoly_r2DF.append(_ser1,ignore_index=True)\n\n    return FeaturePoly_r2DF.sort_values('diff(val2,val1)',ascending=False)\n","c268858f":"train = pd.read_csv('\/kaggle\/input\/train_0irEZ2H.csv')\ntest = pd.read_csv('\/kaggle\/input\/test_nfaJ3J5.csv')\nsub = pd.read_csv('\/kaggle\/input\/sample_submission_pzljTaX.csv')\nlabel_col='units_sold'\ntrain.shape,test.shape","0d1d7690":"train['total_price'].fillna(train['total_price'].median(),inplace=True)\npd.DataFrame({'trainnull': train.isna().sum() ,'testnull':test.isna().sum(),'type':train.dtypes})","9077c23a":"# polydf = poly(['store_id', 'sku_id', 'total_price', 'base_price', 'month', 'day',\n#        'year', 'quarter', 'week_of_year', 'profit_margin',\n#        'sku_id_raking_by_unitsold', 'store_id_ranking_by_unitsold'],train,3)\n","6547847b":"def split_data(train_data,test_data):\n    #week_of_year_formated=pd.to_datetime(train['week'])#.week_of_year\n    #train_data = train_data.drop([\"sku_id\",\"store_id\"],1)\n    #test_data = test_data.drop([\"sku_id\",\"store_id\"],1)\n    train_data['week'] = pd.to_datetime(train_data['week'])\n    test_data['week'] = pd.to_datetime(test_data['week'])\n    \n    # https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/series.html#datetime-properties\n    train_data['month'] = train_data['week'].dt.month\n    train_data['day'] = train_data['week'].dt.dayofweek\n    train_data['year'] = train_data['week'].dt.year\n    train_data['quarter'] = train_data['week'].dt.quarter\n    train_data['week_of_year'] = train_data['week'].dt.weekofyear\n    train_data['profit_margin'] =  train_data['base_price'] - train_data['total_price']\n    train_data['relative_diff_base'] = train_data['profit_margin']\/train_data['base_price']\n    train_data['relative_diff_total'] = train_data['profit_margin']\/train_data['total_price']\n    sku_id_ranking=train.groupby('sku_id')[label_col].mean().sort_values()#.reset_index().reset_index()[['sku_id','index']]\n    train_data['sku_id_raking_by_unitsold'] = train_data['sku_id'].apply(lambda x:sku_id_ranking[x])\n    store_id_ranking=train.groupby('store_id')[label_col].mean().sort_values()\n    train_data['store_id_ranking_by_unitsold'] = train_data['store_id'].apply(lambda x:store_id_ranking[x])\n    \n    sku_id_ranking_base=train.groupby('sku_id')['base_price'].mean().sort_values()#.reset_index().reset_index()[['sku_id','index']]\n    train_data['sku_id_raking_by_base_price'] = train_data['sku_id'].apply(lambda x:sku_id_ranking_base[x])\n    store_id_ranking_base=train.groupby('store_id')['base_price'].mean().sort_values()\n    train_data['store_id_ranking_by_base_price'] = train_data['store_id'].apply(lambda x:store_id_ranking_base[x])\n    \n    \n    #ss=train_data.groupby(['store_id','sku_id'])['base_price'].mean().sort_values()\n    #ss=ss.to_dict()\n    #train_data['store_id_sku_id_ranking_by_base_price']= train_data[['store_id','sku_id']].apply(lambda y: ss[(y.store_id,y.sku_id)],1)\n    \n    #tot=train_data.groupby(['store_id','sku_id'])['total_price'].mean().sort_values()\n    #tot=tot.to_dict()\n    #train_data['store_id_sku_id_ranking_by_total_price']= train_data[['store_id','sku_id']].apply(lambda y: tot[(y.store_id,y.sku_id)],1)\n    \n    st= train_data[['store_id','sku_id']].drop_duplicates().groupby(['store_id']).count()\n    train_data['#sku_id_perStore_id']  =   train_data['store_id'].apply(lambda y: st.loc[y].values[0] )\n    sk= train[['store_id','sku_id']].drop_duplicates().groupby(['sku_id']).count()\n    train_data['#store_id_perSku_id']  =  train_data['sku_id'].apply(lambda y: sk.loc[y].values[0] )\n\n    test_data['month'] = test_data['week'].dt.month\n    test_data['day'] = test_data['week'].dt.dayofweek\n    test_data['year'] = test_data['week'].dt.year\n    test_data['quarter'] = test_data['week'].dt.quarter\n    test_data['week_of_year'] = test_data['week'].dt.weekofyear\n    test_data['profit_margin'] =  test_data['base_price'] - test_data['total_price']\n    test_data['relative_diff_base'] = test_data['profit_margin']\/test_data['base_price']\n    test_data['relative_diff_total'] = test_data['profit_margin']\/test_data['total_price']\n    test_data['sku_id_raking_by_unitsold'] = test_data['sku_id'].apply(lambda x:sku_id_ranking[x])\n    test_data['store_id_ranking_by_unitsold'] = test_data['store_id'].apply(lambda x:store_id_ranking[x])\n    #test_data['store_id_sku_id_ranking_by_base_price']= test_data[['store_id','sku_id']].apply(lambda y: ss[(y.store_id,y.sku_id)],1)\n    #test_data['store_id_sku_id_ranking_by_total_price']= test_data[['store_id','sku_id']].apply(lambda y: tot[(y.store_id,y.sku_id)],1)\n    \n    test_data['sku_id_raking_by_base_price'] = test_data['sku_id'].apply(lambda x:sku_id_ranking_base[x])\n    test_data['store_id_ranking_by_base_price'] = test_data['store_id'].apply(lambda x:store_id_ranking_base[x])\n    \n    \n    test_data['#sku_id_perStore_id']  =   test_data['store_id'].apply(lambda y: st.loc[y].values[0] )\n    test_data['#store_id_perSku_id']  =  test_data['sku_id'].apply(lambda y: sk.loc[y].values[0] )\n\n    col = [i for i in test_data.columns if i not in ['week','record_ID']]\n    y = train_data[label_col]\n    #train_x, test_x, train_y, test_y = train_test_split(train_data[col],train_data[y], test_size=0.2, random_state=2018)\n    return (train_data[col], y,test_data[col])\n\ntrain_x,  train_y,test_data = split_data(train,test)\ntrain_x.shape, train_y.shape, test_data.shape","08b84c27":"#ss=train.groupby(['store_id','sku_id'])['base_price'].mean()","495a0847":"#sku_id_ranking=\n#train.groupby(['week'])['week'].max()\n#.sort_values()#.reset_index().reset_index()[['sku_id','index']]\n#test['sku_id'].apply(lambda x:sku_id_ranking[x])\n","e65f2298":"# Best param Evaulated\nlgb1= lgb.LGBMRegressor(**{'bagging_fraction': 0.73,'n_estimators':3500,\n 'boosting': 'gbdt',\n 'feature_fraction': 0.67,\n 'lambda_l1': 0.01416357346505337,\n 'lambda_l2': 2.5960957064519636,\n 'learning_rate': 0.180623531498291,\n 'max_bin': 241,\n 'max_depth': 20,\n 'metric': 'MAE',\n 'min_data_in_bin': 131,\n 'min_data_in_leaf': 108,\n 'min_gain_to_split': 0.13,\n 'num_leaves': 1512,\n 'objective': 'gamma',\n 'subsample': 0.580232207898679})\nregressionCrossVal(lgb1,train_x,train_y)\n\n# pred=lgb1.predict(test_data)\n# pred.shape,sub.shape\n# sub['units_sold']=pred\n# sub.to_csv(\"AV_WEEKLY_SALES.csv\",index=False)","26abccf3":"# Prediction make \npred=lgb1.predict(test_data)\npred.shape,sub.shape\nsub['units_sold']=pred\nsub.to_csv(\"AV_WEEKLY_SALES.csv\",index=False)","cd6c363c":"#     params = {\n#         'nthread': 10,\n#          'max_depth': 5,\n#         'task': 'train',\n#         'boosting_type': 'gbdt',\n#         'objective': 'regression_l1',\n#         'metric': 'mape', # this is abs(a-e)\/max(1,a)\n\n#         'num_leaves': 64,\n#         'learning_rate': 0.2,\n#        'feature_fraction': 0.9,\n#        'bagging_fraction': 0.8,\n#         'bagging_freq': 5,\n#         'lambda_l1': 3.097758978478437,\n#         'lambda_l2': 2.9482537987198496,\n#         'verbose': 1,\n#         'min_child_weight': 6.996211413900573,\n#         'min_split_gain': 0.037310344962162616,\n#         }\n    \n\n","ca0b4ebe":"\ndef regressionCrossVal(_model,_X,_Y,stop=5):\n    fold = KFold(n_splits=5,  random_state=2020) # for Regression problem\n    # fold = StratifiedKFold(n_splits=5,  random_state=2020) # for Classification\n\n    i = 1\n    test_res=pd.DataFrame()\n    cv_score_train=[]\n    cv_score_val=[]\n    for train_index, test_index in fold.split(_X, _Y):\n        x_train, x_val = _X.iloc[train_index], _X.iloc[test_index]\n        y_train, y_val = _Y.iloc[train_index], _Y.iloc[test_index]\n        # x_train.shape,x_val.shape,y_train.shape,y_val.shape\n        \n        _model.fit(x_train,y_train)\n        y_pred = _model.predict(x_train)\n        train_rmse= np.sqrt(mean_squared_log_error(y_train,y_pred))\n        \n        y_pred_val = _model.predict(x_val)\n        val_rmse = np.sqrt(mean_squared_log_error(y_val,y_pred_val))\n        print(\"iter\", i ,\"train_MSE = \", train_rmse*100,\"validation_MSE = \", val_rmse*100)\n        cv_score_train.append(train_rmse)\n        cv_score_val.append(val_rmse)\n        if i == stop:\n            break\n        i = i + 1\n","92c6f8ec":"\nparam_grid = {\n    'num_leaves': list(range(8, 92, 4)),\n    'min_data_in_leaf': [10, 20, 40, 60, 100],\n    'max_depth': [3, 4, 5, 6, 8, 12, 16, -1],\n    'learning_rate': [0.1, 0.05, 0.01, 0.005],\n    'bagging_freq': [3, 4, 5, 6, 7],\n    'bagging_fraction': np.linspace(0.6, 0.95, 10),\n    'reg_alpha': np.linspace(0.1, 0.95, 10),\n    'reg_lambda': np.linspace(0.1, 0.95, 10)\n}\n#regressionCrossVal(lgb1,train_x,train_y)\nfixed_params = {\n    'objective': 'huber',\n    'boosting': 'gbdt',\n    'verbosity': -1,\n    'random_seed': 19,\n    'n_estimators': 50000,\n    'metric': 'mae',\n    'bagging_seed': 11\n}","3f6a0a2b":"#GLOBAL HYPEROPT PARAMETERS\nNUM_EVALS = 1000 #number of hyperopt evaluation rounds\nN_FOLDS = 5 #number of cross-validation folds on data in each evaluation round\n\n#LIGHTGBM PARAMETERS\nLGBM_MAX_LEAVES = 2**11 #maximum number of leaves per tree for LightGBM\nLGBM_MAX_DEPTH = 25 #maximum tree depth for LightGBM\nEVAL_METRIC_LGBM_REG = 'mae' #LightGBM regression metric. Note that 'rmse' is more commonly used \nEVAL_METRIC_LGBM_CLASS = 'auc'#LightGBM classification metric\n\n#XGBOOST PARAMETERS\nXGB_MAX_LEAVES = 2**12 #maximum number of leaves when using histogram splitting\nXGB_MAX_DEPTH = 25 #maximum tree depth for XGBoost\nEVAL_METRIC_XGB_REG = 'mae' #XGBoost regression metric\nEVAL_METRIC_XGB_CLASS = 'auc' #XGBoost classification metric\n\n#CATBOOST PARAMETERS\nCB_MAX_DEPTH = 8 #maximum tree depth in CatBoost\nOBJECTIVE_CB_REG = 'MAE' #CatBoost regression metric\nOBJECTIVE_CB_CLASS = 'Logloss' #CatBoost classification metric\n\n#OPTIONAL OUTPUT\nBEST_SCORE = 0\n\ndef quick_hyperopt(data, labels, package='lgbm', num_evals=NUM_EVALS, diagnostic=False):\n    \n    #==========\n    #LightGBM\n    #==========\n    \n    if package=='lgbm':\n        \n        print('Running {} rounds of LightGBM parameter optimisation:'.format(num_evals))\n        #clear space\n        gc.collect()\n        \n        integer_params = ['max_depth',\n                         'num_leaves',\n                          'max_bin',\n                         'min_data_in_leaf',\n                         'min_data_in_bin']\n        \n        def objective(space_params):\n            \n            #cast integer params from float to int\n            for param in integer_params:\n                space_params[param] = int(space_params[param])\n            \n            #extract nested conditional parameters\n            if space_params['boosting']['boosting'] == 'goss':\n                top_rate = space_params['boosting'].get('top_rate')\n                other_rate = space_params['boosting'].get('other_rate')\n                #0 <= top_rate + other_rate <= 1\n                top_rate = max(top_rate, 0)\n                top_rate = min(top_rate, 0.5)\n                other_rate = max(other_rate, 0)\n                other_rate = min(other_rate, 0.5)\n                space_params['top_rate'] = top_rate\n                space_params['other_rate'] = other_rate\n            \n            subsample = space_params['boosting'].get('subsample', 1.0)\n            space_params['boosting'] = space_params['boosting']['boosting']\n            space_params['subsample'] = subsample\n            \n            #for classification, set stratified=True and metrics=EVAL_METRIC_LGBM_CLASS\n            cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=False,\n                                early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_REG, seed=42)\n            \n            best_loss = cv_results['l1-mean'][-1] #'l2-mean' for rmse\n            #for classification, comment out the line above and uncomment the line below:\n            #best_loss = 1 - cv_results['auc-mean'][-1]\n            #if necessary, replace 'auc-mean' with '[your-preferred-metric]-mean'\n            return{'loss':best_loss, 'status': STATUS_OK }\n        \n        train = lgb.Dataset(data, labels)\n                \n        #integer and string parameters, used with hp.choice()\n        boosting_list = [{'boosting': 'gbdt',\n                          'subsample': hp.uniform('subsample', 0.5, 1)},\n                         \n                         {'boosting': 'goss',\n                          'subsample': 1.0,\n                         'top_rate': hp.uniform('top_rate', 0, 0.5),\n                         'other_rate': hp.uniform('other_rate', 0, 0.5)}] #if including 'dart', make sure to set 'n_estimators'\n        \n        metric_list = ['MAE', 'RMSE'] \n        \n        #for classification comment out the line above and uncomment the line below\n        #metric_list = ['auc'] #modify as required for other classification metrics\n        objective_list_reg = ['huber', 'gamma', 'fair', 'tweedie']\n        objective_list_class = ['binary', 'cross_entropy']\n        #for classification set objective_list = objective_list_class\n        objective_list = objective_list_reg\n\n        space ={'boosting' : hp.choice('boosting', boosting_list),\n                'num_leaves' : hp.quniform('num_leaves', 2, LGBM_MAX_LEAVES, 1),\n                'max_depth': hp.quniform('max_depth', 2, LGBM_MAX_DEPTH, 1),\n                'max_bin': hp.quniform('max_bin', 32, 255, 1),\n                'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 256, 1),\n                'min_data_in_bin': hp.quniform('min_data_in_bin', 1, 256, 1),\n                'min_gain_to_split' : hp.quniform('min_gain_to_split', 0.1, 5, 0.01),\n                'lambda_l1' : hp.uniform('lambda_l1', 0, 5),\n                'lambda_l2' : hp.uniform('lambda_l2', 0, 5),\n                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n                'metric' : hp.choice('metric', metric_list),\n                'objective' : hp.choice('objective', objective_list),\n                'feature_fraction' : hp.quniform('feature_fraction', 0.5, 1, 0.01),\n                'bagging_fraction' : hp.quniform('bagging_fraction', 0.5, 1, 0.01)\n            }\n        \n        #optional: activate GPU for LightGBM\n        #follow compilation steps here:\n        #https:\/\/www.kaggle.com\/vinhnguyen\/gpu-acceleration-for-lightgbm\/\n        #then uncomment lines below:\n        #space['device'] = 'gpu'\n        #space['gpu_platform_id'] = 0,\n        #space['gpu_device_id'] =  0\n\n        trials = Trials()\n        best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=num_evals, \n                    trials=trials)\n                \n        #fmin() will return the index of values chosen from the lists\/arrays in 'space'\n        #to obtain actual values, index values are used to subset the original lists\/arrays\n        best['boosting'] = boosting_list[best['boosting']]['boosting']#nested dict, index twice\n        best['metric'] = metric_list[best['metric']]\n        best['objective'] = objective_list[best['objective']]\n                \n        #cast floats of integer params to int\n        for param in integer_params:\n            best[param] = int(best[param])\n        \n        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n        if diagnostic:\n            return(best, trials)\n        else:\n            return(best)\n    \n    #==========\n    #XGBoost\n    #==========\n    \n    if package=='xgb':\n        \n        print('Running {} rounds of XGBoost parameter optimisation:'.format(num_evals))\n        #clear space\n        gc.collect()\n        \n        integer_params = ['max_depth']\n        \n        def objective(space_params):\n            \n            for param in integer_params:\n                space_params[param] = int(space_params[param])\n                \n            #extract multiple nested tree_method conditional parameters\n            #libera te tutemet ex inferis\n            if space_params['tree_method']['tree_method'] == 'hist':\n                max_bin = space_params['tree_method'].get('max_bin')\n                space_params['max_bin'] = int(max_bin)\n                if space_params['tree_method']['grow_policy']['grow_policy']['grow_policy'] == 'depthwise':\n                    grow_policy = space_params['tree_method'].get('grow_policy').get('grow_policy').get('grow_policy')\n                    space_params['grow_policy'] = grow_policy\n                    space_params['tree_method'] = 'hist'\n                else:\n                    max_leaves = space_params['tree_method']['grow_policy']['grow_policy'].get('max_leaves')\n                    space_params['grow_policy'] = 'lossguide'\n                    space_params['max_leaves'] = int(max_leaves)\n                    space_params['tree_method'] = 'hist'\n            else:\n                space_params['tree_method'] = space_params['tree_method'].get('tree_method')\n                \n            #for classification replace EVAL_METRIC_XGB_REG with EVAL_METRIC_XGB_CLASS\n            cv_results = xgb.cv(space_params, train, nfold=N_FOLDS, metrics=[EVAL_METRIC_XGB_REG],\n                             early_stopping_rounds=100, stratified=False, seed=42)\n            \n            best_loss = cv_results['test-mae-mean'].iloc[-1] #or 'test-rmse-mean' if using RMSE\n            #for classification, comment out the line above and uncomment the line below:\n            #best_loss = 1 - cv_results['test-auc-mean'].iloc[-1]\n            #if necessary, replace 'test-auc-mean' with 'test-[your-preferred-metric]-mean'\n            return{'loss':best_loss, 'status': STATUS_OK }\n        \n        train = xgb.DMatrix(data, labels)\n        \n        #integer and string parameters, used with hp.choice()\n        boosting_list = ['gbtree', 'gblinear'] #if including 'dart', make sure to set 'n_estimators'\n        metric_list = ['MAE', 'RMSE'] \n        #for classification comment out the line above and uncomment the line below\n        #metric_list = ['auc']\n        #modify as required for other classification metrics classification\n        \n        tree_method = [{'tree_method' : 'exact'},\n               {'tree_method' : 'approx'},\n               {'tree_method' : 'hist',\n                'max_bin': hp.quniform('max_bin', 2**3, 2**7, 1),\n                'grow_policy' : {'grow_policy': {'grow_policy':'depthwise'},\n                                'grow_policy' : {'grow_policy':'lossguide',\n                                                  'max_leaves': hp.quniform('max_leaves', 32, XGB_MAX_LEAVES, 1)}}}]\n        \n        #if using GPU, replace 'exact' with 'gpu_exact' and 'hist' with\n        #'gpu_hist' in the nested dictionary above\n        \n        objective_list_reg = ['reg:linear', 'reg:gamma', 'reg:tweedie']\n        objective_list_class = ['reg:logistic', 'binary:logistic']\n        #for classification change line below to 'objective_list = objective_list_class'\n        objective_list = objective_list_reg\n        \n        space ={'boosting' : hp.choice('boosting', boosting_list),\n                'tree_method' : hp.choice('tree_method', tree_method),\n                'max_depth': hp.quniform('max_depth', 2, XGB_MAX_DEPTH, 1),\n                'reg_alpha' : hp.uniform('reg_alpha', 0, 5),\n                'reg_lambda' : hp.uniform('reg_lambda', 0, 5),\n                'min_child_weight' : hp.uniform('min_child_weight', 0, 5),\n                'gamma' : hp.uniform('gamma', 0, 5),\n                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n                'eval_metric' : hp.choice('eval_metric', metric_list),\n                'objective' : hp.choice('objective', objective_list),\n                'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1, 0.01),\n                'colsample_bynode' : hp.quniform('colsample_bynode', 0.1, 1, 0.01),\n                'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),\n                'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n                'nthread' : -1\n            }\n        \n        trials = Trials()\n        best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=num_evals, \n                    trials=trials)\n        \n        best['tree_method'] = tree_method[best['tree_method']]['tree_method']\n        best['boosting'] = boosting_list[best['boosting']]\n        best['eval_metric'] = metric_list[best['eval_metric']]\n        best['objective'] = objective_list[best['objective']]\n        \n        #cast floats of integer params to int\n        for param in integer_params:\n            best[param] = int(best[param])\n        if 'max_leaves' in best:\n            best['max_leaves'] = int(best['max_leaves'])\n        if 'max_bin' in best:\n            best['max_bin'] = int(best['max_bin'])\n        \n        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n        \n        if diagnostic:\n            return(best, trials)\n        else:\n            return(best)\n    \n    #==========\n    #CatBoost\n    #==========\n    \n    if package=='cb':\n        \n        print('Running {} rounds of CatBoost parameter optimisation:'.format(num_evals))\n        \n        #clear memory \n        gc.collect()\n            \n        integer_params = ['depth',\n                          #'one_hot_max_size', #for categorical data\n                          'min_data_in_leaf',\n                          'max_bin']\n        \n        def objective(space_params):\n                        \n            #cast integer params from float to int\n            for param in integer_params:\n                space_params[param] = int(space_params[param])\n                \n            #extract nested conditional parameters\n            if space_params['bootstrap_type']['bootstrap_type'] == 'Bayesian':\n                bagging_temp = space_params['bootstrap_type'].get('bagging_temperature')\n                space_params['bagging_temperature'] = bagging_temp\n                \n            if space_params['grow_policy']['grow_policy'] == 'LossGuide':\n                max_leaves = space_params['grow_policy'].get('max_leaves')\n                space_params['max_leaves'] = int(max_leaves)\n                \n            space_params['bootstrap_type'] = space_params['bootstrap_type']['bootstrap_type']\n            space_params['grow_policy'] = space_params['grow_policy']['grow_policy']\n                           \n            #random_strength cannot be < 0\n            space_params['random_strength'] = max(space_params['random_strength'], 0)\n            #fold_len_multiplier cannot be < 1\n            space_params['fold_len_multiplier'] = max(space_params['fold_len_multiplier'], 1)\n                       \n            #for classification set stratified=True\n            cv_results = cb.cv(train, space_params, fold_count=N_FOLDS, \n                             early_stopping_rounds=25, stratified=False, partition_random_seed=42)\n           \n            best_loss = cv_results['test-MAE-mean'].iloc[-1] #'test-RMSE-mean' for RMSE\n            #for classification, comment out the line above and uncomment the line below:\n            #best_loss = cv_results['test-Logloss-mean'].iloc[-1]\n            #if necessary, replace 'test-Logloss-mean' with 'test-[your-preferred-metric]-mean'\n            \n            return{'loss':best_loss, 'status': STATUS_OK}\n        \n        train = cb.Pool(data, labels.astype('float32'))\n        \n        #integer and string parameters, used with hp.choice()\n        bootstrap_type = [{'bootstrap_type':'Poisson'}, \n                           {'bootstrap_type':'Bayesian',\n                            'bagging_temperature' : hp.loguniform('bagging_temperature', np.log(1), np.log(50))},\n                          {'bootstrap_type':'Bernoulli'}] \n        LEB = ['No', 'AnyImprovement', 'Armijo'] #remove 'Armijo' if not using GPU\n        #score_function = ['Correlation', 'L2', 'NewtonCorrelation', 'NewtonL2']\n        grow_policy = [{'grow_policy':'SymmetricTree'},\n                       {'grow_policy':'Depthwise'},\n                       {'grow_policy':'Lossguide',\n                        'max_leaves': hp.quniform('max_leaves', 2, 32, 1)}]\n        eval_metric_list_reg = ['MAE', 'RMSE', 'Poisson']\n        eval_metric_list_class = ['Logloss', 'AUC', 'F1']\n        #for classification change line below to 'eval_metric_list = eval_metric_list_class'\n        eval_metric_list = eval_metric_list_reg\n                \n        space ={'depth': hp.quniform('depth', 2, CB_MAX_DEPTH, 1),\n                'max_bin' : hp.quniform('max_bin', 1, 32, 1), #if using CPU just set this to 254\n                'l2_leaf_reg' : hp.uniform('l2_leaf_reg', 0, 5),\n                'min_data_in_leaf' : hp.quniform('min_data_in_leaf', 1, 50, 1),\n                'random_strength' : hp.loguniform('random_strength', np.log(0.005), np.log(5)),\n                #'one_hot_max_size' : hp.quniform('one_hot_max_size', 2, 16, 1), #uncomment if using categorical features\n                'bootstrap_type' : hp.choice('bootstrap_type', bootstrap_type),\n                'learning_rate' : hp.uniform('learning_rate', 0.05, 0.25),\n                'eval_metric' : hp.choice('eval_metric', eval_metric_list),\n                'objective' : OBJECTIVE_CB_REG,\n                #'score_function' : hp.choice('score_function', score_function), #crashes kernel - reason unknown\n                'leaf_estimation_backtracking' : hp.choice('leaf_estimation_backtracking', LEB),\n                'grow_policy': hp.choice('grow_policy', grow_policy),\n                #'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),# CPU only\n                'fold_len_multiplier' : hp.loguniform('fold_len_multiplier', np.log(1.01), np.log(2.5)),\n                'od_type' : 'Iter',\n                'od_wait' : 25,\n                'task_type' : 'GPU',\n                'verbose' : 0\n            }\n        \n        #optional: run CatBoost without GPU\n        #uncomment line below\n        #space['task_type'] = 'CPU'\n            \n        trials = Trials()\n        best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=num_evals, \n                    trials=trials)\n        \n        #unpack nested dicts first\n        best['bootstrap_type'] = bootstrap_type[best['bootstrap_type']]['bootstrap_type']\n        best['grow_policy'] = grow_policy[best['grow_policy']]['grow_policy']\n        best['eval_metric'] = eval_metric_list[best['eval_metric']]\n        \n        #best['score_function'] = score_function[best['score_function']] \n        #best['leaf_estimation_method'] = LEM[best['leaf_estimation_method']] #CPU only\n        best['leaf_estimation_backtracking'] = LEB[best['leaf_estimation_backtracking']]        \n        \n        #cast floats of integer params to int\n        for param in integer_params:\n            best[param] = int(best[param])\n        if 'max_leaves' in best:\n            best['max_leaves'] = int(best['max_leaves'])\n        \n        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n        \n        if diagnostic:\n            return(best, trials)\n        else:\n            return(best)\n    \n    else:\n        print('Package not recognised. Please use \"lgbm\" for LightGBM, \"xgb\" for XGBoost or \"cb\" for CatBoost.')  ","4b8c9457":"import gc\nfrom hyperopt import hp, tpe, Trials, STATUS_OK\nfrom hyperopt.fmin import fmin\nlgbm_params = quick_hyperopt(train_x, train_y, 'lgbm', 50)","b6802801":"poly_col=['profit_margin^2']#,'year_quarter_profit_margin']#,'year_week_of_year_profit_margin']#,'month_year_profit_margin']#]\npolydf_train = poly(['store_id', 'sku_id', 'total_price', 'base_price', 'month', 'day',\n       'year', 'quarter', 'week_of_year', 'profit_margin',\n       'sku_id_raking_by_unitsold', 'store_id_ranking_by_unitsold'],train,3)\npolydf_train[poly_col]\ntrain_x_final= pd.concat([train_x,polydf_train[poly_col]],1)","3b1cb2a4":"test_temp = test_data.copy()\ntest_temp[label_col]=pred","fb3d01d1":"polydf_test = poly(['store_id', 'sku_id', 'total_price', 'base_price', 'month', 'day',\n       'year', 'quarter', 'week_of_year', 'profit_margin',\n       'sku_id_raking_by_unitsold', 'store_id_ranking_by_unitsold'],test_temp,3)\npolydf_test[poly_col]\ntest_data_final= pd.concat([test_data,polydf_test[poly_col]],1)","236d9233":"lgb1= lgb.LGBMRegressor(**{'bagging_fraction': 0.73,\n 'boosting': 'gbdt',\n 'feature_fraction': 0.67,\n 'lambda_l1': 0.01416357346505337,\n 'lambda_l2': 2.5960957064519636,\n 'learning_rate': 0.180623531498291,\n 'max_bin': 241,\n 'max_depth': 20,\n 'metric': 'MAE',\n 'min_data_in_bin': 131,\n 'min_data_in_leaf': 108,\n 'min_gain_to_split': 0.13,\n 'num_leaves': 1512,\n 'objective': 'gamma',\n 'subsample': 0.580232207898679})\nregressionCrossVal(lgb1,train_x_final,train_y)","08b0dce3":"Other Similar Approach\n\nhttps:\/\/github.com\/AnilBetta\/AV-Janatahack-DemandForecasting\/blob\/master\/lgbm.ipynb\n\nhttps:\/\/www.kaggle.com\/piyushrg\/av-demand-forecasting\n\nhttps:\/\/github.com\/aashu0706\/AV-JanataHack-Demand-Forecasting\n\nhttps:\/\/github.com\/saikrithik\/JanataHack-Demand-Forecasting","b2eb44b7":"# Above Code is ok to submit\n# Below is just experimentation","39f6e024":"More Datetime transformation\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/series.html#datetime-properties","0fb29171":"# About DataSet ( Dataset is taken from Analytical Vidhya)\nIts is regression progression problem and solved using light gbm\n\n## Live competition -> Rank achieved= 94\n\nOne of the largest retail chains in the world wants to use their vast data source to build an efficient forecasting model to predict the sales for each SKU in its portfolio at its 76 different stores using historical sales data for the past 3 years on a week-on-week basis. Sales and promotional information is also available for each week - product and store wise. \n\nHowever, no other information regarding stores and products are available. Can you still forecast accurately the sales values for every such product\/SKU-store combination for the next 12 weeks accurately? If yes, then dive right in!\n\nFor more Detail over competetion\nhttps:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-demand-forecasting\/?utm_source=sendinblue&utm_campaign=Now_LIVE_JanataHack__Demand_Forecasting&utm_medium=email#ProblemStatement\n"}}