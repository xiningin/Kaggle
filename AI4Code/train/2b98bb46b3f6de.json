{"cell_type":{"ec5ad1e3":"code","bcea7bde":"code","433ea17e":"code","c8462014":"code","4ca192e9":"code","3841f6b1":"code","a9fa837f":"code","4f504b98":"code","64fc406b":"code","56cf7412":"code","9980025a":"code","95d079f4":"code","97eb8c39":"code","6df5aef6":"code","5a901a36":"code","d6d7991d":"code","6969edc1":"code","0810199f":"code","e6b9309a":"code","1b8efef4":"code","fd479f3a":"code","a92a236d":"code","d7d6d236":"code","a25d18f5":"code","96763b80":"code","c561541a":"code","39d874dd":"code","9cda0343":"markdown","6b6d31db":"markdown","529a0935":"markdown","23eeb2b4":"markdown","c7969a7d":"markdown","63894d45":"markdown"},"source":{"ec5ad1e3":"# Libraries\nlibrary(readr, quietly=TRUE)\nlibrary(jsonlite, quietly=TRUE)\nlibrary(tidyverse)\n\nlibrary(lubridate)\n\nlibrary(pROC, quietly=TRUE)\nlibrary(microbenchmark, quietly=TRUE)\nlibrary(gbm, quietly=TRUE)\nlibrary(xgboost, quietly=TRUE)\nlibrary(lightgbm, quietly=TRUE)\nlibrary(magrittr, quietly=TRUE)","bcea7bde":"train <- read_csv(\"..\/input\/train.csv\")\ntest <- read_csv(\"..\/input\/test.csv\")","433ea17e":"# Reference: https:\/\/www.kaggle.com\/erikbruin\/google-analytics-eda-with-screenshots-of-the-app\n# Reference: https:\/\/www.kaggle.com\/mrlong\/r-flatten-json-columns-to-make-single-data-frame\n#JSON columns are \"device\", \"geoNetwork\", \"totals\", \"trafficSource\"\n\ntr_device <- paste(\"[\", paste(train$device, collapse = \",\"), \"]\") %>% fromJSON(flatten = T)\ntr_geoNetwork <- paste(\"[\", paste(train$geoNetwork, collapse = \",\"), \"]\") %>% fromJSON(flatten = T)\ntr_totals <- paste(\"[\", paste(train$totals, collapse = \",\"), \"]\") %>% fromJSON(flatten = T)\ntr_trafficSource <- paste(\"[\", paste(train$trafficSource, collapse = \",\"), \"]\") %>% fromJSON(flatten = T)\n\nte_device <- paste(\"[\", paste(test$device, collapse = \",\"), \"]\") %>% fromJSON(flatten = T)\nte_geoNetwork <- paste(\"[\", paste(test$geoNetwork, collapse = \",\"), \"]\") %>% fromJSON(flatten = T)\nte_totals <- paste(\"[\", paste(test$totals, collapse = \",\"), \"]\") %>% fromJSON(flatten = T)\nte_trafficSource <- paste(\"[\", paste(test$trafficSource, collapse = \",\"), \"]\") %>% fromJSON(flatten = T)\n\n#Combine to make the full training and test sets\ntrain <- train %>%\n    cbind(tr_device, tr_geoNetwork, tr_totals, tr_trafficSource) %>%\n    select(-device, -geoNetwork, -totals, -trafficSource)\n    \ntest <- test %>%\n    cbind(te_device, te_geoNetwork, te_totals, te_trafficSource) %>%\n    select(-device, -geoNetwork, -totals, -trafficSource)\n\n#Number of columns in the new training and test sets. \n#ncol(train)\n#ncol(test)\n\n#Remove temporary tr_ and te_ sets\nrm(tr_device)\nrm(tr_geoNetwork)\nrm(tr_totals)\nrm(tr_trafficSource)\nrm(te_device)\nrm(te_geoNetwork)\nrm(te_totals)\nrm(te_trafficSource)","c8462014":"# Reference: https:\/\/www.kaggle.com\/erikbruin\/google-analytics-eda-with-screenshots-of-the-app\n#already converting some character variables into factors (the obvious ones)\nfactorVars <- c(\"channelGrouping\", \"socialEngagementType\", \"browser\", \"operatingSystem\", \"deviceCategory\", \"continent\", \"subContinent\", \"country\", \"source\", \"medium\", \"keyword\", \"referralPath\", \"campaign\")\ntrain[, factorVars] <- lapply(train[, factorVars], as.factor)\ntest[, factorVars] <- lapply(test[, factorVars], as.factor)\n\n#also converting the data variable to the date format\ntrain$date <- ymd(train$date)\ntest$date <- ymd(test$date)\n\n#converting character variables into numeric\nnumVars <- c(\"visits\", \"hits\", \"bounces\", \"pageviews\", \"newVisits\")\ntrain[, numVars] <- lapply(train[, numVars], as.numeric)\ntrain$transactionRevenue <- as.numeric(train$transactionRevenue)\ntest[, numVars] <- lapply(test[, numVars], as.numeric)\n\n#converting visit start times to POSIXct\ntrain$visitStartTime <- as.POSIXct(train$visitStartTime, tz=\"UTC\", origin='1970-01-01')","4ca192e9":"head(train)","3841f6b1":"colnames(train)","a9fa837f":"print('TRAIN SET')\nsprintf('Rows: %s', nrow(train))\nsprintf('Columns: %s', ncol(train))\nsprintf('Features: %s', colnames(train))\nprint('\\n')\nprint('TEST SET')\nsprintf('Rows: %s', nrow(test))\nsprintf('Columns: %s', ncol(test))\nsprintf('Features: %s', colnames(test))","4f504b98":"str(train)","64fc406b":"features <- c('hits', 'fullVisitorId', 'transactionRevenue')\ntarget <- 'transactionRevenue'","56cf7412":"# removing NaN's for now\ntrain_few_cols <- select(train, features)\ntrain_few_cols$fullVisitorId <- as.numeric(train_few_cols$fullVisitorId)\ntrain_few_cols <- train_few_cols[complete.cases(train_few_cols),]\n\ntest_few_cols <- select(test, c('hits', 'fullVisitorId'))\ntest_few_cols$fullVisitorId <- as.numeric(test_few_cols$fullVisitorId)\n#test_few_cols <- test_few_cols[complete.cases(test_few_cols),]","9980025a":"# split train into train and validation\n#applying log to revenue\ntrain_few_cols$transactionRevenue <- log(train_few_cols$transactionRevenue)\ntrain.validation.split <- sample(2\n\t, nrow(train_few_cols)\n\t, replace = TRUE\n\t, prob = c(0.7, 0.3))\ntrain2 = train_few_cols[train.validation.split == 1,]\nvalidation = train_few_cols[train.validation.split == 2,]","95d079f4":"str(train_few_cols)","97eb8c39":"# one hot encoding\n# dummies = dummyVars(~ country, city,  , data = train)\n# df_all_ohe <- as.data.frame(predict(dummies, newdata = train))\n# df_all_combined <- cbind(data[,-c(which(colnames(data) %in% ohe_feats))],df_all_ohe)","6df5aef6":"# Get the time to train the GBM model\nsystem.time(\n\tgbm.model <- gbm(transactionRevenue ~ .\n\t\t, distribution = \"gaussian\"\n\t\t, data = train2\n\t\t, n.trees = 700 # number of trees\n\t\t, interaction.depth = 3\n\t\t, n.minobsinnode = 100 # minimum number of obs needed in each node\n\t\t, shrinkage = 0.01 # learning rate\n\t\t, bag.fraction = 0.5 # subsampling\n\t\t, train.fraction = 0.5\n\t\t, cv.folds = 10      # do 10-fold cross-validation\n\t\t, verbose = FALSE  # don't print progress\n\t\t)\n)","5a901a36":"# Determine best iteration based on test data\nbest.iter = gbm.perf(gbm.model, method = \"test\")","d6d7991d":"\n# Get feature importance\ngbm.feature.imp = summary(gbm.model, n.trees = best.iter)","6969edc1":"# Plot and calculate AUC on test data\ngbm.test = predict(gbm.model, newdata = validation, n.trees = best.iter)","0810199f":"RMSE = function(m, o){\n  sqrt(mean((m - o)^2))\n}","e6b9309a":"# RSME\nprint(RMSE(validation$transactionRevenue, gbm.test))","1b8efef4":"Submission_pred = predict(gbm.model, newdata = test_few_cols, n.trees = best.iter)","fd479f3a":"summary(Submission_pred)","a92a236d":"my_submission <- data_frame('fullVisitorId' = test_few_cols$fullVisitorId, 'PredictedLogRevenue' = Submission_pred)\n","d7d6d236":"head(my_submission)","a25d18f5":"nrow(my_submission)","96763b80":"# group by id and sum revenue\nmy_submission <- my_submission %>% \n  group_by(fullVisitorId) %>% \n  summarise(PredictedLogRevenue = sum(PredictedLogRevenue))\n","c561541a":"nrow(my_submission)","39d874dd":"write_csv(my_submission, '1th_gbm_submission.csv')","9cda0343":"Function to load and convert files borrowed from this [kernel](https:\/\/www.kaggle.com\/mrlong\/r-flatten-json-columns-to-make-single-data-frame). And to convert infromation from this [kernel](https:\/\/www.kaggle.com\/erikbruin\/google-analytics-eda-with-screenshots-of-the-app)","6b6d31db":"## LGBM - Google Analytics Customer Revenue Prediction\n* Note: this is just a starting point, there's a lot of work to be done.*\n* I also have a [deep learning](https:\/\/www.kaggle.com\/dimitreoliveira\/deep-learning-keras-ga-revenue-prediction) version of this code, this one is supposed to be a comparation between the models.\n* I'm new to LGBM if you have any tip or correction please let me know.","529a0935":"### Dependencies","23eeb2b4":"### This is how our data looks like","c7969a7d":"### Auxiliar functions","63894d45":"### About the train data"}}