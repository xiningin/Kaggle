{"cell_type":{"5d8eb7a8":"code","bef9c30d":"code","5dff1ab7":"code","70682faa":"code","3f31faf2":"code","c57a39c8":"code","10563c4f":"code","b53e46b3":"code","b26585af":"code","77939e34":"markdown","d851a638":"markdown","1c5ba191":"markdown","788fedde":"markdown","20c43b02":"markdown","341802f7":"markdown","e849569b":"markdown","7a93de09":"markdown"},"source":{"5d8eb7a8":"# Install additional packages\n!pip install opfython","bef9c30d":"import numpy as np\nimport pandas as pd\n\nimport opfython.math.general as g\nfrom opfython.models.supervised import SupervisedOPF","5dff1ab7":"# Loading training and testing sets\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","70682faa":"# Extracts information regarding the availability of non-null values\n# Note that `Cabin` can be discarded away as there are roughly only 23% available data\nprint(train.info(), test.info())\n\n# Let us print some information regarding how a row of data looks like\n# Note that we can also initially discard `Name` and `Ticket` are they are string objects and might not be\n# the ideal type to employ in a na\u00efve version of a supervised classifier\nprint(train.head(), test.head())","3f31faf2":"# Further, let us gather the median of `Age` column and fills the `NaN` values\n# Note that we need to ignore its missing values or the median will not be properly calculated\ntrain_age_median = np.median(train['Age'][train['Age'].notna()])\ntrain['Age'] = train['Age'].fillna(train_age_median)\n\n# Analyzes the distribution of `Embarked` samples\n# Note that `S` is the most occurring value, thus, we will fill up the 2 remaining rows with `S`\n# print(train['Embarked'].value_counts())\ntrain['Embarked'] = train['Embarked'].fillna('S')\n\n# We also need to perform the same operation to the testing set `Age` column\n# Note that the testing set has also a missing fare value\ntest_age_median = np.median(test['Age'][test['Age'].notna()])\ntest_fare_median = np.median(test['Fare'][test['Fare'].notna()])\ntest['Age'] = test['Age'].fillna(test_age_median)\ntest['Fare'] = test['Fare'].fillna(test_fare_median)","c57a39c8":"# Defines the features that will be used\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n\n# Gathers the training samples and convert to a normalized numpy array\n# Note that OPF labels start from `1`\nx_train = g.normalize(pd.get_dummies(train[features]).to_numpy())\ny_train = train['Survived'].to_numpy() + 1\n\n# Gathers the testing samples and convert to a normalized numpy array\nx_test = g.normalize(pd.get_dummies(test[features]).to_numpy())","10563c4f":"# Defines the OPF-based model\nopf = SupervisedOPF()\n\n# Fits the model\nopf.fit(x_train, y_train)","b53e46b3":"# Predicts the unseen data\n# Remember that we need to subtract `1` due to OPF labeling\npreds = np.asarray(opf.predict(x_test)) - 1","b26585af":"# Creates the submission data and outputs to a .csv file\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': preds})\noutput.to_csv('titanic-opf_submission.csv', index=False)","77939e34":"## Final Outputting\n\nThe last step is to output the final result into a `.csv` file holding the passengers' identifiers and their survival predictions. Such a file will be ready to be submitted into the competition.","d851a638":"# Data Handling\n\n## Loading the Data\n\nThe first step should be pretty straightforward. Just fire up `Pandas` and loads the provided `.csv` files.","1c5ba191":"## Pre-Processing the Data\n\nBefore preparing the data and creating the corresponding features and labels arrays, it is important to perform an exploratory analysis over the data, pre-process it for missing values or even remove some unwanted information that might impact our training procedure.\n\nThe following colums are available: `PassengerId`, `Survied`, `Pclass`, `Name`, `Sex`, `Age`, `SibSp`, `Parch`, `Ticket`, `Fare`, `Cabin`, `Embarked`.\n\nIn this example, we will only be using simple pre-processing pipelines, such as filling the missing `Age` values with their median, as well as filling the missing `Embarked` values with the most common one.","788fedde":"# Modelling\n\n## Training an Optimum-Path Forest\n\nWith the data prepared, it is possible to instantiate a `SupervisedOPF()` class and fit the training data.\n\nThere is no additional required argument to pass to the class, yet one can pass the type of distance that is going to be used to find OPF prototypes, as follows:\n\n`opf = SupervisedOPF(distance='log_squared_euclidean')`\n\nA list of the available distance functions can be found [here](https:\/\/github.com\/gugarosa\/opfython\/blob\/master\/opfython\/math\/distance.py).","20c43b02":"## Preparing the Data\n\nWith the pre-processing ready, we can now select the features that will be used and further convert them into normalized `x` and `y` numpy arrays.\n\nNote that the further `fit()` function requires arrays of training samples and labels. Also, the labels should start their indexing with $1$ instead of $0$.","341802f7":"# Installation\n\n## Adding OPFython\n\nInitially, we are required to install any additional packages that might be used within this notebook. The [Optimum-Path Forest](https:\/\/ic.unicamp.br\/~afalcao\/mo815-grafos\/a21.pdf) Python-based implementation is available at the [OPFython](https:\/\/github.com\/gugarosa\/opfython) package, which can be installed through `pip`.","e849569b":"## Importing the Packages\n\nFollowing up, we are going to define all required imports to work within this notebook. In such case, we will be using `Pandas` to perform the data's input and output, as well as `Numpy` to perform the transiction between data frames and arrays.\n\nFinally, we will be importing the `SupervisedOPF` class from `OPFython` and its `general` mathematical package as it contains the normalization function we will be using later.","7a93de09":"## Predicting Unseen Data\n\nAfter performing the training, we can predict the unseen data. Note that the `predict()` function returns a list with labels indexed within the interval $[1, n]$, where $n$ stands for the maximum number of classes. Thus, we need to subtract $1$ to recover their original values."}}