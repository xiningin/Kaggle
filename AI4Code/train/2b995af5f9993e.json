{"cell_type":{"43f9a3c2":"code","b93134c4":"code","9eb54524":"code","6efea523":"code","716c13e2":"code","bb019e24":"code","6561eca1":"code","96997125":"code","e6bef81a":"code","f88ccdb5":"code","0a9b78e0":"code","fb3eb82d":"code","2c4bfef0":"code","270cd80a":"code","bc9ad66c":"code","932f2a56":"code","1602a4eb":"code","14a82a95":"code","5877c09e":"code","48ba10e9":"markdown","1d35d3c4":"markdown","b715889d":"markdown","2c39f29f":"markdown","d2862238":"markdown","4dd464ac":"markdown","1bd00179":"markdown","5bdc5e42":"markdown","460a4338":"markdown","dde18333":"markdown","11057d33":"markdown"},"source":{"43f9a3c2":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","b93134c4":"import numpy as np \nimport pandas as pd","9eb54524":"#READING INPUT\ndata = pd.read_csv(\"\/kaggle\/input\/spooky-author-identification\/train.csv\")\ndata.head()","6efea523":"data['author_num'] = data[\"author\"].map({'EAP':0, 'HPL':1, 'MWS':2})\ndata.head()","716c13e2":"X = data['text']\ny = data['author_num']","bb019e24":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","6561eca1":"from sklearn.feature_extraction.text import CountVectorizer","96997125":"text=[\"My name is Paul my life is Jane! And we live our life together\" , \"My name is Guido my life is Victoria! And we live our life together\"]\ntoy = CountVectorizer(stop_words = 'english')\ntoy.fit_transform(text)\nmatrix = toy.transform(text)\nfeatures = toy.get_feature_names()\ndf_res = pd.DataFrame(matrix.toarray(), columns=features)\ndf_res","e6bef81a":"vect = CountVectorizer(stop_words = 'english')","f88ccdb5":"X_train_matrix = vect.fit_transform(X_train) ","0a9b78e0":"from sklearn.naive_bayes import MultinomialNB\nclf=MultinomialNB()\nclf.fit(X_train_matrix, y_train)\nprint(clf.score(X_train_matrix, y_train))\nX_test_matrix = vect.transform(X_test) \nprint (clf.score(X_test_matrix, y_test))","fb3eb82d":"predicted_result=clf.predict(X_test_matrix)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predicted_result))","2c4bfef0":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words = 'english')\n\nX_train_tfidf = vectorizer.fit_transform(X_train) \nX_train_tfidf.shape","270cd80a":"from sklearn.naive_bayes import MultinomialNB\nclf2=MultinomialNB()\nclf2.fit(X_train_tfidf, y_train)\nprint(clf2.score(X_train_tfidf, y_train))\nX_test_tfidf = vectorizer.transform(X_test) \nprint (clf2.score(X_test_tfidf, y_test))","bc9ad66c":"predicted_result_2=clf2.predict(X_test_tfidf)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predicted_result_2))","932f2a56":"sample = pd.read_csv(\"\/kaggle\/input\/spooky-author-identification\/sample_submission.csv\")\nsample.head()","1602a4eb":"test = pd.read_csv(\"\/kaggle\/input\/spooky-author-identification\/test.csv\")\ntest_matrix = vect.transform(test[\"text\"])\npredicted_result = clf.predict_proba(test_matrix)","14a82a95":"result=pd.DataFrame()\nresult[\"id\"]=test[\"id\"]\nresult[\"EAP\"]=predicted_result[:,0]\nresult[\"HPL\"]=predicted_result[:,1]\nresult[\"MWS\"]=predicted_result[:,2]\nresult.head()","5877c09e":"result.to_csv(\"submission_v1.csv\", index=False)","48ba10e9":"## Model 1 with count vectorizer","1d35d3c4":"* there might be something to learn from the predictions on class 2","b715889d":"* example\n* below: the word \"life\" has been found 2 times in sentence 0 and in sentence 1\n* the word paul has been found 1 time in sentence 0 and 0 times in sentence 1\n* and so on...","2c39f29f":"## Split training and test data","d2862238":"# Submission","4dd464ac":"## Model 2 with TfidVectorizer","1bd00179":"#### Tf-idf: \n\n* Since longer documents will have higher average count values than shorter documents, even though they might talk about the same topics, we can divide the number of occurrences of each word in a document by the total number of words in the document: **tf** for Term Frequencies.\n\n* **idf** for \u201cTerm Frequency times Inverse Document Frequency\u201d : Downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n\n* CountVectorizer and TfidTransformer steps into one using [TfidVectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html):","5bdc5e42":"we map \"EAP\" to 0 \"HPL\" to 1 and \"MWS\" to 2 as it will be more convenient for our classifier. \nIn other words we are just telling our computer that if classifier predicts 0 for the text then it means that it is preicting \"EAP\", if 1 then it means that it is predicting \"HPL\", if 2 then it means that it is predicting \"MWS\".","460a4338":"## Define X and y","dde18333":"## Vectorisation\n\n#### Count Vectorizer: builds a dictionary of features and transforms documents to feature vectors.\n\n","11057d33":"* it doesn't perform better in term of accuracy"}}