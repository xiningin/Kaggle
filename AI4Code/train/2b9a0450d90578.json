{"cell_type":{"09fd9ddb":"code","e6fc63eb":"code","c8528596":"code","35e4c9d7":"code","2fdba066":"code","aaa96fcc":"code","e6672574":"code","4dcb328c":"code","79ab4abc":"code","cc1fbfab":"code","39338b12":"code","e382bbd0":"code","e37d5e3c":"code","d085b9a6":"code","da766216":"code","ae9cb683":"code","4ea67fab":"code","20698c89":"code","393d0484":"code","644ea045":"code","805bf735":"code","ab3de31c":"code","7c77eaec":"code","4f9ac4b6":"code","6ce54eb7":"code","e80f6234":"code","a3665d19":"code","06c3eabe":"code","5e361289":"code","db23d868":"code","d74f9c8c":"code","a497fa4d":"code","14000747":"code","b6926970":"code","35f5df5d":"code","63ebf36b":"code","8a3cc254":"code","7572054b":"code","5cd9b577":"markdown","0e0d3273":"markdown","4d68488f":"markdown","f87604bf":"markdown","17e8f7f1":"markdown","130b9776":"markdown","3f65e024":"markdown","955f9fa4":"markdown","8393e690":"markdown","74187b9d":"markdown","6bec4ed9":"markdown","746f6046":"markdown","5b1315f8":"markdown","d9a39834":"markdown","3b7303be":"markdown","1e9f50fe":"markdown","3d8bc366":"markdown","863df18e":"markdown","be681d28":"markdown","2334e46d":"markdown","d7dcdd73":"markdown","dc6dc64f":"markdown","113f44c6":"markdown","4c0ea7f4":"markdown","84777e60":"markdown","4ecae25f":"markdown","b9c73f71":"markdown","84a3433a":"markdown","61b57d89":"markdown"},"source":{"09fd9ddb":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil import parser\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom matplotlib import pyplot\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM, GRU\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Activation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\n\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import confusion_matrix as cm\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import KFold \nfrom statistics import mean\n\nfrom mlxtend.frequent_patterns import apriori, association_rules \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e6fc63eb":"# Loads data\ndataset = pd.read_csv('..\/input\/employee-attrition-data\/MFG10YearTerminationData.csv')","c8528596":"# Checks size of dataset\ndataset.shape","35e4c9d7":"# Checks 18 columns names and types \ndataset.dtypes","2fdba066":"# Checks categorical values\ncategoricalValues = dataset.drop([\"recorddate_key\", \"birthdate_key\", \"orighiredate_key\", \"terminationdate_key\"], axis=1).select_dtypes(include=['object'])\n\nfor col in categoricalValues:\n    print(\"Column Name: \" + col + \"\\nValues =>\")\n    print(categoricalValues[col].unique())\n    print(\"\\n\")","aaa96fcc":"# First 5 rows of dataset\ndataset.head(5)","e6672574":"# Checks records for Employee #1318\ndataset.loc[dataset['EmployeeID'] == 1318]","4dcb328c":"# Lists if any changes on ['city_name','department_name','job_title','BUSINESS_UNIT'] for all duplicated EmployeeIDs\nnewDataset = dataset[['EmployeeID', 'city_name', 'department_name', 'job_title', 'BUSINESS_UNIT']]\nnewDataset = newDataset[newDataset.duplicated('EmployeeID', keep=False)]\nduplicateds = newDataset[newDataset.duplicated(keep='last')]\n\nmerged = pd.merge(newDataset,duplicateds, how='outer', indicator=True)\nmerged.loc[merged._merge == 'left_only', ['EmployeeID']]\n","79ab4abc":"# Checks Active and Terminated columns because we keep last ones\nstatusTerminatedCount = len(dataset[dataset.STATUS == \"TERMINATED\"])\nstatusActiveCount = len(dataset[dataset.STATUS == \"ACTIVE\"])\n\nplt.bar([\"Terminated\", \"Active\"],[statusTerminatedCount, statusActiveCount])\nplt.xlabel(\"Employees\")\nplt.ylabel(\"Count\")\nplt.title(\"Working Status\")\nplt.show()\nprint(\"Terminated: \", statusTerminatedCount)\nprint(\"Active: \", statusActiveCount)\n","cc1fbfab":"# Makes cities dummy with their population\ncity_pop_2020 = {'Vancouver':2313328,\n                 'Victoria':289625,\n                 'Nanaimo':84905,\n                 'New Westminster':58549,\n                 'Kelowna':125109,\n                 'Burnaby':202799,\n                 'Kamloops':68714,\n                 'Prince George':65558,\n                 'Cranbrook':18610,\n                 'Surrey':394976,\n                 'Richmond':182000,\n                 'Terrace':19443,\n                 'Chilliwack':77000,\n                 'Trail':9707,\n                 'Langley':23606,\n                 'Vernon':47274,\n                 'Squamish':19512,\n                 'Quesnel':13799,\n                 'Abbotsford':151683,\n                 'North Vancouver':48000,\n                 'Fort St John':17402,\n                 'Williams Lake':14168,\n                 'West Vancouver':42694,\n                 'Port Coquitlam':114565,\n                 'Aldergrove':12363,\n                 'Fort Nelson':3561,\n                 'Nelson':9813,\n                 'New Westminister':58549,\n                 'Grand Forks':4049,\n                 'White Rock':66450,\n                 'Haney':82256,\n                 'Princeton':2828,\n                 'Dawson Creek':10802,\n                 'Bella Bella':1019,\n                 'Ocean Falls':129,\n                 'Pitt Meadows':174410,\n                 'Cortes Island':1042,\n                 'Valemount':1021,\n                 'Dease Lake':335,\n                 'Blue River':157}\n#Make a copy of city names\ndataset['Pop'] = dataset['city_name']\n\n# Map from city name to population\ndataset['Pop'] = dataset.Pop.map(city_pop_2020)\n\n# Make a new column for population category\ndataset['Pop_category'] = dataset.Pop\n\n# Categorise according to population size\n# >= 100,000 is City\n# 10,000 to 99,999 is Rural\n# < 10,000 is Remote\n# Thanks for @dredlaw for this classification example\n# Data is taken from https:\/\/worldpopulationreview.com\/countries\/cities\/canada\ncity_ix = (dataset['Pop'] >= 100000)\nrural_ix = ((dataset['Pop'] < 100000) & (dataset['Pop'] >= 10000))\nremote_ix = (dataset['Pop'] < 10000)\ndataset.loc[city_ix, 'Pop_category'] = 'City'\ndataset.loc[rural_ix, 'Pop_category'] = 'Rural'\ndataset.loc[remote_ix, 'Pop_category'] = 'Remote'\n\ndataset['Pop_category'] = dataset['Pop_category'].map({'City' : 0, 'Rural' : 1, 'Remote' : 2})","39338b12":"# Clusters job title\nboard = ['VP Stores', 'Director, Recruitment', 'VP Human Resources', 'VP Finance',\n         'Director, Accounts Receivable', 'Director, Accounting',\n         'Director, Employee Records', 'Director, Accounts Payable',\n         'Director, HR Technology', 'Director, Investments',\n         'Director, Labor Relations', 'Director, Audit', 'Director, Training',\n         'Director, Compensation']\n\nexecutive = ['Exec Assistant, Finance', 'Exec Assistant, Legal Counsel',\n             'CHief Information Officer', 'CEO', 'Exec Assistant, Human Resources',\n             'Exec Assistant, VP Stores']\n\nmanager = ['Customer Service Manager', 'Processed Foods Manager', 'Meats Manager',\n           'Bakery Manager', 'Produce Manager', 'Store Manager', 'Trainer', 'Dairy Manager']\n\n\nemployee = ['Meat Cutter', 'Dairy Person', 'Produce Clerk', 'Baker', 'Cashier',\n            'Shelf Stocker', 'Recruiter', 'HRIS Analyst', 'Accounting Clerk',\n            'Benefits Admin', 'Labor Relations Analyst', 'Accounts Receiveable Clerk',\n            'Accounts Payable Clerk', 'Auditor', 'Compensation Analyst',\n            'Investment Analyst', 'Systems Analyst', 'Corporate Lawyer', 'Legal Counsel']\n\ndef changeTitle(row):\n    if row in board:\n        return 'board'\n    elif row in executive:\n        return 'executive'\n    elif row in manager:\n        return 'manager'\n    else:\n        return 'employee'\n    \ndataset['job_title'] = dataset['job_title'].apply(changeTitle)","e382bbd0":"# Makes all values useful for modelling\ndataset = pd.get_dummies(dataset,columns=['job_title'])\ndataset['gender'] = dataset['gender_short'].replace({'F':1, 'M':0})\ndataset['status'] = dataset['STATUS'].replace({'ACTIVE':1, 'TERMINATED':0})\ndataset['BUSINESS_UNIT'] = dataset['BUSINESS_UNIT'].replace({'STORES':1, 'HEADOFFICE':0})","e37d5e3c":"# Makes all employees unique because there is no change on categoric columns\n# Keeps last records\nnonSequentialData = dataset.sort_values(['EmployeeID', 'length_of_service'], ascending=[True, True]).drop_duplicates('EmployeeID', keep='last').reset_index(drop=True)","d085b9a6":"# Checks Active and Terminated columns because we keep last ones\nstatusTerminatedCount = len(nonSequentialData[nonSequentialData.STATUS == \"TERMINATED\"])\nstatusActiveCount = len(nonSequentialData[nonSequentialData.STATUS == \"ACTIVE\"])\n\nplt.bar([\"Terminated\", \"Active\"],[statusTerminatedCount, statusActiveCount])\nplt.xlabel(\"Employees\")\nplt.ylabel(\"Count\")\nplt.title(\"Working Status\")\nplt.show()\nprint(\"Terminated: \", statusTerminatedCount)\nprint(\"Active: \", statusActiveCount)\n","da766216":"# Correlation Map\nsns.heatmap(nonSequentialData.corr())","ae9cb683":"# Sorted absolute values of correalation matrix\nnonSequentialData.corr().abs()['status'].sort_values(ascending=False)","4ea67fab":"# Dropping operation\nnonSequentialData = nonSequentialData.drop(\n    [\n     'gender_short', 'gender_full', 'STATUS',\n     'city_name', 'EmployeeID', 'recorddate_key',\n     'birthdate_key', 'orighiredate_key', 'terminationdate_key',\n     'department_name','termreason_desc', 'termtype_desc'\n     ], \n     axis=1\n)","20698c89":"# Keeps EmployeeID and recorddate_key\nsequentialData = dataset.drop(\n    [\n     'gender_short', 'gender_full', 'STATUS',\n     'city_name',\n     'birthdate_key', 'orighiredate_key', 'terminationdate_key',\n     'department_name','termreason_desc', 'termtype_desc'\n     ], \n     axis=1\n)","393d0484":"# Seperates labeled value\nlabel = nonSequentialData.status\nnonSequentialData = nonSequentialData.drop('status', axis=1)","644ea045":"# Split data as test and train\nx_train, x_test, y_train, y_test = train_test_split(nonSequentialData, label, test_size=0.3,\n                                                    random_state=10)","805bf735":"# Make prediciton by a given model\ndef prediction(model, is_svc=False):\n  model.fit(x_train, y_train)\n  y_pred = model.predict(x_test)\n  score = round(accuracy_score(y_test, y_pred), 3)\n\n  cm1 = cm(y_test, y_pred)\n\n  sns.heatmap(cm1, annot=True, fmt=\".0f\")\n  plt.xlabel('Predicted Values')\n  plt.ylabel('Actual Values')\n  plt.title('Accuracy Score: {0}'.format(score), size = 15)\n  plt.show()\n\n  y_proba = None\n\n  if not is_svc:\n    y_proba = model.predict_proba(x_test)\n  else:\n    model = CalibratedClassifierCV(model) \n    model.fit(x_train, y_train)\n    y_proba = model.predict_proba(x_test)\n\n  return y_pred, y_proba","ab3de31c":"model_dt = tree.DecisionTreeClassifier()\ny_pred_dt, y_proba_dt = prediction(model_dt)","7c77eaec":"model_lr = LogisticRegression(C=0.001, penalty='l2')\ny_pred_lr, y_proba_lr = prediction(model_lr)","4f9ac4b6":"model_knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=14, p=2, weights='distance')\ny_pred_knn, y_proba_knn = prediction(model_knn)","6ce54eb7":"model_svc = SVC(C=10, gamma=0.01, kernel='rbf', random_state=10)\ny_pred_svc, y_proba_svc = prediction(model_svc, True)","e80f6234":"model_rf = RandomForestClassifier(bootstrap=False, max_depth=50, max_features=10, min_samples_leaf=5, min_samples_split=8, n_estimators=100, random_state=10)\ny_pred_rf, y_proba_rf = prediction(model_rf)","a3665d19":"model_mlp = MLPClassifier(alpha=0.05, hidden_layer_sizes=(100, ), activation='relu', solver='adam', max_iter=100)\ny_pred_mlp, y_proba_mlp = prediction(model_mlp)","06c3eabe":"# ensemble model\n\nensemble_proba = (y_proba_dt + y_proba_knn + y_proba_lr + y_proba_svc + y_proba_rf + y_proba_mlp) \/ 5\n\ny_pred_ens = []\n\nfor y in ensemble_proba:\n    y_pred = np.where(y == np.amax(y))[0][0]\n    y_pred_ens.append(y_pred)\n\ny_pred_ens = np.array(y_pred_ens)\n\ntp = tn = fp = fn = 0\n\nfor i, real in enumerate(y_test, start=0):\n  pred = y_pred_ens[i]\n  if (real == 1 and pred == 1):\n    tp += 1\n  elif (real == 0 and pred == 0):\n    tn += 1\n  elif (real == 0 and pred == 1):\n    fp += 1\n  else:\n    fn += 1\n\nprecision_1 = tp \/ (tp + fp)\nrecall_1 = tp \/ (tp + fn)\nf1_1 = 2 * precision_1 * recall_1 \/ (precision_1 + recall_1)\n\nprecision_0 = tn \/ (tn + fn)\nrecall_0 = tn \/ (tn + fp)\nf1_0 = 2 * precision_0 * recall_0 \/ (precision_0 + recall_0)\n\nfor var in ['tp','fp','tn','fn', '', 'precision_1', 'recall_1', 'f1_1', '', 'precision_0', 'recall_0', 'f1_0']:\n    if var == '':\n      print()\n    else:\n      print(var + \":         \" + str(vars()[var]))","5e361289":"def display_eval_metrics(precision_1, recall_1, f1_1, precision_0, recall_0, f1_0):\n  print()\n  for var in display_eval_metrics.__code__.co_varnames:\n    if var == \"precision_0\":\n      print()\n    if var == \"var\":\n      continue\n    print(var + \": \" + str(vars()[var]))\n  print(\"------------------------------------------------------------------------\\n\")\n\n\ndef get_eval_metrics(cm):\n  tp = cm[1][1]\n  tn = cm[0][0]\n  fp = cm[0][1]\n  fn = cm[1][0]\n\n  precision_1 = tp \/ (tp + fp)\n  recall_1 = tp \/ (tp + fn)\n  f1_1 = (2 * precision_1 * recall_1) \/ (precision_1 + recall_1)\n\n  precision_0 = tn \/ (tn + fn)\n  recall_0 = tn \/ (tn + fp)\n  f1_0 = (2 * precision_0 * recall_0) \/ (precision_0 + recall_0)\n\n  return precision_1, recall_1, f1_1, precision_0, recall_0, f1_0\n\n\ndef draw_roc_curve(predict_obj_list):\n\n  for predict_obj in predict_obj_list:\n    model_name = predict_obj[0]\n    model_pred = predict_obj[1]\n\n    fpr, tpr, thresholds = roc_curve(y_test.round(0).values.tolist(), model_pred.round(2).tolist(), drop_intermediate=False)\n    auc_ = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label = model_name + ' ROC curve (AUC = %0.2f)' % auc_);\n\n  plt.legend(loc=\"lower right\")\n  plt.xlabel('False Positive Rate')\n  plt.ylabel('True Positive Rate');\n \n\ndef get_cross_val_eval(model_name, model):\n\n  k = 10\n  kf = KFold(n_splits=k, random_state=None)\n  \n  acc_score = []\n  \n  X = nonSequentialData\n  y = label\n\n  precision_1_list = []\n  recall_1_list = []\n  f1_1_list = []\n  precision_0_list = []\n  recall_0_list = []\n  f1_0_list = []\n\n  for train_index , test_index in kf.split(X):\n      X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\n      y_train , y_test = y[train_index] , y[test_index]\n      \n      model.fit(X_train, y_train)\n      \n      pred_values = model.predict(X_test)\n      \n      acc = accuracy_score(pred_values , y_test)\n      acc_score.append(acc)\n      \n      cm1 = cm(y_test, pred_values)\n\n      precision_1, recall_1, f1_1, precision_0, recall_0, f1_0 = get_eval_metrics(cm1)\n\n      precision_1_list.append(precision_1)\n      recall_1_list.append(recall_1)\n      f1_1_list.append(f1_1)\n      precision_0_list.append(precision_0)\n      recall_0_list.append(recall_0)\n      f1_0_list.append(f1_0)\n\n  print(\"*** \" + model_name + \" - average metrics\\n\")\n\n  avg_acc_score = sum(acc_score)\/k\n  \n  print('Avg accuracy : {}'.format(avg_acc_score))\n  display_eval_metrics(mean(precision_1_list), mean(recall_1_list), mean(f1_1_list), mean(precision_0_list), mean(recall_0_list), mean(f1_0_list))  \n","db23d868":"draw_roc_curve([\n     (\"KNN\", y_pred_knn), \n     (\"RF\", y_pred_rf), \n     (\"LR\", y_pred_svc), \n     (\"SCV\", y_pred_svc), \n     (\"MLP\", y_pred_mlp),\n     (\"ENS\", y_pred_ens)\n])","d74f9c8c":"get_cross_val_eval(\"MLP\", model_mlp)\nget_cross_val_eval(\"KNN\", model_knn)\nget_cross_val_eval(\"RF\", model_rf)\nget_cross_val_eval(\"SVC\", model_svc)\nget_cross_val_eval(\"DT\", model_dt)\nget_cross_val_eval(\"LR\", model_lr)","a497fa4d":"def format_date(raw_date):\n    processed_data = parser.parse(str(raw_date))\n    return processed_data\n\ndate_column = sequentialData.apply(lambda row: format_date(row[\"recorddate_key\"]), axis=1)\nsequentialData[\"recorddate_key\"] = date_column\n\nsequentialData = sequentialData.sort_values(['EmployeeID', 'recorddate_key'], ascending=[True, True])\n\nemployee_list = sequentialData[\"EmployeeID\"].unique()\n\nstructured_data = []\nlabel_list = []\n\nmax_len = 0\nnumber_of_features = 6\nfor employee_id in employee_list:\n    employee_frame = sequentialData.loc[sequentialData['EmployeeID'] == employee_id]\n    if(employee_frame.shape[0] > max_len):\n        max_len = employee_frame.shape[0]\n\n# creating a tensor for each customer\nfor i, employee_id in enumerate(employee_list, start=0):\n\n    employee_frame = sequentialData.loc[sequentialData['EmployeeID'] == employee_id]\n    last_status_value = employee_frame.iloc[[-1]][\"status\"].values[0]\n    employee_frame = employee_frame.drop(columns=['EmployeeID', 'recorddate_key', 'status'])\n\n    tensor = np.zeros((max_len, employee_frame.shape[1]))\n\n    count = 0\n    for index, row in employee_frame.iterrows():  \n        # padding based on max_len\n        tensor[max_len - employee_frame.shape[0] + count] = row.to_numpy()\n        count += 1\n        \n    structured_data.append(tensor)    \n    label_list.append(last_status_value)    \n\nstructured_data = np.array(structured_data)\nlabel_list = np.array(label_list)","14000747":"# Splits train test data\ntrain_X, test_X, train_y, test_y = train_test_split(structured_data, label_list, test_size=0.2, random_state=42)","b6926970":"model_lstm = Sequential()\nmodel_lstm.add(LSTM(250, dropout=0.2, recurrent_dropout=0.2, input_shape=(train_X.shape[1], train_X.shape[2])))\n\nmodel_lstm.add(Dense(250, activation='relu'))\nmodel_lstm.add(Dense(2, activation='softmax')) #2 -> number of labels\nmodel_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# fit network\nearly_stop = EarlyStopping(monitor=\"val_loss\", verbose=2, mode='min', patience=3)\nhistory = model_lstm.fit(train_X, train_y, epochs=30, batch_size=128, validation_data=(test_X, test_y), verbose=2, shuffle=True)\n\n# plot history\npyplot.plot(history.history['accuracy'], label='train')\npyplot.plot(history.history['val_accuracy'], label='test')\npyplot.legend()\npyplot.show()","35f5df5d":"model_gru = Sequential()\nmodel_gru.add(GRU(250, dropout=0.2, recurrent_dropout=0.2, input_shape=(train_X.shape[1], train_X.shape[2])))\n\nmodel_gru.add(Dense(250, activation='tanh'))\nmodel_gru.add(Dense(2, activation='softmax')) #2 -> number of labels\nmodel_gru.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# fit network\nearly_stop=EarlyStopping(monitor=\"val_accuracy\",verbose=2,mode='max',patience=3)\nhistory = model_gru.fit(train_X, train_y, epochs=30, batch_size=64, validation_data=(test_X, test_y), verbose=2, shuffle=True)\n\n# plot history\npyplot.plot(history.history['accuracy'], label='train')\npyplot.plot(history.history['val_accuracy'], label='test')\npyplot.legend()\npyplot.show()","63ebf36b":"yhat_lstm = model_lstm.predict(test_X, verbose=0)\nyhat_gru = model_gru.predict(test_X, verbose=0)\n\nyhat_ens = (yhat_gru + yhat_lstm) \/ 2","8a3cc254":"def evaluate_sequential_model(model_name, predictions, real_values):\n\n  test_X_predictions = []\n\n  for y in predictions:\n      y = np.round(y, decimals=3)\n      y_pred = np.where(y == np.amax(y))[0][0]\n      test_X_predictions.append(y_pred)\n\n  tp = tn = fp = fn = 0\n\n  for i, real in enumerate(real_values, start=0):\n    pred = test_X_predictions[i]\n    if (real == 1 and pred == 1):\n      tp += 1\n    elif (real == 0 and pred == 0):\n      tn += 1\n    elif (real == 0 and pred == 1):\n      fp += 1\n    else:\n      fn += 1\n\n\n  precision_1 = tp \/ (tp + fp)\n  recall_1 = tp \/ (tp + fn)\n  f1_1 = 2 * precision_1 * recall_1 \/ (precision_1 + recall_1)\n\n  precision_0 = tn \/ (tn + fn)\n  recall_0 = tn \/ (tn + fp)\n  f1_0 = 2 * precision_0 * recall_0 \/ (precision_0 + recall_0)\n\n  print(model_name)  \n  print(\"---\")\n\n  for var in ['tp','fp','tn','fn', '', 'precision_1', 'recall_1', 'f1_1', '', 'precision_0', 'recall_0', 'f1_0']:\n    if var == '':\n      print()\n    else:\n      print(var + \":         \" + str(vars()[var]))\n\n  print(\"-----------------------------------\\n\")","7572054b":"evaluate_sequential_model(\"LSTM\", yhat_lstm, test_y)\nevaluate_sequential_model(\"GRU\", yhat_gru, test_y)\nevaluate_sequential_model(\"ENS\", yhat_ens, test_y)","5cd9b577":"<a id=\"4-1-7-ensemble\"><\/a>\n#### Ensemble Model","0e0d3273":"<a id=\"3-3-1-dummy-variable\"><\/a>\n#### Dummy Variable & Categorical Variable Encoding","4d68488f":"<a id=\"4-1-6-nn\"><\/a>\n#### Neural Network","f87604bf":"<a id=\"4-1-8-eva\"><\/a>\n#### Evaluation","17e8f7f1":"<a id=\"3-3-3-seq-prepare\"><\/a>\n#### For Sequential Models","130b9776":"<a id=\"4-1-nonseq-models\"><\/a>\n### Non Sequential Models","3f65e024":"<a id=\"4-2-3-gru\"><\/a>\n#### GRU","955f9fa4":"<a id=\"4-modelling-and-evaluation\"><\/a>\n## Modelling & Evaluation","8393e690":"<a id=\"4-1-3-knn\"><\/a>\n#### KNN","74187b9d":"<a id=\"4-2-4-ensemble\"><\/a>\n#### Ensemble Model","6bec4ed9":"<a id=\"4-1-5-rf\"><\/a>\n#### Random Forest","746f6046":"<a id=\"4-1-2-lr\"><\/a>\n#### Logistic Regression","5b1315f8":"<a id=\"2-data-exploration\"><\/a>\n## Data Exploration","d9a39834":"<a id=\"3-3-2-nonseq-prepare\"><\/a>\n#### For Non Sequential Models","3b7303be":"<a id=\"4-1-1-dt\"><\/a>\n#### Decision Tree","1e9f50fe":"<a id=\"3-3-2-2-drop-unnec\"><\/a>\n##### Dropping Unnecassary Features","3d8bc366":"<a id=\"4-2-seq\"><\/a>\n### Sequential Models","863df18e":"<a id=\"3-data-processing\"><\/a>\n## Data Processing","be681d28":"<a id=\"3-3-2-1-remove-duplicateds\"><\/a>\n##### Removing Duplicated Employees","2334e46d":"# Employee Attrition\nIn this notebook, various models for predicting churning employees using Employee Attrition dataset were built. As a different approach from the other implementations, both sequential and non-sequential models were built with their ensemble models. Besides, the evaluation metrics for the minor class are calculated as well, to be able to see if the models are successful or they just favor the major class (which commonly occurs with imbalanced datasets). The parameters for the models were determined via grid search. Apart from that, the k-fold cross-validation is applied for estimating the performance of the models.\n\nThe sequential models : LSTM, GRU, Sequential Ensemble\n\nThe non-sequential models : Decision Tree, Random Forest, K-Nearest Neighbors, Support Vector \nClassifier, Logistic Regression, Multilayer Perceptron, Non-Sequential Ensemble\n\nAccording to the results, Random Forest is the most successful one after the Non-Sequential Ensemble model when considering both classes. Due to the small amount of data, sequential models are not the best ones. The dataset does not have many features, but the approach can be used as preliminary work for more complicated employee churn models.\n\n* [Import Libraries](#1-import-libraries)\n* [Data Exploration](#2-data-exploration)\n* [Data Processing](#3-data-processing)\n    * [Clustering Cities by Population](#3-1-clustring-cities)\n    * [Clustering Job Titles](#3-2-clusterng-jobs)\n    * [Preparing Dataset for Modelling](#3-3-preparing-dataset)\n        * [Dummy Variable & Categorical Variable Encoding](#3-3-1-dummy-variable)\n        * [For Non Sequential Models](#3-3-2-nonseq-prepare)\n            * [Removing Duplicated Employees](#3-3-2-1-remove-duplicateds)\n            * [Dropping Unnecassary Features](#3-3-2-2-drop-unnec)\n        * [For Sequential Models](#3-3-3-seq-prepare)\n* [Modelling & Evaluation](#4-modelling-and-evaluation)\n    * [Non Sequential Models](#4-1-nonseq-models)\n        * [Decision Tree](#4-1-1-dt)\n        * [Logistic Regression](#4-1-2-lr)\n        * [KNN](#4-1-3-knn)\n        * [SVC](#4-1-4-svc)\n        * [Random Forest](#4-1-5-rf)\n        * [Neural Network](#4-1-6-nn)\n        * [Ensemble Model](#4-1-7-ensemble)\n        * [Evaluation](#4-1-8-eva)\n    * [Sequential Models](#4-2-seq)\n        * [Creating Sequential Data Structure](#4-2-1-seq-data)\n        * [LSTM](#4-2-2-lstm)\n        * [GRU](#4-2-3-gru)\n        * [Ensemble Model](#4-2-4-ensemble)\n        * [Evaluation](#4-2-5-evaluation)","d7dcdd73":"We saw there is no changes on above columns for same employee. So we going to remove duplicated ones in section [Removing Duplicated Employees](#remove_duplicated) for non sequential models","dc6dc64f":"<a id=\"1-import-libraries\"><\/a>\n## Import Libraries","113f44c6":"<a id=\"4-1-4-svc\"><\/a>\n#### SVC","4c0ea7f4":"<a id=\"4-2-5-evaluation\"><\/a>\n#### Evaluation","84777e60":"<a id=\"4-2-1-seq-data\"><\/a>\n#### Creating Sequential Data Structure for LSTM and GRU. The data is sorted by date and all the rows for each customer are formed as sequences. To determine the shape of the tensor, max_len is found. For the customer having fewer rows than max_len, the sequences are padded with 0.","4ecae25f":"<a id=\"3-3-preparing-dataset\"><\/a>\n### Preparing Dataset for Modelling","b9c73f71":"<a id=\"3-2-clusterng-jobs\"><\/a>\n### Clustering Job Titles","84a3433a":"<a id=\"4-2-2-lstm\"><\/a>\n#### LSTM","61b57d89":"<a id=\"3-1-clustring-cities\"><\/a>\n### Clustering Cities by Population"}}