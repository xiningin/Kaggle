{"cell_type":{"9ee335de":"code","acffb827":"code","26ede985":"code","0e1cd536":"code","4d2c6184":"code","714b0ce9":"code","acfff6ab":"code","4c3a3fad":"code","452cc758":"code","293fbbe2":"code","f9aaea4c":"code","8e806697":"code","2fdea2b1":"code","7b10d5b3":"code","97cdd102":"code","11c527cb":"code","2d98beb6":"code","55299c73":"code","4e5eb791":"code","6fda351a":"code","b502f15d":"code","f4058ec9":"code","56f185d4":"code","38e178ee":"code","a51ff80e":"code","834c65ce":"code","47ac5969":"code","cc56c663":"code","9b6b76f2":"code","128e444b":"code","2eac2f27":"markdown","4daa2ebf":"markdown","13c39f12":"markdown","1c9e7230":"markdown","4ad3e27c":"markdown","200b4e6e":"markdown","e2c3652d":"markdown","27073a3e":"markdown","418b54cb":"markdown","89a768a0":"markdown","743e0d3c":"markdown","1a9cf995":"markdown","79080512":"markdown","e40a6561":"markdown","d2ea639e":"markdown","7f9f44b8":"markdown"},"source":{"9ee335de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","acffb827":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split , cross_validate\nfrom sklearn.metrics import accuracy_score , precision_score\nfrom sklearn.preprocessing import StandardScaler , OneHotEncoder\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier","26ede985":"train = '..\/input\/titanic\/train.csv'\ntrain = pd.read_csv(train)\n\ntest = '..\/input\/titanic\/test.csv'\ntest = pd.read_csv(test)","0e1cd536":"train.head()","4d2c6184":"test.head()","714b0ce9":"train[train.Survived<1].Sex.value_counts()        #that could not survived","acfff6ab":"train[train.Survived>0].Sex.value_counts()         #the one survived","4c3a3fad":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\n\nwomen = train[train['Sex']=='female']\n\nmen = train[train['Sex']=='male']\n\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","452cc758":"sns.barplot(x='Pclass',y='Survived',data=train)\nplt.show()","293fbbe2":"train['Pclass'].isnull().sum()","f9aaea4c":"train['Sex'].isnull().sum()","8e806697":"train['Age'].isnull().sum()","2fdea2b1":"train[\"Age\"].describe()","7b10d5b3":"train['Age'].fillna(value=29,inplace = True)","97cdd102":"train.head(10)  ","11c527cb":"features = ['Pclass']\n\n\nss = StandardScaler()\n\ntrain_ss = pd.DataFrame(data = train)\ntrain_ss[features] = ss.fit_transform(train_ss[features])","2d98beb6":"train_ss.head(11)","55299c73":"train_ss.replace(to_replace =\"male\",value =1,inplace = True) \ntrain_ss.replace(to_replace =\"female\",value =0,inplace = True)","4e5eb791":"train_ss","6fda351a":"test.head()","b502f15d":"test['Age'].isnull().sum()","f4058ec9":"test[\"Age\"].describe()","56f185d4":"test['Age'].fillna(value=30,inplace = True)","38e178ee":"features = ['Pclass']\n\n\nss = StandardScaler()\n\ntest_ss = pd.DataFrame(data = test)\ntest_ss[features] = ss.fit_transform(test_ss[features])","a51ff80e":"test_ss.replace(to_replace =\"male\",value =1,inplace = True) \ntest_ss.replace(to_replace =\"female\",value =0,inplace = True)","834c65ce":"y_train = train_ss[['Survived']]\n\nprint(y_train)","47ac5969":"ft = ['Pclass','Sex','Age','SibSp','Parch']\n\nx =(train_ss[ft])\nx_test =(test_ss[ft])","cc56c663":"\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(x, y_train)\npred = decision_tree.predict(x_test)\nacc_decision_tree = round(decision_tree.score(x, y_train) * 100, 2)\nacc_decision_tree","9b6b76f2":"output = pd.DataFrame({'PassengerId': test_ss.PassengerId, 'Survived': pred})\noutput.to_csv('my_new_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","128e444b":"print(output)","2eac2f27":"# Data Preprocessing","4daa2ebf":"**same is done for the TEST dataset**","13c39f12":"**the code below helps us to save the predicted values in a csv file which can be later downloaded.**\n\n","1c9e7230":"# Plotting the Graph\n**here we will plot the graph showcasing the count of the each MALE and FEMALE that died and survived that wreck.**","4ad3e27c":"next code represents the total count of the passengers who could not survived in each gender.\n\ni.e\n\n**468 MEN  &  81 WOMEN**","200b4e6e":"# Titanic - Machine Learning from Disasters\n\nRMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning \nhours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224\npassengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters. \nRMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners \noperated by the White Star Line. She was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, chief naval architect \nof the shipyard at the time, died in the disaster.\n\n# here we will try to predict the survival rate i.e the passenger survived (1) or not(0)","e2c3652d":"here we count the total number of the passengers who survived the wreck \n\ni.e.\n\n**109 MALE & 233 WOMEN**","27073a3e":"**we can use OneHotEncoder here too but i was unable to do so, because of which i had to go an easy way**\n\nHere we have replaced all the categorical values to a dummy values so that while fitting model gets all the values in numerical form.","418b54cb":"# Now here we'll try to find a pattern \n\nwhile sinking , originally women and children were preferred more to save compared to that of men.\nclasses('***Pclass***' here) of the respective passenger was also taken in consideration.\n\n**so here, we compared the probablity of surviving of the respective passenger belonging to their gender.**","89a768a0":"**Filling the null values of 'Age' with the mean of the 'Age'**","743e0d3c":"checking for null values","1a9cf995":"# import all the necessary modules given below\nsuch as ****SKLEARN,NUMPY,PANDAS****\n\nhere the line %matplotlib inline helps you to plot all the graphs within the cell and thus avoiding a new dialogue box and making the code look clean.\n\nI have used Decision Tree Classifier here to achive a good accuracy.\nStandardScaler is used so as to  standardized '***Pclass***'  column so that the model does not get biased.\n\nOneHotEncoder:\nThe input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the sparse parameter).\n\nHere we have used this in '***sex***' column.\n","79080512":"# since, we can see that there is a relation.\nnow we know on which columns do we need to work.","e40a6561":"# Read the training and testing data","d2ea639e":"the next cell shows the comparision between class and the survival chance of the passengers.\n\n**i.e to show how class of passenger was linked to his\/her survival chances.**","7f9f44b8":"**Now it is time for us to Fit the model we made so far**\n\n**Here we will use Decision Tree Classifier**\n\n"}}