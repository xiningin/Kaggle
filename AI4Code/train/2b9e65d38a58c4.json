{"cell_type":{"fdc16107":"code","1d74c3e7":"code","3cc828f6":"code","2b63b90a":"code","1df6d8df":"code","4ba7b7b0":"code","ff057fe1":"code","498dab57":"code","603ce4eb":"code","773a6a7b":"code","a9b28778":"code","b60e95af":"code","31698fca":"code","1697638c":"code","68c8577d":"code","20703ae7":"code","baed1151":"code","087d279c":"code","2509ff97":"code","b8d11b44":"code","d78082a9":"code","b654cd57":"code","b36e18bd":"code","0f874eac":"code","d2b1ec3e":"code","d9d5b645":"code","ac6a600a":"code","1a056f6d":"code","831c9f5b":"code","ca8d60c6":"code","727f82d3":"code","f9a7c446":"code","882a9a56":"code","d2a8f275":"code","79d086e9":"code","52425a44":"code","ba40bc39":"code","24111420":"code","1e9dd153":"code","5e952839":"code","670549d2":"code","0099bd9a":"code","026e94b1":"code","154a7dff":"code","7c2c469e":"code","4378716a":"code","3f1d0fc9":"code","747f62ee":"code","1a3e42e6":"code","896c6be2":"code","075512e1":"code","b2d5b9d6":"code","ce219ef6":"markdown","421951ea":"markdown","37c7c21e":"markdown","c7181a3d":"markdown","a0fabb90":"markdown","52fa08a0":"markdown","c8eead8f":"markdown","953a0dc1":"markdown","3b13dd65":"markdown","9da52a7d":"markdown","06b869ff":"markdown","e85aab3e":"markdown","8d446c58":"markdown","8a941e05":"markdown","84e0f106":"markdown","5ed1ebf8":"markdown","7635a7b9":"markdown","58aea923":"markdown","35e95878":"markdown","ae922364":"markdown","1020a49b":"markdown","eddde130":"markdown","ae7db01c":"markdown","f8562a31":"markdown","aa5ec3b5":"markdown","c598f7d9":"markdown"},"source":{"fdc16107":"# load library\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom scipy import stats\nimport pylab as pl\n\n# Display HTML\nfrom IPython.core.display import display, HTML","1d74c3e7":"data =  pd.read_csv('..\/input\/iris\/Iris.csv')","3cc828f6":"data.head()","2b63b90a":"data.shape","1df6d8df":"data.columns","4ba7b7b0":"sns.FacetGrid(data, hue='Species', size=5)\\\n.map(plt.scatter, 'SepalLengthCm', 'SepalWidthCm')\\\n.add_legend()","ff057fe1":"sns.pairplot(data, hue='Species')","498dab57":"X = data.iloc[:, :-1].values    #   X -> Feature Variables\ny = data.iloc[:, -1].values #   y ->  Target\n\n# Splitting the data into Train and Test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nX_train.shape, y_test.shape","603ce4eb":"# Feature Variables \nXL = data.iloc[:, :-1].values \n\n# Target \nyL = data.iloc[:, -1]","773a6a7b":"# Convert object data into int  by labelEncoder\nfrom sklearn.preprocessing import LabelEncoder \nle = LabelEncoder()\nY_train = le.fit_transform(yL)","a9b28778":"Y_train","b60e95af":"# split train test data \nX_trainL, X_testL, y_trainL, y_testL = train_test_split(\n    XL, Y_train, test_size = .3, random_state=0)","31698fca":"# Create Linear Regression model \nfrom sklearn.linear_model import LinearRegression\n\nmodelLR = LinearRegression()\nmodelLR.fit(X_trainL, y_trainL)\ny_predict = modelLR.predict(X_testL)","1697638c":"# Evaluationg model score \nfrom sklearn import metrics\nprint('y_intercept                  = ', modelLR.intercept_)\nprint('Best coefficients            = ', modelLR.coef_)\nprint('Mean Abs Error               = ', metrics.mean_absolute_error(y_testL,y_predict))\nprint('Mean Squared Error           = ', metrics.mean_squared_error(y_testL,y_predict))\nprint('Root Mean Sqrt Error or RMSE = ', np.sqrt(metrics.mean_squared_error(y_testL, y_predict)))\nprint('r2 Values                    = ', metrics.r2_score(y_testL, y_predict))\n\n","68c8577d":"from sklearn.tree import DecisionTreeClassifier \n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","20703ae7":"# Summary of DT \nprint('Confusion Matrix: ')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Classification report: ', classification_report(y_test, y_pred))\n\nprint(' Accuracy: ', accuracy_score(y_pred, y_test))\n","baed1151":"from sklearn.ensemble import RandomForestClassifier \nmodel  = RandomForestClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","087d279c":"# Summary\nprint('Classifications Report :')\nprint(classification_report(y_test, y_pred))\n\nprint('Confision Matrix :')\nprint(confusion_matrix(y_pred, y_test))\n\nprint('Accuracy:', accuracy_score(y_pred, y_test))","2509ff97":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)","b8d11b44":"print('Classification Report: ')\nprint(classification_report(y_test, y_pred))\n\nprint('Confusion Matrix : ')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Accuracy:' , accuracy_score(y_pred, y_test))","d78082a9":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=10)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)","b654cd57":"# summary \nprint('Classification Report: ')\nprint(classification_report(y_test, y_pred))\n\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Accuracy: ', accuracy_score(y_pred, y_test))","b36e18bd":"from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)","0f874eac":"# Summary \nprint('Classification Report')\nprint(classification_report(y_test, y_pred))\n\nprint('Confusion Matrix: ')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Accuracy: ', accuracy_score(y_pred, y_test))","d2b1ec3e":"from sklearn.svm import SVC\n\nmodel = SVC()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)","d9d5b645":"# Summary \nprint('Classification report:')\nprint(classification_report(y_test, y_pred))\n\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Accuracy: ', accuracy_score(y_pred, y_test))","ac6a600a":"from sklearn.svm import NuSVC\n\nmodel = NuSVC()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)","1a056f6d":"# Summary \nprint('Classification report:')\nprint(classification_report(y_test, y_pred))\n\nprint('Confusion Matrux:')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Accuracy: ', accuracy_score(y_pred, y_test))","831c9f5b":"from sklearn.svm import LinearSVC\n\nmodel = LinearSVC()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)","ca8d60c6":"# Summary \nprint('Classification Report:')\nprint(classification_report(y_test, y_pred))\n\nprint('Confusion Matrix: ')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Accuracy:', accuracy_score(y_pred, y_test))","727f82d3":"from sklearn.neighbors import RadiusNeighborsClassifier\n\nmodel = RadiusNeighborsClassifier(radius=10)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)","f9a7c446":"# Summary \nprint('Classification Report:')\nprint(classification_report(y_test, y_pred))\n\nprint('Confision Report')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Accuracy:', accuracy_score(y_pred, y_test))","882a9a56":"from sklearn.linear_model import PassiveAggressiveClassifier\n\nmodel = PassiveAggressiveClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)","d2a8f275":"# Summary \nprint('Classification Report:')\nprint(classification_report(y_test, y_pred))\n\nprint('Confision Report')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Accuracy:', accuracy_score(y_pred, y_test))","79d086e9":"from sklearn.naive_bayes import BernoulliNB\n\nmodel = BernoulliNB()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)","52425a44":"# Summary \nprint('Classification Report:')\nprint(classification_report(y_test, y_pred))\n\nprint('Confision Report')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Accuracy:', accuracy_score(y_pred, y_test))","ba40bc39":"from sklearn.tree import ExtraTreeClassifier\n\nmodel = ExtraTreeClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)","24111420":"# Summary \nprint('Classification Report:')\nprint(classification_report(y_test, y_pred))\n\nprint('Confision Report')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Accuracy:', accuracy_score(y_pred, y_test))","1e9dd153":"from sklearn.ensemble import BaggingClassifier\n\nmodel=BaggingClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\n","5e952839":"# Summary \nprint('Classification Report:')\nprint(classification_report(y_test, y_pred))\n\nprint('Confision Report')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Accuracy:', accuracy_score(y_pred, y_test))","670549d2":"from sklearn.ensemble import AdaBoostClassifier\n\nmodel=AdaBoostClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)","0099bd9a":"# Summary \nprint('Classification Report:')\nprint(classification_report(y_test, y_pred))\n\nprint('Confision Report')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Accuracy:', accuracy_score(y_pred, y_test))","026e94b1":"from sklearn.ensemble import GradientBoostingClassifier\n\nmodel=AdaBoostClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)","154a7dff":"# Summary \nprint('Classification Report:')\nprint(classification_report(y_test, y_pred))\n\nprint('Confision Report')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Accuracy:', accuracy_score(y_pred, y_test))","7c2c469e":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nmodel=LinearDiscriminantAnalysis()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)","4378716a":"# Summary \nprint('Classification Report:')\nprint(classification_report(y_test, y_pred))\n\nprint('Confision Report')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Accuracy:', accuracy_score(y_pred, y_test))","3f1d0fc9":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n\nmodel=QuadraticDiscriminantAnalysis()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)","747f62ee":"# Summary \nprint('Classification Report:')\nprint(classification_report(y_test, y_pred))\n\nprint('Confision Report')\nprint(confusion_matrix(y_test, y_pred))\n\nprint('Accuracy:', accuracy_score(y_pred, y_test))","1a3e42e6":"x = data.iloc[:, [1,2,3,4]].values\n\n#finding the optiomal number of cluster \n\nfrom sklearn.cluster import KMeans\nwcss = [] # within cluster sum of squares\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', \n                   max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)\n    \n    ","896c6be2":"# Plot the result \n\nplt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of cluster')\nplt.ylabel('WCSS')","075512e1":"# Apply Kmeans to the data \n\nkmeans  = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300,\n                n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(x)","b2d5b9d6":"#Visualising the clusters\nplt.scatter(x[y_kmeans == 0,0], x[y_kmeans == 0, 1],\n           s=100, c= 'red', label='Iris-Setosa')\n\nplt.scatter(x[y_kmeans == 1,0], x[y_kmeans == 1, 1],\n           s=100, c= 'blue', label='Iris-Versicoloure')\n\nplt.scatter(x[y_kmeans == 2,0], x[y_kmeans == 2, 1],\n           s=100, c= 'green', label='Iris-Virginica')\n\n\n#Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], \n            kmeans.cluster_centers_[:,1], \n            s = 100, c = 'yellow', label = 'Centroids',\n            marker='*')\n\nplt.legend()\n\n\nplt.show()","ce219ef6":"###### y_intercept  = Distance from zero point to y axis line start point . If it is less than 0 then line will start below the 0 position of a graph. ","421951ea":"## 4. Logistic Regression ","37c7c21e":"# 18. Quadratic Discriminant Analysis\nA classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule.","c7181a3d":"# 19. K- means\nIt is a type of unsupervised algorithm which solves the clustering problem. ","a0fabb90":"# 11 Passive Aggressive Classifier\nPA algorithm is a margin based online learning algorithm for binary classification. Unlike PA algorithm, which is a hard-margin based method, PA-I algorithm is a soft margin based method and robuster to noise.","52fa08a0":"# 6. Naive Bayes","c8eead8f":"It is a classifications algorithm not a regression algorithm\nHence, it is also known as logit regression. Since, it predicts the probability,\nits output values lies between 0 and 1 (as expected).","953a0dc1":"# 13. ExtraTreeClassifie\nExtraTreesClassifier is an ensemble learning method fundamentally based on decision trees.","3b13dd65":"# Visualization","9da52a7d":"# 16. Gradient Boosting Classifier\nGBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power.","06b869ff":"## 3.RandomForest","e85aab3e":"# 14. Bagging classifier\nBagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.","8d446c58":"# First things first. Lests understand what is confusion matrix, precision, recall, f1-score, support and how to calculate Accuracy. \n\n# Confusion Matrix:\n![](https:\/\/skappal7.files.wordpress.com\/2018\/08\/confusion-matrix.jpg?w=748)\nA Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. For example if your target class or feature has three possible output then your confusion matrix will be 3*3 matrix. \n\n## It has four properties \n## True Positive (TP) \n\n1. The predicted value matches the actual value.\n2. The actual value was positive and the model predicted a positive value.\n\n## True Negative (TN) \n\n1. The predicted value matches the actual value\n2. The actual value was negative and the model predicted a negative value.\n\n## False Positive (FP) \u2013 Type 1 error\n\n1. The predicted value was falsely predicted.\n2. The actual value was negative but the model predicted a positive value. Also known as the Type 1 error\n\n## False Negative (FN) \u2013 Type 2 error\n\n1. The predicted value was falsely predicted.\n2. The actual value was positive but the model predicted a negative value. Also known as the Type 2 error\n\n## Precision:\nIt will tell us what population of positive identifications are actually correct?\n### **Precision = (TP \/ TP + FP)** \n\n\n## Recall:\nWhat proportion of actual positives was identified correctly?\n### Recall = (TP \/ TP + FN) \n\n## F-1 Score:\n F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost.\n###  F-1 Score = 2*(Recall * Precision) \/ (Recall + Precision)\n\n### Accuracy: TP + TN \/ TP + TN + FP + FN\n\n# In short, Which model gives highest accuracy can predecit best.","8a941e05":"# 7. SVM\nIt is a classification method. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate.","84e0f106":"# 17. Linear Discriminant Analysis\nA classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule.","5ed1ebf8":"## 2. Decision Tree","7635a7b9":"# Implement all Machine learning by using SK-learn Library","58aea923":"# 8. Nu-Support Vector Machine \nSimilar as SVM. But uses a parameter to control the number of support vectors ","35e95878":"# 10 Radius Neighbors Classifier\nIn scikit-learn RadiusNeighborsClassifier is very similar to KNeighborsClassifier with the exception of two parameters. First, in RadiusNeighborsClassifier we need to specify the radius of the fixed area used to determine if an observation is a neighbor using radius.","ae922364":"# 15. AdaBoost classifier\nAn AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.","1020a49b":"# Prepare Train and Test ","eddde130":"# 1. Linear Regression","ae7db01c":"Here our used dataset is Iris ","f8562a31":"# 12 BernoulliNB\nLike MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary\/boolean features.","aa5ec3b5":"# 5. k-Nearest Neighbors\nIt can be used for both classification and regression problems.","c598f7d9":"# 9. Linear Support Vector Machine \nSimilar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples."}}