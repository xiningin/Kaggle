{"cell_type":{"5c2c1b1a":"code","167df8fe":"code","01a03d89":"code","d80c81c5":"code","723ce607":"code","9707ee62":"code","c6d7b336":"code","6d5aabab":"code","e9d03440":"code","ba45756e":"code","dc055c43":"code","04ba3aef":"code","e2b5b363":"code","490a5ec4":"code","37dbeacb":"code","0f42916b":"code","8471582e":"code","97214107":"code","fae62e23":"code","a3b56781":"code","0edb9d95":"code","4b6e2216":"code","d7f70964":"code","41f72efb":"code","aea6987d":"code","dc0d5960":"code","5acbf647":"code","e2b678ab":"code","f5e239dd":"code","7afd3b75":"code","c1d537f5":"code","8bd902ec":"code","2572061f":"code","c2761e9a":"code","2e4583cd":"markdown","066fad4e":"markdown","22132dd4":"markdown","301849a5":"markdown","b0d89c51":"markdown","785a2e8f":"markdown","12d887da":"markdown","2be6503e":"markdown","b3f6778e":"markdown","1dc2096c":"markdown","77334d08":"markdown","e298e14e":"markdown","a8d5affc":"markdown","46a6e151":"markdown","d0db7ab5":"markdown","c5cbbce1":"markdown","550ea5c5":"markdown","8aabebdd":"markdown","95902574":"markdown","3fb69966":"markdown","2d7813d4":"markdown","f43831b1":"markdown","9289a927":"markdown","4e7af4ea":"markdown","a5869c20":"markdown","c832195c":"markdown"},"source":{"5c2c1b1a":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\nimport re\n\nprint(\"Tensorflow Version\",tf.__version__)","167df8fe":"df = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',\n                 encoding = 'latin',header=None)\ndf.head()","01a03d89":"df.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\ndf.head()","d80c81c5":"df = df.drop(['id', 'date', 'query', 'user_id'], axis=1)","723ce607":"lab_to_sentiment = {0:\"Negative\", 4:\"Positive\"}\ndef label_decoder(label):\n  return lab_to_sentiment[label]\ndf.sentiment = df.sentiment.apply(lambda x: label_decoder(x))\ndf.head()","9707ee62":"val_count = df.sentiment.value_counts()\n\nplt.figure(figsize=(8,4))\nplt.bar(val_count.index, val_count.values)\nplt.title(\"Sentiment Data Distribution\")","c6d7b336":"import random\nrandom_idx_list = [random.randint(1,len(df.text)) for i in range(10)] # creates random indexes to choose from dataframe\ndf.loc[random_idx_list,:].head(10) # Returns the rows with the index and display it","6d5aabab":"stop_words = stopwords.words('english')\nstemmer = SnowballStemmer('english')\n\ntext_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"","e9d03440":"def preprocess(text, stem=False):\n  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n  tokens = []\n  for token in text.split():\n    if token not in stop_words:\n      if stem:\n        tokens.append(stemmer.stem(token))\n      else:\n        tokens.append(token)\n  return \" \".join(tokens)","ba45756e":"df.text = df.text.apply(lambda x: preprocess(x))","dc055c43":"from wordcloud import WordCloud\n\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'Positive'].text))\nplt.imshow(wc , interpolation = 'bilinear')","04ba3aef":"plt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'Negative'].text))\nplt.imshow(wc , interpolation = 'bilinear')","e2b5b363":"TRAIN_SIZE = 0.8\nMAX_NB_WORDS = 100000\nMAX_SEQUENCE_LENGTH = 30","490a5ec4":"train_data, test_data = train_test_split(df, test_size=1-TRAIN_SIZE,\n                                         random_state=7) # Splits Dataset into Training and Testing set\nprint(\"Train Data size:\", len(train_data))\nprint(\"Test Data size\", len(test_data))","37dbeacb":"train_data.head(10)","0f42916b":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data.text)\n\nword_index = tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Vocabulary Size :\", vocab_size)","8471582e":"from keras.preprocessing.sequence import pad_sequences\n\nx_train = pad_sequences(tokenizer.texts_to_sequences(train_data.text),\n                        maxlen = MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(tokenizer.texts_to_sequences(test_data.text),\n                       maxlen = MAX_SEQUENCE_LENGTH)\n\nprint(\"Training X Shape:\",x_train.shape)\nprint(\"Testing X Shape:\",x_test.shape)","97214107":"labels = train_data.sentiment.unique().tolist()","fae62e23":"encoder = LabelEncoder()\nencoder.fit(train_data.sentiment.to_list())\n\ny_train = encoder.transform(train_data.sentiment.to_list())\ny_test = encoder.transform(test_data.sentiment.to_list())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","a3b56781":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove.6B.zip","0edb9d95":"GLOVE_EMB = '\/kaggle\/working\/glove.6B.300d.txt'\nEMBEDDING_DIM = 300\nLR = 1e-3\nBATCH_SIZE = 1024\nEPOCHS = 10\nMODEL_PATH = '...\/output\/kaggle\/working\/best_model.hdf5'","4b6e2216":"embeddings_index = {}\n\nf = open(GLOVE_EMB)\nfor line in f:\n  values = line.split()\n  word = value = values[0]\n  coefs = np.asarray(values[1:], dtype='float32')\n  embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' %len(embeddings_index))","d7f70964":"embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n  embedding_vector = embeddings_index.get(word)\n  if embedding_vector is not None:\n    embedding_matrix[i] = embedding_vector","41f72efb":"embedding_layer = tf.keras.layers.Embedding(vocab_size,\n                                          EMBEDDING_DIM,\n                                          weights=[embedding_matrix],\n                                          input_length=MAX_SEQUENCE_LENGTH,\n                                          trainable=False)","aea6987d":"from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.callbacks import ModelCheckpoint","dc0d5960":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel = tf.keras.Model(sequence_input, outputs)","5acbf647":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodel.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n              metrics=['accuracy'])\nReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n                                     min_lr = 0.01,\n                                     monitor = 'val_loss',\n                                     verbose = 1)","e2b678ab":"print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")","f5e239dd":"history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])","7afd3b75":"s, (at, al) = plt.subplots(2,1)\nat.plot(history.history['accuracy'], c= 'b')\nat.plot(history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n\nal.plot(history.history['loss'], c='m')\nal.plot(history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')","c1d537f5":"def decode_sentiment(score):\n    return \"Positive\" if score>0.5 else \"Negative\"\n\n\nscores = model.predict(x_test, verbose=1, batch_size=10000)\ny_pred_1d = [decode_sentiment(score) for score in scores]","8bd902ec":"import itertools\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=20)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, fontsize=13)\n    plt.yticks(tick_marks, classes, fontsize=13)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=17)\n    plt.xlabel('Predicted label', fontsize=17)","2572061f":"cnf_matrix = confusion_matrix(test_data.sentiment.to_list(), y_pred_1d)\nplt.figure(figsize=(6,6))\nplot_confusion_matrix(cnf_matrix, classes=test_data.sentiment.unique(), title=\"Confusion matrix\")\nplt.show()","c2761e9a":"print(classification_report(list(test_data.sentiment), y_pred_1d))","2e4583cd":"##  Importing Dependencies\n   We shall start by importing all the neccessary libraries. I will explain the exact use of each library later in this notebook.","066fad4e":"### Negative Words","22132dd4":"You can see the columns are without any proper names. Lets rename them for our reference","301849a5":"# Model Training - LSTM\nWe are clear to build our Deep Learning model. While developing a DL model, we should keep in mind of key things like Model Architecture, Hyperparmeter Tuning and Performance of the model.\n\nAs you can see in the word cloud, the some words are predominantly feature in both Positive and Negative tweets. This could be a problem if we are using a Machine Learning model like Naive Bayes, SVD, etc.. That's why we use **Sequence Models**.\n\n### Sequence Model\n![Sequence Model](https:\/\/miro.medium.com\/max\/1458\/1*SICYykT7ybua1gVJDNlajw.png)\n\nReccurent Neural Networks can handle a seqence of data and learn a pattern of input seqence to give either sequence or scalar value as output. In our case, the Neural Network outputs a scalar value prediction. \n\nFor model architecture, we use\n\n1) **Embedding Layer** - Generates Embedding Vector for each input sequence.\n\n2) **Conv1D Layer** - Its using to convolve data into smaller feature vectors. \n\n3) **LSTM** - Long Short Term Memory, its a variant of RNN which has memory state cell to learn the context of words which are at further along the text to carry contextual meaning rather than just neighbouring words as in case of RNN.\n\n4) **Dense** - Fully Connected Layers for classification\n","b0d89c51":"# Word Emdedding\nIn Language Model, words are represented in a way to intend more meaning and for learning the patterns and contextual meaning behind it. \n\n**Word Embedding** is one of the popular representation of document vocabulary.It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n\nBasically, it's a feature vector representation of words which are used for other natural language processing applications.\n\nWe could train the embedding ourselves but that would take a while to train and it wouldn't be effective. So going in the path of Computer Vision, here we use **Transfer Learning**. We download the pre-trained embedding and use it in our model.\n\nThe pretrained Word Embedding like **GloVe & Word2Vec** gives more insights for a word which can be used for classification. If you want to learn more about the Word Embedding, please refer some links that I left at the end of this notebook.\n\n\nIn this notebook, I use **GloVe Embedding from Stanford AI** which can be found [here](https:\/\/nlp.stanford.edu\/projects\/glove\/)","785a2e8f":"### Label Encoding \nWe are building the model to predict class in enocoded form (0 or 1 as this is a binary classification). We should encode our training labels to encodings.","12d887da":"**Aaww.. It is clean and tidy now. Now let's see some word cloud visualizations of it.**\n\n### Positive Words","2be6503e":"## Train and Test Split","b3f6778e":"### Confusion Matrix\nConfusion Matrix provide a nice overlook at the model's performance in classification task","1dc2096c":"It's a very good dataset without any skewness. Thank Goodness.\n\nNow let us explore the data we having here... ","77334d08":"Let's start training... It takes a heck of a time if training in CPU, be sure your GPU turned on... May the CUDA Cores be with you....","e298e14e":"It's a pretty good model we trained here in terms of NLP. Around 80% accuracy is good enough considering the baseline human accuracy also pretty low in these tasks. Also, you may go on and explore the dataset, some tweets might have other languages than English. So our Embedding and Tokenizing wont have effect on them. But on practical scenario, this model is good for handling most tasks for Sentiment Analysis.","a8d5affc":"# Natural Language Processing\n**Natural Language Processing or NLP** is a branch of Artificial Intelligence which deal with bridging the machines understanding humans in their Natural Language. Natural Language can be in form of text or sound, which are used for humans to communicate each other. NLP can enable humans to communicate to machines in a natural way. \n\n\n**Text Classification** is a process involved in Sentiment Analysis. It is classification of peoples opinion or expressions into different sentiments. Sentiments include *Positive, Neutral*, and *Negative*, *Review Ratings* and *Happy, Sad*. Sentiment Analysis can be done on different consumer centered industries to analyse people's opinion on a particular product or subject. \n![Sentiment Analysis](https:\/\/media-exp1.licdn.com\/dms\/image\/C4D12AQHPAZFZZxBtng\/article-cover_image-shrink_600_2000\/0?e=1593648000&v=beta&t=eQAR5WOihE2_ZCCAJbsgNyJlaI_GW7u8lDw45zGbfuU)\n> Sentiment Classification is a perfect problem in NLP for getting started in it. You can really learn a lot of concepts and techniques to master through doing project. Kaggle is a great place to learn and contribute your own ideas and creations. I learnt lot of things from other, now it's my turn to make document my project.\n\nI will go through all the key and fundament concepts of NLP and Sequence Models, which you will learn in this notebook. \n![Sentiment Analysis](https:\/\/fiverr-res.cloudinary.com\/images\/t_main1,q_auto,f_auto,q_auto,f_auto\/gigs\/121192228\/original\/677c209a0a064cb9253973d3663684acf91dab84\/do-nlp-projects-with-python-nltk-gensim.jpg)\nLet's get started with code without furthur ado.\n\n<font color='red'> If you find this notebook helpful, please leave a UPVOTE to encourage me<\/font>","46a6e151":"Now we got a `tokenizer` object, which can be used to covert any word into a Key in dictionary (number).\n\nSince we are going to build a sequence model. We should feed in a sequence of numbers to it. And also we should ensure there is no variance in input shapes of sequences. It all should be of same lenght. But texts in tweets have different count of words in it. To avoid this, we seek a little help from `pad_sequence` to do our job. It will make all the sequence in one constant length `MAX_SEQUENCE_LENGTH`.","d0db7ab5":"Looks like we have a nasty data in text. Because in general we use lot of punctuations and other words without any contextual meaning. It have no value as feature to the model we are training. So we need to get rid of them.\n\n# Text Preprocessing\nTweet texts often consists of other user mentions, hyperlink texts, emoticons and punctuations. In order to use them for learning using a Language Model. We cannot permit those texts for training a model. So we have to clean the text data using various preprocessing and cleansing methods. Let's continue\n![Data Science Meme](https:\/\/miro.medium.com\/max\/800\/1*Xhm9c9qDfXa3ZCQjiOvm_w.jpeg)\n","c5cbbce1":"Here are decoding the labels. We map **0 -> Negative and 1 -> Positive** as directed by the datset desciption. Now that we decoded we shall now analyse the dataset by its distribution. Because it's important that we have almost small amount of examples for given classes.","550ea5c5":"The model will output a prediction score between 0 and 1. We can classify two classes by defining a threshold value for it. In our case, I have set 0.5 as THRESHOLD value, if the score above it. Then it will be classified as **POSITIVE** sentiment.","8aabebdd":"#  Dataset Preprocessing\nIn this notebook, I am using **Sentiment-140** from [Kaggle](https:\/\/www.kaggle.com\/kazanova\/sentiment140). It contains a labels data of 1.6 Million Tweets and I find it a good amount of data to train our model.","95902574":"### Stemming\/ Lematization\nFor grammatical reasons, documents are going to use different forms of a word, such as *write, writing and writes.* Additionally, there are families of derivationally related words with similar meanings. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n\nStemming usually refers to a process that chops off the ends of words in the hope of achieving goal correctly most of the time and often includes the removal of derivational affixes. \n\nLemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base and dictionary form of a word\n![Stemming and Lematization](https:\/\/qph.fs.quoracdn.net\/main-qimg-cd7f4bafaa42639deb999b1580bea69f)\n\n### Hyperlinks and Mentions\nTwitter is a social media platform where people can tag and mentions other people's ID and share videos and blogs from internet. So the tweets often contain lots of Hyperlinks and twitter mentions.\n\n- Twitter User Mentions - Eg. @arunrk7, @andrewng\n- Hyperlinks - Eg. https:\/\/keras.io, https:\/\/tensorflow.org\n\n### Stopwords\nStopwords are commonly used words in English which have no contextual meaning in an sentence. So therefore we remove them before classification. Some stopwords are...\n![Stopwords English](https:\/\/4.bp.blogspot.com\/-yiEr-jCVv38\/Wmk10d84DYI\/AAAAAAAAk0o\/IfgjfjpgrxM5NosUQrGw7PtLvgr6DAG8ACLcBGAs\/s1600\/Screen%2BShot%2B2018-01-24%2Bat%2B5.41.21%2BPM.png)\n\nThat looks like a tedious process, isn't?. Don't worry there is always some library in Python to do almost any work. The world is great!!!\n\n**NLTK** is a python library which got functions to perform text processing task for NLP.\n\n","3fb69966":"`train_test_split` will shuffle the dataset and split it to gives training and testing dataset. It's important to shuffle our dataset before training.","2d7813d4":"### Classification Scores","f43831b1":"# Model Evaluation\nNow that we have trained the model, we can evaluate its performance. We will some evaluation metrics and techniques to test the model.\n\nLet's start with the Learning Curve of loss and accuracy of the model on each epoch.","9289a927":"### Optimization Algorithm\nThis notebook uses Adam, optimization algorithm for Gradient Descent. You can learn more about Adam [here](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/optimizers\/Adam)\n\n### Callbacks\nCallbacks are special functions which are called at the end of an epoch. We can use any functions to perform specific operation after each epoch. I used two callbacks here,\n\n- **LRScheduler** - It changes a Learning Rate at specfic epoch to achieve more improved result. In this notebook, the learning rate exponentionally decreases after remaining same for first 10 Epoch.\n\n- **ModelCheckPoint** - It saves best model while training based on some metrics. Here, it saves the model with minimum Validity Loss.","4e7af4ea":"<h3>Some of the resource and people who help me learn some concepts<\/h3>\n<font color='#008080'>\n    <ul>\n        <li> <b>Andrew NG's Seqence Model Course<\/b> at <a href=\"https:\/\/www.coursera.org\/learn\/nlp-sequence-models\"> Coursera<\/a> <\/li>\n    \n<li> <b>Andrej Karpathy's Blog<\/b> on <a href=\"http:\/\/karpathy.github.io\/2015\/05\/21\/rnn-effectiveness\/\">Effectiveness of RNN<\/a><\/li>\n\n<li> <b>Intuitive Understanding of GloVe Embedding<\/b> on <a href=\"https:\/\/towardsdatascience.com\/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010\">TDS<\/a><\/li>\n\n<li> <b>Keras tutorial on Word Embedding<\/b> <a href=\"https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html\"> here<\/a><\/li>\n\n<\/ul>\n<\/font>","a5869c20":"We are going to train only on text to classify its sentiment. So we can ditch the rest of the useless columns.","c832195c":"# Tokenization\nGiven a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called *tokens* , perhaps at the same time throwing away certain characters, such as punctuation. The process is called **Tokenization.**\n![Tokenization](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/11\/tokenization.png)\n\n`tokenizer` create tokens for every word in the data corpus and map them to a index using dictionary.\n\n`word_index` contains the index for each word\n\n`vocab_size` represents the total number of word in the data corpus"}}