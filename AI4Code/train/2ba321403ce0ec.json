{"cell_type":{"c0932712":"code","07b34e62":"code","ac58e031":"code","911b477d":"code","f78e453f":"code","95cb2ac2":"code","6915f8ac":"code","a13717c9":"code","e5254424":"code","bf06022b":"code","b35fd623":"code","ff4b14bd":"code","1eb603a2":"code","60cd2bfb":"code","122faa12":"code","1f48030c":"code","03beac94":"code","159538f1":"code","d1ae116a":"code","65608108":"code","5e169cee":"code","5bc3b169":"code","661e0df0":"code","e66d5d41":"code","6329c213":"code","e1480942":"code","66af9d04":"code","d664aab8":"code","9bfec500":"code","d7c2366a":"code","b22c785b":"code","1af9d8a4":"code","ca9e79ea":"code","b88d4d5e":"code","59cd3c86":"code","b0e50724":"code","5dac3238":"code","9605daed":"code","e8caef57":"markdown"},"source":{"c0932712":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas_profiling import ProfileReport\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","07b34e62":"##### importing data file into a variable dataframe df\ndf = pd.read_csv(\"..\/input\/headbrain\/headbrain.csv\")","ac58e031":"df","911b477d":"df.head()","f78e453f":"\"\"\"\nuse this to understand \n  1. multi-collinearity about features ( among X)\n  2. feature importance of each feature with target,( how much related to target)\n  3. do not use multi-collearnity with categories as it will make less sense.\n  4. note: do  not remove any feature initially\n\"\"\"\ndf.corr()","95cb2ac2":"df.columns","6915f8ac":"df.dtypes","a13717c9":"# eda - exploratory analysis\n\nprofile = ProfileReport(df)","e5254424":"profile.to_file(\"headbrain_data_EDA.html\")","bf06022b":"X = df.drop('Brain Weight(grams)' , axis = 1)\nY = df['Brain Weight(grams)']","b35fd623":"type(df)","ff4b14bd":"type(X)","1eb603a2":"#Features and data types\nX.dtypes","60cd2bfb":"X.head()","122faa12":"type(Y)","1f48030c":"Y.head()","03beac94":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=2)","159538f1":"X_train","d1ae116a":"Y_train","65608108":"from sklearn.linear_model import LinearRegression","5e169cee":"model = LinearRegression()\nmodel.fit(X_train,Y_train)","5bc3b169":"#### Y = MX +C\n#### brainwght =  m1*x1 + m2*x2 +m3*x3 + C","661e0df0":"# number of coef is equal to number of feature\n# every feature will have a coef\/ slope\nmodel.coef_","e66d5d41":"X_train.columns","6329c213":"model.intercept_","e1480942":"#brain weight = -16.87* gender + -24.5* Age + 0.24* Head Size + 456.62\n# head-size = 3000\n#age = 2\n#gender 2\n","66af9d04":"X_test","d664aab8":"pred = model.predict(X_test)","9bfec500":"len(pred)","d7c2366a":"X_test.shape","b22c785b":"print(pred)","1af9d8a4":"# load metrics for error calculation\nfrom sklearn.metrics import mean_absolute_error , mean_squared_error\n","ca9e79ea":"mae = mean_absolute_error(Y_test , pred)\nprint(\"mae = \",mae)","b88d4d5e":"mse = mean_squared_error(Y_test , pred)\nprint(\"mse = \",mse)","59cd3c86":"#root_mean_sqr error\nrmse = np.sqrt(mean_squared_error(Y_test , pred))\nprint(\"rmse = \",rmse)","b0e50724":"#R2 score - it helps to predict the variations in Y given x1,x2,x3 features\n# it does not have ability to determine overfit, it is rarely used in today world\nmodel.score(X_train, Y_train)\n\n# always use rmse, mse, mape , mae to measure your model","5dac3238":"# mean absolute percentage error\nmape1 = np.mean(np.abs((Y_test - pred)\/Y_test))*100","9605daed":"accuracy = 100- mape1\nprint(\"accuracy = \",accuracy)","e8caef57":"slope of a line with three features \/ dimensions\nY = m1 * x1 + m2 * x2 + m3 * x3 + intercept(c)"}}