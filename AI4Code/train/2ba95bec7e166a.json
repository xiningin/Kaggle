{"cell_type":{"7f842682":"code","3f5a5b00":"code","61c44e85":"code","c0760a82":"code","35e3ebcc":"code","38f7c697":"code","28825cc4":"code","a4b31a21":"code","afb95151":"code","70784ef9":"code","7079f60b":"code","e4a1c1e8":"code","94ad4be3":"code","ddceb78b":"code","cd2a9d0d":"code","ac725397":"code","df8bdd0d":"code","9fdde3f8":"code","dec05eec":"code","ae490e35":"code","d586935c":"code","341a34cf":"code","9af1eafc":"code","e0c322b7":"code","13ca4c9c":"code","c710da48":"code","f8dad7e0":"code","9f2b70fd":"code","1a503f28":"code","a8bdb244":"code","4b1e0c02":"code","56179f27":"code","c0d0b733":"code","45a2c8a2":"code","1f58b3d1":"code","40ed36fb":"code","0862e455":"code","32829cd2":"code","ac2b30b7":"code","5b38fc78":"code","e4a733c9":"code","04c54478":"code","cc36a040":"code","8ea14053":"code","787fbf76":"code","b051f66c":"code","a5ac63ea":"code","5d2a230a":"code","ae9e3cdb":"code","0b793d82":"code","77c25604":"code","e8050a71":"code","d9984451":"code","1e260471":"code","4f1848ab":"code","ee49b4f4":"code","945ce02d":"code","8bd305ba":"code","3057e212":"code","779b3a99":"code","e7f9da29":"code","ed30a813":"code","26ebc35b":"code","073a119b":"code","4c5ed5dc":"code","b05f19a2":"code","2b39bf3e":"code","68b21177":"code","35664b13":"code","44cccfb0":"code","66440e28":"code","2b4feb03":"code","c720804d":"code","6c90e1d5":"code","f52a340a":"code","e974019c":"code","7c1a10fc":"code","a8bcb64d":"code","e378d4a1":"code","3dea9a79":"code","9a66b355":"code","9212263a":"code","d6e7494f":"code","059ea4c8":"code","c9f384d5":"code","730c636c":"code","4fa7bc1e":"code","1a17a4b3":"code","79df33a6":"code","3bdf7526":"code","67d32042":"code","da2e18b1":"code","2c4ea900":"code","5f91adf8":"code","f44422c6":"code","0a89eac9":"code","890cc047":"code","35b8347a":"code","60f1d462":"code","3bc368d7":"code","07ad4a39":"code","418ddb35":"code","973ab163":"code","7d5bf80b":"code","ca356660":"code","6185cdb4":"code","bae67541":"code","a0dddcb6":"code","8c88ad15":"code","10d336bc":"code","c5148663":"code","0a3d29e7":"code","2a7b1438":"code","e1528adc":"code","0fe41483":"code","83269ee9":"code","3a84fb86":"code","4e9e8456":"code","217f5700":"code","1adee8f7":"code","0d8fc2e6":"markdown","56e81e55":"markdown","55f547b9":"markdown","37394d47":"markdown","973871dd":"markdown","6500aed2":"markdown","e54b9e9e":"markdown","ec8279ba":"markdown","6b7a7abe":"markdown","0820b22e":"markdown","d99bccd0":"markdown","40dfa086":"markdown","67a0452d":"markdown","cf4364e8":"markdown","d027d876":"markdown","2e04d7aa":"markdown","87966dce":"markdown","8dde1781":"markdown","29094a11":"markdown","8a3428bf":"markdown","13941b4b":"markdown","6225743f":"markdown","4d1c210b":"markdown"},"source":{"7f842682":"%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.decomposition import TruncatedSVD\n\n\n!pip install -q wordcloud\nimport wordcloud\n\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\n%load_ext autoreload\n%autoreload 2","3f5a5b00":"from sklearn.model_selection import GridSearchCV","61c44e85":"data = pd.read_csv('..\/input\/fake-news-dataset\/news.csv')","c0760a82":"data.head()","35e3ebcc":"data['label'].value_counts()   # No class imbalance","38f7c697":"data = data.rename(columns = {'Unnamed: 0': 'Id'})     # Replacing the unnamed column with 'Id'","28825cc4":"# Creating new feature : Number of words in news 'title'\n\ndef num_words(string):\n    words = string.split()\n    return len(words)\n\ndata['num_word_title'] = data['title'].apply(num_words)\n\nprint(data.groupby(data['label']).mean())\n\ncols = ['title','num_word_title','text', 'label']\ndata = data[cols]","a4b31a21":"data.head()","afb95151":"data[data['num_word_title']>25].groupby('label').count()    # This clearly shows if title length is more than 25, it's highly likely to be a fake news.","70784ef9":"# Function to split data into train and test set\ndef train_test_split(df, train_percent=.80, validate_percent=.20, seed=10):\n    np.random.seed(seed)\n    perm = np.random.permutation(df.index)\n    m = len(df.index)\n    train_end = int(train_percent * m)\n    train = df.iloc[perm[:train_end]]\n    test = df.iloc[perm[train_end:]]\n    return train, test","7079f60b":"train, test = train_test_split(data[['num_word_title','text','label']], seed = 12)","e4a1c1e8":"train.shape, test.shape","94ad4be3":"# Necessary for lemmatization\nclass LemmaTokenizer:\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self, doc):\n        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]","ddceb78b":"# CountVectorizer\ncount_vectorizer = CountVectorizer(stop_words = 'english', tokenizer=LemmaTokenizer(), \n                                   ngram_range = (1,2), dtype = np.uint8)\n\ncount_train = count_vectorizer.fit_transform(train['text'].values)","cd2a9d0d":"count_test = count_vectorizer.transform(test['text'].values)","ac725397":"\"\"\"\nWe won't use TfidfVectorizer. However, if any one wants to use it, pre-processing step is similar.\n# TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize a TfidfVectorizer object: tfidf_vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_df = 0.7, tokenizer=LemmaTokenizer(), \n                                   ngram_range = (1,2), dtype = np.float32)\n\n# Transform the training data: tfidf_train \ntfidf_train = tfidf_vectorizer.fit_transform(train['text'].values)\n# Transform the test data\ntfidf_test = tfidf_vectorizer.transform(test['text'].values)\n\"\"\"","df8bdd0d":"from sklearn.decomposition import TruncatedSVD","9fdde3f8":"lsa_count = TruncatedSVD(n_components = 400, random_state = 20)\nlsa_count.fit(count_train)\nprint(lsa_count.explained_variance_ratio_.sum())          # Explained_variance = 84.66 %","dec05eec":"#count_train_df = pd.DataFrame.sparse.from_spmatrix(count_train, columns = count_vectorizer.get_feature_names())","ae490e35":"count_train_lsa = pd.DataFrame(lsa_count.transform(count_train))\ncount_test_lsa = pd.DataFrame(lsa_count.transform(count_test))","d586935c":"# Adding number of words in news title as a feature\ncount_train_lsa['num_word_title'] = train['num_word_title'] \/ data['num_word_title'].max()\ncount_test_lsa['num_word_title'] = test['num_word_title'] \/ data['num_word_title'].max()","341a34cf":"count_train_lsa.fillna(count_train_lsa.mean(), inplace = True)\ncount_test_lsa.fillna(count_test_lsa.mean(), inplace = True)","9af1eafc":"count_train_lsa.shape","e0c322b7":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns","13ca4c9c":"# Trying out with Gaussian Naive Bayes and CountVectorizer model\n# Here we are iteratively doing grid search with different hyperparameters in an informed manner.\n# params_NB = {'var_smoothing' : [1e-1, 1, 40, 100, 1000]} # var_smoothing = 40 gives the best result.\nparams_NB = {'var_smoothing' : [20,25,30,35,40,45, 50, 55, 60, 80]} # var_smoothing = 55 gives the best result.\n\n","c710da48":"clf_NB = GridSearchCV(estimator = GaussianNB(),param_grid = params_NB, cv = 3, refit = True, scoring = 'accuracy', n_jobs = 4)","f8dad7e0":"clf_NB.fit(count_train_lsa, train['label'])","9f2b70fd":"clf_NB.best_params_","1a503f28":"clf_NB.best_score_","a8bdb244":"test_count_pred_NB = clf_NB.predict(count_test_lsa)","4b1e0c02":"accuracy_score(test['label'], test_count_pred_NB)","56179f27":"cm_NB = confusion_matrix(test['label'], test_count_pred_NB, labels = ['FAKE', 'REAL'])\nsns.heatmap(cm_NB\/ np.sum(cm_NB),fmt='.2%', annot=True, cmap = 'Blues', xticklabels = ['FAKE', 'REAL'], yticklabels = ['FAKE', 'REAL'])\n","c0d0b733":"from sklearn.metrics import classification_report\n\ncount_report_NB = classification_report(test['label'], test_count_pred_NB, labels = ['FAKE','REAL'], output_dict = True)\ncount_report_NB = pd.DataFrame(count_report_NB).transpose()\ncount_report_NB","45a2c8a2":"from sklearn.linear_model import LogisticRegression\n","1f58b3d1":"params_LR = {'C' : [10, 5, 1,0.7, 0.5,0.3]}\n\n# C = 0.5 gives the best result after Grid Search.\n\n","40ed36fb":"clf_LR = GridSearchCV(estimator = LogisticRegression(class_weight = 'balanced', random_state = 6),param_grid = params_LR, \n                      cv = 3, refit = True, scoring = 'accuracy', n_jobs = 4)","0862e455":"clf_LR.fit(count_train_lsa, train['label'])","32829cd2":"clf_LR.best_score_","ac2b30b7":"clf_LR.best_params_","5b38fc78":"test_count_pred_LR = clf_LR.predict(count_test_lsa)","e4a733c9":"cm_LR = confusion_matrix(test['label'], test_count_pred_LR, labels = ['FAKE', 'REAL'])\nsns.heatmap(cm_LR\/ np.sum(cm_LR),fmt='.2%', annot=True, cmap = 'Blues', xticklabels = ['FAKE', 'REAL'], yticklabels = ['FAKE', 'REAL'])\n","04c54478":"from sklearn.metrics import classification_report\n\ncount_report_LR = classification_report(test['label'], test_count_pred_LR, labels = ['FAKE','REAL'], output_dict = True)\ncount_report_LR = pd.DataFrame(count_report_LR).transpose()\ncount_report_LR","cc36a040":"from sklearn.neighbors import KNeighborsClassifier","8ea14053":"# Here we are iteratively doing grid search with different hyperparameters in an informed manner.\n# params_knn = {'n_neighbors' : [2, 4, 8, 16] } # We obtain 4 as the best hyperparameter.\nparams_knn = {'n_neighbors' : [3,4,5,6,7] }     # 5 is the final tuned hyperparameter.","787fbf76":"clf_knn = GridSearchCV(estimator = KNeighborsClassifier(algorithm = 'ball_tree'), param_grid = params_knn, \n                       scoring = 'accuracy', n_jobs = 4, cv = 3, refit = True, verbose = 3)","b051f66c":"clf_knn.fit(count_train_lsa, train['label'])","a5ac63ea":"clf_knn.best_score_","5d2a230a":"clf_knn.best_params_","ae9e3cdb":"test_count_pred_knn = clf_knn.predict(count_test_lsa)","0b793d82":"cm_knn = confusion_matrix(test['label'], test_count_pred_knn, labels = ['FAKE', 'REAL'])\nsns.heatmap(cm_knn\/ np.sum(cm_knn),fmt='.2%', annot=True, cmap = 'Blues', xticklabels = ['FAKE', 'REAL'], yticklabels = ['FAKE', 'REAL'])\n","77c25604":"from sklearn.metrics import classification_report\n\ncount_report_knn = classification_report(test['label'], test_count_pred_knn, labels = ['FAKE','REAL'], output_dict = True)\ncount_report_knn = pd.DataFrame(count_report_knn).transpose()\ncount_report_knn","e8050a71":"from sklearn.svm import SVC","d9984451":"# Here we are iteratively doing grid search with different hyperparameters in an informed manner.\n# params_svc = {'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'], 'C' : [0.1, 1, 50]} # 'rbf' and 50 give the best combination\n# params_svc = {'kernel' : ['rbf', 'sigmoid'], 'C' : [10, 30, 50,100]}  # 100 and 'rbf' give the best combination.\nparams_svc = {'kernel' : ['rbf', 'sigmoid'], 'C' : [100, 150, 200]}   # 100 and 'rbf' give the best combination.","1e260471":"clf_svc = GridSearchCV(estimator = SVC(class_weight = 'balanced', random_state = 6), param_grid = params_svc, \n                       scoring = 'accuracy', n_jobs = 4, cv = 3, refit = True, verbose = 2)","4f1848ab":"clf_svc.fit(count_train_lsa, train['label'])","ee49b4f4":"clf_svc.best_params_","945ce02d":"clf_svc.best_score_","8bd305ba":"test_count_pred_svc = clf_svc.predict(count_test_lsa)","3057e212":"cm_svc = confusion_matrix(test['label'], test_count_pred_svc, labels = ['FAKE', 'REAL'])\nsns.heatmap(cm_svc\/ np.sum(cm_svc),fmt='.2%', annot=True, cmap = 'Blues', xticklabels = ['FAKE', 'REAL'], yticklabels = ['FAKE', 'REAL'])\n","779b3a99":"from sklearn.metrics import classification_report\n\ncount_report_svc = classification_report(test['label'], test_count_pred_svc, labels = ['FAKE','REAL'], output_dict = True)\ncount_report_svc = pd.DataFrame(count_report_svc).transpose()\ncount_report_svc","e7f9da29":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis","ed30a813":"params_LDA = {'solver' : ['svd', 'lsqr', 'eigen'], 'shrinkage' : ['auto', None]}\n\n# (shrinkage = None and solver = svd) give the best result.","26ebc35b":"clf_LDA = GridSearchCV(estimator = LinearDiscriminantAnalysis(), param_grid = params_LDA, scoring = 'accuracy', n_jobs = 4,\n                       cv = 3, refit = True, verbose = 2)","073a119b":"clf_LDA.fit(count_train_lsa, train['label'])","4c5ed5dc":"clf_LDA.best_params_","b05f19a2":"clf_LDA.best_score_","2b39bf3e":"test_count_pred_LDA = clf_LDA.predict(count_test_lsa)","68b21177":"cm_LDA = confusion_matrix(test['label'], test_count_pred_LDA, labels = ['FAKE', 'REAL'])\nsns.heatmap(cm_LDA\/ np.sum(cm_LDA),fmt='.2%', annot=True, cmap = 'Blues', xticklabels = ['FAKE', 'REAL'], yticklabels = ['FAKE', 'REAL'])\n","35664b13":"from sklearn.metrics import classification_report\n\ncount_report_LDA = classification_report(test['label'], test_count_pred_LDA, labels = ['FAKE','REAL'], output_dict = True)\ncount_report_LDA = pd.DataFrame(count_report_LDA).transpose()\ncount_report_LDA","44cccfb0":"from sklearn.tree import DecisionTreeClassifier","66440e28":"params_dt = {'criterion' : ['entropy'], 'min_samples_split' : [2, 4, 8, 16, 32], \n             'min_samples_leaf' : [1,2,4,8],'max_depth' : [4, 7, 10], 'max_features' : ['sqrt', None],  'class_weight' : ['balanced']}\n\n# (max_depth = 7, min_samples_leaf = 4, min_samples_split = 32, max_features = None) give the best result.","2b4feb03":"clf_dt = GridSearchCV(estimator = DecisionTreeClassifier(random_state = 7), param_grid = params_dt, \n                      scoring = 'accuracy', n_jobs = 4, cv = 3, refit = True, verbose = 3)","c720804d":"clf_dt.fit(count_train_lsa, train['label'])","6c90e1d5":"clf_dt.best_params_","f52a340a":"clf_dt.best_score_","e974019c":"test_count_pred_dt = clf_dt.predict(count_test_lsa)","7c1a10fc":"cm_dt = confusion_matrix(test['label'], test_count_pred_dt, labels = ['FAKE', 'REAL'])\nsns.heatmap(cm_dt\/ np.sum(cm_dt),fmt='.2%', annot=True, cmap = 'Blues', xticklabels = ['FAKE', 'REAL'], yticklabels = ['FAKE', 'REAL'])\n","a8bcb64d":"from sklearn.metrics import classification_report\n\ncount_report_dt = classification_report(test['label'], test_count_pred_dt, labels = ['FAKE','REAL'], output_dict = True)\ncount_report_dt = pd.DataFrame(count_report_dt).transpose()\ncount_report_dt","e378d4a1":"from sklearn.ensemble import RandomForestClassifier","3dea9a79":"params_RF = {'n_estimators' : [400, 1000, 1600], 'criterion' : ['entropy'], 'min_samples_split' : [2, 4, 8, 16], \n             'min_samples_leaf' : [1,2], 'class_weight' : ['balanced']}  \n\n# (min_samples_leaf = 1, min_samples_split = 4 , n_estimators = 1000) give the best result.","9a66b355":"clf_RF = GridSearchCV(estimator = RandomForestClassifier(oob_score = True, random_state = 7), param_grid = params_RF, \n                      scoring = 'accuracy', n_jobs = 4, cv = 3, refit = True, verbose = 2)","9212263a":"clf_RF.fit(count_train_lsa, train['label'])","d6e7494f":"clf_RF.best_params_","059ea4c8":"clf_RF.best_score_","c9f384d5":"test_count_pred_RF = clf_RF.predict(count_test_lsa)","730c636c":"cm_RF = confusion_matrix(test['label'], test_count_pred_RF, labels = ['FAKE', 'REAL'])\nsns.heatmap(cm_RF\/ np.sum(cm_RF),fmt='.2%', annot=True, cmap = 'Blues', xticklabels = ['FAKE', 'REAL'], yticklabels = ['FAKE', 'REAL'])\n","4fa7bc1e":"from sklearn.metrics import classification_report\n\ncount_report_RF = classification_report(test['label'], test_count_pred_RF, labels = ['FAKE','REAL'], output_dict = True)\ncount_report_RF = pd.DataFrame(count_report_RF).transpose()\ncount_report_RF","1a17a4b3":"from sklearn.ensemble import AdaBoostClassifier","79df33a6":"# Here we are iteratively doing grid search with different hyperparameters in an informed manner.\n# params_ada = {'n_estimators' : [50, 100, 500], 'learning_rate' : [1, 0.3]} # 500 and 1 give the best combination.\nparams_ada = {'n_estimators' : [500, 1000, 1500, 2000], 'learning_rate' : [1]} \n# (n_estimators = 1500, learning_rate = 1) is the best combination.","3bdf7526":"clf_ada = GridSearchCV(estimator = AdaBoostClassifier(random_state = 8), param_grid = params_ada, \n                       scoring = 'accuracy', n_jobs = 4, cv = 3, refit = True, verbose = 2)","67d32042":"clf_ada.fit(count_train_lsa, train['label'])","da2e18b1":"clf_ada.best_params_","2c4ea900":"clf_ada.best_score_","5f91adf8":"test_count_pred_ada = clf_ada.predict(count_test_lsa)","f44422c6":"cm_ada = confusion_matrix(test['label'], test_count_pred_ada, labels = ['FAKE', 'REAL'])\nsns.heatmap(cm_ada\/ np.sum(cm_ada),fmt='.2%', annot=True, cmap = 'Blues', xticklabels = ['FAKE', 'REAL'], yticklabels = ['FAKE', 'REAL'])\n","0a89eac9":"from sklearn.metrics import classification_report\n\ncount_report_ada = classification_report(test['label'], test_count_pred_ada, labels = ['FAKE','REAL'], output_dict = True)\ncount_report_ada = pd.DataFrame(count_report_ada).transpose()\ncount_report_ada","890cc047":"import lightgbm as lgb","35b8347a":"# Here we are iteratively doing grid search with different hyperparameters in an informed manner.\n#params_lgb = {'n_estimators' : [100, 400, 800], 'learning_rate' : [0.03, 0.1], 'min_child_samples' : [4, 12, 24]}\n# 800, 0.1 , 4 are the best ones.\n#params_lgb = {'n_estimators' : [1200, 2400], 'learning_rate' : [ 0.1], 'min_child_samples' : [2,4]}  \n# (n_estimators = 2400 and min_child_samples = 2) are the best ones.\n# Let's just try with n_estimators 3600.\nparams_lgb = {'n_estimators' : [3600], 'learning_rate' : [ 0.1], 'min_child_samples' : [1,2]} \n# (n_estimators = 3600, learning_rate = 0.1, min_child_samples = 1) perform best.","60f1d462":"clf_lgb = GridSearchCV(estimator = lgb.LGBMClassifier(), param_grid = params_lgb, scoring = 'accuracy', n_jobs = 4,\n                       cv = 3, refit = True, verbose = 2)","3bc368d7":"clf_lgb.fit(count_train_lsa, train['label'])","07ad4a39":"clf_lgb.best_params_","418ddb35":"print(clf_lgb.best_score_)","973ab163":"test_count_pred_lgb = clf_lgb.predict(count_test_lsa)   \n\n","7d5bf80b":"cm_lgb = confusion_matrix(test['label'], test_count_pred_lgb, labels = ['FAKE', 'REAL'])\nsns.heatmap(cm_lgb\/ np.sum(cm_lgb),fmt='.2%', annot=True, cmap = 'Blues', xticklabels = ['FAKE', 'REAL'], yticklabels = ['FAKE', 'REAL'])\n","ca356660":"from sklearn.metrics import classification_report\n\ncount_report_lgb = classification_report(test['label'], test_count_pred_lgb, labels = ['FAKE','REAL'], output_dict = True)\ncount_report_lgb = pd.DataFrame(count_report_lgb).transpose()\ncount_report_lgb","6185cdb4":"import catboost as cb","bae67541":"params_cat = {'n_estimators' : [800,1000,2000] }   #  2000 is the best one","a0dddcb6":"clf_cat = GridSearchCV(estimator = cb.CatBoostClassifier(task_type = 'GPU', learning_rate = 0.2, max_depth = 6), param_grid = params_cat,\n                       scoring = 'accuracy', n_jobs = 1, cv = 3, refit = True, verbose = 2 )","8c88ad15":"clf_cat.fit(count_train_lsa, train['label'])","10d336bc":"clf_cat.best_params_","c5148663":"test_count_pred_cat = clf_cat.predict(count_test_lsa)\n","0a3d29e7":"cm_cat = confusion_matrix(test['label'], test_count_pred_cat, labels = ['FAKE', 'REAL'])\nsns.heatmap(cm_cat\/ np.sum(cm_cat),fmt='.2%', annot=True, cmap = 'Blues', xticklabels = ['FAKE', 'REAL'], yticklabels = ['FAKE', 'REAL'])\n","2a7b1438":"from sklearn.metrics import classification_report\n\ncount_report_cat = classification_report(test['label'], test_count_pred_cat, labels = ['FAKE','REAL'], output_dict = True)\ncount_report_cat = pd.DataFrame(count_report_cat).transpose()\ncount_report_cat","e1528adc":"from tpot import TPOTClassifier","0fe41483":"clf_tpot = TPOTClassifier(generations = 6, population_size = 6, random_state = 3, cv = 2, n_jobs = 4)","83269ee9":"clf_tpot.fit(count_train_lsa, train['label'])","3a84fb86":"clf_tpot.fitted_pipeline_","4e9e8456":"test_count_pred_tpot = clf_tpot.predict(count_test_lsa)","217f5700":"cm_tpot = confusion_matrix(test['label'], test_count_pred_tpot, labels = ['FAKE', 'REAL'])\nsns.heatmap(cm_tpot\/ np.sum(cm_tpot),fmt='.2%', annot=True, cmap = 'Blues', xticklabels = ['FAKE', 'REAL'], yticklabels = ['FAKE', 'REAL'])\n","1adee8f7":"from sklearn.metrics import classification_report\n\ncount_report_tpot = classification_report(test['label'], test_count_pred_tpot, labels = ['FAKE','REAL'], output_dict = True)\ncount_report_tpot = pd.DataFrame(count_report_tpot).transpose()\ncount_report_tpot","0d8fc2e6":"## 3.8. AdaBoost Classifier","56e81e55":"## 3.4. Support Vector Machine Classifier","55f547b9":"## 3.5. Linear Discriminant Analysis","37394d47":"## 3.1. Naive-Bayes Model","973871dd":"## 2.1. Train-Test Split","6500aed2":"# 1. Importing Libraries","e54b9e9e":"## 2.3. Latent Semantic Analysis (For Dimensionality Reduction)","ec8279ba":"## 3.2. Logistic Regression","6b7a7abe":"We observe that boosting techniques like LGBM, CatBoost and AdaBoost gave good results. Also some linear techniques like Logistic Regression and SVM performed well.","0820b22e":"# 2. Data Pre-processing","d99bccd0":"## 3.6. Decision Tree Classifier","40dfa086":"Steps involved in fake news detection can be described as below. <center>Background Information --- Data Pre-processing --- Model Building<center>","67a0452d":"## 2.2. Feature Generation (CountVectorizer)","cf4364e8":"<h1><center>Fake News Detection with NLP and ML Models<\/center><\/h1>","d027d876":"## 3.10. CatBoosting","2e04d7aa":"**1. Background Information :** Research suggests that fake news articles typically have longer titles and their contents are repetitive in nature.<br>\n**2. Data Pre-Processing :** Based on the hypothesis, we pre-processed the data so as to use number of words in title and count of words in text as features. (Using CountVectorizer)<br>\n**3. Model Building :** Multiple machine learning models (Logistic Regression, Random Forest, Light Gradient Boosting etc.) were built and hyperparameters were tuned.","87966dce":"## 3.3. K-Nearest Neighbor Method","8dde1781":"Fake news detection is an important classification problem in machine learning community. In this article, we present the implementation of fake news detection with 11 different ML models and compare their results. This notebook will definitely be helpful for beginners. Kindly upvote if you find it useful.","29094a11":"## 3.9. Light Gradient Boosting Method ","8a3428bf":"# 3. Model Building","13941b4b":"So, let's get started.","6225743f":"## 3.7. Random Forest Classifier","4d1c210b":"## 3.11. TPOT Classifier (Genetic Algorithm)"}}