{"cell_type":{"837a977b":"code","161377d9":"code","f73ca7e5":"code","d990e4b5":"code","0b886d35":"code","248c975f":"code","74a554cd":"code","186f754c":"code","db3c2359":"code","5bc1c6ce":"code","03987112":"code","18ee05b3":"code","23dd7011":"code","07afb253":"code","afb7244b":"code","03a413cf":"code","65b36125":"code","2f84cbcb":"code","9d757de3":"code","47460e7c":"code","0739338d":"code","3cdb7cc4":"code","4cbcd4c1":"code","c799f392":"code","f46c2aec":"code","fd0203c8":"code","dc882c35":"code","b8ec065a":"code","70225162":"code","7e69dfb5":"code","72478654":"markdown","79ad0253":"markdown","9de7deee":"markdown","919c6ae5":"markdown","4aa8d732":"markdown","1ce9d95b":"markdown","d129e999":"markdown","6a8e4027":"markdown","af4c5b9b":"markdown","532bc051":"markdown"},"source":{"837a977b":"#!pip install --upgrade pip\n!pip install tensorflow --upgrade\n","161377d9":"#!pip uninstall tensorflow","f73ca7e5":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing import sequence, text\nfrom keras.layers import Input, Embedding\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nfrom collections import Counter #counting of words in the texts\nimport operator\nimport datetime as dt\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport string\nimport re\nwarnings.filterwarnings('ignore')","d990e4b5":"print(tf.__version__)","0b886d35":"#load data as tf.data.Dataset\nseed=42\ndata_paths = '..\/input\/sanad-dataset'\nlabels=os.listdir(data_paths) \nraw_data = tf.keras.preprocessing.text_dataset_from_directory(\n    data_paths,\n    labels=\"inferred\",\n    label_mode=\"int\",\n    #class_names=classes,\n    #batch_size=1,\n    max_length=None,\n    shuffle=True,\n    seed=seed,\n    validation_split=None,\n    subset=None,\n    follow_links=False,\n)","248c975f":"print(\"Article classes are:\\n\",raw_data.class_names)","74a554cd":"x=[]\ny=[]\nfor text_batch, label_batch in raw_data:\n    for i in range(len(text_batch)):\n        s=text_batch.numpy()[i].decode(\"utf-8\") \n        x.append(s)\n        y.append(raw_data.class_names[label_batch.numpy()[i]])\n        #print(label_batch.numpy()[i])\nprint(len(x))\nprint(len(y))","186f754c":"unique, counts = np.unique(y, return_counts=True)\nplt.figure(\"classe Pie\", figsize=(10, 10))\nplt.title(\"Pie plot of the class frequencies\")\nplt.pie(counts, labels=labels)\nplt.legend(unique)\nplt.show();","db3c2359":"plt.bar( labels,counts)\nplt.show();","5bc1c6ce":"#convert to DataFrame for EDA flexibility\ndata =pd.DataFrame({\"text\":x,\"label\":y}) \nstop_words = list(set(stopwords.words('arabic')))\nprint(stop_words)\n","03987112":"#Function count stop words in text\ndef word_count(text, word_list):\n    count_w = dict()\n    for w in word_list:\n        count_w[w] = 0\n        words = text.lower().split()\n        for word in words:\n            _word = word.strip('.,:-)()')\n            if _word in count_w:\n                count_w[_word] +=1\n\n    return count_w","18ee05b3":"#Count the stop word distribution in x=1000 examples of the dataset\nlst=[]\nsample=1000\nA=word_count(data['text'][1],stop_words)\nlst=list(A.values())[:]\nfor  i in range(1,sample):\n    A=word_count(data['text'][i],stop_words)\n    for j in range(len(A)-1):\n        lst[j]=lst[j]+list(A.values())[j]","23dd7011":"#visualize the distribution of \nplt.figure(\"stop words Pie\", figsize=(10, 10))\nplt.pie(lst, labels=stop_words)\nplt.show();\n#We conclude that the most stop words used in this dataset are:\"\u0625\u0644\u0649-\u0645\u0646-\u0641\u064a-\u0623\u0646-\u0639\u0644\u0649\". This to indicate place and causality.","07afb253":"# # Feature extraction\/Text Mining\n#Count features:\n#Feature 1: Count the number of words in each example.\n\n#Feature 2: Count the number of characters in each example.\n\n#Feature 3: Average length of words used in statement.\n\n#Feature 4: Count stop word per text.\n\n#Feature 5: Getting top 50 used words.\n\n#Feature 6: Count of punctuations in the input.\n\n\n\n#Function for removing punctuations from string\ndef remove_punctuations_from_string(string1):\n    string1 = string1.lower() #changing to lower case\n    translation_table = dict.fromkeys(map(ord, string.punctuation), ' ') #creating dictionary of punc & None\n    string2 = string1.translate(translation_table) #translating string1\n    return string2\n#Function for removing stopwords.\ndef remove_stopwords_from_string(string1):\n    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('arabic')) + r')\\b\\s*') #compiling all stopwords.\n    string2 = pattern.sub('', string1) #replacing the occurrences of stopwords in string1\n    return string2","afb7244b":"#Lets take backup of un-processed text, we might need it for future functions:\ndata['text_backup']=data['text']\ndata.head()\n#We need to remove the stop words and the punctuation in the text:\ndata[\"text\"] = data[\"text\"].apply(lambda x:remove_punctuations_from_string(x))\ndata[\"text\"] = data[\"text\"].apply(lambda x:remove_stopwords_from_string(x))\nprint(data[\"text\"][1])","03a413cf":"#Feature 1: Count the number of words in each example:\ndata['Feature_1']= data[\"text_backup\"].apply(lambda x: len(str(x).split()))\n#Feature 2: Count the number of characters in each example:\ndata['Feature_2']= data[\"text_backup\"].apply(lambda x: len(str(x)))\n#Feature 3: Average length of words used in statement\ndata['Feature_3']= data[\"Feature_2\"]\/data['Feature_1']\ndata.head()\n","65b36125":"#Feature 4: Count stop word per text\ndata['Feature_4'] = data[\"text_backup\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\ndata.head()","2f84cbcb":"#Getting top 50 used word in the dataset and thier frequency in each sample.\nall_text_without_sw = ''\nfor i in data.itertuples():\n    all_text_without_sw = all_text_without_sw +  str(i.text)\n#getting counts of each word:\ncounts = Counter(re.findall(r\"[\\w']+\", all_text_without_sw))\n#deleting ' from counts\ndel counts[\"'\"]\n#getting top 50 used words:\nsorted_x = dict(sorted(counts.items(), key=operator.itemgetter(1),reverse=True)[:50])","9d757de3":" #Feature 5:getting top 50 used words:\ndata['Feature_5'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in sorted_x]))\ndata.head()","47460e7c":"#Feature 6: Count of punctuations in the input.\ndata['Feature_6'] = data['text_backup'].apply(lambda x: len([w for w in str(x) if w in string.punctuation]) )\ndata.head()","0739338d":"#Lets visualize these 3 last extracted features:\n\n                    #Feature 4: Count stop word per text.\n\n                    #Feature 5: Getting top 50 used words.\n\n                    #Feature 6: Count of punctuations in the input\n            \n            \ndef plot_bar_chart_from_dataframe(dataframe1,key_column,columns_to_be_plotted):\n    import pandas as pd\n    test_df1 = dataframe1.groupby(key_column).sum()\n    test_df2 = pd.DataFrame()\n    for column in columns_to_be_plotted:\n        test_df2[column] = round(test_df1[column]\/ test_df1[column].sum()*100,2)\n    test_df2 = test_df2.T \n    \n    ax = test_df2.plot(kind='bar', stacked=True, figsize =(10,5),legend = 'reverse',title = '% Distribution over classes')\n    for p in ax.patches:\n        a = p.get_x()+0.4\n        ax.annotate(str(p.get_height()), (a, p.get_y()), xytext=(5, 10), textcoords='offset points')\n\nkey_column = 'label'\ncolumns_to_be_plotted = ['Feature_4','Feature_5','Feature_6']\nplot_bar_chart_from_dataframe(data,key_column,columns_to_be_plotted)","3cdb7cc4":"#Feature 7: Count of Most words start with.\n\nstarting_words = sorted(list(map(lambda word : word[:2],filter(lambda word : len(word) > 3,all_text_without_sw.split()))))\nsw_counts = Counter(starting_words)\ntop_30_sw = dict(sorted(sw_counts.items(), key=operator.itemgetter(1),reverse=True)[:30])\ndata['Feature_7'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w[:2] in top_30_sw and w not in stop_words]) )\n\n#Feature 8: Count of Most words end with.\nending_words = sorted(list(map(lambda word : word[-2:],filter(lambda word : len(word) > 3,all_text_without_sw.split()))))\new_counts = Counter(ending_words)\ntop_30_ew = dict(sorted(sw_counts.items(), key=operator.itemgetter(1),reverse=True)[:30])\ndata['Feature_8'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w[:2] in top_30_ew and w not in stop_words]) )\ndata.head()","4cbcd4c1":"tokenized_all_text = word_tokenize(all_text_without_sw) #tokenize the text\nlist_of_tagged_words = nltk.pos_tag(tokenized_all_text) #adding POS Tags to tokenized words\nset_pos  = (set(list_of_tagged_words))                  # set of POS tags & words\nset_pos","c799f392":"nouns = ['NN','NNS','NNP','NNPS'] #POS tags of nouns\nlist_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  nouns, set_pos)))\ndata['Feature_9'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n","f46c2aec":"pronouns = ['PRP','PRP$','WP','WP$'] # POS tags of pronouns\nlist_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  pronouns, set_pos)))\ndata['Feature_10'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )","fd0203c8":"verbs = ['VB','VBD','VBG','VBN','VBP','VBZ'] #POS tags of verbs\nlist_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  verbs, set_pos)))\ndata['Feature_11'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n","dc882c35":"adverbs = ['RB','RBR','RBS','WRB'] #POS tags of adverbs\nlist_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  adverbs, set_pos)))\ndata['Feature_12'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n","b8ec065a":"adjectives = ['JJ','JJR','JJS'] #POS tags of adjectives\nlist_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  adjectives, set_pos)))\ndata['Feature_13'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n","70225162":"data.head()","7e69dfb5":"data.to_csv(r'data.csv', index = False)","72478654":"# ## Exploratory Analysis:\nSANAD: Single-Label Arabic News Articles Dataset for Automatic Text Categorization.\nhttps:\/\/data.mendeley.com\/datasets\/57zpx667y9\/2","79ad0253":"#Getting top 50 used word in the dataset and thier frequency in each sample.","9de7deee":"**Religious** through \"\u0627\u0644\u0644\u0647\"\/God's word with 81822 ocurences and **Fianance** via '\u062f\u0631\u0647\u0645' with 66785 occurences are the most used words wich can inform about the main topics discussed in these news article:**** **Religion** and **Finance** are the most discussed topics.   ","919c6ae5":"**Religious** through \"\u0627\u0644\u0644\u0647\"\/God's word with 81822 ocurences and **Fianance** via '\u062f\u0631\u0647\u0645' with 66785 occurences are the most used words wich can inform about the main topics discussed in these news article:**** **Religion** and **Finance** are the most discussed topics.  ","4aa8d732":"#We conclude that the most stop words used in this dataset are:\"\u0625\u0644\u0649-\u0645\u0646-\u0641\u064a-\u0623\u0646-\u0639\u0644\u0649\". This to indicate place and causality. ","1ce9d95b":"***Load data***","d129e999":"# Lets visualize these 3 last extracted features:\n #Feature 4: Count stop word per text.\n #Feature 5: Getting top 50 used words.\n #Feature 6: Count of punctuations in the input.\n","6a8e4027":"# #We can clearly observe that the dataset is balanced","af4c5b9b":"# #Feature tagging:","532bc051":"# # # Feature extraction\/Text Mining\n ## Count features:\n #Feature 1: Count the number of words in each example.\n #Feature 2: Count the number of characters in each example.\n #Feature 3: Average length of words used in statement.\n #Feature 4: Count stop word per text.\n #Feature 5: Getting top 50 used words.\n #Feature 6: Count of punctuations in the input.\n #Feature 7: Count of Most words start with.\n #Feature 8: Count of Most words end with.\n \n\n\n "}}