{"cell_type":{"dcaa90aa":"code","f25bfe8d":"code","6fbca374":"code","7d473205":"code","5a4ace66":"code","78e08e6f":"code","cf9adc7e":"code","e490641b":"code","fe8b169f":"code","cccf375c":"code","098d8fdd":"code","bd9fa135":"code","e5d415e5":"code","279b79ce":"code","257836fe":"code","abb6e0db":"code","e122e60d":"code","2b62f5f2":"code","fc232bf7":"code","48e7bbfb":"code","cae60841":"code","0bb630fc":"code","74034325":"code","eec4b475":"code","4d8b79ba":"code","8a77d5ef":"code","b28e3a7c":"code","cffc9ab7":"code","739ed06c":"code","892ed4b6":"code","7ffe3865":"code","fa9424e9":"code","78f7525c":"code","8501f2ad":"code","52ccd51b":"code","2a70ff2e":"code","b2f6d74c":"code","3fb3a03d":"markdown","3ae60f0a":"markdown","3b71bbd1":"markdown","01c5a79e":"markdown","33834a55":"markdown","8e8422a2":"markdown","2a42dd85":"markdown","fd164b77":"markdown","2435e160":"markdown","0638560b":"markdown","dabb73a6":"markdown","86f181ed":"markdown","3a4ddb6c":"markdown","a45f92e3":"markdown","5d90e3aa":"markdown","ae1a8fd8":"markdown","776bd988":"markdown","5baabd45":"markdown","66c6d3bd":"markdown","179b3fef":"markdown","16e23e38":"markdown","54025271":"markdown","e9330790":"markdown","be475a72":"markdown","1ad87e3b":"markdown","d058b994":"markdown","f820e588":"markdown","e82e6b96":"markdown","ea9a7b53":"markdown","5780396e":"markdown","922cb60e":"markdown","e2c2220e":"markdown","38d7e17c":"markdown","5325ec6b":"markdown","b5efb019":"markdown","126bec82":"markdown","6368e6f0":"markdown","1ec881fd":"markdown","8571d24d":"markdown","bcd1fc1d":"markdown","1f12749d":"markdown","e8f5bd83":"markdown","2fa7551b":"markdown","4e7c5391":"markdown","49da3c9c":"markdown","5b0eba0e":"markdown","714942eb":"markdown","4166f9f2":"markdown"},"source":{"dcaa90aa":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # for data visualization\nimport matplotlib.pyplot as plt # to plot charts\nfrom collections import Counter\nimport os\n\n# Modeling\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, train_test_split\n\n\n# Directory Structure\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f25bfe8d":"df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","6fbca374":"# Get familier with dataset structure\n\ndf.info()","7d473205":"df.describe()","5a4ace66":"\ndf.describe()","78e08e6f":"# Explore missing values\n\ndf.isnull().sum()","cf9adc7e":"df['Glucose'] = df['Glucose'].replace(0,df['Glucose'].mean())\ndf.Glucose.value_counts()","e490641b":"# Correcting missing values in blood pressure\n\ndf[df['BloodPressure'] == 0]['BloodPressure'].value_counts()\ndf['BloodPressure'] = df['BloodPressure'].replace(0,df['BloodPressure'].mean())","fe8b169f":"# Correcting missing values in BMI\n\ndf[df['BMI'] == 0]['BMI'].value_counts()\ndf['BMI'] = df['BMI'].replace(0, df['BMI'].median())","cccf375c":"# Correct missing values in Insulin and SkinThickness\n\ndf['SkinThickness'] = df['SkinThickness'].replace(0, df['SkinThickness'].median())\ndf['Insulin'] = df['Insulin'].replace(0, df['Insulin'].median())","098d8fdd":"# Review dataset statistics\n\ndf.describe()","bd9fa135":"# Show top 5 rows\ndf.head()","e5d415e5":"plt.figure(figsize=(13,10))\nsns.heatmap(df.corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","279b79ce":"# Explore Pregnancies vs Outcome\nplt.figure(figsize=(13,6))\ng = sns.kdeplot(df[\"Pregnancies\"][df[\"Outcome\"] == 1], color=\"Red\", shade = True)\ng = sns.kdeplot(df[\"Pregnancies\"][df[\"Outcome\"] == 0], ax =g, color=\"Green\", shade= True)\ng.set_xlabel(\"Pregnancies\")\ng.set_ylabel(\"Frequency\")\ng.legend([\"Positive\",\"Negative\"])\n","257836fe":"sns.countplot('Outcome',data=df)\n","abb6e0db":"df","e122e60d":"plt.figure(figsize=(10,6))\nsns.violinplot(data=df, x=\"Outcome\", y=\"Glucose\",\n               split=True, inner=\"quart\", linewidth=1)","2b62f5f2":"# Explore Glucose vs Outcome\n\nplt.figure(figsize=(13,6))\ng = sns.kdeplot(df[\"Glucose\"][df[\"Outcome\"] == 1], color=\"Red\", shade = True)\ng = sns.kdeplot(df[\"Glucose\"][df[\"Outcome\"] == 0], ax =g, color=\"Green\", shade= True)\ng.set_xlabel(\"Glucose\")\ng.set_ylabel(\"Frequency\")\ng.legend([\"Positive\",\"Negative\"])\n","fc232bf7":"# Glucose vs BMI vs Age\n\nplt.figure(figsize=(20,10))\nsns.scatterplot(data=df, x=\"Glucose\", y=\"BMI\", hue=\"Age\", size=\"Age\")","48e7bbfb":"# Explore Age vs Sex, Parch , Pclass and SibSP\ng = sns.catplot(y=\"BloodPressure\",x=\"Outcome\",data=df,kind=\"box\")\ng.set_ylabels(\"Blood Pressure\")\ng.set_xlabels(\"Outcome\")","cae60841":"# Explore Age\n\ng = sns.catplot(y=\"Age\",x=\"Outcome\",data=df,kind=\"box\")\ng.set_ylabels(\"Age\")\ng.set_xlabels(\"Outcome\")","0bb630fc":"sns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(7,5))\n\nsns.boxenplot(x=\"Outcome\", y=\"DiabetesPedigreeFunction\",\n              color=\"b\", \n              scale=\"linear\", data=df)\ng.set_ylabels(\"Diabetes Pedigree Function\")\ng.set_xlabels(\"Outcome\")","74034325":"def detect_outliers(df,n,features):\n    outlier_indices = []\n    \"\"\"\n    Detect outliers from given list of features. It returns a list of the indices\n    according to the observations containing more than n outliers according\n    to the Tukey method\n    \"\"\"\n    # iterate over features(columns)\n    for col in features:\n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col],75)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from numeric features\noutliers_to_drop = detect_outliers(df, 2 ,[\"Pregnancies\", 'Glucose', 'BloodPressure', 'BMI', 'DiabetesPedigreeFunction', 'SkinThickness', 'Insulin', 'Age'])","eec4b475":"df.loc[outliers_to_drop] # Show the outliers rows","4d8b79ba":"df.drop(df.loc[outliers_to_drop].index, inplace=True)","8a77d5ef":"q  = QuantileTransformer()\nX = q.fit_transform(df)\ntransformedDF = q.transform(X)\ntransformedDF = pd.DataFrame(X)\ntransformedDF.columns =['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']","b28e3a7c":"transformedDF.head()","cffc9ab7":"## Separate train dataset and test dataset\nfeatures = df.drop([\"Outcome\"], axis=1)\nlabels = df[\"Outcome\"]\nx_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.30, random_state=7)","739ed06c":"def evaluate_model(models):\n    \"\"\"\n    Takes a list of models and returns chart of cross validation scores using mean accuracy\n    \"\"\"\n    \n    # Cross validate model with Kfold stratified cross val\n    kfold = StratifiedKFold(n_splits = 10)\n    \n    result = []\n    for model in models :\n        result.append(cross_val_score(estimator = model, X = x_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\n    cv_means = []\n    cv_std = []\n    for cv_result in result:\n        cv_means.append(cv_result.mean())\n        cv_std.append(cv_result.std())\n\n    result_df = pd.DataFrame({\n        \"CrossValMeans\":cv_means,\n        \"CrossValerrors\": cv_std,\n        \"Models\":[\n            \"LogisticRegression\",\n            \"DecisionTreeClassifier\",\n            \"AdaBoostClassifier\",\n            \"SVC\",\n            \"RandomForestClassifier\",\n            \"GradientBoostingClassifier\",\n            \"KNeighborsClassifier\"\n        ]\n    })\n\n    # Generate chart\n    bar = sns.barplot(x = \"CrossValMeans\", y = \"Models\", data = result_df, orient = \"h\")\n    bar.set_xlabel(\"Mean Accuracy\")\n    bar.set_title(\"Cross validation scores\")\n    return result_df","892ed4b6":"# Modeling step Test differents algorithms \nrandom_state = 30\nmodels = [\n    LogisticRegression(random_state = random_state, solver='liblinear'),\n    DecisionTreeClassifier(random_state = random_state),\n    AdaBoostClassifier(DecisionTreeClassifier(random_state = random_state), random_state = random_state, learning_rate = 0.2),\n    SVC(random_state = random_state),\n    RandomForestClassifier(random_state = random_state),\n    GradientBoostingClassifier(random_state = random_state),\n    KNeighborsClassifier(),\n]\nevaluate_model(models)","7ffe3865":"# Import libraries\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report","fa9424e9":"def analyze_grid_result(grid_result):\n    '''\n    Analysis of GridCV result and predicting with test dataset\n    Show classification report at last\n    '''\n\n    # Best parameters and accuracy\n    print(\"Tuned hyperparameters: (best parameters) \", grid_result.best_params_)\n    print(\"Accuracy :\", grid_result.best_score_)\n    \n    means = grid_result.cv_results_[\"mean_test_score\"]\n    stds = grid_result.cv_results_[\"std_test_score\"]\n    for mean, std, params in zip(means, stds, grid_result.cv_results_[\"params\"]):\n        print(\"%0.3f (+\/-%0.03f) for %r\" % (mean, std * 2, params))\n    print()\n\n    print(\"Detailed classification report:\")\n    print()\n    y_true, y_pred = y_test, grid_result.predict(x_test)\n    print(classification_report(y_true, y_pred))\n    print()","78f7525c":"# Define models and parameters for LogisticRegression\nmodel = LogisticRegression(solver='liblinear')\nsolvers = ['newton-cg', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n\n# Define grid search\ngrid = dict(solver = solvers, penalty = penalty, C = c_values)\ncv = StratifiedKFold(n_splits = 50, random_state = 1, shuffle = True)\ngrid_search = GridSearchCV(estimator = model, param_grid = grid, cv = cv, scoring = 'accuracy', error_score = 0)\nlogi_result = grid_search.fit(x_train, y_train)\n\n# Logistic Regression Hyperparameter Result\nanalyze_grid_result(logi_result)","8501f2ad":"# Define models and parameters for LogisticRegression\nmodel = SVC()\n\n# Define grid search\ntuned_parameters = [\n    {\"kernel\": [\"rbf\"], \"gamma\": [1e-3, 1e-4], \"C\": [1, 10, 100, 1000]},\n    {\"kernel\": [\"linear\"], \"C\": [1, 10, 100, 1000]},\n]\ncv = StratifiedKFold(n_splits = 2, random_state = 1, shuffle = True)\ngrid_search = GridSearchCV(estimator = model, param_grid = tuned_parameters, cv = cv, scoring = 'accuracy', error_score = 0)\nscv_result = grid_search.fit(x_train, y_train)\n\n# SVC Hyperparameter Result\nanalyze_grid_result(scv_result)","52ccd51b":"# Define models and parameters for LogisticRegression\nmodel = RandomForestClassifier(random_state=42)\n\n\n# Define grid search\ntuned_parameters = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\ncv = StratifiedKFold(n_splits = 2, random_state = 1, shuffle = True)\ngrid_search = GridSearchCV(estimator = model, param_grid = tuned_parameters, cv = cv, scoring = 'accuracy', error_score = 0)\ngrid_result = grid_search.fit(x_train, y_train)\n\n# SVC Hyperparameter Result\nanalyze_grid_result(grid_result)","2a70ff2e":"y_pred = logi_result.predict(x_test)\nprint(classification_report(y_test, y_pred))","b2f6d74c":"x_test['pred'] = y_pred\nx_test","3fb3a03d":"**Glucose**","3ae60f0a":"# Prediction","3b71bbd1":"I observed that there is no missing values in dataset however the features like Glucose, BloodPressure, Insulin, SkinThickness has 0 values which is not possible. We have to replace 0 values with either mean or median values of specific column","01c5a79e":"Please upvote and share my notebook if you find it useful - It keeps me motivated :)","33834a55":"**Age vs Outcome**","8e8422a2":"**DiabetesPedigreeFunction**","2a42dd85":"# Feature Enginnering","fd164b77":"Next, i will cleanup the dataset which is the important part of data science. Missing data can lead to wrong statistics during modeling and predictions.","2435e160":"I have successfully removed all outliers from dataset now. The next step is to split the dataset in train and test and procceed the modeling","0638560b":"There are more people who do not have diabetes in dataset which is around 65% and 35% people has diabetes","dabb73a6":"# Modeling","86f181ed":"# Missing Value Analysis","3a4ddb6c":"# <font size=\"5\">Transforming Data<\/font>\n","a45f92e3":"# Importing Data","5d90e3aa":"# Diabetes Prediction using Python + Pandas\nDiabetes is a chronic (long-lasting) health condition that affects how your body turns food into energy. Most of the food you eat is broken down into sugar (also called glucose) and released into your bloodstream. When your blood sugar goes up, it signals your pancreas to release insulin.\n","ae1a8fd8":"As per my obversation, in LogisticRegression it returned best score 0.78 with `{'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}` parameters. Next i will perform tuning for other models.","776bd988":"# Exploratory Data Analysis","5baabd45":"SVC Model gave max 0.77 accuracy which is bit less than LogisticRegression. I will not use this model anymore.","66c6d3bd":"**BloodPressure**","179b3fef":"Before i split the dataset i need to transform the data into quantile using `sklearn.preprocessing`","16e23e38":"According to observation, features like Pregnancies, Gluecose, BMI, and Age is more correlated with Outcome","54025271":"# <font size=\"5\">RandomForestClassifier<\/font>","e9330790":"First of all i have imported GridSearchCV and classification_report from sklearn library. Then, i have defined `analyze_grid_result` method which will show prediction result. I called this method for each Model used in SearchCV","be475a72":"# Hyperparameter Tuning","1ad87e3b":"# <font size=\"5\">Cross Validate Models<\/font>\n","d058b994":"**Pregnancies**","f820e588":"Excepting BMI and DiabetesPedigreeFunction all the columns are integer. Outcome is the label containing 1 and 0 values. 1 means person has diabetes and 0 mean person is not diabetic","e82e6b96":"# Installing Libraries","ea9a7b53":"Till now, i worked on EDA, Feature Engineering, Cross Validation of Models, and Hyperparameter Tuning and find the best working Model for my dataset. Next, I did prediction from my test dataset and storing the result in CSV","5780396e":"# <font size=\"5\">LogisticRegression<\/font>","922cb60e":"The chances of diabetes is gradually increasing with level of Glucose","e2c2220e":"As per observation there are some outliers in features. We need to remove outliers in feature engineering","38d7e17c":"**Explore Glucose vs BMI vs Age**","5325ec6b":"**Outcome**","b5efb019":"# <font size=\"5\">SVC<\/font>","126bec82":"# <font size=\"5\">Outlier Detection<\/font>","6368e6f0":"Till now, i explored the dataset, did missing value corrections and data visualization. Next, i have started feature engineering. Feature engineering is useful to improve the performance of machine learning algorithms and is often considered as applied machine learning. Selecting the important features and reducing the size of the feature set makes computation in machine learning and data analytic algorithms more feasible.","1ec881fd":"# Introduction\nAccording to WHO, Diabetes is a chronic disease that occurs either when the pancreas does not produce enough insulin or when the body cannot effectively use the insulin it produces. Insulin is a hormone that regulates blood sugar. Hyperglycaemia, or raised blood sugar, is a common effect of uncontrolled diabetes and over time leads to serious damage to many of the body's systems, especially the nerves and blood vessels.\n\nBetween 2000 and 2016, there was a 5% increase in premature mortality rates (i.e. before the age of 70) from diabetes. In high-income countries the premature mortality rate due to diabetes decreased from 2000 to 2010 but then increased in 2010-2016. In lower-middle-income countries, the premature mortality rate due to diabetes increased across both periods.\n\nIn this notebook, i will do some feature analysis and try to find out the rootcauses\n\n# <font size=\"5\">Objectives<\/font>\n1. To experiment with different classification methods to see which yields the highest accuracy\n2. Classify whether someone has diabetes or not from given features\n3. To determine which features are the most indicative of diabetes\n\n# <font size=\"5\">Dataset<\/font>\nI have used [Pima Indians Diabetes Database](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database) Kaggle Dataset\n\nThe dataset contains below features and labels:\n1. Pregnancies\n2. Glucose\n3. BloodPressure\n4. SkinThickness\n5. Insulin\n6. BMI\n7. DiabetesPedigreeFunction\n8. Age\n9. Outcome\n","8571d24d":"# <font size=\"5\">Data Splitting<\/font>\n","bcd1fc1d":"As per above observation, i found that SVC, RandomForestClassifier, and LogisticRegression model has more accuracy. Next, i will do hyper parameter tuning on three models","1f12749d":"There are 768 records in the dataset, in which mean age of people is 33","e8f5bd83":"**Correlation**","2fa7551b":"I have imported most common libraries used in python for machine learning such as Pandas, Seaborn, Matplitlib etc","4e7c5391":"Hyperparameter tuning is choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a model argument whose value is set before the learning process begins. The key to machine learning algorithms is hyperparameter tuning.\n\nI have done tuning process for SVC, RandomForestClassifier, and LogisticRegression models one by one","49da3c9c":"# Table Content\n\n1. Introduction\n2. Installing Libraries\n3. Importing Data\n4. Missing Value Analysis\n5. Exploratory Data Analysis\n6. Feature Engineering\n7. Modeling\n8. Hyperparameter Tuning\n9. Prediction","5b0eba0e":"There are 35 records with 0 BloodPressure in dataset","714942eb":"Randomforest model gave max 0.76% accuracy which is not best comparing to other model. So i decided to use LogisticRegression Model for prediction","4166f9f2":"Now i have dataset without missing values in features which is good"}}