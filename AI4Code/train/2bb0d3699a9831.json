{"cell_type":{"e42daba3":"code","b0e71fd3":"code","77d98f50":"code","699adf56":"code","1ebdebc8":"code","39efb6d7":"code","683f2a49":"code","45304a17":"code","de112261":"code","dc98bb56":"code","d209b6d7":"code","53fd039d":"code","693a7d9d":"code","6c00c691":"code","366fd766":"code","21e5589a":"code","197418e2":"code","150ab04b":"code","640c2665":"code","6d2ceb37":"code","cc710072":"code","05fac56b":"code","3e0accb6":"code","fcfabc25":"code","fdc3d199":"code","780f635c":"code","05a64e68":"code","2985f6f0":"code","828cab60":"code","1548686c":"code","b0fb9651":"code","063723aa":"code","14c99bc8":"code","471eb653":"code","51f39f3d":"code","b9eb0901":"code","80d9eca0":"code","cd5fe014":"code","2e5767d5":"code","0f2e73ce":"code","42c1f978":"code","ab5507e2":"code","a3a21d6b":"code","a9f48efd":"code","717d587b":"code","b68d2da5":"code","79aaa0c2":"code","a2923ec7":"code","124ebc79":"code","7be2c041":"code","f481ea7b":"code","e32ad367":"code","b643b284":"code","b1f9a2e2":"code","fdc79b90":"code","6a787a02":"code","b0fb4689":"code","e855e08f":"code","1b488895":"code","0d5e1f8f":"code","56a437ef":"code","546458a9":"code","fad863e4":"code","9ec260df":"code","a9585691":"code","10012bcd":"code","5fd20e1e":"code","7e103281":"code","e15e7639":"code","f4e32b18":"code","8db5e4e4":"code","f2fffdcc":"code","ed633aa6":"code","900d719e":"code","3259a95e":"code","1d68ef7f":"code","90393214":"code","7bf9a6c3":"code","91ad840c":"code","a027b343":"code","4520e871":"code","1b570b0a":"code","c3d5c024":"code","236dfc1d":"code","81fcc4ce":"code","4796ecad":"code","8721b45e":"code","b24e22bb":"code","05dcdf02":"code","eca1fc33":"code","450a6dc9":"code","c417e305":"code","a4e30d52":"code","eead76f2":"code","4756fa8b":"code","3c3940fe":"code","902cb898":"code","a759f476":"code","96772da4":"code","10015a1f":"code","29fb0010":"code","510b792f":"code","c3906dec":"code","346dd77e":"code","cc3443a8":"code","c7cb1b8b":"code","e9988c34":"code","02def038":"code","615089d6":"code","11ebe056":"code","0c695907":"code","e6ae9652":"code","70d51dd8":"code","d75c9920":"code","ea72f226":"code","96b3ae73":"code","1e5e2181":"code","82e7f8fb":"code","2fbcc9a7":"code","4de649fd":"code","2e73d560":"code","82189e08":"code","4c55cf0b":"code","8b7afcf8":"code","7281cadc":"code","8aae45f8":"markdown","6c446fec":"markdown","9652fbc4":"markdown","98e44b8f":"markdown","1f94360c":"markdown","30237193":"markdown","5ded83d5":"markdown","756a3dd0":"markdown","fe1f43cf":"markdown","1bf6f645":"markdown","3f51a6ad":"markdown","3e03483f":"markdown","0f3fb956":"markdown","ee1198af":"markdown","b5e1426b":"markdown","f10de4fc":"markdown","e734158b":"markdown","126081c5":"markdown","13826db9":"markdown","9d085571":"markdown"},"source":{"e42daba3":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","b0e71fd3":"import os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","77d98f50":"# Chardet: The Universal Character Encoding Detector (ASCII, UTF-8, UTF-16, UTF-32,Big5, GB2312, EUC-TW, HZ-GB-2312 etc)\nimport chardet","699adf56":"# Checking the encoding of day file\nwith open(\"..\/input\/hackerearth-customer-segmentation-hackathon\/avhacklive\/Train.csv\", 'rb') as rawdata:\n    result = chardet.detect(rawdata.read())\n\n# check what the character encoding might be\nprint(result)","1ebdebc8":"# Checking the encoding of day file\nwith open(\"..\/input\/hackerearth-customer-segmentation-hackathon\/avhacklive\/Test.csv\", 'rb') as rawdata:\n    result = chardet.detect(rawdata.read())\n\n# check what the character encoding might be\nprint(result)","39efb6d7":"train_df = pd.read_csv(\"..\/input\/hackerearth-customer-segmentation-hackathon\/avhacklive\/Train.csv\", encoding='palmos')\ntrain_df.info()","683f2a49":"test_df = pd.read_csv(\"..\/input\/hackerearth-customer-segmentation-hackathon\/avhacklive\/Test.csv\", encoding='palmos')\ntest_df.info()","45304a17":"# Way to display & see the column names\ntrain_df.head()","de112261":"# Way to display & see the column names\ntest_df.head()","dc98bb56":"train_df.drop('term_deposit_subscribed',inplace=True,axis=1)","d209b6d7":"# To count the number of dtypes in the given object.\ntrain_df.dtypes.value_counts()","53fd039d":"#Check duplicates in train dataset\nduplicate = train_df[train_df.duplicated(keep='first')]\n  \nprint(\"Duplicate Rows :\")\n  \n# Print the resultant Dataframe\nduplicate.shape","693a7d9d":"#Check duplicates in test dataset\nduplicate = test_df[test_df.duplicated(keep='first')]\n  \nprint(\"Duplicate Rows :\")\n  \n# Print the resultant Dataframe\nduplicate.shape","6c00c691":"train_df.shape","366fd766":"test_df.shape","21e5589a":"#Creating a function to plot Count plot\ndef count_plot(df,feature):\n    sns.set(color_codes = 'Blue', style=\"whitegrid\")\n    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n    sns.set_context(rc = {'patch.linewidth': 0.0})\n    fig = plt.subplots(figsize=(10,3))\n    sns.countplot(x=feature, data=df, color = 'steelblue') # countplot\n    plt.show()","197418e2":"#Creating a function to plot Box plot and Histogram\ndef hist_box_plot(df,feature, fig_num):\n    sns.set(color_codes = 'Blue', style=\"whitegrid\")\n    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n    sns.set_context(rc = {'patch.linewidth': 0.0})\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,4))\n    filtered = df.loc[~np.isnan(df[feature]), feature]\n    sns.boxplot(filtered, ax = ax1, color = 'steelblue') # boxplot\n    sns.distplot(filtered, kde=True, hist=True, kde_kws={'linewidth': 1}, color = 'steelblue', ax = ax2) # histogram\n    plt.show()","150ab04b":"def get_MissingValues(df):\n    cols = []\n    column_with_nan = df.columns[df.isnull().any()]\n    for column in column_with_nan:\n        percent_missing = round(df[column].isnull().mean()*100,2)        \n        cols.append({'colname':column,'percentage':percent_missing,'featureDtype':str(df[column].dtype)})  \n    \n    if len(cols) > 0:     \n        # Creates DataFrame. \n        print(f'{len(cols)} features have missing value')\n        missingDF = pd.DataFrame(cols).sort_values(by=['percentage'], ascending=False)\n    else:\n        missingDF = \"No missingValues\"\n        \n    return missingDF","640c2665":"train_missing_df = get_MissingValues(train_df)\ntrain_missing_df.head(train_missing_df.shape[0])","6d2ceb37":"test_missing_df = get_MissingValues(test_df)\ntest_missing_df.head(test_missing_df.shape[0])","cc710072":"train_df.drop('days_since_prev_campaign_contact',axis=1,inplace=True)\ntest_df.drop('days_since_prev_campaign_contact',axis=1,inplace=True)","05fac56b":"train_df.drop('id',axis=1,inplace=True)\ntest_df.drop('id',axis=1,inplace=True)","3e0accb6":"train_df.shape","fcfabc25":"test_df.shape","fdc3d199":"train_df[\"marital\"].value_counts()","780f635c":"train_df[\"marital\"] = train_df[\"marital\"].fillna(\"unknown\")\ntest_df[\"marital\"] = test_df[\"marital\"].fillna(\"unknown\")","05a64e68":"print(\"Train\")\nprint(train_df['marital'].value_counts())\nprint()\nprint(\"Test\")\nprint(test_df['marital'].value_counts())","2985f6f0":"train_df[\"personal_loan\"]=train_df[\"personal_loan\"].fillna(\"unknown\")\ntest_df[\"personal_loan\"]=test_df[\"personal_loan\"].fillna(\"unknown\")","828cab60":"print(\"Train\")\nprint(train_df['personal_loan'].value_counts())\nprint()\nprint(\"Test\")\nprint(test_df['personal_loan'].value_counts())","1548686c":"cont_feat = ['customer_age', 'balance', 'last_contact_duration', 'num_contacts_in_campaign']","b0fb9651":"fig_num = 1        \nfor col in train_df.select_dtypes(include=[np.number]).columns:\n    if col in cont_feat:\n        hist_box_plot(train_df,col, fig_num)\n        fig_num = fig_num + 1","063723aa":"fig_num = 1        \nfor col in test_df.select_dtypes(include=[np.number]).columns:\n    if col in cont_feat:\n        hist_box_plot(test_df,col, fig_num)\n        fig_num = fig_num + 1","14c99bc8":"train_df[\"balance\"].min()","471eb653":"train_df[\"balance\"]=train_df[\"balance\"].fillna(-10000000)\ntest_df[\"balance\"]=test_df[\"balance\"].fillna(-10000000)","51f39f3d":"train_df[\"balance\"].min()","b9eb0901":"married_train = train_df[train_df[\"marital\"]==\"married\"][\"customer_age\"].median()\nsingle_train = train_df[train_df[\"marital\"]==\"single\"][\"customer_age\"].median()\ndivorced_train = train_df[train_df[\"marital\"]==\"divorced\"][\"customer_age\"].median()\nunknown_train = train_df[train_df[\"marital\"]==\"unknown\"][\"customer_age\"].median()\n\nsingle_test = test_df[test_df[\"marital\"]==\"single\"][\"customer_age\"].median()\nmarried_test = test_df[test_df[\"marital\"]==\"married\"][\"customer_age\"].median()\ndivorced_test = test_df[test_df[\"marital\"]==\"divorced\"][\"customer_age\"].median()\nunknown_test = test_df[test_df[\"marital\"]==\"unknown\"][\"customer_age\"].median()","80d9eca0":"for i in range(len(train_df)):\n    if np.isnan(train_df[\"customer_age\"][i]):\n        if train_df[\"marital\"][i] == \"single\":\n            train_df[\"customer_age\"][i] = round(single_train)\n        if train_df[\"marital\"][i] == \"married\":\n            train_df[\"customer_age\"][i] = round(married_train)\n        if train_df[\"marital\"][i] == \"divorced\":\n            train_df[\"customer_age\"][i] = round(divorced_train)\n        if train_df[\"marital\"][i] == \"unknown\":\n            train_df[\"customer_age\"][i] = round(unknown_train)\n\nfor i in range(len(test_df)):\n    if np.isnan(test_df[\"customer_age\"][i]):\n        if test_df[\"marital\"][i] == \"single\":\n            test_df[\"customer_age\"][i] = round(single_test)\n        if test_df[\"marital\"][i] == \"married\":\n            test_df[\"customer_age\"][i] = round(married_test)\n        if test_df[\"marital\"][i] == \"divorced\":\n            test_df[\"customer_age\"][i] = round(divorced_test)\n        if test_df[\"marital\"][i] == \"unknown\":\n            test_df[\"customer_age\"][i] = round(unknown_test)","cd5fe014":"for column in ['num_contacts_in_campaign', 'last_contact_duration']:\n  train_df[column]=train_df[column].fillna(train_df[column].median())\n  test_df[column]=test_df[column].fillna(test_df[column].median())","2e5767d5":"print(\"Train :\")\nprint(get_MissingValues(train_df))\nprint()\nprint(\"Test : \")\nprint(get_MissingValues(test_df))","0f2e73ce":"print(\"Train Shape:\")\nprint(test_df.shape)\nprint()\nprint(\"Test Shape: \")\nprint(test_df.shape)","42c1f978":"train_df.dtypes.value_counts()","ab5507e2":"numerical_df = train_df.select_dtypes(include=['float64','int64'])\nnumerical_df.head()","a3a21d6b":"categorical_df=train_df.select_dtypes(include='object')\ncategorical_df.head()","a9f48efd":"for col in categorical_df.columns:\n    if col in ['job_type','marital','education', 'default','housing_loan','personal_loan']:\n        count_plot(train_df,col)","717d587b":"for col in categorical_df.columns:\n    if col in ['job_type','marital','education', 'default','housing_loan','personal_loan']:\n        count_plot(test_df,col)","b68d2da5":"plt.figure(figsize=(10,10))\nsns.heatmap(numerical_df.corr(), cmap='RdYlBu_r', annot=True);","79aaa0c2":"train_df.describe(percentiles=[0.1, .25, .50, .75, .85, .90, .95, .98, .99])","a2923ec7":"test_df.describe(percentiles=[0.1, .25, .50, .75, .85, .90, .95, .98, .99])","124ebc79":"def outlier_capping_lowerUpperBound(df):\n    numeric_features_columns = numerical_df.columns.to_list()\n\n    for feature in numeric_features_columns:    \n        q1 = df[feature].quantile(0.25)\n        q3 = df[feature].quantile(0.75)\n        iqr = q3 - q1\n        lower_bound = q1 -(1.5 * iqr)\n        upper_bound = q3 +(1.5 * iqr)\n        df[feature][df[feature] < lower_bound] = lower_bound\n        df[feature][df[feature] > upper_bound] = upper_bound\n    return df","7be2c041":"train_df = outlier_capping_lowerUpperBound(train_df)\n    \ntrain_df.describe(percentiles=[0.1, .25, .50, .75, .85, .90, .95, .98, .99])","f481ea7b":"test_df = outlier_capping_lowerUpperBound(test_df)\n    \ntest_df.describe(percentiles=[0.1, .25, .50, .75, .85, .90, .95, .98, .99])","e32ad367":"train_df.drop('num_contacts_prev_campaign',axis=1,inplace=True)\ntest_df.drop('num_contacts_prev_campaign',axis=1,inplace=True)","b643b284":"numerical_df = train_df.select_dtypes(include=['float64','int64'])\nnumerical_df.head()","b1f9a2e2":"train_clean_df = train_df.copy()\ntest_clean_df = test_df.copy()\n\nprint(train_clean_df.info())\nprint() \nprint(test_clean_df.info())\nprint(\"\\n\"*2) \nprint(train_clean_df.shape)\nprint() \nprint(test_clean_df.shape)","fdc79b90":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# define standard scaler\nscaler = StandardScaler()\n# scaler = MinMaxScaler()\n\n# transform data\ntrain_df[numerical_df.columns] = scaler.fit_transform(train_df[numerical_df.columns])\n\n\n# transform data\ntest_df[numerical_df.columns] = scaler.fit_transform(test_df[numerical_df.columns])","6a787a02":"train_df.head()","b0fb4689":"test_df.head()","e855e08f":"#Hopkins Statistic is a way of measuring the cluster tendency of a data set.\nfrom sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) # heuristic from article [1]\n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) \/ (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H","1b488895":"#Checking whether data can be clustered\nhopkins(train_df[numerical_df.columns])","0d5e1f8f":"! pip install umap-learn","56a437ef":"import umap.umap_ as umap\nfrom sklearn.preprocessing import PowerTransformer","546458a9":"#Preprocessing numerical\nnumerical = train_df.select_dtypes(exclude='object')\n\nfor c in numerical.columns:\n    pt = PowerTransformer()\n    numerical.loc[:, c] = pt.fit_transform(np.array(numerical[c]).reshape(-1, 1))\n    \n##preprocessing categorical\ncategorical = train_df.select_dtypes(include='object')\ncategorical = pd.get_dummies(categorical)\n\n#Percentage of columns which are categorical is used as weight parameter in embeddings later\ncategorical_weight = len(train_df.select_dtypes(include='object').columns) \/ train_df.shape[1]","fad863e4":"fit1 = umap.UMAP(metric='braycurtis').fit(numerical)\nfit2 = umap.UMAP(metric='jaccard').fit(categorical)","9ec260df":"categorical_weight","a9585691":"intersection = umap.general_simplicial_set_intersection(fit1.graph_, fit2.graph_, weight=0.4)\nintersection = umap.reset_local_connectivity(intersection)","10012bcd":"embedding = umap.simplicial_set_embedding(fit1._raw_data, intersection, fit1.n_components, \n                                                fit1._initial_alpha, fit1._a, fit1._b, \n                                                fit1.repulsion_strength, fit1.negative_sample_rate, \n                                                200, 'random', np.random, fit1.metric, \n                                                fit1._metric_kwds, False, False, False)","5fd20e1e":"plt.figure(figsize=(20, 10))\nplt.scatter(embedding[0][:20000,0], embedding[0][:20000,1], s=2, cmap='Spectral', alpha=1.0)\nplt.axis('off')\nplt.show();","7e103281":"data = pd.get_dummies(train_df)\nfor c in data.columns:\n    pt = PowerTransformer()\n    data.loc[:, c] = pt.fit_transform(np.array(data[c]).reshape(-1, 1))","e15e7639":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom tqdm import tqdm\nimport plotly.graph_objects as grpobj","f4e32b18":"#Elbow method to choose the optimal number of clusters\nsse = {}\nfor k in tqdm(range(2, 7)):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(data)\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\n","8db5e4e4":"kmeans = KMeans(n_clusters=3).fit(data)\nkmeans_labels = kmeans.labels_","f2fffdcc":"fig = grpobj.Figure(data=grpobj.Scatter(x=list(sse.keys()), y=list(sse.values())))\nfig.show()","ed633aa6":"pd.Series(kmeans_labels).value_counts()","900d719e":"train_df.head()","3259a95e":"kprot_data = train_df.copy()\nfor c in train_df.select_dtypes(exclude='object').columns:\n    pt = PowerTransformer()\n    kprot_data[c] =  pt.fit_transform(np.array(kprot_data[c]).reshape(-1, 1))","1d68ef7f":"! pip install kmodes\nfrom kmodes.kprototypes import KPrototypes","90393214":"# Get the position of categorical columns\ncatColumnsPos = [train_df.columns.get_loc(col) for col in list(train_df.select_dtypes('object').columns)]\nprint('Categorical columns           : {}'.format(list(train_df.select_dtypes('object').columns)))\nprint('Categorical columns position  : {}'.format(catColumnsPos))","7bf9a6c3":"costs = []\nn_clusters = []\nclusters_assigned = []\n\nfor i in tqdm(range(2, 7)):\n    try:\n        kproto = KPrototypes(n_jobs = -1, n_clusters = i, init = 'Huang', random_state = 41, verbose=2)\n        clusters = kproto.fit_predict(kprot_data, categorical = catColumnsPos)\n        costs.append(kproto.cost_)\n        n_clusters.append(i)\n        clusters_assigned.append(clusters)\n    except:\n        print(f\"Can't cluster with {i} clusters\")","91ad840c":"fig = grpobj.Figure(data=grpobj.Scatter(x=n_clusters, y=costs ))\nfig.show()","a027b343":"# Fit the cluster\nkprototype = KPrototypes(n_jobs = -1, n_clusters = 3, init = 'Huang', random_state = 41)\nkpclusters = kprototype.fit_predict(kprot_data, categorical = catColumnsPos)","4520e871":"pd.Series(kpclusters).value_counts()","1b570b0a":"proto_clusters = kprototype.labels_","c3d5c024":"# Cluster centorid\nkprototype.cluster_centroids_","236dfc1d":"# Check the iteration of the clusters created\nkprototype.n_iter_","81fcc4ce":"# Check the cost of the clusters created\nkprototype.cost_","4796ecad":"kpr_orig_df = train_clean_df.reset_index()","8721b45e":"# Add the cluster to the dataframe\nkpr_orig_df['Cluster Labels'] = kprototype.labels_\nkpr_orig_df['Segment'] = kpr_orig_df['Cluster Labels'].map({0:'First', 1:'Second', 2:'Third'})","b24e22bb":"# Order the cluster\nkpr_orig_df['Segment'] = kpr_orig_df['Segment'].astype('category')\nkpr_orig_df['Segment'] = kpr_orig_df['Segment'].cat.reorder_categories(['First','Second','Third'])","05dcdf02":"# Cluster interpretation\nkpr_orig_df.rename(columns = {'Cluster Labels':'Total'}, inplace = True)\nkpr_orig_df.groupby('Segment').agg(\n    {\n        'customer_age':'median',\n        'job_type': lambda x: x.value_counts().index[0],\n        'marital': lambda x: x.value_counts().index[0],\n        'education': lambda x: x.value_counts().index[0],\n        'default': lambda x: x.value_counts().index[0],     \n        'balance': 'mean',     \n        'housing_loan': lambda x: x.value_counts().index[0],\n        'personal_loan': lambda x: x.value_counts().index[0],\n        'communication_type': lambda x: x.value_counts().index[0],\n        'day_of_month': lambda x: x.value_counts().index[0],\n        'default': lambda x: x.value_counts().index[0],\n        'month': lambda x: x.value_counts().index[0],     \n        'last_contact_duration': 'mean',\n        'num_contacts_in_campaign': 'mean',\n        'prev_campaign_outcome': lambda x: x.value_counts().index[0]\n    }\n).reset_index()","eca1fc33":"from kmodes.kmodes import KModes","450a6dc9":"cost = []\nclusters = []\nind_clusters = []\nfor num_clusters in list(range(2,7)):\n#     # kmode = KModes(n_clusters=num_clusters, init = \"Cao\", n_init = 1, verbose=1)\n  try:  \n    kmode = KModes(n_clusters=num_clusters, init = \"Huang\", n_init = 1, verbose=1, random_state = 41)\n    kmode.fit_predict(train_df)\n    cost.append(kmode.cost_)\n    ind_clusters.append(num_clusters)\n    clusters.append(kmode)\n    print('Cluster initiation: {}'.format(num_clusters))\n  except:\n    print(f\"Can't cluster with {num_clusters} clusters\")\n    break","c417e305":"fig = grpobj.Figure(data=grpobj.Scatter(x=ind_clusters, y=cost ))\nfig.show()","a4e30d52":"# Converting the results into a dataframe and plotting them\ndf_cost = pd.DataFrame({'Cluster': range(2,7), 'Cost': cost})","eead76f2":"# Import module for data visualization\nfrom plotnine import *\nimport plotnine\n# Data viz\nplotnine.options.figure_size = (8, 4.8)\n(\n    ggplot(data = df_cost)+\n    geom_line(aes(x = 'Cluster',\n                  y = 'Cost'))+\n    geom_point(aes(x = 'Cluster',\n                   y = 'Cost'))+\n    geom_label(aes(x = 'Cluster',\n                   y = 'Cost',\n                   label = 'Cluster'),\n               size = 10,\n               nudge_y = 1000) +\n    labs(title = 'Optimal number of cluster with Elbow Method')+\n    xlab('Number of Clusters k')+\n    ylab('Cost')+\n    theme_minimal()\n)","4756fa8b":"km_cao = KModes(n_clusters=3, init = \"Huang\", n_init = 1, verbose=1, random_state = 41)\nfitClusters_cao = km_cao.fit_predict(train_df)","3c3940fe":"kmode_clusters = km_cao.labels_","902cb898":"orig_df = train_clean_df.reset_index()","a759f476":"clustersDf = pd.DataFrame(fitClusters_cao)\nclustersDf.columns = ['cluster_predicted']\ncombinedDf = pd.concat([orig_df, clustersDf], axis = 1).reset_index()\ncombinedDf = combinedDf.drop(['index', 'level_0'], axis = 1)","96772da4":"combinedDf.head()","10015a1f":"plt.subplots(figsize = (25,12))\nsns.countplot(x=combinedDf['customer_age'],order=combinedDf['customer_age'].value_counts().index,hue=combinedDf['cluster_predicted'])\nplt.show()","29fb0010":"plt.subplots(figsize = (5,5))\nsns.countplot(x=combinedDf['marital'],order=combinedDf['marital'].value_counts().index,hue=combinedDf['cluster_predicted'])\nplt.show()","510b792f":"plt.subplots(figsize = (15,5))\nsns.countplot(x=combinedDf['education'],order=combinedDf['education'].value_counts().index,hue=combinedDf['cluster_predicted'])\nplt.show()","c3906dec":"plt.subplots(figsize = (20,5))\nsns.countplot(x=combinedDf['job_type'],order=combinedDf['job_type'].value_counts().index,hue=combinedDf['cluster_predicted'])\nplt.show()","346dd77e":"f, axs = plt.subplots(1,3,figsize = (15,5))\nsns.countplot(x=combinedDf['default'],order=combinedDf['default'].value_counts().index,hue=combinedDf['cluster_predicted'],ax=axs[0])\nsns.countplot(x=combinedDf['housing_loan'],order=combinedDf['housing_loan'].value_counts().index,hue=combinedDf['cluster_predicted'],ax=axs[1])\nsns.countplot(x=combinedDf['personal_loan'],order=combinedDf['personal_loan'].value_counts().index,hue=combinedDf['cluster_predicted'],ax=axs[2])\n\nplt.tight_layout()\nplt.show()","cc3443a8":"f, axs = plt.subplots(1,2,figsize = (15,5))\nsns.countplot(x=combinedDf['month'],order=combinedDf['month'].value_counts().index,hue=combinedDf['cluster_predicted'],ax=axs[0])\nsns.countplot(x=combinedDf['day_of_month'],order=combinedDf['day_of_month'].value_counts().index,hue=combinedDf['cluster_predicted'],ax=axs[1])\n\nplt.tight_layout()\nplt.show()","c7cb1b8b":"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import cross_val_score","e9988c34":"!pip install shap\nimport shap","02def038":"#Setting the objects to category \nfinal_eval_df = train_df.copy()\nfor c in final_eval_df.select_dtypes(include='object'):\n    final_eval_df[c] = final_eval_df[c].astype('category')","615089d6":"#KMeans clusters\nclf_km = LGBMClassifier(colsample_by_tree=0.8)\ncv_scores_km = cross_val_score(clf_km, final_eval_df, kmeans_labels, scoring='f1_weighted')\nprint(f'CV F1 score for K-Means clusters is {np.mean(cv_scores_km)}')","11ebe056":"#Fit the model\nclf_km.fit(final_eval_df, kmeans_labels)","0c695907":"#SHAP values\nexplainer_km = shap.TreeExplainer(clf_km)\nshap_values_km = explainer_km.shap_values(final_eval_df)\nshap.summary_plot(shap_values_km, final_eval_df, plot_type=\"bar\", plot_size=(15, 10))","e6ae9652":"#KPrototype clusters\nclf_kp = LGBMClassifier(colsample_by_tree=0.8)\ncv_scores_kp = cross_val_score(clf_kp, final_eval_df, proto_clusters, scoring='f1_weighted')\nprint(f'CV F1 score for K-Prototypes clusters is {np.mean(cv_scores_kp)}')","70d51dd8":"clf_kp.fit(final_eval_df, proto_clusters)","d75c9920":"#SHAP values\nexplainer_kp = shap.TreeExplainer(clf_kp)\nshap_values_kp = explainer_kp.shap_values(final_eval_df)\nshap.summary_plot(shap_values_kp, final_eval_df, plot_type=\"bar\", plot_size=(15, 10))","ea72f226":"# KMode Clusters\nclf_kmode = LGBMClassifier(colsample_by_tree=0.8)\ncv_scores_kmode = cross_val_score(clf_kmode, final_eval_df, kmode_clusters, scoring='f1_weighted')\nprint(f'CV F1 score for K-Prototypes clusters is {np.mean(cv_scores_kmode)}')","96b3ae73":"clf_kmode.fit(final_eval_df, kmode_clusters)","1e5e2181":"#SHAP values\nexplainer_kmode = shap.TreeExplainer(clf_kmode)\nshap_values_kmode = explainer_kmode.shap_values(final_eval_df)\nshap.summary_plot(shap_values_kmode, final_eval_df, plot_type=\"bar\", plot_size=(15, 10))","82e7f8fb":"train_clean_df = pd.get_dummies(train_clean_df)\nfor c in train_clean_df.columns:\n    pt = PowerTransformer()\n    train_clean_df.loc[:, c] = pt.fit_transform(np.array(train_clean_df[c]).reshape(-1, 1))","2fbcc9a7":"from sklearn.tree import _tree, DecisionTreeClassifier\nfrom IPython.display import display, HTML","4de649fd":"def pretty_print(df):\n    return display( HTML( df.to_html().replace(\"\\\\n\",\"<br>\") ) )","2e73d560":"def get_class_rules(tree: DecisionTreeClassifier, feature_names: list):\n  inner_tree: _tree.Tree = tree.tree_\n  classes = tree.classes_\n  class_rules_dict = dict()\n\n  def tree_dfs(node_id=0, current_rule=[]):\n    # feature[i] holds the feature to split on, for the internal node i.\n    split_feature = inner_tree.feature[node_id]\n    if split_feature != _tree.TREE_UNDEFINED: # internal node\n      name = feature_names[split_feature]\n      threshold = inner_tree.threshold[node_id]\n      # left child\n      left_rule = current_rule + [\"({} <= {})\".format(name, threshold)]\n      tree_dfs(inner_tree.children_left[node_id], left_rule)\n      # right child\n      right_rule = current_rule + [\"({} > {})\".format(name, threshold)]\n      tree_dfs(inner_tree.children_right[node_id], right_rule)\n    else: # leaf\n      dist = inner_tree.value[node_id][0]\n      dist = dist\/dist.sum()\n      max_idx = dist.argmax()\n      if len(current_rule) == 0:\n        rule_string = \"ALL\"\n      else:\n        rule_string = \" and \".join(current_rule)\n      # register new rule to dictionary\n      selected_class = classes[max_idx]\n      class_probability = dist[max_idx]\n      class_rules = class_rules_dict.get(selected_class, [])\n      class_rules.append((rule_string, class_probability))\n      class_rules_dict[selected_class] = class_rules\n    \n  tree_dfs() # start from root, node_id = 0\n  return class_rules_dict","82189e08":"def cluster_report(data: pd.DataFrame, clusters, min_samples_leaf=50, pruning_level=0.01):\n    # Create Model\n    tree = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf, ccp_alpha=pruning_level)\n    tree.fit(data, clusters)\n    \n    # Generate Report\n    feature_names = data.columns\n    class_rule_dict = get_class_rules(tree, feature_names)\n\n    report_class_list = []\n    for class_name in class_rule_dict.keys():\n        rule_list = class_rule_dict[class_name]\n        combined_string = \"\"\n        for rule in rule_list:\n            combined_string += \"[{}] {}\\n\\n\".format(rule[1], rule[0])\n        report_class_list.append((class_name, combined_string))\n        \n    cluster_instance_df = pd.Series(clusters).value_counts().reset_index()\n    cluster_instance_df.columns = ['class_name', 'instance_count']\n    report_df = pd.DataFrame(report_class_list, columns=['class_name', 'rule_list'])\n    report_df = pd.merge(cluster_instance_df, report_df, on='class_name', how='left')\n    pretty_print(report_df.sort_values(by='class_name')[['class_name', 'instance_count', 'rule_list']])","4c55cf0b":"cluster_report(train_clean_df, kmeans_labels, min_samples_leaf=20, pruning_level=0.05)","8b7afcf8":"cluster_report(train_clean_df, proto_clusters, min_samples_leaf=20, pruning_level=0.05)","7281cadc":"cluster_report(train_clean_df, kmode_clusters, min_samples_leaf=20, pruning_level=0.05)","8aae45f8":"Visualizing for train and test data - to check for outliers","6c446fec":"**num_contacts_prev_campaign** the statistical value for this feature is zero. So dropping the from both train_df and test_df","9652fbc4":"## Rule generation from decision tree","98e44b8f":"##Clustering","1f94360c":"Result: This test is run (code: (MATEVZKUNAVER, 2017)) on all the numerical variables of the entire dataset and the test statistic we got is 0.75 which indicates that data has high tendency to cluster and therefore likely to have statistically significant clusters.","30237193":"Has more categorical values, so I opt to model with other than logistic regression.","5ded83d5":"Checking for Duplicates","756a3dd0":"**K-prototypes**<br>\nFor K-Prototypes, I'll apply the transformation to numerical data. Categorical data doesn't need any pre-processing.","fe1f43cf":"### **Hopkins Statistics**\nTo understand if the dataset can be clustered, we used the Hopkins statistic, which tests the spatial randomness of the data and indicates the cluster tendency or how well the data can be clustered. It calculates the probability that a given data is generated by a uniform distribution (Alboukadel Kassambara, n.d.). The inference is as follows for a data of dimensions \u2018d\u2019:\n\n\nIf the value is around 0.5 or lesser, the data is uniformly distributed and hence it is unlikely to have statistically significant clusters.<br>\nIf the value is between {0.7, ..., 0.99}, it has a high tendency to cluster and therefore likely to have statistically significant clusters.","1bf6f645":"**days_since_prev_campaign_contact** is having more than 80% of missing data. So drop the feature from both train and test","3f51a6ad":"**id** is not applicable for this analysis. So dropping the feature","3e03483f":"## Checking data for Clustering","0f3fb956":"Train data is having 18 features and test data is having 17 features. So, dropping the feature \"term_deposit_subscribed\" from train_df as it is not present in test_df","ee1198af":"Filling balance with a low negative value manually as the missing % is 1.26 for train_df and 1.33 for test_df","b5e1426b":"Common function to print missing values","f10de4fc":"### Checking Duplicates","e734158b":"### Handling Missing Values","126081c5":"As 'customer_age', 'num_contacts_in_campaign', 'last_contact_duration' are having outliers, median is taken","13826db9":"With refernce to this dataset defenition, the missing values are filled as per the feature data.\nhttps:\/\/docs.1010data.com\/Tutorials\/MachineLearningExamples\/BankMarketingDataSet_2.html","9d085571":"### Outlier Treatment"}}