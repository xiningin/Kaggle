{"cell_type":{"564b9213":"code","9aaf8948":"code","69f7c8ae":"code","b0feb735":"code","62fdfcb6":"code","341bf75e":"code","69d49835":"code","c96261ca":"code","51ee066b":"code","035050c5":"code","602b57bf":"code","20c03f73":"code","b06328c1":"code","240bf948":"code","e6640db1":"code","aa226e0d":"code","98e68a79":"code","c46ffc35":"code","9a20d491":"code","313c43d4":"code","5e32b0c9":"code","0d67f4fe":"code","86d838b5":"code","40e5fbdd":"code","018a43eb":"code","2e950ae9":"code","35758c79":"code","4aa8ba63":"code","1c7b6fae":"code","b5432f92":"code","97829072":"code","3559b0c3":"code","5d0a3e39":"code","5c210e81":"code","70158c8f":"code","0dd2af12":"code","9ea562d3":"code","88334de4":"code","872f89bb":"code","7fa02320":"code","1aa16197":"code","7b791ffc":"code","02ddde9c":"code","667e8b48":"code","73f806ec":"code","2915273b":"code","1874fabb":"markdown","6509d522":"markdown","2a6fc644":"markdown","235b10af":"markdown","97fa71d2":"markdown","00b7fbd5":"markdown","209866c4":"markdown","6a75ea78":"markdown","12a76aa7":"markdown","de3410e6":"markdown","46ba8ebc":"markdown","6402616b":"markdown","37640f78":"markdown","fe32f82a":"markdown","8e52509b":"markdown","9e6f8066":"markdown","9f67c2fc":"markdown","a5cb687f":"markdown","2353bab1":"markdown","f4fbd3b6":"markdown","e65fe5dc":"markdown","7e9d7458":"markdown","a5ae8403":"markdown","ba954ea5":"markdown","a0308118":"markdown","2dc05f73":"markdown"},"source":{"564b9213":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tqdm\nimport seaborn as sns\n\nfrom tensorflow import keras\nfrom keras import Input, Model, Sequential\nfrom tensorflow.keras.regularizers import l2\nfrom keras.layers import Dense, Flatten, InputLayer, Reshape, BatchNormalization, Dropout, Conv2D, MaxPooling2D\nfrom tensorflow.keras.utils import plot_model\n\n%matplotlib inline","9aaf8948":"# Dict of labels\ncategories = {\n                'cloudy': 0,\n                'desert': 1,\n                'green_area': 2,\n                'water': 3 \n            }","69f7c8ae":"def load_images(images_folder, img_size = (128,128), scale=False):\n\n    # Store paths to images\n    image_path = []\n    for dirname, _, filenames in os.walk(images_folder):\n        for filename in filenames:\n            image_path.append(os.path.join(dirname, filename))\n\n    print(\"There are {} images in {}\".format(len(image_path), images_folder))\n    \n    # Load images and associated labels\n    images = []\n    labels = []\n\n    for path in tqdm.tqdm(image_path):\n\n        img = cv2.imread(path)    \n        img = cv2.resize(img, img_size) # Resize the images\n\n        img = np.array(img)\n\n        images.append(img)\n        labels.append(categories[path.split('\/')[-2]]) # last folder before the image name is the category\n\n    images = np.array(images)  \n    images = images.astype(np.int64)\n    \n    if scale:\n        images = images\/255 # scale\n        \n    return image_path, images, np.asarray(labels)","b0feb735":"img_size = (128,128)\nimages_folder = os.path.join('\/', 'kaggle', 'input', 'satellite-image-classification', 'data')\nimage_path, images, labels = load_images(images_folder, img_size=img_size)\n\n# Resize\n# images = np.array(images).reshape(-1,128,128,1)\nimages.shape","62fdfcb6":"set(labels)","341bf75e":"labels","69d49835":"plt.figure(figsize=(10,10))\nrandom_inds = np.random.choice(len(image_path),36)\nfor i in range(36):\n    plt.subplot(6,6,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    image_ind = random_inds[i]\n    plt.imshow(np.squeeze(images[image_ind]), cmap=plt.cm.binary)\n    \n    label = list(categories.keys())[list(categories.values()).index(labels[image_ind])]\n    plt.title(label)","c96261ca":"labels_df = pd.DataFrame(labels) \nlabels_df.value_counts()","51ee066b":"len(categories)","035050c5":"from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n\ndef build_cnn_model():\n    cnn_model=tf.keras.Sequential([\n      Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=images.shape[1:]),\n      MaxPooling2D(2,2),\n      BatchNormalization(),\n      Dropout(0.4),\n\n      Conv2D(filters=64,kernel_size=(3,3),activation='relu', padding='same'),\n      Conv2D(filters=64,kernel_size=(3,3),activation='relu', padding='same'),\n      MaxPooling2D((2,2)),\n      BatchNormalization(),\n      Dropout(0.4),\n\n      Conv2D(filters=128,kernel_size=(3,3),activation='relu', padding='same'),\n      Conv2D(filters=128,kernel_size=(3,3),activation='relu', padding='same'),\n      MaxPooling2D(2,2),\n      BatchNormalization(),\n      Dropout(0.4),\n        \n      Conv2D(filters=256,kernel_size=(3,3),activation='relu', padding='same'),\n      Conv2D(filters=256,kernel_size=(3,3),activation='relu', padding='same'),\n      MaxPooling2D(2,2),\n      BatchNormalization(),\n      Dropout(0.4),\n        \n\n      Conv2D(filters=128,kernel_size=(3,3),activation='relu', padding='same'),\n      Conv2D(filters=128,kernel_size=(3,3),activation='relu', padding='same'),\n      MaxPooling2D(2,2),\n      BatchNormalization(),\n      Dropout(0.4),\n        \n      Conv2D(filters=64,kernel_size=(3,3),activation='relu', padding='same'),\n      Conv2D(filters=64,kernel_size=(3,3),activation='relu', padding='same'),\n      MaxPooling2D((2,2)),\n      BatchNormalization(),\n      Dropout(0.4),\n\n      Flatten(),\n\n      Dense(units=len(categories),activation='softmax')\n    ])\n\n    return cnn_model\n  \nmodel = build_cnn_model()\n# Initialize the model by passing some data through\nmodel.predict(images[[0]])\n# Print the summary of the layers in the model.\nprint(model.summary())","602b57bf":"tf.keras.utils.plot_model(model, show_shapes=True)","20c03f73":"labels[:10]","b06328c1":"from tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import shuffle\nle = LabelEncoder()\nlabels = le.fit_transform(labels)\nlabels = to_categorical(labels)\nlabels[:10]","240bf948":"model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])","e6640db1":"checkpoint_filepath = '\/kaggle\/working\/checkpoint.hdf5'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True,\n    save_freq=500)","aa226e0d":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy', \n    patience=10, \n    min_delta=0.001, \n    mode='max',\n    restore_best_weights=True\n)","98e68a79":"from tensorflow.keras.callbacks import ReduceLROnPlateau\nreducelr = ReduceLROnPlateau(monitor = \"val_accuracy\",factor = 0.3, patience = 3,\n                            min_delta = 0.001,mode = 'auto',verbose=1)","c46ffc35":"# images[0][0]","9a20d491":"print(images.shape)\nprint(labels.shape)","313c43d4":"from sklearn.model_selection import train_test_split\n# Train, validation and test split\n\nX_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.10, random_state=7)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=1) ","5e32b0c9":"print(\"*-*-*-*-*-*\")\nprint(\"Train\")\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(\"*-*-*-*-*-*\")\nprint(\"Validation\")\nprint(X_val.shape)\nprint(y_val.shape)\n\nprint(\"*-*-*-*-*-*\")\nprint(\"Test\")\nprint(X_test.shape)\nprint(y_test.shape)","0d67f4fe":"y_train[0]","86d838b5":"y_val[0]","40e5fbdd":"history = model.fit(X_train, y_train, \n                    batch_size = 32, \n                    epochs = 1000, \n                    verbose = 1, \n                    validation_data = (X_val, y_val),\n                    callbacks=[model_checkpoint_callback, early_stopping, reducelr])","018a43eb":"plt.plot(history.history[\"accuracy\"])\nplt.plot(history.history[\"val_accuracy\"])\nplt.title(\"Model accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Train\", \"Test\"], loc = \"upper left\")\nplt.show()","2e950ae9":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"Model loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Train\", \"Test\"], loc = \"upper left\")\nplt.show()","35758c79":"def predict_class(img):\n    # Resize\n    img = img.reshape(1,128,128,3)\n    # Predict\n    predictions = model.predict(img)\n    true_prediction = [tf.argmax(pred) for pred in predictions]\n    true_prediction = np.array(true_prediction)\n    \n    # Return label corresponding to predicted index\n    return list(categories.keys())[list(categories.values()).index(true_prediction)]\n    ","4aa8ba63":"X_test[0].shape","1c7b6fae":"predict_class(X_test[0])","b5432f92":"# Predict on test set\ny_pred = model.predict(X_test)","97829072":"y_pred.shape","3559b0c3":"# From categorical outputs to discrete values\ny_pred_ = [np.argmax(y) for y in y_pred]\ny_test_ = [np.argmax(y) for y in y_test]","5d0a3e39":"from sklearn.metrics import classification_report\nprint(classification_report(y_test_, y_pred_))","5c210e81":"from sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\ncm = confusion_matrix(y_test_,y_pred_)\nplot_confusion_matrix(conf_mat = cm,figsize=(8,7),class_names =categories.keys(),\n                     show_normed = True)","70158c8f":"plt.figure(figsize=(10,10))\nrandom_inds = np.random.choice(X_test.shape[0],36)\nfor i in range(36):\n    plt.subplot(6,6,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    image_ind = random_inds[i]\n    plt.imshow(np.squeeze(X_test[image_ind]), cmap=plt.cm.binary)\n    \n    # Predict and get label\n    label = predict_class(X_test[image_ind])\n    plt.xlabel(label)","0dd2af12":"model.save(\"sattelite_image_classifier.h5\")","9ea562d3":"from tensorflow.keras.applications import VGG16, DenseNet201\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\ndef build_model(input_shape=(128,128,3)):\n    \n    base_model = DenseNet201(input_shape=input_shape,include_top=False,weights='imagenet')#,pooling='avg'\n    base_model.trainable = False\n    model = base_model.output\n    model = GlobalAveragePooling2D()(model)\n    model = Dropout(0.4)(model)\n    model = Dense(128, activation='relu')(model)\n    model = Flatten()(model)\n    model = Dense(len(categories),activation='softmax')(model)\n\n    model = Model(inputs = base_model.input, outputs = model)\n    \n    return model\n\ndensenet_model = build_model()\ndensenet_model.summary()","88334de4":"from sklearn.model_selection import train_test_split\n# Train, validation and test split\n\nX_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.10, random_state=7)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=1) ","872f89bb":"print(\"*-*-*-*-*-*\")\nprint(\"Train\")\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(\"*-*-*-*-*-*\")\nprint(\"Validation\")\nprint(X_val.shape)\nprint(y_val.shape)\n\nprint(\"*-*-*-*-*-*\")\nprint(\"Test\")\nprint(X_test.shape)\nprint(y_test.shape)","7fa02320":"densenet_model.compile(optimizer =\"adam\", loss = \"binary_crossentropy\",metrics = ['accuracy'])","1aa16197":"y_train[0]","7b791ffc":"history = densenet_model.fit(X_train, y_train, \n                    batch_size = 128, \n                    epochs = 1000, \n                    verbose = 1, \n                    validation_data = (X_val, y_val),\n                    callbacks=[model_checkpoint_callback, early_stopping, reducelr])","02ddde9c":"y_pred = densenet_model.predict(X_test)","667e8b48":"y_pred_ = [np.argmax(y) for y in y_pred]\n# y_test_ = [np.argmax(y) for y in y_test]","73f806ec":"from sklearn.metrics import classification_report\nprint(classification_report(y_test_, y_pred_))","2915273b":"from sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\ncm = confusion_matrix(y_test_,y_pred_)\nplot_confusion_matrix(conf_mat = cm,figsize=(8,7),class_names =categories.keys(),\n                     show_normed = True)","1874fabb":"How are the categories distributed?","6509d522":"We have good results. We will see if we can do better with transfer learning this time.","2a6fc644":"# Confusion matrix","235b10af":"## Split images","97fa71d2":"![spacex-VBNb52J8Trk-unsplash.jpg](attachment:cd3a9226-6d90-4988-88ac-e162710f3114.jpg)","00b7fbd5":"# Model building","209866c4":"### Plot the performances","6a75ea78":"### Save the model","12a76aa7":"# Model training","de3410e6":"## Compile model and proceed to training","46ba8ebc":"Credit photo: @spacex from Unsplash","6402616b":"### Plot some images and predicted labels","37640f78":"## Predict on test set and check performances","fe32f82a":"We add some callbacks to save the weights of the model and for early stopping.","8e52509b":"# Sattelite Image Classification","9e6f8066":"## Convert labels to categorical","9f67c2fc":"# Predict on new data","a5cb687f":"# Split the data","2353bab1":"Let's take a look at the categories (target values) :","f4fbd3b6":"## Plot the model","e65fe5dc":"### Some callbacks","7e9d7458":"Plot some of the images from the train set","a5ae8403":"# Load images","ba954ea5":"There is a lot of confusion from the model between the \"water\" and \"green_area\" images. ","a0308118":"# Visualizing","2dc05f73":"# Transfer learning with VGG16"}}