{"cell_type":{"d69d2a4a":"code","afb76c9a":"code","1400f200":"code","3871e0b6":"code","cca03357":"code","711cff29":"code","41634c94":"code","0351c579":"code","7da76cf0":"code","277b34f0":"code","837ecd07":"code","c282b263":"code","533b44e8":"code","6b79a41e":"code","6a0ff5b2":"code","f25ba12b":"code","30d14e16":"code","e27d3133":"code","32eb636d":"code","21100e54":"code","7bd4ad2e":"code","471cf3af":"code","ca849b49":"code","a4dd4d14":"code","0e1bd3b9":"code","12b257e7":"code","3191fc57":"code","13fdd67b":"code","abedd190":"code","d2c0bec7":"code","9b41c925":"code","265539a5":"code","fe35f44a":"code","2771eb68":"code","549fa082":"code","913573d5":"code","9b995124":"code","8a792c0a":"code","11260460":"code","4fd82b66":"code","66d580f7":"code","95a2013a":"code","dcb8912e":"code","ec14bab2":"code","63393431":"code","1a6305c2":"code","4d5cb230":"code","d69456e9":"code","88bc49cd":"code","31cc30cc":"code","d21e2138":"code","11f4ddd9":"markdown","c6e628be":"markdown","dd83eb6a":"markdown","27f6e80c":"markdown","1a573029":"markdown","6cb40acc":"markdown","b8f4576f":"markdown","12f1613d":"markdown","91ed2377":"markdown","636d68c0":"markdown","b27887ed":"markdown","85e5d246":"markdown","5c80e68e":"markdown","9ae5f63a":"markdown","8e957b32":"markdown","5a717b6e":"markdown","e35a195f":"markdown","855cb1da":"markdown"},"source":{"d69d2a4a":"import numpy as np \nimport pandas as pd \nimport pandas_profiling as pp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport eli5\n\nimport lightgbm as lgbm\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('max_columns',100)\npd.set_option('max_rows',100)","afb76c9a":"# Thanks to : https:\/\/www.kaggle.com\/aantonova\/some-new-risk-and-clusters-features\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","1400f200":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain = reduce_mem_usage(train)","3871e0b6":"train.head(3)","cca03357":"train.info()","711cff29":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest = reduce_mem_usage(test)","41634c94":"test.head(3)","0351c579":"test.info()","7da76cf0":"submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission.head(3)","277b34f0":"train.describe()","837ecd07":"test.describe()","c282b263":"numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train.columns.values.tolist()\nfor col in features:\n    if train[col].dtype in numerics: continue\n    categorical_columns.append(col)\n\nfor col in categorical_columns:\n    if col in train.columns:\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))","533b44e8":"target = train['SalePrice']\ndel train['SalePrice']","6b79a41e":"train = reduce_mem_usage(train)","6a0ff5b2":"train.info()","f25ba12b":"train = train.fillna(-1)\ntest = test.fillna(-1)","30d14e16":"X = train\nz = target","e27d3133":"#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\ntrain_set = lgbm.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgbm.Dataset(Xval, Zval, silent=False)","32eb636d":"params = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 45,\n        'learning_rate': 0.01,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgbm.train(params, train_set = train_set, num_boost_round=5000,\n                   early_stopping_rounds=50,verbose_eval=50, valid_sets=valid_set)","21100e54":"fig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgbm.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()","7bd4ad2e":"feature_score = pd.DataFrame(train.columns, columns = ['feature']) \nfeature_score['score_lgb'] = modelL.feature_importance()","471cf3af":"# Prediction\ny_train_lgb = modelL.predict(train, num_iteration=modelL.best_iteration).astype('int')\ny_preds_lgb = modelL.predict(test, num_iteration=modelL.best_iteration)","ca849b49":"#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]","a4dd4d14":"parms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:squarederror',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\n\nprint('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))","0e1bd3b9":"pred_xgb = modelx.predict(xgb.DMatrix(test))","12b257e7":"fig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()","3191fc57":"feature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\nfeature_score","13fdd67b":"# Prediction\ny_train_xgb = modelx.predict(xgb.DMatrix(train)).astype('int')\ny_preds_xgb = modelx.predict(xgb.DMatrix(test))","abedd190":"# Standardization for regression models\nScaler_train = preprocessing.MinMaxScaler()\ntrain = pd.DataFrame(\n    Scaler_train.fit_transform(train),\n    columns=train.columns,\n    index=train.index\n)","d2c0bec7":"test = pd.DataFrame(\n    Scaler_train.fit_transform(test),\n    columns=test.columns,\n    index=test.index\n)","9b41c925":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train, target)\ncoeff_logreg = pd.DataFrame(train.columns.delete(0))\ncoeff_logreg.columns = ['feature']\ncoeff_logreg[\"score_logreg\"] = pd.Series(logreg.coef_[0])\ncoeff_logreg.sort_values(by='score_logreg', ascending=False)","265539a5":"len(coeff_logreg)","fe35f44a":"# the level of importance of features is not associated with the sign\ncoeff_logreg[\"score_logreg\"] = coeff_logreg[\"score_logreg\"].abs()\nfeature_score = pd.merge(feature_score, coeff_logreg, on='feature')","2771eb68":"# Eli5 visualization\neli5.show_weights(logreg)","549fa082":"# Prediction\ny_train_logreg = logreg.predict(train).astype('int')\ny_preds_logreg = logreg.predict(test)","913573d5":"# Linear Regression\n\nlinreg = LinearRegression()\nlinreg.fit(train, target)\ncoeff_linreg = pd.DataFrame(train.columns.delete(0))\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"score_linreg\"] = pd.Series(linreg.coef_)\ncoeff_linreg.sort_values(by='score_linreg', ascending=False)","9b995124":"# Eli5 visualization\neli5.show_weights(linreg)","8a792c0a":"# the level of importance of features is not associated with the sign\ncoeff_linreg[\"score_linreg\"] = coeff_linreg[\"score_linreg\"].abs()","11260460":"feature_score = pd.merge(feature_score, coeff_linreg, on='feature')\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\nfeature_score","4fd82b66":"# Prediction\ny_train_linreg = linreg.predict(train).astype('int')\ny_preds_linreg = linreg.predict(test)","66d580f7":"#Thanks to https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# MinMax scale all importances\nfeature_score = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns,\n    index=feature_score.index\n)\n\n# Create mean column\nfeature_score['mean'] = feature_score.mean(axis=1)\n\n# Plot the feature importances\nfeature_score.sort_values('mean', ascending=False).plot(kind='bar', figsize=(20, 7))","95a2013a":"feature_score.sort_values('mean', ascending=False)","dcb8912e":"# Set weight of models\nw_lgb = 0.48\nw_xgb = 0.48\nw_logreg = 0.03\nw_linreg = 1 - w_lgb - w_xgb - w_logreg\nw_linreg","ec14bab2":"# Merging FI diagram\n# Create total column with different weights\n\n# Create merging column with different weights\nfeature_score['total'] = w_lgb*feature_score['score_lgb'] + w_xgb*feature_score['score_xgb'] + w_logreg*feature_score['score_logreg'] + w_linreg*feature_score['score_linreg']\n\n# Plot the feature importances\nplot_title = \"Consolidation feature importance diagrams (by merging values)\"\nfeature_score_sort = feature_score.sort_values('total', ascending=False)\nfeature_score_sort[:29].plot(kind='bar', figsize=(20, 10))","63393431":"feature_score.sort_values('total', ascending=False)","1a6305c2":"feature_score_sort = feature_score.sort_values('total', ascending=False)\nfeature_score_sort['features'] = feature_score_sort.index\nfeature_score_sort.columns = ['score_lgb', 'score_xgb', 'score_linreg', 'mean', 'total', 'feature']\nfeature_score_sort = feature_score_sort[['feature', 'score_lgb', 'score_xgb', 'score_linreg', 'mean', 'total']]\nfeature_score_sort","4d5cb230":"feature_score_sort.to_csv('feature_score_sort.csv', index=False)","d69456e9":"y_preds = w_lgb*y_preds_lgb + w_xgb*y_preds_xgb + w_logreg*y_preds_logreg + w_linreg*y_preds_linreg","88bc49cd":"submission['SalePrice'] = [1 if x>0.5 else 0 for x in y_preds]\nsubmission.head()","31cc30cc":"submission['SalePrice'].hist()","d21e2138":"submission.to_csv('submission.csv', index=False)","11f4ddd9":"[Go to Top](#0)","c6e628be":"### 5.1 LGBM <a class=\"anchor\" id=\"5.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","dd83eb6a":"### 8. Merging solutions and submission<a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","27f6e80c":"### 5.2 XGB<a class=\"anchor\" id=\"5.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","1a573029":"## 5. Tuning models, building the feature importance diagrams and prediction<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","6cb40acc":"## 2. Download datasets <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","b8f4576f":"### 5.3 Logistic Regression <a class=\"anchor\" id=\"5.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","12f1613d":"### 6. Showing Confusion Matrices<a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","91ed2377":"# Showing Confusion Matrix\n# Thanks to https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title, figsize=(5,4)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","636d68c0":"# Showing Confusion Matrix for XGB model\nplot_cm(y_train_xgb, z, 'Confusion matrix for XGB model', figsize=(7,7))","b27887ed":"# Showing Confusion Matrix for LGB model\nplot_cm(y_train_lgb, z, 'Confusion matrix for LGB model', figsize=(7,7))","85e5d246":"### 5.4 Linear Regression <a class=\"anchor\" id=\"5.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","5c80e68e":"## 3. FE & EDA <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","9ae5f63a":"# Showing Confusion Matrix for Logistic Regression\nplot_cm(y_train_logreg, z, 'Confusion matrix for Logistic Regression', figsize=(7,7))","8e957b32":"# Showing Confusion Matrix for Linear Regression\nplot_cm(y_train_linreg, z, 'Confusion matrix for Linear Regression', figsize=(7,7))","5a717b6e":"## 4. Preparing to modeling <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","e35a195f":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","855cb1da":"### 7. Comparison and merging of all feature importance diagrams <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}