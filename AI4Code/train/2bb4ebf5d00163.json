{"cell_type":{"12e8f9e8":"code","c051cc3b":"code","6073ee1d":"code","4739e365":"code","7603b326":"code","a3b55413":"code","c1a6ce20":"code","9b3c9a37":"code","3be401ba":"code","aecaa242":"code","6bae8519":"code","531b91d2":"code","fefad834":"code","2322e63a":"code","22a1731f":"code","759223d0":"code","da5dd95a":"code","5de638a2":"code","8f2a3c31":"code","0e78846b":"code","e61e85f7":"code","6a5dcabf":"code","fc09bc30":"code","f02af352":"code","a28cba06":"code","f4e39f2c":"code","c95aee7d":"code","45f8c312":"markdown","5a74fd73":"markdown","b30dd6a9":"markdown","93f6024c":"markdown","4bb39c58":"markdown"},"source":{"12e8f9e8":"import torch\nimport torch.nn as nn","c051cc3b":"class conv_block(nn.Module):\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(conv_block, self).__init__()\n        self.relu = nn.ReLU()\n        self.cnn = nn.Conv2d(in_channels, out_channels, **kwargs)\n        self.batchnorm = nn.BatchNorm2d(out_channels)\n    \n    def forward(self, x):\n        return self.relu(self.batchnorm(self.cnn(x)))","6073ee1d":"class inception_block(nn.Module):\n    def __init__(self, in_channels, out1x1, red3x3, out3x3, red5x5, out5x5, out1x1pool):\n        super(inception_block, self).__init__()\n        \n        self.branch1 = conv_block(in_channels, out1x1, kernel_size=1)\n        \n        self.branch2 = nn.Sequential(\n            \n            conv_block(in_channels, red3x3, kernel_size=1),\n            conv_block(red3x3, out3x3, kernel_size=3, padding=1)\n        )\n        \n        self.branch3 = nn.Sequential(\n            conv_block(in_channels, red5x5, kernel_size=1),\n            conv_block(red5x5, out5x5, kernel_size=5, padding=2)\n        )\n        \n        self.branch4 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n            conv_block(in_channels, out1x1pool, kernel_size=1)\n        )\n        \n    def forward(self, x):\n        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1)","4739e365":"class GoogLeNet(nn.Module):\n    def __init__(self, in_channels=3, num_classes=104):\n        super(GoogLeNet,self).__init__()\n        self.conv1 = conv_block(in_channels, 64,kernel_size=(7,7), stride=(2,2),padding=(3,3))\n        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.conv2 = conv_block(64, 192, kernel_size=3, stride=1, padding=1)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.inception3a = inception_block(192, 64, 96, 128, 16, 32, 32)\n        self.inception3b = inception_block(256, 128, 128, 192, 32, 96, 64)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.inception4a = inception_block(480, 192, 86, 208, 16, 48, 64)\n        self.inception4b = inception_block(512, 160, 112, 224, 24, 64, 64)\n        self.inception4c = inception_block(512, 128, 128, 256, 24, 64, 64)\n        self.inception4d = inception_block(512, 112, 144, 288, 32, 64, 64)\n        self.inception4e = inception_block(528, 256, 160, 320, 32, 128, 128)\n        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.inception5a = inception_block(832, 256, 160, 320, 32, 128, 128)\n        self.inception5b = inception_block(832, 384, 192, 384, 48, 128, 128)\n        \n        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n        self.dropout = nn.Dropout(p=0.4)\n        \n        self.fc1 = nn.Linear(1024,num_classes)\n    \n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.maxpool1(x)\n        \n        x = self.conv2(x)\n        x = self.maxpool2(x)\n        \n        x = self.inception3a(x)\n        x = self.inception3b(x)\n        x = self.maxpool3(x)\n        \n        x = self.inception4a(x)\n        x = self.inception4c(x)\n        x = self.inception4d(x)\n        x = self.inception4e(x)\n        x = self.maxpool4(x)\n        \n        x = self.inception5a(x)\n        x = self.inception5b(x)\n        x = self.avgpool(x)\n        x = self.dropout(x)\n        \n        x = x.reshape(x.shape[0], -1)\n        x = self.fc1(x)\n        \n        return x\n        \n        \n        \n        \n    ","7603b326":"model = GoogLeNet()","a3b55413":"x = torch.randn(10, 3, 224,224)\ny = model(x)\ny.shape","c1a6ce20":"from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimg = Image.open('..\/input\/104-flowers-garden-of-eden\/jpeg-224x224\/train\/balloon flower\/10108.jpeg')\nimg = np.array(img)\nimg = torch.tensor(img)\nplt.imshow(img)\nimg.shape\n","9b3c9a37":"x = img.permute(2,0,1)\nx = x.unsqueeze(0)\nx = x.float()\nprint(x.shape)\ny = model(x)\n\"output shape: \" , y.shape","3be401ba":"from torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n","aecaa242":"normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\ntrain_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n                                      transforms.RandomHorizontalFlip(0.5),\n                                      transforms.ToTensor(),\n                                      normalize])","6bae8519":"test_transform = transforms.Compose([transforms.Resize(224),\n                                     transforms.ToTensor(),\n                                     normalize])","531b91d2":"from torchvision.datasets import ImageFolder","fefad834":"train_data = ImageFolder(root=\"..\/input\/104-flowers-garden-of-eden\/jpeg-224x224\/train\", transform=train_transform)\n\nz,y = train_data[0]\nprint(z.shape,y)\n\nplt.imshow(z.permute(1,2,0))","2322e63a":"train_dataloader = DataLoader(dataset=train_data, shuffle=True, batch_size=64)","22a1731f":"test_data = ImageFolder(root=\"..\/input\/104-flowers-garden-of-eden\/jpeg-224x224\/val\", transform=test_transform)\n\nz,y = test_data[0]\nprint(z.shape, y)\n\nplt.imshow(z.permute(1,2,0))","759223d0":"test_dataloader = DataLoader(dataset=test_data,shuffle=True, batch_size=64)","da5dd95a":"num_epoch = 5\nbatch_size= 64\nlearning_rate = 0.001","5de638a2":"from torchvision.models import googlenet","8f2a3c31":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","0e78846b":"model = googlenet(pretrained=True)","e61e85f7":"model.fc = nn.Linear(in_features=1024, out_features=104)","6a5dcabf":"model = model.to(device=device)","fc09bc30":"loss_criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","f02af352":"for epoch in range(num_epoch):\n    for data, target in train_dataloader:\n        data = data.to(device=device)\n        target = target.to(device=device)\n        \n        score = model(data)\n        optimizer.zero_grad()\n        loss = loss_criterion(score,target)\n        \n        loss.backward()\n        \n        optimizer.step()\n    print(f\"for epoch {epoch}, loss: {loss}\")\n        \n        ","a28cba06":"def check_accuracy(model, loader):\n    model.eval()\n    \n    correct_sample = 0\n    total_sample = 0\n    \n    for x, y  in loader:\n        x = x.to(device=device)\n        y = y.to(device=device)\n        \n        score = model(x)\n        \n        _, predictions = score.max(1)\n        \n        correct_sample = (y==predictions).sum()\n        total_sample = predictions.shape[0]\n    \n    model.train()\n    print(f\"Total accuracy : {float(correct_sample\/total_sample)*100}\")\n    ","f4e39f2c":"check_accuracy(model, train_dataloader)","c95aee7d":"check_accuracy(model, test_dataloader)","45f8c312":"The link to the paper: `https:\/\/static.googleusercontent.com\/media\/research.google.com\/en\/\/pubs\/archive\/43022.pdf`\n\nThe salient features:\n\n1. Uses 1x1 cnn for dimensionality reduction\n\nInception net\n\n![](https:\/\/i.ytimg.com\/vi\/KfV8CJh7hE0\/maxresdefault.jpg)","5a74fd73":"I dont want to wastetime by training from scratch so lets use pretrained model.","b30dd6a9":"So it has multiple conv block and inception block lets build them one by one\n# Conv block & Inception block","93f6024c":"lets use flower dataset","4bb39c58":"![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20200429201421\/Inception-layer-by-layer.PNG)"}}