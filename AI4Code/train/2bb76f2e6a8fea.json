{"cell_type":{"58e89c1d":"code","c7b1fe9a":"code","26db3893":"code","99cc2a63":"code","b4c8038c":"code","c6b58fb5":"code","152a4b72":"code","59d02406":"markdown","0058718d":"markdown","a90209f7":"markdown","f76283c2":"markdown","e84d3a47":"markdown","83686a54":"markdown","52a7b038":"markdown"},"source":{"58e89c1d":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport gc\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import *\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom pickle import load\nimport matplotlib.pyplot as plt\nimport json\n!cp ..\/input\/ventilator-feature-engineering\/VFE.py .","c7b1fe9a":"train = np.load('..\/input\/ventilator-feature-engineering\/x_train.npy')\ntargets = np.load('..\/input\/ventilator-feature-engineering\/y_train.npy')","26db3893":"# model creation\ndef create_lstm_model():\n\n    x0 = tf.keras.layers.Input(shape=(train.shape[-2], train.shape[-1]))  \n\n    lstm_layers = 4 # number of LSTM layers\n    lstm_units = [940, 540, 462, 316]\n    lstm = Bidirectional(keras.layers.LSTM(lstm_units[0], return_sequences=True))(x0)\n    for i in range(lstm_layers-1):\n        lstm = Bidirectional(keras.layers.LSTM(lstm_units[i+1], return_sequences=True))(lstm)    \n    lstm = Dropout(0.002)(lstm)\n    lstm = Dense(lstm_units[-1], activation='swish')(lstm)\n    lstm = Dense(1)(lstm)\n\n    model = keras.Model(inputs=x0, outputs=lstm)\n    model.compile(optimizer=\"adam\", loss=\"mae\")\n    \n    return model","99cc2a63":"BATCH_SIZE = 256\nNFOLDS = 5\nSEED = 777\nEPOCHS = 300\n\nparams = {}\nparams['BATCH_SIZE'] = BATCH_SIZE\nparams['NFODLS'] = NFOLDS\nparams['SEED'] = SEED\nparams['EPOCHS'] = EPOCHS\nwith open('train_params.json', 'w') as fp:\n    json.dump(params, fp, indent=4)","b4c8038c":"# Function to get hardware strategy\ndef get_hardware_strategy():\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        tf.config.optimizer.set_jit(True)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    return tpu, strategy\n\ntpu, strategy = get_hardware_strategy()","c6b58fb5":"hist = []\nfolds = [0,1] # folds to train\n\nwith strategy.scope():\n    kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n    \n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        if fold in folds:\n            print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n            folds.append(fold)\n            X_train, X_valid = train[train_idx], train[test_idx]\n            y_train, y_valid = targets[train_idx], targets[test_idx]\n            \n            model = create_lstm_model()\n            model.compile(optimizer=\"adam\", loss=\"mae\")\n            \n            #checkpoint_filepath = f\"lstm_fold_{fold}.hdf5\"\n            checkpoint_filepath = '\/kaggle\/working\/lstm_fold{}.hdf5'.format(fold)\n\n            scheduler = ExponentialDecay(1e-3, 400*((len(train)*0.8)\/BATCH_SIZE), 1e-5)\n            #lr = LearningRateScheduler(scheduler, verbose=0)\n            lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n            es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n            sv = keras.callbacks.ModelCheckpoint(\n                checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n                save_weights_only=False, mode='auto', save_freq='epoch',\n                options=None\n            )\n            hist.append(model.fit(X_train, y_train, \n                                  validation_data=(X_valid, y_valid), \n                                  epochs=EPOCHS, batch_size=BATCH_SIZE, \n                                  callbacks=[lr, es, sv]))\n        \n            del X_train, X_valid, y_train, y_valid, model\n            gc.collect()","152a4b72":"colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\nplt.figure(figsize=(16,16))\nfor i in range(len(hist)):\n    plt.plot(hist[i].history['loss'], linestyle='-', color=colors[i], label='Train, fold #{}'.format(str(folds[i])))\nfor i in range(len(hist)):\n    plt.plot(hist[i].history['val_loss'], linestyle='--', color=colors[i], label='Validation, fold #{}'.format(str(folds[i])))\nplt.ylim(top=1)\nplt.title('Model Loss')\nplt.ylabel('MAE')\nplt.xlabel('Epoch')\nplt.legend()\nplt.grid(which='major', axis='both')\nplt.show();","59d02406":"# Training - LSTM based model\nThis notebook is part of a series:  \n  * [Ventilator: Feature engineering](https:\/\/www.kaggle.com\/mistag\/ventilator-feature-engineering)\n  * [Keras model tuning with Optuna](https:\/\/www.kaggle.com\/mistag\/keras-model-tuning-with-optuna)\n  * [train] Ventilator LSTM Model - part I\n  * [[train] Ventilator LSTM Model - part 2](https:\/\/www.kaggle.com\/mistag\/train-ventilator-lstm-model-part-ii)\n  * [[pred] Ventilator LSTM Model](https:\/\/www.kaggle.com\/mistag\/pred-ventilator-lstm-model)\n  \n## References\nThe code is based on these references:  \n  * [Improvement base on Tensor Bidirect LSTM](https:\/\/www.kaggle.com\/kensit\/improvement-base-on-tensor-bidirect-lstm-0-173\/notebook) by [Ken Sit](https:\/\/www.kaggle.com\/kensit)\n  * [Ensemble Folds with MEDIAN - [0.153]](https:\/\/www.kaggle.com\/cdeotte\/ensemble-folds-with-median-0-153) by [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte)","0058718d":"# Model","a90209f7":"# Training\nFirst define a few parameters that will also be used in other notebooks:","f76283c2":"HW strategy:","e84d3a47":"Let's take a look at the learning curves.","83686a54":"![logo](https:\/\/cdn.freelogovectors.net\/wp-content\/uploads\/2018\/07\/tensorflow-logo.png)","52a7b038":"# Dataset"}}