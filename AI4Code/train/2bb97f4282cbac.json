{"cell_type":{"00aae6fe":"code","4974838d":"code","49755e5e":"code","c60bbf6c":"code","93d1617a":"code","610c5ddb":"code","555ae0d2":"code","99e0130b":"code","52c37216":"code","1b5f6c09":"code","1ee9f01a":"code","28cdac04":"markdown","6050333d":"markdown","611a8d1e":"markdown","6d7e488f":"markdown","bfd1c551":"markdown","aaf6e4ee":"markdown","e2532063":"markdown"},"source":{"00aae6fe":"!pip install kaggle-environments -U\nfrom kaggle_environments import make\n# create the environment. You can also specify configurations for seed and loglevel as shown below. If not specified, a random seed is chosen. \n# loglevel default is 0. \n# 1 is for errors, 2 is for match warnings such as units colliding, invalid commands (recommended)\n# 3 for info level, and 4 for everything (not recommended)\n# set annotations True so annotation commands are drawn on visualizer\n# set debug to True so print statements get shown\n# env = make(\"lux_ai_2021\", configuration={\"seed\": 562124210, \"loglevel\": 2, \"annotations\": True}, debug=True)\n# run a match between two simple agents, which are the agents we will walk you through on how to build!\n# steps = env.run([\"simple_agent\", \"simple_agent\"])\n# if you are viewing this outside of the interactive jupyter notebook \/ kaggle notebooks mode, this may look cutoff\n# render the game, feel free to change width and height to your liking. We recommend keeping them as large as possible for better quality.\n# you may also want to close the output of this render cell or else the notebook might get laggy\n# env.render(mode=\"ipython\", width=1200, height=800)","4974838d":"%%writefile agent.py\n# for kaggle-environments\nfrom lux.game import Game\nfrom lux.game_map import Cell, RESOURCE_TYPES, Position\nfrom lux.constants import Constants\nfrom lux.game_constants import GAME_CONSTANTS\nfrom lux import annotate\nimport math\nimport sys\n\nimport numpy as np\nimport pprint","49755e5e":"# Some helper functions (not used by agent, just for EDA)\n\ndef obj_dir(obj):\n    \"\"\"\n    >>> obj_dir(game_state.players[0])\n    \"\"\"\n    print(type(obj))\n    print()\n    for a in dir(obj):\n        if not a.startswith('__'):\n            print(a, getattr(obj, a))\n        \n\ndef find_by_type(game_state, type_name='resource'):\n    \"\"\"\n    type_name (list): 'resource', 'citytile', 'road'\n    \"\"\"\n    resource_tiles: list[Cell] = []\n    width, height = game_state.map_width, game_state.map_height\n    for y in range(height):\n        for x in range(width):\n            cell = game_state.map.get_cell(x, y)\n            if getattr(cell, type_name):\n                resource_tiles.append(cell)\n    return resource_tiles\n\n# the next snippet finds the closest resources that we can mine given position on a map\ndef find_closest_resources(pos, player, resource_tiles):\n    closest_dist = math.inf\n    closest_resource_tile = None\n    for resource_tile in resource_tiles:\n        # we skip over resources that we can't mine due to not having researched them\n        if resource_tile.resource.type == Constants.RESOURCE_TYPES.COAL and not player.researched_coal(): continue\n        if resource_tile.resource.type == Constants.RESOURCE_TYPES.URANIUM and not player.researched_uranium(): continue\n        dist = resource_tile.pos.distance_to(pos)\n        if dist < closest_dist:\n            closest_dist = dist\n            closest_resource_tile = resource_tile\n    return closest_resource_tile","c60bbf6c":"%%writefile -a agent.py\n# Helper functions used by agent\n\ndef get_researched_mask(research_points):\n    \"\"\"Return a mask of available resources\"\"\"\n    mask = [Constants.RESOURCE_TYPES.WOOD]\n    if research_points >= GAME_CONSTANTS['PARAMETERS']['RESEARCH_REQUIREMENTS']['URANIUM']:\n        mask += [Constants.RESOURCE_TYPES.COAL, Constants.RESOURCE_TYPES.URANIUM]\n    elif research_points >= GAME_CONSTANTS['PARAMETERS']['RESEARCH_REQUIREMENTS']['COAL']:\n        mask += [Constants.RESOURCE_TYPES.COAL]\n    return mask\n\ndef closest_node(node, nodes):\n    \"\"\"Credit @glmcdona \n    https:\/\/www.kaggle.com\/glmcdona\/lux-ai-deep-reinforcement-learning-ppo-example\n    https:\/\/codereview.stackexchange.com\/questions\/28207\/finding-the-closest-point-to-a-list-of-points\n    \"\"\"\n    dist_2 = np.sum((nodes - node)**2, axis=1)\n    return np.argmin(dist_2)\n\ndef min_distance(node, nodes):\n    assert node.shape == (2,), print(node.shape)\n    assert nodes.shape[1] == 2\n    dist_2 = np.sum((nodes - node)**2, axis=1)\n    return np.min(dist_2)\n\ndef rank_resources(resources: np.ndarray, \n                   units: np.ndarray, \n                   top_k: int = None, \n                   d_thd: int = None) -> tuple:\n    \"\"\"Given a list of units, rank the list of resources by proximity to any unit.\n    \n    e.g. \n    \n    >>> units = np.array([[  4,  27, 100,   0,   0,   0,   0,   0],])\n\n    >>> resources = np.array([[  4,  27, 238,   0,   0,   0,   0,   0],\n                              [  4,  11, 209,   0,   0,   0,   0,   0],\n                              [  4,  28, 238,   0,   0,   0,   0,   0],\n                              [  5,  11, 126,   0,   0,   0,   0,   0],\n                              [  3,  28,  32,   0,   0,   0,   0,   0],])\n\n    >>> d, r = rank_resources(resources, units)\n    \n    If top_k is provided, return only the top K resources by distance\n    If d_thd is provided, return only the resources within thd distance\n    If both are provided, top K takes precedence\n    \"\"\"\n    distances = [min_distance(resource, units[:, :2]) for resource in resources[:, :2]] # TODO: this will break when I add team\/unit type to input vectors\n    to_be_sorted = np.c_[distances, resources]\n    done_sorted = to_be_sorted[np.argsort(to_be_sorted[:, 0])]\n    if top_k:\n        return done_sorted[:top_k, 0], done_sorted[:top_k, 1:]\n    elif d_thd:\n        return done_sorted[np.where(done_sorted[:, 0] <= d_thd)][:, 0], done_sorted[np.where(done_sorted[:, 0] <= d_thd)][:, 1:]\n    return done_sorted[:, 0], done_sorted[:, 1:]  # distances, resources","93d1617a":"%%writefile -a agent.py\n\n# from dataclasses import dataclass\nfrom typing import Tuple\n\nclass Agent:\n    UNIT_ACTION_RANGE: Tuple[int] = (-1, 4)\n    CITYTILE_ACTION_RANGE: Tuple[int] = (-1, 2)\n    \n    def __init__(self, team=0):\n        self.team = team\n    \n    def get_observation(self, game_state: Game):\n        \"\"\"\n        Returns:\n            - units\n            - citytiles\n            - resources\n            - research\n          \n        NOTES:\n        Observations\/updates:\n            \"rp 1 25\"                       research_points player amount\n            \"r wood 0 5 500\"                resource type pos.x pos.y amount\n            \"r coal 6 0 379\"\n            \"r uranium 6 6 323\"             \n            \"u 0 1 u_28 10 10 0 40 0 0\"     unit type team id pos.x pos.y cooldown cargo.wood cargo.coal cargo.uranium\n            \"c 0 c_1 210 23\"                city team id fuel light_upkeep\n            \"ct 0 c_1 3 9 6\"                citytile team city.id pos cooldown\n            \"ccd 0 4 6\"                     cellcooldown pos.x pos.y cooldown  (i.e. road or citytile, citytiles have ccd=6)\n        \"\"\"        \n        # CITY: 'cityid', 'citytiles', 'fuel', 'get_light_upkeep', 'light_upkeep', 'team'\n        # CITYTILE: 'build_cart', 'build_worker', 'can_act', 'cityid', 'cooldown', 'pos', 'research', 'team'\n        \n        # Collate units and citytiles that can act this turn, in the appropriate vector format\n        v_units = []\n        v_citytiles = []\n        v_research = []\n        \n        # TODO: ignoring opponent for now\n        # for team, player in enumerate(game_state.players[:1]): \n        player = game_state.players[self.team]\n        self._active_citytiles = [(city, citytile) for city in player.cities.values() for citytile in city.citytiles if citytile.can_act()]\n        self._active_units = [unit for unit in player.units if unit.can_act()]\n            \n        # Research\n        v_research.append(player.research_points)\n\n        # Units\n        for unit in self._active_units:\n            v_units.append([\n                              # team, \n                              # unit.type,  # TODO: will need this once I incorporate carts\n                              unit.pos.x, unit.pos.y,\n                              unit.cargo.wood, unit.cargo.coal, unit.cargo.uranium,\n                              0, 0, 0,  # citytiles padding\n            ])\n\n        # Citytiles\n        for city, citytile in self._active_citytiles:\n            v_citytiles.append([\n                                  # team, \n                                  # 2,  # i.e. \"made up\" unit type\n                                  citytile.pos.x, citytile.pos.y,\n                                  0, 0, 0,  # units padding\n                                  int(city.cityid.split('_')[-1]), city.fuel, city.light_upkeep\n            ])\n        \n        \n        # Doing this here so that it is outside the \"for player in players\" loop, \n        # to collate both players (once I incorporate opponent)\n        v_research = np.array(v_research, dtype=np.int16)\n        v_units = np.array(v_units, dtype=np.int16)\n        v_citytiles = np.array(v_citytiles, dtype=np.int16)\n\n        # Resources relative to this player (based on what's researched, what's nearest to units)\n        researched_resources = get_researched_mask(player.research_points)\n        v_r = []\n        for r in game_state.map.map:\n            for cell in r:\n                crs = [0, 0, 0]\n                if cell.resource and (cell.resource.type in researched_resources):\n                    crs[researched_resources.index(cell.resource.type)] = cell.resource.amount\n                    v_r.append([\n                                  # -1,  # team (impartial) \n                                  # 3,  # \"made up\" type\n                                  cell.pos.x, cell.pos.y,\n                                  *crs,\n                                  0, 0, 0,  # citytile padding\n                                  # cell.road\n                                 ])\n        v_resources = np.array(v_r, dtype=np.int16)\n\n        # Check that units are still alive, otherwise no need to sort resources\n        # TODO: (although citytiles could still make more units later on)\n        if not v_units.shape == (0,):\n            distances, v_resources = rank_resources(v_resources, v_units, top_k=10)\n            # TODO: not using distances further for now (only for ranking resources)\n        \n        return v_research, v_units, v_citytiles, v_resources\n    \n#     def _get_action_vectors(self):\n#         pass\n    \n#     def _convert_to_actions(self, action_vectors):\n#         pass\n    \n    def get_actions(self, game_state: Game) -> list:    \n        \"\"\"\n        Actions:\n             \"m u_18 n\"                 move u_18 north\n             \"?\"                        pillage road\n             \"t u_14 u_24 coal 2000\"    transfer \n             \"bcity u_17\"               build citytile by u_17\n\n             \"bw 1 7\"                   build worker in the citytile at (1, 7)\n             \"bc 3 9\"                   build cart in the citytile at (3, 9)\n             \"r 4 13\"                   research in the citytile at (4, 13) \n        \"\"\"\n        player = game_state.players[self.team]\n        v_research, v_units, v_citytiles, v_resources = self.get_observation(game_state)\n        \n        # Get encoded actions vector\n        action_vector = []\n        for unit in self._active_units:\n            # TODO: random actions for now\n            action_vector.append(np.random.randint(*self.UNIT_ACTION_RANGE))\n            # TODO: if not unit.can_build(game_map) and chosen action is 'bcity': penalise\n            # TODO: if moving onto an occupied position: penalise (once this is part of observation)\n        for _, citytile in self._active_citytiles:\n            action_vector.append(np.random.randint(*self.CITYTILE_ACTION_RANGE))\n            \n        # NOTE: the values (e.g. \"move south\") separated by spaces are method, argument\n        action_map = {'unit': {-1: 'build_city',  # TODO: this is for readability, it could just be a list with \"build_city\" last\n                                0: 'DO_NOTHING',\n                                1: 'move n',\n                                2: 'move s',\n                                3: 'move e',\n                                4: 'move w',  # TODO: add other actions\n                              },  \n                     'citytile': {-1: 'research', \n                                   0: 'DO_NOTHING',\n                                   1: 'build_worker',\n                                   #2: 'build_cart'\n                                 }\n                     }\n        \n        actions = []\n        # TODO: vectorise this operation for speed\n        # Get actual actions from encoded actions\n        for i, unit in enumerate(self._active_units):\n            if action_vector[i]:  # i.e. if the action chosen is not \"DO_NOTHING\"\n                s = action_map['unit'][action_vector[i]].split()\n                if len(s) > 1:\n                    action = getattr(unit, s[0])(*s[1:])\n                else:\n                    action = getattr(unit, s[0])()\n                actions.append(action)\n                \n        for j, cc in enumerate(self._active_citytiles, start=len(self._active_units)):\n            citytile = cc[1]  # city = cc[0]\n            if action_vector[j]:  # i.e. if the action chosen is not \"DO_NOTHING\"\n                s = action_map['citytile'][action_vector[j]].split()\n                if len(s) > 1:\n                    action = getattr(citytile, s[0])(*s[1:])\n                else:\n                    action = getattr(citytile, s[0])()\n                actions.append(action)\n\n        return actions","610c5ddb":"%%writefile -a agent.py\n\ngame_state = None\nmyagent = None\nresources_, units_, citytiles_ = None, None, None  # for debugging later\ndef agent(observation, configuration):\n    global game_state\n    global myagent\n    # for debugging later\n#     global units_\n#     global citytiles_\n#     global resources_\n\n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n    else:\n        game_state._update(observation[\"updates\"])\n\n    ###################################################################\n    \n    if observation['step'] == 0:\n        myagent = Agent(team=observation.player)\n    \n    actions = myagent.get_actions(game_state)\n    \n    ###################################################################\n    # DEBUGGING\n#     if not game_state.turn % 60:\n#         print(np.c_[distances[:10], all_cells[:10]])\n    \n#     if game_state.turn == 6:\n#         print(np.c_[distances[:10], all_cells[:10]])\n#         print('Research points:', player.research_points)\n#         print()\n#         print(f'Units {all_units.shape}:')\n#         print(all_units)\n#         print()\n#         print(f'City tiles {all_citytiles.shape}:')\n#         print(all_citytiles)\n#         print()\n#         print(f'Resources {all_cells.shape}:')\n#         print(all_cells)\n#         print()\n#         print(f'Distances {distances.shape}:')\n#         print(distances)\n    ###################################################################\n    \n    return actions","555ae0d2":"env = make(\"lux_ai_2021\", configuration={\n    #\"seed\": 562124210, \n    \"loglevel\": 2, \n    \"annotations\": True}, debug=True)\nsteps = env.run([agent, \"simple_agent\"])\nenv.render(mode=\"ipython\", width=1200, height=800)","99e0130b":"# env = make(\"lux_ai_2021\", configuration={\n#     #\"seed\": 562124210, \n#     \"loglevel\": 2, \n#     \"annotations\": True}, debug=True)\n\n# # Training agent in first position (player 1) against the default random agent.\n# trainer = env.train([None, \"random\"])\n\n# obs = trainer.reset()\n# for _ in range(100):\n#     env.render()\n#     action = 0 # Action for the agent being trained.\n#     obs, reward, done, info = trainer.step(action)\n#     if done:\n#         obs = trainer.reset()","52c37216":"# pad units and citytiles for input\n# units_ = np.pad(units_, ((0,10), (0,0)), 'constant', constant_values=-1)\n# citytiles_ = np.pad(citytiles_, ((0,10), (0,0)), 'constant', constant_values=-1)","1b5f6c09":"!tar -czf submission.tar.gz *","1ee9f01a":"import json\nreplay = env.toJSON()\nwith open(\"replay.json\", \"w\") as f:\n    json.dump(replay, f)","28cdac04":"## Submit\nNow open the \/kaggle\/working folder and find submission.tar.gz, download that file, navigate to the \"MySubmissions\" tab in https:\/\/www.kaggle.com\/c\/lux-ai-2021\/ and upload your submission! It should play a validation match against itself and once it succeeds it will be automatically matched against other players' submissions. Newer submissions will be prioritized for games over older ones. Your team is limited in the number of succesful submissions per day so we highly recommend testing your bot locally before submitting.","6050333d":"## CLI Tool\n\nThere's a separate CLI tool that can also be used to run matches. It's recommended for Jupyter Notebook users to stick with just this notebook, and all other users including python users to follow the instructions on https:\/\/github.com\/Lux-AI-Challenge\/Lux-Design-2021\n\nThe other benefit however of using the CLI tool is that it generates much smaller, \"stateless\" replays and also lets you run a mini leaderboard on multiple bots ranked by various ranking algorithms","611a8d1e":"# LuxAI Season 1 (2021): Random Agent\n\nThe purpose of this notebook was to get acquainted with the problem, map the input and output spaces, reduce their complexity.\n\nI massage the observations and convert them to a vector space for future use with RL agents. For now, just a random agent to check interface.\n\n\u2014[Jaime](http:\/\/jaime.rs)","6d7e488f":"## Suggestions \/ Strategies\n\nThere are a lot of places that could be improved with the agent we have in this tutorial notebook. Here are some!\n\n- Using the build city action to build new cities and thus build new units\n- Having cities perform research each turn to unlock new resources\n- Writing collision-free code that lets units move smoothly around and through each other when navigating to targets\n- Mining resources near your opponent's citytiles so they have less easy access to resources\n- Using carts to deliver resources from far away clusters of wood, coal, uranium to a city in need\n- Sending worker units over to the opponent's roads and pillaging them to slow down their agent\n- Optimizing over how much to mine out of forests before letting them regrow so you can build more cities and get sustainable fuel","bfd1c551":"We have something that survives! We are now ready to submit something to the leaderboard. The code below compiles all we have built so far into one file that you can then submit to the competition leaderboard","aaf6e4ee":"## Create a submission\nNow we need to create a .tar.gz file with main.py (and agent.py) at the top level. We can then upload this!","e2532063":"## Additional things to check out\n\nMake sure you check out the Bot API at https:\/\/github.com\/Lux-AI-Challenge\/Lux-Design-2021\/tree\/master\/kits\n\nThis documents what you can do using the starter kit files in addition to telling you how to use the annotation debug commands that let you annotate directly on a replay (draw lines, circle things etc.)\n\nYou can also run the following below to save a episode to a JSON replay file. These are the same as what is shown on the leaderbaord and you can upload the replay files to the online replay viewer https:\/\/2021vis.lux-ai.org\/\n\n\nFor a local (faster) version of the replay viewer, follow installation instructions here https:\/\/github.com\/Lux-AI-Challenge\/Lux-Viewer-2021"}}