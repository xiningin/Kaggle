{"cell_type":{"6f592d42":"code","23583562":"code","c3fff4d1":"code","f3217b23":"code","f908386c":"code","0f63a6bd":"code","2b148420":"code","a19c9be0":"code","4f390cad":"code","1af8eb5f":"code","66025358":"code","f4c8f8f2":"code","d73ac0d7":"code","5e4fca2c":"code","4752a0ee":"code","df68842b":"code","69da958d":"code","fd15eb5e":"code","9a2e34bb":"code","9cc66c30":"code","83ed26b0":"code","f6a062de":"code","2fcc9463":"markdown","064273df":"markdown","b7499d0a":"markdown","d4ff2ef2":"markdown","28246924":"markdown","756ccc7f":"markdown","213517ef":"markdown","404494ba":"markdown","4a27066c":"markdown","7f70cd20":"markdown","e7084270":"markdown","11f09a42":"markdown","0f0274b8":"markdown","3803e951":"markdown","b5a48aa0":"markdown","10f121b1":"markdown","ead4adec":"markdown","ead9f050":"markdown","b5b3c5d1":"markdown","7f280b67":"markdown","6987de2e":"markdown","4c4541a2":"markdown","89f24a25":"markdown","35f696ef":"markdown","7ed6920b":"markdown","913e1df3":"markdown","c38de913":"markdown","a23baaba":"markdown","9cf4e3c0":"markdown","7eb1a35a":"markdown","0a0195a6":"markdown","623bce8f":"markdown","28f12f44":"markdown","2ebfd0df":"markdown","fdb25006":"markdown","d2f65380":"markdown","369ec2c8":"markdown","6085d13c":"markdown","0e44619e":"markdown","a6c3a53a":"markdown"},"source":{"6f592d42":"# Importing basic libraries\n\n# To work with tables and arrays\nfrom numpy import ravel\nimport numpy as np\nimport pandas as pd\n\n# To work with dates\nimport datetime\n\n# To visualize\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# To work with SQL if we need\nfrom pandasql import sqldf\npysqldf = lambda q: sqldf(q, globals())\n\n# To do grid searchs\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","23583562":"features   = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip')\nstores     = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv')\ntrain      = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip')\ntest       = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip')\nsample_sub = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip')","c3fff4d1":"from IPython.core.display import HTML\n\ndef multi_table(table_list):\n    ''' Accepts a list of IpyTable objects and returns a table which contains each IpyTable in a cell\n    '''\n    return HTML(\n        '<table><tr style=\"background-color:white;\">' + \n        ''.join(['<td>' + table.to_html(max_rows=7) + '<\/td>' for table in table_list]) +\n        '<\/tr><\/table>'\n    )\n\nmulti_table([features,stores,train,test])","f3217b23":"# Merging\nstores_features = pd.merge(features,stores,on='Store',how='inner')\ndf_main = pd.merge(stores_features,train,on=['Store','Date','IsHoliday'],how='inner')\ndf_main_test = pd.merge(stores_features,test,on=['Store','Date','IsHoliday'],how='inner')\n\n# Visual check\ndf_main.head(3)","f908386c":"for col in df_main.columns:\n    pct_missing = np.mean(df_main[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))","0f63a6bd":"# Taking only numerical or important columns\nfeatures = df_main[['Temperature','Fuel_Price','MarkDown1', 'MarkDown2','MarkDown3','MarkDown4','MarkDown5','CPI','Unemployment','IsHoliday','Size','Dept']]\n\n# Iterating and plotting\nfor column in features:\n    plt.figure()\n    sns.boxplot(data=features, x=column, width=30)","2b148420":"# Turning string type date into timestamp\ndf_main['Date'] = pd.to_datetime(df_main['Date'])\ndf_main_test['Date'] = pd.to_datetime(df_main_test['Date'])\n\n# Creating Year, Month, Week_number and Quarter columns using date column\ndf_main['Year'],df_main['Month'],df_main['Week_number'],df_main['Quarter'] = df_main['Date'].dt.year, df_main['Date'].dt.month, df_main['Date'].dt.strftime(\"%V\"), df_main['Date'].dt.quarter\ndf_main_test['Year'],df_main_test['Month'],df_main_test['Week_number'],df_main_test['Quarter'] = df_main_test['Date'].dt.year, df_main_test['Date'].dt.month, df_main_test['Date'].dt.strftime(\"%V\"), df_main_test['Date'].dt.quarter\n\n# Viewing the new columns\ndf_main[['Year','Month','Week_number','Quarter']]","a19c9be0":"# Creating Holiday_names dict\nholiday_names = {'Holiday': ['Super Bowl','Super Bowl','Super Bowl','Super Bowl','Labor Day','Labor Day','Labor Day','Labor Day','Thanksgiving','Thanksgiving','Thanksgiving','Thanksgiving','Christmas','Christmas','Christmas','Christmas']\n                 ,'Date':['12-Feb-10', '11-Feb-11', '10-Feb-12', '8-Feb-13','10-Sep-10', '9-Sep-11', '7-Sep-12', '6-Sep-13','26-Nov-10', '25-Nov-11', '23-Nov-12', '29-Nov-13','31-Dec-10', '30-Dec-11', '28-Dec-12', '27-Dec-13']}\n\n# Turning dict to pandas table and cleaning timestamp column\nholiday_names = pd.DataFrame(holiday_names)\nholiday_names['Date'] = pd.to_datetime(holiday_names['Date'])\n\n# Getting the information to main dataframes\ndf_main = pd.merge(df_main,holiday_names,on='Date',how='left')\ndf_main_test = pd.merge(df_main_test,holiday_names,on='Date',how='left')\n\n# Viewing\ndf_main.columns\ndf_main.loc[df_main['Holiday'] != None,'Holiday']","4f390cad":"# Dividing Stores sizes in tree parts to create tree categories using it\nlow_max = df_main.Size.max()\/3\nprint(low_max)\n\nmid_max = df_main.Size.max()\/3*2\nprint(mid_max)\n\nhigh_max = df_main.Size.max()\nprint(high_max)","1af8eb5f":"# Creating the new column using rules above\ndf_main['Store_Size_cat'] = pd.cut(df_main['Size'], bins=[0,df_main.Size.max()\/3,df_main.Size.max()\/3 * 2,df_main.Size.max()], labels=[\"Low\", \"Mid\", \"High\"]).astype(str)\ndf_main_test['Store_Size_cat'] = pd.cut(df_main_test['Size'], bins=[0,df_main_test.Size.max()\/3,df_main_test.Size.max()\/3 * 2,df_main_test.Size.max()], labels=[\"Low\", \"Mid\", \"High\"]).astype(str)\n\n# Viewing\ndf_main['Store_Size_cat']","66025358":"# Filling null values only to test correlation \ndf_main = df_main.fillna(-999)\ndf_main_test = df_main_test.fillna(-999)\n\n# Getting dummies for categorical features\ndf_main = pd.get_dummies(df_main, columns=['Type','Holiday','Store_Size_cat'])\ndf_main_test = pd.get_dummies(df_main_test, columns=['Type','Holiday','Store_Size_cat'])\ndf_main['IsHoliday'] = df_main['IsHoliday'].astype(int)\ndf_main_test['IsHoliday'] = df_main_test['IsHoliday'].astype(int)\n\n# Applying Pearson correlation matrix\ncorrMatrix = df_main.corr() * 100\ncorrMatrix = corrMatrix.round(0)\n\n# Showing like a heatmap\nmask = np.zeros_like(corrMatrix)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(15, 17))\n    ax = sns.heatmap(corrMatrix,mask=mask,annot=True,cmap=\"coolwarm\", square=True, linewidths=.5, vmin=-100, vmax=100)","f4c8f8f2":"df_main.drop(columns=['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'], inplace=True)\ndf_main_test.drop(columns=['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'], inplace=True)\n\n# Showing\ndf_main.head()","d73ac0d7":"# Getting X and y\nX_train = df_main.drop(columns=['Weekly_Sales','Date'])\nX_test  = df_main.drop(columns=['Weekly_Sales','Date'])\ny_train = df_main.Weekly_Sales.to_numpy().reshape(-1,1)\n\n# X_test with date to use on model deploy\nX_test_with_date  = df_main.drop(columns=['Weekly_Sales'])\n\n# Changing columns to integer\nX_train.IsHoliday   = X_train.IsHoliday.astype(int)\nX_test.IsHoliday    = X_train.IsHoliday.astype(int)\nX_train.Week_number = X_train.Week_number.astype(int)\nX_test.Week_number = X_train.Week_number.astype(int)\n\n# Scaling method import\nfrom sklearn.preprocessing import StandardScaler\n\n# Scaling features\nscaler_x = StandardScaler()\nX_train_scaled = scaler_x.fit_transform(X_train)\n\n# Scaling targets\nscaler_y = StandardScaler()\ny_train_scaled = scaler_y.fit_transform(y_train)","5e4fca2c":"holiday_week = df_main.IsHoliday.apply(lambda x: True if x else False)\n\n# Function that calculate the error\ndef wmae(targ,y_pred):\n    sumOfWeights = 0\n    sumofCol_B_X_Col_E = 0\n    \n    for i in range(0, len(y_pred)):\n        weight = 0\n        if holiday_week[i]: \n            weight = 5\n        else:\n            weight = 1\n        \n        Col_B_X_Col_E = abs(targ[i] - y_pred[i])*weight\n        sumOfWeights += weight \n        sumofCol_B_X_Col_E += Col_B_X_Col_E\n    WMAE = sumofCol_B_X_Col_E\/sumOfWeights\n    return WMAE","4752a0ee":"from sklearn.metrics import make_scorer\nWMAE_error = make_scorer(wmae, greater_is_better=False)","df68842b":"# Linear regression library\nfrom sklearn.linear_model import LinearRegression\nli_reg = LinearRegression()\n\n# Parameters to Grid search\ntuned_parameters = [{'normalize':[True,False]}]\n\n# Grid search\nli_gs = GridSearchCV(estimator=li_reg, param_grid=tuned_parameters, scoring=WMAE_error, cv=5, n_jobs=-1)\nli_gs.fit(X_train_scaled,y_train_scaled)\nprint('The best parameters for Linear Regression is: ',li_gs.best_params_)","69da958d":"# XGBoost regressor library\nimport xgboost as xgb\nxgb_reg = xgb.XGBRegressor(n_estimators=150)\n\n# Parameters to Grid search\ntuned_parameters = [{'learning_rate':[0.1],'max_depth':[3,4,5,10]}]\n\n# Grid search\nfrom sklearn.model_selection import GridSearchCV\nxgb_gs = GridSearchCV(estimator=xgb_reg, param_grid=tuned_parameters, scoring=WMAE_error, cv=5, n_jobs=10)\nxgb_gs.fit(X_train,y_train)\nprint('The best parameters for XGBoost Regression is: ',xgb_gs.best_params_)","fd15eb5e":"from sklearn.tree import DecisionTreeRegressor\ndt_reg = DecisionTreeRegressor(random_state=0)\n\n# Parameters to Grid search\ntuned_parameters = [{'max_depth':[2,3,4,5,10]}]\n\n# Grid search\ndt_gs = GridSearchCV(estimator=dt_reg, param_grid=tuned_parameters, scoring=WMAE_error, cv=5, n_jobs=-1)\ndt_gs.fit(X_train,y_train)\nprint('The best parameters for SVRegression is: ',dt_gs.best_params_)","9a2e34bb":"print ('linear Regression: ',       wmae(y_train,li_gs.predict(X_train_scaled)).round(5))\nprint ('XGBoost Regression: ',      wmae(y_train,xgb_gs.predict(X_train)).round(5))\nprint ('Decision Tree Regression: ',wmae(y_train,dt_gs.predict(X_train)).round(5))","9cc66c30":"fig, axs = plt.subplots(2, 2,figsize=(8,8))\n\naxs[0, 0].scatter(xgb_gs.predict(X_train), y_train,)\naxs[0, 0].set_title('XGBoost Regression')\n\naxs[0, 1].scatter(li_gs.predict(X_train_scaled), y_train,)\naxs[0, 1].set_title('Linear Regression')\n\naxs[1, 0].scatter(dt_gs.predict(X_train), y_train,)\naxs[1, 0].set_title('Decision Tree Regression')\n\n\nfor ax in axs.flat:\n    ax.set(xlabel='Predicted', ylabel='Real')\n    \n\n# Hide x labels and tick labels for top plots and y ticks for right plots.\nfor ax in axs.flat:\n    ax.label_outer()","83ed26b0":"y_test = xgb_gs.predict(X_test)","f6a062de":"# Menaging final_df to have competition rules standard\nfinal_df = X_test_with_date[['Store','Dept','Date']]\nfinal_df['Id'] = final_df['Store'].astype(str) + '-' + final_df['Dept'].astype(str) + '-' + final_df['Date'].astype(str)\nfinal_df.drop(columns=['Store','Dept','Date'],inplace=True)\nfinal_df['Weekly_Sales'] = y_test\n\nfinal_df.head(10)","2fcc9463":"The GridSearchCV is a really good method to fit and choose the best parameters to our model, but at the kaggle notebook we have small processing resources and when we use own scoring error method it takes more processing than normal, so we'll test a small number of tunning parameters, at the real life we can run the fit() for a several hours (at the night) or use more precessing power to do this faster.\n\nObs* In a kaggle notebook we can run a fit() until 9 hours long.","064273df":"Looking at the plots of predicted vs actual prices, you can also see that the data points in XGBoost are closer to each other and farther apart in Decision tree and Linear Regression.\n\nObs* Linear regression is a horrible choosing to thi case, so it make visible the necessity of testing more than one machine learning algorithyms at all models.","b7499d0a":"## 2.3. Tables overview","d4ff2ef2":"### 3.1.2. Outliers\n\nHere we can see the behavior of each column.\nThe boxplot presented us with the following conclusions:\n    * The temperatures column have one value out of frequency bounds, but we'll don't drop it because 0\u00baF or 0\u00baC is normal in some countries.\n    \n    * The columns with no documentation have some values out of frequency bounds, but we'll don't drop it now, because until have the correlation test to do with these columns.\n    \n    * The unemployment column have some values out of frequency bounds, but we'll don't drop it because based in the global rating we know that the range of ~3-~15 is normal.\nobs* To unknown exactly which information the  columns represents make the thing harder.","28246924":"### 3.2.1. Dates","756ccc7f":"### 3.1.1. Missing values\n\nHere we can see that the columns wihtout documentation have many null values, so we have tree choices:\n    * To fill with some techniques like the median or average of the column\n    \n    * To drop now\n    \n    * To do more tests to know if it will really improve the model\n\nUnlike real life projects, we don't know which information these columns represents, so we cannot fill it.\n\nWe know that missing values is a problem to machine learning models, so we'll do another tests before drop these columns","213517ef":"## 4.2. Testing regression algorithms","404494ba":"To create new categorical features using numerical ones (binning) is a tecnic used to minimize the skew of features, making the model to generalize better.","4a27066c":"The holiday names and your dates can be an important feature to this model, it's not in the gave databases, so let's construct.\n\nObs* The names and dates of Holidays is in the competition rules.","7f70cd20":"# 1. Business Understanding\n\nWalmart is one of the largest retailers in the world, so it have to base your choses in data to have sucsess with your sells or gain more profitability.\n\nIf you know how much the stores will sales at the next months or weeks, you can better manage your stock, employees, bills and much more things.\n\nThe objective of this notebook is predict next weeks sales of walmart stores and departments, trying to minimize the WMAE error.","e7084270":"**Decision tree regression**","11f09a42":"# 3. Data preparation","0f0274b8":"Thanks a lot!","3803e951":"### 3.2.3. New store size column","b5a48aa0":"As we can see, the best model for this case is XGBoost\n\nLet's take a look using a graph:","10f121b1":"### 3.2.2. Holidays","ead4adec":"## 3.3. Testing Correlation","ead9f050":"# Introduction\n\nThe act of predict sales or demand is very normal at the business world, so regression techniques of machine learning is so significant to the people and enterprises.\n\nAt this notebook i'll use the CRISP-DM metodology of data mining.","b5b3c5d1":"Now we'll test 3 types of regressors:\n    * Linear regression\n    * XGBoost regressoion\n    * Decision tree regression","7f280b67":"## 2.2. Data loading","6987de2e":"New Store_Size_cat column rules:\n    \n             Size            Category\n            0 - 73207.3    ->  Low       \n      73207.3 - 146414.6   ->  Mid       \n     146414.6 - 219622     ->  High","4c4541a2":"## 4.3 Applying the model","89f24a25":"## 2.1. Libraries","35f696ef":"## 2.4. Merging dataframes","7ed6920b":"## 4.3. Evaluating","913e1df3":"**Linear regression**","c38de913":"## 3.2. Construct data","a23baaba":"Here we'll try to make four more features, check the correlations of them and turn date into a form that model can understand.\n\nThe features is:\n    * The week numbers in the year\n    \n    * The quarter of year the week is\n    \n    * The holiday names\n    \n    * The categorical size of the store","9cf4e3c0":"After we'll test Linear Regression, so we have the scaled form of dataset too.","7eb1a35a":"The string type of date column is little usefull to the model, so let's turn it to integer columns and get the week number and quarter too.","0a0195a6":"The make_scorer() function is used to set my own WMAE error as the main metric to chose the best parameters of the GridSearchCV().","623bce8f":"Correlation test Conclusions:\n    * We finnally can drop 'MarkDown' columns, beacause it have many missing values, an assimetric distribution, and very low correlation to the target (WeeklySales)\n    \n    * The store size, dept and month have some correlation to the target, so was a good idea to create 'quarter' and 'store_size_cat' columns","28f12f44":"## 3.1. Data cleaning","2ebfd0df":"![image.png](attachment:image.png)","fdb25006":"# 2. Data Understanding","d2f65380":"At final, we can predict the test!","369ec2c8":"## 3.4. Dataset scaling","6085d13c":"## 4.1. Evaluation metric\n\n![image.png](attachment:image.png)\n\nIn the cell below have the evaluation metric function based on the competition rules.","0e44619e":"**XGBoost regression**","a6c3a53a":"# 4. Modeling\/Evaluation"}}