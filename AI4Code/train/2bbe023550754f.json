{"cell_type":{"c1b2762e":"code","692c01b8":"code","162c8fcf":"code","1106042e":"code","a5af9050":"code","46e38c65":"code","c087c67c":"code","86d87745":"code","0e422f24":"code","59285e9b":"code","deee85ac":"code","76131290":"code","fc305ed2":"code","594fd7c6":"code","cce1698f":"code","b829f545":"code","3ee9eda3":"code","78f185db":"code","aa1dc960":"code","4c2d72b5":"code","7fe43063":"code","aae832cc":"code","cf112512":"code","66879265":"code","084f37dc":"code","7d536c64":"code","570a46cc":"code","5e977c05":"code","afa7f372":"code","4f6c5f7f":"code","73b9efce":"code","b10d91e8":"code","32cbf940":"code","dbd48d01":"code","16fe6754":"code","b3460bb0":"code","d9276c0a":"code","26174f90":"code","5a4aa9f2":"code","dd23c446":"code","19f328d1":"code","2abbbd9c":"markdown","46da86c4":"markdown","d9adaeec":"markdown","1750879f":"markdown","51bc27b7":"markdown","0ea9277d":"markdown","deb373b7":"markdown","86ea95fc":"markdown","c1d7ada7":"markdown","542d8383":"markdown"},"source":{"c1b2762e":"# PACKAGES\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\nimport random\n\n# plots\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\n# NLP\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n# H2O\nimport h2o\nfrom h2o.estimators import H2OWord2vecEstimator\nfrom h2o.estimators import H2OGradientBoostingEstimator\n\n# UMAP\nimport umap","692c01b8":"# read data\nt1 = time.time()\ndf = pd.read_csv('..\/input\/history-of-philosophy\/phil_nlp.csv')\nt2 = time.time()\nprint('Elapsed time: ', np.round(t2-t1,2))","162c8fcf":"# structure of data\ndf.info()","1106042e":"# preview\ndf","a5af9050":"# add derived features\ndf['n_tokens'] = list(map(len,map(eval,df.tokenized_txt)))","46e38c65":"# categorical features\nfeatures_cat = ['title', 'author', 'school']\n\n# plot distributions\nfor f in features_cat:\n    plt.figure(figsize=(14,5))\n    df[f].value_counts().plot(kind='bar')\n    plt.title(f)\n    plt.grid()\n    plt.show()","c087c67c":"# sentence length\nprint(df.sentence_length.describe())\n\nplt.figure(figsize=(12,5))\ndf.sentence_length.plot(kind='hist', bins=200)\nplt.title('Sentence Length')\nplt.grid()\nplt.show()\n\nplt.figure(figsize=(12,5))\nnp.log10(df.sentence_length).plot(kind='hist', bins=50)\nplt.title('log10(Sentence Length)')\nplt.grid()\nplt.show()","86d87745":"# number of tokens\nprint(df.n_tokens.describe())\n\nplt.figure(figsize=(12,5))\ndf.n_tokens.plot(kind='hist', bins=200)\nplt.title('Number of Tokens')\nplt.grid()\nplt.show()","0e422f24":"schools = df.school.unique().tolist()\nprint(schools)","59285e9b":"# plot sentence length split by school\nplt.figure(figsize=(16,5))\nsns.violinplot(x='school', y='sentence_length', data=df)\nplt.title('Sentence Length - By School')\nplt.grid()","deee85ac":"# plot number of tokens split by school\nplt.figure(figsize=(16,5))\nsns.violinplot(x='school', y='n_tokens', data=df)\nplt.title('Number of Tokens - By School')\nplt.grid()","76131290":"# mean of numerical features\ndf.groupby(by=['school','author']).mean().round(2)","fc305ed2":"# sentence count (wrap in DataFrame to get nicer display)\npd.DataFrame( df.groupby(by=['school','author'])['title'].count() )","594fd7c6":"# drilldown further to title level (wrap in DataFrame to get nicer display)\npd.DataFrame( df.groupby(by=['school','author','title'])['title'].count() )","cce1698f":"stopwords = set(STOPWORDS)","b829f545":"t1 = time.time()\nfor sc in schools:\n    df_temp = df[df.school==sc]\n    \n    print('School = ', sc.upper(), ':')\n    \n    # render wordcloud\n    text = \" \".join(txt for txt in df_temp.sentence_lowered)\n    wordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=500,\n                          width = 600, height = 400,\n                          background_color=\"white\").generate(text)\n    plt.figure(figsize=(12,8))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\nt2 = time.time()\nprint('Elapsed time: ', np.round(t2-t1,2))","3ee9eda3":"# start H2O\nh2o.init()","78f185db":"# utility function for tokenization\ndef tokenize(sentences, stop_word = stopwords): # use stop words from wordcloud package\n    tokenized = sentences.tokenize(\"\\\\W+\")\n    tokenized_lower = tokenized.tolower()\n    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(stop_word)),:]\n    return tokenized_words","aa1dc960":"# upload data to H2O environment\ntext_h2o = h2o.H2OFrame(df[['school','sentence_lowered']])","4c2d72b5":"# tokenize text\nt1 = time.time()\nwords = tokenize(text_h2o['sentence_lowered'])\nt2 = time.time()\nprint('Elapsed time:', np.round(t2-t1,2), 'secs')","7fe43063":"# train Word2Vec model\nrandom.seed(1234)\n\nt1 = time.time()\nw2v_model = H2OWord2vecEstimator(vec_size = 50,\n                                 window_size = 5,\n                                 sent_sample_rate = 0.001,\n                                 init_learning_rate = 0.025,\n                                 epochs = 10)\nw2v_model.train(training_frame=words)\nt2 = time.time()\nprint('Elapsed time:', np.round(t2-t1,2), 'secs')","aae832cc":"# check model\nw2v_model.find_synonyms('knowledge', count = 5)","cf112512":"# create vector representation for each sentence (as average of the word vectors)\ntext_vec = w2v_model.transform(words, aggregate_method = 'AVERAGE')\n# and add target 'school' to vectors\ntext_vec = text_vec.cbind(text_h2o['school'])\ntext_vec.head()","66879265":"# vector features (columns w\/o the label \"school\")\nfeatures = text_vec.columns\nfeatures.remove('school')","084f37dc":"# convert H2O frame to Pandas data frame\ndf_text_vec = text_vec.as_data_frame();\n\n# drop rows with missing values\ndf_text_vec = df_text_vec.dropna(axis=0)","7d536c64":"# let's first make a simple visualization: boxplot for each column\nplt.figure(figsize=(18,6))\ndf_text_vec[features].boxplot()\nplt.show()","570a46cc":"# use subset only (for performance and clarity of plot)\ndf_text_vec = df_text_vec.sample(25000)","5e977c05":"# run UMAP algorithm to get a low dimensional (in our case 2D) representation\ndim_reducer = umap.UMAP(random_state=111)\n\nt1 = time.time()\ntext_vec_umap = dim_reducer.fit_transform(df_text_vec[features])\nt2 = time.time()\nprint('Elapsed time:', np.round(t2-t1,2), 'secs')\n\n# convert result matrix to data frame\ndf_text_vec_umap = pd.DataFrame(text_vec_umap, columns=['x','y'])\n# and add school again\ndf_text_vec_umap['school'] = df_text_vec.school.tolist()","afa7f372":"# show result\ndf_text_vec_umap.head()","4f6c5f7f":"# now plot\nplt.figure(figsize=(12,10))\nsns.scatterplot(data=df_text_vec_umap, x='x', y='y', \n                hue='school', alpha=0.5, s=10) # adjust marker size => avoid overplotting\nplt.legend(loc='upper right')\nplt.grid()\nplt.show()","73b9efce":"# train\/test split\nrandom.seed(1234)\nperc_train = 0.7\ndata_split = text_vec.split_frame(ratios=[perc_train]) # => data_split[0]:training, data_split[1]:validation","b10d91e8":"# export to file - for potential external processing\nh2o.export_file(data_split[0], 'df_train.csv')\nh2o.export_file(data_split[1], 'df_test.csv')","32cbf940":"# define gradient boosting model\nn_CV = 5 # number of cross validations\nfit_1 = H2OGradientBoostingEstimator(ntrees=200,\n                                     max_depth=4,\n                                     col_sample_rate=0.5,\n                                     min_rows=10,\n                                     nfolds=n_CV,\n                                     seed=999)","dbd48d01":"# train model\nt1 = time.time()\nfit_1.train(x = features,\n                y = 'school',\n                training_frame = data_split[0],\n                validation_frame = data_split[1])\nt2 = time.time()\nprint('Elapsed time:', np.round(t2-t1,2), 'secs')","16fe6754":"# show cross validation metrics\nfit_1.cross_validation_metrics_summary()","b3460bb0":"# show scoring history - training vs cross validation\nfor i in range(n_CV):\n    cv_model_temp = fit_1.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_classification_error, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_classification_error, \n                c='darkorange', label='validation')\n    plt.ylim(0,1)\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.legend()\n    plt.grid()\n    plt.show()","d9276c0a":"# variable importance\nfit_1.varimp_plot(20)","26174f90":"# predictions on validation set\npred_valid = fit_1.predict(data_split[1])\npred_valid = pred_valid.as_data_frame() # back to pandas\n\n# show preview\npred_valid.head()","5a4aa9f2":"# actual values on validation set\nactuals = data_split[1]['school'].as_data_frame()\nactuals = actuals.school","dd23c446":"# evaluate confusion matrix\nconf_valid = pd.crosstab(pred_valid.predict, actuals)","19f328d1":"# visualize confusion matrix (validation set)\nplt.figure(figsize=(10,8))\nsns.heatmap(data=conf_valid, annot=True, fmt='g', cmap='Blues')\nplt.show()","2abbbd9c":"<a id='3'><\/a>\n# Evaluate by School and Author","46da86c4":"<a id='7'><\/a>\n# GBM model based on word embeddings","d9adaeec":"<a id='6'><\/a>\n# Visualize Word Embeddings using UMAP","1750879f":"<a id='1'><\/a>\n# Categorical Features","51bc27b7":"<a id='5'><\/a>\n# Word2Vec - Word Embeddings\n#### Using code from: https:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science\/word2vec.html","0ea9277d":"### Evaluate on validation set","deb373b7":"<a id='4'><\/a>\n# Wordclouds by School","86ea95fc":"<a id='2'><\/a>\n# Numerical Features","c1d7ada7":"### By School:","542d8383":"# Trying a statistical approach to philosophy :-)\n\n## Table of Contents\n* [Categorical Features](#1)\n* [Numerical Features](#2)\n* [Evaluate by School and Author](#3)\n* [Wordclouds by School](#4)\n* [Word2Vec - Word Embeddings](#5)\n* [Visualize Word Embeddings using UMAP](#6)\n* [GBM model based on word embeddings](#7)"}}