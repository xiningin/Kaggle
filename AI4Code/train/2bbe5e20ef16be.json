{"cell_type":{"7a9dc55a":"code","ebcbc138":"code","1c0224e5":"code","9eb6c1b0":"code","3d62e982":"code","2393a6ad":"code","612cfd30":"code","e012292f":"code","ffadd40a":"code","7a8a657f":"code","cb959186":"code","e8eea552":"code","4dff1cf6":"code","11f15c92":"code","7ca37e0a":"code","6467626b":"code","27d6c371":"code","bfee5c86":"code","877c3f35":"code","435f7a09":"code","8b056dc3":"code","3662e288":"code","23e9bf11":"code","96e6ec06":"code","88255600":"code","46b83774":"code","ae5e762e":"markdown","b71eb591":"markdown","08660b53":"markdown","afc21746":"markdown","c5fbc337":"markdown","e2aee630":"markdown","9975cc8d":"markdown","843998e0":"markdown","e62b5243":"markdown","39218ab0":"markdown","019d95a7":"markdown","b344a0aa":"markdown","7fffe274":"markdown","787eada3":"markdown","6bf66d70":"markdown"},"source":{"7a9dc55a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy import stats\nfrom IPython.display import display\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ebcbc138":"# Import training data set\nfile_path = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv'\nhome_data = pd.read_csv(file_path, index_col='Id')\n\nhome_data.head()","1c0224e5":"num_features = [col for col in home_data.drop(columns='SalePrice').select_dtypes(exclude='object')]\ncat_features = [col for col in home_data.drop(columns='SalePrice').select_dtypes(include='object')]\n\n# Create target and features object\ny = home_data.SalePrice\n\nX = home_data.drop(columns='SalePrice')\n\n# Split into validation and training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=1)","9eb6c1b0":"#Get names of columns with missing values\ncat_missing_cols = [col for col in X_train[cat_features].columns if X_train[col].isnull().any()]\n\nprint(cat_missing_cols)","3d62e982":"# Simple imputation for categorical features\ncat_imputer = SimpleImputer(strategy='most_frequent')\n\nX_train_cat = X_train[cat_features].copy()\nX_valid_cat = X_valid[cat_features].copy()\n\nX_train_cat[cat_features] = cat_imputer.fit_transform(X_train[cat_features])\nX_valid_cat[cat_features] = cat_imputer.transform(X_valid[cat_features])","2393a6ad":"# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: X_train_cat[col].nunique(), cat_features))\nd = dict(zip(cat_features, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nprint(sorted(d.items(), key=lambda x: x[1]))","612cfd30":"# OneHotEncoding or dummy encoding for categorical variables with number of unique values less than 10\nlow_cardinality = [col for col in cat_features if X_train_cat[col].nunique() <= 10]\n\n# Merge imputed training and test categorical columns to get dummies for all data value in both data sets\nX_cat_imputed = pd.concat([X_train_cat, X_valid_cat])\n\nX_train_cat_LC = pd.get_dummies(X_cat_imputed[low_cardinality]).loc[X_train.index, :]\nX_valid_cat_LC = pd.get_dummies(X_cat_imputed[low_cardinality]).loc[X_valid.index, :]","e012292f":"# Frequency ratio for categorical variables with number of unique values greater than 10\nhigh_cardinality = list(set(cat_features)-set(low_cardinality))\n\nX_train_cat_HC = pd.DataFrame(index=X_train.index)\nX_valid_cat_HC = pd.DataFrame(index=X_valid.index)\n\nfor col in high_cardinality:\n    X_train_cat_HC[col] = X_train_cat.groupby(col)[col].transform('count')\/X_train_cat.shape[0]\n    d = dict(zip(X_train_cat[col], X_train_cat_HC[col]))\n    for i in list(set(X_valid_cat[col].unique()) - set(X_train_cat[col].unique())):\n        d[i] = 0\n    X_valid_cat_HC[col] = X_valid_cat[col].replace(d)","ffadd40a":"X_train_cat_prep = pd.concat([X_train_cat_HC, X_train_cat_LC], axis=1)\nX_valid_cat_prep = pd.concat([X_valid_cat_HC, X_valid_cat_LC], axis=1)","7a8a657f":"#Get names of columns with missing values\nnum_missing_cols = [col for col in X_train[num_features].columns if X_train[col].isnull().any()]\n\nprint(num_missing_cols)","cb959186":"# Simple imputation numerical features\nsimple_imputer = SimpleImputer()\n\nX_train_num_S1 = X_train[num_features].copy()\nX_valid_num_S1 = X_valid[num_features].copy()\n\nX_train_num_S1[num_features] = simple_imputer.fit_transform(X_train[num_features])\nX_valid_num_S1[num_features] = simple_imputer.transform(X_valid[num_features])","e8eea552":"# Merge categorical and numerical features\nX_train_S1 = pd.concat([X_train_cat_prep, X_train_num_S1], axis=1)\nX_valid_S1 = pd.concat([X_valid_cat_prep, X_valid_num_S1], axis=1)","4dff1cf6":"# KNN imputation for numerical features\nKNN_imputer = KNNImputer()\n\nX_train_num_S2 = X_train[num_features].copy()\nX_valid_num_S2 = X_valid[num_features].copy()\n\nX_train_num_S2[num_features] = KNN_imputer.fit_transform(X_train[num_features])\nX_valid_num_S2[num_features] = KNN_imputer.transform(X_valid[num_features])","11f15c92":"# Merge categorical and numerical features\nX_train_S2 = pd.concat([X_train_cat_prep, X_train_num_S2], axis=1)\nX_valid_S2 = pd.concat([X_valid_cat_prep, X_valid_num_S2], axis=1)","7ca37e0a":"# Define the model, and tune params using RandomizedSearchCV\n\nrf_model = RandomForestRegressor(random_state=1)\nn_estimators_grid = {'n_estimators': stats.randint(100, 300)}\n\nmodel_CV_S1_rf = RandomizedSearchCV(rf_model, param_distributions=n_estimators_grid,\n                                    scoring='neg_root_mean_squared_error', n_iter=25, random_state=1, n_jobs=-1)\nmodel_CV_S1_rf.fit(X_train_S1, y_train)\n\n# Best score and parameter with RandomizedSearchCV\nbest_param_S1_rf = model_CV_S1_rf.best_params_\nprint('Best parameter: {}'.format(best_param_S1_rf))\nprint('Best score: {}'.format(model_CV_S1_rf.best_score_))\n\nval_predictions = model_CV_S1_rf.predict(X_valid_S1)\nval_mae = mean_squared_error(y_true=y_valid, y_pred=val_predictions, squared=False)\n\nprint(\"Validation MAE for the best Random Forest model with Strategy 1: {:,.0f}\".format(val_mae))","6467626b":"XGB_model_S1 = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nXGB_model_S1.fit(X_train_S1, y_train, early_stopping_rounds=5,\n              eval_set=[(X_valid_S1, y_valid)], verbose=False)\n\n# Best score and parameter with RandomizedSearchCV\nbest_param_S1_xgb = XGB_model_S1.best_iteration\nprint('Best parameter: {}'.format(best_param_S1_xgb))\nprint('Best score: {}'.format(XGB_model_S1.best_score))\n\nval_predictions = XGB_model_S1.predict(X_valid_S1)\nval_mae = mean_squared_error(y_true=y_valid, y_pred=val_predictions, squared=False)\n\nprint(\"Validation MAE for the best XGBoost model with Strategy 1: {:,.0f}\".format(val_mae))","27d6c371":"# Define the model, and tune params using RandomizedSearchCV\n\nrf_model = RandomForestRegressor(random_state=1)\nn_estimators_grid = {'n_estimators': stats.randint(100, 300)}\n\nmodel_CV_S2 = RandomizedSearchCV(rf_model, param_distributions=n_estimators_grid,\n                                 scoring='neg_root_mean_squared_error', n_iter=25, random_state=1, n_jobs=-1)\nmodel_CV_S2.fit(X_train_S2, y_train)\n\n# Best score and parameter with RandomizedSearchCV\nbest_param_S2 = model_CV_S2.best_params_\nprint('Best parameter: {}'.format(best_param_S2))\nprint('Best score: {}'.format(model_CV_S2.best_score_))\n\nval_predictions = model_CV_S2.predict(X_valid_S2)\nval_mae = mean_squared_error(y_true=y_valid, y_pred=val_predictions, squared=False)\n\nprint(\"Validation MAE for the best Random Forest model with Strategy 2: {:,.0f}\".format(val_mae))","bfee5c86":"XGB_model_S2 = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nXGB_model_S2.fit(X_train_S2, y_train, early_stopping_rounds=5,\n              eval_set=[(X_valid_S2, y_valid)], verbose=False)\n\n# Best score and parameter with RandomizedSearchCV\nbest_param_S2_xgb = XGB_model_S2.best_iteration\nprint('Best parameter: {}'.format(best_param_S2_xgb))\nprint('Best score: {}'.format(XGB_model_S2.best_score))\n\nval_predictions = XGB_model_S2.predict(X_valid_S2)\nval_mae = mean_squared_error(y_true=y_valid, y_pred=val_predictions, squared=False)\n\nprint(\"Validation MAE for the best XGBoost model with Strategy 2: {:,.0f}\".format(val_mae))","877c3f35":"# Best model\nbest_model = XGB_model_S2","435f7a09":"# Import test data\ntest_data_path = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv'\ntest_data = pd.read_csv(test_data_path, index_col='Id')","8b056dc3":"# Simple imputation for categorical features\ncat_imputer = SimpleImputer(strategy='most_frequent')\n\nX_cat = X[cat_features].copy()\nX_test_cat = test_data[cat_features].copy()\n\nX_cat[cat_features] = cat_imputer.fit_transform(X[cat_features])\nX_test_cat[cat_features] = cat_imputer.transform(test_data[cat_features])\n\n# OneHotEncoding or dummy encoding for categorical variables with number of unique values less than 10\n# Merge imputed training and test categorical columns to get dummies for all data value in both data sets\nX_cat_imputed = pd.concat([X_cat, X_test_cat])\n\nX_cat_LC = pd.get_dummies(X_cat_imputed[low_cardinality]).loc[X.index, :]\nX_test_cat_LC = pd.get_dummies(X_cat_imputed[low_cardinality]).loc[test_data.index, :]\n\n# Frequency ratio for categorical variables with number of unique values greater than 10\nX_cat_HC = pd.DataFrame(index=X.index)\nX_test_cat_HC = pd.DataFrame(index=test_data.index)\n\nfor col in high_cardinality:\n    X_cat_HC[col] = X_cat.groupby(col)[col].transform('count')\/X_cat.shape[0]\n    d = dict(zip(X_cat[col], X_cat_HC[col]))\n    for i in list(set(X_test_cat[col].unique()) - set(X_cat[col].unique())):\n        d[i] = 0\n    X_test_cat_HC[col] = X_test_cat[col].replace(d)\n    \nX_cat = pd.concat([X_cat_HC, X_cat_LC], axis=1)\nX_test_cat = pd.concat([X_test_cat_HC, X_test_cat_LC], axis=1)","3662e288":"\"\"\"\n# Strategy 1\n# Simple imputation numerical features\nsimple_imputer = SimpleImputer()\n\nX_num = X[num_features].copy()\nX_test_num = test_data[num_features].copy()\n\nX_num[num_features] = simple_imputer.fit_transform(X[num_features])\nX_test_num[num_features] = simple_imputer.transform(test_data[num_features])\n\n# Merge categorical and numerical features\nX = pd.concat([X_cat, X_num], axis=1)\nX_test = pd.concat([X_test_cat, X_test_num], axis=1)\n\"\"\"","23e9bf11":"# Strategy 2\n# KNN imputation numerical features\nKNN_imputer = KNNImputer()\n\nX_num = X[num_features].copy()\nX_test_num = test_data[num_features].copy()\n\nX_num[num_features] = KNN_imputer.fit_transform(X[num_features])\nX_test_num[num_features] = KNN_imputer.transform(test_data[num_features])\n\n# Merge categorical and numerical features\nX = pd.concat([X_cat, X_num], axis=1)\nX_test = pd.concat([X_test_cat, X_test_num], axis=1)","96e6ec06":"\"\"\"\n# Train best model on full training data\nrf_model = RandomForestRegressor(n_estimators=best_param_random['n_estimators'], random_state=1)\nrf_model.fit(X, y)\n\n# make predictions which we will submit\ntest_preds = rf_model.predict(X_test)\n\"\"\"","88255600":"# Train best model\nXGB_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nXGB_model.fit(X_train_S2, y_train, early_stopping_rounds=10,\n              eval_set=[(X_valid_S2, y_valid)], verbose=False)\n\n# make predictions which we will submit\ntest_preds = XGB_model.predict(X_test)","46b83774":"# Save predictions in format used for competition scoring\n\noutput = pd.DataFrame({'Id': test_data.index,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)","ae5e762e":"XGBoost Regressor","b71eb591":"**Model with strategy 1**","08660b53":"Low cordinality categorical features","afc21746":"Random Forest Regressor","c5fbc337":"**Handling missing values**","e2aee630":"Random Forest Regressor","9975cc8d":"# **Transform categorical variables**","843998e0":"**Missing vaues: Strategy 2**","e62b5243":"XGBoost Regressor","39218ab0":"**Missing vaues: Strategy 1**","019d95a7":"# **Model selection**","b344a0aa":"# **Predictions**","7fffe274":"# **Transform numerical varibles**","787eada3":"**Model with strategy 2**","6bf66d70":"High cordinality categorical features"}}