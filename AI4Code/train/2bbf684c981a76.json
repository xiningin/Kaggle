{"cell_type":{"3eb03734":"code","4e49d63d":"code","21d13fd5":"code","b552f1d8":"code","7bc693e5":"code","7c0e9f6d":"code","e766ff80":"code","cb8ddbf4":"code","f296cce1":"code","4d941cbe":"code","a9775c10":"code","a0736e84":"code","e8bd0712":"code","a06e6639":"code","7b2b4c58":"code","a8a25278":"code","7f81c15b":"code","d54111fd":"code","6dfb131e":"code","9fe1d04e":"code","51050854":"code","0a8e6bba":"code","f0fca68a":"code","7796531f":"code","11cfc80f":"code","7fc8f1a0":"code","a8c3841c":"code","2dd0f1d5":"code","c87350bf":"code","c700188c":"code","cc96e54b":"code","ff18b82d":"code","59e4cde3":"code","04e74e1e":"code","fcd6479e":"code","60970320":"code","fd5651f0":"markdown","87dbb07a":"markdown","d329bb73":"markdown","f6798b7b":"markdown","01724661":"markdown","65e26138":"markdown","6f368138":"markdown","8c77d729":"markdown","077f0f78":"markdown","b98c123a":"markdown","d247f64c":"markdown","c76caef6":"markdown","8d2ca64c":"markdown","511ad57c":"markdown","8c103556":"markdown","b1b6da15":"markdown"},"source":{"3eb03734":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport csv\nimport hashlib\nimport re\nimport geopy\n\nfrom math import sqrt\nfrom io import StringIO\nfrom unicodedata import normalize\nfrom time import sleep\nfrom geopy import distance\n\nfrom sklearn.metrics import mean_squared_error, make_scorer\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder","4e49d63d":"# Estimadores\nfrom lightgbm import LGBMRegressor\nfrom xgboost.sklearn import XGBRegressor","21d13fd5":"# Desliga warnings desnecess\u00e1rios\npd.set_option('mode.chained_assignment',None)","b552f1d8":"# Remove os atributos especificados na lista [attrs]\ndef drop_attr(df_pre, attrs):\n    df_pre = df_pre.copy()\n    for a in attrs:\n        if a in df_pre.columns:\n            df_pre.drop([a],axis=1,inplace=True)\n    return df_pre    ","7bc693e5":"# Expande coluna \"amenities\"\ndef expand_amenities(df_pre,df_test):\n    df_pre = df_pre.copy()\n    df_test = df_test.copy()\n    # cria dataframes para as novas colunas\n    df_amenities = pd.DataFrame(index=df_pre.index)\n    df_test_amenities = pd.DataFrame(index=df_test.index)\n    # varre dataframe de treino\n    for i in df_pre.index:\n        # l\u00ea cada linha da coluna \"amenities\" como uma string csv\n        reader = csv.reader(StringIO((df_pre.amenities[i])[1:-1]), delimiter=',')\n        for row in reader:\n            for item in row:\n                column = re.sub('[^A-Za-z0-9]+', '_', item)\n                # inclui uma nova coluna no dataframe \"amenities\", valor default para rows=0\n                if column not in df_amenities.columns:\n                    df_amenities[column] = 0\n                if column not in df_test_amenities.columns:\n                    df_test_amenities[column] = 0\n                # coloca 1 na row porque a 'amenity' est\u00e1 na linha lida\n                df_amenities[column][i] = 1        \n    # varre dataframe de teste\n    for i in df_test.index:\n        # l\u00ea cada linha da coluna \"amenities\" como uma string csv\n        reader = csv.reader(StringIO((df_test.amenities[i])[1:-1]), delimiter=',')\n        for row in reader:\n            for item in row:\n                column = re.sub('[^A-Za-z0-9]+', '_', item)\n                # inclui uma nova coluna no dataframe \"amenities\", valor default para rows=0\n                if column not in df_amenities.columns:\n                    df_amenities[column] = 0\n                if column not in df_test_amenities.columns:\n                    df_test_amenities[column] = 0\n                # coloca 1 na row porque a 'amenity' est\u00e1 na linha lida\n                df_test_amenities[column][i] = 1        \n\n    # elimina coluna \"amenities\" original\n    df_pre.drop(['amenities'],axis=1,inplace=True)            \n    df_test.drop(['amenities'],axis=1,inplace=True)            \n    # concatena novas colunas \"amenities\" expandidas\n    df_pre = pd.concat([df_pre,df_amenities],axis=1)\n    df_test = pd.concat([df_test,df_test_amenities],axis=1)\n    return df_pre, df_test","7c0e9f6d":"# Conserta campo zipcode\ndef zipcode_process(df_pre):\n    df_pre = df_pre.copy()\n    for i in df_pre.index:\n        s = str(df_pre['zipcode'][i]).strip()\n        zipcode = re.search('[\\d]{4}[-\\s]{1}[\\d]{4}',s)\n        if zipcode != None:\n            r = str(zipcode.group())\n            df_pre['zipcode'][i] = r[0:4]+r[5]+'-'+r[6:9]\n        else:\n            zipcode = re.search('[\\d]{5}[-\\s]{1}[\\d]{3}',s)\n            if zipcode != None:\n                r = str(zipcode.group())\n                df_pre['zipcode'][i] = r\n            else:\n                zipcode = re.search('[\\d]{8}',s)\n                if zipcode != None:\n                    r = str(zipcode.group())\n                    df_pre['zipcode'][i] = r[0:5]+'-'+r[5:8]\n                else:\n                    zipcode = re.search('[\\d]{5}',s)\n                    if zipcode != None:\n                        r = str(zipcode.group())\n                        df_pre['zipcode'][i] = r+'-000'\n        s = str(df_pre['zipcode'][i]).strip()\n        if len(s) != 9 or s == '00000-000':    \n            df_pre['zipcode'][i] = 'ND'\n    return df_pre\n\n# Procura bairro pelo cep na lista de cep\/bairro    \ndef search_bairro_from_cep(cep,df_zip):\n    try:\n        return df_zip.loc[df_zip['zipcode'] == cep].loc[df_zip['neighbourhood'] != 'ND']['neighbourhood'].values[0]\n    except:\n        return None    \n\n# Procura cep pelo bairro na lista de cep\/bairro    \ndef search_cep_from_bairro(bairro,df_zip):\n    try:\n        return df_zip.loc[df_zip['neighbourhood'] == bairro].loc[df_zip['zipcode'] != 'ND']['zipcode'].values[0]\n    except:\n        return None\n\n# Retorna cep,bairro por lat\/lng\ndef address_by_geo(lat,lng):\n    global geolocator\n    geolocator = geopy.Nominatim(user_agent='my-application')\n    sleep(0.1)\n    try:\n        location = geolocator.reverse((lat,lng))\n        cep = location.raw['address']['postcode']\n        if len(cep) == 8:\n            cep = cep[0:5]+'-'+cep[5:8]\n        elif len(cep) == 5:\n            cep = cep+'-000'\n        bairro = location.raw['address']['suburb'].lower().strip()\n        p = re.search('\\([^\\)]+\\)',bairro)\n        if p != None:\n            bairro = str(p.group())[1:-1].strip()\n    except:\n        cep = None\n        bairro = None\n    return cep, bairro\n\n# Busca ceps e bairros faltantes por lat\/lng\ndef fill_nd(df_zip):\n    geolocator = geopy.Nominatim(user_agent='my-application')\n    k = 0\n    for i in df_zip.index:\n        k = k + 1\n        zipcode = df_zip['zipcode'][i]\n        neighbourhood = df_zip['neighbourhood'][i]\n        if zipcode == 'ND' or neighbourhood == 'ND':\n            bairro = search_bairro_from_cep(zipcode,df_zip)\n            cep = search_cep_from_bairro(neighbourhood,df_zip)\n            if (zipcode == 'ND' and cep == None) or (neighbourhood == 'ND' and bairro == None):\n                lat = df_zip['latitude'][i]\n                lng = df_zip['longitude'][i]\n                cep, bairro = address_by_geo(lat,lng)\n            if zipcode == 'ND' and cep != None and len(cep) == 9:\n                print(cep,bairro,'- get zipcode:',k,'of',len(df_zip.index))\n                df_zip['zipcode'][i] = cep\n            if neighbourhood == 'ND' and bairro != None and len(bairro) != 0:\n                print(cep,bairro,'- get neighbourhood:',k,'of',len(df_zip.index))\n                df_zip['neighbourhood'][i] = bairro\n    return df_zip\n\n# Faz uma lista de bairros\/cep\ndef create_zipcode_list(df_pre,df_test,df_zip):\n    df_zip = pd.concat([df_pre,df_test],axis=0,ignore_index=True)\n    df_zip = pd.DataFrame(df_zip,\n                          columns=['neighbourhood','zipcode',\n                                   'latitude','longitude'])\n    # busca ceps e bairros faltantes\n    df_zip = fill_nd(df_zip)\n    # elimina repeticoes de zipcode\n    df_zip = df_zip.drop_duplicates(subset='zipcode', keep='first')\n    return df_zip\n\n# Transforma bairros 'ND' a partir do cep\ndef zip2neighbourhood(df_pre,df_zip):\n    df_pre = df_pre.copy()\n    for i in df_pre.index:\n        if df_pre['neighbourhood'][i] == 'ND' and df_pre['zipcode'][i] != 'ND':\n            n = df_zip.loc[df_zip['zipcode'].str.contains(df_pre['zipcode'][i][0:5])]['neighbourhood'].values\n            if len(n) != 0:\n                df_pre['neighbourhood'][i] = n[0]\n    return df_pre\n\n# Transforma cep 'ND' a partir do bairro\ndef neighbourhood2zip(df_pre,df_zip):\n    df_pre = df_pre.copy()\n    for i in df_pre.index:\n        if df_pre['zipcode'][i] == 'ND' and df_pre['neighbourhood'][i] != 'ND':\n            n = df_zip.loc[df_zip['neighbourhood'] == df_pre['neighbourhood'][i]]['zipcode'].values\n            if len(n) != 0:\n                df_pre['zipcode'][i] = n[0]\n    return df_pre\n\n# Transforma cep 'ND' e bairro 'ND' a partir de lat\/lng\ndef neighbourhood_zip_from_geo(df_pre):\n    df_pre = df_pre.copy()\n    for i in df_pre.index:\n        if df_pre['zipcode'][i] == 'ND' and df_pre['neighbourhood'][i] == 'ND':\n            lat = df_pre['latitude'][i]\n            lng = df_pre['longitude'][i]\n            cep, bairro = address_by_geo(lat,lng)\n            if cep != None and len(cep) == 9:\n                df_pre['zipcode'][i] = cep\n            if bairro != None and len(bairro) != 0:\n                df_pre['neighbourhood'][i] = bairro\n    return df_pre\n\n\n# Processa colunas 'zipcode' e 'neighbourhood'\n# Preenche dados faltantes, se possivel\ndef process_zipcode_neighbourhood(df_pre,df_test,df_zip):\n    df_pre = df_pre.copy()\n    df_test = df_test.copy()\n    df_zip = df_zip.copy()\n    # conserta cep\n    df_pre = zipcode_process(df_pre)\n    df_test = zipcode_process(df_test)\n    # faz uma lista de bairros\/cep\n    df_zip = create_zipcode_list(df_pre,df_test,df_zip)\n    # transforma bairros 'ND' a partir do cep\n    df_pre = zip2neighbourhood(df_pre,df_zip)\n    df_test = zip2neighbourhood(df_test,df_zip)\n    # transforma cep 'ND' a partir do bairro\n    df_pre = neighbourhood2zip(df_pre,df_zip)\n    df_test = neighbourhood2zip(df_test,df_zip)\n    # Transforma cep 'ND' e bairro 'ND' a partir de lat\/lng\n    df_pre = neighbourhood_zip_from_geo(df_pre)\n    df_test = neighbourhood_zip_from_geo(df_test)\n    return df_pre, df_test, df_zip","e766ff80":"# Calcula dist\u00e2ncia para o centro do RJ\ndef distance_to_center(df_pre, lat_field='latitude', lon_field='longitude'):\n    # Centro do RJ -22.9005659,-43.1868733\n    return distance.distance((-22.9005659,-43.1868733),(df_pre[lat_field],df_pre[lon_field])).km\n\n# Substitui colunas 'latitude' e 'longitude' por 'distance_center'\n# 'distance_center': Dist\u00e2ncia em km do centro do Rio de Janeiro\ndef process_latitude_longitude(df_pre,df_test):\n    df_pre = df_pre.copy()\n    df_test = df_test.copy()\n    dist = df_pre.apply(distance_to_center, axis=1)\n    df_pre['distance_center'] = dist\n    dist = df_test.apply(distance_to_center, axis=1)\n    df_test['distance_center'] = dist\n    # elimina colunas 'latitude' e 'longitude'\n    df_pre.drop(['latitude','longitude'],axis=1,inplace=True)            \n    df_test.drop(['latitude','longitude'],axis=1,inplace=True)\n    return df_pre, df_test","cb8ddbf4":"# Remove acentos\ndef str_normalize(s):\n    return normalize('NFKD',str(s)).encode('ASCII','ignore').decode('ASCII')\n\n# calcula hash (int64) de string\ndef md5_hash(x):\n    encoder = hashlib.md5(str(x).encode('utf-8'))\n    return int(encoder.hexdigest()[16:],16) # apenas 64 bits (16 caracteres hex)\n    \n# Converte os atributos categ\u00f3ricos em quantitativos discretos\n# Usa metade de hash md5 como retorno\ndef cat_to_discrete_by_hash(df_pre,df_test):\n    df_pre = df_pre.copy()\n    df_test = df_test.copy()\n    for column in df_pre.columns:\n        if str(df_pre[column].dtype) not in ['float64', 'int64']:\n            df_pre[column] = df_pre[column].apply(lambda x: md5_hash(str_normalize(x)))\n    for column in df_test.columns:\n        if str(df_test[column].dtype) not in ['float64', 'int64']:\n            df_test[column] = df_test[column].apply(lambda x: md5_hash(str_normalize(x)))\n    return df_pre, df_test\n\n# Converte os atributos categ\u00f3ricos em quantitativos discretos\n# Usa LabelEncode como conversor\ndef cat_to_discrete_by_labelencode(df_pre,df_test):\n    df_pre = df_pre.copy()\n    df_test = df_test.copy()\n    encoder = LabelEncoder()\n    df_all = pd.concat([df_pre,df_test],axis=0,ignore_index=True)\n    for column in df_all.columns:\n        if str(df_all[column].dtype) not in ['float64', 'int64']:\n            df_all[column] = df_all[column].apply(lambda x: str_normalize(x))\n            encoder.fit(list(df_all[column].values))\n            if column in df_pre.columns:\n                df_pre[column] = df_pre[column].apply(lambda x: str_normalize(x))\n                df_pre[column] = encoder.transform(list(df_pre[column].values))\n            if column in df_test.columns:\n                df_test[column] = df_test[column].apply(lambda x: str_normalize(x))\n                df_test[column] = encoder.transform(list(df_test[column].values))\n    return df_pre, df_test\n\n# Converte os atributos categ\u00f3ricos em quantitativos discretos\n# Usa get_dummies como conversor\n# Expande categorias como colunas\ndef cat_to_discrete_by_dummies(df_pre,df_test):\n    df_pre = df_pre.copy()\n    df_test = df_test.copy()\n    df_dummies = pd.DataFrame(index=df_pre.index)\n    df_dummies_test = pd.DataFrame(index=df_test.index)\n    # varre dataframe de treino\n    for column in df_pre.columns:\n        if str(df_pre[column].dtype) not in ['float64', 'int64']:\n            df_pre[column] = df_pre[column].apply(lambda x: str_normalize(x))\n            dummies = pd.get_dummies(df_pre[column], columns=[column], prefix=column)\n            for c in dummies.columns:\n                if c not in df_dummies_test.columns:\n                    df_dummies_test[c] = 0\n                if c not in df_dummies.columns:\n                    df_dummies[c] = 0\n                df_dummies[c] = dummies[c]\n            # remove coluna original\n            df_pre.drop([column],axis=1,inplace=True)\n    # varre dataframe de teste\n    for column in df_test.columns:\n        if str(df_test[column].dtype) not in ['float64', 'int64']:\n            df_test[column] = df_test[column].apply(lambda x: str_normalize(x))\n            dummies = pd.get_dummies(df_test[column], columns=[column], prefix=column)\n            for c in dummies.columns:\n                if c not in df_dummies.columns:\n                    df_dummies[c] = 0\n                if c not in df_dummies_test.columns:\n                    df_dummies_test[c] = 0\n                df_dummies_test[c] = dummies[c]\n            # remove coluna original\n            df_test.drop([column],axis=1,inplace=True)\n    # concatena novas colunas de categorias expandidas\n    df_pre = pd.concat([df_pre,df_dummies],axis=1)\n    df_test = pd.concat([df_test,df_dummies_test],axis=1)\n    return df_pre, df_test","f296cce1":"# Normaliza os dados discretos\ndef data_normalize(df_pre,df_test):\n    df_pre = df_pre.copy()\n    df_test = df_test.copy()\n    #scaler = MinMaxScaler()\n    #scaler = RobustScaler()\n    scaler = StandardScaler()\n    # dataframe treino+teste \n    df_a = pd.concat([df_pre,df_test],axis=0)\n    df_a.drop(['id','price'],axis=1,inplace=True) #n\u00e3o converte id e price\n    # faz fit\n    scaler.fit(df_a.values)\n    # faz transform no dataframe de treino\n    df_X = df_pre.drop(['id','price'],axis=1) #n\u00e3o converte id e price\n    df_id = df_pre['id']\n    df_price = df_pre['price']\n    scaled_X = scaler.transform(df_X.values)\n    df_pre = pd.DataFrame(scaled_X,columns=df_X.columns,index=df_X.index)\n    df_pre['price'] = df_price\n    df_pre['id'] = df_id\n    # faz transform no dataframe de teste\n    df_X = df_test.drop(['id'],axis=1) #n\u00e3o converte id\n    df_id = df_test['id']\n    scaled_X = scaler.transform(df_X.values)\n    df_test = pd.DataFrame(scaled_X,columns=df_X.columns,index=df_X.index)\n    df_test['id'] = df_id\n    return df_pre,df_test","4d941cbe":"# Seleciona atributos com melhor correla\u00e7\u00e3o\n# 1.0 \u00e9 a melhor correla\u00e7\u00e3o\ndef reduce_dimension(est,df_pre,df_test,threshold=0.01):\n    df_pre = df_pre.copy()\n    df_test = df_test.copy()\n    X = df_pre.drop(['id','price'],axis=1) # elimina id e price de X\n    y = df_pre['price'] # y \u00e9 o price\n    est.fit(X,y)\n    # dataframe com os atributos classificados por import\u00e2ncia\n    df_importances = pd.DataFrame({'col':X.columns,\n                  'importance':est.feature_importances_}).sort_values('importance', ascending=False)\n    # varre dataframe de atributos\n    for i in df_importances.index:\n        # elimina dos dataframes os atributos mais irrelevantes\n        if (df_importances.importance[i] < threshold):\n            if df_importances.col[i] in df_pre.columns: # dataframe de treino\n                df_pre.drop(df_importances.col[i],axis=1,inplace=True)\n            if df_importances.col[i] in df_test.columns: # dataframe de teste\n                df_test.drop(df_importances.col[i],axis=1,inplace=True)\n    \n    return df_pre,df_test,df_importances","a9775c10":"# Treina um estimador, otimiza hiperpar\u00e2metros,\n# avalia performance (CV) e retorna m\u00e9tricas de desempenho\ndef build(X,y,est,grid):\n    est_name = est.__class__.__name__\n    print('************************')\n    print('Testando o estimador',est_name,'...')\n    print('************************')\n    # Otimiza modelos\n    # Uso RMSE como m\u00e9trica\n    rmse_scorer = make_scorer(mean_squared_error, squared=False)\n    regr = GridSearchCV(est,grid,scoring=rmse_scorer,n_jobs=-1,cv=5,verbose=100) # 5 folds\n    regr.fit(X,y)\n    # Obt\u00e9m as m\u00e9tricas de desempenho - o quanto nosso estimador acertou?\n    return regr.best_estimator_, est_name, regr.best_score_, regr.best_params_","a0736e84":"# Semente aleat\u00f3ria a ser usada ao longo desse notebook\nrandom_state=2020\n\n# Nome do arquivo fornecido pelo desafio com os dados rotulados para treino\nnome_arquivo_com_rotulos_para_treino = '..\/input\/desafioiamp2020\/' + 'treino.csv'\n\n# Nome do arquivo fornecido pelo desafio com os dados n\u00e3o rotulados, que dever\u00e3o ser analisados pelo modelo constru\u00eddo aqui\nnome_arquivo_sem_rotulos = '..\/input\/desafioiamp2020\/' + 'teste.csv'\n\n# Nome do arquivo que ser\u00e1 criado com os r\u00f3tulos gerados pelo modelo\n# Esse \u00e9 o arquivo se ser\u00e1 submetido \u00e0 p\u00e1gina do desafio\nnome_arquivo_rotulado_regressor = '..\/working\/' + 'submissao-equipe.csv'","e8bd0712":"# Carrega os dados da base rotulada\ndf = pd.read_csv(nome_arquivo_com_rotulos_para_treino, index_col=None, engine='python', sep =';', encoding=\"utf-8\")\nprint('Total de registros carregados:',len(df))\n# Exibe uma amostra dos dados\ndf.head()","a06e6639":"# Carrega os dados da base n\u00e3o rotulada\ndf_test = pd.read_csv(nome_arquivo_sem_rotulos, index_col=None, engine='python', sep =';', encoding=\"utf-8\")\nprint('Total de registros carregados:',len(df_test))\n# Exibe uma amostra dos dados\ndf_test.head()","7b2b4c58":"# Processa colunas 'zipcode' e 'neighbourhood'\ndf_zip = pd.DataFrame()\ndf,df_test,df_zip = process_zipcode_neighbourhood(df,df_test,df_zip)\n# Exibe uma amostra dos dados\ndf.head()","a8a25278":"# Exibe uma amostra dos dados\ndf_test.head()","7f81c15b":"# Processa colunas 'latitude' e 'longitude'\ndf,df_test = process_latitude_longitude(df,df_test)\n# Exibe uma amostra dos dados\ndf.head()","d54111fd":"# Exibe uma amostra dos dados\ndf_test.head()","6dfb131e":"# expande coluna \"amenities\"\ndf, df_test = expand_amenities(df, df_test)\n# Exibe uma amostra dos dados\ndf.head()","9fe1d04e":"# Exibe uma amostra dos dados\ndf_test.head()","51050854":"# remove atributo \"host_name\"\ndf = drop_attr(df,['host_name'])\n# Exibe uma amostra dos dados\ndf.head()","0a8e6bba":"# remove atributo \"host_name\"\ndf_test = drop_attr(df_test,['host_name'])\n# Exibe uma amostra dos dados\ndf_test.head()","f0fca68a":"# converte os atributos categ\u00f3ricos em quantitativos discretos\ndf, df_test = cat_to_discrete_by_labelencode(df, df_test)\n# Exibe uma amostra dos dados\ndf.head()","7796531f":"# Exibe uma amostra dos dados\ndf_test.head()","11cfc80f":"# escala\/normaliza os atributos\ndf, df_test = data_normalize(df, df_test)\n# Exibe uma amostra dos dados\ndf.head()","7fc8f1a0":"# Exibe uma amostra dos dados\ndf_test.head()","a8c3841c":"# Algoritmos de predicao\nestimators = [\n  {'est': XGBRegressor(), \n   'grid':{ \n      'random_state': [random_state],\n      'nthread':[4],\n      'objective':['reg:squarederror'],\n      'learning_rate': [.03],\n      #'learning_rate': [.03, 0.05, .07],\n      'max_depth': [8],\n      #'max_depth': [5, 6, 7],\n      'min_child_weight': [4],\n      'subsample': [.7],\n      'colsample_bytree': [.7],\n      'n_estimators': [800],\n   },\n   'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': LGBMRegressor(), \n  # 'grid':{ \n  #    'random_state': [random_state],\n  #    'learning_rate': [.03,.05],\n  #    'num_leaves': [31,64],\n  #    'objective': ['binary'],\n  #    'n_estimators': [20,100,800],\n  # },\n  # 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': GradientBoostingRegressor(), \n  # 'grid':{ \n  #    'random_state': [random_state],\n  #    'loss': ['ls'],\n  #    #'loss': ['ls', 'lad', 'huber', 'quantile'],\n  #    'n_estimators': [120],\n  #    #'n_estimators': range(20,81,10),\n  #    'max_depth': [11], \n  #    #'max_depth':range(5,16,2), \n  #    'min_samples_split': [200], \n  #    #'min_samples_split':range(200,1001,200),\n  #    'min_samples_leaf': [90],\n  #    #'min_samples_leaf':range(30,71,10),\n  # },\n  # 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': HistGradientBoostingRegressor(), \n  # 'grid':{ \n  #    'random_state': [random_state],\n  #    'max_depth': [15], \n  #    #'max_depth':range(5,16,2), \n  #    'min_samples_leaf': [60],\n  #    #'min_samples_leaf':range(30,71,10),\n  # },\n  # 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': MLPRegressor(), \n  # 'grid':{ \n  #    'random_state': [random_state],\n  #    'hidden_layer_sizes': [(50,100,50)],\n  #    #'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,1)],\n  #    'activation': ['relu'],\n  #    #'activation': ['relu','tanh','logistic'],\n  #    'alpha': [0.0001],\n  #    #'alpha': [0.0001, 0.05],\n  #    'learning_rate': ['constant'],\n  #    #'learning_rate': ['constant','adaptive'],\n  #    'solver': ['adam'],\n  #    'max_iter': [100],\n  # },\n  # 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': LinearRegression(), 'grid':{}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': RandomForestRegressor(), 'grid':{'random_state': [random_state],}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': BaggingRegressor(), 'grid':{ 'random_state': [random_state],}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': AdaBoostRegressor(), 'grid':{ 'random_state': [random_state],}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': DecisionTreeRegressor(), 'grid':{ 'random_state': [random_state],}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': ARDRegression(), 'grid':{}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': BayesianRidge(), 'grid':{}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': CCA(), 'grid':{}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': ElasticNet(), 'grid':{ 'random_state': [random_state],}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': ExtraTreeRegressor(), 'grid':{ 'random_state': [random_state],}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': ExtraTreesRegressor(), 'grid':{ 'random_state': [random_state],}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': GammaRegressor(), 'grid':{}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': HuberRegressor(), 'grid':{}, 'est_name':'', 'rmse':0.0},\n  #{'est': KNeighborsRegressor(), 'grid':{}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': KernelRidge(), 'grid':{}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': Lasso(), 'grid':{ 'random_state': [random_state],}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': Lars(), 'grid':{ 'random_state': [random_state],}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': LassoLars(), 'grid':{ 'random_state': [random_state],}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': LassoLarsIC(), 'grid':{}, 'est_name':'', 'rmse':0.0, 'params':{}},\n  #{'est': LinearSVR(), 'grid':{ 'random_state': [random_state],}, 'est_name':'', 'rmse':0.0, 'params':{}},\n]","2dd0f1d5":"X_train = df.drop(['id','price'],axis=1)\ny_train = df['price'].tolist()\n# Treina\/testa estimador\nfor estimator in estimators:\n    estimator['est'], estimator['est_name'], estimator['rmse'], estimator['params'] = build(X_train,y_train,estimator['est'],estimator['grid'])\n","c87350bf":"# Sele\u00e7\u00e3o do melhor estimador\ndef get_rmse(estimator):\n    return estimator.get('rmse')\n\nestimators.sort(key=get_rmse, reverse=False)\n\nfor estimator in estimators:\n    print(estimator['est_name'],'- RMSE:',estimator['rmse'],'- Params:',estimator['params'])\n\n# escolhe melhor estimador\nregr = estimators[0]['est']\nprint('\\nSelecionado: ',estimators[0]['est_name'])","c700188c":"# reduz a dimensionalidade (remove atributos de baixa correla\u00e7\u00e3o)\ndf, df_test, df_importances = reduce_dimension(regr, df, df_test, 0.001)\n# Exibe uma amostra dos dados\ndf.head()","cc96e54b":"# Exibe uma amostra dos dados\ndf_test.head()","ff18b82d":"# Treina o modelo com a massa de treino\n# Avalia a performance do modelo treinado\nX_train = df.drop(['id','price'],axis=1)\ny_train = df['price'].tolist()\nrmse_scorer = make_scorer(mean_squared_error, squared=False)\nscores = cross_validate(regr, X_train, y_train, cv=5, n_jobs=-1, scoring=rmse_scorer, verbose=100)","59e4cde3":"# Treina o estimador com toda a base fornecida\nX_train = df.drop(['id','price'],axis=1)\ny_train = df['price'].tolist()\nregr.fit(X_train, y_train)","04e74e1e":"# Prepara os dados para regress\u00e3o\nif 'price' in df_test.columns:\n    df_test.drop(['price'],axis=1,inplace=True)\n\nX_test = df_test.drop(['id'],axis=1)\n# Exibe uma amostra dos dados\nX_test.head()","fcd6479e":"# Executa a predi\u00e7\u00e3o dos registros n\u00e3o rotulados\ny_pred = regr.predict(X_test)\ndf_test['price'] = y_pred\ndf_test['price'] = df_test['price'].round(decimals=2)\n\n# Exibe uma amostra dos resultados\ndf_test.head(10)","60970320":"# Salva os registros\ndf_test.to_csv(nome_arquivo_rotulado_regressor, index=False, sep=\",\", encoding=\"utf-8\", columns=['id','price'])","fd5651f0":"# Desafio IA SERPRO 2020 - MP\n\nRobson de Sousa Martins [https:\/\/www.robsonmartins.com](https:\/\/www.robsonmartins.com)\n________________________________________________________________________________________________________________________\n\n**P\u00e1gina do Desafio:** [https:\/\/www.kaggle.com\/c\/desafioiamp2020](https:\/\/www.kaggle.com\/c\/desafioiamp2020)","87dbb07a":"# Submetendo os resultados \u00e0 p\u00e1gina do desafio\nO arquivo de resultados no passo anterior deve ser submetido na plataforma de avali\u00e7\u00e3o do desafio. Assim que avaliado, a pontua\u00e7\u00e3o ser\u00e1 exibida no leaderboard.","d329bb73":"# Analisando os registros n\u00e3o rotulados para o desafio\n","f6798b7b":"# Carregando os dados","01724661":"# Bibliotecas Utilizadas","65e26138":"### Reduz dimensionalidade","6f368138":"# Escolhendo, treinando e testando um modelo preditivo","8c77d729":"# Inicializa\u00e7\u00e3o","077f0f78":"### Trata valores faltantes\/incorretos","b98c123a":"### Expande colunas agregadas e faz outras transforma\u00e7\u00f5es","d247f64c":"### Remove atributos irrelevantes","c76caef6":"# Treinando, testando e avaliando o modelo\nRMSE: Raiz do erro m\u00e9dio quadr\u00e1tico - \u00c9 uma m\u00e9trica que visa medir a diferen\u00e7a, ou erro, entre os valores previstos e os realizados. Quanto menor o erro, melhor \u00e9 o modelo.","8d2ca64c":"### Normaliza atributos","511ad57c":"# Preparando os dados para a predi\u00e7\u00e3o","8c103556":"### Converte atributos categ\u00f3ricos em quantitativos discretos","b1b6da15":"# Fun\u00e7\u00f5es"}}