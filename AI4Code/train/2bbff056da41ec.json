{"cell_type":{"0f200d89":"code","4d2afdfb":"code","4a131b40":"code","df115cd2":"code","dd60a77a":"code","1d2dbbb1":"code","355fd25b":"code","b0e41c03":"code","4ea4df4d":"code","ee0eccac":"code","ef300973":"code","2fa87470":"code","23aa1627":"code","b615f42f":"code","e01aac65":"code","9bbe38b9":"code","510ada0b":"code","bd458858":"code","d44d39cb":"code","c1be552d":"code","f6d80a2e":"code","5476bf25":"code","3927312d":"code","c655a7e1":"code","7148c633":"code","afa0dbe3":"code","64ace207":"code","4583b4fd":"code","b33dab1c":"code","1acb9b1d":"code","5ffcaa9e":"code","dae6f7a2":"code","c8d20a16":"code","caa44d02":"code","d9e5e4c7":"code","dd4eba4c":"code","f09279e0":"code","5b2eb6c7":"code","576c3e29":"code","16b41422":"code","2bc39c82":"code","00223fa4":"code","2d86a2c8":"code","fe076fe1":"code","d247f14c":"code","dee939ab":"code","48e0cd70":"code","afe23d22":"code","23ddcf66":"code","21720bdc":"code","9b05cb7d":"markdown","5409ef54":"markdown","a0a81b95":"markdown","3391fa2d":"markdown","757fc939":"markdown","89d9f9a2":"markdown","cfe0eac3":"markdown","e28624a1":"markdown","a1018066":"markdown","117b03bc":"markdown","812aea17":"markdown","4eeabb43":"markdown","305a0d77":"markdown","f3262d22":"markdown","c616ba29":"markdown","f1d144a7":"markdown","cd699366":"markdown","fa7abcfb":"markdown","543e83aa":"markdown","2e6c5462":"markdown","754d57c3":"markdown","29087c26":"markdown","c631c08e":"markdown","5af8202b":"markdown","672797a7":"markdown","f4e27eb0":"markdown","727d7b74":"markdown","0fdfb6d8":"markdown","cd5d3306":"markdown"},"source":{"0f200d89":"# End of Intro\n#=========================================================================================#\n# Start of Part 1","4d2afdfb":"from numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nimport os\nimport cv2\n\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","4a131b40":"# Number of samples we want in each class.Total images used = SAMPLE_SIZE X 2\n# The minority class is class 1 with 78786 samples.\n\nSAMPLE_SIZE = 78786\n\nIMAGE_SIZE = 50","df115cd2":"os.listdir('..\/input\/IDC_regular_ps50_idx5')","dd60a77a":"# Check the number of patient folders.\n\npatients = os.listdir('..\/input\/IDC_regular_ps50_idx5')\n\nlen(patients)","1d2dbbb1":"# Create a new directory to store all available images\nall_images_dir = 'all_images_dir'\nos.mkdir(all_images_dir)\n","355fd25b":"# check that the new diectory has been created\n!ls","b0e41c03":"# This code copies all images from their seperate folders into the same \n# folder called all_images_dir.\n\n# Create a list with all the patient id numbers.\n# Each patient id folder has 2 sub folders --> folder 0 and folder 1\n\n# Example:\n    # '10285'\n        # '0'\n        # '1'\n\n# create a list of all patient id's\npatient_list = os.listdir('..\/input\/IDC_regular_ps50_idx5')\n\nfor patient in patient_list:\n    \n    path_0 = '..\/input\/IDC_regular_ps50_idx5\/' + str(patient) + '\/0'\n    path_1 = '..\/input\/IDC_regular_ps50_idx5\/' + str(patient) + '\/1'\n\n\n    # create a list of all files in folder 0\n    file_list_0 = os.listdir(path_0)\n    # create a list of list all file in folder 1\n    file_list_1 = os.listdir(path_1)\n\n    # move the 0 images to all_images_dir\n    for fname in file_list_0:\n\n        # source path to image\n        src = os.path.join(path_0, fname)\n        # destination path to image\n        dst = os.path.join(all_images_dir, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n\n    # move the 1 images to all_images_dir\n    for fname in file_list_1:\n\n        # source path to image\n        src = os.path.join(path_1, fname)\n        # destination path to image\n        dst = os.path.join(all_images_dir, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n","4ea4df4d":"# check how many images are in all_images_dir\n# should be 277,524\n\n# size: 2.5GB\n\nlen(os.listdir('all_images_dir'))","ee0eccac":"image_list = os.listdir('all_images_dir')\n\ndf_data = pd.DataFrame(image_list, columns=['image_id'])\n\ndf_data.head()","ef300973":"# Define Helper Functions\n\n# Each file name has this format:\n# '14211_idx5_x2401_y1301_class1.png'\n\ndef extract_patient_id(x):\n    # split into a list\n    a = x.split('_')\n    # the id is the first index in the list\n    patient_id = a[0]\n    \n    return patient_id\n\ndef extract_target(x):\n    # split into a list\n    a = x.split('_')\n    # the target is part of the string in index 4\n    b = a[4]\n    # the ytarget i.e. 1 or 2 is the 5th index of the string --> class1\n    target = b[5]\n    \n    return target\n\n# extract the patient id\n\n# create a new column called 'patient_id'\ndf_data['patient_id'] = df_data['image_id'].apply(extract_patient_id)\n# create a new column called 'target'\ndf_data['target'] = df_data['image_id'].apply(extract_target)\n\ndf_data.head(10)","2fa87470":"df_data.shape","23aa1627":"# source: https:\/\/www.kaggle.com\/gpreda\/honey-bee-subspecies-classification\n\ndef draw_category_images(col_name,figure_cols, df, IMAGE_PATH):\n    \n    \"\"\"\n    Give a column in a dataframe,\n    this function takes a sample of each class and displays that\n    sample on one row. The sample size is the same as figure_cols which\n    is the number of columns in the figure.\n    Because this function takes a random sample, each time the function is run it\n    displays different images.\n    \"\"\"\n    \n\n    categories = (df.groupby([col_name])[col_name].nunique()).index\n    f, ax = plt.subplots(nrows=len(categories),ncols=figure_cols, \n                         figsize=(4*figure_cols,4*len(categories))) # adjust size here\n    # draw a number of images for each location\n    for i, cat in enumerate(categories):\n        sample = df[df[col_name]==cat].sample(figure_cols) # figure_cols is also the sample size\n        for j in range(0,figure_cols):\n            file=IMAGE_PATH + sample.iloc[j]['image_id']\n            im=cv2.imread(file)\n            ax[i, j].imshow(im, resample=True, cmap='gray')\n            ax[i, j].set_title(cat, fontsize=16)  \n    plt.tight_layout()\n    plt.show()","b615f42f":"IMAGE_PATH = 'all_images_dir\/'\n\ndraw_category_images('target',4, df_data, IMAGE_PATH)","e01aac65":"# What is the class distribution?\n\ndf_data['target'].value_counts()","9bbe38b9":"# take a sample of the majority class 0 (total = 198738)\ndf_0 = df_data[df_data['target'] == '0'].sample(SAMPLE_SIZE, random_state=101)\n# take a sample of class 1 (total = 78786)\ndf_1 = df_data[df_data['target'] == '1'].sample(SAMPLE_SIZE, random_state=101)\n\n# concat the two dataframes\ndf_data = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n\n# Check the new class distribution\ndf_data['target'].value_counts()","510ada0b":"# train_test_split\n\n# stratify=y creates a balanced validation set.\ny = df_data['target']\n\ndf_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101, stratify=y)\n\nprint(df_train.shape)\nprint(df_val.shape)","bd458858":"df_train['target'].value_counts()","d44d39cb":"df_val['target'].value_counts()","c1be552d":"# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# now we create 2 folders inside 'base_dir':\n\n# train_dir\n    # a_no_idc\n    # b_has_idc\n\n# val_dir\n    # a_no_idc\n    # b_has_idc\n\n\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\na_no_idc = os.path.join(train_dir, 'a_no_idc')\nos.mkdir(a_no_idc)\nb_has_idc = os.path.join(train_dir, 'b_has_idc')\nos.mkdir(b_has_idc)\n\n\n# create new folders inside val_dir\na_no_idc = os.path.join(val_dir, 'a_no_idc')\nos.mkdir(a_no_idc)\nb_has_idc = os.path.join(val_dir, 'b_has_idc')\nos.mkdir(b_has_idc)\n","f6d80a2e":"# check that the folders have been created\nos.listdir('base_dir\/train_dir')","5476bf25":"# Set the id as the index in df_data\ndf_data.set_index('image_id', inplace=True)","3927312d":"# Get a list of train and val images\ntrain_list = list(df_train['image_id'])\nval_list = list(df_val['image_id'])\n\n\n\n# Transfer the train images\n\nfor image in train_list:\n    \n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname = image\n    # get the label for a certain image\n    target = df_data.loc[image,'target']\n    \n    # these must match the folder names\n    if target == '0':\n        label = 'a_no_idc'\n    if target == '1':\n        label = 'b_has_idc'\n    \n    # source path to image\n    src = os.path.join(all_images_dir, fname)\n    # destination path to image\n    dst = os.path.join(train_dir, label, fname)\n    # move the image from the source to the destination\n    shutil.move(src, dst)\n    \n\n# Transfer the val images\n\nfor image in val_list:\n    \n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname = image\n    # get the label for a certain image\n    target = df_data.loc[image,'target']\n    \n    # these must match the folder names\n    if target == '0':\n        label = 'a_no_idc'\n    if target == '1':\n        label = 'b_has_idc'\n    \n\n    # source path to image\n    src = os.path.join(all_images_dir, fname)\n    # destination path to image\n    dst = os.path.join(val_dir, label, fname)\n    # move the image from the source to the destination\n    shutil.move(src, dst)","c655a7e1":"# check how many train images we have in each folder\n\nprint(len(os.listdir('base_dir\/train_dir\/a_no_idc')))\nprint(len(os.listdir('base_dir\/train_dir\/b_has_idc')))","7148c633":"# check how many val images we have in each folder\n\nprint(len(os.listdir('base_dir\/val_dir\/a_no_idc')))\nprint(len(os.listdir('base_dir\/val_dir\/b_has_idc')))\n","afa0dbe3":"# End of Data Preparation\n### ================================================================================== ###\n# Start of Model Building","64ace207":"train_path = 'base_dir\/train_dir'\nvalid_path = 'base_dir\/val_dir'\n\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\n\n\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)\nval_steps = np.ceil(num_val_samples \/ val_batch_size)","4583b4fd":"datagen = ImageDataGenerator(rescale=1.0\/255)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical')\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","b33dab1c":"# Source: https:\/\/www.kaggle.com\/fmarazzi\/baseline-keras-cnn-roc-fast-5min-0-8253-lb\n\nkernel_size = (3,3)\npool_size= (2,2)\nfirst_filters = 32\nsecond_filters = 64\nthird_filters = 128\n\ndropout_conv = 0.3\ndropout_dense = 0.3\n\n\nmodel = Sequential()\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu', \n                 input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size)) \nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(Dense(2, activation = \"softmax\"))\n\nmodel.summary()","1acb9b1d":"model.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n              metrics=['accuracy'])\n","5ffcaa9e":"filepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=3, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                    validation_data=val_gen,\n                    validation_steps=val_steps,\n                    epochs=60, verbose=1,\n                   callbacks=callbacks_list)","dae6f7a2":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","c8d20a16":"# Here the best epoch will be used.\n\nmodel.load_weights('model.h5')\n\nval_loss, val_acc = \\\nmodel.evaluate_generator(test_gen, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","caa44d02":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()","d9e5e4c7":"# make a prediction\npredictions = model.predict_generator(test_gen, steps=len(df_val), verbose=1)","dd4eba4c":"predictions.shape","f09279e0":"# This is how to check what index keras has internally assigned to each class. \ntest_gen.class_indices","5b2eb6c7":"# Put the predictions into a dataframe.\n# The columns need to be oredered to match the output of the previous cell\n\ndf_preds = pd.DataFrame(predictions, columns=['no_idc', 'has_idc'])\n\ndf_preds.head()","576c3e29":"# Get the true labels\ny_true = test_gen.classes\n\n# Get the predicted labels as probabilities\ny_pred = df_preds['has_idc']","16b41422":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_true, y_pred)","2bc39c82":"# Source: Scikit Learn website\n# http:\/\/scikit-learn.org\/stable\/auto_examples\/\n# model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n# selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","00223fa4":"# Get the labels of the test images.\n\ntest_labels = test_gen.classes","2d86a2c8":"test_labels.shape","fe076fe1":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))","d247f14c":"# Print the label associated with each class\ntest_gen.class_indices","dee939ab":"# Define the labels of the class indices. These need to match the \n# order shown above.\ncm_plot_labels = ['no_idc', 'has_idc']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')\n","48e0cd70":"from sklearn.metrics import classification_report\n\n# Generate a classification report\n\n# For this to work we need y_pred as binary labels not as probabilities\ny_pred_binary = predictions.argmax(axis=1)\n\nreport = classification_report(y_true, y_pred_binary, target_names=cm_plot_labels)\n\nprint(report)","afe23d22":"!pip install tensorflowjs","23ddcf66":"# Use the command line conversion tool to convert the model\n\n!tensorflowjs_converter --input_format keras model.h5 tfjs_model_1\/model","21720bdc":"# Delete all_images_dir and base_dir directory to prevent a Kaggle error.\n# Kaggle allows a max of 500 files to be saved.\n\nshutil.rmtree('all_images_dir')\nshutil.rmtree('base_dir')","9b05cb7d":"### Create the Model Architecture","5409ef54":"### Balance the class distribution","a0a81b95":"### Plot the Training Curves","3391fa2d":"### Convert the model to from Keras to Tensorflowjs\nThis conversion needs to be done so that the model can be loaded into the web app.","757fc939":"In this section we will create the IDC_model. This model will predict whether or not Invasive Ductal Carcinoma is present. \n\n**Dataset**\n\nWe will use the 'Breast Histopathology Images' dataset. This dataset consists of 277,524 image patches of size 50x50 (198,738 IDC negative and 78,786 IDC positive). The images are in png format.\n\n**Results**\n\nOur cnn model will achieve an accuracy and F1 score that is approximately 0.88. This is based on a classification threshold of 0.5. It should be noted that the creators of the [paper](https:\/\/www.researchgate.net\/publication\/263052166_Automatic_detection_of_invasive_ductal_carcinoma_in_whole_slide_images_with_Convolutional_Neural_Networks) realting to this dataset used a threshold of 0.29.\n","89d9f9a2":"### Evaluate the model using the val set","cfe0eac3":"### Part 1 - IDC Model","e28624a1":"### Conclusion","a1018066":"### Project Objective\n\n> The goal of this project is to create a web based tool that can batch analyze histopathology image patches and predict if breast cancer is present. The web app will be able to detect two forms of breast cancer:<br>\n> \n> - Invasive Ductal Carcinoma (IDC)\n> - Metastatic Cancer\n\nWe will create two CNN models, an IDC_model and a Metastatic_model. These models will be loaded into a Tensorflowjs web app. \n\n**A pathologist will be able to**:<br>\n- select which cancer he or she wants to detect,\n- submit many images at the same time and, \n- get an instant prediction indicating whether or not breast cancer is present in those images.\n\nTo ensure privacy and patient confidentiality all images will be processed locally and never uploaded to an external server.\n\nWe will create the models in two kernels. In part 1 we will build the IDC_model and in part 2 the Metastatic_model. \n\nIn part 2 I will also cover some lessons that I learned while building the app.\n\n**Results**\n\nDue to the large amount of high quality data that's available for training, our models will produce the following results (approx):<br>\n\n> **IDC_model**<br>\n> Accuracy: 0.88<br>\n> F1 Score: 0.88\n> \n> **Metastatic_model:**<br>\n> Accuracy: 0.94<br>\n> F1 Score: 0.94\n\n\n\n\n***\n\nAll the html, css, and javascript code used to build the web app is available on Github. The technology that enables this app to work is new. Therefore, I recommend using the latest version of the Chrome browser. When using Safari for example, you may see a message indicating that the model is loading but the app may actually be frozen.\n\nLive web app:<br>\nhttp:\/\/histo.test.woza.work\/<br>\nGithub:<br>\nhttps:\/\/github.com\/vbookshelf\/Breast-Cancer-Analyzer","117b03bc":"In part 2 we will use the same workflow to build a model to detect metastatic cancer.\n\nMany thanks to Paul Mooney for making the Breast Histopathology Images dataset available on Kaggle. \n\nSee you in part 2.","812aea17":"### Transfer the images into the folders\u00b6","4eeabb43":"### Make a prediction on the val set\nWe need these predictions to calculate the AUC score, print the Confusion Matrix and calculate the F1 score.","305a0d77":"### Display a random sample of train images by class","f3262d22":"**Recall **= Given a class, will the classifier be able to detect it?<br>\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?<br>\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\n","c616ba29":"### Train the Model","f1d144a7":"### What problem needs to be solved?\n\n> The process that's used to detect breast cancer is time consuming and small malignant areas can be missed.\n> \n> In order to detect cancer, a tissue section is put on a glass slide. A pathologist then examines this slide under a microscope. The pathologist needs to visually scan large regions where there's no cancer in order to ultimately find malignant areas.\n> \n> Because these glass slides can now be digitized, computer vision can be used to speed up a pathologist's workflow and provide diagnosis support. By deploying a machine learning solution as a web app, it can be made available to medical personnel anywhere in the world for free.\n> \n***","cd699366":"*Link to Part 2:*<br>\nhttps:\/\/www.kaggle.com\/vbookshelf\/part-2-breast-cancer-analyzer-web-app","fa7abcfb":"### Create a Confusion Matrix","543e83aa":"### Glossary of Terms\n\n**Histopathology**<br>\nThis inolves examining glass tissue slides under a microscope to see if disease is present. In this case that would be examining tissue samples from lymp nodes in order to detect breast cancer.\n\n**Whole Slide Image (WSI)**<br> \nA digitized high resolution image of a glass slide taken with a scanner. The images can be several gigabytes in size. Therefore, to allow them to be used in machine learning these digital images are cut up into Patches.\n\n**Patch** <br>\nA patch is a small, usually rectangular, piece of an image. For example, a 50x50 patch is a square patch containing 2500 pixels, taken from a larger image of size say 1000x1000 pixels.\n\n**Lymph Node**<br>\nThis is a small bean shaped structure that's part of the body's immune system. Lymph nodes filter substances that travel through the lymphatic fluid. They contain lymphocytes (white blood cells) that help the body fight infection and disease.\n\n\n**Sentinel Lymph Node**<br>\nA blue dye and\/or radioactive tracer is injected near the tumor. The first lymph node reached by this injected substance is called the sentinel lymph node. The images that we will be using are all of tissue samples taken from sentinel lymph nodes.\n\n**Invasive Ductal Carcinoma**<br> \nThis is the most common subtype of all breast cancers. Almost 80% of diagnosed breast cancers are of this subtype.\n\n**Metastasis**<br>\nThe spread of cancer cells to new areas of the body, often via the lymph system or bloodstream.\n\n**Metastatic Cancer**<br> \nA metastatic cancer or metastatic tumor is one that has spread from the site where it started into different area\/s of the body.\n\n***\n\n","2e6c5462":"### Reference Papers\n\nThese are the papers that are associated with the two datasets that we will be using:\n\n> Automatic detection of invasive ductal carcinoma in whole slide images with Convolutional Neural Networks<br>\n> [Link](https:\/\/www.researchgate.net\/publication\/263052166_Automatic_detection_of_invasive_ductal_carcinoma_in_whole_slide_images_with_Convolutional_Neural_Networks)\n> \n> \n> 1399 H&E-stained sentinel lymph node sections of breast cancer patients: the CAMELYON dataset<br>\n> [Link](https:\/\/academic.oup.com\/gigascience\/article\/7\/6\/giy065\/5026175)\n\n\n***","754d57c3":"### Create a Classification Report","29087c26":"### What is the potential of computational pathology?\n\nThis is what two experts have to say:\n\nThis is a 1:21 video featuring Prof Jeroen van der Laak, a co-author of the CAMELYON paper.<br>\nhttps:\/\/www.youtube.com\/watch?v=MCRtJDDrdcA\n\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/MCRtJDDrdcA?rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>\n\n***\n\nThis 1:46 video features Ian Ellis, Professor of Cancer Pathology at Nottingham University<br>\nhttps:\/\/www.youtube.com\/watch?v=gcFnyuc7Bew\n\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/gcFnyuc7Bew?rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>\n\n***","c631c08e":"### What files are available?\n\nThe images are grouped into 279 folders by patient_id. Each patient folder has two sub-folders that groups together images with the same class --> 0 or 1. There are a lot of folders to work with.","5af8202b":"### Set Up the Generators","672797a7":"### Create a Directory Structure","f4e27eb0":"### Copy all images into one directory\nThis will make it easier to work with this data.","727d7b74":"### Create a dataframe containing all the information","0fdfb6d8":"### Create the train and  val sets\n","cd5d3306":"### What is the AUC Score?"}}