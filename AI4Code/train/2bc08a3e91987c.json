{"cell_type":{"63c5c664":"code","ba65559d":"code","04936c11":"code","545219fb":"code","6c67eb0f":"code","0be90b7a":"code","54fb4f6b":"code","f60c8f68":"code","79cec752":"code","104cfc24":"code","3f9e9082":"code","265d6951":"code","bfe4b3fd":"code","440a4c8b":"code","6e160c58":"code","a0cc689e":"code","eed384a9":"code","1fe55012":"code","3d030212":"code","281a498d":"code","428a2b32":"code","18faf539":"code","ba76c949":"markdown","d6d09d72":"markdown","d55800dc":"markdown","2bcf2f74":"markdown","50db0ff5":"markdown"},"source":{"63c5c664":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ba65559d":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport keras\nimport copy\nimport string\nimport copy","04936c11":"tweets_reader=pd.read_csv(\"\/kaggle\/input\/democratvsrepublicantweets\/ExtractedTweets.csv\",chunksize=1000)\nusers_df=pd.read_csv(\"\/kaggle\/input\/democratvsrepublicantweets\/TwitterHandles.csv\")\n","545219fb":"\nword_cts_df=pd.DataFrame()\nhandles_df=pd.DataFrame(columns=[\"Party\"])\ntweets_reader=pd.read_csv(\"\/kaggle\/input\/democratvsrepublicantweets\/ExtractedTweets.csv\",chunksize=1000)\nfor i,chunk in enumerate(tweets_reader):\n    #if i>10:\n        #break\n    chunk[\"Text\"]=chunk[\"Tweet\"].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)).lower())\n    chunk[\"Tokenized\"]=chunk[\"Text\"].apply(lambda x: x.split(\" \"))\n    corpus = list(chunk[\"Text\"])\n    handles1_df=chunk.drop_duplicates(subset=[\"Handle\"])[[\"Handle\",\"Party\"]]\n    handles1_df.index=handles1_df.Handle\n    handles1_df=handles1_df[\"Party\"]\n    handles_df.append(handles1_df)\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(corpus)\n    chunk_freq=np.sum(X.toarray(),0)\n    c=pd.Series(chunk_freq)\n    c.index=pd.Series(vectorizer.get_feature_names())\n    chunk_cts_df=pd.DataFrame(columns=[\"Count\"],data={\"Count\":c},\n                              index=pd.Series(vectorizer.get_feature_names()))\n    word_cts_df=word_cts_df.add(chunk_cts_df,fill_value=0)\n    if len(word_cts_df)>100000:\n        word_cts_df=word_cts_df.sort_values(\"Count\",ascending=False)\n        word_cts_df=word_cts_df.iloc[range(20000)]\n    #if i>=50:\n        #break\n    \n    \ntweets_reader.close()\nold_cts_df=copy.copy(word_cts_df)\n","6c67eb0f":"handles_df=pd.read_csv(\"\/kaggle\/input\/democratvsrepublicantweets\/ExtractedTweets.csv\")\\\n.drop_duplicates(subset=[\"Handle\"])\nhandles_df.index=handles_df.Handle\nhandles_df=handles_df[[\"Party\"]]\nlen(handles_df)","0be90b7a":"N_words=2000\nword_cts_df=old_cts_df.sort_values(\"Count\",ascending=False)\nword_cts_df=word_cts_df.iloc[range(N_words)]\nword_cts_df","54fb4f6b":"def filter_tweet(tweet_tok,words_index):\n    return(list(map (lambda w:w if w in words_index else '?',tweet_tok)))\ndef vectorize_tweet(tweet_tok,words_index,max_words,sentence_length):\n    tweet_list=list(map (lambda w:list(words_index).index(w) \n                     if w in words_index else max_words,tweet_tok))\n    if len(tweet_list)>sentence_length:\n        tweet_list=tweet_list[:sentence_length]\n    if len(tweet_list)<sentence_length:\n        tweet_list+=(sentence_length-len(tweet_list))*[max_words]\n    return(np.array(tweet_list))\n\nprint(filter_tweet(chunk[\"Tokenized\"].iloc[100],word_cts_df.index))\ntest_tok=copy.copy(chunk[\"Tokenized\"].iloc[100])\n#test_tok+=5*[\"the\"]+100*[\"national\"]\nvec_test=vectorize_tweet(test_tok,word_cts_df.index,N_words,20)\nprint(vec_test,\"Length:\",len(vec_test))\nprint(chunk[\"Tokenized\"].iloc[100])\n#chunk[\"Tokenized\"]\n#\"www\" in word_cts_df.index # Now we have the first 1000 words\nword_cts_df.index","f60c8f68":"# Now lets tokenize the tweets, then use the words to look up in table and return \"?\" if nothing.\ntweets_reader=pd.read_csv(\"\/kaggle\/input\/democratvsrepublicantweets\/ExtractedTweets.csv\",chunksize=1000)\nsentence_length=20\nhandle_seq=pd.Series()\nvec_data=[]\nclasses=[]\nfor i,chunk in enumerate(tweets_reader):\n    print(i)\n    chunk[\"Text\"]=chunk[\"Tweet\"].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)).lower())\n    chunk[\"Tokenized\"]=chunk[\"Text\"].apply(lambda x: x.split(\" \"))\n    chunk[\"Replaced\"]=chunk[\"Tokenized\"].apply(lambda x: filter_tweet(x,word_cts_df.index))\n    chunk[\"Vectorized\"]=chunk[\"Tokenized\"].apply(lambda x: vectorize_tweet(x,word_cts_df.index,\n                                                                           N_words,sentence_length))\n    \n    handle_seq=handle_seq.append(chunk[\"Handle\"])\n    vec_data+=list(chunk[\"Vectorized\"])\n    corpus = list(chunk[\"Text\"])\n    vectorizer = CountVectorizer()\n    classes+=list(chunk[\"Party\"]==\"Democrat\")\n    X = vectorizer.fit_transform(corpus)\n    \n    # Lets also append a list of all the handles together so we can \n    #if i>=1000:\n        #break\n","79cec752":"old_vec_data=np.array(vec_data)\nold_classes=np.array(classes)\nvec_data=np.array(vec_data)\nclasses=np.array(classes)\nrand_perm=np.random.permutation(len(vec_data))\nvec_data=vec_data[rand_perm,:]\nclasses=classes[rand_perm]\ninput_train=vec_data[:int(.8*np.floor(len(vec_data))),:]\ny_train=classes[:int(np.floor(.8*len(vec_data)))]\n","104cfc24":"input_test=vec_data[int(.8*np.floor(len(vec_data))):,:]\ny_test=classes[int(np.floor(.8*len(vec_data))):]","3f9e9082":"len(classes)","265d6951":"from keras.models import Sequential\nfrom keras.layers import Embedding, SimpleRNN\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\nmodel = Sequential()\n\nmodel.add(Embedding(N_words+1,128))\nmodel.add(LSTM(8))\nmodel.add(Dense(8,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',\n             loss='binary_crossentropy',\n             metrics=['acc'])","bfe4b3fd":"history = model.fit(input_train, y_train,\n                   epochs=15,\n                    batch_size=128,\n                    validation_split=.25)","440a4c8b":"from matplotlib import pyplot as plt\nacc =  history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1,len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Trainin acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.legend()\nplt.figure()\nplt.plot(epochs,loss,'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.legend()","6e160c58":"results = model.evaluate(input_test, y_test, batch_size=128)\nprint(\"test loss, test acc:\", results)","a0cc689e":"# Now lets withold some accounts from the training and validating to see if we can accurately \nusers_df=pd.read_csv(\"\/kaggle\/input\/democratvsrepublicantweets\/TwitterHandles.csv\")","eed384a9":"import pandas as pd\nhandles_df=pd.read_csv(\"\/kaggle\/input\/democratvsrepublicantweets\/ExtractedTweets.csv\")\\\n.drop_duplicates(subset=[\"Handle\"])\nhandles_df.index=handles_df.Handle\nhandles_df=handles_df[[\"Party\"]]\n\n\n# pick a politician from handles df\np_ind=list(np.random.choice(len(handles_df),12))\n\nhandle_seq\n\nold_vec_data\nold_classes\nimport functools\ntest_set_bool=functools.reduce( lambda x,y: x|y,[handle_seq==handles_df.iloc[i].name for i in p_ind])\ntest_input=old_vec_data[test_set_bool,:]\ny_test=old_classes[test_set_bool]\n\ntrain_input=old_vec_data[~test_set_bool,:]\ny_train=old_classes[~test_set_bool]\n#[handle_seq==handles_df.iloc[i].name for i in p_ind]\nlen(y_test)\nlen(test_input)\nlen(y_train)\n#len(old_classes)","1fe55012":"from keras.models import Sequential\nfrom keras.layers import Embedding, SimpleRNN\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\nmodel2 = Sequential()\n\nmodel2.add(Embedding(N_words+1,256))\nmodel2.add(LSTM(8))\nmodel2.add(Dense(8,activation='relu'))\nmodel2.add(Dense(1,activation='sigmoid'))\nmodel2.compile(optimizer='rmsprop',\n             loss='binary_crossentropy',\n             metrics=['acc'])","3d030212":"rand_perm=np.random.permutation(len(y_train))\nhistory = model2.fit(train_input[rand_perm,:], y_train[rand_perm],\n                   epochs=10,\n                    batch_size=128,\n                    validation_split=.2)","281a498d":"predictions=model2.predict(test_input)\n","428a2b32":"from matplotlib import pyplot as plt\ntweets_df=pd.read_csv(\"\/kaggle\/input\/democratvsrepublicantweets\/ExtractedTweets.csv\")\n","18faf539":"from sklearn.metrics import roc_curve, auc\ny_score=predictions\nfpr, tpr, thresholds=roc_curve(y_test,predictions)\nplt.figure(figsize=(10,10))\nplt.plot(fpr,tpr)\n#plt.plot([0,1],[0,1])\n#fpr","ba76c949":"# ROC Curve of held out politicians","d6d09d72":"# USA Politician Twitter Classification with LSTM NN","d55800dc":"# Apply function to tokenized vect that grows tweet to a fixed length adding \"?\" + replace removed words with string of \"?\"","2bcf2f74":"# Definition of LSTM with embedding\n# Low complexity model does a good job if we have ~5000 words: ~77% classification\n\n# Retaining 2000 words gives us approximately 70%","50db0ff5":"# We only want to use a subset of most common words"}}