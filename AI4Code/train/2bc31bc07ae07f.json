{"cell_type":{"3332e3d5":"code","8c049657":"code","67304c3d":"code","90b209a5":"code","14f9d186":"code","6feda347":"code","e4ba2915":"code","20c67562":"code","c3330bcc":"code","6cb3cea3":"code","c877adb6":"code","1efe0211":"code","f0cb2a9c":"code","d33594b1":"code","215e224e":"code","dec00f14":"code","83e22688":"code","71435ab2":"code","ad09faec":"code","86414dec":"code","93f34028":"code","810f5bed":"code","a4c0017f":"code","ca10c64c":"code","e99e01f1":"code","ea365212":"code","0a313110":"code","6a899743":"code","c331c1ba":"code","5a06adc5":"code","2c6b2c32":"code","f7bd2f36":"code","9379691a":"code","9d0b032f":"code","0d7bfea1":"code","f9965e38":"code","2cc95101":"code","683a8ab1":"code","57ab6b52":"code","f810b426":"code","85d0b0c1":"code","93961cb8":"code","e3b093e4":"code","a168cf96":"code","a12a3124":"code","8fa2f295":"code","c223317d":"code","57414f64":"code","a48c38ba":"code","7093fb15":"code","53778fa0":"markdown","3e95f6c9":"markdown","1eed0ae4":"markdown","351d6310":"markdown","9ee27547":"markdown","2f8c1945":"markdown","0a839d95":"markdown","dc97a71a":"markdown","464b9651":"markdown","3abf3ee0":"markdown","c240a934":"markdown","ced71dc1":"markdown","7d4042c5":"markdown","71bab7e6":"markdown","3ea42efc":"markdown","5cd3c422":"markdown","06b6c840":"markdown","e3481c81":"markdown","ae9bed40":"markdown","43a1bc9a":"markdown","544770eb":"markdown","95ad91b1":"markdown","7d584775":"markdown","fd6c67ed":"markdown","177edb11":"markdown","f921c21d":"markdown","ba4f2292":"markdown","5182f294":"markdown"},"source":{"3332e3d5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport nltk \nimport seaborn as sns\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n%matplotlib inline\n\n\n# Load Huggingface transformers\nfrom transformers import TFBertModel,  BertConfig, BertTokenizerFast, TFAutoModel\n\n# Then what you need from tensorflow.keras\nfrom tensorflow.keras.layers import Input, Dropout, Dense, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\n\n\npd.set_option('display.max_colwidth', 100)","8c049657":"df = pd.read_csv('\/kaggle\/input\/sinhala-unicode-hate-speech\/sinhala-hate-speech-dataset.csv', encoding='utf8')","67304c3d":"df.shape","90b209a5":"df.columns","14f9d186":"df.head()","6feda347":"count = df.isna().sum()\nprecentange = df.isna().mean().round(4) * 100\n\ndfMissing = pd.DataFrame({'count': count, 'precentange': precentange})\ndfMissing.sort_values('count', ascending=False)","e4ba2915":"df.groupby(['label'])['label'].describe()[['count']]","20c67562":"#calculate length of the comment\ndf['length'] = df['comment'].apply(len)\ndf.head()","c3330bcc":"temp = df.groupby('label').count()['comment'].reset_index().sort_values(by='comment',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","6cb3cea3":"from plotly import graph_objs as go\nplt.figure(figsize=(12,6))\nsns.countplot(x='label',data=df)\nfig = go.Figure(go.Funnelarea(\n    text =temp.label,\n    values = temp.comment,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n    ))\nfig.show()","c877adb6":"df.hist(column='length',by='label',figsize=(15,6), bins=150,color='orange')","1efe0211":"df[df['label'] == 1]['length'].describe()","f0cb2a9c":"#delete id from dataset\n#del df['id']","d33594b1":"exclude = set(\",.:;'\\\"-?!\/\u00b4`%\")\ndef removePunctuation(txt):\n    return ''.join([(c if c not in exclude else \" \") for c in txt])\n    \ndef removeNumbers(txt):\n    return ''.join(c for c in txt if not c.isnumeric())","215e224e":"df['cleaned'] = df['comment'].apply(lambda x: removePunctuation(x))\ndf.head()","dec00f14":"df['cleaned'] = df['cleaned'].apply(lambda x: removeNumbers(x))\ndf.head()","83e22688":"from collections import Counter\n\nplt.rc('font', family='Lohit Devanagari')\n\nresults = Counter()\ndf.cleaned.str.split().apply(results.update)\n\nmost = results.most_common()\nprint(most[:10])","71435ab2":"words = [word for i in df[df['label'] == 1]['cleaned'].str.split() for word in i]\n\ncounter = Counter(words)\nmost = counter.most_common(20)\n    \nmost","ad09faec":"words = [word for i in df[df['label'] == 0]['cleaned'].str.split() for word in i]\n\ncounter = Counter(words)\nmost = counter.most_common(20)\n    \nmost","86414dec":"df.head()","93f34028":"import re,string,unicodedata\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier,ExtraTreesClassifier\nfrom collections import Counter\nimport string\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem import LancasterStemmer,WordNetLemmatizer\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\n# sklearn\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\n\n# 1. Convert text into vectors using TF-IDF\n# 2. Instantiate MultinomialNB classifier\n# 3. Split feature and label\n# 1. Convert text into vectors using TF-IDF\n# 2. Instantiate MultinomialNB classifier\n# 3. Split feature and label\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nfrom sklearn.pipeline import Pipeline","810f5bed":"# Name of the BERT model to use\nmodel_name = 'bert-base-uncased'\n\n# Max length of tokens\nmax_length = 128\n\n# Load transformers config and set output_hidden_states to False\nconfig = BertConfig.from_pretrained(model_name)\n#config.output_hidden_states = False\n\n# Load BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\nbert = TFAutoModel.from_pretrained(model_name)","a4c0017f":"input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\nattention_mask = Input(shape=(max_length,), name='attention_mask', dtype='int32') \ninputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\nx = bert.bert(inputs)","ca10c64c":"x","e99e01f1":"df","ea365212":"df_la = pd.get_dummies(df, columns = ['label'])\ndf_la","0a313110":"train_sentences = df[\"cleaned\"].values\nlist_classes = ['label_0','label_1']\ntrain_y = df_la[list_classes].values","6a899743":"#x2 =Dense(512, activation='relu')(x[1])\nx2 = GlobalAveragePooling1D()(x[0])\n#x3 = Dropout(0.5)(x2)\ny =Dense(len(list_classes), activation='sigmoid', name='outputs')(x2)\n\nmodel = Model(inputs=inputs, outputs=y)\n#model.layers[2].trainable = False\n\n# Take a look at the model\nmodel.summary()","c331c1ba":"optimizer = Adam(lr=1e-5, decay=1e-6)\nmodel.compile(loss='binary_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy'])","5a06adc5":"# Tokenize the input \nx = tokenizer(\n    text=list(train_sentences),\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)","2c6b2c32":"history = model.fit(\n    x={'input_ids': x['input_ids'], 'attention_mask': x['attention_mask']},\n    #x={'input_ids': x['input_ids']},\n    y={'outputs': train_y},\n    validation_split=0.1,\n    batch_size=32,\n    epochs=10)","f7bd2f36":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df['cleaned'], df['label'],\n                                                    test_size = 0.2, random_state = 42)\nprint(f'Data Split done.')","9379691a":"vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nprint(f'Vectoriser fitted.')\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))","9d0b032f":"X_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)\nprint(f'Data Transformed.')","0d7bfea1":"def model_Evaluate(model):\n    \n    # Predict values for Test dataset\n    y_pred = model.predict(X_test)\n\n    # Print the evaluation metrics for the dataset.\n    print(classification_report(y_test, y_pred))\n    \n    cm=confusion_matrix(y_pred , y_test)\n    plt.figure()\n    plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\n    plt.xticks(range(2), ['Negative',  'Positive'], fontsize=16,color='black')\n    plt.yticks(range(2), ['Negative', 'Positive'], fontsize=16)\n    plt.show()","f9965e38":"from sklearn.metrics import roc_curve, auc\ndtc= DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\nmodel_Evaluate(dtc)","2cc95101":"pred_dtc = dtc.predict_proba(X_test)[:,1]\nfpr_dtc,tpr_dtc,_ = roc_curve(y_test.values,pred_dtc)\nroc_auc_dtc = auc(fpr_dtc,tpr_dtc)\n\n\nf, axes = plt.subplots(1, 1,figsize=(7,7))\naxes.plot(fpr_dtc, tpr_dtc, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_dtc))\naxes.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes.set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes.set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Decision Tree Classifier')\naxes.legend(loc='lower right', fontsize=13)","683a8ab1":"lr=LogisticRegression()\nlr.fit(X_train, y_train)\nmodel_Evaluate(lr)","57ab6b52":"pred_lr = lr.predict_proba(X_test)[:,1]\nfpr_lr,tpr_lr,_ = roc_curve(y_test.values,pred_lr)\nroc_auc_lr = auc(fpr_lr,tpr_lr)\n\n\nf, axes = plt.subplots(1, 1,figsize=(7,7))\naxes.plot(fpr_lr, tpr_lr, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_lr))\naxes.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes.set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes.set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Logistic Regression')\naxes.legend(loc='lower right', fontsize=13)","f810b426":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n\nknn.fit(X_train,y_train)\n\nmodel_Evaluate(knn)","85d0b0c1":"pred_knn = knn.predict_proba(X_test)[:,1]\nfpr_knn,tpr_knn,_ = roc_curve(y_test.values,pred_knn)\nroc_auc_knn = auc(fpr_knn,tpr_knn)\n\n\nf, axes = plt.subplots(1, 1,figsize=(7,7))\naxes.plot(fpr_knn, tpr_knn, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_knn))\naxes.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes.set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes.set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'KNN')\naxes.legend(loc='lower right', fontsize=13)","93961cb8":"abc = AdaBoostClassifier()\n\nabc.fit(X_train,y_train)\n\nmodel_Evaluate(abc)","e3b093e4":"pred_abc = abc.predict_proba(X_test)[:,1]\nfpr_abc,tpr_abc,_ = roc_curve(y_test.values,pred_abc)\nroc_auc_abc = auc(fpr_abc,tpr_abc)\n\n\nf, axes = plt.subplots(1, 1,figsize=(7,7))\naxes.plot(fpr_abc, tpr_abc, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_abc))\naxes.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes.set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes.set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Ada Boost Classifier')\naxes.legend(loc='lower right', fontsize=13)","a168cf96":"mnb = MultinomialNB()\n\nmnb.fit(X_train,y_train)\n\n\nmodel_Evaluate(mnb)","a12a3124":"pred_mnb = mnb.predict_proba(X_test)[:,1]\nfpr_mnb,tpr_mnb,_ = roc_curve(y_test.values,pred_mnb)\nroc_auc_mnb = auc(fpr_mnb,tpr_mnb)\n\n\nf, axes = plt.subplots(1, 1,figsize=(7,7))\naxes.plot(fpr_mnb, tpr_mnb, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_mnb))\naxes.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes.set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes.set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Multi Nomial NB')\naxes.legend(loc='lower right', fontsize=13)","8fa2f295":"gbc = GradientBoostingClassifier()\n\ngbc.fit(X_train,y_train)\n\nmodel_Evaluate(gbc)","c223317d":"pred_gbc = gbc.predict_proba(X_test)[:,1]\nfpr_gbc,tpr_gbc,_ = roc_curve(y_test.values,pred_gbc)\nroc_auc_gbc = auc(fpr_gbc,tpr_gbc)\n\n\nf, axes = plt.subplots(1, 1,figsize=(7,7))\naxes.plot(fpr_gbc, tpr_gbc, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_gbc))\naxes.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes.set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes.set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Gradient Boosting Classifier')\naxes.legend(loc='lower right', fontsize=13)","57414f64":"from sklearn.ensemble import RandomForestClassifier as RFC\nrfc = RFC(random_state=42)\n\nrfc.fit(X_train,y_train)\n\nmodel_Evaluate(rfc)","a48c38ba":"pred_rfc = rfc.predict_proba(X_test)[:,1]\nfpr_rfc,tpr_rfc,_ = roc_curve(y_test.values,pred_rfc)\nroc_auc_rfc = auc(fpr_rfc,tpr_rfc)\n\n\nf, axes = plt.subplots(1, 1,figsize=(7,7))\naxes.plot(fpr_rfc, tpr_rfc, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_rfc))\naxes.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes.set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes.set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Random Forest Classifer')\naxes.legend(loc='lower right', fontsize=13)","7093fb15":"from sklearn.svm import SVC\nsvm=SVC()\n\n\nsvm.fit(X_train,y_train)\n\nmodel_Evaluate(svm)","53778fa0":"<a id=\"#\"><\/a>\n    \n<font size=\"+4\" color=\"indigo\"><center><b>Sinhala Hate Speech<\/b><\/center><\/font><br>\n<font size=-1 color=\"orange\"><center><b>*Series: All about NLP Text and Sentiment analysis  <\/b><\/right><\/font>","3e95f6c9":"<a id=\"#\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>MultiNomial NB<\/b><\/font><br>","1eed0ae4":"Okay, we can see now punctuations have been removed from the comments. As a second step, let's remove numbers and assign cleaned data to new column.\n","351d6310":"<a id=\"#\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>KNeighbors Clssifier<\/b><\/font><br>","9ee27547":"It's time to preprocess the data, we can see there are some unwanted data in the comments such as numbers, punctuations which do not add significant meaning to the data. Therefore let's write functions to remove numbers and punctuations.\n","2f8c1945":"<a id=\"#\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>SVC<\/b><\/font><br>","0a839d95":"Okay, I think that's the only cleaning we need. Therefore let's tokenize the comments. Let's check most common 10 words.\n","dc97a71a":"<a id=\"#\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>BERT<\/b><\/font><br>\n","464b9651":"<a id=\"#\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>Gradient Boosting Classifier<\/b><\/font><br>","3abf3ee0":"\n## <font size=+2 color=\"green\"><b>Table of Content:<\/b><\/font>\n\n* [1. Importing data & libraries](#1)\n* [2. Data Preprocessing](#2)\n* [3. Exploratory Data Analysis](#3)\n* [4. Applying Machine Learning Algorithms](#4)\n* [5. Comparing Algorithms Performance](#5)","c240a934":"Good news, dataset doesn't contain any missing values.","ced71dc1":"<a id='1'><\/a>\n# <p style=\"background-color:violet; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 10px 25px;\">Importing necessary modules and libraries\ud83d\udcda<\/p>","7d4042c5":"According to the histograms, cannot see significant different between length of hate and normal comments.","71bab7e6":"54% of comments are labelled as hate speeches and 45% of comments are labelled as non hate speeches. Okay now we have some idea about comments. let's calcualte length of comments.","3ea42efc":"<a id=\"#\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>Decision Tree Classifier<\/b><\/font><br>","5cd3c422":"Let's explore the dataset :)","06b6c840":"Let's see how data has been distributed.","e3481c81":"<a id=\"#\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>Random Forest Clssifier<\/b><\/font><br>","ae9bed40":"Let's, check missing values.","43a1bc9a":"Dataset contains 6345 rows and 3 columns.","544770eb":"<a id=\"#\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>AdaBoost Classifier<\/b><\/font><br>","95ad91b1":"\n<a id=\"#\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>Applying Machine Learning Algorithms<\/b><\/font><br>","7d584775":"posting hateful, abusive or insulting comments, most commonly known as Hate Speech has currently been a major issue in social media websites. Hence it would be great if there are datasets that can be used to predict these abusive comments on the internet. Even though there are several datasets available in the English language, currently there are no datasets based on the native language of Sri Lanka, which is Sinhala.\nTherefore this dataset will help to predict hateful, abusive or insulting comments which are posted on the social media platforms using Sinhala Unicode.\n\nContent\nThis dataset contains alphabetically ordered comments made using Sinhala Unicode on Facebook and a label which can be used to identify whether the comment is a hate speech or not.\n\nInspiration\nCan this dataset be used to predict hateful, abusive or insulting comments which are made in the Sinhala language?","fd6c67ed":"## <a id='2'><\/a>\n# <p style=\"background-color:blue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 10px 25px;\">Data Exploration && Data Wrangling\ud83d\udcda<\/p>","177edb11":"Let's see most common 20 words of both hate and normal comments.","f921c21d":"We don' need id colum therefore let's remove that.","ba4f2292":"Let's remove numbers first and assign cleaned data to new column.\n","5182f294":"<a id=\"#\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>Logistic Regression<\/b><\/font><br>"}}