{"cell_type":{"98c5bb98":"code","e71124c3":"code","727abd97":"code","5abab7c5":"code","a7ea5534":"code","0e9d9081":"code","67498d36":"code","b8b2e62d":"code","0ef107d0":"code","6c70fae6":"code","f3ccb5ab":"code","8c88a2a8":"code","be04f9c2":"code","1839c616":"code","83993767":"code","99d04abd":"code","e6fc3dd0":"code","8157f6fc":"code","dee4b0bc":"code","b6fb5345":"code","0cef6248":"markdown"},"source":{"98c5bb98":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom matplotlib import pyplot as plt \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e71124c3":"from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time","727abd97":"data = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\nprint (data.info())\n# print (data.head(10))","5abab7c5":"print (data[data['Class'] == 1])","a7ea5534":"g = data.corr()\nsns.heatmap(g)","0e9d9081":"count_0, count_1 = data.Class.value_counts()\n\ndata_class_0 = data[data.Class == 0] \ndata_class_1 = data[data.Class == 1]\n\ndata_sampled = data_class_0.sample(count_1)\ndata_sampled = pd.concat([data_sampled, data_class_1], axis=0)\ndata_sampled.Class.value_counts().plot(kind='bar', title='Count (Class)');\n","67498d36":"fig, ax = plt.subplots(figsize=(10,8))     \ng = data_sampled.corr()\nsns.heatmap(g, cmap = 'coolwarm_r', annot_kws={'size':60},ax = ax)","b8b2e62d":"from scipy.stats import norm\n\nnew_df = data_sampled\n\nf, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n\nv14_fraud_dist = new_df['V14'].loc[new_df['Class'] == 1].values\nsns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nv12_fraud_dist = new_df['V12'].loc[new_df['Class'] == 1].values\nsns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n\n\nv10_fraud_dist = new_df['V10'].loc[new_df['Class'] == 1].values\nsns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nplt.show()\n","0ef107d0":"from collections import Counter\n\ndef detect_outliers(df, features):\n    outlier_indices = []\n    \n    for col in features:\n        Q1 = np.percentile(df[col], 25)\n        \n        Q3 = np.percentile(df[col], 75)\n        \n        IQR = Q3 - Q1\n        outlier_step = 2.5 * IQR\n    \n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > 1 )\n    return multiple_outliers\n    \n    \noutliers_to_drop = detect_outliers(data_sampled, ['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13',\n                                              'V14','V15','V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26',\n                                              'V27','V28'])   \n","6c70fae6":"new_df = data_sampled.drop(outliers_to_drop, axis = 0).reset_index(drop=True)","f3ccb5ab":"from scipy.stats import norm\n\nf, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n\nv14_fraud_dist = new_df['V14'].loc[new_df['Class'] == 1].values\nsns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nv12_fraud_dist = new_df['V12'].loc[new_df['Class'] == 1].values\nsns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n\n\nv10_fraud_dist = new_df['V10'].loc[new_df['Class'] == 1].values\nsns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nplt.show()\n\n","8c88a2a8":"X = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n\n# T-SNE Implementation\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=3, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))\n\n# PCA Implementation\nt0 = time.time()\nX_reduced_pca = PCA(n_components=3, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"PCA took {:.2} s\".format(t1 - t0))\n\n# TruncatedSVD\nt0 = time.time()\nX_reduced_svd = TruncatedSVD(n_components=3, algorithm='randomized', random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"Truncated SVD took {:.2} s\".format(t1 - t0))","be04f9c2":"from sklearn.model_selection import train_test_split\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state= 2)","1839c616":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n\nclassifiers = {\n    \"LogisticRegression\": LogisticRegression(),\n    \"Support Vector Classifier\": SVC(),\n    \"KNN\": KNeighborsClassifier(),\n    \"DecisionTree\": DecisionTreeClassifier(),\n}","83993767":"from sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=10)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","99d04abd":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n#LogisiticRegression \nlogreg_par = {'C':[0.1, 0.3, 1, 3, 10, 30]}\n\nlogreg_grid = GridSearchCV(LogisticRegression(), logreg_par)\nlogreg_grid.fit(X_train, y_train)\nlogreg_bestfit = logreg_grid.best_estimator_\nprint (logreg_bestfit)\n\nsvc_par = {'gamma':[0.1, 0.3, 1, 3, 10, 30],'C':[0.1, 0.3, 1, 3, 10, 30]}\n\nsvc_grid = GridSearchCV(SVC(), svc_par)\nsvc_grid.fit(X_train, y_train)\nsvc_bestfit = svc_grid.best_estimator_\nprint (svc_bestfit)\n\nknn_par = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\nknn_grid = GridSearchCV(KNeighborsClassifier(), knn_par)\nknn_grid.fit(X_train, y_train)\nknn_bestfit = knn_grid.best_estimator_\nprint (knn_bestfit)\n\ntree_par = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ntree_grid =GridSearchCV(DecisionTreeClassifier(), tree_par)\ntree_grid.fit(X_train,y_train)\ntree_bestfit = tree_grid.best_estimator_\nprint (tree_bestfit)\n","e6fc3dd0":"classifiers = {\n    \"LogisticRegression\": logreg_bestfit,\n    \"Support Vector Classifier\": svc_bestfit,\n    \"KNN\": knn_bestfit,\n    \"DecisionTree\": tree_bestfit,\n}\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=10)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","8157f6fc":"from sklearn.model_selection import ShuffleSplit \nfrom sklearn.model_selection import learning_curve\ndef plot_learning_curve(estimator, X, y, ax,label, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5),):\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax.set_title( label+\" Curve\", fontsize=14)\n    ax.set_xlabel('Training size (m)')\n    ax.set_ylabel('Score')\n    ax.grid(True)\n    ax.legend(loc=\"best\")\n    ","dee4b0bc":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\ndef plot_curve(e1,e2,e3,e4):\n    f,((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    lst = [[e1,ax1],[e2,ax2],[e3,ax3],[e4,ax4]]\n    plot_learning_curve(e1, X_train, y_train, ax1,\"logistic regression\" ,(0.87, 1.01), cv=cv, n_jobs=10)\n    plot_learning_curve(e2, X_train, y_train, ax2,\"svc\" ,(0.87, 1.01), cv=cv, n_jobs=10)\n    plot_learning_curve(e3, X_train, y_train, ax3,\"knn\",(0.87, 1.01), cv=cv, n_jobs=10)\n    plot_learning_curve(e4, X_train, y_train, ax4,\"decision tree\",(0.87, 1.01), cv=cv, n_jobs=10)\nplot_curve(logreg_bestfit, svc_bestfit, knn_bestfit, tree_bestfit)","b6fb5345":"from sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score,recall_score, f1_score\n\n\nX = data.drop('Class', axis=1)\ny = data['Class']\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state= 2)\n\nfig = plt.figure(figsize=(12,6))\n\nlogreg_pred = logreg_bestfit.predict(X_train)\n\nprecision, recall, threshold = precision_recall_curve(y_train, logreg_pred)\n\npre = precision_score(y_train, logreg_pred)\nrec = recall_score(y_train, logreg_pred)\nscore = logreg_bestfit.score(X_test, y_test)\nf1 = f1_score(y_train, logreg_pred)\n\nplt.step(recall, precision, color='#004a93', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='#48a6ff')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n# plt.title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n#           undersample_average_precision), fontsize=16)\n\n\nprint (confusion_matrix(y_train, logreg_pred))\nprint (\"precsion:\",pre,\"recall\",rec)\nprint (\"f1 score:\",f1)\nprint (\"accuracy score\",score)","0cef6248":"## Resampling"}}