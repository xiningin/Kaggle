{"cell_type":{"1f8d37f6":"code","9cb15d8a":"code","9b65e6c5":"code","5c6c5aaa":"code","f50c1a14":"code","5c0c121a":"code","b4097e36":"code","a30b11a7":"code","29fc1854":"code","22258fd3":"code","6cdcca33":"code","3b96aef9":"code","f073a31f":"code","1047b792":"code","feeb58cf":"code","a5679ebb":"markdown","e9873d15":"markdown","3c17a459":"markdown","956b84b6":"markdown","75c9ec99":"markdown","37f71e30":"markdown","24580f8d":"markdown","b285c632":"markdown","867255a0":"markdown"},"source":{"1f8d37f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9cb15d8a":"json_1 = '..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json'\njson_2 = '..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json'","9b65e6c5":"import json\nimport os \nimport tensorflow as tf\nimport sklearn\nimport seaborn as sbs\nimport sklearn.naive_bayes \nimport sklearn.model_selection\nimport sklearn.metrics","5c6c5aaa":"import json\n\ndef load_json(jfile):\n    data = []\n    with open(jfile) as f:\n        for line in f.readlines():\n            j = json.loads(line)\n            url, headline, sarcastic = j['article_link'], j['headline'], j['is_sarcastic']\n            data.append([url, headline, sarcastic])\n    return pd.DataFrame(data, columns=['article_link', 'headline', 'is_sarcastic'])\n\nprint(\"\u2705\u2705\u2705 SESSION DONE\")","f50c1a14":"df = pd.concat([load_json(json_1), load_json(json_2)])\nsbs.distplot(df.headline.str.len())","5c0c121a":"vocab_size = 30000  # There are 30884 words in total\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(df.headline)\nprint(\"Vocabulary size\", len(tokenizer.word_index))\n\ntrain_inputs = tokenizer.texts_to_sequences(df.headline)\nsbs.distplot([len(l) for l in train_inputs])","b4097e36":"train_inputs = tf.keras.preprocessing.sequence.pad_sequences(train_inputs, padding='post', maxlen=20)\ntrain_labels = df['is_sarcastic']\n\n# Split data into train \/validation \nX_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(train_inputs, train_labels, \n                                                                          test_size=0.2, random_state=0)\ntrain_inputs[0]","a30b11a7":"max_len = 20\ntext_input = tf.keras.Input(shape=(max_len, ))\nembed_text = tf.keras.layers.Embedding(vocab_size, 128)(text_input)\n\nnet = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(embed_text)\nnet = tf.keras.layers.GlobalMaxPool1D()(net)\nnet = tf.keras.layers.Dense(64, activation='relu')(net)\nnet = tf.keras.layers.Dropout(0.4)(net)\nnet = tf.keras.layers.Dense(32, activation='relu')(net)\nnet = tf.keras.layers.Dropout(0.4)(net)\n\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(net)\nmodel = tf.keras.models.Model(text_input, output)\nmodel.summary()","29fc1854":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmc = tf.keras.callbacks.ModelCheckpoint('model_best.hdf5', monitor='val_accuracy', \n                                        verbose=1, save_best_only=True, mode='max')\nes = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=5)\n    \nhistory = model.fit(X_train, y_train,\n                    epochs=30, batch_size=256, callbacks=[mc, es], \n                    validation_split=0.1, verbose=1)","22258fd3":"model = tf.keras.models.load_model('model_best.hdf5')\ny_preds = model.predict(X_val, batch_size=1024).round().astype(int)\nprint(\"Test accuracy score\", sklearn.metrics.accuracy_score(y_val, y_preds))","6cdcca33":"import tensorflow_hub as hub\n\nhub_url = \"https:\/\/tfhub.dev\/google\/tf2-preview\/nnlm-en-dim128\/1\"\nhub_layer = hub.KerasLayer(hub_url, output_shape=[128], input_shape=[],\n                           dtype=tf.string, trainable=True)","3b96aef9":"# Split data into train \/validation \nX_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(df.headline.values, df['is_sarcastic'], \n                                                                          test_size=0.2, random_state=0)","f073a31f":"%%time \n# input = tf.keras.layers.Input(shape=(), name=\"Input\", dtype=tf.string)\nmodel = tf.keras.Sequential()\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.4))\nmodel.add(tf.keras.layers.Dense(32, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.4))\nmodel.add(tf.keras.layers.Dense(1))\n\nmodel.summary()\n","1047b792":"%%time\nmc = tf.keras.callbacks.ModelCheckpoint('model_best_embed.hdf5', monitor='val_accuracy', \n                                        verbose=1, save_best_only=True, mode='max')\nes = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=5)\n    \nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train,\n                    epochs=30, batch_size=256, callbacks=[mc, es],\n                    validation_split=0.1, verbose=1)\n","feeb58cf":"model.load_weights('model_best_embed.hdf5')\ny_preds = model.predict(X_val, batch_size=1024)\ny_preds = (y_preds >= 0.5).astype(int)\nprint(\"Test accracy score\", sklearn.metrics.accuracy_score(y_val, y_preds))","a5679ebb":"Results: \n1. using BERT embedding does NOT improve our result","e9873d15":"By far we have tried: \n1. Naive Bayes on `Dataset.json`, validation accuracy `0.58`\n2. LSTM on `Dataset.json`, validation accuracy `0.85548`\n3. LSTM + Word embedding on `Dataset.json`, validation accuracy `0.8627`\n4. BERT+ LSTM on `Dataset.json`, validation accuracy `0.92849`\n5. LSTM on `Dataset.json` + `Dataset_v2.json`, validation accuracy `0.95138`\n\nWe are confident that BERT + LSTM on the whole datasets will achieve a higher validation accuracy score (97% or even higher). We will test it later, but first we want to do experiments on BERT embedding, to see whether it helps improving the result. \n\nNote that, pretrained word vector model didn't improve much on the validation accuracy score, see points 2 and 3.","3c17a459":"We split the dataset into two parts: train \/ validation. Both part shoud be tokenized, which is automatically done with KerasLayer.","956b84b6":"Check on [this kernel](https:\/\/www.kaggle.com\/xiu0714\/detect-sarcasm-with-bert-92-accuracy?scriptVersionId=34532422) to find out how to achieve `0.924` accuracy with BERT and only one dataset. \n\nThis kernel will focus on using LSTM + word embedding. We will also use both dataset to see the limits of LSTM. \n","75c9ec99":"Load data and merge them. ","37f71e30":"Set the `max len` of vector to 20, since most headlines contain no more than 20 words","24580f8d":"# What to do next ?","b285c632":"Now let's find out how the best model works on our validation dataset. \n\nAnd ................. the result is pretty good: accuracy score is `0.9513`. Note that, in a [previous kernel](https:\/\/www.kaggle.com\/xiu0714\/detect-sarcasm-with-bert-92-accuracy?scriptVersionId=34527911), the same model trained on `Dataset.json` only achieved `accuracy_score = 0.85`. And now with an extra dataset, we've improved the result by 10%. \n\nData is the new oil, it is true. \n\n","867255a0":"Build model and train"}}