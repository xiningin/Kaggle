{"cell_type":{"77045fcf":"code","df9e21e5":"code","cb8e956d":"code","94a5ed6e":"code","2972826e":"code","39ae5588":"code","e92a3881":"code","99878dd2":"code","957304b7":"code","f0934ff8":"code","108c4f80":"code","756f13dd":"code","c99a04b7":"code","b7ef0e84":"markdown","811d35f1":"markdown","0cfab7ef":"markdown","0db4136b":"markdown","0d7cd2c7":"markdown","20aeddd2":"markdown","452ca42d":"markdown","62feafda":"markdown","712a7a4d":"markdown","b2fd77f0":"markdown"},"source":{"77045fcf":"from copy import copy\nimport gc\nimport joblib\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport sys\nfrom warnings import simplefilter\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input, backend as K\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Dropout\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.optimizers import Adam\nfrom transformers import TFBartModel, BartConfig, BartTokenizerFast\n\nsimplefilter('ignore')\nplt.style.use('fivethirtyeight')","df9e21e5":"# limit the GPU memory growth\ngpu = tf.config.list_physical_devices('GPU')\nprint(\"Num GPUs Available: \", len(gpu))\nif len(gpu) > 0:\n    tf.config.experimental.set_memory_growth(gpu[0], True)","cb8e956d":"model_name = 'tfbart_v7'\n\ndata_dir = Path('..\/input\/commonlitreadabilityprize')\ntrain_file = data_dir \/ 'train.csv'\ntest_file = data_dir \/ 'test.csv'\nsample_file = data_dir \/ 'sample_submission.csv'\n\nbuild_dir = Path('.\/build\/')\noutput_dir = build_dir \/ model_name\ntrn_encoded_file = output_dir \/ 'trn.enc.joblib'\nval_predict_file = output_dir \/ f'{model_name}.val.txt'\nsubmission_file = 'submission.csv'\n\npretrained_dir = Path('..\/input\/tfbart-base')\n\nid_col = 'id'\ntarget_col = 'target'\ntext_col = 'excerpt'\n\nmax_len = 205\nn_fold = 5\nn_est = 9\nn_stop = 2\nbatch_size = 8\nseed = 42","94a5ed6e":"output_dir.mkdir(parents=True, exist_ok=True)","2972826e":"trn = pd.read_csv(train_file, index_col=id_col)\ntst = pd.read_csv(test_file, index_col=id_col)\ny = trn[target_col].values\nprint(trn.shape, y.shape, tst.shape)\ntrn.head()","39ae5588":"# For local experiments, download the pretrained tokenizer and model if not available.\npretrained_dir.mkdir(exist_ok=True)\n\ndef load_tokenizer():\n    if not os.path.exists(pretrained_dir \/ 'vocab.json'):\n        print('downloading the pretrained tokenizer')\n        tokenizer = BartTokenizerFast.from_pretrained(\"facebook\/bart-base\")\n        tokenizer.save_pretrained(pretrained_dir)\n    else:\n        print('loading the saved pretrained tokenizer')\n        tokenizer = BartTokenizerFast.from_pretrained(str(pretrained_dir))\n        \n    config = BartConfig.from_pretrained(str(pretrained_dir))\n    config.output_hidden_states = True\n    return tokenizer, config\n\ndef load_bart(config):\n    if not os.path.exists(pretrained_dir \/ 'tf_model.h5'):\n        print('downloading the pretrained model')\n        model = TFBartModel.from_pretrained(\"facebook\/bart-base\", config=config)\n        model.save_pretrained(pretrained_dir)\n    else:\n        print('loading the saved pretrained model')\n        model = TFBartModel.from_pretrained(pretrained_dir, config=config)\n    return model","e92a3881":"def bart_encode(texts, tokenizer, max_len=max_len):\n    input_ids = []\n    attention_mask = []\n    \n    for text in texts:\n        token = tokenizer(text, max_length=max_len, truncation=True, padding='max_length',\n                         add_special_tokens=True)\n        input_ids.append(token['input_ids'])\n        attention_mask.append(token['attention_mask'])\n    \n    return np.array(input_ids), np.array(attention_mask)","99878dd2":"tokenizer, config = load_tokenizer()\n\nX = bart_encode(trn[text_col].values, tokenizer, max_len=max_len)\nX_tst = bart_encode(tst[text_col].values, tokenizer, max_len=max_len)\ny = trn[target_col].values\nprint(X[0].shape, X_tst[0].shape, y.shape)","957304b7":"joblib.dump(X, trn_encoded_file)","f0934ff8":"def build_model(bart_model, max_len=max_len):    \n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\n    sequence_output = bart_model(input_ids, attention_mask=attention_mask)[0]\n    clf_output = sequence_output[:, 0, :]\n    clf_output = Dropout(.1)(clf_output)\n    out = Dense(1, activation='linear')(clf_output)\n    \n    model = Model(inputs=[input_ids, attention_mask], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n    \n    return model","108c4f80":"def scheduler(epoch, lr, warmup=5, decay_start=10):\n    if epoch <= warmup:\n        return lr \/ (warmup - epoch + 1)\n    elif warmup < epoch <= decay_start:\n        return lr\n    else:\n        return lr * tf.math.exp(-.1)\n\nls = LearningRateScheduler(scheduler)\nes = EarlyStopping(patience=n_stop, restore_best_weights=True)\n\ncv = KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n\np = np.zeros_like(y, dtype=float)\np_tst = np.zeros((X_tst[0].shape[0], ), dtype=float)\nfor i, (i_trn, i_val) in enumerate(cv.split(X[0]), 1):\n    print(f'training CV #{i}:')\n    tf.random.set_seed(seed + i)\n    \n    bart_model = load_bart(config)\n    clf = build_model(bart_model, max_len=max_len)\n    if i == 1:\n        print(clf.summary())\n\n    history = clf.fit([x[i_trn] for x in X], y[i_trn],\n                      validation_data=([x[i_val] for x in X], y[i_val]),\n                      epochs=n_est,\n                      batch_size=batch_size,\n                      callbacks=[ls])\n    clf.save_weights(f'{model_name}_cv{i}.h5')\n    \n    p[i_val] = clf.predict([x[i_val] for x in X]).flatten()\n    p_tst += clf.predict(X_tst).flatten() \/ n_fold\n    \n    K.clear_session()\n    del bart_model, clf\n    gc.collect()","756f13dd":"print(f'CV RMSE: {mean_squared_error(y, p, squared=False):.6f}')\nnp.savetxt(val_predict_file, p, fmt='%.6f')","c99a04b7":"sub = pd.read_csv(sample_file, index_col=id_col)\nsub[target_col] = p_tst\nsub.to_csv(submission_file)\nsub.head()","b7ef0e84":"# Submission","811d35f1":"## Print CV RMSE and Save CV Predictions","0cfab7ef":"This notebook shows how to train a neural network model with pre-trained BART in Tensorflow\/Keras. It is based on @xhlulu's [Disaster NLP: Keras BERT using TFHub](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\n) notebook.\n\nThis competition is a code competition without access to internet. So we add the `transformers` tokenizer and pre-trained BART base model through Kaggle Datasets instead. You can find the `transformers` tokenizer and pre-trained models for TF\/Keras as follows:\n* [TF BERT Base Uncased Pretrained Model\/Tokenizer](https:\/\/www.kaggle.com\/jeongyoonlee\/tfbert-base-uncased)\n* [TF BERT Large Uncased Pretrained Model\/Tokenizer](https:\/\/www.kaggle.com\/jeongyoonlee\/tfbert-large-uncased)\n* [TF RoBERTa Base Model\/Tokenizer](https:\/\/www.kaggle.com\/jeongyoonlee\/tfroberta-base)\n* [TF RoBERTa Lare Model\/Tokenizer](https:\/\/www.kaggle.com\/jeongyoonlee\/tfroberta-large)\n* [TF BART Base Pretrained Model\/Tokenizer](https:\/\/www.kaggle.com\/jeongyoonlee\/tfbart-base)\n* [TF BART Large Pretrained Model\/Tokenizer](https:\/\/www.kaggle.com\/jeongyoonlee\/tfbart-large)\n\nPlease check out my other notebooks with the same code structure as follows:\n* [PyTorch Lightning RoBERTa (Training\/Inference)](https:\/\/www.kaggle.com\/jeongyoonlee\/pytorch-lightning-roberta-training-inference): PyTorch Lightning RoBERTa using `transformers`\n* [TF\/Keras BERT Baseline (Training\/Inference)](https:\/\/www.kaggle.com\/jeongyoonlee\/tf-keras-bert-baseline-training-inference): TF\/Keras BERT using TFHub (-v8) and `transformers` (v10+)\n\nHope it helps.","0db4136b":"# Model Training with Cross-Validation\n\nSimple model with only an output dense layer added to the pre-trained BERT model.","0d7cd2c7":"## Save Encoded Training Data","20aeddd2":"# Changelogs\n\n| Version  | CV Score | Public Score | Changes | Comment |\n|----------|----------|--------------|---------|---------|\n|  v7 | TBD     | TBD | initial baseline | |","452ca42d":"If you find it helpful, please upvote the notebook. Also check out my other notebooks below:\n\n* [TF\/Keras BERT Baseline (Training\/Inference)](https:\/\/www.kaggle.com\/jeongyoonlee\/tf-keras-bert-baseline-training-inference): shares the TF\/Keras BERT baseline with 5-fold CV\n* [PyTorch Lightning RoBERTa (Training\/Inference)](https:\/\/www.kaggle.com\/jeongyoonlee\/pytorch-lightning-roberta-training-inference\/notebook): shares the Pytorch Lightning baseline with 5-fold CV    \n* [All Zero Submission](https:\/\/www.kaggle.com\/jeongyoonlee\/all-zero-submission): shows the public LB score for all zero submission\n* [DAE with 2 Lines of Code with Kaggler](https:\/\/www.kaggle.com\/jeongyoonlee\/dae-with-2-lines-of-code-with-kaggler): shows how to generate Denoising AutoEncoder features using `Kaggler`\n\nHappy Kagglging~!","62feafda":"# Tokenization Using `transformers`","712a7a4d":"# Load Libraries and Data","b2fd77f0":"Training the model with early stopping and a learning-rate scheduler"}}