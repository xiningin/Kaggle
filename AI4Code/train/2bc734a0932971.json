{"cell_type":{"b084f2b0":"code","0c5567ae":"code","ad253736":"code","45dc8583":"code","d32095f0":"code","2f96c9a3":"code","92fc8695":"code","0753eb95":"code","b8d5203b":"code","c08c7abc":"code","c7546595":"code","04f28bb9":"code","f498418e":"code","c3b3bb79":"code","638e5e20":"code","697dd643":"code","99441f4f":"code","86a61e04":"code","9186c50c":"code","5495df00":"code","aa8c1d07":"code","9c8f85c9":"code","ce248dba":"code","a6183916":"code","b24864a4":"code","fb59b353":"code","8078f81d":"code","70c845a1":"code","c8a49d6b":"code","260bd964":"code","839a6f6f":"code","e399ff04":"code","950b3224":"code","93c08c22":"code","ee539efa":"code","f824fbe7":"code","50178f11":"code","e0ff28dd":"code","d1141d71":"code","9db0f7bb":"code","f8b969d0":"code","15579dd8":"code","52b3e979":"code","9875a002":"code","0b3f3542":"code","d65e05bc":"code","34d21a98":"code","83434059":"code","29b833e7":"code","8410bf8f":"code","f53e638a":"code","4b09ce13":"code","a28eef58":"code","4c388f6b":"code","a4a78a08":"code","1ecb7343":"code","a116cc24":"code","3e741550":"code","b7559673":"code","600aab7f":"code","ad31a4eb":"code","3f43d45f":"code","171884a3":"code","08e74b6b":"code","56ef04f2":"code","dbe3dcd8":"code","dceb3362":"code","6d1387a9":"code","d7f04304":"code","aa69d463":"code","748aca53":"code","e2fc69d5":"code","8759a51b":"code","9ee6fd0f":"code","805d135c":"code","4e88b326":"code","c8a05f9e":"code","525185e0":"code","15af4d83":"code","18683975":"code","2d2de510":"code","c6ff5456":"code","b7e1fb8b":"code","00c25e93":"code","63344c08":"code","e7db5dfc":"code","b8b1472d":"code","6a829281":"code","18a074ec":"code","ecf8cd7f":"markdown","9b756389":"markdown","2ca98979":"markdown","8407e080":"markdown","41256f65":"markdown","4c72f424":"markdown","3542e790":"markdown","1694333d":"markdown","62e7f566":"markdown","059c2019":"markdown","51926ff1":"markdown","65839bbc":"markdown","e0016c90":"markdown","e242c6a8":"markdown","8090ae0c":"markdown","af8c6bc6":"markdown","917cef82":"markdown","29a30d55":"markdown","46296bb1":"markdown","c962a48c":"markdown","5c0c92e1":"markdown","2d1bcafa":"markdown","7f25f759":"markdown","e3740876":"markdown","bbb18a52":"markdown","6367bbbc":"markdown","3147a785":"markdown","a3f23009":"markdown","a4fd4cc1":"markdown","dccc6c80":"markdown","770aca71":"markdown","16d152d7":"markdown","75d57a95":"markdown","7536ebc1":"markdown","7ce617c6":"markdown","09dd0dce":"markdown","6ce42e4c":"markdown","74520818":"markdown","bd4c6be1":"markdown","a5fe1a79":"markdown","b3e8822a":"markdown","f4252b6f":"markdown","c54f7de5":"markdown","4782e026":"markdown","139e3442":"markdown","73bd4950":"markdown","ce49df02":"markdown","d78675ec":"markdown","200c7eb0":"markdown","ce4793ed":"markdown","751e5ed6":"markdown","608aba8e":"markdown","7480b318":"markdown","ddc0beda":"markdown","b42b8600":"markdown","e5fd3847":"markdown","376e2381":"markdown","759d03b2":"markdown","adde05cc":"markdown","9eeabb9a":"markdown","00586dd0":"markdown","a4d892fd":"markdown","955b5e3c":"markdown","fcc258e6":"markdown"},"source":{"b084f2b0":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords \n\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom bs4 import BeautifulSoup\n\nimport string\nfrom string import digits\nfrom nltk.tokenize import word_tokenize, TreebankWordTokenizer\nfrom nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.preprocessing import LabelEncoder\n\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\n\n\nclass color:\n     BOLD = '\\033[1m'\n     UNDERLINE = '\\033[4m'\n     END = '\\033[0m'\n        \nimport warnings\nwarnings.filterwarnings(\"ignore\")","0c5567ae":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_train.head()","ad253736":"test_df = pd.read_csv('..\/input\/test.csv')\ntest_df.head()","45dc8583":"df_train['type'].unique()","d32095f0":"print (color.BOLD + \"Train data:\" + color.END)\nprint (\"Number of columns: \" + str (df_train.shape[1]))\nprint (\"Number of rows: \" + str (df_train.shape[0]))\n\nprint(color.BOLD + '\\nTest data: ' + color.END)\nprint (\"Number of columns:\" + str (test_df.shape[1]))\nprint (\"Number of rows:\" +  str (test_df.shape[0]))","2f96c9a3":"print('Datasets Total=',len(df_train) + len(test_df))","92fc8695":"print('Training set consist of', round(len(df_train)\/8675*100),'% data')\nprint('Testing set consist of', round(len(test_df)\/8675*100),'% data')","0753eb95":"print(df_train.info(), '\\n')\nprint(test_df.info())","b8d5203b":"df_train.posts.iloc[0][0:2000]","c08c7abc":"len(df_train.iloc[1,1].split('|||'))","c7546595":"train_val_count=df_train['type'].value_counts()\ntrain_val_count","04f28bb9":"plt.figure(figsize=(12,4))\n\nsns.barplot(train_val_count.index, train_val_count.values,palette= 'Accent_r', ec='black' )\n\n\nplt.ylabel('Number of occurrences per type', fontsize=10)\nplt.xlabel('Personality types', fontsize=10)\nplt.title('Total posts for each personality type')\nplt.show()","f498418e":"I_E= df_train['type'].map(lambda type: type[0]).value_counts()\nN_S= df_train['type'].map(lambda type: type[1]).value_counts()\nT_F= df_train['type'].map(lambda type: type[2]).value_counts()\nJ_P= df_train['type'].map(lambda type: type[3]).value_counts()","c3b3bb79":"print(color.BOLD  + 'Introversion (I) \u2013 Extroversion (E):' +color.END ,'\\n',I_E, '\\n')\nprint(color.BOLD  + 'Intuition (N) \u2013 Sensing (S):' +color.END, '\\n', N_S, '\\n')\nprint(color.BOLD  + 'Thinking (T) \u2013 Feeling (F):' +color.END, '\\n', T_F, '\\n')\nprint(color.BOLD  + 'Judging (J) \u2013 Perceiving (P):' +color.END,'\\n', J_P)","638e5e20":"print(color.BOLD +\"Introverts and Extroverts Percentages:\" + color.END)\nIntroversion_perc= 4998\/len(df_train)\nprint('Introversion_percentage is:',round(Introversion_perc *100),'%')\nExtroversion_perc= 1508\/len(df_train)\nprint('Extroversion_percentage is:',round(Extroversion_perc *100),'%','\\n')\n\nprint(color.BOLD +\"Intuition and Sensing Percentages:\" + color.END)\nIntuition_perc= 5612\/len(df_train)\nprint('Intuition_percentage is:',round(Intuition_perc *100),'%')\nSensing_perc= 894\/len(df_train)\nprint('Sensing_percentage is:',round(Sensing_perc *100),'%','\\n')\n\nprint(color.BOLD +\"Thinking and Feeling Percentages:\" + color.END)\nThinking_perc= 3518\/len(df_train)\nprint('Thinking_percentage is:',round(Thinking_perc *100),'%')\nFeeling_perc= 2988\/len(df_train)\nprint('Feeling_percentage is:',round(Feeling_perc *100),'%','\\n')\n\nprint(color.BOLD +\"Judging and Perceiving Percentages:\" + color.END)\nJudging_perc= 3932\/len(df_train)\nprint('Thinking_percentage is:',round(Judging_perc *100),'%')\nPerceiving_perc= 2574\/len(df_train)\nprint('Perceiving_percentage is:',round(Perceiving_perc *100),'%')","697dd643":"temp = {'Introverts':[77],\n        'Extroverts':[23],\n        'Intuition':[86],\n        'Sensing':[14],\n        'Thinking':[54],\n        'Feeling':[46],\n        'Judging':[60],\n        'Perceiving':[40]}\n\nresults = pd.DataFrame.from_dict(temp, orient='index', columns=['Percentages'])\nresults","99441f4f":"my_func= lambda x: float(x)\nresults['Percentages']=results['Percentages'].apply(my_func)","86a61e04":"results.plot(kind='bar',colormap='PuRd_r')\n\nplt.title('Total percentage posts for each personality',size=12)\nplt.xlabel('Personality types', size = 12)\nplt.ylabel('Number of posts available per type', size = 12)\nplt.show()","9186c50c":"p = df_train.copy()\nz = test_df.copy()","5495df00":"p['clean'] = p['posts'].apply(lambda x: ' '.join(x.split('|||')))\np.head()","aa8c1d07":"z['clean'] = z['posts'].apply(lambda x: ' '.join(x.split('|||')))\nz.head()","9c8f85c9":"p.shape,z.shape","ce248dba":"pattern_url = r'http[s]?:\/\/(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\nsubs_url = r'url-web'\np['clean'] = p['clean'].replace(to_replace = pattern_url, value = subs_url, regex = True)\np.head()","a6183916":"z['clean'] = z['clean'].replace(to_replace = pattern_url, value = subs_url, regex = True)\nz.tail()","b24864a4":"p['clean_2'] = p['clean'].str.lower()\np.head()","fb59b353":"p['clean_2'] = p['clean_2'].apply(lambda x : x.translate(str.maketrans(' ',' ',string.punctuation)))\np.head()","8078f81d":"p['clean_2'] = p['clean_2'].apply(lambda x : x.translate(str.maketrans(' ',' ',digits)))\np.head()","70c845a1":"p['clean_2'] = p['clean_2'].str.strip()\np.head()","c8a49d6b":"tokeniser = TreebankWordTokenizer()\n\np['tokens'] = p['clean_2'].apply(tokeniser.tokenize)\np.head()","260bd964":"stemmer = SnowballStemmer('english')\n\ndef mbti_stemmer(words, stemmer):\n    return [stemmer.stem(word) for word in words] ","839a6f6f":"p['stem'] = p['tokens'].apply(mbti_stemmer, args=(stemmer, ))\np.head()","e399ff04":"Stops = set(stopwords.words('english'))","950b3224":"p['no_stop'] = p['stem'].apply(lambda x: [word for word in list(x) if word not in Stops])\np.head()","93c08c22":"p['no_stop'] = p['no_stop'].apply(lambda x: ' '.join(x))\np.head()","ee539efa":"unique_type = list(p['type'].unique())\nencoder = LabelEncoder().fit(unique_type)\n\ncodes = []\n\nfor i in range(0, len(p)):\n    codes.append(p['type'][i])\ncoder = encoder.transform(codes)","f824fbe7":"list(p['type'].unique())","50178f11":"p['codes'] = coder\np.head()","e0ff28dd":"#Features\nX = p.clean \n\n#Labels\ny = p.codes","d1141d71":"X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=14)","9db0f7bb":"Vect = CountVectorizer(ngram_range=(1, 1), stop_words='english', lowercase = True, max_features = 5000)","f8b969d0":"LRmodel = LogisticRegression(class_weight=\"balanced\", C=0.005, penalty = \"l2\")\npipe = Pipeline([('vec', Vect), ('model', LRmodel)])","15579dd8":"pipe.fit(X_train, y_train)","52b3e979":"print(LRmodel.intercept_[0])\nprint(LRmodel.coef_)","9875a002":"y_pred_train= pipe.predict(X_train)","0b3f3542":"print('Accuracy: '+ str(metrics.accuracy_score(y_train, y_pred_train)))\nprint('Precision: '+ str( metrics.precision_score(y_train, y_pred_train, average='macro')))\nprint('Recall: '+ str(metrics.recall_score(y_train, y_pred_train, average='macro')))\nprint('F1_Score: '+ str( metrics.f1_score(y_train, y_pred_train, average='macro')))","d65e05bc":"y_pred_test = pipe.predict(X_test)","34d21a98":"print('Accuracy: '+ str(metrics.accuracy_score(y_test, y_pred_test)))\nprint('Precision: '+ str( metrics.precision_score(y_test, y_pred_test, average='macro')))\nprint('Recall: '+ str(metrics.recall_score(y_test, y_pred_test, average='macro')))\nprint('F1_Score: '+ str( metrics.f1_score(y_test, y_pred_test, average='macro')))","83434059":"cm_logistic_reg = np.array(metrics.confusion_matrix(y_test, y_pred_test))\ncm_logistic = pd.DataFrame(cm_logistic_reg, index=['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP',\n                                       'ESTJ', 'ESTP', 'INFJ', 'INFP', 'INTJ', 'INTP',\n                                       'ISFJ', 'ISFP', 'ISTJ', 'ISTP'], \n                            columns=['predict_ENFJ','predict_ENFP','predict_ENTJ',\n                                     'predict_ENTP','predict_ESFJ','predict_ESFP',\n                                     'predict_ESTJ','predict_ESTP','predict_INFJ',\n                                     'predict_INFP','predict_INTJ','predict_INTP',\n                                     'predict_ISFJ','predict_ISFP','predict_ISTJ',\n                                     'predict_ISTP'])\ncm_logistic","29b833e7":"fig, ax = plt.subplots(figsize=(10,8)) \nsns.heatmap(cm_logistic, robust=True, annot=True, linewidth=0.3, \n            fmt='', cmap='BrBG', vmax=303, ax=ax)\nplt.title('Confusion Matrix for Logistic Classifier', fontsize=10,\n          fontweight='bold', y=1.00)\n\nplt.xticks(fontsize=10)\nplt.yticks(rotation=0, fontsize=10);","8410bf8f":"print(metrics.classification_report(y_test, y_pred_test, target_names=unique_type))","f53e638a":"test_preds = pipe.predict(z['clean'])","4b09ce13":"test_preds","a28eef58":"z['type'] = encoder.inverse_transform(test_preds)\nz.head()","4c388f6b":"z['E or I'] = z.apply(lambda x: x['type'][0], axis = 1)\nz['N or S'] = z.apply(lambda x: x['type'][1], axis = 1)\nz['T or F'] = z.apply(lambda x: x['type'][2], axis = 1)\nz['J or P'] = z.apply(lambda x: x['type'][3], axis = 1)\n\nmind = z['E or I'].astype(str).apply(lambda x: x[0] == 'E').astype('int')\nenergy = z['N or S'].astype(str).apply(lambda x: x[0] == 'N').astype('int')\nnature = z['T or F'].astype(str).apply(lambda x: x[0] == 'T').astype('int')\ntactics = z['J or P'].astype(str).apply(lambda x: x[0] == 'J').astype('int')\n\nz.head()","a4a78a08":"df_LogReg = pd.DataFrame({\"id\":test_df['id'], \"mind\":mind, \"energy\":energy, \"nature\":nature, 'tactics':tactics})\ndf_LogReg.head()","1ecb7343":"df_LogReg.shape","a116cc24":"df_LogReg.to_csv('EDSA_Team_8_Classification1.csv', index = False)","3e741550":"LRmodel2 = LogisticRegression(class_weight=\"balanced\", C=0.004, penalty = \"l2\")\npipe2 = Pipeline([('vec', Vect), ('model', LRmodel2)])","b7559673":"pipe2.fit(X_train, y_train)","600aab7f":"y_pred_train2= pipe2.predict(X_train)","ad31a4eb":"print('Accuracy: '+ str(metrics.accuracy_score(y_train, y_pred_train2)))\nprint('Precision: '+ str( metrics.precision_score(y_train, y_pred_train2, average='macro')))\nprint('Recall: '+ str(metrics.recall_score(y_train, y_pred_train2, average='macro')))\nprint('F1_Score: '+ str( metrics.f1_score(y_train, y_pred_train2, average='macro')))","3f43d45f":"y_pred_test2 = pipe2.predict(X_test)","171884a3":"print('Accuracy: '+ str(metrics.accuracy_score(y_test, y_pred_test2)))\nprint('Precision: '+ str( metrics.precision_score(y_test, y_pred_test2, average='macro')))\nprint('Recall: '+ str(metrics.recall_score(y_test, y_pred_test2, average='macro')))\nprint('F1_Score: '+ str( metrics.f1_score(y_test, y_pred_test2, average='macro')))","08e74b6b":"con_matrix_test2= metrics.confusion_matrix(y_test, y_pred_test2)\n\ncm_logistic_df = pd.DataFrame(con_matrix_test2, index=['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP',\n                                       'ESTJ', 'ESTP', 'INFJ', 'INFP', 'INTJ', 'INTP',\n                                       'ISFJ', 'ISFP', 'ISTJ', 'ISTP'], \n                            columns=['predict_ENFJ','predict_ENFP','predict_ENTJ',\n                                     'predict_ENTP','predict_ESFJ','predict_ESFP',\n                                     'predict_ESTJ','predict_ESTP','predict_INFJ',\n                                     'predict_INFP','predict_INTJ','predict_INTP',\n                                     'predict_ISFJ','predict_ISFP','predict_ISTJ',\n                                     'predict_ISTP'])\n","56ef04f2":"fig, ax = plt.subplots(figsize=(10,8)) \nsns.heatmap(cm_logistic_df, robust=True, annot=True, linewidth=0.3, \n            fmt='', cmap='BrBG', vmax=303, ax=ax)\nplt.title('Confusion Matrix for Logistic Classifier for the test dataset', fontsize=10,\n          fontweight='bold', y=1.00)\n\nplt.xticks(fontsize=10)\nplt.yticks(rotation=0, fontsize=10)\nplt.show()","dbe3dcd8":"print(metrics.classification_report(y_test, y_pred_test2, target_names=unique_type))","dceb3362":"test_preds2 = pipe2.predict(z['clean'])","6d1387a9":"test_preds2","d7f04304":"z['type'] = encoder.inverse_transform(test_preds2)\nz.head()","aa69d463":"z['E or I'] = z.apply(lambda x: x['type'][0], axis = 1)\nz['N or S'] = z.apply(lambda x: x['type'][1], axis = 1)\nz['T or F'] = z.apply(lambda x: x['type'][2], axis = 1)\nz['J or P'] = z.apply(lambda x: x['type'][3], axis = 1)\n\nmind = z['E or I'].astype(str).apply(lambda x: x[0] == 'E').astype('int')\nenergy = z['N or S'].astype(str).apply(lambda x: x[0] == 'N').astype('int')\nnature = z['T or F'].astype(str).apply(lambda x: x[0] == 'T').astype('int')\ntactics = z['J or P'].astype(str).apply(lambda x: x[0] == 'J').astype('int')\n\nz.head()\n","748aca53":"df_LogReg2 = pd.DataFrame({\"id\":test_df['id'], \"mind\":mind, \"energy\":energy, \"nature\":nature, 'tactics':tactics})\ndf_LogReg2.head()","e2fc69d5":"df_LogReg2.shape","8759a51b":"df_LogReg2.to_csv('EDSA_Team_8_Classification2.csv', index = False)","9ee6fd0f":"# Fit and score a Random Forest Classifier on SMOTEd data using the parameters identified by the grid search\nrandom= RandomForestClassifier(min_samples_leaf=2, min_samples_split=3, n_estimators=79, \n                             criterion='entropy', bootstrap='False', n_jobs= -1, random_state=123)\nrandom_pipe= Pipeline([('vec', Vect), ('model', random)])","805d135c":"random_pipe.fit(X_train, y_train)","4e88b326":"y_pred_random=random_pipe.predict(X_train)","c8a05f9e":"print('Accuracy: '+ str(metrics.accuracy_score(y_train, y_pred_random)))\nprint('Precision: '+ str( metrics.precision_score(y_train, y_pred_random, average='macro')))\nprint('Recall: '+ str(metrics.recall_score(y_train, y_pred_random, average='macro')))\nprint('F1_Score: '+ str( metrics.f1_score(y_train, y_pred_random, average='macro')))","525185e0":"print(\"Classification Report:\")\nprint(metrics.classification_report(y_train, y_pred_random, target_names=unique_type))","15af4d83":"y_pred_random_test = random_pipe.predict(X_test)","18683975":"print('Accuracy: '+ str(metrics.accuracy_score(y_test, y_pred_random_test)))\nprint('Precision: '+ str( metrics.precision_score(y_test, y_pred_random_test, average='macro')))\nprint('Recall: '+ str(metrics.recall_score(y_test, y_pred_random_test, average='macro')))\nprint('F1_Score: '+ str( metrics.f1_score(y_test, y_pred_random_test, average='macro')))","2d2de510":"con_matrix_random= metrics.confusion_matrix(y_test, y_pred_random_test)\n\ncm_logistic_rf = pd.DataFrame(con_matrix_random, index=['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP',\n                                       'ESTJ', 'ESTP', 'INFJ', 'INFP', 'INTJ', 'INTP',\n                                       'ISFJ', 'ISFP', 'ISTJ', 'ISTP'], \n                            columns=['predict_ENFJ','predict_ENFP','predict_ENTJ',\n                                     'predict_ENTP','predict_ESFJ','predict_ESFP',\n                                     'predict_ESTJ','predict_ESTP','predict_INFJ',\n                                     'predict_INFP','predict_INTJ','predict_INTP',\n                                     'predict_ISFJ','predict_ISFP','predict_ISTJ',\n                                     'predict_ISTP'])\n\nfig, ax = plt.subplots(figsize=(10,8)) \nsns.heatmap(cm_logistic_rf, robust=True, annot=True, linewidth=0.3, \n            fmt='', cmap='BrBG', vmax=303, ax=ax)\nplt.title('Confusion Matrix for Random Forest Classifier ', fontsize=10,\n          fontweight='bold', y=1.00)\n\nplt.xticks(fontsize=10)\nplt.yticks(rotation=0, fontsize=10)\nplt.show()","c6ff5456":"print(\"Classification Report:\")\nprint(metrics.classification_report(y_test, y_pred_random_test, target_names=unique_type))","b7e1fb8b":"test_preds_random = random_pipe.predict(z['clean'])","00c25e93":"z['type'] = encoder.inverse_transform(test_preds_random)\nz.head()","63344c08":"z['E or I'] = z.apply(lambda x: x['type'][0], axis = 1)\nz['N or S'] = z.apply(lambda x: x['type'][1], axis = 1)\nz['T or F'] = z.apply(lambda x: x['type'][2], axis = 1)\nz['J or P'] = z.apply(lambda x: x['type'][3], axis = 1)\n\nmind = z['E or I'].astype(str).apply(lambda x: x[0] == 'E').astype('int')\nenergy = z['N or S'].astype(str).apply(lambda x: x[0] == 'N').astype('int')\nnature = z['T or F'].astype(str).apply(lambda x: x[0] == 'T').astype('int')\ntactics = z['J or P'].astype(str).apply(lambda x: x[0] == 'J').astype('int')\n\nz.head()","e7db5dfc":"df_random = pd.DataFrame({\"id\":test_df['id'], \"mind\":mind, \"energy\":energy, \"nature\":nature, 'tactics':tactics})\ndf_random.head()","b8b1472d":"df_random.shape","6a829281":"df_random.to_csv('EDSA_Team_8_Classification_random.csv', index = False)","18a074ec":"D = {'1':12.52206, '2': 21.72241, '3':19.58775,'4':13.27084,\n    '5': 7.04959, '6':6.81062 ,'7':6.76286, '8': 6.66727,\n    '9':6.52388, '10': 5.79104, '11':5.63173 ,'12':5.23311, '13': 4.93871,\n     '14':4.93074, '15': 4.91481, '16':4.88295 ,'17':4.80329,\n    '18':9.85352, '19': 4.82719, '20':4.84312 ,'21':7.54346}\n\nsubmission = list(D.keys())           \nkaggle_score = list(D.values())        \nplt.plot(submission, kaggle_score)\nplt.show()","ecf8cd7f":"# Test dataset predictions- For Kaggle","9b756389":"# 3. Data Preprocessing\n\nWe will start by pre-processing the data so that we can run it through the algorithm. This involves:\n* Splitting the data into features and labels\n* Cleaning Data\n* Transforming the categorical features \n* Splitting the data into training and testing data","2ca98979":"### 3.5. Remove numeric values\n\nBelow we are going to remove the numerical values","8407e080":"#### Test Model on Test_data_df","41256f65":"### 3.6. Remove whitespace","4c72f424":"**From the 1 row view we see that:**\n - The comments\/posts are separated by three pipes (|||)\n - We have URL's (to vids and pics)\n - Some of the MBTI types are mentioned","3542e790":"We have 50 posts by 6506 people along with their 4 letter MBTI code. ","1694333d":"**We got a kaggle score of 4.84312 for this model**","62e7f566":"# Table of content\n1. Importing the libraries\n2. Importing the datasets\n3. Data Preprocessing\n4. Building the models\n5. Results- Kaggle scores\n6. Conclusion\n7. Reference\n","059c2019":"**Here we are viewing 1 row from the 6506 rows of the train dataset to see how the data is structured**","51926ff1":"Below we are going to fit out test dataset(z) in our model.","65839bbc":"### 3.7. Tokenize\nBelow we are going to tokenise the posts.\n\nAnd tokenisation is the process of breaking strings into tokens.","e0016c90":"# **MBTI personality profile prediction**\n\n## EDSA Team 8 Classification Sprint","e242c6a8":"This model only also performed very well on the train set. The accuracy score is 0.85 on the train set and on the classification report for the test set we got a weighted avg of 67% for precision, recall and f1_score, meaning that the recall and f1_score increased by 1% and that is good.","8090ae0c":"#### Counter Vectorizer\n\nAs we know, computers understand numbers and in this case our dataset is made up of texts. Therefore,here we are going to use CountVectorizer which is one of the most basic ways we can represent text data numerically.\n\nCounterVectorizer simply creates vectors that have a dimensionality equivalent to the size of our vocabulary, and if the text data includes that vocabulary word, a 1 will be placed in that dimension. Each time that word is encountered again, the count will be increased, leaving 0s everywhere we didn't encounter the word even once.\n","af8c6bc6":"# 2. Importing the datasets","917cef82":"### Training the Model","29a30d55":"#### Train Test split","46296bb1":"### Test predictions","c962a48c":"### Training the Model\n\nBelow we are going to set out C parameter to 0.004","5c0c92e1":"### Test predictions","2d1bcafa":"Since the dataset comes from an Internet forum where individuals communicate strictly via written text,cleaning of the data was necessary.\n\nPeople write texts in different ways,some write in a certain way to emphasize their points and some use slang to get their points accross. Therefore, what did above is,we started off by splitting the posts,then we simply lowered the datasets to have a our datasets in the same format. We removed any punctuations and numeric values in the datasets as well.Moreover, We noticed that we had few occurrences where by the posts contained links to websites, so we replaced the links with just a string of url-web. And finally, since we wanted every word in the data to be as meaningful as possible, we removed the so-called \u201dstop words\u201d from the text this are words such as (a,or,the,and,is,are etc) using python\u2019s NLTK,as this words are referred to as useless data, they dont add any value but just create noise in the data.\n\nNext,broke strings into tokens by using TreebankWordTokenizer object, then we used nltk.stem.SnowballStemmer to stem the text. Stemming refers to normalizing the word into its base form or root form.Therefore, a stemming algorithm works by cutting off the end or the beginning of the word, taking into account the common of prefixes ,suffixes that can be found in an inflected word.\n\nWe did all of that to ensure when we build our model it will generalize to the English language and there is no noise in the dataset to avoid overfitting and inaccurate results.\n","7f25f759":"___","e3740876":"### 3.9. Removing Stop Words","bbb18a52":"### Predictions-Testing the Model","6367bbbc":"## MBTI types and descriptions\nHere's a brief description of the type column in our dataset.\n\nMBTI type|Type description\n---|---\n1. ISTJ|Quiet, serious, earn success by thoroughness and dependability. Practical, matter-of-fact, realistic, and responsible. Decide logically what should be done and work toward it steadily, regardless of distractions. Take pleasure in making everything orderly and organized - their work, their home, their life. Value traditions and loyalty.\n2. ISFJ|Quiet, friendly, responsible, and conscientious. Committed and steady in meeting their obligations. Thorough, painstaking, and accurate. Loyal, considerate, notice and remember specifics about people who are important to them, concerned with how others feel. Strive to create an orderly and harmonious environment at work and at home.\n3. INFJ|Seek meaning and connection in ideas, relationships, and material possessions. Want to understand what motivates people and are insightful about others. Conscientious and committed to their firm values. Develop a clear vision about how best to serve the common good. Organized and decisive in implementing their vision. \n4. INTJ|Have original minds and great drive for implementing their ideas and achieving their goals. Quickly see patterns in external events and develop long-range explanatory perspectives. When committed, organize a job and carry it through. Skeptical and independent, have high standards of competence and performance - for themselves and others.\n5. ISTP|Tolerant and flexible, quiet observers until a problem appears, then act quickly to find workable solutions. Analyze what makes things work and readily get through large amounts of data to isolate the core of practical problems. Interested in cause and effect, organize facts using logical principles, value efficiency. \n6. ISFP|Quiet, friendly, sensitive, and kind. Enjoy the present moment, what's going on around them. Like to have their own space and to work within their own time frame. Loyal and committed to their values and to people who are important to them. Dislike disagreements and conflicts, do not force their opinions or values on others. \n7. INFP|Idealistic, loyal to their values and to people who are important to them. Want an external life that is congruent with their values. Curious, quick to see possibilities, can be catalysts for implementing ideas. Seek to understand people and to help them fulfill their potential. Adaptable, flexible, and accepting unless a value is threatened.\n8. INTP|Seek to develop logical explanations for everything that interests them. Theoretical and abstract, interested more in ideas than in social interaction. Quiet, contained, flexible, and adaptable. Have unusual ability to focus in depth to solve problems in their area of interest. Skeptical, sometimes critical, always analytical.\n9. ESTP|Flexible and tolerant, they take a pragmatic approach focused on immediate results. Theories and conceptual explanations bore them - they want to act energetically to solve the problem. Focus on the here-and-now, spontaneous, enjoy each moment that they can be active with others. Enjoy material comforts and style. Learn best through doing. \n10. ESFP|Outgoing, friendly, and accepting. Exuberant lovers of life, people, and material comforts. Enjoy working with others to make things happen. Bring common sense and a realistic approach to their work, and make work fun. Flexible and spontaneous, adapt readily to new people and environments. Learn best by trying a new skill with other people.\n11. ENFP|Warmly enthusiastic and imaginative. See life as full of possibilities. Make connections between events and information very quickly, and confidently proceed based on the patterns they see. Want a lot of affirmation from others, and readily give appreciation and support. Spontaneous and flexible, often rely on their ability to improvise and their verbal fluency.\n12. ENTP|Quick, ingenious, stimulating, alert, and outspoken. Resourceful in solving new and challenging problems. Adept at generating conceptual possibilities and then analyzing them strategically. Good at reading other people. Bored by routine, will seldom do the same thing the same way, apt to turn to one new interest after another.\n13. ESTJ|Practical, realistic, matter-of-fact. Decisive, quickly move to implement decisions. Organize projects and people to get things done, focus on getting results in the most efficient way possible. Take care of routine details. Have a clear set of logical standards, systematically follow them and want others to also. Forceful in implementing their plans.\n14. ESFJ|Warmhearted, conscientious, and cooperative. Want harmony in their environment, work with determination to establish it. Like to work with others to complete tasks accurately and on time. Loyal, follow through even in small matters. Notice what others need in their day-by-day lives and try to provide it. Want to be appreciated for who they are and for what they contribute.\n15. ENFJ|Warm, empathetic, responsive, and responsible. Highly attuned to the emotions, needs, and motivations of others. Find potential in everyone, want to help others fulfill their potential. May act as catalysts for individual and group growth. Loyal, responsive to praise and criticism. Sociable, facilitate others in a group, and provide inspiring leadership.\n16. ENTJ|Frank, decisive, assume leadership readily. Quickly see illogical and inefficient procedures and policies, develop and implement comprehensive systems to solve organizational problems. Enjoy long-term planning and goal setting. Usually well informed, well read, enjoy expanding their knowledge and passing it on to others. Forceful in presenting their ideas.\n","3147a785":"# Team Members:","a3f23009":"#### The values counts for each axis of the types","a4fd4cc1":"# 6. Conclusion","dccc6c80":"**We got a kaggle score of 7.54346 for this model**","770aca71":"1. Nqobile Shabangu\n2. Leshaan Padayachy\n3. Vusimuzi Mhlanga\n4. Rethabile Mphahlele","16d152d7":"### 3.2. Remove URL\n\nHere we are going to replace the websites with just 'url-web'.","75d57a95":"In the train dataset there are `6506 rows` and `2 columns` and the column names are `'type' and 'posts'` and both of them are 'object' datatypes.\n\nIn the test datase there are `2169 rows` and `2 columns` as well and the column names are `'id'` which is an integer datatype and `'posts'` which is an 'object' datatype.\n\nAnd we do not have null values both in the train and test dataset.\n","7536ebc1":"# Random Classification","7ce617c6":"---","09dd0dce":"Who are you?. Or what kind of a person are you? This are some of the questions we get asked everytime by people who what to know or understand who we are. Most likely an answer to this kind of questions would include your \nname, hobbies or passions, your job title, birthplace ,a description of your beliefs and values etc.\n\nEvery one of us has a different answer to this question, and each answer tells a story about who we are. While we may have a lot in common with our fellow humans, like race, religion, sexual orientation, skills, and eye color, there is one thing that makes us each unique: personality.\n\nSo what is personality and MBTI?\nWell accoring to britannica.com personality is a characteristic way of thinking, feeling, and behaving. Personality embraces moods, attitudes, and opinions and is most clearly expressed in interactions with other people. It includes behavioral characteristics, both inherent and acquired, that distinguish one person from another and that can be observed in people\u2019s relations to the environment and to the social group.\n\nWith personality having such a large impact on our lives, it\u2019s important to have a reliable way to conceptualize and measure it.\n\nHave you at any point heard somebody portray themselves as an INTJ or an ESTP and pondered what those enigmatic sounding letters could mean? What these individuals are referring to is their personality type based on the Myers-Briggs Type Indicator (MBTI).\n\nThe Myers-Briggs Personality Type Indicator is a self-report inventory designed to identify a person's personality type, strengths, and preferences. The questionnaire was developed by Isabel Myers and her mother Katherine Briggs based on their work with Carl Jung's theory of personality types.\n\nThe questionnaire itself is made up of four different scales:\n- Extroversion (E) - Introversion (I)\n- Sensing (S) - Intuition (N)\n- Thinking (T) - Feeling (F)\n- Judging (J) - Perceiving (P)\n\nAnd based on the answers to the questions on the questionnaire, people are identified as having one of 16 personality types. Therefore, each person will have only one of the two categories for each variable above. Combining the four variables gives the final personality type of an individual, e.g, a person who an Introvert, Intuitive, Feeling and Judging will be identified as having the INFJ personality type.\n\nTherefore, in this challenge, we are required to build and train a model (or many models) capable of predicting a person's MBTI label using only what they post in online forums. We will use Natural Language Processing to convert the data into machine learning format and then finally use the this data to train a classifier capable of assigning MBTI labels to a person's online forum posts.\n\nAnd when submitting our predictions, we will be scored on mean column-wise log loss across each of the four categories.\n\nLastly,the datasets that we will be using is taken from Kaggle, and here is the link: https:\/\/www.kaggle.com\/c\/edsa-mbti\/data\n","6ce42e4c":"# Logistic Regression\n\nSo what is logistic regression classifier:\nLogistic regression classifiers can be used to predict an output that can be one of two classes such as pass or fail,win or lose ,healthy or sick, etc and in this case we are going to use to classify the personality types.\n\nLogistic regression algorithms work by implementing a linear equation first with independent predictors to predictive value and then this value is then converted into probability that could range from zero to one.\n\nLogistic regression really is the go to machine learning algorithms when we are working with two or more classes.\n\nBelow we are going to use the Logistic regression classifier with a class_weight of 'balanced' to balance the data as we have said that it is not normally distributed.\n\nPipeline class will also be used to enforce desired order of application steps which in turn will help in reproducibility and creating a convenient work-flow. \n","74520818":"# Introduction","bd4c6be1":"**We got a kaggle score of 4.82719 for this model**\n\nThis means that our model actually predicted the types very well, but we gonna try to improve the score by tuning the C parameter in our Logistic Regression classifier and we are also going to use Random Forest classifier and we shall see if it performs better or worse.\n\n\n","a5fe1a79":"# 1. Importing the libraries","b3e8822a":"### 3.1. Split at |||\n\nBelow we are going to split the post by spaces:' '","f4252b6f":"**Above we just copied the datasets into new variables names;**\n- p - is the train dataset\n- z - and z is the test dataset","c54f7de5":"**To our supervisor Mr Jaco, we appreaciate your guidance and time.**","4782e026":"We can see that the data is not normally distributed. And since our dataset is severely disproportional, it is clear to us that some cleaning of the proportional representation of each MBTI type would be necessary to avoid any misinterpretation of results due to skewed representation of classes in the dataset.","139e3442":"`Random Forest Classifier` is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n\nBelow we are going to build models using the random forest classifier.","73bd4950":"Above plot shows all our kaggle scores. We start off with a score of `12.52206`. And we were happy to see that we are on the right track but but the second submission on kaggle our score went to `21.72241` which was bad. We went back and continued with cleaning the data and using different parameters and classifiers and the more we did that the more our score improved and our final score was `4.82719` using the Logistic Regression model.\nAnd what we did to get that score is we used the penalty of L2 which is ridge, we set C parameter to 0.005 and we balanced our dataset using class_weight.","ce49df02":"### Visualisation\n\nBelow we are going to graph the above train_val count for better visualization.\n","d78675ec":"#### Creating encodes:\n\nBelow we are going to start off by creating encodes for the MBTI types in our datasets.","200c7eb0":"As we can see from the above heatmap INFJ,INFP,INTJ,INTP were the most types that were accurately predicted.\nAnd the classification report show that we have a weighted avg of 67% for presicion ,66% for recall and f1_score which shows the accuracy of the model.\n\n**Here is a brief guide to interpreting the Classification Report:**\n- Precision = True Positives \/ (True Positives + False Positives)\n    - A precision score of 1 indicates that the classifier never mistakenly added observations from another class. A precision score of 0 would mean that the classifier misclassified every instance of the current class.\n- Recall = True Positives \/ (True Positives + False Negatives)\n    - A recall score of 1 indicates that the classifier correctly predicted (found) all observations of the current class (by implication, no false negatives, or misclassifications of the current class). A recall score of 0 alternatively means that the classifier missed all observations of the current class.\n- F1-Score = 2 x (Precision x Recall) \/ (Precision + Recall)\n    - The f1-score's best value is 1 and worst value is 0, like the precision and recall scores. It is a useful metric for taking into account both measures at once.\n- Support is simply the number of observations of the labelled class.","ce4793ed":"### 3.3. LowerCase\n\nHere we are going to lower case the posts.","751e5ed6":"Below we are going to be exploring train and test datasets.","608aba8e":"### 3.4. Remove punctuation\n\nBelow we are going to remove all the punctuations in the posts.","7480b318":"The above confusion matrix is not clear, we will visualise a heatmap below","ddc0beda":"# Test dataset predictions- For Kaggle","b42b8600":"####  Features and labels:\nThen here we are going to set our x and y variables accordingly.","e5fd3847":"# 4. Building Models","376e2381":"Using min_samples_leaf=2, min_samples_split=3, n_estimators=79, criterion='entropy', bootstrap='False', n_jobs= -1, random_state=123) as our parameter setting for the random forest classifier,this gives us a training accuracy of 0.99 and testing accuracy of 0.46. Since the accuracy score for the training set is very high and for the testing set is very low, meaning that the model can not perform very well on unseen data, suggesting that the model overfitted.\n\n\n\n","759d03b2":"# Improving the Model\n# LogisticRegression","adde05cc":"# 5. Results","9eeabb9a":"### 3.8. Stemming\n","00586dd0":"# Test dataset predictions- For Kaggle","a4d892fd":"# 7. Reference\n\nhttps:\/\/www.myersbriggs.org\/my-mbti-personality-type\/mbti-basics\/the-16-mbti-types.htm\nhttps:\/\/www.britannica.com\/topic\/personality\nhttps:\/\/towardsdatascience.com\/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e\n","955b5e3c":"We have created an instance of the `LogisticRegression()` object using the class_weight, C, penalty  parameters,then we used the `fit()` method to train the model and finally we printed the `intercept` and `coefficients` of the model. The acccuracy of the model is very good with an accuracy score of `0.86`.\n\nBelow we will run predictions for the X_test and y_test and will see how the model performs on unseen data.","fcc258e6":"In conclusion, we started off by cleaning the datasets to ensure that it is in the right format by removing punctuations, removing stopwords ,numeric values and lower casing the posts.\n\nThen we used the countervectorizer ensure that the posts are in a matrix format.\n\nAnd finally we built our models and some of them performed very bad ,some of them performed better and the best one that performed very well at the end after using the right parameters. The best one that we got was 4.82719 using the Logistic Regression classifier.Meaning that our model actually performed very well on unseen data.\n\nAnd we have also seen that Random Forest doesnt perform better than Logistic regression at the end.\nMaybe if we used find the right parameter tuning and other models such as Decision Tree the score would have improved or not.\n\n\n"}}