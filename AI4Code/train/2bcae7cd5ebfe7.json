{"cell_type":{"31ec8e8f":"code","10cf9ab4":"code","e1ddb667":"code","49026d90":"code","026c55dc":"code","170cd7dc":"code","935329e3":"code","3a03436b":"code","d313dbdc":"code","2522710f":"code","66033298":"code","f84d00d5":"code","5b830fab":"code","5aae783c":"code","7bdba973":"code","72c9c77a":"code","59474f1f":"code","292258f6":"code","d33cd1ab":"code","dc0f7d11":"code","b96221ec":"code","b857f2b1":"code","7c643046":"code","3f178406":"code","a22744b7":"code","11ba642f":"code","a5a37cc0":"code","8a44ae94":"code","0761d6b8":"code","e467f7d9":"code","01083f58":"code","19743484":"code","125c02d1":"code","aac55433":"code","64635aab":"code","ffef751b":"code","0b7a83d8":"code","26ead741":"code","fc43776e":"code","ca2df703":"code","f9cdd819":"code","3120719b":"code","3c994b1b":"code","3cf32bea":"code","bcea787c":"code","316e5735":"code","2ddabefb":"code","842ac930":"code","8fbf83df":"code","a8201837":"code","1c2c53a1":"code","663ef939":"code","d960ac31":"code","79ed3867":"code","7b4dd160":"code","519313a7":"code","dbdd656d":"code","56d18833":"code","42c1e591":"code","94d539bd":"code","2f67c6f9":"code","01825531":"code","bdebcaad":"code","4ddf1a2c":"code","b9df7f05":"code","e3637e76":"code","35b88cef":"code","adcb288d":"code","50bca570":"code","7e36ad86":"code","d61e81a9":"code","3f48f27e":"code","634ba93e":"code","8837edaa":"code","31e44aaf":"code","c91337f8":"code","118e2e0c":"code","8fc9ab33":"code","ee8bd5bf":"code","c3cd96a5":"code","eef00aed":"code","be0d8489":"code","50076263":"code","e4f92351":"code","e1b6cff1":"code","79e2c62e":"code","396743bd":"code","84b6105b":"code","32bcc0f7":"code","a4c0ca76":"code","62c2767f":"code","05ce7edf":"code","46c1d549":"code","682bf5a5":"code","5381ccf3":"code","7c2caa49":"code","f1e06af3":"code","8538b69e":"code","33a41ace":"code","cf3df7ff":"code","e1f81b72":"code","e1dbabea":"code","a1d09577":"code","cffbae40":"code","d8571994":"code","2faf1ec5":"code","4182e8ce":"code","9f82b71f":"code","0f740ed6":"code","5477826f":"code","ed40caac":"code","ec271936":"code","bf65ff62":"code","7217f81a":"markdown","b13837ee":"markdown","77e7602a":"markdown","1b0ef7a2":"markdown","97a52031":"markdown","833fcb92":"markdown","590290f9":"markdown","c1a9e43a":"markdown","f7a51309":"markdown","8df1916b":"markdown","5e004748":"markdown","34650621":"markdown","a42949b7":"markdown","30a6cfd2":"markdown","50ab56e6":"markdown","170da9d9":"markdown","bb8c04dc":"markdown","1f81ec8f":"markdown","4c2061c7":"markdown","c59644ff":"markdown","dac77d17":"markdown","e96d2a5b":"markdown","1be32011":"markdown","b3836c7e":"markdown","b3c94b4f":"markdown","95b29749":"markdown","022caf70":"markdown","4696ad76":"markdown","e30ee765":"markdown","8a44ac2c":"markdown","dff1ff3d":"markdown","dbc84d6b":"markdown","03c29c9a":"markdown","739134ff":"markdown","5b697a39":"markdown","fb908b23":"markdown","f3856c11":"markdown","6a6d7b59":"markdown","118ddac0":"markdown","9fb61a9d":"markdown","7a87685c":"markdown","fed8bbe2":"markdown","0f5135d3":"markdown","7b1dd4f0":"markdown","9a462cee":"markdown","6a6c849a":"markdown","c20c0534":"markdown","22773074":"markdown","2721b041":"markdown","a8c15935":"markdown","eda2177d":"markdown","b166d2f7":"markdown","8543b383":"markdown","9bfca1f2":"markdown"},"source":{"31ec8e8f":"import numpy as np\nimport re\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\nimport pandas as pd \n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 5000)","10cf9ab4":"text1 = \"For whomsoever shall hold this hammer, if he shall be worthy shall possess the Power of Thor\" \ntext2 = \"Did you do it ? yeS\"\ntext3 = \"What dId it coSt ? EveryThing\"\ntext4 = text2 +' '+ text3","e1ddb667":"print(text1, text2, text3, text4, sep=\"\\n\")","49026d90":"print(len(text1), len(text2), len(text3), len(text4), sep=\"\\n\")","026c55dc":"text1a = text1.split(' ')\ntext4a = text4.split(' ')\nprint(len(text1a), len(text4a))\nprint(text1a, text4a, sep='\\n')","170cd7dc":"# Long words: Words that have more than 3 characters\n[w for w in text1a if len(w)>3] \n# Long words: Words that have less than 3 characters\n[w for w in text1a if len(w)<3] ","935329e3":"print([w for w in text1a if w.istitle()])              # Words that have 1st character in Capital\nprint([w for w in text1a if w.endswith('r')])          # Words that end with character 'r'","3a03436b":"print(\"Words that are unique without preprocessing (1 '?', 1 'it', 2 'Did & dId'\")\nprint(set(text4a))\nprint(\"\\nWords that are unique with preprocessing (1 '?', 1 'it', 1 'did'\")\nprint(set([w.lower() for w in text4a]))","d313dbdc":"print('Uppercase : ', [w.upper() for w in text4a], '\\n') # Convert all words to lowercase\nprint('Lowercase : ', [w.lower() for w in text4a], '\\n') # Convert all words to uppercase\nprint('Title     : ', [w.title() for w in text4a], '\\n') # Convert all words to title(1st letter caps)","2522710f":"print(text1)\nprint(text1.split('shall'), '\\n')","66033298":"print('shall'.join(text1.split('shall')))\nprint('will'.join(text1.split('shall')))\nprint('should'.join(text1.split('shall')))","f84d00d5":"temp = \"ouagadougou\"\nprint(\"split a string - ouagadougou(eg:sentence\/word) into substring - ou(eg:words):\")\nprint(\"ouagadougou\".split('ou')) ","5b830fab":"print(\"\\nsplit a substring - ouagadougou(eg:word) into separate characters:\")\nprint(\"Method 1\", list(\"ouagadougou\"))       \nprint(\"Method 2\", [c for c in \"ouagadougou\"])","5aae783c":"\"ouagadougou\".split('ou')# Error         --> split a word into character","7bdba973":"textnick = \"      See, it's things like this that give me trust issues.  \"\nprint(textnick.split(' '))","72c9c77a":"textnick = \"      See, it's things like this that give me trust issues.  \"\nprint(textnick.strip().split(' '))","59474f1f":"Movies = \"Marvel - Hulk, Marvel - Thor, Marvel - Ironman, Marvel - Captain America, Marvel - Avengers\"\nprint(\"INDEX of First occurance of the substring Marvel in the sentence Movies from the start\", Movies.find('Marvel'))\nprint(\"\\nINDEX of First occurance of the substring Marvel in the sentence Movies from the end\", Movies.rfind('Marvel'))\nprint(\"\\nAll occurances of Marvel in the sentence is replaced by DC:\\n\", Movies.replace('Marvel', 'DC'))","292258f6":"f = open('..\/input\/UNHDR.txt', 'r')\nEOL_space = f.readline()\nEOL_space","d33cd1ab":"EOL_space.rstrip()  # To remove the \\n character from the end of the line use strip() and its add on versions ","dc0f7d11":"f = open('..\/input\/UNHDR.txt', 'r')\nf.seek(0) # Reset the reading pointer to the start\nHDRUN = f.read()\nprint(\"There are {} sentences in this file\".format(len(HDRUN.splitlines())))\nHDRUN.splitlines()","b96221ec":"text5 = '\"Ethics are built right into the ideals and objectives of the United Nations\" #UNSG @ NY Society for Ethical Culture bit.ly\/2guVelr @UN @UN_Women'\ntext5a = text5.split(' ')\nprint(text5a)","b857f2b1":"print([w for w in text5a if w.startswith('#')])\nprint([w for w in text5a if w.startswith('@')])","7c643046":"import re\n[w for w in text5a if re.search('@[A-Za-z0-9_]+', w)]","3f178406":"[w for w in text5a if re.search('@\\w+', w)]","a22744b7":"temp = \"ouagadougou\"\nre.findall(r'[aeiou]', temp)","11ba642f":"re.findall(r'[^aeiou]', temp)","a5a37cc0":"# Regular Expression for Dates\ndateStr = '01-11-2018\\n01\/11\/2018\\n01\/11\/18\\n11\/01\/2018\\n11 Nov 2018\\n11 November 2018\\nNov 11, 2018\\nNovember 11, 2018\\n'\nprint(re.findall(r'\\d{2}[\/-]\\d{2}[\/-]\\d{4}', dateStr))\nprint(re.findall(r'\\d{2}[\/-]\\d{2}[\/-]\\d{2,4}', dateStr))\nprint(re.findall(r'\\d{1,2}[\/-]\\d{1,2}[\/-]\\d{2,4}', dateStr))","8a44ae94":"print(re.findall(r'\\d{2} (Jan|Feb|Mar|Apr|May|Jun|July|Aug|Sep|Oct|Nov|Dec) \\d{4}', dateStr))\nprint(re.findall(r'\\d{2} (?:Jan|Feb|Mar|Apr|May|Jun|July|Aug|Sep|Oct|Nov|Dec) \\d{4}', dateStr))\nprint(re.findall(r'\\d{2} (?:Jan|Feb|Mar|Apr|May|Jun|July|Aug|Sep|Oct|Nov|Dec)[a-z]* \\d{4}', dateStr))\nprint(re.findall(r'(?:\\d{2}? )?(?:Jan|Feb|Mar|Apr|May|Jun|July|Aug|Sep|Oct|Nov|Dec)[a-z]* (?:\\d{2}, )?\\d{4}', dateStr))\nprint(re.findall(r'(?:\\d{1,2}? )?(?:Jan|Feb|Mar|Apr|May|Jun|July|Aug|Sep|Oct|Nov|Dec)[a-z]* (?:\\d{1,2}, )?\\d{2,4}', dateStr))","0761d6b8":"import pandas as pd\ntime_sentences = [\"Monday: The doctor's appointment is at 2:45pm.\", \n                  \"Tuesday: The dentist's appointment is at 11:30 am.\",\n                  \"Wednesday: At 7:00pm, there is a basketball game!\",\n                  \"Thursday: Be back home by 11:15 pm at the latest.\",\n                  \"Friday: Take the train at 08:10 am, arrive at 09:00am.\"]\n\ndf = pd.DataFrame(time_sentences, columns=['text'])\ndf","e467f7d9":"df['text'].str.len()","01083f58":"df['text'].str.split().str.len()","19743484":"print(\"\\nColumns with Word Appointment : \\n\",df['text'].str.contains('appointment'))\nprint(\"\\nColumns with Word day : \\n\",df['text'].str.contains('day'))\nprint(\"\\nColumns with timestamp at night - pm : \\n\",df['text'].str.contains('pm'))","125c02d1":"# How many times a digit occurs in every sentence\ndf['text'].str.count(r'\\d')","aac55433":"df['text'].str.findall(r'\\d')","64635aab":"df['text'].str.findall(r'(\\d?\\d):(\\d\\d)')","ffef751b":"df['text'].str.replace(r'\\w+day\\b', ':-)   ')","0b7a83d8":"df['text'].str.replace(r'(\\w+day\\b)', lambda x: x.group()[0][:3])","26ead741":"df['text'].str.extract(r'(\\d?\\d):(\\d\\d)')","fc43776e":"df['text'].str.extractall(r'((\\d?\\d):(\\d\\d) ?([ap]m))')","ca2df703":"df['text'].str.extractall(r'(?P<Time>(?P<Hour>\\d?\\d):(?P<Minute>\\d\\d) ?(?P<Period>[ap]m))')","f9cdd819":"import nltk","3120719b":"print(dir(nltk))","3c994b1b":"from nltk.book import *","3cf32bea":"text7","bcea787c":"sents()","316e5735":"print(len(sent7))\nprint(sent7)","2ddabefb":"len(text7)","842ac930":"len(set(text7))","8fbf83df":"len(set([w.lower() for w in text7]))","a8201837":"print(list(set(text7))[:10])","1c2c53a1":"dist = FreqDist(text7)\nprint(type(dist))\nprint(len(dist))\nprint(dist)","663ef939":"print([w for w in dist.items()][1:500])","d960ac31":"type(dist.items())","79ed3867":"vocab = dist.keys()\nlist(vocab)[:10]","7b4dd160":"dist['four']","519313a7":"freqwords = [w for w in vocab if len(w)>5 and dist[w]>100]\nfreqwords","dbdd656d":"input1 = \"List listed lists listing listings\"\nwords1 = input1.lower().split(' ')\nwords1","56d18833":"porter = nltk.PorterStemmer()\n[porter.stem(t) for t in words1]","42c1e591":"input2 = \"Trouble troubling troubled troubler\"\nwords2 = input2.lower().split(' ')\nwords2","94d539bd":"porter = nltk.PorterStemmer()\n[porter.stem(t) for t in words2]","2f67c6f9":"lancast = nltk.LancasterStemmer()\n[lancast.stem(t) for t in words2]","01825531":"udhr = nltk.corpus.udhr.words('English-Latin1')\nprint(udhr[:20])","bdebcaad":"WNlemma = nltk.WordNetLemmatizer()\nprint([WNlemma.lemmatize(t) for t in udhr[:20]])","4ddf1a2c":"text11 = \"Children shouldn't drink a sugary drink before bed.\"\ntext11.split(' ')","b9df7f05":"print(nltk.word_tokenize(text11))","e3637e76":"text12 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\nsentences = nltk.sent_tokenize(text12)\nprint(\"There are {} sentences in the above document|text corpus\".format(len(sentences)))\nsentences","35b88cef":"from nltk.help import upenn_tagset\ndir(upenn_tagset)","adcb288d":"nltk.help.upenn_tagset(tagpattern='MD')","50bca570":"nltk.help.upenn_tagset(tagpattern='V*')","7e36ad86":"text11 = \"Children shouldn't drink a sugary drink before bed.\"\nnltk.pos_tag(nltk.word_tokenize(text11))","d61e81a9":"nltk.help.upenn_tagset(tagpattern='NNP')\nnltk.help.upenn_tagset(tagpattern='RB')\nnltk.help.upenn_tagset(tagpattern='VB')\nnltk.help.upenn_tagset(tagpattern='DT')\nnltk.help.upenn_tagset(tagpattern='JJ')\nnltk.help.upenn_tagset(tagpattern='NN')\nnltk.help.upenn_tagset(tagpattern='IN')","3f48f27e":"nltk.pos_tag(nltk.word_tokenize(\"Visiting aunts can be a nuisance\"))","634ba93e":"nltk.pos_tag(nltk.word_tokenize(\"I never said you stole my money\"))","8837edaa":"text15 = nltk.word_tokenize(\"Alice loves Bob\")\ngrammar = nltk.CFG.fromstring(\"\"\"\nS -> NP VP\nVP -> V NP\nNP -> 'Alice' | 'Bob'\nV -> 'loves'\n\"\"\")\n\nparser = nltk.ChartParser(grammar=grammar)\ntrees = parser.parse_all(text15)\nfor tree in trees:\n    print(tree)","31e44aaf":"text16 = nltk.word_tokenize(\"I saw the man with a telescope\")\ngrammar1 = nltk.data.load('..\/input\/mygrammar.cfg')\ngrammar1","c91337f8":"parser = nltk.ChartParser(grammar1)\ntrees = parser.parse_all(text16)\nfor tree in trees:\n    print(tree)","118e2e0c":"from nltk.corpus import treebank\ntext17 = treebank.parsed_sents('wsj_0001.mrg')[0]\nprint(text17)\n","8fc9ab33":"text18 = nltk.word_tokenize(\"The old man the boat\")\nnltk.pos_tag(text18)","ee8bd5bf":"text19 = nltk.word_tokenize(\"Colorless green ideas sleep furiously\")\nnltk.pos_tag(text19)","c3cd96a5":"import time\nstart = time.time()\ndf = pd.read_csv('..\/input\/\/Amazon_Unlocked_Mobile.csv')\nend = time.time()","eef00aed":"df['Reviews']","be0d8489":"df.dropna(inplace=True)\ndf = df[df['Rating']!=3]\ndf.head()","50076263":"df['Positively_Rated'] = np.where(df['Rating'] >3,1,0)\ndf.head()","e4f92351":"df['Positively_Rated'].mean()","e1b6cff1":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df['Reviews'], df['Positively_Rated'], random_state=0)\n\nprint('X_train first entry:\\n\\n', X_train.iloc[0])\nprint('\\n\\nX_train shape: ', X_train.shape)","79e2c62e":"from sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer().fit(X_train)","396743bd":"vect.get_feature_names()[::2000]","84b6105b":"len(vect.get_feature_names())","32bcc0f7":"X_train_vectorized = vect.transform(X_train)\nX_train_vectorized","a4c0ca76":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)","62c2767f":"from sklearn.metrics import roc_auc_score\n\ny_predictions = model.predict(vect.transform(X_test))\nprint('AUC : ',roc_auc_score(y_test, y_predictions))","05ce7edf":"# get the feature names as numpy array\nfeature_names = np.array(vect.get_feature_names())\n\n# Sort the coefficients from the model\nsorted_coef_index = model.coef_[0].argsort()","46c1d549":"# The 10 largest coefficients are being indexed using [:-11:-1] \n# so the list returned is in order of largest to smallest\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","682bf5a5":"from sklearn.feature_extraction import text\nprint([w for w in dir(text) if not w.startswith('_')])\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(min_df=5).fit(X_train)\nlen(vect.get_feature_names())","5381ccf3":"X_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\ny_predictions = model.predict(vect.transform(X_test))\nprint('AUC : ',roc_auc_score(y_test, y_predictions))","7c2caa49":"feature_names = np.array(vect.get_feature_names())\n\nsorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n\nprint('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\nprint('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))","f1e06af3":"sorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","8538b69e":"print(model.predict(vect.transform(['not an issue, phone is working',\n                                    'an issue, phone is not working'])))","33a41ace":"print(model.predict(vect.transform(['it is not that i am so smart it is just that i stay with problems longer'])))","cf3df7ff":"vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nlen(vect.get_feature_names())","e1f81b72":"model = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\ny_predictions = model.predict(vect.transform(X_test))\nprint('AUC: ', roc_auc_score(y_test, y_predictions))","e1dbabea":"feature_names = np.array(vect.get_feature_names())\nsorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","a1d09577":"print(model.predict(vect.transform(['not an issue, phone is working',\n                                    'an issue, phone is not working'])))","cffbae40":"print([w for w in dir(nltk) if ('classifier' in w.lower())|('tree' in w.lower())])","d8571994":"from nltk.classify import NaiveBayesClassifier\n\nvect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nprint(X_train_vectorized.shape, y_train.shape)\n\nclassifier = NaiveBayesClassifier.train(X_train_vectorized, y_train)\nclassifier.classify(X_test)\n\nnltk.classify.util.accuracy(classifier, y_test)\nclassifier.label()","2faf1ec5":"from nltk.corpus import wordnet as wn","4182e8ce":"deer = wn.synset('deer.n.01')\nelk = wn.synset('elk.n.01')\nhorse = wn.synset('horse.n.01')","9f82b71f":"deer.path_similarity(elk)","0f740ed6":"deer.path_similarity(horse)","5477826f":"from nltk.corpus import wordnet_ic","ed40caac":"brown_ic = wordnet_ic.ic('ic-brown.dat')\n\ndeer.lin_similarity(elk, brown_ic)","ec271936":"deer.lin_similarity(horse, brown_ic)","bf65ff62":"import gensim\nfrom gensim import corpora, models","7217f81a":"#### How many times a large word occurs in the text corpus","b13837ee":"# NLTK - Natural Language Processing Toolkit","77e7602a":"### transform the documents in the training data to a document-term matrix ==> Bad of Words Representation","1b0ef7a2":"# Handling larger Texts\n## Reading files by line","97a52031":"###  Python Regular Expressions Made Easy (2017) -->  https:\/\/www.youtube.com\/playlist?list=PLGKQkV4guDKH1TpfM-FvPGLUyjsPGdXOg","833fcb92":"### unique words in a list of words using set() function","590290f9":"### This is just the first draft version. Will include some more of my own code with lots of updates in parts 2,3,4,5","c1a9e43a":" ## Name groups in str","f7a51309":"#### Note that not all . are end of sentence like in \"U.S.\" and \"2.99.\" ","8df1916b":"### Tokenization","5e004748":"### Information Criteria to find Lin Similarity","34650621":"### these reviews are now correctly identified","a42949b7":"### these reviews are treated the same by our current model but they are not (two different)","30a6cfd2":"### Find (Ctr + F) all consonants ","50ab56e6":"### Split the sentence in words - How ? \n### Split them by a delimiter. CSV(comma separated value) files are read by discarding the *commas* in between data, same way these words are separated by whitespace.\n\n> https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/tokenization-1.html","170da9d9":"### Lemmatization","bb8c04dc":"#### Find count of digits in the data ","1f81ec8f":"### Now the words are tokenized(chopping sentence into pieces\/tokens). This O\/P is then fed as I\/P for Parsing or Text Mining\n\n# String Operations","4c2061c7":" #### Find all unique digits in the data ","c59644ff":"## File Operations","dac77d17":"#### Find the 10 smallest and 10 largest coefficients","e96d2a5b":"## Count Vectorizor\n","1be32011":"## Case Study : Sentiment Analysis","b3836c7e":"## Find first occurance of a substring\/character in a String\/Sentence\n#### To find all occurances, we can use *Regular Expressions* re.find_iter function \n> https:\/\/docs.python.org\/2\/library\/re.html#module-contents\n","b3c94b4f":"## Reading the full file","95b29749":"## Tf-idf","022caf70":"### Convert all words to lowercase || uppercase || title","4696ad76":"#### Counting the vocabulary of words","e30ee765":"### Find (Ctr + F) all vowels","8a44ac2c":"### Encode Ratings 4,5 as 1 (Positive Sentiment)\n### Encode Ratings 1,2 as 0 (Negative Sentiment)\n\n","dff1ff3d":"## Remember List Comprehension from C1 - Iterate through collections in one line","dbc84d6b":"#### Most ratings are positive","03c29c9a":"#### Note that not & . are separate tokens  (inorder to account for negations in the text)","739134ff":"\n## Stemming and Lemmatization \n#### https:\/\/www.youtube.com\/watch?v=p1ccbR2P_xA\n\n### Normalization and Stemming\n","5b697a39":"## LDA Model","fb908b23":"# To Perform String Operations on Series\/Dataframe - .str is used","f3856c11":"### Drop missing values and remove neutral ratings (3)\n\n","6a6d7b59":"### Meta Characters in Regular Expressions\n\n```\n.       --> Wilcard Character that matches a single character\n^       --> Start of the String\n$       --> End of the String\n[]      --> Matches one of the charaters, that are within the square brackets \n[a-z]   --> Matches one of the range of characters a,b,c,d,e,f,g......x,y,z\n[^abc]  --> Matches any character except a,b,c - Exclude a,b,c\n[a|b]   --> Matches either a or b, where a & b are strings\n()      --> Scoping for Operators - just normal use\n\\       --> Escape character for special characters (\\n,\\t,\\b,\\d, \\D)\n\n```\n\n### Meta Characters: Character Symbols\n```\n\\b      --> Matches Word boundary\n\\d      --> Any digit, equivalent to [0-9]\n\\D      --> Any non-digit, equivalent to [^0-9]\n\\s      --> Any whitespace, equivalent to [\\t\\n\\r\\f\\v]\n\\S      --> Any non-whitespace, equivalent to [^\\t\\n\\r\\f\\v]\n\\w      --> Any alphanumeric Character, equivalent to [a-zA-Z0-9_]\n\\W      --> Any non-alphanumeric Character, equivalent to [^a-zA-Z0-9_]\n```\n\n### Meta Characters : Repetitions\n```\n*       --> Matches zero or more occurences\n+       --> Matches one or more occurences\n?       --> Matches zero or one occurences\n{n}     --> Matches exactly in repetitions, n>=0\n{n,0}   --> Matches Atleast n repetition    \n{0,n}   --> Matches Atmost n repetition\n{m,n}   --> Atleast Atleast m and Atmost n repetitions  \n```","118ddac0":"# Most Annoying part of Text preprocessing - Regular Expressions [Regex]\n## But also the most important part","9fb61a9d":"### Models in NLTK for Text classification","7a87685c":"## Date Variations \n> 01-11-2018<br\/>\n> 01\/11\/2018<br\/>\n> 01\/11\/18<br\/>\n> 11\/01\/2018<br\/>\n> 11 Nov 2018<br\/>\n> 11 November 2018<br\/>\n> Nov 11, 2018<br\/>\n> November 11, 2018<br\/>","fed8bbe2":"### Parsing sentence structure","0f5135d3":"### Note Below, the disappearance of the whitespaces at the beginning and end of the string","7b1dd4f0":"## WordNet","9a462cee":"### Splitting a sentence based a word(stopper word\/connectors)","6a6c849a":"\n#### Ambiguity in Parsing sentences \"I saw the man with a telescope\" \n#### 1. Did you see with the telescope ?\n#### 2. Did you see the man who was holding a telescope ?\n","c20c0534":"### Looking at the mean - we have imbalanced classes","22773074":"### POS tagging and parsing ambiguity","2721b041":"## Advanced NLP taks with NLTK\n### POS (Parts-of-Speech)Tagging ","a8c15935":"#### Find time values in the form of digits in the data ","eda2177d":"### Extract elements with special symbols (meanings) @ and # are used in tweets","b166d2f7":"### Frequency of Words","8543b383":"## n-grams\n#### Fit the CountVectorizer to the training data specifiying a minimum document frequency of 5 and extracting 1-grams and 2-grams","9bfca1f2":"### Merging 3 sentences with a Connector (and\/will\/hence etc) using .join() function"}}