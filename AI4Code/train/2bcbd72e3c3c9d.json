{"cell_type":{"39806553":"code","4c64f627":"code","4cc9dc29":"code","dd8d045b":"code","1bed7c55":"code","6eb4fcb9":"code","5ff23022":"code","741429c8":"code","2d5a9011":"code","f2348bf2":"code","5e1dc97a":"code","5ecf88e5":"code","f12faac8":"code","49e47c91":"code","56f7f13e":"code","db453fda":"markdown","66873fad":"markdown","1309eb53":"markdown","a5c828e8":"markdown","bc8ea5f7":"markdown","a8390185":"markdown","b764349d":"markdown","c33c78e8":"markdown","7a16e163":"markdown","cfbcd400":"markdown"},"source":{"39806553":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom pathlib import Path\nimport random, math, cv2\nfrom tqdm import tqdm_notebook as tqdm\n\nimport torch\nfrom torch import nn\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import Compose, ToTensor\n\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4c64f627":"MNIST_DATA_DIR = Path('\/kaggle\/working')\nBSDS_DATA_DIR = Path('..\/input\/berkeley-segmentation-dataset-500-bsds500')\nMODEL_FILE = Path('..\/input\/pretrain-source-model-for-domain-adaptation-mnist\/best_source_weights_mnist.pth')\n\nbatch_size = 64\niterations = 500\nepochs = 4\nk_disc = 1\nk_clf = 10\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","4cc9dc29":"def visualize_digits(dataset, k=80, mnistm=False, cmap=None, title=None):\n    \n    ncols = 20\n    indices = random.choices(range(len(dataset)), k=k)\n    nrows = math.floor(len(indices)\/ncols)\n    \n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols,nrows+0.4), gridspec_kw=dict(wspace=0.1, hspace=0.1), subplot_kw=dict(yticks=[], xticks=[]))\n    axes_flat = axes.reshape(-1)\n    fig.suptitle(title, fontsize=20)\n    \n    for list_idx, image_idx in enumerate(indices[:ncols*nrows]):\n        ax = axes_flat[list_idx]\n        image = dataset[image_idx][0]\n        image = image.numpy().transpose(1, 2, 0)\n        ax.imshow(image, cmap=cmap)\n\ndef set_requires_grad(model, requires_grad=True):\n    for param in model.parameters():\n        param.requires_grad = requires_grad\n\ndef loop_iterable(iterable):\n    while True:\n        yield from iterable\n\nclass GrayscaleToRgb:\n    \"\"\"Convert a grayscale image to rgb\"\"\"\n    def __call__(self, image):\n        image = np.array(image)\n        image = np.dstack([image, image, image])\n        return Image.fromarray(image)","dd8d045b":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.feature_extractor = nn.Sequential(\n            nn.Conv2d(3, 10, kernel_size=5),\n            nn.MaxPool2d(2),\n            nn.ReLU(),\n            nn.Conv2d(10, 20, kernel_size=5),\n            nn.MaxPool2d(2),\n            nn.Dropout2d(),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(320, 50),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(50, 10),\n            nn.LogSoftmax(),\n        )\n\n    def forward(self, x):\n        features = self.feature_extractor(x)\n        features = features.view(x.shape[0], -1)\n        logits = self.classifier(features)\n        return logits","1bed7c55":"source_model = Net().to(device)\nif MODEL_FILE:\n    source_model.load_state_dict(torch.load(MODEL_FILE, map_location=device))\nsource_model.eval()\nset_requires_grad(source_model, requires_grad=False)\n\nclf = source_model\nsource_model = source_model.feature_extractor\n\ntarget_model = Net().to(device)\nif MODEL_FILE:\n    target_model.load_state_dict(torch.load(MODEL_FILE, map_location=device))\ntarget_model = target_model.feature_extractor\n\ndiscriminator = nn.Sequential(\n    nn.Linear(320, 120),\n    nn.ReLU(),\n    nn.Linear(120, 20),\n    nn.ReLU(),\n    nn.Linear(20, 1)\n).to(device)","6eb4fcb9":"class BSDS500(Dataset):\n\n    def __init__(self):\n        image_folder = BSDS_DATA_DIR \/ 'images'\n        self.image_files = list(map(str, image_folder.glob('*\/*.jpg')))\n\n    def __getitem__(self, i):\n        image = cv2.imread(self.image_files[i], cv2.IMREAD_COLOR)\n        tensor = torch.from_numpy(image.transpose(2, 0, 1))\n        return tensor\n\n    def __len__(self):\n        return len(self.image_files)\n\n\nclass MNISTM(Dataset):\n\n    def __init__(self, train=True):\n        super(MNISTM, self).__init__()\n        self.mnist = datasets.MNIST(MNIST_DATA_DIR \/ 'mnist', train=train,\n                                    download=True)\n        self.bsds = BSDS500()\n        # Fix RNG so the same images are used for blending\n        self.rng = np.random.RandomState(42)\n\n    def __getitem__(self, i):\n        digit, label = self.mnist[i]\n        digit = transforms.ToTensor()(digit)\n        bsds_image = self._random_bsds_image()\n        patch = self._random_patch(bsds_image)\n        patch = patch.float() \/ 255\n        blend = torch.abs(patch - digit)\n        return blend, label\n\n    def _random_patch(self, image, size=(28, 28)):\n        _, im_height, im_width = image.shape\n        x = self.rng.randint(0, im_width-size[1])\n        y = self.rng.randint(0, im_height-size[0])\n        return image[:, y:y+size[0], x:x+size[1]]\n\n    def _random_bsds_image(self):\n        i = self.rng.choice(len(self.bsds))\n        return self.bsds[i]\n\n    def __len__(self):\n        return len(self.mnist)","5ff23022":"half_batch = batch_size \/\/ 2\n\nsource_dataset = MNIST(MNIST_DATA_DIR\/'mnist', train=True, download=True, transform=Compose([GrayscaleToRgb(), ToTensor()]))\nsource_loader = DataLoader(source_dataset, batch_size=half_batch, shuffle=True, num_workers=16, pin_memory=True)\n\ntarget_train_dataset, target_test_dataset = MNISTM(train=True), MNISTM(train=False)\ntarget_train_loader = DataLoader(target_train_dataset, batch_size=half_batch, shuffle=True, num_workers=16, pin_memory=True)\ntarget_test_loader = DataLoader(target_test_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n\ndiscriminator_optim = torch.optim.Adam(discriminator.parameters())\ntarget_optim = torch.optim.Adam(target_model.parameters())\ncriterion_train = nn.BCEWithLogitsLoss()\ncriterion_test = nn.NLLLoss()","741429c8":"visualize_digits(dataset=target_train_dataset, k=200, mnistm=True, title='Sample MNIST-M Images')","2d5a9011":"visualize_digits(dataset=source_dataset, k=120, cmap='gray', title='Sample MNIST Images')","f2348bf2":"disc_losses, disc_accuracies, disc_train_counter = [], [], []\nclf_disc_losses, clf_disc_train_counter = [], []\nclf_losses, clf_accuracies = [], []\nclf_test_counter = [idx*iterations*k_clf*target_train_loader.batch_size for idx in range(0, epochs+1)]","5e1dc97a":"test_loss, test_accuracy = 0, 0\nclf.eval()\ntqdm_bar = tqdm(target_test_loader, desc=f'Testing ', total=int(len(target_test_loader)))\nfor idx, (images, labels) in enumerate(tqdm_bar):\n    images, labels = images.to(device), labels.to(device)\n    with torch.no_grad():\n        outputs = clf(images)\n        loss = criterion_test(outputs, labels)\n    test_loss += loss.item()\n    outputs = torch.argmax(outputs, dim=1).type(torch.FloatTensor).to(device)\n    test_accuracy += torch.mean((outputs == labels).type(torch.FloatTensor)).item()\n    tqdm_bar.set_postfix(test_loss=(test_loss\/(idx+1)), test_accuracy=test_accuracy\/(idx+1))\nclf_losses.append(test_loss\/len(target_test_loader))\nclf_accuracies.append(test_accuracy\/len(target_test_loader))","5ecf88e5":"for epoch in range(epochs):\n    target_batch_iterator = loop_iterable(target_train_loader)\n    batch_iterator = zip(loop_iterable(source_loader), loop_iterable(target_train_loader))\n    disc_loss, disc_accuracy = 0, 0\n    clf_disc_loss = 0\n    test_loss, test_accuracy = 0, 0\n    tqdm_bar = tqdm(range(iterations), desc=f'Training Epoch {epoch} ', total=iterations)\n    for iter_idx in tqdm_bar:\n        # Train discriminator\n        set_requires_grad(target_model, requires_grad=False)\n        set_requires_grad(discriminator, requires_grad=True)\n        for disc_idx in range(k_disc):\n            (source_x, _), (target_x, _) = next(batch_iterator)\n            source_x, target_x = source_x.to(device), target_x.to(device)\n            source_features = source_model(source_x).view(source_x.shape[0], -1)\n            target_features = target_model(target_x).view(target_x.shape[0], -1)\n            discriminator_x = torch.cat([source_features, target_features])\n            discriminator_y = torch.cat([torch.ones(source_x.shape[0], device=device), torch.zeros(target_x.shape[0], device=device)])\n            preds = discriminator(discriminator_x).squeeze()\n            loss = criterion_train(preds, discriminator_y)\n            discriminator_optim.zero_grad()\n            loss.backward()\n            discriminator_optim.step()\n            disc_loss += loss.item()\n            disc_losses.append(loss.item())\n            disc_batch_accuracy = ((preds > 0).long() == discriminator_y.long()).float().mean().item()\n            disc_accuracy += disc_batch_accuracy\n            disc_accuracies.append(disc_batch_accuracy)\n            disc_train_counter.append((disc_idx+1)*source_x.size(0) + iter_idx*k_disc*target_train_loader.batch_size + epoch*iterations*k_disc*target_train_loader.batch_size)\n\n        # Train classifier\n        set_requires_grad(target_model, requires_grad=True)\n        set_requires_grad(discriminator, requires_grad=False)\n        for clf_idx in range(k_clf):\n            _, (target_x, _) = next(batch_iterator)\n            target_x = target_x.to(device)\n            target_features = target_model(target_x).view(target_x.shape[0], -1)\n            # Flipped Labels\n            discriminator_y = torch.ones(target_x.shape[0], device=device)\n            preds = discriminator(target_features).squeeze()\n            loss = criterion_train(preds, discriminator_y)\n            target_optim.zero_grad()\n            loss.backward()\n            target_optim.step()\n            clf_disc_loss += loss.item()\n            clf_disc_losses.append(loss.item())\n            clf_disc_train_counter.append(source_x.size(0) + clf_idx*half_batch + iter_idx*k_clf*half_batch + epoch*iterations*k_clf*half_batch)\n        tqdm_bar.set_postfix(disc_loss=disc_loss\/((iter_idx+1)*k_disc), disc_accuracy=disc_accuracy\/((iter_idx+1)*k_disc),\n                             clf_disc_loss=clf_disc_loss\/((iter_idx+1)*k_clf))\n\n    # Test full target model\n    test_loss, test_accuracy = 0, 0\n    clf.feature_extractor = target_model\n    clf.eval()\n    tqdm_bar = tqdm(target_test_loader, desc=f'Testing Epoch {epoch} (Full Target Model)', total=int(len(target_test_loader)))\n    for idx, (images, labels) in enumerate(tqdm_bar):\n        images, labels = images.to(device), labels.to(device)\n        with torch.no_grad():\n            outputs = clf(images)\n            loss = criterion_test(outputs, labels)\n        test_loss += loss.item()\n        outputs = torch.argmax(outputs, dim=1).type(torch.FloatTensor).to(device)\n        test_accuracy += torch.mean((outputs == labels).type(torch.FloatTensor)).item()\n        tqdm_bar.set_postfix(test_loss=(test_loss\/(idx+1)), test_accuracy=test_accuracy\/(idx+1))\n    clf_losses.append(test_loss\/len(target_test_loader))\n    clf_accuracies.append(test_accuracy\/len(target_test_loader))\n    if np.argmax(clf_accuracies) == len(clf_accuracies)-1:\n        torch.save(clf.state_dict(), 'adda_target_weights.pth')\n        ","f12faac8":"fig = plotly.subplots.make_subplots(specs=[[{\"secondary_y\": True}]])\nfig.add_trace(go.Scatter(x=disc_train_counter, y=disc_losses, mode='lines', name='Disc Loss'), secondary_y=False)\nfig.add_trace(go.Scatter(x=disc_train_counter, y=disc_accuracies, mode='lines', name='Disc Accuracy', line_color='lightseagreen'), secondary_y=True)\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Discriminator Loss vs Accuracy\")\nfig.update_xaxes(title_text=\"Number of training examples seen\")\nfig.update_yaxes(title_text=\"Discriminator <b>Loss<\/b> (BCE)\", secondary_y=False)\nfig.update_yaxes(title_text=\"Discriminator <b>Accuracy<\/b>\", secondary_y=True)\nfig.show()","49e47c91":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=clf_disc_train_counter, y=clf_disc_losses, mode='lines', name='Clf-Disc Train Loss'))\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Clf-Disc Loss\",\n    xaxis_title=\"Number of training examples seen\",\n    yaxis_title=\"Binary Cross Entropy Loss\"),\nfig.show()","56f7f13e":"fig = plotly.subplots.make_subplots(specs=[[{\"secondary_y\": True}]])\nfig.add_trace(go.Scatter(x=clf_test_counter, y=clf_accuracies, marker_symbol='star-diamond', \n                         marker_line_color=\"orange\", marker_line_width=1, marker_size=9, mode='lines+markers', \n                         name='Target Accuracy'), secondary_y=False)\nfig.add_trace(go.Scatter(x=clf_test_counter, y=clf_losses, marker_symbol='star-square', \n                         marker_line_color=\"lightseagreen\", marker_line_width=1, marker_size=9, mode='lines+markers',\n                         name='Target Loss'), secondary_y=True)\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Full Target Model Loss vs Accuracy\")\nfig.update_xaxes(title_text=\"Number of training examples seen\")\nfig.update_yaxes(title_text=\"Target <b>Accuracy<\/b>\", secondary_y=False)\nfig.update_yaxes(title_text=\"Target <b>Loss<\/b> (NLLLoss)\", secondary_y=True)\nfig.show()","db453fda":"## Introduction\n\n### In this notebook, we use [Adversarial Discriminative Domain Adaptation](https:\/\/arxiv.org\/abs\/1702.05464) technique to perform Domain Adaptation on MNIST-M (target dataset) using pre-trained source model trained on MNIST \n### [[Pre-training Source Model on MNIST dataset](https:\/\/www.kaggle.com\/balraj98\/pretrain-source-model-for-domain-adaptation-mnis)]\n\n<h3><center>Domain Adaptation Task<\/center><\/h3>\n<img src=\"https:\/\/media.arxiv-vanity.com\/render-output\/3708497\/x1.png\" width=\"500\" height=\"500\"\/>\n<h4><\/h4>\n<h4><center>Image Source: <a href=\"https:\/\/arxiv.org\/abs\/1702.05464\"> Adversarial Discriminative Domain Adaptation [E. Tzeng et al.]<\/a><\/center><\/h4>","66873fad":"### Visualize Training & Testing Results \ud83d\udcc8","1309eb53":"### Libraries \ud83d\udcda\u2b07","a5c828e8":"<h3><center>Model Architecture<\/center><\/h3>\n<img src=\"https:\/\/miro.medium.com\/max\/700\/1*2VhnQ_PxrJWM88MMXE6F2A.png\" width=\"1000\" height=\"750\"\/>\n<h4><\/h4>\n<h4><center>Image Courtesy: <a href=\"https:\/\/arxiv.org\/abs\/1702.05464\"> Adversarial Discriminative Domain Adaptation [E. Tzeng et al.]<\/a><\/center><\/h4>","bc8ea5f7":"### Visualize MNIST-M & MNIST Data \ud83d\uddbc\ufe0f","a8390185":"### Work in Progress ...","b764349d":"### Define Dataset Classes","c33c78e8":"### Adversarial Discriminative Domain Adaptation","7a16e163":"### Define Model","cfbcd400":"### Get Dataset & Dataloaders"}}