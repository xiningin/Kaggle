{"cell_type":{"65b0636c":"code","960d99fe":"code","33eb1fe5":"code","8c25a8f2":"code","aaad68d4":"code","c43d1663":"code","3ee9816a":"code","e0ce9b50":"code","896f570b":"code","a4e1402f":"code","501bef0d":"code","cd2c7ef0":"code","a96d01c1":"code","e0885cb0":"code","e8ff10eb":"code","112b84b2":"code","a6a4a1aa":"code","17233cbb":"code","be5c9fd5":"code","07646774":"code","288f09b4":"code","0911cb37":"code","892174cd":"code","712ae8d2":"code","24abcb93":"code","360b37b9":"code","731b1756":"code","6d1d7e78":"code","a2c52501":"code","dd12dfa0":"code","810dd52b":"code","7c880b4c":"code","c47a42c2":"code","010296a2":"code","5b4fbc83":"code","51ccd56c":"code","007b3d6f":"code","063b2b17":"code","6bdcdd4d":"code","c3b77bb1":"code","5ab7d6eb":"code","297cbc74":"markdown","c5a33114":"markdown","16e14d3f":"markdown","e9b57be6":"markdown"},"source":{"65b0636c":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","960d99fe":"import torch\nfrom torch.autograd import Variable\nfrom torch import optim\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport torch.nn.functional as F\nimport time\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport math\nfrom nltk.corpus import stopwords","33eb1fe5":"!apt-get install p7zip\n!p7zip -d -f -k \/kaggle\/input\/mercari-price-suggestion-challenge\/train.tsv.7z\n!unzip -o \/kaggle\/input\/mercari-price-suggestion-challenge\/test_stg2.tsv.zip\n# !p7zip -d -f -k \/kaggle\/input\/mercari-price-suggestion-challenge\/test.tsv.7z","8c25a8f2":"# Read data\ntrain = pd.read_csv(\"train.tsv\", sep='\\t')\ntest = pd.read_csv(\"test_stg2.tsv\", sep='\\t')\n# test = pd.read_csv(\"test.tsv\", sep='\\t')","aaad68d4":"# train shape & train info\nprint(train.shape)\ntrain.info(memory_usage=\"deep\")","c43d1663":"train.describe()","3ee9816a":"print(test.shape)\ntest.info(memory_usage=\"deep\")","e0ce9b50":"test.describe()","896f570b":"# price analysis\ntrain.price.describe()","a4e1402f":"# Item condition analysis\nprint(train['item_condition_id'].value_counts())\nprint('item_condition_id is null:', train['item_condition_id'].isnull().sum())","501bef0d":"# Shipping analysis\nprint(train['shipping'].value_counts())\nprint('shipping is null:', train['shipping'].isnull().sum())","cd2c7ef0":"# Brand name analysis\nprint(train['brand_name'].value_counts())\nprint('brand_name isn\\'t null:', train['brand_name'].count())\nprint('brand_name is null:', train['brand_name'].isnull().sum())","a96d01c1":"# Category name analysis\nprint(train['category_name'].value_counts())\nprint('category_name isn\\'t null:', train['category_name'].count())\nprint('category_name is null:', train['category_name'].isnull().sum())","e0885cb0":"# split item category name into 3 different fields: general_cat, subcat_1, subcat_2\n# eg:(Women\/Athletic Apparel\/Pants) => (Women), (Athletic Apparel), (Pants) \n\ndef split_cat(text):\n    try: return text.split(\"\/\")\n    except: return (\"None\", \"None\", \"None\")\n    \ntrain['general_cat'], train['subcat_1'], train['subcat_2'] = zip(*train['category_name'].apply(lambda x: split_cat(x)))\ntest['general_cat'], test['subcat_1'], test['subcat_2'] = zip(*test['category_name'].apply(lambda x: split_cat(x)))\n\nprint(train['general_cat'].value_counts(),end=\"\\n\\n\")\nprint(train['subcat_1'].value_counts(),end=\"\\n\\n\")\nprint(train['subcat_2'].value_counts())","e8ff10eb":"# Plotting some histograms of categorical Variables\nplt.figure(figsize=(10,10))\nplt.subplot(3,3,1)\ncount_classes_general_cat = pd.value_counts(train.general_cat, sort = True)\ncount_classes_general_cat.plot(kind = 'bar')\nplt.title(\"General Category histogram\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")\n# subcategory 1\nplt.subplot(3,3,3)\ncount_classes_subcat_1 = pd.value_counts(train.subcat_1, sort = True)[:15]\ncount_classes_subcat_1.plot(kind = 'bar')\nplt.title(\"Sub Category 1 histogram\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")\n# subcategory 2\nplt.subplot(3,3,9)\ncount_classes_subcat_2 = pd.value_counts(train.subcat_2, sort = True)[:15]\ncount_classes_subcat_2.plot(kind = 'bar')\nplt.title(\"Sub Category 2 histogram\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","112b84b2":"# Description analysis\nprint(train['item_description'].value_counts())\nprint('item_description isn\\'t null:', train['item_description'].count())\nprint('item_description is null:', train['item_description'].isnull().sum())","a6a4a1aa":"# Handle missing values\ndef handle_missing(dataset):\n    dataset.brand_name.fillna(value=\"None\", inplace=True)\n    dataset.item_description.fillna(value=\"None\", inplace=True)\n    dataset.category_name.fillna(value=\"None\", inplace=True)\n    return (dataset)\n\ntrain = handle_missing(train)\ntest = handle_missing(test)\nprint(train.shape)\nprint(test.shape)\ntrain.isnull().sum()","17233cbb":"train.head()","be5c9fd5":"# Normalize labels, encoder brand_name & category_name text data\ndef encode_text(column):\n    le = LabelEncoder()\n    le.fit(np.hstack([train[column], test[column]]))\n    train[column+'_index'] = le.transform(train[column])\n    test[column+'_index'] = le.transform(test[column])\n    \nencode_text('brand_name')\nencode_text('general_cat')\nencode_text(\"subcat_1\")\nencode_text('subcat_2')\nencode_text('category_name')\ntrain.head()","07646774":"class Category:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n        self.n_words = 2 \n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1","288f09b4":"import re\n\n# Lowercase, trim, and remove non-letter characters\ndef normalizeString(s):\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    return s\n\ndef normalizeLine(sentence):\n    return [normalizeString(s) for s in sentence.split('\\t')]\n\n\ndef prepareData(lang1,data):\n    \n    input_cat = Category(lang1)\n    print(\"Counting words:\")\n    print(input_cat.name, end=\" \")\n    for sentence in data:\n        normalize_line = [normalizeString(s) for s in sentence.split('\\t')]\n        input_cat.addSentence(normalize_line[0])\n        \n    print(input_cat.n_words)\n    return input_cat\n\ndef indexesFromSentence(lang, sentence):\n    return [lang.word2index[word] for word in sentence.split(' ')]\n\ndef variableFromSentence(lang, sentence):\n    indexes = indexesFromSentence(lang, sentence)\n    return indexes\n\n\ndef token_fit(column):\n    raw_text = np.hstack([(train[column]).str.lower(), (test[column]).str.lower()])\n    cat1 = prepareData(column,raw_text)\n    train[column + '_seq'] = [variableFromSentence(cat1,normalizeLine(sentence.lower())[0]) for sentence in train[column]]\n    test[column + '_seq'] = [variableFromSentence(cat1,normalizeLine(sentence.lower())[0]) for sentence in test[column]]\n    ","0911cb37":"token_fit('name')\ntoken_fit('item_description')\ntrain.head()","892174cd":"# handle price using log and scale, excep test data don't have 'price' so we make test taget =0\ntest[\"target\"] = 0\ntrain[\"target\"] = np.log(train.price+1)\ntarget_scaler = MinMaxScaler(feature_range=(-1, 1))\ntrain[\"target\"] = target_scaler.fit_transform(train.target.values.reshape(-1,1))\ntrain.head(10)","712ae8d2":"pd.DataFrame(train.target).hist()","24abcb93":"# Split train\/validation data\ndtrain, dvalid = train_test_split(train, random_state=123, train_size=0.99)","360b37b9":"#SEQUENCES VARIABLES ANALYSIS\nmax_name_seq = np.max([np.max(train.name_seq.apply(lambda x: len(x))), np.max(test.name_seq.apply(lambda x: len(x)))])\nmax_item_description_seq = np.max([np.max(train.item_description_seq.apply(lambda x: len(x)))\n                                   , np.max(test.item_description_seq.apply(lambda x: len(x)))])\nprint(\"max name seq \"+str(max_name_seq))\nprint(\"max item desc seq \"+str(max_item_description_seq))\n\n#EMBEDDINGS MAX VALUE\nMAX_NAME_SEQ = 10\nMAX_ITEM_DESC_SEQ = 75\nMAX_TEXT_NAME = np.max([np.max(train.name_seq.max()) \n                   , np.max(test.name_seq.max())])+2\nMAX_TEXT_ITEM = np.max([np.max(train.item_description_seq.max()) \n                   , np.max(test.item_description_seq.max())])+2\nMAX_GEN_CATEGORY = np.max([train.general_cat_index.max(), test.general_cat_index.max()])+1\nMAX_SUB_CAT1_CATEGORY = np.max([train.subcat_1_index.max(), test.subcat_1_index.max()])+1\nMAX_SUB_CAT2_CATEGORY = np.max([train.subcat_2_index.max(), test.subcat_2_index.max()])+1\nMAX_BRAND = np.max([train.brand_name_index.max(), test.brand_name_index.max()])+1\nMAX_CONDITION = np.max([train.item_condition_id.max(), test.item_condition_id.max()])+1\nMAX_CATEGORY_NAME = np.max([train.category_name_index.max(), test.category_name_index.max()])+1","731b1756":"def pad(tensor, length):\n    if length > tensor.size(0):\n        return torch.cat([tensor, tensor.new(length - tensor.size(0), *tensor.size()[1:]).zero_()])\n    else:\n        return torch.split(tensor, length, dim=0)[0]","6d1d7e78":"# Convert ndarrays in sample to Tensors\nclass ToTensor(object):\n\n    def __call__(self, sample):\n        name, item_desc,brand_name,cat_name,general_category,subcat1_category,subcat2_category, \\\n        item_condition,shipping,target = sample['name'], sample['item_desc'], sample['brand_name'], \\\n        sample['cat_name'], sample['general_category'], sample['subcat1_category'], sample['subcat2_category'], \\\n        sample['item_condition'], sample['shipping'],sample['target']    \n        return {'name': pad(torch.from_numpy(np.asarray(name)).long().view(-1),MAX_NAME_SEQ),\n                'item_desc': pad(torch.from_numpy(np.asarray(item_desc)).long().view(-1),MAX_ITEM_DESC_SEQ),\n               'brand_name':torch.from_numpy(np.asarray(brand_name)),\n               'cat_name':torch.from_numpy(np.asarray(cat_name)),\n               'general_category':torch.from_numpy(np.asarray(general_category)),\n               'subcat1_category':torch.from_numpy(np.asarray(subcat1_category)),\n               'subcat2_category':torch.from_numpy(np.asarray(subcat2_category)),\n               'item_condition':torch.from_numpy(np.asarray(item_condition)),\n               'shipping':torch.torch.from_numpy(np.asarray(shipping)),\n               'target':torch.from_numpy(np.asarray(target))}\n\n#  Define the Dataset to use in a DataLoader\nclass MercariDataset(Dataset):\n\n    def __init__(self, data_pd, transform=None):\n        self.mercari_frame = data_pd\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.mercari_frame)\n\n    def __getitem__(self, idx):\n        name = [self.mercari_frame.name_seq.iloc[idx]]\n        item_desc = [self.mercari_frame.item_description_seq.iloc[idx]]\n        brand_name = [self.mercari_frame.brand_name_index.iloc[idx]]\n        cat_name = [self.mercari_frame.category_name_index.iloc[idx]]\n        general_category = [self.mercari_frame.general_cat_index.iloc[idx]]\n        subcat1_category = [self.mercari_frame.subcat_1_index.iloc[idx]]\n        subcat2_category = [self.mercari_frame.subcat_2_index.iloc[idx]]\n        item_condition = [self.mercari_frame.item_condition_id.iloc[idx]]\n        shipping = [self.mercari_frame.shipping.iloc[idx]]\n        target = [self.mercari_frame.target.iloc[idx]]\n        sample = {'name': name,\n                'item_desc': item_desc,\n               'brand_name': brand_name,\n               'cat_name': cat_name,   \n               'general_category': general_category,\n               'subcat1_category': subcat1_category,\n               'subcat2_category': subcat2_category,\n               'item_condition': item_condition,\n               'shipping': shipping,\n               'target': target}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample","a2c52501":"# take data into batch dataloader \n\n### Test data\nmercari_test =  MercariDataset(test,transform=transforms.Compose([ToTensor()]))           \ntest_sizes = len(mercari_test)\ntest_dataloaders = torch.utils.data.DataLoader(mercari_test, batch_size=50, shuffle=False)\n\n### Train data\nmercari_datasets = {'train': MercariDataset(dtrain,transform=transforms.Compose([ToTensor()])), \n                    'val': MercariDataset(dvalid,transform=transforms.Compose([ToTensor()]))}\ndataset_sizes = {x: len(mercari_datasets[x]) for x in ['train', 'val']}\nmercari_dataloaders = {x: torch.utils.data.DataLoader(mercari_datasets[x], batch_size=50, shuffle=True) for x in ['train', 'val']}\n\n\nprint(\"number of data in mercari train: \", dataset_sizes['train'])\nprint(\"number of batch in mercari train: \", dataset_sizes['train']\/50)\nprint()\n\nprint(\"number of data in mercari validate: \", dataset_sizes['val'])\nprint(\"number of batch in mercari validate: \", dataset_sizes['val']\/50)\nprint()\n\n\nprint(\"number of data in mercari test: \", test_sizes)\nprint(\"number of batch in mercari test: \", test_sizes\/50)\nprint()\n","dd12dfa0":"import sys\n\n# Definition of the Pytorch Model\nclass RegressionNeural(nn.Module):\n    def __init__(self, max_sizes):\n        super(RegressionNeural, self).__init__()\n        self.name_embedding = nn.Embedding(max_sizes['max_text_name'].item()+100000, 50)\n        self.item_embedding = nn.Embedding(max_sizes['max_text_item'].item()+100000, 50)\n        self.brand_embedding = nn.Embedding(max_sizes['max_brand'].item(), 10)\n        self.gencat_embedding = nn.Embedding(max_sizes['max_gen_category'].item(), 10)\n        self.subcat1_embedding = nn.Embedding(max_sizes['max_subcat1_category'].item(), 10)\n        self.subcat2_embedding = nn.Embedding(max_sizes['max_subcat2_category'].item(), 10)\n        self.condition_embedding = nn.Embedding(max_sizes['max_condition'].item(), 5)\n        self.catname_embedding = nn.Embedding(max_sizes['max_cat_name'].item(), 10)\n        \n        self.conv1_name = nn.Conv1d(50, 1, 2, stride=1)\n        self.conv2_name = nn.Conv1d(16, 8, 2, stride=1)\n        self.conv3_name = nn.Conv1d(8, 4, 2, stride=1)\n        \n        self.conv1_item_desc = nn.Conv1d(50, 1, 5, stride=5) \n        self.conv2_item_desc = nn.Conv1d(64, 16, 5, stride=1)\n        self.conv3_item_desc = nn.Conv1d(16, 4, 5, stride=1)\n        \n        self.dropout = nn.Dropout(p=0.2)\n        \n        self.input_fc1_count = 50 \n        self.fc1 = nn.Linear(self.input_fc1_count, 64)\n        self.fc2 = nn.Linear(64,32)\n        self.fc3 = nn.Linear(32,1)\n        \n        self.relu = nn.ReLU()  \n            \n    def forward(self, x, batchsize):\n        embed_name = self.name_embedding(x['name']) \n        embed_name = F.relu(self.conv1_name(embed_name.transpose(1,2)))\n        embed_item = self.item_embedding(x['item_desc'])\n        embed_item = F.relu(self.conv1_item_desc(embed_item.transpose(1,2)))\n        embed_brand = self.brand_embedding(x['brand_name'])\n        embed_gencat = self.gencat_embedding(x['general_category'])\n        embed_subcat1 = self.subcat1_embedding(x['subcat1_category'])\n        embed_subcat2 = self.subcat2_embedding(x['subcat2_category'])\n        embed_condition = self.condition_embedding(x['item_condition'])\n        embed_catname = self.catname_embedding(x['cat_name'])\n        \n        out = torch.cat((embed_brand.view(batchsize,-1), embed_catname.view(batchsize,-1), \\\n                         embed_condition.view(batchsize,-1),embed_name.view(batchsize,-1), \\\n                         embed_item.view(batchsize,-1),x['shipping']),1)\n        \n        out = (self.fc1(out))\n        out = F.relu(self.dropout(out))\n        out = (self.fc2(out))\n        out = (self.dropout(out))\n        out = self.fc3(out)\n        return out\n\nmax_sizes = {'max_text_name':MAX_TEXT_NAME,'max_text_item':MAX_TEXT_ITEM,'max_name_seq':MAX_NAME_SEQ,'max_item_desc_seq':MAX_ITEM_DESC_SEQ, \\\n             'max_brand':MAX_BRAND,'max_cat_name':MAX_CATEGORY_NAME,'max_gen_category':MAX_GEN_CATEGORY,\\\n             'max_subcat1_category':MAX_SUB_CAT1_CATEGORY,'max_subcat2_category':MAX_SUB_CAT2_CATEGORY,\\\n             'max_condition':MAX_CONDITION} \n\ndeep_learn_model = RegressionNeural(max_sizes)","810dd52b":"max_sizes","7c880b4c":"def train_model(model, criterion, optimizer, num_epochs=1, print_every = 1000, device=\"cpu\"):\n    start = time.time()\n\n    best_acc = 0.0\n    print_loss_total = 0 \n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train(True)  # Set model to training mode\n            else:\n                model.train(False)  # Set model to evaluate mode\n\n            running_loss = 0.0\n            num_batches = dataset_sizes[phase]\/50.\n            print(\"number of batch in\", phase, np.uint64(num_batches))\n            print(\"comming....\")\n            \n            for i_batch, sample_batched in enumerate(mercari_dataloaders[phase]): \n            # get inputs\n                inputs = {'name':Variable(sample_batched['name']).to(device), \n                          'item_desc':Variable(sample_batched['item_desc']).to(device), \\\n                        'brand_name':Variable(sample_batched['brand_name']).to(device), \\\n                        'cat_name':Variable(sample_batched['cat_name']).to(device), \\\n                        'general_category':Variable(sample_batched['general_category']).to(device), \\\n                        'subcat1_category':Variable(sample_batched['subcat1_category']).to(device), \\\n                        'subcat2_category':Variable(sample_batched['subcat2_category']).to(device), \\\n                        'item_condition':Variable(sample_batched['item_condition']).to(device), \\\n                        'shipping':Variable(sample_batched['shipping'].float()).to(device)}\n                \n                # get price\n                prices = Variable(sample_batched['target'].float().to(device))  \n        \n                batch_size = len(sample_batched['shipping'])   \n                optimizer.zero_grad()\n                \n                model.to(device)\n                prices.to(device)\n                \n                outputs = model(inputs, batch_size)\n                loss = criterion(outputs, prices)\n\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n\n                running_loss += loss.data\n                print_loss_total += loss.data                \n                \n                if (i_batch+1) % print_every == 0:\n                    print_loss_avg = print_loss_total \/ print_every\n                    print_loss_total = 0\n                    print('(%d %d%%) Loss_avg: %.4f' % (i_batch, i_batch \/ num_batches*100, print_loss_avg), end=', ')\n                    time_ongoing = time.time() - start\n                    print('Training in {:.0f}m {:.0f}s'.format(time_ongoing \/\/ 60, time_ongoing % 60))\n            epoch_loss = running_loss \/ num_batches\n            print('{} Loss: {:.4f}'.format(phase, epoch_loss), end=\"\\n\\n\")\n            \n        print()\n\n    time_elapsed = time.time() - start\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n\n    return model        ","c47a42c2":"criterion = nn.MSELoss()\noptimizer_ft = optim.SGD(deep_learn_model.parameters(), lr=0.001, momentum=0.9)\n\n\ndevice = torch.device('cuda')\ntrain_model(deep_learn_model,criterion,optimizer_ft,num_epochs=20, device=device)","010296a2":"# predict the model results against test data\ndef validate(model, dataloader, print_every = 500, device = 'cpu'):\n    start = time.time()\n    num_batches = len(dataloader)\n    print('num batches: ',num_batches)\n    \n    y_pred_full = np.array([])\n    for i_batch, sample_batched in enumerate(dataloader): \n        inputs = {'name':Variable(sample_batched['name']).to(device), \n                  'item_desc':Variable(sample_batched['item_desc']).to(device), \\\n                  'brand_name':Variable(sample_batched['brand_name']).to(device), \\\n                  'cat_name':Variable(sample_batched['cat_name']).to(device), \\\n                  'general_category':Variable(sample_batched['general_category']).to(device), \\\n                  'subcat1_category':Variable(sample_batched['subcat1_category']).to(device), \\\n                  'subcat2_category':Variable(sample_batched['subcat2_category']).to(device), \\\n                  'item_condition':Variable(sample_batched['item_condition']).to(device), \\\n                  'shipping':Variable(sample_batched['shipping'].float()).to(device)}\n        batch_size = len(sample_batched['shipping'])\n        \n        model.to(device)\n        try:\n            outputs = model(inputs,batch_size)\n\n            val_preds = target_scaler.inverse_transform(outputs.cpu().data.numpy())\n            val_preds = np.exp(val_preds)-1\n            y_pred = val_preds[:,0]\n        except:\n            print(i_batch, \"err, make 0 price\")\n            y_pred = np.zeros(batch_size)\n        \n        y_pred_full= np.append(y_pred_full,y_pred)\n        if (i_batch+1) % print_every == 0:\n            print('(%d %d%%)' % (i_batch, i_batch \/ num_batches*100), end=\",\")\n#             print('inputname shape, input item desc:', inputs['name'].shape, inputs['item_desc'].shap)\n             \n    return y_pred_full","5b4fbc83":"y_pred_test = validate(deep_learn_model,test_dataloaders,device=device)","51ccd56c":"submit = pd.DataFrame({'test_id': test['test_id'], \n                    'price': y_pred_test})\nsubmit.to_csv('submission.csv', index=False)","007b3d6f":"submit","063b2b17":"# !pip install torchviz\n# from torchviz import make_dot\n\n# for i_batch, sample_batched in enumerate(mercari_dataloaders['train']): \n#     inputs = {'name':Variable(sample_batched['name']), 'item_desc':Variable(sample_batched['item_desc']), \\\n#         'brand_name':Variable(sample_batched['brand_name']), \\\n#         'cat_name':Variable(sample_batched['cat_name']), \\\n#         'general_category':Variable(sample_batched['general_category']), \\\n#         'subcat1_category':Variable(sample_batched['subcat1_category']), \\\n#         'subcat2_category':Variable(sample_batched['subcat2_category']), \\\n#         'item_condition':Variable(sample_batched['item_condition']), \\\n#         'shipping':Variable(sample_batched['shipping'].float())}\n#     prices = Variable(sample_batched['target'].float())   \n#     batch_size = len(sample_batched['shipping'])\n#     if i_batch ==0:\n#         a = inputs\n#         break\n    \n    \n# batch = a\n# deep_learn_model.to('cpu')\n# deep_learn_model.eval()\n# yhat = deep_learn_model(batch, 50)\n# make_dot(yhat, params=dict(list(deep_learn_model.named_parameters()))).render(\"rnn_torchviz\")","6bdcdd4d":"from sklearn.metrics import mean_squared_log_error\ndef RMSLE(y_true:np.ndarray, y_pred:np.ndarray) -> np.float64:\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))","c3b77bb1":"y_pred_val = validate(deep_learn_model,mercari_dataloaders['val'],device=device)\ny_true_val = dvalid['price']\n\nprint('RMSLE in validate data: ', RMSLE(y_true_val, y_pred_val))","5ab7d6eb":"y_pred_train = validate(deep_learn_model,mercari_dataloaders['train'],device=device)\ny_true_train = dtrain['price']\n\nprint('RMSLE in validate data: ', RMSLE(y_true_train, y_pred_train))","297cbc74":"# 4. Chu\u1ea9n b\u1ecb m\u00f4 h\u00ecnh","c5a33114":"# 2. Ph\u00e2n t\u00edch d\u1eef li\u1ec7u","16e14d3f":"# 1. Unzip, load d\u1eef li\u1ec7u train\/test\n* D\u1eef li\u1ec7u train n\u1eb1m trong file train.tsv.7z\n* D\u1eef li\u1ec7u test n\u1eb1m trong file test_stg2.tsv.zip","e9b57be6":"# 3. X\u1eed l\u00fd d\u1eef li\u1ec7u:\n- X\u1eed l\u00fd d\u1eef li\u1ec7u tr\u1ed1ng\n- M\u00e3 h\u00f3a 'brand_name', 'general_cat', 'subcat_1', 'subcat_2', 'category_name'\n- Chu\u1ea9n h\u00f3a, lowercase, lo\u1ea1i b\u1ecf k\u00fd t\u1ef1 kh\u00f4ng h\u1ee3p l\u1ec7, ph\u00e2n \u0111o\u1ea1n chu\u1ed7i k\u1ef9 t\u1ef1 c\u1ee7a 'name' v\u00e0 'item_description', m\u00e3 h\u00f3a ch\u00fang v\u00e0 ghi v\u00e0o tr\u01b0\u1eddng m\u1edbi \u0111u\u00f4i _seq"}}