{"cell_type":{"370dad04":"code","bd130c69":"code","0949cecb":"code","18f2ff34":"code","cf171ba3":"code","fd8108aa":"code","cca854d5":"code","874cef7a":"code","e8c99623":"code","07cb4002":"code","010175a8":"code","e3f63ab6":"code","f81ce485":"code","d50876fd":"code","10e926ad":"markdown"},"source":{"370dad04":"import glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","bd130c69":"# hyperparameter setting\nimage_size = 196\nbatch_size = 16\nepochs = 30","0949cecb":"# input files\nbase_dir = '..\/input\/chest-xray-pneumonia\/chest_xray'\n\npneumonia_files = glob.glob(base_dir + '\/**\/PNEUMONIA\/*.jpeg')\nnormal_files = glob.glob(base_dir + '\/**\/NORMAL\/*.jpeg')\n\nprint(\"pneumonia_files : {}\".format(len(pneumonia_files)))\nprint(\"normal_files : {}\".format(len(normal_files)))","18f2ff34":"# splitting the data\n# split the total into 8:2 (train + val : test) \n# and split the first part into 8:2 again (train : val)\ntrain_pneumonia, test_pneumonia = train_test_split(pneumonia_files, test_size=0.2)\ntrain_pneumonia, val_pneumonia = train_test_split(train_pneumonia, test_size=0.2)\n\ntrain_normal, test_normal = train_test_split(normal_files, test_size=0.2)\ntrain_normal, val_normal = train_test_split(train_normal, test_size=0.2)\n\nprint(len(train_pneumonia), len(val_pneumonia), len(test_pneumonia))\nprint(len(train_normal), len(val_normal), len(test_normal))","cf171ba3":"# adding labels (pneumonia: 1, normal: 0)\ntrain_data = []\nval_data = []\ntest_data = []\n\nfor sample in train_pneumonia:\n    train_data.append([sample, '1'])\n\nfor sample in val_pneumonia:\n    val_data.append([sample, '1'])\n    \nfor sample in test_pneumonia:\n    test_data.append([sample, '1'])\n    \nfor sample in train_normal:\n    train_data.append([sample, '0'])\n    \nfor sample in val_normal:\n    val_data.append([sample, '0'])\n    \nfor sample in test_normal:\n    test_data.append([sample, '0'])\n    \n# convert them into dataframes\ntrain_df = pd.DataFrame(train_data, columns=['image', 'label'])\nval_df = pd.DataFrame(val_data, columns=['image', 'label'])\ntest_df = pd.DataFrame(test_data, columns=['image', 'label'])\n\nprint(len(train_df), len(val_df), len(test_df))","fd8108aa":"# data augmentation (train)\ntrain_image_gen = ImageDataGenerator(rescale=1.\/255,\n                                     rotation_range=10,\n                                     zoom_range=0.1,\n                                     width_shift_range=0.1,\n                                     height_shift_range=0.1)\n\ntrain_set = train_image_gen.flow_from_dataframe(dataframe=train_df,\n                                                x_col='image',\n                                                y_col='label',\n                                                target_size=(image_size, image_size),\n                                                batch_size=batch_size,\n                                                shuffle=True,\n                                                class_mode='binary',\n                                                color_mode='grayscale')\n\n# rescaling (validation)\nval_rescaled = ImageDataGenerator(rescale = 1.\/255)\n\nval_set = val_rescaled.flow_from_dataframe(dataframe=val_df,\n                                           x_col='image',\n                                           y_col='label',\n                                           target_size=(image_size, image_size),\n                                           batch_size=batch_size,\n                                           shuffle=True,\n                                           class_mode='binary',\n                                           color_mode='grayscale')\n\n# rescaling (test)\ntest_rescaled = ImageDataGenerator(rescale = 1.\/255)\n\ntest_set = test_rescaled.flow_from_dataframe(dataframe=test_df,\n                                             x_col='image',\n                                             y_col='label',\n                                             target_size=(image_size, image_size),\n                                             batch_size=1,\n                                             shuffle=False,\n                                             class_mode='binary',\n                                             color_mode='grayscale')","cca854d5":"# building the CNN model \nmodel = Sequential([\n    Conv2D(32, (3,3), activation='relu', padding='same', \n           input_shape=(image_size, image_size, 1)),\n    Conv2D(32, (3,3), activation='relu', padding='same'),\n    MaxPool2D(pool_size=(2,2)),\n    \n    Conv2D(64, (3,3), activation='relu', padding='same'),\n    Conv2D(64, (3,3), activation='relu', padding='same'),\n    MaxPool2D(pool_size=(2,2)),\n    \n    Conv2D(128, (3,3), activation='relu', padding='same'),\n    Conv2D(128, (3,3), activation='relu', padding='same'),\n    Dropout(0.2),\n    MaxPool2D(pool_size=(2,2)),\n    \n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(1, activation='sigmoid')\n])","874cef7a":"# model configuration\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nlr_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 3, \n                                 verbose=1,factor=0.5, min_lr=0.000001)","e8c99623":"# training the model\nhistory = model.fit_generator(train_set,\n                              steps_per_epoch=len(train_set),\n                              epochs=epochs,\n                              validation_data=val_set,\n                              validation_steps=len(val_set),\n                              callbacks=[lr_reduction])","07cb4002":"# plotting the accuracy and loss of the model\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\nfor i, e in enumerate(['accuracy', 'loss']):\n    ax[i].plot(history.history[e])\n    ax[i].plot(history.history['val_' + e])\n    ax[i].set_title(e, fontsize=20)\n    ax[i].set_xlabel('epochs', fontsize=15)\n    ax[i].set_ylabel(e, fontsize=15)\n    ax[i].legend(['train', 'val'])","010175a8":"# testing the model\npredictions = model.predict_classes(test_set)\n\ncm = confusion_matrix(test_set.classes, predictions)\ncm_pd = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])","e3f63ab6":"plt.figure(figsize = (6,5))\nsns.heatmap(cm_pd,cmap= \"Blues\", linecolor = 'black' , \n            linewidth = 1 , annot = True, fmt='',\n            xticklabels = ['Predicted Normal', 'Predicted Pneumonia'],\n            yticklabels = ['Actual Normal', 'Actual Pneumonia'])\nplt.yticks(rotation=0)\nplt.show()","f81ce485":"test_accuracy = (cm[0, 0] + cm[1, 1]) \/ len(test_set)\nprint(\"Test Accuracy: {}%\".format(round(test_accuracy * 100, 3)))","d50876fd":"# saving the model to use it in the web application\nmodel.save('\/kaggle\/working\/CNN_model.h5')","10e926ad":"### The purpose of this notebook\n- I worked on a project using tensorflow and python django framework. It's a simple web application to predict the probability of pneumonia for a chest X-ray image and archive the image files with the calculated results.\n- For this purpose, I trained and tested a CNN model in this notebook, and saved it to use it in the application.\n- For those who are interested, I leave the url of the github repository below (which includes a short setup guide and demo video). \n- I have tried to deploy it in AWS EC2 (free tier) but couldn't because of the memory shortage. Please help me if someone has an idea for this.\n- https:\/\/github.com\/samuelkim7\/pneumonia_cnn\n\n### Imbalanced distribution of the dataset \n- Because of imbalanced distribution of the dataset, I think there were some confusion in choosing train, validation, and test set from it.\n- So I have put them altogether and split them into the ratio of 8:2 (train + val : test). And then I split the first part into the ratio of 8:2 again (train : val).\n- I think this approach resulted in a high test accuracy (96.246%).\n- I didn't use batch normalization because in this setting, this seemed to result in some fluctuation in the validation accuracy.\n- I am not an expert in this area (came from developer background), so I am not sure if this approach is proper. Please leave the comment below if it's not.\n\n### Giving thanks\n- I have looked up the following notebooks on this subject and give thanks to those who wrote them.\n- https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays\n- https:\/\/www.kaggle.com\/michalbrezk\/x-ray-pneumonia-cnn-tensorflow-2-0-keras-94\n- https:\/\/www.kaggle.com\/pcbreviglieri\/covid-19-revisiting-pneumonia-detection"}}