{"cell_type":{"de2d1737":"code","523891c2":"code","3320d8a4":"code","ca8d86f8":"code","a6717ad0":"markdown"},"source":{"de2d1737":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns","523891c2":"train=pd.read_csv('\/kaggle\/input\/learn-together\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/learn-together\/test.csv')\n\ntrain.drop('Id', axis=1, inplace=True)\ny_train = train.pop('Cover_Type')\ntest_ids = test.pop('Id')","3320d8a4":"n_splits = 3\n\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=2019)\nyoof = np.zeros(len(train))\nyhat = np.zeros(len(test))\n\nfold = 0\nn_est = 2000\nlr = 0.2\nfor in_index, oof_index in kf.split(train, y_train):\n    fold += 1\n    print(f'fold {fold} of {n_splits}')\n    X_in, X_oof = train.values[in_index], train.values[oof_index]\n    y_in, y_oof = y_train.values[in_index], y_train.values[oof_index]\n\n    model = XGBClassifier(n_estimators=n_est, learning_rate=lr, random_state=2019, tree_method = 'gpu_exact')\n\n    model.fit(X_in, y_in, early_stopping_rounds=20, eval_set=[(X_oof, y_oof)], verbose=100,eval_metric=['merror', 'mlogloss'])\n\n    print('## lr:',lr,'n_est:',n_est)\n    print('Best iteration: '+ str(model.best_iteration), 'Best ntree_limit: '+ str(model.best_ntree_limit), 'Best score:', str(model.best_score))\n\n    yoof[oof_index] = model.predict(X_oof)\n    yhat += model.predict(test.values)\n\nyhat \/= n_splits\n","ca8d86f8":"cm=confusion_matrix(y_train, yoof)\nsns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=range(1,8), yticklabels=range(1,8))\nprint('Accuracy:', accuracy_score(y_train, yoof))\n\n","a6717ad0":"# Intro\nThis is basicly a XGBoostClassifier starter. It contains KFolding and early stopping."}}