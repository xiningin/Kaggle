{"cell_type":{"57a3576f":"code","816defc2":"code","a33cd6e7":"code","f9c5228b":"code","39e4f46c":"code","cdb33226":"code","49803855":"code","f49d23ab":"code","187e535b":"code","2069c574":"code","a3e02a28":"code","88a755a9":"code","59af6aa7":"code","98e69eb8":"code","b04a957d":"code","03b3dc0d":"code","93297c7a":"code","67810c39":"code","cf2083f1":"markdown","e3629e48":"markdown","e3ace83e":"markdown","30097667":"markdown","b3c0dfaa":"markdown","d68385e9":"markdown","0708e3a3":"markdown","816b2eac":"markdown","65fd8db1":"markdown","a2f8546e":"markdown","847a4bc6":"markdown","167f43f5":"markdown","b5f01622":"markdown","2c7c2799":"markdown","0366c660":"markdown","dc854cab":"markdown","f61b40cf":"markdown","018172dd":"markdown","96a621f4":"markdown","55a8fafb":"markdown","8352c25d":"markdown","a7657c2b":"markdown","fea5cd91":"markdown","20243bf9":"markdown","70876f98":"markdown"},"source":{"57a3576f":"# Import order: data manipulating -> machine\/deep learning -> utilities\/helpers\/improvement -> configuration\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\n\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\n\nfrom joblib import Parallel, delayed\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)\n\n# Define data_directory and data_read func\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test","816defc2":"# Need to define the problem before we want to solve (could be a iterative way while we collec more and more information)\nsample = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/sample_submission.csv\")\nsample","a33cd6e7":"test = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/test.csv\")\ntest","f9c5228b":"train = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/train.csv\")\ntrain","39e4f46c":"book_example = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0')\nbook_example","cdb33226":"trade_example = pd.read_parquet(\"..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id=0\")\ntrade_example","49803855":"# Calculate 1st WAP\ndef calc_wap1(df):\n    wap = (df['ask_price1'] * df['bid_size1'] + df['bid_price1'] * df['ask_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n# Calculate 2nd WAP\ndef calc_wap2(df):\n    wap = (df['ask_price2'] * df['bid_size2'] + df['bid_price2'] * df['ask_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap","f49d23ab":"# Calculate Log Return\ndef log_return(series):\n    return np.log(series).diff() # log(x \/ y) = log(x) - log(y), ref[2]","187e535b":"# Realized Volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))","2069c574":"# Count Unique Elements of Series\ndef count_unique(series):\n    return len(np.unique(series))","a3e02a28":"# Calculate features as specifized feature_dict\ndef calc_features(df, feature_dict):\n    # Calculate STATs (sum, mean, std) for different time-window (seconds in bucket)\n    def calc_certain_window(window, add_suffix=False):\n        # Filter by time-window, Groupy by time_id, then Apply feature_dict\n        df_feature = df[df['seconds_in_bucket'] >= window].groupby(['time_id']).agg(feature_dict).reset_index()\n        # Rename features\/columns by joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix for different time-window\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(window))\n        return df_feature\n    \n    windows = [0, 150, 300, 450]\n    df_feature = pd.DataFrame()\n    \n    for window in windows:\n        if window == 0:\n            df_feature = calc_certain_window(window=window, add_suffix=False)\n        else:\n            df_feature_tmp = calc_certain_window(window=window, add_suffix=True)\n            df_feature = df_feature.merge(df_feature_tmp, how='left', left_on='time_id_', right_on='time_id__'+str(window))\n        \n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n        \n    return df_feature","88a755a9":"# Preprocess book-data (applied for each stock_id)\ndef preprocess_book(file_path):\n    df = pd.read_parquet(file_path)\n    \n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # Calculate Log-Return\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate Wap-Balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate Various-Spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Feature (Generating) Dict for aggregated operations\n    feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std]\n    }\n    \n    df_feature = calc_features(df, feature_dict=feature_dict)\n    \n    # Generate row_id (for later merge)\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    # Drop the left time_id_ (after using for generating row_id)\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    \n    return df_feature","59af6aa7":"# Preprocess trade-Data (applied for each stock_id)\ndef preprocess_trade(file_path):\n    df = pd.read_parquet(file_path)\n    \n    # Calculate Log-Return\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Feature (Generating) Dict for aggregated operations\n    feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    df_feature = calc_features(df, feature_dict=feature_dict)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    \n    return df_feature","98e69eb8":"# Preprocess\/Feature-Engineering in parallel (applied for each stock_id)\ndef preprocess(list_stock_ids, is_train=True):\n    \n    def preprocess_for_stock_id(stock_id):\n        # Generate file_path for train-dataset\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # ... for test-dataset\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book- and trade- data, then merge both\n        df_tmp = pd.merge(preprocess_book(file_path_book), preprocess_trade(file_path_trade), on='row_id', how='left')\n\n        return df_tmp\n    \n    # Parallelize Preprocessing for Every stock_id\n    df = Parallel(n_jobs=-1, verbose=1)(delayed(preprocess_for_stock_id)(stock_id) for stock_id in list_stock_ids)\n    \n    # Concatenate All Dataframes from Parallelized Preprocessing\n    df = pd.concat(df, ignore_index=True)\n    \n    return df","b04a957d":"# Calculate STATs (mean, std, max, min) for realized volatility while groupped by stock_id and time_id\ndef get_time_stock(df_feature):\n    # Enumerate realized volatility features\/columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n\n    # Group by the stock id\n    df_stock_id = df_feature.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df_feature.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df_feature = df_feature.merge(df_stock_id, how='left', left_on=['stock_id'], right_on=['stock_id__stock'])\n    df_feature = df_feature.merge(df_time_id, how='left', left_on=['time_id'], right_on=['time_id__time'])\n    df_feature.drop(['stock_id__stock', 'time_id__time'], axis=1, inplace=True)\n    \n    return df_feature","03b3dc0d":"# Calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","93297c7a":"def train_and_evaluate(train, test):\n    # Hyperparammeters (basic here, could be tuned with GridSearch later)\n    params = {\n      'objective': 'rmse',  \n      'boosting_type': 'gbdt',\n      'num_leaves': 100,\n      'n_jobs': -1,\n      'learning_rate': 0.1,\n      'feature_fraction': 0.8,\n      'bagging_fraction': 0.8,\n      'verbose': -1\n    }\n    \n    # Split features and target\n    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n    y = train['target']\n    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n    \n    # Transform stock id to a numeric value\n    x['stock_id'] = x['stock_id'].astype(int)\n    x_test['stock_id'] = x_test['stock_id'].astype(int)\n    \n    # Create out of folds array\n    oof_predictions = np.zeros(x.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(x_test.shape[0])\n    # Create a KFold object\n    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n        model = lgb.train(params = params, \n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          num_boost_round = 10000, \n                          early_stopping_rounds = 50, \n                          verbose_eval = 50,\n                          feval = feval_rmspe)\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val)\n        # Predict the test set\n        test_predictions += model.predict(x_test) \/ 5\n        \n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    # Return test predictions\n    return test_predictions","67810c39":"# Read train and test\ntrain, test = read_train_test()\n\n# Get unique stock_id (as prediction by stock_id)\ntrain_stock_ids = train['stock_id'].unique()\n\n# Generate features\ntrain_feature = preprocess(train_stock_ids, is_train=True)\n# Merge with intiail train data\ntrain = train.merge(train_feature, on=['row_id'], how='left')\n\n# Same for test datas\ntest_stock_ids = test['stock_id'].unique()\ntest_feature = preprocess(test_stock_ids, is_train=False)\ntest = test.merge(test_feature, on=['row_id'], how='left')\n\n# Further generate features with realized-volatility\ntrain = get_time_stock(train)\ntest = get_time_stock(test)\n\n# Traing and evaluate\ntest_predictions = train_and_evaluate(train, test)\n\n# Save test predictions\ntest['target'] = test_predictions\ntest[['row_id', 'target']].to_csv('submission.csv', index=False)","cf2083f1":"As this notebook focuses on building the baseline, we will be \"a bit lazy\" on tuning the model for better performance rather than using basic hyperparams. \n\nThere should be a lot of room for me to discuss gridsearch, local testing\/debug env, perhaps using cloud computing for a better speed (take around 30mins for training on Kaggle and about 15mins on my 2019 MacBook). So, see me in another notebook soon! ","e3629e48":"**Realized Volatility**   \nMotivation - need a scalar to evaluate volatility   \nWhy square - measure changes in 2 directions (postivie\/negative)   \nWhy square root - back to original unit (a typical math operation, as for meter -> suare will convert to square meter, so we root it back)","e3ace83e":"**WAP**  \nMotivation - take price and size of orders into considertaion","30097667":"# Summary","b3c0dfaa":"# Feature Engineering","d68385e9":"# Libs Import & Dataset Setup","0708e3a3":"This is my first attended live competition since learning quite a few about machine learning... Looking back, I would like to say that we perhaps don't need to learn that much in the 1st place while ignoring how important of applying them to solving problems (so you will master them deeper and faster finally!). \n\nSo like the Ockham's Razor, try to learn the basics enough for us to start working on a warm-up problem. Then we perhaps will have quite a few questions for sure and things we think \"ugh...guess I need to learn a bit more about this\". Then we \"come back online school\" to learn them by priorities.\n\nLearn around \"enough\" -> Work on a problem (a bit more than comfortable) -> Resolve questions and learning points -> Next practice...\n\nHmm, seems we got our iteration\/loop\/sprint (as you want ;) to be a Machine Learning Engineer?!","816b2eac":"**Version Notes**\n- V2.0 - here the [[Model Tuning] LGBM Baseline](https:\/\/www.kaggle.com\/austinzhao\/model-tuning-lgbm-baseline), improved a bit & shared ideas | 3rd Sep 2021\n- V1.0 - 1st published & submitted version | 29th Aug 2021\n\n**Work Principles**\n- A clean & comfortable format to help our brains digest\n- Occam's Razor - Non sunt multiplicanda entia sine necessitate\n- Refactoring \n\n**General Notes**\n- This notebook will be focusing on building the baseline, and further improvement, like model tuning and features improving, will be added in the following work.","65fd8db1":"# Trainning & Evaluating","a2f8546e":"Luckily, we had this dataset with a relatively clean format (no lots of None or NaN values we need to clean first). Guess this credit should be due to the data from the online trading platform -- data got formatted and specified already. \n\nSo we will have a brief check over book's and trade's train and test data, also the submission format. Just have a general idea about which columns we have here.","847a4bc6":"**Utility Func - Calculate Features by Specifized feature_dict**","167f43f5":"**Log Return**   \nMotivation - quantify stock prices changes   \nWhy log - the addtivie attribute by log (so \"addable\" for different time window), and possible to go below 100% as ref[1]","b5f01622":"# Utility Funcs ","2c7c2799":"A good point I learned from ref[2] is the \"single-entry\" for all steps, which offers us a clear view of each step. And feel free to dig into any of them to make improvements for us! ","0366c660":"**Preprocess(Generate Features) Func for Book & Trade Data**","dc854cab":"**Further Feature Generating - Utlize Realized Volatility under Different Time Window**","f61b40cf":"As ref[2], a general 3-step will be used to generate features for our later training:\n1. Compute basic quantities used in the problem field as ref[1] -- 1st-level features\n2. Combine with aggregated STAT (sum, mean, std) operations on 1st-level under different time windows -- 2nd-level features\n3. Cheery-pick Realized-Volatility under different time windows to apply STAT (mean, std, max, min) operations -- 3rd-level features\n\nA reflection point here is, for primarily numerical data, we could use these STAT operations to form 2nd-level features to see if it helps. Also believe, there will be improvement room with time series.","018172dd":"[[1] Introduction to financial concepts and data](https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data)   \n[[2] Optiver Realized Volatility LGBM Baseline](https:\/\/www.kaggle.com\/ragnar123\/optiver-realized-volatility-lgbm-baseline)","96a621f4":"A valuable point from ref[2] is that the parallized processing, as for each stock_id (no overlap). Should check [Embarrassingly parallel for loops](https:\/\/joblib.readthedocs.io\/en\/latest\/parallel.html) to master this \"booster\". ","55a8fafb":"# Reference","8352c25d":"# Overview\n- Libs Import & Dataset Setup\n- EDA\n- Model\n    - Utility Funcs\n    - Feature Engineering\n    - Trainning & Evaluating\n    - Main Func\n- Summary\n- Reference","a7657c2b":"# Main Func","fea5cd91":"# EDA","20243bf9":"Reproduction is a good way for me\/us to quickly build the initial baseline and momentum on a new problem, also a scientific way to prove the proposed approach while answering quite a few what & why questions in our mind. (Of course, we should raise our concerns when we found something perhaps not that many senses)\n\nSo don't be shy, even feeling shame on \"redo\" the work. The difference between a \"reproduction\" and a \"copy & paste\" is full of Q & A, which helps us know the problem and further build a better solution with our dimensions.\n\nAppreciate the work [1] [2] I referred and learned from to help me reach here.\n\n(Here the following model tuning https:\/\/www.kaggle.com\/austinzhao\/model-tuning-lgbm-baseline added on 3rd Sep 2021)","70876f98":"**The Preprocess Entry Func - Call Parallelized Preprocessing for Train\/Test Data**"}}