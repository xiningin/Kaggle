{"cell_type":{"33423e00":"code","6a4cae82":"code","716e874d":"code","4f8516a6":"code","ceacbf10":"code","a5e7b008":"code","c0f154dd":"code","3f957c53":"code","0e3b2819":"code","d6026658":"code","3b7df0e1":"code","03059c0f":"code","adadfa4e":"code","afd28a54":"code","c699243b":"code","be50ff5f":"code","977bb384":"markdown","9762a68a":"markdown","a8e42dac":"markdown","48c44c75":"markdown","70541113":"markdown"},"source":{"33423e00":"# Importing Libraries\nimport os\nimport random\nimport math\nimport sys\nimport gc\n\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport cv2\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\n\n\nimport torch\nimport albumentations\nfrom albumentations.pytorch import ToTensorV2\nimport torch.nn.functional as F\nfrom torch.nn import BCEWithLogitsLoss\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\n\n\nsys.path.append(\"..\/input\/timmmaster\/\")\nfrom timm import create_model","6a4cae82":"# Constants  \nimg_size = 384\nBatch_size = 8\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nepochs = 2\n\n# Directories\ntrain_dir = \"\/kaggle\/input\/petfinder-pawpularity-score\/train\/\"\ntest_dir = \"\/kaggle\/input\/petfinder-pawpularity-score\/test\/\"\n\n\ndef seed_everything():\n    os.environ['PYTHONHASHSEED'] = str(123)\n    np.random.seed(123)\n    random.seed(123)\n    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n    os.environ['PYTHONHASHSEED'] = str(123)\n\nseed_everything()","716e874d":"# Reading dataset train, test in df and df_test respectively\ndf = pd.read_csv(\"\/kaggle\/input\/petfinder-pawpularity-score\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/petfinder-pawpularity-score\/test.csv\")\nId = df_test[\"Id\"].copy()\n\n\n# Converting Id column for taking images\ndf[\"Id\"] = df[\"Id\"].apply(lambda x : \"\/kaggle\/input\/petfinder-pawpularity-score\/train\/\" + x + \".jpg\")\ndf_test[\"Id\"] = df_test[\"Id\"].apply(lambda x : \"\/kaggle\/input\/petfinder-pawpularity-score\/test\/\" + x + \".jpg\")\n\n\n# Dropping unwanted columns and shuffling dataset\ndense_features = [\"Subject Focus\", \"Eyes\", \"Face\", \"Near\", \"Action\", \"Accessory\", \n                 \"Group\", \"Collage\", \"Human\", \"Occlusion\", \"Info\", \"Blur\"]\ndf = df.sample(frac = 1).reset_index(drop = True)\n\n# For treating problem as classification\ndf[\"Pawpularity\"] = df[\"Pawpularity\"] \/ 100.0","4f8516a6":"df.sample(5)","ceacbf10":"# Defining Dataset class\nclass Dataset:\n    def __init__(self, img_paths, targets, augmentations, mixup = False):\n        self.img_paths = img_paths\n        self.targets = targets\n        self.augmentations = augmentations\n        self.mixup = mixup\n    \n    def __len__(self):\n        return len(self.img_paths)\n    \n    def __getitem__(self, item):\n        if self.mixup: # For Mixup\n            image1 = cv2.imread(self.img_paths[item])\n            image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n            target1 = self.targets[item]\n\n            img_idx = random.randint(0, len(self.img_paths)-1)\n            image2 = cv2.imread(self.img_paths[img_idx])\n            image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n            target2 = self.targets[img_idx]\n            \n            if self.augmentations is not None:\n                augmented = self.augmentations(image=image1)\n                image1 = augmented[\"image\"]\n                augmented = self.augmentations(image=image2)\n                image2 = augmented[\"image\"]\n\n            alpha = 0.8\n            lam = np.random.beta(alpha, alpha)\n            image = lam * image1 + (1 - lam) * image2\n            target = lam * target1 + (1- lam) * target2\n            \n        else: # Without Mixup\n            image = cv2.imread(self.img_paths[item])\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            if self.augmentations is not None:\n                augmented = self.augmentations(image=image)\n                image = augmented[\"image\"]\n                \n            target = self.targets[item]\n        \n        return {\n            \"images\" : image,\n            \"labels\" : torch.tensor(target, dtype=torch.float)\n        }","a5e7b008":"# For augmentation\naug = albumentations.Compose(\n    [\n        albumentations.Resize(img_size, img_size, p=1),\n        albumentations.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n            max_pixel_value=255.0,\n            p=1.0,\n        ),\n        albumentations.HueSaturationValue(\n            hue_shift_limit=0.2, sat_shift_limit=0.2,\n            val_shift_limit=0.2, p=0.5\n        ),\n        albumentations.RandomBrightnessContrast(\n            brightness_limit=(-0.1, 0.1),\n            contrast_limit=(-0.1, 0.1), p=0.5\n        ),\n        ToTensorV2(p=1.0)\n    ]\n)\n\ntest_aug = albumentations.Compose(\n    [\n        albumentations.Resize(img_size, img_size, p=1),\n        albumentations.Normalize(\n              mean=[0.485, 0.456, 0.406],\n              std=[0.229, 0.224, 0.225],\n              max_pixel_value=255.0,\n          ),\n        ToTensorV2(p=1.0)\n    ]\n)","c0f154dd":"# Creating bins using Struges rule\nnum_bins = int(1 + (3.3 * np.log2(len(df))))\n\ndf[\"bins\"] = pd.cut(df[\"Pawpularity\"], bins = num_bins, labels = False)\ndf[\"bins\"].hist()","3f957c53":"# Creating folds\ndf['fold'] = -1\n\n\nN_FOLDS = 5\nstrat_kfold = StratifiedKFold(n_splits=N_FOLDS, random_state=1024, shuffle=True)\nfor i, (_, train_index) in enumerate(strat_kfold.split(df.index, df['bins'])):\n    df.iloc[train_index, -1] = i\n    \ndf['fold'] = df['fold'].astype('int')","0e3b2819":"# Creating Test datset\ntest_dataset = Dataset(\n    img_paths = df_test[\"Id\"].values,\n    targets = np.ones(len(df_test)),\n    augmentations = test_aug\n)\n\ntest_loader = DataLoader(test_dataset, batch_size = 1) # batch_size = 1 for predicting values one by one","d6026658":"# Defining model\ndef swin():\n    return create_model(\"swin_large_patch4_window12_384\", pretrained = True, num_classes = 1)","3b7df0e1":"# Defining RMSE score and sigmoid functions\ndef petfinder_rmse(inp,target):\n    return 100*torch.sqrt(F.mse_loss(torch.sigmoid(inp.flatten()), target))\n\ndef sigmoid(x):\n    return 1 \/ (1 + math.exp(-x))","03059c0f":"# Defining model class \nclass Pawpularity_model:\n    def __init__(self, criterion, optimizer, metrics, scheduler):\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.metrics = metrics\n        self.scheduler = scheduler\n    \n    def train_batch_loop(self, model, train_loader):\n        train_loss = 0 \n        train_acc = 0\n\n        for data in tqdm(train_loader):\n            images = data[\"images\"].to(device)\n            labels = data[\"labels\"].to(device)\n\n            logits = model(images)\n            loss = criterion(logits.flatten(), labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            \n            train_loss += loss.item()\n            train_acc += metrics(logits, labels).item()\n        \n        return train_loss \/ len(train_loader), train_acc \/ len(train_loader)\n    \n    def valid_batch_loop(self, model, valid_loader):\n        valid_loss = 0 \n        valid_acc = 0\n\n        with torch.no_grad():\n            for data in tqdm(valid_loader):\n                images = data[\"images\"].to(device)\n                labels = data[\"labels\"].to(device)\n\n                logits = model(images)\n                loss = criterion(logits.flatten(), labels)\n\n                valid_loss += loss.item()\n                valid_acc += metrics(logits, labels).item()\n        \n        return valid_loss \/ len(valid_loader), valid_acc \/ len(valid_loader)\n    \n    def fit(self, model, train_loader, valid_loader, epochs):\n\n        for i in range(epochs):\n\n            model.train()\n            avg_train_loss, avg_train_acc = self.train_batch_loop(model, train_loader)\n            \n\n            model.eval()\n            avg_valid_loss, avg_valid_acc = self.valid_batch_loop(model, valid_loader)\n\n            print(\"Epoch : {} \".format(i+1))\n            print(\"Train Loss : {:.6f} Train MSE : {:.6f}\".format(avg_train_loss, avg_train_acc))\n            print(\"Valid Loss : {:.6f} Valid MSE : {:.6f}\".format(avg_valid_loss, avg_valid_acc),\"\\n\")","adadfa4e":"# Training model\nfinal_pred = []\nfor i in range(N_FOLDS):\n    print(\"#\" * 20)\n    print(\"#### FOLD {}\".format(i+1))\n    print(\"#\" * 20, \"\\n\")\n    train = df[df[\"fold\"] != i].reset_index(drop = True)\n    valid = df[df[\"fold\"] == i].reset_index(drop = True)\n    \n    # Creating Train and Validation dataset\n    train_dataset = Dataset(\n        img_paths = train[\"Id\"].values,\n        targets = train[\"Pawpularity\"].values,\n        augmentations = aug,\n        mixup = True\n    )\n\n    valid_dataset = Dataset(\n        img_paths = valid[\"Id\"].values,\n        targets = valid[\"Pawpularity\"].values,\n        augmentations = test_aug,\n    )\n\n    train_loader = DataLoader(train_dataset, batch_size = Batch_size, shuffle = True,drop_last = True)\n    valid_loader = DataLoader(valid_dataset, batch_size = Batch_size, shuffle = True,drop_last = True)\n    \n    \n    model = swin().to(device)\n    criterion = BCEWithLogitsLoss()\n    optimizer = Adam(model.parameters(), lr = 1e-4)\n    metrics = petfinder_rmse\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 10, T_mult=1, eta_min=1e-6)\n\n    \n    trainer = Pawpularity_model(criterion, optimizer, metrics, scheduler)\n    trainer.fit(model, train_loader, valid_loader, epochs)\n    \n    torch.save(model, f\"swin_model_f{i}.bin\")\n    \n    # Predicting for Test dataset\n    print(\"=> Predicting Test Dataset\",\"\\n\")\n    pred = []\n    for data in test_loader:\n        images = data[\"images\"].to(device)\n        pred.append(sigmoid(model(images).item()) * 100) \n    \n    final_pred.append(pred)\n    \n    del model\n    gc.collect()\n    torch.cuda.empty_cache()","afd28a54":"# Mean of predicted values\nnp.mean(np.stack(final_pred))","c699243b":"# Creating submission file\nans = np.mean(np.stack(final_pred), axis=0)\n\nfinal = pd.DataFrame()\nfinal[\"Id\"] = Id\nfinal[\"Pawpularity\"] = ans\nfinal.to_csv(\"submission.csv\", index=False)","be50ff5f":"final.head()","977bb384":"## Loading and Processing Data","9762a68a":"## Importing Libraries\n","a8e42dac":"## Preparing Dataset","48c44c75":"## Creating Model","70541113":"## Training and Evaluating Model"}}