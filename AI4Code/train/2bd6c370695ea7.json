{"cell_type":{"d51b4a2b":"code","0d48d3d0":"code","bfac98b4":"code","75235f10":"code","5d4c7913":"code","5bb21822":"code","c73b3459":"code","83231d31":"code","045fbd4a":"code","434911f6":"code","3a128354":"code","ade4215f":"code","dd11e084":"code","cec1cb13":"code","596ae56b":"code","78ca904f":"code","b1ba4a64":"code","1d7822a3":"code","0a02ef73":"code","946adbef":"code","2b87420c":"code","51191fb4":"code","e6b09753":"code","a9c13dae":"code","2bdc9071":"code","bce97167":"code","34f771fa":"code","c5f4f117":"code","3782d837":"code","fc3a2424":"code","466cb8f5":"code","a5caa785":"code","2abec61c":"code","6991b288":"code","29d6e910":"code","05517e91":"code","8cc18973":"code","4dfc026e":"code","704cce95":"code","8e8bad44":"code","f847f834":"code","3e0553c7":"code","3e7f5a26":"code","3c053fa2":"code","04d52641":"code","7ede0d69":"code","49693164":"code","5cd64903":"code","1d2576a3":"code","c455c784":"code","01cacf8f":"code","3c394881":"code","a66a716e":"code","949c0f10":"code","76638b8e":"code","af7dadc7":"code","41280bc5":"code","24cba630":"code","6c42374a":"code","522567a2":"code","26698534":"code","4849c55a":"code","9248c7f5":"code","ce260562":"code","b7717075":"code","665f4a88":"code","b8b133bf":"code","4d115e28":"code","0f6039f1":"code","46ccddfb":"code","b6b2b569":"code","b9a4323d":"code","34ea2a65":"code","10b0179d":"code","d297de7d":"code","71e0fb1b":"code","6e8595ce":"code","8f8b7ea4":"code","c6374d35":"code","8b5630b2":"code","c86b9fd7":"code","7b47e2d8":"code","782a60b4":"code","194662ee":"code","5c038c99":"code","79ee3b2f":"code","f01ec0ee":"code","25b6f632":"code","548fe145":"code","cb33f6b2":"code","454cacc6":"code","41697eea":"code","bb7cd4e8":"code","eccf4f9d":"code","21782cf4":"code","8e9820c6":"code","cccc45b9":"code","b6775f20":"code","4ee57e45":"code","a96a1be0":"code","f9b55701":"code","7cf0ceb6":"code","bad9af7d":"code","4dc665f6":"code","e6a7e8da":"code","3263d5fd":"code","f8c1c46a":"code","b107d10e":"code","dba539bd":"code","87f42592":"code","eb670b64":"code","14094381":"code","680002f9":"code","276b020b":"code","40412b3f":"code","3f56438b":"code","3a5b903b":"code","4bb93828":"code","657b23e9":"code","808b4638":"markdown","1b0e0e45":"markdown","1e4891ae":"markdown","c1788baa":"markdown","9fe0ea9d":"markdown","d7744067":"markdown","76319bb7":"markdown","7a2db851":"markdown","cb738f0b":"markdown","4da85fde":"markdown","2b461896":"markdown","5db32775":"markdown","834142ce":"markdown","44d19b6b":"markdown","f73f3af9":"markdown","e5759af6":"markdown","1f3a7307":"markdown","1e222ac3":"markdown","8328031f":"markdown","6a005316":"markdown","6d159145":"markdown","b6ab3483":"markdown","d89eaa18":"markdown","8dd6ce06":"markdown","28b1207a":"markdown","3a1f7c06":"markdown","27d109e0":"markdown","8e4a953f":"markdown","accb0882":"markdown","02d4bce7":"markdown","cbf5d2a1":"markdown"},"source":{"d51b4a2b":"import os\nimport cv2\nimport json\nimport pickle\nimport random\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\n\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\nfrom pathlib import Path\nfrom datetime import datetime as dt\nfrom functools import partial\nfrom collections import Counter, defaultdict\n\nfrom PIL import Image\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom tqdm import tqdm\nfrom fastprogress import master_bar, progress_bar\n\npd.options.display.max_columns = 128\ntorch.multiprocessing.set_start_method(\"spawn\")","0d48d3d0":"input_dir = Path(\"..\/input\/petfinder-adoption-prediction\/\")\ntrain = pd.read_csv(input_dir \/ \"train\/train.csv\")\ntest = pd.read_csv(input_dir \/ \"test\/test.csv\")\nsample_submission = pd.read_csv(input_dir \/ \"test\/sample_submission.csv\")\n\ntrain_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()","bfac98b4":"train.PhotoAmt.mean(), test.PhotoAmt.mean()","75235f10":"!cp ..\/input\/pytorch-pretrained-image-models\/* .\/\n!ls ","5d4c7913":"def jopen(path):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        json_file = json.load(f)\n    return json_file\n\n\ndef parse_sentiment_file(path):\n    file = jopen(path)\n    language: str = file[\"language\"]\n\n    sentiment: list = file[\"documentSentiment\"]\n    entities: list = [x[\"name\"] for x in file[\"entities\"]]\n    entity = \" \".join(entities)\n\n    sentence_sentiment: list = [x[\"sentiment\"] for x in file[\"sentences\"]]\n    magnitude: np.ndarray = np.array(\n        [x[\"magnitude\"] for x in sentence_sentiment])\n    score: np.ndarray = np.array([x[\"score\"] for x in sentence_sentiment])\n\n    return_js = {\n        \"magnitude_sum\": magnitude.sum(),\n        \"magnitude_mean\": magnitude.mean(),\n        \"magnitude_var\": magnitude.var(),\n        \"score_sum\": score.sum(),\n        \"score_mean\": score.mean(),\n        \"score_var\": score.var(),\n        \"language\": language,\n        \"entity\": entity,\n        \"document_magnitude\": sentiment[\"magnitude\"],\n        \"document_score\": sentiment[\"score\"]\n    }\n    return return_js\n\n\ndef parse_metadata(path):\n    file: dict = jopen(path)\n    file_keys = list(file.keys())\n    name_specified = 0\n    if \"labelAnnotations\" in file_keys:\n        file_annots = file[\"labelAnnotations\"]\n        file_mean_score = np.asarray([x[\"score\"] for x in file_annots]).mean()\n        file_desc = \" \".join([x[\"description\"] for x in file_annots])\n        if \"cat\" in file_desc or \"dog\" in file_desc:\n            name_specified = 1\n    else:\n        file_mean_score = np.nan\n        file_desc = \"\"\n\n    file_colors: list = file[\"imagePropertiesAnnotation\"][\"dominantColors\"][\n        \"colors\"]\n    file_crops: list = file[\"cropHintsAnnotation\"][\"cropHints\"]\n\n    color_score = np.asarray([x[\"score\"] for x in file_colors]).mean()\n    pixel_frac = np.asarray([x[\"pixelFraction\"] for x in file_colors]).mean()\n    crop_conf = np.asarray([x[\"confidence\"] for x in file_crops]).mean()\n\n    if \"importanceFraction\" in file_crops[0].keys():\n        crop_importance = np.asarray(\n            [x[\"importanceFraction\"] for x in file_crops]).mean()\n    else:\n        crop_importance = np.nan\n    metadata = {\n        \"annot_score\": file_mean_score,\n        \"color_score\": color_score,\n        \"pixel_frac\": pixel_frac,\n        \"crop_conf\": crop_conf,\n        \"crop_importance\": crop_importance,\n        \"desc\": file_desc,\n        \"specified\": name_specified\n    }\n    return metadata\n\n\ndef additinal_features_per_id(pet_id, sentiment_path: Path, meta_path: Path):\n    sentiment_path = sentiment_path \/ f\"{pet_id}.json\"\n    try:\n        sentiment = parse_sentiment_file(sentiment_path)\n        sentiment[\"pet_id\"] = pet_id\n    except FileNotFoundError:\n        sentiment = {}\n\n    meta_files = sorted(meta_path.glob(f\"{pet_id}*.json\"))\n    metadata_list = []\n    if len(meta_files) > 0:\n        for f in meta_files:\n            metadata = parse_metadata(f)\n            metadata[\"pet_id\"] = pet_id\n            metadata_list.append(metadata)\n    return sentiment, metadata_list\n\n\ndef load_additional_features(ped_ids: list, sentiment_path: Path,\n                             meta_path: Path):\n    features = Parallel(\n        n_jobs=-1, verbose=1)(\n            delayed(additinal_features_per_id)(i, sentiment_path, meta_path)\n            for i in ped_ids)\n    sentiments = [x[0] for x in features if len(x[0]) > 0]\n    metadatas = [x[1] for x in features if len(x[1]) > 0]\n    sentiment_keys = sentiments[0].keys()\n    metadata_keys = metadatas[0][0].keys()\n    sentiment_dict = {}\n    metadata_dict = {}\n    for key in sentiment_keys:\n        sentiment_dict[key] = [x[key] for x in sentiments]\n\n    for key in metadata_keys:\n        meta_list = []\n        for meta_per_pid in metadatas:\n            meta_list += [meta[key] for meta in meta_per_pid]\n        metadata_dict[key] = meta_list\n\n    sentiment_df = pd.DataFrame(sentiment_dict)\n    metadata_df = pd.DataFrame(metadata_dict)\n    return sentiment_df, metadata_df\n\n\ndef aggregate_metadata(metadata_df: pd.DataFrame,\n                       aggregates=[\"sum\", \"mean\", \"var\"]):\n    meta_desc: pd.DataFrame = metadata_df.groupby([\"pet_id\"])[\"desc\"].unique()\n    meta_desc = meta_desc.reset_index()\n    meta_desc[\"desc\"] = meta_desc[\"desc\"].apply(lambda x: \" \".join(x))\n\n    meta_gr: pd.DataFrame = metadata_df.drop([\"desc\"], axis=1)\n    for i in meta_gr.columns:\n        if \"pet_id\" not in i:\n            meta_gr[i] = meta_gr[i].astype(float)\n    meta_gr = meta_gr.groupby([\"pet_id\"]).agg(aggregates)\n    meta_gr.columns = pd.Index(\n        [f\"{c[0]}_{c[1].upper()}\" for c in meta_gr.columns.tolist()])\n    meta_gr = meta_gr.reset_index()\n    return meta_gr, meta_desc\n\n\ndef aggregate_sentiment(sentiment_df: pd.DataFrame, aggregates=[\"sum\"]):\n    sentiment_desc: pd.DataFrame = sentiment_df.groupby(\n        [\"pet_id\"])[\"entity\"].unique()\n    sentiment_desc = sentiment_desc.reset_index()\n    sentiment_desc[\"entity\"] = sentiment_desc[\"entity\"].apply(\n        lambda x: \" \".join(x))\n    sentiment_lang = sentiment_df.groupby(\n        [\"pet_id\"])[\"language\"].unique()\n    sentiment_lang = sentiment_lang.reset_index()\n    sentiment_lang[\"language\"] = sentiment_lang[\"language\"].apply(\n        lambda x: \" \".join(x))\n    sentiment_desc = sentiment_desc.merge(\n        sentiment_lang, how=\"left\", on=\"pet_id\")\n    \n\n    sentiment_gr: pd.DataFrame = sentiment_df.drop([\"entity\", \"language\"],\n                                                   axis=1)\n    for i in sentiment_gr.columns:\n        if \"pet_id\" not in i:\n            sentiment_gr[i] = sentiment_gr[i].astype(float)\n    sentiment_gr = sentiment_gr.groupby([\"pet_id\"]).agg(aggregates)\n    sentiment_gr.columns = pd.Index(\n        [f\"{c[0]}\" for c in sentiment_gr.columns.tolist()])\n    sentiment_gr = sentiment_gr.reset_index()\n    return sentiment_gr, sentiment_desc","5bb21822":"input_dir = Path(\"..\/input\/petfinder-adoption-prediction\/\")\ntrain = pd.read_csv(input_dir \/ \"train\/train.csv\")\ntest = pd.read_csv(input_dir \/ \"test\/test.csv\")\nsample_submission = pd.read_csv(input_dir \/ \"test\/sample_submission.csv\")","c73b3459":"sp_train = input_dir \/ Path(\"train_sentiment\/\")\nmp_train = input_dir \/ Path(\"train_metadata\/\")\nsp_test = input_dir \/ Path(\"test_sentiment\/\")\nmp_test = input_dir \/ Path(\"test_metadata\/\")","83231d31":"train_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()","045fbd4a":"train_sentiment_df, train_metadata_df = load_additional_features(\n    train_pet_ids, sp_train, mp_train)\n\ntest_sentiment_df, test_metadata_df = load_additional_features(\n    test_pet_ids, sp_test, mp_test)","434911f6":"train_meta_gr, train_meta_desc = aggregate_metadata(train_metadata_df)\ntest_meta_gr, test_meta_desc = aggregate_metadata(test_metadata_df)\ntrain_sentiment_gr, train_sentiment_desc = \\\n    aggregate_sentiment(train_sentiment_df)\ntest_sentiment_gr, test_sentiment_desc = \\\n    aggregate_sentiment(test_sentiment_df)","3a128354":"train_proc = train.copy()\ntrain_proc = train_proc.merge(\n    train_sentiment_gr, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntrain_proc = train_proc.merge(\n    train_meta_gr, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntrain_proc = train_proc.merge(\n    train_sentiment_desc, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntrain_proc = train_proc.merge(\n    train_meta_desc, how=\"left\", left_on=\"PetID\", right_on = \"pet_id\")\n\ntest_proc = test.copy()\ntest_proc = test_proc.merge(\n    test_sentiment_gr, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntest_proc = test_proc.merge(\n    test_meta_gr, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntest_proc = test_proc.merge(\n    test_sentiment_desc, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntest_proc = test_proc.merge(\n    test_meta_desc, how=\"left\", left_on=\"PetID\", right_on = \"pet_id\")","ade4215f":"print(train_proc.shape, test_proc.shape)\nassert train_proc.shape[0] == train.shape[0]\nassert test_proc.shape[0] == test.shape[0]","dd11e084":"train_proc.drop(train_proc.filter(\n    regex=\"pet_id\", axis=1).columns.tolist(), \n    axis=1, \n    inplace=True)\n\ntest_proc.drop(test_proc.filter(\n    regex=\"pet_id\", axis=1).columns.tolist(),\n    axis=1,\n    inplace=True)\n\ntrain_proc.head()","cec1cb13":"train_proc.language.fillna(\"\", inplace=True)\ntest_proc.language.fillna(\"\", inplace=True)\n\nlangs = train_proc.language.unique()\nencode_dict = {k: i for i, k in enumerate(langs)}\n\ntrain_proc.language = train_proc.language.map(encode_dict)\ntest_proc.language = test_proc.language.map(encode_dict)","596ae56b":"labels_breed = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/breed_labels.csv\")\nlabels_state = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/state_labels.csv\")\nlabels_color = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/color_labels.csv\")","78ca904f":"train_breed_main = train_proc[[\"Breed1\"]].merge(\n    labels_breed, how=\"left\",\n    left_on=\"Breed1\", right_on=\"BreedID\",\n    suffixes=(\"\", \"_main_breed\"))\ntrain_breed_main = train_breed_main.iloc[:, 2:]\ntrain_breed_main = train_breed_main.add_prefix(\"main_breed_\")\n\ntrain_breed_second = train_proc[[\"Breed2\"]].merge(\n    labels_breed, how=\"left\",\n    left_on=\"Breed2\", right_on=\"BreedID\",\n    suffixes=(\"\", \"_second_breed\"))\ntrain_breed_second = train_breed_second.iloc[:, 2:]\ntrain_breed_second = train_breed_second.add_prefix(\"second_breed_\")\n\ntrain_proc = pd.concat([\n    train_proc, train_breed_main, train_breed_second\n], axis=1)\n\ntest_breed_main = test_proc[[\"Breed1\"]].merge(\n    labels_breed, how=\"left\",\n    left_on=\"Breed1\", right_on=\"BreedID\",\n    suffixes=(\"\", \"_main_breed\"))\ntest_breed_main = test_breed_main.iloc[:, 2:]\ntest_breed_main = test_breed_main.add_prefix(\"main_breed_\")\n\ntest_breed_second = test_proc[[\"Breed2\"]].merge(\n    labels_breed, how=\"left\",\n    left_on=\"Breed2\", right_on=\"BreedID\",\n    suffixes=(\"\", \"_second_breed\"))\ntest_breed_second = test_breed_second.iloc[:, 2:]\ntest_breed_second = test_breed_second.add_prefix(\"second_breed_\")\n\ntest_proc = pd.concat([\n    test_proc, test_breed_main, test_breed_second\n], axis=1)\n\nprint(train_proc.shape, test_proc.shape)\ntrain_proc.head()","b1ba4a64":"X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)\nX_temp = X.copy()\n\ntext_columns = [\n    \"Description\",\n    \"entity\",\n    \"desc\"]\ncategorical_columns = [\n    \"Type\", \"Breed1\", \"Breed2\", \"Gender\",\n    \"Color1\", \"Color2\", \"Color3\", \"MaturitySize\",\n    \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\",\n    \"State\", \"language\", \"main_breed_BreedName\", \"second_breed_BreedName\"\n]\ncat_c = [\"main_breed_BreedName\", \"second_breed_BreedName\"]\ndrop_columns = [\n    \"PetID\", \"Name\", \"RescuerID\"\n]","1d7822a3":"for i in cat_c:\n    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]","0a02ef73":"X_text = X_temp[text_columns]\nfor i in X_text.columns:\n    X_text[i] = X_text[i].fillna(\"none\")","946adbef":"X_temp[\"len_description\"] = X_text[\"Description\"].map(len)\nX_temp[\"len_meta_desc\"] = X_text[\"desc\"].map(len)\nX_temp[\"len_entity\"] = X_text[\"entity\"].map(len)","2b87420c":"import re\n\nfrom nltk.corpus import stopwords\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nfrom nltk.tokenize import RegexpTokenizer\nimport nltk.stem as stm\nfrom nltk import WordNetLemmatizer, word_tokenize\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)","51191fb4":"X_text[\"cleaned_text\"] = X_text[\"Description\"].map(lambda x: x.lower())\nX_text[\"cleaned_text\"] = X_text[\"cleaned_text\"].map(lambda x: clean_text(x))\nX_text[\"cleaned_text\"] = X_text[\"cleaned_text\"].map(lambda x: clean_numbers(x))\nX_text[\"cleaned_text\"] = X_text[\"cleaned_text\"].map(lambda x: replace_typical_misspell(x))","e6b09753":"eng_stopwords = set(stopwords.words(\"english\"))\nimport string\n\nX_temp[\"len_description\"] = X_text[\"Description\"].map(len)\nX_temp[\"len_meta_desc\"] = X_text[\"desc\"].map(len)\nX_temp[\"len_entity\"] = X_text[\"entity\"].map(len)\n\nX_temp[\"num_description_words\"] = X_text[\"Description\"].map(lambda x: len(str(x).split()))\nX_temp[\"num_desc_words\"] = X_text[\"desc\"].map(lambda x: len(str(x).split()))\nX_temp[\"num_entity_words\"] = X_text[\"entity\"].map(lambda x: len(str(x).split()))\n\nX_temp[\"uniq_description_words\"] = X_text[\"Description\"].map(lambda x: len(set(str(x).split())))\nX_temp[\"uniq_desc_words\"] = X_text[\"desc\"].map(lambda x: len(set(str(x).split())))\nX_temp[\"uniq_entity_words\"] = X_text[\"entity\"].map(lambda x: len(set(str(x).split())))\n\nX_temp[\"num_description_stopwords\"] = X_text[\"Description\"].map(lambda x: len([\n    w for w in str(x).lower().split() if w in eng_stopwords]))\nX_temp[\"num_desc_stopwords\"] = X_text[\"desc\"].map(lambda x: len([\n    w for w in str(x).lower().split() if w in eng_stopwords]))\nX_temp[\"num_entity_stopwords\"] = X_text[\"entity\"].map(lambda x: len([\n    w for w in str(x).lower().split() if w in eng_stopwords]))\n\nX_temp[\"num_description_punctuation\"] = X_text[\"Description\"].map(lambda x: len([\n    c for c in str(x) if c in string.punctuation]))","a9c13dae":"state_gdp = {\n    41336: 116.679,\n    41325: 40.596,\n    41367: 23.02,\n    41401: 190.075,\n    41415: 5.984,\n    41324: 37.274,\n    41332: 42.389,\n    41335: 52.452,\n    41330: 67.629,\n    41380: 5.642,\n    41327: 81.284,\n    41345: 80.167,\n    41342: 121.414,\n    41326: 280.698,\n    41361: 32.270\n}\n\nstate_population = {\n    41336: 33.48283,\n    41325: 19.47651,\n    41367: 15.39601,\n    41401: 16.74621,\n    41415: 0.86908,\n    41324: 8.21110,\n    41332: 10.21064,\n    41335: 15.00817,\n    41330: 23.52743,\n    41380: 2.31541,\n    41327: 15.61383,\n    41345: 32.06742,\n    41342: 24.71140,\n    41326: 54.62141,\n    41361: 10.35977\n}\n\nstate_area ={\n    41336:19102,\n    41325:9500,\n    41367:15099,\n    41401:243,\n    41415:91,\n    41324:1664,\n    41332:6686,\n    41335:36137,\n    41330:21035,\n    41380:821,\n    41327:1048,\n    41345:73631,\n    41342:124450,\n    41326:8104,\n    41361:13035\n}\nX_temp[\"state_gdp\"] = X_temp.State.map(state_gdp)\nX_temp[\"state_population\"] = X_temp.State.map(state_population)\nX_temp[\"state_area\"] = X_temp.State.map(state_area)\n\nX_temp[\"state_gdp_per_person\"] = X_temp[\"state_gdp\"] \/ X_temp[\"state_population\"] * 1e4\nX_temp[\"fee_per_gdp_per_person\"] = X_temp.Fee \/ X_temp[\"state_gdp_per_person\"]","2bdc9071":"X_temp.head()","bce97167":"import re\ndef has_name(x):\n    if isinstance(x, float):\n        return 0\n    if \"no name\" in x.lower():\n        return 0\n    return 1\n\n\ndef num_name_words(x):\n    if isinstance(x, float):\n        return 0\n    name_words = x.split(\" \")\n    return len(name_words)\n\n\ndef contains_amp(x):\n    if isinstance(x, float):\n        return 0\n    if \"&\" in x:\n        return 1\n    if \"and\" in x.lower():\n        return 1\n    if \"+\" in x.lower():\n        return 1\n    return 0\n\n\ndef contains_comma(x):\n    if isinstance(x, float):\n        return 0\n    if \",\" in x:\n        return 1\n    return 0\n\n\ndef start_with_number(x):\n    if isinstance(x, float):\n        return 0\n    match = re.match(f\"\\d\", x)\n    if match:\n        return int(match.group())\n    return 0\n\n\ndef contains_paren(x):\n    if isinstance(x, float):\n        return 0\n    if \"(\" in x:\n        return 1\n    if \")\" in x:\n        return 1\n    return 0\n\n\ndef contains_number(x):\n    if isinstance(x, float):\n        return 0\n    if re.match(r\".*\\d\", x):\n        return 1\n    return 0\n\n\ndef safe_calc_len(x):\n    if isinstance(x, float):\n        return 1\n    return len(x)\n\n\ndef num_unlike_letters(x):\n    if isinstance(x, float):\n        return 0\n    letters = {\n        ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', \n        '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+',  \n        '\u2022',  '~', '@', '\u00a3', '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',\n        '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', \n        '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', \n        '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', \n        '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', \n        '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', \n        '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u2219', '\uff09', '\u2193', '\u3001', \n        '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', \n        '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', \"\u00e3\", \"\u00e7\", \"\u00e5\", \"\u00e4\",\n        \"\u00b6\", \"\u00f0\"}\n\n    letter_in = set(x)\n    intersection = letter_in.intersection(letters)\n    if len(intersection) == 0:\n        return 0\n    else:\n        unlike_num = 0\n        for l in intersection:\n            unlike_num += len(re.findall(re.escape(l), x))\n        return unlike_num\n\nX_temp[\"num_name_words\"] = X_temp.Name.map(lambda x: num_name_words(x))\nX_temp[\"contains_amp\"] = X_temp.Name.map(lambda x: contains_amp(x))\nX_temp[\"contains_comma\"] = X_temp.Name.map(lambda x: contains_comma(x))\nX_temp[\"start_with_number\"] = X_temp.Name.map(lambda x: start_with_number(x))\nX_temp[\"contains_paren\"] = X_temp.Name.map(lambda x: contains_paren(x))\nX_temp[\"contains_number\"] = X_temp.Name.map(lambda x: contains_number(x))\nX_temp[\"name_length\"] = X_temp.Name.map(lambda x: safe_calc_len(x))\nX_temp[\"num_unlike_letters\"] = X_temp.Name.map(lambda x: num_unlike_letters(x))\nX_temp[\"rate_unlike_letters\"] = X_temp.num_unlike_letters \/ X_temp.name_length\nX_temp.head()","34f771fa":"n_components = 16\ntext_features = []\n\nfor i in text_columns:\n    print(f\"generating features from: {i}\")\n    tfv = TfidfVectorizer(\n        min_df=2,\n        strip_accents=\"unicode\",\n        analyzer=\"word\",\n        token_pattern=r\"(?u)\\b\\w+\\b\",\n        ngram_range=(1, 3),\n        use_idf=1,\n        smooth_idf=1,\n        sublinear_tf=1)\n    svd = TruncatedSVD(\n        n_components=n_components,\n        random_state=1337)\n    tfidf_col = tfv.fit_transform(X_text.loc[:, i].values)\n    svd_col = svd.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix(\"Tfidf_{}_\".format(i))\n    \n    text_features.append(svd_col)\n\ntext_features = pd.concat(text_features, axis=1)\nX_temp = pd.concat([X_temp, text_features], axis=1)\n\nfor i in text_columns:\n    X_temp.drop(i, axis=1, inplace=True)","c5f4f117":"import os\nimport glob\n\ntrain_image_files = sorted(\n    glob.glob(\"..\/input\/petfinder-adoption-prediction\/train_images\/*.jpg\"))\ntest_image_files = sorted(\n    glob.glob(\"..\/input\/petfinder-adoption-prediction\/test_images\/*.jpg\"))\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntest_df_imgs = pd.DataFrame(test_image_files)\ntrain_df_imgs.columns = [\"image_file_name\"]\ntest_df_imgs.columns = [\"image_file_name\"]\n\ntrain_imgs_pets = train_df_imgs[\"image_file_name\"].apply(\n    lambda x: x.split(\"\/\")[-1].split(\"-\")[0])\ntest_imgs_pets = test_df_imgs[\"image_file_name\"].apply(\n    lambda x: x.split(\"\/\")[-1].split(\"-\")[0])\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n\ndef get_size(filename):\n    st = os.stat(filename)\n    return st.st_size\n\ndef get_dimensions(filename):\n    img_size = Image.open(filename).size\n    return img_size\n\ntrain_df_imgs[\"image_size\"] = train_df_imgs[\"image_file_name\"].apply(get_size)\ntest_df_imgs[\"image_size\"] = test_df_imgs[\"image_file_name\"].apply(get_size)\ntrain_df_imgs[\"temp_size\"] = train_df_imgs[\"image_file_name\"].apply(get_dimensions)\ntest_df_imgs[\"temp_size\"] = test_df_imgs[\"image_file_name\"].apply(get_dimensions)\ntrain_df_imgs[\"width\"] = train_df_imgs[\"temp_size\"].apply(lambda x: x[0])\ntest_df_imgs[\"width\"] = test_df_imgs[\"temp_size\"].apply(lambda x: x[0])\ntrain_df_imgs[\"height\"] = train_df_imgs[\"temp_size\"].apply(lambda x: x[1])\ntest_df_imgs[\"height\"] = test_df_imgs[\"temp_size\"].apply(lambda x: x[1])\ntrain_df_imgs.drop([\"temp_size\"], axis=1, inplace=True)\ntest_df_imgs.drop([\"temp_size\"], axis=1, inplace=True)\n\naggs = {\n    \"image_size\": [\"sum\", \"mean\", \"var\"],\n    \"width\": [\"sum\", \"mean\", \"var\"],\n    \"height\": [\"sum\", \"mean\", \"var\"]\n}\nagg_train_imgs = train_df_imgs.groupby(\"PetID\").agg(aggs)\nnew_columns = [\n    k + \"_\" + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_test_imgs = test_df_imgs.groupby(\"PetID\").agg(aggs)\nagg_train_imgs.columns = new_columns\nagg_train_imgs = agg_train_imgs.reset_index()\n\nagg_test_imgs.columns = new_columns\nagg_test_imgs = agg_test_imgs.reset_index()\n","3782d837":"agg_imgs = pd.concat([agg_train_imgs, agg_test_imgs], axis=0).reset_index(drop=True)\nX_temp = X_temp.merge(agg_imgs, how=\"left\", on=\"PetID\")\nX_temp.head()","fc3a2424":"hasnans = []\nfor c in X_temp.columns:\n    if X_temp[c].hasnans:\n        hasnans.append(c)","466cb8f5":"sanitize_col = list(\n    set(hasnans) - {\n        \"Name\", \"AdoptionSpeed\"\n    })\nX_temp[sanitize_col] = X_temp[sanitize_col].fillna(0.0)\nX_temp.head()","a5caa785":"big_num_cols = [\"len_description\", \"len_meta_desc\", \"len_entity\",\n                \"num_description_words\", \"num_desc_words\", \"num_entity_words\", \n                \"uniq_description_words\", \"uniq_desc_words\", \"uniq_entity_words\",\n                \"num_description_stopwords\", \"num_desc_stopwords\", \"num_entity_stopwords\",\n                \"num_description_punctuation\",\n                \"state_gdp\", \"state_population\", \"state_area\", \"state_gdp_per_person\",\n                \"image_size_sum\", \"image_size_mean\", \"image_size_var\", \"width_sum\",\n                \"width_mean\", \"width_var\", \"height_sum\", \"height_mean\", \"height_var\"]\nfor c in big_num_cols:\n    X_temp[c] = X_temp[c].map(lambda x: np.log1p(x))\nX_temp.head()","2abec61c":"train_resc = train.RescuerID\ntest_resc = test.RescuerID","6991b288":"X_temp.drop(drop_columns, axis=1, inplace=True)\nX_temp.head()","29d6e910":"train.shape[0]","05517e91":"train.shape\nn_train = train.shape[0]\nX_train = X_temp.loc[:n_train-1, :]\nX_test = X_temp.loc[n_train:, :]\nX_test.drop([\"AdoptionSpeed\"], axis=1, inplace=True)\n\nassert X_train.shape[0] == train.shape[0]\nassert X_test.shape[0] == test.shape[0]","8cc18973":"train_cols = X_train.columns.tolist()\ntrain_cols.remove(\"AdoptionSpeed\")\n\ntest_cols = X_test.columns.tolist()\nassert np.all(train_cols == test_cols)","4dfc026e":"X_train_non_null = X_train.fillna(-1)\nX_test_non_null = X_test.fillna(-1)\n\nX_train_non_null.isnull().any().any(), X_test_non_null.isnull().any().any()","704cce95":"cat_features = [\"Type\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\",\n                \"MaturitySize\", \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\",\n                \"Health\", \"Quantity\", \"State\", \"language\", \"main_breed_BreedName\", \n                \"second_breed_BreedName\"]\n\nX_train_cat = X_train_non_null.loc[:, cat_features]\nX_test_cat = X_test_non_null.loc[:, cat_features]","8e8bad44":"def stratified_group_k_fold(X, y, groups, k, seed=None):\n    labels_num = np.max(y) + 1\n    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n    y_distr = Counter()\n    for label, g in zip(y, groups):\n        y_counts_per_group[g][label] += 1\n        y_distr[label] += 1\n\n    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n    groups_per_fold = defaultdict(set)\n\n    def eval_y_counts_per_fold(y_counts, fold):\n        y_counts_per_fold[fold] += y_counts\n        std_per_label = []\n        for label in range(labels_num):\n            label_std = np.std([y_counts_per_fold[i][label] \/ y_distr[label] for i in range(k)])\n            std_per_label.append(label_std)\n        y_counts_per_fold[fold] -= y_counts\n        return np.mean(std_per_label)\n    \n    groups_and_y_counts = list(y_counts_per_group.items())\n    random.Random(seed).shuffle(groups_and_y_counts)\n\n    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n        best_fold = None\n        min_eval = None\n        for i in range(k):\n            fold_eval = eval_y_counts_per_fold(y_counts, i)\n            if min_eval is None or fold_eval < min_eval:\n                min_eval = fold_eval\n                best_fold = i\n        y_counts_per_fold[best_fold] += y_counts\n        groups_per_fold[best_fold].add(g)\n\n    all_groups = set(groups)\n    for i in range(k):\n        train_groups = all_groups - groups_per_fold[i]\n        test_groups = groups_per_fold[i]\n\n        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n        yield train_indices, test_indices","f847f834":"y = X_train_non_null.AdoptionSpeed\nX_train_cat_y = X_train_cat.copy()\nX_train_cat_y[\"AdoptionSpeed\"] = y\n\nX_train_cat_encoded = np.zeros((X_train_cat.shape[0], len(cat_features) * 2))\nX_test_cat_encoded = np.zeros((X_test_cat.shape[0], len(cat_features) * 2))\nk = 10\nfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=1213)\n\nfor trn_idx, val_idx in fold.split(X_train_cat, y.values.astype(int)):\n    X_trn, X_val = X_train_cat_y.loc[trn_idx, :], X_train_cat_y.loc[val_idx, :]\n    for j, c in enumerate(cat_features):\n        X_trn_enc = X_trn.groupby(c).agg({\n            \"AdoptionSpeed\": [\"mean\", \"std\"]\n        })\n        cte_columns = [f\"{c}_{x}\" for x in [\"mean\", \"std\"]]\n        X_trn_enc.columns = cte_columns\n        X_temp = np.zeros((X_test_cat.shape[0], 2))\n        X_temp_df = pd.DataFrame(data=X_temp, columns=cte_columns)\n        for x in X_trn_enc.columns:\n            X_val[x] = X_val[c].map(X_trn_enc[x])\n            X_temp_df[x] = X_test_cat[c].map(X_trn_enc[x]).reset_index(drop=True)\n        X_train_cat_encoded[val_idx, 2 * j:2 * (j + 1)] = X_val[X_trn_enc.columns].values\n        X_test_cat_encoded[:, 2 * j: 2 * (j + 1)] += X_temp_df.values \/ k","3e0553c7":"X_train_cat_encoded.shape, X_test_cat_encoded.shape","3e7f5a26":"columns = [f\"{c}_{x}\" for c in cat_features for x in [\"mean\", \"std\"]]\nX_train_cat_encoded_df = pd.DataFrame(data=X_train_cat_encoded, columns=columns)\nX_test_cat_encoded_df = pd.DataFrame(data=X_test_cat_encoded, columns=columns)\nX_test_cat_encoded_df.head()","3c053fa2":"X_train_num = X_train_non_null.drop(cat_features + [\"AdoptionSpeed\"], axis=1)\nX_test_num = X_test_non_null.drop(cat_features, axis=1)\n\ntarget = X_train_non_null[\"AdoptionSpeed\"]","04d52641":"X_train_num = pd.concat([X_train_num, X_train_cat_encoded_df], axis=1)\n\nX_test_cat_encoded_df.index = X_test_num.index\nX_test_num = pd.concat([X_test_num, X_test_cat_encoded_df], axis=1)","7ede0d69":"X_train_num.fillna(0.0, inplace=True)\nX_test_num.fillna(0.0, inplace=True)","49693164":"X_train_num.replace(np.inf, np.nan).fillna(0.0, inplace=True)\nX_test_num.replace(np.inf, np.nan).fillna(0.0, inplace=True)\n\nX_train_num.replace(-np.inf, np.nan).fillna(0.0, inplace=True)\nX_test_num.replace(-np.inf, np.nan).fillna(0.0, inplace=True)","5cd64903":"X_train_num.values[np.isinf(X_train_num)]","1d2576a3":"X_all_num = pd.concat([X_train_num, X_test_num], axis=0)\nss = StandardScaler()\nss.fit(X_all_num)\n\nX_train_ss = ss.transform(X_train_num)\nX_test_ss = ss.transform(X_test_num)","c455c784":"X_train_ss[np.isnan(X_train_ss)] = 0.0\nX_test_ss[np.isnan(X_test_ss)] = 0.0","01cacf8f":"cat_cat = pd.concat([X_train_cat, X_test_cat])\n\nn_breed1 = cat_cat[\"Breed1\"].nunique()\nn_breed2 = cat_cat[\"Breed2\"].nunique()\nn_langs = cat_cat[\"language\"].nunique()\nn_color1 = cat_cat[\"Color1\"].nunique()\nn_color2 = cat_cat[\"Color2\"].nunique()\nn_color3 = cat_cat[\"Color3\"].nunique()","3c394881":"from sklearn.preprocessing import LabelEncoder\nfor c in X_train_cat.columns:\n        le = LabelEncoder()\n        le.fit(cat_cat[c])\n        X_train_cat[c] = le.transform(X_train_cat[c])\n        X_test_cat[c] = le.transform(X_test_cat[c])","a66a716e":"import gc\ndef load_fasttext(word_index):    \n    EMBEDDING_FILE = '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\n    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(100000, len(word_index)) + 1\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    words_list = []\n    for word, i in word_index.items():\n        if i >= 100000: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n            words_list.append(word)\n    return embedding_matrix, set(words_list)","949c0f10":"tokenizer = Tokenizer(num_words=100000)\ntokenizer.fit_on_texts(X_text[\"cleaned_text\"] + \" \" + X_text[\"desc\"] + \" \" + X_text[\"entity\"])\nlen(tokenizer.word_index)","76638b8e":"train_text, test_text = X_text.loc[:n_train-1, :], X_text.loc[n_train:, :]\ntrain_text.shape, test_text.shape","af7dadc7":"x_train_text = tokenizer.texts_to_sequences(train_text[\"cleaned_text\"])\nx_test_text = tokenizer.texts_to_sequences(test_text[\"cleaned_text\"])\n\nx_train_text = pad_sequences(x_train_text, maxlen=70)\nx_test_text = pad_sequences(x_test_text, maxlen=70)","41280bc5":"%%time\nembedding_matrix, words_set = load_fasttext(tokenizer.word_index)","24cba630":"gc.collect()","6c42374a":"def calc_vector(text, word_index, words_set, embedding_matrix):\n    words = set(text.split())\n    n_skip = 0\n    vec = np.zeros((embedding_matrix.shape[1],))\n    if len(words) == 0:\n        return vec\n    for n_w, word in enumerate(words):\n        if word in words_set:\n            idx = word_index.get(word)\n            vec_ = embedding_matrix[idx, :]\n        else:\n            n_skip += 1\n            continue\n        if n_w == 0:\n            vec = vec_\n        else:\n            vec = vec + vec_\n    vec = vec \/ (n_w - n_skip + 1)\n    return vec","522567a2":"tqdm.pandas()","26698534":"train_entity = train_text.entity.progress_apply(lambda x: calc_vector(\n    x, tokenizer.word_index, words_set, embedding_matrix))\ntest_entity = test_text.entity.progress_apply(lambda x: calc_vector(\n    x, tokenizer.word_index, words_set, embedding_matrix))\n\ntrain_entity = np.vstack(train_entity.values.tolist())\ntest_entity = np.vstack(test_entity.values.tolist())\n\ntrain_entity[np.isnan(train_entity)] = 0.0\ntest_entity[np.isnan(test_entity)] = 0.0\n\ntrain_desc = train_text.desc.progress_apply(lambda x: calc_vector(\n    x, tokenizer.word_index, words_set, embedding_matrix))\ntest_desc = test_text.desc.progress_apply(lambda x: calc_vector(\n    x, tokenizer.word_index, words_set, embedding_matrix))\n\ntrain_desc = np.vstack(train_desc.values.tolist())\ntest_desc = np.vstack(test_desc.values.tolist())\n\ntrain_desc[np.isnan(train_desc)] = 0.0\ntest_desc[np.isnan(test_desc)] = 0.0","4849c55a":"normalize = transforms.Normalize(\n    mean=[0.485, 0.456, 0.406],\n    std=[0.229, 0.224, 0.225]\n)\nds_trans = transforms.Compose([transforms.Resize(224),\n                               transforms.CenterCrop(224),\n                               transforms.ToTensor(),\n                               normalize])","9248c7f5":"class ImageDataset(data.Dataset):\n    def __init__(self, pet_ids, root_dir, transform):\n        self.pet_ids = pet_ids\n        self.root_dir = root_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.pet_ids)\n    \n    def __getitem__(self, idx):\n        imgs = torch.zeros((4, 3, 224, 224))\n        for i in range(4):\n            img_name = f\"{self.pet_ids[idx]}-{i+1}.jpg\"\n            fullname = self.root_dir \/ Path(img_name)\n            try:\n                image = Image.open(fullname).convert(\"RGB\")\n            except FileNotFoundError:\n                image = np.zeros((3, 224, 224), dtype=np.uint8).transpose(1, 2, 0)\n                image = Image.fromarray(np.uint8(image))\n            if self.transform:\n                image = self.transform(image)\n            imgs[i, :, :, :] = image\n        return [self.pet_ids[idx], imgs]","ce260562":"class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        \n    def forward(self, x):\n        return x\n\nclass ImagePretrained(nn.Module):\n    def __init__(self, path):\n        super(ImagePretrained, self).__init__()\n        self.densenet121 = models.densenet121()\n        self.densenet121.load_state_dict(torch.load(path))\n        self.densenet121.classifier = Classifier()\n        dense = nn.Sequential(*list(self.densenet121.children())[:-1])\n        for param in dense.parameters():\n            param.requires_grad = False\n            \n    def forward(self, x):\n        converted = torch.zeros(x.size(0), 4, 1024)\n        for i in range(4):\n            out = self.densenet121(x[:, i, :, :, :])\n            converted[:, i, :] = out\n        return converted","b7717075":"train_img_dataset = ImageDataset(train_pet_ids,\n                             \"..\/input\/petfinder-adoption-prediction\/train_images\/\",\n                             transform=ds_trans)\ntest_img_dataset = ImageDataset(test_pet_ids,\n                            \"..\/input\/petfinder-adoption-prediction\/test_images\/\",\n                            transform=ds_trans)\ntrain_img_loader = data.DataLoader(train_img_dataset, batch_size=128, shuffle=False)\ntest_img_loader = data.DataLoader(test_img_dataset, batch_size=128, shuffle=False)","665f4a88":"train_pids = []\ntrain_img_matrix = np.zeros((len(train_pet_ids), 4, 1024))\nmodel = ImagePretrained(\"densenet121.pth\")\nmodel.to(\"cuda:0\")\nfor i, (pid, tensor) in tqdm(enumerate(train_img_loader)):\n    train_pids += [*pid]\n    tensor = tensor.to(\"cuda:0\")\n    pred = model(tensor).detach().cpu().numpy()\n    train_img_matrix[i * 128:(i + 1) * 128, :, :] = pred\n    \ntest_pids = []\ntest_img_matrix = np.zeros((len(test_pet_ids), 4, 1024))\nfor i, (pid, tensor) in tqdm(enumerate(test_img_loader)):\n    test_pids += [*pid]\n    tensor = tensor.to(\"cuda:0\")\n    pred = model(tensor).detach().cpu().numpy()\n    test_img_matrix[i * 128:(i + 1) * 128, :, :] = pred","b8b133bf":"gc.collect()","4d115e28":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return -cohen_kappa_score(y, preds, weights='quadratic')\n    \n    def fit(self, X, y, initial_coef=[]):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = initial_coef\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']\n    \ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    assert len(rater_a) == len(rater_b)\n\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_rating = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_rating)] for j in range(num_rating)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    rater_a = y\n    rater_b = y_pred\n    min_rating = None\n    max_rating = None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n\n    assert len(rater_a) == len(rater_b)\n\n    min_rating = min(min(rater_a), min(rater_b))\n    max_rating = max(max(rater_a), max(rater_b))\n\n    conf_mat = confusion_matrix(rater_a, rater_b, min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (\n                hist_rater_a[i] * hist_rater_b[j]) \/ num_scored_items\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)","0f6039f1":"class PetDataset(data.Dataset):\n    def __init__(self, \n                 img_tensor,\n                 cat_features, \n                 num_features,\n                 entity_tensor,\n                 desc_tensor,\n                 seq_tensor,\n                 labels):\n        self.img_tensor = img_tensor\n        self.cat_features = cat_features\n        self.num_features = num_features\n        self.entity_tensor = entity_tensor\n        self.desc_tensor = desc_tensor\n        self.seq_tensor = seq_tensor\n        if labels is not None:\n            self.labels = labels\n        else:\n            self.labels = None\n        \n    def __len__(self):\n        return len(self.cat_features)\n    \n    def __getitem__(self, idx):\n        img_feature = self.img_tensor[idx]\n        cat_feature = self.cat_features[idx]\n        num_feature = self.num_features[idx]\n        entity_feature = self.entity_tensor[idx]\n        desc_feature = self.desc_tensor[idx]\n        seq_feature = self.seq_tensor[idx]\n\n        if self.labels is not None:\n            label = self.labels[idx]\n            return [\n                img_feature, cat_feature, num_feature, \n                entity_feature, desc_feature, seq_feature, label]\n        else:\n            return [img_feature, cat_feature, num_feature,\n                    entity_feature, desc_feature, seq_feature]","46ccddfb":"def get_n_params(model):\n    pp=0\n    for p in list(model.parameters()):\n        nn=1\n        for s in list(p.size()):\n            nn = nn*s\n        pp += nn\n    return pp\n\ndef seed_torch(seed=1029):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\ndef run_xgb(params,\n            X,\n            y,\n            X_test,\n            resc,\n            n_splits=10,\n            num_rounds=60000,\n            early_stop=500,\n            verbose_eval=1000):\n    oof_train = np.zeros((X.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n    fold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1213)\n\n    for i, (trn_index, val_index) in enumerate(fold.split(X, y.astype(int))):\n        X_tr = X.iloc[trn_index, :]\n        X_val = X.iloc[val_index, :]\n\n        y_tr = y[trn_index]\n        y_val = y[val_index]\n        d_train = xgb.DMatrix(\n            data=X_tr, label=y_tr, feature_names=X_tr.columns)\n        d_valid = xgb.DMatrix(\n            data=X_val, label=y_val, feature_names=X_val.columns)\n\n        watchlist = [(d_train, \"train\"), (d_valid, \"valid\")]\n        model = xgb.train(\n            params=params,\n            dtrain=d_train,\n            num_boost_round=num_rounds,\n            evals=watchlist,\n            early_stopping_rounds=early_stop,\n            verbose_eval=verbose_eval)\n        valid_pred = model.predict(\n            xgb.DMatrix(X_val, feature_names=X_val.columns),\n            ntree_limit=model.best_ntree_limit)\n        test_pred = model.predict(\n            xgb.DMatrix(X_test, feature_names=X_test.columns),\n            ntree_limit=model.best_ntree_limit)\n        oof_train[val_index] = valid_pred\n        oof_test[:, i] = test_pred\n    return model, oof_train, oof_test\n\n\nclass Trainer:\n    def __init__(self, \n                 model,\n                 resc,\n                 n_splits=5, \n                 seed=42, \n                 device=\"cuda:0\", \n                 train_batch=16,\n                 val_batch=32,\n                 kwargs={}):\n        self.model = model\n        self.n_splits = n_splits\n        self.seed = seed\n        self.device = device\n        self.train_batch = train_batch\n        self.val_batch = val_batch\n        self.kwargs = kwargs\n        self.resc = resc\n        \n        self.best_score = None\n        self.tag = dt.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n        \n        self.loss_fn = nn.MSELoss(reduction=\"mean\").to(self.device)\n        path = Path(f\"bin\/{self.tag}\")\n        path.mkdir(exist_ok=True, parents=True)\n        self.path = path\n        \n    def fit(self, \n            img_feats, \n            cat_feats, \n            num_feats, \n            entity_feats,\n            desc_feats,\n            seq_feats, \n            answer, \n            n_epochs=30):\n        self.train_preds = np.zeros((train.shape[0]))\n        answer = answer.values.astype(int)\n        fold = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n        cat_feats = cat_feats.values\n        for i, (trn_idx, val_idx) in enumerate(fold.split(img_feats, \n                                                          answer)):\n            self.fold_num = i\n            print(f\"Fold: {i+1}\")\n            img_train, img_val = img_feats[trn_idx], img_feats[val_idx]\n            cat_train, cat_val = cat_feats[trn_idx], cat_feats[val_idx]\n            num_train, num_val = num_feats[trn_idx], num_feats[val_idx]\n            ent_train, ent_val = entity_feats[trn_idx], entity_feats[val_idx]\n            dsc_train, dsc_val = desc_feats[trn_idx], desc_feats[val_idx]\n            seq_train, seq_val = seq_feats[trn_idx], seq_feats[val_idx]\n            y_train, y_val = answer[trn_idx] \/ 4, answer[val_idx] \/ 4\n            \n            valid_preds = self._fit(img_train, \n                                    cat_train, \n                                    num_train,\n                                    ent_train,\n                                    dsc_train,\n                                    seq_train,\n                                    y_train,\n                                    n_epochs,\n                                    img_val,\n                                    cat_val,\n                                    num_val,\n                                    ent_val,\n                                    dsc_val,\n                                    seq_val,\n                                    y_val)\n            self.train_preds[val_idx] = valid_preds\n        \n    def _fit(self, \n             img, \n             cat, \n             num,\n             ent,\n             dsc,\n             seq,\n             y, \n             n_epochs, \n             img_val, \n             cat_val, \n             num_val,\n             ent_val,\n             dsc_val,\n             seq_val,\n             y_val):\n        seed_torch(self.seed)\n        img_tensor = torch.tensor(img, dtype=torch.float32).to(self.device)\n        cat_tensor = torch.tensor(cat, dtype=torch.long).to(self.device)\n        num_tensor = torch.tensor(num, dtype=torch.float32).to(self.device)\n        ent_tensor = torch.tensor(ent, dtype=torch.float32).to(self.device)\n        dsc_tensor = torch.tensor(dsc, dtype=torch.float32).to(self.device)\n        seq_tensor = torch.tensor(seq, dtype=torch.long).to(self.device)\n        y_tensor = torch.tensor(y[:, np.newaxis], dtype=torch.float32).to(self.device)\n        train = PetDataset(img_tensor, \n                           cat_tensor, \n                           num_tensor,\n                           ent_tensor,\n                           dsc_tensor,\n                           seq_tensor,\n                           y_tensor)\n        train_loader = data.DataLoader(train, \n                                       batch_size=self.train_batch, shuffle=True)\n        img_eval = torch.tensor(img_val, dtype=torch.float32).to(self.device)\n        cat_eval = torch.tensor(cat_val, dtype=torch.long).to(self.device)\n        num_eval = torch.tensor(num_val, dtype=torch.float32).to(self.device)\n        ent_eval = torch.tensor(ent_val, dtype=torch.float32).to(self.device)\n        dsc_eval = torch.tensor(dsc_val, dtype=torch.float32).to(self.device)\n        seq_eval = torch.tensor(seq_val, dtype=torch.long).to(self.device)\n        y_eval = torch.tensor(y_val[:, np.newaxis], dtype=torch.float32).to(self.device)\n        eval_ = PetDataset(img_eval,\n                           cat_eval,\n                           num_eval,\n                           ent_eval,\n                           dsc_eval,\n                           seq_eval,\n                           y_eval)\n        eval_loader = data.DataLoader(eval_,\n                                      batch_size=self.val_batch, shuffle=False)\n        \n        model = self.model(**self.kwargs)\n        model = model.to(self.device)\n        optimizer = optim.Adam(model.parameters())\n        best_score = np.inf\n        mb = master_bar(range(n_epochs))\n        \n        for epoch in mb:\n            model.train()\n            avg_loss = 0.\n            for i_batch, c_batch, n_batch, e_batch, d_batch, s_batch, y_batch in progress_bar(train_loader, parent=mb):\n                y_pred = model(i_batch, c_batch, n_batch, e_batch, d_batch, s_batch)\n                loss = self.loss_fn(y_pred, y_batch)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                avg_loss += loss.item() \/ len(train_loader)\n            valid_preds, avg_val_loss = self._val(eval_loader, model)\n            print(f\"epoch {epoch+1}\/{n_epochs}\")\n            print(f\"avg_loss: {avg_loss:.4f}\")\n            print(f\"avg_val_loss: {avg_val_loss:.4f}\")\n            if best_score > avg_val_loss:\n                torch.save(model.state_dict(),\n                           self.path \/ f\"best{self.fold_num}.pt\")\n                print(f\"Save model on epoch {epoch + 1}\")\n                best_score = avg_val_loss\n        model.load_state_dict(torch.load(self.path \/ f\"best{self.fold_num}.pt\"))\n        valid_preds, avg_val_loss = self._val(eval_loader, model)\n        print(f\"Validation loss: {avg_val_loss}\")\n        return valid_preds\n    \n    def _val(self, loader, model):\n        model.eval()\n        valid_preds = np.zeros(loader.dataset.cat_features.size(0))\n        avg_val_loss = 0.\n\n        for i, (i_batch, c_batch, n_batch, e_batch, d_batch, s_batch, y_batch) in enumerate(loader):\n            with torch.no_grad():\n                y_pred = model(i_batch, c_batch, n_batch, e_batch, d_batch, s_batch).detach()\n                avg_val_loss += self.loss_fn(y_pred,\n                                             y_batch).item() \/ len(loader)\n                valid_preds[i * self.val_batch:(i + 1) * self.val_batch] = y_pred.cpu().numpy()[:, 0]\n        return valid_preds, avg_val_loss","b6b2b569":"class Attention(nn.Module):\n    def __init__(self, feature_dims, step_dims, n_middle, n_attention,\n                 **kwargs):\n        super(Attention, self).__init__(**kwargs)\n        self.support_masking = True\n        self.feature_dims = feature_dims\n        self.step_dims = step_dims\n        self.n_middle = n_middle\n        self.n_attention = n_attention\n        self.features_dim = 0\n\n        self.lin1 = nn.Linear(feature_dims, n_middle, bias=False)\n        self.lin2 = nn.Linear(n_middle, n_attention, bias=False)\n\n    def forward(self, x, mask=None):\n        step_dims = self.step_dims\n\n        eij = self.lin1(x)\n        eij = torch.tanh(eij)\n        eij = self.lin2(eij)\n\n        a = torch.exp(eij).reshape(-1, self.n_attention, step_dims)\n\n        if mask is not None:\n            a = a * mask\n\n        a = a \/ torch.sum(a, 2, keepdim=True) + 1e-10\n\n        weighted_input = torch.bmm(a, x)\n        return torch.sum(weighted_input, 1)","b9a4323d":"class NeuralNet(nn.Module):\n    def __init__(self, \n                 emb_dims, \n                 num_dims, \n                 img_linear,\n                 ent_linear,\n                 dsc_linear,\n                 seq_linear,\n                 linear_size,\n                 embedding_matrix,\n                 hidden_size,\n                 maxlen,\n                 n_attention):\n        super(NeuralNet, self).__init__()\n        self.img_linear = img_linear\n        n_features, embed_size = embedding_matrix.shape\n        self.img_lin = nn.Linear(1024, img_linear)\n        self.img_attn = Attention(img_linear, 4, 2, 2)\n        self.ent_lin = nn.Linear(300, ent_linear)\n        self.dsc_lin = nn.Linear(300, dsc_linear)\n        self.seq_emb = nn.Embedding(n_features, embed_size)\n        self.seq_emb.weight = nn.Parameter(torch.tensor(\n            embedding_matrix, dtype=torch.float32))\n        self.seq_emb.weight.requires_grad = False\n        self.seq_emb_dropout = nn.Dropout2d(0.2)\n        self.lstm = nn.LSTM(\n            embed_size, hidden_size, bidirectional=True, batch_first=True)\n        self.attn = Attention(hidden_size * 2, maxlen, n_attention,\n                              n_attention)\n        self.seq_lin = nn.Linear(hidden_size * 2, seq_linear)\n\n        self.embeddings = nn.ModuleList(\n            [nn.Embedding(x, y) for x, y in emb_dims])\n        n_emb_out = sum([y for x, y in emb_dims])\n        self.fc1 = nn.Linear(\n            img_linear + n_emb_out + num_dims + ent_linear + dsc_linear + seq_linear, \n            linear_size)\n        self.bn1 = nn.BatchNorm1d(linear_size)\n        self.fc2 = nn.Linear(linear_size, 1)\n        self.drop = nn.Dropout(0.2)\n        self.relu = nn.ReLU()\n        self.tanh = nn.Tanh()\n\n    def forward(self, i, c, n, e, d, s):\n        imgs = torch.zeros(i.size(0), 4, self.img_linear).to(\"cuda:0\")\n        for j in range(4):\n            img_f = self.img_lin(i[:, j, :])\n            imgs[:, j, :] = img_f\n        img_feats = self.drop(self.tanh(self.img_attn(imgs)))\n        ent_feats = self.drop(self.tanh(self.ent_lin(e)))\n        dsc_feats = self.drop(self.tanh(self.dsc_lin(d)))\n        emb = [\n            emb_layer(c[:, j]) for j, emb_layer in enumerate(self.embeddings)\n        ]\n        emb = self.drop(self.tanh(torch.cat(emb, 1)))\n        \n        h_seq = self.seq_emb(s)\n        h_seq = torch.squeeze(\n            self.seq_emb_dropout(torch.unsqueeze(h_seq, 0)))\n        h_lstm, _ = self.lstm(h_seq)\n        h_attn = self.attn(h_lstm)\n        h_lin = self.tanh(self.seq_lin(h_attn))\n        data = torch.cat([img_feats, emb, n, ent_feats, dsc_feats, h_lin], 1)\n        out = self.relu(self.fc1(data))\n        # out = self.drop(out)\n        out = self.bn1(out)\n        out = self.fc2(out)\n        return out","34ea2a65":"emb_dims = [(2, 1), (n_breed1, 3), (n_breed2, 3), (3, 1), (n_color1, 1),\n            (n_color2, 1), (n_color3, 1), (4, 1), (3, 1), (3, 1),\n            (3, 1), (3, 1), (3, 1), (19, 1), (14, 1),(n_langs, 1)]\nnum_dims = X_test_ss.shape[1]\ntrainer = Trainer(\n    NeuralNet,\n    resc=train_resc,\n    n_splits=10,\n    train_batch=64,\n    val_batch=512,\n    seed=328,\n    kwargs={\n        \"emb_dims\": emb_dims,\n        \"num_dims\": num_dims,\n        \"img_linear\": 48,\n        \"linear_size\": 144,\n        \"ent_linear\": 10,\n        \"dsc_linear\": 10,\n        \"seq_linear\": 20,\n        \"embedding_matrix\": embedding_matrix,\n        \"hidden_size\": 64,\n        \"maxlen\": 70,\n        \"n_attention\": 20\n    })","10b0179d":"trainer.fit(train_img_matrix, \n            X_train_cat, \n            X_train_ss, \n            train_entity, \n            train_desc, \n            x_train_text, target, 10)","d297de7d":"bin_path = trainer.path\ntest_preds = np.zeros((X_test_cat.shape[0]))\ni_tensor = torch.tensor(test_img_matrix, dtype=torch.float32).to(trainer.device)\nc_tensor = torch.tensor(X_test_cat.values, dtype=torch.long).to(trainer.device)\nn_tensor = torch.tensor(X_test_ss, dtype=torch.float32).to(trainer.device)\ne_tensor = torch.tensor(test_entity, dtype=torch.float32).to(trainer.device)\nd_tensor = torch.tensor(test_desc, dtype=torch.float32).to(trainer.device)\ns_tensor = torch.tensor(x_test_text, dtype=torch.long).to(trainer.device)\ntest_dataset = PetDataset(i_tensor, \n                          c_tensor, \n                          n_tensor,\n                          e_tensor,\n                          d_tensor,\n                          s_tensor,\n                          labels=None)\ntest_loader = data.DataLoader(test_dataset, batch_size=512, shuffle=False)\n\nfor path in bin_path.iterdir():\n    print(f\"using {str(path)}\")\n    model = NeuralNet(**trainer.kwargs)\n    model.to(\"cuda:0\")\n    model.load_state_dict(torch.load(path))\n\n    model.eval()\n    temp = np.zeros((X_test_cat.shape[0]))\n    for i, (i_batch, c_batch, n_batch, e_batch, d_batch, s_batch) in enumerate(test_loader):\n        i_batch = i_batch.to(trainer.device)\n        with torch.no_grad():\n            y_pred = model(i_batch, c_batch, n_batch, e_batch, d_batch, s_batch).detach()\n            temp[i * 512:(i + 1) * 512] = y_pred.cpu().numpy()[:, 0]\n    test_preds += temp \/ trainer.n_splits","71e0fb1b":"test_preds.mean()","6e8595ce":"class ImageDataset(data.Dataset):\n    def __init__(self, imat):\n        self.imat = imat\n        \n    def __len__(self):\n        return len(self.imat)\n    \n    def __getitem__(self, idx):\n        image = self.imat[idx]\n        \n        return [image]","8f8b7ea4":"train_i_tensor = torch.tensor(train_img_matrix, dtype=torch.float32).to(\"cuda:0\")\ntest_i_tensor = torch.tensor(test_img_matrix, dtype=torch.float32).to(\"cuda:0\")\ntrain_dataset = ImageDataset(train_i_tensor)\nbatch = 256\nn_img_dim = 48\ntrain_loader = data.DataLoader(train_dataset,\n                               batch_size=batch,\n                               shuffle=False)\nX_train_img = np.zeros((len(train_pet_ids), n_img_dim))\n\ntest_dataset = ImageDataset(test_i_tensor)\ntest_loader = data.DataLoader(test_dataset,\n                              batch_size=batch,\n                              shuffle=False)\nX_test_img = np.zeros((len(test_pet_ids), n_img_dim))\nbin_path = trainer.path\nfor path in bin_path.iterdir():\n    model = NeuralNet(**trainer.kwargs)\n    model.to(\"cuda:0\")\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    temp = np.zeros((len(train_pet_ids), n_img_dim))\n    \n    for i, (i_batch, ) in tqdm(enumerate(train_loader)):\n        with torch.no_grad():\n            imgs = torch.zeros(i_batch.size(0), 4, n_img_dim).to(\"cuda:0\")\n            for j in range(4):\n                pre = model.img_lin(i_batch[:, j, :])\n                imgs[:, j, :] = pre\n            y_pred = model.img_attn(imgs).detach()\n            temp[i * batch:(i + 1) * batch, :] = y_pred.cpu().numpy()\n    X_train_img += temp \/ trainer.n_splits\n    \n    temp = np.zeros((len(test_pet_ids), n_img_dim))\n    for i, (i_batch, ) in tqdm(enumerate(test_loader)):\n        with torch.no_grad():\n            imgs = torch.zeros(i_batch.size(0), 4, n_img_dim).to(\"cuda:0\")\n            for j in range(4):\n                pre = model.img_lin(i_batch[:, j, :])\n                imgs[:, j, :] = pre\n            y_pred = model.img_attn(imgs).detach()\n            temp[i * batch:(i + 1) * batch, :] = y_pred.cpu().numpy()\n    X_test_img += temp \/ trainer.n_splits","c6374d35":"X_train_img.shape, X_test_img.shape","8b5630b2":"train_img = pd.DataFrame(data=X_train_img, columns=[\n    f\"img{i}\" for i in range(X_train_img.shape[1])\n])\ntest_img = pd.DataFrame(data=X_test_img, columns=[\n    f\"img{i}\" for i in range(X_test_img.shape[1])\n])","c86b9fd7":"num_columns = X_train_num.columns\nX_train_num_df = pd.DataFrame(data=X_train_ss, columns=num_columns)\nX_test_num_df = pd.DataFrame(data=X_test_ss, columns=num_columns)\n\nX_test_num_df.index = X_test_cat.index\ntest_img.index = X_test_cat.index\n\nX_train_all = pd.concat([X_train_num_df, X_train_cat, train_img], axis=1)\nX_test_all = pd.concat([X_test_num_df, X_test_cat, test_img], axis=1)\n\nprint(X_train_all.shape, X_test_all.shape)\nX_train_all.head()","7b47e2d8":"\"AdoptionSpeed\" in X_train_all.columns, \"AdoptionSpeed\" in X_test_all.columns","782a60b4":"X_train_all.columns.tolist() == X_test_all.columns.tolist()","194662ee":"class CategoryDataset(data.Dataset):\n    def __init__(self, category):\n        self.category = category\n        \n    def __len__(self):\n        return len(self.category)\n    \n    def __getitem__(self, idx):\n        category = self.category[idx, :]\n        return [category]","5c038c99":"c_train = torch.tensor(X_train_cat.values, dtype=torch.long).to(\"cuda:0\")\nc_test = torch.tensor(X_test_cat.values, dtype=torch.long).to(\"cuda:0\")\ntrain_dataset = CategoryDataset(c_train)\ntest_dataset = CategoryDataset(c_test)\ntrain_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=False)\ntest_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nX_train_cat_ = np.zeros((len(train_pet_ids), 20))\nX_test_cat_ = np.zeros((len(test_pet_ids), 20))\nbin_path = trainer.path\nfor path in bin_path.iterdir():\n    model = NeuralNet(**trainer.kwargs)\n    model.to(\"cuda:0\")\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    temp = np.zeros((len(train_pet_ids), 20))\n    for i, (c_batch, ) in tqdm(enumerate(train_loader)):\n        with torch.no_grad():\n            y_pred = [model.embeddings[i](c_batch[:, i]) for i in range(len(model.embeddings))]\n            y_pred = torch.cat(y_pred, 1).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_train_cat_ += temp \/ trainer.n_splits\n    temp = np.zeros((len(test_pet_ids), 20))\n    for i, (c_batch, ) in tqdm(enumerate(test_loader)):\n        with torch.no_grad():\n            y_pred = [model.embeddings[i](c_batch[:, i]) for i in range(len(model.embeddings))]\n            y_pred = torch.cat(y_pred, 1).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_test_cat_ += temp \/ trainer.n_splits","79ee3b2f":"X_train_cat_.shape, X_test_cat_.shape","f01ec0ee":"train_emb = pd.DataFrame(data=X_train_cat_, columns=[\n    f\"emb{i}\" for i in range(X_train_cat_.shape[1])\n])\ntest_emb = pd.DataFrame(data=X_test_cat_, columns=[\n    f\"emb{i}\" for i in range(X_test_cat_.shape[1])\n])","25b6f632":"test_emb.index = X_test_all.index\n\nX_train_all = pd.concat([X_train_all, train_emb], axis=1)\nX_test_all = pd.concat([X_test_all, test_emb], axis=1)\n\nprint(X_train_all.shape, X_test_all.shape)\nX_train_all.head()","548fe145":"class WordVecDataset(data.Dataset):\n    def __init__(self, wv):\n        self.wv = wv\n        \n    def __len__(self):\n        return len(self.wv)\n    \n    def __getitem__(self, idx):\n        wv = self.wv[idx, :]\n        return [wv]","cb33f6b2":"e_train = torch.tensor(train_entity, dtype=torch.float32).to(\"cuda:0\")\ne_test = torch.tensor(test_entity, dtype=torch.float32).to(\"cuda:0\")\ntrain_dataset = WordVecDataset(e_train)\ntest_dataset = WordVecDataset(e_test)\ntrain_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=False)\ntest_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nX_train_ent = np.zeros((len(train_pet_ids), 10))\nX_test_ent = np.zeros((len(test_pet_ids), 10))\nbin_path = trainer.path\nfor path in bin_path.iterdir():\n    model = NeuralNet(**trainer.kwargs)\n    model.to(\"cuda:0\")\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    temp = np.zeros((len(train_pet_ids), 10))\n    for i, (e_batch, ) in tqdm(enumerate(train_loader), ascii=True):\n        with torch.no_grad():\n            y_pred = model.ent_lin(e_batch).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_train_ent += temp \/ trainer.n_splits\n    temp = np.zeros((len(test_pet_ids), 10))\n    for i, (e_batch, ) in tqdm(enumerate(test_loader), ascii=True):\n        with torch.no_grad():\n            y_pred = model.ent_lin(e_batch).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_test_ent += temp \/ trainer.n_splits","454cacc6":"X_train_ent.shape, X_test_ent.shape","41697eea":"train_ent = pd.DataFrame(data=X_train_ent, columns=[\n    f\"ent{i}\" for i in range(X_train_ent.shape[1])\n])\ntest_ent = pd.DataFrame(data=X_test_ent, columns=[\n    f\"ent{i}\" for i in range(X_test_ent.shape[1])\n])\ntest_ent.index = X_test_all.index\n\nX_train_all = pd.concat([X_train_all, train_ent], axis=1)\nX_test_all = pd.concat([X_test_all, test_ent], axis=1)\n\nprint(X_train_all.shape, X_test_all.shape)\nX_train_all.head()","bb7cd4e8":"d_train = torch.tensor(train_desc, dtype=torch.float32).to(\"cuda:0\")\nd_test = torch.tensor(test_desc, dtype=torch.float32).to(\"cuda:0\")\ntrain_dataset = WordVecDataset(d_train)\ntest_dataset = WordVecDataset(d_test)\ntrain_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=False)\ntest_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nX_train_dsc = np.zeros((len(train_pet_ids), 10))\nX_test_dsc = np.zeros((len(test_pet_ids), 10))\nbin_path = trainer.path\nfor path in bin_path.iterdir():\n    model = NeuralNet(**trainer.kwargs)\n    model.to(\"cuda:0\")\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    temp = np.zeros((len(train_pet_ids), 10))\n    for i, (d_batch, ) in tqdm(enumerate(train_loader), ascii=True):\n        with torch.no_grad():\n            y_pred = model.dsc_lin(d_batch).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_train_dsc += temp \/ trainer.n_splits\n    temp = np.zeros((len(test_pet_ids), 10))\n    for i, (d_batch, ) in tqdm(enumerate(test_loader), ascii=True):\n        with torch.no_grad():\n            y_pred = model.dsc_lin(d_batch).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_test_dsc += temp \/ trainer.n_splits","eccf4f9d":"X_train_dsc.shape, X_test_dsc.shape","21782cf4":"train_dsc = pd.DataFrame(data=X_train_dsc, columns=[\n    f\"dsc{i}\" for i in range(X_train_dsc.shape[1])\n])\ntest_dsc = pd.DataFrame(data=X_test_dsc, columns=[\n    f\"dsc{i}\" for i in range(X_test_dsc.shape[1])\n])\ntest_dsc.index = X_test_all.index\n\nX_train_all = pd.concat([X_train_all, train_dsc], axis=1)\nX_test_all = pd.concat([X_test_all, test_dsc], axis=1)\n\nprint(X_train_all.shape, X_test_all.shape)\nX_train_all.head()","8e9820c6":"class SeqDataset(data.Dataset):\n    def __init__(self, seq):\n        self.seq = seq\n        \n    def __len__(self):\n        return len(self.seq)\n    \n    def __getitem__(self, idx):\n        seq = self.seq[idx, :]\n        return [seq]","cccc45b9":"s_train = torch.tensor(x_train_text, dtype=torch.long).to(\"cuda:0\")\ns_test = torch.tensor(x_test_text, dtype=torch.long).to(\"cuda:0\")\ntrain_dataset = SeqDataset(s_train)\ntest_dataset = SeqDataset(s_test)\ntrain_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=False)\ntest_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nX_train_seq = np.zeros((len(train_pet_ids), 20))\nX_test_seq = np.zeros((len(test_pet_ids), 20))\nbin_path = trainer.path\nfor path in bin_path.iterdir():\n    model = NeuralNet(**trainer.kwargs)\n    model.to(\"cuda:0\")\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    temp = np.zeros((len(train_pet_ids), 20))\n    for i, (s_batch, ) in tqdm(enumerate(train_loader), ascii=True):\n        with torch.no_grad():\n            h_emb = model.seq_emb(s_batch)\n            h_emb = torch.squeeze(model.seq_emb_dropout(torch.unsqueeze(h_emb, 0)))\n            h_lstm, _ = model.lstm(h_emb)\n            h_attn = model.attn(h_lstm)\n            y_pred = model.seq_lin(h_attn).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_train_seq += temp \/ trainer.n_splits\n    temp = np.zeros((len(test_pet_ids), 20))\n    for i, (s_batch, ) in tqdm(enumerate(test_loader), ascii=True):\n        with torch.no_grad():\n            h_emb = model.seq_emb(s_batch)\n            h_emb = torch.squeeze(model.seq_emb_dropout(torch.unsqueeze(h_emb, 0)))\n            h_lstm, _ = model.lstm(h_emb)\n            h_attn = model.attn(h_lstm)\n            y_pred = model.seq_lin(h_attn).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_test_seq += temp \/ trainer.n_splits","b6775f20":"X_train_seq.shape, X_test_seq.shape","4ee57e45":"train_seq = pd.DataFrame(data=X_train_seq, columns=[\n    f\"seq{i}\" for i in range(X_train_seq.shape[1])\n])\ntest_seq = pd.DataFrame(data=X_test_seq, columns=[\n    f\"seq{i}\" for i in range(X_test_seq.shape[1])\n])\ntest_seq.index = X_test_all.index\n\nX_train_all = pd.concat([X_train_all, train_seq], axis=1)\nX_test_all = pd.concat([X_test_all, test_seq], axis=1)\n\nprint(X_train_all.shape, X_test_all.shape)\nX_train_all.head()","a96a1be0":"xgb_params = {\n    \"eval_metric\": \"rmse\",\n    \"seed\": 1337,\n    \"eta\": 0.0123,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.85,\n    \"tree_method\": \"gpu_hist\",\n    \"device\": \"gpu\",\n    \"silent\": 1\n}\n\nxgb_X = X_train_all\nxgb_y = target\nxgb_X_test = X_test_all\n\nmodel, oof_train_xgb, oof_test_xgb= run_xgb(\n    xgb_params, \n    xgb_X, \n    xgb_y, \n    xgb_X_test,\n    resc=train_resc,\n    n_splits=10,\n    num_rounds=10000)","f9b55701":"def run_lgb(params,\n            X,\n            y,\n            X_test,\n            resc,\n            cat_features,\n            n_splits=10,\n            early_stop=500):\n    oof_train = np.zeros((X.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n    fold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=229)\n\n    for i, (trn_index, val_index) in enumerate(fold.split(X, \n                                                        y.astype(int))):\n        X_tr = X.iloc[trn_index, :]\n        X_val = X.iloc[val_index, :]\n\n        y_tr = y[trn_index]\n        y_val = y[val_index]\n        model = lgb.LGBMRegressor(**params)\n        model.fit(X_tr, \n                  y_tr, \n                  eval_set=(X_val, y_val),\n                  verbose=500,\n                  early_stopping_rounds=early_stop,\n                  categorical_feature=cat_features)\n        valid_pred = model.predict(X_val, num_iteration=model.best_iteration_)\n        test_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n        oof_train[val_index] = valid_pred\n        oof_test[:, i] = test_pred\n    return model, oof_train, oof_test","7cf0ceb6":"lgb_params = {\n    \"boosting_type\": \"gbdt\",\n    \"num_leaves\": 146,\n    \"max_depth\": 12,\n    \"max_bin\": 32,\n    \"learning_rate\": 0.01,\n    \"n_estimators\": 10000,\n    \"subsample\": 0.9212945843023237,\n    \"subsample_freq\": 2,\n    \"colsample_bytree\": 0.6334740217238963,\n    \"reg_lambda\": 1.543309192604612,\n    \"min_child_samples\": 45,\n    \"min_child_weight\": 0.5878240657385082,\n    \"min_split_gain\": 0.004619759404679957,\n    \"n_jobs\": -1\n}\n\nmodel, oof_train, oof_test = run_lgb(\n    lgb_params, \n    xgb_X, \n    xgb_y, \n    xgb_X_test,\n    train_resc,\n    cat_features,\n    n_splits=6,\n    early_stop=500)","bad9af7d":"xgb_X.columns[np.argwhere(model.feature_importances_ > 2000).reshape(-1)]","4dc665f6":"def plot_pred(pred):\n    sns.distplot(pred, kde=True, hist_kws={\"range\": [0, 5]})","e6a7e8da":"plot_pred(oof_train)","3263d5fd":"plot_pred(oof_train_xgb)","f8c1c46a":"oof_train.shape","b107d10e":"plot_pred(oof_test.mean(axis=1))","dba539bd":"plot_pred(oof_test_xgb.mean(1))","87f42592":"plot_pred(trainer.train_preds * 4)","eb670b64":"plot_pred(test_preds * 4)","14094381":"lgb_xgb = 0.5 * oof_train + 0.5 * oof_train_xgb\nlgb_xgb_test = 0.5 * oof_test.mean(1) + 0.5 * oof_test_xgb.mean(1)\nplot_pred(lgb_xgb)\nplot_pred(lgb_xgb_test)","680002f9":"nn_preds = np.clip(trainer.train_preds, a_min=0.0, a_max=1.0)\nnn_preds_test = np.clip(test_preds, a_min=0.0, a_max=1.0)\nplot_pred(nn_preds * 4)\nplot_pred(nn_preds_test * 4)","276b020b":"lgb_xgb_nn = 0.9 * lgb_xgb + 0.1 * (nn_preds * 4)\nlgb_xgb_nn_test = 0.9 * lgb_xgb_test + 0.1 * nn_preds_test * 4","40412b3f":"plot_pred(lgb_xgb_nn)","3f56438b":"plot_pred(lgb_xgb_nn_test)","3a5b903b":"opt = OptimizedRounder()\nopt.fit(lgb_xgb, target, [1.6, 2.1, 2.8, 3.5])\ncoeff = opt.coefficients()\nvalid_pred = opt.predict(lgb_xgb, coeff)\nqwk = quadratic_weighted_kappa(xgb_y, valid_pred)\nprint(\"QWK = \", qwk)\ncoeffs = coeff.copy()\ntrain_predictions = opt.predict(lgb_xgb, coeffs).astype(np.int8)\nprint(f\"train_preds: {Counter(train_predictions)}\")\ntest_predictions = opt.predict(lgb_xgb_test, coeffs).astype(np.int8)\nprint(f\"test_preds: {Counter(test_predictions)}\")\nsubmission = pd.DataFrame({\"PetID\": test.PetID.values, \"AdoptionSpeed\": test_predictions})\nsubmission.to_csv(\"submission_tree.csv\", index=False)\nsubmission.head()","4bb93828":"opt = OptimizedRounder()\nopt.fit(lgb_xgb_nn, target, [1.5, 2.0, 2.5, 3.5])\ncoeff = opt.coefficients()\nvalid_pred = opt.predict(lgb_xgb_nn, coeff)\nqwk = quadratic_weighted_kappa(xgb_y, valid_pred)\nprint(\"QWK = \", qwk)\ncoeffs = coeff.copy()\ntrain_predictions = opt.predict(lgb_xgb_nn, coeffs).astype(np.int8)\nprint(f\"train_preds: {Counter(train_predictions)}\")\ntest_predictions = opt.predict(lgb_xgb_nn_test, coeffs).astype(np.int8)\nprint(f\"test_preds: {Counter(test_predictions)}\")\nsubmission = pd.DataFrame({\"PetID\": test.PetID.values, \"AdoptionSpeed\": test_predictions})\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","657b23e9":"opt = OptimizedRounder()\nopt.fit(nn_preds * 4, target, [1.5, 2.0, 2.5, 3.5])\ncoeff = opt.coefficients()\nvalid_pred = opt.predict(nn_preds * 4, coeff)\nqwk = quadratic_weighted_kappa(target, valid_pred)\nprint(\"QWK = \", qwk)\ncoeffs = coeff.copy()\ntrain_predictions = opt.predict(nn_preds * 4, coeffs).astype(np.int8)\nprint(f\"train_preds: {Counter(train_predictions)}\")\ntest_predictions = opt.predict(nn_preds_test * 4, coeffs).astype(np.int8)\nprint(f\"test_preds: {Counter(test_predictions)}\")\nsubmission = pd.DataFrame({\"PetID\": test.PetID.values, \"AdoptionSpeed\": test_predictions})\nsubmission.to_csv(\"submission_nn.csv\", index=False)\nsubmission.head()","808b4638":"## Image","1b0e0e45":"## Drop columns","1e4891ae":"## Sequences","c1788baa":"## Load data","9fe0ea9d":"## Categorical Target Encoding","d7744067":"## Sanitize","76319bb7":"## XGBoost","7a2db851":"### State stats","cb738f0b":"## Entity and Desc","4da85fde":"## Data Loading","2b461896":"## Sequences","5db32775":"### Name features","834142ce":"## Image model loading","44d19b6b":"## Feature engineering","f73f3af9":"## DataLoader","e5759af6":"## Train","1f3a7307":"## Add Breed Mapping","1e222ac3":"## Trainer","8328031f":"## Run LGBM","6a005316":"## Images","6d159145":"## Post process","b6ab3483":"## Merge processed DataFrames with base train\/test DataFrame","d89eaa18":"## Category Embedding","8dd6ce06":"## Metrics","28b1207a":"## Aggregate sentiment data and metadata","3a1f7c06":"## Numerical Features","27d109e0":"## Tfidf","8e4a953f":"## Metadata and Sentiment data","accb0882":"## Image size features","02d4bce7":"## Libraries","cbf5d2a1":"## GroupStratifiedKFold"}}