{"cell_type":{"29a8e925":"code","cbf0ade7":"code","240b11e9":"code","bde57765":"code","a3fc598e":"code","836f05e8":"code","2e473905":"code","ef33131b":"code","b3a93469":"code","19203013":"code","58b79ed0":"code","5b9f0aa8":"code","1cbc6540":"code","efc31065":"code","7f6c5ff0":"code","4362e24c":"code","44acf292":"code","9305cf9e":"code","2e11b72f":"code","05c2c01a":"code","06f12825":"code","2b30fcb0":"code","079f6a5c":"code","f8231ec4":"code","409eb539":"code","7fc04246":"code","74a61747":"code","ce0bb144":"code","827358f6":"code","d1660186":"code","5dbcafb5":"code","73840a3b":"markdown","d6311e56":"markdown","4af2b9ed":"markdown","03138238":"markdown","710b5a44":"markdown","36156bf5":"markdown","58141f3d":"markdown","e2899001":"markdown","a0be3f93":"markdown","5ec4849f":"markdown","9150f217":"markdown","06d7e7a5":"markdown","1695d8b5":"markdown","a8309b0c":"markdown","b3bf624a":"markdown","97641e8f":"markdown","4775dbdf":"markdown","b40aed0f":"markdown","2428c9b0":"markdown","165fc792":"markdown","f835d03e":"markdown","3261d437":"markdown","62f63735":"markdown","956ade9c":"markdown","ec627ba0":"markdown","e401635c":"markdown","22c45fc7":"markdown","f7db2610":"markdown","335300f1":"markdown","2a02f834":"markdown","b4feda4a":"markdown","54acdb6f":"markdown","d139abc5":"markdown","69b91bc1":"markdown","61313c84":"markdown","33c58ee2":"markdown","5bedacf6":"markdown","59a87c38":"markdown","c90ebf88":"markdown"},"source":{"29a8e925":"from pathlib import Path\n\nDATA_DIR = Path(\"\/kaggle\/input\")\nif (DATA_DIR \/ \"ucfai-core-fa19-cnns\").exists():\n    DATA_DIR \/= \"ucfai-core-fa19-cnns\"\nelif DATA_DIR.exists():\n    # no-op to keep the proper data path for Kaggle\n    pass\nelse:\n    # You'll need to download the data from Kaggle and place it in the `data\/`\n    #   directory beside this notebook.\n    # The data should be here: https:\/\/kaggle.com\/c\/ucfai-core-fa19-cnns\/data\n    DATA_DIR = Path(\"data\")","cbf0ade7":"# standard imports (Numpy, Pandas, Matplotlib)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\nfrom PIL import Image\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n# PyTorch imports \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.utils\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch import optim\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.models import resnet18\nfrom torchvision import transforms\nfrom torchsummary import summary\n\n# Extras\nimport time\nimport os\nimport glob","240b11e9":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n%pylab inline\nrandom.seed(42)","bde57765":"input_size = (224,224)\nbatch_size = 32\nnum_workers = 4","a3fc598e":"data_transforms = {\n    'Train': transforms.Compose([transforms.Resize(input_size),\n                          transforms.ToTensor(),\n                          transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                 std=[0.5, 0.5, 0.5])\n    ]),\n    'Validation': transforms.Compose([transforms.Resize(input_size),\n                          transforms.ToTensor(),\n                          transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                 std=[0.5, 0.5, 0.5])\n    ]),\n    'Test': transforms.Compose([transforms.Resize(input_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                 std=[0.5, 0.5, 0.5])\n    ])\n}","836f05e8":"image_datasets = {\n    x: ImageFolder(os.path.join(DATA_DIR, x),data_transforms[x])\n    for x in ['Train', 'Validation']\n}\n\n# dataset class to load images with no labels, for our testing set to submit to\n#   the competition\nclass ImageLoader(Dataset):\n    def __init__(self, root, transform=None):\n        # get image file paths\n        self.images = sorted(\n            glob.glob(os.path.join(root, \"*\")),\n            key=self.glob_format\n        )\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img = Image.open(self.images[idx])\n        if self.transform is not None:\n            img = self.transform(img)\n            return img\n        else:\n            return transforms.ToTensor(img)\n        \n    @staticmethod\n    def glob_format(key):     \n        key = key.split(\"\/\")[-1].split(\".\")[0]     \n        return \"{:04d}\".format(int(key))\n    \nimage_datasets['Test'] = ImageLoader(\n    str(DATA_DIR \/ \"Test\"),\n    transform=data_transforms[\"Test\"]\n)","2e473905":"dataloaders = {\n    x: DataLoader(\n        image_datasets[x],\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers\n    )\n    for x in ['Train', 'Validation']\n}\n\ntest_loader = DataLoader(\n    dataset=image_datasets['Test'],\n    batch_size=1,\n    shuffle=False\n)","ef33131b":"dog_breeds = image_datasets['Train'].classes\nprint(dog_breeds)","b3a93469":"# Just printing the number of images in each dataset we created\n\ndataset_sizes = {x: len(image_datasets[x]) for x in ['Train', 'Validation', 'Test']}\n\nprint('Train Length: {} | Valid Length: {} | Test Length: {}'.format(\n    dataset_sizes['Train'], \n    dataset_sizes['Validation'],\n    dataset_sizes['Test']\n))","19203013":"# Here we're defining what component we'll use to train this model\n# We want to use the GPU if available, if not we use the CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","58b79ed0":"# Plots a given number of images from a PyTorch Data\ndef show_random_imgs(num_imgs):\n    for i in range(num_imgs):\n        # We're plotting images from the training set\n        train_dataset = image_datasets['Train']\n        \n        # Choose a random image\n        rand = np.random.randint(0, len(train_dataset) + 1)\n        \n        # Read in the image\n        ex = img.imread(train_dataset.imgs[rand][0])\n        \n        # Get the image's label\n        breed = dog_breeds[train_dataset.imgs[rand][1]]\n        \n        # Show the image and print out the image's size\n        #   (really the shape of it's array of pixels)\n        plt.imshow(ex)\n        print('Image Shape: ' + str(ex.shape))\n        plt.axis('off')\n        plt.title(breed)\n        plt.show()\n        \n\n# Plots a batch of images served up by PyTorch    \ndef show_batch(batch):\n    # Undo the transformations applied to the images when loading a batch\n    batch = batch.numpy().transpose((1, 2, 0))\n    mean = np.array([0.5, 0.5, 0.5])\n    std = np.array([0.5, 0.5, 0.5])\n    batch = std * batch + mean\n    batch = np.clip(batch, 0, 1)\n    \n    # Plot the batch\n    plt.axis('off')\n    plt.imshow(batch)\n    \n    # pause a bit so that plots are updated\n    plt.pause(0.001)","5b9f0aa8":"show_random_imgs(3)","1cbc6540":"# Get a batch of training data (32 random images)\nimgs, classes = next(iter(dataloaders['Train']))\n\n# This PyTorch function makes a grid of images from a batch for us\nbatch = torchvision.utils.make_grid(imgs)\n\nshow_batch(batch)","efc31065":"# It is good practice to maintain input dimensions as the image is passed\n#   through convolution layers\n# With a default stride of 1, and no padding, a convolution will reduce image\n#   dimenions to:\n#     out = in - m + 1, where _m_ is the size of the kernel and _in_ is a\n#        dimension of the input\n\n# Use this function to calculate the padding size neccessary to create an output\n#   of desired dimensions\n\ndef get_padding(input_dim, output_dim, kernel_size, stride):\n    # Calculates padding necessary to create a certain output size,\n    # given a input size, kernel size and stride\n    padding = (((output_dim - 1) * stride) - input_dim + kernel_size) \/\/ 2\n  \n    if padding < 0:\n        return 0\n    else:\n        return padding","7f6c5ff0":"# Make sure you calculate the padding amount needed to maintain the spatial\n#   size of the input after each Conv layer\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        \n        # nn.Sequential() is simply a container that groups layers into one object\n        # Pass layers into it separated by commas\n        self.block1 = nn.Sequential(\n            \n            # The first convolutional layer. Think about how many channels the\n            #   input starts off with\n            # Let's have this first layer extract 32 features\n            # YOUR CODE HERE\n            raise NotImplementedError()\n            \n            # Don't forget to apply a non-linearity\n            # YOUR CODE HERE\n            raise NotImplementedError()\n        \n        self.block2 =  nn.Sequential(\n            \n            # The second convolutional layer. How many channels does it receive,\n            #   given the number of features extracted by the first layer?\n            # Have this layer extract 64 features\n            # YOUR CODE HERE\n            raise NotImplementedError()\n            \n            # Non linearity\n            # YOUR CODE HERE\n            raise NotImplementedError()\n            \n            # Lets introduce a Batch Normalization layer\n            # YOUR CODE HERE\n            raise NotImplementedError()\n            \n            # Downsample the input with Max Pooling\n            # YOUR CODE HERE\n            raise NotImplementedError()\n        )\n        \n        # Mimic the second block here, except have this block extract 128\n        #   features\n        self.block3 =  nn.Sequential(\n            # YOUR CODE HERE\n            raise NotImplementedError()\n        )\n        \n        # Applying a global pooling layer\n        # Turns the 128 channel rank 4 tensor into a rank 2 tensor of size\n        #   32 x 128 (32 128-length arrays, one for each of the inputs in a\n        #   batch)\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        \n        # Fully connected layer\n        self.fc1 = nn.Linear(128, 512)\n        \n        # Introduce dropout to reduce overfitting\n        self.drop_out = nn.Dropout(0.5)\n        \n        # Final fully connected layer creates the prediction array\n        self.fc2 = nn.Linear(512, len(dog_breeds))\n    \n    # Feed the input through each of the layers we defined \n    def forward(self, x):\n        \n        # Input size changes from (32 x 3 x 224 x 224) to (32 x 32 x 224 x 224)\n        x = self.block1(x)\n        \n        # Size changes from (32 x 32 x 224 x 224) to (32 x 64 x 112 x 112)\n        #   after max pooling\n        x = self.block2(x)\n        \n        # Size changes from (32 x 64 x 112 x 112) to (32 x 128 x 56 x 56)\n        #   after max pooling\n        x = self.block3(x)\n        \n        # Reshapes the input from (32 x 128 x 56 x 56) to (32 x 128)\n        x = self.global_pool(x)\n        x = x.view(x.size(0), -1)\n        \n        # Fully connected layer, size changes from (32 x 128) to (32 x 512)\n        x = self.fc1(x)\n        x = self.drop_out(x)\n        \n        # Size change from (32 x 512) to (32 x 133) to create prediction arrays\n        #   for each of the images in the batch\n        x = self.fc2(x)\n        \n        return x","4362e24c":"model = CNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\nepochs = 5\n\nmodel.to(device)\nsummary(model, (3, 224, 224))","44acf292":"def run_epoch(epoch, model, optimizer, dataloaders, device, phase):\n  \n    running_loss = 0.0\n    running_corrects = 0\n\n    if phase == 'Train':\n        model.train()\n    else:\n        model.eval()\n\n    # Looping through batches\n    for i, (inputs, labels) in enumerate(dataloaders[phase]):\n    \n        # ensures we're doing this calculation on our GPU if possible\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # Zero parameter gradients\n        optimizer.zero_grad()\n    \n        # Calculate gradients only if we're in the training phase\n        with torch.set_grad_enabled(phase == 'Train'):\n      \n            # This calls the forward() function on a batch of inputs\n            outputs = model(inputs)\n\n            # Calculate the loss of the batch\n            loss = criterion(outputs, labels)\n\n            # Gets the predictions of the inputs (highest value in the array)\n            _, preds = torch.max(outputs, 1)\n\n            # Adjust weights through backpropagation if we're in training phase\n            if phase == 'Train':\n                loss.backward()\n                optimizer.step()\n\n        # Document statistics for the batch\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n    \n    # Calculate epoch statistics\n    epoch_loss = running_loss \/ image_datasets[phase].__len__()\n    epoch_acc = running_corrects.double() \/ image_datasets[phase].__len__()\n\n    return epoch_loss, epoch_acc\n","9305cf9e":"def train(model, criterion, optimizer, num_epochs, dataloaders, device):\n    start = time.time()\n\n    best_model_wts = model.state_dict()\n    best_acc = 0.0\n    \n    print('| Epoch\\t | Train Loss\\t| Train Acc\\t| Valid Loss\\t| Valid Acc\\t| Epoch Time |')\n    print('-' * 86)\n    \n    # Iterate through epochs\n    for epoch in range(num_epochs):\n        \n        epoch_start = time.time()\n       \n        # Training phase\n        train_loss, train_acc = run_epoch(epoch, model, optimizer, dataloaders, device, 'Train')\n        \n        # Validation phase\n        val_loss, val_acc = run_epoch(epoch, model, optimizer, dataloaders, device, 'Validation')\n        \n        epoch_time = time.time() - epoch_start\n           \n        # Print statistics after the validation phase\n        print(\"| {}\\t | {:.4f}\\t| {:.4f}\\t| {:.4f}\\t| {:.4f}\\t| {:.0f}m {:.0f}s     |\"\n                      .format(epoch + 1, train_loss, train_acc, val_loss, val_acc, epoch_time \/\/ 60, epoch_time % 60))\n\n        # Copy and save the model's weights if it has the best accuracy thus far\n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_model_wts = model.state_dict()\n\n    total_time = time.time() - start\n    \n    print('-' * 74)\n    print('Training complete in {:.0f}m {:.0f}s'.format(total_time \/\/ 60, total_time % 60))\n    print('Best validation accuracy: {:.4f}'.format(best_acc))\n\n    # load best model weights and return them\n    model.load_state_dict(best_model_wts)\n    return model","2e11b72f":"def test_model(model, num_images):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure(num_images, (10,10))\n\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(dataloaders['Validation']):\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(images.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images\/\/2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title('Actual: {} \\n Prediction: {}'.format(dog_breeds[labels[j]], dog_breeds[preds[j]]))\n                \n                image = images.cpu().data[j].numpy().transpose((1, 2, 0))\n                \n                mean = np.array([0.5, 0.5, 0.5])\n                std = np.array([0.5, 0.5, 0.5])\n                image = std * image + mean\n                image = np.clip(image, 0, 1)\n                \n                plt.imshow(image)\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)","05c2c01a":"# Make sure to comment this out when you go to \"Commit\" the kaggle notebook!\n# otherwise, it'll run this model along with your other models down below.\nmodel = train(model, criterion, optimizer, epochs, dataloaders, device)","06f12825":"torch.save({\n    'model' : CNN(),\n    'epoch' : epochs,\n    'model_state_dict': model.state_dict(),\n    'optimizer' : optimizer,\n    'optimizer_state_dict': optimizer.state_dict(),\n    'criterion' : criterion,\n    'device' : device\n}, 'base_model.pt')","2b30fcb0":"def load_checkpoint(filepath):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    checkpoint = torch.load(filepath, map_location=device)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer = checkpoint['optimizer']\n    optimizer = optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    criterion = checkpoint['criterion']\n    epoch = checkpoint['epoch']\n    model.to(device)\n\n    return model, optimizer, criterion, epoch","079f6a5c":"model, optimizer, criterion, epoch = load_checkpoint('base_model.pt')","f8231ec4":"test_model(model, 6)","409eb539":"class PreTrained_Resnet(nn.Module):\n    def __init__(self):\n        super(PreTrained_Resnet, self).__init__()\n        \n        # Loading up a pretrained ResNet18 model\n        resnet = resnet18(pretrained = True)\n        \n        # Freeze the entire pretrained network\n        for layer in resnet.parameters():\n            layer.requires_grad = False\n            \n        self.feature_extraction = resnet\n        \n        # Write the classifier block for this network      \n            # Tip: ResNet18's feature extraction portion ends up with 1000\n            #   feature maps, and then implements a Global Average Pooling layer\n            # So what would the size and dimension of the output tensor be?\n            # Think about how can we take that output tensor and transform it\n            #   into an array of dog breed predictions...\n        self.classifier = nn.Sequential(\n            # YOUR CODE HERE\n            raise NotImplementedError()\n        )\n    \n    # Write the forward method for this network (it's quite simple since we've\n    #   defined the network in blocks already)\n    def forward(self, x):\n        # YOUR CODE HERE\n        raise NotImplementedError()","7fc04246":"# Instantiate a pretrained network using the class we've just defined (call it\n#  'pretrained')\n\n# YOUR CODE HERE\nraise NotImplementedError()\n\n# Then define the loss function and optimizer to use for training (let's use\n#   Adam again, with the same parameters as before)\n# YOUR CODE HERE\nraise NotImplementedError()\n\n# Define your number of epochs to train and map your model to the gpu\n# Keep epochs to 5 for time purposes during the workshop\n# YOUR CODE HERE\nraise NotImplementedError()\n\nsummary(pretrained, (3,224,224))","74a61747":"pretrained = train(\n    pretrained,\n    criterion2,\n    optimizer2,\n    epochs2,\n    dataloaders,\n    device\n)","ce0bb144":"torch.save({\n    'model' : PreTrained_Resnet(),\n    'epoch' : epochs2,\n    'model_state_dict': pretrained.state_dict(),\n    'optimizer' : optimizer2,\n    'optimizer_state_dict': optimizer2.state_dict(),\n    'criterion' : criterion2,\n    'device' : device\n}, 'pretrained.pt')","827358f6":"pretrained, optimizer2, criterion2, epoch2 = load_checkpoint('pretrained.pt')","d1660186":"test_model(pretrained, 6)","5dbcafb5":"# Run this to generate the submission file for the competition!\n### Make sure to name your model variable \"pretrained\" ###\n\n# generate predictions\npreds = []\npretrained = pretrained.to(device)\npretrained.eval()\nfor img in test_loader:\n    outputs = pretrained(img.to(device))\n    _, outputs = torch.max(outputs, 1)\n    preds += [outputs.item()]\n\n# create our pandas dataframe for our submission file. Squeeze removes\n#   dimensions of 1 in a numpy matrix Ex: (161, 1) -> (161,)\nindicies = [\"{}.jpg\".format(x) for x in range(len(image_datasets['Test']))]\npreds = pd.DataFrame({'Id': indicies, 'Class': np.squeeze(preds)})\n\n# save submission csv\npreds.to_csv('submission.csv', header=['Id', 'Class'], index=False)\nprint(\"Submission generated!\")","73840a3b":"After defining these functions, training and testing our model is straightforward from here on out. Simply call the train() function with the required parameters and let your GPU go to work!","d6311e56":"## Set up ","4af2b9ed":"Loading our model up...","03138238":"This quick example shows the power of transfer learning. With relatively few lines of code we're able to achieve over an 80% accuracy on this dog breeds dataset! And there are still a number of things we could have done, or do from here, to achieve even better performance. This includes things such as:\n- Unfreezing the last few layers of the ResNet base and training some more on our specific dataset (more on this in a bit)\n- Optimizing the hyperparameters of our model (learning rate, etc.)\n- Utilizing an even more powerful pretrained architecture (`ResNet34`, `ResNet50`, etc.)\n- Creating a custom learning rate schedule\n\nWe'll save the model, then load it back up using the function we defined earlier.","710b5a44":"#### Testing a model\n\nCreating a function that generates and prints predictions on a given number of images from our test set:","36156bf5":"In transfer learning, we take the architecture and weights of a pre-trained model (one that has been trained on millions of images belonging to 1000\u2019s of classes, on several high power GPU\u2019s for several days) and use the pre-learned features to solve our own novel problem.\n\nPyTorch actually comes with a number of models which have already been trained on the Imagenet dataset we discussed earlier, making it quite simple for us to apply this method of transfer learning. We'll be using a powerful but lighweight model called `ResNet18`, which we import like so:\n```python\nfrom torchvision.models import resnet18\n```\n\nThe next block of code might look a bit foreign. What we're doing is actually looping through all of the model's pretrained weights and **freezing** them. This means that during training, these weights will not be updating at all. We then take the entire ResNet model and put it into one block of our model, named feature_extraction. It's important to understand that when you load a pretrained model you are only receiving the feature extraction block, or the convolutional layers. It's up to us to define a classification block which can take all of the features the ResNet model extracted and use them to actually classify an image.","58141f3d":"#### Visualizing the dataset","e2899001":"#### PyTorch data transformations","a0be3f93":"In this example, we simply took a pretrained model and added our classification (fully connected layers) block right on top. We froze the entire pretrained network and only updated the weights of our fully connected layers. This means we didn't change the pretrained weights at all, and only used what it had 'learned' from the dataset which it was trained on. \n\nHowever, I mentioned earlier that we could achieve even better performance if we unfroze the last few layers of the pretrained model and trained them some on our specific dataset. But why?\n\n<img src = \"https:\/\/drive.google.com\/uc?id=10ce5aTD47lIsO1eYfZmbs_sbDDUfaZiT\"> <img src = \"https:\/\/drive.google.com\/uc?id=1BfHJXrWwl4oVyPZ2_p602nD9HkF4RoSR\">\n\nGoing back to the layer visualizations we saw earlier, we know the earlier layers of the pretrained network learn to recognize simple lines, patterns, objects, etc. However, as we progress in the network, the layers learn to recognize things more specific to the dataset which it was trained on. In this case, ImageNet, which we described a bit earlier.\n\nIf you remember, ImageNet contains images that are *somewhat* similar to our dog breeds dataset, so much of what the model 'learned' also applied to our dataset. Hence why we were able to achieve a pretty good accuracy without adjusting the pretrained model whatsoever. \n\nOf course, much of what the deeper layers learned from ImageNet did **not** apply to dog images. This is why training the last few layers would be beneficial. It would allow the model to adjust and recognize rich features specific to **only dogs**. Things such as types of dog ears, tails, fur, noses, etc. etc.","5ec4849f":"The pixel array of each image is actually quite large, so it'd be inefficient to load the entire dataset onto your RAM at once. Instead, we use PyTorch DataLoaders to load up batches of images on the fly. Earlier we defined a batch size of 32, so in each iteration the loaders will load 32 images and apply our transformations, before returning them to us.\n\nFor the most part, Neural Networks are trained on **batches** of data so these `DataLoader`s greatly simplify the process of loading and feeding data to our network. The rank 4 tensor returned by the `DataLoader` is of size (32, 224, 224, 3).","9150f217":"## Training a model in PyTorch ","06d7e7a5":"Every PyTorch `Dataset` has an attribute, `.classes`, which is an array containing all of the image classes in the dataset. In our case, breeds of dog in the dataset. ","1695d8b5":"The first step in doing so is to define the transformations that will be applied to our data. These are simply the preprocessing steps that are applied to each image before being fed into our model.\n\nAs you can see above, the pictures are all different dimensions, while most CNNs expect each input to be a consistent size... So we define a fixed size for every image as well as a few other constants which I'll explain in a bit.","a8309b0c":"Our next step is to create a PyTorch `Dataset` for each of our training, validation, and test sets. `torch.utils.data.Dataset` is an abstract class that represents a dataset and has several handy attributes we'll utilize from here on out.\n\nIf you look at the folder of images we downloaded earlier you'll see it's structured something like this:\n```\n\/imageFolder\/Train\/Breed1\/image_1.jpg\n\/imageFolder\/Train\/Breed1\/image_2.jpg\n.\n.\n\/imageFolder\/Train\/Breed_133\/image_3.jpg\n\/imageFolder\/Train\/Breed_133\/image_4.jpg\n\n\/imageFolder\/Validation\/Breed1\/image_5.jpg\n\/imageFolder\/Validation\/Breed1\/image_6.jpg\n.\n.\n\/imageFolder\/Validation\/Breed_133\/image_7.jpg\n\/imageFolder\/Validation\/Breed_133\/image_8.jpg\n\n\/imageFolder\/Test\/Breed1\/image_9.jpg\n\/imageFolder\/Test\/Breed1\/image_10.jpg\n.\n.\n\/imageFolder\/Test\/Breed_133\/image_11.jpg\n\/imageFolder\/Test\/Breed_133\/image_12.jpg\n```\nThis structure with subfolders for each class of image is so popular that PyTorch created this function, ImageFolder, which takes a folder and returns a Dataset class for us. The label for each image is automatically interpretted from the name of the folder it sits in. In the line of code below we use this function to create a dictionary of PyTorch Datasets (Train, Validation, Test), passing in the dictionary of transformations we defined above.","b3bf624a":"#### Introduction to the dataset\n\nThe dataset which we'll be working with is the popular dog breeds dataset, which contains a few thousand pictures of 133 different breeds of dogs. Naturally, our goal will be to create a model which can predict the breed of dog of any given image.","97641e8f":"#### Defining a network in PyTorch","4775dbdf":"As expected, our model is predicting the wrong breed for the majority of test images. Why is this?\n\nIn short, building and training a CNN from scratch is possible, however most problems require significantly more complex models, trained on huge amounts of data. Of course, the computational power and amount of data needed to train these networks accurately are not always available. This is why the idea of **Transfer Learning** has become so popular. It allows everyday people, like me and you, to build accurate and powerful models with limited resources.","b40aed0f":"At this point we're finally ready to train our model! In PyTorch we have to write our own training loops before getting to actually train the model. This can seem daunting at first, so let's break up each stage of the training process. \n\nThe bulk of the function is handled by a nested for loop, the outer looping through each epoch and the inner looping through all of the batches of images in our dataset. Each epoch has a training and validation phase, where batches are served from their respective loaders. Both phases begin by feeding a batch of inputs into the model, which implicity calls the `forward()` function on the input. Then we calculate the loss of the outputs against the true labels of the batch. \n\nIf we're in training mode, here is where we perform back-propagation and adjust our weights. To do this, we first zero the gradients, then perform backpropagation by calling `.backward()` on the loss variable. Finally, we call `optimizer.step()` to adjust the weights of the model in accordance with the calculated gradients.\n\nThe remaining portion of one epoch is the same for both training and validation, and simply involves calculating and tracking the accuracy achieved in both phases. A nifty addition to this training loop is that it tracks the highest validation accuracy and only saves weights which beat that accuracy, ensuring that the best performing weights are returned from the function.","2428c9b0":"Ouch! Our model doesn't seem to be performing very well at all. After 20 epochs of training we're barely able to achieve a 10% accuracy on our validation set... Hang in there, in a bit I'll go into some methods we can use to achieve a much better accuracy.\n\nIn the meantime, let's quickly take a look at how we can save our PyTorch models. Then we'll test and visualize our model. ","165fc792":"Example of a Bulldog <br>\n<img src = \"https:\/\/drive.google.com\/uc?id=12BamjMMri9N3nkiGvS186US7FXKrUnLH\">\n <br>Here's a German Shepard<br>\n<img src= \"https:\/\/drive.google.com\/uc?id=1KuIfY2niIJ-7e-B5gNzz1joCAbkqcpXe\">","f835d03e":"## Transfer Learning  ","3261d437":"This code defines the transformations for each of our datasets (Training, Validation, and Test sets). **Compose()** simply chains together PyTorch transformations. \n\nThe first transformation we apply is the resizing step we discussed above. The next step, `ToTensor()`, transforms the pixel array into a PyTorch `Tensor` and rescales each pixel value to be between $[0, 1]$. This is required for an input to be consumed by PyTorch. Finally, we normalize each `Tensor` to have a mean $\\mu = 0$ and variance $\\sigma^2 = 1$. Research supports that Neural Networks tend to perform much better on normalized data... \n","62f63735":"Once we've set up our PyTorch datasets and dataloaders, grabbing individual images or batches of images is super simple. Below I've defined 2 functions we can use to take a look at the dogs in our dataset.\n\nThe first one here indexes into our training set, grabs a given number of random images, and plots them. A PyTorch `Dataset` is *sort of* a 2D array, where the first dimension represents the images themselves, and the second dimension contains the pixel array and the label of the image.\n\nThe second function allows us to plot a batch of images served up by our PyTorch dataloader.","956ade9c":"<img\n    style=\"border-radius: 0.5em;\"\n    src=\"https:\/\/ucfai.org\/groups\/core\/fa19\/cnns\/banner.png\">\n\n<div class=\"col-12\">\n    <h1> How We Give Our Computers Eyes and Eyes <\/h1>\n    <hr>\n<\/div>\n\n<div style=\"line-height: 2em;\">\n    <p>by: \n        <a href=\"https:\/\/ucfai.org\/authors\/danielzgsilva\">@danielzgsilva<\/a> and\n        <a href=\"https:\/\/ucfai.org\/authors\/brandons209\">@brandons209<\/a> on Oct 16, 2019<\/p>\n<\/div>","ec627ba0":"Creating a function which unpacks the `.pt` file we saved earlier and loads up the model's saved weights and optimizer state:","e401635c":"## Convolutional Neural Networks and Transfer Learning Workshop\nThere is a notebook in the github repo for this workshop that has much of the content from the slides in there for your convenience. ","22c45fc7":"Now that we understand the details behind CNNs, let's take a look at how we can build one of these networks using the **[PyTorch](https:\/\/pytorch.org\/docs\/stable\/index.html)** framework. As I mentioned earlier, CNNs can be used to understand all sorts of data, but for this meeting we'll build a network to classify images. This is called **Computer Vision**.\n\nBefore we can begin building our model, we need to set up our dataset in such a way that allows PyTorch to properly load each image.","f7db2610":"Now its time to finally build our CNN.  In PyTorch, a model is represented by a normal Python class that inherits from the master nn.Module class. Inheriting from this master class grants your model all the methods and attributes needed to train and work with your model. There are, however, 2 things you need to write yourself:\n - `__init__(self)`: Here is where you define the layers and overall architecture of your model\n - `forward(self, x)`: This method takes an input, x, computes a forward pass through the network and outputs predictions. Writing it essentially involves connecting your layers and setting up the flow of the input through your layers.\n \n ","335300f1":"Finally we can test our new pretrained ResNet model! As you can see, with transfer learning we can create quite accurate models relatively easily.","2a02f834":"Below are the signatures of the PyTorch functions that create each of the layers we discussed. Try to use them to build your first CNN! I provided some comments that hopefully guide you in terms of what should happen at each step.\n\n- `nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)`\n- `nn.ReLU(x)`\n- `nn.MaxPool2d(kernel_size, stride, padding)`\n- `nn.BatchNorm2d(num_features)` - num_features is the number of channels it receives\n- `nn.Dropout(p)` - p is probability of an element to be zeroed\n- `nn.Linear(in_features, out_features)` \u2013 fully connected layer (matrix multiplications used in the classification portion of a network)","b4feda4a":"## Saving a model in PyTorch ","54acdb6f":"## Building a Convolutional Neural Network with PyTorch","d139abc5":"There are many ways to save a PyTorch model, however the most robust method is described below. This allows you to load up a model for both testing and further training.\n\nThe most important part to understand from the code below is what the `model` and `optimizer` `.state_dict()`s are. The model state_dict is essentially a dictionary which contains all of the learned weights and biases in the model, while the optimizer contains information about the optimizer\u2019s state hyperparameters used.\n\nOther than the state_dicts, we also save the class used to build the model architecture, as well as the optimizer and loss function. Putting all of this together allows us to save, move around, and later restore our model to it's exact state after training.. A `.pt` file extension is commonly used to bundle all of this together.","69b91bc1":"#### PyTorch datasets and dataloaders","61313c84":"## Thank you for coming out tonight! \n\n## Don't forget to sign in at <a href=\"ucfai.org\/signin\">ucfai.org\/signin<\/a> if you didn't get the chance to swipe in, and see you next week!","33c58ee2":"Now we create an instance of this `CNN()` class and define the loss function and optimizer we'll use to train our model. In our case we'll use `CrossEntropyLoss`. You'll notice we never added a `Softmax` activation after our last layer. That's because PyTorch's `CrossEntropyLoss` applies a softmax before calculating log loss, a commonly used loss function for single label classification problems.\n\nFor the optimizer we'll use `Adam`, an easy to apply but powerful optimizer which is an extension of the popular Stochastic Gradient Descent method. We need to pass it all of the parameters it'll train, which PyTorch makes easy with `model.parameters()`, and also the learning rate we'll use.","5bedacf6":"#### More on Transfer Learning","59a87c38":"Importing some of the libraries we'll be using, as well as PyTorch:","c90ebf88":"Let's test our model on a couple of dogs!"}}