{"cell_type":{"fab01301":"code","6256b3d4":"code","af950e45":"code","fae9b907":"code","92b48203":"code","c22bc248":"code","76e2cb67":"code","a21a460a":"code","7fe60a63":"code","7f9936fa":"code","6e7bf575":"code","ae5d9111":"code","8a8809da":"code","00f583b4":"code","58853e40":"code","10b85cdb":"code","988ec691":"code","80278194":"code","337d9a2e":"code","713ed0ce":"code","2f45be18":"code","8f9a2984":"code","e41fa704":"code","bd62d07a":"markdown","1d0effc9":"markdown","f3697eb8":"markdown","10b4ccbf":"markdown","701ef1f5":"markdown","ff588874":"markdown","7cce47c4":"markdown","cefe4999":"markdown","1f618db7":"markdown","d853c5d4":"markdown","8b4a3903":"markdown","9a33e653":"markdown"},"source":{"fab01301":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, train_test_split\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6256b3d4":"train = pd.read_csv('..\/input\/allstate-claims-severity\/train.csv')\ntest = pd.read_csv('..\/input\/allstate-claims-severity\/test.csv')\nsample = pd.read_csv('..\/input\/allstate-claims-severity\/sample_submission.csv')","af950e45":"print(train.shape, test.shape, sample.shape)","fae9b907":"train.head()","92b48203":"fig, ax = plt.subplots(1,2,figsize=(20,8))\nsns.distplot(train['loss'],kde=False, ax=ax[0])\nsns.distplot(train['loss'],hist=False, ax=ax[1])","c22bc248":"train.dtypes.value_counts()","76e2cb67":"train.isna().any().sum()","a21a460a":"train.describe()","7fe60a63":"train.corr()","7f9936fa":"plt.figure(figsize=(14,10))\nsns.heatmap(train.corr(), annot=True)","6e7bf575":"train_correlations = train.drop([\"loss\"], axis=1).corr()\ntrain_correlations = train_correlations.values.flatten()\ntrain_correlations = train_correlations[train_correlations != 1]\n\ntest_correlations = test.corr()\ntest_correlations = test_correlations.values.flatten()\ntest_correlations = test_correlations[test_correlations != 1]\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_correlations, color=\"Red\", label=\"train\")\nsns.distplot(test_correlations, color=\"Green\", label=\"test\")\nplt.xlabel(\"Correlation values found in train (except 1)\")\nplt.ylabel(\"Density\")\nplt.title(\"Are there correlations between features?\"); \nplt.legend();","ae5d9111":"sns.pairplot(train.sample(frac=0.1), vars=['cont1', 'cont2', 'cont3', 'cont4', 'cont5','cont6', 'cont7'])","8a8809da":"sns.pairplot(train.sample(frac=0.1), x_vars=['cont1', 'cont2', 'cont3', 'cont4', 'cont5','cont6', 'cont7'], y_vars=['cont8', 'cont9', 'cont10', 'cont11', 'cont12',\n       'cont13', 'cont14'])","00f583b4":"sns.pairplot(train.sample(frac=0.1), vars=['cont8', 'cont9', 'cont10', 'cont11', 'cont12',\n       'cont13', 'cont14'])","58853e40":"sns.pairplot(train.sample(frac=0.1), x_vars=['cont8', 'cont9', 'cont10', 'cont11', 'cont12',\n       'cont13', 'cont14'], y_vars=['cont1', 'cont2', 'cont3', 'cont4', 'cont5','cont6', 'cont7'])","10b85cdb":"train = train.drop(['cont1', 'cont11', 'cont10'], axis=1)\ntest = test.drop(['cont1', 'cont11', 'cont10'], axis=1)","988ec691":"fig,axes = plt.subplots(39,3,figsize=(20,180))\nax = axes.flatten()\n\nfor i in range(116):\n    sns.countplot(train[f'cat{i+1}'], ax=ax[i])","80278194":"cat_cols = train.select_dtypes(include='object').columns\n\nle = LabelEncoder()\n\nfor i in cat_cols:\n    test_unique = test[i].unique()\n    train_unique = train[i].unique()\n    labels = list(set(test_unique) | set(train_unique))\n    \n    le.fit(labels)\n    train[i] = le.transform(train[i])\n    test[i] = le.transform(test[i])","337d9a2e":"X = train.drop(['loss'], axis=1)\ny = np.log(train['loss']+1)","713ed0ce":"X_train,X_val,y_train, y_val = train_test_split(X,y,test_size=0.1)\n\nmodel = LGBMRegressor(n_estimators=300, learning_rate=0.1, random_state=123)\nmodel.fit(X_train,y_train)\npreds = model.predict(X_val)\n\nprint(mean_absolute_error(preds,y_val))","2f45be18":"feature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,X.columns)), columns=['Value','Feature'])\n\nfig,ax = plt.subplots(1,1,figsize=(20,30))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False), ax=ax)\nplt.title('LightGBM Features')","8f9a2984":"test_predictions = model.predict(test)\nsample['loss'] = np.expm1(test_predictions)\nsample.to_csv('submission.csv', index=False)","e41fa704":"plt.figure(figsize=(8,6))\nsns.distplot(sample['loss'])","bd62d07a":"Now, the categorical features.","1d0effc9":"Insights:\n1. 'B' value is rare or less than 'A' in all features.\n2. We can bin the features with more than 10 values, but for now let's go with label encoding.","f3697eb8":"Some insights,\n1. For all numerical features, mean value is approx. 0.5 and  standard deviation is approx. 0.2\n2. Median values varies from 0.45 to 0.55\n\nLet's plot some numerical features first.","10b4ccbf":"## EDA and Preprocessing","701ef1f5":"There are some features with correlation values between 0.5 and 1. We need to remove one feature from such highly correlated feature pairs.","ff588874":"## References:\n\n1. https:\/\/www.kaggle.com\/sharmasanthosh\/exploratory-study-on-ml-algorithms\nLearned a lot from this amazing kernel.","7cce47c4":"1. cont1 and cont10,cont9 and cont6, cont10 and cont6 are highly correlated.\n2. No feature is highly correlated with the 'loss' column.","cefe4999":"## Baseline Submission ","1f618db7":"There are no missing values in the dataset.","d853c5d4":"We have 116 categorical features and 14 numerical features. ","8b4a3903":"Highly correlated features:\n1. cont1 and cont9\n2. cont11 and cont12\n3. cont10 and cont6\n\nLet's remove one from these pairs.","9a33e653":"'loss' is our target variable. This is a regression problem."}}