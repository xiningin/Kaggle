{"cell_type":{"0cf110f5":"code","33b6e121":"code","c9550915":"code","82e1effb":"code","052e2380":"code","5ac4f1fe":"code","aa8a5af9":"code","3eb20a74":"code","6a44022e":"code","3f3e24d2":"code","cc791e55":"code","7ae47dea":"code","8f329f64":"code","ab1860c0":"code","e3e59743":"code","6224ba0a":"code","5773b018":"code","d906cbcc":"code","f8a638cb":"code","daf6c8e7":"code","2166389b":"code","815f6fb1":"code","621d506f":"code","ce52ba43":"code","cfcf862d":"code","e757ceab":"code","e786a759":"code","79ed454a":"code","80d55b94":"code","70a8f043":"code","e5a3d591":"code","6078c29a":"code","dade74ba":"code","e7cffbfe":"code","9d84fc6e":"code","d8092224":"code","9acad5b6":"code","24a25910":"code","dad52431":"code","8007545a":"code","90565e91":"code","ac114958":"code","ba6bb2ef":"code","0c12b56a":"code","0e5cca4d":"code","c9891a28":"code","48b07abf":"code","0981d782":"code","6a064fc9":"code","f08e1431":"code","1599d6a5":"code","fbfd2c12":"code","15a2e12d":"code","093afc78":"code","7ccf540d":"code","6f7e2e70":"code","c57ba5ff":"code","90ae92ab":"code","cf8988fa":"code","f5a12323":"code","c579ae3f":"code","6d5bd07a":"code","182a61d9":"code","ae2ea428":"code","31bd232b":"code","5f512057":"code","209607f9":"code","473017d7":"code","f12e3a3d":"code","8ec9a86a":"code","181fe3cf":"markdown","0baf7ebf":"markdown","23cd5eed":"markdown","ad59de48":"markdown","0367300d":"markdown","d4872d09":"markdown","768bacff":"markdown","75a27106":"markdown","c4a84756":"markdown","41c00b3d":"markdown","ac03d641":"markdown","696ac1a0":"markdown","450978e8":"markdown","f4bc83f3":"markdown","d3d5a2e6":"markdown","adc28a5f":"markdown"},"source":{"0cf110f5":"# Importing some important librarys\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","33b6e121":"df = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')","c9550915":"df.shape","82e1effb":"df.head()\n\n# We got some categorical data, and it's a binary classification (Yes, NO)","052e2380":"df.info()\n\n# We have missing data , we will handle them as we go","5ac4f1fe":"# Describe the numerical data\n\ndf.describe()","aa8a5af9":"# we will change the type of Credit_History to object becaues we can see that it is 1 or 0\n\ndf['Credit_History'] = df['Credit_History'].astype('O')","3eb20a74":"# describe categorical data (\"objec\")\n\ndf.describe(include='O')","6a44022e":"# we will drop ID because it's not important for our model and it will just mislead the model\n\ndf.drop('Loan_ID', axis=1, inplace=True)","3f3e24d2":"df.duplicated().any()\n\n# we got no duplicated rows","cc791e55":"# let's look at the target percentage\n\nplt.figure(figsize=(8,6))\nsns.countplot(df['Loan_Status']);\n\nprint('The percentage of Y class : %.2f' % (df['Loan_Status'].value_counts()[0] \/ len(df)))\nprint('The percentage of N class : %.2f' % (df['Loan_Status'].value_counts()[1] \/ len(df)))\n\n# We can consider it as imbalanced data, but for now i will not","7ae47dea":"df.columns","8f329f64":"df.head(1)","ab1860c0":"# Credit_History\n\ngrid = sns.FacetGrid(df,col='Loan_Status', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'Credit_History');\n\n# we didn't give a loan for most people who got Credit History = 0\n# but we did give a loan for most of people who got Credit History = 1\n# so we can say if you got Credit History = 1 , you will have better chance to get a loan\n\n# important feature","e3e59743":"# Gender\n\ngrid = sns.FacetGrid(df,col='Loan_Status', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'Gender');\n\n# most males got loan and most females got one too so (No pattern)\n\n# i think it's not so important feature, we will see later","6224ba0a":"# Married\nplt.figure(figsize=(15,5))\nsns.countplot(x='Married', hue='Loan_Status', data=df);\n\n# most people who get married did get a loan\n# if you'r married then you have better chance to get a loan :)\n# good feature","5773b018":"# Dependents\n\nplt.figure(figsize=(15,5))\nsns.countplot(x='Dependents', hue='Loan_Status', data=df);\n\n# first if Dependents = 0 , we got higher chance to get a loan ((very hight chance))\n# good feature","d906cbcc":"# Education\n\ngrid = sns.FacetGrid(df,col='Loan_Status', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'Education');\n\n# If you are graduated or not, you will get almost the same chance to get a loan (No pattern)\n# Here you can see that most people did graduated, and most of them got a loan\n# on the other hand, most of people who did't graduate also got a loan, but with less percentage from people who graduated\n\n# not important feature","f8a638cb":"# Self_Employed\n\ngrid = sns.FacetGrid(df,col='Loan_Status', size=3.2, aspect=1.6)\ngrid.map(sns.countplot, 'Self_Employed');\n\n# No pattern (same as Education)","daf6c8e7":"# Property_Area\n\nplt.figure(figsize=(15,5))\nsns.countplot(x='Property_Area', hue='Loan_Status', data=df);\n\n# We can say, Semiurban Property_Area got more than 50% chance to get a loan\n\n# good feature","2166389b":"# ApplicantIncome\n\nplt.scatter(df['ApplicantIncome'], df['Loan_Status']);\n\n# No pattern","815f6fb1":"# the numerical data\n\ndf.groupby('Loan_Status').median() # median because Not affected with outliers\n\n# we can see that when we got low median in CoapplicantInocme we got Loan_Status = N\n\n# CoapplicantInocme is a good feature","621d506f":"df.isnull().sum().sort_values(ascending=False)","ce52ba43":"# We will separate the numerical columns from the categorical\n\ncat_data = []\nnum_data = []\n\nfor i,c in enumerate(df.dtypes):\n    if c == object:\n        cat_data.append(df.iloc[:, i])\n    else :\n        num_data.append(df.iloc[:, i])","cfcf862d":"cat_data = pd.DataFrame(cat_data).transpose()\nnum_data = pd.DataFrame(num_data).transpose()","e757ceab":"cat_data.head()","e786a759":"num_data.head()","79ed454a":"# cat_data\n# If you want to fill every column with its own most frequent value you can use\n\ncat_data = cat_data.apply(lambda x:x.fillna(x.value_counts().index[0]))\ncat_data.isnull().sum().any() # no more missing data ","80d55b94":"# num_data\n# fill every missing value with their previous value in the same column\n\nnum_data.fillna(method='bfill', inplace=True)\nnum_data.isnull().sum().any() # no more missing data ","70a8f043":"from sklearn.preprocessing import LabelEncoder  \nle = LabelEncoder()\ncat_data.head()","e5a3d591":"# transform the target column\n\ntarget_values = {'Y': 0 , 'N' : 1}\n\ntarget = cat_data['Loan_Status']\ncat_data.drop('Loan_Status', axis=1, inplace=True)\n\ntarget = target.map(target_values)","6078c29a":"# transform other columns\n\nfor i in cat_data:\n    cat_data[i] = le.fit_transform(cat_data[i])","dade74ba":"target.head()","e7cffbfe":"cat_data.head()","9d84fc6e":"df = pd.concat([cat_data, num_data, target], axis=1)","d8092224":"df.head()","9acad5b6":"X = pd.concat([cat_data, num_data], axis=1)\ny = target ","24a25910":"# we will use StratifiedShuffleSplit to split the data Taking into consideration that we will get the same ratio on the target column\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train, test in sss.split(X, y):\n    X_train, X_test = X.iloc[train], X.iloc[test]\n    y_train, y_test = y.iloc[train], y.iloc[test]\n    \nprint('X_train shape', X_train.shape)\nprint('y_train shape', y_train.shape)\nprint('X_test shape', X_test.shape)\nprint('y_test shape', y_test.shape)\n\n# almost same ratio\nprint('\\nratio of target in y_train :',y_train.value_counts().values\/ len(y_train))\nprint('ratio of target in y_test :',y_test.value_counts().values\/ len(y_test))\nprint('ratio of target in original_data :',df['Loan_Status'].value_counts().values\/ len(df))","dad52431":"# we will use 4 different models for training\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodels = {\n    'LogisticRegression': LogisticRegression(random_state=42),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'SVC': SVC(random_state=42),\n    'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=1, random_state=42)\n}","8007545a":"# loss\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score\n\ndef loss(y_true, y_pred, retu=False):\n    pre = precision_score(y_true, y_pred)\n    rec = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    loss = log_loss(y_true, y_pred)\n    acc = accuracy_score(y_true, y_pred)\n    \n    if retu:\n        return pre, rec, f1, loss, acc\n    else:\n        print('  pre: %.3f\\n  rec: %.3f\\n  f1: %.3f\\n  loss: %.3f\\n  acc: %.3f' % (pre, rec, f1, loss, acc))","90565e91":"# train_eval_train\n\ndef train_eval_train(models, X, y):\n    for name, model in models.items():\n        print(name,':')\n        model.fit(X, y)\n        loss(y, model.predict(X))\n        print('-'*30)\n        \ntrain_eval_train(models, X_train, y_train)\n\n# we can see that best model is LogisticRegression at least for now, SVC is just memorizing the data so it is overfitting .","ac114958":"X_train.shape","ba6bb2ef":"# train_eval_cross\n# in the next cell i will be explaining this function\n\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n\ndef train_eval_cross(models, X, y, folds):\n    # we will change X & y to dataframe because we will use iloc (iloc don't work on numpy array)\n    X = pd.DataFrame(X) \n    y = pd.DataFrame(y)\n    idx = [' pre', ' rec', ' f1', ' loss', ' acc']\n    for name, model in models.items():\n        ls = []\n        print(name,':')\n\n        for train, test in folds.split(X, y):\n            model.fit(X.iloc[train], y.iloc[train]) \n            y_pred = model.predict(X.iloc[test]) \n            ls.append(loss(y.iloc[test], y_pred, retu=True))\n        print(pd.DataFrame(np.array(ls).mean(axis=0), index=idx)[0])  #[0] because we don't want to show the name of the column\n        print('-'*30)\n        \ntrain_eval_cross(models, X_train, y_train, skf)\n\n# ohhh, as i said SVC is just memorizing the data, and you can see that here DecisionTreeClassifier is better than LogisticRegression ","0c12b56a":"# some explanation of the above function\n\nx = []\nidx = [' pre', ' rec', ' f1', ' loss', ' acc']\n\n# we will use one model\nlog = LogisticRegression()\n\nfor train, test in skf.split(X_train, y_train):\n    log.fit(X_train.iloc[train], y_train.iloc[train])\n    ls = loss(y_train.iloc[test], log.predict(X_train.iloc[test]), retu=True)\n    x.append(ls)\n    \n# thats what we get\npd.DataFrame(x, columns=idx)\n\n# (column 0 represent the precision_score of the 10 folds)\n# (row 0 represent the (pre, rec, f1, loss, acc) for the first fold)\n# then we should find the mean of every column\n# pd.DataFrame(x, columns=idx).mean(axis=0)","0e5cca4d":"# ooh, we got it right for most of the features, as you can see we've say at the first of the kernel ,\n# that Credit_Histroy and Married etc, are good features, actually Credit_Histroy is the best .\n\ndata_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);\n\n# here we got 58% similarity between LoanAmount & ApplicantIncome \n# and that may be bad for our model so we will see what we can do","c9891a28":"X_train.head()","48b07abf":"# I will try to make some operations on some features, here I just tried diffrent operations on diffrent features,\n# having experience in the field, and having knowledge about the data will also help\n\nX_train['new_col'] = X_train['CoapplicantIncome'] \/ X_train['ApplicantIncome']  \nX_train['new_col_2'] = X_train['LoanAmount'] * X_train['Loan_Amount_Term'] ","0981d782":"data_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);\n\n# new_col 0.03 , new_col_2, 0.047\n# not that much , but that will help us reduce the number of features","6a064fc9":"X_train.drop(['CoapplicantIncome', 'ApplicantIncome', 'Loan_Amount_Term', 'LoanAmount'], axis=1, inplace=True)","f08e1431":"train_eval_cross(models, X_train, y_train, skf)\n\n# ok, SVC is improving, but LogisticRegression is overfitting\n# i wan't change nothing so we can see what will happen as we go","1599d6a5":"# first lets take a look at the value counts of every label\n\nfor i in range(X_train.shape[1]):\n    print(X_train.iloc[:,i].value_counts(), end='\\n------------------------------------------------\\n')","fbfd2c12":"# new_col_2\n\n# we can see we got right_skewed\n# we can solve this problem with very simple statistical teqniq , by taking the logarithm of all the values\n# because when data is normally distributed that will help improving our model\n\nfrom scipy.stats import norm\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\n\nsns.distplot(X_train['new_col_2'], ax=ax[0], fit=norm)\nax[0].set_title('new_col_2 before log')\n\nX_train['new_col_2'] = np.log(X_train['new_col_2'])  # logarithm of all the values\n\nsns.distplot(X_train['new_col_2'], ax=ax[1], fit=norm)\nax[1].set_title('new_col_2 after log');","15a2e12d":"# now we will evaluate our models, and i will do that continuously ,so i don't need to mention that every time\n\ntrain_eval_cross(models, X_train, y_train, skf)\n\n# wooow our models improved really good by just doing the previous step .","093afc78":"# new_col\n\n# most of our data is 0 , so we will try to change other values to 1\n\nprint('before:')\nprint(X_train['new_col'].value_counts())\n\nX_train['new_col'] = [x if x==0 else 1 for x in X_train['new_col']]\nprint('-'*50)\nprint('\\nafter:')\nprint(X_train['new_col'].value_counts())","7ccf540d":"train_eval_cross(models, X_train, y_train, skf)\n\n# ok we are improving our models as we go ","6f7e2e70":"for i in range(X_train.shape[1]):\n    print(X_train.iloc[:,i].value_counts(), end='\\n------------------------------------------------\\n')\n    \n# looks better","c57ba5ff":"# we will use boxplot to detect outliers\n\nsns.boxplot(X_train['new_col_2']);\nplt.title('new_col_2 outliers', fontsize=15);\nplt.xlabel('');","90ae92ab":"threshold = 1.5  # this number is hyper parameter , as much as you reduce it, as much as you remove more points\n                 # you can just try different values the deafult value is (1.5) it works good for most cases\n                 # but be careful, you don't want to try a small number because you may loss some important information from the data .\n                 \n            \nnew_col_2_out = X_train['new_col_2']\nq25, q75 = np.percentile(new_col_2_out, 25), np.percentile(new_col_2_out, 75) # Q25, Q75\nprint('Quartile 25: {} , Quartile 75: {}'.format(q25, q75))\n\niqr = q75 - q25\nprint('iqr: {}'.format(iqr))\n\ncut = iqr * threshold\nlower, upper = q25 - cut, q75 + cut\nprint('Cut Off: {}'.format(cut))\nprint('Lower: {}'.format(lower))\nprint('Upper: {}'.format(upper))\n\noutliers = [x for x in new_col_2_out if x < lower or x > upper]\nprint('Nubers of Outliers: {}'.format(len(outliers)))\nprint('outliers:{}'.format(outliers))\n\ndata_outliers = pd.concat([X_train, y_train], axis=1)\nprint('\\nlen X_train before dropping the outliers', len(data_outliers))\ndata_outliers = data_outliers.drop(data_outliers[(data_outliers['new_col_2'] > upper) | (data_outliers['new_col_2'] < lower)].index)\n\nprint('len X_train before dropping the outliers', len(data_outliers))","cf8988fa":"X_train = data_outliers.drop('Loan_Status', axis=1)\ny_train = data_outliers['Loan_Status']","f5a12323":"sns.boxplot(X_train['new_col_2']);\nplt.title('new_col_2 without outliers', fontsize=15);\nplt.xlabel('');\n\n# good :)","c579ae3f":"train_eval_cross(models, X_train, y_train, skf)","6d5bd07a":"# Self_Employed got really bad corr (-0.00061) , let's try remove it and see what will happen\n\ndata_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);","182a61d9":"#X_train.drop(['Self_Employed'], axis=1, inplace=True)\n\ntrain_eval_cross(models, X_train, y_train, skf)\n\n# looks like Self_Employed is not important\n# KNeighborsClassifier improved\n\n# droping all the features Except for Credit_History actually improved KNeighborsClassifier and didn't change anything in other models\n# so you can try it by you self\n# but don't forget to do that on testing data too\n\n#X_train.drop(['Self_Employed','Dependents', 'new_col_2', 'Education', 'Gender', 'Property_Area','Married', 'new_col'], axis=1, inplace=True)","ae2ea428":"data_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);","31bd232b":"X_test.head()","5f512057":"X_test_new = X_test.copy()","209607f9":"x = []\n\nX_test_new['new_col'] = X_test_new['CoapplicantIncome'] \/ X_test_new['ApplicantIncome']  \nX_test_new['new_col_2'] = X_test_new['LoanAmount'] * X_test_new['Loan_Amount_Term']\nX_test_new.drop(['CoapplicantIncome', 'ApplicantIncome', 'Loan_Amount_Term', 'LoanAmount'], axis=1, inplace=True)\n\nX_test_new['new_col_2'] = np.log(X_test_new['new_col_2'])\n\nX_test_new['new_col'] = [x if x==0 else 1 for x in X_test_new['new_col']]\n\n#X_test_new.drop(['Self_Employed'], axis=1, inplace=True)\n\n# drop all the features Except for Credit_History\n#X_test_new.drop(['Self_Employed','Dependents', 'new_col_2', 'Education', 'Gender', 'Property_Area','Married', 'new_col'], axis=1, inplace=True)","473017d7":"X_test_new.head()","f12e3a3d":"X_train.head()","8ec9a86a":"for name,model in models.items():\n    print(name, end=':\\n')\n    loss(y_test, model.predict(X_test_new))\n    print('-'*40)","181fe3cf":"# evaluate the models on Test_data\n\nhere we will just repeat what we did in training data","0baf7ebf":"### first we will go through the categorical features\n","23cd5eed":" # Classification Loan Status\n\n### Before we start:\n\nIf you like my work, please upvote this kernel as it will keep me motivated to do more in the future and share the kernel with others so we can all benefit from it .\n\n### Introduction:\nIn this kernel ,I will try to show you how different models can improve just by doing simple process on the data .\n\nwe are going to work on **binary classification problem**,\nwhere we got some information about sample of peoples , and we need to predict whether we should give some one a loan or not depending on his information .\nwe actually have a few sample size (614 rows), so we will go with machine learning techniques to solve our problem .\n\n### what you will learn in this kernel ?\n\n* basics of visualizing the data .\n* how to compare between **feature importance** (at less in this data) .\n\n   * **feature selection**\n   \n   * **feature engineer**\n   \n   \n* some simple techniques to **process** the data .\n* handling **missing data** .\n* how to deal with **categorical** and **numerical** data .\n* **outliers** data detection\n* but the most important thing that you will learn , is how to **evaluate your model** at every step you take .\n    \n### what we will use ?\n\n* some important libraries like **sklearn, matplotlib, numpy, pandas, seaborn, scipy**\n\n\n* fill the values using **backward 'bfill' method** for numerical columns , and **most frequent value** for categorical columns (simple techniques)\n\n\n* 4 different models to train your data, so we can compare between them \n\n    **a) logistic regression**\n    \n    **b) KNeighborsClassifier**\n    \n    **C) SVC**\n    \n    **d) DecisionTreeClassifier**\n    \n    \n**Note** : I am writing this kernel while i am still studying and learning about this field , so if there is any mistake i have made, please feel free to tell me in the comment below, and you can ask me any question about this kernel . \n\n So let's start\n","ad59de48":"# let's look deeper in the data","0367300d":"# Train the data\n\n* we will stop here for know and train the data.\n\n    we are going to use **StratifiedShuffleSplit**, for more [information](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedShuffleSplit.html) .","d4872d09":"# features selection","768bacff":"# simple look on the data","75a27106":"# Let's start to improve our model","c4a84756":"# features engineer","41c00b3d":"### categorical columns\n\n* we are going to use **LabelEncoder** :\n\n    what it is actually do it encode labels with value between 0 and n_classes-1 , [for more examples](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html) .","ac03d641":"### Missing values\n\nhere i am just going to use a simple techniques to handle the missing data","696ac1a0":"### we will work on the features that have varied values","450978e8":"**Conclusion:**\n\nwhat ever we do, our **recall score** will not improving , maybe because we don't have a good amount of data, so I think if we got **more data** and we try more **complex models** our accuracy will improve,I am not sure about this, so please if I made any mistakes in this kernel , or if you have any suggestions which can improve the accuracy please feel free to share it with us in the comments .\n\nThanks :)","f4bc83f3":"# Outliers\n\n#### there is different techniques to handle outliers, here we are going to use [**IQR**](https:\/\/www.youtube.com\/watch?v=qLYYHWYr8xI)","d3d5a2e6":"# build functions\n\n### we are going to build 3 functions :\n1) **loss** : to evaluate our models\n* [precision](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.precision_score.html)\n* [recall](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.recall_score.html)\n* [f1](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html)\n* [log_loss](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.log_loss.html)\n* [accuracy_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html)\n\n2) **train_eval_train** : to evaluate our models in the same data that we train it on .\n\n3) **train_eval_cross** : to evaluate our models using different data that we train the model on .\n* [StratifiedKFold](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html)\n\n### so you may ask why we don't just train our model and evaluate it without building this functions ?\n\nactually you can do that,but mostly your model will not work good at beginning, so you need to change something about your data to improve your accuracy ,\nby changing i mean **data processing**, and every step you will make, you should **evaluate your model** to see if it is improving or not, so to not do this step every time, this functions will make life easy as you go :)\n","adc28a5f":"# Simple process for the data"}}