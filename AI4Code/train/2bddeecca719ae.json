{"cell_type":{"2fed358f":"code","56fd8678":"code","040eea44":"code","8645f874":"code","59425ea1":"code","b3d76ff3":"code","0ccd4e17":"code","5f7a4432":"code","cfa81321":"code","ff6150cf":"code","40ffa35a":"code","bf2a02ad":"code","bb1720d3":"code","2cf80fc0":"code","cad3166c":"code","6f1582b8":"code","29f96712":"code","6ed680f0":"code","52639f00":"code","c129304d":"code","c3503524":"code","b4200ab7":"code","4a3b9ec6":"code","87ab316d":"code","69e3de2a":"code","106f5039":"code","c87d17e9":"code","5a8ef114":"code","e8cbabb2":"code","233635a7":"code","983fd967":"code","62c7d5f1":"code","a39869f6":"code","6d10e5cc":"code","7ad3a368":"code","17242113":"code","eeb740e0":"markdown","2ae8508a":"markdown","04151c01":"markdown","f06749f5":"markdown","7941c53c":"markdown","5b2cf2fc":"markdown","8e6e3d62":"markdown","5a4cf159":"markdown","572e2946":"markdown","14b16b7f":"markdown","aac24c0b":"markdown","270fc253":"markdown","dcf625bf":"markdown","7a182f0e":"markdown","fe649c95":"markdown","2f21d699":"markdown","284b6d12":"markdown","6587227b":"markdown","e1cdcd79":"markdown","3e84f993":"markdown","4fbdaa7b":"markdown","7a013eed":"markdown","f77e5339":"markdown","08b72f26":"markdown","f2d5993c":"markdown","51977252":"markdown","5b677ae3":"markdown","20b9291b":"markdown","ead25c6e":"markdown","3079eff1":"markdown","62d4d314":"markdown","162f9fe1":"markdown","89dd8f41":"markdown"},"source":{"2fed358f":"from IPython.display import HTML\nHTML('<center><iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/bn8rVBuIcFg?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>')","56fd8678":"!pip install dabl","040eea44":"import gc\nimport time\nimport dabl\nimport math\nimport datetime\nfrom itertools import cycle\nfrom math import log, floor\nfrom sklearn.neighbors import KDTree\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.utils import shuffle\nfrom tqdm.notebook import tqdm as tqdm\n\nimport missingno as msno\nimport seaborn as sns\nfrom matplotlib import colors\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport pywt\nfrom statsmodels.robust import mad\nimport plotly.offline as py\n\n\nimport scipy\nimport statsmodels\nimport pyflux as pf\nfrom scipy import signal\nimport statsmodels.api as sm\nfrom fbprophet import Prophet\nfrom scipy.signal import butter, deconvolve\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \n        \ncolor_ = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\npd.options.display.max_columns = 999","8645f874":"sales_train = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ncalendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nsell_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')","59425ea1":"ids = sorted(list(set(sales_train['id'])))\nd_cols = [c for c in sales_train.columns if 'd_' in c]\nx_1 = sales_train.loc[sales_train['id'] == ids[2]].set_index('id')[d_cols].values[0]\nx_2 = sales_train.loc[sales_train['id'] == ids[1]].set_index('id')[d_cols].values[0]\nx_3 = sales_train.loc[sales_train['id'] == ids[17]].set_index('id')[d_cols].values[0]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,\n                    mode='lines', name=\"First sample\",\n                         marker=dict(color=\"mediumseagreen\")),\n             row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,\n                    mode='lines', name=\"Second sample\",\n                         marker=dict(color=\"violet\")),\n             row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,\n                    mode='lines', name=\"Third sample\",\n                         marker=dict(color=\"dodgerblue\")),\n             row=3, col=1)\n\nfig.update_layout(height=1200, width=800, title_text=\"Sample sales\")\nfig.show()","b3d76ff3":"n_samples = 800000\n\n# Sample duration is 20 miliseconds\nsample_duration = 0.02\n\n# Sample rate is the number of samples in one second\n# Sample rate will be 40mhz\nsample_rate = n_samples * (1 \/ sample_duration)\n\ndef maddest(d, axis=None):\n    \"\"\"\n    Mean Absolute Deviation\n    \"\"\"\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef high_pass_filter(x, low_cutoff=1000, sample_rate=sample_rate):\n    \"\"\"\n    From @randxie https:\/\/github.com\/randxie\/Kaggle-VSB-Baseline\/blob\/master\/src\/utils\/util_signal.py\n    Modified to work with scipy version 1.1.0 which does not have the fs parameter\n    \"\"\"\n    \n    # nyquist frequency is half the sample rate https:\/\/en.wikipedia.org\/wiki\/Nyquist_frequency\n    nyquist = 0.5 * sample_rate\n    norm_low_cutoff = low_cutoff \/ nyquist\n    \n    # Fault pattern usually exists in high frequency band. According to literature, the pattern is visible above 10^4 Hz.\n    # scipy version 1.2.0\n    #sos = butter(10, low_freq, btype='hp', fs=sample_fs, output='sos')\n    \n    # scipy version 1.1.0\n    sos = butter(10, Wn=[norm_low_cutoff], btype='highpass', output='sos')\n    filtered_sig = signal.sosfilt(sos, x)\n\n    return filtered_sig\n\ndef denoise_signals( x, wavelet='db4', level=1):\n    \"\"\"\n    1. Adapted from waveletSmooth function found here:\n    http:\/\/connor-johnson.com\/2016\/01\/24\/using-pywavelets-to-remove-high-frequency-noise\/\n    2. Threshold equation and using hard mode in threshold as mentioned\n    in section '3.2 denoising based on optimized singular values' from paper by Tomas Vantuch:\n    http:\/\/dspace.vsb.cz\/bitstream\/handle\/10084\/133114\/VAN431_FEI_P1807_1801V001_2018.pdf\n    \"\"\"\n    \n    # Decompose to get the wavelet coefficients\n    coeff = pywt.wavedec( x, wavelet, mode=\"per\" )\n    \n    # Calculate sigma for threshold as defined in http:\/\/dspace.vsb.cz\/bitstream\/handle\/10084\/133114\/VAN431_FEI_P1807_1801V001_2018.pdf\n    # As noted by @harshit92 MAD referred to in the paper is Mean Absolute Deviation not Median Absolute Deviation\n    sigma = (1\/0.6745) * maddest( coeff[-level] )\n\n    # Calculte the univeral threshold\n    uthresh = sigma * np.sqrt( 2*np.log( len( x ) ) )\n    coeff[1:] = ( pywt.threshold( i, value=uthresh, mode='hard' ) for i in coeff[1:] )\n    \n    # Reconstruct the signal using the thresholded coefficients\n    return pywt.waverec( coeff, wavelet, mode='per' )","0ccd4e17":"ids = sorted(list(set(sales_train['id'])))\nd_cols = [c for c in sales_train.columns if 'd_' in c]\nx_1 = sales_train.loc[sales_train['id'] == ids[0]].set_index('id')[d_cols].values[0][:90]\nx_2 = sales_train.loc[sales_train['id'] == ids[4]].set_index('id')[d_cols].values[0][1300:1400]\nx_3 = sales_train.loc[sales_train['id'] == ids[65]].set_index('id')[d_cols].values[0][350:450]\n\nx_h = high_pass_filter(x_1, low_cutoff=10000, sample_rate=sample_rate)\nx_hp = high_pass_filter(x_2, low_cutoff=10000, sample_rate=sample_rate)\nx_hpp = high_pass_filter(x_3, low_cutoff=10000, sample_rate=sample_rate)\n\n\n\ny_w1 = denoise_signals(x_h)\ny_w2 = denoise_signals(x_hp)\ny_w3 = denoise_signals(x_hpp)\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_1)), mode='lines+markers', y=x_1, marker=dict(color=\"mediumaquamarine\"), showlegend=False,\n               name=\"Original signal\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_1)), y=y_w1, mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_2)), mode='lines+markers', y=x_2, marker=dict(color=\"thistle\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_2)), y=y_w2, mode='lines', marker=dict(color=\"purple\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_3)), mode='lines+markers', y=x_3, marker=dict(color=\"lightskyblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_3)), y=y_w3, mode='lines', marker=dict(color=\"navy\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Original (pale) vs. Denoised (dark) sales\")\nfig.show()","5f7a4432":"from dabl import plot\n\n\nplot(sales_train, 'dept_id')\nplt.show()","cfa81321":"d_cols = [c for c in sales_train.columns if 'd_' in c] \n\nsales_train.loc[sales_train['id'] == 'FOODS_3_090_CA_3_validation'] \\\n    .set_index('id')[d_cols] \\\n    .T \\\n    .plot(figsize=(15, 5),\n          title='FOODS_3_090_CA_3 sales by \"d\" number',\n          color=next(color_))\nplt.legend('')\nplt.show()","ff6150cf":"one_of_the_store = sales_train.loc[sales_train['id'] == 'FOODS_3_090_CA_3_validation'][d_cols].T\none_of_the_store = one_of_the_store.rename(columns={8412:'FOODS_3_090_CA_3'}) \none_of_the_store = one_of_the_store.reset_index().rename(columns={'index': 'd'}) \none_of_the_store = one_of_the_store.merge(calendar, how='left', validate='1:1')\none_of_the_store.set_index('date')['FOODS_3_090_CA_3'] \\\n    .plot(figsize=(15, 5),\n          color=next(color_),\n          title='FOODS_3_090_CA_3 sales by actual sale dates')\nplt.show()","40ffa35a":"thirty_examples = sales_train.sample(30, random_state=529) \\\n        .set_index('id')[d_cols] \\\n    .T \\\n    .merge(calendar.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')","bf2a02ad":"fig, axs = plt.subplots(10, 2, figsize=(20, 20))\nax = axs.flatten()\nax_idx = 0\nfor item in thirty_examples.columns:\n    thirty_examples[item].plot(title=item,color=next(color_),ax=ax[ax_idx])\n    ax_idx += 1\nplt.tight_layout()\nplt.show()","bb1720d3":"import datetime\n\ndef scatter_plot(cnt_srs, color):\n    trace = go.Scatter(\n        x=cnt_srs.index[::-1],\n        y=cnt_srs.values[::-1],\n        showlegend=False,\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\ncnt_srs = one_of_the_store.groupby('year')['FOODS_3_090_CA_3'].size()\n\n\ntrace = scatter_plot(cnt_srs, 'red')\n\nlayout = go.Layout(\n    height=400,\n    width=800,\n    paper_bgcolor='rgb(233,233,233)',\n    title='years in calendar set for FOODS_3_090_CA_3 id store'\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"ActivationDate\")","2cf80fc0":"past_sales = sales_train.set_index('id')[d_cols] \\\n    .T \\\n    .merge(calendar.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\n\nstore_list = sales_train['store_id'].unique()\nmeans = []\nfig = go.Figure()\nfor s in store_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n    means.append(np.mean(past_sales[store_items].sum(axis=1)))\n    fig.add_trace(go.Scatter(x=np.arange(len(data)), y=data, name=s))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (per store)\")","cad3166c":"df = pd.DataFrame(np.transpose([means, store_list]))\ndf.columns = [\"Mean sales\", \"Store name\"]\npx.bar(df, y=\"Mean sales\", x=\"Store name\", color=\"Store name\", title=\"Mean sales vs. Store name\")","6f1582b8":"store_list = sell_prices['store_id'].unique()\nfig = go.Figure()\nmeans = []\nstores = []\nfor i, s in enumerate(store_list):\n    if \"ca\" in s or \"CA\" in s:\n        store_items = [c for c in past_sales.columns if s in c]\n        data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n        means.append(np.mean(past_sales[store_items].sum(axis=1)))\n        stores.append(s)\n        fig.add_trace(go.Scatter(x=np.arange(len(data)), y=data, name=s))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (California)\")","29f96712":"past_sales = sales_train.set_index('id')[d_cols] \\\n    .T \\\n    .merge(calendar.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\n\n\nstore_list = sell_prices['store_id'].unique()\nfig = go.Figure()\nmeans = []\nstores = []\nfor i, s in enumerate(store_list):\n    if \"wi\" in s or \"WI\" in s:\n        store_items = [c for c in past_sales.columns if s in c]\n        data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n        means.append(np.mean(past_sales[store_items].sum(axis=1)))\n        stores.append(s)\n        fig.add_trace(go.Scatter(x=np.arange(len(data)), y=data, name=s))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (Wisconsin)\")","6ed680f0":"store_list = sell_prices['store_id'].unique()\nfig = go.Figure()\nmeans = []\nstores = []\nfor i, s in enumerate(store_list):\n    if \"tx\" in s or \"TX\" in s:\n        store_items = [c for c in past_sales.columns if s in c]\n        data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n        means.append(np.mean(past_sales[store_items].sum(axis=1)))\n        stores.append(s)\n        fig.add_trace(go.Scatter(x=np.arange(len(data)), y=data, name=s))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (Texas)\")","52639f00":"past_sales = sales_train.set_index('id')[d_cols] \\\n    .T \\\n    .merge(calendar.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\n\n\n\nstore_list = sales_train['dept_id'].unique()\nmeans = []\nfig = go.Figure()\nfor s in store_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n    means.append(np.mean(past_sales[store_items].sum(axis=1)))\n    fig.add_trace(go.Scatter(x=np.arange(len(data)), y=data, name=s))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (per dept)\")","c129304d":"df = pd.DataFrame(np.transpose([means, store_list]))\ndf.columns = [\"Mean sales\", \"dept name\"]\npx.bar(df, y=\"Mean sales\", x=\"dept name\", color=\"dept name\", title=\"Mean sales vs. dept name\")","c3503524":"past_sales_t = sales_train.set_index('id')[d_cols] \\\n    .T \\\n    .merge(calendar.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\n\n\nfor i in sales_train['cat_id'].unique():\n    items_col = [c for c in past_sales_t.columns if i in c]\n    past_sales[items_col] \\\n        .sum(axis=1) \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Total Sales by Item Type')\nplt.legend(sales_train['cat_id'].unique())\nplt.show()","b4200ab7":"past_sales = sales_train.set_index('id')[d_cols] \\\n    .T \\\n    .merge(calendar.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')","4a3b9ec6":"HTML('<center><iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/Y2khrpVo6qI?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>')","87ab316d":"my_model = pf.ARIMA(data=past_sales, ar=4, ma=4, family=pf.Normal())\nprint(my_model.latent_variables)\n\nresult = my_model.fit(\"MLE\")\nresult.summary()\n\nmy_model.plot_z(figsize=(15,5))  #We can plot the latent variables z^MLE: using the plot_z() method\nmy_model.plot_fit(figsize=(15,10)) #We can plot the in-sample fit using plot_fit()\nmy_model.plot_predict_is(h=50, figsize=(15,5))  #We can get an idea of the performance of our model by using rolling in-sample prediction through the plot_predict_is(): method:\nmy_model.plot_predict(h=20,past_values=20,figsize=(15,5)) #If we want to plot predictions, we can use the plot_predict(): method:","69e3de2a":"model = pf.EGARCH(past_sales, p=1, q=1)\nx = model.fit()\nx.summary()\n\nmodel.plot_z(figsize=(25,5)) #We can plot the EGARCH latent variables with plot_z()\nmodel.plot_fit(figsize=(15,10)) #We can plot the fit with plot_fit()\nmodel.plot_predict(h=10) #And plot predictions of future conditional volatility with plot_predict()\nmodel.plot_predict_is(h=50, figsize=(15,5)) #We can view how well we predicted using in-sample rolling prediction with plot_predict_is()","106f5039":"train_dataset = sales_train[d_cols[-100:-30]]\nval_dataset = sales_train[d_cols[-30:]]","c87d17e9":"fig = go.Figure(data=[go.Surface(z=train_dataset.values)])\n\nfig.update_layout(title='Mt Bruno Elevation', autosize=False,\n                  width=500, height=500,\n                  margin=dict(l=65, r=50, b=65, t=90))\n\nfig.show()","5a8ef114":"fig = go.Figure(data=[go.Surface(z=val_dataset.values)])\n\nfig.update_layout(title='Mt Bruno Elevation', autosize=False,\n                  width=500, height=500,\n                  margin=dict(l=65, r=50, b=65, t=90))\n\nfig.show()","e8cbabb2":"fig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines+markers', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Original signal\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines+markers', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines+markers', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Train (blue) vs. Validation (orange) sales\")\nfig.show()","233635a7":"predictions = []\nfor row in tqdm(train_dataset[train_dataset.columns[-30:]].values[:3]):\n    fit = sm.tsa.statespace.SARIMAX(row, seasonal_order=(0, 1, 1, 7)).fit()\n    predictions.append(fit.forecast(30))\npredictions = np.array(predictions).reshape((-1, 30))\nerror_arima = np.linalg.norm(predictions[:3] - val_dataset.values[:3])\/len(predictions[0])","983fd967":"pred_1 = predictions[0]\npred_2 = predictions[1]\npred_3 = predictions[2]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines+markers', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"),\n               name=\"Val\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n               name=\"Pred\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines+markers', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines+markers', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"ARIMA\")\nfig.show()","62c7d5f1":"predictions = []\nfor row in tqdm(train_dataset[train_dataset.columns[-30:]].values[:3]):\n    fit = Holt(row).fit(smoothing_level = 0.3, smoothing_slope = 0.01)\n    predictions.append(fit.forecast(30))\npredictions = np.array(predictions).reshape((-1, 30))\nerror_holt = np.linalg.norm(predictions - val_dataset.values[:len(predictions)])\/len(predictions[0])","a39869f6":"pred_1 = predictions[0]\npred_2 = predictions[1]\npred_3 = predictions[2]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines+markers', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"),\n               name=\"Val\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n               name=\"Pred\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines+markers', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines+markers', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Holt linear\")\nfig.show()","6d10e5cc":"HTML('<center><iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/95-HMzxsghY?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>')","7ad3a368":"dates = [\"2007-12-\" + str(i) for i in range(1, 31)]\npredictions = []\nfor row in tqdm(train_dataset[train_dataset.columns[-30:]].values[:3]):\n    df = pd.DataFrame(np.transpose([dates, row]))\n    df.columns = [\"ds\", \"y\"]\n    model = Prophet(daily_seasonality=True)\n    model.fit(df)\n    future = model.make_future_dataframe(periods=30)\n    forecast = model.predict(future)[\"yhat\"].loc[30:].values\n    predictions.append(forecast)\npredictions = np.array(predictions).reshape((-1, 30))\nerror_prophet = np.linalg.norm(predictions[:3] - val_dataset.values[:3])\/len(predictions[0])","17242113":"pred_1 = predictions[0]\npred_2 = predictions[1]\npred_3 = predictions[2]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines+markers', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"),\n               name=\"Val\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n               name=\"Pred\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines+markers', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines+markers', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Prophet\")\nfig.show()","eeb740e0":"## Merging the data with calendar real dates\n\n1. We have calendar with additional information about past and future dates.\n2. we will merge the calendar data with our days data\n3. From this we can find weekly and annual trends","2ae8508a":"# EDA \n\n### AS we have so many item \n\n* Lets take a random item that sell a lot and see how it's sales look across the training data.\n* <code>FOODS_3_090_CA_3_validation<\/code> sells a lot\n* Note there are days where it appears the item is unavailable and sales flatline","04151c01":"From the above graph, we can see the same trends: Californian stores have the highest variance and mean among all the stores in the dataset.","f06749f5":"# modeling \n\nLet's run an ARIMA Model. We can build an ARIMA model as follows, specifying the order of model we want, as well as a pandas DataFrame or numpy array carrying the data.\n\n## ARIMA :\n\nAutoregressive integrated moving average (ARIMA) models were popularised by Box and Jenkins (1970). An ARIMA model describes a univariate time series as a combination of autoregressive (AR) and moving average (MA) lags which capture the autocorrelation within the time series. The order of integration denotes how many times the series has been differenced to obtain a stationary series.\n\nWe write an ARIMA(p,d,q) model for some time series data yt, where p is the number of autoregressive lags, d is the degree of differencing and q is the number of moving average lags as:\n\n<img src=\"https:\/\/www.machinelearningplus.com\/wp-content\/uploads\/2019\/02\/Equation-3-min.png\" width=\"400px\">\n\n\nARIMA models are associated with a Box-Jenkins approach to time series. According to this approach, you should difference the series until it is stationary, and then use information criteria and autocorrelation plots to choose the appropriate lag order for an ARIMA process. You then apply inference to obtain latent variable estimates, and check the model to see whether the model has captured the autocorrelation in the time series. For example, you can plot the autocorrelation of the model residuals. Once you are happy, you can use the model for retrospection and forecasting.","7941c53c":"<img src=\"https:\/\/h7f7z2r7.stackpathcdn.com\/sites\/default\/files\/images\/articles\/walmartmain_7.jpg\" width=\"400px\">","5b2cf2fc":"Now let us see how ARIMA performs on our miniature dataset. The training data is in <font color=\"blue\">blue<\/font>, validation data in <font color=\"darkorange\">orange<\/font>, and predictions in <font color=\"green\">green<\/font>.","8e6e3d62":"First, we need to create miniature training and validation sets to train and validate our models. I will take the last 30 days as the validation data and the 70 days before that as the training data. We need to predict the sales in the validation data using the sales in the training data.","5a4cf159":"\n## well come with more soon \n\n\n# Ending note\n\n<font color=\"red\" size=4>This concludes my kernel. Please upvote if you like it. It motivates me to produce more quality content ;)<\/font>\n<font color=\"red\" size=4>thank you so much<\/font>","572e2946":"lets just see the data in 3D :)","14b16b7f":"## Holt linear <a id=\"3.4\"><\/a>\n\nThe **Holt linear** is completely different from the first two methods. Holt linear attempts to capture the high-level trends in the time series data and fits the data with a straight line. The method can be summarized as follows:\n\n### Forecast, level, and trend equations respectively\n\n\n<img src=\"https:\/\/i.imgur.com\/MHgcgGo.png\" width=\"180px\">\n<img src=\"https:\/\/i.imgur.com\/3ImRHEO.png\" width=\"300px\">\n<img src=\"https:\/\/i.imgur.com\/XExnvMX.png\" width=\"300px\">\n\n\nIn the above equations, $\\alpha$ and $\\beta$ are constants which can be configured. The values *l<sub>t<\/sub>* and *b<sub>t<\/sub>* represent the **level** and **trend** values repsectively. The trend value is the slope of the linear forecast function and the level value is the *y*-intercept of the linear forecast function. The slope and *y*-intercept values are continuously updated using the second and third update equations. Finally, the slope and *y*-intercept are used to calculate the forecast *y<sub>t+h<\/sub>* (in equation 1), which is *h* time steps ahead of the current time step. Now let us see how this model performs on our miniature dataset. The training data is in <font color=\"blue\">blue<\/font>, validation data in <font color=\"darkorange\">orange<\/font>, and predictions in <font color=\"green\">green<\/font>.","aac24c0b":"# The dataset <a id=\"1\"><\/a>\n\nThe dataset consists of five .csv files.\n\n* <code>calendar.csv<\/code> - Contains the dates on which products are sold. The dates are in a <code>yyyy\/dd\/mm<\/code> format.\n\n* <code>sales_train_validation.csv<\/code> - Contains the historical daily unit sales data per product and store <code>[d_1 - d_1913]<\/code>.\n\n* <code>submission.csv<\/code> - Demonstrates the correct format for submission to the competition.\n\n* <code>sell_prices.csv<\/code> - Contains information about the price of the products sold per store and date.\n\n* <code>sales_train_evaluation.csv<\/code> - Available one month before the competition deadline. It will include sales for <code>[d_1 - d_1941]<\/code>.\n\nIn this competition, we need to predict the sales for <code>[d_1942 - d_1969]<\/code>. These rows form the evaluation set. The rows <code>[d_1914 - d_1941]<\/code> form the validation set, and the remaining rows form the training set. Now since we understand the dataset and know what to predict, let us visualize the dataset.","270fc253":"In the above graph, we can see a very low disparity in sales among Wisconsin stores. The sales curves intersect each other very often. This may indicate that most parts of Wisconsin have a similar \"development curve\" and that there is a greater equity in development across the state. There are no specific \"hotspots\" or \"hubs\" of development. The average sales in descending order are <code>WI_2, WI_3, WI_1<\/code>. The store <code>WI_2<\/code> has the maximum sales while the store <code>WI_1<\/code> has the minimum sales. \n\n\n## Rolling Average Sales vs. Time (Texas)","dcf625bf":"# Introduction\n\n\nWelcome to the \"M5 Forecasting - Accuracy\" competition! In this competition, contestants are challenged to forecast future sales at Walmart based on heirarchical sales in the states of California, Texas, and Wisconsin. Forecasting sales, revenue and stock prices is a classic application of machine learning in economics, and it is important because it allows investors to make guided decisions based on forecasts made by algorithms. \n\nIn this kernel, I will briefly explain the structure of dataset. Then, I will visualize the dataset using Matplotlib and Plotly. And finally, I will demonstrate how this problem can be approached with a variety of forecasting algorithms.\n\n<font size=3 color=\"red\">Please upvote this kernel if you like it. It motivates me to produce more quality content ;)<\/font>","7a182f0e":"## similarly we will see 30 sample item with the sales by actual sale data","fe649c95":"this says that in our dataset we have 7types of dept_id they are \n\n* <code>FOODS_1<\/code>\n* <code>FOODS_2<\/code>\n* <code>FOODS_3<\/code>\n* <code>HOBBIES_1<\/code>\n* <code>HOBBIES_2<\/code>\n* <code>HOUSEHOLD_1<\/code>\n* <code>HOUSEHOLD_2<\/code>\n\nand the following is the distribution with  <code>cat_id<\/code> , <code>state_id<\/code>  , <code>store_id<\/code>\n\nseem like the store are from three different state in us \n\n<code>CA<\/code>\n<code>TX<\/code>\n<code>WI<\/code>\n\nand they are three types of cat_id\n\n<code>FOOD<\/code>\n<code>HOBBIES<\/code>\n<code>HOUSEHOLD<\/code>\n\n# What exactly are we trying to predict?\n\nWe are trying for forecast sales for 28 forecast days. The sample submission has the following format:\n\n* The columns represent 28 forecast days. We will fill these forecast days with our predictions.\n* The rows each represent a specific item. This id tells us the item type, state, and store. We don't know what these items are exactly.\n\nas we are trying to forecast sales for 28 forecast days.let just check the distribution and move on to EDA","2f21d699":"Now let us see how Prophet performs on our miniature dataset. The training data is in <font color=\"blue\">blue<\/font>, validation data in <font color=\"darkorange\">orange<\/font>, and predictions in <font color=\"green\">green<\/font>.","284b6d12":"In the above graphs, the dark lineplots represent the denoised signals and the light lineplots represent the original signals. We can see Wavelet denoising is able to successfully the \"general trend\" in the sales data without getting distracted by the noise. Finding these macroscopic trends or patterns in the sales may be useful in generating features to train a model.","6587227b":"# Discrete wavelet transform <a id=\"2.3\"><\/a>\n\n\nwe can clearly see that the sales data is very erratic and volatile. Sometimes, the sales are 0 for few days in a row, and at other times, it remains at its peak value for a few days. Therefore, we need some sort of \"denoising\" techniques to understand the underlying trends in the sales data and make predictions accordingly.\n\n\nNow, I will show these volatile sales prices can be denoised in order to extract underlying trends. This method may lose some information from the original time series, but it may be useful in extracting certain features regarding the trends in the time series.\n\n\nWavelet denoising (usually used with electric signals) is a way to remove the unnecessary noise from a time series. This method calculates coefficients called the \"wavelet coefficients\". These coefficients decide which pieces of information to keep (signal) and which ones to discard (noise).\n\nWe make use of the MAD value (mean absolute deviation) to understand the randomness in the signal and accordingly decide the minimum threshold for the wavelet coefficients in the time series. We filter out the low coefficients from the wavelet coefficients and reconstruct the sales data from the remaining coefficients and that's it; we have successfully removed noise from the sales data.\n**","e1cdcd79":"seem like out of all the <code>dept_id<\/code> FOOD have good sales than HOUSEHOLD and HOBBIES","3e84f993":"to see the difference in change of prediction lets make plots and see the difference ","4fbdaa7b":"## Stores and states <a id=\"2.4\"><\/a>\n\nNow, I will look at the sales data across different stores and states and make comparisons.","7a013eed":"### Sample sales data","f77e5339":"<img src=\"https:\/\/miro.medium.com\/max\/640\/1*ghTHwruT5QxQ_g_2rvu-3w.png\" width=\"300px\">\n<img src=\"https:\/\/miro.medium.com\/max\/822\/1*X0Zo0_QK7Ac-87HKM6xZFQ.png\" width=\"300px\">\n","08b72f26":"lets see the datasets are from which years ","f2d5993c":"### Rolling Average Price vs. Time (storewise)","51977252":"### Rolling Average Price vs. Time (CA)","5b677ae3":"lets quickly make EDA with just one line of code for that we have we have dabl(Data Analysis Baseline Library) which tries to help make supervised machine learning more accessible for beginners, and reduce boiler plate for common tasks.lets see ","20b9291b":"In the above graph, I have plotted rolling sales across all stores in the dataset. Almost every sales curve has \"linear oscillation\" trend at the macroscopic level. Basically, the sales oscillate like a sine wave about a certain mean value, but this mean value has an upward linear trend. This implies that the sales are oscillating at a higher and higher level every few months.\n\nhere economies of the stores have short-term oscillatory fluctuations but grow linearly in the long run","ead25c6e":"## Prophet <a id=\"3.7\"><\/a>\n\nProphet is an opensource time series forecasting project by <font color=\"blue\">Facebook<\/font>. It is based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. It is also supposed to be more robust to missing data and shifts in trend compared to other models. The video below explains Prophet very well:","3079eff1":"so the dataset is in a period of <code>2011 to 2016<\/code>","62d4d314":"In the above graph, we can see the large disparity in sales among Californian stores. The sales curves almost never intersect each other. This may indicate that there are certain \"hubs\" of development in California which do not change over time. And other areas always remain behind these \"hubs\". The average sales in descending order are <code>CA_3, CA_1, CA_2, CA_4<\/code>. The store <code>CA_3<\/code> has the maximum sales while the store <code>CA_4<\/code> has the minimum sales. \n\n## Rolling Average Sales vs. Time (Wisconsin)","162f9fe1":"## Beta-t-EGARCH models\n\nBeta-t-EGARCH models were proposed by Harvey and Chakravarty (2008). They extend upon GARCH models by using the conditional score of a t-distribution drive the conditional variance. This allows for increased robustness to outliers through a \u2018trimming\u2019 property of the t-distribution score. Their formulation also follows that of an EGARCH model, see Nelson (1991), where the conditional volatility is log-transformed, which prevents the need for restrictive parameter constraints as in GARCH models.\n\nelow is the formulation for a Beta-t-EGARCH(p,q) model:\n\n\n<img src=\"https:\/\/v8doc.sas.com\/sashtml\/ets\/chap8\/images\/auteq88.gif\" width=\"400px\">\n\n\n","89dd8f41":"In the above graph, we can once again see that a very low disparity in sales among Texan stores. The sales curves intersect each other often, albeit not as often as in Wisconsin. This might once again indicate that most parts of Texas have a similar \"development curve\" and that there is a greater equity in development across the state. The variance here is higher than in Wisconsin though, so there might be \"hubs\" of development in Texas as well, but not as pronounced as in California. The average sales in descending order are <code>TX_2, TX_3, TX_1<\/code>. The store <code>TX_2<\/code> has the maximum sales while the store <code>TX_1<\/code> has the minimum sales.\n\n\n## Rolling Average Sales vs. Time (per dept)"}}