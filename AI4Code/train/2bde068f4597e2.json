{"cell_type":{"45860b26":"code","3cb2ba0c":"code","a579a801":"code","7776a209":"code","d612d487":"code","2bb34f52":"code","5ada6754":"code","29367f48":"code","957143e2":"code","fb3f9cc5":"code","e5651f2d":"code","7c387a11":"code","e1ba2021":"code","61fa8dd9":"code","f1a139a4":"code","171d66ef":"code","17d9b265":"code","b329f793":"code","8ea80d6a":"code","ef291c90":"code","64161059":"code","ab6082c7":"markdown","3d693c52":"markdown","b8748e65":"markdown","afd08e00":"markdown","c020c919":"markdown","3dbdb93b":"markdown","e306292b":"markdown","b643b08b":"markdown","2707ecd3":"markdown","00dc8414":"markdown","2149ee64":"markdown","084edecf":"markdown","2c0c6f2e":"markdown","892dd9e2":"markdown","7b581bda":"markdown","e68db207":"markdown","378bb343":"markdown","08a8e3a1":"markdown","67db2996":"markdown","c8fcc1ff":"markdown","5ed58671":"markdown","0706b2a4":"markdown","5f07ab25":"markdown"},"source":{"45860b26":"# To store data\nimport pandas as pd\n\n# To do linear algebra\nimport numpy as np\nfrom numpy import pi\n\n# To create plots\nfrom matplotlib.colors import rgb2hex\nfrom matplotlib.cm import get_cmap\nimport matplotlib.pyplot as plt\n\n# To create nicer plots\nimport seaborn as sns\n\n# To create interactive plots\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\n# To get new datatypes and functions\nfrom collections import Counter\nfrom cycler import cycler\n\n# To investigate distributions\nfrom scipy.stats import norm, skew, probplot\nfrom scipy.optimize import curve_fit\n\n# To build models\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# To gbm light\nfrom lightgbm import LGBMClassifier\n\n# To measure time\nfrom time import time","3cb2ba0c":"# Load datasets\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n\n# Combine boths dataframes\ntrain_df['Data'] = 'Train'\ntest_df['Data'] = 'Test'\nboth_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\nboth_df['subject'] = '#' + both_df['subject'].astype(str)\n\n# Create label\nlabel = both_df.pop('Activity')\n\nprint('Shape Train:\\t{}'.format(train_df.shape))\nprint('Shape Test:\\t{}\\n'.format(test_df.shape))\n\ntrain_df.head()","a579a801":"# Group and count main names of columns\npd.DataFrame.from_dict(Counter([col.split('-')[0].split('(')[0] for col in both_df.columns]), orient='index').rename(columns={0:'count'}).sort_values('count', ascending=False)","7776a209":"# Get null values and dataframe information\nprint('Null Values In DataFrame: {}\\n'.format(both_df.isna().sum().sum()))\nboth_df.info()","d612d487":"# Plotting data\nlabel_counts = label.value_counts()\n\n# Get colors\nn = label_counts.shape[0]\ncolormap = get_cmap('viridis')\ncolors = [rgb2hex(colormap(col)) for col in np.arange(0, 1.01, 1\/(n-1))]\n\n# Create plot\ndata = go.Bar(x = label_counts.index,\n              y = label_counts,\n              marker = dict(color = colors))\n\nlayout = go.Layout(title = 'Smartphone Activity Label Distribution',\n                   xaxis = dict(title = 'Activity'),\n                   yaxis = dict(title = 'Count'))\n\nfig = go.Figure(data=[data], layout=layout)\niplot(fig)","2bb34f52":"# Create datasets\ntsne_data = both_df.copy()\ndata_data = tsne_data.pop('Data')\nsubject_data = tsne_data.pop('subject')\n\n# Scale data\nscl = StandardScaler()\ntsne_data = scl.fit_transform(tsne_data)\n\n# Reduce dimensions (speed up)\npca = PCA(n_components=0.9, random_state=3)\ntsne_data = pca.fit_transform(tsne_data)\n\n# Transform data\ntsne = TSNE(random_state=3)\ntsne_transformed = tsne.fit_transform(tsne_data)\n\n\n# Create subplots\nfig, axarr = plt.subplots(2, 1, figsize=(15,10))\n\n### Plot Activities\n# Get colors\nn = label.unique().shape[0]\ncolormap = get_cmap('viridis')\ncolors = [rgb2hex(colormap(col)) for col in np.arange(0, 1.01, 1\/(n-1))]\n\n# Plot each activity\nfor i, group in enumerate(label_counts.index):\n    # Mask to separate sets\n    mask = (label==group).values\n    axarr[0].scatter(x=tsne_transformed[mask][:,0], y=tsne_transformed[mask][:,1], c=colors[i], alpha=0.5, label=group)\naxarr[0].set_title('TSNE: Activity Visualisation')\naxarr[0].legend()\n\n\n### Plot Subjects\n# Get colors\nn = subject_data.unique().shape[0]\ncolormap = get_cmap('gist_ncar')\ncolors = [rgb2hex(colormap(col)) for col in np.arange(0, 1.01, 1\/(n-1))]\n\n# Plot each participant\nfor i, group in enumerate(subject_data.unique()):\n    # Mask to separate sets\n    mask = (subject_data==group).values\n    axarr[1].scatter(x=tsne_transformed[mask][:,0], y=tsne_transformed[mask][:,1], c=colors[i], alpha=0.5, label=group)\n\naxarr[1].set_title('TSNE: Participant Visualisation')\nplt.show()","5ada6754":"# Split training testing data\nenc = LabelEncoder()\nlabel_encoded = enc.fit_transform(label)\nX_train, X_test, y_train, y_test = train_test_split(tsne_data, label_encoded, random_state=3)\n\n# Create the model\nlgbm = LGBMClassifier(n_estimators=500, random_state=3)\nlgbm = lgbm.fit(X_train, y_train)\n\n# Test the model\nscore = accuracy_score(y_true=y_test, y_pred=lgbm.predict(X_test))\nprint('Accuracy on testset:\\t{:.4f}\\n'.format(score))","29367f48":"# Store the data\ndata = []\n# Iterate over each activity\nfor activity in label_counts.index:\n    # Create dataset\n    act_data = both_df[label==activity].copy()\n    act_data_data = act_data.pop('Data')\n    act_subject_data = act_data.pop('subject')\n    \n    # Scale data\n    scl = StandardScaler()\n    act_data = scl.fit_transform(act_data)\n\n    # Reduce dimensions\n    pca = PCA(n_components=0.9, random_state=3)\n    act_data = pca.fit_transform(act_data)\n\n    \n    # Split training testing data\n    enc = LabelEncoder()\n    label_encoded = enc.fit_transform(act_subject_data)\n    X_train, X_test, y_train, y_test = train_test_split(act_data, label_encoded, random_state=3)\n\n\n    # Fit basic model\n    print('Activity: {}'.format(activity))\n    lgbm = LGBMClassifier(n_estimators=500, random_state=3)\n    lgbm = lgbm.fit(X_train, y_train)\n    \n    score = accuracy_score(y_true=y_test, y_pred=lgbm.predict(X_test))\n    print('Accuracy on testset:\\t{:.4f}\\n'.format(score))\n    data.append([activity, score])","957143e2":"# Create duration datafrae\nduration_df = (both_df.groupby([label, subject_data])['Data'].count().reset_index().groupby('Activity').agg({'Data':'mean'}) * 1.28).rename(columns={'Data':'Seconds'})\nactivity_df = pd.DataFrame(data, columns=['Activity', 'Accuracy']).set_index('Activity')\nactivity_df.join(duration_df)","fb3f9cc5":"# Create dataset\ntsne_data = both_df[label=='WALKING'].copy()\ndata_data = tsne_data.pop('Data')\nsubject_data = tsne_data.pop('subject')\n\n# Scale data\nscl = StandardScaler()\ntsne_data = scl.fit_transform(tsne_data)\n\n# Split training testing data\nenc = LabelEncoder()\nlabel_encoded = enc.fit_transform(subject_data)\nX_train, X_test, y_train, y_test = train_test_split(tsne_data, label_encoded, random_state=3)\n\n\n# Create model\nlgbm = LGBMClassifier(n_estimators=500, random_state=3)\nlgbm = lgbm.fit(X_train, y_train)\n\n# Get importances\nfeatures = both_df.drop(['Data', 'subject'], axis=1).columns\nimportances = lgbm.feature_importances_\n\n# Sum importances\ndata = {'Gyroscope':0, 'Accelerometer':0}\nfor importance, feature in zip(importances, features):\n    if 'Gyro' in feature:\n        data['Gyroscope'] += importance\n    if 'Acc' in feature:\n        data['Accelerometer'] += importance\n        \n# Create dataframe and plot\nsensor_df = pd.DataFrame.from_dict(data, orient='index').rename(columns={0:'Importance'})\nsensor_df.plot(kind='barh', figsize=(14,4), title='Sensor Importance For Classifing Participants By Walking Style (Feature Importance Sum)')\nplt.show()","e5651f2d":"# Group the data by participant and compute total duration of staircase walking\nmask = label.isin(['WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS'])\nduration_df = (both_df[mask].groupby([label[mask], 'subject'])['Data'].count() * 1.28)\n\n# Create plot\nplot_data = duration_df.reset_index().sort_values('Data', ascending=False)\nplot_data['Activity'] = plot_data['Activity'].map({'WALKING_UPSTAIRS':'Upstairs', 'WALKING_DOWNSTAIRS':'Downstairs'})\n\nplt.figure(figsize=(15,5))\nsns.barplot(data=plot_data, x='subject', y='Data', hue='Activity')\nplt.title('Participants Compared By Their Staircase Walking Duration')\nplt.xlabel('Participants')\nplt.ylabel('Total Duration [s]')\nplt.show()","7c387a11":"# Create data and plot\nplt.figure(figsize=(15,5))\nplot_data = ((duration_df.loc['WALKING_UPSTAIRS'] \/ duration_df.loc['WALKING_DOWNSTAIRS']) -1).sort_values(ascending=False)\nsns.barplot(x=plot_data.index, y=plot_data)\nplt.title('By What Percentage Is The Participant Faster In Walking Downstairs Than Upstairs?')\nplt.xlabel('Participants')\nplt.ylabel('Percent')\nplt.show()","e1ba2021":"def plotSkew(x):\n    # Fit label to norm\n    (mu, sigma) = norm.fit(x)\n    alpha = skew(x)\n\n    fig, axarr = plt.subplots(1, 2, figsize=(15,4))\n\n    # Plot label and fit\n    sns.distplot(x , fit=norm, ax=axarr[0])\n    axarr[0].legend(['$\\mu=$ {:.2f}, $\\sigma=$ {:.2f}, $\\\\alpha=$ {:.2f}'.format(mu, sigma, alpha)], loc='best')\n    axarr[0].set_title('Staircase Walking Duration Distribution')\n    axarr[0].set_ylabel('Frequency')\n\n    # Plot probability plot\n    res = probplot(x, plot=axarr[1])\n    plt.show()\n    \n    \nplotSkew(duration_df)","61fa8dd9":"fig, axarr = plt.subplots(5, 6, figsize=(15,6))\n\nfor person in range(0, 30):\n    # Get data\n    single_person = both_df[(label=='WALKING') & (both_df['subject']=='#{}'.format(person+1))].drop(['subject', 'Data'], axis=1)\n    # Scale data\n    scl = StandardScaler()\n    tsne_data = scl.fit_transform(single_person)\n    # Reduce dimensions\n    pca = PCA(n_components=0.9, random_state=3)\n    tsne_data = pca.fit_transform(tsne_data)\n    # Transform data\n    tsne = TSNE(random_state=3)\n    tsne_transformed = tsne.fit_transform(tsne_data)\n    \n    # Create plot\n    axarr[person\/\/6][person%6].plot(tsne_transformed[:,0], tsne_transformed[:,1], '.-')\n    axarr[person\/\/6][person%6].set_title('Participant #{}'.format(person+1))\n    axarr[person\/\/6][person%6].axis('off')\n    \nplt.tight_layout()\nplt.show()","f1a139a4":"# Group the data by participant and compute total duration of walking\nmask = label=='WALKING'\nduration_df = (both_df[mask].groupby('subject')['Data'].count() * 1.28)\n\n# Create plot\nplot_data = duration_df.reset_index().sort_values('Data', ascending=False)\n\nplt.figure(figsize=(15,5))\nsns.barplot(data=plot_data, x='subject', y='Data')\nplt.title('Participants Compared By Their Walking Duration')\nplt.xlabel('Participants')\nplt.ylabel('Total Duration [s]')\nplt.show()","171d66ef":"# Create subplots\nfig, axarr = plt.subplots(10, 6, figsize=(15,15))\n\n# Iterate over each participant\nfor person in range(0, 30):\n    # Get data\n    single_person_up = both_df[(label=='WALKING_UPSTAIRS') & (both_df['subject']=='#{}'.format(person+1))].drop(['subject', 'Data'], axis=1)\n    single_person_down = both_df[(label=='WALKING_DOWNSTAIRS') & (both_df['subject']=='#{}'.format(person+1))].drop(['subject', 'Data'], axis=1)\n    # Scale data\n    scl = StandardScaler()\n    tsne_data_up = scl.fit_transform(single_person_up)\n    tsne_data_down = scl.fit_transform(single_person_down)\n    # Reduce dimensions\n    pca = PCA(n_components=0.9, random_state=3)\n    tsne_data_up = pca.fit_transform(tsne_data_up)\n    tsne_data_down = pca.fit_transform(tsne_data_down)\n    # Transform data\n    tsne = TSNE(random_state=3)\n    tsne_transformed_up = tsne.fit_transform(tsne_data_up)\n    tsne_transformed_down = tsne.fit_transform(tsne_data_down)\n    \n    # Create plot\n    axarr[2*person\/\/6][2*person%6].plot(tsne_transformed_up[:,0], tsne_transformed_up[:,1], '.b-')\n    axarr[2*person\/\/6][2*person%6].set_title('Up: Participant #{}'.format(person+1))\n    axarr[2*person\/\/6][2*person%6].axis('off')\n    axarr[2*person\/\/6][(2*person%6)+1].plot(tsne_transformed_down[:,0], tsne_transformed_down[:,1], '.g-')\n    axarr[2*person\/\/6][(2*person%6)+1].set_title('Down: Participant #{}'.format(person+1))\n    axarr[2*person\/\/6][(2*person%6)+1].axis('off')\n    \nplt.tight_layout()\nplt.show()","17d9b265":"# Use SS class fro jdarcy\nclass SSA(object):    \n    __supported_types = (pd.Series, np.ndarray, list)\n    \n    def __init__(self, tseries, L, save_mem=True):\n        '''\n        Decomposes the given time series with a singular-spectrum analysis. Assumes the values of the time series are\n        recorded at equal intervals.\n        \n        Parameters\n        ----------\n        tseries : The original time series, in the form of a Pandas Series, NumPy array or list. \n        L : The window length. Must be an integer 2 <= L <= N\/2, where N is the length of the time series.\n        save_mem : Conserve memory by not retaining the elementary matrices. Recommended for long time series with\n            thousands of values. Defaults to True.\n        \n        Note: Even if an NumPy array or list is used for the initial time series, all time series returned will be\n        in the form of a Pandas Series or DataFrame object.\n        '''\n        \n        # Tedious type-checking for the initial time series\n        if not isinstance(tseries, self.__supported_types):\n            raise TypeError('Unsupported time series object. Try Pandas Series, NumPy array or list.')\n        \n        # Checks to save us from ourselves\n        self.N = len(tseries)\n        if not 2 <= L <= self.N\/2:\n            raise ValueError('The window length must be in the interval [2, N\/2].')\n        \n        self.L = L\n        self.orig_TS = pd.Series(tseries)\n        self.K = self.N - self.L + 1\n        \n        # Embed the time series in a trajectory matrix\n        self.X = np.array([self.orig_TS.values[i:L+i] for i in range(0, self.K)]).T\n        \n        # Decompose the trajectory matrix\n        self.U, self.Sigma, VT = np.linalg.svd(self.X)\n        self.d = np.linalg.matrix_rank(self.X)\n        \n        self.TS_comps = np.zeros((self.N, self.d))\n        \n        if not save_mem:\n            # Construct and save all the elementary matrices\n            self.X_elem = np.array([ self.Sigma[i]*np.outer(self.U[:,i], VT[i,:]) for i in range(self.d) ])\n\n            # Diagonally average the elementary matrices, store them as columns in array.           \n            for i in range(self.d):\n                X_rev = self.X_elem[i, ::-1]\n                self.TS_comps[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])]\n            \n            self.V = VT.T\n        else:\n            # Reconstruct the elementary matrices without storing them\n            for i in range(self.d):\n                X_elem = self.Sigma[i]*np.outer(self.U[:,i], VT[i,:])\n                X_rev = X_elem[::-1]\n                self.TS_comps[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])]\n            \n            self.X_elem = 'Re-run with save_mem=False to retain the elementary matrices.'\n            \n            # The V array may also be very large under these circumstances, so we won't keep it.\n            self.V = 'Re-run with save_mem=False to retain the V matrix.'\n        \n        # Calculate the w-correlation matrix.\n        self.calc_wcorr()\n            \n    def components_to_df(self, n=0):\n        '''\n        Returns all the time series components in a single Pandas DataFrame object.\n        '''\n        if n > 0:\n            n = min(n, self.d)\n        else:\n            n = self.d\n        \n        # Create list of columns - call them F0, F1, F2, ...\n        cols = ['F{}'.format(i) for i in range(n)]\n        return pd.DataFrame(self.TS_comps[:, :n], columns=cols, index=self.orig_TS.index)\n            \n    \n    def reconstruct(self, indices):\n        '''\n        Reconstructs the time series from its elementary components, using the given indices. Returns a Pandas Series\n        object with the reconstructed time series.\n        \n        Parameters\n        ----------\n        indices: An integer, list of integers or slice(n,m) object, representing the elementary components to sum.\n        '''\n        if isinstance(indices, int): indices = [indices]\n        \n        ts_vals = self.TS_comps[:,indices].sum(axis=1)\n        return pd.Series(ts_vals, index=self.orig_TS.index)\n    \n    def calc_wcorr(self):\n        '''\n        Calculates the w-correlation matrix for the time series.\n        '''\n             \n        # Calculate the weights\n        w = np.array(list(np.arange(self.L)+1) + [self.L]*(self.K-self.L-1) + list(np.arange(self.L)+1)[::-1])\n        \n        def w_inner(F_i, F_j):\n            return w.dot(F_i*F_j)\n        \n        # Calculated weighted norms, ||F_i||_w, then invert.\n        F_wnorms = np.array([w_inner(self.TS_comps[:,i], self.TS_comps[:,i]) for i in range(self.d)])\n        F_wnorms = F_wnorms**-0.5\n        \n        # Calculate Wcorr.\n        self.Wcorr = np.identity(self.d)\n        for i in range(self.d):\n            for j in range(i+1,self.d):\n                self.Wcorr[i,j] = abs(w_inner(self.TS_comps[:,i], self.TS_comps[:,j]) * F_wnorms[i] * F_wnorms[j])\n                self.Wcorr[j,i] = self.Wcorr[i,j]\n    \n    def plot_wcorr(self, min=None, max=None):\n        '''\n        Plots the w-correlation matrix for the decomposed time series.\n        '''\n        if min is None:\n            min = 0\n        if max is None:\n            max = self.d\n        \n        if self.Wcorr is None:\n            self.calc_wcorr()\n        \n        ax = plt.imshow(self.Wcorr)\n        plt.xlabel(r'$\\tilde{F}_i$')\n        plt.ylabel(r'$\\tilde{F}_j$')\n        plt.colorbar(ax.colorbar, fraction=0.045)\n        ax.colorbar.set_label('$W_{i,j}$')\n        plt.clim(0,1)\n        \n        # For plotting purposes:\n        if max == self.d:\n            max_rnge = self.d-1\n        else:\n            max_rnge = max\n        \n        plt.xlim(min-0.5, max_rnge+0.5)\n        plt.ylim(max_rnge+0.5, min-0.5)\n        \n        \n# Euclidean norm of the acceleration\nwalking_series = both_df[(label=='WALKING') & (both_df['subject']=='#1')][['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z']].reset_index(drop=True)\nwalking_series = (walking_series**2).sum(axis=1)**0.5\n\n# Decomposing the series\nseries_ssa = SSA(walking_series, 30)\n\n# Plotting the decomposition\nplt.figure(figsize=(15,5))\nseries_ssa.reconstruct(0).plot()\nseries_ssa.reconstruct([1,2]).plot()\nseries_ssa.reconstruct([3,4]).plot()\nseries_ssa.orig_TS.plot(alpha=0.4)\nplt.title('Walking Time Series: 3 Main Components')\nplt.xlabel(r'$t$ (s)')\nplt.ylabel('Accelerometer')\nlegend = [r'$\\tilde{{F}}^{{({0})}}$'.format(i) for i in range(3)] + ['Original Series']\nplt.legend(legend);","b329f793":"# Both walking styles from a single participant\nstyle1 = both_df.loc[78:124][['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z']].reset_index(drop=True)\nstyle1 = ((style1**2).sum(axis=1)**0.5)\nstyle1 -= style1.mean()\nstyle2 = both_df.loc[248:295][['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z']].reset_index(drop=True)\nstyle2 = (style2**2).sum(axis=1)**0.5\nstyle2 -= style2.mean()\n\n# Decompose\nstyle1_ssa = SSA(style1, 20)\nstyle2_ssa = SSA(style2, 20)\n\n# Create plot\nfig, axarr = plt.subplots(1, 2, figsize=(15,5))\n\n# Plotting the decomposition style 1\n(style1_ssa.reconstruct([0,1])-0.1).plot(ax=axarr[0])\nstyle1_ssa.orig_TS.plot(alpha=0.4, ax=axarr[0])\naxarr[0].set_title('Walking Experiment\/Style 1: Main Component')\naxarr[0].set_xlabel(r'$t$ (s)')\naxarr[0].set_ylabel('Standardised Accelerometer')\nlegend = [r'$\\tilde{{F}}^{{({0})}}$'.format(i) for i in range(1)] + ['Original Series']\naxarr[0].legend(legend);\n\n# Plotting the decomposition style 2\n(style2_ssa.reconstruct([0,1])-0.1).plot(ax=axarr[1])\nstyle2_ssa.orig_TS.plot(alpha=0.4, ax=axarr[1])\naxarr[1].set_title('Walking Experiment\/Style 2: Main Component')\naxarr[1].set_xlabel(r'$t$ (s)')\naxarr[1].set_ylabel('Standardised Accelerometer')\nlegend = [r'$\\tilde{{F}}^{{({0})}}$'.format(i) for i in range(3)] + ['Original Series']\naxarr[1].legend(legend);","8ea80d6a":"# Function to fit a sinus\ndef fit_sin(tt, yy):\n    '''Fit sin to the input time sequence, and return fitting parameters \"amp\", \"omega\", \"phase\", \"offset\", \"freq\", \"period\" and \"fitfunc\"'''\n    tt = np.array(tt)\n    yy = np.array(yy)\n    # Assume uniform spacing\n    ff = np.fft.fftfreq(len(tt), (tt[1]-tt[0]))\n    Fyy = abs(np.fft.fft(yy))\n    # Exclude the zero frequency \"peak\"\n    guess_freq = abs(ff[np.argmax(Fyy[1:])+1])\n    guess_amp = np.std(yy) * 2.**0.5\n    guess_offset = np.mean(yy)\n    guess = np.array([guess_amp, 2.*np.pi*guess_freq, 0., guess_offset])\n\n    # Sinus\n    def sinfunc(t, A, w, p, c):  \n        return A * np.sin(w*t + p) + c\n    \n    # Fit sinus\n    popt, pcov = curve_fit(sinfunc, tt, yy, p0=guess)\n    A, w, p, c = popt\n    f = w\/(2.*pi)\n    fitfunc = lambda t: A * np.sin(w*t + p) + c\n    return {\"amp\": A, \"omega\": w, \"phase\": p, \"offset\": c, \"freq\": f, \"period\": 1.\/f, \"fitfunc\": fitfunc, \"maxcov\": np.max(pcov), \"rawres\": (guess,popt,pcov)}\n\n# Get data\nmain_style1 = style1_ssa.reconstruct([0, 1])\ntt1 = main_style1.index\nyy1 = main_style1.values\ntt_res1 = np.arange(0, 48, 0.1)\n# Fit data\nres1 = fit_sin(tt1, yy1)\n\n# Get data\nmain_style2 = style2_ssa.reconstruct([0, 1])\ntt2 = main_style2.index\nyy2 = main_style2.values\ntt_res2 = np.arange(0, 48, 0.1)\n# Fit data\nres2 = fit_sin(tt2, yy2)\n\n# Plot data\nfig, axarr = plt.subplots(1, 2, figsize=(15,5))\n\n# Plot data\naxarr[0].plot(tt1, yy1, \"-ok\", label='Data', linewidth=2)\naxarr[0].plot(tt_res1, res1['fitfunc'](tt_res1), \"r-\", label='Fit', linewidth=2)\naxarr[0].set_title('Style 1 Walking Fit: {:.2f} Steps Per Second'.format((res1['omega']*1.28)\/(pi)))\naxarr[0].legend(loc=\"best\")\n\naxarr[1].plot(tt2, yy2, \"-ok\", label='Data', linewidth=2)\naxarr[1].plot(tt_res2, res2['fitfunc'](tt_res2), \"r-\", label='Fit', linewidth=2)\naxarr[1].set_title('Style 2 Walking Fit: {:.2f} Steps Per Second'.format((res2['omega']*1.28)\/(pi)))\naxarr[1].legend(loc=\"best\")\nplt.show()","ef291c90":"# Get data\ntsne_data = both_df[label=='WALKING'].copy()\ndata_data = tsne_data.pop('Data')\nsubject_data = tsne_data.pop('subject')\n\n# Scale data\nscl = StandardScaler()\ntsne_data = scl.fit_transform(tsne_data)\n\n# Reduce dimensions\npca = PCA(n_components=0.9, random_state=3)\ntsne_data = pca.fit_transform(tsne_data)\n\n# Transform data\ntsne = TSNE(random_state=3)\ntsne_transformed = tsne.fit_transform(tsne_data)\n\n\n# Create subplots\nfig, axarr = plt.subplots(1, 1, figsize=(15,10))\n\n### Plot Subjects\n# Get colors\nn = subject_data.unique().shape[0]\ncolormap = get_cmap('gist_ncar')\ncolors = [rgb2hex(colormap(col)) for col in np.arange(0, 1.01, 1\/(n-1))]\n\nfor i, group in enumerate(subject_data.unique()):\n    # Mask to separate sets\n    mask = (subject_data==group).values\n    axarr.scatter(x=tsne_transformed[mask][:,0], y=tsne_transformed[mask][:,1], c=colors[i], alpha=0.5, label=group)\n\naxarr.set_title('TSNE Walking Style By Participant')\nplt.show()","64161059":"data = [['Walking Downstairs', 8.82, 15], \n        ['Walking Downstairs', 9.49, 15], \n        ['Walking Downstairs', 10.06, 15], \n        ['Walking Downstairs', 8.77, 15], \n        ['Walking Fast', 42.25, 80], \n        ['Walking Slow', 62.64, 80], \n        ['Walking Upstairs', 11.16, 15], \n        ['Walking Upstairs', 12.06, 15], \n        ['Walking Upstairs', 11.08, 15], \n        ['Walking Upstairs', 11.12, 15], \n        ['Walking Upstairs', 42.47, 60],\n        ['Walking Downstairs', 10.334, 15],\n        ['Walking Downstairs', 10.785, 15],\n        ['Walking Downstairs', 10.487, 15],\n        ['Walking Slow', 165.449, 200],\n        ['Walking Fast', 115.360, 200],\n        ['Walking Upstairs', 11.823, 15],\n        ['Walking Upstairs', 11.872, 15],\n        ['Walking Upstairs', 11.928, 15],\n        ['Walking Upstairs', 11.351, 15]]\n\ndf = pd.DataFrame(data, columns=['Activity', 'Duration', 'Steps'])\nactivity_df = df.groupby('Activity').sum()\nactivity_df['Frequency'] = activity_df['Steps'] \/ activity_df['Duration']\nactivity_df.sort_values('Frequency', ascending=False)['Frequency'].plot(kind='barh', grid=True, figsize=(15,5))\nplt.title('Self-Experiment: Steps Per Second')\nplt.show()","ab6082c7":"Nearly all participants have more data for walking upstairs than downstairs. Assuming an equal number of up- and down-walks the **participants need longer walking upstairs.**<br>\nFurthermore the range of the duration is narrow and adjusted to the conditions. A young person being ~50% fast in walking upstairs than an older one is reasonable.\n\n### <a id=5.5>How Much Does The Up-\/Downstairs Ratio Vary?<\/a>","3d693c52":"The directors of the study created an incredible number of features from the two sensors.\n\n## <a id=3>Dataset Exploration<\/a>\n### <a id=3.1>Which Features Are There?<\/a>\n\nThe features seem to have a main name and some information on how they have been computed attached. Grouping the main names will reduce the dimensions for the first impression.","b8748e65":"As aspected from most real world data the duration walking on the staircase is **normally distributed**\n\n### <a id=5.7>Is There A Unique Walking Style For Each Participant?<\/a>","afd08e00":"In most of the plots a structure with **two clusters** is again recognizable. Going **up and down the stairs for two times** is likely for the experiment.<br>\nI will review the durations for this assumption with a small experiment on my stairs.\n\n## <a id=6>Exploring Personal Information<\/a>\n### <a id=6.1>What Is The Walking Frequency Of A Single Participant?<\/a>\n\nThanks to the [Singular-Spectrum Analysis notebook](https:\/\/www.kaggle.com\/jdarcy\/introducing-ssa-for-time-series-decomposition) (SSA) from [jdarcy](https:\/\/www.kaggle.com\/jdarcy) I could extract the main components of the walking style of the participants using only the euclidean norm of the three accelerometer axes.<br>\nFirst of all I combined both walking experiments (clusters) of a single participant and tried to decompose the components.","c020c919":"There is a wide range in between the participants for their **ratio of up-\/down-walking.** Since this represents their physical condition I can imagine a **correlation to their age and health** (speculative).\n\n### <a id=5.6>Are There Conspicuities In The Staircase Walking Duration Distribution?<\/a>","3dbdb93b":"Except from the label and the newly created 'Data' and 'subject' features there is only numerical data. Fortunately there are no missing values.\n\n### <a id=3.3>How Are The Labels Distributed?<\/a>","e306292b":"Although there are fluctuations in the label counts, the labels are quite equally distributed.\n\nAssuming the participants had to walk the same number of stairs upwards as well as downwards and knowing the smartphones had a constant sampling rate, there should be the same amount of datapoints for walking upstairs and downstairs. <br>\nDisregarding the possibility of flawed data, the participants seem to **walk roughly 10% faster downwards.**\n\n## <a id=4>Activity Exploration<\/a>\n### <a id=4.1>Are The Activities Separable?<\/a>\n\nThe dataset is geared towards classifying the activity of the participant. Let us investigate the separability of the classes.","b643b08b":"Since the duration of each participant walking is distributed over a range I assume the participants had a **fixed walking distance for their experiment** rather than a fixed duration.\n\n### <a id=5.9>Is There A Unique Staircase Walking Style For Each Participant?<\/a>","2707ecd3":"The accelerometer supplies slightly more information. Both sensors are important for classification and refraining from using both sensors will be a drawback for the quality of the model.\n\n### <a id=5.4>How Long Does The Participant Use The Staircase?<\/a>\n\nSince the dataset has been created in an scientific environment nearly equal preconditions for the participants can be assumed. It is highly likely for the participants to have been walking up and down the same number of staircases. Let us investigate their activity durations.","00dc8414":"For this visualisation i am assuming the **datapoints were not shuffled** and are in the correct order (time series).\n\nVisualising the walking structure for each participant you can see some **outliers** (e. g. #4,  #18 and #26) in the data, which **could be 'starting to walk', 'stopping' or 'stumble'.** Additional there are **two clusters for each participant.** How these clusters should be interpreted is not clear.<br>\nIt cannot be the steps for each foot, since there would be connections between the clusters for each alternating step. Due to the fact that there is (mostly) only a single connection between the clusters and each cluster has just about the same size I conclude **each cluster represents a single walking experiment.**\n\n### <a id=5.8>How Long Does The Participant Walk?<\/a>","2149ee64":"Mainly there are 'acceleration' and 'gyroscope' features. A few 'gravity' features are there as well.\n\nImpressive how many features there are in regard of the limited number of sensors used.\n\n### <a id=3.2>What Types Of Data Are There?<\/a>","084edecf":"## <a id=2>Load Data<\/a>","2c0c6f2e":"For the decomposition I have only shown a single participant with the two walking styles. By computing a t-SNE visualization of the walking of all participants you can see two clusters for each participant (Some more distinct than others). Therefore the option of splitting the styles by decomposition suggests itself.\n\n### <a id=6.3>What Are The Frequencies In A Self-Experiment?<\/a>\n\nI recorded a few examples for the frequencies in a self-experiment to compare the extracted frequencies from the smartphones with real world data. For that reason I counted the steps while walking and m[](http:\/\/)easured the corresponding time.","892dd9e2":"In plot 1 you can clearly see the **activities are mostly separable.**\n\nPlot 2 reveals **personal information** of the participants. Everybody has for example an **unique\/sparable walking style** (on the upper right). Therefore the smartphone should be able to **detect what you are doing and also who is using the smartphone** (if you are moving around with it).\n\n### <a id=4.2>How Good Are The Activities Separable?<\/a>\n\nWithout much preprocessing and parameter tuning a simple LGBMClassifier should work decently.","7b581bda":"**Fitting a sinus curve** to the main component of the walking experiments of participant #1 reveals the **computed speed of both experients differed by a factor of nearly 2.**<br>\nSince the second speed is suspiciously low there could be a sampling problem and information could have been irrevocably lost while aggregating the dataset ([Sampling Theorem](https:\/\/en.wikipedia.org\/wiki\/Nyquist%E2%80%93Shannon_sampling_theorem)). Potentially the datapoints are to wide spread to reconstruct the underlying frequency in a correct way. This will be compared to a self-experiment in the next section to back up these statements.","e68db207":"You can see the original data from the accelerometer can be decomposed into three main components. The first one (blue) reveals the overall constant trend. The other two **(yellow, green) represent the oscillating walking frequencies.**<br>\nThe **frequency change** in the middle of the plot underlines the assumption of **two distinct walking experiments.** Furthermore it can be stated that both experiments had **different walking speeds** (e. g. walking and running)\n\n### <a id=6.2>What Is The Walking Frequency Of Both Found Speeds?<\/a>\n\nBoth experiments of a single person have been split and will be analysed separately.","378bb343":"**Detecting the correct participant** regarind their current activity is not alone possible but **astonishing accurate** regarding the 30 different persons **(94% by walking style)**.<br>\nNoticable is that the accuracy seems to rise if the participant moves around. This implies a unique walking\/movement style for each person.\n\n### <a id=5.2>How Long Does The Smartphone Gather Data For This Accuracy?<\/a>\n\nThe description of the data states; \"fixed-width sliding windows of 2.56 sec and 50% overlap\" for each datapoint.<br>\nTherefore a single datapoint is gathered every 1.28 sec.","08a8e3a1":"With a basic untuned model the **activity of the smartphone user** can be predicted with an **accuracy of 95%.**<br>\nThis is pretty striking regarding six equally distributed labels.\n\n**Summary:**<br>\nIf the smartphone or an App wants to know what you are doing, this is feasible.\n\n## <a id=5>Participant Exploration<\/a>\n### <a id=5.1>How Good Are the Participants Separable?<\/a>\n\nAs we have seen in the second t-SNE plot the separability of the participants seem to vary regarding their activity. Let us investigate this a little bit by fitting the same basic model to the data of each activity separately.","67db2996":"## <a id=7>Conclusion<\/a>\n\nWithin a short time **(1-1.5 min)** the smartphone has enough data to determine what its user is doing (**95%**: 6 activities) or who the user is (**Walking 94%**: 30 participants) and even the basics of a persons specific walking style (**Slow steps per second**). By linking this insights to more personal data of the participants extensiv options open up.<br>\nIn addition this insights have been extracted from only two smartphone sensors which probably could be accessed by most of our Apps.<br>\n**Let us not consider for how long we have been carrying our gadgets around.**\n\nI hope this was interesting for you.<br>\nHave a good day.","c8fcc1ff":"# What Does Your Smartphone Know About You?\n\n[Video-Discussion of this notebook](https:\/\/youtu.be\/mCS1bmB0A-s)\n\nIn this notebook I will try to extract as much information about the smartphone user as possible.<br>\nFeel free to suggest new ideas for exploration.\n\n30 participants performed activities of daily living while carrying a waist-mounted smartphone. The phone was configured to record two implemented sensors (accelerometer and gyroscope). For these time series the directors of the underlying study performed feature generation and generated the dataset by moving a fixed-width window of 2.56s over the series. Since the windows had 50% overlap the resulting points are equally spaced (1.28s).\n\n[1 Import Libraries](#1)    \n[2 Load Data](#2)    \n[3 Dataset Exploration](#3)    \n&nbsp;&nbsp;&nbsp;&nbsp;[3.1 Which Features Are There?](#3.1)    \n&nbsp;&nbsp;&nbsp;&nbsp;[3.2 What Types Of Data Are There?](#3.2)    \n&nbsp;&nbsp;&nbsp;&nbsp;[3.3 How Are The Labels Distributed?](#3.3)    \n[4 Activity Exploration](#4)    \n&nbsp;&nbsp;&nbsp;&nbsp;[4.1 Are The Activities Separable?](#4.1)    \n&nbsp;&nbsp;&nbsp;&nbsp;[4.2 How Good Are The Activities Separable?](#4.2)    \n[5 Participant Exploration](#5)    \n&nbsp;&nbsp;&nbsp;&nbsp;[5.1 How Good Are the Participants Separable?](#5.1)    \n&nbsp;&nbsp;&nbsp;&nbsp;[5.2 How Long Does The Smartphone Gather Data For This Accuracy?](#5.2)    \n&nbsp;&nbsp;&nbsp;&nbsp;[5.3 Which Sensor Is More Important For Classifying Participants By Walking Style?](#5.3)    \n&nbsp;&nbsp;&nbsp;&nbsp;[5.4 How Long Does The Participant Use The Staircase?](#5.4)    \n&nbsp;&nbsp;&nbsp;&nbsp;[5.5 How Much Does The Up-\/Downstairs Ratio Vary?](#5.5)    \n&nbsp;&nbsp;&nbsp;&nbsp;[5.6 Are There Conspicuities In The Staircase Walking Duration Distribution?](#5.6)    \n&nbsp;&nbsp;&nbsp;&nbsp;[5.7 Is There A Unique Walking Style For Each Participant?](#5.7)    \n&nbsp;&nbsp;&nbsp;&nbsp;[5.8 How Long Does The Participant Walk?](#5.8)    \n&nbsp;&nbsp;&nbsp;&nbsp;[5.9 Is There A Unique Staircase Walking Style For Each Participant?](#5.9)    \n[6 Exploring Personal Information](#6)    \n&nbsp;&nbsp;&nbsp;&nbsp;[6.1 What Is The Walking Frequency Of A Single Participant?](#6.1)    \n&nbsp;&nbsp;&nbsp;&nbsp;[6.2 What Is The Walking Frequency Of Both Found Speeds?](#6.2)    \n&nbsp;&nbsp;&nbsp;&nbsp;[6.3 What Are The Frequencies In A Self-Experiment?](#6.3)    \n[7 Conclusion](#7)    \n\n## <a id=1>Import Libraries<\/a>","5ed58671":"The smartphone is **quite fast (1 - 1.5 min)** in guessing correctly.\n\n### <a id=5.3>Which Sensor Is More Important For Classifying Participants By Walking Style?<\/a>\n\nI will fit another basic model to the walking data and investigate the feature importances afterwards. Since there are so many features I am going to group them by their sensor (accelerometer = Acc, gyroscope = Gyro)","0706b2a4":"Decomposing both experiments separatly offers highly improved results. Fitting a sin-curve to the **main component** should reveal the **step frequency of the participant.**","5f07ab25":"**Please have in mind:** since the dataset and my experiment both only provide a small amount of data the variance of the results can be high.\n\nThe first walking speed in the dataset could correspond to my **slow walking style (Dataset: 0.94 Steps\/Second, Self-Experiment: 1.23 Steps\/Second)**.<br>\nIn contrast to this the **second walking speed in the dataset raises some issues. Walking with 0.50 Steps\/Second seems highly unlikely** after trying it for myself. Furthermore a fast walking style has around ~1.5 steps per aggregated datapoint (1.28 Seconds\/Point). Therefore a **fast walking style cound hardly be extracted with this small amount of data**. Since the sinus can be fitted to the data the correct step frequency can be found within integral multiples of the found frequency (0.5\/1.0\/1.5\/... Steps\/Second).\n<br>A dataset with raw data or a higher sampling frequency for the points should reveal some more insights regarding the faster walking styles.\n"}}