{"cell_type":{"aab54f83":"code","2d2cd7dc":"code","51a49bbc":"code","ebfa4cbf":"code","0a96bba9":"code","2f371f69":"code","e686b202":"code","3a1c5492":"code","d81415c8":"code","2a1e76bb":"code","caa8d082":"code","d8c191ee":"code","b747d77d":"code","053ca4d3":"code","b4bac589":"markdown","4b52030e":"markdown","ca7f0cf2":"markdown","9a6e8ac5":"markdown","9a9359c7":"markdown","ef659e4d":"markdown","492831e5":"markdown","fb52a2c3":"markdown","b5ee4a69":"markdown","5a9f94a1":"markdown"},"source":{"aab54f83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2d2cd7dc":"import numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\nimport keras\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout, Conv2D, MaxPooling2D, Flatten\nfrom keras.optimizers import RMSprop, Adam\nimport pandas as pd\nimport matplotlib.pyplot as plt","51a49bbc":"# Read data from file\ndata_train = np.loadtxt('..\/input\/train.csv', skiprows=1, delimiter=',')\ndata_test = np.loadtxt('..\/input\/test.csv', skiprows=1, delimiter=',')","ebfa4cbf":"# Normalize and split data set\ndata_test = data_test\/255.0\nX_train = data_train[:,1:]\/255.0\ny_train = data_train[:,0]\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2)\nimage_height = 28\nimage_width = 28","0a96bba9":"# Visualize some samples\nnplot=10\nfig, axes = plt.subplots(nplot, nplot, sharex=True, sharey=True, figsize=(9,9))\nidex = np.arange(0,X_train.shape[0])\nfor i in range(nplot):\n    for j in range(nplot):\n        axes[i,j].imshow(X_train[np.random.choice(idex,1,replace=False)[0],:].reshape(image_height,image_width), cmap=plt.get_cmap('gray'))\n        axes[i,j].set_xticklabels([])\n        axes[i,j].set_yticklabels([])\n        axes[i,j].axis('off')\nplt.show()","2f371f69":"# Check the balance of data set\nx = np.arange(0,10)\nnumnum = []\nfor i in range(len(x)):\n    numnum.append((y_train == x[i]).sum())\nplt.bar(x, numnum, color='r')\nplt.xticks(range(10))\nplt.xlabel('Digit')\nplt.ylabel('# of Samples')\nplt.show()","e686b202":"acc_lrg = []\nfor C in [0.00001, 0.001, 1.0, 100]:\n    lrg = LogisticRegression(C=C)\n    lrg.fit(X_train,y_train)\n    acc_lrg.append([C, lrg.score(X_train, y_train), lrg.score(X_test, y_test)])\nacc_lrg = np.array(acc_lrg)","3a1c5492":"plt.plot(np.log10(acc_lrg[:,0]), acc_lrg[:,1], marker='o', label='Training')\nplt.plot(np.log10(acc_lrg[:,0]), acc_lrg[:,2], marker='s', label='Testing')\nplt.xlabel(r'log$_{10}$(C)')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","d81415c8":"acc_rfc = []\nfor n_est in [10, 20, 40, 60, 80, 120]:\n    rfc = RandomForestClassifier(n_estimators=n_est, bootstrap=False)\n    rfc.fit(X_train, y_train)\n    acc_rfc.append([n_est, rfc.score(X_train, y_train), rfc.score(X_test, y_test)])\nacc_rfc = np.array(acc_rfc)","2a1e76bb":"plt.plot(acc_rfc[:,0], acc_rfc[:,1], marker='o', label='Training')\nplt.plot(acc_rfc[:,0], acc_rfc[:,2], marker='s', label='Testing')\nplt.xlabel('# of Tree')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","caa8d082":"n_class = 10\ny_train_nn = keras.utils.to_categorical(y_train, n_class)\ny_test_nn = keras.utils.to_categorical(y_test, n_class)\nX_train_cnn = X_train.reshape(X_train.shape[0], image_height, image_width, 1)\nX_test_cnn = X_test.reshape(X_test.shape[0], image_height, image_width, 1)","d8c191ee":"# Define model\ncnn_model = Sequential()\ncnn_model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(image_height, image_width, 1)))\ncnn_model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2,2)))\ncnn_model.add(Dropout(0.25))\n\ncnn_model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\ncnn_model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2,2)))\ncnn_model.add(Flatten())\n\ncnn_model.add(BatchNormalization())\ncnn_model.add(Dense(128, activation='relu'))\ncnn_model.add(Dropout(0.25))\ncnn_model.add(Dense(n_class, activation='softmax'))\n\ncnn_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\ncnn_model.summary()","b747d77d":"# Training the model\ncnn_model_his = cnn_model.fit(X_train_cnn, y_train_nn, epochs=20, validation_data=(X_test_cnn, y_test_nn))","053ca4d3":"# Reshape input data\ndata_test_cnn = data_test.reshape(data_test.shape[0], image_height, image_width, 1)\nanswer = cnn_model.predict(data_test_cnn)\nanswer = np.argmax(answer, axis=1)\nsubmission = pd.Series(answer, name='Label')\nsubmission = pd.concat([pd.Series(range(1,28001), name='ImageId'), submission], axis=1)\nsubmission.to_csv(\"my_submission.csv\", index=False)","b4bac589":"## Convolutional Neural Network (CNN)\nPreparing data for CNN model: converting categorial label of digit to vector representation.","4b52030e":"## Predictive models\nWe will go from simple models like Logistic Regression to more complex Random Forest and to complex and advanced Convolutional Neural Network (CNN).\n### Logistic Regression\nWe need to tune the regularization prefactor parameter to avoid overfitting for logistic regression.","ca7f0cf2":"Submit my prediction","9a6e8ac5":"## Random Forest","9a9359c7":"In this CNN model, several Dropout and BatchNormalization layers will be used. Dropout layer will help to reduce overfit. BatchNormalization layers will help to stabilize the learing process and it also partially help reducing overfit as it introduces some noises to the learning process.","ef659e4d":"The data set is pretty balance so we don't need to use cost-sensitive trick like penalize","492831e5":"## Reading and Preparing data\nRead the train and final tes data from file. Normalize the data, split the train data to train-test sets and visualize some samples. And check the balance of data set.","fb52a2c3":"We can get quite better accuracy of 0.965 with Random Forest model using 60 trees. The accuracy is already very high but as we will see latter that with comlex and advanced CNN model, we can get higher than 0.99!\nWe already tried to increase number of tree to reduce overfit, but it looks clearly from plot that the model here is still having issues of overfit. More tuning other parameter such as min_samples_split or min_samples_leaf may help reduce more overfitting.","b5ee4a69":"With 10 epoch we already can get accuracy of > 0.99 for test set.","5a9f94a1":"For C = 1 we get the best tesing accuracy of ~ 0.91, which is very impresive already for this simple model. However, we need to go to complex and advanced CNN model which is known superior for image\/visulizaton applications to get more than 0.99 accuracy."}}