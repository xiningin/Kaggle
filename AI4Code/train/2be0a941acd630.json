{"cell_type":{"d45ce2c6":"code","15507211":"code","e7d723eb":"code","ee95dad4":"code","eccd95a5":"code","fd582b8b":"code","cc0c9d52":"code","fe2c960e":"code","793b5704":"code","7aba3732":"code","e4a64774":"code","41d5a178":"code","671d96d5":"code","0361f955":"code","b97bd7f1":"markdown","464cde15":"markdown","179eba93":"markdown","824c29f2":"markdown","17ce0893":"markdown","b3df45e5":"markdown","cc582fc6":"markdown","c7ca134c":"markdown"},"source":{"d45ce2c6":"# All users currently ranked here or higher are shown\nSHOW_TOP_RANKS = 100\n\n# Words to show as text\/search links, above the word cloud\nSHOW_TOP_WORDS = 30\n\n# Top words per user to save in CSV file\nSAVE_TEXT_WORDS = 200\n\n# If >MAX_DF of users use a word\/bigram\/trigram it will be ignored\n# Look in the stop_words.txt output file to see what this excludes.\nMAX_DF = 0.8\n\n# Colormap names:\n# https:\/\/matplotlib.org\/3.1.0\/tutorials\/colors\/colormaps.html\nCOLORS = {\n    4: 'Wistia',  # GM\n    3: 'autumn',  # Master\n    2: 'spring',  # Expert\n}\nHOST = 'https:\/\/www.kaggle.com'\nTIER_NAMES = ['Novice', 'Contributor', 'Expert', 'Master', 'Grand Master']\nTIER_COLORS = ['green', 'blue', 'purple', 'orange', 'gold', 'black']\nIMGS = {\n    2: f\"{HOST}\/static\/images\/tiers\/expert@48.png\",\n    3: f\"{HOST}\/static\/images\/tiers\/master@48.png\",\n    4: f\"{HOST}\/static\/images\/tiers\/grandmaster@48.png\"\n}","15507211":"from jt_mk_utils import *","e7d723eb":"%matplotlib inline\nimport gc, os, re, sys, time\nimport pandas as pd, numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML, display\nimport plotly.express as px\nfrom wordcloud import WordCloud\nfrom bs4 import BeautifulSoup\nimport zlib\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nfrom nltk.stem import SnowballStemmer\nstemmer = SnowballStemmer(\"english\")\n\nID = 'Id'\nKEY = 'ForumId'\nUSER_RANKS_CSV = Path(f'..\/input\/kaggle-discussion-user-rankings')\n\nFONT_PATH = '\/usr\/share\/fonts\/truetype\/dejavu\/DejaVuSans.ttf'\n\nEMOJI = ''.join([chr(c) for c in range(0x1f600, 0x1f641)])\nEMOTICON = r\"[:;8X]['`\\-\\^]?[\\)\\(\/dp]\"\n\nREGEX = [\n    re.compile(r'(?:[\\'\\\"]?https?:\/\/\\S+)'),         # strip URLs   \n    re.compile(r'(?:\\'s\\b)'),                       # strip 's\n    re.compile(r'([a-fA-F0-9_\\-]{12,})'),           # long hash-like strings\n    re.compile(r'\\[quote.*\\[\/quote\\]', flags=re.S)  # strip [quote]\n]\n\nTOKEN_PATTERNS = [\n    f\"(?:[{EMOJI}])\",\n    f\"(?:{EMOTICON})\",\n    r\"(?:\\bt-sne\\b)\",\n    r\"(?:\\b\\w[\\w']+\\w+\\b)\",\n    r\"(?:\\b\\w\\.[\\w\\.]+\\b)\", # e.g.  i.e.\n    r\"(?:\\b\\w\\w+\\b)\"\n]\nTOKEN_PATTERN = re.compile(\"|\".join(TOKEN_PATTERNS))\n\ndef bigrams(words):\n    return list(map(' '.join, zip(words, words[1:])))\n\ndef trigrams(words):\n    return list(map(' '.join, zip(words, words[1:], words[2:])))\n\ndef tokenize(s):\n    soup = BeautifulSoup(s, 'lxml')\n    text = soup.get_text('\\n', strip=True)\n    # text cleaning\n    for r in REGEX:\n        text = r.sub(' ', text)\n    lines = text.splitlines()\n    tokens = []\n    # tokenize and add 2-grams and 3-grams *within lines*\n    for line in lines:\n        l = TOKEN_PATTERN.findall(line.lower())\n        tokens.extend(l)\n        tokens.extend(bigrams(l))\n        tokens.extend(trigrams(l))\n    return tokens\n\n# this is to ensure :-p :-D and XD are upper case\ndef fix_emoticons(s):\n    return re.sub(f'({EMOTICON}\\\\b)', lambda m: m.group(1).upper(), s,\n                  flags=re.IGNORECASE)\n\ndef simple_slug(txt):\n    return re.sub('[^a-zA-Z0-9\\-_]+', '-', txt.lower())\n\ndef search_url(q):\n    return f'{HOST}\/search?q={q}'\n\ndef compress(s):\n    return zlib.compress(s.lower().encode('utf-8'))","ee95dad4":"def add_discussion_tier_old(dst, col):\n    # UserAchievements.csv is out of date, see:\n    # https:\/\/www.kaggle.com\/kaggle\/meta-kaggle\/discussion\/181048\n    # and no longer maintained:\n    # https:\/\/www.kaggle.com\/kaggle\/meta-kaggle\/discussion\/255635#1405368\n    df = pd.read_csv(MK \/ 'UserAchievements.csv')\n    df = df.query('AchievementType==\"Discussion\"').set_index('UserId')\n    dst['DiscussionTier'] = dst[col].map(df.Tier)\n    dst['DiscussionRanking'] = dst[col].map(df.CurrentRanking) \n\ndef add_discussion_tier(dst, col):\n    # my replacement for UserAchievements.csv\n    df = pd.read_csv(USER_RANKS_CSV \/ 'DiscussionRankings.csv').set_index('UserId')\n    dst['DiscussionTier'] = dst[col].map(df.Tier)\n    dst['DiscussionRanking'] = dst[col].map(df.CurrentRanking) \n\nusers = read_users(index_col=ID)\nforums = read_forums(index_col=ID)\ntopics = read_forum_topics(index_col=ID)\nposts_df = read_forum_messages(index_col=ID).dropna(subset=['Message'])\nposts_df = posts_df.sort_index()\nadd_discussion_tier(posts_df, 'PostUserId')\nposts_df.insert(0, 'ForumId', posts_df.ForumTopicId.map(topics.ForumId))\nposts_df.insert(0, 'ParentForumId', posts_df.ForumId.map(forums.ParentForumId))\nposts_df.shape","eccd95a5":"# fork to try \"Medal>0 and (DiscussionTier>=3 or DiscussionRanking<=@SHOW_TOP_RANKS)\"\nsub_df = posts_df.query(\"DiscussionTier>=3 or DiscussionRanking<=@SHOW_TOP_RANKS\")\nsub_df.shape","fd582b8b":"# tokenize all messages\ntokenized_messages = sub_df.Message.apply(tokenize)\ntokenized_messages.shape","cc0c9d52":"# concatenate all the tokenized messages for each user\nKEY_LIST = ['DiscussionRanking', 'PostUserId']\ndocs = tokenized_messages.groupby([sub_df[k] for k in KEY_LIST]).sum()\ndocs.shape","fe2c960e":"# Full list of settings here:\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html\ntfv = TfidfVectorizer(analyzer=lambda l: l,\n                      max_df=MAX_DF,\n                      dtype=np.float32)\nxall = tfv.fit_transform(docs)\ndisplay(HTML(f\"fit_transform shape: {xall.shape}\"))\n# Post processing!\ntfv.vocabulary_ = {fix_emoticons(k): v for k, v in tfv.vocabulary_.items()}\nwords = tfv.get_feature_names()\nnp.savetxt(f'stop_words.txt', list(sorted(tfv.stop_words_)), '%s')","793b5704":"# Save IDF weights (rarer words have a higher weight)\nidf = pd.Series(tfv.idf_, words).sort_values()\nidf.to_frame('IDF').to_csv(f'idf_weights.csv', index_label='Words')","7aba3732":"def generate_clouds():\n    rows = []\n    for row, ((rank, uid), df) in enumerate(sub_df.groupby(KEY_LIST)):\n        u = users.loc[uid]\n        s = pd.Series(index=words, data=xall[row].toarray().ravel())\n        s = s[s > 0].sort_values(ascending=False)\n        \n        top_words = s.head(SAVE_TEXT_WORDS).index.str.replace(' ', '_').tolist()\n        rows.append([rank, u.UserName, len(s), s.max(), s.mean()] + top_words)\n\n        if 0:\n            # filter out words\/bigrams that are in longer bi\/trigrams\n            s = s.head(1000)\n            # sort by token length, longest first\n            s = s[s.index.str.len().argsort()[::-1]]\n            use = {}\n            for w, c in s.items():\n                if all(w not in prev for prev in use): # O(N^2) runtime alert!\n                    use[w] = c\n            s = pd.Series(use)\n            s = s.sort_values(ascending=False)\n\n        tier = df.iloc[0].DiscussionTier\n        days = df.PostDate.dt.date.nunique()\n        chars = df.Message.str.len().sum()\n\n        top = s.head(SHOW_TOP_WORDS).index\n        top = [f\"<a href='{search_url(w)}'>{w}<\/a>\" for w in top]\n        top = ', '.join(top)\n\n        html = (\n            f'<img src=\"{IMGS[tier]}\" style=\"display: inline;\" \/> &nbsp; '\n            f'<h1 style=\"display: inline;\" id=\"{u.UserName}\">#{rank:.0f} {u.DisplayName}<\/h1> '\n            f'(@{u.UserName}) '\n            f'- Joined {u.RegisterDate.strftime(\"%A %-d %b %Y\")}'\n            #\n            f'<ul>'\n            f'<li><a href=\"{HOST}\/{u.UserName}\/discussion\">Discussion index<\/a>'\n            f'<li>Posted in {df.ForumTopicId.nunique()} unique topics'\n            f'<li>{days} unique days;'\n            f'    {df.shape[0]\/days:.1f} posts per day'\n            f'<li>{df.shape[0]} messages;'\n            f'    {chars} raw characters;'\n            f'    {int(chars\/df.shape[0])} chars per message'\n            f'<li>Top {SHOW_TOP_WORDS} words: {top}'\n            f'<\/ul>'\n            f'<h3>Top Forums<\/h3>'\n        )\n        \n        display(HTML(html))\n        topf = forums.loc[forums.index.intersection(df.ForumId)].Title.value_counts()\n        topf = topf.to_frame(\"Message Count\")\n        topf.index.name = \"Forum\"\n        display(topf.head(5))\n        \n        wc = WordCloud(background_color='black',\n                       width=800,\n                       height=600,\n                       colormap=COLORS[tier],\n                       font_path=FONT_PATH,\n                       random_state=1 + row,\n                       min_font_size=10,\n                       max_font_size=200).generate_from_frequencies(s)\n        fig, ax = plt.subplots(figsize=(12, 9))\n        ax.imshow(wc, interpolation='bilinear')\n        ax.axis('off')\n        plt.tight_layout()\n        plt.show()\n        \n    # save stats of all users to one file\n    c1 = [ 'Rank', 'UserName', 'count', 'max', 'mean' ]\n    c2 = [f'tok{i}' for i in range(SAVE_TEXT_WORDS)]\n    df = pd.DataFrame(rows, columns=c1+c2).set_index('Rank')\n    df.to_csv(f'user_word_stats.csv')","e4a64774":"generate_clouds()","41d5a178":"from sklearn.decomposition import TruncatedSVD\nNSVD = 80\nsvd = TruncatedSVD(n_components=NSVD, random_state=42)\nxc = svd.fit_transform(xall)\nsvd.explained_variance_ratio_.cumsum()","671d96d5":"from sklearn.manifold import TSNE\ntsne = TSNE(perplexity=20, early_exaggeration=1, init='pca', method='exact', learning_rate=5, n_iter=5000)\nx2 = tsne.fit_transform(xc)\n\nusers_df = pd.DataFrame(list(docs.index), columns=KEY_LIST)\nusers_df = pd.concat((users_df, pd.DataFrame(x2).add_prefix('tsne')), axis=1)\nusers_df = users_df.set_index('PostUserId')\nusers_df = users_df.join(users)\nusers_df = users_df.sort_values('DiscussionRanking', ascending=False)","0361f955":"fig = px.scatter(\n    users_df.assign(Tier=np.asarray(TIER_NAMES)[users_df.PerformanceTier],\n                    Size=(10 \/ np.log1p(users_df.DiscussionRanking)).round(3),\n                    RegisterDate=users_df.RegisterDate.dt.strftime(\"%A %-d %b %Y\"),\n                    Year=users_df.RegisterDate.dt.year),\n    title='Kaggle Writers 2D Semantic Space',\n    x='tsne0',\n    y='tsne1',\n    #symbol='Year',\n    size='Size',\n    hover_name='DisplayName',\n    hover_data=['UserName', 'RegisterDate', 'DiscussionRanking'],\n    color='Tier',\n    color_discrete_map=dict(zip(TIER_NAMES, TIER_COLORS)))\nfig.update_traces(marker=dict(line=dict(width=1, color='DarkSlateGrey')),\n                  selector=dict(mode='markers'))\nfig.update_layout(showlegend=False)","b97bd7f1":"# Read Forums","464cde15":"It seems there is a set of *core* users who've been on the site longer, in the middle of the plot, probably using similar terms.\n\nSome of the points are so close they are overlapping - hover over them to see the names or click+drag to zoom in.\n\nIdea: use voting information to connect the dots. Do users who write using similar vocubularies up-vote **each other** more often?!","179eba93":"# Kaggle Writers 2D Semantic Space\n\nIs there a way to see how similar each word cloud is to the others?\n\nWe have one row of data per user, and one column per word or bi-gram, so many columns. We can use linear algebra (TruncatedSVD) to compress it to fewer dimensions, making it easier to compare entries. Then use manifold learning (T-SNE) to force it down to just two dimensions. The result is that each user should be positioned in the 2D space, close to other users with similar word and bi-gram usage. We may see clusters of users that use similar language, or outliers, where users are posting one specific kind of message a lot.\n","824c29f2":"# TfidfVectorizer","17ce0893":"# User Reports","b3df45e5":"# What Are You Talking About?\n\nThis notebook investigates the words and phrases used by the top Kaggle users in the discussions rankings - all discussion masters and grandmasters are also included.\nIt uses the forum posts HTML source available in [Meta Kaggle][1].\n\nUsing [Tf\/Idf](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf) from [sklearn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html), by treating all of a user's discussion posts as one document, words (and bigrams\/trigrams) common to all *shown* users will be downweighted or ignored; so the visualisations highlight words and phrases that are particular to each user.\nBeautifulSoup is used for parsing messages and cleaning is fairly minimal - for example source code in posts is left in (possible improvement: remove *&lt;code&gt;* tags).\n\n***NOTE:*** *Competition details (including forum posts) are not included in [Meta Kaggle][1] until* ***after the competition deadline***.\n\n### Directions\n\n&uarr; Use *Edit &rarr; Find in This Page...* or *CMD+F* to search words (top N per user are shown. e.g. search \"vowpal\")\n\n&rarr; Use the navigation bar to jump around.\n\n## Contents\n\n * [Read Forums](#Read-Forums)\n * [TfidfVectorizer](#TfidfVectorizer)\n * [User Reports](#User-Reports)\n * [Kaggle Writers 2D Semantic Space](#Kaggle-Writers-2D-Semantic-Space)\n\n<!--\n### Ideas for Improvement\n\n - use color function: e.g. words relating to metrics, sponsors, users get their own color.\n - better mapping of infrequent plural terms to the singular version\n - remove text within \"code\" tags?\n - count &lt;img&gt; tags (and others? links?) in posts\n - more user stats e.g. gold\/silver\/bronze counts - although these are available on the [main rankings page][5].\n\n### Done\n\n - custom tokenizing to include smileys :) (although smileys are already sideways - when they are printed vertically in the word cloud they end up upside down!)\n\n-->\n\n### Main customization settings are in the first cell - feel free to fork & edit, e.g. if you're ranked 101 ;-P\n\n\n[1]: https:\/\/www.kaggle.com\/kaggle\/meta-kaggle \"Meta Kaggle\"\n[2]: https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf\n[3]: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html \"TfidfVectorizer\"\n[4]: https:\/\/www.kaggle.com\/jtrotman\/wordclouds-of-competition-forums\n[5]: https:\/\/www.kaggle.com\/rankings?group=discussion\n[6]: https:\/\/www.kaggle.com\/search\n","cc582fc6":"Now plot it!","c7ca134c":"### See Also\n\n[Wordclouds of Competition Forums](https:\/\/www.kaggle.com\/jtrotman\/wordclouds-of-competition-forums)"}}