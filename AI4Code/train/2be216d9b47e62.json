{"cell_type":{"11d23864":"code","af5889c7":"code","ab01d262":"code","e0ca69da":"code","7a703af7":"code","2bdde86c":"code","d04e9b03":"code","b55ec7fc":"code","79cf1a3f":"code","1b3bc0ca":"code","25e7c0c2":"code","5e991b78":"code","0a1c2818":"code","21e4d632":"code","abfb050c":"code","1d58b2aa":"code","54e0dd70":"code","ddcd5743":"code","d76851dc":"code","a09fc66a":"code","43321996":"code","590e7179":"code","f622d3dd":"code","6302530f":"code","ae0b79a1":"code","b292b345":"code","ddb2a7c3":"code","49654b34":"code","f619e02c":"code","75c27c51":"code","8af9dcb9":"code","3afcfa23":"code","28a06556":"code","842b837c":"code","a298725c":"code","218aed42":"code","2fa97a7f":"code","dd3a853a":"code","ccc444bd":"code","12a470cb":"markdown","9ea796e8":"markdown","0eb96a13":"markdown","6e789006":"markdown","1969cc0a":"markdown","7e1ef229":"markdown","28dd0cc2":"markdown","3441ee3b":"markdown","933c283c":"markdown","873ca704":"markdown","52ffa7df":"markdown","d22761c2":"markdown","99786e5a":"markdown","41287364":"markdown","c0c2ab4e":"markdown","abde5fff":"markdown","a49a4c2d":"markdown","0eacc5c9":"markdown","0c3824c9":"markdown","88197710":"markdown","f5e1a10a":"markdown","074dfd8c":"markdown","3b137a9e":"markdown","88832478":"markdown","692e1ab9":"markdown","778d613f":"markdown","3d643e50":"markdown","0ca4b270":"markdown"},"source":{"11d23864":"import numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nfrom fastai.tabular import * \nfrom fastai import *\n\nimport os, shutil\nimport sys","af5889c7":"#https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/discussion\/150697#845512\nimport logging\nlogging.getLogger().setLevel(logging.NOTSET)\n# Notebook Settings\n\n# np.set_printoptions(threshold=sys.maxsize)","ab01d262":"kaggle_path = Path('.')\nkaggle_input_path = Path('\/kaggle\/input\/trends-assessment-prediction') # '\/kaggle\/input\/trends-assessment-prediction'\ncluster_input_path = Path('\/kaggle\/input\/trends-cluster-sfnc-groups')\nav_input_path = Path('\/kaggle\/input\/trends-av-probs-of-test-and-s2')\nimg_53000_path = Path('\/kaggle\/input\/fork-of-trends-image-features-53-100')\n\n#https:\/\/www.kaggle.com\/mks2192\/trends-cluster-sfnc-groups\/output\n#for dirname, _, filenames in os.walk(kaggle_input_path):\n#    print(dirname)#, filenames)","e0ca69da":"INCLUDE_FNC_DATA = True\n\nINCLUDE_FNC_CLUSTERS = False\nINCLUDE_FNC_DIST_CCENTER = False\n\nINCLUDE_FNC_CLUSTERS_2 = True\nINCLUDE_FNC_DIST_CCENTER_2 = True\n\nINCLUDE_IMG_53000 = False\n\nINCLUDE_AV_DATA = False\n\nIMPUTATION_STRAT = 'IGNORE_ON_TRAIN' # 'IGNORE_ON_TRAIN', 'MEAN' \n#LOSS_BASE = 'MIX' #'MSE' # 'MSE' # 'L1' 'MIX'\nLOSS_WEIGHTS = [0.3, 0.175, 0.175, 0.175, 0.175] #[0.2,0.2,0.2,0.2,0.2] #[0,0,0,0,1]#\nBS = 128\n\nDEP_VAR = ['age','domain1_var1','domain1_var2', 'domain2_var1', 'domain2_var2']","7a703af7":"l_data = pd.read_csv(kaggle_input_path\/'loading.csv').drop('IC_20',axis=1)\n\nif INCLUDE_FNC_DATA:\n    f_data = pd.read_csv(kaggle_input_path\/'fnc.csv')\n    l_data = l_data.merge(f_data, on='Id', how = 'inner')\n\nif INCLUDE_FNC_CLUSTERS:\n    c_data = pd.read_csv(cluster_input_path\/'sfnc_group_clusters.csv')\n    l_data = l_data.merge(c_data, on='Id', how = 'inner')\n\nif INCLUDE_FNC_DIST_CCENTER:\n    cc_data = pd.read_csv(cluster_input_path\/'sfnc_dist_to_cluster_center.csv')\n    l_data = l_data.merge(cc_data, on='Id', how = 'inner')\n    \n\nif INCLUDE_FNC_CLUSTERS_2:\n    c2_data = pd.read_csv(cluster_input_path\/'sfnc_group_clusters_2c.csv')\n    temp_col = []\n    for c in c2_data.columns:\n        if c != 'Id':\n            temp_col += [c+'_2']\n        else:\n            temp_col += [c]\n    c2_data.columns = temp_col\n    l_data = l_data.merge(c2_data, on='Id', how = 'inner')\n\nif INCLUDE_FNC_DIST_CCENTER_2:\n    cc2_data = pd.read_csv(cluster_input_path\/'sfnc_dist_to_cluster_center_2c.csv')\n    temp_col = []\n    for c in cc2_data.columns:\n        if c != 'Id':\n            temp_col += [c+'_2']\n        else:\n            temp_col += [c]\n    cc2_data.columns = temp_col\n    l_data = l_data.merge(cc2_data, on='Id', how = 'inner')\n    \n\nif INCLUDE_IMG_53000:\n    i_data = pd.read_csv(img_53000_path\/'train_features.csv')\n    i_data = i_data.append(pd.read_csv(img_53000_path\/'test_features.csv'))\n    l_data = l_data.merge(i_data, on='Id', how = 'inner')\n\n    \nif INCLUDE_AV_DATA:\n    av_data = pd.read_csv(av_input_path\/'test_s2_probs.csv')\n    l_data = l_data.merge(av_data, on='Id', how = 'inner')\n\n\n\ny_data = pd.read_csv(kaggle_input_path\/'train_scores.csv')\n\nidx_site2 = pd.read_csv(kaggle_input_path\/'reveal_ID_site2.csv')\n#submission = pd.read_csv(kaggle_input_path\/'sample_submission.csv')","2bdde86c":"#i_data.info()","d04e9b03":"display(y_data.head())\ndisplay(y_data.describe())\ny_data.shape","b55ec7fc":"display(l_data.tail())\ndisplay(l_data.describe()),\nl_data.shape","79cf1a3f":"y_data.hist()","1b3bc0ca":"if IMPUTATION_STRAT == 'IGNORE_ON_TRAIN':\n    ## will later ignore the value when executing the loss function\n    y_data = y_data.fillna(0)\nelse: #'MEAN'\n    y_data = y_data.fillna(y_data.mean())\n    \ny_data","25e7c0c2":"y_data.hist()\n\n# lots of imputed data on domain1_var1\/2","5e991b78":"train_df = l_data.merge(y_data, on='Id', how='inner').sort_values(by='Id').reset_index(drop = True)\nidx_train = train_df.pop('Id') \ntrain_df","0a1c2818":"test = l_data.merge(y_data, on='Id', how='outer', indicator = True)\ntest = test[test['_merge'] == 'left_only'].drop(['age',\n                                                 'domain1_var1', \n                                                 'domain1_var2',\n                                                 'domain2_var1',\n                                                 'domain2_var2',\n                                                 '_merge'], axis = 1).sort_values(by='Id').reset_index(drop = True)\nidx_test = test.pop('Id') \ntest","21e4d632":"for d in DEP_VAR:\n    y_data['bin_'+d] = pd.qcut(y_data[d].rank(method='first'), q=4, labels=False)\n\ny_data['bin_all'] = (y_data['bin_age']+\n                    y_data['bin_domain1_var1']*10+\n                    y_data['bin_domain1_var2']*100+\n                    y_data['bin_domain2_var1']*1000+\n                    y_data['bin_domain2_var2']*10000)\n\ny_data['bin_age_10'] = pd.qcut(y_data['age'].rank(method='first'), q=10, labels=False)\n\ny_data['bin_age_7'] = pd.qcut(y_data['age'].rank(method='first'), q=7, labels=False)\n\nif INCLUDE_AV_DATA:\n    y_data = y_data.merge(av_data[['Id','is_test_prob']])\n    y_data['bin_test_7'] = pd.qcut(y_data['is_test_prob'].rank(method='first'), q=7, labels=False)\n\ny_data","abfb050c":"def norm_absolute_error(preds, targs):\n    # variation of https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/metrics.py#L85\n    \"Normalized absolute error between `pred` and `targ`.\"\n    \n    ## use sign for 0 imputeted empty targets, so they wan't be evaluated\n    sg=targs.sign()\n    y=targs*sg\n        \n    pred, targ = flatten_check(preds*sg, y)\n    return torch.abs(targ - pred).sum() \/ targ.sum()\n\ndef weighted_nae(preds, targs, details = False):\n    \n    ## use sign for 0 imputeted empty targets, so they wan't be evaluated\n    if IMPUTATION_STRAT == 'IGNORE_ON_TRAIN':\n        sg = targs.float().sign()\n        x0 = preds[:,0].float()*sg[:,0]\n        x1 = preds[:,1].float()*sg[:,1]\n        x2 = preds[:,2].float()*sg[:,2]\n        x3 = preds[:,3].float()*sg[:,3]\n        x4 = preds[:,4].float()*sg[:,4]\n    else: # 'MEAN'\n        sg = 1\n        x0 = preds[:,0].float()\n        x1 = preds[:,1].float()\n        x2 = preds[:,2].float()\n        x3 = preds[:,3].float()\n        x4 = preds[:,4].float()\n            \n    y = targs.float()*sg\n    \n    return norm_absolute_error(x0,y[:,0]), \\\n           norm_absolute_error(x1,y[:,1]), \\\n           norm_absolute_error(x2,y[:,2]), \\\n           norm_absolute_error(x3,y[:,3]), \\\n           norm_absolute_error(x4,y[:,4]), \\\n           0.3 * norm_absolute_error(x0,y[:,0]) + \\\n           0.175 * norm_absolute_error(x1,y[:,1]) + \\\n           0.175 * norm_absolute_error(x2,y[:,2]) + \\\n           0.175 * norm_absolute_error(x3,y[:,3]) + \\\n           0.175 * norm_absolute_error(x4,y[:,4])\n\ndef plot_diff(y, y_truth):\n    y_df = pd.DataFrame(y.numpy())\n    y_df.columns = DEP_VAR\n    y_df = y_df.melt()\n    y_df['Id'] = y_df.index\n    \n    y_truth_df = pd.DataFrame(y_truth.numpy())\n    y_truth_df.columns = DEP_VAR\n    y_truth_df = y_truth_df.melt()\n    y_truth_df['Id'] = y_truth_df.index\n\n    plot_df = y_truth_df.merge(y_df, on=['variable', 'Id'], how='inner').drop('Id', axis = 1)\n    plot_df.columns = ['Category', 'Target', 'Prediction']\n\n    g = sns.relplot(x=\"Target\", y=\"Prediction\",\n                  col=\"Category\", hue=\"Category\", style=\"Category\",\n                  kind=\"scatter\", data=plot_df)\n","1d58b2aa":"# variation of https:\/\/www.kaggle.com\/iafoss\/grapheme-fast-ai-starter-lb-0-964\n\nclass Metric_idx(Callback):\n    def __init__(self, idx):\n        super().__init__()\n        self.idx = idx\n        \n    def on_epoch_begin(self, **kwargs):\n        self.targs, self.preds = Tensor([]), Tensor([])\n    \n    def on_batch_end(self, last_output:Tensor, last_target:Tensor, **kwargs):\n        last_output = last_output[self.idx]\n        last_target = last_target[self.idx]\n        \n        self.preds = torch.cat((self.preds, last_output.float().cpu()))\n        self.targs = torch.cat((self.targs, last_target.float().cpu()))\n        \n    def _norm_absolute_error(self):\n        return norm_absolute_error(self.preds, self.targs)\n    \n    def on_epoch_end(self, last_metrics, **kwargs): \n        return add_metrics(last_metrics, self._norm_absolute_error())\n\n    \nMetric_age = partial(Metric_idx,0)\nMetric_domain1_var1 = partial(Metric_idx,1)\nMetric_domain1_var2 = partial(Metric_idx,2)\nMetric_domain2_var1 = partial(Metric_idx,3)\nMetric_domain2_var2 = partial(Metric_idx,4)\n\nclass Metric_total(Callback):\n    def __init__(self):\n        super().__init__()\n        self.age = Metric_idx(0)\n        self.domain1_var1 = Metric_idx(1)\n        self.domain1_var2 = Metric_idx(2)\n        self.domain2_var1 = Metric_idx(3)\n        self.domain2_var2 = Metric_idx(4)\n        \n    def on_epoch_begin(self, **kwargs):\n        self.age.on_epoch_begin(**kwargs)\n        self.domain1_var1.on_epoch_begin(**kwargs)\n        self.domain1_var2.on_epoch_begin(**kwargs)\n        self.domain2_var1.on_epoch_begin(**kwargs)\n        self.domain2_var2.on_epoch_begin(**kwargs)\n        \n    \n    def on_batch_end(self, last_output:Tensor, last_target:Tensor, **kwargs):\n        self.age.on_batch_end(last_output, last_target, **kwargs)\n        self.domain1_var1.on_batch_end(last_output, last_target, **kwargs)\n        self.domain1_var2.on_batch_end(last_output, last_target, **kwargs)\n        self.domain2_var1.on_batch_end(last_output, last_target, **kwargs)\n        self.domain2_var2.on_batch_end(last_output, last_target, **kwargs)\n \n        \n    def on_epoch_end(self, last_metrics, **kwargs): \n        return add_metrics(last_metrics, \n                           0.3 * self.age._norm_absolute_error() +\n                           0.175*self.domain1_var1._norm_absolute_error()  +\n                           0.175*self.domain1_var2._norm_absolute_error()  +\n                           0.175*self.domain2_var1._norm_absolute_error()  +\n                           0.175*self.domain2_var2._norm_absolute_error()\n                          )","54e0dd70":"# variation of https:\/\/www.kaggle.com\/iafoss\/grapheme-fast-ai-starter-lb-0-964\n\nclass Loss_combine(nn.Module):\n    def __init__(self, loss_weights = [0.4,0.15,0.15,0.15,0.15], loss_base = 'MSE'):\n        super().__init__()\n        \n        self.loss_base = loss_base\n        \n        self.loss_weights = loss_weights\n        self.fw = Tensor(LOSS_WEIGHTS).cuda()\n          \n            \n    def forward(self, input, target,reduction='mean'): #mean\n        \n        x0,x1,x2,x3,x4 = input.T\n        \n        ## use sign for 0 imputeted empty targets, so they wan't be evaluated\n        if IMPUTATION_STRAT == 'IGNORE_ON_TRAIN':\n            sg = target.float().sign()\n            x0,x1,x2,x3,x4 = x0.float()*sg[:,0],x1.float()*sg[:,1],x2.float()*sg[:,2],x3.float()*sg[:,3],x4.float()*sg[:,4]\n        else: # 'MEAN'\n            sg = 1\n            x0,x1,x2,x3,x4 = x0.float(),x1.float(),x2.float(),x3.float(),x4.float()\n            \n        y = target.float()*sg\n        \n        loss1 = 0\n        loss2 = 0\n        if self.loss_base in ('MSE','MIX'):\n            loss_func = F.mse_loss \n            #reduction = 'sum'\n            #loss1 = self.fw[0]*loss_func(x0,y[:,0],reduction=reduction)\/sum(y[:,0]**2) + \\\n            #   self.fw[1]*loss_func(x1,y[:,1],reduction=reduction)\/sum(y[:,1]**2) + \\\n            #   self.fw[2]*loss_func(x2,y[:,2],reduction=reduction)\/sum(y[:,2]**2) + \\\n            #   self.fw[3]*loss_func(x3,y[:,3],reduction=reduction)\/sum(y[:,3]**2) + \\\n            #   self.fw[4]*loss_func(x4,y[:,4],reduction=reduction)\/sum(y[:,4]**2)\n            \n            loss1 = (self.fw*(F.mse_loss(input*sg,y,reduction='none').sum(dim=0))\/((y**2.5).sum(dim=0)))*3\n            loss1 = loss1.sum()\n        \n        if self.loss_base in ('L1','MIX'):\n            loss_func = F.l1_loss \n            #reduction = 'mean'\n            #loss2 =  self.fw[0]*loss_func(x0,y[:,0],reduction=reduction) + \\\n            #   self.fw[1]*loss_func(x1,y[:,1],reduction=reduction) + \\\n            #   self.fw[2]*loss_func(x2,y[:,2],reduction=reduction) + \\\n            #   self.fw[3]*loss_func(x3,y[:,3],reduction=reduction) + \\\n            #   self.fw[4]*loss_func(x4,y[:,4],reduction=reduction)\n            \n            reduction = 'sum'\n            loss2 =  self.fw[0]*loss_func(x0,y[:,0],reduction=reduction)\/sum(y[:,0]) + \\\n               self.fw[1]*loss_func(x1,y[:,1],reduction=reduction)\/sum(y[:,1]) + \\\n               self.fw[2]*loss_func(x2,y[:,2],reduction=reduction)\/sum(y[:,2]) + \\\n               self.fw[3]*loss_func(x3,y[:,3],reduction=reduction)\/sum(y[:,3]) + \\\n               self.fw[4]*loss_func(x4,y[:,4],reduction=reduction)\/sum(y[:,4])\n            loss2 = loss2\/4\n\n        return loss1 + loss2 #\/100\n","ddcd5743":"class TailBlock(nn.Module):\n    \n    def __init__(self, bs):\n        super(TailBlock, self).__init__()\n        \n        self.bs = bs\n        self.lstm = nn.GRU(input_size = 53, hidden_size = 20, num_layers = 64, bidirectional=False, dropout = 0.3) # input, layers\n        self.linear = nn.Linear(40, 8)\n        \n        # init hidden cells\n        #self.hidden_cell = (torch.zeros(64,bs, 20).cuda(), \n        #                    torch.zeros(64,bs, 20).cuda())  #h_n, c_n => num_layers, batch_size, hidden_dim\n        self.hidden_cell = torch.zeros(128,bs, 20).cuda() #,\n                           # torch.zeros(128,bs, 20).cuda())\n\n    def forward(self,x_cont):# x_cat, x_cont):\n        print(x_cont.size())\n        #print(x_cat.size())\n        \n        x_non_lstm_feat = x_cont[:,:-5300]\n        \n        x53 = x_cont[:,-5300:].view(-1, 100, 53)\n        print('x53',x53.shape)\n        lstm_out, self.hidden_cell = self.lstm(x53.view(100, self.bs, -1), self.hidden_cell)\n        print('lout',lstm_out.shape)\n        x_lstm_out  = self.linear(torch.cat([x_non_lstm_feat.view(1,-1) , x_lstm_out.view(1,-1)], dim=1)) #lstm_out[-1].view(self.bs, -1))\n        \n        #print(torch.cat([x_non_lstm_feat , x_lstm_out.view(-1)], dim=1).shape)\n        #return x_cat, \n        return x_lstm_out #,torch.cat([x_non_lstm_feat.view(1,-1) , x_lstm_out.view(1,-1)], dim=1)","d76851dc":"class NeckBlock(nn.Module):\n    \n    def __init__(self, nf, ps=0.3):\n        super(NeckBlock,self).__init__()\n        \n        self.sa = SelfAttention(256)\n        \n        # age\n        self.bn1 = nn.BatchNorm1d(nf)\n        self.d1 = nn.Dropout(ps)\n        self.l1 = nn.Linear(nf, 1)\n        #self.l1b = nn.Linear(nf, 1)\n        self.act1 = nn.LeakyReLU(0.1, inplace=True)\n        \n        # dom_var\n        self.bn2 = nn.BatchNorm1d(nf)\n        self.d2 = nn.Dropout(ps)\n        self.l2 = nn.Linear(nf, 4)\n        self.act2 = nn.LeakyReLU(0.1, inplace=True)\n        \n        #self.bn2b = nn.BatchNorm1d(nf)\n        #self.d2b = nn.Dropout(0.3)\n        #self.l2b = nn.Linear(nf, 4)\n        #self.act2b = nn.LeakyReLU(0.1, inplace=True)\n        \n    def forward(self, x):\n        ident = x\n        \n        x0 = self.sa(x)\n        \n        x1 = self.bn1(x0)\n        x1 = self.d1(x1)\n        x1 = self.l1(x1)\n        #x1 = self.l1b(x1)\n        x1 = self.act1(x1+ident)\n        \n        x2 = self.bn2(x0) #+x1\n        x2 = self.d2(x2)\n        x2 = self.l2(x2)\n        #x2 = self.l2b(x2)\n        x2 = self.act2(x2+ident)\n               \n        return x1, xn\n        \n","a09fc66a":"def prep_data(bs=128, valid_idx=range(200, 400), train_df = train_df):\n    procs = [FillMissing, Categorify, Normalize]\n    \n    cat_names = []\n    if INCLUDE_FNC_CLUSTERS:\n        cat_names = cat_names + list(set(c_data.columns)-set(['Id']))\n\n    if INCLUDE_FNC_CLUSTERS_2:\n        cat_names = cat_names + list(set(c2_data.columns)-set(['Id']))\n    \n    \n    cont_names = list(set(train_df.columns) - set(cat_names) - set(['age','domain1_var1','domain1_var2', 'domain2_var1', 'domain2_var2'])-set(DEP_VAR))\n    tlist = (TabularList.from_df(train_df, \n                                path=kaggle_path, \n                                cat_names=cat_names, \n                                cont_names=cont_names, \n                                procs=procs))\n\n    if valid_idx == None:\n        tlist = tlist.split_none()\n    else:\n        tlist = tlist.split_by_idx(valid_idx)\n\n    data = (tlist.label_from_df(cols=DEP_VAR)\n                 .add_test(TabularList.from_df(test, \n                                               cat_names=cat_names,\n                                               cont_names=cont_names, \n                                               procs = procs))\n                 .databunch(path = kaggle_path, bs = bs))\n    \n    return data\n\n\ndef prep_learn(data, loss_base = 'MSE'):\n    \n    learn = tabular_learner(data, \n                        #! only one hidden 8 neuron layer works best\n                        layers = [8], #[64,8], # [256,128,256,128,64], #[1024,1024,128,1024,128,1024,1024],#[8], #\n                        # Drop Out 0.3 works best\n                        ps = 0.3, #.4\n                        loss_func = Loss_combine(loss_weights = LOSS_WEIGHTS,  loss_base= loss_base),\n                        metrics=[Metric_age(),\n                                 Metric_domain1_var1(),\n                                 Metric_domain1_var2(),\n                                 Metric_domain2_var1(),\n                                 Metric_domain2_var2(),\n                                 Metric_total()],\n                       # not sure if y_range is applied when using the NeckBlock module as head\n                       y_range=(Tensor([12,12,0,0,0]).cuda(),Tensor([90,90,100,100,100]).cuda())\n                       #y_range=(Tensor([0,0,0,0,0]).cuda(),Tensor([1,1,1,1,1]).cuda())\n                       #y_range=(Tensor([12,12,0,0,0]).cuda(),Tensor([90,90,100,100,100]).cuda())     \n                       )#.to_fp16()\n    \n    learn.clip_grad = 1.0\n    # adding head\/neck module\n    learn.model.layers[4].add_module('NeckBlock', NeckBlock(5, 0.4))\n    # learn.model = nn.Sequential(TailBlock(1),learn.model)\n    # learn.model.bn_cont.add_module('TailBlock', TailBlock(1))\n    #learn.model.layers[0].add_module('TailBlock', TailBlock(1))\n    #learn.model.cuda()\n    return learn\n    ","43321996":"#train500= train_df.head(500)#","590e7179":"#train500.shape","f622d3dd":"bs=128\nvalid_idx=range(200, 400)\n\ndata = prep_data(bs, valid_idx, train_df=train_df)\nlearn = prep_learn(data)","6302530f":"#learn.model #summary()","ae0b79a1":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","b292b345":"lr = 2e-2\nreduceLR = callbacks.ReduceLROnPlateauCallback(learn=learn, monitor = 'valid_loss', mode = 'auto', patience = 2, factor = 0.2, min_delta = 0)\nlearn.fit_one_cycle(10, lr, callbacks=[reduceLR])","ddb2a7c3":"yv, yv_truth= learn.get_preds(ds_type=DatasetType.Valid)\nyt, yt_truth= learn.get_preds(ds_type=DatasetType.Train)\n\nprint(f'Without augmentation:')\nprint(f'Weighted normalized absolute error (Valid): {weighted_nae(yv,yv_truth)[-1]}')\nfor i, dv in enumerate(DEP_VAR):\n    print(f'NAE {dv} (Valid): {weighted_nae(yv,yv_truth)[i]}')\nplot_diff(yv, yv_truth)","49654b34":"print(f'Weighted normalized absolute error (Train): {weighted_nae(yt,yt_truth)[-1]}')\nfor i, dv in enumerate(DEP_VAR):\n    print(f'NAE {dv} (Train): {weighted_nae(yt,yt_truth)[i]}')\nplot_diff(yt, yt_truth)","f619e02c":"LOSS_WEIGHTS = [.3, .175, .175, .175, .175] #[.4, .15, .15, .15, .15] #[1,0,0,0,0,] #\n\nkf_split = 7\nkf = StratifiedKFold(n_splits=kf_split, shuffle=True, random_state=2020)\ny_oof = None\ny_truth_oof = None\ny_oof_g = None\ny_truth_oof_g = None\nval_ids = []\n\nfor i, (train_id, val_id) in enumerate(kf.split(train_df,y_data['bin_age_7'].values)):\n    print('Fold #:',i)\n    data = prep_data(BS, val_id.tolist())\n    learn = prep_learn(data, loss_base = 'MSE')\n    \n    lr = 4e-2\n    reduceLR = callbacks.ReduceLROnPlateauCallback(learn=learn, monitor = 'valid_loss', mode = 'auto', patience = 2, factor = 0.2, min_delta = 0)\n    learn.fit_one_cycle(10, lr, callbacks=[reduceLR]) \n    \n    val_ids = val_ids+val_id.tolist()\n\n    yv, yv_truth= learn.get_preds(ds_type=DatasetType.Valid)\n    if y_oof == None:\n        y_oof = yv\n        y_truth_oof = yv_truth\n    else:\n        y_oof=torch.cat((y_oof,yv),0)\n        y_truth_oof=torch.cat((y_truth_oof,yv_truth),0)\n\n    \n    #print(f'Weighted normalized absolute error (Valid): {weighted_nae(yv,yv_truth)[-1]}')\n    #for i, dv in enumerate(DEP_VAR):\n    #    print(f'NAE {dv} (Valid): {weighted_nae(yv,yv_truth)[i]}')\n    #plot_diff(yv, y_truthv)\n\nprint(f'### Total:')\nprint(f'Weighted normalized absolute error (Valid): {weighted_nae(y_oof,y_truth_oof)[-1]}')\nfor i, dv in enumerate(DEP_VAR):\n    print(f'NAE {dv} (Valid): {weighted_nae(y_oof,y_truth_oof)[i]}')\nplot_diff(y_oof, y_truth_oof) # 6: 155","75c27c51":"#pd.DataFrame((y_oof+torch.tensor([9,16,19,11,12])-y_truth_oof).numpy()).hist()\n\noffset=torch.tensor([.3,.3,.8,.8,.8])\n\nprint(f'### Total:')\nprint(f'Weighted normalized absolute error (Valid): {weighted_nae(y_oof+offset,y_truth_oof)[-1]}')\nfor i, dv in enumerate(DEP_VAR):\n    print(f'NAE {dv} (Valid): {weighted_nae(y_oof+offset,y_truth_oof)[i]}')\nplot_diff(y_oof+offset, y_truth_oof)","8af9dcb9":"def gather_preds(learn, ignore_vars=[], offset = torch.ones(5)):\n    preds = learn.get_preds(ds_type=DatasetType.Test)[0]\n    preds = preds + offset\n    \n    submission=None\n\n    rec_flatt = pd.DataFrame(idx_test)\n    rec_flatt.columns=['Id']\n    \n    for t, tcol in enumerate(DEP_VAR):\n        if tcol not in ignore_vars:\n            rec = pd.DataFrame(idx_test)\n            rec['Id'] = rec['Id'].astype(str)+'_'+tcol\n            rec['Predicted'] = preds[:,t]\n            if isinstance(submission, pd.DataFrame):\n                submission = submission.append(rec)\n            else:\n                submission = rec\n            \n        rec_flatt[tcol] = preds[:,t]\n            \n    return submission, rec_flatt\n\n\ndef make_submission(gathered_preds, filename = ''):\n    #submission = gathered_preds.sort_values('Id').reset_index(drop=True)\n    submission = gathered_preds.groupby('Id').mean().sort_values('Id').reset_index(drop=False)\n    print('Shape of submission:', submission.shape)\n    display(submission.head(10))\n    submission.to_csv(filename, index=False)","3afcfa23":"data = prep_data(BS, valid_idx = None)\nlearn = prep_learn(data)\n\nlr = 4e-2\nlearn.fit_one_cycle(10, lr)","28a06556":"gath_pred_no_split, _ = gather_preds(learn)\nmake_submission(gath_pred_no_split, filename = 'submission_wo_split.csv')","842b837c":"LOSS_WEIGHTS = [.3, .175, .175, .175, .175]\n\nkf_split = 10\nkf = StratifiedKFold(n_splits=kf_split, shuffle=True, random_state=2020)\ny_oof = None\ny_truth_oof = None\nval_ids = []\ngathered_preds = None\ngathered_preds_flat = None\n\nfor i, (train_id, val_id) in enumerate(kf.split(train_df,y_data['bin_age_7'].values)):    \n    print('Fold #:',i)\n    data = prep_data(BS, val_id.tolist())\n    learn = prep_learn(data, loss_base = 'MIX')\n\n    lr = 4e-2\n    reduceLR = callbacks.ReduceLROnPlateauCallback(learn=learn, monitor = 'valid_loss', mode = 'auto', patience = 2, factor = 0.2, min_delta = 0)\n    learn.fit_one_cycle(10, lr, callbacks=[reduceLR])\n\n    val_ids = val_ids+val_id.tolist()\n\n    yv, yv_truth= learn.get_preds(ds_type=DatasetType.Valid)\n    if y_oof == None:\n        y_oof = yv\n        y_truth_oof = yv_truth\n    else:\n        y_oof=torch.cat((y_oof,yv),0)\n        y_truth_oof=torch.cat((y_truth_oof,yv_truth),0)\n    \n    g_preds, g_preds_flat = gather_preds(learn) #, ignore_vars=['age'])\n    if isinstance(gathered_preds, pd.DataFrame):\n        gathered_preds = gathered_preds.append(g_preds)\n        gathered_preds_flat = gathered_preds_flat.append(g_preds_flat)\n    else:\n        gathered_preds = g_preds\n        gathered_preds_flat = g_preds_flat\n\n\ny_oof_1 = y_oof\ny_truth_oof_1 = y_oof\nval_ids_1 = val_ids\nprint(f'K-Fold:',kf_split)\nprint(f'Weighted normalized absolute error (Valid): {weighted_nae(y_oof,y_truth_oof)[-1]}')\nfor i, dv in enumerate(DEP_VAR):\n    print(f'NAE {dv} (Valid): {weighted_nae(y_oof,y_truth_oof)[i]}')\nplot_diff(y_oof, y_truth_oof)\n\ngathered_preds_1 = gathered_preds\ngathered_preds_flat_1 = gathered_preds_flat\n\nmake_submission(gathered_preds_1.sort_values('Id').reset_index(drop=True), filename = 'submission_10fold_MIX.csv')\nmake_submission(gathered_preds_flat_1.sort_values('Id').reset_index(drop=True), filename = 'mix_test_preds_out.csv')","a298725c":"train_preds_out = pd.DataFrame(y_oof_1.numpy(), index=val_ids_1).sort_index()\ntrain_preds_out.columns = DEP_VAR\ntrain_preds_out['Id'] = idx_train\ntrain_preds_out.to_csv('mix_train_preds_out.csv', index = False)\n\ntrain_preds_out1 = train_preds_out.copy()\n\ntrain_preds_out1","218aed42":"LOSS_WEIGHTS = [.3, .175, .175, .175, .175]\n\noffset=torch.tensor([.3,.3,.8,.8,.8])\n\nkf_split = 10\nkf = StratifiedKFold(n_splits=kf_split, shuffle=True, random_state=2020)\ny_oof = None\ny_truth_oof = None\nval_ids = []\ngathered_preds = None\ngathered_preds_flat = None\n\nfor i, (train_id, val_id) in enumerate(kf.split(train_df,y_data['bin_age_7'].values)):    #bin_age_7\n\n    print('Fold #:',i)\n    data = prep_data(BS, val_id.tolist())\n    learn = prep_learn(data, loss_base = 'MSE')\n\n    lr = 4e-2\n    reduceLR = callbacks.ReduceLROnPlateauCallback(learn=learn, monitor = 'valid_loss', mode = 'auto', patience = 2, factor = 0.2, min_delta = 0)\n    learn.fit_one_cycle(10, lr, callbacks=[reduceLR]) #MixUpCallback(learn, alpha=0.6),\n\n    val_ids = val_ids+val_id.tolist()\n\n    yv, yv_truth= learn.get_preds(ds_type=DatasetType.Valid)\n    if y_oof == None:\n        y_oof = yv\n        y_truth_oof = yv_truth\n    else:\n        y_oof=torch.cat((y_oof,yv),0)\n        y_truth_oof=torch.cat((y_truth_oof,yv_truth),0)\n    \n    g_preds, g_preds_flat = gather_preds(learn, offset = offset) #, ignore_vars = ['domain1_var1','domain1_var2', 'domain2_var1', 'domain2_var2'])\n    if isinstance(gathered_preds, pd.DataFrame):\n        gathered_preds_flat = gathered_preds_flat.append(g_preds_flat)\n        gathered_preds = gathered_preds.append(g_preds)\n    else:\n        gathered_preds = g_preds\n        gathered_preds_flat = g_preds_flat\n\n        \ny_oof_2 = y_oof\ny_truth_oof_2 = y_oof\nval_ids_2 = val_ids\nprint(f'K-Fold with offset:',kf_split)\nprint(f'Weighted normalized absolute error (Valid): {weighted_nae(y_oof,y_truth_oof)[-1]}')\nfor i, dv in enumerate(DEP_VAR):\n    print(f'NAE {dv} (Valid): {weighted_nae(y_oof,y_truth_oof)[i]}')\nplot_diff(y_oof, y_truth_oof)\n\n\ngathered_preds_2 = gathered_preds\ngathered_preds_flat_2 = gathered_preds_flat\n\nmake_submission(gathered_preds_2.sort_values('Id').reset_index(drop=True), filename = 'submission_10fold_MSE_offset.csv')\nmake_submission(gathered_preds_flat_2.sort_values('Id').reset_index(drop=True), filename = 'mse_test_preds_out.csv')","2fa97a7f":"train_preds_out = pd.DataFrame(y_oof_2.numpy(), index=val_ids_2).sort_index()\ntrain_preds_out.columns = DEP_VAR\ntrain_preds_out['Id'] = idx_train\ntrain_preds_out.to_csv('mse_train_preds_out.csv', index = False)\n\ntrain_preds_out2 = train_preds_out.copy()\n\ntrain_preds_out2","dd3a853a":"gathered_preds_flat_1","ccc444bd":"offset = tensor([.3,.3,.3,.3,.3])\n\nprint(f'K-Fold blended')\nprint(f'Weighted normalized absolute error (Valid): {weighted_nae((y_oof_1 + y_oof_2)\/2 + offset, y_truth_oof)[-1]}')\nfor i, dv in enumerate(DEP_VAR):\n    print(f'NAE {dv} (Valid): {weighted_nae((y_oof_1 + y_oof_2)\/2 + offset, y_truth_oof)[i]}')\nplot_diff(y_oof, y_truth_oof)\n\n# offset\ngp1 = gathered_preds_1.copy()\ngp2 = gathered_preds_2.copy()\ngp1['Predicted']=gathered_preds_1['Predicted']+.3\ngp2['Predicted']=gathered_preds_2['Predicted']+.3\n\ngp_flat1 = gathered_preds_flat_1.copy()\ngp_flat2 = gathered_preds_flat_2.copy()\nfor dv in DEP_VAR:\n    gp_flat1[dv]=gathered_preds_flat_1[dv]+.3\n    gp_flat2[dv]=gathered_preds_flat_2[dv]+.3\n\nmake_submission(gp1.append(gp2).sort_values('Id').reset_index(drop=True), filename = 'submission_10kfold_blended.csv')\n\nmake_submission(gp_flat1.append(gp_flat2).sort_values('Id').reset_index(drop=True), filename = 'blended_test_preds_out.csv')\nmake_submission(train_preds_out1.append(train_preds_out1).sort_values('Id').reset_index(drop=True), filename = 'blended_train_preds_out.csv')\n","12a470cb":"## Combine Xs and Ys","9ea796e8":"# Prepare data","0eb96a13":"## Metrics","6e789006":"The unseen validation set fitting best on the age target. The metrics in the epoch stats table are different than the final scores. But the tendencies in all the experiments I run where the same, so I didn't investigate further.","1969cc0a":"## Let's run a first training","7e1ef229":"### K-Fold Playground","28dd0cc2":"Imputation strategie **IGNORE_ON_TRAIN**:\n- Some of the target values are empty. To ignore the empty one when calculating the loss the missing data is filled with 0 (0 is not in the target range). 0 is used later on as flag in the loss function as prediction to ignore.\n\nImputation strategie **MEAN**:\n- Impute missing targets with mean. They will be included in the loss function. This strategie is worse than IGNORE_ON_TRAIN.","3441ee3b":"### 10 fold MSE-loss + offset","933c283c":"## Loss function\nThe customized loss function which consideres five outputs is a variation of https:\/\/www.kaggle.com\/iafoss\/grapheme-fast-ai-starter-lb-0-964. It weights and combines the five single losses.\n\n***MSE*** works best. Best weights are 0.3, 4x0.175 (0.4 and 4x0.15 was promising too).","873ca704":"LB: 0.159\n\n## Credits\nMany ideas of my notebook are derived from this [notebook](https:\/\/www.kaggle.com\/iafoss\/grapheme-fast-ai-starter-lb-0-964#MixUp) from the Bengaliai competition earlier this year. Please go there and upvote if you find this or other references usefull.\nHere are the references in detail:\n- https:\/\/www.kaggle.com\/iafoss\/grapheme-fast-ai-starter-lb-0-964#MixUp: Multi head - metrics, - loss function\n\n- Schaefer 2018 -features: https:\/\/www.kaggle.com\/kpriyanshu256\/trends-image-features-1000\/output\n- Analysis of optimal Cluster Size: https:\/\/www.kaggle.com\/mks2192\/trends-cluster-sfnc-groups\n- Using Schaefer 2018-features as time series: https:\/\/www.kaggle.com\/kpriyanshu256\/trends-time-series","52ffa7df":"## Impute missing data","d22761c2":"10 Folds on ***bin_age_7*** with 10 epochs and max_lr = 4e-2 work the best.","99786e5a":"Routing time series features through an LSTM and bypassing the other feature to the linear model.\n\nTutorials to build LSTM in pytorch: https:\/\/stackabuse.com\/time-series-prediction-using-lstm-with-pytorch-in-python\/, https:\/\/www.jessicayung.com\/lstms-for-time-series-in-pytorch\/","41287364":"## Prepare validation splits\n\n### Bin y data for stratified k fold.\n\nThe targets are classified in about ***n*** equaly sized buckets. The first bucket containing the smallest values and the n^th bucket containing the highest values. Later on the stratified split will treat these buckets as classes, so every split gets about the same target distribution.\n\nA stratified k fold with 7 folds works best on ***bin_age_7***. ","c0c2ab4e":"The customized metric callback is a variation of https:\/\/www.kaggle.com\/iafoss\/grapheme-fast-ai-starter-lb-0-964. \nIt is used to keep track of the five single targets and the combined metric. ","abde5fff":"## Predict without split (cross validation)","a49a4c2d":"Define the dataloader (wrapped in a fastai databunch).\n\nDefine the fastai learner. Surprisingly the best model was just 1 hidden layer with 8 neurons! ps=0.3 (DropOut) is used for regularization. y_range defines the range bounderies for the predictions and has some positive effect on some configurations.","0eacc5c9":"### Total:\nWeighted normalized absolute error (Valid): 0.15890708565711975\nNAE age (Valid): 0.144345223903656\nNAE domain1_var1 (Valid): 0.15140166878700256\nNAE domain1_var2 (Valid): 0.15128813683986664\nNAE domain2_var1 (Valid): 0.18171468377113342\nNAE domain2_var2 (Valid): 0.17618706822395325\n\n### Total:\nWeighted normalized absolute error (Valid): 0.15910734236240387\nNAE age (Valid): 0.14428390562534332\nNAE domain1_var1 (Valid): 0.15133097767829895\nNAE domain1_var2 (Valid): 0.15144872665405273\nNAE domain2_var1 (Valid): 0.18138742446899414\nNAE domain2_var2 (Valid): 0.17767387628555298","0c3824c9":"# Model","88197710":"I think it's also good to have a look after predicting your train data. It's not saying anything about unseen data. But it gives you a feeling how over-\/underfit the model is.","f5e1a10a":"# Parameters","074dfd8c":"A neck or head module that separetes the \"route\" of one feature from four others. The idea is that ***age*** needs to be evaluated differntly than the ***domain_var*** values.\n\nIt only has a tiny impact on the model. I tried it as head and as neck. Head worked better. Not sure if it considered the target y_ranges (see **prep_learn**-definition below) if it is used as head.","3b137a9e":"### Best of blended","88832478":"Including all tabular data but IC_20 (removing or adding IC_20 doesn't influence the public LB).","692e1ab9":"# TReNDS - Tabular NN 0.159","778d613f":"## Predict K-Fold\n10 Folds on ***bin_age_7*** with 10 epochs and max_lr = 4e-2 work the best.\n\n### 10-fold MIX-loss","3d643e50":"## Build the model","0ca4b270":"# Prediction"}}