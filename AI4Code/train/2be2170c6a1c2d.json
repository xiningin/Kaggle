{"cell_type":{"400c020d":"code","b14f3f05":"code","0943f876":"code","ee5658dd":"code","a2978f54":"code","bdf766f0":"code","cda61933":"code","8507f24d":"code","5cd858b1":"code","3ca692ba":"code","8cf55657":"code","1066b262":"code","2f30b439":"code","ef19e76a":"code","11a64a94":"code","959fc08a":"code","54f62552":"code","020a205d":"code","f8de7a34":"code","08324a71":"code","5760abc2":"code","478afb60":"code","a7df5dd7":"code","fb92ef4c":"code","21cc19c9":"code","78f5dd4e":"code","63b6aa4d":"code","1c13727d":"code","1525cc65":"markdown","0385b961":"markdown","0dab6355":"markdown","6b406afc":"markdown","5270022a":"markdown","a4b9c07c":"markdown","90434811":"markdown","4f3f9667":"markdown","7219d2b6":"markdown","176f7126":"markdown","9a74d0a2":"markdown","f6687323":"markdown","4d41c64f":"markdown","bb0961d2":"markdown","7a475358":"markdown"},"source":{"400c020d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt","b14f3f05":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0943f876":"train = pd.read_csv(\"\/kaggle\/input\/fake-news\/train.csv\")\ntrain","ee5658dd":"test = pd.read_csv(\"\/kaggle\/input\/fake-news\/test.csv\")\ntest","a2978f54":"sample = pd.read_csv(\"\/kaggle\/input\/fake-news\/submit.csv\")\nsample","bdf766f0":"target_count = train.groupby('label').label.count()\ntarget_count","cda61933":"percent_target = (target_count \/ len(train)) * 100\npercent_target","8507f24d":"train.groupby('label').label.count().plot.bar(ylim=0)\nplt.show()","5cd858b1":"target = train.label\ntrain.drop(['label'], axis=1, inplace=True)\ntrain","3ca692ba":"test_id = test.id\n\ntrain_len = len(train)\n\nframes = [train, test]\n\ncombo = pd.concat(frames)\ncombo.drop(['id'], axis=1, inplace=True)\ncombo","8cf55657":"combo.isnull().sum()","1066b262":"combo = combo.fillna(\"Not Listed\")\ncombo.isnull().sum()","2f30b439":"combo","ef19e76a":"# Importing HTMLParser\nfrom html.parser import HTMLParser\nhtml_parser = HTMLParser()","11a64a94":"# Created a new columns i.e. clean_tweet contains the same tweets but cleaned version\ncombo['processedtext'] = combo['text'].apply(lambda x: html_parser.unescape(x))","959fc08a":"import nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nstemmer = PorterStemmer()\nwords = stopwords.words(\"english\")\n\ncombo['processedtext'] = combo['processedtext'].apply(lambda x: \" \".join([stemmer.stem(i) \nfor i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n","54f62552":"#make all words lower case\ncombo['processedtext'] = combo['processedtext'].str.lower()\n\n# remove special characters, numbers, punctuations\ncombo['processedtext'] = combo['processedtext'].str.replace(\"[^a-zA-Z#]\", \" \")\n\n#remove words less than 3 characters\ncombo['processedtext'] = combo['processedtext'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","020a205d":"fake_words = ' '.join([text for text in combo['processedtext']])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(fake_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","f8de7a34":"#define X and y\ny = target\nX_combo = combo['processedtext']","08324a71":"X = X_combo[: train_len]\nX_test = X_combo[train_len:]","5760abc2":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\nX_tfIdf = vectorizer_tfidf.fit_transform(X.values.astype('U'))\nX_test = vectorizer_tfidf.transform(X_test.values.astype('U'))\nprint(vectorizer_tfidf.get_feature_names()[:10])","478afb60":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X_tfIdf, y, test_size=0.10, random_state=1, shuffle=True)\nX_train.shape, X_val.shape, y_train.shape,y_val.shape, X_test.shape","a7df5dd7":"from sklearn.linear_model import PassiveAggressiveClassifier\n\nmodel = PassiveAggressiveClassifier(max_iter=10000, random_state=1,tol=1e-3).fit(X_train, y_train)\nprint(model.score(X_train, y_train))","fb92ef4c":"y_pred = model.predict(X_val)\nprint(model.score(X_val, y_val))","21cc19c9":"from sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_val,y_pred))","78f5dd4e":"prediction = model.predict(X_test)","63b6aa4d":"output = pd.DataFrame({'id': test_id, 'label': prediction})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","1c13727d":"#upload submission\nsubmission = pd.read_csv(\"submission.csv\")\nsubmission","1525cc65":"Problem statement\n\nDevelop a machine learning program to identify when an article might be fake news. Run by the UTK Machine Learning Club.\n\ntrain.csv: A full training dataset with the following attributes:\n\nid: unique id for a news article\n\ntitle: the title of a news article\n\nauthor: author of the news article\n\ntext: the text of the article; could be incomplete\n\nlabel: a label that marks the article as potentially unreliable\n\n1: unreliable\n\n0: reliable\n\ntest.csv: A testing training dataset with all the same attributes at train.csv without the label.\n\nsubmit.csv: A sample submission that you can","0385b961":"Make submission","0dab6355":"Split X for training and validation","6b406afc":"Preprocess raw data and get ready for machine learning","5270022a":"Select model","a4b9c07c":"Load files","90434811":"Check for null values","4f3f9667":"Confusio matrix","7219d2b6":"Predict on test set","176f7126":"Analyse label","9a74d0a2":"Predict on validation set","f6687323":"Replace values with \"Not listed\"","4d41c64f":"Convert text to word frequency vectors","bb0961d2":"Define X and y","7a475358":"Merge train and test"}}