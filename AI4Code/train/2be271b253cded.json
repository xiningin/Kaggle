{"cell_type":{"1a51e19f":"code","a049d25c":"code","e271667c":"code","3f4b0cd0":"code","47bf58a4":"code","009fae79":"code","1b536332":"code","414de66a":"code","986c142e":"code","f3232429":"code","9a65375e":"code","6a06a07b":"markdown","cce1b201":"markdown","efd1263a":"markdown","0908e275":"markdown","e3e09faa":"markdown","e33557d2":"markdown","b637ac05":"markdown","b771b5f4":"markdown","c533ae51":"markdown","6d6b8c3a":"markdown","6d00ad84":"markdown","83695c29":"markdown","8d32bc97":"markdown"},"source":{"1a51e19f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport random\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a049d25c":"train=pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntrain.head()","e271667c":"train=pd.get_dummies(train,columns=['label'])\nX=train[train.columns[:-10]]\nY=train[train.columns[-10:]]","3f4b0cd0":"print('minimuim value {} \\nmaximum value {}'.format(train[:-10].to_numpy().min(),train[:-10].to_numpy().max()))","47bf58a4":"#let's see how many value is different than 0\n(X>0).sum().sum()\/X.size","009fae79":"#scale the data\nX=X\/255","1b536332":"# since we gonna use Sigmoid as our activation , also we need the sgmoid prime for the back propgation\ndef sigmoid(z):\n    return 1.0\/(1.0+ np.exp(-z))\ndef sigmoid_prime(z):\n    return sigmoid(z)*(1- sigmoid(z))","414de66a":"import random\n# Third -party libraries\nimport numpy as np\ndef cost( output_activations , y):\n    return ( (output_activations -y))\nclass Network():\n    def __init__(self , sizes):\n        #Build The network and initialize the weights and the Biases\n        self. num_layers = len(sizes)\n        self.sizes = sizes\n        self.biases = [np.random.randn(y,1) for y in sizes [1:]]\n        self.weights = [np.random.randn(y, x) for x, y in zip(sizes [:-1], sizes [1:])]\n    \n    def learn(self,training_data,epochs,mini_batch_size,eta,validation_data=None):\n        def evaluate(test):\n            Y=test[:,-10:]\n            X=test[:,:-10]\/255\n            test_results = [(np.argmax(self.feedforward (x)), np.argmax(y))  for (x, y) in zip(X,Y)]\n            return sum(int(x == y) for (x, y) in test_results )\n        \n        \n        n=len(training_data)\n        history=[]\n        for i in range(epochs):\n            random.shuffle(training_data)\n            mini_batches=[training_data[k:k+mini_batch_size] for k in range(0,n-mini_batch_size,mini_batch_size)]\n            for mini_batch in mini_batches:\n                self.backprop(mini_batch,eta)\n            if type(validation_data):\n                score=evaluate(validation_data)\/len(validation_data)\n                history.append((i,score))\n                print(\"Epoch {} validation_score {}\".format(i,score))\n        return history\n    # this for giving the result of given input   \n    def feedforward(self,X):\n        X=np.expand_dims(X,axis=1)\n        a=X\n        for w,b in zip(self.weights,self.biases):\n            z=np.dot(w,a)+b\n            a=sigmoid(z)\n        return a\n    def backprop(self,mini_batch,eta):\n        mini_batch_size=len(mini_batch)\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n\n        Y=mini_batch[:,-10:]\n        X=mini_batch[:,:-10]\/255\n        #forwardpass( same as forward pass above just store parameters z and a per layer during the pass)\n        for x,y in zip(X,Y):\n            y=np.expand_dims(y,axis=1)\n            x=np.expand_dims(x,axis=1)\n            act=x\n            acts=[x]\n            zs=[]\n            #storing while pass\n            for w,b in zip(self.weights,self.biases):\n                z=np.dot(w,act)+b\n                zs.append(z)\n                act=sigmoid(z)\n                acts.append(act)\n                \n            #backwardpass\n            # calculate the delta ,(the error)\n            delta=cost(acts[-1],y)*sigmoid_prime(zs[-1])\n            delta_b=[delta]\n            delta_w=[np.dot(delta,acts[-2].transpose())]\n            for l in range(2,self.num_layers):\n                z=zs[-l]\n                delta=np.dot(self.weights[-l+1].transpose(),delta)*sigmoid_prime(z)\n                delta_b.append(delta)\n                delta_w.append(np.dot(delta,acts[-l-1].transpose()))\n            nabla_b = [nb+dnb for nb ,dnb in zip(nabla_b , delta_b[::-1])]\n            nabla_w = [nw+dnw for nw ,dnw in zip(nabla_w , delta_w[::-1])]\n        #updating W and B \n        for j in range(len(self.weights)):\n            self.weights[j]-=(eta\/mini_batch_size)*nabla_w[j] \n            self.biases[j]-=(eta\/mini_batch_size)*nabla_b[j] \n        \n    def predict(self,data):\n        X=data\/255\n        test_results = [np.argmax(self.feedforward (x)) for x in X ]\n        return test_results\n        \n\n             \n        \n","986c142e":"#Define The Network and split the data (Naive way of splitting)\n# i defined naive neural network , feel free to add layers (just by adding other sizes etc ....)\nnet = Network ([784 , 30, 10])\nvalidation=train.iloc[-5000:,].to_numpy()\ntrain=train.iloc[:-5000,:].to_numpy()","f3232429":"history=net.learn(train,10,10,3,validation)","9a65375e":"#Evolution of The validation accuracy \nimport matplotlib.pyplot as plt\nplt.plot(*zip(*history))","6a06a07b":"![image.png](attachment:image.png)","cce1b201":"## Thanks for following (if you find it beneficial upvoting would be Nice) ","efd1263a":"### Gradient descent \nin order to minimize the loss function we'll use the famous gradient descent , which may seem daunting but it's easy (just people tend to use fancy vocabulary that makes it look difficult than it really is XDD), let's think of it in 1-d , \n                                    cost=f(w)  (f is our loss function) , \nwhat we'll do is just take the gradient of this function with respect to w (gradient is just fancy way to say derivative ), later we move our interecept in the direction of that gradient since it will indicate that the right way to reach our minimum (PS : loss function always converge to a minimum ) \nwell all what i said is explained in these 2 amazing video https:\/\/www.youtube.com\/watch?v=sDv4f4s2SB8 \nhttps:\/\/www.youtube.com\/watch?v=IHZwWFHWa-w\n\n\nAlso i found this good analogy from michael nielson book networks  and deep learning \", there is a beautiful analogy which suggests\nan algorithm which works pretty well. We start by thinking of our function as a kind of a\nvalley. If you squint just a little at the plot above, that shouldn\u2019t be too hard. And we imagine\na ball rolling down the slope of the valley. Our everyday experience tells us that the ball\nwill eventually roll to the bottom of the valley. Perhaps we can use this idea as a way to\nfind a minimum for the function? We\u2019d randomly choose a starting point for an (imaginary)\nball, and then simulate the motion of the ball as it rolled down to the bottom of the valley.\nWe could do this simulation simply by computing derivatives (and perhaps some second\nderivatives) of C \u2013 those derivatives would tell us everything we need to know about the\nlocal \u201cshape\u201d of the valley, and therefore how our ball should roll.\"\n\n\n\nPS : later on i'll be using one variant of gradient descent \"Mini Batch gradient descent\" , where i'll feedforward and backprogate one batch(protion of the Data) at a time and then update the parameters using the average of the gradients that i have https:\/\/towardsdatascience.com\/gradient-descent-algorithm-and-its-variants-10f652806a3 ","0908e275":"### Backpropagation \nwell this is kinda the tricky part of a neural network , the idea is to after calculating our loss we backpropgate it through the network , think of it as we take the loss as our input and reverse our network and then do a simple feedforward pass , this will give us the gradient of the cost with respect to the parameters of each layer , by using the Famous \"Chain Rule\", at the core of it this is backpropagation , i'll keep 2 videos dive in the math behind it https:\/\/www.youtube.com\/watch?v=Ilg3gGewQ5U ,https:\/\/www.youtube.com\/watch?v=tIeHLnjs5U8","e3e09faa":"![image.png](attachment:image.png)","e33557d2":"##### prepocessing the y variable (label) create dummies since later on we'll be predicting the probabilty of certain class not the class (in other way we'll be predicting the probabilty that certain image represent certain number not the number itself)","b637ac05":"### 1st let's import and discover the data","b771b5f4":"## The feedforward pass\nlet's say we have our initiliazed neural network , the feedforward pass is propgate the input from input layer to the output layer , this happen using linear algebra \n![image.png](attachment:image.png)\nZ=W*X+B , *:dot product ,W the weight matrix of the layer , b: the bias matrix --> activation=function(z) with function is our chosen activation function( sigmoid , relu , tanh ...), so technically what happens is the input of the layer is multiplied by the weight and added certain bias to it ,on microscopique scale what happens in the neurons is  as simple as linear equation,so what we really doing is just doing a lot of linear equation at simultaneously using linear algebra .Later  we pass the result Z to the activation function which output the input of the next layer etc \n\n","c533ae51":"### The loss calculation \nwhen we come to the output layer we'll have our predictions , in order to quantify our loss (how far our prediction from the True value) we use loss functions in this case i'll use the qudratic loss function \n![image.png](attachment:image.png)\nHere, w denotes the collection of all weights in the network, b all the biases, n is the total\nnumber of training inputs, a is the vector of outputs from the network when x is input, and\nthe sum is over all training inputs, x \nwe Should keep in mind that the final goal is minimize this loss function by changing the weights and biases of our netwrok , by minimizing this loss function we'll maximize our accuracy (our predictions are more and more closer to the True Values)","6d6b8c3a":"#### Prepare usefull functions we'll use later","6d00ad84":"\n![image.png](attachment:image.png)","83695c29":"#### just 19% of the data is different than 0 (expected since the MNIST contain a lot of white space) More importantly, in Neural Network updates, the weights corresponding to a 0 in the input are not going to be updated. You can see it experimentally by observing the evolution of the weights of your neural network after training (resp. W_begin and W_end) also the variables should be on a comparable scale That's why we should scale down the data (we can go with standarize the data we substract the mean and divide by the std we gonna end up by the data in range[-1,1] or we can just divide by the max data end up between [0,1] , There is no rule on how we should normalize ,depends on the data","8d32bc97":"#### To build any neural network we should understand how to feedforward the data and how to backpropagate it through the network if we do understand this we can build any neural network from scratch \n\n"}}