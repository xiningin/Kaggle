{"cell_type":{"dce4b786":"code","e6bcb9dc":"code","45514542":"code","8b5f10ab":"code","49fdd2f0":"code","8a8983f7":"code","c891c7a1":"code","681084fa":"code","5bf49ccb":"code","9da70328":"code","98f71018":"code","7554be1e":"code","12511e6d":"code","ea8931f6":"code","d859e882":"code","88ae53d4":"code","59c01567":"code","1ec47e1b":"code","1016e4df":"code","b753792c":"code","034af278":"code","f872eb0c":"code","e29a8e39":"code","661977a0":"code","8412ddb2":"code","98b46b5f":"code","656c04a6":"code","6d4d00e7":"code","ff9ed376":"code","77ec5f70":"code","e429711b":"code","a3034360":"code","f056a962":"code","c6b1865e":"code","2d525bbf":"code","5cc7b3d1":"code","399cbf1d":"code","a6bf9257":"code","85532e7d":"code","c2e2eaac":"code","863eb5e8":"markdown","e2cf3a7d":"markdown","b14ad52b":"markdown","31b33602":"markdown","8d00ef0d":"markdown","72fe1915":"markdown","185061c0":"markdown","ecc12ce7":"markdown","dbda0e10":"markdown","3a7faa61":"markdown","4c6a79a0":"markdown","4025e3ef":"markdown","eea9cfca":"markdown","1c9c3e90":"markdown","446501ac":"markdown","5875a4af":"markdown","542e3fb0":"markdown","eccc4010":"markdown","bf964d7a":"markdown","d5a83a33":"markdown","8e26c0ed":"markdown","bb954847":"markdown","02d7306e":"markdown","84c8e940":"markdown","a763066d":"markdown","d4f0e476":"markdown"},"source":{"dce4b786":"# feature extractoring and preprocessing data\n%matplotlib inline\nimport os\nimport pathlib\nimport csv\nimport cv2\nimport librosa\nimport librosa.display\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport IPython.display as ipd\n\n#Keras and Tensorflow\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.utils import np_utils\n# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler,MinMaxScaler,scale\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import GridSearchCV, train_test_split, RepeatedStratifiedKFold, cross_val_score, KFold,StratifiedKFold \nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score,roc_curve,roc_auc_score, auc\n\nimport warnings\nwarnings.filterwarnings('ignore')","e6bcb9dc":"metadata_train_challenge = pd.read_csv('..\/input\/aicovidvn115m\/aicv115m_public_train\/aicv115m_public_train\/metadata_train_challenge.csv')\nprint('len metadata_train_challenge',len(metadata_train_challenge.iloc[:,-1]))\nmetadata_train_challenge.head(4)","45514542":"cough_trial_extended = pd.read_csv('..\/input\/coughclassifier-trial\/cough_trial_extended.csv')\nprint('len cough_trial_extended',len(cough_trial_extended.iloc[:,-1]))\ncough_trial_extended.head(4)","8b5f10ab":"header = 'filePath label'\nheader = header.split()\n\nfile = open('data_file_Path.csv', 'w')\nwith file:\n    writer = csv.writer(file)\n    writer.writerow(header)\ndata =[]\nfor i,label in enumerate(cough_trial_extended['class']):\n    if label =='not_covid':\n        label = '0'\n    else:\n        label = '1'\n    filename = cough_trial_extended.iloc[i ,0]\n    filePath = '..\/input\/coughclassifier-trial\/trial_covid\/' + str(filename)\n    data = [filePath, label]\n    file = open('data_file_Path.csv', 'a')\n    with file:\n        writer = csv.writer(file)\n        writer.writerow(data)\n","49fdd2f0":"data =[]\nfor i,label in enumerate(metadata_train_challenge['assessment_result']):\n    filename = metadata_train_challenge.iloc[i ,-1]\n    filePath = '..\/input\/aicovidvn115m\/aicv115m_public_train\/aicv115m_public_train\/train_audio_files_8k\/train_audio_files_8k\/' + str(filename)\n    data = [filePath, label]\n    file = open('data_file_Path.csv', 'a')\n    with file:\n        writer = csv.writer(file)\n        writer.writerow(data)\ndata = pd.read_csv('.\/data_file_Path.csv')\nprint ('len data', len(data.iloc[:,1]))\ndata.head(8)    ","8a8983f7":"Features = []\nNumber = len(data.iloc[:,1]) #Number of files I want to try on Number = len(data.iloc[:,1]) for all data\nfor file in data['filePath'][:Number]:\n\n    y,sr=librosa.load(file)\n    if librosa.get_duration(y=y, sr=sr) > 30:\n        y,sr=librosa.load(file, duration = 30)\n\n    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, S=None, n_fft=2048, \n                                              hop_length=512, win_length=None, window='hann', \n                                              center=True, pad_mode='reflect', power=2.0)\n    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr, S=None, norm=None, n_fft=2048, \n                                hop_length=512, win_length=None, window='hann', \n                                center=True, pad_mode='reflect', tuning=None, n_chroma=12)\n    MFCC = librosa.feature.mfcc(y=y, sr=sr, S=None, n_mfcc=20, dct_type=2, norm='ortho', lifter=0)\n    \n    ZCR = librosa.feature.zero_crossing_rate(y, frame_length=2048, hop_length=512, center=True)\n    \n    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, S=None, n_fft=2048, \n                                      hop_length=512, freq=None, win_length=None, window='hann', \n                                                          center=True, pad_mode='reflect')\n    \n    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr, S=None, n_fft=2048, \n                                       hop_length=512, win_length=None, window='hann', \n                                       center=True, pad_mode='reflect', freq=None, centroid=None, norm=True, p=2)\n    \n    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, S=None, n_fft=2048, \n                                     hop_length=512, win_length=None, window='hann', \n                                     center=True, pad_mode='reflect', freq=None, roll_percent=0.85)\n    feature = np.concatenate((mel_spec,chroma_stft,MFCC,ZCR,spectral_centroid,spectral_bandwidth,spectral_rolloff), \n                             axis=0)\n    feature = librosa.power_to_db(feature, ref=np.max)\n    Features.append(feature)\n    #print ('feature',feature.shape)\nprint ('Features',len(Features) )","c891c7a1":"plt.figure(num='Features',figsize=(9,9))\nfor i, img in enumerate(Features[:3]):\n    plt.subplot(3,1,i+1) \n    plt.title('Feature {}'.format(i))\n    #S_dB = librosa.power_to_db(img, ref=np.max)\n    #img = librosa.display.specshow(S_dB, x_axis='time',y_axis='mel', sr=sr,fmax=8000)        \n    plt.imshow(img)\n    plt.colorbar(format='%+2.0f dB')\n    plt.tight_layout(pad=3.0)\n    #plt.axis('off')    \nplt.show()","681084fa":"#header = 'features label'\n#header = header.split()\n#file = open('raw_features.csv', 'w')\n#with file:\n#    writer = csv.writer(file)\n#    writer.writerow(header)\n#value =[]\n#for i,feature in enumerate (Features):\n#    feature = feature.flatten()\n#    feature = ( \" \".join( str(e) for e in feature ) )  \n#    label  = data['label'][i]\n#   value = [feature, label]\n#    file = open('raw_features.csv', 'a')\n#    with file:\n#        writer = csv.writer(file)\n#        writer.writerow(value)\n#raw_feature = pd.read_csv('.\/raw_features.csv')\n#raw_feature.head(8)    ","5bf49ccb":"Time = np.array([x.shape[1] for x in Features])\nunique, counts = np.unique(Time, return_counts=True)\n#print (dict(zip(unique, counts)))\nmax_length = np.max(Time)\nprint ('max_length',max_length)\n\nwidth = 3  # the width of the bars\nplt.figure(num='Time',figsize=(9,9))\nplt.xlabel('Duration')\nplt.ylabel('Number of files')\n\nplt.title('time duration')\nplt.bar(unique,counts, width ,ec='blue')   \nplt.show()","9da70328":"\ndef preprocess(feature, featureSize):\n\n    widthTarget, heightTarget = featureSize \n    height, width = feature.shape \n\n    # scale according to factor\n    newSize = (int(width \/ 4),41) \n    #print ('newSize ={}, old size = {}'.format(newSize, feature.shape ))\n    feature = cv2.resize(feature, newSize)\n    # Normalization\n    feature = scaler.fit_transform(feature)\n    feature = np.pad(feature, ((0, 0), (0, widthTarget - feature.shape[1])), 'constant')\n    #transpose\n    feature = np.transpose(feature)\n    \n    return feature","98f71018":"scaler = StandardScaler()\n#print ('raw Features', Features[1])\nscale_features =[]\nfor feature in Features[:Number]:\n    feature = preprocess(feature,featureSize = (int(max_length\/4),41)) #41 = 164\/4\n    scale_features.append(feature)\n\n#print ('scale feature',scale_features[1])    \nplt.figure(num='Features transpose',figsize=(9,9))\n\nfor i, img in enumerate(scale_features[:6]):\n    plt.subplot(3,3,i+1) \n    plt.tight_layout(pad=3.0)\n    plt.title('Feature {}'.format(i))\n    plt.imshow(img)\n    #plt.axis('off')    \nplt.show()","7554be1e":"genre_list = data.iloc[:Number, -1]\n#print ('genre_list\\n',genre_list)\nencoder = LabelEncoder()\ny = encoder.fit_transform(genre_list) #G\u00e1n nh\u00e3n 0,1 cho class. C\u00f3 th\u1ec3 n\u00f3i l\u00e0 \u0111\u01b0a v\u1ec1 one hot coding\nneg, pos = np.bincount(y)\ntotal = neg + pos\nprint ('positive: {} ({:.2f}% of total) \\nnegative cases: {}'.format(pos, 100 * pos\/total ,neg))","12511e6d":"scale_features = np.array(scale_features).reshape(-1, int(max_length\/4), 41, 1)\nindices = range(len(scale_features))","ea8931f6":"x_train, x_test, y_train, y_test, indices_train,indices_test = train_test_split(scale_features, y, indices, test_size=0.15, shuffle = True, \n                                                    random_state = None, stratify = y)\n\nX_train, X_valid, Y_train, Y_valid,Indices_train,Indices_valid = train_test_split(x_train, y_train,indices_train, test_size=0.2, shuffle = True, \n                                                    random_state = None, stratify = y_train )\n\nY_train = np_utils.to_categorical(Y_train, 2)\nY_valid = np_utils.to_categorical(Y_valid, 2)\nprint (len(Y_valid))","d859e882":"print ('\\nlen(X_train)',len(X_train))\nprint ('len(X_valid)',len(X_valid))\nprint ('\\n X_train.shape',X_train.shape)\nprint ('\\n X_valid.shape',X_valid.shape)","88ae53d4":"def build_model(img_width = int(max_length\/4),img_height = 41):\n    # Inputs to the model\n\n    input_img = layers.Input(\n        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n    )\n    \n    # First conv block\n    x = layers.Conv2D(\n        64,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv1\",\n    )(input_img)\n    x = layers.MaxPooling2D((2, 2),strides = 2, name=\"pool1\")(x)\n\n    # Second conv block\n    x = layers.Conv2D(\n        128,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv2\",\n    )(x)\n    x = layers.MaxPooling2D((2, 2), strides = 2, name=\"pool2\")(x)\n\n    # Third conv block\n    x = layers.Conv2D(\n        256,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv3\",\n    )(x)\n  \n\n    # Fourth conv block\n    x = layers.Conv2D(\n        256,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv4\",\n    )(x)\n\n    x = layers.MaxPooling2D((1, 2), name=\"pool4\")(x)\n\n    # Fifth conv block\n    x = layers.Conv2D(\n        512,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv5\",\n    )(x)\n\n    x = layers.BatchNormalization(momentum = 0.8, name=\"BatchNormalization_1\")(x)\n    \n\n    # Sixth conv block\n    x = layers.Conv2D(\n        512,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv6\",\n    )(x)\n\n    x = layers.BatchNormalization(momentum = 0.8, name=\"BatchNormalization_2\")(x)\n\n    x = layers.MaxPooling2D((1, 2), name=\"pool6\")(x)\n\n    # Seventh conv block\n    x = layers.Conv2D(\n        512,\n        (2, 2),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"valid\",\n        name=\"Conv7\",\n    )(x)\n\n\n    # The number of filters in the last layer is 512. Reshape accordingly before\n    # passing the output to the RNN part of the model\n\n    new_shape = (int(max_length\/16)-1,512) #Kh\u00f4ng c\u1ea7n downsampling #N\u00ean coi shape l\u1edbp tr\u01b0\u1edbc\n\n    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n\n    def attention_rnn(inputs):\n        # inputs.shape = (batch_size, time_steps, input_dim)\n        input_dim = int(inputs.shape[2])\n        timestep = int(inputs.shape[1])\n        a = layers.Permute((2, 1))(inputs) #Permutes the dimensions of the input according to a given pattern.\n        a = layers.Dense(timestep, activation='softmax')(a) #\/\/ Alignment Model + Softmax\n        a = layers.Lambda(lambda x: keras.backend.mean(x, axis=1), name='dim_reduction')(a)\n        a = layers.RepeatVector(input_dim)(a)\n        a_probs = layers.Permute((2, 1), name='attention_vec')(a)\n        output_attention_mul = layers.multiply([inputs, a_probs], name='attention_mul') #\/\/ Weighted Average \n        return output_attention_mul\n\n    x = attention_rnn(x)\n    # RNNs\n    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n    x  =layers.Flatten()(x)\n    # Output layer\n    \n    x = layers.BatchNormalization(momentum = 0.8)(x)\n    x = layers.Dense(512 , activation=\"relu\")(x)\n    x = layers.Dense(256 , activation=\"relu\")(x) # \n    \n    y_pred = layers.Dense(2 , activation=\"softmax\", name=\"last_dense\")(x) # y pred\n    model = keras.models.Model(inputs=input_img, outputs=y_pred, name=\"model\")\n    \n    return model\nmodel = build_model()\nmodel.summary()","59c01567":"epochs = 50\nbatch_size = 32\nearly_stopping_patience = 10\n\ndef scheduler(epoch):\n    if epoch <= 10:\n        return 1e-3  \n    elif 10 < epoch <=15:\n        return 1e-4\n    else:\n        return 1e-5\n\n# Add early stopping\nmy_callbacks = [\n    tf.keras.callbacks.LearningRateScheduler(scheduler),\n    tf.keras.callbacks.ModelCheckpoint(filepath='.\/covid_model\/covid_model_{epoch:02d}.h5', \n                                    save_freq='epoch',\n                                    monitor='val_loss',\n                                    mode='min',\n                                    save_best_only=True,\n                                    period = 5),\n    tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n    )\n]\n\nmodel.compile(optimizer=keras.optimizers.Adam(),loss='categorical_crossentropy')                 \n\nhistory = model.fit(x= X_train, y= Y_train, \n                validation_data=(X_valid, Y_valid),\n                epochs = epochs, \n                batch_size = batch_size,\n                callbacks = my_callbacks,\n                )\n\n# list all data in history\nprint(history.history.keys())\n\n# summarize history for loss\n\nfig, ax = plt.subplots()\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.savefig('.\/covid_model\/covid_model_loss.png')\nplt.show()","1ec47e1b":"load_model =0\nload_model = build_model()\nload_model.load_weights('.\/covid_model\/covid_model_10.h5')","1016e4df":"predictions = []\npredictions = model.predict(x_test)\ny_predict =[]\nfor i in range(len(predictions)):\n    predict = np.argmax(predictions[i])\n    y_predict.append(predict)\n#print ('y_predict',y_predict)\n#print ('y test\\n', y_test)","b753792c":"import seaborn as sns\ndef evaluate_matrix(y_test, y_predict, name):\n    cm = confusion_matrix(y_test, y_predict)\n    cm_df = pd.DataFrame(cm, index=[\"Negative\", \"Positive\"], columns=[\"Negative\", \"Positive\"])\n\n    plt.figure(figsize=(10, 10))\n\n    sns.set(font_scale=1)\n\n    ax = sns.heatmap(cm_df, annot=True, square=True, fmt='d', linewidths=.2, cbar=0, cmap=plt.cm.Blues)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n\n    plt.ylabel(\"True labels\",fontsize = 'x-large')\n    plt.xlabel(\"Predicted labels\",fontsize = 'x-large')\n    plt.tight_layout()\n    plt.title(name,fontsize = 'xx-large',pad = 20)\n\n    plt.show()\n\n    print(classification_report(y_test, y_predict, target_names=[\"Negative\", \"Positive\"]))\n    \n#evaluate_matrix(y_test, y_predict,'Evaluate_matrix on orginal data')","034af278":"def ROC_curve(y_test,predictions,name):\n    \n    # calculate roc curves\n    lr_fpr, lr_tpr, _ = roc_curve(y_test, predictions[:,1])\n    print ('AUC = {:.3f}'. format( auc(lr_fpr, lr_tpr)))\n    # plot the roc curve for the model\n    lw = 2\n    plt.plot(lr_fpr, lr_tpr, color=\"darkorange\",\n             lw=lw, label=\"ROC curve (area = %0.2f)\" % auc(lr_fpr, lr_tpr))\n    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n    plt.xlim([-0.02, 1.0])\n    plt.ylim([0.0, 1.05])\n    # axis labels\n    pyplot.xlabel('False Positive Rate',fontsize = 'x-large')\n    pyplot.ylabel('True Positive Rate',fontsize = 'x-large')\n    plt.title(name,fontsize = 'xx-large', pad =20)\n    # show the legend\n    pyplot.legend()\n    # show the plot\n    pyplot.show()\n\n#ROC_curve(y_test,predictions,'AUC on original data')","f872eb0c":"#print ('indices_train',indices_train)  # This is the dataset that I used to split train, validation set\n#print ('y_train',y_train)","e29a8e39":"pos_indices = []\nAug_feature = []\nfor i,value in enumerate(y_train):\n    if value == 1:\n        pos_indices.append(indices_train[i])  \n        \ny, sr = librosa.load(data['filePath'][pos_indices[1]])        \n\ndef white_noise(y):\n    wn = np.random.randn(len(y))\n    y_wn = y + random.uniform(0, 0.005)*wn\n    return y_wn\n\ndef time_shift(y):\n    y = np.roll(y, random.randint(-10000,10000))\n    return y\n\ndef Gain(y):\n    y = y + random.uniform(-0.2,0.2)*y\n    return y       \n                            \ndef stretch(y, rate=random.uniform(0.8,1.2)):\n    y = librosa.effects.time_stretch(y, rate)\n    return y\n\nplt.figure(num='Data Augmentation',figsize=(9,9))\n\nplt.title ('original sound')\nplt.plot(y) \nplt.show()\n\nplt.title ('time_shift')\nplt.plot(time_shift(y))\nplt.show()\n\nplt.title ('Gain')\nplt.plot(Gain(y))\nplt.show()\n\nplt.title ('Time stretch')\nplt.plot(stretch(y))\nplt.show()\n\nplt.title ('white_noise')\nplt.plot(white_noise(y))\n\nplt.tight_layout(pad=3.0)\n#plt.axis('off')    \nplt.show()","661977a0":"print ('white noise')\nipd.Audio(white_noise(y), rate=sr)","8412ddb2":"print ('time shift')\nipd.Audio(time_shift(y), rate=sr)","98b46b5f":"print ('Gain')\nipd.Audio(Gain(y), rate=sr)","656c04a6":"print ('time stretch')\nipd.Audio(stretch(y),rate=sr)","6d4d00e7":"num_aug = (neg - pos)\/1.5\nif num_aug > len(pos_indices):\n    iteration = int(num_aug\/len(pos_indices))\nelse:\n    iteration =1\nprint ('Number of file generated based on one positive case',iteration)\n\nfor file in data['filePath'][pos_indices]:\n    if librosa.get_duration(y=y, sr=sr) > 30:\n        y,sr=librosa.load(file, duration = 30)\n    for i in range (iteration):\n        y,sr=librosa.load(file)\n        \n        chance = random.randint(0,100)\n        if chance <=20:\n            stretch(y)\n        if chance <=40:\n            time_shift(y)\n        if chance <=60:\n            Gain(y)\n        if chance <=80:\n            white_noise(y)\n\n        ZCR = librosa.feature.zero_crossing_rate(y, frame_length=2048, hop_length=512, center=True)\n        if ZCR.shape[1] >= max_length:\n            continue\n        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, S=None, n_fft=2048, \n                                                  hop_length=512, win_length=None, window='hann', \n                                                  center=True, pad_mode='reflect', power=2.0)\n        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr, S=None, norm=None, n_fft=2048, \n                                    hop_length=512, win_length=None, window='hann', \n                                    center=True, pad_mode='reflect', tuning=None, n_chroma=12)\n        MFCC = librosa.feature.mfcc(y=y, sr=sr, S=None, n_mfcc=20, dct_type=2, norm='ortho', lifter=0)\n\n\n        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, S=None, n_fft=2048, \n                                          hop_length=512, freq=None, win_length=None, window='hann', \n                                                              center=True, pad_mode='reflect')\n\n        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr, S=None, n_fft=2048, \n                                           hop_length=512, win_length=None, window='hann', \n                                           center=True, pad_mode='reflect', freq=None, centroid=None, norm=True, p=2)\n\n        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, S=None, n_fft=2048, \n                                         hop_length=512, win_length=None, window='hann', \n                                         center=True, pad_mode='reflect', freq=None, roll_percent=0.85)\n        aug_feature = np.concatenate((mel_spec,chroma_stft,MFCC,ZCR,spectral_centroid,spectral_bandwidth,spectral_rolloff), \n                                 axis=0)\n        aug_feature = librosa.power_to_db(aug_feature, ref=np.max)\n        Aug_feature.append(aug_feature)\n    #print ('Aug_feature',Aug_feature.shape)\nprint ('Aug_feature',len(Aug_feature) )        ","ff9ed376":"plt.figure(num='Aug_feature',figsize=(9,9))\nfor i, img in enumerate(Aug_feature[:3]):\n    plt.subplot(3,1,i+1) \n    plt.title('Aug_feature {}'.format(i))      \n    plt.imshow(img)\n    plt.colorbar(format='%+2.0f dB')\n    plt.tight_layout(pad=3.0)\n    #plt.axis('off')    \nplt.show()","77ec5f70":"y_aug = np.ones((1,len(Aug_feature)),dtype = np.uint8)[0]\n#print (y_aug)","e429711b":"scale_aug_features =[]\nfor feature in Aug_feature:\n    feature = preprocess(feature,featureSize = (int(max_length\/4),41)) #41 = 164\/4\n    scale_aug_features.append(feature)\n\n#print ('scale feature',scale_features[1])    \nplt.figure(num='Features transpose',figsize=(9,9))\n\nfor i, img in enumerate(scale_aug_features[:3]):\n    plt.subplot(3,3,i+1) \n    plt.tight_layout(pad=3.0)\n    plt.title('scale_aug_features {}'.format(i))\n    plt.imshow(img)\n    #plt.axis('off')    \nplt.show()","a3034360":"scale_aug_features = np.array(scale_aug_features).reshape(-1, int(max_length\/4), 41, 1)\nx_train = np.concatenate((x_train,scale_aug_features), axis=0)\nprint (len(x_train))","f056a962":"y_train = np.concatenate((y_train,y_aug), axis=0)","c6b1865e":"X_train, X_valid, Y_train, Y_valid = train_test_split(x_train, y_train, test_size=0.2, shuffle = True, \n                                                    random_state = None, stratify = y_train )\n\nY_train = np_utils.to_categorical(Y_train, 2)\nY_valid = np_utils.to_categorical(Y_valid, 2)\n#print (Y_valid)\nprint (len(Y_valid))\nprint('X_train.shape',X_train.shape)\nprint('X_valid.shape',X_valid.shape)","2d525bbf":"new_model = 0\nnew_model = build_model()","5cc7b3d1":"my_callbacks = [\n    tf.keras.callbacks.LearningRateScheduler(scheduler),\n    tf.keras.callbacks.ModelCheckpoint(filepath='.\/covid_model\/new_covid_model_{epoch:02d}.h5', \n                                    save_freq='epoch',\n                                    monitor='val_loss',\n                                    mode='min',\n                                    save_best_only=True,\n                                    period = 5),\n    tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n    )\n]\n\nnew_model.compile(optimizer=keras.optimizers.Adam(),loss='categorical_crossentropy')                 \n\nnew_history = new_model.fit(x= X_train, y= Y_train, \n                validation_data=(X_valid, Y_valid),\n                epochs = epochs, \n                batch_size = batch_size,\n                callbacks = my_callbacks,\n                )\n\n# list all data in history\nprint(new_history.history.keys())\n\n# summarize history for loss\n\nfig, ax = plt.subplots()\nplt.plot(new_history.history['loss'])\nplt.plot(new_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.savefig('.\/covid_model\/new_covid_model_loss.png')\nplt.show()","399cbf1d":"new_load_model = 0\nnew_load_model = build_model()\nnew_load_model.load_weights('.\/covid_model\/new_covid_model_10.h5')","a6bf9257":"new_predictions = []\nnew_predictions = new_load_model.predict(x_test)\ny_new_predict =[]\nfor i in range(len(new_predictions)):\n    predict = np.argmax(new_predictions[i])\n    y_new_predict.append(predict)\n#print ('y_predict',y_predict)\n#print ('y test\\n', y_test)","85532e7d":"evaluate_matrix(y_test, y_new_predict,'Evaluate_matrix on Augmented data')\nROC_curve(y_test,new_predictions,'AUC on Augmented data')","c2e2eaac":"evaluate_matrix(y_test, y_predict,'Evaluate_matrix on Original data')\nROC_curve(y_test,predictions,'AUC on Original data')","863eb5e8":"**Saving features**\n\nBecause our audio lengths are diferent from each others, so how can we recover data from flatten array?\n\n`len(data)\/164`. The number of column are the same and equal to 164. I need to save it for my project, so I'll hide it from you guys","e2cf3a7d":"**Visualize Data**\n\nI had to convert these images into dB value to have a better view on the problem. You can see that I concaternate features with the vertical axis. It helps retain the time information. From there, our CRNN can work better\n\nFurthermore. In the first phase of ths project [here](https:\/\/www.kaggle.com\/bomaich\/covid-19-cough-classification). I didn't start audio file and what confused me was the compute the mean value for all features on time axis. I think that method destroy the data properties.\n\nSome information need to be used on time axis to work well like Mel-spectrogram, MFCC or ZCR\/energy. In fact, they usually combine ZCR\/Energy to distinguish when it is sound and when it is not. When there is sound ZCR will suddenly decrease and energy suddenly rise at the same time","b14ad52b":"Now we will padding the files","31b33602":"This part is for displaying the length of data.\n\nAs you can see, the majority of the files are around 15 seconds long. Only two files are longer than that, and they're far too lengthy (over 1 min). So I'm going to shorten that file to 30 seconds. And it won't make much of a difference. Not to mention if I use the length of\u00a0that file to padding other pictures. The dataset would be significantly distorted","8d00ef0d":"You can listen to these file to point out the diferences","72fe1915":"Cause the original file is so big, so I set newSize into a quarter of original size. The file length would be padding as the longest file","185061c0":"In the file cough_trial_extended.csv have sowm information that the author have done before. ","ecc12ce7":"Let's see what's in the 2 datasets. \n\nAs we can see. The metadata for Aicovid 115M dataset have file_path, gende, age and I am not gonna use them. The assessment_result (0: negative, 1:positive)","dbda0e10":"# PREPARE FOR TRAINING","3a7faa61":"I have done a COVID-19 Cough Classification project [here](https:\/\/www.kaggle.com\/bomaich\/covid-19-cough-classification)\nand It helped me a lot on figuring out how to deal with audio data, imbalanced data, evaluate with k-fold. And that was phase 1.\n\nNow with phase 2. I will use diffenrent dataset:\n\nAicovidVN115M: https:\/\/www.kaggle.com\/blinkpm1\/aicovidvn115m\n\nCOVID-19 Cough Recordings: https:\/\/www.kaggle.com\/himanshu007121\/coughclassifier-trial\n\nThese dataset is from `.wav` file so that I can extract diffenrent features that I want, and how I extract is different for phase 1 too.","4c6a79a0":"# DATASET","4025e3ef":"I will save the indices for the Data Augmentation stage","eea9cfca":"I'll combine augmented data with x_train data, then divide it into train and valid sets.","1c9c3e90":"After using Data Augmentation, Our predictions were undeniably better\nThe accuracy rise but it's not the matter cause the topic like this usually evaluate with other metrics\n* the positive metrics are all increase (that's good)","446501ac":"Due to the dataset imbalanced problems as the result above\n> positive: 481 (35.14% of total) \n> negative cases: 888\n\nI need to generate around 200 to 250 positive cases more","5875a4af":"We'll scale data and encode label","542e3fb0":"# DATA AUGMENTATION\nSo the result above is quite bad and it can't be used in real life. I guess that Data Augmentation would help improve the outcome\n\nMethods:\n* Time Shift\n* Adding background noise\n* Stretching the sound (just a little bit)\n* Changing Gain\n\nI try not to generate fake sounds that rarely happen in real life, and distort the sound so much. So I don't recommend\n* Time stretch (too much)\n* Mix up \n\nNote: You have to use data augmentation on just the train and valid set","eccc4010":"# PREPROCESSING\nI will extract 2D features like:\n* Mel-frequency Spectrogram\n* chroma\n\nAnd then combine them into image to feed into the model\nI may add 1D features and mean them through time (not mean them on the whole audio like phase 1):\n* MFCCs\n* Spectral Centroid\n* Spectral Bandwidth\n* Spectral Roll-off\n* ZCR + energy\n","bf964d7a":"I want to utilize the best model here, thus I loaded the best weights into the model. Otherwise, it would use the last model, which appears to be overfit to forecast, and that prediction is often poorer. \n\nChoosing which output weights to use is based on the training history, and I have to change the path manualy all the time","d5a83a33":"# BUILDING MODEL","8e26c0ed":"The ideal is to combine the 2 data into 1 csv file with 2 column, The first one is the file path and the second one is encoded labels (0: negative, 1:positive)","bb954847":"I used CRNN + attention due to it's power on time series data. The output will be flatten to classify ","02d7306e":"Here I want to see the range of file duration. We have 2 options:\n1. Sorted padding with Batch\n2. Padding on all the dataset based on the longest file\nI know it's hard but tensorflow doesn't allow me to feed data with variable lengths into the model\n\nI'd choose the first option because it's much more simple and faster","84c8e940":"# Dividing data into TRAINING, VALIDATION and TEST set","a763066d":"# TESTING","d4f0e476":"# TRAINING"}}