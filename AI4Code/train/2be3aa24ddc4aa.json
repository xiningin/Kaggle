{"cell_type":{"a8818196":"code","f2a9293f":"code","4e8cbaa9":"code","b868e624":"code","3d1ec6cc":"code","04df61b1":"code","22ddec8e":"code","b21d3a4d":"code","ab2cf2e6":"code","54c77411":"code","1db58f58":"code","3f2617a5":"code","d3343b1f":"code","27e3e44a":"code","89d4fb7a":"code","d2ab0437":"code","e32fb41e":"code","5afdfed0":"code","d3cc0881":"code","2c084702":"code","2831f9c9":"code","3a70372d":"code","67351fb6":"code","686e1053":"code","e1f11754":"code","1258eeed":"code","064b4dbb":"code","859d13e8":"code","a1a09fcd":"code","973515eb":"code","e2f98c4f":"code","b44099a3":"code","566ad20e":"code","360c1734":"code","d6c80a48":"code","fec16ed4":"code","382dbe94":"code","e4d7b818":"code","10d40703":"code","605bc2e3":"code","d4a8eb29":"code","1636f46f":"code","44998890":"markdown","224e21db":"markdown","ba3bb629":"markdown","f1977167":"markdown","a1a10029":"markdown","00bf7157":"markdown","863b6cab":"markdown","332f39c5":"markdown","1c5d541b":"markdown","1299adde":"markdown","6d89160f":"markdown","3bef3d48":"markdown","e0a170e5":"markdown","578e70ac":"markdown","7dc41a86":"markdown","d56ab453":"markdown","c29e9068":"markdown","8bbcb376":"markdown","6f7db178":"markdown","07db1a4b":"markdown","a8a89312":"markdown","04fcdd82":"markdown","854f9e75":"markdown","b4a3f91f":"markdown","1c68f6fa":"markdown"},"source":{"a8818196":"# !pip install pandas-profiling","f2a9293f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt #visualisation\nsns.set(color_codes=True)\nsns.set(color_codes=True)\nfrom sklearn.preprocessing import StandardScaler \nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom pandas_profiling import ProfileReport\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","4e8cbaa9":"train = pd.read_csv(\"https:\/\/vincentarelbundock.github.io\/Rdatasets\/csv\/plm\/Wages.csv\")\ntrain.head()","b868e624":"train.shape","3d1ec6cc":"train.describe()","04df61b1":"train.head(20)","22ddec8e":"train.tail(20)","b21d3a4d":"train.isnull().sum()","ab2cf2e6":"ProfileReport(train)","54c77411":"sns.distplot(train[\"lwage\"])","1db58f58":"corr = train.corr()\nplt.figure(figsize=(12,8))\nsns.heatmap(corr,cmap=\"GnBu\", annot=True)","3f2617a5":"train.columns","d3343b1f":"train.info()","27e3e44a":"train_new = train.drop(['Unnamed: 0','bluecol','south','smsa','married','sex','union','black'], axis=1)","89d4fb7a":"train_new.info()","d2ab0437":"train.isnull().sum()","e32fb41e":"fig ,ax  = plt.subplots(figsize = (13,8))\nsns.boxplot(data = train_new , ax =  ax)","5afdfed0":"Q1 = train_new.quantile(0.25)\nQ3 = train_new.quantile(0.75)\nQ1,Q3\n","d3cc0881":"IQR = Q3 - Q1\nIQR","2c084702":"lower_limit = Q1 - 1.5*IQR\nupper_limit = Q3 + 1.5*IQR\nlower_limit, upper_limit","2831f9c9":"train_new[(train_new < lower_limit) |(train_new > upper_limit) ]","3a70372d":"train_new = train_new[(train_new > lower_limit) & (train_new < upper_limit) ]\n","67351fb6":"train_new","686e1053":"# keeping threshold as 70%\nthreshold = 0.7\n#filter columns with mean missing value higher than threshold\ntrain_new = train_new[train_new.columns[train_new.isnull().mean() < threshold]]\n\n#filter rows with mean missing value rate higher than threshold\ntrain_new = train_new.loc[train_new.isnull().mean(axis=1) < threshold]\n\ntrain_new","e1f11754":"train_new.isnull().sum()","1258eeed":"# Filling missing values with medians of wks,ed,lwage columns\ntrain_new['wks'] = train_new[\"wks\"].fillna(train_new['wks'].median())\ntrain_new['ed'] = train_new[\"ed\"].fillna(train_new['ed'].median())\ntrain_new['lwage'] = train_new[\"lwage\"].fillna(train_new['lwage'].median())\n\ntrain_new.head()","064b4dbb":"train_new.isnull().sum()","859d13e8":"fig ,ax  = plt.subplots(figsize = (13,8))\nsns.boxplot(data = train_new , ax = ax)","a1a09fcd":"y = train_new['lwage']\ny","973515eb":"X = train_new.drop(columns=['lwage'])\nX","e2f98c4f":"scalar = StandardScaler()\nX_scaled = pd.DataFrame(scalar.fit_transform(X))\n","b44099a3":"fig ,ax  = plt.subplots(figsize = (13,8))\nsns.boxplot(data = X_scaled , ax = ax)","566ad20e":"train_scaled = pd.DataFrame(scalar.fit_transform(train_new))\nfig ,ax  = plt.subplots(figsize = (13,8))\nsns.boxplot(data = train_scaled , ax = ax)","360c1734":"x_train, x_test, y_train, y_test = train_test_split(X_scaled , y , test_size = 0.3 , random_state = 100)","d6c80a48":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","fec16ed4":"model = DecisionTreeRegressor().fit(x_train,y_train)\ny_pred=model.predict(x_test)","382dbe94":"from sklearn.metrics import mean_absolute_error\nprint(mean_absolute_error(y_test,y_pred))","e4d7b818":"y_pred","10d40703":"y_test","605bc2e3":"print(y_pred.mean())\ny_pred_perc = y_pred.mean()","d4a8eb29":"print(y_test.mean())\ny_test_perc = y_test.mean()","1636f46f":"off_percentage = 100-(y_pred_perc\/y_test_perc)*100\nprint(off_percentage)","44998890":"## \ud83d\udcc8Exploring the target variable & features with visualisations","224e21db":"## \ud83e\udd37\u200d\u2642\ufe0fProblem Statement\n\nBefore I start this, I wish everyone who are jobless,pursuing their dream or seeking for pay rise, one day you will get it.Keep trying.\n\nThe data is collected by Panel Study of Income Dynamics (PSID) which is the longest running longitudinal household survey in the world.A panel of 595 individuals from 1976 to 1982, taken from the Panel Study of Income Dynamics (PSID) in the United States.This analysis and prediction is about 'lwage'. The interpretation of using the logarithm of wages is that we are looking at wage gaps of a particular percentage, rather than of a particular dollar value.\n\nGoverment and public were in confusion of what are the exact factors that contribute to ones  wages.Hence, by knowing the factos that contribute to ones wage,students and jobless people can be better prepared to increase their employability.\nThe approach taken: Since predicting 'lwage' is a regression problem, we will be using decision tree regressor as our model.\n","ba3bb629":"Here are the outliers","f1977167":"## \ud83c\udf1fModel Spliting","a1a10029":"## \ud83d\udcc3 Overview of all Steps to be Performed\n\n1.Setting up \n\n2.Hypothesis Generation \n\n3.Understanding the data\n\n4.Exploratory Data Analysis (EDA)\n\n5.Cleaning and transforming (outlier treatment & scaling)\n\n\n6.Model Building \n\n\ni) Decision Tree\n\n7.Other Questions\n\n","00bf7157":"Visualise correlation between variables ","863b6cab":"Loading(Reading) the data","332f39c5":"### \ud83d\udcd0Transforming by scaling the data to have a normalised data","1c5d541b":"#### \ud83d\udca1Insights\nThe logarithmic wage variable follows normal distribution.","1299adde":"The prediction of logarithmic wages based on the variables in this data is off by 0.15% which is means the prediction almost perfect.","6d89160f":"Remove outliers","3bef3d48":"#### \ud83d\udca1 Insights\n\n    \n - 1.No null values, 5 numeric ,6 boolean , 2 categorial values which mean we need to one-hot encoding later.\n \n - 2.The 'ed' variable:\n     mean: 12.84\n     kurtois:-0.27\n     variance:7.78\n  \n - 3.The 'exp' variable:\n     mean: 19.85\n     kurtois: -0.93\n     variance:120.26\n   \n   \n   \nIf the kurtosis is greater than 3, then the dataset has heavier tails than a normal distribution (more in the tails). If the kurtosis is less than 3, then the dataset has lighter tails than a normal distribution (less in the tails).\n\nMeasures variability from the average. \n\n\nBased on the distribution graphs and descriptive statistics above, Some variables are skewed and do no follow normal distribution.Hence we will do outlier treatment and standard scaling.\n\n","e0a170e5":"Continuos variable distribution","578e70ac":"3 Descriptive facts(mean,mode,iqr) with analysis and graphs","7dc41a86":"## \ud83e\udd14 Hypothesis Generation \nAll the factors that influence the target variable ('lwage') \n- 1.The higher the 'ed' , higher the  'lwage'.\n- 2.The higher the 'exp, higher the 'lwage'.\n","d56ab453":"### \ud83e\uddf9Cleaning up again by filtering columns and rows with 70% threshold & filling up missing values with numerical imputation (median)","c29e9068":"## \ud83d\ude4c Setting up by importing libraries,reading data & knowing the size of data","8bbcb376":"1. How do find a way to classify this dataset into 3 homogeneous groups?\nStratified Sampling method.\n\n2. Today we would like to sample some data from this dataset for further study. What size of sample would you recommend?\n10% of the total dataset. Since the total number of records is 4165, the sample size will be 417 records.\n\n3. How would you select and allocate these samples.\nThis stratified Sampling divides the elements of the population into small subgroups (strata) based on the similarity in such a way that the elements within the group are homogeneous and heterogeneous among the other subgroups formed. And then the elements are randomly selected from m each of these strata.\n\n4. Please explain why sampling is important and usually a preferred methodology?\nWhat are the pros and cons of sampling?\n* Sampling is important for inferential statistics like chi-square, confidence interval and etc. In practice, the researcher will collect sample data and from these data estimate parameters of the population distribution. Thus, knowledge of the sampling distribution can be very useful in making inferences about the overall population.\n\n* A. Advantages \n1. Low cost if use sampling \nIf data were collected for the whole population, the cost will be quite high. A sample is a small portion of a population.\nSo, when the data is collected for only a sample of the population the cost will be lower.\n\n2. Scope of sampling is high\nIf the researcher is concerned with the generalization of the data. To study the whole population in order to arrive at a generalization would be impractical. Before the measurement has been completed, the population would have changed. But the process of sampling makes it possible to arrive at generalization by studying the variables within a relatively small portion of the population.\n\n* B.Disadvantages\n1. Chance of bias\nThe main drawback of the sampling method is that it involves biased selection and thereby leads us to draw erroneous conclusions. Bias happens when the method of selection of sample used is faulty.\n\n2. Inadequate domain knowledge\nThe use of the sampling method requires adequate domain knowledge. Sampling involves statistical analysis. When the researcher lack specialized knowledge in sampling, he may commit serious mistakes.\n\n\n5. What are the factors that would affect sampling results.\n\n  The factors are study design, method of sampling, significance level, and a few more.\n \n","6f7db178":"#### \ud83d\udca1Insights\n2 values that have high correlation to 'ed' are 'exp' to 'lwage'.Our hypothesis is spot on.","07db1a4b":"### Other Questions \ud83e\udd29","a8a89312":"## \ud83d\udcbe Data\nThe data has 1 file: wages.csv\n\nIt contain the following 13 columns(12 features and 1 target) :\n\n1.exp\nyears of full-time work experience.\n\n2.wks\nweeks worked.\n\n3.bluecol\nblue collar?\n\n4.ind\nworks in a manufacturing industry?\n\n5.south\nresides in the south?\n\n6.smsa\nresides in a standard metropolitan statistical area?\n\n7.married\nmarried?\n\n8.sex\na factor with levels \"male\" and \"female\"\n\n9.union\nindividual's wage set by a union contract?\n\n10.ed\nyears of education.\n\n11.black\nis the individual black?\n\n12.lwage\nlogarithm of wage.\n","04fcdd82":"### \ud83e\uddd9\ud83c\udffc\u200d\u2640\ufe0fTransforming data by removing Outliers using IQR","854f9e75":"### \ud83e\uddf9Cleaning up by dropping unnecessary columns ","b4a3f91f":"## \ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb Model Building","1c68f6fa":"## \ud83e\udd13 Understanding the data by knowing the variables types, data types, statistics of the data & null,NAN values."}}