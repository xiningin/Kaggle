{"cell_type":{"4048bd99":"code","8460bf2b":"code","6034936e":"code","ebb00b4c":"code","d5c18155":"code","13eb9397":"code","dc4fac52":"code","3bfb75de":"markdown","70b31dbd":"markdown","5f6f4828":"markdown","cb1c66fc":"markdown","f7a1963d":"markdown","56c206c9":"markdown","f5de681d":"markdown","25291d03":"markdown"},"source":{"4048bd99":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import TimeSeriesSplit\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch","8460bf2b":"training_path = '..\/input\/ubiquant-market-prediction\/train.csv'\n\ndtypes = {\n    'row_id': 'str',\n    'time_id': 'uint16',\n    'investment_id': 'uint16',\n    'target': 'float32',\n}\n\ndtypes.update({f'f_{i}': 'float32' for i in range(300)})\n\ntrain = pd.read_csv(training_path,usecols=list(dtypes.keys()),dtype=dtypes)","6034936e":"np.random.seed(1338)\ncmap_data = plt.cm.Paired\ncmap_cv = plt.cm.coolwarm\nn_splits = 4\n\n# Generate the class\/group data\nn_points = 100\nX = np.random.randn(100, 10)\n\npercentiles_classes = [0.1, 0.3, 0.6]\ny = np.hstack([[ii] * int(100 * perc) for ii, perc in enumerate(percentiles_classes)])\n\n# Evenly spaced groups repeated once\ngroups = np.hstack([[ii] * 10 for ii in range(10)])\n\n\ndef visualize_groups(classes, groups, name):\n    # Visualize dataset groups\n    fig, ax = plt.subplots()\n    ax.scatter(\n        range(len(groups)),\n        [0.5] * len(groups),\n        c=groups,\n        marker=\"_\",\n        lw=50,\n        cmap=cmap_data,\n    )\n    ax.scatter(\n        range(len(groups)),\n        [3.5] * len(groups),\n        c=classes,\n        marker=\"_\",\n        lw=50,\n        cmap=cmap_data,\n    )\n    ax.set(\n        ylim=[-1, 5],\n        yticks=[0.5, 3.5],\n        yticklabels=[\"Data\\ngroup\", \"Data\\nclass\"],\n        xlabel=\"Sample index\",\n    )\n\ndef plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n\n    # Generate the training\/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n        # Fill in indices with the training\/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(\n            range(len(indices)),\n            [ii + 0.5] * len(indices),\n            c=indices,\n            marker=\"_\",\n            lw=lw,\n            cmap=cmap_cv,\n            vmin=-0.2,\n            vmax=1.2,\n        )\n\n    # Plot the data classes and groups at the end\n    ax.scatter(\n        range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data\n    )\n\n    ax.scatter(\n        range(len(X)), [ii + 2.5] * len(X), c=group, marker=\"_\", lw=lw, cmap=cmap_data\n    )\n\n    # Formatting\n    yticklabels = list(range(n_splits)) + [\"class\", \"group\"]\n    ax.set(\n        yticks=np.arange(n_splits + 2) + 0.5,\n        yticklabels=yticklabels,\n        xlabel=\"Sample index\",\n        ylabel=\"CV iteration\",\n        ylim=[n_splits + 2.2, -0.2],\n        xlim=[0, 100],\n    )\n    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=15)\n    return ax","ebb00b4c":"this_cv = TimeSeriesSplit(n_splits=5)\nfig, ax = plt.subplots(figsize=(6, 3))\nplot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n\nax.legend(\n    [Patch(color=cmap_cv(0.8)), Patch(color=cmap_cv(0.02))],\n    [\"Testing set\", \"Training set\"],\n    loc=(1.02, 0.8),\n)\n# Make the legend fit\nplt.tight_layout()\nfig.subplots_adjust(right=0.7)\nplt.show()","d5c18155":"this_cv = TimeSeriesSplit(n_splits=5,max_train_size=25)\nfig, ax = plt.subplots(figsize=(6, 3))\nplot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n\nax.legend(\n    [Patch(color=cmap_cv(0.8)), Patch(color=cmap_cv(0.02))],\n    [\"Testing set\", \"Training set\"],\n    loc=(1.02, 0.8),\n)\n# Make the legend fit\nplt.tight_layout()\nfig.subplots_adjust(right=0.7)\nplt.show()","13eb9397":"import sklearn\n\nprint(sklearn.__version__)","dc4fac52":"cv = TimeSeriesSplit(n_splits=5, max_train_size=300)\n\nfor fold, (train_idx, test_idx) in enumerate(cv.split(train['time_id'].unique(),groups=train['time_id'].unique())):\n    \n    print('Fold: {}'.format(fold))\n\n    train.loc[(train['time_id'].isin(train_idx))].reset_index().to_feather('train_fold_'+str(fold)+'.feather')\n    train.loc[(train['time_id'].isin(test_idx))].reset_index().to_feather('test_fold_'+str(fold)+'.feather')","3bfb75de":"# Imports","70b31dbd":"From: https:\/\/www.kaggle.com\/lucamassaron\/training-data-to-feather-python-r-low-mem\n\nAdding: Time series split from sklearn","5f6f4828":"With maximum training size:","cb1c66fc":"Feather files are fast to be read, they can be used both for Python (using pandas) and R, keep the dtype of your columns and reduce the overall memory usage.","f7a1963d":"More recents version include gap parameters but not our current version of sklearn:","56c206c9":"Alway training from the beggining:","f5de681d":"# Time series split:\n\nTools to plot from: https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html#time-series-split","25291d03":"# Reading Data"}}