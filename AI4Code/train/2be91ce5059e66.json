{"cell_type":{"eaeb848f":"code","5dc0a880":"code","65b011ce":"code","735403ab":"code","a3246e39":"code","79a41042":"code","a4363f00":"code","c341c081":"code","fe2b01cf":"code","b25a4b39":"code","0d0d7f66":"code","83df6f65":"code","6e01e809":"code","733ef660":"code","3bce4a63":"code","272464ed":"code","22a5e067":"code","42bfd096":"code","5ca4f2de":"code","f34b7b01":"code","0af51076":"code","6eb9272d":"code","8f901bd8":"code","643218a6":"code","4267c8b2":"code","a3006476":"code","6bfc1ecb":"code","78139188":"code","0d93f553":"code","688a6ddb":"code","0ea9a29a":"code","561c66ae":"code","ae91ff94":"code","4abb5393":"code","34923baa":"code","a674301d":"code","1998d1d1":"code","96dfba1b":"code","d39e5404":"code","3bb543de":"code","c4d42a8b":"code","2975f7b9":"code","e0ca6af9":"code","6ec0ee14":"code","814e4426":"code","754d1779":"code","72687bba":"code","2f630f93":"code","9b4c80e1":"code","26be0b62":"code","bf093126":"code","2cb592f6":"code","45f8251c":"code","aaf83084":"code","82c928c4":"code","56dfd5d8":"code","52f411c3":"code","e58704d4":"code","57a321cc":"code","e64aa134":"code","d7d41ff6":"code","08d56960":"code","e05122cd":"code","3969e388":"code","9f8c9e25":"code","2959c8ef":"code","69f4c411":"code","a4b61eda":"code","313d818c":"code","b6fb85db":"code","657145a5":"code","6f075f60":"code","e19e111c":"code","b0918569":"markdown","c26bf0cf":"markdown","b0b8d336":"markdown","3822754e":"markdown","114f3aca":"markdown","1c9f5de5":"markdown","ee178181":"markdown","9d9dd94c":"markdown","b1a6d283":"markdown","d9f0ffb9":"markdown","70c0e7ab":"markdown"},"source":{"eaeb848f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5dc0a880":"from sklearn.svm import SVR\nfrom catboost import CatBoostRegressor as cb,Pool\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n","65b011ce":"#Loading the data and looking at the data\ntrain= pd.read_csv('\/kaggle\/input\/tabular-playground-series-jul-2021\/train.csv')\ntrain.head()","735403ab":"#checking the null data\ntrain.isnull().sum()","a3246e39":"# it is full functional data without any null variables.","79a41042":"# lets look at the dtype and the data\ntrain.info()","a4363f00":"# our target variable are target_benzene,target_carbon_monoxide and target_nitrogen_oxides.","c341c081":"# now lets change the date_time from object to float\ntrain['date_time'] = train['date_time'].astype('datetime64[ns]').astype(np.int64)\/10**9","fe2b01cf":"# lets look at the date_time \ntrain['date_time']","b25a4b39":"#so now date_time is  usable as a  feature ","0d0d7f66":"\nfeatures= ['relative_humidity','sensor_1','sensor_2','absolute_humidity','sensor_3','sensor_4','sensor_5','deg_C','date_time']\nx= train[features]\nx","83df6f65":"y= train['target_carbon_monoxide']","6e01e809":"from sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(x,y):\n    mi_scores = mutual_info_regression(x,y)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=x.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(x,y)\nmi_scores","733ef660":"#mi scores show the relation btw variables and target\n# you can do it for all the target features which i didnt to save time because data is highly correlated with target features","3bce4a63":"\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)","272464ed":"import seaborn as sns\nsns.relplot(x=\"sensor_2\", y=\"target_carbon_monoxide\", data=train);","22a5e067":"# it shows a strong relationshio btw sensor 2 Nd carbon_monoxide","42bfd096":"sns.lmplot(x=\"relative_humidity\", y=\"target_carbon_monoxide\", data=train);","5ca4f2de":"# even with the lowest feature it shows a strong relationship","f34b7b01":"# lets put the other  two target in different variables\nbenzene= train['target_benzene']\nnitrogen= train['target_nitrogen_oxides']","0af51076":"from sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y,val_y = train_test_split(x,y, test_size=0.33, random_state=0)","6eb9272d":"############IMPORTANT#########\n# we will create 3 models for prediction and then add these models then divide by 3 to get the final prediction for 1 target variable\n# this trick will give you a slightly better model than single model.","8f901bd8":"rf= RandomForestRegressor()\nrf.fit(train_X,train_y)\npredictions= rf.predict(val_X)\nprint(\"mean ab error: \"+ str(mean_absolute_error(val_y,predictions) ))","643218a6":"\n\nxgb = XGBRegressor(n_estimators=600,n_jobs=8,learning_rate=0.1)\nxgb.fit(train_X, train_y,early_stopping_rounds=5,eval_set=[(val_X,val_y)],verbose=False)\nfrom sklearn.metrics import mean_absolute_error\n\npredictions = xgb.predict(val_X)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, val_y)))","4267c8b2":"train_dataset = Pool(train_X, train_y)\n\n","a3006476":"cbr = cb(loss_function='RMSE')\n \ngrid = {'iterations': [100, 150, 200],\n        'learning_rate': [0.03, 0.1],\n        'depth': [2, 4, 6, 8],\n        'l2_leaf_reg': [0.2, 0.5, 1, 3]}\ncbr.grid_search(grid,train_dataset )","6bfc1ecb":"cbr_pred = cbr.predict(val_X)\nrmse = (np.sqrt(mean_squared_error(val_y, pred)))\nr2 = r2_score(val_y, pred)\nprint('Testing performance')\nprint('RMSE: {:.2f}'.format(rmse))\nprint('R2: {:.2f}'.format(r2))","78139188":"print(\"Mean Absolute Error: \" + str(mean_absolute_error(val_y,cbr_pred)))\n","0d93f553":"train_target_carbon_monoxide= (xgb.predict(x)+cbr.predict(x)+rf.predict(x))\/3","688a6ddb":"train_target_carbon_monoxide","0ea9a29a":"# it is doing good in the test so we will move on to the test predictions","561c66ae":"# we got the values of target_carbon_monoxide it is doing good in the training so we are going to do in the test\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jul-2021\/test.csv')\ntest.head()","ae91ff94":"# copying test-date_time for submission\ntest_date_time = test['date_time'].copy()","4abb5393":"# changing dates for making it usable\ntest['date_time'] = test['date_time'].astype('datetime64[ns]').astype(np.int64)\/10**9\n","34923baa":"X = test[features]\nX","a674301d":"# now predicting the values of test set of target carbon_mono_oxide\ntarget_carbon_monoxide= (xgb.predict(X)+rf.predict(X)+cbr.predict(X))\/3","1998d1d1":"target_carbon_monoxide","96dfba1b":"# we  got the first target values ,and the values are just fine\n# lets move on to second and third target\n","d39e5404":"# new target feature benzene , \ny= benzene ","3bb543de":"# we have to do the same as above\n# first splitting data into train and test","c4d42a8b":"train_X, val_X, train_y,val_y = train_test_split(x,y, test_size=0.3, random_state=0)","2975f7b9":"# now creating and fitting the models`","e0ca6af9":"# hyper parameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","6ec0ee14":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(train_X, train_y)","814e4426":"rf_random.best_params_","754d1779":"rf = RandomForestRegressor(n_estimators = 1000,min_samples_split= 2,min_samples_leaf= 1,max_features= 'sqrt',max_depth= 20,bootstrap= True)\nrf.fit(train_X, train_y)","72687bba":"predictions= rf.predict(val_X)\nprint(\"mean ab error: \"+ str(mean_absolute_error(val_y,predictions) ))","2f630f93":"xgb = XGBRegressor(n_estimators=600,n_jobs=8,learning_rate=0.1)\nxgb.fit(train_X, train_y,early_stopping_rounds=5,eval_set=[(val_X,val_y)],verbose=False)\nfrom sklearn.metrics import mean_absolute_error\n\npredictions = xgb.predict(val_X)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, val_y)))","9b4c80e1":"cbr = cb(iterations=1000,\n                             learning_rate=0.02,\n                             depth=12,\n                             bagging_temperature = 0.2,\n                            verbose=False,\n                             od_wait=100)\ncbr.fit(train_X,train_y)\npredictions = cbr.predict(val_X)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, val_y)))","26be0b62":"# let's check \u2611 training predictions now","bf093126":"# we didn't choose random forest this time because model was not doing much well , so we opt for that only\ntrain_benzene= (cbr.predict(x)+rf.predict(x))\/2","2cb592f6":"train_benzene","45f8251c":"# well train data does somewhat fine on predicting the values , so lets move on to target prediction","aaf83084":"# now we will do it in test data\n# we are choosing only catboost and xgboost because random forest not doing much good prediction in this case\ntarget_benzene = (rf.predict(X)+cbr.predict(X)+xgb.predict(X))\/3","82c928c4":"target_benzene","56dfd5d8":"#we will move to our last target","52f411c3":"y=nitrogen","e58704d4":"train_X, val_X, train_y, val_y = train_test_split(x, y, random_state=1)","57a321cc":"# hyper parameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(train_X, train_y)","e64aa134":"rf_random.best_params_","d7d41ff6":"rf = RandomForestRegressor(n_estimators = 400,min_samples_split= 2,min_samples_leaf=1 ,max_features= 'sqrt',max_depth= None,bootstrap= False)\nrf.fit(train_X, train_y)","08d56960":"predictions= rf.predict(val_X)\nprint(\"mean ab error: \"+ str(mean_absolute_error(val_y,predictions) ))","e05122cd":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor(n_estimators=600,n_jobs=8,learning_rate=0.1)\nxgb.fit(train_X, train_y,early_stopping_rounds=100,eval_set=[(val_X,val_y)],verbose=False)\nfrom sklearn.metrics import mean_absolute_error\n\npredictions = xgb.predict(val_X)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, val_y)))","3969e388":"train_dataset = Pool(train_X, train_y)\n\n","9f8c9e25":"cbr = cb(loss_function='RMSE')\n \ngrid = {'iterations': [100, 150, 200],\n        'learning_rate': [0.03, 0.1],\n        'depth': [2, 4, 6, 8],\n        'l2_leaf_reg': [0.2, 0.5, 1, 3]}\ncbr.grid_search(grid,train_dataset )","2959c8ef":"cbr_pred = cbr.predict(val_X)\nrmse = (np.sqrt(mean_squared_error(val_y, cbr_pred)))\nr2 = r2_score(val_y, cbr_pred)\nprint('Testing performance')\nprint('RMSE: {:.2f}'.format(rmse))\nprint('R2: {:.2f}'.format(r2))","69f4c411":"print(\"Mean Absolute Error: \" + str(mean_absolute_error(val_y,cbr_pred)))\n","a4b61eda":"# to check how close this prediction is to the real data\n\ntrain_nitrogen= (cbr.predict(x)+xgb.predict(x)+rf.predict(x))\/3","313d818c":"# to check how close this prediction is to the real data","b6fb85db":"# model is doing good lets do it in target variable","657145a5":"target_nitrogen_oxides= (cbr.predict(X)+xgb.predict(X)+rf.predict(X))\/3","6f075f60":"target_nitrogen_oxides","e19e111c":"submission= pd.DataFrame({\"date_time\": test_date_time,\n                         \"target_carbon_monoxide\":target_carbon_monoxide,\n                          \"target_benzene\":target_benzene,\n                          \"target_nitrogen_oxides\":target_nitrogen_oxides})\nsubmission.to_csv('submission.csv',index=False)","b0918569":"# now we will select the target","c26bf0cf":"# splitting the data (train and test)","b0b8d336":"# RF MODEL","3822754e":"# Now, submitting the file.","114f3aca":"#  catboost regressor model","1c9f5de5":"#  XG BOOST MODEL-","ee178181":"## Loading the libraries","9d9dd94c":"#  Random forest model","b1a6d283":"# selecting the features","d9f0ffb9":"#  CAT MODEL","70c0e7ab":"## XGboost Regressor Model"}}