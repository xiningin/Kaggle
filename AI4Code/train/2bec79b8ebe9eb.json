{"cell_type":{"01cb81ba":"code","9cee89c6":"code","cfed744a":"code","0c5d87b6":"code","d131bf62":"code","d3e2904c":"code","a11c4776":"code","7979040e":"code","f9261ebb":"code","50c6e6bf":"code","05ad5c97":"code","29e4d647":"code","abe80cdb":"code","49bfd553":"code","1701512e":"code","8eb356d2":"code","825ef324":"code","4e9d119e":"code","a591074a":"code","3722286b":"code","8e720588":"code","98ae825d":"code","77783cd5":"code","99ae572d":"markdown"},"source":{"01cb81ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9cee89c6":"import matplotlib.pyplot as plt, seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold,\\\nGridSearchCV, RandomizedSearchCV\nfrom sklearn.decomposition import PCA, IncrementalPCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom xgboost import XGBRegressor","cfed744a":"df = pd.read_csv('\/kaggle\/input\/yeh-concret-data\/Concrete_Data_Yeh.csv')\ndf.head()","0c5d87b6":"df.info()","d131bf62":"sns.pairplot(df)","d3e2904c":"df.plot(x='csMPa', kind='box', subplots=True, layout=(2,4), figsize=(20,10))\nplt.show()","a11c4776":"# Removing outlier for age feature\nQ3 = df.age.quantile(0.75)\nQ1 = df.age.quantile(0.25)\nIQR = Q3-Q1\nupper = Q3+(1.5*IQR)\nlower = Q1-(1.5*IQR)\n\ndf = df[(df.age>=lower) & (df.age<=upper)]\ndf.info()","7979040e":"df.age.plot.box()","f9261ebb":"# Splitting data into train and test data sets\nX = df.drop('csMPa',axis=1)\ny = df.csMPa\n\nX_train,X_test, y_train,y_test = train_test_split(X,y, train_size=0.7, random_state=100)","50c6e6bf":"# Performing PCA\npca_comps = [2,3,4,5,6,7]\nevr = []\n\nfor i in pca_comps:\n    pca = PCA(n_components=i)\n    pca.fit_transform(X_train)\n    evr.append(sum(pca.explained_variance_ratio_))\n    \nplt.plot(pca_comps, evr, marker='o')\nplt.grid(axis='y', alpha=0.7)\nplt.show()","05ad5c97":"# Ridge Regression Model\nridge = Pipeline([('scaler', MinMaxScaler()),\n                 ('pca', IncrementalPCA()),\n                 ('ridge', Ridge())])\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=56)\nhyp = {'ridge__alpha':[0.00000001,0.000001,0.0001,0.01,0.1,0.15,0.18,0.2,0.24,0.3,0.6,0.8],\n      'pca__n_components':[2,3,4,5,6,7,8]}\n\ngrid_ridge = GridSearchCV(estimator=ridge, param_grid=hyp, cv=folds, n_jobs=-1, scoring='r2',\n                   verbose=1, return_train_score=True)\ngrid_ridge.fit(X_train,y_train)\n\ngrid_ridge.best_score_","29e4d647":"ridge_model = grid_ridge.best_estimator_\nridge_model","abe80cdb":"# XGBoost Model\nxgb = Pipeline([('scaler', MinMaxScaler()),\n                 ('pca', IncrementalPCA()),\n                 ('xgb', XGBRegressor(random_state=50))])\n\ncv_score = cross_val_score(estimator=xgb, X=X_train, y=y_train, cv=folds, scoring='r2')\ncv_score.mean()","49bfd553":"# Hyperparameter tuning of XGBoost model using RandomsearchCV\nhyp = [{'pca__n_components':[2,3,4,5,6,7,8],\n       'xgb__n_estimators':[100,200,300,400],\n       'xgb__learning_rate':[0.05,0.08,0.1,0.2,0.4,0.6,0.8],\n       'xgb__gamma':[0.000001,0.0001,0.01,0.1,0.3,0.5,0.8],\n       'xgb__reg_lambda':[0.000000001,0.000001,0.001,0.01,0.1,0.5,0.7]}]\n\nrnd = RandomizedSearchCV(estimator=xgb, param_distributions=hyp, n_iter=50, n_jobs=-1,\n                        cv=folds, scoring='r2', verbose=1, random_state=10)\nrnd.fit(X_train,y_train)","1701512e":"rnd.best_score_, rnd.best_params_","8eb356d2":"# Performing GridsearchCV corresponding to the results of RandomsearchCV\nxgb = Pipeline([('scaler', MinMaxScaler()),\n                 ('pca', IncrementalPCA(n_components=8)),\n                 ('xgb', XGBRegressor(n_estimators=400,random_state=50))])\n\ngrid_hyp = {'xgb__reg_lambda': [0.09,0.10,0.11],\n            'xgb__learning_rate': [0.07,0.08,0.09,0.1],\n            'xgb__gamma': [0.000001,0.00001,0.0000001]}\n\ngrid_xgb = GridSearchCV(estimator=xgb, param_grid=grid_hyp, n_jobs=-1, cv=folds,\n                       scoring='r2', verbose=1)\ngrid_xgb.fit(X_train,y_train)\n\ngrid_xgb.best_score_","825ef324":"xgb_model = grid_xgb.best_estimator_\nxgb_model","4e9d119e":"# StackingRegressor Model\nmodels = [('ridge', ridge_model), ('xgb', xgb_model)]\n\nlr = LinearRegression()\nstk = StackingRegressor(estimators=models, final_estimator=lr)\n\ncv_score = cross_val_score(estimator=stk, X=X_train, y=y_train, cv=folds, scoring='r2')\ncv_score.mean()","a591074a":"# Training and prediction\nstk.fit(X_train,y_train)\ny_pred = stk.predict(X_test)","3722286b":"r2 = r2_score(y_test, y_pred)\nRMSE = (mean_squared_error(y_test, y_pred))**0.5\nprint('Validation R-squared score = {0}'.format(round(r2,2)))\nprint('RMSE = {0}'.format(round(RMSE,2)))","8e720588":"pred = pd.DataFrame({'actual':y_test, 'pred':y_pred})\npred['error'] = pred.actual - pred.pred\npred = pred.sort_index()\npred.head()","98ae825d":"plt.figure(figsize=(20,10))\nplt.plot(pred.actual, label='actual')\nplt.plot(pred.pred, label='predicted')\nplt.xlabel('index', fontsize=20)\nplt.ylabel('csMPa', fontsize=20)\nplt.legend(fontsize=20)\nplt.show()","77783cd5":"plt.figure(figsize=(20,8))\nplt.scatter(pred.index,pred.error, color='red')\nplt.axhline(0)\nplt.xlabel('index', fontsize=20)\nplt.ylabel('errors', fontsize=20)\nplt.show()","99ae572d":"### Stacking Regressor combining the before two models performs better."}}