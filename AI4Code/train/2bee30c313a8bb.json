{"cell_type":{"a62cce59":"code","af9f388c":"code","af4e6712":"code","8bf01988":"code","033b77b8":"code","93c2061b":"code","affb82a5":"code","528cfa99":"code","fc5d0a00":"code","669aed33":"code","5ff5c737":"code","15d788da":"code","3c5aa227":"code","0a1dabec":"code","ddc818a9":"code","a7a00936":"code","48c067de":"code","552a1b82":"code","aab49873":"code","7af592fe":"code","15d763d4":"code","86bcfc00":"code","be75e268":"code","812570c4":"code","04aae578":"code","f7f33988":"code","18878cb3":"code","fde3fe39":"code","2d8bca27":"code","7797a714":"code","14526d82":"code","9566652f":"code","7731050c":"code","889a66fd":"code","d32d98be":"code","f1f5d6a0":"code","5fb58db6":"code","eb0119b4":"code","980f98e4":"code","45472805":"code","4e2fceac":"code","70d21976":"code","141b0ad0":"code","7efdd585":"markdown","892efb47":"markdown","ba33967c":"markdown","f55c600f":"markdown","741a54b2":"markdown","cec110d0":"markdown","380eea9d":"markdown","b5dae444":"markdown","23020fae":"markdown","b414c126":"markdown","82e4888a":"markdown","c0826502":"markdown","7a75e13b":"markdown","1cb2c604":"markdown","e7c84c1e":"markdown","90956923":"markdown","eb9d2249":"markdown","2302fae6":"markdown","95cb510a":"markdown","173e9f99":"markdown","b43bd86c":"markdown","bd50691a":"markdown","a62d530e":"markdown","ed8692ac":"markdown","ce1e4500":"markdown","4f1630bb":"markdown","4ef9f8f7":"markdown","6ecbfe26":"markdown","250f115b":"markdown"},"source":{"a62cce59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","af9f388c":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain_data.head()","af4e6712":"test_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest_data.head()","8bf01988":"train_data.describe()","033b77b8":"test_data.describe()","93c2061b":"data_dt = train_data.append(test_data)\ndata_dt.describe()","affb82a5":"data_dt.isna().sum()","528cfa99":"data_dt.drop(['Cabin'], axis=1, inplace=True)\n\ndata_dt.head()","fc5d0a00":"data_dt_proc = data_dt.copy()\n\nprint(\"Mediana de idade: {}\".format(data_dt_proc.Age.median()))\ndata_dt_proc.Age.fillna(data_dt_proc.Age.median(), inplace=True)","669aed33":"data_dt_proc.head()","5ff5c737":"data_dt_proc.Fare.fillna(data_dt_proc.Fare.median(), inplace=True)","15d788da":"data_dt_proc.isna().sum()","3c5aa227":"data_dt_proc.drop(['Ticket', 'PassengerId'], axis=1, inplace=True)","0a1dabec":"data_dt_proc['Title'] = data_dt_proc['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\ndata_dt_proc.head()","ddc818a9":"data_dt_proc['Title'].value_counts()\n","a7a00936":"print( len(data_dt_proc['Title'].value_counts()) )","48c067de":"Title_Dictionary = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Royalty\",\n    \"Dona\" : \"Mrs\"\n}\n\ndata_dt_proc['Title'] = data_dt_proc.Title.map(Title_Dictionary)\n\ndata_dt_proc['Title'].value_counts()","552a1b82":"data_dt_proc['FamilySize'] = data_dt_proc['Parch'] + data_dt_proc['SibSp'] + 1\ndata_dt_proc.head()","aab49873":"grouped = data_dt_proc.groupby(['Sex', 'Pclass', 'Title'])\ngrouped.first()","7af592fe":"grouped_median = grouped.median()\ngrouped_median = grouped_median.reset_index()[['Sex', 'Pclass', 'Title', 'Age']]\n\ngrouped_median","15d763d4":"def fill_age(row, grouped_median):\n    condition = (\n        (grouped_median['Sex'] == row['Sex']) &\n        (grouped_median['Pclass'] == row['Pclass']) &\n        (grouped_median['Title'] == row['Title'])\n    )\n    return grouped_median[condition]['Age'].values[0]    \n\ndata_dt['Title'] = data_dt_proc['Title'] \n\n\nprint(\"Idade com mediana: {}\".format(data_dt_proc['Age'].iloc[65]))\n\ndata_dt_proc['Age'] = data_dt.apply(lambda row: fill_age(row,grouped_median) if np.isnan(row['Age']) else row['Age'], axis=1)\n\nprint(\"Mediana agrupada: {}\".format(data_dt_proc['Age'].iloc[65]))","86bcfc00":"data_dt[(data_dt['Title'] == \"Master\") & data_dt['Age'].isna() ]","be75e268":"data_dt[(data_dt['Title'] == \"Master\") ]","812570c4":"data_dt.drop(['Title'], axis=1, inplace=True)\n\ndata_dt_proc.drop(['Name'], axis=1, inplace=True)\n","04aae578":"data_dt_proc_pre_dummies = data_dt_proc.copy()\ndata_dt_proc = pd.get_dummies(data_dt_proc)\ndata_dt_proc.head()","f7f33988":"data_dt_proc['Pclass_1'] = pd.get_dummies(data_dt_proc['Pclass'])[1]\ndata_dt_proc['Pclass_2'] = pd.get_dummies(data_dt_proc['Pclass'])[2] \ndata_dt_proc['Pclass_3'] = pd.get_dummies(data_dt_proc['Pclass'])[3]\n\ndata_dt_proc.head()","18878cb3":"from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n\nx = np.concatenate([np.ones(25), np.ones(10), np.zeros(25), np.zeros(40)])\ny = np.concatenate( [np.ones(25), np.zeros(10), np.ones(25), np.zeros(40)])\n\nprint(\"Acur\u00e1cia {:.2f}\".format(accuracy_score(y,x)))\nprint(\"Precis\u00e3o {:.2f}\".format(precision_score(y,x)))\nprint(\"Recall {:.2f}\".format(recall_score(y,x)))\nprint(\"F1 {:.2f}\".format(f1_score(y,x)))","fde3fe39":"data_dt_proc.head()","2d8bca27":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.figure(figsize=(10,10))\nsns.countplot(x='Survived',data=train_data)","7797a714":"survived = train_data[\"Survived\"][:891]\nrate_survived = sum(survived)\/len(survived) * 100\n\nprint(\"Classificador Zero Rule:\")\nprint(\"% de sobreviventes: {:.2f}\".format(rate_survived))\nprint(\"% de mortos: {:.2f}\".format(100-rate_survived))","14526d82":"print(\"Classificador One Rule:\")\n\n\nwomen = train_data[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)*100\n\nprint(\"% de mulheres que sobrevivem: {:.2f}\".format(rate_women))","9566652f":"data_dt_proc.head()","7731050c":"data_dt_proc_pre_dummies.head()","889a66fd":"data_dt_proc_pre_dummies['Sex'] = pd.factorize(data_dt_proc_pre_dummies['Sex'])[0]\ndata_dt_proc_pre_dummies['Embarked'] = pd.factorize(data_dt_proc_pre_dummies['Embarked'])[0]\ndata_dt_proc_pre_dummies['Title'] = pd.factorize(data_dt_proc_pre_dummies['Title'])[0]","d32d98be":"plt.figure(figsize=(10,10))\nsns.heatmap(data_dt_proc_pre_dummies.corr(), annot=True, cmap=\"Blues\")\n","f1f5d6a0":"pd.DataFrame(data_dt_proc.corr()['Survived'][:])","5fb58db6":"y = train_data['Survived']\n\ndata_dt_proc.drop(['Survived'], axis=1, inplace=True)\n\nfeatures_all = [\"Pclass\", \"Age\", \"Pclass_1\",\"Pclass_2\",\"Pclass_3\", \"Fare\", \"Sex_female\",\"Sex_male\", \"SibSp\", \"Parch\", 'FamilySize', 'Embarked_C', 'Embarked_Q', 'Embarked_S', \n            'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Officer', 'Title_Royalty']\n\nfeatures_selected = [\"Pclass_1\",\"Pclass_2\",\"Pclass_3\", \"Sex_female\",\"Sex_male\", \"Fare\", 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Officer', 'Title_Royalty']\n\nfeatures = [\"Pclass_1\",\"Pclass_2\",\"Pclass_3\", \"Sex_female\",\"Sex_male\", \"Fare\", \"Parch\", 'FamilySize', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n\n\ntrain_data_all = data_dt_proc[features_all][:891]\ntest_data_all = data_dt_proc[features_all][891:]\n\ntrain_data_selected = data_dt_proc[features_selected][:891]\ntest_data_selected = data_dt_proc[features_selected][891:]\n\ntrain_data_proc = data_dt_proc[features][:891]\ntest_data_proc = data_dt_proc[features][891:]\n\ny_test =  pd.read_csv(\"..\/input\/titanic-leaked\/titanic.csv\")\ny_test = y_test['Survived']","eb0119b4":"from xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.metrics import accuracy_score\n\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\ndef run_all_classifiers(x, y, x_test, y_test):\n    classifiers = {    \n        'knn': KNeighborsClassifier(1),\n        'svm': SVC(probability=True),\n        'decision_tree' : DecisionTreeClassifier(),\n        'random_forest' : RandomForestClassifier(n_estimators=50, max_depth=5, random_state=1),\n        'ada_boost' : AdaBoostClassifier(),\n        'gradient_boost' : GradientBoostingClassifier(),\n        'xgboost': XGBClassifier(),\n        'gaussian_nb' : GaussianNB(),\n        'linear_disc' : LinearDiscriminantAnalysis(),\n        'quadratic_disc' : QuadraticDiscriminantAnalysis(),\n        'log_regression' : LogisticRegression(verbose=False),\n        'mlp' : MLPClassifier(solver='adam', alpha=1e-3, hidden_layer_sizes=(12,1), random_state=1, max_iter=500,verbose=False)\n    }\n\n    best_acc = {'classifier' : None,\n                'accuracy' : 0.0}\n\n    all_results = {}\n\n\n    for clf in classifiers:\n        clf_model = classifiers[clf]\n        clf_model.fit(x, y)\n        y_pred = clf_model.predict(x_test)\n\n        all_results[clf] = y_pred\n\n        acc = accuracy_score(y_pred, y_test) * 100\n\n        if acc > best_acc['accuracy']:\n            best_acc['accuracy'] = acc\n            best_acc['classifier'] = clf\n\n        print(\"Acur\u00e1cia de {}: {:.2f}%\".format(clf, acc))\n\n    print(\"\\nO melhor classificador foi {} com acur\u00e1cia de {:.2f}%\".format(best_acc['classifier'], best_acc['accuracy']))\n    \n    return all_results, best_acc","980f98e4":"print(\"Resultado com todas as caracter\u00edsticas: \")\nall_results_all, best_acc_all = run_all_classifiers(train_data_all, y, test_data_all, y_test)","45472805":"print(\"Resultado com as caracter\u00edsticas selecionadas apenas via correla\u00e7\u00e3o: \")\nall_results_selected, best_acc_selected = run_all_classifiers(train_data_selected, y, test_data_selected, y_test)","4e2fceac":"print(\"Resultado com as caracter\u00edsticas selecionadas manualmente: \")\nall_results_proc, best_acc_proc = run_all_classifiers(train_data_proc, y, test_data_proc, y_test)","70d21976":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': all_results_proc[best_acc_proc['classifier']] })\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Arquivo de submiss\u00e3o gerado com sucesso!!\")","141b0ad0":"y_gender =  pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ny_gender = y_gender['Survived']\n\nacc_gender = round(accuracy_score(y_gender, y_test) * 100, 2)\n\nprint(\"Acur\u00e1cia com Genero: {}\".format(acc_gender))\nprint(\"Diferen\u00e7a com proc de {:.2f}%\".format( best_acc_proc['accuracy'] - acc_gender ))\nprint(\"Diferen\u00e7a com selected de {:.2f}%\".format( best_acc_selected['accuracy'] - acc_gender ))\nprint(\"Diferen\u00e7a com all de {:.2f}%\".format( best_acc_all['accuracy'] - acc_gender ))","7efdd585":"Podemos ver na tabela abaixo com todos os registros com 'Title' == 'Master' que a mediana de 7 anos faz muito mais sentido.","892efb47":"Vamos excluir a vari\u00e1vel 'Cabin', visto que h\u00e1 uma grande quantidade de inst\u00e2ncias com valores vazios, dessa forma essa vari\u00e1vel dificilmente apresenter\u00e1 uma informa\u00e7\u00e3o relevante.\nPara isso basta usar a fun\u00e7\u00e3o .drop(), com o par\u00e2metro axis=1 digo que excluirei a coluna, j\u00e1 o inplace informa que o DataFrame ser\u00e1 sobreescrito pelo resultado do .drop()","ba33967c":"<h3>Ainda h\u00e1 informa\u00e7\u00f5es que pouca agregam no treinamento do meu modelo?<\/h3>\n\nRetiramos todas!","f55c600f":"# Limpeza dos dados\n\nAlgumas vari\u00e1veis tem valores unicos para cada passageiro, que podem n\u00e3o representar nenhuma informa\u00e7\u00e3o relevante, por isso iremos remov\u00ea-las tamb\u00e9m.","741a54b2":"<h3>Posso criar mais vari\u00e1veis para meu treinamento?<\/h3>\n\nCriamos v\u00e1rias, mas com certeza ainda \u00e9 poss\u00edvel manipular mais os dados. Mas por hora manteremos assim.","cec110d0":"# Voltando ao tratamento de valores vazios\n\nDurante o processo de tratamento de dados podemos voltar atr\u00e1s e melhorar nosso processo.\n\u00c9 poss\u00edvel melhorar o preenchimento de valores vazios da coluna 'Age', utilizar apenas a m\u00e9dia parece uma forma muito simples de resolver o problema. Talvez identificar pequenos grupos e usar a m\u00e9dia de cada um possa ser mais interessante.\nA chance de um passageiro com 'Age' vazio ter a mesma idade que um outro com as mesmas caracter\u00edsticas \u00e9 muito maior e real.\n\nPor isso, vamos tentar identificar pequenos grupos baseados em informa\u00e7\u00f5es completas como sexo ('Sex'), classe no navio ('Pclass') e t\u00edtulo ('Title'). A fun\u00e7\u00e3o .groupby() faz esse agrupamento.","380eea9d":"<h3>Qual o valor m\u00ednimo de acertos que o meu modelo deve ter?<\/h3>\n\n* Classificador sem regra (Zero Rule): todas as sa\u00eddas na label com mais ocorr\u00eancias.\n* Classificador uma regra (One Rule): uso apenas uma das entradas (vari\u00e1veis) para classificar a sa\u00edda.","b5dae444":"# Prepara\u00e7\u00e3o dos dados\n\n1. Identifica\u00e7\u00e3o das Vari\u00e1veis\n* Limpeza de dados\n* Tratamento de valores vazios\n* Cria\u00e7\u00e3o de Vari\u00e1veis\n* Transforma\u00e7\u00e3o de vari\u00e1veis\n* <p style=\"color:red\"><del>Identifica\u00e7\u00e3o de anomalias<\/del><\/p>","23020fae":"# Treinamento do modelo <\/h1>","b414c126":"# An\u00e1lise explorat\u00f3ria dos dados\n\nA an\u00e1lise dos dados tamb\u00e9m \u00e9 uma etapa importante, pois permite que fa\u00e7amos a extra\u00e7\u00e3o de informa\u00e7\u00f5es importantes que podem nos auxiliar no treinamento do modelo. Informa\u00e7\u00f5es como:\n\n* Qual m\u00e9trica devo utilizar?\n* Qual o valor m\u00ednimo de acertos que o meu modelo deve ter?\n* Ainda h\u00e1 informa\u00e7\u00f5es que pouca agregam no treinamento do meu modelo?\n* Posso criar mais vari\u00e1veis para meu treinamento?\n* Devo usar todas as caracter\u00edsticas (vari\u00e1veis) no treinamento?\n\n<h3>Qual m\u00e9trica devo utilizar?<\/h3>\n\nEm geral as m\u00e9tricas s\u00e3o baseadas na matriz de confus\u00e3o.\n\n![](https:\/\/miro.medium.com\/max\/490\/0*cBSeArnwoU_FRso7.gif)\n\nNessa caso o Kaggle j\u00e1 n\u00f3s d\u00e1 a informa\u00e7\u00e3o: Acur\u00e1cia!\n\n![](https:\/\/miro.medium.com\/max\/700\/1*s7VB26Cfo1LdVZcLou-e0g.png)\n\n![](https:\/\/miro.medium.com\/max\/500\/1*t1vf-ofJrJqtmam0KSn3EQ.png)\n\n* Acur\u00e1cia: indica uma performance geral. Dentre todas as classifica\u00e7\u00f5es, quantas est\u00e3o certas.\n* Precis\u00e3o: de todos os dados classificados como positivos, quantos realmente s\u00e3o positivos.\n* Recall: qual a porcentagem de dados classificados como positivos comparado com a quantidade real de positivos.\n* F1-Score: une precis\u00e3o e recall em uma m\u00e9trica geral.\n\n","82e4888a":"Com a fun\u00e7\u00e3o .value_counts() podemos ver a soma para cada valor \u00fanico da nova coluna criada.\nComo podemos ver abaixo, existem diversas abrevia\u00e7\u00f5es para o mesmo t\u00edtulo. Podemos agrupar algumas dessas informa\u00e7\u00f5es.","c0826502":"Muitas vezes podemos criar novas vari\u00e1veis atrav\u00e9s de informa\u00e7\u00f5es contidas em outras vari\u00e1veis que aparentemente eram in\u00fateis.\nUm exemplo disso \u00e9 a vari\u00e1vel 'Name', cada passageiro tem um nome \u00fanico, por\u00e9m acompanhado de um t\u00edtulo ou pronome de tratamento.\nPodemos extrair esse pronome de tratamento da coluna 'Name' e utiliza-lo no treinamento de nosso modelo de Machine Learning.","7a75e13b":"<h1>Comparando com um classificador por gen\u00earo<\/h1>\n\nVamos comparar nosso resultado com o baseline que haviamos definido: classificador de uma regra - Todas as mulheres sobrevivem.","1cb2c604":"# Defini\u00e7\u00e3o do problema e Aquisi\u00e7\u00e3o dos dados\n\nPara esse projeto o problema e os dados s\u00e3o nos dado pelo Kaggle. Nosso \u00fanico trabalho \u00e9 tratar os dados e desenvolver um modelo.\n\n<h3>Qual \u00e9 o problema?<\/h3>\n\nO naufr\u00e1gio do Titanic \u00e9 um dos mais famosos da hist\u00f3ria, inclusive um grande sucesso do cinema. \nEm 15 de abril de 1912, durante sua viagem de inaugura\u00e7\u00e3o, o Titanic afundou depois de colidir com um iceberg, matando 1502 dos 2224 passageiros e tripula\u00e7\u00e3o. Essa tr\u00e1gedia chocou o mundo e levou na melhoria de leis na navega\u00e7\u00e3o.\n\nUm das raz\u00f5es que levou a tantas mortes foi a falta de botes suficientes para passageiros e tripula\u00e7\u00e3o. Al\u00e9m disso, alguns elementos que envolvendo sorte tamb\u00e9m contribuiram para a sobreviv\u00eancia, sendo que alguns grupos tinham maior chance de sobreviver, como mulheres, crian\u00e7as e passageiros de alto poder aquisitivo.\n\n<b>Nessa problema, o objetivo \u00e9 aplicar t\u00e9cnicas de Machine Learing para prever quais passageiros sobreviveriam \u00e0 trag\u00e9dia.<\/b>\n\nHabilidades pr\u00e1ticas:\n* Classifica\u00e7\u00e3o bin\u00e1ria\n* Python b\u00e1sico\n\n<h3>Os dados<\/h3>\n\nOs dados s\u00e3o nos fornecidos em forma de planilha no formato .csv, j\u00e1 divididos em dois grupos: treinamento e teste.\nUsaremos os dados de treinamento para construir modelos de Machine Learning. Usaremos dados como idade, sexo e classe social para classificar quais passageiros \"deveriam\" ter sobrevivido.\n\nOs dados fornecidos s\u00e3o:\n\n<table>\n  <tr>\n    <th>Vari\u00e1vel<\/th>\n    <th>Defini\u00e7\u00e3o<\/th>\n    <th>Valor<\/th>\n  <\/tr>\n  <tr>\n    <td>survival<\/td>\n    <td>Sobreviveu<\/td>\n    <td>0 = N\u00e3o e 1 = Sim<\/td>\n  <\/tr>\n  <tr>\n    <td>pclass<\/td>\n    <td>Classe no navio<\/td>\n    <td>1 = 1\u00aa, 2 = 2\u00aa e 3 = 3\u00aa<\/td>\n  <\/tr>\n  <tr>\n    <td>sex<\/td>\n    <td>Sexo<\/td>\n    <td>Feminino ou Masculino<\/td>\n  <\/tr>\n  <tr>\n    <td>age<\/td>\n    <td>Idade<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>sibsp<\/td>\n    <td>N\u00fameros de irm\u00e3os e conjuges a bordo do Titanic<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>parch<\/td>\n    <td>N\u00famero de pais ou filhos abordo do Titanic<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>ticket<\/td>\n    <td>N\u00famero da passagem<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>fare<\/td>\n    <td>Custo da passagem<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>cabin<\/td>\n    <td>N\u00famero da cabine<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>embarked<\/td>\n    <td>Porto que embarcou<\/td>\n    <td>C = Cherbourg, Q = Queenstown, S = Southampton<\/td>\n  <\/tr>\n<\/table>","e7c84c1e":"Calculamos ent\u00e3o a mediana de cada grupo.\nNa segunda linha de c\u00f3digo \u00e9 apenas feito um tratamento para exibir as informa\u00e7\u00f5es que estamos analisando.","90956923":"# Transforma\u00e7\u00e3o de vari\u00e1veis\n\nMuitas vezes \u00e9 preciso transformar algumas vari\u00e1veis, principalmente quando se tratam de vari\u00e1veis categ\u00f3ricas.\nNo caso dessa base de dados, ainda temos como vari\u00e1veis categ\u00f3ricas 'Sex', 'Embarked' e 'Title'.\nA melhor transforma\u00e7\u00e3o para esse tipo de vari\u00e1vel \u00e9 o one hot encoding. Nessa transforma\u00e7\u00e3o cada categoria se torna uma vari\u00e1vel bin\u00e1ria.\nPor exemplo, a categoria Embarked \u00e9 quebrada em 3: Embarked_C, Embarked_Q e Embarked_S. Ent\u00e3o todos os dados s\u00e3o transformados, se um passageiro embarcou em Southampton, sua nova classifica\u00e7\u00e3o ser\u00e1 para Pclass1, Embarked_C, Embarked_Q e Embarked_S, respectivamente, 0, 0 e 1.","eb9d2249":"<h3>Devo usar todas as caracter\u00edsticas no treinamento?<\/h3>\n\nNem sempre todas as vari\u00e1veis ou caracter\u00edsticas representam conhecimento valioso, muitas vezes algumas caracteristicas podem apresentar muito ru\u00eddo o que acaba influenciando no desempenho do classificador. Por isso \u00e9 necess\u00e1rio analisar a importancia de cada caracter\u00edstica.\nUma forma de analisar isso \u00e9 utilizando a correla\u00e7\u00e3o.","2302fae6":"Tamb\u00e9m podemos criar uma vari\u00e1vel relacionada ao tamanho da fam\u00edlia, juntando duas vari\u00e1veis: 'SibSp' (n\u00famero de irm\u00e3os\/conjuges) e 'Parch' (n\u00famero de pais \/ filhos), formando a vari\u00e1vel 'Family Size'.","95cb510a":"# Tratamento de valores vazios\n\nVamos verificar os valores vazios com a fun\u00e7\u00e3o .isna(), acrescida da fun\u00e7\u00e3o .sum() que mostra o total de valores vazios para cada vari\u00e1vel.","173e9f99":"O dicion\u00e1rio Title_Dictionary mostra um mapeamento que podemos aplicar \u00e0 coluna 'Title' criada.\nVamos limitar os t\u00edtulos \u00e0 Officer, Royalty, Mrs, Miss, Mr e Master.\nA fun\u00e7\u00e3o .map() nos auxilia nesse processo.","b43bd86c":"![](https:\/\/miro.medium.com\/max\/1346\/1*s0aMRNsHq7A3bCA9gX_qXQ.png) ","bd50691a":"Identificados as vari\u00e1veis com valores vazios, temos algumas abordagens \u00e0 seguir:\n1. <p style=\"color:red\"><del>Excluir linhas<\/del><\/p>\n2. Excluir colunas\n3. Preencher com a m\u00e9dia ou mediana\n4. Preencher com m\u00e9dia ou mediana baseado em outros atributos","a62d530e":"<a href=\"https:\/\/www.venturus.org.br\/en\/\"><img src=\"https:\/\/s3-sa-east-1.amazonaws.com\/prod-jobsite-files.kenoby.com\/uploads\/venturus-1544703795-vnt-mainpng.png\" ><\/a>\n<br>\n<p >This notebook was specially made for Machine Learning students from Venturus company, located in Campinas, Brazil.\n<br>\n<a href=\"https:\/\/www.venturus.org.br\/en\/\">https:\/\/www.venturus.org.br\/en\/<\/a>\n<\/p>","ed8692ac":"# Identifica\u00e7\u00e3o das vari\u00e1veis\n\nPrimeiramente vamos carregar os dados. Para isso usamos a biblioteca Pandas, qual j\u00e1 \u00e9 preparada para a leitura de diversos tipos de arquivos.\nPara dados tabulares o pandas nos fornece os dados em um DataFrame, uma classe qual facilita a manipula\u00e7\u00e3o dos dados.","ce1e4500":"# Cria\u00e7\u00e3o de vari\u00e1veis","4f1630bb":"<h1>Opa, ent\u00e3o quer dizer que o Gradient Boosting \u00e9 o melhor?<\/h1>\n\n<h1 style='color: red'>N\u00c3O!<\/h1>\n\n* 0.83253 com k-NN: https:\/\/www.kaggle.com\/konstantinmasich\/titanic-0-82-0-83\/notebook\n* 0.85167 com WCG + XGBoost: https:\/\/www.kaggle.com\/cdeotte\/titanic-wcg-xgboost-0-84688 \n\n1.0 (100%) \u00e9 imposs\u00edvel...\n\nManipular os dados \u00e9 o segredo!\nO mesmo vale para texto e imagens!\n\n![](https:\/\/miro.medium.com\/max\/1018\/1*umWjAXc8dY3aMdFGnor8QA.png)\n\n\n<h1>Fa\u00e7a voc\u00ea mesmo: Experimente com imagens<\/h1>\n    \nClassifica\u00e7\u00e3o de flores: https:\/\/www.kaggle.com\/c\/tpu-getting-started\n\n* Cotas de GPU: 40h\/semana\n* Cotas de TPU: 30h\/semana\n\nNotebooks recomendados:\n* Primeira submiss\u00e3o com TPU: https:\/\/www.kaggle.com\/ryanholbrook\/create-your-first-submission\n* Melhorando com uma rede pr\u00e9-treinada: https:\/\/www.kaggle.com\/dimitreoliveira\/flower-classification-with-tpus-eda-and-baseline\n* O poder do data augmentation: https:\/\/www.kaggle.com\/cdeotte\/rotation-augmentation-gpu-tpu-0-96\n\nZera todas as madrugadas de sexta para sab\u00e1do \u00e0 meia-noite (UTC-0)","4ef9f8f7":"Vamos utilizar o DataFrame original para sobreescrever os valores vazios de idade que hav\u00edamos preenchido.\nPara isso criamos uma fun\u00e7\u00e3o que verifica se a linha processada tem o mesma valores que uma das regras, ent\u00e3o aplica o valor de idade mediano.\nUsamos a fun\u00e7\u00e3o .apply() do DataFrame para isso. Essa fun\u00e7\u00e3o aplica uma outra fun\u00e7\u00e3o no DataFrame.\nO lambda nos ajuda a fazer linha a linha, sendo que o valor de idade ser\u00e1 preenchido com a mediana do grupo Y para cada registro X, se a idade for vazia.\n\nPodemos ver um exemplo para o registro 65, anteriormente com a mediana ele havia recebido o valor 28.0, agora recebeu 7.0.","6ecbfe26":"A vari\u00e1vel idade tamb\u00e9m tem um grande n\u00famero de vazios. Entretanto, pelo hist\u00f3rico do naufr\u00e1gio, sabemos que a idade dos passageiros pode ser uma informa\u00e7\u00f5es importante. Por isso, n\u00e3o iremos descarta-l\u00e1, mas sim preenche-la com valores. Uma boa estrat\u00e9gia \u00e9 usar a mediana da idade de todos os passageiros.\nA fun\u00e7\u00e3o .fillna() auxilia no preenchimento dos valores vazios pela mediana.","250f115b":"O DataFrame tem fun\u00e7\u00f5es simples como .describe() que nos mostra est\u00e1tisticas simples dos dados."}}