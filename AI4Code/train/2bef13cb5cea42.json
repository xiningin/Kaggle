{"cell_type":{"5c93144f":"code","a4afd59e":"code","b160224e":"code","e276bbd9":"code","01ab1f9e":"code","5db33e01":"code","904918c3":"code","e486b555":"code","c696c688":"code","e21a3c2a":"code","b4edfa78":"code","43d67ce8":"markdown","1645c7b7":"markdown","9de30050":"markdown","bac76576":"markdown","4be59c8f":"markdown","82df83cc":"markdown","bd796e88":"markdown","ce39a509":"markdown","e1ed0c34":"markdown","a99cb706":"markdown","1bf838a2":"markdown"},"source":{"5c93144f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression","a4afd59e":"raw_df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","b160224e":"np.random.seed(0)\n\nall_independent_variables = [c for c in raw_df.columns if c not in ['Class','Time']]\ntrain_X,test_X,train_y,test_y = train_test_split(raw_df[all_independent_variables],raw_df['Class'],test_size = .3)\n\neda_data = raw_df.iloc[train_X.index]","e276bbd9":"eda_data.describe()","01ab1f9e":"corr = eda_data.corr()\ncorr.style.background_gradient()","5db33e01":"def get_metrics(pred,act,is_train = True):\n    acc = sum(pred == act)\/act.shape[0]\n    recall = sum(pred[act == 1] == act[act == 1])\/act[act == 1].shape[0]\n    precision = sum(pred[pred == 1] == act[pred == 1])\/pred[pred == 1].shape[0]\n    prefix = 'Train' if is_train else 'Test'\n    print(f'{prefix} Acc: {acc:.2%}   Recall: {recall:.2%}   Precision: {precision:.2%}')","904918c3":"corr_class = corr['Class'].loc[all_independent_variables].sort_values(key=lambda x:abs(x),ascending=False)\nindependent_vars = corr_class.index[[0]]\n\ntrain_X_model = train_X[independent_vars]\ntest_X_model = test_X[independent_vars]\n\nlogreg = LogisticRegression()\nlogreg.fit(train_X_model,train_y)\n\ntrain_y_pred = logreg.predict(train_X_model)\ntest_y_pred = logreg.predict(test_X_model)\n\nget_metrics(train_y_pred,train_y,is_train = True)\nget_metrics(test_y_pred,test_y,is_train = False)","e486b555":"independent_vars = corr_class.index[:3]\n\ntrain_X_model = train_X[independent_vars]\ntest_X_model = test_X[independent_vars]\n\nlogreg = LogisticRegression()\nlogreg.fit(train_X_model,train_y)\n\ntrain_y_pred = logreg.predict(train_X_model)\ntest_y_pred = logreg.predict(test_X_model)\n\nget_metrics(train_y_pred,train_y,is_train = True)\nget_metrics(test_y_pred,test_y,is_train = False)","c696c688":"independent_vars = corr_class.index[:]\n\ntrain_X_model = train_X[independent_vars]\ntest_X_model = test_X[independent_vars]\n\nlogreg = LogisticRegression()\nlogreg.fit(train_X_model,train_y)\n\ntrain_y_pred = logreg.predict(train_X_model)\ntest_y_pred = logreg.predict(test_X_model)\n\nget_metrics(train_y_pred,train_y,is_train = True)\nget_metrics(test_y_pred,test_y,is_train = False)","e21a3c2a":"from sklearn.ensemble import RandomForestClassifier","b4edfa78":"%%time\nnp.random.seed(0)\nindependent_vars = corr_class.index#[:3]\n\ntrain_X_model = train_X[independent_vars]\ntest_X_model = test_X[independent_vars]\n\nrf_reg = RandomForestClassifier(n_estimators=100)\nrf_reg.fit(train_X_model,train_y)\n\ntrain_y_pred = rf_reg.predict(train_X_model)\ntest_y_pred = rf_reg.predict(test_X_model)\n\nget_metrics(train_y_pred,train_y,is_train = True)\nget_metrics(test_y_pred,test_y,is_train = False)","43d67ce8":"**Comments**:\n* Wow! We have all of our metrics got higher!\n* Lets try with ALL variables!","1645c7b7":"**Comments**:\n* Random Forest model has the best results until now!","9de30050":"## Random Forest","bac76576":"## Multiple Logistic Regression\n\nThis time, we are trying our logistic regression with 3 variables which have highest correlations with our dependent variable.","4be59c8f":"# Fraud Detection","82df83cc":"## Exploratory Data Analysis","bd796e88":"**Comments**:\n* *V variables* seem totally independent from each other as expected. PCA converts a bunch of variables into less numbered uncorrelated features. \n* *Class* is our dependent variable. Some independent variables have negative correlation with *Class* about 30%. I really wonder what if we run a logistic regression merely with the variable with highest correlation.\n\n","ce39a509":"## Simple Logistic Regression","e1ed0c34":"**Comments**\n* Class\n    * Our dependent variable *Class* has a mean of ~0.002 which means only 2 of 1,000 activities are fradulent. **A prediction model which claims all activities are safe will have an accuracy over 99%!**\n    * However, ignoring fradulent activity is a huge loss for a financial institution. Thus, **our main aim is to get best recall without lowering accuracy too much**.\n    * Recall = of TP\/(TP+FN) i.e. **(Correctly Labeled Frauds)\/(All Frauds)**\n\n","a99cb706":"**Comments**:\n* Wow! Accuracy is already imressive for both training (99.88%) and test sets (99.88%)!\n    * But we have just expected that due to excessive amount of normal activities.\n* We have recall metrics between 35%-40% for training and test sets.\n    * i.e. **If 100 activities are fradulent, our model captures ~37.5% of them**,\n    * The ratio is not so promising. But hey, it is our first trial!\n    * **Unbalanced data** generally leads to kind of low recall.\n* Precisions are around 80%:\n    * i.e. **If our model labeled 100 activities as fradulent, 80% of them really are.**\n    * It seems not so bad.\n* With this high precision and low recall rates, **we can use this model as a basic initial filter for fradulent activities**.\n\nLets try same model with three independent variables!","1bf838a2":"**Comments**:\n* Results are slightly better when all variables used in model."}}