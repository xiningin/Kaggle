{"cell_type":{"e141a5d6":"code","347bdc71":"code","2f8fa762":"code","1ed563fb":"code","c1d570f3":"code","38dab686":"code","b7d98a47":"code","45d1e364":"code","fccb5e8d":"code","6366cff5":"code","0f59d3c7":"code","91c45b6e":"code","8ef1a6bb":"code","8a8a6300":"code","4be3ed44":"code","35d9027c":"code","dcabdcc8":"code","6c08abff":"code","22e62448":"code","ea33d20f":"code","bbb27619":"code","3331ca0e":"code","24d64c42":"code","75513a14":"code","4d1c61ad":"markdown","75e06fc0":"markdown","1146d31c":"markdown","a372cda8":"markdown","33216f1d":"markdown","9cadee7d":"markdown","019e11e7":"markdown","1dd9d231":"markdown","ca7edfea":"markdown","ad8a007a":"markdown","0ddc8310":"markdown","ef39c871":"markdown","a0d2ae99":"markdown","c3d92dc1":"markdown","d6fee8f9":"markdown","489c10bb":"markdown","71ea77a3":"markdown"},"source":{"e141a5d6":"#load libraries for data manipulation and visualization\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb","347bdc71":"# load the data sets\nsales_train_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\nitem_categories = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\n\nprint('Number of Training Samples = {}'.format(sales_train_df.shape[0]))\nprint('Number of Test Samples = {}\\n'.format(test_df.shape[0]))\n\nprint('Training X Shape = {}'.format(sales_train_df.shape))\nprint('Test X Shape = {}'.format(test_df.shape))\nprint('Test y Shape = {}\\n'.format(test_df.shape[0]))\n\nprint('Index of Train set:\\n', sales_train_df.columns)\nprint(sales_train_df.info())\nprint('\\nIndex of Test set:\\n', test_df.columns)\n\n\nprint('\\nMissing values of Train set:\\n', sales_train_df.isnull().sum())\nprint('\\nNull values of Train set:\\n', sales_train_df.isna().sum())","2f8fa762":"# sample train set\nsales_train_df.sample(10)","1ed563fb":"# sample test set\ntest_df.head()","c1d570f3":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\nsales_train_df = downcast_dtypes(sales_train_df)\nprint(sales_train_df.info())","38dab686":"# converting date object to datetime format\nsales_train_df['date'] = pd.to_datetime(sales_train_df['date'],format = '%d.%m.%Y')\nprint('Min date from train set: %s' % sales_train_df['date'].min().date())\nprint('Max date from train set: %s' % sales_train_df['date'].max().date())","b7d98a47":"# print min and max num assigned to the months\nprint('Min date_block_num from train set: %s' % sales_train_df['date_block_num'].min())\nprint('Max date_block_num from train set: %s' % sales_train_df['date_block_num'].max())","45d1e364":"ts=sales_train_df.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nplt.figure(figsize=(15,8))\nplt.title('Total Sales')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts);","fccb5e8d":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsb.boxplot(x=sales_train_df['item_cnt_day'])\nprint('Sale volume outliers:',sales_train_df['item_id'][sales_train_df['item_cnt_day']>900].unique())\n\nplt.figure(figsize=(10,4))\nplt.xlim(sales_train_df['item_price'].min(), sales_train_df['item_price'].max())\nsb.boxplot(x=sales_train_df['item_price'])\nprint('Item price outliers:',sales_train_df['item_id'][sales_train_df['item_price']>300000].unique())","6366cff5":"# remove outliers\nsales_train_df = sales_train_df[(sales_train_df.item_price < 300000 ) & (sales_train_df.item_cnt_day < 900)]","0f59d3c7":"#create a pivot table from train_sales\ntrain_data = sales_train_df.pivot_table(index = ['shop_id','item_id'],values = ['item_cnt_day'],columns = ['date_block_num'],fill_value = 0,aggfunc='sum')\n# reset indices for easy manipulation\ntrain_data.reset_index(inplace = True)\ntrain_data.head()","91c45b6e":"# merge train_data and test_df as to be suitable for prediction\nall_data = pd.merge(test_df,train_data,on = ['item_id','shop_id'],how = 'left')","8ef1a6bb":"# fill all NaN values with 0\nall_data.fillna(0,inplace = True)\n# display\nall_data.head()","8a8a6300":"all_data.drop(['ID','shop_id','item_id'],inplace = True, axis = 1)\nall_data.head()","4be3ed44":"# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the dataset\n        if end_ix > len(sequences):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)","35d9027c":"# create dummy samples\ndummy_samples = [[0] * 34] * 2\n# rename all_data columns to match dummy_samples dataframe\nall_data.columns = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33]\n# create dummy_samples dataframe and concatenate with all_data\nall_data = pd.concat([pd.DataFrame(dummy_samples), all_data], ignore_index=True)\n# print shape of all_data\nall_data.shape","dcabdcc8":"# keep all columns execpt the last two \ntrain_data = np.expand_dims(all_data.values[:,:-2],axis = 2)\n# keep all columns execpt the first and last\nvalidation_data = np.expand_dims(all_data.values[:,1:-1],axis = 2)\n# keep all columns execpt the first two\ntest_data = np.expand_dims(all_data.values[:,2:],axis = 2)\n# print the shapes \nprint(train_data.shape, validation_data.shape, test_data.shape)","6c08abff":"from numpy import array\n# choose a number of time steps\nn_steps = 3\n# convert into input\/output\nX_train, y = split_sequences(train_data, n_steps)\nX_val, y_val = split_sequences(validation_data, n_steps)\nX_test, y_test = split_sequences(test_data, n_steps)\nprint(X_train.shape, y.shape)","22e62448":"X = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\nX_val = X_val.reshape((X_val.shape[0], X_train.shape[1], X_train.shape[2]))\nX_test = X_test.reshape((X_test.shape[0], X_train.shape[1], X_train.shape[2]))","ea33d20f":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM\nimport  tensorflow.keras.optimizers as optimizers\n# number of features\nn_features = X.shape[2]\n# define model\nmodel = Sequential()\nmodel.add(LSTM(64, activation= 'relu', dropout=0.2, recurrent_dropout=0.2, return_sequences=True,  input_shape=(n_steps, n_features)))\nmodel.add(LSTM(64, activation= 'relu', dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1))\nmodel.compile(optimizer=optimizers.Adam(lr=.0001), loss= 'mse', metrics = ['mean_squared_error'])     \nmodel.summary()","bbb27619":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\ncallbacks = [\n    EarlyStopping(patience=5, verbose=1),\n    ReduceLROnPlateau(factor=0.25, patience=2, min_lr=0.000001, verbose=1),\n    ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)\n]","3331ca0e":"# fit model\nmodel.fit(X, y, epochs=15, callbacks=callbacks, validation_data=(X_val, y_val))","24d64c42":"# create submission file \nsubmit = model.predict(X_test)\n# clip between 0 and 20\nsubmit = submit.clip(0,20)\n# creating dataframe with required columns \nsubmission = pd.DataFrame({'ID':test_df['ID'],'item_cnt_month':submit.ravel()})\n# creating csv file from dataframe\nsubmission.to_csv('submit2a.csv',index = False)\n","75513a14":"submission.head()","4d1c61ad":"Split function adapted from the book [deep-learning-for-time-series-forecasting](https:\/\/machinelearningmastery.com\/deep-learning-for-time-series-forecasting\/)","75e06fc0":"The train data is from January 2013 date_block_num 0 to October 2015 date_block_num 33. We are to use the sales record for the past 34 months to predict the 35th month which is November 2015 date_block_num 34.","1146d31c":"The sales_train sample and test samples are organized differently, the sales_train samples records daily sales per shop_id and per item_id while the test sample has an ID for the shop_id and item_id pair neccessary to aggregate monthly sales as required. ","a372cda8":"Downcast the data types to reduce memory consumption","33216f1d":"#### Aggregation\nRestructure the sales_train_df by item_id and shop_id pairs in a format suitable to combine it with the test_df","9cadee7d":"## LSTM\nDefine and run LSTM model","019e11e7":"Creating subsequences using 3 time steps to predict 1 output, this will result in the loss of the 1st 2 samples because there are no previous samples to create a lag feature for them. 2 dummy samples filled with zero will be created and added on top of the dataset to prevent loss of the 2 samples for prediction","1dd9d231":"# Introduction\nPredicting future sales based on sequential set of data points measured over time is called time series forecasting. There are about 3 approaches to time series forecasting, use of machine learning, statistical methods and use of neural networks. The best method might depend on the data set, this kernel will explore use of neural networks. The challenge is to predict monthly sales per item_id and shop_id pairs of an enterprise.","ca7edfea":"##  Data Preparation\nTransform time series data to fit a supervised learning model, a model must learn from a series of past observation in order to predict the next sequence.The challenge is a multivariate time series with multiple parrallel input time series where the output is dependent on the input series. The sequence of observations will be transformed into multiple subsamples  with rows for time steps and one series per column into the input\/output pairs suitable for processing.","ad8a007a":"Preserving the temporal structure of a time series forecasting problem is very important in spliting data into train, validation and test.The date_num_block 0 -31 will be used in training while 1 - 32 for validation and 2 - 33 for testing","0ddc8310":"#### Outliers\nIdentify outliers by item_cnt_day and item_price","ef39c871":"## Data Exploration\nLoad and explore data","a0d2ae99":"#### Train, Validate and Test ","c3d92dc1":"Drop columns ID, shop_id, and item_id that are not sequence of observation over a time period. Retain only observations that can be used in creating time series lag features, i.e. features at previous levels to help predicting outcome at a future time.","d6fee8f9":"Plotting total sales of the company indicate seasonal peaks with decreasing trend","489c10bb":"Reshape input to 3 dimentional structure of [samples, timesteps, features]","71ea77a3":"Creating subsamples for training and testing"}}