{"cell_type":{"4b0d595f":"code","172b5387":"code","1e2859e9":"code","a616217d":"code","ed1b6214":"code","3e8aba22":"code","223e5a95":"code","192e8019":"code","0392bb4d":"code","d7fe0a82":"code","b9b12b58":"code","9e3bcbfe":"code","b7eab9af":"code","238b5824":"code","c8b5b75d":"code","237ae881":"code","119485c7":"code","283a3c24":"code","536bff6a":"code","d7ef203d":"code","57cff7bc":"code","c79f7f09":"code","0f2ed31b":"code","90e3a8c9":"code","cc3ba088":"code","1b88f5bd":"code","8e9f2b02":"code","bf94b4ba":"code","5f540eff":"code","b39de018":"code","67d4ab49":"code","99614445":"code","a271f6c8":"code","f22967ac":"code","ccf77e36":"code","607a0e56":"markdown","5a1e4f9a":"markdown","bde30bef":"markdown","2d82786c":"markdown","a8558a0f":"markdown","7cd3341e":"markdown","30b7e7e8":"markdown","ade22e2e":"markdown","a4563f68":"markdown","633e8537":"markdown","f9ec599a":"markdown","b763e82d":"markdown","137c6097":"markdown","8d79d31f":"markdown","a5b56b59":"markdown","fbb56ea8":"markdown","9c666dfa":"markdown","3320b882":"markdown","b52c5ad6":"markdown","3f98b84d":"markdown","c5516760":"markdown","8d6164e9":"markdown","25438928":"markdown","87781e86":"markdown","f8709ea4":"markdown","f26803a7":"markdown","c4777e9d":"markdown","00576065":"markdown","f9a008fa":"markdown","f332849a":"markdown","c5f8c0b7":"markdown","0d38d891":"markdown","953167b3":"markdown","2beee231":"markdown","8c819226":"markdown","8021c4f2":"markdown","2930dbaa":"markdown","53090a5e":"markdown","97c5edd5":"markdown","88f63957":"markdown","d58170a0":"markdown","56a04327":"markdown","c2c4356e":"markdown","ae0ea066":"markdown","1450ceda":"markdown","24eea38a":"markdown"},"source":{"4b0d595f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n!pip install seaborn==0.11.0\nimport seaborn as sns","172b5387":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1e2859e9":"%%time\ndf = pd.read_csv(\"..\/input\/mbti-type-and-digital-footprints-for-reddit-users\/reddit_psychometric_data.csv\")\ndf.head()","a616217d":"df.shape","ed1b6214":"print(\"The number of missing data in the Total data is :\", df.isnull().sum().sum())","3e8aba22":"df.describe()","223e5a95":"df.dtypes","192e8019":"df['mbti_type'].value_counts()","0392bb4d":"#sns.set(font_scale=1.4)\ndf['mbti_type'].value_counts().plot(kind='bar', figsize=(12, 6), rot=0)\nplt.xlabel(\"MBTI_Personality_Type\", labelpad=10)\nplt.ylabel(\"Count of People\", labelpad=10)\nplt.title(\"Count of People Who Received Tips by their Personality\", y=1.02);","d7fe0a82":"lIntro = ['INTP','INFP','INTJ','INFJ','ISTP','ISFP','ISTJ','ISFJ']\nfor i in lIntro:\n    df.mbti_type = df.mbti_type.replace(i, \"Introversion\")\nlExtra = ['ENTP','ENFP','ENTJ','ENFJ','ESTP','ESFP','ESTJ','ESFJ']\nfor i in lExtra:\n    df.mbti_type = df.mbti_type.replace(i, \"Extraversion\")","b9b12b58":"df.head(3)","9e3bcbfe":"df.mbti_type = df.mbti_type.replace({'Extraversion':1,'Introversion':0})","b7eab9af":"df.head()","238b5824":"X = df.iloc[:,1:].values\ny = df.iloc[:,0].values","c8b5b75d":"from sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X) ","237ae881":"from sklearn.decomposition import PCA\n\npca = PCA()\nX = pca.fit_transform(X)","119485c7":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","283a3c24":"from sklearn.metrics import accuracy_score","536bff6a":"from sklearn.linear_model import LogisticRegression\nlrg = LogisticRegression()\nlrg.fit(X_train, y_train)\ny_pred = lrg.predict(X_test)\nprint('Logistic Regression Accuracy' , accuracy_score(y_test, y_pred))","d7ef203d":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_train, y_train)\ny_pred = lda.predict(X_test)\nprint('LinearDiscriminantAnalysis Accuracy' , accuracy_score(y_test, y_pred))","57cff7bc":"from sklearn.naive_bayes import GaussianNB\ngb = GaussianNB()\ngb.fit(X_train, y_train)\ny_pred = gb.predict(X_test)\nprint('Gaussian Na\u00eeve Bayes Accuracy' , accuracy_score(y_test, y_pred))","c79f7f09":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(max_depth=2, random_state=0)\nrfc.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = rfc.predict(X_test)\nprint('Random Forest Classifier Accuracy' , accuracy_score(y_test, y_pred))","0f2ed31b":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nprint('Accuracy Suport Vector Classifier' , accuracy_score(y_test, y_pred))","90e3a8c9":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\ny_pred = dtc.predict(X_test)\nprint('Accuracy Decision Tree Classifier' , accuracy_score(y_test, y_pred))","cc3ba088":"from sklearn.neighbors import KNeighborsClassifier\nknc = KNeighborsClassifier()\nknc.fit(X_train, y_train)\ny_pred = knc.predict(X_test)\nprint('Accuracy KNeighbors Classifier' , accuracy_score(y_test, y_pred))","1b88f5bd":"from keras.models import Sequential\nfrom keras import layers\n\ninput_dim = X_train.shape[1]\n\nmodel = Sequential()\nmodel.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","8e9f2b02":"model.compile(loss='binary_crossentropy', \n               optimizer='adam', \n               metrics=['accuracy'])\nmodel.summary()","bf94b4ba":"history = model.fit(X_train, y_train,\n                     epochs= 30,\n                     verbose=False,\n                     validation_data=(X_test, y_test),\n                     batch_size=10)","5f540eff":"loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","b39de018":"print(history.history.keys())","67d4ab49":"plt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(14, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","99614445":"plot_history(history)","a271f6c8":"print(X_train.shape)\nprint(y_train.shape)\nprint(y_pred.shape)","f22967ac":"my_submission = pd.DataFrame({'mbti': y_test[:], 'predicted_value': y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)\nmy_submission.tail()","ccf77e36":"my_submission.predicted_value.value_counts()","607a0e56":"Naive Bayes is a classification algorithm for binary and multi-class classification problems. The technique is easiest to understand when described using binary or categorical input values.","5a1e4f9a":"# 4. Random Forest Classifier","bde30bef":"Now it's time for evaluation .evaluate() method measure the accuracy of the model. You can do this both for the training data and testing data. We expect that the training data has a higher accuracy then for the testing data. The longer you would train a neural network, the more likely it is that it starts overfitting.\n\nNote that if you rerun the .fit() method, you\u2019ll start off with the computed weights from the previous training. Make sure to compile the model again before you start training the model again. Now let\u2019s evaluate the accuracy model:","2d82786c":"Support vector machines Classifier (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n\nI will use it because of its advantages:\n\n* Effective in high dimensional spaces.\n\n* Still effective in cases where number of dimensions is greater than the number of samples.","a8558a0f":"Our data personalities are distributed as followed : ","7cd3341e":"**Standard Scaler :**\n\nStandardScaler transform your data such that its distribution will have a mean value 0 and standard deviation of 1.\n\n* Standard Scaler is useful for classification.\n* it's transform the data between [-1,1]","30b7e7e8":"Our data contain 3586 rows and 27091 columns (sooo large isn't it)","ade22e2e":"# Objective\n\nOur objectf is to make a predictive model that classifies reddit users as either extraverts or introverts based on the data recording their interaction with various subreddits.","a4563f68":"**Spliting the data into train & test sets**","633e8537":"The first preprocessing step is to divide the dataset into a features set and corresponding personality type. The following script performs this task:","f9ec599a":"Now it is time for training with the .fit() function.\n\nSince the training in neural networks is an iterative process, the training won\u2019t just stop after it is done. You have to specify the number of iterations you want the model to be training. Those completed iterations are commonly called epochs. We want to run it for 100 epochs to be able to see how the training loss and accuracy are changing after each epoch.\n\nAnother parameter you have to your selection is the batch size. The batch size is responsible for how many samples we want to use in one epoch, which means how many samples are used in one forward\/backward pass. This increases the speed of the computation as it need fewer epochs to run, but it also needs more memory, and the model may degrade with larger batch sizes. Since we have a small training set, we can leave this to a low batch size:","b763e82d":"# The Personalities Types\n\n![The Personalities Types](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/1f\/MyersBriggsTypes.png)","137c6097":"# 3. Gaussian Naive Bayes","8d79d31f":"# 7. KNeighbor Classifier","a5b56b59":"A good way to see when the model starts overfitting is when the loss of the validation data starts rising again. This tends to be a good point to stop the model.","fbb56ea8":"# 1. Logistic Regression","9c666dfa":"                                               MBTI Types\n\n![](https:\/\/www.verywellmind.com\/thmb\/h3i7cVWZ0vZPHQ--hn-WDXmRU6g=\/1500x782\/filters:fill(ABEAC3,1)\/the-myers-briggs-type-indicator-2795583_FINAL-5c4b6112c9e77c00014af95f.png)","3320b882":"# Neural Network With Keras","b52c5ad6":"# Understand Our Data","3f98b84d":"# Classification Model Algorithms","c5516760":"Linear discriminant analysis is used as a tool for classification, dimension reduction, and data visualization. It has been around for quite some time now. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results.","8d6164e9":"The next preprocessing step is to divide data into training and test sets. Execute the following script to do so:\n","25438928":"# 5. Support Vector Classifier","87781e86":"we can see that the accuracy of our classification algorithms does not exceed the accuracy of 70%. And if we try to create a deeper model then what will be our precision !","f8709ea4":"# Preprocessing the data","f26803a7":"So we can see clearly that the most existing personality type is INPT :\n\n![](https:\/\/www.verywellmind.com\/thmb\/f53mBgKUJGSHvpymqOtfMPVxlAY=\/700x0\/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)\/intp-introverted-intuitive-thinking-perceiving-2795989-5c2e4533c9e77c0001cb80e9.png)\n\n\nINTP (introverted, intuitive, thinking, perceiving) is one of the 16 personality types described by the Myers-Briggs Type Indicator (MBTI).1\n\n\ufeff People who score as INTP are often described as quiet and analytical. They enjoy spending time alone, thinking about how things work and coming up with solutions to problems. INTPs have a rich inner world and would rather focus their attention on their internal thoughts rather than the external world. They typically do not have a wide social circle, but they do tend to be close to a select group of people. \n  \n* **Popular INTP Careers**\nChemist\nPhysicist\nComputer programmer\nForensic scientist\nEngineer\nMathematician\nPharmacist\nSoftware developer\nGeologist","c4777e9d":"# Import Libraries Needed","00576065":"Before we begin on building our model, we need to know the input dimension of our feature vectors. This happens only in the first layer since the following layers can do automatic shape inference. In order to build the Sequential model, you can add layers one by one in order as follows:","f9a008fa":"For a model trained on a classification problem with a validation dataset, this might produce the listing above :","f332849a":"PCA\u2019s goal is to reduce the curse of dimensionality. It will reduce the features in such a way that it retains most principal information of the features in its principal components.","c5f8c0b7":"The Random Forest Classifier is a set of decision trees from randomly selected subset of training set. It aggregates the votes from different decision trees to decide the final class of the test object.","0d38d891":"**Types of data**","953167b3":"\nIntroversion : INTP\n\n               INFP\n               INTJ\n               INFJ\n               ISTP    \n               ISFP\n               ISTJ\n               ISFJ\n    \n  \nExtraversion : ENTP \n\n               ENFP    \n               ENTJ         \n               ENFJ         \n               ESTP          \n               ESFP     \n               ESTJ     \n               ESFJ \n    ","2beee231":"**Principal Component Analysis**\n\nThis method combines highly correlated variables together to form a smaller number of an artificial set of variables which is called \"principal components\" that account for most variance in the data.","8c819226":"# Prepare Submission File","8021c4f2":"You can use this little helper function to visualize the loss and the accuracy for the training and testing data both based on the History callback. This callback, which is automatically applied to each Keras model, records the loss and additional metrics that can be added in the .fit() method. In this case, we are only interested in the accuracy. We will try to complete the task by using the matplotlib plotting library:","2930dbaa":"**Checking the Missing Data**","53090a5e":"K-Nearest Neighbor is a supervised learning algorithm that can be used for regression as well as classification problems. But KNN is widely used for classification problems in machine learning. KNN works on a principle assuming that every data point falling near to each other will fall in the same class. That means similar things are near to each other.","97c5edd5":"Decision Trees Classifiers are a type of Supervised Machine Learning meaning we build a model, we feed training data matched with correct outputs and then we let the model learn from these patterns. Then we give our model new data that it hasn't seen before so that we can see how it performs. And because we need to see what exactly is to be trained for a Decision Tree, let's see what exactly a decision tree is.\n\n[To see More](https:\/\/programmerbackpack.com\/decision-tree-explained\/)","88f63957":"We will consider that :\n\n    Extraversion  -->  1\n        &\n    Introversion  -->  0","d58170a0":"Let's analys the **accuracy model** : As you can see in the diagram, the accuracy increases unbelievably in the first epoch, indicating that the network is learning fast. Afterwards, the curve flattens indicating that not too many epochs are required to train the model further. Generally, if the training data accuracy (\u201caccuracy\u201d) keeps improving and the validation data accuracy (\u201cval_acc\u201d) gets decreasing which is a good thing.\n\n","56a04327":"so with .compile() will help me to specify my optimizer which will be adam ,and the loss function. In order to configure the learning process.\n\nKeras also includes a handy .summary() function to give an overview of the model and the number of parameters available for training just as below:","c2c4356e":"Logistic Regression is used when the dependent variable(target) is categorical.\n\nLike in our case : Extraversion:1, Introversion:0.","ae0ea066":"# 6. Decision Tree","1450ceda":"# Access Model Training History in Keras\n\nKeras provides the capability to register callbacks when training a deep learning model.\n\nOne of the default callbacks that is registered when training all deep learning models is the History callback. It records training metrics for each epoch. This includes the loss and the accuracy (for classification problems) as well as the loss and accuracy for the validation dataset.\n\nThe history object is returned from calls to the fit() function used to train the model. Metrics are stored in a dictionary in the history member of the object returned.\n\nFor example, you can list the metrics collected in a history object using the following snippet of code after a model is trained:","24eea38a":"# 2. Linear Discriminant Analysis"}}