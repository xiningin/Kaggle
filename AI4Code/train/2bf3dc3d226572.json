{"cell_type":{"6227e1d1":"code","9b187a0c":"code","324c284f":"code","14122a99":"code","1e3dd0c1":"code","6b9cd8e0":"code","8d13e2f5":"code","c23adf09":"code","d238c0b8":"code","9d410f23":"code","e91b326b":"code","5a848ca4":"code","6dd8154c":"code","2ef2651b":"code","d313382f":"code","2411bb15":"code","83d51190":"code","b5edfc7a":"code","6fdae1df":"code","1060be74":"code","47785b4d":"code","370e732d":"code","a7938753":"code","3f58213d":"code","743c84d3":"code","03d0b3dc":"code","a215a086":"code","5a311203":"code","fada0fd6":"code","feff2397":"code","5cd0e1ef":"code","5fe9c824":"code","e5c43d9b":"code","8d9d6cab":"code","dcbff618":"code","1dbfcd21":"code","d6a5e24f":"code","4e27a70f":"code","f77fae7a":"code","17f17f12":"code","b2adc82d":"code","d843579e":"code","1de85244":"code","432cf37e":"code","d835d2b0":"code","2f306208":"code","fa27a4da":"code","9e9ed64b":"code","9c9079ab":"code","b5769c52":"code","c7273ff3":"code","d5f1119d":"code","904a4c18":"code","5fb964ba":"code","43f08e64":"code","d15c95bc":"code","d87891b8":"code","a3700da7":"code","1830a4c9":"code","b22418fa":"code","35a017fb":"code","e54f39b2":"code","c8559506":"code","77ed8a78":"code","194a62d5":"code","6d332b9a":"code","318843ea":"code","fe3961e4":"code","c5717da8":"code","a4d1ce00":"code","30486b58":"code","949b71ce":"code","653cc187":"code","f5af8838":"code","80b3301b":"code","d575d7b0":"code","ad0229cc":"code","b80462ba":"code","806e0d4d":"code","2b4267aa":"code","c7f6b571":"code","220eed08":"code","f11acaef":"code","23a1a45b":"code","99f1e7df":"code","dfaec092":"code","e970522d":"code","f3ca6e69":"code","949d75fe":"code","c10f1d0d":"code","c89de705":"code","314adb6e":"code","73964174":"code","25375541":"code","44d9f431":"code","73c5d7d0":"code","11b6dd31":"code","6c8f171e":"code","22160ecf":"code","b76ec98a":"code","98a98b61":"code","1d03bb9f":"code","0d085a49":"code","e7f910d3":"code","0737ad90":"code","90c68908":"code","38f21b51":"code","b24d64ff":"code","e1c5a207":"code","a3706265":"code","35a9e087":"code","676f05de":"code","7e6c61b7":"code","862c0dd3":"code","b0368f0f":"code","b86e3898":"code","35cececb":"code","1292e942":"code","33cece0c":"code","c56459f9":"code","d37c8a20":"code","b055c5be":"code","1ef9967d":"code","624473f2":"code","032b861d":"code","3d1aa702":"code","18a18943":"code","899c79f1":"markdown","b0dd67f7":"markdown","bc8e5641":"markdown","fbf0db96":"markdown","5857729a":"markdown","d2aed849":"markdown","59e82909":"markdown","71070573":"markdown","58a100f3":"markdown","210aa97d":"markdown","2b45dcc0":"markdown","946d67e5":"markdown","b5254b05":"markdown","42c4c9e1":"markdown","ab1a47cb":"markdown","d67eede7":"markdown","bce04cd5":"markdown","80456ead":"markdown","81f6c21f":"markdown","1d53f4c5":"markdown","980796a8":"markdown","628dbfaa":"markdown","d82dc9e1":"markdown","9ccee028":"markdown","10ac44cb":"markdown","f7944b2c":"markdown","072b5421":"markdown","867f017e":"markdown","f1967d0b":"markdown","28389c2e":"markdown","90eb5252":"markdown","3718a243":"markdown","47e71edf":"markdown","37af6cd5":"markdown","86efd290":"markdown","48f65ee8":"markdown","1b71a819":"markdown","a715cd33":"markdown","4fa50f0d":"markdown","87d41a58":"markdown","67fbf716":"markdown","a93c1fae":"markdown","0183eb35":"markdown","025fa6e1":"markdown","be85f4c3":"markdown","2fde497f":"markdown","49225ff0":"markdown","1dc6028a":"markdown","a976200a":"markdown","be7cd3d9":"markdown","f0d9044c":"markdown","45f2aac3":"markdown","2225e1e7":"markdown","ca96fa1b":"markdown","99d259b6":"markdown","d285a956":"markdown","e522fe90":"markdown","1f46f593":"markdown","3f47dfdf":"markdown","08d17f5c":"markdown","620bd898":"markdown","cc3ab2b1":"markdown","97042367":"markdown","147106f6":"markdown","3b86ea45":"markdown","d19b9aca":"markdown","30bc3e6c":"markdown","63afd63b":"markdown","4104d34c":"markdown","9beb284a":"markdown"},"source":{"6227e1d1":"import numpy as np\nimport pandas as pd\n\nimport missingno\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nimport scipy as sp\nfrom scipy.stats import skew\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9b187a0c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","324c284f":"# Load data desctiption\n# with open(\"\/kaggle\/input\/home-data-for-ml-course\/data_description.txt\", 'r') as f:\n    # print(f.read())","14122a99":"# Load the training dataset\ntrain_df = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/train.csv\")\ntrain_df.head()","1e3dd0c1":"# train_df.info()","6b9cd8e0":"len(train_df)","8d13e2f5":"train_df[train_df.duplicated()]","c23adf09":"train_missing = train_df.isna().sum()\n\ntrain_missing = 100 * (train_missing[train_missing > 0] \/ len(train_df))\ntrain_missing","d238c0b8":"# Load the test dataset\ntest_df = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/test.csv\")\ntest_df.head()","9d410f23":"#test_df.info()","e91b326b":"test_df[test_df.duplicated()]","5a848ca4":"test_missing = test_df.isna().sum()\n\ntest_missing = 100 * (test_missing[test_missing > 0] \/ len(test_df))\ntest_missing","6dd8154c":"fig, ax = plt.subplots(figsize=(10,4))\nsns.distplot(train_df['SalePrice'], bins=30, kde=True, ax=ax)","2ef2651b":"# One way of doing it\nfig, ax = plt.subplots(figsize=(10,4))\nsns.distplot(np.log1p(train_df['SalePrice']), bins=30, kde=True, ax=ax);\n# Perform log transformation \ntrain_df['SalePrice'] = np.log1p(train_df['SalePrice'])","d313382f":"train_df['SalePrice'].isna().sum()","2411bb15":"# Concatenate train\/test datasets\ndf = pd.concat([train_df, test_df], axis=0)","83d51190":"# Change these features into object type\nchange_type = ['MSSubClass','OverallQual','OverallCond','YearBuilt','YearRemodAdd','GarageYrBlt','MoSold','YrSold']\n\nfor col in change_type:\n    df[col] = df[col].astype(\"object\")","b5edfc7a":"# Describe numeric columns\ndf.drop(\"Id\", axis=1).describe(include=['number']).T","6fdae1df":"num_feat = [x for x in df.columns if df[x].dtype !=\"object\"]\n\nnum_feat.remove(\"Id\")","1060be74":"# Correlation between numerical variables\ncorr_matrix = df[num_feat].corr()\nplt.figure(figsize=(16,12))\nsns.heatmap(corr_matrix.T, annot=True, cbar=False, cmap='coolwarm');","47785b4d":"# Correlated variables greater than 0.8\ncorr_matrix = df[num_feat].corr()\nplt.figure(figsize=(12,12))\nsns.heatmap(corr_matrix.T, annot=True, mask= corr_matrix < 0.8 ,cbar=False, cmap='coolwarm');","370e732d":"price_corr_ser = df[num_feat].corr()['SalePrice']\nprice_corr_ser = price_corr_ser.sort_values(ascending=False)\nprice_corr_ser = price_corr_ser.drop(\"SalePrice\")\n\nfig, ax = plt.subplots(figsize=(10,12))\nsns.barplot(x=price_corr_ser.values, y=price_corr_ser.index, palette=\"rocket_r\")\nplt.title(\"Numeric Feature Correlation with Traget Column\");","a7938753":"# Remove one of the highly correlated variables\nhigh_correlated_var = [\"GarageArea\",'1stFlrSF','TotRmsAbvGrd']\ndf = df.drop(high_correlated_var, axis=1)\n\n# Remove it from list of numeric columns\nfor c in high_correlated_var:\n    num_feat.remove(c)","3f58213d":"# Plot distribution of numeric variables\nfig = plt.figure(figsize=(20,20))\n\nfor i in range(len(num_feat)):\n    plt.subplot(14,5, i+1)\n    sns.distplot(df[num_feat[i]], rug=True, hist=False, kde_kws={'bw':0.1})\n    plt.title(num_feat[i])\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Count\")\n    plt.tight_layout()\nfig.show()","743c84d3":"# Visualize relation between numeric features and target column\nfig = plt.figure(figsize=(20,20))\n# numeric_df = num_df.drop('SalePrice', axis=1)\n\nfor i, col in enumerate(df[num_feat].columns):\n    plt.subplot(12,5, i+1)\n    sns.scatterplot(x=df[col], y=df['SalePrice'])\n    plt.tight_layout()\n    \nfig.show()","03d0b3dc":"fig = plt.figure(figsize=(24,15))\n\nplt.subplot(4,3,1)\nsns.distplot(df[\"LotArea\"])\n\nplt.subplot(4,3,2)\nsns.scatterplot(x=\"LotArea\", y=\"SalePrice\", data=df)","a215a086":"df[\"LotArea\"].describe()","5a311203":"# Create binary column 1 if the house has a pool, 0 if not\ndf['isPool'] = df['PoolArea'].apply(lambda x: 0 if x == 0 else 1)\ndf['isPool'] = df['isPool'].astype(\"object\")\ndf = df.drop('PoolArea',axis=1)\nnum_feat.remove(\"PoolArea\")","fada0fd6":"# create a new column where I concatenate all Porch columns\nporch_col = ['OpenPorchSF','EnclosedPorch', '3SsnPorch', 'ScreenPorch']\n\ndf['totalPorch'] = np.zeros(len(df)).reshape(len(df),1)\n\nfor col in porch_col:\n    df['totalPorch'] += df[col]\n    \n# Remove porch col from dataset\nfor c in porch_col:\n    df.drop(c, axis=1, inplace=True)\n\n# Remove it from the list of numerical columns\nto_remove = [\"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\"]\nfor c in to_remove:\n    num_feat.remove(c)\n\n# Add column to the list of numeric\nnum_feat.append(\"totalPorch\")","feff2397":"# Create new columns and drop relevant ones\ndf[\"TotBathAbvGrade\"] = df[\"FullBath\"] + (0.5 * df[\"HalfBath\"])\ndf[\"TotBsmtBath\"] = df[\"BsmtFullBath\"] + (0.5 * df[\"BsmtHalfBath\"])\n\n# Remove columns\nto_remove = [\"FullBath\",\"HalfBath\",\"BsmtFullBath\", \"BsmtHalfBath\"]\n\nfor col in to_remove:\n    df.drop(col, axis=1, inplace=True)\n    num_feat.remove(col)\n\n# Append new ones to the numeric columns\nnum_feat.append(\"TotBathAbvGrade\")\nnum_feat.append(\"TotBsmtBath\")","5cd0e1ef":"# Remove useless numerical column\ndf.drop(\"LotFrontage\", axis=1, inplace=True)\nnum_feat.remove(\"LotFrontage\")","5fe9c824":"# Create a plot again\nfig = plt.figure(figsize=(15,15))\n\nfor i, col in enumerate(num_feat):\n    plt.subplot(12,5, i+1)\n    sns.scatterplot(x=df[col], y='SalePrice', data=df)\n    plt.tight_layout()\n\nfig.show()","e5c43d9b":"# Show missing values\nmissingno.matrix(df[num_feat], figsize=(20,4))","8d9d6cab":"# Remove SalePrice temporary\nnum_feat.remove(\"SalePrice\")","dcbff618":"from sklearn.impute import SimpleImputer\n\nimp = SimpleImputer(missing_values=np.nan, strategy='median')\nimp.fit(df[num_feat])\ndf[num_feat] = imp.transform(df[num_feat])","1dbfcd21":"for col in df[num_feat]:\n    df[col] = df[col].apply(lambda x: np.log1p(x))\n    \n# Append SalePrice back to numeric columns\nnum_feat.append(\"SalePrice\")","d6a5e24f":"# List of categorical columns\ncat_feat = [x for x in df.columns if df[x].dtype == \"object\"]\n\n# Create a multi plot with categorical features\nfig = plt.figure(figsize=(18, 30))\n\nfor i , col in enumerate(cat_feat):\n    plt.subplot(12,5, i+1)\n    sns.boxplot(x=col, y='SalePrice', data=df)\n    plt.ylabel(\"Log() SalePrice\")\n    plt.tight_layout()\n    \nfig.show()","4e27a70f":"cat_missing = df[cat_feat].isna().sum()\n\ncat_missing = 100 * (cat_missing[cat_missing > 0] \/ len(df[cat_feat]))\n\nplt.figure(figsize=(10,5))\nsns.barplot(x= cat_missing.sort_values(ascending=False).values, y= cat_missing.sort_values(ascending=False).index)\nplt.title(\"Missing Categorical Values in %\");","f77fae7a":"# Fill missing values in categorical columns with a string\nfor col in cat_feat:\n    if df[col].isna().sum() > 0:\n        df[col] = df[col].fillna(value=\"NA\")\n    else:\n        continue","17f17f12":"missingno.matrix(df[cat_feat], figsize=(20,4))","b2adc82d":"# for col in cat_missing.columns:\n    # print(f\" Column '{col}' has unique values {df[col].unique()}\")","d843579e":"cat_missing = df[cat_feat].isna().sum()\n\ncat_missing = 100 * (cat_missing[cat_missing > 0] \/ len(df[cat_feat]))\n\ncat_missing","1de85244":"# Creating new series \nis_garage = df['GarageYrBlt'].apply(lambda x: 1 if x != \"NA\" else 0)\n\n# Plot new series\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n\nsns.countplot(is_garage, ax=axes[0])\nsns.boxplot(x=is_garage.values, y='SalePrice', data=df, ax=axes[1])\n\naxes[0].set_xlabel(\"Is Garage\")\naxes[0].set_ylabel(\"SalePrice \")\n\naxes[1].set_xlabel(\"Is Garage\")\naxes[1].set_ylabel(\"SalePrice \");","432cf37e":"to_remove = []\n# Add to the list of columns to remove\nto_remove.append(\"GarageYrBlt\")\n\n# Create new column from GaragYrBlt\ndf['isGarage'] = is_garage.astype('object')\ncat_feat.append(\"isGarage\")","d835d2b0":"df['YearRemodAdd'].unique()\nto_remove.append(\"YearRemodAdd\")","2f306208":"# Create a series of how old a house was when sold\nhow_old = (df['YrSold'].astype(int) - df['YearBuilt'].astype(int))\n\n# New column from \ndf['Old_in_Years'] = pd.Series(how_old)\n# Update to numertic list\nnum_feat.append(\"Old_in_Years\")\n\n# Add columns for remove\nto_remove.append('YrSold')\nto_remove.append('YearBuilt')","fa27a4da":"fig = plt.figure(figsize=(18,10))\n\n# Distribution of new column\nplt.subplot(4,2, 1)\nsns.distplot(df['Old_in_Years'])\nplt.ylabel(\"count\")\n\n# Scatterplot of new column\nplt.subplot(4,2, 2)\nsns.scatterplot(x=df['Old_in_Years'].values, y='SalePrice', data=df)\n\n# Labels\nplt.xlabel(\"Old in Years\")\nplt.ylabel(\"SalePrice\");","9e9ed64b":"to_remove","9c9079ab":"df['Condition1'].value_counts()","b5769c52":"df['Condition2'].value_counts()","c7273ff3":"fig = plt.figure(figsize=(18,10))\n\ncondition1 = df['Condition1'].apply(lambda x: x if x == \"Norm\" else \"Other\")\n\nplt.subplot(3,2, 1)\nsns.countplot(condition1)\n\nplt.subplot(3,2, 2)\nsns.boxplot(x=condition1.values, y='SalePrice', data=df);","d5f1119d":"fig = plt.figure(figsize=(18,10))\n\ncondition2 = df['Condition2'].apply(lambda x: x if x == \"Norm\" else \"Other\")\n\nplt.subplot(4,2, 1)\nsns.countplot(condition2)\n\nplt.subplot(4,2,2)\nsns.boxplot(x=condition2.values, y='SalePrice', data=df);","904a4c18":"ordinal_feat = ['OverallQual','OverallCond','ExterQual','ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure','BsmtFinType1',\n               'BsmtFinType2','HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual','GarageCond','PoolQC']\n\ndf['BsmtExposure'] = df['BsmtExposure'].apply(lambda x: x if x !='No' else \"NA\")\n\nfor col in ordinal_feat:\n    # Remove ordinal from list  \n    cat_feat.remove(col)\n        \n    print(f\" Column '{col}' has unique values {df[col].unique()}\")","5fb964ba":"# Map ordinal columns and change their type\nord_map = {\"NA\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4,\"Ex\":5}\nord_map1 = {\"NA\":0, \"Unf\":1, \"LwQ\":2, \"Rec\":3, \"BLQ\":4, \"ALQ\":5, \"GLQ\":6}\nord_map2 = {\"NA\":0, \"No\":1, \"Mn\":2, \"Av\":3, \"Gd\":4}\n\nfor col in ordinal_feat:\n        \n    if len(df[col].unique()) <= 6 and col !=\"BsmtExposure\":\n        df[col] = df[col].map(ord_map)\n        df[col] = df[col].astype(int)\n        \n    elif col in ['OverallQual', 'OverallCond']:\n        df[col] = df[col].astype(int)\n        \n    elif df[col].name in ['BsmtFinType1', 'BsmtFinType2']:\n        df[col] = df[col].map(ord_map1)\n        df[col] = df[col].astype(int)\n        \n    else:\n        df[col] = df[col].map(ord_map2) \n        df[col] = df[col].astype(int)\n        \nfor col in ordinal_feat:\n    # Append nominal features to the numerical features\n    num_feat.append(col)\n    print(f\" Column '{col}' has unique values :{df[col].unique()}, dtype: {df[col].dtypes}\")\n    ","43f08e64":"# Let's remove some categorical columns we do not need anymore\nfor col in to_remove:\n    df.drop(col, axis=1, inplace=True)\n    cat_feat.remove(col)","d15c95bc":"plt.figure(figsize=(14,12))\n\nsns.heatmap(df[num_feat].corr(), mask= df[num_feat].corr()  < 0.8 , cbar=False, cmap='coolwarm', annot=True);","d87891b8":"corr_to_remove = ['GarageCond', 'FireplaceQu', 'BsmtFinType1','BsmtFinType2','BsmtCond',]\n\nfor col in corr_to_remove:\n    df = df.drop(col, axis=1)\n    num_feat.remove(col)\n    ordinal_feat.remove(col)","a3700da7":"# Create dummy variables \ndummy_df = pd.get_dummies(df[cat_feat], drop_first=True)\n\nfor col in cat_feat:\n    df.drop(col, axis=1, inplace=True)\n    \ndf_with_dummies = pd.concat([df, dummy_df], axis=1)","1830a4c9":"# Split dataframe into test\/train dataset\nclean_train_df = df_with_dummies[df_with_dummies[\"SalePrice\"] > 0].copy()\nclean_test_df = df_with_dummies[df_with_dummies[\"SalePrice\"].isna()].copy()\n\n# Drop SalePrice column from test set\nclean_test_df.drop(\"SalePrice\", axis=1, inplace=True)","b22418fa":"clean_test_df.shape, clean_train_df.shape","35a017fb":"skewed_features = clean_train_df[num_feat].skew().sort_values(ascending=False)\nskewed_features = skewed_features[skewed_features > 0.5]\nskewed_index = skewed_features.index","e54f39b2":"fig = plt.figure(figsize=(12,4), dpi=120)\nskewed_features.sort_values(ascending=False).plot(kind='bar')\nplt.xticks(rotation=45)\nplt.xticks(horizontalalignment=\"right\")\nplt.title(\"Skewed Feature above 0.5 upper limit\")\nplt.tight_layout();","c8559506":"right_skewness_col = ['PoolQC', 'LowQualFinSF']\nfor col in right_skewness_col:\n    clean_train_df.drop(col, axis=1, inplace=True)\n    clean_test_df.drop(col, axis=1, inplace= True)\n    num_feat.remove(col)","77ed8a78":"skewed_index = skewed_index.drop(['PoolQC','LowQualFinSF'])","194a62d5":"for col in skewed_index:\n    q3 = np.quantile(clean_train_df[col], 0.75)\n    q1 = np.quantile(clean_train_df[col], 0.25)\n    iqr = q3 - q1\n    # Upper limit for outliers\n    upper_limit = q3 + (1.5*iqr)\n    col_limit =  clean_train_df[col].apply(lambda x: x <= upper_limit)\n    clean_train_df = clean_train_df[col_limit]","6d332b9a":"clean_train_df.shape","318843ea":"# Save id column for submission\nrow_id = pd.Series(clean_test_df[\"Id\"])\nclean_train_df = clean_train_df.drop(\"Id\", axis=1).astype(\"float64\")\nclean_test_df = clean_test_df.drop(\"Id\", axis=1).astype(\"float64\")","fe3961e4":"from sklearn.model_selection import train_test_split\n\n# Split training dataset into X\/y first\nX = clean_train_df.drop([\"SalePrice\"], axis=1)\ny = clean_train_df['SalePrice']\n\n# Then, split it into training and validation sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","c5717da8":"num_feat.remove('SalePrice')","a4d1ce00":"from sklearn.preprocessing import StandardScaler\nscaled_Xtrain = X_train.copy()\nscaled_Xtest = X_test.copy()\n\nscaler = StandardScaler()\n\nscaled_Xtrain[num_feat] = scaler.fit_transform(scaled_Xtrain[num_feat])\nscaled_Xtest[num_feat] = scaler.transform(scaled_Xtest[num_feat])","30486b58":"scaled_Xtrain.head()","949b71ce":"from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score\n\n# Create function for model evaluation\ndef model_evaluation(algo,algoname):\n    \"\"\"\n    This function  fit and  evaluate \n    given algorithm. It takes 3 arguments:\n    \n    First: algorithm of a choice without parentheses.\n    Second: the name of a algorithm as a string.\n    \"\"\"\n\n    # Fit given model\n    algo.fit(scaled_Xtrain, y_train)\n    y_pred = algo.predict(scaled_Xtest)\n    \n    # Calculate metrics\n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    \n    # R-squared \n    r2score = r2_score(y_test, y_pred)\n    \n    print(f\"**{algoname} Metrics**\")\n    print(f\"**MAE: {mae:}\")\n    print(f\"**RMSE: {rmse:}\")\n    print(f\"**R-squared: {r2score:.2f}%\")\n    \n    return mae, rmse, r2score, y_pred, algo","653cc187":"# Create a base model\nfrom sklearn.linear_model import SGDRegressor\n\nsgd_base_model = SGDRegressor(random_state=101)\n\nsgd_base_mae, sgd_base_rmse, sgd_base_r2score, sgd_y_pred, _ = model_evaluation(sgd_base_model, \n                                                                                \"SGDRegressor\")","f5af8838":"def plot_residuals(y_pred, algoname):\n    \"\"\"\n    Function plots probability and residuals plot\n    \"\"\"\n    residuals = pd.Series(y_test - y_pred, \n                          name=\"residuals\")\n    \n    fig, axes = plt.subplots(ncols=2, \n                             nrows=2, \n                             figsize=(14,4), \n                             dpi=120)\n    # Plot probability\n    sp.stats.probplot(residuals, plot=axes[0,0])\n    # Plot kde\n    sns.distplot(residuals, ax=axes[0,1], hist=False)\n    # Plot residuals\n    sns.scatterplot(x=y_test, y=residuals, ax=axes[1,0])\n    axes[1,0].axhline(y=0, c='red',ls='--')\n    # Plot distribution\n    sns.boxplot(residuals, ax=axes[1,1])\n    plt.tight_layout()","80b3301b":"plot_residuals(sgd_y_pred, \"SGDRegressor\")","d575d7b0":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr_model = GradientBoostingRegressor()\ngbr_base_mae, gbr_base_rmse, gbr_base_r2score, gbr_y_pred, gbr_model = model_evaluation(gbr_model, \n                                                                                        \"GradientBostingRegressor\")","ad0229cc":"plot_residuals(gbr_y_pred, \"Gradient Boosting Regressor\")","b80462ba":"from sklearn.ensemble import RandomForestRegressor\n\nrfr_model = RandomForestRegressor()\nrfr_base_mae, rfr_base_rmse, rfr_base_r2score, rfr_y_pred, rfr_model = model_evaluation(rfr_model, \n                                                                                        \"RandomForestRegressor\")","806e0d4d":"plot_residuals(rfr_y_pred, \"Random Forest Regressor\")","2b4267aa":"from xgboost import XGBRegressor\n\nxgboost_model = XGBRegressor()\nxgboost_base_mae, xgboost_base_rmse, xgboost_base_r2score, xgboost_y_pred, xgboost_model = model_evaluation(xgboost_model, \n                                                                                                            \"Extreme Gradient Boosting\")","c7f6b571":"plot_residuals(xgboost_y_pred, \"Extreme Gradient Boosting\")","220eed08":"from sklearn.neighbors import KNeighborsRegressor\n\nknn_model = KNeighborsRegressor()\nknn_base_mae, knn_base_rmse, knn_base_r2score, knn_y_pred, knn_model = model_evaluation(knn_model, \n                                                                                        \"KNeighborsRegressor\")","f11acaef":"plot_residuals(knn_y_pred, \"KNeighborsRegressor\")","23a1a45b":"# Instantiate StandardScaler and copy dataset\nsc = StandardScaler()\nscaled_X = X.copy()\nscaled_test = clean_test_df.copy()\n\n# Scale the data\nscaled_X[num_feat] = sc.fit_transform(X[num_feat])\nscaled_test[num_feat] = sc.transform(clean_test_df[num_feat])\n\n# Instantiate the final model\n# final_base_model = GradientBoostingRegressor()\n\n# Fit the model\n# final_base_model.fit(scaled_X, y)\n\n# final_predictions = final_base_model.predict(scaled_test)\n\n\n# Make predictions and save it to the dataframe\n# final_base_model_df = pd.DataFrame({\"id\":row_id,\n                                    # \"SalePrice\": np.expm1(final_predictions)})","99f1e7df":"# final_base_model_df.to_csv(\"house_price_final_base_sub.csv\", index=False)","dfaec092":"from sklearn.linear_model import ElasticNetCV\n\nelastic_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1])\n\nel_base_mae, el_base_rmse, el_base_r2score, el_base_y_pred, elastic_model = model_evaluation(elastic_model,\n                                                                                             \"ElasticNetCV\")","e970522d":"plot_residuals(el_base_y_pred, \"ElasticNetCV\")","f3ca6e69":"elastic_model.l1_ratio_","949d75fe":"from sklearn.linear_model import LassoCV\n\nlasso_cv_model = LassoCV(eps=0.01, n_alphas=200, cv=10, max_iter=1000000)\n\n\nlassoCV_mae, lassoCV_rmse, lassoCV_r2score, lassoCV_y_pred, lasso_cv_model = model_evaluation(lasso_cv_model, \"LassoCV\")","c10f1d0d":"plot_residuals(lassoCV_y_pred, \"LassoCV\")","c89de705":"from sklearn.linear_model import RidgeCV\n\nridge_model = RidgeCV(alphas=[0.1, 1.0, 10.0])\nridge_cv_mae, ridge_cv_rmse, ridge_cv_r2, ridge_cv_y_pred, ridge_model = model_evaluation(ridge_model,\n                                                                                          \"RidgeCV\")","314adb6e":"ridge_model.alpha_","73964174":"plot_residuals(ridge_cv_y_pred, \"RidgeCV\")","25375541":"from sklearn.svm import SVR\n\nsvr_base_model = SVR()\n\nsvr_base_mae, svr_base_rmse, svr_base_r2score, svr_base_y_red, svr_base_model = model_evaluation(svr_base_model, \n                                                                                                 \"Support Vector Regressor\")","44d9f431":"plot_residuals(svr_base_y_red, \"SVR\")","73c5d7d0":"from catboost import CatBoostRegressor\n\ncat_base = CatBoostRegressor(verbose=0, random_state=101)\n\ncat_base_mae, cat_base_rmse, cat_base_r2, cat_base_y_pred, cat_base_model = model_evaluation(cat_base,\n                                                                                             \"CatBoostRegressor\")","11b6dd31":"feat_imp = cat_base.get_feature_importance(prettified=True)\n\n# Plotting top 20 features' importance\n\nplt.figure(figsize = (12,8))\nsns.barplot(feat_imp['Importances'][:20],feat_imp['Feature Id'][:20], orient = 'h', palette=\"coolwarm_r\")\nplt.title(\"Feature Importance\")\nplt.show()","6c8f171e":"import shap\nfrom catboost import Pool\n\n# Feature importance Interactive Plot \n\ntrain_pool = Pool(scaled_Xtrain)\nval_pool = Pool(scaled_Xtest)\n\nexplainer = shap.TreeExplainer(cat_base_model) # insert your model\nshap_values = explainer.shap_values(train_pool) # insert your train Pool object\n\nshap.summary_plot(shap_values, scaled_Xtrain)","22160ecf":"base_score_df = pd.DataFrame({\"Model\":[\"SGDRegressor\", \"GradientBoostingRegressor\",\n                                       \"RandomForestRegressor\", \"Extreme Gradient Boosting\",\n                                       \"KNeighborsRegressor\" , \"LassoCV\", \"SVR\", \"RidgeCV\",\n                                       \"CatBoost\"],\n                              \n                              \"R-square\":[sgd_base_r2score, gbr_base_r2score, rfr_base_r2score,\n                                         xgboost_base_r2score, knn_base_r2score, lassoCV_r2score,\n                                         svr_base_r2score, ridge_cv_r2, cat_base_r2],\n                              \n                              \"RMSE\":[sgd_base_rmse, gbr_base_rmse, rfr_base_rmse, xgboost_base_rmse,\n                                      knn_base_rmse, lassoCV_rmse, svr_base_rmse, ridge_cv_rmse,\n                                      cat_base_rmse],\n                              \n                              \"MAE\": [sgd_base_mae, gbr_base_mae, rfr_base_mae, xgboost_base_mae,\n                                      knn_base_mae, lassoCV_mae, svr_base_mae, ridge_cv_mae,\n                                      cat_base_mae]})\n\nbase_score_df = base_score_df.sort_values(by=[\"R-square\"], \n                                          ascending=False).reset_index(drop=True)","b76ec98a":"print(\"**Base Models Metrics**\")\nbase_score_df","98a98b61":"# Visualize the table above\nfig, ax = plt.subplots(figsize=(8,5))\n\nsns.barplot(x=\"Model\", y=\"R-square\", data=base_score_df, ax=ax, palette=\"magma\")\nsns.lineplot(x=\"Model\", y=\"RMSE\", data=base_score_df, color=\"red\", ax=ax,legend='brief', label=\"rmse\")\nsns.lineplot(x=\"Model\", y=\"MAE\", data=base_score_df, color='green', ax=ax, legend='brief', label=\"mae\")\n\nplt.xticks(rotation=45, horizontalalignment=\"right\")\nplt.title(\"Regression Model Performance Metrics\")\nplt.ylabel(\"R_squared\")\nplt.legend();","1d03bb9f":"from sklearn.ensemble import VotingRegressor\n\nensemble1_model = VotingRegressor(estimators=[(\"ridgecv\", ridge_model),\n                                             (\"catboost\", cat_base_model),\n                                             (\"gbr\", gbr_model),\n                                             (\"lassocv\", lasso_cv_model),\n                                             (\"svr\", svr_base_model),\n                                             (\"forest\", rfr_model)])","0d085a49":"ensemble1_mae, ensemble1_rmse, ensemble1_r2, _ , ensemble1_model = model_evaluation(ensemble1_model,\n                                                                                    \"Voting Regressor\")","e7f910d3":"# Fit the model\n# ensemble_model1.fit(scaled_X, y)\n\n# final_ensemble = ensemble_model1.predict(scaled_test)\n\n\n# Make predictions and save it to the dataframe\n# final_base_ensemble_df = pd.DataFrame({\"id\":row_id,\n                                       # \"SalePrice\": np.expm1(final_ensemble)})","0737ad90":"# final_base_ensemble_df.to_csv(\"house_price_final_ensemble_base_sub.csv\", index=False)","90c68908":"from sklearn.model_selection import GridSearchCV\n\ndef model_gridsearchCV(algo,param,name):\n    \"\"\"\n    Function will perform gridsearchCV for given algorithm\n    and parameter grid. Returns grid model, y_pred. Prints out \n    mean absolute error, root mean squared error, R-square score\n    \"\"\"\n    # Instatiate base model\n    model = algo()\n    \n    # Instantiate grid for a model\n    model_grid = GridSearchCV(model, \n                             param,\n                             scoring=\"r2\",\n                             # verbose=2,\n                             n_jobs=-1,\n                             cv=3)\n    # Fit the grid model\n    model_grid.fit(scaled_Xtrain, y_train)\n    \n    # Make prediction\n    y_pred = model_grid.predict(scaled_Xtest)\n    \n    # Evaluate model\n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    r2score = r2_score(y_test, y_pred)\n    \n    # Print \n    print(f\"**{name} with GridSearchCV**\")\n    print(f\"MAE: {mae:}\")\n    print(f\"RMSE: {rmse:}\")\n    print(f\"R-squared: {r2score:.2f}%\")\n    \n    return mae, rmse, r2score, y_pred, model_grid","38f21b51":"param_grid = {#\"loss\":[\"ls\",\"lad\",\"huber\",\"quantile\"],\n              \"learning_rate\": [ 0.01, 0.1, 0.3, 1],\n              \"subsample\": [0.5, 0.2, 0.1],\n              \"n_estimators\": [500, 1000],\n              \"max_depth\": [3,6,8]}\n\ngbr_grid_mae, gbr_grid_rmse, gbr_grid_r2, _ , gbr_grid = model_gridsearchCV(GradientBoostingRegressor, \n                                                                            param_grid,\n                                                                            \"GradientBoostingRegressor\")","b24d64ff":"gbr_grid.best_params_","e1c5a207":"param_grid = {\"n_estimators\": [500,1000, 1500],\n              \"max_features\": ['auto','sqrt'],\n              \"max_depth\": [None,5,10,],\n              \"min_samples_split\": [2,5,10],\n              \"min_samples_leaf\": [1,2,5,10]}\n\nrfr_grid_mae, rfr_grid_rmse, rfr_grid_r2, _ , rfr_grid_model = model_gridsearchCV(RandomForestRegressor,\n                                                                                  param_grid,\n                                                                                  \"RandomForestRegressor\")","a3706265":"rfr_grid_model.best_params_","35a9e087":"param_grid = {\"kernel\":[\"linear\",\"rbf\",],\n              \"gamma\": [\"scale\",\"auto\"],\n              \"C\": [0.1, 0.5, 1, 10],\n              \"epsilon\": [0.1, 0.01, 0.001]}\n\nsvr_grid_mae, svr_grid_rmse, svr_grid_r2, svr_grid_y_pred, svr_grid_model = model_gridsearchCV(SVR,\n                                                                                param_grid,\n                                                                               \"SVR\")","676f05de":"svr_grid_model.best_params_","7e6c61b7":"svr_grid_model.best_score_","862c0dd3":"from sklearn.linear_model import Ridge\n\nparam_grid = {\"solver\": [\"auto\",\"svd\",\"lsqr\",\"saga\"],\n              \"max_iter\": [1000, 10000],\n              \"tol\": [1e-3,1e-2],\n              \"alpha\": [0.1, 1.0, 10.0, 30.0]}\n\nridge_gr_mae, ridge_gr_rmse, ridge_gr_r2,_ , ridge_gr_model = model_gridsearchCV(Ridge,\n                                                                                 param_grid,\n                                                                                 \"Ridge\")","b0368f0f":"ridge_gr_model.best_params_","b86e3898":"param_grid = {\"learning_rate\":[0.05, 0.10, 0.15, 0.20, 0.30],\n              \"max_depth\":[3,4,5,6,8,15],\n              \"min_child_weight\":[1,3,5,7],\n              \"gamma\":[0.0, 0.1, 0.2, 0.3, 0.4],\n              \"colsample_bytree\":[0.3, 0.4, 0.5, 0.7]}\n\nxboost_gr_mae, xboost_gr_rmse, xboost_gr_r2, _ , xboost_gr_model = model_gridsearchCV(XGBRegressor,\n                                                                                      param_grid,\n                                                                                      \"XGBoost\")","35cececb":"xboost_gr_model.best_params_","1292e942":"#param_grid = {'iterations': [250,100,500,1000],\n              #'learning_rate': [0.01,0.1,0.2,0.3],\n              #'depth': [4, 6],\n              #'l2_leaf_reg': [3,1,5,10,100]}\n\n\n# cat_grid_mae, cat_grid_rmse, cat_grid_r2, _ , cat_grid_model = model_gridsearchCV(CatBoostRegressor,\n                                                                                  # param_grid,\n                                                                                  # \"CatBoost\")","33cece0c":"cat_grid_model.best_params_","c56459f9":"grCV_metrics_df = pd.DataFrame({\"Model\":[\"GradientBoostingRegressor\", \"RandomForestRegressor\", \n                                         \"SVR\", \"Ridge\", \"XGBRegressor\", \"CatBoost\"],\n                                        \n                                \"R-square\":[gbr_grid_r2, rfr_grid_r2, svr_grid_r2, \n                                            ridge_gr_r2, xboost_gr_r2, cat_grid_r2],\n                                        \n                                \"RMSE\":[gbr_grid_rmse, rfr_grid_rmse, svr_grid_rmse, \n                                        ridge_gr_rmse, xboost_gr_rmse, cat_grid_rmse],\n                                        \n                                \"MAE\":[gbr_grid_mae, rfr_grid_mae, svr_grid_mae, \n                                      ridge_gr_mae, xboost_gr_mae, cat_grid_mae]})\n\ngrCV_mertics_df = grCV_metrics_df.sort_values(by=[\"R-square\"],\n                                              ascending=False).reset_index(drop=True)\n\nprint(\"**GridSearchCV Models Metrics**\")\ngrCV_mertics_df","d37c8a20":"# Visualize the table above\nfig, ax = plt.subplots(figsize=(8,5))\n\nlist_order = list(grCV_mertics_df['Model'].values)\n# R-squared\nsns.barplot(x=\"Model\", y=\"R-square\", \n            data=grCV_metrics_df, ax=ax, \n            palette=\"magma\", order= list_order)\n# Root Mean Squared Error\nsns.lineplot(x=\"Model\", y=\"RMSE\", data=grCV_metrics_df, \n             color=\"red\", ax=ax,legend='brief', label=\"rmse\")\n# Mean Absolute Error\nsns.lineplot(x=\"Model\", y=\"MAE\", data=grCV_metrics_df, \n             color='green', ax=ax, legend='brief', label=\"mae\")\n\nplt.xticks(rotation=45, horizontalalignment=\"right\")\nplt.title(\"Regression Models with GridSearchCV Metrics\")\nplt.ylabel(\"R_squared\")\nplt.legend();","b055c5be":"ensemble2_model = VotingRegressor(estimators=[(\"ridgecv\", ridge_gr_model.estimator),\n                                             (\"catboost\", cat_grid_model.estimator),\n                                             (\"gbr\", gbr_grid.estimator),\n                                             (\"lassocv\", lasso_cv_model),\n                                             (\"svr\", svr_base_model),\n                                             (\"forest\", rfr_model.base_estimator)])","1ef9967d":"# Fit the model\n# ensemble2_model.fit(scaled_Xtrain, y_train)","624473f2":"# Evaluate ensemble model\nensemble2_y_pred = ensemble2_model.predict(scaled_Xtest)\n\nensemble2_mae = mean_absolute_error(y_test, ensemble2_y_pred)\nensemble2_rmse = np.sqrt(mean_squared_error(y_test, ensemble2_y_pred))\n    \n# R-squared \nensemble2_r2 = r2_score(y_test, ensemble2_y_pred)\n    \nprint(f\"**VotingRegressor Metrics**\")\nprint(f\"**MAE: {ensemble2_mae}\")\nprint(f\"**RMSE: {ensemble2_rmse}\")\nprint(f\"**R-squared: {ensemble2_r2:.2f}%\")","032b861d":"best_ensemble = VotingRegressor(estimators=[(\"gbr\", gbr_grid.estimator),\n                                            (\"forest\", rfr_grid_model.estimator),\n                                            (\"svr\", svr_grid_model.estimator),\n                                            (\"ridge\", ridge_gr_model.estimator),\n                                            (\"xgboost\", xboost_gr_model.estimator),\n                                            (\"catboost\", cat_grid_model.estimator)])","3d1aa702":"#fit the model\n#best_ensemble.fit(scaled_X, y)\n\n#final_ensemble2 = best_ensemble.predict(scaled_test)\n\n\n#Make predictions and save it to the dataframe\n#final_ensemble_df = pd.DataFrame({\"id\":row_id,\"SalePrice\": np.expm1(final_ensemble2)})","18a18943":"final_ensemble_df.to_csv(\"house_price_grid_ensemble_sub.csv\", index=False)","899c79f1":"**CatBoost with GridSearchCV**\n1. MAE: 0.08686650731538992\n2. RMSE: 0.12366782337837257\n3. R-squared: 0.92%","b0dd67f7":"### Missing values in numeric features","bc8e5641":"**RidgeCV**","fbf0db96":"### Numerical outliers","5857729a":"## Numerical Features","d2aed849":"I will remove outliers from this continues numeric column later on as it would effect my test dataset for submission if I do it now.","59e82909":"There is a warning when running this algoritm, but It should not prevent your code from running, nor should it lead to different results.","71070573":"Let's check how these correlated variables to each other are correlated to the target column, so I can decide which of them remove from further analysis.","58a100f3":"**SVR**","210aa97d":"#### Missing Values in Categorical","2b45dcc0":"**Ordinal values**","946d67e5":"I have to admit that spent all that time on testing didn't help to improve the model in this case .After submmision to Kaggle competition I end up in top 8%. Now, I need to figure it out what to do next or which techniques I could implement to make this model even more robust. I've got some ideas already in my mind, anyway, back to reading and searching. So, if you Kagglers have some ideas let me know. Don't forget to leave feadback.","b5254b05":"**KNeighbors**","42c4c9e1":"**YearRemodAdd column**","ab1a47cb":"## Foreword","d67eede7":"**Nominal values**","bce04cd5":"**Extreme Gradient Boosting**","80456ead":"**Submmit to Kaggle with the best score to Kaggle competition**","81f6c21f":"**CatBoostRegressor**","1d53f4c5":"I'm not sure if creating another column from two old ones will improve my model or it will carry the same information as newly created numerical one? If someone can clear that for me that would be great. For now I won't create it.","980796a8":"**YearBuilt & YrSold columns**","628dbfaa":"## Creating and testing our models\n\n\n### Models score baseline","d82dc9e1":"#### Missing data in categorical columns","9ccee028":"**GradientBoostingRegressor**","10ac44cb":"I think, that the only reason doing it is to reduce dimensionality of our dataframe. I am going to to keep these columns in unchange form now.","f7944b2c":"Different methods can be apply to convert nominal variables into numbers so I future algorithm can work with them. All of them have prons and cons but I am not going to write about it here. One of the simpliest and easy to understand is pandas \"get_dummies\" method, however you need to remember not to indroduce nulticollinearity what is also called (dummy trap). ","072b5421":"### Nominal and Ordinal Columns","867f017e":"**SGDRegressor**","f1967d0b":"## Splitting and standarization of the data","28389c2e":"Some of the categorical columns have nominal or ordinal values and that needs to be addressed. As a remminder, nominal data is when we can only classify the data, while ordinal data can be classified and ordered.","90eb5252":"Hi Kagglers!\nThe score of the House Price predictions project https:\/\/www.kaggle.com\/godzill22\/house-price-predictions\/comments I did a year ago it's nothing to be proud of. Over that period I've learnt more interesting techniques that I would like to try and see whether I can impove my model. This notebook is an attempt of systematic approach of how to deal with high dimension dataset and with different features type. I think, the data from House Price competition is ideal to practice those skills. Overall, preparation range anywhere from 60\u201380% of the total time spent on a Data Science project.","3718a243":"GridSearchCV is a exhaustive search over specified hyperparameters values for an estimator. It allow us to find the best combination of best parameters for a chosen model. I will split data again with test size of 0.1.","47e71edf":"There are many methods to impute data, some of them are very sophisticated, but there is one flaw, we impute artificially created values. In case of categorical variables imputing mode of a column could be one of them, but I will fill missing values with a string \"None\" so I could retain the orginal information.","37af6cd5":"I introduce correlation between features when I converted some ordinal features and I need to remove one of the correlated feature. There is also high correlation between \"SalePrice\" and \"OverallQual\" (0.817185) but this is all right.","86efd290":"## Load the dataset","48f65ee8":"The only numerical column left with some missing values (less than 1%) so I will fill them with a mean of the column. I don't need to fill missing values in SalePrice Columns.","1b71a819":"**totalPorch**","a715cd33":"**Skewness**","4fa50f0d":"**CatBoostRegressor**","87d41a58":"**Random Forest Regressor**","67fbf716":"**Now, this is very important that we split dataset back into test and train dataset before we scale the data.**","a93c1fae":"## Categorical features","0183eb35":"### Check target column first","025fa6e1":"**Extreme Gradient Boosting**","be85f4c3":"I don't see any value from this column, therefore I will drop it later on.","2fde497f":"We all know how important is to standarize\/normalize dataset for our algorithms but there are exceptions depend on what algorithm we are going to use. In order to do it right we need to remember about few things. Some suggest that it only descrete variables should be standarized and definitely we have to fit and transform train set and then only transforming test set. The reason for that we are not creating what is known as data leakage.","49225ff0":"Second method for converting nominal categorical variables is OneHotEncode, but I am not going to use it in this notebook. Another common method used by practitioners is Label Encoding which suits more with variables who have some sort of order. ","1dc6028a":"**ElasticNetCV**","a976200a":"There are some features that are useless(to many variables or the same information). First, I will try to create new features from them.","be7cd3d9":"**PoolArea**","f0d9044c":"**GarageYrBlt column**","45f2aac3":"### Feature engineering for categorical variables","2225e1e7":"### Base model scores metrics","ca96fa1b":"First of all I will remove highly skewed features from dataset and then trim off the rest of them by 0.5 limit. Acceptable values for skewness are between -0.5 and 0.5 while -2 and 2 for Kurtosis.","99d259b6":"**Gradient Boosting Regressor**","d285a956":"#### Submmit Voting Ensamble Model with base models","e522fe90":"**Random Forest Regressor**","1f46f593":"####  Check numerical features correlation again","3f47dfdf":"### Make predictions submmision to Kaggle","08d17f5c":"###  Ensemble model with best parameters","620bd898":"First of all I will concatenate these 2 dataframe for feature engineering. It will help me to avoid a problem where train and test dataset discrete features are different from each other.","cc3ab2b1":"**Bathroom columns**","97042367":"Columns LotFrontage(Linear feet of street connected to property) and LotArea(Lot size in square feet) are highly correlated, so I will drop feature with missing values.","147106f6":"### GridSearchCV for the best hyperparameters","3b86ea45":"### Distribution of numeric features","d19b9aca":"Great place to start for someone who ask what algorithm I shoud use is sklearn algorithm cheat-sheet https:\/\/scikit-learn.org\/stable\/tutorial\/machine_learning_map\/index.html. Follow their recommendation I am going to choose first Stochastic Gradient Descent. Let's create Stochastic Gradient Descent first.","30bc3e6c":"**Condition1 & Condition2 columns**","63afd63b":"**Submmit ensemble model to the competition**","4104d34c":"I am going to use ElasticNetCV in the base line models predictions as it will allow me to choose between Ridge(L2 regularization) or Lasso (L1 regularization). The benefit is that elastic net allows a balance of both penalties, which can result in better performance than a model with either one or the other penalty on some problems.","9beb284a":"**Ridge**\n\nRidgeCV had the best metrics and I want to see if GridSearchCV can improve the model."}}