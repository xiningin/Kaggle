{"cell_type":{"d6b249b7":"code","8f1cd1d8":"code","45e7d71d":"code","4f90cb67":"code","2b4e5732":"code","ae11b0ad":"code","4dd23f0d":"code","1abc09fa":"code","10df3354":"code","453a10c4":"code","f7158061":"code","f94aa1ed":"code","ef4de6bb":"code","690f40e1":"code","010ae8e3":"code","50e83fa4":"code","80fac5b9":"code","a7268113":"code","8caa6373":"code","498550c6":"code","da974353":"code","344a9db5":"code","709f53b3":"code","1897af0e":"code","b1381654":"code","3c0abab3":"code","2439b4d8":"markdown","d734cdd9":"markdown","140aa689":"markdown","a3605290":"markdown","b2b0759f":"markdown","ddd3a449":"markdown","ae2d3f6d":"markdown","4814108f":"markdown","35d36ae9":"markdown","c73e32ef":"markdown","d31958b2":"markdown","9842c9db":"markdown","f65533cd":"markdown","3c15be9e":"markdown","cfd91f52":"markdown","8fc69207":"markdown","1bf1c927":"markdown","bb2d8d3b":"markdown","19b482ba":"markdown","c18f049b":"markdown","1c36153d":"markdown","f580e328":"markdown","556c93ad":"markdown"},"source":{"d6b249b7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8f1cd1d8":"df = pd.read_csv('\/kaggle\/input\/ccdata\/CC GENERAL.csv')","45e7d71d":"df.head()","4f90cb67":"df.describe()","2b4e5732":"df.info()","ae11b0ad":"df.isna().mean()*100","4dd23f0d":"df.drop(['CUST_ID'], axis=1, inplace=True)","1abc09fa":"df.dropna(subset=['CREDIT_LIMIT'], inplace=True)","10df3354":"df['MINIMUM_PAYMENTS'].fillna(df['MINIMUM_PAYMENTS'].median(), inplace=True)","453a10c4":"plt.figure(figsize=(20,35))\nfor i, col in enumerate(df.columns):\n    if df[col].dtype != 'object':\n        ax = plt.subplot(9, 2, i+1)\n        sns.kdeplot(df[col], ax=ax)\n        plt.xlabel(col)\n        \nplt.show()","f7158061":"cols = ['BALANCE', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'ONEOFF_PURCHASES_FREQUENCY','PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT']","f94aa1ed":"for col in cols:\n    df[col] = np.log(1 + df[col])","ef4de6bb":"plt.figure(figsize=(15,20))\nfor i, col in enumerate(cols):\n    ax = plt.subplot(6, 2, i+1)\n    sns.kdeplot(df[col], ax=ax)\nplt.show()","690f40e1":"plt.figure(figsize=(12,12))\nsns.heatmap(df.corr(), annot=True)\nplt.show()","010ae8e3":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95)\nX_red = pca.fit_transform(df)","50e83fa4":"from sklearn.cluster import KMeans\n\nkmeans_models = [KMeans(n_clusters=k, random_state=23).fit(X_red) for k in range (1, 10)]\ninnertia = [model.inertia_ for model in kmeans_models]\n\nplt.plot(range(1, 10), innertia)\nplt.title('Elbow method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","80fac5b9":"from sklearn.metrics import silhouette_score\n\nsilhoutte_scores = [silhouette_score(X_red, model.labels_) for model in kmeans_models[1:4]]\nplt.plot(range(2,5), silhoutte_scores, \"bo-\")\nplt.xticks([2, 3, 4])\nplt.title('Silhoutte scores vs Number of clusters')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhoutte score')\nplt.show()","a7268113":"from sklearn.metrics import silhouette_score\n\nkmeans = KMeans(n_clusters=2, random_state=23)\nkmeans.fit(X_red)\n\nprint('Silhoutte score of our model is ' + str(silhouette_score(X_red, kmeans.labels_)))","8caa6373":"df['cluster_id'] = kmeans.labels_","498550c6":"for col in cols:\n    df[col] = np.exp(df[col])","da974353":"plt.figure(figsize=(10,6))\nsns.scatterplot(data=df, x='ONEOFF_PURCHASES', y='PURCHASES', hue='cluster_id')\nplt.title('Distribution of clusters based on One off purchases and total purchases')\nplt.show()","344a9db5":"plt.figure(figsize=(10,6))\nsns.scatterplot(data=df, x='CREDIT_LIMIT', y='PURCHASES', hue='cluster_id')\nplt.title('Distribution of clusters based on Credit limit and total purchases')\nplt.show()","709f53b3":"kmeans = KMeans(n_clusters=3, random_state=23)\nkmeans.fit(X_red)","1897af0e":"df['cluster_id'] = kmeans.labels_","b1381654":"plt.figure(figsize=(10,6))\nsns.scatterplot(data=df, x='ONEOFF_PURCHASES', y='PURCHASES', hue='cluster_id')\nplt.title('Distribution of clusters based on One off purchases and total purchases')\nplt.show()","3c0abab3":"plt.figure(figsize=(10,6))\nsns.scatterplot(data=df, x='CREDIT_LIMIT', y='PURCHASES', hue='cluster_id')\nplt.title('Distribution of clusters based on Credit limit and total purchases')\nplt.show()","2439b4d8":"## Load the data","d734cdd9":"Looking at the above 2 plots it seems like our model has clustered customers with low usage of credit card in one cluster and model with higher usage of clusters in other. Great! we direct of resources accordingly.","140aa689":"I know it may not look like an ideal distribution but it is better than what we had and it is our job as data scientist to help our model as much as possible.","a3605290":"## Model Training\n\nIt is quite interesting and intuitive to know how KMeans clustering algorithm actually works.\n\nKMeans clustering is an unsupervised clustering algorithm which groups together similar data in same cluster to form k clusters. As a result we get groups of similar records which can then be labelled and acted upon accordingly.\n\nHow does the algorithm find the clusters?\n\n1. Given number of clusters k, it first selects k random point (may not be points in dataset) to be the k cluster centroids.\n\n2. Then we assign each point to the closest centroid and form k clusters.\n\n3. Once all points are assigned to a cluster, we then compute new centroids for each cluster.\n\n4. Then we reassign points to the closest centroid.\n\n5. If any reassignment takes place in step 4 we repeat steps 3 and 4. If no reassignment takes place then our model is ready and we have extracted k clusters from our dataset.\n\nThis process is explained in the diagram below.\n\n![Kmeans.png](attachment:Kmeans.png)\n\nThere is a catch though. As the algorithm itself starts with a random initialization of k points alot depends on that initialization. Since our real world data is not as well separated as in the above figure , it may so happen that the model intializes the k points such a way that we may end up with a suboptimal solution.\n\nTo avoid such a case, we can run the algorithm multiple times with random initial points for each iteration. Running the model multiple times ensures that atleast one of the time we avoid bad initalization and reach the optimal solution. Scikit learn actually trains a KMeans model 10 times and it can we controlled with the help if *n_init* hyperparameter.\n\nTo measure which model performs better out the n random initalization we can use model innertia or wcss (Within Cluster Summation of Squares). It measure sum of distance of each point from its centroid. So we want compact clusters with point as close as possible to its centroid.\n\nAnother approch to avoid bad initialization is using KMeans++ algorithm to initialize centroids. This algorithms initializes centroids in such a way that the cenrtoid chosen are as far as possible to each other thus ensures that we do not have suboptimal solution. This alogrithm along with the previous approach ensures that we obtain the best solution possible. Scikit learn uses KMeans++ algorithm to initalize centroids and is given by *init* hyperparameter.\n\nThis entire discussion until this point revolves around the assumption that we know number of clusters n. Thus n is the most important hyperparameter here and it is necessary to initalize n to its appropriate value.\n\nTo get the value of n we cannot use innertia as our metric as innertia keeps on increasing as we increase number of clusters. Think about it, if we initialize n to the number of points in dataset innertia will be minimum.\n\nOne way could be to plot the graph of n versus innertia and as we plot the graph we can spot an elbow after which the innertia decreases at a much lower rate. If we can use n corresponding to this elbow point as our number of clusters.\n\nAnother approach is to calculate silhouette score which is given as (b-a)\/min(a,b) where b -> mean distance to the instances of nearest cluster, a -> mean distance to other instances of same cluster. So silhouette score penalizes the model is the mean distance to points of other cluster decreases and mean distance to the points of same cluster increases. While it rewards the model if mean distance to points of other cluster increases and mean distance to points of same cluster decreases. Thus we can select a model with highest silhouette score.\n\nPhew! You still here? Well enough of theory. Let's see all of this in practice.","b2b0759f":"Thanks for hanging around till the end. I would love to read your thoughts below.","ddd3a449":"### Extra:\n\n3 seems to be an elbow so I wonder what would the model look like with 3 clusters lets see.","ae2d3f6d":"Personally to me this seems like better clustering as it really segments out top half of customers having more than usual amount of usage of credit card and the customers which have very low usage. This seems to be more actionable result if we want to direct our marketing strategies according to usage of credit card.","4814108f":"You see the elbow there? It seems like the elbow is around 3 or 4 . We will use silhouette score to see which one performs better.","35d36ae9":"Well, we were wrong n=2 seems to have higher silhouette than the other two. It is what it is, we will select 2 as our number of clusters.","c73e32ef":"## Data Processing","d31958b2":"Customer ID seems to be an unique id for each customer and hence won't play any role in determining the cluster.","9842c9db":"Looking at the reports we can see that most of the features have mean way greater than their median. This is a sign of some skewness in the dataset and we have to see if we can do something about it.\n\nWe also have some Nan values to impute there.","f65533cd":"We have seen that credit limit feature has just 0.01% of records having Nan value i.e just 1 record here has missing value. So we don't have to take the trouble to impute it. We can simply drop it and never think about it again.","3c15be9e":"We've got some correlated features there. There are many ways to handle this. We would go ahead with dimensionality reduction and we will bring our data down to lower dimension.\n\nWe will use PCA for our dimensionality reduction.\n\nTo explain briefly what PCA does under the hood is that it finds new dimension\/axis for the dataset such that it explains maximum variance. That axis is then the first principal component. Then it chooses another component perpendicular to first principal component which explains maximum variance.\n\n![PCA.gif](attachment:PCA.gif)\n\nSo for above image, If we project all points on PCA 1st dimension then the points are more spread out than if we do it on any other axis. This means that PCA 1st dimension explains maximum variance and hence it is our 1st principal component. Now we consider components which are perpendicular to this component and since this data is 2 dimensional we just have 1 component perpendicular to our 1st principal component and it becomes our 2nd principal component.\n\nNote: If we had 3rd dimension coming out from the screen then we would have 2 components perpendicular to our 1st principal components and we would have to choose the one which explains maximum variance of the two.\n\nOnce we have these principal components, we can choose the number of components we wish to have and then express our data in terms of those principal components and thus reducing the dimensions.\n\nSo let's do the same in our case. We will select the number of components such that our data in the lower dimensions explains 95% of variance of our original data.","cfd91f52":"## Model Evaluation and Inference.","8fc69207":"Assigning labels as cluster index to our dataset.","1bf1c927":"## Credit Card Customer Segmentation\n\nThe problem described in this dataset requires us to extract segments of customers depending on their behaviour patterns provided in the dataset, to focus marketing strategy of the company on a particular segment.\n\nLet's first load the dataset and take a quick look at it to decide our approach towards solving this problem.","bb2d8d3b":"Inverse transforming the log transformation that we did earlier to visualize the results on original scale.","19b482ba":"Woah!! There is a lot of skewness and they are varied. It is kind of expected from datasets like these as there will always be a few customers who do very high amount of transactions.\n\nNow it depends on our application whether we want to handle the skewness in our dataset or not for a clustering problem. For instance if we want to do clustering for anamoly detection in that case we wouldn't want to handle the outliers as we would like our model to detect them and group them in a cluster. For our application I am looking for a good visualization so I would like to handle the skewness as much as possible as it will help the model to form better clusters.\n\nLet's see if we can do something about this.","c18f049b":"With that we are ready to do what we have always wanted to do i.e Clustering. We will use Kmeans clustering algorithm to extraxts clusters of information from our dataset.","1c36153d":"For imputing minimum payments feature, I don't see any column which has relation with this feature and help us estimate values for missing records. It seems that the values are missing at random and we can simply use median to replace the Nan values as the distribution for minimum payments is skewed and hence median gives a better estimation of the central tendency of this feature.","f580e328":"Are we even data scientist if we don't visualize?\n\nLet's visualize how skewed our dataset is.","556c93ad":"Looking for some correlation now."}}