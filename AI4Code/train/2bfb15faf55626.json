{"cell_type":{"8d53b798":"code","711f20a2":"code","33e316fa":"code","df40d56b":"code","5d7c1e83":"code","149429c1":"code","fd174fe8":"code","4ca64335":"code","41b96e59":"code","526c581a":"code","0f366b9a":"code","5996fb7c":"code","1485f0be":"code","4ef127a8":"code","98e71ab2":"code","a6b5377c":"code","7e2d2456":"code","6d91e015":"code","84fceabd":"code","a458d75d":"code","0a4c6079":"code","c17daacb":"code","c39b52b9":"code","173d9097":"code","f0b46d36":"code","793a5ea1":"code","85257c75":"code","399fdec2":"code","a423f959":"code","21e4b9a3":"code","fac6915f":"markdown","9f335ab7":"markdown","d0099204":"markdown","d883c26c":"markdown","690fbbef":"markdown","975dd7f5":"markdown","eb164182":"markdown","e2895c13":"markdown","4cfd9917":"markdown","55f803bc":"markdown","7704aba6":"markdown","bf04d985":"markdown","ae7f7ffc":"markdown","9b0f834a":"markdown","967acc41":"markdown","5fb5bb27":"markdown","e3c19aed":"markdown","cfab9e7b":"markdown","d364ea32":"markdown","21b3f448":"markdown","1abf831a":"markdown","9a4f06ae":"markdown","76c0dcc4":"markdown","550bed4f":"markdown","9b289cbc":"markdown","4c7ea910":"markdown"},"source":{"8d53b798":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#set max columns and rows-display\npd.set_option(\"display.max_columns\", 100)\npd.set_option(\"display.max_rows\", 100)","711f20a2":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\ndf_list = [train_df, test_df] #list of df so actions quickly can be made on all df's","33e316fa":"print(train_df.head()) #In reality I look at the DataFrame in Spyder.\nprint(train_df.columns.values)","df40d56b":"print(train_df.info())\nprint(\"--------------------------\")\nprint(test_df.info())","5d7c1e83":"print(train_df.describe())","149429c1":"print(train_df.describe(include=['O']))","fd174fe8":"for df in df_list:\n    A = df[\"Name\"].str.split(\",\", expand = True)\n    df[\"LastName\"] = A[0].str.replace(' ', '')\n    B = A[1].str.split(\".\", expand = True)    \n    df[\"Title\"] = B[0].str.replace(' ', '')\n    df.drop([\"PassengerId\", \"Ticket\", \"Name\"], axis=1, inplace=True)","4ca64335":"print(train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False)\n                .mean().sort_values(by='Survived', ascending=False))\n\nprint(train_df[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False)\n                .mean().sort_values(by='Survived', ascending=False))\n\nprint(train_df[['Parch', 'Survived']].groupby(['Parch'], as_index=False)\n               .mean().sort_values(by='Survived', ascending=False))","41b96e59":"for df in df_list:\n    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"]\n    df.drop([\"SibSp\", \"Parch\"], axis=1, inplace=True)\n    \nplt.figure()\nsns.distplot(train_df.FamilySize)","526c581a":"for df in df_list:   \n    df.loc[(df['FamilySize'] == 0), 'FamilySize'] = 0\n    df.loc[(df['FamilySize'] == 1), 'FamilySize'] = 1\n    df.loc[(df['FamilySize'] > 1) , 'FamilySize'] = 2","0f366b9a":"print(train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False)\n                .mean().sort_values(by='Survived', ascending=False))","5996fb7c":"sns.FacetGrid(train_df, col='Survived').map(plt.hist, 'Age', bins=10)","1485f0be":"print(pd.crosstab(train_df['Title'], train_df['Sex']))","4ef127a8":"plt.figure()\nplt.figure(figsize=(20,10))\nplt.title(\"Age vs Title\")\nsns.boxplot(x=\"Title\",y=\"Age\", data=train_df)","98e71ab2":"for df in df_list:\n    df.loc[df.Sex == \"female\", \"Title\"] = df[\"Title\"].apply(lambda title: \"Miss\" if (title == \"Ms\") or (title == \"Mlle\") or (title == \"Mme\") or (title == \"Miss\")\n              else \"Mrs\")\n    df.loc[df.Sex == \"male\", \"Title\"] = df[\"Title\"].apply(lambda title: \"Master\" if (title == \"Master\")\n              else \"Mr\")","a6b5377c":"print(train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean().sort_values(by='Survived', ascending=False))","7e2d2456":"grid = sns.FacetGrid(train_df, row='Pclass', col='Title', height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","6d91e015":"titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4}\nfor df in df_list:\n    df['Title'] = df['Title'].map(titles)\n    df['Title'] = df['Title'].fillna(0)","84fceabd":"guess_ages = np.zeros((4,3))\n#print(guess_ages)\n\n\nfor df in df_list:\n    for i in range(0, 4):\n        for j in range(0, 3):\n            guess_df = df[(df['Title'] == i+1) & (df['Pclass'] == j+1)]['Age'].dropna()\n\n            age_mean = guess_df.mean()\n            age_std = guess_df.std()\n            age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n\n    for i in range(0, 4):\n        for j in range(0, 3):\n            df.loc[ (df.Age.isnull()) & (df.Title == i+1) & (df.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    df['Age'] = df['Age'].astype(int)\n    \n#Finally - let's create age bands as we aim to create only numerical values for our model.\n    \nfor df in df_list:   \n    df.loc[ df['Age'] <= 12, 'Age'] = 0\n    df.loc[(df['Age'] > 12) & (df['Age'] <= 18), 'Age'] = 1\n    df.loc[(df['Age'] > 18) & (df['Age'] <= 35), 'Age'] = 2\n    df.loc[(df['Age'] > 35) & (df['Age'] <= 64), 'Age'] = 3\n    df.loc[ df['Age'] > 64, 'Age'] = 4  ","a458d75d":"print(train_df[['Age', 'Survived']].groupby(['Age'], as_index=False)\n                .mean().sort_values(by='Survived', ascending=False))","0a4c6079":"##Convert Cabin to \"Deck\"\n#for df in df_list:\n#    df[\"Deck\"] = df[\"Cabin\"].str[0]\n#    df.drop([\"Cabin\"], axis=1, inplace=True)\n#This was a dead end -> So I decided to drop both Cabin and LastName.\nfor df in df_list:\n    df.drop([\"Cabin\", \"LastName\"], axis=1, inplace=True)","c17daacb":"grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', height=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","c39b52b9":"plt.figure()\ntrain_df[train_df[\"Embarked\"].isnull()][\"Fare\"].value_counts().sort_index().plot.bar()\n","173d9097":"for df in df_list:\n    df[\"Embarked\"] = df[\"Embarked\"].fillna(value=\"C\")\n    df['Embarked'] = df['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    df['Sex'] = df['Sex'].map( {'female': 0, 'male': 1} ).astype(int)","f0b46d36":"\nguess_fare = np.zeros((6,3))\nfor df in df_list:\n    for i in range(0, 6):\n        for j in range(0, 3):\n            guess_df = df[(df['Age'] == i) & (df['Pclass'] == j+1)]['Fare'].dropna()\n\n            fare_mean = guess_df.mean()\n            fare_std = guess_df.std()\n            fare_guess = rnd.uniform(fare_mean - fare_std, fare_mean + fare_std)\n\n            guess_fare[i,j] = float(fare_guess)\n\n    for i in range(0, 6):\n        for j in range(0, 3):\n            df.loc[ (df.Fare.isnull()) & (df.Age == i) & (df.Pclass == j+1),\\\n                    'Fare'] = guess_fare[i,j]\n\n    df['Fare'] = df['Fare'].astype(int)\n\nplt.figure()\nsns.kdeplot(train_df.Fare)\n\nplt.figure()\nsns.kdeplot(train_df.query('Fare < 200').Fare) ","793a5ea1":"for df in df_list:   \n    df.loc[ df['Fare'] <= 10, 'Fare'] = 0\n    df.loc[(df['Fare'] > 10) & (df['Fare'] <= 50), 'Fare'] = 1\n    df.loc[(df['Fare'] > 50) & (df['Fare'] <= 100), 'Fare'] = 2\n    df.loc[(df['Fare'] > 100) & (df['Fare'] <= 200), 'Fare'] = 3\n    df.loc[ df['Fare'] > 200, 'Fare'] = 4 ","85257c75":"print(train_df[['Fare', 'Survived']].groupby(['Fare'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n","399fdec2":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.copy()\nX_train.shape, Y_train.shape, X_test.shape","a423f959":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n\ncoeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\nprint(coeff_df.sort_values(by='Correlation', ascending=False))\n\n# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n\n# Pattern Recognition\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n\n# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n\n# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n\n# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n\n# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n\n# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n\n# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)","21e4b9a3":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nprint(models.sort_values(by='Score', ascending=False))","fac6915f":"Cathegorical Data (Data that can take on one or more Fixed values) -> Survived, Sex & Embarked. \nPclass is Ordinal (Which is a kind of Cathegorical data \n(From Wiki: Ordinal data is a categorical, statistical data type where the variables have natural, ordered categories and the distances between the categories is not known.)\n\nNumerical Data (Which can be used or regression analysis etc) -> Age, Fare are continous whereas SibSp and Parch are discrete numbers.\n\nTicket and Cabin \u00e4r alphanumerical and might be able to convert into something useful.","9f335ab7":"\n891 entrys in training data where there is missing values in cathegory Age, Cabin and Embarked (Just 2 Embarked) in the training Data.\n\n418 entrys in test data where there is missing values in Age, cabin and Fare (Just 1 Fare)","d0099204":"**Data manipulation**","d883c26c":"This looks promising! :) Let's use this and add it to our model","690fbbef":"I've chosen to use the sklearn-library and models that use cathegorical and numerical data. This comes from decisions made further down in this kernel where I decided to convert all data to cathegorical (where some are ordinal and some not) data.","975dd7f5":"Age definitely looks like something worth including in our model. But we're missing lot of values. Let's try to use the \"Title\"-column and see if it can be used to make better guesses for the missing ages.","eb164182":"First we set up our training and test-variabels.","e2895c13":"And now let's compare their scores to eachother and pick the best one!","4cfd9917":"**Machine learning models**","55f803bc":"We can actually see here that it's more probable that the Embarked city should be C and not S if the ticketprice was high. \nAnd here it was high -> So the port will be C. I should probably make a prettier function -> But not tonight!","7704aba6":"This looks like a better cathegory! So let's keep it and use it in my model.","bf04d985":"There is a clear correlation from the Pclass attribute -> Pclass stays.\n\nWhen it comes to the siblings and the parents\/child-columns we have values with Zero correlation. this is a bit problematic - so I'll try to add them together and create a new column where we have cathegories of \"No family\", \"Small family\" and \"Large family\" ","ae7f7ffc":"These plots gives an ok indication of where to put our different bands. (I first had cathegory 1 divided into two cathegories - but changed my mind since the correlation was very similar)","9b0f834a":"Then we try a couple of different models:","967acc41":"**Getting to know the Data**","5fb5bb27":"Here we can see  that when guessing age Pclass and title can be used to make our guesses a bit better!\nFirst we convert the titles to numbers so we can work with them","e3c19aed":"Both \"Sex\" and \"Embarked\" are still not numerical -> Let's fix that and let's fill in the missing values we had in the embarked-column. \nThe easy way would be to just set them as S (Which by far is the most common) -> But let's see if the port had anything to do with Sex and Fare first.","cfab9e7b":"We can see that the title acually complements the \"Sex\"-column a bit when it comes to correlation - so we'll keep it and include it in our model.","d364ea32":"2 sexes with 577\/891 being male\nThere are either duplicates when it comes to cabins or people shared cabin\n644\/889 Embarked in the same city \"S\" - which was Southamption","21b3f448":"Tells us that 38% of the people in the training data survived, the mean age was 29.7 years, (We can read on Kaggle that the actual survival-rate was closer to 32%)\nThat there were were few old people with 75% being 38 or younger.\nMost people didn't travel with their their children\/parents. And \"only\" 30~% traveled with their siblings or spouses.\n50% or more treveled with Pclass 3\nWe can also see that there were extreme variance in the price paid for the Fare.","1abf831a":"From this we can drop PassengerID-column and Ticket-column because it looks highly unlikely that either contributes to survival.\nThere are alot of nan-values in the cabin-column and the data looks a bit weird - one Could convert them into a Deck-column and try to use the passangers last name and the cost for the Fare to try to complete it - but since there is missing so much data tht would probably do more harm than good.\n\nWe'll drop the \"Name\" column also - but first we''ll extract the last name and the title from it to see if we can use them for something.\n","9a4f06ae":"We have something to do with \"Fare\"-price so let's fill that \"nan\"-value we caught earlier with the mean value for its Pclass and Age \n","76c0dcc4":"First of all! A big thank you to this kernel -> https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions which has been very helpful!\n\nAnyways - This is my first real attempt at machine learning - I've done the tutorial here but this is something we're I've tried to be a bit more independent.\n\nAll feedback is greatly appriciated!","550bed4f":"We can probably make due with 4 cathegories \"Mr\" for the older males, \"Master\" for the young ones. \"Mrs\" and \"Miss\" for the older and younger females. \nMs, Mlle, and Mme will therefore be sett as \"Miss\", all other females are set as \"Mrs\" and all Males that are not \"Master\" are set as Mr.","9b289cbc":"So let's look for some correlations to see what to include in our model.","4c7ea910":"The graph shows that my first guess is quite ok. I have several options but \"Alone\": 0, \"Small family\": 1 and \"Large family\": >1 looks like a good start. "}}