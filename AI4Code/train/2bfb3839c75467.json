{"cell_type":{"1dd9d2cf":"code","c3b2c62d":"code","ba057576":"code","5cb93847":"code","1a27c526":"code","a8bed424":"code","c9b5d40f":"code","12b427d3":"code","814111d4":"code","4a83a88b":"code","68fd48e9":"code","a82f8262":"code","45a96f9d":"code","acfe1b5a":"code","64fbd303":"code","1c67172a":"code","1614b001":"code","f84291d5":"code","a7e0c62e":"code","a5092d9c":"code","9b84a898":"code","b475d8bc":"code","6bf84a42":"code","6e8cd5ee":"code","ea688067":"code","d9d3b88f":"code","90213b67":"code","a6326d98":"code","9855333c":"code","5d48d673":"code","7d031672":"code","dd2328f6":"code","2686f480":"code","d11731e9":"code","3bf014b1":"code","2e25522b":"code","38668421":"code","b0558d09":"code","42f869b4":"code","6c4296a1":"code","23c5325b":"code","178b2785":"code","b4856e4b":"code","34991664":"code","6075263b":"code","e656bb71":"code","fd4fda32":"code","59c97619":"code","1f9668cd":"code","4c817925":"code","90c6b40f":"code","754be705":"code","85518854":"code","4797a0d1":"code","fb8926be":"code","b483537d":"code","b720e731":"code","2303b33d":"code","d0487a64":"code","f0b9d044":"code","a0a605d4":"code","6598f0b2":"code","04b5bb56":"code","19b381fc":"code","64b5a1e7":"code","9a4a17d5":"code","72840da5":"code","3fb35099":"code","ed5ae675":"code","d8908de2":"markdown","baceeaae":"markdown","b8003a49":"markdown","5e1a5072":"markdown","5a42c37e":"markdown","f69fc1ea":"markdown","f4074abd":"markdown","7f2aea69":"markdown","2e708abb":"markdown","efda84de":"markdown","8c3b7665":"markdown","a8077d1e":"markdown","8a23f29f":"markdown","41f9f949":"markdown","705eaaf3":"markdown","79723f61":"markdown","ac5566cc":"markdown","4d6d3f9e":"markdown","84bb93f8":"markdown","3683c88c":"markdown","8272ab88":"markdown","9ef1ef73":"markdown","c1e78ce2":"markdown","d9239fc3":"markdown","97ceb289":"markdown","ba6e8433":"markdown"},"source":{"1dd9d2cf":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport seaborn as sns\nimport matplotlib as plt\nfrom math import sin, cos, sqrt, atan2, radians\nimport plotly.express as px\nimport json\nfrom scipy import stats\nimport ast\nimport re\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# Look at a few to see how we can merge them\n\nfile_dir = '..\/input\/zameencom-data'\n\n# Get a python list of csv files\nfiles = glob.glob(os.path.join(file_dir, \"*.csv\"))\n\ndf_from_each_file = (pd.read_csv(f) for f in files)\ndf   = pd.concat(df_from_each_file, ignore_index=True)\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\ndf.head()","c3b2c62d":"df.describe()","ba057576":"df.columns","5cb93847":"df['price'].astype(str).astype(int)","1a27c526":"df.groupby([\"city\"])['city'].count().plot(kind=\"bar\",figsize=(20, 5),logy=True,\n                         title=\"Cities and the amount of Data in them\")","a8bed424":"city_comparison=df[df['area'].str.contains('1 Kanal1 Kanal|10 Marla|3 Marla3 Marla') & df['city'].str.contains(\"Lahore|Karachi|Islamabad|Rawalpindi\")].groupby([\"city\",\"area\"])['price'].median().to_frame('price').reset_index()\ncity_comparison.style.background_gradient(cmap='summer')\n","c9b5d40f":"sns.lineplot(data=city_comparison, x=\"area\", y=\"price\", hue=\"city\")\n","12b427d3":"top_6=df.groupby([\"city\"])['city'].count().nlargest(6).to_frame('count').reset_index()\ntop_6.index=top_6[\"city\"]\ntop_6.drop(columns=['city'],inplace=True)\ntop_6.plot.pie(y='count', figsize=(15, 8))","814111d4":"\ntop_7=df.groupby([\"type\"])['type'].count().nlargest(7).to_frame('count').reset_index()\ntop_7.index=top_7[\"type\"]\ntop_7.drop(columns=['type'],inplace=True)\ntop_7.plot.pie(y='count', figsize=(30, 8))","4a83a88b":"ten_marla=df[df['area'].str.contains('10 Marla')].groupby([\"locationName\",\"city\"])['price'].median().to_frame('median_price').reset_index()\nten_marla=ten_marla.sort_values('median_price', ascending=True).head(15)\n\nfig = px.bar(ten_marla, y='city', x='median_price', orientation='h', color='locationName', barmode='group')\nfig.show()","68fd48e9":"one_kanal=df[df['area'].str.contains('1 Kanal1 Kanal')].groupby([\"locationName\",\"city\"])['price'].median().to_frame('median_price').head(15)\none_kanal.sort_values('median_price', ascending=True).style.background_gradient(cmap='summer')\n","a82f8262":"three_marla=df[df['area'].str.contains('3 Marla3 Marla')].groupby(['locationName','price','city'])['price'].median().to_frame('median_price').head(15)\nthree_marla.sort_values('median_price', ascending=True).style.background_gradient(cmap='summer')\n","45a96f9d":"one_kanal_lahore=df[df['area'].str.contains('1 Kanal1 Kanal') & df['city'].str.contains('Lahore')].groupby(['locationName','price'])['price'].median().to_frame('median_price').head(15)\none_kanal_lahore.sort_values('median_price', ascending=True).head(15).style.background_gradient(cmap='winter')\n","acfe1b5a":"one_kanal_lahore=one_kanal_lahore.sort_values('price', ascending=True).head(15)\none_kanal_lahore.plot.line(subplots=True)","64fbd303":"ten_marla_lahore=df[df['area'].str.contains('10 Marla10 Marla') & df['city'].str.contains('Lahore')].groupby([\"locationName\"])['locationName','price'].min()\nten_marla_lahore.sort_values('price', ascending=True).head(15).style.background_gradient(cmap='winter')","1c67172a":"ten_marla_lahore=ten_marla_lahore.sort_values('price', ascending=True).head(15)\nten_marla_lahore.plot.line(subplots=True)","1614b001":"three_marla_lahore=df[df['area'].str.contains('3 Marla3 Marla') & df['city'].str.contains('Lahore')].groupby([\"locationName\"])['locationName','price','area','city'].median()\nthree_marla_lahore.sort_values('price', ascending=True).head(15).style.background_gradient(cmap='winter')","f84291d5":"three_marla_lahore=three_marla_lahore.sort_values('price', ascending=True).head(15)\nthree_marla_lahore.plot.line(subplots=True)","a7e0c62e":"ten_marla_lahore=df[ df['city'].str.contains('Lahore')].groupby([\"locationName\"])['locationName','price','area','city'].median()\nten_marla_lahore.sort_values('price', ascending=False).head(15).style.background_gradient(cmap='winter')","a5092d9c":"df[df['city'].str.contains('Lahore')].groupby([\"bed\"] )['bed'].count().plot(kind=\"bar\",figsize=(20, 5),logy=True,\n                         title=\"Cities and the amount of Data in them\")","9b84a898":"top_20=df[df['city'].str.contains('Lahore')].groupby([\"area\"])['area'].count().nlargest(20).to_frame('count').reset_index()\ntop_20.index=top_20[\"area\"]\ntop_20.drop(columns=['area'],inplace=True)\ntop_20.plot.pie(y='count', figsize=(50, 15))","b475d8bc":"amenities_df =df\namenities_df.dropna(subset=['restaurants', 'hospitals','parks','schools'], how='all',inplace=True)\namenities_df.fillna({'restaurants':'', 'hospitals':'','parks':'','schools':''},inplace=True)\n","6bf84a42":"def amneties_count(obj):\n    if obj =='':\n        return 0\n    item_dict = json.dumps(ast.literal_eval(obj))\n    item_dict=json.loads(item_dict)\n    \n    return len(item_dict)\namenities_df['restaurants_count']=amenities_df['restaurants'].apply(amneties_count)\namenities_df['hospitals_count']=amenities_df['hospitals'].apply(amneties_count)\namenities_df['parks_count']=amenities_df['parks'].apply(amneties_count)\namenities_df['schools_count']=amenities_df['schools'].apply(amneties_count)\n\n\namenities_df.head()","6e8cd5ee":"def calculate_distance(lat1,lat2,lon1,lon2):\n    R = 6373.0\n    lat1 = radians(lat1)\n    lon1 = radians(lon1)\n    lat2 = radians(lat2)\n    lon2 = radians(lon2)\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat \/ 2)**2 + cos(lat1) * cos(lat2) * sin(dlon \/ 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n\n    distance = R * c\n    \n    return distance\n\ndef proximity_of_restaurants(latitude,longitude,restaurants,col):\n    #print(restaurants)\n    distance={}\n    if restaurants =='':\n        return 0\n    item_dict = json.dumps(ast.literal_eval(restaurants))\n    item_dict=json.loads(item_dict)\n    count=0\n    for obj in item_dict:\n        \n        key=col+str(count)\n        distance[key]=calculate_distance(float(obj['latitude']),float(latitude),float(longitude),float(obj['longitude']))\n        count+=1\n    return distance\n    \n    \namenities_df['restaurants_proximity']=amenities_df.apply(lambda x:proximity_of_restaurants(x.latitude,x.longitude,x.restaurants,\"restaurant\"), axis=1)\nrest_df = amenities_df[\"restaurants_proximity\"].apply(pd.Series )\nrest_df=rest_df[['restaurant0','restaurant1']]\nrest_df.fillna(99,inplace=True)\n\namenities_df['hospitals_proximity']=amenities_df.apply(lambda x:proximity_of_restaurants(x.latitude,x.longitude,x.hospitals,\"hospital\"), axis=1)\nhostp_df = amenities_df[\"hospitals_proximity\"].apply(pd.Series )\nhostp_df=hostp_df[['hospital0','hospital1']]\nhostp_df.fillna(99,inplace=True)\n\n\namenities_df['parks_proximity']=amenities_df.apply(lambda x:proximity_of_restaurants(x.latitude,x.longitude,x.parks,\"park\"), axis=1)\nparks_df = amenities_df[\"parks_proximity\"].apply(pd.Series )\nparks_df=parks_df[['park0','park1']]\nparks_df.fillna(99,inplace=True)\n\namenities_df['schools_proximity']=amenities_df.apply(lambda x:proximity_of_restaurants(x.latitude,x.longitude,x.schools,\"school\"), axis=1)\nschools_df = amenities_df[\"schools_proximity\"].apply(pd.Series )\nschools_df=schools_df[['school0','school1']]\nschools_df.fillna(99,inplace=True)\n\n\nresult = pd.concat([df, rest_df,hostp_df,parks_df,schools_df], axis=1)\n","ea688067":"result.head()","d9d3b88f":"ax = sns.boxplot(x=\"type\", y=\"price\", data=result)","90213b67":"from scipy import stats\nrefined_data = []\n\nproperty_types=result[\"type\"].unique().tolist()\n\nfor property_type in property_types:\n    property_type_df=result[result['type'].str.contains(property_type)]\n    z = np.abs(stats.zscore(property_type_df['price']))\n    property_type_df= property_type_df[(np.abs(stats.zscore(property_type_df['price'])) < 2)]\n    refined_data.append(property_type_df)\nrefined_data = pd.concat(refined_data)\n\n","a6326d98":"ax = sns.boxplot(x=\"type\", y=\"price\", data=refined_data)","9855333c":"extreme_outliers_data = []\nfor property_type in property_types:\n    property_type_df=result[result['type'].str.contains(property_type)]\n    z = np.abs(stats.zscore(property_type_df['price']))\n    property_type_df= property_type_df[(np.abs(stats.zscore(property_type_df['price'])) > 2)]\n    extreme_outliers_data.append(property_type_df)\n","5d48d673":"home=extreme_outliers_data[5]\nfarm_houses=extreme_outliers_data[3]\nupper_portion=extreme_outliers_data[2]\nflat=extreme_outliers_data[1]\nhouse=extreme_outliers_data[0]\n","7d031672":"home = home[['area', 'rawPrice','city','locationName']]\nhome","dd2328f6":"house = house[['area', 'rawPrice','city','locationName']]\nhouse","2686f480":"flat = flat[['area', 'rawPrice','city','locationName']]\nflat","d11731e9":"farm_houses = farm_houses[['area', 'rawPrice','city','locationName']]\nfarm_houses","3bf014b1":"upper_portion = upper_portion[['area', 'rawPrice','city','locationName']]\nupper_portion","2e25522b":"len(refined_data)","38668421":"def nearby_commercial_activity(row):\n    if row['restaurant0'] < 0.2 or  row['restaurant1']< 0.2 or row['hospital0'] < 0.2 or  row['hospital1']< 0.2 :\n        val = 1\n    else:\n        val = 0\n    return val\n\nrefined_data[\"commercial_activity\"]=refined_data.apply(nearby_commercial_activity, axis=1)\n","b0558d09":"def location_impact(row):\n    if row['park0'] < 0.3 or  row['park1']< 0.3 or row['school0'] < 0.3 or  row['school1']< 0.3 or row['restaurant0'] < 0.3 or  row['restaurant1']< 0.3 or row['hospital0'] < 0.3 or  row['hospital1']< 0.3:\n        val = 1  \n    else:\n        val = 0\n    return val\n\nrefined_data[\"location_impact\"]=refined_data.apply(location_impact, axis=1)\n\n","42f869b4":"type_with_commercial_impact=refined_data[refined_data['type'].str.contains('House') & refined_data['area'].str.contains('1 Kanal1 Kanal|10 Marla|3 Marla3 Marla|5 Marla5 Marla|8 Marla8 Marla') & refined_data['city'].str.contains('Lahore')].groupby([\"area\",\"commercial_activity\"])['price'].median().reset_index()\nax = sns.barplot(x=\"area\", y=\"price\", hue=\"commercial_activity\", data=type_with_commercial_impact)","6c4296a1":"import locale\ndef convert_to_square_feet(x):\n    if 'Marla'in x :\n        return float(re.split('\\s+', x)[0]) * 272.251\n    elif 'Kanal' in x:\n        return float(re.split('\\s+', x)[0].replace(',', '')) * 5445.0\n        \nrefined_data['area_sqfeet']=refined_data[\"area\"].apply(convert_to_square_feet)\nrefined_data  = refined_data[refined_data['area_sqfeet'].notna()]","23c5325b":"def no_of_beds(x):\n    digit=re.findall(r'\\d+', x)\n    if not digit:\n        return -1\n    else :\n        return int(digit[0])\n\nrefined_data['no_of_beds']=refined_data[\"bed\"].apply(no_of_beds)\nrefined_data = refined_data[refined_data.no_of_beds != -1]\n\n\n\ndef no_of_bath(x):\n    digit=re.findall(r'\\d+', x)\n    if not digit:\n        return -1\n    else :\n        return int(digit[0])\n\nrefined_data['no_of_baths']=refined_data[\"bath\"].apply(no_of_beds)\nrefined_data = refined_data[refined_data.no_of_baths != -1]\n\n","178b2785":"refined_data=refined_data[refined_data['area_sqfeet']>10]\n","b4856e4b":"\nrefined_data.drop(columns=['propertyId','area','bath','bed','amenities','link','rawPrice','location','address','purpose','name','latitude','longitude','restaurants','hospitals','description','parks','schools','restaurant0','restaurant1','hospital0','hospital1','park0','park1','school0','school1','restaurants_count','hospitals_count','parks_count','schools_count','hospitals_proximity', 'parks_proximity', 'schools_proximity','restaurants_proximity'],inplace=True)\n","34991664":"refined_data.columns","6075263b":"#plt.figure(figsize=(16, 6))\n# Store heatmap object in a variable to easily access it when you want to include more features (such as title).\n# Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.\nheatmap = sns.heatmap(refined_data.corr(), vmin=-1, vmax=1, annot=True)\n# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);\n","e656bb71":"from sklearn.preprocessing import LabelEncoder\n# creating instance of labelencoder\nlabelencoder = LabelEncoder()\n# Assigning numerical values and storing in another column\nrefined_data['locationName']= labelencoder.fit_transform(refined_data['locationName'])\nrefined_data['city']= labelencoder.fit_transform(refined_data['city'])\nrefined_data['type']= labelencoder.fit_transform(refined_data['type'])\n\n","fd4fda32":"heatmap = sns.heatmap(refined_data.corr(), vmin=-1, vmax=1, annot=True)\n# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);\n","59c97619":"refined_data.drop(columns=['no_of_baths','location_impact'],inplace=True)\n","1f9668cd":"import matplotlib.pyplot as plt \n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 8), sharey=True)\nsns.countplot(x=refined_data['no_of_beds'])\n","4c817925":"fig, ax =plt.subplots(1,2)\nsns.distplot(refined_data['area_sqfeet'], ax=ax[0])\nsns.distplot(np.log10(refined_data['area_sqfeet']), ax=ax[1])\nplt.xlabel('log10(area_sqfeet)')\nfig.show()","90c6b40f":"refined_data[\"log_area_sqfeet\"]=np.round(np.log10(refined_data['area_sqfeet']), 3)","754be705":"fig, ax =plt.subplots(1,2)\nsns.distplot(refined_data['price'], ax=ax[0])\nsns.distplot(np.sqrt(refined_data['price']), ax=ax[1])\nplt.xlabel('log10(price)')\nfig.show()\n","85518854":"refined_data[\"log_price\"]=np.round(np.sqrt(refined_data['price']), 3)","4797a0d1":"plt.scatter(refined_data['log_area_sqfeet'], refined_data['log_price'])\nplt.xlabel(\"Area\")\nplt.ylabel(\"Price\")\nplt.show()","fb8926be":"refined_data.drop(columns=['price','area_sqfeet'],inplace=True)\n","b483537d":"harea_index_list = refined_data[refined_data['log_area_sqfeet'] > 7].index.tolist()\nlarea_index_list = refined_data[refined_data['log_area_sqfeet'] < 2].index.tolist()\nprice_index_list = refined_data[refined_data['log_price'] < 4].index.tolist()\n\n\n\nrefined_data.drop(harea_index_list, inplace=True)\nrefined_data.drop(larea_index_list, inplace=True)\nrefined_data.drop(price_index_list, inplace=True)\n\n\n","b720e731":"# Again plottng the LotArea - SalePrice graph after the outlier removal\n\nplt.scatter(refined_data[\"log_area_sqfeet\"], refined_data[\"log_price\"])\nplt.title(\"After Outliers Removal\")\nplt.xlabel(\"Lot Area\")\nplt.ylabel(\"log10(Sale Price)\")\nplt.show()","2303b33d":"\nclass Models(object):\n    \n    global seed \n    seed = 34234\n    \n    # Initialization \n    def __init__(self, x_train, x_validation, y_train, y_validation):\n        # changing input as dataframe to list\n#         self.x_train = [x_train.iloc[i].tolist() for i in range(len(x_train))]\n#         self.x_validation = [x_validation.iloc[i].tolist() for i in range(len(x_validation))]\n#         self.y_train = y_train.tolist()\n#         self.y_validation = y_validation.tolist()\n        self.x_train=x_train\n        self.x_validation =x_validation\n        self.y_train=y_train\n        self.y_validation=y_validation\n    \n    @staticmethod\n    def print_info(cross_val_scores, mse):\n        print(\"Cross Validation Scores: \", cross_val_scores)\n        print(\"Mean Squared Error: \", mse)\n        \n        \n    # Linear Regression \n    def linear_regression(self, x_train, x_validation,  y_train, y_validation):\n        reg = linear_model.LinearRegression()\n        # X = np.array(X).reshape([-1, 1])\n        reg.fit(self.x_train, self.y_train)\n        y_pred_list = reg.predict(self.x_validation)\n        mse = mean_squared_error(self.y_validation, y_pred_list)\n        kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=seed)\n        cross_val_scores = cross_val_score(reg, self.x_train, self.y_train, cv=kfold)\n        print(\"\\nLinear Regression Model\")\n        self.print_info(cross_val_scores, mse)\n        return cross_val_scores, mse\n        \n    # Random Forest Regression model \n    def random_forest(self, x_train, x_validation,  y_train, y_validation):\n        rfr = RandomForestRegressor(n_estimators=8, max_depth=8, random_state=12, verbose=0)\n        # X = np.array(X).reshape([-1, 1])\n        rfr.fit(self.x_train, self.y_train)\n        y_pred_list = rfr.predict(self.x_validation)\n        mse = mean_squared_error(self.y_validation, y_pred_list)\n        kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=seed)\n        cross_val_scores = cross_val_score(rfr, self.x_train, self.y_train, cv=kfold)\n        print(\"\\nRandom Forest Regressor\")\n        self.print_info(cross_val_scores, mse)\n        return cross_val_scores, mse\n            \n    # Lasso method \n    def lasso(self, x_train, x_validation,  y_train, y_validation):\n        reg = linear_model.Lasso(alpha = 0.1)\n        # X = np.array(X).reshape([-1, 1])\n        reg.fit(self.x_train, self.y_train)\n        y_pred_list = reg.predict(self.x_validation)\n        mse = mean_squared_error(self.y_validation, y_pred_list)\n        kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=seed)\n        cross_val_scores = cross_val_score(reg, self.x_train, self.y_train, cv=kfold)\n        print(\"\\nLasso Regression Model\")\n        self.print_info(cross_val_scores, mse)\n        return cross_val_scores, mse\n    \n    # Gradient Boosing Regressor\n    def GBR(self, x_train, x_validation,  y_train, y_validation):\n        gbr = GradientBoostingRegressor(n_estimators=175, learning_rate=0.08, max_depth=3, random_state=1232, loss='ls')\n        gbr.fit(self.x_train, self.y_train)\n        kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=seed)\n        cross_val_scores = cross_val_score(gbr, self.x_train, self.y_train, cv=kfold)\n        mse = mean_squared_error(self.y_validation, gbr.predict(self.x_validation))\n        print('\\nGradient Boosting Regressor')\n        self.print_info(cross_val_scores, mse)\n        return cross_val_scores, mse\n#     def ridge_regression(self,x_train, x_validation,  y_train, y_validation):\n        \n#         ridge= Ridge(alpha=1.0)\n#         ridge.fit(self.x_train, self.y_train)\n#         kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=seed)\n#         cross_val_scores = cross_val_score(ridge, self.x_train, self.y_train, cv=kfold)\n#         mse = mean_squared_error(self.y_validation, ridge.predict(self.x_validation))\n#         print('\\nRidge Regressor')\n#         self.print_info(cross_val_scores, mse)","d0487a64":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV\n\nimport lightgbm\n\nfeature= refined_data.drop(['log_price'], axis=1)\ntarget= refined_data[['log_price']]\nfeature_train, feature_test, target_train, target_test= train_test_split(feature, target, test_size=0.12)\n\n\nprint('total feature training features: ', len(feature_train))\nprint('total feature testing features: ', len(feature_test))\nprint('total target training features: ', len(target_train))\nprint('total target testing features: ', len(target_test))","f0b9d044":"# We use GridSearchCV to find out the best set of parameters for GBR and use it for the \n#regression model analysis and prediction\nparameters = {\n    'n_estimators' : [170, 175, 180],\n    'learning_rate' : [0.075, 0.08, 0.1],\n    'max_depth' : [2, 3, 4]\n}\n\ngbr = GradientBoostingRegressor(n_estimators=250, learning_rate=0.1, max_depth=5, random_state=232, loss='ls')\ngs_cv = GridSearchCV(gbr, parameters).fit(feature_train, target_train.values.ravel())\ngs_cv.best_params_\n\n\n","a0a605d4":"from types import FunctionType\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import svm\nfrom sklearn.linear_model import Ridge\n\nmethods = [x for x, y in Models.__dict__.items() if type(y) == FunctionType]\nmethods.remove('__init__')\n# Now calling the all regression methods\ncross_scores_list, mse_list = [], []\nfor model in methods:\n    reg = Models(feature_train,feature_test ,target_train.values.ravel(), target_test.values.ravel())\n    cross_val_scores, mse = getattr(reg, model)(feature_train,feature_test , target_train.values.ravel(), target_test.values.ravel())\n    cross_scores_list.append(cross_val_scores)\n    mse_list.append(mse)","6598f0b2":"from sklearn.svm import SVR\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nprint(\"SVM\")\n\nclassifierSVR = SVR(kernel='linear', degree=3, gamma='auto', coef0=0.0, tol=0.001,\n                    C=.01, epsilon=0.1, shrinking=True, cache_size=200,\n                    verbose=False, max_iter=-1) \n\nkf = KFold(5, random_state=7, shuffle=True)    \ncv_y = []\ncv_pred = []\nfold = 0\n\nfor training, test in kf.split(feature_train):\n    fold+=1\n    pred = []\n        \n    scaler = preprocessing.StandardScaler()\n    x_train_fold = scaler.fit_transform(feature_train)\n    x_test_fold = scaler.transform(feature_test)\n\n    y_train_fold = (target_train)    \n    y_test_fold = (target_test)\n    \n    classifierSVR = classifierSVR.fit(x_train_fold, y_train_fold)\n    pred = classifierSVR.predict(x_test_fold)\n    cv_y.append(y_test_fold)\n    cv_pred.append(pred)        \n\n\ncv_y = np.concatenate(cv_y)\ncv_pred = np.concatenate(cv_pred)\nscore = np.sqrt(metrics.mean_squared_error(cv_y,cv_pred))\nprint(\"\\n Average RMSE: {}\".format(score))    \n\nRMSE_results['SVM'] = score","04b5bb56":"plot_df = pd.DataFrame()\nfor i in range(len(methods)):\n    plot_df[methods[i]] = cross_scores_list[i]","19b381fc":"plt.figure(figsize=(20,6))\nplt.title('Comparison of Algorithms')\nsns.boxplot(plot_df['linear_regression'])\nplt.ylabel('Cross Val Score')\nplt.show()","64b5a1e7":"plt.figure(figsize=(20,6))\nplt.title('Comparison of Algorithms')\nsns.boxplot(plot_df['random_forest'])\nplt.ylabel('Cross Val Score')\nplt.show()","9a4a17d5":"plt.figure(figsize=(20,6))\nplt.title('Comparison of Algorithms')\nsns.boxplot(plot_df['GBR'])\nplt.ylabel('Cross Val Score')\nplt.show()","72840da5":"plt.figure(figsize=(20,6))\nplt.title('Comparison of Algorithms')\nsns.boxplot(plot_df['lasso'])\nplt.ylabel('Cross Val Score')\n","3fb35099":"# Plot Mean Squared Error\n\nplt.plot(mse_list, c='b')\nplt.title('Comparision of Algorithms')\nplt.ylabel('Mean Squared Error')\nx = np.array([0,1,2,3])\nplt.scatter(x, mse_list, c='r', marker=\"s\")\nplt.xticks(x, methods)\nplt.show()\n","ed5ae675":"mse_list","d8908de2":"# Remove Outliers from each property type","baceeaae":"# Hot properties","b8003a49":"# Boosted regression trees & Grid Search\n![](http:\/\/)","5e1a5072":"# 1 kanal area ","5a42c37e":"# Lahore City ","f69fc1ea":"# Z-score\n\nZ-score is a parametric measure and it takes two parameters \u2014 mean and standard deviation.\nOnce you calculate these two parameters, finding the Z-score of a data point is easy.\n\nI used an arbitrary threshold of 2, beyond which all data points are flagged as outliers.\nThe rule of thumb is to use 2, 2.5, 3 or 3.5 as threshold.\n","f4074abd":"area when 1 kanal check least prices","7f2aea69":"* drop rows having sqfeet very less","2e708abb":"## Top 7 properties types in Pakistan\n\n\nPeople are more interested on giving The Upper Portion then Lower Portion ,And Farm Houses interestingly taking position in the Top 4\n","efda84de":"# Regression Models","8c3b7665":"## Cleaning data","a8077d1e":"# Data Normalaization\n# Normalizing the right skewed SalePrice\n","8a23f29f":"# Commercial area impact on price","41f9f949":"# proximity of Commercial activities","705eaaf3":"## Top 6 most Property selling cities\n   We can see that the Top 4 cities with respect to properties in Pakistan are Karachi, Lahore, Islamabad and Rawalpindi","79723f61":"# FEATURE ENGINEERING","ac5566cc":"# Least property prices wrt Area in Pakistan","4d6d3f9e":"location impact","84bb93f8":"* # Most Properties Selling Cities","3683c88c":"# Encoding\n\nThat\u2019s primarily the reason we need to convert categorical columns to numerical columns so that a machine learning algorithm understands it. This process is called categorical encoding.\n\n**Different Approaches to Categorical Encoding\n**\n* Label Encoding\n* One-Hot Encoding\n\nChallenges with Label Encoding\nIn the above scenario, the Country names do not have an order or rank. But, when label encoding is performed, the country names are ranked based on the alphabets. \n\n\nWe apply One-Hot Encoding when:\n\nThe categorical feature is not ordinal (like the countries above)\nThe number of categorical features is less so one-hot encoding can be effectively applied\nWe apply Label Encoding when:\n\nThe categorical feature is ordinal (like Jr. kg, Sr. kg, Primary school, high school)\n The number of categories is quite large as one-hot encoding can lead to high memory consumption\n \n\n","8272ab88":"# 3 Marla area","9ef1ef73":"# correlation","c1e78ce2":"## there is a high correlation between no_of_beds and no_of_baths so we are dropping one","d9239fc3":"## drop unnecessary row\n","97ceb289":"# Top 20 property areas in Lahore","ba6e8433":"# Amenities impact"}}