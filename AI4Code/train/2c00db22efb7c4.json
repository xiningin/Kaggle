{"cell_type":{"cf5f1d71":"code","196cc116":"code","9db64769":"code","4f3b1933":"code","99694801":"code","db3ad2a4":"code","92d99840":"code","4d360827":"code","ea288421":"code","26d50d2c":"code","3cf1f2ff":"code","16dc80fd":"code","365996fa":"code","51bb5b82":"code","4cec9d4b":"code","deb95df5":"code","4075e1c4":"code","27089c05":"code","f1d7b230":"code","f3b0c460":"code","8eef2883":"code","5695c5e2":"code","4c164b2e":"code","f6849fe5":"code","f5173d97":"code","15a9faed":"code","776bdaa3":"code","c94b92db":"code","133f93db":"code","14dcdada":"code","b4335823":"code","e9f728b0":"code","3b648756":"code","6cced752":"code","2ae9fdf6":"code","598205b0":"code","a9b5b22f":"markdown","52c4aa1d":"markdown","68bf2b80":"markdown","3ea29ebf":"markdown","9750210e":"markdown","d33e5fec":"markdown","eb368250":"markdown","5f95ba92":"markdown","bdb5a27a":"markdown"},"source":{"cf5f1d71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","196cc116":"import numpy as np \nimport pandas as pd \nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection, feature_extraction, linear_model\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.stem.snowball import SnowballStemmer\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9db64769":"# Reading in data\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","4f3b1933":"# Dimensions of train data\nprint(\"Train Dimensions: \", train.shape)\n# Dimensions of test data\nprint(\"Test Dimensions: \", test.shape)","99694801":"# Viewing first 5 rows of train data\ntrain.head()","db3ad2a4":"# Viewing first 5 rows of test data\ntest.head()","92d99840":"train.isnull().sum()","4d360827":"test.isnull().sum()","ea288421":"# Creating dataframe with percentage of missing values for train data\n\ntrain_perc_missing = train.isnull().mean()*100\npercentage_missing = pd.DataFrame({'Train Missing Percentage': train_perc_missing.sort_values(ascending=False)})\npercentage_missing","26d50d2c":"# Creating dataframe with percentage of missing values for test data\n\ntest_perc_missing = test.isnull().mean()*100\npercentage_missing = pd.DataFrame({'Test Missing Percentage': test_perc_missing.sort_values(ascending=False)})\npercentage_missing","3cf1f2ff":"# Selecting 'text' values that are non-disastrous\nnon_disastrous = train[train['target']==0]['text']\n\n# I inputted 4 to select the 4th row of the non-disastrous values\nnon_disastrous.values[4]","16dc80fd":"# Selecting 'text' values that are disastrous\ndisastrous = train[train['target']==1]['text']\n\n# I inputted 2 to select the 2th row of the disastrous values\ndisastrous.values[2]","365996fa":"# Viewing number of disastrous and non disastrous tweets\ntrain['target'].value_counts()","51bb5b82":"# Plotting number of disastrous and non disastrous tweets\nsns.barplot(train['target'].value_counts().index, train['target'].value_counts())","4cec9d4b":"# Checking number of unique values in 'keyword' feature\ntrain['keyword'].nunique()","deb95df5":"# Plotting the first 20 most common keywords\nfigure = plt.figure(figsize=(14,12))\nsns.barplot(y=train['keyword'].value_counts().index[:20], x=train['keyword'].value_counts()[:20])","4075e1c4":"print(train['location'].nunique())","27089c05":"# Viewing first 20 most common locations\nfigure = plt.figure(figsize=(14,12))\nsns.barplot(y=train['location'].value_counts().index[:20], x=train['location'].value_counts()[:20])","f1d7b230":"# Viewing last 20 least common locations\nfigure = plt.figure(figsize=(14,12))\nsns.barplot(y=train['location'].value_counts().index[-20:], x=train['location'].value_counts()[-20:])","f3b0c460":"def lowercase_text(text):\n    text = text.lower()\n    return text\n\ntrain['text'] = train['text'].apply(lambda x: lowercase_text(x))\ntest['text'] = test['text'].apply(lambda x: lowercase_text(x))","8eef2883":"train['text'].head()","5695c5e2":"train['text'].head()","4c164b2e":"# Removing punctuation, html tags, symbols, numbers, etc.\ndef remove_noise(text):\n    # Dealing with Punctuation\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","f6849fe5":"# Calling remove_noise function in order to remove noise\ntrain['text'] = train['text'].apply(lambda x: remove_noise(x))\ntest['text'] = test['text'].apply(lambda x: remove_noise(x))","f5173d97":"train['text'].head(20)","15a9faed":"#Remove StopWords\n\n!pip install nlppreprocess\nfrom nlppreprocess import NLP\n\nnlp = NLP()\n\ntrain['text'] = train['text'].apply(nlp.process)\ntest['text'] = test['text'].apply(nlp.process)  ","776bdaa3":"train['text'].head(20)","c94b92db":"#Stemming\nstemmer = SnowballStemmer(\"english\")\n\ndef stemming(text):\n    text = [stemmer.stem(word) for word in text.split()]\n    return ' '.join(text)\n\ntrain['text'] = train['text'].apply(stemming)\ntest['text'] = test['text'].apply(stemming)","133f93db":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfig, (ax1) = plt.subplots(1, figsize=[24,20])\nwordcloud = WordCloud( background_color='white',\n                        width=600,\n                        height=600).generate(\" \".join(train['text']))\nax1.imshow(wordcloud)\nax1.axis('off')\nax1.set_title('Frequent Words',fontsize=16);","14dcdada":"# Using CountVectorizer to change the teweets to vectors\ncount_vectorizer = CountVectorizer(analyzer='word', binary=True)\ncount_vectorizer.fit(train['text'])\n\ntrain_vectors = count_vectorizer.fit_transform(train['text'])\ntest_vectors = count_vectorizer.transform(test['text'])\n\n\n# Printing first vector\nprint(train_vectors[0].todense())","b4335823":"y = train['target']","e9f728b0":"# Creating a simple MultinomialNB model\nmodel = MultinomialNB(alpha=1)\n\n# Using cross validation to print out our scores\nscores = model_selection.cross_val_score(model, train_vectors, y, cv=3, scoring=\"f1\")\nscores","3b648756":"# Training model with train_vectors and target variable\nmodel.fit(train_vectors, y)","6cced752":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","2ae9fdf6":"# Predicting model with the test data that was vectorized (test_vectors)\nsample_submission['target'] = model.predict(test_vectors)\n\n\n# Viewing submission\nsample_submission.head()","598205b0":"# Submission\nsample_submission.to_csv(\"submission.csv\", index=False)","a9b5b22f":"<a id='3'><\/a>\n## Text Preprocessing\n\n\n### Text PreProcessing Method:\n- **LOWERCASE:** Make all words **UPPERCASE** or **lowercase**\n- **STOPWORDS:** Remove stopwords - words that commonly come up and don't really have much meaning. For Ex: \"I\"\n- **TOKENIZATION:** Basically splitting strings into numbers  we want\n- **NOISE:** Removing Punctuation, Numbers, etc.\n- **STEMMING:** Transforming words to their base root by cutting off the end of the word. (e.g. 'Running', 'Runner' = 'Run). Some words may be cut off too much (e.g. 'Spotify' -> 'Spotif') but it doesn't make a big difference\n- **LEMMATIZATION:** Similar to stemming, except instead of cutting off the end of the word it basically counts similar words as one lemma. (e.g. 'Great', 'Good' = 'Good') Note: We don't include this step because we already used stemming so it isn't necessary.\n\nRemember, you do need to use all of these steps. An extensive text preprocessing method may lead to worse results.","52c4aa1d":"### Missing Values\n\nChecking the count and percentage of missing values","68bf2b80":"### What do these features mean?\n- `id`: Used for submission\n- `keyword`: Keyword of the tweet\n- `location`: Location the tweet was sent from\n- `text`: The Tweet itself","3ea29ebf":"### Removing Text Noise\n\nWe create a prebuilt function and call it in order to remove punctuation, html tags, urls, numbers, etc.\n","9750210e":"There are way too many different locations. So there may be possible duplicates of places.\n\n**For Ex:**\n\n- 'Mumbai, India':'India'\n\n- 'Ireland, Britain':'Britain'\n\n- 'United States':'USA'\n\nLets view the most common places.","d33e5fec":"<a id='2'><\/a>\n## Reading and Inspecting Data\n\n**Section Contents:**\n- Read Data \n- Missing Values \n- Viewing tweets\n- Inspecting target feature\n- Inspecting keyword feature\n- Inspecting location feature","eb368250":"We then use this function in order to deal with Text Noise","5f95ba92":"After viewing the data:\n1. Train data consists of 5 features (id, keyword, location, text and target)\n2. Test data consists of 4 features (id, keyword, location, and text)\n\nTest data doesnt contain the'target' because that is what we are trying to predict with our train data!","bdb5a27a":"### Lowercase Text\n\nWe lowercase every word so the model doesn't treat an uppercased version different from the lowercased version\n\n**For example:**\n- EARTH and earth are supposed to be identical\n"}}