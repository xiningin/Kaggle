{"cell_type":{"ae82cdd9":"code","2e991a84":"code","5184c976":"code","b20b92d2":"code","ab43463d":"code","a99362f0":"code","207fec59":"code","b237d1ef":"code","392d20ae":"code","1f3be87a":"code","4e789c18":"code","0b20f681":"code","1075d417":"code","55ade328":"code","f303ac50":"code","7d8f0383":"code","a22b00af":"code","06dcf269":"code","def2f413":"code","b2713757":"code","80a87021":"code","073f8933":"code","66e8a3fd":"code","6a69f27b":"code","47d24993":"code","fc2d8631":"code","29b2c745":"code","982c4c61":"code","f8e116e8":"code","5e476f5e":"code","daa6ff2e":"code","b6287bc3":"code","b0a27053":"code","e822977a":"code","070ecdda":"code","5992851c":"code","07461a90":"code","27ff7e2b":"code","2e3c16b3":"code","e7ee73c8":"code","115e854d":"code","d19e3415":"code","c44e04d3":"code","e6ffe367":"code","da620d9c":"code","ed86afd6":"code","0d1d87a1":"code","4f1979c1":"code","71c9a163":"code","30f1296f":"code","f9e95ceb":"code","8c49704b":"code","81d3efa4":"code","863b6b63":"code","88c3fc4e":"code","c3e23037":"code","72cf630a":"code","948ba34b":"code","49ce8a7d":"code","798dbaa8":"code","0bc458ff":"code","25a2097f":"code","273d2373":"code","1cc9e0d2":"code","7bf31c4d":"code","80175276":"code","a1eef09e":"code","efda76f9":"code","b3b71fd4":"code","055fdf8e":"code","b5981e8b":"code","8b51f43a":"code","66134443":"code","e43d37ab":"code","1388573a":"code","e7ca69a6":"code","96ed5a66":"code","9499b9cf":"code","a043ca77":"code","90a2ce8f":"code","9a5f1576":"markdown","43887b81":"markdown","da72cd84":"markdown","88d838bb":"markdown","6a3a467f":"markdown","142bebe9":"markdown","dffe4833":"markdown","98f5c991":"markdown","5cdbabe5":"markdown","1ad4deb5":"markdown","acfc81e4":"markdown","2534fd22":"markdown","6ac79619":"markdown","25ef9e64":"markdown","b597bc44":"markdown","7ed363e9":"markdown","899801cf":"markdown","b560d172":"markdown","5b651c04":"markdown","4267cf0d":"markdown","d60fbf7d":"markdown","904b6fbe":"markdown","0d0e0124":"markdown","62b950e1":"markdown","69fd5a09":"markdown","ad5b81b1":"markdown","3c80bc78":"markdown","f30f65d4":"markdown","a5daea74":"markdown","499143eb":"markdown","8da949f4":"markdown","46813dd0":"markdown","9b9b2bcd":"markdown","092abd40":"markdown","a6e27a90":"markdown","a886b858":"markdown","26157fa0":"markdown","645eb2d7":"markdown","de14a9bd":"markdown","864a230a":"markdown","f8064dc2":"markdown","e95c6041":"markdown","836a6314":"markdown"},"source":{"ae82cdd9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2e991a84":"import re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nfrom collections import defaultdict\nfrom collections import Counter\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.initializers import Constant\nfrom keras.layers import (LSTM, \n                          Embedding, \n                          BatchNormalization,\n                          Dense, \n                          TimeDistributed, \n                          Dropout, \n                          Bidirectional,\n                          Flatten, \n                          GlobalMaxPool1D)\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\n\nfrom sklearn.metrics import (\n    precision_score, \n    recall_score, \n    f1_score, \n    classification_report,\n    accuracy_score\n)","5184c976":"# Defining all our palette colours.\nprimary_blue = \"#496595\"\nprimary_blue2 = \"#85a1c1\"\nprimary_blue3 = \"#3f4d63\"\nprimary_grey = \"#c6ccd8\"\nprimary_black = \"#202022\"\nprimary_bgcolor = \"#f4f0ea\"\n\nprimary_green = px.colors.qualitative.Plotly[2]","b20b92d2":"df = pd.read_csv(\"\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv\", encoding=\"latin-1\")\n\ndf = df.dropna(how=\"any\", axis=1)\ndf.columns = ['target', 'message']\n\ndf.head()","ab43463d":"df['message_len'] = df['message'].apply(lambda x: len(x.split(' ')))\ndf.head()","a99362f0":"max(df['message_len'])","207fec59":"balance_counts = df.groupby('target')['target'].agg('count').values\nbalance_counts","b237d1ef":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    x=['ham'],\n    y=[balance_counts[0]],\n    name='ham',\n    text=[balance_counts[0]],\n    textposition='auto',\n    marker_color=primary_blue\n))\nfig.add_trace(go.Bar(\n    x=['spam'],\n    y=[balance_counts[1]],\n    name='spam',\n    text=[balance_counts[1]],\n    textposition='auto',\n    marker_color=primary_grey\n))\nfig.update_layout(\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Dataset distribution by target<\/span>'\n)\nfig.show()","392d20ae":"ham_df = df[df['target'] == 'ham']['message_len'].value_counts().sort_index()\nspam_df = df[df['target'] == 'spam']['message_len'].value_counts().sort_index()\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=ham_df.index,\n    y=ham_df.values,\n    name='ham',\n    fill='tozeroy',\n    marker_color=primary_blue,\n))\nfig.add_trace(go.Scatter(\n    x=spam_df.index,\n    y=spam_df.values,\n    name='spam',\n    fill='tozeroy',\n    marker_color=primary_grey,\n))\nfig.update_layout(\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Data Roles in Different Fields<\/span>'\n)\nfig.update_xaxes(range=[0, 70])\nfig.show()","1f3be87a":"# Special thanks to https:\/\/www.kaggle.com\/tanulsingh077 for this function\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","4e789c18":"df['message_clean'] = df['message'].apply(clean_text)\ndf.head()","0b20f681":"stop_words = stopwords.words('english')\nmore_stopwords = ['u', 'im', 'c']\nstop_words = stop_words + more_stopwords\n\ndef remove_stopwords(text):\n    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n    return text\n    \ndf['message_clean'] = df['message_clean'].apply(remove_stopwords)\ndf.head()","1075d417":"stemmer = nltk.SnowballStemmer(\"english\")\n\ndef stemm_text(text):\n    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n    return text","55ade328":"df['message_clean'] = df['message_clean'].apply(stemm_text)\ndf.head()","f303ac50":"def preprocess_data(text):\n    # Clean puntuation, urls, and so on\n    text = clean_text(text)\n    # Remove stopwords\n    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n    # Stemm all the words in the sentence\n    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n    \n    return text","7d8f0383":"df['message_clean'] = df['message_clean'].apply(preprocess_data)\ndf.head()","a22b00af":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(df['target'])\n\ndf['target_encoded'] = le.transform(df['target'])\ndf.head()","06dcf269":"twitter_mask = np.array(Image.open('\/kaggle\/input\/masksforwordclouds\/twitter_mask3.jpg'))\n\nwc = WordCloud(\n    background_color='white', \n    max_words=200, \n    mask=twitter_mask,\n)\nwc.generate(' '.join(text for text in df.loc[df['target'] == 'ham', 'message_clean']))\nplt.figure(figsize=(18,10))\nplt.title('Top words for HAM messages', \n          fontdict={'size': 22,  'verticalalignment': 'bottom'})\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","def2f413":"twitter_mask = np.array(Image.open('\/kaggle\/input\/masksforwordclouds\/twitter_mask3.jpg'))\n\nwc = WordCloud(\n    background_color='white', \n    max_words=200, \n    mask=twitter_mask,\n)\nwc.generate(' '.join(text for text in df.loc[df['target'] == 'spam', 'message_clean']))\nplt.figure(figsize=(18,10))\nplt.title('Top words for SPAM messages', \n          fontdict={'size': 22,  'verticalalignment': 'bottom'})\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","b2713757":"# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\nx = df['message_clean']\ny = df['target_encoded']\n\nprint(len(x), len(y))","80a87021":"# Split into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\nprint(len(x_train), len(y_train))\nprint(len(x_test), len(y_test))","073f8933":"from sklearn.feature_extraction.text import CountVectorizer\n\n# instantiate the vectorizer\nvect = CountVectorizer()\nvect.fit(x_train)","66e8a3fd":"# Use the trained to create a document-term matrix from train and test sets\nx_train_dtm = vect.transform(x_train)\nx_test_dtm = vect.transform(x_test)","6a69f27b":"vect_tunned = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.1, max_df=0.7, max_features=100)","47d24993":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer()\n\ntfidf_transformer.fit(x_train_dtm)\nx_train_tfidf = tfidf_transformer.transform(x_train_dtm)\n\nx_train_tfidf","fc2d8631":"texts = df['message_clean']\ntarget = df['target_encoded']","29b2c745":"# Calculate the length of our vocabulary\nword_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(texts)\n\nvocab_length = len(word_tokenizer.word_index) + 1\nvocab_length","982c4c61":"def embed(corpus): \n    return word_tokenizer.texts_to_sequences(corpus)\n\nlongest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))\nlength_long_sentence = len(word_tokenize(longest_train))\n\ntrain_padded_sentences = pad_sequences(\n    embed(texts), \n    length_long_sentence, \n    padding='post'\n)\n\ntrain_padded_sentences","f8e116e8":"embeddings_dictionary = dict()\nembedding_dim = 100\n\n# Load GloVe 100D embeddings\nwith open('\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt') as fp:\n    for line in fp.readlines():\n        records = line.split()\n        word = records[0]\n        vector_dimensions = np.asarray(records[1:], dtype='float32')\n        embeddings_dictionary [word] = vector_dimensions\n\n# embeddings_dictionary","5e476f5e":"# Now we will load embedding vectors of those words that appear in the\n# Glove dictionary. Others will be initialized to 0.\n\nembedding_matrix = np.zeros((vocab_length, embedding_dim))\n\nfor word, index in word_tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n        \nembedding_matrix","daa6ff2e":"import plotly.figure_factory as ff\n\nx_axes = ['Ham', 'Spam']\ny_axes =  ['Spam', 'Ham']\n\ndef conf_matrix(z, x=x_axes, y=y_axes):\n    \n    z = np.flip(z, 0)\n\n    # change each element of z to type string for annotations\n    z_text = [[str(y) for y in x] for x in z]\n\n    # set up figure \n    fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n\n    # add title\n    fig.update_layout(title_text='<b>Confusion matrix<\/b>',\n                      xaxis = dict(title='Predicted value'),\n                      yaxis = dict(title='Real value')\n                     )\n\n    # add colorbar\n    fig['data'][0]['showscale'] = True\n    \n    return fig","b6287bc3":"# Create a Multinomial Naive Bayes model\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\n\n# Train the model\nnb.fit(x_train_dtm, y_train)","b0a27053":"# Make class anf probability predictions\ny_pred_class = nb.predict(x_test_dtm)\ny_pred_prob = nb.predict_proba(x_test_dtm)[:, 1]","e822977a":"# calculate accuracy of class predictions\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred_class))\n\nconf_matrix(metrics.confusion_matrix(y_test, y_pred_class))","070ecdda":"# Calculate AUC\nmetrics.roc_auc_score(y_test, y_pred_prob)","5992851c":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([('bow', CountVectorizer()), \n                 ('tfid', TfidfTransformer()),  \n                 ('model', MultinomialNB())])","07461a90":"# Fit the pipeline with the data\npipe.fit(x_train, y_train)\n\ny_pred_class = pipe.predict(x_test)\n\nprint(metrics.accuracy_score(y_test, y_pred_class))\n\nconf_matrix(metrics.confusion_matrix(y_test, y_pred_class))","27ff7e2b":"import xgboost as xgb\n\npipe = Pipeline([\n    ('bow', CountVectorizer()), \n    ('tfid', TfidfTransformer()),  \n    ('model', xgb.XGBClassifier(\n        learning_rate=0.1,\n        max_depth=7,\n        n_estimators=80,\n        use_label_encoder=False,\n        eval_metric='auc',\n        # colsample_bytree=0.8,\n        # subsample=0.7,\n        # min_child_weight=5,\n    ))\n])","2e3c16b3":"# Fit the pipeline with the data\npipe.fit(x_train, y_train)\n\ny_pred_class = pipe.predict(x_test)\ny_pred_train = pipe.predict(x_train)\n\nprint('Train: {}'.format(metrics.accuracy_score(y_train, y_pred_train)))\nprint('Test: {}'.format(metrics.accuracy_score(y_test, y_pred_class)))\n\nconf_matrix(metrics.confusion_matrix(y_test, y_pred_class))","e7ee73c8":"# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    train_padded_sentences, \n    target, \n    test_size=0.25\n)","115e854d":"# Model from https:\/\/www.kaggle.com\/mariapushkareva\/nlp-disaster-tweets-with-glove-and-lstm\/data\n\ndef glove_lstm():\n    model = Sequential()\n    \n    model.add(Embedding(\n        input_dim=embedding_matrix.shape[0], \n        output_dim=embedding_matrix.shape[1], \n        weights = [embedding_matrix], \n        input_length=length_long_sentence\n    ))\n    \n    model.add(Bidirectional(LSTM(\n        length_long_sentence, \n        return_sequences = True, \n        recurrent_dropout=0.2\n    )))\n    \n    model.add(GlobalMaxPool1D())\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model\n\nmodel = glove_lstm()\nmodel.summary()","d19e3415":"# Load the model and train!!\n\nmodel = glove_lstm()\n\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)\nhistory = model.fit(\n    X_train, \n    y_train, \n    epochs = 7,\n    batch_size = 32,\n    validation_data = (X_test, y_test),\n    verbose = 1,\n    callbacks = [reduce_lr, checkpoint]\n)","c44e04d3":"def plot_learning_curves(history, arr):\n    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n    for idx in range(2):\n        ax[idx].plot(history.history[arr[idx][0]])\n        ax[idx].plot(history.history[arr[idx][1]])\n        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)\n        ax[idx].set_xlabel('A ',fontsize=16)\n        ax[idx].set_ylabel('B',fontsize=16)\n        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)","e6ffe367":"plot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])","da620d9c":"y_preds = (model.predict(X_test) > 0.5).astype(\"int32\")\nconf_matrix(metrics.confusion_matrix(y_test, y_preds))","ed86afd6":"# install transformers\n!pip install transformers","0d1d87a1":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer","4f1979c1":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \nexcept:\n    strategy = tf.distribute.get_strategy()\n    \nprint('Number of replicas in sync: ', strategy.num_replicas_in_sync)","71c9a163":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n\ndef bert_encode(data, maximum_length) :\n    input_ids = []\n    attention_masks = []\n\n    for text in data:\n        encoded = tokenizer.encode_plus(\n            text, \n            add_special_tokens=True,\n            max_length=maximum_length,\n            pad_to_max_length=True,\n\n            return_attention_mask=True,\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n        \n    return np.array(input_ids),np.array(attention_masks)","30f1296f":"texts = df['message_clean']\ntarget = df['target_encoded']\n\ntrain_input_ids, train_attention_masks = bert_encode(texts,60)","f9e95ceb":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\n\ndef create_model(bert_model):\n    \n    input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n\n    output = bert_model([input_ids,attention_masks])\n    output = output[1]\n    output = tf.keras.layers.Dense(32,activation='relu')(output)\n    output = tf.keras.layers.Dropout(0.2)(output)\n    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n    \n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    return model","8c49704b":"from transformers import TFBertModel\nbert_model = TFBertModel.from_pretrained('bert-base-uncased')","81d3efa4":"model = create_model(bert_model)\nmodel.summary()","863b6b63":"history = model.fit(\n    [train_input_ids, train_attention_masks],\n    target,\n    validation_split=0.2, \n    epochs=3,\n    batch_size=10\n)","88c3fc4e":"plot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])","c3e23037":"df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\", encoding=\"latin-1\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\", encoding=\"latin-1\")\n\ndf = df.dropna(how=\"any\", axis=1)\ndf['text_len'] = df['text'].apply(lambda x: len(x.split(' ')))\n\ndf.head()","72cf630a":"balance_counts = df.groupby('target')['target'].agg('count').values\nbalance_counts","948ba34b":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    x=['Fake'],\n    y=[balance_counts[0]],\n    name='Fake',\n    text=[balance_counts[0]],\n    textposition='auto',\n    marker_color=primary_blue\n))\nfig.add_trace(go.Bar(\n    x=['Real disaster'],\n    y=[balance_counts[1]],\n    name='Real disaster',\n    text=[balance_counts[1]],\n    textposition='auto',\n    marker_color=primary_grey\n))\nfig.update_layout(\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Dataset distribution by target<\/span>'\n)\nfig.show()","49ce8a7d":"disaster_df = df[df['target'] == 1]['text_len'].value_counts().sort_index()\nfake_df = df[df['target'] == 0]['text_len'].value_counts().sort_index()\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=disaster_df.index,\n    y=disaster_df.values,\n    name='Real disaster',\n    fill='tozeroy',\n    marker_color=primary_blue,\n))\nfig.add_trace(go.Scatter(\n    x=fake_df.index,\n    y=fake_df.values,\n    name='Fake',\n    fill='tozeroy',\n    marker_color=primary_grey,\n))\nfig.update_layout(\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Data Roles in Different Fields<\/span>'\n)\nfig.show()","798dbaa8":"def remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n# Special thanks to https:\/\/www.kaggle.com\/tanulsingh077 for this function\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub(\n        'http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \n        '', \n        text\n    )\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    text = remove_url(text)\n    text = remove_emoji(text)\n    text = remove_html(text)\n    \n    return text","0bc458ff":"# Test emoji removal\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","25a2097f":"stop_words = stopwords.words('english')\nmore_stopwords = ['u', 'im', 'c']\nstop_words = stop_words + more_stopwords\n\nstemmer = nltk.SnowballStemmer(\"english\")\n\ndef preprocess_data(text):\n    # Clean puntuation, urls, and so on\n    text = clean_text(text)\n    # Remove stopwords and Stemm all the words in the sentence\n    text = ' '.join(stemmer.stem(word) for word in text.split(' ') if word not in stop_words)\n\n    return text","273d2373":"test_df['text_clean'] = test_df['text'].apply(preprocess_data)\n\ndf['text_clean'] = df['text'].apply(preprocess_data)\ndf.head()","1cc9e0d2":"def create_corpus_df(tweet, target):\n    corpus=[]\n    \n    for x in tweet[tweet['target']==target]['text_clean'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","7bf31c4d":"corpus_disaster_tweets = create_corpus_df(df, 1)\n\ndic=defaultdict(int)\nfor word in corpus_disaster_tweets:\n    dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\ntop","80175276":"twitter_mask = np.array(Image.open('\/kaggle\/input\/masksforwordclouds\/twitter_mask3.jpg'))\n\nwc = WordCloud(\n    background_color='white', \n    max_words=200, \n    mask=twitter_mask,\n)\nwc.generate(' '.join(text for text in df.loc[df['target'] == 1, 'text_clean']))\nplt.figure(figsize=(18,10))\nplt.title('Top words for Real Disaster tweets', \n          fontdict={'size': 22,  'verticalalignment': 'bottom'})\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","a1eef09e":"corpus_disaster_tweets = create_corpus_df(df, 0)\n\ndic=defaultdict(int)\nfor word in corpus_disaster_tweets:\n    dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\ntop","efda76f9":"wc = WordCloud(\n    background_color='white', \n    max_words=200, \n    mask=twitter_mask,\n)\nwc.generate(' '.join(text for text in df.loc[df['target'] == 0, 'text_clean']))\nplt.figure(figsize=(18,10))\nplt.title('Top words for Fake messages', \n          fontdict={'size': 22,  'verticalalignment': 'bottom'})\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","b3b71fd4":"# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\nx = df['text_clean']\ny = df['target']\n\n# Split into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\nprint(len(x_train), len(y_train))\nprint(len(x_test), len(y_test))","055fdf8e":"pipe = Pipeline([\n    ('bow', CountVectorizer()), \n    ('tfid', TfidfTransformer()),  \n    ('model', xgb.XGBClassifier(\n        use_label_encoder=False,\n        eval_metric='auc',\n    ))\n])\nfrom sklearn import metrics\n\n# Fit the pipeline with the data\npipe.fit(x_train, y_train)\n\ny_pred_class = pipe.predict(x_test)\ny_pred_train = pipe.predict(x_train)\n\nprint('Train: {}'.format(metrics.accuracy_score(y_train, y_pred_train)))\nprint('Test: {}'.format(metrics.accuracy_score(y_test, y_pred_class)))\n\nconf_matrix(metrics.confusion_matrix(y_test, y_pred_class))","b5981e8b":"train_tweets = df['text_clean'].values\ntest_tweets = test_df['text_clean'].values\ntrain_target = df['target'].values","8b51f43a":"# Calculate the length of our vocabulary\nword_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(train_tweets)\n\nvocab_length = len(word_tokenizer.word_index) + 1\nvocab_length","66134443":"def show_metrics(pred_tag, y_test):\n    print(\"F1-score: \", f1_score(pred_tag, y_test))\n    print(\"Precision: \", precision_score(pred_tag, y_test))\n    print(\"Recall: \", recall_score(pred_tag, y_test))\n    print(\"Acuracy: \", accuracy_score(pred_tag, y_test))\n    print(\"-\"*50)\n    print(classification_report(pred_tag, y_test))\n    \ndef embed(corpus): \n    return word_tokenizer.texts_to_sequences(corpus)","e43d37ab":"longest_train = max(train_tweets, key=lambda sentence: len(word_tokenize(sentence)))\nlength_long_sentence = len(word_tokenize(longest_train))\n\ntrain_padded_sentences = pad_sequences(\n    embed(train_tweets), \n    length_long_sentence, \n    padding='post'\n)\ntest_padded_sentences = pad_sequences(\n    embed(test_tweets), \n    length_long_sentence,\n    padding='post'\n)\n\ntrain_padded_sentences","1388573a":"# Load GloVe 100D embeddings\n# We are not going to do it here as they were loaded earlier.","e7ca69a6":"# Now we will load embedding vectors of those words that appear in the\n# Glove dictionary. Others will be initialized to 0.\n\nembedding_matrix = np.zeros((vocab_length, embedding_dim))\n\nfor word, index in word_tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n        \nembedding_matrix","96ed5a66":"# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    train_padded_sentences, \n    train_target, \n    test_size=0.25\n)","9499b9cf":"# Load the model and train!!\n\nmodel = glove_lstm()\n\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)\nhistory = model.fit(\n    X_train, \n    y_train, \n    epochs = 7,\n    batch_size = 32,\n    validation_data = (X_test, y_test),\n    verbose = 1,\n    callbacks = [reduce_lr, checkpoint]\n)","a043ca77":"plot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])","90a2ce8f":"preds = model.predict_classes(X_test)\nshow_metrics(preds, y_test)","9a5f1576":"### Pad_sequences\n\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/sequence\/pad_sequences\n\n```python\ntf.keras.preprocessing.sequence.pad_sequences(\n    sequences, maxlen=None, dtype='int32', padding='pre',\n    truncating='pre', value=0.0\n)\n```\n\nThis function transforms a list (of length num_samples) of sequences (lists of integers) into a 2D Numpy array of shape (num_samples, num_timesteps). num_timesteps is either the maxlen argument if provided, or the length of the longest sequence in the list.\n\n```python\n>>> sequence = [[1], [2, 3], [4, 5, 6]]\n>>> tf.keras.preprocessing.sequence.pad_sequences(sequence, padding='post')\narray([[1, 0, 0],\n       [2, 3, 0],\n       [4, 5, 6]], dtype=int32)\n```","43887b81":"<a id='6.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">6.3 XGBoost<\/p>","da72cd84":"<a id='9.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">9.2 Data preprocessing<\/p>","88d838bb":"<a id='3.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">3.2 Stemming \ud83d\udee0<\/p>\n\n### Stemming\/ Lematization\nFor grammatical reasons, documents are going to use different forms of a word, such as *write, writing and writes*. Additionally, there are families of derivationally related words with similar meanings. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n\n**Stemming** usually refers to a process that chops off the ends of words in the hope of achieving goal correctly most of the time and often includes the removal of derivational affixes.\n\n**Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base and dictionary form of a word\n\n![stemm-lemmatization.png](attachment:stemm-lemmatization.png)\n\nAs far as the meaning of the words is not important for this study, we will focus on stemming rather than lemmatization.\n\n### Stemming algorithms\n\nThere are several stemming algorithms implemented in NLTK Python library:\n1. **PorterStemmer** uses *Suffix Stripping* to produce stems. **PorterStemmer is known for its simplicity and speed**. Notice how the PorterStemmer is giving the root (stem) of the word \"cats\" by simply removing the 's' after cat. This is a suffix added to cat to make it plural. But if you look at 'trouble', 'troubling' and 'troubled' they are stemmed to 'trouble' because *PorterStemmer algorithm does not follow linguistics rather a set of 05 rules for different cases that are applied in phases (step by step) to generate stems*. This is the reason why PorterStemmer does not often generate stems that are actual English words. It does not keep a lookup table for actual stems of the word but applies algorithmic rules to generate stems. It uses the rules to decide whether it is wise to strip a suffix.\n2. One can generate its own set of rules for any language that is why Python nltk introduced **SnowballStemmers** that are used to create non-English Stemmers!\n3. **LancasterStemmer** (Paice-Husk stemmer) is an iterative algorithm with rules saved externally. One table containing about 120 rules indexed by the last letter of a suffix. On each iteration, it tries to find an applicable rule by the last character of the word. Each rule specifies either a deletion or replacement of an ending. If there is no such rule, it terminates. It also terminates if a word starts with a vowel and there are only two letters left or if a word starts with a consonant and there are only three characters left. Otherwise, the rule is applied, and the process repeats.","6a3a467f":"As we can see, the `ham` message length tend to be lower than `spam` message length.","142bebe9":"### Real disasters","dffe4833":"<a id='9.5'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">9.5 GloVe - LSTM<\/p>\n\nThanks to: https:\/\/www.kaggle.com\/mariapushkareva\/nlp-disaster-tweets-with-glove-and-lstm\n\nWe are going to use **LSTM (long short-term memory)** model .","98f5c991":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Loading Data \ud83d\udc8e<\/p>\n\nJust load the dataset and global variables for colors and so on.","5cdbabe5":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 15px 50px;\">Table of Content<\/p>\n\n* [1. Loading Data \ud83d\udc8e](#1)\n* [2. EDA \ud83d\udcca](#2)\n* [3. Data Preprocessing \u2699\ufe0f](#3)\n    * [3.1 Cleaning the corpus \ud83d\udee0](#3.1)\n    * [3.2 Stemming \ud83d\udee0](#3.2)\n    * [3.3 All together \ud83d\udee0](#3.3)\n    * [3.4 Target encoding \ud83d\udee0](#3.4)\n* [4. Tokens visualization \ud83d\udcca](#4)\n* [5. Vectorization](#5)\n    * [5.1 Tunning CountVectorizer](#5.1)\n    * [5.2 TF-IDF](#5.2)\n    * [5.3 Word Embeddings: GloVe](#5.3)\n* [6. Modeling](#6)\n    * [6.1 Naive Bayes DTM](#6.1)\n    * [6.2 Naive Bayes TF-IDF](#6.2)\n    * [6.3 XGBoost](#6.3)\n* [7. LSTM](#7)\n* [8. BERT](#8)\n* [9. NLP: Disaster tweets](#9)\n    * [9.1 EDA](#9.1)\n    * [9.2 Data preprocessing](#9.2)\n    * [9.3 WordCloud](#9.3)\n    * [9.4 Modeling](#9.4)\n    * [9.5 GloVe - LSTM](#9.5)","1ad4deb5":"<a id='6'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">6. Modeling<\/p>","acfc81e4":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">Natural Languague Processing \ud83d\udcdd A complete Guide<\/p>","2534fd22":"###  Results","6ac79619":"<a id='7'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">7. LSTM<\/p>","25ef9e64":"### Fake disasters","b597bc44":"<a id='5.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">5.1 Tunning CountVectorizer<\/p>\n\nCountVectorizer has a few parameters you should know.\n\n1. **stop_words**: Since CountVectorizer just counts the occurrences of each word in its vocabulary, extremely common words like \u2018the\u2019, \u2018and\u2019, etc. will become very important features while they add little meaning to the text. Your model can often be improved if you don\u2019t take those words into account. Stop words are just a list of words you don\u2019t want to use as features. You can set the parameter stop_words=\u2019english\u2019 to use a built-in list. Alternatively you can set stop_words equal to some custom list. This parameter defaults to None.\n\n2. **ngram_range**: An n-gram is just a string of n words in a row. E.g. the sentence \u2018I am Groot\u2019 contains the 2-grams \u2018I am\u2019 and \u2018am Groot\u2019. The sentence is itself a 3-gram. Set the parameter ngram_range=(a,b) where a is the minimum and b is the maximum size of ngrams you want to include in your features. The default ngram_range is (1,1). In a recent project where I modeled job postings online, I found that including 2-grams as features boosted my model\u2019s predictive power significantly. This makes intuitive sense; many job titles such as \u2018data scientist\u2019, \u2018data engineer\u2019, and \u2018data analyst\u2019 are 2 words long.\n\n3. **min_df, max_df**: These are the minimum and maximum document frequencies words\/n-grams must have to be used as features. If either of these parameters are set to integers, they will be used as bounds on the number of documents each feature must be in to be considered as a feature. If either is set to a float, that number will be interpreted as a frequency rather than a numerical limit. min_df defaults to 1 (int) and max_df defaults to 1.0 (float).\n\n4. **max_features**: This parameter is pretty self-explanatory. The CountVectorizer will choose the words\/features that occur most frequently to be in its\u2019 vocabulary and drop everything else. \n\nYou would set these parameters when initializing your CountVectorizer object as shown below.","7ed363e9":"<a id='9.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">9.3 WordCloud<\/p>","899801cf":"<a id='9'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">9. NLP: Disaster Tweets<\/p>","b560d172":"We need to perform **tokenization** - the processing of segmenting text into sentences of words. In the process we throw away punctuation and extra symbols too. The benefit of tokenization is that it gets the text into a format that is easier to convert to raw numbers, which can actually be used for processing","5b651c04":"<a id='6.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">6.1 Naive Bayes DTM<\/p>\n\nIn statistics, naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (na\u00efve) independence assumptions between the features. They are among the simplest Bayesian network models, but coupled with kernel density estimation, they can achieve higher accuracy levels.\n\nNa\u00efve Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features\/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression, which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.\n\n![naive-bayes.jpg](attachment:naive-bayes.jpg)","4267cf0d":"<a id='5'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">5. Vectorization<\/p>\n\nCurrently, we have the messages as lists of tokens (also known as lemmas) and now we need to convert each of those messages into a vector the SciKit Learn's algorithm models can work with.\n\nWe'll do that in three steps using the bag-of-words model:\n\n1. Count how many times does a word occur in each message (Known as term frequency)\n2. Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n\nLet's begin the first step:\n\n","d60fbf7d":"<a id='9.4'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">9.4 Modeling<\/p>","904b6fbe":"### GloVe\n\nGloVe method is built on an important idea,\n\n> You can derive semantic relationships between words from the co-occurrence matrix.\n\nTo obtain a vector representation for words we can use an unsupervised learning algorithm called **GloVe (Global Vectors for Word Representation)**, which focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together.\n\nWord embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. They have learned representations of text in an n-dimensional space where words that have the same meaning have a similar representation. Meaning that two similar words are represented by almost similar vectors that are very closely placed in a vector space.\n\nThus when using word embeddings, all individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network.","0d0e0124":"<a id='8'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">8. BERT<\/p>\n\nBERT (Bidirectional Encoder Representations from Transformers) is a recent paper published by researchers at Google AI Language. It has caused a stir in the Machine Learning community by presenting state-of-the-art results in a wide variety of NLP tasks, including Question Answering (SQuAD v1.1), Natural Language Inference (MNLI), and others.\n\n\nBERT\u2019s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training. The paper\u2019s results show that a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models. In the paper, the researchers detail a novel technique named Masked LM (MLM) which allows bidirectional training in models in which it was previously impossible.\n\nRef: https:\/\/towardsdatascience.com\/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270","62b950e1":"As we can see, the classes are imbalanced, so we can consider using some kind of resampling. We will study later. Anyway, it doesn't seem to be necessary.","69fd5a09":"**Natural Language Processing or NLP** is a branch of Artificial Intelligence which deal with bridging the machines understanding humans in their Natural Language. Natural Language can be in form of text or sound, which are used for humans to communicate each other. NLP can enable humans to communicate to machines in a natural way.\n\n**Text Classification** is a process involved in Sentiment Analysis. It is classification of peoples opinion or expressions into different sentiments. Sentiments include Positive, Neutral, and Negative, Review Ratings and Happy, Sad. Sentiment Analysis can be done on different consumer centered industries to analyse people's opinion on a particular product or subject.\n\nNatural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, a task that involves the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence.\n\n![Natural-Language-Processing.png](attachment:Natural-Language-Processing.png)\n\nIn this kernel we are going to focus on text classification and sentiment analysis part. In the next lessons we will study Information retrival, Question answering, etc","ad5b81b1":"### Pad_sequences","3c80bc78":"<a id='5.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">5.3 Word Embeddings: GloVe<\/p>\n\nThanks to: https:\/\/www.kaggle.com\/mariapushkareva\/nlp-disaster-tweets-with-glove-and-lstm","f30f65d4":"<a id='6.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">6.2 Naive Bayes<\/p>","a5daea74":"### Stopwords\nStopwords are commonly used words in English which have no contextual meaning in an sentence. So therefore we remove them before classification. Some examples removing stopwords are:\n\n![stopwords.png](attachment:stopwords.png)","499143eb":"<a id='9.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">9.1 EDA<\/p>","8da949f4":"### Lets see the results","46813dd0":"<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Data Pre-processing \ud83d\udee0<\/p>\n\nNow we are going to engineering the data to make it easier for the model to clasiffy.\n\nThis section is very important to reduce the dimensions of the problem.","9b9b2bcd":"<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. EDA \ud83d\udcca<\/p>\n\nNow we are going to take a look about the target distribution and the messages length.","092abd40":"<a id='3.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">3.1 Cleaning the corpus \ud83d\udee0<\/p>","a6e27a90":"<a id='5.2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 15px 50px;\">5.2 TF-IDF<\/p>\n\nIn information retrieval, tf\u2013idf, **TF-IDF**, or TFIDF, **short for term frequency\u2013inverse document frequency**, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf\u2013idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. \n\n**tf\u2013idf** is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf\u2013idf.\n\n![tdidf2.png](attachment:tdidf2.png)","a886b858":"<a id='3.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">3.3 All together \ud83d\udee0<\/p>","26157fa0":"We need to perform **tokenization** - the processing of segmenting text into sentences of words. The benefit of tokenization is that it gets the text into a format that is easier to convert to raw numbers, which can actually be used for processing.\n\n![tokenization.jpg](attachment:tokenization.jpg)","645eb2d7":"<a id='4'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">4. Tokens visualization \ud83d\udcca<\/p>","de14a9bd":"### Model LSTM","864a230a":"### GloVe\n\nTo obtain a vector representation for words we can use an unsupervised learning algorithm called **GloVe (Global Vectors for Word Representation)**, which focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together.","f8064dc2":"**Balanced Dataset:** \u2014 Let\u2019s take a simple example if in our data set we have positive values which are approximately same as negative values. Then we can say our dataset in balance.\n\n![balanced-dataset.png](attachment:balanced-dataset.png)\n\nConsider Orange color as a positive values and Blue color as a Negative value. We can say that the number of positive values and negative values in approximately same.\n\n**Imbalanced Dataset:** \u2014 If there is the very high different between the positive values and negative values. Then we can say our dataset in Imbalance Dataset.\n\n![imbalanced-dataset.png](attachment:imbalanced-dataset.png)\n\n","e95c6041":"Each vector will have as many dimensions as there are unique words in the SMS corpus. We will first use SciKit Learn's **CountVectorizer**. This model will convert a collection of text documents to a matrix of token counts.\n\nWe can imagine this as a 2-Dimensional matrix. Where the 1-dimension is the entire vocabulary (1 row per word) and the other dimension are the actual documents, in this case a column per text message.\n\n![vectorization.png](attachment:vectorization.png)","836a6314":"<a id='3.4'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">3.4 Target encoding \ud83d\udee0<\/p>"}}