{"cell_type":{"d90e6c78":"code","23ef68ed":"code","1c77f24c":"code","3cc34f05":"code","24187acb":"code","1e0f28ce":"code","cbfa2d08":"code","dd82d8b4":"code","96a199bf":"code","dba80f9c":"code","fcf98a6d":"code","cb6d7e4a":"code","07750c16":"code","45a09cbf":"code","60ef4bc6":"code","1ff9af6a":"code","f85b168c":"code","b069394a":"code","a690b683":"code","1b528e77":"code","b4218d4b":"code","449dbaaf":"code","afbc8736":"code","7e98dd7f":"code","c1efd44e":"code","09c2df17":"code","c0738df2":"code","985593ca":"code","9617ac07":"code","84d0b624":"code","1dc2e2ea":"code","b9572fa7":"code","b82979fc":"code","4e0a82b0":"code","9953e9e2":"code","8bc2b0c8":"code","d097e774":"code","30066a52":"code","7285747f":"code","b11a79db":"code","e1838ecf":"code","b5f90aa6":"code","34c0fcd7":"code","47b7e8f6":"code","9ce08964":"code","957627ad":"code","4f4ed8d9":"code","63e85af4":"code","9d4f7a4c":"code","12ec412e":"code","ca917a06":"code","2e0e8d3e":"code","80c48eab":"code","9c739859":"code","96ba271f":"code","b2f7c3e5":"code","e7fb5311":"code","941a7fe4":"code","5932bfa5":"code","bb813523":"code","630a0716":"code","55c2bfc3":"code","ade92ba8":"code","0994a946":"code","8c921fcc":"code","6b1f1233":"code","b32ea6e6":"code","c0221848":"code","71c3641d":"markdown","b97de0a7":"markdown","e65a8efe":"markdown","e56afbdf":"markdown","f0e9928a":"markdown","65892edb":"markdown","0535de64":"markdown","55d00df0":"markdown","610f9f03":"markdown","1518e14a":"markdown","013cc231":"markdown","950e2260":"markdown","ec15fad3":"markdown","6cf7d0d4":"markdown","8c551232":"markdown","acd436c3":"markdown","b41f2bac":"markdown","223ea125":"markdown","4d5e90f5":"markdown","9c0e9bae":"markdown","cfdd3074":"markdown","78c34c89":"markdown","aa9dbd0a":"markdown","d1e92536":"markdown","5570e4d1":"markdown","e5c628ac":"markdown","6847aebd":"markdown","56e3f689":"markdown","aac84e38":"markdown","5450a824":"markdown","c6a49727":"markdown","fadec208":"markdown","9219ca2d":"markdown","b806d88f":"markdown","afe6d799":"markdown","41727522":"markdown","0993b326":"markdown","848c872f":"markdown","fc591ebb":"markdown","caa351cf":"markdown","26e825ca":"markdown","ca43cad4":"markdown","821aa6e1":"markdown","57c8c120":"markdown","d9ab5708":"markdown","baea3776":"markdown","b0637bf0":"markdown","f11ab510":"markdown","9760afa7":"markdown","a1283735":"markdown","6749f935":"markdown","9ffe59ae":"markdown","21d1108c":"markdown","243b02ae":"markdown","19343bf2":"markdown","d0876e59":"markdown","cf2bf97b":"markdown","f5c49aec":"markdown","5148e6bc":"markdown","69c27e43":"markdown","84e44768":"markdown","a2df7afc":"markdown","7cfa4b12":"markdown","abfe89b7":"markdown","d5485e18":"markdown","35a36d52":"markdown","bed90ff0":"markdown","21a26e4f":"markdown","52918b2a":"markdown","c3ccda50":"markdown","903fe6f9":"markdown","32551d30":"markdown","d8014dc3":"markdown","8cbf526d":"markdown","dcb0ff73":"markdown","7899d2a5":"markdown","3934585a":"markdown","59ddf80e":"markdown","f853c379":"markdown","1f268cae":"markdown","277a808e":"markdown","6b3efb9e":"markdown","e5f1a390":"markdown","cac3fafe":"markdown","a4e2735e":"markdown","eab61610":"markdown","9d6f19c1":"markdown","db4fe442":"markdown","c5af9f28":"markdown"},"source":{"d90e6c78":"import numpy as np\nimport pandas as pd\nimport scipy.stats as st\npd.set_option('display.max_columns', None)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nimport missingno as msno\n\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","23ef68ed":"train_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","1c77f24c":"train_data.head()","3cc34f05":"train_data.shape","24187acb":"train_data = train_data.drop(columns=['Id'])","1e0f28ce":"total_rows = train_data.shape[0]\ntotal_columns = train_data.shape[1]\nprint('Total rows', total_rows)\nprint('Total columns', total_columns)","cbfa2d08":"all_columns = train_data.columns\nall_columns","dd82d8b4":"train_data['SalePrice'].sort_values(ascending=False).head()","96a199bf":"train_data['SalePrice'].sort_values(ascending=True).head()","dba80f9c":"len(train_data['SalePrice'].unique())","fcf98a6d":"train_data['SalePrice'].value_counts().head(20)","cb6d7e4a":"train_data.info()","07750c16":"train_data.describe()","45a09cbf":"train_data.describe(include=['object', 'bool'])","60ef4bc6":"print(train_data['SalePrice'].mean())\nprint('-' * 20)\nprint(train_data['SalePrice'].median())\nprint('-' * 20)\nprint('Difference between mean and median', train_data['SalePrice'].mean() - train_data['SalePrice'].median())","1ff9af6a":"train_data.skew()","f85b168c":"train_data.kurt()","b069394a":"plt.figure(figsize=(12, 7))\nsns.distplot(train_data.skew(),color='green',axlabel ='Skewness')\nplt.show()","a690b683":"plt.figure(figsize=(12, 7))\nsns.distplot(train_data.kurt(),color='orange',axlabel ='Kurtosis',norm_hist= False, kde = True,rug = False)\nplt.show()","1b528e77":"total = train_data.isnull().sum().sort_values(ascending=False)\npercent = ((train_data.isnull().sum() \/ total_rows) * 100).sort_values(ascending=False)\nnull_data = pd.concat([total, percent], axis=1,join='outer', keys=['Null count', 'Percentage %'])\nnull_data.index.name ='Columns'\nnull_data = null_data[null_data['Null count'] > 0].reset_index()\nnull_data","b4218d4b":"null_columns = null_data['Columns']\nnull_columns","449dbaaf":"null_rows = train_data.isnull().sum(axis=1).sort_values(ascending=False).head(20)\n# null_rows = null_rows.head(20)\nnull_rows","afbc8736":"null_rows.index","7e98dd7f":"((null_rows *100) \/ 80)","c1efd44e":"train_data.loc[null_rows.index].head()","09c2df17":"# msno.bar.__code__.co_varnames\nmsno.bar(train_data.sample(1460), labels=True, fontsize=12)","c0738df2":"msno.heatmap(train_data)","985593ca":"msno.dendrogram(train_data)","9617ac07":"copy_data = train_data.copy()","84d0b624":"for column in null_columns:\n    copy_data[column] = np.where(train_data[column].isnull(), 1, 0)\n    \n    plot_data = copy_data.groupby(by=[column])['SalePrice'].median()\n    plot_data = pd.DataFrame(plot_data)\n    plot_data = plot_data.reset_index()\n    sns.barplot(x=plot_data[column], y=plot_data['SalePrice'], data=plot_data, palette=\"Blues_d\")\n    \n    plt.xticks(plot_data[column], ('value(0)', 'Null(1)'))\n    plt.xlabel(column)\n    plt.ylabel('SalePrice')\n    plt.show()","1dc2e2ea":"numerical_data = train_data.select_dtypes(include=[np.number])\nnumerical_columns = numerical_data.columns\nnumerical_columns","b9572fa7":"discrete_column = []\ncontinious_column = []\nyear_column = []","b82979fc":"for column in numerical_columns:\n    if 'Year' in column or 'Yr' in column:\n#         print(column)\n        year_column.append(column)","4e0a82b0":"for column in numerical_columns:\n    if column != 'SalePrice' and column not in year_column:\n        if len(train_data[column].unique()) < 25:\n            discrete_column.append(column)\n        else:\n            continious_column.append(column)","9953e9e2":"discrete_column","8bc2b0c8":"continious_column","d097e774":"categorical_data = train_data.select_dtypes(include=[np.object])\ncategorical_columns = categorical_data.columns\ncategorical_columns","30066a52":"year_column = []","7285747f":"for column in numerical_columns:\n    if 'Year' in column or 'Yr' in column:\n#         print(column)\n        year_column.append(column)","b11a79db":"train_data[year_column]","e1838ecf":"for column in year_column:\n    plt.figure(figsize=(10, 7))\n    train_data.groupby(by=[column])['SalePrice'].median().plot(color = ['c', 'y'])\n    plt.show()","b5f90aa6":"year_data = train_data.copy()","34c0fcd7":"\nfor column in year_column:\n    if column != 'YrSold':\n        plt.figure(figsize=(10, 7))\n        year_data[column] = year_data['YrSold']-year_data[column]\n        \n        sns.scatterplot(x=year_data[column], y=year_data['SalePrice'], data=year_data)\n        plt.xlabel(column)\n        plt.ylabel('SalePrice')\n        plt.show()","47b7e8f6":"for column in continious_column:\n    plt.figure(figsize=(10, 7))\n    train_data[column].plot.hist(color = \"skyblue\")\n    plt.xlabel(column)\n    plt.show()","9ce08964":"copy_data = train_data.copy()","957627ad":"for column in continious_column:\n    if 0 in copy_data[column].unique():\n        pass\n    else:\n#         print(column)\n        plt.figure(figsize=(10, 7))\n        con_data = np.log(copy_data[column])\n        con_data.plot.hist(color = \"skyblue\")\n        plt.xlabel(column)\n        plt.show()","4f4ed8d9":"for column in discrete_column:\n    plt.figure(figsize=(10, 7))\n    train_data.groupby(by=[column])['SalePrice'].median().plot.bar(color = \"skyblue\")\n    \n    plt.show()","63e85af4":"for column in continious_column:\n    if 0 in train_data[column].unique():\n        pass\n    else:\n        plt.figure(figsize=(10, 7))\n        sns.scatterplot(x=train_data[column], y=train_data['SalePrice'], data=train_data)\n        \n        plt.xlabel(column)\n        plt.ylabel('SalePrice')\n        plt.show()","9d4f7a4c":"copy_data = train_data.copy()","12ec412e":"copy_data['SalePrice'] = np.log(copy_data['SalePrice'])\nfor column in continious_column:\n    if 0 in copy_data[column].unique():\n        pass\n    else:\n        plt.figure(figsize=(10, 7))\n        copy_data[column] = np.log(copy_data[column])\n        sns.scatterplot(x=copy_data[column], y=copy_data['SalePrice'], data=copy_data)\n        \n        plt.xlabel(column)\n        plt.ylabel('SalePrice')\n        plt.show()","ca917a06":"for column in categorical_columns:\n    plt.figure(figsize=(10, 7))\n    copy_data.groupby(by=column)['SalePrice'].median().plot.bar(color = \"skyblue\")\n    \n    plt.xlabel(column)\n    plt.ylabel('SalePrice')\n    plt.show()","2e0e8d3e":"var = 'OverallQual'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","80c48eab":"feature_corr = numerical_data.corr()\nfeature_corr['SalePrice'].sort_values(ascending=False)","9c739859":"plt.figure(figsize=(15, 12))\nsns.heatmap(feature_corr,square = True,  vmax=0.8)","96ba271f":"k = 11\ncols = feature_corr.nlargest(k, 'SalePrice')['SalePrice'].index\ncols\ncm = np.corrcoef(train_data[cols].values.T) # transformed data\ncm\nf , ax = plt.subplots(figsize = (14,12))\nsns.heatmap(cm, vmax=.8, linewidths=0.01,square=True,annot=True, linecolor=\"white\",xticklabels = cols.values ,annot_kws = {'size':12},yticklabels = cols.values)\n","b2f7c3e5":"columns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.pairplot(train_data[columns],size = 2 ,kind ='scatter',diag_kind='kde')\nplt.show()","e7fb5311":"#standardizing data\nsaleprice_scaled = StandardScaler().fit_transform(train_data['SalePrice'][:,np.newaxis]);\n# saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10] # specifies that you want to slice out a 1D vector of length 97 from a 2D array.\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","941a7fe4":"# copy_data[column] = np.log(copy_data[column])\nplt.figure(figsize=(10, 7))\ntrain_data.boxplot(column='SalePrice', notch=True, vert=False)\n\nplt.show()","5932bfa5":"copy_data = train_data.copy()","bb813523":"copy_data['SalePrice'] = np.log(copy_data['SalePrice'])\nplt.figure(figsize=(10, 7))\ncopy_data.boxplot(column='SalePrice', notch=True, vert=False)\n\nplt.show()","630a0716":"for column in continious_column:\n    if 0 in copy_data[column].unique():\n        pass\n    else:\n        plt.figure(figsize=(10, 7))\n        copy_data[column] = np.log(copy_data[column])\n        copy_data.boxplot(column=column , notch=True, vert=False)\n        \n        plt.show()","55c2bfc3":"y = train_data['SalePrice']\n\nplt.figure(figsize=(10, 7))\nplt.figure(1); plt.title('Johnson SU')\nsns.distplot(y, kde=True, fit=st.johnsonsu, color='#636efa')\n\nplt.figure(figsize=(10, 7))\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=True, fit=st.norm, color='#636efa')\n\nplt.figure(figsize=(10, 7))\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=True, fit=st.lognorm, color='#636efa')","ade92ba8":"plt.figure(figsize=(10, 7))\nres = stats.probplot(train_data['SalePrice'], plot=plt)","0994a946":"copy_data = train_data.copy()","8c921fcc":"#applying log transformation\nplt.figure(figsize=(10, 7))\ncopy_data['SalePrice'] = np.log(copy_data['SalePrice'])\nres = stats.probplot(copy_data['SalePrice'], plot=plt)","6b1f1233":"print(\"Skewness:\", train_data['SalePrice'].skew())\nprint('-' * 30)\nprint(\"Kurtosis:\", train_data['SalePrice'].kurt())","b32ea6e6":"# plt.figure(figsize=(5, 3))\nplt.figure(figsize=(10, 7))\nplt.hist(train_data['SalePrice'],orientation = 'vertical',histtype = 'bar', color ='#21bf73')\nplt.show()","c0221848":"# plt.figure(figsize=(5, 3))\nplt.figure(figsize=(10, 7))\ntarget = np.log(train_data['SalePrice'])\ntarget.skew()\nplt.hist(target,color='#21bf73')","71c3641d":"It is apparent that SalePrice doesn't follow normal distribution, so before performing regression it has to be transformed. While log transformation does pretty good job, best fit is unbounded Johnson distribution.","b97de0a7":"Just load the train dataset. We have no plans for a test dataset here.","e65a8efe":"Its looks much better now, after log well distributed.","e56afbdf":"its told us that **OverallQual** have good relation with sale price, when quality increases sale price exponentially increases. and also have some good relation in FullBath, TotRMSAdvGrd, GarageCars with sale price as well","f0e9928a":"**This plot list is little bit longer if u want to see those please click \"Show\" button****","65892edb":"# 1. Basic Analysis with Pandas","0535de64":"* A normal distribution is called mesokurtic and has kurtosis of or around zero\n* Mesokurtic [normal] k = 0\n* A platykurtic distribution has negative kurtosis and tails are very thin compared to the normal distribution.\n* platykurtic [negative] k < 0\n* Leptokurtic distributions have kurtosis greater than 3 and the fat tails mean that the distribution produces more extreme values and that it has a relatively small standard deviation\n* Leptokurtic [positive] k > 0\n\n","55d00df0":"* Here you can notice the maximum case mean value is greater than the median value of each column which is represented by 50%(50th percentile) in the index column.\n* Some are weird also like \"MasVnrArea\" mean is 103.685262 but median is 0.00000\n* There is notably a large difference between 75th %tile and max values of predictors \u201cMSSubClass\u201d,\u201dLotArea\u201d,\u201dMasVnrArea\u201d so on.\n* Thus observations suggests that there are extreme values-Outliers in our data set.\n* One important thing is that in salePrice our minimum price is larger than zero. We don't have one of those personal traits that would destroy our model.\n* and other things is that all most all int columns have value","610f9f03":"## Import necessary libraries and files","1518e14a":"heatmap is a good way to understand correlation. but actually here we see nothing. for that we can select first 11 strongly correlated columns to see heatmap correlation","013cc231":"its little bit hard to find out discrete and continuous columns when u don't know much about your all data. that's why i do this kind of things","950e2260":"we can shown them into pai plot also to see correlation","ec15fad3":"Outliers is one of the most important task in EDA. Outliers have much influence in model. Here we get more important insights about our features and also can find out which point of data are not following other observation. While doing the EDA a quick visual way to check for the outliers for continuous data is via scatterplots and boxplots.","6cf7d0d4":"## 5.2 Bivariate analysis of Categorical Columns","8c551232":"To starts with,I import necessary libraries (which i done in above cell) and loaded the data set with pandas \"read_csv\" method.","acd436c3":"# 3. Features Classification","b41f2bac":"now we can see have some distribution","223ea125":"* 'GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice'. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of 'TotalBsmtSF', we can see that the slope of the linear relationship is particularly high.\n* 'OverallQual' and 'YearBuilt' also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual', where the box plot shows how sales prices increase with the overall quality.\n* its told us that '**OverallQual**', '**GrLivArea**' and '**TotalBsmtSF**' are strongly correlated with '**SalePrice**'.\n* '**GarageCars**' and '**GarageArea**' are strongly correlated.\n* '**TotRmsAdvGrd**' and '**GrLivArea**' are strongly correlated.\n* '**istFlrSF**' and '**TotalBsmtSF**' are strongly correlated. \n","4d5e90f5":"## 4.1 Analyze Date Time Columns","9c0e9bae":"With help of \u201c .head()\u201d method of pandas library which returns first five observations of the data set.Similarly \u201c.tail()\u201d returns last five observations of the data set.","cfdd3074":"<p>Few key insights just by looking at dependent variable are as follows:<\/p>\n<p>Now we want to see top 5 and lowest 5 saleprices. basically its has no direct benefit but we want to see those to know whats are top and lowest prices on this place.<\/p>","78c34c89":"top 5","aa9dbd0a":"# 8. Central tendency and distribution of target columns","d1e92536":"# Goals of EDA\n<ol>\n    <li>Find out patterns<\/li>\n    <li>Find out Relationship among variables<\/li>\n    <li>Find out Anomalies<\/li>\n    <li>Check Assumptions<\/li>\n    <li>Frame Hypothesis<\/li>\n<\/ol>","5570e4d1":"* Low range values are similar and not too far from 0.\n* High range values are far from 0 and the 7.something values are really out of range.\n\nFor now, we'll not consider any of these values as an outlier but we should be careful with those two 7.something values.\n","e5c628ac":"we will convert or map our null value into 1 and have value into 0","6847aebd":"<p>here we can see our all null columns and their null amount and percentage. top 5 null columns are <\/p>\n<ul>\n<li> PoolQC <\/li>\n<li> MiscFeature <\/li>\n<li> Alley <\/li>\n<li> Fence <\/li>\n<li> FireplaceQu <\/li>\n<\/ul>","56e3f689":"* there lots of columns have null value\n* we can see the unique row. from this we can get some basic idea about categorical columns ","aac84e38":"# Steps of EDA:\n<ol>\n    <li>Basic Analysis with Pandas\n        <ol>\n            <li> Descriptive analysis <\/li>\n        <\/ol>\n    <\/li>\n    <li>Missing Values<\/li>\n    <li>Features Classification\n        <ol>\n            <li>Numerical Features<\/li>\n            <li>Categorical Features<\/li>\n        <\/ol>\n    <\/li>\n    <li>Univariate Analysis\n        <ol>\n            <li>Analyze Date Time Columns<\/li>\n            <li>Univariate Analysis of Numerical Features<\/li>\n        <\/ol>\n    <\/li>\n    <li>Bivariate analysis\n        <ol>\n            <li>Bivariate analysis of Numerical Features<\/li>\n            <li>Bivariate analysis of Categorical Features<\/li>\n        <\/ol>\n     <\/li>\n    <li>Correlation coefficients<\/li>\n    <li>Outliers<\/li>\n    <li>Central tendency and distribution of target columns<\/li>\n<\/ol>","5450a824":"Estimate Skewness and Kurtosis","c6a49727":"Missing value of each rows","fadec208":"missingno is a awesome library for showing missing value. with the help of dendrogram, heatmap and bar chat you can plot missing value very well.","9219ca2d":"Here we see our all columns name with the respect of our target column \"SalePrice\". its help us to operate some loop and copy paste columns name \ud83d\ude01","b806d88f":"The describe() function in pandas is very handy in getting various summary statistics.This function returns the count, mean, standard deviation, minimum and maximum values and the quantiles of the data.","afe6d799":"At first i want to drop \"Id\" column. which is unnecessary for me now","41727522":"* npw its looks like something better. and shown have some relation with salePrice\n* It seems that 'SalePrice' and 'GrLivArea' are really old friends, with a linear relationship.\n* Moreover, it's clear that sometimes 'TotalBsmtSF' closes in itself and gives zero credit to 'SalePrice'.","0993b326":"* In normal \n* Deviate from the normal distribution.\n* Have appreciable positive skewness.\n* Show peakedness.","848c872f":"The two values with bigger 'GrLivArea' and 'LotFrontage' seem strange and they are not following the crowd. We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price. I'm not sure about this but I'm quite confident that these two points are not representative of the typical case. Therefore, we'll define them as outliers and delete them.","fc591ebb":"we can also say it **Bivariate visualization**. Bivariate visualization\u200a\u2014\u200ais performed to find the relationship between each variable in the dataset and the target variable of interest","caa351cf":"it's shown us that some are categorical features have some correlation with 'salePrice'. but most of them are not correlated with 'saleprice'.","26e825ca":"**This plot list is little bit longer if u want to see those please click \"Show\" button****","ca43cad4":"<h3>Acknowledgement:<\/h3>\n<ol>\n    <li><a href=\"https:\/\/towardsdatascience.com\/intro-to-descriptive-statistics-252e9c464ac9\">https:\/\/towardsdatascience.com\/intro-to-descriptive-statistics-252e9c464ac9<\/a><\/li>\n    <li><a href=\"https:\/\/towardsdatascience.com\/exploratory-data-analysis-8fc1cb20fd15\">https:\/\/towardsdatascience.com\/exploratory-data-analysis-8fc1cb20fd15<\/a><\/li>\n    <li><a href=\"https:\/\/www.youtube.com\/watch?v=FLuqwQgSBDw&list=PLupD_xFct8mFDeCqoUAWZpUddeqmT28_L\">https:\/\/www.youtube.com\/watch?v=FLuqwQgSBDw&list=PLupD_xFct8mFDeCqoUAWZpUddeqmT28_L<\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\">https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python<\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/pavansanagapati\/a-simple-tutorial-on-exploratory-data-analysis\">https:\/\/www.kaggle.com\/pavansanagapati\/a-simple-tutorial-on-exploratory-data-analysis<\/a><\/li>\n    <li><a href=\"https:\/\/www.youtube.com\/watch?v=ioN1jcWxbv8&list=PLZoTAELRMXVMcRQwR5_J8k9S7cffVFq_U\">https:\/\/www.youtube.com\/watch?v=ioN1jcWxbv8&list=PLZoTAELRMXVMcRQwR5_J8k9S7cffVFq_U<\/a><\/li>\n<\/ol>","821aa6e1":"now we can do some plotting with those null value","57c8c120":"now we can see its looks ok coz when a house is 140 years old that has less price and we see it in other features also","d9ab5708":"the value zero doesn't allow us to do log transformations.","baea3776":"* So our Dataset contains 1460 rows and 81 columns ","b0637bf0":"for total number of rows and columns we use pandas \".shape()\" method.","f11ab510":"* Our target column or dependent column is continuous numerical in nature\n* top 20 repeated or most saling, \"salePrice\". if u have 130000 to 160000 u have most chance to buy a house","9760afa7":"Observation:\n\n* Alley, PoolQC, Fence, MiscFeature features contain a number of null values in that order for the training dataset. and all are categorical columns\n\n* 38 features are of float and int type and 43 features are type of object\n\n","a1283735":"<h2>Note: If you think it's helpful please <i style=\"color: red;\">upvoted<\/i>. And inspire me to do more. Thank you<\/h2>","6749f935":"## 3.1. Numerical Features","9ffe59ae":"We can use the info() method to output some general information about the dataframe:","21d1108c":"## 4.2 Univariate Analysis of Numerical Columns","243b02ae":"---\n\n<h3 style=\"text-align: center;font-size: 20px;\">In data science process, you can see where is EDA.<\/h3>\n\n---\n\n<center><img style=\"width: 700px;\" src=\"https:\/\/blog.camelot-group.com\/wp-content\/uploads\/2019\/03\/Picture2.png\"><\/center>\n\n---\n<i>image from Google<\/i>","19343bf2":"we see that we have lots of null values. we want to know that how much effect they have in our target column \"salePrice\"","d0876e59":"lowesst 5","cf2bf97b":"they shown us of top 20 missing values rows and their percentages","f5c49aec":"now i have 1460 rows and 80 columns","5148e6bc":"Univariate visualization\u200a\u2014\u200aprovides summary statistics for each field in the raw data set","69c27e43":"Conclusion:\nThat's all. I think we do some good exploratory data analysis here.","84e44768":"though 'OverallQual' is not numeric feature but we keep it here to see some relation and plotting","a2df7afc":"first we trying to find out date time variable and then trying to plotting them and analyzing them","7cfa4b12":"**Normal probability plot - Data distribution should closely follow the diagonal that represents the normal distribution.**","abfe89b7":"* A skewness value of 0 in the output denotes a symmetrical distribution.\n* Normal skewness (mean = median = mode)\n* A negative skewness value in the output indicates an asymmetry and the tail will be larger towards the left hand side of the distribution.\n* Negative skewness (mean < median < mode)\n* A positive skewness value in the output indicates an asymmetry and the tail will be larger towards the right hand side of the distribution.\n* Positive skewness (mode < median < mean)","d5485e18":"* YearBuilt YearRemodAdd GarageYrBlt sown us when year is increasing price is also increasing\n* but in YrSold we see something different we need to find out why","35a36d52":"# 6. Correlation coefficients","bed90ff0":"almost 18% of all those rows don't have values.","21a26e4f":"# 4. Univariate analysis","52918b2a":"\n'OverallQual' is a ordinal categorical type. we can use boxplot to plot it, to see correlation with 'SalePrice'","c3ccda50":"## 3.2. Categorical Features","903fe6f9":"see some null value have effect on our target column sale prices","32551d30":"Now let's try printing out column names using columns:","d8014dc3":"## 5.2 Bivariate analysis of Numerical Columns","8cbf526d":"---\n\n<h1 style=\"text-align: center;font-size: 30px;\">Exploratory Data Analysis<\/h1>\n\n---\n\n<center><img style=\"width: 700px;\" src=\"https:\/\/www.researchify.co.uk\/generator\/data.gif\"><\/center>\n\n---\n<i>image from Google<\/i>","dcb0ff73":"\n# 5. Bivariate analysis","7899d2a5":"##### For this Using logarithm transformation","3934585a":"# 7. Outliers","59ddf80e":"## 1.1 Descriptive analysis","f853c379":"# 2. Missing Values","1f268cae":"**We'll consider that when more than 20% to 30% of the data is missing, we should delete the corresponding variable and pretend it never existed. but we will do it in feature engineering part.**\n\n**Moreover, looking closer at the variables, we could say that variables like 'PoolQC', 'MiscFeature' and 'FireplaceQu' are strong candidates for outliers**","277a808e":"in code there was an a shortcut but want to use xtricks thats why i code like this. if u know shortcut way to present it let me know in comments","6b3efb9e":"the value zero doesn't allow us to do log transformations.","e5f1a390":"Missing value of each columns","cac3fafe":"**This plot list is little bit longer if u want to see those please click \"Show\" button****","a4e2735e":"# Exploratory Data Analysis (EDA)\n\n<p>There was an a quotes of Epictetus, something like that<\/p>\n<h3>Know, first, Who you are, and then adorn yourself accordingly.\n~ Epictetus\n <\/h3>\n<p>Exploratory Data Analysis (EDA) something like that, know your data first and do feature engineering accordingly. To knowing your data in data science, its called Exploratory Data Analysis in short EDA.<\/p>\n<p> So basically Exploratory Data Analysis is the examination of data and find out relationships among variables through both numerical and graphical methods.  EDA is a task of analyze data, investigate data to the way we find out patterns, relationship, outliers and distribution of data. It is one of the most important task for data scientist to do data science task. <\/p>\n<p> Doing EDA is good practice to know the data first. And find out inside information, relations as much as posible<\/p>\n<p> To starting data science task normally we start it with EDA. So solving \n<a href=\"https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\">House Prices: Advanced Regression Techniques<\/a> we starting it with here. This is <b> part-1 <\/b> of this problem.<\/p>\n<p>Other part of this solution for Feature Engineering & Prediction, You find it here.<\/p>\n<a href=\"https:\/\/www.kaggle.com\/snanilim\/feature-engineering-prediction-house-prices\">Feature Engineering & Prediction - House Prices<\/a>","eab61610":"with the help of boxplot we can see lots of outliers on those columns. we will taken care of in feature engineering section","9d6f19c1":"**This plot list is little bit longer if u want to see those please click \"Show\" button****","db4fe442":"Missing value of each rows percentage","c5af9f28":"we can see our continuous variable is not well distributed. let's do something on this"}}