{"cell_type":{"70160471":"code","c6951e7c":"code","f7e68297":"code","491cba2e":"code","b9b0fd0e":"code","0c31649f":"code","98fed4b4":"code","f2fe2b3c":"code","e9244a1a":"code","ab40c56b":"code","da23095d":"code","c938abe3":"code","012aca82":"code","17302c56":"code","99da0b65":"code","619f7ccf":"code","05ff4eb7":"code","758224c8":"code","24fb92cc":"code","852ddbc0":"code","098a9ddd":"code","b4348615":"code","eff4cf2c":"code","ec658450":"code","fd373eda":"code","c8bb9ba1":"code","643c9744":"code","9ff07790":"code","3ccb6707":"code","dab92111":"markdown","5684ab22":"markdown","92906058":"markdown","525df36a":"markdown","a90759f0":"markdown","eedd5da3":"markdown","a48d71d2":"markdown","e5adb09c":"markdown"},"source":{"70160471":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6951e7c":"!pip install modelzoo-client[transformers]","f7e68297":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom transformers import AutoTokenizer, BertModel, BertConfig\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)","491cba2e":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","b9b0fd0e":"train = pd.read_csv('..\/input\/machine-hack\/MH train.csv')\ntest = pd.read_csv('..\/input\/machine-hack\/MH test.csv')","0c31649f":"aux_cols = [col for col in train.columns if col not in ('Id', 'Review')]","98fed4b4":"sns.set()\nplt.figure(figsize=(10, 4))\ntrain[aux_cols].drop('Polarity', axis=1).sum(axis=0).plot.bar()\nplt.show()","f2fe2b3c":"sns.set()\ntrain.Polarity.hist()\nplt.show()","e9244a1a":"sns.set()\ntrain['Review'].apply(lambda x: len(str(x))).hist(bins=100)\nplt.show()","ab40c56b":"train['Review_len'] = train['Review'].apply(lambda x: len(str(x)))","da23095d":"sns.scatterplot(data = train,  x=\"Review_len\", y=\"Polarity\", hue=\"Polarity\", palette = \"hsv\", s=40, \nlegend=True)\nplt.show()","c938abe3":"train = train.drop('Review_len', axis=1)\ntrain['targets'] = train[train.columns[2:]].values.tolist()","012aca82":"test['targets'] = test[test.columns[2:]].values.tolist()","17302c56":"MAX_LEN = 500\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nEPOCHS = 20\nLEARNING_RATE = 1e-05\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')","99da0b65":"class CustomDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = df\n        self.review_text = df.Review\n        self.targets = self.data.targets\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.review_text)\n    \n    def __getitem__(self, index):\n        review_text = str(self.review_text[index])\n        review_text = ' '.join(review_text.split())\n        \n        inputs = self.tokenizer.encode_plus(\n                                        review_text,\n                                        None, \n                                        add_special_tokens=True,\n                                        max_length=self.max_len,\n                                        pad_to_max_length=True,\n                                        return_token_type_ids=True,\n                                        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)}","619f7ccf":"train_dataset=train.sample(frac=0.8, random_state= 42)\nval_dataset=train.drop(train_dataset.index).reset_index(drop=True)\ntrain_dataset = train_dataset.reset_index(drop=True)","05ff4eb7":"training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\nval_set = CustomDataset(val_dataset, tokenizer, MAX_LEN)","758224c8":"test_set = CustomDataset(test, tokenizer, MAX_LEN)","24fb92cc":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\nval_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': False,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\nval_loader = DataLoader(val_set, **val_params)","852ddbc0":"test_loader = DataLoader(test_set, **test_params)","098a9ddd":"class BERTClass(torch.nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        self.l1 = BertModel.from_pretrained('bert-base-uncased')\n        self.l2 = torch.nn.Dropout(0.3)\n        self.l3 = torch.nn.Linear(768, 256)\n        self.l4 = torch.nn.Linear(256, 12)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids,return_dict=False)\n        output_2 = self.l2(output_1)\n        output_3 = self.l3(output_2)\n        output = self.l4(output_3)\n        return output\n\nmodel = BERTClass()\nmodel.to(device)","b4348615":"def loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)","eff4cf2c":"optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)","ec658450":"for epoch in range(EPOCHS):\n    model.train()\n    for _,data in enumerate(training_loader, 0):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.float)\n\n        outputs = model(ids, mask, token_type_ids)\n\n        optimizer.zero_grad()\n        loss = loss_fn(outputs, targets)\n        if _%5000==0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    scheduler.step()","fd373eda":"def validation():\n    model.eval()\n    fin_targets=[]\n    fin_outputs=[]\n    with torch.no_grad():\n        for _, data in enumerate(val_loader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.float)\n            outputs = model(ids, mask, token_type_ids)\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","c8bb9ba1":"outputs, targets = validation()\noutputs = np.array(outputs) >= 0.5\naccuracy = metrics.accuracy_score(targets, outputs)\nf1_score_micro = metrics.f1_score(targets, outputs, average='micro')\nf1_score_macro = metrics.f1_score(targets, outputs, average='macro')\nprint(f\"Accuracy Score = {accuracy}\")\nprint(f\"F1 Score (Micro) = {f1_score_micro}\")\nprint(f\"F1 Score (Macro) = {f1_score_macro}\")","643c9744":"preds = []\nfor i, data in enumerate(test_loader, 0):\n    ids = data['ids'].to(device, dtype = torch.long)\n    mask = data['mask'].to(device, dtype = torch.long)\n    token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n    targets = data['targets'].to(device)\n    outputs = model(ids, mask, token_type_ids)\n    preds.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())","9ff07790":"prediction = pd.DataFrame(preds)","3ccb6707":"prediction.to_csv('final.csv')","dab92111":"# Import Required Libraries and Data","5684ab22":"# Inference","92906058":"# Train Model","525df36a":"\ud83d\udcccThe data set is imbalanced w.r.t Polarity.\n\n\ud83d\udcccMost the reviews have positive sentiment.","a90759f0":"\ud83d\udcccMost of the reviews have character length of 500.\n\n\ud83d\udcccThe max character length is 1893.","eedd5da3":"# Exploratory Analysis","a48d71d2":"\ud83d\udcccMost of the reviews are about functionality.\n\n\ud83d\udcccAnd second highest is the Quality of the product.\n\n\ud83d\udcccThere are less number of reviews about the material.","e5adb09c":"\ud83d\udcccThe review lengths are uniform irrespective of the polarity."}}