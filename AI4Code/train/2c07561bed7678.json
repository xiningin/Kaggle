{"cell_type":{"54bbfdaa":"code","67c6affc":"code","d3646c18":"code","ef1ab0fc":"code","a1ec4522":"code","6c2b7d08":"code","66e9bb19":"code","4f71a273":"code","d65079d7":"code","e6ab99c1":"code","cf0c82de":"code","05adab65":"code","57bd576e":"code","30b8a576":"code","50bebf20":"code","8d6755ec":"code","8a744731":"code","66c6a23f":"code","57cc0408":"code","0e98f55f":"code","963007c0":"code","effd949e":"code","3b1a3acd":"code","a9631460":"code","cc6939b9":"code","befbc5d9":"markdown","a1acf6d7":"markdown","d1111813":"markdown","62ade33e":"markdown","dd654c9c":"markdown","73ea3168":"markdown","24fb976a":"markdown","6cf46e5a":"markdown","9b99fd75":"markdown","fa64b226":"markdown","299721bb":"markdown","464f2bf6":"markdown","aedec2f3":"markdown","a292f9bc":"markdown","755107aa":"markdown","b4469be6":"markdown","fdecef9f":"markdown","07b10361":"markdown","7ad1ae8e":"markdown","d5ac4a7f":"markdown","5b2170a9":"markdown","eccb992a":"markdown","ed4085bb":"markdown","d14c0f7b":"markdown","1a765b7b":"markdown","459ee0cb":"markdown","c0be1bc4":"markdown","9023dd10":"markdown","2deaf8e8":"markdown","bc87e8a6":"markdown","f51ba651":"markdown","abffe98f":"markdown","04a95212":"markdown"},"source":{"54bbfdaa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","67c6affc":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression\n \nfrom sklearn.model_selection import train_test_split\n \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","d3646c18":"df = pd.read_csv('\/kaggle\/input\/diamonds\/diamonds.csv')\ndf.head(5)","ef1ab0fc":"df.columns","a1ec4522":"df = df[['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z', 'price']]\ndf","6c2b7d08":"# sns.pairplot(df, hue='cut', plot_kws={'s':1})","66e9bb19":"df.describe()","4f71a273":"df.color = df.color.replace(to_replace = ['J', 'I', 'H', 'G', 'F', 'E', 'D'], value=[0, 1, 2, 3, 4, 5, 6])\ndf.cut = df.cut.replace(to_replace = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'], value=[0, 1, 2, 3, 4])\ndf.clarity = df.clarity.replace(to_replace = ['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF', 'FL'], value=[0, 1, 2, 3, 4, 5, 6, 7, 8])\ndf\n","d65079d7":"df = df.drop(df[(df.x == 0) | (df.y == 0) | (df.z == 0)].index)\ndf.describe()","e6ab99c1":"df.corr().price","cf0c82de":"sns.heatmap( df.corr(), annot = True ) ","05adab65":"model = LinearRegression()\nX = df[['x', 'y', 'z', 'carat', 'cut', 'color', 'clarity', 'depth', 'table']]\nY = df.price\ntrain_X, test_X, train_Y, test_Y, = train_test_split(X, Y, test_size = 0.5)\nmodel.fit(train_X, train_Y)\npredictions = model.predict( test_X )\n\nmean_absolute_error(test_Y, predictions)","57bd576e":"model.coef_","30b8a576":"x_pred = 5.7 * -842.778\ny_pred = 5.7 * 25.769\nz_pred = 3.5 * -50.734\ncarat_pred = 0.8 * 10729.770\ncut_pred = 2 * 124.074\ncolor_pred = 4 * 327.155\nclarity_pred = 5 * 500.737\ndepth_pred = 61.8 * -74.869\ntable_pred = 57.5 * -26.585\nprice = x_pred + y_pred + z_pred + carat_pred + cut_pred + color_pred + clarity_pred + depth_pred + table_pred\nprice + model.intercept_","50bebf20":"model = LinearRegression()\nX = df[['carat', 'cut', 'color', 'clarity']]\nY = df.price\ntrain_X, test_X, train_Y, test_Y, = train_test_split(X, Y, test_size = 0.5)\nmodel.fit(train_X, train_Y)\npredictions = model.predict( test_X )\n\nmean_absolute_error(test_Y, predictions)","8d6755ec":"model = LinearRegression()\nX = df[['carat']]\nY = df.price\ntrain_X, test_X, train_Y, test_Y, = train_test_split(X, Y, test_size = 0.5)\nmodel.fit(train_X, train_Y)\npredictions = model.predict( test_X )\n\nmean_absolute_error(test_Y, predictions)","8a744731":"model = LinearRegression()\nX = df[['x', 'y', 'z', 'carat']]\nY = df.price\ntrain_X, test_X, train_Y, test_Y, = train_test_split(X, Y, test_size = 0.5)\nmodel.fit(train_X, train_Y)\npredictions = model.predict( test_X )\n\nmean_absolute_error(test_Y, predictions)","66c6a23f":"model = LinearRegression()\nX = df[['x', 'y', 'z', 'carat', 'cut', 'color', 'clarity', 'depth', 'table']]\nY = df.price\ntrain_X, test_X, train_Y, test_Y, = train_test_split(X, Y, test_size = 0.5)\nmodel.fit(train_X, train_Y)\npredictions = model.predict( test_X )\n\nmean_absolute_error(test_Y, predictions)\n\ndf2 = df\ndf2['predictions'] = df2.x * model.coef_[0] + df2.y * model.coef_[1] + df2.z * model.coef_[2] + df2.carat * model.coef_[3] + df2.cut * model.coef_[4] + df2.color * model.coef_[5] + df2.clarity * model.coef_[6] + df2.depth * model.coef_[7] + df2.table * model.coef_[8] + model.intercept_\ndf2['abs_error'] = abs(df2.price - df2.predictions)\ndf2","57cc0408":"df2.describe()","0e98f55f":"df2.plot.scatter(x = 'price', y = 'predictions', s = 0.1, title = 'LinearRegression')","963007c0":"model = DecisionTreeRegressor()\nX = df[['x', 'y', 'z', 'carat', 'cut', 'color', 'clarity', 'depth', 'table']]\nY = df.price\ntrain_X, test_X, train_Y, test_Y, = train_test_split(X, Y, test_size = 0.5)\nmodel.fit(train_X, train_Y)\npredictions = model.predict( test_X )\n\nmean_absolute_error(test_Y, predictions)","effd949e":"df2.predictions = model.predict(df2[['x', 'y', 'z', 'carat', 'cut', 'color', 'clarity', 'depth', 'table']])\ndf2.plot.scatter(x = 'price', y = 'predictions', s = 0.1, title = 'Test')","3b1a3acd":"model = RandomForestRegressor()\nX = df[['x', 'y', 'z', 'carat', 'cut', 'color', 'clarity', 'depth', 'table']]\nY = df.price\ntrain_X, test_X, train_Y, test_Y, = train_test_split(X, Y, test_size = 0.5)\nmodel.fit(train_X, train_Y)\npredictions = model.predict( test_X )\n\nmean_absolute_error(test_Y, predictions)","a9631460":"df2.predictions = model.predict(df2[['x', 'y', 'z', 'carat', 'cut', 'color', 'clarity', 'depth', 'table']])\ndf2.plot.scatter(x = 'price', y = 'predictions', s = 0.1, title = 'Test')","cc6939b9":"model = RandomForestRegressor()\nX = df[['x', 'y', 'z', 'carat', 'cut', 'color', 'clarity', 'depth', 'table']]\nY = df.price\ntrain_X, test_X, train_Y, test_Y, = train_test_split(X, Y, test_size = 0.5)\nmodel.fit(train_X, train_Y)\npredictions = model.predict( test_X )\n\nmean_absolute_error(test_Y, predictions)\ndf2['abs_error'] = abs(df2.price - df2.predictions)\ndf2","befbc5d9":"### Import other libraries as you need them here.\n#### You will need to re-run all or just this one cell if you add more imports later. You may also ignore this and import as you need later.","a1acf6d7":"Carat, x, y, and z all seem to have high correlation values. \nCut, color, clarity, depth, and table all have relativilty low correlation vlaues. \nCarat and price are very closely related, it may benefit from removing them.","d1111813":"It is weird that the set with the best model used depth and table which had very little corelation.\n\nMaybe it's simply a factor of the more data, the better the model.\n\nIf a feature was very random and had no correlation what so ever, it make be helpful to remove.","62ade33e":"#### (14)\nDisplay the coefficients your linear model computed in a nice chart showing the variable name and the computed coefficient.\n\n","dd654c9c":"#### (21)\nDisplay the full statistics on your DF again. Look particularly at the mean on the abs_error column. Does it match the number calculated above or is it slightly off? If it is off, can you explain why that may be?","73ea3168":"Depth and Table vs price all have a fairly verticle graph. This means that they wouldn't be able to accuratly predict the price of a diamond.\n\nCarat and X both look like they could be used to predict an accurate price.\n\nAny time x, y, or z = 0 should be removed.","24fb976a":"#### (26)\nDisplay a scatter plot comparing your price column to your predictions column. Ideally this would be an almost perfect line. You may want to make the size of the individual plotted points to be smaller. Title your plot with the type of regression used.\n","6cf46e5a":"#### (15)\n\nHard-code a line of code using each of your found coefficients to calculate\/display the predicted price of the following diamond:\n\nCarat: 0.8  \nCut: Very Good  \nColor: F  \nClarity: VVS2   \nDepth: 61.8  \nTable: 57.5  \nx: 5.7  \ny: 5.7  \nz: 3.5  ","9b99fd75":"#### (11)\nNow we will look a bit more at those correlations as a whole.  \n\nA heatmap can be used to display the different correlations in a graphical way. A correlation can be anywhere from -1 to 1. A correlation of 0 means there is almost nothing connecting one variable to another. Closer to 1 means that it is almost guaranteed that as one variable goes up, another goes up almost linearly the same. Closer to -1 means as one goes up, another goes down almost linearly.\n\nTo display a heatmap, use: sns.heatmap( your dataframe's correlation ) or sns.heatmap( your dataframe's correlations, annot=True). Try out both to see the difference.","fa64b226":"#### (23)\nNow run the model as a decision tree using all variables. Again show the mean absolute error.","299721bb":"#### (10)\nDiamonds are often talked about with their \"Four C's\". Cut, Color, Clarity, and Carat. According to these correlations, rank the \"Four C's\" from most important to least important in price. (Think for a moment on this one!)  \n  \nMost Important  \n1)  \n2)  \n3)  \n4)  \nLeast Important  ","464f2bf6":"#### (25)\nNow run the model as a random forest.","aedec2f3":"#### (9)\nDisplay the correlations of your DF, sorted in increasing order. You can either display all of them or limit the display only to the price column, which is the only correlation that will ultimately matter for our predictions.","a292f9bc":"#### (20)\nCreate a copy of your dataframe, then add a new column 'predictions' and fill it with the results from predicting your model on the entire original X with all of the columns. (If you re-used the same variable for the last few regression models again, make sure you reset the model back to using all of the columns).  \n\nCreate another new column called abs_error. Set this column to equal the absolute value of the price column minus the predictions column to show how far off the prediction was.\n\nDisplay your DF.","755107aa":"#### (22)\nDisplay a scatter plot comparing your price column to your predictions column. Ideally this would be an almost perfect line. You may want to make the size of the individual plotted points to be smaller. Title your plot with the type of regression used (Linear Regression in this case).","b4469be6":"#### (2)\nDisplay the column names only.","fdecef9f":"#### (6)\n\nDisplay all of the description statistics of the df.","07b10361":"Carrot, Color, clarity, and then Cut.","7ad1ae8e":"#### (1)\nLoad your DF. Display the first 5 rows.","d5ac4a7f":"#### (19)\nCompare the last three models used which utilized less features than the first.\n\n1) Is there anything that seems non-intuitive about which of these three sets is best?\n\n2) Can you attempt to explain why it is the best of these three?\n\n3) Using all possible features gave the best result (hopefully). Assuming you had no concern over how long your model took to execute, can you think of a reason why you may want to leave a feature out of your model?\n","5b2170a9":"#### (13)\nBuild a linear regression model.\nChoose all of the features (aside from price) for your X values.\nChoose price for your y values.\n\nSplit your data into two equal halves for training and validating.\nRun your model on your training set.\nPredict your y values based on your validation X values.\n\nWhat is your mean absolute error?  \n","eccb992a":"#### (17)\nTry again, this time only with carat.","ed4085bb":"#### (18)\nOne more time with carat, x, y, and z (the four highest correlating features compared to price).","d14c0f7b":"#### (16)\nRun another LinearRegression model, but this time use only the \"Four C's\" as your features. Display your mean absolute error.","1a765b7b":"#### (7)\n\nYou should notice that some of the columns are missing in the statistics and were also missing in the PairGrid. This is because these are categorical data instead of numerical. Most machine learning algorithms will not be able to work with categorical data without converting the data first into numerical values. There are libraries which can convert categories to numbers automatically for you, but they will just assign a number to each category with no regards to the 'value' of one category over another if the categories could be sorted. For instance, Bristow may get a 0, Gainesville may get a 1, Haymarket may get a 2. The order here could be arbitrary or alphabetic and it would not matter.  \n\nSome categorical values could be ordered. In diamonds, there is an order to the cut, color, and clarity values. One is 'better' than another.  \n\nConvert your categorical data into numerical data.  \nFair < Good < Very Good < Premium < Ideal  \nJ < I < H < ... < D  \nI1 < SI2 < SI1 < VS2 < VS1 < VVS2 < VVS1 < IF < FL\n\nDisplay your DF to check the categories converted.","459ee0cb":"It is fairly close but not exact. This is because I am testing with the whole table, where as that value was calculated using just the testing table.","c0be1bc4":"#### (12)\nThese are similar questions to the earlier ones.\n\n1) Which features seem to be the best to be used to predict the price of a diamond?  \n\n2) Which features may need to be thrown out during predictions?  \n\n3) There are several variables that are correlated so highly that perhaps it would be okay to remove some of them to cut down on computing times. Which variable(s) may you be able to cut and which variable(s) would you keep because it covers the same overall trends?","9023dd10":"#### (3)\n\nData clean up time.\n\nFirst, let's re-index all of the columns so that price becomes the final column. While doing so, we will remove the excess column by not placing in the list to replace.\n\nDisplay the df.","2deaf8e8":"#### (8)\n\nLet's deal with some of the outliers you may have noticed now. These may be explainable, but we are going to assume they are data that will interfere with our predictions. You should notice a few points in the y and z features that are definitely not consistent. There are also diamonds showing x\/y\/z values as 0, which present incomplete data we cannot use. Figure out a way to remove a good portion of these pieces of data that will interfere with our predictions.\n\nRe-display all the main statistics of your DF.\n\n","bc87e8a6":"\n#### (5)\n\nSome early project questions:\n\n1) Which two potential features (variables) may not be useful for predicting price?\n\n2) Which potential features look like they may be most useful for predicing price?\n\n3) Can you identify some potential incorrect data that needs to be removed? Name a few features and some ranges we may want to remove. (example: remove data with age > 100 )\n","f51ba651":"#### (24)\nDisplay a scatter plot comparing your price column to your predictions column. Ideally this would be an almost perfect line. You may want to make the size of the individual plotted points to be smaller. Title your plot with the type of regression used.","abffe98f":"#### (4)\n\nLet's take a look at the current data graphically. This will be useful later when we want to clean up some potential misleading data.  \n\nSeaborn is a library designed for some amazing data visualizations.  \nImport: seaborn as sns  \nThen use: sns.pairplot( your data frame, hue='cut', plot_kws={'s':1} )  \nThis will take a bit of time to run. You can try a different column for the hue, color or clarity should work well. The plot_kws is to make the size of the individual plots be smaller.  \nAfter running it and studying it you may want to open the image in a new tab then comment this line out while you work on the rest of the project so you do not run it again. Uncomment it before submitting.\n","04a95212":"#### (27)\nChoose the model with the least mean absolute error. Update the prediction column and the abs_error column to use this model's predictions.\nDisplay your final df."}}