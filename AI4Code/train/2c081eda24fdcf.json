{"cell_type":{"028050fc":"code","55127c7b":"code","f3a78aa2":"code","872abdf0":"code","465f156a":"code","4bf7e4be":"code","df527be8":"code","08a53e17":"code","02aed500":"code","fe894547":"code","4514e38c":"code","d4dc317c":"code","ef451b19":"code","5c42f418":"code","7a551bad":"code","300a0c82":"code","c48058af":"code","34f58c4d":"code","e1d0342d":"code","c85ef5fa":"code","16ca812a":"code","6b37c25f":"code","d86cc0db":"code","d4aeaca9":"code","db65f087":"code","e03f011b":"code","1370c6ac":"code","35268f69":"code","69e21120":"code","249cefd3":"code","fe9ff6fe":"code","eaaada6c":"code","0b21af9a":"code","d97cb7f8":"code","9ddf53b6":"code","3591904e":"code","32891758":"code","9607a66b":"code","d857c203":"code","b79261d1":"code","d136683b":"code","477ce03b":"code","69f49e6d":"code","fccc3237":"code","99fc46ce":"code","4e77afdf":"code","58f6b8bc":"code","28445487":"code","d59aab4d":"code","a67b869c":"code","cb3eca6a":"code","6e3d1b62":"code","c0775ae5":"code","95463663":"code","811829b1":"code","4c766636":"code","25d687e4":"code","d64aefd5":"code","47b910f3":"markdown","e1fbe558":"markdown","c5c19401":"markdown","83a27de2":"markdown","7054ba22":"markdown","e6d2c275":"markdown","199d25c0":"markdown","a6e1512c":"markdown","80760c39":"markdown","0677f1ab":"markdown","dafe6bd2":"markdown","98eb2c47":"markdown","af1721ba":"markdown","6f60c92f":"markdown","04f2b364":"markdown","56944c74":"markdown","80a9904a":"markdown","d577f5ec":"markdown","7aa8bb84":"markdown","b62847cd":"markdown","75a0f828":"markdown","aa195218":"markdown","baa16539":"markdown","b31973bc":"markdown","6d7492ef":"markdown","f209e00a":"markdown","cc21400e":"markdown","a18fb763":"markdown","9e629b9f":"markdown","b643bd22":"markdown","8a6aa790":"markdown","d55bccfd":"markdown","c7387499":"markdown","1721deb8":"markdown","eebf2a74":"markdown","9e4c2176":"markdown","dd1125a1":"markdown","3ba74471":"markdown","b164f7f6":"markdown","1afddfd9":"markdown","99c5c7bd":"markdown","91db772e":"markdown","fa1bad2f":"markdown","405b5327":"markdown","ccd80e8f":"markdown","8a990075":"markdown","c24bb294":"markdown","49260495":"markdown","6a5c5cb0":"markdown","e49365d6":"markdown","71c85e3e":"markdown","ff7c3cf4":"markdown","302dfc6d":"markdown","b81d4e46":"markdown"},"source":{"028050fc":"import numpy as np\nimport os\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport warnings; warnings.simplefilter('ignore')\n\nimport pandas as pd\nfrom pandas import read_csv\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler","55127c7b":"#Read the data\nfilename = '..\/input\/sandp500\/all_stocks_5yr.csv'\nstock = read_csv(filename)\nprint(\"***Structure of data with all its features***\")\nstock.head()","f3a78aa2":"ticker_name = 'AAPL'\nstock_a = stock[stock['Name'] == ticker_name]\nstock_a.shape","872abdf0":"stock.info()","465f156a":"stock_a.describe()","4bf7e4be":"stock_a['changeduringday'] = ((stock['high'] - stock['low'] )\/ stock['low'])*100\n\nstock_a['changefrompreviousday'] = (abs(stock_a['close'].shift() - stock_a['close'] )\/ stock['close'])*100\n","df527be8":"print(\"**The new features 'changeduring day & change from previous day are added to the dataset. Note: The first row for change from previous day for each stock is NA or blank always\")\nstock_a.head()","08a53e17":"stock_a.hist(bins=50, figsize=(20,15))\nplt.show()","02aed500":"stock_a.plot(kind=\"line\", x=\"date\", y=\"close\", figsize=(15, 10))","fe894547":"corr_matrix = stock_a.corr()","4514e38c":"corr_matrix[\"close\"].sort_values(ascending=False)","d4dc317c":"from pandas.plotting import scatter_matrix\n\nattributes = [\"high\", \"low\", \"open\", \"changefrompreviousday\", \"changeduringday\", \"volume\"]\n\nscatter_matrix(stock_a[attributes], figsize=(20, 15))","ef451b19":"import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\ncorr = stock_a[[\"high\", \"low\", \"open\", \"changefrompreviousday\", \"changeduringday\", \"volume\"]].corr()\n\n# generate a mask for the lower triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 12))\n\n# generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3,\n            square=True, \n            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax);","5c42f418":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,Normalizer\nX_stock_a = stock_a.drop(['date', 'Name','close'], axis=1)\ny_stock_a = stock_a['close']\n\nX_stock_train, X_stock_test, y_stock_train, y_stock_test = train_test_split(X_stock_a, y_stock_a, test_size=0.2, \n                                                                            random_state=42)\n","7a551bad":"#Data prep pipeline\ndata_pipeline = Pipeline([\n        ('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #Use the \"median\" to impute missing vlaues\n        ('scaler',StandardScaler())\n#        ('normalizer', Normalizer()),\n    ])","300a0c82":"from sklearn.preprocessing import Imputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler,Normalizer\n\nfrom sklearn.pipeline import Pipeline\n\nLr_pipeline_nor = Pipeline([\n        ('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #Use the \"median\" to impute missing vlaues\n        ('normalizer',Normalizer()),\n        ('lr', LinearRegression())\n        \n    ])\n\nLr_pipeline_nor.fit(X_stock_train, y_stock_train)","c48058af":"from sklearn.svm import SVR\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\n\nsvr_pipeline_nor = Pipeline([\n        ('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #Use the \"median\" to impute missing vlaues\n        ('normalizer',Normalizer()),\n        ('svr', SVR(kernel=\"linear\"))\n        \n    ])\n\nsvr_pipeline_nor.fit(X_stock_train, y_stock_train)\n","34f58c4d":"from sklearn.svm import SVR\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\n\nsvrrbf_pipeline_nor = Pipeline([\n        ('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #Use the \"median\" to impute missing vlaues\n        ('normalizer',Normalizer()),\n        ('svr', SVR(kernel=\"rbf\"))\n        \n    ])\n\nsvrrbf_pipeline_nor.fit(X_stock_train, y_stock_train)","e1d0342d":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ndt_pipeline_nor = Pipeline([\n        ('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #Use the \"median\" to impute missing vlaues\n        ('normalizer',Normalizer()),\n        ('dt', DecisionTreeRegressor(random_state=42))\n        \n    ])\n\ndt_pipeline_nor.fit(X_stock_train, y_stock_train)","c85ef5fa":"from sklearn.preprocessing import Imputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n#Data prep pipeline\n\nLr_pipeline_std = Pipeline([\n        ('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #Use the \"median\" to impute missing vlaues\n        ('scaler',StandardScaler()),\n        ('lr', LinearRegression())\n        \n    ])\n\nLr_pipeline_std.fit(X_stock_train, y_stock_train)\n\n","16ca812a":"from sklearn.svm import SVR\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\n\nsvr_pipeline_std = Pipeline([\n        ('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #Use the \"median\" to impute missing vlaues\n        ('scaler',StandardScaler()),\n        ('svr', SVR(kernel=\"linear\"))\n        \n    ])\n\nsvr_pipeline_std.fit(X_stock_train, y_stock_train)\n","6b37c25f":"from sklearn.svm import SVR\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\n\nsvrrbf_pipeline_std = Pipeline([\n        ('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #Use the \"median\" to impute missing vlaues\n        ('scaler',StandardScaler()),\n        ('svrrbf', SVR(kernel=\"rbf\"))\n        \n    ])\n\nsvrrbf_pipeline_std.fit(X_stock_train, y_stock_train)","d86cc0db":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ndt_pipeline_std = Pipeline([\n        ('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #Use the \"median\" to impute missing vlaues\n        ('scaler',StandardScaler()),\n        ('dt', DecisionTreeRegressor(random_state=42))\n        \n    ])\n\ndt_pipeline_std.fit(X_stock_train, y_stock_train)","d4aeaca9":"#Doing Regularization Ridge \n\n#1. Fine tune Ridge Regression using Random search\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection  import cross_validate\n\npipeline = Pipeline([('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #Use the \"median\" to impute missing vlaues\n        ('scaler',StandardScaler()),\n    ('RidgeReg', Ridge()),\n])\n\nparam_distribs = {#listed in the form of \"pipelineStep__parameter\", e.g, \"RidgeLogReg__alpha\":(100., 10., 1.,0.1)\n    \"RidgeReg__alpha\":(1e2, 1e1, 1., 1e-1, 1e-2, 1e-3, 1e-4),\n    \"RidgeReg__fit_intercept\":(True,False)\n}\n\n\nrnd_search_ridge_cv=  RandomizedSearchCV(pipeline, param_distribs,\n                                 cv=10, scoring='neg_mean_squared_error',\n                                verbose=2, n_jobs=-1)\n\nrnd_search_ridge_cv.fit(X_stock_train, y_stock_train)\n\n\n#Mean CV scores for Ridge\nridgescores = rnd_search_ridge_cv.cv_results_\nfor mean_score, params in zip(ridgescores[\"mean_test_score\"], ridgescores[\"params\"]):\n    print(np.sqrt(-mean_score), params)\nprint ('Best CV score for Ridge: ', -rnd_search_ridge_cv.best_score_)","db65f087":"from sklearn.metrics import mean_squared_error\n\nrnd_search_stock_predictions = rnd_search_ridge_cv.best_estimator_.predict(X_stock_test)\nrnd_search_mse = mean_squared_error(y_stock_test, rnd_search_stock_predictions)\nrnd_search_rmse = np.sqrt(rnd_search_mse)\nprint('Ridge regression best estimator RMSE with Standardization', rnd_search_rmse)","e03f011b":"#Lasso\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n\npipeline = Pipeline([('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #Use the \"median\" to impute missing vlaues\n        ('scaler',StandardScaler()),\n    ('LassoReg', Lasso()),\n])\n\nparam_distribs = {\n    \"LassoReg__alpha\":(1e2, 1e1, 1., 1e-1, 1e-2, 1e-3, 1e-4),\n    \"LassoReg__fit_intercept\":(True,False)\n}\n\n\nrnd_search_lasso_cv = RandomizedSearchCV(pipeline, param_distribs,\n                                 cv=10, n_iter=10,scoring='neg_mean_squared_error',\n                                verbose=2, n_jobs=-1,random_state=42)\n\nrnd_search_lasso_cv.fit(X_stock_train, y_stock_train)\n\n\n#Mean CV scores for Lasso\nLasscores = rnd_search_lasso_cv.cv_results_\nfor mean_score, params in zip(Lasscores[\"mean_test_score\"], Lasscores[\"params\"]):\n    print(np.sqrt(-mean_score), params)\nprint ('Best CV score for Lasso: ', -rnd_search_lasso_cv.best_score_)","1370c6ac":"rnd_search_lasso_stock_predictions = rnd_search_lasso_cv.best_estimator_.predict(X_stock_test)\nrnd_search_lasso_mse = mean_squared_error(y_stock_test, rnd_search_lasso_stock_predictions)\nrnd_search_lasso_rmse = np.sqrt(rnd_search_lasso_mse)\nprint('Lasso regression best estimator RMSE with Standardization', rnd_search_lasso_rmse)","35268f69":"\n\n#1. Fine tune Linear Regression using Random search\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n\nlin_reg = LinearRegression()\n\n\n\nX_stock_train = data_pipeline.fit_transform(X_stock_train)\n\nscores = cross_validation.cross_val_score(lin_reg, X_stock_train, y_stock_train, scoring='neg_mean_squared_error', cv=10,)\n\n\n#Metrics - Mean CV scores for Linear Regression\nprint (-scores)\nprint ('Mean score for Linear Regression: ', np.mean(-scores))","69e21120":"#2. Fine tune Decision Tree Regressor using Random search\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_distribs = {\n        'max_depth': [1,2,3,4,5,6,7,8,9,10],\n    }\n\ntree_reg = DecisionTreeRegressor(random_state=42)\nrnd_search_tree = RandomizedSearchCV(tree_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=10, scoring='neg_mean_squared_error',\n                                verbose=2, n_jobs=-1, random_state=42)\n\nX_stock_fe_prep = data_pipeline.transform(X_stock_train)\nrnd_search_tree.fit(X_stock_fe_prep, y_stock_train)\n\n#Metrics - Mean CV scores for Decistion Tree\ncvres2 = rnd_search_tree.cv_results_\nfor mean_score, params in zip(cvres2[\"mean_test_score\"], cvres2[\"params\"]):\n    print(np.sqrt(-mean_score), params)\nprint ('Best CV score for Decision Tree: ', -rnd_search_tree.best_score_)","249cefd3":"#3. Fine tune SVM regression using Random search\n\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import expon, reciprocal\n\n\n# Note: gamma is ignored when kernel is \"linear\"\nparam_distribs = {\n        'kernel': ['linear', 'rbf'],\n        'C': reciprocal(20, 1000),\n        'gamma': expon(scale=1.0),\n    }\n\nsvm_reg = SVR()\nrnd_search_svr = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=2, scoring='neg_mean_squared_error',\n                                verbose=2, n_jobs=-1, random_state=42)\nX_stock_fe_prep = data_pipeline.fit_transform(X_stock_train[0:500])\nrnd_search_svr.fit(X_stock_fe_prep, y_stock_train[0:500])\n\n\n#Mean CV scores for SVR\ncvres3 = rnd_search_svr.cv_results_\nfor mean_score, params in zip(cvres3[\"mean_test_score\"], cvres3[\"params\"]):\n    print(np.sqrt(-mean_score), params)\nprint ('Best CV score for SVR: ', -rnd_search_svr.best_score_)","fe9ff6fe":"#Lasso Regression\nlasso_stock_predictions_std = rnd_search_lasso_cv.best_estimator_.predict(X_stock_test)\nlasso_mse_std = mean_squared_error(y_stock_test, lasso_stock_predictions_std)\nlasso_rmse_std = np.sqrt(lasso_mse_std)\nprint('Lasso Regression RMSE with Standardization', lasso_rmse_std)\n\n#Ridge Regression\nridge_stock_predictions_std = rnd_search_ridge_cv.best_estimator_.predict(X_stock_test)\nridge_mse_std = mean_squared_error(y_stock_test, ridge_stock_predictions_std)\nridge_rmse_std = np.sqrt(ridge_mse_std)\nprint('Ridge Regression RMSE with Standardization', ridge_rmse_std)\n\n","eaaada6c":"from sklearn.metrics import mean_absolute_error\n\n#Linear Regression with normalisation and standardisation\nlr_stock_predictions_nor = Lr_pipeline_nor.predict(X_stock_test)\nlr_mae_nor = mean_absolute_error(y_stock_test, lr_stock_predictions_nor)\nprint('Lr MAE with Normalization', lr_mae_nor)\n\nlr_stock_predictions_std = Lr_pipeline_std.predict(X_stock_test)\nlr_mae_std = mean_absolute_error(y_stock_test, lr_stock_predictions_std)\nprint('Lr MAE with standardization', lr_mae_std)\n\n#SVM with normalisation and standardisation\nsvm_stock_predictions_nor = svr_pipeline_nor.predict(X_stock_test)\nsvm_mae_nor = mean_absolute_error(y_stock_test, svm_stock_predictions_nor)\nprint('SVM MAE with Normalization', svm_mae_nor)\n\nsvm_stock_predictions_std = svr_pipeline_std.predict(X_stock_test)\nsvm_mae_std = mean_absolute_error(y_stock_test, svm_stock_predictions_std)\nprint('SVM MAE with standardization', svm_mae_std)\n\n\n#SVM with RFB Kernel with normalisation and standardisation\nsvmrbf_stock_predictions_nor = svrrbf_pipeline_nor.predict(X_stock_test)\nsvmrbf_mae_nor = mean_absolute_error(y_stock_test, svmrbf_stock_predictions_nor)\nprint('SVM RBF MAE with Normalization', svmrbf_mae_nor)\n\n\nsvmrbf_stock_predictions_std = svrrbf_pipeline_std.predict(X_stock_test)\nsvmrbf_mae_std = mean_absolute_error(y_stock_test, svmrbf_stock_predictions_std)\nprint('SVM RBF MAE with standardization', svmrbf_mae_std)\n\n#Decision Tree with normalisation and standardisation\ndt_stock_predictions_nor = dt_pipeline_nor.predict(X_stock_test)\ndt_mae_nor = mean_absolute_error(y_stock_test, dt_stock_predictions_nor)\nprint('DecisionTree MAE with Normalization', dt_mae_nor)\n\ndt_stock_predictions_std = dt_pipeline_std.predict(X_stock_test)\ndt_mae_std = mean_absolute_error(y_stock_test, dt_stock_predictions_std)\nprint('DecisionTree MAE with standardization', dt_mae_std)\n\n\n#Lasso Regression\nlasso_stock_predictions_std = rnd_search_lasso_cv.best_estimator_.predict(X_stock_test)\nlasso_mae_std = mean_absolute_error(y_stock_test, lasso_stock_predictions_std)\nprint('Lasso Regression MAE with Standardization', lasso_mae_std)\n\n#Ridge Regression\nridge_stock_predictions_std = rnd_search_ridge_cv.best_estimator_.predict(X_stock_test)\nridge_mae_std = mean_absolute_error(y_stock_test, ridge_stock_predictions_std)\nprint('Ridge Regression MAE with Standardization', ridge_mae_std)\n\n","0b21af9a":"import pandas as pd\nimport numpy as np\n\n#Predict and report RMSE\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.metrics import mean_squared_error\n\n#Linear Regression with normalisation and standardisation\nlr_stock_predictions_nor = Lr_pipeline_nor.predict(X_stock_test)\nlr_mse_nor = mean_squared_error(y_stock_test, lr_stock_predictions_nor)\nlr_rmse_nor = np.sqrt(lr_mse_nor)\nprint('Lr RMSE with Normalization', lr_rmse_nor)\n\nlr_stock_predictions_std = Lr_pipeline_std.predict(X_stock_test)\nlr_mse_std = mean_squared_error(y_stock_test, lr_stock_predictions_std)\nlr_rmse_std = np.sqrt(lr_mse_std)\nprint('Lr RMSE with standardization', lr_rmse_std)\n\n#SVM with normalisation and standardisation\nsvm_stock_predictions_nor = svr_pipeline_nor.predict(X_stock_test)\nsvm_mse_nor = mean_squared_error(y_stock_test, svm_stock_predictions_nor)\nsvm_rmse_nor = np.sqrt(svm_mse_nor)\nprint('SVM RMSE with Normalization', svm_rmse_nor)\n\nsvm_stock_predictions_std = svr_pipeline_std.predict(X_stock_test)\nsvm_mse_std = mean_squared_error(y_stock_test, svm_stock_predictions_std)\nsvm_rmse_std = np.sqrt(svm_mse_std)\nprint('SVM RMSE with standardization', svm_rmse_std)\n\n\n#SVM with RFB Kernel with normalisation and standardisation\nsvmrbf_stock_predictions_nor = svrrbf_pipeline_nor.predict(X_stock_test)\nsvmrbf_mse_nor = mean_squared_error(y_stock_test, svmrbf_stock_predictions_nor)\nsvmrbf_rmse_nor = np.sqrt(svmrbf_mse_nor)\nprint('SVM RBF RMSE with Normalization', svmrbf_rmse_nor)\n\n\nsvmrbf_stock_predictions_std = svrrbf_pipeline_std.predict(X_stock_test)\nsvmrbf_mse_std = mean_squared_error(y_stock_test, svmrbf_stock_predictions_std)\nsvmrbf_rmse_std = np.sqrt(svmrbf_mse_std)\nprint('SVM RBF RMSE with standardization', svmrbf_rmse_std)\n\n#Decision Tree with normalisation and standardisation\ndt_stock_predictions_nor = dt_pipeline_nor.predict(X_stock_test)\ndt_mse_nor = mean_squared_error(y_stock_test, dt_stock_predictions_nor)\ndt_rmse_nor = np.sqrt(dt_mse_nor)\nprint('DecisionTree RMSE with Normalization', dt_rmse_nor)\n\ndt_stock_predictions_std = dt_pipeline_std.predict(X_stock_test)\ndt_mse_std = mean_squared_error(y_stock_test, dt_stock_predictions_std)\ndt_rmse_std = np.sqrt(dt_mse_std)\nprint('DecisionTree RMSE with standardization', dt_rmse_std)\n\n\n#Lasso Regression\nlasso_stock_predictions_std = rnd_search_lasso_cv.best_estimator_.predict(X_stock_test)\nlasso_mse_std = mean_squared_error(y_stock_test, lasso_stock_predictions_std)\nlasso_rmse_std = np.sqrt(lasso_mse_std)\nprint('Lasso Regression RMSE with Standardization', lasso_rmse_std)\n\n#Ridge Regression\nridge_stock_predictions_std = rnd_search_ridge_cv.best_estimator_.predict(X_stock_test)\nridge_mse_std = mean_squared_error(y_stock_test, ridge_stock_predictions_std)\nridge_rmse_std = np.sqrt(ridge_mse_std)\nprint('Ridge Regression RMSE with Standardization', ridge_rmse_std)\n\nlr_std = ['1',\"Linear Regression with standardisation\",np.round(lr_rmse_std,3),np.round(lr_mae_std,3)]\nlr_nor = ['2',\"Linear Regression with normalisation\",np.round(lr_rmse_nor,3),np.round(lr_mae_nor,3)]\ndt_std = ['3',\"Decision Tree with standardisation\",np.round(dt_rmse_std,3),np.round(dt_mae_std,3)]\ndt_nor = ['4',\"Decision Tree with normalisation\",np.round(dt_rmse_nor,3),np.round(dt_mae_nor,3)]\n\nsvm_std = ['5',\"SVM with standardisation\",np.round(svm_rmse_std,3),np.round(svm_mae_std,3)]\nsvm_nor = ['6',\"SVM with normalisation\",np.round(svm_rmse_nor,3),np.round(svm_mae_nor,3)]\n\nsvmrfb_std = ['7',\"SVM RFB with standardisation\",np.round(svmrbf_rmse_std,3),np.round(svmrbf_mae_std,3)]\nsvmrfb_nor = ['8',\"SVM RFB with normalisation\",np.round(svmrbf_rmse_nor,3),np.round(svmrbf_mae_nor,3)]\nridge_std = ['9',\"Ridge Regression with standardisation\",np.round(ridge_rmse_std,3),np.round(ridge_mae_std,3)]\nlasso_std = ['10',\"Lasso Regression with standardisation\",np.round(lasso_rmse_std,3),np.round(lasso_mae_std,3)]\n\n\nlinear_model_result= pd.DataFrame([lr_std,lr_nor,dt_std,dt_nor,svm_std,svm_nor,svmrfb_std,svmrfb_nor,ridge_std,lasso_std],columns=[ \"ExpID\", \"Model\", \"RMSE\",\"MAE\"])\n\nlinear_model_result","d97cb7f8":"#function to return all the models for a given ticker\nfrom sklearn.preprocessing import Imputer\n    \ndef allModelsResultForAllStocks():\n    \n    best_result_per_ticker = pd.DataFrame(columns=['Ticker','Model','RMSE'])\n    ticker_list = np.unique(stock[\"Name\"])\n    best_result_per_ticker = list()\n    for ticker_name in ticker_list:\n        result = pd.DataFrame(columns=['Ticker','Model','RMSE'])\n        stock_a = stock[stock['Name'] == ticker_name]\n        #Adding new features \n        #1 Price movement during day time \n        stock_a['changeduringday'] = ((stock['high'] - stock['low'] )\/ stock['low'])*100\n\n        #2 Price movement \n        stock_a['changefrompreviousday'] = (abs(stock_a['close'].shift() - stock_a['close'] )\/ stock['close'])*100\n\n        X_stock_a = stock_a.drop(['date', 'Name','close'], axis=1)\n        y_stock_a = stock_a['close']\n\n        \n        imputer = Imputer(missing_values='NaN', strategy='median')\n        \n        imputer.fit_transform(X_stock_a)\n       \n        X_stock_train, X_stock_test, y_stock_train, y_stock_test = train_test_split(X_stock_a, y_stock_a, test_size=0.2, \n                                                                                random_state=42)\n\n\n        Lr_pipeline_std.fit(X_stock_train, y_stock_train)\n        Lr_pipeline_nor.fit(X_stock_train, y_stock_train)\n\n        svr_pipeline_nor.fit(X_stock_train, y_stock_train)\n        svr_pipeline_std.fit(X_stock_train, y_stock_train)\n\n        svrrbf_pipeline_nor.fit(X_stock_train, y_stock_train)\n        svrrbf_pipeline_std.fit(X_stock_train, y_stock_train)\n\n\n        dt_pipeline_nor.fit(X_stock_train, y_stock_train)\n        dt_pipeline_std.fit(X_stock_train, y_stock_train)    \n   \n        # Predict & Calculate RMSE for all the models \n\n        #Linear Regression with normalisation and standardisation\n        lr_stock_predictions_nor = Lr_pipeline_nor.predict(X_stock_test)\n        lr_mse_nor = mean_squared_error(y_stock_test, lr_stock_predictions_nor)\n        lr_rmse_nor = np.sqrt(lr_mse_nor)\n        rmse_row =   [ticker_name,'Lr RMSE with Normalization', lr_rmse_nor]\n\n        result.loc[-1] = rmse_row  # adding a row\n        result.index = result.index + 1  # shifting index\n     \n    \n        lr_stock_predictions_std = Lr_pipeline_std.predict(X_stock_test)\n        lr_mse_std = mean_squared_error(y_stock_test, lr_stock_predictions_std)\n        lr_rmse_std = np.sqrt(lr_mse_std)\n        rmse_row =   [ticker_name,'Lr RMSE with standardization', lr_rmse_std]\n    \n    \n\n        result.loc[-1] = rmse_row  # adding a row\n        result.index = result.index + 1  # shifting index\n\n        #SVM with normalisation and standardisation\n        svm_stock_predictions_nor = svr_pipeline_nor.predict(X_stock_test)\n        svm_mse_nor = mean_squared_error(y_stock_test, svm_stock_predictions_nor)\n        svm_rmse_nor = np.sqrt(svm_mse_nor)\n        rmse_row =   [ticker_name,'SVM RMSE with Normalization', svm_rmse_nor]\n        \n\n        result.loc[-1] = rmse_row  # adding a row\n        result.index = result.index + 1  # shifting index\n\n        svm_stock_predictions_std = svr_pipeline_std.predict(X_stock_test)\n        svm_mse_std = mean_squared_error(y_stock_test, svm_stock_predictions_std)\n        svm_rmse_std = np.sqrt(svm_mse_std)\n        rmse_row =   [ticker_name,'SVM RMSE with standardization', svm_rmse_std]\n    \n        result.loc[-1] = rmse_row  # adding a row\n        result.index = result.index + 1  # shifting index\n\n\n        #SVM with RFB Kernel with normalisation and standardisation\n        svmrbf_stock_predictions_nor = svrrbf_pipeline_nor.predict(X_stock_test)\n        svmrbf_mse_nor = mean_squared_error(y_stock_test, svmrbf_stock_predictions_nor)\n        svmrbf_rmse_nor = np.sqrt(svmrbf_mse_nor)\n        rmse_row =   [ticker_name,'SVM RBF RMSE with Normalization', svmrbf_rmse_nor]\n   \n        result.loc[-1] = rmse_row  # adding a row\n        result.index = result.index + 1  # shifting index\n\n\n        svmrbf_stock_predictions_std = svrrbf_pipeline_std.predict(X_stock_test)\n        svmrbf_mse_std = mean_squared_error(y_stock_test, svmrbf_stock_predictions_std)\n        svmrbf_rmse_std = np.sqrt(svmrbf_mse_std)\n        rmse_row =   [ticker_name,'SVM RBF RMSE with standardization', svmrbf_rmse_std]\n    \n        result.loc[-1] = rmse_row  # adding a row\n        result.index = result.index + 1  # shifting index\n\n        #Decision Tree with normalisation and standardisation\n        dt_stock_predictions_nor = dt_pipeline_nor.predict(X_stock_test)\n        dt_mse_nor = mean_squared_error(y_stock_test, dt_stock_predictions_nor)\n        dt_rmse_nor = np.sqrt(dt_mse_nor)\n        rmse_row =   [ticker_name,'DecisionTree RMSE with Normalization', dt_rmse_nor]\n\n        result.loc[-1] = rmse_row  # adding a row\n        result.index = result.index + 1  # shifting index\n\n        dt_stock_predictions_std = dt_pipeline_std.predict(X_stock_test)\n        dt_mse_std = mean_squared_error(y_stock_test, dt_stock_predictions_std)\n        dt_rmse_std = np.sqrt(dt_mse_std)\n        rmse_row = [ticker_name,'DecisionTree RMSE with standardization', dt_rmse_std]\n \n        result.loc[-1] = rmse_row  # adding a row\n        result.index = result.index + 1  # shifting index\n        result = result.sort_values(by = ['RMSE'])\n        \n       \n        best_result_per_ticker.append(np.array(result.iloc[0, :]))\n       \n\n\n    best_result_per_ticker_df = pd.DataFrame(data=best_result_per_ticker, columns=['Ticker','Model','RMSE'])\n    \n    \n    return best_result_per_ticker_df\n\nbest_result_per_ticker = allModelsResultForAllStocks()","9ddf53b6":"#Statistics Significance test  \n\nfrom sklearn.model_selection import cross_val_score\nfrom scipy import stats\n\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\n# A sampling based bakeoff using K-fold cross-validation: \n# it randomly splits the training set into K distinct subsets (k=30)\n# this bakeoff framework can be used for regression or classification\n#Control system is a linear regression based pipeline\n\nkFolds=30\n\nlin_scores = cross_val_score(Lr_pipeline_std, X_stock_train, y_stock_train,\n                             scoring=\"neg_mean_squared_error\", cv=kFolds)\ncontrol = lin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n\nscores = cross_val_score(svr_pipeline_std, X_stock_train, y_stock_train,\n                         scoring=\"neg_mean_squared_error\", cv=kFolds)\ntreatment = tree_rmse_scores = np.sqrt(-scores)\ndisplay_scores(tree_rmse_scores)\n\n#paired t-test; two-tailed p-value (aka two-sided)\n(t_score, p_value) = stats.ttest_rel(control, treatment)\nprint(\"The p-value is %0.5f for a t-score of %0.5f.\" %(p_value, t_score))\n#\"The p-value is 0.00019 for a t-score of -4.28218.\" \n","3591904e":"if p_value > 0.05\/2:  #Two sided \n    print('There is no significant difference between the two machine learning pipelines (Accept H0)')\nelse:\n    print('The two machine learning pipelines are different (reject H0) \\n(t_score, p_value) = (%.2f, %.5f)'%(t_score, p_value) )\n    if t_score < 0.0:\n        print('Machine learning pipeline Linear regression is better than linear SVR pipeline')\n    else:\n        print('Machine learning pipeline linear SVR pipeline is better than Linear regression')","32891758":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\n\nparam_grid = {'fit_intercept':[True,False], 'normalize':[True,False]}\n\nran_srch = GridSearchCV(lin_reg,param_grid= param_grid,cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\n\nX_stock_fe_prep1 = data_pipeline.fit_transform(X_stock_train)\nran_srch.fit(X_stock_fe_prep1, y_stock_train)\n\n","9607a66b":"#feature_importances = ran_srch.best_estimator_.feature_importances_\nfeature_importances = ran_srch.best_estimator_.coef_\nfeature_importances","d857c203":"from sklearn.ensemble import RandomForestRegressor\nparam_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    #{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nran_reg = RandomForestRegressor(random_state=42)\nran_srch = GridSearchCV(ran_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\n\nX_stock_fe_prep1 = data_pipeline.fit_transform(X_stock_train[0:500])\nran_srch.fit(X_stock_fe_prep1, y_stock_train[0:500])\n","b79261d1":"feature_importances = ran_srch.best_estimator_.feature_importances_\n\nfeature_importances","d136683b":"#Classification function homegrown logic based on stock price mean variation  \ndef classify (meanValue):\n    if meanValue <=1.5:\n        return 'Low'\n    elif meanValue >1.5 and  meanValue <=2.5:\n        return 'Medium'\n    elif meanValue >2.5:\n        return 'High'","477ce03b":"#function to get linear model for given ticker \n\ndef linearModel(ticker):\n    stock_a = stock[stock['Name'] == ticker]\n    #Adding new features \n    #1 Price movement during day time \n    stock_a['changeduringday'] = ((stock['high'] - stock['low'] )\/ stock['low'])*100\n\n    #2 Price movement \n    stock_a['changefrompreviousday'] = (abs(stock_a['close'].shift() - stock_a['close'] )\/ stock['close'])*100\n\n    X_stock_a = stock_a.drop(['date', 'Name','close'], axis=1)\n    y_stock_a = stock_a['close']\n\n    Lr_pipeline_std.fit(X_stock_a, y_stock_a)\n    \n    model = Lr_pipeline_std.named_steps['lr']\n    \n    return model,stock_a","69f49e6d":"\n#using all the 500 stocks for training \nticker_list = np.unique(stock['Name'])\n\ndf = pd.DataFrame(columns=['TICKER','CLASS','Coef for open','Coef for high','Coef for low','Coef for volume','Coef for change within day','Coef for change from prev day'])\nfor ticker in ticker_list[:50]:\n    \n    model,stock_a = linearModel(ticker)    \n    \n    #print(\"Mean value:\",stock_a[\"changeduringday\"].mean())\n    #adding target class \n    stock_features = np.concatenate((np.asarray([ticker,classify(stock_a[\"changeduringday\"].mean())]),model.coef_))\n    \n    df.loc[-1] = stock_features  # adding a row\n    df.index = df.index + 1  # shifting index\n    df = df.sort_index() \n   \n#print(df)\n\n#saving feature coefficients and target class for 500 stocks \ndf.to_csv('coeff1.csv', mode='a',header=['TICKER','CLASS','Coef for open','Coef for high','Coef for low','Coef for volume','Coef for change within day','Coef for change from prev day'])\n    \n","fccc3237":"# loading libraries\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\n\nX_class = np.array(df.ix[:, 2:8]) \ny_class = np.array(df['CLASS']) \n\n\n# split into train and test\nX_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class, y_class, test_size=0.2, random_state=42)","99fc46ce":"\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n# instantiate learning model (k = 3)\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# fitting the model\nknn.fit(X_train_class, y_train_class)\n\n# predict the response\npred = knn.predict(X_test_class)\n\n# evaluate accuracy\nprint (\"Accuracy of KNN \", accuracy_score(y_test_class, pred))","4e77afdf":"#Using K-means model to cluster the stocks linear model coefficients \n\nfrom sklearn.cluster import KMeans\n\nX_class = np.array(df.ix[:, 2:8]) \t# end index is exclusive\n\nk_mean = KMeans()\n\n#number of clusters will be decided by K-mean++ , by default \nk_mean_model = k_mean.fit(X_class)\n\nprint(\"Number of clusters\",k_mean_model.n_clusters) ","58f6b8bc":"#Predicting the cluster assignment for 500 stocks \n\n#Dropping the CLASS column added as part of classification , we don't need for clustering\ndf_cluster = df.drop(['CLASS'], axis=1)\n\n#Selecting features from dataframe , there are 6 features \nX_cluster = np.array(df_cluster.ix[:, 1:7])\n\ny_pred = k_mean_model.predict(X_cluster)\n\npred_df = pd.DataFrame({ 'Stock': df_cluster.ix[:, 0],'Cluster': y_pred})\n\n","28445487":"#Cluster assignment for the stocks , there are total 8 clusters \npred_df.head(n=20)\n","d59aab4d":"#Taking investor's current portfolio in following form \n#[['Stock1','$dollar investment'],['Stock2','$investment']]\n#[['AAPL','4000'],['GOOGL','5000'],['GE','7000']]\nfrom pandas import read_csv\nstock_portfolio_file = input(\"Enter the stock portfolio CSV file name (default is portfolio.csv) ?\") or '..\/input\/stock-portfolio\/stock portfolio.csv'\nprint(stock_portfolio_file)\n      \nstock_portfolio_csv = read_csv(stock_portfolio_file)\nstock_portfolio_csv.head()","a67b869c":"#Pie chart for the stock investment distribution based on individual stocks \n\nimport matplotlib.pyplot as plt\n\nimport matplotlib as mpl\nmpl.rcParams['font.size'] = 12.0\n\n# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nlabels = stock_portfolio_csv['Ticker']\nsizes = stock_portfolio_csv['Investment']\n\nfig1, ax1 = plt.subplots(figsize=(12, 10))\n\npatches, texts, autotexts = ax1.pie(sizes,  labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nax1.set_title(\"Investment diversification per stock\")\nplt.legend(patches, labels, loc=\"best\")\n\nplt.show()","cb3eca6a":"#Predicting cluster for investor's portfolio stocks based on the clustering model \n\npred_stock_portfolio_df = pd.DataFrame(columns=['Ticker','Predicted_cluster','Investment'])\n\n#iterate over all the stock tickers and predict cluster \nfor index, row  in stock_portfolio_csv.iterrows():\n    customer_stock_model,stock_modified = linearModel(row['Ticker'])\n    customer_stock_cluster_pred = k_mean_model.predict([customer_stock_model.coef_])[0]\n  \n    pred_stock_portfolio_df.loc[len(pred_stock_portfolio_df)] = [row['Ticker'],customer_stock_cluster_pred , row['Investment']]\n    \nprint (pred_stock_portfolio_df)","6e3d1b62":"#Grouping the investment per cluster ,as we want investor to balance the investment per cluster \n\npred_stock_portfolio_grouped_df =pred_stock_portfolio_df.groupby('Predicted_cluster', as_index=False).agg({\"Investment\": \"sum\"})\n\n\n#for cluster_n in range(k_mean_model.n_clusters):\n#   pred_stock_portfolio_df.loc[len(pred_stock_portfolio_df)]= [cluster_n,1]\n    \n\n#pred_stock_portfolio_df =pred_stock_portfolio_df.groupby('Predicted_cluster', as_index=False).agg({\"Investment\": \"sum\"})\n#pred_stock_portfolio_df\n\npred_stock_portfolio_grouped_df","c0775ae5":"# Pie chart, to show distribution of investment per cluster \nlabels = pred_stock_portfolio_grouped_df['Predicted_cluster']\nsizes = pred_stock_portfolio_grouped_df['Investment']\n\n\nfig1, ax1 = plt.subplots(figsize=(8, 8))\npatches, texts, autotexts = ax1.pie(sizes,  labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\n\nplt.legend(patches, labels, loc=\"best\")\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nax1.set_title(\"Investment diversification per Cluster\")\n\nplt.show()","95463663":"#Taking Investors input regarding the new investment she\/he wants to make \n\nstock_customer = input(\"Enter the stock ticker customer is interested in buying(we will use clustering) ?\") or 'AAPL'\nprint(stock_customer)","811829b1":"customer_stock_model,stock_modified = linearModel(stock_customer)\ncustomer_stock_class_pred = knn.predict([customer_stock_model.coef_])","4c766636":"print(\"Class Prediction for Investor's new Stock\",customer_stock_class_pred)","25d687e4":"customer_stock_model,stock_modified = linearModel(stock_customer)\n\n#print(customer_stock_model.coef_)\n\ncustomer_stock_class_pred = k_mean_model.predict([customer_stock_model.coef_])\n\n","d64aefd5":"print(\"Cluster for Investor's new Stock:\",stock_customer, \" is :\",customer_stock_class_pred[0])","47b910f3":"#### 3.1.1 Normalize the data and execute Linear Regression","e1fbe558":"#### 3.2.1 Standardize the data and execute Linear Regression","c5c19401":"### 2. EDA (Exploratory Data Analysis)","83a27de2":" ##### Let's see graphical representation of the distribution of one stock","7054ba22":"#### 3.2.5 Standardize the data and execute Ridge Regression","e6d2c275":"## Contents:\n\n   #### 1. Grab and load the data\n           1.1 Data and features description\n           1.2 Load pre downloaded CSV\n  #### 2. EDA (Exploratory Data Analysis)\n  #### 3. Regression Model for Stock representation\n              3.1 Baseline Model for Normalization & Regression \n                  3.1.1 Normalize the data and execute Linear Regression\n                  3.1.2 Normalize the data and execute Linear SVR\n                  3.1.3 Normalize the data and execute RBF SVR\n                  3.1.4 Normalize the data and execute Decision Tree Regressor\n           3.2 Baseline Model for Standardization & Regression\n                  3.2.1 Standardize the data and execute Linear SVR\n                  3.2.2 Standardize the data and execute RBF SVR\n                  3.2.3 Standardize the data and execute Decision Tree Regression\n                  3.2.4 Standardize the data and execute Ridge Regression\n                  3.2.5 Standardize the data and execute Lasso Regression\n              3.3 Fine Tune Model\n                  3.3.1 Linear Regression using Cross Validation\n                  3.3.2 Fine tune Decision Tree\n                  3.3.3 Fine tune SVM Regression using Random Search\n   #### 4 Metrics \n                4.1 RMSE for Regularization\n                4.2  MAE for Regression\n                4.3 RMSE for Regression\n                4.4 RMSE for All stocks \n   #### 5  Statistics Significance test \n   #### 6  Classification Model of Stocks \n                6.1 KNN Classification of Stock Movement\n   #### 7 Clustering Model of Stocks \n                7.1 K-Means model using K-mean++ for deciding number of clusters \n  #### 8. Success \/ Failure analysis\n \n  #### 9. User interaction and usecases ","199d25c0":"#### 1.1 Data and Features Description","a6e1512c":"### 8. Success \/ Failure Analysis","80760c39":"> ### All features open, close, high, low, volume, price & change from previous day are important and contributed to the model","0677f1ab":"#### 7.1 K-Means model using K-mean++ for deciding number of clusters ","dafe6bd2":"### 3.3 Fine Tune Model","98eb2c47":"#### 3.2.4 Standardize the data and execute Decision Tree Regression","af1721ba":"##### Check the distribution of data to understand variance in features to plan for scaling","6f60c92f":"  ### 6 . Classification Model of Stocks ","04f2b364":"#### 3.3.1 Linear Regression using Cross Validation","56944c74":"#### 3.1.3 Normalize the data and execute RBF SVR","80a9904a":"### 1. Grab and load the data","d577f5ec":"\n**Project Team :**\n\nThis project is executed by a group of 3 members. Below are the members names:\n                         \n    * Rohan Kayan                    2000331378\n    * Sathish Nandakumar             2000319482\n    * Khaled Abou Samak              2000277173","7aa8bb84":"#### 6.1 KNN Classification of Stocks based on the linear model coefficients ","b62847cd":"The data is presented 5 years of stock data for 500 S&P stocks. The dataset is taken from Kaggle public dataset (https:\/\/www.kaggle.com\/camnugent\/sandp500).\n\n\nAll stocks has following columns: \n* Date - in format: yy-mm-dd \n* Open - price of the stock at market open (this is NYSE data so all in USD)\n* High - Highest price reached in the day\n* Low Close - Lowest price reached in the day\n* Volume - Number of shares traded\n* Name - the stock's ticker name","75a0f828":"#### 4.3 RMSE for Regression\n#####  RMSE for all of the models (including Regularization , Standardization and normalization) ","aa195218":"##### Split the data into train and testing using Sklearn","baa16539":"##### Building the correlation matirix to know the correlation bewtween close price(target) and the other features","b31973bc":"#### 4.2  MAE for Regression\n##### We tried Mean Absolute Error to understand how the metrics look but we use RMSE for the actual accuracy measurement of our model","6d7492ef":"##### Create heldout dataset by splitting the AAPL stock into train and test using Scikit-Learn's train_test_split. Also remove the 'date' & 'name' features from the dataset as it has least significance in prediction day close stock price. Also remove the target feature close from input features and add it to target feature to measure prediction accuracy","f209e00a":"####  Finding best model to predict closing price for Stocks \n            *We removed the features 'Name' & 'Date' as the initial testing with these features but after removing these features the model performance has greatly increated \n            *We used regression model to predict the close day stock price, here we tried different methods to pre-process the data by scaling and \n            normalizing\n            \n            * Normalization is not giving us a better result , as we are training the linear model per stock , using each stock's 5 year of trading data . Results shows it clearly that Standardization is a better fit for us.\n            * Based on the RMSE values in the below table, Linear Regression, SVR with Linear kernel performed good with Standardization, but SVR with RBF kernel, Decision Tree performed bad with both standardization and normalization\n            * We tried Ridge and Lasso regularization using cross validation to experiment, how the model performs with penalty, these regularizations performed better than SVR with RBF and Decision Tree, however it didn't outperform the Linear Regression with standardization.\n            * We did tuning of the model for Linear Regression, SVR with Linear and RBF kernels and Decision Tree regressor, the performance for SVR has improved with Linear kernel after tuning, but Deicsion Tree and Linear regrssion didn't show much difference, however the overall performance on both train and test dataset was better in Linear Regression\n            * We performed the Statistics significance test for top 2 models i.e. Linear Regression model and Linear SVR , to find out whether they have significant difference in their performance . And result showed that Linear Regression model is significantly better than linear SVR. \n           * We started with validation of all the models with just 1 stock , then we improved our logic to validate all the model combinations for all the stocks . This validated our assumption that Linear regression with Standardization is the best fitted model for our data.\n####  Classification and clustering \n    *We started with homegrown target class assignment and training classification model using that data , and we kept the # of classes as 3 . This model had many limitations. Then we improved our modelling to use Clustering , this way there was no need to assign stocks pre defined classes and # of cluster were also not limited to 3 . \n    *We used K-mean ++ to decide # of clusters \n     \n\n         ","cc21400e":"#### 3.2.3 Standardize the data and execute RBF SVR[](http:\/\/)","a18fb763":"#### Using clustering  model  to predict cluster of the new stock","9e629b9f":"### 3.1 Baseline Model for Normalization & Regression","b643bd22":"#### 5  Statistics Significance test ","8a6aa790":"### Feature Importance","d55bccfd":"##### Checking number of stock records and features for a specific stock 'AAPL'","c7387499":"Run prediction using different models which were fitted above and calculate metrics in loop for the list of stocks . This will give use the name of the model giving best RMSE. If we check the result for most of the stocks Linear regression with Standardization is the best model . So for our clustering and classification we will use Linear Regresssion model with Standardization.\n","1721deb8":"## Project Name: Stock Portfolio Diversification using multi-layered machine learning approach \n\n**Introduction:** Stock portfolio deversification is very important for risk mitigation , many times  investor thinks that she\/he is diversifying the portfolio , but in actual she\/he is putting money in the similar type of stocks, increasing the risk . Machine learning could be very useful tool here to diversify the portfolio using historical information of stocks . We are proposing a multi layered machine learning approach which will provide a flexible and extensible model to fit and cluster stock portfolio and help investor to diversify .   \n\n![image.png](attachment:image.png)\nFollowing are the 3 high level tasks\n   - Fit Stock for given trade and other historical infromation , and come up with effective model to represent the stock\n   - Assign a class\/cluster to stocks based on the model learned in task 1 \n   - Investor input of his\/her existing portfolio and the new investment ,he\/she is interested in, and use of clustering model to diversify \n   \n**Task #1: Fit Stock for given trade and other historical infromation , and come up with effective model to represent the stock**  -  In this task , we have treding data for S&P 500 stocks and for every stock, we are evaluating multiple regression models with hyper parameter tuning to find a best fit model. \n\n**Task #2 Clustering of stocks** - In this task, training data is the feature coefficients of the regression models from Task#1 , i.e. 500 stocks * 6 features coefficients . Using this training data , we are trying to classify\/cluster stocks in different similarity groups.\n\n**Task#3 Taking investor's portfolio details and the new stock , he\/she is interested in** - In this task , we are going to use the clustering and regression model which we learned in Task #1 & #2.  For investor's current portfolio , we will find out the clusters with most of the investments and if the new stock also belongs to the cluster with high coverage , we will suggest to invest in some other stock otherwise will give a go ahead. ","eebf2a74":"#### 9  User interaction and usecases \n#### Task #3 - Interaction with investor and usages of Linear model and clustering models ","9e4c2176":"##### We landed on the second section of our project, here we will use the Linear model feature coefficients of all S&P 500 stocks and their target classes(homegrown logic for assigning the class) for training and validation of classififcation model . Once classification model is ready , this model will be used to predict the class of  investor's stock and by this way the model will help investor to diversify his\/her portfolio.\n","dd1125a1":"#### 3.3.2 Fine tune Decision Tree","3ba74471":"#### 3.1.4 Normalize the data and execute Decision Tree Regressor","b164f7f6":"#### 4 Metrics \n\n#### 4.1 RMSE for Regularization\nCalculated RMSE for Lasso and Ridge separately as it was ran thorugh the Cross Validation. The RMSE didn't outperform Linear Regression","1afddfd9":" ### 7 . Clustering Model of Stocks \n\nEarlier we manually assigned classes to the training data and used classification model to predict the class of test data. \nNow we are using unsupervised learning (Clustering Method) to split stocks in similar clusters and will use cluster model to predict the cluster for the investor's  stock. \n\n","99c5c7bd":"#### 3.2.2 Standardize the data and execute Linear SVR","91db772e":"##### Variations in day close price of the stock AAPL","fa1bad2f":"##### Check for null values to plan for pre-processing of data","405b5327":"### 3.2 Baseline Model for Standardization & Regression","ccd80e8f":"#### 1.2 Load pre-downloaded csv file","8a990075":"#### 3.1.2 Normalize the data and execute Linear SVR ","c24bb294":" #### 3.3.3 Fine tune SVM Regression using Random Search","49260495":"##### Adding the new features to understand the price variations of stock within a day and from previous day, these additional features will help in predicting the day close stock price with atmost accuracy as well as these features helps the model to classsify the volatility of a stock","6a5c5cb0":"##### Heap map for the feature correlation","e49365d6":"#### 3.2.6 Standardize the data and execute Lasso Regression","71c85e3e":"#### Using classification  model  to predict class of the new stock","ff7c3cf4":"#####  Fit and Predict using KNN classifier with 3 nearest neighbhors","302dfc6d":"  #### 4.4 RMSE for All stocks \n  ","b81d4e46":"#### 3. Regression model for stock representation"}}