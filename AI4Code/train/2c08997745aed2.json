{"cell_type":{"1068b486":"code","d4a8a3bf":"code","affd6754":"code","f4c5a600":"code","8573e2cc":"code","99a447de":"code","fa153c19":"code","3f7fe3b6":"code","50f5c5c3":"code","291b29f4":"code","f7e33fa6":"code","275e2fc9":"code","7b16fd7c":"code","af506edd":"code","fd36eb35":"code","11963904":"code","3b975830":"code","a095a545":"code","696dd6fd":"code","48dbef80":"code","8913e745":"code","592478e8":"markdown","65797f1a":"markdown","5b49a647":"markdown","216e2792":"markdown","528a0836":"markdown","9d688a6d":"markdown","fce45dd0":"markdown"},"source":{"1068b486":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)'\nimport matplotlib.pyplot as plt\nfrom tensorflow.python.keras.applications import ResNet50\nfrom tensorflow.python.keras.applications import VGG16\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\nfrom tensorflow.python.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.utils import class_weight\nfrom tensorflow.python.keras import optimizers\n\n\nimport cv2\nimport math\nfrom IPython.display import clear_output\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/fire-detection-from-cctv\/data\/data\/img_data\"))\nprint(os.listdir(\"..\/input\/fire-detection-from-cctv\/data\/data\/video_data\/test_videos\"))\nprint(os.listdir(\"..\/input\"))\n\nprint(os.listdir(\"..\/input\/resnet50\"))\nprint(os.listdir(\"..\/input\/vgg16\"))\n\n# Any results you write to the current directory are saved as output.","d4a8a3bf":"IMG_SIZE = 224\nNUM_EPOCHS = 20\nNUM_CLASSES = 3\nTRAIN_BATCH_SIZE = 77\nTEST_BATCH_SIZE = 1 \n","affd6754":"def create_model( model_size ):\n    my_new_model = Sequential()\n    if  model_size == 'L':\n        resnet_weights_path = '..\/input\/resnet50\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        resnet = ResNet50(include_top=False, pooling='avg', weights=resnet_weights_path)\n        #resnet.summary()\n        my_new_model.add(resnet)\n        my_new_model.layers[0].trainable = False\n    else:\n        vgg_weights_path = '..\/input\/vgg16\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        vgg= VGG16(include_top=False, weights=vgg_weights_path ) \n        vgg.summary()\n        my_new_model.add(vgg)\n        my_new_model.add(GlobalAveragePooling2D())\n        my_new_model.layers[0].trainable = False\n        my_new_model.layers[1].trainable = False\n        \n    my_new_model.add(Dense(NUM_CLASSES, activation='softmax'))\n   \n    # Say no to train first layer (ResNet) model. It is already trained\n    \n    opt = optimizers.adam()\n    my_new_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return my_new_model","f4c5a600":"def train_model( model ):\n    #ata_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n\n    data_generator_with_aug = ImageDataGenerator(preprocessing_function=preprocess_input,\n                                width_shift_range=0.1,\n                                height_shift_range=0.1,\n                                #sear_range=0.01,\n                                zoom_range=[0.9, 1.25],\n                                horizontal_flip=True,\n                                vertical_flip=False,\n                                data_format='channels_last',\n                                brightness_range=[0.5, 1.5]\n                               )\n                                       \n    train_generator = data_generator_with_aug.flow_from_directory(\n            '..\/input\/fire-detection-from-cctv\/data\/data\/img_data\/train',\n            target_size=(IMG_SIZE, IMG_SIZE),\n            batch_size=TRAIN_BATCH_SIZE,\n            class_mode='categorical')\n    \n   \n    validation_generator = data_generator_with_aug.flow_from_directory(\n            '..\/input\/fire-detection-from-cctv\/data\/data\/img_data\/test',\n            target_size=(IMG_SIZE, IMG_SIZE),\n            batch_size=TEST_BATCH_SIZE,\n            shuffle = False,\n            class_mode='categorical')\n    \n        \n    #y_train = get_labels(train_generator)\n    #weights = class_weight.compute_class_weight('balanced',np.unique(y_train), y_train)\n    #dict_weights = { i: weights[i] for i in range(len(weights)) }\n       \n    H = model.fit_generator(\n            train_generator,\n            steps_per_epoch=train_generator.n\/TRAIN_BATCH_SIZE,\n            epochs=NUM_EPOCHS,\n            validation_data=validation_generator,\n            validation_steps=1 #,\n            #class_weight=dict_weights\n                )\n    \n    plot_history( H, NUM_EPOCHS )\n    \n    return model, train_generator,validation_generator","8573e2cc":"def get_label_dict(train_generator ):\n# Get label to class_id mapping\n    labels = (train_generator.class_indices)\n    label_dict = dict((v,k) for k,v in labels.items())\n    return  label_dict   ","99a447de":"def get_labels( generator ):\n    generator.reset()\n    labels = []\n    for i in range(len(generator)):\n        labels.extend(np.array(generator[i][1]) )\n    return np.argmax(labels, axis =1)","fa153c19":"def get_pred_labels( test_generator):\n    test_generator.reset()\n    pred_vec=model.predict_generator(test_generator,\n                                     steps=test_generator.n, #test_generator.batch_size\n                                     verbose=1)\n    return np.argmax( pred_vec, axis = 1), np.max(pred_vec, axis = 1)\n    ","3f7fe3b6":"def plot_history( H, NUM_EPOCHS ):\n    plt.style.use(\"ggplot\")\n    fig = plt.figure()\n    fig.set_size_inches(15, 5)\n    \n    fig.add_subplot(1, 3, 1)\n    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"loss\"], label=\"train_loss\")\n    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"val_loss\"], label=\"val_loss\")\n    plt.title(\"Training Loss and Validation Loss on Dataset\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss\")\n    plt.legend(loc=\"lower left\")\n\n    \n    fig.add_subplot(1, 3, 2)\n    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"loss\"], label=\"train_loss\")\n    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"acc\"], label=\"train_acc\")\n    plt.title(\"Training Loss and Accuracy on Dataset\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss\/Accuracy\")\n    plt.legend(loc=\"lower left\")\n    \n    fig.add_subplot(1, 3, 3)\n    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"val_loss\"], label=\"val_loss\")\n    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"val_acc\"], label=\"val_acc\")\n    plt.title(\"Validation Loss and Accuracy on Dataset\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss\/Accuracy\")\n    plt.legend(loc=\"lower left\")\n\n\n    plt.show()\n    #plt.savefig(\"plot.png\")\n    ","50f5c5c3":"def draw_prediction( frame, class_string ):\n    x_start = frame.shape[1] -600\n    cv2.putText(frame, class_string, (x_start, 75), cv2.FONT_HERSHEY_SIMPLEX, 2.5, (255, 0, 0), 2, cv2.LINE_AA)\n    return frame","291b29f4":"def prepare_image_for_prediction( img):\n   \n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    # The below function inserts an additional dimension at the axis position provided\n    img = np.expand_dims(img, axis=0)\n    # perform pre-processing that was done when resnet model was trained.\n    return preprocess_input(img)","f7e33fa6":"model = create_model('L')","275e2fc9":"trained_model_l, train_generator,validation_generator = train_model(model)\nlabel_dict_l = get_label_dict(train_generator )","7b16fd7c":"model = create_model('S')","af506edd":"trained_model_s, train_generator,validation_generator = train_model(model)\nlabel_dict_s = get_label_dict(train_generator)","fd36eb35":"def get_display_string(pred_class, label_dict):\n    txt = \"\"\n    for c, confidence in pred_class:\n        txt += label_dict[c]\n        if c :\n            txt += '['+ str(confidence) +']'\n    #print(\"count=\"+str(len(pred_class)) + \" txt:\" + txt)\n    return txt","11963904":"\ndef predict(  model, video_path, filename, label_dict ):\n    \n    vs = cv2.VideoCapture(video_path)\n    fps = math.floor(vs.get(cv2.CAP_PROP_FPS))\n    ret_val = True\n    writer = 0\n    \n    while True:\n        ret_val, frame = vs.read()\n        if not ret_val:\n            break\n       \n        resized_frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n        frame_for_pred = prepare_image_for_prediction( resized_frame )\n        pred_vec = model.predict(frame_for_pred)\n        #print(pred_vec)\n        pred_class =[]\n        confidence = np.round(pred_vec.max(),2) \n        \n        if confidence > 0.4:\n            pc = pred_vec.argmax()\n            pred_class.append( (pc, confidence) )\n        else:\n            pred_class.append( (0, 0) )\n        if pred_class:\n            txt = get_display_string(pred_class, label_dict)       \n            frame = draw_prediction( frame, txt )\n        #print(pred_class)\n        #plt.axis('off')\n        #plt.imshow(frame)\n        #plt.show()\n        #clear_output(wait = True)\n        if not writer:\n            fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n            writer = cv2.VideoWriter(filename, fourcc, fps,(frame.shape[1], frame.shape[0]), True)\n            \n        # write the out\n        writer.write(frame)\n        \n    vs.release()\n    writer.release()\n      \n   ","3b975830":"#test_lables = get_test_labels( validation_generator )\n#print(test_lables)","a095a545":"#pred_lables, confidence = get_pred_labels( validation_generator )\n#print( pred_lables )\n","696dd6fd":"video_path = '..\/input\/fire-detection-from-cctv\/data\/data\/video_data\/test_videos\/test1.mp4'\npredict ( trained_model_l, video_path, 'test1_9.avi',  label_dict_l) \n\nvideo_path = '..\/input\/fire-detection-from-cctv\/data\/data\/video_data\/test_videos\/test2.mp4'\npredict ( trained_model_l, video_path, 'test2_9.avi',  label_dict_l) \n\nvideo_path = '..\/input\/fire-detection-from-cctv\/data\/data\/video_data\/test_videos\/test3.mp4'\npredict ( trained_model_l, video_path, 'test3_9.avi',  label_dict_l) ","48dbef80":"from IPython.display import YouTubeVideo\nYouTubeVideo('cHlTG6WL0OY', width=800, height=450)\n","8913e745":"from IPython.display import YouTubeVideo\nYouTubeVideo('Lk7_qDy60CI', width=800, height=450)\n","592478e8":"## Introduction\nDetection of fire and smoke in CCTV footage by fire authorities can significantly increase the response time to such tragedies saving many lives. This kernel translates this task into an image regonition problem on sampled CCTV video frames. ","65797f1a":"## Data Preparation\nThe CCTV footage for training and testing is downloaded from publically available videos on Youtube. The training set is prepared by sampling the videos at a constant rate and manually distributing the sampled frames into 'default', 'fire' and 'smoke' categories. Frames are kept in directories named after the class to which  they belong.","5b49a647":"## Prediction\n\nPrediction involves sampling the test videos into frames and predicting the class probabilities in each sampled frame. The class with the maximum probbality is assigned to the frame. However, if the max probablility is less than 0.5, the frame is assigned to the default class.  The predicted class and the class probability is drawn on the frame before writing it into a separate video file using VideoWriter","216e2792":"## Training\n\nThe frames for training and testing are read from the directories using ImageDataGenerator.Data augmentation is performed by horizontally flipping each image by setting the horizontal_flip to True in the ImageDataGenerator.","528a0836":"## Results:\nBelow are the three test videos that were passed through the trained model. It can be seen that in case there is both fire and smoke, the model is biased towards predicting it as smoke. If we look at the training vs validtion accuracy, it is clear that the model is overfitting the training data. There is still some work that needs to be done on this.","9d688a6d":"## References:\n\n[1] https:\/\/www.pyimagesearch.com\/2018\/12\/24\/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial\/  \n[2] https:\/\/medium.com\/@vijayabhaskar96\/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720  \n[3] https:\/\/www.learnopencv.com\/read-write-and-display-a-video-using-opencv-cpp-python\/  \n[4] https:\/\/github.com\/bikz05\/ipython-notebooks\/blob\/master\/computer-vision\/displaying-video-in-ipython-notebook.ipynb\n","fce45dd0":"## Model Creation\n\nThe model is created by appending 'Dense' layer, with number of activation units equivalent to the number classes (3), to a pre-trained Reset50 model. This significantly reduces the training time.\n\n"}}