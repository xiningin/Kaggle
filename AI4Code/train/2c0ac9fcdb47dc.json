{"cell_type":{"6c6b5031":"code","2a105dfc":"code","727453a3":"code","68abe415":"code","3f7bdeeb":"code","415cafd1":"code","fbb13b6f":"code","f2e047b0":"code","499ff4ef":"code","2b09d442":"code","e51fb24c":"code","18f43989":"code","0c113307":"code","a2c4f1a1":"code","71d45f29":"code","575b69c6":"code","b886ff02":"code","acf67633":"code","58d7a1d1":"code","f371973b":"code","239c7973":"code","fb843ba6":"code","a24fb3a8":"code","02ce0ec6":"code","13e7c7a9":"code","d70cee96":"code","b7ec6d63":"code","19c21de3":"code","ec933914":"code","aa020fad":"code","93015c7d":"markdown","87cc6698":"markdown","a1720612":"markdown","9e885e04":"markdown","1a0b33cf":"markdown","280366d0":"markdown","e0e47f09":"markdown","f2d4871f":"markdown","dc8d6b41":"markdown","5bc690d7":"markdown","3641aa2a":"markdown","58409bfb":"markdown","02b90ad5":"markdown","bd56d2b6":"markdown","12cf258c":"markdown","8c57d915":"markdown","8b752d96":"markdown","7428b9b8":"markdown","89228a03":"markdown","41bc4363":"markdown","901615db":"markdown","50707c55":"markdown","fb69fdb5":"markdown","0c608c8d":"markdown","2a7ce4b3":"markdown"},"source":{"6c6b5031":"# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Modeling\nimport lightgbm as lgb\n\n# Evaluation of the model\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.size'] = 18\n%matplotlib inline\n\n# Governing choices for search\nN_FOLDS = 5\nMAX_EVALS = 5","2a105dfc":"features = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\n\n# Sample 16000 rows (10000 for training, 6000 for testing)\nfeatures = features.sample(n = 16000, random_state = 42)\n\n# Only numeric features\nfeatures = features.select_dtypes('number')\n\n# Extract the labels\nlabels = np.array(features['TARGET'].astype(np.int32)).reshape((-1, ))\nfeatures = features.drop(columns = ['TARGET', 'SK_ID_CURR'])\n\n# Split into training and testing data\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 6000, random_state = 42)\n\nprint('Train shape: ', train_features.shape)\nprint('Test shape: ', test_features.shape)\n\ntrain_features.head()","727453a3":"model = lgb.LGBMClassifier(random_state=50)\n\n# Training set\ntrain_set = lgb.Dataset(train_features, label = train_labels)\ntest_set = lgb.Dataset(test_features, label = test_labels)","68abe415":"# Default hyperparamters\nhyperparameters = model.get_params()\n\n# Using early stopping to determine number of estimators.\ndel hyperparameters['n_estimators']\n\n# Perform cross validation with early stopping\ncv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, metrics = 'auc', \n           early_stopping_rounds = 100, verbose_eval = False, seed = 42)\n\n# Highest score\nbest = cv_results['auc-mean'][-1]\n\n# Standard deviation of best score\nbest_std = cv_results['auc-stdv'][-1]\n\nprint('The maximium ROC AUC in cross validation was {:.5f} with std of {:.5f}.'.format(best, best_std))\nprint('The ideal number of iterations was {}.'.format(len(cv_results['auc-mean'])))","3f7bdeeb":"model.n_estimators = len(cv_results['auc-mean'])\n\nmodel.fit(train_features, train_labels)\npreds = model.predict_proba(test_features)[:, 1]\nbaseline_auc = roc_auc_score(test_labels, preds)\n\nprint('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))","415cafd1":"import csv\nfrom hyperopt import STATUS_OK\nfrom timeit import default_timer as timer\n\ndef objective(hyperparameters):\n    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization.\n       Writes a new line to `outfile` on every iteration\"\"\"\n    \n    # \u786e\u5b9a\u662f\u54ea\u4e00\u7ec4\u9a8c\u8bc1\u96c6\n    global ITERATION\n    \n    ITERATION += 1\n    \n    # \u4f7f\u7528 early stopping \u786e\u5b9a\u5f31\u5b66\u4e60\u5668\u7684\u6570\u91cf\n    if 'n_estimators' in hyperparameters:\n        del hyperparameters['n_estimators']\n    \n    # \u56e0\u4e3a\u8bbe\u5b9a\u53c2\u6570 \u4e0d\u540c\u7684boosting type\u9700\u8981\u4e0d\u540c\u7684subsample\u53c2\u6570\uff0c\u6240\u4ee5\u91cd\u65b0\u6784\u9020\u8c03\u53c2\u8303\u56f4\n    subsample = hyperparameters['boosting_type'].get('subsample', 1.0)\n    hyperparameters['boosting_type'] = hyperparameters['boosting_type']['boosting_type']\n    hyperparameters['subsample'] = subsample\n    \n    # \u786e\u8ba4\u6574\u6570\u53c2\u6570\u7684\u60c5\u51b5\n    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n        \n    start = timer()\n    \n    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, \n                        early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n\n    run_time = timer() - start\n    \n    best_score = cv_results['auc-mean'][-1]\n    \n    # \u635f\u5931\u51fd\u6570 \u8d8a\u5c0f\u8d8a\u597d\n    loss = 1 - best_score\n    \n    # \u8fd4\u56de\u6700\u4f18\u7ed3\u679c\u7684 Boosting \u8f6e\u6570\n    n_estimators = len(cv_results['auc-mean'])\n    \n    hyperparameters['n_estimators'] = n_estimators\n\n    # Write to the csv file ('a' means append)\n    of_connection = open(OUT_FILE, 'a')\n    writer = csv.writer(of_connection)\n    writer.writerow([loss, hyperparameters, ITERATION, run_time, best_score])\n    of_connection.close()\n    \n    # Dictionary with information for evaluation\n    return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n            'train_time': run_time, 'status': STATUS_OK}","fbb13b6f":"from hyperopt import hp\nfrom hyperopt.pyll.stochastic import sample","f2e047b0":"learning_rate = {'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2))}","499ff4ef":"learning_rate_dist = []\n\n# Draw 10000 samples from the learning rate domain\nfor _ in range(10000):\n    learning_rate_dist.append(sample(learning_rate)['learning_rate'])\n    \nplt.figure(figsize = (8, 6))\nsns.kdeplot(learning_rate_dist, color = 'red', linewidth = 2, shade = True);\nplt.title('Learning Rate Distribution', size = 18); plt.xlabel('Learning Rate', size = 16); plt.ylabel('Density', size = 16);","2b09d442":"num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\nnum_leaves_dist = []\n\nfor _ in range(10000):\n    num_leaves_dist.append(sample(num_leaves)['num_leaves'])\n    \nplt.figure(figsize = (8, 6))\nsns.kdeplot(num_leaves_dist, linewidth = 2, shade = True);\nplt.title('Number of Leaves Distribution', size = 18); plt.xlabel('Number of Leaves', size = 16); plt.ylabel('Density', size = 16);","e51fb24c":"boosting_type = {'boosting_type': hp.choice('boosting_type', \n                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('subsample', 0.5, 1)}, \n                                             {'boosting_type': 'dart', 'subsample': hp.uniform('subsample', 0.5, 1)},\n                                             {'boosting_type': 'goss', 'subsample': 1.0}])}\n\nhyperparams = sample(boosting_type)\nhyperparams","18f43989":"space = {\n    'boosting_type': hp.choice('boosting_type', \n                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n                                             {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n                                             {'boosting_type': 'goss', 'subsample': 1.0}]),\n    'num_leaves': hp.quniform('num_leaves', 20, 150, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n    'is_unbalance': hp.choice('is_unbalance', [True, False]),\n}","0c113307":"x = sample(space)\n\n# \u56e0\u4e3a\u4e4b\u524d\u5bf9 \u76f8\u4e92\u5f71\u54cd\u7684\u53c2\u6570\u8fdb\u884c\u4e86\u7279\u6b8a\u8bbe\u7f6e\uff0c\u73b0\u5728\u4e5f\u9700\u8981\u7279\u6b8a\u5904\u7406\u4e00\u4e0b\uff0c\u8ba9\u6574\u4e2a\u53c2\u6570\u5904\u4e8e\u4e00\u4e2a\u5c42\u7ea7\nsubsample = x['boosting_type'].get('subsample', 1.0)\nx['boosting_type'] = x['boosting_type']['boosting_type']\nx['subsample'] = subsample\n\nx","a2c4f1a1":"x = sample(space)\nsubsample = x['boosting_type'].get('subsample', 1.0)\nx['boosting_type'] = x['boosting_type']['boosting_type']\nx['subsample'] = subsample\nx","71d45f29":"# \u521b\u5efa\u4e00\u4e2a\u6587\u4ef6\uff0c\u7528\u6765\u5b58\u8c03\u53c2\u7ed3\u679c\nOUT_FILE = 'bayes_test.csv'\nof_connection = open(OUT_FILE, 'w')\nwriter = csv.writer(of_connection)\n\nITERATION = 0\n\n# \u5199\u5165\u5217\u540d\nheaders = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']\nwriter.writerow(headers)\nof_connection.close()\n\n# \u6d4b\u8bd5\u76ee\u6807\u51fd\u6570\nresults = objective(sample(space))\nprint('The cross validation loss = {:.5f}.'.format(results['loss']))\nprint('The optimal number of estimators was {}.'.format(results['hyperparameters']['n_estimators']))","575b69c6":"from hyperopt import tpe\n\n# Create the algorithm\ntpe_algorithm = tpe.suggest","b886ff02":"from hyperopt import Trials\n\n# Record results\ntrials = Trials()","acf67633":"OUT_FILE = 'bayes_test.csv'\nof_connection = open(OUT_FILE, 'w')\nwriter = csv.writer(of_connection)\n\nITERATION = 0\n\nheaders = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']\nwriter.writerow(headers)\nof_connection.close()","58d7a1d1":"from hyperopt import fmin","f371973b":"global  ITERATION\n\nITERATION = 0\n\n# \u8fd9\u4e00\u6b65\u5c31\u662f\u8c03\u53c2\u7684\u8fd0\u884c\u8fc7\u7a0b\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,\n            max_evals = MAX_EVALS)\n\nbest","239c7973":"results = pd.read_csv(OUT_FILE)","fb843ba6":"import ast\n\ndef evaluate(results, name):\n    \"\"\"evaluate \u51fd\u6570\u7528\u6765\u8bc4\u4f30\u6700\u4f73\u53c2\u6570\u7684\u8868\u73b0\n    \u8fd4\u56de\u7684\u7ed3\u679c\u662f \u5c06\u4e4b\u524d\u7528csv\u8bb0\u5f55\u7684\u7ed3\u679c\uff0c\u7ed3\u6784\u5316\u8fd4\u56de\uff0c\u65b9\u4fbf\u540e\u7eed\u7edf\u8ba1\u5206\u6790\u53c2\u6570\u7684\u5206\u5e03\"\"\"\n    \n    new_results = results.copy()\n    # \u4f7f\u7528ast.literal_eval str -> dic\n    new_results['hyperparameters'] = new_results['hyperparameters'].map(ast.literal_eval)\n    \n    # Sort with best values on top\n    new_results = new_results.sort_values('score', ascending = False).reset_index(drop = True)\n    \n    # \u6253\u5370\u6700\u9ad8\u5206\u6570\n    print('The highest cross validation score from {} was {:.5f} found on iteration {}.'.format(name, new_results.loc[0, 'score'], new_results.loc[0, 'iteration']))\n    \n    # \u4f7f\u7528\u6700\u4f73\u53c2\u6570\u5efa\u6a21\u8bad\u7ec3\uff0c\u8fd4\u56de\u5206\u6570\n    hyperparameters = new_results.loc[0, 'hyperparameters']\n    model = lgb.LGBMClassifier(**hyperparameters)\n    model.fit(train_features, train_labels)\n    preds = model.predict_proba(test_features)[:, 1]\n    \n    print('ROC AUC from {} on test data = {:.5f}.'.format(name, roc_auc_score(test_labels, preds)))\n    \n    # \u5c06dict\u5b58\u50a8\u7684\u53c2\u6570\u8f6c\u5316\u4e3a \u7ed3\u6784\u5316\u7684\u6570\u636e df\n    hyp_df = pd.DataFrame(columns = list(new_results.loc[0, 'hyperparameters'].keys()))\n\n    for i, hyp in enumerate(new_results['hyperparameters']):\n        hyp_df = hyp_df.append(pd.DataFrame(hyp, index = [0]), \n                               ignore_index = True)\n        \n    # \u589e\u52a0 iteration score \u4e24\u5217 \n    hyp_df['iteration'] = new_results['iteration']\n    hyp_df['score'] = new_results['score']\n    \n    return hyp_df","a24fb3a8":"bayes_results = evaluate(results, name = 'Bayesian')\nbayes_results","02ce0ec6":"bayes_results = pd.read_csv('..\/input\/home-credit-model-tuning\/bayesian_trials_1000.csv').sort_values('score', ascending = False).reset_index()\nrandom_results = pd.read_csv('..\/input\/home-credit-model-tuning\/random_search_trials_1000.csv').sort_values('score', ascending = False).reset_index()\nrandom_results['loss'] = 1 - random_results['score']\n\nbayes_params = evaluate(bayes_results, name = 'Bayesian')\nrandom_params = evaluate(random_results, name = 'random')","13e7c7a9":"# Dataframe of just scores\nscores = pd.DataFrame({'ROC AUC': random_params['score'], 'iteration': random_params['iteration'], 'search': 'Random'})\nscores = scores.append(pd.DataFrame({'ROC AUC': bayes_params['score'], 'iteration': bayes_params['iteration'], 'search': 'Bayesian'}))\n\nscores['ROC AUC'] = scores['ROC AUC'].astype(np.float32)\nscores['iteration'] = scores['iteration'].astype(np.int32)\n\nscores.head()# Dataframe of just scores\nscores = pd.DataFrame({'ROC AUC': random_params['score'], 'iteration': random_params['iteration'], 'search': 'Random'})\nscores = scores.append(pd.DataFrame({'ROC AUC': bayes_params['score'], 'iteration': bayes_params['iteration'], 'search': 'Bayesian'}))\n\nscores['ROC AUC'] = scores['ROC AUC'].astype(np.float32)\nscores['iteration'] = scores['iteration'].astype(np.int32)\n\nscores.head()","d70cee96":"best_random_params = random_params.iloc[random_params['score'].idxmax(), :].copy()\nbest_bayes_params = bayes_params.iloc[bayes_params['score'].idxmax(), :].copy()","b7ec6d63":"best_random_params","19c21de3":"# Plot of scores over the course of searching\nsns.lmplot('iteration', 'ROC AUC', hue = 'search', data = scores, size = 8);\nplt.scatter(best_bayes_params['iteration'], best_bayes_params['score'], marker = '*', s = 400, c = 'orange', edgecolor = 'k')\nplt.scatter(best_random_params['iteration'], best_random_params['score'], marker = '*', s = 400, c = 'blue', edgecolor = 'k')\nplt.xlabel('Iteration'); plt.ylabel('ROC AUC'); plt.title(\"Validation ROC AUC versus Iteration\");","ec933914":"fig, axs = plt.subplots(3, 1, figsize = (24, 22))\n\n# \u7b2c\u4e00\u5f20\u53c2\u6570\u7684\u5206\u5e03\nhyper = 'learning_rate'\n# sns.regplot('iteration', 'learning_rate', data = bayes_params, ax = axs[0])\n# axs[i].scatter(best_bayes_params['iteration'], best_bayes_params[hyper], marker = '*', s = 200, c = 'k')\n# axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n\nsns.kdeplot(learning_rate_dist, label='Sampling Distribution', linewidth=4, ax=axs[0])\nsns.kdeplot(random_params['learning_rate'], label='Random Search', linewidth=4, ax=axs[0])\nsns.kdeplot(bayes_params['learning_rate'], label='Bayes Optimization', linewidth=4, ax=axs[0])\naxs[0].vlines(x=best_random_params['learning_rate'], ymin=0.0, \n              ymax=20.0, linestyles='--', linewidth=4, colors=['orange', 'green'])\naxs[0].set(xlabel='Learning Rate', ylabel='Learning Rate', title = 'Bayes Optimization Search');\n\n# \u7b2c\u4e8c\u7ae0 \u8d1d\u53f6\u65af\u4f18\u5316\u7684\u65f6\u5e8f\u5206\u5e03\nsns.regplot('iteration', hyper, data = bayes_params, ax = axs[1])\naxs[1].scatter(best_bayes_params['iteration'], best_bayes_params[hyper], marker = '*', s = 200, c = 'k')\naxs[1].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = 'Bayes Optimization Search vs Random Search');\n\n# \u7b2c\u4e09\u7ae0 \u968f\u673a\u641c\u7d22\u7684\u65f6\u5e8f\u5206\u5e03\nsns.regplot('iteration', hyper, data=random_params, ax=axs[2])\naxs[2].scatter(best_random_params['iteration'], best_random_params[hyper], marker='*', s=300, c='k')\naxs[2].set(xlabel='Iteration', ylabel='{}'.format(hyper), title='Random Search')\n\nplt.show()","aa020fad":"hyper_list = ['colsample_bytree', \n              'min_child_samples', \n              'num_leaves',\n              'reg_alpha',\n              'reg_lambda',\n              'subsample_for_bin']\n\nvline_heights = [10, 0.01, 0.012, 2.6, 1.7, 0.000007]\n\nfor hyper, vheight in zip(hyper_list, vline_heights):\n        \n    fig, axs = plt.subplots(3, 1, figsize = (24, 22))\n\n    # \u7b2c\u4e00\u5f20\u53c2\u6570\u7684\u5206\u5e03\n    sns.kdeplot([sample(space[hyper]) for _ in range(1000)], label = 'Sampling Distribution', linewidth = 4, ax=axs[0])\n    sns.kdeplot(random_params[hyper], label='Random Search', linewidth=4, ax=axs[0])\n    sns.kdeplot(bayes_params[hyper], label='Bayes Optimization', linewidth=4, ax=axs[0])\n    axs[0].vlines(x=[best_bayes_params[hyper],best_random_params[hyper]], ymin=0.0, \n                  ymax=vheight, linestyles='--', linewidth=4, colors=['orange', 'green'])\n    axs[0].set(xlabel=hyper, ylabel='density', title = 'Bayes Optimization Search vs Random Search');\n\n    # \u7b2c\u4e8c\u7ae0 \u8d1d\u53f6\u65af\u4f18\u5316\u7684\u65f6\u5e8f\u5206\u5e03\n    sns.regplot('iteration', hyper, data = bayes_params, ax = axs[1])\n    axs[1].scatter(best_bayes_params['iteration'], best_bayes_params[hyper], marker = '*', s = 200, c = 'k')\n    axs[1].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = 'Bayes Optimization');\n\n    # \u7b2c\u4e09\u7ae0 \u968f\u673a\u641c\u7d22\u7684\u65f6\u5e8f\u5206\u5e03\n    sns.regplot('iteration', hyper, data=random_params, ax=axs[2])\n    axs[2].scatter(best_random_params['iteration'], best_random_params[hyper], marker='*', s=300, c='k')\n    axs[2].set(xlabel='Iteration', ylabel='{}'.format(hyper), title='Random Search')\n\n    plt.show()\n","93015c7d":"[\u5b98\u65b9\u6587\u6863\u4e2d\u5173\u4e8e\u8c03\u53c2\u8303\u56f4\u5404\u79cd\u8bbe\u7f6e\u7684\u65b9\u5f0f](https:\/\/github.com\/hyperopt\/hyperopt\/wiki\/FMin#21-parameter-expressions)\uff0c\u53ea\u662f\u4e0a\u9762\u4e24\u79cd\u662f\u7528\u7684\u6700\u591a\u7684\u3002","87cc6698":"\u548c\u539f\u6587\u4e0d\u540c\u6211\u5c1d\u8bd5\u5c06\u6bcf\u4e2a\u53c2\u6570\u7684 \u6982\u7387\u5206\u5e03\u548c\u65f6\u5e8f\u5206\u5e03\u653e\u5230\u4e00\u8d77\u6765\u5c55\u793a\uff0c\u6548\u679c\u53ef\u80fd\u6bd4\u539f\u6587\u6e05\u695a\u70b9\u3002","a1720612":"\u8fd9\u91cc\u8fd0\u884c\u53ef\u80fd\u6709\u70b9\u5c0f\u95ee\u9898\uff0c\u53ef\u4ee5\u53c2\u7167\u8fd9\u4e2a[\u89e3\u51b3](https:\/\/blog.csdn.net\/FontThrone\/article\/details\/79012616)","9e885e04":"#### 4 \u7ed3\u679c\u8bb0\u5f55","1a0b33cf":"\u53ef\u4ee5\u770b\u5230\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u524d40%\u5de6\u53f3\u7684\u641c\u7d22\u6a21\u5f0f\u548c\u968f\u673a\u641c\u7d22\u5f88\u76f8\u4f3c\uff0c\u5230\u4e86\u540e\u671f\u624d\u96c6\u4e2d\u5230\u53ef\u80fd\u5206\u6570\u8f83\u9ad8\u7684\u533a\u57df\u3002","280366d0":"- \u5176\u4ed6\u8c03\u53c2\u8303\u56f4\u7684\u5b8c\u6574\u8bbe\u5b9a","e0e47f09":"\u56e0\u4e3a\u5982\u679c\u8bbe\u5b9a\u4e86boosting_type\u4e3agoss\uff0csubsample\u5c31\u53ea\u80fd\u4e3a1\uff0c\u6240\u4ee5\u8981\u628a\u4e24\u4e2a\u53c2\u6570\u653e\u5230\u4e00\u8d77\u8bbe\u5b9a","f2d4871f":"ast.literal_eval \u662f\u5347\u7ea7\u7248\u7684 eval \uff0c\u80fd\u591f\u5c06\u5b57\u7b26\u4e32\u89e3\u6790\u4e3apython\u5bf9\u8c61\uff0c\u5b89\u5168\u6027\u6bd4 eval \u9ad8","dc8d6b41":"#### 3 \u8c03\u53c2\u7b97\u6cd5\u7684\u9009\u62e9 Optimization Algorithm","5bc690d7":"## \u540e\u7eed\n\n\u540e\u7eed\u7684\u4e00\u4e9b\u63a2\u7d22\u4e0d\u5217\u51fa\u6765\u4e86\uff1a\n- \u5728\u975e\u6570\u503c\u578b\u53c2\u6570\u4e0a\uff0c\u53c2\u6570\u503c\u8d1d\u53f6\u65af\u4f18\u5316\u8fc7\u7a0b\u4e2d\u4e5f\u6709\u96c6\u4e2d\u8d8b\u52bf\uff0c\u8fd9\u751a\u81f3\u53ef\u4ee5\u6307\u5bfc\u6211\u4eec\u8fdb\u884c GridSearch\u548cRandomSearch\n- Random \u5728\u6d4b\u8bd5\u96c6\u4e0a\u6548\u679c\u66f4\u597d\uff0c\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u9a8c\u8bc1\u96c6\u4e0a\u6548\u679c\u8f83\u597d\n- \u8fc1\u79fb\u5230\u6574\u4f53\u6570\u636e\u96c6\u4e0a\uff0c\u8d1d\u53f6\u65af\u4f18\u5316\u5c45\u7136\u8fbe\u5230\u4e86<https:\/\/www.kaggle.com\/jsaguiar\/updated-0-792-lb-lightgbm-with-simple-features>\u7ed9\u51fa\u7684 LB\u5206\u6570 0.792\uff0c\u8fc1\u79fb\u6548\u679c\u5c45\u7136\u8fd8\u4e0d\u9519\n\n## \u7ed3\u8bba\n\n- \u8d1d\u53f6\u65af\u4f18\u5316\u5728\u6d4b\u8bd5\u96c6\u4e0a\u6709\u66f4\u597d\u7684\u6cdb\u5316\u6548\u679c\n- \u76f8\u6bd4GridSearch\u548cRandomSearch\uff0c\u8d1d\u53f6\u65af\u4f18\u5316\u53ea\u9700\u8981\u66f4\u5c11\u7684\u8fed\u4ee3\u6b21\u6570\n\n\n","3641aa2a":"\u4f5c\u8005\u5bf9 \u8d1d\u53f6\u65af\u4f18\u5316 \u548c \u968f\u673a\u641c\u7d22\u7684\u54041000\u7ec4\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u66f4\u76f4\u89c2\u7684\u6bd4\u8f83\u4e24\u79cd\u8c03\u53c2\u65b9\u5f0f\u7684\u7279\u6027\u3002\n\n- \u53c2\u6570\u5206\u5e03\uff0c\u89c2\u5bdf\u53c2\u6570\u7684\u96c6\u4e2d\u8d8b\u52bf\n- \u65f6\u5e8f\u5206\u5e03\uff0c\u89c2\u5bdf\u968f\u7740\u7b97\u6cd5\u8fed\u4ee3\uff0c\u53c2\u6570\u662f\u5982\u4f55\u5206\u5e03\u7684\n\n\u539f\u4f5c\u8005\u662f\u5148\u540e\u8fdb\u884c\u53ef\u89c6\u5316\u7684\uff0c\u6211\u89c9\u5f97\u5bf9\u4e00\u4e2a\u53c2\u6570\u540c\u65f6\u8fdb\u884c\u4e24\u79cd\u53ef\u89c6\u5316\u53ef\u80fd\u66f4\u6709\u610f\u601d\u70b9\uff0c\u6211\u7528\u4f5c\u8005\u601d\u8def\u548c\u4ee3\u7801\u91cd\u65b0\u8bbe\u8ba1\u4e00\u4e0b","58409bfb":"- \u76f8\u4e92\u5f71\u54cd\u7684\u53c2\u6570\u5904\u7406","02b90ad5":"- \u5747\u5300\u5206\u5e03","bd56d2b6":"\u9009\u62e9\u8c03\u53c2\u7b97\u6cd5\uff0c\u5176\u5b9e\u5c31\u662f\u9009\u62e9\u4e86\u4e00\u79cd\u6784\u5efa\u51fd\u6570\u6a21\u578b\u7684\u65b9\u5f0f\u3002\u7136\u540e\u6839\u636e\u8fd9\u4e2a\u7b97\u6cd5\u53ef\u4ee5\u7b97\u51fa\u4e0b\u4e00\u6b65\u5c1d\u8bd5\u7684\u53c2\u6570\uff0c\u54ea\u4e2a\u540e\u9a8c\u6982\u7387\u6700\u5927\u3002\n\n\u4ee5\u4e0b\u662f\u4e24\u4e2a\u4f5c\u8005\u63a8\u8350\u7684\u6587\u6863\uff0c\u6211\u53cd\u6b63\u6682\u65f6\u6ca1\u770b\u3002\u3002\u3002\n\n[\u6280\u672f\u7ec6\u8282](https:\/\/papers.nips.cc\/paper\/4443-algorithms-for-hyper-parameter-optimization.pdf)\n\n[\u6982\u5ff5\u6027\u7684\u89e3\u91ca](https:\/\/towardsdatascience.com\/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)\uff0c\u8fd9\u4e2akernel\u6587\u6863\u4f5c\u8005\u81ea\u5df1\u7684\u535a\u6587","12cf258c":"#### 2 \u5b9a\u4e49\u641c\u5bfb\u8303\u56f4","8c57d915":"[Automated Model Tuning\n](https:\/\/www.kaggle.com\/willkoehrsen\/automated-model-tuning)\n\nThank Will Koehresn for the great spirit of sharing\n\n\u4e00\u5982\u65e2\u5f80\u7684\u5b66\u4e60\u4e00\u4e0bWill Koehresn\u7684\u4ee3\u7801\uff0cKaggle\u7684\u5206\u4eab\u7cbe\u795e\u5b9e\u5728\u592a\u597d\u4e86\uff0c\u5e0c\u671b\u4f2a\u6c49\u5316\u5de5\u4f5c\u771f\u7684\u5f88\u6e23\uff0c\u65e8\u5728\u7763\u4fc3\u81ea\u5df1\u5b66\u4e60\uff0c\u5404\u4f4d\u8fd8\u662f\u5c3d\u91cf\u770b\u539f\u6587\u3002\n\n\n## \u5173\u4e8e LightGBm\n\n\u68af\u5ea6\u63d0\u5347\u7b97\u6cd5\u6700\u8fd1\u6210\u4e3a\u4e86\u9876\u7ea7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e4b\u4e00\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7ed3\u6784\u5316\u6570\u636e\u3002\u7b80\u5355\u7684\u8bf4\u6a21\u578b\u7684\u601d\u8def\uff0c\u4e0d\u540c\u4e8e\u968f\u673a\u68ee\u6797\u7684\u5404\u4e2a\u5b50\u6a21\u578b\u4e4b\u95f4\u76f8\u4e92\u72ec\u7acb\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e5f\u76f8\u4e92\u5e73\u884c\uff0cGBM\u6bcf\u4e2a\u65b0\u7684\u5b50\u6a21\u578b\u90fd\u4f1a\u501f\u9274\u4e4b\u524d\u6a21\u578b\u7684\u9519\u8bef\u5206\u7c7b\uff0c\u6bcf\u6b21\u589e\u52a0\u7684\u5b50\u6a21\u578b\u90fd\u80fd\u591f\u6700\u5927\u964d\u4f4e\u6a21\u578b\u635f\u5931\u3002\n\n\u56e0\u4e3a\u6a21\u578b\u7ed3\u6784\u8f83\u4e3a\u590d\u6742\uff0c\u6240\u4ee5\u53c2\u6570\u4e5f\u5c31\u6bd4\u8f83\u591a\uff0c\u867d\u7136\u539fkernel\u57fa\u672c\u4e0a\u662f\u76f4\u63a5\u5bf9\u6240\u6709\u53c2\u6570\u8fdb\u884c\u641c\u7d22\uff0c\u4f46\u662f\u6211\u89c9\u5f97\u53ef\u4ee5\u53c2\u8003\u4e00\u4e0b[scikit-learn \u68af\u5ea6\u63d0\u5347\u6811(GBDT)\u8c03\u53c2\u5c0f\u7ed3](http:\/\/www.cnblogs.com\/pinard\/p\/6143927.html)\u7684\u8c03\u53c2\u601d\u8def,\u4e3b\u8981\u662f\u5904\u7406\u6846\u67b6\u53c2\u6570\uff0c\u548c\u5f31\u5b66\u4e60\u5668\u53c2\u6570\u7684\u4e0d\u540c\u7b56\u7565\u3002\n\n[\u53c2\u6570](http:\/\/lightgbm.apachecn.org\/cn\/latest\/Parameters.html), \u4e5f\u6709\u4e2d\u6587\u6587\u6863\u53ef\u4ee5\u8be6\u7ec6\u4e86\u89e3\u4e00\u4e0b\u76f8\u5173\u53c2\u6570\u542b\u4e49\u3002\n\n## \u8fdb\u884c\u8c03\u53c2\u7684\u56db\u4e2a\u6b65\u9aa4\n\n1. \u786e\u5b9a\u76ee\u6807\u51fd\u6570\n2. \u5b9a\u4e49\u641c\u5bfb\u8303\u56f4\n3. \u9009\u62e9\u8c03\u53c2\u7b97\u6cd5\n4. \u8f93\u51fa\u7ed3\u679c\n\n## \u8d1d\u53f6\u65af\u4f18\u5316\u8c03\u53c2\u521d\u6b65\n\n\u8d1d\u53f6\u65af\u4f18\u5316\u8c03\u53c2\u662f\u4e0d\u540c\u4e8eGridSearch\u548c\u968f\u673a\u8c03\u53c2\u7684\u53e6\u5916\u4e00\u79cd\u8c03\u53c2\u601d\u8def\uff0c\u6700\u91cd\u8981\u7684\u533a\u522b\u662f\u8d1d\u53f6\u65af\u4f18\u5316\u662f\u4e00\u79cd informed methods\uff0c\u4e5f\u5c31\u662f\u5229\u7528\u5df2\u7ecf\u5c1d\u8bd5\u8fc7\u7684\u8c03\u53c2\u7ed3\u679c\uff0c\u6765\u51b3\u5b9a\u4e0b\u4e00\u7ec4\u5c1d\u8bd5\u7684\u53c2\u6570\u3002\u8fd9\u6837\u7684\u597d\u5904\u662f\u662f\uff0c\u5728\u66f4\u6709\u53ef\u80fd\u5f97\u5230\u6700\u4f18\u7ed3\u679c\u7684\u533a\u57df\uff0c\u80fd\u591f\u8fdb\u884c\u66f4\u591a\u7684\u5c1d\u8bd5\u3002\n\n## \u5173\u4e8eHyperopt\n\n\u53ef\u4ee5\u8fdb\u884c\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u5f00\u6e90\u5e93\u6709\u5f88\u591a\uff0cSpearmint (\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b) and SMAC (\u57fa\u4e8e\u968f\u673a\u68ee\u6797)\u3002Hyperopt\u662f\u57fa\u4e8eTree Parzen Estimator\u7b97\u6cd5\u6784\u5efa\u4ee3\u66ff\u51fd\u6570\u6765\u9009\u62e9\u4e0b\u4e00\u4ee3\u7684\u53c2\u6570\u3002\u4f5c\u8005\u9009\u62e9\u8fd9\u4e2a\u662f\u56e0\u4e3a\u6587\u6863\u6bd4\u8f83\u5b8c\u6574\u3002\u60f3\u5c3d\u91cf\u628a\u7528\u8d77\u6765\uff0c\u6ca1\u6709\u7ea0\u7ed3\u7b97\u6cd5\u7ec6\u8282\uff0c\u4ee5\u540e\u6709\u7a7a\u770b\u770b\u3002\u3002\n\n## \u5f00\u59cb\u8bd5\u9a8c\n\n\u57fa\u672c\u7684\u5b9e\u9a8c\u8bbe\u5b9a\n\n- 5\u6298\u4ea4\u53c9\u9a8c\u8bc1\n- Early Stopping \u4e3a 100\n- \u9009\u53d610000\u4e2a\u6837\u672c\u4e3a\u8bad\u7ec3\u96c6\uff0c6000\u4e3a\u8bd5\u9a8c\u96c6\u8bb0\u6027\u5b9e\u9a8c\uff0c\u8fd9\u6837\u53ef\u4ee5\u5feb\u901f\u8fed\u4ee3\u5b9e\u9a8c\uff0c\u5b66\u4e60\u8c03\u53c2\u65b9\u6cd5\uff0c\u6700\u540e\u770b\u770b\u8c03\u597d\u7684\u53c2\u6570\u662f\u5426\u80fd\u591f\u8fc1\u79fb\u5230\u6574\u4e2a\u6570\u636e\u96c6\u4e0a\n\n\n\n\n\n\n\n\n\n","8b752d96":"#### 2 \u6784\u5efa\u76ee\u6807\u51fd\u6570","7428b9b8":"\u4ece\u8fd9\u5f20\u770b\u51fa\u6765\uff0c\u5176\u5b9e\u968f\u673a\u641c\u7d22\u7684\u6548\u679c\u771f\u7684\u5f88\u597d\uff0c\u5982\u679c\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\uff0c\u5728\u5c11\u91cf\u7684\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u5c31\u80fd\u5f97\u5230\u8f83\u597d\u7684\u53c2\u6570\uff0c\u4f46\u662f\u7531\u4e8e\u968f\u673a\u7684\u6548\u679c\uff0c\u968f\u7740\u8fed\u4ee3\u6570\u91cf\u7684\u589e\u52a0\uff0c\u4e0d\u80fd\u660e\u663e\u63d0\u5347\u5206\u6570\n\n\u8d1d\u53f6\u65af\u4f18\u5316\u53c2\u6570\u6548\u679c\u6709\u4e00\u4e2a\u7a33\u5b9a\u63d0\u5347\u7684\u8fdb\u7a0b\uff0c\u867d\u7136\u7406\u8bba\u4e0a\u53ef\u80fd\u662f\u9700\u8981\u8f83\u5c11\u7684\u641c\u7d22\u53ef\u4ee5\u5f97\u5230\u8f83\u597d\u7684\u6548\u679c\uff0c\u4f46\u8fd8\u662f\u9700\u8981\u4e00\u5b9a\u91cf\u7684\u8fed\u4ee3\u4e4b\u540e\u624d\u80fd\u53d1\u6325\u3002","89228a03":"**\u5bf9\u8c03\u53c2\u8303\u56f4\u8fdb\u884c\u62bd\u6837\u7684\u65b9\u5f0f**","41bc4363":"#### 1 \u7528\u539f\u59cb\u53c2\u6570\u8bad\u7ec3Baseline Model\uff0c\u7528\u6765\u6bd4\u8f83\u6a21\u578b\u8c03\u53c2\u7684\u6548\u679c","901615db":"## \u8c03\u53c2\u5b9e\u8df5","50707c55":"## \u641c\u7d22\u7ed3\u679c\u7684\u63a2\u7d22","fb69fdb5":"- log \u5747\u5300\u5206\u5e03","0c608c8d":"\u4ee5\u4e0a\u662f\u91c7\u6837\u4e00\u4e07\u4e2a\u5b66\u4e60\u7387\u7684\u5206\u5e03\u60c5\u51b5\uff0c\u56e0\u4e3a\u662flog\u5747\u5300\u5206\u5e03\uff0c\u6240\u4ee5\u5728\u8f83\u5c0f\u7684\u5730\u65b9\u5206\u5e03\u6bd4\u8f83\u5bc6\u96c6","2a7ce4b3":"Hyperopt \u63d0\u4f9b\u4e86\u8bb0\u5f55\u7ed3\u679c\u7684\u5de5\u5177\uff0c\u4f46\u662f\u6211\u4eec\u81ea\u5df1\u8bb0\u5f55\uff0c\u53ef\u4ee5\u65b9\u4fbf\u5b9e\u65f6\u76d1\u63a7\u3002"}}