{"cell_type":{"e48e5c88":"code","6ca4aa8b":"code","26330716":"code","9ecd66ff":"code","c796a917":"code","f1f552d9":"code","9cc8297f":"code","754f6f7c":"code","8d17c027":"code","214a244f":"code","bb4d5f20":"code","db7c1f3c":"code","6d4c73b6":"code","71de8389":"code","7ff222f5":"code","3208e962":"code","4c4b8ca5":"code","d7073254":"code","304b8ded":"code","a051e19f":"code","16a480ae":"markdown","68ca30f3":"markdown","82cae8b3":"markdown","dd621eca":"markdown","ed3db08c":"markdown","8551887d":"markdown","dbc9d078":"markdown","cd26659a":"markdown","3f07a5d1":"markdown","714473ea":"markdown","940dcfb6":"markdown","2b61ad55":"markdown","62cf16db":"markdown","2b40468c":"markdown","0975b040":"markdown","31b0aed5":"markdown","cf42957f":"markdown"},"source":{"e48e5c88":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.utils import resample,shuffle\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn import mixture\nfrom sklearn.metrics import recall_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import plot_precision_recall_curve\nimport optuna","6ca4aa8b":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","26330716":"data.shape","9ecd66ff":"data.info()","c796a917":"data['Class'].value_counts()","f1f552d9":"print('No Frauds', round(data['Class'].value_counts()[0]\/len(data) * 100,2), '% of all transactions')","9cc8297f":"corr = data.corr()\ncorr.style.apply(lambda x: [\"background:yellow\" if abs(v) > 0.2 and abs(v) < 0.5 and v!=1 else \"background:red\" if abs(v) > 0.5 and v!=1 else \"\" for v in x], axis = 1)","754f6f7c":"corr['Class'].sort_values(ascending=False).drop('Class').iloc[np.r_[0:5, -5:0]]","8d17c027":"y = data['Class']\nX = data.drop(['Class'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22)\ntrain_data = pd.concat([X_train, y_train], axis=1)","214a244f":"X_train[train_data['Class']==1].value_counts().sum()","bb4d5f20":"train_data_0 = train_data[train_data['Class']==0]\ntrain_data_1 = train_data[train_data['Class']==1]\ntrain_data_0_downsampled = resample(train_data_0, replace=True, n_samples=399, random_state=22)\ntrain_data_balanced = pd.concat([train_data_0_downsampled, train_data_1])\ntrain_data_balanced = shuffle(train_data_balanced)\ntrain_data_balanced.Class.value_counts()\nX_train = train_data_balanced.drop(['Class'], axis=1)\ny_train = train_data_balanced['Class']","db7c1f3c":"models = [XGBClassifier(random_state=0, n_jobs=-1, verbosity=0, use_label_encoder=False), LogisticRegression(), RandomForestClassifier()]\n\nfor model in models:\n    \n    print(type(model).__name__)\n    model.fit(X_train, y_train)\n    y_pred_prob = model.predict_proba(X_test)[:,1]\n    y_pred = model.predict(X_test)\n    \n    Conf_Mat = confusion_matrix(y_test, y_pred)\n    figs, axes = plt.subplots(1,2, figsize=(18,6))\n    labels = ['No Fraud', 'Fraud']\n    disp = plot_confusion_matrix(model, X_test, y_test, display_labels=labels, cmap=plt.cm.Reds, ax=axes[0])\n    \n    Class_rep = classification_report(y_test, y_pred)\n    print(Class_rep)\n    \n    fpr, tpr, Threshold = roc_curve(y_test, y_pred_prob)\n    plt.title('')\n    plt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc_score(y_test, y_pred))\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')\n    plt.xlim([-0.1,1.0])\n    plt.ylim([-0.1,1.01])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n    \n    #disp = plot_precision_recall_curve(model, X_test, y_test, ax=axes)\n\n    print('Accuracy score:', accuracy_score(y_test, y_pred).round(4))\n    print('AUC score:', roc_auc_score(y_test, y_pred).round(4))\n    print('Recall score:', recall_score(y_test, y_pred).round(4))\n    \n    print('\\n')\n    print('---' * 41)\n    print('\\n')","6d4c73b6":"import optuna\n\n    \ndef objective(trial):\n\n    param = {\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n    }\n    \n    model = xgb.XGBClassifier(**param)  \n    \n    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n\n    preds = model.predict(X_test)\n    \n    evaluation_score = roc_auc_score(y_test, preds)\n    \n    return evaluation_score","71de8389":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)","7ff222f5":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","3208e962":"study.trials_dataframe().head()","4c4b8ca5":"optuna.visualization.plot_optimization_history(study)","d7073254":"optuna.visualization.plot_param_importances(study)","304b8ded":"print('Best trial AOC ROC score:', study.best_value)","a051e19f":"model = XGBClassifier(**study.best_trial.params, n_jobs=-1, verbosity=0, use_label_encoder=False)\n\nmodel.fit(X_train, y_train)\ny_pred_prob = model.predict_proba(X_test)[:,1]\ny_pred = model.predict(X_test)\n    \nConf_Mat = confusion_matrix(y_test, y_pred)\nfigs, axes = plt.subplots(1,2, figsize=(18,6))\nlabels = ['No Fraud', 'Fraud']\ndisp = plot_confusion_matrix(model, X_test, y_test, display_labels=labels, cmap=plt.cm.Reds, ax=axes[0])\n    \nClass_rep = classification_report(y_test, y_pred)\nprint(Class_rep)\n    \nfpr, tpr, Threshold = roc_curve(y_test, y_pred_prob)\nplt.title('')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc_score(y_test, y_pred))\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n    \n#disp = plot_precision_recall_curve(model, X_test, y_test, ax=axes)\n\nprint('Accuracy score:', accuracy_score(y_test, y_pred).round(4))\nprint('AUC score:', roc_auc_score(y_test, y_pred).round(4))\nprint('Recall score:', recall_score(y_test, y_pred).round(4))","16a480ae":"### Introduction","68ca30f3":"### Importing libraries","82cae8b3":"### Importing data","dd621eca":"#### Precision\/Recall\nThe more precise our model is the less frauds it will detect. In this case recall is much more important for our model. It is much worse if our model have high number of false positives than a high number of false negatives. ","ed3db08c":"We will use Optuna for hyperparameter tuning. We will optimize hyperparameters to get a higher auc roc score.","8551887d":"We will test XGBoost, LogisticRegression and RandomForestClassifier. Using LogisticRegression we will get the highes aoc roc score. However it is only slightly higher than the score we get using XGBoost. Thus, we expect that XGBoost will outperform LogisticRegression when tuned.","dbc9d078":"There are no NULL values, thus we don't need to replace values.\nwe have highely imbalanced dataset. 99.83% of all transactions are non-fraud. We will deal with this using undersampling technique. Because if we use this dataframe our models will overfit since they will assume that most of transactions are non-fraud.","cd26659a":"First we split the data on train and test data. Then we determine how many transactions are fraud so we can bring the same number of non-fraud transactions. In this case this number is 399. Now we have a 50\/50 ration betweem fraud and non-fraud in our train data. Last this we have to do is to shuffle the data. Using downsampling method we have a great information loss which will decrease accuracy of our models. There are other methods for dealing with inbalaced datasets but in this kernel we will use downsampling.\n\nThe model shoud be tested on test data which has not been previusly balanced. ","3f07a5d1":"# Credit Card Fraud Detection","714473ea":"### Data Analysis ","940dcfb6":"### Spliting Data and Downsampling","2b61ad55":"### Correlations","62cf16db":"## Hyperparameter Tuning","2b40468c":"We increased the auc score from 0.954 to 0.9631 while our recall score stayed the same. We got a better result with XGBoost with hyperparameter tuning than with Logistic Regression.","0975b040":"In the table above fieds with absolute correlation over 0.2 are painted yellow and over 0.5 are painted red.\nWe see that many of the features are strongly correlated with the Amount, but we won't deal with that because we don't know anuynthing about those features.\nThe features with highest correlation with Class are V17, V14, V12, V10, V16, V11 and we espect that these features will have the most importance in predicting fraud.","31b0aed5":"In this kernel we will use varius models to prefict credit card fraud. Data features are scaled and their names are not shown, thus we won't be doing any data preprocessing. Due to highely imbalanced dataset we will use aoc roc score for evaluation. We will use Optuna to tune our model.","cf42957f":"## Modeling"}}