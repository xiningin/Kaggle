{"cell_type":{"f0ce2a37":"code","befaac45":"code","fc9327cd":"code","c43d37cb":"code","2feb062e":"code","28fc3293":"code","9a125059":"code","53497f4e":"markdown","aa2e7f96":"markdown","98bdc587":"markdown","c495df03":"markdown"},"source":{"f0ce2a37":"import numpy as np\nimport pandas as pd\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random","befaac45":"spy_prices = pd.read_csv('..\/input\/quantitative-trading\/SPY_2018.csv')\nspy_prices.head(3)","fc9327cd":"class DeepEvolutionStrategy:\n\n    inputs = None\n\n    def __init__(\n        self, weights, reward_function, population_size, sigma, learning_rate\n    ):\n        self.weights = weights\n        self.reward_function = reward_function\n        self.population_size = population_size\n        self.sigma = sigma\n        self.learning_rate = learning_rate\n\n    def _get_weight_from_population(self, weights, population):\n        weights_population = []\n        for index, i in enumerate(population):\n            jittered = self.sigma * i\n            weights_population.append(weights[index] + jittered)\n        return weights_population\n\n    def getWeights(self):\n        return self.weights\n\n    def train(self, epoch = 100, print_every = 1):\n        lasttime = time.time()\n        for i in range(epoch):\n            population = []\n            rewards = np.zeros(self.population_size)\n            for k in range(self.population_size):\n                x = []\n                for w in self.weights:\n                    x.append(np.random.randn(*w.shape))\n                population.append(x)\n            for k in range(self.population_size):\n                weights_population = self._get_weight_from_population(\n                    self.weights, population[k]\n                )\n                rewards[k] = self.reward_function(weights_population)\n            rewards = (rewards - np.mean(rewards)) \/ (np.std(rewards) + 1e-7)\n            for index, w in enumerate(self.weights):\n                A = np.array([p[index] for p in population])\n                self.weights[index] = (\n                    w\n                    + self.learning_rate\n                    \/ (self.population_size * self.sigma)\n                    * np.dot(A.T, rewards).T\n                )\n            if (i + 1) % print_every == 0:\n                print(\n                    f'\u8a13\u7df4\u9031\u671f {i + 1}. \u6700\u7d42\u734e\u52f5\uff1a{self.reward_function(self.weights)}'\n                )\n        print('=====================================')\n        print(f'\u8a13\u7df4\u6642\u9593\uff1a{time.time() - lasttime} \u79d2')\n\n\nclass Model:\n    def __init__(self, input_size, layer_size, output_size):\n        self.weights = [\n            np.random.randn(input_size, layer_size),\n            np.random.randn(layer_size, output_size),\n            np.random.randn(1, layer_size),\n        ]\n\n    def predict(self, inputs):\n        feed = np.dot(inputs, self.weights[0]) + self.weights[-1]\n        decision = np.dot(feed, self.weights[1])\n        return decision\n\n    def getWeights(self):\n        return self.weights\n\n    def set_weights(self, weights):\n        self.weights = weights","c43d37cb":"class Agent:\n\n    POPULATION_SIZE = 15\n    SIGMA = 0.1\n    LEARNING_RATE = 0.03\n\n    def __init__(self, model, window_size, trend, skip, initial_money):\n        self.model = model\n        self.window_size = window_size\n        self.half_window = window_size \/\/ 2\n        self.trend = trend\n        self.skip = skip\n        self.initial_money = initial_money\n        self.es = DeepEvolutionStrategy(\n            self.model.getWeights(),\n            self.getReward,\n            self.POPULATION_SIZE,\n            self.SIGMA,\n            self.LEARNING_RATE,\n        )\n\n    def act(self, sequence):\n        decision = self.model.predict(np.array(sequence))\n        return np.argmax(decision[0])\n    \n    def getState(self, t):\n        window_size = self.window_size + 1\n        d = t - window_size + 1\n        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n        res = []\n        for i in range(window_size - 1):\n            res.append(block[i + 1] - block[i])\n        return np.array([res])\n\n    def getReward(self, weights):\n        initial_money = self.initial_money\n        starting_money = initial_money\n        self.model.weights = weights\n        state = self.getState(0)\n        inventory = []\n        quantity = 0\n        for t in range(0, len(self.trend) - 1, self.skip):\n            action = self.act(state)\n            next_state = self.getState(t + 1)\n            \n            if action == 1 and starting_money >= self.trend[t]:\n                inventory.append(self.trend[t])\n                starting_money -= close[t]\n                \n            elif action == 2 and len(inventory):\n                bought_price = inventory.pop(0)\n                starting_money += self.trend[t]\n\n            state = next_state\n        return ((starting_money - initial_money) \/ initial_money) * 100\n\n    def fit(self, iterations, checkpoint):\n        self.es.train(iterations, print_every = checkpoint)\n\n    def buy(self):\n        initial_money = self.initial_money\n        state = self.getState(0)\n        starting_money = initial_money\n        states_sell = []\n        states_buy = []\n        inventory = []\n        for t in range(0, len(self.trend) - 1, self.skip):\n            action = self.act(state)\n            next_state = self.getState(t + 1)\n            \n            if action == 1 and initial_money >= self.trend[t]:\n                inventory.append(self.trend[t])\n                initial_money -= self.trend[t]\n                states_buy.append(t)\n                print('\u7b2c %s \u65e5 > \u8cfc\u8cb7 1 \u80a1 \u50f9\u683c %s\uff0c\u7e3d\u8cc7\u7522 %s' % (\n                    str(t).rjust(3),\n                    str(\"%.5f\" % self.trend[t]).rjust(10),\n                    str(\"%.3f\" % initial_money).rjust(10)\n                ))\n            \n            elif action == 2 and len(inventory):\n                bought_price = inventory.pop(0)\n                initial_money += self.trend[t]\n                states_sell.append(t)\n                try:\n                    invest = ((close[t] - bought_price) \/ bought_price) * 100\n                except:\n                    invest = 0\n                print('\u7b2c %s \u65e5 > \u8ce3\u51fa 1 \u80a1 \u50f9\u683c %s\uff0c\u7e3d\u8cc7\u7522 %s > \u7372\u5229 %s %%' % (\n                    str(t).rjust(3),\n                    str(\"%.5f\" % close[t]).rjust(10),\n                    str(\"%.3f\" % initial_money).rjust(10),\n                    str(\"%.2f\" % invest).rjust(5)\n                ))\n            state = next_state\n\n        invest = ((initial_money - starting_money) \/ starting_money) * 100\n        total_gains = initial_money - starting_money\n        return states_buy, states_sell, total_gains, invest","2feb062e":"close = spy_prices.Close.values.tolist()\nwindow_size = 30\nskip = 1\ninitial_money = 10000\n\nmodel = Model(input_size = window_size, layer_size = 500, output_size = 3)\nagent = Agent(model = model, \n              window_size = window_size,\n              trend = close,\n              skip = skip,\n              initial_money = initial_money)\nagent.fit(iterations = 500, checkpoint = 100)","28fc3293":"states_buy, states_sell, total_gains, invest = agent.buy()","9a125059":"fig = plt.figure(figsize=(15, 5))\nplt.plot(close, color='r', lw=2.)\nplt.plot(close, '^', markersize=10, color='m', label = '\u8cb7\u5165\u4fe1\u865f', markevery = states_buy)\nplt.plot(close, 'v', markersize=10, color='k', label = '\u8ce3\u51fa\u4fe1\u865f', markevery = states_sell)\nplt.title(f'\u7e3d\u7372\u5229 {total_gains}\uff0c\u6295\u8cc7\u5831\u916c\u7387 {invest} %')\nplt.legend()\nplt.show()","53497f4e":"\u6a5f\u5668\u5b78\u7fd2\u7684\u65b9\u5f0f\u53ef\u4ee5\u5927\u81f4\u5206\u6210\uff1a\u76e3\u7763\u5f0f\u5b78\u7fd2\u3001\u975e\u76e3\u7763\u5f0f\u5b78\u7fd2\u3001\u5f37\u5316\u5b78\u7fd2\n\n\u524d\u9762\u4ecb\u7d39\u4e86\u6c7a\u7b56\u6a39\u6a21\u578b\u3001\u5faa\u74b0\u795e\u7d93\u7db2\u8def\u7b49\u76e3\u7763\u5f0f\u5b78\u7fd2\u7684\u6a21\u578b\uff0c\u90fd\u662f\u900f\u904e\u5df2\u77e5\u50f9\u683c\u53bb\u4fee\u6b63\u9810\u6e2c\u50f9\u683c\uff0c\u8b93\u6a21\u578b\u66f4\u7cbe\u6e96\u3002\n\n\u800c\u5f37\u5316\u5b78\u7fd2\u7684\u4e0d\u540c\u4e4b\u8655\u70ba\uff1a\n\n- \u8a08\u7b97\u5f0f\u88e1\u9762\u6c92\u6709\u76e3\u7763\u8a0a\u865f\uff0c\u4e5f\u6c92\u6709 Label (\u5df2\u77e5\u50f9\u683c)\u3002\u53ea\u6709 Reward (\u53cd\u994b)\n- \u53cd\u994b\u6709\u5ef6\u6642\uff0c\u4e0d\u662f\u80fd\u7acb\u5373\u53cd\u6620\n- \u8a13\u7df4\u6642\u7684\u8f38\u5165\u8207\u6642\u5e8f\u76f8\u95dc\n- Agent(\u6c7a\u7b56\u8005) \u57f7\u884c\u7684\u52d5\u4f5c\u6703\u5f71\u97ff\u4e4b\u5f8c\u7684\u8cc7\u6599\n\n\u6211\u5011\u73fe\u5728\u8981\u505a\u7684\u662f\uff1a\n\n- \u91dd\u5c0d\u4e00\u500b\u5177\u9ad4\u554f\u984c (\u8df3\u7684\u6108\u9060\u6108\u597d) \u5f97\u5230\u4e00\u500b\u6700\u4f73\u7684\u7b56\u7565 (\u8df3\u8e8d\u7684\u6642\u6a5f\u9ede)\n- \u4f7f\u5f97\u5728\u8a72\u7b56\u7565\u4e0b\u7372\u5f97\u7684\u56de\u5831 (\u5206\u6578) \u6700\u5927\n- \u9019\u88e1\u7684\u7b56\u7565\u5176\u5be6\u5c31\u662f\u4e00\u7cfb\u5217\u7684 Action (\u5e8f\u5217\u8cc7\u6599)","aa2e7f96":"### \u7528\u4e0a\u6700\u57fa\u672c\u7684\u904b\u7b97\u67b6\u69cb\uff0c\u9084\u6c92\u7528\u4e0aGPU\u4e26\u884c\u904b\u7b97\uff0c\u4e0b\u500b\u90e8\u4efd\u5c07\u8abf\u7528 TF-Agents \u5b8c\u6210","98bdc587":"## \u89c0\u5bdf Agent \u7e3e\u6548","c495df03":"## \u4f7f\u7528\u5f37\u5316\u5b78\u7fd2\u6a21\u64ec\u4ea4\u6613\n\n[\u53c3\u8003\u7b97\u6cd5](https:\/\/github.com\/SaAPro\/agent-trading-deep-evolution-strategy)"}}