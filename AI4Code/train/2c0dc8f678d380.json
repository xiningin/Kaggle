{"cell_type":{"5c30a4c0":"code","c2af4557":"code","cf343092":"code","ccd57b58":"code","a3f5f8ca":"code","2fe5a0c9":"code","a29b4621":"code","d0932373":"code","59d73d8e":"code","ac87a11b":"code","4d1411c3":"code","b2e54d8a":"code","feda1b06":"code","b2957564":"code","8851c414":"code","0843a592":"code","994db072":"markdown","be1f489f":"markdown","f6d5b913":"markdown","ae620c4a":"markdown","7fd5ae5e":"markdown","54dca500":"markdown","40ac76af":"markdown","6a6e1135":"markdown","213ae4d4":"markdown","7d2b474e":"markdown","c9b4db54":"markdown","0bc51bea":"markdown"},"source":{"5c30a4c0":"# Import libraries\n\nimport pandas as pd\nimport numpy as np","c2af4557":"# Import data\n\nsample = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\nevaluation = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\n\n# Break into the training and validation set\n\ndf_train = evaluation.iloc[:, :-28]\ndf_valid = evaluation.iloc[:, -28:]","cf343092":"df_train.head()","ccd57b58":"# Melt the dataframe\n\nd_cols = ['d_' + str(i + 1) for i in range(1913)]\n\nts_df = pd.melt(frame = df_train, \n                     id_vars = ['id', 'item_id', 'cat_id', 'store_id'],\n                     var_name = 'd',\n                     value_vars = d_cols,\n                     value_name = 'sales')","a3f5f8ca":"# Aggregate total sales by store\n\nstore_level = ts_df[['store_id', 'sales']].groupby('store_id').agg('sum')\n\n# Aggregate total sales by store and item\n\nstore_item_level = ts_df[['store_id', 'item_id', 'sales']].groupby(['store_id', 'item_id']).agg('sum')\n\nstore_level = store_level.reset_index()\nstore_item_level = store_item_level.reset_index()","2fe5a0c9":"# For each store, figure out how much each item contributes to overall sales\n\ntotals = store_level.merge(store_item_level, on = 'store_id', how = 'left')\ntotals['ratio'] = totals.apply(lambda row: row['sales_y'] \/ row['sales_x'], axis = 1)\n\n# Make two copies of this dataframe, which will u\n\ntotals['id'] = totals['item_id'] + '_' + totals['store_id'] + '_' + 'evaluation'","a29b4621":"# Make two copies of the dataframe\ntotals1 = totals.copy()\ntotals2 = totals.copy()\n\n# Add an 'id' column\ntotals1['id'] = totals1['item_id'] + '_' + totals1['store_id'] + '_' + 'evaluation'\ntotals2['id'] = totals2['item_id'] + '_' + totals2['store_id'] + '_' + 'validation'\n\n# Concatenate vertically\ntotals = pd.concat([totals2, totals1], axis = 0)\ntotals = totals.reset_index().drop('index', axis = 1)\n\njoin_df = sample.merge(totals, on = 'id', how = 'inner')","d0932373":"# Get a list of all the stores and all the items\nstores = df_train['store_id'].unique()\nitems = df_train['item_id'].unique()\n\n# Get a list of the days we're predicting for\nf_cols = ['F' + str(i + 1) for i in range(28)]","59d73d8e":"from statsmodels.tsa.arima_model import ARIMA\nimport matplotlib.pyplot as plt","ac87a11b":"%%capture \n# surpress output from fitting ARMA models\n\n# Make an ARMA model for each store\n\nfor store in stores:\n    \n    print('starting store: ' + store)\n    \n    # Generate a time series by grouping the sales data by store\n    store_data = ts_df[ts_df['store_id'] == store]\n    store_data_sales = store_data[['d', 'sales']].groupby('d').agg(sum)\n    store_data_sales = store_data_sales.reset_index()\n    store_data_sales['d'] = store_data_sales['d'].apply(lambda x: int(x.split('_')[1]))\n    store_data_sales = store_data_sales.sort_values('d')\n    ts = store_data_sales['sales']\n    ts.index = store_data_sales['d']\n\n    # Make an ARMA model\n    if store in ['WI_1', 'WI_3']:\n        order = (7, 0, 2)\n    elif store == 'WI_2':\n        order = (6, 0, 2)\n    else:\n        order = (10, 0, 2)\n    \n    model = ARIMA(ts[-100:], order = order)\n    results_AR = model.fit(disp = -1)\n\n    # b holds the predictions. Note that we predict for 56 additional days (28 validation and 28 evaluation)\n    a = results_AR.predict(start = 0, end = len(ts[-100:]) + 56 - 2)\n    b = a[-56:]\n    b.index = ts[-56:].index + 56 \n    \n    # Add the predictions to our dataframe that we'll ultimately use for submission\n    df = join_df[(join_df['store_id'] == store) & (join_df['id'].str.contains('validation'))]\n    \n    for i, row in df.iterrows():\n        print('working on row ' + str(i))\n        join_df.loc[i, f_cols] = [b.values[j]*row['ratio'] for j in range(28)]\n        join_df.loc[i + 30490, f_cols] = [b.values[j + 28]*row['ratio'] for j in range(28)]\n        \n# Supress output (from ARMA model generation)\n;","4d1411c3":"from typing import Union\nfrom tqdm.notebook import tqdm_notebook as tqdm\n\n## evaluation metric\n## from https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/133834 and edited to get scores at all levels\nclass WRMSSEEvaluator(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 0  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight \/ lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score \/ scale).map(np.sqrt)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        group_ids = []\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n            group_ids.append(group_id)\n            all_scores.append(lv_scores.sum())\n\n        return group_ids, all_scores  ","b2e54d8a":"## public LB rank\ndef get_lb_rank(score):\n    \"\"\"\n    Get rank on public LB as of 2020-05-31 23:59:59\n    \"\"\"\n    df_lb = pd.read_csv(\"..\/input\/m5-accuracy-final-public-lb\/m5-forecasting-accuracy-publicleaderboard-rank.csv\")\n\n    return (df_lb.Score <= score).sum() + 1","feda1b06":"## reading data\ndf_calendar = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/calendar.csv\")\ndf_prices = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sell_prices.csv\")\ndf_sample_submission = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sample_submission.csv\")\ndf_sample_submission[\"order\"] = range(df_sample_submission.shape[0])\n\nevaluator = WRMSSEEvaluator(df_train, df_valid, df_calendar, df_prices)","b2957564":"# The validation predictions are in the first 30490 rows of our prediction dataframe\n\npreds_valid = join_df.loc[:30490,].drop(['store_id', 'sales_x', 'item_id', 'sales_y', 'ratio'], axis = 1)\npreds_valid = preds_valid[preds_valid.id.str.contains(\"validation\")]\npreds_valid = preds_valid.merge(df_sample_submission[[\"id\", \"order\"]], on = \"id\").sort_values(\"order\").drop([\"id\", \"order\"], axis = 1).reset_index(drop = True)\npreds_valid.rename(columns = {\n    \"F1\": \"d_1914\", \"F2\": \"d_1915\", \"F3\": \"d_1916\", \"F4\": \"d_1917\", \"F5\": \"d_1918\", \"F6\": \"d_1919\", \"F7\": \"d_1920\",\n    \"F8\": \"d_1921\", \"F9\": \"d_1922\", \"F10\": \"d_1923\", \"F11\": \"d_1924\", \"F12\": \"d_1925\", \"F13\": \"d_1926\", \"F14\": \"d_1927\",\n    \"F15\": \"d_1928\", \"F16\": \"d_1929\", \"F17\": \"d_1930\", \"F18\": \"d_1931\", \"F19\": \"d_1932\", \"F20\": \"d_1933\", \"F21\": \"d_1934\",\n    \"F22\": \"d_1935\", \"F23\": \"d_1936\", \"F24\": \"d_1937\", \"F25\": \"d_1938\", \"F26\": \"d_1939\", \"F27\": \"d_1940\", \"F28\": \"d_1941\"\n}, inplace = True)\n","8851c414":"## evaluating random submission\ngroups, scores = evaluator.score(preds_valid)\n\nscore_public_lb = np.mean(scores)\nscore_public_rank = get_lb_rank(score_public_lb)\n\nfor i in range(len(groups)):\n    print(f\"Score for group {groups[i]}: {round(scores[i], 5)}\")\n\nprint(f\"\\nPublic LB Score: {round(score_public_lb, 5)}\")\nprint(f\"Public LB Rank: {score_public_rank}\")","0843a592":"submission = join_df.drop(['store_id', 'sales_x', 'item_id', 'sales_y', 'ratio'], axis = 1)\nsubmission.to_csv('submission.csv', index = False)","994db072":"Melting the dataframe will make it so days are rows, rather than columns. It is needed later when sales are aggregated by stores.","be1f489f":"## 2. For each store, find how much each item contributes to overall sales","f6d5b913":"The code below is taken from the notebook above, and the evaluation metric was eventually defined here: https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/133834","ae620c4a":"### Thanks for looking at my kernel! I didn't have as much time as I would have liked to for this one, but I tried to get something together quickly :)","7fd5ae5e":"# Concept: model each store as a time series, then divide the store predictions by items. \n\nAfter looking at the data a bit, it looks like item sales are random. Sales at the store level, however, show seasonality, and look more like time series. Can we make predictions at the store level and then break those into item level based on each item's overall sales?\n\nNote that by taking the time series approach, there's a lot of data I *didn't* utilize (price, events, etc.). So it would be an interesting next step to see how those variables could be included.","54dca500":"## 5. Submit predictions","40ac76af":"### Outline:\n1. Import, melt data\n2. For each store, find the ratio at which each item contributes to overall sales\n3. Build an ARMA model for each store and decompose predictions based on (2)\n4. Utilize the notebook recently posted by Vopani (https:\/\/www.kaggle.com\/rohanrao\/m5-how-to-get-your-public-lb-score-rank) to estimate public leaderboard score\n5. Submit predictions","6a6e1135":"## 3. Build an ARMA model for each store and decompose predictions based on (2)","213ae4d4":"This is where the magic happens!\n\nThe orders of the ARMA model are hard-coded, found through experimenting. This notebook isn't about the theory of time series as much as the approach of building a larger time series and decomposing it, so I'll leave out the details of experimenting.\n\nDo note that it takes a minute to run. Commenting out the '%%capture' at the top of the cell will give status updates.","7d2b474e":"## 1. Import data","c9b4db54":"Now we'll build a dataframe that we'll ultimately use for the submission file","0bc51bea":"## 4. Utilize the notebook recently posted by Vopani (https:\/\/www.kaggle.com\/rohanrao\/m5-how-to-get-your-public-lb-score-rank) to estimate public leaderboard score"}}