{"cell_type":{"e961870e":"code","12b49ea1":"code","57b95998":"code","bb9d7c0d":"code","55165f40":"code","e4c1553b":"code","5c75c613":"code","4c11998c":"code","b476981b":"code","6be29726":"code","862150be":"code","7ae2cdb7":"code","df41cb23":"code","d0433c36":"code","49fc2b6a":"code","d764140c":"code","e2bc4106":"code","cc69e805":"code","fe51d819":"code","cb907f12":"code","8bd9dea0":"code","02e78987":"code","420001d2":"code","a8af9386":"code","f690f2cb":"markdown","b33f876d":"markdown","561e4d7e":"markdown","7770e2c6":"markdown","0c2d9606":"markdown","5781ce28":"markdown","83e8a770":"markdown","d17fa7f1":"markdown","64fd607a":"markdown","0b44b438":"markdown","e7dd6974":"markdown","926b3c17":"markdown","35b9a550":"markdown","44ee6676":"markdown","ad7a8ec5":"markdown","6d43035b":"markdown","e14647ec":"markdown","d0910b70":"markdown","24af2b62":"markdown","a0364ec8":"markdown","4e48871b":"markdown","57ca0400":"markdown","5bdfd1e1":"markdown","ff5e1881":"markdown","32096e6f":"markdown","a64ef7d7":"markdown","e7ec3ede":"markdown","82681def":"markdown","48e7b790":"markdown","8942136c":"markdown","e4981863":"markdown","acba467d":"markdown","e800a49a":"markdown","3e4ee5f8":"markdown","571afd62":"markdown","bfe7cf6c":"markdown","9a9c9894":"markdown","1b947669":"markdown","c00ce6ff":"markdown"},"source":{"e961870e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\n#load packages\nimport sys #access to system parameters https:\/\/docs.python.org\/3\/library\/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\n#misc libraries\nimport random\nimport time\n\n\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","12b49ea1":"#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import autocorrelation_plot\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","57b95998":"#Loading the single csv file to a variable named 'customer'\ncustomer=pd.read_csv(\"..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv\")","bb9d7c0d":"#Lets look at a glimpse of table\ncustomer.head()","55165f40":"print (\"The shape of the  data is (row, column):\"+ str(customer.shape))\nprint (customer.info())","e4c1553b":"#Looking at the datatypes of each factor\ncustomer.dtypes","5c75c613":"import missingno as msno \nmsno.matrix(customer)","4c11998c":"print('Data columns with null values:',customer.isnull().sum(), sep = '\\n')","b476981b":"#Donut Chart\nlabels = ['Male','Female']\nsizes = customer['Gender'].value_counts()\ncolors = plt.cm.magma(np.linspace(0, 1, 5))\n\n\nplt.rcParams['figure.figsize'] = (9, 9)\nplt.pie(sizes, labels = labels, colors = colors, shadow = True,autopct='%1.0f%%', \n        pctdistance=1.1,labeldistance=1.2,startangle=90)\n\n#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n#Lets reveal\nplt.title('Gender Distribution', fontsize = 20)\nplt.legend()\nplt.show()","6be29726":"sns.distplot(customer['Age'])","862150be":"fig,ax = plt.subplots(figsize=(10,7))\nsns.violinplot(x='Gender', y='Annual Income (k$)',split=True,data=customer)","7ae2cdb7":"sns.pairplot(customer,vars = ['Spending Score (1-100)', 'Annual Income (k$)', 'Age'], hue=\"Gender\")","df41cb23":"sns.set(style=\"ticks\")\ny= customer['Spending Score (1-100)']\nx = customer['Annual Income (k$)']\nsns.jointplot(x, y, kind=\"hex\", color=\"#4CB391\")","d0433c36":"cust_new = customer.drop('CustomerID', 1)\nsns.heatmap(cust_new.corr(),annot=True,fmt='.1g',cmap='Greys')","49fc2b6a":"plt.scatter(customer['Annual Income (k$)'],customer['Spending Score (1-100)'])\nplt.xlabel('Annual Income')\nplt.ylabel('Spending Score')","d764140c":"from sklearn.cluster import KMeans\n#Creating a copy of the dataset\nx=customer.drop(customer.loc[:,'CustomerID':'Age'].columns, axis = 1) \n# Create an object (which we would call kmeans)\n# The number in the brackets is K, or the number of clusters we are aiming for, lets divide into half first\nkmeans = KMeans(2)\n# Fit the data\nkmeans.fit(x)","e2bc4106":"# Create a copy of the input data\nclusters = x.copy()\n# predicted clusters \nclusters['cluster_pred']=kmeans.fit_predict(x)","cc69e805":"plt.style.use('default')\nplt.scatter(clusters['Annual Income (k$)'],clusters['Spending Score (1-100)'],c=clusters['cluster_pred'],cmap='rainbow')\nplt.xlabel('Annual Income')\nplt.ylabel('Spending Score')","fe51d819":"# Import a library which can do that easily\nfrom sklearn import preprocessing\n# Scale the inputs\n# preprocessing.scale scales each variable (column in x) with respect to itself\n# The new result is an array\nx_scaled = preprocessing.scale(x)\nx_scaled","cb907f12":"# Createa an empty list\nwcss =[]\n\n# Create all possible cluster solutions with a loop\n# We have chosen to get solutions from 1 to 9 clusters; you can ammend that if you wish\nfor i in range(1,10):\n    # Clsuter solution with i clusters\n    kmeans = KMeans(i)\n    # Fit the STANDARDIZED data\n    kmeans.fit(x_scaled)\n    # Append the WCSS for the iteration\n    wcss.append(kmeans.inertia_)\n    \n# Check the result\nwcss","8bd9dea0":"# Plot the number of clusters vs WCSS\nplt.plot(range(1,10),wcss)\n# Name your axes\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')","02e78987":"# Fiddle with K (the number of clusters)\nkmeans_new = KMeans(5)\n# Fit the data\nkmeans_new.fit(x_scaled)\n# Create a new data frame with the predicted clusters\nclusters_new = x.copy()\nclusters_new['cluster_pred'] = kmeans_new.fit_predict(x_scaled)","420001d2":"# Check if everything seems right\nclusters_new","a8af9386":"# Final Segments\nfig, ax = plt.subplots()\nscatter=ax.scatter(clusters_new['Annual Income (k$)'],clusters_new['Spending Score (1-100)'],c=clusters_new['cluster_pred'],cmap='rainbow')\nlegend1 = ax.legend(*scatter.legend_elements(),\n                    loc=\"top right\", title=\"Segments\")\nax.add_artist(legend1)\nplt.xlabel('Annual Income ')\nplt.ylabel('Spending Score')","f690f2cb":"## 3e.Checking for missing data\nDatasets in the real world are often messy, However, this dataset is almost clean and simple. Lets analyze and see what we have here.","b33f876d":"# 2.Kernel Goals\nThere are three primary goals of this kernel.\n\n1. Do a **exploratory analysis** of the mall customer segmentation dataset\n2. Do an **visualization analysis **of the mall customer segmentation dataset\n3. **Segmentation**: What are the segment which lies within the customer data and how are we going to treat them ?","561e4d7e":"Now, let's see how the features are looking by creating some visualizations.","7770e2c6":"After loading the dataset we can see a number of things. These **5 columns** provide a  rich amount of information for deep data exploration we can do on this dataset. We have **3 numerical data and 1 categorical data** (Excluding customer ID as it is of no use)","0c2d9606":"## Clustering\nLets drop ID, Age and Gender which aint going to serve purpose and fit the model","5781ce28":"Woof! We have nearly **200 records** and **4 factors** ","83e8a770":"**Inference:**\n* People who's annual income around 75k dollars have high footprints, must be pretty cozy mall\n* Male have an upperhand by occupying 150k dollar annual income.\n","d17fa7f1":"## 4c. Annual Income based on gender - Violin Plot","64fd607a":"As you see we have **0 null records**, lets get the visual party started!","0b44b438":"# 1. Introduction\nTo become more profitable, you need to be able to differentiate your customers to more effectively satisfy the needs of the **different segments**. But how do you segment your customers ? The answer lies in this notebook !\n\nEven if you have a highly-targeted customer demographic in your business, there are still variations between individual customers. Recognising these differences will allow you to tailor your approach to the needs of varying customer segments and allow you to effectively serve a wider group of people.\n\n## What is Customer Segmentation?\n![Segmentation-2.png](attachment:Segmentation-2.png)\nCustomer segmentation is the practice of **dividing a customer base into groups of individuals that are similar in specific ways**. You can provide different value propositions to different customer groups. Customer segments are usually **determined on similarities**, such as personal characteristics, preferences or behaviours that should correlate with the same behaviours that drive customer profitability.\n\nA customer segmentation model allows for the** effective allocation of marketing resources** and the maximisation of cross and up-selling opportunities. When a group of customers is sent an email that is specific to their needs, it\u2019s easier for companies to send those customers special offers.\n\nOther benefits of customer segmentation include staying a step ahead of the competition and identifying new products that existing or potential customers could be interested in.\n\nCredits: https:\/\/www.visma.com\/blog\/customer-segmentation-important\/","e7dd6974":"## Before we Begin:\nIf you liked my work, please upvote this kernel since it will keep me motivated to perform more in-depth reserach towards further datasets and produce more accurate models","926b3c17":"## 3c.Extracting data","35b9a550":"**Let's visualize the two cluster **","44ee6676":"## 4e.Spending vs Income- Hexplot ","ad7a8ec5":"# 3. Importing libraries and exploring Data\n## 3a.Importing Libraries\n\nPython is a fantastic language with a vibrant community that produces many amazing libraries. I am not a big fan of importing everything at once for the newcomers. So, I am going to introduce a few necessary libraries for now, and as we go on, we will keep unboxing new libraries when it seems appropriate","6d43035b":"**Inference:**\n* We can see till 5 clusters, the wcss drops drastically and the drop rate becomes slow after 5 clusters\n* Considering 5 clusters as threshold we can proceed with segmenting into **5 clusters**","e14647ec":"## Visualizing the clusters","d0910b70":"**Inference:**\n* Age and Spending has **poor correlation**,which shows that spending has nothing to do with age\n* Age and Income has **slightly negative correlation**,which shows Age doesn't determine income \n* Income and Spending has positive correlation, but **not strongly positive** as it does not have linear relation","24af2b62":"**Assumptions:**\n* **Gender:** Let's have the general assumption that men footprints to the mall is higher than female.\n* **Age**: Age above 18 and below 60 would have footprints in the mall\n* **Annual Income**: Annual income higher than 15K dollars have footprints in the mall \n","a0364ec8":"## Standardizing the variables","4e48871b":"**Inference:**\n* Even though Men have higher income, Women's spending score is higher than Men\n* There is no linear relationship between Income and Spending \n\nLet's see the relationship between Income and Spending in different plot","57ca0400":"## 4d. Income vs Spending- Pair Plot","5bdfd1e1":"# 6. Elbow Method\nIn cluster analysis, the elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use.","ff5e1881":"# 4.Data Visualization\nLet us create some assumptions on the features of dataset and verify whether it is true.","32096e6f":"**Inference:**\n* The Age is **normally distributed**\n* Many footprints are between the age of **35-45**\n* Next to that interval **millenials(around 20's) are high**\n* People who aged **around 60 (Old people) are low**\n\nOur assumption of age above 18 and below 60 holds somewhat true. Yet there are few people from above 60 bracket","a64ef7d7":"# 5. Divide and Rule- K means\n\nLet's begin to segment and capture the customers, Before starting the process, lets visualise the scatter plot which we are going to segmentize","e7ec3ede":"**Inference:**\n* It has divided into higher **income and lower income customers**, The threshold is set near 60k by the model\n* Looks more like a mirror image, it didn't serve any meaning and **can't help in making strategies with just two customer segments**\n\nWe should increase the cluster count, we can use **WCSS( Within-Cluster-Sum-of-Squares) ELBOW METHOD** to identify the required clusters","82681def":"## 4e.Correlation- Heatmap ","48e7b790":"We are going to visualize the segments in this scatter plot","8942136c":"**From the customer dataset we can infer**\n* Gender is the only categorical variable\n* Annual income of individuals are given in 1000's\n*  Spending Score: It is the score(out of 100) given to a customer by the mall authorities, based on the money spent and the behavior of the customer.\n* We have Age and unique customer ID which are self explanatory ","e4981863":"## 4a. Percentage of Gender - Donut Chart","acba467d":"# Hit upvote if you like my work and also comment for suggestions to improve my notebook quality","e800a49a":"# 7. Customer Segmentation conclusion:\n\n### 1. Alien(Segment 4):\nThese customers are termed as Aliens. They are in the safe category, They get less income and less spending score. They have less possibility to become a customer. It is a waste of money to spend time targetting these customers. They mostly do window shopping and leave the mall. Even if we target our marketing ads to them, the max they can fall to is **Segment 2**\n### 2. Super Spender(Segment 2):\nThese customers are termed as Super Spender. They get more income and spend most of them. These segment can also be categorised as **Fans\/Loyal**. This segment doesn't require much marketing expenditure as they will be loyal always. They are highly reliable and probable to revisit the mall. We can provide loyalty cards, email marketing and provide information about the products to make them stay loyal to the seller \n### 3. Mainstream Mojo(Segment 1):\nThese customers are termed as Mainstream Mojo. This is the most potential segment we can focus to increase our Sales. We can increase our marketing strategies through emails, sms, social media marketing, limited time promotions, point based reward programs which can make a significant impact to move from **Segment 3 and Segment 4 ** and thereby increase our market size\n### 4. Spendthrift Sport (Segment 3):\nThese customers are termed as Spendthrift Sport. This segment have **high probability for retention**. If we fail to show any concern to them, they can fall into Segment 1 easily as they have low Income. Besides it's lucky for the mall as they are spending whatever they earn.\n### 5. Stingy Sage (Segment 0):\nThese customers are termed as Stingy Sage. This segment has a lot of income and **they never do spend anything**. Unless we provide some very high discount, they won't come to mall. Besides we can assume that these are online shoppers as this is a mall shop sales and it is a hard task to make them visit a mall unless we have some groundbreaking strategy.","3e4ee5f8":"**Inference:**\n* There is **no linear relationship** between two features\n* People who has annual income around **40-60k dollars** have higher spending score","571afd62":"**Inference:**\n* Percentage of male footprints-56%\n* Percentage of female footprints-44%\n\nAs we have assumed number of male footprints were higher but not very huge difference","bfe7cf6c":"## 3d. Examining the dataset","9a9c9894":"Woah. This data set seems to have **no missing values**,Phew. Now we don't need to clean or fill any NaN values, But lets confirm it numerically as well","1b947669":"## 3b.Importing Visualization and ML Libraries\n\nIt is important for an analysis to have data visualization and develop machine learning models to get accurate prediction. Here we are going use sklearn and matplotlib for machine learning and plotting respectively","c00ce6ff":"## 4b. Age - Distribution Plots"}}