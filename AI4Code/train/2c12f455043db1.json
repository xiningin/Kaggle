{"cell_type":{"0c10c523":"code","f37c32c8":"code","c089ddd5":"code","9e28e51f":"code","45650fba":"code","00da9f9d":"code","ccc83419":"code","97976506":"code","e9f3a5a8":"code","b8c281e1":"code","162ff62e":"code","d00b701a":"code","18cccb6c":"code","eae809a3":"code","9cdbc054":"code","629cb1f8":"code","c84f084f":"code","fefd731c":"code","77f56522":"code","6fd46ddb":"code","1affa92e":"code","3f731700":"code","f03e40ff":"code","2c596825":"code","ed40adb3":"code","3958b198":"code","3ac1c498":"code","40bc6107":"code","d4ba0108":"code","0074fec5":"code","3de072aa":"code","f1743701":"code","9ea7f0b8":"code","7bf16d6d":"code","ecda680d":"code","50189191":"code","e2d6764e":"code","b448cce4":"code","dbd1554b":"code","8b2e3bff":"code","2cf5abcb":"code","16dc007c":"code","ad87d05b":"code","3b8149b9":"code","2f06b1b7":"code","c3d0e753":"code","b585e93b":"code","5ff853ba":"code","83ff3c3c":"code","cbfe204b":"code","514be605":"markdown","d8adc981":"markdown","5e85b1c5":"markdown","af7ebb76":"markdown"},"source":{"0c10c523":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n\n\nfrom sklearn.metrics import classification_report,plot_confusion_matrix,plot_roc_curve,accuracy_score\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f37c32c8":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","c089ddd5":"# Step 1 calculated Null values\n# step 2 see numerical distribution and outliers\n# step 3 fill mean of age as per (train and test)\n# # Name,Ticket no , cabin , id Unique should be deleted after fillind embarked and fare","9e28e51f":"# Null\/ missing values report \n\ndef nanPercentage(data):\n    df = data.isnull().sum().reset_index()\n    df.columns = ['ColumnName','Total Missing']\n    df = df[df['Total Missing'] > 0]\n    df['Percentage'] = df['Total Missing']\/len(data)\n    df = pd.DataFrame(data = df)\n    df = df.sort_values('Total Missing',ascending=False)\n    return df\n\nprint('Train :',nanPercentage(train))\nprint('Test  : ',nanPercentage(test))\n","45650fba":"# Combining Train and test\ntr = train.copy()\nte = test.copy()\ndf = tr.drop('Survived',axis=1)\ndf = pd.concat([df,te])\ndf","00da9f9d":"# Variable Selection \n# We can drop PassengerID  == UNIQUE\n# pclass has a Reverse ordinal relation --- > use reverse weighting or OneHotEncoding\n# Name can be dropped \n# sex -- > One Hot Endcoding \n\ndef NumericSeperator(df):\n    Numerical_features = pd.DataFrame()\n    for i in df.columns:\n        if len(df[i].unique()) > 25:\n            Numerical_features[i] = df[i]\n    return Numerical_features\n\ndef CategoricSeperator(df):\n    Categorical_features = pd.DataFrame()\n    for i in df.columns:\n        if i not in Numerical_features.columns:\n            Categorical_features[i] = df[i]\n    \n    return Categorical_features\n\nNumerical_features = NumericSeperator(df)\nCategorical_features = CategoricSeperator(df)\n#Target = train['Survived']\n\n# Numerical_features_test = NumericSeperator(test)\n# Categorical_features_test = CategoricSeperator(test)","ccc83419":"def unique(df):\n    for i in df.columns:\n        print('{}\"s Unique values: {}'.format(i,df[i].unique()))\n        \nprint(unique(Categorical_features))","97976506":"df","e9f3a5a8":"Categorical_features","b8c281e1":"Numerical_features","162ff62e":"# Average age of people is between 18 - 32 approx \n# few were old 25 people (approx)\n# maximum young \nsns.histplot(data=Numerical_features,x='Age',kde=True)","d00b701a":"sns.heatmap(Numerical_features.corr(),annot=True)","18cccb6c":"# Overall Males were more on the ship\nsns.countplot(data = Categorical_features,x='Sex')","eae809a3":"plt.figure(figsize=(12,6))\nsns.histplot(Numerical_features['Fare'])","9cdbc054":"# Those with high Fare had booked multiple cabins (1,Pclass) (Upper Class)\n# Maximum of them had to pay between 0-100 (2,3 pclass) (Loq + middle class family)\n\ndf[df['Fare'] > 200]","629cb1f8":"# Most of them were low class but there we can see that higher class is more than middle\nsns.countplot(data = Categorical_features,x='Pclass')","c84f084f":"# Siblings and all \nsns.countplot(data = Categorical_features,x='SibSp')","fefd731c":"# Parent child\nsns.countplot(data = Categorical_features,x='Parch')","77f56522":"# C = Cherbourg, Q = Queenstown, S = Southampton\n# Most people were from Southampton\nsns.countplot(data = Categorical_features,x='Embarked')","6fd46ddb":"\"\"\"print(df[(df['Embarked']== 'S') & (df['Pclass']==1)].isnull().sum() )\nprint(df[(df['Embarked']== 'C') & (df['Pclass']==1)].isnull().sum() )\nprint(df[(df['Embarked']== 'Q') & (df['Pclass']==1)].isnull().sum() )\"\"\"\n\n# Here fill fare avg of s and pclass ==3 in 152\n\ndf[(df['Embarked'].isna()) | (df['Cabin'] == 'B28')  | (df['Fare'].isna()) | (df['Ticket'] == 3701)]\n\n","1affa92e":"sns.countplot(data=train,x='SibSp',hue='Survived')","3f731700":"sns.countplot(data=train,x='Parch',hue='Survived')","f03e40ff":"nanPercentage(df)","2c596825":"train = train.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)","ed40adb3":"test = test.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)","3958b198":"train = train[train['Embarked'].notna()]","3ac1c498":"test['Fare'] = test['Fare'].fillna(df[(df['Pclass']==3) & (df['Embarked']=='S')]['Fare'].mean())","40bc6107":"# using test + train data age mean as replacement of nan\ntrain['Age'] = train['Age'].fillna(value = Numerical_features['Age'].mean())\ntest['Age'] = test['Age'].fillna(value = Numerical_features['Age'].mean())","d4ba0108":"print(nanPercentage(train))\nprint(nanPercentage(test))","0074fec5":"\"\"\"train['Pclass'] = train['Pclass'].astype('str')\ntest['Pclass'] = test['Pclass'].astype('str')\n\ntrain['SibSp'] = train['SibSp'].astype('str')\ntest['SibSp'] = test['SibSp'].astype('str')\n\ntrain['Parch'] = train['Parch'].astype('str')\ntest['Parch'] = test['Parch'].astype('str')\"\"\"","3de072aa":"train['Parch'].unique(),test['Parch'].unique()","f1743701":"sns.countplot(data=test,x='Parch')","9ea7f0b8":"sns.countplot(data=train,x='Parch')","7bf16d6d":"# I have repeated steps with Min Max scaler , standerd scaler \n# Min max scaling = (SVC , Adaboost winner : )\n# SVC {'C': 10, 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf'} acc = 82 %\n# ada Boast = 51 estimaters\n\n# Standard scaling\n# Winner adabooast,gradient , knn with 82,82,81\n\n\n\nX = train.drop('Survived',axis=1)\nX = X.drop(['Fare','Age'],axis=1)\ny = train['Survived']\n\nX = pd.get_dummies(X,drop_first=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n\nscaler = MinMaxScaler()\nscaler.fit(X_train)\n\n#scaler = StandardScaler()\n#scaler.fit(X_train)\n\nX_train =  scaler.transform(X_train)\nX_test =  scaler.transform(X_test)\n\ndef predict(model,X_train, X_test, y_train, y_test):\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    print(classification_report(y_test,y_pred))\n    print(plot_confusion_matrix(model,X_test,y_test))\n    \nmodel = LogisticRegression()\npredict(model,X_train, X_test, y_train, y_test)\n\n","ecda680d":"model = SVC()\n#predict(model,X_train, X_test, y_train, y_test)\nparam_grid = {'C':[0.1,0.01,1,5,10,12,20,50],'kernel':['linear','rbf'],'degree':[3,4,5],\n    'gamma':['scale','auto']}\n\ngridModel = GridSearchCV(model,param_grid=param_grid,cv=5)\ngridModel.fit(X_train,y_train)\ngridModel.best_params_","50189191":"predict(gridModel,X_train, X_test, y_train, y_test)","e2d6764e":"model = KNeighborsClassifier(n_neighbors=20)\npredict(model,X_train, X_test, y_train, y_test)","b448cce4":"model = DecisionTreeClassifier()\npredict(model,X_train, X_test, y_train, y_test)","dbd1554b":"X","8b2e3bff":"error_rates = []\n\nfor i in range(50,200):\n    model = RandomForestClassifier(n_estimators=i)\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    err = 1 - accuracy_score(y_test,y_pred)\n    error_rates.append(err)\n    \nplt.plot(range(50,200),error_rates)\n","2cf5abcb":"model = RandomForestClassifier(n_estimators=53)\npredict(model,X_train, X_test, y_train, y_test)","16dc007c":"model = AdaBoostClassifier()\n#predict(model,X_train, X_test, y_train, y_test)\nerror_rates = []\n\nfor i in range(1,200):\n    model = AdaBoostClassifier(n_estimators=i)\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    err = 1 - accuracy_score(y_test,y_pred)\n    error_rates.append(err)\n    \nplt.plot(range(1,200),error_rates)","ad87d05b":"model = AdaBoostClassifier(n_estimators=152)\npredict(model,X_train, X_test, y_train, y_test)","3b8149b9":"model = GradientBoostingClassifier()\n#predict(model,X_train, X_test, y_train, y_test)\nparam_grid = {'n_estimators':[50,100,20,10,80,90,120],\n             'learning_rate':[0.1,0.05,0.2,0.3,0.07],\n             'max_depth':[2,3,4,5,6]}\n\ngridModel = GridSearchCV(model,param_grid=param_grid,cv=5)\ngridModel.fit(X_train,y_train)\ngridModel.best_params_","2f06b1b7":"y_pred = gridModel.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(plot_confusion_matrix(gridModel,X_test,y_test))","c3d0e753":"X = train.drop('Survived',axis=1)\ny = train['Survived']\n\nX = pd.get_dummies(X,drop_first=True)\nt = pd.get_dummies(test,drop_first=True)\n\nscaler = MinMaxScaler()\nscaler.fit(X)\nX =  scaler.transform(X)\nt =  scaler.transform(t)\n\ndef predict(model,X_train, X_test,y):\n    model.fit(X_train,y)\n    y_pred = model.predict(X_test)\n    return y_pred\n    \n\nModel = AdaBoostClassifier(n_estimators=152)\ny_pred = predict(Model,X,t,y)","b585e93b":"t.shape","5ff853ba":"gender_submission = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","83ff3c3c":"gender_submission['Survived'] = y_pred","cbfe204b":"gender_submission.to_csv('gender_submission',index=False)","514be605":"# EDA","d8adc981":"# Model Training","5e85b1c5":"# Final Model","af7ebb76":"# Feature Engineering "}}