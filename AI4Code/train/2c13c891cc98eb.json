{"cell_type":{"3f982e1a":"code","0bdf4994":"code","9daae082":"code","cd78ef52":"code","b624c0d2":"code","efd97f9a":"code","240b0b92":"code","94ce52aa":"code","fb1ee9c2":"code","fe10743d":"code","2e0bffdc":"code","033ac92a":"code","49cc53f9":"code","ca4e1bf2":"code","2c38c2b2":"code","9732dd43":"markdown"},"source":{"3f982e1a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings \n\nwarnings.filterwarnings('ignore')\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0bdf4994":"heart = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\nheart.head()","9daae082":"import tensorflow as tf\n","cd78ef52":"X, y = heart.drop('target', axis=1), heart['target']\nX.shape, y.shape","b624c0d2":" y = tf.reshape(y, shape=(303, 1))","efd97f9a":"# set random seed \ntf.random.set_seed(100)\n\n\n# create a model \n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(4, activation='relu'),\n    tf.keras.layers.Dense(4, activation='relu'),\n\n \n\n\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(\n    loss = tf.keras.losses.BinaryCrossentropy(),\n    optimizer = tf.keras.optimizers.Adam(lr=0.0001),\n    metrics = ['accuracy']\n)\n\n\nmodel.fit(X, y, epochs=100, verbose=0)\nmodel.evaluate(X, y)","240b0b92":"from sklearn.compose import make_column_transformer \nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\n# create a  columns transformer \nct = make_column_transformer(\n    (MinMaxScaler(), ['age', 'trestbps', 'cp', 'chol', 'thalach', 'oldpeak', 'thal']))\n\n\n# create X and Y values \nX, y = heart.drop('target', axis=1), heart['target']\n#y = tf.reshape(y, shape = (len(y),1))\n# build our trian and test sets \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# fit the column transformer to our training data \nct.fit(X_train)\n\n# transforming training and test data \n\nX_train_normal = ct.transform(X_train)\nX_test_normal = ct.transform(X_test)","94ce52aa":"heart.head(2)","fb1ee9c2":"y.shape","fe10743d":"y_train = tf.reshape(y_train, shape = (len(y_train),1))\ny_test = tf.reshape(y_test, shape = (len(y_test),1))\n","2e0bffdc":"# set random seed \ntf.random.set_seed(100)\n\n\n# create a model \n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(4, activation='relu'),\n    tf.keras.layers.Dense(4, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(\n    loss = tf.keras.losses.BinaryCrossentropy(),\n    optimizer = tf.keras.optimizers.Adam(),\n    metrics = ['accuracy']\n)\n\n\nmodel_history = model.fit(X_train_normal, y_train, epochs=100, verbose=0)","033ac92a":"model.evaluate(X_test_normal, y_test)","49cc53f9":"history = pd.DataFrame(model_history.history)\nhistory","ca4e1bf2":"import seaborn as sns\nimport matplotlib.pyplot as plt","2c38c2b2":"plt.subplots(1,1, figsize=(16, 6))\nsns.lineplot(history.index, history.loss)\nsns.lineplot(history.index, history.accuracy)\nplt.legend(['loss', 'accuracy'])\nplt.xlabel('Epochs')\nplt.title(\"Loss Vs Accuracy\", size=22)\nplt.show()","9732dd43":"# Let' Normalise the data and then create a model"}}