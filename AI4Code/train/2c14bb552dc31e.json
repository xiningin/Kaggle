{"cell_type":{"aa735ce9":"code","df02e4bf":"code","77d454bf":"code","8091e583":"code","87966ce0":"code","fda4db7d":"code","aca7bd4b":"code","6dd11d11":"code","27cc327c":"code","5214ff5b":"code","2d6a2e54":"code","e56d218b":"code","b7be5b7b":"code","41551efa":"code","feebff94":"code","28467523":"code","0040d9c8":"code","ea40f3ca":"code","5da68a77":"code","46a6ce10":"code","08568486":"code","d33e89f5":"code","3d0081eb":"code","5fdf4639":"code","9b9ac503":"code","3dd23418":"code","cfd6a338":"code","fb53cbd1":"code","82d208f7":"code","3ad50664":"code","5c8e034c":"code","7e3009ea":"code","4c921f4f":"code","3305f967":"code","0633ab33":"code","15d1383f":"code","1a52ee14":"code","ef97af8f":"code","39447a5f":"code","1d9d9866":"code","60a70563":"code","409bbed0":"code","e90477c7":"code","a394f45d":"code","a2efe4b1":"code","a8aa6106":"code","606e9a78":"code","5ef19f15":"code","ad21fce5":"code","04f0602f":"code","bbca761e":"markdown","8814e3da":"markdown","ac3a4b8c":"markdown","e4489ac6":"markdown","dd60e443":"markdown","453bfcb6":"markdown","adb1716f":"markdown","0f003eb5":"markdown","c6280310":"markdown","5a8aa4f0":"markdown","51f65655":"markdown","1b05d6c7":"markdown","aea077f8":"markdown","ff12661f":"markdown","4462ab34":"markdown","d1eb2a51":"markdown","8f6a3d95":"markdown","50f28009":"markdown","1d943eef":"markdown","7b99a1b4":"markdown","a0f9904e":"markdown"},"source":{"aa735ce9":"# Importing all the libraries that we will need\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import confusion_matrix\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D , MaxPooling2D , Flatten\nfrom keras.layers import Dense , Dropout , Dense\nfrom keras.preprocessing.image import load_img\nfrom sklearn.utils import shuffle\nfrom random import randint\n\n\n# We will print all the directory names from our input to get a idea what all we have to make the magic happen\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    print(dirname)\n","df02e4bf":"# Training data\n# filenames_train is here a list to store all our images with their paths\n# category_train is here a list to store each image's category\n\nfilenames_train = []\ncategory_train = []\n\n# saving the training data path to variable training_data\ntraining_data = os.listdir(\"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\")\n\n# As every image is in it's particular directory we will have to enter in each of the respective directory\nfor dir in training_data:\n    \n    print(str(dir))\n    \n    for file in os.listdir(\"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/\"+dir):\n        \n        # Appending the files to filenames_train list\n        filenames_train.append(\"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/\"+dir+\"\/\"+file)\n        \n        # Appending the Categories to category_train\n        category_train.append(dir)\n        \n    print(\"Process finished for :\" + str(dir))\n    \n    \n    \n# Creating a Dataframe from our Lists\n\ndf_train = pd.DataFrame({\n    \"File_name\":filenames_train,\n    \"Category\":category_train\n})","77d454bf":"# First five instances \n\ndf_train.head(10)","8091e583":"# Last 10 instances\n\ndf_train.tail(10)","87966ce0":"# Getting count of each Category\n\ndf_train['Category'].value_counts()","fda4db7d":"# Plotting a bar graph of count for each category\n\ndf_train['Category'].value_counts().plot.bar()","aca7bd4b":"# Same process as training\n# filenames_test is here a list to store all our images with their paths\n# category_test is here a list to store each image's category\n\nfilenames_test = []\ncategory_test = []\n\n# saving the testing data path to variable training_data\ntesting_data = os.listdir(\"\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test\")\n\n# As every image is in it's particular directory we will have to enter in each of the respective directory\nfor dir in testing_data:\n    \n    print(str(dir))\n    \n    for file in os.listdir(\"\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test\/\"+dir):\n        \n        # Appending the files to filenames_train list\n        filenames_test.append(\"\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test\/\"+dir+\"\/\"+file)\n        \n        # Appending the Categories to category_train\n        category_test.append(dir)\n        \n    print(\"Process finished for :\" + str(dir))\n        \n        \n# Creating a Dataframe from our Lists    \n        \ndf_test = pd.DataFrame({\n    \"File_name\" : filenames_test,\n    \"Category\" : category_test\n})","6dd11d11":"# First 10 instances of testing data\n\ndf_test.head(10)","27cc327c":"# Last 10 instances of testing data\n\ndf_test.tail(10)","5214ff5b":"# Shuffling the training dataset\n\ndf_train = shuffle(df_train)\ndf_train.head(10)","2d6a2e54":"# Shuffling the testing dataset\n\ndf_test = shuffle(df_test)\ndf_test.head(10)","e56d218b":"# Just to Clarify that images have appropriate labels\n\ni = 0\nfor index , row in df_train.iterrows():\n    if i <=10: \n        print(row['File_name'] + \"----->\" + row['Category'])\n        i += 1","b7be5b7b":"img = load_img(\"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/mountain\/3641.jpg\")\nplt.imshow(img)\nplt.title(\"Mountain\")\nplt.show()","41551efa":"img = load_img(\"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/mountain\/3641.jpg\")\nimg = np.array(img)\nimg.shape","feebff94":"img = load_img(\"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/sea\/2229.jpg\")\nplt.imshow(img)\nplt.title(\"Sea\")\nplt.show()","28467523":"img = load_img(\"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/sea\/2229.jpg\")\nimg = np.array(img)\nimg.shape","0040d9c8":"img = load_img(\"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/glacier\/16182.jpg\")\nplt.imshow(img)\nplt.title(\"Glacier\")\nplt.show()","ea40f3ca":"img = load_img(\"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/glacier\/16182.jpg\")\nimg = np.array(img)\nimg.shape","5da68a77":"img = load_img(\"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/buildings\/18084.jpg\")\nplt.imshow(img)\nplt.title(\"Building\")\nplt.show()","46a6ce10":"img = load_img(\"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/buildings\/18084.jpg\")\nimg = np.array(img)\nimg.shape","08568486":"img = load_img(\"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/forest\/13444.jpg\")\nplt.imshow(img)\nplt.title(\"Forest\")\nplt.show()","d33e89f5":"img = load_img(\"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/forest\/13444.jpg\")\nimg = np.array(img)\nimg.shape","3d0081eb":"img = load_img(\"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/street\/13586.jpg\")\nplt.imshow(img)\nplt.title(\"Street\")\nplt.show()","5fdf4639":"img = load_img(\"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/street\/13586.jpg\")\nimg = np.array(img)\nimg.shape","9b9ac503":"# X_train and Y_train are lists here to store our training data, where X_train will store the images and Y_train will store the labels\n\nX_train = []\nY_train = []\n\n# The df.iterrows() function will iterate through each row\nfor index , row in df_train.iterrows():\n    \n    try:\n        \n        # Reading the image\n        img = cv2.imread(row['File_name'] , cv2.IMREAD_COLOR)\n        \n        # Resizing the image to our desired dimensions\n        img = cv2.resize(img ,(128,128))\n    \n        # Appending the image as numpy arrays to X_train\n        X_train.append(np.array(img))\n        \n        # Appending the labels to Y_train\n        Y_train.append(row['Category'])\n    except:\n        pass\n    \n# Just to check how many images were skipped and X_train and Y_train have same data count    \nprint(len(X_train))\nprint(len(Y_train))","3dd23418":"# X_test and Y_test are lists here to store our testing data, where X_test will store the images and Y_test will store the labels\n\nX_test = []\nY_test = []\n\n# The df.iterrows() function will iterate through each row\nfor index , row in df_test.iterrows():\n    \n    try:\n        \n        # Reading the image\n        img = cv2.imread(row['File_name'] , cv2.IMREAD_COLOR)\n        \n        # Resizing the image to our desired dimensions\n        img = cv2.resize(img ,(128,128))\n    \n        # Appending the image as numpy arrays to X_test\n        X_test.append(img)\n        \n        # Appending the labels to Y_test\n        Y_test.append(row['Category'])\n    except:\n       pass\n    \nprint(len(X_test))\nprint(len(Y_test))","cfd6a338":"# We will be plotting some images with the labels\n\nimport random\n\nfig , ax = plt.subplots(2,10)\nplt.subplots_adjust(bottom=0.3 , top = 0.5 , hspace = 0)\nfig.set_size_inches(25,25)\n\nfor i in range(0,2):\n    for j in range(0,10):\n        l = random.randint(0,len(Y_train))\n        ax[i,j].imshow(X_train[l])\n        ax[i,j].set_title(Y_train[l])\n        ax[i,j].set_aspect('equal')","fb53cbd1":"# Converting our datasets to numpy arrays\n\nX_train = np.array(X_train)\nY_train = np.array(Y_train)\nX_test = np.array(X_test)\nY_test = np.array(Y_test)","82d208f7":"# Reshaping our Y data to be a array of [value , 1]\n\nY_train = Y_train.reshape(-1,1)\nY_test = Y_test.reshape(-1,1)\nohe = OneHotEncoder()\nY_train = ohe.fit_transform(Y_train)\nY_test = ohe.fit_transform(Y_test)","3ad50664":"# Displaying all the Categories \n\nohe.categories_","5c8e034c":"# Getting the output of the transformed data\n\nprint(Y_train[1])\n\n# The shape here will be (1 , 6), becuase one hot encoder encoder the data as (1 , numoffeatures array)\n# like assuming Building is encoded to [1,0,0,0,0,0]\nprint(Y_train[1].shape)","7e3009ea":"model = Sequential()\n\n#First Conv layer\nmodel.add(Conv2D(32 , (3,3) , activation = 'relu' , input_shape = (128 , 128 , 3)))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\n\n#Second Conv layer\nmodel.add(Conv2D(64 , (3,3) , activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\n\n#Third Conv layer\nmodel.add(Conv2D(128 , (3,3) , activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\n\n#Fourth Conv layer\nmodel.add(Conv2D(256, (3,3) , activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\n\n#Fifth Conv layer\nmodel.add(Conv2D(256, (3,3) , activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\n\nmodel.add(Flatten())\n\n#First Dense layer\nmodel.add(Dense(256 , activation = 'relu'))\n\n# 25% Neurons will get deactivated simulataneously\nmodel.add(Dropout(0.25))\n\n# Second Dense layer\nmodel.add(Dense(64 , activation = 'relu'))\n\n# 50% Neurons will get deactivated simultaneously\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(6 , activation = 'softmax'))","4c921f4f":"# This will give us the summary of our model\n\nmodel.summary()","3305f967":"# Defining the rules for our model\n\nmodel.compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'])","0633ab33":"# Fitting our model to training data\n\nmodel.fit(X_train , Y_train , epochs = 40 , batch_size = 64)","15d1383f":"# Evaluating our model on test data\n\nloss , accuracy = model.evaluate(X_test , Y_test , batch_size = 32)\n\nprint('Test accuracy: {:2.2f}%'.format(accuracy*100))","1a52ee14":"# Transforming the data back to original and saving it to Y_test_labele_data\n\nY_test_labeled_data  = ohe.inverse_transform(Y_test)","ef97af8f":"# Assuring the data if converted or not\n\nY_test_labeled_data[0:5]","39447a5f":"# Predicting the values for X_test\n\nY_pred = model.predict(X_test).round()","1d9d9866":"# Transforming the data to labels\n\nY_pred = ohe.inverse_transform(Y_pred)","60a70563":"# Assuring if data is converted or not\n\nY_pred[0:5]","409bbed0":"# Generating a heat map for confusion matrix\n\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\nx_ticklabels = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n\ny_ticklabels = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n\ncm = confusion_matrix(Y_test_labeled_data , Y_pred) \n\nprint(cm)\n\nplt.subplots(figsize = (20,15))\n\nsns.heatmap(cm , xticklabels = x_ticklabels , yticklabels = y_ticklabels)","e90477c7":"# Using the data from seg_pred to predict some sample images\n\nseg_pred = os.listdir(\"\/kaggle\/input\/intel-image-classification\/seg_pred\/seg_pred\")\nfig , ax = plt.subplots(5,10)\nplt.subplots_adjust(top = 0.7 , bottom = 0.3 , hspace = 0.7)\nfig.set_size_inches(25,25)\nrandom_values = []\n\n# Plotting the some random Images\nfor i in range(5):\n    for j in range(10):\n        l = random.randint(0, len(seg_pred))\n        img = cv2.imread(\"\/kaggle\/input\/intel-image-classification\/seg_pred\/seg_pred\/\"+seg_pred[l])\n        ax[i,j].imshow(img)\n        ax[i,j].set_title(seg_pred[l])\n        ax[i,j].set_aspect('equal')\n        random_values.append(seg_pred[l])\n    \nprint(len(random_values))","a394f45d":"# Image processing converting the images before feeding them to the model\n\nimages_to_be_predicted = []\nfor i in random_values:\n    img = cv2.imread(\"\/kaggle\/input\/intel-image-classification\/seg_pred\/seg_pred\/\"+i)\n    img = cv2.resize(img , (128,128))\n    images_to_be_predicted.append(np.array(img))\n    \nprint(images_to_be_predicted[1].shape)","a2efe4b1":"images_to_be_predicted = np.array(images_to_be_predicted)","a8aa6106":"predicted_values = model.predict(images_to_be_predicted).round()","606e9a78":"print(predicted_values[0:5])","5ef19f15":"predicted_values = ohe.inverse_transform(predicted_values)","ad21fce5":"print(predicted_values[0:5])","04f0602f":"# PLotting Image with their predicted labels, you are the witness of how good the model performs\n\nfig , ax = plt.subplots(5,10)\nplt.subplots_adjust(top = 0.7 , bottom = 0.3 , hspace = 0.7)\nfig.set_size_inches(25,25)\nk = 0\n\nfor i in range(5):\n    for j in range(10):\n        ax[i,j].imshow(images_to_be_predicted[k])\n        ax[i,j].set_title(predicted_values[k])\n        ax[i,j].set_aspect('equal')\n        k +=1\n        ","bbca761e":"> ### Forest image","8814e3da":"Orginial dimension of Image:\n*  height = 150\n*  width = 150\n*  channels = 3","ac3a4b8c":"## Step 7 : Predicting and Plotting the new data.","e4489ac6":"## Step 5 : Implementing Convolutional Neural Network","dd60e443":"> ### Building Image","453bfcb6":"Orginial dimension of Image:\n*  height = 150\n*  width = 150\n*  channels = 3","adb1716f":"# Classification of Images using ConvNets ( Convolutional Neural Network).\n\n### In this Jupyter notebook, we will learn some cool stuff regarding the below topics.\n\n1. DataFrame Creation\n2. Image Processing\n4. ConvNet Implementation\n5. Making Predictions\n6. Creating a Sample data\n\n### So let's get started, so we will start with understanding the dataset. this dataset contains images of some scenarios, this scenarios are listed below:\n\n1. Buildings\n2. Glaciers\n3. Street\n4. Forest\n5. Sea\n6. Mountain\n\n### and what we have to do is create a ConvNet that can classify if given image is one of the scenarios we got. So this dataset can be confusing at times becuase if you convert the images to black and white.\n### then the model would predict mountain as glacier and glacier as mountain and also same with some Images of streets and buildings. The original dimension of images are (150 , 150 ,3 ), i.e are described below:\n\n#### Orginial dimension of Image:\n*  height = 150\n*  width = 150\n*  channels = 3\n","0f003eb5":"> ### Mountain Image","c6280310":"Orginial dimension of Image:\n*  height = 150\n*  width = 150\n*  channels = 3","5a8aa4f0":"> ### Glacier Image","51f65655":"### Please upvote, if this impementation of ConvNets was interesting and you learnt something. And comment if I missed something or messed up the code somewhere, after all we all are programmers ","1b05d6c7":"## Step 4 : Plotting the images","aea077f8":"> ### Street image","ff12661f":"## Step 6 : Generating data to be predicted","4462ab34":"Orginial dimension of Image:\n*  height = 150\n*  width = 150\n*  channels = 3","d1eb2a51":"## Step 1 : Importing our Libraries","8f6a3d95":"## Step 2 : Preparing our dataset","50f28009":"Orginial dimension of Image:\n*  height = 150\n*  width = 150\n*  channels = 3","1d943eef":"Orginial dimension of Image:\n*  height = 150\n*  width = 150\n*  channels = 3","7b99a1b4":"# Step 3 : Image Preprocessing","a0f9904e":"> ### Sea Image"}}