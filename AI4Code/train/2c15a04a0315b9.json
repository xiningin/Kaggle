{"cell_type":{"990a8795":"code","8de04b2a":"code","91a17c36":"code","858fd2d9":"code","cd3830ed":"code","6120e805":"code","199d0bd0":"code","05c2d997":"code","017f0ec7":"code","ab2dfc22":"code","206de4bb":"code","7a62a5d5":"code","09d56e4c":"code","77ffe5be":"code","3f32e20f":"code","55adc451":"code","1ca3263e":"code","91c24e00":"code","626e7e49":"code","648a93bf":"code","f6240260":"code","2d2dbd75":"code","78175063":"code","c230ab94":"code","68c9d3a5":"code","eba9f645":"code","10d4a02b":"code","193290e1":"code","69795d54":"code","1c0345af":"code","1be472ce":"code","73a12016":"code","c9cb6fe1":"code","4ebb90dc":"code","7a779789":"markdown","2497390c":"markdown","13089ef0":"markdown","22d7fb78":"markdown","9966af1a":"markdown","4b57a152":"markdown","005a6eb5":"markdown","1821d825":"markdown","9d216325":"markdown","686ec805":"markdown","b03a94d9":"markdown"},"source":{"990a8795":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","8de04b2a":"testLength = 14 #days","91a17c36":"path_to_inputs = '\/kaggle\/input\/covid19-granular-demographics-and-times-series\/'\ninput_filename1=\"departments_static_data_divBySubPop.csv\"\ninput_filename2=\"time_series_covid_incidence_divBySubPop.csv\"\ninput_filenameNORM=\"population_used_for_normalization.csv\"\n# filename=\"departments_static_data.csv\"","858fd2d9":"## static data \ninput_filename1 = 'departments_static_data_divBySubPop.csv' ## this file may contain the same pre-proc done in the cell below\ndf1 = pd.read_csv(path_to_inputs+input_filename1, delimiter=',')\ndf1 # .describe()","cd3830ed":"# for col in df1.columns[2:]:\n#     plt.figure()\n#     plt.title(col)\n#     plt.hist(df1[col], 30)\n","6120e805":"# dynamic data (to be predicted)\ndf2 = pd.read_csv(path_to_inputs+input_filename2)","199d0bd0":"df5 = pd.read_csv(path_to_inputs+input_filenameNORM)\ndf5","05c2d997":"df1 = df1.fillna(df1.mean())","017f0ec7":"df2 = df2.fillna(0)","ab2dfc22":"codes_dynamic_file = np.array(df2.iloc[::24,0])\ndynamic_file_column_names = np.array(df2.iloc[:24,1])\ncodes_dynamic_file, dynamic_file_column_names","206de4bb":"y = np.array(df2.iloc[:,2:])\nndepartments = 100\nnvals = y.shape[0]\/\/ndepartments\nnvals","7a62a5d5":"yr = y.reshape( (ndepartments, nvals, y.shape[1]))\n\nreshaped = []\nfor n in range(ndepartments) :\n    reshaped.append(yr[n].transpose().copy())\nreshaped = np.array(reshaped)","09d56e4c":"ndep = 54\nnval = 6\ny[24*ndep:24*(ndep+1),nval], y.shape","77ffe5be":"yr[ndep,:,nval], yr.shape","3f32e20f":"reshaped[ndep,nval]","55adc451":"df1 = df1.sort_values(by=['code'])\nXs = np.array(df1.iloc[:,2:])\nXs","1ca3263e":"import sklearn.decomposition\nXs.shape\n\n## standardize static data: as usual\nmean_Xs = Xs.mean(axis=0)\nstd_Xs  = Xs.std (axis=0)\nscaled_Xs = (Xs-mean_Xs)\/std_Xs\nXstatic = scaled_Xs.copy()","91c24e00":"def quick_look_at_pca(X, n_components):\n    pca = sklearn.decomposition.PCA(n_components=n_components)\n    pca.fit(X)\n    Xp = pca.transform(X)\n    plt.plot(pca.explained_variance_ratio_[:10], label=\"explained_variance_ratio\")\n    plt.legend()\n    plt.xlabel(\"n_components (PCA)\")\n    plt.ylim([0,1])\n    Xrecov = pca.inverse_transform(Xp)\n    print(\"reconstruciton (Mean Absolute) Error: \", abs(X-Xrecov).mean())\n    print(\"Xp.shape, pca.noise_variance_\", Xp.shape, pca.noise_variance_)\n    return Xp    ","626e7e49":"n_components = None\nXp = quick_look_at_pca(Xstatic, n_components)","648a93bf":"def reconstruction_errors(X):\n    recos = []\n    for n_components in range(1,20, 1):\n        pca = sklearn.decomposition.PCA(n_components=n_components)\n        pca.fit(X)\n        Xp = pca.transform(X)\n        Xrecov = pca.inverse_transform(Xp)\n        reconstruction_MAE = abs(X-Xrecov).mean()\n        recos.append( (n_components, reconstruction_MAE,pca.noise_variance_) )\n    return np.array(recos)\nrecos = reconstruction_errors(Xstatic)\nplt.plot(recos[:,0],recos[:,1], marker='o', label=\"reconstruction error (MAE)\")\nplt.plot(recos[:,0],recos[:,2], marker='o', label=\"noise variance\")\nplt.xlabel(\"n_components (PCA)\")\nplt.legend()","f6240260":"n_components = 7\nXp = quick_look_at_pca(Xstatic, n_components)","2d2dbd75":"Xs = np.array(df1.iloc[:,2:])\ndynamicData = reshaped.copy() ## dynamic data (all of it)\nXd  = dynamicData[:, :-testLength ].copy() ## dynamic data used as features \nyd  = dynamicData[:,  -testLength:].copy() ## dynamic data used as ground truth labels\/values (to be predicted)\n\n## standardize static data: as usual\nmean_Xs = Xs.mean(axis=0)\nstd_Xs  = Xs.std (axis=0)\nscaled_Xs = (Xs-mean_Xs)\/std_Xs\nXstatic = scaled_Xs.copy()\n\n## we re-build the data-set ##\nn_components = 9\npca = sklearn.decomposition.PCA(n_components=n_components)\nXpca = pca.fit_transform(Xstatic)\n# pop = np.array(df5[\"Pop_sex=all_age=all_Population\"])\n\n\n## standardize dynamic data:\n## we use the last 3 days of (available) data as typical value\nmean_Xd = Xd[:,-3:].mean(axis=1).mean(axis=0) \n## TODO: try other scales\nstd_Xd  = Xd[:,-3:].std(axis=1).std(axis=0)\nscaled_Xd = (Xd-mean_Xd)\/std_Xd\nndep = scaled_Xd.shape[0]\nnday_train = scaled_Xd.shape[1]\nnvalues = scaled_Xd.shape[2]\nscaled_Xd = scaled_Xd.reshape( (ndep, nday_train*nvalues) )\n\n## standardize output with same scales as input\nnday_test = testLength\nscaled_yd = (yd-mean_Xd)\/std_Xd ## we cannot know in advance the scaling factor of future data !!\nscaled_yd = scaled_yd.mean(axis=1) ## we average the next two weeks to make it less noisy\nnday_test = 1 \nscaled_yd = scaled_yd.reshape( (ndep, nday_test*nvalues))\n## \ny = scaled_yd.copy()\n\nprint(ndep, nday_train, nvalues, nday_test)","78175063":"## combine static and dynamic input\n## here this is a dumb way (nto accounting for the temporal specificity of our data)\nX = np.concatenate( (Xpca, scaled_Xd), axis=1) ## using both \n\nX = Xpca  ## only static !\n\nX = scaled_Xd ## only dynamic (no socio-demographics)\n","c230ab94":"Xpca.shape, scaled_Xd.shape, scaled_yd.shape, X.shape, y.shape","68c9d3a5":"## consider  also sklearn.model_selection.TimeSeriesSplit(n_splits=5, max_train_size=None)\n\ndef train_test_pop_split(X,y,test_ratio, seed):\n    ## train-test split, KEEPING TRACK of the departmental populations ##\n    rng = np.random.default_rng(seed)\n    Nexamples = X.shape[0]\n    indexes = np.arange(Nexamples, dtype=int)\n    Ntest = int(Nexamples*test_ratio)\n    test_indexes = rng.choice(indexes, size=Ntest, replace=False)\n    train_indexes = []\n    for ind in indexes:\n        if ind not in test_indexes:\n            train_indexes.append(ind)\n    train_indexes = np.array(train_indexes)\n\n    X_train= X[train_indexes]\n    y_train= y[train_indexes]\n#     y_train_pop = pop[train_indexes].reshape( (Nexamples-Ntest,1) )\n\n    X_test = X[test_indexes]\n    y_test = y[test_indexes]\n#     y_test_pop = pop[test_indexes].reshape( (Ntest,1) )\n\n    return X_train, X_test, y_train, y_test # , y_train_pop, y_test_pop","eba9f645":"import sklearn.neural_network\n\nX = np.concatenate( (Xpca, scaled_Xd), axis=1) ## using both \n# X = Xpca  ## only static !\n# X = scaled_Xd ## only dynamic (no socio-demographics)\n\nseed = 42\ntest_ratio=0.33\nX_train, X_test, y_train, y_test = train_test_pop_split(X, y, test_ratio, seed)\n#, y_train_pop, y_test_pop = train_test_pop_split(X, y, pop, test_ratio, seed)\n\n## model (cheap) ##\n# model = sklearn.linear_model.LinearRegression(normalize=False)\nnetwork_layers = (500,100)\n# model = sklearn.neural_network.MLPRegressor(network_layers,  solver='lbfgs', max_iter=1000)\nmodel = sklearn.neural_network.MLPRegressor(network_layers, learning_rate='adaptive', early_stopping=True, validation_fraction=0.2,n_iter_no_change=20)\n# model = sklearn.svm.LinearSVR()\nprint(X_train.shape, y_train.shape)\nmodel.fit(X=X_train, y=y_train)\n","10d4a02b":"## predictions\nypred = model.predict(X_test)\nypred_all = model.predict(X)\nytrue = y_test.copy()\n\n# test_ndep = y_test.shape[0]\n# nday_train = y_test.shape[1]\n# test_nvalues = y_test.shape[2]\n# test_nday_test = scaled_yd.shape[1]\n\nypred = ypred.reshape((y_test.shape[0], nday_test, nvalues))\n\nytrue = ytrue.reshape((y_test.shape[0], nday_test, nvalues))\n\n# def raw_number(y, std_Xd, mean_Xd, pop):\n#     ## re-scaled predictions\n#     raw_number_y = (y*std_Xd+mean_Xd)*pop\n#     return raw_number_y\n\n# ypred = raw_number(ypred, std_Xd, mean_Xd, y_test_pop)\n# ytrue = raw_number(ytrue, std_Xd, mean_Xd, y_test_pop)","193290e1":"24*67*72, 22512\nypred_all  = ypred_all.reshape((y.shape[0], nday_test, nvalues))","69795d54":"model.score(X,y, sample_weight=None), model.score(X_train,y_train, sample_weight=None), model.score(X_test,y_test, sample_weight=None)","1c0345af":"# try again with:\n# X = scaled_Xd ## only dynamic (no socio-demographics)\n\nmodel.score(X,y, sample_weight=None), model.score(X_train,y_train, sample_weight=None), model.score(X_test,y_test, sample_weight=None)","1be472ce":"diff = (ypred-ytrue)\/ytrue\nindicator =23\n# for dep in range(33): \n#     plt.plot(diff[dep, :, indicator])","73a12016":"diff = (ypred-ytrue)\/ytrue\nindicator =23\ncodes_dynamic_file, dynamic_file_column_names\nfor indicator in range(24): \n    err = np.median(np.abs(diff[:, :, indicator]))\n    print( err , dynamic_file_column_names[indicator])","c9cb6fe1":"diff = (ypred_all-y.reshape((y.shape[0], nday_test, nvalues)))\/ypred_all\nfor indicator in range(24): \n    err = np.median(np.abs(diff[:, :, indicator]))\n    print( err , dynamic_file_column_names[indicator])","4ebb90dc":"## we show some departments, not all of the test set, for clarity\nNshow=15\n\nimport matplotlib.cm as cm\nNcolors = Nshow+1\ngradient = cm.jet( np.linspace(0.0, 1.0, Ncolors+1 ) )\n# color = tuple(gradient[dep])\n\nfor dep in range(Nshow):\n    color = tuple(gradient[dep])\n    plt.figure(1)\n    plt.semilogy(ytrue [dep], ls='-', lw=3, color=color, label= \"true\")\n    plt.plot(ypred [dep], ls=':', lw=2, color=color, label= \"predicted\")\n    \n    plt.figure(2)\n    plt.loglog(ytrue [dep], ypred [dep], ls='', marker='x', markersize=5, color=color)\nplt.figure(2)\nplt.xlabel(\"ytrue\")\nplt.ylabel(\"ypred\")\n    ","7a779789":"# Pre-processing 1: Dealing with MISSING VALUES\n\nWe could replace NaNs with the mean value of their column","2497390c":"# Pre-processing 5: standardization (+PCA)\n\nWe should keep track of those variables, to be able to re-scale our predictions back into their original form.","13089ef0":"#### We go Beyond score measures\n\nit is good to compare predicted and actual numbers on actual plots","22d7fb78":"#### We check the reshaping","9966af1a":"# Predictions : very simple models (not taking temporal aspect into account)","4b57a152":"# Pre-processing 3: export to numpy\n\n### Preparing DataFrame.s for export to numpy \n\nNumpy does not like string-index. \n\nBut if we combine data sets using numpy, we have to be very careful that the index of departments (i.e. examples) do match.","005a6eb5":"#### Static data correlations: Conclusion\n\nThe (static) inputs appear to be quite correlated (between them), but not so much","1821d825":"# Pre-processing 4: PCA,\n### or \"how correlated are our entries ?\"\n\nLet's do a PCA on the static (socio-demographics) indicators","9d216325":"## Load the data ","686ec805":"# Pre-processing 2 : reshaping the dynamic data \n\n(it is originally in a multi-index Pandas DataFrame)","b03a94d9":"# Introduction\n\nThis is a stater Kernel that **we** edited for your convenience (not just the kaggle-bot). \n\nIt loads some of the main files, makes some simple pre-processing on static data, \n\nand makes some very simple predictions\n\nClick the blue \"Edit Notebook\" or \"Fork Notebook\" button at the top of this kernel to begin editing."}}