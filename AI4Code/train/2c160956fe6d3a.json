{"cell_type":{"f6e4fc6b":"code","a0a7e533":"code","4400c145":"code","30eedff8":"code","b1e8fdd5":"code","7f18707a":"code","a59c4a65":"code","add5a63c":"code","7e08c1ec":"code","7f715043":"code","ca241ba1":"code","a36f0710":"code","d8bf38bb":"code","f1e91868":"code","e073477a":"code","4dddd544":"code","63f5f433":"code","bbd6ee79":"code","f8a58314":"code","e6a5159f":"code","343e9d69":"code","846c29f4":"code","60438e21":"code","db5a6d3f":"code","0a8a7010":"code","b8e0580c":"code","17144cf9":"code","e23e1cc8":"code","fa15c1b0":"code","2f0e4ad4":"code","2094b577":"code","62cf70f5":"code","b3ca768e":"code","2e351183":"code","26f89b7b":"code","8c044b76":"code","a1ebb216":"code","6f7e9249":"code","e5811ca6":"code","5dc4ec32":"code","556d3b8e":"code","6c8ed4bd":"code","186a0a8b":"markdown","87047310":"markdown","1a265f24":"markdown","f4323f56":"markdown","52302921":"markdown","9688b410":"markdown","df8d0715":"markdown","eb2d52fe":"markdown","82eb4775":"markdown","49d91688":"markdown","1c5fcf3a":"markdown","f07be584":"markdown","3cd89b45":"markdown","24ce553a":"markdown","efe113f8":"markdown","8ea6897d":"markdown","9b622bec":"markdown","8e337f92":"markdown","b8bb936e":"markdown"},"source":{"f6e4fc6b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.io.json import json_normalize\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import linear_model\nfrom sklearn.cluster import KMeans\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a0a7e533":"sns.set()","4400c145":"train_df = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/test.csv')","30eedff8":"train_df.info()","b1e8fdd5":"cols_list = train_df.columns.tolist()\n#for col in cols_list:\n#    print(train_df[col].value_counts().sort_values(ascending=False))\n\ncols_list","7f18707a":"train_df.isna().sum()","a59c4a65":"test_df.info()","add5a63c":"specs = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/specs.csv')\n\nspecs.head()","7e08c1ec":"labels = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv')\n\nlabels.head()","7f715043":"#train_df = train_df.loc[(train_df['event_code'] == 4100) | (train_df['event_code'] == 4110)]\n#train_df = train_df.loc[train_df['type'] == 'Assessment']\n\n#train_df = pd.merge(train_df, specs, on='event_id', how='inner')\n\n#train_df.head()","ca241ba1":"train_df = pd.merge(train_df,labels,on=['game_session','installation_id'],how='inner')\n\ntrain_df.info()","a36f0710":"train_df['timestamp'] = pd.to_datetime(train_df['timestamp'], infer_datetime_format=True)\ntest_df['timestamp'] = pd.to_datetime(test_df['timestamp'], infer_datetime_format=True)\n\ntrain_df['event_hour'] = train_df['timestamp'].dt.hour\ntrain_df['event_day'] = train_df['timestamp'].dt.dayofweek\n\ntest_df['event_hour'] = test_df['timestamp'].dt.hour\ntest_df['event_day'] = test_df['timestamp'].dt.dayofweek","d8bf38bb":"train_df[['title_x','world','type']] = train_df[['title_x','world','type']].astype('category')\n\ndf=pd.get_dummies(train_df,columns=['title_x','world','type'], prefix=['title','world','type'])\n\ndf.info()","f1e91868":"df=df.rename(columns={\"title_Bird Measurer (Assessment)\":\"BirdMeasurer\",\"title_Cart Balancer (Assessment)\":\"CartBalancer\",\n                               \"title_Cauldron Filler (Assessment)\":\"CauldronFiller\",\"title_Chest Sorter (Assessment)\":\"ChestSorter\",\n                               \"title_Mushroom Sorter (Assessment)\":\"MushroomSorter\",\"world_CRYSTALCAVES\":\"CRYSTALCAVES\",\n                               \"world_MAGMAPEAK\":\"MAGMAPEAK\",\"world_TREETOPCITY\":\"TREETOPCITY\",\"type_Assessment\":\"type\"})","e073477a":"train_df_gp = df.groupby('installation_id')['timestamp'].agg('max').reset_index()\ndf = pd.merge(df,train_df_gp,on=['installation_id','timestamp'],how='inner')\n\ndf.info()","4dddd544":"df.head()","63f5f433":"cols = ['game_session',\n 'installation_id',\n 'event_day',\n 'event_hour',\n 'BirdMeasurer',\n 'CartBalancer',\n 'CauldronFiller',\n 'ChestSorter',\n 'MushroomSorter',\n 'CRYSTALCAVES',\n 'MAGMAPEAK',\n 'TREETOPCITY',\n 'accuracy',\n 'accuracy_group']\n\ndf = df[cols]\ndf.head()","bbd6ee79":"#test_df = test_df.loc[(test_df['event_code'] == 4100) | (test_df['event_code'] == 4110)]\n#test_df = test_df.loc[test_df['type'] == 'Assessment']","f8a58314":"test_df[['title','world','type']] = test_df[['title','world','type']].astype('category')\n\ndf_test=pd.get_dummies(test_df,columns=['title','world','type'], prefix=['title','world','type'])","e6a5159f":"df_test=df_test.rename(columns={\"title_Bird Measurer (Assessment)\":\"BirdMeasurer\",\"title_Cart Balancer (Assessment)\":\"CartBalancer\",\n                               \"title_Cauldron Filler (Assessment)\":\"CauldronFiller\",\"title_Chest Sorter (Assessment)\":\"ChestSorter\",\n                               \"title_Mushroom Sorter (Assessment)\":\"MushroomSorter\",\"world_CRYSTALCAVES\":\"CRYSTALCAVES\",\n                               \"world_MAGMAPEAK\":\"MAGMAPEAK\",\"world_TREETOPCITY\":\"TREETOPCITY\",\"type_Assessment\":\"type\"})","343e9d69":"test_df_gp = df_test.groupby('installation_id')['timestamp'].agg('max').reset_index()\ntest_df_merge = pd.merge(df_test,test_df_gp,on=['installation_id','timestamp'],how='inner')","846c29f4":"test_df_merge.info()","60438e21":"cols_test = ['game_session',\n 'installation_id',\n 'event_day',\n 'event_hour',\n 'BirdMeasurer',\n 'CartBalancer',\n 'CauldronFiller',\n 'ChestSorter',\n 'MushroomSorter',\n 'CRYSTALCAVES',\n 'MAGMAPEAK',\n 'TREETOPCITY',\n]\n\ntest_df_subset = test_df_merge[cols_test]\n\ntest_df_subset.head()\n","db5a6d3f":"cat1 = sns.catplot(x=\"BirdMeasurer\", hue=\"accuracy_group\", data=df,\n                height=6, aspect=1.5, kind=\"count\", palette=\"colorblind\")\n\ncat1","0a8a7010":"\ncat2 = sns.catplot(x=\"CartBalancer\", hue=\"accuracy_group\", data=df,\n                height=6, aspect=1.5, kind=\"count\", palette=\"colorblind\")\n\ncat2","b8e0580c":"cat3 = sns.catplot(x=\"CauldronFiller\", hue=\"accuracy_group\", data=df,\n                height=6, aspect=1.5, kind=\"count\", palette=\"colorblind\")\n\ncat3","17144cf9":"cat4 = sns.catplot(x=\"ChestSorter\", hue=\"accuracy_group\", data=df,\n                height=6, aspect=1.5, kind=\"count\", palette=\"colorblind\")\n\ncat4","e23e1cc8":"cat5 = sns.catplot(x=\"MushroomSorter\", hue=\"accuracy_group\", data=df,\n                height=6, aspect=1.5, kind=\"count\", palette=\"colorblind\")\n\ncat5","fa15c1b0":"cat6 = sns.catplot(x=\"CRYSTALCAVES\", hue=\"accuracy_group\", data=df,\n                height=6, aspect=1.5, kind=\"count\", palette=\"colorblind\")\n\ncat6","2f0e4ad4":"cat7 = sns.catplot(x=\"MAGMAPEAK\", hue=\"accuracy_group\", data=df,\n                height=6, aspect=1.5, kind=\"count\", palette=\"colorblind\")\n\ncat7","2094b577":"cat8 = sns.catplot(x=\"TREETOPCITY\", hue=\"accuracy_group\", data=df,\n                height=6, aspect=1.5, kind=\"count\", palette=\"colorblind\")\n\ncat8","62cf70f5":"def perform_logistic_regression(df_X, df_Y, test_df_X):\n    logistic_regression = LogisticRegression()\n    logistic_regression.fit(df_X, df_Y)\n    pred_Y = logistic_regression.predict(test_df_X)\n    accuracy = round(logistic_regression.score(df_X, df_Y) * 100,2)\n    returnval = {'model':'Logistic Regression','accuracy':accuracy}\n    return returnval","b3ca768e":"def perform_svc(df_X, df_Y, test_df_X):\n    svc_clf = SVC()\n    svc_clf.fit(df_X, df_Y)\n    pred_Y = svc_clf.predict(test_df_X)\n    accuracy = round(svc_clf.score(df_X, df_Y) * 100, 2)\n    returnval = {'model':'SVC', 'accuracy':accuracy}\n    return returnval","2e351183":"def perform_linear_svc(df_X, df_Y, test_df_X):\n    svc_linear_clf = LinearSVC()\n    svc_linear_clf.fit(df_X, df_Y)\n    pred_Y = svc_linear_clf.predict(test_df_X)\n    accuracy = round(svc_linear_clf.score(df_X, df_Y) * 100, 2)\n    returnval = {'model':'LinearSVC', 'accuracy':accuracy}\n    return returnval","26f89b7b":"def perform_rfc(df_X, df_Y, test_df_X):\n    rfc_clf = RandomForestClassifier(n_estimators = 100 ,oob_score=True, max_features=None)\n    rfc_clf.fit(df_X, df_Y)\n    pred_Y = rfc_clf.predict(test_df_X)\n    accuracy = round(rfc_clf.score(df_X, df_Y) * 100, 2)\n    returnval = {'model':'RandomForestClassifier','accuracy':accuracy}\n    return returnval","8c044b76":"def perform_knn(df_X, df_Y, test_df_X):\n    knn = KNeighborsClassifier(n_neighbors=3)\n    knn.fit(df_X, df_Y)\n    pred_Y = knn.predict(test_df_X)\n    accuracy = round(knn.score(df_X, df_Y) *100,2)\n    returnval = {'model':'KNeighborsClassifier','accuracy':accuracy}\n    return returnval","a1ebb216":"def perform_gnb(df_X, df_Y, test_df_X):\n    gnb = GaussianNB()\n    gnb.fit(df_X, df_Y)\n    pred_Y = gnb.predict(test_df_X)\n    accuracy = round(gnb.score(df_X, df_Y)*100,2)\n    returnval = {'model':'GaussianNB','accuracy':accuracy}\n    return returnval","6f7e9249":"def perform_dtree(df_X, df_Y, test_df_X):\n    dtree = DecisionTreeClassifier()\n    dtree.fit(df_X, df_Y)\n    pred_Y = dtree.predict(test_df_X)\n    accuracy = round(dtree.score(df_X, df_Y)*100,2)\n    returnval = {'model':'DecisionTreeClassifier','accuracy':accuracy}\n    return returnval","e5811ca6":"def perform_linear_regression(df_X, df_Y, test_df_X):\n    linear_regression = LinearRegression()\n    linear_regression.fit(df_X, df_Y)\n    pred_Y = linear_regression.predict(test_df_X)\n    # size_y = pred_Y.size\n    # cks = cohen_kappa_score(df_Y[:size_y], pred_Y, weights=\"quadratic\")\n    accuracy = round(linear_regression.score(df_X, df_Y)*100,2)\n    returnval = {'model':'LinearRegression','accuracy':accuracy}\n    return returnval","5dc4ec32":"X = df.drop(['game_session','installation_id','accuracy','accuracy_group'],axis=1)\ny = df['accuracy_group']\n\ntest_X = test_df_subset.drop(['game_session','installation_id'],axis=1)","556d3b8e":"linreg_val = perform_linear_regression(X, y, test_X)\nlr_val = perform_logistic_regression(X, y, test_X)\nsvc_val = perform_svc(X, y, test_X)\nsvc_lin_val = perform_linear_svc(X, y, test_X)\nrfc_val = perform_rfc(X, y, test_X)\nknn_val = perform_knn(X, y, test_X)\ngnb_val = perform_gnb(X, y, test_X)\ndtree_val = perform_dtree(X, y, test_X)\n    \nmodel_accuracies = pd.DataFrame()\nmodel_accuracies = model_accuracies.append([linreg_val, lr_val,svc_val,svc_lin_val, rfc_val, knn_val, gnb_val, dtree_val])\n# [linreg_val, lr_val,svc_val,svc_lin_val, rfc_val, knn_val, gnb_val, dtree_val]\ncols = list(model_accuracies.columns.values)\ncols = cols[-1:] + cols[:-1]\nmodel_accuracies = model_accuracies[cols]\nmodel_accuracies = model_accuracies.sort_values(by='accuracy')\nprint(model_accuracies)\nplt.figure()\nplt.xticks(rotation=90)\nsns.barplot(x='model', y='accuracy', data=model_accuracies)","6c8ed4bd":"lg = RandomForestClassifier(n_estimators = 100 ,oob_score=True, max_features=None).fit(X, y)\n\ny_pred = lg.predict(test_X)\n\ny_pred = lg.predict(test_X)\n\ntest_X['accuracy_group'] = y_pred\n\ntest_X['accuracy_group'] = test_X['accuracy_group'].astype('int')\n\ntest_X['installation_id'] = test_df_subset['installation_id']\n\nfinal_df = test_X[['installation_id','accuracy_group']]\nfinal_df.to_csv('submission.csv',sep=',',index=False)\n\n#final_df['accuracy_group'].value_counts()\n","186a0a8b":"# Introduction\n\nThe main outcome of this competition is to uncover new insights in early childhood education and whether media can support learning outcomes. The data for this competition is provided by **PBS KIDS**, specifically from the **PBS KIDS Measure UP** app, a game based learning tool developed as part of CPB-PBS Ready to Learn initiative. The data used in this competition is anonymous, tabular data of interactions with the app. ","87047310":"## Evaluating models","1a265f24":"# Preparing training and test dataframes for model evaluation","f4323f56":"# Loading Libraries","52302921":"# Loading and exploring necessary datasets","9688b410":"### Rename column names","df8d0715":"## About the Data\n\nIn PBS KIDS MeasureUp app, children navigate a map and complete various levels, which may be activities, video clips, games, or assessments. Each assessment is designed to test a child's comprehension of a certain set of measurement-related skills. There are five assessments: Bird Measurer, Cart Balancer, Cauldron Filler, Chest Sorter, and Mushroom Sorter.\n\nThe intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment (an incorrect answer is counted as an attempt). Each application install is represented by an installation_id. This will typically correspond to one child, but there is some noise from issues such as shared devices. The training set, contains full history of gameplay data whereas the test set, has been truncated to the history after the start event of a single assessment, chosen randomly, for which we must predict the number of attempts. Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n\nThe outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n    3: the assessment was solved on the first attempt\n    2: the assessment was solved on the second attempt\n    1: the assessment was solved after 3 or more attempts\n    0: the assessment was never solved\n\nThe file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set. Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110. If the attempt was correct, it contains \"correct\":true.","eb2d52fe":"### Rename column names","82eb4775":"### Group test dataframe by installation_id and pick record with max timestamp value","49d91688":"### Group training dataframe by installation_id and pick record with max timestamp value","1c5fcf3a":"### Converting timestamp to datetime type and appending hour and dayofweek values to dataframes","f07be584":"# Data Wrangling\n\n## Training Dataframe\n### Merging training and labels dataframes","3cd89b45":"### Converting categorical variables into indicator variables","24ce553a":"## Test Dataframe","efe113f8":"### Converting categorical variables to indicator variables ","8ea6897d":"# Summary\n\nUsing RandomForestClassifier has given public score of 0.279 which is the highest so far I achieved. I know there is lot to improve on my part in terms of data manipulation, feature selection and picking the right model. Please provide any inputs that will help me to improve.","9b622bec":"# Model definitions","8e337f92":"# Predicting output\n\nBased on the model accuries from above section it is evident that RandomForestClassifier has highest accuracy value. So I am going to use it to predict output. ","b8bb936e":"# EDA"}}