{"cell_type":{"a1845bdf":"code","d45b672d":"code","75d95b0e":"code","58adbfc0":"code","286e94b6":"code","d2a7469f":"code","3ae9f2a6":"code","0915f1cc":"code","cc0815a8":"code","cd865d3f":"code","18702124":"markdown","2fd28636":"markdown"},"source":{"a1845bdf":"# Code you have previously used to load data\nimport matplotlib.pyplot as plt\nimport mlxtend\nimport numpy as np\nimport pandas as pd\n\nfrom bayes_opt import BayesianOptimization\nfrom datetime import datetime as dt\nfrom lightgbm import LGBMRegressor\nfrom sklearn.base import clone\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Imputer, RobustScaler, StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom warnings import filterwarnings\nfrom xgboost import XGBRegressor\n\nfilterwarnings('ignore')","d45b672d":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\nprint(df_train.shape)\nprint(df_test.shape)\nn_train = df_train.shape[0]\n\ntarget = df_train['SalePrice']\ndf_train.drop(columns=['SalePrice', 'Id'], inplace=True)\ntest_id = df_test['Id']\ndf_test.drop(columns=['Id'], inplace=True)\nassert((df_train.columns == df_test.columns).all())\n\ndf_total = pd.concat([df_train, df_test])\ndf_numeric = df_total.select_dtypes(include='number')\ndf_onehot = pd.get_dummies(df_total)\nprint(df_numeric.shape[1], 'vs', df_onehot.shape[1])","75d95b0e":"plt.title('Target')\nplt.hist(target)\nplt.hist(target, 100);","58adbfc0":"target_log = np.log(target)\nX_train, X_test, y_train, y_test = train_test_split(df_onehot[:n_train], target_log, test_size=0.1, random_state=42)\nprint(np.mean(y_train), np.median(y_train))","286e94b6":"def root_mean_squared_error(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\n\ndef root_mean_squared_log_error(y_true, y_pred):\n     return np.sqrt(mean_squared_log_error(y_true, y_pred))\n\n\ndef RMSLE(estimator, X, y):\n    return root_mean_squared_error(np.log(y + 1), np.log(estimator.predict(X) + 1))\n\n\ndef negRMSLE(estimator, X, y):\n    return -RMSLE(estimator, X, y)\n\n\ndef RMSE(estimator, X, y):\n    return root_mean_squared_error(y, estimator.predict(X))\n\n\ndef negRMSE(estimator, X, y):\n    return -RMSE(estimator, X, y)\n\n\ndef elastic_baseline():\n    pipe = Pipeline([\n        ('imputer', Imputer()),\n        ('scaler', RobustScaler()),\n        ('regressor', ElasticNet(random_state=42))\n    ])\n    params = {\n        'imputer__strategy': ['mean', 'median'],\n        'regressor__alpha': [10**i for i in range(-3, 4)],\n        'regressor__l1_ratio': [0.0, 0.001, 0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n    }\n    grid = GridSearchCV(pipe, params, scoring='neg_mean_squared_error', cv=10, n_jobs=-1, return_train_score=True, verbose=2)\n    grid.fit(X_train, y_train)\n    \n#     df_scores = pd.DataFrame(grid.cv_results_)\n    print('Best CV-score: {:.5f}'.format(grid.best_score_))\n    print('Best params:', grid.best_params_)\n    print('Time to refit on whole X_train: {:.2f}s'.format(grid.refit_time_))\n\n    model_baseline = pipe\n    model_baseline.fit(X_train, y_train)\n    print('Baseline train score: {:.5f}'.format(RMSE(model_baseline, X_train, y_train)))\n    print('Baseline test score: {:.5f}'.format(RMSE(model_baseline, X_test, y_test)))\n\n    model = grid.best_estimator_\n    model.fit(X_train, y_train)\n    print('Optimized train score: {:.5f}'.format(RMSE(model, X_train, y_train)))\n    print('Optimized test score: {:.5f}'.format(RMSE(model, X_test, y_test)))","d2a7469f":"# elastic_baseline()","3ae9f2a6":"def make_model(method, **kwargs):\n    if method == 'xgb':\n        model_baseline = XGBRegressor(random_state=42, n_jobs=-1)\n    elif method == 'lgb':\n        model_baseline = LGBMRegressor(random_state=42, n_jobs=-1)\n    elif method == 'elnet':\n        model_baseline = Pipeline([\n            ('imputer', Imputer()),\n            ('scaler', StandardScaler()),\n            ('regressor', ElasticNet(random_state=42))\n        ])\n    model = clone(model_baseline).set_params(**kwargs)\n    return model, model_baseline\n\n\ndef process_params(method, **kwargs):\n    kw = dict()\n    if method == 'xgb' or method == 'lgb':\n        kw['n_estimators'] = int(kwargs['n_estimators'] + 0.5)\n        kw['max_depth'] = int(kwargs['max_depth'] + 0.5)\n        kw['num_leaves'] = int(kwargs['num_leaves'] + 0.5)\n        kw['min_child_samples'] = int(kwargs['min_child_samples'] + 0.5)\n        kw['learning_rate'] = 10**kwargs['learning_rate']\n        kw['reg_alpha'] = 10**kwargs['reg_alpha']\n        kw['reg_lambda'] = 10**kwargs['reg_lambda']\n        kw['subsample'] = kwargs['subsample']\n        kw['colsample_bytree'] = kwargs['colsample_bytree']\n    elif method == 'elnet':\n        kw['imputer__strategy'] = 'mean' if kwargs['imputer__strategy'] <= 0.5 else 'median'\n        kw['regressor__alpha'] = 10**kwargs['regressor__alpha']\n        kw['regressor__l1_ratio'] = kwargs['regressor__l1_ratio']\n    return kw\n\n\ndef get_cv_score(X, y, method, cv=10, **kwargs):\n    kwargs = process_params(method, **kwargs)\n    model, _ = make_model(method, **kwargs)\n    scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_squared_error', n_jobs=-1)\n    return -np.sqrt(-scores.mean())\n\n\ndef optimize(X, y, method, n_iter=15, init_points=5, kappa=10):\n    def fun(**kwargs): \n        return get_cv_score(\n            X,\n            y,\n            method,\n            **kwargs\n        )\n\n\n    if method == 'xgb' or method == 'lgb':\n        pbounds={\n            'n_estimators': (100, 1000),\n            'max_depth': (3, 30),\n            'num_leaves': (8, 64),\n            'min_child_samples': (8, 64),\n            'learning_rate': (-2, -1),\n            'reg_alpha': (-3, 3),\n            'reg_lambda': (-3, 3),\n            'subsample': (0.1, 1),\n            'colsample_bytree': (0.1, 1)\n        }\n    elif method == 'elnet':\n        pbounds={\n            'imputer__strategy': (0, 1),\n            'regressor__alpha': (-3, 3),\n            'regressor__l1_ratio': (0, 1)\n        }\n    \n    \n    optimizer = BayesianOptimization(fun, pbounds, random_state=42)\n    optimizer.maximize(n_iter=n_iter, init_points=init_points, kappa=kappa)\n    print('\\tFinal result:\\n', optimizer.max)\n    return optimizer\n\n\ndef test_model(method, thresh=0, show_cols=False, **best_params):\n    def print_cols(cols):\n        print('[', end='')\n        for col in cols:\n            print(\"'{}'\".format(col), end=', ')\n        print(']')\n    \n    model, model_baseline = make_model(method, **best_params)\n    \n    model_baseline.fit(X_train, y_train)\n    print('Baseline train score: {:.5f}'.format(RMSE(model_baseline, X_train, y_train)))\n    print('Baseline test score: {:.5f}'.format(RMSE(model_baseline, X_test, y_test)))\n          \n    model.fit(X_train, y_train)\n    print('Optimized train score: {:.5f}'.format(RMSE(model, X_train, y_train)))\n    print('Optimized test score: {:.5f}'.format(RMSE(model, X_test, y_test)))\n    \n    if model == 'elnet':\n        cols = X_train.columns\n    else:\n        plt.hist(np.array(model.feature_importances_))\n        plt.hist(np.array(model.feature_importances_), bins=100)\n        imp = pd.Series(index=X_train.columns, data=model.feature_importances_)\n        imp = imp.sort_values(ascending=False)\n        cols = list((imp[imp > thresh]).index)\n        if show_cols:\n            print_cols(cols)\n    \n    return cols","0915f1cc":"opt = optimize(X_train, y_train, 'lgb')\nprint('Best CV-score: {:.5f}'.format(opt.max['target']))\nbest_params = process_params('lgb', **opt.max['params'])\nprint('Best params:', best_params)\n# best_params = params = {'n_estimators': 635, 'max_depth': 4, 'num_leaves': 61, 'min_child_samples': 8, 'subsample': 0.6181601499844832,\n#           'colsample_bytree': 0.17834063704632525, 'learning_rate': 0.04238876990190508, 'reg_alpha': 0.015776461541004157,\n#           'reg_lambda': 0.00102918700714727}\ncols = test_model('lgb', 0, **best_params)","cc0815a8":"def make_submission(model, df, target, use_exp=False, cols=None):\n    if cols is None:\n        cols = df.columns\n    model.fit(df[:n_train][cols], target)\n    test_pred = model.predict(df[n_train:][cols])\n    if use_exp:\n        test_pred = np.exp(test_pred)\n    output = pd.DataFrame({'Id': test_id, 'SalePrice': test_pred})\n    now = dt.now()\n    fname = 'subm_{}_{}__{}_{}.csv'.format(now.day, now.month, now.hour, now.minute)\n    output.to_csv(fname, index=False)","cd865d3f":"# model = Pipeline([\n#         ('imputer', Imputer(strategy='median')),\n#         ('scaler', StandardScaler()),\n#         ('regressor', ElasticNet(alpha=1.0, l1_ratio=0.3, random_state=42))\n#     ])\n# make_submission(model, df_onehot)\n\nmodel = Pipeline([\n    ('imputer', Imputer()),\n    ('scaler', StandardScaler()),\n    ('regressor', ElasticNet(random_state=42))\n])\nparams = {'imputer__strategy': 'mean', 'regressor__alpha': 0.1, 'regressor__l1_ratio': 0.05}\nmodel.set_params(**params)\nmake_submission(model, df_onehot, target_log, use_exp=True)\n\n# params = {'n_estimators': 975, 'max_depth': 22, 'num_leaves': 62, 'min_child_samples': 28, 'subsample': 0.37079047883509275,\n#           'colsample_bytree': 0.3908826388186797, 'learning_rate': 0.010903884523201108, 'reg_alpha': 0.03241110030999171,\n#           'reg_lambda': 0.9627001408471512}\n# cols1 = ['GrLivArea', 'TotalBsmtSF', 'LotArea', '1stFlrSF', 'GarageArea', 'BsmtFinSF1', 'LotFrontage', 'GarageYrBlt', 'BsmtUnfSF', 'YearBuilt', 'YearRemodAdd', 'OpenPorchSF', '2ndFlrSF', 'MasVnrArea', 'OverallQual', 'WoodDeckSF', 'OverallCond', 'MoSold', 'TotRmsAbvGrd', 'YrSold', 'MSSubClass', 'Fireplaces', 'GarageCars', 'BedroomAbvGr', 'BsmtFullBath', 'FullBath', 'BsmtExposure_No', 'HalfBath', 'SaleCondition_Normal', 'LandContour_Bnk', 'LotShape_Reg', 'BsmtQual_Gd', 'FireplaceQu_Gd', 'Functional_Typ', 'GarageType_Attchd', 'HouseStyle_1Story', 'BsmtFinType1_GLQ', 'Neighborhood_Crawfor', 'KitchenQual_Gd', 'KitchenQual_TA', 'BsmtExposure_Gd', 'SaleType_New', 'ScreenPorch', 'HeatingQC_Ex', 'LotShape_IR1', 'HouseStyle_2Story', 'EnclosedPorch', 'Neighborhood_NoRidge', 'GarageType_Detchd', 'BsmtFinType1_ALQ', 'Exterior1st_BrkFace', 'HeatingQC_TA', 'ExterQual_TA', 'MasVnrType_BrkFace', 'Condition1_Norm', 'MSZoning_RL', 'SaleCondition_Abnorml', 'BsmtFinSF2', 'LandContour_Lvl', 'CentralAir_N', 'GarageFinish_Unf', 'MSZoning_RM', 'BsmtQual_Ex', 'GarageFinish_RFn', 'ExterQual_Gd', 'KitchenQual_Ex', 'Neighborhood_Somerst', 'LotConfig_CulDSac', 'RoofStyle_Gable', 'Neighborhood_Edwards', 'BsmtQual_TA', 'BsmtFinType1_Unf', 'Neighborhood_NAmes', 'Exterior1st_VinylSd', 'Condition1_Artery', 'ExterCond_TA', 'SaleCondition_Partial', 'Foundation_PConc', 'Neighborhood_BrkSide', 'Neighborhood_OldTown', 'MasVnrType_None', 'KitchenAbvGr', 'BsmtCond_Fa', 'BsmtCond_TA', 'GarageFinish_Fin', 'CentralAir_Y', 'LandSlope_Gtl', 'GarageCond_TA', 'LotConfig_Inside', 'SaleType_WD', 'Foundation_CBlock', 'Exterior1st_MetalSd', 'FireplaceQu_TA', 'RoofStyle_Hip', 'Exterior2nd_VinylSd', 'PavedDrive_Y', 'Exterior1st_HdBoard', 'GarageQual_TA', 'ExterQual_Ex', 'Exterior1st_Plywood', 'Fence_GdWo', 'Foundation_BrkTil', 'BsmtExposure_Av', 'Exterior1st_Wd Sdng', 'MSZoning_FV', 'BldgType_1Fam', 'Exterior2nd_Wd Sdng', 'Exterior2nd_MetalSd', ]\n\n# params = {'n_estimators': 763, 'max_depth': 4, 'num_leaves': 62, 'min_child_samples': 8, 'subsample': 0.2936474414836979,\n#           'colsample_bytree': 0.13176165663335335, 'learning_rate': 0.03292552347864284, 'reg_alpha': 0.02023613728543995,\n#           'reg_lambda': 0.008391229998184617}\n# params = {'n_estimators': 635, 'max_depth': 4, 'num_leaves': 61, 'min_child_samples': 8, 'subsample': 0.6181601499844832,\n#           'colsample_bytree': 0.17834063704632525, 'learning_rate': 0.04238876990190508, 'reg_alpha': 0.015776461541004157,\n#           'reg_lambda': 0.00102918700714727}\n# model = LGBMRegressor(random_state=42, n_jobs=-1)\n\n# model.set_params(**params)\n# make_submission(model, df_onehot, cols)","18702124":"# Model Search","2fd28636":"# EDA"}}