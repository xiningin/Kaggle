{"cell_type":{"0cc40496":"code","6f7b3137":"code","524581f7":"code","60c1f41a":"code","be9aff54":"code","c3aaa136":"code","fa331288":"code","54ccea13":"code","e357d3c6":"code","dcdaff7b":"code","4c8872e4":"code","b0f46a07":"code","36037781":"markdown","422dc649":"markdown","780da855":"markdown","7d915ad6":"markdown","c40a4c5d":"markdown","7a2add2a":"markdown","c9c24a4d":"markdown","b3494f54":"markdown","170e98b0":"markdown"},"source":{"0cc40496":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom datetime import datetime\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\n\npd.set_option('display.max_columns', 200)","6f7b3137":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n","524581f7":"train_df = pd.read_csv('..\/input\/train.csv', engine='python')\ntest_df = pd.read_csv('..\/input\/test.csv', engine='python')\n\n#Experimenting with a small subset\ntrain_df = train_df[1:10000]","60c1f41a":"import subprocess\nprint((subprocess.check_output(\"lscpu\", shell=True).strip()).decode())","be9aff54":"# A parameter grid for XGBoost\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 5, 7, 10],\n        'learning_rate': [0.01, 0.02, 0.05]    \n        }","c3aaa136":"%%time \nfolds = 3\nparam_comb = 1\n\ntarget = 'target'\npredictors = train_df.columns.values.tolist()[2:]\n\nX = train_df[predictors]\nY = train_df[target]\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nxgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic', nthread=1)\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,Y), verbose=3, random_state=1001)\n\n# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nrandom_search.fit(X, Y)\ntimer(start_time) # timing ends here for \"start_time\" variable\n","fa331288":"!nvidia-smi","54ccea13":"%%time \nfolds = 3\nparam_comb = 1\n\ntarget = 'target'\npredictors = train_df.columns.values.tolist()[2:]\n\nX = train_df[predictors]\nY = train_df[target]\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nxgb = XGBClassifier(learning_rate=0.02, n_estimators=1000, objective='binary:logistic',\n                    silent=True, nthread=6, tree_method='gpu_hist', eval_metric='auc')\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,Y), verbose=3, random_state=1001 )\n\n# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nrandom_search.fit(X, Y)\ntimer(start_time) # timing ends here for \"start_time\" variable","e357d3c6":"%%time \n\nfolds = 3\nparam_comb = 20\n\ntarget = 'target'\npredictors = train_df.columns.values.tolist()[2:]\n\nX = train_df[predictors]\nY = train_df[target]\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nxgb = XGBClassifier(learning_rate=0.02, n_estimators=1000, objective='binary:logistic',\n                    silent=True, nthread=6, tree_method='gpu_hist', eval_metric='auc')\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,Y), verbose=3, random_state=1001 )\n\n# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nrandom_search.fit(X, Y)\ntimer(start_time) # timing ends here for \"start_time\" variable","dcdaff7b":"y_test = random_search.predict_proba(test_df[predictors])\ny_test.shape\n","4c8872e4":"sub_df = pd.DataFrame({\"ID_code\": test_df.ID_code.values, \"target\": y_test[:,1]})\nsub_df[:10]","b0f46a07":"sub_df.to_csv(\"xgboost_gpu_randomsearch.csv\", index=False)","36037781":"The GPU acceleration provide a 8x speedup. Now we can afford to perform a more thorough search with 20 random configurations.","422dc649":"<a id=\"1\"><\/a>\n## 2. Hyper-parameter search with CPU","780da855":"<a id=\"2\"><\/a>\n## 3. Submission","7d915ad6":"<a id=\"0\"><\/a> \n## 1. Loading the data","c40a4c5d":"We carry out a quick search on the CPU with 3-fold cross valiation and 1 random parameter combo. This takes ~60m for a single parameter combination. So in order to carry out a random search over 100 combinations, the estimated time will be ~100h.","7a2add2a":"The only change we need to make is to set `TREE_METHOD = 'gpu_hist'` when initializing Xgboost.","c9c24a4d":"# Accelerating hyper-parameter searching with GPU\n\nThis kernel perform a random hyper-parameter seach using the Xgboost models. Since running time on CPU is prohibitively long, we accelerate the search with the K80 GPU available in Kaggle to achieve a 6x speed-up over the CPU.\n\nTo turn GPU support on in Kaggle, in notebook settings, set the \"GPU beta\" option to \"GPU on\". Xgboost provides out-of-the-box support for single GPU training. On a local workstation, a GPU-ready xgboost docker image can be obtained from https:\/\/hub.docker.com\/r\/rapidsai\/rapidsai\/.\n\n## Notebook  Content\n1. [Loading the data](#0) \n1. [Hyper-parameter search with CPU](#1)\n1. [Hyper-parameter search with GPU](#2)\n1. [Submission](#3)\n\n","b3494f54":"We will accelerate hyper-parameter search with the K80 GPU available in Kaggle.","170e98b0":"<a id=\"2\"><\/a>\n## 3. Hyper-parameter search with GPU"}}