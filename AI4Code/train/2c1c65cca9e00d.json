{"cell_type":{"b5c2c325":"code","8ce08c2c":"code","6e9a1d9e":"code","fffbc2e1":"code","fa8946af":"code","24d7395b":"code","0be5c032":"code","58f0d109":"code","febfa75b":"code","3ed855fc":"code","768aa107":"code","41401ad6":"code","027f1326":"code","edd30074":"code","da1828f9":"markdown","08f823d8":"markdown"},"source":{"b5c2c325":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","8ce08c2c":"'''\n    Helper Function's\n'''\ndef get_roc_auc_score(Model, X, y):\n    predict = Model.predict(X)\n    predict = list(predict.argmax(1))\n    return roc_auc_score(y, predict)\n\ndef get_missing_vitals_df(df):\n    vital = [v.split('d1_')[1].split('_max')[0] for v in df[(df.columns.str.startswith('d1')&df.columns.str.endswith('max'))]]\n    vitals = []\n    vitals_h1_max = []\n    vitals_h1_min = []\n    vitals_d1_max = []\n    vitals_d1_min = []\n    for v in vital:\n        vitals.append(v)\n        vitals_h1_max.append(f'h1_{v}_max')\n        vitals_h1_min.append(f'h1_{v}_min')\n        vitals_d1_max.append(f'd1_{v}_max')\n        vitals_d1_min.append(f'd1_{v}_min')\n\n    temp = pd.DataFrame({\n        'vitals' : vitals,\n        'vitals_d1_max' : df[vitals_d1_max].isna().sum().values,\n        'vitals_d1_min' : df[vitals_d1_min].isna().sum().values,\n        'vitals_h1_max' : df[vitals_h1_max].isna().sum().values,\n        'vitals_h1_min' : df[vitals_h1_min].isna().sum().values}\n    ).set_index('vitals')\n    return temp\n\ndef gelu(x):\n    return 0.5*x*(1 + tf.tanh(tf.sqrt(2 \/ np.pi)*(x + 0.044715 * tf.pow(x,3))))\n\ndef clean_data(df):\n    df = df.drop(columns = ['Unnamed: 0','encounter_id','hospital_id','icu_id'])\n    col_cat = []\n    for col in df.columns:\n        if(df.dtypes[col]=='object'):\n            df[col] = df[col].fillna('Na')\n            df[col] = df[col].astype('str')\n            col_cat.append(col)\n        elif(Column_Dt_Mapping[col]=='binary'):\n            df[col] = df[col].fillna('2')\n            df[col] = df[col].astype('str')\n            col_cat.append(col)\n    for col in ['apache_3j_diagnosis','apache_2_diagnosis']:\n        col_cat.append(col)\n        df[col] = df[col].fillna('Na')\n        df[col] = df[col].astype('str')\n    \n    for col in ['height', 'weight', 'bmi']:\n        df[f'{col}_missing'] = np.where(df[f'{col}'].isna(),1,0)\n        df[col] = np.where((df[col].isna() & (df.gender == 'M')),df[df['gender']=='M'][col].mean(),df[col])\n        df[col] = np.where((df[col].isna() & (df.gender == 'F')),df[df['gender']=='F'][col].mean(),df[col])\n        df[col] = np.where((df[col].isna() & (df.gender == 'Na')),df[df['gender']=='Na'][col].mean(),df[col])\n\n    invasive_min_max = [v.split('h1_')[1].split('_invasive_min')[0] for v in df.columns[(df.columns.str.startswith('h1_') & df.columns.str.endswith('_invasive_min'))]]\n\n    for col in invasive_min_max:\n        df[f'h1_{col}_max_missing'] = np.where(df[f'h1_{col}_max'].isna(),1,0)\n        df[f'h1_{col}_min_missing'] = np.where(df[f'h1_{col}_min'].isna(),1,0)\n        df[f'd1_{col}_max_missing'] = np.where(df[f'd1_{col}_max'].isna(),1,0)\n        df[f'd1_{col}_min_missing'] = np.where(df[f'd1_{col}_min'].isna(),1,0)\n        df[f'h1_{col}_invasive_max_missing'] = np.where(df[f'h1_{col}_invasive_max'].isna(),1,0)\n        df[f'h1_{col}_invasive_min_missing'] = np.where(df[f'h1_{col}_invasive_min'].isna(),1,0)\n        df[f'h1_{col}_noninvasive_max_missing'] = np.where(df[f'h1_{col}_noninvasive_max'].isna(),1,0)\n        df[f'h1_{col}_noninvasive_min_missing'] = np.where(df[f'h1_{col}_noninvasive_min'].isna(),1,0)\n        df[f'd1_{col}_invasive_max_missing'] = np.where(df[f'd1_{col}_invasive_max'].isna(),1,0)\n        df[f'd1_{col}_invasive_min_missing'] = np.where(df[f'd1_{col}_invasive_min'].isna(),1,0)\n        df[f'd1_{col}_noninvasive_max_missing'] = np.where(df[f'd1_{col}_noninvasive_max'].isna(),1,0)\n        df[f'd1_{col}_noninvasive_min_missing'] = np.where(df[f'd1_{col}_noninvasive_min'].isna(),1,0)\n        df[f'h1_{col}_max'].fillna(df[f'h1_{col}_max'].median(),inplace=True)\n        df[f'h1_{col}_min'].fillna(df[f'h1_{col}_min'].median(),inplace=True)\n        df[f'd1_{col}_max'].fillna(df[f'd1_{col}_max'].median(),inplace=True)\n        df[f'd1_{col}_min'].fillna(df[f'd1_{col}_min'].median(),inplace=True)\n        df[f'h1_{col}_invasive_max'] = np.where(df[f'h1_{col}_invasive_max'].isna(),df[f'h1_{col}_max'],df[f'h1_{col}_invasive_max'])\n        df[f'h1_{col}_invasive_min'] = np.where(df[f'h1_{col}_invasive_min'].isna(),df[f'h1_{col}_min'],df[f'h1_{col}_invasive_min'])\n        df[f'h1_{col}_noninvasive_max'] = np.where(df[f'h1_{col}_noninvasive_max'].isna(),df[f'h1_{col}_max'],df[f'h1_{col}_noninvasive_max'])\n        df[f'h1_{col}_noninvasive_min'] = np.where(df[f'h1_{col}_noninvasive_min'].isna(),df[f'h1_{col}_min'],df[f'h1_{col}_noninvasive_min'])\n        df[f'd1_{col}_invasive_max'] = np.where(df[f'd1_{col}_invasive_max'].isna(),df[f'd1_{col}_max'],df[f'd1_{col}_invasive_max'])\n        df[f'd1_{col}_invasive_min'] = np.where(df[f'd1_{col}_invasive_min'].isna(),df[f'd1_{col}_min'],df[f'd1_{col}_invasive_min'])\n        df[f'd1_{col}_noninvasive_max'] = np.where(df[f'd1_{col}_noninvasive_max'].isna(),df[f'd1_{col}_max'],df[f'd1_{col}_noninvasive_max'])\n        df[f'd1_{col}_noninvasive_min'] = np.where(df[f'd1_{col}_noninvasive_min'].isna(),df[f'd1_{col}_min'],df[f'd1_{col}_noninvasive_min'])\n        \n        \n    for col in ['albumin','bilirubin','creatinine','glucose','hematocrit','resprate','sodium','temp','wbc','bun']:\n        df[f'd1_{col}_max_missing'] = np.where(df[f'd1_{col}_max'].isna(),1,0)\n        df[f'd1_{col}_min_missing'] = np.where(df[f'd1_{col}_min'].isna(),1,0)\n        df[f'h1_{col}_max_missing'] = np.where(df[f'h1_{col}_max'].isna(),1,0)\n        df[f'h1_{col}_min_missing'] = np.where(df[f'h1_{col}_min'].isna(),1,0)\n        df[f'{col}_apache_missing'] = np.where(df[f'{col}_apache'].isna(),1,0)\n        df[f'd1_{col}_max'].fillna(df[f'd1_{col}_max'].median(),inplace=True)\n        df[f'd1_{col}_min'].fillna(df[f'd1_{col}_min'].median(),inplace=True)\n        df[f'h1_{col}_max'] = np.where(df[f'h1_{col}_max'].isna(),df[f'd1_{col}_max'],df[f'h1_{col}_max'])\n        df[f'h1_{col}_min'] = np.where(df[f'h1_{col}_min'].isna(),df[f'd1_{col}_min'],df[f'h1_{col}_min'])\n        df[f'{col}_apache'] = np.where(df[f'{col}_apache'].isna(),df[f'd1_{col}_max'],df[f'{col}_apache'])\n        \n        \n    for col in ['spo2','calcium','hco3','hemaglobin','inr','lactate','platelets','potassium','arterial_pco2','arterial_ph','arterial_po2','heartrate','pao2fio2ratio']:\n        df[f'd1_{col}_max_missing'] = np.where(df[f'd1_{col}_max'].isna(),1,0)\n        df[f'd1_{col}_min_missing'] = np.where(df[f'd1_{col}_min'].isna(),1,0)    \n        df[f'h1_{col}_max_missing'] = np.where(df[f'h1_{col}_max'].isna(),1,0)\n        df[f'h1_{col}_min_missing'] = np.where(df[f'h1_{col}_min'].isna(),1,0)    \n        df[f'd1_{col}_max'].fillna(df[f'd1_{col}_max'].median(),inplace=True)\n        df[f'd1_{col}_min'].fillna(df[f'd1_{col}_min'].median(),inplace=True)\n        df[f'h1_{col}_max'] = np.where(df[f'h1_{col}_max'].isna(),df[f'd1_{col}_max'],df[f'h1_{col}_max'])\n        df[f'h1_{col}_min'] = np.where(df[f'h1_{col}_min'].isna(),df[f'd1_{col}_min'],df[f'h1_{col}_min']) \n        \n\n    for col in ['fio2','gcs_eyes','gcs_motor','gcs_verbal','heart_rate','map','urineoutput','paco2','paco2_for_ph','pao2','ph']:\n        df[f'{col}_apache_missing'] = np.where(df[f'{col}_apache'].isna(),1,0) \n        df[f'{col}_apache'].fillna(df[f'{col}_apache'].median(),inplace=True)\n        \n        \n    return col_cat, df","6e9a1d9e":"Base_dir = '..\/input\/widsdatathon2021\/'\ntrain = pd.read_csv(os.path.join(Base_dir+'TrainingWiDS2021.csv'))\ntest = pd.read_csv(os.path.join(Base_dir+'UnlabeledWiDS2021.csv'))\nDataDictionary = pd.read_csv(os.path.join(Base_dir,'DataDictionaryWiDS2021.csv'))","fffbc2e1":"test_clean = test.copy()\ntrain_clean = train.copy()\n\nColumn_Dt_Mapping = dict(zip(DataDictionary['Variable Name'],DataDictionary['Data Type']))\n\ntest_clean.at[8360,'apache_3j_diagnosis'] = np.nan   \ntest_clean.at[8494,'apache_3j_diagnosis'] = np.nan   \n\ntrain_clean['age'] = np.where(train_clean['age'].isna(),train_clean['age'].median(),train_clean['age'])\ntest_clean['age'] = test_clean['age'].astype('float64')\n\n_, train_clean = clean_data(train_clean)\ncol_cat, test_clean = clean_data(test_clean)\n\nfor col in col_cat:\n    le = LabelEncoder()\n    train_clean[col] = le.fit_transform(train_clean[col])\n    test_clean[col] = le.transform(test_clean[col])","fa8946af":"missing_columns_tr = {k: v for k, v in train_clean.isna().sum().items() if v}\nmissing_columns_te = {k: v for k, v in test_clean.isna().sum().items() if v}\nprint(f'No. of columns having missing items in training columns: {len(missing_columns_tr)}, in test columns: {len(missing_columns_te)}')","24d7395b":"_missing = [col for col in train_clean.columns if col.endswith('_missing') == True]\ncol_cat.extend(_missing)\n_missing.append('diabetes_mellitus')","0be5c032":"for col in train_clean.columns:\n    if((train_clean[col].nunique() == 1)and(train_clean[col].dtype!='float64')):\n        print(col)","58f0d109":"train_clean = train_clean.drop('readmission_status',1)\ntest_clean = test_clean.drop('readmission_status',1)\ncol_cat.remove('readmission_status')\ncol_cont = [col for col in train_clean.columns if col not in col_cat]\ncol_cont.remove('diabetes_mellitus')","febfa75b":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Model, load_model\nfrom keras.layers import Dropout, Dense, BatchNormalization, Embedding, Input, Concatenate, SpatialDropout1D, Reshape, Flatten, concatenate, Activation, LeakyReLU\nfrom keras.metrics import AUC\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.utils import get_custom_objects\nfrom keras.optimizers import Adam, SGD\nfrom keras.losses import binary_crossentropy\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras import backend as K\nget_custom_objects().update({'gelu': Activation(gelu)})\nget_custom_objects().update({'leaky-relu': Activation(LeakyReLU(alpha=0.2))})\n\n\n\nCFG = {\n    'feature_selection_dropout' : 0.2,\n    'categorical_dropout' : 0.1,\n    'first_dense' : 256,\n    'second_dense' : 256,\n    'dense_dropout' : 0.2,\n    'activation_type' : 'leaky-relu',\n    'activation' : 'sigmoid',\n    'epochs' : 200,\n    'loss' : 'binary_crossentropy',\n    'optimizer' : Adam(learning_rate=0.0003),\n    'mon_metrics' : 'val_auc',\n    'num_folds' : 5\n      }","3ed855fc":"def dnn_embedding(feature_selection_dropout = CFG['feature_selection_dropout'], categorical_dropout = CFG['categorical_dropout'],\n                first_dense = CFG['first_dense'], second_dense = CFG['second_dense'], dense_dropout = CFG['dense_dropout'], \n                activation_type = CFG['activation_type'], activation = CFG['activation']):\n\n    inputs = []\n    embeddings = []\n\n    for category in  col_cat:\n        categorical_inputs = Input(shape=[1], name=category)\n        num_unique_vals = int(train_clean[category].nunique())\n        embed_dim = int(min(np.ceil(num_unique_vals \/ 2), 50)) \n        categorical_outputs = Embedding(num_unique_vals+1, \n                      embed_dim, \n                      name = category + \"_embed\")(categorical_inputs)\n        categorical_outputs = SpatialDropout1D(categorical_dropout)(categorical_outputs)\n        categorical_outputs = Reshape(target_shape=(embed_dim,))(categorical_outputs)\n        inputs.append(categorical_inputs)\n        embeddings.append(categorical_outputs)\n        \n        \n    numerical_inputs = Input(shape=(len(col_cont),))\n    numerical_normalization = BatchNormalization()(numerical_inputs)\n    #numerical_feature_selection = Dropout(feature_selection_dropout)(numerical_normalization)\n    inputs.append(numerical_inputs)\n    embeddings.append(numerical_normalization)    \n    \n    x = concatenate(embeddings)\n    x = Dense(first_dense, activation=activation_type)(x)\n    x = Dropout(dense_dropout)(x)\n    x = Dense(second_dense, activation=activation_type)(x)\n    x = Dropout(dense_dropout)(x)\n   \n    y = Dense(1, activation=activation)(x)\n    model = Model(inputs = inputs , outputs = y)\n    \n    return model ","768aa107":"dnn_embedding().summary()","41401ad6":"oof_preds = np.zeros((len(train)))\ntest_preds = np.zeros((len(test)))\n\ny = train_clean['diabetes_mellitus']\ny = y.astype(int)\n#train_clean = train_clean.drop('diabetes_mellitus',1)\nX = train_clean.drop('diabetes_mellitus',1)\n\nskf = StratifiedKFold(n_splits=CFG['num_folds'], shuffle = True, random_state=42)\n\nfor folds, (tdx, vdx) in enumerate(skf.split(X, y.values)):\n    X_train, X_valid, y_train, y_valid = X.iloc[tdx], X.iloc[vdx], y[tdx], y[vdx]\n\n    test_data = test_clean.copy()\n    pt = PowerTransformer(method='yeo-johnson',standardize=True)\n    #qt = QuantileTransformer(n_quantiles=10, output_distribution='normal',random_state=0)\n    scaler = StandardScaler()\n    vt = VarianceThreshold(threshold=0.1)\n    \n    X_train[col_cont] = pt.fit_transform(X_train[col_cont])\n    X_valid[col_cont] = pt.transform(X_valid[col_cont])\n    test_data[col_cont] = pt.transform(test_data[col_cont])\n\n    \n    X_train[col_cont] = scaler.fit_transform(X_train[col_cont])\n    X_valid[col_cont] = scaler.transform(X_valid[col_cont])\n    test_data[col_cont] = scaler.transform(test_data[col_cont])\n\n    X_train = [np.absolute(X_train[col]) for col in col_cat] + [X_train[col_cont]]\n    X_valid = [np.absolute(X_valid[col]) for col in col_cat] + [X_valid[col_cont]]\n    test_data = [np.absolute(test_data[col]) for col in col_cat] + [test_data[col_cont]]\n    \n    model = dnn_embedding()\n\n\n    model.compile(loss = CFG['loss'], optimizer = CFG['optimizer'], metrics = ['accuracy','AUC'])\n\n    es = EarlyStopping(monitor = CFG['mon_metrics'], min_delta = 0.001, patience = 20,\n                    verbose = 1, mode = 'max', baseline = None, restore_best_weights = True)\n\n    rlr = ReduceLROnPlateau(monitor=CFG['mon_metrics'], factor = 0.5, patience=5, mode='max', verbose=1, min_lr = 1e-6)\n    \n                                        \n    model.fit(X_train,\n              y_train,\n              validation_data = (X_valid, y_valid),\n              verbose = 1,\n              batch_size = 5024,\n              callbacks=[es],\n              epochs=CFG['epochs']\n             )\n\n    valid_fold_preds = model.predict(X_valid)\n    test_fold_preds = model.predict(test_data)\n    oof_preds[vdx] = valid_fold_preds.ravel()\n    test_preds += test_fold_preds.ravel()\n    print(f'Fold: {str(folds)}, AUC: {roc_auc_score(y_valid, valid_fold_preds)}')\n    K.clear_session()","027f1326":"AUC_FINAL = roc_auc_score(y.values, oof_preds)\ntest_predictions = test_preds\/(folds+1)\nprint(f'Overall AUC ROC: {AUC_FINAL}')","edd30074":"Base_dir = '..\/input\/widsdatathon2021\/'\nUnlabeled1 = pd.read_csv(os.path.join(Base_dir,'UnlabeledWiDS2021.csv'))\nsubmit = Unlabeled1[['encounter_id']]\nsubmit['diabetes_mellitus'] = test_predictions\nsubmit.to_csv('ann_embedded.csv',index=False)\nsubmit.head()","da1828f9":"## Imputing Missing Values","08f823d8":"This is the youtube link to understand the meaning behind all I have done here in this video\nSpeaker: **Luca Massaron - Google Developer Expert**\n<br>Here is a link to his linkedin https:\/\/www.linkedin.com\/in\/lmassaron\/<br>\nHe does a great job of explaining how to use NN for tabular data and is a must watch\nhttps:\/\/www.youtube.com\/watch?v=nQgUt_uADSE"}}