{"cell_type":{"ff91f707":"code","f0467a00":"code","7ae0cbd1":"code","9542f484":"code","a441fe59":"code","dd49c8c9":"code","540fd320":"code","f10adacb":"code","11911c12":"code","8a3f7fb8":"code","94020ecc":"code","9d17ff44":"code","387ed7f3":"code","4a6670a7":"code","cf5c5414":"code","82133de1":"code","de20612b":"code","a9ea5f15":"code","4ad5ca3f":"code","859ca8e2":"code","beb0def0":"code","82bf3484":"code","4ff3fd7a":"code","a28f17dd":"code","8bed2230":"code","e7af420e":"code","2ea40b92":"code","ee105214":"code","2d2d4f7d":"code","6c177f19":"code","827fc5bc":"code","d59e3a93":"code","6c3558cc":"markdown","e79e1030":"markdown","b53dfea7":"markdown","8bc66862":"markdown","10aceee1":"markdown","ac0e921e":"markdown","ccdd5fdc":"markdown","ad35c840":"markdown","a7a4961d":"markdown","56c48929":"markdown","005dfb12":"markdown","edfad2af":"markdown","d121088b":"markdown","9e24ea4a":"markdown","b8834145":"markdown","824f9a0c":"markdown","8f92e0b4":"markdown","5126ec66":"markdown","27f19718":"markdown","9d1313ff":"markdown","18066d4c":"markdown","71c78736":"markdown","14bd8b12":"markdown","663f2581":"markdown","c81398f1":"markdown","f45fa899":"markdown","d6265c2a":"markdown","5081e0b0":"markdown"},"source":{"ff91f707":"!mkdir data","f0467a00":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nfrom wordcloud import WordCloud, STOPWORDS\nimport tensorflow as tf\nimport missingno as msno\nfrom collections import defaultdict\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport json\n\n%matplotlib inline","7ae0cbd1":"train = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv\")","9542f484":"train.shape, test.shape","a441fe59":"# Finding the missing values\nfig, axes = plt.subplots(1, 2, figsize=(15,6))\nplt.tight_layout(pad = 10)\nmsno.bar(train, ax = axes[0])\nmsno.bar(test, ax = axes[1])","dd49c8c9":"print(train[train[\"text\"].isnull() == True])\n# We can drop this row\ntrain.dropna(inplace = True)","540fd320":"fig, axes = plt.subplots(1, 2, figsize=(15,6))\nplt.tight_layout(pad = 10)\nmsno.bar(train, ax = axes[0])\nmsno.bar(test, ax = axes[1])","f10adacb":"# Distribution of sentiment class\nfig, axes = plt.subplots(1, 2, figsize=(15,8))\nfig.suptitle(\"Comparing Ratio of Neutral Negative and Positive in train and test data\", fontsize = 25)\nplt.tight_layout(pad = 3.5)\nsns.countplot(x = \"sentiment\", data = train, ax = axes[0])\nsns.countplot(x = \"sentiment\", data = test, ax = axes[1])\naxes[0].set_xlabel(\"Sentiment\", fontsize = 20)\naxes[0].set_ylabel(\"Count\", fontsize = 20)\naxes[1].set_xlabel(\"Sentiment\", fontsize = 20)\naxes[1].set_ylabel(\"Count\", fontsize = 20)\nsns.despine()","11911c12":"# Percentage of neutral, negative, positive words in train and test data\ndef pert_count(data, category):\n    return (len(data[data[\"sentiment\"] == category])\/len(data)) * 100\n\nprint(f\"Percentage of neutral words in train --> {pert_count(train, 'neutral')}%\")\nprint(f\"Percentage of negative words in train --> {pert_count(train, 'negative')}%\")\nprint(f\"Percentage of positive words in train --> {pert_count(train, 'positive')}%\")\nprint(f\"Percentage of neutral words in test --> {pert_count(test, 'neutral')}%\")\nprint(f\"Percentage of negative words in test --> {pert_count(test, 'negative')}%\")\nprint(f\"Percentage of positive words in test --> {pert_count(test, 'positive')}%\")","8a3f7fb8":"# Length of words for in each category\ndef len_sent(data):\n    return len(data.split())\n\ntrain[\"num_words_text\"] = train[\"text\"].apply(lambda x : len_sent(x))\ntest[\"num_words_text\"] = test[\"text\"].apply(lambda x : len_sent(x))\ntrain[\"num_words_selected_text\"] = train[\"selected_text\"].apply(lambda x : len_sent(x))","94020ecc":"fig, axes = plt.subplots(3, 1, sharey = True, figsize = (15, 20))\nfig.suptitle(\"Comparing train and test data based on sentiment length\", fontsize = 25)\nsns.kdeplot(train[train[\"sentiment\"] == \"neutral\"][\"num_words_text\"].values, ax  = axes[0], shade = True, color = \"blue\", label = \"train\")\nsns.kdeplot(test[test[\"sentiment\"] == \"neutral\"][\"num_words_text\"].values, ax  = axes[0], shade = True, color = \"red\", label = \"test\")\nsns.kdeplot(train[train[\"sentiment\"] == \"negative\"][\"num_words_text\"].values, ax  = axes[1], shade = True, color = \"blue\", label = \"train\")\nsns.kdeplot(test[test[\"sentiment\"] == \"negative\"][\"num_words_text\"].values, ax  = axes[1], shade = True, color = \"red\", label = \"test\")\nsns.kdeplot(train[train[\"sentiment\"] == \"positive\"][\"num_words_text\"].values, ax  = axes[2], shade = True, color = \"blue\", label = \"train\")\nsns.kdeplot(test[test[\"sentiment\"] == \"positive\"][\"num_words_text\"].values, ax  = axes[2], shade = True, color = \"red\", label = \"test\")\naxes[0].set_xlabel(\"Sentiment_length_neutral\", fontsize = 20)\naxes[0].set_ylabel(\"Distribution\", fontsize = 20)\naxes[1].set_xlabel(\"Sentiment_length_negative\", fontsize = 20)\naxes[1].set_ylabel(\"Distribution\", fontsize = 20)\naxes[2].set_xlabel(\"Sentiment_length_positive\", fontsize = 20)\naxes[2].set_ylabel(\"Distribution\", fontsize = 20)\nplt.legend()","9d17ff44":"# Comparing test and selected text column\nfig, axes = plt.subplots(3, 1, sharey = True, figsize = (15, 20))\nfig.suptitle(\"\", fontsize = 25)\nsns.kdeplot(train[train[\"sentiment\"] == \"neutral\"][\"num_words_text\"].values, ax  = axes[0], shade = True, color = \"blue\", label = \"text\")\nsns.kdeplot(train[train[\"sentiment\"] == \"neutral\"][\"num_words_selected_text\"].values, ax  = axes[0], shade = True, color = \"red\", label = \"selected_text\")\nsns.kdeplot(train[train[\"sentiment\"] == \"negative\"][\"num_words_text\"].values, ax  = axes[1], shade = True, color = \"blue\", label = \"text\")\nsns.kdeplot(train[train[\"sentiment\"] == \"negative\"][\"num_words_selected_text\"].values, ax  = axes[1], shade = True, color = \"red\", label = \"selected_text\")\nsns.kdeplot(train[train[\"sentiment\"] == \"positive\"][\"num_words_text\"].values, ax  = axes[2], shade = True, color = \"blue\", label = \"text\")\nsns.kdeplot(train[train[\"sentiment\"] == \"positive\"][\"num_words_selected_text\"].values, ax  = axes[2], shade = True, color = \"red\", label = \"selected_text\")\naxes[0].set_xlabel(\"Sentiment_length_neutral\", fontsize = 20)\naxes[0].set_ylabel(\"Distribution\", fontsize = 20)\naxes[1].set_xlabel(\"Sentiment_length_negative\", fontsize = 20)\naxes[1].set_ylabel(\"Distribution\", fontsize = 20)\naxes[2].set_xlabel(\"Sentiment_length_positive\", fontsize = 20)\naxes[2].set_ylabel(\"Distribution\", fontsize = 20)\nplt.legend()","387ed7f3":"train[\"text\"] = train[\"text\"].apply(lambda x : x.strip())\ntrain[\"selected_text\"] = train[\"selected_text\"].apply(lambda x : x.strip())\n\n\ntrain[\"is_equal\"] = (train[\"text\"] == train[\"selected_text\"])\ndf_neutral = train[train[\"sentiment\"] == \"neutral\"]\npercentage = (len(df_neutral[df_neutral[\"is_equal\"] == True])\/len(df_neutral)) * 100\nprint(f\"Percentage of text column sentences is equal selected_text column for neutral sentiment --> {percentage}\")","4a6670a7":"# Punctuation count in train[\"text\"], train[\"selected_text\"]\ndef punc_count(data):\n    return len([w for w in data if w in string.punctuation])\n\ntrain[\"text_punc_count\"] = train[\"text\"].apply(lambda x : punc_count(x))\ntrain[\"selected_text_punc_count\"] = train[\"selected_text\"].apply(lambda x : punc_count(x))","cf5c5414":"plt.figure(figsize = (15, 8))\nsns.kdeplot(train[\"text_punc_count\"].values, shade = True, color = \"blue\", label = \"Text punc count\")\nsns.kdeplot(train[\"selected_text_punc_count\"].values, shade = True, color = \"yellow\", label = \"Selected text punc count\")\nplt.title(\"Punctuation Count\", fontsize = 30)\nplt.xlabel(\"Punctuation Count\", fontsize = 20)\nplt.ylabel(\"Distribution\", fontsize = 20)\nsns.despine()\nplt.legend(loc = \"lower right\")","82133de1":"# Most repeated words in text column and selected_text\nstopwords = set(STOPWORDS)\ndef word_cloud(data, title):\n    wordcloud = WordCloud(\n    background_color = \"black\",\n    max_font_size = 40,\n    max_words = 200,\n    stopwords = stopwords,\n    scale = 3).generate(str(data))\n    fig = plt.figure(figsize = (15, 15))\n    plt.axis(\"off\")\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.25)\n\n    plt.imshow(wordcloud)\n    plt.show()","de20612b":"word_cloud(train[\"text\"], \"Most Repeated words in train['text']\")","a9ea5f15":"word_cloud(train[train[\"sentiment\"] == \"neutral\"][\"text\"], \"Most Repeated words in neural sentences train['text']\")","4ad5ca3f":"word_cloud(train[train[\"sentiment\"] == \"negative\"][\"text\"], \"Most Repeated words in negative sentences train['text']\")","859ca8e2":"word_cloud(train[train[\"sentiment\"] == \"positive\"][\"text\"], \"Most Repeated words in positive sentences train['text']\")","beb0def0":"# N-Grams for neutral, positive negative sentences\ndef n_grams(ngram, data):\n    freq_dict = defaultdict(int)\n    for text in data:\n        tokens = [w for w in text.lower().split() if w != \" \" if w not in stopwords]\n        ngrams = zip(*[tokens[i:] for i in range(ngram)])\n        list_grams = [\" \".join(ngram) for ngram in ngrams]\n        for word in list_grams:\n            freq_dict[word] += 1\n    fd_sorted =  pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])   \n    fd_sorted.columns = [\"word\", \"wordcount\"]\n    return fd_sorted\n                    \nfd_sorted_neutral1 = n_grams(1, train[train[\"sentiment\"] == \"neutral\"][\"text\"])    \nfd_sorted_negative1 = n_grams(1, train[train[\"sentiment\"] == \"negative\"][\"text\"])    \nfd_sorted_positive1 = n_grams(1, train[train[\"sentiment\"] == \"positive\"][\"text\"]) \n\nfd_sorted_neutral2 = n_grams(2, train[train[\"sentiment\"] == \"neutral\"][\"text\"])    \nfd_sorted_negative2 = n_grams(2, train[train[\"sentiment\"] == \"negative\"][\"text\"])    \nfd_sorted_positive2 = n_grams(2, train[train[\"sentiment\"] == \"positive\"][\"text\"]) \n\nfd_sorted_neutral3 = n_grams(3, train[train[\"sentiment\"] == \"neutral\"][\"text\"])    \nfd_sorted_negative3 = n_grams(3, train[train[\"sentiment\"] == \"negative\"][\"text\"])    \nfd_sorted_positive3 = n_grams(3, train[train[\"sentiment\"] == \"positive\"][\"text\"])","82bf3484":"fig, axes = plt.subplots(3, 1, figsize = (10, 18))\nplt.tight_layout(pad = 7.5)\nplt.suptitle(\"Unigrams\", fontsize = 25)\nsns.despine()\nl = [\"neutral\", \"negative\", \"positive\"]\nfor i in range(3):\n    sns.barplot(x = \"wordcount\", y = \"word\", data = globals()[\"fd_sorted_\" + str(l[i]) + str(1)].iloc[:20, :], ax = axes[i])\n    axes[i].set_title(f\"Most repeated words in {l[i]} sentences\", fontsize = 20)\n    axes[i].set_xlabel(\"WordCount\", fontsize = 15)    \n    axes[i].set_ylabel(\"Word\", fontsize = 15)","4ff3fd7a":"fig, axes = plt.subplots(3, 1, figsize = (10, 18))\nplt.tight_layout(pad = 7.5)\nplt.suptitle(\"Bigrams\", fontsize = 25)\nsns.despine()\nl = [\"neutral\", \"negative\", \"positive\"]\nfor i in range(3):\n    sns.barplot(x = \"wordcount\", y = \"word\", data = globals()[\"fd_sorted_\" + str(l[i]) + str(2)].iloc[:20, :], ax = axes[i])\n    axes[i].set_title(f\"Most repeated words in {l[i]} sentences\", fontsize = 20)\n    axes[i].set_xlabel(\"WordCount\", fontsize = 15)    \n    axes[i].set_ylabel(\"Word\", fontsize = 15)","a28f17dd":"fig, axes = plt.subplots(3, 1, figsize = (10, 18))\nplt.tight_layout(pad = 7.5)\nplt.suptitle(\"Trigrams\", fontsize = 25)\nsns.despine()\nl = [\"neutral\", \"negative\", \"positive\"]\nfor i in range(3):\n    sns.barplot(x = \"wordcount\", y = \"word\", data = globals()[\"fd_sorted_\" + str(l[i]) + str(3)].iloc[:20, :], ax = axes[i])\n    axes[i].set_title(f\"Most repeated words in {l[i]} sentences\", fontsize = 20)\n    axes[i].set_xlabel(\"WordCount\", fontsize = 15)    \n    axes[i].set_ylabel(\"Word\", fontsize = 15)","8bed2230":"train_array = np.array(train.iloc[:, :4])\ntest_array = np.array(test.iloc[:, :3])\nuse_cuda = True","e7af420e":"# Getting starting index of selected_text found in text\ndef start_index(text, selected_text):\n    start_index = text.lower().find(selected_text.lower())\n    l.append(start_index)\n    \nl = []\nfor i in range(len(train_array)):\n    start_index(train_array[i, 1], train_array[i, 2])","2ea40b92":"# We are taking\n# question --> sentiment\n# context --> text\n# answer --> selected_text\n\ndef quesa_format_train(train):\n    out = []\n    for i, row in enumerate(train):\n        qas = []\n        con = []\n        ans = []\n        question = row[-1]\n        answer = row[2]\n        context = row[1]\n        qid = row[0]\n        answer_start = l[i]\n        ans.append({\"answer_start\": answer_start, \"text\": answer.lower()})\n        qas.append({\"question\": question, \"id\": qid, \"is_impossible\": False, \"answers\": ans})\n        out.append({\"context\": context.lower(), \"qas\": qas})\n\n    return out\n        \n    \ntrain_json_format = quesa_format_train(train_array)\nwith open('data\/train.json', 'w') as outfile:\n    json.dump(train_json_format, outfile)","ee105214":"# Similarly for text data\n\ndef quesa_format_test(train):\n    out = []\n    for i, row in enumerate(train):\n        qas = []\n        con = []\n        ans = []\n        question = row[-1]\n#         answer = row[2]\n        context = row[1]\n        qid = row[0]\n        answer_start = l[i]\n        ans.append({\"answer_start\": 1000000, \"text\": \"__None__\"})\n        qas.append({\"question\": question, \"id\": qid, \"is_impossible\": False, \"answers\": ans})\n        out.append({\"context\": context.lower(), \"qas\": qas})\n    return out\n        \n    \ntest_json_format = quesa_format_test(test_array)\n\nwith open('data\/test.json', 'w') as outfile:\n    json.dump(test_json_format, outfile)","2d2d4f7d":"!pip install '\/kaggle\/input\/train-requirements\/seqeval-0.0.12-py3-none-any.whl' -q\n!pip install '\/kaggle\/input\/train-requirements\/simpletransformers-0.22.1-py3-none-any.whl' -q","6c177f19":"from simpletransformers.question_answering import QuestionAnsweringModel\n\nmodel_path = '\/kaggle\/input\/transformers-pretrained-distilbert\/distilbert-base-uncased-distilled-squad\/'\n# MODEL_PATH = QuestionAnsweringModel.from_pretrained('distilbert-base-uncased-distilled-squad'\n\n# Create the QuestionAnsweringModel\nmodel = QuestionAnsweringModel('distilbert', \n                               model_path, \n                               args={'reprocess_input_data': True,\n                                     'overwrite_output_dir': True,\n                                     'learning_rate': 5e-5,\n                                     'num_train_epochs': 4,\n                                     'max_seq_length': 128,\n                                     'doc_stride': 64,\n                                     'fp16': False,\n                                    },\n                              use_cuda=use_cuda)\n\nmodel.train_model('data\/train.json')","827fc5bc":"pred = model.predict(test_json_format)","d59e3a93":"df = pd.DataFrame.from_dict(pred)\nsample_submission[\"selected_text\"] = df[\"answer\"]\n# new_df = sample_submission.merge(test,how=\"inner\",on=\"textID\")\n# new_df[\"selected_text\"] = np.where((new_df[\"sentiment\"] == \"neutral\"),new_df[\"text\"], new_df[\"selected_text\"])\n# submission = new_df[[\"textID\", \"selected_text\"]]\nsample_submission.to_csv(\"submission.csv\", index = False)\nprint(\"File submitted successfully.\")","6c3558cc":"**2.1 Finding the missing values**","e79e1030":"# 5. Word Cloud","b53dfea7":" **1.2 Reading Data**","8bc66862":"# 3. Analysis of text and selected_text","10aceee1":"**Do upvote this kernal also. It helped me alot in BERT modelling**\n\nhttps:\/\/www.kaggle.com\/jonathanbesomi\/question-answering-starter-pack","ac0e921e":"# Evaluation Metric\n\nEvaluation metric here is **Jacard Score**. Which is also called Intersection over union and used mainly in CNN tasks. \n![](https:\/\/www.displayr.com\/wp-content\/uploads\/2018\/09\/Jaccard-formula.png)\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/c\/c7\/Intersection_over_Union_-_visual_equation.png)","ccdd5fdc":"We can see from above graphs that for neutral sentences the distribution of length is approximately the same in text and selected_text column. For negative and positive sentiment the length of selected_text column is shorter than text column","ad35c840":"We can see from above distribution plots that distribution of word length is quite similar in train and test data for each sentiment category","a7a4961d":"There is one missing value in text and selected_text column in train data","56c48929":"**We can see from above analysis that the percentage of neutral, positive and negative words are quite similar. It may be due to the fact that train and test data came from same sample**","005dfb12":"# 7.1 Data preparation","edfad2af":"# 1. Dataset and Libraries","d121088b":"# 2. Exploratory Data Analysis","9e24ea4a":"<span style=\"color:green\">**Do upvote this kernel if you found this helpful. Stay tuned!, i will update this kernel regularly**<\/span>","b8834145":"No null value left in the train and test data","824f9a0c":"# 6. N Grams","8f92e0b4":"# 4. Text Statistics","5126ec66":"**4.2 Punctuation count**","27f19718":"**3.3 Comparing test and selected text column**","9d1313ff":" **4.1 Percentage of text column sentences is equal selected_text column for neutral sentiment**","18066d4c":"**1.1 Importing Libraries**","71c78736":"# 7 DistilBERT + SQuAD + Question Answering Technique","14bd8b12":"**As most of the train[\"text\"] data is same as train[\"selected_text\"] for neutral sentiment. So I will simply copy paste test[\"text\"] for neutral sentiment prediction","663f2581":"**3.1 Percentage of neutral, negative, positive words in train and test data**","c81398f1":"# Table of Content\n- 1. Dataset and Libraries\n   - 1.1 Importing libraries\n   - 1.2 Reading Data\n- 2. Exploratory Data Analysis\n   - 2.1 Finding the missing values\n   - 2.2 Distribution of sentiment class\n- 3. Analysis of text and selected_data\n   - 3.1 Percentage of neutral, negative, positive words in train and test data\n   - 3.2 Length of words for in each category\n   - 3.3 Comparing test and selected text column\n- 4. Text Statistics\n   - 4.1 Percentage of text column sentences is equal selected_text column for neutral sentiment\n   - 4.2 Punctuation Count\n- 5. Word Cloud\n- 6. N Grams\n- 7. DistilBERT + SQuAD + Question Answering Technique\n   - 7.1 Data Preprocessing","f45fa899":"# Problem Statement:\nThis problem is quite different than default NLP task in which we are asked to predict the sentiment of sentence. In this competition given, the polarity we are asked to determine\/select the words that are going to decide the polarity of the sentence.","d6265c2a":"**2.2 Distribution of missing class**","5081e0b0":"**3.2 Length of words for in each category**"}}