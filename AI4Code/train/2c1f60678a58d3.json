{"cell_type":{"0e43d4bb":"code","707746a9":"code","16b6f30a":"code","57b97d5d":"code","d5e34140":"code","6896e7b1":"code","11eb505f":"code","0876aa84":"code","3bb1ffb6":"code","f4cabd6b":"code","b2d80777":"code","adb28d50":"code","73db3f65":"code","2d3cc490":"code","943b4244":"code","23ce824b":"code","67c0fc18":"code","f4421288":"code","2976dd58":"markdown","526d098e":"markdown","6e9ad137":"markdown","b0bb57aa":"markdown","8227fa28":"markdown","d3824805":"markdown","00ea5ec7":"markdown","e02b835f":"markdown","30a04344":"markdown","67994eb5":"markdown","a3b04b03":"markdown","84a2e692":"markdown","eae21811":"markdown","a71d3ca2":"markdown","2337617c":"markdown","2e72471b":"markdown","be3bc3dc":"markdown","ed90f885":"markdown","68f5f88e":"markdown","4f532c4e":"markdown","a448b310":"markdown","5d332017":"markdown","f633fc0a":"markdown","3b85511b":"markdown","95eb5ae2":"markdown","b83ca458":"markdown","9aa5ec27":"markdown","61054db2":"markdown","c552d115":"markdown","ddfd77e5":"markdown","0c0f5655":"markdown","fbf0f4fe":"markdown"},"source":{"0e43d4bb":"# Imports\n\n# Import basic libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\nfrom PIL import Image\n\n# Import PyTorch\nimport torch # import main library\nfrom torch.autograd import Variable\nimport torch.nn as nn # import modules\nfrom torch import optim # import optimizers for demonstrations\nimport torch.nn.functional as F # import torch functions\nfrom torchvision import transforms # import transformations to use for demo\nfrom torch.utils.data import Dataset, DataLoader ","707746a9":"# Define a transform\ntransform = transforms.Compose([transforms.ToTensor()])","16b6f30a":"class FashionMNIST(Dataset):\n    '''\n    Dataset clas to load Fashion MNIST data from csv.\n    Code from original kernel:\n    https:\/\/www.kaggle.com\/arturlacerda\/pytorch-conditional-gan\n    '''\n    def __init__(self, transform=None):\n        self.transform = transform\n        fashion_df = pd.read_csv('..\/input\/fashion-mnist_train.csv')\n        self.labels = fashion_df.label.values\n        self.images = fashion_df.iloc[:, 1:].values.astype('uint8').reshape(-1, 28, 28)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        label = self.labels[idx]\n        img = Image.fromarray(self.images[idx])\n        \n        if self.transform:\n            img = self.transform(img)\n\n        return img, label\n\n# Load the training data for Fashion MNIST\ntrainset = FashionMNIST(transform=transform)\n# Define the dataloader\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)","57b97d5d":"def train_model(model, device):\n    '''\n    Function trains the model and prints out the training log.\n    '''\n    #setup training\n    \n    #define loss function\n    criterion = nn.NLLLoss()\n    #define learning rate\n    learning_rate = 0.003\n    #define number of epochs\n    epochs = 5\n    #initialize optimizer\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    model.to(device)\n\n    #run training and print out the loss to make sure that we are actually fitting to the training set\n    print('Training the model \\n')\n    for e in range(epochs):\n        running_loss = 0\n        for images, labels in trainloader:\n            \n            images, labels = images.to(device), labels.to(device)\n            images = images.view(images.shape[0], -1)\n            log_ps = model(images)\n            loss = criterion(log_ps, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        else:\n            # print out the loss to make sure it is decreasing\n            print(f\"Training loss: {running_loss}\")","d5e34140":"# create class for basic fully-connected deep neural network\nclass Classifier(nn.Module):\n    '''\n    Demo classifier model class to demonstrate in-place operations\n    '''\n    def __init__(self, inplace = False):\n        super().__init__()\n\n        # initialize layers\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 10)\n        \n        self.relu = nn.ReLU(inplace = inplace) # pass inplace as parameter to ReLU\n\n    def forward(self, x):\n        # make sure the input tensor is flattened\n        x = x.view(x.shape[0], -1)\n\n        # apply activation function\n        x = self.relu(self.fc1(x))\n\n        # apply activation function\n        x = self.relu(self.fc2(x))\n        \n        # apply activation function\n        x = self.relu(self.fc3(x))\n        \n        x = F.log_softmax(self.fc4(x), dim=1)\n\n        return x","6896e7b1":"# empty caches and setup the device\ntorch.cuda.empty_cache()\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\ndevice","11eb505f":"def get_memory_allocated(device, inplace = False):\n    '''\n    Function measures allocated memory before and after the ReLU function call.\n    '''\n    \n    # Create a large tensor\n    t = torch.randn(10000, 10000, device=device)\n    \n    # Measure allocated memory\n    torch.cuda.synchronize()\n    start_max_memory = torch.cuda.max_memory_allocated() \/ 1024**2\n    start_memory = torch.cuda.memory_allocated() \/ 1024**2\n    \n    # Call in-place or normal ReLU\n    if inplace:\n        F.relu_(t)\n    else:\n        output = F.relu(t)\n    \n    # Measure allocated memory after the call\n    torch.cuda.synchronize()\n    end_max_memory = torch.cuda.max_memory_allocated() \/ 1024**2\n    end_memory = torch.cuda.memory_allocated() \/ 1024**2\n    \n    # Return amount of memory allocated for ReLU call\n    return end_memory - start_memory, end_max_memory - start_max_memory","0876aa84":"memory_allocated, max_memory_allocated = get_memory_allocated(device, inplace = False)\nprint('Allocated memory: {}'.format(memory_allocated))\nprint('Allocated max memory: {}'.format(max_memory_allocated))","3bb1ffb6":"memory_allocated_inplace, max_memory_allocated_inplace = get_memory_allocated(device, inplace = True)\nprint('Allocated memory: {}'.format(memory_allocated_inplace))\nprint('Allocated max memory: {}'.format(max_memory_allocated_inplace))","f4cabd6b":"# initialize classifier\nmodel = Classifier(inplace = False)\n\n# measure allocated memory\ntorch.cuda.synchronize()\nstart_max_memory = torch.cuda.max_memory_allocated() \/ 1024**2\nstart_memory = torch.cuda.memory_allocated() \/ 1024**2\n\n# train the classifier\ntrain_model(model, device)\n\n# measure allocated memory after training\ntorch.cuda.synchronize()\nend_max_memory = torch.cuda.max_memory_allocated() \/ 1024**2\nend_memory = torch.cuda.memory_allocated() \/ 1024**2","b2d80777":"print('Allocated memory: {}'.format(end_memory - start_memory))\nprint('Allocated max memory: {}'.format(end_max_memory - start_max_memory))","adb28d50":"# initialize model with in-place ReLU\nmodel = Classifier(inplace = True)\n\n# measure allocated memory\ntorch.cuda.synchronize()\nstart_max_memory = torch.cuda.max_memory_allocated() \/ 1024**2\nstart_memory = torch.cuda.memory_allocated() \/ 1024**2\n\n# train the classifier with in-place ReLU\ntrain_model(model, device)\n\n# measure allocated memory after training\ntorch.cuda.synchronize()\nend_max_memory = torch.cuda.max_memory_allocated() \/ 1024**2\nend_memory = torch.cuda.memory_allocated() \/ 1024**2","73db3f65":"print('Allocated memory: {}'.format(end_memory - start_memory))\nprint('Allocated max memory: {}'.format(end_max_memory - start_max_memory))","2d3cc490":"def silu(input):\n    '''\n    Normal implementation of SiLU activation function\n    https:\/\/arxiv.org\/pdf\/1606.08415.pdf\n    '''\n    return input * torch.sigmoid(input)","943b4244":"def silu_inplace_1(input):\n    '''\n    Incorrect implementation of in-place SiLU activation function\n    https:\/\/arxiv.org\/pdf\/1606.08415.pdf\n    '''\n    return input * torch.sigmoid_(input) # THIS IS INCORRECT!!!","23ce824b":"t = torch.randn(3)\n\n# print result of original SiLU\nprint(\"Original SiLU: {}\".format(silu(t)))\n\n# change the value of t with in-place function\nsilu_inplace_1(t)\nprint(\"In-place SiLU: {}\".format(t))","67c0fc18":"def silu_inplace_2(input):\n    '''\n    Example of implementation of in-place SiLU activation function using torch.sigmoid_\n    https:\/\/arxiv.org\/pdf\/1606.08415.pdf\n    '''\n    result = input.clone()\n    torch.sigmoid_(input)\n    input *= result\n    return input","f4421288":"t = torch.randn(3)\n\n# print result of original SiLU\nprint(\"Original SiLU: {}\".format(silu(t)))\n\n# change the value of t with in-place function\nsilu_inplace_2(t)\nprint(\"In-place SiLU #2: {}\".format(t))","2976dd58":"Run in-place ReLU:","526d098e":"The code above __incorrectly__ implements in-place SiLU. We can make sure of that:","6e9ad137":"[_Photo by Fancycrave.com from Pexels_](https:\/\/www.pexels.com\/photo\/green-ram-card-collection-825262\/)","b0bb57aa":"## Additional References\nLinks to the additional resources and further reading:\n\n1. [PyTorch Autograd documentation](https:\/\/pytorch.org\/docs\/stable\/notes\/autograd.html#in-place-operations-with-autograd)","8227fa28":"Today's advanced deep neural networks have millions of parameters (for example, see the comparison in [this paper](https:\/\/arxiv.org\/pdf\/1905.11946.pdf)) and trying to train them on free GPU's like Kaggle or Goggle Colab often leads to running out of memory on GPU. There are several simple ways to reduce the GPU memory occupied by the model, for example:\n* Consider changing the architecture of the model or using the type of model with fewer parameters (for example choose [DenseNet](https:\/\/arxiv.org\/pdf\/1608.06993.pdf)-121 over DenseNet-169). This approach can affect model's performance metrics.\n* Reduce the batch size or manually set the number of data loader workers. In this case it will take longer for the model to train.\n\nUsing in-place operations in neural networks may help to avoid the downsides of approaches mentioned above while saving some GPU memory. However it is strongly __not recommended to use in-place operations__ for several reasons.\n\nIn this kernel I would like to:\n* Describe what are the in-place operations and demonstrate how they might help to save the GPU memory.\n* Tell why we should avoid the in-place operations or use them with great caution.","d3824805":"### Load the Data","00ea5ec7":"## Seeting Up The Demo\nIh this section I will prepare everything for the demonstration:\n* Load Fashion MNIST dataset from PyTorch,\n* Introduce transformations for Fashion MNIST images using PyTorch,\n* Prepare model training procedure.\n\nIf you are familiar with PyTorch basics, just skip this part and go straight to the rest of the kernel.","e02b835f":"### Setup Training Procedure\nI wrote a small training procedure, which runs 5 training epochs and prints the loss for each epoch:","30a04344":"Run training with in-place ReLU:","67994eb5":"Let's compare memory usage for one single call of ReLU activation function:","a3b04b03":"## In-place Operations\n`In-place operation is an operation that changes directly the content of a given linear algebra, vector, matrices(Tensor) without making a copy. The operators which helps to do the operation is called in-place operator.` See the [tutorial](https:\/\/www.tutorialspoint.com\/inplace-operator-in-python) on in-place operations in Python.\n\nAs it is said in the definition, in-place operations don't make a copy of the input, that is why they can help to reduce the memory usage, when operating with high-dimentional data.","84a2e692":"## PS\n![echo logo](https:\/\/github.com\/Lexie88rus\/Activation-functions-examples-pytorch\/blob\/master\/assets\/echo_logo.png?raw=true)\n\nI participate in implementation of a __Echo package__ with mathematical backend for neural networks, which can be used with most popular existing packages (TensorFlow, Keras and [PyTorch](https:\/\/pytorch.org\/)). We have done a lot for PyTorch and Keras so far. Here is a [link to a repository on GitHub](https:\/\/github.com\/digantamisra98\/Echo\/tree\/Dev-adeis), __I will highly appreciate your feedback__ on that.","eae21811":"It is easy to see that the function `silu_inplace_1` in fact returns `sigmoid(input) * sigmoid(input)` !","a71d3ca2":"Run out of place ReLU:","2337617c":"The other reason of being careful with in-place operations is that their implementation is extremely tricky. That is why I would __recommend to use PyTorch standard in-place operations__  (like `torch.tanh_` or `torch.sigmoid_`) instead of implementing one manually.\n\nLet's see an example of [SiLU](https:\/\/arxiv.org\/pdf\/1606.08415.pdf) (or Swish-1) activation function. This is the normal implementation of SiLU:","2e72471b":"To load the data I used standard Dataset and Dataloader classes from PyTorch and [FashionMNIST class code from this kernel](https:\/\/www.kaggle.com\/arturlacerda\/pytorch-conditional-gan):","be3bc3dc":"## Downsides of In-place Operations","ed90f885":"The working example of the in-place implementation of SiLU using `torch.sigmoid_` could be:","68f5f88e":"I would like to run a simple model on [Fashion MNIST dataset](https:\/\/www.kaggle.com\/zalando-research\/fashionmnist) to demonstrate how in-place operations help to consume less GPU memory. In this demonstration I use simple fully-connected deep neural network with four linear layers and [ReLU](https:\/\/pytorch.org\/docs\/stable\/nn.html#relu) activation after each hidden layer.","4f532c4e":"![image](https:\/\/github.com\/Lexie88rus\/Activation-functions-examples-pytorch\/raw\/master\/assets\/background-card-chip.jpg)","a448b310":"## Conclusion\nIn this article: \n* I described the in-place operations and their purpose. Demonstrated how in-place operations help to __consume less GPU memory__.\n* Described the major __downsides of in-place operations__. One should be very careful about using them and check the result twice.","5d332017":"## Introduction\n","f633fc0a":"Looks like using in-place ReLU really helps us to save some GPU memory. But we should be __extremely cautious when using in-place operations and check twice__. In the next section I will show why.","3b85511b":"This small example demonstrates why we should be extremely careful and check twice when using the in-place operations.","95eb5ae2":"The major downside of in-place operations is the fact that __they might overwrite values required to compute gradients__ which means breaking the training procedure of the model. That is what [the official PyTorch autograd documentation](https:\/\/pytorch.org\/docs\/stable\/notes\/autograd.html#in-place-operations-with-autograd) says:\n> Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you\u2019re operating under heavy memory pressure, you might never need to use them.\n\n> There are two main reasons that limit the applicability of in-place operations:\n\n> 1. In-place operations can potentially overwrite values required to compute gradients.\n> 2. Every in-place operation actually requires the implementation to rewrite the computational graph. Out-of-place versions simply allocate new objects and keep references to the old graph, while in-place operations, require changing the creator of all inputs to the Function representing this operation. This can be tricky, especially if there are many Tensors that reference the same storage (e.g. created by indexing or transposing), and in-place functions will actually raise an error if the storage of modified inputs is referenced by any other Tensor.","b83ca458":"Let's try to implement in-place SiLU using torch.sigmoid_ in-place function:","9aa5ec27":"The most efficient way to transform the input data is to use buil-in PyTorch transformations:","61054db2":"## Compare Memory Usage for In-place and Vanilla Operations\n","c552d115":"Now let's do the same while training a simple classifier.\nRun training with vanilla ReLU:","ddfd77e5":"# In-Place Operations in PyTorch\n_What are they and why avoid them_","0c0f5655":"### Introduce Transformations","fbf0f4fe":"### Define the Model\n\nPyTorch provides us with in-place implementation of ReLU activation function. I will run consequently training with in-place ReLU implementation and with vanilla ReLU."}}