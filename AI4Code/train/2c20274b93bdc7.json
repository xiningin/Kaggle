{"cell_type":{"1c5332c0":"code","47e05e8b":"code","148754a0":"code","e4aecdba":"code","a9752147":"code","253ae64b":"code","98d9fe4b":"code","da2bfabb":"code","76a4b713":"code","4a48f811":"code","7831a439":"code","17736ec4":"code","007bb2d9":"code","3918ca15":"code","b8c5991c":"code","674b0b64":"code","5f895e21":"code","8fcf959d":"code","c309e6a6":"code","26bedeea":"code","38fcff67":"code","b2c5c524":"code","52271b1a":"code","0585d805":"code","96928cce":"code","5001ef74":"markdown","9f13b5b4":"markdown","6217156b":"markdown","40f49d71":"markdown","daae7186":"markdown","7f07e11e":"markdown","c683e638":"markdown","215a8377":"markdown","9baf7b33":"markdown","0334aa61":"markdown","818699c6":"markdown","91de65db":"markdown"},"source":{"1c5332c0":"import pandas as pd\nimport numpy as np\nimport seaborn as sns","47e05e8b":"pip install openpyxl","148754a0":"df=pd.read_excel('..\/input\/news-prediction\/News_category.xlsx')\ndf.head()","e4aecdba":"df.SECTION.value_counts().plot.bar()   #visualizing the target variables classes (multiclass)","a9752147":"df.isnull().mean()            #no null values","253ae64b":"df.describe()","98d9fe4b":"df.info()","da2bfabb":"import tensorflow as tf","76a4b713":"from tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\n\nimport nltk\nimport re\nfrom nltk.corpus import stopwords","4a48f811":"nltk.download('stopwords')","7831a439":"voc_size=6000","17736ec4":"from nltk.stem.porter import PorterStemmer\nps=PorterStemmer()\ncorpus=[]\nfor i in range(0,len(df['STORY'])):\n    news=re.sub('[^a-zA-Z]',' ',df['STORY'][i])\n    news=news.lower()\n    news=news.split()\n    news=[ps.stem(word) for word in news if not word in stopwords.words('english')]\n    news=' '.join(news)\n    corpus.append(news)","007bb2d9":"corpus","3918ca15":"onehot_repr=[one_hot(words,voc_size)for words in corpus]\nonehot_repr","b8c5991c":"sent_length=50\nembedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)","674b0b64":"embedded_docs[0]","5f895e21":"embedding_vector_features=40\nmodel=Sequential()\nfrom tensorflow.keras.layers import Dropout\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel.add(Dropout(0.7))\nmodel.add(LSTM(250))\nmodel.add(Dense(4,activation='softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","8fcf959d":"X_final=np.array(embedded_docs)","c309e6a6":"y_final=np.array(df.SECTION)","26bedeea":"from sklearn.model_selection import train_test_split","38fcff67":"X_train,X_test,y_train,y_test=train_test_split(X_final,y_final,test_size=0.3,random_state=42)","b2c5c524":"model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)","52271b1a":"y_pred=np.argmax(model.predict(X_test), axis=-1)","0585d805":"from sklearn.metrics import accuracy_score\n","96928cce":"accuracy_score(y_test,y_pred)","5001ef74":"We convert all the words into vectors based on the vocabulary size.","9f13b5b4":"I recently attempted a Data Science Hackathon where we had to predict the **category** of the **News Articles** i.e. sports, politics etc. The dataset comprised of two variables. The target variable was the SECTION and independent variable was the STORY.\n\nI solved this problem statement using **LSTM**. In this kernel, we will go through the entire workflow of the project. \nSo lets begin!!","6217156b":"# *5) LSTM Model and dropout regularization*","40f49d71":"**Please leave an upvote if you liked the kernel :)**","daae7186":"1. Importing Dataset and libraries\n2. Text Data Pre-processing\n3. One Hot representation of text\n4. Passing those One hot vectors to embedding layer\n5. Creating LSTM model\n6. Dropout Regularization\n7. Submission","7f07e11e":"# *2) Text Data pre-processing*","c683e638":"# Steps in the project","215a8377":"# *1) Importing Dataset and Libraries*","9baf7b33":"# INTRODUCTION","0334aa61":"In various applications of NLP like Amazon Alexa , Google translator etc. , text data is generally our input . We tend to go and preprocess each and every word of a sentence and convert into vectors . Our first job is convert all words to lower cases, remove stopwords, split them and apply stemming or lemmatization on them. We use NLTK library to perform all this.\n\nAftet that, we will convert those words into vectors by One Hot representation. We will define a vocabulary size first and each word will be replaced with a index based on vocab size. Later, we will pass these vectors to embedding layer.","818699c6":"# *4) Passing One hot vectors to embedding layer*","91de65db":"# *C) One Hot representation*"}}