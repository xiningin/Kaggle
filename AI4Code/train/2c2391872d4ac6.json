{"cell_type":{"e5ae05b1":"code","fee893c6":"code","529e1ccd":"code","a3e4198f":"code","7e0bfd55":"code","df304e08":"code","d4087a3a":"code","15007075":"code","6712f428":"code","6b7a716d":"code","8e722e64":"code","ba79b0af":"code","d7e2e001":"code","08aaccb5":"code","039f93e0":"code","b9557c84":"code","90174c43":"code","1c7f70a2":"code","6dfcfcf1":"code","2246a014":"code","079a8904":"code","7301663b":"code","590633cd":"code","e650da52":"code","9e83e8d5":"code","f012ed88":"code","c0684f7d":"code","bedd698b":"markdown","7f0e5e3c":"markdown","e078f37a":"markdown","f458ea7b":"markdown","5d1d47e0":"markdown","ca58cabc":"markdown","5e4c85c3":"markdown","6262782f":"markdown","0b522dbe":"markdown","3c60da44":"markdown","fab42cef":"markdown","52d37107":"markdown","e7ed82b7":"markdown","a6f87ab3":"markdown"},"source":{"e5ae05b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fee893c6":"train_data = pd.read_csv(\"\/kaggle\/input\/breastcancer\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/breastcancer\/test.csv\")","529e1ccd":"# displaying train data\ntrain_data.head()","a3e4198f":"X_train = train_data.iloc[:,1:-1] # removing id and target variables\ny_train = train_data.iloc[:,-1]","7e0bfd55":"X_train","df304e08":"y_train","d4087a3a":"# describing dataset\nX_train.info()","15007075":"X_train.describe()","6712f428":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_new = sc.fit_transform(X_train)","6b7a716d":"X_train_new","8e722e64":"y_train.value_counts()","ba79b0af":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0)\nclf.fit(X_train_new, y_train)","d7e2e001":"y_pred_lr = clf.predict(X_train_new)\ny_pred_lr","08aaccb5":"# evaluating model\nfrom sklearn.metrics import accuracy_score, classification_report\nprint(\"Accuracy:\", accuracy_score(y_train,y_pred_lr))\nprint(classification_report(y_train,y_pred_lr))","039f93e0":"from sklearn.svm import SVC\nsvc = SVC(random_state=0)\nsvc.fit(X_train_new,y_train)","b9557c84":"y_pred_svc = svc.predict(X_train_new)\ny_pred_svc","90174c43":"# evaluating model\nfrom sklearn.metrics import accuracy_score, classification_report\nprint(\"Accuracy:\", accuracy_score(y_train,y_pred_svc))\nprint(classification_report(y_train,y_pred_svc))","1c7f70a2":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(max_depth=5, random_state=0)\nrfc.fit(X_train_new,y_train)","6dfcfcf1":"y_pred_rfc = rfc.predict(X_train_new)\ny_pred_rfc","2246a014":"# evaluating model\nfrom sklearn.metrics import accuracy_score, classification_report\nprint(\"Accuracy:\", accuracy_score(y_train,y_pred_rfc))\nprint(classification_report(y_train,y_pred_rfc))","079a8904":"test_data ","7301663b":"id_col = test_data.iloc[:,0]\nid_col","590633cd":"X_test = test_data.iloc[:,1:]\nX_test","e650da52":"X_test_new = sc.transform(X_test)\nX_test_new","9e83e8d5":"y_test_pred = rfc.predict(X_test_new)\ny_test_pred","f012ed88":"data = pd.DataFrame(zip(id_col,y_test_pred),columns=['id','Cancer'])\ndata","c0684f7d":"data.to_csv(\"submission.csv\",index=False)","bedd698b":"The above result implies that there are not any null values","7f0e5e3c":"#### See class distribution","e078f37a":"### EDA","f458ea7b":"CONCLUSION: Since the values varies a lot, so we need to apply standard scaling on top of data","5d1d47e0":"#### Creating X and y out of training data","ca58cabc":"#### 2) SVM Classifier","5e4c85c3":"##### Let's do standard scaling on test data","6262782f":"#### 3) Random Forest Classifier","0b522dbe":"#### 1) Logistic Regression","3c60da44":"This shows that there is not much class imbalance","fab42cef":"Now, among different tried models, this one seems to be the best, so let's use it","52d37107":"#### Reading train and test data","e7ed82b7":"### Trying out different models","a6f87ab3":"#### Let's create submission.csv now"}}