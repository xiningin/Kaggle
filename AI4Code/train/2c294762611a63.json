{"cell_type":{"b84575bd":"code","d7846897":"code","ba21b6cd":"code","f85a80ca":"code","70b0f4b8":"code","6c6f199c":"code","5b939367":"code","4c4f7239":"code","59e43534":"code","1918a88b":"code","274eee8d":"code","ea1fcfe4":"code","10c9d545":"code","7b8e3ab5":"code","2053b20e":"code","572b93cd":"code","4ebe5586":"code","0fb43d78":"code","565df605":"code","742e921c":"code","a5cc5fe6":"code","5645e0d9":"code","6c57df11":"code","900865f6":"code","1f678205":"code","a9438d0c":"markdown","e956d793":"markdown","c0cb368e":"markdown","8ac27b8d":"markdown","ac4efb57":"markdown","b483749e":"markdown","e05aeaa0":"markdown","2f21ed9e":"markdown","662560a6":"markdown","cc179390":"markdown","fa4e0c2e":"markdown","4e94b33c":"markdown"},"source":{"b84575bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nimport warnings \n  \nwarnings.filterwarnings(action = 'ignore') \n  \nimport gensim \nfrom gensim.models import Word2Vec \nprint(os.listdir(\"..\/input\/data 4\"))\n  \n\n# Any results you write to the current directory are saved as output.","d7846897":"import pandas as pd\n\n\n\n# HBP\ndata = pd.read_csv('..\/input\/data 4\/hbp.txt', sep=\">\",header=None)\nsequences=data[0].dropna()\nlabels=data[1].dropna()\nsequences.reset_index(drop=True, inplace=True)\nlabels.reset_index(drop=True, inplace=True)\nlist_of_series=[sequences.rename(\"sequences\"),labels.rename(\"Name\")]\ndf_hbp = pd.concat(list_of_series, axis=1)\ndf_hbp['label']='hbp'\ndf_hbp.head()\n\n\n\n\n","ba21b6cd":"# not HBP\ndata = pd.read_csv('..\/input\/data 4\/non-hbp.txt', sep=\">\",header=None)\nsequences=data[0].dropna()\nlabels=data[1].dropna()\nsequences.reset_index(drop=True, inplace=True)\nlabels.reset_index(drop=True, inplace=True)\nlist_of_series=[sequences.rename(\"sequences\"),labels.rename(\"Name\")]\ndf_N_hbp = pd.concat(list_of_series, axis=1)\ndf_N_hbp['label']='non-hbp'\ndf_N_hbp.head()","f85a80ca":"frames = [df_hbp,df_N_hbp]\ndf=pd.concat(frames)\ndf.head()","70b0f4b8":"from sklearn.preprocessing import LabelBinarizer\nimport keras\n# Transform labels to one-hot\nlb = LabelBinarizer()\nY = lb.fit_transform(df.label)\nCat_y=keras.utils.to_categorical(Y,num_classes=2)","6c6f199c":"arr=[]\nfor i in df.sequences:\n    arr.append(len(i))\n    \narr=np.asarray(arr)\nprint(\"Minimum length of string is = \",(arr.min()))\nminlength=arr.min()\nfrom keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\n\n# maximum length of sequence, everything afterwards is discarded!\nmax_length = minlength\n\n#create and fit tokenizer\ntokenizer = Tokenizer(char_level=True)\ntokenizer.fit_on_texts(df.sequences)\n#represent input data as word rank number sequences\nX = tokenizer.texts_to_sequences(df.sequences)\nX = sequence.pad_sequences(X, maxlen=max_length)","5b939367":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten,Dropout,Conv2D\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\n\nembedding_dim = 8\n\n# create the model\nmodel = Sequential()\nmodel.add(Embedding(len(tokenizer.word_index)+1, embedding_dim, input_length=max_length))\nmodel.add(Conv1D(filters=16, kernel_size=2, padding='same', activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv1D(filters=8, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\n\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nX_train, X_test, y_train, y_test = train_test_split(X, Cat_y, test_size=.3)\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=1)","4c4f7239":"from keras.utils import plot_model\nplot_model(model, to_file='model.png')","59e43534":"from sklearn.metrics import classification_report\nimport numpy as np\n\n\nprint(classification_report(Y_test, y_pred))","1918a88b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.callbacks import LambdaCallback\nfrom keras.layers import Conv1D, Flatten\nfrom keras.layers import Dense ,Dropout,BatchNormalization\nfrom keras.models import Sequential \nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical \nfrom keras import regularizers\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import  VotingClassifier\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn import metrics\nfrom sklearn import ensemble\nfrom sklearn import gaussian_process\nfrom sklearn import linear_model\nfrom sklearn import naive_bayes\nfrom sklearn import neighbors\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn import discriminant_analysis\nfrom sklearn import model_selection\nfrom xgboost.sklearn import XGBClassifier \n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n","274eee8d":"#Machine Learning Algorithm (MLA) Selection and Initialization\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.3)\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]\n\n\n\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy' ]\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n   # cv_results = model_selection.cross_validate(alg, X_train, y_train)\n    alg.fit(X_train, y_train)\n    y_pred=alg.predict(X_test)\n    score=metrics.accuracy_score(y_test, y_pred)\n    \n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] =score\n\n    \n    \n    row_index+=1\n\n    \n\nMLA_compare\n\nMLA_compare.to_csv(\"classifier.csv\")\n#MLA_predict","ea1fcfe4":"X= df.sequences.astype(str).str[0:96]\nX=X.values\nprint(\"Every String has length equal to =\",len(X[1]))\n# Every Sequence Length is now 96","10c9d545":"def onehot(ltr):\n     return [1 if i==ord(ltr) else 0 for i in range(97,123)]\n\ndef onehotvec(s):\n     return [onehot(c) for c in list(s.lower())]\n\nsequence_encode=[]\n\nfor i in range(0,len(X)):\n    \n    X[i]=X[i].lower()\n    a=onehotvec(X[i])\n    a=np.asarray(a)\n    sequence_encode.append(a)\n    \nsequence_encode=np.asarray(sequence_encode)  \nprint(\"Shape of One Hot Encoded Sequence\",sequence_encode.shape)","7b8e3ab5":"X=sequence_encode.reshape(-1,96,26,1)\nX.shape","2053b20e":"from keras.layers import Conv2D,LeakyReLU,MaxPooling2D\nfrom keras.layers.core import Activation\n\n# create the model\nmodel = Sequential()\nmodel.add(Conv2D(16,kernel_size = (2,2),input_shape=(96,26,1)))\nmodel.add(Activation(\"relu\"))\nmodel.add(Conv2D(32,kernel_size = (2,2),input_shape=(96,26,1)))\nmodel.add(Activation(\"relu\"))\n\nmodel.add(MaxPooling2D(pool_size=(2, 2),strides=2, padding='same', data_format=None))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(64,kernel_size = (2,2),input_shape=(96,26,1)))\nmodel.add(Activation(\"relu\"))\nmodel.add(Conv2D(82,kernel_size = (2,2),input_shape=(96,26,1)))\nmodel.add(Activation(\"relu\"))\n\n\n\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nX_train, X_test, y_train, y_test = train_test_split(X, Cat_y, test_size=.8)\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=1)","572b93cd":"# Import Keras and other Deep Learning dependencies\nfrom keras.models import Sequential\nimport time\nfrom keras.optimizers import Adam\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\nfrom keras.models import Model\nimport seaborn as sns\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\nfrom keras.layers.merge import Concatenate\nfrom keras.layers.core import Lambda, Flatten, Dense\nfrom keras.initializers import glorot_uniform\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.optimizers import *\nfrom keras.engine.topology import Layer\nfrom keras import backend as K\nfrom keras.regularizers import l2\nK.set_image_data_format('channels_last')\nimport cv2\nimport os\nfrom skimage import io\nimport numpy as np\nfrom numpy import genfromtxt\nimport pandas as pd\nimport tensorflow as tf\n\nimport numpy.random as rng\nfrom sklearn.utils import shuffle\n\n%matplotlib inline\n%load_ext autoreload\n%reload_ext autoreload\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","4ebe5586":"X_train=X_train.reshape(X_train.shape[0],96,26,1)\nX_test=X_test.reshape(X_test.shape[0],96,26,1)\ntrain_groups = [X_train[np.where(y_train==i)[0]] for i in np.unique(y_train)]\ntest_groups = [X_test[np.where(y_test==i)[0]] for i in np.unique(y_test)]\nprint('train groups:', [X.shape[0] for X in train_groups])\nprint('test groups:', [X.shape[0] for X in test_groups])","0fb43d78":"def gen_random_batch(in_groups, batch_halfsize = 8):\n    out_img_a, out_img_b, out_score = [], [], []\n    all_groups = list(range(len(in_groups)))\n    for match_group in [True, False]:\n        group_idx = np.random.choice(all_groups, size = batch_halfsize)\n        out_img_a += [in_groups[c_idx][np.random.choice(range(in_groups[c_idx].shape[0]))] for c_idx in group_idx]\n        if match_group:\n            b_group_idx = group_idx\n            out_score += [1]*batch_halfsize\n        else:\n            # anything but the same group\n            non_group_idx = [np.random.choice([i for i in all_groups if i!=c_idx]) for c_idx in group_idx] \n            b_group_idx = non_group_idx\n            out_score += [0]*batch_halfsize\n            \n        out_img_b += [in_groups[c_idx][np.random.choice(range(in_groups[c_idx].shape[0]))] for c_idx in b_group_idx]\n            \n    return np.stack(out_img_a,0), np.stack(out_img_b,0), np.stack(out_score,0)","565df605":"from keras.models import Model\nfrom keras.layers import Input, Conv2D, BatchNormalization, MaxPool2D, Activation, Flatten, Dense, Dropout\nimg_in = Input(shape = X_train.shape[1:], name = 'FeatureNet_ImageInput')\nn_layer = img_in\nfor i in range(2):\n    n_layer = Conv2D(8*2**i, kernel_size = (3,3), activation = 'linear')(n_layer)\n    n_layer = BatchNormalization()(n_layer)\n    n_layer = Activation('relu')(n_layer)\n    n_layer = Conv2D(16*2**i, kernel_size = (3,3), activation = 'linear')(n_layer)\n    n_layer = BatchNormalization()(n_layer)\n    n_layer = Activation('relu')(n_layer)\n    n_layer = MaxPool2D((2,2))(n_layer)\nn_layer = Flatten()(n_layer)\nn_layer = Dense(32, activation = 'linear')(n_layer)\nn_layer = Dropout(0.5)(n_layer)\nn_layer = BatchNormalization()(n_layer)\nn_layer = Activation('relu')(n_layer)\nfeature_model = Model(inputs = [img_in], outputs = [n_layer], name = 'FeatureGenerationModel')\nfeature_model.summary()","742e921c":"from keras.layers import concatenate\nimg_a_in = Input(shape = X_train.shape[1:], name = 'ImageA_Input')\nimg_b_in = Input(shape = X_train.shape[1:], name = 'ImageB_Input')\nimg_a_feat = feature_model(img_a_in)\nimg_b_feat = feature_model(img_b_in)\ncombined_features = concatenate([img_a_feat, img_b_feat], name = 'merge_features')\ncombined_features = Dense(16, activation = 'linear')(combined_features)\ncombined_features = BatchNormalization()(combined_features)\ncombined_features = Activation('relu')(combined_features)\ncombined_features = Dense(4, activation = 'linear')(combined_features)\ncombined_features = BatchNormalization()(combined_features)\ncombined_features = Activation('relu')(combined_features)\ncombined_features = Dense(1, activation = 'sigmoid')(combined_features)\nsimilarity_model = Model(inputs = [img_a_in, img_b_in], outputs = [combined_features], name = 'Similarity_Model')\nsimilarity_model.summary()","a5cc5fe6":"similarity_model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['mae'])","5645e0d9":"def siam_gen(in_groups, batch_size = 4):\n    while True:\n        pv_a, pv_b, pv_sim = gen_random_batch(train_groups, batch_size\/\/2)\n        yield [pv_a, pv_b], pv_sim\n# we want a constant validation group to have a frame of reference for model performance\nvalid_a, valid_b, valid_sim = gen_random_batch(test_groups, 10)\nloss_history = similarity_model.fit_generator(siam_gen(train_groups), \n                               steps_per_epoch = 100,\n                               validation_data=([valid_a, valid_b], valid_sim),\n                                              epochs = 100,\n                                             verbose = True)","6c57df11":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n# Split Data\nlb = LabelBinarizer()\nY = lb.fit_transform(df.label)\n\nX_train, X_test,y_train,y_test = train_test_split(df.sequences, Y, test_size = 0.2, random_state = 1)\n\n\n\n\n\n\ny_test_cat=keras.utils.to_categorical(y_test)\ny_train_cat=keras.utils.to_categorical(y_train)\n# Create a Count Vectorizer to gather the unique elements in sequence\nvect = CountVectorizer(analyzer = 'char_wb', ngram_range = (4,4))\n\n# Fit and Transform CountVectorizer\nvect.fit(X_train)\nX_train_df = vect.transform(X_train)\nX_test_df = vect.transform(X_test)\n\n#Print a few of the features\nprint(vect.get_feature_names()[-20:])","900865f6":"X_train_df.shape","1f678205":"#Machine Learning Algorithm (MLA) Selection and Initialization\n\n\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]\n\n\n\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy' ]\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n   # cv_results = model_selection.cross_validate(alg, X_train, y_train)\n    alg.fit(X_train_df.toarray(), y_train)\n    y_pred=alg.predict(X_test_df.toarray())\n    score=metrics.accuracy_score(y_test, y_pred)\n    \n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] =score\n\n    \n    \n    row_index+=1\n\n    \n\nMLA_compare\n#MLA_predict","a9438d0c":"# Merging HBP and Non-HBP Sequences ","e956d793":"# HBP Reading","c0cb368e":"# One Hot Encoder","8ac27b8d":"# Tokenizer\nUsing the ** keras** library for text processing, \n1. ** Tokenizer**: translates every character of the sequence into a number\n2. **pad_sequences:** ensures that every sequence has the same length (max_length). I decided to use a maximum length of 100, which should be sufficient for most sequences. \n3. **train_test_split:** from sklearn splits the data into training and testing samples.","ac4efb57":"# Conv 2D Training","b483749e":"# Bag Of words as feature Extractor","e05aeaa0":"# Non-HBP Reading","2f21ed9e":"# Siamese Neural Network (Clustring ALgorithm)","662560a6":"# Conclusion \n\nFrom this we can conclude that because of less number of samples we are not doing that much good by using deep learning.\nI will upload my next kernal in which I will do Feature engineering and try some machine learning algorithm , Optimization Methods and ensemble technique,","cc179390":"# Conv1D Training ","fa4e0c2e":"# Trying different Classifiers","4e94b33c":"We are going to use Deep learning for classify Protein sequnces that they are HBPs or NON-HBPS. \nAlgorithm Used\n\n* Convolutional Neural Network 1d with Embiding Layer\n* Convolutional Neural Network 2d\n* Siamese Neural Network\n* Some Machine Learning Algorithms without Feature Engineering"}}