{"cell_type":{"8755fa5b":"code","a2020006":"code","e3991692":"code","cd99fec4":"code","837a0bfa":"code","9d89ae3a":"code","7c81cbbb":"code","b5da80a8":"code","25df5645":"code","fdcaa3c2":"code","d7721a04":"code","cd1d198c":"code","1471e686":"code","ec4452b8":"code","083c0472":"code","d5dd901e":"code","e0fc30dd":"code","decbb101":"code","28a10783":"code","4799378a":"code","272f5569":"code","d7662b7d":"code","ad84dc12":"code","9fa81884":"code","b108c3b4":"code","bebb5b8f":"code","eb0da9c6":"code","8a4d86c3":"code","bdd4099d":"code","1d45e700":"code","e12f63e6":"code","6d52a64a":"code","11984d0b":"code","6d25601f":"code","b870d784":"code","4b192dc4":"code","93adca6e":"code","f79c0cdb":"code","604196b6":"code","edbc12e7":"code","af553b1c":"markdown","b0acda43":"markdown","44355190":"markdown","4a4510ed":"markdown","e10c0084":"markdown","9e9aa3f9":"markdown","c1c2bf7e":"markdown","59361509":"markdown","92b17398":"markdown","0b63fcf3":"markdown","0998d5d2":"markdown","48c6bf4a":"markdown","c4188ec5":"markdown","33268457":"markdown","40d20679":"markdown","a5e7fabe":"markdown","3ec6129c":"markdown"},"source":{"8755fa5b":"import pandas as pd\nimport os\nimport numpy as np\n\ndef load_data(path):\n    \"\"\"\n    Takes path oof csv file\n    and returns dataframe object.\n    \"\"\"\n    try:\n        data = pd.read_csv(path)\n        return data\n    except:\n        print(\"Not a valid file\")\n\nclass Dataframe(pd.DataFrame):\n    \n    def __init__(self, dataframe):\n        super().__init__(dataframe)\n\n       \n    def feature_columns(self, drop_col=list()):\n        \"\"\"\n        Extract feature columns from dataframe\n        with reference to drop_col\n        \"\"\"\n        return [col for col in self.drop(drop_col, axis=1).columns]\n    \n    def int_feature_dtypes(self):\n        \"\"\"\n        Get integer dtypes\n        pandas has a method already for this\n        df[df.dtypes==dtype]\n        \"\"\"\n        \n        return list(filter(lambda x: True if x[1]==np.dtype('int64') else False, zip(self.columns,self.dtypes)))\n    \n    def pos_features(self):\n        \"\"\"\n        Get positive features\n        from the data.\n        \"\"\"\n        return list(filter(lambda x: all(df_train[x]>0), df_train.columns[:-1]))\n    ","a2020006":"#path\npath = \"\/kaggle\/input\/tabular-playground-series-aug-2021\/\"\n\n#training and test files location\ncsv_files = [os.path.join(path, x) for x in [\"train.csv\", \"test.csv\"]]\n\n#tuple object of csv files\ntables = (load_data(csv) for csv in csv_files)","e3991692":"# Create Class instance \ndf_train, df_test = tables\ndf_train = Dataframe(df_train)\ndf_test = Dataframe(df_test)\ndf_train.columns","cd99fec4":"#check shapes \nprint(df_train.shape, df_test.shape)\n\n#get all names of feature columns\nfeatures = df_train.feature_columns(drop_col=['id','loss'])","837a0bfa":"#Total number of features\n\ndf_train.drop([\"id\"], axis=1, inplace=True)\nprint(len(features))\n\n#Purely Integer Features\n\ninteger_features = df_train.int_feature_dtypes()\nprint(*integer_features,sep='\\n')","9d89ae3a":"#Strictly Positive Features\n\npos_features = df_train.pos_features()\nprint(pos_features)","7c81cbbb":"#visualization libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b5da80a8":"#plots in rows and cols\ndef plot_dics():\n    \n    sns_distributions={'kde':sns.kdeplot,'histplot':sns.histplot}\n    return sns_distributions\n\ndef skew_plot(skew_values):\n    \"\"\"\n    Plots skew from a series of skew values.\n    \"\"\"\n    skew_values_positive = skew_values[skew_values>0]\n    skew_values_negative = skew_values[skew_values<0]\n    skew_values_zero = skew_values[skew_values==0]\n    row_size = len(skew_values)\n    index_list = dict(map(lambda x: (x[1], x[0]),enumerate(skew_values.index)))\n    \n    #plot values positive values\n    plt.scatter(x=[index_list[key] for key in skew_values_positive.index], y=skew_values_positive,color='blue')\n    plt.plot(range(row_size), np.ones(row_size), color ='blue')\n    \n    #plot negative values\n    plt.scatter(x=[index_list[key] for key in skew_values_negative.index], y=skew_values_negative,color='red')\n    plt.plot(range(row_size), -1*np.ones(row_size), color ='red')\n    \n    plt.scatter(x=[index_list[key] for key in skew_values_zero.index], y=skew_values_zero,color='green')\n    plt.plot(range(row_size), np.zeros(row_size), color ='green')\n    plt.xticks(np.arange(0, len(skew_values), 3), skew_values.index[::3]); #set ticks\n    plt.title(\"Scatter Plot of Skew values for each column\")\n    plt.grid()\n\ndef legend(sns_plot,value):\n    sns_plot.legend(f\"skew:{value:.2f}\", loc=\"best\")\n    \ndef dist_plots(sns_plot, dataframe, features, figsize=(6, 6), rows=1, cols=1,sharex=False, colors=False):\n    \"\"\"\n    Plots histplots and kernel density estimators using seaborn\n    \"\"\"\n    fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=figsize,sharex=sharex)\n    feature_index = 0\n    if colors:\n        colors = sns.color_palette(n_colors=(rows*cols))\n    else :\n        colors = [None]*(rows*cols)\n    try:\n        if rows*cols >= len(features):\n            if rows > 1 and cols > 1:\n                for i in range(rows):\n                    for j in range(cols):\n                        if feature_index <= len(features)-1:\n                            g = sns_plot(dataframe[features[feature_index]], ax=ax[i][j], color=colors[feature_index])\n                            feature_index+=1\n                        else:\n                            break          \n            elif rows > 1 and cols == 1:\n                 for i in range(rows):\n                        if feature_index <= len(features)-1:\n                            g = sns_plot(dataframe[features[feature_index]], ax=ax[i], color=colors[feature_index])\n                            feature_index += 1\n                        else:\n                            break  \n            elif rows and cols:\n                g = sns_plot(dataframe[features[feature_index]], ax=ax, color=colors[feature_index])\n                \n        else:\n            raise ValueError()\n    except ValueError:\n        raise ValueError(\"rows*cols should be greater than or equal to the number of features\")\n        ","25df5645":"sns_hist = plot_dics()['kde']\ndist_plots(sns_hist, df_train, list(map(lambda x:x[0], integer_features)), figsize=(12, 8),rows=3, cols=2, colors=True)","fdcaa3c2":"#get all skewed values\nskew_values=df_train.skew()\nprint(\"=============\")\n\n#print columns with skew values greater than 1\nprint(*skew_values[skew_values>1].index, sep=\" \")\n\n#print columns with skew values less than 1\nprint(\"=============\")\nprint(*skew_values[skew_values<-1].index, sep=\" \")","d7721a04":"plt.figure(figsize=(12, 8))\nskew_plot(skew_values)","cd1d198c":"from sklearn.model_selection import train_test_split\n\n#Shape before the split\nprint(df_train.shape)\n\n# Split the set in train, cross-validation, test set\nX_train,X_,y_train, y_ = train_test_split(df_train.drop([\"loss\"],axis=1),df_train[\"loss\"], test_size=0.3, shuffle=True, random_state=23)\nX_cv, X_test, y_cv, y_test = train_test_split(X_,y_, test_size=1\/2,shuffle=True, random_state=100)\n\n#Trainging set\nprint(f\"Training Size: {X_train.shape}\")\n\n#cross-validation set\nprint(f\"CV Size: {X_cv.shape}\")\n\n#test test\nprint(f\"Test Size: {X_test.shape}\")","1471e686":"from sklearn.preprocessing import StandardScaler, Normalizer, RobustScaler, PowerTransformer\n\n# all transformers\ndef transform_data(df, transformer_name='standardscaler'):\n    transformer_dict = {\"standardscaler\":StandardScaler,\n                        \"normalizer\":Normalizer,\n                        \"gaussian\":PowerTransformer,\n                        \"robustscaler\": RobustScaler}\n    return transformer_dict[transformer_name]().fit_transform(df)\n\ntemp_list = []\nfor df in [X_train, X_cv, X_test]:\n    temp_list.append(transform_data(transform_data(df, transformer_name=\"standardscaler\"), transformer_name=\"normalizer\"))","ec4452b8":"#Unpack the values\nX_train_gaussian,X_cv_gaussian, X_test_gaussian = temp_list","083c0472":"df_train_gaussian = pd.DataFrame(X_train_gaussian, columns=df_train.columns[:-1])\ndf_cv_gaussian = pd.DataFrame(X_cv_gaussian, columns=df_train.columns[:-1])\ndf_test_gaussian = pd.DataFrame(X_test_gaussian, columns=df_train.columns[:-1])\n\n#After transform skewed values\nprint(\"=============\")\nplt.figure(figsize=(12, 8))\nskew_values_gaussian = df_train_gaussian.skew()\nskew_plot(skew_values_gaussian)","d5dd901e":"dist_plots(sns_hist, df_test_gaussian, list(map(lambda x:x[0], integer_features[:-1])), figsize=(12, 8),rows=3, cols=2, colors=True)","e0fc30dd":"from sklearn.metrics import mean_squared_error, r2_score\n\n#classical\nfrom sklearn.linear_model import Ridge, BayesianRidge, LinearRegression, Lasso\n\n#Experimental feature in scikit-learn\nfrom sklearn.experimental import enable_hist_gradient_boosting\n\n#Ensemble regressors\nfrom sklearn.ensemble import RandomForestRegressor, StackingRegressor, HistGradientBoostingRegressor\nimport xgboost as xgb\n\nmodel_dict = {\"ridge\":Ridge, \"bayesridge\":BayesianRidge,\n                  \"linearreg\":LinearRegression, \"lasso\":Lasso}\n\n#Fitting without any parameters adjustment\ndef fit_model(X, y, model=\"linearreg\"):\n    model = model_dict[model]()\n    model.fit(X, y)\n    return model\n\n#predict function\ndef predict_y(X, model):\n    y_pred = model.predict(X)\n    return y_pred\n  \n# root mean square error  \ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\n# Coefficient Histogram and line plots\ndef coefficient_plot(model_name,models,ax, color):\n    for model in models:\n        ax[0].plot(model.coef_, linestyle='-', color=color[models.index(model)])\n    ax[0].legend(model_name)\n    for model in models:\n        ax[1].bar([list(model.coef_).index(val) for val in list(model.coef_)], model.coef_, color=color[models.index(model)],label=features)\n    ax[1].legend(model_name)","decbb101":"def run_base_model(X_train, y_train, X_test, y_test):\n    \"\"\"\n    Run base models and select the best model\n    \"\"\"\n    error_train = None\n    error_rmse = None\n    base_model = None\n    for key in model_dict.keys():\n        model_error = 0\n        _=20\n        model_name = model_dict[key].__name__\n        print(\"=\"*_+f\"{model_name}\"+\"=\"*(abs(len(model_name)-_)), end=\"\\n\")\n        model = fit_model(X_train, y_train, model=key)\n        \n        model_error = rmse(predict_y(X_train, model), y_train)   \n        if error_train is None:\n            error_train = model_error\n        elif error_train > model_error:\n            base_model = model\n            error_train = model_error\n            print(f\"Low error found {model_error}--{model_name}\")\n        print(f\"Train Root Mean Squared Error: {model_error}\", end=\"\\n\")\n        print(\"\\n\")\n    \n    print(\"Base Model with low error on training set \"+str(base_model).replace(\"()\",\"\"))\n    print(\"\\n\")\n    print(\"Base Model Prediction on Cross Validation Set\")\n    y_pred_cv = predict_y(X_test, base_model)\n    error_rmse = rmse(y_pred_cv, y_test)\n    print(f\"Prediction Root Mean Squared Error: {error_rmse}\")\n    \n    return base_model, error_train\nbase_model, error_train = run_base_model(X_train_gaussian, y_train, X_cv_gaussian, y_cv)\nerror_train","28a10783":"import matplotlib as mpl\nsns.set_style(\"ticks\")\nfig_coef, ax_coef = plt.subplots(nrows=1, ncols=2, figsize=(24, 8))\ncolormap = mpl.cm.Dark2.colors\nmodel_names = [str(base_model).replace(\"()\",\"\")]\ncoefficient_plot(model_names, [base_model], ax_coef, colormap)","4799378a":"a = list(zip(features,base_model.coef_))\nfig_bar, ax_bar = plt.subplots( figsize=(12, 8))\na.sort(key=lambda x: x[1])\na=dict(a[::-1])\nsns.barplot(y=list(a.values()), x=list(a.keys()), ax=ax_bar)\nax_bar.set_title(\"Feature Coefficients\");\nax_bar.set_xticks(range(0, len(a.keys()), 3))\nax_bar.set_xticklabels(list(a.keys())[::3]);\ndrop_features = list(filter(lambda x: True if -0.05<=a[x]<=0.05 else False, a.keys()))\nprint(drop_features)","272f5569":"\n#Predicting on actual test set\nreal_test = transform_data(transform_data(df_test.drop([\"id\"],axis=1), transformer_name=\"standardscaler\"),transformer_name=\"normalizer\")\nreal_test.shape","d7662b7d":"def submission(y_pred, col, file_name):\n    \"\"\"\n    Submission Function \n    with filename\n    \"\"\"\n    data = pd.DataFrame({\"id\":col, \"loss\":y_pred})\n    sub_files = []\n    for dirname,_, filenames in os.walk(\".\/\"):\n            for file in filenames:\n                if file.endswith(\".csv\"):\n                    sub_files.append(file)\n    if len(sub_files)>0:\n        new_n = str(int(sub_files[-1].split(\".\")[0][-1])+1)\n    else:\n        new_n=1\n    data.to_csv(f\".\/{file_name}{new_n}.csv\", index=False)            ","ad84dc12":"#using LR\ny_pred_real = base_model.predict(real_test)\nsubmission(y_pred_real, df_test[\"id\"], \"bayes\")","9fa81884":"# dist_plots(sns_hist, , [\"predicted_loss\",\"test_loss\"], rows=2, cols=1, colors=True)\n# sns_plot(dataframe[features[feature_index]], ax=ax[i][j], color=colors[feature_index])\nfig,ax = plt.subplots(figsize=(16, 8))\ng1 = sns.kdeplot( \"predicted_loss\", data = pd.DataFrame({\"predicted_loss\":base_model.predict(X_cv_gaussian), \"test_loss\":y_cv}), color='red', ax = ax)\ng2 = sns.kdeplot(\"test_loss\", data=pd.DataFrame({\"predicted_loss\":base_model.predict(X_cv_gaussian), \"test_loss\":y_cv}), color='blue', ax=ax)\n\nax.grid(linestyle='--', linewidth=2)\nax.legend([\"predicted loss\", \"actual loss\"]);\nax.set_title(\"Predicted loss vs actual loss density\");\n              ","b108c3b4":"def square_root(col_list, data):\n    \n    #add square root columns\n    new_data = data.copy()\n    for col in col_list:\n        new_data[\"sqrt_\"+col]= np.sqrt(data[col])\n    return new_data\n\ndef square(col_list, data):\n    \n    #add squares for col\n    new_data = data.copy()\n    for col in col_list:\n        new_data[\"sqr\"+col]= (data[col])*(data[col])\n    return new_data\n\ndef reciprocals(col_list, data):\n    #add reciprocals for col_list\n    new_data = data.copy()\n    for col in col_list:\n        new_data[\"rec\"+col]= 1\/(abs(data[col])+1)                      \n    return new_data\n\ndef product_col(data):\n    #all product gives one column\n    new_data = data.copy()\n    temp = data[data.columns[0]]\n    for col in data.columns:\n        temp *= data[col]\n    new_data[\"all_product\"] = temp\n    return new_data\n    \n\ndef update_sets(data_list):\n    temp_list = []\n    for df in data_list:\n        temp_list.append(square(df.columns, data=df))\n    return temp_list","bebb5b8f":"X_train_f1 = reciprocals(features, data=square(features, data=X_train))\nX_cv_f1 = reciprocals(features, data= square(features, data=X_cv))\nX_test_f1 = reciprocals(features, data=square(features, data=X_test))\ntemp_list1 = []\nfor df in [X_train_f1, X_cv_f1, X_test_f1]:\n    temp_list1.append(transform_data(transform_data(df, transformer_name=\"standardscaler\"), transformer_name=\"normalizer\"))","eb0da9c6":"X_train_sqrt_gaussian, X_cv_sqrt_gaussian, X_test_sqrt_gaussian = temp_list1","8a4d86c3":"base_model_v1, error_v1 = run_base_model(X_train_sqrt_gaussian, y_train, X_cv_sqrt_gaussian, y_cv)","bdd4099d":"fig_v1,ax_v1 = plt.subplots(figsize=(16, 8))\ng1 = sns.kdeplot( \"predicted_loss\", data = pd.DataFrame({\"predicted_loss\":base_model_v1.predict(X_cv_sqrt_gaussian), \"cv_loss\":y_cv}),\n                 color='red', ax = ax_v1)\ng2 = sns.kdeplot(\"cv_loss\", data=pd.DataFrame({\"predicted_loss\":base_model_v1.predict(X_cv_sqrt_gaussian), \"cv_loss\":y_cv}),\n                 color='blue', ax=ax_v1)\n\nax_v1.grid(linestyle='--', linewidth=2)\nax_v1.legend([\"predicted loss\", \"actual loss\"]);\nax_v1.set_title(\"Predicted loss vs actual loss density\");\n              ","1d45e700":"real_test_v1 = reciprocals(features, data=square(features, data=df_test.drop([\"id\"],axis=1)))\nreal_test_v1 = transform_data(transform_data(real_test_v1, transformer_name=\"standardscaler\"),transformer_name=\"normalizer\")\nreal_test_v1.shape","e12f63e6":"#using linear regression with squared features\ny_pred_real = base_model_v1.predict(real_test_v1)\nsubmission(y_pred_real, df_test[\"id\"], \"submission\")","6d52a64a":"#Method was taken from a kaggle notebook from last feb i don't rememeber the link\ndef objective(trial):\n    \n    train_x, test_x, train_y, test_y = X_train_sqrt_gaussian, X_cv_sqrt_gaussian, y_train, y_cv\n    param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': 4000,\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,90, 100, 120]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n    }\n    model = xgb.XGBRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","11984d0b":"# ! pip install optuna\nimport optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","6d25601f":"import json\nwith open(\".\/best_param_120RS.json\", \"w\") as f:\n    json.dump(study.best_trial.params, f)\nstudy.best_trial.params","b870d784":"from sklearn.model_selection import RepeatedKFold, KFold\n\nhp = study.best_trial.params\n\nhp[\"tree_method\"]=\"gpu_hist\"\nhp[\"verbosity\"] = 0\nhp[\"objective\"] = \"reg:squarederror\"\nhp[\"n_estimators\"] = 4000\n\ntrain = pd.concat([pd.DataFrame(X_train_sqrt_gaussian, columns=X_train_f1.columns,index=X_train_f1.index),\n                   pd.DataFrame(X_cv_sqrt_gaussian, columns=X_cv_f1.columns,index=X_cv_f1.index)])\n                   \ndf_y_train = pd.concat([y_train,y_cv])\ntest = pd.DataFrame(real_test_v1, columns=X_train_f1.columns)\n\ny_preds = np.zeros(test.shape[0])\nkf = KFold(n_splits=5,random_state=48,shuffle=True)\nrmse_error=[]  # list contains rmse for each fold\nn=0\nfor trn_idx, test_idx in kf.split(train, df_y_train):\n    \n    X_tr,X_val=train.iloc[trn_idx],train.iloc[test_idx]\n    y_tr,y_val=df_y_train.iloc[trn_idx],df_y_train.iloc[test_idx]\n    \n    model_xgb = xgb.XGBRegressor(**hp)\n    model_xgb.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    \n    y_preds+=model_xgb.predict(test)\/kf.n_splits\n    \n    rmse_error.append(mean_squared_error(y_val, model_xgb.predict(X_val), squared=False))\n    \n    print(\"Fold\", n+1,rmse_error[n])\n    \n    n+=1","4b192dc4":"submission(y_preds, df_test[\"id\"], \"1_submission\")","93adca6e":"optuna.visualization.plot_optimization_history(study)","f79c0cdb":"optuna.visualization.plot_parallel_coordinate(study)","604196b6":"model_xgb.save_model(\".\/xgb_model_Rs120\")","edbc12e7":"optuna.importance.get_param_importances(study)","af553b1c":"The loss ditribution of loss is asymmetric and so are many features we will normalize them.","b0acda43":"## Plotting Functions","44355190":"### Plots after scaling","4a4510ed":"### Skew Plots","e10c0084":"Lowest error on 25% of data: 7.94038 using bayesridge using standardscaler with no tuning\nHighest error on 25% of data: 7.9433 using robustscaler and normazlier","9e9aa3f9":"## [Contents](#h1)\n* <p style=\"color:violet\"> Data and EDA<\/p>\n* <p style=\"color:violet\"> Plotting Functions<\/p>\n* <p style=\"color:violet\"> Running Base Model<\/p>\n* <p style=\"color:violet\"> Engineer Features<\/p>\n* <p style=\"color:violet\">Optuna and XGB <\/p>","c1c2bf7e":"## Data and EDA","59361509":"## Regression","92b17398":"## Running Base Model","0b63fcf3":"Observe that the same linear regression gives a error of 7.783425 on validation set with added features","0998d5d2":"NICE! Adding Square and reciprocal terms Terms brought the error down by 1.36% that is 92439 previous error was 94038.","48c6bf4a":"# Feature Engineering","c4188ec5":"## Optuna and XGB","33268457":"7.8306 on CV with hundred estimators","40d20679":"Best Parameters Obtained\n\n{\"lambda\": 0.04001717971729262, \"alpha\": 0.001537777755465056, \"colsample_bytree\": 0.5, \"subsample\": 0.7, \"learning_rate\": 0.009, \"max_depth\": 9, \"random_state\": 48, \"min_child_weight\": 282}","a5e7fabe":"<h1 style=\"color: blue\"> TPS August <\/h1>\n\n\n","3ec6129c":"### Scaling Data"}}