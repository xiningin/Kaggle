{"cell_type":{"4dcda56c":"code","9114651d":"code","f8967a81":"code","2cae27c5":"code","7b35be0c":"code","954b4e36":"code","24ec3c62":"code","0f4e68c5":"code","c906ebdf":"code","bda697d8":"code","a6e0e3b0":"code","4af6dff5":"code","a453670d":"code","5542f853":"code","b334273f":"code","ea5cab03":"code","ac599295":"code","43d57e42":"code","b78c35dd":"code","5c60514c":"code","ccb6529f":"code","fa61f483":"code","707fe6a6":"code","3efb3191":"code","41af33f2":"code","54ce8a0c":"code","69b44b26":"code","f831f9ea":"code","420267f1":"code","ae798b43":"markdown","d4580f84":"markdown","fba2fa41":"markdown","48cafbab":"markdown"},"source":{"4dcda56c":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","9114651d":"import os\ntweet= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntweet.head(3)","f8967a81":"df=pd.concat([tweet,test], sort = False)\ndf.shape","2cae27c5":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","7b35be0c":"df['text']=df['text'].apply(lambda x : remove_URL(x))","954b4e36":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","24ec3c62":"df['text']=df['text'].apply(lambda x : remove_html(x))","0f4e68c5":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","c906ebdf":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","bda697d8":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","a6e0e3b0":"df['text']=df['text'].apply(lambda x : remove_punct(x))","4af6dff5":"#!pip install pyspellchecker","a453670d":"#from spellchecker import SpellChecker","5542f853":"#spell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)","b334273f":"#Not working - Need to check\n#df['text']=df['text'].apply(lambda x : correct_spellings(x)#)","ea5cab03":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","ac599295":"corpus=create_corpus(df)","43d57e42":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","b78c35dd":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","5c60514c":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","ccb6529f":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","fa61f483":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","707fe6a6":"model.summary()","3efb3191":"train=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]","41af33f2":"X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","54ce8a0c":"history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)","69b44b26":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","f831f9ea":"y_pre=model.predict(test)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\nsub.to_csv('submission.csv',index=False)","420267f1":"sub.head()","ae798b43":"## Globe Vectorization","d4580f84":"## Base Model","fba2fa41":"# Import Libraries","48cafbab":"# Import Data"}}