{"cell_type":{"a40f0cb9":"code","435a75f4":"code","1205b51b":"code","780f9b8c":"code","55a1a40b":"code","f12f63e2":"code","8d889a04":"code","c3fc3dd9":"code","80c1df71":"code","0a0e189b":"code","3e8bcc0f":"code","3e20dc4a":"code","4058937d":"code","69a41b38":"code","a42f2386":"code","7f83a436":"code","5aab33f7":"code","9956164b":"code","b7112237":"code","5e2a128a":"code","aa2a5ccb":"code","127916be":"code","2dcae073":"code","875c9959":"code","4895f89d":"code","cdf23dc8":"code","59d0877f":"code","3f0c476c":"code","91d0c2dd":"code","d3635d26":"code","a695618d":"code","7add3bba":"code","5fd47654":"code","23353738":"code","311bb57b":"code","11f370df":"code","9b240ff8":"code","d43cf3e0":"code","05a79f19":"code","be20dae4":"code","a1c57832":"code","e02b0a73":"code","970b95da":"code","f530bbf0":"markdown","71ef1b97":"markdown","e4e6a228":"markdown","76462c0d":"markdown","c140d429":"markdown","539cc959":"markdown","319e0477":"markdown","051a1820":"markdown","8ddf976f":"markdown"},"source":{"a40f0cb9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom IPython.display import display, HTML\n\nhome = pd.read_csv('..\/input\/property-data\/property.csv')\ndisplay(home.head())\n\nprint(home.shape)\n\nmissing = home.isnull().sum()\nmissing = missing[missing>0]\ndisplay(missing)\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(12, 5))\nplt.title('Missing Vals Dists')\nmissing.plot.bar(color='teal')","435a75f4":"from scipy.stats import norm, beta\nsns.set()\nplt.style.use('seaborn-poster')\n\nplt.figure(figsize=(10, 4))\nsns.distplot(home['monthly_rent'] , bins=80, kde=True, hist=True, fit=norm, color = 'teal');","1205b51b":"\n\nplt.figure(figsize=(10, 4))\nsns.distplot(home['unit_area'] , bins=80, kde=True, hist=True, fit=norm, color = 'teal');","780f9b8c":"\n\nplt.figure(figsize=(10, 5))\nsns.distplot(home['deposit'] , bins=80, kde=True, hist=True, fit=norm, color = 'teal');","55a1a40b":"\nplt.figure(figsize=(10, 5))\nsns.distplot(home['property_age'] , bins=80, kde=True, hist=True, fit=norm, color = 'teal');","f12f63e2":"home.describe()","8d889a04":"home.drop(list(home[home.unit_area <20].index), inplace=True)\nhome.reset_index(drop=True, inplace=True)\nhome.drop(list(home[home.property_age >55].index), inplace=True)\nhome.reset_index(drop=True, inplace=True)\nhome.shape","c3fc3dd9":"cats = ['district_uuid', 'has_elevator', 'has_storage_area']\n\nfrom sklearn.preprocessing import *\nfor c in cats:\n    le = LabelEncoder()\n    home[c] =le.fit_transform(home[c].astype('str'))\n","80c1df71":"feats = [c for c in home.columns if c not in ['item_id']]\nfeats","0a0e189b":"home_orig = home.copy()\n\ndel home['item_id']\n\nhome.fillna(home.mean(), inplace=True)\nrb = StandardScaler()\nhome = rb.fit_transform(home)\nhome = pd.DataFrame(home, columns=feats)\nhome.head()","3e8bcc0f":"from sklearn.cluster import DBSCAN\n\nclustering = DBSCAN(eps=3, min_samples=2).fit(home)\nclustering.labels_","3e20dc4a":"home['ClusterDBSCAN'] = pd.Series(clustering.labels_)\n\n\nprint(home.ClusterDBSCAN.unique().size)\n\nhome[home.ClusterDBSCAN==-1]","4058937d":"from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nX = home.copy()\nXtsne = TSNE(n_components=2).fit_transform(X)\ndftsne = pd.DataFrame(Xtsne)\ndftsne['cluster'] = clustering.labels_\ndftsne.columns = ['x1','x2','cluster']\n\npca2 = PCA(n_components=2)\nskillsPCA2 = pca2.fit_transform(home)\ndfskillsPCA2 = pd.DataFrame(skillsPCA2)\ndfskillsPCA2['cluster'] = clustering.labels_\ndfskillsPCA2.columns = ['x1','x2','cluster']\n\nfig, ax = plt.subplots(1, 2, figsize=(30,15))\nsns.scatterplot(data=dftsne,x='x1',y='x2',hue='cluster',legend=\"full\",alpha=0.5,palette=\"Set1\", ax=ax[0])\nax[0].set_title('Visualized on TSNE 2D')\nsns.scatterplot(data=dfskillsPCA2,x='x1',y='x2',hue='cluster',legend=\"full\",alpha=0.5,palette=\"Set1\",ax=ax[1])\nax[1].set_title('Visualized on PCA 2D')\nfig.suptitle('Comparing clustering result when visualized using TSNE2D vs. PCA2D')\n#display(fig)","69a41b38":"home.shape, home_orig.shape","a42f2386":"home.head()","7f83a436":"home_orig.head()","5aab33f7":"lr = home[home.ClusterDBSCAN==-1].index\nhome.drop(list(home[home.ClusterDBSCAN==-1].index), inplace=True)\nhome_orig.drop(list(lr), inplace=True)\nhome.reset_index(drop=True, inplace=True)\nhome_orig.reset_index(drop=True, inplace=True)\nhome.shape, home_orig.shape\ndel home['ClusterDBSCAN']\nlr2 = home[home.monthly_rent>15].index\nhome.drop(list(home[home.monthly_rent>15].index), inplace=True)\nhome_orig.drop(list(lr2), inplace=True)\nhome.to_csv('home_noiseRem.csv', index=False)\nhome.shape, home_orig.shape","9956164b":"import matplotlib.style as style\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (14,10))\n\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(home.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(home.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, );\n## Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 25);","b7112237":"home.tail()","5e2a128a":"home_orig.tail()","aa2a5ccb":"from sklearn.mixture import GaussianMixture as GMM\n\nn_components = np.arange(40, 60)\nmodels = [GMM(n, covariance_type='full', random_state=0).fit(home)\n          for n in n_components]\nplt.style.use('ggplot')\nplt.figure(figsize=(10, 5))\nplt.plot(n_components, [m.bic(home) for m in models], label='BIC')\nplt.plot(n_components, [m.aic(home) for m in models], label='AIC')\nplt.legend(loc='best')\nplt.xlabel('n_components');","127916be":"\ngmm = GMM(n_components=51, covariance_type='diag', max_iter=300, random_state=42).fit(home)\nlabels = gmm.predict(home)\n\n\nhome_orig['Cluster_GMM'] = list(labels)\nhome_orig.head()","2dcae073":"labels","875c9959":"cCs = home_orig.groupby('Cluster_GMM')['item_id'].count()\ncCs","4895f89d":"home_orig.tail()","cdf23dc8":"home.shape, home_orig.shape","59d0877f":"home['Cluster_GMM'] = list(labels)\nhome.head()","3f0c476c":"home.to_csv('home_w_gmmcluster.csv', index=False)","91d0c2dd":"del home['Cluster_GMM']","d3635d26":"from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nX = home.copy()\nXtsne = TSNE(n_components=2).fit_transform(X)\ndftsne = pd.DataFrame(Xtsne)\ndftsne['cluster'] = labels\ndftsne.columns = ['x1','x2','cluster']\n\npca2 = PCA(n_components=2)\nskPCA2 = pca2.fit_transform(home)\ndfskPCA2 = pd.DataFrame(skPCA2)\ndfskPCA2['cluster'] = labels\ndfskPCA2.columns = ['x1','x2','cluster']\n\nfig, ax = plt.subplots(1, 2, figsize=(35,22))\nsns.scatterplot(data=dftsne,x='x1',y='x2',hue='cluster',legend=\"full\",alpha=0.5,palette=\"Set2\", ax=ax[0])\nax[0].set_title('Visualized on TSNE 2D')\nsns.scatterplot(data=dfskillsPCA2,x='x1',y='x2',hue='cluster',legend=\"full\",alpha=0.5,palette=\"Set2\",ax=ax[1])\nax[1].set_title('Visualized on PCA 2D')\nfig.suptitle('Comparing clustering result when visualized using TSNE2D vs. PCA2D')\n#display(fig)","a695618d":"home =pd.read_csv('home_w_gmmcluster.csv')\nhome.head()","7add3bba":"from sklearn.decomposition import PCA\npca = PCA(n_components=4).fit(home)\npca_samples = pca.transform(home)\npca.fit(home)\n\n\n#cumulative explaned variance\nprint (np.cumsum(pca.explained_variance_ratio_))","5fd47654":"pca = PCA(n_components=4).fit(home)\nreduced_data = pca.transform(home)\n\n","23353738":"\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n#keep the scores for each cluster size\nsil_scores = []\n\nrandom_state = 7\n\nfor i in range(35,17,-1):\n    clusterer = KMeans(i, random_state=random_state).fit(reduced_data)\n    # TODO: Predict the cluster for each data point\n    preds = clusterer.predict(reduced_data)\n\n    # TODO: Find the cluster centers\n    centers = clusterer.cluster_centers_\n\n    # TODO: Predict the cluster for each transformed sample data point\n   # sample_preds = clusterer.predict(home)\n\n    # TODO: Calculate the mean silhouette coefficient for the number of clusters chosen\n    score = silhouette_score(reduced_data, preds)\n    sil_scores.append(score)\n    print(i, 'clusters:', score.round(5))\n\n# plot the scores\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12, 7))\nplt.style.use(\"fivethirtyeight\")\n_ = plt.plot(np.arange(35,17,-1), sil_scores, '-o')","311bb57b":"import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\nkm = KMeans(init='random', n_clusters=21, verbose=0,\n            n_init=1, max_iter=200,)\nkm.fit(reduced_data)\n\nl = km.labels_\n\nprint(\"LABLES\")\nprint(l)\n\n","11f370df":"home_orig['Cluster_Km'] = list(pd.Series(l))\nhome_orig.head(20)","9b240ff8":"cCs = home_orig.groupby('Cluster_Km')['item_id'].count()\ncCs","d43cf3e0":"home_orig.head()","05a79f19":"home_orig.to_csv('home_orig_clusters_full.csv')","be20dae4":"from sklearn.manifold import TSNE\nX = home.copy()\nXtsne = TSNE(n_components=2).fit_transform(X)\ndftsne = pd.DataFrame(Xtsne)\ndftsne['cluster'] = km.labels_\ndftsne.columns = ['x1','x2','cluster']\nplt.figure(figsize=(15,10))\nplt.title('KMeans Result')\nsns.scatterplot(data=dftsne,x='x1',y='x2',hue='cluster',legend=\"full\",alpha=0.5)\ndel X","a1c57832":"from sklearn.cluster import Birch\n\n#from sklearn.mixture import GMM\nbirch = Birch(threshold=.5, branching_factor=50, n_clusters=20).fit(home)\nbirch_labels = birch.predict(home)\n#plt.scatter(home.iloc[:, 2], home.iloc[:, 8], c=labels, s=40, cmap='viridis');","e02b0a73":"birch_labels","970b95da":"from sklearn.manifold import TSNE\nX = home.copy()\nXtsne = TSNE(n_components=2).fit_transform(X)\ndftsne = pd.DataFrame(Xtsne)\ndftsne['cluster'] = birch_labels\ndftsne.columns = ['x1','x2','cluster']\nplt.figure(figsize=(15,10))\nplt.title('Birch Result')\nsns.scatterplot(data=dftsne,x='x1',y='x2',hue='cluster',legend=\"full\",palette='bone', alpha=0.5)\ndel X","f530bbf0":"## Birch\n","71ef1b97":">## <font color=SkyBlue> DBSCAN<\/font>\n","e4e6a228":"### Removing Noise","76462c0d":">#### **Note:**\n>#### Though GMM is often categorized as a clustering algorithm, fundamentally it is an algorithm for density estimation. \n>#### That is to say, the result of a GMM fit to some data is technically not a clustering model, but a generative probabilistic\n>#### model describing the distribution of the data. I used it as a novel method to use result to kmeans!\n","c140d429":">## <font color=darkcyan> EDA and Property Segmentation:<\/font>\n>### Preliminary EDA and clustering for property data. The things i didn't do are:\n -  More feat eng for kmeans\n -  k-means results are sensitive to the order of observations, and it is worth to run algorithm several times, shuffling data in between, averaging resulting clusters and running \n    final evaluations with those averaged clusters centers as starting points. i run kmeans only once.\n -  Advanced outlier removal (used visuals)\n -  Tune DBSCAN\n -  Final descriptive analysis of clusters\n -  Didn't apply advanced technique for imputing missing values\n\n","539cc959":"\n>## <font color=teal> GMM<\/font>\n","319e0477":"\n>## <font color=darkcyan> KMeans (with density estimation feature!)<\/font>\n","051a1820":"\n>## <font color=darkcyan> Agile EDA<\/font>","8ddf976f":">#### **Note:**\n**k-means results are sensitive to the order of observations, and it is worth to run algorithm several times, shuffling data in between, averaging \nresulting clusters and running final evaluations with those averaged  clusters centers as starting points. \nI ran kmeans only once.**\n\n\n\n"}}