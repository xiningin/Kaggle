{"cell_type":{"60f9c582":"code","eb4d029d":"code","024e6132":"code","83938962":"code","da15909c":"code","45f36446":"code","5a54116e":"code","62d773bf":"code","11bd59bf":"code","5750036a":"code","8502269b":"code","87f4d213":"code","5beb68a7":"code","8a7a8fa6":"code","2829d018":"code","d962ec56":"code","6daf7f7c":"code","c5a42ba1":"code","2551380b":"code","90fd595d":"code","8450c11a":"code","5ba5e157":"code","01cf4c21":"code","f7879a8e":"code","7db3bcbf":"code","cc110a56":"code","449a95f0":"code","3b37ddb7":"code","af213010":"code","4bd83c7d":"code","20ff7dd9":"code","8238ef64":"code","18171897":"code","7a9dc18b":"code","077e8c4e":"code","e50116e6":"code","f5559901":"markdown","ef81b800":"markdown","e5ab1c44":"markdown","e54c659e":"markdown","5bb093e2":"markdown","af50235d":"markdown"},"source":{"60f9c582":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport scipy as sp\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n\nfrom sklearn.metrics import mean_squared_log_error\n\nprint(f\"Tensorflow version {tf.__version__}\")","eb4d029d":"train_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jul-2021\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jul-2021\/test.csv\")\nsample_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jul-2021\/sample_submission.csv\")","024e6132":"train_df['date_time'] = pd.to_datetime(train_df['date_time'])\ntest_df['date_time'] = pd.to_datetime(test_df['date_time'])","83938962":"# Feature engineering\ntrain_df['year'] = train_df['date_time'].dt.year\ntrain_df['month'] = train_df['date_time'].dt.month\ntrain_df['hour'] = train_df['date_time'].dt.hour\ntrain_df['day'] = train_df['date_time'].dt.day\n\ntest_df['year'] = test_df['date_time'].dt.year\ntest_df['month'] = test_df['date_time'].dt.month\ntest_df['hour'] = test_df['date_time'].dt.hour\ntest_df['day'] = test_df['date_time'].dt.day","da15909c":"# Set data_time column as index as it is needed for RNN\ntrain = train_df.set_index(\"date_time\").copy()\ntest = test_df.set_index(\"date_time\").copy()","45f36446":"target_cols = [col for col in train.columns if col.startswith('target')]\nfeat_cols = [col for col in train.columns if col not in target_cols]","5a54116e":"# Calculate percentage of a dataset to be a test set\ntest_percent = 0.1\ntest_point = np.round(len(train)*test_percent)\ntest_idx = int(len(train)-test_point)","62d773bf":"# Devide a dataset into train and test sets\nXtrain = train.drop(target_cols[:0], axis=1).iloc[:test_idx]\nXtest = train.drop(target_cols[:0], axis=1).iloc[test_idx:]","11bd59bf":"Xtrain","5750036a":"def plot_predictions(col_idx, predictions):\n    plt.figure(figsize=(25,5))\n    sns.lineplot(x=Xtest[LENGTH:].index, y=Xtest[LENGTH:][target_cols[col_idx]],label=\"True labels\")\n    sns.lineplot(x=Xtest[LENGTH:].index, y=predictions.reshape(-1), label=\"Predictions\")\n    plt.title(f\"Prediction for {target_cols[col_idx]}    RMSLE={np.sqrt(mean_squared_log_error(Xtest[target_cols[col_idx]][LENGTH:], np.abs(predictions.reshape(-1))))}\")\n    plt.legend();","8502269b":"LENGTH = 48  # use 48 observation to test_generator 49\nBATCH_SIZE = 1 # usually this batch size works well\nTARGET_IDX = 0","87f4d213":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nXtrain_scaled = scaler.fit_transform(Xtrain.drop(target_cols[TARGET_IDX],axis=1))\nXtest_scaled = scaler.transform(Xtest.drop(target_cols[TARGET_IDX],axis=1))","5beb68a7":"N_FEATURES = Xtrain_scaled.shape[1]","8a7a8fa6":"train_generator = TimeseriesGenerator(data=Xtrain_scaled,\n                                      targets=Xtrain[target_cols[TARGET_IDX]],\n                                      length=LENGTH,\n                                      batch_size=BATCH_SIZE)\ntest_generator = TimeseriesGenerator(data=Xtest_scaled,\n                                     targets=Xtest[target_cols[TARGET_IDX]],\n                                     length=LENGTH,\n                                     batch_size=BATCH_SIZE)","2829d018":"tf.random.set_seed(45)\nrnn_model = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(48, input_shape=(LENGTH, N_FEATURES)),\n    tf.keras.layers.Dense(48),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1)\n])\n\nrnn_model.compile(optimizer='adam',loss='mse', metrics=['mse'])\n\nrnn_history = rnn_model.fit(train_generator, epochs=10, validation_data=test_generator)","d962ec56":"rnn_df = pd.DataFrame(rnn_history.history)","6daf7f7c":"rnn_df[['loss','val_loss']].plot()","c5a42ba1":"rnn_preds = rnn_model.predict(test_generator)","2551380b":"plot_predictions(col_idx=0,\n                 predictions=rnn_preds)","90fd595d":"lstm_model = tf.keras.Sequential([\n    tf.keras.layers.LSTM(48, return_sequences=True, input_shape=(LENGTH, N_FEATURES), dropout=0.2),\n    tf.keras.layers.LSTM(48),\n    tf.keras.layers.Dense(1)\n])\n\nlstm_model.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.mean_squared_error, metrics=['mse'])\n\nlstm_history = lstm_model.fit(train_generator, epochs=10, validation_data=test_generator)","8450c11a":"lstm_df = pd.DataFrame(lstm_history.history)\nlstm_df[['loss','val_loss']].plot()","5ba5e157":"lstm_preds = lstm_model.predict(test_generator)","01cf4c21":"plot_predictions(col_idx=0,\n                 predictions=lstm_preds)","f7879a8e":"def rmsle_custom(y_true, y_pred):\n    msle = tf.keras.losses.MeanSquaredLogarithmicError()\n    return K.sqrt(msle(y_true, y_pred))\n\n\nes = tf.keras.callbacks.EarlyStopping(monitor='val_rmsle_custom', \n                                      mode='min',patience=4, \n                                      restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', # val_rmsle_custom try\n                                               mode='min',\n                                               patience=2, \n                                               verbose=1)\n\nweights_initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1., seed=45)","7db3bcbf":"def lstm_train_test_model():\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.LSTM(48, return_sequences=True, \n                             input_shape=(LENGTH, N_FEATURES),\n                             dropout=0.1, \n                             kernel_initializer=weights_initializer),\n        tf.keras.layers.LSTM(48, dropout=0.1, \n                             kernel_initializer=weights_initializer),\n        tf.keras.layers.Dense(1)  \n    ])\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n                  loss=tf.keras.losses.mean_squared_error,\n                  metrics=rmsle_custom)\n    \n    history = model.fit(train_generator,\n                        epochs=30,\n                        validation_data=test_generator,\n                        callbacks=[es,plateau],\n                        verbose=1)\n    \n    return history, model","cc110a56":"lstm_2_history, lstm_model = lstm_train_test_model()","449a95f0":"lstm_2_history_df = pd.DataFrame(lstm_2_history.history)\nlstm_2_history_df[['loss','val_loss']].plot()","3b37ddb7":"lstm_2_preds = lstm_model.predict(test_generator)","af213010":"plot_predictions(col_idx=0,\n                 predictions=lstm_2_preds)","4bd83c7d":"K.clear_session()","20ff7dd9":"def rmsle_custom(y_true, y_pred):\n    msle = tf.keras.losses.MeanSquaredLogarithmicError()\n    return K.sqrt(msle(y_true, y_pred))\n\n\nes = tf.keras.callbacks.EarlyStopping(monitor='val_rmsle_custom', \n                                      mode='min',patience=6, \n                                      restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', # val_rmsle_custom try\n                                               mode='min',\n                                               patience=2, \n                                               verbose=1)\n\nweights_initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1., seed=45)","8238ef64":"def lstm_autoencoder():\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.LSTM(48, return_sequences=True, \n                             input_shape=(LENGTH, N_FEATURES),\n                             kernel_initializer=weights_initializer),\n        tf.keras.layers.LSTM(24, return_sequences=True,\n                             kernel_initializer=weights_initializer),\n        tf.keras.layers.LSTM(12, kernel_initializer=weights_initializer),\n        tf.keras.layers.RepeatVector(LENGTH),\n        tf.keras.layers.LSTM(12, return_sequences=True, kernel_initializer=weights_initializer),\n        tf.keras.layers.LSTM(24, return_sequences=True, kernel_initializer=weights_initializer),\n        tf.keras.layers.LSTM(48, return_sequences=True,  kernel_initializer=weights_initializer),\n        tf.keras.layers.TimeDistributed(Dense(N_FEATURES)),\n        ])\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n                  loss=tf.keras.losses.mean_squared_error,\n                  metrics=rmsle_custom)\n    \n    history = model.fit(train_generator,\n                        epochs=30,\n                        validation_data=test_generator,\n                        callbacks=[plateau,es],\n                        verbose=1)\n    \n    return history, model","18171897":"history_autoencoder, lstm_autoenc = lstm_autoencoder()","7a9dc18b":"autoenc_history_df = pd.DataFrame(history_autoencoder.history)\nautoenc_history_df[['loss','val_loss']].plot()","077e8c4e":"autoenc_preds = lstm_autoenc.predict(test_generator batch_size=BATCH_SIZE)","e50116e6":"lK.clear_session()","f5559901":"### Simple RNN","ef81b800":"### Stacked LSTM","e5ab1c44":"### LSTM with big guns\n\nNow is time to build our LSTM model with everything we can to improve our models predictions.","e54c659e":"## Time to create a model for submission","5bb093e2":"### LSTM Autoencoder","af50235d":"# Simple Recurrent Neural Networks (RNN) and LSTM\n\nHi Kagglers,\nThis notebook is continuation of my first notebook for this competition, I couldn't finished everything on first [notebook](https:\/\/www.kaggle.com\/godzill22\/tps-07-eda-statistical-analysis) (which was my intetion ), however for some reason when I run my notebook my computer is slow and freezes. So I decided that I leave EDA notebook as it is now and focuse on modeling in this one."}}