{"cell_type":{"4a7f97cd":"code","8fbfd778":"code","cc3678e7":"code","cdd1c5e7":"code","404195ad":"code","7218edbe":"code","004eac9c":"code","a3dea2b9":"code","02cb9cd2":"code","82d1ed16":"code","3e36787a":"code","1264f2ee":"code","f49fd542":"code","a12d7193":"code","35bd5446":"code","96b92e95":"code","ce49e296":"code","4ab17955":"code","b891f5d3":"code","05250b54":"code","28bc0163":"code","f7e673f5":"code","40f9fe18":"code","2b064174":"code","1eeb48f8":"code","42a90b99":"code","c198d7a8":"code","19deff1e":"code","706dad18":"code","6cb0154a":"code","882f5fec":"code","9f672f30":"code","01775adb":"code","0e6e26eb":"code","a85e46e0":"code","8e63e0da":"code","6e1e926d":"code","0579ea8a":"code","93e83a78":"code","f5a736f4":"code","d6e2fa8a":"code","99856f50":"code","3fa65e81":"code","58fb74b8":"code","bb863e27":"code","35c95cc1":"code","746667bf":"code","ef246892":"markdown","23af74b6":"markdown","a38b6066":"markdown","5de1ca14":"markdown","4024ef91":"markdown","dece158f":"markdown","25ab2c0e":"markdown","f72591a0":"markdown","c008ecb9":"markdown","5669fe67":"markdown","7c99e33b":"markdown","85770f39":"markdown","94d1a79c":"markdown","6fc99bfa":"markdown","e145c3c4":"markdown","8213bdf8":"markdown","ec8b85ec":"markdown","90021830":"markdown","028c6555":"markdown","836d25e9":"markdown","3686f632":"markdown","b3d1becf":"markdown","96dca126":"markdown","d0abab9c":"markdown","72a6dd69":"markdown","0d31c384":"markdown"},"source":{"4a7f97cd":"import pandas as pd       \nimport matplotlib as mat\nimport matplotlib.pyplot as plt    \nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\npd.options.display.max_colwidth = 100\n\nimport random\nimport os\n\nfrom numpy.random import seed\nseed(42)\n\nrandom.seed(42)\nos.environ['PYTHONHASHSEED'] = str(42)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport glob\nimport cv2\n\nfrom tensorflow.random import set_seed\nset_seed(42)\n\nimport warnings\nwarnings.filterwarnings('ignore')","8fbfd778":"IMG_SIZE = 224\nBATCH = 32\nSEED = 42","cc3678e7":"#main_path = \"..\/input\/chest-xray-pneumonia\/chest_xray\/\"\nmain_path = \"..\/input\/labeled-chest-xray-images\/chest_xray\"\n\n\ntrain_path = os.path.join(main_path,\"train\")\ntest_path=os.path.join(main_path,\"test\")\n\ntrain_normal = glob.glob(train_path+\"\/NORMAL\/*.jpeg\")\ntrain_pneumonia = glob.glob(train_path+\"\/PNEUMONIA\/*.jpeg\")\n\ntest_normal = glob.glob(test_path+\"\/NORMAL\/*.jpeg\")\ntest_pneumonia = glob.glob(test_path+\"\/PNEUMONIA\/*.jpeg\")","cdd1c5e7":"train_list = [x for x in train_normal]\ntrain_list.extend([x for x in train_pneumonia])\n\ndf_train = pd.DataFrame(np.concatenate([['Normal']*len(train_normal) , ['Pneumonia']*len(train_pneumonia)]), columns = ['class'])\ndf_train['image'] = [x for x in train_list]\n\ntest_list = [x for x in test_normal]\ntest_list.extend([x for x in test_pneumonia])\n\ndf_test = pd.DataFrame(np.concatenate([['Normal']*len(test_normal) , ['Pneumonia']*len(test_pneumonia)]), columns = ['class'])\ndf_test['image'] = [x for x in test_list]","404195ad":"df_train","7218edbe":"df_test","004eac9c":"plt.figure(figsize=(6,4))\n\nax = sns.countplot(x='class', data=df_train, palette=\"mako\")\n\nplt.xlabel(\"Class\", fontsize= 12)\nplt.ylabel(\"# of Samples\", fontsize= 12)\nplt.ylim(0,5000)\nplt.xticks([0,1], ['Normal', 'Pneumonia'], fontsize = 11)\n\nfor p in ax.patches:\n    ax.annotate((p.get_height()), (p.get_x()+0.30, p.get_height()+300), fontsize = 13)\n    \nplt.show()","a3dea2b9":"plt.figure(figsize=(7,5))\n\ndf_train['class'].value_counts().plot(kind='pie',labels = ['',''], autopct='%1.1f%%', colors = ['darkcyan','blue'], explode = [0,0.05], textprops = {\"fontsize\":15})\n\nplt.legend(labels=['Pneumonia', 'Normal'])\nplt.show()","02cb9cd2":"plt.figure(figsize=(6,4))\n\nax = sns.countplot(x='class', data=df_test, palette=\"mako\")\n\nplt.xlabel(\"Class\", fontsize= 12)\nplt.ylabel(\"# of Samples\", fontsize= 12)\nplt.ylim(0,500)\nplt.xticks([0,1], ['Normal', 'Pneumonia'], fontsize = 11)\n\nfor p in ax.patches:\n    ax.annotate((p.get_height()), (p.get_x()+0.32, p.get_height()+20), fontsize = 13)\n    \nplt.show()","82d1ed16":"plt.figure(figsize=(7,5))\n\ndf_test['class'].value_counts().plot(kind='pie',labels = ['',''], autopct='%1.1f%%', colors = ['darkcyan','blue'], explode = [0,0.05], textprops = {\"fontsize\":15})\n\nplt.legend(labels=['Pneumonia', 'Normal'])\nplt.show()","3e36787a":"print('Train Set - Normal')\n\nplt.figure(figsize=(12,12))\n\nfor i in range(0, 12):\n    plt.subplot(3,4,i + 1)\n    img = cv2.imread(train_normal[i])\n    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n    plt.imshow(img)\n    plt.axis(\"off\")\n\nplt.tight_layout()\n\nplt.show()","1264f2ee":"print('Train Set - Pneumonia')\n\nplt.figure(figsize=(12,12))\n\nfor i in range(0, 12):\n    plt.subplot(3,4,i + 1)\n    img = cv2.imread(train_pneumonia[i])\n    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n    plt.imshow(img)\n    plt.axis(\"off\")\n\nplt.tight_layout()\n\nplt.show()","f49fd542":"print('Test Set - Normal')\n\nplt.figure(figsize=(12,12))\n\nfor i in range(0, 12):\n    plt.subplot(3,4,i + 1)\n    img = cv2.imread(test_normal[i])\n    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n    plt.imshow(img)\n    plt.axis(\"off\")\n\nplt.tight_layout()\n\nplt.show()","a12d7193":"print('Test Set - Pneumonia')\n\nplt.figure(figsize=(12,12))\n\nfor i in range(0, 12):\n    plt.subplot(3,4,i + 1)\n    img = cv2.imread(test_pneumonia[i])\n    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n    plt.imshow(img)\n    plt.axis(\"off\")\n\nplt.tight_layout()\n\nplt.show()","35bd5446":"train_df, val_df = train_test_split(df_train, test_size = 0.20, random_state = SEED, stratify = df_train['class'])","96b92e95":"train_df","ce49e296":"val_df","4ab17955":"# https:\/\/vijayabhaskar96.medium.com\/tutorial-on-keras-flow-from-dataframe-1fd4493d237c\n\ntrain_datagen = ImageDataGenerator(rescale=1\/255.,\n                                  zoom_range = 0.1,\n                                  #rotation_range = 0.1,\n                                  width_shift_range = 0.1,\n                                  height_shift_range = 0.1)\n\nval_datagen = ImageDataGenerator(rescale=1\/255.)\n\nds_train = train_datagen.flow_from_dataframe(train_df,\n                                             #directory=train_path, #dataframe contains the full paths\n                                             x_col = 'image',\n                                             y_col = 'class',\n                                             target_size = (IMG_SIZE, IMG_SIZE),\n                                             class_mode = 'binary',\n                                             batch_size = BATCH,\n                                             seed = SEED)\n\nds_val = val_datagen.flow_from_dataframe(val_df,\n                                            #directory=train_path,\n                                            x_col = 'image',\n                                            y_col = 'class',\n                                            target_size = (IMG_SIZE, IMG_SIZE),\n                                            class_mode = 'binary',\n                                            batch_size = BATCH,\n                                            seed = SEED)\n\nds_test = val_datagen.flow_from_dataframe(df_test,\n                                            #directory=test_path,\n                                            x_col = 'image',\n                                            y_col = 'class',\n                                            target_size = (IMG_SIZE, IMG_SIZE),\n                                            class_mode = 'binary',\n                                            batch_size = 1,\n                                            shuffle = False)","b891f5d3":"#Setting callbakcs\n\nearly_stopping = callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    min_delta=1e-7,\n    restore_best_weights=True,\n)\n\nplateau = callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor = 0.2,                                     \n    patience = 2,                                   \n    min_delt = 1e-7,                                \n    cooldown = 0,                               \n    verbose = 1\n) ","05250b54":"def get_model():\n    \n    #Input shape = [width, height, color channels]\n    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n    \n    # Block One\n    x = layers.Conv2D(filters=16, kernel_size=3, padding='valid')(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.MaxPool2D()(x)\n    x = layers.Dropout(0.2)(x)\n\n    # Block Two\n    x = layers.Conv2D(filters=32, kernel_size=3, padding='valid')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.MaxPool2D()(x)\n    x = layers.Dropout(0.2)(x)\n    \n    # Block Three\n    x = layers.Conv2D(filters=64, kernel_size=3, padding='valid')(x)\n    x = layers.Conv2D(filters=64, kernel_size=3, padding='valid')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.MaxPool2D()(x)\n    x = layers.Dropout(0.4)(x)\n\n    # Head\n    #x = layers.BatchNormalization()(x)\n    x = layers.Flatten()(x)\n    x = layers.Dense(64, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n    \n    #Final Layer (Output)\n    output = layers.Dense(1, activation='sigmoid')(x)\n    \n    model = keras.Model(inputs=[inputs], outputs=output)\n    \n    return model","28bc0163":"keras.backend.clear_session()\n\nmodel = get_model()\nmodel.compile(loss='binary_crossentropy'\n              , optimizer = keras.optimizers.Adam(learning_rate=3e-5), metrics='binary_accuracy')\n\nmodel.summary()","f7e673f5":"history = model.fit(ds_train,\n          batch_size = BATCH, epochs = 50,\n          validation_data=ds_val,\n          callbacks=[early_stopping, plateau],\n          steps_per_epoch=(len(train_df)\/BATCH),\n          validation_steps=(len(val_df)\/BATCH));","40f9fe18":"fig, ax = plt.subplots(figsize=(20,8))\nsns.lineplot(x = history.epoch, y = history.history['loss'])\nsns.lineplot(x = history.epoch, y = history.history['val_loss'])\nax.set_title('Learning Curve (Loss)')\nax.set_ylabel('Loss')\nax.set_xlabel('Epoch')\nax.set_ylim(0, 0.5)\nax.legend(['train', 'val'], loc='best')\nplt.show()","2b064174":"fig, ax = plt.subplots(figsize=(20,8))\nsns.lineplot(x = history.epoch, y = history.history['binary_accuracy'])\nsns.lineplot(x = history.epoch, y = history.history['val_binary_accuracy'])\nax.set_title('Learning Curve (Accuracy)')\nax.set_ylabel('Accuracy')\nax.set_xlabel('Epoch')\nax.set_ylim(0.80, 1.0)\nax.legend(['train', 'val'], loc='best')\nplt.show()","1eeb48f8":"score = model.evaluate(ds_val, steps = len(val_df)\/BATCH, verbose = 0)\nprint('Val loss:', score[0])\nprint('Val accuracy:', score[1])","42a90b99":"score = model.evaluate(ds_test, steps = len(df_test), verbose = 0)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","c198d7a8":"base_model = tf.keras.applications.ResNet152V2(\n    weights='imagenet',\n    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n    include_top=False)\n\nbase_model.trainable = False\n\ndef get_pretrained():\n    \n    #Input shape = [width, height, color channels]\n    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n    \n    x = base_model(inputs)\n\n    # Head\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.1)(x)\n    \n    #Final Layer (Output)\n    output = layers.Dense(1, activation='sigmoid')(x)\n    \n    model = keras.Model(inputs=[inputs], outputs=output)\n    \n    return model","19deff1e":"keras.backend.clear_session()\n\nmodel_pretrained = get_pretrained()\nmodel_pretrained.compile(loss='binary_crossentropy'\n              , optimizer = keras.optimizers.Adam(learning_rate=5e-5), metrics='binary_accuracy')\n\nmodel_pretrained.summary()","706dad18":"history = model_pretrained.fit(ds_train,\n          batch_size = BATCH, epochs = 50,\n          validation_data=ds_val,\n          callbacks=[early_stopping, plateau],\n          steps_per_epoch=(len(train_df)\/BATCH),\n          validation_steps=(len(val_df)\/BATCH));","6cb0154a":"fig, ax = plt.subplots(figsize=(20,8))\nsns.lineplot(x = history.epoch, y = history.history['loss'])\nsns.lineplot(x = history.epoch, y = history.history['val_loss'])\nax.set_title('Learning Curve (Loss)')\nax.set_ylabel('Loss')\nax.set_xlabel('Epoch')\nax.set_ylim(0, 0.5)\nax.legend(['train', 'val'], loc='best')\nplt.show()","882f5fec":"fig, ax = plt.subplots(figsize=(20,8))\nsns.lineplot(x = history.epoch, y = history.history['binary_accuracy'])\nsns.lineplot(x = history.epoch, y = history.history['val_binary_accuracy'])\nax.set_title('Learning Curve (Accuracy)')\nax.set_ylabel('Accuracy')\nax.set_xlabel('Epoch')\nax.set_ylim(0.80, 1.0)\nax.legend(['train', 'val'], loc='best')\nplt.show()","9f672f30":"score = model_pretrained.evaluate(ds_val, steps = len(val_df)\/BATCH, verbose = 0)\nprint('Val loss:', score[0])\nprint('Val accuracy:', score[1])","01775adb":"score = model_pretrained.evaluate(ds_test, steps = len(df_test), verbose = 0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","0e6e26eb":"base_model.trainable = True\n\n# Freeze all layers except for the\nfor layer in base_model.layers[:-13]:\n    layer.trainable = False","a85e46e0":"# Check which layers are tuneable (trainable)\nfor layer_number, layer in enumerate(base_model.layers):\n    print(layer_number, layer.name, layer.trainable)","8e63e0da":"model_pretrained.compile(loss='binary_crossentropy'\n              , optimizer = keras.optimizers.Adam(learning_rate=2e-6), metrics='binary_accuracy')\n\nmodel_pretrained.summary()","6e1e926d":"history = model_pretrained.fit(ds_train,\n          batch_size = BATCH, epochs = 50,\n          validation_data=ds_val,\n          callbacks=[early_stopping, plateau],\n          steps_per_epoch=(len(train_df)\/BATCH),\n          validation_steps=(len(val_df)\/BATCH));","0579ea8a":"fig, ax = plt.subplots(figsize=(20,8))\nsns.lineplot(x = history.epoch, y = history.history['loss'])\nsns.lineplot(x = history.epoch, y = history.history['val_loss'])\nax.set_title('Learning Curve (Loss)')\nax.set_ylabel('Loss')\nax.set_xlabel('Epoch')\nax.set_ylim(0, 0.3)\nax.legend(['train', 'val'], loc='best')\nplt.show()","93e83a78":"fig, ax = plt.subplots(figsize=(20,8))\nsns.lineplot(x = history.epoch, y = history.history['binary_accuracy'])\nsns.lineplot(x = history.epoch, y = history.history['val_binary_accuracy'])\nax.set_title('Learning Curve (Accuracy)')\nax.set_ylabel('Accuracy')\nax.set_xlabel('Epoch')\nax.set_ylim(0.90, 1.0)\nax.legend(['train', 'val'], loc='best')\nplt.show()","f5a736f4":"score = model_pretrained.evaluate(ds_val, steps = len(val_df)\/BATCH, verbose = 0)\nprint('Val loss:', score[0])\nprint('Val accuracy:', score[1])","d6e2fa8a":"score = model_pretrained.evaluate(ds_test, steps = len(df_test), verbose = 0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","99856f50":"num_label = {'Normal': 0, 'Pneumonia' : 1}\nY_test = df_test['class'].copy().map(num_label).astype('int')","3fa65e81":"ds_test.reset()\npredictions = model_pretrained.predict(ds_test, steps=len(ds_test), verbose=0)\npred_labels= np.where(predictions>0.5, 1, 0)","58fb74b8":"print(\"Test Accuracy: \", accuracy_score(Y_test, pred_labels))","bb863e27":"confusion_matrix = metrics.confusion_matrix(Y_test, pred_labels)\nsns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n\nplt.xlabel(\"Predicted Label\", fontsize= 12)\nplt.ylabel(\"True Label\", fontsize= 12)\n\nplt.show()","35c95cc1":"print(metrics.classification_report(Y_test, pred_labels, labels = [0, 1]))","746667bf":"roc_auc = metrics.roc_auc_score(Y_test, predictions)\nprint('ROC_AUC: ', roc_auc)\n\nfpr, tpr, thresholds = metrics.roc_curve(Y_test, predictions)\n\nplt.plot(fpr, tpr, label = 'ROC_AUC = %0.3f' % roc_auc)\n\nplt.xlabel(\"False Positive Rate\", fontsize= 12)\nplt.ylabel(\"True Positive Rate\", fontsize= 12)\nplt.legend(loc=\"lower right\")\n\nplt.show()","ef246892":"As expected, the fine-tuning approach has reached the best score. We end this notebook by showing a few performance metrics.","23af74b6":"- https:\/\/vijayabhaskar96.medium.com\/tutorial-on-keras-flow-from-dataframe-1fd4493d237c\n- https:\/\/github.com\/mrdbourke\/tensorflow-deep-learning\/blob\/main\/03_convolutional_neural_networks_in_tensorflow.ipynb\n- https:\/\/www.tensorflow.org\/guide\/keras\/transfer_learning\n- https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator\n- https:\/\/keras.io\/api\/applications\/\n- https:\/\/keras.io\/api\/applications\/resnet\/#resnet152v2-function","a38b6066":"# <a id=\"5\">Custom CNN<\/a>","5de1ca14":"## <center> If you find this notebook useful, support with an upvote! <center>","4024ef91":"# <a id=\"4\">Preparing the Data<\/a> ","dece158f":"Our last approach is called Fine Tuning. In the last section, all the layers from the pretrained model were \u2018frozen\u2019, preserving the weights calculated during its training on the ImageNet dataset. Now, we are going to unfreeze a few of its last layers and continue the training, tuning the weights from these layers according to our dataset.","25ab2c0e":"# Overview\n\nThe goal of this notebook is to use Convolutional Neural Networks on Chest X-Ray images to determine which samples are from patients with Pneumonia. In this dataset (version 3), there is one folder representing the train set and another one for the test set. The train folder is later split in the notebook into train\/validation sets.\n\nI use three different approaches for image classification: 1) A simple CNN, 2) Transfer Learning, using a pretrained model with frozen layers as the base for feature extraction and 3) Fine Tuning, unfreezing the last layers of the pretrained model.\n\nNote: I\u2019m using the third version of the Chest X-Ray dataset [(link)](https:\/\/www.kaggle.com\/tolgadincer\/labeled-chest-xray-images)","f72591a0":"# <a id=\"1\">Dataset Information<\/a> \n\nThis dataset contains 5,856 validated Chest X-Ray images. The images are split into a training set and a testing set of independent patients. Images are labeled as (disease:NORMAL\/BACTERIA\/VIRUS)-(randomized patient ID)-(image number of a patient).\n\nChest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children\u2019s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients\u2019 routine clinical care.\n\nFor the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.","c008ecb9":"The recall was close to 100%. Even without expertise on the medical field, it\u2019s reasonable to assume that false negatives are more \u2018costly\u2019 than false positives in this case. Reaching such recall with a relatively small dataset for training as this one, while also reaching a pretty good recall, is a good indicative of the model\u2019s capabilities. Such capabilities are also confirmed by the high ROC-AUC value.","5669fe67":"# <a id=\"9\">References<\/a> ","7c99e33b":"<br>\n<h1 style = \"font-size:30px; font-weight : bold; color : blue; text-align: center; border-radius: 10px 15px;\">Chest X-Ray (Pneumonia): Image Classification w\/Convolutional Neural Networks and Transfer Learning<\/h1>\n<br>\n\n---","85770f39":"Now, we are ready for the next stage: creating and training the image classification models.","94d1a79c":"The distributions from these datasets are a little different from each other. Both are slightly imbalanced, having more samples from the positive class (Pneumonia), with the training set being a little more imbalanced.\n\nBefore we move on to the next section, we will take a look at a few examples from each dataset.","6fc99bfa":"# <a id=\"6\">Transfer Learning<\/a> ","e145c3c4":"<img src=\"https:\/\/www.collinsdictionary.com\/images\/full\/xray_560923156_1000.jpg\" width=\"620\" height=\"360\" align=\"center\"\/>","8213bdf8":"Now, we\u2019re going to load the images from the folders and prepare them to feed our models. \n\nWe begin by defining the data generators. With Keras Image Data Generator, we can rescale the pixel values and apply random transformation techniques for data augmentation on the fly. We define two different generators. The val_datagen is used to simply rescale the validation and test sets. The train_datagen includes some transformations to augment the train set.\n\nWe apply those generators on each dataset using the flow_from_dataframe method. Apart from the transformations defined in each generator, the images are also resized based on the target_size set.","ec8b85ec":"# <a id=\"2\">Importing Packages and Dataset<\/a> ","90021830":"# <a id=\"7\">Fine Tuning<\/a> ","028c6555":"# <a id='0'>Content<\/a>\n\u200b\n- <a href='#1'>Dataset Information<\/a>  \n- <a href='#2'>Importing Packages and Dataset<\/a>  \n- <a href='#3'>Exploring the Data<\/a>  \n- <a href='#4'>Preparing the Data<\/a>\n- <a href='#5'>Custom Model<\/a>\n- <a href='#6'>Transfer Learning<\/a>\n- <a href='#7'>Fine Tuning<\/a>\n- <a href='#8'>Performance Metrics<\/a>\n- <a href='#9'>References<\/a>","836d25e9":"The second approach, called transfer learning, consists of using a pretrained model as a feature extractor. In this notebook, the selected model was the ResNet152V2 available on the Keras Package [(link)](https:\/\/keras.io\/api\/applications\/resnet\/#resnet152v2-function). \n\nThis model was already trained in another dataset (ImageNet). What we do here is to set include_top to false, removing the \u2018head\u2019, responsible for assigning the classes in this other dataset, and keep all the previous layers. Then, we include our last few layers, including the one responsible for generating the output.\n\n","3686f632":"# <a id=\"3\">Exploring the Data<\/a> ","b3d1becf":"First, we need to create a validation set. To do that, we apply a simple stratified split on the original train dataset, using 80% for actual training and 20% for validation purposes.","96dca126":"# <a id=\"8\">Performance Metrics<\/a> ","d0abab9c":"Let\u2019s define our first model \u2018from scratch\u2019 and see how it performs.","72a6dd69":"Let's check the target distribution on each set","0d31c384":"## <center> If you find this notebook useful, support with an upvote! <center>"}}