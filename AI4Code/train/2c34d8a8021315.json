{"cell_type":{"8161f7d3":"code","5f8b15cd":"code","39a52f8d":"code","42a9c276":"code","428e0387":"code","fa886416":"code","d7c549fe":"code","8d4f263d":"code","632f6caa":"code","126a78e1":"code","875241d4":"code","d8633ded":"code","7a1f23bc":"code","c68c2f96":"code","0b96a58a":"code","0d9842e8":"code","ccf39bf4":"code","5ee1dd47":"code","9285ac76":"code","584196d4":"code","08e20c77":"code","899a5314":"code","ca192961":"code","3c00873b":"code","8f2068fa":"code","4a2bc96c":"code","1fe12f62":"code","1f335c14":"code","295b03ad":"code","01d55e86":"code","54317d0c":"code","f1291da8":"code","2bed48da":"code","01554da6":"code","3115d7a7":"code","e3f74d80":"code","933f3891":"code","426735dc":"code","aafd7a91":"code","93972370":"code","87909174":"code","d6411f91":"code","2f883927":"code","32528490":"code","bcdf2208":"code","92c54fbe":"markdown","9696ee52":"markdown","129c0c5f":"markdown","a02a0074":"markdown","bb099140":"markdown","882c3cfd":"markdown","3df31395":"markdown","90910e93":"markdown","dd2099bf":"markdown","1b0c2bc2":"markdown","c0c9caff":"markdown","713e3cc8":"markdown","7463c399":"markdown","d88135b9":"markdown","a2bab7bf":"markdown","b8107fce":"markdown","ca9b9ea0":"markdown","d780810a":"markdown","f1e4c18d":"markdown"},"source":{"8161f7d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5f8b15cd":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","39a52f8d":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder","42a9c276":"train_data","428e0387":"test_data","fa886416":"train_data.dtypes","d7c549fe":"null_values = train_data.isna().sum()\nnull_values","8d4f263d":"train_count = train_data[\"Survived\"].value_counts()\ntrain_count","632f6caa":"pie, ax = plt.subplots(figsize=[10,7])\nlabels = train_count.keys()\nplt.pie(x=train_count, autopct=\"%.1f%%\", explode=[0.05]*2, labels=labels, pctdistance=0.5)\nplt.title(\"passengers: Survived or Died\")\n","126a78e1":"train_data = train_data.drop([\"PassengerId\", \"Ticket\", \"Name\"], axis=1)\ntrain_data","875241d4":"test_data = test_data.drop([\"PassengerId\", \"Ticket\", \"Name\"], axis=1)\ntest_data","d8633ded":"train_data[\"Age\"] = train_data[\"Age\"].fillna((train_data[\"Age\"].mean())).astype(int)\nprint(train_data[\"Age\"].unique())\nprint(train_data[\"Age\"].dtype)","7a1f23bc":"test_data[\"Age\"] = train_data[\"Age\"].fillna((train_data[\"Age\"].mean())).astype(int)\nprint(train_data[\"Age\"].unique())\nprint(train_data[\"Age\"].dtype)","c68c2f96":"train_data[\"Embarked\"].describe()","0b96a58a":"train_data[\"Embarked\"].unique()","0d9842e8":"df = [train_data, test_data]\ntop_value = \"S\"\nfor dataset in df:\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(top_value)\ntrain_data[\"Embarked\"].unique()","ccf39bf4":"train_data[\"Cabin\"].unique()","5ee1dd47":"import re\ndeck = {\"A\":1, \"B\":2, \"C\":3, \"D\":4, \"E\":5, \"F\":6, \"G\":7, \"U\":8}\ndf = [train_data, test_data]\n\nfor dataset in df:\n    dataset[\"Deck\"] = dataset[\"Cabin\"].fillna(\"U0\")\n    dataset[\"Deck\"] = dataset[\"Deck\"].map(lambda x: re.compile(\"([A-Z]+)\").search(x).group())\n    dataset[\"Deck\"] = dataset[\"Deck\"].map(deck)\n    dataset[\"Deck\"] = dataset[\"Deck\"].fillna(0).astype(int)","9285ac76":"train_data[\"Deck\"].describe()","584196d4":"train_data[\"Deck\"].isna().sum()","08e20c77":"train_data = train_data.drop([\"Cabin\"], axis=1)\ntest_data = test_data.drop([\"Cabin\"],axis=1)","899a5314":"train_data","ca192961":"test_data","3c00873b":"train_data[\"SibSp\"].describe()","8f2068fa":"train_data[\"Parch\"].describe()","4a2bc96c":"df= [train_data, test_data]\n\nfor dataset in df:\n    dataset[\"Small_family\"] = np.where((dataset[\"SibSp\"] <= 2), 1, 0)\n    dataset[\"Lonely\"] = np.where((dataset[\"Parch\"] == 1), 1, 0)\n    dataset[\"Family\"] = dataset[\"SibSp\"] + dataset[\"Parch\"]\n    ","1fe12f62":"train_data = train_data.drop([\"SibSp\", \"Parch\"], axis=1)\ntest_data = test_data.drop([\"SibSp\", \"Parch\"], axis=1)","1f335c14":"train_data","295b03ad":"test_data","01d55e86":"X_data = train_data.drop([\"Survived\"], axis=1)\ny_data = train_data.Survived","54317d0c":"ct = make_column_transformer(\n    (MinMaxScaler(), [\"Age\", \"Fare\"]),\n    (OneHotEncoder(handle_unknown=\"ignore\"), [\"Pclass\", \"Sex\", \"Embarked\", \"Deck\"])\n)","f1291da8":"ct.fit(X_data)","2bed48da":"Norm_X_train = ct.transform(X_data)\nNorm_X_train\n","01554da6":"Norm_test_data = ct.transform(test_data)\nNorm_test_data","3115d7a7":"X_train, X_val, y_train, y_val = train_test_split(Norm_X_train, y_data, test_size=0.2, random_state=42)","e3f74d80":"X_train.shape, X_val.shape, y_train.shape, y_val.shape","933f3891":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(10, activation=\"relu\"),\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(2, activation=\"sigmoid\")   \n])\n\nmodel.compile(\n    loss=tf.keras.losses.binary_crossentropy,\n    optimizer=tf.keras.optimizers.Adam(lr=0.0009),\n    metrics=[\"accuracy\"]\n)\n\nhistory = model.fit(\n    X_train,\n    tf.one_hot(y_train, depth=2),\n    batch_size=64,\n    epochs=100,\n    validation_data=(X_val, tf.one_hot(y_val, depth=2))\n)","426735dc":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(10, activation=\"relu\"),\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(2, activation=\"sigmoid\")   \n])\n\nmodel.compile(\n    loss=tf.keras.losses.binary_crossentropy,\n    optimizer=tf.keras.optimizers.Adam(lr=0.0009),\n    metrics=[\"accuracy\"]\n)\n\nhistory = model_2.fit(\n    Norm_X_train,\n    tf.one_hot(y_data, depth=2),\n    batch_size=64,\n    epochs=100,\n    #validation_data=(X_val, tf.one_hot(y_val, depth=2))\n)","aafd7a91":"pd.DataFrame(history.history).plot()","93972370":"y_probs = model_2.predict(Norm_test_data)","87909174":"y_preds=[]\nfor i in y_probs:\n    y_preds.append(i.argmax())","d6411f91":"y_preds","2f883927":"submission","32528490":"submission.Survived = y_preds","bcdf2208":"submission.to_csv(\"\/kaggle\/working\/submission.csv\", index=False)","92c54fbe":"1. Data Deletion\n\n2. Mean\/ Mode\/ Median Imputation for both categorical and continuous data\n\n3. Filling data with numeric values\n\n4. Filling with common\/top value among the categorical data for that specific column","9696ee52":"**Consider only cabin section such as A,B,C etc for Cabin column**","129c0c5f":"# SibSp and Parch to Small_family, Lonely and Family","a02a0074":"# Missing Values Treatmet","bb099140":"# Age","882c3cfd":"**Visualize data types of training data**","3df31395":"**Check null values in dataframe**","90910e93":"**Visualize test data**","dd2099bf":"# Cabin to Deck","1b0c2bc2":"**Add mean age in Nan values of age Column**","c0c9caff":"**Read csv file using Pandas**","713e3cc8":"# Normalize Data","7463c399":"# Embarked","d88135b9":"**Visualize train data**","a2bab7bf":"**Add most comman value in Nan values of Embarked Column**","b8107fce":"**Import required libraries**","ca9b9ea0":"# Prepare Data","d780810a":"**Visualize survival ratio**","f1e4c18d":"**Drop unnessosory columns**"}}