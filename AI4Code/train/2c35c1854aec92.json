{"cell_type":{"800ad0aa":"code","cf6e5f68":"code","1d5c3e7b":"code","7f1b62e2":"code","479159ff":"code","3eb1ad91":"code","0bb47b55":"code","7ff20def":"code","e3e2e67e":"code","1b6faefa":"code","bb558e01":"code","2ac4c04d":"code","90d57101":"code","8f6e8ad8":"code","dc9e6e17":"code","32a3e6e5":"code","e3dc0f39":"code","63702e9c":"code","93f43942":"code","394e07ac":"code","36234f5b":"code","22925bad":"code","b2002b23":"code","acdca7b1":"code","3b8c7d5f":"code","c18c909f":"code","96f0cf95":"code","c6b87959":"code","79810a62":"code","646c1542":"code","033c1b06":"code","e45859db":"code","e4579c25":"code","a7c34f41":"code","2f5841e7":"code","f1d05bbf":"code","bc2be74e":"code","ef324f5e":"markdown","714f4537":"markdown","8fff576b":"markdown"},"source":{"800ad0aa":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n   \nd0 = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\n\nl = d0['label']\n\nd = d0.drop(\"label\", axis=1)\nprint(d.head())","cf6e5f68":"print(d.shape)\nprint(l.shape)","1d5c3e7b":"from sklearn import datasets, model_selection, svm, metrics\n\ntrain_size = 14000\ntest_size = 28000\ndata_train, data_test, label_train, label_test = model_selection.train_test_split(d, l, test_size=test_size, train_size=train_size)\n\nclf = svm.SVC()\nclf.fit(data_train, label_train)\npre = clf.predict(data_test)\n\nac_score = metrics.accuracy_score(label_test, pre)\nprint(ac_score)","7f1b62e2":"import datetime","479159ff":"def create_submission(submission_path, t):\n    result_dict = {\n        'ImageId': np.arange(1, len(t) + 1),\n        'Label': t\n    }\n    \n    now = datetime.datetime.now()\n    created_time = now.strftime('_%Y%m%d_%H%M')\n    \n    submit_file = submission_path + 'submission' + created_time + '.csv'  \n    \n    df = pd.DataFrame(result_dict)\n    df.to_csv(submit_file,\n              index_label=False, index=False)","3eb1ad91":"create_submission('\/kaggle\/working\/', pre)\nsubmit_file = pd.read_csv('\/kaggle\/working\/submission_20200721_0626.csv')","0bb47b55":"len(submit_file)","7ff20def":"\n# core and utility packages\nimport pandas as pd\nimport numpy as np\nimport itertools\n\n# visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\nsns.set(style='white', context='notebook', palette='deep')\n\n# modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n# keras\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler\nfrom keras import models\n\nnp.random.seed(2)","e3e2e67e":"model = Sequential()\n\nmodel.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu',\n                 input_shape = (28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\nmodel.summary()","1b6faefa":"X_train = data_train \/ 255.0\ntest = data_test \/ 255.0\n\nprint(X_train.shape)\nprint(test.shape)","bb558e01":"X_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)\nX_train.shape","2ac4c04d":"# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY_train_value = label_train # keep the origianl label\nY_train = to_categorical(label_train, num_classes = 10)\nY_train.shape\n#Y_train_value = np.argmax(Y_train, axis=1) # keep the origianl label","90d57101":"# Split the train and the validation set for the fitting\nrandom_seed = 2\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)\nX_train.shape","8f6e8ad8":"# Define the optimizer\noptimizer = Adam(lr=1e-4)","dc9e6e17":"model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","32a3e6e5":"# With data augmentation to prevent overfitting\n\ndatagen = ImageDataGenerator(\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1)  # randomly shift images vertically (fraction of total height)\n\ndatagen.fit(X_train)","e3dc0f39":"# Set a learning rate annealer\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n\n# Turn epochs to 30 to get 0.9967 accuracy\nepochs = 30 \nbatch_size = 86","63702e9c":"# Fit the model\nhistory = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_val,Y_val),\n                              verbose = 2, steps_per_epoch=X_train.shape[0] \/\/ batch_size\n                              , callbacks=[annealer])","93f43942":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\n#ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\n#ax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","394e07ac":"# predict results\nresults = model.predict(test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","36234f5b":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","22925bad":"plt.figure(figsize=(7,7))\nidx = 1\n\ngrid_data = d.iloc[idx].to_numpy().reshape(28,28)\nplt.imshow(grid_data, interpolation = \"none\", cmap = \"gray\")\nplt.show()\n\nprint(l[idx])","b2002b23":"# using 18000 dataset for training\n\nlabels = l.head(18000)\ndata = d.head(18000)\n\nprint(\"the shape of sample data = \", data.shape)\n","acdca7b1":"# Data-preprocessing: Standardizing the data\n\nfrom sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(data)\nprint(standardized_data.shape)\n","3b8c7d5f":"#find the co-variance matrix which is : A^T * A\nsample_data = standardized_data\n\n# matrix multiplication using numpy\ncovar_matrix = np.matmul(sample_data.T , sample_data)\n\nprint ( \"The shape of variance matrix = \", covar_matrix.shape)\n","c18c909f":"\n# for projecting onto a 2-Dim space.\n\nfrom scipy.linalg import eigh \n\n# the parameter 'eigvals' is defined (low value to heigh value) \n# eigh function will return the eigen values in asending order\n# this code generates only the top 2 (782 and 783) eigenvalues.\nvalues, vectors = eigh(covar_matrix, eigvals=(782,783))\n\nprint(\"Shape of eigen vectors = \",vectors.shape)\n# converting the eigen vectors into (2,d) shape for easyness of further computations\nvectors = vectors.T\n\nprint(\"Updated shape of eigen vectors = \",vectors.shape)\n# here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector\n# here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector","96f0cf95":"# projecting the original data sample on the plane \n#formed by two principal eigen vectors by vector-vector multiplication.\n\nimport matplotlib.pyplot as plt\nnew_coordinates = np.matmul(vectors, sample_data.T)\n\nprint (\" resultanat new data points' shape \", vectors.shape, \"X\", sample_data.T.shape,\" = \", new_coordinates.shape)","c6b87959":"import pandas as pd\n\n# appending label to the 2d projected data\nnew_coordinates = np.vstack((new_coordinates, labels)).T\n\n# creating a new data frame for ploting the labeled points.\ndataframe = pd.DataFrame(data=new_coordinates, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nprint(dataframe.head())","79810a62":"# ploting the 2d data points with seaborn\nimport seaborn as sn\nsn.FacetGrid(dataframe, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","646c1542":"# initializing the pca\nfrom sklearn import decomposition\npca = decomposition.PCA()\n","033c1b06":"# configuring the parameteres\n# the number of components = 2\npca.n_components = 2\npca_data = pca.fit_transform(sample_data)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)\n\n","e45859db":"# attaching the label for each 2-d data point \npca_data = np.vstack((pca_data.T, labels)).T\n\n# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nsn.FacetGrid(pca_df, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","e4579c25":"# PCA for dimensionality redcution (non-visualization)\n\npca.n_components = 784\npca_data = pca.fit_transform(sample_data)\n\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()\n\n\n# If we take 200-dimensions, approx. 90% of variance is expalined.","a7c34f41":"# TSNE\n\nfrom sklearn.manifold import TSNE\n\n# Picking the top 1000 points as TSNE takes a lot of time for 15K points\ndata_1000 = standardized_data[0:15000,:]\nlabels_1000 = labels[0:15000]\n\nmodel = TSNE(n_components=2, random_state=0)\n# configuring the parameteres\n# the number of components = 2\n# default perplexity = 30\n# default learning rate = 200\n# default Maximum number of iterations for the optimization = 1000\n\ntsne_data = model.fit_transform(data_1000)\n\n\n# creating a new data frame which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.show()","2f5841e7":"model = TSNE(n_components=2, random_state=0, perplexity=50)\ntsne_data = model.fit_transform(data_1000) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 50')\nplt.show()","f1d05bbf":"model = TSNE(n_components=2, random_state=0, perplexity=50,  n_iter=5000)\ntsne_data = model.fit_transform(data_1000) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 50, n_iter=5000')\nplt.show()","bc2be74e":"model = TSNE(n_components=2, random_state=0, perplexity=2)\ntsne_data = model.fit_transform(data_1000) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 2')\nplt.show()","ef324f5e":"# PCA using Scikit-Learn","714f4537":"# t-SNE using Scikit-Learn","8fff576b":"# PCA for dimensionality redcution (not for visualization)"}}