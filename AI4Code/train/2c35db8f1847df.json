{"cell_type":{"d4c73671":"code","0c1ad9a0":"code","567e1b84":"code","a0bb0c78":"code","232b50ea":"code","8d3dbd99":"code","19a1873e":"code","223455c1":"code","260076a3":"code","b1b494cf":"code","b437b4bb":"code","458e993f":"code","f629d02f":"code","351c4210":"code","96a73439":"code","8ba6bc6b":"code","3f664c26":"code","b86ec910":"code","892259ad":"code","11fc83dc":"code","cd1825c6":"markdown","59aa4975":"markdown","10aaa770":"markdown","9b8e2d01":"markdown"},"source":{"d4c73671":"import os\nimport numpy as np\nimport pandas as pd\nfrom pprint import pprint \nimport  matplotlib.pyplot as plt\n\nimport networkx as nx\n\n%load_ext autoreload\n%autoreload 2\n","0c1ad9a0":"!pip install git+https:\/\/github.com\/syasini\/NLOOP.git@master","567e1b84":"from nloop import Text","a0bb0c78":"data_fname = os.path.join(\"..\",\"input\",\"CORD-19-research-challenge\", \"metadata.csv\")\ndata = pd.read_csv(data_fname, index_col=0,)\n\n\ndata = data.sample(5000) # let's look at a small sample of the data \ndata.reset_index(inplace=True)","232b50ea":"# drop all the nan values in titles and abstracts\ndata.dropna(subset = [\"abstract\", \"title\"], inplace=True)\n\n# combine title and abstract into new column called 'text'\ndata[\"text\"] = data[\"title\"].combine(data[\"abstract\"], lambda s1, s2: \". \".join([s1, s2]))","8d3dbd99":"# process text with nloop\ntext = Text(data[\"text\"], fast=False)\n\n# This will take a while for the entire corpus\n# use fast=True if you're only interested in clean tokens\n# and don't need dependencies, named entities, and keywords","19a1873e":"# show word cloud\ntext.show_wordcloud()","223455c1":"#show the most common tokens\ntext.token_counter.most_common(20)","260076a3":"# run LDA \ntext.lda.run(num_topics=10)","b1b494cf":"# show all the topics\npprint(text.lda.model.show_topics(10))","b437b4bb":"# extract keywords and their ranks (NLOOP uses the pytextrank pipeline from spacy)\nkeywords_list = [x for item in text.keywords.texts for x in item]\nranks_list = [x for item in text.keywords.ranks for x in item]","458e993f":"# only keep top keywords that pass the rank_cutoff threshold\nrank_cutoff = 0.1 \n\ntop_keywords = []\nfor keywords, ranks in zip(text.keywords.texts , text.keywords.ranks ):\n    top_keywords.append((np.array(keywords)[np.array(ranks)>rank_cutoff]).tolist())\n    ","f629d02f":"from itertools import combinations, chain\nfrom collections import Counter\n\n# form pairs of keywords from each document\nkeyword_pair = list(chain(\n                    *[list(combinations(kw_list,2)) for kw_list in top_keywords])\n                   )\n","351c4210":"# count all the unique pairs\npair_counter = Counter(keyword_pair).items()","96a73439":"print(len(pair_counter))","8ba6bc6b":"def get_pair_graph(pair_counter, weight_times=1, degree_cutoff=50):\n    \"\"\"construct the co-ocurrence graph from the keyword pairs\n    \n    Parameters\n    ----------\n    pair_counter: Counter dictionary\n        consists of keyword pairs as keys and their counts as values\n    weight_times: int or float (scalar)\n        multplicative factor for weights of edges in the graph\n    degree_cutoff: int\n        nodes with degrees below this number will be ignored\n        \n    Return\n    ------\n    G: networkx graph instance\n    \n    \"\"\"\n    \n    G = nx.Graph()\n\n    #construct the graph from the edges\n    for pair, weight in pair_counter:\n    \n        G.add_edge(*pair, weight=weight_times*(weight))\n    \n    # remove nodes with degrees smaller than the cutoff\n    node_list = []\n    for node in np.copy(G.nodes):\n        if G.degree(node)<degree_cutoff:\n            \n            G.remove_node(node) \n    \n    return G","3f664c26":"# get the keyword pair graph\nG = get_pair_graph(pair_counter, degree_cutoff=50)","b86ec910":"# calculate the node sizes using arbitrary transformation \nnode_sizes= [20*G.degree[node]**2+100 for node in G.nodes]\n\n# construct the label dictionary\nlabels = {i:i for i in list(G.nodes)}","892259ad":"print(len(G.nodes))","11fc83dc":"# draw the graph\nplt.figure(figsize=(10,10),dpi=100)\n\npos = nx.spring_layout(G, k=3, \n                       fixed=[\"viruses\"], pos={\"viruses\":(0,0)}, \n                       dim=2, iterations=50)\n\n\nnx.draw_networkx_nodes(G, pos, \n                       #with_labels=True, \n                       node_color=\"tab:orange\",\n                       node_size=node_sizes, \n                       node_shape=\"8\", \n                       edgecolors=\"tab:red\",\n                      )\n\nnx.draw_networkx_edges(G, pos, \n                       #with_labels=True, \n                       edgecolors=\"grey\",\n                       alpha=0.1,\n                      )\n\n_= nx.draw_networkx_labels(G, pos, \n                        labels=labels, \n                        )","cd1825c6":"NLOOP is a python package that provides a convenient interface for exploring and analyzing text data. \nBehind the scene, NLOOP uses spaCy and gensim to take care of cleaning, tokenization, dependency parsing, keyword extraction and much more in one fell swoop. Here I will use it to build a keyword co-occurrence graph from the titles and abstracts of the research papers in the provided dataset. \n\nYou can install NLOOP from the following address. Checkout the github repository for more examples. ","59aa4975":"# [NLOOP](https:\/\/github.com\/syasini\/NLOOP)","10aaa770":"**Challenge**: Find the bats!","9b8e2d01":"# Covid-19 Keyword Co-occurrence Graph"}}