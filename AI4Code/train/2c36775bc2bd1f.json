{"cell_type":{"71e9f2b4":"code","da435be4":"code","d20dffd1":"code","7a4fa5be":"code","94ac9b92":"code","17338408":"code","37ae6257":"markdown","e49d46e5":"markdown"},"source":{"71e9f2b4":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder","da435be4":"train_data = pd.read_csv('..\/input\/healthcareanalyticsii\/train.csv')\n\ntrain_data.drop(['case_id', 'patientid', 'Stay'], axis = 1, inplace = True)\n\ntest_data = pd.read_csv('..\/input\/healthcareanalyticsii\/test.csv')\n\ntest_data.drop(['case_id', 'patientid'], axis = 1, inplace = True)","d20dffd1":"train_data['City_Code_Patient'] = train_data['City_Code_Patient'].fillna(-1)\ntrain_data['Bed Grade'] = train_data['Bed Grade'].fillna(-1)\n\ntest_data['City_Code_Patient'] = test_data['City_Code_Patient'].fillna(-1)\ntest_data['Bed Grade'] = test_data['Bed Grade'].fillna(-1)\n\n\ntrain_data['istrain'] = 1\n\ntest_data['istrain'] = 0\n\ncombined_data = pd.concat([train_data, test_data], axis = 0)","7a4fa5be":"df_numeric = combined_data.select_dtypes(exclude=['object'])\n\ndf_obj = combined_data.select_dtypes(include=['object']).copy()\n    \nfor c in df_obj:\n    df_obj[c] = pd.factorize(df_obj[c])[0]\n    \ncombined_data = pd.concat([df_numeric, df_obj], axis=1)\n\ny = combined_data['istrain']\n\ncombined_data.drop('istrain', axis = 1, inplace = True)\n","94ac9b92":"skf = StratifiedShuffleSplit(n_splits = 5, random_state = 44,test_size =0.3)\nxgb_params = {\n        'learning_rate': 0.1, 'max_depth': 6,'subsample': 0.9,\n        'colsample_bytree': 0.9,'objective': 'binary:logistic',\n        'n_estimators':100, 'gamma':1,\n        'min_child_weight':4\n        }   \nclf = xgb.XGBClassifier(**xgb_params, seed = 10)     ","17338408":"for train_index, test_index in skf.split(combined_data, y):\n       \n        x0, x1 = combined_data.iloc[train_index], combined_data.iloc[test_index]\n        \n        y0, y1 = y.iloc[train_index], y.iloc[test_index]        \n        \n        print(x0.shape)\n        \n        clf.fit(x0, y0, eval_set=[(x1, y1)],\n               eval_metric='logloss', verbose=False,early_stopping_rounds=10)\n                \n        prval = clf.predict_proba(x1)[:,1]\n        \n        print(roc_auc_score(y1,prval))","37ae6257":"[Adversarial validation][1]\n\nThe general idea is to check the degree of similarity between training and tests in terms of feature distribution: if they are difficult to distinguish, the distribution is probably similar and the usual validation techniques should work. It does not seem to be the case, so we can suspect they are quite different. This intuition can be quantified by combining train and test sets, assigning 0\/1 labels (0 - train, 1-test) and evaluating a binary classification task.\n\n\n  [1]: http:\/\/fastml.com\/adversarial-validation-part-two\/","e49d46e5":"As we can see, the separation is almost indistiguishable. Train and Test set must come from similar distribution and normal validation techniques should work"}}