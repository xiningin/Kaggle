{"cell_type":{"faa2fbf0":"code","d2098436":"code","fa46b8bc":"code","a75f31e3":"code","eeb9112b":"code","70c78d60":"code","01522034":"code","1a5c8fae":"code","d4f06fd0":"code","ac06dbba":"code","88ab2f6e":"code","d48e9dc3":"code","737c217f":"code","3369ba6e":"code","a9bb766e":"code","61825fc4":"code","1f751a4c":"code","e8cf74e6":"code","733beedf":"code","7bde3f30":"code","189e0adc":"code","3da4d75c":"code","5d2fa27d":"code","df57131a":"code","9f0a6a2f":"code","f787b54a":"code","c78ed6ff":"code","ffdba0d4":"code","f2d0ae8c":"code","c9518191":"code","9f4db297":"code","2b754f13":"code","e7b5e2e0":"code","42f2de3c":"code","ae0b1f63":"code","52f9a3e9":"code","671c8a84":"code","faa5ab84":"code","c6ee9f3d":"code","2eaf60aa":"code","ac634f57":"code","b28aded5":"code","ff4eadaa":"code","2491f500":"code","69363877":"code","c59c02fc":"code","8f56825a":"code","99239b50":"code","6cbb9be1":"code","b0183b4d":"markdown","94db9c43":"markdown","8a54cf18":"markdown","6d8bda80":"markdown","69314339":"markdown","02f0b613":"markdown","41841832":"markdown","89f0cade":"markdown","dad76bec":"markdown","b1384d40":"markdown","60f9e7ee":"markdown","5ffe5422":"markdown","1e1cb176":"markdown","5db65325":"markdown","91dab1f8":"markdown","02fdd45b":"markdown","e1f3c868":"markdown","68897b9d":"markdown","5ad5508a":"markdown","905b7e0c":"markdown","9675786d":"markdown","10faa657":"markdown","f18087eb":"markdown","0e2923a2":"markdown","6022cfc3":"markdown","84e1a984":"markdown","e5d5074a":"markdown","af055e82":"markdown","35a85741":"markdown","7ab7890f":"markdown","c1eb5502":"markdown","4f1e6715":"markdown","185dc0c8":"markdown","b83274ae":"markdown","ba4e88fa":"markdown","c52e802b":"markdown","9be57744":"markdown","9977a08d":"markdown","f26f2c4f":"markdown","f8037d9b":"markdown","85b8df51":"markdown","3ac904dc":"markdown","90c02096":"markdown","37c392fd":"markdown","d62a3036":"markdown","36f3e032":"markdown","ca5966e1":"markdown","230deda5":"markdown"},"source":{"faa2fbf0":"#-------------------------------------\n# General\n#-------------------------------------\n\nimport math\nfrom statistics import mode\n\n#-------------------------------------\n# Data Analysis\n#-------------------------------------\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport pandas_profiling\n\n#-------------------------------------\n# Visualisation\n#-------------------------------------\n\n# matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import cm\nimport warnings\nimport matplotlib.cbook\nwarnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\nimport matplotlib.dates as mdates\n\n# seaborn\nimport seaborn as sns\nsns.set_theme(style='dark')\n\n# html\nfrom IPython.core.display import display, HTML\n\n#-------------------------------------\n# Feature Extraction\n#-------------------------------------\n\n# Numeric\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n# Categorical\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Transformer\nfrom sklearn.compose import ColumnTransformer\n\n# Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import make_column_transformer","d2098436":"def heading_3(title, description):\n    display(HTML('<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> ' + title + '<\/h3><div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>' + description + '<br><br><br>'))","fa46b8bc":"def plt_font():\n    small_size = 10\n    title_size = 16\n    plt.rc('font', size=small_size)          # controls default text sizes\n    plt.rc('axes', titlesize=small_size)     # fontsize of the axes title\n    plt.rc('axes', labelsize=small_size)    # fontsize of the x and y labels\n    plt.rc('xtick', labelsize=small_size)    # fontsize of the tick labels\n    plt.rc('ytick', labelsize=small_size)    # fontsize of the tick labels\n    plt.rc('legend', fontsize=small_size)    # legend fontsize\n    plt.rc('figure', titlesize=small_size)  # fontsize of the figure title   \n    plt.rc('xtick.major', size=small_size)  # fontsize of the figure title   \n    plt.rc('ytick.major', size=small_size)  # fontsize of the figure title   \n    plt.rc('xtick.minor', size=small_size)  # fontsize of the figure title   \n    plt.rc('ytick.minor', size=small_size)  # fontsize of the figure title   \n    plt.xticks(fontsize=small_size)\n    plt.yticks(fontsize=small_size)","a75f31e3":"purple = '\\033[95m'\ncyan = '\\033[96m'\ndarkcyan = '\\033[36m'\nblue = '\\033[94m'\ngreen = '\\033[92m'\nyellow = '\\033[93m'\nred = '\\033[91m'\nbold = '\\033[1m'\nunderline = '\\033[4m'\nend = '\\033[0m'","eeb9112b":"%%html\n<style type=\"text\/css\" >\n    table\n    {\n        display: inline-block;\n        text-align: left !important;\n    }\n\n    th\n    {\n        background-color: rgb(75, 207, 173) !important;\n        color: white;\n    }\n    \n    th:first-of-type\n    {\n        background-color: rgb(245, 245, 245) !important;\n        color: grey;\n        font-weight: normal;\n    }\n    \n    th:empty\n    {\n        background-color: rgb(75, 207, 173) !important;        \n    }\n        \n    tr:nth-child(even)\n    {\n        background-color: rgb(245, 245, 245) !important;\n    }\n    \n    td, th, tr\n    {\n        text-align: left !important;\n        \n    }\n<\/style>","70c78d60":"target_feature = \"Attrition\"\n\n# feature extraction\npairwise_correlation_features = []\nlow_variation_features = []\nsingle_value_features = []\ndrop_features = []\ndrop_rows_missing = []\nimpute_missing = []\nduplicate_rows = []\nhigh_outliers = []\ncorrelation_ranking = []\n\n# categorical\ncategorical_features = []\nnominal_features = []\nordinal_features = []\n\n# numerical\nnumerical_features = []\ncontinuous_features = []\ndiscrete_features = []","01522034":"# load data\n_train = pd.read_csv(\"..\/input\/employee-attrition\/employee_attrition_train.csv\")\n_test = pd.read_csv(\"..\/input\/employee-attrition\/employee_attrition_test.csv\")\n_train.name = \"hr_train\"\n_test.name = \"hr_test\"\n\n# create a seperate train variable to use for EDA\ntrain = _train.copy()","1a5c8fae":"# !pip install pandas-profiling==3.0.0","d4f06fd0":"pandas_profiling.ProfileReport(train)","ac06dbba":"def get_table_info(df):\n    \n    \"\"\"\n    High level information that can be picked up without looking at what is in the columns, rows or cells yet\n    \"\"\"\n    \n    df_shape = df.shape\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    \n    data = {\n    'Columns (Shape)':df_shape[1], \n    'Rows (Shape)':df_shape[0], \n    'Duplicate Rows':df.duplicated().sum(),\n    'Columns with NaN':df.isnull().any(axis=0).sum(),\n    'Rows with NaN':df.isnull().any(axis=1).sum(),\n    'Cells with NaN': df.isnull().sum().sum(),\n    }\n    \n    # add value counts to the end of the series\n    data.update(df.dtypes.value_counts())\n    \n    # convert series to dataframe in order to view as a table\n    df_ret = pd.DataFrame(data, index=[\"Amount\"]).transpose().reset_index()\n    \n    display(df_ret)","88ab2f6e":"# get high level information of the data\nget_table_info(train)","d48e9dc3":"def get_column_info(df):\n    \n    # ------------------------------------------------------\n    # gives the following information:\n    #\n    # - column names\n    # \n    # ------------------------------------------------------\n    \n    global single_value_features\n    \n    col_single_unique = []\n    col_unique_sum = []\n    percentage_missing = []\n    amount_missing = []\n    data_type = []\n    skewness = []\n    \n    # get data types\n    for i in df.dtypes:\n        data_type.append(i)\n    \n    for i in df.columns:\n        \n        # get skewness\n        try:\n            skewness.append(train[i].skew())\n        except:\n            skewness.append(0)\n        \n        # get percentage missing\n        perc_mis = round(df[i].isna().sum() * 100 \/ len(df[i]), 2)\n        if perc_mis == 0:\n            perc_mis = \"\"\n        percentage_missing.append(perc_mis)\n        \n        # get amount missing\n        amount_mis = df[i].isna().sum()\n        if amount_mis == 0:\n            amount_mis = \"\"\n        amount_missing.append(amount_mis)\n        \n        col_unique_sum.append(df[i].nunique())\n        if df[i].nunique() <= 1:\n            col_single_unique.append(True)\n            single_value_features.extend([i])\n        else:\n            col_single_unique.append(\"\")\n            \n    data = {\n    'Column Name':df.columns, \n    '1 Unique Value (Drop Col)':col_single_unique, \n    'Unique Amount of ' + str(df.shape[0]):col_unique_sum, \n    '% Missing':percentage_missing,\n    'Amount Missing of ' + str(df.shape[0]): amount_missing,\n    '% Data Type':data_type,\n    'Skew':skewness\n    }\n    \n    df_table = pd.DataFrame(data)\n    df_describe = pd.DataFrame(df.describe(include = 'all').transpose()).reset_index().drop(['index'], axis = 1)\n    \n    df_table = pd.concat([df_table, df_describe], axis=1)\n    \n    display(HTML('<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> ' + 'Dataframe Information' + '<\/h3><div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>' + 'Couple of common functions shown in one view.' + '<br><br>'))\n    display(df_table)\n    display(HTML('<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> ' + 'Dataframe Head\/Tail' + '<\/h3><div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>' + '' + '<br>'))\n    display(df)\n    \n    # histogram to view the distribution of the data\n    _hist = df.hist(bins=10, figsize=(20,20))\n    plt_font()\n    heading_3(\"Histogram (all features)\", '')\n    plt.show()","737c217f":"# get information about the data's columns\nget_column_info(train)","3369ba6e":"# adding actions to global variables\n\ndrop_rows_missing.extend(['Age'])\nimpute_missing.extend(['BusinessTravel', 'MaritalStatus', 'DailyRate', 'DistanceFromHome'])\ndrop_features.extend(['EmployeeNumber'])","a9bb766e":"categorical_features = ['Attrition', 'BusinessTravel', 'Department', 'Education', 'EducationField', 'EnvironmentSatisfaction', 'Gender', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus', 'OverTime', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'WorkLifeBalance']\nnumerical_features = list(set(train.columns.tolist()) - set(categorical_features) -set(single_value_features))","61825fc4":"# prints the unique values of each feature marked as categorical\ndef unique_cat_features(df, _categorical_features):\n    \n    print(bold + \"The following are the unique values of each categorical feature:\" + end + \"\\n\")\n    \n    for i in _categorical_features:\n        print(bold + underline + i + end)\n        print(list(df[i].unique()))\n        print('---------------------------------------')","1f751a4c":"unique_cat_features(train, categorical_features)","e8cf74e6":"# define nominal vs ordinal\n# define the nominal and then subtract the nominal from the categorical to get the left over ordinal\nnominal_features = ['Attrition', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime']\nordinal_features = list(set(categorical_features) - (set(nominal_features)))","733beedf":"def view_numerical_features(df, _numerical_features):\n    display(df[_numerical_features])\n    display(df[_numerical_features].dtypes)\n    \n    print(\"============================\")\n    print(\"The following columns should actually be int types\")\n    \n    _float = ['float16', 'float32', 'float64']\n    \n    for i in df[_numerical_features]:\n        \n        if df[i].dtypes in _float:\n            \n            col_check = df[i].dropna()\n            if np.array_equal(col_check, col_check.astype(int)) == True:\n                print(' - ' + i)","7bde3f30":"view_numerical_features(train, numerical_features)","189e0adc":"discrete_features = ['YearsInCurrentRole', 'TotalWorkingYears', 'MonthlyRate', 'DistanceFromHome', 'YearsWithCurrManager', 'HourlyRate', 'TrainingTimesLastYear', 'DailyRate', 'PercentSalaryHike', 'EmployeeNumber', 'NumCompaniesWorked', 'YearsAtCompany', 'YearsSinceLastPromotion', 'Age', 'MonthlyIncome']\ncontinuous_features = list(set(numerical_features) - set(discrete_features))","3da4d75c":"def uni_numerical_boxplots(df, numerical_features):\n    \n    '''\n    Creates a boxplot of all the numerical values\n    '''\n    \n    # normalize columns\n    x_scaled = preprocessing.MinMaxScaler().fit_transform(df[numerical_features].values)\n    new_df = pd.DataFrame(x_scaled)\n    new_df.columns = numerical_features\n    new_df = new_df.melt()\n    \n    # boxplot\n    plt.figure(figsize=(15,10))\n    bp=sns.boxplot(x='variable',y='value', data = new_df)\n    bp.set_xticklabels(bp.get_xticklabels(),rotation=90)\n    plt_font()\n    heading_3(\"Boxplot (normalized numerical features)\", '')\n    plt.show()\n    \n    # non-normalize columns\n    non_norm_df = pd.DataFrame(df[numerical_features].values)\n    non_norm_df.columns = numerical_features\n    non_norm_df = non_norm_df.melt()\n    \n    fig = plt.figure(figsize=(15,10))\n    \n    cols_count = 5\n    rows_count = math.ceil(len(numerical_features)\/cols_count)\n    \n    counter = 1\n    for i in numerical_features:\n        \n        plt.subplot(rows_count,cols_count,counter) # 1 row, 2 columns, Plot 1\n        sns.boxplot(x='variable',y='value', data = non_norm_df[non_norm_df['variable'] == i])\n\n        counter += 1\n        \n    fig.tight_layout()\n    plt_font()\n    heading_3(\"Boxplot (non-normalized numerical features)\", '')\n    plt.show()\n        \n    # min\/max\n    heading_3('Min\/Max Range', 'Check amount of variation to see if columns should be dropped')\n    minmax_df = pd.DataFrame()\n    minmax_df['min'] = df[numerical_features].min()\n    minmax_df['max'] = df[numerical_features].max()\n    minmax_df['range'] = df[numerical_features].max() - df[numerical_features].min()\n    display(minmax_df)","5d2fa27d":"uni_numerical_boxplots(train, numerical_features)","df57131a":"def bi_numerical_boxplots(df, numerical_features, _target):\n    \n    '''\n    Creates a boxplot of all the numerical values\n    '''\n    \n    # normalize columns\n    numeric_data = df[numerical_features].melt()\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(df[numerical_features].values)\n    new_df = pd.DataFrame(x_scaled)\n    new_df.columns = numerical_features\n    new_df[_target] = df[_target]\n    new_df = new_df.melt(id_vars=['Attrition'])\n    \n    # boxplot\n    plt.figure(figsize=(15,10))\n    bp=sns.boxplot(x='variable',y='value',hue = _target, data = new_df)\n    bp.set_xticklabels(bp.get_xticklabels(),rotation=90)\n    plt_font()\n    heading_3(\"Boxplot (normalized numerical features)\", '')\n    plt.show()\n    \n    # non-normalize columns\n    numeric_data = df[numerical_features].melt()\n    x_scaled = df[numerical_features].values\n    non_norm_df = pd.DataFrame(x_scaled)\n    non_norm_df.columns = numerical_features\n    non_norm_df[_target] = df[_target]\n    non_norm_df = non_norm_df.melt(id_vars=['Attrition'])\n    \n    # boxplot\n#     plt.figure(figsize=(15,10))\n#     bp=sns.boxplot(x='variable',y='value',hue = _target, data = new_df)\n#     bp.set_xticklabels(bp.get_xticklabels(),rotation=90)\n#     plt_font()\n#     heading_3(\"Boxplot (normalized numerical features)\", '')\n#     plt.show()\n    \n    fig = plt.figure(figsize=(15,10))\n    \n    cols_count = 5\n    rows_count = math.ceil(len(numerical_features)\/cols_count)\n    \n    counter = 1\n    for i in numerical_features:\n        \n        plt.subplot(rows_count,cols_count,counter) # 1 row, 2 columns, Plot 1\n        sns.boxplot(x='variable',y='value', hue = _target, data = non_norm_df[non_norm_df['variable'] == i])\n\n        counter += 1\n        \n    fig.tight_layout()\n    plt_font()\n    heading_3(\"Boxplot (non-normalized numerical features)\", '')\n    plt.show()","9f0a6a2f":"bi_numerical_boxplots(train, numerical_features, target_feature)","f787b54a":"def uni_categorical_countplot(df, _categorical_features, _target):\n    \n    uni_cat = df[_categorical_features].melt(id_vars=[_target])\n    \n    uni_cat_fg = sns.FacetGrid(uni_cat, col='variable', sharex=False, sharey=False, dropna=True, height=4, col_wrap=3)\n    histPlot=uni_cat_fg.map(sns.countplot,'value')\n    uni_cat_fg.add_legend()\n    [plt.setp(ax.get_xticklabels(), rotation=45) for ax in uni_cat_fg.axes.flat]\n    uni_cat_fg.fig.tight_layout()\n    plt.show()","c78ed6ff":"global target_feature\nglobal categorical_features\nuni_categorical_countplot(train, categorical_features, target_feature)","ffdba0d4":"def countplot(x, hue, **kwargs):\n    data=kwargs.pop(\"data\")\n    sns.countplot(x=x, hue=hue, data=data,**kwargs)\n\n\ndef categorical_countplot(df, _categorical_features, _target):\n    #Bivariate Analysis for actual categorical Variables with Attrition\n    NumericBVData = df[_categorical_features].melt(id_vars=[_target])\n    \n    g = sns.FacetGrid(NumericBVData, col='variable', dropna=True, height=4, col_wrap=4, sharex=False, sharey=False)\n    g.map_dataframe(countplot, 'value',hue=_target, alpha=.6, palette=sns.color_palette())\n\n    [plt.setp(ax.get_xticklabels(), rotation=45) for ax in g.axes.flat]\n    g.fig.tight_layout()\n    plt.show()\n    \n    \nglobal target_feature\nglobal categorical_features\ncategorical_countplot(train, categorical_features, target_feature)","f2d0ae8c":"from scipy import stats\n# display(train[(np.abs(stats.zscore(train[numerical_features])) < 3)])\ndisplay(train[(np.abs(stats.zscore(train[numerical_features])) < 3)])","c9518191":"def view_outliers(df, numerical_features):\n\n    Q1 = df.quantile(0.25)\n    Q3 = df.quantile(0.75)\n    IQR = Q3 - Q1\n\n    df_outliers = (df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))\n    \n    # heatmap\n    plt.figure(figsize=(15, 15))\n    plt_font()\n    heading_3('Heatmap: Visualise Outlier Data', '')\n    missing_hm = sns.heatmap(df_outliers ==True, yticklabels=False, cbar=False, cmap=cm.Blues)#'viridis')\n    plt.show()\n    \n    total = 0\n    outlier_list = []\n    \n    for i in df_outliers.columns:\n        total += len(df_outliers[df_outliers[i]==True])\n    \n        non_outlier_count = len(df_outliers[df_outliers[i]==False])\n        df_rows_count = df_outliers.shape[0]\n        outlier_count = len(df_outliers[df_outliers[i]==True])\n        outlier_percentage = round((100\/df_rows_count)*outlier_count, 2)\n        \n        from pandas.api.types import is_string_dtype\n        from pandas.api.types import is_numeric_dtype\n\n        if is_numeric_dtype(df[i]):\n            IQR_value = IQR[i]\n        else:\n            IQR_value = np.nan\n\n        outlier_list.append([IQR_value , non_outlier_count, outlier_count, outlier_percentage])\n            \n    outlier_df = pd.DataFrame(outlier_list, columns=['IQR', 'Non Outliers Count', 'Outlier Count', 'Outlier %'], index=df_outliers.columns)\n    \n    heading_3('Table: Understand Outlier Data', '')\n    display(outlier_df)\n    \nview_outliers(train, numerical_features)","9f4db297":"high_outliers.extend(['PerformanceRating', 'TrainingTimesLastYear', 'YearsSinceLastPromotion'])","2b754f13":"def data_correlation(df, _target):\n    \n    global correlation_ranking\n    \n    new_df = df.copy()\n    new_df[_target] = new_df[_target].astype('category').cat.codes\n\n    # heatmap\n    plt.figure(figsize=(18,15))\n    sns.heatmap(new_df.corr(),annot=True, cmap=cm.Blues)\n    plt.show()\n\n    # sorted matrix\n    corr_matrix = new_df.corr()\n    correlation_ranking = corr_matrix[_target].abs().sort_values(ascending=False)\n    display(correlation_ranking)","e7b5e2e0":"data_correlation(train, target_feature)","42f2de3c":"import missingno as msno\n\ndef visualize_missing_data(df):\n    \n    cols_list = df.columns.tolist()\n    \n    # heatmap\n    # missing data heat map to visualise how many of the collumns share missing data\n    plt.figure(figsize=(5, 5))\n    plt_font()\n    heading_3('Heatmap: Visualise Missing Data', 'Missing data heat map to visualise how many of the collumns\/features share missing data. Look for where cells\/lines group together or where multiple cells in rows are highlighted together.')\n    missing_hm = sns.heatmap(df[cols_list].isnull(), yticklabels=False, cbar=False, cmap=cm.Blues)#'viridis')\n    plt.show()\n    \n    # heatmap    \n    msno.heatmap(df[cols_list], figsize=(5, 5), cmap=cm.Blues)\n    plt_font()\n    heading_3(\"Heatmap: Missing Data Correlation\", '')\n    plt.show()\n    \n    # dendrogram\n    msno.dendrogram(df[df.columns], figsize=(10, 2.5), fontsize = 10)\n    plt_font()\n    heading_3(\"Dendrogram\", 'Visualise hierarchical relationship between features.')\n    plt.show()\n    \n","ae0b1f63":"visualize_missing_data(train)","52f9a3e9":"get_column_info(train[numerical_features])","671c8a84":"def feature_attributes_series(feature_list, attribute_list):\n    \n    feature_has_attribute = []\n    for i in feature_list:\n        feature_has_attribute.append(i in attribute_list)\n        \n    pd.Series(feature_has_attribute, index = feature_list)\n        \n    return(pd.Series(feature_has_attribute, index = feature_list).T)","faa5ab84":"att_list = [numerical_features, continuous_features, discrete_features]\n\natt_dict = {\n    'numerical' : feature_attributes_series(train.columns, numerical_features),\n    'continuous' : feature_attributes_series(train.columns, continuous_features),\n    'discrete' : feature_attributes_series(train.columns, discrete_features),\n    'categorical' : feature_attributes_series(train.columns, categorical_features),\n    'nominal' : feature_attributes_series(train.columns, nominal_features),\n    'ordinal' : feature_attributes_series(train.columns, ordinal_features),\n    'high correlation' : feature_attributes_series(train.columns, pairwise_correlation_features),\n    'single value' : feature_attributes_series(train.columns, single_value_features),\n    'low variance' : feature_attributes_series(train.columns, low_variation_features),\n    'duplicat rows' : feature_attributes_series(train.columns, duplicate_rows),\n    'drop features' : feature_attributes_series(train.columns, drop_features),\n    'drop rows missing' : feature_attributes_series(train.columns, drop_rows_missing),\n    'impute missing' : feature_attributes_series(train.columns, impute_missing),\n    'high outliers' : feature_attributes_series(train.columns, high_outliers),\n    'correlation ranking' : correlation_ranking,\n}\n\natt_df = pd.DataFrame(att_dict)\ndisplay(att_df.style.applymap(lambda x: 'background-color : #fd7e14; color: white' if x==True else ''))\n\n\ndrop_combined_list = []\nfor i in [single_value_features, drop_features]:\n    drop_combined_list.extend(i)\n    \nprint(drop_combined_list)    ","c6ee9f3d":"train = _train.copy()\ntest = _test.copy()","2eaf60aa":"train.drop(drop_combined_list, axis=1, inplace=True)\n\nfor i in numerical_features:\n    if i in drop_combined_list or i == target_feature:\n        numerical_features.remove(i)\n\nfor i in categorical_features:\n    if i in drop_combined_list or i == target_feature:\n        categorical_features.remove(i)\n        \n# if target_feature in categorical_features: \n#     categorical_features.remove(target_feature)\n# if target_feature in numerical_features: \n#     numerical_features.remove(target_feature)","ac634f57":"from sklearn.model_selection import train_test_split\n\n# split X & y\nX, y = train.drop(target_feature, axis=1), train[target_feature]\n\n# get train & test\nX_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y,test_size=0.25,random_state=23)\n\n# convert target to numerical\ny_train = y_train.map(dict(Yes=1, No=0))\ny_test = y_test.map(dict(Yes=1, No=0))#pd.get_dummies(y_test)\n\n\n# print(y_train)\n# print(y_test)\nprint(len(X_train))\nprint(len(y_train))\n# X_test, y_test = test.drop(target_feature, axis=1), test[target_feature]\n","b28aded5":"# p = make_pipeline(preprocessor, LinearRegression())","ff4eadaa":"# numeric_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='median')),\n#     ('scaler', StandardScaler())])\n\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n#     ('one_hot', OneHotEncoder())])\n\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numeric_transformer, numeric_features),\n#         ('cat', categorical_transformer, categorical_features)\n#     ])\n\n# pipe = Pipeline(steps=[('preprocessor', preprocessor),\n#                       ('classifier',  LogisticRegression(class_weight='balanced', random_state=0))])\n    \n# model = pipe.fit(X_train, y_train)","2491f500":"# create preprocessor\npreprocessor = make_column_transformer(\n    \n    # categorical\n    (\n        make_pipeline\n        (\n        SimpleImputer(strategy='constant', fill_value='missing'),\n        OneHotEncoder(handle_unknown='ignore')\n        )\n        ,categorical_features\n    ),\n    \n    # numerical\n    (\n        make_pipeline\n        (\n            SimpleImputer(strategy='median'),\n            StandardScaler()\n        )\n        ,numerical_features\n    )\n    \n    ,remainder = 'passthrough'\n)\n\npreprocessor.fit(X_train)\nX_train = (make_pipeline(preprocessor).transform(X_train))\nprint(len(X_train))\nprint(len(y_train))\n\n# print(preprocessor.fit_transform(X_train)[0])","69363877":"# from sklearn.linear_model import LinearRegression\n# from sklearn.metrics import r2_score\n\n# # create pipeline\n# p = make_pipeline(preprocessor, LinearRegression())\n\n# # fit\n# # p.fit(X_train, y_train)\n\n# # make_pipeline(preprocessor).fit_transform(X_train, y_train)\n\n# # # predict\n# # y_pred = p.predict(X_test)\n\n# # # score\n# # r2_score(y_test, y_pred)","c59c02fc":"# y_train = (make_pipeline(preprocessor).transform(y_train))","8f56825a":"# print(X_train[0])","99239b50":"import sklearn\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(max_iter=1000),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n#     svm.LinearSVC(max_iter=10000),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n#     discriminant_analysis.LinearDiscriminantAnalysis(),\n#     discriminant_analysis.QuadraticDiscriminantAnalysis(tol=0.0001),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')    \n    ]\n\n#split dataset in cross-validation with this splitter class: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = []\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n    \n    print(alg)\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n#     print(len(X_train))\n#     print(len(y_train))\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, X_train, y_train, return_train_score =True, cv  = cv_split)\n    \n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n    \n\n#     #save MLA predictions - see section 6 for usage\n#     alg.fit(X_train, y_train)\n#     MLA_predict[MLA_name] = alg.predict(X_train)\n    \n    row_index+=1\n\n    \n#print and sort table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n#MLA_predict","6cbb9be1":"#barplot using https:\/\/seaborn.pydata.org\/generated\/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https:\/\/matplotlib.org\/api\/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","b0183b4d":"<div style=\"background-color:#FFFCDF; padding:15px 15px 7px; color:#332906; font-style:italic; border-radius: 5px\">\n\n* Attrition feature is highly imbalanced","94db9c43":"<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> \n    Split Numerical Features (Discrete vs Continuous)\n<\/h3>\n<div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>","8a54cf18":"<div>\n<h1 id=\"heading\" style=\"padding:0px; margin:0px; font-size:0px; max-height:0px; line-height:0px\">\n0. Load Data\n<a id=\"import_libraries\" class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/nealliddle\/avocado-price-prediction-linear-regression\/notebook#heading\">\u00b6<\/a>\n<\/h1>\n<h1 style=\"padding:0px; margin:0px; font-size:0px; max-height:0px; line-height:0px\">0. Import Libraries<\/h1>\n<h2 style=\"margin-top:0px; background-color:#4fc1e9; color:ghostwhite; font-weight:300; min-height:50px; line-height:50px\"><span style=\"background-color:#3bafda; min-width:50px;min-height:50px; display: inline-block;text-align: center\">\n0.\n<\/span>\nFeature Selection\n<\/h2>\n<\/div>","6d8bda80":"# feature extraction\nhigh_correlation_features  \nsingle_value  \ndrop_features  \ndrop_missing  \nimpute_missing  \n\n# categorical\ncategorical_features  \nnominal_features  \nordinal_features  \n\n# numerical\nnumerical_features  \ncontinuous_features  \ndiscrete_features  ","69314339":"<h2 style=\"line-height:40px; border-radius: 5px; margin-right:10px; padding: 0px 10px; background-color:#68CAEC; font-weight:400; color:ghostwhite; min-height:40px; display: inline-block;text-align: center\"> \n    Distinguish Data Types\n<\/h2>","02f0b613":"<link rel=\"stylesheet\" href=\"https:\/\/use.fontawesome.com\/releases\/v5.14.0\/css\/all.css\">\n\n<div style=\"background-color:#4bcfad; color:ghostwhite; text-align:center\">\n    <div style=\"margin: 0px !important; color:ghostwhite; padding:15px; background-color:#37bc98\"> \n        <i style=\"font-size:10em; padding:30px\" class=\"fas fa-user-slash\"><\/i>\n        <h1 style=\"font-weight:700; padding:0px; margin:0px;color:ghostwhite\"> Employee Attrition <\/h1> \n    <\/div>    \n    <div style=\"text-align:center; color:ghostwhite; padding: 50px;\">\n        <i>  \n         The objective is to identify and improve factors to prevent loss of good people.  \n        <br>\n        <a style=\"color:ghostwhite\" href=\"https:\/\/www.kaggle.com\/colearninglounge\/employee-attrition\"><u>TAST LINK<\/u><\/a>\n        <\/i>\n    <\/div>\n<\/div>","41841832":"<h2 style=\"line-height:40px; border-radius: 5px; margin-right:10px; padding: 0px 10px; background-color:#68CAEC; font-weight:400; color:ghostwhite; min-height:40px; display: inline-block;text-align: center\"> \n    Bivariate Categorical\n<\/h2>","89f0cade":"<div style=\"background-color:#E6F6FC; padding:15px 15px 15px;margin:0px\">\n\nTips\n* 1 Unique Value  \nColumns with only 1 unique value add no value and must be dropped\n\n* Unique Amount & Data Type  \nThese columns assist in understanding what type of data we are looking at\n    \n* Amount Missing  \nMissing Values below 10% can be dropped. But if they are important columns, they must be imputed. Also, we have to make sure that overall, there should not be major loss of information due to dropping of missing values","dad76bec":"<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> \n    Split Categorical Features (Ordinal vs Nominal)\n<\/h3>\n<div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>","b1384d40":"<div>\n<h1 id=\"heading\" style=\"padding:0px; margin:0px; font-size:0px; max-height:0px; line-height:0px\">\n0. Load Data\n<a id=\"import_libraries\" class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/nealliddle\/avocado-price-prediction-linear-regression\/notebook#heading\">\u00b6<\/a>\n<\/h1>\n<h1 style=\"padding:0px; margin:0px; font-size:0px; max-height:0px; line-height:0px\">0. Import Libraries<\/h1>\n<h2 style=\"margin-top:0px; background-color:#4fc1e9; color:ghostwhite; font-weight:300; min-height:50px; line-height:50px\"><span style=\"background-color:#3bafda; min-width:50px;min-height:50px; display: inline-block;text-align: center\">\n0.\n<\/span>\nExploratory Data Analysis\n<\/h2>\n<\/div>","60f9e7ee":"<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> \n    Drop unwanted features\n<\/h3>\n<div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>","5ffe5422":"<h2 style=\"line-height:40px; border-radius: 5px; margin-right:10px; padding: 0px 10px; background-color:#68CAEC; font-weight:400; color:ghostwhite; min-height:40px; display: inline-block;text-align: center\"> \n    Bivariate Numerical (Target vs Features)\n<\/h2>","1e1cb176":"<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> \n    Split Features (Categorical vs Numerical)\n<\/h3>\n<div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>","5db65325":"<div style=\"background-color:#E6F6FC; padding:15px 15px 15px;margin:0px\">\nNow that we've seperated the features into categorical and numerical, we can seperate the numerical features further into discrete vs continuous.","91dab1f8":"<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> \n    Understand the Dataframe\n<\/h3>\n<div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>","02fdd45b":"<h2 style=\"line-height:40px; border-radius: 5px; margin-right:10px; padding: 0px 10px; background-color:#68CAEC; font-weight:400; color:ghostwhite; min-height:40px; display: inline-block;text-align: center\"> \n    Univariate Numerical\n<\/h2>","e1f3c868":"# Outliers","68897b9d":"<div style=\"background-color:#E6F6FC; padding:15px 15px 15px;margin:0px\">\nLet's explore each numerical feature in isolation.","5ad5508a":"<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> \n    Understanding the Features\n<\/h3>\n<div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>","905b7e0c":"<h2 style=\"line-height:40px; border-radius: 5px; margin-right:10px; padding: 0px 10px; background-color:#68CAEC; font-weight:400; color:ghostwhite; min-height:40px; display: inline-block;text-align: center\"> \n    Univariate Categorical\n<\/h2>","9675786d":"<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> \n    Table Style\n<\/h3>\n<div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>","10faa657":"<h2 style=\"line-height:40px; border-radius: 5px; margin-right:10px; padding: 0px 10px; background-color:#68CAEC; font-weight:400; color:ghostwhite; min-height:40px; display: inline-block;text-align: center\"> \n    Pipeline\n<\/h2>","f18087eb":"<div style=\"background-color:#FFFCDF; padding:15px 15px 7px; color:#332906; font-style:italic; border-radius: 5px\">\n\n* Based on the fact that 'Rows with NaN' and 'Cells with NaN' have very similar amounts we can assume the NaN's are spread out and not generally shared in rows.","0e2923a2":"<div style=\"background-color:#FFFCDF; padding:15px 15px 7px; color:#332906; font-style:italic; border-radius: 5px\">\n\n* Even though the dtypes are float64 for 3 of the features, they are actually integers\n* All values appear to be rounded so there are no continuous features.","6022cfc3":"<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> \n    Revert to original data\n<\/h3>\n<div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>","84e1a984":"<div style=\"background-color:#FFFCDF; padding:15px 15px 7px; color:#332906; font-style:italic; border-radius: 5px\">\n\nThe following 3 columns have a hight amount of outliers:  \n* PerformanceRating  \n* TrainingTimesLastYear  \n* YearsSinceLastPromotion  ","e5d5074a":"<div>\n<h1 id=\"heading\" style=\"padding:0px; margin:0px; font-size:0px; max-height:0px; line-height:0px\">\n0. Load Data\n<a id=\"import_libraries\" class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/nealliddle\/avocado-price-prediction-linear-regression\/notebook#heading\">\u00b6<\/a>\n<\/h1>\n<h1 style=\"padding:0px; margin:0px; font-size:0px; max-height:0px; line-height:0px\">0. Import Libraries<\/h1>\n<h2 style=\"margin-top:0px; background-color:#4fc1e9; color:ghostwhite; font-weight:300; min-height:50px; line-height:50px\"><span style=\"background-color:#3bafda; min-width:50px;min-height:50px; display: inline-block;text-align: center\">\n0.\n<\/span>\nLoad Data\n<\/h2>\n<\/div>","af055e82":"<div style=\"background-color:#E6F6FC; padding:15px 15px 15px;margin:0px\">\nView the numerical features when filtered by the target values.","35a85741":"<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> \n    Libraries\n<\/h3>\n<div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>","7ab7890f":"<div>\n<h1 id=\"heading\" style=\"padding:0px; margin:0px; font-size:0px; max-height:0px; line-height:0px\">\n0. Load Data\n<a id=\"import_libraries\" class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/nealliddle\/avocado-price-prediction-linear-regression\/notebook#heading\">\u00b6<\/a>\n<\/h1>\n<h1 style=\"padding:0px; margin:0px; font-size:0px; max-height:0px; line-height:0px\">0. Import Libraries<\/h1>\n<h2 style=\"margin-top:0px; background-color:#4fc1e9; color:ghostwhite; font-weight:300; min-height:50px; line-height:50px\"><span style=\"background-color:#3bafda; min-width:50px;min-height:50px; display: inline-block;text-align: center\">\n0.\n<\/span>\nMissing Data\n<\/h2>\n<\/div>","c1eb5502":"<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> \n    Global Variables\n<\/h3>\n<div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>","4f1e6715":"<div>\n<h1 id=\"heading\" style=\"padding:0px; margin:0px; font-size:0px; max-height:0px; line-height:0px\">\n0. Load Data\n<a id=\"import_libraries\" class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/nealliddle\/avocado-price-prediction-linear-regression\/notebook#heading\">\u00b6<\/a>\n<\/h1>\n<h1 style=\"padding:0px; margin:0px; font-size:0px; max-height:0px; line-height:0px\">0. Import Libraries<\/h1>\n<h2 style=\"margin-top:0px; background-color:#4fc1e9; color:ghostwhite; font-weight:300; min-height:50px; line-height:50px\"><span style=\"background-color:#3bafda; min-width:50px;min-height:50px; display: inline-block;text-align: center\">\n0.\n<\/span>\nCorrelation\n<\/h2>\n<\/div>","185dc0c8":"<div style=\"background-color:#E6F6FC; padding:15px 15px 15px;margin:0px\">\nNow that we've seperated the features, we can seperate the categorical features further into nominal vs ordinal. Let's first see what each categorical feature consists of (unique values).","b83274ae":"<h2 style=\"margin-top:0px; background-color:#4fc1e9; color:ghostwhite; font-weight:300; min-height:50px; line-height:50px\"><span style=\"background-color:#3bafda; min-width:50px;min-height:50px; display: inline-block;text-align: center\">\n0.\n<\/span>\nTable of Contents\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/nealliddle\/hr-employee-attrition\/notebook#heading\">\u00b6<\/a>\n<\/h2>\n\n[0. Import & Setup](#import_libraries)  \n  * Libraries  \n  * Print Styles\n  * Table Style\n  * Global Variables\n  \n[1. Load Data](#import_data)  \n\n[2. Exploratory Data Analysis](#rename_features)  \n  * Determine Data Structures\n    * Understand the Dataframe \n    * Understanding the Features \n      * Dataframe Information\n      * Dataframe Head\/Tail\n  * Distinguish Attributes \n    * Split Features (Categorical vs Numerical) \n    * Split Categorical Features (Ordinal vs Nominal) \n    * Split Numerical Features (Discrete vs Continuous) \n\n\n[3. Feature Descriptions](#feature_descriptions)  \n[4. Pre-split Inspect Data](#pre-split_inspect_data)  ","ba4e88fa":"<div style=\"background-color:#E6F6FC; padding:15px 15px 15px;margin:0px\">\nFrom what we've explored so far, we can determine which of the features are categorical vs numerical.","c52e802b":"<div>\n<h1 id=\"heading\" style=\"padding:0px; margin:0px; font-size:0px; max-height:0px; line-height:0px\">\n0. Import Libraries\n<a id=\"import_libraries\" class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/nealliddle\/avocado-price-prediction-linear-regression\/notebook#heading\">\u00b6<\/a>\n<\/h1>\n<h1 style=\"padding:0px; margin:0px; font-size:0px; max-height:0px; line-height:0px\">0. Import Libraries<\/h1>\n<h2 style=\"margin-top:0px; background-color:#4fc1e9; color:ghostwhite; font-weight:300; min-height:50px; line-height:50px\"><span style=\"background-color:#3bafda; min-width:50px;min-height:50px; display: inline-block;text-align: center\">\n0.\n<\/span>\nImport & Setup\n<\/h2>\n<\/div>","9be57744":"<div style=\"background-color:#FFFCDF; padding:15px 15px 7px; color:#332906; font-style:italic; border-radius: 5px\">\n\n**Observation:**\n* Attrition (the target) constist of only 2 values (Yes, No)\n* 2 columns with semi-high amount of missing values (StandardHours, Age)...first need some investigation to see what to do with them\n* 5 columns with some missing values...not a lot considering there are 35 columns\n* Age is between 18 - 60 (working - retiring age?)  \n* 3 columns containing only one variable, so they are of no use and must be dropped. \n    * EmployeeCount\n    * Over18\n    * StandardHours\n* EmployeeNumber serves an ID, which brings no value...drop.","9977a08d":"<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> \n    Functions\n<\/h3>\n<div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>","f26f2c4f":"<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> \n    Define feature variables\n<\/h3>\n<div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>","f8037d9b":"* drop unwanted columns\n* drop unwanted rows\n* impute rows\n* convert cat\n* convert num","85b8df51":"<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> \n    Preprocessor\n<\/h3>\n<div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>","3ac904dc":"<div style=\"background-color:#E6F6FC; padding:15px 15px 15px;margin:0px\">\n\nThe following is required for a basic understanding of the data:\n* High level understanding of the dataframe\n* High level understanding of the columns\/features\n* Split features into categorical and numerical\n* Split categorical features into ordinal and nominal","90c02096":"<h2 style=\"line-height:40px; border-radius: 5px; margin-right:10px; padding: 0px 10px; background-color:#68CAEC; font-weight:400; color:ghostwhite; min-height:40px; display: inline-block;text-align: center\"> \n    Determine Data Structures\n<\/h2>","37c392fd":"<div style=\"background-color:#E6F6FC; padding:15px 15px 15px;margin:0px\">\nBefore working with the data it's important to split the distinguish the data by types. This is required later when filtering by type.","d62a3036":"**Attrition**\n> *A company\u2019s success is rooted in its employees\u2019 talents as well as on its capacity to optimize a group\u2019s strengths to push it to continuously surpass itself. Therefore one of the key tasks is to determine which factors keep employees at a company and which prompt others to leave. The objective is to identify which factors we can change to prevent the loss of good people.*","36f3e032":"<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> \n    Print Styles\n<\/h3>\n<div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>","ca5966e1":"<div>\n<h1 id=\"heading\" style=\"padding:0px; margin:0px; font-size:0px; max-height:0px; line-height:0px\">\n0. Load Data\n<a id=\"import_libraries\" class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/nealliddle\/avocado-price-prediction-linear-regression\/notebook#heading\">\u00b6<\/a>\n<\/h1>\n<h1 style=\"padding:0px; margin:0px; font-size:0px; max-height:0px; line-height:0px\">0. Import Libraries<\/h1>\n<h2 style=\"margin-top:0px; background-color:#4fc1e9; color:ghostwhite; font-weight:300; min-height:50px; line-height:50px\"><span style=\"background-color:#3bafda; min-width:50px;min-height:50px; display: inline-block;text-align: center\">\n0.\n<\/span>\nModels\n<\/h2>\n<\/div>","230deda5":"<h3 style=\"font-weight:400; color:#81D3EF; max-height:5px; display: inline-block;text-align: center\"> \n    Split X & y\n<\/h3>\n<div style=\"background-color:#81D3EF; max-height:1px; margin-bottom: 10px\"> <br><\/div>"}}