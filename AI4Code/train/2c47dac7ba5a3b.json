{"cell_type":{"32948c78":"code","c0158871":"code","66ee1dc1":"code","07ce551a":"code","d39544ac":"code","03c3dce7":"code","9e4d0092":"code","aead983e":"code","d534cb78":"code","182671ec":"code","54802412":"code","9486131e":"code","8727baae":"code","f1e832a1":"code","052e0bb3":"code","9e2df26c":"code","b6aa2812":"code","bfb54158":"code","edf6a08e":"code","0d2614a4":"code","47e4cfc6":"code","44e484ee":"code","efe0e803":"code","3217c7e5":"markdown","61fc92c6":"markdown","0ee32ee9":"markdown","b493ffbf":"markdown","09a72fe5":"markdown","71658d33":"markdown","791e6051":"markdown","e636d7c8":"markdown","3491e5d9":"markdown","c70d3428":"markdown","8623db42":"markdown","bc5aacd4":"markdown","2f359c66":"markdown","07261284":"markdown","2ddc13f0":"markdown","702e7747":"markdown","d9dc9b46":"markdown","0da4a5b5":"markdown","0039f22e":"markdown","d94254db":"markdown","e0cd490c":"markdown","ceece2fc":"markdown","e9d8aac4":"markdown","89a03ca7":"markdown","75927c91":"markdown","8e5678f8":"markdown","b0e1dafd":"markdown","9465af76":"markdown"},"source":{"32948c78":"import numpy as np\nimport scipy\nimport pandas as pd\nimport math\nimport random\nimport sklearn\nfrom nltk.corpus import stopwords\n# from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt","c0158871":"articles_df = pd.read_csv('..\/input\/shared_articles.csv')\narticles_df = articles_df[articles_df['eventType'] == 'CONTENT SHARED']\narticles_df.head(2)","66ee1dc1":"interactions_df = pd.read_csv('..\/input\/users_interactions.csv')\ninteractions_df.head(2)","07ce551a":"event_type_strength = {\n   'VIEW': 1.0,\n   'LIKE': 2.0, \n   'BOOKMARK': 2.5, \n   'FOLLOW': 3.0,\n   'COMMENT CREATED': 4.0,  \n}\n\ninteractions_df['eventStrength'] = interactions_df['eventType'].apply(lambda x: event_type_strength[x])","d39544ac":"users_interactions_count_df = interactions_df.groupby(['personId', 'contentId'])\\\n                                             .size()\\\n                                             .groupby('personId')\\\n                                             .size()\nprint('Total # of users that we get: {}'.format(len(users_interactions_count_df)))\nusers_with_enough_interactions_df = users_interactions_count_df[users_interactions_count_df >= 5]\\\n                                             .reset_index()[['personId']]\nprint('Users with >= 5 interactions: {}'.format(len(users_with_enough_interactions_df)))","03c3dce7":"print('Total # of interactions: {}'.format(len(interactions_df)))\ninteractions_from_selected_users_df = interactions_df\\\n                                        .merge(users_with_enough_interactions_df, \n                                               how = 'right',\n                                               left_on = 'personId',\n                                               right_on = 'personId')\nprint('Interactions from users with >= 5: {}'.format(len(interactions_from_selected_users_df)))","9e4d0092":"def smooth_user_preference(x):\n    return math.log(1+x,2)\n    \ninteractions_full_df = interactions_from_selected_users_df \\\n                        .groupby(['personId', 'contentId', 'timestamp'])['eventStrength']\\\n                        .sum()\\\n                        .apply(smooth_user_preference)\\\n                        .reset_index()\ninteractions_full_df.head(5)","aead983e":"# Interaction before 2016-09-01:\ninteractions_train_df = interactions_full_df[pd.to_datetime(\n                                                interactions_full_df.timestamp, \n                                                unit='s')\\\n                                             < pd.Timestamp(2016,9,1)].copy()\n# Interaction after 2016-09-01:\ninteractions_test_df = interactions_full_df[(pd.to_datetime(\n                                                interactions_full_df.timestamp,\n                                                unit='s')\\\n                                             >= pd.Timestamp(2016,9,1))].copy()\n\n# Here we also filter out all users from test set that are not in the train set: \ninteractions_test_df = interactions_test_df[np.isin(interactions_test_df.personId, interactions_train_df.personId)]\ninteractions_train_df = interactions_train_df[np.isin(interactions_train_df.personId, interactions_test_df.personId)]\ntrain_len = len(interactions_train_df)\ntest_len = len(interactions_test_df)\nfull_len = train_len + test_len\n\nprint('# interactions on Train set: {:.2f}% [{} in total]'.format(train_len\/full_len*100, train_len))\nprint('# interactions on Test  set: {:.2f}% [{} in total]'.format(test_len\/full_len*100, test_len))","d534cb78":"#Indexing by personId to speed up the searches during evaluation\ninteractions_full_indexed_df = interactions_full_df.set_index('personId')\ninteractions_train_indexed_df = interactions_train_df.set_index('personId')\ninteractions_test_indexed_df = interactions_test_df.set_index('personId')","182671ec":"def get_items_interacted(person_id, interactions_df):\n    # Get the user's data and merge in the movie information.\n    interacted_items = interactions_df.loc[person_id]['contentId']\n    return set(interacted_items if type(interacted_items) == pd.Series else [interacted_items])","54802412":"def apk(actual, predicted, k=10):\n    if len(predicted)>k:\n        predicted = predicted[:k]\n    score = 0.0\n    num_hits = 0.0\n    for i,p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits \/ (i+1.0)\n    if not actual.all():\n        return 0.0\n    return score \/ min(len(actual), k)","9486131e":"#Top-N accuracy metrics consts\nEVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS = 100\n\nclass ModelEvaluator:\n\n\n    def get_not_interacted_items_sample(self, person_id, sample_size, seed=42):\n        interacted_items = get_items_interacted(person_id, interactions_full_indexed_df)\n        all_items = set(articles_df['contentId'])\n        non_interacted_items = all_items - interacted_items\n\n        random.seed(seed)\n        non_interacted_items_sample = random.sample(non_interacted_items, sample_size)\n        return set(non_interacted_items_sample)\n\n    def _verify_hit_top_n(self, item_id, recommended_items, topn):        \n            try:\n                index = next(i for i, c in enumerate(recommended_items) if c == item_id)\n            except:\n                index = -1\n            hit = int(index in range(0, topn))\n            return hit, index\n\n    def evaluate_model_for_user(self, model, person_id):\n        #Getting the items in test set\n        interacted_values_testset = interactions_test_indexed_df.loc[person_id]\n        if type(interacted_values_testset['contentId']) == pd.Series:\n            person_interacted_items_testset = set(interacted_values_testset['contentId'])\n        else:\n            person_interacted_items_testset = set([int(interacted_values_testset['contentId'])])  \n        interacted_items_count_testset = len(person_interacted_items_testset) \n\n        #Getting a ranked recommendation list from a model for a given user\n        person_recs_df = model.recommend_items(person_id, \n                                               items_to_ignore=get_items_interacted(person_id, \n                                                                                    interactions_train_indexed_df), \n                                               topn=10000000000)\n\n        hits_at_5_count = 0\n        hits_at_10_count = 0\n        #For each item the user has interacted in test set\n        for item_id in person_interacted_items_testset:\n            #Getting a random sample (100) items the user has not interacted \n            #(to represent items that are assumed to be no relevant to the user)\n            non_interacted_items_sample = self.get_not_interacted_items_sample(person_id, \n                                                                          sample_size=EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS, \n                                                                          seed=item_id%(2**32))\n\n            #Combining the current interacted item with the 100 random items\n            items_to_filter_recs = non_interacted_items_sample.union(set([item_id]))\n\n            #Filtering only recommendations that are either the interacted item or from a random sample of 100 non-interacted items\n            valid_recs_df = person_recs_df[person_recs_df['contentId'].isin(items_to_filter_recs)]                    \n            valid_recs = valid_recs_df['contentId'].values\n            #Verifying if the current interacted item is among the Top-N recommended items\n            hit_at_5, index_at_5 = self._verify_hit_top_n(item_id, valid_recs, 5)\n            hits_at_5_count += hit_at_5\n            hit_at_10, index_at_10 = self._verify_hit_top_n(item_id, valid_recs, 10)\n            hits_at_10_count += hit_at_10\n\n        #Recall is the rate of the interacted items that are ranked among the Top-N recommended items, \n        #when mixed with a set of non-relevant items\n        recall_at_5 = hits_at_5_count \/ float(interacted_items_count_testset)\n        recall_at_10 = hits_at_10_count \/ float(interacted_items_count_testset)\n        \n        #Additional: map@5 and map@10\n        ap_5 = apk(np.append([], interacted_values_testset.contentId.tolist()), person_recs_df.contentId.tolist()[:5], 5)\n        ap_10 = apk(np.append([], interacted_values_testset.contentId.tolist()), person_recs_df.contentId.tolist()[:10], 10)\n        \n        person_metrics = {'hits@5_count':hits_at_5_count, \n                          'hits@10_count':hits_at_10_count, \n                          'interacted_count': interacted_items_count_testset,\n                          'recall@5': recall_at_5,\n                          'recall@10': recall_at_10,\n                          'ap@5': ap_5,\n                          'ap@10': ap_10}\n        return person_metrics\n\n    def evaluate_model(self, model):\n        #print('Running evaluation for users')\n        people_metrics = []\n        for idx, person_id in enumerate(list(interactions_test_indexed_df.index.unique().values)):\n            if idx % 100 == 0 and idx > 0:\n                print('%d users processed' % idx)\n            person_metrics = self.evaluate_model_for_user(model, person_id)  \n            person_metrics['_person_id'] = person_id\n            people_metrics.append(person_metrics)\n        print('%d users processed' % idx)\n\n        detailed_results_df = pd.DataFrame(people_metrics) \\\n                            .sort_values('interacted_count', ascending=False)\n        \n        global_recall_at_5 = detailed_results_df['hits@5_count'].sum() \/ float(detailed_results_df['interacted_count'].sum())\n        global_recall_at_10 = detailed_results_df['hits@10_count'].sum() \/ float(detailed_results_df['interacted_count'].sum())\n        \n        #map@k\n        map_at_5 = detailed_results_df['ap@5'].values.mean()\n        map_at_10 = detailed_results_df['ap@10'].values.mean()\n        \n        global_metrics = {'modelName': model.get_model_name(),\n                          'recall@5': global_recall_at_5,\n                          'recall@10': global_recall_at_10,\n                          'map@5': map_at_5,\n                          'map@10': map_at_10}    \n        return global_metrics, detailed_results_df\n    \nmodel_evaluator = ModelEvaluator()    ","8727baae":"#Ignoring stopwords (words with no semantics) from English and Portuguese (as we have a corpus with mixed languages)\nstopwords_list = stopwords.words('english') + stopwords.words('portuguese')\n\n#Trains a model whose vectors size is 5000, composed by the main unigrams and bigrams found in the corpus, ignoring stopwords\nvectorizer = TfidfVectorizer(analyzer='word',\n                     ngram_range=(1, 2),\n                     min_df=0.003,\n                     max_df=0.5,\n                     max_features=5000,\n                     stop_words=stopwords_list)\n\nitem_ids = articles_df['contentId'].tolist()\ntfidf_matrix = vectorizer.fit_transform(articles_df['title'] + \"\" + articles_df['text'])\ntfidf_feature_names = vectorizer.get_feature_names()\ntfidf_matrix","f1e832a1":"def get_item_profile(item_id):\n    idx = item_ids.index(item_id)\n    item_profile = tfidf_matrix[idx:idx+1]\n    return item_profile\n\ndef get_item_profiles(ids):\n    item_profiles_list = [get_item_profile(x) for x in ids]\n    item_profiles = scipy.sparse.vstack(item_profiles_list)\n    return item_profiles\n\ndef build_users_profile(person_id, interactions_indexed_df):\n    interactions_person_df = interactions_indexed_df.loc[person_id]\n    user_item_profiles = get_item_profiles(interactions_person_df['contentId'])\n    \n    user_item_strengths = np.array(interactions_person_df['eventStrength']).reshape(-1,1)\n    #Weighted average of item profiles by the interactions strength\n    user_item_strengths_weighted_avg = np.sum(user_item_profiles.multiply(user_item_strengths), axis=0) \/ np.sum(user_item_strengths)\n    user_profile_norm = sklearn.preprocessing.normalize(user_item_strengths_weighted_avg)\n    return user_profile_norm\n\ndef build_users_profiles(): \n    interactions_indexed_df = interactions_full_df[interactions_full_df['contentId'] \\\n                                                   .isin(articles_df['contentId'])].set_index('personId')\n    user_profiles = {}\n    for person_id in interactions_indexed_df.index.unique():\n        user_profiles[person_id] = build_users_profile(person_id, interactions_indexed_df)\n    return user_profiles","052e0bb3":"user_profiles = build_users_profiles()\nlen(user_profiles)","9e2df26c":"class ContentBasedRecommender:\n    \n    MODEL_NAME = 'Content-Based'\n    \n    def __init__(self, items_df=None):\n        self.item_ids = item_ids\n        self.items_df = items_df\n        \n    def get_model_name(self):\n        return self.MODEL_NAME\n        \n    def _get_similar_items_to_user_profile(self, person_id, topn=1000):\n        #Computes the cosine similarity between the user profile and all item profiles\n        cosine_similarities = cosine_similarity(user_profiles[person_id], tfidf_matrix)\n        #Gets the top similar items\n        similar_indices = cosine_similarities.argsort().flatten()[-topn:]\n        #Sort the similar items by similarity\n        similar_items = sorted([(item_ids[i], cosine_similarities[0,i]) for i in similar_indices], key=lambda x: -x[1])\n        return similar_items\n        \n    def recommend_items(self, user_id, items_to_ignore=[], topn=10, verbose=False):\n        similar_items = self._get_similar_items_to_user_profile(user_id)\n        #Ignores items the user has already interacted\n        similar_items_filtered = list(filter(lambda x: x[0] not in items_to_ignore, similar_items))\n        \n        recommendations_df = pd.DataFrame(similar_items_filtered, columns=['contentId', 'recStrength']) \\\n                                    .head(topn)\n\n        if verbose:\n            if self.items_df is None:\n                raise Exception('\"items_df\" is required in verbose mode')\n\n            recommendations_df = recommendations_df.merge(self.items_df, how = 'left', \n                                                          left_on = 'contentId', \n                                                          right_on = 'contentId')[['recStrength', 'contentId', 'title', 'url', 'lang']]\n\n\n        return recommendations_df\n    \ncontent_based_recommender_model = ContentBasedRecommender(articles_df)","b6aa2812":"print('Evaluating Content-Based Filtering model...')\ncb_global_metrics_1, cb_detailed_results_df_1 = model_evaluator.evaluate_model(content_based_recommender_model)\nprint('\\nGlobal metrics:\\n%s' % cb_global_metrics_1)\ncb_detailed_results_df_1.sample(10)","bfb54158":"from sklearn.preprocessing import MinMaxScaler\nminmaxScaler = MinMaxScaler()\n\n# Get length of the title and text\narticles_df['title_len'] = articles_df.title.map(len)\narticles_df['text_len'] = articles_df.text.map(len)\n# We use the min-amx scaler here because TF-IDF data is sparse and every value there is less than 0.5\narticles_df_dummies_titletext_len = minmaxScaler.fit_transform(articles_df[['title_len', 'text_len']])\ntext_art_names = ['title_len', 'text_len']\n\n# Get infornation about authors' regions\narticles_df_dummies_region = pd.get_dummies(\n                                articles_df.authorRegion.map(\n                                        lambda x: np.nan\n                                        if x not in frozenset(['SP', 'MG', 'NY', 'NJ'])\n                                        else x))\nregion_names = articles_df_dummies_region.columns.tolist()\n\n# Get infornation about authors' country\narticles_df_dummies_country = pd.get_dummies(articles_df.authorCountry)\ncountry_names = articles_df_dummies_country.columns.tolist()\n\n# Get the list of authors (we consider only authors that have at least 20 articles)\nauthors_num_art = articles_df.authorPersonId.value_counts()\narticles_df_dummies_author = pd.get_dummies(\n                                articles_df.authorPersonId.map(\n                                        lambda x: np.nan \n                                        if authors_num_art[x] < 20 \n                                        else 'author_' + str(x)))\nauthors_names = ['A'+str(i) for i in range(articles_df_dummies_author.shape[1])]\n\n# Get infornation about the day of week\nday_of_week = pd.get_dummies(pd.to_datetime(articles_df.timestamp, unit='s')\\\n                             .dt.weekday.map(\n                                {0:'Mo',1:'Tu',2:'Wd',3:'Th',4:'Fr',5:'Su',6:'Sa'}))\nday_of_week_names = day_of_week.columns.tolist()\n\n# Concatinate all the new information into one table\narticles_df_dummies_all = np.hstack([articles_df_dummies_titletext_len, \n                                     articles_df_dummies_region, \n                                     articles_df_dummies_country, \n                                     articles_df_dummies_author,\n                                     day_of_week])\nnames_all = text_art_names + region_names + country_names \\\n            + authors_names + day_of_week_names\nprint(names_all, len(names_all))","edf6a08e":"# Augmintation\nfrom scipy.sparse import csr_matrix\ntfidf_matrix = csr_matrix(scipy.sparse.hstack([0.3 * articles_df_dummies_all, tfidf_matrix]))\ntfidf_matrix","0d2614a4":"user_profiles = build_users_profiles()\nlen(user_profiles)","47e4cfc6":"print('Evaluating Content-Based Filtering model...')\ncb_global_metrics_2, cb_detailed_results_df_2 = model_evaluator.evaluate_model(content_based_recommender_model)\nprint('\\nGlobal metrics:\\n%s' % cb_global_metrics_2)\ncb_detailed_results_df_2.sample(10)","44e484ee":"fig, axarr = plt.subplots(nrows=1, ncols=3, sharey=True)\nfig.suptitle('Recall@5')\nfig.set_figwidth(15)\n# 1\naxarr[0].hist(cb_detailed_results_df_1['recall@5'], rwidth=0.9);\naxarr[0].set_title('Raw TF-IDF');\n# 2\naxarr[1].hist(cb_detailed_results_df_2['recall@5'], rwidth=0.9, color='green');\naxarr[1].set_title('Augmented TF-IDF');\n# 3\naxarr[2].hist(cb_detailed_results_df_1['recall@5'], rwidth=0.9, color='blue', alpha=0.5);\naxarr[2].hist(cb_detailed_results_df_2['recall@5'], rwidth=0.9, color='green', alpha=0.5);\naxarr[2].set_title('Raw vs. Augmented');","efe0e803":"fig, axarr = plt.subplots(nrows=1, ncols=3, sharey=True)\nfig.suptitle('AP@5')\nfig.set_figwidth(15)\n# 1\naxarr[0].hist(cb_detailed_results_df_1['ap@5'], rwidth=0.9);\naxarr[0].set_title('Raw TF-IDF');\n# 2\naxarr[1].hist(cb_detailed_results_df_2['ap@5'], rwidth=0.9, color='green');\naxarr[1].set_title('Augmented TF-IDF');\n# 3\naxarr[2].hist(cb_detailed_results_df_1['ap@5'], rwidth=0.9, color='blue', alpha=0.5);\naxarr[2].hist(cb_detailed_results_df_2['ap@5'], rwidth=0.9, color='green', alpha=0.5);\naxarr[2].set_title('Raw vs. Augmented');","3217c7e5":"# Conclusion","61fc92c6":"Contains logs of user interactions on shared articles. It can be joined to **articles_shared.csv** by **contentId** column.\n\nThe eventType values are:  \n- **VIEW**: The user has opened the article. \n- **LIKE**: The user has liked the article. \n- **COMMENT CREATED**: The user created a comment in the article. \n- **FOLLOW**: The user chose to be notified on any new comment in the article. \n- **BOOKMARK**: The user has bookmarked the article for easy return in the future.","0ee32ee9":"## TF-IDF Augmentation [Model 2]\n\nNow, we will try to squeeze some more information out of items (articles), not only raw TF-IDF, but also information about the lengths of titles and texts, languages,  authors and when the articles was shared.","b493ffbf":"In the next cell we have an implementation of `MAP@k` metric. This metric was not user in [Recommender Systems in Python 101](https:\/\/www.kaggle.com\/gspmoreira\/recommender-systems-in-python-101).","09a72fe5":"In this notebook, we use a dataset we've shared on Kaggle Datasets: [Articles Sharing and Reading from CI&T Deskdrop](https:\/\/www.kaggle.com\/gspmoreira\/articles-sharing-reading-from-cit-deskdrop).  ","71658d33":"# Content-Based Filtering model","791e6051":"We have a significant jump in **Recall@5** to **0.4895**, which means that about **48%** (earlier it was 41%) of interacted items in test set were ranked by this model among the top-5 items (from lists with 100 random items).  And **Recall@10** is now **0.6041 (60%)** (it was 51%).  MAP@k scope is also changed from **0.31** and **0.26** at 5 and 10 respectively to **0.36** and **0.30** for augmented TF-IDF.\n\nIt seems to show better scores than a hybrid system from the original kernel.\n\n## Comparing the two systems\n\nLet's also take a look at some pictures to compare two systems augmentd vs. non-augmented.","e636d7c8":"Content-based filtering approaches leverage description or attributes from items the user has interacted to recommend similar items. It depends only on the user previous choices, making this method robust to avoid the *cold-start* problem.\nFor textual items, like articles, news and books, it is simple to use the raw text to build item profiles and user profiles.  \nHere we are using a very popular technique in information retrieval (search engines) named [TF-IDF](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf). This technique converts unstructured text into a vector structure, where each word is represented by a position in the vector, and the value measures how relevant a given word is for an article. As all items will be represented in the same [Vector Space Model](https:\/\/en.wikipedia.org\/wiki\/Vector_space_model), it is to compute similarity between articles.  \nSee this [presentation](https:\/\/www.slideshare.net\/gabrielspmoreira\/discovering-users-topics-of-interest-in-recommender-systems-tdc-sp-2016) (from slide 30) for more information on TF-IDF and Cosine similarity.","3491e5d9":"## users_interactions.csv","c70d3428":"# Evaluation","8623db42":"# Content-Based Filtering","bc5aacd4":"## shared_articles.csv","2f359c66":"## Data munging","07261284":"Recommender systems have a problem known as ***user cold-start***, in which is hard do provide personalized recommendations for users with none or a very few number of consumed items, due to the lack of information to model their preferences.  \nFor this reason, we are keeping in the dataset only users with at leas 5 interactions.","2ddc13f0":"Now we add the new info to the TF-IDF data. We use the weight of 0.3 to suppress the new info a bit since TF-IDF is quite sparse and the correspondind values are small (much less than 1). Why exactly 0.3? Let's consider that as a hyperparameter of our model :)","702e7747":"In this notebook, we've explored Content-Based Filtering technique.\n\nPlease check out the kernel [Recommender Systems in Python 101](https:\/\/www.kaggle.com\/gspmoreira\/recommender-systems-in-python-101) by [Gabriel Moreira](https:\/\/www.kaggle.com\/gspmoreira) for more information about other techniques. \n","d9dc9b46":"# Loading data: CI&T Deskdrop dataset","0da4a5b5":"Here below, we also add `MAP@5` and `MAP@10` metrics to the model evaluator.","0039f22e":"## Metrics\n\nThese could be tricky.\n\n**The fist metric.** Here as in [Recommender Systems in Python 101](https:\/\/www.kaggle.com\/gspmoreira\/recommender-systems-in-python-101) we work with **Top-N accuracy metrics**, which evaluates the accuracy of the top recommendations provided to a user, comparing to the items the user has actually interacted in test set.  This evaluation method works as follows:\n\n* For each user\n    * For each item the user has interacted in test set\n        * Sample 100 other items the user has never interacted.   \n        Ps. Here we naively assume those non interacted items are not relevant to the user, which might not be true, as the user may simply not be aware of those not interacted items. But let's keep this assumption.\n        * Ask the recommender model to produce a ranked list of recommended items, from a set composed one interacted item and the 100 non-interacted (\"non-relevant!) items\n        * Compute the Top-N accuracy metrics for this user and interacted item from the recommendations ranked list\n* Aggregate the global Top-N accuracy metrics\n\nThe Top-N accuracy metric choosen was **Recall@N** which evaluates whether the interacted item is among the top N items (hit) in the ranked list of 101 recommendations for a user.  \n\n**The second metric.** Additionally, we also compute **MAP@k** (Mean Average Precision), whose score calculation takes into account the position of the relevant item in the ranked list (max. value if relevant item is in the first position). You can find a reference to implement this metrics in this [post](http:\/\/fastml.com\/what-you-wanted-to-know-about-mean-average-precision\/).","d94254db":"In Deskdrop, users are allowed to view an article many times, and interact with them in different ways (eg. like or comment). Thus, to model the user interest on a given article, we aggregate all the interactions the user has performed in an item by a weighted sum of interaction type strength and apply a log transformation to smooth the distribution.","e0cd490c":"So, the results are consistent with [Recommender Systems in Python 101](https:\/\/www.kaggle.com\/gspmoreira\/recommender-systems-in-python-101).  We got \u0435\u0440\u0443 **Recall@5** approximately **0.4129**, which means that about **41%** of interacted items in test set were ranked by this model among the top-5 items (from lists with 100 random items).  And **Recall@10** is about **0.5167 (52%)**. We got a slightly different results because we conducted a different train-test split (as we mention above, we using here a split by a reference date, where the train set is composed by all interactions before the certain date, and the test set are interactions after that date).","ceece2fc":"Let's consider a situation we have information about some **items** or **entities** (in our case that will be **articles**) and **users** that interact with the items (in our case users **veiw**, **like**, **share**, etc.). If we know the list of items (articles) a user had interaction with, how can we build a system that will recommend some new items (articles) based on their preference. In other words, it seems we would like to build a so-called **content-based** recommender system.\n\nThis kernel is base on the kernel [Recommender Systems in Python 101](https:\/\/www.kaggle.com\/gspmoreira\/recommender-systems-in-python-101) by [Gabriel Moreira](https:\/\/www.kaggle.com\/gspmoreira), where you can find a lot of information about different types of recommender systems, for instance, *popularity model* (non-personalized recommendation), *content-based filtering* (personalized recommendation), *collaborative filtering* (personalized recommendation) and some *hybrid approaches*. Here below, we will consider **only content-based approach**.\n\nIn short, we will try to augment users' profiles with some additional information to get a slightly bettter result with only one recommender.","e9d8aac4":"[**Content-Based Filtering**](http:\/\/recommender-systems.org\/content-based-filtering\/) uses **only** information about the description and attributes of the **items** users has **previously interacted** with to model user's preferences. In other words, these algorithms try to recommend items that are similar to those that a user liked in the past (or is examining in the present). In particular, various candidate items are compared with items previously rated by the user and the best-matching items are recommended.  ","89a03ca7":"As there are different interactions types, we associate them with a weight or strength, assuming that, for example, a comment in an article indicates a higher interest of the user on the item than a like, or than a simple view.","75927c91":"## Raw TF-IDF [Model 1]\n\nAn approach used in  [Recommender Systems in Python 101](https:\/\/www.kaggle.com\/gspmoreira\/recommender-systems-in-python-101).\n\nTo model the user profile, we take all the item profiles the user has interacted and average them. The average is weighted by the interaction strength, in other words, the articles the user has interacted the most (eg. liked or commented) will have a higher strength in the final user profile.   ","8e5678f8":"Evaluation is important for machine learning projects, because it allows to compare objectivelly different algorithms and hyperparameter choices for models.  \nOne key aspect of evaluation is to ensure that the trained model generalizes for data it was not trained on, using **Cross-validation** techniques. We are using here a simple cross-validation approach named **holdout**, in which a random data sample (~ 30% in this case) are kept aside in the training process, and exclusively used for evaluation. All evaluation metrics reported here are computed using the **test set**.\n\nAs mentioned here [Recommender Systems in Python 101](https:\/\/www.kaggle.com\/gspmoreira\/recommender-systems-in-python-101), we will implement an evaluation approach, when we **split train and test** sets by a **reference date**, where the train set is composed by all interactions before that date, and the test set are interactions after that date. That way we do not \"look in to the future\" while training the model.","b0e1dafd":"In this section, we load the [Deskdrop dataset](https:\/\/www.kaggle.com\/gspmoreira\/articles-sharing-reading-from-cit-deskdrop), which contains a real sample of 12 months logs (Mar. 2016 - Feb. 2017) from CI&T's Internal Communication platform (DeskDrop). It contains about 73k logged users interactions on more than 3k public articles shared in the platform.\nIt is composed of two CSV files:  \n- **shared_articles.csv**\n- **users_interactions.csv**\n\nTake a look in this kernels for a better picture of the dataset: \n- Deskdrop datasets EDA \n- DeskDrop Articles Topic Modeling","9465af76":"Contains information about the articles shared in the platform. Each article has its sharing date (timestamp), the original url, title, content in plain text, the article' lang (Portuguese: pt or English: en) and information about the user who shared the article (author).\n\nThere are two possible event types at a given timestamp: \n- CONTENT SHARED: The article was shared in the platform and is available for users. \n- CONTENT REMOVED: The article was removed from the platform and not available for further recommendation.\n\nFor the sake of simplicity, we only consider here the \"CONTENT SHARED\" event type, assuming (naively) that all articles were available during the whole one year period. For a more precise evaluation (and higher accuracy), only articles that were available at a given time should be recommended, but we let this exercice for you."}}