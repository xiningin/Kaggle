{"cell_type":{"b11887d6":"code","85226c40":"code","85d7a415":"code","a0967915":"code","5bf3ca86":"code","a383583f":"code","e617af7b":"code","583afaa3":"code","3a83b268":"code","6b112309":"code","9fcba47b":"code","5f61831d":"code","799414fd":"code","1fb6277a":"code","28bf51de":"code","ac492907":"code","9a692765":"code","ec6e3c18":"code","b9a900bb":"code","92b25f3f":"code","e2d86aa1":"code","27b05f2a":"code","4f779eae":"code","7137eec4":"code","93e33f8a":"code","c1d4ad73":"code","1d91f2eb":"code","517627eb":"code","031ce5a8":"code","72337aaa":"code","c3cd5992":"code","6d60073f":"code","d2d8f558":"code","cf85ada7":"code","98624848":"code","c42aabaa":"code","3a8c2eaa":"code","e103199b":"code","857c4dec":"code","abe99812":"code","eb236f71":"code","71d4f2ed":"code","6885370d":"code","3113fce8":"code","266073f7":"code","887b8d5e":"code","7bab8c0e":"code","c2baff96":"code","47769526":"code","bcecabec":"code","48e6b137":"code","faeba910":"code","58d28e1d":"code","56cf37c1":"code","c1d28cae":"code","da911bb2":"code","4207d68e":"code","584189d7":"code","c8b33004":"code","5f038115":"code","d7cdd794":"code","e484c582":"markdown","16341dde":"markdown","d0213ab6":"markdown","bf5cb2e7":"markdown","f45a98c5":"markdown","766fbb5a":"markdown","1c45d5ab":"markdown","cfb34c3e":"markdown","3af9fed2":"markdown","9423566f":"markdown","4bb358c3":"markdown","c1db497e":"markdown","0716aecb":"markdown","28e2525d":"markdown","47916f6c":"markdown","d535d4f5":"markdown","f4454e06":"markdown","d927a06a":"markdown"},"source":{"b11887d6":"# Import libraries \nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport glob \nfrom sklearn.metrics import confusion_matrix\nimport IPython.display as ipd  # To play sound in the notebook\nimport os\nimport sys\nimport warnings\n# ignore warnings \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","85226c40":"#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\nTESS = \"\/kaggle\/input\/toronto-emotional-speech-set-tess\/tess toronto emotional speech set data\/TESS Toronto emotional speech set data\/\"\nRAV = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/\"\nSAVEE = \"\/kaggle\/input\/surrey-audiovisual-expressed-emotion-savee\/ALL\/\"\nCREMA = \"\/kaggle\/input\/cremad\/AudioWAV\/\"\n\n# Run one example \ndir_list = os.listdir(SAVEE)\ndir_list[0:5]","85d7a415":"# Get the data location for SAVEE\ndir_list = os.listdir(SAVEE)\n\n# parse the filename to get the emotions\nemotion=[]\npath = []\nfor i in dir_list:\n    if i[-8:-6]=='_a':\n        emotion.append('male_angry')\n    elif i[-8:-6]=='_d':\n        emotion.append('male_disgust')\n    elif i[-8:-6]=='_f':\n        emotion.append('male_fear')\n    elif i[-8:-6]=='_h':\n        emotion.append('male_happy')\n    elif i[-8:-6]=='_n':\n        emotion.append('male_neutral')\n    elif i[-8:-6]=='sa':\n        emotion.append('male_sad')\n    elif i[-8:-6]=='su':\n        emotion.append('male_surprise')\n    else:\n        emotion.append('male_error') \n    path.append(SAVEE + i)\n\n# Now check out the label count distribution \nSAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\nSAVEE_df['source'] = 'SAVEE'\nSAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\nSAVEE_df.labels.value_counts()","a0967915":"# use the well known Librosa library for this task \nfname = SAVEE + 'DC_f11.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","5bf3ca86":"# Lets play a happy track\nfname = SAVEE + 'DC_h11.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","a383583f":"dir_list = os.listdir(RAV)\ndir_list.sort()\n\nemotion = []\ngender = []\npath = []\nfor i in dir_list:\n    fname = os.listdir(RAV + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')\n        emotion.append(int(part[2]))\n        temp = int(part[6])\n        if temp%2 == 0:\n            temp = \"female\"\n        else:\n            temp = \"male\"\n        gender.append(temp)\n        path.append(RAV + i + '\/' + f)\n\n        \nRAV_df = pd.DataFrame(emotion)\nRAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\nRAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\nRAV_df.columns = ['gender','emotion']\nRAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\nRAV_df['source'] = 'RAVDESS'  \nRAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\nRAV_df.labels.value_counts()","e617af7b":"# Pick a fearful track\nfname = RAV + 'Actor_14\/03-01-06-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","583afaa3":"# Pick a happy track\nfname = RAV + 'Actor_14\/03-01-03-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","3a83b268":"dir_list = os.listdir(TESS)\ndir_list.sort()\ndir_list","6b112309":"path = []\nemotion = []\n\nfor i in dir_list:\n    fname = os.listdir(TESS + i)\n    for f in fname:\n        if i == 'OAF_angry' or i == 'YAF_angry':\n            emotion.append('female_angry')\n        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n            emotion.append('female_disgust')\n        elif i == 'OAF_Fear' or i == 'YAF_fear':\n            emotion.append('female_fear')\n        elif i == 'OAF_happy' or i == 'YAF_happy':\n            emotion.append('female_happy')\n        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n            emotion.append('female_neutral')                                \n        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n            emotion.append('female_surprise')               \n        elif i == 'OAF_Sad' or i == 'YAF_sad':\n            emotion.append('female_sad')\n        else:\n            emotion.append('Unknown')\n        path.append(TESS + i + \"\/\" + f)\n\nTESS_df = pd.DataFrame(emotion, columns = ['labels'])\nTESS_df['source'] = 'TESS'\nTESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nTESS_df.labels.value_counts()","9fcba47b":"# lets play a fearful track \nfname = TESS + 'YAF_fear\/YAF_dog_fear.wav' \n\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","5f61831d":"# lets play a happy track \nfname =  TESS + 'YAF_happy\/YAF_dog_happy.wav' \n\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","799414fd":"dir_list = os.listdir(CREMA)\ndir_list.sort()\nprint(dir_list[0:10])","1fb6277a":"gender = []\nemotion = []\npath = []\nfemale = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n\nfor i in dir_list: \n    part = i.split('_')\n    if int(part[0]) in female:\n        temp = 'female'\n    else:\n        temp = 'male'\n    gender.append(temp)\n    if part[2] == 'SAD' and temp == 'male':\n        emotion.append('male_sad')\n    elif part[2] == 'ANG' and temp == 'male':\n        emotion.append('male_angry')\n    elif part[2] == 'DIS' and temp == 'male':\n        emotion.append('male_disgust')\n    elif part[2] == 'FEA' and temp == 'male':\n        emotion.append('male_fear')\n    elif part[2] == 'HAP' and temp == 'male':\n        emotion.append('male_happy')\n    elif part[2] == 'NEU' and temp == 'male':\n        emotion.append('male_neutral')\n    elif part[2] == 'SAD' and temp == 'female':\n        emotion.append('female_sad')\n    elif part[2] == 'ANG' and temp == 'female':\n        emotion.append('female_angry')\n    elif part[2] == 'DIS' and temp == 'female':\n        emotion.append('female_disgust')\n    elif part[2] == 'FEA' and temp == 'female':\n        emotion.append('female_fear')\n    elif part[2] == 'HAP' and temp == 'female':\n        emotion.append('female_happy')\n    elif part[2] == 'NEU' and temp == 'female':\n        emotion.append('female_neutral')\n    else:\n        emotion.append('Unknown')\n    path.append(CREMA + i)\n    \nCREMA_df = pd.DataFrame(emotion, columns = ['labels'])\nCREMA_df['source'] = 'CREMA'\nCREMA_df = pd.concat([CREMA_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nCREMA_df.labels.value_counts()","28bf51de":"# use the well known Librosa library for this task \nfname = CREMA + '1012_IEO_HAP_HI.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","ac492907":"# A fearful track\nfname = CREMA + '1012_IEO_FEA_HI.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","9a692765":"df = pd.concat([SAVEE_df, RAV_df, TESS_df, CREMA_df], axis = 0)\nprint(df.labels.value_counts())\ndf.head()\ndf.to_csv(\"Data_path.csv\",index=False)","ec6e3c18":"# Import our libraries\nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport os\nimport IPython.display as ipd  # To play sound in the notebook","b9a900bb":"# Source - RAVDESS; Gender - Female; Emotion - Angry \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_08\/03-01-05-02-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","92b25f3f":"# Source - RAVDESS; Gender - Male; Emotion - Angry \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_09\/03-01-05-01-01-01-09.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","e2d86aa1":"# Source - RAVDESS; Gender - Female; Emotion - Happy \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_12\/03-01-03-01-02-01-12.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","27b05f2a":"# Source - RAVDESS; Gender - Male; Emotion - Happy \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_11\/03-01-03-01-02-02-11.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)\n","4f779eae":"# Source - RAVDESS; Gender - Female; Emotion - Angry \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_08\/03-01-05-02-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nfemale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# Source - RAVDESS; Gender - Male; Emotion - Angry \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_09\/03-01-05-01-01-01-09.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(female, label='female')\nplt.plot(male, label='male')\nplt.legend()","7137eec4":"# Source - RAVDESS; Gender - Female; Emotion - happy \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_12\/03-01-03-01-02-01-12.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nfemale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# Source - RAVDESS; Gender - Male; Emotion - happy \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_11\/03-01-03-01-02-02-11.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n# Plot the two audio waves together\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(female, label='female')\nplt.plot(male, label='male')\nplt.legend()","93e33f8a":"# Importing required libraries \n# Keras\nimport keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\n# sklearn\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Other  \nimport librosa\nimport librosa.display\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport seaborn as sns\nimport glob \nimport os\nimport pickle\nimport IPython.display as ipd  # To play sound in the notebook","c1d4ad73":"# lets pick up the meta-data that we got from our first part of the Kernel\nref = pd.read_csv(\"\/kaggle\/input\/datapath\/Data_path.csv\")\nref.head()","1d91f2eb":"# Note this takes a couple of minutes (~10 mins) as we're iterating over 4 datasets \ndf = pd.DataFrame(columns=['feature'])\n\n# loop feature extraction over the entire dataset\ncounter=0\nfor index,path in enumerate(ref.path):\n    X, sample_rate = librosa.load(path\n                                  , res_type='kaiser_fast'\n                                  ,duration=2.5\n                                  ,sr=44100\n                                  ,offset=0.5\n                                 )\n    sample_rate = np.array(sample_rate)\n    \n    # mean as the feature. Could do min and max etc as well. \n    mfccs = np.mean(librosa.feature.mfcc(y=X, \n                                        sr=sample_rate, \n                                        n_mfcc=13),\n                    axis=0)\n    df.loc[counter] = [mfccs]\n    counter=counter+1   \n\n# Check a few records to make sure its processed successfully\nprint(len(df))\ndf.head()","517627eb":"# Now extract the mean bands to its own feature columns\ndf = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\ndf[:5]","031ce5a8":"# replace NA with 0\ndf=df.fillna(0)\nprint(df.shape)\ndf[:5]","72337aaa":"# Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n                                                    , df.labels\n                                                    , test_size=0.25\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n# Lets see how the data present itself before normalisation \nX_train[150:160]","c3cd5992":"# Lts do data normalization \nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)\/std\nX_test = (X_test - mean)\/std\n\n# Check the dataset now \nX_train[150:160]\n","6d60073f":"# Lets few preparation steps to get it into the correct format for Keras \nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\nprint(X_train.shape)\nprint(lb.classes_)\n#print(y_train[0:10])\n#print(y_test[0:10])\n\n# Pickel the lb object for future use \nfilename = 'labels'\noutfile = open(filename,'wb')\npickle.dump(lb,outfile)\noutfile.close()","d2d8f558":"X_train = np.expand_dims(X_train, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape","cf85ada7":"# New model\nmodel = Sequential()\nmodel.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(256, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(14)) # Target class number\nmodel.add(Activation('softmax'))\n# opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n# opt = keras.optimizers.Adam(lr=0.0001)\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nmodel.summary()","98624848":"model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\nmodel_history=model.fit(X_train, y_train, batch_size=16, epochs=100, validation_data=(X_test, y_test))","c42aabaa":"plt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","3a8c2eaa":"# Save model and weights\nmodel_name = 'Emotion_Model.h5'\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\n\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Save model and weights at %s ' % model_path)\n\n# Save the model to disk\nmodel_json = model.to_json()\nwith open(\"model_json.json\", \"w\") as json_file:\n    json_file.write(model_json)","e103199b":"# loading json and model architecture \njson_file = open('model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"saved_models\/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n \n# Keras optimiser\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","857c4dec":"preds = loaded_model.predict(X_test, \n                         batch_size=16, \n                         verbose=1)\n\npreds=preds.argmax(axis=1)\npreds","abe99812":"# predictions \npreds = preds.astype(int).flatten()\npreds = (lb.inverse_transform((preds)))\npreds = pd.DataFrame({'predictedvalues': preds})\n\n# Actual labels\nactual=y_test.argmax(axis=1)\nactual = actual.astype(int).flatten()\nactual = (lb.inverse_transform((actual)))\nactual = pd.DataFrame({'actualvalues': actual})\n\n# Lets combined both of them into a single dataframe\nfinaldf = actual.join(preds)\nfinaldf[170:180]","eb236f71":"# Write out the predictions to disk\nfinaldf.to_csv('Predictions.csv', index=False)\nfinaldf.groupby('predictedvalues').count()","71d4f2ed":"# the confusion matrix heat map plot\ndef print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n    \n    Arguments\n    ---------\n    confusion_matrix: numpy.ndarray\n        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n        Similarly constructed ndarrays can also be used.\n    class_names: list\n        An ordered list of class names, in the order they index the given confusion matrix.\n    figsize: tuple\n        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n        the second determining the vertical size. Defaults to (10,7).\n    fontsize: int\n        Font size for axes labels. Defaults to 14.\n        \n    Returns\n    -------\n    matplotlib.figure.Figure\n        The resulting confusion matrix figure\n    \"\"\"\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n        \n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Gender recode function\ndef gender(row):\n    if row == 'female_disgust' or 'female_fear' or 'female_happy' or 'female_sad' or 'female_surprise' or 'female_neutral':\n        return 'female'\n    elif row == 'male_angry' or 'male_fear' or 'male_happy' or 'male_sad' or 'male_surprise' or 'male_neutral' or 'male_disgust':\n        return 'male'","6885370d":"# Get the predictions file \nfinaldf = pd.read_csv(\"Predictions.csv\")\nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \n\n# Confusion matrix \nc = confusion_matrix(finaldf.actualvalues, finaldf.predictedvalues)\nprint(accuracy_score(finaldf.actualvalues, finaldf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","3113fce8":"# Classification report \nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))","266073f7":"modidf = finaldf\nmodidf['actualvalues'] = finaldf.actualvalues.replace({'female_angry':'female'\n                                       , 'female_disgust':'female'\n                                       , 'female_fear':'female'\n                                       , 'female_happy':'female'\n                                       , 'female_sad':'female'\n                                       , 'female_surprise':'female'\n                                       , 'female_neutral':'female'\n                                       , 'male_angry':'male'\n                                       , 'male_fear':'male'\n                                       , 'male_happy':'male'\n                                       , 'male_sad':'male'\n                                       , 'male_surprise':'male'\n                                       , 'male_neutral':'male'\n                                       , 'male_disgust':'male'\n                                      })\n\nmodidf['predictedvalues'] = finaldf.predictedvalues.replace({'female_angry':'female'\n                                       , 'female_disgust':'female'\n                                       , 'female_fear':'female'\n                                       , 'female_happy':'female'\n                                       , 'female_sad':'female'\n                                       , 'female_surprise':'female'\n                                       , 'female_neutral':'female'\n                                       , 'male_angry':'male'\n                                       , 'male_fear':'male'\n                                       , 'male_happy':'male'\n                                       , 'male_sad':'male'\n                                       , 'male_surprise':'male'\n                                       , 'male_neutral':'male'\n                                       , 'male_disgust':'male'\n                                      })\n\nclasses = modidf.actualvalues.unique()  \nclasses.sort() \n\n# Confusion matrix \nc = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\nprint(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","887b8d5e":"# Classification report \nclasses = modidf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))","7bab8c0e":"modidf = pd.read_csv(\"Predictions.csv\")\nmodidf['actualvalues'] = modidf.actualvalues.replace({'female_angry':'angry'\n                                       , 'female_disgust':'disgust'\n                                       , 'female_fear':'fear'\n                                       , 'female_happy':'happy'\n                                       , 'female_sad':'sad'\n                                       , 'female_surprise':'surprise'\n                                       , 'female_neutral':'neutral'\n                                       , 'male_angry':'angry'\n                                       , 'male_fear':'fear'\n                                       , 'male_happy':'happy'\n                                       , 'male_sad':'sad'\n                                       , 'male_surprise':'surprise'\n                                       , 'male_neutral':'neutral'\n                                       , 'male_disgust':'disgust'\n                                      })\n\nmodidf['predictedvalues'] = modidf.predictedvalues.replace({'female_angry':'angry'\n                                       , 'female_disgust':'disgust'\n                                       , 'female_fear':'fear'\n                                       , 'female_happy':'happy'\n                                       , 'female_sad':'sad'\n                                       , 'female_surprise':'surprise'\n                                       , 'female_neutral':'neutral'\n                                       , 'male_angry':'angry'\n                                       , 'male_fear':'fear'\n                                       , 'male_happy':'happy'\n                                       , 'male_sad':'sad'\n                                       , 'male_surprise':'surprise'\n                                       , 'male_neutral':'neutral'\n                                       , 'male_disgust':'disgust'\n                                      })\n\nclasses = modidf.actualvalues.unique() \nclasses.sort() \n\n# Confusion matrix \nc = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\nprint(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","c2baff96":"# Classification report \nclasses = modidf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))","47769526":"from keras.models import Sequential, Model, model_from_json\nimport matplotlib.pyplot as plt\nimport keras \nimport pickle\nimport wave  # !pip install wave\nimport os\nimport pandas as pd\nimport numpy as np\nimport sys\nimport warnings\nimport librosa\nimport librosa.display\nimport IPython.display as ipd  # To play sound in the notebook\n\n# ignore warnings \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","bcecabec":"data, sampling_rate = librosa.load('\/kaggle\/input\/externaltest\/DC_d02.wav')\nipd.Audio('\/kaggle\/input\/externaltest\/DC_d02.wav')","48e6b137":"plt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)","faeba910":"# loading json and model architecture \njson_file = open('model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"saved_models\/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n \n# Keras optimiser\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","58d28e1d":"# Lets transform the dataset so we can apply the predictions\nX, sample_rate = librosa.load('\/kaggle\/input\/externaltest\/DC_d02.wav'\n                              ,res_type='kaiser_fast'\n                              ,duration=2.5\n                              ,sr=44100\n                              ,offset=0.5\n                             )\n\nsample_rate = np.array(sample_rate)\nmfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\nnewdf = pd.DataFrame(data=mfccs).T\nnewdf\n","56cf37c1":"# Apply predictions\nnewdf= np.expand_dims(newdf, axis=2)\nnewpred = loaded_model.predict(newdf, \n                         batch_size=16, \n                         verbose=1)\n\nnewpred","c1d28cae":"filename = '\/kaggle\/input\/labels\/labels'\ninfile = open(filename,'rb')\nlb = pickle.load(infile)\ninfile.close()\n\n# Get the final predicted label\nfinal = newpred.argmax(axis=1)\nfinal = final.astype(int).flatten()\nfinal = (lb.inverse_transform((final)))\nprint(final) #emo(final) #gender(final) ","da911bb2":"data, sampling_rate = librosa.load('\/kaggle\/input\/externaltest\/DC_d06.wav')\nipd.Audio('\/kaggle\/input\/externaltest\/DC_d06.wav')","4207d68e":"plt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)","584189d7":"# loading json and model architecture \njson_file = open('model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"saved_models\/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n \n# Keras optimiser\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","c8b33004":"# Lets transform the dataset so we can apply the predictions\nX, sample_rate = librosa.load('\/kaggle\/input\/externaltest\/DC_d06.wav'\n                              ,res_type='kaiser_fast'\n                              ,duration=2.5\n                              ,sr=44100\n                              ,offset=0.5\n                             )\n\nsample_rate = np.array(sample_rate)\nmfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\nnewdf = pd.DataFrame(data=mfccs).T\nnewdf","5f038115":"# Apply predictions\nnewdf= np.expand_dims(newdf, axis=2)\nnewpred = loaded_model.predict(newdf, \n                         batch_size=16, \n                         verbose=1)\n\nnewpred","d7cdd794":"filename = '\/kaggle\/input\/labels\/labels'\ninfile = open(filename,'rb')\nlb = pickle.load(infile)\ninfile.close()\n\n# Get the final predicted label\nfinal = newpred.argmax(axis=1)\nfinal = final.astype(int).flatten()\nfinal = (lb.inverse_transform((final)))\nprint(final) #emo(final) #gender(final) ","e484c582":"### Crowd Sourced Emotional Multimodal Actors Dataset (CREDMA-D) ","16341dde":"## Feature Extraction ","d0213ab6":"### Emotion vs Gender Accuracy ","bf5cb2e7":"### Gender Accuracy ","f45a98c5":"## Modelling ","766fbb5a":"### Surrey Audio-Visual Expressed Emotion (SAVEE)","1c45d5ab":"### Splitting the Dataset into Training and Test ","cfb34c3e":"### Description of CNN-Model","3af9fed2":"### Emotion Accuracy ","9423566f":"# Speech Emotion Recognition with CNN","4bb358c3":"### Toronto Emotional Speech Set (TESS) ","c1db497e":"### Combining the 4 Datasets ","0716aecb":"### Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) ","28e2525d":"### Predictions ","47916f6c":"## Data Preparation  & Processing ","d535d4f5":"## Testing the Model on External Audio ","f4454e06":"## Data Exploration","d927a06a":"## Second Test "}}