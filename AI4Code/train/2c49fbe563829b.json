{"cell_type":{"595960ae":"code","293d5e49":"code","360168d9":"code","097ebeea":"code","5e251d45":"code","232018cc":"code","2070b02a":"code","85873681":"code","20f9d4e9":"code","056f3344":"code","2418506a":"code","b916630a":"code","4fd52b3d":"code","26056c20":"code","05e12f96":"code","44eb32a7":"code","a2cbe989":"code","7fc1772d":"code","809301f6":"code","98f92dd3":"code","0f3ca369":"code","1e35b487":"code","791fc3d2":"code","926f826c":"code","4399324c":"code","02964422":"code","c25edc26":"code","1c3560ac":"markdown","d42260e2":"markdown","1671cea7":"markdown","bffccda0":"markdown","2c19cccf":"markdown","f18b938a":"markdown","d0bf7f63":"markdown","1e3c3637":"markdown","d51d4735":"markdown","d8f0133e":"markdown","e7883bba":"markdown","cd3f3970":"markdown","01bbe261":"markdown","a3900219":"markdown","708c68bb":"markdown","6981035b":"markdown","96e57434":"markdown","f65882f3":"markdown","a60fdddd":"markdown","549a14e8":"markdown","d1319f34":"markdown","7b96110d":"markdown","c7016684":"markdown","51b030cf":"markdown","22e0dfe1":"markdown","b436ebe8":"markdown","d539b258":"markdown"},"source":{"595960ae":"text_data = [\" Interrobang. By HK Henriette \",\n                \"Parking And Going. By Karl Gautier\",\n                \" Today Is The night. By Jarek Prakash \"]","293d5e49":"string_without_whitespace = [string.strip() for string in text_data]\nstring_without_whitespace","360168d9":"string_without_period = [string.replace(\".\", \"\") for string in string_without_whitespace]\nstring_without_period","097ebeea":"capital_text = [string.upper() for string in string_without_period]\ncapital_text","5e251d45":"from bs4 import BeautifulSoup\nimport nltk","232018cc":"# Create sample HTML content\nhtml = \"\"\"\n<div class='full_name'><span style='font-weight:bold'>\nMasego<\/span> Azra<\/div>\"\n\"\"\"\n\n# Parse html\nsoup = BeautifulSoup(html, \"lxml\")\n\n#Find div with class name 'full_name'\nsoup.find('div', {'class':'full_name'}).text","2070b02a":"import string\ntext_data = ['Hi!!!! I. Love. This. Song....',\n            '10000% Agree!!!! #LoveIT',\n            'Right?!?!']\nres_list = []\nfor string_tmp in text_data:\n    res = ''\n    for c in string_tmp:\n        if c not in string.punctuation:\n            res = res+c\n    res_list.append(res)\nres_list","85873681":"from nltk.tokenize import word_tokenize\nstring = \"The science of today is the technology of tomorrow. Thus the end\"\n\nword_tokenize(string)","20f9d4e9":"from nltk.corpus import stopwords","056f3344":"text = \"I am going to market today I had enjoyed your ride\"\ntext = text.lower()\nlist_of_words = word_tokenize(text)\n\ntmp = stopwords.words('english')\ntext_without_stopwords = [word for word in list_of_words if word not in tmp]\n\ntext_without_stopwords","2418506a":"tmp[:10]    #Here tmp is list of stopwords as assigned in above cell","b916630a":"from nltk.stem.porter import PorterStemmer\n\n# Here we already have tokenized words from above output, i.e. text_without_stopwords\n\n#Create porter\nporter = PorterStemmer()\n\nroot_words = [porter.stem(word) for word in text_without_stopwords]\nroot_words","4fd52b3d":"from nltk import pos_tag\nfrom nltk import word_tokenize","26056c20":"text_data = \"Chris loved outdoor running\"","05e12f96":"# Use pre-trained part of speech tagger\ntext_tagged = pos_tag(word_tokenize(text_data))\ntext_tagged","44eb32a7":"'''\nNNP: Proper noun, singular\nNN: Noun, singular or mass\nRB: Adverb\nVBD: Verb, past tense\nVBG: Verb, gerund or present participle\nJJ: Adjective\nPRP: Personal pronoun\n'''\nprint()","a2cbe989":"# Example to get all nouns\nnouns = [word for word, tag in text_tagged if tag in {'NN','NNS','NNP','NNPS'}]\nnouns","7fc1772d":"tweets = [\"I am eating a burrito for breakfast\",\n        \"Political science is an amazing field\",\n        \"San Francisco is an awesome city\"]\n\n# Create list\ntagged_tweets = []","809301f6":"# Tag each word and each tweet\nfor tweet in tweets:\n    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n    tagged_tweets.append([tag for word, tag in tweet_tag])\n    \ntagged_tweets","98f92dd3":"# Using one hot encoder\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n# Use one-hot encoding to convert the tags into features\none_hot_multi = MultiLabelBinarizer()\none_hot_multi.fit_transform(tagged_tweets)","0f3ca369":"# Using classes_ we can see that each feature is part-of-speech-tag\none_hot_multi.classes_","1e35b487":"import numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Create text\ntext_data = np.array(['I love India. India!',\n                        'Pune is the best',\n                        'xyz beats both'])\n\n# Create the bag of words feature matrix\ncount = CountVectorizer()\nbag_of_words = count.fit_transform(text_data)\n\n# Show feature matrix\nbag_of_words","791fc3d2":"# Create feature matrix with arguments\ncount_2gram = CountVectorizer(ngram_range=(1,2),\n    stop_words=\"english\")\n\nbag = count_2gram.fit_transform(text_data)\n# View feature matrix\nbag.toarray()","926f826c":"count_2gram.vocabulary_","4399324c":"# import requirements:\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer","02964422":"# Create sample data\ntext_data = np.array(['I love India. India!',\n                        'Japan is the best',\n                        'NY beats both'])","c25edc26":"# Create the tf-idf feature matrix\ntfidf = TfidfVectorizer()\nfeature_matrix = tfidf.fit_transform(text_data)\n\nfeature_matrix.toarray()","1c3560ac":"\nHere is the list of sample acronyms and their meaning:","d42260e2":"## Realistic case study of use of Parts-of-speech:\nConsider we have data where every observation has tweet. We want to convert those sentences into features for individual parts of speech. (e.g., a feature with 1 if a proper noun is present, and 0 otherwise)","1671cea7":"# AND THE MOST IMPORTANT CONCEPT OF TEXT HANDLING ->\n# * Weighting word importance\n## Problem:\nYou want a bag of words, but with words weighted by their importance to an observation.\n\n## Solution:\nCompare the frequency of the word in a document (i.e., the dataset of tweets, movie reviews, speech\ntranscripts, etc.) with the frequency of the word in all other documents using term\nfrequency-inverse document frequency (tf-idf). scikit-learn makes this easy with\nTfidfVectorizer:","bffccda0":"'''\nThis output is a sparse array, which is often necessary when we have a large amount\nof text. However, in our toy example we can use toarray to view a matrix of word\ncounts for each observation:\n'''\n\nbag_of_words.toarray()","2c19cccf":"It is important to be conscious of the fact that punctuation contains information (e.g.,\n\u201cRight?\u201d versus \u201cRight!\u201d). Removing punctuation is often a necessary evil to create\nfeatures; however, if the punctuation is important we should make sure to take that\ninto account.","f18b938a":"### The higher the resulting value, the more important the word is to a document.","d0bf7f63":"Stemming reduces word to its stem - without altering the original meaning of word. For example, both \u201ctradition\u201d and\n\u201ctraditional\u201d have \u201ctradit\u201d as their stem, indicating that while they are different words\nthey represent the same general concept. This process removes suffixes, prefixes etc. to get original root words.","1e3c3637":"By combining these two statistics, we can assign a score to every word representing\nhow important that word is in a document. Specifically, we multiply tf to the inverse\nof document frequency (idf):","d51d4735":"###### One of the most common methods of transforming text into features is by using a bag-of-words model. Bag-of-words models output a feature for every unique word in text data, with each feature containing a count of occurrences in observations.\n###### The text data in our solution was purposely small. In the real world, a single observation of text data could be the contents of an entire book! Since our bag-of-words model creates a feature for every unique word in the data, the resulting matrix can contain thousands of features. This means that the size of the matrix can sometimes become very large in memory. However, luckily we can exploit a common characteristic of bag-of-words feature matrices to reduce the amount of data we need to store.\n###### Most words likely do not occur in most observations, and therefore bag-of-words feature matrices will contain mostly 0s as values. We call these types of matrices \u201csparse.\u201d Instead of storing all values of the matrix, we can only store nonzero values and then assume all other values are 0. This will save us memory when we have large feature matrices. One of the nice features of CountVectorizer is that the output is a sparse matrix by default.\n###### CountVectorizer comes with a number of useful parameters to make creating bag-of-words feature matrices easy. First, while by default every feature is a word, that does not have to be the case. Instead we can set every feature to be the combination of two words (called a 2-gram) or even three words (3-gram). ngram_range sets the minimum and maximum size of our n-grams. For example, (2,3) will return all 2- grams and 3-grams. Second, we can easily remove low-information filler words using stop_words either with a built-in list or a custom list.","d8f0133e":"Tokenization, especially word tokenization, is a common task after cleaning text data\nbecause it is the first step in the process of turning the text into data we will use to\nconstruct useful features.","e7883bba":"# *Encoding Text as a Bag of Words\n## Problem:\nYou have text data and want to create a set of features indicating the number of times\nan observation\u2019s text contains a particular word.\n\n## Solution:\nUse scikit-learn\u2019s CountVectorizer:","cd3f3970":"In contrast, if a word appears in many documents, it is likely less important to any\nindividual document. For example, if every document in some text data contains the\nword 'after', then it is probably an unimportant word. We call this document frequency\n(df).","01bbe261":"Despite the strange name, Beautiful Soup is a powerful Python library designed for\nscraping HTML. Typically Beautiful Soup is used scrape live websites, but we can just\nas easily use it to extract text data embedded in HTML. The full range of Beautiful\nSoup operations is beyond the scope of this blog, but even the few methods used in\nour solution show how easily we can parse HTML code to extract the data we want.","a3900219":"The more a word appears in a document, the more likely it is important to that document.\nFor example, if the word economy appears frequently, it is evidence that the\ndocument might be about economics. We call this term frequency (tf).","708c68bb":"# * Removing punctuation\n## Problem:\nYou have a feature of text data and want to remove punctuation.\n\n## Solution:\nDefine a function that uses translate with a dictionary of punctuation characters:","6981035b":"#### tf_idf(t, d) = tf(t,d) \u00d7 idf(t)\nwhere t is a word and d is a document.","96e57434":"# * Stemming words\nYou have tokenized words and want to convert them into their root forms.\n\n## Problem:\nYou have tokenized words and want to convert them into their root forms.\n\n## Solution:\nUse NLTK\u2019s PorterStemmer:","f65882f3":"Once we've tags for each word, we can use it to find certain parts of speech.","a60fdddd":"##### Note that NLTK\u2019s stopwords assumes the tokenized words are all lowercased.","549a14e8":"# That's it folks... These are basics about handling text data in data mining process. You can get more information from post at AnalyticsVidhya:\nhttps:\/\/www.analyticsvidhya.com\/blog\/2017\/01\/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python\/\n\n# See you next week!","d1319f34":"# * Tokenizing text\n## Problem:\nYou have text and want to break it up into individual words.\n\n## Solution:\nNLP Toolkit of python (NLTK) has some really powerful methods including text tokenizing.","7b96110d":"The output is a list of tuples with the word and the tag of the part of speech.","c7016684":"# * Tagging Parts of Speech\n## Problem:\nYou have text data and want to tag each word or character with its part of speech.\n\n## Solution:\nUse NLTK\u2019s pre-trained parts-of-speech tagger:","51b030cf":"# * Parsing and cleaning HTML\n## Problem:\nYou have text data with HTML elements and want to extract just the text.\n\n## Solution:\nUse Beautiful Soup\u2019s extensive set of options to parse and extract from HTML:","22e0dfe1":"While \u201cstop words\u201d can refer to any set of words we want to remove before processing,\nfrequently the term refers to extremely common words that themselves contain\nlittle information value. NLTK has a list of common stop words that we can use to\nfind and remove stop words in our tokenized words","b436ebe8":"# * Cleaning Text\nThe most basic text cleaning operations should only use Python\u2019s core string operations,\nin particular strip, replace, and split:","d539b258":"# * Removing stop words\n## Problem:\nGiven tokenized words, remove common words.\n\n## Solution:\nUse NLTK's stop words"}}