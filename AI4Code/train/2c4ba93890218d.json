{"cell_type":{"541a5b2c":"code","3a333f2d":"code","dec89d99":"code","78020c00":"code","31d29ae2":"code","c6f6eaaa":"code","41b12d13":"code","22f229b1":"code","cc764b43":"code","d5432f0b":"code","28e561e8":"code","293ab008":"code","795e0373":"code","cd85222b":"code","4299dd64":"code","604a70a4":"markdown","62529c08":"markdown"},"source":{"541a5b2c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3a333f2d":"from google.cloud import bigquery\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom bq_helper import BigQueryHelper\n\nbq_assistant = BigQueryHelper(\"bigquery-public-data\", \"epa_historical_air_quality\")","dec89d99":"eda_query = \"\"\"\nselect \n    state_name\n    ,county_name\n    ,city_name\n    ,latitude\n    ,longitude\n    ,site_num\n    ,date_local\n    ,aqi as o3_aqi\n    ,arithmetic_mean as o3_mean\nfrom `bigquery-public-data.epa_historical_air_quality`.o3_daily_summary\nwhere poc = 1\n    and extract(year from date_local) > 2015\n    and state_name in ('Tennessee', 'Georgia')\n    and county_name in ('Shelby','Davidson','Fulton')\norder by state_name, county_name, date_local\n\"\"\"\n\ndf = bq_assistant.query_to_pandas(eda_query)\ndf.head()","78020c00":"group_df = df.groupby(['state_name','county_name','city_name', 'date_local'],as_index=False).mean()\ngroup_df['date'] = pd.to_datetime(group_df['date_local'])\n\ngroup_df.head()","31d29ae2":"atl_df = group_df[group_df['county_name'] == 'Fulton']\n\nsns.lineplot(x= 'date', y = 'o3_mean', data = atl_df)","c6f6eaaa":"# defining some helpers for testing\nmin_date = '2010-01-01'\nmax_date = '2019-12-31'\n# counties = \"('Davidson','Fulton','Dade','Shelby','Los Angeles')\"\n# states = \"('Tennessee', 'New York', 'California')\"\n\npollutants = ['o3', 'co', 'no2', 'so2', 'pm25_frm', 'pm10', 'temperature']","41b12d13":"base_query = \"\"\"\nselect \n    state_name\n    ,county_name\n    ,city_name\n    ,cbsa_name as metro_area\n    ,latitude\n    ,longitude\n    ,site_num\n    ,date_local\n    ,sample_duration\n    ,max(aqi) as {}_aqi\n    ,max(arithmetic_mean) as {}_mean\nfrom `bigquery-public-data.epa_historical_air_quality`.{}_daily_summary\nwhere date_local >= '{}'\n    and date_local <= '{}'\ngroup by state_name\n    ,county_name\n    ,city_name\n    ,cbsa_name \n    ,latitude\n    ,longitude\n    ,site_num\n    ,date_local\n    ,sample_duration\norder by state_name, county_name, site_num, date_local\n\"\"\"","22f229b1":"# create a dictionary of dataframes containing relevant info\n# will write each of these to a flat file, and we can join them to create our daily set of measures for each location\ndfs = dict()\n\nfor pol in pollutants:\n    print('Starting on {}'.format(pol))\n    dfs[pol] = bq_assistant.query_to_pandas(base_query.format(pol, pol, pol, min_date, max_date))\n    \ndfs.keys()","cc764b43":"# save these into output\n# everything after this is irrelevant\nfor pol in dfs.keys():\n    dfs[pol].to_csv(pol + '.csv', index=False)","d5432f0b":"# need to pay attention to these sample durations - duplicated measurements for some pollutants\ndfs['co']['sample_duration'].value_counts()","28e561e8":"# dfs['co'] = dfs['co'][dfs['co']['sample_duration'] == '8-HR RUN AVG END HOUR']","293ab008":"# join_cols = ['metro_area','state_name','county_name','city_name','latitude',\n#              'longitude','site_num','date_local', 'poc']\n\n# full_df = dfs['o3']\\\n#     .merge(dfs['co'], on = join_cols, how = 'outer')\\\n#     .merge(dfs['no2'], on = join_cols, how = 'outer')","795e0373":"# dfs['no2']['poc'].value_counts()","cd85222b":"# getting mean readings for each area - definitely easier to do this in BigQuery\/SQL with Pandas inefficiencies\n# agg_df = full_df\\\n#     .groupby(['metro_area','date_local'], as_index=False)\\\n#     .agg({'latitude':pd.Series.mean,'longitude':pd.Series.mean,'site_num':pd.Series.count,\n#          'o3_mean':pd.Series.mean,'o3_aqi':pd.Series.mean,'co_mean':pd.Series.mean,'co_aqi':pd.Series.mean,\n#          'no2_mean':pd.Series.mean, 'no2_aqi':pd.Series.mean})","4299dd64":"# print('{} total rows'.format(agg_df.shape[0]))\n# agg_df[(~agg_df['o3_aqi'].isnull()) & (~agg_df['co_aqi'].isnull()) & (~agg_df['no2_aqi'].isnull())]","604a70a4":"# Exploration of EPA Data\nThis kernel is where I'm starting to familiarize myself with BigQuery and the Air Quality data before diving into more advanced analysis.","62529c08":"## Creating a Working Dataset\nThis will be where I write ETL to pivot the pollutants we care about for each date at each location. In addition to the measures themselves, here are some potentially helpful predictors for future air quality:\n\n* Location (latitude\/longitude, clustered lat\/lon, encoded metro area, etc.)\n* Day of week\n* Temperature (?)\n* Related info from surrounding areas"}}