{"cell_type":{"d88bd98b":"code","b6252ba9":"code","04b36772":"code","48a081d7":"code","8acd648d":"code","a2e5909f":"code","5a965b6c":"code","84c93280":"code","f5b37e6e":"code","29553569":"code","b58d51f8":"code","82046519":"code","aad6bc8e":"code","0b9ba85a":"code","9e1510fe":"code","5947b0f7":"code","f2f4ea96":"code","65002b4e":"code","dce8486e":"code","efc7948b":"code","4217251e":"code","af6a119b":"code","7e0cd1ed":"code","cf910c39":"markdown","be005bd7":"markdown","8f5b99bc":"markdown","ec8f75f7":"markdown","8d5275c0":"markdown","a4486d4a":"markdown","efa2df70":"markdown","4b8aa479":"markdown","21d778a9":"markdown","d31b0692":"markdown","55d6e820":"markdown","83368f56":"markdown","227dbec5":"markdown","b03d2441":"markdown","2414bcd6":"markdown","655dc94e":"markdown","503486bb":"markdown","85912982":"markdown","4867fee0":"markdown","87777ad8":"markdown","d484f7bc":"markdown","49d628ca":"markdown","b6ec6e1d":"markdown","a5171ce0":"markdown","3b7252e6":"markdown"},"source":{"d88bd98b":"import nltk\nimport numpy as np\nimport pandas as pd\nimport re\nimport string\n\nfrom nltk.corpus import twitter_samples\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer","b6252ba9":"#nltk.download('twitter_samples')\n#nltk.download('stopwords')","04b36772":"def process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n\n    return tweets_clean","48a081d7":"def build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs","8acd648d":"# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')","a2e5909f":"print(\"An example of a positive tweet not preprocessed yet: \\n\\n {}\".format(all_positive_tweets[0]))","5a965b6c":"# split the data into two pieces, one for training and one for testing (validation set) \ntest_pos = all_positive_tweets[4000:]\ntrain_pos = all_positive_tweets[:4000]\ntest_neg = all_negative_tweets[4000:]\ntrain_neg = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg \ntest_x = test_pos + test_neg","84c93280":"print(\"An example of a positive tweet not preprocessed yet: \\n\\n {}\".format(train_x[0]))","f5b37e6e":"preprocessed_tweet_ex = process_tweet(train_x[0])\n\nprint(\"An example of a preprocessed positive tweet: \\n\\n {}\".format(preprocessed_tweet_ex))","29553569":"# combine positive and negative labels\ntrain_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\ntest_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)","b58d51f8":"# Print the shape train and test sets\nprint(\"train_y.shape = \" + str(train_y.shape))\nprint(\"test_y.shape = \" + str(test_y.shape))","82046519":"# create frequency dictionary\nfreqs = build_freqs(train_x, train_y)\n\n# check the output\nprint(\"type(freqs) = \" + str(type(freqs)))\nprint(\"len(freqs) = \" + str(len(freqs.keys())))","aad6bc8e":"# test the function below\nprint('This is an example of a positive tweet: \\n', train_x[100])\nprint('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[100]))","0b9ba85a":"def extract_features(tweet, freqs):\n    '''\n    Input: \n        tweet: a list of words for one tweet\n        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n    Output: \n        x: a feature vector of dimension (1,3)\n    '''\n    # process_tweet tokenizes, stems, and removes stopwords\n    word_l = process_tweet(tweet)\n    \n    # 3 elements in the form of a 1 x 3 vector\n    x = np.zeros((1, 3)) \n    \n    #bias term is set to 1\n    x[0,0] = 1 \n    \n    \n    # loop through each word in the list of words\n    for word in word_l:\n        \n        # increment the word count for the positive label 1\n        x[0,1] += freqs.get((word, 1.0),0)\n        \n        # increment the word count for the negative label 0\n        x[0,2] += freqs.get((word, 0.0),0)\n        \n    assert(x.shape == (1, 3))\n    return x","9e1510fe":"extract_features(train_x[0], freqs)","5947b0f7":"def sigmoid(z): \n    \n    # calculate the sigmoid of z\n    h = 1 \/ (1 + np.exp(-z))\n    \n    return h","f2f4ea96":"def gradientDescent(x, y, theta, alpha, num_iters):\n    '''\n    Input:\n        x: matrix of features which is (m,n+1)\n        y: corresponding labels of the input matrix x, dimensions (m,1)\n        theta: weight vector of dimension (n+1,1)\n        alpha: learning rate\n        num_iters: number of iterations you want to train your model for\n    Output:\n        J: the final cost\n        theta: your final weight vector\n    Hint: you might want to print the cost to make sure that it is going down.\n    '''\n    # get 'm', the number of rows in matrix x\n    m = x.shape[0]\n    \n    for i in range(0, num_iters):\n        \n        # get z, the dot product of x and theta\n        z = np.dot(x,theta)\n        \n        # get the sigmoid of z\n        h = sigmoid(z)\n        \n        # calculate the cost function\n        J = -1.\/m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))    \n\n        # update the weights theta\n        theta = theta = theta - (alpha\/m) * np.dot(x.transpose(),(h-y))\n        \n        if i % 100 == 0:\n            print(\"Cost at step {}: {}\".format(i, J))\n        \n    J = float(J)\n    return J, theta","65002b4e":"# collect the features 'x' and stack them into a matrix 'X'\nX = np.zeros((len(train_x), 3))\nfor i in range(len(train_x)):\n    X[i, :]= extract_features(train_x[i], freqs)\n\n# training labels corresponding to X\nY = train_y\n\n# Apply gradient descent\nJ, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 2500)\n\nprint(f\"The cost after training is {J:.8f}.\")","dce8486e":"print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")","efc7948b":"def predict_tweet(tweet, freqs, theta):\n    '''\n    Input: \n        tweet: a string\n        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n        theta: (3,1) vector of weights\n    Output: \n        y_pred: the probability of a tweet being positive or negative\n    '''    \n    # extract the features of the tweet and store it into x\n    x = extract_features(tweet,freqs)\n    \n    # make the prediction using x and theta\n    y_pred = sigmoid(np.dot(x,theta))\n        \n    return y_pred","4217251e":"def test_logistic_regression(test_x, test_y, freqs, theta):\n    \"\"\"\n    Input: \n        test_x: a list of tweets\n        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n        freqs: a dictionary with the frequency of each pair (or tuple)\n        theta: weight vector of dimension (3, 1)\n    Output: \n        accuracy: (# of tweets classified correctly) \/ (total # of tweets)\n    \"\"\"\n        \n    # the list for storing predictions\n    y_hat = []\n    \n    for tweet in test_x:\n        # get the label prediction for the tweet\n        y_pred = predict_tweet(tweet, freqs, theta)\n        \n        if y_pred > 0.5:\n            # append 1.0 to the list\n            y_hat.append(1)\n        else:\n            # append 0 to the list\n            y_hat.append(0)\n\n    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n    accuracy = (y_hat==np.squeeze(test_y)).sum()\/len(test_x)\n    \n    return accuracy","af6a119b":"test_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\nprint(f\"Logistic regression model's accuracy = {test_accuracy:.4f}\")","7e0cd1ed":"my_tweet = \"@Naser I am very happy to enter the field of artificial intelligence and proud of myself for reaching this level and I will continue to learn more :)\"\n\nmy_tweet_y_pred = predict_tweet(my_tweet, freqs, theta)\n\nif my_tweet_y_pred >= 0.5:\n    print(\"Positive Tweet\")\nelse:\n    print(\"Negative Tweet\")\n    \nprint(\"\\n y_pred = {}\".format(float(my_tweet_y_pred) ))","cf910c39":"**By now we have our (x_train, y_train) pairs, (x_test, y_test) pairs and Frequencies . Now we are ready for feature extraction and LR training.**","be005bd7":"We will take the first 4000 tweets from the negative examples and the first 4000 tweets in the positive examples to form the training dataset and the rest will be used to form the test dataset","8f5b99bc":"Now we will extract the features and using them to train the model using gradient descent algorithm","ec8f75f7":"Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.","8d5275c0":"- After the tweets are clean, to perform **Negative & Positive Frequencies ** method, we need to find the Frequencies  which is a dictionary mapping each (word, sentiment) pair to its frequency: the number of times that a word showed up in a class.\n\n\n\n- This will done using the following function **build_freqs**","a4486d4a":"# 5- Model Evaluation ","efa2df70":"The cost function used for logistic regression is the average of the log loss across all training examples:","4b8aa479":"# 4- Logistic Regression & Training ","21d778a9":"Now lets try the model on a tweet that I will write","d31b0692":"# 2- Data Preprocessing ","55d6e820":"In this section we will use Frequencies  dict. to extract useful features for semantic analysis. \n\nFor each tweet we will use the following feature thet extracted from  the tweet totrain the Logistic Regression: \n    \nFeature_of_tweet_m = [Bias,  Sum(freq. for Positive Freqs for each word in tweet),  Sum(freq. for Negative Freqs for each word in tweet)]","83368f56":"The process_tweet transform the tweet that full of meaningless things that will not help the Logistic Regreesion (LR) to oerform its task, so we need to clean the tweet using utilities in nltk. \n\n- remove stock market tickers\n- remove old style retweet text \"RT\"\n- remove hyperlinks\n- remove hashtags,  only removing the hash # sign from the word\n- and then tokenize the tweet. To transform the tweet into list of meaningfull words we will remove stopwords and punctuation, and finally stem the word. ","227dbec5":"The function **test_logistic_regression** calculate the accuracy of our model on the test dataset","b03d2441":"Now the data will be splitted into training and test datasets ","2414bcd6":"**train_x** will contain 8000 tweets where the first 4000 are positive tweets and the next 4000 are the negative tweets. \n\n**test_x** will contain 2000 tweets where the first 1000 are positive tweets and the next 1000 are the negative tweets.","655dc94e":"# 3- Feature Extraction ","503486bb":"**extract_features** function will take a single tweet and the dictionary corresponding to the frequencies of each tuple (word, label) and return a feature vector of dimension (1,3)\n\n- x will be a numpy  array of shape (1,3), where the first entry will be the bias and equal to 1, the second entry is Sum(freq. for Positive Freqs for each word in tweet) and the final one is Sum(freq. for Negative Freqs for each word in tweet).","85912982":"- Data source : <a href=\"https:\/\/www.nltk.org\/howto\/twitter.html\">Twitter_samples dataset.<\/a>\n\n- Code source : <a href=\"https:\/\/www.coursera.org\/specializations\/natural-language-processing \">coursera: Natural Language Processing<\/a>","4867fee0":"**Negative & Positive Frequencies ** method need a clean tweets, so we need to preprocess the tweets to get the best performance using this mehtod. \n\nPreprocessing tweets can be done by removing any meaningless words, letters and symbols such as **stop words and punctuations.** Some examples of stop words: and, or, a, is, are, for, has, at, ...","87777ad8":"# 1- Imports","d484f7bc":"# 6- Thank you \n\n**Thank you for reading, I hope you enjoyed and benefited from it.**\n\n**If you have any questions or notes please leave it in the comment section.**\n\n**If you like this notebook please press upvote and thanks again.**","49d628ca":"**train_y** is an np array of the shape (8000, 1) where the first 4000 entries are equal to 1 and the second 4000 entries are equal to 0\n\n**test_y** is an np array of the shape (2000, 1) where the first 1000 entries are equal to 1 and the second 1000 entries are equal to 0","b6ec6e1d":"In the example above, the array contains 3 entries:\n- **First item** is the **bias** and equal to 1 \n\n\n- **second entry** is equal to 3020 which means that the words in this tweet appear in a **positive examples** 3020 times \n\n\n- **Third entry** is equal to 61 which means that the words in this tweet appear in a **negative examples** 61 times ","a5171ce0":"The function **predict_tweet** takes a tweet as a string, freqs, and the trained weights. The output of this function is y_pred which is a value between 0 and 1. y_pred is the threshold so that if y_pred >= 0.5 the tweet is  positive  and if y_pred < 0.5 the tweet is negative. ","3b7252e6":"# Sentiment Analysis Using Negative & Positive Frequencies "}}