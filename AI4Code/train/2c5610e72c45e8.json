{"cell_type":{"8ab3da2a":"code","e729880f":"code","1aa06eea":"code","eb416687":"code","c3a8404b":"code","376f3b84":"code","c83ab4f7":"code","c4b82411":"code","efabfa0c":"code","a35af7da":"code","978bbf08":"code","0a841fbc":"code","6032028e":"code","e86524ad":"code","b8ed2e44":"code","8b518130":"code","362811c8":"code","be3e2d95":"code","2a8f85ea":"code","935e9a4c":"code","c7efe19d":"markdown","e4d9b723":"markdown","2d1cf7b2":"markdown","51a85060":"markdown","b8ed3905":"markdown","8692cbd9":"markdown","a7cfac02":"markdown","1ab59af4":"markdown","b87a5ee5":"markdown","43998eac":"markdown","c805fdfe":"markdown","469605f8":"markdown","c3839745":"markdown","d9bea711":"markdown","78ae7699":"markdown","195321f8":"markdown","e3bcf2f4":"markdown","ced30fd4":"markdown","b64255e1":"markdown","c324e985":"markdown","30e51956":"markdown","95a9f2c8":"markdown"},"source":{"8ab3da2a":"import pandas as pd\nimport numpy as np\nfrom spacy.training.example import Example\nimport random\nfrom tqdm import tqdm\nfrom spacy.scorer import Scorer\nimport warnings\nwarnings.filterwarnings('ignore')\nimport spacy\nimport pickle\n","e729880f":"df = pd.read_csv('..\/input\/ner-raw-data\/NER_data_raw.csv')\ndf.head()","1aa06eea":"# load the training data\nimport json\nwith open('..\/input\/labelled-sample-data\/project-16-at-2022-01-31-17-05-27165836.json','rb') as fp1:\n    training_data = json.load(fp1)\n    TRAIN_DATA = []\n    entities = []\n    for text in training_data:\n        info = text.get('text')\n        if text.get('label') is not None:\n            for label in text.get('label'):\n                entities.append([label.get('text'), label.get('labels')[0]])\nlabels_ls =pd.DataFrame(entities, columns = ['val', 'head'])\nlabels_ls['head'].unique()\n\n# load all the keywords from sample - labelled data\nperson = list(labels_ls[labels_ls['head']=='PER']['val'].str.lower().unique())\nloc = list(labels_ls[labels_ls['head']=='LOC']['val'].str.lower().unique())\ntime_ = list(labels_ls[labels_ls['head']=='TIME']['val'].str.lower().unique())\norg = list(labels_ls[labels_ls['head']=='ORG']['val'].str.lower().unique())\n# save to a pickle file\n\n\ndef pickle_keywords(key, file_name):\n    filename = str(file_name)+'.pickle'\n    with open(filename, 'wb') as file:\n#     licenses = pickle.load(s5)\n        pickle.dump(key, file)\n\n    \npickle_keywords(person,'person')\npickle_keywords(loc,'loc')\npickle_keywords(time_,'time_')\npickle_keywords(org,'org')\n\n","eb416687":"def load_keywords(file_name):\n    filename = str(file_name)+'.pickle'\n    with open(filename, 'rb') as file:\n        return pickle.load(file)\n\nperson = load_keywords('person')\nloc = load_keywords('loc')\ntime_= load_keywords('time_')\norg= load_keywords('org')","c3a8404b":"def annotate(text, texts_lists, tags, nlp, as_docs = True):\n    doc = nlp.tokenizer(text)\n    matcher = spacy.matcher.PhraseMatcher(nlp.vocab)\n    \n    for tag, texts_list in zip(tags, texts_lists):\n#         print(texts_list)\n        \n        patterns = list(nlp.tokenizer.pipe(texts_list))\n        matcher.add(tag, patterns)\n    \n    matches = matcher(doc)\n    results = []\n    entities = set()\n    for match_id, start, end in matches:\n        tag = nlp.vocab.strings[match_id]\n        results.append({\n            'from_name': 'label',\n            'to_name': 'text',\n            'type': 'labels',\n            'value': {\n                'start': doc[start:end].start_char,\n                'end': doc[start:end].end_char,\n                'text': str(doc[start:end]),\n                'labels': [tag]\n            }\n        })\n    entities.add(tag)\n    return results, entities","376f3b84":"!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.4.0\/en_core_sci_sm-0.4.0.tar.gz","c83ab4f7":"tags = ['person','loc','time','org']\ntags_lists = [person,loc,time_,org]\ndata = 'The Pakistani and iran military launched its offensive in Orakzai to hunt Taliban insurgents'\n# import scispacy\n\nnlp = spacy.load(\"en_core_sci_sm\")\nout,entities= annotate(data, tags_lists, tags, nlp,True)\nout","c4b82411":"texts = df['text'].head(1000) # reducing to 1000 rows for computation in kaggle\n\nentities = set()\ntasks = []\n\nfor text in texts:\n    predictions = []\n    text = text\n    spans,ents= annotate(text, tags_lists, tags, nlp,True)\n    entities |= ents\n    predictions.append({'model_version': 'annatation', 'result': spans})\n    tasks.append({\n        'data': {'text': text},\n        'predictions': predictions\n    })\n\n# Save Label Studio tasks.json\n# print(f'Save {len(tasks)} tasks to \"tasks.json\"')\nwith open('tasks.json', mode='w') as f:\n    json.dump(tasks, f)","efabfa0c":"TRAIN_DATA = []\n\ndef import_label_studio_data(filename):\n    with open(filename,'rb') as fp:\n        training_data = json.load(fp)\n    for text in training_data:\n        entities = []\n        info = text.get('text')\n        entities = []\n        if text.get('label') is not None:\n            list_ = []\n            for label in text.get('label'):\n                list_.append([label.get('start'), label.get('end')])\n            a = np.array(list_)\n            overlap_ind =[]\n            for i in range(0,len(a[:,0])):\n                a_comp = a[i]\n                x = np.delete(a, (i), axis=0)\n                overlap_flag = any([a_comp[0] in range(j[0], j[1]+1) for j in x])\n                if overlap_flag:\n                    overlap_ind.append(i)\n                    \n            for ind, label in enumerate(text.get('label')):\n                if ind in overlap_ind:\n                    iop=0\n                else:\n                    if label.get('labels') is not None:\n                        entities.append((label.get('start'), label.get('end') ,label.get('labels')[0]))\n        TRAIN_DATA.append((info, {\"entities\" : entities}))","a35af7da":"import_label_studio_data('..\/input\/labelledsample-data2\/labelled_data.json')\nimport_label_studio_data('..\/input\/labelled-sample-data\/project-16-at-2022-01-31-17-05-27165836.json')\n\n","978bbf08":"TRAIN_DATA[0]","0a841fbc":"# Load pre-existing spacy model\nnlp=spacy.load('en_core_sci_sm')\nner=nlp.get_pipe(\"ner\")","6032028e":"model = None\nif model is not None:\n    nlp = spacy.load(model)  \n    print(\"Loaded model '%s'\" % model)\nelse:\n    nlp = spacy.blank('en')  \n    print(\"Created blank 'en' model\")\n\n#set up the pipeline\n\nif 'ner' not in nlp.pipe_names:\n    ner = nlp.add_pipe('ner')\n#     nlp.add_pipe(ner, last=True)\nelse:\n    ner = nlp.get_pipe('ner')","e86524ad":"def train_test_split(data, test):\n    train_end = int(len(data) * (100-test)*0.01)\n    print(train_end)\n    test_start = int(len(data) * (100-test))+1\n    return data[0:train_end], data[train_end+1:len(data)]","b8ed2e44":"train_data, test_data = train_test_split(TRAIN_DATA, test=50)","8b518130":"def train_ner_model(train_data_m,n_iter=1):\n    for _, annotations in train_data_m:\n        for ent in annotations.get('entities'):\n            ner.add_label(ent[2])\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        optimizer = nlp.begin_training()\n        for itn in range(n_iter):\n            random.shuffle(train_data_m)\n            losses = {}\n            for text, annotations in tqdm(train_data_m):\n                doc = nlp.make_doc(text)\n                example = Example.from_dict(doc, annotations)\n                # Update the model\n                nlp.update([example], losses=losses, drop=0.2)\n            print(losses)\n        return nlp","362811c8":"nlp_model = train_ner_model(train_data,n_iter=20)","be3e2d95":"for text, _ in test_data:\n#     text = \"i AM USING polo \"\n    doc = nlp(text)\n    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])","2a8f85ea":"def evaluate(nlp, examples):\n    scorer = Scorer()\n    example = []\n    for input_, annot in examples:\n        pred = nlp(input_)\n#         print(pred,annot)\n#         temp = Example.from_dict(pred, dict.fromkeys(annot))\n        temp = Example.from_dict(pred, annot)\n        example.append(temp)\n    scores = scorer.score(example)\n    return scores","935e9a4c":"results = evaluate(nlp, test_data)\nprint(results)","c7efe19d":"# Loading Raw Dataset","e4d9b723":"## Create Project:   \n\n![image.png](attachment:70eae54c-962c-4c68-855f-8c9a57c661fa.png)\n## Upload Dataset:   \n\nNER_data_raw.csv,Treat csv\/TSV as time series\n![Screenshot (762).png](attachment:b54ccb74-18ce-4b7b-bf39-f41661024042.png)\n\nand then \n\n### add Labels:\n\n1. PER\n2. LOC\n3. TIME\n4. ORG","2d1cf7b2":"# Score explanation\n\n> 1. **ents_p** is average precision score of the model for all entities\n> 2. **ents_r** is average recall score of the model for all entities\n> 3. **ents_f** is average f1 score  of the model for all entities\n\n\n**under each entity:**\nORG: with ***p,r,f are precision, recall, f1 score*** of the specific entity **ORG**\n\n","51a85060":"# Annotate the data using The created vocabulary lists and this can be later load to Label-studio for correction","b8ed3905":"# Evaluation","8692cbd9":"# Pre-annotation of dataset\n\nLoad the labelled sample data to annotate all the other rows in the dataset, this will help to labelled all the samples in which the keywords are already known","a7cfac02":"We are going to use **Label-studio to Label** this dataset which contains fields\n1. PER -->Person\n2. TIME --> Date and time\n3. ORG - Organization\n4. LOC - Location \n\n\n","1ab59af4":"# Following are included in this Notebook: \n\n> 1. Data collection\n2. Label-studio installation \n> 3. Label - studio annotation using 1% of samples\n> 4. using Vocabulary list and label -studio annotated files for preannotation vocabulary creation and pickle the output\n> 5. Pre-annatation code for label-studio\n> 6. uploading to label-studio and correction and downloading of dataset\n> 7. Train and test split\n> 8. training a custom model \n> 9. Testing the model\n> 10. Metrics explanation ","b87a5ee5":"# Load the pickled keywords for vocabulary from here on","43998eac":"### Start labelling the dataset: atleast 1 % of the dataset\n\n![image.png](attachment:301f60cd-c974-44b2-ab58-57cdda2cfc07.png)","c805fdfe":"# Label-Studio Installation \n* Install Virtual environment in cmd\n    * pip install virtualenv\n* Create a virtual environment for the project\n    * virtualenv NER_project\n    * NER_project\\Scripts\\activate\nonce the virtual environment activated \n* Install label-studio\n    * pip install label-studio \n* Initiate Label-studio instance and create an account in it to store all the labelling tasks, call label-studio in cmd\n    * label-studio\n    \n","469605f8":"# Testing","c3839745":"# Apply Annotation to all the rows","d9bea711":"### once labelled export data as json-min\n\n![image.png](attachment:749d6b8e-a84e-4222-a143-a1e520001cc5.png)","78ae7699":"# Check code works on a sample text","195321f8":"**Create a blank model**","e3bcf2f4":"# Introduction:\n#### This Notebook is made to help Data-scientist in creating a Custom NER (Named Entity Recognition) from Scratch with raw unlabelled dataset\n\n#### Labelling is done using the Label-studio tool and Model is trained and tested using SpaCy NER model","ced30fd4":"## import labelled dataset","b64255e1":"# Training a custom NER model","c324e985":"## Load tasks.json to Label-studio and correct the mislabelled text and label missed texts \nThe above pre-annotation helps to reduce the labour of labelling the keywords again and again and saves time and effort\n\nOnce the labelling is done for all the samples, export as json min and Do model training\n\nHere, model training is done just the labelled sampled data","30e51956":"# Convert Label-studio data to spacy NER format","95a9f2c8":"**Import scores:**\n![image.png](attachment:59898b61-12f0-463d-9e6b-5eca8318906c.png)"}}