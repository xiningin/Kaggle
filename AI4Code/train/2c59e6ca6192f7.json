{"cell_type":{"db8a72aa":"code","78a48187":"code","b29d4cd6":"code","bafd95d8":"code","0aa86426":"code","a046c01c":"code","89265155":"markdown","7bd4877a":"markdown","fe35ee42":"markdown","c56db506":"markdown","4dc9828d":"markdown","ea2b5d93":"markdown","479d6595":"markdown","c1141c42":"markdown","fe4457fd":"markdown","a81d30ec":"markdown","383a8bdf":"markdown","9be47b9e":"markdown","9a656784":"markdown","58dc2b9b":"markdown","b43de3c1":"markdown","8bd15a99":"markdown","393de531":"markdown","4907051e":"markdown"},"source":{"db8a72aa":"import math\nimport numpy.linalg\nimport numpy as np\n\ndef chernoff_distance(s, means, variances, univariate = False):\n    \"\"\" Returns the Chernoff Distance, as defined in (3.150), p.98 of \n        Fukunaga, 1990, Introduction to Statistical Pattern Recognition, 2nd Edition\n    \"\"\"\n\n    from math import log\n    from math import sqrt\n    from math import pow\n    \n    from numpy.linalg import det\n    from numpy import transpose\n    from numpy.linalg import inv\n    from numpy import array\n\n    if univariate:\n        part1 = 0.25 * log(0.25 * (variances[0] \/ variances[1] + variances[1] \/ variances[0]) + 0.5)\n        part2 = 0.25 * pow(means[0] - means[1], 2) \/ (variances[0] + variances[1])\n        return part1 + part2\n\n    mean_diff = array(means[0]) - array(means[1])\n    var_avg = (array(variances[0]) + array(variances[1])) * 0.5\n    ln_coeff = 0.5 * log(det(var_avg) \/ sqrt(det(array(variances[0])) * det(array(variances[1]))))\n\n    #return 0.125 * transpose(mean_diff) * inv(var_avg) * mean_diff + ln_coeff\n    return 0.125 * np.matmul(np.matmul(mean_diff.T, inv(var_avg)), mean_diff) + ln_coeff \n    \n\nif __name__ == '__main__':\n    #import doctest\n    #doctest.testmod()\n\n    mean1 = (1, 2)\n    cov1 = [[1, 0], [0, 1]]\n    mean2 = (3, 4)\n    cov2 = [[2, 0], [0, 2]]\n\n    s = 0.5\n\n    distance = chernoff_distance(s, [mean1, mean2], [cov1, cov2])\n\n    print('chernoff_distance(', end='')\n    print(s, str([mean1, mean2]), str([cov1, cov2]), end='')\n    print('):')\n    print(distance)\n\n    from numpy import exp\n    print('Error', exp(-distance))\n\n    distance2 = chernoff_distance(s, [mean1, mean1], [cov1, cov1])\n\n    print('chernoff_distance(', end='')\n    print(s, str([mean1, mean1]), str([cov1, cov1]), end='')\n    print('):')\n    print(distance2)\n\n    from numpy import exp\n    print('Error', exp(-distance2))","78a48187":"def Kolmogorov_Smirnov_Dist(XX, YY):\n  \n    import numpy as np\n    nx = len(XX)\n    ny = len(YY)\n    n = nx + ny\n\n    XY = np.concatenate([XX,YY])\n    X2 = np.concatenate([np.repeat(1\/nx, nx), np.repeat(0, ny)])\n    Y2 = np.concatenate([np.repeat(0, nx), np.repeat(1\/ny, ny)])\n\n    S_Ind = np.argsort(XY)\n    XY_Sorted = XY[S_Ind]\n    X2_Sorted = X2[S_Ind]\n    Y2_Sorted = Y2[S_Ind]\n\n    Res = 0;\n    E_CDF = 0;\n    F_CDF = 0;\n    power = 1;\n\n    for ii in range(0, n-2):\n        E_CDF = E_CDF + X2_Sorted[ii]\n        F_CDF = F_CDF + Y2_Sorted[ii]\n        if XY_Sorted[ii+1] != XY_Sorted[ii]: height = abs(F_CDF-E_CDF)\n        if height > Res: Res = height\n\n    KS_Dist = Res**power\n    \n    return KS_Dist\n\nif __name__ == '__main__':\n\n    XX = np.random.normal(1, 1, 1000)\n    YY = np.random.normal(3, 1, 1000)\n    \n    Dist = Kolmogorov_Smirnov_Dist(XX, YY)\n    \n    print(Dist)","b29d4cd6":"def Kuiper_Dist(XX, YY):\n  \n    import numpy as np\n    nx = len(XX)\n    ny = len(YY)\n    n = nx + ny\n\n    XY = np.concatenate([XX,YY])\n    X2 = np.concatenate([np.repeat(1\/nx, nx), np.repeat(0, ny)])\n    Y2 = np.concatenate([np.repeat(0, nx), np.repeat(1\/ny, ny)])\n\n    S_Ind = np.argsort(XY)\n    XY_Sorted = XY[S_Ind]\n    X2_Sorted = X2[S_Ind]\n    Y2_Sorted = Y2[S_Ind]\n\n    up = 0\n    down = 0\n    Res = 0\n    E_CDF = 0\n    F_CDF = 0\n    power = 1\n\n    for ii in range(0, n-2):\n        E_CDF = E_CDF + X2_Sorted[ii]\n        F_CDF = F_CDF + Y2_Sorted[ii]\n        if XY_Sorted[ii+1] != XY_Sorted[ii]: height = F_CDF-E_CDF\n        if height > up: up = height\n        if height < up: down = height\n\n    K_Dist = abs(down)**power + abs(up)**power\n    \n    return K_Dist\n\nif __name__ == '__main__':\n\n    XX = np.random.normal(1, 1, 1000)\n    YY = np.random.normal(3, 1, 1000)\n    \n    Dist = Kuiper_Dist(XX, YY)\n    \n    print(Dist)","bafd95d8":"# Cramer-Von Mises Distance\ndef CVM_Dist(XX, YY):\n  \n    import numpy as np\n    nx = len(XX)\n    ny = len(YY)\n    n = nx + ny\n\n    XY = np.concatenate([XX,YY])\n    X2 = np.concatenate([np.repeat(1\/nx, nx), np.repeat(0, ny)])\n    Y2 = np.concatenate([np.repeat(0, nx), np.repeat(1\/ny, ny)])\n\n    S_Ind = np.argsort(XY)\n    XY_Sorted = XY[S_Ind]\n    X2_Sorted = X2[S_Ind]\n    Y2_Sorted = Y2[S_Ind]\n\n    Res = 0;\n    E_CDF = 0;\n    F_CDF = 0;\n    power = 1;\n\n    for ii in range(0, n-2):\n       E_CDF = E_CDF + X2_Sorted[ii]\n       F_CDF = F_CDF + Y2_Sorted[ii]\n       height = abs(F_CDF - E_CDF)\n       if XY_Sorted[ii+1] != XY_Sorted[ii]: Res = Res + height**power\n\n    CVM_Dist = Res\n    \n    return CVM_Dist\n\nif __name__ == '__main__':\n\n    XX = np.random.normal(1, 1, 1000)\n    YY = np.random.normal(3, 1, 1000)\n    \n    Dist = CVM_Dist(XX, YY)\n    \n    print(Dist)","0aa86426":"def Anderson_Darling_Dist(XX, YY):\n  \n    import numpy as np\n    nx = len(XX)\n    ny = len(YY)\n    n = nx + ny\n\n    XY = np.concatenate([XX,YY])\n    X2 = np.concatenate([np.repeat(1\/nx, nx), np.repeat(0, ny)])\n    Y2 = np.concatenate([np.repeat(0, nx), np.repeat(1\/ny, ny)])\n\n    S_Ind = np.argsort(XY)\n    XY_Sorted = XY[S_Ind]\n    X2_Sorted = X2[S_Ind]\n    Y2_Sorted = Y2[S_Ind]\n\n    Res = 0\n    E_CDF = 0\n    F_CDF = 0\n    G_CDF = 0\n    height = 0\n    SD = 0\n    power = 1\n\n    for ii in range(0, n-2):\n        E_CDF = E_CDF + X2_Sorted[ii]\n        F_CDF = F_CDF + Y2_Sorted[ii]\n        G_CDF = G_CDF + 1\/n\n        SD = (n * G_CDF * (1-G_CDF))**0.5\n        height = abs(F_CDF - E_CDF)\n        if XY_Sorted[ii+1] != XY_Sorted[ii]: \n            if SD>0: \n                Res = Res + (height\/SD)**power\n\n    AD_Dist = Res\n    \n    return AD_Dist\n\nif __name__ == '__main__':\n\n    XX = np.random.normal(1, 1, 1000)\n    YY = np.random.normal(3, 1, 1000)\n    \n    Dist = Anderson_Darling_Dist(XX, YY)\n    \n    print(Dist)","a046c01c":"def Wasserstein_Dist(XX, YY):\n  \n    import numpy as np\n    nx = len(XX)\n    ny = len(YY)\n    n = nx + ny\n\n    XY = np.concatenate([XX,YY])\n    X2 = np.concatenate([np.repeat(1\/nx, nx), np.repeat(0, ny)])\n    Y2 = np.concatenate([np.repeat(0, nx), np.repeat(1\/ny, ny)])\n\n    S_Ind = np.argsort(XY)\n    XY_Sorted = XY[S_Ind]\n    X2_Sorted = X2[S_Ind]\n    Y2_Sorted = Y2[S_Ind]\n\n    Res = 0;\n    E_CDF = 0;\n    F_CDF = 0;\n    power = 1;\n\n    for ii in range(0, n-2):\n        E_CDF = E_CDF + X2_Sorted[ii]\n        F_CDF = F_CDF + Y2_Sorted[ii]\n        height = abs(F_CDF-E_CDF)\n        width = XY_Sorted[ii+1] - XY_Sorted[ii]\n        Res = Res + (height ** power) * width;  \n\n    WS_Dist = Res\n    \n    return WS_Dist\n\nif __name__ == '__main__':\n\n    XX = np.random.normal(1, 1, 1000)\n    YY = np.random.normal(3, 1, 1000)\n    \n    Dist = Wasserstein_Dist(XX, YY)\n    \n    print(Dist)","89265155":"The topic of this story can be considered as \u201crobustness to distributional shift\u201d issue in AI safety. Based on the following figure, the SafeML approach that we are going to explain includes Safe Machine Learning (SafeML) and Safe Deep Learning (SafeDL).\n\n![Left: Artificial Intelligence (AI) vs. Machine Learning (ML) vs. Deep Learning (DL). Right: Safe Artificial Intelligence (SafeAI) vs. Safe Machine Learning (SafeML) vs. Deep Learning (SafeDL).](https:\/\/miro.medium.com\/max\/1800\/1*PxdhSo2acWe0wn_XgFlWzA.png)","7bd4877a":"## 3.4 Cramer-Von Mises Distance\n\nECDF is formed by many small steps. Consider the absolute difference of two steps in the same interval as height. If we calculate the summation of all calculated height values for all steps, then we have the Cramer-Von Mises Distance.\n\n![Cramer-Von Mises distance measure illustration](https:\/\/cdn-images-1.medium.com\/max\/560\/1*8h6_9ePE7PSvzqFuuUAgNg.png)\n\nYou can check the sample python code for Cramer-Von Mises Distance as follows:","fe35ee42":"<a id = \"conclusion\"><\/a>\n## 4. Conclusion\n\nIn this story, the topic of AI safety has briefly introduced and the idea of SafeML has explained with some possible applications. Some of the well-known ECDF-based distance measures algorithms have been provided with their simple python example. In our next stories, the above-mentioned applications of SafeML will be provided with code implementation. Each of the above-mentioned ECDF-based approaches works well for a certain class of system. Thus, the relation between the system\u2019s characteristics and ECDF-based distance measures will be discussed in the next stories.\n\nThe SafeML is still in an early stage of development and it is aimed to extend it for dealing with time-series data, prediction and regression algorithms (i.e. Schulam, P., et al. (2019)) and domain adaptation (i.e. Shen, J., et al. (2018)). It is also possible to use SafeML as Explainable AI that will be discussed later. It should be mentioned that in parallel with \u201cAI Safety\u201d research, there are some other research works focusing on the application of AI for improving safety models (Gheraibia, Y., et al. (2019)).\n\n","c56db506":"<a id=\"github_project\"><\/a>\n## Related GitHub Projects\n\n[SafeML Project](https:\/\/github.com\/ISorokos\/SafeML): The idea that has been briefly explained in this story.\n\n[NN-Dependability-KIT Project](https:\/\/github.com\/dependable-ai\/nn-dependability-kit): Toolbox for software dependability engineering of artificial neural networks.\n\n[Confident-NN Project](https:\/\/github.com\/cfinlay\/confident-nn): Toolbox for empirical confidence estimation in neural networks-based classification.\n\n[SafeAI Project](https:\/\/eth-sri.github.io\/research\/safeai): Different toolboxes like DiffAI, DL2 and ERAN from SRILab ETH Z\u00fcrich focusing on robust, safe and interpretable AI.","4dc9828d":"The above python codes are also available on Google Colab. If one uses R for Programming the twosamples library is suggested. The above Python codes have been rewritten from this library. For MATLAB users, a set of ECDF-based distance measures functions are recommended.\n\nTo see some examples and case studies with SafeML idea please check the following GitHub project:\n\nhttps:\/\/github.com\/ISorokos\/SafeML\n\nMore details about SafeML is available in our recent paper [[arXiv](https:\/\/arxiv.org\/abs\/2005.13166v1)].","ea2b5d93":"<a id=\"Safeml_idea\"><\/a>\n## 2. SafeML Idea\n\nSafeML idea has been proposed by (Aslansefat et al., 2020-b), and the goal was to monitor the decisions of the classifiers when there is no available label.\n\nThe following figure demonstrates the flowchart of the SafeML idea. In this flowchart, there are two main sections including training phase and application phase.\n\n![Flowchart of SafeML (Aslansefat et al. 2020-b)](https:\/\/miro.medium.com\/max\/1832\/1*iIpJOHOL1GWrp234jfYW0g.png)\n\nA) The training phase is an offline procedure in which a trusted or certified dataset will be used to train the intelligent algorithm that can be a machine learning or deep learning algorithm. Thus, using a trusted dataset the classifier will be trained and its performance will be measured with existing KPIs (e.g. ROC, Accuracy and Kappa). Meanwhile, the statistical parameters and distributions of each class will be estimated and stored to be used for comparison (e.g. Mean values, Variance Values, and the Empirical Cumulative Distribution Functions (ECDFs)).\n\nB) The application phase is an online procedure in which real-time and unlabelled data is going to be feed to the system. For example, consider a security attack detector that has been trained to detect different security attacks and it should filter the attacker IPs. Therefore, in the application phase, the trained classifier should distinguish between the normal network traffic and the security attacks (classification task). One important and critical issue in the application phase is that the data does not have any label. So, it cannot be assured that the classifier can operate as accurate as of the training phase.\n\nIn the application phase, the buffered data will be separated according to the classifier decision (that is not certified) and the statistical parameters of each class will be stored to be compared with the one in trained phased. Using the statistical distance measures that will be explained in the next section, the distance between features for each class in the trained phase and application phase will be compared. If the calculated distance and expected confidence (defined by an expert) difference was very low, the classifier results and its accuracy can be trusted (the system is acting autonomously), if the difference was low, the system can ask for more data and re-evaluation to make sure about the distance. In case of larger difference, the classifier results and accuracy are no longer valid, and the system should use an alternative approach or notify a human agent (In this example, the system will ask the responsible security agent to check the network traffic manually and make a decision).\n\n![Application of SafeML in security intrusion detection (e.g. datasets like CICIDS2017)](https:\/\/miro.medium.com\/max\/3642\/1*qBLu6THDIyIJueq5BBKjng.png)\n\nThere are countless applications of Machine Learning and Deep Learning for early detection or diagnosis in various kinds of disease. For example, (Scudellari, S. (2020)) wrote about \u201cHospitals Deploy AI Tools to Detect COVID-19 on Chest Scans\u201d. Can this AI tools be fully autonomous in detection or diagnosis? Are they Safe? What is our definition of their safety? It is believed that SafeML or similar approaches can be a possible answer for those questions. The following figure illustrates the application of SafeML in medical diagnosis (e.g. COVID-19 diagnosis using lung scans).\n\n![Application of SafeML in medical diagnosis (e.g. COVID-19 Diagnosis using Lung Scans)](https:\/\/miro.medium.com\/max\/3642\/1*3kWKBUjjfUMZ8CdBPPYKVw.png)\n\nAnother example can be a traffic sign detector in an autonomous car or self-driving vehicle that uses Machine Learning or Deep Learning to detect the traffic sign and generate the required action(s). The following block diagram shows how SafeML can be used in this case study. This can be used also for autonomous platoon systems (Kabir, S., et al. (2020)).\n\n![Application of SafeML in self-driving cars (e.g. Traffic Sign Detection) [Car vector created by freepik \u2014 www.freepik.com]](https:\/\/miro.medium.com\/max\/3642\/1*gX5xxr3kgM43c8O552vJGg.png)\n\nEach of the above-mentioned applications will be implemented using SafeML with some well-known case studies in our next stories (Part II, Part III and Part IV).","479d6595":"## 3.1 Chernoff and Bhattacharyya based Upper Bound Error\n\nThe following python example of upper bound error probability estimation based on Chernoff method is provided. In this code, if one considers \u201cs = 0.5\u201d, then it will be the Bhattacharyya upper bound error estimation.","c1141c42":"<a id=\"references\"><\/a>\n## References\n\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & Man\u00e9, D. (2016). Concrete problems in AI safety. arXiv preprint arXiv:1606.06565.\n\nAslansefat, K., Gogani, M. B., Kabir, S., Shoorehdeli, M. A., & Yari, M. (2020-a). Performance evaluation and design for variable threshold alarm systems through semi-Markov process. ISA transactions, 97, 282\u2013295. https:\/\/doi.org\/10.1016\/j.isatra.2019.08.015\n\nAslansefat, K., Sorokos, I., Whiting, D., Kolagari, R. T., & Papadopoulos, Y. (2020-b). SafeML: Safety Monitoring of Machine Learning Classifiers through Statistical Difference Measure. arXiv preprint arXiv:2005.13166.\n\nBalunovic, M., Baader, M., Singh, G., Gehr, T., & Vechev, M. (2019). Certifying Geometric Robustness of Neural Networks. In Advances in Neural Information Processing Systems (pp. 15287\u201315297).\n\nGehr, T., Mirman, M., Drachsler-Cohen, D., Tsankov, P., Chaudhuri, S., & Vechev, M. (2018, May). Ai2: Safety and robustness certification of neural networks with abstract interpretation. In IEEE Symposium on Security and Privacy (SP) (pp. 3\u201318). https:\/\/doi.org\/10.1109\/SP.2018.00058\n\nGulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A. C. (2017). Improved training of Wasserstein gans. In Advances in neural information processing systems (pp. 5767\u20135777).\n\nGheraibia, Y., Kabir, S., Aslansefat, K., Sorokos, I., & Papadopoulos, Y. (2019). Safety+ AI: A Novel Approach to Update Safety Models Using Artificial Intelligence. IEEE Access, 7, 135855\u2013135869. https:\/\/doi.org\/10.1109\/ACCESS.2019.2941566\n\nKabir, S., Sorokos, I., Aslansefat, K., Papadopoulos, Y., Gheraibia, Y., Reich, J., \u2026 & Wei, R. (2019, October). A Runtime Safety Analysis Concept for Open Adaptive Systems. In International Symposium on Model-Based Safety and Assessment (pp. 332\u2013346). Springer, Cham. https:\/\/doi.org\/10.1007\/978-3-030-32872-6_22\n\nScudellari, S. (2020) Hospitals Deploy AI Tools to Detect COVID-19 on Chest Scans, IEEE Spectrum.\n\nSchulam, P., & Saria, S. (2019). Can you trust this prediction? Auditing pointwise reliability after learning. arXiv preprint arXiv:1901.00403.\n\nShen, J., Qu, Y., Zhang, W., & Yu, Y. (2018, April). Wasserstein distance guided representation learning for domain adaptation. In Thirty-Second AAAI Conference on Artificial Intelligence.\n\n","fe4457fd":"<a id = \"introduction\"><\/a>\n## 1. Introduction\n\nArtificial intelligence (AI) is growing rapidly and its applications dominating many different subjects. In parallel with this rapid growth, there are increasing concerns about AI safety. For safety-critical systems where human life, the environment, finances and privacy are at risk, the AI safety cannot be neglected. (Amodei et al., 2016) have discussed  issues in certifying modern machine learning systems operating in the field. As shown in the following figure, safety issues can be categorised into five categories including A) Safe exploration, B) Robustness to distributional shift, C) Avoiding negative side effects, D) Avoiding \u201creward hacking\u201d and \u201cwire heading\u201d, E) Scalable oversight. \n\n![Five main categories of AI Safety](https:\/\/miro.medium.com\/max\/1706\/1*ZvPJNCKd4cEajc3SYZaaMg.png)","a81d30ec":"\n\n*This is a story about estimating the accuracy of Machine Learning\/Deep Learning classifiers measuring using statistical distance measures to measure distributional shifts in inputs (SafeML, and SafeDL)*\n\n    Koorosh Aslansefat & Yiannis Papadopoulos \n    \n    Dependable Intelligent Systems (DEIS) Research Group \n    Department of Computer Science & Technology\n    University of Hull\n\n*The SafeML\/SafeDL techniques are developed at the University of Hull in collaboration with Fraunhofer IESE and Nuremberg Institute of Technology*\n\n![SafeML logo from: https:\/\/github.com\/ISorokos\/SafeML](https:\/\/miro.medium.com\/max\/700\/1*H0lN2Q9lmSRgfaGj9VqqGA.png)\n\n## Table of contents \n2. [Introduction](#introduction)\n3. [SafeML Idea](#Safeml_idea)\n4. [Statistical Distances](#section-three)\n5. [Conclusion](#conclusion)\n6. [References](#references)\n7. [Related GitHub Projects](#github_project)","383a8bdf":"It can be proved that the probability of error is correlated with the distance between the cumulative distribution functions (CDFs) (Aslansefat, K. et al. 2020-b). Some of the well-known CDF-based distance measures can be listed as follows:\n\n![Some of the well-known CDF-based distance measures](https:\/\/miro.medium.com\/max\/1856\/1*dKycNPEDOQUNDSuR4YRG9Q.png)\n\nSometimes it can be easier to use the empirical cumulative distribution function (ECDF) of features. Python examples of ECDF-based distance measures are provided as follows.","9be47b9e":"In section 2, the idea of SafeML is discussed briefly, and section three addresses the application of statistical difference measures with a python example. A short conclusion is provided in section 5. Some of the related medium posts and Github projects are suggested at the end of the story.","9a656784":"## 3.5 Anderson-Darling Distance\n\nThe Anderson-Darling distance is similar to the Cramer-Von Mises Distance. The only difference is that Anderson-Darling normalizes height values by their standard deviation (SD). Please check the following python example for Anderson-Darling distance.","58dc2b9b":"## 3.3 Kuiper Distance\n\nThe Kuiper distance has similar functionality to the Kolmogorov-Smirnov distance. However, this method considers two maximum distance as shown below; a) when blue ECDF has a larger value than red ECDF and b) when red ECDF has a larger value than blue ECDF. The Kuiper distance can be obtained by adding two maximum values.\n\n![Kuiper distance measure illustration](https:\/\/miro.medium.com\/max\/1120\/1*BQFEZR1hA9Ll_OVq-jPGcQ.png)\n\nAn example of Kuiper distance measure in python is provided as follows.","b43de3c1":"<a id=\"section-three\"><\/a>\n## 3. Statistical Distances and their Potential Applications in Accuracy Estimation\n\nA threshold line can be considered as the simplest version of a classifier. Consider the following figure; in this simple classifier any point bellow the threshold line (Xtp) will be considered as class 1 and any point above the threshold line will be counted as class 2. Assume that we know that the points between time 0 to 20 are class 1 and other points are class 2. As can be seen in this simple example, x and v points are misclassified.\n\n![A hypothetical signal and a simple threshold of Xtp (Aslansefat et al., 2020-a)](https:\/\/miro.medium.com\/max\/1176\/1*K_ZVaVXRN8XawL6jWOmUfg.png)\n\nIf we estimate the probability density function of each class (the following figure), then the probability of error can be calculated as:\n\n$$P\\left(error\\right)\\ =\\ P\\left(x\\in\\ R_1,Class\\ 1\\right)+P\\left(x\\in R_2,Class\\ 2\\right) \\\\ =\\int_{R_1} P\\left(x|Class\\ 1\\right)P\\left(Class\\ 1\\right)dx+\\int_{R_2} P\\left(x|Class\\ 2\\right)P\\left(Class\\ 2\\right)dx$$\n\nThe classifier accuracy can be easily obtained using 1-P(error). In this simple classifier, the area that two probability density function merge causes the error.\n\n![Probability density functions of class 1 and class 2](https:\/\/miro.medium.com\/max\/992\/1*0CHUkdkKothRBKKP7M7-XQ.png)\n\nFukunaga, K. (1990) showed that the upper bound error can be calculated using probability density function (PDF)-based distances like Bhattacharyya distance. The PDF-based distance measures usually rely on mean and variance distance as shown in the following figure. However, some exiting advanced methods can also compare the shape of different PDFs.\n\n![Distance measures based on probability density functions (PDFs)](https:\/\/cdn-images-1.medium.com\/max\/1024\/1*q_2Hhg5yJ-n6I8FWThhDog.png)\n\nThe following figure shows four well-known PDF-based distance measure methods.\n\n![Some of the well-known PDF-based distance measures](https:\/\/miro.medium.com\/max\/1824\/1*bE1k03PA2dfl7N_IIWzONg.png)","8bd15a99":"There are different approaches for increasing the safety and robustness of ML algorithms. Some papers investigate the uncertainty evaluation of results in a classifier while others focus on the improvement of robustness against uncertainties. As an example, the following figure shows the ETH Robustness Analyzer for Neural Networks (ERAN) that uses possible perturbations for input \u201c8\u201d and tries to create a shape that abstracts all possible outputs. If the created shape violates the defined boundary and the results cannot be certified. Otherwise, the outputs will be guaranteed. For more details please check (Gehr, T., et al. 2018 and Balunovic, M., et al. 2019).\n\n![](https:\/\/miro.medium.com\/max\/4998\/1*eMUbL2a2Xb3iD7kNLIkFYw.png)","393de531":"## 3.6 Wasserstein Distance\n\nWasserstein distance has been used in many applications. For example, it has been used as a loss function in generative adversarial neural networks (GANs) (Gulrajani, I. 2017). The Wasserstein distance we consider both height values and width values for all steps. This method somehow measures the area between two ECDFs when we consider power equal to one. When the power factor is one, Wasserstein distance is equal to Earth Mover Distance.\n\n![Wasserstein distance measure illustration](https:\/\/miro.medium.com\/max\/1120\/1*o6TdftuIWMgmYbbRVecHWw.png)\n\nYou can check the following python example of Wasserstein distance.","4907051e":"## 3.2 Kolmogorov-Smirnov Distance\n\nConsider we have a dataset with two classes and one feature. The following figure shows the ECDF of the feature for class 1 (blue) and class 2 (red). The Kolmogorov-Smirnov simply finds the maximum exiting distance between two ECDFs.\n\n![Kolmogorov-Smirnov distance measure Illustration](https:\/\/miro.medium.com\/max\/1120\/1*UZf7od5wagCkfO2egPPMVQ.png)\n\nAs an example, you can check the following python code for Kolmogorov-Smirnov distance measure:"}}