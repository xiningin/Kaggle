{"cell_type":{"bc63afe4":"code","309eae00":"code","0e0f7494":"code","d7d7d631":"code","c1dbb480":"code","6302bda9":"code","091cafd7":"code","eda245a7":"code","383c0165":"code","7fb07c25":"code","ff94fd15":"code","4ee2251b":"code","6726c651":"code","bddd45ce":"code","23e7a092":"code","d7ef5833":"code","c9e205ab":"code","18308272":"code","6a9785f6":"code","3140d792":"code","02b1bff6":"code","910d030e":"code","9e0b9ab7":"code","36780b7b":"code","e7be7a6b":"code","bf3c8e9e":"markdown","8c13f793":"markdown","400cffd7":"markdown","c16172eb":"markdown","ec9c6772":"markdown","149b2362":"markdown","1693314d":"markdown","e4d05217":"markdown","9aaa0b6d":"markdown","70699070":"markdown","56780c02":"markdown","e363a6ae":"markdown"},"source":{"bc63afe4":"!pip -q install transformers","309eae00":"import numpy as np\nimport pandas as pd\nimport os\nimport transformers\nimport torch\nfrom tqdm.notebook import tqdm","0e0f7494":"df_train = pd.read_csv('\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')\ndf_train.head()","d7d7d631":"from transformers import AutoModelForQuestionAnswering, AutoTokenizer,TrainingArguments, Trainer,default_data_collator","c1dbb480":"#pt_name = \"deepset\/xlm-roberta-large-squad2\"\npt_name = \"..\/input\/huggingface-question-answering-models\/multilingual\/xlm-roberta-large-squad2\"\n\ntokenizer = AutoTokenizer.from_pretrained(pt_name)\nmodel = AutoModelForQuestionAnswering.from_pretrained(pt_name)","6302bda9":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","091cafd7":"model = model.to(device)","eda245a7":"article = df_train.context.values\nquestion = df_train.question.values","383c0165":"df_train.iloc[0]","7fb07c25":"temp_inputs = tokenizer.encode_plus(question[0], article[0], add_special_tokens=True, return_tensors=\"pt\")\ntemp_inputs","ff94fd15":"temp_inputs['input_ids']","4ee2251b":"tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(temp_inputs['input_ids'][0]))","6726c651":"def predict(question, context):\n    lines = context.split('\\n')\n    max_score = -999999\n    ans_list = []\n    max_ans = ''\n    for line in lines:\n        if len(line) > 800:\n            line = line[:800]\n        # Encode data\n        inputs = tokenizer.encode_plus(question, line, add_special_tokens=True, return_tensors=\"pt\").to(device)\n        input_ids = inputs[\"input_ids\"].tolist()[0] \n        text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n        # Prediction\n        with torch.no_grad():\n            pred = model(**inputs)\n        # Find socre\n        score = torch.max(pred['start_logits']).cpu() + torch.max(pred['end_logits']).cpu()\n        startkey = torch.argsort(pred['start_logits'],descending=True).cpu().int()[0]\n        endkey =  torch.argsort(pred['end_logits'],descending=True).cpu().int()[0]\n        # Answer is start index to end index\n        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[startkey[0]:endkey[0]+1]))\n        ans_list.append(answer)\n        #print(score)\n        if score > max_score and answer != '<s>' and answer != '<UNK\/>' :\n            #print(score)\n            max_score = score\n            max_ans = answer\n    return max_ans","bddd45ce":"answers_list = []\nmodel.eval()\nfor i in tqdm(range(len(question))):\n    answer = predict(question[i], article[i])\n    answers_list.append(answer)","23e7a092":"sum(df_train['answer_text'] == answers_list)\/len(answers_list)","d7ef5833":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","c9e205ab":"jl = []\nfor s1,s2 in zip(df_train['answer_text'],answers_list):\n    jl.append(jaccard(s1,s2))","18308272":"sum(jl)\/len(jl)","6a9785f6":"df_test = pd.read_csv('\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')\ndf_test.head()","3140d792":"len(df_test)","02b1bff6":"article = df_test.context.values\nquestion = df_test.question.values","910d030e":"answers_list = []\nmodel.eval()\nfor i in tqdm(range(len(question))):\n    answer = predict(question[i], article[i])\n    answers_list.append(answer)","9e0b9ab7":"df_test[\"PredictionString\"] = answers_list","36780b7b":"df_test","e7be7a6b":"df_test[[\"id\",\"PredictionString\"]].to_csv('submission.csv',index=False)","bf3c8e9e":"### Jaccard score","8c13f793":"## Read data csv","400cffd7":"##  Prediction\nI split long article by line (\"\\n\") and use answer from line that return maximum score  \n\nremake this function is pretty slow because batch size is 1","c16172eb":"## Tokenizer example","ec9c6772":"## Read Pretrained model\nNormally we can download pretrain from pretrained name ( https:\/\/huggingface.co\/models ) ex. **\"deepset\/xlm-roberta-large-squad2\"**  \nBut in this competition we cant use Internet while summiting so I used the model that downloaded in kaggle dataset (https:\/\/www.kaggle.com\/c\/chaii-hindi-and-tamil-question-answering\/discussion\/266015) (https:\/\/www.kaggle.com\/sauravmaheshkar\/huggingface-question-answering-models)","149b2362":"# Use huggingface transformers pretrained QA ","1693314d":"Encode data to bert QA format   \n![](https:\/\/www.researchgate.net\/profile\/Hussein-Mozannar\/publication\/333773105\/figure\/fig2\/AS:769535128903681@1560482880271\/Architecture-of-our-open-domain-question-answering-system-SOQAL-BERT-illustration-is.ppm)","e4d05217":"Decoded token -> text","9aaa0b6d":"## Install Lib","70699070":"example data","56780c02":"### Exact Match score","e363a6ae":"## Test data"}}