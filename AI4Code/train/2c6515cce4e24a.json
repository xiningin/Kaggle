{"cell_type":{"d6d451fb":"code","7936a828":"code","1fbabf4a":"code","b3f4b3a1":"code","58d948dd":"code","d6580eb1":"code","a2e570eb":"code","4086deec":"code","05ee83b5":"code","13527b32":"code","bbb29f0d":"code","c8b21f12":"code","8bd4e365":"code","98504a01":"code","8d7eb1ad":"code","a85c6a5c":"code","9f118670":"code","41772d14":"code","793e08bc":"code","bc240dec":"code","2e8e9ee8":"code","becee156":"code","1cf7bc02":"markdown","bece711b":"markdown","a876d080":"markdown","966dd354":"markdown","89bdfd94":"markdown","17495655":"markdown","12e03ec0":"markdown","fddd1536":"markdown","7e40a8bd":"markdown"},"source":{"d6d451fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\ninit_notebook_mode(connected=True)\n\nimport os\nprint(os.listdir(\"..\/input\"))\nsns.set_style('darkgrid')\n# Any results you write to the current directory are saved as output.","7936a828":"os.listdir(\"..\/input\/csvs_per_year\/csvs_per_year\/\")\nmyDf = pd.read_csv('..\/input\/csvs_per_year\/csvs_per_year\/madrid_2016.csv')\n","1fbabf4a":"print (\"IDs of Madrid air monitoring stations\")\nprint(*myDf['station'].unique())","b3f4b3a1":"# Fixing datatypes\nstation1 = myDf[myDf['station'] == 28079008].dropna()\nstation1 = station1[['date', 'PM10']]\nstation1['date'] = pd.to_datetime(station1['date'])","58d948dd":"station1.sort_values(['date'], inplace = True)\nflag = False\nfor i in range (1, len(station1['date'])):\n    if station1['date'].iloc[i-1]  >  station1['date'].iloc[i]:\n        print (\" Erranious at {}\".format(i))\n        flag = True\nif flag == False:\n    print(\"All Good\")","d6580eb1":"\ndatetime_trace = go.Scatter(\n    x = station1['date'],\n    y = station1['PM10']\n)\n\ndata = [datetime_trace]\niplot(data)\n","a2e570eb":"from keras.models import Sequential\nfrom keras.layers import Dense","4086deec":"station1.columns","05ee83b5":"myDataset = pd.DataFrame(station1['PM10'])\nDataset = myDataset.values\nDataset = Dataset.astype('float32')","13527b32":"train_size = int(len(Dataset) * 0.67)\ntest_size = len(Dataset) - train_size\ntrain, test = Dataset[0:train_size,:], Dataset[train_size:len(Dataset),:]\nprint(len(train), len(test))","bbb29f0d":"def createDataset(data, look_back = 12):\n    dataX, dataY = [], []\n    for i in range(len(data) - look_back - 1):\n        a = data[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(data[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)","c8b21f12":"look_back = 12\ntrainX, trainY = createDataset(train, look_back)\ntestX, testY = createDataset(test, look_back)\n#trainY","8bd4e365":"# Making a multilayer perceptron\nmodel = Sequential()\nmodel.add(Dense(8, input_dim = look_back ,activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(loss = 'mean_squared_error', optimizer= 'adam')\nmodel.summary()\n","98504a01":"model.fit(trainX, trainY, epochs = 100, batch_size = 2, verbose = 2)","8d7eb1ad":"import math\ntrainScore = model.evaluate(trainX, trainY, verbose = 0)\nprint('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, math.sqrt(trainScore)))\ntestScore = model.evaluate(testX, testY, verbose=0)\nprint('Test Score: %.2f MSE (%.2f RMSE)' % (testScore, math.sqrt(testScore)))","a85c6a5c":"trainPred = model.predict(trainX)\ntestPred = model.predict(testX)\n#test","9f118670":"testPred.flatten()\nvisGraph = pd.DataFrame({'True': testY, 'Predicted':testPred.flatten()})","41772d14":"Actual_level = go.Scatter(\n    x = visGraph.index,\n    y = visGraph['True']\n)\nPredicted_level = go.Scatter(\n    x = visGraph.index,\n    y = visGraph['Predicted']\n)\n\ndata = [Actual_level, Predicted_level]\niplot(data)","793e08bc":"def myAccuracyTracker(predictions, truth, acceptance = 5):\n    totals = len(predictions)\n    wrongs = 0\n    for i in range(totals):\n        if abs(truth[i] - predictions[i] > acceptance):\n            wrongs += 1\n    return (totals-wrongs)\/totals","bc240dec":"accuracy_array = []\nfor acceptance in range(0, 21):\n    accuracy_array.append(myAccuracyTracker(testPred.flatten(), testY, acceptance) * 100)\naccuracy_array = np.array(accuracy_array)\n\n","2e8e9ee8":"fig2 = plt.figure(figsize=(15,5))\nplt.plot(accuracy_array)\nplt.xlabel('Acceptance  Value')\nplt.ylabel('Accuracy')\n","becee156":"trace_new = go.Scatter(\n        y = accuracy_array,\n        x = np.array(range(21)),\n        name = \"Accuracy Curve\"\n)\nlayout = dict(title = 'Accuracy and Acceptance Payoff',\n              xaxis = dict(title = 'Accuracy (in percentage)'),\n              yaxis = dict(title = 'Acceptance (in PPM)'),\n              )\nmydata = [trace_new]\nmeow = dict(data = mydata, layout= layout)\niplot(meow)","1cf7bc02":"# Making our personal multilayer perceptron!","bece711b":"As you can see, here is a perfect date and PM10 level stats side by side","a876d080":"# A simple accuracy rundown varying with the error acceptance\n\nThis should be exponentially increasing ( as it seems intuitive)","966dd354":"# Aim of the kernel : Analysing the PM10 levels through the course of 2016 and making a predictive model\n\n![The Royal Palace](https:\/\/www.esmadrid.com\/sites\/default\/files\/styles\/content_type_full\/public\/recursosturisticos\/infoturistica\/palacio_real.jpg?itok=CW_Pl9fz)","89bdfd94":"# Training is completed!\nNow since the training is completed, why not analyse the data inset and check the accuracy for the same!","17495655":"# Checking out the number of stations\n\nAs we can see that there are many different stations spread around Madrid for the sole purpose of air quality monitoring, but if you explore a little deeper, not every station records the status of **PM10** through the course of the day. That's why, after exploration of the data, I have decided to consider station number **28079008** for this analysis","12e03ec0":"Now we can define a function to create a new dataset as described above. The function takes two arguments, the dataset which is a NumPy array that we want to convert into a dataset and the look_back which is the number of previous time steps to use as input variables to predict the next time period, in this case, defaulted to 1.\n\nThis default will create a dataset where X is the PM10 level at a given time (t) and Y is the PM10 at the next time (t + 1).","fddd1536":"Making our training and testing datasets ","7e40a8bd":"# Making sure there are no anomilies and everything is sorted to our liking"}}