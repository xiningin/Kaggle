{"cell_type":{"ac20d7a0":"code","7ade0807":"code","afec68c0":"code","3d25609f":"code","45d10db7":"code","d5d51ca8":"code","c12e3bd7":"code","068b6829":"code","bc08d843":"code","4c49f9cf":"code","5daf9137":"code","a118f071":"code","513514b1":"code","4c42b85e":"code","9d636533":"code","2cf67512":"code","bb468fcb":"code","6ae476af":"code","501cc452":"code","5b389ee5":"markdown","5e127f9f":"markdown","008c480c":"markdown","254f73e5":"markdown","88013006":"markdown","12b54122":"markdown","69b72cbb":"markdown","a4262cd5":"markdown","7ebcca77":"markdown","88efd6e8":"markdown","726e6855":"markdown","db725ae3":"markdown","9f4e2209":"markdown","43694d28":"markdown","7c59f928":"markdown","7989819a":"markdown","ba668db4":"markdown","0375fda1":"markdown","a6fd3206":"markdown","3af37ea4":"markdown","6112116c":"markdown","09a031e6":"markdown","918f3efa":"markdown","8f937fc8":"markdown","3b0ebb1e":"markdown","35077a06":"markdown","80f227ad":"markdown","16abc797":"markdown","292bb786":"markdown","16941353":"markdown","4cc2d98b":"markdown","7d23225a":"markdown","5d85fa93":"markdown","d698d91f":"markdown","c7daefb0":"markdown","9a576dec":"markdown","e46a8654":"markdown","44d54f7e":"markdown","2b2fab69":"markdown","083bbb29":"markdown","0e3d2e0f":"markdown"},"source":{"ac20d7a0":"# Packages\n# -----------------------------------------------------------------------------\n\n# Install\n!pip install pyjsparser\n!pip install js2xml\n\n# Base\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport plotly.express as ex\n\n# Web Scraping\nimport requests\nfrom bs4 import BeautifulSoup\n# !pip install pyjsparser\n# from pyjsparser import parse\nimport pyjsparser\n\n# Time Sleep\nimport sys\nimport time\nfrom datetime import datetime\nfrom termcolor import colored\n\n# GC\nimport gc\n\n# Itertools\nimport itertools\n\n# Grafikten Data \u00c7ekmek i\u00e7in\nimport re\nimport js2xml\nfrom itertools import repeat    \nfrom pprint import pprint as pp\n\n# String Manipulation\nimport re \n\n# Configurations\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.set_option('display.max_columns', None)\n\n# Helpers\n# -----------------------------------------------------------------------------\n# To learn process time\ndef get_time(cond):\n    if cond == \"start\":\n        p = \"This process started at \"\n    elif cond == \"end\":\n        p = \"The process completed at \"\n    print(\"\") \n    print(colored(p + str(datetime.now().strftime(\"%H:%M:%S\")), \"green\", \"on_white\", attrs=[\"bold\",'reverse', 'blink']))\n    \n    \ndef Filter(string, substr): \n        return [str for str in string if\n                any(sub in str for sub in substr)] \n    \ndef NOTFilter(string, substr): \n    return [str for str in string if\n            any(sub not in str for sub in substr)] ","7ade0807":"# Leagues & Seasons\nleagues = [\n    \"https:\/\/www.transfermarkt.com.tr\/premier-league\/startseite\/wettbewerb\/GB1\/saison_id\/\",\n    # \"https:\/\/www.transfermarkt.com.tr\/bundesliga\/startseite\/wettbewerb\/L1\/saison_id\/\",\n    # \"https:\/\/www.transfermarkt.com.tr\/laliga\/startseite\/wettbewerb\/ES1\/saison_id\/\",\n    # \"https:\/\/www.transfermarkt.com.tr\/serie-a\/startseite\/wettbewerb\/IT1\/saison_id\/\",\n    # \"https:\/\/www.transfermarkt.com.tr\/ligue-1\/startseite\/wettbewerb\/FR1\/saison_id\/\"\n]\n\ndef all_league_urls(url, season_range = [2017,2021]):\n    league_url = []\n    for i in url:\n        league_url.append(list(map(lambda x : i + str(x), np.arange(season_range[0], season_range[1]+1, 1))))\n    league_url  = list(itertools.chain(*league_url))\n    return league_url\n    \n\nleague_url = all_league_urls(leagues)\nleague_url","afec68c0":"def find_team_urls(league_urls):\n    # Teams\n    headers = {'User-Agent': 'Mozilla\/5.0 (X11; Linux x86_64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/47.0.2526.106 Safari\/537.36'}\n    team_url = []\n\n    for i in league_urls:\n        soup = BeautifulSoup(requests.get(i, headers=headers).content, \"html.parser\") \n        team_urls = soup.find(\"table\", class_ = \"items\").find_all(\"a\", {\"class\":\"vereinprofil_tooltip\"})\n        team_url.append(pd.Series(list(map(lambda x: \"https:\/\/www.transfermarkt.com.tr\" + x[\"href\"], team_urls))).unique().tolist())\n\n    team_url  = list(itertools.chain(*team_url))\n    return team_url\n\nteam_url = find_team_urls(league_url)\nprint(\"Number of historical urls for teams:\",len(team_url))\nteam_url[:10]","3d25609f":"def find_player_urls(team_url):\n    \n    get_time(\"start\")\n    tm_player_url = []\n    \n    for completed,i in enumerate(team_url):\n        headers = {'User-Agent': 'Mozilla\/5.0 (X11; Linux x86_64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/47.0.2526.106 Safari\/537.36'}\n        soup = BeautifulSoup(requests.get(i, headers=headers).content, \"html.parser\") \n        p = soup.find(\"table\", class_ = \"items\").find_all(\"a\", {\"class\":\"spielprofil_tooltip\"})\n        tm_player_url.append(pd.Series(list(map(lambda x: \"https:\/\/www.transfermarkt.com.tr\" + x[\"href\"], p))).unique().tolist())\n\n        # Print Message: How many teams have scraped?\n        sys.stdout.write(\"\\r{0} teams have just scraped from TM!\".format(completed+1))\n        sys.stdout.flush()\n\n    get_time(\"end\")\n    tm_player_url  = pd.Series(list(itertools.chain(*tm_player_url))).unique().tolist()\n    \n    \n    tm_player_url_df = pd.DataFrame({\"TMURL\":tm_player_url})\n    tm_player_url_df[\"TMId\"] = tm_player_url_df.TMURL.str.split(\"spieler\/\", expand = True)[1]\n\n    # import re \n    # def Filter(string, substr): \n    #    return [str for str in string if\n    #            any(sub in str for sub in substr)] \n\n    get_time(\"start\")\n    new_tm_player_url = []\n    for i in tm_player_url:\n        soup = BeautifulSoup(requests.get(i, headers=headers).content, \"html.parser\") \n        \n        # Kaleci is a Turkish word. It means Goal Keeper. It should change if there is a problem!\n        if len(Filter(list(map(lambda x: x.text, soup.find_all(\"span\", {\"class\":\"dataValue\"}))), [\"Kaleci\"])) < 1:\n            new_tm_player_url.append(i)\n\n    get_time(\"end\")\n\n    tm_player_url_df = pd.DataFrame({\"TMURL\":new_tm_player_url})\n    tm_player_url_df[\"TMId\"] = tm_player_url_df.TMURL.str.split(\"spieler\/\", expand = True)[1]\n    \n    return tm_player_url_df.drop_duplicates()\n    \n\n\n# An example of Chelsea Players in 2017 season. Attention: I chose one team!\ntm_player_url_df = find_player_urls(team_url[:1])\nprint(\"Number of urls for players:\",len(tm_player_url_df))\ntm_player_url_df","45d10db7":"def market_value(url):\n    \n    # Url & Player Id\n    url = url.replace(\"profil\", \"marktwertverlauf\")\n    playerid = url.split(\"spieler\/\")[1]\n    \n    # Request\n    headers = {'User-Agent': 'Mozilla\/5.0 (X11; Linux x86_64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/47.0.2526.106 Safari\/537.36'}\n    r = requests.get(url, headers=headers, timeout=1000)\n    soup = BeautifulSoup(r.content, \"html.parser\")\n    \n    # Name\n    name = soup.find(\"h1\").get_text()\n    \n    # Parsed \n    script = soup.find(\"script\", text=re.compile(\"Highcharts.Chart\")).text\n    parsed = pyjsparser.parse(script) \n    \n    # Find Data from parsed info\n    sc = parsed[\"body\"][26][\"expression\"][\"arguments\"][1][\"body\"][\"body\"][1][\"declarations\"][0][\"init\"][\"arguments\"][0][\"properties\"][5][\"value\"][\"elements\"][0][\"properties\"][2][\"value\"][\"elements\"]\n    mv = list(map(lambda x: int(x[\"properties\"][0][\"value\"][\"value\"]), sc))\n    club = list(map(lambda x: x[\"properties\"][1][\"value\"][\"value\"], sc))\n    date = list(map(lambda x: x[\"properties\"][4][\"value\"][\"value\"], sc))\n    \n    mv_df = pd.DataFrame({\"TMId\":playerid,\"Player\":name,\"MarketValue\":mv, \"Club\":club, \"Date\":date})\n    \n    # Date format is Turkish. We have to arrange format. If you adapt all code by using English urls, you can remove this\n    months = {\"Oca\":\"January\", \"\u015eub\":\"February\", \"Mar\":\"March\", \"Nis\":\"April\", \"May\":\"May\", \"Haz\":\"June\", \n              \"Tem\":\"July\", \"A\u011fu\":\"August\",\"Eyl\":\"September\", \"Eki\":\"October\", \"Kas\":\"November\", \"Ara\":\"December\"}\n    mv_df['Date']=pd.to_datetime(mv_df['Date'].replace(months, regex=True), dayfirst=True, errors = \"coerce\")\n    mv_df = mv_df[mv_df.Date.notnull()]\n    \n    return mv_df\n\n\n\n# Test\nmv = market_value(\"https:\/\/www.transfermarkt.com.tr\/mesut-ozil\/marktwertverlauf\/spieler\/35664\")\nmv","d5d51ca8":"ex.line(mv, x = \"Date\", y = \"MarketValue\",hover_name = \"Club\", title = \"Mesut \u00d6zil\")","c12e3bd7":"def injury_table(url):\n    # URL & PLAYER ID\n    url = url.replace(\"profil\", \"verletzungen\")\n    pid = url.split(\"spieler\/\")[1]\n    \n    # Request\n    headers = {'User-Agent': 'Mozilla\/5.0 (X11; Linux x86_64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/47.0.2526.106 Safari\/537.36'}\n    r=requests.get(url, headers = headers)\n    soup=BeautifulSoup(r.content, \"html.parser\")\n    \n    try:\n        \n        temp = pd.read_html(str(soup.find(\"table\", class_ = \"items\")))[0]\n        \n        try:\n            # Find page number\n            page = len(soup.find(\"div\", class_ = \"pager\").find_all(\"li\", class_ = \"page\")) \n            if page > 1:\n                for page_num in np.arange(2, page+1, 1):\n                    url2 = url + \"\/ajax\/yw1\/page\/\"+str(page_num)\n                    soup2 = BeautifulSoup(requests.get(url2, headers=headers).content, \"html.parser\")  \n                    temp_table2 = pd.read_html(str(soup2.find(\"table\", class_ = \"items\")))[0]\n                    temp = temp.append(temp_table2)\n            \n        except:\n            pass\n        \n        temp[\"TMId\"]=pid  \n        \n        return temp\n    \n    except:\n        pass\n    \n# Test\ninjury_table(\"https:\/\/www.transfermarkt.com.tr\/mesut-ozil\/profil\/spieler\/35664\") \n","068b6829":"def transfer_history(url):\n    playerid = url.split(\"spieler\/\")[1]\n    \n    headers = {'User-Agent': 'Mozilla\/5.0 (X11; Linux x86_64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/47.0.2526.106 Safari\/537.36'}\n    r = requests.get(url, headers=headers, timeout=1000)\n    soup = BeautifulSoup(r.content, \"html.parser\")\n    name = soup.find(\"h1\").get_text()\n    temp = pd.read_html(str(soup.find(\"div\", class_ = \"responsive-table\").find(\"table\")))[0]\n    temp = temp[temp.Sezon != \"Toplam transfer kazanc\u0131 :\"].drop([\"Unnamed: 10\", \"Veren kul\u00fcp\", \"Veren kul\u00fcp.1\", \"Alan kul\u00fcp\", \"Alan kul\u00fcp.1\"], axis = 1).rename({\"Veren kul\u00fcp.2\":\"VerenKulup\", \"Alan kul\u00fcp.2\":\"AlanKulup\"}, axis = 1)\n    temp[\"TMId\"] = playerid\n    temp[\"Player\"] = name\n    return temp\n\n# Test\ntransfer_history(\"https:\/\/www.transfermarkt.com.tr\/mesut-ozil\/profil\/spieler\/35664\") ","bc08d843":"def suspensions_absences(url):\n    url = url.replace(\"profil\", \"ausfaelle\")\n    playerid = url.split(\"spieler\/\")[1]\n    headers = {'User-Agent': 'Mozilla\/5.0 (X11; Linux x86_64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/47.0.2526.106 Safari\/537.36'}\n    \n    r=requests.get(url, headers = headers)\n    soup = BeautifulSoup(r.content, \"html.parser\")\n    \n    try:\n        temp = pd.read_html(str(soup.find(\"table\")))[0]\n        temp.columns = [\"Sezon\", \"EksikCeza\", \"Lig\", \"Baslangic\", \"Bitis\", \"Gun\", \"KacirilanOyun\"]\n\n\n        temp[\"Lig\"] = NOTFilter(list(map(lambda x: x[\"title\"], soup.find(\"table\").find_all(\"img\"))), [\"\\xa0\"])\n        temp[\"Gun\"] = pd.to_numeric(temp[\"Gun\"].apply(lambda x: str(x).replace(\"Tage\", \"\").strip()), errors=\"coerce\")\n\n        months = {\"Oca\":\"January\", \"\u015eub\":\"February\", \"Mar\":\"March\", \"Nis\":\"April\", \"May\":\"May\", \"Haz\":\"June\", \n                  \"Tem\":\"July\", \"A\u011fu\":\"August\",\"Eyl\":\"September\", \"Eki\":\"October\", \"Kas\":\"November\", \"Ara\":\"December\"}\n        \n        for i in [\"Baslangic\", \"Bitis\"]:\n            temp[i]=pd.to_datetime(temp[i].replace(months, regex=True), dayfirst=True, errors=\"coerce\")\n            \n        temp[\"TMId\"] = playerid\n        \n        return temp\n    \n    except:\n        pass\n\n    \n# Test \nsuspensions_absences(\"https:\/\/www.transfermarkt.com.tr\/mesut-ozil\/profil\/spieler\/35664\") ","4c49f9cf":"# England: 9 | Italy: 11 | Spain: 12 | France: 13 | Germany: 20\ndef fbref_league_history(league_id = [9,11,12,13,20], first_season = 2017):\n    history = []\n    for i in league_id:\n        comp_history_url = \"https:\/\/fbref.com\/en\/comps\/\" + str(i) + \"\/history\" \n        #print(comp_history_url)\n\n        r=requests.get(comp_history_url)\n        soup=BeautifulSoup(r.content, \"html.parser\")\n\n        # Tablodan t\u00fcm sezonun bilgilerine ula\u015f\u0131l\u0131r.\n        find_seasons = soup.find_all(class_ = \"left\")\n\n        all_seasons_url = []\n        for k in range(0, len(find_seasons)):\n            if find_seasons[k].get('data-stat') == \"season\":\n                temp = \"https:\/\/fbref.com\" + find_seasons[k].find_all(\"a\")[0].attrs[\"href\"]\n                all_seasons_url.append(temp)\n\n        history.append(all_seasons_url)\n        time.sleep(0.1)\n\n    # T\u00fcm historyler tek bir arrayde\n    history  = list(itertools.chain(*history))\n\n    seasons = list(map(lambda x: str(x)+\"-\"+str(x+1), np.arange(1950, first_season, 1)))\n    for i in seasons:\n        history = NOTFilter(history, [i])\n    del seasons\n\n    # history = NOTFilter(history, [\"2021-2022\"])\n\n    return history\n\n# Test for Big 5\nhistory = fbref_league_history(league_id = [9,11,12,13,20])\nhistory","5daf9137":"def fbref_team_url_history(league_history):\n    team_season_url = []\n    for league_season_url in league_history:\n        r=requests.get(league_season_url)\n        soup=BeautifulSoup(r.content, \"html.parser\")\n        teams = soup.find(\"table\").find_all(\"a\")\n        teams = list(map(lambda x: \"https:\/\/fbref.com\" + x[\"href\"], teams))\n        teams = Filter(teams, [\"\/en\/squads\/\"])\n        team_season_url.append(teams)\n\n    # T\u00fcm historyler tek bir arrayde\n    team_season_url  = list(itertools.chain(*team_season_url))\n    return team_season_url\n\n# Test for Premier League Last Season\nteam_season_url = fbref_team_url_history(history[:1])\nteam_season_url","a118f071":"def fbref_player_url(team_season_url):\n    player_url = []\n    for turl in team_season_url:\n        r=requests.get(turl)\n        soup=BeautifulSoup(r.content, \"html.parser\")\n        soup.find(\"div\", {\"id\":\"all_stats_standard\"})\n        players = soup.find(\"tbody\").find_all(\"a\")\n        players = list(map(lambda x: x[\"href\"], players))\n        players = Filter(players, [\"\/en\/players\/\"])\n        players = NOTFilter(players, [\"matchlogs\"])\n        player_url.append(list(map(lambda x: \"https:\/\/fbref.com\" + x, players)))\n        time.sleep(0.01)\n    player_url  = list(itertools.chain(*player_url))\n    return player_url\n\n\n# Test for Chelsea Players\nplayer_url = fbref_player_url(team_season_url[:1])\nplayer_url","513514b1":"def fbref_player_info(player_url):\n    player_info = []\n    for completed, i in enumerate(player_url):\n\n        # PlayerId\n        playerId = i.replace(\"https:\/\/fbref.com\/en\/players\/\", \"\").split(\"\/\")[0]\n\n        # Request\n        r=requests.get(i)\n        soup=BeautifulSoup(r.content, \"html.parser\")\n\n        # Meta\n        meta = soup.find(\"div\", {\"id\":\"meta\"})\n\n        # Player Name\n        playerName = soup.find(\"h1\").find(\"span\").get_text()\n        \n        # Nationality\n        birthplace = meta.find(\"span\", {\"itemprop\": \"birthPlace\"}).text.replace(\"\\n\", \"\").strip().split(\", \")\n        nationality = birthplace[len(birthplace)-1]\n        \n\n        # Player Photos\n        try:\n            photo = soup.find(\"div\", {\"class\":\"media-item\"}).find(\"img\").attrs[\"src\"]\n        except:\n            photo = np.nan\n\n\n        # Birth\n        try:\n            birth = meta.find(\"span\", {\"itemprop\": \"birthDate\"}).text.replace(\"\\n\", \"\").strip()\n            #soup.find(\"div\", {\"id\":\"meta\"}).find(\"span\", {\"id\":\"necro-birth\"})['data-birth']\n        except:\n            birth = np.nan\n\n        # Height\n        try:\n            height = meta.find(\"span\", {\"itemprop\":\"height\"}).text.replace(\"cm\", \"\")\n        except:\n            height = np.nan\n\n        # Weight\n        try:\n            weight = soup.find(\"div\", {\"id\":\"meta\"}).find(\"span\", {\"itemprop\":\"weight\"}).text.replace(\"kg\", \"\")\n        except :\n            weight = np.nan\n\n\n        detail = meta.find_all(\"p\")\n\n        # Player Full Name\n        if len(Filter([detail[0].text], [\"Position\", \"Club\", \"Born\", \"National Team\", \"Citizenship\"])) > 0:\n                playerFullName = np.nan\n        else:\n            playerFullName = detail[0].get_text()\n\n        # Position & Footed\n        fp = list(map(lambda x: str(x), detail))\n        position = Filter(fp, [\"Position\"])\n        footed = Filter(fp, [\"Footed\"])\n        if len(position) > 0:\n            position = position[0].split(\"<strong>\")[1].replace(\"Position:<\/strong>\",\"\").replace(\"\\n\", \"\").replace(\"<p>\", \"\").replace(\"<\/p>\", \"\").replace(\"\\xa0\", \"\").replace(\"\u25aa\", \"\").split(\"<span\")[0].strip()\n        else:\n            position = np.nan\n\n        if len(footed) > 0:\n            footed = footed[0].split(\"<strong>Footed:<\/strong>\")[1].split(\"<span\")[0].strip().replace(\"<\/p>\", \"\").upper()\n            footed = footed.split(\"% \")\n            if len(footed) > 1:\n                foot = footed[1]\n                foot_ability = int(footed[0]) \n            else:\n                foot = footed[0]\n                foot_ability = 100\n        else:\n            foot = np.nan\n            foot_ability = np.nan\n\n        # International Reputation\n        try:\n            ir = soup.find(\"ul\", {\"id\":\"bling\"}).find_all(\"a\")\n            ir = list(map(lambda x: x.text.strip(), ir))\n            ir = '||'.join(map(str, ir))  # De\u011fi\u015fken yap\u0131lacak iken || ile ayr\u0131lmal\u0131\n        except:\n            ir = np.nan\n            \n        #Social Media\n        sm = Filter(list(map(lambda x: x[\"href\"], meta.find_all(\"a\", href = True))), [\"twitter\", \"instagram\"])\n        try:\n            tw = Filter(sm, [\"twitter\"])[0]\n        except:\n            tw = np.nan\n        try:\n            ins = Filter(sm, [\"instagram\"])[0]\n        except:\n            ins = np.nan\n\n        # Data Frame\n        temp = pd.DataFrame({\n            \"FBRefId\":[playerId],\n            \"PlayerName\":[playerName],\n            \"PlayerFullName\":[playerFullName],\n            \"Nationality\":[nationality],\n            \"Photo\":[photo],\n            \"Birth\":[birth],\n            \"Height\":[height],\n            \"Weight\":[weight],\n            \"Position\":[position],\n            \"Foot\":[foot],\n            \"FootAbility\":[foot_ability],\n            \"InternationalReputation\":[ir],\n            \"PlayerUrl\":[i],\n            \"Twitter\":[tw],\n            \"Instagram\":[ins]\n        })    \n\n        temp[\"PlayerFullName\"] = np.where(temp.PlayerFullName.isnull(), temp.PlayerName, temp.PlayerFullName)\n\n        player_info.append(temp)\n\n        # Print Message\n        sys.stdout.write(\"\\r{0} players have just scraped from FBRef!\".format(completed+1))\n        sys.stdout.flush()\n\n        # System Sleep\n        time.sleep(0.01) \n\n    # Write Player Info\n    player_info = pd.concat(player_info)\n    \n    return player_info\n\n# Test\nplayer_info = fbref_player_info(player_url)\nplayer_info","4c42b85e":"def player_news(player_id):\n    \n    url = \"https:\/\/fbref.com\/en\/news\/\" + player_id\n    \n    # Request\n    r=requests.get(url)\n    soup=BeautifulSoup(r.content, \"html.parser\")\n    \n    news = soup.find_all(\"div\", {\"class\":\"section_wrapper\"})\n    news_count = []\n    for i in news:\n        try:\n            when = i.find(\"span\")[\"data-label\"].replace(\"News -- \", \"\").replace(\" (\", \"-\").replace(\")\", \"\")\n            when = pd.DataFrame({\"FBRefId\":[player_id], \"News\":when})\n            when[[\"Date\", \"NewsCount\"]] = when.News.str.split(\"-\", expand = True)\n            when.drop(\"News\", axis = 1, inplace = True)\n            when.Date = pd.to_datetime(when.Date)\n            news_count.append(when)\n        except:\n            pass\n    \n    return news_count\n    \n    \nnews = []\nfor completed, i in enumerate(player_info.FBRefId.tolist()[:1]):\n    \n    news.append(player_news(i))\n    \n    # Print Message: How many players have scraped?\n    sys.stdout.write(\"\\r{0} players have just scraped from FBRef!\".format(completed+1))\n    sys.stdout.flush()\n    \n    # Sleep the system\n    time.sleep(0.001)\n    \nget_time(\"end\")\n\n# Combine all data\nnews  = list(itertools.chain(*news))\nnews = pd.concat(news)\n\n\nnews","9d636533":"# Domestic League | Domestic Cups | International Cups |National Team\nstats_dict = {}\nfor i in [\"dom_lg\", \"dom_cup\", \"intl_cup\", \"nat_tm\"]:\n    stats_list = [\"div_stats_standard_\", \"div_stats_shooting_\",\"div_stats_passing_\", \"div_stats_passing_types_\",\"div_stats_gca_\", \"div_stats_defense_\", \"div_stats_possession_\",\"div_stats_playing_time_\", \"div_stats_misc_\"]\n    stats_list = list(map(lambda x: str(x)+i, stats_list))\n    stats_dict[i] = stats_list\ndel i, stats_list\n\n\ndef get_stats(soup, purl, id_, first_season = 2017, final_season = 2021):\n    \n    # Player ID\n    playerid = purl.replace(\"https:\/\/fbref.com\/en\/players\/\", \"\").split(\"\/\")[0]\n    \n    # Get Table\n    temp = pd.read_html(str(soup.find(\"div\", {\"id\":id_}).find(\"table\")))[0]\n        \n    # Select Correct Rows\n    seasons = list(map(lambda x: str(x)+\"-\"+str(x+1), np.arange(first_season, final_season+1, 1)))\n    seasons = seasons + list(map(lambda x: str(x), np.arange(first_season,final_season+1,1)))\n    temp = temp[temp[\"Unnamed: 0_level_0\"].Season.isin(seasons)]\n    \n\n    # Rename Columns\n    temp.columns = temp.columns.map('_'.join).str.strip(\"_\")\n    rename_cols = lambda x,y: str(x).replace(\"_level_0\", \"\").replace(\"Unnamed: \", \"\").replace(\" \", \"_\").replace(str(y)+\"_\", \"\")\n    temp.columns = list(map(rename_cols, temp.columns, np.arange(0,40,1)))\n\n    # Drop\n    temp.drop(\"Matches\", axis = 1, inplace = True)\n    \n     # Reduce Memory\n    temp[\"Age\"] = temp.Age.astype(\"int8\")\n    \n    # Player ID\n    temp[\"FBRefId\"] = playerid  \n\n    # Set Index\n    if \"nat_tm\" in id_ or \"intl_cup\" in id_ :\n        temp.set_index([\"FBRefId\", \"Season\", \"Squad\", \"Comp\", \"Age\", \"LgRank\"], inplace = True)\n    else:\n        temp[\"Country\"] = temp[\"Country\"].str.split(\" \", expand = True)[1]\n        temp.set_index([\"FBRefId\",\"Season\", \"Squad\", \"Comp\", \"Age\", \"Country\", \"LgRank\"], inplace = True)\n        \n    # Rename\n    prefix = id_.replace(\"div_stats_\", \"\").replace(\"dom_lg\", \"\").replace(\"dom_cup\", \"\").replace(\"intl_cup\", \"\").replace(\"nat_tm\", \"\").upper()\n    temp.columns = list(map(lambda x: prefix+str(x), temp.columns.tolist()))\n        \n        \n    # Reduce Memory\n    temp = temp.astype(\"float\")\n    red_cols = temp.select_dtypes(\"float\").columns\n    temp[red_cols] = temp[red_cols].astype(\"float16\")\n\n    return temp\n\n\n\ndef fbref_player_stats(playerid):\n    # Start the process and display time\n    ################################################################\n    get_time(\"start\")\n\n    # Create Empty Lists for stats\n    ################################################################\n    dom_lg_df = []\n    dom_cup_df = []\n    intl_cup_df = []\n    nat_tm_df = []\n\n    # Start a loop for every player\n    ################################################################\n    for completed, select_player in enumerate(playerid):  \n\n        # Rewrite url\n        select_player = 'https:\/\/fbref.com\/en\/players\/' + select_player + '\/all_comps\/'\n\n        # Request\n        r=requests.get(select_player)\n        soup=BeautifulSoup(r.content, \"html.parser\")\n\n        # Create Empty List As Temporary\n        dom_lg_df_temp = []\n        dom_cup_df_temp = []\n        intl_cup_df_temp = []\n        nat_tm_df_temp = []\n\n        # Domestic League | Domestic Cups | International Cups |National Team\n        for key in stats_dict.keys():\n            temp = []\n            for i in range(0,9):\n                x = stats_dict[key][i]\n                try: \n                    x = get_stats(soup, select_player, id_ = x)\n                    temp.append(x)\n                except:\n                    pass\n\n            try:\n                if key == 'dom_lg':\n                    dom_lg_df_temp.append(pd.concat(temp, axis = 1))\n                elif key == 'dom_cup':\n                    dom_cup_df_temp.append(pd.concat(temp, axis = 1))\n                elif key =='intl_cup':\n                    intl_cup_df_temp.append(pd.concat(temp, axis = 1))\n                elif key =='nat_tm':\n                    nat_tm_df_temp.append(pd.concat(temp, axis = 1))\n            except: \n                pass\n\n        try: \n            # Return Dataframe\n            dom_lg_df.append(pd.concat(dom_lg_df_temp).drop_duplicates())\n            dom_cup_df.append(pd.concat(dom_cup_df_temp).drop_duplicates())\n            intl_cup_df.append(pd.concat(intl_cup_df_temp).drop_duplicates())\n            nat_tm_df.append(pd.concat(nat_tm_df_temp).drop_duplicates())\n        except:\n            pass\n\n\n        # Print Message: How many players have scraped?\n        sys.stdout.write(\"\\r{0} players have just scraped from FBRef!\".format(completed+1))\n        sys.stdout.flush()\n\n        # Sleep the system\n        time.sleep(0.001)\n\n\n    # Reduce Memory\n    ################################################################\n    del key, temp, i, dom_lg_df_temp, dom_cup_df_temp, intl_cup_df_temp, nat_tm_df_temp\n    gc.collect()\n\n    # Finish the process and display time\n    ################################################################\n    get_time(\"end\")\n\n\n    # Transorm Data Frame\n    ################################################################\n    dom_lg_df2 = pd.concat(dom_lg_df).reset_index()\n    dom_cup_df2 = pd.concat(dom_cup_df).reset_index()\n    intl_cup_df2 = pd.concat(intl_cup_df).reset_index()\n    nat_tm_df2 = pd.concat(nat_tm_df).reset_index()\n    print(dom_lg_df2.shape, dom_cup_df2.shape, intl_cup_df2.shape, nat_tm_df2.shape)\n    print(dom_lg_df2.FBRefId.nunique(), dom_cup_df2.FBRefId.nunique(), intl_cup_df2.FBRefId.nunique(), nat_tm_df2.FBRefId.nunique())\n\n    # Write Data\n    ################################################################\n    dom_lg_df2.to_csv(\"performance_dom_lg.csv\", index = None)\n    dom_cup_df2.to_csv(\"performance_dom_cup.csv\", index = None)\n    intl_cup_df2.to_csv(\"performance_intl_cup.csv\", index = None)\n    nat_tm_df2.to_csv(\"performance_nat_tm.csv\", index = None)\n    \n    return dom_lg_df2, dom_cup_df2, intl_cup_df2, nat_tm_df2\n    \n    \n# Test\ndom_lg, dom_cup, intl_cup, nat_tm = fbref_player_stats(player_info.FBRefId[:1])","2cf67512":"dom_lg","bb468fcb":"dom_cup","6ae476af":"intl_cup","501cc452":"nat_tm","5b389ee5":"<div class=\"alert alert-success\" role=\"alert\">\n    <h4> <strong>NOTE:<\/strong> Always find and keep sub-urls!<\/h4>\n<\/div>\n\n<a id='tm2'><\/a>\n<h1 style=\"color:green\" >Finding urls of the teams<\/h1>\n\nYou've just learned all historic urls for Premier League. We need much more! Also we need to reach all historic urls for each teams. Let's see **find_team_urls** function.","5e127f9f":"<hr>","008c480c":"<hr>\n\n<div class=\"alert alert-danger\" role=\"alert\">\n    <h5> <strong>Don't forget to use Time Sleep for your projects! :)<\/strong><\/h5>\n<\/div>","254f73e5":"<hr>","88013006":"<hr>","12b54122":"<hr>","69b72cbb":"<a id='tm'><\/a>\n<center><img src=\"https:\/\/tmsi.akamaized.net\/head\/transfermarkt_logo.svg\" style=\"width:50%;height:10%;\"><\/center>\n\n<br>\n\nIn this section, you will learn the basics of scraping from Transfermarkt. First, you need to know there are many leagues, clubs and players. That means a lot of data. You have to be focused on what you want. That's why, you must ask that big question yourself **\"What type of analysis I want to do?\"**.\n\n- *Do I need the data for a player or players?*\n- *Do I need the data for a club or clubs?*\n- *Do I need the data for a league or leagues?*\n- *Which season do I need?*\n- *Or do I need all of them?*\n\nYou can collect all data from all of leagues. Data preprocessing, analysis, modelling and so on will be hard for you in the end of the data collecting. Therefore, the more you are focused on, the more you get better results.\n\n\n<div class=\"alert alert-success\" role=\"alert\">\n    <h4> <strong>NOTE:<\/strong> Always find and keep urls!<\/h4>\n<\/div>\n\n\nFirst, you should learn the url of league what you want to scrape from. You can see the Big 5 leagues below. These are the examples of the leagues. I trimmed the year after **\"saison_id\"** because I want to scrape last seasons as well. \n\n- https:\/\/www.transfermarkt.com.tr\/bundesliga\/startseite\/wettbewerb\/L1\/saison_id\/\n- https:\/\/www.transfermarkt.com.tr\/premier-league\/startseite\/wettbewerb\/GB1\/saison_id\/\n- https:\/\/www.transfermarkt.com.tr\/laliga\/startseite\/wettbewerb\/ES1\/saison_id\/\n- https:\/\/www.transfermarkt.com.tr\/serie-a\/startseite\/wettbewerb\/IT1\/saison_id\/\n- https:\/\/www.transfermarkt.com.tr\/ligue-1\/startseite\/wettbewerb\/FR1\/saison_id\/\n\nIn the first code, you will see **all_league_urls** function. It needs a list and season range. If you give a list with trimmed urls of the leagues and define the season range, you will get all historical urls for each leagues.","a4262cd5":"<a id='fbref3'><\/a>\n<h1 style=\"color:green\" >Finding urls of the players<\/h1>\n\nLet's find player urls of Chelsea! We use these urls constantly for finding player info, stats, news etc.","7ebcca77":"<a id='tm3'><\/a>\n<h1 style=\"color:green\" >Finding urls of the players<\/h1>\n\nWe've just scraped our first data! We found all the urls of the teams for each season what we defined. **find_player_urls** function will almost close us to our goal. After running the function, we will have a dataframe for each player in a team in a season. This data frame will be most important element for scraping data of each players.","88efd6e8":"<center><img \n     src=\"https:\/\/galeri14.uludagsozluk.com\/766\/bilmeyenler-icin-mesut-ozil_2119245.gif\" style=\"width:75%;height:10%;\"><\/center>\n ","726e6855":"<hr>","db725ae3":"### Domestic Cups Stats","9f4e2209":"<a id='tm6'><\/a>\n<h1 style=\"color:green\" >Transfer History<\/h1>\n\nAnother data that we will look at is **Transfer History**. I think that it is a resourceful data. You can learn transfer date, fees, transfer types, previous club and next club for players. But it needs a little bit data manipulation to use a healthy data.\n\n<center><img\nsrc=\"https:\/\/github.com\/EkremBayar\/Kaggle\/blob\/main\/Images\/5.PNG?raw=true\" style=\"width:75%;height:50%;\">\n<\/center>","43694d28":"Yes, we made it! Let's visualize the dataframe for Mesut \u00d6zil and see his market value changing.","7c59f928":"<a id='tm4'><\/a>\n<h1 style=\"color:green\" >Market Value<\/h1>\n\nI think, Market Value data is the most important data in Transfermarkt. On the other hand, it may include biased data because, it is based on [The Wisdom of Crowds](https:\/\/www.google.com\/search?q=the+wisdom+of+crowds&rlz=1C1SQJL_trTR868TR868&ei=y8BcYZrxOI-drwSX-76gBg&ved=0ahUKEwjagt_mmLTzAhWPzosKHZe9D2QQ4dUDCA4&uact=5&oq=the+wisdom+of+crowds&gs_lcp=Cgdnd3Mtd2l6EAMyCAguEIAEEJMCMgYIABAWEB4yBggAEBYQHjIGCAAQFhAeMgYIABAWEB4yBggAEBYQHjIGCAAQFhAeMgYIABAWEB4yBggAEBYQHjIGCAAQFhAeOgcIABBHELADOgcIABCwAxBDOg0ILhDIAxCwAxBDEJMCOgoILhDIAxCwAxBDOggIABAHEB4QEzoECAAQEzoECC4QEzoHCCEQChCgAToECAAQHjoECAAQQzoECC4QQzoICC4QgAQQsQM6CwguEIAEELEDEJMCOgsILhCABBCxAxCDAToFCC4QgAQ6CAgAEIAEELEDOgUIABCABEoFCDgSATFKBAhBGABQjYwCWI_uAmCm8AJoAXACeACAAbMCiAHBLZIBCTAuMTUuMTIuMZgBAKABAcgBD8ABAQ&sclient=gws-wiz). When you take a look at the literature, you will see that some of people don't trust Transfermarkt. You can see an example from [Jan Van Haaren's Tweet](https:\/\/twitter.com\/JanVanHaaren\/status\/1340297124551012352) below.\n\n<center><img\nsrc=\"https:\/\/github.com\/EkremBayar\/Kaggle\/blob\/main\/Images\/1.PNG?raw=true\" style=\"width:55%;height:50%;\">\n<\/center>\n\n\n\n<h2>\n   <span style=\"color:green\">Come to <\/span><span class=\"blue\" style=\"color:blue\"><strong>Fener<\/strong><\/span><span class=\"gray2\" style=\"color:gold\"><strong>bah\u00e7e<\/strong><\/span><span style=\"color:green\"> like Mesut \u00d6zil!<\/span>\n<\/h2>\n\n\n**Mesut \u00d6zil - Transfermarkt Profile**\n- *English:* https:\/\/www.transfermarkt.co.uk\/mesut-ozil\/marktwertverlauf\/spieler\/35664\n- *Turkish:* https:\/\/www.transfermarkt.com.tr\/mesut-ozil\/marktwertverlauf\/spieler\/35664\n\n![image.png](attachment:5f3184c7-ed7b-45d6-a0f9-4b179248c004.png)\n\n![](https:\/\/github.com\/EkremBayar\/Kaggle\/blob\/main\/Images\/6.PNG?raw=true)\n\n<div class=\"alert alert-danger\" role=\"alert\">\n    <h5> <strong>NOTE:<\/strong> You can find other data on Navbar. Injury History and Suspensions & Absences data are inside STATS section. We will use them soon.<\/h5>\n<\/div>\n\n\nWhen you enter a player profile in Transfermarkt, you will see an interactive graph for Market Value below. It is difficult to scrape data from a graph actually. But we can scrape the data by using pyjsparser and js2xml packages. Our next function is **market_value**. It helps us to scrape all data from graph for a player and it returns a dataframe.\n\n<center><img\nsrc=\"https:\/\/github.com\/EkremBayar\/Kaggle\/blob\/main\/Images\/2.PNG?raw=true\" style=\"width:75%;height:50%;\">\n<\/center>","7989819a":"<hr>","ba668db4":"<a id='fbref2'><\/a>\n<h1 style=\"color:green\" >Finding urls of the teams<\/h1>\n\nWe've just found all historic league urls, so we can find all historic team urls as well. When we run the function, it gives us all team urls in last season.","0375fda1":"<hr>","a6fd3206":"<a id='fbref5'><\/a>\n<h1 style=\"color:green\" >Player News<\/h1>\n\nIf you would like to see popularity of the players, you can look at the news about them. I only present frequencies for you but you can also collect text from news if you would like to analyze texts.\n\nYou can see an example of player news with Messi below.\n\n![](https:\/\/github.com\/EkremBayar\/Kaggle\/blob\/main\/Images\/9.PNG?raw=true)\n\nWe will run the function for a Chelsea player that we've just found the url. By the way, this player will be [**Antonio R\u00fcdiger**](https:\/\/fbref.com\/en\/players\/18b896d6\/Antonio-Rudiger) from Chelsea.","3af37ea4":"<a id='libraries'><\/a>\n<h1 style=\"color:green\" >Packages & Helpers<\/h1>\n\nI used many packages for scraping and you know familiar ones. You can look at them below that you may not know.","6112116c":"<hr>","09a031e6":"<a id='tm5'><\/a>\n<h1 style=\"color:green\" >Injury History<\/h1>\n\nOne of the most important issues for teams is injured players. Especially, the star players shouldn't be injured. Due to of injured players, teams can be affected in terms of losing championship or matches. Also treatment cost of the players might be too expensive for teams. That's why, preventing injuries are very important. If we have some information about player healths, injury history and so on, we can treat or rest them before injured by using data.\n\nPlease, look at the [injury history of Mesut \u00d6zil](https:\/\/www.transfermarkt.co.uk\/mesut-ozil\/verletzungen\/spieler\/35664) and see the patterns if there is any.\n\n<center><img\nsrc=\"https:\/\/github.com\/EkremBayar\/Kaggle\/blob\/main\/Images\/3.PNG?raw=true\" style=\"width:75%;height:50%;\">\n<\/center>\n\n<br>\n\nThe next function is **injury_table**. It returns a dataframe for a player.","918f3efa":"<a id='tm7'><\/a>\n<h1 style=\"color:green\" >Suspensions & Absences<\/h1>\n\nI'm not sure this data can help you for your analysis. But it can be useful if you know what you do. It includes number of **Games Missed** and types of suspensions or absences for a player.\n\n<center><img\nsrc=\"https:\/\/github.com\/EkremBayar\/Kaggle\/blob\/main\/Images\/4.PNG?raw=true\" style=\"width:75%;height:50%;\">\n<\/center>","8f937fc8":"### International Cups Stats","3b0ebb1e":"<hr>","35077a06":"<hr>","80f227ad":"<hr>","16abc797":"<a id='fbref6'><\/a>\n<h1 style=\"color:green\" >Player Performance Stats<\/h1>\n\nOne of the most important part in notebook is scraping of the players. The players are able to play different competitions like domestic leagues, domestic cups, international cups and national team. We can collect all data with following code. We will have 4 different dataframes at the end of the process.\n\n- *Messi FBRef Profile:* https:\/\/fbref.com\/en\/players\/d70ce98e\/Lionel-Messi\n![](https:\/\/github.com\/EkremBayar\/Kaggle\/blob\/main\/Images\/8.PNG?raw=true)","292bb786":"<a id='fbref'><\/a>\n<center><img \n     src=\"https:\/\/d2p3bygnnzw9w3.cloudfront.net\/req\/202109021\/logos\/fb-logo.svg\" style=\"width:50%;height:10%;\"><\/center>\n     \n<br>\n     \n[**Football Reference**](https:\/\/fbref.com\/en\/) is like a player database. You can find many of the leagues and tournements here. Also FBRef is supported with some useful data such as Expected Goal and other advanced stats by [**StatsBomb**](https:\/\/statsbomb.com\/).\n\nIn this section, we will collect info, news and stats of the players. First of all we shouldn't forget to find all urls. We will start with Leagues again.\n\nYou can find competition ids for Big 5 leagues. If you want to scrape other leagues, you have to find their competition id. You can do it by using scraping also.\n- England | Premier League: 9\n- Italy | Serie A: 11\n- Spain | La Liga: 12\n- France | Ligue 1: 13 \n- Germany | Bundesliga: 20","16941353":"<hr>","4cc2d98b":"**Congratulations!**\n\nYou have a list of the players in Chelsea in 2017 season. You can have all urls for leagues, teams and players. Now, we can pass next step!","7d23225a":"<hr>","5d85fa93":"<a id='tm1'><\/a>\n<h1 style=\"color:green\" >Finding urls of the leagues<\/h1>\n\nWhen the function applies, you will have all seasons for Premier League. You can add the urls of the other leagues in the list.","d698d91f":"<hr>","c7daefb0":"### National Team Stats","9a576dec":"<a id='fbref4'><\/a>\n<h1 style=\"color:green\" >Player Info<\/h1>\n\nThe urls of the players are important to us. We will scape all data here what we want to scrape. Firstly, we can collect their personal info. **fbref_player_info** helps us to create a data frame with all player info.\n\n- *Messi FBRef Profile:* https:\/\/fbref.com\/en\/players\/d70ce98e\/Lionel-Messi\n<br>\n\n![](https:\/\/github.com\/EkremBayar\/Kaggle\/blob\/main\/Images\/7.PNG?raw=true)\n\nIn our example, we chose Chelsea players to see a result below.","e46a8654":"### Domestic Leagues Stats","44d54f7e":"<center> <h1 style=\"background-color:green; color:white\" >How to Scrape Football Data from Transfermarkt & Football Reference<\/h1> <\/center>\n\n<center><img\nsrc=\"https:\/\/www.woodhouseparklifestylecentre.co.uk\/wp-content\/uploads\/2014\/04\/Football-pitch-banner.jpg\" style=\"width:100%;height:40%;\">\n\n    \n<br>    \n    \n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" style=\"background-color:green; color:white\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Content!<\/h3>  \n<\/div> <\/center>\n      \n    \n<a id=\"table-of-contents\"><\/a>\n1. [Introduction](#intro)\n2. [Packages & Helpers](#libraries)\n3. [Transfermarkt Intro](#tm)\n    * [Finding urls of the leagues](#tm1)\n    * [Finding urls of the teams](#tm2)\n    * [Finding urls of players](#tm3)\n    * [Market Value](#tm4)\n    * [Injury History](#tm5)\n    * [Transfer History](#tm6)\n    * [Suspensions & Absences](#tm7)\n4. [Football Reference Intro](#fbref)\n    * [Finding urls of the leagues](#fbref1)\n    * [Finding urls of the teams](#fbref2)\n    * [Finding urls of players](#fbref3)\n    * [Player Intro](#fbref4)\n    * [Player News](#fbref5)\n    * [Player Stats](#fbref6)\n\n<a id='intro'><\/a>\n<h1 style=\"color:green\" >Introduction<\/h1>\n\n**Key Words:** *Web Scraping, Football, Soccer, Market Value, Player Performance, Injury, Transfer, Suspensions, Absences, Beautiful Soup, Transfermarkt, Football Reference, Sports Analytics, Football Analytics*\n\nMany of us like sports. Some of us are talented to be able to do any branch of sports, some of us aren't! But we are still interested in analyzing, if there are some data. Unfortunately, accessing free sports data is not easy. There are some reasons why you don't access them.\n\nFirst, collecting sports data is very difficult process. Second, the companies who collect the data don't share whole data for free and there is a price. Third, none of the clubs or federations don't share the data.\n\nYou can say \"there are some web sites and free data\". You are partly right. However, it probably contains basic stats such as minutes played, goal, asist, cards, passing, etc. If you would like to get various data, you should scrape from multiple web sites. \n\nAlso there are some data providers who share some limited free data like StatsBomb and WyScout. You can learn many things about sports analytics by using them. I shared [**StatsBomb World Cup Event Data**](https:\/\/www.kaggle.com\/ekrembayar\/fifa-world-cup-statsbomb-event-data-introduction\/data) on Kaggle and created a [**StatsBomb Notebook**](https:\/\/www.kaggle.com\/ekrembayar\/fifa-world-cup-statsbomb-event-data-introduction). You can start with basics and learn here for event data.\n\nMaking an project you have to gather data from different sources. In this notebook, I'm going to teach you how to scrape football data from [**Transfermarkt**](https:\/\/www.transfermarkt.com.tr\/) and [**Football Reference**](https:\/\/fbref.com\/en\/). You will be learning to scrape all useful data like below.\n\n**Transfermarkt Data**\n- <code>Market Value<\/code> \n- <code>Injury History<\/code>\n- <code>Transfer History<\/code>\n- <code>Suspensions & Absences<\/code>\n\n**FBRef - Football Reference**\n- <code>Player Info<\/code>\n- <code>Player News<\/code>\n- <code>Performance Data<\/code>\n\nIt isn't easy to make a career on sports because of many reasons. When you watched the **MONEYBALL**, probably you were so motivated to learn everything about sports analytics. After then, you faced with the reality. But don't worry! I firmly belive that this notebook will help you and give you an inspiration to be hardworking. \n\n<center><img\nsrc=\"https:\/\/indigodergisi.com\/wp-content\/uploads\/2019\/07\/Moneyball-Afi%C5%9F.jpg\" style=\"width:40%;height:40%;\"><\/center>\n\nYou might not have high quality data for football. But if you know how to scrape data, you can make many creative projects with limited data.\n\n<div class=\"alert alert-warning\" role=\"alert\">\n    <h5> <strong>NOTE:<\/strong> I used Turkish web site domain for Transfermarkt. For this reason, when you will scrape data by using the codes of this notebook, you might face with url problems or Turkish variable names. Don't worry about that. It won't be a big problem, you will be able to adapt the code easily.  <\/h5>\n<\/div>\n\n**Let's get start!**","2b2fab69":"<a id='fbref1'><\/a>\n<h1 style=\"color:green\" >Finding urls of the leagues<\/h1>\n\n**fbref_league_history** function gives us all historic league urls. You just need to give a competition id list and first season info.","083bbb29":"<hr>","0e3d2e0f":"<hr>"}}