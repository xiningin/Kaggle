{"cell_type":{"93ef7a2f":"code","f10f4011":"code","61243fb9":"code","21d44c68":"code","9f3b76b7":"code","466ee7a8":"code","027073f4":"code","508cdeea":"code","45b37e1d":"code","97a13c2d":"code","2efde620":"code","3590d92f":"code","a6805821":"markdown","1715d3b7":"markdown","fd48046f":"markdown","370aeec0":"markdown","85d91cf6":"markdown","2e75eea7":"markdown","39c23d31":"markdown","f708e0f9":"markdown","1d52ca6b":"markdown","36ba86e5":"markdown","2e428275":"markdown"},"source":{"93ef7a2f":"# PREDICTING THE EFFECTS OF GENETIC VARIATIONS USING LGBM\n# BY - OMKAR SABNIS - 29-05-2018\n#Importing library\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import confusion_matrix,mean_squared_error\nfrom sklearn.model_selection import KFold, cross_val_score,train_test_split\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb","f10f4011":"# READING THE DATASETS\ntrain = pd.read_csv(\"..\/input\/training_variants\")\ntrainx = pd.read_csv('..\/input\/training_text',sep = '\\|\\|', engine= 'python', header=None, \n                     skiprows=1, names=[\"ID\",\"Text\"])\ntrain = pd.merge(train, trainx, how = 'left', on = 'ID').fillna('')\ntrain.head()","61243fb9":"test = pd.read_csv(\"..\/input\/stage2_test_variants.csv\")\ntestx = pd.read_csv('..\/input\/stage2_test_text.csv',sep = '\\|\\|', engine= 'python', header=None, \n                     skiprows=1, names=[\"ID\",\"Text\"])\ntest = pd.merge(test, testx, how = 'left', on = 'ID').fillna('')\ntest.head()","21d44c68":"train.Gene.nunique()\ntrain['Gene'].unique()\n\nk = train.groupby('Gene')['Gene'].count()\n\nplt.figure(figsize=(12,6))\nplt.hist(k, bins=150,log=True)\nplt.xlabel('Number of times Gene appared')\nplt.ylabel('Log of count')\nplt.title('Appearence of gene')\nplt.show()\n\n#count Gene\nfrom collections import Counter\nplt.figure(figsize=(12,10))\nsns.countplot((train['Gene']))\nplt.xticks()\ngenecount = Counter(train['Gene'])\nprint('Genes and their appearence:')\nprint(genecount,'\\n',len(genecount))\n\ntrain.Variation.nunique()\ntrain['Variation'].unique()\n\nk = train.groupby('Variation')['Variation'].count()\nplt.title('Graph of Gene vs Count')\nplt.figure(figsize=(12,6))","9f3b76b7":"def textlen(train):\n    k = train['Text'].apply(lambda x: len(str(x).split()))\n    l = train['Text'].apply(lambda x: len(str(x)))\n    return k, l\n\ntrain['Text_no_word'], train['Text_no_char'] = textlen(train)\ntest['Text_no_word'], test['Text_no_char'] = textlen(test)","466ee7a8":"tfidf = TfidfVectorizer(\n\tmin_df=1, max_features=1600, strip_accents='unicode',lowercase =True,\n\tanalyzer='word', token_pattern=r'\\w+', ngram_range=(1, 3), use_idf=True, \n\tsmooth_idf=True, sublinear_tf=True, stop_words = 'english')\nX_train = tfidf.fit_transform(train['Text']).toarray()\nprint(X_train)\nX_test = tfidf.fit_transform(test['Text']).toarray()\n\ndef encoding(df,col):\n    le = LabelEncoder()\n    for i in col:\n        df[i] = le.fit_transform(df[i])\ntrain.columns\ncol = ['Gene', 'Variation', 'Class']\nencoding(train,col)\nencoding(test,['Gene', 'Variation'])\n\nX_train = pd.DataFrame(X_train)\nX_train = X_train.join(train[['Gene', 'Variation', 'Text_no_word','Text_no_char']]) \nX_test = pd.DataFrame(X_test)\nX_test = X_test.join(test[['Gene', 'Variation', 'Text_no_word','Text_no_char']])","027073f4":"# FEATURE SCALING\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\ny_train = train['Class']","508cdeea":"xtr,xvl,ytr,yvl = train_test_split(X_train,y_train,test_size=0.3,random_state=10)","45b37e1d":"# NAIVE BAYES\nnbc = GaussianNB()\nnbc.fit(xtr,ytr)\ny_nbcP = nbc.predict(xvl)\ny_nbc = nbc.predict_proba(X_test)\nprint(\"Confusion Matrix using Naive Bayes:\")\nprint(confusion_matrix(yvl,y_nbcP))\nprint(\"\\n\")\n\n# RANDOM FOREST\nrfc = RandomForestClassifier(n_estimators=50,max_depth=8,min_samples_split=4)\nrfc.fit(xtr,ytr)\ny_rfcp = rfc.predict(xvl)\ny_rfc=rfc.predict_proba(X_test)\nprint(\"Confusion Matrix using Random Forest:\")\nprint(confusion_matrix(yvl,y_rfcp))","97a13c2d":"def runLgb(Xtr,Xvl,ytr,yvl,test,num_rounds=10,max_depth=10,eta=0.5,subsample=0.8,\n           colsample=0.8,min_child_weight=1,early_stopping_rounds=50,seeds_val=2017):\n    \n    param = {'task': 'train',\n             'boosting_type': 'gbdt',\n             'objective':'multiclass',\n             'num_class':9,\n             'learning_rate':eta,\n             'metric':{'multi_logloss'},\n             'max_depth':max_depth,\n             #'min_child_weight':min_child_weight,\n             'bagging_fraction':subsample,\n             'feature_fraction':colsample,\n             'bagging_seed':seeds_val,\n             'num_iterations': num_rounds, \n             'num_leaves': 95,           \n             'min_data_in_leaf': 60, \n             'lambda_l1': 1.0,\n             'verbose':10,\n             'nthread':-1}\n    lgtrain = lgb.Dataset(Xtr,label=ytr)\n    lgval = lgb.Dataset(Xvl,label=yvl)\n    model = lgb.train(param,lgtrain,num_rounds,valid_sets=lgval,\n                      early_stopping_rounds=early_stopping_rounds,verbose_eval=20)\n    pred_val = model.predict(Xvl,num_iteration = model.best_iteration)\n    pred_test = model.predict(test,num_iteration=model.best_iteration)\n    return pred_test,pred_val,model","2efde620":"kf = KFold(n_splits=10,random_state=111,shuffle=True)\ncv_score = []\npred_test_full=0\n\nfor train_index,test_index in kf.split(X_train):\n    Xtr,Xvl = X_train[train_index],X_train[test_index]\n    ytr,yvl = y_train[train_index],y_train[test_index]\n    \n    pred_test,pred_val,model = runLgb(Xtr,Xvl,ytr,yvl,X_test,num_rounds=10,max_depth=3,\n                            eta=0.02,)\n    pred_test_full +=pred_test\npred_test = pred_test_full\/10","3590d92f":"# SUBMISSION OF FILE IN CSV FORMAT:\nsubmit = pd.DataFrame(test.ID)\nsubmit = submit.join(pd.DataFrame(pred_test))\nsubmit.columns = ['ID', 'class1','class2','class3','class4','class5','class6','class7','class8','class9']\nsubmit.to_csv('submission.csv', index=False) ","a6805821":"**(5). Bag of words and converting the variables into categorical variables:**","1715d3b7":"**(10). K- Fold Cross Validation of Model:**","fd48046f":"**(3). Data Exploration:**","370aeec0":"**(2). Reading and Visualizing the data:**","85d91cf6":"**(4). Determining the length of the text:**","2e75eea7":"**(8). Modelling - Naive Bayes and Random Forest:**","39c23d31":"**(9). Modelling - Light Gradient Boosting Machine(LGBM):**","f708e0f9":"**(7). Splitting the Dataset into training and testing set:**","1d52ca6b":"# Predicting the effects of Genetic Variations using Light GBM\nIn this notebook, we will use Light GBM as out algorithm for classifying genetic mutations based on clinical evidence. There are 9 different classes a genetic mutation can be classified based upon.\n**Light GBM - Some basic information:**\n* A decision tree based algorithm, LGBM splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. \n* Advantages:\n    * Faster training speed and higher accuracy.\n    * Compatible with huge datasets.\n    * Better than XGBoost and other boosting algorithms.   \n* Let's start this kernel!   ","36ba86e5":"**(1). Importing all the necessary modules:**","2e428275":"**(6). Feature Scaling:**"}}