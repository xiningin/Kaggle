{"cell_type":{"aaecb321":"code","d91de16d":"code","0bec7f13":"code","d2b5b16b":"code","e867d3b0":"code","95645f18":"code","57132f29":"code","375e8df0":"code","29d6d3d2":"code","64dd3649":"code","aab23b62":"markdown","86f3c986":"markdown","ad9f2e98":"markdown","1da76bdc":"markdown","921b4fb5":"markdown","98e69dd0":"markdown","a49dc91a":"markdown","d4ac8818":"markdown","0019541e":"markdown"},"source":{"aaecb321":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import cross_validate\n\n\nimport lightgbm as lgbm\n\nimport gc\nfrom pathlib import Path\ngc.enable()","d91de16d":"import warnings \nwarnings.filterwarnings('ignore')","0bec7f13":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n#                     df[col] = df[col].astype(np.float16)\n#                 elif\n\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","d2b5b16b":"data_dir = Path('..\/input\/tabular-playground-series-oct-2021\/')\n\n# this constant depend amount of our experiment data amount\nsmall_amout = 500000\n\ndf_train = pd.read_csv(\n    data_dir \/ \"train.csv\",\n    index_col='id',\n    nrows=small_amout, \n)\n\ndf_train = reduce_mem_usage(df_train)\n\nFEATURES = df_train.columns[:-1]\nTARGET = df_train.columns[-1]\n\nX = df_train.loc[:, FEATURES]\ny = df_train.loc[:, TARGET]\n\nseed = 0\nfold = 5","e867d3b0":"'''model_lgbm = lgbm.LGBMClassifier(\n    num_iterations=100,\n    objective = \"binary\",\n    num_leaves= 31,\n    feature_pre_filter = False\n    )\ndef score(X, y, model_lgbm, cv):\n    scoring = [\"roc_auc\"]\n    scores = cross_validate(\n        model_lgbm, X, y, scoring=scoring, cv=cv, return_train_score=True\n    )\n    scores = pd.DataFrame(scores).T\n    return scores.assign(\n        mean = lambda x: x.mean(axis=1),\n        std = lambda x: x.std(axis=1),\n    )\n\nscores = score(X, y, model_lgbm, cv=fold)\ndisplay(scores)\n'''","95645f18":"'''\ndef score(X, y, model_lgbm, cv):\n    scoring = [\"roc_auc\"]\n    scores = cross_validate(\n        model_lgbm, X, y, scoring=scoring, cv=cv, return_train_score=True\n    )\n    scores = pd.DataFrame(scores).T\n    return scores.assign(\n        mean = lambda x: x.mean(axis=1),\n        std = lambda x: x.std(axis=1),\n    )\n\ntest_roc_auc_row = []\n\nfor num_iter in range(90, 301, 40):\n    for max_d in range (4, 8, 1):\n        model_lgbm = lgbm.LGBMClassifier(\n        num_iterations=num_iter,\n        objective = \"binary\",\n        feature_pre_filter = False,\n        max_depth = max_d\n        )\n\n        res = {}\n        res['num_iter'] = num_iter\n        res['max_depth'] = max_d\n        scores = score(X, y, model_lgbm, cv=fold)\n        res['test_roc_auc'] = scores.loc['test_roc_auc','mean']\n        print(num_iter, max_d, res['test_roc_auc'])\n\n        test_roc_auc_row.append(res)\n'''","57132f29":"'''test_roc_auc_row'''","375e8df0":"'''df = pd.DataFrame(test_roc_auc_row)\ndf.sort_values(by='test_roc_auc', ascending=False).head(10)'''","29d6d3d2":"model_lgbm = LGBMClassifier(\n    num_iterations=290,\n    objective = \"binary\",\n    feature_pre_filter = False,\n    max_depth = 7\n    )\ndef score(X, y, model_lgbm, cv):\n    scoring = [\"roc_auc\"]\n    scores = cross_validate(\n        model_lgbm, X, y, scoring=scoring, cv=cv, return_train_score=True\n    )\n    scores = pd.DataFrame(scores).T\n    return scores.assign(\n        mean = lambda x: x.mean(axis=1),\n        std = lambda x: x.std(axis=1),\n    )\n\nscores = score(X, y, model_lgbm, cv=fold)\ndisplay(scores)","64dd3649":"model_lgbm.fit(X, y, eval_metric='auc')\nX_test = pd.read_csv(data_dir \/ \"test.csv\", index_col='id')\n\ny_pred_lgbm = pd.Series(\n    model_lgbm.predict_proba(X_test)[:, 1],\n    index=X_test.index,\n    name=TARGET,\n)\ny_pred_lgbm.to_csv(\"submission.csv\")","aab23b62":"## 3. LGBM parameters tuning and modeling.\n\nAll machine learning algorithms have different paramters which can seriously affect to final score. To test LGBM pramters we will use small amout of data to seed up our simlpe research.  \n\nFirst cell is used to create data frames to training and grid search optimization.","86f3c986":"## 2. Utils\n\nThis simple utils is used to reduce memory usage. We have to use it to speed up our code if we work with huge amount of data in our dataset. ","ad9f2e98":"# Discription\n\nThis notebook explain how to create very simple submission with lightgbm optimization.\n\nNotebook plan:\n1. Modules import.\n2. Utils.\n3. LGBM parameters tuning and modeling.\n4. Full model training.","1da76bdc":"The second cell contains the example code: how we can train our lgbm-model and diplay results. ","921b4fb5":"Train final model.","98e69dd0":"## 1. Modules\n\nWe use only gentelmen pack of modules: numpy, pandas, matplotlib, lightgbm and sklearn. ","a49dc91a":"# Discussion\n\n1. After applying the optimization procedure when working with 100,000 test data, the score increased by 0.012, which is a significant result in this task.\n2. The scatter of the score on the grid parameters is 0.02.\n3. Using all data to train with optimized parametrs gives score 0.85212 (+0.002).","d4ac8818":"Attention!\n\nFor 10000 rows num_iter = 210 and max_depth=2 gives score 0.837904 (max)\nFor 100000 rows num_iter = 370 with max_depth = 3 gives score 0.850794 (max)\n\nFinal results depends from your training dataset size.","0019541e":"This cell contains greed search code. We work only with a few parametrs:\n*  max_depth\n*  num_iterations\n\nAll parametrs describe [here](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html)."}}