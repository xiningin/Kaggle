{"cell_type":{"0bc731aa":"code","264630c1":"code","f3ec29c1":"code","3b13108c":"code","4b04dce1":"code","9f5fa4e8":"code","923782a8":"code","3bc09eb3":"code","a700181e":"code","5d79d3fb":"code","540ba04b":"code","a74f2bb7":"code","6367e776":"code","35753a32":"code","30b4bba6":"code","20ee9ded":"code","757ba6fe":"code","cbd9cc0f":"code","1bfd2c69":"code","96656135":"code","f72f4e70":"code","74ba73d5":"code","7c882d51":"code","9b4d5e35":"code","f78d7878":"code","f5ff3063":"code","e2d49d85":"code","e1a3be52":"code","d043e692":"code","08a1e6ec":"code","3e6667dd":"code","4fb6e362":"code","c14ed8ca":"code","19e22692":"code","a62d5625":"code","167a6979":"code","61b9a38d":"code","ce202ef3":"code","0d67bf88":"code","f27eb924":"code","eaa7ed84":"code","b208444b":"code","4a8507b3":"code","80dd7e4d":"code","1c3d3c50":"code","030866b4":"code","ed5536d6":"code","910a2d45":"code","e13c8c8c":"code","c0d94b13":"code","977ab5e6":"code","1ba72975":"code","085295f5":"markdown","c4d12698":"markdown","d30549d2":"markdown","0b5219f4":"markdown","7b62a17b":"markdown","2427dec9":"markdown","7267bc2b":"markdown","48f7de5d":"markdown","0ede3345":"markdown","2e076ac0":"markdown","a7f18ffe":"markdown","3c68f9ee":"markdown","9aba5bce":"markdown","aba35129":"markdown","7d360734":"markdown","d4c83f5a":"markdown","7c974b96":"markdown","114d8902":"markdown","15061524":"markdown","69f20a73":"markdown","cf318e8d":"markdown","52561d04":"markdown","b538cef1":"markdown","637b2ac4":"markdown","cada0916":"markdown","c5ee1166":"markdown","edbe110f":"markdown","e9d1620e":"markdown","40dffcf9":"markdown","1c9d2525":"markdown"},"source":{"0bc731aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","264630c1":"import pandas as pd\nimport numpy as np\nfrom numpy import arange\nimport matplotlib.pyplot as plt\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\n\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    RandomizedSearchCV,\n    cross_val_score,\n    cross_validate,\n    cross_val_predict,\n    train_test_split,\n)\n\nfrom sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\nfrom sklearn.preprocessing import (\n    MinMaxScaler,\n    StandardScaler)\n\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score, make_scorer","f3ec29c1":"df = pd.read_csv(\"..\/input\/quality-prediction-in-a-mining-process\/MiningProcess_Flotation_Plant_Database.csv\")\ndf.head()","3b13108c":"df.info()","4b04dce1":"df.columns = df.columns.str.replace(' ','_')\ndf.columns = df.columns.str.replace('%','percent')\ndf.columns= df.columns.str.lower()\ndf.head()","9f5fa4e8":"# Date is dropped from the dataset.\ndf = df.drop(['date'], axis=1)","923782a8":"for column in df.columns:\n    df[column] = df[column].apply(lambda x: x.replace(',', '.'))\n    \ndf = df.astype('float')    ","3bc09eb3":"df.info()","a700181e":"df.describe()","5d79d3fb":"df.profile_report()","540ba04b":"df.isna().sum()","a74f2bb7":"df[df.duplicated(subset = None, keep = False)]","6367e776":"# drop duplicated records, retain only one copy for each\ndf = pd.DataFrame.drop_duplicates(df)\ndf.shape","35753a32":"cor =df.iloc[:,:23].corr();\nplt.figure(figsize=(23,23))\nsns.set(font_scale=1)\nsns.heatmap(cor, annot=True, cmap=plt.cm.Blues);","30b4bba6":"cor =df.iloc[:,7:14].corr();\nplt.figure(figsize=(7,7))\nsns.set(font_scale=1)\nsns.heatmap(cor, annot=True, cmap=plt.cm.Blues);","20ee9ded":"cor =df.iloc[:,14:21].corr();\nplt.figure(figsize=(7,7))\nsns.set(font_scale=1)\nsns.heatmap(cor, annot=True, cmap=plt.cm.Blues);","757ba6fe":"train_df, test_df = train_test_split(df, test_size=0.3, random_state=111)\ntrain_df.head()","cbd9cc0f":"X_train = train_df.drop(columns=['percent_silica_concentrate'])\ny_train = train_df['percent_silica_concentrate']\n\nX_test = test_df.drop(columns=['percent_silica_concentrate'])\ny_test = test_df['percent_silica_concentrate']","1bfd2c69":"X_train.columns","96656135":"numeric_features = ['percent_iron_feed', 'percent_silica_feed', 'starch_flow', 'amina_flow',\n       'ore_pulp_flow', 'ore_pulp_ph', 'ore_pulp_density',\n       'flotation_column_01_air_flow', 'flotation_column_02_air_flow',\n       'flotation_column_03_air_flow', 'flotation_column_04_air_flow',\n       'flotation_column_05_air_flow', 'flotation_column_06_air_flow',\n       'flotation_column_07_air_flow', 'flotation_column_01_level',\n       'flotation_column_02_level', 'flotation_column_03_level',\n       'flotation_column_04_level', 'flotation_column_05_level',\n       'flotation_column_06_level', 'flotation_column_07_level',\n       'percent_iron_concentrate']","f72f4e70":"numeric_transformer = make_pipeline(StandardScaler())\npreprocessor = make_column_transformer(    \n    (numeric_transformer, numeric_features)    \n)","74ba73d5":"preprocessor.fit(X_train)\npreprocessor.named_transformers_","7c882d51":"dummy = DummyRegressor()\npd.DataFrame(cross_validate(dummy, X_train, y_train, return_train_score=True))","9b4d5e35":"lr = make_pipeline(preprocessor, LinearRegression())\nlr.fit(X_train, y_train);\nlr_preds = lr.predict(X_test)\nlr_preds[:10]","f78d7878":"y_train.max(), y_train.min()","f5ff3063":"lr_preds.max(), lr_preds.min()","e2d49d85":"print(\"Smallest coefficient: \", lr.named_steps['linearregression'].coef_.min()) \nprint(\"Largest coefficient:\", lr.named_steps['linearregression'].coef_.max())","e1a3be52":"lr_ridge = make_pipeline(preprocessor, Ridge())\nlr_ridge.fit(X_train, y_train);","d043e692":"lr_ridge_coefs = pd.DataFrame(data=lr_ridge[1].coef_, index=numeric_features, columns=[\"Coefficient\"])\nlr_ridge_coefs.head(20)","08a1e6ec":"lr_ridge_preds = lr_ridge.predict(X_test)\nlr_ridge_preds[:10]","3e6667dd":"lr_ridge_preds.max(), lr_ridge_preds.min()","4fb6e362":"print(\"Smallest coefficient: \", lr_ridge.named_steps['ridge'].coef_.min()) \nprint(\"Largest coefficient:\", lr_ridge.named_steps['ridge'].coef_.max())","c14ed8ca":"pd.DataFrame(cross_validate(lr_ridge, X_train, y_train, cv=10, return_train_score=True))","19e22692":"alphas = 10.0**np.arange(-5,5,1)\ntrain_scores = []\ncv_scores = []\nfor alpha in alphas:\n    lr_ridge = make_pipeline(preprocessor, Ridge(alpha=alpha))\n    results = cross_validate(lr_ridge, X_train, y_train, return_train_score=True)\n    train_scores.append(np.mean(results[\"train_score\"]))\n    cv_scores.append(np.mean(results[\"test_score\"]))","a62d5625":"plt.semilogx(alphas, train_scores, label=\"train\");\nplt.semilogx(alphas, cv_scores, label=\"cv\");\nplt.legend();\nplt.xlabel('alpha');\nplt.ylabel('score');","167a6979":"best_alpha = alphas[np.argmax(cv_scores)]\nbest_alpha","61b9a38d":"lr_ridge_best = make_pipeline(preprocessor, Ridge(alpha=best_alpha))","ce202ef3":"lr_ridge.fit(X_train, y_train)","0d67bf88":"lr_ridge.score(X_test,y_test)","f27eb924":"rf_reg = RandomForestRegressor(max_depth=2, random_state=0)","eaa7ed84":"rf_reg.fit(X_train, y_train)","b208444b":"pd.DataFrame(cross_validate(rf_reg, X_train, y_train, cv=10, return_train_score=True))","4a8507b3":"lr_tuned = make_pipeline(preprocessor, Ridge(alpha=best_alpha))\nlr_tuned.fit(X_train, y_train);","80dd7e4d":"preds = lr_tuned.predict(X_train)","1c3d3c50":"# Calculating MSE\nnp.mean((y_train - preds)**2)","030866b4":"# Calculating MSE using sklearn library\nfrom sklearn.metrics import mean_squared_error \nmean_squared_error(y_train, preds)","ed5536d6":"np.mean((y_train*100 - preds*100)**2)","910a2d45":"r2_score(y_train, preds)","e13c8c8c":"r2_score(preds, y_train)","c0d94b13":"np.sqrt(mean_squared_error(y_train, lr_tuned.predict(X_train)))","977ab5e6":"plt.scatter(y_train, lr_tuned.predict(X_train), alpha=0.3)\ngrid = np.linspace(y_train.min(), y_train.max(), 1000)\nplt.plot(grid, grid, '--k');\nplt.xlabel(\"true percent silica concentration\");\nplt.ylabel(\"predicte percent silica concentration\");","1ba72975":"plt.hist(y_train, bins=100);","085295f5":"The gap between train and validation scores are similar through all alpha values.","c4d12698":"## 2. EDA","d30549d2":"#### - The numeric data is saved as objects. \n#### - ',' is used for decimal position.","0b5219f4":"The intuition is that larg alphas lead to smaller coefficients which make the model less sensitive with change in features.","7b62a17b":"### Lets focus on the two squares that we observe dark blue squares:","2427dec9":"### 4.3 Ridge\nRidge has the hyperparameter alpha through which we can control fundamental tradeoff.\nUsing basic Ridge model with default alpha.","7267bc2b":"#### - Features Dtype is object however they are numeric so they are changed to numeric.","48f7de5d":"### 4.1 Dummy regressor\n\nCross validation is carried out with Dummy regressor.","0ede3345":"#### - Numeric column names:","2e076ac0":"#### - Creating pipeline","a7f18ffe":"### 5.1 Mean Squared Error (MSE)","3c68f9ee":"## 3. Applying feature transformation.","9aba5bce":"The problem with linear regression is that it predicts out of scale numbers. For example it is predicting negative number for percent silica! The reason for this is the colinearity in features.","aba35129":"### 5.2 $R^2$ \n\nKey points:\n- The maximum is 1 for perfect predictions\n- Negative values are very bad: \"worse than DummyRegressor\" (very bad)\n\nWarning: MSE is \"reversible\" but $R^2$ is not:","7d360734":"### The target '% Silica Concentration' is numeric so we are solving a regression problem. ","d4c83f5a":"#### - Creating X_train, y_train, X_test and y_test","7c974b96":"As it was expected for this baseline modle the score is zero.","114d8902":"## ","15061524":"The test scores are not good!","69f20a73":"#### - Column names have spaces.\n#### - '%' is replaced with 'percent'.\n#### - ',' is used instead of '.' for decimal position pointer.","cf318e8d":"## 4. Model building starting with simplest model.","52561d04":"## 5. Regression Score Functions","b538cef1":"## 1. Importing necessary packages.","637b2ac4":"### 4.4 Ridge with cross_validation","cada0916":"### 5.3 Root mean squared error or RMSE\n\n- The MSE above is in (percent slilica concentration)^2.\n- A more relatable metric would be the root mean squared error, or RMSE","c5ee1166":"### Looking at the correlations we observe that there are high correlations between floatation air flows and floatations column's levels. Having the process flowchart will show how the floatation columns are connected and will help explain this high correlations.","edbe110f":"## 3. Splitting data to train and test set.","e9d1620e":"### 4.5 Random Forest Regressor","40dffcf9":"### The default score for regression problems is $R^2$.","1c9d2525":"### 4.2 Linear Regression\n#### with default hyperparameter"}}