{"cell_type":{"002e4372":"code","c5ccae7a":"code","866a8e21":"code","7f14af46":"code","15642a4c":"code","00b2971e":"code","6d0ef68f":"code","f7d23a45":"code","86e5987f":"code","2914d92b":"code","2e046923":"code","60bcfe1c":"code","82c05f45":"code","c7242462":"code","1c7b0935":"code","a6f573ed":"code","61bf98a0":"code","969de431":"code","2b196d53":"code","a3a6f9df":"code","5b6e964e":"code","b5eeb49c":"code","3f1cd66d":"code","f4690d66":"code","32cd0bf0":"code","9ea21902":"code","0b4a70cc":"code","d27944f4":"code","7f4ead04":"markdown","383bf7c9":"markdown","5b3ac77a":"markdown","f552d253":"markdown","43f56db5":"markdown","0d9fd0f4":"markdown","04795d7e":"markdown","9bec6943":"markdown","8cce8c43":"markdown","08f120b0":"markdown","fcae9209":"markdown","d53c0b0f":"markdown","36ca2630":"markdown","32118fb6":"markdown","800c5764":"markdown","f0c0395a":"markdown","a53e0c4a":"markdown"},"source":{"002e4372":"#Importing library\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","c5ccae7a":"row_data = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")","866a8e21":"row_data.head(5)","7f14af46":"row_data.columns","15642a4c":"row_data.shape","00b2971e":"row_data.info()","6d0ef68f":"row_data.isnull().sum()","f7d23a45":"row_data.drop(\"id\" , axis = 1 , inplace = True)\nrow_data.drop(\"Unnamed: 32\" , axis = 1 , inplace = True)","86e5987f":"row_data.info()","2914d92b":"row_data.tail(5)","2e046923":"sns.countplot(\"diagnosis\" , data = row_data , palette = \"inferno\")\nplt.title(\"Count of diagnosis\")\nplt.show()\nprint(row_data[\"diagnosis\"].value_counts())","60bcfe1c":"fig = plt.figure(figsize = (15,15))\nfig.subplots_adjust(hspace = 0.4 , wspace = 0.4)\nfor i in range(1 , 31):\n    ax = fig.add_subplot(6 , 5 , i)\n    sns.stripplot(x = \"diagnosis\" , y = row_data.columns[i] , data = row_data , palette = \"inferno\" , ax = ax , alpha = 0.5)\nplt.show()","82c05f45":"def _displot(data):\n    fig = plt.figure(figsize=(15,15))\n    fig.subplots_adjust(hspace = 0.4 , wspace = 0.4)\n    \n    for i in range( 0 , len(data.columns)):\n        ax = fig.add_subplot(2,5, i+1 )\n        sns.distplot(data.iloc[:,i])\n    plt.show()\n\ndef _violinplot(data):\n    fig = plt.figure(figsize=(15,15))\n    fig.subplots_adjust(hspace = 0.4 , wspace = 0.4)\n    \n    data = pd.concat([row_data[\"diagnosis\"] , data] , axis = 1)\n    \n    for i in range(1 , len(data.columns)):\n        ax = fig.add_subplot(2,5, i )\n        sns.violinplot(x = \"diagnosis\" , y = data.iloc[:,i] , data = data , inner = \"point\" , palette = \"inferno\")\n    plt.show()\n    \ndef _scatterplot(data):\n    \n    data = pd.concat([row_data[\"diagnosis\"] , data] , axis = 1)\n    \n    g = sns.PairGrid(data , hue = \"diagnosis\" , palette = \"inferno\")\n    g.map_upper(sns.scatterplot)\n    plt.show()\n    \n\ndef _visualization(data):\n    \n    _displot(data)\n    _violinplot(data)\n    _scatterplot(data)","c7242462":"_visualization(row_data.iloc[:,1:11])","1c7b0935":"_visualization(row_data.iloc[:,11:21])","a6f573ed":"_visualization(row_data.iloc[:,21:])","61bf98a0":"categories = row_data[\"diagnosis\"].copy()\ninput_data = row_data.iloc[:,1:].copy()\n\nprint(categories.tail(15))","969de431":"# The categorical values are converted to numerical values\ncategories[categories == \"M\"] = 0\ncategories[categories == \"B\"] = 1\n\nprint(categories.tail(15))","2b196d53":"from sklearn.model_selection import train_test_split\n\nx_train , x_test , y_train , y_test = train_test_split(input_data , categories , random_state = 0 , test_size = 0.33)\n\nprint(\"x_train shape:\" , x_train.shape)\nprint(\"y_train shape:\" , y_train.shape)\nprint(\"x_test shape:\" , x_test.shape)\nprint(\"y_test shape:\" , y_test.shape)","a3a6f9df":"from sklearn.preprocessing import StandardScaler\n\nst = StandardScaler()\n\nx_train = st.fit_transform(x_train)\nx_test = st.transform(x_test)\n\nprint(x_train[1,:5] , x_train.shape)\nprint(x_test[1,:5] , x_test.shape)","5b6e964e":"x_train = x_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\n\ny_train = y_train.astype(np.float32)\ny_test = y_test.astype(np.float32)","b5eeb49c":"from keras import models , layers\n\ndef build_model(hl , lt):\n    model = models.Sequential()\n    model.add(layers.Dense(hl , activation = \"relu\" , input_shape = (x_train.shape[1] , )))\n    \n    for i in range(1 , lt):\n        model.add(layers.Dense(hl , activation = \"relu\"))\n        \n    model.add(layers.Dense(1 , activation = \"sigmoid\"))\n    model.compile(optimizer=\"rmsprop\" , loss = \"binary_crossentropy\" , metrics = [\"accuracy\"] )\n    \n    return model\n    ","3f1cd66d":"def k_fold(k , num_epochs , hl , lt):\n    all_acc = []\n    all_loss = []\n    \n    all_val_acc = []\n    all_val_loss = []\n    \n    num_val_sample = len(x_train) \/\/ k\n    \n    for i in range(k):\n        print(\"Process\" , i+1)\n        \n        val_data = x_train[i*num_val_sample : (i+1)*num_val_sample]\n        val_target = y_train[i*num_val_sample : (i+1)*num_val_sample]\n        \n        partial_train_data = np.concatenate([x_train[:i*num_val_sample] , x_train[(i+1)*num_val_sample :]] , axis = 0)\n        partial_train_target = np.concatenate([y_train[:i*num_val_sample] , y_train[(i+1)*num_val_sample :]] , axis = 0)\n        \n        model = build_model(hl , lt)\n        \n        history = model.fit(partial_train_data,\n                            partial_train_target, \n                            epochs = num_epochs, \n                            batch_size = 15, \n                            verbose = 0, \n                            validation_data = (val_data , val_target))\n        \n        all_loss.append(history.history[\"loss\"])\n        all_acc.append(history.history[\"accuracy\"])\n        \n        all_val_loss.append(history.history[\"val_loss\"])\n        all_val_acc.append(history.history[\"val_accuracy\"])\n        \n    \n    average_loss = [np.mean([j[i] for j in all_loss]) for i in range(num_epochs)]\n    average_acc = [np.mean([j[i] for j in all_acc]) for i in range(num_epochs)]\n    \n    average_val_loss = [np.mean([j[i] for j in all_val_loss]) for i in range(num_epochs)]\n    average_val_acc = [np.mean([j[i] for j in all_val_acc]) for i in range(num_epochs)]\n    \n    return{\"loss\" : average_loss,\n           \"accuracy\" : average_acc,\n           \"val_loss\" : average_val_loss,\n           \"val_accuracy\" : average_val_acc}","f4690d66":"def Draw(epochs , model):\n    \n    plt.title(\"Val and Train Accuracy\")\n    plt.plot(range(1 , epochs+1) , model[\"accuracy\"] , \"bo\" , label = \"Train Acc\")\n    plt.plot(range(1 , epochs+1) , model[\"val_accuracy\"] , \"r\" , label = \"Val Acc\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.show()\n    \n    plt.title(\"Val and Train Loss\")\n    plt.plot(range(1 , epochs+1) , model[\"loss\"] , \"bo\" , label = \"Train Loss\")\n    plt.plot(range(1 , epochs+1) , model[\"val_loss\"] , \"r\" , label = \"Val Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()","32cd0bf0":"k_model = k_fold(4,100,32,3)\nDraw(100 , k_model)","9ea21902":"k_model = k_fold(4,100,16,2)\nDraw(100,k_model)","0b4a70cc":"k_model = k_fold(4,30,16,2)\nDraw(30,k_model)","d27944f4":"last_model = build_model(16,2)\nlast_model.fit(x_train , y_train , epochs = 12 , batch_size = 15)\nresults = last_model.evaluate(x_test , y_test)\nprint(results)","7f4ead04":"## Train - Test - Split\n---","383bf7c9":"## Loading dataset\n---","5b3ac77a":"## Controling type of entries","f552d253":"### Visualization the MEAN columns\n---","43f56db5":"* **The parameters cause overfitting**\n* **The model will be trained with less parameters.**","0d9fd0f4":"## Visualization the dataset\n---\n\n### Count of entries","04795d7e":"### Visualization the SE columns\n---","9bec6943":"**The dataset has 569 row and 33 columns**","8cce8c43":"## Building Keras Model\n---","08f120b0":"## Building K-Fold\n---\n### Why k-fold?\n* The dataset must be splitted 3 pieces: training, validation and test. When the dataset splits, it will have 381 row for training. And then, the training dataset will be splitted 2 pieces for training and validation. After all of this process, there will be relatively few data to train the ML model.\n* This situation can cause different validation score, depending on which data  will be validation data.\n* The validation score will have high variance, according to validation dataset.\n* K-Fold is used to avoid this situation.\n\n### How does It work?\n1. Determine a K values.\n2. Split dataset into K parts.\n3. Determine a piece for validation, and use others(k-1 pieces) for training.\n4. Train ML model with k-1 pieces, and Test it with validation dataset.\n5. Save the validation score.\n6. Repeat 3rd step to 5th step, till every pieces is used for validation.\n7. Finally, Calculate average the validation scores which are saved in 5 step.\n8. The average validation score show success of the Ml model.\n","fcae9209":"## Scaling Processing\n---","d53c0b0f":"**Id and Unnamed: 32(it has a lot of null values) will be droped from the dataset.**","36ca2630":"## Examine the data set\n---","32118fb6":"* **The model cause overfitting, after 12th epochs.**\n* **Let's create new model and test it on the test dataset, after It trains.**","800c5764":"* **\"diagnosis\" column has catecorical value.**\n* **Apart from \"diagnosis\" has numerical value.**","f0c0395a":"* **This looks like better.**\n* **Let's reduce the epochs to see better.**","a53e0c4a":"### Visualization the WORST columns"}}