{"cell_type":{"8dda390a":"code","62ae6d8a":"code","bd843eb3":"code","26daf7b7":"code","a3f07a5b":"code","f2b81f49":"code","0165f90d":"code","858cbd2f":"code","72683eb3":"code","6f3c7d9f":"code","873b4067":"code","a33f7d83":"code","d414e544":"code","01ef9338":"code","4a54d828":"code","91c5ac89":"code","9c596f09":"code","928e72db":"code","3bc6f889":"code","2a3cc173":"code","61514766":"code","29431a08":"code","8e629870":"code","a3a96865":"code","d5849576":"code","b5e96e3f":"code","089a04e0":"code","88b32060":"code","3aaaafbd":"code","cc42478e":"code","bc089b61":"code","6ef8da16":"code","68c0320d":"code","cee2b73e":"code","84e6ed47":"code","fbd3c06c":"code","e4b9e90c":"code","5117e085":"code","ae658559":"code","38fe2a1c":"code","29623bfb":"code","1a39a7fa":"code","f9ae2591":"code","a038bdbd":"code","68e9f521":"markdown","8da60b26":"markdown","521a7d62":"markdown","0b1e0004":"markdown","5dde0cf3":"markdown","b3c35867":"markdown","cf8f0743":"markdown","2098bd7e":"markdown","ae3e8b74":"markdown","d3d9894e":"markdown","94c2e8bc":"markdown","56de6af2":"markdown","f33836b9":"markdown","80f191e5":"markdown","999dbfbc":"markdown","f39361ca":"markdown","ddb95289":"markdown","b7fc8d23":"markdown","5fc00207":"markdown","4487f412":"markdown","f4972b2b":"markdown","7612e95a":"markdown","c7bcb306":"markdown","340fa688":"markdown","bcb038e7":"markdown","bc84704f":"markdown","e1077fb0":"markdown","f545e3f5":"markdown","1d32541d":"markdown"},"source":{"8dda390a":"# Import Libraries\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib\nimport sklearn\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\nimport optuna\nfrom optuna.integration import lightgbm\n\n\nimport pickle\nimport os\n\n# Print versions of libraries\nprint(f\"Numpy version : Numpy {np.__version__}\")\nprint(f\"Pandas version : Pandas {pd.__version__}\")\nprint(f\"Matplotlib version : Matplotlib {matplotlib.__version__}\")\nprint(f\"Seaborn version : Seaborn {sns.__version__}\")\nprint(f\"SkLearn version : SkLearn {sklearn.__version__}\")\n\n# Magic Functions for In-Notebook Display\n%matplotlib inline\n\n# Setting seabon style\nsns.set(style='darkgrid', palette='colorblind')\nplt.show()\n\n# To see all column names & rows without being truncated\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","62ae6d8a":"train = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv', encoding='latin_1')\ntest  = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv', encoding='latin_1')\nsub = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv', encoding='latin_1')","bd843eb3":"train.head()","26daf7b7":"test.head()","a3f07a5b":"# Ref : https:\/\/www.kaggle.com\/kirillklyukvin\/playground-series-february-21\n\ndef simple_eda(df):\n    \n    \"\"\"\n    This function helps us with simple data analysis.\n    We may explore the common information about the dataset, missing values, features distribution and duplicated rows\n    \"\"\"\n    \n    # applying info() method\n    print('---')\n    print('Common information')\n    print('---')\n    print()\n    print(df.info())\n    \n    # missing values\n    print()\n    print('---')\n    if df.isna().sum().sum() == 0:\n        print('There are no missing values')\n        print('---')\n    else:\n        print('Detected')\n        display(df.isna().sum())\n    \n    \n    # applying describe() method for categorical features\n    print()\n    print('---')\n    print('Categorical columns')\n    print('Total {}'.format(len(df.select_dtypes(include='object').columns)))\n    print('---')\n    display(df.describe(include = 'object'))\n    \n    # same describe() but for continious features\n    print('---')\n    print('Continuous columns')\n    print('Total {}'.format(len(df.select_dtypes(include=['int', 'float']).columns)))\n    print('---')\n    display(df.describe())\n    \n    #checking for duplicated rows\n    if df.duplicated().sum() == 0:\n        print('---')\n        print('There are no duplicates')\n        print('---')\n    else:\n        print('---')\n        print('Duplicates found')\n        print('---')\n        display(df[df.duplicated()])\n    \n    print()\n    print('---')\n    print('End of the report')","f2b81f49":"simple_eda(train)","0165f90d":"simple_eda(test)","858cbd2f":"categorical_cols=['cat'+str(i) for i in range(10)]\ncontinous_cols=['cont'+str(i) for i in range(14)]\n\nprint(\"Categorical columns\", categorical_cols)\nprint(\"Continous columns\", continous_cols)","72683eb3":"train.columns","6f3c7d9f":"# 'id' is just unique key column, and it will not helpfull for model building\nColsToReject=['id']\n\ntrain.drop(ColsToReject, axis=1,inplace=True)\ntest.drop(ColsToReject, axis=1,inplace=True)\n\ntrain.head()","873b4067":"plt.figure(figsize=(10,5))\nsns.histplot(train['target'])\nplt.show()","a33f7d83":"plt.figure(figsize=(8,6))\nsns.boxplot(data = train, x='target')\nplt.show()","d414e544":"# Create a function to return the outliers\ndef detect_outliers(x, c = 1.5):\n    \"\"\"\n    Function to detect outlivers.\n    \"\"\"\n    q1, q3 = np.percentile(x, [25,75])\n    #print(\"q1 - \",q1, \" q3 - \", q3)\n    \n    iqr = (q3 - q1)\n    #print(\"iqr --\", iqr)\n    \n    lob = q1 - (iqr * c)\n    #print(\"lob - \",lob)\n    \n    uob = q3 + (iqr * c)\n    #print(\"uob - \",uob)\n    \n    # Generate outliers\n    indicies = np.where((x > uob) | (x < lob))\n\n    return indicies\n\n\n# Detect all Outliers \npriceOutliers = detect_outliers(train['target'])\nprint(\"Total Outliers count : \",len(priceOutliers[0]))\n\nprint(\"Shape before removing outliers : \",train.shape)\n\n# Remove outliers\ntrain.drop(priceOutliers[0],inplace=True)\n\nprint(\"Shape after removing outliers : \",train.shape)","01ef9338":"plt.figure(figsize=(10,5))\nsns.histplot(train['target'])\nplt.show()","4a54d828":"plt.figure(figsize=(8,6))\nsns.boxplot(data = train, x='target')\nplt.show()","91c5ac89":"# Plotting multiple bar charts at once for categorical variables\ndef PlotBarCharts(inpData, colsToPlot, rows, cols):\n    idx = 0\n    f, axes = plt.subplots(rows,cols, sharex=True, figsize=(12,20))\n    plt.suptitle('Categorical features distribution', size=16, y=(0.94))\n\n    for row in range(rows):\n        for col in range(cols):\n            data = inpData[colsToPlot[idx]].value_counts()\n            sns.barplot(x = data.values, y = data.index, palette='deep', ax=axes[row, col])\n            axes[row,col].set_title(colsToPlot[idx])\n            idx += 1","9c596f09":"# Calling the function\nPlotBarCharts(inpData=train, colsToPlot=categorical_cols, rows=5, cols=2)","928e72db":"# Plotting histograms of multiple columns together\ntrain.hist(continous_cols, figsize=(18,16))\nplt.show()","3bc6f889":"train.corr()['target'].sort_values(ascending=False).head(10)","2a3cc173":"mask = np.zeros_like(train[continous_cols].join(train['target']).corr(), dtype=np.bool) \nmask[np.triu_indices_from(mask)] = True \n\nf, ax = plt.subplots(figsize=(10,10))\nplt.title('Pearson Correlation Matrix',fontsize=25)\n\nsns.heatmap(train[continous_cols].join(train['target']).corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"viridis\",\n            linecolor='w',annot=True,annot_kws={\"size\":8},mask=mask,cbar_kws={\"shrink\": .9})\n\nplt.show()","61514766":"# Defining a function to find the statistical relationship with all the categorical variables\ndef FunctionAnova(inpData, TargetVariable, CategoricalPredictorList):\n    from scipy.stats import f_oneway\n\n    # Creating an empty list of final selected predictors\n    SelectedPredictors=[]\n    \n    print('##### ANOVA Results ##### \\n')\n    for predictor in CategoricalPredictorList:\n        CategoryGroupLists=inpData.groupby(predictor)[TargetVariable].apply(list)\n        AnovaResults = f_oneway(*CategoryGroupLists)\n        \n        # If the ANOVA P-Value is <0.05, that means we reject H0\n        if (AnovaResults[1] < 0.05):\n            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n#             SelectedPredictors.append(predictor)\n        else:\n            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n    \n#     return(SelectedPredictors)","29431a08":"# Calling the function to check which categorical variables are correlated with target\nFunctionAnova(inpData=train, \n              TargetVariable='target', \n              CategoricalPredictorList=categorical_cols)","8e629870":"train.head()","a3a96865":"for e in categorical_cols:\n    le = LabelEncoder()\n    train[e]=le.fit_transform(train[e])\n    test[e]=le.transform(test[e])\n    \n\n    \n# # Treating all the nominal variables at once using dummy variables\n# # DataForML_Numeric=pd.get_dummies(DataForML)\n\n# train = pd.get_dummies(train)\n# test = pd.get_dummies(test)\n\n# Printing sample rows\ntrain.head()","d5849576":"train.columns","b5e96e3f":"# Separate Target Variable and Predictor Variables\n# TargetVariable = 'target'\n# Predictors = categorical_cols+continous_cols\n\n# Separate Target Variable and Predictor Variables\nTargetVariable = 'target'\nPredictors = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6',\n       'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\n\n# X = trainDataForML[Predictors].values\n# y = trainDataForML[TargetVariable].values\n\nX = train[Predictors]\ny = train[TargetVariable]\n\n\nX_test = test.copy()\n# y_test = testDataForML['target']\n\n# Quick sanity check with the shapes of Training and testing datasets\nprint(\"X - \",X.shape)\nprint(\"y - \",y.shape)\nprint(\"X_test - \",X_test.shape)\n# print(\"y_test - \",y_test.shape)","089a04e0":"scaler = MinMaxScaler()\nX = scaler.fit_transform(X)","88b32060":"playgroundDataCleaned = X\n\n# Saving the Python objects as serialized files can be done using pickle library\n# Here let us save the Final Data set after all the transformations as a file\nwith open('playgroundDataCleaned.pkl', 'wb') as fileWriteStream:\n    pickle.dump(playgroundDataCleaned, fileWriteStream)\n    # Don't forget to close the filestream!\n    fileWriteStream.close()\n    \nprint('pickle file is saved at Location:',os.getcwd())","3aaaafbd":"# Split the data into training and testing set\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.20, shuffle=True, random_state=48)\n\n# Quick sanity check with the shapes of Training and testing datasets\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_validation - \",X_validation.shape)\nprint(\"y_validation - \",y_validation.shape)","cc42478e":"# !pip install optuna ","bc089b61":"# Ref : https:\/\/www.kaggle.com\/hamzaghanmi\/lgbm-hyperparameter-tuning-using-optuna\n\ndef objective(trial,data=X,target=y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2,random_state=48)\n    param = {\n        'metric': 'rmse', \n        'random_state': 48,\n        'n_estimators': 1000,\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.006,0.008,0.01,0.014,0.017,0.02]),\n        'max_depth': trial.suggest_categorical('max_depth', [10,20,100]),\n        'num_leaves' : trial.suggest_int('num_leaves', 1, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'cat_smooth' : trial.suggest_int('min_data_per_groups', 1, 100)\n    }\n    model = LGBMRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","6ef8da16":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=22)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint(\"\\n\")","68c0320d":"study.trials_dataframe()","cee2b73e":"#plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point.\noptuna.visualization.plot_optimization_history(study)","84e6ed47":"#Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","fbd3c06c":"#Visualize empirical distribution function\noptuna.visualization.plot_edf(study)","e4b9e90c":"params=study.best_params \nprint(\"Best Parameter : \", params)","5117e085":"params['random_state'] = 48\nparams['n_estimators'] = 20000 \nparams['metric'] = 'rmse'\n\n# Changed min_data_per_groups to cat_smooth beacuse there is no parameter named min_data_per_groups in LGBM.\nparams['cat_smooth'] = params.pop('min_data_per_groups')","ae658559":"print(\"Best Parameter : \", params)","38fe2a1c":"X_test.head()","29623bfb":"columns = categorical_cols+continous_cols\npreds = np.zeros(test.shape[0])\nkf = KFold(n_splits=5,random_state=48,shuffle=True)\nrmse=[]  # list contains rmse for each fold\nn=0\nfor trn_idx, test_idx in kf.split(train[columns],train['target']):\n    X_tr,X_val=train[columns].iloc[trn_idx],train[columns].iloc[test_idx]\n    y_tr,y_val=train['target'].iloc[trn_idx],train['target'].iloc[test_idx]\n    model = LGBMRegressor(**params)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    preds+=model.predict(test[columns])\/kf.n_splits\n    rmse.append(mean_squared_error(y_val, model.predict(X_val), squared=False))\n    print(n+1,rmse[n])\n    n+=1","1a39a7fa":"np.mean(rmse)","f9ae2591":"lightgbm.plot_importance(model, max_num_features=10, figsize=(10,10))\nplt.show()","a038bdbd":"sub['target']=preds\nsub.to_csv('submission.csv', index=False)","68e9f521":"## Scaling","8da60b26":"All categorical variables are correlated with the Target variable, so we will keep all categorical features for model building.","521a7d62":"We have a few multicollinear columns. ","0b1e0004":"## Feature Selection\nNow its time to finally choose the best columns(Features) which are correlated to the Target variable.\nThis can be done directly by measuring the correlation values or ANOVA\/Chi-Square tests. However, it is always helpful to visualize the relation between the Target variable and each of the predictors to get a better sense of data.","5dde0cf3":"### Exploring categorical features","b3c35867":"## Exploratory Data Analysis\nWe're going to consider the features in the dataset and how complete they are. ","cf8f0743":"### Delete the useless columns\n\nDeleting those columns which are not useful in predictive analysis because these variables are qualitative","2098bd7e":"## Read in and Explore the Data \nIt's time to read in our training and testing data using `pd.read_csv`, and take a first look at the training data using the `describe()` function.","ae3e8b74":"### EDA for Test set","d3d9894e":"## Encode categorical features\nData Pre-processing for Machine Learning","94c2e8bc":"## Submission","56de6af2":"### Distribution of Target\n\n* If target variable's distribution is too skewed then the predictive modeling will not be possible.\n* Bell curve is desirable but slightly positive skew or negative skew is also fine\n* When performing Regression, make sure the histogram looks like a bell curve or slight skewed version of it. **Otherwise it impacts the Machine Learning algorithms ability to learn all the scenarios.**","f33836b9":"## Visual Exploratory Data Analysis","80f191e5":"### Exploring Continous features","999dbfbc":"## Import Necessary Libraries\nFirst off, we need to import several Python libraries such as numpy, pandas, matplotlib and seaborn.","f39361ca":"## Seperate Categorical & Continous features","ddb95289":"### Statistical Feature Selection (Categorical Vs Continuous) using ANOVA test\nAnalysis of variance(ANOVA) is performed to check if there is any relationship between the given continuous and categorical variable\n* Assumption(H0): There is NO relation between the given variables (i.e. The average(mean) values of the numeric Target variable is same for all the groups in the categorical Predictor variable)\n* ANOVA Test result: Probability of H0 being true","b7fc8d23":"### Correlation Among Explanatory Variables\n\nHaving too many features in a model is not always a good thing because it might cause overfitting and worser results when we want to predict values for a new dataset. Thus, if a feature does not improve your model a lot, not adding it may be a better choice.\n\nLets find out top 10 features which are highly correlaed with target.","5fc00207":"#### Histogram Interpretation\nHistograms shows us the data distribution for a single continuous variable.\n\nThe X-axis shows the range of values and Y-axis represent the number of values in that range. For example, in the above histogram of \"cont4\", there are around 120000 rows in data that has a value between 0.3 to 0.4.\n\nThe ideal outcome for histogram is a bell curve or slightly skewed bell curve. If there is too much skewness, then outlier treatment should be done and the column should be re-examined, if that also does not solve the problem then only reject the column.","4487f412":"## Saving preprossed data as serialized files\n\nTo deploy the predictive models built we save them along with the required data files as serialized file objects\nWe save cleaned and processed input data, tuned predictive models as files so that they can later be re-used\/shared","f4972b2b":"### Splitting data into Training and Validation samples","7612e95a":"### Find and treat Outliers","c7bcb306":"## Reference:\n\n* https:\/\/www.kaggle.com\/hamzaghanmi\/lgbm-hyperparameter-tuning-using-optuna#Let's-do-some-Quick-Visualization-for-Hyperparameter-Optimization-Analysis\n* https:\/\/www.kaggle.com\/kirillklyukvin\/playground-series-february-21","340fa688":"## Quick Visualization for Hyperparameter Optimization Analysis\n\nOptuna provides various visualization features in optuna.visualization to analyze optimization results visually","bcb038e7":"### EDA for Train set","bc84704f":"<p style=\"font-weight: bold;color:#FF4500\">Highlights<\/p>\n\n* Dataset comprises of 300000 observations and 26 fields.\n\n* Feature 'Target' is the response variable and it takes continous values.\n\n* Features 'cat0' to 'cat9' are categorical and features 'cont0' to 'cont13' are continues values. \n\n* Feature 'id' is unique values and useless, it will be removed later on.\n\n* There are no missing values present in the dataset. ","e1077fb0":"##  If you find this notebook helpful, please upvote it.","f545e3f5":"## LGBMRegressor model with the best hyperparameters","1d32541d":"## Model Building\n\nThe below function uses LGBMRegressor model, takes\n* the data\n* the target\n* trial(How many executions we will do) and returns\n* RMSE(Root Mean Squared Rrror)"}}