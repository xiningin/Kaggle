{"cell_type":{"f025a0c0":"code","d8446d43":"code","2a5d39cc":"code","5cdfb1e1":"code","1d1871fe":"code","b0186497":"code","47428c35":"code","9bd3933d":"code","7c6f1950":"code","e5e0506c":"code","b9a0e622":"code","e4b48e5a":"code","0ec4db21":"code","ecf6d2c8":"code","6ed1448d":"code","ccceca72":"code","bf1fdb81":"code","27992f09":"code","e7375652":"code","6d1f86cd":"code","2700dcd7":"code","ddf877b0":"code","ba4db4d0":"code","978c8baf":"code","bf4e3a11":"code","ac255c1e":"code","319c4c97":"code","4f144ca8":"code","f74b318b":"code","4c68dfdf":"code","8c8f4c3f":"code","fc7eb07d":"code","9bd509cd":"code","e29255d6":"code","05fed1e1":"code","5f9aef0e":"markdown","0a9dadc5":"markdown","0b8572d9":"markdown","f99aefff":"markdown","a25e831c":"markdown","45370030":"markdown","60ceea04":"markdown"},"source":{"f025a0c0":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import combinations\nimport random\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\nimport time\nfrom tqdm import tqdm\nimport warnings\nwarnings.simplefilter('ignore')\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","d8446d43":"train_X = pd.read_csv('..\/input\/X_train.csv').iloc[:,3:].values.reshape(-1,128,10)\ntest_X  = pd.read_csv('..\/input\/X_test.csv' ).iloc[:,3:].values.reshape(-1,128,10)\nprint('train_X shape:', train_X.shape, ', test_X shape:', test_X.shape)","2a5d39cc":"df_train_y = pd.read_csv('..\/input\/y_train.csv')\n\n# build a dict to convert surface names into numbers\nsurface_names = df_train_y['surface'].unique()\nnum_surfaces = len(surface_names)\nsurface_to_numeric = dict(zip(surface_names, range(num_surfaces)))\nprint('Convert to numbers: ', surface_to_numeric)\n\n# y and group data as numeric values:\ntrain_y = df_train_y['surface'].replace(surface_to_numeric).values\ntrain_group = df_train_y['group_id'].values","5cdfb1e1":"def sq_dist(a,b):\n    ''' the squared euclidean distance between two samples '''\n    \n    return np.sum((a-b)**2, axis=1)\n\n\ndef find_run_edges(data, edge):\n    ''' examine links between samples. left\/right run edges are those samples which do not have a link on that side. '''\n\n    if edge == 'left':\n        border1 = 0\n        border2 = -1\n    elif edge == 'right':\n        border1 = -1\n        border2 = 0\n    else:\n        return False\n    \n    edge_list = []\n    linked_list = []\n    \n    for i in range(len(data)):\n        dist_list = sq_dist(data[i, border1, :4], data[:, border2, :4]) # distances to rest of samples\n        min_dist = np.min(dist_list)\n        closest_i   = np.argmin(dist_list) # this is i's closest neighbor\n        if closest_i == i: # this might happen and it's definitely wrong\n            print('Sample', i, 'linked with itself. Next closest sample used instead.')\n            closest_i = np.argsort(dist_list)[1]\n        dist_list = sq_dist(data[closest_i, border2, :4], data[:, border1, :4]) # now find closest_i's closest neighbor\n        rev_dist = np.min(dist_list)\n        closest_rev = np.argmin(dist_list) # here it is\n        if closest_rev == closest_i: # again a check\n            print('Sample', i, '(back-)linked with itself. Next closest sample used instead.')\n            closest_rev = np.argsort(dist_list)[1]\n        if (i != closest_rev): # we found an edge\n            edge_list.append(i)\n        else:\n            linked_list.append([i, closest_i, min_dist])\n            \n    return edge_list, linked_list\n\n\ndef find_runs(data, left_edges, right_edges):\n    ''' go through the list of samples & link the closest neighbors into a single run '''\n    \n    data_runs = []\n\n    for start_point in left_edges:\n        i = start_point\n        run_list = [i]\n        while i not in right_edges:\n            tmp = np.argmin(sq_dist(data[i, -1, :4], data[:, 0, :4]))\n            if tmp == i: # self-linked sample\n                tmp = np.argsort(sq_dist(data[i, -1, :4], data[:, 0, :4]))[1]\n            i = tmp\n            run_list.append(i)\n        data_runs.append(np.array(run_list))\n    \n    return data_runs","1d1871fe":"train_left_edges, train_left_linked  = find_run_edges(train_X, edge='left')\ntrain_right_edges, train_right_linked = find_run_edges(train_X, edge='right')\nprint('Found', len(train_left_edges), 'left edges and', len(train_right_edges), 'right edges.')","b0186497":"train_runs = find_runs(train_X, train_left_edges, train_right_edges)","47428c35":"flat_list = [series_id for run in train_runs for series_id in run]\nprint(len(flat_list), len(np.unique(flat_list)))","9bd3933d":"df_train_y['run_id'] = 0\ndf_train_y['run_pos'] = 0\n\nfor run_id in range(len(train_runs)):\n    for run_pos in range(len(train_runs[run_id])):\n        series_id = train_runs[run_id][run_pos]\n        df_train_y.at[ series_id, 'run_id'  ] = run_id\n        df_train_y.at[ series_id, 'run_pos' ] = run_pos\n\ndf_train_y.to_csv('y_train_with_runs.csv', index=False)\ndf_train_y.tail()","7c6f1950":"test_left_edges, test_left_linked  = find_run_edges(test_X, edge='left')\ntest_right_edges, test_right_linked = find_run_edges(test_X, edge='right')\nprint('Found', len(test_left_edges), 'left edges and', len(test_right_edges), 'right edges.')","e5e0506c":"test_runs = find_runs(test_X, test_left_edges, test_right_edges)","b9a0e622":"lost_samples = np.array([ i for i in range(len(test_X)) if i not in np.concatenate(test_runs) ])\nprint(lost_samples)\nprint(len(lost_samples))","e4b48e5a":"find_run_edges(test_X[lost_samples], edge='left')[1][0]","0ec4db21":"lost_run = np.array(lost_samples[find_runs(test_X[lost_samples], [0], [5])[0]])\ntest_runs.append(lost_run)","ecf6d2c8":"df_test_y = pd.read_csv(\"..\/input\/sample_submission.csv\")\ndf_test_y['run_id'] = 0\ndf_test_y['run_pos'] = 0\n\nfor run_id in range(len(test_runs)):\n    for run_pos in range(len(test_runs[run_id])):\n        series_id = test_runs[run_id][run_pos]\n        df_test_y.at[ series_id, 'run_id'  ] = run_id\n        df_test_y.at[ series_id, 'run_pos' ] = run_pos\n\ndf_test_y.to_csv('y_test_with_runs.csv', index=False)\n\ndf_test_y.drop(\"surface\", axis=1, inplace=True)\n\ncheat_json = df_train_y.groupby(['run_id'])['surface'].unique().reset_index().to_dict()\ndf_test_y['surface'] = df_test_y['run_id'].apply(lambda x: cheat_json['surface'][x][0])\ndf_test_y.head()","6ed1448d":"%%time\ncounter = 0\nagg_dict = df_train_y.groupby(['run_id'])['series_id'].unique().reset_index()['series_id'].to_dict()\ntwo_sampled_dict = {}\nfor key, value in agg_dict.items():\n    two_sampled_dict[key] = []\n#     for item in list(combinations(agg_dict[key].tolist(), 2)):\n    llist = list(combinations(agg_dict[key].tolist(), 2))\n    if len(llist) > 50:\n        two_sampled_dict[key] = random.sample(llist, 50)\n        counter += 50\n    else:\n        two_sampled_dict[key] = random.sample(llist, len(llist))\n#         two_sampled_dict[key].append(item)\n        counter += len(llist)\nprint(counter)\ndel llist\ndel counter","ccceca72":"train = pd.read_csv(\"..\/input\/X_train.csv\")\ntest = pd.read_csv(\"..\/input\/X_test.csv\")\nlabel = pd.read_csv(\"..\/input\/y_train.csv\")\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\nlabel = reduce_mem_usage(label)","bf1fdb81":"le = LabelEncoder()\nlabel['surface'] = le.fit_transform(label['surface'])\nprint(le.classes_)","27992f09":"train.drop(['row_id', 'measurement_number'], axis=1, inplace=True)\ntest.drop(['row_id', 'measurement_number'], axis=1, inplace=True)","e7375652":"start_time = time.time()\n\nnew_train = train.copy()\nprint(\"Initial Train Size :: \", new_train.shape)\n\nnew_label = {}\nlast_series_id = 3810\n# for item in range(149205):\n\nfor key, value in tqdm(two_sampled_dict.items(), total=len(two_sampled_dict)):\n# for key, value in two_sampled_dict.items():\n    \n    for item in value:\n        \n        idx1 = item[0]\n        idx2 = item[1]\n        \n        df = pd.DataFrame(columns=train.columns)\n\n        # Creating Train\n        for col in df.columns[1: ]:\n            df[col] = new_train[col][(new_train['series_id'] == idx1) | (new_train['series_id'] == idx2)]\n        df['series_id'] = last_series_id\n        \n        df.reset_index(inplace=True)\n        df.drop(['index'], axis=1, inplace=True)\n\n        # Creating in Label\n        new_label[last_series_id] = df_train_y['surface'][(df_train_y['series_id'] == idx1) | (df_train_y['series_id'] == idx2)].value_counts(ascending=False).index[0]\n        last_series_id += 1\n        \n        new_train = pd.concat([new_train, df], ignore_index=True)\n        \nprint(\"Final Train Size :: \", new_train.shape)\nprint(\"Time Taken :: \", time.time() - start_time)","6d1f86cd":"def FE(data):\n    \n    df = pd.DataFrame()\n    data['totl_anglr_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 +\n                             data['angular_velocity_Z']**2)** 0.5\n    data['totl_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 +\n                             data['linear_acceleration_Z']**2)**0.5\n#     data['totl_xyz'] = (data['orientation_X']**2 + data['orientation_Y']**2 +\n#                              data['orientation_Z'])**0.5\n   \n    data['acc_vs_vel'] = data['totl_linr_acc'] \/ data['totl_anglr_vel']\n    \n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number', 'orientation_X', 'orientation_Y', 'orientation_Z', 'orientation_W']:\n            continue\n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n        df[col + '_maxtoMin'] = df[col + '_max'] \/ df[col + '_min']\n        df[col + '_mean_abs_chg'] = data.groupby(['series_id'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])\/2\n    return df","2700dcd7":"%%time\nnew_train = FE(new_train)\ntest = FE(test)\nprint(new_train.shape, test.shape)","ddf877b0":"new_train.fillna(0, inplace = True)\ntest.fillna(0, inplace = True)\nnew_train.replace(-np.inf, 0, inplace = True)\nnew_train.replace(np.inf, 0, inplace = True)\ntest.replace(-np.inf, 0, inplace = True)\ntest.replace(np.inf, 0, inplace = True)","ba4db4d0":"def k_folds(clf, X, y, X_test, k):\n    folds = StratifiedKFold(n_splits = k, shuffle=True, random_state=13)\n    y_test = np.zeros((X_test.shape[0], 9))\n    y_oof = np.zeros((X.shape[0]))\n    score = 0\n    for i, (train_idx, val_idx) in  enumerate(folds.split(X, y)):\n#         clf =  RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n        clf.fit(X.iloc[train_idx], y[train_idx])\n        y_oof[val_idx] = clf.predict(X.iloc[val_idx])\n        y_test += clf.predict_proba(X_test) \/ folds.n_splits\n        score += clf.score(X.iloc[val_idx], y[val_idx])\n        print('Fold: {} score: {}'.format(i,clf.score(X.iloc[val_idx], y[val_idx])))\n    print('Avg Accuracy', score \/ folds.n_splits) \n        \n    return y_oof, y_test","978c8baf":"new_label = pd.DataFrame.from_dict(new_label, orient='index').reset_index()\nnew_label.columns = ['series_id', 'surface']\nnew_label.head()","bf4e3a11":"label = pd.read_csv(\"..\/input\/y_train.csv\")\nlabel = pd.concat([label, new_label], ignore_index=True)\nlabel.head()","ac255c1e":"label['surface'] = le.transform(label['surface'])\nlabel.surface.head()","319c4c97":"rand = RandomForestClassifier(n_estimators=500, random_state=13)\ny_oof, y_test_rand = k_folds(rand, new_train, label['surface'], test, k=5)","4f144ca8":"ext = ExtraTreesClassifier(n_estimators=500, random_state=13)\ny_oof, y_test_ext = k_folds(ext, new_train, label['surface'], test, k=5)","f74b318b":"confusion_matrix(y_oof,label['surface'])","4c68dfdf":"# Submitting averaging\n\ny_test = y_test_ext + y_test_rand\ny_test = np.argmax(y_test, axis=1)\nsubmission = pd.read_csv(os.path.join(\"..\/input\/\", 'sample_submission.csv'))\nsubmission['surface'] = le.inverse_transform(y_test)\nsubmission.to_csv('submission.csv', index=False)\nsubmission.surface.value_counts()","8c8f4c3f":"df_test_y['sub'] = submission['surface']\ndf_test_y.head()","fc7eb07d":"agg = df_test_y.groupby(['run_id', 'sub'])['sub'].count()\nagg = pd.DataFrame(agg)\nagg.columns = ['count']\nagg.reset_index(inplace=True)\nagg = df_test_y.groupby(['run_id']).agg(lambda x: x.value_counts().index[0]).reset_index()[['run_id', 'sub']]\nagg_dict = agg.to_dict()\nsubmission['surface'] = df_test_y['run_id'].apply(lambda x: agg_dict['sub'][x])\nsubmission['surface'][df_test_y['run_id'] == 39] = 'hard_tiles'\nsubmission.surface.value_counts()","9bd509cd":"submission.surface.value_counts() \/ submission.shape[0]","e29255d6":"submission.shape","05fed1e1":"submission.to_csv(\"with_model.csv\", index=False)","5f9aef0e":"# ExtraTrees - 0.7617245030663025 (#50)#without orientations","0a9dadc5":"# Without Orientations : (50 * 76 samples(256) added)\n\nAvg Accuracy 0.7465495699892621","0b8572d9":"# Now adding samples of 256 size\n\nWe can get 256 sized samples by just adding two parts of same run_id i.e groups which have same surface.\nI've done this by taking randomly selecting 50 combinations for every run_id. We could have done for more nbut the kernel space would exceed so I remained for 50.","f99aefff":"# Trick (Every run has only one surface)\n# So taking the max of every run as the surface for each element","a25e831c":"# 15 Solution (Private: [0.77])\n** *If I would've submitted according to my local CV but I got mesmerized by the Leaderboard Scores and so I fall.**","45370030":"# Transforming Train with new series i.e 256 sized series.\nThe newer series having 256 size are given newer series_id starting from 3809.","60ceea04":"# Modelling"}}