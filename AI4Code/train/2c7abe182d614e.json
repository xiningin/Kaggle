{"cell_type":{"d385a0cc":"code","3b240eef":"code","7d989043":"code","4c02abcb":"code","bfe74e5b":"code","bd44fcc6":"code","9f6e855b":"code","019070dc":"code","aa1e2e98":"code","bfba2a00":"code","2e7943d9":"code","d3067d57":"code","e304b73f":"code","7808e31b":"code","33e51281":"code","41d4f9b7":"code","e5de4a4a":"code","db34c709":"code","30832f29":"code","43c3553c":"code","c14db6e7":"code","ac698935":"code","fe6af60b":"code","2ffe6591":"code","049d2a39":"code","51b3be1e":"code","b1701c74":"code","ef603009":"code","17779eca":"code","e886472e":"code","ab884701":"code","0e1fc6ca":"code","9b61abae":"code","9aaa6b39":"code","58fc210a":"code","c9a5b99e":"code","a9f5ab86":"code","0eb1319c":"code","90c21a1f":"code","20abbee6":"code","6c21aa52":"code","d353c43a":"code","843af027":"code","aabbd784":"code","f0f1120c":"code","3c597b8f":"code","58177522":"code","b2d84ab7":"code","791c40f4":"code","51aefeae":"code","d2ef2578":"code","cb15ff0d":"code","a70a69c9":"code","1dfca1f8":"code","5b9145ea":"code","3f7c39d4":"code","a7485de5":"code","9d837269":"code","78b8c145":"code","1200d912":"code","f8608318":"code","7e4643ca":"code","b7d2c1f0":"code","0c8fe25d":"code","bc93b189":"code","8a5fcedd":"code","17aaff26":"code","8d921aa8":"code","b8fecd63":"code","d8ff1a56":"code","751557fd":"code","150c1d67":"code","eadd8ac8":"code","db76f66b":"code","fc07c1b3":"code","82bad77b":"code","859641d4":"code","6d480e9a":"code","ab281a8d":"code","3ca46c08":"code","6a5f8154":"code","7d3bfa49":"markdown","2bb62e6b":"markdown","1bb3a33d":"markdown","50bec4c4":"markdown","eb805816":"markdown","410dcf6d":"markdown","35e2e023":"markdown","0bd48e0d":"markdown","2e5345e5":"markdown","98de186a":"markdown","e0cd6ec0":"markdown","7f0d0bf9":"markdown","cdda0e8a":"markdown"},"source":{"d385a0cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b240eef":"# Import libraries \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import preprocessing\n","7d989043":"# Import data set. \n\ntrain_data = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')","4c02abcb":"# Display first five rows.\n\ntrain_data.head()","bfe74e5b":"# Check number of entries and columns types. There are missing values in the data set.\n\ntrain_data.info() ","bd44fcc6":"# Gather all columns that are numerical, exclude Loan_ID because it's not necessary for what we do and Credit History because it only contains 0 or 1.\n\nnum_cols = train_data.dtypes != 'object' \nexclude_id = train_data.columns != 'Loan_ID'\nexclude_credit_history = train_data.columns != 'Credit_History'\n\nnum_columns = num_cols[num_cols & exclude_id & exclude_credit_history].index\nnum_columns","9f6e855b":"# Boxplot to get an idea of the range of the numerical values, looking for eventual outliers.\n\ni = 1\nplt.figure(figsize=(18,10))\nsns.set_theme(style=\"darkgrid\")\n\nfor col in num_columns:\n    plt.subplot(2,2, i)\n    sns.boxplot(x = train_data[col] , palette='viridis')\n    i = i + 1\n    \nplt.show()","019070dc":"# Visualize numerical values according to their Loan Status (YES\/NO) to observe outliers.\n    \nfor col in num_columns:\n    sns.catplot(x = 'Loan_Status', y=col, kind=\"strip\", palette='viridis', data=train_data)\nplt.show()","aa1e2e98":"# Gather all columns that are categoricals and check what are their possible values.\n\nobj_cols = train_data.dtypes == 'object'\nexclude_loan_id = train_data.columns != 'Loan_ID'\nexclude_loan_status = train_data.columns != 'Loan_Status'\n\n\ncategorical_columns = obj_cols[obj_cols & exclude_loan_id & exclude_loan_status].index\nfor col in categorical_columns:\n    print(train_data[col].value_counts())","bfba2a00":"# Distribution of the categorical values according to their Loan Status (YES\/NO).\n\ni = 1\nplt.figure(figsize=(20,15))\nfor catcol in categorical_columns:\n    plt.subplot(3,3, i)\n    sns.countplot(x = catcol, data=train_data, palette='viridis', hue='Loan_Status')\n    plt.xlabel(catcol, fontsize=14)\n    plt.ylabel(' ')\n    i = i + 1\nplt.show()","2e7943d9":"# Now that we have a better idea of the data set, we need to treat the outliers and missing values.","d3067d57":"# Copy of the train data.\n\ndf_train_no_outliers = train_data.copy()\ndf_train_no_outliers.shape","e304b73f":"condAI = (df_train_no_outliers['ApplicantIncome'] > 30000) | (df_train_no_outliers['ApplicantIncome'] < 1000)\ndf_train_no_outliers.drop(df_train_no_outliers[condAI].index, axis=0, inplace=True)","7808e31b":"condCAI = (df_train_no_outliers['CoapplicantIncome'] > 15000)\ndf_train_no_outliers.drop(df_train_no_outliers[condCAI].index, axis=0, inplace=True)","33e51281":"condLA = (df_train_no_outliers['LoanAmount'] > 400) | (df_train_no_outliers['LoanAmount'] < 40)\ndf_train_no_outliers.drop(df_train_no_outliers[condLA].index, axis=0, inplace=True)","41d4f9b7":"condLAT = (df_train_no_outliers['Loan_Amount_Term'] > 400) | (df_train_no_outliers['Loan_Amount_Term'] < 100)\ndf_train_no_outliers.drop(df_train_no_outliers[condLAT].index, axis=0, inplace=True)","e5de4a4a":"# We need to re index our data frame with the new number of entries.\n\ndf_train_no_outliers.reset_index(drop=True, inplace=True)\ndf_train_no_outliers.shape","db34c709":"# Number of rows deleted :\n\ntrain_data.shape[0] - df_train_no_outliers.shape[0]","30832f29":"# Shape after deleting all the outliers.\n\ndf_train_no_outliers.info()","43c3553c":"# Copy of our df to a new one. It allows us to go back to each step of the process.\n\ndf_no_omv = df_train_no_outliers.copy() # = df_no_outliers missing values.","c14db6e7":"# Display columns with missing values.\n\ncol_missing_values = df_no_omv.isna().sum()\ncol_missing_values = col_missing_values[col_missing_values > 0]\n\nprint(col_missing_values)","ac698935":"# How important are those missing values in regard to the total number of cells in our Data frame ?\n\ntotal_cells = train_data.shape[0] * train_data.shape[1]\nprint('Total cells :', total_cells)    \nprint('Total missing values :', col_missing_values.sum())                                                     \nprint('Percentage of missing cells (missing\/total values) :' , (col_missing_values.sum()\/total_cells) * 100, \"%\")","fe6af60b":"# Treating missing values for numerical values LoanAmount & Loan_Amount_Term with their mean.","2ffe6591":"df_no_omv.LoanAmount.describe()","049d2a39":"df_no_omv.Loan_Amount_Term.describe()","51b3be1e":"df_no_omv['LoanAmount'].fillna(df_no_omv['LoanAmount'].mean(), inplace=True)\ndf_no_omv['Loan_Amount_Term'].fillna(df_no_omv['Loan_Amount_Term'].mean(), inplace=True)","b1701c74":"# Credit_History has only 2 values possible 0 and 1\n\n# We are going fill the missing value with the most frequent one : 1\n\ndf_no_omv['Credit_History'].describe()","ef603009":"df_no_omv['Credit_History'].fillna(df_no_omv['Credit_History'].quantile(0.25), inplace=True)","17779eca":"# Now we are left with missing values only in categorical columns\n\ncol_missing_values = df_no_omv.isna().sum()\ncol_missing_values = col_missing_values[col_missing_values > 0]\n\nprint(col_missing_values)","e886472e":"# We fill those missing values with the most frequent one (that we get by taking the first value of the value_count return)\n\ncol_miss = ['Gender','Married','Dependents','Self_Employed']\n\nfor col in col_miss: \n    df_no_omv[col].fillna(df_no_omv[col].value_counts().index[0], inplace=True)","ab884701":"# No missing values left\n\ndf_no_omv.isna().sum()","0e1fc6ca":"# Now, we are going to encode our categorical columns in numerical value (1\/2) so our dataframe has only numerical values","9b61abae":"categorical_columns","9aaa6b39":"encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nmy_df_encoded = pd.DataFrame(encoder.fit_transform(df_no_omv[categorical_columns])) # Encoding our categorical columns \n\nmy_df_encoded.columns = encoder.get_feature_names(categorical_columns) # One Hot Encoder remove colmuns names so we need to put them back\n\ndf_no_omv.drop(categorical_columns ,axis=1, inplace=True) # We drop the old categorical columns in our original df\n\nmy_df_encoded = pd.concat([df_no_omv, my_df_encoded], axis=1) # We concat the encoded columns with the numerical columns of our original df\n","58fc210a":"my_df_encoded.head() # Each values for categorical column has now their own column with 0 or 1 as values.","c9a5b99e":"my_df_encoded.info() # Their types has change from object to numeric","a9f5ab86":"# Encoding the target columns Loan_Status. The column we want to predict. It's recommanded to encode the tharget with labelBinarize.\n\nlb = preprocessing.LabelBinarizer()\ntarget_encoded = pd.DataFrame(lb.fit_transform(my_df_encoded['Loan_Status']), columns=['Loan_Status']) # Encoding the columns with 0\/1.","0eb1319c":"my_df_encoded.drop('Loan_Status',axis=1, inplace=True) # Drop the Loan_Status columns of our original df.\n\nmy_df_encoded = pd.concat([target_encoded, my_df_encoded], axis=1) # Concat the encoded df with the target_encoded.","90c21a1f":"my_df_encoded.head() # We can see that our target is now encoded","20abbee6":"# Distribution of the data after processing them (missing values, outliers, encoding..)\n\ncol_features = ['Credit_History', 'Gender_Female',\n       'Gender_Male', 'Married_No', 'Married_Yes', 'Dependents_0',\n       'Dependents_1', 'Dependents_2', 'Dependents_3+', 'Education_Graduate',\n       'Education_Not Graduate', 'Self_Employed_No', 'Self_Employed_Yes',\n       'Property_Area_Rural', 'Property_Area_Semiurban',\n       'Property_Area_Urban']\n\ni = 1\nplt.figure(figsize=(20,25))\nfor feature in col_features:\n    plt.subplot(4,4, i)\n    sns.countplot(x = feature, data=my_df_encoded,  palette='viridis', hue='Loan_Status')\n    plt.xlabel(feature, fontsize=14)\n    plt.ylabel(' ')\n    i = i + 1\nplt.show()","6c21aa52":"# Distribution of the data in the target. \n\n# Our model must do at minimum 70%. It has to be at least better than if we were predicting based on the approved loan (yes)\n\n\n\n\nax = sns.countplot(x = 'Loan_Status', data = my_df_encoded,  palette='viridis')\n\n# To put a rate number for each bar.\ntotal = 168 + 389\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width() \/ 2.,\n            height + 3,\n            '{:1.1f}'.format(height \/ total),\n            ha=\"center\") ","d353c43a":"# First feature selection, the best correlation with Loan_Status seems to be the Credit_History\n\ncorrelations = my_df_encoded.corr()\nmatrix = np.triu(correlations) # We do a mask on the upper triangle of the heatmap to make it more clear\n\nplt.figure(figsize=(15,9))\nsns.heatmap(correlations, mask=matrix, cmap=\"GnBu\",annot=True, cbar=False)","843af027":"# It's a classification problem so we are going to use :\n\n#- Logistic regression  \n#- Random Forest\n#- Naive Bayes\n\n# Then we'll get deeper into the metrics of the logistic regression","aabbd784":"# We import all the good stuff from sklearn :-)\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.preprocessing import binarize\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import StratifiedKFold\nfrom math import sqrt\n\n","f0f1120c":"# Let's start by testing with all the feature in X and Loan_Status in the target\n\nX = my_df_encoded.iloc[:, 2:] # [first_row : last_row , first_col : last_col]\ny = my_df_encoded.iloc[:,0] ","3c597b8f":"# Split the data into a train set and a test set\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","58177522":"lgr = LogisticRegression(max_iter=1000)\n\nlgr.fit(X_train, y_train) # Train the model\n\ny_pred = lgr.predict(X_test) # Predict based on our X_test from the train_test_split\n\nprint('Train set score : ', lgr.score(X_train, y_train))\nprint('Test set score : ', accuracy_score(y_test,y_pred)) ","b2d84ab7":"# Good result. No overfitting (variance between result you get on the train set and the test set, when your model generalize poorly).\n\n# What are the best features to choose ? ","791c40f4":"# Maybe we can optimize those results using less features.\n\n# We can do this with RFECV = Recursive Feature Elimination and Cross-Validated selection of the best number of features.\n\n\n# In KFolds, each test set should not overlap, even with shuffle. \n# With KFolds and shuffle, the data is shuffled once at the start, \n# and then divided into the number of desired splits. \n# The test data is always one of the splits, the train data is the rest.\n\nskf = StratifiedKFold(n_splits=10) # Cross-validation 10 times\nestimator = LogisticRegression(C=10,max_iter=1000, penalty='l2', solver='lbfgs') # The estimator used is Logistic Regression.\n\nselector = RFECV(estimator, step=1, cv=skf, scoring=\"accuracy\") # Run RFE.\nselector = selector.fit(X, y) # Fit the datasolvers support only.","51aefeae":"#Ranking of the best features\n\nprint(selector.ranking_)\nprint(selector.support_)","d2ef2578":"# Credit_history is the best feature. Which is what we found with the heatmap. Other features seem interesting as well.\n\nmy_df_encoded.iloc[:, 2:].columns","cb15ff0d":"# Let's try only with the top 1 feature, Credit_History.\n\nX = my_df_encoded.loc[:, ['Credit_History']]\ny = my_df_encoded.iloc[:,0]","a70a69c9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Split our data","1dfca1f8":"lgr = LogisticRegression(max_iter=1000)\n\nlgr.fit(X_train, y_train)\n\ny_pred = lgr.predict(X_test)\n\nprint('Train set score : ', lgr.score(X_train, y_train))\nprint('Test set score : ', accuracy_score(y_test,y_pred)) ","5b9145ea":"# We get the same score..","3f7c39d4":"# The Credit_History is the column that has the most influences on the results, \n\n# but we are going to test with the top 5 features so we can make our model more stable","a7485de5":"top_5_features = ['Credit_History', 'Property_Area_Semiurban', 'Education_Graduate', 'Dependents_2', 'Married_Yes']\nprint('Top 5 features : ', top_5_features)","9d837269":"X = my_df_encoded.loc[:, top_5_features] #Features with the top 5.\ny = my_df_encoded.iloc[:,0]","78b8c145":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","1200d912":"lgr = LogisticRegression(max_iter=1000)\n\nlgr.fit(X_train, y_train)\n\ny_pred = lgr.predict(X_test)\n\nprint('Train set score : ', lgr.score(X_train, y_train))\nprint('Test set score : ', accuracy_score(y_test,y_pred)) ","f8608318":"# We still get our score. It seems to be a good idea to keep those columns.\n# It seems risky to me to base our prediction only on one column. With more data, maybe the situation would change so it's better to back it up with other features.","7e4643ca":"X = my_df_encoded.iloc[:, 2:] # Select all features\ny = my_df_encoded.iloc[:,0]","b7d2c1f0":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","0c8fe25d":"dtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\ny_pred_dtc = dtc.predict(X_test)\nscore = accuracy_score(y_test, y_pred_dtc) \n\nprint('Train set score : ', dtc.score(X_train, y_train))\nprint('Test set score : ', accuracy_score(y_test,y_pred_dtc)) ","bc93b189":"rfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\ny_pred_rfc = rfc.predict(X_test)\n\nprint('Train set score : ', rfc.score(X_train, y_train))\nprint('Test set score : ', accuracy_score(y_test,y_pred_rfc)) ","8a5fcedd":"print('==============================')\nprint('GAUSSIAN :')\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred1 = gnb.predict(X_test)\nprint('Train set score : ', gnb.score(X_train, y_train))\nprint('Test set score : ', accuracy_score(y_test,y_pred1)) \n\n\nprint('==============================')\nprint('MULTINOMIAL :')\nmnb = MultinomialNB()\nmnb.fit(X_train, y_train)\ny_pred2 = mnb.predict(X_test)\nprint('Train set score : ', mnb.score(X_train, y_train))\nprint('Test set score : ', accuracy_score(y_test,y_pred2)) \n\nprint('==============================')\nprint('BERNOUILLI :')\nbnm = BernoulliNB()\nbnm.fit(X_train, y_train)\ny_pred3 = bnm.predict(X_test)\nprint('Train set score : ', bnm.score(X_train, y_train))\nprint('Test set score : ', accuracy_score(y_test,y_pred3)) ","17aaff26":"# Our Logisitic Regression does as good as those others classifiers if we select the top 5 features but has overfitting or poor results when we select all columns.","8d921aa8":"# Most of our models are more accurate than a dummy classifier (which use very simple rules to make prediction)\n\nfrom sklearn.dummy import DummyClassifier\n\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_train, y_train)\n\ny_pred_dummy = dummy_clf.predict(X_test)\nscr = dummy_clf.score(X_train, y_train)\nscore = accuracy_score(y_test, y_pred_dummy) \nprint('Score on train set : ', scr)\nprint('Score on training set : ', score)","b8fecd63":"# Let's run again the model we selected...\n\nX = my_df_encoded.loc[:, top_5_features] # Features within the top 5.\ny = my_df_encoded.iloc[:,0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nlgr = LogisticRegression(max_iter=1000)\n\nlgr.fit(X_train, y_train)\n\ny_pred = lgr.predict(X_test)\n\nprint('Train set score : ', lgr.score(X_train, y_train))\nprint('Test set score : ', accuracy_score(y_test,y_pred)) ","d8ff1a56":"print(classification_report(y_pred, y_test))","751557fd":"#Plot the confusion matrix for the test set and the train set\n\nplot_confusion_matrix(lgr, X_test, y_test, cmap=plt.cm.Blues, display_labels=['No', 'Yes'], normalize='true')  \nplt.grid(False)\nplt.title('Confusion matrix test set')\nplt.show()\n\nplot_confusion_matrix(lgr, X_train, y_train, cmap=plt.cm.Blues, display_labels=['No', 'Yes'], normalize='true')  \nplt.grid(False)\nplt.title('Confusion matrix train set')\nplt.show()","150c1d67":"# For our 10 first prediction, we can see the distribution of the probabilities.\n\ny_pred_proba = lgr.predict_proba(X_test)\ny_pred_proba[:10] ","eadd8ac8":"# Distribution of the probabilities for the No and the Yes for each X_test prediction\n\nplt.figure(figsize=(13,7))\nplt.hist(y_pred_proba)\nplt.xlim(0,1)\nplt.xlabel('Pred proba of Yes\/No')\nplt.ylabel('Frequency')\nplt.title('Histogram of predicted probabilities')","db76f66b":"# Changing the threshold would be detrimental for both side. It's not as if we could just change to 0.55 the threshold and have better sensibility.\n\n# I'm going to show you the process just for the learning. But it won't help us here.\n\n\ny_pred_class = binarize(y_pred_proba, 0.75)[:,1] # We make prediction according to a 0.75 threshold on the right column of y_pred_proba.","fc07c1b3":"y_pred_class[0:5] # Results we get","82bad77b":"y_pred_proba[0:5] # We can see that it matches the right column of the y_pred_proba. 0.68 is put to 0 because it's < 0.75.","859641d4":"print(confusion_matrix(y_test, y_pred)) # confusion matrix for a threshold of 0.5.","6d480e9a":"print(confusion_matrix(y_test, y_pred_class)) # confusion matrix for threshold of 0.75.","ab281a8d":"# When we compare our confusion matrix we can see that the false positive 16 become 6. We have lowered them. \n# But it's at the cost of getting a lower true positive (77 become 58)\n# So it may not really be interesting.\n\n# We can find the best balance of both with the ROC curve","3ca46c08":"fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n\nplt.figure(figsize=(12,7))\nplt.plot(fpr, tpr)\nplt.title('ROC curve for logistic regression')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])","6a5f8154":"# If we want a false positive rate at 0.2 we have to accept a true positive rate at 0.4 ...\n\n# The curve doesn't show us the threshold value. We have to calculate it from those datas.\n\n# We can see that for our 0.97 of true positive rate, we got around 0.5 of false positive rate. That's what our confusion matrix showed us.","7d3bfa49":"# **Can we predict whether or not a loan should be granted ?**\n\n\n<h3><b>This is my first notebook on Kaggle, I hope you'll appreciate it  :-).<\/b><\/h3>\n\n## What to expect ?\n\n<h3><ul>\n  <li><a href=\"#exploratory\">Exploratory analysis & Data visualization<\/a><\/li>\n  <li><a href=\"#outliers\">Treatement of outliers<\/a><\/li>\n  <li><a href=\"#missing_values\"> Treatement of missing values<\/a><\/li>\n  <li><a href=\"#encoding\">Encoding<\/a><\/li>\n  <li><a href=\"#features_selection\">Features selection<\/a><\/li>\n  <li><a href=\"#use_of_models\">Use of different models [Decision Tree \/ Naive Bayes \/ Logistic Regression]<\/a><\/li>\n  <li><a href=\"#metrics_threshold\">Model metrics & classification threshold modification<\/a><\/li>\n  <li><a href=\"#conclusion\">Conclusion<\/a><\/li>\n<\/ul><\/h3>","2bb62e6b":"What do we see ? There seem to be an unbalanced toward the Yes for 0.99. It's the True positive rate. As we saw, our data are already biais toward the Yes (70%).\n\nOur model does poorly when it comes to predict No. It predicts a Yes instead for 0.56. That's the False positive rate. Which isn't good.\nThe choice of the metric depend on the business goals and risk\nWe can imagine that for a bank accepting wrongly a loan can be very costly.\n\nSo what would be interesting is to change the threshold of the probabilities\nIndeed, Sci-Kit learn is using a threshold of P>0.5 for binary classifications\nThe sensibility to accept a loan should be higher. \n\nLet's try to do that!!","1bb3a33d":"<h1 id=\"exploratory\">Exploratory analysis & Data visualization<\/h1>","50bec4c4":"<h1 id=\"conclusion\">Conclusion<\/h1>\n\n### The score of false positive (incorrect attribution of loan) can be improved but this is going to be costly for our true positive rate (correct attribution of loan). The choice depends of the business goal. We don't have enough data to make a strong conclusion. \n\n### Our best model still get an accuracy of :\n    \n## **==> 83.9% <==**\n\n\n### If there is any way I can improve my work, please feel free to make a suggestion.\n### Thank you! :)","eb805816":"<h1 id=\"metrics_threshold\">Model metrics & classification threshold modification<h1>\n","410dcf6d":"## **Random Forest**","35e2e023":"<h1 id=\"encoding\">Encoding<\/h1>","0bd48e0d":"## **Decision Tree**","2e5345e5":"<h1 id=\"missing_values\">Treatement of mising values<\/h1>","98de186a":"## **Naive Bayes**","e0cd6ec0":"<h1 id=\"features_selection\">Features selection<\/h1>","7f0d0bf9":"<h1 id=\"use_of_models\">Use of different models [Decision Tree \/ Naive Bayes \/ Logistic Regression]<\/h1>","cdda0e8a":"<h1 id=\"outliers\">Treatement of outliers<\/h1>"}}