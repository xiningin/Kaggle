{"cell_type":{"814efdb0":"code","1d0a7800":"code","5aa63c7f":"code","9290735e":"code","00bf2011":"code","3d0e5e08":"code","49b7bd73":"code","3446d1e2":"code","a198e957":"code","0ad69070":"code","74d92771":"code","e0a0e3e6":"code","2897168d":"markdown","0d4f5200":"markdown","6de90a42":"markdown"},"source":{"814efdb0":"!pip uninstall -y typing\n!pip install l5kit","1d0a7800":"import numpy as np\nimport os\nimport torch\n\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torchvision.models.resnet import resnet50\nfrom tqdm import tqdm\nfrom typing import Dict\n\nfrom l5kit.data import LocalDataManager, ChunkedDataset\nfrom l5kit.dataset import AgentDataset, EgoDataset\nfrom l5kit.evaluation import write_pred_csv\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.geometry import transform_points","5aa63c7f":"DIR_INPUT = \"\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\"\n\n# Training notebook's output.\nWEIGHT_FILE = \"\/kaggle\/input\/model-resnet50\/model_resnet50.pth\"","9290735e":"cfg = {\n    'format_version': 4,\n    'model_params': {\n        'history_num_frames': 10,\n        'history_step_size': 1,\n        'history_delta_time': 0.1,\n        'future_num_frames': 50,\n        'future_step_size': 1,\n        'future_delta_time': 0.1\n    },\n    \n    'raster_params': {\n        'raster_size': [300, 300],\n        'pixel_size': [0.5, 0.5],\n        'ego_center': [0.25, 0.5],\n        'map_type': 'py_semantic',\n        'satellite_map_key': 'aerial_map\/aerial_map.png',\n        'semantic_map_key': 'semantic_map\/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5\n    },\n    \n    'test_data_loader': {\n        'key': 'scenes\/test.zarr',\n        'batch_size': 8,\n        'shuffle': False,\n        'num_workers': 4\n    }\n\n}","00bf2011":"# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\ndm = LocalDataManager(None)","3d0e5e08":"def build_model(cfg: Dict) -> torch.nn.Module:\n    # load pre-trained Conv2D model\n    model = resnet50(pretrained=True, progress=True)\n\n    # change input channels number to match the rasterizer's output\n    num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n    num_in_channels = 3 + num_history_channels\n    # change the first layer input\n    model.conv1 = nn.Conv2d(\n        num_in_channels,\n        model.conv1.out_channels,\n        kernel_size=model.conv1.kernel_size,\n        stride=model.conv1.stride,\n        padding=model.conv1.padding,\n        bias=False,\n    )\n    # change output size to (X, Y) * number of future states\n    num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n    model.fc = nn.Linear(in_features=2048, out_features=num_targets)\n\n    return model\n\n\ndef forward(data, model, device, criterion):\n    inputs = data[\"image\"].to(device)\n    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n    targets = data[\"target_positions\"].to(device)\n    # Forward pass\n    outputs = model(inputs).reshape(targets.shape)\n    loss = criterion(outputs, targets)\n    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n    loss = loss * target_availabilities\n    loss = loss.mean()\n    return loss, outputs","49b7bd73":"criterion = nn.MSELoss(reduction=\"none\")","3446d1e2":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = build_model(cfg)\nmodel.to(device)\n\nmodel_state = torch.load(WEIGHT_FILE, map_location=device)\nmodel.load_state_dict(model_state)","a198e957":"# ===== INIT DATASET\ntest_cfg = cfg[\"test_data_loader\"]\n\n# Rasterizer\nrasterizer = build_rasterizer(cfg, dm)\n\n# Test dataset\/dataloader\ntest_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\ntest_mask = np.load(f\"{DIR_INPUT}\/scenes\/mask.npz\")[\"arr_0\"]\ntest_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)\ntest_dataloader = DataLoader(test_dataset,\n                             shuffle=test_cfg[\"shuffle\"],\n                             batch_size=test_cfg[\"batch_size\"],\n                             num_workers=test_cfg[\"num_workers\"])\n\n\nprint(test_dataloader)","0ad69070":"# # ==== EVAL LOOP\n# model.eval()\n# torch.set_grad_enabled(False)\n\n# # store information for evaluation\n# future_coords_offsets_pd = []\n# timestamps = []\n# agent_ids = []\n\n# progress_bar = tqdm(test_dataloader)\n# for data in progress_bar:\n#     _, ouputs = forward(data, model, device, criterion)\n    \n#     # convert agent coordinates into world offsets\n#     agents_coords = ouputs.cpu().numpy()\n#     world_from_agents = data[\"world_from_agent\"].numpy()\n#     centroids = data[\"centroid\"].numpy()\n#     coords_offset = []\n    \n#     for agent_coords, world_from_agent, centroid in zip(agents_coords, world_from_agents, centroids):\n#         coords_offset.append(transform_points(agent_coords, world_from_agent) - centroid[:2])\n    \n#     future_coords_offsets_pd.append(np.stack(coords_offset))\n#     timestamps.append(data[\"timestamp\"].numpy().copy())\n#     agent_ids.append(data[\"track_id\"].numpy().copy())","74d92771":"model.eval()\n\nfuture_coords_offsets_pd = []\ntimestamps = []\nagent_ids = []\n\nwith torch.no_grad():\n    dataiter = tqdm(test_dataloader)\n    \n    for data in dataiter:\n        inputs = data[\"image\"].to(device)\n        target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n        targets = data[\"target_positions\"].to(device)\n\n        _, outputs = forward(data, model, device, criterion)\n        \n        future_coords_offsets_pd.append(outputs.cpu().numpy().copy())\n        timestamps.append(data[\"timestamp\"].numpy().copy())\n        agent_ids.append(data[\"track_id\"].numpy().copy())","e0a0e3e6":"write_pred_csv('submission.csv',\n               timestamps=np.concatenate(timestamps),\n               track_ids=np.concatenate(agent_ids),\n               coords=np.concatenate(future_coords_offsets_pd))","2897168d":"# Model","0d4f5200":"# Load Test Data","6de90a42":"# Predicting"}}