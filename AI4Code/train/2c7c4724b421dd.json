{"cell_type":{"37568df7":"code","a0a2ccb7":"code","b07aa30a":"code","1d0f65ce":"code","c3787618":"code","8e0102d9":"code","09cfe504":"code","483ecb61":"code","0eda41d4":"code","b7449ba7":"code","26e252e4":"code","fcc672e5":"code","ddc27e3e":"code","993208a0":"code","a9530c2a":"code","1bd7b37c":"code","49cc3974":"code","f9a824b6":"code","7f13a11d":"code","564b37bd":"code","567465ca":"code","57ad09e5":"code","1188cadd":"code","3d6c098b":"code","a2a6daa0":"code","f603ee11":"code","11402fc3":"code","2f5bff2f":"code","c2c10bc6":"code","658a6a28":"code","b6b8e09a":"code","e9f0c188":"code","338f1cad":"code","2bd74190":"code","2d58012f":"code","21502214":"code","f7663eab":"code","6ae7ce41":"code","3eaa3b6b":"code","e1807395":"code","35c10682":"code","a2f9cadc":"code","473ee556":"code","ff99357b":"code","2e02fa57":"code","d6a86c88":"code","a5aaeaa5":"code","53e20b45":"code","d5f7cbe9":"code","6db65ce6":"code","a42ec487":"code","0f2a0f6c":"code","bf4187e9":"code","79e8470c":"code","b11589cc":"code","3927bc60":"code","90ad02b3":"code","19767b9e":"code","a27d190a":"code","b34e1ed8":"code","933a3386":"code","d4138941":"code","9ffde668":"code","a14342f8":"code","221a7625":"code","f5083ad1":"code","355613e9":"code","01daa16c":"code","bfdee8f3":"code","e78af39e":"code","f8c9659c":"code","fdc39dc9":"code","b99947ed":"code","03736282":"code","e035bfb2":"code","c59d0000":"code","16f40361":"code","2f21a714":"code","38090b40":"code","49221446":"code","20520065":"code","65f0dbdd":"code","9c810f5f":"code","71e9943b":"code","b612cc9c":"code","985dde81":"code","7ff39b98":"code","ba039fb5":"code","8e25eb10":"code","88d90091":"code","b1166079":"code","81b05856":"code","ae62612d":"code","b8494c1d":"code","d1d63766":"code","ed401162":"code","9970b43e":"code","bca56c3a":"code","034138ea":"code","13f6928c":"code","0b8234ee":"code","546a5ee3":"code","da5e55da":"code","58313282":"code","93fee09e":"code","92425ae2":"code","5f54258a":"code","afba76bb":"code","b115a831":"code","ee2d45e1":"code","51cd1642":"code","e2ccd1b6":"code","e110b84f":"code","bae7d10e":"code","c54cc5e3":"code","442d5358":"code","e2943ecc":"markdown","153d7407":"markdown","f846d722":"markdown","ca85dada":"markdown","708caf13":"markdown","58008d92":"markdown","56f5a7c7":"markdown","dbf74687":"markdown","b3308102":"markdown","b3f7bf6a":"markdown","bfcf561c":"markdown"},"source":{"37568df7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a0a2ccb7":"#open and convert data to pandas dataframe \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n\n\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n%matplotlib inline\n\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n","b07aa30a":"test.head(3)","1d0f65ce":"train.head(3)","c3787618":"#compare the test and train dataframes shapes\ntrain.shape","8e0102d9":"test.shape","09cfe504":"#correlation among columns\ntrain.corr()","483ecb61":"#giving a new name to preserve original train df\nHomes=train\nHomes.head(3)","0eda41d4":"Homes.dtypes","b7449ba7":"#only 38 columns are numerical, 43 are objects\nHomes.describe()","26e252e4":"def graph_insight(data):\n    print(set(data.dtypes.tolist()))\n    df_num = data.select_dtypes(include = ['float64', 'int64'])\n    df_num.hist(figsize=(16, 16), bins=50, xlabelsize=8, ylabelsize=8)\n    \n\ngraph_insight(Homes)","fcc672e5":"#data distribution of house prices\nsns.distplot(Homes['SalePrice'])","ddc27e3e":"#Preprocessing data\n\n#managing null or missing data\n#replacing NaN with 0\nHomes.fillna(0, inplace=True)","993208a0":"Homes.isnull().sum().max() #to verify that there's no missing data missing","a9530c2a":"#Standardize data to address outliers\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nPrice_scaled = StandardScaler().fit_transform(Homes['SalePrice'][:,np.newaxis])\nlow_range = Price_scaled[Price_scaled[:,0].argsort()][:10]\nhigh_range= Price_scaled[Price_scaled[:,0].argsort()][-10:]\nprint('low outlier in price:')\nprint(low_range)\nprint('high outlier in price:')\nprint(high_range)","1bd7b37c":"#simple linear regression\n\nimport seaborn as seabornInstance \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n","49cc3974":"Homes.plot(x='Id', y='SalePrice', style='o')  \nplt.xlabel('Id')  \nplt.ylabel('SalePrice')  \nplt.show()","f9a824b6":"X = Homes['Id'].values.reshape(-1,1)\ny = Homes['SalePrice'].values.reshape(-1,1)","7f13a11d":"#split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","564b37bd":"regressor = LinearRegression()  \nregressor.fit(X_train, y_train) #training the model","567465ca":"#The intercept:\nprint(regressor.intercept_)\n#The slope:\nprint(regressor.coef_)","57ad09e5":"y_pred = regressor.predict(X_test)","1188cadd":"df = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})\ndf","3d6c098b":"plt.scatter(X_test, y_test,  color='purple')\nplt.plot(X_test, y_pred, color='yellow', linewidth=2)\nplt.show()","a2a6daa0":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","f603ee11":"#multiple regression\n\nimport statsmodels.api as sm\nX_ = Homes[['YrSold','LotArea']] # here we have 2 variables for multiple regression. If you just want to use one variable for simple linear regression, then use X = df['Interest_Rate'] for example.Alternatively, you may add additional variables within the brackets\nY = Homes['SalePrice']\n \n# with sklearn\nregr = LinearRegression()\nregr.fit(X_, Y)\n\nprint('Intercept: ', regr.intercept_)\nprint('Coefficients: ', regr.coef_)\n\nprint (regr)","11402fc3":"y_pred1 = regressor.predict(X_test)","2f5bff2f":"df1= pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})\ndf1","c2c10bc6":"print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","658a6a28":"from scipy.stats.stats import pearsonr\ndf3 = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                      test.loc[:,'MSSubClass':'SaleCondition']))","b6b8e09a":"#log transform the target:\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#log transform skewed numeric features:\nnumeric_feats = df3.dtypes[df3.dtypes != \"object\"].index\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\ndf3[skewed_feats] = np.log1p(df3[skewed_feats])","e9f0c188":"df3 = pd.get_dummies(df3) #categorical data","338f1cad":"#filling NA's with the mean of the column:\ndf3 = df3.fillna(df3.mean())","2bd74190":"df3.shape","2d58012f":"#create matrices for sklearn\nX_train = df3[:train.shape[0]]\nX_test = df3[train.shape[0]:]\ny = train.SalePrice","21502214":"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\n\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse) #Root mean square error","f7663eab":"#Lasso model\nmodel_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)\n\nrmse_cv(model_lasso).mean()","6ae7ce41":"coef = pd.Series(model_lasso.coef_, index = X_train.columns)","3eaa3b6b":"print(\"Lasso of \" + str(sum(coef != 0)) + \" variables and eliminated the remaining \" +  str(sum(coef == 0)) + \" variables\")","e1807395":"imp_coef = pd.concat([coef.sort_values().head(10),\n                     coef.sort_values().tail(10)])\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","35c10682":"#Improve the model by adding an xgboost model \nimport xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, label = y)\ndtest = xgb.DMatrix(X_test)\n\nparams = {\"max_depth\":2, \"eta\":0.1}\nmodel = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)","a2f9cadc":"model.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()","473ee556":"model_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) #the params were tuned using xgb.cv\nmodel_xgb.fit(X_train, y)","ff99357b":"xgb_preds = np.expm1(model_xgb.predict(X_test))\nlasso_preds = np.expm1(model_lasso.predict(X_test))","2e02fa57":"predictions = pd.DataFrame({\"xgb\":xgb_preds, \"lasso\":lasso_preds})\npredictions.plot(x = \"xgb\", y = \"lasso\", kind = \"scatter\", color='red')","d6a86c88":"from sklearn.ensemble import RandomForestRegressor\n\n(Homes['Id']).astype('int64')\n","a5aaeaa5":"(Homes['SalePrice']).astype('float64')","53e20b45":"#feature and target\nx = pd.DataFrame(Homes.iloc[::,0])\nx","d5f7cbe9":"y = pd.DataFrame(Homes.iloc[:,-1])  \ny","6db65ce6":"x = Homes['Id'].values.reshape(-1,1)\ny = Homes['SalePrice'].values.reshape(-1,1)","a42ec487":"#split the data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)","0f2a0f6c":"regressor=RandomForestRegressor(n_estimators = 100, random_state = 0)","bf4187e9":"# fit the regressor with x and y data \nregressor.fit(X_train,y_train) ","79e8470c":"\n# Visualising the Random Forest Regression results \n  \n# arange for creating a range of values \n# from min value of x to max  \n# value of x with a difference of 0.01  \n# between two consecutive values \nX_grid = np.arange(min(x), max(x), 0.01)  \n  \n# reshape for reshaping the data into a len(X_grid)*1 array  \n# (to make a column out of the X_grid value)                   \nX_grid = X_grid.reshape((len(X_grid), 1)) \n  \n# Scatter plot for original data \nplt.scatter(x, y, color = 'blue')   \n  \n# plot predicted data \nplt.plot(X_grid, regressor.predict(X_grid),  \n         color = 'green')  \nplt.title('Random Forest Regression') \nplt.xlabel('Id') \nplt.ylabel('Price') \nplt.show()\n\n","b11589cc":"#predict\ny_pred_ = regressor.predict(X_train)\ny_pred_","3927bc60":"#test set predictions\nPred_DF = pd.DataFrame({'Price': y_pred_ })\nPred_DF","90ad02b3":"Pred_DF1 = pd.DataFrame({'Id': X_train.flatten() ,'SalePrice': y_pred_.flatten() })\nPred_DF1","19767b9e":"# Predictions with the TEST data set\nYtest_pred= regressor.predict(X_test)\nYtest_pred","a27d190a":"TEST_df = pd.DataFrame({'Id': X_test.flatten() ,'SalePrice': y_test.flatten() })\nTEST_df","b34e1ed8":"#giving a new name to preserve original test df\nEstates=test.copy()\nEstates.head(3)","933a3386":"#get SalePrice from train set\n# Merge two Dataframes \ncombo=pd.concat([Homes,Estates], ignore_index=False, axis=1)\ncombo.head(3)","d4138941":"#drop duplicate columns\nnewcombo = combo.loc[:,~combo.columns.duplicated()]\nnewcombo.head(3)","9ffde668":"newcombo.shape","a14342f8":"#I look for outliers in SalePrice\nimport seaborn as sns\nsns.boxplot(x=newcombo['SalePrice'])","221a7625":"#libraries I did not have before\n#import some necessary librairies\n\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn # to ignore warnings (from sklearn and seaborn)\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limits floats to 3 decimal points\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #checks files in the directory\n","f5083ad1":"#check the numbers of samples and features\nprint(\"TRAIN data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"TEST data size before dropping Id feature is : {} \".format(test.shape))","355613e9":"#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","01daa16c":"##Data processing\n##Visualize outliers\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'], color='pink')\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","bfdee8f3":"#remove outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'], color='teal')\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","e78af39e":"from scipy import stats\n#Target: SalePrice\nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","f8c9659c":"#Log-transformation\n#Use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","fdc39dc9":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","b99947ed":"#missing data\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","03736282":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","e035bfb2":"#correlation\n#Correlation map of features correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","c59d0000":"###Imputing missing values    \n\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")    \n\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n\n","16f40361":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","2f21a714":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n","38090b40":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","49221446":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n","20520065":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","65f0dbdd":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n","9c810f5f":"all_data = all_data.drop(['Utilities'], axis=1)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","71e9943b":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","b612cc9c":"#transform data\n#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","985dde81":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))\n","7ff39b98":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","ba039fb5":"#skewed features\nnumeric_feats1 = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats1 = all_data[numeric_feats1].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness1 = pd.DataFrame({'Skew' :skewed_feats1})\nskewness1.head(10)","8e25eb10":"skewness2 = skewness1[abs(skewness1) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness2.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features2 = skewness2.index\nlam = 0.15\nfor feat in skewed_features2:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","88d90091":"#getdummies for categorical data\nall_data = pd.get_dummies(all_data)\nprint(all_data.shape)","b1166079":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","81b05856":"#missing model libraries\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.linear_model import Lasso,  BayesianRidge\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb","ae62612d":"##Corss-validating\n#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","b8494c1d":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","d1d63766":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","ed401162":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","9970b43e":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","bca56c3a":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n","034138ea":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","13f6928c":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n","0b8234ee":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","546a5ee3":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","da5e55da":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","58313282":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","93fee09e":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","92425ae2":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","5f54258a":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","afba76bb":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","b115a831":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","ee2d45e1":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","51cd1642":"from sklearn.metrics import mean_squared_error\nstacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","e2ccd1b6":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","e110b84f":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","bae7d10e":"print('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","c54cc5e3":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","442d5358":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","e2943ecc":"I consulted the work in:    \n**Stacked Regressions to predict House Prices\n*Serigne* July 2017**   \nBelow, I adapt what I miss with what I fond there to reach answer.   \n(This is a learning excercise and I make no claim to the results below).","153d7407":"The **random forest** is a model made up of many decision trees. Rather than just simply averaging the prediction of trees (which we could call a \u201cforest\u201d), this model uses two key concepts that gives it the name random:\n\n Random sampling of training data points when building trees   \n Random subsets of features considered when splitting nodes   \n \n predictions of the random forest are made by averaging the predictions of each individual tree.   \n the Receiver Operating Characteristic Area Under the Curve (ROC AUC), a measure from 0 (worst) to 1 (best) with a random guess scoring 0.5.   \n ","f846d722":"The simple linear regression above is highly erroneous. ","ca85dada":"Classic regression models above are highly erroneous. The models have little preprocessing or feature engineering of the train set.   \nBelow, regressions improve.   \n","708caf13":"**Averaging base STACKING models**\n Build a new **class**  to extend scikit-learn with the model and also to laverage encapsulation and code reuse ([inheritance]) ","58008d92":"There seems to be a difference of 1 column, 1 row.   \nBelow, explore the train set.      \n","56f5a7c7":"Competition:   \nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.   \nPractice Skills:   \nCreative feature engineering    \nAdvanced regression techniques like random forest and gradient boosting   \n\n**Goal**: to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable.    \nEval. metrics: Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price.    \n(Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)   \n","dbf74687":"Feature Engineering","b3308102":"The RSME here is low. This model seems to fit.","b3f7bf6a":"**Ensembling** StackedRegressor, XGBoost and LightGBM","bfcf561c":"**Regressions**"}}