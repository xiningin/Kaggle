{"cell_type":{"295175ec":"code","22e9acce":"code","b68753e1":"code","ac8fe684":"code","bc2b26c4":"code","a4ee2d80":"code","19b76f23":"code","1326e325":"code","0aa40af5":"markdown","ec603a95":"markdown"},"source":{"295175ec":"#Required Libraries\nimport numpy as np  \nfrom numpy.random import rand\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder","22e9acce":"#dataset\nx = np.array(([0.9,0.8],[0.6,0.3],[0.9,0.1],[0.9,0.8], [0.2, 0.35], [0.4, 0.39]))  #Features\ny = np.array((['Netural'],['Positive'],['Positive'],['Netural'], ['Negative'], ['Negative']))  #Labels (0,1)\n\n","b68753e1":"#Encode categorical features as a one-hot numeric array.\nenc = OneHotEncoder(sparse=False, categories='auto')\ny = enc.fit_transform(y.reshape(len(y), -1))\n ","ac8fe684":"enc.categories_","bc2b26c4":"y","a4ee2d80":"# Activation function\ndef sigmoid(Z):\n    return 1\/(1+np.exp(-Z))\n\n# Activation function\ndef softmax(Z):\n    return np.exp(Z) \/ np.sum(np.exp(Z), axis=1, keepdims=True)\n\n#Multi-layers feedforward neural network (2 hidden layers)\nclass NeuralNetwork:\n    def __init__(self, x, y, nodes_in_layer1 = 4, nodes_in_layer2 = 3, nodes_in_layer3 = 3, l_rate = 1):\n        #define x, y\n        self.inputs_in_layer0 = x  #layer 0\n        self.y = y\n        \n        self.l_rate = l_rate  #learning rate\n        \n        #define and set the number of neurons in each layers\n        self.nodes_in_layer1 = nodes_in_layer1\n        self.nodes_in_layer2 = nodes_in_layer2\n        self.nodes_in_layer3 = nodes_in_layer3\n        \n        #intialize the wieghts (theta) metrices\n        self.thetas_layer0 = np.random.rand(self.inputs_in_layer0.shape[1] + 1,self.nodes_in_layer1)  #shape: [2+1, 4]\n        self.thetas_layer1 = np.random.rand(self.nodes_in_layer1 + 1,self.nodes_in_layer2)  #shape: [4+1, 3]\n        self.thetas_layer2 = np.random.rand(self.nodes_in_layer2+1, nodes_in_layer3)  # shape: [3+1, 3]\n\n    def feedforward(self):      \n        #compute all the nodes (a1, a2, a3, a4) in layer1\n        n = self.inputs_in_layer0.shape[0]\n\n        self.Z1 = self.thetas_layer0[0] + np.dot(self.inputs_in_layer0, self.thetas_layer0[1:])\n        self.layer1 = sigmoid(self.Z1)  #values of a1, a2, a3, a4 in layer 1\n        \n        #compute all the nodes (a1, a2, a3) in layer2\n        self.Z2 = self.thetas_layer1[0] + np.dot(self.layer1, self.thetas_layer1[1:])\n        self.layer2 = sigmoid(self.Z2)  #values of a1, a2, a3 in layer 2\n        \n        #compute all the nodes (a1, a2, a3) in layer3\n        self.Z3 = self.thetas_layer2[0] + np.dot(self.layer2, self.thetas_layer2[1:])\n        self.layer3 = softmax(self.Z3)  #output layer      \n        \n        return self.layer3\n    \n    def cost_func(self):\n        \n        self.n = self.inputs_in_layer0.shape[0] #number of training examples\n        self.cost = (1\/self.n) * np.sum(-self.y * np.log(self.layer3)) #cross entropy\n        return self.cost \n\n    \n    def backprop(self):\n        \n        self.dE_dZ3 = (1\/self.n) * (self.layer3 - self.y)\n\n        self.dE_dtheta2 = np.dot(self.layer2.T, self.dE_dZ3)\n        self.dE_dbias2 = np.dot(np.ones(self.n), self.dE_dZ3)\n        \n        #dervative of E with respect to theta and bias in layer1\n        self.dE_dlayer2 = np.dot(self.dE_dZ3, self.thetas_layer2[1:].T)\n        self.dE_dZ2 = np.multiply(self.dE_dlayer2, sigmoid(self.Z2)* (1-sigmoid(self.Z2)))\n        self.dE_dtheta1 = np.dot(self.layer1.T, self.dE_dZ2)\n        self.dE_dbias1 = np.dot(np.ones(self.n), self.dE_dZ2)\n        \n        #dervative of E with respect to theta and bias in layer0\n        self.dE_dlayer1 = np.dot(self.dE_dZ2, self.thetas_layer1[1:].T)\n        self.dE_dZ1 = np.multiply(self.dE_dlayer1, sigmoid(self.Z1)* (1-sigmoid(self.Z1)))\n        self.dE_dtheta0 = np.dot(self.inputs_in_layer0.T, self.dE_dZ1)\n        self.dE_dbias0 = np.dot(np.ones(self.n), self.dE_dZ1)\n        \n        #updating theta using gradient descent in layers 2, 1, and 0\n        self.thetas_layer2[1:] = self.thetas_layer2[1:] - self.l_rate * self.dE_dtheta2\n        self.thetas_layer1[1:] = self.thetas_layer1[1:] - self.l_rate * self.dE_dtheta1\n        self.thetas_layer0[1:] = self.thetas_layer0[1:] - self.l_rate * self.dE_dtheta0\n        \n        #updating bias using gradient descent in layers 2, 1, and 0\n        self.thetas_layer2[0] = self.thetas_layer2[0] - self.l_rate * self.dE_dbias2\n        self.thetas_layer1[0] = self.thetas_layer1[0] - self.l_rate * self.dE_dbias1\n        self.thetas_layer0[0] = self.thetas_layer0[0] - self.l_rate * self.dE_dbias0\n        \n        \n        return self\n","19b76f23":"NN = NeuralNetwork(x,y)\nepochs = 500\nlosses = []\nfor i in range(epochs):\n    predicted_output = NN.feedforward()\n    error = NN.cost_func()\n    losses.append(error)\n    NN.backprop()\n    print (\"iteration # \", i+1)\n    print (\"Actual Output: \\n\", y)\n    print(\"Predicted Output: \\n\", predicted_output, \"\\n\")\n    print (\"Cost: \\n\" , error, \"\\n\")\n","1326e325":"plt.scatter(range(epochs), losses)\nplt.title('Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","0aa40af5":"![softdervative.png](attachment:softdervative.png)\n![bacprop3.png](attachment:bacprop3.png)\n![bacprop2.png](attachment:bacprop2.png)","ec603a95":"![nnsoftmax.png](attachment:nnsoftmax.png)\n\n![BB.png](attachment:BB.png)"}}