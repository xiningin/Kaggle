{"cell_type":{"8ed15008":"code","f0c15062":"code","466d112a":"code","80c89b77":"code","97c73832":"code","6c5c37b5":"code","e773c10d":"code","08089fec":"code","15c1f952":"code","97722028":"code","5a12d79c":"code","c10ce4fe":"code","7554e27f":"code","5e585e8f":"code","d0a51e2d":"code","4df46630":"code","d9f5a312":"code","02052d0c":"code","fe058a36":"code","036a6ef1":"code","82b11625":"code","cb956d40":"markdown","fba6a533":"markdown","653b5a0f":"markdown","680d04d0":"markdown","b400a0a1":"markdown","60d411c6":"markdown","93ad7cdb":"markdown","83f73a59":"markdown","e516731b":"markdown","48d4d50d":"markdown","7fc703c8":"markdown","fac75fef":"markdown","4961faf9":"markdown","275e205d":"markdown","27a0f174":"markdown","37a07932":"markdown","a556f2e7":"markdown","cf1fb269":"markdown","514586e8":"markdown","d6651f86":"markdown","4b453c85":"markdown","7cd32321":"markdown","3b0d8600":"markdown","575bb590":"markdown","17bd51e8":"markdown","b405b4ee":"markdown"},"source":{"8ed15008":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f0c15062":"data = pd.read_csv('..\/input\/titanic\/train.csv')\ndata.head()","466d112a":"data.isnull().sum()","80c89b77":"# keeping threshold as 70%\nthreshold = 0.7\n#filter columns with mean missing value higher than threshold\ndata = data[data.columns[data.isnull().mean() < threshold]]\n\n#filter rows with mean missing value rate higher than threshold\ndata = data.loc[data.isnull().mean(axis=1) < threshold]\n\ndata.head()","97c73832":"# display total null in each columm\ndata.isnull().sum()","6c5c37b5":"# Filling missing values with medians of Age column\ndata['Age'] = data.Age.fillna(data.Age.median())\n\ndata.head()","e773c10d":"# Check again total number of nan in each feature\n\ndata.isnull().sum()","08089fec":"data.Embarked.value_counts()","15c1f952":"# id with max value_counts\n\ndata.Embarked.value_counts().idxmax()","97722028":"# lets impute nan in column Embarked with maximum occured values in it. \ndata['Embarked'].fillna(data.Embarked.value_counts().idxmax(),inplace=True)\n\n#check total count of nan again in each feature\ndata.isnull().sum()","5a12d79c":"#We will need same data for other techniques as well so better we leave originial undisturbed by copying it into new_data\nnew_data = data.copy()\n\n# Lets define our factor=3 as our factor to calculate our limit\nfactor = 3\n\n# Calculate upper limit and lower limit  of 'Fare' column to decide the threshold to detect outlier\n#Using: upper_lim=mean+std*factor\n#          loeer_lim=mean-std*factor\n\nupper_lim = new_data.Fare.mean() + new_data.Fare.std() * factor\nlower_lim = new_data.Fare.mean() - new_data.Fare.std() * factor","c10ce4fe":"#Now filter our 'Fare' with data lower than upper_lim and greater than lower_lim\nnew_data = new_data[(new_data.Fare < upper_lim) & (new_data.Fare > lower_lim)]\n\n#print the length of original and new data\nprint(f'Length of orignial data: {len(data)}')\nprint(f'Length of new data: {len(new_data)}')","7554e27f":"#copy data again into new_data\nnew_data =data.copy()\n\n#create upper and lower limits for column 'Fare' based on 95% and 5% respectively.\nupper_lim = new_data.Fare.quantile(0.95) #using 0.95 quantile value on Fare \nlower_lim = new_data.Fare.quantile(0.05) #using 0.05 quantile value on Fare \n\n#Dropping the outlier rows in 'Fare' with created limited.\n\nnew_data = new_data[(new_data.Fare < upper_lim) & (new_data.Fare > lower_lim)] #filter with Fare less tha upper_lim and more than lower_lim\n\n#print the length of both original and new data after filter\nprint(f'Length of orignial data: {len(data)}')\nprint(f'Length of new data: {len(new_data)}')","5e585e8f":"#copy data into new_data\nnew_data = data.copy()\n\n#create upper and lower limits for column 'Fare' based on 95% and 5% respectively.\nupper_lim = new_data.Fare.quantile(0.95) #using 0.95 quantile value on Fare \nlower_lim = new_data.Fare.quantile(0.05) #using 0.05 quantile value on Fare\n\n#Capping the outlier rows with Percentiles\nnew_data.loc[(new_data.Fare < lower_lim),\"Fare\"] = upper_lim\nnew_data.loc[(new_data.Fare > upper_lim),\"Fare\"] = lower_lim\n\n# print length of original and new data after capping.\nprint(f'Length of orignial data: {len(data)}')\nprint(f'Length of new data: {len(new_data)}')","d0a51e2d":"# copy the data into new_data\nnew_data = data.copy()\n\n#Normalizing the Age column using the given normalization formula on column Age\nnew_data['Age'] = (new_data.Age - new_data.Age.min()) \/ (new_data.Age.max()-new_data.Age.min())\n\n#Similarly Normalizing the Fare column using the given formula\nnew_data['Fare'] = (new_data.Fare - new_data.Fare.min()) \/ (new_data.Fare.max()-new_data.Fare.min())\n\nnew_data.head()","4df46630":"# copy the data into new_data\nnew_data = data.copy()\n\n#Standardizing the Age column using the Z formula given above\nnew_data['Age'] = (new_data.Age - new_data.Age.mean()) \/ (new_data.Age.std())\n\n#Standardizing the Fare column  using the Z formula given above\nnew_data['Fare'] = (new_data.Fare - new_data.Fare.mean()) \/ (new_data.Fare.std())\n\nnew_data.head()","d9f5a312":"# copy the data into new_data\nnew_data = data.copy()\n\n# Impliment binning on Age and make another column names Age_group with  bins=[0,18,40,100] and label them as \"Children\", \"Adult\" and\"Old\"\nnew_data['Age_group'] = pd.cut(new_data.Age,bins=[0,18,45,80],labels=['child','adult','old'])  # as max age observed was 80\n\n#count total values of each category\nnew_data['Age_group'].value_counts()","02052d0c":"#Checking 'Age_group' distribution\nnew_data['Age_group'].hist()","fe058a36":"# Count unique value counts for Sex\nprint(data['Sex'].value_counts(),'\\n')\n\n# Count unique value counts for Embarked\nprint(data['Embarked'].value_counts())","036a6ef1":"# Import Label encode\nfrom sklearn.preprocessing import LabelEncoder\n\n# creating instance of labelencoder\nlabelencoder = LabelEncoder()\n\n# Assigning numerical values in column Sex and storing in another column named Sex_encode\ndata['Sex_encode'] = labelencoder.fit_transform(data['Sex'])\n\n#print data.head()\ndata.head()","82b11625":"#Using get_dummies function encode and concat encoded features in data\ndata = pd.concat([data,pd.get_dummies(data.Embarked)],axis=1) \n\n#data.head()\ndata.head()","cb956d40":"----\n----\n\n# other notebook links:\n* [Simple Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/linear-regression-basic)\n* [Multiple Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/multiple-linear-regression-basic)\n* [Polynomial Regression](https:\/\/www.kaggle.com\/mukeshmanral\/polynomial-regression-basic)\n* [Advanced Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/advance-linear-regression-basic-gridsearchcv-hpt)\n\n\n\n\n\n\n* [Feature Engineering 1](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-1-basic)\n* [Feature Engineering 2](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-2-basic)\n* [Feature Engineering 3](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-3-basic)\n* [Feature Engineering 4](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-4-basic)\n\n----\n----","fba6a533":"1.","653b5a0f":"# 7. Variable Transformation\n* Linear and Logistic Regression model assume that variables follow a normal distribution\n    * More likely variables in real datasets will follow more OF a skewed distribution\n\nBy applying a number of transformations to these variables, and mapping their skewed distribution to a normal distribution, we can increase the performance of our models\n\n#### How can we transform variables?\nThe most commonly-used methods to transform variables are the following:\n\n* `Logarithmic transformation` for +ve vly skewed (right skewed)\n* `Square root transformation`\n* `Reciprocal transformation`\n* `Exponential` or `Power transformation` for -ve vly skewed (left skewed)","680d04d0":"1. ","b400a0a1":"* scikit learn already has library to scale data with your choice of technique","60d411c6":"# 5. Binning\nA data pre-processing method used to minimize effects of small observation errors\n\n* Binning is done to create bins for continuous variables where they are converted to categorical variables\n    * Original data values are divided into small intervals known as bins and then \n    * they are replaced by a general value calculated for that bin\n* This has a smoothing effect on input data and may also reduce chances of overfitting in case of small datasets\n\nI am performing Binning for `numerical age data` so that i can prevent overfitting of model and rather deal this column as a categorial feature\n\n`It was observed that grouping into age groups improved model performance, so sometimes in such cases grouping make more sense instead of using continuous numerical distribution for your model`","93ad7cdb":"----\n----\n\n# other notebook links:\n* [Simple Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/linear-regression-basic)\n* [Multiple Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/multiple-linear-regression-basic)\n* [Polynomial Regression](https:\/\/www.kaggle.com\/mukeshmanral\/polynomial-regression-basic)\n* [Advanced Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/advance-linear-regression-basic-gridsearchcv-hpt)\n\n\n\n\n\n\n* [Feature Engineering 1](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-1-basic)\n* [Feature Engineering 2](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-2-basic)\n* [Feature Engineering 3](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-3-basic)\n* [Feature Engineering 4](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-dataset-4-basic)\n\n----\n----","83f73a59":"Now Embarked column still has 2 null values but how do we deal with such categorical imputations ?","e516731b":"1.","48d4d50d":"`Z-score can be used instead of the formula above`\n\n`Z-score`(or Standard score) standardizes distance between a value and mean using standard deviation\n\n\n","7fc703c8":"# An Outlier Dilemma: [Drop or Cap]\n* Another option for handling outliers is to cap them instead of dropping\n* So we can keep data size and at the end of the day, it might be better for final model performance\n* On the other hand, `capping can affect distribution of data`, thus it better not to exaggerate it","fac75fef":"* dropna with thresh parameter set as 70% of data size can also be used to do this","4961faf9":"# 4. Scaling\n* In most cases numerical features of dataset do not have a certain range and they differ from each other\n* In real life, it is nonsense to expect age and Ticket Fare columns to have same range\n    * But from the machine learning point of view, how these two columns can be compared?\n* Scaling solves this problem, continuous features become identical in terms of range, after a scaling process\n    * This process is not mandatory for many algorithms, but it might be still nice to apply\n    \n**`Two common ways of scaling`**:\n1. Normalization (min-max Scaler)\n    * `Xnorm = (X - Xmin) \/ (Xmax - Xmin)`\n    * scales all values in a fixed range between 0 and 1\n    * This transformation does not change distribution of feature and due to the decreased standard deviations, effects of outliers increases\n        * before normalization it is recommended to handle the outliers\n\n\n2. Standardization (z-score normalization)\n    * `Z = (X - Xmean) \/ sigma` ; here sigma = standard deviation\n    * Scales values while taking into account standard deviation\n    * If standard deviation of features is different, their range also would differ from each other\n    * Reduces effect of outliers in features","275e205d":"* Sex is having 2 categories\n* Embarked has 3 categories\n\nLet's use encoding on both the columns","27a0f174":"Observe 'cabin' column from data is dropped\n\nBut that's not it there're still cloulmns where there are NaN values which we need to take care of","37a07932":"2.","a556f2e7":"2.","cf1fb269":"# 3. Handling Outliers\nBest way to detect outliers is to demonstrate data visually \n\nAll other statistical methodologies are open to making mistakes, whereas visualizing outliers gives a chance to take a decision with high precision\n\nStatistical methodologies are less precise, but on the other hand, they have a superiority they are fast \n\n\n`Two different ways of handling outliers` \n1. Standard deviation\n    * If a value has a distance to the `average higher than x * standard deviation`, it can be assumed as an outlier\n    * Then what x should be? There is no trivial solution for x, but usually a value between 2 and 4 seems practical\n2. Percentiles \n    * Assume a certain percent of value from top or bottom as an outlier \n    * Key point here is to set percentage value once again, and this depends on distribution of your data as mentioned earlier\n    * `A common mistake is using percentiles according to range of data`\n        * In other words, if data ranges from 0 to 100, top 5% is not the values between 96 and 100\n        * Top 5% means here values that are out of the 95th percentile of data\n\n","514586e8":"# Categorical Imputation\nReplacing missing values with maximum occurred value in a column is a good option as well for handling categorical columns \n\nBut if you think the values in column are distributed uniformly and there is not a dominant value, imputing a category like \u201cOther\u201d might be more sensible, because in such a case, your imputation is likely to converge a random selection","d6651f86":"2.","4b453c85":"I am trying to show some more parts of Feature Enginnering in this notebook for previous Feature Creation part go to this notebook [Link](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-diff-dataset-1\/edit)\n# Feature Engineering Why? an its Significance\nOne of the most important steps on the way to building a great machine learning model\n\nWhat is a feature and why we need to engineer it?\n\ndetermine which features are most important with mutual information\ninvent new features in several real-world problem domains\nencode high-cardinality categoricals with a target encoding\nprepare proper input dataset, compatible with machine learning algorithm requirements\nimprove performance of machine learning models\nAccording to a survey in Forbes, data scientists spend 80% of their time on data preparation. That's why we need feature engineering for extracting useful features from raw data using maths, statistics and domain knowledge\n\nThe best way to achieve expertise is practicing different techniques on various datasets and observing their effect on model performances. So fellas\n\n**`Come Dive with me into various techniques of Feature Engineering`**","7cd32321":"# 6. Encoding Categorical Variables\n* Some algorithms can work with categorical data directly\n    * For example, Decision Tree can be learned directly from categorical data with no data transform required (this depends on the specific implementation)\n\n* Many machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric\n\n\nIn general, this is mostly a constraint of efficient implementation of machine learning algorithms rather than hard limitations on the algorithms themselves\n\n* This means that categorical data must be converted to a numerical form \n* If categorical variable is an output variable, you may also want to convert predictions by the model back into a categorical form in order to present them or use them in some application","3b0d8600":"##  Logarithmic transformation `[ f(x) = ln(x) ]`\n* Simplest and most popular among different types of transformations and involves a substantial transformation that significantly affects distribution shape\n\n* Can use it (`natural logarithmic ln or log base 10`) to make extremely skewed distributions less skewed, especially for right-skewed (+ve vly) distributions\n\n`Note: that this function is defined only for strictly positive numbers`","575bb590":"# Feature Encoding\n1. Integer Encoding (label encoding)\n    * Replacing categories with digits from 1 to n (or 0 to n-1, depending on implementation), where n is number of variable\u2019s distinct categories (cardinality), and these numbers are assigned arbitrarily\n    * For example, \u201cred\u201d is 1, \u201cgreen\u201d is 2, and \u201cblue\u201d is 3\n    * `Note:` disadvantage is that numeric values can be misinterpreted by algorithms as having some sort of hierarchy\/order in them\n    * This ordering issue is addressed in another common alternative approach called \u2018One-Hot Encoding\u2019\n2. One-Hot Encoding\n    * Each category value is converted into a new column and assigned a 1 or 0 (notation for true\/false) value to the column\n","17bd51e8":"# 2. Imputation\n* Most common problem you will encounter when you try to prepare your data for machine learning is about missing values which do affect performance of your machine learning model. \n* Reason for missing values can be human errors, interruptions in data flow, privacy concerns, and so on\n* Most simple solution to the missing values one can think about is to \n    * drop rows or entire column\n    * There is not an optimum threshold for dropping but you can use 70% as an example value and try to drop rows and columns which have missing values with higher than this threshold","b405b4ee":"# Numerical Imputation\nImputation is a more preferable option rather than dropping because it preserves data size\n\n* There is an important selection of what you impute to missing values \n* I suggest beginning with considering a possible default value of missing values in column \n    * For example, if you have a column that only has 1 and NA, then it is likely that the NA rows correspond to 0 \n    * For another example, if you have a column that shows the \u201ccustomer visit count in last month\u201d, missing values might be replaced with 0 as long as you think it is a sensible solution\n* Another reason for missing values is joining tables with different sizes and in this case, imputing 0 might be reasonable as well\n    * Except for the case of having a default value for missing values \n* I guess best imputation way is to use medians of columns As the averages of columns are sensitive to outlier values, while medians are more solid in this respect"}}