{"cell_type":{"f8acc67e":"code","a324ea07":"code","1518593e":"code","6286b466":"code","17c84cec":"code","9268d002":"code","0a55503c":"code","b05e6b33":"code","51495fe3":"code","c5c29cfd":"code","d4cfac08":"code","2e82b918":"code","ac958d20":"code","6b9565ee":"code","5bd056b3":"code","67898888":"code","d80752e9":"code","70f01de9":"code","ee314389":"code","b5e416b1":"code","b1399c74":"code","daee95f4":"code","5c81187d":"code","1835df8f":"code","51725428":"code","1b1b9d1f":"code","51ec5232":"code","862f735f":"code","26cb07ef":"code","2630c669":"code","3eb78825":"code","637d3a29":"code","7a68c706":"code","97b9026f":"code","7ba59c13":"code","b1a76133":"code","ef7d379c":"code","02c7d5db":"code","eed996ff":"markdown","7efd07d4":"markdown","e2e9cb54":"markdown","524de1a2":"markdown","75e073c4":"markdown","d717a52d":"markdown","290cd8a2":"markdown","071927d5":"markdown","a1fb0285":"markdown","5b651a01":"markdown","6e80b049":"markdown","2b889b6a":"markdown","fc27dabc":"markdown","f3495caa":"markdown","b31de267":"markdown","d58d7fbb":"markdown","f8f273ea":"markdown","57a827b2":"markdown","48b759e7":"markdown","13133277":"markdown","2c350f5f":"markdown","f493a3b1":"markdown","e8945f5a":"markdown","5c19cd4e":"markdown"},"source":{"f8acc67e":"# Libraries\nimport numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn import metrics\nimport json\nimport ast\nimport time\nfrom sklearn import linear_model\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap\nfrom tqdm import tqdm_notebook\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFE\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom catboost import CatBoostClassifier\n\n# import json\nimport altair as alt\nfrom  altair.vega import v3\nfrom IPython.display import HTML\n\nfrom plotly import tools\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)","a324ea07":"!pip install ujson","1518593e":"import ujson as json","6286b466":"# Preparing altair. I use code from this great kernel: https:\/\/www.kaggle.com\/notslush\/altair-visualization-2018-stackoverflow-survey\n\nvega_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega@' + v3.SCHEMA_VERSION\nvega_lib_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lib'\nvega_lite_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lite@' + alt.SCHEMA_VERSION\nvega_embed_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-embed@3'\nnoext = \"?noext\"\n\npaths = {\n    'vega': vega_url + noext,\n    'vega-lib': vega_lib_url + noext,\n    'vega-lite': vega_lite_url + noext,\n    'vega-embed': vega_embed_url + noext\n}\n\nworkaround = \"\"\"\nrequirejs.config({{\n    baseUrl: 'https:\/\/cdn.jsdelivr.net\/npm\/',\n    paths: {}\n}});\n\"\"\"\n\n#------------------------------------------------ Defs for future rendering\ndef add_autoincrement(render_func):\n    # Keep track of unique <div\/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n            \n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    chart_str = \"\"\"\n    <div id=\"{id}\"><\/div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    <\/script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n\nHTML(\"\".join((\n    \"<script>\",\n    workaround.format(json.dumps(paths)),\n    \"<\/script>\",\n)))","17c84cec":"train = pd.read_csv('..\/input\/train_features.csv', index_col='match_id_hash')\ntarget = pd.read_csv('..\/input\/train_targets.csv', index_col='match_id_hash')\ntest = pd.read_csv('..\/input\/test_features.csv', index_col='match_id_hash')","9268d002":"target.head()","0a55503c":"target['radiant_win'].value_counts()","b05e6b33":"train.head()","51495fe3":"print(f'Number of samples in train: {train.shape[0]}')\nprint(f'Number of columns in train: {train.shape[1]}')\nfor col in train.columns:\n    if train[col].isnull().any():\n        print(col, train[col].isnull().sum())","c5c29cfd":"train['game_mode'].value_counts()","d4cfac08":"ax = train['game_mode'].value_counts().plot(kind='bar', title='Counts of games in different modes');\nax.set_xlabel(\"Game mode\");\nax.set_ylabel(\"Counts\");","2e82b918":"train_modes = train['game_mode'].value_counts().reset_index().rename(columns={'index': 'game_mode', 'game_mode': 'count'})\ntrain_modes","ac958d20":"plt.bar(range(len(train_modes['game_mode'])), train_modes['count']);\nplt.xticks(range(len(train_modes['game_mode'])), train_modes['game_mode']);\nplt.xlabel('Game mode');\nplt.ylabel('Counts');\nplt.title('Counts of games in different modes');","6b9565ee":"sns.countplot(data=train, x='game_mode', order=train['game_mode'].value_counts().index);\nplt.title('Counts of games in different modes');","5bd056b3":"train_modes['game_mode'] = train_modes['game_mode'].astype(str)\ndata=[go.Bar(\n    x=train_modes['game_mode'],\n    y=train_modes['count'],\n    name='Game mode'\n)]\n\nlayout = go.Layout(title='Counts of games in different modes',\n                  xaxis=dict(title='Game mode'),\n                  yaxis=dict(title='Count'))\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='bar')","67898888":"render(alt.Chart(train_modes).mark_bar().encode(\n    x=alt.X(\"game_mode:N\", axis=alt.Axis(title='Game modes'), sort=list(train_modes['game_mode'].values)),\n    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n    tooltip=['game_mode', 'count']\n).properties(title=\"Counts of games in different modes\", width=400).interactive())","d80752e9":"train['radiant_win'] = target['radiant_win']","70f01de9":"plt.hist(train['game_time'], bins=40, label='Train');\nplt.hist(test['game_time'], bins=40, label='Test');\nplt.title('Distribution of game time');\nplt.legend();","ee314389":"train_games = alt.Chart(train_modes).mark_bar().encode(\n    x=alt.X(\"game_mode:N\", axis=alt.Axis(title='Game modes'), sort=list(train_modes['game_mode'].values)),\n    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n    tooltip=['game_mode', 'count']\n).properties(title=\"Counts of games in different modes in train data\", width=400).interactive()\n\ntest_modes = test['game_mode'].value_counts().reset_index().rename(columns={'index': 'game_mode', 'game_mode': 'count'})\ntest_games = alt.Chart(test_modes).mark_bar().encode(\n    x=alt.X(\"game_mode:N\", axis=alt.Axis(title='Game modes'), sort=list(train_modes['game_mode'].values)),\n    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n    tooltip=['game_mode', 'count']\n).properties(title=\"Counts of games in different modes in test data\", width=400).interactive()\n\nd = train.groupby(['game_mode', 'radiant_win'])['game_time'].count().reset_index().rename(columns={'game_time': 'count'})\ntrain_r = alt.Chart(d).mark_bar().encode(\n    x=alt.X(\"radiant_win:N\", axis=alt.Axis(title='Radiant win'), sort=list(train_modes['game_mode'].values)),\n    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n    column='game_mode',\n    color='radiant_win:N',\n    tooltip=['game_mode', 'radiant_win', 'count']\n).properties(title=\"Counts of wins and losses by game mode\", width=100).interactive()\n\nrender(train_games | test_games)","b5e416b1":"render(train_r)","b1399c74":"plt.hist(train.loc[train['radiant_win'] == True, 'r1_gold'], bins=40, label='Gold by r1 player of a winning team');\nplt.hist(train.loc[train['radiant_win'] == False, 'r1_gold'], bins=40, label='Gold by r1 player of a losing team');\nplt.hist(test['r1_gold'], bins=40, label='Gold by r1 player in test data');\nplt.title('Distribution of game time');\nplt.legend();","daee95f4":"n_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)","5c81187d":"def train_model(X, X_test, y, params, folds, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.loc[train_index], X.loc[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if model_type == 'lgb':\n            train_data = lgb.Dataset(X_train, label=y_train)\n            valid_data = lgb.Dataset(X_valid, label=y_valid)\n            \n            model = lgb.train(params,\n                    train_data,\n                    num_boost_round=20000,\n                    valid_sets = [train_data, valid_data],\n                    verbose_eval=1000,\n                    early_stopping_rounds = 200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_train.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_train.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict_proba(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            # print(f'Fold {fold_n}. AUC: {score:.4f}.')\n            # print('')\n            \n            y_pred = model.predict_proba(X_test)[:, 1]\n            \n        if model_type == 'glm':\n            model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n            model_results = model.fit()\n            model_results.predict(X_test)\n            y_pred_valid = model_results.predict(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            \n            y_pred = model_results.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000, learning_rate=0.05, loss_function='Logloss',  eval_metric='AUC', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n            y_pred = model.predict_proba(X_test)[:, 1]\n            \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(roc_auc_score(y_valid, y_pred_valid))\n\n        if averaging == 'usual':\n            prediction += y_pred\n        elif averaging == 'rank':\n            prediction += pd.Series(y_pred).rank().values  \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importance()\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction, scores\n    \n    else:\n        return oof, prediction, scores","1835df8f":"params = {'boost': 'gbdt',\n          'feature_fraction': 0.05,\n          'learning_rate': 0.01,\n          'max_depth': -1,  \n          'metric':'auc',\n          'min_data_in_leaf': 50,\n          'num_leaves': 32,\n          'num_threads': -1,\n          'verbosity': 1,\n          'objective': 'binary'\n         }\n\nX = train.drop(['radiant_win'], axis=1).reset_index(drop=True)\ny = train['radiant_win']\nX_test = test.copy().reset_index(drop=True)\n\noof_lgb, prediction_lgb, scores = train_model(X, X_test, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)","51725428":"for c in ['kills', 'deaths', 'assists', 'denies', 'gold', 'lh', 'xp', 'health', 'max_health', 'max_mana', 'level', 'x', 'y', 'stuns', 'creeps_stacked', 'camps_stacked', 'rune_pickups',\n          'firstblood_claimed', 'teamfight_participation', 'towers_killed', 'roshans_killed', 'obs_placed', 'sen_placed']:\n    r_columns = [f'r{i}_{c}' for i in range(1, 6)]\n    d_columns = [f'd{i}_{c}' for i in range(1, 6)]\n    \n    train['r_total_' + c] = train[r_columns].sum(1)\n    train['d_total_' + c] = train[d_columns].sum(1)\n    train['total_' + c + '_ratio'] = train['r_total_' + c] \/ train['d_total_' + c]\n    \n    test['r_total_' + c] = test[r_columns].sum(1)\n    test['d_total_' + c] = test[d_columns].sum(1)\n    test['total_' + c + '_ratio'] = test['r_total_' + c] \/ test['d_total_' + c]\n    \n    train['r_std_' + c] = train[r_columns].std(1)\n    train['d_std_' + c] = train[d_columns].std(1)\n    train['std_' + c + '_ratio'] = train['r_std_' + c] \/ train['d_std_' + c]\n    \n    test['r_std_' + c] = test[r_columns].std(1)\n    test['d_std_' + c] = test[d_columns].std(1)\n    test['std_' + c + '_ratio'] = test['r_std_' + c] \/ test['d_std_' + c]\n    \n    train['r_mean_' + c] = train[r_columns].mean(1)\n    train['d_mean_' + c] = train[d_columns].mean(1)\n    train['mean_' + c + '_ratio'] = train['r_mean_' + c] \/ train['d_mean_' + c]\n    \n    test['r_mean_' + c] = test[r_columns].mean(1)\n    test['d_mean_' + c] = test[d_columns].mean(1)\n    test['mean_' + c + '_ratio'] = test['r_mean_' + c] \/ test['d_mean_' + c]","1b1b9d1f":"X = train.drop(['radiant_win'], axis=1).reset_index(drop=True)\ny = train['radiant_win']\nX_test = test.copy().reset_index(drop=True)\n\noof_lgb, prediction_lgb, scores = train_model(X, X_test, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)","51ec5232":"with open(os.path.join('..\/input\/', 'train_matches.jsonl')) as fin:\n    # read the 18-th line\n    for i in range(18):\n        line = fin.readline()\n    \n    # read JSON into a Python object \n    match = json.loads(line)","862f735f":"match.keys()","26cb07ef":"def read_matches(matches_file):\n    \n    MATCHES_COUNT = {\n        'test_matches.jsonl': 10000,\n        'train_matches.jsonl': 39675,\n    }\n    _, filename = os.path.split(matches_file)\n    total_matches = MATCHES_COUNT.get(filename)\n    \n    with open(matches_file) as fin:\n        for line in tqdm_notebook(fin, total=total_matches):\n            yield json.loads(line)","2630c669":"import collections\n\nMATCH_FEATURES = [\n    ('game_time', lambda m: m['game_time']),\n    ('game_mode', lambda m: m['game_mode']),\n    ('lobby_type', lambda m: m['lobby_type']),\n    ('objectives_len', lambda m: len(m['objectives'])),\n    ('chat_len', lambda m: len(m['chat'])),\n]\n\nPLAYER_FIELDS = [\n    'hero_id',\n    \n    'kills',\n    'deaths',\n    'assists',\n    'denies',\n    \n    'gold',\n    'lh',\n    'xp',\n    'health',\n    'max_health',\n    'max_mana',\n    'level',\n\n    'x',\n    'y',\n    \n    'stuns',\n    'creeps_stacked',\n    'camps_stacked',\n    'rune_pickups',\n    'firstblood_claimed',\n    'teamfight_participation',\n    'towers_killed',\n    'roshans_killed',\n    'obs_placed',\n    'sen_placed',\n]\n\ndef extract_features_csv(match):\n    row = [\n        ('match_id_hash', match['match_id_hash']),\n    ]\n    \n    for field, f in MATCH_FEATURES:\n        row.append((field, f(match)))\n        \n    for slot, player in enumerate(match['players']):\n        if slot < 5:\n            player_name = 'r%d' % (slot + 1)\n        else:\n            player_name = 'd%d' % (slot - 4)\n\n        for field in PLAYER_FIELDS:\n            column_name = '%s_%s' % (player_name, field)\n            row.append((column_name, player[field]))\n        row.append((f'{player_name}_ability_level', len(player['ability_upgrades'])))\n        row.append((f'{player_name}_max_hero_hit', player['max_hero_hit']['value']))\n        row.append((f'{player_name}_purchase_count', len(player['purchase_log'])))\n        row.append((f'{player_name}_count_ability_use', sum(player['ability_uses'].values())))\n        row.append((f'{player_name}_damage_dealt', sum(player['damage'].values())))\n        row.append((f'{player_name}_damage_received', sum(player['damage_taken'].values())))\n            \n    return collections.OrderedDict(row)\n    \ndef extract_targets_csv(match, targets):\n    return collections.OrderedDict([('match_id_hash', match['match_id_hash'])] + [\n        (field, targets[field])\n        for field in ['game_time', 'radiant_win', 'duration', 'time_remaining', 'next_roshan_team']\n    ])","3eb78825":"%%time\nPATH_TO_DATA = '..\/input\/'\ndf_new_features = []\ndf_new_targets = []\n\nfor match in read_matches(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')):\n    match_id_hash = match['match_id_hash']\n    features = extract_features_csv(match)\n    targets = extract_targets_csv(match, match['targets'])\n    \n    df_new_features.append(features)\n    df_new_targets.append(targets)\n    ","637d3a29":"df_new_features = pd.DataFrame.from_records(df_new_features).set_index('match_id_hash')\ndf_new_targets = pd.DataFrame.from_records(df_new_targets).set_index('match_id_hash')","7a68c706":"test_new_features = []\nfor match in read_matches(os.path.join(PATH_TO_DATA, 'test_matches.jsonl')):\n    match_id_hash = match['match_id_hash']\n    features = extract_features_csv(match)\n    \n    test_new_features.append(features)\ntest_new_features = pd.DataFrame.from_records(test_new_features).set_index('match_id_hash')","97b9026f":"df_new_features.shape","7ba59c13":"for c in ['kills', 'deaths', 'assists', 'denies', 'gold', 'lh', 'xp', 'health', 'max_health', 'max_mana', 'level', 'x', 'y', 'stuns', 'creeps_stacked', 'camps_stacked', 'rune_pickups',\n          'firstblood_claimed', 'teamfight_participation', 'towers_killed', 'roshans_killed', 'obs_placed', 'sen_placed', 'ability_level', 'max_hero_hit', 'purchase_count',\n          'count_ability_use', 'damage_dealt', 'damage_received']:\n    r_columns = [f'r{i}_{c}' for i in range(1, 6)]\n    d_columns = [f'd{i}_{c}' for i in range(1, 6)]\n    \n    df_new_features['r_total_' + c] = df_new_features[r_columns].sum(1)\n    df_new_features['d_total_' + c] = df_new_features[d_columns].sum(1)\n    df_new_features['total_' + c + '_ratio'] = df_new_features['r_total_' + c] \/ df_new_features['d_total_' + c]\n    \n    test_new_features['r_total_' + c] = test_new_features[r_columns].sum(1)\n    test_new_features['d_total_' + c] = test_new_features[d_columns].sum(1)\n    test_new_features['total_' + c + '_ratio'] = test_new_features['r_total_' + c] \/ test_new_features['d_total_' + c]\n    \n    df_new_features['r_std_' + c] = df_new_features[r_columns].std(1)\n    df_new_features['d_std_' + c] = df_new_features[d_columns].std(1)\n    df_new_features['std_' + c + '_ratio'] = df_new_features['r_std_' + c] \/ df_new_features['d_std_' + c]\n    \n    test_new_features['r_std_' + c] = test_new_features[r_columns].std(1)\n    test_new_features['d_std_' + c] = test_new_features[d_columns].std(1)\n    test_new_features['std_' + c + '_ratio'] = test_new_features['r_std_' + c] \/ test_new_features['d_std_' + c]\n    \n    df_new_features['r_mean_' + c] = df_new_features[r_columns].mean(1)\n    df_new_features['d_mean_' + c] = df_new_features[d_columns].mean(1)\n    df_new_features['mean_' + c + '_ratio'] = df_new_features['r_mean_' + c] \/ df_new_features['d_mean_' + c]\n    \n    test_new_features['r_mean_' + c] = test_new_features[r_columns].mean(1)\n    test_new_features['d_mean_' + c] = test_new_features[d_columns].mean(1)\n    test_new_features['mean_' + c + '_ratio'] = test_new_features['r_mean_' + c] \/ test_new_features['d_mean_' + c]","b1a76133":"X = df_new_features.reset_index(drop=True)\nX_test = test_new_features.copy().reset_index(drop=True)\n\noof_lgb, prediction_lgb, scores = train_model(X, X_test, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)","ef7d379c":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['radiant_win_prob'] = prediction_lgb\nsub.to_csv('submission.csv', index=False)\nsub.head()","02c7d5db":"plt.hist(prediction_lgb, bins=40);\nplt.title('Distribution of predictions');","eed996ff":"### game_mode","7efd07d4":"### game_time","e2e9cb54":"### Pandas plotting","524de1a2":"### Player features\n\nHere we have features for separate players. Let's have a look at the distribution of one of them.","75e073c4":"### players\n\n@yorko has already extracted a lot of features from players data, but let's try to add more.\nHere is the list of additional features:\n- max ability level,\n- max_hero_hit,\n- amount of purchases,\n- count of abilities used,\n- total damage dealt,\n- total damage received,","d717a52d":"Nothing interesting... as it should be expected!\nThis feature shows gold earned by a first player. Of course distributions of this feature in train and test should be similar, as first player is chosen randomly.\n\nLet's build a first model on the existing features and look at its quality.","290cd8a2":"## A short notice on data visualization\n\nCurrently there are a lot of great libraries for data visualization in Python and I usually use several of them:\n- basic pandas plotting: this is a simplified version of using matplotlib, which can be used for some fast and simple plotting;\n- matplotlib: you can do everything, though it can require a lot of code. Also making interactive plots is difficult or impossible;\n- seaborn: it is good when you want to plot interactions between several features;\n- plotly: I used it to make interactive plots and it is great for this, but recently I switched to altair;\n- altair: this is a python wrapper for vega-lite. It is possible to make almost any plot there and interactivity is great. On the other hand it could be difficult to get used to it's syntax;\n\nLet's see how to plot a barchart in different libraries. We'll plot a countplot for game_mode in train data.","071927d5":"### Seaborn","a1fb0285":"First thing worth noticing - there are 8 game modes in train data and only 7 games in test data. But the game mode `16` in train data has only 2 samples, so we can combine it with other game types. Otherwise the distribution of game modes seems similar.\n\nWins and losses are more or less equally distributed over game modes.","5b651a01":"Training function","6e80b049":"### Plotly","2b889b6a":"### First model","fc27dabc":"### Feature engineering on basic data\n\nMy idea behind this FE is the following:\nLet's take gold, for example. Gold earned by each player can't give us a lot of information. But what is we take total gold by the team? Maybe teams with more gold earned usually win. What if we take mean and std of players' gold in a team? Maybe teams where players tend to have similar parameters are more likely to win. Let's try creating these features.","f3495caa":"## Data exploration","b31de267":"### Matplotlib","d58d7fbb":"We can see that the distribution of game time in train and test data is quite similar.","f8f273ea":"So this was our baseline model.\nWe can see that features in top-50 show variuos aspects of player activity, but gold and health are prevalent. Let's get started with feature engineering.","57a827b2":"Now let's try building a new model!","48b759e7":"### General information\n\nThis kernel is intented to help mlcourse.ai participants with doing EDA, Feature Engineering and building models.\n\n* At first I'll do basic EDA of the data;\n* After this I'll build a baseline model to see how good model can be on the basic data;\n* Then I'll create new features based on the main features and train a model again to see whether there is an improvement;\n* After this I'll try to extract new features from json files and see whether it helps;","13133277":"### Altair\nFor altair to show in kaggle kernels, it is necessary to wrap in in a special function.","2c350f5f":"## Data overview","f493a3b1":"There is some disbalance in data, but it isn't too high, so we don't need to do anything special to tackle it.","e8945f5a":"### Working with json data\n\nI'll use functions from this kernel: https:\/\/www.kaggle.com\/kashnitsky\/dota-2-win-prediction-random-forest-starter","5c19cd4e":"As we can see, the score increased substantially thanks to the new features. Surprisingly hero id is still one of the top features.\n\nBut we used only the features from the basic features. Let's try working with json data to get more cool data!"}}