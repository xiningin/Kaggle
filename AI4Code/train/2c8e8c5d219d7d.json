{"cell_type":{"c0ba920f":"code","48701050":"code","9122ff64":"code","953d0c9b":"code","76b52021":"code","5ea7b4f5":"code","afe6d383":"code","9584405c":"code","28ae828b":"code","9b087041":"code","2a282149":"code","c8491092":"code","8b2d14b2":"markdown","9167b613":"markdown","be41e050":"markdown","a661a855":"markdown","3b219597":"markdown","d5c7926e":"markdown","e59ddabd":"markdown","7e04e5df":"markdown","2e9cd6b2":"markdown","78bbd5b3":"markdown","6aa1fc8b":"markdown","bce3f720":"markdown","56e749b7":"markdown","516e596c":"markdown","7ee696ff":"markdown","9a4a7b6b":"markdown","34648d78":"markdown","7537d2e2":"markdown","4eb5ed79":"markdown","243cb4b0":"markdown","64010654":"markdown","c86b0525":"markdown","aedfc48b":"markdown"},"source":{"c0ba920f":"from IPython.display import Image\nImage(\"..\/input\/windygridworld\/Ex6.5_WindyGridWorld.png\")","48701050":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport pylab as pl\nimport matplotlib.pyplot as plt \nimport time # We will use this to time our experiments for various observations\n\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9122ff64":"class gridWorld(object):\n    \n    def __init__(self):\n        super(gridWorld, self).__init__()\n        self.start = 0\n        self.goal = 0\n        # Provide the setting of gridWorld rows and columns\n        self.row = 7\n        self.col = 10\n        self.x_max = self.col - 1\n        self.y_max = self.row - 1\n        # Declare windy columns and their effect\n        self.wind_1 = [3, 4, 5, 8]\n        self.wind_2 = [6, 7]\n        # Provide action list : Up\/Down\/Left\/Right\n        self.actions_list = ['N', 'E', 'S', 'W']\n        \n    def cell(self,pos):\n        return pos[1] + self.col * pos[0]\n    \n    def setTerminal(self, startState, goalState):\n        # startState and goalState are tuples\n        self.start = self.cell(startState)\n        self.goal = self.cell(goalState)\n        \n    def nextState(self, state, action):\n        # The \"state\" parameter is an integer that represents position in grid\n        x = state % self.col\n        y = (state - x) \/ self.col\n        # Now let's define and interpret actions\n        del_x = 0\n        del_y = 0\n        if action == 'E':\n            del_x = 1\n        elif action == 'W':\n            del_x = -1\n        elif action == 'N':\n            del_y = -1\n        elif action == 'S':\n            del_y = 1\n        else:\n            raise('Invalid action! Actions taken must be in: ',self.actions_list)\n        # Now move to new position\n        new_x = max(0, min(x + del_x, self.x_max))\n        new_y = max(0, min(y + del_y, self.y_max))\n        # When wind blows to a new state\n        if new_x in self.wind_1:\n            new_y = max(0, new_y - 1)\n        if new_x in self.wind_2:\n            new_y = max(0, new_y - 2)\n        # now let's return these new state values\n        return self.cell((new_y,new_x))\n    \n    def checkTerminal(self, state):\n        return state == self.goal\n    \n    def rewardFunction(self, state_prime):\n        # When it reaches goal state then reward is 0 else all actions should yield -1\n        if state_prime == self.goal:\n            return 0\n        else:\n            return -1\n        \ndef trajectoryPath(world, traj):\n    # Initialize gridworld\n    world_map = np.zeros((world.row, world.col))\n    for i,state in enumerate(traj):\n        x = int(state % world.col)\n        y = int((state - x) \/ world.col)\n        world_map[y, x] = i + 1\n    print(world_map)\n    print(\"\\n\")","953d0c9b":"def gridWorld_SARSA(world, startState, goalState, alpha=0.1, gamma=1, epsilon=0.1):\n    # Consider input parameters:\n    # gamma = 1 as discounted factor\n    # default values of alpha and epsilon considered, it will be changed for multiple analysis\n    world.setTerminal(startState, goalState)\n    # Initialize Q(s,a)\n    q_table = {}\n    for state in range(world.row * world.col):\n        q_table[state] = {}\n        for act in world.actions_list:\n            q_table[state][act] = 0\n    \n    # function for greedy action\n    def epsGreedy(episode, q_dict):\n        def greedyAct(_q_dict):\n            greedy_act = ''\n            max_q = -1e10\n            for act in world.actions_list:\n                if _q_dict[act] > max_q:\n                    greedy_act = act\n                    max_q = _q_dict[act]\n            return greedy_act\n        \n        m = len(world.actions_list)\n        greedy_act = greedyAct(q_dict)\n        p = []\n        for act in world.actions_list:\n            if act == greedy_act:\n                p.append((epsilon * 1. \/ m) + 1 - epsilon)\n            else:\n                p.append(epsilon * 1. \/ m)\n        choice = np.random.choice(world.actions_list,size = 1, p = p)\n        return choice[0]\n    \n    ep = 1 # Episode number\n    ep_max = 300 # number of max episodes\n    step_ep_list = []\n    step = 0\n    while ep < ep_max:\n        #print(\"Episode: \", ep)\n        # Initialize state\n        state = world.cell(startState)\n        trajectory = [state]\n        # Choose action from state\n        act = epsGreedy(ep, q_table[state])\n        while not world.checkTerminal(state):\n            state_prime = world.nextState(state, act)\n            reward = world.rewardFunction(state_prime)\n            act_prime = epsGreedy(ep, q_table[state_prime])\n            q_table[state][act] += alpha * (reward + gamma * q_table[state_prime][act_prime] - q_table[state][act])\n            state = state_prime\n            act = act_prime\n            # Increase Step Counter\n            step += 1\n            # Store the index of the episode of this time step\n            step_ep_list.append(ep)\n            # Update the trajectory\n            trajectory.append(state)\n        \n        if ep == (ep_max - 1):\n            trajectoryPath(world, trajectory)\n        # Increase the episode counter\n        ep += 1\n    \n    start_time = time.time()\n    plt.plot(step_ep_list)\n    print(\"Time elapsed is (in Secs): \", time.time() - start_time)\n    plt.title('WindyGridWorld_SARSA ', fontsize = 'large')\n    plt.xlabel(\"Number of Steps taken\")\n    plt.ylabel(\"Number of Episodes\")\n    plt.show()","76b52021":"if __name__ == '__main__':\n    startState = (3, 0)\n    goalState = (3, 7)\n    world = gridWorld()\n    print(\"Plot for Option: \u2208=0.1,\u03b1=0.5 \\n\")\n    gridWorld_SARSA(world, startState, goalState, alpha=0.5, epsilon=0.1)","5ea7b4f5":"if __name__ == '__main__':\n    startState = (3, 0)\n    goalState = (3, 7)\n    world = gridWorld()\n    print(\"Plot for Option: \u2208=0.2,\u03b1=0.1 \\n\")\n    gridWorld_SARSA(world, startState, goalState, alpha=0.1, epsilon=0.2)","afe6d383":"if __name__ == '__main__':\n    startState = (3, 0)\n    goalState = (3, 7)\n    world = gridWorld()\n    print(\"Plot for Option: \u2208=0.05,\u03b1=0.2 \\n\")\n    gridWorld_SARSA(world, startState, goalState, alpha=0.2, epsilon=0.05)","9584405c":"def gridWorld_QLearning(world, startState, goalState, alpha, gamma=1, ep_max=300, eps=0.1):\n    world.setTerminal(startState, goalState) \n    # Initialize Q(s, a)\n    q_dict = {}\n    for state in range(world.row * world.col):\n        q_dict[state] = {}\n        for act in world.actions_list:\n            if world.checkTerminal(state):\n                q_dict[state][act] = 0\n            else:\n                q_dict[state][act] = np.random.rand()\n\n    def greedyAct(_q_dict):\n        greedy_act = ''\n        max_q = -1e10\n        for act in world.actions_list:\n            if _q_dict[act] > max_q:\n                greedy_act = act\n                max_q = _q_dict[act]\n        return greedy_act\n\n    def epsGreedy(episode, q_dict):\n        m = len(world.actions_list)\n        greedy_act = greedyAct(q_dict)\n        p = []\n        for act in world.actions_list:\n            if act == greedy_act:\n                p.append((eps * 1. \/ m) + 1 - eps)\n            else:\n                p.append(eps * 1. \/ m)\n        choice = np.random.choice(world.actions_list, size=1, p=p)\n        return choice[0]\n\n    ep_wrt_step = []\n    trajectory = []\n    for ep in range(1, ep_max + 1):\n        s = world.start\n        trajectory = []\n        while not world.checkTerminal(s):\n            # Choose act according to behaviour policy\n            act = epsGreedy(ep, q_dict[s])\n            # take act, observe s_prime & reward\n            s_prime = world.nextState(s, act)\n            reward = world.rewardFunction(s_prime)\n\n            # choose act_prime according to target policy\n            act_prime = greedyAct(q_dict[s_prime])\n            # Update Q(s, a)\n            q_dict[s][act] += alpha * (reward + gamma * q_dict[s_prime][act_prime] - q_dict[s][act])\n\n            # store trajectory\n            trajectory.append(s)\n\n            # update current state\n            s = s_prime\n            # store the index of this episode for plot\n            ep_wrt_step.append(ep)\n        trajectory.append(world.goal)\n    return trajectory, ep_wrt_step","28ae828b":"if __name__ == '__main__':\n    _start = (3, 0)\n    _goal = (3, 7)\n    world = gridWorld()\n    start_time = time.time()\n    trajectory, ep_wrt_step = gridWorld_QLearning(world, startState, goalState, alpha=0.5,gamma=0.9,ep_max=300, eps=0.1)\n    print(\"Time elapsed: \", time.time() - start_time)\n    trajectoryPath(world, trajectory)\n    pl.figure(1)\n    pl.plot(ep_wrt_step)\n    plt.title('WindyGridworld_Q-learning (\u2208=0.1,\u03b1=0.5)', fontsize = 'large')\n    pl.xlabel(\"Number of steps taken\")\n    pl.ylabel(\"Number of episodes\")\n    pl.show()","9b087041":"if __name__ == '__main__':\n    _start = (3, 0)\n    _goal = (3, 7)\n    world = gridWorld()\n    start_time = time.time()\n    trajectory, ep_wrt_step = gridWorld_QLearning(world, startState, goalState, alpha=0.1,gamma=0.9,ep_max=300, eps=0.2)\n    print(\"Time elapsed: \", time.time() - start_time)\n    trajectoryPath(world, trajectory)\n    pl.figure(1)\n    pl.plot(ep_wrt_step)\n    plt.title('WindyGridworld_Q-learning (\u2208=0.2,\u03b1=0.1)', fontsize = 'large')\n    pl.xlabel(\"Number of steps taken\")\n    pl.ylabel(\"Number of episodes\")\n    pl.show()","2a282149":"if __name__ == '__main__':\n    _start = (3, 0)\n    _goal = (3, 7)\n    world = gridWorld()\n    start_time = time.time()\n    trajectory, ep_wrt_step = gridWorld_QLearning(world, startState, goalState, alpha=0.2,gamma=0.9,ep_max=300, eps=0.05)\n    print(\"Time elapsed: \", time.time() - start_time)\n    trajectoryPath(world, trajectory)\n    pl.figure(1)\n    pl.plot(ep_wrt_step)\n    plt.title('WindyGridworld_Q-learning (\u2208=0.05,\u03b1=0.2)', fontsize = 'large')\n    pl.xlabel(\"Number of steps taken\")\n    pl.ylabel(\"Number of episodes\")\n    pl.show()","c8491092":"from IPython.display import Image\nImage(\"..\/input\/windygridworld2\/WindyGridworld_SARSAvsQlearning.png\")","8b2d14b2":"# 3.3 Experiment Observations for SARSA, \u2208=0.05,\u03b1=0.2","9167b613":"#### Observation: We observed that it took 18 steps to reach GOAL state for Q-learning with \u2208=0.1,\u03b1=0.5 option. Discounted factor gamma is set as 1.","be41e050":"# 5. Summary of Interpretation","a661a855":"#### We observed that it took around 26 steps to reach GOAL state for Q-learning with \u2208=0.2,\u03b1=0.1 option. Discounted factor gamma is set as 1.\n\n#### Now, we will try to experiment the next option.","3b219597":"# 2. Define GridWorld Class\n\n#### Let us define the gridWorld class with settings that are considered.","d5c7926e":"# 3.2 Experiment Observations for SARSA, \u2208=0.2,\u03b1=0.1","e59ddabd":"#### Observation: We observed that it took 18 steps to reach GOAL state for Q-learning with \u2208=0.05,\u03b1=0.2 option. Discounted factor gamma is set as 1.","7e04e5df":"* Apply SARSA algorithm with below considerations:\n    * a) Epsilon = 0.1, Alpha = 0.5\n    * b) Epsilon = 0.2, Alpha = 0.1\n    * c) Epsilon = 0.05, Alpha = 0.2\n\n* Apply Q-Learning algorithm with below considerations:\n    * a) Epsilon = 0.1, Alpha = 0.5\n    * b) Epsilon = 0.2, Alpha = 0.1\n    * c) Epsilon = 0.05, Alpha = 0.2\n\nWe will have to run above experiments for 300 episodes and plot charts. (Episodes vs No of time steps)\n\n* We also need to report the minimum length episode that each algorithm is able to learn.","2e9cd6b2":"#### We observed that it took 18 steps to reach GOAL state for SARSA with \u2208=0.05,\u03b1=0.2 option. Discounted factor gamma is set as 1.\n\n#### We will also see what is the minimum number of episode that the SARSA algorithm is able to learn.","78bbd5b3":"#### We are considering the \"Windy Grid World\" setting here as in the Sutton and Barto Book. \n#### We will consider the same setting of (0-0-0-1-1-1-2-2-1-0).","6aa1fc8b":"So we have experimented both SARSA and Q-learning algorithms for 300 episodes and drawn plots for each of the 3 options of \u2208 and \u03b1. We summarize the following:\n\n- 3a) SARSA, 300 episodes, \u2208=0.1, \u03b1=0.5 => we observed 19 minimum steps taken to reach goal state\n- 3b) SARSA, 300 episodes, \u2208=0.2, \u03b1=0.1 => we observed 25 minimum steps taken to reach goal state\n- 3c) SARSA, 300 episodes, \u2208=0.05, \u03b1=0.2 => we observed 18 minimum steps taken to reach goal state\n\n\n- 4a) Q-learning, 300 episodes, \u2208=0.1, \u03b1=0.5 => we observed 18 minimum steps taken to reach goal state\n- 4b) Q-learning, 300 episodes, \u2208=0.2, \u03b1=0.1 => we observed 28 minimum steps taken to reach goal state\n- 4c) Q-learning, 300 episodes, \u2208=0.05, \u03b1=0.2 => we observed 19 minimum steps taken to reach goal state\n\nWe have plotted charts for \"Number of Episodes\" vs \"Number of Steps taken\" for each of the above experiments.\n\nWe have observed that initially both algorithms learn in a similar fashion for few episodes. After 25-30 episodes, Q-learning started learning better and quickly compared to SARSA for \u2208=0.1, \u03b1=0.5\n\nWe have also reported minimum length episode that each algorithm \/ and it's experiments have taken to learn.","bce3f720":"# 4.2 Experiment Observations for Q-learning, \u2208=0.2,\u03b1=0.1","56e749b7":"# 1. Import Libraries","516e596c":"# 3. SARSA Algorithm\n\n#### Let us consider SARSA algorithm. We have considered 300 episodes.","7ee696ff":"#### Observation: We observed that it took 25 steps to reach GOAL state for SARSA with \u2208=0.1,\u03b1=0.5 option. Discounted factor gamma is set as 1.","9a4a7b6b":"# 3.1 Experiment Observations for SARSA, \u2208=0.1,\u03b1=0.5","34648d78":"# 4.1 Experiment Observations for Q-learning, \u2208=0.1,\u03b1=0.5","7537d2e2":"#### Observation: We observed that it took 19 steps to reach GOAL state for SARSA with \u2208=0.1,\u03b1=0.5 option. Discounted factor gamma is set as 1.","4eb5ed79":"# Reinforcement Learning (RL Solution)\n\n# Windy GridWorld Problem (SARSA and Q-Learning Approaches used)","243cb4b0":"# 4. Q-Learning Algorithm","64010654":"Now we will execute SARSA algorithm with different combinations of \u2208 and \u03b1.\n\nWe will plot the chart for Number of Episodes vs Number of Steps taken for 3 combinations.\n\n* \u2208=0.1,\u03b1=0.5\n* \u2208=0.2,\u03b1=0.1\n* \u2208=0.05,\u03b1=0.2\n\nWe will also see the path taken from steps perspective to reach from START state to GOAL state.","c86b0525":"# 4.3 Experiment Observations for Q-learning, \u2208=0.05,\u03b1=0.2","aedfc48b":"#### Let us consider Q-learning algorithm. We have considered 300 episodes.\n\n#### Here also we will consider 3 different variations.\n\n* Epsilon = 0.1, Alpha = 0.5\n* Epsilon = 0.2, Alpha = 0.1\n* Epsilon = 0.05, Alpha = 0.2"}}