{"cell_type":{"72ea4c82":"code","e0e88da6":"code","cb7d25af":"code","02301b08":"code","985f58c9":"code","7a30edb3":"code","e2c4b263":"code","4b0d1377":"code","cce46715":"code","f325e39b":"code","0445e538":"code","3f243ca4":"code","2cb69177":"code","28daa950":"code","ada75f14":"code","8f25a47a":"code","594bdfc0":"code","7aa1ed32":"code","09624011":"code","57a3738a":"code","64a2569b":"code","59e61bcd":"code","354d7c46":"code","788ca4e9":"code","e8a669f7":"code","02eb6623":"code","57597be8":"code","3a0f5ef5":"code","c7814eb1":"code","357b001a":"code","31490a09":"code","a1e3ac3c":"code","79d06f6c":"code","5f7b497a":"code","559196b1":"code","8cf84368":"code","d5f4dea3":"code","934f0815":"code","f2eaf47e":"code","fdeed90e":"code","cf6bbd76":"code","c318542a":"code","53c4ec11":"markdown","296d1b96":"markdown","4c784234":"markdown","dfd2e51f":"markdown","555ad4fe":"markdown","58084e6c":"markdown","fdcf386e":"markdown","01ab4b76":"markdown","f89e2f53":"markdown","a99e4588":"markdown","d2ed4343":"markdown","4972a9b8":"markdown","39e381a5":"markdown","6b865fa0":"markdown","519d37de":"markdown","a6f9b519":"markdown"},"source":{"72ea4c82":"!pip install transformers\n#!pip install git+git:\/\/github.com\/williamFalcon\/pytorch-lightning.git@master --upgrade\n!pip install pytorch-lightning","e0e88da6":"ls ..\/input\/tweet-sentiment-extraction\/","cb7d25af":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split","02301b08":"# Row 314 of train set is nan\ntrain = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').dropna()#.head(1000)\ntest = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')#.head(1000)\n\n# Set random 13% as the validation set (make validation set similar in size to test set)\ntrain, val = train_test_split(train, test_size=0.13, random_state=42)","985f58c9":"train.shape, test.shape, val.shape","7a30edb3":"set(test.textID.values).intersection(train.textID.values), set(val.textID.values).intersection(train.textID.values)","e2c4b263":"train.head(), test.head(), val.head()","4b0d1377":"train.columns","cce46715":"# Input\nfor a,b,_ in zip(train.sentiment.values[:10], train.text.values[:10], train.selected_text.values[:10]):\n    print(\"sentiment:\", a, \"tweet:\", b)","f325e39b":"# Target (what we're trying to predict)\nfor _,_,c in zip(train.sentiment.values[:10], train.text.values[:10], train.selected_text.values[:10]):\n    print(c)","0445e538":"# Checking out the GPU we have access to\n!nvidia-smi","3f243ca4":"import torch\ntorch.cuda.is_available()","2cb69177":"# Checking for NaNs\ntrain.isna().sum().sum(), test.isna().sum().sum(), val.isna().sum().sum()","28daa950":"# Append EOS token to target text\n# This is the standard format for T5 targets\n# More info in transformers docs: https:\/\/huggingface.co\/transformers\/model_doc\/t5.html\ntrain['selected_text'] = train['selected_text'] + ' <\/s>'\nval['selected_text'] = val['selected_text'] + ' <\/s>'\n\n# Apply Q&A structure\n# From Appendix D in the T5 paper\nprocessed_input_train = (\"question: \" + train.sentiment + \" context: \" + train.text)\nprocessed_input_test = (\"question: \" + test.sentiment + \" context: \" + test.text)\nprocessed_input_val = (\"question: \" + val.sentiment + \" context: \" + val.text)\n\n# Save data as string separated by \\n (new line)\nprocessed_input_str_train = '\\n'.join(processed_input_train.values.tolist())\nprocessed_input_str_test = '\\n'.join(processed_input_test.values.tolist())\nselected_text_str_train = '\\n'.join(train['selected_text'].values.tolist())\nprocessed_input_str_val = '\\n'.join(processed_input_val.values.tolist()[:500])\nselected_text_str_val = '\\n'.join(val['selected_text'].values.tolist()[:500])\n","ada75f14":"processed_input_train[0], train['selected_text'][0]\n","8f25a47a":"processed_input_test[0]","594bdfc0":"# Save source files\n\nwith open('train.source', 'w') as f:\n    f.write(processed_input_str_train)\n    \n# Making dev similar in this case\nwith open('test.source', 'w') as f:\n    f.write(processed_input_str_test)\n    \nwith open('val.source', 'w') as f:\n    f.write(processed_input_str_val)","7aa1ed32":"!head train.source","09624011":"!head test.source","57a3738a":"!head val.source","64a2569b":"with open('train.target', 'w') as f:\n    f.write(selected_text_str_train)\n    \nwith open('val.target', 'w') as f:\n    f.write(selected_text_str_val)","59e61bcd":"!head train.target","354d7c46":"!head val.target","788ca4e9":"ls","e8a669f7":"import os\n\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom transformers.tokenization_utils import trim_batch\n\n\ndef encode_file(tokenizer, data_path, max_length, pad_to_max_length=True, return_tensors=\"pt\"):\n    \"\"\"\n    This function reads the text files that we prepared and returns them in tokenized form.\n\n    Actually tokenizer.batch_encode_plus returns these as a list of dictionaries where \n    each dictionary contains the word piece indices among other relevant inputs for training & inference\n    \"\"\"\n    examples = []\n    with open(data_path, \"r\") as f:\n        for text in f.readlines():\n            tokenized = tokenizer.batch_encode_plus(\n                [text], max_length=max_length, pad_to_max_length=pad_to_max_length, return_tensors=return_tensors,\n            )\n            examples.append(tokenized)\n    return examples\n\n\nclass T5Dataset(Dataset):\n    \"\"\"\n    This is the T5 dataset that can read our train, test, and dev files separately\n\n    This was patterned after the SummarizationDataset from the `transformer` library's summarization example (compatible with T5)\n    \"\"\"\n    def __init__(\n        self,\n        tokenizer,\n        data_dir=\"..\/working\/\",\n        type_path=\"train\",\n        max_source_length=1024,\n        max_target_length=56,\n    ):\n        super().__init__()\n        # Store the tokenizer\n        self.tokenizer = tokenizer\n        self.type_path = type_path\n        # Read the source and target files for the type of file (train, test, or val)\n        self.source = encode_file(tokenizer, os.path.join(data_dir, type_path + \".source\"), max_source_length)\n        self.target = None\n        if self.type_path != \"test\":\n            self.target = encode_file(tokenizer, os.path.join(data_dir, type_path + \".target\"), max_target_length)\n\n    def __len__(self):\n        return len(self.source)\n\n    def __getitem__(self, index):\n        # Return example as a dictionary containing source_ids, src_mask, and target_ids\n        source_ids = self.source[index][\"input_ids\"].squeeze() # (1024,)\n        src_mask = self.source[index][\"attention_mask\"].squeeze()\n\n        if self.type_path == \"test\":\n            return {\"source_ids\": source_ids, \"source_mask\": src_mask}\n\n        target_ids = self.target[index][\"input_ids\"].squeeze() # (56, )\n        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids}\n\n    # Static methods, much like class methods, are methods that are bound to a class rather than its object.\n    # They do not require a class instance creation. So, they are not dependent on the state of the object.\n    # https:\/\/www.programiz.com\/python-programming\/methods\/built-in\/staticmethod\n    @staticmethod\n    def trim_seq2seq_batch(batch, pad_token_id, test=False):\n        # Remove columns that are populated exclusively by pad_token_id\n        # This ensures that each batch is padded only uptil the \"max sequence length\"\n        # https:\/\/github.com\/huggingface\/transformers\/blob\/1e51bb717c04ca4b01a05a7a548e6b550be38628\/src\/transformers\/tokenization_utils.py\n        source_ids, source_mask = trim_batch(batch[\"source_ids\"], pad_token_id, attention_mask=batch[\"source_mask\"])\n        if test:\n            return source_ids, source_mask, None\n        y = trim_batch(batch[\"target_ids\"], pad_token_id)\n        return source_ids, source_mask, y\n\n    def collate_fn(self, batch):\n        \"\"\"\n        The tensors are stacked together as they are yielded.\n\n        Collate function is applied to the output of a DataLoader as it is yielded.\n        \"\"\"\n        input_ids = torch.stack([x[\"source_ids\"] for x in batch]) # BS x SL\n        masks = torch.stack([x[\"source_mask\"] for x in batch]) # BS x SL\n        pad_token_id = self.tokenizer.pad_token_id\n        source_ids, source_mask = trim_batch(input_ids, pad_token_id, attention_mask=masks)\n        if self.type_path == \"test\":\n            return {\"source_ids\": source_ids, \"source_mask\": source_mask}\n\n        target_ids = torch.stack([x[\"target_ids\"] for x in batch]) # BS x SL\n        # Remove columns that are purely padding\n        y = trim_batch(target_ids, pad_token_id)\n        # Return dictionary containing tensors\n        return {\"source_ids\": source_ids, \"source_mask\": source_mask, \"target_ids\": y}","02eb6623":"import argparse\nimport logging\nimport os\nimport random\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom transformers import (\n    AdamW,\n    AutoConfig,\n    AutoModelWithLMHead,\n    AutoTokenizer,\n    get_linear_schedule_with_warmup,\n)\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef set_seed(args: argparse.Namespace):\n    \"\"\"\n    Set all the seeds to make results replicable\n    \"\"\"\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\nclass T5Module(pl.LightningModule):\n    \"\"\"\n    Base Transformer model that uses Pytorch Lightning as a PyTorch wrapper.\n\n    T5 specific methods are implemented in T5Trainer\n    \"\"\"\n    def __init__(self, hparams: argparse.Namespace, **config_kwargs):\n        \"Initialize a model.\"\n\n        super().__init__()\n        self.hparams = hparams\n        cache_dir = self.hparams.cache_dir if self.hparams.cache_dir else None\n        # Read the config file of the T5 model (T5Config)\n        # AutoConfig allows you to read the configuration for a specified model (e.g. in this case, t5-base)\n        # Reference: https:\/\/huggingface.co\/transformers\/model_doc\/auto.html#autoconfig\n        self.config = AutoConfig.from_pretrained(self.hparams.model_name_or_path)\n        # Read the tokenizer of the T5 model (T5Tokenizer)\n        # AutoTokenizer allows you to read the tokenizer for a specified model (e.g. in this case, t5-base)\n        # Reference: https:\/\/huggingface.co\/transformers\/model_doc\/t5.html#t5tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.hparams.model_name_or_path,\n            cache_dir=cache_dir,\n        )\n        # Read the model file for the pre-trained T5 model (T5ForConditionalGeneration)\n        # AutoModelWithLMHead allows you to read any of the language modelling models from the transformers library (e.g. in this case, t5-base)\n        # Automodels reference: https:\/\/huggingface.co\/transformers\/model_doc\/auto.html#automodel\n        self.model = AutoModelWithLMHead.from_pretrained(\n            self.hparams.model_name_or_path,\n            from_tf=bool(\".ckpt\" in self.hparams.model_name_or_path), # Checkpoint is a TF format\n            config=self.config,\n            cache_dir=cache_dir,\n        )\n\n        # Save dataset params\n        self.dataset_kwargs: dict = dict(\n            data_dir=self.hparams.data_dir,\n            max_source_length=self.hparams.max_source_length,\n            max_target_length=self.hparams.max_target_length,\n        )\n\n    # Forward function\n    # Defines the forward pass of the module\n\n    def forward(\n        self,\n        input_ids, # Indices of input sequence tokens in the vocabulary. \n        attention_mask=None, # Mask to avoid performing attention on padding token indices\n        decoder_input_ids=None, # T5 uses the pad_token_id as the starting token for decoder_input_ids generation.\n        lm_labels=None # Labels for computing the sequence classification\/regression loss (see T5Model). Note: loss is returned when lm_label is provided.\n        ):\n        \"\"\"\n         loss (torch.FloatTensor of shape (1,), optional, returned when lm_label is provided\n        \"\"\"\n        # Details on how to use this in the Hugging Face T5 docs: https:\/\/huggingface.co\/transformers\/model_doc\/t5.html\n        return self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            lm_labels=lm_labels,\n        )\n\n    # Data preparation\n\n    def get_dataloader(self, type_path: str, batch_size: int, shuffle: bool = False) -> DataLoader:\n        dataset = T5Dataset(self.tokenizer, type_path=type_path, **self.dataset_kwargs)\n        dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, shuffle=shuffle)\n        return dataloader\n\n    def train_dataloader(self) -> DataLoader:\n        dataloader = self.get_dataloader(\"train\", batch_size=self.hparams.train_batch_size, shuffle=True)\n        t_total = (\n            (len(dataloader.dataset) \/\/ (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n            \/\/ self.hparams.gradient_accumulation_steps\n            * float(self.hparams.num_train_epochs)\n        )\n        scheduler = get_linear_schedule_with_warmup(\n            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n        )\n        self.lr_scheduler = scheduler\n        return dataloader\n\n    def val_dataloader(self) -> DataLoader:\n        return self.get_dataloader(\"val\", batch_size=self.hparams.eval_batch_size)\n\n    def test_dataloader(self) -> DataLoader:\n        return self.get_dataloader(\"test\", batch_size=self.hparams.eval_batch_size)\n\n    # Configure optimizers\n\n    def configure_optimizers(self):\n        \"Prepare optimizer and schedule (linear warmup and decay)\"\n\n        model = self.model\n        # Weight decay will not be applied to \"bias\" and \"LayerNorm.weight\" parameters\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n\n        # Group parameters to those that will and will not have weight decay applied\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": self.hparams.weight_decay,\n            },\n            {\n                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        # Use AdamW as an optimizer\n        # Intro here: https:\/\/www.fast.ai\/2018\/07\/02\/adam-weight-decay\/\n        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n        self.opt = optimizer\n        return [optimizer]\n\n    # Forward pass and calculate loss per batch (step)\n\n    def _step(self, batch, return_text=False):\n        \"\"\"\n        Runs forward pass and calculates loss per batch. Applied for training_step, and validation_step\n        \"\"\"\n        pad_token_id = self.tokenizer.pad_token_id\n        source_ids, source_mask, y = batch[\"source_ids\"], batch[\"source_mask\"], batch[\"target_ids\"]\n        y_ids = y[:, :-1].contiguous()\n        lm_labels = y[:, 1:].clone()\n        # Change pad_token_id to -100\n        lm_labels[y[:, 1:] == pad_token_id] = -100\n        # Run forward pass and calculate loss\n        outputs = self(source_ids, attention_mask=source_mask, decoder_input_ids=y_ids, lm_labels=lm_labels,)\n        # Only get loss from the output since that's all we need to apply our optimizer\n        loss = outputs[0]\n        if return_text:\n            target_text = [self.tokenizer.decode(ids) for ids in y_ids]\n            return loss, target_text\n        else:\n            return loss\n\n    # Step during training\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        Runs forward pass, calculates loss, and returns loss (and logs) in a dict\n        \"\"\"\n        loss = self._step(batch)\n\n        # Notice that each training step loss is recorded on tensorboard, which makes sense since we're tracking loss per batch\n        tensorboard_logs = {\"train_loss\": loss}\n        return {\"loss\": loss, \"log\": tensorboard_logs}\n\n    # Adjust weights based on calculated gradients and learning rate scheduler\n\n    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n        \"\"\"\n        Adjust weights based on calculated gradients + learning rate scheduler, and refresh gradients\n        Reference for optimizer_step: https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/optimizers.html\n        \"\"\"\n        if self.trainer.use_tpu:\n            xm.optimizer_step(optimizer)\n        else:\n            # Adjust weights based on calculated gradients\n            optimizer.step()\n\n        # Refresh gradients (to zero)\n        optimizer.zero_grad()\n        # Update the learning rate scheduler\n        self.lr_scheduler.step()\n\n    # Step during validation\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n        Runs forward pass, calculates loss, and returns loss in a dict\n        \"\"\"\n\n        # Return source and target text to calculate jaccard score only for validation\n        loss, target_text = self._step(batch, return_text=True)\n\n        preds = self.test_step(batch, batch_idx)\n        preds_text = preds[\"preds\"]\n        # Track jaccard score to get validation accuracy\n        jaccard_score = [jaccard(p, t) for p, t in zip(preds_text, target_text)]\n\n        return {\"val_loss\": loss, \"jaccard_score\": jaccard_score}\n\n    # Show loss after validation\n\n    def validation_end(self, outputs):\n        \"\"\"\n        Calculate average loss for all the validation batches\n        \"\"\"\n        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n        jaccard_scores = sum([x[\"jaccard_score\"] for x in outputs], [])\n        avg_jaccard_score = np.mean(jaccard_scores)\n        tensorboard_logs = {\"val_loss\": avg_loss, \"jaccard_score\": avg_jaccard_score}\n        return {\"avg_val_loss\": avg_loss, \"avg_jaccard_score\": avg_jaccard_score, \"log\": tensorboard_logs}\n\n    # Step during testing\n\n    def test_step(self, batch, batch_idx):\n        \"\"\"\n        Runs forward pass on test set and returns calculated loss, predictions, and targets\n        Note: this assumes that your test set has targets (doesn't have for kaggle).\n        \"\"\"\n        pad_token_id = self.tokenizer.pad_token_id\n        source_ids, source_mask, _ = T5Dataset.trim_seq2seq_batch(batch, pad_token_id, test=True)\n        # NOTE: the following kwargs get more speed and lower quality summaries than those in evaluate_cnn.py\n        # Generate reference: https:\/\/github.com\/huggingface\/transformers\/blob\/3e0f06210646a440509efa718b30d18322d6a830\/src\/transformers\/modeling_utils.py#L769\n        # For the sentiment span extraction task, turning off early stopping proved superior\n        generated_ids = self.model.generate(\n            input_ids=source_ids,\n            attention_mask=source_mask,\n            num_beams=1,\n            max_length=80,\n            repetition_penalty=2.5,\n            length_penalty=1.0,\n            early_stopping=True,\n            use_cache=True,\n        )\n        preds = [\n            self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n            for g in generated_ids\n        ]\n\n        return {\"preds\": preds}\n\n    # Note: we don't attempt to print the loss from the test set, because it's assumed that we don't have the test targets\n    def test_end(self, outputs):\n        \"\"\"\n        \"\"\"\n        preds = []\n        for pred in outputs:\n            preds += pred[\"preds\"]\n        return {\"preds\": preds}\n\n    def test_epoch_end(self, outputs):\n        \"\"\"\n        Save test predictions and targets as text files and return the calculated loss for the test set\n        \"\"\"\n        output_test_predictions_file = os.path.join(self.hparams.output_dir, \"test_predictions.txt\")\n        # write predictions and targets for later rouge evaluation.\n        with open(output_test_predictions_file, \"w+\") as p_writer:\n            for output_batch in outputs:\n                p_writer.writelines(s + \"\\n\" for s in output_batch[\"preds\"])\n            p_writer.close()\n\n        return self.test_end(outputs)\n\n    def get_tqdm_dict(self):\n        \"\"\"\n        Print average loss and learning rate at each step\n        \"\"\"\n        avg_loss = getattr(self.trainer, \"avg_loss\", 0.0)\n        tqdm_dict = {\"loss\": \"{:.3f}\".format(avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n        return tqdm_dict\n\n    def _feature_file(self, mode):\n        return os.path.join(\n            self.hparams.data_dir,\n            \"cached_{}_{}_{}\".format(\n                mode,\n                list(filter(None, self.hparams.model_name_or_path.split(\"\/\"))).pop(),\n                str(self.hparams.max_seq_length),\n            ),\n        )\n\n    def is_logger(self):\n        return self.trainer.proc_rank <= 0\n\n    @staticmethod\n    def add_model_specific_args(parser, root_dir):\n        parser.add_argument(\n            \"--model_name_or_path\",\n            default=None,\n            type=str,\n            required=True,\n            help=\"Path to pretrained model or model identifier from huggingface.co\/models\",\n        )\n        parser.add_argument(\n            \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n        )\n        parser.add_argument(\n            \"--tokenizer_name\",\n            default=\"\",\n            type=str,\n            help=\"Pretrained tokenizer name or path if not the same as model_name\",\n        )\n        parser.add_argument(\n            \"--cache_dir\",\n            default=\"\",\n            type=str,\n            help=\"Where do you want to store the pre-trained models downloaded from s3\",\n        )\n        parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n        parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n        parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n        parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n        parser.add_argument(\n            \"--num_train_epochs\", default=3, type=int, help=\"Total number of training epochs to perform.\"\n        )\n\n        parser.add_argument(\"--train_batch_size\", default=32, type=int)\n        parser.add_argument(\"--eval_batch_size\", default=32, type=int)\n\n        parser.add_argument(\n            \"--max_source_length\",\n            default=1024,\n            type=int,\n            help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n            \"than this will be truncated, sequences shorter will be padded.\",\n        )\n        parser.add_argument(\n            \"--max_target_length\",\n            default=56,\n            type=int,\n            help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n            \"than this will be truncated, sequences shorter will be padded.\",\n        )\n\n        parser.add_argument(\n            \"--data_dir\",\n            default=None,\n            type=str,\n            required=True,\n            help=\"The input data dir. Should contain the dataset files for the text generation task.\",\n        )\n        return parser\n\n\nclass LoggingCallback(pl.Callback):\n    def on_validation_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n        logger.info(\"***** Validation results *****\")\n        if pl_module.is_logger():\n            metrics = trainer.callback_metrics\n            # Log results\n            for key in sorted(metrics):\n                if key not in [\"log\", \"progress_bar\"]:\n                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n\n\ndef add_generic_args(parser, root_dir):\n    parser.add_argument(\n        \"--output_dir\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n\n    parser.add_argument(\n        \"--fp16\",\n        action=\"store_true\",\n        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n    )\n\n    parser.add_argument(\n        \"--fp16_opt_level\",\n        type=str,\n        default=\"O1\",\n        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n        \"See details at https:\/\/nvidia.github.io\/apex\/amp.html\",\n    )\n\n    parser.add_argument(\"--n_gpu\", type=int, default=1)\n    parser.add_argument(\"--n_tpu_cores\", type=int, default=0)\n    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n    parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Whether to run predictions on the test set.\")\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"Number of updates steps to accumulate before performing a backward\/update pass.\",\n    )\n\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n\n\ndef generic_train(model: T5Module, args: argparse.Namespace):\n    # init model\n    set_seed(args)\n\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n\n    # Can take out checkpoint saving after each epoch to save memory\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n        filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n    )\n\n    train_params = dict(\n        accumulate_grad_batches=args.gradient_accumulation_steps,\n        gpus=args.n_gpu,\n        max_epochs=args.num_train_epochs,\n        early_stop_callback=False,\n        gradient_clip_val=args.max_grad_norm,\n        checkpoint_callback=checkpoint_callback,\n        callbacks=[LoggingCallback()],\n    )\n\n    if args.fp16:\n        train_params[\"use_amp\"] = args.fp16\n        train_params[\"amp_level\"] = args.fp16_opt_level\n\n    if args.n_tpu_cores > 0:\n        global xm\n        import torch_xla.core.xla_model as xm\n\n        train_params[\"num_tpu_cores\"] = args.n_tpu_cores\n        train_params[\"gpus\"] = 0\n\n    if args.n_gpu > 1:\n        train_params[\"distributed_backend\"] = \"ddp\"\n\n    trainer = pl.Trainer(**train_params)\n\n    if args.do_train:\n        trainer.fit(model)\n\n    return trainer","57597be8":"import argparse\nimport glob\nimport logging\nimport os\nimport time\n\nlogging.basicConfig(level = logging.INFO)\n\nlogger = logging.getLogger(__name__)\n\ndef main(args):\n\n    # If output_dir not provided, a folder will be generated in pwd\n    if not args.output_dir:\n        args.output_dir = os.path.join(\".\/results\", f\"{args.task}_{time.strftime('%Y%m%d_%H%M%S')}\",)\n        os.makedirs(args.output_dir)\n    model = T5Module(args)\n    trainer = generic_train(model, args)\n\n    # Save the last model as model.bin\n    #checkpoints = list(sorted(glob.glob(os.path.join(args.output_dir, \"checkpointepoch=*.ckpt\"), recursive=True)))\n    #model = model.load_from_checkpoint(checkpoints[-1])\n    model.model.save_pretrained(args.output_dir)\n    # Save tokenizer files\n    model.tokenizer.save_pretrained('.\/')\n    \n    # Optionally, predict on dev set and write to output_dir\n    if args.do_predict:\n        # See https:\/\/github.com\/huggingface\/transformers\/issues\/3159\n        # pl use this format to create a checkpoint:\n        # https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\\\n        # \/pytorch_lightning\/callbacks\/model_checkpoint.py#L169\n        trainer.test(model)\n    return trainer\n","3a0f5ef5":"#!rm -r \/content\/output","c7814eb1":"mkdir output","357b001a":"# Will set gpu on as soon as at least 1 batch works on cpu\n# TODO: Consider factors here: https:\/\/github.com\/huggingface\/transformers\/issues\/3387\n# Change LR to 1e-3 to 1e-4\n# \nARGS_STR = \"\"\"\n--data_dir=.\/ \\\n--model_name_or_path=t5-base \\\n--learning_rate=3e-5 \\\n--train_batch_size=32 \\\n--output_dir=output\/ \\\n--do_train \\\n--n_gpu=1 \\\n--num_train_epochs 1 \\\n--max_source_length 80 \\\n\"\"\"\n#\n#--eval_batch_size=3 \\\n#--do_predict \\\n\nparser = argparse.ArgumentParser()\nadd_generic_args(parser, os.getcwd())\nparser = T5Module.add_model_specific_args(parser, os.getcwd())\nargs = parser.parse_args(ARGS_STR.split())\ntrainer = main(args)","31490a09":"ls -l output\/","a1e3ac3c":"ls","79d06f6c":"ls lightning_logs\/","5f7b497a":"#cat lightning_logs\/version_0\/hparams.yaml","559196b1":"from transformers import T5Tokenizer, T5ForConditionalGeneration","8cf84368":"tokenizer = T5Tokenizer.from_pretrained('t5-base')\nt5 = T5ForConditionalGeneration.from_pretrained('output\/')","d5f4dea3":"def get_span(text):\n    input_ids = tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)  # Batch size 1\n    t5.eval()\n    generated_ids = t5.generate(\n        input_ids=input_ids,\n        num_beams=1,\n        max_length=80,\n        #repetition_penalty=2.5\n    ).squeeze()\n    predicted_span = tokenizer.decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    return predicted_span","934f0815":"get_span(\"question: negative context: I`m in VA for the weekend, my youngest son turns 2 tomorrow......it makes me kinda sad, he is getting so big, check out my twipics\")","f2eaf47e":"get_span(\"question: negative context: Recession hit Veronique Branquinho, she has to quit her company, such a shame!\")","fdeed90e":"get_span(\"question: negative context: My bike was put on hold...should have known that.... argh total bummer\")","cf6bbd76":"get_span(\"question: positive context: On the monday, so i wont be able to be with you! i love you\")","c318542a":"get_span(\"question: positive I liked it. Did you record it yourself? If so you have a very soothing voice.\")","53c4ec11":"Reading the data","296d1b96":"## Model\n\nBased on this summarization (encoder-decoder) example from [transformers](https:\/\/github.com\/huggingface\/transformers\/blob\/master\/examples\/summarization\/bart\/finetune.py), which is compatible for both BART and T5","4c784234":"## Key results\n\n1. Training T5 with 5 epochs gives an accuracy (Jaccard score) of 0.665\n  - This is still much lower than the 0.714 at the top 10%\n  - But, much is yet to be done in terms of post training optimization\n    - Ensembling, stacking, etc\n2. The amazing thing for me is the confirmation that a generative model like T5 can perform extractive tasks with an accuracy comparable to a token classification version of BERT\n\nI'm confident that T5 can reach leaderboard level results with more experiments!","dfd2e51f":"## Preparing the T5 Dataset\n\nBased on this summarization example from [transformers](https:\/\/github.com\/huggingface\/transformers\/blob\/master\/examples\/summarization\/bart\/finetune.py), which is compatible for both BART and T5","555ad4fe":"## Installation\n\nInstalling the [transformers](https:\/\/github.com\/huggingface\/transformers) + [Pytorch Lightning](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning) libraries. The rest of the required packages are already installed in Google Colab by default.\n\nI'd describe PyTorch Lightning as a PyTorch wrapper that simplifies the process of writing PyTorch code.","58084e6c":"Related links\n1. [T5 Paper](https:\/\/arxiv.org\/pdf\/1910.10683.pdf) - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n2. [T5 Implementation on PyTorch](https:\/\/github.com\/huggingface\/transformers\/blob\/455c6390938a5c737fa63e78396cedae41e4e87e\/src\/transformers\/modeling_t5.py) by HuggingFace\n3. [T5 Exploration Notebook](https:\/\/colab.research.google.com\/drive\/1dYqSqq4OCDV0nN3gkagjqXoWX8ZqrE-7?usp=sharing) demonstrating how to use t5-base for summarization and Q&A\n4. [SQuAD Dataset](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/) for extractive Q&A\n5. [Tweet Sentiment Extraction](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction) (Kaggle Competition)\n6. [Bert Base Uncased for Sentiment Span Extraction (Token Classification)](https:\/\/www.kaggle.com\/enzoamp\/commented-bert-base-uncased-using-pytorch\/comments) (Commented version of Abhishek Thakur's solution)","fdcf386e":"Confirm that the train set doesn't have any overlaps with the test and validation sets","01ab4b76":"# Save target file","f89e2f53":"# Setup\n\n","a99e4588":"## Save input files\n\nWe save the inputs as text files since our DataLoader class takes in data in this format.","d2ed4343":"Let's see the GPU we get from Colab.\n\nIf you're lucky, you get a P100, but you will typically get a K80.","4972a9b8":"Checking out how the data looks\n\nMy biggest concern is that sometimes the span is a single word like \"afraid\", but then sometimes it's a whole phrase, like \"jip i have a good one\". I suspect that this variation may be harder to model with deep learning.","39e381a5":"Test if saved model works w sample inference","6b865fa0":"# T5 for Sentiment Span Extraction\n\nThis introduction [notebook](https:\/\/github.com\/enzoampil\/t5-intro\/blob\/master\/t5_qa_training_pytorch_span_extraction.ipynb) is featured in Abhishek Thakur's Talks #3 webinar on [youtube](https:\/\/www.youtube.com\/watch?v=4LYw_UIdd4A).\n\nThis notebook is purely for training, to do inference with the trained model, you can use this [notebook](https:\/\/www.kaggle.com\/enzoamp\/t5-q-a-inference-5-epochs-pytorch) as a reference.\n\n## Goals for this tutorial\n\n1. Introduce T5 and how it works\n2. Explain T5's significance for the future of NLP\n3. Illustrate how to use T5 for Sentiment Span Extraction\n\n\n## T5 Overview\n\nT5 is a recently released encoder-decoder model that reaches SOTA results by solving NLP problems with a text-to-text approach. This is where text is used as both an input and an output for solving all types of tasks. This was introduced in the recent paper, *Exploring the Limits of Transfer Learning with a\nUnified Text-to-Text Transformer* ([paper](https:\/\/arxiv.org\/pdf\/1910.10683.pdf)). I've been deeply interested in this model the moment I read about it.\n\nI believe that the combination of text-to-text as a universal interface for NLP tasks paired with multi-task learning (single model learning multiple tasks) will have a huge impact on how NLP deep learning is applied in practice.\n\nIn this presentation I aim to give a brief overview of T5, explain some of its implications for NLP in industry, and demonstrate how it can be used for sentiment span extraction on tweets. I hope this material helps you guys use T5 for your own purposes!\n\n\n## Key points from T5 paper\n\n<img src=\"https:\/\/drive.google.com\/uc?id=15XQ-H7IdT3DVtbL7fgwYIm08OYWbB8VZ\" width=\"700\" height=\"300\" \/>\n\n1. **Treats each NLP problem as a \u201ctext-to-text\u201d problem** - input: text, output: text\n\n2. **Unified approach for NLP Deep Learning** - Since the task is reflected purely in the text input and output, you can use the same model, objective, training procedure, and decoding process to ANY task. Above framework can be used for any task - show Q&A, summarization, etc.\n\n3. **Multiple NLP tasks can live in the same model** - E.g. Q&A, semantic similarity, etc. However, there is a problem called *task interference* where good results on one task can also mean worse results on another task. E.g., a good summarizer may be bad at Q&A and vice versa. All the tasks above can live in the same model, which is how it works with the released T5 models (t5-small, t5-base, etc.)\n\n<img src=\"https:\/\/drive.google.com\/uc?id=1-1SXVF78l3EbsuTeDUEv0hUvgfMIch3f\" width=\"700\" height=\"165\" \/>\n\n4. **New dataset: \u201cColossal Clean Crawled Corpus\u201d (C4)** - a dataset consisting of ~750GB of clean English text scraped from the web. Created with a month of data from the Common Crawl corpus cleaned with a set of heuristics to filter out \"unhelpful\" text (e.g. offensive language, placeholder text, source code). This is a lot larger than the 13GB of data used for BERT, and 126GB of data used for XLNet.\n\n\n5. **A simple denoising training objective was used for pretraining** Basically, masked language modelling but while considering contiguous masks as a single \u201cspan\u201d to predict, and where the final prediction is an actual text sequence containing the answers (represented by \u201csentinel tokens\u201d). This was compared to a language modeling pre-training objective and results consistently improved.\n\n<img src=\"https:\/\/drive.google.com\/uc?id=1iPG7UxZPvy6c2iwixQXYetpTzcHiKDuW\" width=\"500\" height=\"200\" \/>\n\n6. **Full encoder-decoder transformer architecture is used** - this is in contrast to previous architectures that were either encoder based (e.g. BERT), or decoder based (e.g. GPT-2). This was found effective for both generation & classification tasks.\n\n<img src=\"https:\/\/drive.google.com\/uc?id=1tUxL_os-pn8JTBSCY6l-9rQI0EMPC5Zz\" width=\"400\" height=\"500\" \/>\n\n\n## Key insight\n\nMultiple NLP tasks can be learned by a single model since every NLP problem can be represented in a unified way - as a controllable text generation problem.\n\n## Expected impact\n\nIncreased adoption of multi-task models like T5 due to SOTA accuracy paired with lower time, compute, & storage costs for both deployments and experiments in NLP.\n\n## T5 for Sentiment Span Extraction (PyTorch)\n\n1. This is a dataset from an existing Kaggle [competition](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/data) - Tweet Sentiment Extraction\n2. Most of the existing model implementations use some sort of token classification task\n  - The index of the beginning and ending tokens are predicted and use to *extract* the span\n\n3. T5 is an approach that is purely *generative*, like a classic language modelling task\n  - This is similar to abstractize summarization, translation, and overall text generation\n  - For our data, the span is not *extracted* by predicting indices, but by generating the span from scratch\n\n## Let's get started!\n\n-----------------------------------------------------------------------------\n","519d37de":"## Finetuning\n\nBased on this summarization example from [transformers](https:\/\/github.com\/huggingface\/transformers\/blob\/master\/examples\/summarization\/bart\/finetune.py), which is compatible for both BART and T5","a6f9b519":"## Format data to input text Q&A format\n\nFor the T5 model, I decided to build on top of the existing Q&A task already learned by T5 since the nature of Q&A based on the SQuAD dataset is **extractive** in nature.\n\nIn other words, utilizing the Q&A task's text formatting should theoretically allow us to utilize T5's existing knowledge of extracting spans from an input text, which is exactly what we want to do for this sentiment span extraction task."}}