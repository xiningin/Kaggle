{"cell_type":{"6b025119":"code","dbe7d476":"code","69bd4cf9":"code","705351d3":"code","fe093d77":"code","8a68f2db":"code","dd97952f":"code","5f8c263d":"code","857b5175":"code","30ab8c45":"code","b8f2405b":"code","ea9bf1e4":"code","10de2120":"code","d1e93251":"code","7018893b":"markdown","b35000aa":"markdown","1aad04a8":"markdown","55caa69b":"markdown","69dd0ed1":"markdown","798d35f3":"markdown","6d3e5759":"markdown","d3576bdf":"markdown","ee6f1e4a":"markdown","b2eac9e4":"markdown","31d4c2c9":"markdown","8d1aade6":"markdown"},"source":{"6b025119":"#Importing libraries\n\nimport numpy as np\nimport keras\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense,Flatten\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import *\nfrom sklearn.metrics import confusion_matrix\nimport os\n%matplotlib inline","dbe7d476":"from keras import applications\n\n# build the VGG16 network\nmodel_vgg16 = applications.VGG16(include_top=True, weights='..\/input\/vgg16\/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\nmodel_vgg16.summary() \ntype(model_vgg16)\n","69bd4cf9":"model1 = Sequential()\nfor layer in model_vgg16.layers:\n    model1.add(layer)","705351d3":"type(model1)","fe093d77":"for layer in model1.layers:\n    layer.trainable = False","8a68f2db":"model1.add(Dense(2,activation = 'softmax', name='output'))","dd97952f":"model1.summary()","5f8c263d":"train_path = '..\/input\/traindata\/train'\ntest_path = '..\/input\/testset\/test'\nvalid_path = '..\/input\/validset\/valid'","857b5175":"train_batches = ImageDataGenerator().flow_from_directory(train_path,target_size=(224,224),classes=['cats','dogs'],batch_size=4)\ntest_batches = ImageDataGenerator().flow_from_directory(test_path,target_size=(224,224),classes=['cats','dogs'],batch_size=12)\nvalid_batches = ImageDataGenerator().flow_from_directory(valid_path,target_size=(224,224),classes=['cats','dogs'],batch_size=4)\n","30ab8c45":"model1.compile(Adam(lr=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])\nmodel1.fit_generator(train_batches,steps_per_epoch=10,validation_data=valid_batches,validation_steps=4,epochs=10,verbose=2)","b8f2405b":"\nprediction = model1.predict_generator(test_batches,steps=1,verbose=1)\n","ea9bf1e4":"test_imgs,test_labels = next(test_batches)\ntest_labels = test_labels[:,0]\ntest_labels\nprediction = np.round(prediction[:,0])\nprediction\n","10de2120":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(test_labels,prediction)\ncm","d1e93251":"import seaborn as sns\nsns.heatmap(cm,xticklabels=['cats-actual','dogs-actual'], yticklabels=['dogs-predicted','cats-predicted'], annot=True,cbar=False,cmap=\"YlGnBu\")","7018893b":"In this notebook we are going to see how we can use a pre-trained models like VGG-16 to get high accuracy.\n\n**Introduction**\n\nPretrained models are a wonderful source of help for people looking to learn an algorithm or try out an existing framework. Due to time restrictions or computational restraints, it\u2019s not always possible to build a model from scratch which is why pretrained models exist! You can use a pretrained model as a benchmark to either improve the existing model, or test your own model against it. The potential and possibilities are vast.\n\nVGG-16 is a great starting point for learning how pre-trained models work.\n\nIn this we are going to use keras framework for building our model.","b35000aa":"First we will import VGG16 model and check it's architecture.","1aad04a8":"Now lets see what our confusion matrix tell us:\n\nthe next() returns a tuple of (images, labels). We can use the images to plot imgs and labels to compare it with the pridictions. And thats what we are gonna do.","55caa69b":"Now we are gonna add our **last output layer**","69dd0ed1":"Lets now define our paths to the training, test, validation sets.\nIt's important to note that all images should be seperately moved to corresponding zip folder.","798d35f3":"Now, we are gonna **freeze the parameters** of the pre-trained model, so that they doesn't get affected on training","6d3e5759":"Amazing!!\n\nWe can see with just 10 epochs the model can get an accuracy of 81% and that's pretty good actually as we have not played with lots of parameters.\nThats what can be done by Fine-Tuning a pre-trained model.\n\nThere are several other pre-trained models you can practice on:\n\n* Mask R-CNN\n* YOLOv2\n* MobileNet\n* VGG-Face Model\n* 3D Face Reconstruction from a Single Image\n* Semantic Image Segmentation \u2013 Deeplabv3\n* Robot Surgery Segmentation","d3576bdf":"**Predictions**","ee6f1e4a":"The last step is to compile the model\n\nHere we will be using **Adam optimizer**, Why? Bcoz it's a tested optimizer for these kinds of classifiaction problems.\nWe will choose our loss-function as **categorical crossentropy**.\n\n**Steps per epoch = no. of images (data files) in the set \/ batch size**\n","b2eac9e4":"Here we can see that the model is not of type **Sequentia**l.\nThus, we will first convert it to a sequential model.","31d4c2c9":"**Lets style it a little bit**","8d1aade6":"Now, we are gonna input images to different batches,\n\n* target_size = desired size of the images\n* batch_size = no. of images in one batch"}}