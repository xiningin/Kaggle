{"cell_type":{"46f3551c":"code","a73a0231":"code","60232a5e":"code","3591eeb0":"code","b3e1cdfc":"code","412a63ff":"code","1c950ed3":"code","11133e6d":"code","bb23912a":"code","88884acc":"code","dbc5088e":"code","bcf399f4":"code","0f1776a6":"code","b39335f2":"code","c206958f":"code","380f94f8":"code","a14885dd":"code","c94e5a47":"code","8b11cc75":"code","353d04ea":"code","a8e0b771":"code","450ef8bb":"code","90e43c54":"code","3fccff2b":"code","b3168966":"code","80ac3aae":"code","3c618d0b":"code","1cd033dc":"code","ee539e82":"code","0226c391":"code","4b1b7fa0":"code","da8edd1a":"code","4ad0193a":"code","b00dc305":"code","ea38e471":"code","4304a177":"code","b9c7b0a1":"code","ed947884":"code","148ba4ad":"code","5d1860af":"code","fbdfb828":"code","53861c95":"code","630530b0":"code","55c59dcf":"code","d294221e":"code","12ae6ead":"code","dc9a5844":"code","4fd5f18a":"code","4dd2eba0":"code","ac5ac8e3":"markdown","3d3c63aa":"markdown","ecb6fe4b":"markdown","cc9c3c10":"markdown","3683b781":"markdown","6edf0114":"markdown","4c8a89f7":"markdown","24a14167":"markdown","ade4c78a":"markdown","d6019d0c":"markdown","106bc63c":"markdown","53cace84":"markdown","73e04f79":"markdown","099c4feb":"markdown","3a41c845":"markdown","7441f34a":"markdown","c4384743":"markdown","5e53e669":"markdown","dac49cc0":"markdown","55c7a7d0":"markdown"},"source":{"46f3551c":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier\nfrom sklearn.model_selection import cross_validate, GridSearchCV, cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=Warning)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)","a73a0231":"data = pd.read_csv('..\/input\/zomato-bangalore-restaurants\/zomato.csv')","60232a5e":"df = data.copy()","3591eeb0":"df.info()","b3e1cdfc":"df = df.drop_duplicates(subset=['address', 'name']).reset_index().drop('index', axis=1)\n# dropping duplicated restaurants","412a63ff":"df['approx_cost(for two people)'] = (df['approx_cost(for two people)'].str.replace(',', '')).astype('float64')\n\ndf['rate'] = df['rate'].replace('NEW', np.nan)\ndf['rate'] = df['rate'].replace('-', np.nan)\ndf['rate'] = (df['rate'].str.rstrip('\/5')).astype('float64')\n\n# these are actually numerical values seen as strings, we made some string operations to put them in the right format","1c950ed3":"# location vs rate barplot\nloc_rate = pd.DataFrame(df.groupby(['location'])['rate'].mean().sort_values(ascending=False).head(10))\nloc_rate.reset_index(inplace=True)\nax = sns.barplot(x='rate', y='location', data=loc_rate, palette=\"bright\")","11133e6d":"# rest_type vs rate barplot\nrestype_rate = pd.DataFrame(df.groupby(['rest_type'])['rate'].mean().sort_values(ascending=False).head(10))\nrestype_rate.reset_index(inplace=True)\nax = sns.barplot(x='rate', y='rest_type', data=restype_rate, palette=\"viridis\")","bb23912a":"# cuisines barplot\ncuisine_count = pd.DataFrame(df['cuisines'].value_counts().sort_values(ascending=False)[:10])\ncuisine_count.reset_index(inplace=True)\nax = sns.barplot(y='index', x='cuisines', data=cuisine_count, palette=\"Set1\")","88884acc":"# online_order countplot\nax = sns.countplot(y='online_order', data=df, palette=\"Set1\")","dbc5088e":"# online_order vs rate\nonline_rate = pd.DataFrame(df.groupby(['online_order'])['rate'].mean().sort_values(ascending=False).head(10))\nonline_rate.reset_index(inplace=True)\nax = sns.barplot(x='rate', y='online_order', data=online_rate, palette=\"Set1\")","bcf399f4":"# book_table countplot\nax = sns.countplot(y='book_table', data=df, palette=\"Set1\")","0f1776a6":"# book_table vs rate\nbook_rate = pd.DataFrame(df.groupby(['book_table'])['rate'].mean().sort_values(ascending=False).head(10))\nbook_rate.reset_index(inplace=True)\nax = sns.barplot(x='rate', y='book_table', data=book_rate, palette=\"Set1\")","b39335f2":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","c206958f":"drop_list = ['url', 'address', 'phone', 'reviews_list', 'menu_item', 'name', 'listed_in(city)', 'listed_in(type)']\ndf.drop(drop_list, axis=1, inplace=True)\n# We can drop some columns those have no useful data & low feature importance according to my previous trials","380f94f8":"def missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns\n    \n    \ndf.isnull().sum().sort_values(ascending=False)","a14885dd":"# dropping missing rates, we have enough data and we'll be using tree methods\ndf.dropna(subset=['rate'], inplace=True)","c94e5a47":"# Percentages of missing values\n(df.isnull().sum() \/ df.shape[0] * 100).sort_values(ascending=False)\nmissing_values_table(df)","8b11cc75":"# we will drop the missing values for some features those have % 0.01 missing values.\ndf.dropna(subset=['rest_type', 'approx_cost(for two people)', 'cuisines'], inplace=True)","353d04ea":"# new feature: number of the cuisines in restaurants\ndf['num_of_cuisines'] = [len(cui) for cui in df['cuisines'].str.split(',')]\n# new feature: number of the dishes liked in restaurants\ndf['num_of_dish_liked'] = [len(dish) if dish is not np.NAN else 0 for dish in df['dish_liked'].str.split(',')]","a8e0b771":"# No measurement point, high cardinality.\ndf.drop('dish_liked', axis=1, inplace=True)\ndf.drop('cuisines', axis=1, inplace=True)","450ef8bb":"# Rare Encoding\ndef rare_encoder(dataframe, cat_cols, rare_perc=0.01):\n    rare_columns = [col for col in cat_cols if (dataframe[col].value_counts() \/ len(dataframe) < rare_perc).sum() > 1]\n\n    for col in rare_columns:\n        tmp = dataframe[col].value_counts() \/ len(dataframe)\n        rare_labels = tmp[tmp < rare_perc].index\n        dataframe[col] = np.where(dataframe[col].isin(rare_labels), 'Rare', dataframe[col])\n\n    return dataframe\n# the percentages that i decide for thresholds: location 0.03, rest_type 0.02, cuisines 0.01\nrare_encoder(df, ['location'], rare_perc=0.03)\nrare_encoder(df, ['rest_type'], rare_perc=0.02)","90e43c54":"df.describe().T","3fccff2b":"# The mean and the median of rate is nearly the same. We're gonna use 3.7 as threshold for labelling \n# the restaurants as successful or not.\ndf['label'] = df['rate'].apply(lambda x: 1 if x >= 3.7 else 0)\ndf.drop('rate', axis=1, inplace=True)\ndf['label'].value_counts()\/len(df)\n# the proportions of labels are 0.51 and 0.49","b3168966":"cat_cols, num_cols, cat_but_car = grab_col_names(df)\nfor col in ['num_of_cuisines', 'num_of_dish_liked']:\n    num_cols.append(col)","80ac3aae":"def correlation_matrix(df, cols):\n    fig = plt.gcf()\n    fig.set_size_inches(10, 8)\n    plt.xticks(fontsize=10)\n    plt.yticks(fontsize=10)\n    fig = sns.heatmap(df[cols].corr(), annot=True, linewidths=0.5, annot_kws={'size': 12}, linecolor='w', cmap='RdBu')\n    plt.show(block=True)\n    \ncorrelation_matrix(df, num_cols)\n# We don't have any high correlations in our numerical variables","3c618d0b":"# Outliers\n# We have outlier values in some features but will not touch the outliers, cause we'll use decision tree methods those are not affected by them.\n\n# Binary Encoding\nbinary_cols = [col for col in df.columns if df[col].dtype not in [int, float]\n                   and df[col].nunique() == 2 and col != 'label']\ndef label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\nfor col in binary_cols:\n    df = label_encoder(df, col)","1cd033dc":"# One-Hot Encoding\nohe_cols = [col for col in df.columns if 10 >= df[col].nunique() > 2 and col != 'num_of_cuisines'\n            and col != 'num_of_dish_liked']\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\ndf = one_hot_encoder(df, ohe_cols)","ee539e82":"# Robust Scaler\nscaler = RobustScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])\n\n# The regex code for editing the column names to avoid LightGBM warnings\ndf = df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))\n\ndf.columns = [col.upper() for col in df.columns]","0226c391":"y = df[\"LABEL\"]\nX = df.drop([\"LABEL\"], axis=1)\nmeth = []\nmetric = []\n# Base Models\nclassifiers = [(\"CART\", DecisionTreeClassifier()),\n               (\"RF\", RandomForestClassifier()),\n               ('Adaboost', AdaBoostClassifier()),\n               ('GBM', GradientBoostingClassifier()),\n               ('XGBoost', XGBClassifier(eval_metric='logloss', use_label_encoder=False)),\n               ('LightGBM', LGBMClassifier(force_col_wise=True)),\n               ('CatBoost', CatBoostClassifier(verbose=False))\n               ]\n\nfor name, classifier in classifiers:\n    cv_results = cross_validate(classifier, X, y, cv=3, scoring=[\"roc_auc\"])\n    print(f\"AUC: {round(cv_results['test_roc_auc'].mean(),4)} ({name}) \")\n    metric.append(name)\n    meth.append(round(cv_results['test_roc_auc'].mean(), 4))\nbase_df = pd.DataFrame(meth, metric).reset_index()\nbase_df.columns = ['Model', 'AUC']\nbase_df.sort_values(\"AUC\", ascending=False, inplace=True)\nsns.barplot(y='Model', x='AUC', data=base_df)","4b1b7fa0":"cart_params = {'max_depth': range(8, 12),\n               \"min_samples_split\": range(2, 8)}\n\nrf_params = {\"max_depth\": [7, None],\n             \"min_samples_split\": [2, 6, 10],\n             \"n_estimators\": [500, 1000]}\n\n# xgboost_params = {\"max_depth\": [7, 10],\n#                   \"n_estimators\": [500, 1000],\n#                   \"colsample_bytree\": [0.6, 0.8]}\n\nlightgbm_params = {\"learning_rate\": [0.01, 0.1, 0.2],\n                   \"n_estimators\": [300, 500, 1500],\n                   \"colsample_bytree\": [0.5, 0.7, 1]}\n\ncatboost_params = {}\n\nclassifiers = [(\"CART\", DecisionTreeClassifier(), cart_params),\n               (\"RF\", RandomForestClassifier(), rf_params),\n#                ('XGBoost', XGBClassifier(eval_metric='logloss', use_label_encoder=False), xgboost_params),\n               ('LightGBM', LGBMClassifier(force_col_wise=True), lightgbm_params),\n               ('CatBoost', CatBoostClassifier(verbose=False), catboost_params)\n               ]\n\nbest_models = {}","da8edd1a":"met_name, met_acc_bef, met_f1_bef, met_auc_bef, met_acc_aft, met_f1_aft, met_auc_aft = [], [], [], [], [], [], []\nfor name, classifier, params in classifiers:\n    print(f\"########## {name} ##########\")\n    cv_results = cross_validate(classifier, X, y, cv=3, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\n    print(f\"ACCURACY (Before): {round(cv_results['test_accuracy'].mean(),4)}\")\n    print(f\"F1 (Before): {round(cv_results['test_f1'].mean(), 4)}\")\n    print(f\"AUC (Before): {round(cv_results['test_roc_auc'].mean(), 4)}\")\n    met_name.append(name)\n    met_acc_bef.append(round(cv_results['test_accuracy'].mean(), 4))\n    met_f1_bef.append(round(cv_results['test_f1'].mean(), 4))\n    met_auc_bef.append(round(cv_results['test_roc_auc'].mean(), 4))\n\n    gs_best = GridSearchCV(classifier, params, cv=3, n_jobs=-1, verbose=False).fit(X, y)\n    final_model = classifier.set_params(**gs_best.best_params_)\n\n    cv_results = cross_validate(final_model, X, y, cv=3, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\n    print(f\"ACCURACY (After): {round(cv_results['test_accuracy'].mean(), 4)}\")\n    print(f\"F1 (After): {round(cv_results['test_f1'].mean(), 4)}\")\n    print(f\"AUC (After): {round(cv_results['test_roc_auc'].mean(), 4)}\")\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n    best_models[name] = final_model\n\n    met_acc_aft.append(round(cv_results['test_accuracy'].mean(), 4))\n    met_f1_aft.append(round(cv_results['test_f1'].mean(), 4))\n    met_auc_aft.append(round(cv_results['test_roc_auc'].mean(), 4))\n\n\nhyp_df = pd.DataFrame(list(zip(met_acc_bef, met_f1_bef, met_auc_bef, met_acc_aft, met_f1_aft, met_auc_aft)),\n                      columns=['acc_bef', 'f1_bef', 'auc_bef', 'acc_aft', 'f1_aft', 'auc_aft'], index=[met_name])\nhyp_df.sort_values(\"auc_aft\", ascending=False, inplace=True)\nhyp_df.T.plot(kind='barh', figsize=(10, 10))","4ad0193a":"voting_clf = VotingClassifier(\n    estimators=[('CatBoost', best_models[\"CatBoost\"]),\n                ('RF', best_models[\"RF\"]),\n                ('LightGBM', best_models[\"LightGBM\"])],\n    voting='soft')\n\nvoting_clf.fit(X, y)\n\ncv_results = cross_validate(voting_clf, X, y, cv=3, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\ncv_results['test_accuracy'].mean()\ncv_results['test_f1'].mean()\ncv_results['test_roc_auc'].mean()","b00dc305":"def plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title(f'Feature Importances of {model}')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')","ea38e471":"for name, classifier, params in classifiers:\n    model = classifier\n    model_best_grid = GridSearchCV(model,\n                                   params,\n                                   cv=3,\n                                   n_jobs=-1,\n                                   verbose=True).fit(X, y)\n\n    final_model = model.set_params(**model_best_grid.best_params_, random_state=17).fit(X, y)\n    plot_importance(final_model, X)","4304a177":"# There are slight differences in data preprocessing in this part\n\ndf = data.copy()\ndf = df.drop_duplicates(subset=['address', 'name']).reset_index().drop('index', axis=1)\n\n# We can drop some columns those have no useful data.\ndrop_list = ['url', 'address', 'phone', 'reviews_list', 'menu_item', 'name', 'listed_in(city)', 'listed_in(type)']\ndf.drop(drop_list, axis=1, inplace=True)\n\ndf['approx_cost(for two people)'] = (df['approx_cost(for two people)'].str.replace(',', '')).astype('float64')\n\ndf['rate'] = df['rate'].replace('NEW', np.nan)\ndf['rate'] = df['rate'].replace('-', np.nan)\ndf['rate'] = (df['rate'].str.rstrip('\/5')).astype('float64')\n\n# dropping missing rates, we have enough data\ndf.dropna(subset=['rate'], inplace=True)\n\n# we will drop the missing values for some features those have % 0.01 missing values.\ndf.dropna(subset=['rest_type', 'approx_cost(for two people)', 'cuisines'], inplace=True)\n\n# new feature: number of the cuisines in restaurants\ndf['num_of_cuisines'] = [len(cui) for cui in df['cuisines'].str.split(',')]\n# new feature: number of the dishes liked in restaurants\ndf['num_of_dish_liked'] = [len(dish) if dish is not np.NAN else 0 for dish in df['dish_liked'].str.split(',')]\n\n# No measurement point, high cardinality.\ndf.drop('dish_liked', axis=1, inplace=True)\ndf.drop('cuisines', axis=1, inplace=True)\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n\nfor col in ['num_of_cuisines', 'num_of_dish_liked']:\n    num_cols.append(col)\nnum_cols.pop(0)  # rate\n# We're gonna do clustering, so we're not gonna use the target feature","b9c7b0a1":"correlation_matrix(df, num_cols)\n# We don't have any high correlations in our numerical variables","ed947884":"# Boxplot\n# One of the best ways of checking for outliers is Boxplot\ndf[\"votes\"].describe([0.05, 0.25, 0.45, 0.50, 0.65, 0.75, 0.95, 0.99]).T\nsns.boxplot(x=df[\"votes\"])","148ba4ad":"df[\"approx_cost(for two people)\"].describe([0.05, 0.25, 0.45, 0.50, 0.65, 0.75, 0.95, 0.99]).T\nsns.boxplot(x=df[\"approx_cost(for two people)\"])","5d1860af":"def outlier_thresholds(dataframe, col_name, q1=0.01, q3=0.85):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef remove_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    df_without_outliers = dataframe[~((dataframe[col_name] < low_limit) | (dataframe[col_name] > up_limit))]\n    return df_without_outliers","fbdfb828":"remove_outlier(df, 'votes')\nremove_outlier(df, 'approx_cost(for two people)')","53861c95":"inv_df = df\ndf = df[num_cols]\n\n# Min-Max Scaler\nsc = MinMaxScaler((0, 1))\ndf = sc.fit_transform(df)","630530b0":"pca = PCA(n_components=3)\npca_fit = pca.fit_transform(df)\npca.explained_variance_ratio_\nnp.cumsum(pca.explained_variance_ratio_)\ndf = pd.DataFrame(pca_fit)","55c59dcf":"# Detecting Optimal Cluster Number\n\nkmeans = KMeans()\nelbow = KElbowVisualizer(kmeans, k=(2, 20))\nelbow.fit(df)\nelbow.show()","d294221e":"elbow.elbow_value_","12ae6ead":"kmeans = KMeans(n_clusters=elbow.elbow_value_).fit(df)\nkumeler = kmeans.labels_\n\ndf[\"CLUSTER_NO\"] = kumeler\ndf[\"CLUSTER_NO\"] = df[\"CLUSTER_NO\"] + 1\n\ndf.head()","dc9a5844":"# 3d scatterplot using Matplotlib\nfig = plt.figure(figsize=(15, 15))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(df.loc[df['CLUSTER_NO'] == 1, 0], df.loc[df['CLUSTER_NO'] == 1, 1], df.loc[df['CLUSTER_NO'] == 1, 2],\n           s=40, color='green', label=\"SEGMENT 1\")\nax.scatter(df.loc[df['CLUSTER_NO'] == 2, 0], df.loc[df['CLUSTER_NO'] == 2, 1], df.loc[df['CLUSTER_NO'] == 2, 2],\n           s=40, color='red', label=\"SEGMENT 2\")\nax.scatter(df.loc[df['CLUSTER_NO'] == 3, 0], df.loc[df['CLUSTER_NO'] == 3, 1], df.loc[df['CLUSTER_NO'] == 3, 2],\n           s=40, color='cyan', label=\"SEGMENT 3\")\nax.scatter(df.loc[df['CLUSTER_NO'] == 4, 0], df.loc[df['CLUSTER_NO'] == 4, 1], df.loc[df['CLUSTER_NO'] == 4, 2],\n           s=40, color='purple', label=\"SEGMENT 4\")\nax.scatter(df.loc[df['CLUSTER_NO'] == 5, 0], df.loc[df['CLUSTER_NO'] == 5, 1], df.loc[df['CLUSTER_NO'] == 5, 2],\n           s=40, color='blue', label=\"SEGMENT 5\")\nax.scatter(df.loc[df['CLUSTER_NO'] == 6, 0], df.loc[df['CLUSTER_NO'] == 6, 1], df.loc[df['CLUSTER_NO'] == 6, 2],\n           s=40, color='yellow', label=\"SEGMENT 6\")\n\nax.set_xlabel('PCA_COMP_1-->')\nax.set_ylabel('PCA_COMP_2-->')\nax.set_zlabel('PCA_COMP_3-->')\nax.legend()","4fd5f18a":"# Cluster Analysis\ndf.shape\ninv_df.shape\n\ninv_df['CLUSTER_NO'] = df['CLUSTER_NO'].astype('int64')\ninv_df.head()\nfin_df = inv_df.groupby('CLUSTER_NO')['rate', 'votes', 'approx_cost(for two people)', 'num_of_dish_liked',\n                                      'num_of_cuisines'].mean()\nfin_df.sort_values(by='rate', ascending=False)\n","4dd2eba0":"# Cluster\tComments\n# 2.0\tOptimal restaurants\n# 4.0\tGood but not affordable, high prices!\n# 1.0\tThe second best option!\n# 6.0\tAverage, not bad!\n# 5.0\tLow performance, high prices!\n# 3.0\tThe last choice!","ac5ac8e3":"# Data Preprocessing & Feature Engineering","3d3c63aa":"# PCA (Principal Component Analysis) - Dimension Reduction","ecb6fe4b":"<span style=\"color:#12AD2B;font-size:36px;font-family:arial\">Dataset Story<\/span>\n<p><span style=\"font-size:16px;font-family:arial\"> Bangalore has more than 12,000 restaurants those are serving dishes from all over the world and the purpose of analysing of Zomato Bangalore Restaurants dataset is to find the key factors affecting the success of those restaurants. <\/span><\/p>\n<p><span style=\"font-size:16px;font-family:arial\"> Bangalore, India's largest IT exporter, is often referred to as the silicon valley of India. Most of the people here are immigrants and dependent mainly on the restaurant food as they don\u2019t have time to cook for themselves. As the time goes on, new restaurants are opening, the industry is not saturated yet and the demand is increasing day by day. In spite of increasing demand, it has become difficult for new restaurants to compete with established restaurants.<\/span><\/p>\n<p><span style=\"font-size:16px;font-family:arial\"><b>Phase I<\/b><\/span><\/p>\n<p><span style=\"font-size:16px;font-family:arial\">Success Prediction of Zomato Bangalore Restaurants<\/span><\/p>\n<p><span style=\"font-size:16px;font-family:arial\"><b>Phase II<\/b><\/span><\/p>\n<p><span style=\"font-size:16px;font-family:arial\">Clustering of Restaurants<\/span><\/p>","cc9c3c10":"![resim.png](attachment:0be0e788-db36-444a-8d7d-5d439fb48c23.png)","3683b781":"<span style=\"color:#12AD2B;font-size:36px;font-family:arial\">Phase II<\/span>","6edf0114":" <span style=\"color:#12AD2B;font-size:36px;font-family:arial\"> Variables & Descriptions: <\/span>\n <hr>\n \n <span style=\"font-size:24px;font-family:arial;\">\n    <table style='font-family:\"Arial\"; font-size:14px'>\n      <colgroup span=\"4\"><\/colgroup>\n      <tr>\n        <th>Variable<\/th>\n        <th>Description<\/th>\n      <\/tr>\n      <tr>\n        <td>url:<\/td>\n        <td>contains the url of the restaurant in the zomato website<\/td>\n      <\/tr>\n      <tr>\n        <td>address:<\/td>\n        <td>contains the address of the restaurant in Bangalore<\/td>\n      <\/tr>\n      <tr>\n        <td>name:<\/td>\n        <td>contains the address of the restaurant in Bangalore<\/td>\n      <\/tr>\n      <tr>\n        <td>online_order:<\/td>\n        <td>whether online ordering is available in the restaurant or not<\/td>\n      <\/tr>\n      <tr>\n        <td>book_table:<\/td>\n        <td>table book option available or not<\/td>\n      <\/tr>\n      <tr>\n        <td>rate:<\/td>\n        <td>contains the overall rating of the restaurant out of 5<\/td>\n      <\/tr>\n      <tr>\n        <td>votes:<\/td>\n        <td>contains total number of rating for the restaurant as of the above mentioned date<\/td>\n      <\/tr>\n      <tr>\n        <td>phone:<\/td>\n        <td>contains the phone number of the restaurant<\/td>\n      <\/tr>\n      <tr>\n        <td>location:<\/td>\n        <td>contains the neighborhood in which the restaurant is located<\/td>\n      <\/tr>\n      <tr>\n        <td>rest_type:<\/td>\n        <td>restaurant type<\/td>\n      <\/tr>\n      <tr>\n        <td>dish_liked:<\/td>\n        <td>dishes people liked in the restaurant<\/td>\n      <\/tr>\n      <tr>\n        <td>rest_type:<\/td>\n        <td>restaurant type<\/td>\n      <\/tr>\n      <tr>\n        <td>cuisines:<\/td>\n        <td>food styles, separated by comma<\/td>\n      <\/tr>\n      <tr>\n        <td>approx_cost(for two people):<\/td>\n        <td>contains the approximate cost for meal for two people<\/td>\n      <\/tr>\n      <tr>\n        <td>reviews_list:<\/td>\n        <td>list of tuples containing reviews for the restaurant, each tuple<\/td>\n      <\/tr>\n      <tr>\n        <td>menu_item:<\/td>\n        <td>contains list of menus available in the restaurant<\/td>\n      <\/tr>\n      <tr>\n        <td>listed_in(city):<\/td>\n        <td>contains the neighborhood in which the restaurant is listed<\/td>\n      <\/tr>\n    <\/table>    \n <\/span>","4c8a89f7":"<span style=\"color:#12AD2B;font-size:36px;font-family:arial\">Data Preprocessing & Feature Engineering<\/span>","24a14167":"<span style=\"color:#12AD2B;font-size:36px;font-family:arial\">Exploratory Data Analysis<\/span>","ade4c78a":"# Correlation Analysis","d6019d0c":"# Outliers","106bc63c":"# K-Means","53cace84":"# Derived Features","73e04f79":"<span style=\"color:#12AD2B;font-size:36px;font-family:arial\">Restaurant Clustering with PCA & K-Means<\/span>","099c4feb":"# Hyperparameter Optimization","3a41c845":"# Thanks for your interest!\n# If you like this notebook, please like or leave a comment :)","7441f34a":"# Feature Importances","c4384743":"# Final Clusters","5e53e669":"<span style=\"color:#12AD2B;font-size:36px;font-family:arial\">Modeling<\/span>","dac49cc0":"# Missing Data","55c7a7d0":"# Voting Classifier"}}