{"cell_type":{"1c67ae25":"code","3296fa30":"code","c62f3616":"code","1b39588a":"code","71912377":"code","75418207":"code","5ed342ee":"code","299de0eb":"code","4e24cde6":"code","36d795b8":"code","fc104369":"code","779dc386":"code","2a6c34c5":"code","555364f6":"code","f0acc2ec":"code","76db73a9":"code","77cad342":"code","6069ceca":"code","90173540":"code","ac780f6f":"code","e513a00f":"code","037235d0":"code","33527c35":"code","be565991":"code","59b18873":"code","9d517259":"code","4e59bad7":"code","fb739093":"code","f6db2fa8":"markdown","c829c15d":"markdown","2d3a0786":"markdown","fa1dd742":"markdown","68814572":"markdown","e1057f92":"markdown","94292c2e":"markdown","33284604":"markdown","50a2869a":"markdown","0db42e2a":"markdown","78307534":"markdown","fb98182e":"markdown","e38d413f":"markdown","fb49f797":"markdown","f1d07f05":"markdown","f412c763":"markdown"},"source":{"1c67ae25":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.impute import SimpleImputer\nwarnings.filterwarnings(\"ignore\")","3296fa30":"df=pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest=pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsample=pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")","c62f3616":"df.head()","1b39588a":"useful_features=[col for col in test.columns if col not in ['id']]","71912377":"print(\"no. of features in train set: \", df.shape[1])\nprint(\"no. of features in test set: \", test.shape[1])\nprint(\"--------------------------------------\")\nprint(\"no. of observations in train set: \",len(df))\nprint(\"no. of observations in train set: \",len(test))","75418207":"datatype=[]\nfor col in useful_features:\n    datatype.append(df[col].dtype)\nDataType=pd.DataFrame()\nDataType['features']=useful_features[:59]\nDataType['data type']=datatype[:59]\nDataType[' features ']=useful_features[59:]\nDataType[' data type ']=datatype[59:]\nprint(DataType)","5ed342ee":"sns.heatmap(df.isnull(),yticklabels=False,cmap='viridis')","299de0eb":"#Null Values in df\nprint(\"-----NULL VALUES IN TRAIN DATA------\")\nnulls=[]\nfor col in useful_features:\n    nulls.append(df[col].isna().sum())\n\nnull_df=pd.DataFrame()\nnull_df['columns']=useful_features\nnull_df['nulls']=nulls\nnull_df[r'null%']=null_df['nulls']*100\/len(df)\nnull_df.head(10)","4e24cde6":"#Null Values in test\nprint(\"------NULL VALUES IN TEST DATA-------\")\nnulls=[]\nfor col in useful_features:\n    nulls.append(test[col].isna().sum())\n\nnull_test=pd.DataFrame()\nnull_test['columns']=useful_features\nnull_test['nulls']=nulls\nnull_test[r'null%']=null_test['nulls']*100\/test.shape[0]\nnull_test.head(10)","36d795b8":"print(\"------NUMBER OF NULL VALUES IN TRAIN AND TEST SETS-------\")\nplt.figure(figsize=(15,5))\na=sns.scatterplot(x=null_test['columns'],y=null_test['nulls'],color='r',label='test nulls')\nsns.scatterplot(x=null_df['columns'],y=null_df[\"nulls\"],color='b',ax=a,label='train nulls')\na.legend(loc='center right',prop={'size':15})","fc104369":"print(\"------NUMBER OF NULL VALUES(%) IN TRAIN AND TEST SETS-------\")\nplt.figure(figsize=(15,5))\na=sns.scatterplot(x=null_test['columns'],y=null_test['null%'],color='r',label='test nulls')\nsns.scatterplot(x=null_df['columns'],y=null_df[\"null%\"],color='b',ax=a,label='train nulls')\na.legend(loc='lower right',prop={'size':15})\nplt.grid(which='major',linewidth=2)","779dc386":"#Data Loss if we drop null values\nprint(\"data loss if we drop null values = {0:.2f}%\".format(len(df.dropna())*100\/len(df)))","2a6c34c5":"#target distribution\nsns.countplot(df['claim'])","555364f6":"print(\"No. of 0 claim observations\/data \",df['claim'].value_counts()[0])\nprint(\"No. of 1 claim observations\/data \",df['claim'].value_counts()[1])","f0acc2ec":"#distribution check\nprint(\"----FEATURE DISTRIBUTIONS OF TRAIN AND TEST SETS-----\")\nplt.figure(figsize=(15,30))\nfor i,col in enumerate(useful_features):\n    plt.subplot(20,6,i+1)\n    x1=sns.distplot(df[col],color='#e74c3c',label='train')\n    sns.distplot(test[col],color='#2ecc71',ax=x1,label='test')\n    x1.legend(loc='upper right',prop={'size':5})\nplt.show()","76db73a9":"#distribution of features for claim=0 and claim=1\nprint(\"---FEATURE DISTRIBUTIONS FOR DIFFERENT CLAIM VALUES(0 & 1)---\")\nplt.figure(figsize=(15,30))\nfor i,col in enumerate(useful_features):\n    plt.subplot(20,6,i+1)\n    x1=sns.distplot(df[df['claim']==0][col],color='#e74c3c',label='claim 0')\n    sns.distplot(df[df['claim']==1][col],color='#2ecc71',ax=x1,label='claim 1')\n    x1.legend(loc='upper right',prop={'size':5})\nplt.show()","77cad342":"#feature distribution without nulls and with nulls replaced with mean\nimputer=SimpleImputer(strategy='mean')\ndf_impute=imputer.fit_transform(df[useful_features])\ntest_impute=imputer.transform(test[useful_features])\ndf_impute=pd.DataFrame(df_impute, columns=useful_features)\ntest_impute=pd.DataFrame(test_impute, columns=useful_features)","6069ceca":"#distribution check\nprint(\"TRAIN FEATURE DISTRIBUTION WITHOUT NULLS AND WITH NULLS REPLACED WITH MEAN\")\nplt.figure(figsize=(15,30))\nfor i,col in enumerate(useful_features):\n    plt.subplot(20,6,i+1)\n    x1=sns.distplot(df[col],color='#e74c3c',label='With Nulls')\n    sns.distplot(df_impute[col],color='#2ecc71',ax=x1,label='Without Nulls')\n    x1.legend(loc='upper right',prop={'size':5})\nplt.show()","90173540":"#distribution check\nprint(\"TEST FEATURE DISTRIBUTION WITHOUT NULLS AND WITH NULLS REPLACED WITH MEAN\")\nplt.figure(figsize=(15,30))\nfor i,col in enumerate(useful_features):\n    plt.subplot(20,6,i+1)\n    x1=sns.distplot(test[col],color='#e74c3c',label='With Nulls')\n    sns.distplot(test_impute[col],color='#2ecc71',ax=x1,label='Without Nulls')\n    x1.legend(loc='upper right',prop={'size':5})\nplt.show()","ac780f6f":"#No. of unique values in each faeture(cardinality check)\nprint(\"---NUMBER OF UNIQUE VALUES IN EACH FEATURE----\")\nunique_train=[]\nunique_test=[]\nfor col in useful_features:\n    unique_train.append(df[col].nunique())\n    unique_test.append(test[col].nunique())\nuniques=pd.DataFrame()\nuniques['feature']=useful_features\nuniques['train_unique']=unique_train\nuniques['test_unique']=unique_test\n\nuniques.head()","e513a00f":"print(\"----NO. OF UNIQUE VALUES IN EACH FEATURE FOR TRAIN AND TEST DATA----\")\nuniques=uniques.sort_values(by='train_unique')\nplt.figure(figsize=(15,6))\na=sns.barplot(x=uniques.feature[:40],y=uniques.train_unique, color=\"#e74c3c\",label='train_uniques')\nsns.barplot(x=uniques.feature[:40],y=uniques.test_unique,ax=a,color='b',label='test_uniques')\nplt.ylabel(\"unique counts\")\na.legend(loc=\"upper left\")","037235d0":"print(\"---NO. OF UNIQUE VALUES IN EACH FEATURE FOR TRAIN AND TEST DATA----\")\nuniques=uniques.sort_values(by='train_unique')\nplt.figure(figsize=(15,6))\na=sns.barplot(x=uniques.feature[40:],y=uniques.train_unique, color=\"#e74c3c\",label='train_uniques')\nsns.barplot(x=uniques.feature[40:],y=uniques.test_unique,ax=a,color='b',label='test_uniques')\nplt.ylabel(\"unique counts\")\na.legend(loc=\"upper left\")","33527c35":"print('f97 has the lowest cardinality in train data with unique values: ',df['f97'].nunique())\nprint('f97 has the lowest cardinality in test data with unique values: ',test['f97'].nunique())","be565991":"plt.figure(figsize=(15,8))\na=sns.histplot(df['f97'], bins=429)\nsns.histplot(test['f97'],bins=431,ax=a,color='r')","59b18873":"a=df.corr()\n\ncor=[]\nfor col in useful_features:\n    cors=pd.DataFrame()\n    cors['feature1']=a[col].index.tolist()\n    cors['feature2']=col\n    cors['corelation value']=a[col].values.tolist()\n    cor.append(cors)\n    \ncore=pd.DataFrame()\nfor i in cor:\n    core=pd.concat([core,i],axis=0,ignore_index=True)\ncore=core[core['feature1']!=core['feature2']]","9d517259":"core=core.sort_values(by='corelation value')\nprint('---corelation table with least corelation values----')\ncore.head(10)","4e59bad7":"core=core.sort_values(by='corelation value',ascending=False)\nprint('---corelation table with maximum corelation values---')\ncore.head(10)","fb739093":"plt.figure(figsize=(15,15))\nsns.heatmap(df.corr(),cmap='viridis')","f6db2fa8":"- Null values in both train and test sets lie between 1.56% - 1.66%","c829c15d":"## Data Distributions","2d3a0786":"## Null Values","fa1dd742":"## Cardinality","68814572":"- Distribution of target seems balanced","e1057f92":"- distribution of both train and test sets seems very similar\n- only few features are normally distributed, remaining are either skewed or multimodal distributions","94292c2e":"## Correlation","33284604":"## Data Preparation","50a2869a":"- In test data also, similar peaks can be observed after imputing null values with mean","0db42e2a":"feature f97 has the lowest cardinality while feature f10 has the highest cardinality","78307534":"- Distributions of data according to different target values seem similar","fb98182e":"- No Categorical columns in our data","e38d413f":"- we cannot drop all the null values as it will be a huge data loss","fb49f797":"- There is some change in distributions after imputing null values with mean\n- A sharp peak can be observed in distributions when null values are imputed","f1d07f05":"## Data Info","f412c763":"- null values of train set are between 15168 to 15678 for each feature\n- null values of test set are between 7733 to 8141 for each feature"}}