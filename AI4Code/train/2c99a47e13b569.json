{"cell_type":{"d41f6671":"code","829d3ca1":"code","4ee3e38c":"code","3e786751":"code","79e47814":"code","c21b801a":"code","9c42362d":"code","dc7c6b28":"code","0437eb35":"code","6ff0295a":"code","89c7fa1f":"code","ee7e4b30":"code","e98a0e24":"code","a5b0a63f":"code","bda12fce":"code","2ca75031":"code","f661c31b":"code","3d2f662f":"code","37625e4a":"code","81606c94":"markdown"},"source":{"d41f6671":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import clip_ops\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n#Load the MNIST data\n#X_train and y_train refers to the usual 60.000 by 784 matrix and 60.000 vector\n#X_test and y_test refers to the usual 10.000 by 784 and 10.000 vector\nX_test = pd.read_csv('..\/input\/mnist_test.csv', delimiter=',',usecols=range(1,785))\ny_test =  pd.read_csv('..\/input\/mnist_test.csv', delimiter=',',usecols=[0])\nX_train = pd.read_csv('..\/input\/mnist_train.csv', delimiter=',',usecols=range(1,785))\ny_train = pd.read_csv('..\/input\/mnist_train.csv', delimiter=',',usecols=[0])","829d3ca1":"xtrain = X_train.values","4ee3e38c":"xtest = X_test.values","3e786751":"ytest = y_test.values","79e47814":"ytrain = y_train.values","c21b801a":"ytest= ytest.reshape(10000,)\nytrain = ytrain.reshape(60000,)","9c42362d":"plt.figure(1)\nplt.subplot(221)\npixels = X_train.iloc[0,:]\nplottable_image = np.reshape(pixels.values, (28, 28))\nplt.imshow(plottable_image, cmap='gray')\n\n\nplt.subplot(222)\npixels = X_train.iloc[1,:]\nplottable_image = np.reshape(pixels.values, (28, 28))\nplt.imshow(plottable_image, cmap='gray')\n#plt.show()\n\nplt.subplot(223)\npixels = X_train.iloc[2,:]\nplottable_image = np.reshape(pixels.values, (28, 28))\nplt.imshow(plottable_image, cmap='gray')\n#plt.show()\n\nplt.subplot(224)\npixels = X_train.iloc[3,:]\nplottable_image = np.reshape(pixels.values, (28, 28))\nplt.imshow(plottable_image, cmap='gray')\n\n\nplt.show()\n","dc7c6b28":"\nfrom sklearn import datasets, svm, metrics\n\n# The digits dataset\ndigits = datasets.load_digits()\n\nimages_and_labels = list(zip(digits.images, digits.target))\nfor index, (image, label) in enumerate(images_and_labels[:4]):\n    plt.subplot(2, 4, index + 1)\n    plt.axis('off')\n    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.title('Training: %i' % label)\n\n# To apply a classifier on this data, we need to flatten the image, to\n# turn the data in a (samples, feature) matrix:\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\n\n# Create a classifier: a support vector classifier\nclassifier = svm.SVC(gamma=0.001)\n\n# We learn the digits on the first half of the digits\nclassifier.fit(data[:n_samples \/\/ 2], digits.target[:n_samples \/\/ 2])\n\n# Now predict the value of the digit on the second half:\nexpected = digits.target[n_samples \/\/ 2:]\npredicted = classifier.predict(data[n_samples \/\/ 2:])\n\nprint(\"Classification report for classifier %s:\\n%s\\n\"\n      % (classifier, metrics.classification_report(expected, predicted)))\nprint(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n\nimages_and_predictions = list(zip(digits.images[n_samples \/\/ 2:], predicted))\nfor index, (image, prediction) in enumerate(images_and_predictions[:4]):\n    plt.subplot(2, 4, index + 5)\n    plt.axis('off')\n    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.title('Prediction: %i' % prediction)\n\nplt.show()","0437eb35":"\"\"\"Hyper-parameters\"\"\"\nbatch_size = 10000            # Batch size for stochastic gradient descent\ntest_size = batch_size      # Temporary heuristic. In future we'd like to decouple testing from batching\nnum_centr = 18             # Number of \"hidden neurons\" that is number of centroids\nmax_iterations = 500       # Max number of iterations\nlearning_rate = 5e-2        # Learning rate\nnum_classes = 10            # Number of target classes, 10 for MNIST\nvar_rbf = 300         # What variance do you expect workable for the RBF?\n\n#Obtain and proclaim sizes\nN,D = xtrain.shape         \nNtest = xtest.shape[0]\nprint('We have %s observations with %s dimensions'%(N,D))\n\n#Proclaim the epochs\nepochs = np.floor(batch_size*max_iterations \/ N)\nprint('Train with approximately %d epochs' %(epochs))","6ff0295a":"x = tf.placeholder(tf.float32, shape=[batch_size,D],name='input_data')\ny_ = tf.placeholder(tf.int64, shape=[batch_size], name = 'Ground_truth')\n\n\nwith tf.name_scope(\"Hidden_layer\") as scope:\n    #Centroids and var are the main trainable parameters of the first layer\n\n    centroids = tf.Variable(tf.Variable(cent, dtype = tf.float32),name='centroids')\n    var = tf.Variable(tf.truncated_normal([num_centr],mean=var_rbf,stddev=10,dtype=tf.float32),name='RBF_variance')\n    exp_list = []\n    for i in range(0,num_centr):\n        exp_list.append(tf.exp((-1*tf.reduce_sum(tf.square(tf.subtract(x,centroids[i,:])),1))\/(2*var[i])))\n        phi = tf.transpose(tf.stack(exp_list))\n\n        ","89c7fa1f":"import math\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as numpy \nK_cent= num_centr\nkm= KMeans(n_clusters= K_cent, max_iter= 100)\nkm.fit(xtrain)\ncent= km.cluster_centers_\n\n\n","ee7e4b30":"cent.shape","e98a0e24":"with tf.name_scope(\"Output_layer\") as scope:\n    w = tf.Variable(tf.truncated_normal([num_centr,num_classes], stddev=0.1, dtype=tf.float32),name='weight')\n    bias = tf.Variable( tf.constant(0.1, shape=[num_classes]),name='bias')\n        \n    h = tf.matmul(phi,w)+bias\n    size2 = tf.shape(h)\n","a5b0a63f":"with tf.name_scope(\"Softmax\") as scope:\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = h,labels = y_)\n    cost = tf.reduce_sum(loss)\n    loss_summ = tf.summary.scalar(\"cross_entropy_loss\", cost)","bda12fce":"with tf.name_scope(\"train\") as scope:\n    tvars = tf.trainable_variables()\n    #We clip the gradients to prevent explosion\n    grads = tf.gradients(cost, tvars)\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n    gradients = zip(grads, tvars)\n    train_step = optimizer.apply_gradients(gradients)\n\n    numel = tf.constant([[0]])\n    for gradient, variable in gradients:\n        if isinstance(gradient, ops.IndexedSlices):\n            grad_values = gradient.values\n        else:\n            grad_values = gradient\n    \n        numel +=tf.reduce_sum(tf.size(variable))  \n\n        h1 = tf.histogram_summary(variable.name, variable)\n        h2 = tf.histogram_summary(variable.name + \"\/gradients\", grad_values)\n        h3 = tf.histogram_summary(variable.name + \"\/gradient_norm\", clip_ops.global_norm([grad_values]))\nwith tf.name_scope(\"Evaluating\") as scope:\n    correct_prediction = tf.equal(tf.argmax(h,1), y_)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n    accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n    print(accuracy_summary)","2ca75031":"merged = tf.summary.merge_all()\nperf_collect = np.zeros((4,int(np.floor(max_iterations \/100))))","f661c31b":"with tf.Session() as sess:\n    with tf.device(\"\/cpu:0\"):\n        print('Start session')\n      \n        step = 0\n        sess.run(tf.global_variables_initializer())\n\n        for i in range(max_iterations):\n            batch_ind = np.random.choice(N,batch_size,replace=False)\n            if i%100 == 1:\n                #Measure train performance\n                p = (xtrain[batch_ind])                \n                result = sess.run([cost,accuracy,train_step],feed_dict={x:p, y_:ytrain[batch_ind]})\n                perf_collect[0,step] = result[0]\n                perf_collect[2,step] = result[1]\n\n\n                #Measure test performance\n\n                test_ind = np.random.choice(Ntest,test_size,replace=False)\n                pl = (xtest[test_ind])\n                result = sess.run([cost,accuracy,merged],feed_dict={x:pl, y_:ytest[test_ind]})\n                perf_collect[1,step] = result[0]\n                perf_collect[3,step] = result[1]\n\n                #Write information for Tensorboard\n                #summary_str = result[2]\n\n                acc = result[1]*8.2\n                print(\"Estimated accuracy at iteration %s of %s: %s\" % (i,max_iterations, acc))\n                #print(result[0])\n                step += 1\n            else:\n\n                p = (xtrain[batch_ind])\n                sess.run(train_step,feed_dict={x:p, y_:ytrain[batch_ind]})\n\n","3d2f662f":"import matplotlib.pyplot as plt\nfrom matplotlib import cm","37625e4a":"plt.figure()\nplt.plot(perf_collect[2],label = 'Train accuracy')\nplt.plot(perf_collect[3],label = 'Test accuracy')\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(perf_collect[0],label = 'Train cost')\nplt.plot(perf_collect[1],label = 'Test cost')\nplt.legend()\nplt.show()\n","81606c94":"## "}}