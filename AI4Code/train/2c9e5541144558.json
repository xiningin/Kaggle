{"cell_type":{"1646f688":"code","8f5494a4":"code","215083f6":"code","1949d063":"code","88c625ff":"code","2402637c":"code","fd67dd42":"code","2afbe73c":"code","b38a3254":"code","ceff2a28":"code","efbf6c8e":"code","504211ee":"code","c81c37ce":"code","8e1a88b2":"code","13587c34":"code","e9e536a1":"code","9ff2e7d4":"code","774e9e0c":"code","19418357":"code","1354095c":"markdown","5679ce16":"markdown","03f8b7e5":"markdown","b1240cf5":"markdown","708d0c8e":"markdown","ec053954":"markdown","8006e941":"markdown"},"source":{"1646f688":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8f5494a4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas import plotting\n\n#plotly \nimport plotly.offline as py\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nsns.set(style=\"whitegrid\")","215083f6":"df=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","1949d063":"df.columns","88c625ff":"df=df.drop('Unnamed: 32', axis=1)\n","2402637c":"df.shape\n","fd67dd42":"diagnosis={'M':1, 'B':0}\ndf['diagnosis']=[diagnosis[x] for x in df['diagnosis']]","2afbe73c":"X=df.drop('diagnosis', axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","b38a3254":"from sklearn.feature_selection import SelectKBest, chi2\nbestfeatures=SelectKBest(score_func=chi2, k=15)\nfit=bestfeatures.fit(X,y)\ndfscore=pd.DataFrame(fit.scores_)\ndfcolumns=pd.DataFrame(X.columns)\nfeaturescore=pd.concat([dfcolumns, dfscore], axis=1)\nfeaturescore.columns=['Name', 'Score']\nfeaturescore","ceff2a28":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel=ExtraTreesClassifier()\nmodel.fit(X, y)\nprint(model.feature_importances_)","efbf6c8e":"feat_importance=pd.Series(model.feature_importances_ ,index=X.columns)\nfeat_importance.nlargest(30).plot(kind=\"barh\", figsize=(10,10))\nplt.show()","504211ee":"from imblearn.under_sampling import RandomUnderSampler\nsam=RandomUnderSampler(random_state=0)\nX_resampled_under, y_resampled_under=sam.fit_resample(X_train, y_train)","c81c37ce":"X_resampled_under.shape","8e1a88b2":"y_resampled_under.shape","13587c34":"y_resampled_under.value_counts().plot(kind='pie')","e9e536a1":"from imblearn.over_sampling import RandomOverSampler\noversam=RandomOverSampler(sampling_strategy='minority')\nX_over, y_over=oversam.fit_resample(X, y)","9ff2e7d4":"X_over.shape","774e9e0c":"y_over.value_counts()","19418357":"y_over.value_counts().plot(kind='pie')","1354095c":"# 1 Under-sampling majority class","5679ce16":"# 2 Over Sampling Minority class by duplication\n> Oversampling minority class will resample the minority class points in the data to make them equal to the majority class.","03f8b7e5":"# ExtraTreesClassifier method\n> In this method, the ExtraTreesClassifier method will help to give the importance of each independent feature with a dependent feature. Feature importance will give you a score for each feature of your data, the higher the score more important or relevant to the feature towards your output variable","b1240cf5":"# Feature Selection\n1. Univariate Selection","708d0c8e":"# Handling imbalanced data\n> Why need to handle imbalanced data? Because of to reduce overfitting and underfitting problem\n> We see that this data is imbalanced one.","ec053954":"# obs\n* Hence it is undersampled the dataset,\n","8006e941":"* Which feature has the highest score will be more related to the dependent feature and choose those features for the model."}}