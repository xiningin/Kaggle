{"cell_type":{"0e6831af":"code","2cc1a0d0":"code","3bab88a2":"code","2830e8f5":"code","a8e73709":"code","a075460e":"code","855279f3":"code","78494a2a":"code","b3db3d5a":"code","e469dfa0":"code","894f7268":"code","2220eaa2":"code","0fb7bd2f":"code","ccabe5f1":"code","9dab2d6d":"code","c8f78dc8":"code","b6980c3a":"code","b713c131":"code","dab0583e":"code","663f5f0c":"code","29c62d77":"code","ef186a7b":"code","56f9e6fd":"code","d18ea35b":"code","c7ed0515":"markdown","64943844":"markdown","59dca949":"markdown","a4b813ab":"markdown","267745df":"markdown"},"source":{"0e6831af":"%matplotlib inline","2cc1a0d0":"import os\nimport cv2\nimport pandas as pd\nimport albumentations as A\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm","3bab88a2":"DATA_ROOT = os.path.join('..', 'input')\nDATA_COMPT = os.path.join(DATA_ROOT, 'plant-pathology-2021-fgvc8')\nDATA_TRAIN_IMAGES = os.path.join(DATA_COMPT, 'train_images')","2830e8f5":"DATA_OUTPUT = '.\/'","a8e73709":"DATA_TRAIN_IMAGES_2672x4000 = os.path.join(DATA_OUTPUT, 'train_images_2672x4000')","a075460e":"DATA_TRAIN_IMAGES_224 = os.path.join(DATA_OUTPUT, 'train_images_224')","855279f3":"DATA_TRAIN_IMAGES_224x336 = os.path.join(DATA_OUTPUT, 'train_images_224x336')","78494a2a":"DATA_TRAIN_IMAGES_448 = os.path.join(DATA_OUTPUT, 'train_images_448')","b3db3d5a":"DATA_TRAIN_IMAGES_448x670 = os.path.join(DATA_OUTPUT, 'train_images_448x670')","e469dfa0":"df_train = pd.read_csv(os.path.join(DATA_COMPT, 'train.csv'))","894f7268":"df_train.head()","2220eaa2":"def show_img(image):\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image)","0fb7bd2f":"def resize_images(path_in:str, path_out:str, filenames:list, resize_to:dict):\n    if not os.path.isdir(path_out): os.mkdir(path_out)\n    \n    trfTranspose = A.Transpose(p=1)\n    trfResize = A.Resize (height=resize_to['height'], width=resize_to['width'], p=1)\n    for filename in tqdm(filenames):\n        image = cv2.imread( os.path.join(path_in, filename) )\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        height, width, _ = image.shape\n        if height\/width > 1: image = trfTranspose(image=image)['image']\n        \n        image_resized = trfResize(image=image)['image']\n        \n        save_to = os.path.join(path_out, filename)\n        image_resized = cv2.cvtColor(image_resized, cv2.COLOR_RGB2BGR)\n        if not cv2.imwrite(filename=save_to, img=image_resized): print(f'Failed to save image: {filename} to dir:{path_out}')","ccabe5f1":"filenames = df_train['image'].tolist()","9dab2d6d":"resize_to_2672x4000={'height':2672, 'width':4000, 'dir':DATA_TRAIN_IMAGES_2672x4000}","c8f78dc8":"#resize_images(path_in=DATA_TRAIN_IMAGES, path_out=resize_to_2672x4000['dir'], filenames=filenames, resize_to=resize_to_2672x4000)","b6980c3a":"resize_to_224={'height':224, 'width':224, 'dir':DATA_TRAIN_IMAGES_224}","b713c131":"#resize_images(path_in=DATA_TRAIN_IMAGES, path_out=resize_to_224['dir'], filenames=filenames, resize_to=resize_to_224)","dab0583e":"resize_to_224x336 = {'height':224, 'width':336, 'dir':DATA_TRAIN_IMAGES_224x336}","663f5f0c":"%%time\nresize_images(path_in=DATA_TRAIN_IMAGES, path_out=resize_to_224x336['dir'], filenames=filenames, resize_to=resize_to_224x336)","29c62d77":"resize_to_448={'height':448, 'width':448, 'dir':DATA_TRAIN_IMAGES_448}","ef186a7b":"#resize_images(path_in=DATA_TRAIN_IMAGES, path_out=resize_to_448['dir'], filenames=filenames, resize_to=resize_to_448)","56f9e6fd":"resize_to_448x670 = {'height':448, 'width':670, 'dir':DATA_TRAIN_IMAGES_448x670}","d18ea35b":"%%time\nresize_images(path_in=DATA_TRAIN_IMAGES, path_out=resize_to_448x670['dir'], filenames=filenames, resize_to=resize_to_448x670)","c7ed0515":"# Images","64943844":"# Loading metadata","59dca949":"# Defining environment","a4b813ab":"Information on PyTorch native pre-trained models on imagenet data with resolution 224x224 can be found [here](https:\/\/pytorch.org\/vision\/stable\/models.html)","267745df":"# Loading packages"}}