{"cell_type":{"fd47e0b5":"code","ad1d6649":"code","ffcbb66e":"code","e06ad181":"code","09e8e94f":"code","072d4797":"code","cfa6e050":"code","d0641f48":"code","232726a8":"code","887dd7c0":"code","a215cca8":"code","8be35268":"code","893f5ba6":"code","a18609e9":"code","6e0f7806":"code","7203aad0":"code","c3e8c9e5":"code","8ed9b7f6":"code","70c0dc6b":"code","0bf2ea94":"code","ce79c501":"code","6879eb0d":"code","6e05f324":"markdown","c07d4dc0":"markdown","6df47533":"markdown","359ca98a":"markdown","d5df2b6a":"markdown","eb71357d":"markdown","e3fb1c78":"markdown","3290921c":"markdown","ab626bbc":"markdown","6e39be3c":"markdown","73aaa3a0":"markdown","9a3e1ceb":"markdown","e4c30023":"markdown","c575f86d":"markdown","e1c87f7b":"markdown","c343541e":"markdown"},"source":{"fd47e0b5":"import pandas\nimport seaborn\nimport matplotlib.pyplot as plot\nimport numpy as np\nfrom scipy.stats import zscore\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.neighbors import NearestNeighbors\n\n# Enter the route to your datasets below\n\ndatasetContacts = pandas.read_csv(\"..\/input\/tarea2\/DatasetContactos.csv\") \ndatasetTrxs = pandas.read_csv(\"..\/input\/tarea2\/DatasetTrxs.csv\")\ndataset = pandas.merge(datasetTrxs, datasetContacts, on='CUST_ID', how='inner')","ad1d6649":"dataset.head(15)","ffcbb66e":"dataset.info()","e06ad181":"dataset.describe()","09e8e94f":"print(dataset.shape)\nnewDataset = dataset.drop(['PHONE', 'CUST_ID'], axis='columns')\nz = np.abs(zscore(newDataset))\noutliers_position = []\nfor i in np.where(z > 3):\n    outliers_position = np.concatenate((outliers_position, i))\nfinal_outliers = list(dict.fromkeys(outliers_position))\nprint(len(final_outliers))\ndataset = dataset.drop(final_outliers)\nprint(dataset.shape)","072d4797":"dataset = dataset.dropna(subset=['CREDIT_LIMIT', 'MINIMUM_PAYMENTS'])","cfa6e050":"dataset.info()","d0641f48":"selected_features = [\n    'BALANCE_FREQUENCY',\n    'PURCHASES',\n    'PURCHASES_INSTALLMENTS_FREQUENCY',\n    'CREDIT_LIMIT',\n    'PRC_FULL_PAYMENT',\n    'MINIMUM_PAYMENTS',\n    'ONEOFF_PURCHASES_FREQUENCY'\n]","232726a8":"selected_data = dataset[selected_features]\nscaler = MinMaxScaler()\n\nscaler.fit(selected_data)\nnormalized_data = scaler.transform(selected_data)","887dd7c0":"pca = PCA(n_components=7)\nprincipalComponents = pca.fit_transform(normalized_data)\n\nfeatures = range(pca.n_components_)\nplot.bar(features, pca.explained_variance_ratio_, color='black')\nplot.xlabel('PCA features')\nplot.ylabel('variance %')\nplot.xticks(features)\n\nPCA_components = pandas.DataFrame(principalComponents)","a215cca8":"pca = PCA(3)\nPCA_dataset = pca.fit_transform(normalized_data)\n\nkmeans = KMeans(n_clusters=2)\nlabel = kmeans.fit_predict(PCA_dataset)\nprint(label)","8be35268":"centers = np.array(kmeans.cluster_centers_)\n\nplot.figure(figsize=(15,15))\nuniq = np.unique(label)\n\nfor i in uniq:\n  plot.scatter(PCA_dataset[label == i , 0] , PCA_dataset[label == i , 1] , label = i)\nplot.xlabel('PC1')\nplot.ylabel('PC2')\nplot.scatter(centers[:,0], centers[:,1], marker=\"x\", color='k')\nplot.legend()\nplot.show()","893f5ba6":"dataset.insert(20, \"CLUSTER\", label, True)","a18609e9":"dataset.head(15)","6e0f7806":"dataset[dataset['CLUSTER'] == 0].describe()","7203aad0":"dataset[dataset['CLUSTER'] == 1].describe()","c3e8c9e5":"dataset_to_contact = dataset[dataset['CONTACTS'] < 2]\ndataset_to_contact.head(10)","8ed9b7f6":"neighbors = NearestNeighbors(n_neighbors=50)\nneighbors_fit = neighbors.fit(normalized_data)\ndistances, indices = neighbors_fit.kneighbors(normalized_data)\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplot.plot(distances)","70c0dc6b":"dbscan = DBSCAN(eps=0.15, min_samples=40)\nlabel_db = dbscan.fit_predict(normalized_data)\n\nn_clusters_ = len(set(label_db)) - (1 if -1 in label_db else 0)\nn_noise_ = list(label_db).count(-1)\n\ndataset.insert(21, \"DBSCAN\", label_db, True)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)","0bf2ea94":"dataset[dataset['DBSCAN'] == 0].describe()","ce79c501":"dataset[dataset['DBSCAN'] == 1].describe()","6879eb0d":"X = dataset[selected_features]\nY = dataset['CLUSTER']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\ntreeClass = DecisionTreeClassifier(max_depth=len(selected_features))\ntreeClass = treeClass.fit(X_train, Y_train)\n\nfig = plot.figure(figsize=(100,100))\nplot_tree(treeClass, \n           feature_names=selected_features,  \n           class_names=['Tarjeta Internet Global', 'Tarjeta Platinum'])","6e05f324":"Se inserta una nueva columna en el dataset que indique a que cluster pertenece el cliente.","c07d4dc0":"# Parte 3\n\nEn primer lugar, se seleccionaron algunos features que se creyo que podian resultar en una buena clusterizacion de los clientes del banco.\nLos seleccionados fueron:\n* BALANCE_FREQUENCY: Frecuencia de actualizaci\u00f3n del saldo. Esto nos ayudara a ver que tan intenso es el uso de la tarjeta de credito.\n* PURCHASES: Importe de las compras realizadas. Nos indicara que tanto dinero el cliente gasta usando su tarjeta, y por ende indicara si tiene un mayor o menor nivel adquisitivo.\n* ONEOFF_PURCHASES_FREQUENCY: Compras frecuentes son realizadas de una sola vez. Nos indicara con que frecuencia el cliente decide comprar con la tarjeta de credito en un solo pago.\n* PURCHASES_INSTALLMENTS_FREQUENCY: Frecuencia de compras en cuotas. Analogo al feature anterior, nos indica que tanto el cliente decide comprar en cuotas.\n* CREDIT_LIMIT: Limite de cr\u00e9dito. Asumiremos que si el limite de credito es mas grande, es porque el banco considero que tenian un buen nivel adquisitivo.\n* PRC_FULL_PAYMENT: Porcentaje de pagos totales hechos por el usuario. Nos indica que tantas veces el cliente paga todo el monto de su tarjeta de credito y esto tambien esta asociando a un mayor poder adquisitivo.\n* MINIMUM_PAYMENTS: Cantidad de pagos m\u00ednimos hechos por el usuario. Si esta cantidad es muy alta, consideramos que el cliente no tiene tan buen nivel adquisitivo ya que no le es posible pagar la factura completa.\n\n\n","6df47533":"# Parte 1\n\nEDA para ambos datasets.\nLo primero que se hizo fue modificar el csv de contactos ya que tenia los valores separados por ';' en vez de ',' y eso no permitia leerlo como csv.\nAdemas se mergeo ambos datasets en uno solo usando el CUST_ID para tener toda la informacion en un solo dataset.\nA continuacion, se imprimen las primeras 15 filas del dataset obtenido.","359ca98a":"Se obtienen 2 clusters y 2.439 puntos considerados como ruido. Estos valores se pueden modificar al cambiar los valores de epsilon y el minimo de puntos que tiene que haber en cada cluster.\nA continuacion podemos ver que en el cluster 0 quedan 4.883 puntos y en el 1 quedan solo 98.\nCreemos que la clusterizacion obtenida usando k-means es mucho mas representativa y divide a los clientes en 2 tipos bien definidos, pero en el caso de la clusterizacion con DBSCAN no nos es posible inferir nada de los clusters. \nDe cualquier manera, si hubiera que elegir a que cluster ofrecerle la tarjeta platinum, seria al cluster 1. ","d5df2b6a":"Se filtra el dataset para obtener solo los clientes a los que se puede llamar a ofrecer el producto.","eb71357d":"A continuacion, se obtiene el cluster asociado a cada fila en el dataset.","e3fb1c78":"A continuacion se obtienen las estadisticas de los datos incluidos en cada cluster.\nPodemos ver que en el cluster 0 hacen un uso mas intensivo de la tarjeta y que el gasto que se realiza, en promedio, es mucho mayor. Se podria suponer, entonces que en el cluster 0 quedaron asignados los clientes con mayor poder adquisitivo. Por lo tanto, a ellos se les ofreceria la tarjeta Platinum. Ademas, vemos que hacen mas pagos completos que los del cluster 1, pero de cualquier manera, hay muchos realizando pagos minimos que es algo que no deseamos para la tarjeta platinum. Por eso, solo ofreceriamos esta tarjeta a aquellos que tengan un numero de pagos minimos menor a 700, que es un numero cercano al cuartil del 75%.","3290921c":"# Parte 2\n\nEn esta parte, se limpiara el dataset. En primer lugar, se eliminaran los outliers que son valores atipicos que pueden afectar las predicciones. Se puede ver que en los datos habia 1.387 filas con outliers.\nLuego, se eliminaran los valores nulos que ya se vio que habia en CREDIT_LIMIT y MINIMUM_PAYMENTS.\nLa decision que se tomo fue eliminar las filas con outliers y con valores nulos, no rellenarlos con valores esperados.\nLuego de esto, el dataset que se usara para clusterizar, queda con 7420 entradas.","ab626bbc":"Luego, se obtienen algunas estadisticas de las columnas numericas del dataset que ayudaran a comprender mejor los datos y su distribucion.\nSe tienen el promedio, la desviacion estandar y los principales cuartiles para cada feature.","6e39be3c":"Se grafican los clusters y sus centros.","73aaa3a0":"# Parte 4\n\nDensity-based spatial clustering of applications with noise (DBSCAN) es un algoritmo de clusterizaci\u00f3n basado en densidad.\nPara clusterizar (lograr el agrupamiento de conjuntos de objetos no etiquetados, para lograr construir subconjuntos de datos conocidos como Clusters), DBSCAN clasifica a los puntos de la siguiente manera: \n* Puntos \"core\": son los puntos interiores de un cluster, cuando tienen, por lo menos, un n\u00famero m\u00ednimo de puntos a una distancia E de p (minPts). \n* Puntos \"border\" (frontera): tienen menos de minPts a una distancia E de p, y est\u00e1n en el vecindario de alg\u00fan punto core.\n* Puntos noise (ruido): cualquier punto que no forma parte de un cluster core ni del border\n\nPara decidir cual va a ser el epsilon, se busca el punto de mayor curvatura en la siguiente grafica. Y se cree que es en 0.15","9a3e1ceb":"# Parte 5 \n\nLas preguntas nos ayudarian a reconstruir los features que se usaron para la clusterizacion y serian:\n\n* Del 1 al 5, que tanto utilizaria una tarjeta de credito?\n* Cuanto dinero cree que gastaria usando la tarjeta de credito?\n* Del 1 al 5, que tanto utilizaria la tarjeta de credito para comprar en cuotas?\n* Del 1 al 5, que tanto utilizaria la tarjeta de credito para comprar en una unica cuota?\n* Cual seria el limite minimo para tu tarjeta de credito ideal?\n* Cuantas veces al a\u00f1o crees que pagarias el total de la factura? Y el minimo?\n\nSe realiza un arbol que dependiendo de la respuesta del futuro cliente toma un camino y le asigna una tarjeta. Para poder usar el arbol de decision, se debe mapear las respuestas a los valores que trae el dataset. Por ejemplo las frecuencias serias del 0 al 1, no del 1 al 5.","e4c30023":"Se uso MinMaxScaler para normalizar las columnas elegidas a valores entre 0 y 1 para clusterizar. Esto se realiza para evitar que se le de mayor importancia a columnas con diferencias grandes entre los valores. ","c575f86d":"En primer lugar, obtendremos informacion sobre el tama\u00f1o, los valores nulos y el tipo de los features del dataset.\nPodemos ver que todas las columnas son numericas salvoel CUST_ID y el PHONE.\nAdemas, hay valores nulos en MINIMUM_PAYMENTS y en CREDIT_LIMIT. Se trabajara para eliminarlos y que no afecten la clusterizacion mas adelante.","e1c87f7b":"Para decidir cuantos principal components se usarian para la clusterizacion, nos fijamos en cuales se contenia la mayoria de la informacion. Se puede ver que en el 0, 1 y 2 ya se tiene mas del 80%, por eso se usaran 3 PC para la clusterizacion.","c343541e":"# Parte 6\n\nPara concluir, esta tarea logr\u00f3 que se llevaran a pr\u00e1ctica los conocimientos te\u00f3ricos de algoritmos de aprendizaje autom\u00e1tico no supervisado, y se pudieron observar muchas cosas.\n\nEn primer lugar, al no ser supervisado, no hay una columna independiente para realizar el an\u00e1lisis, sino que se debe elegir y seleccionar las features que se consideren relevantes. Para hacer esta selecci\u00f3n, se debi\u00f3 evaluar principalmente cu\u00e1l era el objetivo del an\u00e1lisis, y entender en profundidad la utilidad de cada uno de los features. Previo a la selecci\u00f3n final de features, se realizo un recorrido por otras que parec\u00edan relevantes pero luego se not\u00f3 claramente que no eran tan importantes como pod\u00eda parecer en primera instancia.\n\nComo el dataset en s\u00ed mismo no trae etiquetadas las filas en clases, la custerizaci\u00f3n es responsabilidad completa del desarrollador. En este caso espec\u00edfico, evaluar qu\u00e9 clientes recibir\u00edan qu\u00e9 tarjeta.\n\nAl tener que elegir las columnas importantes para realizar el an\u00e1lisis, se debe entender sobre el tema a trabajar, y, adem\u00e1s, se debe evaluar previamente si no existen outliers que puedan perjudicar el estudio final. Es por esto que es muy importante realizar el an\u00e1lisis completo sobre un dataset \"limpio\", es decir, sin outliers alevosos.\n\nPara terminar, se concluye que en este tipo de algoritmos de aprendizaje autom\u00e1tico no supervisado es m\u00e1s dif\u00edcil evaluar los resultados. No hay algo bien o algo mal. Solamente se realiz\u00f3 la separaci\u00f3n de clientes para entregar al banco y el banco luego de contactarlos podr\u00e1 evaluar la efectividad del algoritmo. Pero sin llegar a ese paso, no se puede saber si efectivamente fue realizado correctamente o no.\n"}}