{"cell_type":{"43d45931":"code","01623751":"code","6211fdd3":"code","5f0b2853":"code","95d63178":"code","f4a30856":"code","5fb99ec2":"code","bd8661bd":"code","3b51cbaa":"code","7a5e42e9":"code","f98d6e47":"markdown","94b635cd":"markdown","1d24aea3":"markdown","990d5c05":"markdown","b9eb5d41":"markdown","fbea102b":"markdown"},"source":{"43d45931":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\nks = pd.read_csv('..\/input\/kickstarter-projects\/ks-projects-201801.csv',\n                 parse_dates=['deadline', 'launched'])\n\n# Drop live projects\nks = ks.query('state != \"live\"')\n\n# Add outcome column, \"successful\" == 1, others are 0\nks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))\n\n# Timestamp features\nks = ks.assign(hour=ks.launched.dt.hour,\n               day=ks.launched.dt.day,\n               month=ks.launched.dt.month,\n               year=ks.launched.dt.year)\n\n# Label encoding\ncat_features = ['category', 'currency', 'country']\nencoder = LabelEncoder()\nencoded = ks[cat_features].apply(encoder.fit_transform)\n\ndata_cols = ['goal', 'hour', 'day', 'month', 'year', 'outcome']\nbaseline = ks[data_cols].join(encoded)","01623751":"# Defining  functions that will help us test our encodings\nimport lightgbm as lgb\nfrom sklearn import metrics\n\ndef get_data_splits(dataframe, valid_fraction=0.1):\n    valid_fraction = 0.1\n    valid_size = int(len(dataframe) * valid_fraction)\n\n    train = dataframe[:-valid_size * 2]\n    # valid size == test size, last two sections of the data\n    valid = dataframe[-valid_size * 2:-valid_size]\n    test = dataframe[-valid_size:]\n    \n    return train, valid, test\n\ndef train_model(train, valid):\n    feature_cols = train.columns.drop('outcome')\n\n    dtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\n    dvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n\n    param = {'num_leaves': 64, 'objective': 'binary', \n             'metric': 'auc', 'seed': 7}\n    print(\"Training model!\")\n    bst = lgb.train(param, dtrain, num_boost_round=1000, valid_sets=[dvalid], \n                    early_stopping_rounds=10, verbose_eval=False)\n\n    valid_pred = bst.predict(valid[feature_cols])\n    valid_score = metrics.roc_auc_score(valid['outcome'], valid_pred)\n    print(f\"Validation AUC score: {valid_score:.4f}\")\n    return bst","6211fdd3":"# Training a model on the baseline data\ntrain, valid, _ = get_data_splits(baseline)\nbst = train_model(train, valid)","5f0b2853":"from sklearn.decomposition import TruncatedSVD\n\n# Use 3 components in the latent vectors\nsvd = TruncatedSVD(n_components=3)","95d63178":"train, valid, _ = get_data_splits(baseline)\n\n# Create a sparse matrix with cooccurence counts\npair_counts = train.groupby(['country', 'category'])['outcome'].count()\npair_counts.head(10)","f4a30856":"pair_matrix = pair_counts.unstack(fill_value=0)\npair_matrix","5fb99ec2":"svd_encoding = pd.DataFrame(svd.fit_transform(pair_matrix))\nsvd_encoding.head(10)","bd8661bd":"encoded = svd_encoding.reindex(baseline['country']).set_index(baseline.index)\nencoded.head(10)","3b51cbaa":"# Join encoded feature to the dataframe, with info in the column names\ndata_svd = baseline.join(encoded.add_prefix(\"country_category_svd_\"))\ndata_svd.head()","7a5e42e9":"train, valid, _ = get_data_splits(data_svd)\nbst = train_model(train, valid)","f98d6e47":"# Encoding with Singular Value Decomposition\n\nHere I'll use singular value decomposition (SVD) to learn encodings from pairs of categorical features. SVD is one of the more complex encodings, but it can also be very effective. We'll construct a matrix of co-occurences for each pair of categorical features. Each row corresponds to a value in feature A, while each column corresponds to a value in feature B. Each element is the count of rows where the value in A appears together with the value in B.\n\nYou then use singular value decomposition to find two smaller matrices that equal the count matrix when multiplied.\n\n<center><img src=\"https:\/\/i.imgur.com\/mnnsBKJ.png\" width=600px><\/center>\n\nYou can choose how long each factor vector will be. Longer vectors will contain more information at the cost of more memory\/computation. To get the encodings for feature A, you multiply the count matrix by the small matrix for feature B.\n\nI'll show you how you can do this for one pair of features using scikit-learn's `TruncatedSVD` class.","94b635cd":"Now we have a series with a two-level index. We want to convert this into a matrix with `country` on one axis and `category` on the other. To do this, we can use `.unstack`. By default it'll put `NaN`s where data doesn't exist, but we can tell it to fill those spots with zeros.","1d24aea3":"This gives us a mapping of the values in the country feature, the index of the dataframe, to our encoded vectors. Next, we need to replace the values in our data with these vectors. We can do this using the `.reindex` method. This method takes the values in the country column and creates a new dataframe from from `svd_encoding` using those values as the index. Then we need to set the index back to the original index. Note that I learned the encodings from the training data, but I'm applying them to the whole dataset.","990d5c05":"Setting up a few things first, then I'll get into how to encode categorical features with singular value decomposition (SVD).","b9eb5d41":"The baseline score was 0.7467, while we get a slight improvement to 0.7472 with the SVD encodings.","fbea102b":"First we can use `.groupby` to count up co-occurences for any pair of features."}}