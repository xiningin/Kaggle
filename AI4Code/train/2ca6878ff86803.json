{"cell_type":{"4bfc749f":"code","cdd452b2":"code","cdcd7490":"code","34d77cfb":"code","fa64ddda":"code","2dd99924":"code","e343614e":"code","53cb3a31":"code","cccf324f":"code","6d981628":"markdown","12403279":"markdown","d1383dff":"markdown","9d0f6dc0":"markdown"},"source":{"4bfc749f":"!git clone https:\/\/github.com\/TylerYep\/torchinfo","cdd452b2":"%cd torchinfo","cdcd7490":"import torch \nfrom torchvision import transforms, models \nimport cv2, os, time, pickle \nfrom PIL import Image \nimport matplotlib.pyplot as plt \nimport matplotlib.animation as animate\nimport numpy as np \nfrom glob import glob \nfrom typing import Dict, List, Any, Union, Tuple \nfrom torchinfo import summary # !git clone https:\/\/github.com\/TylerYep\/torchinfo\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nprint(device)\ntorch.manual_seed(0)\nnp.random.seed(0)","34d77cfb":"class Transform(object):\n    def __init__(self):\n        self.data_trans = transforms.Compose([\n            transforms.ToTensor() # 0~1\n        ])\n    def __call__(self, img):\n        return self.data_trans(img)\n    \ndef load_model():\n    model = models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n    model.to(device)\n    model.eval()\n    return model ","fa64ddda":"summary(load_model(), input_size=(1, 3, 224, 224))","2dd99924":"# https:\/\/pytorch.org\/vision\/stable\/models.html\nCOCO_INSTANCE_CATEGORY_NAMES = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N\/A', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'N\/A', 'backpack', 'umbrella', 'N\/A', 'N\/A',\n    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'N\/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N\/A', 'dining table',\n    'N\/A', 'N\/A', 'toilet', 'N\/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N\/A', 'book',\n    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\nLABEL = np.arange(len(COCO_INSTANCE_CATEGORY_NAMES)).tolist() \nobject_car = [1, 2, 3, 4, 5, 6, 7, 8, 9] # Objects that appear to be on the road","e343614e":"def inference(img_tensor: torch.Tensor) -> Dict[str, torch.Tensor]:\n    model = load_model()\n    with torch.no_grad():\n        outputs = model(img_tensor)\n    return outputs[0]","53cb3a31":"def detect():\n    start = time.time()\n    fig = plt.figure()\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n    ims = []\n    for img_path in glob(\"..\/..\/input\/car-object-detection\/data\/testing_images\/*.jpg\")[:2]:\n        img = Image.open(img_path)\n        trans = Transform()\n        img_tensor = trans(img).unsqueeze(0)\n        bbox = inference(img_tensor)\n        orlImg = np.array(img, dtype=np.uint8)\n        for box, score, label in zip(bbox[\"boxes\"], bbox[\"scores\"], bbox[\"labels\"]):\n            box = box.detach().cpu().numpy()\n            label = label.item()\n            score = score.item()\n            if score >= 0.5 and label in LABEL:\n                pos1 = (int(box[0]), int(box[1])) # (xmin, ymin)\n                pos2 = (int(box[2]), int(box[3])) # (xmax, ymax)\n                \n                if int(label) in object_car: \n                    color = (0, 0, 192) # red \n                else:\n                    color = (0, 192, 0) # green \n\n                orlImg = cv2.rectangle(orlImg, pos1, pos2, color, thickness=2)\n        orlImg = orlImg[:, :, ::-1] # BGR -> RGB\n        im = plt.imshow(orlImg, animated=True)\n        plt.xticks([])\n        plt.yticks([])\n        ims.append([im])\n    \n    ani = animate.ArtistAnimation(fig, ims, interval=100, blit=True, repeat_delay=1000)\n    ani.save(\"car-detection.mp4\", writer=\"ffmpeg\")\n    end = time.time()\n    print(f\"sucessfully saving video {end-start:.2f}s\")\n    \ndetect()","cccf324f":"def segment():\n    start = time.time()\n    fig = plt.figure()\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n    ims = []\n    for img_path in glob(\"..\/..\/input\/car-object-detection\/data\/testing_images\/*.jpg\")[:2]:\n        img = Image.open(img_path)\n        trans = Transform()\n        img_tensor = trans(img).unsqueeze(0)\n        bbox = inference(img_tensor)\n        orlImg = np.array(img, dtype=np.uint8)\n        masks = None \n        for score, mask, label in zip(bbox[\"scores\"], bbox[\"masks\"], bbox[\"labels\"]):\n            socre = score.item()\n            label = label.item()\n            if score >= 0.5 and label in object_car:\n                if masks is None:\n                    masks = mask \n                else:\n                    masks = torch.max(masks, mask)\n        if masks is None: continue\n        masks = masks[0].cpu().numpy() # (width, height)\n        im = plt.imshow(masks, animated=True)\n        plt.xticks([])\n        plt.yticks([])\n        ims.append([im])\n        \n    ani = animate.ArtistAnimation(fig, ims, interval=300, blit=True, repeat_delay=2000)\n    ani.save(\"segmentation.mp4\", writer=\"ffmpeg\")\n    plt.show()\n    end = time.time()\n    print(f\"sucessfully saving video {end-start:.2f}s\")\n    \nsegment()","6d981628":"### Inference model execution and video capture  \n\n---\n* object detecion  \n* segmentation","12403279":"### results URL    \n---\n[detection video](https:\/\/user-images.githubusercontent.com\/75005025\/132973247-580ef05b-d4ac-4b37-8112-1082e244851e.mp4)  \n[segmentation video](https:\/\/user-images.githubusercontent.com\/75005025\/132973321-a359a7dc-1b98-41e6-a9c4-89c2ea6163cc.mp4)","d1383dff":"### model structure  \n---","9d0f6dc0":"### loading trained model from torchvision  \n---"}}