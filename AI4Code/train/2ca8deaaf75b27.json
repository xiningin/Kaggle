{"cell_type":{"6ee2e7a7":"code","e0190ce4":"code","aef9fcb5":"code","200c06e6":"code","5be6101f":"code","fd47e515":"code","45951fc3":"code","6ed6d386":"code","f5e471f9":"code","2ccf5ce7":"code","f79e73a6":"code","8259adf8":"code","e38b3e64":"code","6a4338d6":"code","ed43c471":"code","2b90c7c6":"code","1758ce7a":"code","5bf749ed":"code","a6ac7a11":"code","3bcbce3b":"code","40a92cf4":"code","32b4d5e4":"code","16f8a269":"code","22bf0df7":"code","c6647460":"code","39cdd020":"code","27e70bb2":"code","1bec8c78":"markdown","19e21075":"markdown"},"source":{"6ee2e7a7":"# Make necessary imports\nimport numpy as np\nimport pandas as pd\nimport itertools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix","e0190ce4":"# Read the data\ndf = pd.read_csv('..\/input\/fake-news\/train.csv')","aef9fcb5":"# Rows and columns in the data\ndf.shape","200c06e6":"# Take a glimpse of the data\ndf.head()","5be6101f":"# Get more information about the data\ndf.info()","fd47e515":"# Check for missing data in each feature\/column\ndf.isna().sum()","45951fc3":"# Drop unrelated features first, then drop missing data\ndf = df.drop(columns=['title', 'author']).dropna()","6ed6d386":"# Double check missing data\ndf.isna().sum()","f5e471f9":"# Get the labels. 1: unreliable, 0: reliable\nlabels = df.label\nlabels.head()","2ccf5ce7":"# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(df['text'], labels, test_size = 0.2, random_state = 7)","f79e73a6":"# Initialize a TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_df = 0.7)\n\n# Fit and transform train set, transform test set\ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test = tfidf_vectorizer.transform(X_test)","8259adf8":"# Initialize a PassiveAggressiveClassifier\npac = PassiveAggressiveClassifier(max_iter = 50)\npac.fit(tfidf_train, y_train)\n\n# Predict on the test set and calculate accuracy\ny_pred = pac.predict(tfidf_test)\nscore = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {round(score*100, 2)}%')","e38b3e64":"# Build confusion matrix. 1: unreliable, 0: reliable\nconfusion_matrix(y_test, y_pred, labels=[1, 0])","6a4338d6":"# Read the data\ntest_data = pd.read_csv('..\/input\/fake-news\/test.csv')","ed43c471":"# Assign ids to an object to use it later for Kaggle submission\ntest_id = test_data['id']","2b90c7c6":"# Rows and columns in the data\ntest_data.shape","1758ce7a":"# Take a glimpse of the data\ntest_data.head()","5bf749ed":"# Get some information about the data\ntest_data.info()","a6ac7a11":"# How many missing data in each feature\/column\ntest_data.isna().sum()","3bcbce3b":"# Drop unrelated features first, then fill missing data\n# Fill NAs instead of dropping, since the submission is expecting same number of observations as the original one\ntest_data = test_data.drop(columns=['id','title', 'author']).fillna('fake and unreliable')","40a92cf4":"# Double check missing data\ntest_data.isna().sum()","32b4d5e4":"# Confirm that cleaned test data has same observations as the original one (i.e. 5200)\ntest_data.shape","16f8a269":"# Transform test data\ntest_vectorized = tfidf_vectorizer.transform(test_data['text'])","22bf0df7":"# Predict test data\ntest_predictions = pac.predict(test_vectorized)","c6647460":"# Join test data's ids with their respective predicted labels\nsubmission = pd.DataFrame({'id':test_id, 'label':test_predictions})\nsubmission.shape","39cdd020":"submission.head()","27e70bb2":"# Save the submission file\nsubmission.to_csv('submission.csv', index=False)","1bec8c78":"## Test Data","19e21075":"# Fake News Detector\nBuilding a system to identify unreliable news articles."}}