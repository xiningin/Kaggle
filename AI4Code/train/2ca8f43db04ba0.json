{"cell_type":{"a8525a24":"code","c2e73618":"code","4bc13ef6":"code","74747161":"code","37ffa898":"code","0e47963a":"code","c236dea5":"code","0eac20d8":"code","7d3a2c61":"code","ba800789":"code","de5e9164":"code","32f9b62f":"code","719f3870":"code","4448ff90":"code","ebe5fcd4":"code","984b54c8":"code","d0053f22":"code","c6bc346d":"code","8d6c84f4":"code","f7f37872":"code","4c35df2b":"code","2fe74835":"code","a971124f":"code","31e6d55b":"code","ce56190a":"code","4242fcfe":"code","0d65d48d":"code","54bb1feb":"markdown","bfeaf388":"markdown","61f89d4a":"markdown","2c5ba7a1":"markdown","2515f43b":"markdown","202d81e6":"markdown","26821550":"markdown","9657f4ec":"markdown","bf7e37a9":"markdown","ff851f4b":"markdown","473ef471":"markdown","8ebad26f":"markdown","757d3cb9":"markdown"},"source":{"a8525a24":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport cv2\nimport tensorflow as tf\nfrom keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout\nfrom keras.models import Sequential","c2e73618":"img_dir = \"\/kaggle\/input\/face-mask-detection-dataset\/Medical mask\/Medical mask\/Medical Mask\/images\"\ntrain_full = pd.read_csv(\"\/kaggle\/input\/face-mask-detection-dataset\/train.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/face-mask-detection-dataset\/submission.csv\")","4bc13ef6":"train_full.head()","74747161":"options = ['face_with_mask','face_no_mask']\ntrain = train_full[train_full['classname'].isin(options)].sort_values('name')\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n# encode classname column since it has inly to classes\ntrain['target'] = le.fit_transform(train.classname)\nprint(\"Number of unique images in train set: \", train.name.nunique())\ntrain.head()","37ffa898":"plt.bar(['face_with_mask','face_no_mask'], train.classname.value_counts(), color = ['lightblue','pink']);\nplt.title('The ratio of people who wear a mask to those who do not');","0e47963a":"print(\"Number of unique images in submission set: \", submission.name.nunique())\nsubmission.head()","c236dea5":"train_images = train.name.unique().tolist()\ntest_images = submission.name.unique().tolist()","0eac20d8":"# pick a random index from train_images\ni = np.random.choice(np.arange(1801, len(train_images)))\n# read image\nimg = plt.imread(os.path.join(img_dir,train_images[i]))\n# plot image\nplt.imshow(img)","7d3a2c61":"# pick a random index from test_images\ni = np.random.choice(len(test_images))\n# read image\nimg = plt.imread(os.path.join(img_dir,test_images[i]))\n# plot image\nplt.imshow(img)","ba800789":"# create a list that would contain bounding boxes for each face\nbounding_box=[]\nfor i in range(len(train)):\n    lst = []\n    # extract coordinates of bounding box\n    for box in train.iloc[i][[\"x1\",'x2','y1','y2']]:\n        lst.append(box)\n    bounding_box.append(lst)\n    \n#add new column with bounding boxes\ntrain[\"bounding_box\"] = bounding_box  \n\n# get box(es) for each unique image\ndef get_boxes(filename):\n    boxes = []\n    # get bounding_box column for all rows where train[\"name\"] == filename\n    for box in train[train[\"name\"] == filename][\"bounding_box\"]:\n        boxes.append(box)\n    return boxes","de5e9164":"# draw an image with detected objects\ndef draw_facebox(image, boxes):\n    # plot the image\n    plt.imshow(image)\n    # get the context for drawing boxes\n    ax = plt.gca()\n    # plot each box\n    for box in boxes:\n    # get coordinates\n        x, y, width, height = box[0], box[1], box[2], box[3],\n        # create the shape\n        rect = plt.Rectangle((x, y), width-x, height-y, \n                             fill=False, color='b', linewidth=1)\n        # draw the box\n        ax.add_patch(rect)\n    # show the plot\n    plt.show()\n    \n# pick a random index from train_images\ni = np.random.choice(np.arange(1801, len(train_images)))\n# read image\nimage = plt.imread(os.path.join(img_dir,train_images[i]))\n# get boxes for image\nboxes = get_boxes(train_images[i])\nprint(boxes)\ndraw_facebox(image, boxes)","32f9b62f":"img_size=128\ndata=[]\nfor i in range(len(train)):\n    x,y,width,height = train.iloc[i]['bounding_box']\n    image = train.iloc[i]['name']\n    # read image with green channel\n    img_array = cv2.imread(os.path.join(img_dir,image), 1)\n    # crop image with bounding box\n    img_cropped = img_array[y:height,x:width] \n    # resize cropped image\n    img = cv2.resize(img_cropped,(img_size,img_size))\n    data.append([img,train.iloc[i]['target']])     ","719f3870":"# Pick a random index from data\ni = np.random.choice(range(len(data)))\nplt.imshow(data[i][0]);","4448ff90":"X=[]\nY=[]\nfor features, labels in data:\n    X.append(features)\n    Y.append(labels)\nX = np.array(X)\/255 \nprint('Shape of X:', X.shape)\nY = np.array(Y)\nprint('Shape of Y:', Y.shape)","ebe5fcd4":"# build the model\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size=5, activation=\"relu\", padding='same', input_shape=(128,128,3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding='same'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding='same'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n \nmodel.add(Flatten())\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","984b54c8":"from sklearn.model_selection import train_test_split\n# split our data into train and validation sets\nX_train,X_val,y_train,y_val = train_test_split(X, Y,train_size=0.8,random_state=0)","d0053f22":"model.compile(optimizer=tf.keras.optimizers.Adam(epsilon=0.01),\n              loss='binary_crossentropy', metrics=['accuracy']) \n# fit the model\nhistory = model.fit(X_train,y_train,batch_size=32,\n                 epochs=40,\n                 validation_data=(X_val, y_val))","c6bc346d":"history_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nplt.title('Training Loss vs Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\n\nhistory_frame.loc[:, ['accuracy', 'val_accuracy']].plot();\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')","8d6c84f4":"pip install mtcnn","f7f37872":"def draw_facebox(image, boxes):\n    # plot the image\n    plt.imshow(image)\n    # get the context for drawing boxes\n    ax = plt.gca()\n    # plot each box\n    for box in boxes:\n    # get coordinates\n        x, y, width, height = box[0], box[1], box[2], box[3],\n        # create the shape\n        rect = plt.Rectangle((x, y), width, height, \n                             fill=False, color='r', linewidth=1)\n        # draw the box\n        ax.add_patch(rect)\n    # show the plot\n    plt.show()","4c35df2b":"from mtcnn.mtcnn import MTCNN\ndetector = MTCNN()\n\ni = np.random.choice(len(test_images))\n# load image from file\nimage = plt.imread(os.path.join(img_dir, df.name[i]))\n# detect faces in the image\nfaces = detector.detect_faces(image)\nboxes = [face['box'] for face in faces if face['confidence']>0.99]\nprint(boxes)\n# display faces on the original image\ndraw_facebox(image, boxes)","2fe74835":"# create lists which would contain filenames of images and bounding boxes\nnames = []\nbboxes = []\nfor img_name in test_images:\n    # load image from file\n    image = plt.imread(os.path.join(img_dir, img_name))\n    # detect faces in the image\n    faces = detector.detect_faces(image)\n    for face in faces: \n        if face['confidence']>0.99:\n            names.append(img_name)\n            bboxes.append(face['box'])\n\ndf = pd.DataFrame({'name' : names, 'bounding_box' : bboxes})\ndf.head()","a971124f":"data=[]\nfor i in range(len(df)):\n    # replace any negative value with zero\n    x,y,width,height = [0 if value < 0 else value for value in df.iloc[i]['bounding_box']]\n    image = df.iloc[i]['name']\n    # read image with green channel\n    img_array = cv2.imread(os.path.join(img_dir,image), 1)\n    # crop image with bounding box\n    img_cropped = img_array[y:y+height,x:x+width]  \n    # resize cropped image\n    img = cv2.resize(img_cropped,(img_size,img_size))\n    data.append(img)","31e6d55b":"X = np.array(data)\/255\npredict = model.predict(X)\ndf['classname'] = ['face_with_mask' if i > 0.8 else 'face_with_no_mask' for i in predict]\ndf","ce56190a":"plt.bar(['face_with_mask','face_no_mask'], df.classname.value_counts(), color = ['lightblue','pink']);\nplt.title('The ratio of people who wear a mask to those who do not');","4242fcfe":"# choose 9 random indixes from df.index\nindexes = np.random.choice(df.index, 9)\n\nfig, axes = plt.subplots(nrows=3, ncols=3) \ni = 0\nfig.set_figheight(15)\nfig.set_figwidth(15)\nfor row in axes:\n    for col in row:\n        # plot the cropped image\n        col.imshow(data[indexes[i]])\n        col.set_title(df.iloc[indexes[i]]['classname'],fontsize=16,fontweight='bold' )\n        i+=1\nplt.show()","0d65d48d":"# save the result to csv\ndf.to_csv('submission_1.csv')","54bb1feb":"# Predict classes","bfeaf388":"**Plot a random image from with bounding box(es)**","61f89d4a":"# Preprocessing test images","2c5ba7a1":"# Create training data","2515f43b":"# Create new datasets","202d81e6":"# Build and fit the model","26821550":"**Sample image from test set**","9657f4ec":"**Plot a random cropped image from data**","bf7e37a9":"**I used MTCNN library to easily create face's bounding boxes though sometimes this tool doesn't identify faces with masks.**","ff851f4b":"## Importing Libraries","473ef471":"# Loading datasets","8ebad26f":"**Sample image from train set**","757d3cb9":"# Face mask detection\n\nHere I am going to detect whether a person is wearing mask or not. I am focussing on only two classes that are 'face_with_mask' and face_no_mask'."}}