{"cell_type":{"932e1b8a":"code","78b2bd18":"code","d12144f7":"code","8fc41e3d":"code","7b943483":"code","993d3323":"code","6d8c1ef7":"code","c5339dbf":"code","e1cb360a":"code","ef7b6f9f":"code","044d650d":"code","85a6dbcf":"code","d441cc02":"code","759f7e7f":"code","9314529f":"code","5dcac344":"code","d397c3d2":"code","0dcd00da":"code","c41c9fcf":"code","6849d987":"code","582754c9":"code","9e025431":"code","ee877019":"code","381aaf12":"code","15bcbecf":"code","53a29fed":"code","a16f9e03":"code","5f4787dc":"code","3a740565":"code","298d83d6":"code","67a7766c":"code","31360db0":"code","4e59ad86":"code","3c4916ce":"code","ff335bf3":"code","4614f50c":"code","d624ab6e":"code","83022beb":"code","af70ee6b":"code","acdf2837":"code","4286bd3b":"code","d3e19d13":"code","c3d7b4bf":"code","676e493f":"code","c37400b4":"code","a75deb88":"code","3e3c595f":"code","794caf0e":"code","f6a296f5":"code","cb86fc20":"code","89582910":"code","91773004":"code","3723793c":"code","5d032d4b":"code","987c2acd":"code","4e004faf":"code","253b3508":"code","1875ed4b":"code","ffa1427a":"code","e18b96ff":"code","2b80fef8":"markdown","8dfc35aa":"markdown","02133479":"markdown","36f5c9ea":"markdown","e0808e59":"markdown","b8e799b3":"markdown","957934aa":"markdown","0c114866":"markdown","d779382f":"markdown","48a5d7dd":"markdown","bf5ea547":"markdown","cbf280e3":"markdown","1346dedd":"markdown","c8d1931d":"markdown","ba18d849":"markdown","4ec59a32":"markdown","650f702b":"markdown","d163af2b":"markdown","40500207":"markdown","2befa166":"markdown","a6b33021":"markdown","a2e9cf34":"markdown","c3a02552":"markdown","fee52716":"markdown","d2fedb33":"markdown","2cb48948":"markdown","5b631da5":"markdown","cd123570":"markdown","763feff4":"markdown","df2279e3":"markdown","8dcf8829":"markdown","27751f88":"markdown","78c7abcd":"markdown","68985965":"markdown","95c70d56":"markdown","815dca94":"markdown","1bbe2055":"markdown","7377b86f":"markdown","c2e0613a":"markdown","8811f3b3":"markdown","27774939":"markdown","a3335d3c":"markdown","220e73b2":"markdown","30caebb8":"markdown","963a1134":"markdown","3fbfb018":"markdown","064b62f0":"markdown","a7c3d7b7":"markdown"},"source":{"932e1b8a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","78b2bd18":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.special import boxcox1p\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.feature_selection import RFE\nfrom sklearn.neural_network import MLPRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") # ignoring annoying warnings\n\nfrom pandasql import sqldf\npysqldf = lambda q: sqldf(q, globals())\n\nimport pickle # for saving the model\n\n","d12144f7":"features = pd.read_csv('..\/input\/dataset\/features.csv')\ntrain = pd.read_csv('..\/input\/dataset\/train.csv')\nstores = pd.read_csv('..\/input\/dataset\/stores.csv')\ntest = pd.read_csv('..\/input\/dataset\/test.csv')\nsample_submission = pd.read_csv('..\/input\/dataset\/sampleSubmission.csv')","8fc41e3d":"feat_sto = features.merge(stores, how='inner', on='Store')","7b943483":"feat_sto.head()","993d3323":"pd.DataFrame(feat_sto.dtypes, columns=['Type'])","6d8c1ef7":"pd.DataFrame({'Type_Train': train.dtypes, 'Type_Test': test.dtypes})","c5339dbf":"feat_sto.Date = pd.to_datetime(feat_sto.Date)\ntrain.Date = pd.to_datetime(train.Date)\ntest.Date = pd.to_datetime(test.Date)","e1cb360a":"feat_sto['Week'] = feat_sto.Date.dt.week \nfeat_sto['Year'] = feat_sto.Date.dt.year","ef7b6f9f":"train_detail = train.merge(feat_sto, \n                           how='inner',\n                           on=['Store','Date','IsHoliday']).sort_values(by=['Store','Dept','Date']).reset_index(drop=True)","044d650d":"train_detail.head()","85a6dbcf":"test_detail = test.merge(feat_sto, \n                           how='inner',\n                           on=['Store','Date','IsHoliday']).sort_values(by=['Store',\n                                                                            'Dept',\n                                                                            'Date']).reset_index(drop=True)\n","d441cc02":"del features, train, stores, test","759f7e7f":"null_columns = (train_detail.isnull().sum(axis = 0)\/len(train_detail)).sort_values(ascending=False).index\nnull_data = pd.concat([\n    train_detail.isnull().sum(axis = 0),\n    (train_detail.isnull().sum(axis = 0)\/len(train_detail)).sort_values(ascending=False),\n    train_detail.loc[:, train_detail.columns.isin(list(null_columns))].dtypes], axis=1)\nnull_data = null_data.rename(columns={0: '# null', \n                                      1: '% null', \n                                      2: 'type'}).sort_values(ascending=False, by = '% null')\nnull_data = null_data[null_data[\"# null\"]!=0]\nnull_data","9314529f":"pysqldf(\"\"\"\nSELECT\n    T.*,\n    case\n        when ROW_NUMBER() OVER(partition by Year order by week) = 1 then 'Super Bowl'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 2 then 'Labor Day'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 3 then 'Thanksgiving'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 4 then 'Christmas'\n    end as Holyday,\n    case\n        when ROW_NUMBER() OVER(partition by Year order by week) = 1 then 'Sunday'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 2 then 'Monday'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 3 then 'Thursday'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 4 and Year = 2010 then 'Saturday'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 4 and Year = 2011 then 'Sunday'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 4 and Year = 2012 then 'Tuesday'\n    end as Day\n    from(\n        SELECT DISTINCT\n            Year,\n            Week,\n            case \n                when Date <= '2012-11-01' then 'Train Data' else 'Test Data' \n            end as Data_type\n        FROM feat_sto\n        WHERE IsHoliday = True) as T\"\"\")","5dcac344":"weekly_sales_2010 = train_detail[train_detail.Year==2010]['Weekly_Sales'].groupby(train_detail['Week']).mean()\nweekly_sales_2011 = train_detail[train_detail.Year==2011]['Weekly_Sales'].groupby(train_detail['Week']).mean()\nweekly_sales_2012 = train_detail[train_detail.Year==2012]['Weekly_Sales'].groupby(train_detail['Week']).mean()\nplt.figure(figsize=(20,10))\nsns.lineplot(weekly_sales_2010.index, weekly_sales_2010.values)\nsns.lineplot(weekly_sales_2011.index, weekly_sales_2011.values)\nsns.lineplot(weekly_sales_2012.index, weekly_sales_2012.values)\nplt.grid()\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\nplt.title('Average Weekly Sales - Per Year', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Week', fontsize=16)\nplt.show()","d397c3d2":"train_detail.loc[(train_detail.Year==2010) & (train_detail.Week==13), 'IsHoliday'] = True\ntrain_detail.loc[(train_detail.Year==2011) & (train_detail.Week==16), 'IsHoliday'] = True\ntrain_detail.loc[(train_detail.Year==2012) & (train_detail.Week==14), 'IsHoliday'] = True\ntest_detail.loc[(test_detail.Year==2013) & (test_detail.Week==13), 'IsHoliday'] = True","0dcd00da":"weekly_sales_mean = train_detail['Weekly_Sales'].groupby(train_detail['Date']).mean()\nweekly_sales_median = train_detail['Weekly_Sales'].groupby(train_detail['Date']).median()\n\nplt.figure(figsize=(20,8))\nsns.lineplot(weekly_sales_mean.index, weekly_sales_mean.values)\nsns.lineplot(weekly_sales_median.index, weekly_sales_median.values)\n\nplt.grid()\nplt.legend(['Mean', 'Median'], loc='best', fontsize=16)\nplt.title('Weekly Sales - Mean and Median', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Date', fontsize=16)\nplt.show()","c41c9fcf":"weekly_sales = train_detail['Weekly_Sales'].groupby(train_detail['Store']).mean()\nplt.figure(figsize=(20,10))\nsns.barplot(weekly_sales.index, weekly_sales.values, palette='dark')\nplt.grid()\nplt.title('Average Sales - per Store', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Store', fontsize=16)\nplt.show()\n","6849d987":"weekly_sales = train_detail['Weekly_Sales'].groupby(train_detail['Dept']).mean()\nplt.figure(figsize=(20,10))\nsns.barplot(weekly_sales.index, weekly_sales.values, palette='dark')\nplt.grid()\nplt.title('Average Sales - per Dept', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Dept', fontsize=16)\nplt.show()","582754c9":"sns.set(style=\"white\")\n\ncorr = train_detail.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nf, ax = plt.subplots(figsize=(20, 20))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.title('Correlation Matrix', fontsize=18)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.show()","9e025431":"train_detail = train_detail.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])\ntest_detail = test_detail.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])","ee877019":"def boxplot(feature):\n    fig = plt.figure(figsize=(20,8))\n    gs = GridSpec(1,2)\n    sns.boxplot(y=train_detail.Weekly_Sales, x=train_detail[feature], ax=fig.add_subplot(gs[0,0]))\n    plt.ylabel('Sales', fontsize=16)\n    plt.xlabel(feature, fontsize=16)\n    sns.stripplot(y=train_detail.Weekly_Sales, x=train_detail[feature], ax=fig.add_subplot(gs[0,1]))\n    plt.ylabel('Sales', fontsize=16)\n    plt.xlabel(feature, fontsize=16)\n    fig.show()","381aaf12":"def boxcox(feature):\n    \n    fig = plt.figure(figsize=(18,15))\n    gs = GridSpec(2,2)\n    \n    j = sns.scatterplot(y=train_detail['Weekly_Sales'], \n                        x=boxcox1p(train_detail[feature], 0.15), ax=fig.add_subplot(gs[0,1]), palette = 'blue')\n\n    plt.title('BoxCox 0.15\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(boxcox1p(train_detail[feature], 0.15)),2)) +\n              ', Skew: ' + str(np.round(stats.skew(boxcox1p(train_detail[feature], 0.15), nan_policy='omit'),2)))\n    \n    j = sns.scatterplot(y=train_detail['Weekly_Sales'], \n                        x=boxcox1p(train_detail[feature], 0.25), ax=fig.add_subplot(gs[1,0]), palette = 'blue')\n\n    plt.title('BoxCox 0.25\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(boxcox1p(train_detail[feature], 0.25)),2)) +\n              ', Skew: ' + str(np.round(stats.skew(boxcox1p(train_detail[feature], 0.25), nan_policy='omit'),2)))\n    \n    j = sns.distplot(train_detail[feature], ax=fig.add_subplot(gs[1,1]), color = 'green')\n\n    plt.title('Distribution\\n')\n    \n    j = sns.scatterplot(y=train_detail['Weekly_Sales'], \n                        x=train_detail[feature], ax=fig.add_subplot(gs[0,0]), color = 'red')\n\n    plt.title('Linear\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(train_detail[feature]),2)) + ', Skew: ' + \n               str(np.round(stats.skew(train_detail[feature], nan_policy='omit'),2)))\n    \n    fig.show()","15bcbecf":"boxplot('IsHoliday')","53a29fed":"boxplot('Type')","a16f9e03":"train_detail.Type = train_detail.Type.apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))\ntest_detail.Type = test_detail.Type.apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))","5f4787dc":"boxcox('Temperature')","3a740565":"train_detail = train_detail.drop(columns=['Temperature'])\ntest_detail = test_detail.drop(columns=['Temperature'])","298d83d6":"boxcox('Unemployment')","67a7766c":"train_detail = train_detail.drop(columns=['Unemployment'])\ntest_detail = test_detail.drop(columns=['Unemployment'])","31360db0":"boxcox('CPI')","4e59ad86":"train_detail = train_detail.drop(columns=['CPI'])\ntest_detail = test_detail.drop(columns=['CPI'])","3c4916ce":"boxcox('Size')","ff335bf3":"def WMAE(dataset, real, predicted):\n    weights = dataset.IsHoliday.apply(lambda x: 5 if x else 1)\n    return np.round(np.sum(weights*abs(real-predicted))\/(np.sum(weights)), 2)","4614f50c":"train_detail","d624ab6e":"test_detail","83022beb":"def knn():\n    knn = KNeighborsRegressor(n_neighbors=10)\n    return knn\n\ndef extraTreesRegressor():\n    clf = ExtraTreesRegressor(n_estimators=60, max_features=3, verbose=1, n_jobs=1)\n    return clf\n    \ndef randomForestRegressor():\n    clf = RandomForestRegressor(n_estimators = 100, max_features = 'log2', verbose = 1, bootstrap = True)\n    return clf\n\ndef linear_reg():\n    regr = LinearRegression()\n    return regr\n\ndef predict_(m, test_x):\n    return pd.Series(m.predict(test_x))\n\ndef model_():\n#     return knn()\n#     return extraTreesRegressor()\n    return randomForestRegressor()   \n#     return linear_reg()\n\ndef train_(train_x, train_y):\n    m = model_()\n    m.fit(train_x, train_y)\n    return m\n\ndef train_and_predict(train_x, train_y, test_x):\n    m = train_(train_x, train_y)\n    return predict_(m, test_x), m","af70ee6b":"kf = KFold(n_splits=5)\nsplited = []\n# dataset2 = dataset.copy()\nfor name, group in train_detail.groupby([\"Store\", \"Dept\"]):\n    group = group.reset_index(drop=True)\n    trains_x = []\n    trains_y = []\n    tests_x = []\n    tests_y = []\n    if group.shape[0] <= 5:\n        f = np.array(range(5))\n        np.random.shuffle(f)\n        group['fold'] = f[:group.shape[0]]\n        continue\n    fold = 0\n    for train_index, test_index in kf.split(group):\n        group.loc[test_index, 'fold'] = fold\n        fold += 1\n    splited.append(group)\n\nsplited = pd.concat(splited).reset_index(drop=True)\n","acdf2837":"splited","4286bd3b":"best_model = None\nerror_cv = 0\nbest_error = np.iinfo(np.int32).max\nfor fold in range(5):\n    train_detail2 = splited.loc[splited['fold'] != fold]\n    test_detail2 = splited.loc[splited['fold'] == fold]\n    train_y = train_detail2['Weekly_Sales']\n    train_x = train_detail2[['Store','Dept','IsHoliday','Size','Week','Type','Year']]\n    test_y = test_detail2['Weekly_Sales']\n    test_x = test_detail2[['Store', 'Dept', 'IsHoliday', 'Size', 'Week', 'Type', 'Year']]\n    print(train_detail2.shape, test_detail2.shape)\n    predicted, model = train_and_predict(train_x, train_y, test_x)\n#     weights = test_x['isHoliday'].replace(True, 5).replace(False, 1)\n#     error = calculate_error(test_y, predicted, weights)\n    error = WMAE(test_x, test_y, predicted)\n    print(error)\n    error_cv += error\n    print(fold, error)\n    if error < best_error:\n        print('Find best model')\n        best_error = error\n        best_model = model\nerror_cv \/= 5","d3e19d13":"error_cv # Average Error","c3d7b4bf":"best_error #least error","676e493f":"def random_forest(n_estimators, max_depth):\n    result = []\n    for estimator in n_estimators:\n        for depth in max_depth:\n            wmaes_cv = []\n            for i in range(1,5):\n                print('k:', i, ', n_estimators:', estimator, ', max_depth:', depth)\n                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n                RF = RandomForestRegressor(n_estimators=estimator, max_depth=depth)\n                RF.fit(x_train, y_train)\n                predicted = RF.predict(x_test)\n                wmaes_cv.append(WMAE(x_test, y_test, predicted))\n            print('WMAE:', np.mean(wmaes_cv))\n            result.append({'Max_Depth': depth, 'Estimators': estimator, 'WMAE': np.mean(wmaes_cv)})\n    return pd.DataFrame(result)","c37400b4":"def random_forest_II(n_estimators, max_depth, max_features):\n    result = []\n    for feature in max_features:\n        wmaes_cv = []\n        for i in range(1,5):\n            print('k:', i, ', max_features:', feature)\n            x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n            RF = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=feature)\n            RF.fit(x_train, y_train)\n            predicted = RF.predict(x_test)\n            wmaes_cv.append(WMAE(x_test, y_test, predicted))\n        print('WMAE:', np.mean(wmaes_cv))\n        result.append({'Max_Feature': feature, 'WMAE': np.mean(wmaes_cv)})\n    return pd.DataFrame(result)","a75deb88":"def random_forest_III(n_estimators, max_depth, max_features, min_samples_split, min_samples_leaf):\n    result = []\n    for split in min_samples_split:\n        for leaf in min_samples_leaf:\n            wmaes_cv = []\n            for i in range(1,5):\n                print('k:', i, ', min_samples_split:', split, ', min_samples_leaf:', leaf)\n                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n                RF = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, \n                                           min_samples_leaf=leaf, min_samples_split=split)\n                RF.fit(x_train, y_train)\n                predicted = RF.predict(x_test)\n                wmaes_cv.append(WMAE(x_test, y_test, predicted))\n            print('WMAE:', np.mean(wmaes_cv))\n            result.append({'Min_Samples_Leaf': leaf, 'Min_Samples_Split': split, 'WMAE': np.mean(wmaes_cv)})\n    return pd.DataFrame(result)","3e3c595f":"X_train = train_detail[['Store','Dept','IsHoliday','Size','Week','Type','Year']]\nY_train = train_detail['Weekly_Sales']","794caf0e":"n_estimators = [56, 58, 60]\nmax_depth = [25, 27, 30]\n\nrandom_forest(n_estimators, max_depth)","f6a296f5":"max_features = [2, 3, 4, 5, 6, 7]\n\nrandom_forest_II(n_estimators=56, max_depth=30, max_features=max_features)","cb86fc20":"min_samples_split = [2, 3, 4]\nmin_samples_leaf = [1, 2, 3]\n\nrandom_forest_III(n_estimators=56, max_depth=30, max_features=7, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)","89582910":"RF = RandomForestRegressor(n_estimators=56, max_depth=30, max_features=7, min_samples_split=3, min_samples_leaf=1)\nRF.fit(X_train, Y_train)","91773004":"model_save_file = open(\"SavedModel.sav\", 'wb')\npickle.dump(RF, model_save_file)\nmodel_save_file.close()","3723793c":"load_saved_model_file = open('SavedModel.sav', 'rb')\nloaded_model = pickle.load(load_saved_model_file)\nload_saved_model_file.close()","5d032d4b":"X_test = test_detail[['Store', 'Dept', 'IsHoliday', 'Size', 'Week', 'Type', 'Year']]\nloaded_predict = loaded_model.predict(X_test)","987c2acd":"X_test = test_detail[['Store', 'Dept', 'IsHoliday', 'Size', 'Week', 'Type', 'Year']]\npredict = RF.predict(X_test)","4e004faf":"print(np.all(loaded_predict==predict)) # to show that saved model is loading perfectly by comparing the predictions\n# True if all elements are equal, False otherwise.","253b3508":"sample_submission['Weekly_Sales'] = loaded_predict","1875ed4b":"sample_submission","ffa1427a":"sample_submission.to_csv('Submission.csv', index=False)","e18b96ff":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"submission.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe\ncreate_download_link(sample_submission)","2b80fef8":"# Error(WMAE) on Walmart's testing data (hidden)\n# 2776.28705","8dfc35aa":"The correlations doesn't seem to change at any skewness toh we will drop this feature.","02133479":"### Tuning max_features","36f5c9ea":"Correlation Metrics:\n\n* 0: no correlation at all\n* 0-0.3: weak correlation\n* 0.3-0.7: moderate correlaton\n* 0.7-1: strong correlation\n\n**Positive Correlation indicates that when one variable increase, the other also does. Negative is the opposite.**","e0808e59":"# Weekly Sales vs Unemployement","b8e799b3":"### Observation 4\n'**MarkDown**' 1 to 5 are not strong correlated to '**Weekly_Sales**' and they have a lot of null values, then we can drop them.\n\nAlso, '**Fuel_Price**' is strong correlated to '**Year**'. One of them must be dropped else they would carry similar information to the model. 'Year' will not be dropped, because it differentiate same Weeks for 'Store'+'Dept'.\n\nOther variables that have weak correlation with '**Weekly_Sales**' can be analyzed to see if they are useful.","957934aa":"We don't know what 'Type' is, but we can assume that A > B > C in terms of Sales Median. So, let's treat it as an ordinal variable and replace its values.","0c114866":"# Weighted Mean Average Error Function","d779382f":"Min_Samples_Leaf - 1 \\\nMin_Samples_Split - 3","48a5d7dd":"# Training Model","bf5ea547":"# Average Weekly Sales - Per Year","cbf280e3":"### **Adding Easter**","1346dedd":"# Correlation Matrix - Pearson Correlation","c8d1931d":"# Saving the Model","ba18d849":"### Errors(WMAE) for Different Models:\n   * knn - 3789.4\n   * Extra Tree - 4024.48\n   * Random Forest - 3672\n   * Linear Regression - 3332.86","4ec59a32":"# Average Sales per Store and Department","650f702b":"Max_Feature - 7","d163af2b":"Tuning 'n_estimators' and 'max_depth'.\n\nHere, it is possible to test a lot of values. These are the final ones, after a bit of testing.","40500207":"# Algorithms (Without thinking about efficency)\n* After trying different algorithms we concluded that we would get the best results by choosing Random Forest and optimizing","2befa166":"# Tuning","a6b33021":"## Predicting on the loaded model","a2e9cf34":"### Tuning n_estimators and max_depth","c3a02552":"## Observation 2\nAs we can see, there is one important Holiday not included in 'IsHoliday'. It's the Easter Day. It is always in a Sunday, but can fall on different weeks.\n\nIn 2010 is in Week 13\nIn 2011, Week 16\nWeek 14 in 2012\nand, finally, Week 13 in 2013 for Test set\nSo, we can change to 'True' these Weeks in each Year.","fee52716":"### Droping Temperature","d2fedb33":"# Random Forest (Split into 3 parts for optimizing)","2cb48948":"# Weekly Sales vs Size","5b631da5":"Same for **CPI**","cd123570":"# Analysing Variables\n1. BoxPlot and StripPlot for Discrete Variables\n2. BoxCox for Continous Variables","763feff4":"# 2. Exploratory Analysis and Data Cleaning\n* Merging \"Features\" and \"Stores\"\n* ","df2279e3":"# Weekly Sales vs CPI","8dcf8829":"We will continue with this variable, since it has moderate correlation with **WeeklySales**.","27751f88":"# Predictions","78c7abcd":"# Holidays Analysis\n\nIf, for a certain Week, there are more pre-holiday days in one Year than another, then it is very possible that the Year with more pre-holiday days will have greater Sales for the same Week. So, the model will not take this consideration and we might need to adjust the predicted values at the end.\n\nAnother thing to take into account is that Holiday Weeks but with few or no pre-holiday days might have lower Sales than the Week before.\n\n","68985965":"# Weekly Sales - Mean and Median","95c70d56":"### Tuning min_samples_spilt and min_samples_leaf","815dca94":"**Droping Markdown values**","1bbe2055":"# Weekly Sales vs Temperature (Continuous)","7377b86f":"# Weekly Sales vs IsHoliday","c2e0613a":"Project Phases:\n* Libraries and Data Loading\n* Exploratory Analysis and Data Cleaning\n* Machine Learning\n* Christmas Adjustment","8811f3b3":"## **Observation 1**\n\n* All Holidays fall on the same week\n* Christmas has 0 pre-holiday days in 2010, 1 in 2011 and 3 in 2012. The model will not consider more Sales in 2012 for Test Data, so we are going to adjust it at the end, with a formula and an explanation.","27774939":"# 1. Libraries and Data Loading \n\nWe will be loading some libraries to make processing easier and some to view the data\nlibraries used :\n* Pandas\n* Numpy\n* MatPlotLib\n* Seaborn\n* SciPy\n* SkLearn\n* PandaSQL\n* Warning (ignore any warning we might come across)","a3335d3c":"## Obeservation 3\nThere are Sales difference between the Store and Departments. Also some Depts are not in the list, like number '15', for example.","220e73b2":"# Average Sales - Department Wise","30caebb8":"# K-Fold Cross Validation","963a1134":"Same for **Unemployment** rate.\n\n","3fbfb018":"The result based on WAME \\\nMax_Depth - 30 \\\nEstimators - 56","064b62f0":"# Loading the Model","a7c3d7b7":"# Final Fitting"}}