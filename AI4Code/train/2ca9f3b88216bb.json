{"cell_type":{"baab6993":"code","e0384c51":"code","82c799a4":"code","636e4503":"code","15bd9933":"code","e62254df":"code","87f6149e":"code","9081955c":"code","58ed2044":"code","207efe06":"code","3840b4ab":"code","e92bc202":"code","5168b5a4":"code","428e5d1e":"code","c54d0220":"code","4b26590a":"code","c40a64f1":"code","06ac2f4d":"code","94f17e6f":"markdown","b125bd3e":"markdown"},"source":{"baab6993":"# =======================================================\n# TPS Nov 2021 - EDA\n# =======================================================\n# Name: B\u00e1rbara Sulpis\n# Date: 1-nov-2021\n# Description: I will analyze TPS data to have an idea of following steps...\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy.stats as st # statistical functions\nimport os\n\nfrom sklearn.model_selection import train_test_split\n\n#Lgbm\nimport lightgbm as lgb\n\n# roc\nimport sklearn.metrics as metrics   # Para la curva ROC\nimport matplotlib.pyplot as plt     # Para la curva ROC\n\n# for hystograms\nimport seaborn as sns\n\n\n# ---------------------------\n# Input data:\n# Go to file -> add or upload data -> \"Competition\" data tab and select the commpetition which you want to add the csv data data \"\n# files are available in the read-only \"..\/input\/\" directory\n# ---------------------------\n\nlist =  os. getcwd()\nprint(list) # shoud be in \"kaggle\" directory\n\n# I left this commented if you want to check that the files are there\n# i = 0\n# for subdir, dirs, files in os.walk('.\/'):\n#     for file in files:\n#         print(file)\n#         i+= 1\n#         if i>20: \n#             break\n\n\ndata = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")        \nsubm = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")  \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0384c51":"# Size of the dataset\ndata.shape","82c799a4":"# With this setting we can see all rows of the dataset\npd.set_option(\"display.max_columns\", 300)\n# We have a look to the data\ndata","636e4503":"# Before working with the data, we reduce the use of memory, so we can improve performance\n# REFERENCE: https:\/\/www.kaggle.com\/smiles28\/tps10-optuna-xgb-catb-lgbm-stacking\n\n# What the function does is to deduce the data types and cast each column to its most performant type\n\ndef reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n    \n            # test if column can be converted to an integer\n            asint = props[col].astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return props\n","15bd9933":"data = reduce_mem_usage(data)","e62254df":"subm = reduce_mem_usage(subm)","87f6149e":"# ------------------------------------------------------------\n#   Search for MISSING values\n# ------------------------------------------------------------\n# First we make a dataframe with the number of not-null values for each column\ncount = pd.DataFrame(data.count(), columns=['count'])\n\n# Then we get the fields that has a number smaller than 600k (the number of rows in train set)\ncount.query(\"count < 600000\")\n\n# As we can see there are not null values in the dataset. ","9081955c":"subm.shape","58ed2044":"# We can make the same check for the submission dataset (\"test.csv dataset\")\ncount = pd.DataFrame(subm.count(), columns=['count'])\ncount.query(\"count < 540000\")\n\n# As expected, there are not null values in test dataset neither.","207efe06":"# ------------------------------------------------------------\n#   Variable CORRELATION\n# ------------------------------------------------------------\n# Correlation matrix\n# --------------------\n# We make a correlation matrix to check if there are relations between the different fields.\ncorrmat = data.corr()","3840b4ab":"# Let's draw the corrmat\nf, ax = plt.subplots(figsize =(40, 40))\nsns.heatmap(corrmat, ax = ax, cmap =\"YlGnBu\", linewidths = 0.1)\n# Explanation of the graph: The blue diagonal \"line\" represents a 100% of correlation between each feature and itself\n#   the other points, as the right vertical correlation rule indicates, seems not to have correlation with other features except of itself. ","e92bc202":"# Distribution of the target:\ndata.groupby('target').count()\n# 296394\n# 303606\n# The data is quite perfectly balanced","5168b5a4":"# ------------------------------------------------------------\n#   Variable DISTRIBUTIONS\n# ------------------------------------------------------------\n# I will draw the hystograms for all variables\ndata.hist(grid = False, figsize=(25,80), layout=(29, 10), bins=50)","428e5d1e":"# ------------------------------------------------------------------------------\n#  CARDINALITY OF VARIABLES\n# ------------------------------------------------------------------------------\n# After watching the output we can appreciate that there are many features that seems to have few different values\n# So, let's see theyr cardinality\npd.set_option(\"display.max_rows\", 300)\n\ndata.nunique()","c54d0220":"# What we can see below is that there are NO variables with few different values. (low cardinality)\n","4b26590a":"# We can find handy this other histogram plot, that makes two plots overlapped\n# Superposition of the two graphs: target==1 and target==0\n# We will only plot the first 5 features\n\n# REFERENCE: https:\/\/stackoverflow.com\/questions\/37911731\/seaborn-histogram-with-4-panels-2-x-2-in-python\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndata_hist = pd.melt(data[['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'target']], \"target\", var_name=\"target distributions\")\ng = sns.FacetGrid(data_hist, hue=\"target\", col=\"target distributions\", col_wrap=5, sharex=False, sharey=False)\ng.map(plt.hist, \"value\", bins=20, alpha=.4)\n","c40a64f1":"# ------------------------------------------------------------\n#  Checking SKEWNESS for continuous data\n# ------------------------------------------------------------\n# Last of all, the following code is to calculate the skewed data. In this example left skewed data.\n# This could be used to correct skewness with log or exponential transformations \n\ndata_skewed = pd.concat([pd.DataFrame(data.columns), pd.DataFrame(st.skew(data))], axis=1)\ndata_skewed.columns = ['names', 'skewness']\n# I only get fields that has a skewness bigger than 3\nskewed = data_skewed.query('skewness > 3')['names']\nskewed","06ac2f4d":"# ------------------------------------------------------------\n#  Best performing algorithms\n# ------------------------------------------------------------\n# As part of the EDA I can add the output of the LazyPredict (TPS Oct 2021) used in other of my notebooks.\n# REFERENCE: https:\/\/www.kaggle.com\/brbarasulpis\/tps-2021-oct-automl-lazypredict-lazyclassifier","94f17e6f":"# CONCLUSIONS\nAfter this exploratory data analisys we now know that:\n* There are no missing values in the dataset\n* The target is balanced (nearly half values in 1 and half in 0)\n* There is no correlations between the variables\n* There are no categorical features\n* Many continuous features are left skewed\n* Many continuous features seems to have a compound distribution","b125bd3e":"# Exploratory Data Analisys for Tabular Playground Series (Nov 2021)"}}