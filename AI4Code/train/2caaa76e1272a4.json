{"cell_type":{"d053be3c":"code","0ec1bb82":"code","117b6b9e":"code","94048960":"code","2d5b68e9":"code","fac2ebfa":"code","bb8776fb":"code","62a409b2":"code","57fadf2a":"code","8eda1cb1":"code","c22db770":"code","2f97c811":"code","6ada9de4":"code","878ff0cb":"code","579fd608":"code","7b1a5ad0":"code","a265a371":"code","6c481c89":"code","9caccd07":"markdown","cc988596":"markdown","29ff418c":"markdown","1f3d50cb":"markdown","14b72399":"markdown","d68fad54":"markdown","4d55393a":"markdown","e1187613":"markdown","6b3a746d":"markdown","95d5a70f":"markdown","119f9917":"markdown","abefb331":"markdown","f40421bb":"markdown","29ecb040":"markdown","de21a519":"markdown","724966b8":"markdown","fd0e4bf6":"markdown","fc3ede54":"markdown","640625f3":"markdown"},"source":{"d053be3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nimport sklearn.linear_model as linMod\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.decomposition import PCA\nimport sklearn.tree as tree\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom scipy import stats\n\nUP_DIR = \"\/kaggle\/input\/pakistan-house-price-prediction\"\n\n# Open data\ncsvPath = os.path.join(UP_DIR, 'Entities.csv')\nhousesRaw = pd.read_csv(csvPath)","0ec1bb82":"# Copy and filter housesRaw and print\nhousesClean = housesRaw[['property_type', 'purpose', 'bedrooms',\n                        'baths', 'Total_Area', 'city', 'province_name',\n                         'latitude', 'longitude', 'price']].rename(\n    columns={'Total_Area': 'area', 'property_type': 'type', 'province_name': 'province'})\n\nhousesClean = housesClean[housesClean.baths != 0]\nhousesClean = housesClean[housesClean.bedrooms != 0]\nhousesClean = housesClean[housesClean.area != 0]\nhousesClean = housesClean[housesClean.purpose == 'For Sale']\n\nhousesClean = housesClean.drop(['purpose'], axis=1)\n\n# Remove Outliers\nhousesClean = housesClean[(\n    np.abs(stats.zscore(housesClean['area'])) < 3)]\nhousesClean = housesClean[(\n    np.abs(stats.zscore(housesClean['bedrooms'])) < 3)]\nhousesClean = housesClean[(\n    np.abs(stats.zscore(housesClean['baths'])) < 3)]\nhousesClean = housesClean[(\n    np.abs(stats.zscore(housesClean['price'])) < 3)]\n\ncolors = {'Karachi': 'red', 'Lahore': 'blue', 'Islamabad': 'green',\n          'Faisalabad': 'yellow', 'Rawalpindi': 'purple'}\n\nprint(housesClean)","117b6b9e":"sns.set(style='white', font_scale=1, color_codes=True)\n\nmaxPrice = housesClean['price'].max()\nupperY = maxPrice*1.25\n\n\npair = sns.pairplot(housesClean.sample(10000),\n                    plot_kws={'alpha': 0.1},\n                    x_vars=['type', 'bedrooms', 'baths', 'area'],\n                    y_vars=['price'],\n                    hue='city')\nfor axis in pair.fig.axes:\n    axis.set_xticklabels(axis.get_xticklabels(), rotation=45)\nplt.show()\n\npair = sns.pairplot(housesClean.sample(10000),\n                    plot_kws={'alpha': 0.1},\n                    x_vars=['city', 'province', 'latitude', 'longitude'],\n                    y_vars=['price'],\n                    hue='city')\nfor axis in pair.fig.axes:\n    axis.set_xticklabels(axis.get_xticklabels(), rotation=45)\nplt.show()","94048960":"housesClean.plot(x='bedrooms',\n                 y='price',\n                 kind='scatter',\n                 alpha=.1,\n                 c=housesClean['city'].map(colors))\nplt.xlim([0, 15])\nplt.ylim([0, upperY])\nplt.title('Bedrooms x Price')\nplt.show()","2d5b68e9":"housesClean.plot(x='baths',\n                 y='price',\n                 kind='scatter',\n                 alpha=.1,\n                 c=housesClean['city'].map(colors))\nplt.xlim([0, 15])\nplt.ylim([0, upperY])\nplt.title('Bathrooms x Price')\nplt.show()","fac2ebfa":"housesClean.plot(x='area',\n                 y='price',\n                 kind='scatter',\n                 alpha=.1,\n                 c=housesClean['city'].map(colors))\nplt.xlim([0, 100000])\nplt.ylim([0, upperY])\nsns.regplot(housesClean.area, housesClean.price, order=1,\n            ci=None, scatter=False, color='black')\nplt.title('Area x Price')\nplt.show()","bb8776fb":"housesClean.plot(x='city',\n                 y='price',\n                 kind='scatter',\n                 alpha=.1,\n                 c=housesClean['city'].map(colors))\nplt.ylim([0, upperY])\nplt.title('City x Price')\nplt.show()","62a409b2":"housesClean.plot(x='type',\n                 y='price',\n                 kind='scatter',\n                 alpha=.1,\n                 c=housesClean['city'].map(colors))\nplt.ylim([0, upperY])\nplt.title('Type x Price')\nplt.xticks(rotation=45)\nplt.show()","57fadf2a":"housesClean.plot(x='province',\n                 y='price',\n                 kind='scatter',\n                 alpha=.1,\n                 c=housesClean['city'].map(colors))\nplt.ylim([0, upperY])\nplt.title('Province x Price')\nplt.show()","8eda1cb1":"housesClean = pd.get_dummies(\n    data=housesClean, drop_first=True).rename(\n    columns={'type_Lower Portion': 'type_Lower_Portion',\n             'type_Upper Portion': 'type_Upper_Portion',\n             'purpose_For Sale': 'purpose_For_Sale'})\n\nprint(housesClean)","c22db770":"plt.figure(figsize=(15, 10))\nsns.heatmap(housesClean.corr(), annot=True)\nplt.show()","2f97c811":"X = housesClean.drop(columns='price')\ny = housesClean['price']\n\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=.2)","6ada9de4":"print(\"Multiple Linear Regression\")\nlinRegress = linMod.LinearRegression()\nlinRegress.fit(Xtrain, ytrain)\n\nprint(\"Linear R2:\", r2_score(ytest, linRegress.predict(Xtest)))\nprint(linRegress.coef_)\nprint(linRegress.intercept_)","878ff0cb":"# Principal Component Analysis\npca = PCA(n_components=3)\nXtrainPCA = pca.fit_transform(Xtrain)\nXtestPCA = pca.fit_transform(Xtest)\n\n# Polynomial Regression deg=2\nprint(\"Polynomial Regression deg=2\")\npoly = PolynomialFeatures(degree=2)\nXtrainP = poly.fit_transform(XtrainPCA)\nXtestP = poly.fit_transform(XtestPCA)\npolyRegress = linMod.LinearRegression().fit(XtrainP, ytrain)\n\nprint(\"Quad R2:\", r2_score(ytest, polyRegress.predict(XtestP)))\nprint(polyRegress.coef_)\nprint(polyRegress.intercept_)","579fd608":"KNN = KNeighborsRegressor()\nKNN.fit(Xtrain, ytrain)\n\nprint(\"K-Nearest R2:\", r2_score(ytest, KNN.predict(Xtest)))\n\n# Test different K values in K Nearest Neighbors\n\nrmse_val = []\nBestK = 0\nBestR2 = 0\n\nfor K in range(1, 20):\n    KNN = KNeighborsRegressor(n_neighbors=K)\n    KNN.fit(Xtrain, ytrain)\n    error = np.sqrt(mean_squared_error(\n        ytest, KNN.predict(Xtest)))  # calculate rmse\n    rmse_val.append(error)  # store rmse values\n\n    if r2_score(ytest, KNN.predict(Xtest)) > BestR2:  # Update best K value so far\n        BestR2 = r2_score(ytest, KNN.predict(Xtest))\n        BestK = K\n\nprint('Best K value (Minkowski) is', BestK)\nprint('Best R2 score (Minkowski) is', BestR2)\n\ncurve = pd.DataFrame(rmse_val)  # elbow curve\ncurve.plot()\nplt.show()","7b1a5ad0":"rmse_val = []\nBestK = 0\nBestR2 = 0\n\nfor K in range(1, 20):\n    KNN = KNeighborsRegressor(n_neighbors=K, metric='manhattan')\n    KNN.fit(Xtrain, ytrain)\n    error = np.sqrt(mean_squared_error(\n        ytest, KNN.predict(Xtest)))  # calculate rmse\n    rmse_val.append(error)  # store rmse values\n\n    if r2_score(ytest, KNN.predict(Xtest)) > BestR2:  # Update best K value so far\n        BestR2 = r2_score(ytest, KNN.predict(Xtest))\n        BestK = K\n\nprint('Best K value (Manhattan) is', BestK)\nprint('Best R2 score (Manhattan) is', BestR2)\n\ncurve = pd.DataFrame(rmse_val)  # elbow curve\ncurve.plot()\nplt.show()","a265a371":"treeRegress = DecisionTreeRegressor()\ntreeRegress.fit(Xtrain, ytrain)\n\nprint(\"Decision Tree R2:\", r2_score(ytest, treeRegress.predict(Xtest)))","6c481c89":"RFRegress = RandomForestRegressor()\nRFRegress.fit(Xtrain, ytrain)\n\nprint(\"Random Forest R2:\", r2_score(ytest, RFRegress.predict(Xtest)))","9caccd07":"#### Type x Price","cc988596":"#### City x Price","29ff418c":"## 2. Dataset\nThe dataset we are using was scraped by Huzefa Khan from a popular Pakistani real\nestate website, Zameen.com, and uploaded to Open Data Pakistan and Kaggle. \u201cZameen\u201d is an\nUrdu and Hindi word that translates to \u201cground\u201d or \u201cland\u201d. The dataset we are using contains\nalmost 170,000 instances of data. Originally the dataset contained 18 columns, some were\nunnecessary for our purposes. The columns in the original dataset are as follows.\n\n- INT property_id: A unique ID given to each house by Zameen.com\n- INT location_id: An ID given to each plot by Zameen.com\n- STRING page_url: Link where listing was posted\n- STRING property_type: Type of property (\u2018House\u2019, \u2018Farm House\u2019, \u2018Flat\u2019, \u2018Upper\nPortion\u2019, \u2018Lower Portion\u2019, \u2018Room\u2019, \u2018Penthouse\u2019)\n- INT price: For Sale or For Rent price in Pakistani Rupees\n- STRING location: Block\/street name\n- STRING city: City name (\u2018Karachi\u2019, \u2018Lahore\u2019, \u2018Islamabad\u2019, \u2018Rawalpindi\u2019, \u2018Faisalabad\u2019)\n- STRING province_name: Province\/state name (\u2018Sindh\u2019, \u2018Punjab\u2019, \u2018Islamabad Capital\u2019)\n- FLOAT latitude: Latitude coordinate\n- FLOAT longitude: Longitude coordinate\n- INT baths: Number of bathrooms\n- STRING purpose: Purpose of listing (\u2018For Sale\u2019, \u2018For Rent\u2019)\n- INT bedrooms: Number of bedrooms\n- STRING date_added: The listing date on Zameen.com\n- STRING agency: Company selling property\n- STRING agent: Property dealer\n- FLOAT Total_Area: Property area in square feet\n\nI dropped some of the columns I found to be unnecessary e,g \u2018page_url\u2019, \u2018agency\u2019,\n\u2018agent\u2019, \u2018date_added\u2019, \u2018location\u2019 (since there are too many street names to keep track of. Not\nenough properties share the same street name, therefore will not help us in our analysis),\n\u2018purpose\u2019 (because we will only be looking at properties that are \u2018For Sale\u2019), \u2018property_id\u2019, and\n\u2018location_id\u2019. The target column is the \u2018price\u2019 of the house given in Pakistani Rupees (PKR). The\ncleaned dataset contains the following columns.\n\n- STRING type: Type of property (\u2018House\u2019, \u2018Farm House\u2019, \u2018Flat\u2019, \u2018Upper Portion\u2019, \u2018Lower Portion\u2019, \u2018Room\u2019, \u2018Penthouse\u2019)\n- INT bedrooms: Number of bedrooms\n- INT baths: Number of bathrooms\n- FLOAT area: Property area in square feet\n- STRING city: City name (\u2018Karachi\u2019, \u2018Lahore\u2019, \u2018Islamabad\u2019, \u2018Rawalpindi\u2019, \u2018Faisalabad\u2019)\n- STRING province: Province\/state name (\u2018Sindh\u2019, \u2018Punjab\u2019, \u2018Islamabad Capital\u2019)\n- FLOAT latitude: Latitude coordinate\n- FLOAT longitude: Longitude coordinate\n- INT price: For sale or for rent price in Pakistani Rupees.\n\nMany datasets are prone to human error, including this one. Human error can be\nintroduced by Zameen.com when entering values or during the process of web scraping. The\ndataset contains entries that have 0 bedrooms, 0 baths, and 0 square feet. These entries were\ndropped since they don\u2019t make sense in our context. We also removed a few entries that were\noutliers. There were many outliers in terms of \u2018price\u2019, \u2018bedrooms\u2019, \u2018bathrooms\u2019, and \u2018area\u2019 that\nwould skew our models if not removed. We begin by exploring the overall data in the hopes of\nfinding patterns. Below we have multiple plots of various predictors compared against price. We\nwant to see if there exists any relationship between the price and any of the predictors.\n### Clean up data","1f3d50cb":"#### Random Forest Regression\nOur model can still be improved. Using Random Forest, we can create an even more\naccurate model. Random Forest, in general, performs better since this method creates multiple\nDecision Trees and then takes the majority vote from all of them. By calling\n\u201csklearn.ensemble.RandomForestRegressor()\u201d, we can create our model. Running this model\ngives us an R-squared score of 0.91. This is the highest score we have achieved out of all the\nmethods we have tried.","14b72399":"### Visualize data\n#### Pairplot (divided into 2 for easy viewing)","d68fad54":"#### Bathrooms x Price","4d55393a":"## 4. Conclusion\nThroughout this paper, we used various types of models to assess the Pakistani house\nprice dataset. From our pre-analysis steps, we found that the number of bedrooms and the\nnumber of bathrooms had a positive correlation to price. We also found that the property area\nwas positively correlated to price. After experimenting with many different models, we found\nthat Linear Regression was not well suited for this dataset. Linear models assumed the predictors\nhad a linear correlation with the target. It turns out that this was not the case. Simple Linear\nRegression achieved the lowest performance out of all the methods we tried. Quadratic\nRegression slightly increased performance, but not by much. Using K-Nearest Neighbors\nRegression, we had a significant increase in accuracy. Trying different values for k, and trying\ndifferent metrics, we conclude that k = 4 or 5 yields the best result. Lastly, we used decision trees\nto make predictions. Using a simple decision tree we again saw an increase in accuracy. Random\nForest achieved the highest accuracy out of all of our models with an R-squared score of 0.91.\nAll this being said, we now understand the main factors that determine house prices in Pakistan,\nand how we can accurately predict them.","e1187613":"#### Correlation Heat Map","6b3a746d":"### 3.1 Multiple Linear Regression\n#### Linear Regression\nWe begin by using Multiple Linear Regression to predict the prices on various properties.\nLinear Regression is simple to implement and is not as complex as other models. Linear\nRegression can be impacted by outliers, which is why we removed outliers from our dataset.\nUsing Multiple Linear Regression, \u201csklearn.linear_model.LinearRegression()\u201d, our model\nachieves an R-squared score of 0.409 with an intercept of 133230926.52 and coefficients of\n[2.65e+06, 4.87e+06 2.51e+02, -3.75e+06, -2.93e+05, -1.16e+07, -8.42e+06, -1.34e+07,\n-1.19e+07, -7.09e+06, -1.44e+07, 1.20e+07, -8.83e+06, 7.17e+06, 8.26e+06, -3.14e+06,\n-8.83e+06].\n","95d5a70f":"# An Analysis of Pakistani Homes\n## 1. Introduction\nWe will be describing machine learning methods used on a large dataset to analyze the\nhousing market in multiple cities in Pakistan. Pakistan is a very populous country, currently\nhaving the fifth-highest population out of all countries. The country has more than 220 million\npeople with a yearly growth rate of 2%. This yearly change is on the higher end. For comparison,\nthe rates of the three most populous countries, China, India, and the United States are 0.39%,\n0.99%, and 0.59% respectively. It is clear to see that the housing market in Pakistan is\nexpanding, and will continue to expand. Understanding the trends in this market, and which\nfactors cause those trends, is essential for the Pakistani Government, land developers, real estate\nagencies, and real estate investors.\n\nThe goal of my analysis is to compare the performance of different machine learning\nmodels, all predicting the prices of houses based on known factors. We will build different types\nof models and see how the results compare. We will also look at how different factors impact the\nprice of a house, and which factors, in particular, are the most impactful, and the least impactful.\nThe following machine learning methods will be tested throughout this paper: Multiple Linear\nRegression, Quadratic Regression, K-Nearest Neighbors, Decision Trees, and Random Forest.\nWe will split our data into disjoint training and testing sets in order to have accurate assessments\nand reliable results. This will allow us to fairly judge the performance of a specific model.\nWhenever possible, we will try to employ techniques to improve the quality of our model.\n### Imports","119f9917":"### 3.3 Decision Trees Methods\n#### Decision Tree Regression\nNext, we will discuss Decision Trees. Decision Trees are not as easily influenced by\noutliers as Linear Regression of K-Nearest Neighbors. Like K-Nearest Neighbors, Decision\nTrees are non-parametric and will not make assumptions about the data. First, we start with a\nstandard Decision Tree using \u201cDecisionTreeRegressor()\u201d. Without setting a \u2018max_leaf_node\u2019, we\nallow the method to run indefinitely. This can cause overfitting. With our current model, we\nachieve an R-squared score of 0.87. Even with overfitting, this model outperforms K-Nearest\nNeighbors.","abefb331":"#### Polynomial Regression\nNext, we try to make a more complex model with higher degrees of freedom in the hopes\nthat our model will fit the data better. First, I take the first three principal components of the data\nset. This is done using \u201csklearn.decomposition.PCA(n_components=3)\u201d. Then we fit a Quadratic\nRegression model to it. A Quadratic Regression model is a Polynomial Regression model with\ndegree = 2. This is done by calling \u201csklearn.preprocessing.PolynomialFeatures(degree = 2)\u201d. The\nresults are slightly better. We get an R-squared score of 0.420. Our intercept is 20266031.50 and\nour coefficients are\n[0, 4.05e+02, 5.75e+05, -5.77e+06, -1.12e-03, 1.43e+01, -1.77e+01, 5.33e+03, 2.08e+05,\n6.65e+04].","f40421bb":"### 3.2 K-Nearest Neighbors Regression Methods\n#### Minkowski metric\nNext, we use K-Nearest Neighbors. K-Nearest Neighbors is another simple model that is\neasy to implement. One benefit this model has over Linear Regression is that K-Nearest\nNeighbors is non-parametric, meaning it has no assumption about the data. Linear Regression\nassumes the predictors have a linear relationship to the target. This model is also sensitive to\noutliers, but we don\u2019t need to worry about that. We start with default settings by calling\n\u201cKNeighborsRegressor()\u201d. Our k = 5 (by default) and our distance metric is the \u201cMinkowski\ndistance\u201d (by default). Using K-nearest neighbors, we already see better results compared to\nMultiple Linear Regression. With no alterations made, we achieve an R-squared score of 0.859.","29ecb040":"#### Manhattan metric\nWe want to see if we can improve this model. Next, we will iterate over a range of k and\nuse both the \u201cMinkowski distance\u201d and the \u201cManhattan distance\u201d. I created a KNN loop that\ntests all integers from 1 to 20. We did not test any higher k values, revealing one of the flaws of\nK-Nearest Neighbors, it is very computationally expensive. For the \u201cMinkowski distance\u201d, we\nfind that a k = 4 or 5 is ideal, giving us a similar R-squared score as above. For the \u201cManhattan\ndistance\u201d we find the same result. Changing the distance metric did not impact our model. Below\nwe have elbow graphs charting k to RMSE.","de21a519":"We can see, in general, the price of a property increases as the number of bedrooms and\nbathrooms increases. After the 6th or 7th bedroom or bathroom, the price begins to plateau. Also a\nnote on the price axis: The Pakistani Rupee has undergone a lot of inflation. The value of the\nPakistani Rupee is very low compared to the United States Dollar. The relationship between the\ntwo is 1 USD = 179.46 PKR at the of writing. This explains the large scale for the price axis. We\ncan see, especially towards the lower end of the area plot, that the graph \u201csplits\u201d into two\ndirections. This shows us that area, overall, has a relationship with price, but other factors also\nhave an impact. Another thing to notice is the prices of houses in Rawalpindi and Faisalabad are\nlower than those in Lahore, Karachi, or Islamabad. This is because Lahore, Karachi, and\nIslamabad are much more industrial cities. After undergoing One Hot Encoding, we come up\nwith the following correlation heat map.\n#### One Hot Encoding","724966b8":"From the above plots, I can see there is a correlation between \u2018bathrooms\u2019 and \u2018price\u2019,\n\u2018bedrooms\u2019 and \u2018price\u2019, and between \u2018area\u2019 and \u2018price\u2019. Below are plots for multiple predictors against 'price'. The plot for \u2018area\u2019 compared to\n\u2018price\u2019 is given with a regression line to give us an idea of the trend.\n#### Bedrooms x Price","fd0e4bf6":"#### Area x Price","fc3ede54":"#### Province x Price","640625f3":"## 3. Analysis\nMany different models can be used in regression situations. We start\nby splitting our data into disjoint training and testing sets. This is to ensure we are getting good\nresults. I reserve 20% of the data for testing.\n#### Train Test Split"}}