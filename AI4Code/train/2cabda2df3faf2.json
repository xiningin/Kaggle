{"cell_type":{"6ff7e649":"code","3244f883":"code","765dd556":"code","bbf34a77":"code","4d3185e3":"code","a62b0a77":"code","ad2c5cc5":"code","4f1b8272":"code","5c037ba8":"code","1413377f":"code","f763029a":"code","d91c3bb5":"code","d62b8470":"code","d77bb8f9":"code","19c771b9":"code","7d1281fc":"code","81742f3d":"code","a78749c2":"code","05bf3220":"code","0dc7d9ff":"code","51061150":"code","c2124aaa":"code","477d6430":"code","9c3ff15d":"code","2e5ed218":"code","c6038c32":"code","1be573d1":"code","722514d8":"code","d606a50c":"code","b7467d8e":"code","ad372aaf":"code","a1cbad53":"code","86d92bd1":"code","1716dfe7":"code","463304c9":"code","f52d052d":"code","49bc0844":"code","300eddbb":"code","22393430":"code","86a22e57":"code","54551e4c":"code","be705b33":"code","e962a253":"markdown","09f55b8a":"markdown","d01e86b8":"markdown","48e3ac41":"markdown","669f2252":"markdown","8bcaae78":"markdown","740bdd67":"markdown","bd324927":"markdown","23cf5afe":"markdown","72a20bf9":"markdown","b3f8ba10":"markdown","7f31cf27":"markdown","c7e495a9":"markdown","fbc296d2":"markdown","99b54c3e":"markdown","c8625bd8":"markdown","c3d04131":"markdown","cff1840f":"markdown","bd66e6e0":"markdown","faf03bf2":"markdown","aed2f6c5":"markdown"},"source":{"6ff7e649":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3244f883":"# Following are several helpful packages to load in \n# Imported Libraries are as fallows\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Classification tree libraries\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\n# Other Libraries\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n","765dd556":"# Read the credit card dataset\nbase_dataset=pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndf=base_dataset","bbf34a77":"base_dataset.head()","4d3185e3":"base_dataset.shape","a62b0a77":"sns.countplot(base_dataset['Class'])","ad2c5cc5":"\"\"\" iterate through all the columns of a dataframe and modify the data type\n    to reduce memory usage.        \n\"\"\"\nstart_mem = base_dataset.memory_usage().sum() \/ 1024**2\nprint('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\nfor col in base_dataset.describe().columns:\n    col_type = base_dataset[col].dtype\n\n    if col_type != object:\n        c_min = base_dataset[col].min()\n        c_max = base_dataset[col].max()\n        if str(col_type)[:3] == 'int':\n            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                base_dataset[col] = base_dataset[col].astype(np.int8)\n            elif c_min > base_dataset.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                base_dataset[col] = base_dataset[col].astype(np.int16)\n            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                base_dataset[col] = base_dataset[col].astype(np.int32)\n            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                base_dataset[col] = base_dataset[col].astype(np.int64)  \n        else:\n            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                base_dataset[col] = base_dataset[col].astype(np.float16)\n            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                base_dataset[col] = base_dataset[col].astype(np.float32)\n            else:\n                base_dataset[col] = base_dataset[col].astype(np.float64)\n    else:\n        base_dataset[col] = base_dataset[col].astype('category')\n\nend_mem = base_dataset.memory_usage().sum() \/ 1024**2\nprint('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\nprint('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))    ","4f1b8272":"base_dataset.isna().sum()","5c037ba8":"for i in base_dataset.columns:\n    if i != 'Class' and i !='Time':\n        sns.boxplot(x=base_dataset[i])\n        plt.show()","1413377f":"from IPython.display import Image\nImage(\"\/kaggle\/input\/outlierimage\/outlier.png\")","f763029a":"def outliers_transform(base_dataset):\n    for i in base_dataset.var().sort_values(ascending=False).index[0:10]:\n        x=np.array(base_dataset[i])\n        qr1=np.quantile(x,0.25)\n        qr3=np.quantile(x,0.75)\n        iqr=qr3-qr1\n        utv=qr3+(1.5*(iqr))\n        ltv=qr1-(1.5*(iqr))\n        y=[]\n        #\"\"\"Based on clients input(ltv,utv) run the below code \"\"\"\n        for p in x:\n            if p <ltv or p>utv:\n                y.append(np.median(x))\n            else:\n                y.append(p)\n        base_dataset[i]=y","d91c3bb5":"outliers_transform(base_dataset)","d62b8470":"sns.boxplot(x=base_dataset['V1'])","d77bb8f9":"sns.boxplot(x=base_dataset['V2'])","19c771b9":"base_dataset.columns","7d1281fc":"sd = StandardScaler()\nsd.fit_transform(pd.DataFrame(base_dataset['Amount']))\nz1=sd.transform(pd.DataFrame(base_dataset['Amount']))\nbase_dataset['Amount']=z1\n\nz2 =sd.fit_transform(pd.DataFrame(base_dataset['Time']))\nbase_dataset['Time']=z2\n\n","81742f3d":"base_dataset.head()","a78749c2":"for i in base_dataset.var().index:\n    sns.distplot(base_dataset[i],kde=False)\n    plt.show()","05bf3220":"plt.figure(figsize=(20,10))\nsns.heatmap(base_dataset.corr())","0dc7d9ff":"y = base_dataset['Class']\nx = base_dataset.drop('Class',axis=1)","51061150":"y.head(10)","c2124aaa":"x.head()","477d6430":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=43)","9c3ff15d":"print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","2e5ed218":"final_accuracy_scores_decisionTree_gini=[]\ndt=DecisionTreeClassifier(criterion='gini')\ndt.fit(X_train,y_train)\ndt.predict(X_train)\ndt.predict(X_test)\nprint(\"\")\nprint(\"---------------------------------------------------------------------------------------------------------\")\nprint(\"For the machine learning model : {}\".format(i))\nprint(\"Confusion matrix for test samples\")\nprint(confusion_matrix(y_test,dt.predict(X_test)))\nprint(\"Accuracy score for test samples\",accuracy_score(y_test,dt.predict(X_test)))\nprint(\"Confusion matrix for training samples\")\nprint(confusion_matrix(y_train,dt.predict(X_train)))\nprint(\"Accuracy score for training samples\",accuracy_score(y_train,dt.predict(X_train)))\nfinal_accuracy_scores_decisionTree_gini.append([dt,confusion_matrix(y_test,dt.predict(X_test)),accuracy_score(y_test,dt.predict(X_test)),confusion_matrix(y_train,dt.predict(X_train)),accuracy_score(y_train,dt.predict(X_train))])\nfrom sklearn.model_selection import cross_val_score\nprint(\"K-Fold results for machine learning model : {} \".format(dt))\nprint(cross_val_score(dt,X_train,y_train,cv=10))","c6038c32":"predicted_decisionTree_gini = dt.predict(X_test)","1be573d1":"predicted_decisionTree_gini","722514d8":"final_accuracy_scores_randomForest_gini=[]\ndt=RandomForestClassifier(criterion='gini')\ndt.fit(X_train,y_train)\ndt.predict(X_train)\ndt.predict(X_test)\nprint(\"\")\nprint(\"---------------------------------------------------------------------------------------------------------\")\nprint(\"For the machine learning model : {}\".format(i))\nprint(\"Confusion matrix for test samples\")\nprint(confusion_matrix(y_test,dt.predict(X_test)))\nprint(\"Accuracy score for test samples\",accuracy_score(y_test,dt.predict(X_test)))\nprint(\"Confusion matrix for training samples\")\nprint(confusion_matrix(y_train,dt.predict(X_train)))\nprint(\"Accuracy score for training samples\",accuracy_score(y_train,dt.predict(X_train)))\nfinal_accuracy_scores_randomForest_gini.append([dt,confusion_matrix(y_test,dt.predict(X_test)),accuracy_score(y_test,dt.predict(X_test)),confusion_matrix(y_train,dt.predict(X_train)),accuracy_score(y_train,dt.predict(X_train))])\nfrom sklearn.model_selection import cross_val_score\nprint(\"K-Fold results for machine learning model : {} \".format(dt))\nprint(cross_val_score(dt,X_train,y_train,cv=10))","d606a50c":"predicted_randomForest_gini = dt.predict(X_test)","b7467d8e":"predicted_randomForest_gini","ad372aaf":"final_accuracy_scores_Bagging=[]\ndt=BaggingClassifier()\ndt.fit(X_train,y_train)\ndt.predict(X_train)\ndt.predict(X_test)\nprint(\"\")\nprint(\"---------------------------------------------------------------------------------------------------------\")\nprint(\"For the machine learning model : {}\".format(i))\nprint(\"Confusion matrix for test samples\")\nprint(confusion_matrix(y_test,dt.predict(X_test)))\nprint(\"Accuracy score for test samples\",accuracy_score(y_test,dt.predict(X_test)))\nprint(\"Confusion matrix for training samples\")\nprint(confusion_matrix(y_train,dt.predict(X_train)))\nprint(\"Accuracy score for training samples\",accuracy_score(y_train,dt.predict(X_train)))\nfinal_accuracy_scores_Bagging.append([dt,confusion_matrix(y_test,dt.predict(X_test)),accuracy_score(y_test,dt.predict(X_test)),confusion_matrix(y_train,dt.predict(X_train)),accuracy_score(y_train,dt.predict(X_train))])\nfrom sklearn.model_selection import cross_val_score\nprint(\"K-Fold results for machine learning model : {} \".format(dt))\nprint(cross_val_score(dt,X_train,y_train,cv=10))","a1cbad53":"predicted_bagging = dt.predict(X_test)","86d92bd1":"predicted_bagging","1716dfe7":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,dt.predict(X_test))","463304c9":"final_accuracy_scores_DecisionTree_entropy=[]\ndt=DecisionTreeClassifier(criterion='entropy')\ndt.fit(X_train,y_train)\ndt.predict(X_train)\ndt.predict(X_test)\nprint(\"\")\nprint(\"---------------------------------------------------------------------------------------------------------\")\nprint(\"For the machine learning model : {}\".format(i))\nprint(\"Confusion matrix for test samples\")\nprint(confusion_matrix(y_test,dt.predict(X_test)))\nprint(\"Accuracy score for test samples\",accuracy_score(y_test,dt.predict(X_test)))\nprint(\"Confusion matrix for training samples\")\nprint(confusion_matrix(y_train,dt.predict(X_train)))\nprint(\"Accuracy score for training samples\",accuracy_score(y_train,dt.predict(X_train)))\nfinal_accuracy_scores_DecisionTree_entropy.append([dt,confusion_matrix(y_test,dt.predict(X_test)),accuracy_score(y_test,dt.predict(X_test)),confusion_matrix(y_train,dt.predict(X_train)),accuracy_score(y_train,dt.predict(X_train))])\nfrom sklearn.model_selection import cross_val_score\nprint(\"K-Fold results for machine learning model : {} \".format(dt))\nprint(cross_val_score(dt,X_train,y_train,cv=10))","f52d052d":"predicted_decisionTree_entropy = dt.predict(X_test)","49bc0844":"predicted_decisionTree_entropy","300eddbb":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,dt.predict(X_test))","22393430":"final_accuracy_scores_RandomForest_entropy=[]\ndt=RandomForestClassifier(criterion='entropy')\ndt.fit(X_train,y_train)\ndt.predict(X_train)\ndt.predict(X_test)\nprint(\"\")\nprint(\"---------------------------------------------------------------------------------------------------------\")\nprint(\"For the machine learning model : {}\".format(i))\nprint(\"Confusion matrix for test samples\")\nprint(confusion_matrix(y_test,dt.predict(X_test)))\nprint(\"Accuracy score for test samples\",accuracy_score(y_test,dt.predict(X_test)))\nprint(\"Confusion matrix for training samples\")\nprint(confusion_matrix(y_train,dt.predict(X_train)))\nprint(\"Accuracy score for training samples\",accuracy_score(y_train,dt.predict(X_train)))\nfinal_accuracy_scores_RandomForest_entropy.append([dt,confusion_matrix(y_test,dt.predict(X_test)),accuracy_score(y_test,dt.predict(X_test)),confusion_matrix(y_train,dt.predict(X_train)),accuracy_score(y_train,dt.predict(X_train))])\nfrom sklearn.model_selection import cross_val_score\nprint(\"K-Fold results for machine learning model : {} \".format(dt))\nprint(cross_val_score(dt,X_train,y_train,cv=10))","86a22e57":"predicted_randomForest_entropy = dt.predict(X_test)","54551e4c":"predicted_randomForest_entropy","be705b33":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,dt.predict(X_test))","e962a253":"Running classification algorithms such as DecisionTreeClassifier, RandomForestClassifier, BaggingClassifier to check the accuracy score for each alogorithm. The algorithms are executed with the K-Fold cross validations.","09f55b8a":"The memory management will save the memory space and thus increase the speed of execution by using below code","d01e86b8":"Bivariate analysis:- is performed to find the relationship between each variable in the dataset and the target variable of interest (or) using 2 variables and finding realtionship between them","48e3ac41":"#### Problem Statement : Credit card fraud transaction detection\n\nThe main goal is to build machine learning models to predect the fradulent credit card tranctions for any new transaction made. We are building follwing classification tree machine learning models using k-fold cross validation techiques.\n1. Decision Tree,\n2. Random Forest \n3. Bagging\n","669f2252":"**Memory Management**","8bcaae78":"We have plotted all columns except Class column just to see if the outliers exists in the columns values. \nWe can clearly see the outliers in the columns in the boxplot. \nOur main goal is to remove the outliers from the feature columns.\n\n","740bdd67":"Splitting the training and test data to run machine learn algoriths","bd324927":"#### Supervised","23cf5afe":"### Creation of base dataset","72a20bf9":"We taken two columns to show how the outliers have been removed from V1 and V2. The oulier treatement is done for all columns. So now the dataset is free from outliers","b3f8ba10":"We can see from the above results that there are no blank or null values in all the column values \nthen null value treatement is not required \nin this case","7f31cf27":"### Pre Processing","c7e495a9":"**Boxplots** are a standardized way of displaying the distribution of data based on a five number summary (\u201cminimum\u201d, first quartile (Q1), median, third quartile (Q3), and \u201cmaximum\u201d).\n\n**Interquartile range (IQR)**: The IQR is the range between 25th percentile to 75th percentile.\n\n**Maximum** = Q3 + 1.5*IQR\n\n**Minimum** = Q1 - 1.5*IQR\n\n**Outliers**: Anything lying outside maximum or minimum is call outliers\n","fbc296d2":"#### Outlier treatment","99b54c3e":"### Model Building","c8625bd8":"From the above plot we can clearly see dataset is imbalanced.\nSo we need to build classificaiton alorithms like accuracy score","c3d04131":"Univariate analysis:- provides summary statistics for each field in the raw data set (or) summary only on one variable. ","cff1840f":"#### We need to scale Time and Amount columns because they are not inline with the other columns. So Time and Amount should be scaled as other columns.","bd66e6e0":"#### Univariate analysis (EDA)","faf03bf2":"### Problem definition","aed2f6c5":"####  Bivariate analysis (EDA)"}}