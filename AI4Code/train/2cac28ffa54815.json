{"cell_type":{"b5a7385d":"code","5eec62f6":"code","1b662309":"code","2dd75cf5":"code","c4eeba5d":"code","15b71bac":"code","dd5daa26":"code","4e74e9a9":"code","b328305c":"code","9dd1fdce":"code","419e845a":"code","90ec6afc":"code","fff2af77":"code","fe8c9224":"code","a2712e39":"code","87754c50":"code","c97b0ea5":"code","38c2a6f4":"code","0e61d0d6":"code","91bf5a3e":"code","4fc04e93":"code","2041e2c4":"code","5a31e084":"code","753857b4":"code","ee173ed5":"code","2ca6b64e":"code","48569a92":"code","fd849a07":"code","69781b7a":"code","973d63f5":"code","db757151":"code","28f10d56":"code","c77f7c97":"code","502c0943":"code","53d44458":"code","04288cb5":"code","94f53cc8":"code","e8d3964f":"code","ba329e8b":"code","bc668791":"code","84e0a07f":"code","5ab57fe1":"markdown","e2cd1f83":"markdown","faffefe7":"markdown","7e19db01":"markdown","a8b39206":"markdown","37823c75":"markdown","a2dc71ec":"markdown","9904d626":"markdown","da4f6717":"markdown","5df2d89d":"markdown","5735cfc9":"markdown","3ba8424d":"markdown","72dc8c79":"markdown","124bb2fc":"markdown","e1cd527d":"markdown","95c80d52":"markdown","dcd9fb88":"markdown","1b1e64a5":"markdown","67b77eaf":"markdown","e3acb40a":"markdown","cec15897":"markdown","ab92d797":"markdown","69c266da":"markdown","e63fa00d":"markdown"},"source":{"b5a7385d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5eec62f6":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain.info()\ntest.info()","1b662309":"combine = [train, test]\nfor dataset in combine:\n    dataset.drop(['PassengerId', 'Cabin'], axis=1, inplace=True)    ","2dd75cf5":"train['Embarked'].mode()","c4eeba5d":"for dataset in combine:\n    dataset['Sex'], sex_val = pd.factorize(dataset['Sex'])\n    dataset['Embarked'], emb_val = pd.factorize(dataset['Embarked'].fillna('S'))\n    \ntrain.info()    ","15b71bac":"third_cls_mean = train['Fare'][train['Pclass']==3].mean()\ntest['Fare'].fillna(third_cls_mean, inplace=True)\ntest.info()","dd5daa26":"train['Ticket'].value_counts()\nfor dataset in combine:\n    dataset.drop(['Ticket'], axis=1, inplace=True)","4e74e9a9":"train.info()","b328305c":"X = train.iloc[:,[1,3,4,5,6,7,8]]  # drop Survived since we need the imputer to have same number of inputs for train and test imputation\nX.info()","9dd1fdce":"test.info()","419e845a":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nimputer = IterativeImputer()\ncols = X.columns  # after imputation, the df becomes a np matrix object without column names, so we have to save columns names and post them later\n\ndf_imp = imputer.fit_transform(X)\n\ndf = pd.DataFrame(df_imp, columns=cols)\ndf.info()\n\ncol = ['Pclass','Sex','SibSp','Parch','Embarked'] \ndf[col] = df[col].applymap(np.int64) # to convert categories from float to int\ndf.info()","90ec6afc":"t = test.iloc[:,[0,2,3,4,5,6,7]]\ntcols = t.columns\ntest_imp = imputer.transform(t)\ntst = pd.DataFrame(test_imp, columns=tcols)\n\ntst[col] = tst[col].applymap(np.int64)\ntst.info()","fff2af77":"test.Name","fe8c9224":"df.insert(0, 'Name', train.Name, object)\ntst.insert(0, 'Name', test.Name, object)\ndf.insert(1, 'Survived', train.Survived, int)","a2712e39":"# required packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns","87754c50":"df.Survived.value_counts(normalize=True) # Survival percentages\nsns.countplot(df.Survived)","c97b0ea5":"df.groupby(['Pclass'])['Survived'].value_counts()\nsns.countplot('Pclass', hue='Survived', data=df)","38c2a6f4":"df.groupby(['Sex'])['Survived'].value_counts(normalize=True)","0e61d0d6":"sns.countplot('Sex', hue='Survived', data=df)","91bf5a3e":"f, ax = plt.subplots(1,2, figsize=(18,8))\nsns.violinplot('Pclass', 'Age', hue='Survived', data=df, split=True, ax=ax[0])\nax[0].set_title('Pclass and Age according to Survived')\nax[0].set_yticks(range(0,110,10))\nsns.violinplot('Sex', 'Age', hue='Survived', data=df, split=True, ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')","4fc04e93":"pd.crosstab(df.Parch, df.Survived)","2041e2c4":"sns.catplot('SibSp', 'Survived', kind='point', data=df)\nsns.catplot('Parch', 'Survived', kind='point', data=df)","5a31e084":"cor = df.corr().round(2)\nsns.heatmap(cor, annot=True, vmax=0.6)","753857b4":"combo = [df, tst]\nfor dataset in combo:\n    dataset['Family'] = dataset['SibSp'] + dataset['Parch']\n    dataset['Class_fare'] = pd.Series(dataset['Pclass'] * np.log(dataset['Fare'] + 1)) # log to eliminate adverse outlier effect, +1 for kids under 1 year","ee173ed5":"df.Name.str.split(' ', expand=True)[1].value_counts()","2ca6b64e":"for dataset in combo:\n    dataset['Title'] = 0\n    dataset['Title'] = dataset['Name'].str.extract('([A-Za-z]+)\\.')\n    dataset.Title.replace(['Mlle','Mme','Ms','Dr','Major','Rev','Capt','Lady','Countess','Dona','Jonkheer','Col','Sir','Don'],\n                    ['Miss','Mrs','Master','other','other','other','other','other','other','other','other','other','other','other'],\n                    inplace=True)\n    dataset['Title'], title_val= pd.factorize(dataset['Title'])\n    dataset.drop(['Name'], axis=1, inplace=True)","48569a92":"sns.distplot(df.Fare) ","fd849a07":"sns.distplot(df.Age)","69781b7a":"df.info()","973d63f5":"from sklearn.preprocessing import StandardScaler, RobustScaler\nstd = StandardScaler()\nrobust = RobustScaler()\nage = ['Age']\ndf[age] = std.fit_transform(df[age])\ntst[age] = std.transform(tst[age])\ncol_names = ['Fare', 'Class_fare']\ndf[col_names] = robust.fit_transform(df[col_names])\ntst[col_names] = robust.transform(tst[col_names])","db757151":"df.Family.value_counts()\nsns.catplot('Family', 'Survived', kind='point', data=df)","28f10d56":"for dataset in combo:\n    dataset['Family'] = pd.cut(dataset['Family']+1, bins=[0,1,4,7,11], labels=['solo','small','medium','large'])\n    dataset['Family'], fam_values = pd.factorize(dataset['Family'])","c77f7c97":"df = pd.get_dummies(df, columns=['Pclass', 'Title', 'Embarked'], drop_first=True)\ntst = pd.get_dummies(tst, columns=['Pclass', 'Title', 'Embarked'], drop_first=True)","502c0943":"df.info()","53d44458":"from sklearn.model_selection import train_test_split\ndtrain, dvalid = train_test_split(df, test_size=0.2)\ndvalid.info()","04288cb5":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(solver='saga', max_iter=1000, random_state=43, n_jobs=-1, verbose=0)\nparams = dict(C=[0.001,0.01,0.1,1], tol = [.00001,.0001,.001], penalty=['l1', 'l2'])\n\nclf = RandomizedSearchCV(logreg, params)\nsearch.best_params_ # the best parameters as per the CV search\nx = dtrain.drop(['Survived'], axis=1)\ny = dtrain['Survived']\nsearch = clf.fit(x,y)\nlogfit = search.best_estimator_","94f53cc8":"model = logfit.fit(x,y)\ny_pred = logfit.predict(dvalid.drop(['Survived'], axis=1))\n\nfrom sklearn.metrics import accuracy_score\ny_true = dvalid['Survived']\naccuracy_score(y_true, y_pred) ","e8d3964f":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_jobs=-1, random_state=43)\nparam = dict(criterion=['gini', 'entropy'], max_depth=[None, 5,10,20,30],\n            max_features=['sqrt','log2',4], min_samples_leaf=[1,2,3,4])\nrf_clf = RandomizedSearchCV(rf, param)\nsearch = rf_clf.fit(x,y)\nrf_fit = search.best_estimator_\nsearch.best_params_","ba329e8b":"rf_model = rf_fit.fit(x,y)\nrf_pred = rf_fit.predict(dvalid.drop(['Survived'], axis=1))\naccuracy_score(dvalid['Survived'], rf_pred)","bc668791":"log_pred = logfit.predict(tst)\nrfpred = rf_fit.predict(tst)","84e0a07f":"log_sub = pd.DataFrame({'PassengerId': test['Name'], 'Survived': log_pred})\nlog_sub.to_csv('log_sub', index=False)","5ab57fe1":" **Pre-processing\/Cleaning**\n\nWe need to check for and sort out missing values, duplicates and convert data types into a workable format before exploratory data analysis (EDA).\nFrom the above output, Age and cabin are have the most number of missing values. Cabin especially, is missing about 80% of its data so we will drop it. PassengerId has no importance except for prediction submission later. So lets cobine train and test and perform these operations.","e2cd1f83":"**Exploratory Data Analysis**\n\nIn this section we explore the relationships between Survived and the other variables. ","faffefe7":"Next we convert Sex and Embarked to int except for Name which we'll use later. This while we impute the missing values for Embarked with its mode which is the value 'S' (point of embarkation - Southampton). ","7e19db01":"Next we check out general correlations in the entire dataset. Pclass and Fare has a relatively strong negative correlation- meaning passengers paid less classes increased from 1 to 3. As seen earlier, Sex also has a strong correlation with Survived. Interestingly, SibSp is negatively correlated with age meaning older passengers were less likely to have a sibling, or spouse, along with them on the cruise. ","a8b39206":"This notebook is an end-to-end analysis of the titanic dataset with notes and code on data cleaning, exploratory data analysis, model fitting and prediction. My objective is to get the simplest model, in terms of explainability, that yields a high accuracy score.\n\nWe first load the dataset. ","37823c75":"Now we one-hot encode Pclass, Title and Embarked variables","a2dc71ec":"**Feature Engineering**\n\nNow that our dataset is clean, we can extract some features to help the algorithm improve its accuracy. The steps I chose in this section are:\n1. Include new features, Family- an addition of SibSp and Parch, and class_fare- an interaction of the two variables\n2. Extract Title from the Name variable since it played an huge role in predicting survival\n3. Standardize Age, Class_fare and Fare to reduce noise and outlier effects. \n4. Category encode Pclass, Title and Embarked","9904d626":"First we create the variables Family and Class_fare","da4f6717":"The Ticket variable is also interesting, we could convert it into categories using the category encoder package and use it in the model. But it has high cardinality, too many categories with most having little or no information. Since including it would most likely result into overfitting, we drop it.","5df2d89d":"Impute Age for the test dataset using the information learnt from train","5735cfc9":"Next we append Name and Survived to train\/df and Name to test dataset\/tst.","3ba8424d":"Less than the score we got from logistic regression but I have a feeling it's closer to the true figure.\nLet's use these models on the actual test set and see the scores from the leaderboard.","72dc8c79":"Let's look at the Name variable. We see each name has a salutation i.e Mr, Miss, Mlle, Countess e.t.c. The key question: were the salutations significant in determining chance of survival? Does having a special salutation, for instance, Capt or Don, have major implications in survival chances?\n\nLet's find out!","124bb2fc":"**Model Fitting and Feature Selection**\n\nLet's fit 2 models here: \n1. Logistic regression with either l1 or l2 regularization\n2. Random forest and check the importance of each feature. First we run a randomized search over each algorithm's hyperparameter space to choose the best hyperparameters. \n\nBut before that, let's split our training data, df, into a train and validation set for testing our algorithms.","e1cd527d":"Now let's try the Random Forest algorithm","95c80d52":"The violin plots below show an excellent visual between Age, Pclass\/Sex and Survived. We can infer from the left plot that age slightly decreases as Pclass increases.\nFrom the right side plot, we see that average age in both sexes was roughly 25 with females having a slightly higher figure.","dcd9fb88":"Females had a higher chance of survival, approximately 74%, as compared to males, 19%. ","1b1e64a5":"From the plot below 1st and 2nd class passengers had a higher survival rate than 3rd class.","67b77eaf":"The score is an impressive 0.87 which seems like a slight case of overfitting. Let's make a visual of the most important features according to this model.","e3acb40a":"These two figures below show that small families with less than 4 members, had a higher chance of survival. This holds true for the variables SibSp(siblings or spouses) and Parch(parents and children).","cec15897":"Imputing for Age, I chose an Iterative imputer to capture the more complex relationships between the variables as compared to the popular method of imputing via median or mean. We also drop the Name variable since iterative imputation cannot process an object datatype.","ab92d797":"For the two other floats, Age, class_fare and Fare, we apply transformations to reduce random noise and standardize the variables. Age has many values below 0 and has a normal distribution. Therefore I choose Standardscaler for the Age variable and RobustScaler for Fare and class_fare, since they have high leverage outliers.","69c266da":"The next steps are to bin the Family variable and factorize it into a workable format for our learning algorithms","e63fa00d":"That worked!\nA closer look at the test dataset shows that Fare is missing a row. We simply impute with mean according to the passenger class, Pclass, of that row. This is done using train data to avoid information leakage into our model."}}