{"cell_type":{"1ff60967":"code","1a530722":"code","7906df82":"code","20d44efe":"code","f68d4faf":"code","5bf8ac4f":"code","15ef1bc4":"code","9ba5ad55":"code","631758b7":"code","6e4fb09f":"code","1c8e1140":"code","80e0dac3":"code","6757169f":"code","ec723b9d":"code","624f5554":"code","639c5ca6":"code","28820ea7":"code","d3bf8a59":"code","d80aefbf":"code","22595b4e":"code","feb33e8e":"code","19d4ad63":"code","bf008c93":"code","eba1c8b6":"code","c928cdfe":"code","3935aa82":"code","cd5eac53":"markdown","af9eba48":"markdown","0be0ad6b":"markdown","65db4a00":"markdown","adf9f55b":"markdown","cef6757a":"markdown","43ab0518":"markdown","d6d3211a":"markdown","7581719e":"markdown","4a4e891e":"markdown","8fd14bbd":"markdown","fdefc008":"markdown","ea60ce47":"markdown","8c638d01":"markdown","8e786977":"markdown","f057a2c8":"markdown","242f99ec":"markdown","a3179bb3":"markdown","9cb75931":"markdown","286ba1c7":"markdown","429fafeb":"markdown"},"source":{"1ff60967":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# There are too many paths and printing them takes\n# up too much space so I don't do this normally\nif 1==0: \n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            prv","1a530722":"!pip install scispacy","7906df82":"!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_sm-0.2.4.tar.gz","20d44efe":"# import commands to\n# convert text to a matrix of\n# tokens\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# import sklearn LDA function\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# import scispacy, a repo of commands\n# to deal with scientific documents\nimport scispacy\n\n# import spaCy, a repo of commands\n# to deal with natural language processing \n# (NLP)\nimport spacy\n\n# Of the spaCy library, \n# the en_core_sci_sm library contains\n# a full spaCy pipeline for biomedical data \nimport en_core_sci_sm\n\n# import command to measure\n# the Jensen-Shannon distance (metric)\nfrom scipy.spatial.distance import jensenshannon\n\n# import joblib, a repo of commands\n# to run python functions as pipeline\n# jobs\nimport joblib\n\n# import pyLDAvis, a repo of commands\n# to create interactic topic model visualizations\nimport pyLDAvis\nimport pyLDAvis.sklearn\npyLDAvis.enable_notebook()","f68d4faf":"input_dir = \"\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/\"\nmeta_data_file = (\"%s\/all_sources_metadata_2020-03-13.csv\" % input_dir)\nmeta_data_df = pd.read_csv(meta_data_file)\nmeta_data_df = meta_data_df.reset_index().rename({\"index\":\"paper_id\"}, axis='columns')\nmeta_data_df.head()","5bf8ac4f":"print(\"Number of Rows in Meta Data: %i\" % len(meta_data_df))\nprint(\"Number of Titles: %i \" % meta_data_df['title'].count())\nprint(\"Number of Abstracts: %i \" % meta_data_df['abstract'].count())","15ef1bc4":"# replace empty abstracts with empty strings\nabstract_series=meta_data_df.abstract.replace(np.nan, '', regex=True)\nall_abstracts = abstract_series.str.replace('\\n\\n', '')","9ba5ad55":"all_abstracts[58][:500]","631758b7":"# initalize the spaCy pipeline\n# for biomedical data\nnlp = en_core_sci_sm.load()\n\ndef spacy_tokenizer(sentence):\n    return [word.lemma_ for word in nlp(sentence)]\n\ndef print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"\\nTopic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()","6e4fb09f":"# ignore some words that are irrelevant for the content\nstop_words = text.ENGLISH_STOP_WORDS.union({'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'fig', '10'})\n\ntf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n                                stop_words = stop_words,\n                                lowercase = True\n                               )\n\ntf = tf_vectorizer.fit_transform(all_abstracts)\n\ntf.shape","1c8e1140":"# the code below takes a bit so you can skip this and load the model which I attached to this notebook\n#lda_tf = LatentDirichletAllocation(n_components=20, random_state=0)\n#lda_tf.fit(tf)\n#joblib.dump(lda_tf, 'lda.csv')","80e0dac3":"lda_tf = joblib.load('..\/input\/corona-time-lda-v1\/lda.csv')","6757169f":"tfidf_feature_names = tf_vectorizer.get_feature_names()\nprint_top_words(lda_tf, tfidf_feature_names, 20)","ec723b9d":"viz = pyLDAvis.sklearn.prepare(lda_tf, tf, tf_vectorizer)","624f5554":"pyLDAvis.display(viz)","639c5ca6":"pyLDAvis.save_html(viz, 'lda.html')","28820ea7":"topic_dist = pd.DataFrame(lda_tf.transform(tf))\n\ntopic_dist.head()\n","d3bf8a59":"def get_k_nearest_docs(doc_dist, k=5, use_jensenshannon=True):\n    '''\n    doc_dist: topic distribution (sums to 1) of one article\n    \n    Returns the index of the k nearest articles (as by Jensen\u2013Shannon divergence\/ Euclidean distance in topic space). \n    '''\n    \n    if use_jensenshannon:\n            distances = topic_dist.apply(lambda x: jensenshannon(x, doc_dist), axis=1)\n    else:\n        diff_df = topic_dist.sub(doc_dist)\n        distances = np.sqrt(np.square(diff_df).sum(axis=1)) # euclidean distance (faster)\n        \n    return distances[distances != 0].nsmallest(n=k).index","d80aefbf":"def recommendation(paper_id, k=5):\n    '''\n    Returns the title of the k papers that are closest (topic-wise) to the paper given by paper_id.\n    '''\n    \n    print(meta_data_df.title[meta_data_df.paper_id == paper_id].values[0])\n    print('\\nRELATED DOCUMENTS: \\n')\n    recommended = get_k_nearest_docs(topic_dist[meta_data_df.paper_id == paper_id].iloc[0], k)\n    for i in recommended:\n        print('- ', meta_data_df.title[i] )","22595b4e":"recommendation(58, k=5)","feb33e8e":"recommendation(243, k=5)","19d4ad63":"def relevant_articles(tasks, k=3):\n    tasks = [tasks] if type(tasks) is str else tasks \n    \n    tasks_tf = tf_vectorizer.transform(tasks)\n    tasks_topic_dist = pd.DataFrame(lda_tf.transform(tasks_tf))\n\n    for index, bullet in enumerate(tasks):\n        print('\\n=============================================')\n        print('\\nBULLET: ' + bullet + '\\n')\n\n        print('\\nRELATED DOCUMENTS: \\n')\n        recommended = get_k_nearest_docs(tasks_topic_dist.iloc[index], k)\n        for i in recommended:\n            print('- ', meta_data_df.title[i] )","bf008c93":"npi_task_raw=[\"Guidance on ways to scale up non-pharmaceutical interventions in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.\", \\\n\"Rapid design and execution of experiments to examine and compare non-pharmaceutical interventions currently being implemented. Department of Homeland Security Centers for Excellence could potentially be leveraged to conduct these experiments.\", \\\n\"Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.\", \\\n\"Methods to control the spread in communities, barriers to compliance and how these vary among different populations..\", \\\n\"Models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status.\", \\\n\"Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with non-pharmaceutical interventions.\", \\\n\"Research on why people fail to comply with public health advice, even if they want to do so (e.g., social or financial costs may be too high).\", \\\n\"Research on the economic impact of this or any pandemic. This would include identifying policy and programmatic alternatives that lessen\/mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses, treatment, and needed care, regardless of ability to pay.\"]","eba1c8b6":"relevant_articles(npi_task, 5)","c928cdfe":"npi_custom=[\"scale non-pharmaceutical interventions funding infrastructure\", \\\n\"Design and execution of experiments to examine and compare non-pharmaceutical interventions currently being implemented. Department of Homeland Security Centers for Excellence\", \\\n\"school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.\", \\\n\"spread in communities, barriers to compliance and how these vary among different populations..\", \\\n\"potential interventions accounting race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status.\", \\\n\"Policy changes on individuals with limited resources and the underserved with non-pharmaceutical interventions.\", \\\n\"people fail to comply with public health advice despite (social or financial costs).\", \\\n\"Research on the economic impact of pandemics\"]","3935aa82":"relevant_articles(npi_custom, 5)","cd5eac53":"Shoutout to @danielwolffram who already  did some topic modeling: https:\/\/www.kaggle.com\/danielwolffram\/topic-modeling-finding-related-articles","af9eba48":"So the meta_data table contains most of the titles and abstracts. Before I dive deeper into the data, I want review the goal of this notebook:","0be0ad6b":"# Papers on Topic: What do we know about non-pharmaceutical interventions?\nWe can now also map a task or bullet point into the topic space and find related articles that might help to solve the question at hand.","65db4a00":"Now that all our libraries are loaded we need data. We explore the full text in the files such as was done in the following notebook:\nhttps:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\n\nor we can be lazy and just use the meta_data table.","adf9f55b":"# Data Preparation\nSince I am too lazy to go through the full texts let's consider the abstract text data.","cef6757a":"My code below is inspired by his notebook.","43ab0518":"Let's transform our abstracts into a token matrix.","d6d3211a":"# Install\/Load Packages","7581719e":"Each article is a mixture of topics","4a4e891e":"next I am going to simply manually change the task details from the kaggle website to see what I get.","8fd14bbd":"** Use NLP and training data to list what we know about non-pharmaceutical interventions **","fdefc008":"The first block of code is (almost) directly from kaggle","ea60ce47":"To start, we would want to filter for specific papers that discuss this topic.","8c638d01":"# Get \"Nearest\" Papers (in Topic Space)","8e786977":"# Latent Dirichlet Allocation (LDA)\n[LDA](https:\/\/en.wikipedia.org\/wiki\/Latent_Dirichlet_allocation) is an example of a [topic model](https:\/\/en.wikipedia.org\/wiki\/Topic_model). In other words,the general purpose of LDA is to classify documents into topics. For example, if we have a good topic model, we expect that documents with words such as \"bone\", \"puppy\", \"bark\", \"dogs\" would be classified as documents with \"dog\" topics, and documents with words such as \"meow\",\"litter\",\"feline\",\"cat\" would be classified as \"cat\" topics.","f057a2c8":"Next we installl scispacy, a repo of commands to deal with scientific documents. *Note that internet access needs to be switched on for this to work!*","242f99ec":"# Task\nWhat do we know about non-pharmaceutical interventions?","a3179bb3":"So the meta data table has 14 columns:\n* 'sha': hash of the PDFs\n* 'source_x'\n * There are four sources:\n  * CZI 1236 records\n  * PMC 27337\n  * bioRxiv 566\n  * medRxiv 361\n* 'title'\n* 'doi': populated for all BioRxiv\/MedRxiv paper records and most of the other records (26357 non null)\n* 'pmcid': populated for all PMC paper records (27337 non null)\n* 'pubmed_id': populated for some of the records\n* 'license'\n* 'abstract'\n* 'publish_time'\n* 'authors'\n* 'journal'\n* 'Microsoft Academic Paper ID': populated for some of the records\n* 'WHO #Covidence': populated for all CZI records and none of the other records (1236 non null)\n* 'has_full_text' (boolean): True if PDF was processed with fulltext ","9cb75931":"# Introduction","286ba1c7":"first I am going to simply copy and paste the task details from the kaggle website to see what I get.","429fafeb":"# Discovered Topics"}}