{"cell_type":{"07e8919c":"code","ef478ab1":"code","46855ab2":"code","1acb36fe":"code","f563ecad":"code","4d10da4e":"code","0bdbcc25":"code","c51324d1":"code","dbe5ab39":"code","d6518cc1":"code","766310ce":"code","5a222d46":"code","64faaa45":"code","f9ef5ce1":"code","9e1f972a":"code","ae47adb4":"markdown"},"source":{"07e8919c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef478ab1":"from matplotlib import pyplot as plt\nimport seaborn as sns\nimport sklearn\n\npd.set_option('display.max_columns', None)\n\ntrain_data = pd.read_csv(r\"\/kaggle\/input\/santander-customer-transaction-prediction\/train.csv\")\ntest_data = pd.read_csv(r\"\/kaggle\/input\/santander-customer-transaction-prediction\/test.csv\")\n\ntrain_X = train_data.drop(columns=['ID_code','target'])\ntrain_y = train_data['target']\ntest_X = test_data.drop(columns=['ID_code'])\n\nseed = 12","46855ab2":"# random forest feature importance\n\nfrom sklearn.ensemble import RandomForestClassifier\nimport gc\n\ngc.collect()\n\nprint('start fitting random forest model')\nforest = RandomForestClassifier(random_state=seed, n_jobs=-1, max_depth=4)\n%timeit forest.fit(train_X, train_y)\n\nfeature_importance = forest.feature_importances_","1acb36fe":"feature_importance_map = {}\nfor idx, score in enumerate(feature_importance):\n    feature_importance_map['var_%d' % idx] = score\n\nfeature_importance_map = {k:v for k,v in sorted(feature_importance_map.items(), key=lambda x: x[1], reverse=True)}","f563ecad":"import seaborn as sns\n\nfig, ax = plt.subplots(figsize=(20,6))\n\nsns.barplot(x=list(feature_importance_map.keys())[:50], y=list(feature_importance_map.values())[:50], ax=ax)\nplt.xticks(rotation=45)\nplt.show()","4d10da4e":"## Let's plot the 5 best variable selected by the random forest model\ncols = list(feature_importance_map.keys())[:5]\ncols","0bdbcc25":"## violin\n\nfig, ax = plt.subplots(2,1, figsize=(20,20))\ntrain_df = train_data[cols + ['target']].melt(id_vars = ['target'], var_name = 'Vars', value_name = 'Values')\nsns.violinplot(x=\"Vars\",y=\"Values\",data=train_df, hue = 'target', split=True, inner=\"quart\", ax=ax[0])\nax[0].set_title(\"violin plot for train data\")\n\ntest_df = test_data[cols].melt(var_name = 'Vars', value_name = 'Values')\nsns.violinplot(x=\"Vars\",y=\"Values\",data=test_df, inner=\"quart\", ax=ax[1])\nax[1].set_title(\"violin plot for test data\")\nplt.show()","c51324d1":"## quintile based binning and one-hot encoding\ntrain_X_copy = train_X.copy()\n\nfor col in cols:\n    train_X_copy[col + '_bin'] = pd.qcut(train_X_copy[col], 5, labels=False)\n    train_X_copy = pd.concat((train_X_copy, pd.get_dummies(data=train_X_copy[col+'_bin'], prefix=col)),1)\n    \ntrain_X_copy = train_X_copy.drop(columns=cols)  \ntrain_X_copy = train_X_copy.drop(columns=[col + '_bin' for col in cols])","dbe5ab39":"train_X_copy.head()","d6518cc1":"# code below are copied from the weighted logistic regression notebook\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nscale = np.logspace(0,2,num=10)\ngrid = [{0:1, 1:weight} for weight in scale]\ngrid = {\"class_weight\": grid }\nl2_model = LogisticRegression(penalty='l2', random_state=25, solver='sag')\ngrid_search = GridSearchCV(l2_model,grid,scoring=\"roc_auc\", n_jobs=-1, refit=True, verbose=2)\n\nprint(grid)\nprint(\"-----------------Start Fitting Weighted Logistic Regression Model With L-2 regularization-----------------\")\ngrid_search.fit(train_X_copy, train_y)\n\ntrain_pred = grid_search.predict(train_X_copy)","766310ce":"## compute the confusion matrix for binary classification problem\ndef confusion_matrix(y, y_pred, threshold=None):\n    assert(len(y) == len(y_pred))\n    \n    if threshold is not None:\n        y_pred = y_pred >= threshold\n    \n    \n    #  TP  |  FN\n    #  ---------\n    #  FP  |  TN\n    \n    mat = np.zeros((2,2))\n    \n    for i in np.arange(len(y)):\n        if y_pred[i] == y[i]:\n            if y[i] == 1:\n                ## TP\n                mat[0,0] = mat[0,0] + 1\n            if y[i] == 0:\n                ## TN\n                mat[1,1] = mat[1,1] + 1\n        else:\n            if y[i] == 1:\n                ## FN\n                mat[0,1] = mat[0,1] + 1\n            if y[i] == 0:\n                ## FP:\n                mat[1,0] = mat[1,0] + 1\n    \n    return mat","5a222d46":"confusion_mat = confusion_matrix(train_y, train_pred)\nconfusion_mat","64faaa45":"tpr = confusion_mat[0,0] \/ (confusion_mat[0,0] + confusion_mat[1,1])\nfpr = confusion_mat[1,0] \/ (confusion_mat[1,1] + confusion_mat[1,0])\nprecision = confusion_mat[0,0] \/ (confusion_mat[0,0] + confusion_mat[1,0])\nprint(\"True positive rate: \" + str(tpr))\nprint(\"False positive rate: \" + str(fpr))\nprint(\"precision\" + str(precision))","f9ef5ce1":"## quintile based binning and one-hot encoding for test data\ntest_X_copy = test_X.copy()\n\nfor col in cols:\n    test_X_copy[col + '_bin'] = pd.qcut(test_X_copy[col], 5, labels=False)\n    test_X_copy = pd.concat((test_X_copy, pd.get_dummies(data=test_X_copy[col+'_bin'], prefix=col)),1)\n    \ntest_X_copy = test_X_copy.drop(columns=cols)  \ntest_X_copy = test_X_copy.drop(columns=[col + '_bin' for col in cols])","9e1f972a":"### submission\n### https:\/\/www.kaggle.com\/dansbecker\/submitting-from-a-kernel\n\ntest_pred = grid_search.predict(test_X_copy)\nmy_submission = pd.DataFrame({'ID_code': test_data.ID_code, 'target': test_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","ae47adb4":"The kernel distributions look different between the training and test set, but the quartiles and mean are close.  \nWe also notice that in the violin plot for the training data, the distribution of target=1 and target=0 are quite different from each other. These 5 variables will be useful when fitting models."}}