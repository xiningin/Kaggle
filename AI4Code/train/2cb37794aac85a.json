{"cell_type":{"9c1e9ad9":"code","b10108d4":"code","ddf94f61":"code","70486456":"code","faa930e6":"code","cf24c23f":"code","a41a2685":"code","f2901948":"code","33f7c1f9":"code","96bb33d0":"code","66dccefa":"code","af7d3040":"code","44129da5":"code","b7a69ff0":"code","f767ccc6":"code","0d44636f":"code","3389ba44":"code","88c2b6a4":"code","f69c6fe0":"code","e9a70454":"code","4ff67e01":"code","918f87a3":"code","73991e07":"markdown","61bcf2ca":"markdown","9a2092db":"markdown","3d7cb41c":"markdown","7262ff19":"markdown","5646009e":"markdown","a5c7ee64":"markdown","e6546331":"markdown","a1b505c0":"markdown"},"source":{"9c1e9ad9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b10108d4":"import pandas as pd\npd.set_option('display.max_columns', 1000)\n\ndf_train_raw = pd.read_csv('\/kaggle\/input\/sanbercode-data-science-0620\/train.csv', index_col='id')\ndf_test_raw = pd.read_csv('\/kaggle\/input\/sanbercode-data-science-0620\/test.csv', index_col='id')\n\ndf_train_chart_preview = pd.get_dummies(df_train_raw, columns=['Gaji'])\n\ndisplay(df_train_raw, df_test_raw, df_train_chart_preview)","ddf94f61":"from matplotlib import pyplot as plt\n\ndef draw_chart(df, col_index):\n    col_name = df.columns[col_index]\n    df_chart = df[[col_name, 'Gaji_<=7jt', 'Gaji_>7jt']].groupby(col_name).sum()\n    \n    fig, ax = plt.subplots(nrows=2, figsize=(20, 8))\n    title = f\"Count category 'Gaji' by {col_name}\"\n    ax[0].set_title(title)\n    ax[1].set_title(f\"{title} %\")\n    \n    for i in range(len(df_chart.index)):\n        x = df_chart.index[i]\n        y1 = df_chart.iloc[i][1]\n        y2 = df_chart.iloc[i][0]\n        yt = y1 + y2\n        y1p = y1 \/ yt\n        y2p = y2 \/ yt\n        ax[0].bar(x, y1, color='g')\n        ax[0].bar(x, y2, color='r', bottom = y1)        \n        ax[1].bar(x, y1p, color='g')\n        ax[1].bar(x, y2p, color='r', bottom = y1p)\n    \n    plt.show()","70486456":"draw_chart(df_train_chart_preview, 0)","faa930e6":"draw_chart(df_train_chart_preview, 1)","cf24c23f":"draw_chart(df_train_chart_preview, 3)","a41a2685":"draw_chart(df_train_chart_preview, 4)","f2901948":"draw_chart(df_train_chart_preview, 5)","33f7c1f9":"draw_chart(df_train_chart_preview, 6)","96bb33d0":"draw_chart(df_train_chart_preview, 7)","66dccefa":"draw_chart(df_train_chart_preview, 10)","af7d3040":"from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nstd_scaler = StandardScaler()\nminmax = MinMaxScaler()\n\nirrelevants = ['Berat Akhir', 'Keuntungan Kapital', 'Kerugian Capital']\ndummies = ['Kelas Pekerja', 'Status Perkawinan', 'Pekerjaan', 'Pendidikan']\ncolumn_class = 'Gaji'\n\ndf_train = df_train_raw.drop(irrelevants, axis=1).replace({'Perempuan': 0, 'Laki2': 1}).replace({'<=7jt': 0, '>7jt': 1})\ndf_train = pd.get_dummies(df_train, columns=dummies)\n\nX = df_train.drop(column_class, axis=1)\ny = df_train[column_class]\n\nX = pd.DataFrame(minmax.fit_transform(X), columns=X.columns)\nX['Umur'] = pd.DataFrame(std_scaler.fit_transform(pd.DataFrame(X['Umur'])))\n\ndf_test = df_test_raw.drop(irrelevants, axis=1).replace({'Perempuan': 0, 'Laki2': 1}).replace({'<=7jt': 0, '>7jt': 1})\ndf_test = pd.get_dummies(df_test, columns=dummies)\ndf_test = pd.DataFrame(minmax.transform(df_test), columns=df_test.columns)\ndf_test['Umur'] = pd.DataFrame(std_scaler.transform(pd.DataFrame(df_test['Umur'])))\ndf_test = df_test.set_index(df_test_raw.index)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\ndisplay(X, y, X_train, X_test, y_train, y_test, df_test)","44129da5":"from sklearn.tree import ExtraTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm.classes import OneClassSVM\nfrom sklearn.neural_network.multilayer_perceptron import MLPClassifier\nfrom sklearn.neighbors.classification import RadiusNeighborsClassifier\nfrom sklearn.neighbors.classification import KNeighborsClassifier\nfrom sklearn.linear_model.stochastic_gradient import SGDClassifier\nfrom sklearn.linear_model.ridge import RidgeClassifierCV\nfrom sklearn.linear_model.ridge import RidgeClassifier\nfrom sklearn.linear_model.passive_aggressive import PassiveAggressiveClassifier    \nfrom sklearn.gaussian_process.gpc import GaussianProcessClassifier\nfrom sklearn.ensemble.weight_boosting import AdaBoostClassifier\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\nfrom sklearn.ensemble.bagging import BaggingClassifier\nfrom sklearn.ensemble.forest import ExtraTreesClassifier\nfrom sklearn.ensemble.forest import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.semi_supervised import LabelPropagation\nfrom sklearn.semi_supervised import LabelSpreading\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.naive_bayes import MultinomialNB  \nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.svm import NuSVC\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.mixture import GaussianMixture\n\nclassifiers = [\n    ExtraTreeClassifier(),\n    DecisionTreeClassifier(),\n    OneClassSVM(),\n    MLPClassifier(),\n    RadiusNeighborsClassifier(),\n    KNeighborsClassifier(),\n    SGDClassifier(),\n    RidgeClassifierCV(),\n    RidgeClassifier(),\n    PassiveAggressiveClassifier(),\n    #GaussianProcessClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    BaggingClassifier(),\n    ExtraTreesClassifier(),\n    RandomForestClassifier(),\n    BernoulliNB(),\n    CalibratedClassifierCV(),\n    GaussianNB(),\n    LabelPropagation(),\n    #LabelSpreading(),\n    LinearDiscriminantAnalysis(),\n    LinearSVC(),\n    LogisticRegression(),\n    LogisticRegressionCV(),\n    MultinomialNB(),\n    NearestCentroid(),\n    NuSVC(),\n    Perceptron(),\n    QuadraticDiscriminantAnalysis(),\n    SVC(),\n    GaussianMixture()\n]","b7a69ff0":"# Logging for Visual Comparison\n\nfrom sklearn.metrics import accuracy_score, log_loss\nimport time\n\nlog_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\", \"Run Time\"]\nlog = pd.DataFrame(columns=log_cols)\n\nfor clf in classifiers:\n    run_time = 0\n    start = int(round(time.time() * 1000))\n\n    name = clf.__class__.__name__\n    print(\"=\"*30)\n    print(name)\n    \n    try:\n        clf.fit(X_train, y_train)\n        train_predictions = clf.predict(X_test)\n        acc = accuracy_score(y_test, train_predictions)\n        train_predictions = clf.predict_proba(X_test)\n        ll = log_loss(y_test, train_predictions)\n        run_time = (int(round(time.time() * 1000)) - start) \/ 1000\n        \n        print(\"Accuracy: {:.4%}\".format(acc))\n        print(\"Log Loss: {}\".format(ll))\n        \n        log_entry = pd.DataFrame([[name, acc*100, ll, run_time]], columns=log_cols)\n        log = log.append(log_entry)\n        \n    except:\n        print(\"Error occured.\")\n        \n    print(f\"Run time: {run_time} seconds.\")\n    \nprint(\"=\"*30)","f767ccc6":"log = log.set_index('Classifier').sort_values('Accuracy', ascending=False)\nlog","0d44636f":"from sklearn.model_selection import GridSearchCV","3389ba44":"model_1 = GradientBoostingClassifier()\nparam_grid_1 = {\n    'loss' : ['deviance', 'exponential'],\n    'max_features' : ['sqrt', 'log2', 'None'],\n    'max_depth' : [2, 3, 4],\n    'learning_rate' : [0.1, 0.2],\n    'n_estimators' : [100, 200],\n    'min_samples_leaf' : [1, 2],\n    'min_samples_split' : [2, 3],\n}\ngscv_1 = GridSearchCV(estimator=model_1, param_grid=param_grid_1, scoring='roc_auc', cv=3, verbose=10)\ngscv_1.fit(X_train, y_train)\n\ny_pred_1 = gscv_1.predict(X_test)\naccuracy_1 = accuracy_score(y_test, y_pred_1)\npredict_proba_1 = gscv_1.predict_proba(X_test)\nlog_loss_1 = log_loss(y_test, predict_proba_1)\n\nprint(f\"Best Hyperparameters: {gscv_1.best_params_}, score: {gscv_1.best_score_}\")\nprint(f\"Accuracy: {accuracy_1}, Log loss: {log_loss_1}\")","88c2b6a4":"model_2 = AdaBoostClassifier()\nparam_grid_2 = {\n    'n_estimators' : [50, 100, 200]\n}\ngscv_2 = GridSearchCV(estimator=model_2, param_grid=param_grid_2, scoring='roc_auc', cv=5, verbose=10)\ngscv_2.fit(X_train, y_train)\n\ny_pred_2 = gscv_2.predict(X_test)\naccuracy_2 = accuracy_score(y_test, y_pred_2)\npredict_proba_2 = gscv_2.predict_proba(X_test)\nlog_loss_2 = log_loss(y_test, predict_proba_2)\n\nprint(f\"Best Hyperparameters: {gscv_2.best_params_}, score: {gscv_2.best_score_}\")\nprint(f\"Accuracy: {accuracy_2}, Log loss: {log_loss_2}\")","f69c6fe0":"model_3 = MLPClassifier()\nparam_grid_3 = {\n    'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n    'learning_rate' : ['constant', 'invscaling', 'adaptive'],\n    'solver' : ['lbfgs', 'sgd', 'adam']\n}\ngscv_3 = GridSearchCV(estimator=model_3, param_grid=param_grid_3, scoring='roc_auc', cv=3, verbose=10)\ngscv_3.fit(X_train, y_train)\n\ny_pred_3 = gscv_3.predict(X_test)\naccuracy_3 = accuracy_score(y_test, y_pred_3)\npredict_proba_3 = gscv_3.predict_proba(X_test)\nlog_loss_3 = log_loss(y_test, predict_proba_3)\n\nprint(f\"Best Hyperparameters: {gscv_3.best_params_}, score: {gscv_3.best_score_}\")\nprint(f\"Accuracy: {accuracy_3}, Log loss: {log_loss_3}\")","e9a70454":"selected_model = gscv_2\n\nselected_model.fit(X, y)\ny_pred = selected_model.predict(df_test)\n\ndf_test['Gaji'] = y_pred","4ff67e01":"df_test['Gaji']","918f87a3":"df_test['Gaji'].to_csv('result.csv')","73991e07":"### 1. Preview raw data","61bcf2ca":"---","9a2092db":"---","3d7cb41c":"### 2. Preview with charts (Green = Gaji_>7jt, Red = Gaji_<=7jt)","7262ff19":"### 4. Testing some Classifiers","5646009e":"### 6. Predict and Save result to csv","a5c7ee64":"# Final Project\n\n## Agung Budiman\n\n### 1st.agung.budiman@gmail.com","e6546331":"### 5. Hyperparameter optimization for best 3","a1b505c0":"### 3. Preprocessing data"}}