{"cell_type":{"c3852bc8":"code","97668aaa":"code","5ac5289d":"code","1ba2327b":"code","30850348":"code","9d3b5e63":"code","44c50b34":"code","c37aa7a1":"code","c63d5a66":"code","137ca260":"code","1932daee":"code","335e44ec":"code","6fc2665a":"code","9f9aa00d":"code","3ace9cf6":"code","42287986":"code","f1682ac5":"code","77033707":"code","4478854d":"code","e1fbfa5a":"code","cae72587":"code","824f79d9":"code","bf581da5":"code","1f31fdd1":"code","d6b62f7d":"code","ed196ed4":"code","4a55595d":"code","fc18854f":"code","40954065":"code","cc4ffbc7":"code","4525dda1":"code","559eb6bd":"code","f2575b57":"code","acd65345":"code","3ad48fca":"code","e8bc3efc":"code","b2b45a0e":"code","648b4574":"code","b16e02c8":"code","f9895b28":"code","8529b02c":"code","adaf165c":"code","46bf02af":"code","fa48924f":"code","1d4fb62f":"code","e6cdf0a3":"code","838e710a":"code","d91438c3":"code","75664bf0":"code","f6d54b28":"code","b4563355":"code","825837dd":"code","809ffa13":"code","61347aab":"code","e3a825b3":"code","104a4eab":"code","5e0a3dbb":"code","f4369abe":"code","b1496fcf":"code","d391c72b":"markdown","cd254dbf":"markdown","ffad26fc":"markdown","bca21d11":"markdown","1e037289":"markdown","a0a7a9f1":"markdown","5bcf1795":"markdown","5315fbc7":"markdown","e001f1d9":"markdown","df48b5c1":"markdown","e1a38d2a":"markdown","6674fd5f":"markdown","ab5c41e5":"markdown","04072675":"markdown","eba7a951":"markdown","4b02f363":"markdown","470a9d51":"markdown","3db88658":"markdown"},"source":{"c3852bc8":"from numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\n#import keras\n#from keras import backend as K\n\nimport tensorflow\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Activation,LeakyReLU,Flatten,Input,Dense, Dropout , GlobalAveragePooling2D,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint,Callback\n\nimport os\n\n\n\n\n# Pre-trained model\nfrom tensorflow.keras.applications.densenet import DenseNet201\n# from keras.models import Sequential\n# from keras.optimizers import Adam\n# from keras import layers\nfrom tensorflow.keras.applications import DenseNet121,DenseNet169,MobileNet\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline","97668aaa":"os.listdir('..\/input')","5ac5289d":"\n\n# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# now we create 7 folders inside 'base_dir':\n\n# train_dir\n    # nv\n    # mel\n    # bkl\n    # bcc\n    # akiec\n    # vasc\n    # df\n \n# val_dir\n    # nv\n    # mel\n    # bkl\n    # bcc\n    # akiec\n    # vasc\n    # df\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN, VALIDATION AND TEST FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\nnv = os.path.join(train_dir, 'nv')\nos.mkdir(nv)\nmel = os.path.join(train_dir, 'mel')\nos.mkdir(mel)\nbkl = os.path.join(train_dir, 'bkl')\nos.mkdir(bkl)\nbcc = os.path.join(train_dir, 'bcc')\nos.mkdir(bcc)\nakiec = os.path.join(train_dir, 'akiec')\nos.mkdir(akiec)\nvasc = os.path.join(train_dir, 'vasc')\nos.mkdir(vasc)\ndf = os.path.join(train_dir, 'df')\nos.mkdir(df)\n\n\n\n# create new folders inside val_dir\nnv = os.path.join(val_dir, 'nv')\nos.mkdir(nv)\nmel = os.path.join(val_dir, 'mel')\nos.mkdir(mel)\nbkl = os.path.join(val_dir, 'bkl')\nos.mkdir(bkl)\nbcc = os.path.join(val_dir, 'bcc')\nos.mkdir(bcc)\nakiec = os.path.join(val_dir, 'akiec')\nos.mkdir(akiec)\nvasc = os.path.join(val_dir, 'vasc')\nos.mkdir(vasc)\ndf = os.path.join(val_dir, 'df')\nos.mkdir(df)\n\n\n\n","1ba2327b":"df_data = pd.read_csv('..\/input\/skin-cancer-mnist-ham10000\/HAM10000_metadata.csv')\n\ndf_data.head()","30850348":"# this will tell us how many images are associated with each lesion_id\ndf = df_data.groupby('lesion_id').count()\n\n# now we filter out lesion_id's that have only one image associated with it\ndf = df[df['image_id'] == 1]\n\ndf.reset_index(inplace=True)\n\ndf.head()","9d3b5e63":"# here we identify lesion_id's that have duplicate images and those that have only\n# one image.\n\ndef identify_duplicates(x):\n    \n    unique_list = list(df['lesion_id'])\n    \n    if x in unique_list:\n        return 'no_duplicates'\n    else:\n        return 'has_duplicates'\n    \n# create a new colum that is a copy of the lesion_id column\ndf_data['duplicates'] = df_data['lesion_id']\n# apply the function to this new column\ndf_data['duplicates'] = df_data['duplicates'].apply(identify_duplicates)\n\ndf_data.head()","44c50b34":"df_data['duplicates'].value_counts()","c37aa7a1":"# now we filter out images that don't have duplicates\ndf = df_data[df_data['duplicates'] == 'no_duplicates']\n\n\ndf.shape","c63d5a66":"df.head()","137ca260":"# now we create a val set using df because we are sure that none of these images\n# have augmented duplicates in the train set\ny = df['dx']\n\n_, df_val = train_test_split(df, test_size=0.17, random_state=101, stratify=y)\n\ndf_val.shape\n","1932daee":"df_val['dx'].value_counts()","335e44ec":"# This set will be df_data excluding all rows that are in the val set\n\n# This function identifies if an image is part of the train\n# or val set.\ndef identify_val_rows(x):\n    # create a list of all the lesion_id's in the val set\n    val_list = list(df_val['image_id'])\n    \n    if str(x) in val_list:\n        return 'val'\n    else:\n        return 'train'\n\n# identify train and val rows\n\n# create a new colum that is a copy of the image_id column\ndf_data['train_or_val'] = df_data['image_id']\n# apply the function to this new column\ndf_data['train_or_val'] = df_data['train_or_val'].apply(identify_val_rows)\n   \n# filter out train rows\ndf_train = df_data[df_data['train_or_val'] == 'train']\n\n\n\nprint(len(df_train))\nprint(len(df_val))\n","6fc2665a":"df_train.head()","9f9aa00d":"df_train['dx'].value_counts()","3ace9cf6":"df_val.head()","42287986":"df_val['dx'].value_counts()","f1682ac5":"# Set the image_id as the index in df_data\ndf_data.set_index('image_id', inplace=True)","77033707":"# Get a list of images in each of the two folders\nfolder_1 = os.listdir('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_1')\nfolder_2 = os.listdir('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_2')\n\n# Get a list of train and val images\ntrain_list = list(df_train['image_id'])\nval_list = list(df_val['image_id'])\n\n\n\n# Transfer the train images\n\nfor image in train_list:\n    \n    fname = image + '.jpg'\n    label = df_data.loc[image,'dx']\n    \n    if fname in folder_1:\n        # source path to image\n        src = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_1', fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        # source path to image\n        src = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_2', fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n\n# Transfer the val images\n\nfor image in val_list:\n    \n    fname = image + '.jpg'\n    label = df_data.loc[image,'dx']\n    \n    if fname in folder_1:\n        # source path to image\n        src = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_1', fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        # source path to image\n        src = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/ham10000_images_part_2', fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n        \n        \n\n        ","4478854d":"# check how many train images we have in each folder\n\nprint(len(os.listdir('base_dir\/train_dir\/nv')))\nprint(len(os.listdir('base_dir\/train_dir\/mel')))\nprint(len(os.listdir('base_dir\/train_dir\/bkl')))\nprint(len(os.listdir('base_dir\/train_dir\/bcc')))\nprint(len(os.listdir('base_dir\/train_dir\/akiec')))\nprint(len(os.listdir('base_dir\/train_dir\/vasc')))\nprint(len(os.listdir('base_dir\/train_dir\/df')))","e1fbfa5a":"# check how many val images we have in each folder\n\nprint(len(os.listdir('base_dir\/val_dir\/nv')))\nprint(len(os.listdir('base_dir\/val_dir\/mel')))\nprint(len(os.listdir('base_dir\/val_dir\/bkl')))\nprint(len(os.listdir('base_dir\/val_dir\/bcc')))\nprint(len(os.listdir('base_dir\/val_dir\/akiec')))\nprint(len(os.listdir('base_dir\/val_dir\/vasc')))\nprint(len(os.listdir('base_dir\/val_dir\/df')))","cae72587":"# note that we are not augmenting class 'nv'\nclass_list = ['mel','bkl','bcc','akiec','vasc','df']\n\nfor item in class_list:\n    \n    # We are creating temporary directories here because we delete these directories later\n    # create a base dir\n    aug_dir = 'aug_dir'\n    os.mkdir(aug_dir)\n    # create a dir within the base dir to store images of the same class\n    img_dir = os.path.join(aug_dir, 'img_dir')\n    os.mkdir(img_dir)\n\n    # Choose a class\n    img_class = item\n\n    # list all images in that directory\n    img_list = os.listdir('base_dir\/train_dir\/' + img_class)\n\n    # Copy images from the class train dir to the img_dir e.g. class 'mel'\n    for fname in img_list:\n            # source path to image\n            src = os.path.join('base_dir\/train_dir\/' + img_class, fname)\n            # destination path to image\n            dst = os.path.join(img_dir, fname)\n            # copy the image from the source to the destination\n            shutil.copyfile(src, dst)\n\n\n    # point to a dir containing the images and not to the images themselves\n    path = aug_dir\n    save_path = 'base_dir\/train_dir\/' + img_class\n\n    # Create a data generator\n    datagen = ImageDataGenerator(\n        rotation_range=180,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.1,\n        horizontal_flip=True,\n        vertical_flip=True,\n        #brightness_range=(0.9,1.1),\n        fill_mode='nearest')\n\n    batch_size = 50\n\n    aug_datagen = datagen.flow_from_directory(path,\n                                           save_to_dir=save_path,\n                                           save_format='jpg',\n                                                    target_size=(224,224),\n                                                    batch_size=batch_size)\n\n\n\n    # Generate the augmented images and add them to the training folders\n    \n    ###########\n    #changed this\n    num_aug_images_wanted = 7000 # total number of images we want to have in each class \n    \n    ###########\n    \n    num_files = len(os.listdir(img_dir))\n    num_batches = int(np.ceil((num_aug_images_wanted-num_files)\/batch_size))\n\n    # run the generator and create about 6000 augmented images\n    for i in range(0,num_batches):\n\n        imgs, labels = next(aug_datagen)\n        \n    # delete temporary directory with the raw image files\n    shutil.rmtree('aug_dir')","824f79d9":"# Check how many train images we now have in each folder.\n# This is the original images plus the augmented images.\n\nprint(len(os.listdir('base_dir\/train_dir\/nv')))\nprint(len(os.listdir('base_dir\/train_dir\/mel')))\nprint(len(os.listdir('base_dir\/train_dir\/bkl')))\nprint(len(os.listdir('base_dir\/train_dir\/bcc')))\nprint(len(os.listdir('base_dir\/train_dir\/akiec')))\nprint(len(os.listdir('base_dir\/train_dir\/vasc')))\nprint(len(os.listdir('base_dir\/train_dir\/df')))","bf581da5":"# Check how many val images we have in each folder.\n\nprint(len(os.listdir('base_dir\/val_dir\/nv')))\nprint(len(os.listdir('base_dir\/val_dir\/mel')))\nprint(len(os.listdir('base_dir\/val_dir\/bkl')))\nprint(len(os.listdir('base_dir\/val_dir\/bcc')))\nprint(len(os.listdir('base_dir\/val_dir\/akiec')))\nprint(len(os.listdir('base_dir\/val_dir\/vasc')))\nprint(len(os.listdir('base_dir\/val_dir\/df')))","1f31fdd1":"print(len(os.listdir('base_dir\/train_dir')))","d6b62f7d":"# plots images with labels within jupyter notebook\n# source: https:\/\/github.com\/smileservices\/keras_utils\/blob\/master\/utils.py\n\ndef plots(ims, figsize=(20,15), rows=4, interp=False, titles=None): # 12,6\n    if type(ims[0]) is np.ndarray:\n        ims = np.array(ims).astype(np.uint8)\n        if (ims.shape[-1] != 3):\n            ims = ims.transpose((0,2,3,1))\n    f = plt.figure(figsize=figsize)\n#     cols = len(ims)\/\/rows if len(ims) % 2 == 0 else len(ims)\/\/rows + 1\n    cols=4\n\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, cols, i+1)\n        sp.axis('Off')\n        if titles is not None:\n            sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i], interpolation=None if interp else 'none')\n        \nplots(imgs, titles=labels) # titles=labels will display the image labels","ed196ed4":"# End of Data Preparation\n### ===================================================================================== ###\n# Start of Model Building","4a55595d":"train_path = 'base_dir\/train_dir'\nvalid_path = 'base_dir\/val_dir'\n\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\nimage_size = 224\n\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)\n\nval_steps = np.ceil(num_val_samples \/ val_batch_size)\nprint(train_steps,val_steps)\n","fc18854f":"\ndatagen = ImageDataGenerator(\n    preprocessing_function= \\\n    tensorflow.keras.applications.mobilenet.preprocess_input)\n\n\n\n\ntrain_batches = datagen.flow_from_directory(train_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=train_batch_size)\n\nvalid_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=val_batch_size)\n\n\ntest_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=1,\n                                            shuffle=False)","40954065":"# from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\n# from tensorflow.keras import backend as K\n# from tensorflow.keras.activations import sigmoid\n\n# def attach_attention_module(net, attention_module):\n#   if attention_module == 'se_block': # SE_block\n#     net = se_block(net)\n#   elif attention_module == 'cbam_block': # CBAM_block\n#     net = cbam_block(net)\n#   else:\n#     raise Exception(\"'{}' is not supported attention module!\".format(attention_module))\n\n#   return net\n\n# def se_block(input_feature, ratio=8):\n# \t\"\"\"Contains the implementation of Squeeze-and-Excitation(SE) block.\n# \tAs described in https:\/\/arxiv.org\/abs\/1709.01507.\n# \t\"\"\"\n\t\n# \tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n# \tchannel = input_feature._keras_shape[channel_axis]\n\n# \tse_feature = GlobalAveragePooling2D()(input_feature)\n# \tse_feature = Reshape((1, 1, channel))(se_feature)\n# \tassert se_feature._keras_shape[1:] == (1,1,channel)\n# \tse_feature = Dense(channel \/\/ ratio,\n# \t\t\t\t\t   activation='relu',\n# \t\t\t\t\t   kernel_initializer='he_normal',\n# \t\t\t\t\t   use_bias=True,\n# \t\t\t\t\t   bias_initializer='zeros')(se_feature)\n# \tassert se_feature._keras_shape[1:] == (1,1,channel\/\/ratio)\n# \tse_feature = Dense(channel,\n# \t\t\t\t\t   activation='sigmoid',\n# \t\t\t\t\t   kernel_initializer='he_normal',\n# \t\t\t\t\t   use_bias=True,\n# \t\t\t\t\t   bias_initializer='zeros')(se_feature)\n# \tassert se_feature._keras_shape[1:] == (1,1,channel)\n# \tif K.image_data_format() == 'channels_first':\n# \t\tse_feature = Permute((3, 1, 2))(se_feature)\n\n# \tse_feature = multiply([input_feature, se_feature])\n# \treturn se_feature\n\n# def cbam_block(cbam_feature, ratio=8):\n# \t\"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n# \tAs described in https:\/\/arxiv.org\/abs\/1807.06521.\n# \t\"\"\"\n\t\n# \tcbam_feature = channel_attention(cbam_feature, ratio)\n# \tcbam_feature = spatial_attention(cbam_feature)\n# \treturn cbam_feature\n\n# def channel_attention(input_feature, ratio=8):\n\t\n# \tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n# \tchannel = input_feature._keras_shape[channel_axis]\n\t\n# \tshared_layer_one = Dense(channel\/\/ratio,\n# \t\t\t\t\t\t\t activation='relu',\n# \t\t\t\t\t\t\t kernel_initializer='he_normal',\n# \t\t\t\t\t\t\t use_bias=True,\n# \t\t\t\t\t\t\t bias_initializer='zeros')\n# \tshared_layer_two = Dense(channel,\n# \t\t\t\t\t\t\t kernel_initializer='he_normal',\n# \t\t\t\t\t\t\t use_bias=True,\n# \t\t\t\t\t\t\t bias_initializer='zeros')\n\t\n# \tavg_pool = GlobalAveragePooling2D()(input_feature)    \n# \tavg_pool = Reshape((1,1,channel))(avg_pool)\n# \tassert avg_pool._keras_shape[1:] == (1,1,channel)\n# \tavg_pool = shared_layer_one(avg_pool)\n# \tassert avg_pool._keras_shape[1:] == (1,1,channel\/\/ratio)\n# \tavg_pool = shared_layer_two(avg_pool)\n# \tassert avg_pool._keras_shape[1:] == (1,1,channel)\n\t\n# \tmax_pool = GlobalMaxPooling2D()(input_feature)\n# \tmax_pool = Reshape((1,1,channel))(max_pool)\n# \tassert max_pool._keras_shape[1:] == (1,1,channel)\n# \tmax_pool = shared_layer_one(max_pool)\n# \tassert max_pool._keras_shape[1:] == (1,1,channel\/\/ratio)\n# \tmax_pool = shared_layer_two(max_pool)\n# \tassert max_pool._keras_shape[1:] == (1,1,channel)\n\t\n# \tcbam_feature = Add()([avg_pool,max_pool])\n# \tcbam_feature = Activation('sigmoid')(cbam_feature)\n\t\n# \tif K.image_data_format() == \"channels_first\":\n# \t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\t\n# \treturn multiply([input_feature, cbam_feature])\n\n# def spatial_attention(input_feature):\n# \tkernel_size = 7\n\t\n# \tif K.image_data_format() == \"channels_first\":\n# \t\tchannel = input_feature._keras_shape[1]\n# \t\tcbam_feature = Permute((2,3,1))(input_feature)\n# \telse:\n# \t\tchannel = input_feature._keras_shape[-1]\n# \t\tcbam_feature = input_feature\n\t\n# \tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n# \tassert avg_pool._keras_shape[-1] == 1\n# \tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n# \tassert max_pool._keras_shape[-1] == 1\n# \tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n# \tassert concat._keras_shape[-1] == 2\n# \tcbam_feature = Conv2D(filters = 1,\n# \t\t\t\t\tkernel_size=kernel_size,\n# \t\t\t\t\tstrides=1,\n# \t\t\t\t\tpadding='same',\n# \t\t\t\t\tactivation='sigmoid',\n# \t\t\t\t\tkernel_initializer='he_normal',\n# \t\t\t\t\tuse_bias=False)(concat)\t\n# \tassert cbam_feature._keras_shape[-1] == 1\n\t\n# \tif K.image_data_format() == \"channels_first\":\n# \t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\t\t\n# \treturn multiply([input_feature, cbam_feature])","cc4ffbc7":"# mobile = MobileNet(weights='imagenet', include_top=False,\n#                      input_shape=(224,224,3))","4525dda1":"mobile = DenseNet121(weights='..\/input\/densenet-keras\/DenseNet-BC-121-32-no-top.h5', \n                     include_top=False,\n                     input_shape=(224,224,3))","559eb6bd":"mobile.summary()","f2575b57":"# model_type = mobie+'_'+attention_module","acd65345":"type(mobile.layers)","3ad48fca":"# How many layers does MobileNet have?\nlen(mobile.layers)","e8bc3efc":"# Define Top2 and Top3 Accuracy\n\nfrom tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n\ndef top_3_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n\ndef top_2_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=2)","b2b45a0e":"model = Sequential()\nmodel.add(mobile)\n\nmodel.add(layers.GlobalAveragePooling2D())\n# changed this\nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(7, activation='softmax'))\n    \n\n\n\n","648b4574":"model.summary()","b16e02c8":"model.compile(Adam(lr=0.001), loss='categorical_crossentropy', \n              metrics=[categorical_accuracy, top_2_accuracy, top_3_accuracy])\n\n","f9895b28":"model.summary()","8529b02c":"# Get the labels that are associated with each index\nprint(valid_batches.class_indices)","adaf165c":"# Add weights to try to make the model more sensitive to melanoma\n\n# class_weights={\n#     0: 1.0, # akiec\n#     1: 1.0, # bcc\n#     2: 1.0, # bkl\n#     3: 1.0, # df\n#     4: 3.0, # mel # Try to make the model more sensitive to Melanoma.\n#     5: 1.0, # nv\n#     6: 1.0, # vasc\n# }","46bf02af":"\nfilepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_top_3_accuracy', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_top_3_accuracy', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_batches, steps_per_epoch=train_steps,\n                    validation_data=valid_batches,\n                    validation_steps=val_steps,\n                    epochs=30, verbose=1,\n                   callbacks=callbacks_list)\n","fa48924f":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","1d4fb62f":"# Here the the last epoch will be used.\n\nval_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\nmodel.evaluate_generator(test_batches, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_cat_acc:', val_cat_acc)\nprint('val_top_2_acc:', val_top_2_acc)\nprint('val_top_3_acc:', val_top_3_acc)","e6cdf0a3":"# Here the best epoch will be used.\n\nmodel.load_weights('model.h5')\n\nval_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\nmodel.evaluate_generator(test_batches, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_cat_acc:', val_cat_acc)\nprint('val_top_2_acc:', val_top_2_acc)\nprint('val_top_3_acc:', val_top_3_acc)","838e710a":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\ntrain_top2_acc = history.history['top_2_accuracy']\nval_top2_acc = history.history['val_top_2_accuracy']\ntrain_top3_acc = history.history['top_3_accuracy']\nval_top3_acc = history.history['val_top_3_accuracy']\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training cat acc')\nplt.plot(epochs, val_acc, 'b', label='Validation cat acc')\nplt.title('Training and validation cat accuracy')\nplt.legend()\nplt.figure()\n\n\nplt.plot(epochs, train_top2_acc, 'bo', label='Training top2 acc')\nplt.plot(epochs, val_top2_acc, 'b', label='Validation top2 acc')\nplt.title('Training and validation top2 accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, train_top3_acc, 'bo', label='Training top3 acc')\nplt.plot(epochs, val_top3_acc, 'b', label='Validation top3 acc')\nplt.title('Training and validation top3 accuracy')\nplt.legend()\n\n\nplt.show()","d91438c3":"# Get the labels of the test images.\n\ntest_labels = test_batches.classes","75664bf0":"# We need these to plot the confusion matrix.\ntest_labels","f6d54b28":"# Print the label associated with each class\ntest_batches.class_indices","b4563355":"# make a prediction\npredictions = model.predict_generator(test_batches, steps=len(df_val), verbose=1)","825837dd":"predictions.shape","809ffa13":"# Source: Scikit Learn website\n# http:\/\/scikit-learn.org\/stable\/auto_examples\/\n# model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n# selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n\n\n","61347aab":"test_labels.shape","e3a825b3":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))","104a4eab":"test_batches.class_indices","5e0a3dbb":"# Define the labels of the class indices. These need to match the \n# order shown above.\ncm_plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel','nv', 'vasc']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","f4369abe":"# Get the index of the class with the highest probability score\ny_pred = np.argmax(predictions, axis=1)\n\n# Get the labels of the test images.\ny_true = test_batches.classes","b1496fcf":"from sklearn.metrics import classification_report\n\n# Generate a classification report\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\n\nprint(report)","d391c72b":"### Set Up the Generators","cd254dbf":"### Train the Model","ffad26fc":"**Recall** = Given a class, will the classifier be able to detect it?<br>\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?<br>\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.","bca21d11":"### ***Mobile net***","1e037289":"### Transfer the Images into the Folders","a0a7a9f1":"# **Dense net**","5bcf1795":"### Evaluate the model using the val set","5315fbc7":"### Create Train and Val Sets","e001f1d9":"### Copy the train images  into aug_dir","df48b5c1":"### Visualize  augmented images","e1a38d2a":"### Plot the Training Curves","6674fd5f":"### Create the directory structure\n\nIn these folders we will store the images that will later be fed to the Keras generators. ","ab5c41e5":"### Create a train set that excludes images that are in the val set","04072675":"### Create a stratified val set","eba7a951":"Thabks to Marsh for his amazing notebook which helped me a lot! (https:\/\/www.kaggle.com\/vbookshelf\/skin-lesion-analyzer-tensorflow-js-web-app)\n\nIn this notebook I tried to classify the skin leason images using densenet to make a better result. I also edited the attention module compatable with this piece of code. But it takes too much time to train while using the attention module in kaggle so I had to give that up. I kept the attention module code so that if any one needs it the can take it from here as I had to stuggle to make this part run. Happy kaggeling! \n","4b02f363":"### Create a Confusion Matrix","470a9d51":"# ***Attention module***","3db88658":"### Generate the Classification Report"}}