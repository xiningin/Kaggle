{"cell_type":{"9d0ff270":"code","bb5cd7b0":"code","dcb08122":"code","af3b24a2":"code","6d06a286":"code","894469f2":"code","44aa124c":"code","8099c63d":"code","8480d43f":"code","3ecfc103":"code","a421ec89":"code","19a5f32b":"code","fdc57387":"code","467de14f":"code","4f422198":"code","0bdece96":"code","3e572e63":"code","e80c9e32":"code","08b1811f":"code","12a55c81":"code","3886a64f":"code","1bf1e7bc":"code","a94a96ba":"code","f01e0d05":"code","e0f4939b":"code","9c8f54fb":"code","c023eba6":"code","8e92509b":"code","473478ad":"code","02e33139":"code","c1d31a1d":"code","b4879cf8":"code","b852c617":"code","872234fc":"code","2c864bac":"code","191cef63":"code","acf8ad32":"code","40dc0d0e":"code","2d2e6db9":"code","be507172":"code","f7f658f3":"code","d2ca24aa":"code","66b247a8":"code","f35d57e6":"code","c3950c5c":"code","ee051842":"code","c01dc5b2":"code","5dc19851":"code","d32fbd3e":"code","5dcbe23f":"code","fc662413":"code","45e39eac":"code","6cf09766":"code","6c7b722f":"code","c01189fd":"code","19bd487b":"code","06fe80f5":"code","217c31c6":"code","013d1220":"code","633031a2":"code","265d8bc7":"code","0a628dc6":"code","d404dfc5":"code","bb1c1cb0":"code","8947e21c":"code","835d6f24":"code","0160b8b0":"code","ca921ade":"code","716f338e":"code","a8e8daf3":"code","a9431b1a":"code","a71e7115":"code","f2cd78e6":"code","a38d1fe5":"code","2ad9a092":"code","ee785e35":"markdown","f0a982e9":"markdown","3e171849":"markdown","109028a4":"markdown","af275813":"markdown","74aa5494":"markdown","aaa30f91":"markdown","d7ad063f":"markdown","1829090f":"markdown","d8ea0041":"markdown","8ee4e006":"markdown","b16400ea":"markdown","3ae03d7b":"markdown","462818d7":"markdown","4aef9f59":"markdown","fa4f0302":"markdown","bfc7630a":"markdown","ca298e0a":"markdown","8365cccc":"markdown","954733e2":"markdown","c961b975":"markdown","e3fe0857":"markdown","e195428d":"markdown","b74c6988":"markdown","dca0c707":"markdown","aea3fbfd":"markdown","56b08d00":"markdown","5fe0304a":"markdown","7607bee4":"markdown","7e25e1e3":"markdown","68fe7ec8":"markdown","078a6fb6":"markdown","ac8cfc5b":"markdown","82e8d8da":"markdown"},"source":{"9d0ff270":"!pip install sentence-transformers","bb5cd7b0":"#importing the required libraries\nimport glob\nimport json\nimport logging\nimport os\nimport prettytable\nimport pickle\nimport re\nimport shutil\nimport tqdm\nimport textwrap\nimport warnings\nimport hashlib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport seaborn as sns\nimport plotly.offline as py\nimport scattertext as st\n\nfrom scipy.spatial.distance import cdist\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport nltk\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer, word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom nltk.translate.bleu_score import sentence_bleu\n\nimport torch\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers import models, SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModel\nfrom transformers import BertForQuestionAnswering, BertTokenizer\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom absl import logging\nfrom IPython.core.display import display, HTML\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom ipywidgets import *\nfrom wordcloud import WordCloud\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nwarnings.simplefilter('ignore')\n\nprint(\"Packages imported successfully\")","dcb08122":"from IPython.core.display import display, HTML \ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))\npd.set_option('display.max_columns', 500)","af3b24a2":"DATA_PATH = '\/kaggle\/input\/datatask6'\nOUTPUT_PATH = '\/kaggle\/working\/'\nMODEL_PATH = '\/kaggle\/input\/models\/scibert-nli\/'","6d06a286":"# Specify the Kaggle Username and Key to use the Kaggle Api\n\n# os.environ['KAGGLE_USERNAME'] = '*************'\n# os.environ['KAGGLE_KEY'] = '****************'","894469f2":"# from kaggle.api.kaggle_api_extended import KaggleApi\n\n# api = KaggleApi()\n# api.authenticate()\n\n# api.dataset_download_files(dataset=\"allen-institute-for-ai\/CORD-19-research-challenge\", path=DATA_PATH, unzip=True)","44aa124c":"# HTML('''<script>\n# code_show=true; \n# function code_toggle() {\n#  if (code_show){\n#  $('div.input').hide();\n#  } else {\n#  $('div.input').show();\n#  }\n#  code_show = !code_show\n# } \n# $( document ).ready(code_toggle);\n# <\/script>\n# The raw code for this IPython notebook is by default hidden for easier reading.\n# To toggle on\/off the raw code, click <a href=\"javascript:code_toggle()\">here<\/a>.''')","8099c63d":"# Helper class to extract data and convert it to df\nclass PaperReader:\n    \n    # initializing\n    def __init__(self, root_path):\n        self.root_path = root_path\n        self.filenames = glob.glob('{}\/**\/*.json'.format(root_path), recursive=True)\n        print(str(len(self.filenames))+' files were found')\n    \n    # load files\n    def load_files(self):\n        raw_files = []\n\n        for filename in tqdm(self.filenames):\n            file = json.load(open(filename, 'rb'))\n            raw_files.append(file)\n\n        return raw_files\n    \n    # extract values from keys\n    def extract_value(self, key, dictionary):\n        \n        for k, v in dictionary.items():\n            \n            if k == key:\n                yield v\n                break\n            elif isinstance(v, dict):\n                for result in self.extract_value(key, v):\n                    yield result\n            elif isinstance(v, list):\n                \n                for d in v:\n                    if type(d) == dict:\n                        for result in self.extract_value(key, d):\n                            yield result  \n\n    # Function to generate Clean DF\n    def generate_clean_df(self):\n        \n        raw_files = self.load_files()\n        cleaned_files = []\n\n        for content in tqdm(raw_files):\n            \n            # extract paper_id ( sha)\n            paper_id = list(self.extract_value('paper_id', content))\n            paper_id = paper_id[0]\n\n            if 'metadata' in content.keys():\n                # extract title\n                title = list(self.extract_value('metadata', content)) \n                title = title[0]['title']\n#             else:\n#                 title = np.nan\n\n            # extract abstract\n            if 'abstract' in content.keys():\n                abstract = list(self.extract_value('abstract', content)) \n                abstract = ' \\n\\n\\n '.join([element['text'] for element in abstract[0] if len(element['text']) > 200 ])\n#            else:\n#                abstract = np.nan\n\n            # extract body\n            if 'body_text' in content.keys():\n                body = list(self.extract_value('body_text', content)) \n                body = ' \\n\\n\\n '.join([element['section'] + ' \\n ' + element['text'] for element in body[0] if len(element['text']) > 200])\n#             else:\n#                 body = np.nan\n\n            # extract bib_entries\n            if 'bib_entries' in content.keys():\n                bib_entries = list(self.extract_value('bib_entries', content)) \n                bib = []\n                for edx, el in enumerate(bib_entries[0]):\n                    #index = bib_entries[0][el]['ref_id']\n                    bib_title = bib_entries[0][el]['title']\n                    bib.append(bib_title)\n\n                bib = ' \\n\\n'.join(bib)\n#             else:\n#                 bib = np.nan\n\n            # extract red entries\n            if 'ref_entries' in content.keys():\n                ref_entry = list(self.extract_value(\"ref_entries\",  content)) \n                ref_ent = []\n                for rdx, re in enumerate(ref_entry[0]): \n                    #print(re)\n                    #print(ref_entry[0][re])\n                    ref_ent.append(ref_entry[0][re]['type'] + ' \\n ' + ref_entry[0][re]['text'] )\n                ref_entry = ' \\n\\n\\n '.join(ref_ent)\n#             else:\n#                 ref_entry = np.nan\n\n            # extract back matter\n            if 'back_matter' in content.keys():\n                back_matter = list(self.extract_value(\"back_matter\", content)) \n                back_matter = ' \\n\\n\\n '.join([element['section'] + ' \\n ' + element['text'] for element in back_matter[0]])\n#             else:\n#                 back_matter = np.nan\n\n            # create a dataframe of extracted json file\n            file_dataframe = pd.DataFrame.from_dict({   'paper_id': [paper_id], \n                                            'title': [title],\n                                            'abstract': [abstract],\n                                            'text' : [body],\n                                            'bib_entries': [bib],\n                                            'ref_entries': [ref_entry],\n                                            'back_matter': [back_matter]\n                                        })\n\n            # append file_dataframe to file_paths_df list\n            cleaned_files.append(file_dataframe)\n        \n        # concat all the dataframes present in file_paths_df list         \n        data = pd.concat(cleaned_files)\n        \n        # reset index of data dataframe\n        data.reset_index(inplace = True, drop = True)\n        \n        return data","8480d43f":"#PR = PaperReader(INPUT_PATH)","3ecfc103":"# generate Dataframe\n#papers_df = PR.generate_clean_df()\n#papers_df.head()","a421ec89":"# save extracted data\n# papers_df.to_excel(DATA_PATH + 'extracted_data.xlsx', index = False)","19a5f32b":"# helper function to locate the json files\ndef find_file(filename):\n    result = []\n    for root, dir, files in os.walk(DATA_PATH):\n        if filename in files:\n            result.append(os.path.join(root, filename))\n    return result[0]","fdc57387":"task_questions = [\"Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.\",\n            \"Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments.\",\n            \"Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.\",\n            \"Methods to control the spread in communities, barriers to compliance and how these vary among different populations\",\n            \"Models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status.\",\n            \"Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs.\",\n            \"Research on why people fail to comply with public health advice, even if they want to do so (e.g., social or financial costs may be too high).\",\n            \"Research on the economic impact of this or any pandemic. This would include identifying policy and programmatic alternatives that lessen\/mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses, treatment, and needed care, regardless of ability to pay.\"]","467de14f":"questions = [\"How to scale up Non Pharmaceutical Interventions for COVID-19 ?\",\n            \"What are the methods to compare Non Pharmaceutical Interventions for COVID-19 ?\",\n            \"What are the effect of travel bans for COVID-19 ?\",\n            \"What are the methods to control the community spread for COVID-19 ?\",\n            \"What are the Non Pharmaceutical Interventions models to predict costs and benefits for COVID-19 ?\",\n            \"What are the required policy changes to enable the compliance of individuals with limited resources for COVID-19 ?\",\n            \"Why people fail to comply with public health advice for COVID-19 ?\",\n            \"What is the economic impact of pandemic ?\"]","4f422198":"question = questions[2]\nprint(f\"This is the question you are looking at: \")\nprint(f\"  ==>   {question}\")","0bdece96":"papers_df = pd.read_excel(DATA_PATH + '\/extracted_data.xlsx')","3e572e63":"papers_df.columns","e80c9e32":"papers_df.head()","08b1811f":"papers_df.isnull().sum()","12a55c81":"papers_df['Embedding_Col'] = papers_df['abstract'].copy(deep= True)\npapers_df.head()","3886a64f":"# creating mask where \"abstract\" is missing\nabs_mask = papers_df['Embedding_Col'].isna()\n#abs_mask","1bf1e7bc":"# copy title to Embdding_Col where abstract is missing\npapers_df['Embedding_Col'][abs_mask] = papers_df['title'][abs_mask].copy(deep = True)","a94a96ba":"# create mask for not null values in Embdding_Col column \nabs_sec_mask = papers_df['Embedding_Col'].notna()\nabs_sec_mask.value_counts()","f01e0d05":"working_Data = papers_df[abs_sec_mask].copy(deep = True)","e0f4939b":"working_Data.isnull().sum()","9c8f54fb":"# Model Training part\n\n# if not os.path.isdir(MODEL_PATH+'SciBERT-NLI Pretrained\/):\n#     os.mkdir(MODEL_PATH+'SciBERT-NLI Pretrained\/)\n# else:\n#     pass\n\n\n# tokenizer = AutoTokenizer.from_pretrained(\"gsarti\/scibert-nli\")\n\n# model = AutoModel.from_pretrained(\"gsarti\/scibert-nli\")\n\n# model.save_pretrained(MODEL_PATH+'SciBERT-NLI Pretrained\/)\n\n# tokenizer.save_pretrained(MODEL_PATH+'SciBERT-NLI Pretrained\/)\n# word_embedding_model = models.BERT(MODEL_PATH+'SciBERT-NLI Pretrained\/)\n\n# pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n\n#                        pooling_mode_mean_tokens=True,\n\n#                        pooling_mode_cls_token=False,\n\n#                        pooling_mode_max_tokens=False)\n\n# model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\n# shutil.rmtree(MODEL_PATH+'SciBERT-NLI Pretrained\/)\n\n# model.save(MODEL_PATH+'SciBERT-NLI Pretrained\/)","c023eba6":"# load the scibert-nli model\nsciBert_model = SentenceTransformer(MODEL_PATH)","8e92509b":"# remove duplicate values \nworking_Data.drop_duplicates(['Embedding_Col'], keep='first', inplace=True)","473478ad":"metadata_df = pd.read_csv(DATA_PATH + '\/metadata.csv')","02e33139":"metadata_df.rename(columns={'sha': 'paper_id'}, inplace=True)","c1d31a1d":"merged_data = pd.merge(working_Data, \n                       metadata_df[['paper_id','cord_uid', 'source_x', 'publish_time', 'url']], \n                      on = 'paper_id', \n                       how='left')","b4879cf8":"merged_data.head()","b852c617":"str_mask = merged_data['Embedding_Col'].str.contains('2019-nCoV')","872234fc":"merged_data['Embedding_Col'][str_mask]  = merged_data['Embedding_Col'][str_mask].str.replace('2019-nCoV', 'covid-19')","2c864bac":"merged_data.drop_duplicates(['Embedding_Col'], keep='first', inplace=True)","191cef63":"# merged_data.to_excel(DATA_PATH + '\/merged_data.xlsx', index = False)","acf8ad32":"# merged_data = pd.read_excel(DATA_PATH + '\/merged_data.xlsx')","40dc0d0e":"merged_data['Embedding_Col'] = merged_data['Embedding_Col'].str.strip()","2d2e6db9":"corpus = [re.sub(' \\n\\n\\n ','',x) for x in merged_data['Embedding_Col'].to_list()]","be507172":"# create embedding for the text in Embdding_Col column \n# fulldfembedding = sciBert_model.encode(corpus, show_progress_bar=True)","f7f658f3":"# with open(DATA_PATH + '\/Abstract_Title_embd.pkl', 'wb') as emb:\n#     pickle.dump(fulldfembedding, emb)","d2ca24aa":"# load pickled file of embeddings\nwith open(DATA_PATH + '\/Abstract_Title_embd.pkl', 'rb') as emb:\n    fulldfembedding = pickle.load(emb)","66b247a8":"def ask_question(query, model, corpus, corpus_embed, top_k=20):\n    \"\"\"\n    Adapted from https:\/\/www.kaggle.com\/dattaraj\/risks-of-covid-19-ai-driven-q-a\n    \"\"\"\n    queries = [query]\n    query_embeds = model.encode(queries, show_progress_bar=False)\n    for query, query_embed in zip(queries, query_embeds):\n        distances = scipy.spatial.distance.cdist([query_embed], corpus_embed, \"cosine\")[0]\n        distances = zip(range(len(distances)), distances)\n        distances = sorted(distances, key=lambda x: x[1])\n        results = []\n        for count, (idx, distance) in enumerate(distances[0:top_k]):\n            results.append([count + 1, idx, corpus[idx], round(1 - distance, 4)])\n    return results\n\n\ndef show_answers(results):\n    table = prettytable.PrettyTable(\n        ['Rank', 'S.No.', 'Embedding Column', 'Score']\n    )\n    for res in results:\n        rank = res[0]\n        sno = res[1]\n        text = res[2]\n        text = textwrap.fill(text, width=75)\n        text = text + '\\n\\n'\n        score = res[3]\n        table.add_row([\n            rank,\n            sno,\n            text,\n            score\n        ])\n    print('\\n')\n    print(str(table))\n    print('\\n')","f35d57e6":"# get the results from the question 3 -> \"What are the effect of travel bans for COVID-19 ?\"\nquestion = questions[2]\nresults = ask_question(question, sciBert_model, corpus, fulldfembedding)\n#results\n#question","c3950c5c":"#@supress_stdout\ndef hash_results(results, merged_data):\n    hashed_paras = []\n    context = []\n    para_from_paperid = {}\n    para_from_papertitle = {}\n    para_from_paperurl = {}\n\n    for result in results:\n        index = result[1]\n        paper_title = merged_data['title'].iloc[index]\n        #print(f\"result: {paper_title}\")\n        paper_sha = merged_data['paper_id'].iloc[index]\n        paper_cord_uid = merged_data['cord_uid'].iloc[index]\n        paper_url = merged_data['url'].iloc[index]\n        #print(f\"type of paper_sha is: {type(paper_sha)} paper_sha is: {paper_sha}\")\n        body = merged_data['text'].iloc[index]\n        if isinstance(body, str):\n            for para in body.split(' \\n\\n\\n '):\n                mystring = para.replace('\\n', '')\n\n                context.append(mystring)\n                #mystring = 'b' + \"'\" +mystring\n                # Assumes the default UTF-8\n                hash_object = hashlib.sha256(mystring.encode(\"utf-8\"))\n                hex_dig = hash_object.hexdigest()\n                hashed_paras.append(hex_dig)\n                para_from_paperid[hex_dig] = merged_data['paper_id'].iloc[1306]\n                para_from_papertitle[hex_dig] = paper_title\n                para_from_paperurl[hex_dig] = paper_url\n    \n    return hashed_paras,context, para_from_paperid,para_from_papertitle, para_from_paperurl    ","ee051842":"hashed_paras,context, para_from_paperid,para_from_papertitle, para_from_paperurl = hash_results(results, merged_data)","c01dc5b2":"show_answers(results)","5dc19851":"context_list = [x for x in context if len(x) > 150]","d32fbd3e":"def find_top_para_bert(context, question):\n    # create embeddings for paragraphs \n    para_embeddings = sciBert_model.encode(context, show_progress_bar=True)\n    # create embeddings for question \n    query_embed = sciBert_model.encode(question, show_progress_bar=True)\n    for query, query_emb in zip(question, query_embed):\n        distances = cdist([query_emb], para_embeddings, \"cosine\")[0]\n        distances = zip(range(len(distances)), distances)\n        distances = sorted(distances, key=lambda x: x[1])\n        results = []\n        for count, (idx, distance) in enumerate(distances[0:10]):\n            results.append([count+1, idx, context[idx], round(1 - distance, 4) ])\n    return results\n    ","5dcbe23f":"top_para_bert = find_top_para_bert(context_list, question)\nshow_answers(top_para_bert)","fc662413":"def find_top_para_CVect(context, question):\n    para_list = context\n    #tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    count_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2))\n    count_vec_total_text = count_vectorizer.fit_transform(para_list)\n    count_vec_question = count_vectorizer.transform([question])\n    distance_array = cosine_similarity(count_vec_question, count_vec_total_text)\n    distance_array = distance_array[0]\n    \n    top_n = 10\n    top_passages = []\n    for index in distance_array.argsort()[::-1][:top_n]:\n        #print(index, ' ', para_list[index])\n        #print('-' * 120)\n        top_passages.append((index, para_list[index]))\n    return top_passages","45e39eac":"top_para_CVec = find_top_para_CVect(context_list, question)","6cf09766":"for index, para in top_para_CVec:\n    print(index, para)\n    print('-' * 120)","6c7b722f":"tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\nsquad_model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")","c01189fd":"def squad_bert_results(top_para_bert, top_para_CVec):\n    cv_indexes = [i[0] for i in top_para_CVec ]\n    scibert_indexes = [x[1] for x in top_para_bert]\n    indices  = set(cv_indexes + scibert_indexes )\n    countVec_para_list = [x[1].replace(' \\n ', ' ') for x in top_para_CVec if x[0] in indices ]\n    sciBert_para_list = [x[2].replace(' \\n ', ' ') for x in top_para_bert if x[1] in indices]\n    all_paras = countVec_para_list + sciBert_para_list\n    predicted_answer_list = []\n    predicted_passage_list = []\n    results = {}\n    for padx, passage in enumerate(all_paras):\n        inputs = tokenizer.encode_plus(question, passage, add_special_tokens=True, return_tensors=\"pt\",max_length=512)\n        input_ids = inputs[\"input_ids\"].tolist()[0]\n\n        text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n        answer_start_scores, answer_end_scores = squad_model(**inputs)\n\n        answer_start = torch.argmax(answer_start_scores)  # Get the most likely beginning of answer with the argmax of the score\n        answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n\n        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n\n        Flag = False\n        hash_object = hashlib.sha256(passage.encode(\"utf-8\"))\n        hex_dig = hash_object.hexdigest()\n        results[padx] = [question, answer,passage, para_from_paperid[hex_dig], para_from_papertitle[hex_dig], para_from_paperurl[hex_dig] ]\n        if len(answer) >10 and '[CLS]' not in answer and '[SEP]' not in answer :\n            predicted_answer_list.append([padx, answer])\n            Flag = True\n            hash_object = hashlib.sha256(passage.encode(\"utf-8\"))\n            hex_dig = hash_object.hexdigest()\n            results[padx] = [Flag, question, answer,passage, para_from_paperid[hex_dig], para_from_papertitle[hex_dig], para_from_paperurl[hex_dig] ]\n            predicted_passage_list.append(passage)\n            \"\"\"\n            print(f\"==============================================================\")\n            print(f\"QUESTION: {question}\")\n            print(f\"--------------------------------------------------------------\")\n            print(f\"ANSWER: {answer}\\n\")\n            print(f\"this is from paragraph: \")\n            print(passage)\n            print(f\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n            print(f\"paper cord_uid: {para_from_paperid[hex_dig]}\")\n            print(f\"paper title: {para_from_papertitle[hex_dig]}\")\n            print(f\"paper url: {para_from_paperurl[hex_dig]}\")\n            print(f\"==============================================================\")  \n            \"\"\"\n    return results, predicted_answer_list","19bd487b":"squad_results, predicted_answer_list = squad_bert_results(top_para_bert, top_para_CVec)","06fe80f5":"predicted_answer_list","217c31c6":"def bleu_post_process(predicted_answer_list):\n    predicted_answer = predicted_answer_list\n    \n    bleu_matrix = np.zeros((len(predicted_answer),len(predicted_answer)))\n    for idx, i in enumerate(predicted_answer):\n        for jdx,j in enumerate(predicted_answer):\n            if idx != jdx:\n                reference = [word_tokenize(predicted_answer[idx][1])]\n                candidate = word_tokenize(predicted_answer[jdx][1])\n                bleu_matrix[idx,jdx] = sentence_bleu(reference, candidate, weights=(0, 1, 0, 0))   \n            elif idx == jdx:\n                bleu_matrix[idx,jdx] = 1\n    twoGram=pd.DataFrame(bleu_matrix)\n    elements =[]\n\n    elements2remove = []\n\n    for row in range(0,len(twoGram)):\n        elements.append([row,twoGram.iloc[row][(twoGram.iloc[row] >=0.001) & (twoGram.iloc[row] != 1)].index])\n\n    for pdx, p in enumerate(elements):\n        if len(p[1]) >0 :\n            for item in p[1]:\n                if item > pdx:\n                    elements2remove.append(item)\n                    \n    remove_indices = list(set(elements2remove))\n    \n    Processed_answers = [[j,i] for j, i in enumerate(predicted_answer) if j not in remove_indices]\n    \n    return twoGram, Processed_answers\n\ntwoGram, Processed_answers =bleu_post_process(predicted_answer_list)\n\ncm = sns.light_palette(\"blue\", as_cmap=True)\n#twoGram=pd.DataFrame(bleu_matrix)\nprint('\\n')\nprint(\"2-Gram BLEU Score Matrix\")\ntwoGram_Style=twoGram.style.background_gradient(cmap=cm)\ndisplay(twoGram_Style)","013d1220":"for pdx, p in enumerate(Processed_answers):\n    print(f\"==============================================================\")\n    print(f\"QUESTION: {squad_results[p[1][0]][1]}\")\n    print(f\"--------------------------------------------------------------\")\n    print(f\"ANSWER: {squad_results[p[1][0]][2]}\\n\")\n    print(f\"this is from paragraph: \")\n    print(squad_results[p[1][0]][3])\n    print(f\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n    print(f\"paper cord_uid: {squad_results[p[1][0]][4]}\")\n    print(f\"paper title: {squad_results[p[1][0]][5]}\")\n    print(f\"paper url: {squad_results[p[1][0]][6]}\")\n    print(f\"==============================================================\")  \n    \n","633031a2":"solutions = {}\nfor qdx, question in enumerate(questions):\n    results = ask_question(question, sciBert_model, corpus, fulldfembedding)\n    hashed_paras,context, para_from_paperid,para_from_papertitle, para_from_paperurl = hash_results(results, merged_data)\n    top_para_bert = find_top_para_bert(context, question)\n    top_para_CVec = find_top_para_CVect(context, question)\n    squad_results, predicted_answer_list = squad_bert_results(top_para_bert, top_para_CVec)\n    twoGram, Processed_answers =bleu_post_process(predicted_answer_list)\n    df_list = []\n    for pdx, p in enumerate(Processed_answers):\n        df = pd.DataFrame({'QUESTION': [squad_results[p[1][0]][1]],    \n                            'paper cord_uid': [squad_results[p[1][0]][4]],\n                            'paper title': [squad_results[p[1][0]][5]],\n                            'paper url': [squad_results[p[1][0]][6]],\n                              'Short Answer ': [squad_results[p[1][0]][2]],\n                                'Relevant Paragraph' :[squad_results[p[1][0]][3]]})\n        df_list.append(df)\n    frame = pd.concat(df_list)\n    frame.reset_index(inplace=True)\n    frame.drop(['index'], inplace = True, axis = 1)\n    solutions[qdx+1]=frame ","265d8bc7":"final_list = [solutions[keys] for keys in solutions.keys()]\nfinal_df = pd.concat(final_list)\nfinal_df.reset_index(inplace=True)\nfinal_df.drop(['index'], inplace = True, axis = 1)\nfinal_df.head()\n#final_df.head().style.set_properties(subset=['Relevant Paragraph'], **{'width': '300px'})","0a628dc6":"!jupyter nbextension enable --py widgetsnbextension","d404dfc5":"import ipywidgets as widgets\n#from ipywidgets import interactive\nfrom IPython.display import display\nfrom IPython.html.widgets import interactive \nitems = ['All']+sorted(final_df['QUESTION'].unique().tolist())\n \ndef view(Question=''):\n    if Question=='All': display(final_df)\n    display(final_df[final_df['QUESTION']==Question])\n    \ndef question_ask(question, sciBert_model, merged_data, fulldfembedding):\n    corpus = [re.sub(' \\n\\n\\n ','',x) for x in merged_data['Embedding_Col'].to_list()]\n    results = ask_question(question, sciBert_model, corpus, fulldfembedding)\n    hashed_paras,context, para_from_paperid,para_from_papertitle, para_from_paperurl = hash_results(results, merged_data)\n    top_para_bert = find_top_para_bert(context, question)\n    top_para_CVec = find_top_para_CVect(context, question)\n    squad_results, predicted_answer_list = squad_bert_results(top_para_bert, top_para_CVec)\n    twoGram, Processed_answers =bleu_post_process(predicted_answer_list)\n    df_list = []\n    for pdx, p in enumerate(Processed_answers):\n        df = pd.DataFrame({'QUESTION': [squad_results[p[1][0]][1]],    \n                            'paper cord_uid': [squad_results[p[1][0]][4]],\n                            'paper title': [squad_results[p[1][0]][5]],\n                            'paper url': [squad_results[p[1][0]][6]],\n                              'Short Answer ': [squad_results[p[1][0]][2]],\n                                'Relevant Paragraph' :[squad_results[p[1][0]][3]]})\n        df_list.append(df)\n    frame = pd.concat(df_list)\n    frame.reset_index(inplace=True)\n    frame.drop(['index'], inplace = True, axis = 1)\n    return frame\n \nw = widgets.Dropdown(options=items)\nq = widgets.Text(value='question')\n#interactive(view, Question=w)","bb1c1cb0":"#create tabs\ntab_nest = widgets.Tab()\n# tab_nest.children = [tab_visualise]\ntab_nest.set_title(0, 'Results')\n#tab_nest.set_title(1, 'Ask Question')\n\n\n#interact function in isolation\nf = interactive(view, Question=w);\n#f1 = interactive(question_ask, sciBert_model=sciBert_model, merged_data=merged_data, fulldfembedding=fulldfembedding, question=q);\ntab_nest.children = [VBox(children = f.children) ]\ndisplay(tab_nest)","8947e21c":"#question = input('Enter Question: ')\n#question_ask(question, sciBert_model, merged_data, fulldfembedding)","835d6f24":"#@title Load the Universal Sentence Encoder's TF Hub module\n\nmodule_url = \"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\" #@param [\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\", \"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\"]\nmodel = hub.load(module_url)\nprint (\"module %s loaded\" % module_url)\ndef embed(input):\n    return model(input)","0160b8b0":"def plot_similarity(labels, features, rotation):\n    corr = np.inner(features, features)\n    sns.set(font_scale=0.8)\n    g = sns.heatmap(\n      corr,\n      xticklabels=labels,\n      yticklabels=labels,\n      vmin=0,\n      vmax=1,\n      cmap=\"YlOrRd\")\n    g.set_xticklabels(labels, rotation=rotation)\n    g.set_title(\"Semantic Textual Similarity\")\n\ndef run_and_plot(messages_):\n    message_embeddings_ = embed(messages_)\n    plot_similarity([textwrap.fill(x)[:100] for x in messages_], message_embeddings_, 90)","ca921ade":"# showing only first 100 leters in the labels\nl = [x[1][1] for x in Processed_answers]\n\n#np.inner(embed(l), embed(l))\nrun_and_plot(l)","716f338e":"final_df.head()","a8e8daf3":"## Excel Results reading to Pandas\n#****### The following code lets the reader view the most relevant answers in the already run results. The excel file is organized in tabs 6.1, 6.2, 6.3... 6.8, corresponding to each sub-task. The result for each sub-task can be passed by the corresponding sheet name to the code below.\n### Note the display is limited up to top 5 results, a complete set of paper results is captured in the attached excel\n#References: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_excel.html\n# import pandas as pd\n\n# df = pd.read_excel (r'Task6 Results and Analysis_final.xlsx', sheet_name='6.3')\n# pd.set_option('display.max_colwidth', None)\n# df.head()","a9431b1a":"predicted_list = [y[1] for y in predicted_answer_list]\n#predicted_list","a71e7115":"def gen_wordcloud(passage_list, ericsson_style=False):\n    '''\n    i\/p: a list of passages: list[str]\n    o\/p: None, plots the picture\n    '''\n    all_passages = ' '.join(predicted_list)\n    \n    stopwords = set(STOPWORDS)\n    \n    if ericsson_style:\n        mask = np.array(Image.open('\/kaggle\/input\/ericssonlogo\/ericsson-logo-clipart.jpg'))\n\n        wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", mode=\"RGBA\", max_words=1000, mask=mask).generate(all_passages)\n\n        # create coloring from image\n        image_colors = ImageColorGenerator(mask)\n        plt.figure(figsize=[18,10])\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n        plt.axis(\"off\")\n        plt.show()\n\n    else:\n        wordcloud = WordCloud().generate(all_passages)\n\n        wordcloud = WordCloud(max_font_size=40).generate(all_passages)\n        plt.figure(figsize = (15, 10))\n        plt.imshow(wordcloud, interpolation=\"bilinear\")\n        plt.axis(\"off\")\n        plt.show()","f2cd78e6":"# run this function for a paragraph list of each question\ngen_wordcloud(predicted_list)","a38d1fe5":"#os.listdir('\/kaggle\/input\/ericssonlogo\/')","2ad9a092":"# gen_wordcloud(predicted_list, ericsson_style=True)","ee785e35":"### Questions from task 6","f0a982e9":"\nThe input data set from the CORD-19 challenge includes over 45,000+ digitized articles from the literature related to the corona virus families. This notebook analyzes the entire corpus with Natural Language Processing (NLP) techniques and pre-trained models such as SciBert and SQuADBERT to answer specific sub-tasks as outlined above.\n<br \/>\n<br \/>\nA common processing pipeline and modeling approach for the entire task was chosen. A general pre-process data pipeline was adopted from another Ericsson team.\n\n![Datapipeline.JPG](attachment:Datapipeline.JPG)","3e171849":"## 1. Download & Pre-Process Raw Data","109028a4":"### Load uncleaned data, as SciBERT needs dot to see the whole sentence, so stop_words just makes it worse","af275813":"## License Agreements\n\n#### PSF LICENSE AGREEMENT FOR PYTHON 3.8.2\n1. This LICENSE AGREEMENT is between the Python Software Foundation (\"PSF\"), and\n   the Individual or Organization (\"Licensee\") accessing and otherwise using Python\n   3.8.2 software in source or binary form and its associated documentation.\n\n2. Subject to the terms and conditions of this License Agreement, PSF hereby\n   grants Licensee a nonexclusive, royalty-free, world-wide license to reproduce,\n   analyze, test, perform and\/or display publicly, prepare derivative works,\n   distribute, and otherwise use Python 3.8.2 alone or in any derivative\n   version, provided, however, that PSF's License Agreement and PSF's notice of\n   copyright, i.e., \"Copyright \u00a9 2001-2020 Python Software Foundation; All Rights\n   Reserved\" are retained in Python 3.8.2 alone or in any derivative version\n   prepared by Licensee.\n\n3. In the event Licensee prepares a derivative work that is based on or\n   incorporates Python 3.8.2 or any part thereof, and wants to make the\n   derivative work available to others as provided herein, then Licensee hereby\n   agrees to include in any such work a brief summary of the changes made to Python\n   3.8.2.\n\n4. PSF is making Python 3.8.2 available to Licensee on an \"AS IS\" basis.\n   PSF MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED.  BY WAY OF\n   EXAMPLE, BUT NOT LIMITATION, PSF MAKES NO AND DISCLAIMS ANY REPRESENTATION OR\n   WARRANTY OF MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE\n   USE OF PYTHON 3.8.2 WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n\n5. PSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON 3.8.2\n   FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS A RESULT OF\n   MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON 3.8.2, OR ANY DERIVATIVE\n   THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n\n6. This License Agreement will automatically terminate upon a material breach of\n   its terms and conditions.\n\n7. Nothing in this License Agreement shall be deemed to create any relationship\n   of agency, partnership, or joint venture between PSF and Licensee.  This License\n   Agreement does not grant permission to use PSF trademarks or trade name in a\n   trademark sense to endorse or promote products or services of Licensee, or any\n   third party.\n\n8. By copying, installing or otherwise using Python 3.8.2, Licensee agrees\n   to be bound by the terms and conditions of this License Agreement.\n \n \n#### The 3-Clause BSD License\n \nNote: This license has also been called the \"New BSD License\" or \"Modified BSD License\". See also the 2-clause BSD License.\n\nCopyright <YEAR> <COPYRIGHT HOLDER>\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and\/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n#### NumPy license\nCopyright \u00a9 2005-2020, NumPy Developers.\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and\/or other materials provided with the distribution.\n\nNeither the name of the NumPy Developers nor the names of any contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \u201cAS IS\u201d AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n \n#### The MIT License\n \nCopyright <YEAR> <COPYRIGHT HOLDER>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n  \n#### License agreement for matplotlib versions 1.3.0 and later\n=========================================================\n\n1. This LICENSE AGREEMENT is between the Matplotlib Development Team\n(\"MDT\"), and the Individual or Organization (\"Licensee\") accessing and\notherwise using matplotlib software in source or binary form and its\nassociated documentation.\n\n2. Subject to the terms and conditions of this License Agreement, MDT\nhereby grants Licensee a nonexclusive, royalty-free, world-wide license\nto reproduce, analyze, test, perform and\/or display publicly, prepare\nderivative works, distribute, and otherwise use matplotlib\nalone or in any derivative version, provided, however, that MDT's\nLicense Agreement and MDT's notice of copyright, i.e., \"Copyright (c)\n2012- Matplotlib Development Team; All Rights Reserved\" are retained in\nmatplotlib  alone or in any derivative version prepared by\nLicensee.\n\n3. In the event Licensee prepares a derivative work that is based on or\nincorporates matplotlib or any part thereof, and wants to\nmake the derivative work available to others as provided herein, then\nLicensee hereby agrees to include in any such work a brief summary of\nthe changes made to matplotlib .\n\n4. MDT is making matplotlib available to Licensee on an \"AS\nIS\" basis.  MDT MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR\nIMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, MDT MAKES NO AND\nDISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS\nFOR ANY PARTICULAR PURPOSE OR THAT THE USE OF MATPLOTLIB\nWILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n\n5. MDT SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF MATPLOTLIB\n FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR\nLOSS AS A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING\nMATPLOTLIB , OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF\nTHE POSSIBILITY THEREOF.\n\n6. This License Agreement will automatically terminate upon a material\nbreach of its terms and conditions.\n\n7. Nothing in this License Agreement shall be deemed to create any\nrelationship of agency, partnership, or joint venture between MDT and\nLicensee.  This License Agreement does not grant permission to use MDT\ntrademarks or trade name in a trademark sense to endorse or promote\nproducts or services of Licensee, or any third party.\n\n8. By copying, installing or otherwise using matplotlib ,\nLicensee agrees to be bound by the terms and conditions of this License\nAgreement.\n \n#### 3-clause license (\"BSD License 2.0\", \"Revised BSD License\", \"New BSD License\", or \"Modified BSD License\")\nCopyright (c) <year>, <copyright holder>\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright\n      notice, this list of conditions and the following disclaimer in the\n      documentation and\/or other materials provided with the distribution.\n    * Neither the name of the <organization> nor the\n      names of its contributors may be used to endorse or promote products\n      derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL <COPYRIGHT HOLDER> BE LIABLE FOR ANY\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n \n#### Apache License\n\nVersion 2.0, January 2004\n\nhttp:\/\/www.apache.org\/licenses\/\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n1. Definitions.\n\n\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.\n\n\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.\n\n\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.\n\n\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\n\n\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\n\n\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).\n\n\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.\n\n\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n\nYou must give any other recipients of the Work or Derivative Works a copy of this License; and\nYou must cause any modified files to carry prominent notices stating that You changed the files; and\nYou must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and\nIf the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.\n\nYou may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.\n5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and\/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n\n#### Software License\nThe Python Imaging Library (PIL) is\n\n    Copyright \u00a9 1997-2011 by Secret Labs AB\n    Copyright \u00a9 1995-2011 by Fredrik Lundh\n\nBy obtaining, using, and\/or copying this software and\/or its associated documentation, you agree that you have read, understood, and will comply with the following terms and conditions:\n\nPermission to use, copy, modify, and distribute this software and its associated documentation for any purpose and without fee is hereby granted, provided that the above copyright notice appears in all copies, and that both that copyright notice and this permission notice appear in supporting documentation, and that the name of Secret Labs AB or the author not be used in advertising or publicity pertaining to distribution of the software without specific, written prior permission.\n\nSECRET LABS AB AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL SECRET LABS AB OR THE AUTHOR BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\n#### SciPy license\nCopyright \u00a9 2001, 2002 Enthought, Inc.\nAll rights reserved.\n\nCopyright \u00a9 2003-2019 SciPy Developers.\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and\/or other materials provided with the distribution.\n\nNeither the name of Enthought nor the names of the SciPy Developers may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \u201cAS IS\u201d AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","74aa5494":"![Task%206%20banner.JPG](attachment:Task%206%20banner.JPG)","aaa30f91":"#### Simplify the questions, Attention based BERT relies on clear question","d7ad063f":"# corpus = [re.sub(' \\n\\n\\n ','',x) for x in working_Data['Embedding_Col'].to_list()]","1829090f":"## 5. BLEU score to validate and clean up answers\n### Post processing approaches: \nWe tried Cosine similarity and Text summarization approaches. However, we found the results from BLEU (bilingual evaluation understudy) Score are better. BLUE is a metric for evaluating a generated sentence to a reference sentence. In our context, we used to compare the results from each answer to all other answers in the ranking order and consolidate the results. 2-grams option worked best for our scenarios. \nWe pass the results from BLUE to another post-processing technique, Semantic Similarity with TF-Hub Universal Encoder. \n","d8ea0041":"### Encode the Embedding_Col","8ee4e006":"### save embeddings to pickle file so can be loaded later to avoid re-embedding","b16400ea":"## 4. SQUAD BERT to summarize 20 paragraphs to give answer.\n### Why SQuADBERT?\nReady to use off the shelf models and easy to maintain. BERT is a multi-layer bidirectional transformer encoder based on the original implementation in Vaswani etal in 2017\nStanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. We used SQuADBERT on the shortlisted articles from previous steps.\n\n### 10 paragraphs from SciBERT and 10 more paragraphs from CountVectorizer","3ae03d7b":"It takes a lot of time to create embeddings for whole body of the article. Furthermore, since there are missing values in 'Abstract' and \"title\" why we decided to use combination of \"asbtract\" and \"title\". Our first Priority is \"abstract\" as it contains more information than the \"title\" and then use \"title\" wherever \"abstract\" is missing. ","462818d7":"### Rank Paragraphs using CountVectorizer Approach to obtain 10 more paragraphs","4aef9f59":"### Import Packages","fa4f0302":"### SciBERT to obtain top 10 paragraphs","bfc7630a":"## 2. SciBERT to get top 10 relevant paper, save all paragraphs from body text","ca298e0a":"### Team members\n1. Evan Bechtol\n2. Anders Berkeman\n3. Ana Bogic\n4. Rafael Carmargo\n5. Carlos Gonzalez\n6. Arnab Guha\n7. Satish Gujar\n8. Ilyas Habeeb\n9. Paul Lamb\n10. Robin Liu\n11. Randy McDowell\n12. Jody Mitchie\n13. Piyush Mittal\n14. Roland Smith\n15. Sunil Kumar Vuppala\n","8365cccc":"print(f\"above paragraph is from paper cord_uid: {para_from_paperid[context[100]]}\")","954733e2":"## >>> In this example the first question is picked <<<\n\nThe model was previously run against several simplified questions for each sub-task\n### using SciBERT to get top 10 papers, save body text in to one list","c961b975":"## Visulaizations","e3fe0857":"\n### The goal of this task to summarize the effectiveness of non-pharmaceutical interventions based on the available literature. Further details are requested on the equity and barriers to compliance for these interventions.\n<br \/>\n<br \/>\nAs of this writing no cure exists for Covid19. The only clinically available prevention measures are non-pharmaceutical in nature. This task is further broken into sub-tasks which relate to:\n\n1. Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.\n<br \/>\n<br \/>\n2. Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments.\n<br \/>\n<br \/>\n3. Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.\n<br \/>\n<br \/>\n4. Methods to control the spread in communities, barriers to compliance and how these vary among different populations.\n<br \/>\n<br \/>\n5. Models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status.\n<br \/>\n<br \/>\n6. Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs.\n<br \/>\n<br \/>\n7. Research on why people fail to comply with public health advice, even if they want to do so (e.g., social or financial costs may be too high).\n<br \/>\n<br \/>\n8. Research on the economic impact of this or any pandemic. This would include identifying policy and programmatic alternatives that lessen\/mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses, treatment, and needed care, regardless of ability to pay.","e195428d":"### Specify Directories","b74c6988":"## User Based Question","dca0c707":"## 3. SciBERT and countvectorizer to Rank all paragraphs to get top 10 paragraphs each","aea3fbfd":"### Semantic Textual Similarity using universal sentence encoder","56b08d00":"!pip install tqdm\n!pip install PrettyTable\n!pip install pandas\n!pip install nltk\n!pip install sentence-transformers","5fe0304a":"### Packages and licenses\n- Os : https:\/\/github.com\/rancher\/os\/blob\/master\/LICENSE\n- Tqdm : https:\/\/github.com\/tqdm\/tqdm\/blob\/master\/LICENCE\n- Sklearn : https:\/\/scikit-learn.org\/stable\/about.html#citing-scikit-learn\n- Transformers : https:\/\/github.com\/huggingface\/transformers\/blob\/master\/LICENSE\n- Sentence_Transformers : https:\/\/github.com\/UKPLab\/sentence-transformers\/blob\/master\/LICENSE\n- Scibert-nli : https:\/\/huggingface.co\/gsarti\/scibert-nli\n- Nltk : https:\/\/github.com\/nltk\/nltk\/blob\/develop\/LICENSE.txt\n- Pytorch : https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/LICENSE\n- Bert : https:\/\/github.com\/google-research\/bert\/blob\/master\/LICENSE\n- Tensorflow : https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/LICENSE\n- Tensorflow Hub : https:\/\/github.com\/tensorflow\/hub\/blob\/master\/LICENSE","7607bee4":"#### Download Kaggle Data Directly","7e25e1e3":"### Combine 'Abstract' and 'Title' as corpus, not embedding whole body_text","68fe7ec8":"#### Helper Class to Extract Data & Convert it to DataFrame","078a6fb6":"# Install TF-Hub.\n!pip install tensorflow\n!pip install tensorflow-hub\n!pip install seaborn\n!pip install absl","ac8cfc5b":"Post Processing from BELU score removed substantial irrelevant results w.r.t. the questions. However, there some of the reults were still not acceptable. In order to extract more relevant answers, we pipelined the relevant results from BLEU score analysis to create emeddings using Universal Sentence Encoder. We generated heatmap from the similarity score of the embeddings. From the heatmap we can see two clusters of the senteces: one having highly similar and other having very less similairty. ","82e8d8da":"-------"}}