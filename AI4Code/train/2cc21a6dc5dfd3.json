{"cell_type":{"9d78abaf":"code","eafa23ca":"code","bca57bbc":"code","9dcffe57":"code","4b9817c1":"code","7743ba57":"code","35364754":"code","9757dc6b":"code","01256bcb":"code","cf4f1c1e":"code","610c1f5f":"code","52430c6e":"code","4f23cf94":"code","3730808a":"markdown","4f409fa4":"markdown","32007b32":"markdown","01e457b5":"markdown","d47539b7":"markdown","2bcbc38e":"markdown","710eedea":"markdown","cb719435":"markdown","00afc1eb":"markdown","5241433f":"markdown"},"source":{"9d78abaf":"!pip install tensorflow==2.0.0a0","eafa23ca":"!wget https:\/\/github.com\/bonlime\/keras-deeplab-v3-plus\/archive\/master.zip\n!unzip master.zip\n\nimport sys\nsys.path.insert(0, 'keras-deeplab-v3-plus-master')","bca57bbc":"import numpy as np\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\n%pylab inline\n\nfrom model import Deeplabv3","9dcffe57":"deeplab_model = Deeplabv3()","4b9817c1":"def get_mask(image, model):\n    trained_image_width=512 \n    mean_subtraction_value=127.5\n\n    # add 3-th dimension if needed\n    if len(image.shape) == 2:\n        image = np.tile(image[..., None], (1, 1, 3))\n        \n    # resize to max dimension of images from training dataset\n    w, h, _ = image.shape\n    ratio = float(trained_image_width) \/ np.max([w, h])\n    resized_image = np.array(Image.fromarray(image.astype('uint8')).resize((int(ratio * h), int(ratio * w))))\n\n    # apply normalization for trained dataset images\n    resized_image = (resized_image \/ mean_subtraction_value) - 1.\n\n    # pad array to square image to match training images\n    pad_x = int(trained_image_width - resized_image.shape[0])\n    pad_y = int(trained_image_width - resized_image.shape[1])\n    resized_image = np.pad(resized_image, ((0, pad_x), (0, pad_y), (0, 0)), mode='constant')\n\n    # make prediction\n    res = model.predict(np.expand_dims(resized_image, 0))\n    labels = np.argmax(res.squeeze(), -1)\n\n    # remove padding and resize back to original image\n    if pad_x > 0:\n        labels = labels[:-pad_x]\n    if pad_y > 0:\n        labels = labels[:, :-pad_y]\n    labels = np.array(Image.fromarray(labels.astype('uint8')).resize((h, w)))\n    \n    return (labels == 15).astype('uint8')","7743ba57":"image = np.array(Image.open('keras-deeplab-v3-plus-master\/imgs\/image3.jpg'))\nmask = get_mask(image, deeplab_model)\nplt.imshow(mask)","35364754":"plt.figure(figsize=(15,10))\nimg = np.array(Image.open(\"..\/input\/coco2017\/train2017\/train2017\/000000281563.jpg\"))\nlabel = np.array(Image.open(\"..\/input\/coco2017\/stuffthingmaps_trainval2017\/train2017\/000000281563.png\"))\n\nplt.subplot(1,3, 1)\nplt.imshow(img)\nplt.title(\"Image\")\nplt.subplot(1,3, 2)\nplt.imshow(label < 1)\nplt.title(\"Label\")\nmask = get_mask(img, deeplab_model)\nplt.subplot(1,3, 3)\nplt.imshow(mask)\nplt.title(\"Predict\")","9757dc6b":"deeplab_model = Deeplabv3(backbone='xception', OS=8)","01256bcb":"plt.figure(figsize=(15,10))\nimg = np.array(Image.open(\"..\/input\/coco2017\/train2017\/train2017\/000000281563.jpg\"))\nlabel = np.array(Image.open(\"..\/input\/coco2017\/stuffthingmaps_trainval2017\/train2017\/000000281563.png\"))\n\nplt.subplot(1,3, 1)\nplt.imshow(img)\nplt.title(\"Image\")\nplt.subplot(1,3, 2)\nplt.imshow(label < 1)\nplt.title(\"Label\")\nmask = get_mask(img, deeplab_model)\nplt.subplot(1,3, 3)\nplt.imshow(mask)\nplt.title(\"Predict\")","cf4f1c1e":"import pandas as pd\nsample_submission = pd.read_csv('..\/input\/sf-dl-2-person-segmentation\/sample-submission.csv')","610c1f5f":"# \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043c\u0430\u0441\u043a\u0438 \u0432 EncodedPixels\ndef mask_to_rle(mask):\n    mask_flat = mask.flatten('F')\n    flag = 0\n    rle_list = list()\n    for i in range(mask_flat.shape[0]):\n        if flag == 0:\n            if mask_flat[i] == 1:\n                flag = 1\n                starts = i+1\n                rle_list.append(starts)\n        else:\n            if mask_flat[i] == 0:\n                flag = 0\n                ends = i\n                rle_list.append(ends-starts+1)\n    if flag == 1:\n        ends = mask_flat.shape[0]\n        rle_list.append(ends-starts+1)\n    #sanity check\n    if len(rle_list) % 2 != 0:\n        print('NG')\n    if len(rle_list) == 0:\n        rle = np.nan\n    else:\n        rle = ' '.join(map(str,rle_list))\n    return rle","52430c6e":"submit_rle_arr = []\n\nfor img_id in sample_submission.ImageId.values:\n    image = np.array(Image.open(f'..\/input\/coco2017\/val2017\/{img_id}'))\n    mask_out = get_mask(image, deeplab_model)\n    rle = mask_to_rle(mask_out) \n    submit_rle_arr.append(rle)","4f23cf94":"sample_submission['EncodedPixels'] = submit_rle_arr\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head()","3730808a":"## \u0413\u043e\u0442\u043e\u0432\u0430\u044f \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u0441\u0435\u0442\u044c\n\u0421\u043a\u0430\u0447\u0438\u0432\u0430\u0435\u043c \u0440\u0435\u043f\u0443, \u0440\u0430\u0437\u0430\u0440\u0445\u0438\u0432\u0438\u0440\u0443\u0435\u043c \u0438 \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u0432 \u043f\u0443\u0442\u0438","4f409fa4":"\u0418\u0437 \u0440\u0435\u043f\u044b \u0432\u0441\u0435 \u0445\u043e\u0440\u043e\u0448\u043e. \u0410 \u043a\u0430\u043a \u043d\u0430\u0441\u0447\u0435\u0442 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0438\u0437 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430?","32007b32":"\u041d\u0443 \u043a\u0430\u043a-\u0442\u043e \u0442\u0430\u043a \u0441\u0435\u0431\u0435... \u041c\u043e\u0436\u0435\u0442 \u0441 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438 \u0441\u0435\u0442\u0438 \u043f\u043e\u0438\u0433\u0440\u0430\u0442\u044c\u0441\u044f \u043d\u0430\u0434\u043e?\n\u0410, \u0442\u0430\u043a \u0443 \u043d\u0430\u0441 \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e backbone='mobilenetv2', \u0430 \u043e\u043d\u043e \u0431\u044b\u0441\u0442\u0440\u043e\u0435, \u043d\u043e \u043d\u0435 \u0442\u043e\u0447\u043d\u043e\u0435 (\u0432\u0438\u0434\u0438\u043c\u043e). \u041c\u0435\u043d\u044f\u0435\u043c \u043d\u0430 backbone='xception', OS=8","01e457b5":"\u0421\u0432\u0435\u0436\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f tensorflow, \u043d\u0430\u0434\u043e \u0434\u043b\u044f \u0441\u0435\u0442\u043a\u0438","d47539b7":"\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0435 \u0438\u0437 \u0440\u0435\u043f\u044b","2bcbc38e":"\u041d\u0435\u043f\u043e\u043d\u044f\u0442\u043d\u043e, \u043a\u0442\u043e \u043b\u0443\u0447\u0448\u0435... \u041d\u043e \u0442\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043d\u0430 \u0431\u043e\u043b\u0435\u0435 \u043d\u0430\u0432\u043e\u0440\u043e\u0447\u0435\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438.","710eedea":"# \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0441\u0435\u0442\u044c","cb719435":"## \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u0430\u0441\u043a\u0438 \u043b\u044e\u0434\u0435\u0439\n\u041f\u043e \u0441\u0443\u0442\u0438, \u043f\u0440\u0438\u043c\u0435\u0440, \u043e\u0431\u0435\u0440\u043d\u0443\u0442\u044b\u0439 \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u044e","00afc1eb":"# \u0414\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f","5241433f":"# \u0421\u043a\u0430\u0447\u0430\u0435\u043c \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438"}}