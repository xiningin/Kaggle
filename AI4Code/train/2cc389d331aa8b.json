{"cell_type":{"547421e4":"code","0c40d6a2":"code","e74818b5":"code","9808c121":"code","feef7d48":"code","3255a34e":"code","7cddaa23":"code","cabd03a7":"code","3508dad3":"code","d2458906":"code","37105540":"code","4cc3f8a1":"code","100c18d2":"code","f3c5e699":"code","20c8b136":"code","ce8a31d0":"code","fffc0af3":"code","b7bf26be":"code","9aa130e7":"code","a15a9f19":"markdown","688a3b71":"markdown","874236ec":"markdown","67da8407":"markdown","09e83900":"markdown","af0410ed":"markdown","d9d54443":"markdown","0f54168f":"markdown","e7de66bb":"markdown","295a6cc1":"markdown","bf40780e":"markdown","5e9df0d8":"markdown","04c5ce0b":"markdown","baa467bc":"markdown","0105e5e2":"markdown","53d278c4":"markdown"},"source":{"547421e4":"import numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","0c40d6a2":"dataset = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_submission = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","e74818b5":"dataset.info()","9808c121":"dataset.drop(columns=[\"Cabin\"], inplace=True)","feef7d48":"dataset[\"Age\"].fillna(dataset[\"Age\"].dropna().mean(), inplace=True)","3255a34e":"dataset[\"Embarked\"].fillna(dataset[\"Embarked\"].mode()[0], inplace=True)","7cddaa23":"dataset.info()","cabd03a7":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dataset.drop(columns=[\"Survived\"]), dataset[\"Survived\"], test_size=0.3, random_state=42)","3508dad3":"X_train","d2458906":"from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(handle_unknown='ignore')\nX_train_encoded = pd.DataFrame(encoder.fit_transform(X_train[[\"Pclass\", \"Sex\", \"Embarked\"]]).toarray())","37105540":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train_norm = pd.DataFrame(scaler.fit_transform(X_train[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]))","4cc3f8a1":"X_train_norm","100c18d2":"X_train_processed = pd.concat([X_train_encoded, X_train_norm], axis=1)","f3c5e699":"X_train_processed","20c8b136":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_train_processed, y_train)","ce8a31d0":"X_test_norm = pd.DataFrame(scaler.transform(X_test[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]))\nX_test_encoded = pd.DataFrame(encoder.transform(X_test[[\"Pclass\", \"Sex\", \"Embarked\"]]).toarray())\nX_test_processed = pd.concat([X_test_encoded, X_test_norm], axis=1)","fffc0af3":"X_test_processed","b7bf26be":"y_pred = rf.predict(X_test_processed)","9aa130e7":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","a15a9f19":"Now, it's enough concatenate the categorical dataframe and the numerical dataframe","688a3b71":"#### So, let's predict using the classifier","874236ec":"#### Then, let's to encode the categorical columns","67da8407":"#### Let's also normalize the numerical columns","09e83900":"#### To measure the accuracy of the model, we will use accuracy_score from sklearn","af0410ed":"### Loading Dataset","d9d54443":"#### It means that the classifier predict correctly 77% of the test dataset \ud83d\ude00","0f54168f":"Fill Nan values on Age column with mean of column because is a numeric value and continuous. It should be a good choice","e7de66bb":"Deleting column Cabin because it has much Nan values and maybe isn't a significant data","295a6cc1":"#### Now, let's separate the data between train and test data","bf40780e":" We will use the test dataset to evaluate the classifier.\n First, we need to processe it too","5e9df0d8":"So, the data is ready to be use in a machine learning algorithm. I chose the Random Forest algoritm","04c5ce0b":"Fill nan value from column Embarked with mode, because is a categorical data and it is only 3 values missing values","baa467bc":"### Using machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\n","0105e5e2":"Now, we can note that dataset not have missing or Nan values","53d278c4":"#### First, we need to check if the dataset has some Nan values and fill it with something depending the column.\nAbove, we can note that there are some columns with Nan values"}}