{"cell_type":{"5c6ad163":"code","65e090e6":"code","49613a5e":"code","f10a6b94":"code","cf4c576a":"code","a1265df6":"code","ab653378":"code","10fa38a5":"code","e72da9d1":"code","19aea97d":"code","bc3a8071":"code","b76b6222":"code","64d65311":"code","ebf9e6e3":"code","a7386a6b":"code","4b11fa14":"code","9d502856":"code","817ceffc":"code","5f38e6ec":"code","4d7e7d92":"code","708b1ce0":"code","7e1ea079":"code","d3f99300":"code","22da82d0":"code","970333af":"code","f3f07e14":"code","39b97c8d":"code","32c202eb":"code","ee47c446":"code","f75c6674":"code","ade49ecc":"code","b3bb7dc4":"code","e15c6b77":"code","ceb61852":"code","3ac0e272":"code","c119e9da":"code","ad6ad475":"code","38b8d939":"code","6c76b9fa":"code","90b6a5f5":"code","17b4604f":"code","6ddc36ae":"code","f57b05ad":"code","87886d65":"code","9e30abcd":"code","3036ad86":"code","0c02efb9":"code","275c3798":"code","7af9bfe5":"code","d9185a6b":"markdown","76f9118b":"markdown","0a660562":"markdown","c7868584":"markdown","a5c3b6bd":"markdown","8a3d092e":"markdown","4f5471c8":"markdown","5ed8f6fb":"markdown","5be7c9d8":"markdown","7f5bc77e":"markdown","6dae6dfd":"markdown","1bef1cee":"markdown","7b56a875":"markdown","3a9a427e":"markdown","1bc4148f":"markdown","9bf29401":"markdown","c1074027":"markdown","dd363213":"markdown","f1258b1c":"markdown","14005c73":"markdown","e69b38d8":"markdown","e2fec512":"markdown","7a9dca47":"markdown","a829e6b1":"markdown","e97f4c32":"markdown","11a56c76":"markdown","9e95d6c4":"markdown"},"source":{"5c6ad163":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","65e090e6":"df = pd.read_csv('..\/input\/car-price-prediction\/data.csv')\ndf.head()","49613a5e":"df.columns = df.columns.str.lower().str.replace(\" \", \"_\")\ndf.head()","f10a6b94":"strings = list(df.dtypes[df.dtypes == 'object'].index)\nstrings","cf4c576a":"for col in strings:\n    df[col] = df[col].str.lower().str.replace(\" \", \"_\")\n    \ndf.head()","a1265df6":"sns.histplot(df['msrp'], bins = 50)","ab653378":"sns.histplot(df['msrp'][df['msrp'] < 100000], bins = 50)","10fa38a5":"\"\"\"\nApplying log to 'msrp' column\n\"\"\"\nprice_logs = np.log1p(df['msrp'])\nsns.histplot(price_logs, bins = 50)","e72da9d1":"print(df.isnull().sum())\ndf.isnull().sum().plot(kind = 'bar')","19aea97d":"\"\"\"\nCreating length\/number of values of train, validation and test set\n\"\"\"\nn  = len(df)    \n\nn_val = int(n * 0.2)  # Creating Validation Set\nn_test = int(n * 0.2) # Creating test set\nn_train = n - n_val - n_test\n\n\"\"\"\nSetting up for randomizing the values\n\"\"\"\nidx = np.arange(n)\nnp.random.seed(2)\nnp.random.shuffle(idx)\n\n\n\"\"\"\nCreating data set for Train Valid and Test set.\nAlso to randomize the index to avoid bias.\n\"\"\"\n\ndf_train = df.iloc[idx[:n_train]]\ndf_val = df.iloc[idx[n_train:n_train+n_val]]\ndf_test = df.iloc[idx[n_train+n_val:]]\n\n\n\"\"\"\nDropping the indexes from all the datasets as they are of no use now.\n\"\"\"\n\ndf_train = df_train.reset_index(drop = True)\ndf_val = df_val.reset_index(drop = True)\ndf_test = df_test.reset_index(drop = True)\n\n\n\"\"\"\nMaking dependent variables with LOG transformation for all the datasets.\n\"\"\"\n\ny_train = np.log1p(df_train['msrp'].values)\ny_val = np.log1p(df_val['msrp'].values)\ny_test = np.log1p(df_test['msrp'].values)\n\n\"\"\"\nDeleting target \/ dependent variables from training, validation and test sets\n\"\"\"\ndel df_train['msrp']\ndel df_val['msrp']\ndel df_test['msrp']\n\n\"\"\"\nFinally Checking the Length of Datasets created\n\"\"\"\nlen(df_train), len(df_val), len(df_test)","bc3a8071":"\"\"\"\nTaking values of 3 variables for a specific row (10): engine_hp, city_mpg, popularity\nDeclaring Biased Term and Weights for each features\n\"\"\"\nxi = [453, 11, 86] \n\n\nw0 = 0\n\n\"\"\"\nDeclaring Weights for each features\n\"\"\"\nw =  [1, 1, 1]\n\n\"\"\"\nCreating Linear Regression Function\n\"\"\"\ndef linear_regression(xi):\n    n =len(xi)                # Number of features used\n    \n    pred = w0                 # Initial \/ Base prediction\n    \n    for j in range(n):\n        pred += w[j]*xi[j]     # Formula = w0 +sigma[0:n-1]{w[j]*xi[j]}\n    \n    return pred\n\n\"\"\"\nCalling linear_regression function on xi\n\"\"\"\nlinear_regression(xi)","b76b6222":"\"\"\"\nChanging the bias terms and checking the prediction\n\"\"\"\n\n\"\"\"Biased Term :\"\"\"\nw0 = 7.17\n\n\"\"\"#Declaring Weights for each features\"\"\"\nw =  [0.01, 0.04, 0.002]\n\n\"\"\"\nCalling linear_regression function again on xi\n\"\"\"\nlinear_regression(xi)\n","64d65311":"\"\"\"\nUsing exponent to undo the logrithm which we did initially\nAlso recall we did log(1 + msrp). Therefore we will need subtract 1.\n\"\"\"\nprint(\"predicted prices: \", np.expm1(12.312))","ebf9e6e3":"\"\"\"\nDefining a function for dot product\n\"\"\"\ndef dot(xi,w):\n    n = len(xi)\n    \n    res = 0.0\n    \n    for j in range(n):\n        res += xi[j]*w[j]\n    return res\n\n\"\"\"\nImprovising linear_regression\n\"\"\"\ndef linear_regression(xi):\n    return w0 + dot(xi,w)\n\"\"\"\nMore Improvising : using W.T Xi = Xi W.T = w0 + dot product as done before\nWe add weight of 1 to bias term and add 1 to xis also.\n\"\"\"\nw_new = [w0] + w\ndef linear_regression(xi):\n    xi = [1] + xi\n    return dot(xi,w_new)\n\n\n\"\"\"\nchecking the function again\n\"\"\"\nlinear_regression(xi)","a7386a6b":"\"\"\"\nMaking final model with multiple vectors.\n\"\"\"\n\n\"\"\"Biased Term :\"\"\"\nw0 = 7.17\n\n\"\"\"#Declaring Weights for each features\"\"\"\nw =  [0.01, 0.04, 0.002]\n\n\"\"\"We add weight of 1 to bias term\"\"\"\nw_new = [w0] + w\n\n\"\"\"Making Variables\"\"\"\nx1 = [1, 148, 24, 1385]\nx2 = [1, 132, 25, 2031]\nx10 = [1, 453, 11, 86]\n\"\"\"Making X: independent variables array\"\"\"\nX= [x1, x2, x10]\nX = np.array(X)\n\n\"\"\"\nLinear Regression with multiple vectors\n\"\"\"\ndef linear_regression(X):\n    return X.dot(w_new)\n\n\"\"\"\nPredicting Values\n\"\"\"\nlinear_regression(X)","4b11fa14":"\"\"\"Making Variables\"\"\"\nX = [\n    [148, 24, 1385],\n    [132, 25, 2031],\n    [453, 11, 86],\n    [158, 24, 185],\n    [172, 25, 201],\n    [413, 11, 86],\n    [38, 54, 185],\n    [142, 25, 431],\n    [453, 31, 86]\n]\n \n\"\"\"Making X: independent variables array\"\"\"\nX = np.array(X)\n\"\"\"Declaring Target Variable\"\"\"\ny = [10000, 20000, 15000, 20050, 10000, 20000, 15000, 25000, 12000]","9d502856":"\"\"\"\nGram Matrix\n\"\"\"\nXTX = X.T.dot(X)\n\n\"\"\"inverse of Gram Matrix\"\"\"\nXTX_inv = np.linalg.inv(XTX)\n\n\"\"\"\nNot exactly identity matrix but the numbers here are very small. Hence can be treated as 0s and 1s\n\"\"\"\n# XTX.dot(XTX_inv).round(1)\nXTX.dot(XTX_inv)\n\n\"\"\" Model\"\"\"\nw_full = XTX_inv.dot(X.T).dot(y)\n\n\n\"\"\"\nCreating Coefficients\n\"\"\"\nw0 = w_full[0]\nw = w_full[1:]\n\n\"\"\"\nprinting Coefficient\n\"\"\"\nw0, w","817ceffc":"\"\"\"Creating a function for Training Linear Regression Model\"\"\"\ndef train_linear_regression(X,y):\n    \"\"\"\n    Including a biased term\n    \"\"\"\n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones, X])\n    \n    \"\"\"\n    Gram Matrix\n    \"\"\"\n    XTX = X.T.dot(X)\n    \n    \"\"\"inverse of Gram Matrix\"\"\"\n    XTX_inv = np.linalg.inv(XTX)\n    w_full = XTX_inv.dot(X.T).dot(y)\n    \n    return w_full[0], w_full[1:]   \n\n\n\"\"\"Training the Model\"\"\"","5f38e6ec":"\"\"\"\nWe need only numerical columns\n\"\"\"\ndf_train.dtypes\n\n\"\"\"\nCreating subset of only Numerical Columns\n\"\"\"\nbase = ['engine_hp','engine_cylinders','highway_mpg', 'city_mpg', 'popularity']\n\n\"\"\"\nCreating subset of Dataframe to be used as X.\n\"\"\"\nX_train = df_train[base].values\n\n\"\"\" Training model will throw error b'coz of missing values\"\"\"\ntrain_linear_regression(X_train,y_train)","4d7e7d92":"\"\"\"\nchecking missing values in our subset\n\"\"\"\ndf_train[base].isnull().sum()","708b1ce0":"\"\"\"Imputing Missing values with 0 \"\"\"\nX_train = df_train[base].fillna(0).values","7e1ea079":"\"\"\"Building and Training the model \"\"\"\nw0, w = train_linear_regression(X_train,y_train)\n\n\"\"\" Prediction\"\"\"\ny_pred =  w0 + X_train.dot(w)\n\n\"\"\"Visualizing the Predictions\"\"\"\nsns.histplot(y_pred, color = 'red', alpha = 0.5, bins = 50)\nsns.histplot(y_train, color = 'blue', alpha = 0.5, bins = 50);","d3f99300":"def rmse(y,y_pred):\n    error  = y- y_pred\n    squared_error = error ** 2\n    mse = squared_error.mean()\n    return np.sqrt(mse)  ","22da82d0":"rmse(y_train,y_pred)","970333af":"\"\"\"\nCreating subset of only Numerical Columns\n\"\"\"\nbase = ['engine_hp','engine_cylinders','highway_mpg', 'city_mpg', 'popularity']\n\ndef prepare_X(df):\n    df_num = df[base]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","f3f07e14":"\"\"\"Preparing Training set\"\"\"\nX_train = prepare_X(df_train)                            \n\n\"\"\"Building and Training the model on training set\"\"\"\nw0, w = train_linear_regression(X_train,y_train) \n\n\"\"\"Prediction the values of Validation Set\"\"\"\nX_val = prepare_X(df_val)                                 \ny_pred = w0 + X_val.dot(w)                                \n\n\"\"\"Evaluation on Validation set\"\"\"\nrmse(y_val, y_pred)                                      ","39b97c8d":"\"\"\"\nModifying prepare_S function to include Feature engineering step\n\"\"\"\ndef prepare_X(df):\n    df = df.copy()\n    df['age'] = max(df['year']) - df['year']\n    features = base + ['age']\n    df_num = df[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","32c202eb":"X_train = prepare_X(df_train)","ee47c446":"\"\"\"Preparing Training set\"\"\"\nX_train = prepare_X(df_train)                            \n\n\"\"\"Building and Training the model on training set\"\"\"\nw0, w = train_linear_regression(X_train,y_train)  \n\n\"\"\"Prediction the values of Validation Set\"\"\"\nX_val = prepare_X(df_val)                                \ny_pred = w0 + X_val.dot(w)                                \n\n\"\"\"Evaluation on Validation set\"\"\"\nrmse(y_val, y_pred)                                       ","f75c6674":"\"\"\"Visualizing the Predictions\"\"\"\nsns.histplot(y_pred, color = 'red', alpha = 0.5, bins = 50)\nsns.histplot(y_val, color = 'blue', alpha = 0.5, bins = 50);","ade49ecc":"\"\"\"Top 5 makers\"\"\"\nmakes = list(df['make'].value_counts().head().index)\nmakes","b3bb7dc4":"\"\"\"\nModifying prepare_S function for Creating new columns for number of doors\nMaking columns for top 5 makes of the cars\n\"\"\"\ndef prepare_X(df):\n    df = df.copy()\n    features = base.copy()\n     \n    df['age'] = 2017 - df['year']    \n    features.append('age')\n    \n    \"\"\"Making Variables for number of Doors\"\"\"\n    for v in [2,3,4]:\n        df['num_doors_%s' %v] = (df['number_of_doors'] == v).astype('int') \n        features.append('num_doors_%s' %v)\n        \"\"\"Making variables for top 5 makes\"\"\"\n    for m in makes:\n        df['make_%s' %m] = (df['make'] == m).astype('int') \n        features.append('make_%s' %m)\n        \n    df_num = df[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","e15c6b77":"\"\"\"Preparing Training set\"\"\"\nX_train = prepare_X(df_train)                            \n\n\"\"\"Building and Training the model on training set\"\"\"\nw0, w = train_linear_regression(X_train,y_train)  \n\n\"\"\"Prediction the values of Validation Set\"\"\"\nX_val = prepare_X(df_val)                                \ny_pred = w0 + X_val.dot(w)                                \n\n\"\"\"Evaluation on Validation set\"\"\"\nrmse(y_val, y_pred)","ceb61852":"categorical = ['make','engine_fuel_type', 'transmission_type' , 'driven_wheels', 'market_category', \n'vehicle_size', 'vehicle_style' ] ","3ac0e272":"categories = {}\n\nfor c in categorical:\n    categories[c] = list(df[c].value_counts().head().index)","c119e9da":"categories","ad6ad475":"\"\"\"\nModifying to include categorical variables \n\"\"\"\ndef prepare_X(df):\n    df = df.copy()\n    features = base.copy()\n     \n    df['age'] = max(df['year']) - df['year']    \n    features.append('age')\n    \n    \"\"\"Making Variables for number of Doors\"\"\"\n    for v in [2,3,4]:\n        df['num_doors_%s' %v] = (df['number_of_doors'] == v).astype('int') \n        features.append('num_doors_%s' %v)\n        \n    \"\"\"Making Variables for Categorical variables\"\"\"\n    for c,values in categories.items():\n        for v in values:\n            df[\"%s_%s\" %(c,v)] = (df[c] == v).astype('int') \n            features.append(\"%s_%s\" %(c,v))\n        \n    df_num = df[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","38b8d939":"\"\"\"Preparing Training set\"\"\"\nX_train = prepare_X(df_train)                            \n\n\"\"\"Building and Training the model on training set\"\"\"\nw0, w = train_linear_regression(X_train,y_train)  \n\n\"\"\"Prediction the values of Validation Set\"\"\"\nX_val = prepare_X(df_val)                                \ny_pred = w0 + X_val.dot(w)                                \n\n\"\"\"Evaluation on Validation set\"\"\"\nrmse(y_val, y_pred)","6c76b9fa":"w0, w","90b6a5f5":"def train_linear_regression_reg(X, y, r=0.001):\n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones, X])\n    \n    XTX = X.T.dot(X)\n    \"\"\"Adding Regularization term to the diagonals\"\"\"\n    XTX = XTX + r * np.eye(XTX.shape[0])\n    \n    XTX_inv = np.linalg.inv(XTX)\n    w_full = XTX_inv.dot(X.T).dot(y)\n    \n    return w_full[0], w_full[1:]","17b4604f":"\"\"\"Preparing Training set\"\"\"\nX_train = prepare_X(df_train)                            \n\n\"\"\"Building and Training the model on training set\"\"\"\nw0, w = train_linear_regression_reg(X_train, y_train, r=0.01)\n\n\"\"\"Prediction the values of Validation Set\"\"\"\nX_val = prepare_X(df_val)                                \ny_pred = w0 + X_val.dot(w)                                \n\n\"\"\"Evaluation on Validation set\"\"\"\nrmse(y_val, y_pred)","6ddc36ae":"\"\"\"\nTraining the model for different values of r: regularization term\n\"\"\"\nfor r in [0.0, 0.00001, 0.0001, 0.001, 0.1, 1, 10]:\n    X_train = prepare_X(df_train)\n    w0, w = train_linear_regression_reg(X_train, y_train, r=r)\n\n    X_val = prepare_X(df_val)\n    y_pred = w0 + X_val.dot(w)\n    score = rmse(y_val, y_pred)\n    \n    print(r, w0, score)","f57b05ad":"\"\"\"\nSelecting the best r for training our model \n\"\"\"\nr = 0.001\nX_train = prepare_X(df_train)\nw0, w = train_linear_regression_reg(X_train, y_train, r=r)\n\nX_val = prepare_X(df_val)\ny_pred = w0 + X_val.dot(w)\nscore = rmse(y_val, y_pred)\nscore","87886d65":"\"\"\"Concatnating the datasets: train and valid\"\"\"\ndf_full_train = pd.concat([df_train, df_val])\n\n\"\"\"Full Train dataset with index dropped. As they are of no use.\"\"\"\ndf_full_train = df_full_train.reset_index(drop=True)\n\n\"\"\"Creating X and y\"\"\"\nX_full_train = prepare_X(df_full_train)\ny_full_train = np.concatenate([y_train, y_val])\n\n\"\"\"Building model\"\"\"\nw0, w = train_linear_regression_reg(X_full_train, y_full_train, r=0.001)\n\n\"\"\"Preparing X, Predicting and Evaluating model\"\"\"\nX_test = prepare_X(df_test)\ny_pred = w0 + X_test.dot(w)\nscore = rmse(y_test, y_pred)\nscore","9e30abcd":"\"\"\"Checking our model on random car selected\"\"\"\ncar = df_test.iloc[20].to_dict()\ncar","3036ad86":"\"\"\"Making dataset out of given dictionary\"\"\"\ndf_small = pd.DataFrame([car])\ndf_small","0c02efb9":"\"\"\"Creating X\"\"\"\nX_small = prepare_X(df_small)\n\"\"\"Predicting y\"\"\"\ny_pred = w0 + X_small.dot(w)\ny_pred = y_pred[0]\ny_pred","275c3798":"\"\"\"Undoing Log by using exp\"\"\"\nnp.expm1(y_pred)","7af9bfe5":"\"\"\"Checking what was the real value of that car.\"\"\"\nnp.expm1(y_test[20])","d9185a6b":"for col in df.columns:\n    print(col)\n    print(df[col].unique()[:5])\n    print(df[col].nunique())\n    print('\\n')","76f9118b":"## 18. Using the model","0a660562":"## Distribution of price\n","c7868584":"## 7. Validation Framework: Creating Train Validation and Test Split Manually","a5c3b6bd":"## 9. Linear Regression Vector Form","8a3d092e":"## 13. Validating the Model using RMSE","4f5471c8":"## 15. Categorical Variables","5ed8f6fb":"## 14. Feature Engineering","5be7c9d8":"## 16. Regularization","7f5bc77e":"### Generalized Linear Regression","6dae6dfd":"### Making List of Categorical Columns","1bef1cee":"## 8. Linear Regression","7b56a875":"## Training Linear Regression - Normal Equationraining Linear Regression - Normal Equation","3a9a427e":"## Log(msrp +1)","1bc4148f":"## 17. Tuning the Model","9bf29401":"## 2. Importing Libraries","c1074027":"### Linear Regression with Multiple Variables","dd363213":"## 5. Exploratory Data Analysis","f1258b1c":"## 3. Loading and Reading Data","14005c73":"## Content in this Notebook\n## Notebook is a part of FREE ML course by Glexey Grigorev. [Link for the Course](https:\/\/github.com\/alexeygrigorev\/mlbookcamp-code\/tree\/master\/course-zoomcamp\/02-regression)","e69b38d8":"## 6. Missing Values","e2fec512":"### Cleaning Categorical Data in our data set","7a9dca47":"## Distribution of price with msrp less than 100000","a829e6b1":"## 12. Root Mean Squared Error (RMSE)","e97f4c32":"## 4. Data Cleaning: Cleaning Strings in Column and values","11a56c76":"## 11. Car Price Baseline Model","9e95d6c4":"## 1. Introduction\nCars dataset with features including make, model, year, engine, and other properties of the car used to predict its price."}}