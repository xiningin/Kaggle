{"cell_type":{"ce638a33":"code","faa718fa":"code","27f63ff1":"code","7ddb260e":"code","4f164cae":"code","803594c8":"code","26873fcb":"code","40882043":"code","9d3a68a4":"code","443bda3c":"code","93e9a8b8":"code","31ca5abd":"code","058bedf2":"code","452e8feb":"code","ec3e8816":"code","6e807f34":"code","5c2662c7":"code","433ae4f1":"code","83bb6156":"code","b7d584eb":"code","169313fd":"code","0fd9456d":"markdown","cad242f6":"markdown","6933e51a":"markdown","0fb11eaa":"markdown","7019eabe":"markdown","79250fba":"markdown","9ef62c27":"markdown","2ed46a46":"markdown","fd5f0aa7":"markdown"},"source":{"ce638a33":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","faa718fa":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes = True)\n%matplotlib inline","27f63ff1":"data = pd.read_csv(\"\/kaggle\/input\/resale-flat-prices-in-singapore\/flat-prices.csv\")\ndata.head()","7ddb260e":"data.info()","4f164cae":"plt.subplots(figsize=(20,10))\n\nplt.subplot(231)\nsns.countplot(x = 'town',data = data)\n\nplt.subplot(232)\nsns.countplot(x = 'flat_type',data = data)\n\nplt.subplot(233)\nsns.countplot(x = 'storey_range',data = data)\n\nplt.subplot(234)\nsns.countplot(x = 'flat_model',data = data)","803594c8":"count_model = data['flat_model'].value_counts().reset_index()\nprint(count_model)\n\ncount = count_model['flat_model']\nlabels = count_model['index']\n","26873fcb":"plt.subplots(figsize=(20,10))\n\nsns.barplot(x='flat_model',y = 'resale_price',data = data,palette = 'rocket')\n","40882043":"plt.subplots(figsize=(20,10))\nexplode = [0,0,0,0,0,0,0,0,0,0,0,0,0.2]\n\nplt.pie(count,labels = labels,wedgeprops = {'edgecolor':'black'},explode = explode,shadow = True,startangle = 150,autopct = '%1.1f%%')\nplt.title('pie-chart of count of all model')","9d3a68a4":"#Convert categorical variable into numerical \nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\n\nlabel = LabelEncoder()\n\ndata['town'] = label.fit_transform(data['town'])\ndata['flat_type'] = label.fit_transform(data['flat_type'])\ndata['street_name'] = label.fit_transform(data['street_name'])\ndata['storey_range'] = label.fit_transform(data['storey_range'])\ndata['flat_model'] = label.fit_transform(data['flat_model'])","443bda3c":"data.head()","93e9a8b8":"# Deal with Outlier with the help of IQR method.\nQ1 = data['resale_price'].quantile(0.25)\nQ3 = data['resale_price'].quantile(0.75)\nIQR = Q3 - Q1\n\nfilter = (data['resale_price'] >= Q1 - 1.5 * IQR) & (data['resale_price']<= Q3 + 1.5 *IQR)\ntrain2 = data.loc[filter]  \nprint(\"data loss percentage {}%\".format(((len(data) - len(train2))\/len(data))*100))","31ca5abd":"x = train2.drop(['month','block','resale_price'],axis = 1)\nx","058bedf2":"y = train2.iloc[:,-1]\ny","452e8feb":"standard = StandardScaler()\n\nstd_x = standard.fit_transform(x)","ec3e8816":"x_final = np.array(std_x)\ny_final = np.array(y)","6e807f34":"\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=5)\nskf.get_n_splits(x_final, y_final)","5c2662c7":" for train_index, test_index in skf.split(x_final, y_final):\n        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n        X_train, X_test = x_final[train_index], x_final[test_index]\n        y_train, y_test = y_final[train_index], y_final[test_index]","433ae4f1":"from sklearn.ensemble import GradientBoostingRegressor\n\nclf = GradientBoostingRegressor(learning_rate = 0.001,n_estimators = 1000,min_samples_split = 4,min_samples_leaf = 3,max_depth = 7,random_state = 21)\n\nclf.fit(X_train,y_train)\ny_predicted = clf.predict(X_test)","83bb6156":"from sklearn.metrics import mean_squared_log_error\n\nmean_squared_log_error(y_test, y_predicted)\n","b7d584eb":"from sklearn.tree import DecisionTreeRegressor\n\nclf1 = DecisionTreeRegressor(criterion = 'mae',splitter = 'random',max_depth = 7,min_samples_split = 5,min_samples_leaf = 3,random_state = 100)\n\nclf1.fit(X_train,y_train)\ny_predicted = clf1.predict(X_test)","169313fd":"from sklearn.metrics import mean_squared_log_error\n\nmean_squared_log_error(y_test, y_predicted)","0fd9456d":"- 1.Import library\n- 2.Import data\n- 3.Data visualization\n- 4.Outliers [using IQR techniuqe Outlier removed] \n- 5.Encode Categorical variable [I checked with nominal encoding technique however ordinal encoding technique gives best result]\n- 6.Use different preprocessing technique [based on best result I choose technique]\n- 7.Validation Technique[train-test-split,kfold,stratified fold] I use best ethod among you can use any one of them.\n- 8.Model [Gradient boosting,decision tree]","cad242f6":"<a id=\"2\"><\/a>\n<h1 style='background:#a9a799; border:0; color:black'><center>LOADING DATA<\/center><\/h1> ","6933e51a":"<a id=\"3\"><\/a>\n <h2 style='background:#a9a799; border:0; color:black'><center>DATA PREPROCESSING & VISUALIZATION<\/center><\/h2> ","0fb11eaa":"\n<a id=\"6\"><\/a>\n <h1 style='background:#a9a799; border:0; color:black'><center>MINIMIZING ERROR<\/center><\/h1> ","7019eabe":"<a id=\"1\"><\/a>\n<h1 style='background:#a9a799; border:0; color:black'><center>LIBRARIES<\/center><\/h1> \n","79250fba":"\n<a id=\"7\"><\/a>\n <h1 style='background:#a9a799; border:0; color:black'><center>CONCLUSION<\/center><\/h1> \n- Model performing good on gradient boosting. you can play with model using hyperparaeter tuning.\n- While building model I cross verify with different preprocessing technique also I checked with different validation technique you can use differnt methods and play with it.\n- Result is cross check using MSLE [mean square log error] that shows values in points.","9ef62c27":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 style='background:#a9a799; border:0; color:black'><center>INDEX<\/center><\/h1>","2ed46a46":"\n<a id=\"5\"><\/a>\n <h1 style='background:#a9a799; border:0; color:black'><center>MODEL BUILDING<\/center><\/h1> ","fd5f0aa7":"\n<a id=\"4\"><\/a>\n <h1 style='background:#a9a799; border:0; color:black'><center>VALIDATE DATA<\/center><\/h1> "}}