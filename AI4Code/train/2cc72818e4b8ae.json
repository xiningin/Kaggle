{"cell_type":{"0fb2cb79":"code","e8ca26ba":"code","96c5f372":"code","545d7d8a":"code","4fbe4d53":"code","91dffb51":"code","21a29c8c":"code","0f8b104b":"code","e1cec062":"code","3c4fd907":"code","0bb95358":"code","b1b5f2cd":"code","63eaaa49":"code","5db98d67":"code","2381db06":"code","e26dee2d":"code","06e77a89":"code","83b8c9db":"code","de717922":"code","79f1f505":"code","e385cffc":"code","f2c95c5b":"code","99debc32":"code","8292f1e0":"code","779662e9":"code","f8f35b28":"markdown","e0ff746c":"markdown","f6ecb750":"markdown","e3ae3ac6":"markdown","6670d92b":"markdown","e4b3326a":"markdown","2c8aef8a":"markdown","e11b3b34":"markdown","5692073b":"markdown","e6b9ac7b":"markdown"},"source":{"0fb2cb79":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn as sk\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import RandomOverSampler\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e8ca26ba":"train = pd.read_csv('\/kaggle\/input\/it-405-dm-and-bi-2020\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/it-405-dm-and-bi-2020\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/it-405-dm-and-bi-2020\/sample_submission.csv')\n\ntarget = train[\"Target\"]\ntrain.drop([\"Id\", \"Target\"], axis=1, inplace=True)\ntest.drop([\"Id\"], axis=1, inplace=True)\n\ntrain.head()","96c5f372":"train_description = train.describe()\nprint(train_description)\n\nprint(\"\\n\")\n\ntrain_info = train.info()\nprint(train_info)\n\nprint(\"\/n\")\n\ntrain.tail()","545d7d8a":"# handling missing values\nprint(train.tail(17))\n\ntrain = train.replace('?', np.nan)\ntest = test.replace('?', np.nan)\n\ntrain.tail(17)","4fbe4d53":"missing_vals = train.isna().sum() \/ len(train) # get the percentage of missing values\n\nprint(missing_vals)","91dffb51":"train = train.loc[:, missing_vals < 0.6]\ntest = test.loc[:, missing_vals < 0.6]","21a29c8c":"train.head()","0f8b104b":"# change missing values with mean\ntrain.fillna(train.mean(), inplace=True)\ntest.fillna(test.mean(), inplace=True)\n# Count the number of nans and print to verify\nprint(train.isnull().sum())","e1cec062":"\nfor col in train.columns:\n    if train[col].dtypes == 'object':\n        train = train.fillna(train[col].value_counts().index[0])\n        \n\n        \nprint(train.isnull().sum())","3c4fd907":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncorr = train.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nf, ax = plt.subplots(figsize=(20, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","0bb95358":"# Data Preprocessing\n# import LabelEncoder\n# loop over all the vals and extract their data types\nle=LabelEncoder()\n\nfor col in train.columns.values:\n    if train[col].dtypes == \"object\":\n        train[col] = le.fit_transform(train[col])\n        \nscaler = sk.preprocessing.StandardScaler()\ntrain = scaler.fit_transform(train)","b1b5f2cd":"randOvSamp = RandomOverSampler(random_state=0)\nx_resamp, y_resamp = randOvSamp.fit_resample(train, target)","63eaaa49":"# Split data train and test \nx_train, x_test, y_train, y_test = sk.model_selection.train_test_split(x_resamp, y_resamp, test_size=0.30, random_state=42)","5db98d67":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\nlogreg = LogisticRegression(solver='lbfgs',max_iter=1000)\ndecTree = DecisionTreeClassifier()\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# Fit models to the train set\nmodel1 = logreg.fit(x_train,y_train)\nmodel2 = decTree.fit(x_train,y_train)\nmodel3 = knn.fit(x_train,y_train)","2381db06":"from sklearn.metrics import confusion_matrix\n\n# Use logreg to predict instances from the test set and store it\ny_pred = model1.predict(x_test)\ny_pred2 = model2.predict(x_test)\ny_pred3 = model3.predict(x_test)\n# Get the accuracy score of logreg model and print it\nprint(\"Accuracy of logistic regression classifier: \", model1.score(x_test,y_test))\nprint(\"Accuracy of decision tree classifier: \", model2.score(x_test,y_test))\nprint(\"Accuracy of K Neighbour classifier: \", model3.score(x_test,y_test))","e26dee2d":"print(\"Confusion matrix for Logistic regression\")\nconfusion_matrix(y_test,y_pred)","06e77a89":"print(\"Confusion matrix for Decision Tree\")\nconfusion_matrix(y_test,y_pred2)","83b8c9db":"print(\"Confusion matrix for K Neighbour classifier\")\nconfusion_matrix(y_test,y_pred3)","de717922":"# Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the grid of values for tol and max_iter\ntol = [0.01, 0.001 ,0.0001]\nmax_iter = [1000, 1500, 2000, 2500, 3000, 3500, 4000]\n\n# set decision tree params\ntree_param = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\n\n# set knn params\nkNeigh_param = {\n    'n_neighbors' : [3, 5, 11, 19],\n    'weights': ['uniform', 'distance'],\n    'metric': ['euclidean', 'manhattan']\n}\n\n# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\nparam_grid = dict(tol=tol, max_iter=max_iter)","79f1f505":"# Instantiate GridSearchCV with the required parameters\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5) # cross validation\n\n# Use scaler to rescale X and assign it to rescaledX\nrescaledX = scaler.fit_transform(x_train)\n\n# Fit grid_model to the data\ngrid_model_result = grid_model.fit(rescaledX,y_train)\n\n# Summarize results\nbest_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\n\n\nprint(\"Best model #1: %f using %s\" % (best_score, best_params))","e385cffc":"grid_model2 = GridSearchCV(estimator=decTree, param_grid=tree_param , cv=5)\n\ngrid_model_result2 = grid_model2.fit(rescaledX,y_train)\n\nbest_score2, best_params2 = grid_model_result2.best_score_, grid_model_result2.best_params_\n\nprint(\"Best model #2: %f using %s\" % (best_score2, best_params2))","f2c95c5b":"for col in test.columns:\n    if test[col].dtypes == 'object':\n        test = test.fillna(test[col].value_counts().index[0])\n        \n\n        \nprint(test.isnull().sum())","99debc32":"le=LabelEncoder()\n\nfor col in test.columns.values:\n    if test[col].dtypes == \"object\":\n        test[col] = le.fit_transform(test[col])\n        \nscaler = sk.preprocessing.StandardScaler()\ntest = scaler.fit_transform(test)","8292f1e0":"sample_submission['Target'] = model2.predict_proba(test)[:,1]\nsample_submission['Target'].head(10)","779662e9":"sample_submission.to_csv('mukhammad_sobirov_final_submission_results).csv',index=False)","f8f35b28":"# Instantiate a models with default parameter values","e0ff746c":"# Handling the missing values part 3\n* Check the type of a columns\n* If object, imput with the most frequent val\n* Then print the number of nans to verify\n* It'll take some time, since we have over 180 columns","f6ecb750":"* by Mukhammad Sobirov\n* ID: 20182303041","e3ae3ac6":"# Handling the missing values part 1","6670d92b":"# Data Preprocessing part 1\n* Loop over all the values and extract their data types\n1. Convert the non-numeric data into numeric.\n2. Split the data into train and test sets.\n3. Scale the feature values to a uniform range.","e4b3326a":"#  Handling the missing values part 2\n* Fill missing values with means of the classes\n","2c8aef8a":"# Using GridSearchCV we can tune our models to perform better\n* At the next step we define configurations for each of our 3 models","e11b3b34":"## After configs defined, we can run GridSearchCV. Also I'm using 5 folds cross validation","5692073b":"# Decision Tree Classifier vs Logistic Regression\n* After analysing confusion matrixes of all three classifiers we can conclude that decision tree classifier performs better.\n* Grid Search results also prove that the accuracy with the current parameters works well enough to emit needed result\n* However decision tree performs poorly when in comes to test set\n* So, i decided to go with Logistic regression","e6b9ac7b":"# Print confusion matrixes for each model"}}