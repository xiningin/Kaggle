{"cell_type":{"c9b1915e":"code","692718d4":"code","4f210ea7":"code","2e406862":"code","ed5c4e9a":"code","17666e0d":"code","05982a94":"markdown","cdaef773":"markdown","e13c190e":"markdown","ac05949f":"markdown","69692ef2":"markdown","ac2c17a5":"markdown","030a3092":"markdown","ecd02a9c":"markdown","dc0539bd":"markdown"},"source":{"c9b1915e":"import numpy as np\nimport pandas as pd\nimport os\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML\nimport seaborn as sns","692718d4":"df=pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf['TotalCharges']=df['TotalCharges'].convert_objects(convert_numeric=True)\ndataobject=df.select_dtypes(['object'])\n\nfor i in range(1,len(dataobject.columns)):\n    df[dataobject.columns[i]] = LabelEncoder().fit_transform(df[dataobject.columns[i]])\nunwantedcolumnlist=[\"customerID\",\"gender\",\"MultipleLines\",\"PaymentMethod\",\"tenure\"]\n\ndf = df.drop(unwantedcolumnlist, axis=1)\nfeatures = df.drop([\"Churn\"], axis=1).columns\n\ndf_train, df_val = train_test_split(df, test_size=0.30)\ndf_train['TotalCharges'].fillna(-999, inplace=True)\ndf_val['TotalCharges'].fillna(-999, inplace=True)\n\nclf = RandomForestClassifier(n_estimators=30 , oob_score = True, n_jobs = -1,random_state =50, max_features = \"auto\", min_samples_leaf = 50)\nclf.fit(df_train[features], df_train[\"Churn\"])\n\n# Make predictions\npredictions = clf.predict(df_val[features])\nprobs = clf.predict_proba(df_val[features])\n\nscore = clf.score(df_val[features], df_val[\"Churn\"])\nprint(\"Accuracy: \", score)\n\nget_ipython().magic('matplotlib inline')\ncm = pd.DataFrame(\n    confusion_matrix(df_val[\"Churn\"], predictions), \n    columns=[\"Predicted False\", \"Predicted True\"], \n    index=[\"Actual False\", \"Actual True\"]\n)\ndisplay(cm)\n\n# Calculate the fpr and tpr for all thresholds of the classification\nfpr, tpr, threshold = roc_curve(df_val[\"Churn\"], probs[:,1])\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","4f210ea7":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(clf, random_state=1).fit(df_val[features], df_val[\"Churn\"])\neli5.show_weights(perm, feature_names = list(features))","2e406862":"from sklearn.feature_selection import SelectFromModel\n\nsel = SelectFromModel(perm, threshold=-0.0001, prefit=True)\nX_tr = sel.transform(df_train[features])\nX_val = sel.transform(df_val[features])\n\nclf2 = RandomForestClassifier(n_estimators=30 , oob_score = True, n_jobs = -1,random_state =50, max_features = \"auto\", min_samples_leaf = 50)\nclf2.fit(X_tr, df_train[\"Churn\"])\n\n# Make predictions\npredictions = clf2.predict(X_val)\nprobs = clf2.predict_proba(X_val)\n\nscore = clf2.score(X_val, df_val[\"Churn\"])\nprint(\"Accuracy: \", score)\n\nget_ipython().magic('matplotlib inline')\ncm = pd.DataFrame(\n    confusion_matrix(df_val[\"Churn\"], predictions), \n    columns=[\"Predicted False\", \"Predicted True\"], \n    index=[\"Actual False\", \"Actual True\"]\n)\ndisplay(cm)\n\n# Calculate the fpr and tpr for all thresholds of the classification\nfpr, tpr, threshold = roc_curve(df_val[\"Churn\"], probs[:,1])\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","ed5c4e9a":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\nfeatures = df_val.drop([\"Churn\"], axis=1).columns.values\nfeature_to_plot = 'TotalCharges'\npdp_goals = pdp.pdp_isolate(model=clf, dataset=df_val[features], model_features=list(features), feature=feature_to_plot)\n\n# plot it\npdp.pdp_plot(pdp_goals, feature_to_plot)\nplt.show()","17666e0d":"# Create the data that we will plot\nfeature_to_plot = 'Contract'\npdp_goals = pdp.pdp_isolate(model=clf, dataset=df_val[features], model_features=list(features), feature=feature_to_plot)\n\n# plot it\npdp.pdp_plot(pdp_goals, feature_to_plot)\nplt.show()","05982a94":"Conclusion:\n    \"TotalCharges\" and \"Contract\" are important features, and they have negative effect for target.","cdaef773":" This kernel is folked from [here](https:\/\/www.kaggle.com\/sanket30\/churn-prediction-using-rf).\n And I try the method of [kaggle insight challenge](https:\/\/www.kaggle.com\/general\/65605).","e13c190e":"If \"TotalCharges\" increase, target value decrease.","ac05949f":"First, I make Random Forest model.","69692ef2":"\"TotalCharges\" and \"Contract\" are important features.\nSome features have negative effect for prediction. But it is affected randomness, the result will change if you try to run this script again.\nMaybe \"Partner\" and \"DeviceProtection\" have negative effect.","ac2c17a5":"I show PermutationImportance with eli5.","030a3092":"Next I show pdp of important features that shown by PermutationImportance.","ecd02a9c":"I drop PermutationImportance < -0.0001 features.\nBut in many cases, the score decrease.\nIt is because the number of data is few, so PermutationImportance's values is affected much by randomness.","dc0539bd":"If \"Contract\" increase, target value decrease."}}