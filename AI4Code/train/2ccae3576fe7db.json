{"cell_type":{"8b080949":"code","ab2915d9":"code","607658e1":"code","6562bc13":"code","cbb229fe":"code","79179789":"code","f4a6c523":"code","bbd25b04":"code","d4e6c94c":"code","ad3af001":"code","c091662c":"code","cdd83b24":"code","4b78789a":"markdown","6d36c591":"markdown","ac79626e":"markdown","2f2aeca9":"markdown"},"source":{"8b080949":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom IPython.display import Image","ab2915d9":"Image(\"..\/input\/fig1.png\",width=900,height=400)","607658e1":"class Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(2,2)\n        self.s1 = nn.Sigmoid()\n        self.fc2 = nn.Linear(2,2)\n        self.s2 = nn.Sigmoid()\n        self.fc1.weight = torch.nn.Parameter(torch.Tensor([[0.15,0.2],[0.250,0.30]]))\n        self.fc1.bias = torch.nn.Parameter(torch.Tensor([0.35]))\n        self.fc2.weight = torch.nn.Parameter(torch.Tensor([[0.4,0.45],[0.5,0.55]]))\n        self.fc2.bias = torch.nn.Parameter(torch.Tensor([0.6]))\n        \n    def forward(self, x):\n        x= self.fc1(x)\n        x = self.s1(x)\n        x= self.fc2(x)\n        x = self.s2(x)\n        return x\n\nnet = Net()\nprint(net)","6562bc13":"# parameters: weight and bias\nprint(list(net.parameters()))\n# input data\nweight2 = list(net.parameters())[2]\ndata = torch.Tensor([0.05,0.1])","cbb229fe":"# output of last layer\nout = net(data)\ntarget = torch.Tensor([0.01,0.99])  # a dummy target, for example\ncriterion = nn.MSELoss()\nloss = criterion(out, target); loss","79179789":"# A simple hook class that returns the input and output of a layer during forward\/backward pass\nclass Hook():\n    def __init__(self, module, backward=False):\n        if backward==False:\n            self.hook = module.register_forward_hook(self.hook_fn)\n        else:\n            self.hook = module.register_backward_hook(self.hook_fn)\n    def hook_fn(self, module, input, output):\n        self.input = input\n        self.output = output\n    def close(self):\n        self.hook.remove()","f4a6c523":"# register hooks on each layer\nhookF = [Hook(layer[1]) for layer in list(net._modules.items())]\nhookB = [Hook(layer[1],backward=True) for layer in list(net._modules.items())]\n# run a data batch\nout=net(data)\n# backprop once to get the backward hook results\nout.backward(torch.tensor([1,1],dtype=torch.float),retain_graph=True)\n#! loss.backward(retain_graph=True)  # doesn't work with backward hooks, \n#! since it's not a network layer but an aggregated result from the outputs of last layer vs target \n\nprint('***'*3+'  Forward Hooks Inputs & Outputs  '+'***'*3)\nfor hook in hookF:\n    print(hook.input)\n    print(hook.output)\n    print('---'*17)\nprint('\\n')\nprint('***'*3+'  Backward Hooks Inputs & Outputs  '+'***'*3)\nfor hook in hookB:             \n    print(hook.input)          \n    print(hook.output)         \n    print('---'*17)","bbd25b04":"# Confirm the calculations with the print result above\n# the 4th layer - sigmoid\nforward_output = np.array([0.7514, 0.7729]) \ngrad_in = np.array([1,1])  # sigmoid layer\n# grad of sigmoid(x) wrt x is: sigmoid(x)(1-sigmoid(x))\ngrad_out = grad_in*(forward_output*(1-forward_output)); grad_out ","d4e6c94c":"# the 3th layer - linear\nprint([0.1868, 0.1755])  # grad_input * (grad of Wx+b = (w1*x1+w2*x2)+b wrt W) \nprint(0.1868 + 0.1755)   # grad of Wx+b wrt b o\n\ngrad_in = torch.Tensor(grad_out)\ngrad_in.view(1,-1) @ weight2;grad_out  # grad of layer output wrt input: wx+b => w","ad3af001":"# the 2nd layer - sigmoid\nforward_output=np.array([0.5933, 0.5969])\ngrad_in=np.array([0.1625, 0.1806])\ngrad_in*(forward_output*(1-forward_output)) # grad * (grad of sigmoid(x) wrt x)","c091662c":"# gradient of loss wrt prarameters\nnet.zero_grad()\nloss.backward(retain_graph=True)\n[print(p.grad) for p in net.parameters()]","cdd83b24":"class Guided_backprop():\n    \"\"\"\n        Visualize CNN activation maps with guided backprop.\n        \n        Returns: An image that represent what the network learnt for recognizing \n        the given image. \n        \n        Methods: First layer input that minimize the error between the last layers output,\n        for the given class, and the true label(=1). \n        \n        ! Call visualize(image) to get the image representation\n    \"\"\"\n    def __init__(self,model):\n        self.model = model\n        self.image_reconstruction = None\n        self.activation_maps = []\n        # eval mode\n        self.model.eval()\n        self.register_hooks()\n    \n    def register_hooks(self):\n        \n        def first_layer_hook_fn(module, grad_out, grad_in):\n            \"\"\" Return reconstructed activation image\"\"\"\n            self.image_reconstruction = grad_out[0] \n            \n        def forward_hook_fn(module, input, output):\n            \"\"\" Stores the forward pass outputs (activation maps)\"\"\"\n            self.activation_maps.append(output)\n            \n        def backward_hook_fn(module, grad_out, grad_in):\n            \"\"\" Output the grad of model output wrt. layer (only positive) \"\"\"\n            \n            # Gradient of forward_output wrt. forward_input = error of activation map:\n                # for relu layer: grad of zero = 0, grad of identity = 1\n            grad = self.activation_maps[-1] # corresponding forward pass output \n            grad[grad>0] = 1 # grad of relu when > 0\n            \n            # set negative output gradient to 0 #!???\n            positive_grad_out = torch.clamp(input=grad_out[0],min=0.0)\n            \n            # backward grad_out = grad_out * (grad of forward output wrt. forward input)\n            new_grad_out = positive_grad_out * grad\n            \n            del self.forward_outputs[-1] \n            \n            # For hook functions, the returned value will be the new grad_out\n            return (new_grad_out,)\n            \n        # !!!!!!!!!!!!!!!! change the modules !!!!!!!!!!!!!!!!!!\n        # only conv layers, no flattened fc linear layers\n        modules = list(self.model.features._modules.items())\n        \n        # register hooks to relu layers\n        for name, module in modules:\n            if isinstance(module, nn.ReLU):\n                module.register_forward_hook(forward_hook_fn)\n                module.register_backward_hook(backward_hook_fn)\n        \n        # register hook to the first layer \n        first_layer = modules[0][1] \n        first_layer.register_backward_hook(first_layer_hook_fn)\n        \n    def visualize(self, input_image, target_class):\n        # last layer output\n        model_output = self.model(input_image)\n        self.model.zero_grad()\n        \n        # only calculate gradients wrt. target class \n        # set the other classes to 0: eg. [0,0,1]\n        grad_target_map = torch.zeros(model_output.shape,\n                                     dtype=torch.float)\n        grad_target_map[0][target_class] = 1\n        \n        model_output.backward(grad_target_map)\n        \n        # Convert Pytorch variable to numpy array\n        # [0] to get rid of the first channel (1,3,224,224)\n        result = self.image_reconstruction.data.numpy()[0] \n        return result","4b78789a":"## Modify gradients with hooks\n- Hook function doesn't change gradients by default\n- But if __return__ is called, the returned value will be the gradient output","6d36c591":"## What is the input and output of forward and backward pass? \n\n### __Things to notice__: \n1. Because backward pass runs from back to the start, it's __parameter order__ should be reversed compared to the forward pass. Therefore, to be it clearer, I'll use a different naming convention below.\n2. For forward pass, __previous layer__ of layer 2 is layer1; for backward pass, previous layer of layer 2 is layer 3. \n3. __Model output__ is the output of last layer in forward pass. \n\n__layer.register_backward_hook(module, input, output)__\n- __Input__: previous layer's output <br>  \n- __Output__: current layer's output <br>\n\n__layer.register_backward_hook(module, grad_out, grad_in)__\n- __Grad_in__: __gradient of model output wrt. layer output__      &nbsp;&nbsp; &nbsp;&nbsp;  # from forward pass <br>\n    - = a tensor that represent the __error of each neuron in this layer__ (= gradient of model output wrt. layer output = how much it should be improved)\n    - For the last layer: eg. [1,1] <=> gradient of model output wrt. itself, which means calculate all gradients as normal\n    - It can also be considered as a weight map:  eg. [1,0] turn off the second gradient; [2,1] put double weight on first gradient etc.\n- __Grad_out__: Grad_in * (gradient of layer output wrt. layer input)<br> \n   - = __next layer's error__(due to chain rule)\n    \nCheck the print from the cell above to confirm and enhance your understanding!","ac79626e":"# Toy example to understand Pytorch hooks\n\nUse a toy example to understand what Pytorch hooks do and how to use it. \n\nThe toy neural net's back prop is fully calculated in this post: https:\/\/mattmazur.com\/2015\/03\/17\/a-step-by-step-backpropagation-example\/ <br>\nA more detailed example: https:\/\/medium.com\/@14prakash\/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c\n\nConclusion: backpropagation can be intuitively seen as linking total error to individual parameters. Pytorch hook can record the specific error of a parameter(weights, activations...etc) at a specific training time. We can then use these gradient records to do many useful things such as visualizing neural network with [GRAD-CAM](https:\/\/arxiv.org\/pdf\/1610.02391.pdf). ","2f2aeca9":"## Guided backpropagation with hooks - Visualize CNN (deconv)"}}