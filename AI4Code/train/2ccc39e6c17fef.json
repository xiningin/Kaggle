{"cell_type":{"7ed4d0b9":"code","9fdef6df":"code","e6d4782d":"code","798e246a":"code","57c24b1e":"code","7334d104":"code","86467d7f":"code","03b0da02":"code","f8d6fb47":"code","00fe1c76":"code","4fdd6d24":"code","5dab81b6":"code","b898cbe1":"code","121926c8":"code","0d1a45f5":"code","9fab4231":"code","84e4718f":"markdown","ddfcfb08":"markdown","8e1e0290":"markdown","1f067c93":"markdown","bfddcfff":"markdown","02faf183":"markdown","bab22435":"markdown","ae6cb28d":"markdown","f18a0b66":"markdown","f11ad5fd":"markdown","2e55f72d":"markdown","c4defb3c":"markdown"},"source":{"7ed4d0b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9fdef6df":"df = pd.read_csv('..\/input\/test-dataset\/dataset_00_with_header.csv');\ndf","e6d4782d":"df.describe()","798e246a":"\ncols_with_missing = [col for col in df.columns if df[col].isnull().any()]\n\nprint(cols_with_missing)","57c24b1e":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer(strategy='most_frequent')\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(df))\n# imputed_X_valid = pd.DataFrame(my_imputer.transform(data_test))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = df.columns\n# imputed_X_valid.columns = data_test.columns\n\ndf = imputed_X_train\ndf","7334d104":"cols_with_missing = [col for col in df.columns if df[col].isnull().any()]\ncols_with_missing","86467d7f":"string_Data = (df.dtypes == 'object')\nobject_cols = list(string_Data[string_Data].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","03b0da02":"y = df.y\n#############################\nX = df.drop(columns=['y'])","f8d6fb47":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25)","00fe1c76":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error ,explained_variance_score, mean_squared_error","4fdd6d24":"parameters1 = {'learning_rate':  [x \/ 100 for x in range(5, 101, 5)],\n              'max_depth':  list(range(6, 30, 6)),\n              'n_estimators': list(range(50, 1001, 50))}","5dab81b6":"parameters = {'learning_rate':  [0.02],\n              'max_depth':  [6],\n              'n_estimators': [100]}","b898cbe1":"# from sklearn.model_selection import GridSearchCV\n\n# gsearch = GridSearchCV(estimator=XGBRegressor(),\n#                        param_grid = parameters, \n#                        scoring='neg_mean_squared_error',\n#                        n_jobs=4,cv=5,verbose=7)\n\n# gsearch.fit(X_train, y_train)","121926c8":"# print(gsearch.best_params_.get('n_estimators'))\n# print(gsearch.best_params_.get('learning_rate'))\n# print(gsearch.best_params_.get('max_depth'))","0d1a45f5":"# my_model = XGBRegressor(learning_rate = gsearch.best_params_.get('learning_rate'),\n#                          max_depth = gsearch.best_params_.get('max_depth'),\n#               n_estimators = gsearch.best_params_.get('n_estimators'),random_state=1, n_jobs=4)\n\n# my_model.fit(X_train, y_train)\n\n# predictions = my_model.predict(X_test)\n\n# squared_error = mean_squared_error(y_true=y_test,y_pred = predictions)\n\n# print(squared_error)","9fab4231":"my_model = XGBRegressor(learning_rate =0.2,\n                         max_depth = 6,\n              n_estimators = 70,random_state=1, n_jobs=4)\n\nmy_model.fit(X_train, y_train)\n\npredictions = my_model.predict(X_test)\n\nsquared_error = mean_squared_error(y_true=y_test,y_pred = predictions)\n\nprint(squared_error)","84e4718f":"# Read DataSet & Show the data","ddfcfb08":"# I will use the xgboost in order to predict the error rate and build the Model.","8e1e0290":"# Here I will divide the data for training and test data where the test data will be 25% of the total data.","1f067c93":"# Here I will determine the value of the y goal and the x.","bfddcfff":"# Process columns that contain blank data or Nan or Null values by calculating the average calculation of column values.","02faf183":"# Show any columns have missing values","bab22435":"# Here I search for the best values for learning_rate, max_depth and n_estimators.","ae6cb28d":"# After the data is processed, the missing data has been processed and there are no faulty columns and we will continue to work now.","f18a0b66":"# The Best Values...","f11ad5fd":"# Here the Model will be built and fit work for it and predict the results and take out the value of the mean_squared_error.","2e55f72d":"# Here I check whether there is object data to delete it because its presence will negatively affect the final results.","c4defb3c":"# Show describe for dataset"}}