{"cell_type":{"dfe39e41":"code","5433c36b":"code","7b32308f":"code","a35d49bd":"code","6daebae1":"code","566f00d2":"code","d18c419d":"code","33a59178":"code","de195d8d":"code","04229551":"code","11e8624d":"code","36ba6b74":"code","4ea96cc4":"code","74be419c":"code","26412fb9":"code","6c07b525":"code","cf5eefd5":"code","70c50fc5":"code","85b39b53":"code","581c0082":"code","a09570c8":"code","d577c1e7":"code","cef79507":"code","280b23ea":"code","377496eb":"code","f4ea6fb4":"code","bef9715b":"code","7391e1b1":"code","5d243555":"code","b0c4f72b":"code","8ee225df":"code","1842c09d":"code","34195ded":"code","75f5625c":"code","12cca8e0":"code","a5a5b055":"code","8df040d1":"code","3c49eb07":"code","3b1aafbf":"code","ca913913":"code","ded8484d":"code","cf86e6e8":"code","6044e294":"code","702e6a5c":"code","2a05e0fe":"code","a9f63bba":"code","2e8143df":"code","09d8f801":"code","03c8900c":"code","72d7d1d7":"code","4d5359e5":"code","5340aa32":"code","1109bdbc":"code","42795e70":"code","54fc8810":"code","454dcd09":"code","5c67d848":"code","148abdae":"code","c1a6f89b":"code","ee530efb":"code","24170a8a":"code","103027f9":"code","d9943643":"code","f1f82d4e":"code","40390f1a":"markdown","d5591b88":"markdown","b9ea5928":"markdown","ec50a8f8":"markdown","9dabd901":"markdown","ac0bf796":"markdown","31dba8af":"markdown"},"source":{"dfe39e41":"!pip install nagisa","5433c36b":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport time\nimport cv2\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom statsmodels.graphics.gofplots import qqplot\nimport os\nimport nagisa","7b32308f":"from tensorflow import keras\nfrom keras.applications.resnet50 import ResNet50\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import layers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom gensim.models import KeyedVectors\nfrom tensorflow import keras\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications import DenseNet201\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, add\nfrom keras import layers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers.merge import concatenate\n","a35d49bd":"from PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nDIM = 256\nIMG_DIM = (DIM, DIM)\nIMG_SHAPE = (DIM, DIM, 3)\nIMG_PREFIX  = \"..\/input\/rakuten-multimodal-colour-extraction\/images\/images\/\"","6daebae1":"def image_to_array(path_image):\n    # Load image as a PIL Image\n    raw_img = keras.preprocessing.image.load_img(IMG_PREFIX+path_image, target_size=IMG_DIM)\n    \n    try:\n        # Convert PIL Image to NumPy array\n        img_array = keras.preprocessing.image.img_to_array(raw_img)\n    except:\n        return np.nan\n    \n    return img_array","566f00d2":"X_train = pd.read_csv(\"..\/input\/rakuten-multimodal-colour-extraction\/X_train.csv\", index_col=0)\ny_train = pd.read_csv(\"..\/input\/rakuten-multimodal-colour-extraction\/y_train.csv\", index_col=0)\nX_test_vrai = pd.read_csv(\"..\/input\/rakuten-multimodal-colour-extraction\/X_test.csv\", index_col=0)\ntrain_clean = pd.read_csv('..\/input\/rakuten-multimodal-colour-extraction\/train_clean.csv')\ntest_clean = pd.read_csv('..\/input\/rakuten-multimodal-colour-extraction\/test_clean.csv')","d18c419d":"separated_X_train = pd.read_csv('..\/input\/d\/xianlili\/vectorized-x-train\/separated_X_train.csv')","33a59178":"vectorized_X_test = pd.read_csv('..\/input\/d\/xianlili\/vectorized-x-train\/vectorized_X_test.csv', index_col=0)","de195d8d":"vectorized_X_train = pd.read_csv('..\/input\/d\/xianlili\/vectorized-x-train\/vectorized_X_train.csv', index_col=0)","04229551":"#y_train['color_tags'] = y_train['color_tags'].apply(lambda x : x.strip(\"[]\")).apply(lambda x : x.split(', ')).apply(lambda x : [e.strip(\"'\").strip(\"'\") for e in x])\n#y_test = pd.DataFrame(y_train['color_tags'])\n#y_test['image_file_name'] = X_train[\"image_file_name\"]\n#y_test.head(","11e8624d":"#vectorized_X_train['vectorized item description'] = vectorized_X_train['vectorized item description'].apply(lambda x : x.strip(\"[]\"))","36ba6b74":"#vectorized_X_train['vectorized item description'] = vectorized_X_train['vectorized item description'].apply(lambda x : x.strip(\"[]\"))","4ea96cc4":"#vectorized_X_train['vectorized item description'] = vectorized_X_train['vectorized item description'].apply(lambda x : x.strip(\"[]\"))","74be419c":"train = pd.concat([\n    X_train,y_train,vectorized_X_train,train_clean,separated_X_train\n], axis=1)","26412fb9":"train = train.loc[:,~train.columns.duplicated()]\n","6c07b525":"train.isna().sum()\n","cf5eefd5":"train = train.dropna()","70c50fc5":"train = train.reset_index(drop=True)","85b39b53":"train.shape","581c0082":"test = pd.concat([\n    X_test_vrai,y_train,vectorized_X_test,test_clean\n], axis=1)","a09570c8":"test = test.dropna()","d577c1e7":"test.shape","cef79507":"val = train.iloc[4000:4901,:]","280b23ea":"test = X_test_vrai.iloc[:900,:]","377496eb":"train_2 = train.iloc[:4000,:]","f4ea6fb4":"train.shape","bef9715b":"val.shape","7391e1b1":"val = val.reset_index(drop=True)","5d243555":"def encode(encoder, colors):\n    # Encode colors to numerical array\n    indexes = encoder.transform(colors)\n    \n    # Create an array with 0s\n    encoded = np.zeros(len(encoder.classes_))\n    \n    # Put 1s at the specified indexes\n    encoded[indexes] = 1\n    \n    return encoded\n\ndef decode(encoder, encoded):\n    # Decode the encoded\n    decoded = np.where(encoded == 1)[0].ravel()\n            \n    # Get the colors\n    return encoder.inverse_transform(decoded)\n\ndef extract_colors(col):\n    colors = set()\n    \n    # Loop over the rows\n    for row in col.values:\n        # Some cleaning & list transformation\n        row = row[1:-1].split(', ')\n        # Loop over the elements of the list\n        for color in row:\n            # Quick clean\n            color = color.replace(\"'\", \"\")\n            # Add to the set\n            colors.add(color)\n        \n    return list(colors)\n\nclean_tags = lambda x: [e.replace(\"'\", \"\")  for e in x[1:-1].split(', ')]","b0c4f72b":"%%time\n\n# Get colors\ncolors = extract_colors(train['color_tags'])\n\n# Prepare the encoder for color tags\nencoder = LabelEncoder()\nencoder = encoder.fit(colors)","8ee225df":"%%time\n\n# Clean\ntrain[\"color_tags\"] = train[\"color_tags\"].apply(clean_tags)\n\n# Create column encoded which are the encoded colors\ntrain[\"encoded_colors\"] = train[\"color_tags\"].apply(lambda x: encode(encoder, x))\n\n\n\n# Clean\nval[\"color_tags\"] = val[\"color_tags\"].apply(clean_tags)\n\n# Create column encoded which are the encoded colors\nval[\"encoded_colors\"] = val[\"color_tags\"].apply(lambda x: encode(encoder, x))","1842c09d":"print(len(train))\ntrain = train[train[\"color_tags\"].apply(len) < 4]\ntrain_2 = train_2[train_2[\"color_tags\"].apply(len) < 4]\nprint(len(train))","34195ded":"print(len(val))\nval = val[val[\"color_tags\"].apply(len) < 4]\nprint(len(val))","75f5625c":"from sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\nmlb.fit(train[\"color_tags\"].tolist())\nmlb.classes_","12cca8e0":"train_labels = mlb.transform(train[\"color_tags\"].tolist())\ntrain_labels.shape","a5a5b055":"train_2_labels = mlb.transform(train_2[\"color_tags\"].tolist())\ntrain_2_labels.shape","8df040d1":"val_labels = mlb.transform(val[\"color_tags\"].tolist())\nval_labels.shape","3c49eb07":"train[\"vectorized item description\"] = train[\"vectorized item description\"].apply(lambda x : x.strip(\"[]\")).apply(lambda x : x.split(', ')).apply(lambda x : [float(element) for element in x] )\ntrain[\"vectorized item name\"] = train[\"vectorized item name\"].apply(lambda x : x.strip(\"[]\")).apply(lambda x : x.split(', ')).apply(lambda x : [float(element) for element in x] )\n\nvectorized_array_train = np.array([x for x in train['vectorized item description']])","3b1aafbf":"'''%%time\n\nimg_rows , img_cols = 100,100\nnum_classes = 19\nbatch_size = 32\nnb_epoch = 5\n\n\n#filepath = 'color_weights.hdf5'\n#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n#callbacks_list = [checkpoint]\n\ndatagen=ImageDataGenerator(rescale=1.\/255, validation_split = 0.33)\n\ntrain_generator_image=datagen.flow_from_dataframe(dataframe=train,seed=42,\n                                            x_col = \"image_file_name\",\n                                            y_col =\"color_tags\",\n                                            directory=\"..\/input\/rakuten-multimodal-colour-extraction\/images\/images\",\n                                            shuffle=True, subset =\"training\",\n                                            class_mode=\"categorical\", target_size=(256, 256),\n                                            validate_filenames = True)\n\nvalidation_generator=datagen.flow_from_dataframe(dataframe=train,seed=42,\n                                                 x_col = \"image_file_name\",\n                                                 y_col =\"color_tags\",\n                                                 directory=\"..\/input\/rakuten-multimodal-colour-extraction\/images\/images\",\n                                                 shuffle=True, subset =\"validation\",\n                                                 class_mode=\"categorical\", target_size=(256, 256),\n                                                 validate_filenames = True)\ntest_image=next(validation_generator)'''","ca913913":"#y_train['color_tags'] = y_train['color_tags'].apply(lambda x : x.strip(\"[]\")).apply(lambda x : x.split(', ')).apply(lambda x : [e.strip(\"'\").strip(\"'\") for e in x])\n#colors = pd.DataFrame(y_train['color_tags'])\n#colors['image_file_name'] = X_train[\"image_file_name\"]\n#y_test['vectorized_item_description'] = vectorized_X_train['vectorized item description']\n#y_test['item_caption'] = vectorized_X_train['item_caption']\n#colors.head()","ded8484d":"from keras import backend as K\n#IMPLEMENTATION DE LA METRIC UTILISEE PAR LE CONCOURS F1\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","cf86e6e8":"densenet_weights_path = '..\/input\/densenet201-imagenet-pretrained-weights\/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5'\n","6044e294":"#train_2.shape","702e6a5c":"%%time\nimages_train = np.array([image_to_array(x) for x in train_2[\"image_file_name\"]])\n","2a05e0fe":"%%time\nimages_val = np.array([image_to_array(x) for x in val[\"image_file_name\"]])","a9f63bba":"%%time\n#images_test = np.array([image_to_array(x) for x in test[\"image_file_name\"]])","2e8143df":"vectorized_array_train","09d8f801":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model, load_model\nfrom keras.layers import Embedding, LSTM, Dropout, Dense, Input, Bidirectional, Flatten, Conv2D, MaxPooling2D, concatenate, Conv1D, MaxPooling1D\nimport keras.backend as K\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping","03c8900c":"MAX_NB_WORDS = 50000\nMAX_SEQUENCE_LENGTH = train['item_description'].map(len).max()\nEMBEDDING_DIM = 300\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\ntokenizer.fit_on_texts(train['item_description'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\nprint('Max len:', MAX_SEQUENCE_LENGTH)","72d7d1d7":"vectorized_array_train","4d5359e5":"train","5340aa32":"X_text_train = tokenizer.texts_to_sequences(train_2['separated item description'].values)\nX_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of train tensor:', X_text_train.shape)","1109bdbc":"X_text_val = tokenizer.texts_to_sequences(val['separated item description'].values)\nX_text_val = pad_sequences(X_text_val, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of train tensor:', X_text_val.shape)","42795e70":"X_text_train","54fc8810":"#X_text_test = tokenizer.texts_to_sequences(test['item_description'].values)\n#X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n#print('Shape of train tensor:', X_text_test.shape)","454dcd09":"final_model = Sequential()\n\n\n","5c67d848":"from keras.applications import DenseNet201\nfrom keras import optimizers\nfrom tensorflow import keras\n\ndef compile_model(embedding_matrix):\n    \n    lstm_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n    x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], mask_zero=True, input_length=MAX_SEQUENCE_LENGTH, trainable=False)(lstm_input)\n    x = Dropout(0.3)(x)\n    x = LSTM(64, return_sequences = True)(x)\n    x = Dropout(0.3)(x)\n    x = LSTM(64)(x)\n    x = Dropout(0.3)(x)\n    lstm_out = Dense(18, activation = 'relu')(x)\n\n    print(images_train.shape[1], images_train.shape[2], images_train.shape[3])\n    cnn_input = Input(shape=(images_train.shape[1], images_train.shape[2], images_train.shape[3]))\n    y = DenseNet201(include_top = False, pooling = 'avg', weights=densenet_weights_path)(cnn_input)\n    cnn_out = Dense(512, activation='relu')(y)\n\n    concat_inp = concatenate([cnn_out, lstm_out])\n    z = Dense(256, activation='relu')(concat_inp)\n    z = Dropout(0.3)(z)\n    z = Dense(128, activation='relu')(z)\n    z = Dropout(0.3)(z)\n    output = Dense(train_labels.shape[1], activation='sigmoid')(z)\n\n    model = Model(inputs=[cnn_input, lstm_input], outputs=[output])\n    adam = Adam(lr=0.001, decay=1e-5)\n\n    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy',f1])\n    \n    return model\n  ","148abdae":"word2vec_model = compile_model(vectorized_array_train)\nword2vec_model.summary()","c1a6f89b":"word2vec_model.layers[7].trainable = False #freeze DenseNet201\nword2vec_model.layers[1].trainable = False #freeze DenseNet201","ee530efb":"es = EarlyStopping(patience=5)\ncallbacks = [keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.h5\")]","24170a8a":"word2vec_multi_modal_model = word2vec_model.fit([images_train, X_text_train], train_labels, batch_size=32, epochs=7,\n                            validation_data=([images_val, X_text_val], val_labels), callbacks = [es,callbacks])\n\nword2vec_multi_modal_model.save('word2vec_model.h5')","103027f9":"callbacks = [keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.h5\")]\n\nfit_history = model4.fit(\n        train_generator,\n        steps_per_epoch = 500,\n        epochs = 10,\n        validation_data=validation_generator,\n        validation_steps = 50\n)","d9943643":"loss     = word2vec_multi_modal_model.history['loss']\nval_loss = word2vec_multi_modal_model.history['val_loss']\nacc      = word2vec_multi_modal_model.history['accuracy']\nval_acc  = word2vec_multi_modal_model.history['val_accuracy']\nf1       = word2vec_multi_modal_model.history['f1']\nval_f1   = word2vec_multi_modal_model.history['val_f1']\n\n# Visualize the history plots\nplt.figure()\nplt.plot(loss, 'b', label='Training loss')\nplt.plot(val_loss, 'm', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\nplt.figure()\nplt.plot(acc, 'b', label='Training acc')\nplt.plot(f1, 'v', label='Training f1')\nplt.plot(val_acc, 'm', label='Validation acc')\nplt.plot(val_f1, 'y', label='Validation f1')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()","f1f82d4e":"out = word2vec_multi_modal_model.predict([X_img_test, X_text_test], batch_size=256)","40390f1a":"# DenseNet201****","d5591b88":"# OUTLIERS","b9ea5928":"We want to identify outliers, we decided to use the grubbs_test on the number of colors present in one image\n","ec50a8f8":"Judging from the grubbs_test, there are potentially outliers.\n\nTo identify them we are going to try using the IQR Method.\n","9dabd901":"The grubbs_test on the updated dataframe now suggests that there a renor mroe outliers","ac0bf796":"As all the visualization are showing, there are clearly some outliers \nSo let's try to identify them","31dba8af":"We identify then that every item with 4 or more colors perceived are considered outliers."}}