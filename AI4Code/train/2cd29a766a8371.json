{"cell_type":{"281a37c2":"code","3ac7b95a":"code","5f78fc1e":"code","0a816eb7":"code","84b675e5":"code","8ac14249":"code","0c9b0862":"code","c279521d":"code","3c6010bc":"code","b4c05a0a":"code","82c5a3eb":"code","17689628":"code","87656106":"code","b584f3c2":"code","a88586f2":"code","366d6548":"code","f91fda05":"code","6bd66307":"markdown","ef2e67a7":"markdown","5a373145":"markdown"},"source":{"281a37c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3ac7b95a":"df = pd.read_csv(\"..\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")\ndf2 = pd.read_csv(\"..\/input\/biomechanical-features-of-orthopedic-patients\/column_3C_weka.csv\")\nprint(plt.style.available)\nplt.style.use(\"ggplot\")","5f78fc1e":"df.info()\n# its showing data's info as you can see data's  not null value","0a816eb7":"df.head()","84b675e5":"df.describe()\n","8ac14249":"\n# df[\"class\"].value_counts()\nsns.countplot(x = \"class\", data = df)\nplt.show()\ndf[\"class\"].value_counts()\n","0c9b0862":"df.corr()","c279521d":"# split data\nfrom sklearn.model_selection import train_test_split\n\nx,y = df.loc[:,df.columns != \"class\"],df.loc[:,\"class\"]\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 42)\n\n","3c6010bc":"# find the best \"k\" value\nfrom sklearn.neighbors import KNeighborsClassifier\ntrain_accuracy = []\ntest_accuracy = []\nfor k in range(1,30):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train,y_train)\n    train_accuracy.append(knn.score(x_train,y_train))\n    test_accuracy.append(knn.score(x_test,y_test))\n\nprint(\"For K: {},  best accuracy : {}:\".format(1+test_accuracy.index(max(test_accuracy)),max(test_accuracy)))\nprint(\"\")\nknn_score = max(test_accuracy)\n\n# confusion matrix and classification report\nfrom sklearn.metrics import classification_report, confusion_matrix\npredict = knn.predict(x_test)\ncm = confusion_matrix(y_test,predict)\nprint(\"Confusion matrix:\\n\\n\",confusion_matrix(y_test,predict))\nprint(\"\\nTP: {}\\tFP: {}\\tFN: {}\\tTN: {}\\t\".format(cm[0,0],cm[0,1],cm[1,0],cm[1,1]))\nprint(\"\")\nprint(\"Classification Report:\\n\\n\",classification_report(y_test,predict))\n\n","b4c05a0a":"sns.pairplot(data= df,hue=\"class\", kind = \"reg\")\nplt.show()","82c5a3eb":"from sklearn.tree import DecisionTreeClassifier\n\nd_tree = DecisionTreeClassifier()\nd_tree.fit(x_train,y_train)\nd_score = d_tree.score(x_test,y_test)\nprint(\"Decision Tree Score:\",d_score)","17689628":"from sklearn.ensemble import RandomForestClassifier\nr_forest = RandomForestClassifier(n_estimators=100,random_state=1)\nr_forest.fit(x_train,y_train)\nrf_score = r_forest.score(x_test,y_test)\nprint(\"Random Forest Score:\",rf_score)","87656106":"from sklearn.linear_model import LogisticRegression\nl_reg = LogisticRegression(random_state=42,max_iter=100)\nl_reg.fit(x_train,y_train)\nlr_score = l_reg.score(x_test,y_test)\nprint(\"Logistic Regression score:\",lr_score)","b584f3c2":"df[\"class\"] = [1 if i==\"Abnormal\" else 0 for i in df[\"class\"]]\nx,y = df.loc[:,df.columns != \"class\"],df.loc[:,\"class\"]\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state = 42)","a88586f2":"from sklearn.linear_model import Ridge, Lasso\n\n\nridg = Ridge(alpha = 0.1,normalize = True)\nridg.fit(x_train,y_train)\n#ridg_predict = ridg.predict(x_test)\nrg_score = ridg.score(x_test,y_test)\nprint(\"Ridge Score:\",rg_score )","366d6548":"rg_score\nlr_score\nrf_score\nd_score\nknn_score\n\nscore_data_ = {\"Ridge\":[rg_score],\"Logistic Regression\":[lr_score],\n               \"Random Forrest Classifer\":[rf_score],\"Decision Tree\":[d_score],\n              \"KNN\":[knn_score]}\n\nscore_df = pd.DataFrame(score_data_)","f91fda05":"plt.figure(figsize = [10,6])\nsns.barplot(data = score_df)\nplt.xlabel(\"Algorithms\")\nplt.ylabel(\"Score\")\nplt.title(\"ML Algorithms Scores\")\nplt.show()","6bd66307":"Thank you for your vote and comment\n\nHappy Kaggleing :)","ef2e67a7":"...just showing some ML algorithms.... and comparasion their scores","5a373145":"## MACHINE LEARN\u0130NG \n### KNN\n### LOGISTIC REGRESSION\n### DECISION TREE, RANDOM FOREST CLASSIFIER\n### RIDGE"}}