{"cell_type":{"a0f244ab":"code","a7274ca1":"code","91efb2bb":"code","9872d932":"code","20dc50bc":"code","dde10599":"code","686597a7":"code","5b948553":"code","a7839ba6":"code","5d2a27b5":"code","ed3e02cb":"code","cf80d5ab":"code","750fcd4d":"code","33ba52e6":"code","02d1724b":"code","de8fdcab":"code","c5f9423b":"code","d7401848":"code","85b4487c":"code","9f94b3f2":"code","dc90d40f":"code","f8d6da9c":"code","82872a13":"code","a069d1bf":"code","cfd80eb8":"code","3047c080":"code","656f9065":"code","afb06d41":"code","b562cebf":"code","34525267":"code","0229bd0c":"code","e076ee08":"code","fa902e50":"code","56d4bdd8":"code","4913c883":"code","7499ebb1":"code","9029facb":"code","e2fe74a9":"code","051e9b13":"code","30f8f8c7":"code","9ae0afca":"code","1c4eefbe":"markdown","bda57d41":"markdown","f516f5a0":"markdown","be57a3a5":"markdown","fb70822f":"markdown","05af9c3a":"markdown","ceb8d132":"markdown","c482d8be":"markdown","224a5eaf":"markdown","f4498e98":"markdown"},"source":{"a0f244ab":"import numpy as np # linear algebra\nimport pandas as pd # data processing\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer #imputing missing numerical values\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OrdinalEncoder #encoding ordinal features (from categorical to numerical)\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.preprocessing import OneHotEncoder #encoding nominal features (from categorical to numerical)\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt","a7274ca1":"data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndata.shape, test.shape\n#there is one more column in the train set than in the test set: the target SalePrice is not in the test set","91efb2bb":"#separate target from predictors\ny = data.SalePrice\nX = data.drop(['SalePrice'], axis =1)\n#Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,train_size=0.8, test_size=0.2,random_state=0)#80% training set, 20% validation set\n#X_train, X_valid, are dataframes\n#y_train, y_valid are series","9872d932":"print(X_valid.shape)","20dc50bc":"X_train.dtypes","dde10599":"fig, ax=plt.subplots(figsize=(10,8))\nsns.histplot(y_train, color=\"blue\", label=\"SalePrice\", kde=True, stat=\"density\", linewidth=0)","686597a7":"# Skew and kurt\nprint(\"Skewness: %f\" % y_train.skew())\nprint(\"Kurtosis: %f\" % y_train.kurt())\n##Skewness >1 indicates substantially skewed distribution","5b948553":"# Correlation\n#corr = train.corr()\n#plt.subplots(figsize=(15,12))\n#sns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)","a7839ba6":"#dfCorr = train.corr()\n#filteredDf = dfCorr[((dfCorr >= .8) | (dfCorr <= -.8)) & (dfCorr !=1.000)]\n#plt.figure(figsize=(30,10))\n#sns.heatmap(filteredDf, annot=True, cmap=\"Reds\")\n#plt.show()","5d2a27b5":"# map features to their absolute correlation values\n#corr = features.corr().abs()\n\n# set equality (self correlation) as zero\n#corr[corr == 1] = 0\n\n# of each feature, find the max correlation\n# and sort the resulting array in ascending order\n#corr_cols = corr.max().sort_values(ascending=False)\n\n# display the highly correlated features\n#display(corr_cols[corr_cols > 0.8])","ed3e02cb":"#What are the numerical columns?\nnumerical_cols = [col for col in X_train.columns if X_train[col].dtype in ['int64','float64']]\nnumerical_cols","cf80d5ab":"len(numerical_cols)\n#37 numerical columns","750fcd4d":"#What are the categorical columns?\ncategorical_cols = [cols for cols in X_train.columns if X_train[cols].dtype in ['object']]\ncategorical_cols\nnp.shape(categorical_cols)#counting columns\n#43 categorical cols","33ba52e6":"# Which columns contains missing data\ncols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\nsum_cols_with_missing = X_train[cols_with_missing].isnull().sum()\nsum_cols_with_missing\n","02d1724b":"#column type with missing values: object = categorical (contains strings), float=num\nfor cols_with_missing in cols_with_missing:\n    print(cols_with_missing, X_train[cols_with_missing].dtype)\n#so need to handle missing values in both object and float","de8fdcab":"#IMP: AS THE DATA IS SKEWED:There are several or large numbers of data points that act as outliers.Outliers data points will have a significant impact on the mean and hence, in such cases, it is not recommended to use the mean for replacing the missing values. \n#I USED THE MODE INSTEAD OF THE MEAN (I could used mode for categorical and MEDIAN for numerical)\n\n#make copy to avoid changing original data when imputing\nX_train_missing = X_train.copy()\nX_valid_missing = X_valid.copy()\n\n#Imputation to fill the missing values - MODE ALLOWS IMPUTING CATEGORICAL AND NUMERICAL SIMULTANEOUSLY\nmy_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n\nmy_imputer.fit(X_train_missing)\n#transform and convert into dataframe\nimputed_X_train_missing = pd.DataFrame(my_imputer.transform(X_train_missing))\nimputed_X_valid_missing = pd.DataFrame(my_imputer.transform(X_valid_missing))\n\n#imputation removed column names, put them back\nimputed_X_train_missing.columns = X_train_missing.columns\nimputed_X_valid_missing.columns = X_valid_missing.columns\n\n#pandas trick when imputing object and numbers simultaneously\nimputed_X_train_missing = imputed_X_train_missing.convert_dtypes()\nimputed_X_valid_missing = imputed_X_valid_missing.convert_dtypes()","c5f9423b":"X_train_missing.dtypes","d7401848":"imputed_X_train_missing.dtypes","85b4487c":"print(imputed_X_train_missing.shape)\n#this df contains the same info that the original one same number of columns with no missing data","9f94b3f2":"#cheking no missing value\nprint(imputed_X_train_missing.isnull().values.sum())","dc90d40f":"print(imputed_X_valid_missing.isnull().values.sum())","f8d6da9c":"imputed_X_train_missing.shape","82872a13":"#imputed_x_train.columns# it contains 81 columns = 80 inicial columns - id dropped + two new 'index' columns","a069d1bf":"#imputed_x_train.shape","cfd80eb8":"#for column in train:\n    #if train[column].isnull().any():\n       # if(column in colnames_categorical_only):\n           # train[column]=train[column].fillna(train[column].mode()[0])#imputing most frequent value for missing in categorical column\n        #else:\n           #train[column]=train[column].fillna(train[column].mean)##imputing mean for missing in numerical column","3047c080":"ordinal_no_missing = imputed_X_train_missing[['ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC','KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual','GarageCond','PoolQC', 'Fence']]","656f9065":"print(ordinal_no_missing.shape)\n#16 categorical ordinal","afb06d41":"#Make a list from the dataframe with ordinal\n# this is a pandas series not a dataframe\no = (ordinal_no_missing.dtypes=='object')\n#s gives bollean (true or false which is required to fit one hot encoder)\no","b562cebf":"ordinal_list = list(o[o].index)\nordinal_list","34525267":"# Create categories to instruct how ordinal encoder should work:\n#ExterQual\ncat1 = [\"Ex\",\"Gd\",\"TA\",\"Fa\",\"Po\"]\n#'ExterCond'\ncat2 = [\"Ex\",\"Gd\",\"TA\",\"Fa\",\"Po\"]\n#'BsmtQual'\ncat3 = [\"Ex\",\"Gd\",\"TA\",\"Fa\",\"Po\",\"NA\"]\n#'BsmtCond'\ncat4 = [\"Ex\",\"Gd\",\"TA\",\"Fa\",\"Po\",\"NA\"]\n#'BsmtExposure'\ncat5 = [\"Gd\",\"Av\",\"Mn\",\"No\",\"NA\"]  \n#'BsmtFinType1'\ncat6 = [\"GLQ\",\"ALQ\",\"BLQ\",\"Rec\",\"LwQ\",\"Unf\",\"NA\"] \n#'BsmtFinType2'\ncat7 = [\"GLQ\",\"ALQ\",\"BLQ\",\"Rec\",\"LwQ\",\"Unf\",\"NA\"] \n #'HeatingQC'\ncat8 = [\"Ex\",\"Gd\",\"TA\",\"Fa\",\"Po\"]\n#'KitchenQual'\ncat9 =  [\"Ex\",\"Gd\",\"TA\",\"Fa\",\"Po\"]\n#'Functional'\ncat10 = [\"Typ\",\"Min1\",\"Min2\",\"Mod\",\"Maj1\",\"Maj2\",\"Sev\",\"Sal\"]\n#'FireplaceQu'\ncat11 = [\"Ex\",\"Gd\",\"TA\",\"Fa\",\"Po\",\"NA\"]\n#'GarageFinish'\ncat12 = [\"Fin\",\"RFn\",\"Unf\",\"NA\"]\n#'GarageQual'\ncat13 = [\"Ex\",\"Gd\",\"TA\",\"Fa\",\"Po\",\"NA\"]\n#'GarageCond'\ncat14 = [\"Ex\",\"Gd\",\"TA\",\"Fa\",\"Po\",\"NA\"]\n#'PoolQC'\ncat15 = [\"Ex\",\"Gd\",\"TA\",\"Fa\",\"NA\"]\n#'Fence'\ncat16 = [\"GdPrv\",\"MnPrv\",\"GdWo\",\"MnWw\",\"NA\"]\n    \n# Assign attributes to different lists based on the values\ncolumn_to_cat = {\n    \"ExterQual\": cat1,\n    \"ExterCond\": cat2,\n    \"BsmtQual\": cat3,\n    \"BsmtCond\": cat4,\n    \"BsmtExposure\": cat5, \n    \"BsmtFinType1\": cat6,\n    \"BsmtFinType2\": cat7, \n    \"HeatingQC\": cat8, \n    \"KitchenQual\": cat9, \n    \"Functional\": cat10,\n    \"FireplaceQu\": cat11,         \n    \"GarageFinish\": cat12,\n    \"GarageQual\": cat13, \n    \"GarageCond\": cat14, \n    \"PoolQC\":cat15, \n    \"Fence\": cat16\n}\n\nmapper_df = DataFrameMapper(\n    [\n        ([col], OrdinalEncoder(categories = [cat])) for col, cat in column_to_cat.items()\n    ],\n    df_out=True\n)\n#Based on the df already imputed which has no missing values\nOE_train = mapper_df.fit_transform(imputed_X_train_missing)\nOE_valid = mapper_df.transform(imputed_X_valid_missing)\n","0229bd0c":"#This is the new dataframe without missing data and ordinal transformed into numerical \nprint(OE_train.shape)","e076ee08":"#cheking no missing value\nprint(OE_train.isnull().values.sum())","fa902e50":"# drop the ORIGINAL ORDINALS columns \nimputed_x_train_no_ordinal = imputed_X_train_missing.drop(ordinal_no_missing, axis=1)\nimputed_x_valid_no_ordinal = imputed_X_valid_missing.drop(ordinal_no_missing, axis=1)\nimputed_x_train_no_ordinal.shape\n\n#should be 64 COLS (80-16 nominal) but a new column 'index' has been created","56d4bdd8":"imputed_x_train_no_ordinal.columns","4913c883":"#Concat with ordinal data encoded + data without ordinal \nOE_x_train = pd.concat([OE_train,imputed_x_train_no_ordinal], axis=1)\nOE_x_valid = pd.concat([OE_valid,imputed_x_valid_no_ordinal], axis=1)","7499ebb1":"print(OE_x_train.shape)","9029facb":"#37 numerical, 43 categorical (16 ordinal, 27 nominal)\nnominal_list = list(set(categorical_cols) - set(ordinal_list))\nlen(nominal_list) #there are 27 nominal features\n","e2fe74a9":"#creating dataframe WITHOUT MISSING DATA\nnominal_no_missing = imputed_X_train_missing[nominal_list]\nnominal_no_missing_valid = imputed_X_valid_missing[nominal_list]\nprint(nominal_no_missing.shape)","051e9b13":"OHE = OneHotEncoder(handle_unknown='ignore',sparse=False)\n\n#OH encoder produces a nmpy array if sparse = False, that is why I create new dataframe\nOHE.fit(nominal_no_missing)\nnominal_train_OHE= pd.DataFrame(OHE.transform(nominal_no_missing))#transforming original dataset\nnominal_valid_OHE = pd.DataFrame(OHE.transform(nominal_no_missing_valid))\n                                \nprint(nominal_train_OHE.shape)#it contains more columns than imputed_X_train_imp[nominal_list] because of the dummy variables\n\n#column names\n#nominal_train_no_missing_OHE.columns = OHE.get_feature_names(imputed_X_train_missing[nominal_list])\n#nominal_valid_no_missing_OHE.columns = OHE.get_feature_names(imputed_X_valid_missing[nominal_list])\n","30f8f8c7":"#Delete old nominal columns not encoded = only numerical\nOHE_X_train_witout_nominal = OE_x_train.drop(nominal_no_missing, axis=1)\nOHE_X_valid_witout_nominal = OE_x_valid.drop(nominal_no_missing_valid, axis=1)\nprint(OHE_X_train_witout_nominal.shape)","9ae0afca":"#Concat NOMINAL ALREADY ENCODED AND NUMERICAL DATA\n##reset index to avoid generating nan values\nOHE_x_train= pd.concat([OHE_X_train_witout_nominal.reset_index(),nominal_train_OHE.reset_index()], axis=1)#USED OE_X_train because contains the ordinal encoding\n#OE_X_valid_witout_nominal.reset_index(drop=True, inplace=True)\n#nominal_valid_OHE.reset_index(drop=True, inplace=True)\nOHE_x_valid= pd.concat([OHE_X_valid_witout_nominal.reset_index(), nominal_valid_OHE.reset_index()], axis=1)","1c4eefbe":"## Encoding categorical variables \n","bda57d41":"# FEATURE ENGENEERING","f516f5a0":"Some algorithms can work with categorical data directly.","be57a3a5":"# General exploration steps","fb70822f":"## Dealing with Missing values","05af9c3a":"### Ordinal encoding for dealing with ORDINAL - CATEGORICAL features","ceb8d132":"### One Hot encoding for dealing with NOMINAL-Categorical features","c482d8be":"### Imputation with MODE allowed me IMPUTING CATEGORICAL AND NUMERICAL SIMULTANEOUSLY","224a5eaf":"# EXPLORATORY DATA ANALYSIS (EDA)","f4498e98":"The ordinal features in the dataset are: Exter Qual, Exter Cond, Bsmt Qual, Bsmt Cond, Bsmt Exposure, BsmtFin Type 1, BsmtFin Type 2, Heating QC, Central Air, Kitchen Qual, Functional, Fireplace Qu, GarageFinish, Garage Qual, Garage Cond, Pool QC, Land Slope, Fence."}}