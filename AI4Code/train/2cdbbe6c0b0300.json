{"cell_type":{"71424280":"code","dd2d812b":"code","95ff4600":"code","a3ead191":"code","b480a56f":"code","de1e17bc":"code","f4cef90a":"code","6a82b09a":"code","38402cd3":"code","c646d103":"code","187d1b53":"code","93fe31e5":"code","496aa90d":"code","859d99f6":"code","dfdee38c":"code","85000d29":"code","a9973531":"code","cd265298":"code","e6228981":"code","44a3296b":"code","c5be4221":"code","5abf7ce9":"code","a74a17b8":"code","34951091":"code","389a0da8":"code","f0d082d4":"code","9b3e0e12":"code","3e0288ed":"code","3f5af10f":"code","f6b5006f":"code","05c0c7e2":"markdown","b8f26326":"markdown","a3d2a492":"markdown"},"source":{"71424280":"#Importing the relevant packages\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","dd2d812b":"\n#You can start by checking only the first rows with **nrows** argument, like so:\n# df = pd.read_csv('udemy_courses.csv', nrows=5)\n\ndf = pd.read_csv('udemy_courses.csv')","95ff4600":"#Looking at the data from a bird's point of view \n\ndf.describe()","a3ead191":"# If we are looking at course prices - courses that are free distort our data\ndf[df['price']>0].describe()\n","b480a56f":"#Check the total distribution of paid to free courses \ndf['is_paid'].value_counts()","de1e17bc":"# We can also visualize it, but I want to give you a word of warning - \n# Too many visualization might distract you - when analyzing - \n# try to visualize stuff that you can't see through the numbers\ndf['is_paid'].value_counts().plot(kind='bar')","f4cef90a":"# Prive distribution, we can see that the assumption of Normality doesn't apply\ndf['price'].plot(kind='hist')","6a82b09a":"# Because dates can be represented in many ways - if we want to \n# work on a date column where the time has any meaning - This is a good start:\n\ndf['published_timestamp'] = pd.DatetimeIndex(df['published_timestamp'])\n","38402cd3":"# We can see the Maximum and Minimum dates in our dataset, we checked that becuase we talked about COVID-19\n# Which leads us to an important point - Whatever we are experiencing NOW is not necessarily be fully relevant to the dataset \nprint(df['published_timestamp'].dt.strftime('%Y-%m').max())\nprint(df['published_timestamp'].dt.strftime('%Y-%m').min())","c646d103":"#using the inplace argument is not best practice in my eyes - try to either avoid it or use it with care \n# and know that it changes the dataframe itself\n\n# df.drop('url', axis=1, inplace=True)\n","187d1b53":"#Sampling 5 rows\ndf.sample(5)","93fe31e5":"#Let's look for duplicates \n# Note - we could use .drop_duplicates() but let's say that we want to KNOW which courses repeat themselves\ndouble_courses = pd.DataFrame(df['course_id'].value_counts()>=2)\ndouble_courses_index = double_courses[double_courses['course_id']==True].index\n\n#We are also sorting by title, just so we get a better grasp\ndf[df['course_id'].isin(double_courses_index)].sort_values('course_title')","496aa90d":"print(f\"Before dropping duplicates, this is the length - {len(df)}\")\ndf.drop_duplicates(inplace=True)\nprint(f\"After dropping duplicates, this is the length - {len(df)}\")","859d99f6":"#Just making sure...\ndf['course_id'].value_counts()","dfdee38c":"#Looking for unique subjects\ndf['subject'].unique()","85000d29":"#Looking for the number of unique subjects\n\ndf['subject'].nunique()","a9973531":"# Let's try to see when courses were published\nsubject_monthly = df[['published_timestamp','subject']].copy()","cd265298":"subject_monthly['month'] = subject_monthly['published_timestamp'].dt.strftime('%Y-%m')","e6228981":"#Let's see how many course were published per date\nsubject_monthly[['subject','month']].groupby('month').size()","44a3296b":"# We can also do this\nsubject_monthly[['subject','month']].groupby('month').count()","c5be4221":"# How can we get to this format?\n    sub_1,sub_1..,subN\nmonth1\nmotnh2\nmotnh3","5abf7ce9":"#Thanks to Ziv Gostinskey we found another way\nsubject_monthly_unstack =  subject_monthly[['subject','month']].groupby(['subject', 'month']).size().unstack().T","a74a17b8":"subject_monthly_unstack","34951091":"subject_monthly_unstack.plot(figsize=(12,8))","389a0da8":"#Another way:\nsubject_monthly_pivot_table = subject_monthly[['subject','month']].pivot_table(index='month', columns='subject', aggfunc=len)","f0d082d4":"subject_monthly_pivot_table.plot(figsize=(12,8))","9b3e0e12":"# I had a typo here - don't yell at me :)\n## Occam's razor - the simplest explanation is usually the right one\n\n\n##Read more https:\/\/en.wikipedia.org\/wiki\/Occam%27s_razor","3e0288ed":"#Let's try on a daily basis\n\nsubject_monthly['day'] = subject_monthly['published_timestamp'].dt.strftime('%Y-%m-%d')\nsubject_monthly[['subject','day']].groupby(['subject', 'day']).size().unstack().T.plot(figsize=(12,8))\n\n#Looks like way too much for us","3f5af10f":"#Let's work it on yearly basis\nsubject_monthly['year'] = subject_monthly['published_timestamp'].dt.strftime('%Y')\nsubject_monthly[['subject','year']].groupby(['subject', 'year']).size().unstack().T.plot(figsize=(12,8))","f6b5006f":"#Feel free to reach out at - talnmizrachi@gmail.com","05c0c7e2":"# Live Session's Notebook","b8f26326":"This is a part of a live session joint analysis conducted by myslef (Tal Mizrachi), with participents from various sources, and of course, with the full cooperation of Appleseed Academy Data Analysis course - Check'em out!\n\nYou can find the video here: https:\/\/youtu.be\/MKfF25Teyfo","a3d2a492":"## Reading the file into memory"}}