{"cell_type":{"f31d9180":"code","eed8377e":"code","f5a3b4fe":"code","a17ac847":"code","23d96686":"code","926f6190":"code","1583a1df":"code","c31283cf":"code","55f02d12":"code","9d809a2c":"code","9133dfcd":"code","9acf5c4c":"code","4c43b318":"code","177238b4":"code","df943a2a":"code","a88adfe0":"code","58a85eed":"code","81032b3c":"markdown","d930f0d7":"markdown","bafaec8a":"markdown","32394db7":"markdown","41b31fa3":"markdown","002a3973":"markdown","f93f09f7":"markdown","eacfc9ca":"markdown"},"source":{"f31d9180":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","eed8377e":"!pip install tweet-preprocessor","f5a3b4fe":"import preprocessor as p","a17ac847":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')","23d96686":"train_df.count()","926f6190":"train_df = train_df.dropna()\ntrain_df = train_df.drop_duplicates()","1583a1df":"train_df.count()","c31283cf":"train_df.head()","55f02d12":"def preprocess_tweet(row):\n    text = row['text']\n    text = p.clean(text)\n    return text","9d809a2c":"train_df['text'] = train_df.apply(preprocess_tweet, axis=1)","9133dfcd":"train_df.head()","9acf5c4c":"from gensim.parsing.preprocessing import remove_stopwords","4c43b318":"def stopword_removal(row):\n    text = row['text']\n    text = remove_stopwords(text)\n    return text","177238b4":"train_df['text'] = train_df.apply(stopword_removal, axis=1)","df943a2a":"train_df.head()","a88adfe0":"train_df['text'] = train_df['text'].str.lower().str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ')","58a85eed":"train_df.head()","81032b3c":"Tweet has been cleaned to normal text.","d930f0d7":"Dropping duplicates and NaN from the dataframe","bafaec8a":"## ** Tweet Preprocessing **\n\nSince we are dealing with tweets in this competition, we need to do specific tweet text cleaning along with normal text pre-processing. A tweet may contains\n\n* URL's\n* Mentions\n* Hashtags\n* Emojis\n* Smileys\n* Spefic words etc..\n\nTo clean the tweet , we can use a python library [tweet-preprocessor](https:\/\/pypi.org\/project\/tweet-preprocessor\/) instead of writing the cleaning logic ourself.","32394db7":"Now we can apply normal text preprocessing like\n\n* Lowercasing\n* Punctuation Removal\n* Replace extra white spaces\n* Stopwords removal\n\nFor stop word removal , i have used [gensim](https:\/\/pypi.org\/project\/gensim\/) library","41b31fa3":"Remove extra white spaces, punctuation and apply lower casing","002a3973":"Install tweet-preprocessor using pip","f93f09f7":"Apply tweet preprocessing first. Define a preprocess function and use pandas apply to apply it on each value of 'text'","eacfc9ca":"Now input tweet has been pre-processed and its ready to go for a ML training."}}