{"cell_type":{"6c0bafe5":"code","bedcb1ca":"code","279cf419":"code","8172d163":"code","ae6ef956":"code","02d2545a":"code","a1f2853c":"code","e44565f3":"code","8efa50d2":"code","14088c18":"code","348d0f49":"code","b3ce4466":"code","9a868c94":"code","603ed0a2":"code","f6fa4d06":"code","959fa128":"code","d7c6b499":"code","345494a9":"code","2555b03b":"code","42c3b254":"code","908249b1":"code","31c560ac":"code","86ce84ff":"code","e1661125":"code","aad65c33":"code","f1dfe0c6":"code","b24f2574":"code","29743ac1":"code","66156c4f":"code","61eee93a":"code","5025949a":"code","abf526c0":"code","1aea4411":"code","f280adf9":"code","bb0a8d37":"code","087111e8":"code","e93752e0":"code","ea588f0d":"code","253e94df":"code","fe8ab4c0":"code","79eb93da":"code","6e7e88d4":"code","78abf525":"code","21a2b18b":"code","7af25f70":"code","c9df5a11":"code","544d6560":"code","b7f197ea":"code","bed8f13c":"code","2b075e06":"code","21f35855":"code","891f8fd5":"code","7e524891":"code","da4fc7e1":"code","335a7c87":"code","3e1d7541":"code","73f30c9a":"code","9f081d77":"code","4f8eb8fd":"code","2a6da34c":"code","c452efb3":"code","33c5f5b3":"code","93abda27":"code","66176e02":"code","ce6994d9":"code","fca4e64c":"code","b6ef39d5":"code","7f7658f7":"code","3eef1c51":"code","c9ce7167":"code","44f58e26":"code","e8a40d1a":"code","8e6835e4":"code","e79c5fa6":"code","eab4080c":"code","1c1af041":"code","0e228599":"code","69f13bd9":"code","dc73c7c3":"code","9e15f51d":"code","a18b6194":"code","f4385536":"code","d083a959":"code","78f77288":"code","24d3626d":"code","78d49a45":"code","d6d215f0":"code","8624c4db":"code","1aee8fe5":"code","98429317":"code","1c1d17ff":"code","e9b63515":"code","077c871d":"code","6ea9bdc4":"code","c2c66f6c":"code","7c75bffc":"code","a6f89a53":"code","d8d5d838":"code","fb6c76c0":"code","7d73a3ff":"code","0418e9da":"code","043b6c6a":"code","95002b48":"code","469c3f7c":"code","551787bb":"code","eb1ea48e":"code","cf1bf4b2":"markdown","803acebf":"markdown","c46801cd":"markdown","2f38dcfd":"markdown","dd52ea44":"markdown","2aa6a2eb":"markdown","ff2019c0":"markdown","bc44bbe5":"markdown","27cb33ba":"markdown","62782921":"markdown","459ded98":"markdown","d34577e9":"markdown","e07a4e4f":"markdown"},"source":{"6c0bafe5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport string\nimport os\n\nfrom wordcloud import WordCloud\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom bs4 import BeautifulSoup\n!pip install contractions\nimport contractions\nfrom textblob import TextBlob\nimport spacy","bedcb1ca":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","279cf419":"train.head()","8172d163":"test.head()","ae6ef956":"test.info()","02d2545a":"train.info()","a1f2853c":"train.isna().sum()","e44565f3":"test.isna().sum()","8efa50d2":"plt.style.use('ggplot')\n\nplt.rcParams['figure.figsize'] = [8,4]\nplt.rcParams['figure.dpi'] = 120","14088c18":"sns.countplot('target', data = train)\nplt.title('Real or not Disaster tweets')","348d0f49":"# Functions for clean data\n\nfrom spacy.lang.en.stop_words import STOP_WORDS as stopwords\nglobal stopwords\n\ndef get_words_counts(tweets):\n        length = len(str(tweets).split())\n        return length\n    \ndef get_chars_counts(tweets):\n        string = tweets.split()\n        x = ''.join(string)\n        return len(x)\n    \ndef get_average_word_length(tweets):\n        count = get_chars_counts(tweets)\/get_words_counts(tweets)\n        return count\n    \ndef get_stopwords_count(tweets):\n        global stopwords\n        stopwords = len([t for t in tweets.split() if t in stopwords])\n        return stopwords\n    \ndef get_hashtags_tags(tweets):\n        hashtags = len([t for t in tweets.split() if t.startswith('#')])\n        return hashtags\n        \ndef get_email_tags(tweets):\n        email = len([t for t in tweets.split() if t.startwith('@')])\n        return email\n    \ndef get_digit_counts(tweets):\n        digits = re.findall(r'[0-9,.]+', tweets)\n        return digits\n    \ndef get_uppercase_units(tweets):\n        uppercase = len([t for t in tweets.split() if t.isupper()])\n        return uppercase","b3ce4466":"# With this function we can get some features for build a bit EDA\n\ndef get_features(df):\n    if type(df) == pd.core.frame.DataFrame:\n        df['words_counts'] = df['text'].apply(lambda x: get_words_counts(x))\n        df['char_counts'] = df['text'].apply(lambda x: get_chars_counts(x))\n        df['average_word_length'] = df['text'].apply(lambda x: get_average_word_length(x))\n#        df['stopwords_counts'] = df['text'].apply(lambda x: get_stopwords_count(x))\n        df['hashtags_counts'] = df['text'].apply(lambda x: get_hashtags_tags(x))\n#        df['email_counts'] = df['text'].apply(lambda x: get_email_tags(x))\n        df['digits_counts'] = df['text'].apply(lambda x: get_digit_counts(x))\n        df['uppercase_counts'] = df['text'].apply(lambda x: get_uppercase_units(x))\n        \n    else:\n        print('ERROR')\n        \n    return df","9a868c94":"train = pd.DataFrame(train)","603ed0a2":"train = get_features(train)","f6fa4d06":"train","959fa128":"sns.distplot(train['char_counts'])","d7c6b499":"sns.kdeplot(train[train['target'] == 1]['char_counts'], shade=True, color='red')\nsns.kdeplot(train[train['target'] == 0]['char_counts'], shade=True, color= 'blue')","345494a9":"sns.catplot(y='char_counts', data=train, kind='violin', col='target')","2555b03b":"def remove_tweet_username(df):\n    return re.sub('@[^\\s]+','', df)\n\ndef make_lower(df):\n    return df.lower()\n\ndef cont_exp(df):\n    return contractions.fix(df)\n\ndef make_string(df):\n    return str(df)\n\ndef remove_url(df):\n    return re.sub(r'(http|https|ftp|ssh):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', '' , df)\n\ndef remove_email(df):\n    return re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", df)\n\n#Retweets\ndef remove_rt(df):\n    df = str(df)\n    return re.sub(r'\\brt\\b', \"\", df).strip()\n\ndef remove_html(df):\n    return BeautifulSoup(df, 'lxml').get_text().strip()\n\n\ndef remove_dots(df):\n    dot_pattern = re.compile(r'\\.{1,}')\n    single_dot = dot_pattern.sub(' ', df)\n    return single_dot\n\ndef remove_special_chars(df):\n    df = re.sub(r'[^\\w]+', \" \", df)\n    df = ' '.join(df.split())\n    \n    return df\n\ndef make_base(df):\n    df = str(df)\n    x_list = []\n    doc = nlp(df)\n\n    for token in doc:\n        lemma = token.lemma_\n        if lemma == '-PRON-' or lemma == 'be':\n            lemma = token.text\n\n        x_list.append(lemma)\n    return ' '.join(x_list)\n\ndef spelling_correction(df):\n    df = TextBlob(df).correct()\n    return df\n\ndef resub(df):\n    return re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", df)\n\n\n\ndef get_clean_data(df):\n    df = remove_url(df)\n    df = remove_email(df)\n#    df = remove_special_chars(df)\n    df = remove_html(df)\n#    df = remove_dots(df)\n#    df = make_base(df)\n#    df = spelling_correction(df).raw_sentences[0]\n    df = make_lower(df)\n    df = make_string(df)\n    df = cont_exp(df)\n    df = remove_rt(df)\n    df = resub(df)                             \n    df = remove_tweet_username(df)\n    \n    return df","42c3b254":"train['text'] = train['text'].apply(get_clean_data)","908249b1":"test['text'] = test['text'].apply(get_clean_data)","31c560ac":"train['text'] = train['text'].apply(remove_special_chars)\ntest['text'] = test['text'].apply(remove_special_chars)","86ce84ff":"train['text'].head(20)","e1661125":"from gensim.parsing.preprocessing import STOPWORDS","aad65c33":"STOPWORDS.difference()\n\nall_stopwords_gensim = STOPWORDS\nsw_list = {\"not\"}\nall_stopwords_gensim = STOPWORDS.difference(sw_list)","f1dfe0c6":"# Use Gensim\n\nfrom gensim.parsing.preprocessing import remove_stopwords\n\ntrain['text'] = train['text'].apply(remove_stopwords)\ntest['text'] = test['text'].apply(remove_stopwords)\n","b24f2574":"train.text","29743ac1":"# trying to remove all digits from a columns\n\ntrain['text'] = train['text'].str.replace('\\d+', '')\ntest['text'] = test['text'].str.replace('\\d+', '')","66156c4f":"train['text'].head(20)","61eee93a":"def get_word_freqs(df, col):\n    text = ' '.join(df[col])\n    text = text.split()\n    freq = pd.Series(text).value_counts()\n    return freq","5025949a":"real_data = get_word_freqs(train[train['target']==1], 'text')\nreal_data = ' '.join(real_data.index)\nreal_data","abf526c0":"wordcloud = WordCloud().generate(real_data)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.imshow","1aea4411":"not_real_data = get_word_freqs(train[train['target']==0], 'text')\nnot_real_data = ' '.join(not_real_data.index)\nnot_real_data","f280adf9":"wordcloud = WordCloud().generate(not_real_data)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.imshow","bb0a8d37":"text = train['text']\ny = train['target']","087111e8":"tfidf = TfidfVectorizer()\nX = tfidf.fit_transform(text)","e93752e0":"X.shape","ea588f0d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 7, stratify=y)","253e94df":"from sklearn.svm import LinearSVC","fe8ab4c0":"clf_svm = LinearSVC()","79eb93da":"def run_SVM(clf_svm, X_train, X_test, y_train, y_test):\n    clf_svm.fit(X_train, y_train)\n    y_pred = clf_svm.predict(X_test)\n    \n    print()\n    print('Classification Report:')\n    print(classification_report(y_test, y_pred))","6e7e88d4":"run_SVM(clf_svm, X_train, X_test, y_train, y_test)","78abf525":"from sklearn.linear_model import LogisticRegression\n\nclf_lr = LogisticRegression()","21a2b18b":"def run_LR(clf_lr, X_train, X_test, y_train, y_test):\n    clf_lr.fit(X_train, y_train)\n    y_pred = clf_lr.predict(X_test)\n    \n    print()\n    print('Classification Report:')\n    print(classification_report(y_test, y_pred))","7af25f70":"run_LR(clf_lr, X_train, X_test, y_train, y_test)","c9df5a11":"from sklearn.neighbors import KNeighborsClassifier\n\nclf_knn = KNeighborsClassifier()","544d6560":"def run_knn(clf_knn, X_train, X_test, y_train, y_test):\n    clf_knn.fit(X_train, y_train)\n    y_pred = clf_knn.predict(X_test)\n    \n    print()\n    print('Classification Report:')\n    print(classification_report(y_test, y_pred))","b7f197ea":"run_knn(clf_knn, X_train, X_test, y_train, y_test)","bed8f13c":"from sklearn.naive_bayes import MultinomialNB\n\nclf_mnb = MultinomialNB()","2b075e06":"def run_mnb(clf_mnb, X_train, X_test, y_train, y_test):\n    clf_mnb.fit(X_train, y_train)\n    y_pred = clf_mnb.predict(X_test)\n    \n    print()\n    print('Classification Report:')\n    print(classification_report(y_test, y_pred))","21f35855":"run_mnb(clf_mnb, X_train, X_test, y_train, y_test)","891f8fd5":"from sklearn.tree import DecisionTreeClassifier\n\nclf_tree = DecisionTreeClassifier()","7e524891":"def run_tree(clf_tree, X_train, X_test, y_train, y_test):\n    clf_tree.fit(X_train, y_train)\n    y_pred = clf_tree.predict(X_test)\n    \n    print()\n    print('Classification Reprot:')\n    print(classification_report(y_test, y_pred))","da4fc7e1":"run_tree(clf_tree, X_train, X_test, y_train, y_test)","335a7c87":"from sklearn.ensemble import RandomForestClassifier\n\nclf_forest = RandomForestClassifier()","3e1d7541":"def run_forest(clf_forest, X_train, X_test, y_train, y_test):\n    clf_forest.fit(X_train, y_train)\n    y_pred = clf_forest.predict(X_test) \n    \n    print()\n    print(\"Classificiation Report:\")\n    print(classification_report(y_test, y_pred))","73f30c9a":"run_forest(clf_forest, X_train, X_test, y_train, y_test)","9f081d77":"random_forest_score = clf_forest.score(X_test, y_test)","4f8eb8fd":"random_forest_score","2a6da34c":"models_default_tfidf = {'Support Vector Machines': clf_svm.score(X_test, y_test),\n          'Logistic Regression': clf_lr.score(X_test, y_test),\n          'KNearest Neightbors': clf_knn.score(X_test, y_test),\n          'Multinominal Naive Bayes': clf_mnb.score(X_test, y_test),\n          'Decision Tree': clf_tree.score(X_test, y_test),\n          'Random Forest Classifier': clf_forest.score(X_test, y_test)}","c452efb3":"models_default_tfidf","33c5f5b3":"default_models_compare = pd.DataFrame(models_default_tfidf, index=['accuracy'])\ndefault_models_compare.T.plot.bar()","93abda27":"import en_core_web_lg","66176e02":"nlp = en_core_web_lg.load()","ce6994d9":"def get_vec(x):\n    doc = nlp(x)\n    vec = doc.vector\n    return vec","fca4e64c":"train['vectors'] = train['text'].apply(lambda x: get_vec(x))","b6ef39d5":"train.head()","7f7658f7":"X = train['vectors'].to_numpy()\nX = X.reshape(-1, 1)","3eef1c51":"X.shape","c9ce7167":"X = np.concatenate(np.concatenate(X, axis = 0), axis = 0).reshape(-1, 300)","44f58e26":"X.shape","e8a40d1a":"X","8e6835e4":"### Normalization\n\nfrom sklearn.preprocessing import normalize\n\nX = normalize(X)\nX","e79c5fa6":"y = train['target']","eab4080c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 7, stratify=y)","1c1af041":"def word2vec_models(clf, X_train, X_test, y_train, y_test):\n    print('SVM MODEL:')\n    run_SVM(clf_svm, X_train, X_test, y_train, y_test)\n    print('LOGISTIC REGRESSION MODEL:')\n    run_LR(clf_lr, X_train, X_test, y_train, y_test)\n    print('KNEAREST NEIGHBORS MODEL:')\n    run_knn(clf_knn, X_train, X_test, y_train, y_test)\n    #run_mnb(clf_mnb, X_train, X_test, y_train, y_test)\n    print('DECISION TREE MODEL:')\n    run_tree(clf_tree, X_train, X_test, y_train, y_test)\n    print('RANDOM FOREST MODEL:')\n    run_forest(clf_forest, X_train, X_test, y_train, y_test)\n    \n    return word2vec_models\n    ","0e228599":"clf_list = [run_SVM, run_LR, run_knn, run_mnb, run_tree, run_forest]","69f13bd9":"word2vec_models(clf_list, X_train, X_test, y_train, y_test)","dc73c7c3":"models_default_word2vec = {'Support Vector Machines': clf_svm.score(X_test, y_test),\n          'Logistic Regression': clf_lr.score(X_test, y_test),\n          'KNearest Neightbors': clf_knn.score(X_test, y_test),\n          'Decision Tree': clf_tree.score(X_test, y_test),\n          'Random Forest Classifier': clf_forest.score(X_test, y_test)}","9e15f51d":"models_default_word2vec","a18b6194":"models_default_tfidf","f4385536":"# Logistic Regression?","d083a959":"print(clf_lr.get_params().keys())","78f77288":"%%time\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid_lr = {'C': np.logspace(-2,2,8),\n                 'random_state': [7],\n                 'penalty': ['l2', 'l1'],\n                 'solver': ['liblinear','sag', 'saga']}\n\ngrid_lr = GridSearchCV(clf_lr, param_grid_lr, cv=10, n_jobs = -1)\n\ngrid_lr.fit(X_train, y_train)\n","24d3626d":"grid_lr.best_params_","78d49a45":"grid_lr.best_score_","d6d215f0":"#####################################################################","8624c4db":"#####################################################################","1aee8fe5":"test['vectors'] = test['text'].apply(lambda x: get_vec(x))","98429317":"X_test","1c1d17ff":"testX = test['vectors'].to_numpy()","e9b63515":"testX = testX.reshape(-1, 1)","077c871d":"testX.shape","6ea9bdc4":"testX = np.concatenate(np.concatenate(testX, axis = 0), axis=0).reshape(-1, 300)","c2c66f6c":"# from sklearn.preprocessing import normalize\n\nX_norm = normalize(testX)\n","7c75bffc":"X_norm","a6f89a53":"X_norm.shape","d8d5d838":"df = pd.DataFrame(test['id'])","fb6c76c0":"df","7d73a3ff":"final = grid_lr.predict(X_norm)","0418e9da":"df['target'] = final","043b6c6a":"df","95002b48":"df.to_csv('Disaster_tweet.csv', index=False)","469c3f7c":"check = pd.read_csv('Disaster_tweet.csv')","551787bb":"check.head()","eb1ea48e":"check.target.value_counts()","cf1bf4b2":"# WordClouds","803acebf":"# TF_IDF","c46801cd":"# Data Cleaning","2f38dcfd":"## K_Nearest_Neighbors","dd52ea44":"## Support Vector Machines","2aa6a2eb":"## Random Forest Classifier","ff2019c0":"## Logistic Regression","bc44bbe5":"## Word2Vec","27cb33ba":"## Decision Tree","62782921":"## Multinominal Naive Bayes","459ded98":"# Hyperparameter tuning","d34577e9":"# EDA","e07a4e4f":"# Let's import train and test set"}}