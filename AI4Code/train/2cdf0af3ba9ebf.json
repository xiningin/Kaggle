{"cell_type":{"35a2d6af":"code","8d770c3b":"code","0af92567":"code","261d98d5":"code","7b54c785":"code","e2c6272d":"code","2fdb8ad3":"code","eb9cac05":"code","5b1371c2":"code","b345e283":"code","5f72b58d":"code","ea8fc2d8":"code","a9f4920d":"code","fa674e78":"code","031bebe3":"code","70951130":"code","db8a6fb5":"code","9b12d317":"code","b13c1c57":"code","bc5ebe45":"code","4cd047c1":"code","45405630":"code","44bc4496":"code","5ade1428":"code","ebf307d2":"code","0ca81f6b":"code","da1df65a":"code","3c475216":"code","fafdc3d3":"code","af32c21f":"code","7eec72ac":"code","2b200f66":"code","5c00e942":"markdown","77c3563c":"markdown","70640157":"markdown","c1ac3425":"markdown","a934cc04":"markdown","a946d1f6":"markdown","54cfb976":"markdown","a39a8aae":"markdown","bcf9a13e":"markdown","dc33b95b":"markdown","45cdedf8":"markdown","7b8ad710":"markdown","bdf344c2":"markdown","6c61b5db":"markdown","e1a80b08":"markdown","920c4103":"markdown","f2b626d4":"markdown","469813bd":"markdown","cfa9a231":"markdown","73f32230":"markdown","64cece71":"markdown","d1ffa5a4":"markdown"},"source":{"35a2d6af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8d770c3b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","0af92567":"df = pd.read_csv('\/kaggle\/input\/fetal-health-classification\/fetal_health.csv')\ndf.head(5)","261d98d5":"import missingno as msno\nn = msno.bar(df,color=\"gray\")\nprint(n)","7b54c785":"df.describe()","e2c6272d":"df.info()","2fdb8ad3":"sns.countplot(x=\"fetal_health\",data = df)\nplt.show()","eb9cac05":"plt.figure(figsize=(20,10))\nsns.boxplot(data = df,palette = \"Set1\")\nplt.xticks(rotation=90)\nplt.show()","5b1371c2":"# Function to set upper and lower bound to 3rd standard deviation and remove outliers\n\ndef removeOutlier(att, df):\n\n    lowerbound = att.mean() - 3 * att.std()\n    upperbound = att.mean() + 3 * att.std()\n\n    print('lowerbound: ',lowerbound,' -------- upperbound: ', upperbound )\n\n    df1 = df[(att > lowerbound) & (att < upperbound)]\n\n    print((df.shape[0] - df1.shape[0]), ' number of outliers from ', df.shape[0] )\n    print(' ******************************************************')\n    \n    df = df1.copy()\n\n    return df","b345e283":"df = removeOutlier(df.histogram_variance, df)\ndf = removeOutlier(df.histogram_median, df)\ndf = removeOutlier(df.histogram_mean, df)\ndf = removeOutlier(df.histogram_mode, df)\ndf = removeOutlier(df.percentage_of_time_with_abnormal_long_term_variability, df)\ndf = removeOutlier(df.mean_value_of_short_term_variability, df)","5f72b58d":"df.shape","ea8fc2d8":"corrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,10))\ng = sns.heatmap(df[top_corr_features].corr(),annot = True,cmap = \"RdYlGn\")","a9f4920d":"df.fetal_health.value_counts()","fa674e78":"from sklearn.utils import resample\n\n# Separate Target Classes\ndf_1 = df[df.fetal_health==1]\ndf_2 = df[df.fetal_health==2]\ndf_3 = df[df.fetal_health==3]\n \n# Upsample minority class\ndf_2_upsampled = resample(df_2, \n                                 replace=True,     # sample with replacement\n                                 n_samples=1601,    # to match majority class\n                                 random_state=123) # reproducible results\n\ndf_3_upsampled = resample(df_3, \n                                 replace=True,     # sample with replacement\n                                 n_samples=1601,    # to match majority class\n                                 random_state=123) # reproducible results\n\n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_1, df_2_upsampled, df_3_upsampled])\n \n# Display new class counts\ndf_upsampled.fetal_health.value_counts()","031bebe3":"x = df_upsampled.drop('fetal_health', axis = 1)\ny = df_upsampled['fetal_health'] ","70951130":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.25, random_state = 0)","db8a6fb5":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","9b12d317":"# Models I am going to use are: \n# XGBoost\n# AdaBoost\n# CataBoost\n# RandomForest\n# LBGM Classifier\n# Voting Classifier","b13c1c57":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score\nfrom sklearn.preprocessing import LabelBinarizer\nfrom mlxtend.plotting import plot_confusion_matrix\n\ndef evaluator(y_test, y_pred):    \n    \n    # Accuracy:\n    print('Accuracy is: ', accuracy_score(y_test,y_pred))\n    print('')\n    # Classification Report:\n    print('Classification Report: \\n',classification_report(y_test,y_pred))\n\n    # Area Under The Curve Score:\n\n    lb = LabelBinarizer()\n    y_test1 = lb.fit_transform(y_test)\n    y_pred1 =lb.transform(y_pred)\n    print('AUC_ROC Score: ',roc_auc_score(y_test1,y_pred1,average='macro'),'\\n\\n')\n\n    print('Confusion Matrix: \\n\\n')\n    plt.style.use(\"ggplot\")\n    cm = confusion_matrix(y_test,y_pred)\n    plot_confusion_matrix(conf_mat = cm,figsize=(8,6),show_normed=True)","bc5ebe45":"from xgboost import XGBClassifier\n\nxgb_classifier = XGBClassifier()\nxgb_classifier.fit(x_train,y_train)","4cd047c1":"pred_xgb = xgb_classifier.predict(x_test)\n\nevaluator(y_test, pred_xgb)","45405630":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nada_classifier = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1),\n    n_estimators=200\n)\n\nada_classifier.fit(x_train,y_train)","44bc4496":"pred_ada = ada_classifier.predict(x_test)\n\nevaluator(y_test, pred_ada)","5ade1428":"from catboost import CatBoostClassifier\n\ncat_classifier = CatBoostClassifier(iterations=1000, verbose = 0)\n\ncat_classifier.fit(x_train, y_train)","ebf307d2":"pred_cat = cat_classifier.predict(x_test)\n\nevaluator(y_test, pred_cat)","0ca81f6b":"from lightgbm import LGBMClassifier\n\nlgb_classifier = LGBMClassifier()\nlgb_classifier.fit(x_train,y_train)","da1df65a":"pred_lgb = lgb_classifier.predict(x_test)\n\nevaluator(y_test,pred_lgb)","3c475216":"from sklearn.ensemble import RandomForestClassifier\n\nrf_classifier = RandomForestClassifier()\n\nrf_classifier.fit(x_train,y_train)","fafdc3d3":"pred_rf = rf_classifier.predict(x_test)\n\nevaluator(y_test, pred_rf)","af32c21f":"important_features = pd.DataFrame({'Features': x.columns, \n                                   'Importance': rf_classifier.feature_importances_})\n\n# sort the dataframe in the descending order according to the feature importance\nimportant_features = important_features.sort_values('Importance', ascending = False)\n\n# create a barplot to visualize the features based on their importance\nsns.barplot(x = 'Importance', y = 'Features', data = important_features)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Feature Importance', fontsize = 15)\nplt.xlabel('Importance', fontsize = 15)\nplt.ylabel('Features', fontsize = 15)\n\n# display the plot\nplt.show()","7eec72ac":"from sklearn.ensemble import VotingClassifier\n\nvc = VotingClassifier(estimators = [(\"xgb_classifier\",xgb_classifier),('ada_classifier', ada_classifier),('cat _classifier', cat_classifier),(\"lgb_classifier\",lgb_classifier),(\"rf_classifier\",rf_classifier)],voting='soft')\nvc.fit(x_train,y_train)","2b200f66":"pred_vc = vc.predict(x_test)\n\nevaluator(y_test, pred_vc)","5c00e942":"Building Model:","77c3563c":"Removing the outliers: by setting upper and lower threshold","70640157":"# Random Forest Classifier:","c1ac3425":"# XGBOOST","a934cc04":"Description of the Data","a946d1f6":"Separating Fetures and Target Variable","54cfb976":"Correlation HeatMap","a39a8aae":"## Result and Conclusion:\nAll models perform good except for adaboost (after balancing).\n\nAccuracy has significantly increased by 4 to 5 percent after balancing out the data. To balance the data resampling was done by up_sampling i.e, duplicating the minority class to meet the value_count of majority class.\n\nRandom Forest Classifier is performing the best based on the evaluation matrices used.","bcf9a13e":"---\n## Fetal Health Classification Problem\n---\n### Aurthor: Avinash Bagul\n##### MSc Artificial Intelligence (University of Aberdeen)","dc33b95b":"Distribution of Target class: Highly imbalanced","45cdedf8":"Checking for number of missing values in each column.....","7b8ad710":"# AdaBoost:","bdf344c2":"Feature Scaling: Standardization","6c61b5db":"# CatBoost:","e1a80b08":"### Evaluator Function: \nAccuracy, Precision, Recall, f1-Score, roc_auc_score and Confusion Matrix","920c4103":"# LBGM Classifier:","f2b626d4":"---\n### **Thank You**\n---\n\nAuthor: Avinash Vinayak Bagul\n(MSc Artificial Intelligence)","469813bd":"Important Features","cfa9a231":"Looking for outliers in the data","73f32230":"# Voting Classifier:","64cece71":"Balancing Dataset:","d1ffa5a4":"Removing outliers from columns showing outiers in the boxplot visualized above"}}