{"cell_type":{"dc747873":"code","ad5f91c4":"code","b88e92fd":"code","977a25a3":"code","0d93df07":"code","bd7ae8f5":"code","42ade9eb":"code","e9427dfe":"code","56f4f69e":"code","d45574e3":"code","f53c47eb":"code","2e996bc0":"code","6616f0f2":"code","577b2048":"code","23f3c4aa":"code","1a441528":"code","d2906138":"code","b6ca4a8c":"code","12d87a30":"code","e29e3ee6":"code","e62f0e74":"code","07e05226":"code","a1480a65":"code","b7760b03":"code","f28d7c6e":"code","e36738c5":"code","b5a4fbe1":"code","0bc8873f":"code","13fdb2ce":"code","cf9223b0":"code","8eccda2b":"code","02963791":"code","5073845c":"code","f6367df6":"code","40119f65":"code","56eeb313":"code","34acc59f":"code","93b3a99d":"code","7a1bf4e0":"code","453d257c":"code","8baf6c1a":"code","eb55f251":"code","7a3430ba":"code","f467d33b":"markdown","f11e875c":"markdown","9c513edf":"markdown","c7938f1a":"markdown","f5103c17":"markdown","cc890f1c":"markdown","6bceb440":"markdown","ae44eb47":"markdown","f2eda493":"markdown","4ce95c0f":"markdown","03c2ce43":"markdown","a885509e":"markdown","29e95641":"markdown","83f8c5bb":"markdown"},"source":{"dc747873":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad5f91c4":"pip install pyforest","b88e92fd":"import pyforest\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","977a25a3":"df=pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","0d93df07":"df.head()","bd7ae8f5":"df.info()","42ade9eb":"import matplotlib.pyplot as plt\nfor col in df.columns[:-1]:\n    sns.histplot(df, x=col, hue=\"Outcome\", multiple='stack')\n    plt.show()","e9427dfe":"plt.figure(figsize=(10,10))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","56f4f69e":"train,test=train_test_split(df,random_state=0)\ntrain.shape,test.shape","d45574e3":"train.hist(figsize=(10,10))\nplt.show()","f53c47eb":"train.plot(kind='box',figsize=(10,10),subplots=True,layout=(3,5))\nplt.show()","2e996bc0":"X=train.drop('Outcome',axis=1)\ny=train.Outcome\n\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    cv_results = cross_val_score(model, X, y, cv=kfold, scoring='accuracy') \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","6616f0f2":"df1=pd.DataFrame(data=[result.mean().round(3) for result in results],\n                 index=names,columns=['No Preprocessing'])\ndf1","577b2048":"X=train.drop('Outcome',axis=1)\ny=train.Outcome\n\nscaler=MinMaxScaler()\nfrom sklearn.pipeline import Pipeline\n\n\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n\nresults2 = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    pipe=Pipeline([('scaler',scaler),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy') \n    results2.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","23f3c4aa":"df2=pd.DataFrame(data=[result.mean().round(3) for result in results2],\n                 index=names,columns=['MinMaxScaler'])\ndf2","1a441528":"df3=df1.join(df2)\ndf3","d2906138":"X=train.drop('Outcome',axis=1)\ny=train.Outcome\n\nscaler=StandardScaler()\nfrom sklearn.pipeline import Pipeline\n\n\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n\nresults3 = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    pipe=Pipeline([('scaler',scaler),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy') \n    results3.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","b6ca4a8c":"df4=pd.DataFrame(data=[result.mean().round(3) for result in results3],\n                 index=names,columns=['StandardScaler'])\ndf4","12d87a30":"df5=df3.join(df4)\ndf5","e29e3ee6":"X=train.drop('Outcome',axis=1)\ny=train.Outcome\n\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import Pipeline\nscaler=Normalizer()\n\n\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n\nresults4 = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    pipe=Pipeline([('scaler',scaler),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy') \n    results4.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","e62f0e74":"df6=pd.DataFrame(data=[result.mean().round(3) for result in results4],\n                 index=names,columns=['Normalizer'])\ndf6","07e05226":"df7=df5.join(df6)\ndf7","a1480a65":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nskb=SelectKBest(score_func=f_classif,k='all')\nfit=skb.fit(X,y)\npd.DataFrame(data=fit.scores_,\n             index=X.columns,columns=['Scores']).sort_values(by='Scores').plot(kind='barh')","b7760b03":"X=train.drop('Outcome',axis=1)\ny=train.Outcome\n\n\nfrom sklearn.pipeline import Pipeline\nskb=SelectKBest(score_func=f_classif,k=3)\n\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n\nresults5 = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    pipe=Pipeline([('skb',skb),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy') \n    results5.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","f28d7c6e":"df8=pd.DataFrame(data=[result.mean().round(3) for result in results5],\n                 index=names,columns=['Best 3'])\ndf8","e36738c5":"df9=df7.join(df8)\ndf9","b5a4fbe1":"X=train.drop('Outcome',axis=1)\ny=train.Outcome\n\n\nfrom sklearn.pipeline import Pipeline\nskb=SelectKBest(score_func=f_classif,k=5)\n\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n\nresults6 = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    pipe=Pipeline([('skb',skb),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy') \n    results6.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","0bc8873f":"df10=pd.DataFrame(data=[result.mean().round(3) for result in results6],\n                 index=names,columns=['Best 5'])\ndf10","13fdb2ce":"df11=df9.join(df10)\ndf11","cf9223b0":"\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver='liblinear')\nrfe = RFE(model)\nfit = rfe.fit(X, y)\nprint(\"Num Features: %d\" % fit.n_features_)\nprint(\"Selected Features: %s\" % fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)","8eccda2b":"X.loc[:,fit.support_]","02963791":"X=train.drop('Outcome',axis=1)\ny=train.Outcome\n\n\nrfe_model = LogisticRegression(solver='liblinear')\nrfe = RFE(rfe_model)\n\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n\nresults7 = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    pipe=Pipeline([('rfe',rfe),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy') \n    results7.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","5073845c":"df12=pd.DataFrame(data=[result.mean().round(3) for result in results7],\n                 index=names,columns=['RFE'])\ndf12","f6367df6":"df13=df11.join(df12)\ndf13","40119f65":"from sklearn.decomposition import PCA\npca = PCA(n_components=3)\nfit = pca.fit(X)\nprint(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\nprint(fit.components_)","56eeb313":"X=train.drop('Outcome',axis=1)\ny=train.Outcome\n\n\npca=PCA(n_components=3)\n\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n\nresults8 = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    pipe=Pipeline([('pca',pca),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy') \n    results8.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","34acc59f":"df14=pd.DataFrame(data=[result.mean().round(3) for result in results8],\n                 index=names,columns=['PCA3'])\ndf14","93b3a99d":"df15=df13.join(df14)\ndf15","7a1bf4e0":"X=train.drop('Outcome',axis=1)\ny=train.Outcome\n\n\npca=PCA(n_components=5)\n\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n\nresults9 = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    pipe=Pipeline([('pca',pca),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy') \n    results9.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","453d257c":"df16=pd.DataFrame(data=[result.mean().round(3) for result in results9],\n                 index=names,columns=['PCA5'])\ndf16","8baf6c1a":"df17=df15.join(df16)\ndf17.style.background_gradient(axis=1,cmap='BuPu')","eb55f251":"\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier(n_estimators=100)\nmodel.fit(X, y)\nprint(model.feature_importances_)","7a3430ba":"pd.DataFrame(data=model.feature_importances_,\n             index=X.columns,\n             columns=['Feature Importances']).sort_values(by='Feature Importances').plot(kind='barh')","f467d33b":"# MinMaxScaler","f11e875c":"# StandardScaler","9c513edf":"# Feature importance with ExtraTreesClassifier","c7938f1a":"# Best 3 Features","f5103c17":"# Histplot","cc890f1c":"# PCA","6bceb440":"# Best 5 Features","ae44eb47":"# No Preprocessing","f2eda493":"# Heatmap","4ce95c0f":"# Recursive Feature Elimination","03c2ce43":"## 3 Components","a885509e":"## 5 Components ","29e95641":"# Normalizer","83f8c5bb":"# SelectKBest"}}