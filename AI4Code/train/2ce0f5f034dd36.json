{"cell_type":{"f3424b40":"code","49837983":"code","f1b468f6":"code","7063fa73":"code","1d728334":"code","4350b2d8":"code","52843e2d":"code","52325025":"code","d80a5ee2":"code","3b89a1e6":"code","7e8ff4be":"code","1f6a27e6":"code","e42572fd":"code","15deb968":"code","979dc348":"code","51cecd35":"code","bc9262d6":"code","0b242718":"code","f4c5368d":"code","c240fd8c":"code","75bd4c0b":"markdown","557b91c2":"markdown","7bc99801":"markdown","651ff7c7":"markdown","d3cf1c0b":"markdown"},"source":{"f3424b40":"import pandas as pd","49837983":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmiss = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","f1b468f6":"train.head()","7063fa73":"train['target'].unique()","1d728334":"from sklearn import preprocessing, feature_extraction, linear_model, model_selection\nimport spacy\nimport numpy as np\nimport re\nfrom spacy.tokenizer import Tokenizer","4350b2d8":"prefix_re = re.compile('''^\\$[a-zA-Z0-9]''')\n\ndef custom_tokenizer(nlp):\n    \"\"\"Word extraction tokenizer.\"\"\"\n    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search)","52843e2d":"def process_text(array):\n    nlp = spacy.load(\"en_core_web_sm\")\n    nlp.tokenizer = custom_tokenizer(nlp)\n    docs = []\n    for sentence in array:\n        tokens = nlp.tokenizer(sentence)\n        docs.append(list(map(lambda x: x.lemma_, tokens)))\n    return docs","52325025":"token_text = process_text(train['text'])","d80a5ee2":"token_text","3b89a1e6":"class MeanEmbeddingVectorizer(object):\n    \"\"\"Vector to mean.\"\"\"\n\n    def __init__(self, word_model):\n        self.word_model = word_model\n        self.vector_size = word_model.vector_size\n\n    def fit(self): \n        return self\n\n    def transform(self, docs):\n        doc_word_vector = self.word_average_list(docs)\n        return doc_word_vector\n\n    def word_average(self, sent):\n        mean = []\n        for word in sent:\n            if word in self.word_model.vocab:\n                mean.append(self.word_model.get_vector(word))\n\n        if not mean:\n            return np.zeros(self.vector_size)\n        else:\n            mean = np.array(mean).mean(axis=0)\n            return mean\n\n\n    def word_average_list(self, docs):\n        return np.vstack([self.word_average(sent) for sent in docs])","7e8ff4be":"path = '..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin'","1f6a27e6":"import gensim\nw2v_model = gensim.models.KeyedVectors.load_word2vec_format(path,binary=True)","e42572fd":"mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2v_model)","15deb968":"mean_embedded = mean_embedding_vectorizer.transform(token_text)","979dc348":"from sklearn.model_selection import train_test_split","51cecd35":"train_x, test_x, train_y, test_y = train_test_split(mean_embedded, train['target'], test_size=0.25, random_state=7)","bc9262d6":"from sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier","0b242718":"model = linear_model.LogisticRegression()","f4c5368d":"model.fit(train_x, train_y)","c240fd8c":"metrics.accuracy_score(model.predict(test_x), test_y)","75bd4c0b":"Let's break all texts into tokens.","557b91c2":"Each word in the word2vec model corresponds to a vector. Reduce such a vector to a scalar value:","7bc99801":"# Preprocessing","651ff7c7":"Using a trained word2vec model for text vectorization:","d3cf1c0b":"# Linear Model"}}