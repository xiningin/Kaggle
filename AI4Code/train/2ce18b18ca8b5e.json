{"cell_type":{"1cb0d5b0":"code","c4b5bf63":"code","c0432980":"code","ff027779":"code","7330383d":"code","997db190":"code","6f64702c":"code","8b99f43c":"code","3820a56b":"code","4f961ffd":"code","08d95fe2":"code","5ac16034":"code","2b491fee":"code","607e215f":"code","6520e06a":"code","e0b14a52":"markdown"},"source":{"1cb0d5b0":"import numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom pprint import pprint as pp\nimport seaborn as sns\nimport json\nimport networkx as nx\nfrom itertools import product\n\nin_kaggle = True ","c4b5bf63":"data_dir = '.\/data\/' if not in_kaggle else '\/kaggle\/input\/abstraction-and-reasoning-challenge\/'\ntraining_path, evaluation_path, test_path = f'{data_dir}training', f'{data_dir}evaluation', f'{data_dir}test'\ntraining_tasks, evaluation_tasks, test_tasks = sorted(os.listdir(training_path)), sorted(os.listdir(evaluation_path)), sorted(os.listdir(test_path))\ncolors = { cname: c for c, cname in enumerate(['black', 'blue', 'red', 'green', 'yellow', 'gray', 'magenta', 'orange', 'sky', 'brown']) }","c0432980":"createGridDict = lambda m: { (i,j): { } for i in range(m.shape[0]) for j in range(m.shape[1]) }\ncellColor = lambda x, i, j: x[i, j]\n\ndef loadJsonFromPath(path):\n  j = None\n  with open(path, 'r') as f:\n    j = json.load(f)\n  return j\n\ndef getGeneralInfoFromTask(task_path):\n  j = loadJsonFromPath(task_path)\n  j_train, j_test = j['train'], j['test']\n  n_patterns, n_test = len(j_train), len(j_test)\n  return {\n    'task_json': j, \n    'train': j_train, \n    'test': j_test, \n    'n_patterns': n_patterns,\n    'n_test': n_test\n  }\n","ff027779":"import matplotlib.pyplot as plt\nfrom matplotlib import colors as cls\nfrom pprint import pprint as pp\nimport numpy as np\n\ncmap = cls.ListedColormap(['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00','#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\nnorm = cls.Normalize(vmin=0, vmax=9)\n    \ndef plotResults(task_samples, predictions):\n  for sample, prediction in zip(task_samples, predictions):\n    t_in, t_out, prediction = np.array(sample[\"input\"]), np.array(sample[\"output\"]), np.array(prediction)\n    titles = [f'Input {t_in.shape}', f'Output {t_out.shape}', f'Predicted {prediction.shape}']\n    figures = [t_in, t_out, prediction]\n    fig, axs = plt.subplots(1, 3, figsize=(2*3, 32))\n    for i, (figure, title) in enumerate(zip(figures, titles)):\n      plotOne(axs[i], figure, title)\n    plt.show()\n\ndef showTotalColors():\n  # 0:black, 1:blue, 2:red, 3:greed, 4:yellow,\n  # 5:gray, 6:magenta, 7:orange, 8:sky, 9:brown\n  plt.figure(figsize=(5, 2), dpi=100)\n  plt.imshow([list(range(10))], cmap=cmap, norm=norm)\n  plt.xticks(list(range(10)))\n  plt.yticks([])\n  plt.show()\n\ndef plotOne(ax, task_in_out, title):\n  ax.imshow(task_in_out, cmap=cmap, norm=norm)\n  ax.set_title(title)\n  #ax.axis('off')\n  ax.set_yticks([x-0.5 for x in range(1+task_in_out.shape[0])])\n  ax.set_xticks([x-0.5 for x in range(1+task_in_out.shape[1])])  \n  ax.set_xticklabels([])\n  ax.set_yticklabels([])\n  ax.grid(True, which='both', color='lightgrey', linewidth=0.5) \n\ndef plot_task(task_info, n, name, test_task=False):\n  # 0:black, 1:blue, 2:red, 3:greed, 4:yellow,\n  # 5:gray, 6:magenta, 7:orange, 8:sky, 9:brown\n  n_pairs = task_info['n_patterns'] + task_info['n_test']\n  train_info, test_info = task_info['train'], task_info['test']\n  \n  plt.subplots_adjust(wspace=0, hspace=0)\n  fig, axs = plt.subplots(2, n_pairs, figsize=(4*n_pairs,8),  dpi=50)\n  fig_num = 0\n  \n  for i, t in enumerate(task_info['train']):\n    t_in, t_out = np.array(t[\"input\"]), np.array(t[\"output\"])\n    if (t_in > 9).any() or (t_out > 9).any(): print(f\"Number Out of color range ({np.max(t_in)}, {np.max(t_out)}\")\n    plotOne(axs[0, fig_num], t_in, f'{n}: Train Input {i} - {t_in.shape} - {name}')\n    plotOne(axs[1, fig_num], t_out, f'{n}: Train Output {i} - {t_out.shape} - {name}')\n    fig_num += 1\n  for i, t in enumerate(task_info['test']):\n    t_in, t_out = np.array(t[\"input\"]), np.array(t[\"output\"]) if not test_task else None\n    if (t_in > 9).any() or (t_out > 9).any(): print(f\"Number Out of color range ({np.max(t_in)}, {np.max(t_out)}\")\n    plotOne(axs[0, fig_num], t_in, f'{n}: Test Input {i} - {t_in.shape} - {name}')\n    if not test_task: \n      plotOne(axs[1, fig_num], t_out, f'Test Output {i} - {t_out.shape} - {name}')\n    else:\n      axs[1, fig_num].axis('off')\n    fig_num += 1\n\n  plt.tight_layout()\n  plt.show()","7330383d":"color_info = {\n  **{ cname: lambda x,i,j,color: x[i,j] == color for cname, cvalue in colors.items() } # inputs(x, *cell, color)\n}\n\ncaracterize = {\n  **color_info,  # inputs(x, *cell, color)\n}\n\ndef caracterizeIO(m, caracterize, characteristics):\n  \"\"\"Convert a matrix in a dict where keys are position in matrices and values are characteristics\"\"\"\n  m_d = createGridDict(m)\n  for name, f in caracterize.items():\n    for cell, _ in m_d.items():\n      is_same_color = lambda fn,x,i,j: (np.unique(fn(x,i,j))).shape[0] == 1\n      if name in characteristics:\n        if name in colors:\n          m_d[ cell ][ name ] = f(m, *cell, colors[name])\n        else:\n          m_d[ cell ][ name ] = is_same_color(f, m, *cell)\n  return m_d\n\ndef convertToDimentionalMatrix(m, m_d, caracterize):\n  \"\"\"From caracterized matrix (dict of { (cell): characteristics, ... }), get a matrix for each caracteristic\"\"\"\n  n_caracteristics = len(caracterize)\n  dim_m = np.empty((n_caracteristics, *m.shape))\n  for cell, char_cell in m_d.items():\n    color_count, others_count = 0, 0\n    for j, cname in enumerate(caracterize):\n      x,y = cell\n      # from 0 to ncolors, let it be first places, afterwards the others\n      if cname in colors:\n        dim_m[color_count, x, y] = int(char_cell[cname]) if type(char_cell[cname]) == bool else char_cell[cname]\n        color_count += 1\n      else:\n        dim_m[others_count+len(colors), x, y] = int(char_cell[cname]) if type(char_cell[cname]) == bool else char_cell[cname]\n        others_count += 1\n  return dim_m\n\ndef transformCaracterize(m, chars):\n  x = np.array(m)\n  x_d = caracterizeIO(x, caracterize, chars)\n  return convertToDimentionalMatrix(x, x_d, chars)","997db190":"import torch\nimport torch.nn as nn\nimport torch.tensor as tensor\nimport torch.nn.functional as F\nimport torch.optim as optim\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","6f64702c":"class SimpleInputOutputNet(nn.Module):  \n  def __init__(self, init_dict):\n    \"\"\"input_dict.keys() == ['in_ch', 'in_out_ch', 'in_ker', 'out_ch']\"\"\"\n    super(SimpleInputOutputNet, self).__init__()\n    in_ch, in_out_ch, in_ker = init_dict['in_ch'], init_dict['in_out_ch'], init_dict['in_ker']\n    out_ch = init_dict['out_ch']\n    self.conv1 = nn.Conv2d(in_ch, in_out_ch, in_ker, padding=1)\n    self.conv2 = nn.Conv2d(in_out_ch, out_ch, kernel_size=1)\n  def forward(self, x):\n    x = F.relu(self.conv1(x))\n    x = self.conv2(x)\n    return x\n  def is_automata(self): return False","8b99f43c":"def isSameInputOutputShape(task):\n    return all([np.array(el['input']).shape == np.array(el['output']).shape for el in task['train']])\n  \ndef getClassyOutput(task): \n  \"\"\"get simply the output as a class\"\"\"\n  return task['output']\n\ndef getCharacterizedBinaryOutput(task, output_characteristics):\n  \"\"\"get output different dimensions with matrices of 0 and 1\"\"\"\n  return transformCaracterize(task['output'], output_characteristics)\n\ndef getOutput(task, output_characteristics, is_class=True):\n  return getClassyOutput(task) if is_class else getCharacterizedBinaryOutput(task, output_characteristics)\n\ndef getInput(task, input_characteristics):\n  return transformCaracterize(task['input'], input_characteristics)\n\ndef solveTask(epochs, task_samples, net, criterion, input_char, output_char, optim_args={'lr':0.1}, steps=1, is_class=True, train_outToOut=False, show_prints=False):\n  all_losses, step = [], 0\n  for step in range(0, steps):\n    optimizer = optim.Adam(net.parameters(), lr=(optim_args['lr']\/(step+1)))\n    #optimizer = optim.Adam(net.parameters(), **optim_args)\n    is_automata = net.is_automata()\n    losses = []\n    for epoch in range(epochs):\n      optimizer.zero_grad()\n      running_loss = 0.0\n      for t, task_sample in enumerate(task_samples):\n        # -- Get Input and Output --\n        inp, out = getInput(task_sample, input_char), getOutput(task_sample, output_char, is_class=is_class)\n        inp = tensor(inp).unsqueeze(0).float().to(device) # adds_a dimension to the shape, and convert to float\n        out = tensor(out).unsqueeze(0).long().to(device) # adds_a dimension to the shape, and convert to long == int  \n        #inp = tensor(inp2img(sample[\"input\"])).unsqueeze(0).float().to(device) \n        #out = tensor(sample['output']).long().unsqueeze(0).to(device)\n        if show_prints: print(f'(input, output) shape ({inp.shape}, {out.shape})')\n\n        predicted = net(inp) if not is_automata else net(inp, step+1)\n        running_loss += criterion(predicted, out)\n        if show_prints: print(f'(predicted) shape ({predicted.shape})')\n\n        if train_outToOut:\n          out_inp = getOutput(task_sample, input_char, False)\n          out_inp = tensor(out_inp).unsqueeze(0).float().to(device) \n          #out_inp = tensor(inp2img(sample[\"output\"])).unsqueeze(0).float().to(device) \n          if show_prints: print(f'(output as input) shape {out_inp.shape}')\n          predicted = net(out_inp) if not is_automata else net(out_inp, 1)\n          running_loss += criterion(predicted, out)\n      running_loss.backward()\n      optimizer.step()\n      losses.append(running_loss.item())\n    all_losses += losses\n  return net, all_losses, step ","3820a56b":"@torch.no_grad()\ndef predictOutputTenDimColors(net, task, input_char, steps=1 ):\n  predictions = []\n  for sample in task:\n    inp = getInput(sample, input_char)\n    inp = tensor(inp).unsqueeze(0).float().to(device)\n    is_automata = net.is_automata()\n    pred = net(inp) if not is_automata else net(inp, (steps+1))\n    pred = pred.argmax(1) # (shape: [1, 10, _,_]) argmax-Returns the indices of the maximum value of all elements in the input tensor. dim: dimension to reduce\n    pred = pred.squeeze().cpu().numpy() #squeeze returns tesnor with dimensions of 1 removed, so the first dimension in this case\n    predictions.append(pred)\n  return predictions\n\ndef getTaskSuccess(task_samples, predictions):\n  is_same_array = lambda a,b,cell: np.array_equal(cellColor(a, *cell), cellColor(b, *cell))\n  success = []\n  for sample, prediction in zip(task_samples, predictions):\n    output, prediction = np.array(sample['output']), np.array(prediction)\n    out_d, pred_d = createGridDict(output), createGridDict(prediction)\n    same = np.array([ is_same_array(output, prediction, cell) for cell, _ in out_d.items()])\n    success += [ np.all(same) ]\n  good = sum([int(v) for v in success])\n  return success, good, len(success) - good     \n  \ndef fillReport(report, task_path, train, test):\n  train_success, train_good, train_bad = train\n  test_success, test_good, test_bad = test\n  if train_bad == 0: report['trained_success'] += 1\n  if train_bad == 0 and test_bad > 0: report['task_train_success'] += [task_path]\n  if test_bad == 0: report['success'] += 1\n  if train_bad == 0 and test_bad == 0: report['task_success'] += [task_path]\n  if train_good > 0 and train_bad > 0: report['half_sucess'] +=1\n  if train_good == 0: report['fails'] += 1","4f961ffd":"# --- RUN For Simple Convolutional Layer\ndef runSimpleNet(task_paths, init_params, in_out_char, model, criterion=nn.CrossEntropyLoss(), optim_args={'lr':0.1}, is_class=True, train_outToOut=False, steps=1, epochs=100,\n                 show_prints=False, show_plots=False, show_results=True, grabbing_net=False, is_test=False):\n  report = { 'trained_success': 0, 'half_sucess': 0, 'fails': 0, 'success': 0, 'task_train_success': [], 'task_success': [] }\n  nets_info = { }\n  showed = False\n  folder_path = training_path if not is_test else test_path\n  if grabbing_net: net = model(init_params).to(device)\n  for task_path in task_paths:\n    task_info = getGeneralInfoFromTask(f'{folder_path}\/{task_path}')\n    if not isSameInputOutputShape(task_info): continue\n    input_char, output_char = in_out_char['input_char'], in_out_char['output_char']\n    samples = task_info['train']\n    if not grabbing_net: net = model(init_params).to(device)\n    if not showed and show_results: \n      print(net.eval())\n      showed = True\n    net, losses, step = solveTask(\n      epochs=epochs, task_samples=samples,\n      net=net, \n      criterion=criterion, #nn.CrossEntropyLoss() OR nn.MSELoss()\n      optim_args=optim_args,\n      input_char=input_char, output_char=output_char,\n      steps=steps,\n      is_class=is_class,\n      train_outToOut=train_outToOut,\n      show_prints=show_prints\n    )\n    if show_plots: plt.plot(losses)\n    predictions = predictOutputTenDimColors(net, samples, input_char, step)\n    success, good, bad = getTaskSuccess(samples, predictions)\n    if (not show_prints and show_results) or show_prints: print(f'--- {good} out of {len(success)} were ok ---') if bad != 0 else print('---- Successful training!!! :) ----')\n    if show_plots: plotResults(samples, predictions)\n    test_success, test_good, test_bad = [], 0, 1\n    if bad == 0 and not is_test:\n      test_predictions = predictOutputTenDimColors(net, task_info['test'], input_char, step)\n      test_success, test_good, test_bad = getTaskSuccess(task_info['test'], test_predictions)\n      if show_results: print(f'--- {test_good} out of {len(test_success)} were ok ---') if test_bad != 0 else print('---- Successful ALL!!! :) ----')\n    if is_test:\n      test_predictions = predictOutputTenDimColors(net, task_info['test'], input_char, step)\n      nets_info[task_path] = { 'predictions': test_predictions, 'input': [ sample['input'] for sample in task_info['test'] ] }\n    else:\n      fillReport(report, task_path, (success, good, bad), (test_success, test_good, test_bad))\n  return report, nets_info","08d95fe2":"input_char, output_char = color_info.keys(), color_info.keys()\nprint(f'Characteristics for (input - {len(input_char)}, output - {len(output_char)}): \\ninp -> {input_char}\\nout -> {output_char} ')\nreport, net = runSimpleNet(\n  show_plots=True,\n  show_prints=False,\n  show_results=True,\n  model= SimpleInputOutputNet,\n  optim_args={'lr':0.2},\n  task_paths= ['00d62c1b.json'], \n  in_out_char={ 'input_char': input_char, 'output_char': output_char },\n  init_params={ 'in_out_ch': 128, 'in_ker':3, 'in_ch': len(input_char), 'out_ch': len(output_char) } \n)\npp(report)","5ac16034":"output_char, input_char = color_info.keys(), color_info.keys()\nreport, net = runSimpleNet( show_plots=False, show_prints=False, show_results=False, model=SimpleInputOutputNet,\n    steps=1,\n    epochs=50,\n    train_outToOut=True,\n    optim_args={'lr': 0.05},\n    task_paths= training_tasks, # ['00d62c1b.json'], # \n    in_out_char={ 'input_char': input_char, 'output_char': output_char },\n    init_params={ 'in_out_ch': 128, 'in_ker':3, 'in_ch': len(input_char), 'out_ch': len(output_char) } \n  )\npp(report)","2b491fee":"import pandas as pd\nsubmission = pd.read_csv('\/kaggle\/input\/abstraction-and-reasoning-challenge\/sample_submission.csv', index_col='output_id')\ndisplay(submission.head())","607e215f":"output_char, input_char = color_info.keys(), color_info.keys()\nreport, nets_info = runSimpleNet( show_plots=False, show_prints=False, show_results=False, model=SimpleInputOutputNet,\n    steps=1,\n    epochs=50,\n    train_outToOut=True,\n    optim_args={'lr': 0.05},\n    task_paths= test_tasks, \n    in_out_char={ 'input_char': input_char, 'output_char': output_char },\n    init_params={ 'in_out_ch': 128, 'in_ker':3, 'in_ch': len(input_char), 'out_ch': len(output_char) } ,\n    is_test=True\n  )\npp(report)","6520e06a":"def getNGuessesFromExemple(submission_exo, output_id, n=1):\n  out = submission_exo.loc[output_id].output\n  vals = out.split(' ')\n  return vals[:n]\n  \ndef getSubmissionDF(nets_info, submission):\n  submission_dict = {}\n  for task_name in test_tasks:\n    if not task_name in submission_dict: submission_dict[task_name] = { 'indexes': [] }\n  \n  for task_name, info in submission_dict.items():\n    if not task_name in nets_info: continue\n    net_info = nets_info[task_name]\n    predictions, inputs = net_info['predictions'], net_info['input']\n    sub_like_pred, sub_like_inp = '|', '|'\n    submission_dict[task_name]['strings'], submission_dict[task_name]['inputs'] = [], []\n    for inp in inputs:\n      #sub_like_inp += ' |' if sub_like_inp != '' else '|'\n      for row in inp:\n        sub_like_inp += ''.join([ str(v) for v in row]) + '|'\n      sub_like_inp += ' '\n      submission_dict[task_name]['inputs'] += [sub_like_inp]\n    for prediction in predictions:\n      #sub_like_pred += ' |' if sub_like_pred != '' else '|'\n      for row in prediction:\n        sub_like_pred += ''.join([ str(v) for v in row]) + '|'\n      sub_like_pred += ' '\n      submission_dict[task_name]['strings'] += [sub_like_pred]\n      \n\n  to_submit = {}\n  for task_name, info in submission_dict.items():\n    if not task_name in nets_info: continue\n    task, _ = task_name.split('.')\n    net_info = nets_info[task_name]\n    predictions = net_info['predictions']\n    for index, prediction in enumerate(predictions):\n      output_id = f'{task}_{index}'\n      if not 'strings' in info: \n        to_submit[output_id] = getNGuessesFromExemple(submission, output_id)[0] + ' '\n      else :\n        to_submit[output_id] = info['strings'][index]\n        \n  sub_df = pd.DataFrame(to_submit.values(), index=to_submit.keys(), columns=['output'])\n  sub_df.index.name = 'output_id'\n  return sub_df\n\nsubmission = pd.read_csv('\/kaggle\/input\/abstraction-and-reasoning-challenge\/sample_submission.csv')\ndf = getSubmissionDF(nets_info, submission)\ndisplay(df.head())\ndisplay(df.info())\ndf.to_csv('submission.csv')","e0b14a52":"1. [Francois chollet paper: \"On the Measure of Intelligence\"](https:\/\/arxiv.org\/abs\/1911.01547)\n2. [Cellular automata on this competition](https:\/\/www.kaggle.com\/arsenynerinovsky\/cellular-automata-as-a-language-for-reasoning), and idea to solve thinks with somehow \"basic operations\"\n3. A possible solution to mix basic operations: [genetic algorithm](https:\/\/www.kaggle.com\/zenol42\/dsl-and-genetic-algorithm-applied-to-arc)\n4. [This video, an introduction to this challenge](https:\/\/www.youtube.com\/watch?v=K5KDZLHsr1o&fbclid=IwAR2erffIK1pJhMHTSQlCsczIpWXab87pVJftQNK1dKWTuYcmvDCLXtqol0k), you can watch from min 26 because before it's just the explantion of the paper\n5. Search maybe for a baseline or starter notebook in order to obtain basic input and ploting functions or a first idea to approach whith if you liked it (just type \"starter\" or \"basline\" in the search bar in the [competition site](https:\/\/www.kaggle.com\/c\/abstraction-and-reasoning-challenge\/notebooks)"}}