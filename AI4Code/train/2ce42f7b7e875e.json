{"cell_type":{"1e367eb4":"code","3860f6c1":"code","44541a84":"code","e46dc749":"code","e275c912":"code","0e17995a":"code","444fea3c":"code","1ce3ca68":"code","dd4bf5bd":"code","b082d240":"code","84b40e10":"code","6ddeb6e0":"markdown","2ab656bc":"markdown","a5f3d116":"markdown","e698d408":"markdown","6a73800f":"markdown","99f0f83b":"markdown","b828bbe9":"markdown"},"source":{"1e367eb4":"import pandas as pd\nimport numpy as np\nimport re","3860f6c1":"def cleaning(s):\n    s = str(s)\n    s = re.sub('\\s\\W',' ',s)\n    s = re.sub('\\W,\\s',' ',s)\n    s = re.sub(\"\\d+\", \"\", s)\n    s = re.sub('\\s+',' ',s)\n    s = re.sub('[!@#$_]', '', s)\n    s = s.replace(\"co\",\"\")\n    s = s.replace(\"https\",\"\")\n    s = s.replace(\"[\\w*\",\" \")\n    return s","44541a84":"df = pd.read_csv(\"Articles.csv\", encoding=\"ISO-8859-1\") \ndf = df.dropna()\ntext_data = open('Articles.txt', 'w')\nfor idx, item in df.iterrows():\n  article = cleaning(item[\"Article\"])\n  text_data.write(article)\ntext_data.close()","e46dc749":"!pip install transformers","e275c912":"from transformers import TextDataset, DataCollatorForLanguageModeling\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers import Trainer, TrainingArguments","0e17995a":"def load_dataset(file_path, tokenizer, block_size = 128):\n    dataset = TextDataset(\n        tokenizer = tokenizer,\n        file_path = file_path,\n        block_size = block_size,\n    )\n    return dataset\n\n\ndef load_data_collator(tokenizer, mlm = False):\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, \n        mlm=mlm,\n    )\n    return data_collator\n\n\ndef train(train_file_path,model_name,\n          output_dir,\n          overwrite_output_dir,\n          per_device_train_batch_size,\n          num_train_epochs,\n          save_steps):\n  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n  train_dataset = load_dataset(train_file_path, tokenizer)\n  data_collator = load_data_collator(tokenizer)\n\n  tokenizer.save_pretrained(output_dir)\n      \n  model = GPT2LMHeadModel.from_pretrained(model_name)\n\n  model.save_pretrained(output_dir)\n\n  training_args = TrainingArguments(\n          output_dir=output_dir,\n          overwrite_output_dir=overwrite_output_dir,\n          per_device_train_batch_size=per_device_train_batch_size,\n          num_train_epochs=num_train_epochs,\n      )\n\n  trainer = Trainer(\n          model=model,\n          args=training_args,\n          data_collator=data_collator,\n          train_dataset=train_dataset,\n  )\n      \n  trainer.train()\n  trainer.save_model()","444fea3c":"# you need to set parameters \ntrain_file_path = \"\/content\/drive\/MyDrive\/Articles.txt\"\nmodel_name = 'gpt2'\noutput_dir = '\/content\/drive\/MyDrive\/result'\noverwrite_output_dir = False\nper_device_train_batch_size = 8\nnum_train_epochs = 5.0\nsave_steps = 500","1ce3ca68":"# It takes about 30 minutes to train in colab.\ntrain(\n    train_file_path=train_file_path,\n    model_name=model_name,\n    output_dir=output_dir,\n    overwrite_output_dir=overwrite_output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    num_train_epochs=num_train_epochs,\n    save_steps=save_steps\n)","dd4bf5bd":"from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer","b082d240":"def load_model(model_path):\n    model = GPT2LMHeadModel.from_pretrained(model_path)\n    return model\n\n\ndef load_tokenizer(tokenizer_path):\n    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n    return tokenizer\n\n\ndef generate_text(sequence, max_length):\n    model_path = \"\/content\/drive\/MyDrive\/result\"\n    model = load_model(model_path)\n    tokenizer = load_tokenizer(model_path)\n    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n    final_outputs = model.generate(\n        ids,\n        do_sample=True,\n        max_length=max_length,\n        pad_token_id=model.config.eos_token_id,\n        top_k=50,\n        top_p=0.95,\n    )\n    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))","84b40e10":"sequence = input() # oil price\nmax_len = int(input()) # 20\ngenerate_text(sequence, max_len) # oil price for July June which had been low at as low as was originally stated Prices have since resumed","6ddeb6e0":"## Step 2. Model Training","2ab656bc":"# GPT-2 Fine-Tuning","a5f3d116":"#### This is the code I wrote at the company, but I think it would be nice to share it here, so I post it.\n\n#### With this data, we will fine tune GPT-2 to make a sentence generation model. \n\n#### This code is for AI beginners.","e698d408":"## Step 3. Inference","6a73800f":"The following process may be a little more complicated or tedious because you have to write the code one by one, and it takes a long time if you don't have a personal GPU.\n\nThen, how about use Ainize's Teachable NLP? Teachable NLP provides an API to use the model so when data is input it will automatically learn quickly.\n\nTeachable NLP : [https:\/\/ainize.ai\/teachable-nlp](https:\/\/link.ainize.ai\/3tJVRD1)\n\nTeachable NLP Tutorial : [https:\/\/forum.ainetwork.ai\/t\/teachable-nlp-how-to-use-teachable-nlp\/65](https:\/\/link.ainize.ai\/3tATaUh)","99f0f83b":"## Step 1. Data preprocessing","b828bbe9":"#### the data contains unnecessary newlines, tags, and URLs it will be necessary to remove them before preprocessing."}}