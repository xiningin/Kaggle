{"cell_type":{"6e7839e4":"code","4a64d954":"code","8fcbd383":"code","64171384":"code","f9aa5b59":"code","35e504ca":"code","e8733d20":"code","1219068a":"code","175158da":"code","53693fe0":"code","cb63239a":"code","4817c94a":"code","4d57b746":"code","c0671744":"code","30ce33ab":"code","da0389bd":"code","b270ec80":"code","d547fce5":"code","a2611d94":"code","fead997a":"code","696d1ff0":"code","e3158d86":"code","bc2e3a98":"code","59f7463e":"code","90edc065":"code","b4e3bd71":"code","ddd50350":"code","46a7e632":"code","3eaa2445":"code","abfb1504":"code","fb84112b":"code","23469c08":"code","32d64bab":"code","fb05c043":"code","9aaa72f1":"code","4b2490ea":"code","5fd50331":"code","4e7d0b49":"code","451da2ce":"code","6aa749ec":"code","44434c95":"code","4dc9878d":"code","9111c2a9":"code","d5b0d258":"code","8fb890f5":"code","7727865e":"markdown","a8cf5a70":"markdown","a0a78a0d":"markdown","ca050b87":"markdown","ebc54b5c":"markdown","54a909a2":"markdown","650d9d87":"markdown","51117690":"markdown","fed53ca2":"markdown","92cc7fd5":"markdown","83422f17":"markdown","79e6c7b2":"markdown","a258946f":"markdown","c5aa4e76":"markdown","41b4c112":"markdown","69922cde":"markdown","2fb18ee9":"markdown","ad99f763":"markdown","6bd81424":"markdown","cec82606":"markdown","5863e98a":"markdown","725611a8":"markdown","c4facf57":"markdown","e93c1c7c":"markdown","0d1cd57e":"markdown","97a5ae3a":"markdown","5898a90b":"markdown","863aecd0":"markdown","d0cca07f":"markdown"},"source":{"6e7839e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4a64d954":"train_data = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntrain_data.head(5)","8fcbd383":"test_data = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntest_data.head(5)","64171384":"train_data.info()","f9aa5b59":"test_data.info()","35e504ca":"sns.countplot(train_data['target'])","e8733d20":"!pip install BeautifulSoup4","1219068a":"from bs4 import BeautifulSoup # Text Cleaning\nimport re, string # Regular Expressions, String\nfrom nltk.corpus import stopwords # stopwords\nfrom nltk.stem.porter import PorterStemmer # for word stemming\nfrom nltk.stem import WordNetLemmatizer # for word lemmatization\nimport unicodedata\nimport html\n\n# set of stopwords to be removed from text\nstop = set(stopwords.words('english'))\n\n# update stopwords to have punctuation too\nstop.update(list(string.punctuation))\n\ndef clean_tweets(text):\n    \n    # Remove unwanted html characters\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n    'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n    '<br \/>', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n    ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    text = re1.sub(' ', html.unescape(x1))\n    \n    # remove non-ascii characters\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    \n    # strip html\n    soup = BeautifulSoup(text, 'html.parser')\n    text = soup.get_text()\n    \n    # remove between square brackets\n    text = re.sub('\\[[^]]*\\]', '', text)\n    \n    # remove URLs\n    text = re.sub(r'http\\S+', '', text)\n    \n    # remove twitter tags\n    text = text.replace(\"@\", \"\")\n    \n    # remove hashtags\n    text = text.replace(\"#\", \"\")\n    \n    # remove all non-alphabetic characters\n    text = re.sub(r'[^a-zA-Z ]', '', text)\n    \n    # remove stopwords from text\n    final_text = []\n    for word in text.split():\n        if word.strip().lower() not in stop:\n            final_text.append(word.strip().lower())\n    \n    text = \" \".join(final_text)\n    \n    # lemmatize words\n    lemmatizer = WordNetLemmatizer()    \n    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n    text = \" \".join([lemmatizer.lemmatize(word, pos = 'v') for word in text.split()])\n    \n    # replace all numbers with \"num\"\n    text = re.sub(\"\\d\", \"num\", text)\n    \n    return text.lower()\n\ntrain_data['prep_text'] = train_data['text'].apply(clean_tweets)\ntrain_data['prep_text'].head(5)","175158da":"test_data['text'] = test_data['text'].apply(clean_tweets)\ntest_data['text'].head(5)","53693fe0":"from keras.preprocessing.text import Tokenizer # Text tokenization\n\n# Setting up the tokenizer\nvocab_size = 1000\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\ntokenizer.fit_on_texts(list(train_data['prep_text']) + list(test_data['text']))","cb63239a":"# Representing texts as one hot encoded sequence\n\nX_train_ohe = tokenizer.texts_to_matrix(train_data['prep_text'], mode = 'binary')\nX_test_ohe = tokenizer.texts_to_matrix(test_data['text'], mode = 'binary')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_ohe.shape}\")\nprint(f\"X_test shape: {X_test_ohe.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")","4817c94a":"from sklearn.model_selection import train_test_split\nX_train_ohe, X_val_ohe, y_train, y_val = train_test_split(X_train_ohe, y_train, random_state = 42, test_size = 0.2)\n\nprint(f\"X_train shape: {X_train_ohe.shape}\")\nprint(f\"X_val shape: {X_val_ohe.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","4d57b746":"from keras.models import Sequential\nfrom keras import layers, metrics, optimizers, losses\n\ndef setup_model():\n    \n    model = Sequential()\n#     model.add(layers.Dense(16, activation='relu', input_shape=(vocab_size,)))\n#     model.add(layers.Dense(16, activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid', input_shape=(vocab_size,)))\n    \n    model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])\n    \n    return model\n\nmodel = setup_model()\nmodel.summary()","c0671744":"history = model.fit(X_train_ohe, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_ohe, y_val))","30ce33ab":"_, accuracy = model.evaluate(X_val_ohe, y_val)","da0389bd":"import matplotlib.pyplot as plt\n\ndef plot_history(history): \n\n    history_dict = history.history\n    history_dict.keys()\n\n\n    acc = history.history['binary_accuracy']\n    val_acc = history.history['val_binary_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(acc) + 1)\n\n    # \"bo\" is for \"blue dot\"\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    # b is for \"solid blue line\"\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.show()\n    \nplot_history(history)","b270ec80":"X_train_wc = tokenizer.texts_to_matrix(train_data['prep_text'], mode = 'count')\nX_test_wc = tokenizer.texts_to_matrix(test_data['text'], mode = 'count')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_wc.shape}\")\nprint(f\"X_test shape: {X_test_wc.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\n","d547fce5":"X_train_wc, X_val_wc, y_train, y_val = train_test_split(X_train_wc, y_train, random_state = 42, test_size = 0.2)\n\nprint(f\"X_train shape: {X_train_wc.shape}\")\nprint(f\"X_val shape: {X_val_wc.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","a2611d94":"model = setup_model()\nmodel.summary()","fead997a":"history = model.fit(X_train_wc, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_wc, y_val))","696d1ff0":"_, accuracy = model.evaluate(X_val_wc, y_val)","e3158d86":"plot_history(history)","bc2e3a98":"X_train_freq = tokenizer.texts_to_matrix(train_data['prep_text'], mode = 'freq')\nX_test_freq = tokenizer.texts_to_matrix(test_data['text'], mode = 'freq')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_freq.shape}\")\nprint(f\"X_test shape: {X_test_freq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")","59f7463e":"X_train_freq, X_val_freq, y_train, y_val = train_test_split(X_train_freq, y_train, test_size = 0.2, random_state = 42)\nprint(f\"X_train shape: {X_train_freq.shape}\")\nprint(f\"X_val shape: {X_val_freq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","90edc065":"model = setup_model()\nmodel.summary()","b4e3bd71":"history = model.fit(X_train_freq, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_freq, y_val))","ddd50350":"plot_history(history)","46a7e632":"train_data.head()","3eaa2445":"from sklearn.feature_extraction.text import TfidfVectorizer # Term Frequency - Inverse Document Frequency\n\nvectorizer = TfidfVectorizer(max_features = vocab_size)\nvectorizer.fit(list(train_data['prep_text']) + list(test_data['text']))\n\n# Fitting on training and testing data\nX_train_tfidf = vectorizer.transform(list(train_data['prep_text'])).toarray() \nX_test_tfidf = vectorizer.transform(list(test_data['text'])).toarray()\n\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape {X_train_tfidf.shape}\")\nprint(f\"X_test shape {X_test_tfidf.shape}\")\nprint(f\"y_train shape {y_train.shape}\")","abfb1504":"X_train_tfidf, X_val_tfidf, y_train, y_val = train_test_split(X_train_tfidf, y_train, test_size = 0.2, random_state = 42)\nprint(f\"X_train shape: {X_train_tfidf.shape}\")\nprint(f\"X_val shape: {X_val_tfidf.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","fb84112b":"model = setup_model()\nmodel.summary()","23469c08":"history = model.fit(X_train_tfidf, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_tfidf, y_val))","32d64bab":"plot_history(history)","fb05c043":"plt.hist(list(train_data['prep_text'].str.split().map(lambda x: len(x))))","9aaa72f1":"# Loading the embedding dictionary from file\n\nembedding_dict={}\nwith open('..\/input\/glovetwitter27b100dtxt\/glove.twitter.27B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","4b2490ea":"# Sequences creation, truncation and padding\n\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Setting up the tokenizer\nvocab_size = 10000\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\ntokenizer.fit_on_texts(list(train_data['prep_text']) + list(test_data['text']))\n\nmax_len = 15\nX_train_seq = tokenizer.texts_to_sequences(train_data['prep_text'])\nX_test_seq = tokenizer.texts_to_sequences(test_data['text'])\n\nX_train_seq = pad_sequences(X_train_seq, maxlen = max_len, truncating = 'post', padding = 'post')\nX_test_seq = pad_sequences(X_test_seq, maxlen = max_len, truncating = 'post', padding = 'post')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_seq.shape}\")\nprint(f\"X_test shape: {X_test_seq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")","5fd50331":"X_train_seq, X_val_seq, y_train, y_val = train_test_split(X_train_seq, y_train, test_size = 0.2, random_state = 42)\nprint(f\"X_train shape: {X_train_seq.shape}\")\nprint(f\"X_val shape: {X_val_seq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","4e7d0b49":"num_words = len(tokenizer.word_index)\nprint(f\"Number of unique words: {num_words}\")","451da2ce":"# Applying GloVE representations on our corpus\n\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tokenizer.word_index.items():\n    if i < num_words:\n        emb_vec = embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i] = emb_vec    ","6aa749ec":"# Setting up the model\n\nn_latent_factors = 100\nmodel_glove = Sequential()\nmodel_glove.add(layers.Embedding(num_words, n_latent_factors, weights = [embedding_matrix], \n                           input_length = max_len, trainable=True))\nmodel_glove.add(layers.Flatten())\n# model_glove.add(layers.Dense(16, activation='relu'))\nmodel_glove.add(layers.Dropout(0.5))\n# model_glove.add(layers.Dense(16, activation='relu'))\nmodel_glove.add(layers.Dense(1, activation='sigmoid'))\nmodel_glove.summary()","44434c95":"model_glove.compile(optimizer = optimizers.RMSprop(lr=0.001),\n              loss = losses.binary_crossentropy,\n              metrics = [metrics.binary_accuracy])\n\nhistory = model_glove.fit(X_train_seq,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(X_val_seq, y_val))","4dc9878d":"plot_history(history)","9111c2a9":"max_len = 15\nX_train_seq = tokenizer.texts_to_sequences(train_data['prep_text'])\nX_test_seq = tokenizer.texts_to_sequences(test_data['text'])\n\nX_train_seq = pad_sequences(X_train_seq, maxlen = max_len, truncating = 'post', padding = 'post')\nX_test_seq = pad_sequences(X_test_seq, maxlen = max_len, truncating = 'post', padding = 'post')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_seq.shape}\")\nprint(f\"X_test shape: {X_test_seq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\\n\")\n\n# Setting up the model\n\nn_latent_factors = 100\nmodel_glove = Sequential()\nmodel_glove.add(layers.Embedding(num_words, n_latent_factors, weights = [embedding_matrix], \n                           input_length = max_len, trainable=True))\nmodel_glove.add(layers.Flatten())\n# model_glove.add(layers.Dense(16, activation='relu'))\nmodel_glove.add(layers.Dropout(0.5))\n# model_glove.add(layers.Dense(16, activation='relu'))\nmodel_glove.add(layers.Dense(1, activation='sigmoid'))\nprint(f\"{model_glove.summary()}\\n\")\n\n\nmodel_glove.compile(optimizer = optimizers.RMSprop(lr=0.001),\n              loss = losses.binary_crossentropy,\n              metrics = [metrics.binary_accuracy])\n\nhistory = model_glove.fit(X_train_seq,\n                    y_train,\n                    epochs=20,\n                    batch_size=512)","d5b0d258":"# Setting up the tokenizer\nvocab_size = 1000\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\ntokenizer.fit_on_texts(list(train_data['text']) + list(test_data['text']))\n\n# Word count representation\nX_train_wc = tokenizer.texts_to_matrix(train_data['text'], mode = 'count')\nX_test_wc = tokenizer.texts_to_matrix(test_data['text'], mode = 'count')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_wc.shape}\")\nprint(f\"X_test shape: {X_test_wc.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\n\n# Train Validation Split\nX_train_wc, X_val_wc, y_train, y_val = train_test_split(X_train_wc, y_train, test_size = 0.2, random_state = 42)\n\nprint(f\"X_train shape: {X_train_wc.shape}\")\nprint(f\"X_val shape: {X_val_wc.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\\n\")\n\n# Setting up the model\nmodel = setup_model()\n\n# Fitting the model on un-preprocessed text\nhistory = model.fit(X_train_wc, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_wc, y_val))","8fb890f5":"submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntest_pred = model_glove.predict(X_test_seq)\ntest_pred_int = test_pred.round().astype('int')\nsubmission['target'] = test_pred_int\nsubmission.to_csv('submission.csv', index=False)","7727865e":"## Train Validation Split","a8cf5a70":"## Learning Curves","a0a78a0d":"# Data Preprocessing","ca050b87":"## The effect of text preprocessing","ebc54b5c":"## Term Frequency Representation","54a909a2":"## Train Validation Split","650d9d87":"## Training for Submission","51117690":"## Text One-Hot Encoding","fed53ca2":"## Text Cleaning and Preprocessing","92cc7fd5":"## Training on the same architecture","83422f17":"## Learning Curves","79e6c7b2":"## Modeling on a simple Neural Network","a258946f":"## Learning Curves","c5aa4e76":"## Setting up the model","41b4c112":"## Word-Count Representation","69922cde":"## Learning on the Same Architecture","2fb18ee9":"# Final Submission","ad99f763":"## Learning Curves","6bd81424":"## Learning Curves","cec82606":"# Using twitter GloVE embeddings","5863e98a":"## Train Validation Split","725611a8":"## Sequence Length Analysis","c4facf57":"## Using TF-IDF Vectorization","e93c1c7c":"## Setting up a model with the embeddings layer","0d1cd57e":"# Text Representation","97a5ae3a":"# Target Variable Exploration","5898a90b":"## Training on the same architecture","863aecd0":"## Train Test Split","d0cca07f":"It turns out the model overfits because of the noise of the text like stopwords, punctuation, un-stemmed words, etc."}}