{"cell_type":{"966d8306":"code","3e8339ce":"code","827d3e74":"code","47d90276":"code","761d7953":"code","122383c1":"code","56f87b9f":"code","31cf1550":"code","152f4cb7":"code","0a1dede1":"code","529c9250":"markdown","f94f1e01":"markdown","663c1b42":"markdown","aa3e885a":"markdown","9f05e276":"markdown","c333f9a7":"markdown","c308b8e8":"markdown","b9400262":"markdown"},"source":{"966d8306":"import pandas as pd\nimport seaborn as sb\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor \nfrom math import sqrt\nfrom scipy.stats import skew\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\ntrain_df = pd.read_csv(\"\/kaggle\/input\/c\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/c\/\/house-prices-advanced-regression-techniques\/test.csv\")\n\nnp.random.seed(42)","3e8339ce":"num_bins = math.floor(len(train_df) \/ 30)\nplt.hist(x=train_df['SalePrice'], bins=num_bins)\nplt.ylabel('Count')\nplt.title('SalePrice')\nplt.show()\n\ntrain_df['SalePrice'] = np.log(train_df['SalePrice'])\n\nplt.hist(x=train_df['SalePrice'], bins=num_bins)\nplt.ylabel('Count')\nplt.title('Log of SalePrice')\nplt.show()","827d3e74":"corr = train_df.corr()\ncorr = corr['SalePrice'].sort_values(ascending=False)\n\ncorr = corr[0:10]\nprint(corr)\n\nn_df = train_df[['SalePrice', 'GrLivArea', 'GarageArea', 'TotalBsmtSF']]\nnormalized_df=(n_df-n_df.min())\/(n_df.max()-n_df.min())\n\nsb.boxplot( data=normalized_df)\n","47d90276":"\n\n# Get the z-scores for the highly correlated values\nz_score_df = np.abs((train_df[corr.index] - train_df[corr.index].mean())\/train_df[corr.index].std())\n\n# Drop records with extreme values\ntrain_df = train_df.drop(\n        train_df[ z_score_df['SalePrice'] > 5].index)\n\ntrain_df = train_df.drop(\n        train_df[ z_score_df['GrLivArea'] > 5].index)\n\ntrain_df = train_df.drop(\n        train_df[ z_score_df['GarageArea'] > 4].index)\n\ntrain_df = train_df.drop(\n        train_df[ z_score_df['TotalBsmtSF'] > 5].index)","761d7953":"# Hang on to original values\ny_salesprice = train_df.SalePrice.values\nnum_train_recs = train_df.shape[0]\nnum_test_recs = test_df.shape[0]\n\n# Required for submission\ntest_id_df = test_df['Id']\n\n# Combine train and test data so\n# we can look for categorical values\n# present in the entire data set\nall_df = pd.concat([train_df, test_df], axis=0, sort=False).reset_index(drop=True)\n\nall_df = all_df.drop(['SalePrice'], axis=1)\nall_df = all_df.drop('Id', axis=1)\n\n# Identify columns that are mostly \n# null and drop them. Keep a list of\n# junk columns.\njunk_cols = []\nrec_count = len(all_df)\njunk_threshold = 0.25\n\nfor col in all_df.columns:\n    if (all_df[col].count() \/ rec_count) < junk_threshold:\n        junk_cols.append(col)\n    \nprint(f\"Junk columns: {', '.join(junk_cols)}\")  \n\nall_df = all_df.drop(junk_cols, axis=1)","122383c1":"group_df = all_df.groupby(['Neighborhood','HouseStyle'])\n\nfor col in all_df.columns:\n    \n    if all_df[col].isnull().values.any():\n\n        if np.issubdtype(all_df[col].dtype, np.number):\n            \n            all_df[col] = group_df[col].apply(\n                lambda x: x.fillna(all_df[col].median() if pd.isnull(x.median()) else x.median() ))\n\n        else:\n            all_df[col] = group_df[col].apply(\n                lambda x: x.fillna(x.mode().iloc[0] if len(x.mode()) else all_df[col].mode().iloc[0]))","56f87b9f":"skewed_cols = []\nfor col in all_df.columns[all_df.dtypes.apply(lambda t : np.issubdtype(t, np.number))]:\n    \n    if(abs(skew(all_df[col])) > 0.75):\n        skewed_cols.append(col)\n        all_df[col] = np.log1p(all_df[col])\n\nprint(f\"Skewed columns: {', '.join(sorted(skewed_cols))}\")\n","31cf1550":"\n# frequency encode \n\nenc_cols = []\nfor col in all_df.columns:\n    if all_df[col].dtype == 'object':\n        enc_cols.append(col)\n        d = all_df[col].value_counts().to_dict()\n        all_df[col] = all_df[col].map(d)\n        \nprint(f\"Frequency-encoded columns: {', '.join(sorted(enc_cols))}\")        \n","152f4cb7":"\ntrain_new = all_df[:num_train_recs]\ntest_new = all_df[num_train_recs:]\n\nprint(train_new.shape)\nprint(test_new.shape)\n\n\ntrain_X, test_X, train_y, test_y = train_test_split(train_new, y_salesprice, test_size=0.25, random_state=42)\n\n\n\"\"\"\nparams = {\n       'silent': [False],\n        'max_depth': [6, 10, 15, 20],\n        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n        'gamma': [0, 0.25, 0.5, 1.0],\n        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n        'n_estimators': [100],\n        'objective': ['reg:squarederror']\n}\n\n\nxgb_model = XGBRegressor()\n\n#gsearch = GridSearchCV(estimator = xgb_model,\n#                          param_grid = params,                        \n#                           cv = 5,\n#                           n_jobs = -1,\n#                           verbose = 1)\n#gsearch.fit(train_X, train_y )\n#print(gsearch.best_params_)\n\n#gsearch = RandomizedSearchCV(estimator=xgb_model, \n #                            param_distributions=params, n_jobs=-1,\n#                             cv = 5, \n#                             scoring='neg_mean_absolute_error', \n#                             n_iter=50,                              \n#                             return_train_score=True, \n#                             random_state=42, refit=True, \n#                             verbose=1)\n                         \n#gsearch.fit(train_X, train_y)\n#print(gsearch.best_params_)\n#print(gsearch.best_score_)\n#sys.exit()\n\n# Set param to output of cross-validation\n\"\"\"\n\nxgbr = XGBRegressor(\nsubsample= 0.7, \n reg_lambda= 0.1, \n n_estimators= 1000, \n min_child_weight= 7.0, \n max_depth= 20, \n learning_rate= 0.01, \n gamma= 0, \n colsample_bytree= 0.7, \n colsample_bylevel= 0.7,\n objective='reg:squarederror' )\n\nxgbr.fit(train_X, train_y, \n         eval_set=[(test_X, test_y)], verbose=False)\n\ny_pred_xgb = xgbr.predict(test_X)\n\n\nprint(\"Root Mean squared error: \" + \n      str(sqrt(mean_squared_error(test_y,y_pred_xgb))))\n\nprint(\"Root Mean squared error, recovered: \" + \n      str(sqrt(mean_squared_error(np.exp(test_y),np.exp(y_pred_xgb)))))\n\n\n# Plot actual vs. predicted\n\ndf = pd.DataFrame( columns = ['test','pred'])\ndf['test'] = np.exp(test_y)\ndf['pred'] = np.exp(y_pred_xgb)\n\n\ndf = df.sort_values('test')\ndf = df.reset_index(drop=True)\n\nplt.plot( 'pred', data=df, marker='', color='red', linewidth=1, label=\"Predicted\")\nplt.plot( 'test', data=df, marker='', color='blue', linewidth=1, label=\"Actual\")\n\nplt.legend(loc=\"upper left\")\nplt.title(\"Actual vs. Predicted\")\n\n","0a1dede1":"sub_predict_df = xgbr.predict(test_new)\n\nsub_predict_df = xgbr.predict(test_new)\nsub_df = pd.DataFrame(columns=['Id', 'SalePrice'])\nsub_df['Id'] = test_id_df   \nsub_df['SalePrice'] = np.exp(sub_predict_df)\n\nsub_df.to_csv('submission.csv', index=False)","529c9250":"Drop columns that are almost all null. They seem to just add noise to the model.","f94f1e01":"Identify heavily skewed columns and apply a log transform to them. The threshold of 0.75 was arrived at by experimentation.","663c1b42":"Make submission","aa3e885a":"Look at the values that are highly correlated with the sale price and eliminate outliers. Determine outliers by getting the z score for the columns.","9f05e276":"First off, the sale price is heavily skewed. Apply log to it.","c333f9a7":"Impute missing values. Get us mean and mode, but select by neighbourhood and house type.","c308b8e8":"Frequency encode categorical features. Repplace each value with the number of times the given value appears in the column.","b9400262":"Using some ideas found in the following:\n    \nhttps:\/\/www.kaggle.com\/tanushreewandkar\/eda-and-xgboost-model-building\nhttps:\/\/www.slideshare.net\/VishalPatel321\/feature-reduction-techniques\nhttps:\/\/machinelearningmastery.com\/feature-selection-for-regression-data\/\n"}}