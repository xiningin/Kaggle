{"cell_type":{"3bc9d149":"code","5244a81e":"code","9da33c66":"code","4f88561e":"code","02ee4627":"code","a5a9c876":"code","0eb6287f":"code","75e0133c":"code","850ba57e":"code","18afc2d5":"code","ae84b77f":"code","0b5ca1f9":"code","f8a7e8c0":"code","799a01d1":"code","3c118c10":"code","b89da915":"code","c127b5b7":"code","9b4a3782":"code","9c014ea1":"code","6b3163fc":"code","4bff0166":"code","a769be44":"code","07ba89c4":"code","9db2d842":"code","29dda453":"code","fb71164f":"code","d355d7e3":"code","31c8cbcd":"code","1f204f40":"code","1d73d567":"code","7d3a5d31":"code","f3c23a3a":"code","e2a7c0df":"code","b12d6b2f":"code","d0095bb8":"code","3296b278":"code","eb433017":"code","9c88a2ac":"code","99486194":"code","68883a78":"code","71b36553":"code","69b2ed03":"code","34d71495":"markdown","caa4aab9":"markdown","fee72279":"markdown","0c7e9e5b":"markdown","0edcc16f":"markdown","8be8bb90":"markdown","00826088":"markdown","e5a070b9":"markdown","3a502657":"markdown","7ac8f284":"markdown","20880d91":"markdown","a322fae1":"markdown","4d442015":"markdown","b0a60ef0":"markdown","e3b39faf":"markdown","757ede98":"markdown","94259dba":"markdown","821455f7":"markdown","14175cb8":"markdown","bacddbcb":"markdown","bc7943bb":"markdown","8f1fa51f":"markdown","ee5c1201":"markdown","7d24bc00":"markdown","119a9c0c":"markdown","8bd022f8":"markdown","cda31d22":"markdown","0048f1ac":"markdown","4729ed69":"markdown","cedaec7a":"markdown","08ae37e8":"markdown","9c0c5e18":"markdown","e2f7472d":"markdown","bc432858":"markdown","ab65ad4c":"markdown","9bb87626":"markdown","a3bb7c85":"markdown","5487de99":"markdown","b637264e":"markdown","6481ced5":"markdown","f5513fd3":"markdown","517d32ad":"markdown","4188f35a":"markdown","cc587781":"markdown","ec9be881":"markdown","9045775a":"markdown","38a9ba5d":"markdown","09783f84":"markdown","f02e46d5":"markdown","63b3acc8":"markdown","6c7f00ac":"markdown","937ad187":"markdown","3572f8cf":"markdown","ce18c11d":"markdown"},"source":{"3bc9d149":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n","5244a81e":"file_path = '..\/input\/train.csv'\nhome_data = pd.read_csv(file_path, index_col=0)","9da33c66":"home_data.head()","4f88561e":"home_data.tail()","02ee4627":"home_data.shape","a5a9c876":"home_data.select_dtypes(exclude=['object']).columns  # ignoreing object-typed columns ","0eb6287f":"home_data.select_dtypes(exclude=['object']).describe().round(decimals=2)","75e0133c":"home_data.select_dtypes(include=['object']).columns","850ba57e":"home_data.select_dtypes(include=['object']).describe()","18afc2d5":"target = home_data.SalePrice\nplt.figure()\nsns.distplot(target)\nplt.title('Distribution of SalePrice')\nplt.show()","ae84b77f":"sns.distplot(np.log(target))\nplt.title('Distribution of Log-transformed SalePrice')\nplt.xlabel('log(SalePrice)')\nplt.show()","0b5ca1f9":"print('SalePrice has a skew of ' + str(target.skew().round(decimals=2)) + \n      ' while the log-transformed SalePrice improves the skew to ' + \n      str(np.log(target).skew().round(decimals=2)))","f8a7e8c0":"# numerical attributes\nnum_attributes = home_data.select_dtypes(exclude='object').drop('SalePrice', axis=1).copy()\nfig = plt.figure(figsize=(12,18))\nfor i in range(len(num_attributes.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.distplot(num_attributes.iloc[:,i].dropna())  # slice while drop out NA\n    plt.xlabel(num_attributes.columns[i])\n\nplt.tight_layout()\nplt.show()","799a01d1":"fig = plt.figure(figsize=(12, 18))\n\nfor i in range(len(num_attributes.columns)):\n    fig.add_subplot(9, 4, i+1)\n    sns.boxplot(y=num_attributes.iloc[:,i])\n\nplt.tight_layout()\nplt.show()","3c118c10":"f = plt.figure(figsize=(12,20))\n\nfor i in range(len(num_attributes.columns)):\n    f.add_subplot(9, 4, i+1)\n    sns.scatterplot(num_attributes.iloc[:,i], target)\n    \nplt.tight_layout()\nplt.show()","b89da915":"correlation = home_data.corr()\n\nf, ax = plt.subplots(figsize=(14,12))\nplt.title('Correlation of numerical attributes', size=16)\nsns.heatmap(correlation)\nplt.show()\n\n## Heatmap with annotation of correlation values\n# sns.heatmap(home_data.corr(), annot=True)","c127b5b7":"correlation['SalePrice'].sort_values(ascending=False).head(15)","9b4a3782":"num_columns = home_data.select_dtypes(exclude='object').columns\ncorr_to_price = correlation['SalePrice']\nn_cols = 5\nn_rows = 8\nfig, ax_arr = plt.subplots(n_rows, n_cols, figsize=(16,20), sharey=True)\nplt.subplots_adjust(bottom=-0.8)\nfor j in range(n_rows):\n    for i in range(n_cols):\n        plt.sca(ax_arr[j, i])\n        index = i + j*n_cols\n        if index < len(num_columns):\n            plt.scatter(home_data[num_columns[index]], home_data.SalePrice)\n            plt.xlabel(num_columns[index])\n            plt.title('Corr to SalePrice = '+ str(np.around(corr_to_price[index], decimals=3)))\nplt.show()","9c014ea1":"# Show columns with most null values:\nnum_attributes.isna().sum().sort_values(ascending=False).head()","6b3163fc":"cat_columns = home_data.select_dtypes(include='object').columns\nprint(cat_columns)","4bff0166":"var = home_data['KitchenQual']\nf, ax = plt.subplots(figsize=(10,6))\nsns.boxplot(y=home_data.SalePrice, x=var)\nplt.show()","a769be44":"f, ax = plt.subplots(figsize=(12,8))\nsns.boxplot(y=home_data.SalePrice, x=home_data.Neighborhood)\nplt.xticks(rotation=40)\nplt.show()","07ba89c4":"## Count of categories within Neighborhood attribute\nfig = plt.figure(figsize=(12.5,4))\nsns.countplot(x='Neighborhood', data=home_data)\nplt.xticks(rotation=90)\nplt.ylabel('Frequency')\nplt.show()","9db2d842":"    home_data[cat_columns].isna().sum().sort_values(ascending=False).head(20)","29dda453":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split","fb71164f":"# Create copy of dataset  ====================================\nhome_data_copy = home_data.copy()\n\n# Dealing with missing\/null values ===========================\n# Numerical columns:\nhome_data_copy.MasVnrArea = home_data_copy.MasVnrArea.fillna(0)\n# HOW TO TREAT LotFrontage - 259 missing values??\n\n# Categorical columns:\ncat_cols_fill_none = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n                     'GarageCond', 'GarageQual', 'GarageFinish', 'GarageType',\n                     'BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'BsmtCond',\n                     'MasVnrType']\nfor cat in cat_cols_fill_none:\n    home_data_copy[cat] = home_data_copy[cat].fillna(\"None\")","d355d7e3":"# Check for outstanding missing\/null values\n# Scikit-learn's Imputer will be used to address these\nhome_data_copy.isna().sum().sort_values(ascending=False).head()","31c8cbcd":"# Remove outliers based on observations on scatter plots against SalePrice:\nhome_data_copy = home_data_copy.drop(home_data_copy['LotFrontage']\n                                     [home_data_copy['LotFrontage']>200].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['LotArea']\n                                     [home_data_copy['LotArea']>100000].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['BsmtFinSF1']\n                                     [home_data_copy['BsmtFinSF1']>4000].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['TotalBsmtSF']\n                                     [home_data_copy['TotalBsmtSF']>6000].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['1stFlrSF']\n                                     [home_data_copy['1stFlrSF']>4000].index)\nhome_data_copy = home_data_copy.drop(home_data_copy.GrLivArea\n                                     [(home_data_copy['GrLivArea']>4000) & \n                                      (target<300000)].index)\nhome_data_copy = home_data_copy.drop(home_data_copy.LowQualFinSF\n                                     [home_data_copy['LowQualFinSF']>550].index)","1f204f40":"home_data_copy['SalePrice'] = np.log(home_data_copy['SalePrice'])\nhome_data_copy = home_data_copy.rename(columns={'SalePrice': 'SalePrice_log'})","1d73d567":"transformed_corr = home_data_copy.corr()\nplt.figure(figsize=(12,10))\nsns.heatmap(transformed_corr)","7d3a5d31":"# Remove attributes that were identified for excluding when viewing scatter plots & corr values\nattributes_drop = ['SalePrice_log', 'MiscVal', 'MSSubClass', 'MoSold', 'YrSold', \n                   'GarageArea', 'GarageYrBlt', 'TotRmsAbvGrd'] # high corr with other attributes\n\nX = home_data_copy.drop(attributes_drop, axis=1)\n\n# Create target object and call it y\ny = home_data_copy.SalePrice_log\n\n# One-hot-encoding to transform all categorical data\nX = pd.get_dummies(X)\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n# Normalisation - to be added later\n# normaliser = StandardScaler()\n# train_X = normaliser.fit_transform(train_X)\n# val_X = normaliser.transform(val_X)\n\n# Final imputation of missing data - to address those outstanding after previous section\nmy_imputer = SimpleImputer()\ntrain_X = my_imputer.fit_transform(train_X)\nval_X = my_imputer.transform(val_X)","f3c23a3a":"from sklearn.metrics import mean_absolute_error\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor","e2a7c0df":"def inv_y(transformed_y):\n    return np.exp(transformed_y)\n\n# Series to collate mean absolute errors for each algorithm\nmae_compare = pd.Series()\nmae_compare.index.name = 'Algorithm'\n\n# Random Forest. Define the model. =============================\nrf_model = RandomForestRegressor(random_state=5)\nrf_model.fit(train_X,train_y)\nrf_val_predictions = rf_model.predict(val_X)\nrf_val_mae = mean_absolute_error(inv_y(rf_val_predictions),inv_y(val_y))\nmae_compare['RandomForest'] = rf_val_mae\n# print(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))\n\n# XGBoost. Define the model. ======================================\nxgb_model = XGBRegressor(n_estimators=1000,learning_rate=0.05)\nxgb_model.fit(train_X,train_y,early_stopping_rounds=5,eval_set=[(val_X,val_y)],verbose=False)\nxgb_val_predictions = xgb_model.predict(val_X)\nxgb_val_mae = mean_absolute_error(inv_y(xgb_val_predictions), inv_y(val_y))\nmae_compare['XGBoost'] = xgb_val_mae\n# print(\"Validation MAE for XGBoost Model: {:,.0f}\".format(xgb_val_mae))\n\n# Linear Regression =================================================\nlinear_model = LinearRegression()\nlinear_model.fit(train_X,train_y)\nlinear_val_predictions = linear_model.predict(val_X)\nlinear_val_mae = mean_absolute_error(inv_y(linear_val_predictions), inv_y(val_y))\nmae_compare['LinearRegression'] = linear_val_mae\n# print(\"Validation MAE for Linear Regression Model: {:,.0f}\".format(linear_val_mae))\n\n# Lasso ==============================================================\nlasso_model = Lasso(alpha=0.0005,random_state=5)\nlasso_model.fit(train_X,train_y)\nlasso_val_predictions = lasso_model.predict(val_X)\nlasso_val_mae = mean_absolute_error(inv_y(lasso_val_predictions),inv_y(val_y))\nmae_compare['Lasso'] = lasso_val_mae\n# print(\"Validation MAE for Lasso Model: {:,.0f}\".format(lasso_val_mae))\n\n# Ridge ===============================================================\nridge_model = Ridge(alpha=0.002,random_state=5)\nridge_model.fit(train_X,train_y)\nridge_val_predictions = ridge_model.predict(val_X)\nridge_val_mae = mean_absolute_error(inv_y(ridge_val_predictions),inv_y(val_y))\nmae_compare['Ridge'] = ridge_val_mae\n# print(\"Validation MAE for Ridge Regression Model: {:,.0f}\".format(ridge_val_mae))\n\n# ElasticNet ===========================================================\nelastic_net_model = ElasticNet(alpha=0.02, random_state=5, l1_ratio=0.7)\nelastic_net_model.fit(train_X, train_y)\nelastic_net_val_predictions = elastic_net_model.predict(val_X)\nelastic_net_val_mae = mean_absolute_error(inv_y(elastic_net_val_predictions), inv_y(val_y))\nmae_compare['ElasticNet'] = elastic_net_val_mae\n# print(\"Validation MAE for Elastic Net Model: {:,.0f}\".format(elastic_net_val_mae))\n\n# KNN Regression ========================================================\n# knn_model = KNeighborsRegressor()\n# knn_model.fit(train_X, train_y)\n# knn_val_predictions = knn_model.predict(val_X)\n# knn_val_mae = mean_absolute_error(inv_y(knn_val_predictions), inv_y(val_y))\n# mae_compare['KNN'] = knn_val_mae\n# # print(\"Validation MAE for KNN Model: {:,.0f}\".format(knn_val_mae))\n\n# Gradient Boosting Regression ==========================================\ngbr_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, \n                                      max_depth=4, random_state=5)\ngbr_model.fit(train_X, train_y)\ngbr_val_predictions = gbr_model.predict(val_X)\ngbr_val_mae = mean_absolute_error(inv_y(gbr_val_predictions), inv_y(val_y))\nmae_compare['GradientBoosting'] = gbr_val_mae\n# print(\"Validation MAE for Gradient Boosting Model: {:,.0f}\".format(gbr_val_mae))\n\n# # Ada Boost Regression ================================================\nada_model = AdaBoostRegressor(n_estimators=300, learning_rate=0.05, random_state=5)\nada_model.fit(train_X, train_y)\nada_val_predictions = ada_model.predict(val_X)\nada_val_mae = mean_absolute_error(inv_y(ada_val_predictions), inv_y(val_y))\nmae_compare['AdaBoost'] = ada_val_mae\n# # print(\"Validation MAE for Ada Boost Model: {:,.0f}\".format(ada_val_mae))\n\n# # Support Vector Regression ===========================================\n# svr_model = SVR(kernel='linear')\n# svr_model.fit(train_X, train_y)\n# svr_val_predictions = svr_model.predict(val_X)\n# svr_val_mae = mean_absolute_error(inv_y(svr_val_predictions), inv_y(val_y))\n# mae_compare['SVR'] = svr_val_mae\n# print(\"Validation MAE for SVR Model: {:,.0f}\".format(svr_val_mae))\n\nprint('MAE values for different algorithms:')\nmae_compare.sort_values(ascending=True).round()","b12d6b2f":"from sklearn.model_selection import cross_val_score\n\nimputer = SimpleImputer()\nimputed_X = imputer.fit_transform(X)\nn_folds = 10  # if None, use the default 3-fold cross validation\n\nscores = cross_val_score(lasso_model,imputed_X,y,scoring='neg_mean_squared_error',cv=n_folds)\nlasso_mae_scores = np.sqrt(-scores)\nprint('For LASSO model:')\n# print(lasso_mae_scores.round(decimals=2))\nprint('Mean RMSE = ' + str(lasso_mae_scores.mean().round(decimals=3)))\nprint('Error std deviation = ' +str(lasso_mae_scores.std().round(decimals=3)))","d0095bb8":"scores = cross_val_score(gbr_model,imputed_X,y,scoring='neg_mean_squared_error',cv=n_folds)\ngbr_mae_scores = np.sqrt(-scores)\n\nprint('For Gradient Boosting model:')\n# print(lasso_mae_scores.round(decimals=2))\nprint('Mean RMSE = ' + str(gbr_mae_scores.mean().round(decimals=3)))\nprint('Error std deviation = ' +str(gbr_mae_scores.std().round(decimals=3)))","3296b278":"scores = cross_val_score(xgb_model,imputed_X,y,scoring='neg_mean_squared_error',cv=n_folds)\nmae_scores = np.sqrt(-scores)\n\nprint('For XGBoost model:')\n# print(mae_scores.round(decimals=2))\nprint('Mean RMSE = ' + str(mae_scores.mean().round(decimals=3)))\nprint('Error std deviation = ' +str(mae_scores.std().round(decimals=3)))","eb433017":"scores = cross_val_score(rf_model,imputed_X,y,scoring='neg_mean_squared_error',cv=n_folds)\nmae_scores = np.sqrt(-scores)\n\nprint('For Random Forest model:')\n# print(mae_scores.round(decimals=2))\nprint('Mean RMSE = ' + str(mae_scores.mean().round(decimals=3)))\nprint('Error std deviation = ' +str(mae_scores.std().round(decimals=3)))","9c88a2ac":"# Grid search for hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# # Tuning XGBoost\n# param_grid = [{'n_estimators': [1000, 1500], \n#                'learning_rate': [0.01, 0.03] }]\n# #               'max_depth': [3, 6, 9]}]\n\n# top_reg = XGBRegressor()\n\n# Tuning Lasso\nparam_grid = [{'alpha': [0.0005, 0.00045, 0.0004]}]\ntop_reg = Lasso()\n\n# -------------------------------------------------------\ngrid_search = GridSearchCV(top_reg,param_grid,cv=5,scoring='neg_mean_squared_error')\ngrid_search.fit(imputed_X,y)\ngrid_search.best_params_","99486194":"# path to file you will use for predictions\ntest_data_path = '..\/input\/test.csv'\n\n# read test data file using pandas\ntest_data = pd.read_csv(test_data_path)","68883a78":"# create test_X which to perform all previous pre-processing on\ntest_X = test_data.copy()\n\n# Repeat treatments for missing\/null values =====================================\n# Numerical columns:\ntest_X.MasVnrArea = test_X.MasVnrArea.fillna(0)\n\n# Categorical columns:\nfor cat in cat_cols_fill_none:\n    test_X[cat] = test_X[cat].fillna(\"None\")\n\n# Repeat dropping of chosen attributes ==========================================\nif 'SalePrice_log' in attributes_drop:\n    attributes_drop.remove('SalePrice_log')\n\ntest_X = test_data.drop(attributes_drop, axis=1)\n\n# One-hot encoding for categorical data =========================================\ntest_X = pd.get_dummies(test_X)\n\n# ===============================================================================\n# Ensure test data is encoded in the same manner as training data with align command\nfinal_train, final_test = X.align(test_X, join='left', axis=1)\n\n# Imputer for all other missing values in test data. Note: Do not 'fit_transform'\nfinal_test_imputed = my_imputer.transform(final_test)","71b36553":"# Create model - on full set of data (training & validation)\n# Best model = Lasso?\nfinal_model = Lasso(alpha=0.00045, random_state=5)\n# final_model = XGBRegressor(n_estimators=1500, learning_rate=0.03)\nfinal_train_imputed = my_imputer.fit_transform(final_train)\n\n# Fit the model using all the data - train it on all of X and y\nfinal_model.fit(final_train_imputed, y)","69b2ed03":"# make predictions which we will submit. \ntest_preds = final_model.predict(final_test_imputed)\n\n# The lines below shows you how to save your data in the format needed to score it in the competition\n# Reminder: predictions are in log(SalePrice). Need to inverse-transform.\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': inv_y(test_preds)})\n\noutput.to_csv('submission.csv', index=False)","34d71495":"\u5176\u4e2d\uff0c\u4ee5\u4e0b\u5c5e\u6027\u4e3a\u5355\u6a21\u504f\u6001\u5206\u5e03\uff0c\u53ef\u501f\u52a9\u4e8e\u5bf9\u6570\u8f6c\u6362\u8fdb\u884c\u7ea0\u504f\uff1a\n> LotFrontage, LotArea, 1stFlrSF, GrLivArea, OpenPorchSF","caa4aab9":"### \u4f7f\u7528scikit-learn\u4e2d\u7684\u51fd\u6570(GridSearchCV)\uff0c\u7f51\u683c\u641c\u7d22\u6700\u4f73\u7684\u8d85\u53c2\u7ec4\u5408","fee72279":"* \u76ee\u524d\u4e3a\u6b62\uff0c\u6682\u65f6\u5047\u8bbePoolQC\u5230Bsmt\u7b49\u5c5e\u6027\u503c\u7684\u7f3a\u5931\uff0c\u662f\u56e0\u4e3a\u623f\u5c4b\u6ca1\u6709\u8fd9\u4e9b\u8bbe\u65bd(\u6c34\u6c60\u3001\u5730\u4e0b\u5ba4\u7b49)\u3002\u56e0\u6b64\u7f3a\u5931\u503c\u7528\"None\"\u586b\u5199\n* MasVnrType\u67098\u4e2a\u7f3a\u5931\u503c\uff0c\u4e0e\u7f3a\u5c11\u7684MasVnrArea\u503c\u7684\u6570\u91cf\u76f8\u540c\u3002\u53ef\u80fd\u7684\u786e\u662f\u6ca1\u6709\u7816\u77f3\u9970\u9762(MasVnr)\uff0c\u56e0\u6b64\u586b\u5199\"None\"","0c7e9e5b":"### \u5f02\u5e38\u503c(Outlier)\u53d1\u73b0\n\u6570\u636e\u53ef\u89c6\u5316\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u53d1\u73b0\u6f5c\u5728\u7684\u5f02\u5e38\u6570\u636e\uff0c\u4f8b\u5982\uff1a\n1. \u5728\u5355\u53d8\u91cf\u5206\u6790(univariate analysis)\u4e2d\uff0c\u4ee5\u4f7f\u7528\u7bb1\u578b\u56fe\u4e3a\u4f8b\uff0c\u5f02\u5e38\u503c\u4e0e\u4e0a\u56db\u5206\u4f4d\u6570\u6216\u4e0b\u56db\u5206\u4f4d\u6570\u7684\u8ddd\u79bb\u8d85\u8fc7IQR(\u5167\u8ddd\uff0c\u53c8\u79f0\u56db\u5206\u4f4d\u5dee)\u7684\u500d\u6570(1.5-3)\u3002\u5982\u679c\u6570\u636e\u504f\u659c\uff0c\u5c06\u5176\u5148\u8f6c\u6362\u4e3a\u66f4\u5bf9\u79f0\u7684\u5206\u5e03\u53ef\u80fd\u4f1a\u6709\u6240\u5e2e\u52a9\u3002\n2. \u5728\u53cc\u53d8\u91cf\u5206\u6790(bivariate analysis)\u4e2d\uff0c\u4ee5\u4f7f\u7528\u6563\u70b9\u56fe\u4e3a\u4f8b\uff0c\u5728\u76f8\u8fd1x\u503c\u4e0b\u5f02\u5e38\u503c\u7684y\u503c\u4e0e\u5176\u4ed6\u89c2\u5bdf\u5dee\u5f02\u8f83\u5927\u3002\u6b64\u5916\uff0c\u501f\u52a9\u4e8e\u6700\u5c0f\u4e8c\u4e58\u7ebf\u6027\u56de\u5f52\u7684\u6b8b\u5dee\u56fe\u4e5f\u53ef\u4ee5\u6765\u68c0\u6d4b\u5f02\u5e38\u503c\u3002\n\n\u5bf9\u4e8e\u6240\u6709\u7684\u5f02\u5e38\u503c\u5e94\u4ed4\u7ec6\u6838\u67e5\uff0c\u5fc5\u8981\u65f6\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u6e05\u6d17\uff0c\u5254\u9664\u5f02\u5e38\u503c\u3002\n\n[\u8fd4\u56de\u76ee\u5f55](#top)","0edcc16f":"\u6570\u636e\u504f\u659c(Skew of data)\u5bf9\u56de\u5f52\u6a21\u578b\u7684\u9884\u6d4b\u7cbe\u5ea6\u6709\u4e0d\u5229\u5f71\u54cd\uff0c\u5e94\u5bf9\u5176\u8fdb\u884c\u6700\u5c0f\u5316\n\u6ce8\uff1a\u51b3\u7b56\u6811\u548c\u968f\u673a\u68ee\u6797\u4e0d\u9700\u8981\u77eb\u6b63\u504f\u5dee","8be8bb90":"### \u5206\u7c7b\u5217\u4e2d\u7684\u7f3a\u5931\u503c\/ null\u503c\n\n[\u8fd4\u56de\u76ee\u5f55](#top)","00826088":"## 1.2 \u63a2\u7d22\u6570\u503c\u5217\n[\u8fd4\u56de\u76ee\u5f55](#top)","e5a070b9":"<a id='transform_skew'><\/a>","3a502657":"### \u521b\u5efa\u6700\u7ec8\u6a21\u578b","7ac8f284":"<a id='top'><\/a>","20880d91":"\u9ad8\u5ea6\u76f8\u5173\u7684\u5c5e\u6027\u5305\u62ec\uff08\u5de6\u4fa7\u5c5e\u6027\u4e0eSalePrice_log\u5177\u6709\u66f4\u9ad8\u7684\u76f8\u5173\u6027\uff09\n* GarageCars \u548c GarageArea (0.882)\n* YearBuilt \u548c GarageYrBlt (0.826)\n* GrLivArea_log1p \u548c TotRmsAbvGrd (0.826)\n* TotalBsmtSF \u548c 1stFlrSF_log1p (0.780)\n\u53ef\u4ee5\u8003\u8651\u4ece\u4e0a\u8ff0\u5bf9\u4e2d\u5220\u9664\u4e0eSalePrice_log\u5177\u6709\u8f83\u4f4e\u76f8\u5173\u6027\u7684\u5217\uff0c\u5176\u76f8\u5173\u6027\u8d85\u8fc70.8","a322fae1":"### \u6570\u636e\u96c6\u4e2d\u7684\u5206\u7c7b\u5217","4d442015":"### \u8bc4\u4f30\u5c5e\u6027\u95f4\u7684\u76f8\u5173\u6027\n\u4e24\u5217\u6570\u636e\u4e4b\u95f4\u7684\u7ebf\u6027\u76f8\u5173\u6027\u5982\u4e0b\u6240\u793a\uff0c\u9ed8\u8ba4\u4f7f\u7528Pearson\u76f8\u5173\u7cfb\u6570\u6765\u8861\u91cf\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff1a\n1. \u76f8\u5173\u56fe\u548c\u6563\u70b9\u56fe\u7684\u7ec4\u5408\u53ef\u4ee5\u5e2e\u52a9\u7406\u89e3\u662f\u5426\u5b58\u5728\u975e\u7ebf\u6027\u76f8\u5173\n2. \u76f8\u5173\u7cfb\u6570\u53ef\u80fd\u4f1a\u53d7\u5230\u5355\u4e2a\u5f02\u5e38\u503c\u7684\u4e25\u91cd\u5f71\u54cd\n\n\u5f88\u591a\u4f5c\u8005\u5efa\u8bae _\"\u5728\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u8fdb\u884c\u5efa\u6a21\u65f6\uff0c\u6709\u5fc5\u8981\u5220\u9664\u76f8\u5173\u53d8\u91cf\u4ee5\u6539\u8fdb\u6a21\u578b\"_, \u4e14 _\"\u5728\u7279\u5f81\u9009\u62e9\u8fc7\u7a0b\u4e2d\u5220\u9664\u76f8\u5173\u53d8\u91cf\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u505a\u6cd5\"_\n\n\u4e0b\u9762\u662f\u6570\u503c\u5217\u7684\u76f8\u5173\u6027\u70ed\u56fe\uff1a","b0a60ef0":"<a id='check_null'><\/a>","e3b39faf":"### \u6267\u884c\u7279\u5f81\u9009\u62e9&\u7f16\u7801\u5206\u7c7b\u5217","757ede98":"### \u8003\u8651\u9ad8\u5ea6\u76f8\u5173\u7684\u7279\u5f81\n\u5411\u673a\u5668\u7b97\u6cd5\u63d0\u4f9b\u9ad8\u5ea6\u76f8\u5173\u7684\u7279\u5f81\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002 \u56e0\u6b64\uff0c\u5904\u7406\u5982\u4e0b:","94259dba":"\u57fa\u4e8e\u4ee5\u4e0a\u6563\u70b9\u56fe\uff0c\u4f3c\u4e4e\u5b58\u5728\u4e00\u4e9b\u5f02\u5e38\u503c\uff1a\n* LotFrontage ( >200) and LotArea (>100000)\n* BsmtFinSF1 (>4000) and TotalBsmtSF (>6000)\n* 1stFlrSF (>4000)\n* GrLivArea (>4000 AND SalePrice <300000)\n* LowQualFinSF (>550) \n\nReference: [quick link to the implementation](#address_outliers) ","821455f7":"<a id='explore_cat_columns'><\/a>","14175cb8":"## 1.1 \u521d\u6b65\u89c2\u5bdf\n\u6570\u636e\u96c6\u4e2d\u7684\u793a\u4f8b\u5982\u4e0b\u6240\u793a <br><br>\n[\u8fd4\u56de\u76ee\u5f55](#top)","bacddbcb":"### \u8bfb\u53d6\u8f93\u5165\u6587\u4ef6","bc7943bb":"### \u5bfc\u5165\u6a21\u5757","8f1fa51f":"# 5. \u7b97\u6cd5\u9009\u62e9 & \u8c03\u53c2\n\n__\u63d0\u9192:__\n* __\u5728\u6d4b\u8bd5\u6570\u636e\u65f6\u548c\u8bad\u7ec3\u65f6\u7c7b\u4f3c\uff0c\u9700\u8981\u8fdb\u884c\u76f8\u5e94\u7684\u8f6c\u6362\u548c\u89c4\u8303\u5316__\n* __\u786e\u4fdd\u5c06\u9884\u6d4b\u503c\u8fdb\u884c\u53cd\u53d8\u6362\u8f6c\u4e3a SalePrice__\n\n[\u8fd4\u56de\u76ee\u5f55](#top)","ee5c1201":"### \u6570\u503c\u5217\u4e2d\u7684\u7f3a\u5931\u503c\/ null\u503c\n\n[\u8fd4\u56de\u76ee\u5f55](#top)****","7d24bc00":"## 2.2 \u5904\u7406\u5f02\u5e38\u503c\n[\u8fd4\u56de\u76ee\u5f55](#top)<br>\nReference: [Quick link to notes](#notes_outliers)","119a9c0c":"__\u63d0\u9192: \u5f53\u524d\u7684\u9884\u6d4b\u76ee\u6807\u662f log(SalePrice). \u5728\u9884\u6d4b\u540e\u8bb0\u5f97\u8f6c\u6362\u56deSalePrice!__","8bd022f8":"<a id='data_clean'><\/a>","cda31d22":"__\u5355\u53d8\u91cf\u5206\u6790 - \u6570\u503c\u5c5e\u6027\u7684\u7bb1\u5f62\u56fe__","0048f1ac":"<a id='address_outliers' ><\/a>","4729ed69":"__\u53cc\u53d8\u91cf\u5206\u6790 - \u76ee\u6807\u5c5e\u6027\u4e0e\u6570\u503c\u5c5e\u6027\u7684\u6563\u70b9\u56fe__","cedaec7a":"### \u5c5e\u6027\u5206\u5e03","08ae37e8":"# Future Study\n### Exploratory Data Analysis\n* How to assess outliers for categorical columns?\n* How to check for correlation amongst categorical columns?\n\n### Data Cleaning & Preprocessing\n* Add code to perform data normalisation\/standardisation.\n* Create custom scikit-learn preprocessing class so that this can be easily used later with pipelines.\n* Consider scikit-learn's PowerTransformer preprocessing module to fit and transform columns to be more Gaussian-like. \n\n### Feature Selection & Engineering\n* Think about possible combination of attributes to create useful new features, including polynomials.\n* Explore pros and cons of other methods to encode categorical attributes, besides one-hot encoding\n* Consider further advanced preprocessing techniques, such as Principal Components Analysis for dimensionality reduction, to create new features\n\n### Preliminary Assessment of ML Algorithms\n* Use scikit-learn's pipelines!\n* Learn how best to utilise ensemble methods.\n* Explore choices for performance criteria.\n* Plots to diagnose bias vs. variance, and learning curves\n\n### Selection of Best Algorithm(s) & Fine-tuning\n* Test newly engineered features as hyperparameters in grid-search cross-validation.\n* Partial-dependence plots for understanding the final model better.\n\n[\u8fd4\u56de\u76ee\u5f55](#top)","9c0c5e18":"## 2.3 \u6570\u636e\u8f6c\u6362\u4ee5\u7f13\u89e3\u504f\u659c\n\u76ee\u524d\u4ec5\u9650\u4e8e\u76ee\u6807\u53d8\u91cf\n[\u8fd4\u56de\u76ee\u5f55](#top)","e2f7472d":"# 3. \u7279\u5f81\u9009\u62e9 & \u7279\u5f81\u5de5\u7a0b\n\n[\u8fd4\u56de\u76ee\u5f55](#top)","bc432858":"<a id='prelim_explore'><\/a>","ab65ad4c":"<a id='fix_nans'><\/a>","9bb87626":"\u57fa\u4e8e\u6563\u70b9\u56fe\u548c\u76f8\u5173\u6027\u70ed\u56fe\uff0c\u8003\u8651\uff1a\n* \u5254\u9664GarageArea - \u4e0eGarageCars\u9ad8\u5ea6\u76f8\u5173(0.88)\uff0c\u5176\u4e2d GarageCars\u4e0eSalePrice\u9ad8\u5ea6\u76f8\u5173(0.64)\n* \u5254\u9664GarageYrBlt - \u4e0eYearBuilt\u9ad8\u5ea6\u76f8\u5173(0.83), \u5176\u4e2dYearBuilt\u4e0eSalePrice\u8f83\u4e3a\u76f8\u5173(0.52)\n* \u5254\u9664\u6240\u6709\u4e0eSalePrice\u7ebf\u6027\u76f8\u5173\u6027\u4f4e\u4e14\u975e\u7ebf\u6027\u76f8\u5173\u6027\u4e0d\u660e\u786e\u7684\u5c5e\u6027 - \u5982MSSubClass, MoSold, YrSold, MiscVal, BsmtFinSF2, BsmtUnfSF, LowQualFinSF","a3bb7c85":"# \u7b80\u4ecb&\u76ee\u5f55\n[House Prices: 1st Approach to Data Science Process](https:\/\/www.kaggle.com\/cheesu\/house-prices-1st-approach-to-data-science-process) \u4e2d\u6587\u7248\n\n[1. \u63a2\u7d22\u6027\u6570\u636e\u5206\u6790](#explore) <br>\n> [1.1 \u521d\u6b65\u89c2\u5bdf](#prelim_explore) <br>\n   [1.2 \u63a2\u7d22\u6570\u503c\u5217](#explore_num_columns) <br>\n   [1.3 \u63a2\u7d22\u5206\u7c7b\u5217](#explore_cat_columns)  <br>\n\n[2. \u6570\u636e\u6e05\u6d17 & \u9884\u5904\u7406](#data_clean)\n> [2.1 \u5904\u7406\u7f3a\u5931\u503c\/null \u503c](#fix_nans) <br>\n   [2.2 \u5904\u7406\u5f02\u5e38\u503c](#address_outliers)<br>\n   [2.3 \u6570\u636e\u8f6c\u6362\u4ee5\u7f13\u89e3\u503e\u659c](#transform_skew) <br>\n\n[3. \u7279\u5f81\u9009\u62e9 & \u7279\u5f81\u5de5\u7a0b](#feature_eng)\n\n[4. \u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u9884\u4f30](#algorithms)\n\n[5. \u7b97\u6cd5\u9009\u62e9\u4e0e\u8c03\u53c2](#fine_tune)","5487de99":"## 1.3 \u63a2\u7d22\u5206\u7c7b\u5217\n[\u8fd4\u56de\u76ee\u5f55](#top)\n### SalePrice\u4e0e\u5206\u7c7b\u503c\u7684\u7bb1\u5f62\u56fe\u793a\u4f8b","b637264e":"### \u91cd\u590d\u5148\u524d\u5b9a\u4e49\u7684\u9884\u5904\u7406","6481ced5":"<a id='explore_num_columns'><\/a>","f5513fd3":"### \u4ea4\u53c9\u9a8c\u8bc1\n\u501f\u52a9\u4e8escikit-learn\u7684cross_val_score\u6765\u8fdb\u884cK-\u6298\u4ea4\u53c9\u9a8c\u8bc1 ","517d32ad":"<a id='feature_eng'><\/a>","4188f35a":"<a id='algorithms'><\/a>","cc587781":"<a id='fine_tune'><a>","ec9be881":"### \u5bfc\u5165\u673a\u5668\u5b66\u4e60\u6a21\u5757","9045775a":"# 4. \u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u9884\u4f30\n[\u8fd4\u56de\u76ee\u5f55](#top)","38a9ba5d":"<a id='explore'><\/a>","09783f84":"* \u5c1a\u4e0d\u6e05\u695a\u8be5\u5982\u4f55\u5904\u7406LotFrontage\u7684\u7f3a\u5931\u503c\u3002(\u4e2d\u4f4d\u6570\u7684\u7b80\u5355\u4f30\u8ba1\uff1fLotFrontage\u4e0eNeigborhood\u7684\u76f8\u5173\u6027?)\n* GarageYrBlt\u4e0eYearBuilt\u9ad8\u5ea6\u76f8\u5173\uff0c\u5b83\u5728\u673a\u5668\u5b66\u4e60\u6b65\u9aa4\u4e4b\u524d\u88ab\u4e22\u5f03,  \u56e0\u6b64\u65e0\u9700\u91c7\u53d6\u4efb\u4f55\u884c\u52a8\n* MasVnrArea\u67098\u4e2a\u7f3a\u5931\u503c\uff0c\u4e0e\u7f3a\u5c11MasVnrType\u503c\u7684\u6570\u91cf\u76f8\u540c(1460-1452) , \u53ef\u80fd\u662f\u6ca1\u6709\u7816\u77f3\u9970\u9762(MasVnr)\uff0c \u56e0\u6b64\u8bb0\u4e3a0","f02e46d5":"### \u6570\u636e\u96c6\u4e2d\u7684\u6570\u503c\u5217","63b3acc8":"## 2.1 \u5904\u7406\u7f3a\u5931\u503c\/null\u503c\n[\u8fd4\u56de\u76ee\u5f55](#top)","6c7f00ac":"# 1. \u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\n1. \u521d\u6b65\u4e86\u89e3\u53ef\u7528\u6570\u636e\n2. \u68c0\u67e5\u7f3a\u5c11\u7684\u503c\u6216\u7a7a\u503c\n3. \u67e5\u627e\u6f5c\u5728\u7684\u5f02\u5e38\u503c\n4. \u8bc4\u4f30\u5c5e\u6027\/\u7279\u5f81\u4e4b\u95f4\u7684\u76f8\u5173\u6027\n5. \u68c0\u67e5\u6570\u636e\u503e\u659c\n\n[\u8fd4\u56de\u76ee\u5f55](#top)","937ad187":"# 2. \u6570\u636e\u6e05\u6d17 & \u9884\u5904\u7406\n\n[\u8fd4\u56de\u76ee\u5f55](#top)","3572f8cf":"\n* \u6570\u636e\u96c6\u67d0\u4e9b\u884c\u4e2d\u7684\u67d0\u4e9b\u6f5c\u5728\u5f02\u5e38\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5217\u6570\u636e\u7c7b\u578b\u6210\u4e3a\u201c\u5bf9\u8c61\u201d\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u5728\u533a\u5206\u6570\u5b57\u5217\u548c\u5206\u7c7b\u5217\u65f6\u51fa\u9519\u3002\u5982\u4f55\u6709\u6548\u5730\u68c0\u67e5\u8fd9\u4e00\u70b9\uff1f\n\n* \u53ef\u80fd\u6709\u4e00\u4e9b\u6570\u503c\u5217\u7684\u6570\u636e\u662f\u79bb\u6563\u7684\uff0c\u5e76\u4e14\u503c\u7684\u6570\u91cf\u6709\u9650\u3002\u8fd9\u4e9b\u5217\u4e5f\u53ef\u4ee5\u89e3\u91ca\u4e3a\u5206\u7c7b\u6570\u636e\u3002","ce18c11d":"### \u6700\u7ec8\u9884\u6d4b"}}