{"cell_type":{"2f798180":"code","56902f36":"code","6a7f44be":"code","922927cb":"code","768f44b0":"code","4102025e":"code","0b858cb4":"markdown","b68a2065":"markdown","adb346bc":"markdown","a0e1973e":"markdown","b0f76417":"markdown","b94d08d8":"markdown","dcd4f48e":"markdown","cf84b690":"markdown"},"source":{"2f798180":"!\/opt\/conda\/bin\/python3.7 -m pip install --upgrade pip\n!pip uninstall -y typing\n!pip install carefree-learn","56902f36":"\"\"\"\n`cfdata` can help us explore tabular datasets.\n`cflearn` can help us builde neural networks for tabular datasets.\n\"\"\"\n\nimport os\nimport cflearn\n\nfrom cfdata.tabular import *\n\nfile_folder = \"\/kaggle\/input\/titanic\"\nworking_folder = \"\/kaggle\/working\"","6a7f44be":"def scoring(raw_metrics, mean, std):\n    # We encourage larger std because we need to ensemble the models afterwards, and ensembling prefers models with larger variance.\n    return mean + std\n\ndef test():\n    train_file = f\"{file_folder}\/train.csv\"\n    test_file = f\"{file_folder}\/test.csv\"\n    data_config = {\"label_name\": \"Survived\"}\n    hpo = cflearn.tune_with(\n        train_file,\n        model=\"tree_dnn\",\n        temp_folder=f\"{working_folder}\/__hpo__\",\n        task_type=TaskTypes.CLASSIFICATION,\n        estimator_scoring_function=scoring,\n        data_config=data_config,\n        num_parallel=5\n    )\n    results = cflearn.repeat_with(\n        train_file,\n        **hpo.best_param,\n        models=\"tree_dnn\",\n        temp_folder=f\"{working_folder}\/__repeat__\",\n        num_repeat=10, num_jobs=5,\n        data_config=data_config\n    )\n    ensemble = cflearn.EnsemblePattern(results.patterns[\"tree_dnn\"])\n    predictions = ensemble.predict(test_file).ravel()\n    x_te, _ = results.transformer.data.read_file(test_file, contains_labels=False)\n    id_list = DataTuple.with_transpose(x_te, None).xT[0]\n    # Score : achieved ~0.79\n    with open(f\"{working_folder}\/submissions.csv\", \"w\") as f:\n        f.write(\"PassengerId,Survived\\n\")\n        for test_id, prediction in zip(id_list, predictions):\n            f.write(f\"{test_id},{prediction}\\n\")\n    return results\n\n\nif __name__ == '__main__':\n    results = test()","922927cb":"experiments = results.experiments\nms = {k: list(map(cflearn.load_task, v)) for k, v in experiments.tasks.items()}\nprint(ms)","768f44b0":"model = ms[\"tree_dnn\"][0].model\ndata = model.tr_data\n# the original (raw) data read from train.csv\nprint(\"===   Raw     ===\")\nprint(f\"Feature dimension : {len(data.raw.x[0])}\")\nprint()\nfor line in data.raw.x[:3]:\n    print(line)\nprint(data.raw.y[:3])\nprint()\n# the converted data\nprint(\"=== Converted ===\")\nprint(f\"Feature dimension : {data.converted.x.shape[1]}\")\nprint(data.converted.x[:3])\nprint(data.converted.y[:3])\nprint()\n# the processed data\nprint(\"=== Processed ===\")\nprint(f\"Feature dimension : {data.processed.x.shape[1]}\")\nprint(data.processed.x[:3])\nprint(data.processed.y[:3])","4102025e":"print(model.encoders)","0b858cb4":"From the above codes, `results` is returned for further inspection. We can see what neural networks we've got by loading the (completed) `tasks` from the `experiments`:","b68a2065":"## Training & Inference\n\nOne great thing of `carefree-learn` is that it doesn't need any explicit data-preprocessing - it can take files as inputs and predict with files directly!\n\nMore over, some common practises, such as hyper parameter optimization (`cflearn.tune_with`) and ensembling (`cflearn.repeat_with` & `cflearn.EnsemblePattern`), can be completed in a few lines of codes. These APIs also hide some other common practises under the hood, such as:\n\n+ Elegantly deal with data pre-processing.\n    + A `Recognizer` to recognize whether a column is `STRING`, `NUMERICAL` or `CATEGORICAL`.\n    + A `Converter` to convert a column into friendly format ([\"one\", \"two\"] -> [0, 1]).\n    + A `Processor` to further process columns (`OneHot`, `Normalize`, `MinMax`, ...).\n+ Handle datasets saved in files (`.txt`, `.csv`).\n+ Cross validation with different train-valid split.\n+ Parallel running to speed up the whole process (`num_parallel` & `num_jobs` shown below are the corresponding parameters).\n+ Bayesian Optimization (BO) for better hyper parameter optimization.\n\nFrom the above, it comes out that `carefree-learn` could be treated as a minimal **Auto**matic **M**achine **L**earning (AutoML) solution for tabular datasets when it is fully utilized. However, this is not built on the sacrifice of flexibility. In fact, the functionality we've mentioned are all wrapped into individual modules in `carefree-learn` and allow users to customize them easily.\n\nWe'll dive into the details later, but for now let's see how could we generate a `submissions.csv` with fairly good performance (~0.79):","adb346bc":"# Conclusion\n\nIn this notebook I've shown how neural networks could be made EASY with [carefree-learn](https:\/\/github.com\/carefree0910\/carefree-learn), and have shown some of the details under the hood. If you are interested in more details (e.g. details of `cflearn.tune_with`, `cflearn.repeat_with`, etc.), feel free to comment below and I'll update this notebook as soon as possible! \ud83d\ude09","a0e1973e":"From the above we can see that we've trained 10 neural networks (`TreeDNN`) with `cflearn.repeat_with`. These models are trained under the best parameters (`hpo.best_param`) that `cflearn.tune_with` found.\n\nSince I've mentioned that `carefree-learn` will complete many common practises under the hood, let's dive into one of these models and see how `carefree-learn` interpret and pre-process the original dataset:","b0f76417":"From the above we can see that both `OneHot` encoding and `Embedding` encoding are used for categorical columns.","b94d08d8":"> [Here's](https:\/\/github.com\/carefree0910\/carefree-learn\/blob\/dev\/examples\/titanic\/test_titanic.py) the full version of the source codes.\n\n# Introduction\n\nIn this notebook, I'll show you how to use Neural Networks to solve tasks of tabular datasets easily with the help of [carefree-learn](https:\/\/github.com\/carefree0910\/carefree-learn). I'll first show the solution and then dive a little bit into it and see how powerful [carefree-learn](https:\/\/github.com\/carefree0910\/carefree-learn) is.","dcd4f48e":"## Preparation\n\n> Notice that kaggle pre-installed PyTorch so I can install `carefree-learn` directly with pip. If you want to use `carefree-learn` on your own machine, make sure to pre-install PyTorch first! \ud83d\ude04","cf84b690":"From the above we can see that `carefree-learn` maintains three versions of data: `raw`, `converted` and `processed`.\n\n### Raw\nA `raw` data indicates the original data read from the data source, it depends on what kind of data you gave to `carefree-learn` (file, Python list, or numpy array). In this example, we use `train.csv` file as input, so `raw` data will be the contents from the `train.csv` file.\n\n> You might notice that I used `data_config = {\"label_name\": \"Survived\"}` in my codes. This is necessary because `carefree-learn` will treat the last column as labels by default, but `Survived` column is not the last column in `train.csv`.\n\n> You might also notice that the missing values (e.g. some values in `Cabin` column) are stored as `nan` in this data version.\n\n### Converted\nA `converted` data indicates the converted version of the `raw` data. This data version is already available for training, because it already converts string values to categorical \/ numerical values.\n\n> It is worth mentioning that `carefree-learn` is able to recognize whether a feature column is categorical or numerical, based on some handcraft rules with generalization ability. So you can see that some features (e.g. Gender, Pclass) are interpreted as categorical features, while some others (e.g. Age, Fare) are interpreted as numerical features.\n\n> You might also notice that the missing values (`nan`) are properly imputed.\n\n### Processed\nA `processed` data indicates the pre-processed version of the `converted` data (e.g. normalization).\n\nYou might doubt that there's no encoding for categorical features at all. That's because in neural networks some encoding methods contain trainable parameters (e.g. embedding), so it is not a proper way to reveal encodings in the input data. Instead, we should integrate the encoding(s) into our model:"}}