{"cell_type":{"2c5b0afa":"code","a081b536":"code","6519c0e3":"code","ab547941":"code","551d820e":"code","54ff914c":"code","8c13af3e":"code","d2d70e32":"code","d3913e51":"code","472afd27":"code","fbb282cd":"markdown","54c12a6f":"markdown","809148cd":"markdown","e1af7972":"markdown","6b9ee788":"markdown","954da0ba":"markdown","4e2a1f2f":"markdown"},"source":{"2c5b0afa":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing \nimport h2o\nfrom h2o.automl import H2OAutoML\nfrom h2o.estimators import H2OWord2vecEstimator\nh2o.init(max_mem_size='16G')","a081b536":"train = pd.read_csv(\"\/kaggle\/input\/actuarial-loss-estimation\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/actuarial-loss-estimation\/test.csv\")\n\ntrain[\"Test\"]=0\ntest[\"Test\"]=1\n\nX = pd.concat((train, test),sort=True).reset_index(drop=True)\nX.columns = ['Age', 'Desc', 'ClaimNo', 'DateRep','DateAcc', 'DaysWorked', 'Children',\n            'OtherDep', 'Gender', 'HoursWorked','Init', 'Marit', 'FT','Test', 'Ult', 'Wages']","6519c0e3":"X.Wages = pd.to_numeric(X.Wages,'coerce')\nsummary = pd.DataFrame({\"Missing\":len(X)- X.count(),\"Typical\":np.nan})\n\nfor col in summary[(summary.Missing > 0)].index: \n    if col == \"Ult\": continue\n    colmode=X[col].mode()[0]\n    summary.loc[col,\"Typical\"]=colmode\n    X[col].fillna(colmode,inplace=True)\n\nX['Init0']= (X.Init <= 50).astype(int)\nX.loc[X.Init <= 50,'Init']=X.Init.mode()[0]\n\nX.loc[X.Wages.astype(int) < 5,'Wages']=float(X.Wages.mode()[0])\nX.Wages = pd.to_numeric(X.Wages,'coerce')","ab547941":"X['AY'] = X.DateAcc.astype('datetime64').dt.year # add accident year\nX['LogInit']= np.log(X.Init+1) # add log of inital estimate\nX['AgeGroup'] = X.Age \/\/10 # add age bands\nX['Spider']= X.Desc.str.contains(\"SPIDER\").astype(int) \nX['Stress']= X.Desc.str.contains(\"STRESS\").astype(int)\n\nX['target']=  np.log(X.Ult+1) # use log as target to reduce impact of outliers\n\ncat = ['Gender','Marit','AY','AgeGroup','Spider','Stress','Init0'] \nnumeric = ['Init','LogInit','Wages','Age']","551d820e":"# source: h2o word2vec documentation \n# http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science\/word2vec.html\n\nSTOP_WORDS = [\"ax\",\"i\",\"you\",\"edu\",\"s\",\"t\",\"m\",\"subject\",\"can\",\n              \"lines\",\"re\",\"what\",\"there\",\"all\",\"we\",\"one\",\"the\",\n              \"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\"but\",\"is\",\n              \"in\",\"a\",\"not\",\"with\",\"as\",\"was\",\"if\",\"they\",\"are\",\n              \"this\",\"and\",\"it\",\"have\",\"from\",\"at\",\"my\",\"be\",\"by\",\n              \"not\",\"that\",\"to\",\"from\",\"com\",\"org\",\"like\",\"likes\",\n              \"so\"]\n\ndef tokenize(sentences, stop_word = STOP_WORDS):\n    tokenized = sentences.tokenize(\"\\\\W+\")\n    tokenized_lower = tokenized.tolower()\n    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n    return tokenized_words\n\ndef predict(desc,w2v, gbm):\n    words = tokenize(h2o.H2OFrame(desc).ascharacter())\n    desc_vec = w2v.transform(words, aggregate_method=\"AVERAGE\")\n    print(gbm.predict(test_data=desc_vec))\n\n# break claim description into a sequence of words:\nframe = h2o.H2OFrame(X[['Desc']],column_types = [\"string\"])\nwords = tokenize(frame)\nw2v_model = H2OWord2vecEstimator(sent_sample_rate = 0.0, epochs = 10)\nw2v_model.train(training_frame=words)\n\n# calculate a vector for each claim description:\ndesc_vecs = w2v_model.transform(words, aggregate_method = \"AVERAGE\")\n\n# add text vectors to features \nX  = pd.concat([X, desc_vecs.as_data_frame()], axis=1)\ntext_vec = list(desc_vecs.as_data_frame().columns)","54ff914c":"# clip numerical features in the test data to be within the range of the training data\nfor feat in numeric:\n    X.loc[X.Test==1,feat] = np.clip(X.loc[X.Test==1,feat],X.loc[X.Test==0,feat].min(),X.loc[X.Test==0,feat].max())\n    \nle = preprocessing.OrdinalEncoder() \nfor el in cat:\n    X[el]=le.fit_transform(X[el].values.reshape(-1,1).astype(str))\n    \npredictors = cat+numeric+text_vec\nXtrain = h2o.H2OFrame(X.loc[X.Test==0,predictors+['target']])\nXtrain['target']=Xtrain['target'].asnumeric()\n\naml = H2OAutoML(max_models=30, seed=55, max_runtime_secs=5000,stopping_metric='rmse',preprocessing = [\"target_encoding\"])\naml.train(x=predictors, y='target', training_frame=Xtrain)\naml.leaderboard","8c13af3e":"# review contribution from features\naml.explain(Xtrain,include_explanations=['varimp','shap_summary'])","d2d70e32":"# prediction\nh2o.remove(Xtrain)\nXtest = h2o.H2OFrame(X.loc[X.Test==1,predictors])\npreds = aml.predict(Xtest)\nytest = np.exp((preds.as_data_frame()['predict'].values))*1.3 # scale result to mean of training data","d3913e51":"submission = pd.DataFrame.from_dict({\n    'ClaimNumber': test.ClaimNumber,\n    'UltimateIncurredClaimCost': ytest})\nsubmission.to_csv(\"submission.csv\", index=False)","472afd27":"submission.describe()","fbb282cd":"## Create submission","54c12a6f":"# Actuarial loss prediction","809148cd":"## Fill missing values","e1af7972":"## Feature engineering","6b9ee788":"## Create word vectors","954da0ba":"## Fit AutoML models","4e2a1f2f":"## Import data"}}