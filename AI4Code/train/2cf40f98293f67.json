{"cell_type":{"12f97284":"code","20038886":"code","05abedda":"code","f49830bd":"code","fc8fb4d4":"code","5d787899":"code","e64319eb":"code","1a689144":"code","f390458e":"code","a7997faf":"code","4f427be2":"code","33abb8d2":"code","ca1e7183":"code","cbe962d1":"code","1481a209":"code","5d35b96b":"code","29b6a137":"code","527eb4fd":"code","729e9415":"code","14e6a99a":"code","9491548b":"code","94b0d6d1":"code","0f8f0695":"code","c07f6258":"code","3df2afe4":"code","1985b44a":"code","cdbb17e2":"code","86157c6d":"code","ccd34006":"code","d2f2c3c5":"markdown","c80cd68e":"markdown","351827eb":"markdown","7d6f690b":"markdown","9a8af236":"markdown","41539b61":"markdown","5d05771c":"markdown","2aed6385":"markdown","dae1fd0a":"markdown","13ab9a04":"markdown","6534b384":"markdown","a5199e66":"markdown","4fe7220e":"markdown","543c1163":"markdown","95288449":"markdown","91a8ecf4":"markdown","3521f0b7":"markdown"},"source":{"12f97284":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy.stats as st\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\n\npd.set_option('display.max_rows', 10)\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","20038886":"wnvData = pd.read_csv(\"..\/input\/west-nile-virus-wnv-mosquito-test-results.csv\")","05abedda":"wnvData.dtypes","f49830bd":"wnvData.head()","fc8fb4d4":"wnvData.shape","5d787899":"wnvData.describe()","e64319eb":"wnvData.info()","1a689144":"wnvData.isnull().values.any()\n","f390458e":"wnvData.isnull().sum()","a7997faf":"wnvData.columns.values","4f427be2":"s = (wnvData.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","33abb8d2":"\n\nwnvData.drop( wnvData.columns [[3,12, 4, 6]], axis=1, inplace=True)\n\n#wnvData.drop( wnvData.columns [\"BLOCK, \"LOCATION\", 'TRAP', 'TEST DATE'], axis=1, inplace=True)","ca1e7183":"s = (wnvData.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","cbe962d1":"wnvData.head()","1481a209":"from sklearn import preprocessing \nle = preprocessing.LabelEncoder()\n\nle.fit(wnvData['RESULT'])\nwnvData['RESULT'] = le.transform(wnvData['RESULT'])\n\nle.fit(wnvData['TRAP_TYPE'])\nwnvData['TRAP_TYPE'] = le.transform(wnvData['TRAP_TYPE'])\n\nle.fit(wnvData['SPECIES'])\nwnvData['SPECIES'] = le.transform(wnvData['SPECIES'])\n","5d35b96b":"def plot_corr (wnvData, size =14):\n    corr =wnvData.corr()\n    fig, ax= plt.subplots(figsize =(size,size))\n    ax.matshow(corr)\n    plt.xticks(range(len(corr.columns)), corr.columns)     #draw x tick marks\n    plt.yticks(range(len(corr.columns)), corr.columns) ","29b6a137":"plot_corr(wnvData)","527eb4fd":"wnvData.drop( wnvData.columns [[2]], axis=1, inplace=True)","729e9415":"plot_corr(wnvData)","14e6a99a":"wnvData['RESULT'].value_counts()","9491548b":"feature_col_name = [ 'SEASON YEAR', 'WEEK', 'SPECIES', 'TRAP_TYPE',\n       'NUMBER OF MOSQUITOES', 'Wards', 'Census Tracts', \n       'Community Areas', 'Historical Wards 2003-2015']\npredicted_class_name= [ 'RESULT']\n\nX = wnvData[feature_col_name].values\ny = wnvData[predicted_class_name].values\nsplit_test_size = 0.1\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=split_test_size, random_state=42)","94b0d6d1":"from sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer()\nX_train = my_imputer.fit_transform(X_train)\nX_test = my_imputer.transform(X_test)","0f8f0695":"# Writing a function to automate fitting the classifiers and evaluate the algorithms\n\ndef classifier(model,train_independent,train_dependent,test_independent,true):\n    model.fit(train_independent,train_dependent)\n    prediction = model.predict(X_test)\n    print(classification_report(true,prediction))\n    \n    # Confusion Matrix plot\n    \n    cm = confusion_matrix(y_test,prediction)\n    fig= plot_confusion_matrix(conf_mat=cm,figsize=(4,4),cmap=plt.cm.Reds,hide_spines=True)\n    plt.title('Confusion Matrix',fontsize=14)\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Actual Values')\n    plt.grid('off')\n    plt.show()\n\n\n    # 10-fold Cross Validation\n    accuracies = cross_val_score(estimator= model,X= X_train,y=y_train,cv=10)\n    print(\"The average model accuracy score is : %s\" % \"{0:.2%}\".format(accuracies.mean()))\n    print(\"The average accuracy score standard deviation is : %s\" % \"{0:.3%}\".format(accuracies.std()))\n    \n    # Values of the ROC Curve as a probabilistic approach to classification\n    roc_predict = model.predict_proba(X_test)\n    roc_predict = [p[1] for p in roc_predict]\n    area = roc_auc_score(y_test,roc_predict)\n    float(area)\n    print (\"The area under the Reciver Operating Characteristic curve is: \", (round(area,2)))","c07f6258":"\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nclassifier(log_reg,X_train,y_train,X_test,y_test)","3df2afe4":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\nclassifier (dtree,X_train,y_train,X_test,y_test)","1985b44a":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=50)\nclassifier(rfc,X_train,y_train,X_test,y_test)","cdbb17e2":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=20)\nclassifier(knn, X_train,y_train,X_test,y_test)","86157c6d":"\n# Importing the libraries for the XGBoost algorithm\n\nfrom xgboost import XGBClassifier\nxgb_classifier = XGBClassifier()\nclassifier(xgb_classifier,X_train,y_train,X_test,y_test)","ccd34006":"\nModel_Scores ={'Logistic Regression':{'10 Fold Cross Validation Score':\n                                      \"{0:.2%}\".format((cross_val_score(estimator= log_reg,X= X_train,y=y_train,cv=10)).mean()), \n                                      'Standard Deviation':\"{0:.2%}\".format((cross_val_score(estimator= log_reg,X= X_train,y=y_train,cv=10)).std())},\n                'Decision Trees':{'10 Fold Cross Validation Score':\"{0:.2%}\".format((cross_val_score(estimator= dtree,X= X_train,y=y_train,cv=10)).mean()),\n                                     'Standard Deviation':\"{0:.2%}\".format(((cross_val_score(estimator= dtree,X= X_train,y=y_train,cv=10)).std()))},\n               'Random Forest':{'10 Fold Cross Validation Score':\"74.86%\", \n                                 'Standard Deviation':\"5.85%\"},\n               'K Nearest Neighbors':{'10 Fold Cross Validation Score':\"{0:.2%}\".format((cross_val_score(estimator= knn,X= X_train,y=y_train,cv=10)).mean()), \n                                       'Standard Deviation':\"{0:.2%}\".format(((cross_val_score(estimator= knn,X= X_train,y=y_train,cv=10)).std()))},\n               'XGBoost':{'10 Fold Cross Validation Score':\"{0:.2%}\".format((cross_val_score(estimator= xgb_classifier,X= X_train,y=y_train,cv=10)).mean()), \n                           'Standard Deviation':\"{0:.2%}\".format(((cross_val_score(estimator= xgb_classifier,X= X_train,y=y_train,cv=10)).std()))}\n              }\nModel_Scores= pd.DataFrame(Model_Scores)\nModel_Scores","d2f2c3c5":"Exploring the datatypes of the dataframe variables","c80cd68e":"Now we know we do, let's see where they are and how many are present","351827eb":"## We will start the process by introducing LogisticRegression algorithm","7d6f690b":"## .... Finally we will use XGBooster","9a8af236":"## How about Decision Tree algorithm?","41539b61":"## Hello everyone\n#### Thank you for viewing this kernel. With the use of various data preprocessing techniques as well as various machine learning algorithms, I was able to build various models in a bid to predict mosquitoes test results (negative or positive). The dataset used is \"Chicago West Nile Virus Mosquito Test Results\" maintained by kaggle.com. Have a look and share your thoughts on this. Thank you again!","5d05771c":"Let's fetch all the columns with object datatypes as well as drop the irrelvant columns in this mix","2aed6385":"Fetching the data using pandas ","dae1fd0a":"Let's handle the missing values in the train and test data set by using SimpleImputer function. You can learn more by visiting this link https:\/\/www.kaggle.com\/alexisbcook\/missing-values","13ab9a04":"## Let's make an attempt to cross validate our models","6534b384":"## KNeighbors were invited!","a5199e66":"## ..... Random Forest Please!","4fe7220e":"With the use of LabelEncoder, we can transform all the object datatype into a categorical variables says Result from Negative to 0 and Positive to 1","543c1163":"## Conclusion\n\n\nIt looks like most of these algorithms performed performed similarly when comparing the 10 fold cross validation accuracy scores, with the exceptions being the Decision Trees and Random forest algorithms achieving 80.03% and 74.86% accuracy score respectively which is lower then the rest of the algortims. Other techniques will be used to see if the parameters for some of these algorithms can be improved.\n","95288449":"With the use of heatmap, let's inspect the correlation between all the variables","91a8ecf4":"From the heatmap, there is a high correlation between SeasonYear and Test ID. we will drop Test ID","3521f0b7":"Let's see if there are missing values in the dataset"}}