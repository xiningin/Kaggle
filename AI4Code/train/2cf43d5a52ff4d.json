{"cell_type":{"e6ebcff1":"code","fade6a00":"code","c689ffb7":"code","ee5bf721":"code","114d5a60":"code","ac742b46":"code","580d9214":"code","0809501b":"code","afbeab6c":"code","ccee5df0":"code","f7f214c4":"code","2be80a84":"code","5548b994":"code","c383a24a":"code","3ab664e4":"code","588f142b":"code","d33399e1":"code","db240f4f":"code","79c2d9a0":"code","3c6131de":"code","94c91ff5":"code","a1dc7a59":"code","e816a119":"code","4d18c85f":"code","ec5172a7":"code","c041a1fc":"code","06f08400":"code","a3efe3fc":"code","30d4fea2":"code","9f346853":"code","000f3606":"code","0223bc63":"markdown","03ab60b9":"markdown","80f03978":"markdown"},"source":{"e6ebcff1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import roc_auc_score, classification_report\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import ParameterGrid\nimport lightgbm as lgb\n\nfrom typing import List\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","fade6a00":"train = pd.read_csv('\/kaggle\/input\/kaggledatafest\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/kaggledatafest\/test.csv')\ntrain.shape, test.shape","c689ffb7":"train=train.drop([ 'Business and Communication',\n 'Healthcare Recreational',\n 'Nearby Locations and Other Facilities',\n 'Other Facilities'],axis=1)\n\ntrain=train.drop([  'Title','Label'],axis=1)\n\ntest=test.drop([ 'Business and Communication',\n 'Healthcare Recreational',\n 'Nearby Locations and Other Facilities',\n 'Other Facilities'],axis=1)\n\ntest=test.drop([  'Title','Label'],axis=1)","ee5bf721":"def Area_func(Area):\n    \n    if (Area.split(' ')[1].strip() =='Kanal' ):\n        res = float(Area.split(' ')[0]) * 5445\n    else:\n        res = float(Area.split(' ')[0]) * 272.251\n        \n    return res\n\ntest = test.replace('1,000 Kanal','1000 Kanal')\ntrain['Area_sq_ft']=train.Area.apply(Area_func)\ntest['Area_sq_ft']=test.Area.apply(Area_func)\n\ntrain['State']= train.Location.str.rsplit(',').str[-1]\ntest['State']= test.Location.str.rsplit(',').str[-1]\n\ntrain['City'] = train.Location.str.rsplit(',').str[-2]\ntest['City'] = test.Location.str.rsplit(',').str[-2]\n\ntrain['Town'] =  train.Location.str.rsplit(',').str[-3]\ntest['Town'] =  test.Location.str.rsplit(',').str[-3]\n\none_hot = pd.get_dummies(train.Popular)\none_hot.columns =['Popularity_Hot','Popularity_SuperHot']\ntrain = train.join(one_hot)\ntest = test.join(one_hot)","114d5a60":"def get_bedrooms(Rooms):\n    \n    try:\n        return Rooms.split('Bedrooms')[1].split(',')[0].split(':')[1].strip()\n    except:\n        return np.nan\n\ndef get_bathrooms(Rooms):\n    \n    try:\n        return Rooms.split('Bathrooms')[1].split(',')[0].split(':')[1].strip()\n    except:\n        return np.nan\n    \ndef get_serv_quart(Rooms):\n    \n    try:\n        return Rooms.split('Servant Quarters')[1].split(',')[0].split(':')[1].strip()\n    except:\n        return np.nan\n   \n    \ndef get_dining(Rooms):\n   \n    if(len(Rooms.strip()) == 0):\n        return np.nan\n    else:\n        return Rooms.find('Dining Room') != -1\n   \n        \n    \ndef get_drawing(Rooms):\n    \n     if(len(Rooms.strip()) == 0):\n        return np.nan\n     else:\n        return Rooms.find('Drawing Room') != -1\n    \ndef get_kitchen(Rooms):\n    \n    try:\n        return Rooms.split('Kitchens')[1].split(',')[0].split(':')[1].strip()\n    except:\n        return np.nan\n    \n    \ndef get_build_yr(mf):\n    \n    try:\n        return mf.split(',')[0].split(':')[1]\n    except:\n        return np.nan\n","ac742b46":"train['Serv_Quarters'] = train.Rooms.apply(get_serv_quart)\ntrain['Kitchens'] = train.Rooms.apply(get_kitchen)\ntrain['has_Dining'] = train.Rooms.apply(get_dining)\ntrain['has_Drawing'] = train.Rooms.apply(get_drawing)\ntrain['year_build'] = train['Main Features'].apply(get_build_yr)\n\ntest['Serv_Quarters'] = test.Rooms.apply(get_serv_quart)\ntest['Kitchens'] = test.Rooms.apply(get_kitchen)\ntest['has_Dining'] = test.Rooms.apply(get_dining)\ntest['has_Drawing'] = test.Rooms.apply(get_drawing)\ntest['year_build'] = test['Main Features'].apply(get_build_yr)\n\ntrain['Age'] = float(train['Time Stamp'][0].split(' ')[0].rsplit('\/')[-1])- train['year_build'].astype(float)\ntest['Age'] = float(test['Time Stamp'][0].split(' ')[0].rsplit('\/')[-1])- test['year_build'].astype(float)\n\ntrain.drop('Location',axis=1,inplace=True)\ntrain.drop('Area',axis=1,inplace=True)\ntrain.drop('Description',axis=1,inplace=True)\n\ntest.drop('Location',axis=1,inplace=True)\ntest.drop('Area',axis=1,inplace=True)\ntest.drop('Description',axis=1,inplace=True)\n\none_hot = pd.get_dummies(train.State)\ntrain = train.join(one_hot)\ntest = test.join(one_hot)\n\none_hot = pd.get_dummies(train.City)\ntrain = train.join(one_hot)\ntest = test.join(one_hot)\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nord_enc = OrdinalEncoder()\ntrain[\"Town\"] = ord_enc.fit_transform(train[[\"Town\"]])\ntest[\"Town\"] = ord_enc.fit_transform(test[[\"Town\"]])\ntrain[\"Type\"] = ord_enc.fit_transform(train[[\"Type\"]])\ntest[\"Type\"] = ord_enc.fit_transform(test[[\"Type\"]])\ntrain[\"Price_Category\"] = ord_enc.fit_transform(train[[\"Price_Category\"]])\n\ntrain = train.replace(True,1)\ntrain = train.replace(False,1)\ntest = test.replace(True,1)\ntest = test.replace(False,1)\n","580d9214":"train['Baths'] = train['Baths'].replace('-',np.NaN)\ntrain['Bedroom(s)'] = train['Bedroom(s)'].replace('-',np.NaN)\ntrain['Serv_Quarters'] = train['Serv_Quarters'].replace('-',np.NaN)\ntrain['Kitchens'] = train['Kitchens'].replace('-',np.NaN)\ntest['Baths'] = test['Baths'].replace('-',np.NaN)\ntest['Bedroom(s)'] = test['Bedroom(s)'].replace('-',np.NaN)\ntest['Serv_Quarters'] = test['Serv_Quarters'].replace('-',np.NaN)\ntest['Kitchens'] = test['Kitchens'].replace('-',np.NaN)\n\ntrain = train.drop(columns = ['Purpose','Main Features','Rooms','Popular','Time Stamp','State','City','Town',\n                              'year_build','Property_Id','Source','Type',' Islamabad',' Sindh',' Lahore'])\ntest = test.drop(columns = ['Purpose','Main Features','Rooms','Popular','Time Stamp','State','City','Town','year_build',\n                           'Property_Id','Source','Type',' Islamabad',' Sindh',' Lahore'])\n\nfrom sklearn.impute import KNNImputer\n\nimputer = KNNImputer(n_neighbors = 1)\ncolumns = train.columns\ntrain = pd.DataFrame(imputer.fit_transform(train), columns = columns)\ncolumns = test.columns\ntest = pd.DataFrame(imputer.fit_transform(test), columns = columns)\n","0809501b":"train['Baths'] = train['Baths'].astype(np.float64)\ntrain['Bedroom(s)'] = train['Bedroom(s)'].astype(np.float64) \ntrain['Serv_Quarters'] = train['Serv_Quarters'].astype(np.float64)\ntrain['Kitchens'] = train['Kitchens'].astype(np.float64)\n\ntest['Baths'] = test['Baths'].astype(np.float64)\ntest['Bedroom(s)'] = test['Bedroom(s)'].astype(np.float64) \ntest['Serv_Quarters'] = test['Serv_Quarters'].astype(np.float64)\ntest['Kitchens'] = test['Kitchens'].astype(np.float64)","afbeab6c":"import seaborn as sns\n\ncorr = train.corr()\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nf, ax = plt.subplots(figsize=(11, 9))\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","ccee5df0":"corr","f7f214c4":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\ny_train = train['Price_Category']\nX_train = train.drop(columns = ['Price_Category','has_Drawing'])\n#test = test.drop(columns = ['has_Drawing'])\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n\nfrom sklearn.naive_bayes import GaussianNB \ngnb = GaussianNB().fit(X_train, y_train) \ngnb_predictions = gnb.predict(X_test) \n  \n# accuracy on X_test \naccuracy = gnb.score(X_test, y_test) \naccuracy\n  \n# creating a confusion matrix \ncm = confusion_matrix(y_test, gnb_predictions) ","2be80a84":"cm","5548b994":"from sklearn.neighbors import KNeighborsClassifier \nknn = KNeighborsClassifier(n_neighbors = 7).fit(X_train, y_train) \n  \n# accuracy on X_test \naccuracy = knn.score(X_test, y_test) \n\n  \n# creating a confusion matrix \nknn_predictions = knn.predict(X_test)  \ncm = confusion_matrix(y_test, knn_predictions) \n\naccuracy","c383a24a":"cm","3ab664e4":"from lightgbm import LGBMClassifier\ny_train = train['Price_Category']\nX_train = train.drop(columns = ['Price_Category','has_Drawing'])\ncategorical_features = ['has_Dining',' Islamabad Capital',' Punjab',' Karachi',\n                       'Popularity_Hot','Popularity_SuperHot']\nlgtrain = lgb.Dataset(X_train,y_train, categorical_feature=categorical_features)\n\nlgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'multiclass',\n    'num_class': 7,\n    'metric': ['multi_error'],\n    \"learning_rate\": 0.03,\n     \"num_leaves\": 60,\n     \"max_depth\": 9,\n     \"feature_fraction\": 0.45,\n     \"bagging_fraction\": 0.3,\n     \"reg_alpha\": 0.15,\n     \"reg_lambda\": 0.15,\n      \"min_child_weight\": 0\n                }\n\nlgb_cv = lgb.cv(\n    params = lgbm_params,\n    train_set = lgtrain,\n    num_boost_round=2000,\n    stratified=True,\n    nfold = 5,\n    verbose_eval=50,\n    seed = 23,\n    early_stopping_rounds=75)\n\nloss = lgbm_params[\"metric\"][0]\noptimal_rounds = np.argmin(lgb_cv[str(loss) + '-mean'])\nbest_cv_score = min(lgb_cv[str(loss) + '-mean'])\n\nprint(\"\\nOptimal Round: {}\\nOptimal Score: {} + {}\".format(\n    optimal_rounds,best_cv_score,lgb_cv[str(loss) + '-stdv'][optimal_rounds]))\n\nresults = pd.DataFrame(columns = [\"Rounds\",\"Score\",\"STDV\", \"LB\", \"Parameters\"])\nresults = results.append({\"Rounds\": optimal_rounds,\n                          \"Score\": best_cv_score,\n                          \"STDV\": lgb_cv[str(loss) + '-stdv'][optimal_rounds],\n                          \"LB\": None,\n                          \"Parameters\": lgbm_params}, ignore_index=True)\n","588f142b":"results","d33399e1":"tune_parameter = [0.08,0.01]\ntune = 'learning_rate'\nfor param in tune_parameter:\n    print(\"{} Parameter: {}\".format(tune, param))\n    lgbm_params[tune] = param\n    # Find Optimal Parameters \/ Boosting Rounds\n    lgb_cv = lgb.cv(\n        params = lgbm_params,\n        train_set = lgtrain,\n        num_boost_round=1000,\n        stratified=True,\n        nfold = 50,\n        verbose_eval=50,\n        seed = 281990,\n        early_stopping_rounds=100)\n\n    optimal_rounds = np.argmin(lgb_cv[str(loss) + '-mean'])\n    best_cv_score = min(lgb_cv[str(loss) + '-mean'])\n\n    print(\"Optimal Round: {}\\nOptimal Score: {} + {}\".format(\n        optimal_rounds,best_cv_score,lgb_cv[str(loss) + '-stdv'][optimal_rounds]))\n    print(\"###########################################################################################\")\n\n    results = results.append({\"Rounds\": optimal_rounds,\n                              \"Score\": best_cv_score,\n                              \"STDV\": lgb_cv[str(loss) + '-stdv'][optimal_rounds],\n                              \"LB\": None,\n                              \"Parameters\": lgbm_params}, ignore_index=True)\n    \n\nresults","db240f4f":"results","79c2d9a0":"model = LGBMClassifier(**final_model_params)\nmodel.fit(X_train, y_train, categorical_feature=categorical_features, eval_metric=['multi_error'])\nprediction = model.predict(test)","3c6131de":"from sklearn.metrics import mean_squared_error,roc_auc_score,precision_score\ny_pred_1 = [np.argmax(line) for line in y_pred_1]\ny_pred_1\nprecision_score(y_pred_1,y_test,average=None).mean()","94c91ff5":"#Best Parameters\nfinal_model_params = results.iloc[results[\"Score\"].idxmin(),:][\"Parameters\"]\noptimal_rounds = results.iloc[results[\"Score\"].idxmin(),:][\"Rounds\"]\nprint(\"Parameters for Final Models:\\n\",final_model_params)\nprint(\"Score: {} +\/- {}\".format(results.iloc[results[\"Score\"].idxmin(),:][\"Score\"],results.iloc[results[\"Score\"].idxmin(),:][\"STDV\"]))\nprint(\"Rounds: \", optimal_rounds)","a1dc7a59":"model = LGBMClassifier(**final_model_params)\nmodel.fit(X_train, y_train, categorical_feature=categorical_features, eval_metric=['multi_error'])\nprediction = model.predict(test)","e816a119":"from sklearn.metrics import accuracy_score\nfrom sklearn import metrics\n#print(\"Accuracy: {:.3f}\".format(accuracy_score(test, prediction)))\n#print(\"Confusion matrix:\\n{}\".format(confusion_matrix(test, prediction)))\nprint(metrics.confusion_matrix(test, prediction))\n","4d18c85f":"submission = pd.read_csv('\/kaggle\/input\/kaggledatafest\/sample_submission.csv')\nsubmission['Price_Category'] = prediction","ec5172a7":"\nsubmission['Price_Category'] = submission['Price_Category'].replace(3.0,'Semi-Premium')\nsubmission['Price_Category'] = submission['Price_Category'].replace(2.0,'Premium')\nsubmission['Price_Category'] = submission['Price_Category'].replace(0.0,'Affordable')\nsubmission['Price_Category'] = submission['Price_Category'].replace(1.0,'Cheap')\nsubmission.head()","c041a1fc":"submission","06f08400":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder \n\nlc = LabelEncoder() \nlc = lc.fit(y_train) \nlc_y = lc.transform(y_train)\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.20, random_state=5)\nmodel = XGBClassifier() \nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test) \npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions) \nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","a3efe3fc":"prediction_2 = model.predict(test)\nsubmission['Price_Category'] = prediction_2\nsubmission['Price_Category'] = submission['Price_Category'].replace(3.0,'Semi-Premium')\nsubmission['Price_Category'] = submission['Price_Category'].replace(2.0,'Premium')\nsubmission['Price_Category'] = submission['Price_Category'].replace(0.0,'Affordable')\nsubmission['Price_Category'] = submission['Price_Category'].replace(1.0,'Cheap')\nsubmission.head()","30d4fea2":"submission['Price_Category'].value_counts()","9f346853":"submission.to_csv('submission_3.csv')","000f3606":"from sklearn.metrics import accuracy_score\nX_train, X_test, y_train, y_test = train_test_split(\n digits.data, digits.target, random_state=0)\nlr = LogisticRegression().fit(X_train, y_train)\npred = lr.predict(X_test)\nprint(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, pred)))\nprint(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, pred)))\n","0223bc63":"# NAIVE BAYES","03ab60b9":"# Support Vector Machine","80f03978":"# K NEAREST NEIGHBORS"}}