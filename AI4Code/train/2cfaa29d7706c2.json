{"cell_type":{"da28d215":"code","c2266bb4":"code","e4243864":"code","489c156f":"code","ad9d9244":"code","d68d4f4e":"code","a350c549":"code","b0994770":"code","5f9a651e":"code","eac52073":"code","45bf3a70":"code","9aa67ce9":"code","0c144e0f":"code","f52ca18e":"code","ba94c5a0":"code","46cc9eaf":"code","ac572618":"code","bf6164b7":"code","d4bbd3fb":"code","76ec4d0a":"code","a356e6c9":"code","1017152b":"code","bcf4e8bd":"markdown","2b513528":"markdown","17491c36":"markdown"},"source":{"da28d215":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021\/train.csv\", keep_default_na=True)\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021\/test.csv\", keep_default_na=True)\n\ntrain.info()\ntest.info() \n    \ntrain_len = train.shape[0]\nsubmit = test[\"date_time\"].to_frame()\n","c2266bb4":"def plot_days(days, feature):\n    for d in days:\n        train.iloc[24 * d:24 * (d + 1)].plot(x = 'date_time', y = feature, figsize = (12, 4))\n\n#shapes are broadly similar from day to day\n#but values can vary significantly, including between the same days of the week\n    \nplot_days([0, 1, 77, 78, 231, 232], 'target_carbon_monoxide')","e4243864":"all_data = pd.concat([train, test])","489c156f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#sensor_4 is poorly correlated with target_nitrogen_oxides\n\ndef corrplot():\n    sns.set(font_scale=1.1)\n    correlation_train = all_data.iloc[:train_len].corr('spearman')\n    mask = np.triu(correlation_train)\n    plt.figure(figsize=(20, 20))\n    sns.heatmap(correlation_train,\n                annot=True,\n                fmt='.1f',\n                cmap='coolwarm',\n                square=True,\n                mask=mask,\n                linewidths=1,\n                cbar=False)\n\n    plt.show()\n    \ncorrplot()","ad9d9244":"train = all_data.iloc[:train_len]\n\nplt.scatter(train['sensor_1'], train['sensor_4'])\n\n#note the group of outliers in this and other scatterplots","d68d4f4e":"bad_indices = all_data['sensor_1'].between(1000, 1210)\nbad_indices &= all_data['sensor_4'].between(500, 700)\n\nprint(bad_indices.sum())  \n\ntrain_bad_indices = bad_indices.iloc[:train_len]\ntest_bad_indices = bad_indices.iloc[train_len:]\n\nprint(train_bad_indices.sum())  #228\n\nbad_indices_list = []\nfor i, v in bad_indices.items():\n    if v:\n        bad_indices_list.append(i)\n        \n\ntrain_bad_indices_list = bad_indices_list[:train_bad_indices.sum()]\ntest_bad_indices_list = bad_indices_list[train_bad_indices.sum():]\n\nprint(train_bad_indices_list)\nprint(test_bad_indices_list)","a350c549":"#a reasonable interpretation is that these bad indices are when the sensors are not\n#functioning properly, due to e.g bad weather\n\n#I'll train different models for the 'bad' data and the good data\n\ntrain_bad = train.loc[train_bad_indices]\n\ntrain_bad.info()\n\nfig, axes = plt.subplots(5, 3, figsize=(15, 15))\n\nfor i in range(5):\n    axes[i, 0].scatter(train['sensor_' + str(i + 1)], train[\"target_carbon_monoxide\"])\n    axes[i, 1].scatter(train['sensor_' + str(i + 1)], train[\"target_benzene\"])\n    axes[i, 2].scatter(train['sensor_' + str(i + 1)], train[\"target_nitrogen_oxides\"])\n    \n    axes[i, 0].scatter(train_bad['sensor_' + str(i + 1)], train_bad[\"target_carbon_monoxide\"], color = 'red')\n    axes[i, 1].scatter(train_bad['sensor_' + str(i + 1)], train_bad[\"target_benzene\"], color = 'red')\n    axes[i, 2].scatter(train_bad['sensor_' + str(i + 1)], train_bad[\"target_nitrogen_oxides\"], color = 'red')","b0994770":"all_data['hour'] = all_data['date_time'].apply(lambda x: int(x[11:13]))\n\nall_data['date_time']=pd.to_datetime(all_data['date_time'],format='%Y-%m-%d %H:%M:%S')\n\nall_data['is_weekend'] = (all_data[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\n\n#including data from a week ago greatly improves model rmse. \n#Probably two reasons: the strong correlation of the features with the hour and \n#day of week\n#as well as the long sequences of bad indices. Data from 168 hours ago is usually\n#good data\nsensor_offsets = [1, 168]\n\nfor i in range(1, 6):\n    feature = \"sensor_\" + str(i)\n    initial_value = train.at[0, feature]\n    \n    for off in sensor_offsets:\n        offset_feature = feature + \"_p\" + str(off)\n        all_data[offset_feature] = all_data[feature].shift(periods = off, fill_value = initial_value)\n    \n\n\nall_data.info()\nall_data.head()","5f9a651e":"train_by_hour = all_data.iloc[:train_len].groupby(\"hour\")\n\nprint(train_by_hour.median())\n\n#all_data.iloc[:train_len].groupby(\"hour\").plot(x = \"hour\", y = \"target_carbon_monoxide\", kind = \"box\")","eac52073":"#target values are generally lowest at 4 am, with a bimodal distribution\n#that peaks at 8 am and 7 pm, which are commute hours. Representing 4 am as 0\n#in the hour column may make the dataset more amenable to tree classification\n\n#working hours start from ~7 am. Representing 7 am as 0 may also be effective.\n#this seems to outperform introducing an is_working_hour feature.\n\nprint(all_data.head())\n\nall_data.hour = all_data.hour.apply(lambda x: (x + 17) % 24)\n\nall_data.head()","45bf3a70":"train = all_data.iloc[:train_len]\ntest = all_data.iloc[train_len:]","9aa67ce9":"targets = [\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"]\n\ny_all = np.log1p(train[targets])\n\n#naive approach to predicting benzene values at bad indices\n#makes awful predictions\n#bad_benzene_pred = []\n\n#for i in test_bad_indices_list:\n#    hour = test.at[i, \"hour\"]\n#    day = test.at[i, \"date_time\"].dayofweek\n#    pred = train[(train[\"hour\"] == hour) & (train[\"date_time\"].dt.dayofweek == day)][\"target_benzene\"].median()\n#    bad_benzene_pred.append(pred)\n    \n#print(bad_benzene_pred)\n    \ntrain.drop(targets, axis = 1, inplace = True)\ntrain.drop(\"date_time\", axis = 1, inplace = True)\ntest.drop(\"date_time\", axis = 1, inplace = True)\n\n\nprint(train_len)\ny_all.info()\n\ncorrplot()","0c144e0f":"'''\ndef last_good_reading_at_this_hour(index):\n    for i in range(index, 0, -24):\n        if i not in test_bad_indices_list:\n            return i\n    \nprint(test.iloc[1666:1669])    \n    \nimputed = []    \nlgr = []\n\nfor i in test_bad_indices_list:\n    if (i + 1) not in test_bad_indices_list and (i-1) not in test_bad_indices_list:\n        imputed.append(\"Neighbour mean\")\n        test.iloc[i] = (test.iloc[i + 1] + test.iloc[i - 1])\/2\n            \n    else:\n        imputed.append(\"Lgr\")\n        test.iloc[i] = test.iloc[last_good_reading_at_this_hour(i)]\n        lgr.append(last_good_reading_at_this_hour(i))\n            \n            \nprint(lgr)\nprint(len(lgr))\n'''\ntest.drop(targets, axis = 1, inplace = True)","f52ca18e":"offset = 0\nvalid_frac = 5  #every 5th entry (skipping bad indices) will be reserved for validation.\n                #in part, 5 is chosen to be coprime to 7 (number of days of the week)\n                #and 24 (number of hours in a day)\n        \nvalid_indices = [i for i in range(offset, train_len, valid_frac) \n                 if i not in train_bad_indices_list]\n\ntrain_indices = [i for i in range(train_len)\n                 if i not in valid_indices\n                 if i not in train_bad_indices_list]\n\nX_valid = train.iloc[valid_indices]\nX_train = train.iloc[train_indices]\n\ny_valid = y_all.iloc[valid_indices]\ny_train = y_all.iloc[train_indices]\n","ba94c5a0":"import xgboost as xgb\n\n\ndef cv_xgb(dmatrix):\n\n    models = []\n    for c in [1]:\n        for s in [0.7, 0.8, 0.9]:\n            models.append(xgb.XGBRegressor(colsample_bytree = 1, subsample = s, \n                                 max_depth=6, min_child_weight=0.8, \n                                 eta = 0.2, n_estimators=5000,\n                                 reg_alpha = 0.6, reg_lambda = 1.2, \n                                 gamma=0))\n\n    for m in models:        \n        results = []\n        seeds = [11, 22, 33]\n        for seed in seeds:\n            cvresult = xgb.cv(m.get_xgb_params(), dmatrix, num_boost_round=m.get_params()['n_estimators'],\n                   nfold=5, metrics='rmse', early_stopping_rounds = 50, seed = seed)\n            results.append(cvresult.iloc[-1])\n        \n        average = results[0]\n        for r in results[1:]:\n            average += r\n        print(\"Averages for subsample =\", m.get_xgb_params()[\"subsample\"], \"reg_lambda =\", m.get_xgb_params()[\"reg_lambda\"], \":\")\n        print(average\/ len(results))\n\n#carbon_monoxide_dmatrix = xgb.DMatrix(data=train,label=y_carbon_monoxide)\n\n#cv_xgb(carbon_monoxide_dmatrix)\n\n#with few features, colsample_bytree = 1 makes sense\n\n#consider eta 0.01 for final model \n#carbon_monoxide_model = xgb.XGBRegressor(colsample_bytree= 1, subsample = 0.8, \n#                                 max_depth=4, min_child_weight=1.7, \n#                                 eta = 0.02, n_estimators=2000,\n#                                 reg_alpha=0.3, reg_lambda = 1.2, \n#                                 gamma=0)\n\n#benzene_dmatrix = xgb.DMatrix(data=train,label=y_benzene)\n\n#cv_xgb(benzene_dmatrix)\n\n#benzene_model = xgb.XGBRegressor(colsample_bytree=1, subsample = 0.8, \n#                                 max_depth=3, min_child_weight=0.8, \n#                                 eta = 0.02, n_estimators=5000,\n#                                 reg_alpha=0.6, reg_lambda = 1.2, \n#                                 gamma=0)\n\n#nitrogen_oxides_dmatrix = xgb.DMatrix(data=train,label=y_nitrogen_oxides)\n\n#cv_xgb(nitrogen_oxides_dmatrix)\n\n#nitrogen_oxides_model = xgb.XGBRegressor(colsample_bytree=1, subsample = 0.9, \n#                                 max_depth=6, min_child_weight=0.8, \n#                                 eta = 0.02, n_estimators=5000,\n#                                 reg_alpha=0.6, reg_lambda = 1.2, \n#                                 gamma=0)\n\n#carbon_monoxide_model.fit(train, y_carbon_monoxide)\n#submit['target_carbon_monoxide'] = np.expm1(carbon_monoxide_model.predict(test))\n\n#benzene_model.fit(train, y_benzene)\n#submit['target_benzene'] = np.expm1(benzene_model.predict(test))\n\n#nitrogen_oxides_model.fit(train, y_nitrogen_oxides)\n#submit['target_nitrogen_oxides'] = np.expm1(nitrogen_oxides_model.predict(test))","46cc9eaf":"import catboost as cat\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\n\ndef cat_val(col):\n    \n    all_params = {}\n    \n    for l in [1, 1.5, 2, 2.5, 3]: \n        \n            #sensor_1 - 5 have highest correlation with targets, but giving them\n            #higher weights doesn't improve model rmse\n            #feature_weights = { 'sensor_1': w,\n            #                    'sensor_2': w,\n            #                    'sensor_3': w,\n            #                    'sensor_4': w,\n            #                    'sensor_5': w,\n            #            }\n\n            params = { 'depth': 9,\n                       'learning_rate': 0.05,\n                       'random_strength': 1.4,\n                       'l2_leaf_reg': l,\n                       'grow_policy': 'SymmetricTree',\n                       'eval_metric': 'RMSE',\n                       #'monotone_constraints': {'sensor_2': 1}  #this makes the model perform worse for benzene\n                                                                 #despite perfect monotone corr with the good data\n                }\n\n            model = cat.CatBoostRegressor(**params)\n            model.fit(X_train, y_train[col], verbose = False, use_best_model = True,\n                     eval_set = (X_valid, y_valid[col]))\n\n            all_params[col] = model.get_all_params()\n            print(col, mean_squared_error(y_valid[col], model.predict(X_valid)))\n            \n\n#cat_val('target_carbon_monoxide')\n#cat_val('target_benzene')\n#cat_val('target_nitrogen_oxides')\n#\n#print(all_params)","ac572618":"<a id=\"final_models\"><\/a>\n# Final models","bf6164b7":"carbon_monoxide_model = cat.CatBoostRegressor(depth = 9, learning_rate = 0.05, random_strength = 1.6, l2_leaf_reg = 2, verbose = False)\ncarbon_monoxide_model.fit(train.loc[~train_bad_indices], y_all['target_carbon_monoxide'].loc[~train_bad_indices])\n\nbenzene_model = cat.CatBoostRegressor(depth = 6, verbose = False)\nbenzene_model.fit(train.loc[~train_bad_indices], y_all['target_benzene'].loc[~train_bad_indices])\n\nnitrogen_oxides_model = cat.CatBoostRegressor(depth = 9, learning_rate = 0.05, random_strength = 1.4, l2_leaf_reg = 2,verbose = False)\nnitrogen_oxides_model.fit(train.loc[~train_bad_indices], y_all['target_nitrogen_oxides'].loc[~train_bad_indices])","d4bbd3fb":"#fitting the models for the bad data on the entire training set\n#has produced much more accurate models that fitting them on just the bad data\n\nbad_carbon_monoxide_model = cat.CatBoostRegressor(depth = 8, verbose = False)\nbad_carbon_monoxide_model.fit(train, y_all['target_carbon_monoxide'])\n\n#unsure how to deal with the bad benzene 0 values. I'm just leaving it to CatBoost here\n#maybe something like imputing from non-target values with a linear regressor could be effective\nbad_benzene_model = cat.CatBoostRegressor(depth = 6, verbose = False)\nbad_benzene_model.fit(train, y_all['target_benzene'])\n\nbad_nitrogen_oxides_model = cat.CatBoostRegressor(depth = 9, verbose = False)\nbad_nitrogen_oxides_model.fit(train, y_all['target_nitrogen_oxides'])","76ec4d0a":"def merge(good, bad, bad_indices):\n    if len(good) != bad_indices.count(False):\n        raise ValueError(\"Number of False values in bad_indices doesn't match length of good data series\")\n    if len(good) + len(bad) != len(bad_indices):\n        raise ValueError(\"Number of True values in bad_indices doesn't match length of bad data series\")\n    \n    merged = []\n    good_i = 0\n    bad_i = 0\n    for b in bad_indices:\n        if b:\n            merged.append(bad[bad_i])\n            bad_i += 1\n        else:\n            merged.append(good[good_i])\n            good_i += 1\n            \n    return pd.Series(merged)\n","a356e6c9":"carbon_monoxide_pred = np.expm1(carbon_monoxide_model.predict(test[~test_bad_indices]))\nbenzene_pred = np.expm1(benzene_model.predict(test[~test_bad_indices]))\nnitrogen_oxides_pred = np.expm1(nitrogen_oxides_model.predict(test[~test_bad_indices]))\n\nbad_carbon_monoxide_pred = np.expm1(bad_carbon_monoxide_model.predict(test[test_bad_indices]))\nbad_benzene_pred = np.expm1(bad_benzene_model.predict(test[test_bad_indices]))\nbad_nitrogen_oxides_pred = np.expm1(bad_nitrogen_oxides_model.predict(test[test_bad_indices]))","1017152b":"submit[\"target_carbon_monoxide\"] = merge(carbon_monoxide_pred, bad_carbon_monoxide_pred, test_bad_indices.tolist())\n\nsubmit[\"target_benzene\"] = merge(benzene_pred, bad_benzene_pred, test_bad_indices.tolist())\n\nsubmit[\"target_nitrogen_oxides\"] = merge(nitrogen_oxides_pred, bad_nitrogen_oxides_pred, test_bad_indices.tolist())\n    \nprint(submit)\n\nsubmit.to_csv('submission.csv', index=False)","bcf4e8bd":"# Contents\n\n[Dealing with bad indices](#bad_indices)\n\n[Validation](#validation)\n\n[Final models](#final_models)\n","2b513528":"<a id=\"bad_indices\"><\/a>\n# Dealing with bad indices","17491c36":"<a id=\"validation\"><\/a>\n# Validation"}}