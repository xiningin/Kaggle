{"cell_type":{"aa0daaef":"code","0d7216c7":"code","07fadfea":"code","d1af97a4":"code","2a3d5bae":"code","157fda54":"code","537ad7ac":"code","60b05126":"code","43c82ad1":"code","f663d3d5":"code","b67e822f":"code","399cf561":"code","fd323751":"code","faad32e5":"code","4883ea0c":"code","0bf4d8d4":"code","5d2951e5":"code","aeec9e45":"code","2e74aaea":"code","c78bc583":"code","02d5ac15":"code","c580d2fd":"code","57a7d71e":"code","dfc36857":"code","ce4776b3":"code","5b823abf":"code","96dc390b":"code","c321a4b9":"code","b86a6fef":"code","23cf7b0f":"code","3c708529":"code","f8c5f63d":"code","ffceec9d":"code","74538f7f":"code","4b4facbb":"code","65b40b78":"code","2781231e":"code","e53d1fc9":"code","3674fb4d":"code","59f9aca0":"markdown","e750ad9f":"markdown","c5757877":"markdown","5b6d5eac":"markdown","7e5a2759":"markdown","4270b5e4":"markdown","4a1de9c7":"markdown","93eed1c2":"markdown","ee0d599c":"markdown","46bcddc8":"markdown","59cf7e60":"markdown","23c3cdc9":"markdown"},"source":{"aa0daaef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy as scp # scientific data crunching tools\nimport seaborn as sns # nice visualizations \n\nfrom sklearn import linear_model\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, FunctionTransformer\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import ensemble\n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\n# Defining some constant like variables\nMODEL_NAMES = ['MLPR', 'LRG', 'SVR', 'GBR']\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nDataFilePath = ''\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        filepth = os.path.join(dirname, filename)\n        print(filepth)\n        if ('data' in filename) and ('csv' in filename):\n            DataFilePath = filepth\n            \nif len(DataFilePath) < 1:\n    print('Oh no! We have no data file! :(')\nelse:\n    print('Data File:\\n', DataFilePath)\n\n# Any results you write to the current directory are saved as output.","0d7216c7":"CandyDataSetDF = pd.read_csv(DataFilePath)\nCandyDataSetDF.head()","07fadfea":"CandyDataSetDF.describe()","d1af97a4":"sns.distplot(CandyDataSetDF['winpercent'], bins=20, color = 'blue')","2a3d5bae":"scp.stats.kstest(CandyDataSetDF['winpercent'],'norm')","157fda54":"sns.distplot(CandyDataSetDF['sugarpercent'], bins=20, color = 'red')","537ad7ac":"scp.stats.kstest(CandyDataSetDF['sugarpercent'],'norm')","60b05126":"sns.distplot(CandyDataSetDF['pricepercent'], bins=20, color = 'green')","43c82ad1":"scp.stats.kstest(CandyDataSetDF['pricepercent'],'norm')","f663d3d5":"# Define a dictionary for the Y scalers, we need them later...\n# MODEL_NAMES = ['MLPR', 'LRG', 'SVR', 'GBR']\nYScalDict = dict.fromkeys(MODEL_NAMES) # Dictionary for the scalers, later needed for inverse transform\nModelDict = dict.fromkeys(MODEL_NAMES) # Dictionary of the models for convenient multi model handling\nScoreDict = dict.fromkeys(MODEL_NAMES) # Dictionary of the model scores\n\nXindy = np.array(CandyDataSetDF.iloc[:,1:10])\nYdept = np.array(CandyDataSetDF['winpercent'])\n\n# For support vector regression and eventually gradient boosting\nSVRGBXscaler = StandardScaler()\nYScalDict['SVR'] = StandardScaler()\nYScalDict['GBR'] = FunctionTransformer(lambda x : x, validate = True) # returns idenicals, needed for later processing\n\nX4SVGB = SVRGBXscaler.fit_transform(Xindy)\nY4SVGB = YScalDict['SVR'].fit_transform(Ydept.reshape(-1, 1)).flatten()\nYI4GBR = YScalDict['GBR'].fit_transform(Ydept.reshape(-1, 1)).flatten()\nYScalDict['LRG'] = YScalDict['SVR'] #Linear regression also uses the same scaler\n\nYScalDict['MLPR'] = MinMaxScaler(feature_range=(1,100)) # Suprisingly this feature range works far better than (0,1) or (0,100)!\nMLPR_xscaler = MinMaxScaler(feature_range=(0.001,0.999)) # Making X a bit less binary\n\nXtrTmp, XteTmp, YtrTmp, YteTmp = train_test_split(Xindy[:,0:9], Ydept, test_size = 20)\n\nMLPR_xscaler.fit(Xindy[:,0:9])\n\nYScalDict['MLPR'].fit(Ydept.reshape(-1, 1))\nYNNTrain = YScalDict['MLPR'].transform(YtrTmp.reshape(-1, 1)).flatten()\nXNNTrain = MLPR_xscaler.transform(XtrTmp)\n\nYNNTest = YScalDict['MLPR'].transform(YteTmp.reshape(-1, 1)).flatten()\nXNNTest = MLPR_xscaler.transform(XteTmp)","b67e822f":"mylof = LocalOutlierFactor(contamination = 0.1) # contamination only necessary to supress the version 0.22 warning message\n\nplt.figure(figsize=(10, 4))\n\nmylof.fit(np.array(CandyDataSetDF.iloc[:,1:10])) # Resembles Xdept\nsns.lineplot(data = mylof.negative_outlier_factor_)\nmylof.fit(np.array(CandyDataSetDF.iloc[:,1:13])) # Resembles all numerical values in a line\nsns.lineplot(data = mylof.negative_outlier_factor_)","399cf561":"NNlof = LocalOutlierFactor(contamination = 0.1) #, n_neighbors = 8)\nNNlof.fit(XNNTrain) # Training values \nplt.figure(figsize=(10, 4))\nsns.lineplot(data = NNlof.negative_outlier_factor_)","fd323751":"NNlof.fit(XNNTest) # Testing values \nplt.figure(figsize=(10, 4))\nsns.lineplot(data = NNlof.negative_outlier_factor_)","faad32e5":"# Create a view of only the ratio scaled columns\nCandyRatioVar = CandyDataSetDF[['sugarpercent', 'pricepercent', 'winpercent']]\n\nCandyRatioCorr = CandyRatioVar.corr(method = 'pearson')\n\nsns.heatmap(CandyRatioCorr, vmin=0, vmax=1)\nCandyRatioCorr","4883ea0c":"ModelDict['LRG'] = linear_model.LinearRegression()\nModelDict['LRG'].fit(X4SVGB, Y4SVGB) \nScoreDict['LRG'] = ModelDict['LRG'].score(X4SVGB, Y4SVGB)","0bf4d8d4":"coef_linreg = ModelDict['LRG'].coef_\nsns.relplot(data = pd.DataFrame(data = coef_linreg, index = CandyDataSetDF.iloc[:,1:10].columns ), \n            kind = \"line\", legend = False, aspect = 3)","5d2951e5":"param_gridSVR = dict(C = list(np.linspace(0.0000001, 10, 100, endpoint = True)))\n\nModelDict['SVR'] = SVR(max_iter = -1, kernel = 'linear', gamma = 'auto')\n\ngridsvr = GridSearchCV(ModelDict['SVR'], param_gridSVR, cv = 5)\ngridsvr.fit(X4SVGB, Y4SVGB) \nprint('AND THE WINNER IS:\\n', gridsvr.best_params_)\ngridsvrDF = pd.DataFrame(gridsvr.cv_results_)\nModelDict['SVR'] = gridsvr.best_estimator_\nScoreDict['SVR'] = ModelDict['SVR'].score(X4SVGB, Y4SVGB)","aeec9e45":"coef_SVR = ModelDict['SVR'].coef_.reshape(9)\n\nmyplt = sns.relplot(data = pd.DataFrame(data = coef_SVR, index = CandyDataSetDF.iloc[:,1:10].columns),\n                    kind = \"line\", legend = False, aspect = 3)","2e74aaea":"param_fixGBR = dict(\n    #n_estimators = 3000,\n    max_depth = 2,\n    learning_rate = 0.01,\n    min_samples_leaf = 9,\n    max_features = 0.3\n)\n\n# Prepare the parameters for grid search \n# Thx2: https:\/\/www.slideshare.net\/PyData\/gradient-boosted-regression-trees-in-scikit-learn-gilles-louppe\n\nparam_gridGBR = dict(\n    n_estimators = [100, 500, 1500, 3000, 4500, 6000],\n    #max_depth = [2, 4, 6],\n    #learning_rate = [0.1, 0.05, 0.02, 0.01],\n    #min_samples_leaf = [3, 5, 9],\n    #max_features = [1.0, 0.3, 0.1],\n)","c78bc583":"# Thx2: https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py\n# Worked:\n# mygradboostregr = ensemble.GradientBoostingRegressor(n_estimators = 500, max_depth = 4, min_samples_split = 2, learning_rate = 0.01, loss = 'ls')\n\nModelDict['GBR'] = ensemble.GradientBoostingRegressor(**param_fixGBR)\n\ngridGBR = GridSearchCV(ModelDict['GBR'], param_gridGBR, verbose = True, cv = 5) # Five folds\n#gridGBR.fit(X4SVGB, Y4SVGB) \ngridGBR.fit(Xindy, Ydept) # Gradient boosting regression is very robust against scaling effects\n\nModelDict['GBR'] = gridGBR.best_estimator_\nScoreDict['GBR'] = ModelDict['GBR'].score(Xindy, Ydept) # Attention! Must be same parameters as in: gridGBR.fit above\ngridGBRDF = pd.DataFrame(gridGBR.cv_results_)\nprint('AND THE WINNER IS:\\n', gridGBR.best_params_)","02d5ac15":"coef_GBR = ModelDict['GBR'].feature_importances_\nsns.relplot(data = pd.DataFrame(data = coef_GBR, index = CandyDataSetDF.iloc[:,1:10].columns),\n            kind = \"line\", legend = False, aspect = 3)","c580d2fd":"AllCoefDF = pd.DataFrame()\n\nMMSAllCoef = MinMaxScaler()\n\nAllCoefNP = MMSAllCoef.fit_transform(np.vstack([coef_linreg, coef_SVR, coef_GBR]).T)\n\nAllCoefDF['coef_linreg'] = AllCoefNP[:,0]\nAllCoefDF['coef_SVR'] = AllCoefNP[:,1]\nAllCoefDF['coef_GBR'] = AllCoefNP[:,2]\n\nAllCoefDF.index = CandyDataSetDF.iloc[:,1:10].columns \n\nsns.relplot(data = AllCoefDF, kind = \"line\", aspect = 3)","57a7d71e":"# Define the parameters that stay constant and are not default.\nparam_fixMLPR = dict(\n    #alpha = 0.0001,\n    early_stopping = True,\n    solver='adam',\n    learning_rate='adaptive',\n    learning_rate_init=0.0001,\n    hidden_layer_sizes = (100, 100, 50, 50),\n    activation= 'relu',\n    max_iter=100000,\n    verbose=False\n)\n\n# Prepare the parameters for grid search \n# Thx2: https:\/\/scikit-learn.org\/stable\/auto_examples\/neural_networks\/plot_mlp_alpha.html\n\nparam_gridMLPR = dict(\n    #activation = ['identity', 'logistic', 'tanh', 'relu'],\n    alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10],    # Deal with outliers\n    #momentum = [0.25, 0.5, 0.9],\n    #learning_rate_init = [0.001, 0.0001],\n    #learning_rate = ['adaptive', 'constant'],\n    #nesterovs_momentum = [True, False],\n    #early_stopping = [True, False],\n    #solver = ['adam', 'sgd'],\n    #hidden_layer_sizes = [(20,), (50,), (100,), (200,), (100, 50), (100, 50, 50), (100, 100, 50, 50)],   \n)","dfc36857":"ModelDict['MLPR'] = MLPRegressor(**param_fixMLPR)\n\ngridMLPR = GridSearchCV(ModelDict['MLPR'], param_gridMLPR, verbose = True, cv = 5) # Five folds\ngridMLPR.fit(XNNTrain, YNNTrain) \nprint('AND THE WINNER IS:\\n', gridMLPR.best_params_)\ngridMLPRDF = pd.DataFrame(gridMLPR.cv_results_)\n\nModelDict['MLPR'] = gridMLPR.best_estimator_","ce4776b3":"ScoreDict['MLPR'] = ModelDict['MLPR'].score(XNNTest,YNNTest)\nprint('R\u00b2 score: ', ScoreDict['MLPR'])","5b823abf":"MLPRparams = ModelDict['MLPR'].get_params()\nif MLPRparams['solver'] in ['sgd', 'adam']:\n    sns.relplot(data = pd.DataFrame(data = ModelDict['MLPR'].loss_curve_, columns=['Error']),\n                kind = \"line\", legend = False, aspect = 3)","96dc390b":"# ####################################################\n# Essentially we can produce all possible combinations by 9 bit encoding, because of nine binary attributes\n# So, we build a function that will genereate a numpy array with all possible 9 bit combinations that we can \n# then feed into the MLP network for approximation to find the best candidate.\n# Thx2: https:\/\/docs.python.org\/3\/library\/functions.html#bin\n\nmyinputarr = list()\nCandyNames = list()\nFoundNames = list()\nFoundCandy = False\nmyNPinput = np.empty(shape=(pow(2,9),9))\n\nfor mynum in range(pow(2,9)):\n    mybinarystr = \"{:09b}\".format(mynum)\n    for mypos in range(len(mybinarystr)):\n        myinputarr.append(int(mybinarystr[mypos]))\n    for (x,testarr) in enumerate(Xindy):\n        if myinputarr == list(testarr[0:9]):\n            FoundNames.append(CandyDataSetDF.iloc[x,0])\n            FoundCandy = True\n    if FoundCandy == True:\n        CandyNames.append(FoundNames)\n        FoundNames = []\n        FoundCandy = False\n    else:\n        CandyNames.append(mybinarystr)\n                \n    myNPinput[mynum] = myinputarr\n    myinputarr = []\n    \nprint('DONE!')","c321a4b9":"# #################################\n# Now create nice DataFrame with all possible permutations and save the predictions of each model.\n# Sort with index also sorted:\n# Thx2: https:\/\/stackoverflow.com\/a\/52720936\n\nPredDFDict = {}\nPredDFsortDict = {}\n\nfor n in MODEL_NAMES:\n\n    PredDFDict[n] = pd.DataFrame(data = myNPinput, columns = CandyDataSetDF.iloc[:,1:10].columns)\n    PredDFDict[n]['prediction'] = YScalDict[n].inverse_transform(ModelDict[n].predict(myNPinput).reshape(-1, 1))\n    PredDFDict[n]['PredNames'] = CandyNames\n\n    PredDFsortDict[n] = PredDFDict[n].sort_values(by = 'prediction', ascending = False)\n    PredDFsortDict[n].reset_index(drop=True, inplace=True)\n    \n    print(n, 'Sorted DataFrame with predictions created')","b86a6fef":"# Plot the graphs\n# https:\/\/seaborn.pydata.org\/tutorial\/axis_grids.html\n\nPredictionsAllDF = pd.DataFrame(columns=[a for a in PredDFDict])\nfor a in PredDFDict:\n    PredictionsAllDF[a] = PredDFsortDict[a]['prediction']\nsns.relplot(data = PredictionsAllDF, aspect = 2.5)","23cf7b0f":"# Extracting the records with candy names from the output DataFrame and build a dictionary with candy name as key, the true value and the predicted value. \n# Thx2: https:\/\/www.journaldev.com\/23763\/python-remove-spaces-from-string\n\nrankdict = dict()\n\nfor k, MLPPredictDFsort in PredDFDict.items():\n    is_name = MLPPredictDFsort['PredNames'].map(lambda x: not str(x).isdigit())\n    NameSortDF = MLPPredictDFsort[is_name]\n\n\n\n    for rpos, namestr in enumerate(CandyDataSetDF['competitorname']):\n        nammap = NameSortDF['PredNames'].map(lambda x: not str(x).find(namestr) == -1)\n        NamFoundDF = NameSortDF[nammap]\n        if len(NamFoundDF) > 1:\n            namlst = dict() #Dictionary of lists containing the candy names of the respective DataFrame line \n            for tstidx, namstr in enumerate(NamFoundDF['PredNames']):\n                namlst[tstidx] = str(namstr).strip(\"[]\").replace(\"'\",\"\").split(',')\n                for x,s in enumerate(namlst[tstidx]):\n                    namlst[tstidx][x] = s.strip() # delete the remaining whitespaces\n            is_exmatch = False\n            for c,v in namlst.items():\n                for s in v:\n                    if namestr == s:\n                        is_exmatch = True\n                        break\n                if is_exmatch == True:\n                    NamFoundDF = NamFoundDF.iloc[c]\n                    break\n        elif len(NamFoundDF) > 2:\n            raise ValueError('Found candy name more than two times!')\n        if not (k in rankdict): #Key does not exist yet, make a new entry\n            rankdict[k] = dict()\n        rankdict[k][namestr] = [float(CandyDataSetDF.at[rpos, 'winpercent']), float(NamFoundDF['prediction'])]\n\n    print('Done with {}!'.format(k))","3c708529":"# Calculate the Pearson rank correllation and Kendall's Tau \n# Thx2: https:\/\/stackoverflow.com\/a\/24888331\n\nspearmans_R = {}\nkendalls_Tau = {}\nRanksDF = pd.DataFrame(index = [a for a in rankdict], columns=['Kendalls_Tau', 'Spearmans_R'])\n\nfor k in rankdict:\n    tempDF = pd.DataFrame.from_dict(data = rankdict[k], orient='index', columns = ['winpercent', 'prediction']) \n\n    RanksDF['Kendalls_Tau'].loc[k] = scp.stats.kendalltau(tempDF['winpercent'], tempDF['prediction'])[0]\n    RanksDF['Spearmans_R'].loc[k] = scp.stats.spearmanr(tempDF['winpercent'], tempDF['prediction'])[0]\n\n\nRanksDF","f8c5f63d":"# Thx2: https:\/\/stackoverflow.com\/a\/33227833\nsns.lineplot(data = RanksDF.astype(float))\nplt.ylim(0, 1)","ffceec9d":"ScoreDF = pd.DataFrame.from_dict(data = ScoreDict, orient='index', columns=['R\u00b2 coeff'])\n\nsns.lineplot(data = ScoreDF.astype(float))\nplt.ylim(0, 1)","74538f7f":"#######################\n# Create a DataFrame with the weights of MLPR\n# Thx2: https:\/\/stackoverflow.com\/a\/19736406\n\nMLPRLayersDict = dict()\nfor c, a in enumerate(ModelDict['MLPR'].coefs_):\n    print('Layer {}'.format(c+1), len(a), np.shape(a))\n    MLPRLayersDict['Wght_IN_'+str(c)] = a[0]\n    MLPRLayersDict['Wght_OUT_'+str(c)] = a[1]\n    \nMLPRLayersDF = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in MLPRLayersDict.items() ]))\nMLPRLayersDF.head()","4b4facbb":"from time import gmtime, strftime\n\ncurgmtstr = strftime(\"%Y%m%d%H%M%S\", gmtime())\n\n# For some reason Kaggle does not like '+' signs in filenames, so here's the workaround\nposnegstr = lambda x: 'p' if x >= 0.0 else 'n'\ncurscorestr = '_' + posnegstr(ScoreDict['MLPR']) + '{:.6f}'.format(ScoreDict['MLPR']).replace('.','_').replace('-','')\n\n###############################################\n# Write multiple Dataframes in one EXCEL file\n# Thx2: https:\/\/xlsxwriter.readthedocs.io\/example_pandas_multiple.html\n\nMLPRParamsDFDict = dict.fromkeys(MODEL_NAMES)\n\nfor a in MODEL_NAMES:\n    MLPRParamsDFDict[a] = pd.DataFrame.from_dict(ModelDict[a].get_params(), orient = 'index', columns = ['value'])\n\nwith pd.ExcelWriter('results_' + curgmtstr + curscorestr + \".xlsx\", engine='xlsxwriter') as XLSXwriter:\n    for a in PredDFsortDict:\n        PredDFsortDict[a].to_excel(XLSXwriter, sheet_name = 'pred_'+a)\n    AllCoefDF.to_excel(XLSXwriter, sheet_name = 'Lin_Coeffs')\n    MLPRLayersDF.to_excel(XLSXwriter, sheet_name = 'MLPR_Coeffs')\n    RanksDF.to_excel(XLSXwriter, sheet_name = 'RankCorr')\n    ScoreDF.to_excel(XLSXwriter, sheet_name = 'R\u00b2 score')\n    for a in MODEL_NAMES:\n        MLPRParamsDFDict[a].to_excel(XLSXwriter, sheet_name = 'params_'+a)\n    gridMLPRDF.to_excel(XLSXwriter, sheet_name = 'grid_MLPR')\n    gridsvrDF.to_excel(XLSXwriter, sheet_name = 'grid_SVR')\n    gridGBRDF.to_excel(XLSXwriter, sheet_name = 'grid_GBR')","65b40b78":"##################### DEBUG\n# For exporting files directly from the kernel\n# Thx2: https:\/\/www.kaggle.com\/general\/65351#600457\n\n#import os\n#os.chdir(r'\/kaggle\/working')","2781231e":"!ls -l *.xlsx","e53d1fc9":"curgmtstr","3674fb4d":"##################### DEBUG\n#from IPython.display import FileLink\n#FileLink(r'XXXXXXX.xlsx')","59f9aca0":"# Calclulate the rank correlation\n\n* Todo: Take care of the unnecessary string conversions by getting the Dtype in the DataFrame right","e750ad9f":"# Training a MLP regression now\n\nBasically the results of the various regression analysis show us that we agree to disagree with only one exception: **chocolate IS important!**\n\nSince we have so different results this indicates that something more complex is hidden in this data. When it is coming to complex relationships between data points there is a gold standard: artificial neural networks!\n\nHowever, this comes with a price. We don't get convenient weight indicies for the different ingredients, their relationship is more complex. So this leaves us only with the choice of providing the trained network examples for interference. Thus getting a prediciton of the estimated success according to the trained model.","c5757877":"# Having a look at correlations of sugarpercent and pricepercent with winpercent","5b6d5eac":"## Ensemble learning with gradient boosting regressor","7e5a2759":"## Linear Regression","4270b5e4":"# The Ultimate Halloween Frankencandy Generator (TUHFG)\nTUHFG is a fork from [Version 17](https:\/\/www.kaggle.com\/docacs\/the-halloween-frankencandy-generator?scriptVersionId=21464636) of my project [The Halloween Frankencandy Generator](https:\/\/www.kaggle.com\/docacs\/the-halloween-frankencandy-generator) (THFG).\n\nKey take home message: **\"Never start tuning a machine learning solution without having a good validation function for your problem.\"**\n\nThe fork from a quite extensive project was becoming necessary after I finally took the time to implement a validation function to see if the predicitons are *actually* good and not only the model fit. It took me one full day and showed me that the R\u00b2 coefficient and the quality of the prediction, measured by rank correlation, are not necessarily closely correlated.\n\n## Mission statement:\nThis notebook is my second attempt to tackle the [Ultimate Halloween Candy Power Ranking](https:\/\/fivethirtyeight.com\/features\/the-ultimate-halloween-candy-power-ranking\/) challenge. Well, not an official Kaggle challenge, more of a personal one. What would you do if you have to recommend ideal Halloween candy creations to a product development team? The approach in the article in [FiveThirtyEight](https:\/\/fivethirtyeight.com\/) was to use linear regression models. I disagree with this approach. As I showed below, various linear regression models come to quite different conclusions. I am convinced that this problem can be tackled by [Artificial Neural Networks](https:\/\/en.wikipedia.org\/wiki\/Artificial_neural_network), which are able to address the complex non linear interactions assumed here, e.g. shown with the [prediction of ocean wave patterns](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1877050914002816)\n\n## Conclusion:\nWith this dataset it could be shown that simple solutions are sometimes the best ones. This was counterintuitive for me at first, since the simpler regression models come to different results about the importance of the features. Again, implementing a good validation function for your problem is paramount! Would I have done this from the beginning, I would have seen that almost every time a simple linear regression performs better then a computationally demanding artificial neural network (ann). Other regression methods are also computationally less demanding than ann's and lead to even better results than the simple linear regression. *Lesson learned.*\n\nIt remains to be seen if this relationship between the regression methods changes with larger and more complex datasets.\n\nFerenc (2019-11-03)\n\nHappy forking! :)\n","4a1de9c7":"# Arrange data in NumPy arrays","93eed1c2":"## Support Vector Regression\nFinding best parameters via grid search.\n\nThe optimal value for C was 2 from a ```np.linspace(1,100,100)``` run, so let's get a bit more granular here.\n\nhttps:\/\/stackoverflow.com\/questions\/36306555\/scikit-learn-grid-search-with-svm-regression\n\nPython2 code and for sklearn.svm.SVC, so some changes had to be made to make this work\nfor Python3 and sklearn.svm.SVR\n\nAbout the parameter C (penalty parameter):\nhttps:\/\/stats.stackexchange.com\/a\/159051","ee0d599c":"# Fitting some regression models","46bcddc8":"# Distributions and test against normality assumption\nTesting if the continous variables follow a standard distribution and make some visualizations to explore the data","59cf7e60":"# Logbook\n* v3: Implementing the rank correllation code from a helper notebook I have developed on a local Jupyter Notebook. Introduced MinMax Scaling for ```winpercent``` to get a bit more range.\n* v4: Fascinating discovery! \n    - If I use the y scaling by the book, to tune for ReLu, I should compress all y values to ```MinMaxScaler(feature_range=(0.1,0.99))``` the rank correlations gets very bad, vanishing gradient perhaps? If I expand the value range to ```MinMaxScaler(feature_range=(0,100))``` the results gets only a bit better. I get the best results when scaling to ```MinMaxScaler(feature_range=(1,100))```!\n    - Made the graphics a bit nicer.\n    - Integrated some outlier detection.\n    - Scaled X and Y now\n* v5: \n    - Integration of more regression models.\n    - Creation of a integrated diagram for coefficients of all regression models.\n* v6:\n    - Adapting the scaling for SVR binary features too (& GBR), better to use standard scaling according to [SVM Kernels | Support Vector Machines 101 | By Dr. Ry @Stemplicity](https:\/\/youtu.be\/YS20BYYOor4), which was also a quite understandable explanation.\n    - Integration of grid search for MLPR.\n    - Generating protocol files for grid searches.\n* v7:\n    - Got inspired by [Peter Prettenhofer - Gradient Boosted Regression Trees in scikit-learn](https:\/\/youtu.be\/IXZKgIsZRm0) to improve the GBR cells.\n* v8:\n    - Integrated price and sugar as predictors in the regression models.\n    - Identified stable parameters for GBR from grid search.\n* v9: \n    - Changed the regressors for the linear regression to standard scaled ones.\n    - Some cosmetic changes before making the kernel public.\n* v10: v9 did not render, [similar issues reported here](https:\/\/www.kaggle.com\/product-feedback\/29896), trying to simply commit once again...\n* v11:\n    - looking for extending the rank correlation for the other linear regressors too.\n    - reducing the regressors back to 9 for the linear regressors\n    - Models and estimation DataFrames now organized in dictionaries for more consistent processing.\n    - Tuned the graphics a bit\n    - Write multiple DataFrames in one EXCEL to keep things tidy.","23c3cdc9":"# Check for outliers\nDuring various test runs with hyperparameters that seemed to work good, I got sometimes a good model fit, then again negative scores. I suspect that there are outliers in the dataset."}}