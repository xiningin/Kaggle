{"cell_type":{"1a4f4032":"code","d72d2f0f":"code","563076fa":"code","9ecd37af":"code","530b0061":"code","cb6c5894":"code","a21a511e":"code","2ded5522":"code","fef008b9":"code","d1ce5da5":"code","325ad00e":"code","a654943b":"code","57ccbc66":"code","457d238d":"code","8fc8c361":"code","e3b5cc5b":"code","78865549":"code","2aad210a":"code","7d13c38a":"code","474d9480":"code","c3b20792":"markdown","14f4567c":"markdown","c036c99a":"markdown","5bde6e59":"markdown","678352c4":"markdown","a69ce3f7":"markdown"},"source":{"1a4f4032":"import os\nfrom argparse import Namespace\nimport json\nimport re\nimport string\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset,DataLoader\n\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm_notebook","d72d2f0f":"os.listdir(\"..\/input\/\")","563076fa":"df=pd.read_csv(\"..\/input\/news_with_splits.csv\")\ndf.head()","9ecd37af":"class Vocabulary(object):\n    def __init__(self,token_to_idx=None):\n        if token_to_idx is None:\n            self._token_to_idx={}\n        else:\n            self._token_to_idx=token_to_idx\n        self._idx_to_token={idx:token for token,tdx in self._token_to_idx.items()}\n        \n    def add_token(self,token):\n        if token in self._token_to_idx:\n            index=self._token_to_idx[token]\n        else:\n            index=len(self._token_to_idx)\n            self._token_to_idx[token]=index\n            self._idx_to_token[index]=token\n        return index \n    \n    def look_token(self,token):\n        return self.add_token(token)\n        \n    def look_index(self,index):\n        if index not in self._idx_to_token:\n            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n        return self._idx_to_token[index]  ","530b0061":"class SequenceVocabulary(Vocabulary):\n    def __init__(self,token_to_idx=None,unk_token=\"<UNK>\",\n                mask_token=\"<MASK>\",begin_seq_token=\"<BEGIN>\",end_seq_token=\"<END>\"):\n        super(SequenceVocabulary,self).__init__()\n        self._mask_token=mask_token\n        self._unk_token=unk_token\n        self._begin_seq_token=begin_seq_token\n        self._end_seq_token=end_seq_token\n        \n        self._mask_index=self.add_token(self._mask_token)\n        self._unk_index=self.add_token(self._unk_token)\n        self._begin_seq_index=self.add_token(self._begin_seq_token)\n        self._end_seq_index=self.add_token(self._end_seq_token)\n        \n    def look_token(self,token):\n        if self._unk_index>=0:\n            return self._token_to_idx.get(token,self._unk_index)\n        else:\n            return self.add_token(token)","cb6c5894":"class Vectorizer(object):\n    def __init__(self,title_vocab,category_vocab):\n        self.title_vocab=title_vocab\n        self.category_vocab=category_vocab\n        \n    def vectorize(self,title,max_len=-1):\n        indices=[self.title_vocab._begin_seq_index]\n        indices.extend([self.title_vocab.look_token(token) for token in title.split(\" \")])\n        indices.append(self.title_vocab._end_seq_index)\n        \n        if max_len<0:\n            max_len=len(indices)\n        vector=np.zeros(max_len,dtype=np.int64)\n        vector[:len(indices)]=indices\n        vector[len(indices):]=self.title_vocab._mask_index\n        \n        return vector\n        \n    @classmethod\n    def transform_df(cls,news_df,cutoff=25):\n        category_vocab=Vocabulary()\n        for category in news_df.category:\n            category_vocab.add_token(category)\n            \n        word_counts=Counter()\n        for title in news_df.title:\n            for token in title.split(\" \"):\n                if token not in string.punctuation:\n                    word_counts[token]+=1\n        \n        title_vocab=SequenceVocabulary()\n        for word,word_count in word_counts.items():\n            if word_count>cutoff:\n                title_vocab.add_token(word)\n        \n        return cls(title_vocab,category_vocab)","a21a511e":"class NewsDataset(Dataset):\n    def __init__(self,news_df,vectorizer,colname):\n        self.news_df=news_df\n        self._vectorizer=vectorizer\n        \n        self.max_len=max(self.news_df.title.apply(lambda x:len(x.split(\" \"))))+2\n        \n        self.train_df=self.news_df[self.news_df.split==\"train\"]\n        self.train_size=len(self.train_df)\n        self.val_df=self.news_df[self.news_df.split==\"val\"]\n        self.val_size=len(self.val_df)\n        self.test_df=self.news_df[self.news_df.split==\"test\"]\n        self.test_size=len(self.test_df)\n                 \n        self.look_dict={\n            \"train\":(self.train_df,self.train_size),\n            \"val\":(self.val_df,self.val_size),\n            \"test\":(self.test_df,self.test_size)\n        }\n        \n        # class weight \n        class_counts=self.news_df.category.value_counts().to_dict()\n        def sort_key(item):\n            return self._vectorizer.category_vocab.look_token(item[0])\n        sorted_counts=sorted(class_counts.items(),key=sort_key)\n        frequenies=[count for _,count in sorted_counts]\n        self.class_weights=1.0\/torch.tensor(frequenies,dtype=torch.float32)\n        \n        self.set_split(colname)\n        \n    def set_split(self,colname):\n        self._target_df,self._target_size=self.look_dict[colname]\n        \n    def __len__(self):\n        return self._target_size\n    \n    def __getitem__(self,index):\n        row=self._target_df.iloc[index]\n        title_vector=self._vectorizer.vectorize(row.title,self.max_len)\n        category_index=self._vectorizer.category_vocab.look_token(row.category)\n        return title_vector,category_index","2ded5522":"train_dataset=NewsDataset(df,Vectorizer.transform_df(df),\"train\")\nval_dataset=NewsDataset(df,Vectorizer.transform_df(df),\"val\")\ntest_dataset=NewsDataset(df,Vectorizer.transform_df(df),\"test\")","fef008b9":"train_dataset.__getitem__(0)","d1ce5da5":"args=Namespace(\n    use_glove=True,\n    embedding_size=100,\n    hidden_dim=100,\n    nun_channels=100,\n    learning_rate=0.001,\n    dropout_p=0.1,\n    n_epochs=100,\n    batch_size=128,\n    num_classes=4\n)","325ad00e":"train_loader=DataLoader(train_dataset,batch_size=args.batch_size,shuffle=True)\nval_loader=DataLoader(val_dataset,batch_size=args.batch_size,shuffle=True)\ntest_loader=DataLoader(test_dataset,batch_size=1)","a654943b":"class NewsClassifier(nn.Module):\n    def __init__(self,vocabulary_num,embedding_num,num_channels,dropout_p,\n                 hidden_dim,num_classes,pretained_embedding=None,padding_idx=0):\n        super(NewsClassifier,self).__init__()\n        \n        if pretained_embedding is None:\n            self.embedding=nn.Embedding(vocabulary_num,embedding_num,padding_idx)\n        else:\n            pretained_embedding=torch.from_numpy(pretained_embedding).float()\n            self.embedding=nn.Embedding(vocabulary_num,embedding_num,padding_idx,_weight=pretained_embedding)\n            \n        self.connet=nn.Sequential(\n            nn.Conv1d(in_channels=embedding_num,out_channels=num_channels,kernel_size=3),\n            nn.ELU(),\n            nn.Conv1d(in_channels=num_channels,out_channels=num_channels,kernel_size=3,stride=2),\n            nn.ELU(),\n            nn.Conv1d(in_channels=num_channels,out_channels=num_channels,kernel_size=3,stride=2),\n            nn.ELU(),\n            nn.Conv1d(in_channels=num_channels,out_channels=num_channels,kernel_size=3),\n            nn.ELU()\n        )\n        \n        self._dropout_p=dropout_p\n        self.fc1=nn.Linear(num_channels,hidden_dim)\n        self.fc2=nn.Linear(hidden_dim,num_classes)\n        \n    def forward(self,x_in,apply_softmax=False):\n        x=self.embedding(x_in).permute(0,2,1)\n        x=self.connet(x)\n        remaing_size=x.size(dim=2)\n        x=F.avg_pool1d(x,remaing_size).squeeze(dim=2)  # \u5bf9\u6700\u5c0f\u4e00\u7ef4\u53d6\u5e73\u5747\n        x=F.dropout(x,p=self._dropout_p)\n        \n        x=self.fc1(x)\n        x=F.dropout(x,p=self._dropout_p)\n        x=F.relu(x)\n        output=self.fc2(x)\n        \n        if apply_softmax:\n            output=F.softmax(output)\n        return output","57ccbc66":"with open(\"..\/input\/glove.6B.100d.txt\",\"r\",encoding=\"utf-8\") as f:\n    word_to_index={}\n    embedding_glove=[]\n    for index,line in enumerate(f):\n        word_to_index[line.split(\" \")[0]]=index\n        embedding_i=[float(x) for x in line.split(\" \")[1:]]\n        embedding_glove.append(embedding_i)","457d238d":"vec=Vectorizer.transform_df(df)","8fc8c361":"n=len(vec.title_vocab._token_to_idx)\nfinal_embedding=np.ones((n,100))\nfor word,i in vec.title_vocab._token_to_idx.items():\n    word=word.lower()\n    if word in word_to_index:\n        idx=word_to_index[word]\n        final_embedding[i,:]=embedding_glove[idx]\n    else:\n        embedding_ii=torch.ones(1,100)\n        torch.nn.init.xavier_uniform_(embedding_ii)\n        final_embedding[i,:]=embedding_ii","e3b5cc5b":"classifier=NewsClassifier(vocabulary_num=n,embedding_num=args.embedding_size,num_channels=args.nun_channels,dropout_p=0.1,\n                          hidden_dim=args.hidden_dim,num_classes=args.num_classes,pretained_embedding=final_embedding)\nprint(classifier)","78865549":"loss_func=nn.CrossEntropyLoss()\noptimizer=optim.Adam(params=classifier.parameters(),lr=args.learning_rate)","2aad210a":"train_state={\"train_loss\":[],\"val_loss\":[]}\nfor epoch in range(args.n_epochs):\n    running_train_loss=0.0\n    classifier.train()\n    \n    for batch_index,batch_data in enumerate(train_loader):\n        x,y=batch_data\n        classifier.zero_grad()\n        \n        y_pred=classifier(x)\n        loss=loss_func(y_pred,y)\n        loss_train_batch=loss.item()\n        running_train_loss+=(loss_train_batch-running_train_loss)\/(batch_index+1)\n        loss.backward()\n        optimizer.step()\n    train_state[\"train_loss\"].append(running_train_loss)\n    \n    running_val_loss=0.0\n    classifier.eval()\n    for batch_index,batch_data in enumerate(val_loader):\n        x,y=batch_data\n        y_pred=classifier(x)\n        loss=loss_func(y_pred,y)\n        loss_val_batch=loss.item()\n        running_val_loss+=(loss_val_batch-running_val_loss)\/(batch_index+1)\n    train_state[\"val_loss\"].append(running_val_loss)\n    print(\"Epoch: \",epoch,\"Train Loss: \",running_train_loss,\"Valid Loss\",running_val_loss)","7d13c38a":"em=nn.Embedding(num_embeddings=n,embedding_dim=100,padding_idx=0,_weight=torch.from_numpy(final_embedding))","474d9480":"for x,y in train_loader:\n    print(x)\n    o=em(x)\n    print(o.permute(0,2,1).size())\n    break","c3b20792":"## Dataset","14f4567c":"## Model","c036c99a":"## Train","5bde6e59":"## Dataloader","678352c4":"## Vocabulary","a69ce3f7":"## Vectorizer"}}