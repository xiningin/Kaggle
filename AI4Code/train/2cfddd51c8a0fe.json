{"cell_type":{"3810b624":"code","ac32e1ec":"code","98bd2a1e":"code","735c77ac":"code","e62b86d8":"code","e1458748":"code","a5077809":"code","b4d74206":"code","103da32c":"code","ad61c250":"code","80f034e3":"code","431b9173":"code","7788779f":"code","0a58b89a":"code","b6463b4a":"code","584f2ba2":"code","1fb6efdb":"code","778eb068":"code","6dbac368":"code","bcfcbe04":"code","0b64a466":"code","874cd4e6":"code","638b27cf":"code","90b2d354":"code","08458787":"code","b9c421f2":"markdown","670f4098":"markdown","16a28374":"markdown","080f7ac0":"markdown","8e915766":"markdown","c25757c3":"markdown","6fcee4fe":"markdown","61339171":"markdown","8db5c870":"markdown","dfc12d64":"markdown","4977015b":"markdown"},"source":{"3810b624":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport tensorflow as tf","ac32e1ec":"df = pd.read_csv('\/kaggle\/input\/robert-frost-collection\/robert_frost_collection.csv')\ndf = df.iloc[1:]\ndf.head()","98bd2a1e":"content = list(df['Content'])\n\ntext=''\nfor c in content:\n    text+=c +'\\n'\nprint('First 37 characters:\\n')\nprint(text[:37])","735c77ac":"# All unique characters\nvocab = sorted(set(text))\n\n# Creating a mapping from unique characters to indices\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = np.array([char2idx[c] for c in text])","e62b86d8":"print('{} --> {}'.format(repr(text[:37]), text_as_int[:37]))","e1458748":"seq_length = 50\nexamples_per_epoch = len(text)\/\/(seq_length+1)\n\n# Create training examples \/ targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\nfor i in char_dataset.take(5):\n    print(idx2char[i.numpy()])","a5077809":"sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n\nfor item in sequences.take(10):\n    print(repr(''.join(idx2char[item.numpy()])))","b4d74206":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)","103da32c":"for input_example, target_example in  dataset.take(1):\n    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))","ad61c250":"# Batch size\nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","80f034e3":"# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units\nLSTM_units = 1024","431b9173":"def build_model(vocab_size, embedding_dim, LSTM_units, batch_size):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                                  batch_input_shape=[batch_size, None]),\n        tf.keras.layers.LSTM(LSTM_units,\n                            return_sequences=True,\n                            stateful=True,\n                            recurrent_initializer='glorot_uniform'),\n        \n        tf.keras.layers.LSTM(LSTM_units,\n                            return_sequences=True,\n                            stateful=True,\n                            recurrent_initializer='glorot_uniform'),\n        \n        tf.keras.layers.Dense(vocab_size)\n    ])\n    return model","7788779f":"model = build_model(\n    vocab_size=len(vocab),\n    embedding_dim=embedding_dim,\n    LSTM_units=LSTM_units,\n    batch_size=BATCH_SIZE)","0a58b89a":"model.summary()\n","b6463b4a":"def loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)","584f2ba2":"model.compile(optimizer='adam', loss=loss)\n","1fb6efdb":"# Directory where the checkpoints will be saved\ncheckpoint_dir = '.\/training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","778eb068":"EPOCHS = 40\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","6dbac368":"tf.train.latest_checkpoint(checkpoint_dir)","bcfcbe04":"model = build_model(vocab_size, embedding_dim, LSTM_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))","0b64a466":"def generate_text(model, start_string):\n    # Evaluation step (generating text using the learned model)\n\n    # Number of characters to generate\n    num_generate = 250\n\n    # Converting our start string to numbers (vectorizing)\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    # Empty string to store our results\n    text_generated = []\n\n    # Low temperature results in more predictable text.\n    # Higher temperature results in more surprising text.\n    # Experiment to find the best setting.\n    temperature = 0.5\n\n    # Here batch size == 1\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a categorical distribution to predict the character returned by the model\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # Pass the predicted character as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(idx2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))","874cd4e6":"print(generate_text(model, start_string=\"Where\"))\n","638b27cf":"print(generate_text(model, start_string=\"why\"))\n","90b2d354":"print(generate_text(model, start_string=\"The\"))\n","08458787":"print(generate_text(model, start_string=\"In\"))\n","b9c421f2":"# Creating Charles","670f4098":"### Concatenate all poems into a single string","16a28374":"# Creating Curriculum for Charles\n\n## Charles does not actually undertand written language. He is a computer. So we need to transform 108 Robert Frost poems into a format which is more suitable for him to understand.","080f7ac0":"## Example: First 37 characters mapped to their index position. This is the format that Charles will be reading.","8e915766":"# *Charles Waldo Savage* \n## Charles Waldo Savage is an American AI poet who consists of several Long-Short-Term Memory Neural Network Layers. Charles learned to write poems based off of a collection of 108 Robert Frost Poems.","c25757c3":"## How Charles will be learning\nThe way that Charles learns how to write is by learning to predict the next character given a sequence of characters So if we give Charles the input:<br>\n- `Whose woods these `<br>\n\nWe want charles to be able to output:\n- `are I think I know.`","6fcee4fe":"# A few poems by Charles","61339171":"### Creating training examples and targets","8db5c870":"## Preview Data","dfc12d64":"### Map Characters to integers and vice versa","4977015b":"## Preprocessing the data"}}