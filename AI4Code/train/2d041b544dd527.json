{"cell_type":{"3d2c4ec1":"code","3c146660":"code","393b2e48":"code","ade80f0b":"code","3b4ba385":"code","7a557ca1":"code","65eb7ee1":"code","c2664509":"code","aa012494":"code","4fedc870":"code","a4dac567":"code","2c3e7f06":"code","23eadb4a":"code","5af5ed27":"code","a405819e":"code","10aa47bc":"code","de282363":"code","c6d31d5c":"code","9a96710d":"code","38168c11":"code","93ca9d73":"code","cfccba3c":"code","e7a801c2":"code","420a5096":"code","a8c1ef73":"code","2b601e3e":"code","43eaff34":"code","85388771":"code","21146df9":"code","9ffdb1b3":"code","e2a4e647":"code","c1e297a7":"code","8b788972":"code","034ab450":"code","af1aaf15":"code","b9e2c486":"code","68aadb03":"markdown","63243fcc":"markdown","bb8082a0":"markdown","a0ae97fc":"markdown","d622f01c":"markdown","3267dc94":"markdown","928b5ec8":"markdown","d6aa0ecb":"markdown","1a1ec82a":"markdown","53f095f7":"markdown","0a5090f9":"markdown","7fe4051e":"markdown","d7a41e8e":"markdown","034790b7":"markdown","c45dd65b":"markdown","44e6c2ca":"markdown"},"source":{"3d2c4ec1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3c146660":"!pip install transformers","393b2e48":"!pip install simpletransformers==0.32.3","ade80f0b":"import pandas as pd\nfrom nltk.corpus import stopwords\nimport re\nfrom wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (GPT2Config,GPT2LMHeadModel,GPT2Tokenizer)\nimport torch\nfrom string import punctuation as pnc\nfrom collections import Counter\nimport gc\npd.set_option('display.max_colwidth', -1)","3b4ba385":"tweets = pd.read_csv('\/kaggle\/input\/clinton-trump-tweets\/tweets.csv')\ndisplay(tweets.head(1))","7a557ca1":"tweets = tweets[['handle','text','is_retweet']]","65eb7ee1":"print(\"Number of Tweets : \",len(tweets))\nprint(\"Null Count in the 3 columns : \")\nprint(tweets.isna().sum())","c2664509":"print(\"Number of Tweets from Doland and Hillary : \")\ntweets['handle'].value_counts()","aa012494":"tweets['tweetLen'] = tweets['text'].apply(lambda x : len(x.split(\" \")))","4fedc870":"doland_tweets = tweets[tweets['handle']=='realDonaldTrump']\nprint(\"Doland Tweets : \")\ndisplay(doland_tweets['text'].head(5))\nhillary_tweets = tweets[tweets['handle']=='HillaryClinton']\nprint(\"Hillary Tweets : \")\ndisplay(hillary_tweets['text'].head(5))","a4dac567":"doland_tweets['tweetLen'].hist(bins=32)","2c3e7f06":"hillary_tweets['tweetLen'].hist(bins=32)","23eadb4a":"def getWordCloud(df,col):\n  comment_words = '' \n  stopwords = set(STOPWORDS) \n    \n  for val in df[col]: \n        \n      val = str(val) \n      tokens = val.split() \n        \n      for i in range(len(tokens)): \n          tokens[i] = tokens[i].lower() \n        \n      comment_words += \" \".join(tokens)+\" \"\n    \n  wordcloud = WordCloud(width = 800, height = 800, \n                  background_color ='white', \n                  stopwords = stopwords, \n                  min_font_size = 10).generate(comment_words) \n    \n                       \n  plt.figure(figsize = (5, 5), facecolor = None) \n  plt.imshow(wordcloud) \n  plt.axis(\"off\")\n  plt.tight_layout(pad = 0) \n    \n  plt.show()","5af5ed27":"getWordCloud(doland_tweets,'text')","a405819e":"getWordCloud(hillary_tweets,'text')","10aa47bc":"# Prints only the top 20 frequently occured Twitter handles\ndef getTwitterHandlesTagged(df, col):\n    taggedHandlesList = []\n    for tweet in df[col].tolist():\n        taggedHandles = [x for x in tweet.split(\" \") if x.startswith('@')]\n        taggedHandlesList = taggedHandlesList + taggedHandles\n    \n    print(list({k: v for k, v in sorted(dict(Counter(taggedHandlesList)).items(), key=lambda item: item[1], reverse = True)}.items())[:20])","de282363":"getTwitterHandlesTagged(doland_tweets, 'text')","c6d31d5c":"getTwitterHandlesTagged(hillary_tweets, 'text')","9a96710d":"# Prints only the top 20 frequently occured tags\ndef getTags(df, col):\n    tagsList = []\n    for tweet in df[col].tolist():\n        tags = [x for x in tweet.split(\" \") if x.startswith('#')]\n        tagsList = tagsList + tags\n    print(list({k: v for k, v in sorted(dict(Counter(tagsList)).items(), key=lambda item: item[1], reverse = True)}.items())[:20])","38168c11":"getTags(doland_tweets, 'text')","93ca9d73":"getTags(hillary_tweets, 'text')","cfccba3c":"hillary_tweets['textLwr'] = hillary_tweets['text'].str.lower()\nhillary_tweets['hasHillaySubString'] = hillary_tweets['textLwr'].str.contains('hillary')\ndisplay(hillary_tweets[hillary_tweets['hasHillaySubString'] == True]['text'].head(10))","e7a801c2":"def getQuoteAuthor(df, col):\n    quoteAuthorList = []\n    for tweet in df[col].tolist():\n        quoteAuthor = [x for x in tweet.split(\" \") if x.startswith('\u2014')]\n        quoteAuthorList = quoteAuthorList + quoteAuthor\n    print(list({k: v for k, v in sorted(dict(Counter(quoteAuthorList)).items(), key=lambda item: item[1], reverse = True)}.items())[:20])","420a5096":"getQuoteAuthor(hillary_tweets, 'text')","a8c1ef73":"getQuoteAuthor(doland_tweets, 'text')","2b601e3e":"def removeTagTaggedHandlesQuoteAuthor(text):\n    text = \" \".join([x for x in text.split(\" \") if not x.startswith(\"@\")])\n    text = \" \".join([x for x in text.split(\" \") if not x.startswith(\"#\")])\n    text = \" \".join([x for x in text.split(\" \") if not x.startswith(\"\u2014\")])\n    return text\n    \nhillary_tweets['preProcessedText'] = hillary_tweets['text'].str.replace('http\\S+|www.\\S+', '', case=False)\nhillary_tweets['preProcessedText'] = hillary_tweets['preProcessedText'].str.replace('\\n','')\nhillary_tweets['preProcessedText'] = hillary_tweets['preProcessedText'].apply(removeTagTaggedHandlesQuoteAuthor)","43eaff34":"hillary_preprocessedtweets = hillary_tweets['preProcessedText']","85388771":"hillary_preprocessedtweets_train, hillary_preprocessedtweets_eval = train_test_split(hillary_preprocessedtweets,test_size = 0.05)\nprint(\"Number of tweets in training data  : \",len(hillary_preprocessedtweets_train))\nprint(\"Number of tweets in validation data : \",len(hillary_preprocessedtweets_eval))","21146df9":"hillary_preprocessedtweets_train.to_csv('\/kaggle\/working\/hillary_preprocessedtweets_train.txt', header=None, index=None, sep=' ')\nhillary_preprocessedtweets_eval.to_csv('\/kaggle\/working\/hillary_preprocessedtweets_eval.txt', header=None, index=None, sep=' ')","9ffdb1b3":"from simpletransformers.language_modeling import LanguageModelingModel\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger()\nlogger.warning(\"Is this working?\") \ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)","e2a4e647":"args = {\n    \"reprocess_input_data\": True,\n    \"overwrite_output_dir\": True,\n    \"num_train_epochs\": 10,\n    \"train_batch_size\": 32,\n    \"mlm\": False,\n    \"dataset_type\" : \"simple\",\n    \"block_size\" : 24,\n    \"max_seq_length\" : 24,\n    \"evaluate_during_training\": True,\n    \"evaluate_during_training_steps\": 50,\n    \"evaluate_during_training_verbose\": True,\n    \"use_cached_eval_features\": True,\n    \"save_eval_checkpoints\" : False,\n    \"save_model_every_epoch\" : False,\n    \"early_stopping_patience\" : 2,\n    \"use_early_stopping\" : True,\n    \"save_optimizer_and_scheduler \" : False,\n    \"fp16\" : False\n}\n\nhillary_model = LanguageModelingModel(\n    'gpt2', \n    'gpt2',\n    args=args,\n    use_cuda=True,\n)","c1e297a7":"print(\"Get Value of all the hyperparameters  : \")\nfor key in hillary_model.args:\n  print(key, '->', hillary_model.args[key])","8b788972":"hillary_model.train_model(\"hillary_preprocessedtweets_train.txt\", eval_file=\"hillary_preprocessedtweets_eval.txt\")","034ab450":"!ls .\/outputs\/best_model","af1aaf15":"config_class, model_class, tokenizer_class = GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\nBestModel = model_class.from_pretrained('gpt2')\nBestModel.load_state_dict(torch.load(\".\/outputs\/best_model\/pytorch_model.bin\"))","b9e2c486":"prompt_texts = [\"I will reduce Gun violence.\",\"Donald will build a wall\",\"I will make our health care system better\",\"Come rally with us\",\"America is in financial stress\",\"We have to preserve secularism\",\"We will win the election\"]\ntokenizer = tokenizer_class.from_pretrained('gpt2')\nfor prompt_text in prompt_texts:\n  encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n  generated = BestModel.generate(encoded_prompt,max_length = 128, num_beams = 2, repetition_penalty = 5.0,verbose=False)\n  generated = generated.tolist()[0]\n  text = tokenizer.decode(generated, clean_up_tokenization_spaces=True)\n  print(\".\".join(text.split(\".\")[:3]))","68aadb03":"# In this Notebook, I have build a Model by finetuning OpenAI's GPT2 on Hillary Clinton's Tweet Data during US Presidential Election 2016. This Model generates artificial Tweet in the style of Hillary Clinton. Please do upvote this notebook if you liked the content. Thanks!!!","63243fcc":"## Let's see which handles did Doland and Hillary tag in their Tweets","bb8082a0":"### Most of the Tweets have a URL in it. \"https\" occurs in both Doland and Hillary Tweet Word Cloud","a0ae97fc":"## Preprocessing the Tweets. I have removed the New Line Characters, URLs, Tagged handles,Tags and Quote Authors to preprocess Hillary Tweets such that these frequent Tags, Tagged Handles, Quote Authors and random URL does not get generated every time.","d622f01c":"# The Model is working in most of the cases. But sometimes it might not be coherent with the input sequence. It might get better if we finetune on a larger data set.","3267dc94":"## Let's see the Quote Authors in Hillary's Tweet","928b5ec8":"## Number of Words in Doland Vs Hillary Tweets","d6aa0ecb":"## Let's see tweets where Hillary mentioned herself","1a1ec82a":"## Split in Training and Validation DataSet and save as text file. For now we will only generate artificial Tweets in Hillary Style.","53f095f7":"## Generate Text. We have to feed a sequence and the model will generate additional sequences in context of the US 2016 presidential election tweets by Hillary. Here I have taken only 2 additional sentences as output.","0a5090f9":"## Training the Model. We will finetune GPT2 Model(Simple Transformer) using the Hillary's Tweets.","7fe4051e":"## Let's see which Tags did Doland and Hillary use in their Tweets","d7a41e8e":"### Most of the above tweets are in quotes and has \"-Hillary\" at the end","034790b7":"## Word Cloud across Doland Vs Hillary Tweets","c45dd65b":"## Loading the best model weight in GPT2LMHeadModel","44e6c2ca":"### The Best Model Weight is stored in \/kaggle\/working\/outputs\/best_model\/pytorch_model.bin"}}