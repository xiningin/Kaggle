{"cell_type":{"7663b725":"code","a2ed87a6":"code","8981f4d5":"code","0682519e":"code","207ef56f":"code","fa597aa6":"code","9eed15ae":"code","b2e44433":"code","839a0770":"code","ba0400f8":"code","a976de4d":"code","173e7902":"code","81b3f714":"code","d88a99e9":"code","a0b7b802":"code","1ce1407b":"markdown","1d032101":"markdown","1bd993d0":"markdown","eaa8c450":"markdown","876048d6":"markdown","ac8b71af":"markdown","22185165":"markdown","29efc591":"markdown","f88014f0":"markdown","8c4f659d":"markdown","4d07e187":"markdown","32701208":"markdown","657bf9ba":"markdown","54aef30f":"markdown","7984be5e":"markdown","965e6962":"markdown","f3fc9a9b":"markdown"},"source":{"7663b725":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split #Split the data into train and test\nfrom sklearn.ensemble import RandomForestClassifier #Forest for prediction and regression\nfrom sklearn.metrics import mean_squared_error #Error testing\nfrom sklearn.metrics import classification_report #Report of Classification\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2ed87a6":"cancer = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\") #Put our data into a dataframe\ncancer.head() #Take a peek at the data","8981f4d5":"print(len(cancer.index)) #Print the number of rows","0682519e":"print(cancer.isnull().any()) #Check if the columns have null values","207ef56f":"print(cancer.loc[cancer[\"Unnamed: 32\"].isnull() != True]) #Check to see if unnamed32 actually has values","fa597aa6":"cancer = cancer.drop(columns = {\"Unnamed: 32\", \"id\"}) #Drop the null column\ncancer.head() #Take a peek and make sure it dropped","9eed15ae":"diagnosis = cancer[\"diagnosis\"].copy() #Put the diagnosis into its own variable\ncharacteristics = cancer.drop(columns = {\"diagnosis\"}).copy() #Put the characteristics in a separate dataframe\n\n#For each column, print the min and max values for Malignant and Beneign tumors\nfor column in characteristics.columns:\n    mal = cancer.loc[cancer[\"diagnosis\"] == \"M\"][column] #Get the malignant values for the column\n    ben = cancer.loc[cancer[\"diagnosis\"] == \"B\"][column] #Get the beneign values for the column\n    \n    print(\"The max and min malignant values for {} are {}, {}\".format(column, mal.max(), mal.min())) #Print malignant values\n    print(\"The max and min beneign values for {} are {}, {}\".format(column, ben.max(), ben.min())) #Print beneign values","b2e44433":"diagnosis = pd.get_dummies(diagnosis) #Get the encoding for the diagnosis variable\nprint(diagnosis) #Take a peek at the diagnosis dummies","839a0770":"charaTrain, charaTest, diagTrain, diagTest = train_test_split(characteristics, diagnosis, test_size = 0.2) #Create train and test sets\nprint(diagTrain) #Print one of the splits to have an idea about the structure","ba0400f8":"forest = RandomForestClassifier(n_estimators = 100) #Build a forest\nforest.fit(charaTrain, diagTrain) #Fit the forest model","a976de4d":"predict = forest.predict(charaTest) #Get a list of predictions","173e7902":"overallAccuracy = (\"Overall\", forest.score(charaTest, diagTest)) #Get the overall accuracy\nprint(\"Forest Accuracy: \", forest.score(charaTest, diagTest)) #Print the accuracy\nprint(\"Root Mean Square Error: \", np.sqrt(mean_squared_error(diagTest, predict))) #Print the root mean square error\nprint(\"Classification Report:\\n \", classification_report(diagTest, predict, target_names = [\"B\", \"M\"])) #Print a classification report","81b3f714":"attributes = characteristics.columns #Get the tested attributes\nattributes = list(zip(attributes, forest.feature_importances_)) #Zip the attributes together with their coefficient\nsortAtt = sorted(attributes, key = lambda x: x[1], reverse = True) #Sort the zipped attributes by their coefficients\n\nprint(\"According to the Random Forest, the most important factors for cancer status are: \") #Start printing the most important labels\ni=0 #Counter variable so only the top five are printed\n\n#For each attribute in the sorted attributes\nfor label, coef in sortAtt:\n    if i<5: #If there has not been five printed yet\n        print(label) #Print the label as an important factor\n    i += 1 #Increase i by 1","d88a99e9":"#SplitData: splits this data based on whether it is the mean, se, or worst column for the dataset\n#Input: the list of characteristic columns\n#Output: A list that contains the lists of each column (se, mean, and worst)\ndef splitData(charactColumns):\n    se = [] #A list holder for all SE columns\n    mean = [] #A list holder for all columns that end in mean\n    worst = [] #A list holder for all columns that end in worst\n    \n    #For each characteristics column, put it in the correct se, mean, or worst list\n    for column in charactColumns:\n        if column.find(\"se\") > -1: #If the column name contains se (I checked; the only SE sequence is the _se at the end)\n            se.append(column) #Add it to the SE list\n        elif column.find(\"mean\") > -1: #If the column contains mean\n            mean.append(column) #Add it to the mean list\n        else: #If the column contains neither, which means it contains worst\n            worst.append(column) #Add it to the worst list\n    \n    return [se, mean, worst] #Return a list with all the previous lists inside\n\n#RunForest: runs a forest for the specified characteristic type (colType), assuming the diagnosis is in its dummied form\n#Input: the diagnosis, the characteristics, the column\/characteristic type\n#Output: None\ndef runForest(diag, chara, colType):\n    charaTrain, charaTest, diagTrain, diagTest = train_test_split(chara, diag, test_size = 0.2) #Split the data into train and test\n    \n    forest = RandomForestClassifier(n_estimators = 100) #Build a forest for this data\n    forest.fit(charaTrain, diagTrain) #Fit the forest\n    \n    predict = forest.predict(charaTest) #Make predictions for the test set\n    \n    print(\"Forest Accuracy for {}: {}\".format(colType, forest.score(charaTest, diagTest))) #Print the accuracy\n    print(\"Root Mean Square Error for {}: {}\".format(colType, np.sqrt(mean_squared_error(diagTest, predict)))) #Print the root mean square error\n    print(\"Classification Report for {}:\\n {}\".format(colType, classification_report(diagTest, predict, target_names = [\"B\", \"M\"]))) #Print a classification report","a0b7b802":"charact = characteristics.columns #Get the characteristics columns\n\ncolumnList = splitData(charact) #Split the data into se, mean, and worst\ncolTypes = [\"se\", \"mean\", \"worst\"] #Set a list of types to the corresponding column types in order\ni = 0 #Set an i variable to get the correct colType \n\n#For each column type, run a forest with just that type\nfor colList in columnList:\n    chara = cancer[colList] #Get the characteristics of just the columns of the specified type\n    runForest(diagnosis, chara, colTypes[i]) #Run a forest for this specific type\n    \n    i = i + 1 #Increase i so the column type remains consistent","1ce1407b":"Here, I would like to see if a classifier has an easier\/harder time when having all these worst\/mean\/deviation characteristics for the same variable versus just one or the other.","1d032101":"---","1bd993d0":"It seems a malignant tumor tends to have higher min and max values overall, though with overlap to beneign values. This fits most variables, but variables like concavity appear to completely overlap the malignant numbers with the beneign. I believe characteristics like that will have low importance to the classification, but I will have to see.","eaa8c450":"The forest of overall characteristics was able to attain a 97% accuracy on the test set. The worst characteristics appear to be what the forest dubbed most important, so it should be fair to assume the worsts, when alone, will do best in a new classifier.","876048d6":"## Fit a Random Forest Classifier","ac8b71af":"100 estimators appears to be best for accuracy. The worst appears to be the best predictor with an accuracy and recall being 96+% (recall being most important in this case since we need to identify these correctly). Mean is also very close, being 95+% in accuracy and recall (plus or minus depending on the run). This is based on the data, which only has about 500 entries, so I bet these results would change with more data. Despite this, I would say either the worst values or the mean values would do fine if there is no access to the other types.","22185165":"# Build a Forest for Each Worst\/SE\/Mean","29efc591":"# Breast Cancer Diagnostic Classification Project","f88014f0":"---","8c4f659d":"---","4d07e187":"Coded by Luna McBride","32701208":"## Train-Test Split","657bf9ba":"Everything is not null except the column Unnamed: 32, which is entirely null. I will drop Unnamed: 32. I will also drop the ID, since the ID is not something I want to test against","54aef30f":"# Check for Null Values","7984be5e":"---","965e6962":"# Build the Classifier (Full Data)","f3fc9a9b":"# Data Exploration"}}