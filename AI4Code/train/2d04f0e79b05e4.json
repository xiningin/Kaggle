{"cell_type":{"6816ae59":"code","ae7c2837":"code","b83f31f4":"code","ab1be072":"code","0da7a8a5":"code","f9ea44d3":"code","80493bc8":"code","fdb5a4fa":"code","3a7aa38a":"code","484cbfbf":"code","22cf31ff":"code","2eede062":"code","bd4c4f9d":"code","fc0e548b":"code","acdfdeaa":"markdown","b91c1e87":"markdown","0a415f22":"markdown","fece3814":"markdown","3072a3e4":"markdown","72d0e405":"markdown","f9034376":"markdown","48d0f85a":"markdown"},"source":{"6816ae59":"\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nfrom torchvision import datasets, transforms\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.nn.functional as F\n\nimport os\nprint(os.listdir(\"..\/input\/cell_images\/cell_images\/\"))","ae7c2837":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","b83f31f4":"train_transforms = transforms.Compose([transforms.Resize((120, 120)),\n                                       transforms.ColorJitter(0.05),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.RandomVerticalFlip(),\n                                       transforms.RandomRotation(20),\n                                       transforms.ToTensor(), \n                                       transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n                                     ])","ab1be072":"image_dir = \"..\/input\/cell_images\/cell_images\/\"\ntrain_set = datasets.ImageFolder(image_dir, transform=train_transforms)","0da7a8a5":"test_size = 0.2\n\nnum_train = len(train_set)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\n\ntest_split = int(np.floor((test_size) * num_train))\ntest_index, train_index = indices[:test_split - 1], indices[test_split - 1:]\n\ntrain_sampler = SubsetRandomSampler(train_index)\ntest_sampler = SubsetRandomSampler(test_index)\n\ntrain_loader = DataLoader(train_set, sampler=train_sampler, batch_size=104)\ntest_loader = DataLoader(train_set, sampler=test_sampler, batch_size=58)\nprint(\"Images in Test set: {}\\nImages in Train set: {}\".format(len(test_index), len(train_index)))","f9ea44d3":"classes=['infected','uninfected']","80493bc8":"def imshow(img):\n    img = img \/ 2 + 0.5  # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    \nimages, labels = next(iter(train_loader))\n\nfig = plt.figure(figsize=(25, 15))\n\nfor i in range(10):\n    ax = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[], title=classes[labels[i]])\n    imshow(images[i])\nplt.show()","fdb5a4fa":"class MosquitoNet(nn.Module):\n    \n    def __init__(self):\n        super(MosquitoNet, self).__init__()\n        \n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.layer3 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n            \n        self.fc1 = nn.Linear(64*15*15, 512)\n        self.fc2 = nn.Linear(512, 128)\n        self.fc3 = nn.Linear(128, 2)\n        self.drop = nn.Dropout2d(0.2)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = out.view(out.size(0), -1)    # flatten out a input for Dense Layer\n        out = self.fc1(out)\n        out = F.relu(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = F.relu(out)\n        out = self.drop(out)\n        out = self.fc3(out)\n        \n        return out\n        ","3a7aa38a":"model = MosquitoNet()\nmodel.to(device)\nerror = nn.CrossEntropyLoss()\nlearning_rate = 0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nprint(model)","484cbfbf":"num_epochs = 22\nbatch_size = 100 \n\nfor epoch in range(num_epochs):\n    train_loss = 0.\n    model.train()    # explictily stating the training\n    print(\"beginning training:..........\")\n    for i, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n        train = images.view(-1, 3, 120, 120)\n        outputs = model(train)\n        \n        optimizer.zero_grad()\n        loss = error(outputs, labels)\n        loss.backward()    #back-propagation\n        optimizer.step()\n        \n        train_loss += loss.item() * batch_size\n     \n    print(\"Epoch: {}, Loss: {:.4f}\".format(epoch + 1, train_loss \/ len(train_loader.dataset)))","22cf31ff":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\ncorrect = 0\ntotal = 0\nclass_total = [0 for _ in range(2)]\nclass_correct = [0 for _ in range(2)]\nbatch_size = 58\n# Lists used in Confusion Matrix\nactual = []\npredict = []\n\nmodel.eval()    # explicitly stating the testing \nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to (device)\n        actual.append(labels.data.tolist())\n        test = images.view(-1, 3, 120, 120)\n        outputs = model(test)\n        predicted = torch.max(outputs, 1)[1]\n        predict.append(predicted.data.tolist())\n        total += len(labels)\n        correct += (predicted == labels).sum().item()\n        # Calculating classwise accuracy\n        c = (predicted == labels).squeeze()\n        for i in range(batch_size):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n        \nprint(\"Accuracy on the Test set: {:.2f}%\".format(correct * 100 \/ total))\nprint()\nfor i in range(2):\n    print(\"Accuracy of {} :  {:.2f}%   [{} \/ {}]\".format(classes[i], class_correct[i] * 100 \/ class_total[i], \n                                           class_correct[i], class_total[i]))\n    \ndataiter = iter(test_loader)\nimages, labels = dataiter.next()\n\n\nfrom PIL import Image\nmodel.eval()\nwith torch.no_grad():\n    image = (\"\/input\/C101P62ThinF_IMG_20150923_170110_cell_22.png\")\n    image = image.to(device)\n    test = image.view(-1,3,120,120)\n    outputs = model(test)                                                              #left off, need input file string user interface, 2032 7\/17\n    predicted = torch.max(outputs, 1)[1]\n    im = Image.open(\"input\/C101P62ThinF_IMG_20150923_170110_cell_22.png\")\n    im.show()\n    print(outputs, predicted)\n    \n\n\n\n\n\n\n\n\n\n\n\nprint(\"testing finished. Thank you.\")","2eede062":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nimport itertools\n\n#flatten out 2D list into 1D\nactual = list(itertools.chain.from_iterable(actual))\npredict = list(itertools.chain.from_iterable(predict))\n","bd4c4f9d":"results = confusion_matrix(actual, predict)\nprint(\"Accuracy Score: \")\nprint(\"{:.4f}\".format(accuracy_score(actual, predict)))\nprint()\nprint(\"Report: \")\nprint(classification_report(actual, predict))\nprint()\nprint(\"Confusion Matrix: \")\nprint(pd.DataFrame(results, columns=[\"Predicted No\", \"Predicted Yes\"], index=[\"Actual No\", \"Actual Yes\"]))","fc0e548b":"import seaborn as sns\n\nsns.heatmap(results, cmap=\"magma\", annot=True, fmt=\"d\", cbar=False)","acdfdeaa":"> We have images in 2 classes: Infected and Uninfected","b91c1e87":"### Training a Model","0a415f22":"### Testing a model","fece3814":"> Displaying it as a plot","3072a3e4":"> Calculating a Confusion Matrix","72d0e405":"> Visualizing some Images...","f9034376":"> Making a model and defining error and optimizing algorithm.","48d0f85a":"**Importing libraries**"}}