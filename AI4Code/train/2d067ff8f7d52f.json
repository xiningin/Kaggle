{"cell_type":{"07be75a1":"code","444a5618":"code","118e9845":"code","a87b68e1":"code","18d035af":"code","4433f38c":"code","a52aa6bf":"code","7bd8c1d1":"code","7140b6d2":"code","2d44fd45":"code","0d4a6612":"code","8f423f4d":"code","d8213ca1":"code","e8cfdda9":"code","9bc3b5df":"code","4e6ebd5c":"code","d628b99f":"code","4d64e5c3":"code","980fc37c":"code","9fe257a5":"code","fab55229":"code","63c65d1a":"code","15b36fc1":"code","c7fce2e4":"code","148100f9":"code","a527d7d5":"code","d3852031":"code","9eeba341":"code","ef66dff3":"code","4f2410b1":"code","7971d8dc":"code","c6ea511b":"markdown","7fb4b61a":"markdown","b2606445":"markdown","8e0fbf6a":"markdown","b8748dcf":"markdown","6f048e0f":"markdown","9f1160c2":"markdown","acfc24da":"markdown","1c208c91":"markdown","c80a0302":"markdown","20acc4e3":"markdown","d0a45fd2":"markdown","68a5855e":"markdown","62548888":"markdown","e36d030a":"markdown","0d76e3cc":"markdown","bd890215":"markdown","437a2286":"markdown","8c46e897":"markdown","87e279eb":"markdown","01b3143b":"markdown","71b0e0e6":"markdown","49d0956b":"markdown","7d9dbc41":"markdown","8c5d6669":"markdown","0e6042c3":"markdown","15df1f64":"markdown","2a1977ee":"markdown","d8848ac7":"markdown","eb465f2d":"markdown","021f9b23":"markdown","068d38c3":"markdown","738a4c76":"markdown"},"source":{"07be75a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sb\n\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler\nfrom sklearn.utils import check_X_y\nimport sklearn.utils\nfrom sklearn.metrics import classification_report, roc_curve, auc, brier_score_loss\n\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\n\n# hide warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n","444a5618":"# converter for percent values\np2f = lambda s: np.NaN if not s else float(s.strip().strip(\"%\")) \/ 100\n\n# load data for accepted loans\ndata = pd.read_csv(\"..\/input\/accepted_2007_to_2018q4.csv\/accepted_2007_to_2018Q4.csv\", skipinitialspace=True,\n                   converters={\"int_rate\": p2f, \"revol_util\": p2f}, nrows=None, usecols=lambda c: False if c==\"url\" else True)\nprint(f\"Loaded {len(data)} rows, {len(data.columns)} columns.\")","118e9845":"dropna_cols = [\"funded_amnt\", \"avg_cur_bal\", \"bc_util\", \"loan_status\", \"dti\", \"inq_last_6mths\"]\nlen_prev = len(data)\ndata.dropna(subset=dropna_cols, inplace=True)\nn_dropped = len_prev-len(data)\nprint(f\"Dropped {n_dropped} rows ({100*n_dropped\/len_prev:.2f}%) with NaN values, {len(data)} rows remaining.\")","a87b68e1":"print(data[\"loan_status\"].unique())","18d035af":"data[\"defaulted\"] = data[\"loan_status\"].map(lambda x: 1 if x in [\"Charged Off\", \"Default\", \"Does not meet the credit policy. Status:Charged Off\"]\n                                                 else 0 if x in [\"Fully Paid\", 'Does not meet the credit policy. Status:Fully Paid']\n                                                 else -1)\nlen_prev = len(data)\ndata.query(\"defaulted != -1\", inplace=True)\nn_dropped = len_prev - len(data)\nprint(f\"Dropped {n_dropped} rows ({100*n_dropped\/len_prev:.2f}%) with invalid loan_status, {len(data)} rows remaining.\")","4433f38c":"sub_grades = sorted(data[\"sub_grade\"].unique())\nsub_grade_default_prob = {}\nsub_grade_n = {}\nfor sg in sub_grades:\n    sg_rows = data[data[\"sub_grade\"] == sg]\n    default_frac = sg_rows[\"defaulted\"].sum() \/ len(sg_rows)\n    sub_grade_default_prob[sg] = default_frac\n\ndata[\"sg_default_prob\"] = data[\"sub_grade\"].map(lambda x: sub_grade_default_prob[x])","a52aa6bf":"sb.catplot(x=\"sub_grade\", y=\"defaulted\", data=data, kind=\"bar\", aspect=5, order=sub_grades);","7bd8c1d1":"# fill NA values where appropriate\nfill_na_values = {\n    \"emp_length\": \"missing\",\n}\ndata.fillna(value=fill_na_values, inplace=True)","7140b6d2":"# convert binary values to 1\/0 (no need to one hot encode these)\ndata[\"hardship_flag01\"] = data[\"hardship_flag\"].map(lambda x: 1 if x == \"Y\" else 0)\ndata[\"joint_application_flag01\"] = data[\"application_type\"].map(lambda x: 0 if x==\"Individual\" else 1)\ndata[\"listed_as_whole_flag01\"] = data[\"initial_list_status\"].map(lambda x: 1 if x==\"w\" else 1)","2d44fd45":"individual_indices = data[\"joint_application_flag01\"] == 0\ndata.loc[individual_indices, \"annual_inc_joint\"] = data[individual_indices][\"annual_inc\"]\ndata.loc[individual_indices, \"dti_joint\"] = data[individual_indices][\"dti\"]\ndata.loc[individual_indices, \"verification_status_joint\"] = data[individual_indices][\"verification_status\"]\ndata.loc[individual_indices, \"revol_bal_joint\"] = data[individual_indices][\"revol_bal\"]\n\ndropna_cols = [\"annual_inc_joint\", \"dti_joint\", \"verification_status_joint\", \"revol_bal_joint\"]\nlen_prev = len(data)\ndata.dropna(subset=dropna_cols, inplace=True)\nn_dropped = len_prev-len(data)\nprint(f\"Dropped {n_dropped} rows ({100*n_dropped\/len_prev:.2f}%) with NaN values, {len(data)} rows remaining.\")","0d4a6612":"# Our goal is to predict the loan status, given by the boolean \"defaulted\" column\ny = data[\"defaulted\"].values\n\n# We define the columns used as features to train our model\n# continuous valued columns or \nx_columns_cont = [\"funded_amnt\", \"annual_inc\", \"annual_inc_joint\", \n                \"dti\", \"dti_joint\",\"fico_range_low\", \"fico_range_high\", \"inq_last_6mths\", \"mort_acc\",\n                \"open_acc\", \"pub_rec\", \"pub_rec_bankruptcies\", \"revol_bal\", \"revol_bal_joint\", \"revol_util\"]\n# binary categorical (0\/1) columns\nx_columns_bin = [\"joint_application_flag01\", \"listed_as_whole_flag01\"]\n\n# columns with categorical values that need to be one hot encoded\nx_columns_cat = [\"term\", \"purpose\", \"hardship_flag\", \"emp_length\", \"verification_status_joint\",\n                 \"addr_state\"]\n\n\nct = ColumnTransformer(transformers=[\n    (\"identity\", FunctionTransformer(func=lambda x: x, validate=False), x_columns_cont + x_columns_bin),\n    (\"onehot\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\"), x_columns_cat),\n])\n\nX = ct.fit_transform(data)\n\nX_col_labels = x_columns_cont + x_columns_bin + list(ct.named_transformers_[\"onehot\"].get_feature_names())\n\n# check for nans\/infs and other stuff\nX, y = check_X_y(X, y)","8f423f4d":"X_train, X_test, y_train, y_test, _, sg_default_prob_test = train_test_split(X, y, data[\"sg_default_prob\"], test_size=0.2, random_state=42)\ndel X, y, data","d8213ca1":"# Columns to train on\ncol_select = [True for c in X_col_labels]\nX_train_rest = X_train[:,col_select]\nX_test_rest = X_test[:,col_select]\nprint(f\"{len(X_train)} train, {len(X_test)} test samples\")\nscaler = StandardScaler()\nX_train_rest = scaler.fit_transform(X_train_rest)\nX_test_rest = scaler.transform (X_test_rest)\n\ndel X_train, X_test","e8cfdda9":"ndef_train = np.sum(y_train == 1)\nprint(f\"{ndef_train} ({100*ndef_train\/len(y_train):.2f}%) defaulted in training data set\")\n\ntrain_ndefault = np.sum(y_train)\nX_train_rest_nondefault = X_train_rest[y_train == 0]\nX_train_rest_balanced_nondefault = X_train_rest_nondefault[\n    sklearn.utils.random.sample_without_replacement(len(X_train_rest_nondefault),\n                                                    train_ndefault, random_state=42)]\n\nX_train_rest_balanced = np.concatenate([X_train_rest[y_train == 1], X_train_rest_balanced_nondefault])\n\ny_train_balanced = np.concatenate([np.ones(train_ndefault),np.zeros(train_ndefault)])\n# Now the classes are balanced:\nprint(f\"{np.sum(y_train_balanced == 1)} ({100*np.sum(y_train==1)\/len(y_train_balanced):.2f}%) defaulted in balanced training data set\")\n\n# define a function to correct probabilities coming from models trained on the balanced dataset\nbeta = ndef_train \/ (len(y_train) - ndef_train) # ratio of defaults to non-defaults\n# because of numerical errors the probability could be slightly above 1, so we clip the value\ncorrect_balanced_probabilities = lambda probs: np.clip(beta * probs \/ ((beta - 1) * probs + 1), 0, 1)","9bc3b5df":"# Train a random forest classifier\nrf_model = RandomForestClassifier(n_estimators=200, oob_score=True, class_weight=\"balanced\", n_jobs=-1, verbose=True)\nrf_model.fit(X_train_rest, y_train)\n\n# print some statistics\nprint(f\"Out of bag score: {rf_model.oob_score_}\")\ntest_pred_proba_rf = rf_model.predict_proba(X_test_rest)[:,1]\ntest_pred_rf = test_pred_proba_rf > 0.5\nprint(\"Test:\")\nprint(classification_report(y_test, test_pred_rf))\nprint(f\"False positives: {np.sum(np.logical_and(test_pred_rf == 1,y_test == 0))}\")\nprint(f\"False negatives: {np.sum(np.logical_and(test_pred_rf == 0,y_test == 1))}\")\nprint(f\"True positives: {np.sum(np.logical_and(test_pred_rf == 1,y_test == 1))}\")\nprint(f\"True negatives: {np.sum(np.logical_and(test_pred_rf == 0,y_test == 0))}\")\n\nrf_model_feature_importances = rf_model.feature_importances_\ndel rf_model","4e6ebd5c":"# Train a random forest classifier on the balanced dataset\nrf_b_model = RandomForestClassifier(n_estimators=200, n_jobs=-1, verbose=True, class_weight=None)\nrf_b_model.fit(X_train_rest_balanced, y_train_balanced)\n\n# predict\ntest_pred_proba_rf_b = correct_balanced_probabilities(rf_b_model.predict_proba(X_test_rest)[:,1])\n\ndel rf_b_model","d628b99f":"# Train a random forest classifier on the unbalanced dataset without weighting (this should be strictly worse(?))\nrf_ub_uw_model = RandomForestClassifier(n_estimators=200, n_jobs=-1, verbose=True, class_weight=None)\nrf_ub_uw_model.fit(X_train_rest, y_train)\n\n# predict\ntest_pred_proba_rf_ub_uw = rf_ub_uw_model.predict_proba(X_test_rest)[:,1]\n\ndel rf_ub_uw_model","4d64e5c3":"# Train a linear SVM classifier on a subset of the unbalanced dataset using \"balanced\" class weights\nX_train_rest_sample, y_train_sample = sklearn.utils.resample(X_train_rest, y_train, n_samples=20000, random_state=42)\n\nprint(f\"{np.sum(y_train_sample == 1)} ({100*np.sum(y_train_sample==1)\/len(y_train_sample):.2f}%) defaulted in subsampled training data set\")\n\nsvm_model = svm.SVC(kernel=\"linear\", class_weight=\"balanced\", probability=True)\nsvm_model.fit(X_train_rest_sample, y_train_sample)\n\n# predict\ntest_pred_proba_svm = svm_model.predict_proba(X_test_rest)[:,1]\n\ndel svm_model","980fc37c":"# Train a linear SVM classifier on a subset of the balanced dataset (using the full dataset is infeasible as the algorithm is O(n^2))\nX_train_rest_sample, y_train_sample =sklearn.utils.resample(X_train_rest_balanced, y_train_balanced, n_samples=20000, random_state=42)\n\nprint(f\"{np.sum(y_train_sample == 1)} ({100*np.sum(y_train_sample==1)\/len(y_train_sample):.2f}%) defaulted in subsampled balanced training data set\")\n\nsvm_b_model = svm.SVC(kernel=\"linear\", probability=True)\nsvm_b_model.fit(X_train_rest_sample, y_train_sample)\n\n# predict and correct probabilities\ntest_pred_proba_svm_b = correct_balanced_probabilities(svm_b_model.predict_proba(X_test_rest)[:,1])\n\ndel svm_b_model, X_train_rest_sample, y_train_sample","9fe257a5":"# Train an rbf SVM classifier on a subset of the balanced dataset (using the full dataset is infeasible as the algorithm is O(n^2))\n\nX_train_rest_sample, y_train_sample = sklearn.utils.resample(X_train_rest_balanced, y_train_balanced, n_samples=20000, random_state=42)\n\nsvm_rbf_b_model = svm.SVC(kernel=\"rbf\", probability=True)\nsvm_rbf_b_model.fit(X_train_rest_sample, y_train_sample)\ntest_pred_proba_svm_rbf_b = correct_balanced_probabilities(svm_rbf_b_model.predict_proba(X_test_rest)[:,1])\n\ndel svm_rbf_b_model, X_train_rest_sample, y_train_sample","fab55229":"# Train an rbf SVM classifier on a subset of the unbalanced dataset (using the full dataset is infeasible as the algorithm is O(n^2))\n\nX_train_rest_sample, y_train_sample = sklearn.utils.resample(X_train_rest, y_train, n_samples=20000, random_state=42)\n\nsvm_rbf_model = svm.SVC(kernel=\"rbf\", class_weight=\"balanced\", probability=True)\nsvm_rbf_model.fit(X_train_rest_sample, y_train_sample)\ntest_pred_proba_svm_rbf = svm_rbf_model.predict_proba(X_test_rest)[:,1]\n\ndel svm_rbf_model, X_train_rest_sample, y_train_sample","63c65d1a":"nn_train_b_x, nn_cal_b_x, nn_train_b_y, nn_cal_b_y = train_test_split(X_train_rest_balanced, y_train_balanced, test_size=0.1)\nnn_train_ub_x, nn_cal_ub_x, nn_train_ub_y, nn_cal_ub_y = train_test_split(X_train_rest, y_train, test_size=0.025)\n\n# we don't need biases in dense layers as that's covered by batchnorm\n# we don't need scaling in batchnorm layers as that's done by the weights in the next layer\nnn_model_b = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(512, input_shape=(nn_train_b_x.shape[1],), use_bias=False),\n    tf.keras.layers.BatchNormalization(scale=False),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dense(64, use_bias=False),\n    tf.keras.layers.BatchNormalization(scale=False),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dense(64, use_bias=False),\n    tf.keras.layers.BatchNormalization(scale=False),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dense(64, use_bias=False),\n    tf.keras.layers.BatchNormalization(scale=False),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dense(1, activation=tf.keras.activations.linear)\n])\n\nnn_model_ub = tf.keras.models.clone_model(nn_model_b)\n\nnn_model_b.compile(optimizer=tf.keras.optimizers.Adam(lr=0.05, decay=0.005),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nnn_model_b.fit(nn_train_b_x, nn_train_b_y, epochs=5)\n\nnn_model_ub.compile(optimizer=tf.keras.optimizers.Adam(lr=0.05, decay=0.005),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nnn_model_ub.fit(nn_train_ub_x, nn_train_ub_y, epochs=5)","15b36fc1":"from scipy.optimize import minimize\n\nsigmoid = lambda x: 1\/(1+np.exp(-x))\n\n# mean cross entropy loss\nce_cost = lambda y_true, proba: -np.sum((y_true * np.log(proba) + (1-y_true) * np.log(1 - proba))) \/ len(y_true)\n\ndef platt_cal_model(y_true, y_logits):\n    \"\"\"Returns a function that Platt scales the logits output by the model, giving (better) calibrated probabilities.\n    \n    See https:\/\/arxiv.org\/abs\/1706.04599 for more details \/ theoretical background.\n    The inputs y_true and y_logits are the true labels and outputs of the (uncalibrated) model on a dataset that was NOT used for training.\n    \"\"\"\n    \n    print(f\"Cost before: {ce_cost(y_true, sigmoid(y_logits)):.4f}\")\n    res = minimize(lambda x, y_logits, y_true: ce_cost(y_true, sigmoid(y_logits * x[0] + x[1])),\n                   [1.0,0.0], args=(y_logits, y_true), method=\"Nelder-Mead\", options={'maxiter': 500, 'disp': True}, )\n    a, b = res.x\n    \n    cal_function = lambda logits: sigmoid(logits * a + b)\n    \n    print(f\"a:{a}, b:{b}\")\n    print(f\"Cost after : {ce_cost(y_true, cal_function(y_logits)):.4f}\")\n    \n    return cal_function","c7fce2e4":"print(\"NN model trained on balanced dataset\")\n# squeezing the output of the model is necessary, otherwise its shape is (n, 1) and multiplying with a shape (n,) array (e.g. in ce_loss) creates a gigantic (n,n) result...\ntest_pred_logits_nn_b = nn_model_b.predict(X_test_rest).squeeze()\nprint(f\"brier score on test set without balancing correction: {brier_score_loss(y_test, sigmoid(test_pred_logits_nn_b)):.4f}\")\n\ntest_pred_proba_nn_b = correct_balanced_probabilities(sigmoid(test_pred_logits_nn_b))\nprint(f\"brier score on test set with balancing correction: {brier_score_loss(y_test, test_pred_proba_nn_b):.4f}\")\n\ncal_pred_logits_nn_b = nn_model_b.predict(nn_cal_b_x).squeeze()\n\nnn_b_calibration = platt_cal_model(nn_cal_b_y, cal_pred_logits_nn_b)\ntest_pred_proba_nn_b_platt = correct_balanced_probabilities(nn_b_calibration(test_pred_logits_nn_b))\nprint(f\"brier score on test set with balancing correction and calibration: {brier_score_loss(y_test, test_pred_proba_nn_b_platt):.4f}\")\n\nprint(\"\\n\\nNN model trained on unbalanced dataset\")\n\ntest_pred_logits_nn_ub = nn_model_ub.predict(X_test_rest).squeeze()\ntest_pred_proba_nn_ub = sigmoid(test_pred_logits_nn_ub)\nprint(f\"brier score on test set without calibration: {brier_score_loss(y_test, test_pred_proba_nn_ub):.4f}\")\n\ncal_pred_logits_nn_ub = nn_model_ub.predict(nn_cal_ub_x).squeeze()\n\nnn_calibration = platt_cal_model(nn_cal_ub_y, cal_pred_logits_nn_ub)\ntest_pred_proba_nn_ub_platt = nn_calibration(test_pred_logits_nn_ub)\nprint(f\"brier score on test set with platt calibration: {brier_score_loss(y_test, test_pred_proba_nn_ub_platt):.4f}\")","148100f9":"# calculate some model statistics\nmodels = [\n    (sg_default_prob_test, \"subgrade\"),\n    (test_pred_proba_rf_ub_uw, \"random forest raw\"),\n    (test_pred_proba_rf, \"random forest weighted\"),\n    (test_pred_proba_rf_b, \"random forest balanced\"),\n    (test_pred_proba_svm, \"linear SVM weighted\"),\n    (test_pred_proba_svm_b, \"linear SVM balanced\"),\n    (test_pred_proba_svm_rbf, \"RBF SVM weighted\"),\n    (test_pred_proba_svm_rbf_b, \"RBF SVM balanced\"),\n    (test_pred_proba_nn_b, \"NN balanced\"),\n    (test_pred_proba_nn_b_platt, \"NN balanced w. Platt\"),\n    (test_pred_proba_nn_ub, \"NN unbalanced\"),\n    (test_pred_proba_nn_ub_platt, \"NN unbalanced w. Platt\")\n]\nmodel_stats = {}\nfor proba, title in models:\n    fpr, tpr, thresholds = roc_curve(y_test, proba)\n    auc_score = auc(fpr,tpr)\n    brier = brier_score_loss(y_test, proba)\n    model_stats[title] = (fpr, tpr, auc_score, brier)\n","a527d7d5":"plt.figure(figsize=(15,7))\nfor proba, title in models:\n    fpr, tpr, auc_score, brier = model_stats[title]\n    plt.plot(fpr, tpr, label=f\"{title} (auc: {auc_score:.5f}, Brier: {brier:.5f})\")\n    \nplt.plot([0,1], [0,1], label=\"guess\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=\"lower right\");","d3852031":"plt.figure(figsize=(15,10))\nfpr_sg, tpr_sg, _, _ = model_stats[\"subgrade\"]\nfor proba, title in models:\n    fpr, tpr, auc_score, brier = model_stats[title]\n    plt.plot(fpr, tpr-np.interp(fpr, fpr_sg, tpr_sg), label=f\"{title} (auc: {auc_score:.5f}, Brier: {brier:.5f})\")\n    \nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate - Grade True Positive Rate')\nplt.legend(loc=\"lower right\");","9eeba341":"def plot_default_prob_pred_vs_actual(pred_proba, y, prob_min=0, prob_max=0.55, nsteps=15, title=None, ax=None):\n    \"\"\"Generate a calibration plot\"\"\"\n    prob_cutoffs = np.linspace(prob_min, prob_max, num=nsteps)\n    prob_cutoff_actual = []\n    prob_cutoff_n = []\n    xs = []\n    for lower_p, upper_p in zip(prob_cutoffs, prob_cutoffs[1:]):\n        cutoff_mask = np.logical_and(pred_proba > lower_p, pred_proba < upper_p) # predicted prob of default > p\n        n = np.sum(cutoff_mask)\n        defaulted_frac = np.sum(y[cutoff_mask])\/n\n        xs.append((lower_p + upper_p) \/ 2)\n        prob_cutoff_actual.append(defaulted_frac)\n        prob_cutoff_n.append(n)\n        \n    xs = np.array(xs) * 100\n    \n    if ax is None:\n        fig, ax = plt.subplots();\n    color = 'tab:red'\n    ax.set_title(title)\n    ax.set_xlabel('pred. min default prob [%]')\n    ax.set_ylabel('default prob [%]', color=color)\n    ax.plot(xs, np.array(prob_cutoff_actual) * 100, 1, color=color)\n    ax.plot([prob_min * 100,prob_max * 100], [prob_min * 100, prob_max * 100], color=\"gray\", ls=\"dashed\")\n    ax.tick_params(axis='y', labelcolor=color)\n\n    ax2 = ax.twinx()  # instantiate a second axes that shares the same x-axis\n    ax2.set_yscale(\"log\")\n\n    color = 'tab:blue'\n    ax2.set_ylabel('n', color=color)  # we already handled the x-label with ax1\n    ax2.plot(xs, prob_cutoff_n, color=color)\n    ax2.tick_params(axis='y', labelcolor=color)","ef66dff3":"fig, axes = plt.subplots(3, 4, figsize=(20,12))\naxes = [a for row in axes for a in row] # flatten array\nfor (proba, title), axis in zip(models, axes):\n    plot_default_prob_pred_vs_actual(proba, y_test, ax=axis, title=title)\n    \nfig.tight_layout()  # otherwise the right y-label is slightly clipped\n","4f2410b1":"plot_default_prob_pred_vs_actual(sigmoid(test_pred_logits_nn_b), y_test, title=\"balanced NN model without balancing correction\")","7971d8dc":"plt.figure(figsize=(17,3))\nax = plt.axes()\nsb.barplot(X_col_labels, rf_model_feature_importances, ax=ax)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90);","c6ea511b":"We fill columns only applicable for joint applications with the equivalent individiual values for individual applications and drop rows where there are still NaNs left after this","7fb4b61a":"We can plot the ROC curve for all our models and the one derived from the loan grade.\nThis shows the tradeoff between false positives and false negatives at different cutoff probabilities.\n\nThere are multiple ways to compare different models. Measures like the F1 score are less useful here, as we don't really know which cutoff to choose yet. We will look at the area under the ROC curve (AUC-ROC) and the Brier score (mean square error of predicted probability).","b2606445":"We train a random forest classifier on the balanced dataset and on the unbalanced dataset but without weighting","8e0fbf6a":"Subtracting the benchmark model (the one obtained from Lending Clubs loan grades) makes it easier to see the differences.","b8748dcf":"## Loan grade: The model to beat","6f048e0f":"We also train linear and RBF-kernel support vector machines on the balanced and unbalanced datasets.\n\nBecause the training time of these algorithms scales quite badly with the number of training samples we train them on a random subset of 20000 data points.","9f1160c2":"## Neural network model and Platt calibration","acfc24da":"Let's also train a neural network model using tensorflow.\n\nNeural networks, especially deeper ones, do not necessarily return calibrated probabilities out-of-the-box. For details see [Guo, Pleiss, Sin, Weinberger (2017)](https:\/\/arxiv.org\/abs\/1706.04599). This paper also compares multiple ways of transforming the network outputs to improve calibration.\n\nWe will use \"Platt calibration\" i.e. linearly transforming the logits (the raw output of the model) before applying the sigmoid function. The parameters of this linear transformation are optimized to minimize the cross entropy loss on a dataset that was NOT used to train the neural network model.\n\nAs we train the model on the balanced dataset we still need to apply the balancing correction afterwards.\n\n(The model used here is quite small, and actually provides pretty well calibrated estimates. Using Platt calibration here is mostly a learning exercise.)","1c208c91":"All the models are doing pretty well. For low false positive rates they are actually doing better than the grade model, and the neural network models are doing better throughout.\n\nThe NN models do best here, even though they are faster to train then most of the others and the structure was chosen quite arbitrarily. Platt calibration does not affect the ROC curve, as it just shifts the cutoff probability for each false positive rate.","c80a0302":"Some binary variables are encoded as strings, we convert this to 1\/0","20acc4e3":"The data already contains a predictor for the default probability: the loan grade (and the interest rate which is strongly correlated with it)\n\nWe can convert this to a default probability (by just taking the fraction of defaulted loans for each grade).\n\nThis is a great way to judge how well our model is working!\nWe shouldn't expect to be able to do much better than this prediction: The loan grade is already the output of a model (by Lending Club) that presumably uses at least all the data we have, and is probably heavily optimized.\n\nOur model will not use the \"grade\", \"sub_grade\", \"int_rate\", and \"installment\" columns, as otherwise we would just be copying the results of Lending Club's model. (\"installment\" together with \"funded_amount\" and \"term\" could be used to compute \"int_rate\".)","d0a45fd2":"We begin by dropping rows which contain NaN values in essential columns (for other columns we will handle NaN values later on).","68a5855e":"## Random forest models","62548888":"# Model definition & training","e36d030a":"The two classes (default and non-default) are quite imbalanced, which might cause problems depending on the type of model we use.\n\nWe have enough data (compared to the number of features) that we can just subsample to get a balanced dataset.\nHowever this means that the model is trained on data with a very different distribution from the test data.\nThis skews the predicted probabilities, which has to be corrected afterwards. We will use the procedure described in a paper by [Pozzolo, Caelen, Johnson, and Bontempi (2015)](https:\/\/www3.nd.edu\/~rjohns15\/content\/papers\/ssci2015_calibrating.pdf).\n\nAlternatively we can train models on the unbalanced dataset but weight the samples by the inverse ratio of occurence of their class label.\nWe will use both approaches and compare the results.","0d76e3cc":"The results don't seem too encouraging, especially the very low recall for defaults (high false negative rate). But that shouldn't really be a surprise: If we could predict defaults with  much more confidence these people wouldn't have gotten loans in the first place!\n\nLet's train some different types of models for comparison and look at the predicted *probability of default*.","bd890215":"Fill NaN values in some columns with placeholders:\n* Missing \"emp_length\" (employment length) could be a signal itself. ","437a2286":"Our goal is to predict the probability of loan default. The status of the loan is encoded in the loan_status column (how apropos) and can take the following values:","8c46e897":"# Preprocessing","87e279eb":"Split into train and test sets and scale all the features (to get mean=0 variance=1).","01b3143b":"If we wanted to e.g. define our own loan grades we would need to be able to accurately predict the probability of default, not just make predictions using some cutoff value. In other words we want a *well calibrated* estimate of the default probability. Here we plot the actual default probability of loans in the test set (calculated by binning on the estimated default probability and calculating the fraction of defaulted loans in each bin) against the estimated one. A 45\u00b0 line would correspond to perfect calibration.","71b0e0e6":"## Defining the target variable","49d0956b":"Here we define the function to do the Platt calibration. We use scipy.optimize to find the parameters *a* and *b* that minimize the cross-entropy loss of the predicted probabilities.","7d9dbc41":"A nice property of random forest models is that we can easily see which features contribute most to the prediction:","8c5d6669":"All the models are pretty well calibrated. For the models trained on the balanced dataset this is only true because we corrected the resulting probabilities. Plotting the same graph for e.g. the neural network model trained on the balanced dataset without applying the correction shows very bad calibration:","0e6042c3":"Let's load our dataset. Several columns contain percent values, we convert these directly to a fraction.","15df1f64":"## SVM models","2a1977ee":"We define loans with status \"Default\" and \"Charged Off\" as defaulted and those with status \"Fully Paid\" as non-defaulted. (The prefix \"Does not meet the credit policy\" is ignored, see [here ](https:\/\/forum.lendacademy.com\/?topic=2427.msg20813#msg20813) for more on this.)\n\nOur target variable is stored in the new column \"defaulted\", and we remove rows without a valid status there.","d8848ac7":"Let's start by training a random forest model on the unbalanced data, weighting the samples by their class weights.","eb465f2d":"# Model Evaluation","021f9b23":"In this Kernel we will build a number of models to predict the probability of loan default. The main goal is to experiment with different types of models and to see how we can obtain *well calibrated* probabilities from them (especially when dealing with unbalanced data).","068d38c3":"# Balancing the dataset and calibrating probabilities","738a4c76":"We select the columns that are used to train the models. These are selected using several criteria:\n* Only columns containing data that was known at the time the loan was issued. (For many columns this is not completely clear from the LCDataDictionary file.)\n* No column that contain the output of another model, i.e. int_rate, funded_amount,...\n* No categorical columns with a very high number of categories (e.g. emp_title with >400000, and zip_code with ~1000). This is mostly to make interpretation easier and to prevent overfitting problems (especially for the SVM models which are trained on a subset of the data). For a real application these would probably be included, maybe after some preprocessing (e.g. binning).\n\nCategorical columns are then one-hot encoded."}}