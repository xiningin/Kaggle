{"cell_type":{"85100851":"code","7c4f30f0":"code","bce06109":"code","bea9afb5":"code","4aced436":"code","21a424a7":"code","c00764ee":"code","3339e0f2":"code","409995b3":"code","12afd7f1":"code","f24816e9":"code","6daad0d2":"code","be4be042":"code","c23cbb1d":"code","3b94e326":"code","b9bf3f85":"code","9030bf58":"code","b0e77e4f":"code","20595258":"code","e8e75147":"code","f4271079":"code","7318cfa0":"markdown","6a9d19a4":"markdown","769e9030":"markdown","3dd12506":"markdown","4e178606":"markdown","08610f28":"markdown","f250af1c":"markdown","dfc66cfe":"markdown","fac29337":"markdown","b355a063":"markdown","bc1f76b8":"markdown","d739e59b":"markdown","b69c7206":"markdown","a8b8a16f":"markdown","003f121f":"markdown","ad8f4cd4":"markdown","1088b358":"markdown","a8bad398":"markdown","bf54628e":"markdown","d4d31119":"markdown","d9db9873":"markdown","20a1da94":"markdown","747ef801":"markdown","44204fbb":"markdown","6d596c13":"markdown","ff82148e":"markdown","ba1b2515":"markdown","4aaf5088":"markdown"},"source":{"85100851":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport numpy as np\nfrom numpy import mean, std\nimport pandas as pd\nfrom scipy.stats import norm,randint\nfrom math import ceil \nimport time\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import make_regression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier,GradientBoostingRegressor,RandomForestClassifier,RandomForestRegressor,VotingClassifier\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, LogisticRegression, PassiveAggressiveClassifier,RidgeClassifierCV,Ridge\nfrom sklearn.metrics import accuracy_score,auc,classification_report,confusion_matrix,mean_squared_error, precision_score, recall_score,roc_curve\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import cross_val_score,cross_val_predict,cross_validate,train_test_split,GridSearchCV,KFold,RepeatedKFold,learning_curve,RandomizedSearchCV,StratifiedKFold\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.svm import SVC, LinearSVC,SVR\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import ensemble, linear_model,neighbors, svm, tree, model_selection,preprocessing\nfrom sklearn import utils\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nimport lightgbm as lgbm\n\nSEED = 91\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7c4f30f0":"# Get data\ntrain=pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\ntest=pd.read_csv('..\/input\/30-days-of-ml\/test.csv',index_col='id')         \nfull_df=train.copy()\nfulltest_df=test.copy()\nprint('Data Import Complete')\n\n#reset variables\nfeaturereduction='no'\npreprocessing1='none'\npreprocessing2='none'\nmodelname='XGBRegressor'\nvalidation='none'\n\ny = train.target\ntrain.drop(['target'], axis=1, inplace=True)\nprint('Target data separated')","bce06109":"# Create an empty dataframe for performance results\nPreprocessPerformanced_df = pd.DataFrame(columns=['modelname','featurereduction','preprocessing1','preprocessing2','validation','datashape','trainsize','mean_squared_error'])\nfeaturereduction='no'\npreprocessing1='none'\npreprocessing2='none'\nmodelname='XGBRegressor'\nprint('Dataframe created')","bea9afb5":"## Feature Reduction\nInterstingfeatures = ['cont0','cont1','cont2','cont3', 'cont4', 'cont5', 'cont6','cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13','cat0', 'cat1', 'cat5', 'cat7','cat8','cat9'] #list of columns to keep\nUninterestingfeature=['cat2','cat3', 'cat4','cat6']\nfeaturereduction='yes'\n\ntrain=train[Interstingfeatures]\ntest=test[Interstingfeatures]\n\n#print(train.columns)\nprint('Features reduced')","4aced436":"# get dummies encoding\npreprocessing1='getdummies encoding'\n\n# get features\nuseful_features = [c for c in train.columns if c not in (\"id\",\"target\",\"kfold\")]\nprint(useful_features)\n\n# endode features\ntrain = pd.get_dummies(train[useful_features])\ntest=pd.get_dummies(test[useful_features])\n\nprint('Features encoded')","21a424a7":"# Ordinal encoding of Categorical data\npreprocessing1='ordinal encoding'\n\n# Convert categorical data to numeric data\nCfeatures = [col for col in useful_features if 'cat' in col] # dynamic list columns with categorical data\nprint(Cfeatures)\n# ordinal-encode categorical columns\nordinal_encoder = OrdinalEncoder()\ntrain[Cfeatures] = ordinal_encoder.fit_transform(train[Cfeatures])\ntest[Cfeatures] = ordinal_encoder.transform(test[Cfeatures])\nprint('All category columns converted to ordinal')","c00764ee":"# Ordinal & One-hot encoding of Categorical data\npreprocessing1='ordinal & one-hot encoding'\n\n# List of numeric columns\nNfeatures = [cname for cname in train.columns if train[cname].dtype in ['int64', 'float64']]\n# List of categorical columns\nCfeatures = [col for col in useful_features if 'cat' in col] # dynamic list columns with categorical data\n\n# high and low cardinality\nlow_card_columns=[cname for cname in train.columns if train[cname].nunique() < 10 and train[cname].dtype == \"object\"]\nhigh_card_columns=[cname for cname in train.columns if train[cname].nunique() >= 10 and train[cname].dtype == \"object\"]\n\n# ordinal_encode columns with high cardinality\nXtrain = train.copy()\nXtest = test.copy()\nordinal_encoder = OrdinalEncoder()\ntrain[high_card_columns] = ordinal_encoder.fit_transform(train[high_card_columns])\ntest[high_card_columns] = ordinal_encoder.transform(test[high_card_columns])\n\n# one-hot_encode columns with low cardinality\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train[low_card_columns]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(test[low_card_columns]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = train.index\nOH_cols_test.index = test.index\n\n# Relabel columns\nall_cols=OH_cols_train.columns\nnew_cols = [i for i in all_cols if isinstance(i, (int, float))]\nOH_cols_train=OH_cols_train[new_cols].add_prefix('cat_encode_')\nOH_cols_test=OH_cols_test[new_cols].add_prefix('cat_encode_')\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = train.drop(low_card_columns, axis=1)\nnum_X_test = test.drop(low_card_columns, axis=1)\n\n# Add one-hot encoded columns to numerical features\ntrain= pd.concat([num_X_train, OH_cols_train], axis=1)\ntest = pd.concat([num_X_test, OH_cols_test], axis=1)\n\nprint('Data encoded')","3339e0f2":"# One-hot encoding of Categorical data\npreprocessing1='one-hot encoding'\n\n# high and low cardinality\nlow_card_columns=[cname for cname in train.columns if train[cname].nunique() < 20 and train[cname].dtype == \"object\"]\nhigh_card_columns=[cname for cname in train.columns if train[cname].nunique() >= 20 and train[cname].dtype == \"object\"]\n\n# ordinal_encode columns with high cardinality\nXtrain = train.copy()\nXtest = test.copy()\nordinal_encoder = OrdinalEncoder()\ntrain[high_card_columns] = ordinal_encoder.fit_transform(train[high_card_columns])\ntest[high_card_columns] = ordinal_encoder.transform(test[high_card_columns])\n\n# one-hot_encode columns with low cardinality\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train[low_card_columns]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(test[low_card_columns]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = train.index\nOH_cols_test.index = test.index\n\n# Relabel columns\nall_cols=OH_cols_train.columns\nnew_cols = [i for i in all_cols if isinstance(i, (int, float))]\nOH_cols_train=OH_cols_train[new_cols].add_prefix('cat_encode_')\nOH_cols_test=OH_cols_test[new_cols].add_prefix('cat_encode_')\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = train.drop(low_card_columns, axis=1)\nnum_X_test = test.drop(low_card_columns, axis=1)\n\n# Add one-hot encoded columns to numerical features\ntrain= pd.concat([num_X_train, OH_cols_train], axis=1)\ntest = pd.concat([num_X_test, OH_cols_test], axis=1)\ntrain_full=train.copy()\n\nprint('Data encoded')","409995b3":"# Scale numeric data with StandardScaler\npreprocessing2='StandardScaler'\n\n# List of numeric columns\nNfeatures = [cname for cname in train.columns if train[cname].dtype in ['int64', 'float64']]\n\n## Scale\nscaler = preprocessing.StandardScaler()\ntrain[Nfeatures] = scaler.fit_transform(train[Nfeatures])\ntest[Nfeatures] = scaler.fit_transform(test[Nfeatures])\nprint('Scaling complete')","12afd7f1":"# Scale numeric data with StandardScaler\npreprocessing2='RobustScaler'\n\n# List of numeric columns\nNfeatures = [cname for cname in train.columns if train[cname].dtype in ['int64', 'float64']]\n\n## Scale\nscaler = preprocessing.RobustScaler()\ntrain[Nfeatures] = scaler.fit_transform(train[Nfeatures])\ntest[Nfeatures] = scaler.fit_transform(test[Nfeatures])\nprint('Scaling complete')","f24816e9":"# Normalize\npreprocessing2='Normalize'\n\n# List of numeric columns\nNfeatures = [cname for cname in train.columns if train[cname].dtype in ['int64', 'float64']]\n\n\n# Normalize\ntrain[Nfeatures]=preprocessing.normalize(train[Nfeatures], norm='l2')\ntest[Nfeatures]=preprocessing.normalize(test[Nfeatures], norm='l2')\nprint'Data normalised')","6daad0d2":"# separate numerical data\nuseful_features = [c for c in train.columns if c not in (\"id\",\"target\",\"kfold\")]\nNfeatures = [cname for cname in train.columns if train[cname].dtype in ['int64', 'float64']]\ntrain=train[Nfeatures]\ntest=test[Nfeatures]\nprint('Categorical data removed')","be4be042":"#y = train.target\n#train.drop(['target'], axis=1, inplace=True)\nprint('Target data separated')\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(train, y, train_size=0.75, test_size=0.25,random_state=0)\ntrain_size=0.75\ntest=test\nprint('Data split')","c23cbb1d":"# Model name set\nmodelname='XGBRegressor'\nvalidation='none'\ntrainingshape=train.shape\nprint(trainingshape)\n\n# Define the model \nmodel = XGBRegressor(n_estimators=1000, learning_rate=0.03, random_state=1, n_jobs=2)\n\n# Train the model \nmodel.fit(X_train, y_train, early_stopping_rounds = 20, eval_set=[(X_valid, y_valid)], verbose=False)\npreds_valid = model.predict(X_valid)\n\n# score model\nmse_score=mean_squared_error(y_valid, preds_valid, squared=False)\nprint(mse_score)\n\n# Add resuts to file\nPreprocessPerformanced_df = PreprocessPerformanced_df.append({'modelname':modelname,'featurereduction':featurereduction,'preprocessing1':preprocessing1,'preprocessing2':preprocessing2,'validation':validation,'datashape': str(X_train.shape),'trainsize':train_size,'mean_squared_error': mse_score},ignore_index=True)\nprint('Results added to comparison file')\n\n# generate predictions\npredictions = model.predict(test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.index,'target': predictions})\noutput.to_csv('basic_xgboost_submission.csv', index=False)\nprint('basic xgboost submission complete')","3b94e326":"# gradient boosting for regression in scikit-learn\n\n# instaciate model\nmodel = GradientBoostingRegressor()\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n\n# fit the model on the whole dataset\nmodel = GradientBoostingRegressor()\nmodel.fit(X_train, y_train)\n\npreds_valid = model.predict(X_valid)\n\n# score model\nmodelname='GradientBoostingRegressor'\nmse_score=mean_squared_error(y_valid, preds_valid, squared=False)\n\n# Add resuts to file\nPreprocessPerformanced_df = PreprocessPerformanced_df.append({'modelname':modelname,'featurereduction':featurereduction,'preprocessing1':preprocessing1,'preprocessing2':preprocessing2,'validation':validation,'datashape': str(X_train.shape),'trainsize':train_size,'mean_squared_error': mse_score},ignore_index=True)\nprint('Results added to comparison file')\nprint(mse_score)\n\n# make a single prediction\npredictions = model.predict(test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.index,'target': predictions})\noutput.to_csv('LGBM_submission.csv', index=False)\nprint('LGBM submission complete')","b9bf3f85":"# XGBRegressor with randomsearchcv validation\nmodelname='XGBRegressor'\nvalidation='RandomizedSearchCV'\n\n# create parameter grid\ngbm_param_grid = {\n    'booster': ['gbtree'],\n    'n_estimators': [400,10000],\n    'learning_rate': [0.03,0.04,0.05],\n    'reg_lambda': [0.001,0.01],\n    'max_depth': [2, 3, 4, 5],\n    'random_state'=[1,2]\n}\n\n# instanciate model\ngbm = XGBRegressor()\n# instanciate random search\nrandomized_mse = RandomizedSearchCV(param_distributions=gbm_param_grid, \n                                    estimator=gbm, \n                                    scoring='neg_mean_squared_error', \n                                    n_iter=50, \n                                    cv=3, \n                                    verbose=1,\n                                    random_state=0)\n# fit model\nrandomized_mse.fit(X_train,y_train)\n# print results\nprint(randomized_mse.best_params_, np.sqrt(np.abs(randomized_mse.best_score_)))\nprint(randomized_mse.best_estimator_)","9030bf58":"## build model based on results of cross fold validation ##\n\n# Model name set\nmodelname='XGBRegressor'\nvalidation='RandomizedSearchCV'\ntrainingshape=train.shape\nprint(trainingshape)\n\n# Define the model \n#model = XGBRegressor(booster='gbtree',n_estimators=1000,learning_rate=0.05,reg_lambda=0.001,reg_alpha=25.5,subsample=0.9,colsample_bytree=0.12,max_depth=3,random_state=SEED,n_jobs=2) #original\nmodel = XGBRegressor(booster='gbtree',n_estimators=10000,learning_rate=0.03,reg_lambda=0.00087,reg_alpha=23,subsample=0.787,colsample_bytree=0.12,max_depth=3,random_state=42,n_jobs=2,gamma=0, min_child_weight=1)\n#model = XGBRegressor(booster='gbtree',n_estimators=4000,learning_rate=0.11,reg_lambda=68.1,reg_alpha=15.7,subsample=0.98,colsample_bytree=0.13,max_depth=2,random_state=42,n_jobs=2,gamma=0, min_child_weight=1) #from kfold\n\n# Train the model \nmodel.fit(X_train, y_train, early_stopping_rounds = 20, eval_set=[(X_valid, y_valid)], verbose=False)\npreds_valid = model.predict(X_valid)\n\n# score model\nmse_score=mean_squared_error(y_valid, preds_valid, squared=False)\nprint(mse_score)\n\n# Add resuts to file\nPreprocessPerformanced_df = PreprocessPerformanced_df.append({'modelname':modelname,'featurereduction':featurereduction,'preprocessing1':preprocessing1,'preprocessing2':preprocessing2,'validation':validation,'datashape': str(X_train.shape),'trainsize':train_size,'mean_squared_error': mse_score},ignore_index=True)\nprint('Results added to comparison file')\n\n# generate predictions\npredictions = model.predict(test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.index,'target': predictions})\noutput.to_csv('cross_xgboost_submission4.csv', index=False)\nprint('crossfold xgboost submission complete')","b0e77e4f":"# xgboost with Cross fold validation\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {'learning_rate': [.01, .03, .05, .1, .25], #default: .3\n            'max_depth': [2,3,4], #default 2\n            'n_estimators': [400, 1000], \n            'booster':['gbtree']\n            #'seed': 5  \n             }\n# Instantiate model\nxgb = XGBRegressor()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\nxgb_cv = RandomizedSearchCV(xgb, param_dist, cv=10)\n\n# Fit model\nxgb_cv.fit(X_train, y_train)\nprint('Model fit')\n\n# Make prediction\npreds_valid = xgb_cv.predict(X_valid)\n\n# score model\nmse_score=mean_squared_error(y_valid, preds_valid, squared=False)\nprint(mse_score)\n\n# Print results\nprint(\"xgBoost Parameters: {}\".format(xgb_cv.best_params_))\nprint(\"Best score is {}\".format(xgb_cv.best_score_))\nacc_xgb_cv = round(accuracy_score(preds_valid, y_valid) * 100, 2)\nprint(acc_xgb_cv)\n\npredictions=xgb_cv.predict(test)\n\n# Save the predictions to a CSV file\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsample_submission.target = predictions\nsample_submission.to_csv(\"xgboost_with_cv_submission.csv\",index=False)\nprint('xgboost with cv submission complete')","20595258":"#import the data and shape\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nprint(train.shape,test.shape)\n\n## Feature Reduction\n#Interstingfeatures = ['cont0','cont2','cont3', 'cont4', 'cont5', 'cont6','cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cat0', 'cat1','cat5', 'cat7','cat8','cat9'] #list of columns to keep\n#Uninterestingfeature=['cont1','cat2', 'cat3', 'cat4','cat6']\nInterstingfeatures = ['cont0','cont1','cont2','cont3', 'cont4', 'cont5', 'cont6','cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cat1','cat5', 'cat7','cat8'] #list of columns to keep\nUninterestingfeature=['cat0','cat2', 'cat3', 'cat4','cat6','cat9']\nfeaturereduction='yes'\n\n#add kfold one columns and populate\ntrain['kfold']=-1\nkfold = model_selection.KFold(n_splits=10, shuffle= True, random_state = 1)\nfor fold, (train_indicies, valid_indicies) in enumerate(kfold.split(train)):\n    print(fold,train_indicies,valid_indicies)\n    train.loc[valid_indicies,'kfold'] = fold\n\nprint(train.kfold.value_counts()) #total data 300000 = kfold split :5 * 60000\n\n#output of train folds data\ntrain.to_csv(\"trainfold_10.csv\",index=False)\n\n#create variables\nfinal_predictions = []\nscore= []\n\n#features(categorical and numerical datas separate)\nuseful_features = [c for c in train.columns if c not in (\"id\",\"target\",\"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\ntest = test[useful_features]\n\nfor fold in range(10):\n    xtrain = train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #ordinal encode categorical colums and standardscaler is applied (mean0,sd=1)\n    ordinal_encoder = OrdinalEncoder()\n    \n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    scaler = preprocessing.StandardScaler()\n    #scaler = preprocessing.MinMaxScaler()\n    xtrain[numerical_cols] = scaler.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = scaler.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = scaler.transform(xtest[numerical_cols])\n    \n    #Model hyperparameter of XGboostRegressor\n    xgb_params = {'booster': 'gbtree',\n              'n_estimators': 5250,\n              'learning_rate': 0.125,#0.11\n              'subsample': 0.99,#0.98\n              'colsample_bytree': 0.135,#0.13\n              'max_depth': 2,\n              'reg_lambda': 100,\n              'reg_alpha': 15.7,\n              'random_state': 38,\n              }\n    \n    #model= XGBRegressor(**xgb_params,tree_method='gpu_hist',gpu_id=0,predictor='gpu_predictor') #use for gpu acceleration\n    model= XGBRegressor(**xgb_params) #use without gpu\n    model.fit(xtrain,ytrain)\n    preds_valid = model.predict(xvalid)\n    \n    #Training model apply the test data and predict the output\n    test_pre = model.predict(xtest)\n    final_predictions.append(test_pre)\n    \n    #Rootmeansquared output\n    rms = mean_squared_error(yvalid,preds_valid,squared=False)\n    \n    score.append(rms)\n    #way of output is display\n    print(f\"fold:{fold},rmse:{rms}\")\n\n#mean of repeation of fold data and identify the  mean and standard deviation \nprint(np.mean(score),np.std(score))\n\n#prediction of data\npredictions = np.mean(np.column_stack(final_predictions),axis=1)\n#print(predictions)\n\n# Save the predictions to a CSV file\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsample_submission.target = predictions\nsample_submission.to_csv(\"10fold_2500_minmax_xgboost_with_kfold_submission.csv\",index=False)\nprint('xgboost with kfold submission complete')\n\n# Add results to file\nPreprocessPerformanced_df = PreprocessPerformanced_df.append({'modelname':modelname,'featurereduction':featurereduction,'preprocessing1':preprocessing1,'preprocessing2':preprocessing2,'validation':\"kfold\",'datashape': str(X_train.shape),'trainsize':train_size,'mean_squared_error': mse_score},ignore_index=True)\nprint('Results added to comparison file')","e8e75147":"# alternate version of kfold with feature reduction and one-hot encoding on remaining columns #\n#import the data and shape\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nprint(train.shape,test.shape)\n\n## Feature Reduction\nInterstingfeatures = ['cont0','cont1','cont2','cont3', 'cont4', 'cont5', 'cont6','cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cat1','cat3','cat5','cat8'] #list of columns to keep\nUninterestingfeature=['cat0','cat2', 'cat4','cat6','cat7','cat9']\nordinalcolumns=['cat1','cat5',]\nonehotcolumns=['cat3','cat8']\nfeaturereduction='yes'\ntrain=train.drop(Uninterestingfeature, axis=1)\ntest=test.drop(Uninterestingfeature, axis=1)\n\n#add kfold one columns and populate\ntrain['kfold']=-1\nkfold = model_selection.KFold(n_splits=10, shuffle= True, random_state = 1)\nfor fold, (train_indicies, valid_indicies) in enumerate(kfold.split(train)):\n    print(fold,train_indicies,valid_indicies)\n    train.loc[valid_indicies,'kfold'] = fold\n\nprint(train.kfold.value_counts()) #total data 300000 = kfold split :5 * 60000\n\n#output of train folds data\ntrain.to_csv(\"trainfold_10.csv\",index=False)\n\n#create variables\nfinal_predictions = []\nscore= []\n\n#features(categorical and numerical datas separate)\nuseful_features = [c for c in train.columns if c not in (\"id\",\"target\",\"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\ntest = test[useful_features]\n\nfor fold in range(10):\n    xtrain = train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #ordinal encode categorical colums and standardscaler is applied (mean0,sd=1)\n    ordinal_encoder = OrdinalEncoder()\n    \n    xtrain[ordinalcolumns] = ordinal_encoder.fit_transform(xtrain[ordinalcolumns])\n    xvalid[ordinalcolumns] = ordinal_encoder.transform(xvalid[ordinalcolumns])\n    xtest[ordinalcolumns] = ordinal_encoder.transform(xtest[ordinalcolumns])\n    #####\n    # One-hot encoding of Categorical data\n    preprocessing1='one-hot encoding'\n\n    # cardinality\n    low_card_columns=[cname for cname in xtrain.columns if xtrain[cname].nunique() < 20 and xtrain[cname].dtype == \"object\"]\n\n    # one-hot_encode columns with low cardinality\n    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    OH_cols_xtrain = pd.DataFrame(OH_encoder.fit_transform(xtrain[onehotcolumns]))\n    OH_cols_xvalid = pd.DataFrame(OH_encoder.transform(xvalid[onehotcolumns]))\n    OH_cols_xtest = pd.DataFrame(OH_encoder.transform(xtest[onehotcolumns]))\n\n    # One-hot encoding removed index; put it back\n    OH_cols_xtrain.index = xtrain.index\n    OH_cols_xvalid.index = xvalid.index\n    OH_cols_xtest.index = xtest.index\n\n    # Relabel columns\n    all_cols=OH_cols_xtrain.columns\n    new_cols = [i for i in all_cols if isinstance(i, (int, float))]\n    OH_cols_xtrain=OH_cols_xtrain[new_cols].add_prefix('cat_encode_')\n    OH_cols_xvalid=OH_cols_xvalid[new_cols].add_prefix('cat_encode_')\n    OH_cols_xtest=OH_cols_xtest[new_cols].add_prefix('cat_encode_')\n\n    # Remove categorical columns (will replace with one-hot encoding)\n    num_xtrain = xtrain.drop(onehotcolumns, axis=1)\n    num_xvalid = xvalid.drop(onehotcolumns, axis=1)\n    num_xtest = xtest.drop(onehotcolumns, axis=1)\n\n    # Add one-hot encoded columns to numerical features\n    xtrain= pd.concat([num_xtrain, OH_cols_xtrain], axis=1)\n    xvalid = pd.concat([num_xvalid, OH_cols_xvalid], axis=1)\n    xtest = pd.concat([num_xtest, OH_cols_xtest], axis=1)\n    #####\n    #scaler = preprocessing.StandardScaler()\n    #scaler = preprocessing.RobustScaler()\n    scaler = preprocessing.MinMaxScaler()\n    xtrain[numerical_cols] = scaler.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = scaler.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = scaler.transform(xtest[numerical_cols])\n    \n    #Model hyperparameter of XGboostRegressor\n    xgb_params = {'booster': 'gbtree',\n              'n_estimators': 5250,\n              'learning_rate': 0.12,#0.11\n              'subsample': 0.99,#0.98\n              'colsample_bytree': 0.135,#0.13\n              'max_depth': 2,\n              'reg_lambda': 75,\n              'reg_alpha': 15.7,\n              'random_state': 38,\n              }\n    \n    #model= XGBRegressor(**xgb_params,tree_method='gpu_hist',gpu_id=0,predictor='gpu_predictor') #use for gpu acceleration\n    model= XGBRegressor(**xgb_params) #use without gpu\n    model.fit(xtrain,ytrain)\n    preds_valid = model.predict(xvalid)\n    \n    #Training model apply the test data and predict the output\n    test_pre = model.predict(xtest)\n    final_predictions.append(test_pre)\n    \n    #Rootmeansquared output\n    rms = mean_squared_error(yvalid,preds_valid,squared=False)\n    \n    score.append(rms)\n    #way of output is display\n    print(f\"fold:{fold},rmse:{rms}\")\n\n#mean of repeation of fold data and identify the  mean and standard deviation \nprint(np.mean(score),np.std(score))\n\n#prediction of data\npredictions = np.mean(np.column_stack(final_predictions),axis=1)\npredictions2 = np.median(np.column_stack(final_predictions),axis=1)\n#print(predictions)\n\n# Save the predictions to a CSV file\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsample_submission.target = predictions\nsample_submission.to_csv(\"5250_oneordinal_xgboost_with_preprossessing_kfold_minmax_submission.csv\",index=False)\nprint('xgboost with alternate kfold submission complete')\n\n# Add resuts to file\nPreprocessPerformanced_df = PreprocessPerformanced_df.append({'modelname':modelname,'featurereduction':'yes','preprocessing1':'onehot','preprocessing2':'minmax','validation':'kfold','datashape': str(xtrain.shape),'mean_squared_error': np.mean(score)},ignore_index=True)\nprint('Results added to comparison file')","f4271079":"# sort the dataframe\nPreprocessPerformanced_df  = PreprocessPerformanced_df.sort_values(\"mean_squared_error\",ascending=True)\nprint(PreprocessPerformanced_df)\n\n# save the dataframe to csv\nfilename=('preprocessing_'+time.strftime('%Y_%m_%d_%H_%M') + '.csv')\n#output = pd.DataFrame({'Model': BasicModelPerformanced_df.index,'mean_squared_error': BasicModelPerformanced_df.mean_squared_error})\noutput=PreprocessPerformanced_df\noutput.to_csv(filename, index=False)\nprint('\\nreview saved as',filename)","7318cfa0":"## Get dummies encoding","6a9d19a4":"## Normalization","769e9030":"# Stage 5 : Modelling\nRun code for each model that you want to compare.","3dd12506":"# Stage 2 : Preprocess (part 1)\nChoose and run one method for converting Cetegorical data to numeric data in this section if required.","4e178606":"## Split data","08610f28":"## Basic modelling","f250af1c":"## Import modules","dfc66cfe":"### xgboost model (with kfold validation)","fac29337":"## Ordinal & one-hot encoding of categorical data","b355a063":"## All-in-one xgboost with cross fold validation","bc1f76b8":"Numerical Data - Normalization","d739e59b":"## Advanced modelling","b69c7206":"## Remove none numeric columns\nXGBRegressor doesn't take none categorical data so lets remove it.\n","a8b8a16f":"## GradientBoostingRegressor","003f121f":"## One-hot encoding of categorical data","ad8f4cd4":"## Ordinal encoding of categorical data","1088b358":"### xgboost with RandomizedSearchCV","a8bad398":"## About this notebook\n\nThis is my second notebook for the 30 day ml challenge. The first notebook did a lot of data exploration and modelling, but got to be very long. This notebook will skip data exploration and stick to 1 model concentrating on applying several different approaches to data engineering and comparing the results.\n\nTo use the notebook, follow the instructions for each of steps 1-6. This will allow you to set up different preprocessing options and then run xgboost models with severl different validation technices to compare the results. You can repeat this process several times to generate a report comparing the results based on the techniques you ran.  \n\nWithin this project data engneering is limited to blind processing. Effectively all of the data is annonyous so we can't apply any domain knowledge and are simply using trial and error to see which tecnhiques improve score and which don't.\n\nI found that i could get a score of public score of as low as 72.3 using the xgboostregressor with default settings simply by changing the categorical features to include, how they are encode and which method of scaling I used. This score could be further reduced using kfold validation and tuning the model parameters.\n\nThe competition rules allowed any 2 entries submitted to the pubic leaderboard to be taken forward to the final competition. My final submissions to the competition were my best scoring entry and an mean of my best 10 scoring entries from the public leaderboard. These entries repressended my 2nd and 3rd best submissions on the privateboard). When the competition closed and the private leaderboard scores were revealed, my ranking droppped from 477th place to 692th place of 6855 competitors. My best entry scored 0.71594, the winner scoring 0.71533, a score of 0.00061 lower than my submission. ","bf54628e":"## Feature reduction\nLooking at the feature importance and mutuality cat 2,3,4 and 5 do add to the model and can be ignored.","d4d31119":"# Credit where credit's due\n\nThe following notebook's were super inspirational in creating this notebook, and gave me insights I may not have otherwise come up with. Thank you all for sharing your code it's awesome!\n\nFeature Engineering https:\/\/www.kaggle.com\/pranav2109\/featureengineering-ensemble-xgbpipeline by pranav2109 (Pranav Agarwal)\n\nXGBoost Kfold https:\/\/www.kaggle.com\/venkatkumar001\/30days-xgb-eda by Venkatkumar R\n\n## History\n\nThis notebook is build on top of a notebook that I had previous created which concentrated on data exploration and visualisation you can see that here: https:\/\/www.kaggle.com\/davidcoxon\/predicting-insurrance-30-days-of-machine-learning\/edit\/run\/72699320\n\nThis is version 1 of this book.\n\nFuture versions are planned that will look at feature reduction and normalization. \n\n## Using this code\nIf you found this notebook helpful please upvote. \nIf you have any comments, requests or suggestions please comment.\n\n## Conclusion\nUsing minimal feature engineering, k fold validation and a single xgboost model it was possible to achive a public score of 0.71739 and get into the top 500 final enties in a month long Kaggle competition in which 7,804 competitors made 57,142 entries. While It did not come up with a top 10 entry it was only 0.0045 behind the top result of 0.71694. \n","d9db9873":"# Stage 1 : Setup\nRun all code in this stage.","20a1da94":"## Scaling (standard scaler)","747ef801":"## Stage 6 : Review performance","44204fbb":"## Get data","6d596c13":"# Stage 3 : Preprocess (part 2)\nChoose and run one method for preprocessing data in this section if required. ","ff82148e":"## Create file to compare results\n(only run this once per session - or if you want to reset your comparisons)","ba1b2515":"## Scaling (robust scaler)","4aaf5088":"# Stage 4 : Final preparation\nRun all code in this stage."}}