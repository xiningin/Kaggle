{"cell_type":{"66039357":"code","8c2e3bdd":"code","b113e3be":"code","0a47de2a":"code","abc753d6":"code","6ba2591c":"code","858188b4":"code","d99ded07":"code","4c70f2ce":"code","755e0932":"code","d928f511":"code","9573b808":"code","8a64899f":"code","6af1104a":"code","0d5410a5":"code","8233964e":"code","68e285cb":"code","36fa7a14":"code","1adbd5b9":"code","5c1673e2":"code","97102733":"code","3bc948f8":"code","b64595f7":"code","79265629":"code","268808fe":"code","be273327":"code","3e38e4fe":"code","584a3966":"code","f685599a":"code","9eea6c85":"code","63f8080e":"code","b2b15e98":"code","5912bf9f":"code","b4b93d9b":"code","255d2f28":"code","64f89a01":"code","fcf1e6a7":"code","10a49639":"code","fdb07b19":"code","a62401fa":"code","c1108ef4":"code","2205ea39":"code","fd5806ae":"code","29dc70d5":"code","83781e40":"code","01270cf5":"code","3ce9490d":"code","b64c2d00":"code","4dcc5e39":"code","63741872":"code","745630d7":"code","7a036467":"code","f591b1fc":"code","790546da":"code","a6ee86f4":"code","68cd08e4":"code","9c07993f":"code","c616ea7c":"code","1020425a":"code","2af1659d":"code","df530bc7":"code","60cea6bc":"code","02318313":"code","b5a308b9":"code","4f487b4a":"code","e052011b":"code","45d24f2b":"code","8d1b75a6":"code","6909f505":"code","cf895f0e":"code","0deaf532":"code","aa5868a3":"code","720be33a":"code","e7b73fc1":"code","ee520ccf":"code","f8828a7c":"code","ae0c7c57":"code","9cddc173":"code","72ee7732":"code","ef5555c5":"code","e8dc64c6":"code","14c3b3fa":"code","573da9a2":"code","b81fd14e":"code","be2b4d83":"code","943fdb34":"code","58a2a2ac":"code","bd733cfa":"code","85c3de42":"code","378d4f90":"code","e270999a":"code","1fa7c868":"code","1b553a8f":"code","b9193d76":"code","8640702e":"markdown","1fe11e3e":"markdown","eebd9611":"markdown","ac72eadc":"markdown","65d5fbd1":"markdown","4f8425a2":"markdown","52d75ece":"markdown","0a3d8d2a":"markdown","5f765864":"markdown","65eb667e":"markdown","9fdb5ad5":"markdown","ee4ac313":"markdown","68ce44ad":"markdown","17024328":"markdown","7e223b97":"markdown","670ef4f3":"markdown","999cfe5a":"markdown","f69aec8e":"markdown","b365ee2d":"markdown","bfc3e18d":"markdown","77224472":"markdown","d958d953":"markdown","c3101b03":"markdown","b8c4aeae":"markdown","e9fac0f3":"markdown","58611153":"markdown","b5c06039":"markdown","4ed14e2a":"markdown","ef4aeedc":"markdown","d3436686":"markdown","8fd1b0ee":"markdown","bfc2ab7c":"markdown","c0667552":"markdown","39ecea04":"markdown","51f40bff":"markdown","58f37551":"markdown","1c5aeacd":"markdown","4e70b9ad":"markdown","bd69aa8c":"markdown","140ac8b3":"markdown","8fa6f898":"markdown","ddbcc2d3":"markdown","15107aff":"markdown","cf59e1d7":"markdown","a316efd2":"markdown"},"source":{"66039357":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c2e3bdd":"# Importing Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error","b113e3be":"# setting dimensions for plot\nsns.set(rc={'figure.figsize':(11.7, 8.27)})\nsns.set(rc={'figure.figsize':(5,4)})","0a47de2a":"dataset = pd.read_csv('..\/input\/car-pricing-prediction\/cars_sampled.csv')\ndataset2 = dataset.copy() # keeping a deepcopy of original dataset","abc753d6":"# examining dataset\ndataset.info()","6ba2591c":"pd.set_option('display.float_format', lambda x: '%.3f' % x)\n# to convert all the displays in .000 format and avoid scientific values\n\ndataset.describe()","858188b4":"\ndataset = dataset.drop(['dateCrawled', 'dateCreated', 'postalCode',\n                       'lastSeen', 'name'], axis=1)","d99ded07":"dataset.drop_duplicates(keep='first', inplace=True)","4c70f2ce":"print(np.unique(dataset['yearOfRegistration']))\n\nprint(\"\\n\\n\")\n\nprint(dataset['yearOfRegistration'].value_counts())","755e0932":"yearwise_count = dataset['yearOfRegistration'].value_counts().sort_index()\nyearwise_count","d928f511":"sum(dataset['yearOfRegistration'] > 2021)\n# to see how many values are from future (non-relevant)","9573b808":"sum(dataset['yearOfRegistration'] < 1900)\n# too old or wrongly entered year of Registration","8a64899f":"# plot to see how car prices varied over the years\nsns.regplot(x='yearOfRegistration', y='price', scatter=True, \n           fit_reg=False, data=dataset)\n# seaborn's regplots are scatter plots","6af1104a":"sns.distplot(dataset['price'], kde=True, bins=200, color=\"k\")\n# seaborn's distplots' are same as histogram in matplotlib","0d5410a5":"price_count = dataset['price'].value_counts().sort_index()\nprice_count","8233964e":"sns.boxplot(y=dataset['price'])","68e285cb":"# setting a price range\nprint(len(dataset[dataset['price'] < 100].index))\nprint(len(dataset[dataset['price'] > 100].index))\nprint(sum(dataset['price'] > 150000))","36fa7a14":"sns.boxplot(y=dataset[(dataset['price'] > 50000) & (dataset['price'] < 150000)]['price'])","1adbd5b9":"powerPS_count = dataset['powerPS'].value_counts().sort_index()\npowerPS_count","5c1673e2":"sns.distplot(dataset['powerPS'], kde=True, bins=100)","97102733":"dataset['powerPS'].describe()","3bc948f8":"sns.regplot(x='powerPS', y='price', scatter=True, fit_reg=False,\n           data=dataset)","b64595f7":"print(sum(dataset['powerPS'] > 500))\nprint(sum(dataset['powerPS'] < 10))","79265629":"dataset = dataset[(dataset.yearOfRegistration <= 2021)\n                 & (dataset.yearOfRegistration >= 1900)\n                 & (dataset.price >= 100)\n                 & (dataset.price <= 15000)\n                 & (dataset.powerPS >= 10)\n                 & (dataset.powerPS <= 500)]\n","268808fe":"np.unique(dataset['monthOfRegistration'])","be273327":"dataset['monthOfRegistration'] \/= 12\ndataset['monthOfRegistration'] = round(dataset['monthOfRegistration'], 2)\nnp.unique(dataset['monthOfRegistration'])","3e38e4fe":"current_year = 2021\ndataset['ageFromRegistration'] = (current_year - dataset['yearOfRegistration'])\ndataset['ageFromRegistration'] += dataset['monthOfRegistration']\ndataset['ageFromRegistration'].describe()","584a3966":"# dropping month and year columns as new derived feature created which holds their importance\ndataset = dataset.drop(['monthOfRegistration', 'yearOfRegistration'], axis=1)\ndataset.head()","f685599a":"## ageFromRegistration\nsns.distplot(dataset['ageFromRegistration'], kde=True, bins=20)","9eea6c85":"sns.boxplot(y=dataset['ageFromRegistration'])","63f8080e":"sns.distplot(dataset['price'], kde=True, bins=100)","b2b15e98":"sns.boxplot(y=dataset['price'])","5912bf9f":"sns.distplot(dataset['powerPS'], kde=True, bins=100)","b4b93d9b":"sns.boxplot(y=dataset['powerPS'])","255d2f28":"## age VS price\nfig, ax = plt.subplots()\nsns.regplot(x='ageFromRegistration', y='price', scatter=True,\n           fit_reg=True, data=dataset, ax=ax) \n# regplot -- scatter plot in seaborn\n# ax.set(xlim=(0,80))\n# ax.set_ylim(0,30000)\nplt.show()","64f89a01":"## powerPS vs price\nsns.regplot(x='powerPS', y='price', scatter=True, fit_reg=True,\n           data=dataset)","fcf1e6a7":"dataset['seller'].value_counts()","10a49639":"pd.crosstab(dataset['seller'], columns='count', normalize=True)","fdb07b19":"sns.countplot(x='seller', data=dataset)","a62401fa":"dataset['offerType'].value_counts()","c1108ef4":"sns.countplot(x='offerType', data=dataset)","2205ea39":"pd.crosstab(dataset['abtest'], columns='count', normalize=True)","fd5806ae":"sns.boxplot(x='abtest', y='price',data=dataset)","29dc70d5":"pd.crosstab(dataset['vehicleType'],columns='count',normalize=True)","83781e40":"plt.figure(figsize=(10,8))\nsns.countplot(x='vehicleType', data=dataset)","01270cf5":"plt.figure(figsize=(10,8))\nsns.boxplot(x=dataset['vehicleType'],  y=dataset['price'],data=dataset)","3ce9490d":"pd.crosstab(dataset['gearbox'], columns='count', normalize=True) * 100","b64c2d00":"sns.boxplot(x='gearbox', y='price',  data=dataset)","4dcc5e39":"dataset['gearbox'].describe(include= ['O'])","63741872":"cross_tab = pd.crosstab(dataset['model'], columns='count', normalize=True)*100\ncross_tab.sort_values(by='count', ascending=False)","745630d7":"plt.figure(figsize=(28,18))\nsns.boxplot(x='model',  y='price',data=dataset)\nplt.xticks(rotation='vertical')\nplt.show()","7a036467":"pd.crosstab(dataset['fuelType'], columns='count', normalize=True)*100","f591b1fc":"sns.boxplot(x='fuelType', y='price', data=dataset)","790546da":"pd.crosstab(dataset['brand'], columns='count', normalize=True)*100","a6ee86f4":"plt.figure(figsize=(18,16))\nsns.boxplot(x='brand', y='price', data=dataset)\nplt.xticks(rotation='vertical')\nplt.show()","68cd08e4":"pd.crosstab(dataset['notRepairedDamage'], columns='count', normalize=True)*100","9c07993f":"sns.boxplot(x='notRepairedDamage',  y='price', data=dataset)","c616ea7c":"dataset = dataset.drop(['seller', 'offerType', 'abtest'], axis=1)","1020425a":"dataset.corr()\n# we are getting correlation among numerical variable only","2af1659d":"sns.heatmap(dataset.corr(), annot=True)","df530bc7":"dataset_omit = dataset.dropna(axis=0)","60cea6bc":"dataset_omit.info()","02318313":"# Encoding categorical variables\ndataset_omit = pd.get_dummies(dataset_omit, drop_first=True)\ndataset_omit.info()","b5a308b9":"dataset_omit.head()","4f487b4a":"# splitting dependent and independent features\nX = dataset_omit.drop(['price'], axis=1)\ny = dataset_omit.price","e052011b":"# col_names = X.columns\n\n# scaler = MinMaxScaler()\n# x_scaled = scaler.fit_transform(X)\n# X_final = pd.DataFrame(x_scaled, columns = col_names)\n# X_final.head()","45d24f2b":"prices = pd.DataFrame({'1. Before':y, '2. After':np.log(y)})\n# plt.figure(figsize=(12,10))\nprices.hist(bins=10)","8d1b75a6":"# transforming prices to logarithmic values to avoid huge ranges\ny = np.log(y)","6909f505":"# train test split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=1)\n","cf895f0e":"base_pred = np.mean(y_test)\nprint(base_pred)","0deaf532":"# repeating the same value till length of the test data\nbase_pred = np.repeat(base_pred, len(y_test))\nbase_pred","aa5868a3":"# finding the root mean squared error\nbase_rmse_ommited1 = np.sqrt(mean_squared_error(y_test, base_pred))\nprint(base_rmse_ommited1)","720be33a":"lr_model = LinearRegression()\nlr_model.fit(x_train, y_train)","e7b73fc1":"# predicting the test data\nlr_pred = lr_model.predict(x_test)\n\nlr_rmse1 = np.sqrt(mean_squared_error(y_test, lr_pred))\nlr_rmse1","ee520ccf":"# R-squared value\n# R-squared is a statistical measure of how close the data are to the fitted regression line\n# The value varies for  0  to 1.\n# higher values indicates the model has better  fitted the data, model was able to explain all variability of our\n# response (dependent) data around its mean, points are closer to regression line\nr2_linR_test1 = lr_model.score(x_test, y_test) #test values together\nr2_linR_train1 = lr_model.score(x_train, y_train) #train values together\nprint(r2_linR_test1) #0.714851702564\nprint(r2_linR_train1) # 0.722351030384","f8828a7c":"# Regression diagnostic - Residual plot analysis\nresiduals1 = y_test - lr_pred\nsns.regplot(x=lr_pred, y=residuals1, scatter=True, fit_reg=True, label='residual vs pred')\nsns.regplot(x=lr_pred, y=y_test, scatter=True, fit_reg=True, label='residual vs y-true')\nplt.legend(fontsize=11)\n# residuals1.describe()# mean=0.002 which shows the Y_test and predicted values  are very close","ae0c7c57":"rf = RandomForestRegressor(n_estimators=100, max_features='auto',\\\n                           max_depth=100, min_samples_split=10,\\\n                           min_samples_leaf=4,random_state=1)","9cddc173":"# model\nmodel_rf1 = rf.fit(x_train, y_train)\n\n# Predicting  model on test  set\ncars_predictions_rf1 = rf.predict(x_test)\n\n# computing MSE and RMSE\nrf_mse1 = mean_squared_error(y_test, cars_predictions_rf1)\nrf_rmse1 = np.sqrt(rf_mse1)\nprint(rf_rmse1) ","72ee7732":"# Regression diagnostic - Residual plot analysis\nresiduals1 = y_test - cars_predictions_rf1\nsns.regplot(x=cars_predictions_rf1, y=residuals1, scatter=True, fit_reg=True, label='residual vs pred')\nsns.regplot(x=cars_predictions_rf1, y=y_test, scatter=True, fit_reg=True, label='residual vs y-true')\nplt.legend(fontsize=11)\n# residuals1.describe()# mean=0.002 which shows the Y_test and predicted values  are very close","ef5555c5":"# lasso regression\nlasso_r = Lasso()\n\nlasso_r.fit(x_train, y_train)\n\n# predicting for test data\ntest_data_pred = lasso_r.predict(x_test)\n\n# computing MSE and RMSE\nlasso_rmse1 = np.sqrt(mean_squared_error(y_test, test_data_pred))\nprint(lasso_rmse1) ","e8dc64c6":"# Regression diagnostic - Residual plot analysis\nresiduals1 = y_test - test_data_pred\nsns.regplot(x=test_data_pred, y=residuals1, scatter=True, fit_reg=True, label='residual vs pred')\nsns.regplot(x=test_data_pred, y=y_test, scatter=True, fit_reg=True, label='residual vs y-true')\nplt.legend(fontsize=11)\n# residuals1.describe()# mean=0.002 which shows the Y_test and predicted values  are very close","14c3b3fa":"dataset_imputed = dataset.apply(lambda x:x.fillna(x.median()) \\\n                               if x.dtype == 'float' else \\\n                               x.fillna(x.value_counts().index[0]))\n\ndataset_imputed.info()","573da9a2":"# converting categorical variables to numeric using dummy variables\ndataset_imputed = pd.get_dummies(dataset_imputed, drop_first=True)\ndataset_imputed.info()","b81fd14e":"# seperating input  and output features\nX = dataset_imputed.drop(['price'], axis='columns', inplace=False)\ny = dataset_imputed['price']","be2b4d83":"# # feature scaling --- worsen the linear regression model\n# col_names = X.columns\n\n# scaler = MinMaxScaler()\n# x_scaled = scaler.fit_transform(X)\n# X_final = pd.DataFrame(x_scaled, columns = col_names)\n# X_final.head()","943fdb34":"# plotting the variable  price\n# normalval VS log value\nprices = pd.DataFrame({\"1. before\":y, \"2. After\":np.log(y)})\nprices.hist() ","58a2a2ac":"# logarithmic values are giving more bell  shaped graph\n# thus transforming y2 to logarithmic  form\ny = np.log(y)","bd733cfa":"# splitting training and test data\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=1)","85c3de42":"## BASELINE MODEL FOR IMPUTED DATA\n'''\nThe base model is being built using the test data mean value\nThis is to set a benchmark and to compare with out regression model later\n'''\n\nbase_pred = np.mean(y_test) # type - numpy.ndarray\nbase_pred = np.repeat(base_pred, len(y_test)) # to repeat same  value and makeit  of same size as y_test","378d4f90":"# finding the RMSE\nbase_rmse_imputed2 = np.sqrt(mean_squared_error(y_test, base_pred))\nprint(base_rmse_imputed2)","e270999a":"#setting intercept as true\nlgr2 = LinearRegression(fit_intercept=True)\n\n# model\nmodel_lin2 = lgr2.fit(x_train, y_train)\n\n#  predicting model on test set\ncars_predictions_lin2 = lgr2.predict(x_test)\n\n# computing MSE and RMSE\nlin_mse2 = mean_squared_error(y_test, cars_predictions_lin2)\nlin_rmse2 = np.sqrt(lin_mse2)\nprint(lin_rmse2)","1fa7c868":"rf2 = RandomForestRegressor(n_estimators = 100, max_features='auto',\n                            max_depth=100, min_samples_split=10,\n                            min_samples_leaf=4, random_state=1)\n\n# model\nmodel_rf2 = rf2.fit(x_train, y_train)\n\n# Predicting model on test set\ncars_predictions_rf2 = rf2.predict(x_test)\n\n# computing MSE and RMSE\nrf_mse2 = mean_squared_error(y_test, cars_predictions_rf2)\nrf_rmse2 = np.sqrt(rf_mse2)\nprint(rf_rmse2)","1b553a8f":"# lasso regression\nlasso_r2 = Lasso()\n\nlasso_r2.fit(x_train, y_train)\n\n# predicting for test data\ntest_data_pred = lasso_r2.predict(x_test)\n\n# computing MSE and RMSE\nlasso_rmse2 = np.sqrt(mean_squared_error(y_test, test_data_pred))\nprint(lasso_rmse2) ","b9193d76":"print(\"Number of total data points (train+test) in ommited dataset: {}\".format(len(dataset_omit.index)))\nprint(\"Ommited Data, Base Model Error: {}\".format(base_rmse_ommited1))\nprint(\"Ommited Data, Linear Regression Error: {}\".format(lr_rmse1))\nprint(\"Ommited Data, RANDOM FOREST Error: {}\".format(rf_rmse1))\nprint(\"Ommited Data, Lasso Regression Error: {}\".format(lasso_rmse1))\n\nprint(\"\\n\\n\\n\")\n\nprint(\"Number of total data points (train+test) in imputed dataset: {}\".format(len(dataset_imputed.index)))\nprint(\"Imputed Data, Base Model Error: {}\".format(base_rmse_imputed2))\nprint(\"Imputed Data, Linear Regression Error: {}\".format(lin_rmse2))\nprint(\"Imputed Data, RANDOM FOREST Error: {}\".format(rf_rmse2))\nprint(\"Imputed Data, Lasso Regression Error: {}\".format(lasso_rmse2))","8640702e":"##### Observations: because of outliers present in yearOfRegistration, not much could be deducted from the plot","1fe11e3e":"#### 5. Uni-Variate vs Bi-Variable Analysis to select significant features","eebd9611":"#### 1. Dataset - removing rows with missing values\n* Baseline model - using test data mean values\n* Our objective is to build models whose RMSE would be less than Baseline model's RMSE","ac72eadc":"##### LASSO WITH IMPUTED DATA","65d5fbd1":"### Correlations","4f8425a2":"##### 5.g: feature model","52d75ece":"##### LINEAR REGRESSION WITH OMITTED DATA","0a3d8d2a":"##### 5.d: feature abtest","5f765864":"##### 5.i: feature brand","65eb667e":"##### Observations: Clear presence of outlier, because of which the graph is looking highly skewed (right skewed)","9fdb5ad5":"##### RANDOM FOREST WITH OMITTED  DATA","ee4ac313":"##### 5.e: feature vehicleType","68ce44ad":"##### Model Building","17024328":"#### 2. Dropping duplicate records and keeping the first occurrences\n","7e223b97":"##### 5.h: feature fuelType","670ef4f3":"##### 3.a: year of registration (feature)","999cfe5a":"#### Observations:\n* Random Forest performed the best, followed by linear regression and then lasso regression.\n* Although the error values are less in ommited data, but it can be because of less rows\/records present in ommited data as compared to imputed data","f69aec8e":"##### 5.b: feature seller","b365ee2d":"##### 1.b Feature Scaling - MinMaxScaler\n* Feature Scaling was worsening the LinearRegression Model","bfc3e18d":"#### 4: ageFromRegistration -- new feature creation","77224472":"### Getting familier with data","d958d953":"##### RANDOM FOREST WITH IMPUTED DATA","c3101b03":"##### LINEAR REGRESSION WITH IMPUTED DATA","b8c4aeae":"##### Observations: skewness would exist because of 0","e9fac0f3":"##### 1.a Categorical variable Encoding","58611153":"#### 2. Dataset - imputing missing values\n* we will not drop the np.nan holding columns this time,  instead update them  with median(numeric variable) and mode (categorical variables)\n* Baseline model - using test data mean values\n* Our objective is to build models whose RMSE would be less than Baseline model's RMSE","b5c06039":"##### Observations: skewed price=0, which has 1415 records","4ed14e2a":"##### Baseline Model","ef4aeedc":"##### Observations:\n\n* [ageFromRegistration] with increase in age the price mostly decreases. SIGNIFICANT \n* [powerPS] with increase in powerPS the price is also increasing. SIGNIFICANT\n* [seller] commercial category occupies  only 1 row, thus redundant. AS  SELLER IS NOT CATEGORICALLY RICH--> INSIGNIFICANT VARIABLE\n* [offerType] ONLY ONE CATEGORY IS THERE--> INSIGNIFICANT VARIABLE\n* [abtest] for every price  value there  is  almost 50-50 distribution of both categories in abtest (i.e., test, control). So it does not affect price much==> INSIGNIFICANT.\n* [vehicleType] vehicleType  is a SIGNIFICANT VARIABLE, as it has  various  categories and all of them affects the price in different ways.\n* [gearbox] gearbox is a SIGNIFICANT VARIABLE  --> althought it has 2  categories  but  each category is affecting our dependent variable price differently (automatic gearbox cars have higher medean price)\n* [model] we will retain the 'model' variable as it  holds various categories:  golf being the dominant\n* [fuelType] clearly  fuelType affects price as various categories of fuelType gives different prices. Hybrid seems to have higher median prices. SIGNIFICANT VARIABLE\n* [brand] boxplot  makes  it extremely clear that price is highly dependent on brand. Brands like porche has higher median value  in relation to price SIGNIFICANT VARIABLE\n* [notRepairedDamage] boxplot clearly shows  that, car where damage  has been repaired  (no) is having higher median value  with respect to price. SIGNIFICANT VARIABLE\n","d3436686":"### Data Cleaning & Feature Engineering","8fd1b0ee":"#### 3. Removing outliers ","bfc2ab7c":"##### 3.b: price (target variable)","c0667552":"#### 1. Dropping features which will not be much helpful for the problem solving (prediction)","39ecea04":"##### 5.c: feature offerType","51f40bff":"##### 3.d: dropping outliers","58f37551":"##### Target Variable transformation","1c5aeacd":"##### 3.c: powerPS (feature)","4e70b9ad":"### Model Building\n* we will use two types of models \n    * Linear Regression\n    * Random Forest model\n\n* We will use two sets of data\n    * data obtained from removing rows containing even  a single  missing value\n    * data obtained by imputing the missing values","bd69aa8c":"#### ERRORS","140ac8b3":"### Feature Description\n* 1. dateCrawled: when this ad was first crawled, all field-values are taken from this date\n* 2. name: name of the car\n* 3. seller: private or dealer\n* 4. offerType:\n* 5. Price: (target Variable) the price on the ad to sell the car\n* 6. abtest:\n* 7. vehicleType:\n* 8. yearOfRegistration: the year on which car was first registered\n* 9. gearbox\n* 10. powerPS: power of car in PS (horsepower)\n* 11. model\n* 12. kilometer: how many kilometers the car has driven\n* 13. monthOfRegistration: at which month the car was first registered\n* 14. fuelType\n* 15. brand\n* 16. notRepairedDamage: if the car has a damage which is not yet repaired\n* 17. dateCreated: the date for which the ad at ebay was created\n* 18. nrOfPictures: number of pictures added in the ad\n* 19. postalCode\n* 20. lastSeenOnline: when the crawler saw this ad last online","8fa6f898":"##### 5.a: ageFromRegistration and powerPS","ddbcc2d3":"##### LASSO REGRESSION (L1-Regularization) WITH OMITTED  DATA","15107aff":"##### 5.f: feature gearbox","cf59e1d7":"##### Observations: there are some rubbish years, which needs cleaning","a316efd2":"##### 5.j: feature notRepairedDamage\n* one more important variable is notRepairedDamage\n    * yes - car is currently in damaged state and has not been rectified\n    * no - car was damaged but has  also been rectified"}}