{"cell_type":{"d75f7b1d":"code","ba8bf4c8":"code","91627d00":"code","0be02d68":"code","7e98521c":"code","bc14e46b":"code","8d465783":"code","4277b62a":"code","b7bf35ce":"code","1ff80ede":"code","f19c22ce":"code","b8ff4dc3":"code","84e377d8":"code","4dfe0359":"code","1b831329":"code","1b7e0fe0":"code","6670e209":"code","92b0ddd0":"code","1dcbaea3":"code","9b4d9869":"code","3b2e9200":"code","c71f79e0":"code","443c2e24":"code","3726f3d0":"code","d25e1e93":"code","5de0e716":"code","7fa82f91":"code","84ed2b33":"code","9a3a07f2":"code","16a66f06":"code","89e9fac9":"code","d1083d44":"code","fdcb53a9":"code","14209da7":"markdown","88c268be":"markdown","6be2c325":"markdown","c838e0dd":"markdown","2426081c":"markdown","0a18d9b0":"markdown","f9759e28":"markdown","d59ca779":"markdown","c4d31720":"markdown","8af904a4":"markdown","d3124daf":"markdown","59698589":"markdown","a3d56c8d":"markdown","160d8bb2":"markdown","19b42273":"markdown","85ab52ef":"markdown","e30818c7":"markdown","657bb7d5":"markdown"},"source":{"d75f7b1d":"!pip install seaborn --upgrade","ba8bf4c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Added these..\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nprint(\"Setup Complete\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","91627d00":"# Read in the data\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n# Concatenate DataFrames to process features easily...\ndata = pd.concat(objs =[train,test], axis=0).reset_index(drop=True)\n\nprint(f'Exemplars: {data.shape[0]}, features: {data.shape[1]}')\ndata.head()","0be02d68":"# Handle missing data...\ndata.isna().sum()","7e98521c":"# I would like to transform the cabin data to contain the letter component as this likely\n# refers to the area of the ship passengers stayed in which is likely to relate to class...\n\n# Check all data we have is of this format...\nfor cabin in np.unique(data['Cabin'][data['Cabin'].notnull()]):\n    if cabin[0].isalpha() != True:\n        print('Not all cabins begin with letter.')\n        break\n\n# String away letters from known cabins...\ndata[\"Cabin\"] = data['Cabin'].apply(lambda x: x if pd.isnull(x) else x[0])\n\n# Visulise letter frequency for each passenger class...\nfreq_dict = {}\nfig = plt.figure(figsize = (25,10))\nfor c in np.unique(data['Pclass']):\n    ax = fig.add_subplot(1, 3, c)\n    sns.countplot(x='Cabin', data=data[data['Pclass']==c], ax=ax, \\\n                  order = np.unique(data.loc[(data.Pclass == c),'Cabin'].dropna()));\n    plt.xticks(fontsize=25)\n    plt.yticks(fontsize=25)\n    ax.set_xlabel('Cabin', fontsize=25)\n    ax.set_ylabel('Count', fontsize=25)\n    ax.set_title(f'Class {c}', fontsize=30)\n    freqs = []\n    [freqs.append(patch.get_height()) for patch in ax.patches]\n    freqs = np.array(freqs)    \n    freqs = freqs \/ freqs.sum()\n    freq_dict[c] = freqs\n    ","bc14e46b":"# Fill in missing values with cabins from drawn from distribution of cabins by class...\nnp.random.seed(1)\nfor c in np.unique(data['Pclass'].dropna()):\n    choices = np.unique(data.loc[(data.Pclass == c),'Cabin'].dropna())\n    data.loc[(data.Pclass == c),'Cabin'] = data['Cabin'].apply(lambda \\\n                        x: np.random.choice(choices, p=freq_dict[c]) if pd.isnull(x) else x)\nsns.countplot(x='Cabin', data=data).set_title('Cabin distribution with updated values');\n","8d465783":"# Visualise age distributions by class...\nfig = plt.figure(figsize = (25,10))\nfor c in np.unique(data['Pclass']):\n    ax = fig.add_subplot(1, 3, c)\n    sns.histplot(x='Age', data=data[data['Pclass']==c], ax=ax);\n    plt.xticks(fontsize=25)\n    plt.yticks(fontsize=25)\n    ax.set_xlabel('Age', fontsize=25)\n    ax.set_ylabel('Count', fontsize=25)\n    ax.set_title(f'Class {c}', fontsize=30)","4277b62a":"# Fill missing age values with median age according to class...\nnp.random.seed(1)\nfor c in np.unique(data['Pclass'].dropna()):\n    data.loc[(data.Pclass == c),'Age'] = data['Age'].apply(lambda \\\n                        x: data.loc[(data.Pclass == c),'Age'].median() if pd.isnull(x) else x)","b7bf35ce":"# Visualise fare distributions by class...\nfig = plt.figure(figsize = (25,10))\nfor c in np.unique(data['Pclass']):\n    ax = fig.add_subplot(1, 3, c)\n    sns.histplot(x='Fare', data=data[data['Pclass']==c], ax=ax);\n    plt.xticks(fontsize=25)\n    plt.yticks(fontsize=25)\n    ax.set_xlabel('Fare', fontsize=25)\n    ax.set_ylabel('Count', fontsize=25)\n    ax.set_title(f'Class {c}', fontsize=30)","1ff80ede":"# Fill missing fare values with mean age according to class...\nnp.random.seed(1)\nfor c in np.unique(data['Pclass'].dropna()):\n    data.loc[(data.Pclass == c),'Fare'] = data['Fare'].apply(lambda \\\n                        x: data.loc[(data.Pclass == c),'Fare'].mean() if pd.isnull(x) else x)","f19c22ce":"# Fill embarked missing data with mode...\ndata['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])","b8ff4dc3":"# Check missing data has been handled\ndata.isna().sum()","84e377d8":"# Handle ticket data...\ndata['Ticket'].head()\n\n# I assume the letters preceeding the ticket numbers relate the booking company\/method, so I extract\n# these to use as a feature...","4dfe0359":"data[\"Ticket\"] = data['Ticket'].apply(lambda x: 'Z' \\\n                            if x.isdigit()  else x.replace('.', ' ').replace('\/', ' ').strip().split()[0])","1b831329":"ticket_dummies = pd.get_dummies(data[['Ticket']])\ndata = pd.concat(objs=[data, ticket_dummies], axis=1)\ndata = data.drop(['Ticket'], axis=1)","1b7e0fe0":"# Extract titles from names...\ndata['Title'] = data['Name'].apply(lambda x: x.split('.')[0].split(' ')[-1])\ndata['Title'].value_counts()","6670e209":"# Group upper class (UC) titles...\ndata[\"Title\"] = data[\"Title\"].replace(\n    ['Lady','Countess','Capt', 'Mme', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona', 'Mlle'], \n    'UC')\n\n# Group Ms and Mrs\ndata[\"Title\"] = data[\"Title\"].replace(\n    'Ms', \n    'Mrs'\n)\ndata['Title'].value_counts()","92b0ddd0":"# Visualise survival probability of each title\nsns.catplot(x=\"Title\", y=\"Survived\", data=data, kind=\"bar\").set_ylabels(\"Survival probability\");","1dcbaea3":"title_dummies = pd.get_dummies(data[['Title']])\ndata = pd.concat(objs=[data, title_dummies], axis=1)\ndata = data.drop(['Title'], axis=1)\ndata = data.drop(['Name'], axis=1)","9b4d9869":"# Transform male\/female to 0\/1...\ndata[\"Sex\"] = data['Sex'].apply(lambda x: 0 if x == 'male' else 1)","3b2e9200":"embrkd_dummies = pd.get_dummies(data[['Embarked']])\ndata = pd.concat(objs=[data, embrkd_dummies], axis=1)\ndata = data.drop(['Embarked'], axis=1)","c71f79e0":"# Create family size feature...\ndata[\"fam_size\"] = data[\"SibSp\"] + data[\"Parch\"]\n\ndata['Single'] = data['fam_size'].apply(lambda x: 1 if x == 0 else 0)\ndata['Sib\/Sp'] = data['SibSp'].apply(lambda x: 1 if x >= 1  else 0)\ndata['Par\/Ch'] = data['Parch'].apply(lambda x: 1 if x >= 1 else 0)\n\ndata = data.drop(['SibSp'], axis=1)\ndata = data.drop(['Parch'], axis=1)","443c2e24":"cabin_dummies = pd.get_dummies(data[['Cabin']])\ndata = pd.concat(objs=[data, cabin_dummies], axis=1)\ndata = data.drop(['Cabin'], axis=1)","3726f3d0":"cabin_dummies = pd.get_dummies(data[['Pclass']])\ndata = pd.concat(objs=[data, cabin_dummies], axis=1)\ndata = data.drop(['Pclass'], axis=1)","d25e1e93":"# Fare data is left skewed...\nsns.displot(data[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(data[\"Fare\"].skew()));\nplt.legend();","5de0e716":"# Take logs (plus 1 to handle 0s) of fares...\ndata['Fare'] = np.log1p(data['Fare'])\nsns.displot(data[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(data[\"Fare\"].skew()));\nplt.legend();","7fa82f91":"data['Fare'] = (data['Fare'] - data['Fare'].mean()) \/ data['Fare'].std()\ndata['Age'] = (data['Age'] - data['Age'].mean()) \/ data['Age'].std()","84ed2b33":"# Passenger ID isn't useful to remove...\n# Extract test indicies...\ntest_indicies = data['PassengerId'][train.shape[0]:]\ndata = data.drop(['PassengerId'], axis=1)\n\n\ntrain = data[:train.shape[0]]\ntest = data[train.shape[0]:]\n\n# Split training data in train and validate\nsplit = int(train.shape[0] * 0.85)\nx =  np.array(train.loc[:, train.columns != 'Survived'])\ny = np.array(train['Survived'])\nx_train = x[:split]\nx_val = x[split:]\ny_train = y[:split]\ny_val = y[split:]\nx_test =  np.array(test.loc[:, test.columns != 'Survived'])","9a3a07f2":"from scipy import spatial, stats\n\n# Define k_means algorithm...\ndef k_means(\n        x_train,\n        y_train,\n        x_val,\n        y_val,\n        x_test,\n        k,\n        steps=10):\n    \"\"\" Fit and evaluate k-means clusters on data\"\"\"\n    \n    assert k <= x_train.shape[0], \"K must be less than the number of traning data.\"\n\n    # Force k to be an integer...\n    k = int(k)\n\n    # Choose k random clusters to begin...\n    np.random.seed(1)\n    index = np.random.choice(x_train.shape[0], k, replace=False)\n    centroids = x_train[index, :]\n\n    # Find indicies of data closest to each centroid...\n    P = np.argmin(spatial.distance.cdist(x_train, centroids, 'euclidean'), axis=1)\n\n    for _ in range(steps):\n        # Create new centroids at the means of each centroid...\n        centroids = np.vstack([np.mean(x_train[P == i, :], axis=0)\n                               for i in range(k) if len(x_train[P == i, :]) != 0])\n\n        # Find indicies of data closests to each (new) centroid...\n        temp_index = np.argmin(\n            spatial.distance.cdist(\n                x_train,\n                centroids,\n                'euclidean'),\n            axis=1)\n\n        # If no data changes centroid, stop...\n        if np.array_equal(P, temp_index):\n            break\n\n        # Repeat process with new indicies...\n        P = temp_index\n\n    # Find the predictions (weighted average) from each cluster...\n    P = np.argmin(spatial.distance.cdist(x_train, centroids, 'euclidean'), axis=1)\n    distances = np.min(spatial.distance.cdist(x_train, centroids, 'euclidean'), axis=1)\n    cluster_preds = np.zeros(k)\n    cluster_preds = np.stack([stats.mode(y_train[P == i])[0] if len(y_train[P == i]) > 0 else np.array([0]) for i in range(k)])\n\n    # Find indicies of data closest to each final centroid for test and train\n    # data...\n\n    index_final_train = np.argmin(\n        spatial.distance.cdist(\n            x_train,\n            centroids,\n            'euclidean'),\n        axis=1)\n    \n    index_final_val = np.argmin(\n        spatial.distance.cdist(\n            x_val,\n            centroids,\n            'euclidean'),\n        axis=1)\n    \n    index_final_test = np.argmin(\n        spatial.distance.cdist(\n            x_test,\n            centroids,\n            'euclidean'),\n        axis=1)\n    pred_train = np.zeros(x_train.shape[0])\n    pred_val = np.zeros(x_val.shape[0])\n    pred_test = np.zeros(x_test.shape[0]) \n    \n    for i in range(k):\n        pred_train[index_final_train == i] = cluster_preds[i]\n        pred_val[index_final_val == i] = cluster_preds[i]\n        pred_test[index_final_test == i] = cluster_preds[i]\n       \n    \n    # Calculate accuracy on train and validation data...\n    train_accuracy = sum([1 for i, p in enumerate(y_train) if pred_train[i] == p]) \/ y_train.shape[0]\n    val_accuracy = sum([1 for i, p in enumerate(y_val) if pred_val[i] == p]) \/ y_val.shape[0]\n    \n    results = {'Train accuracy': train_accuracy,\n               'Validation accuracy': val_accuracy,\n               'test_preds': pred_test}\n\n    return results","16a66f06":"# Use brute force to optimise k...\nks = np.arange(1,x_train.shape[0])\nres = np.zeros((len(ks),2))\nbest_accuracy = 0\nopt_preds = None\nopt_k = None\nfor i, k in enumerate(ks):\n    results = k_means(x_train, y_train, x_val, y_val, x_test, k)\n    res[i,0] = results['Train accuracy']\n    res[i,1] = results['Validation accuracy']\n    if results['Validation accuracy'] > best_accuracy:\n        opt_preds = results['test_preds']\n        opt_k = k\n        best_accuracy = results['Validation accuracy']\nprint(f\"Best accuracy of {best_accuracy*100:.02f} of validation data using a K of {opt_k}\")        ","89e9fac9":"# Visualise accuracy of each k...\nplt.plot(ks, res[:,0], label = 'Train');\nplt.plot(ks, res[:,1], label = 'Validate');\nplt.xlabel('k')\nplt.ylabel('Accuracy')\nplt.legend();","d1083d44":"# Format and write results...\nout = pd.DataFrame(test_indicies).reset_index(drop=True)\nout['Survived'] = opt_preds.astype(int)\nout.head()","fdcb53a9":"out.to_csv(\"Titanic survival submission (kmeans).csv\",index=False)","14209da7":"### 2.2 Age missing data","88c268be":"### 3.4 Encoding embarked feature","6be2c325":"### 2.1 Cabin data","c838e0dd":"### 2.3 Fare missing data","2426081c":"### 3.9 Standardise fare and age data ","0a18d9b0":"### 3.6 Encode cabin feature","f9759e28":"### 3.5 Combine and encode siblings\/spouse (SibSp) and parents\/childen (Parch) features","d59ca779":"### 3.3 Encoding sex feature","c4d31720":"### 3.7 Encode class feature","8af904a4":"## 2. Handle missing data ","d3124daf":"### 3.8 Transform fare to log space","59698589":"## 3. Convert categorical features","a3d56c8d":"## 1. Read in data","160d8bb2":"### 3.1 Encoding ticket data","19b42273":"### 3.2 Encoding name feature","85ab52ef":"## 4. Evaluate model","e30818c7":"### 2.4 Embarked missing data","657bb7d5":"* Note survived missing values corresponds to the test set."}}