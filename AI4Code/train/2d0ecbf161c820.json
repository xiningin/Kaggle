{"cell_type":{"6a1dca12":"code","70e6b5f8":"code","13d7c5ea":"code","e7276fa7":"code","2dcdca1f":"code","f55c0000":"code","6d317a19":"code","3eeffdf8":"code","197cf24e":"code","2e156436":"code","d6410e2b":"code","ebfef5d4":"code","936d1d44":"code","f1d8a9e1":"code","7871a05f":"code","10d4ce2e":"code","51403704":"code","20711a3e":"code","c5ea7adf":"code","c8ecde49":"code","b65b6e8d":"markdown","5f187809":"markdown","6966fe47":"markdown","351de1ad":"markdown","f8815275":"markdown","75fb3a78":"markdown","9867cdcf":"markdown","94f2d704":"markdown","c4440828":"markdown","4da5196c":"markdown","e3ccd8d2":"markdown","7667388b":"markdown","c74051e4":"markdown","dff3ffa1":"markdown","04f29e64":"markdown","c1719111":"markdown","43ea74fc":"markdown","6b012fa8":"markdown","d2b4ec20":"markdown"},"source":{"6a1dca12":"# Fixing a problem with Skopt (see https:\/\/github.com\/scikit-optimize\/scikit-optimize\/issues\/981)\n!conda install scipy=='1.5.3' --y","70e6b5f8":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport pprint\nimport joblib\nfrom functools import partial\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Classifiers\nimport lightgbm as lgb\n\n# Model selection\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\n\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt.callbacks import DeadlineStopper, DeltaYStopper\nfrom skopt.space import Real, Categorical, Integer\n\n# Data processing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","13d7c5ea":"# Loading data \nX = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","e7276fa7":"# Preparing data as a tabular matrix\ny = X.target\nX = X.set_index('id').drop('target', axis='columns')\nX_test = X_test.set_index('id')","2dcdca1f":"# Stratifying the target\ny_stratified = pd.cut(y, bins=10, labels=False)","f55c0000":"categoricals = [item for item in X.columns if 'cat' in item]","6d317a19":"# Processing categoricals with SVD encoding\nX_seq = X[categoricals].apply(lambda x: \" \".join(list([str(y) + str(i) for i, y in enumerate(x)])), axis=1)\nX_test_seq = X_test[categoricals].apply(lambda x: \" \".join(list([str(y) + str(i) for i, y in enumerate(x)])), axis=1)\n\nlatent_dims = 24\n\nsvd_feats = ['svd_'+str(l) for l in range(latent_dims)]\nvectorizer = TfidfVectorizer()\n\ndim_reductio = TruncatedSVD(n_components=24, random_state=0)\nX[svd_feats] =  dim_reductio.fit_transform(vectorizer.fit_transform(X_seq))\nX_test[svd_feats] = dim_reductio.transform(vectorizer.fit_transform(X_test_seq))","3eeffdf8":"# Processing categoricals with frequency encoding\ncategoricals = [item for item in X.columns if 'cat' in item]\n\nfor cat in categoricals:\n    counts = dict(X[cat].value_counts() \/ len(X))\n    X[cat+'_freq'] = X[cat].replace(counts)\n    X_test[cat+'_freq'] = X_test[cat].replace(counts)\n    \nfrequencies = [cat+'_freq' for cat in categoricals]","197cf24e":"# Dealing with categorical data\nordinal_encoder = OrdinalEncoder()\nX[categoricals] = ordinal_encoder.fit_transform(X[categoricals])\nX_test[categoricals] = ordinal_encoder.transform(X_test[categoricals])","2e156436":"# Reporting util for different optimizers\ndef report_perf(optimizer, X, y, title=\"model\", callbacks=None):\n    \"\"\"\n    A wrapper for measuring time and performances of different optmizers\n    \n    optimizer = a sklearn or a skopt optimizer\n    X = the training set \n    y = our target\n    title = a string label for the experiment\n    \"\"\"\n    start = time()\n    \n    if callbacks is not None:\n        optimizer.fit(X, y, callback=callbacks)\n    else:\n        optimizer.fit(X, y)\n        \n    d=pd.DataFrame(optimizer.cv_results_)\n    best_score = optimizer.best_score_\n    best_score_std = d.iloc[optimizer.best_index_].std_test_score\n    best_params = optimizer.best_params_\n    \n    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n           + u\"\\u00B1\"+\" %.3f\") % (time() - start, \n                                   len(optimizer.cv_results_['params']),\n                                   best_score,\n                                   best_score_std))    \n    print('Best parameters:')\n    pprint.pprint(best_params)\n    print()\n    return best_params","d6410e2b":"# Setting the scoring function\nscoring = make_scorer(partial(mean_squared_error, squared=False), \n                      greater_is_better=False)","ebfef5d4":"# Setting the validation strategy\nskf = StratifiedKFold(n_splits=5, \n                      shuffle=True, \n                      random_state=0)\n\ncv_strategy = list(skf.split(X, y_stratified))","936d1d44":"# Setting the basic regressor\nreg = lgb.LGBMRegressor(boosting_type='dart',\n                        objective='regression',\n                        metric='rmse',\n                        n_jobs=1, \n                        verbose=-1,\n                        random_state=0)","f1d8a9e1":"# Setting the search space\nsearch_spaces = {\n    'reg_sqrt': Categorical([True, False]),\n    'learning_rate': Real(0.01, 1.0, 'log-uniform'),     # Boosting learning rate\n    'n_estimators': Integer(30, 5000),                   # Number of boosted trees to fit\n    'num_leaves': Integer(2, 512),                       # Maximum tree leaves for base learners\n    'max_depth': Integer(-1, 256),                       # Maximum tree depth for base learners, <=0 means no limit\n    'subsample': Real(0.01, 1.0, 'uniform'),             # Subsample ratio of the training instance\n    'subsample_freq': Integer(1, 10),                    # Frequency of subsample, <=0 means no enable\n    'colsample_bytree': Real(0.01, 1.0, 'uniform'),      # Subsample ratio of columns when constructing each tree\n    'reg_lambda': Real(1e-9, 100.0, 'log-uniform'),      # L2 regularization\n    'reg_alpha': Real(1e-9, 100.0, 'log-uniform'),       # L1 regularization\n   }","7871a05f":"# Wrapping everything up into the Bayesian optimizer\nopt = BayesSearchCV(estimator=reg,                                    \n                    search_spaces=search_spaces,                      \n                    scoring=scoring,                           \n                    cv=cv_strategy,                                           \n                    n_iter=60,                                        # max number of trials\n                    n_points=3,                                       # number of hyperparameter sets evaluated at the same time\n                    n_jobs=-1,                                        # number of jobs\n                    iid=False,                                        # if not iid it optimizes on the cv score\n                    return_train_score=False,                         \n                    refit=False,                                      \n                    optimizer_kwargs={'base_estimator': 'GP'},        # optmizer parameters: we use Gaussian Process (GP)\n                    random_state=0)                                   # random state for replicability","10d4ce2e":"# Running the optimizer\noverdone_control = DeltaYStopper(delta=0.0001)               # We stop if the gain of the optimization becomes too small\ntime_limit_control = DeadlineStopper(total_time=60 * 60 * 7) # We impose a time limit (6 hours)\n\nbest_params = report_perf(opt, X, y,'LightGBM_regression', \n                          callbacks=[overdone_control, time_limit_control])","51403704":"# Transferring the best parameters to our basic regressor\nreg = lgb.LGBMRegressor(boosting_type='dart',\n                        objective='regression',\n                        metric='rmse',\n                        n_jobs=1, \n                        verbose=-1,\n                        random_state=0,\n                        **best_params)","20711a3e":"# Cross-validation prediction\nfolds = 10\nskf = StratifiedKFold(n_splits=folds,\n                      shuffle=True, \n                      random_state=0)\n\npredictions = np.zeros(len(X_test))\nfor k, (train_idx, val_idx) in enumerate(skf.split(X, y_stratified)):\n    reg.fit(X.iloc[train_idx, :], y[train_idx])\n    val_preds = reg.predict(X.iloc[val_idx, :])\n    val_rmse = mean_squared_error(y_true=y[val_idx], y_pred=val_preds, squared=False)\n    print(f\"Fold {k} RMSE: {val_rmse:0.5f}\")\n    predictions += reg.predict(X_test).ravel()\n    \npredictions \/= folds","c5ea7adf":"# Preparing the submission\nsubmission = pd.DataFrame({'id':X_test.index, \n                           'target': predictions})\n\nsubmission.to_csv(\"submission.csv\", index = False)","c8ecde49":"submission","b65b6e8d":"In this step by ste tutorial, you will deal Bayesian optimization using LightGBM in few clear steps:\n\n1. Prepare your data, especially your categorical\n2. Define your cross-validation strategy\n3. Define your evaluation metric\n4. Define your base model\n5. Define your hyper-parameter search space\n6. Run optimization for a while","5f187809":"# 1. Data preparation","6966fe47":"We set up a  5-fold cross validation","351de1ad":"As first steps:\n\nwe load the train and test data \nwe separate the target from the training data\nwe separate the ids from the data\nwe convert integer variables to categories (thus our machine learning algorithm can pick them as categorical variables and not standard numeric one)\n\nYou can add further processing, for instance by feature engineering, in order to succeed in this competition","f8815275":"**Machine learning algorithms have lots of knobs, and success often comes from twiddling them a lot.**\n\n*(DOMINGOS, Pedro. A few useful things to know about machine learning.\u00a0Communications of the ACM, 2012, 55.10: 78-87.)*","75fb3a78":"We then define the evaluation metric, using the Scikit-learn function make_scorer allows us to convert the optimization into a minimization problem, as required by Scikit-optimize. We set squared=False by means of a partial function to obtain the root mean squared error (RMSE) as evaluation.","9867cdcf":"# Setting up optimization","94f2d704":"# Kaggle\u2019s 30 days of machine learning:\n## Scikit-optimize for LightGBM (regression tutorial)\n\n![image.png](attachment:49860e47-cfb0-4ad8-8edd-9bbc7e6d1b0c.png)","c4440828":"Having got the best hyperparameters for the data at hand, we instantiate a lightGBM using such values and train our model on all the available examples.\n\nAfter having trained the model, we predict on the test set and we save the results on a csv file.","4da5196c":"We define a search space, expliciting the key hyper-parameters to optimize and the range where to look for the best values.\n","e3ccd8d2":"First, we create a wrapper function to deal with running the optimizer and reporting back its best results.","7667388b":"![image.png](attachment:f8d4cbb2-09ef-4c55-a3f9-a93e6ddd8033.png)","c74051e4":"In optimizing your model for this competition, you may wonder if there is a way to:\n\n * Leverage the information that you get as you explore the hyper-parameter space\n\n * Not necessarily become be an expert of a specific ML algorithm\n\n * Quickly find an optimization","dff3ffa1":"The answer is:\n\n**Bayesian Optimization** (*SNOEK, Jasper; LAROCHELLE, Hugo; ADAMS, Ryan P. Practical bayesian optimization of machine learning algorithms. In: Advances in neural information processing systems. 2012. p. 2951-2959*)","04f29e64":"Finally we runt the optimizer and wait for the results. We have set some limits to its operations: we required it to stop if it cannot get consistent improvements from the search (DeltaYStopper) and time dealine set in seconds (we decided for 6 hours).","c1719111":"We set up a generic LightGBM regressor.","43ea74fc":"# Prediction on test data","6b012fa8":"We then define the Bayesian optimization engine, providing to it our LightGBM, the search spaces, the evaluation metric, the cross-validation. We set a large number of possible experiments and some parallelism in the search operations.","d2b4ec20":"The key idea behind Bayesian optimization is that we optimize a proxy function (the surrogate function) instead than the true objective function (what actually grid search and random search both do). This holds if testing the true objective function is costly (if it is not, then we simply go for random search.\n\nBayesian search balances exploration against exploitation. At start it randomly explores, doing so it builds up a surrogate function of the objective. Based on that surrogate function it exploits an initial approximate knowledge of how the predictor works in order to sample more useful examples and minimize the cost function at a global level, not a local one.\n\nBayesian Optimization uses an acquisition function to tell us how promising an observation will be. In fact, to rule the tradeoff between exploration and exploitation, the algorithm defines an acquisition function that provides a single measure of how useful it would be to try any given point."}}