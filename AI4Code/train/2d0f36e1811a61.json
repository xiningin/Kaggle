{"cell_type":{"9c24d197":"code","36f66726":"code","401eb9a1":"code","970956e1":"code","c15cb0ea":"code","20e27e9a":"code","955322d5":"code","3914fc98":"code","6e1aa662":"code","b44c7ab1":"code","65ac76cd":"code","046f2dd4":"code","7a2c342c":"code","83559451":"code","bc09c85e":"code","9c5f286d":"code","92ea3b06":"markdown","6245b930":"markdown","19b10471":"markdown","7119e29e":"markdown"},"source":{"9c24d197":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","36f66726":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import callbacks\nimport tensorflow_hub as hub\nfrom keras.utils import to_categorical\n\nimport tokenization\n\nsns.set_style(\"whitegrid\")\nnotebookstart = time.time()\npd.options.display.max_colwidth = 500\n\nprint(\"Tensorflow Version: \", tf.__version__)\nprint(\"TF-Hub version: \", hub.__version__)\nprint(\"Eager mode enabled: \", tf.executing_eagerly())\nprint(\"GPU available: \", tf.test.is_gpu_available())","401eb9a1":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","970956e1":"def build_model(bert_layer, out_channels, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(out_channels, activation='softmax')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    \n    return model","c15cb0ea":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","20e27e9a":"MAX_LEN = 64*3\nBATCH_SIZE = 8\nEPOCHS = 15\nSEED = 42\nNROWS = None\nTEXTCOL = \"text\"\nTARGETCOL = \"author\"\nNCLASS = 3","955322d5":"train = pd.read_csv(\"..\/input\/spooky-author-identification\/train.zip\")\ntest = pd.read_csv(\"..\/input\/spooky-author-identification\/test.zip\")\ntestdex = test.id\nsubmission = pd.read_csv(\"..\/input\/spooky-author-identification\/sample_submission.zip\")\n\nsub_cols = submission.columns\n\nprint(\"Train Shape: {} Rows, {} Columns\".format(*train.shape))\nprint(\"Test Shape: {} Rows, {} Columns\".format(*test.shape))\n\nlength_info = [len(x) for x in np.concatenate([train[TEXTCOL].values, test[TEXTCOL].values])]\nprint(\"Train Sequence Length - Mean {:.1f} +\/- {:.1f}, Max {:.1f}, Min {:.1f}\".format(\n    np.mean(length_info), np.std(length_info), np.max(length_info), np.min(length_info)))","3914fc98":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","6e1aa662":"train_input = bert_encode(train[TEXTCOL].values, tokenizer, max_len=MAX_LEN)\ntest_input = bert_encode(test[TEXTCOL].values, tokenizer, max_len=MAX_LEN)\n\n\nlabel_mapper = {name: i for i,name in enumerate(set(train[TARGETCOL].values))}\nnum_label = np.vectorize(label_mapper.get)(train[TARGETCOL].values)\ntrain_labels = to_categorical(num_label)","b44c7ab1":"model = build_model(bert_layer, NCLASS, max_len=MAX_LEN)\nmodel.summary()","65ac76cd":"checkpoint = callbacks.ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\nes = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001,\n                             patience=4, verbose=1, mode='min', baseline=None,\n                             restore_best_weights=False)\n\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=EPOCHS,\n    callbacks=[checkpoint, es],\n    batch_size=BATCH_SIZE\n)","046f2dd4":"import matplotlib.pyplot as plt","7a2c342c":"plot_metrics = ['loss']\n\nf, ax = plt.subplots(1,figsize = [7,4])\nfor p_i,metric in enumerate(plot_metrics):\n    ax.plot(train_history.history[metric], label='Train ' + metric)\n    ax.plot(train_history.history['val_' + metric], label='Val ' + metric)\n    ax.set_title(\"Loss Curve - {}\".format(metric))\n    ax.legend()\nplt.show()","83559451":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)\ntest_pred.shape","bc09c85e":"submission = pd.DataFrame(test_pred, columns=label_mapper.keys())\nsubmission['id'] = testdex\n\nsubmission = submission[sub_cols]\nsubmission.to_csv('submission_bert.csv', index=False)\nprint(submission.shape)","9c5f286d":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)\/60))","92ea3b06":"# About this kernel\n\n- https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\n\nI've seen a lot of people pooling the output of BERT, then add some Dense layers. I also saw various learning rates for fine-tuning. In this kernel, I wanted to try some ideas that were used in the original paper that did not appear in many public kernel. Here are some examples:\n* *No pooling, directly use the CLS embedding*. The original paper uses the output embedding for the `[CLS]` token when it is finetuning for classification tasks, such as sentiment analysis. Since the `[CLS]` token is the first token in our sequence, we simply take the first slice of the 2nd dimension from our tensor of shape `(batch_size, max_len, hidden_dim)`, which result in a tensor of shape `(batch_size, hidden_dim)`.\n* *No Dense layer*. Simply add a sigmoid output directly to the last layer of BERT, rather than experimenting with different intermediate layers.\n* *Fixed learning rate, batch size, epochs, optimizer*. As specified by the paper, the optimizer used is Adam, with a learning rate between 2e-5 and 5e-5. Furthermore, they train the model for 3 epochs with a batch size of 32. I wanted to see how well it would perform with those default values.\n\nI also wanted to share this kernel as a **concise, reusable, and functional example of how to build a workflow around the TF2 version of BERT**. Indeed, it takes less than **50 lines of code to write a string-to-tokens preprocessing function and model builder**.\n\n## References\n\n* Source for `bert_encode` function: https:\/\/www.kaggle.com\/user123454321\/bert-starter-inference\n* All pre-trained BERT models from Tensorflow Hub: https:\/\/tfhub.dev\/s?q=bert","6245b930":"# Load and Preprocess\n\n- Load BERT from the Tensorflow Hub\n- Load CSV files containing training data\n- Load tokenizer from the bert layer\n- Encode the text into tokens, masks, and segment flags","19b10471":"# Model: Build, Train, Predict, Submit","7119e29e":"# Helper Functions"}}