{"cell_type":{"fca5d4c2":"code","bb888300":"code","5d637912":"code","a0f99e6f":"code","c2fd4e04":"code","036e1512":"code","5d42eb17":"code","947e17c6":"code","1c5e3b75":"code","4feecac0":"code","0b854349":"code","4cc58b95":"code","525ce559":"code","d26d514a":"code","e34aa4de":"code","87987a75":"code","5ca3e4a9":"code","21524536":"code","68e966d2":"code","e1991c7d":"code","0851faf4":"markdown","d57ea71e":"markdown","99a9c016":"markdown","61f07517":"markdown","8bb8ee7a":"markdown","fda27c42":"markdown","e4f56a0a":"markdown","a6c93d92":"markdown","642dd67f":"markdown","8c742fd5":"markdown","77bab01b":"markdown","d0a7fe51":"markdown"},"source":{"fca5d4c2":"import os\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\n\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\n\nimport keras\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers","bb888300":"X_train = pd.read_csv(\"..\/input\/X_train.csv\")\nX_test = pd.read_csv(\"..\/input\/X_test.csv\")\ny_train = pd.read_csv(\"..\/input\/y_train.csv\")\nsub = pd.read_csv(\"..\/input\/sample_submission.csv\")","5d637912":"X_train.head()","a0f99e6f":"plt.figure(figsize=(15, 5))\nsns.countplot(y_train['surface'])\nplt.title('Target distribution', size=15)\nplt.show()","c2fd4e04":"X_train.drop(['row_id', \"series_id\", \"measurement_number\"], axis=1, inplace=True)\nX_train = X_train.values.reshape((3810, 128, 10))","036e1512":"X_test.drop(['row_id', \"series_id\", \"measurement_number\"], axis=1, inplace=True)\nX_test = X_test.values.reshape((3816, 128, 10))","5d42eb17":"for j in range(2):\n    plt.figure(figsize=(15, 5))\n    plt.title(\"Target : \" + y_train['surface'][j], size=15)\n    for i in range(10):\n        plt.plot(X_train[j, :, i], label=i)\n    plt.legend()\n    plt.show()","947e17c6":"encode_dic = {'fine_concrete': 0, \n              'concrete': 1, \n              'soft_tiles': 2, \n              'tiled': 3, \n              'soft_pvc': 4,\n              'hard_tiles_large_space': 5, \n              'carpet': 6, \n              'hard_tiles': 7, \n              'wood': 8}","1c5e3b75":"decode_dic = {0: 'fine_concrete',\n              1: 'concrete',\n              2: 'soft_tiles',\n              3: 'tiled',\n              4: 'soft_pvc',\n              5: 'hard_tiles_large_space',\n              6: 'carpet',\n              7: 'hard_tiles',\n              8: 'wood'}","4feecac0":"y_train = y_train['surface'].map(encode_dic).astype(int)","0b854349":"#https:\/\/www.kaggle.com\/suicaokhoailang\/lstm-attention-baseline-0-652-lb\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","4cc58b95":"def make_model():\n    inp = Input(shape=(128, 10))\n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(inp)\n    x = Bidirectional(CuDNNLSTM(32, return_sequences=True))(x)\n    x = Attention(128)(x)\n    # A intermediate full connected (Dense) can help to deal with nonlinears outputs\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dense(9, activation=\"softmax\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","525ce559":"def k_folds(X, y, X_test, k=5):\n    folds = list(StratifiedKFold(n_splits=k, shuffle=True, random_state=2019).split(X, y))\n    y_test = np.zeros((X_test.shape[0], 9))\n    y_oof = np.zeros((X.shape[0]))\n    \n    for i, (train_idx, val_idx) in  enumerate(folds):\n        print(f\"Fold {i+1}\")\n        model = make_model()\n        model.fit(X[train_idx], y[train_idx], batch_size=128, epochs=75, \n                  validation_data=[X[val_idx], y[val_idx]], verbose=0)\n        \n        pred_val = np.argmax(model.predict(X[val_idx]), axis=1)\n        score = accuracy_score(pred_val, y[val_idx])\n        y_oof[val_idx] = pred_val\n        \n        print(f'Scored {score:.3f} on validation data')\n        \n        y_test += model.predict(X_test)\n        \n    return y_oof, y_test                                                                          ","d26d514a":"y_oof, y_test = k_folds(X_train, y_train, X_test, k=5)","e34aa4de":"print(f'Local CV is {accuracy_score(y_oof, y_train): .4f}')","87987a75":"def plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(15, 15))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","5ca3e4a9":"plot_confusion_matrix(y_train, y_oof, encode_dic.keys())","21524536":"y_test = np.argmax(y_test, axis=1)","68e966d2":"sub['surface'] = y_test\nsub['surface'] = sub['surface'].map(decode_dic)\nsub.head()","e1991c7d":"sub.to_csv('submission.csv', index=False)","0851faf4":"### Input","d57ea71e":"### Submission","99a9c016":"## Modeling","61f07517":"## Make Data for the Network","8bb8ee7a":"### Load Data","fda27c42":"### Thanks for reading ! \n##### Please leave an upvote, it is always appreciated!","e4f56a0a":"### Ouput\n\nWe encode our targets","a6c93d92":"This kernel is based on [Theo Viel](https:\/\/www.kaggle.com\/theoviel\/deep-learning-starter), Here I just add more Bidirectional CuDNNLSTM layer 128 and 64 are the number of cells used, too many can overfit and too few can underfit. Add a intermediate full connected (Dense)  help to deal with nonlinears outputs.","642dd67f":"### Confusion Matrix","8c742fd5":"### Model","77bab01b":"### $k$-Folds","d0a7fe51":"###  Attention Layer\nBecause that's fancy"}}