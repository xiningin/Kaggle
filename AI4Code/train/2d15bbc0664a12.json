{"cell_type":{"401bb813":"code","cf8655fd":"code","ed83f4de":"code","efc43087":"code","cf0a9468":"code","50c0a55d":"code","a8018f10":"code","b0761ea2":"code","a7c8d652":"code","c8348a8f":"code","49d049ec":"code","2dfe38f3":"code","1ec939df":"code","9dd260de":"code","f50c3ba7":"code","e1a4a600":"code","16af7bc7":"code","33fc284f":"code","733de84f":"code","82c25237":"code","4a2e1244":"code","4c22df23":"code","bd3adf5d":"code","77e18b38":"code","820237a6":"code","2135ff38":"code","16557d2e":"code","61d770b1":"code","46dbb5c5":"code","7c8e8456":"code","2ef41c4f":"code","0d36abd1":"code","42836f3f":"code","fa8818e1":"code","655f7e50":"code","b9ae3d0d":"code","b4e3f296":"code","c1e07089":"code","b5fe6f55":"code","c60d898b":"code","2e10e930":"code","8c6bef31":"code","c7227bd0":"code","fc17a286":"code","86f245ed":"code","db06edde":"code","7e2ba815":"code","181a76c4":"code","9fd4668e":"code","a234835f":"code","0fea117a":"code","88843440":"code","ecae6905":"code","5ba5dc6e":"code","28a51b0f":"code","f98ca6e3":"code","e0b6ec1f":"code","e2ff963e":"code","edd9a488":"code","a64e34c2":"code","22a146ce":"code","302d4761":"code","3485311a":"code","3d5c5c46":"code","8913d315":"code","d602eec9":"code","5509b776":"code","01290a9d":"code","700aee7c":"code","c94cc5ea":"code","2367b107":"markdown","91105a77":"markdown","7be1c0c9":"markdown","30642d77":"markdown","8f8f699e":"markdown","ea3c9a0b":"markdown","43514de8":"markdown","7cc3b3a2":"markdown","60d5baeb":"markdown","0285edd4":"markdown","2957c010":"markdown","93295fd9":"markdown","e44a9ccb":"markdown","13a0580b":"markdown","c4ad41e2":"markdown","65624ea2":"markdown","41d44096":"markdown","fda09783":"markdown","e228dbba":"markdown","3fc3ca12":"markdown","9a02ac9c":"markdown","1801cb3d":"markdown","0599c02e":"markdown","a6b37497":"markdown","74f455f0":"markdown","439108b4":"markdown","0eeac987":"markdown","0fe7f3dd":"markdown","7bd10bc2":"markdown","8aa4c882":"markdown","3d1db29c":"markdown","4153c30e":"markdown"},"source":{"401bb813":"%%capture\nimport os\nfor dirname, _, filename in os.walk(\"..\/input\"):\n  for files in filename:\n    print(os.path.join(dirname, files))","cf8655fd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n%matplotlib inline\nimport cv2\nfrom tqdm import tqdm_notebook as tqdm","ed83f4de":"train_data = pd.DataFrame(pd.read_csv(\"..\/input\/plant-pathology-2020-fgvc7\/train.csv\"))\ntest_data = pd.DataFrame(pd.read_csv(\"..\/input\/plant-pathology-2020-fgvc7\/test.csv\"))","efc43087":"print(\"Training data shape : = {}\".format(train_data.shape))\nprint(\"Test data shape : = {}\".format(test_data.shape))","cf0a9468":"train_data.head()","50c0a55d":"test_data.head()","a8018f10":"image_folder_path = \"..\/input\/plant-pathology-2020-fgvc7\/images\/\"","b0761ea2":"arr = train_data[\"image_id\"]\ntrain_images = [i for i in arr]  \n\narr = test_data[\"image_id\"]\ntest_images = [i for i in arr]","a7c8d652":"def load_image(image_id) : \n  image_path = image_folder_path +image_id +\".jpg\"\n  image = cv2.imread(image_path) \n  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n  return image\n\ndef resize(image, image_size):\n  image = cv2.resize(image, (image_size[0], image_size[1]), interpolation = cv2.INTER_AREA)\n  return image","c8348a8f":"def extract_classes(s):\n  \"\"\"\n  s can be either of the four classes mentioned above.\n  \"\"\" \n  t = train_data[train_data[s] == 1] \n  arr = t[\"image_id\"]\n  images = [i for i in tqdm(arr)]\n  train_images = [load_image(i) for i in tqdm(images)]\n  return train_images\n\nclasses = [\"healthy\", \"multiple_diseases\", \"rust\", \"scab\"] ","49d049ec":"count_healthy = len(train_data[train_data[\"healthy\"] == 1])\ncount_diseased = len(train_data[train_data[\"multiple_diseases\"] == 1])\ncount_rust = len(train_data[train_data[\"rust\"] == 1])\ncount_scab = len(train_data[train_data[\"scab\"] == 1])\n\nprint(count_healthy)\nprint(count_diseased)\nprint(count_rust)\nprint(count_scab)\nprint(count_healthy + count_diseased + count_rust +  count_scab)","2dfe38f3":"# observe number of cases present in each class\nlabels = [\"Healthy\", \"Multiple Diseased\", \"Rust\", \"Scab\"]\ncounts = [count_healthy, count_diseased, count_rust, count_scab]\nexplode = (0.05, 0.05, 0.05, 0.05)\nfig, ax = plt.subplots(figsize = (20, 12))\nax.pie(counts, explode = explode, labels = labels, shadow = True, startangle = 90)\nax.axis(\"equal\") # equal aspect ratio ensures pie graph is drawn as circle","1ec939df":"red , green, blue = [], [], []","9dd260de":"healthy = extract_classes(\"healthy\")\nfor image in healthy :\n    mean_red = np.mean(image[:,:,0])\n    mean_green = np.mean(image[:,:,1])\n    mean_blue = np.mean(image[:,:,2])\n    \n    red.append(mean_red)\n    green.append(mean_green)\n    blue.append(mean_blue)\n    \nhealthy_image_1 = healthy[100]\nhealthy_image_2 = healthy[200]\nhealthy_image_3 = healthy[300]\ndel healthy # free memory\n\nmd = extract_classes(\"multiple_diseases\")\nfor image in md : \n    mean_red = np.mean(image[:,:,0])\n    mean_green = np.mean(image[:,:,1])\n    mean_blue = np.mean(image[:,:,2])\n    \n    red.append(mean_red)\n    green.append(mean_green)\n    blue.append(mean_blue)\nmd_image_1 = md[1]\nmd_image_2 = md[5]\nmd_image_3 = md[10]\ndel md # free memory\n\nrust = extract_classes(\"rust\")\nfor image in rust : \n    mean_red = np.mean(image[:,:,0])\n    mean_green = np.mean(image[:,:,1])\n    mean_blue = np.mean(image[:,:,2])\n    \n    red.append(mean_red)\n    green.append(mean_green)\n    blue.append(mean_blue)\nrust_image_1 = rust[10]\nrust_image_2 = rust[20] \nrust_image_3 = rust[30]\ndel rust # free memory\n\nscab = extract_classes(\"healthy\")\nfor image in scab : \n    mean_red = np.mean(image[:,:,0])\n    mean_green = np.mean(image[:,:,1])\n    mean_blue = np.mean(image[:,:,2])\n    \n    red.append(mean_red)\n    green.append(mean_green)\n    blue.append(mean_blue)\nscab_image_1 = scab[10]\nscab_image_2 = scab[20]\nscab_image_3 = scab[30] \ndel scab # free memory\n\nimage_collection = [healthy_image_1, healthy_image_2, healthy_image_3, \n                   md_image_1, md_image_2, md_image_3,\n                   rust_image_1, rust_image_2, rust_image_3,\n                   scab_image_1, scab_image_2, scab_image_3]   ","f50c3ba7":"fig, ax = plt.subplots(nrows = 4, ncols = 3, figsize = (25, 15))\nfor i in range(12):\n    ax[i\/\/3, i%3].imshow(image_collection[i]) ","e1a4a600":"# red channel plot\nrange_of_spread = max(red) - min(red)\nplt.figure(figsize = (12, 8))\nplt.rc('font', weight='bold')\nsns.set_style(\"whitegrid\")\nfig = sns.distplot(red,  hist = True, kde = True, label = \"Red Channel intensities\", color = \"r\")\nfig.set(xlabel = \"Mean red channel intensities observed in each image (Sample size = 1000)\", ylabel = \"Probability Density\")\nplt.legend()\nprint(\"The range of spread = {:.2f}\".format(range_of_spread))","16af7bc7":"# Green channel plot\nrange_of_spread = max(green) - min(green)\nplt.figure(figsize = (12, 8))\nplt.rc('font', weight='bold')\nsns.set_style(\"whitegrid\")\nfig = sns.distplot(green,  hist = True, kde = True, label = \"Green Channel intensities\", color = \"g\")\nfig.set(xlabel = \"Mean green channel intensities observed in each image (Sample size = 1000)\", ylabel = \"Probability Density\")\nplt.legend()\nprint(\"The range of spread = {:.2f}\".format(range_of_spread))","33fc284f":"# Blue channel plot\nrange_of_spread = max(blue) - min(blue)\nplt.figure(figsize = (12, 8))\nplt.rc('font', weight='bold')\nsns.set_style(\"whitegrid\")\nfig = sns.distplot(blue,  hist = True, kde = True, rug = False, label = \"Blue Channel intensities\", color = \"b\")\nfig.set(xlabel = \"Mean blue channel intensities observed in each image (Sample size = 1000)\", ylabel = \"Probability Density\")\nplt.legend()\nprint(\"The range of spread = {:.2f}\".format(range_of_spread))","733de84f":"sample_image = rust_image_1\nplt.figure(figsize = (12, 8))\nplt.imshow(sample_image)","82c25237":"def non_local_means_denoising(image) : \n    denoised_image = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)\n    return denoised_image","4a2e1244":"denoised_image = non_local_means_denoising(sample_image)\n\nplt.figure(figsize = (12, 8))\nplt.subplot(1,2,1)\nplt.imshow(sample_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Normal Image\")\n\nplt.subplot(1,2,2)  \nplt.imshow(denoised_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Denoised image\")    \n# Automatically adjust subplot parameters to give specified padding.\nplt.tight_layout()","4c22df23":"def sobel_edge_detection(image):\n  \"\"\"\n  Using Sobel filter\n\n  Sobel filter takes the following arguments : \n  1. Original Image\n  2. Depth of the destination image\n  3. Order of derivative x\n  4. Order of derivative y\n  5. Kernel size for convolutions\n\n  f(Image, depth, order_dx, order_dy, kernel_size) \n  \"\"\"\n  sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize = 5)\n  sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize = 5)\n  return sobel_x, sobel_y","bd3adf5d":"s_img_x, s_img_y = sobel_edge_detection(denoised_image)\n\nplt.figure(figsize = (12, 8))\nplt.subplot(2,2,1)\nplt.imshow(sample_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Sample Image\")\n\nplt.subplot(2,2,2)\nplt.imshow(denoised_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Denoised Image\")\n\nplt.subplot(2,2,3)\nplt.imshow(s_img_x, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Sobel X filtered Image\")\n\nplt.subplot(2,2,4)\nplt.imshow(s_img_y, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Sobel Y filtered Image\")\n\n# Automatically adjust subplot parameters to give specified padding.\nplt.tight_layout()","77e18b38":"from collections import deque\ndef canny_edge_detection(image):\n  edges = cv2.Canny(image, 170, 200) \n  return edges\n\ndef primary_roi(original_image, edge_image):\n  edge_coordinates = deque()\n  for i in tqdm(range(edge_image.shape[0])):\n    for j in range(edge_image.shape[1]):\n      if edge_image[i][j] != 0 :\n        edge_coordinates.append((i, j))\n  \n  min_row = edge_coordinates[np.argsort([coordinate[0] for coordinate in edge_coordinates])[0]][0]\n  max_row = edge_coordinates[np.argsort([coordinate[0] for coordinate in edge_coordinates])[-1]][0]\n  min_col = edge_coordinates[np.argsort([coordinate[1] for coordinate in edge_coordinates])[0]][1]\n  max_col = edge_coordinates[np.argsort([coordinate[1] for coordinate in edge_coordinates])[-1]][1]\n  \n  new_image = original_image.copy()\n  new_edge_image = edge_image.copy()\n  \n  new_image[min_row - 10 : min_row + 10, min_col : max_col] = [255, 0, 0]\n  new_image[max_row - 10 : max_row + 10, min_col : max_col] = [255, 0, 0]\n  new_image[min_row : max_row , min_col - 10 : min_col + 10] = [255, 0, 0]\n  new_image[min_row : max_row , max_col - 10 : max_col + 10] = [255, 0, 0]\n\n  new_edge_image[min_row - 10 : min_row + 10, min_col : max_col] = [255]\n  new_edge_image[max_row - 10 : max_row + 10, min_col : max_col] = [255]\n  new_edge_image[min_row : max_row , min_col - 10 : min_col + 10] = [255]\n  new_edge_image[min_row : max_row , max_col - 10 : max_col + 10] = [255]\n\n  roi_image = new_image[min_row : max_row, min_col : max_col]\n  edge_roi_image = new_edge_image[min_row : max_row, min_col : max_col]\n  \n  \n  return roi_image, edge_roi_image","820237a6":"plt.figure(figsize = (12, 8))\nplt.subplot(1,2,1)\nplt.imshow(sample_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Sample Image\")\n\nedge_image = canny_edge_detection(sample_image) \n\nplt.subplot(1,2,2)\nplt.imshow(edge_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Canny Edge Image\")\n# Automatically adjust subplot parameters to give specified padding.\nplt.tight_layout()","2135ff38":"roi_image, edge_roi_image = primary_roi(sample_image, edge_image)\n\nplt.figure(figsize = (12, 8))\nplt.subplot(1,2,1)\nplt.imshow(roi_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"ROI Image\")\n\nplt.subplot(1,2,2)\nplt.imshow(edge_roi_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Edge ROI Image\")  \n# Automatically adjust subplot parameters to give specified padding.\nplt.tight_layout()","16557d2e":"def histogram_equalization(roi_image):\n  image_ycrcb = cv2.cvtColor(roi_image, cv2.COLOR_RGB2YCR_CB)\n  y_channel = image_ycrcb[:, :, 0] # apply histogram equalization on this channel\n  cr_channel = image_ycrcb[:, :, 1]\n  cb_channel = image_ycrcb[:, :, 2]\n  # local histogram equalization\n  clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n  equalized = clahe.apply(y_channel)\n  equalized_image = cv2.merge([equalized, cr_channel, cb_channel])\n  equalized_image = cv2.cvtColor(equalized_image, cv2.COLOR_YCR_CB2RGB)\n  return equalized_image","61d770b1":"equalized_roi_image = histogram_equalization(roi_image)\n\nplt.figure(figsize = (12, 8))\nplt.subplot(1,2,1)\nplt.imshow(roi_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"ROI Image\")\n\nplt.subplot(1,2,2)\nplt.imshow(equalized_roi_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Histogram Equalized ROI Image\")  \n# Automatically adjust subplot parameters to give specified padding.\nplt.tight_layout()","46dbb5c5":"otsu_threshold, otsu_image = cv2.threshold(cv2.cvtColor(equalized_roi_image, cv2.COLOR_RGB2GRAY), 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n\nplt.figure(figsize = (12, 8))\nplt.subplot(1,2,1)\nplt.imshow(equalized_roi_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"equalized_roi_image\")\n\nplt.subplot(1,2,2)\nplt.imshow(otsu_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Otsu's Thresholded Image\")  \n# Automatically adjust subplot parameters to give specified padding.\n\nplt.tight_layout()","7c8e8456":"def segmentation(image, k, attempts) : \n    vectorized = np.float32(image.reshape((-1, 3)))\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)\n    res , label , center = cv2.kmeans(vectorized, k, None, criteria, attempts, cv2.KMEANS_PP_CENTERS)\n    center = np.uint8(center)\n    res = center[label.flatten()]\n    segmented_image = res.reshape((image.shape))\n    return segmented_image","2ef41c4f":"plt.figure(figsize = (12, 8))\nplt.subplot(2,2,1)\nplt.imshow(equalized_roi_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Histogram Equalized Image\")\n\nsegmented_image = segmentation(equalized_roi_image, 3, 10) # k = 3, attempt = 10\nplt.subplot(2,2,2)\nplt.imshow(segmented_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Segmented Image with k = 3\")\n\nsegmented_image = segmentation(equalized_roi_image, 4, 10) # k = 4, attempt = 10\nplt.subplot(2,2,3)\nplt.imshow(segmented_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Segmented Image with k = 4\")\n\nsegmented_image = segmentation(equalized_roi_image, 5, 10) # k = 5, attempt = 10\nplt.subplot(2,2,4)\nplt.imshow(segmented_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Segmented Image with k = 5\")","0d36abd1":"IMAGE_SIZE = (224, 224, 3)","42836f3f":"train_image = []\nfor id in tqdm(train_data[\"image_id\"]):\n    image = load_image(id)\n    image = resize(image, IMAGE_SIZE)\n    \n    edge_image = canny_edge_detection(image)\n    roi_image, _ = primary_roi(image, edge_image)\n    equalized_roi_image = histogram_equalization(roi_image)\n    \n    train_image.append(image)","fa8818e1":"#x_train = np.ndarray(shape = (len(train_image), IMAGE_SIZE[0], IMAGE_SIZE[1], 3), dtype = np.float32)\nx_train = np.array(train_image)\nx_train = x_train\/255.0\n\ny = train_data.copy()\ndel y[\"image_id\"]\ny_train = np.array(y.values)","655f7e50":"print(x_train.shape)\nprint(y_train.shape)","b9ae3d0d":"sample = x_train[5]\nsample.shape","b4e3f296":"plt.imshow(sample)\nprint(\"Label = \", y_train[5])","c1e07089":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state = 115)\nx_train, y_train = smote.fit_resample(x_train.reshape((-1, IMAGE_SIZE[0] * IMAGE_SIZE[1] * 3)), y_train)\nx_train = x_train.reshape((-1, IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\nprint(x_train.shape)\nprint(y_train.shape)","b5fe6f55":"def label_count(label) :    \n    count = 0 \n    for entry in y_train :\n        if np.array_equal(label, entry):    \n            count += 1\n    return count     ","c60d898b":"healthy = np.array([1,0,0,0])\nmultiple_diseases = np.array([0,1,0,0])\nrust = np.array([0,0,1,0])\nscab = np.array([0,0,0,1])","2e10e930":"count_healthy = label_count(healthy)\ncount_multiple_diseases = label_count(multiple_diseases)\ncount_rust = label_count(rust)\ncount_scab = label_count(scab)\n\nlabels = [\"Healthy\", \"Multiple Diseased\", \"Rust\", \"Scab\"]\ncounts = [count_healthy, count_multiple_diseases, count_rust, count_scab]\nexplode = (0.05, 0.05, 0.05, 0.05)\nfig, ax = plt.subplots(figsize = (20, 12))\nax.pie(counts, explode = explode, labels = labels, shadow = True, startangle = 90)\nax.axis(\"equal\") # equal aspect ratio ensures pie graph is drawn as circle","8c6bef31":"print(x_train.shape)\nprint(y_train.shape)","c7227bd0":"VALIDATION_FACTOR = 0.1\n\nval_size = int(len(x_train) * VALIDATION_FACTOR)\n\ntrain_x = x_train[: len(x_train) - val_size]\ntrain_y = y_train[: len(y_train) - val_size] # len(x_train) = len(y_train)\n\nval_x = x_train[len(x_train) - val_size : len(x_train)]\nval_y = y_train[len(y_train) - val_size : len(y_train)]\n\nprint(\"Shape of training data = \", train_x.shape, train_y.shape)\nprint(\"Shape of validation data = \", val_x.shape, val_y.shape)","fc17a286":"sample_image = train_x[250]\nplt.imshow(sample_image)\nplt.grid(False)\nprint(\"Image label = \", train_y[250])","86f245ed":"monitor_es = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 25, \n                                              restore_best_weights = False, verbose = 0)","db06edde":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\ngpus = tf.config.list_physical_devices(\"GPU\")\nprint(gpus)\nif len(gpus) == 1 : \n    strategy = tf.distribute.OneDeviceStrategy(device = \"\/gpu:0\")\nelse:\n    strategy = tf.distribute.MirroredStrategy()","7e2ba815":"tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\" : True})\nprint(\"Mixed precision enabled\")","181a76c4":"vgg = tf.keras.applications.vgg16.VGG16(include_top = False, weights = \"imagenet\", input_shape = IMAGE_SIZE)","9fd4668e":"vgg.summary()","a234835f":"vgg.input","0fea117a":"vgg.layers[-1].output","88843440":"output = vgg.layers[-1].output\noutput = tf.keras.layers.GlobalAveragePooling2D()(output)\nvgg_model = tf.keras.models.Model(vgg.input, output)","ecae6905":"vgg_model.trainable = True\nset_trainable = False\nfor layer in vgg_model.layers : \n    if layer.name in ['block5_conv1', 'block4_conv1']:\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False","5ba5dc6e":"pd.set_option(\"max_colwidth\", -1)\ninfo = [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]\nanalysis = pd.DataFrame(info, columns = [\"Layer Type\", \"Layer Name\", \"Train Status\"])\nanalysis","28a51b0f":"def get_bottleneck_features(deep_learning_model, input_images) :\n    #tf.keras.backend.clear_session()\n    #with strategy.scope() : #with tf.device(\"\/device:GPU:0\"):\n    with tf.device(\"\/device:GPU:0\"):\n        bottleneck_features = deep_learning_model.predict(input_images, verbose = 1)\n        return bottleneck_features\n\ntraining_bottleneck_features = get_bottleneck_features(vgg_model, train_x)\nvalidation_bottleneck_features = get_bottleneck_features(vgg_model, val_x)\n\nprint(\"Shape of training bottleneck features = \", training_bottleneck_features.shape)\nprint(\"Shape of validation bottleneck features = \", validation_bottleneck_features.shape)","f98ca6e3":"vgg_model.output_shape[1]","e0b6ec1f":"input_shape = vgg_model.output_shape[1]\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.InputLayer(input_shape = (input_shape, )))\n\nmodel.add(tf.keras.layers.Dense(512, activation = \"relu\", input_dim = input_shape))\nmodel.add(tf.keras.layers.Dropout(0.4))\n\nmodel.add(tf.keras.layers.Dense(512, activation = \"relu\"))\nmodel.add(tf.keras.layers.Dropout(0.4))\n          \nmodel.add(tf.keras.layers.Dense(4, activation = \"softmax\"))\n\nmodel.compile(loss='categorical_crossentropy', optimizer = tf.optimizers.Adam(lr = 0.001), metrics=['accuracy'])\nmodel.summary() ","e2ff963e":"BATCH_SIZE = 32\nEPOCHS = 100","edd9a488":"with tf.device(\"\/device:GPU:0\"): #with tf.device(\"\/device:GPU:0\"):\n    history = model.fit(x = training_bottleneck_features, y = train_y,\n                        validation_data = (validation_bottleneck_features, val_y),\n                        batch_size = BATCH_SIZE, epochs= EPOCHS, verbose = 1)","a64e34c2":"X = np.arange(0,EPOCHS,1)\nplt.figure(1, figsize = (20, 12))\nplt.subplot(1,2,1)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.plot(X, history.history[\"loss\"], label = \"Training Loss\")\nplt.plot(X, history.history[\"val_loss\"], label = \"Validation Loss\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.plot(X, history.history[\"accuracy\"], label = \"Training Accuracy\")\nplt.plot(X, history.history[\"val_accuracy\"], label = \"Validation Accuracy\")\nplt.grid(True)\nplt.legend()","22a146ce":"del train_x\ndel val_x\ndel train_y\ndel val_y","302d4761":"del training_bottleneck_features\ndel validation_bottleneck_features","3485311a":"import gc\ngc.collect()","3d5c5c46":"test_data","8913d315":"test_image = []\nfor id in tqdm(test_data[\"image_id\"][:5]):\n    image = load_image(id)\n    image = resize(image, IMAGE_SIZE)\n    \n    edge_image = canny_edge_detection(image)\n    roi_image, _ = primary_roi(image, edge_image)\n    equalized_roi_image = histogram_equalization(roi_image)\n    \n    test_image.append(image)","d602eec9":"test_x = np.array(test_image)\ntest_x = test_x\/255.0","5509b776":"test_bottleneck_features = get_bottleneck_features(vgg_model, test_x)","01290a9d":"y_pred = model.predict(test_bottleneck_features)\ny_pred.shape","700aee7c":"def convert_label(label) : \n    m = max(label)\n    index = list(label).index(m)\n    if index == 0 : \n        return \"Healthy\"\n    elif index == 1 : \n        return \"Multiple Diseased\"\n    elif index == 2 : \n        return \"Rust\"\n    else:\n        return \"Scab\"","c94cc5ea":"plt.figure(figsize = (12, 8))\nplt.subplot(1,2,1)\nplt.imshow(test_x[0], cmap = \"gray\")\nlabel = convert_label(y_pred[0])\nplt.grid(False)\nplt.title(\"First Test Image\")\nprint(\"Label of first testing image\", label)\n\nplt.subplot(1,2,2)\nplt.imshow(test_x[2], cmap = \"gray\")\nlabel = convert_label(y_pred[2])\nplt.grid(False)\nplt.title(\"Second Test Image\")\nprint(\"Label of first testing image\", label)\n\nplt.tight_layout()","2367b107":"# Configure GPU for Deep Learning","91105a77":"Of course, the testing data ought to go through the same pre-processing stages as the training data before we feed it to our model for classification.","7be1c0c9":"# Edge detection Using Sobel filter : \nEdge detection is one of the fundamental operation in image processing. Using this, we can reduce the amount of pixels while maintaining the structural aspect of the images. \n\nThe basic operation involved behind edge detection is called Convolution and is illustrated below : \n\n![image.png](attachment:image.png)\n\nEdges can be detected using various kinds of filters.\n\n* First derivative based Sobel filter(for thicker edges)\n* Second derivative based Laplacian filter(for finer edges)\n\nHere, we want to consider the area containing only the leaf, while ignoring the background green. Hence, we use Sobel filter to identify the prominent edge of the leaf.","30642d77":"# Image Segmentation(Half-Toned Images) : Otsu's Binarization\n\nIn global thresholding, we used an arbitrary chosen value as a threshold. In contrast, Otsu's method avoids having to choose a value and determines it automatically. \n\nWe will apply Otsu's binarization segmentation method on the histogram equalized image obtained in the previous stage.","8f8f699e":"It's evident that all values of k do an amazing job in clustering the image into different segments. K= 5 appears to be the best upon closer inspection.","ea3c9a0b":"# Analysing the color channel distribution in each Image ","43514de8":"Clearly, there is a class imbalance problem which needs to be addressed during the deep learning model construction.","7cc3b3a2":"## Bottleneck features : \n\nThe last activation feature map in the VGG-16 model (output from block5_pool) gives us the bottleneck features, which can then be flattened and fed to a fully connected deep neural network classifier.\n\nWe flatten the bottleneck features in the vgg_model object to make them ready to be fed to our fully connected classifier.\n\nA way to save time in model training is to use this model and extract out all the features from our training and validation datasets and then feed them as inputs to our classifier.","60d5baeb":"# Test Our Model : ","0285edd4":"K-means is mostly useful for applications like image compression or object recognition, because for these types of applications, it is inefficient to process the whole image.\n\n### K-Means Segmentation Approach Using OpenCV\n\n* `samples` : It should be of np.float32 data type, and each feature should be put in a single column. Here we have 3 channels, so every channel features have to be in one column. So, total columns we have are 3, while we don't care about the number of rows, hence -1. So, shape : (-1, 3).\n\n* `nclusters(K)` : Number of clusters required at end.\n\n* `criteria` : It is the iteration termination criteria. When this criteria is satisfied, algorithm iteration stops. Actually, it should be a tuple of 3 parameters. They are ( type, max_iter, epsilon )\n\nType of termination criteria. It has 3 flags as below:\n\n* `cv.TERM_CRITERIA_EPS` - stop the algorithm iteration if specified accuracy, epsilon, is reached.\n* `cv.TERM_CRITERIA_MAX_ITER` - stop the algorithm after the specified number of iterations, max_iter.\n* `cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER` - stop the iteration when any of the above condition is met.\n* `max_iter` - An integer specifying maximum number of iterations. epsilon - Required accuracy\n\n1. `attempts` : Flag to specify the number of times the algorithm is executed using different initial labellings. The algorithm returns the labels that yield the best compactness. This compactness is returned as output.\n\n2. `flags` : This flag is used to specify how initial centers are taken. Normally two flags are used for this : cv.KMEANS_PP_CENTERS and cv.KMEANS_RANDOM_CENTERS.\n\n*Output parameters :\n\n`compactness` : It is the sum of squared distance from each point to their corresponding centers.\n\n`labels` : This is the label array (same as 'code' in previous article) where each element marked '0', '1'.....\n\n`centers` : This is array of centers of clusters.*","2957c010":"# Data Reading","93295fd9":"![image.png](attachment:image.png) ","e44a9ccb":"# Histogram Equalization in ROI section of the whole image\n### First of all, why can we not apply histogram equalization directly to an RGB image?\nHistogram equalization is a non-linear process. Channel splitting and equalizing each channel separately is incorrect. Equalization involves intensity values of the image, not the color components. So for a simple RGB color image, histogram equalization cannot be applied directly on the channels. It needs to be applied in such a way that the intensity values are equalized without disturbing the color balance of the image. So, the first step is to convert the color space of the image from RGB into one of the color spaces that separates intensity values from color components. Some of the possible options are HSV\/HLS, YUV, YCbCr, etc. YCbCr is preferred as it is designed for digital images. Perform histogram equalization on the intensity plane Y. Now convert the resultant YCbCr image back to RGB.\n\n(Excerpt taken from :\n\nhttps:\/\/prateekvjoshi.com\/2013\/11\/22\/histogram-equalization-of-rgb-images\/ )","13a0580b":"# Early Stopping :\n\nStop training when a monitored metric has stopped improving.\n\n* monitor : Quantity to be monitored.\n* patience : Number of epochs with no improvement after which training will be stopped.\n* restore_best_weights : Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.\n* verbose: int. 0: quiet, 1: update messages\n\nMore info on Tensorflow 2.O documentations : https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/EarlyStopping","c4ad41e2":"Unfreezing the last few layers to custom train according to our needs, that is over four classes.","65624ea2":"# Transfer Learning with Fine Tuning : \n\nTransfer learning allows us to train deep networks using significantly less data then we would need if we had to train from scratch. With transfer learning, we are in effect transferring the \u201cknowledge\u201d that a model has learned from a previous task, to our current one. \n\nTransfer learning has been consistently proven to boost model accuracy and reduce require training time. Less data, less time, more accuracy.\n\nHere, we will be using VGG-16 neural network architecture.\n\n## VGG-16 Architecture :\n\nThe VGG-16 model is a 16-layer (convolution and fully connected) network built on the ImageNet database.\n\n![image.png](attachment:image.png)\n\nHere, in the fine tuning step we will freeze the initial pre-trained layers having the weights learned from the imagenet dataset, however the last few convolution-pooling layers will remain unfreezed, allowing us to train them on our dataset. Note that, we will be designing a custom head as the original VGG aimed to predict out of 1000 classes, whereas we only have four. \n\nAlso, it's been proven that adding a Global Average Pooling layer before the fully connected layer imroves the accuracy of the model considerably.\n\n**Global Average Pooling. Global Average Pooling is an operation that calculates the average output of each feature map in the previous layer. This fairly simple operation reduces the data significantly and prepares the model for the final classification layer** ","41d44096":"# Image Segmentation(Colored Images) : K-means Clustering\n\nIn the previous section we explored image segmentation using Otsu's Binarization. However, this is applied normally on half toned, that is binary(black and white) images. In this section, we will explore a Machine Learning technique called K-means clustering to segment the different areas of the image.\n\nOnce again, the operation will be performed on the histogram equalized image of plant leaf.","fda09783":"# Preparing Dataset for Deep Learning","e228dbba":"# Image Denoising :\n\nMany image smoothing techniques like Gaussian Blurring, Median Blurring etc were good to some extent in removing small quantities of noise. In those techniques, we took a small neighbourhood around a pixel and performed some operations like gaussian weighted average, median of the values etc to replace the central element. In short, noise removal at a pixel was local to its neighbourhood.\n\nThere is a property of noise. Noise is generally considered to be a random variable with zero mean.\n\nSuppose we hold a static camera to a certain location for a couple of seconds. This will give us plenty of frames, or a lot of images of the same scene. Then averaging all the frames, we compare the final result and first frame. Reduction in noise would be easily observed.\n\nSo idea is simple, we need a set of similar images to average out the noise. Considering a small window (say 5x5 window) in the image, chance is large that the same patch may be somewhere else in the image. Sometimes in a small neighbourhood around it. Hence, using these similar patches together averaging them can lead to an efficient denoised image.\n\nThis method is Non-Local Means Denoising. It takes more time compared to blurring techniques, but the result are very satisfying.\n\nDenoising illustration :\n\n![image.png](attachment:image.png)","3fc3ca12":"Using sobel filter we found the edges, however for further pre-processing we aim to consider only the area of the leaf, that is the fine textured area we see in the gradient images. For that, we will use a much powerful inbuilt function of open-CV called Canny(). This function will return the edge coordinates.\n\nEntire read is available on the OpenCV webpage :\n\nhttps:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/py_tutorials\/py_imgproc\/py_canny\/py_canny.html#canny","9a02ac9c":"Thank You!","1801cb3d":"### Noise Reduction : \n\nOne way to get rid of the noise on the image, is by applying Gaussian blur to smooth it. To do so, image convolution technique is applied with a Gaussian Kernel (3x3, 5x5, 7x7 etc\u2026). The kernel size depends on the expected blurring effect. Basically, the smallest the kernel, the less visible is the blur. \n\n### Gradient Calculation : \n\nThe Gradient calculation step detects the edge intensity and direction by calculating the gradient of the image using edge detection operators.\n\nThe result is almost the expected one, but we can see that some of the edges are thick and others are thin. Non-Max Suppression step will help us mitigate the thick ones.\n\n### Non-Maximum Supression : \n\nIdeally, the final image should have thin edges. Thus, we must perform non-maximum suppression to thin out the edges.\n\n\n### Double Threshold : \n\nThe double threshold step aims at identifying 3 kinds of pixels: strong, weak, and non-relevant:\n\n* Strong pixels are pixels that have an intensity so high that we are sure they contribute to the final edge.\n\n* Weak pixels are pixels that have an intensity value that is not enough to be considered as strong ones, but yet not small enough to be considered as non-relevant for the edge detection.\n\n* Other pixels are considered as non-relevant for the edge.\n\nTherefore, the significance of having two values in double threshold : \n\n* High threshold is used to identify the strong pixels (intensity higher than the high threshold)\n\n* Low threshold is used to identify the non-relevant pixels (intensity lower than the low threshold)\n\n* All pixels having intensity between both thresholds are flagged as weak and the Hysteresis mechanism (next step) will help us identify the ones that could be considered as strong and the ones that are considered as non-relevant.\n\n\n\n### Hysteresis : \n\nBased on the threshold results, the hysteresis consists of transforming weak pixels into strong ones, if and only if at least one of the pixels around the one being processed is a strong one. \n\nWe will be using OpenCV's implementation of Canny edge detection. This was the theory involved behind the entire process. \n\nFurther information can be found on OpenCV's documentation : https:\/\/docs.opencv.org\/trunk\/da\/d22\/tutorial_py_canny.html ","0599c02e":"**Inference** : \n* Rust leaves has brownish- yellowish patches\n* Scab leaves have brown stains.","a6b37497":"# Skewness in EDA :\n\nSkewness is the measure of symmetry or asymmetry of a data distribution. A distribution or data set is said to be symmetric if it looks same to the left and right point of the center.\n\nTypes of Skewness :\n\nSkewness is generally classified into 2 broad categories-\n\n* Right skewness or Positive skewness\n* Left skewness or Negative skewness\n\n![image.png](attachment:image.png)\n\nIt is very difficult to interpret and analyse the data which is skewed.","74f455f0":"**Inference** : \n\n* Red channel has positive skew, meaning the values are more concentrated at intensities lower than mean(somewhere around 90).\n* Green channel is negative skew, meaning the values are more concentrated at intentities higher than mean(somewhere in the range 130-150). This also means that green channel is more pronounced than red in the sample image set; and thereby the whole data set as they come from the same distribution. This makes sense as images are that of leaves!\n* Similarily, blue channel has a slight positive skew and is very well distributed.\n* The distribution of red and green color channels appears to be mesokurtic, aka normally distributed having k = 0 whereas the blue one appears to be relatively platykurtic having k < 0. Therefore out of the three colors, blue channel appears to be the most different one(relative outlier in the RGB color space). ","439108b4":"# Exploratory Data Analysis\n\nWhen we\u2019re getting started with a machine learning (ML) project, one critical principle to keep in mind is that data is everything. It is often said that if ML is the rocket engine, then the fuel is the (high-quality) data fed to ML algorithms. However, deriving truth and insight from a pile of data can be a complicated and error-prone job. To have a solid start for our ML project, it always helps to analyze the data up front.\n\nDuring EDA, it\u2019s important that we get a deep understanding of:\n\n* The properties of the data, such as schema and statistical properties;\n* The quality of the data, like missing values and inconsistent data types;\n* The predictive power of the data, such as correlation of features against target.","0eeac987":"# Handling Imbalanced Dataset : \n\nIn Exploratory Data Analysis, we plotted a pie chart depticting number of classes in the training data set. It was observed that the 'multiple diseased' class was in minority, hence creating an imbalance between the class distributions. In order to cater that, we will be using SMOTE algorithm to try and increase the minority class in our data set.\n\nSMOTE stands for Synthetic Minority Oversampling Technique.\n\n![image.png](attachment:image.png)\n\nThis algorithm aims to balance class distribution by randomly increasing minority class examples by replicating them. SMOTE synthesizes new minority instances between existing minority instances.\n\nAn amazing video explaining it can be found on YouTube : https:\/\/www.youtube.com\/watch?v=FheTDyCwRdE ","0fe7f3dd":"# Kurtosis :\n\nKurtosis is the characteristics of being flat or peaked. It is a measure whether data is heavy- tailed or light-tailed in a normal distribution\n\nA large kurtosis value often mean that the tails of the distributions are getting toward more extreme values than the tails of normal distributions. This may lead to a length of 6 or 7 standard deviation from the mean. Similarly, If the kurtosis value is very low, then the tails of the distributions will be less lengthier than the those of a normal distribution (less than 3 standard deviation).\n\n![image.png](attachment:image.png)\n\nA large value of kurtosis is often considered as more risky because data may tend to give an outlier value as outcome with greater distance from the mean if applied to any machine learning algorithm.","7bd10bc2":"Now, it's a well balanced dataset :-)","8aa4c882":"# Canny Edge Detector : \n\nThe Canny filter is a multi-stage edge detector. It uses a filter based on the derivative of a Gaussian in order to compute the intensity of the gradients.The Gaussian reduces the effect of noise present in the image. Then, potential edges are thinned down to 1-pixel curves by removing non-maximum pixels of the gradient magnitude. Finally, edge pixels are kept or removed using hysteresis thresholding on the gradient magnitude. \n\nThe Canny has three adjustable parameters: the width of the Gaussian (the noisier the image, the greater the width), and the low and high threshold for the hysteresis thresholding. ","3d1db29c":"## OpenCV implementation of the aforementioned approach :\n\ncv2.fastNlMeansDenoisingColored() - Works on Colored images cv2.fastNlMeansDenoising() - Works on graysacle images\n\nCommon arguments are:\n\nh : parameter deciding filter strength. Higher h value removes noise better, but removes details of image also. (10 is ok)\nhForColorComponents : same as h, but for color images only. (normally same as h)\ntemplateWindowSize : should be odd. (recommended 7)\nsearchWindowSize : should be odd. (recommended 21)","4153c30e":"The Canny edge detection algorithm is composed of 5 steps:\n\n* Noise reduction;\n* Gradient calculation;\n* Non-maximum suppression;\n* Double threshold;\n* Edge Tracking by Hysteresis."}}