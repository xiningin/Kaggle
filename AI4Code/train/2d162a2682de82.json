{"cell_type":{"fee2212d":"code","5e249d06":"code","fdccf011":"code","6c851142":"code","5ea8b1af":"code","26148c65":"code","2ff1de9c":"code","1036afb9":"code","6517ec57":"code","a3baf32c":"code","f4afef05":"code","ff47945f":"code","f9bfebaf":"code","0e9679ef":"code","0f278dfb":"code","ca14406a":"code","eb4f1830":"code","1c05b2f0":"code","18698360":"code","7dd4a2da":"code","cdc547b4":"code","22449c25":"code","71ce8d73":"code","a90350e1":"markdown","dc405b9e":"markdown","58a46269":"markdown","ab30f933":"markdown","03b437e8":"markdown","670d1e20":"markdown","cbb8bae7":"markdown","39cf5132":"markdown","d05a7616":"markdown","9e45b0d2":"markdown","7ca37454":"markdown","740e2018":"markdown","3cba4a56":"markdown","c73f10c3":"markdown","13c293a6":"markdown","0cd8e47e":"markdown","64750918":"markdown","3a9c0936":"markdown"},"source":{"fee2212d":"! pip install mglearn","5e249d06":"import mglearn\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')","fdccf011":"mglearn.plots.plot_knn_classification(n_neighbors=1)\nplt.title(\"forge_one_neighbor\")","6c851142":"mglearn.plots.plot_knn_classification(n_neighbors=3)","5ea8b1af":"X, y = mglearn.datasets.make_forge()\nx_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0)","26148c65":"from sklearn.neighbors import KNeighborsClassifier\n\nclf = KNeighborsClassifier(n_neighbors=3)\nclf.fit(x_train, y_train)","2ff1de9c":"clf.predict(x_test)","1036afb9":"clf.score(x_test, y_test)","6517ec57":"fig, axes = plt.subplots(1,3, figsize=(10,3))\n\nfor n_neighbors, ax, in zip([1,3,9], axes):\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    clf.fit(X, y)\n    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=0.4)\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap=mglearn.cm2)\n    ax.set_title(\"%d neighbors\" %n_neighbors)","a3baf32c":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\ncancer = load_breast_cancer()\nx_train, x_test, y_train, y_test = train_test_split(cancer.data, cancer.target, \n                                                    stratify=cancer.target, random_state=66)","f4afef05":"training_accuracy = []\ntest_accuracy = []\n\n#try n_neighbors from range 1-10\nneighbors_settings = range(1,11)","ff47945f":"for n_neighbors in neighbors_settings:\n    #build_model\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    clf.fit(x_train, y_train)\n    #record training and test accuracy\n    training_accuracy.append(clf.score(x_train, y_train))\n    test_accuracy.append(clf.score(x_test, y_test))","f9bfebaf":"plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\nplt.legend()","0e9679ef":"mglearn.plots.plot_knn_regression(n_neighbors=1)","0f278dfb":"mglearn.plots.plot_knn_regression(n_neighbors=3)","ca14406a":"X, y = mglearn.datasets.make_wave(n_samples=40)\nx_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0)","eb4f1830":"from sklearn.neighbors import KNeighborsRegressor\n\nreg = KNeighborsRegressor(n_neighbors=3)\nreg.fit(x_train, y_train)","1c05b2f0":"reg.predict(x_test)","18698360":"reg.score(x_test, y_test)","7dd4a2da":"fig, axes = plt.subplots(1, 3, figsize=(15,4))\n\n# create 1000 data points evenly spaced between -3 and 3\nline = np.linspace(-3,3, 1000).reshape(-1,1)\n\nplt.suptitle(\"nearest_neighbor_regression\")\nfor n_neighbors, ax in zip([1,3,9], axes):\n    # make predictions using 1,3 and 9 neighbors\n    reg = KNeighborsRegressor(n_neighbors=n_neighbors)\n    reg.fit(X, y)\n    ax.plot(X, y, 'o')\n    ax.plot(X, -3*np.ones(len(X)), 'o')\n    ax.plot(line, reg.predict(line))\n    ax.set_title(\"%d neighbors\" %n_neighbors)","cdc547b4":"training_accuracy = []\ntest_accuracy = []\n\n#try n_neighbors from range 1-10\nneighbors_settings = range(1,11)","22449c25":"for n_neighbors in neighbors_settings:\n    #build_model\n    clf = KNeighborsRegressor(n_neighbors=n_neighbors)\n    clf.fit(x_train, y_train)\n    #record training and test accuracy\n    training_accuracy.append(clf.score(x_train, y_train))\n    test_accuracy.append(clf.score(x_test, y_test))","71ce8d73":"plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\nplt.legend()","a90350e1":"## K-Neighbors Classification","dc405b9e":"<h1><center>K-Nearest Neighbor<\/center><\/h1>","58a46269":"### Analyzing k nearest neighbors regression","ab30f933":"Instead of considering only the closest neighbor, we can also consider an arbitrary\nnumber $k$ of neighbors. This is where the name of the $k$ neighbors algorithm\ncomes from. When considering more than one neighbor, we use voting to assign a\nlabel. This means, for each test point, we count how many neighbors are red, and\nhow many neighbors are blue. We then assign the class that is more frequent: in other\nwords, the majority class among the k neighbors.","03b437e8":"### Analyzing KNeighbors Classification","670d1e20":"* using single neighbor results in a decision boundary that follows the training data closely\n* Considering more and more neighbors leads to a smoother boundary.\n* A smoother boundary corresponds to simple model.\n* <b>Using few neighbors corresponds to high model complexity and using many neighbors corresponds to low model complexity.<\/b>","cbb8bae7":"* KNN algorithm is the simplest machine learning algorithm\n* Building the model only consist of storing the training dataset.\n* To make the prediction for new data point, the algorithm finds the closest data points in the training dataset, its \"nearest neighbors\"","39cf5132":"* Using only a single neighbor, each point in the training set has an obvious influence on the predictions, and the predicted values go through all of the data points. This leads to a very unsteady prediction.<br> \n\n* Considering more neighbors leads to smoother predictions, but these do not fit the training data as well.","d05a7616":"#### KNN regression using scikit-learn","9e45b0d2":"* Considering a single nearest neighbor, the prediction on the training set is perfect. Considering more neighbors, the model becomes more simple, and the training accuracy drops.<br><br>\n* The test set accuracy for using a single neighbor is lower then when using more neighbors, indicating that using a single nearest neighbor leads to a model that is too complex. On the other hand, when considering 10 neighbors, the model is too simple, and performance is even worse. The best performance is somewhere in the middle, around using six neighbors.","7ca37454":"There is also regression variant of the k-nearest neighbors algorithm.","740e2018":"* There are two important paramters to the KNeighbors classifier:\n * the number of neighbors\n * How you measure distance between data points.\n* By default, Euclidean distance is used, which works well in many settings.\n* The model is very easy to understand and often gives reasonable performance without a lot of adjustments.\n* Building nearest neighbor model is usually very fast, but when your training set is very large (number of features or number of samples) prediction can be slow.\n* It is not often used in practice, due to prediction being slow, and ots inability to handle many features.","3cba4a56":"## Strengths, weakness and parameters","c73f10c3":"In the plots above, the blue points are again the responses for the training data, while the green line is the prediction made by the model for all points on the line.","13c293a6":"The algorithm only considers nearest neighbor, which is the closest training data point to the point we want to make prediction for.","0cd8e47e":"## KNeighbors Regression","64750918":"#### K-Neighbor Nearest using scikit-learn","3a9c0936":"Same things can be observed in regression like we have seen in classification."}}