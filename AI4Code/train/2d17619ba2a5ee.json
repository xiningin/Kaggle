{"cell_type":{"6fb703dd":"code","6f787636":"code","31260f23":"code","74f0f041":"code","b0df8cab":"code","1d031fe3":"code","22ce83ca":"code","7ad531f9":"code","cb315c60":"code","655471c5":"code","302c2876":"code","272485ab":"code","dcc297fd":"code","a799f4df":"code","a71a5a0a":"code","82a598b5":"code","be121f1f":"code","78cddb82":"code","97a6e85b":"code","db025660":"code","c3f5f1ee":"code","cfa2f55d":"code","05c71e74":"code","39679a4c":"code","9e894c40":"code","83784ef0":"code","b1a3b17f":"code","1b3c8595":"code","997d3d53":"code","81ef3e32":"code","7c621c12":"code","3c3571e1":"markdown","bfc69241":"markdown","8d860981":"markdown","72f9fee2":"markdown","35f430d8":"markdown","5a4e15b7":"markdown","1a9fda6d":"markdown","45823960":"markdown","7c67e6bf":"markdown","1045a982":"markdown","5b1daa1b":"markdown","78d3c243":"markdown","2f6d6bb9":"markdown","fb335655":"markdown","763a7007":"markdown","b5f98b99":"markdown","c0bedcd2":"markdown","ece72ba0":"markdown","f06291d0":"markdown","9b707b22":"markdown","493fab0e":"markdown","a6eecb27":"markdown","9481e5bf":"markdown","61731a70":"markdown","6166754e":"markdown","6ffeacfa":"markdown","aee3877d":"markdown","45f02561":"markdown","ca1f2047":"markdown","f86b5963":"markdown"},"source":{"6fb703dd":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6f787636":"spam = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv')\nspam.head()","31260f23":"spam.info()","74f0f041":"new = spam[['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']].copy()\nna_string = '-'\nspam['v2'] = spam['v2'].str.cat(new, sep = \", \", na_rep = na_string)\nspam.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace = True)\nspam","b0df8cab":"def to_lower(text):\n    return text.lower()","1d031fe3":"import re\ncontractions_dict = {     \n\"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\",\n\"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n\"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\"he'd\": \"he had\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\",\n\"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I had\", \"I'd've\": \"I would have\",\n\"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it had\",\n\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"iit will have\", \"it's\": \"it is\", \"let's\": \"let us\",\n\"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n\"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she had\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n\"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that had\", \"that'd've\": \"that would have\",\n\"that's\": \"that is\", \"there'd\": \"there had\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they had\",\n\"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n\"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we had\", \"we'd've\": \"we would have\",\n\"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n\"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n\"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n\"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n\"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n\"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you had\",\n\"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n}\n\ndef expand_contraction(text, contraction_dict):\n    contraction_pattern= re.compile('({})'.format('|'.join(contraction_dict.keys())), flags= re.IGNORECASE | re.DOTALL)\n    \n    def expand_match(contraction):\n        match= contraction.group(0)\n        first_char= match[0]\n        expanded_contraction= contraction_dict.get(match) \\\n            if contraction_dict.get(match) \\\n            else contraction_dict.get(match.lower())\n        expanded_contraction= expanded_contraction\n        return expanded_contraction\n        \n    expanded_text= contraction_pattern.sub(expand_match, text)\n    expanded_text= re.sub(\"'\",\"\", expanded_text)\n    return expanded_text\n\ndef main_contraction(text):\n    text = expand_contraction(text, contractions_dict)\n    return text","22ce83ca":"def remove_number(text):\n    output = ''.join(c for c in text if not c.isdigit())\n    return output","7ad531f9":"from string import punctuation\ndef remove_punct(text):\n    return \"\".join(c for c in text if c not in punctuation)","cb315c60":"def to_strip(text):\n    return \" \".join([c for c in text.split() if len(c)>1])","655471c5":"def remove_char(text):\n    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n    return text","302c2876":"def remove_duplicate(text):\n    text = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", text)\n    return text","272485ab":"import nltk\nfrom nltk.corpus import stopwords\nstopwords.words('english')\n\ndef remove_stopwords(text):\n    stop_words= stopwords.words('english')\n    \n    return ' '.join(c for c in nltk.word_tokenize(text) if c not in stop_words)","dcc297fd":"from nltk.stem import WordNetLemmatizer\n\nwordnet_lemma = WordNetLemmatizer()\n\ndef lemma(text):\n    lemmatize_words = [wordnet_lemma.lemmatize(word) for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    return ' '.join(lemmatize_words)","a799f4df":"spam['prep1']= spam['v2'].apply(to_lower)\nspam['prep2']= spam['prep1'].apply(main_contraction)\nspam['prep3']= spam['prep2'].apply(remove_number)\nspam['prep4']= spam['prep3'].apply(remove_punct)\nspam['prep5']= spam['prep4'].apply(to_strip)\nspam['prep6']= spam['prep5'].apply(remove_char)\nspam['prep7']= spam['prep6'].apply(remove_duplicate)\nspam['prep8']= spam['prep7'].apply(remove_stopwords)\nspam['lemma'] = spam['prep8'].apply(lemma)\nspam","a71a5a0a":"import seaborn as sns\nimport matplotlib.pyplot as plt","82a598b5":"spam['length'] = spam['v2'].apply(len)\n\nplt.figure(figsize = (15, 6))\nspam0 = spam[spam['v1'] == 'ham']\nspam1 = spam[spam['v1'] == 'spam']\nsns.distplot(spam0['length'])\nsns.distplot(spam1['length'])\nplt.legend(['Ham', 'Spam'])\nplt.show()","be121f1f":"spam['length'] = spam['v2'].apply(len)\n\nplt.figure(figsize = (15, 6))\nspam2= spam[spam['length']<200]\nspam0 = spam2[spam2['v1'] == 'ham']\nspam1 = spam2[spam2['v1'] == 'spam']\nsns.distplot(spam0['length'])\nsns.distplot(spam1['length'])\nplt.legend(['Ham', 'Spam'])\nplt.show()","78cddb82":"def dictionary(check):\n    check = check.str.extractall('([a-zA_Z]+)')\n    check.columns = ['check']\n    b = check.reset_index(drop=True)\n    check = b['check'].value_counts()\n    \n    dictionary = pd.DataFrame({'word': check.index, 'freq': check.values})\n    dictionary.index = dictionary['word']\n    dictionary.drop('word', axis = 1, inplace=True)\n    dictionary.sort_values('freq', inplace= True, ascending= False)\n    \n    return dictionary\n\ndictionary_clean = dictionary(spam['lemma'])\ndictionary_clean[:20].plot(kind = 'barh',figsize = (10,10))","97a6e85b":"from sklearn.model_selection import train_test_split","db025660":"pd.DataFrame(spam['v1'].value_counts()\/spam.shape[0]*100).round(2)","c3f5f1ee":"text = spam['lemma']\ny = np.where(spam['v1'] == 'spam', 1, 0)\n\ntext.shape","cfa2f55d":"text_train, text_test, y_train, y_test = train_test_split(text, y, \n                                                          stratify = y, \n                                                          test_size = 0.3, \n                                                          random_state = 1672)","05c71e74":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.metrics import classification_report, precision_score, accuracy_score, f1_score\nfrom sklearn.pipeline import Pipeline","39679a4c":"tfidf = TfidfVectorizer(analyzer='word', stop_words='english', token_pattern=r'\\w{1,}')\nlogreg = LogisticRegression(random_state = 1672)\nbayes = MultinomialNB()\nlsvc = LinearSVC(random_state = 1672)\nrf = RandomForestClassifier(random_state = 1672)\n\nlogreg_pipe = Pipeline([('vector', tfidf), ('model', logreg)])\nbayes_pipe = Pipeline([('vector', tfidf), ('model', bayes)])\nlsvc_pipe = Pipeline([('vector', tfidf), ('model', lsvc)])\nrf_pipe = Pipeline([('vector', tfidf), ('model', rf)])\n\ndef model_evaluation(model, metric):\n    model_cv = cross_val_score(model, text_train, y_train, cv = StratifiedKFold(n_splits = 5), scoring = metric)\n    return model_cv\n\nlogreg_pipe_cv = model_evaluation(logreg_pipe, 'f1')\nbayes_pipe_cv = model_evaluation(bayes_pipe, 'f1')\nlsvc_pipe_cv = model_evaluation(lsvc_pipe, 'f1')\nrf_pipe_cv = model_evaluation(rf_pipe, 'f1')\n\nscore_cv = [logreg_pipe_cv.round(5), bayes_pipe_cv.round(5), lsvc_pipe_cv.round(5), rf_pipe_cv.round(5)]\nscore_mean = [logreg_pipe_cv.mean(), bayes_pipe_cv.mean(), lsvc_pipe_cv.mean(), rf_pipe_cv.mean()]\nscore_std = [logreg_pipe_cv.std(), bayes_pipe_cv.std(), lsvc_pipe_cv.std(), rf_pipe_cv.std()]\n\nfor model in [logreg_pipe, bayes_pipe, lsvc_pipe, rf_pipe]:\n    model.fit(text_train, y_train)\n\nscore_f1 = [f1_score(y_test, logreg_pipe.predict(text_test)),\n                  f1_score(y_test, bayes_pipe.predict(text_test)),\n                  f1_score(y_test, lsvc_pipe.predict(text_test)),\n                  f1_score(y_test, rf_pipe.predict(text_test))]\n\nmethod_name = ['Logistic Regression Tfidf Vectorizer', 'Multinomial Naive Bayes Tfidf Vectorizer',\n               'Linear SVC Tfidf Vectorizer', 'Random Forest Classifier Tfidf Vectorizer']\n\nsummary = pd.DataFrame({'method': method_name,\n                            'cv score': score_cv,\n                            'mean score': score_mean,\n                            'std score': score_std,\n                            'f1 score': score_f1})\nsummary","9e894c40":"for model, model_name in zip([logreg_pipe, bayes_pipe, lsvc_pipe, rf_pipe],\n                             ['Logistic Regression Tfidf Vectorizer', 'Multinomial Naive Bayes Tfidf Vectorizer',\n                              'Linear SVC Tfidf Vectorizer', 'Random Forest Classifier Tfidf Vectorizer']):\n    model.fit(text_train, y_train)\n    y_pred = model.predict(text_test)\n    print(model_name+ ':')\n    print(classification_report(y_test, y_pred))","83784ef0":"from sklearn.model_selection import GridSearchCV","b1a3b17f":"lsvc_estimator = Pipeline([('vector', tfidf), ('model', lsvc)])\n\nhyperparam_space = {\n    'vector__analyzer': ['word', 'char','char_wb'],\n    'vector__max_features': [2000, 3000, 5000],\n    'model__C': [0.01, 0.1, 1, 10],\n    'model__multi_class': ['ovr', 'crammer_singer'],\n    'model__class_weight': ['dict', 'balanced'],\n    'model__random_state': [1672]\n}\n\ngrid = GridSearchCV(\n                lsvc_estimator,\n                param_grid = hyperparam_space,\n                cv = StratifiedKFold(n_splits = 5),\n                scoring = 'f1',\n                n_jobs = -1)\n\ngrid.fit(text_train, y_train)\n\nprint('best score', grid.best_score_)\nprint('best param', grid.best_params_)","1b3c8595":"lsvc_pipe.fit(text_train, y_train)\nf1_lsvc = (f1_score(y_test, lsvc_pipe.predict(text_test)))\n\ngrid.best_estimator_.fit(text_train, y_train)\nf1_grid = (f1_score(y_test, grid.predict(text_test)))\n\nscore_list = [f1_lsvc, f1_grid]\nmethod_name = ['Linear SVC Tfidf Vectorizer Before Tuning',\n               'Linear SVC Tfidf Vectorizer After Tuning']\nbest_summary = pd.DataFrame({\n    'method': method_name,\n    'score': score_list\n})\nbest_summary","997d3d53":"for model, model_name in zip([lsvc_pipe, grid.best_estimator_],\n                             ['Linear SVC Tfidf Vectorizer Before Tuning',\n                              'Linear SVC Tfidf Vectorizer After Tuning']):\n    model.fit(text_train, y_train)\n    y_pred = model.predict(text_test)\n    print(model_name+ ':')\n    print(classification_report(y_test, y_pred))","81ef3e32":"sms = [\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.\",\n       \"Too late. I said i have the website. I didn't i have or dont have the slippers\",\n       \"Sir, i am waiting for your call, once free please call me.\",\n       \"how are you? I miss you!\"]","7c621c12":"predictions = lsvc_pipe.predict(sms)\nresult = np.where(predictions == 1, 'Spam', 'Ham')\nresult","3c3571e1":"# Text Pre-processing","bfc69241":"### *Remove or Convert Number*","8d860981":"* After the tuning process, the f1 score is decreasing.","72f9fee2":"# HyperParameter Tuning","35f430d8":"### *Length Of Sentence*\n* Identify if spam and ham text can differ in other content, such as length of text.","5a4e15b7":"### *Remove Special Characters*\n* This regex expression states that match the text string for any alphabets from 'small a' to 'small z or 'capital A' to 'capital Z'. I remove spaces again using the \\s pattern, which refers to a single space.","1a9fda6d":"### *Lemmatization*\n* Lemmatization usually refers to doing proper vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word. I prefer to use Lemmatization rather than Stemming in the text normalization process because Lemmatization considers the context and converts the word to its meaningful base form. Sometimes, the same word can have multiple different Lemma.","45823960":"* This is one way to improve the f1 score with tuning using Grid Search. I choose this library because Grid Search will give the best improvement score. In fact, Grid Search is heavy to run because it's working with the parameters one by one for each data.","7c67e6bf":"* The data is indicated for imbalanced but I decide not to process for further because the score is already higher.","1045a982":"### *Remove StopWords or Particular Words*","5b1daa1b":"### *Expand Contraction*","78d3c243":"* The score is almost equal between before and after tuning. Still, the highest f1 score is the Linear SVC model with TF-IDF Vectorizer before Tuning.","2f6d6bb9":"### *Words Frequency*\n* Can be used to check whether or not there are still words frequantly occur but not meaningful","fb335655":"### *Data Splitting*","763a7007":"# Text Exploration","b5f98b99":"### *Convert To Lower Case*","c0bedcd2":"### *Remove WhiteSpaces*\n* I also remove a word that only has one letter.","ece72ba0":"### *Define Model*\n* Most term frequency is not a good measure of the importance of a word\/term to a document's topic. Very common words like \"the\", \"a\", \"to\" is almost always the terms with the highest frequency in the text. To circumvent the elimination of term-frequency, I normalize it by the inverse document frequency (IDF). The result of that is the TF-IDF matrix. The inverse document frequency is a measurement of how much information the word provides, that is, whether the term is common or rare across all documents in the corpus.","f06291d0":"* This is the score that I will use to continue the process. Based on the Linear SVC model with TF-IDF Vectorizer, it has the highest mean score, the std score is the lowest one meaning it's good, and also has the highest f1 score. Let's continue to see how the classification report supports this score.","9b707b22":"* I use 0.3 as test_size, which means 70% as train data and 30% as test data.","493fab0e":"# Result Comparison","a6eecb27":"### *Remove Duplicates*\n* Removing multiple characters (duplicates) depends on the noise content in the dataset and it should be performed before removing stopwords. (.) match and capture any single character \\1{2,} then match the same character two or more times. The quantity \\1 represents the first capture group in sub.","9481e5bf":"### *Remove Punctuation*","61731a70":"### Result From Text Pre-processing ","6166754e":"# Prediction Testing","6ffeacfa":"* I try to see the differences in the length of words in every class. From the plots, I can conclude that Spam and Ham almost have an equal length, but the Spam density is higher than the Ham.","aee3877d":"* The highest f1 score based on the classification report is the Linear SVM model with TF-IDF Vectorizer.","45f02561":"* *0 = Ham*\n* *1 = Spam*\n\n        - TN: SMS predict with Ham and the actual is Ham\n        - TP: SMS predict with Spam and the actual is Spam\n        - FP: SMS predict with Spam and the actual is Ham\n        - FN: SMS predict with Ham and the actual is Spam\n\nActions:\n* FN: The prediction is inaccurate. The spam SMS will be sent and the receiver will be annoyed because of that although it can be deleted manually.\n* FP: If ham SMS is important, it may be removed as spam based on the prediction.\n\n    -> Of all the results, the metric that I use to determine the score is using f1-score (f1) to anticipate error prediction.","ca1f2047":"# Modeling","f86b5963":"* I join all 3 unnamed columns into v2 because it's still contained with sentences."}}