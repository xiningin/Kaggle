{"cell_type":{"8e385be8":"code","93621470":"code","1e2c4e3c":"code","1cc3e852":"code","5ce4bae0":"code","0e1152d5":"code","b3e90380":"code","81a9a535":"code","03492396":"code","299dc485":"code","f7b02a8f":"code","2897903a":"code","6c4a1e5b":"code","72838969":"code","b97fb395":"code","76d42326":"code","b535127e":"code","fe33c7f3":"code","16c49b0c":"code","a8327b7f":"code","926906be":"code","dc1a0c7e":"code","e619e3b0":"code","e95da91e":"code","9214d57f":"code","b6d4af4f":"code","50d827f9":"code","849ba04a":"code","56923927":"code","1be402a4":"code","bb992eb5":"code","694388ec":"code","57bd8423":"code","f743254f":"code","78c8999e":"code","a84256dc":"code","9ae50b0d":"code","8808a1b3":"code","89d2dfcc":"code","fba5fb0b":"code","20ce82a1":"code","1498d389":"markdown"},"source":{"8e385be8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nimport re\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93621470":"train_data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_data = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","1e2c4e3c":"train_data = train_data.fillna('NaN')\ntest_data = test_data.fillna('NaN')","1cc3e852":"train_data.head()","5ce4bae0":"train_data.iloc[2]['text']","0e1152d5":"link = re.compile(r'http')\ncaps = re.compile(r'[A-Z]{3,9}')\n\ntrain_data['contains_link'] = train_data.text.apply(lambda x: 0 if link.search(x) is None else 1)\ntrain_data['contains_caps'] = train_data.text.apply(lambda x: 0 if caps.search(x) is None else 1)\nprint(train_data.contains_link.value_counts())\ntrain_data.groupby(['contains_link', 'contains_caps']).target.value_counts(normalize = True)","b3e90380":"train_data.location.value_counts()","81a9a535":"train_data.groupby('keyword').target.value_counts(normalize = True, sort = False)","03492396":"table = train_data.groupby('keyword').target.value_counts(normalize = True, sort = False).reset_index(level = [0])\n\ntable_1 = table[table.index == 1]\ntable_0 = table[table.index == 0]\n\nkeyword_dict = {}\n\nfor i in range(len(table_1)):\n    keyword_dict[table_1.iloc[i]['keyword']] = table_1.iloc[i]['target']\n\nfor i in range(len(table_0)):\n    if table_0.iloc[i]['keyword'] not in keyword_dict:\n        keyword_dict[table_0.iloc[i]['keyword']] = 0\n","299dc485":"keyword_dict['wreckage']","f7b02a8f":"print(table_1['keyword'])","2897903a":"train_data.text = train_data.text.str.replace(r'\\W', ' ').str.lower()\ntrain_data.head()","6c4a1e5b":"vectorizer = CountVectorizer(ngram_range = (1,2), min_df = 0.0001, \n                             stop_words = 'english', binary = False)","72838969":"X_train, X_test, y_train, y_test =  train_test_split(train_data.text, \n                                                     train_data.target,\n                                                     train_size = 0.8,\n                                                     stratify = train_data.target)","b97fb395":"train_vector = vectorizer.fit_transform(X_train)\ntest_vector = vectorizer.transform(X_test)","76d42326":"classifier = MultinomialNB()\nclassifier.fit(train_vector, y_train)","b535127e":"print(confusion_matrix(y_train, classifier.predict(train_vector)))","fe33c7f3":"y_pred = classifier.predict(test_vector)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","16c49b0c":"classifier.predict_proba(test_vector)[:, 1]","a8327b7f":"train_data['naive_probability'] = train_data.text.apply(lambda x: classifier.predict_proba(vectorizer.transform([x]))[:, 1][0])\ntrain_data.head()","926906be":"train_data['keyword_probability'] = train_data.keyword.apply(lambda x: keyword_dict[x])\ntrain_data.head()","dc1a0c7e":"ct = ColumnTransformer(\n    [('ohe', OneHotEncoder(handle_unknown = 'ignore'), ['keyword'])],\nremainder = 'drop')\nohe = OneHotEncoder()","e619e3b0":"train_data['keyword']","e95da91e":"keyword_encoded = ohe.fit_transform(train_data[['keyword']])\n\nkeyword_naive_bayes = MultinomialNB()\nkeyword_naive_bayes.fit(keyword_encoded, train_data.target)\n\ntrain_data['encoded_keyword_probability'] = keyword_naive_bayes.predict_proba(keyword_encoded)[:,1]","9214d57f":"keyword_encoded[0,4]","b6d4af4f":"train_data.describe()","50d827f9":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.scatter(train_data['encoded_keyword_probability'], train_data['naive_probability'], c = train_data.target, alpha = 0.2)","849ba04a":"train_data['temp_param'] = train_data.text.apply(lambda x: len(x))","56923927":"features = ['encoded_keyword_probability','keyword_probability', 'naive_probability', 'contains_link', 'contains_caps']","1be402a4":"sns.pairplot(train_data[features + ['target']], hue = 'target')","bb992eb5":"plt.scatter(train_data['naive_probability'], train_data['temp_param'], c = train_data.target, alpha = 0.2)","694388ec":"X_train, X_test, y_train, y_test =  train_test_split(train_data[features], \n                                                     train_data.target,\n                                                     train_size = 0.8,\n                                                     stratify = train_data.target)","57bd8423":"combined_classifier = LogisticRegression()\ncombined_classifier.fit(X_train, y_train)","f743254f":"y_pred = combined_classifier.predict(X_train)\nprint(confusion_matrix(y_train, y_pred))\nprint(classification_report(y_train, y_pred))","78c8999e":"y_pred = combined_classifier.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","a84256dc":"'''print(combined_classifier.coef_)\nprint(combined_classifier.intercept_)'''","9ae50b0d":"test_data.text = test_data.text.str.replace(r'\\W', ' ').str.lower()\ntest_data.head()","8808a1b3":"test_data['naive_probability'] = test_data.text.apply(lambda x: classifier.predict_proba(vectorizer.transform([x]))[:, 1][0])\ntest_data.head()","89d2dfcc":"test_data['keyword_probability'] = test_data.keyword.apply(lambda x: keyword_dict[x])\ntest_data.head()","fba5fb0b":"test_data['target'] = combined_classifier.predict(test_data[features])\ntest_data.head()","20ce82a1":"test_data[['id', 'target']].to_csv('submission.csv', \n                                   index = False)","1498d389":"strategies for prediction\n1. Use only 'keyword' column for prediction.\n2. Use only tweet column for prediction. MutinomialNB\n3. Combine both (how to combine?)"}}