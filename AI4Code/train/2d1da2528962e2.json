{"cell_type":{"ed9e8539":"code","394fe0ae":"code","83fb0d15":"code","3d6693a0":"code","d2073866":"code","4deb7b00":"code","bebf9a5f":"code","027f8d8a":"code","df2a7964":"code","7388d95c":"code","ef5d9375":"code","bff35ea0":"code","8b251ce1":"code","28496448":"code","11745051":"code","d27dc185":"code","ca769806":"code","5aeaa0d9":"code","356a6d13":"code","d4cfd806":"code","93e5bb07":"code","a7492f07":"code","a67deca4":"code","71661237":"code","4add15ff":"code","326dc06a":"code","820db7e0":"code","5a56f12d":"code","0fe85360":"code","6a9483c4":"code","ea2968eb":"code","43b64f4b":"code","b9ef6545":"code","4bada181":"code","aeb25562":"code","bcd11ddd":"code","56249916":"code","f08a19ac":"code","8acfd696":"code","ebe68ce7":"code","baa59b96":"code","726ecbca":"code","00afef17":"code","6f603aa5":"code","b592e0ff":"code","eb41e211":"code","9b809081":"code","3a1c5576":"code","0ffd02a0":"code","268d16b7":"code","71cc247d":"code","90a9c9d1":"markdown","0d8dd53f":"markdown","0242e1b3":"markdown","1e822982":"markdown","39e26e7f":"markdown","8de8f33d":"markdown","ef467ce9":"markdown","2185b5bb":"markdown","63f1cdc8":"markdown","e216f3b8":"markdown","efdd3633":"markdown","56c461f0":"markdown","accd41ba":"markdown","3a843bfe":"markdown"},"source":{"ed9e8539":"import os\nimport numpy as np\nimport pandas as pd\nimport time\nfrom tqdm import tqdm\nimport h2o\n\nimport lightgbm as lgb\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\n\nfrom scipy.sparse import hstack\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","394fe0ae":"import os\nprint(os.listdir(\"..\/input\"))","83fb0d15":"print(os.listdir(\"..\/input\/embeddings\"))","3d6693a0":"train = pd.read_csv('..\/input\/train.csv').fillna(' ')\ntest = pd.read_csv('..\/input\/test.csv').fillna(' ')","d2073866":"train.head()","4deb7b00":"test.head()","bebf9a5f":"train.shape","027f8d8a":"test.shape","df2a7964":"train_target = train['target'].values\n\nnp.unique(train_target)","7388d95c":"train_target.mean()","ef5d9375":"eng_stopwords = set(stopwords.words(\"english\"))","bff35ea0":"## Number of words in the text ##\ntrain[\"num_words\"] = train[\"question_text\"].apply(lambda x: len(str(x).split()))\ntest[\"num_words\"] = test[\"question_text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain[\"num_unique_words\"] = train[\"question_text\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_unique_words\"] = test[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain[\"num_chars\"] = train[\"question_text\"].apply(lambda x: len(str(x)))\ntest[\"num_chars\"] = test[\"question_text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain[\"num_stopwords\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest[\"num_stopwords\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n## Number of punctuations in the text ##\ntrain[\"num_punctuations\"] =train['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest[\"num_punctuations\"] =test['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain[\"num_words_upper\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"num_words_upper\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain[\"num_words_title\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest[\"num_words_title\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain[\"mean_word_len\"] = train[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest[\"mean_word_len\"] = test[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","8b251ce1":"plt.figure(figsize=(12,8))\nsns.violinplot(data=train['num_words'])\nplt.show()\n","28496448":"plt.figure(figsize=(12,8))\nsns.violinplot(data=train['num_unique_words'])\nplt.show()","11745051":"plt.figure(figsize=(12,8))\nsns.violinplot(data=train['num_chars'])\nplt.show()","d27dc185":"plt.figure(figsize=(12,8))\nsns.violinplot(data=train['num_stopwords'])\nplt.show()","ca769806":"plt.figure(figsize=(12,8))\nsns.violinplot(data=train['num_punctuations'])\nplt.show()","5aeaa0d9":"plt.figure(figsize=(12,8))\nsns.violinplot(data=train['num_words_upper'])\nplt.show()","356a6d13":"plt.figure(figsize=(12,8))\nsns.violinplot(data=train['num_words_title'])\nplt.show()","d4cfd806":"plt.figure(figsize=(12,8))\nsns.violinplot(data=train['mean_word_len'])\nplt.show()","93e5bb07":"eng_features = ['num_words', 'num_unique_words', 'num_chars', \n                'num_stopwords', 'num_punctuations', 'num_words_upper', \n                'num_words_title', 'mean_word_len']","a7492f07":"kf = KFold(n_splits=5, shuffle=True, random_state=43)\ntest_pred = 0\noof_pred = np.zeros([train.shape[0],])\n\nx_test = test[eng_features].values\nfor i, (train_index, val_index) in tqdm(enumerate(kf.split(train))):\n    x_train, x_val = train.loc[train_index][eng_features].values, train.loc[val_index][eng_features].values\n    y_train, y_val = train_target[train_index], train_target[val_index]\n    classifier = LogisticRegression(C= 0.1)\n    classifier.fit(x_train, y_train)\n    val_preds = classifier.predict_proba(x_val)[:,1]\n    preds = classifier.predict_proba(x_test)[:,1]\n    test_pred += 0.2*preds\n    oof_pred[val_index] = val_preds","a67deca4":"pred_train = (oof_pred > 0.5).astype(np.int)\nf1_score(train_target, pred_train)","71661237":"f1_score(train_target, pred_train)","4add15ff":"pred_train = (oof_pred > 0.12).astype(np.int)\nf1_score(train_target, pred_train)","326dc06a":"train_text = train['question_text']\ntest_text = test['question_text']\nall_text = pd.concat([train_text, test_text])\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 1),\n    max_features=5000)\nword_vectorizer.fit(all_text)\ntrain_word_features = word_vectorizer.transform(train_text)\ntest_word_features = word_vectorizer.transform(test_text)","820db7e0":"kf = KFold(n_splits=5, shuffle=True, random_state=43)\ntest_pred_tf = 0\noof_pred_tf = np.zeros([train.shape[0],])\n\nfor i, (train_index, val_index) in tqdm(enumerate(kf.split(train))):\n    x_train, x_val = train_word_features[train_index,:], train_word_features[val_index,:]\n    y_train, y_val = train_target[train_index], train_target[val_index]\n    classifier = LogisticRegression(class_weight = \"balanced\", C=0.5, solver='sag')\n    classifier.fit(x_train, y_train)\n    val_preds = classifier.predict_proba(x_val)[:,1]\n    preds = classifier.predict_proba(test_word_features)[:,1]\n    test_pred_tf += 0.2*preds\n    oof_pred_tf[val_index] = val_preds\n","5a56f12d":"pred_train = (oof_pred_tf > 0.8).astype(np.int)\nf1_score(train_target, pred_train)","0fe85360":"0.566075663947416","6a9483c4":"pred_train = (0.8*oof_pred_tf+0.2*oof_pred > 0.68).astype(np.int)\nf1_score(train_target, pred_train)","ea2968eb":"0.5705038831309178","43b64f4b":"import lightgbm as lgb\n\ndef lgb_f1_score(y_hat, data):\n    y_true = data.get_label()\n    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n    return 'f1', f1_score(y_true, y_hat), True\n\nparams = {'learning_rate': 0.05,\n          'application': 'regression',\n          'max_depth': 9,\n          'num_leaves': 100,\n          'verbosity': -1,\n          'metric': 'rmse',\n          'data_random_seed': 3,\n          'bagging_fraction': 0.8,\n          'feature_fraction': 0.4,\n          'nthread': 16,\n          'lambda_l1': 1,\n          'lambda_l2': 1,\n          'num_rounds': 2700,\n          'verbose_eval': 100}\n\n\nkf = KFold(n_splits=5, shuffle=True, random_state=43)\ntest_pred_lgb = 0\noof_pred_lgb = np.zeros([train.shape[0],])\n\nfor i, (train_index, val_index) in tqdm(enumerate(kf.split(train))):\n    x_train, x_val = train_word_features[train_index,:], train_word_features[val_index,:]\n    y_train, y_val = train_target[train_index], train_target[val_index]\n    \n    d_train = lgb.Dataset(x_train, label=y_train)\n    d_valid = lgb.Dataset(x_val, label=y_val)\n\n    num_rounds = 2500\n    model = lgb.train(params,\n                  train_set=d_train,\n                  num_boost_round=num_rounds,\n                  valid_sets=[d_train, d_valid],\n                  valid_names=['train', 'val'],\n                  verbose_eval=0)\n    \n    val_preds = model.predict(x_val)\n    preds = classifier.predict(test_word_features)\n    test_pred_lgb += 0.2*preds\n    oof_pred_lgb[val_index] = val_preds","b9ef6545":"pred_train = (oof_pred_lgb > 0.3).astype(np.int)\nf1_score(train_target, pred_train)","4bada181":"pred_train = (0.65*oof_pred_lgb+0.35*oof_pred_tf+0.1*oof_pred > 0.5).astype(np.int)\nf1_score(train_target, pred_train)","aeb25562":"# Train Vectorizor\nfrom sklearn.feature_extraction.text import CountVectorizer \n\nbow = CountVectorizer()","bcd11ddd":"kf = KFold(n_splits=5, shuffle=True, random_state=43)\ntest_pred_cv = 0\noof_pred_cv = np.zeros([train.shape[0],])\n\n\nfor i, (train_index, val_index) in tqdm(enumerate(kf.split(train))):\n    x_train, x_val = train.loc[train_index]['question_text'].values, train.loc[val_index]['question_text'].values\n    y_train, y_val = train_target[train_index], train_target[val_index]\n    x_test = test['question_text'].values\n    \n    bow = CountVectorizer()\n    x_train = bow.fit_transform(x_train)\n    x_val = bow.transform(x_val)\n    x_test = bow.transform(x_test)\n\n    classifier = LogisticRegression(penalty = \"l1\", C = 1.25, class_weight = \"balanced\")\n    \n    classifier.fit(x_train, y_train)\n    val_preds = classifier.predict_proba(x_val)[:,1]\n    preds = classifier.predict_proba(x_test)[:,1]\n    test_pred_cv += 0.2*preds\n    oof_pred_cv[val_index] = val_preds","56249916":"kf = KFold(n_splits=5, shuffle=True, random_state=43)\ntest_pred_cv_2 = 0\noof_pred_cv_2 = np.zeros([train.shape[0],])\ntest_pred_cv_3 = 0\noof_pred_cv_3 = np.zeros([train.shape[0],])\n\n\nfor i, (train_index, val_index) in tqdm(enumerate(kf.split(train))):\n    x_train, x_val = train.loc[train_index]['question_text'].values, train.loc[val_index]['question_text'].values\n    y_train, y_val = train_target[train_index], train_target[val_index]\n    x_test = test['question_text'].values\n    \n    bow = CountVectorizer()\n    x_train = bow.fit_transform(x_train)\n    x_val = bow.transform(x_val)\n    x_test = bow.transform(x_test)\n    \n    classifier2 = MultinomialNB()\n    classifier3 = BernoulliNB()\n    \n    classifier2.fit(x_train, y_train)\n    val_preds = classifier2.predict_proba(x_val)[:,1]\n    preds = classifier2.predict_proba(x_test)[:,1]\n    test_pred_cv_2 += 0.2*preds\n    oof_pred_cv_2[val_index] = val_preds\n    \n    classifier3.fit(x_train, y_train)\n    val_preds = classifier3.predict_proba(x_val)[:,1]\n    preds = classifier3.predict_proba(x_test)[:,1]\n    test_pred_cv_3 += 0.2*preds\n    oof_pred_cv_3[val_index] = val_preds","f08a19ac":"pred_train = (oof_pred_cv > 0.75).astype(np.int)\nf1_score(train_target, pred_train)","8acfd696":"pred_train = (oof_pred_cv_2 > 0.7).astype(np.int)\nf1_score(train_target, pred_train)","ebe68ce7":"pred_train = (oof_pred_cv_3 > 0.7).astype(np.int)\nf1_score(train_target, pred_train)","baa59b96":"pred_train = (0.7*oof_pred_cv+0.2*oof_pred_cv_2+0.1*oof_pred_cv_3 > 0.7).astype(np.int)\nf1_score(train_target, pred_train)","726ecbca":"pred_train = (0.63*(0.7*oof_pred_cv+0.2*oof_pred_cv_2+0.1*oof_pred_cv_3) +0.37*(0.65*oof_pred_lgb+0.35*oof_pred_tf+0.1*oof_pred)> 0.59).astype(np.int)\nf1_score(train_target, pred_train)","00afef17":"stack_train = np.hstack((oof_pred.reshape(-1,1), oof_pred_tf.reshape(-1,1), oof_pred_lgb.reshape(-1,1), \n                         oof_pred_cv_3.reshape(-1,1), oof_pred_cv_2.reshape(-1,1), oof_pred_cv.reshape(-1,1)))","6f603aa5":"stack_test = np.hstack((test_pred.reshape(-1,1), test_pred_tf.reshape(-1,1), test_pred_lgb.reshape(-1,1), \n                         test_pred_cv_3.reshape(-1,1), test_pred_cv_2.reshape(-1,1), test_pred_cv.reshape(-1,1)))","b592e0ff":"stack_train.shape","eb41e211":"stack_test.shape","9b809081":"kf = KFold(n_splits=5, shuffle=True, random_state=43)\ntest_pred_stack = 0\noof_pred_stack = np.zeros([train.shape[0],])\n\nfor i, (train_index, val_index) in tqdm(enumerate(kf.split(train))):\n    x_train, x_val = stack_train[train_index,:], stack_train[val_index,:]\n    y_train, y_val = train_target[train_index], train_target[val_index]\n    classifier = LogisticRegression(class_weight = \"balanced\", C=0.5, solver='sag')\n    classifier.fit(x_train, y_train)\n    val_preds = classifier.predict_proba(x_val)[:,1]\n    preds = classifier.predict_proba(stack_test)[:,1]\n    test_pred_stack += 0.2*preds\n    oof_pred_stack[val_index] = val_preds","3a1c5576":"score = 0\nthresh = .5\nfor i in np.arange(0.1, 0.951, 0.01):\n    temp_score = f1_score(train_target, (oof_pred_stack > i))\n    if(temp_score > score):\n        score = temp_score\n        thresh = i\n\nprint(\"CV: {}, Threshold: {}\".format(score, thresh))\n","0ffd02a0":"0.6207799320845656","268d16b7":"pred_test = ( test_pred_stack> thresh).astype(np.int)\nsubmission = pd.DataFrame.from_dict({'qid': test['qid']})\nsubmission['prediction'] = pred_test\nsubmission.to_csv('submission.csv', index=False)","71cc247d":"1","90a9c9d1":"## Overview\n\nThe purpose of this kernel is to take a look at the data, come up with some insights, and attempt to create a predictive model or two. This notebook is still **very** raw. I will work on it as my very limited time permits, and hope to expend it in the upcoming days and weeks.\n\n## Packages\n\nFirst, let's load a few useful Python packages. This section will keep growing in subsequent versions of this EDA.","0d8dd53f":"Seems farily straightforward - just ID, text and target firlds. In addition, the train set is very decently sized - 1.3 million records is probably enough for a decent text classifier. \n\nLet's take a look at the targetvariable:\n","0242e1b3":"The following classifiers are inspired by dust's [Naive Bayes notebook](https:\/\/www.kaggle.com\/stardust0\/naive-bayes-and-logistic-regression-baseline):","1e822982":"The following LightGBM model is based on Peter's [LGB Baseline notebook](https:\/\/www.kaggle.com\/peterhurford\/lgb-baseline):","39e26e7f":"This is a kernels-only competition, which means that we can only use tools and data that are available to us in a single Kaggle kernel. The Pyhon libraries that are available to us are the standard Kaggle kernels compute environment. So let's take a look at the data that's available to us: ","8de8f33d":"We see that we have access to four different 300-dimensional embeddings. 300-dimensional embeddings are probably the best ones from the standpoint of a single-best-model, but having access to lower dimensional embeddings would have been nice from the prototyping standpoint. We'll get back to the embeddings later, but let's now take a look at the train and test files.","ef467ce9":"The following Logistic Regression is based on Premvardhan's [Count Vectorizer notebook](https:\/\/www.kaggle.com\/premvardhan\/quora-insincere-question-classification):","2185b5bb":"To be continued ...","63f1cdc8":"Now let's see how well a logistic regression trained on these features does:","e216f3b8":"For EDA and later modeling, it might be a good idea to create some metafeatures. This work is partly based on SRK's great EDAs, and [this one](http:\/\/www.kaggle.com\/sudalairajkumar\/simple-feature-engg-notebook-spooky-author) in particular. The metafeatures that we'll create are:\n\n\n* Number of words in the text\n* Number of unique words in the text\n* Number of characters in the text\n* Number of stopwords\n* Number of punctuations\n* Number of upper case words\n* Number of title case words\n* Average length of the words","efdd3633":"We see that the input folder contains, in addition to the standard train, test, and sample_submission files, another folder which presumably contains various embeddings. Let's take a look at what embeddings are availabel to us. ","56c461f0":"Now we'll train on the full set and make predictions based on that:","accd41ba":"For our second model we'll use TF-IDF with a logistic regression. The next couple of secontions are based on my [LR with n-grams notebook](https:\/\/www.kaggle.com\/tunguz\/lr-with-words-n-grams-baseline). Firtst, let's embed all the text vectors:","3a843bfe":"That's pretty good: just two classes, but the positive class makes just over 6% of the total. So the target is heavily unbalanced, which is why a metric such as F1 seems appropriate for this kind of problem. "}}