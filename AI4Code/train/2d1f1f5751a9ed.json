{"cell_type":{"8b54182d":"code","7b3ff330":"code","96be44b9":"code","2aaeea2c":"code","75d7c876":"code","7784cada":"code","60ebf5e0":"code","c2965032":"code","d0e71030":"code","7e27cc94":"code","3ce77213":"code","35e791a3":"code","785da305":"code","5646246f":"code","b577fa05":"code","00025a65":"code","6164b276":"code","00a09d97":"code","ddfc7752":"code","3232afe8":"code","61bed6e5":"code","e941fec9":"code","35dc65e9":"code","48ef19c2":"code","20f4b70b":"code","f1de2a76":"code","17aec519":"code","c6cc9aca":"code","f7040a80":"code","32e16092":"code","77158a15":"code","22a89994":"code","eb0ab24f":"code","c6e8f65e":"code","ba708297":"code","553830da":"code","91e423a3":"code","8c2b2793":"code","76cdb1bb":"code","1f507016":"code","36f883a3":"code","f0b6c5fa":"code","428ebba8":"code","d07f9dd4":"code","c34099ae":"code","e13df605":"code","c4940043":"code","a9d3bd7a":"code","7801a100":"code","e386c3eb":"markdown","82a2d33c":"markdown","6ab5e9f3":"markdown","9ad2fad7":"markdown","2db1b31a":"markdown","7d88a979":"markdown","d33ec4ef":"markdown","1423b2d6":"markdown","ca6670f1":"markdown","02fbcef0":"markdown","05aa1878":"markdown","a0571380":"markdown","7387fb96":"markdown","30c83207":"markdown","dea09d64":"markdown","4d4b1311":"markdown","93a098e5":"markdown","fa8c6924":"markdown","ae33539f":"markdown","15739e78":"markdown","2bcfc394":"markdown","a1254ad8":"markdown","2446ce00":"markdown","6716111f":"markdown","aa0a1c3f":"markdown","54df4926":"markdown","2095e773":"markdown","2284fcea":"markdown","f1464f1e":"markdown","d06d5d40":"markdown","e4d8a699":"markdown","6d3281a8":"markdown","97150114":"markdown","483272dd":"markdown","8d23f0d8":"markdown","f7461e88":"markdown","33f8288f":"markdown","c14aba09":"markdown"},"source":{"8b54182d":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n% matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#nltk\nimport nltk\n\n#preprocessing\nfrom nltk.corpus import stopwords  #stopwords\nfrom nltk import word_tokenize,sent_tokenize # tokenizing\nfrom nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n\n# for part-of-speech tagging\nfrom nltk import pos_tag\n\n# for named entity recognition (NER)\nfrom nltk import ne_chunk\n\n# vectorizers for creating the document-term-matrix (DTM)\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n\n# BeautifulSoup libraray\nfrom bs4 import BeautifulSoup \n\nimport re # regex\n\n#model_selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#evaluation\nfrom sklearn.metrics import accuracy_score,roc_auc_score \nfrom sklearn.metrics import classification_report\nfrom mlxtend.plotting import plot_confusion_matrix\n\n#preprocessing scikit\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder\n\n#classifiaction.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB\n \n#stop-words\nstop_words=set(nltk.corpus.stopwords.words('english'))\n\n#keras\nimport keras\nfrom keras.preprocessing.text import one_hot,Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM\nfrom keras.models import Model\nfrom keras.preprocessing.text import text_to_word_sequence\n\n#gensim w2v\n#word2vec\nfrom gensim.models import Word2Vec","7b3ff330":"rev_frame=pd.read_csv(r'..\/input\/Reviews.csv')","96be44b9":"df=rev_frame.copy()","2aaeea2c":"df.head()","75d7c876":"df=df[['Text','Score']]","7784cada":"df['review']=df['Text']\ndf['rating']=df['Score']\ndf.drop(['Text','Score'],axis=1,inplace=True)\n","60ebf5e0":"print(df.shape)\ndf.head()","c2965032":"# check for null values\nprint(df['rating'].isnull().sum())\ndf['review'].isnull().sum()  # no null values.","d0e71030":"# remove duplicates\/ for every duplicate we will keep only one row of that type. \ndf.drop_duplicates(subset=['rating','review'],keep='first',inplace=True) ","7e27cc94":"# now check the shape. note that shape is reduced which shows that we did has duplicate rows.\nprint(df.shape)\ndf.head()","3ce77213":"# printing some reviews to see insights.\nfor review in df['review'][:5]:\n    print(review+'\\n'+'\\n')","35e791a3":"def mark_sentiment(rating):\n  if(rating<=3):\n    return 0\n  else:\n    return 1","785da305":"df['sentiment']=df['rating'].apply(mark_sentiment)","5646246f":"df.drop(['rating'],axis=1,inplace=True)","b577fa05":"df.head()","00025a65":"df['sentiment'].value_counts()","6164b276":"# function to clean and pre-process the text.\ndef clean_reviews(review):  \n    \n    # 1. Removing html tags\n    review_text = BeautifulSoup(review,\"lxml\").get_text()\n    \n    # 2. Retaining only alphabets.\n    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n    \n    # 3. Converting to lower case and splitting\n    word_tokens= review_text.lower().split()\n    \n    # 4. Remove stopwords\n    le=WordNetLemmatizer()\n    stop_words= set(stopwords.words(\"english\"))     \n    word_tokens= [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n    \n    cleaned_review=\" \".join(word_tokens)\n    return cleaned_review","00a09d97":"pos_df=df.loc[df.sentiment==1,:][:50000]\nneg_df=df.loc[df.sentiment==0,:][:50000]","ddfc7752":"pos_df.head()","3232afe8":"neg_df.head()","61bed6e5":"#combining\ndf=pd.concat([pos_df,neg_df],ignore_index=True)","e941fec9":"print(df.shape)\ndf.head()","35dc65e9":"# shuffling rows\ndf = df.sample(frac=1).reset_index(drop=True)\nprint(df.shape)  # perfectly fine.\ndf.head()\n","48ef19c2":"# import gensim\n# # load Google's pre-trained Word2Vec model.\n# pre_w2v_model = gensim.models.KeyedVectors.load_word2vec_format(r'drive\/Colab Notebooks\/amazon food reviews\/GoogleNews-vectors-negative300.bin', binary=True) \n","20f4b70b":"tokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\nsentences=[]\nsum=0\nfor review in df['review']:\n  sents=tokenizer.tokenize(review.strip())\n  sum+=len(sents)\n  for sent in sents:\n    cleaned_sent=clean_reviews(sent)\n    sentences.append(cleaned_sent.split()) # can use word_tokenize also.\nprint(sum)\nprint(len(sentences))  # total no of sentences","f1de2a76":"# trying to print few sentences\nfor te in sentences[:5]:\n  print(te,\"\\n\")","17aec519":"import gensim\nw2v_model=gensim.models.Word2Vec(sentences=sentences,size=300,window=10,min_count=1)","c6cc9aca":"w2v_model.train(sentences,epochs=10,total_examples=len(sentences))","f7040a80":"# embedding of a particular word.\nw2v_model.wv.get_vector('like')","32e16092":"# total numberof extracted words.\nvocab=w2v_model.wv.vocab\nprint(\"The total number of words are : \",len(vocab))","77158a15":"# words most similar to a given word.\nw2v_model.wv.most_similar('like')","22a89994":"# similaraity b\/w two words\nw2v_model.wv.similarity('good','like')","eb0ab24f":"print(\"The no of words :\",len(vocab))\n# print(vocab)","c6e8f65e":"# print(vocab)\nvocab=list(vocab.keys())","ba708297":"word_vec_dict={}\nfor word in vocab:\n  word_vec_dict[word]=w2v_model.wv.get_vector(word)\nprint(\"The no of key-value pairs : \",len(word_vec_dict)) # should come equal to vocab size\n  ","553830da":"# # just check\n# for word in vocab[:5]:\n#   print(word_vec_dict[word])","91e423a3":"# cleaning reviews.\ndf['clean_review']=df['review'].apply(clean_reviews)","8c2b2793":"# number of unique words = 56379.\n\n# now since we will have to pad we need to find the maximum lenght of any document.\n\nmaxi=-1\nfor i,rev in enumerate(df['clean_review']):\n  tokens=rev.split()\n  if(len(tokens)>maxi):\n    maxi=len(tokens)\nprint(maxi)","76cdb1bb":"tok = Tokenizer()\ntok.fit_on_texts(df['clean_review'])\nvocab_size = len(tok.word_index) + 1\nencd_rev = tok.texts_to_sequences(df['clean_review'])","1f507016":"max_rev_len=1565  # max lenght of a review\nvocab_size = len(tok.word_index) + 1  # total no of words\nembed_dim=300 # embedding dimension as choosen in word2vec constructor","36f883a3":"# now padding to have a amximum length of 1565\npad_rev= pad_sequences(encd_rev, maxlen=max_rev_len, padding='post')\npad_rev.shape   # note that we had 100K reviews and we have padded each review to have  a lenght of 1565 words.","f0b6c5fa":"# now creating the embedding matrix\nembed_matrix=np.zeros(shape=(vocab_size,embed_dim))\nfor word,i in tok.word_index.items():\n  embed_vector=word_vec_dict.get(word)\n  if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n    embed_matrix[i]=embed_vector\n  # if word is not found then embed_vector corressponding to that vector will stay zero.","428ebba8":"# checking.\nprint(embed_matrix[14])","d07f9dd4":"# prepare train and val sets first\nY=keras.utils.to_categorical(df['sentiment'])  # one hot target as required by NN.\nx_train,x_test,y_train,y_test=train_test_split(pad_rev,Y,test_size=0.20,random_state=42)","c34099ae":"from keras.initializers import Constant\nfrom keras.layers import ReLU\nfrom keras.layers import Dropout\nmodel=Sequential()\nmodel.add(Embedding(input_dim=vocab_size,output_dim=embed_dim,input_length=max_rev_len,embeddings_initializer=Constant(embed_matrix)))\n# model.add(CuDNNLSTM(64,return_sequences=False)) # loss stucks at about \nmodel.add(Flatten())\nmodel.add(Dense(16,activation='relu'))\nmodel.add(Dropout(0.50))\n# model.add(Dense(16,activation='relu'))\n# model.add(Dropout(0.20))\nmodel.add(Dense(2,activation='sigmoid'))  # sigmod for bin. classification.","e13df605":"model.summary()","c4940043":"# compile the model\nmodel.compile(optimizer=keras.optimizers.RMSprop(lr=1e-3),loss='binary_crossentropy',metrics=['accuracy'])","a9d3bd7a":"# specify batch size and epocj=hs for training.\nepochs=5\nbatch_size=64","7801a100":"# fitting the model.\nmodel.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_test,y_test))","e386c3eb":"#### There is nothing much that I can figure out except the fact that there are some stray words and some punctuation that we have to remove before moving ahead.\n\n**But note that if I remove the punctuation now then it will be difficult to break the reviews into sentences which is required by Word2Vec constructor in Gensim. So we will first break text into sentences and then clean those sentences. **","82a2d33c":"### LOADING THE DATASET","6ab5e9f3":"#### Now creating a dictionary with words in vocab and their embeddings. This will be used when we will be creating embedding matrix (for feeding to keras embedding layer).","9ad2fad7":"#### Pre-processing steps :\n\n1 ) First **removing punctuation and html tags** if any. note that the html tas may be present ast the data must be scraped from net.\n\n2) **Tokenize** the reviews into tokens or words .\n\n3) Next **remove the stop words and shorter words** as they cause noise.\n\n4) **Stem or lemmatize** the words depending on what does better. Herer I have yse lemmatizer.","2db1b31a":"#### Parameters: -\n\n**sentences : ** The sentences we have obtained.\n\n**size : ** The dimesnions of the vector used to represent each word.\n\n**window : ** The number f words around any word to see the context.\n\n**min_count : ** The minimum number of times a word should appear for its embedding to be formed or learnt.\n","7d88a979":"#### Let us now see if any of the column has any null values.","d33ec4ef":"####  Now actually creating the word 2 vec embeddings.","1423b2d6":"#### First we need to break our data into sentences which is requires by the constructor of the Word2Vec class in Gensim. For this I have used Punk English tokenizer from the NLTK.","ca6670f1":"As you can see the sentiment column now has sentiment of the corressponding product review.","02fbcef0":"### PREPARING THE DATA FOR KERAS EMBEDDING LAYER.","05aa1878":"Now we have obtained the w2v embeddings. But there are a couple of steps required by Keras embedding layer before we can move on.\n\n**Also note that since w2v embeddings have been made now ; we can preprocess our review column by using the function that we saw above.**","a0571380":"#### Note that loss as well as val_loss is  is still deceasing. You can train for more no of epochs but I am not so patient ;)\n\n**The final accuracy after 5 epochs is about 84% which is pretty decent.**","7387fb96":"## THE END!!!","30c83207":"#### Let us now print some reviews and see if we can get insights from the text.","dea09d64":"#### Now we integer encode the words in the reviews using Keras tokenizer. \n\n**Note that there two important variables: which are the vocab_size which is the total no of unique words while the second is max_doc_len which is the length of every document after padding. Both of these are required by the Keras embedding layer.**","4d4b1311":"#### Now we need to pass the w2v word embeddings to the embedding layer in Keras. For this we will create the embedding matrix and pass it as 'embedding_initializer' parameter to the layer.\n\n**The embedding matrix will be of dimensions (vocab_size,embed_dim) where the word_index of each word from keras tokenizer is its index into the matrix and the corressponding entry is its w2v vector ;)**\n\n**Note that there may be words which will not be present in embeddings learnt by the w2v model. The embedding matrix entry corressponding to those words will be a vector of all zeros.**\n\n**Also note that if u are thinkng why won't a word be present then it is bcoz now we have learnt on out own corpus but if we use pre-trained embedding then it may happen that some words specific to our dataset aren't present then in those cases we may use a fixed vector of zeros to denote all those words that earen;t present in th pre-trained embeddings. Also note that it may also happen that some words are not present ifu have filtered some words by setting min_count in w2v constructor.\n  **","93a098e5":"#### A brief description of the dataset from Overview tab on Kaggle : -\n\nData includes:\n- Reviews from Oct 1999 - Oct 2012\n- 568,454 reviews\n- 256,059 users\n- 74,258 products\n- 260 users with > 50 reviews","fa8c6924":"#### Since here I am concerned with sentiment analysis I shall keep only the 'Text' and the 'Score' column.","ae33539f":"### CREATING GOOGLE WORD2VEC WORD EMBEDDINGS IN GENSIM","15739e78":"#### Now let us print some sentences just to check iff they are in the correct fornat.","2bcfc394":"### CREATING THE EMBEDDING MATRIX","a1254ad8":"## [Please star\/upvote if u like it.]","2446ce00":"#### Let us now print a summary of the model.","6716111f":"## [Please star\/upvote if it was helpful.]","aa0a1c3f":"#### We can now combine reviews of each sentiment and shuffle them so that their order doesn't make any sense.","54df4926":"## Text Classification on Amazon Fine Food Dataset with Google Word2Vec Word Embeddings in Gensim and training using LSTM In Keras.","2095e773":"#### We need to find the maximum lenght of any document or review in our case. WE will pad all reviews to have this same length.This will be required by Keras embedding layer. Must check [this](https:\/\/www.kaggle.com\/rajmehra03\/a-detailed-explanation-of-keras-embedding-layer) kernel on Kaggle for a wonderful explanation of keras embedding layer.","2284fcea":"### DATA CLEANING AND PRE-PROCESSING","f1464f1e":"### FURTHER IDEAS : -\n\n1) ProductId and UserId can be used to track the general ratings of a given product and also to track the review patter of a particular user as if he is strict in reviwing or not.\n \n\n2) Helpfulness feature may tell about the product. This is because gretare the no of people talking about reviews, the mre stronger or critical it is expected to be.\n\n3) Summary column can also give a hint.\n\n4) One can also try the pre-trained embeddings like Glove word vectors etc...\n\n5) Lastly tuning the n\/w hyperparameters is always an option;).\n\n ","d06d5d40":"### BUILDING A MODEL AND FINALLY PERFORMING TEXT CLASSIFICATION","e4d8a699":"Having done all the pre-requisites we finally move onto make model in Keras .\n\n**Note that I have commented the LSTM layer as including it causes the trainig loss to be stucked at a value of about 0.6932. I don;t know why ;(.**\n\n**In case someone knows please comment below. **","6d3281a8":"### IMPORTING THE MODULES","97150114":"#### Note that pre processing all the reviews is taking way too much time and so I will take only 100K reviews. To balance the class  I have taken equal instances of each sentiment.","483272dd":"#### Note that there is no point for keeping rows with different scores or sentiment for same review text.  So I will keep only one instance and drop the rest of the duplicates.","8d23f0d8":"#### Now can try some things with word2vec embeddings. Thanks to Gensim ;)","f7461e88":"### PREPARING TRAIN AND VALIDATION SETS.","33f8288f":"In this section I have actually created the word embeddings in Gensim. Note that I planed touse the pre-trained word embeddings like the google word2vec trained on google news corpusor the famous Stanford Glove embeddings. But as soon as I load the corressponding embeddings through Gensim the runtime dies and kernel crashes ; perhaps because it contains 30L words and which is exceeding the RAM on Google Colab.\n\nBecause of this ; for now I have created the embeddings by training on my own corpus.","c14aba09":"#### Note that since we are doing sentiment analysis I will convert the values in score column to sentiment. Sentiment is 0 for ratings or scores less than 3 and 1 or  +  elsewhere."}}