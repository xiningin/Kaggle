{"cell_type":{"9d93e5fa":"code","26b58a37":"code","3d48a3d9":"code","ff3a9f17":"code","b194323c":"code","0d093391":"code","26a68bfe":"code","b65f7586":"code","46f797b7":"code","f4149816":"code","45d13098":"code","1fbb19d9":"markdown","f5fa44cc":"markdown","a032159d":"markdown","287006e2":"markdown","3b81eba6":"markdown","13f44840":"markdown","2daf428a":"markdown","cfeb380f":"markdown","7a107bd7":"markdown","c51875d7":"markdown"},"source":{"9d93e5fa":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","26b58a37":"import nltk\n\ntagged_sentence = nltk.corpus.treebank.tagged_sents(tagset='universal')\nprint(\"Number of Tagged Sentences \",len(tagged_sentence))","3d48a3d9":"# Let's have a look at some sentences\n\nprint(tagged_sentence[:3][:10])","ff3a9f17":"def word_to_ix(word, ix):\n    return torch.tensor(ix[word], dtype = torch.long)\n\ndef char_to_ix(char, ix):\n    return torch.tensor(ix[char], dtype= torch.long)\n\ndef tag_to_ix(tag, ix):\n    return torch.tensor(ix[tag], dtype= torch.long)\n\ndef sequence_to_idx(sequence, ix):\n    return torch.tensor([ix[s] for s in sequence], dtype=torch.long)\n\n\nword_to_idx = {}\ntag_to_idx = {}\nchar_to_idx = {}\nfor sentence in tagged_sentence:\n    for word, pos_tag in sentence:\n        if word not in word_to_idx.keys():\n            word_to_idx[word] = len(word_to_idx)\n        if pos_tag not in tag_to_idx.keys():\n            tag_to_idx[pos_tag] = len(tag_to_idx)\n        for char in word:\n            if char not in char_to_idx.keys():\n                char_to_idx[char] = len(char_to_idx)","b194323c":"word_vocab_size = len(word_to_idx)\ntag_vocab_size = len(tag_to_idx)\nchar_vocab_size = len(char_to_idx)\n\nprint(\"Unique words: {}\".format(len(word_to_idx)))\nprint(\"Unique tags: {}\".format(len(tag_to_idx)))\nprint(\"Unique characters: {}\".format(len(char_to_idx)))","0d093391":"import random\n\ntr_random = random.sample(list(range(len(tagged_sentence))), int(0.95 * len(tagged_sentence)))\n\ntrain = [tagged_sentence[i] for i in tr_random]\ntest = [tagged_sentence[i] for i in range(len(tagged_sentence)) if i not in tr_random]","26a68bfe":"WORD_EMBEDDING_DIM = 1024\nCHAR_EMBEDDING_DIM = 128\nWORD_HIDDEN_DIM = 1024\nCHAR_HIDDEN_DIM = 128\nDENSE_LAYER_DIM_1 = 256\nDENSE_LAYER_DIM_2 = 256","b65f7586":"class DualLSTMTagger(nn.Module):\n    def __init__(self, word_embedding_dim, word_hidden_dim, char_embedding_dim, char_hidden_dim, dense_layer_dim1, dense_layer_dim2, word_vocab_size, char_vocab_size, tag_vocab_size):\n        super(DualLSTMTagger, self).__init__()\n        self.word_embedding = nn.Embedding(word_vocab_size, word_embedding_dim)\n        \n        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n        self.char_lstm1 = nn.LSTM(char_embedding_dim, char_hidden_dim)\n        self.char_lstm2 = nn.LSTM(char_hidden_dim, char_hidden_dim)\n        \n        self.lstm1 = nn.LSTM(word_embedding_dim + char_hidden_dim * 2, word_hidden_dim)\n        self.lstm2 = nn.LSTM(word_hidden_dim + char_hidden_dim * 2, word_hidden_dim)\n\n        self.hidden2dense = nn.Linear(word_hidden_dim, dense_layer_dim1)\n        self.dense2dense = nn.Linear(dense_layer_dim1, dense_layer_dim2)\n        self.dense2out = nn.Linear(dense_layer_dim2, tag_vocab_size)\n        \n    def forward(self, sentence, words):\n        embeds = self.word_embedding(sentence)\n        \n        char_hidden_final1 = []\n        char_hidden_final2 = []\n        for word in words:\n            char_embeds = self.char_embedding(word)\n            \n            char_output1, (char_hidden1, char_cell_state1) = self.char_lstm1(char_embeds.view(len(word), 1, -1))\n            word_char_hidden_state1 = char_hidden1.view(-1)\n            char_hidden_final1.append(word_char_hidden_state1)\n            \n            char_output2, (char_hidden2, char_cell_state2) = self.char_lstm2(char_output1.view(len(word), 1, -1))\n            word_char_hidden_state2 = char_hidden2.view(-1)\n            char_hidden_final2.append(word_char_hidden_state2)\n            \n        char_hidden_final1 = torch.stack(tuple(char_hidden_final1))\n        char_hidden_final2 = torch.stack(tuple(char_hidden_final2))\n        char_hidden = torch.cat((char_hidden_final1, char_hidden_final2), 1)\n\n        combined = torch.cat((embeds, char_hidden), 1)\n        lstm_out1, _ = self.lstm1(combined.view(len(sentence), 1, -1))\n        \n        lstm_out1 = lstm_out1.view(len(sentence), -1)\n        \n        combined2 = torch.cat((lstm_out1, char_hidden), 1)\n        lstm_out2, _ = self.lstm2(combined.view(len(sentence), 1, -1))\n\n        tag_space = F.relu(self.hidden2dense(lstm_out2.view(len(sentence), -1)))\n        tag_space = F.relu(self.dense2dense(tag_space))\n        tag_space = F.relu(self.dense2out(tag_space))\n        tag_scores = F.log_softmax(tag_space, dim=1)\n        return tag_scores","46f797b7":"model = DualLSTMTagger(WORD_EMBEDDING_DIM, WORD_HIDDEN_DIM, CHAR_EMBEDDING_DIM, CHAR_HIDDEN_DIM, DENSE_LAYER_DIM_1, DENSE_LAYER_DIM_2, word_vocab_size, char_vocab_size, tag_vocab_size)\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n\nif use_cuda:\n    model.cuda()\n\n# Define the loss function as the Negative Log Likelihood loss (NLLLoss)\nloss_function = nn.NLLLoss()\n\n# We will be using a simple SGD optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# The test sentence\nseq = \"everybody read the book and rate the food\".split()\nprint(\"Running a check on the model before training.\\nSentences:\\n{}\".format(\" \".join(seq)))\nwith torch.no_grad():\n    words = [torch.tensor(sequence_to_idx(s[0], char_to_idx), dtype=torch.long).to(device) for s in seq]\n    sentence = torch.tensor(sequence_to_idx(seq, word_to_idx), dtype=torch.long).to(device)\n        \n    tag_scores = model(sentence, words)\n    _, indices = torch.max(tag_scores, 1)\n    ret = []\n    for i in range(len(indices)):\n        for key, value in tag_to_idx.items():\n            if indices[i] == value:\n                ret.append((seq[i], key))\n    print(ret)\n# Training start\nprint(\"Training Started\")\naccuracy_list = []\nloss_list = []\ninterval = round(len(train) \/ 100.)\nepochs = 100\nfor epoch in range(epochs):\n    acc = 0 #to keep track of accuracy\n    loss = 0 # To keep track of the loss value\n    i = 0\n    for sentence_tag in train:\n        i += 1\n        words = [torch.tensor(sequence_to_idx(s[0], char_to_idx), dtype=torch.long).to(device) for s in sentence_tag]\n        sentence = [s[0] for s in sentence_tag]\n        sentence = torch.tensor(sequence_to_idx(sentence, word_to_idx), dtype=torch.long).to(device)\n        targets = [s[1] for s in sentence_tag]\n        targets = torch.tensor(sequence_to_idx(targets, tag_to_idx), dtype=torch.long).to(device)\n        \n        model.zero_grad()\n        \n        tag_scores = model(sentence, words)\n        \n        loss = loss_function(tag_scores, targets)\n        loss.backward()\n        optimizer.step()\n        loss += loss.item()\n        _, indices = torch.max(tag_scores, 1)\n#         print(indices == targets)\n        acc += torch.mean(torch.tensor(targets == indices, dtype=torch.float))\n        if i % interval == 0:\n            print(\"Epoch {}\/{} Running;\\t{}% Complete\".format(epoch + 1, epochs, i \/ interval), end = \"\\r\", flush = True)\n    loss = loss \/ len(train)\n    acc = acc \/ len(train)\n    loss_list.append(loss)\n    accuracy_list.append(acc)\n    if (epoch + 1) % 10 == 0:\n        print(\"Epoch {}\/{} Completed,\\tLoss {}\\tAccuracy: {}\".format(epoch + 1, epochs, loss, acc))","f4149816":"import matplotlib.pyplot as plt\nplt.plot(accuracy_list, c=\"red\", label =\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.show()\n\nplt.plot(loss_list, c=\"blue\", label =\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.show()","45d13098":"with torch.no_grad():\n    words = [torch.tensor(sequence_to_idx(s[0], char_to_idx), dtype=torch.long).to(device) for s in seq]\n    sentence = torch.tensor(sequence_to_idx(seq, word_to_idx), dtype=torch.long).to(device)\n        \n    tag_scores = model(sentence, words)\n    _, indices = torch.max(tag_scores, 1)\n    ret = []\n    for i in range(len(indices)):\n        for key, value in tag_to_idx.items():\n            if indices[i] == value:\n                ret.append((seq[i], key))\n    print(ret)","1fbb19d9":"### Downloading Data\nFirst we will download data from nltk, a very popular python natural language processing toolkit, incase you haven't heard of it.\n\nKindly refer to their site for more information: https:\/\/www.nltk.org\/","f5fa44cc":"## Lets get started","a032159d":"## Train\/Test Splitting the data","287006e2":"### Result:\nSee for yourself","3b81eba6":"### Single Test after Training","13f44840":"### Defining the model parameters here","2daf428a":"Now we will construct a dictionary for all these:\n1. A word\/tag dictionary,\n2. A letter\/character dictionary\n3. A POS tag dictionary","cfeb380f":"## The model classes\nHere I will construct the model classes, the Word LSTM tagger and the Character LSTM Tagger.\nThe LSTM in both cases takes in a sequence (words or characters), embeds the sequence into an embedding space (dimension of the space is a hyperparameter), and runs the LSTM model.\n\nIn our case, first the word level LSTM will take in a sequence of words and convert them into the word embedding space.\nSimilarly, it will take the words sequentially  and run the character level LSTM model, which will first take in the sequence of characters in each word and project it in the character embedding space and then run the LSTM model and take its hidden state and feed it back to the word LSTM model.\n\nUsing the character level hidden representation for every word as well as the word embedding, the word level model then runs LSTM on the sequence of words, and outputs the predictions for every tag. This prediction is later processed to find the corresponding tags.","7a107bd7":"# Introduction\nIn the last notebook we looked at a relatively simple LSTM model for POS tagging. \nPlease have a look at it.\n\nhttps:\/\/www.kaggle.com\/krishanudb\/lstm-character-word-pos-tag-model-pytorch\n\nThe 2nd model in the above notebook was itself comprised of 2 LSTMs, one for character level and another for word level POS tagging. The model did not do that well on a singe test example. So, in this lecture, we are going to explore deep LSTM models for POS Tagging and see if they improve the model performance.","c51875d7":"### Importing all the necesarry packages"}}