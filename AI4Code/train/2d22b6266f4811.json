{"cell_type":{"d6449aee":"code","ec3136e7":"code","dae6dfc4":"code","91601199":"code","3ec387c5":"code","71e1e697":"code","9f8d8177":"code","625e3eb6":"code","b3679749":"code","2dc19a41":"code","9fe03d58":"code","362535b1":"code","a9e60dc4":"code","03737f0e":"code","87b6f1ae":"code","e3a88f3d":"code","5b168c56":"code","3ae71b9d":"code","56413d79":"code","4d37a8af":"code","a9a39443":"code","1d237ccb":"code","f3f5dfd8":"code","b6c1cf37":"code","457999fd":"code","d66efc82":"code","ed413db7":"code","51f72012":"code","257f8313":"code","c13f14f1":"code","e7987653":"code","4d6bd24b":"code","8480c0e4":"code","ed75795c":"code","f8cc6d54":"code","c24f68f3":"code","7b9b20e9":"code","63a47c4e":"code","c81f6480":"code","d7a90eba":"code","5b8f3f48":"code","5b59560a":"code","0a2d507b":"code","d5befbaf":"code","a230dbfb":"code","a8dbdedf":"code","6603db20":"code","439ec006":"code","31721764":"code","5726598a":"code","0ce707ea":"markdown","502d2b2c":"markdown","c2e75e1e":"markdown","a186eed5":"markdown","f062370d":"markdown","dc46a8ed":"markdown","b34fb285":"markdown"},"source":{"d6449aee":"# Import libraries\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn import svm\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.metrics import roc_curve, auc\nimport tensorflow as tf\nfrom keras.utils import to_categorical","ec3136e7":"# Read data\ntrain_raw = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_raw = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","dae6dfc4":"# Check data shape\nprint('Train shape: ', train_raw.shape)\nprint('Test shape: ', test_raw.shape)","91601199":"# Explore data\ntrain_raw.info()","3ec387c5":"test_raw.info()","71e1e697":"# View training data\ntrain_raw.head()","9f8d8177":"# Separate numeric and string(categorical, ordinal) features from test and train data\ndef separate_cols(df):\n    '''\n    Returns the numeric and string feature names present in a dataframe\n    \n    Parameters:\n        df(Dataframe): The Dataframe from which the feature names are to be separated\n    \n    Returns:\n        List: A list which contains two lists: numeric feature name list & string feature name list\n    '''\n    numeric_cols = []\n    string_cols = []\n    for col in df.columns:\n        if(df[col].dtype in ['int','float']):\n            numeric_cols.append(col)\n        elif(df[col].dtype in ['object']):\n            string_cols.append(col)\n    return [numeric_cols,string_cols]\n\nnumeric_cols_train, string_cols_train = separate_cols(train_raw)\nnumeric_cols_test, string_cols_test = separate_cols(test_raw)","625e3eb6":"# Check the number of missing values for each feature of train and test data\ndef print_missing_values(df):\n    '''\n    Prints the feature name along with the number of missing values and missing value percentage in the dataframe\n    \n    Parameters:\n        df(Dataframe): The Dataframe from which the missing values are to be printed\n    '''\n    for col in df.columns:\n        if(df[col].isnull().sum() > 0):\n            print(col, df[col].isnull().sum(), \"%.2f%%\" %((df[col].isnull().sum()\/df.shape[0])*100))\n\nprint(\"Missing values in train data\")\nprint_missing_values(train_raw)\nprint(\"\\nMissing values in test data\")\nprint_missing_values(test_raw)","b3679749":"# Copy raw data for further processing\ntrain = train_raw.copy()\ntest = test_raw.copy()","2dc19a41":"# Drop Name and PassengerID from train and test data as they have no contribution in making predictions\ntrain = train.drop([\"PassengerId\",\"Name\"], axis=1)\ntest = test.drop([\"PassengerId\",\"Name\"], axis=1)","9fe03d58":"# We can observe from the missing value percentage that cabin has \n# more than 70% missing data in both train and test set. \n# We can remove Cabin feature from both train and test set\ntrain = train.drop(['Cabin'], axis=1)\ntest = test.drop(['Cabin'], axis=1)","362535b1":"# Ticket feature is not much useful in prediction\ntrain = train.drop(['Ticket'], axis=1)\ntest = test.drop(['Ticket'], axis=1)","a9e60dc4":"# We can fill the missing age values based on the Pclass feature using the mean of age with a Pclass group\n# age_mean = train[['Age','Pclass']].groupby(['Pclass'], as_index=False).mean()['Age']\n# train.loc[train['Pclass'] == 1, 'Age'].fillna(int(age_mean[0]))","03737f0e":"# Replace all missing values with mean or mode in train data\ntrain['Age'] = train['Age'].fillna(train['Age'].mean())\ntrain['Embarked'] = train['Embarked'].fillna(train['Embarked'].mode()[0])","87b6f1ae":"# Replace all missing values with mean or mode in test data\ntest['Age'] = test['Age'].fillna(test['Age'].mean())\ntest['Fare'] = test['Fare'].fillna(test['Fare'].mean())","e3a88f3d":"# Intermediate copy\ntrain1 = train.copy()\ntest1 = test.copy()","5b168c56":"# Update columns\nnumeric_cols_train, string_cols_train = separate_cols(train1)\nnumeric_cols_test, string_cols_test = separate_cols(test1)","3ae71b9d":"# Add parents and siblings to create a new feature\ntrain1['Family_Size'] = train1['Parch'] + train1['SibSp']\ntest1['Family_Size'] = test1['Parch'] + test1['SibSp']","56413d79":"# Drop Parch and SibSp because we have a new feature named Family_size\ntrain1 = train1.drop(columns=['SibSp', 'Parch'], axis=1)\ntest1 = test1.drop(columns=['SibSp', 'Parch'], axis=1)","4d37a8af":"# Test 1 - Does not perform well\n# Use dummies to onehotencode categorical features\n# Uncomment when required\n\n# dum_train1 = pd.get_dummies(train1[string_cols_train], columns=string_cols_train)\n# dum_test1 = pd.get_dummies(train1[string_cols_test], columns=string_cols_test)\n# train1 = train1.join(dum_train1)\n# test1 = test1.join(dum_test1)","a9a39443":"# Drop Embarked and Sex from train and test data\n# train1 = train1.drop(columns=['Sex', 'Embarked'], axis=1)\n# test1 = test1.drop(columns=['Sex', 'Embarked'], axis=1)","1d237ccb":"# Test 2\n# Use labelencoder for categorical features\nle = LabelEncoder()\nfor col in string_cols_train:\n    train1[col] = le.fit_transform(train1[col])\n    test1[col] = le.transform(test1[col])","f3f5dfd8":"# Scale Age and Fare for train and test data using MinMaxScaler\nscaler = MinMaxScaler()\ntrain_scaled = pd.DataFrame(scaler.fit_transform(train1[['Age', 'Fare']]),columns=['Age', 'Fare'])\ntest_scaled = pd.DataFrame(scaler.transform(test1[['Age', 'Fare']]),columns=['Age', 'Fare'])","b6c1cf37":"# Copy the scaled features to processed dataset\ntrain1[['Age', 'Fare']] = train_scaled[['Age', 'Fare']]\ntest1[['Age', 'Fare']] = test_scaled[['Age', 'Fare']]","457999fd":"# Prepare final training and test data for modeling\nX_train = train1.drop(['Survived'], axis=1).copy()\ny_train = train1['Survived']\nX_test = test1.copy()\n# Using the sample submissions as y_test just to get an idea of how models perform\n# A better approach might be to use train test split on train data\ny_test = submission['Survived']","d66efc82":"X_test.head()","ed413db7":"# Check the shape of final data\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","51f72012":"def nn_model(optimizer='adam'):\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(6,)))\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(tf.keras.layers.Dense(16, activation='relu'))\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\nnn_estimator = KerasClassifier(build_fn=nn_model)","257f8313":"nn_model().summary()","c13f14f1":"# Create hyperparameter space\nepochs = [100, 200, 300]\nbatches = [16, 32, 64]\noptimizers = ['rmsprop', 'adam', 'Nadam']\n\n# Create hyperparameter options\nhyperparameters = dict(optimizer=optimizers, epochs=epochs, batch_size=batches)\n\ngscv = GridSearchCV(estimator=nn_estimator, param_grid=hyperparameters, n_jobs=-1,cv=3, verbose=1)","e7987653":"grid_result = gscv.fit(X_train, y_train)","4d6bd24b":"# View hyperparameters of best neural network\nprint(grid_result.best_params_)\nprint(grid_result.best_score_)","8480c0e4":"# Check the train and test score using gridsearchcv score\nnn_train_score = gscv.score(X_train, y_train) \nnn_test_score = gscv.score(X_test, y_test)\nprint(\"Train accuracy: \", nn_train_score)\nprint(\"Test accuracy: \", nn_test_score)","ed75795c":"# Apply best_params on nn model and get history object\nmodel = nn_model(grid_result.best_params_['optimizer'])\nhistory = model.fit(X_train, y_train, \n                    epochs=grid_result.best_params_['epochs'],\n                    batch_size=grid_result.best_params_['batch_size'],\n                    verbose=1)","f8cc6d54":"# Check the score of train and test using sequential model evaluate\nnn_train_score = model.evaluate(X_train, y_train)[1]\nnn_test_score = model.evaluate(X_test, y_test)[1]\nprint(\"Train accuracy: \", nn_train_score)\nprint(\"Test accuracy: \", nn_test_score)","c24f68f3":"# Make predictions using neural network model\nnn_train_prediction = model.predict(X_train)\nnn_test_prediction = model.predict(X_test)","7b9b20e9":"# Convert probability prediction to binary\nfor i in range(0, len(nn_train_prediction)):\n    if(nn_train_prediction[i] > 0.5):\n        nn_train_prediction[i] = 1\n    else:\n        nn_train_prediction[i] = 0\nfor i in range(0, len(nn_test_prediction)):\n    if(nn_test_prediction[i] > 0.5):\n        nn_test_prediction[i] = 1\n    else:\n        nn_test_prediction[i] = 0","63a47c4e":"# Roc is a good measure for classification problems\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, nn_test_prediction)\nnn_roc_auc = auc(false_positive_rate, true_positive_rate)","c81f6480":"# Plot confusion matrix for neural network predictions\ncm_train = confusion_matrix(y_train, nn_train_prediction)\ncr_train = classification_report(y_train, nn_train_prediction)\nprint(\"Training Data:\")\nprint(cm_train)\nprint(cr_train)\ncm_test = confusion_matrix(y_test, nn_test_prediction)\ncr_test = classification_report(y_test, nn_test_prediction)\nprint(\"Testing Data:\")\nprint(cm_test)\nprint(cr_test)","d7a90eba":"# plot history\npyplot.plot(history.history['accuracy'], label='train')\npyplot.xlabel('Epochs')\npyplot.ylabel('Accuracy')\npyplot.legend()\npyplot.show()\npyplot.plot(history.history['loss'], label='train')\npyplot.xlabel('Epochs')\npyplot.ylabel('Loss')\npyplot.legend()\npyplot.show()","5b8f3f48":"lr = LogisticRegression(C=5)\nlr.fit(X_train, y_train)\n\nlr_train_prediction = lr.predict(X_train)\nlr_test_prediction = lr.predict(X_test)\nlr_train_score = lr.score(X_train, y_train)\nlr_test_score = lr.score(X_test, y_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, lr_test_prediction)\nlr_roc_auc = auc(false_positive_rate, true_positive_rate)","5b59560a":"rc = RidgeClassifier(alpha=100)\nrc.fit(X_train, y_train)\n\nrc_train_prediction = rc.predict(X_train)\nrc_test_prediction = rc.predict(X_test)\nrc_train_score = rc.score(X_train, y_train)\nrc_test_score = rc.score(X_test, y_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, rc_test_prediction)\nrc_roc_auc = auc(false_positive_rate, true_positive_rate)","0a2d507b":"rfc = RandomForestClassifier(n_estimators=1000, max_depth=8, n_jobs=-1)\nrfc.fit(X_train, y_train)\n\nrfc_train_prediction = rfc.predict(X_train)\nrfc_test_prediction = rfc.predict(X_test)\nrfc_train_score = rfc.score(X_train, y_train)\nrfc_test_score = rfc.score(X_test, y_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, rfc_test_prediction)\nrfc_roc_auc = auc(false_positive_rate, true_positive_rate)","d5befbaf":"svc = svm.SVC(C=25, gamma=25)\nsvc.fit(X_train, y_train)\n\nsvc_train_prediction = svc.predict(X_train)\nsvc_test_prediction = svc.predict(X_test)\nsvc_train_score = svc.score(X_train, y_train)\nsvc_test_score = svc.score(X_test, y_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, svc_test_prediction)\nsvc_roc_auc = auc(false_positive_rate, true_positive_rate)","a230dbfb":"gbc = GradientBoostingClassifier(n_estimators=125)\ngbc.fit(X_train, y_train)\n\ngbc_train_prediction = gbc.predict(X_train)\ngbc_test_prediction = gbc.predict(X_test)\ngbc_train_score = gbc.score(X_train, y_train)\ngbc_test_score = gbc.score(X_test, y_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, gbc_test_prediction)\ngbc_roc_auc = auc(false_positive_rate, true_positive_rate)","a8dbdedf":"# Voting Classifier with hard voting \nestimator = [('lr',lr),('rc',rc),('rfr',rfc),('svc',svc),('gbc',gbc)]\nvhc = VotingClassifier(estimators=estimator, voting='hard') \nvhc.fit(X_train, y_train)\n\nvhc_train_prediction = vhc.predict(X_train)\nvhc_test_prediction = vhc.predict(X_test)\nvhc_train_score = vhc.score(X_train, y_train)\nvhc_test_score = vhc.score(X_test, y_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, vhc_test_prediction)\nvhc_roc_auc = auc(false_positive_rate, true_positive_rate)","6603db20":"# Voting Classifier with soft voting \nestimator = [('lr',lr),('rfr',rfc),('gbc',gbc)]\nvsc = VotingClassifier(estimators=estimator, voting='soft') \nvsc.fit(X_train, y_train)\n\nvsc_train_prediction = vsc.predict(X_train)\nvsc_test_prediction = vsc.predict(X_test)\nvsc_train_score = vsc.score(X_train, y_train)\nvsc_test_score = vsc.score(X_test, y_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, vsc_test_prediction)\nvsc_roc_auc = auc(false_positive_rate, true_positive_rate)","439ec006":"# Compare classifiers\ncolumns = ['Classifier', 'Train Score', 'Test Score', 'AUC Score']\npd.DataFrame([\n    ['Neural Network',nn_train_score, nn_test_score, nn_roc_auc],\n    ['Logistic',lr_train_score, lr_test_score, lr_roc_auc],    \n    ['Ridge',rc_train_score, rc_test_score, rc_roc_auc],    \n    ['Random Forest',rfc_train_score, rfc_test_score, rfc_roc_auc],\n    ['SVM',svc_train_score, svc_test_score, svc_roc_auc],\n    ['Gradient Boosting Classifier',gbc_train_score, gbc_test_score, gbc_roc_auc],\n    ['Hard Voting Classifier',vhc_train_score, vhc_test_score, vhc_roc_auc],\n    ['Soft Voting Classifier',vsc_train_score, vsc_test_score, vsc_roc_auc],\n], columns=columns\n)","31721764":"# Voting Hard classifier and Gradient Boosting performed better\nsubmission['Survived'] = vhc_test_prediction.astype(\"int64\")\nsubmission","5726598a":"submission.to_csv(\"..\/working\/TitanicSubmission.csv\", index=False)","0ce707ea":"## Train Neural Network","502d2b2c":"# Titanic: Machine Learning from Disaster\nThis notebook contains the comparison of following models:\n* Neural Network with Hyperparameter Tuning using GridSearchCV\n* Logistic Regression\n* Ridge Classifier\n* Random Forest Classifier\n* Support Vector Classifier\n* Gradient Boosted Classifier\n* Voting Classifier with Hard and Soft Voting","c2e75e1e":"## Comparison of Different Model","a186eed5":"## Train Other Models\n**Prepare other models and compare them**","f062370d":"## Conclusion\n**The comparison shows that Voting classifier with hard voting and grafient boosting classifier performed better than the rest of the classifiers**","dc46a8ed":"## Preprocessing ","b34fb285":"## Submission"}}