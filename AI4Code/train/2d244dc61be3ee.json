{"cell_type":{"1a6cc914":"code","3ddf69c7":"code","87b0b5a2":"code","69229c4d":"code","c87462a7":"code","f29cf277":"code","3b45ab4c":"code","5fe396a4":"code","90717bb2":"code","8868dbd7":"code","a0412d0b":"code","398ba9fa":"code","a1cf0527":"code","59352b6e":"code","03a24a0b":"code","7810c715":"code","d90a64e2":"code","bd890b93":"code","daed9b46":"code","3849018b":"code","219426ed":"code","bf59ab83":"code","26bb21f2":"code","dd120462":"code","55bcc6d0":"code","295bac01":"code","1044a5ec":"code","08b96fe8":"code","5a5a0d12":"code","7d470501":"code","7e4ead27":"code","1107d3d0":"code","e4c9caf6":"code","ce4c706b":"code","63411b93":"code","94a3f621":"code","2a480779":"code","680849e7":"code","e5641f88":"code","09eec958":"code","1ecc3a5d":"code","96374504":"code","344cd4e5":"code","fea6a64f":"code","cb9fb23d":"code","ecd2cc33":"code","a661102d":"code","0bfddb5b":"markdown","36ce3fb1":"markdown","f417b893":"markdown","e1bead3b":"markdown","3d456490":"markdown","2d9fb2c8":"markdown","306ed5b2":"markdown","0f05bfd7":"markdown","9bac1e6d":"markdown","aab3a5ce":"markdown","699620dc":"markdown","0d182857":"markdown","5c35e462":"markdown","3356454b":"markdown","8c280b61":"markdown","389c6c1f":"markdown","836c3207":"markdown","c451607f":"markdown","a95b9e6e":"markdown","8e86dcb8":"markdown","1232a3ad":"markdown","99bff6b7":"markdown","8adaaed4":"markdown","33587b5d":"markdown","d1cc9b3a":"markdown","efda0a7b":"markdown","7cffd15f":"markdown","f9883b95":"markdown","4e23711d":"markdown","48aa6e8f":"markdown","95e55e25":"markdown","b5d854d7":"markdown","65b4b9cc":"markdown","cc42bf1d":"markdown","4a35bd4b":"markdown","9b39fe46":"markdown","15f0ffb9":"markdown","d86448ed":"markdown","48c470eb":"markdown","495875e5":"markdown","bb916bc7":"markdown","c10ab636":"markdown","54415dd7":"markdown","3a3db149":"markdown","006c2bea":"markdown","616887d3":"markdown","1ac18e74":"markdown","ae767b79":"markdown","83d4f646":"markdown","a92b3c93":"markdown","4bddada9":"markdown","89d32901":"markdown","1aab03e0":"markdown","290a7946":"markdown","3d5ccf8c":"markdown","d018293e":"markdown","c96138a7":"markdown"},"source":{"1a6cc914":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nimport scipy\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom tqdm import tqdm_notebook","3ddf69c7":"raw_train = pd.read_csv('..\/input\/cat-in-the-dat-ii\/train.csv', index_col='id')\nraw_test = pd.read_csv('..\/input\/cat-in-the-dat-ii\/test.csv', index_col='id')\n\nprint(raw_train.shape, raw_test.shape)","87b0b5a2":"raw_test.head()","69229c4d":"raw_train.head()","c87462a7":"def plot_missing_values(df):\n\n    cols = df.columns\n    count = [df[col].isnull().sum() for col in cols]\n    percent = [i\/len(df) for i in count]\n    missing = pd.DataFrame({'number':count, 'proportion': percent}, index=cols)\n    \n    fig, ax = plt.subplots(1,2, figsize=(20,7))\n    for i, col in enumerate(missing.columns):\n\n        plt.subplot(1,2,i+1)\n        plt.title(f'Missing values on each columns({col})')\n        sns.barplot(missing[col], missing.index)\n        mean = np.mean(missing[col])\n        std = np.std(missing[col])\n        plt.ylabel('Columns')\n        plt.plot([], [], ' ', label=f'Average {col} of missing values: {mean:.2f} \\u00B1 {std:.2f}')\n        plt.legend()\n    plt.show()\n    return missing.sort_values(by='number', ascending=False)","f29cf277":"missing_train = plot_missing_values(raw_train)\nmissing_train.head()","3b45ab4c":"missing_test = plot_missing_values(raw_test)\nmissing_test.head()","5fe396a4":"plt.figure(figsize=(6,6))\nax = sns.countplot(raw_train.target)\n\nheight = sum([p.get_height() for p in ax.patches])\nfor p in ax.patches:\n        ax.annotate(f'{100*p.get_height()\/height:.2f} %', (p.get_x()+0.3, p.get_height()+5000),animated=True)","90717bb2":"plt.figure(figsize=(10,7))\nnum_cols = raw_train.select_dtypes(exclude=['object']).columns\ncorr = raw_train[num_cols].corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","8868dbd7":"plt.figure(figsize=(10,7))\n\ncorr_0 = raw_train[num_cols][raw_train.target==0].corr()\nsns.heatmap(corr_0, \n            xticklabels=corr_0.columns.values,\n            yticklabels=corr_0.columns.values)","a0412d0b":"plt.figure(figsize=(10,7))\n\ncorr_1 = raw_train[num_cols][raw_train.target==1].corr()\nsns.heatmap(corr_1, \n            xticklabels=corr_1.columns.values,\n            yticklabels=corr_1.columns.values)","398ba9fa":"num_cols = raw_test.select_dtypes(exclude=['object']).columns\nfig, ax = plt.subplots(2,3,figsize=(22,7))\nfor i, col in enumerate(num_cols):\n    plt.subplot(2,3,i+1)\n    plt.xlabel(col, fontsize=9)\n    sns.kdeplot(raw_train[col].values, bw=0.5,label='Train')\n    sns.kdeplot(raw_test[col].values, bw=0.5,label='Test')\n   \nplt.show() ","a1cf0527":"target0 = raw_train.loc[raw_train['target'] == 0]\ntarget1 = raw_train.loc[raw_train['target'] == 1]\n\nfig, ax = plt.subplots(2,3,figsize=(22,7))\nfor i, col in enumerate(num_cols):\n    plt.subplot(2,3,i+1)\n    plt.xlabel(col, fontsize=9)\n    sns.kdeplot(target0[col].values, bw=0.5,label='Target: 0')\n    sns.kdeplot(target1[col].values, bw=0.5,label='Target: 1')\n    sns.kdeplot(raw_test[col].values, bw=0.5,label='Test')\n    \nplt.show() ","59352b6e":"bin_cols = [f'bin_{i}' for i in range(5)]\n\nfig, ax = plt.subplots(1,5, figsize=(22, 5))\n\nfor i, col in enumerate(bin_cols):\n     ax0 = plt.subplot(1,5,i+1)\n     raw_train[col].value_counts().plot.bar(color='pink')\n     height = sum([p.get_height() for p in ax0.patches])\n\n     for p in ax0.patches:\n         ax0.text(p.get_x()+p.get_width()\/2., p.get_height()+4000, f'{100*p.get_height()\/height:.2f} %', ha='center')\n     plt.xlabel(f'{col}')\nplt.suptitle('Distribution over binary feature of train data')\n","03a24a0b":"fig, ax = plt.subplots(1,5, figsize=(22, 5))\n\nfor i, col in enumerate(bin_cols):\n     ax0 = plt.subplot(1,5,i+1)\n     raw_test[col].value_counts().plot.bar(color='lime')\n     height = sum([p.get_height() for p in ax0.patches])\n\n     for p in ax0.patches:\n         ax0.text(p.get_x()+p.get_width()\/2., p.get_height()+4000, f'{100*p.get_height()\/height:.2f} %', ha='center')\n     plt.xlabel(f'{col}')\nplt.suptitle('Distribution over binary feature of test data')","7810c715":"plt.figure(figsize=(22,6))\nplt.title('Day distribution')\nax = sns.countplot(raw_train.day, hue=raw_train.target)\nfor p in ax.patches:\n    ax.text(p.get_x()+p.get_width()\/2., p.get_height()+1000, f'{100*p.get_height()\/height:.2f} %',ha='center')\nplt.show()","d90a64e2":"plt.figure(figsize=(22,6))\nplt.title('Month distribution')\nax = sns.countplot(raw_train.month, hue=raw_train.target)\nfor p in ax.patches:\n    ax.text(p.get_x()+p.get_width()\/2., p.get_height()+1000, f'{100*p.get_height()\/height:.2f} %', ha='center')\nplt.show()","bd890b93":"df_train = raw_train.dropna(subset=['month', 'day'])[['day', 'month', 'target']]\ndf_test = raw_test.dropna(subset=['month', 'day'])[['day', 'month']]\ndf0 = df_train[df_train.target == 0]\ndf1 = df_train[df_train.target == 1]\n\ndef number2datetime(df):\n    time_col = '2019\/' + df.month.astype(int).astype(str) + '\/' + df.day.astype(int).astype(str)\n    df['time'] = pd.to_datetime(time_col , format = '%Y\/%m\/%d')\n    df = df.drop(columns=['day', 'month'])\n    return df\ndf0 = number2datetime(df0)\ndf1 = number2datetime(df1)\ndf_test = number2datetime(df_test)","daed9b46":"count0 = df0.time.value_counts()\/len(df0)\ncount0 = count0.sort_index()\ncount1 = df1.time.value_counts()\/len(df1)\ncount1 = count1.sort_index()\ncount_test = df_test.time.value_counts(normalize=True)","3849018b":"plt.figure(figsize=(20,8))\nsns.lineplot(count0.index, count0.values, label='Target:0')\nsns.lineplot(count1.index, count1.values, label='Target:1')\nsns.lineplot(count_test.index, count_test.values, label='Test')\nplt.legend(loc='upper left')","219426ed":"nom_cols = [f'nom_{i}' for i in range(5)]\nfig, ax = plt.subplots(1,5, figsize=(22, 6))\nfor i, col in enumerate(nom_cols):\n    plt.subplot(1,5,i+1)\n    sns.countplot(f'nom_{i}', hue='target', data= raw_train)\n\nplt.show()","bf59ab83":"plt.figure(figsize=(17, 35)) \nfig, ax = plt.subplots(2,3,figsize=(22,10))\n\nfor i, col in enumerate(raw_train[nom_cols]): \n    tmp = pd.crosstab(raw_train[col], raw_train['target'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\n\n    ax = plt.subplot(2,3,i+1)\n    sns.countplot(x=col, data=raw_train, order=list(tmp[col].values) , palette='Set2') \n    ax.set_ylabel('Count', fontsize=15) # y axis label\n    ax.set_title(f'{col} Distribution by Target', fontsize=18) # title label\n    ax.set_xlabel(f'{col} values', fontsize=15) # x axis label\n\n    # twinX - to build a second yaxis\n    gt = ax.twinx()\n    gt = sns.pointplot(x=col, y='Yes', data=tmp,\n                           order=list(tmp[col].values),\n                           color='black', legend=False)\n    gt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1)\n    gt.set_ylabel(\"Target %True(1)\", fontsize=16)\n    sizes=[] # Get highest values in y\n    total = sum([p.get_height() for p in ax.patches])\n    for p in ax.patches: # loop to all objects\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()\/2.,\n                    height + 2000,\n                    '{:1.2f}%'.format(height\/total*100),\n                    ha=\"center\") \n    ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n\n\nplt.subplots_adjust(hspace = 0.5, wspace=.3)\nplt.show()","26bb21f2":"for col in nom_cols:\n    fig, ax = plt.subplots(1,3,figsize=(22,6))\n    ax[0].set_title(f'Target 0 data {col}')\n    ax[1].set_title(f'Target 1 data {col}')\n    ax[2].set_title(f'Test data {col}')\n\n    explode = np.zeros(raw_train[col].nunique()+1)\n    explode[1] = 0.05   \n    target0_count = target0[col].value_counts(dropna=False)\n    target1_count = target1[col].value_counts(dropna=False)    \n    test_count = raw_test[col].value_counts(dropna=False)\n\n    ax[0].pie(target0_count, labels=target0_count.index, autopct='%1.1f%%', explode=explode, shadow=True, startangle=90)\n    ax[0].legend(labels=target0_count.index,loc=3)\n    ax[1].pie(target1_count, labels=target1_count.index, autopct='%1.1f%%', explode=explode, shadow=True, startangle=90)\n    ax[1].legend(labels=target1_count.index,loc=3)    \n    ax[2].pie(test_count, labels=test_count.index, autopct='%1.1f%%', explode=explode, shadow=False, startangle=90)\n    ax[2].legend(labels=test_count.index,loc=3)","dd120462":"nom_cols = [f'nom_{i}' for i in range(5,10)]\nfig, ax = plt.subplots(5,1, figsize=(22,17))\nfor i,col in enumerate(nom_cols):\n    plt.subplot(5,1,i+1)\n    sns.countplot(raw_train[col])\nplt.show()","55bcc6d0":"ord_cols = [f'ord_{i}' for i in range(3)]\nplt.figure(figsize=(17, 35)) \nfig, ax = plt.subplots(3,1,figsize=(15,15))\n\nfor i, col in enumerate(raw_train[ord_cols]): \n    tmp = pd.crosstab(raw_train[col], raw_train['target'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\n\n    ax = plt.subplot(3,1,i+1)\n    sns.countplot(x=col, data=raw_train, order=list(tmp[col].values) , palette='Set2') \n    ax.set_ylabel('Count', fontsize=15) # y axis label\n    ax.set_title(f'{col} Distribution by Target', fontsize=18) # title label\n    ax.set_xlabel(f'{col} values', fontsize=15) # x axis label\n\n    gt = ax.twinx()\n    gt = sns.pointplot(x=col, y='Yes', data=tmp,\n                           order=list(tmp[col].values),\n                           color='black', legend=False)\n    gt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1)\n    gt.set_ylabel(\"Target %True(1)\", fontsize=16)\n    sizes=[] # Get highest values in y\n    total = sum([p.get_height() for p in ax.patches])\n    for p in ax.patches: # loop to all objects\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()\/2.,\n                    height + 2000,\n                    '{:1.2f}%'.format(height\/total*100),\n                    ha=\"center\") \n    ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n\nplt.subplots_adjust(hspace = 0.5, wspace=.3)\nplt.show()","295bac01":"ord_cols = ['ord_3', 'ord_4']\nplt.figure(figsize=(17, 35)) \nfig, ax = plt.subplots(2,1,figsize=(22,10))\n\nfor i, col in enumerate(raw_train[ord_cols]): \n    tmp = pd.crosstab(raw_train[col], raw_train['target'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\n\n    ax = plt.subplot(2,1,i+1)\n    sns.countplot(x=col, data=raw_train, order=list(tmp[col].values) , palette='Set2') \n    ax.set_ylabel('Count', fontsize=15) # y axis label\n    ax.set_title(f'{col} Distribution by Target', fontsize=18) # title label\n    ax.set_xlabel(f'{col} values', fontsize=15) # x axis label\n\n    # twinX - to build a second yaxis\n    gt = ax.twinx()\n    gt = sns.pointplot(x=col, y='Yes', data=tmp,\n                           order=list(tmp[col].values),\n                           color='black', legend=False)\n    gt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1)\n    gt.set_ylabel(\"Target %True(1)\", fontsize=16)\n    sizes=[] # Get highest values in y\n    total = sum([p.get_height() for p in ax.patches])\n    for p in ax.patches: # loop to all objects\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()\/2.,\n                    height + 2000,\n                    '{:1.2f}%'.format(height\/total*100),\n                    ha=\"center\") \n    ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n\n\nplt.subplots_adjust(hspace = 0.5, wspace=.3)\nplt.show()","1044a5ec":"tmp = pd.crosstab(raw_train['ord_5'], raw_train['target'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\nplt.figure(figsize=(18,7))\n\nplt.subplot()\nax = sns.countplot(x='ord_5', data=raw_train, order=list(tmp['ord_5'].values) , color='chocolate') \nax.set_ylabel('Count', fontsize=17) # y axis label\nax.set_title('ord_5 Distribution', fontsize=20) # title label\nax.set_xlabel('ord_5 values', fontsize=17) # x axis label","08b96fe8":"full_data = pd.concat([raw_train, raw_test], sort=False).drop(columns='target')\nfull_data.shape","5a5a0d12":"# Replace values which doesnt appear in both train and test set with another special value ('xor')\ncate_columns = full_data.select_dtypes(include=['object']).columns\nfor col in cate_columns:\n    train_values = set(raw_train[col].unique())\n    test_values = set(raw_test[col].unique())\n\n    xor_values = test_values - train_values \n    if xor_values:\n        print(f'Replace {len(xor_values)} in {col} column')\n        print('They are: ', xor_values)\n        print()\n        full_data.loc[full_data[col].isin(xor_values), col] = 'xor'","7d470501":"map_ord1 = {'Novice':1, \n            'Contributor':2, \n            'Expert':4, \n            'Master':5, \n            'Grandmaster':6}\nfull_data.ord_1 = full_data.ord_1.map(map_ord1)","7e4ead27":"map_ord2 = {'Freezing':1, \n            'Cold':2, \n            'Warm':3, \n            'Hot':4, \n            'Boiling Hot':5, \n            'Lava Hot':6}\nfull_data.ord_2 = full_data.ord_2.map(map_ord2)","1107d3d0":"# Replace a character with its ASCII value\nfull_data['ord_3_by_ord'] = full_data.ord_3.map(ord, na_action='ignore')\nmap_ord3 = {key:value for value,key in enumerate(sorted(full_data.ord_3.dropna().unique()))}\nfull_data.ord_3 = full_data.ord_3.map(map_ord3)","e4c9caf6":"full_data['ord_4_by_ord'] = full_data.ord_4.map(ord, na_action='ignore')\nmap_ord4 = {key:value for value,key in enumerate(sorted(full_data.ord_4.dropna().unique()))}\nfull_data.ord_4 = full_data.ord_4.map(map_ord4)","ce4c706b":"# ord_5 is a little bit more special(2-characters-string)\n# We divide it into 2 pieces of character but also keep the orgin string and convert to categorical features by Label Encoder\n\nfull_data['ord_5_1'] = full_data.ord_5.map(lambda string: ord(string[0]), na_action='ignore')\nfull_data['ord_5_2'] = full_data.ord_5.map(lambda string: ord(string[1]), na_action='ignore')\n\nmap_ord5 = {key:value for value,key in enumerate(sorted(full_data.ord_5.dropna().unique()))} \nfull_data.ord_5 = full_data.ord_5.map(map_ord5)","63411b93":"num_columns = full_data.select_dtypes(exclude=['object']).columns.drop(['bin_0', 'bin_1', 'bin_2'])\ncate_columns = full_data.columns.drop(num_columns)","94a3f621":"missing_num_columns = [col for col in num_columns if any(full_data[col].isnull())]\nfor col in missing_num_columns:\n    full_data[col+'_is_missing'] = full_data[col].isnull().astype(int)","2a480779":"time_cols = ['day', 'month']\n\nfor col in time_cols:\n    full_data[col+'_sin'] = np.sin(2*np.pi*full_data[col]\/7)\n    full_data[col+'_cos'] = np.cos(2*np.pi*full_data[col]\/12)\nfull_data = full_data.drop(columns=time_cols)","680849e7":"retain_cols = [f'ord_{i}' for i in range(6)] + ['day_sin', 'day_cos', 'month_sin', 'month_cos']\nOH_cols = full_data.columns#.drop(retain_cols)","e5641f88":"print(f\"One-Hot encoding {len(OH_cols)} columns\")\n\nOH_full = pd.get_dummies(\n    full_data,\n    columns=OH_cols,\n    drop_first=True,\n    dummy_na=True,\n    sparse=True,\n).sparse.to_coo()","09eec958":"# Impute numeric features with mean value and normalize afterward \nimputer = SimpleImputer(strategy='mean')\nretain_full  = pd.DataFrame(imputer.fit_transform(full_data[retain_cols]), columns=retain_cols)\nretain_full = retain_full\/retain_full.max()","1ecc3a5d":"encoded_full = scipy.sparse.hstack([OH_full, retain_full, retain_full**2]).tocsr()\nprint(encoded_full.shape)\n\nencoded_train = encoded_full[:len(raw_train)]\nencoded_test = encoded_full[len(raw_train):]","96374504":"model = LogisticRegression(C=0.03, max_iter=300)","344cd4e5":"fig, ax = plt.subplots(figsize=(8,5))\naucs = []\ncv = StratifiedKFold(n_splits=5, shuffle=True)\n\nfor i, (train,valid) in tqdm_notebook(enumerate(cv.split(encoded_train, raw_train.target))):\n    \n    model.fit(encoded_train[train], raw_train.target[train])\n    valid_pred = model.predict_proba(encoded_train[valid])[:, 1]\n    \n    fpr, tpr, threshold = roc_curve(raw_train.target[valid], valid_pred)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label = f'Folf number {i+1} (AUC = {roc_auc:.4f})')\n    aucs.append(roc_auc)\n\nax.plot([0,1], [0,1], label='Luck', linestyle='--', color='r')  \nmean_auc = np.mean(aucs)\nstd_auc = np.std(aucs)\nax.plot(mean_auc, label=f'Average AUC score: {mean_auc:.4f} $\\pm$ {std_auc:.4f}')\nax.legend(loc=\"lower right\")\nax.set(xlim=[-.1, 1.1], ylim=[-.1, 1.1], title='Logistic Regression')\nplt.show()","fea6a64f":"\"\"\"\nmodel = LogisticRegression()\nparam_grid = {'C' : np.logspace(-4, 4, 20), 'penalty' : ['l1', 'l2']}\n\n# Create grid search object\n\nclf = GridSearchCV(LogisticRegression(), scoring='roc_auc', param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n\n# Fit on data\n\nclf.fit(encoded_train, raw_train.target)\n\nprint(\"tuned hpyerparameters :(best parameters) \",clf.best_params_)\nprint(\"Accuracy :\",clf.best_score_)\"\"\"","cb9fb23d":"\"\"\"\nmodel = LogisticRegression(**clf.best_params_)\nfig, ax = plt.subplots(figsize=(8,5))\naucs = []\ncv = StratifiedKFold(n_splits=5, shuffle=True)\n\nfor i, (train,valid) in tqdm_notebook(enumerate(cv.split(encoded_train, raw_train.target))):\n    \n    model.fit(encoded_train[train], raw_train.target[train])\n    valid_pred = model.predict_proba(encoded_train[valid])[:, 1]\n    \n    fpr, tpr, threshold = roc_curve(raw_train.target[valid], valid_pred)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label = f'Folf number {i} (AUC = {roc_auc:.4f})')\n    aucs.append(roc_auc)\n\nax.plot([0,1], [0,1], label='Luck', linestyle='--', color='r')  \nmean_auc = np.mean(aucs)\nstd_auc = np.std(aucs)\nax.plot(mean_auc, label=f'Average AUC score: {mean_auc:.4f} $\\pm$ {std_auc:.4f}')\nax.legend(loc=\"lower right\")\nax.set(xlim=[-.1, 1.1], ylim=[-.1, 1.1], title='Logistic Regression')\nplt.show()\"\"\"","ecd2cc33":"%%time\nmodel = LogisticRegression(C=0.03, max_iter=300)\nmodel.fit(encoded_train, raw_train.target)\ntest_pred = model.predict_proba(encoded_test)[:, 1]","a661102d":"%%time\nsubmiss = pd.DataFrame({\"id\": raw_test.index, \"target\": test_pred})\nsubmiss.to_csv('Phan_Viet_Hoang.csv', index=False)","0bfddb5b":"![](https:\/\/miro.medium.com\/max\/878\/1*WXpoiS7HXRC-uwJPYsy1Dg.png)\n\nOne-hot encoding is a process of binarizing the categorical variable. This is done by transforming a categorical variable with n unique values into n unique columns in the datasets while keeping the number of rows the same\n\n","36ce3fb1":"### Plot trend","f417b893":"## Ordinal features mapping\nOrdinal encoding uses a single column of integers to represent the classes. An optional mapping dict can be passed in; in this case, we use the knowledge that there is some true order to the classes themselves","e1bead3b":"Take a deeper look on different target samples, I will plot separate heatmap for target values 0 and 1.","3d456490":"My GridCV requires total: 40 (candidates) x 5 (folds) = 200 (fits)\n\n So I comment these codes in order to avoid time consuming\n After have the best parameter combination, you can try to refit the LR to encoded data (code below)","2d9fb2c8":"# Feature Engineer\nI will try to extract information from these features which hopefully can be used to improve the performance of my Logistic Regression model","306ed5b2":"These 3 patterns are almost the same, except from the case of ord_1 and month features, the 1-target class has a slightly different distribution. That means, similar to the train data set, there will be an overwhelming number of 1-target samples compare to the rest","0f05bfd7":"## Heatmap\nLet's plot now the train data (all the data) using a heatmap","9bac1e6d":"I tried to encode cyclic feature with trigonometric functions and expect this technique can be useful to understand and extract insights from samples based upon the patterns and behaviors of the data points over a specific time period","aab3a5ce":"## Time features\n\nTime features are forgotten in many machine learnning compettions. But in my personal view, time series are extremely important in statistics and  pattern recognition because many prediction problems that involve a time component\n![](https:\/\/miro.medium.com\/max\/343\/1*70cevmU8wNggGJEdLam1lw.png)","699620dc":"This produces output as a pandas dataframe.Alternatively we can use *OneHotEncoder()* method available in* sklearn* to convert out data to on-hot encoded data.\nBut this method produces a sparse metrix.The advantage of this methos is that is uses very less memory\/cpu resourses, if one try to encode all of these features with sklearn's *OneHotEncoder()*, it pottentially cause a crash session","0d182857":"# Explore Data Analysis","5c35e462":"There are misssing value on almost all columns in both train and test set. However this figure accounts for a small proportion","3356454b":"## Parameter Tuning\nParameter estimation using grid search with cross-validation and taking the best model","8c280b61":"## Density plot of numeric features\n\nNow, I want to investigate numeric features first, using seaborn's distribution plot module","389c6c1f":"First, consider these figures as categorical features. I will plot some simple bar charts using seaborn's countplot","836c3207":"It is impossible for any conclusion when plotting high cardinality features, but I still leave them there, just because they are colorful and impressive (at least for me \ud83d\ude0a\ud83d\ude0a\ud83d\ude0a)\n","c451607f":"## Simple imputer and features encoding","a95b9e6e":"### High cardinality features\n","8e86dcb8":"## Vanilla Logistic Regression\nIt's actually not a vanilla Logistic Regression, the C parameter (inverse of regularization strength) has been set to .03","1232a3ad":"But we only see the affections of time seies data when it is plotted via line charts\n\n","99bff6b7":"One can easily find that, this compettition is an imbalanced binary classification task with 81.28% of 0's and 18.72% of 1's.","8adaaed4":"It is more likely a sample will has the True target if it happens in August (16.54 %), March (15.43 %) and get the False target in December (3.98 %)","33587b5d":"As being shown on the heatmap and numeric features density plots, the test set and the 0-target class in the train data set have the similar patterns whereas the 1-target samples are generated by a little bit different distribution","d1cc9b3a":"![](https:\/\/cdn-images-1.medium.com\/max\/1200\/1*x7P7gqjo8k2_bj2rTQWAfg.jpeg)\n\n# Welcome to my kernel!\n- This is my 'naive' approach for the [Categorical Feature Encoding Challenge II](https:\/\/www.kaggle.com\/c\/cat-in-the-dat-ii) and also one of my first steps in Kaggle platform.\n- This notebook has been implemented during my day-off because of the [Coronavirus](https:\/\/www.kaggle.com\/sudalairajkumar\/novel-corona-virus-2019-dataset) spread (2019-nCoV) \n- And as usual, if my work can make you feel excited, help me to <font color='red' size=3>upvote this kernel <\/font>on the right corner \ud83d\udc96\ud83d\udc96\n\n\nP\/s: I come from Vietnam, so please ignore my English grammar mistakes through out this notebook \ud83d\ude0a\ud83d\ude0a\n","efda0a7b":"As usual, the patterns of 0 target class and test set are similar whereas the 1 class samples's distribution are slightly unalike ","7cffd15f":"## Binary features\nLet take some small visualizations with these bool features","f9883b95":"## Missing value","4e23711d":"Now, let plot them seperately","48aa6e8f":"There're no special correlation between these features, except from the pairs of ( ord_0 - target) and ( month - target)","95e55e25":"## Missing values indicator\n\nSome columns will be imputed below so that I have to keep track of which values were missing, it really makes sense that my imputation would perform better","b5d854d7":"The pattern of binary features is very similar in both train and test set, these figures are extremely close, compare to the [Categorical Feature Encoding Challenge I](https:\/\/www.kaggle.com\/c\/cat-in-the-dat\/) dataset (in which there still a gap between the train and test set) ","65b4b9cc":"# Data\nLet's load the data and take some observations","cc42bf1d":"## XOR values","4a35bd4b":"Great, the frequency are varied  but look at the True values on target proportion, this line graph is covariates (i.e we can see a clearly linear relationship) which means the higher in character order, the more likely they will appear in the 1 target class","9b39fe46":"### High cardinality features","15f0ffb9":"# Model\n- In this kernel, I will implement a sklearn's Logistic Regression model\n\n- You can also take a look at my first (and also very simple) XGBoost model for this competitions [here](https:\/\/www.kaggle.com\/warkingleo2000\/xgboost-with-onehot-and-label-encoding)","d86448ed":"## Ordinal features\nAgain, I will divide them into groups","48c470eb":"Wow, the patterns in both train and test set are almost the same in 5 numeric features, which mean that if I have a good enough model ( good performance on the training set) it seem not to find difficulty in generalization ( i.e archives relatively good performance in the test set)\n\n> The train and test data are generated by a probability distribution over datasets called the data generating process. We typically make a set of assumptions known collectively as the i.i.d. assumptions. These assumptions are that the examples in each dataset are independent from each other, and that the train set and test set are identically distributed, drawn from the same probability distribution as each other. This assumption allows us to describe the data generating  process with a probability distribution over a single example. The same\ndistribution is then used to generate every train example and every test example ([Ian Goodfellow](http:\/\/www.deeplearningbook.org\/))","495875e5":"<p><font size=\"3\" color=\"green\">Last update on 14\/2\/2020<\/font><\/p> \n<p><font size=\"5\" color=\"yellow\">What's next?<\/font><\/p>\nI will try to impove my model, try others esemble methods as well as feature engineering technique and update them in this kernel\n\n<p><font size=\"3\" color=\"red\">Thank you<\/font> for spending time on my kernel!<\/p>\n","bb916bc7":"## Nominal features\nI will divide the nominal features into 2 groups: low- (less than 10 different values) and high-cardinality nominal features (the rest)\n","c10ab636":"We will have greater insight with others plot for separate distribution visualization for samples with target value 0 and 1.","54415dd7":"That is enough for me to explore and understand the data. If you have any others useful pattern recognition, please leave some comments at the end of my kernel\n\nP\/s: There charts make me feel like they come from a IELTS writing test \ud83d\ude01\ud83d\ude01\ud83d\ude01","3a3db149":"I will divide features in to 2 groups, the first will be one hot encoded, and the other will be retained","006c2bea":"#### Bar charts","616887d3":"#### Pie charts","1ac18e74":"## Target distribution","ae767b79":"Grand Master accounts for a small proportion(16,47 %) but has the highest percentage of positive values in the target\nMaybe data was taken from another Kaggle compettion \ud83d\ude0d???\n","83d4f646":" Beautiful charts and provide useful information for me, too. Let get some:\n\n- nom_0 - Blue (~20%) value have the highest % of positive values in the target\n- nom_1 - Trapezoid (~23%) value have the highest % of positive values in the target\n- nom_2 - Lion (~21%) value have the highest % of positive values in the target\n- nom_3 - Russia (~22%) value have the highest % of positive values in the target\n- nom_4 - Bassoon (~21%) value have the highest % of positive values in the target\n\nAny special recognition?\nYes, all the values with highest % of True values on target, are not category's with highest frequency on the nominal category's\nThis is the same property between this and the previous [Cat in the Dat](https:\/\/www.kaggle.com\/c\/cat-in-the-dat\/overview) competition","a92b3c93":"We get nothing special in these maps","4bddada9":"### Low cardinality features","89d32901":"Evaluate my model using 5-fold cross validation and plot the ROC curve","1aab03e0":"## My work\n- Have some feature distribution observations and give several conclusions\n- Take some simple feature engineering\n- Fine-tune a Logistic Regression\n","290a7946":"Combine these encoded dataframe and notice that I've added a term (retain_full$^2$) which will help me to archive higher score (but also reduce the training speed)","3d5ccf8c":"# Predict test set and make submission","d018293e":"### Low cardinality features","c96138a7":"# Preparation\n\nImport necessary libraries "}}