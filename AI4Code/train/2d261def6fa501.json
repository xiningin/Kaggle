{"cell_type":{"c864e2c7":"code","8a20e82e":"code","56b3cefc":"code","a8c0bd7d":"code","0d3fdcfa":"code","452ff540":"code","67865a49":"code","35339ff0":"code","93e4263d":"code","529f5964":"code","8e75144b":"markdown","f71fe6c1":"markdown","55945cdd":"markdown","b0cc6c00":"markdown","59f23271":"markdown"},"source":{"c864e2c7":"import numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import fashion_mnist as mnist\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nfrom sklearn.model_selection import train_test_split","8a20e82e":"!wget --no-check-certificate \\\n   https:\/\/github.com\/ck196\/tensorflow-alexnet\/blob\/master\/17flowers.tar.gz?raw=true \\\n    -O \/kaggle\/working\/17flowers.tar.gz","56b3cefc":"import tarfile\n\nlocal_tar = '\/kaggle\/working\/17flowers.tar.gz'\ntar = tarfile.open(local_tar, \"r:gz\")\ntar.extractall('\/kaggle\/working\/')\ntar.close()","a8c0bd7d":"def load_image(path):\n    img =image.load_img(path,target_size=(227,227))\n    img = image.img_to_array(img)\n    #img = np.expand_dims(img, axis=3)\n    return img","0d3fdcfa":"PATH = '\/kaggle\/working\/17flowers\/jpg'\nlabels = [label for label in os.listdir(PATH) if os.path.isdir(os.path.join(PATH,label))]\nprint(f'LABELS: {labels}')\nx_train = []\ny_train = []\nfor label in labels:\n    path = os.path.join(PATH,label)\n    files = os.listdir(path)\n    for file in files:\n        x_train.append(load_image(os.path.join(path,file)))\n        y_train.append(label)\n        \nx_train = np.array(x_train).astype('uint8')\ny_train = np.array(y_train).astype('uint8')\n\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, shuffle=True)\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","452ff540":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,8))\nfor i in range(10):\n  index=i*100\n  img = np.reshape(x_train[index],(227,227,3))\n  img = img.astype('uint8')\n  plt.subplot(5,5,i+1).set_title(f'Label: {y_train[index]}')\n  plt.imshow(img)\n  plt.xticks([])\n  plt.yticks([])","67865a49":"# To prevent overfitting , ImageDataGenerator used to re-generate images with new scale.\nbatch_size = 64\ntrain_image_generator = ImageDataGenerator(\n    rescale = 1.\/255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\n# for test data generator just uses rescale\nvalidation_image_generator = ImageDataGenerator(\n    rescale = 1.\/255\n)\n\ntrain_gen = train_image_generator.flow(x_train, y_train, batch_size=batch_size)\nvalidation_gen = validation_image_generator.flow(x_test, y_test, batch_size=batch_size)","35339ff0":"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\nmodel = tf.keras.models.Sequential([\n    Conv2D(96, (11,11), input_shape=(227,227,3), strides=(4,4), activation='relu'),\n    MaxPooling2D(2,2),\n    Conv2D(256, (3,3),  strides=(1,1), padding='same', activation='relu'),\n    MaxPooling2D(2,2),\n    Conv2D(384, (3,3),  strides=(1,1), padding='same',  activation='relu'),\n    Conv2D(384, (3,3),  strides=(1,1), padding='same',  activation='relu'),\n    Conv2D(256, (3,3),  strides=(1,1), padding='same', activation='relu'),\n    MaxPooling2D(2,2),\n    Flatten(),\n    Dense(4096, activation='relu'),\n    Dense(4096, activation='relu'),\n    Dropout(0.4),\n    Dense(17, activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n              loss = 'sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","93e4263d":"history = model.fit_generator(train_gen,\n                              steps_per_epoch=len(x_train) \/ batch_size,\n                              epochs=50,\n                              validation_data=validation_gen,\n                              validation_steps=len(x_test) \/ batch_size)\n\nmodel.evaluate(x_test, y_test)","529f5964":"## Visualize the Training Process\nimport matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\nplt.figure()\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n#plt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\n#plt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","8e75144b":"# Visualize the Training Process","f71fe6c1":"# Download Data Set & Normalize\n","55945cdd":"# AlexNet (2012) with tf.keras","b0cc6c00":"# Overview\nAlexNet architecture, based on their paper (<a>https:\/\/papers.nips.cc\/paper\/4824-imagenet-classification-with-deep-convolutional-neural-networks<\/a>). Tf keras used in this implemention.\n\nWith 60M parameters, AlexNet has 8 layers \u2014 5 convolutional and 3 fully-connected. AlexNet just stacked a few more layers onto LeNet-5. At the point of publication, the authors pointed out that their architecture was \u201cone of the largest convolutional neural networks to date on the subsets of ImageNet.\u201d\n\n\u2b50\ufe0fWhat\u2019s novel?\n1. They were the first to implement Rectified Linear Units (ReLUs) as activation functions.","59f23271":"# AlexNet Architecture\n<img src=\"https:\/\/engmrk.com\/wp-content\/uploads\/2018\/10\/AlexNet_Summary_Table.jpg\" alt=\"Architecture\" style=\"width: 500px; float:center\"\/>"}}