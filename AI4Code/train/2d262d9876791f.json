{"cell_type":{"2bed4f38":"code","98b986b1":"code","ec145c15":"code","2e0e1066":"code","f1f3f166":"code","0511301d":"code","e4abcfdd":"code","c923001e":"code","3a0005d2":"code","ac4c08ba":"code","02b1c6ab":"code","55a4bdbd":"code","09f414da":"code","45eafea2":"code","e0185439":"code","db6dd5c7":"code","641df315":"code","d61fe296":"code","b4d2ffbb":"code","2c1b20f9":"code","3f35d14c":"code","867d7fdc":"code","2bed24c6":"code","a256e03d":"code","5844d06c":"code","fa13f422":"code","c3b67f3f":"code","4c304cdf":"code","1c3ee0e2":"code","778a7d06":"code","a45bcea1":"code","6d4adfd4":"code","2c654add":"code","f10683db":"code","2c9891ae":"code","1db709d3":"code","fe56abc5":"code","5db2567c":"code","fa728e81":"code","389282a0":"code","a12ed471":"code","471e2920":"code","9d8f134a":"code","31bae3d5":"code","45c811f4":"markdown","c67effae":"markdown","9839a9f9":"markdown","387869d8":"markdown","01113779":"markdown","8ac7164e":"markdown","5dd64f11":"markdown","c6462022":"markdown","55a7c28f":"markdown","591d12e0":"markdown","f328a7ad":"markdown","0303cd43":"markdown","1a7dde8d":"markdown","6296364c":"markdown","33912413":"markdown","1effbc91":"markdown","5552c0b9":"markdown"},"source":{"2bed4f38":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas_profiling\n\n# Data Viz\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(16,16)})\n\n# import the data\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_joined = pd.concat([df_train, df_test], sort=False)","98b986b1":"df_joined.head()","ec145c15":"print(\"Total Columns: %s\\nTotal Rows: %s\" % (df_joined.shape[1], df_joined.shape[0]))","2e0e1066":"types = {}\nfor d in df_joined.dtypes.tolist():\n    if d not in types.keys():\n        types[d] = 1\n    else:\n        types[d] += 1\nprint(\"Total count of column types\\n-------------------\")\ntypes","f1f3f166":"print(\"Total columns with null values: %s\" % (len(df_joined.columns[df_joined.isna().any()]),))","0511301d":"print(\"Columns with null values\\n-------------------\")\nnull_series = pd.isnull(df_joined).sum()\nnull_series[null_series > 0]","e4abcfdd":"types = {}\nindices = null_series[null_series > 0].index.tolist()\nfor d in df_joined[indices].dtypes.tolist():\n    if d not in types.keys():\n        types[d] = 1\n    else:\n        types[d] += 1\nprint(\"Total count of column types for columns with null values\\n-------------------\")\ntypes","c923001e":"# df_joined.profile_report(style={'full_width': True})","3a0005d2":"sns.barplot(x='SaleType', y='SalePrice', data=df_train)","ac4c08ba":"sns.pointplot(x='SaleCondition', y='SalePrice', data=df_train)","02b1c6ab":"sns.pointplot(x='MSSubClass', y='SalePrice', data=df_train)","55a4bdbd":"sns.pointplot(x='GarageQual', y='SalePrice', data=df_train, order=['Po', 'Fa', 'TA', 'Gd', 'Ex'], color='blue', estimator=np.median)","09f414da":"sns.pointplot(x='GarageCond', y='SalePrice', data=df_train, order=['NA','Po', 'Fa', 'TA', 'Gd', 'Ex'])","45eafea2":"sns.pointplot(x='HeatingQC', y='SalePrice', data=df_train, order=['Po', 'Fa', 'TA', 'Gd', 'Ex'], color='green', estimator=np.median)","e0185439":"sns.pointplot(x='ExterQual', y='SalePrice', data=df_train, order=['Po', 'Fa', 'TA', 'Gd', 'Ex'], color='orange', estimator=np.median)","db6dd5c7":"sns.pointplot(x='KitchenQual', y='SalePrice', data=df_train, order=['Po', 'Fa', 'TA', 'Gd', 'Ex'], color='purple', estimator=np.median)","641df315":"sns.pointplot(x='BsmtCond', y='SalePrice', data=df_train, order=['NA','Po', 'Fa', 'TA', 'Gd', 'Ex'])","d61fe296":"sns.pointplot(x='ExterCond', y='SalePrice', data=df_train, order=['NA','Po', 'Fa', 'TA', 'Gd', 'Ex'])","b4d2ffbb":"sns.pointplot(x='OverallCond', y='SalePrice', data=df_train)","2c1b20f9":"sns.pointplot(x='Functional', y='SalePrice', data=df_train, order=['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'])","3f35d14c":"sns.regplot(x='LotArea', y='SalePrice', data=df_train)","867d7fdc":"sns.regplot(x='1stFlrSF', y='SalePrice', data=df_train)","2bed24c6":"sns.regplot(x='2ndFlrSF', y='SalePrice', data=df_train)","a256e03d":"sns.regplot(x='MasVnrArea', y='SalePrice', data=df_train)","5844d06c":"sns.regplot(x='TotalBsmtSF', y='SalePrice', data=df_train)","fa13f422":"sns.pointplot(x='BldgType', y='SalePrice', data=df_train)","c3b67f3f":"sns.pointplot(x='HouseStyle', y='SalePrice', data=df_train, order=['1Story', '1.5Fin', '1.5Unf', '2Story', '2.5Fin', '2.5Unf', 'SFoyer', 'SLvl'], estimator=np.mean)","4c304cdf":"sns.barplot(x='MSZoning', y='SalePrice', data=df_train)","1c3ee0e2":"chart = sns.barplot(x='Neighborhood', y=\"SalePrice\", data=df_train)\nchart.set_xticklabels(chart.get_xticklabels(), rotation=45)","778a7d06":"sns.pointplot(x='YrSold', y='SalePrice', data=df_train)","a45bcea1":"def num_clean(df_train, df_test):\n    \"\"\" Clean the data before we encode\"\"\"\n    #1) df_train\n    df_train_num = df_train.select_dtypes(include='number') # fetch num columns\n    train_missing_cols = df_train_num.columns[df_train_num.isnull().any()].tolist() # fetch num columns with missing\n    df_train = _fill_num_df(df_train, train_missing_cols)\n    \n    # 2) df_test\n    df_test_num = df_test.select_dtypes(include='number') # fetch num columns\n    test_missing_cols = df_test_num.columns[df_test_num.isnull().any()].tolist() # fetch num columns with missing\n    df_test = _fill_num_df(df_test, test_missing_cols)\n    \n    return df_train, df_test\n    \ndef _fill_num_df(df, cols):\n    \"\"\" Fill in the missing values for the dataframe \"\"\"\n    for col in cols:\n        df[col] = df[col].fillna(df[col].mean())\n    return df\n    ","6d4adfd4":"def cat_clean(df_train, df_test):\n    \"\"\" Clean the data data before we encode \"\"\"\n    #1) df_train\n    df_train_cat = df_train.select_dtypes(include='object') # fetch cat columns\n    train_missing_cols = df_train_cat.columns[df_train_cat.isnull().any()].tolist() # fetch cat columns with missing\n    df_train = _fill_cat_df(df_train, train_missing_cols)\n    \n    # 2) df_test\n    df_test_cat = df_test.select_dtypes(include='object') # fetch cat columns\n    test_missing_cols = df_test_cat.columns[df_test_cat.isnull().any()].tolist() # fetch cat columns with missing\n    df_test = _fill_cat_df(df_test, test_missing_cols)\n    \n    return df_train, df_test\n    \ndef _fill_cat_df(df, cols):\n    \"\"\" Fill in the missing values for the dataframe \"\"\"\n    for col in cols:\n        df[col] = df[col].fillna(df[col].mode().values[0])\n    return df","2c654add":"from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder","f10683db":"def nominal_encode(df_train, df_test, cols):\n    \"\"\"Encode the nominal features\"\"\"\n    # create encoder\n    encoder = OneHotEncoder(dtype='int', sparse=False) # sparse returns array not matrix\n    # transform columns\n    encoded_df_train = pd.DataFrame(encoder.fit_transform(df_train[cols])) # remove target and put back after transform\n    encoded_df_test = pd.DataFrame(encoder.transform(df_test[cols]))\n    # Add index back to the transformed dfs\n    encoded_df_train.index = df_train.index\n    encoded_df_test.index = df_test.index\n    # remove the original cols b\/c we're about add the encoded\n    df_train = df_train.drop(cols, axis=1)\n    df_test = df_test.drop(cols, axis=1)\n    # create the new dfs\n    df_train = pd.concat([df_train, encoded_df_train], axis=1)\n    df_test = pd.concat([df_test, encoded_df_test], axis=1)\n    \n    return df_train, df_test\n    \n    \ndef ordinal_encode(df_train, df_test, cols):\n    \"\"\"Encode the ordinal features\"\"\"\n    # Encoder\n    encoder = OrdinalEncoder(dtype='int')\n    # transform\n    encoded_df_train = pd.DataFrame(encoder.fit_transform(df_train[cols]))\n    encoded_df_test = pd.DataFrame(encoder.transform(df_test[cols]))\n    # add index \n    encoded_df_train.index = df_train.index\n    encoded_df_test.index = df_test.index\n    # remove original columsn b\/c we transformed them\n    df_train = df_train.drop(cols, axis=1)\n    df_test = df_test.drop(cols, axis=1)\n    # concat\n    df_train = pd.concat([df_train, encoded_df_train], axis=1)\n    df_test = pd.concat([df_test, encoded_df_test], axis=1)\n    \n    return df_train, df_test","2c9891ae":"# 1) Clean\ndf_train, df_test = num_clean(df_train, df_test)\ndf_train, df_test = cat_clean(df_train, df_test)\n\n# 2) Encode\ndf_train, df_test = nominal_encode(df_train, df_test, [\"MSZoning\", \"Street\", \"Alley\", \"Utilities\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"MiscFeature\", \"SaleType\", \"Electrical\", \"GarageType\", \"LotShape\", \"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Foundation\", \"Heating\", \"PavedDrive\", \"CentralAir\", \"SaleCondition\"])\ndf_train, df_test = ordinal_encode(df_train, df_test, [\"BsmtQual\", \"BsmtCond\", \"KitchenQual\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Functional\", \"BsmtExposure\", \"GarageFinish\", \"Fence\", \"LandSlope\", \"ExterQual\", \"ExterCond\", \"BsmtFinType1\", \"BsmtFinType2\", \"HeatingQC\"])","1db709d3":"from sklearn.model_selection import train_test_split","fe56abc5":"TEST_SIZE = 0.25\nX_all = df_train.drop(['SalePrice', 'Id'], axis=1)\ny_all = df_train[['SalePrice']]\n\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=TEST_SIZE, random_state=3)","5db2567c":"from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import LinearSVR\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.metrics import max_error,mean_absolute_error,mean_squared_error,median_absolute_error,r2_score\n\nimport math","fa728e81":"models = ['LinearRegression',\n            'LinearSVR',\n            'DecisionTreeRegressor',\n            'GradientBoostingRegressor',\n            'RandomForestRegressor']\n\nresults = {\n    \"models\": [],\n    \"mean_absolute_error\": [],\n    \"root_mean_squared_error\": [],\n    \"median_absolute_error\": [],\n    \"max_error\": [],\n    \"r2_score\": [],\n}\n\nfor model in models:\n    m = eval(model)()\n    m.fit(X_train, y_train)\n    y_pred = m.predict(X_test)\n    \n    results['models'].append(model)\n    results['mean_absolute_error'].append(mean_absolute_error(y_test, y_pred))\n    results['root_mean_squared_error'].append(math.sqrt(mean_squared_error(y_test, y_pred)))\n    results['median_absolute_error'].append(median_absolute_error(y_test, y_pred))\n    results['max_error'].append(max_error(y_test, y_pred))\n    results['r2_score'].append(r2_score(y_test, y_pred))\n                                       \nresults_df = pd.DataFrame(results)\nresults_df.sort_values(by=['root_mean_squared_error', 'r2_score'], ascending=True, inplace=True)\nresults_df","389282a0":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer","a12ed471":"# Define scorer\ndef rmse_metric(y_test, y_pred):\n    score = math.sqrt(mean_squared_error(y_test, y_pred))\n    return score\n\n# Scorer function would try to maximize calculated metric\nrmse_scorer = make_scorer(rmse_metric, greater_is_better=False)\n\ndef tune_model(model, X, y, param_grid, cv):\n    reg = GridSearchCV(estimator=eval(model)(), param_grid=param_grid, cv=cv, scoring=rmse_scorer, n_jobs=-1, verbose=False)\n    reg.fit(X, y)\n    return (model, reg.best_score_, reg.best_estimator_)","471e2920":"tuning_models = ['GradientBoostingRegressor',\n                'RandomForestRegressor']\n\nparam_grid = {\n    'RandomForestRegressor': {\n        'n_estimators': [10, 25, 50],\n        'max_depth': [10, 25, 50],\n        'max_features': ['auto', 'sqrt', 'log2']\n    },\n    'GradientBoostingRegressor': {\n        'learning_rate': [0.01, 0.001, 0.0001],\n        'n_estimators': [100, 150, 200],\n        'max_depth': [3, 5, 10],\n        'max_features': ['auto', 'sqrt', 'log2'],\n    }\n}\n\ntuned_results = {\n    'model': [],\n    'best_score': [],\n    'best_estimator': []\n}\n\nfor model in tuning_models:\n    m, score, est = tune_model(model, X_train, np.ravel(y_train), param_grid[model], 5)\n    tuned_results['model'].append(m)\n    tuned_results['best_score'].append(score)\n    tuned_results['best_estimator'].append(est)\n    \ntuned_results_df = pd.DataFrame(tuned_results)\ntuned_results_df.sort_values(by=['best_score'], ascending=True, inplace=True)\ntuned_results_df","9d8f134a":"best_estimator = tuned_results_df.iloc[0,2]\ny_predict = best_estimator.predict(df_test.drop(['Id'], axis=1))\ny_predict_df = pd.DataFrame({'SalePrice': y_predict})\n\nsubmission_df = pd.concat([df_test[['Id']], y_predict_df], axis=1)\n\nsubmission_df.head()","31bae3d5":"submission_df.to_csv('Housing_Prices_Prediction_1.csv', index=False)","45c811f4":"## Cleaning\nWe have 35 features that have missing values and of those 35, 6 have more than half of their values missing. 23 of the 35 are categorical while the rest are numerical.","c67effae":"We're mainly interested in the Root Mean Square Error (RMSE) since that's what the final prediction will be judged by. Looks like the winners are the **GradientBoostingRegressor** and **RandomForestRegressor** models.","9839a9f9":"## Submission","387869d8":"# Feature Engineering","01113779":"# Data Exploration\n\n## High Level\nLet's first see what we're working with. It's important for us to get a feel for the data so we can start anticipating the cleaning, pre-processing and potential feature engineering. We also want to start understanding feature relationships as well. \n\nSince we have so much data, relatively speaking, we're only interested in the top level aggregate numbers for now. The total number of features available make it too cumbersome to really dive deep on each column as opposed to a dataset like the Titanic dataset. As we can see we have only 3 different types of data (2 of which are numerical) a majority of the columns are *object* type which means we could potentially be dealing with a lot of categorical data. Almost half of the total columns have null values and 6 of those columns have over half null values. The prediction target here is the **SalePrice** column. \n\n## Hypotheses\nBased on the features we have available to us paired with the contextual knowledge of housing prices and costs we can make some hypotheses about which factors would have a large impact on the sale price for a house. Generally speaking homes that have more in terms of size and amenities will be more expensive which in our case include: *Dwelling Type*, *Lot Area*, *House Style*, *Basement Size*, *First Floor Size*, *Second Floor Size*, *Wood Deck*, *Pool Area*, etc. \n\nHomes that are most recently built along with the top quality condition which include: *Pool Quality*, *Fence Quality*, *Garage Quality*, *Fireplace Quality*, *Kitchen Quality*, *Heating Quality*, *Basement Quality*, *Basement Condition*, *External Quality*, *Overall Condition*, *Overall Quality*, etc. Some interesting features that may also have a great impact include *SaleCondition* where the house maybe foreclosed so the price, we assume, would be low; *SaleType* where the home for example could've been sold as soon as it was done constructed.\n\nThe feature that can play a big factor would be the location of the house there are 26 different locations present and some of these locations may have a large disparity in pricing for example because maybe a university is present. If we wanted to dive deeper we can perhaps do some internet digging to find what the housing market looks like for these places using something like a zillow and align it the year that this dataset speaks for.   \n","8ac7164e":"# Modeling","5dd64f11":"## Newer Houses","c6462022":"## Model Selection\nSince we are trying to predict pricing (continuous value) we should explore regressor models. Let's test several regression models and identify the one that gives us the best results. ","55a7c28f":"## Tuning\nLet's try to optimize our model parameters to ensure that we get the best model.","591d12e0":"# Introduction\nThe goal of this notebook is to build a ML model that can accurately predict housing prices from Ames, Iowa based on the features available from the housing data set. Since we're predicing pricing we will most likely be using a regression model as our final model.","f328a7ad":"## Sizing","0303cd43":"## Split Training Data\nGiven our test dataset let's split our training data","1a7dde8d":"# Preprocessing\nBefore we begin modeling we have to make sure that clean our data then encode.","6296364c":"## Encoding\nNext assuming that we've successfully filled the missing values in our dataset we have to encode the **43** categorical features. In the process, let's make sure to differentiate between nominal vs ordinal categorical columns by using the corresponding encoding method. ","33912413":"## Other","1effbc91":"# Data Visualization\nNow let's start poking at the hypotheses we made above by using some graphing to infer some relationships.\n\nThe newer the house the higher the price hypothesis seems to be correct as we can see below indicated by the **Sale Type**, the **Sale Condition** (the *Partial* type indicates that it is a new home see documentation) and the **MSSubClass** where the *1946 & Newer* model homes sold for more comparatively. \n\nThe assumption about the higher quality and higher condition seems to also maintain. As we can see from the **Heating Quality**, **External Quality**, **Kitchen Quality**, **Basement Condition** and generally speaking for **External Condition**,**Overall Condition** and **Functional** type. The Garage quality and condition doesn't have a strong correlation between higher quality\/condition with pricing. Maybe that's because people care less about what the garage looks like maybe.\n\nFinally, the sizing hypothesis seems to stand correct as well if you look at the **First floor**, **Second floor**, **Basement** and **Masonry veneer type**. Although the **Lot area** doesn't have a strong impact.\n\nThere are several other features here that seem real interesting and impactful more importantly they look more like a blanket category. For example the **Year Sold** looks like it can be effective because we know in real estate the market can be up or down from period to period. The same goes for the **Neighborhood** as we know that pricing is consistent and relatively similarly priced for all houses in the neighborhood. **Building type** also makes sense because generally building types will be consistent in pricing across any attribute.  \n\n","5552c0b9":"## Quality & Condition"}}