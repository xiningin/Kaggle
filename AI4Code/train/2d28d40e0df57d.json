{"cell_type":{"65378666":"code","f6286f71":"code","cb84ea02":"code","100f4c92":"code","e4a02ede":"code","45088cad":"code","c99b6fe1":"code","58cf80be":"code","685c55e7":"code","b8e55f04":"code","c67fb2e4":"code","03f33037":"code","d222f8ca":"code","8958f997":"code","05a2746e":"code","0acca9fa":"code","cda9f813":"code","430c0eed":"code","e7486e26":"code","9e178d78":"code","59f1b83a":"code","da08973a":"code","19a48b56":"code","41e42fef":"code","a65196c5":"code","23b9e257":"code","9986a3eb":"code","dcd8cbf9":"code","3aef9987":"code","597d040b":"code","806b28c0":"code","671ea9ce":"code","b7b1fd12":"code","5ad5aba6":"code","c52b6297":"code","29a44920":"code","b7b01ae6":"code","794f8f06":"code","eefc436b":"code","9ceb30c6":"code","55a3acac":"code","cca03ff0":"code","ff3ecfb9":"code","7404fea4":"code","6199471c":"code","8db657a9":"code","ff34ad67":"code","4c406a21":"code","59df917e":"code","f8480b9c":"code","7dc3f60d":"code","d624c32c":"code","219434ef":"code","ecdb2a20":"code","546913c4":"code","e9d026fa":"code","d6a314be":"code","f39e0055":"code","e3c1cb26":"code","c21673b3":"code","b6f12202":"code","da7a7965":"code","34036bec":"code","39f8fefd":"code","673d6a1a":"code","f7d5a94c":"code","06333853":"code","c0dd88b4":"code","c44ddf86":"code","4fe54982":"markdown","61d045be":"markdown","4c000fad":"markdown","7d8cd2f8":"markdown","6c5ad812":"markdown","fbeec234":"markdown","9075ba20":"markdown","d591094b":"markdown","1be49a8e":"markdown","9431aa79":"markdown","1c049a34":"markdown","b109c504":"markdown","bc86601b":"markdown","9bb6fb29":"markdown","2e7e78df":"markdown","9f1b1b62":"markdown","f5b9edb4":"markdown","11fa2afd":"markdown","a3f65b2e":"markdown","97a98560":"markdown","b31928b8":"markdown","a823817e":"markdown","12b956b7":"markdown","bf257479":"markdown","404038bc":"markdown","b6c954c4":"markdown","d1f2cf7d":"markdown","8b456359":"markdown","dac6d8ca":"markdown","c5856207":"markdown","07cf3f7a":"markdown","8daabf98":"markdown","25c2f74d":"markdown","07926c5d":"markdown","8b519cb4":"markdown","3b21ecf8":"markdown","b36cfc32":"markdown","ee5d763b":"markdown","422d276a":"markdown","da28c6b1":"markdown","6f8237ee":"markdown","54887418":"markdown","e8029125":"markdown","9cc753f9":"markdown","ca79a893":"markdown","07c44031":"markdown","ca77ee6d":"markdown","fd9928d3":"markdown","8b5ef488":"markdown","0f405f8f":"markdown","c9e0eda1":"markdown"},"source":{"65378666":"from IPython.display import Image\nImage(\"..\/input\/text-gen-image\/text_gen_image.jpeg\")","f6286f71":"import io\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom tensorflow import keras\nimport tensorflow_datasets as tfds","cb84ea02":"(train_data, test_data), info = tfds.load('imdb_reviews\/subwords8k',\n                                         split=(tfds.Split.TRAIN, tfds.Split.TEST),\n                                         with_info=True, as_supervised=True)","100f4c92":"info","e4a02ede":"info.features","45088cad":"encoder = info.features['text'].encoder\nencoder","c99b6fe1":"string_exemple = \"Marry has a little lamb\"\nprint(\"The string exemple is: {}\".format(string_exemple))\n\nencoded_string = encoder.encode(string_exemple)\nprint(\"The encoded string is: {}\".format(encoded_string))\n\noriginal_string = encoder.decode(encoded_string)\nprint(\"The decoded string is: {}\".format(original_string))\n\nprint(\"\\n\")\nfor index in encoded_string:\n    print(\"{} ----> {}\".format(index, encoder.decode([index])))","58cf80be":"train_data = train_data.shuffle(10000)\n\nval_data = train_data.take(5000) \ntrain_data = train_data.skip(5000)","685c55e7":"BUFFER_SIZE = 10000\nBATCH_SIZE = 32\ntrain_batches = train_data.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE) #The shuffle is used here to shuffle the data in the dataset\nval_batches = val_data.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE).repeat()\ntest_batches = test_data.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)","b8e55f04":"for i in train_batches.take(1):\n    print(i)","c67fb2e4":"for batch in train_batches:\n    print(batch[0])\n    print(\"\\n\")\n    print(batch[1])\n    break","03f33037":"iterator = train_batches.__iter__()\nnext_element = iterator.get_next()\none_batch_of_reviews = next_element[0]\none_batch_of_labels = next_element[1]\n\nprint(one_batch_of_reviews)\nprint('\\n')\nprint(one_batch_of_labels)","d222f8ca":"decoded_review = encoder.decode(one_batch_of_reviews[0])\nprint(decoded_review)\nprint(\"\\n\")\nif one_batch_of_labels[0] == 0:\n    print(\"this person didn't liked the movie\")\nelse:\n    print(\"this person liked this movie\")","8958f997":"def plot_graph(history):\n    history_dict = history.history\n    \n    acc = history_dict['acc'] # We won't display the 3 first epochs in order to have a more precise view of the last points.\n    val_acc = history_dict['val_acc']\n    loss = history_dict['loss']\n    val_loss = history_dict['val_loss']\n\n    epochs = range(1, len(acc) + 1)\n    \n    fig = plt.figure(figsize = (18,8))\n    plt.subplot2grid((1,2), (0,0))\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n\n    plt.subplot2grid((1,2), (0,1))\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","05a2746e":"from keras.models import Sequential\nfrom keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense, Dropout, Flatten, Bidirectional","0acca9fa":"import tensorflow as tf\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","cda9f813":"from keras.models import Sequential\nfrom keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense, Dropout, Flatten, Bidirectional, GlobalAveragePooling1D\n\nembedding_dim = 16\n\nwith strategy.scope():\n    model_pool = keras.Sequential([\n    Embedding(encoder.vocab_size, embedding_dim),\n    GlobalAveragePooling1D(),\n    Dense(16, activation='relu'),\n    Dropout(0.2),\n    Dense(1)\n])\n    \nmodel_pool.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\nmodel_pool.summary()","430c0eed":"history_pool = model_pool.fit(train_batches, epochs=10, validation_data = test_batches)","e7486e26":"plot_graph(history_pool)","9e178d78":"from keras.models import Sequential\nfrom keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense, Dropout, Flatten, Bidirectional\n\nembedding_dim = 16\n\nwith strategy.scope():\n    model_LSTM_stacked = Sequential([\n    Embedding(input_dim = encoder.vocab_size, output_dim = embedding_dim ,mask_zero=True),\n    Bidirectional(LSTM(64, return_sequences=True)),\n    Bidirectional(LSTM(32)),\n    Dense(32, activation='relu'),\n    Dropout(0.2),\n    Dense(1)\n])\n    \nmodel_LSTM_stacked.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\nmodel_LSTM_stacked.summary()","59f1b83a":"history_LSTM_stacked = model_LSTM_stacked.fit(train_batches, epochs=5, validation_data = test_batches)","da08973a":"plot_graph(history_LSTM_stacked)","19a48b56":"from keras.layers import GlobalMaxPool1D\n\nembed_size = 128\n\nwith strategy.scope():\n    model = Sequential()\n    model.add(Embedding(input_dim = encoder.vocab_size, output_dim = embedding_dim ,mask_zero=True))\n    model.add(Bidirectional(LSTM(64, return_sequences = True)))\n    model.add(Bidirectional(LSTM(64, return_sequences = True)))\n    model.add(GlobalMaxPool1D())\n    model.add(Dense(64, activation=\"relu\"))\n    model.add(Dropout(0.1))\n    model.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n\nmodel.fit(train_batches, epochs=5, validation_data = test_batches)","41e42fef":"import tensorflow as tf\n\nimport numpy as np\nimport os\nimport time","a65196c5":"print('Loading the data...')\npath_to_file = tf.keras.utils.get_file('shakespeare.txt','https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/shakespeare.txt')\nprint('...Data loaded.')","23b9e257":"text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n# *open(file_path, open_mode)* if the function used to open any kind of file. You have to specify the directory and the opening mode. Here 'rb' means 'read binary'\n# *read()* returns one big string containing all the characters\n# You then have to decode it. We use the *utf-8* encoding since it is the most common and the data is binary and written in 8bits.","9986a3eb":"print(text[0:250]) #It looks great !","dcd8cbf9":"text[0:250]","3aef9987":"print('The text object is {} of length {}'.format(type(text),len(text))) #so one really big string","597d040b":"vocab = sorted(set(text)) #set selects the unique parameters\nprint(\"There are {} unique characters in the whole text\".format(len(vocab)))","806b28c0":"char2idx = {u:i for i, u in enumerate(vocab)} # *enumerate* is used to count the number of loops, accessible with the variable *i*\nidx2char = np.array(vocab) #Since we sorted the vocabulary and won't touch it anymore,\n# the index that corresponds to the character is just the position of the character in the array, easily accessible.","671ea9ce":"print('{')\nfor char,_ in zip(char2idx, range(20)):\n    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\nprint('  ...\\n}')","b7b1fd12":"text_as_int = np.array([char2idx[c] for c in text]) # here we create a list that contains all the characters as numbers, and then turns it to a numpy array","5ad5aba6":"print(\"the type of text_as_int is {} of shape {}\".format(type(text_as_int),text_as_int.shape))","c52b6297":"print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))","29a44920":"seq_length = 100 # Number of character in one sequence of word (numbers here). For now it contains the input and the target since the size of 100.\nexamples_per_epoch = text_as_int.shape[0]\/\/(seq_length+1) # The number of sequence we have, hence, the number of sample per epoch for later","b7b01ae6":"char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)","794f8f06":"for element in char_dataset.take(13):\n    print(element)","eefc436b":"tiny_dataset = char_dataset.take(5) # *take(x)* creates a new TF dataset that contains at most x elements\nfor element in tiny_dataset.as_numpy_iterator():\n    print(element)","9ceb30c6":"# or in a fancier way:\nprint(list(tiny_dataset.as_numpy_iterator()))","55a3acac":"sequences = char_dataset.batch(seq_length+1, drop_remainder=True)","cca03ff0":"five_sequences = sequences.take(5) # *take(x)* creates a new TF dataset that contains at most x elements\nfor element in five_sequences:\n    print(repr(''.join(idx2char[element.numpy()])))\n    print('\\n')","ff3ecfb9":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return(input_text, target_text)","7404fea4":"dataset = sequences.map(split_input_target)","6199471c":"for element in dataset.take(1):\n    print(\"Each element is now a {} containing {} Tf.Tensors:\\n\".format(type(element),np.ndim(element)))\n    print(element[0])\n    print(element[1])","8db657a9":"for input_example, target_example in  dataset.take(1):\n    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))","ff34ad67":"for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))","4c406a21":"# Batch size\nBATCH_SIZE = 64\nBUFFER_SIZE = 10000 # This value is used to shuffle the dataset. The value has to be greater than the size of the dataset\n# in order to make a good shuffle. If too low, the dataset won't be shuffled completely.\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","59df917e":"# Number of different character in the text\nvocab_size = len(vocab)\n\n# The embedding dimension\n# (you already know this part, as usual, we want vectors that represents well words with the same meaning)\n# Here, each character will be encoded in a vector of dimension embedding_size\nembedding_dim = 256\n\n# Number of RNN units\nrnn_units = 1024","f8480b9c":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                                  batch_input_shape=[batch_size, None]),\n        tf.keras.layers.GRU(rnn_units,\n                            return_sequences=True,\n                            stateful=True,\n                            recurrent_initializer='glorot_uniform'),\n        tf.keras.layers.Dense(vocab_size)\n      ])\n    return model","7dc3f60d":"model = build_model(\n    vocab_size = len(vocab),\n    embedding_dim=embedding_dim,\n    rnn_units=rnn_units,\n    batch_size=BATCH_SIZE)","d624c32c":"for input_example_batch, target_example_batch in dataset.take(1): #Taking the first batch of input and target of the dataset\n    example_batch_predictions = model(input_example_batch) #Let's predict the input_example_batch\n    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")","219434ef":"model.summary()","ecdb2a20":"sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)","546913c4":"sampled_indices # This is a tf.Tensor of shape=(100,1)","e9d026fa":"sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()","d6a314be":"sampled_indices # Now it's just an array of 100 elements. This is the prediction for the first sequence of the batch.","f39e0055":"print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\nprint(\"\\n\")\nprint(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))","e3c1cb26":"def loss(labels, logits):# Since the model gives log of probabilities, we have to flag the from_logits\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)","c21673b3":"example_batch_loss  = loss(target_example_batch, example_batch_predictions)\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())","b6f12202":"model.compile(optimizer='adam', loss=loss)","da7a7965":"# Directory where the checkpoints will be saved\ncheckpoint_dir = '.\/training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","34036bec":"EPOCHS = 20","39f8fefd":"history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","673d6a1a":"tf.train.latest_checkpoint(checkpoint_dir) # Find the checkpoint specified with the *checkpoint_dir* path.","f7d5a94c":"model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1) #Let's build another model but with batches of 3 elements now (to get results of 3 sentences)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) # Load the weight to the model\n\nmodel.build(tf.TensorShape([1, None]))","06333853":"model.summary()","c0dd88b4":"def generate_text(model, start_string):\n  # Evaluation step (generating text using the learned model)\n\n    # Number of characters to generate\n    num_generate = 1000\n\n    # Converting our start string to numbers (vectorizing)\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    # Empty string to store our results\n    text_generated = []\n\n    # Low temperatures results in more predictable text.\n    # Higher temperatures results in more surprising text.\n    # Experiment to find the best setting.\n    temperature = 0.5\n\n    # Here batch size == 1\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a categorical distribution to predict the character returned by the model\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # We pass the predicted character as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(idx2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))","c44ddf86":"print(generate_text(model, start_string=u\"ROMEO: \"))","4fe54982":"### Last step: the full dataset\nThat's right. For now our dataset is not ready. Our model likes the data in batches (\"-Haven't we batched the data already ?? -Well yes, but no. We just bacthed it to get sequences (each sequence in an individual input), now, we want to make packs of inputs to feed it to the model.\" So we'll just re-use the previous method *batch()* **but** since we are good data scientist, we'll shuffle the data a bit.","61d045be":"As you may have noticed it, there's no need to specify the input and output data when the dataset is constructed with tupples containing two elements: the input and the ouput.","4c000fad":"Let's see if the loss works on the predicted sequence:","7d8cd2f8":"...or you have to build an iterator and use the *get_next()* method to display one review. As you can see it, it is already encoded as numbers.","6c5ad812":"What we can see here:\n- a bacth is a tupple composed of two elements:\n    - *batch[0]* has shape (32,1032). This means that we succesfully built a batch of 32 reviews **and** that all reviews has the same lenght: 1032. We can also see that the last \"words\" are all encoded as zeros as expected.\n    - *batch[1]* has a shape (32,) this is just a 1D vector containing all the labels: ones and zeros","fbeec234":"# Building the model\nLet's get into it for real ! But first, let's build a function that will be very useful to display graphs.","9075ba20":"How to build an efficient model now ?\nThe issue with text based problems is that keeping in memory previous words really helps to get the meaning of a sentence (right ?). As a result, we need to build a model that keeps in memory what it've seen. Luckily, we can build models like this in Keras. There are many possibilities: RNN, LSTM, GRU.\nLet's define the ideas behind each of those:\n- **RNN**: When an input is fed to this model (a review (tensor) as shape (number_of_words, length_embedded_word)) a loop is updating the state of the output at each new word seen. As a result, an output is the result of the previous output and the new input. The last output is the combinaison of the last input and the previous output that contains information about all the previous inputs. About the ouput: it is in shape (number_of_words, output_feature) where each vector contains the inforamtion about the previous ones, that's why we often are just interesting in the last one.\n- **LSTM**: The problem with RNN is that at the end of the review, the information stacked in the begining is forgotten the more information comes up. With LSTM layers, we don't have this effect. The information is stored and can be used anytime without any vanishing problem.","d591094b":"Let's start with a model that doesn't use a RNN layer:","1be49a8e":"Note that since we are using the *print* function, the data is well structured and readle. However, that's not what the data looks like. The actual data is just one big string containing characters such as letters, punctiation but also '\\n' to go back to a new line:","9431aa79":"## Generate text sequences","1c049a34":"We can see here many important things:\n- text feature is in type *dtype=tf.int64*. This means that reviews have already been tokenised (I'll explain this later). If we want to read the reviews, we'll need to decode them. The way it is encoded is specified with the variable *encoder*\n- there are 8185 different words used in these reviews (we might want to just use the most commons, not all of them)\n- there are two classes in the label feature, this is a binary classification problem (good or bad review).","b109c504":"Let's decode it:","bc86601b":"Now that we have the text, let's see how it looks:","9bb6fb29":"Let's analyze the shape a bit:\n- We are getting 64 results. Expected since we used a batch containing 64 sequences;\n- 100 because the model predicted 100 characters per sequence.\n- aaaaand 65.\nFor now, the 65 correspond to the 'probability' of getting each character (for each character of each sequence in this batch, there is an array containg a 'prediction' for unique character in order to guess the following character) hence the 65.\nThis is an expected result since we haven't specified the activation function in the last *dense* layer of the model.\n\nwhy 'probability' because it is not yet mathematically a probability. We have to use a logit function to turn it to probability.\n\nTo get the following character, we just have to randomply pick a character following the distribution given by the model.","2e7e78df":"## Let's try our model\njust to see if it behaves as expected","9f1b1b62":"*tf.random.categorical(logits, num_samples)*:\n- *logits* is a 2D float tensor with shape [batch_size, number_of_classes]. Each rows fo the logit tensor (logit[0,:], logit[1,:] ...) represents the event probabilities of a different categorical distribution. The function doesn't expect real probalities, just unnormalized log-probabilites (exactly what we get from the dense layer at the end of our model)\n- *num_samples* is just the number of samples we want to select. Here, the letters with the highest probabilities will comes out more frequently.","f5b9edb4":"## Modeling\nTime to see the other RNN layer: *GRU*. I don't really know a lot about this one. Let's consider that it is a king of LSTM.","11fa2afd":"so yeaaaaaa, we might want to train it a bit. But hey, seems to work !\n## Training the model\nOne very important thing to do here is to select the right loss function. Since letters are encoced as numbers and then vectors,predictions can be compared to the targets. Thanks to the Embedding when the model predicts a letter closely related to another, the loss won't be that high. As a result, we will use the basic sparse_categorical_crossentropy (since the problem is categorical).","a3f65b2e":"This notebook is dedicated to those interested in taking the TensorFlow certification (just like me) but not really interested in enrolling in the [TensorFlow in Practice Specialization](https:\/\/www.coursera.org\/professional-certificates\/tensorflow-in-practice). \n\nThis kernel also adresses those who are already into Deep Learning for a long time as a useful as a reminder.\n\nEverything one needs to know to take the exam (and have a chance to pass it) is specified in the [TensorFlow Developper Certificate](https:\/\/www.tensorflow.org\/site-assets\/downloads\/marketing\/cert\/TF_Certificate_Candidate_Handbook.pdf?hl=fr). More useful ressources will be accessible through the notebook.\n\nTake the time to read it, there are a lot of information that might be useful !\n\nAs result, what will be presented in this notebook is the followings:\n- Build natural language processing systems using TensorFlow;\n- Prepare text to use it in TensorFlow models;\n- Build models that identify the category of a piece of text using binary categorization;\n- Build models that identify the category of a piece of text using multi-class categorization;\n- Use word embeddings in your TensorFlow model;\n- Use LSTMs in your model to classify text either for binary or multi-class categorization;\n- Add RNN and GRU layers to your model;\n- Use RNNs, LSTMs, GRUs and CNNs in models that work with text;\n- Train LSTMs on existing text to generate tet (such as songs and poetry).\n\nAll these points will be seen through two major sections:\n- 1\/ Text classification\n- 2\/ Text generation\n\n**Let's get started !**\n\nPS: If you learn something while reading this kernel, do not hesitate to leave an upvote ! (this motivates me in creating more content !)","97a98560":"Another optional (but still important thing to do) is to configure the callbacks. You might already be familiar with those (you can use them to make the learning_rate decreasing or stops your model when it stops learning). Here we will use one checkpoint to save the weights after the training (because the training is VERY long, especially if you are not using a GPU)","b31928b8":"For the optimize, we'll use a basic *adam*","a823817e":"Why is it built like this ? Well, you know about the Embedding layer, then we have to use a RNN layer more complex than the SimpleRNN to avoid gradient vanishing and then since it is a categorical classification problmen (we want to guess a letter given len(vocab_size) characters)","12b956b7":"Now you have to see this powerful tool...the *map()* fucntion. You have to specifies the function you want as argument and...nothing more, the job is done !\n\nA new dataset is created from the sequences one by splitting each element as an input_text and a target_text.","bf257479":"And with letters we get:","404038bc":"## Preparing the data\nSince we want our model to generate text, we need to train it to predict the character following a given sequence. Since we will be using a RNN model that keeps track of the past characters it sees at each new sequence, we will make it train on sequence of 99 characters. At each character, the model will try to predict the next one until it reaches the 99th and tries to predict the 100th. It will then move to the next sequence and do the same.\n\nAs a result, the next thing we have to do is to split the data into len(text_as_int)\/\/(seq_length+1) parts and put it into a TensorFow Dataset.","b6c954c4":"### Creating a TF Dataset","d1f2cf7d":"# Text Generation\nSince this is a quite complicated subject, I'll dedicate a whole part to it. The whole structure of the code comes right from the official TensorFlow website ([Text generation](https:\/\/www.tensorflow.org\/tutorials\/text\/text_generation)) but since I needed time to understand what was going on, I though it could be helpful for others if I added some comments.\n\nFirst thing first: Here, more than anywhere you need to understand TensorFlow Datasets. I'll try to explain as must as I can but go check the documentation if you have doubts: [tf.data.Dataset](www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset).","8b456359":"The *from_tensor_slices* method is used to create a TF dataset easily from a list or an array. Each element of the array will be an element of the TF dataset.","dac6d8ca":"### Creating the input and the target\nRemember, we want the model to predict the next character after a sequence of 99 letters while training on predicting each letter of the sequence. As the result, the target just have to be the same sequence, but shifted from one character to the right.","c5856207":"Words, right now, are just encoded as random numbers (thanks to the encoder we used previously). This means a tokenisation has already been made before we loaded the data. (A token represents the way the sentences are splitted. A token can be a character, a word or a sequence of words. Here the tokens are words). However, we cannot just put the data like this inside our model (well, we could, but there's more intelligent we can do before). \n\n\nWhat we want to have is words encoded as tensors (vectors of floating numbers) and words with approximately the same meaning should be relatively close vectors (you can figure it out by drawing tensors in a geomtrical space, vectors with close angles between them represents words with the same meaning). This will be done in the first layer of our model, thanks to the *Embedding* layer. The particularity of the embedding is that these vectors are traininable, just like weights during backpropagation of the neural network.\n\nI highly suggest you to check [this page](https:\/\/www.tensorflow.org\/tutorials\/text\/word_embeddings) from the official tensorflow website to have more information about the Embedding. And check [here](https:\/\/keras.io\/api\/layers\/core_layers\/embedding\/) for the arguments passed in this layer.","07cf3f7a":"# Prepare text to use it in TensorFlow models","8daabf98":"### Batching the data to sequences\nThe problem here is that we have a dataset containg only individual numbers, however, we want sequences of len seq_length. One way to concatenate these characters is to batch them. We'll make packs of individual characters to create sequences.\n\nTo do this we use the *batch(batch_length, drop_remainder=False)* method. Here we specify that drop_remainder=False to only get sequence of the same length. The last sequence that is tinier is not considered.","25c2f74d":"## Import the relevant libraries","07926c5d":"So, what is a token and tokenisation ? A token is the most basic element of your sentence that will be encoded as number. It can be word, it can characters or a sequence of words. Here the tokenisation is made mostly on words but also on certain characters.\n\nRight now you might be \"Well, I've dowloaded the data, I know it is encoded as numbers but how can I see an example of a review ?? Where can I see the label of the review ??\" Firstly, let's make the data ready to be fed into the model:","8b519cb4":"## Load the data\nEver wanted to write like Shakespeare ? This is your chance since we'll load the Shakespeare dataset ! (You can actually use any kind of text you want)","3b21ecf8":"This might looks already intimidating, let me explain what've done here. (One very important part of the TensorFlow certification is to understand the input data, how it is structured, what it represents. As a result, we'll spend time on it).\n\nFirst we want to load the data into a training and a testing set, hence the tupple *(train_data, test_data)*. To do so, we need to specify the kind of split we want *split=(tfds.Split.TRAIN, tfds.Split.TEST)*.\n\nAbout the *info* variable, this is a very important thing to keep in mind. If you feel lost with your dataset, just display what's inside the info variable. For instance here we can see that we could've splitted the data with *unsupervised* (but not usefull here).","b36cfc32":"To make it clearer: The model will be given the first tf.Tensor as input. Since we are using an RNN that loops through every timestep of the sequence, we will give it the second tf.Tensor as target. For instance here, the model will be given the number corresponding to the letter 'F' and will try to predict the next letter \"i\". It will then be given \"i\" to predict the \"r\" etc. etc. etc. The loss of the sequence is then calculated by adding the loss corresponding to each prediction.","ee5d763b":"What *padded_batch* does ? The problem with this dataset is that reviews doesn't have the same lenght. As a result, we'll make all the reviews as long as the longest one by filling them with zeros. Once this is done, we can create batches of data. This means that, here, data will be feed to the model by packs of 32 reviews of the same lenght.\n\nWhat is super important to understand is that TensorFlow datasets are composed of batches of data. To access one batch you either have to loop on batches (be careful to use *break*):","422d276a":"Nice ! We got a dataset where each eleemnt is a tupple containing two batches of 64 sequences of 100 letters (numbers actually).\n\nReady to go, let's build the model !","da28c6b1":"As you can see, one element of the dataset is not directly the number. It is a tf.Tensor(element_value, element_shape, element_type). To access only the value of one specific element you need to create an iterator, that goes to the dataset and returns the value of the elements. Since we are using numbers here, you have to use the *as_numpy_iterator* method:","6f8237ee":"Let's see how many different characters are used in all the text:","54887418":"Here we created a variable called *encoder*. This is this encoded that encoded all the reviws. Now we can use it to encode every string we want:","e8029125":"Here I'll use GPU to get faster result. This is optional. If you are not familiar with this, just skip the next part of code. Also delete the line *with strategy.scope():* before the creation of the model.","9cc753f9":"That's it for this kernel, thanks for reading it to this point. I really hope it has been usefull to you ! Next one will be focused on time series forecasting so stay tuned !","ca79a893":"As you guessed it, we have to transform this string a lot before feeding it to the model. The first thing we want to do is to tokenize the text (we want numbers and not characters). Here we will encode each character as a number (at the end of the day the string must be a nice looking array containing numbers ranging from 0 to 64).\n\nWe'll create 2 lookup tables: one to transform characters to numbers, and one that goes reverse. The first one will be a dictionnary while the second one a numpy array.","07c44031":"# Notebook for the TensorFlow certification: Natural language processing (NLP)","ca77ee6d":"## Load the data\nIn this section we will be using the well known \"IMBD dataset\" that contains movie reviews. With this dataset, we'll try (and we will) classify the reviews from people that liked or disliked the movie.","fd9928d3":"And finally, let's read one review:","8b5ef488":"# Part1: RNN for text classification\n\n## Import the relevant libraries","0f405f8f":"First step sucessfully done ! Let's check it:","c9e0eda1":"As you can see it here, we are getting some nice results ! It doesn't really makes sense but this is quite funny to read. You can play with the *temperature* to get more surprising texts !"}}