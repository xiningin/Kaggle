{"cell_type":{"4e2059b8":"code","b008a821":"code","2bdd8980":"code","b87fc153":"code","983aec50":"code","fa0b0642":"code","a3f086a9":"code","3487613f":"code","73892925":"code","c6b48eee":"code","92692f9a":"code","9b90cf63":"code","4d2409b4":"code","ef830f89":"code","4c46dbcd":"code","7e611e3c":"code","0bde04a3":"code","ced40b7c":"code","89bc93f2":"code","10d07666":"code","52a661d2":"code","2b8778ef":"code","1969be66":"code","8991d36c":"code","6b5aaf0a":"code","466bb388":"code","d6c7242e":"code","bb513eed":"code","3b34a382":"code","be1f921b":"code","960a64ba":"code","ed9350d4":"code","8d0260fe":"code","ba51c565":"code","eca1f0b6":"code","897cb728":"code","9aea591d":"code","d3f9fb2f":"markdown","d0baa87d":"markdown","3de5d01e":"markdown","662d6b94":"markdown","38501238":"markdown","a431e078":"markdown","c71fd539":"markdown","717f9c6b":"markdown","1b8dd640":"markdown","f6705784":"markdown","7ba4f4b2":"markdown"},"source":{"4e2059b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b008a821":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom xgboost import XGBClassifier, plot_importance\nfrom lightgbm import LGBMClassifier\n\nimport optuna","2bdd8980":"%%time\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')","b87fc153":"train","983aec50":"test","fa0b0642":"train.target.value_counts()","a3f086a9":"enc = {'Class_1':0,'Class_2':1,'Class_3':2,'Class_4':3}\ntrain.target.replace(enc,inplace=True)\ntrain.head(5)","3487613f":"train.drop(columns='id',inplace=True)\ntest.drop(columns='id',inplace=True)","73892925":"train.info()","c6b48eee":"test.info()","92692f9a":"%%time\n\nig, ax = plt.subplots(figsize=(20 , 20))\n\ncorr_mat = train.corr()\n\nmask = np.zeros_like(corr_mat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\n\nsns.heatmap(corr_mat,\n        square=True, center=0, linewidth=0.2,\n        cmap='inferno',\n        mask=mask, ax=ax) \n\nax.set_title('Correlation Matrix')\nplt.show()","9b90cf63":"fig, ax = plt.subplots()\nplt.grid()\nsns.countplot(x='target', data=train, ax=ax)\nax.set_ylim(0, 60000)\nax.set_title('Target Distribution')\n\nplt.show()","4d2409b4":"train.describe().T","ef830f89":"plt.figure(figsize=(20,8))\nplt.grid()\nplt.xticks(rotation=90)\nplt.stem(train.drop(columns='target').columns, \n         train.drop(columns='target').describe().T['mean']-test.describe().T['mean'])","4c46dbcd":"#train.drop(columns='feature_19',inplace=True)\n#test.drop(columns='feature_19',inplace=True)","7e611e3c":"cols = train.columns\n\nn_col = 2\nn_row = round(len(cols)\/2)\nsize = (n_col * 10, n_row * 4.5)\n\n#Create figure\nplt.subplots(n_row,n_col,figsize=size)\n\nfor  i ,feature  in enumerate(cols , 1):\n    plt.subplot(n_row, n_col , i)\n    sns.countplot(x = feature, data = train)\n    plt.xlabel(feature, fontsize=9); plt.legend()\nplt.show()","0bde04a3":"#Create figure\nplt.subplots(n_row,n_col,figsize=size)\n\nfor  i ,feature  in enumerate(cols , 1):\n    plt.subplot(n_row, n_col , i)\n    sns.boxenplot(x = feature, data = train)\n    plt.xlabel(feature, fontsize=9); plt.legend()\nplt.show()","ced40b7c":"sns.violinplot(x = 'feature_13', y='target', data = train)","89bc93f2":"import plotly.io as plt_io\nimport plotly.graph_objects as go\n%matplotlib inline\n\nfrom sklearn.manifold import TSNE\nimport umap\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA","10d07666":"x = train.drop(columns='target').to_numpy()\ny = train['target'].to_numpy()","52a661d2":"reducer = umap.UMAP(random_state=42,n_components=3)\nembedding = reducer.fit_transform(x)","2b8778ef":"plt.figure(figsize=(10,10))\nsns.scatterplot(x = reducer.embedding_[:, 0], y = reducer.embedding_[:, 1], \n                hue='target', data=train)","1969be66":"model = XGBClassifier(tree_method = 'gpu_hist' ,\n                      use_label_encoder=False)\nmodel.fit(train.drop(columns='target'),train.target)","8991d36c":"fig, ax = plt.subplots(figsize=(10,10))\nplot_importance(model,\n                height=0.5,\n               max_num_features=None,\n               title='Feature importance',\n                xlabel='F score', \n                ylabel='Features',\n               ax=ax)","6b5aaf0a":"train.shape, test.shape","466bb388":"%%time\n\nX = train.drop([\"target\"],axis=1).to_numpy()\ny = train[\"target\"].to_numpy()","d6c7242e":"from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.metrics import log_loss","bb513eed":"def objective(trial):\n    X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.3,random_state=42)    \n    params = {\n        'eval_metric' : 'auc',\n        'booster' : 'gbtree',\n        'tree_method' : 'gpu_hist' , \n        'use_label_encoder' : False , \n        'eval_metric': 'mlogloss',\n        'lambda' : trial.suggest_loguniform('lambda' , 1e-5 , 1.0),  #L2 regularisation\n        #'alpha' : trial.suggest_loguniform('alpha' , 1e-5 , 1.0),   #L1 regularisation\n        'colsample_bytree' : 0.2,\n        'subsample' : 0.8,\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.001 , 0.01),\n        'n_estimators' : trial.suggest_int('n_estimators' , 5000 , 30000),\n        'max_depth' : trial.suggest_int('max_depth' , 3 , 25),\n        'random_state' : 42,\n        'min_child_weight' : trial.suggest_int('min_child_weight' , 1 , 300),\n        'gamma' : trial.suggest_loguniform('gamma' , 1e-5 , 1.0)\n    }\n    \n    model = XGBClassifier(**params)\n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 200, verbose = False)\n    y_pred = model.predict_proba(X_val)\n        \n    return log_loss(y_val, y_pred)","3b34a382":"study = optuna.create_study(direction='minimize', study_name = 'xgbclassifier') \nstudy.optimize(objective, n_trials=30)","be1f921b":"h=9","960a64ba":"print('XGBoost Score')\nprint()\nprint('Best trial params: ' , study.best_trial.params)\nprint()\nprint('Best CV: ' , study.best_value)","ed9350d4":"preds = np.zeros((X_test.shape[0],4))\nkf = StratifiedKFold(n_splits = 5 , random_state = 42 , shuffle = True)\nbest_params = study.best_trial.params\nmodel = XGBClassifier(**best_params)\nll =[]\nn=0\n\nfor tr_idx, test_idx in kf.split(X, y):\n    \n    X_tr, X_val = X[tr_idx], X[test_idx]\n    y_tr, y_val = y[tr_idx], y[test_idx]\n    \nmodel.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    \npreds += model.predict_proba(X_test)\/kf.n_splits\nll.append(log_loss(y_val, model.predict_proba(X_val)))\nprint(n+1,ll[n])","8d0260fe":"j=0","ba51c565":"xgb_submission = pd.DataFrame(pred_xgb,columns=['Class_1','Class_2','Class_3','Class_4'])\nxgb_submission['id']  = test['id']\nxgb_submission = xgb_submission[['id','Class_1','Class_2','Class_3','Class_4']]","eca1f0b6":"xgb_submission","897cb728":"output = xgb_submission.to_csv('submission_xgboost',index=False)","9aea591d":"submission = pd.DataFrame( test_pred)\nsubmission.columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\nsubmission['id'] = test['id']\nsubmission = submission[['id', 'Class_1', 'Class_2', 'Class_3', 'Class_4']]\n\nsubmission.to_csv(\"submission3.csv\", index=False)","d3f9fb2f":"# Boxenplots or Letter Value Plots","d0baa87d":"# Studying feature wise difference in distribution ","3de5d01e":"# Distribution Study using Countplots","662d6b94":"# Violin Plots","38501238":"# Model Preparation","a431e078":"# Feature Importance using XGBoost default Parameters","c71fd539":"# Submission","717f9c6b":"# Visualisation in Reduced Dimension using UMAP, LDA & t-SNE ","1b8dd640":"**UMAP**","f6705784":"Conventional boxplots are useful displays for conveying rough information about the central 50% and the extent of data. For small-sized data sets (n < 200), detailed estimates of tail behavior beyond the quartiles may not be trustworthy. Larger data sets (n ~ 10,000-100,000) afford more precise estimates of quantiles beyond the quartiles. Boxenplot (or letter value plot) conveys more detailed information in the tails using letter values, but only to the depths where the letter values are reliable estimates of their corresponding quantiles.","7ba4f4b2":"# Best Parameters"}}