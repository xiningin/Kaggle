{"cell_type":{"6c52b62f":"code","86854eab":"code","bede7bdd":"code","c4702d5b":"code","57b909ca":"code","d6daa0d3":"code","cbf0e762":"code","f639d6a5":"code","65486590":"code","b33ea54f":"code","ec0a5253":"code","514f3787":"code","d051e45c":"code","ceaad355":"code","bda078a7":"code","0ce82178":"markdown","ed11492c":"markdown","aa74f95d":"markdown","9ad0664f":"markdown"},"source":{"6c52b62f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","86854eab":"import gensim\nfrom tqdm import tqdm_notebook as tqdm","bede7bdd":"def get_coefs(word, *arr):\n    return word, np.asarray(arr[:-1], dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.split(' ')) for line in f)","c4702d5b":"CRAWL_EMBEDDING_PATH = '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'","57b909ca":"%%time\ncrawl_emb_dict = load_embeddings(CRAWL_EMBEDDING_PATH)","d6daa0d3":"len(crawl_emb_dict)","cbf0e762":"crawl_emb_dict['kaggle'].shape","f639d6a5":"crawl_emb_dict['kaggle']","65486590":"def save_word2vec_format(fname, vocab, vector_size, binary=True):\n    \"\"\"Store the input-hidden weight matrix in the same format used by the original\n    C word2vec-tool, for compatibility.\n\n    Parameters\n    ----------\n    fname : str\n        The file path used to save the vectors in.\n    vocab : dict\n        The vocabulary of words.\n    vector_size : int\n        The number of dimensions of word vectors.\n    binary : bool, optional\n        If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n\n\n    \"\"\"\n    \n    total_vec = len(vocab)\n    with gensim.utils.smart_open(fname, 'wb') as fout:\n        print(total_vec, vector_size)\n        fout.write(gensim.utils.to_utf8(\"%s %s\\n\" % (total_vec, vector_size)))\n        # store in sorted order: most frequent words at the top\n        for word, row in tqdm(vocab.items()):\n            if binary:\n                row = row.astype(np.float32)\n                fout.write(gensim.utils.to_utf8(word) + b\" \" + row.tostring())\n            else:\n                fout.write(gensim.utils.to_utf8(\"%s %s\\n\" % (word, ' '.join(repr(val) for val in row))))","b33ea54f":"with open('dummy_text_to_prevent_kernel_page_loding_from_being_heavy.txt', 'w') as f:\n    f.write(':3')\n    ","ec0a5253":"%%time\nsave_word2vec_format(binary=True, fname='crawl-300d-2M.bin', vocab=crawl_emb_dict, vector_size=300)","514f3787":"with open('dummy_text_to_prevent_kernel_page_loding_from_being_heavy2.txt', 'w') as f:\n    f.write(':9')","d051e45c":"%%time\nmodel = gensim.models.KeyedVectors.load_word2vec_format('crawl-300d-2M.bin', binary=True)","ceaad355":"model['kaggle']","bda078a7":"%%time\nmodel.most_similar('kaggle')","0ce82178":"# Load it again","ed11492c":"# Load embedding in a usual way","aa74f95d":"# Save it in gensim w2v binary format","9ad0664f":"This kernel converts a Python dict of word embedding vectors into gensim's word2vec binary format. \n\nThis leads to faster loading and you can do pretty stuffs with gensim.\n\nSource:\n[scikit learn \\- Convert Python dictionary to Word2Vec object \\- Stack Overflow](https:\/\/stackoverflow.com\/questions\/45981305\/convert-python-dictionary-to-word2vec-object)\n"}}