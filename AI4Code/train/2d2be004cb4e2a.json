{"cell_type":{"1b247fd4":"code","fdf188af":"code","ac62db51":"code","22e16385":"code","b7939871":"code","8267b627":"code","41fccabe":"code","2acff5f4":"code","edc51dc8":"code","4c534320":"code","2ebf4803":"code","23b477fa":"code","f854312e":"code","26f52874":"code","50f297b7":"code","b7f344ed":"code","5fea1093":"code","fe697ba8":"code","bafe5642":"code","88d1308c":"code","c035cee7":"code","791ab320":"code","2b2a15b4":"code","5851d40f":"code","74306a54":"markdown"},"source":{"1b247fd4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Standard plotly imports\n#import plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n#import cufflinks\n#import cufflinks as cf\nimport plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\n#cufflinks.go_offline(connected=True)\n\n# Preprocessing, modelling and evaluating\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n## Hyperopt modules\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\n\nimport os\nimport gc\nprint(os.listdir(\"..\/input\"))","fdf188af":"df_train = pd.read_csv(\"..\/input\/trainsample_SEG5_V1.csv\")\ndf_train.head()","ac62db51":"df_train.info()","22e16385":"df_train = df_train.drop(['event1'],axis=1)","b7939871":"plt.figure(figsize=(12,5))\nsns.countplot(x=df_train['event'], data=df, order=[1,0])","8267b627":"df_train.columns","41fccabe":"categorical_var=['event','TOP_PERSO', 'TOP_IMMO', 'TOP_CAV',\n       'TOP_GSM', 'TOP_ASSV', 'TOP_CB_VISA', 'TOP_CB_BUSI', 'TOP_CB_PREMIER',\n       'TOP_CB_INFI', 'TOP_CB_MASTER', 'TOP_CONV_M_1', 'top_clot_cav12',\n       'top_clot_pea12', 'top_clot_IMMO12', 'top_clot_PERSO12', 'top_staff',\n       'TOP_PEA', 'TOP_TITRE', 'TOP_INV', 'top_cars', 'top_carte',\n       'TOP_PAssport', 'Contact_3mois', 'Contact_6mois', 'Contact_9mois',\n       'Contact_12mois']\ncategorical_var #List of categorical variable","2acff5f4":"#Function to plot Graph of each attribute vs target value\ndef PlotAttributeVsTarget(df,target):\n    for y in df.columns:\n        if y!= target:\n            plt.figure(figsize=(12,5))\n            #sns.scatterplot(x=df[y],y=df[target])\n            sns.countplot(x=df[target], hue=df[y], data=df, order=[1,0])\n    \nPlotAttributeVsTarget(df_train_cat,'event')","edc51dc8":"x = df_train.drop(['event'],axis=1)\ny = df_train['event']","4c534320":"from imblearn.over_sampling import SMOTE\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y==0)))\n\nsm = SMOTE(random_state=2)\nX_res, y_res = sm.fit_sample(x, y.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_res==0)))","2ebf4803":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=0)\n","23b477fa":"X_train.columns()","f854312e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","26f52874":"y_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","50f297b7":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","b7f344ed":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","5fea1093":"xg = xgb.XGBClassifier(colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 100)","fe697ba8":"xg.fit(X_train,y_train)\ny_pred = xg.predict(X_test)\nprint('Accuracy of xgb classifier on test set: {:.2f}'.format(xg.score(X_test, y_test)))","bafe5642":"xgb.plot_importance(xg)\nplt.figure(figsize=(12,5))\nplt.show()\n","88d1308c":"f_imp = xg.get_booster().get_score(importance_type='gain')\nf_imp\n","c035cee7":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)\nprint(classification_report(y_test, y_pred))","791ab320":"from numpy import sort\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score\nthresholds = sort(xg.feature_importances_)\nfor thresh in thresholds:\n    # select features using threshold\n    selection = SelectFromModel(xg, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train)\n    # train model\n    selection_model = xgb.XGBClassifier(colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 100)\n    selection_model.fit(select_X_train, y_train)\n    # eval model\n    select_X_test = selection.transform(X_test)\n    y_pred = selection_model.predict(select_X_test)\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))\n    #confusion_matrix = confusion_matrix(y_test, y_pred)\n    #print(classification_report(y_test, y_pred))\n    #print(\"F1 Score: {}\".format(f1_score(y_true,y_pred)))","2b2a15b4":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1,estimator2, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    return plt","5851d40f":"\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(xg,logreg, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)","74306a54":"1. No missing values\n2. Removed Event1 variable "}}