{"cell_type":{"1e7281c5":"code","76eba37b":"code","46e05fa5":"code","45959d61":"code","520eb0cd":"code","28852192":"code","527fe1e2":"code","a963d656":"code","d6540453":"code","c774f85b":"code","be7bb3d2":"code","a57c5339":"code","2430b364":"markdown","c6b68243":"markdown","49558c92":"markdown","2b0634d0":"markdown","cec42f76":"markdown","848106ce":"markdown","e9fb9c38":"markdown","a6f50a14":"markdown","1f9cbfce":"markdown","ac970e3c":"markdown","c7ee76a6":"markdown","a4d7bce2":"markdown","71ebb026":"markdown","ccd6d636":"markdown","ba158eb3":"markdown","a42aa136":"markdown","c54f7c46":"markdown"},"source":{"1e7281c5":"import scipy as sp\nimport numpy as np\n\nfrom collections import Counter\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\n\n# FROM: https:\/\/www.kaggle.com\/myltykritik\/simple-lgbm-image-features\n\n# The following 3 functions have been taken from Ben Hamner's github repository\n# https:\/\/github.com\/benhamner\/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              \/ num_scored_items)\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)","76eba37b":"import pandas as pd\n\ntrain = pd.read_csv('..\/input\/petfinder-adoption-prediction\/train\/train.csv')\ntest = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/test.csv')\nX = pd.concat([train, test], ignore_index=True, sort=False)\nX.head()","46e05fa5":"# taken from https:\/\/www.kaggle.com\/ranjoranjan\/single-xgboost-model\nclass OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return -cohen_kappa_score(y, preds, weights='quadratic')\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']","45959d61":"X_temp = X.copy()\n\nto_drop_columns = ['PetID', 'Name', 'RescuerID', 'Description']\nX_temp = X_temp.drop(to_drop_columns, axis=1)","520eb0cd":"X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_test.drop(['AdoptionSpeed'], axis=1)\nX_train_non_null = X_train.fillna(-1)\nX_test_non_null = X_test.fillna(-1)","28852192":"import xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\n\nxgb_params = {\n    'eval_metric': 'rmse',\n    'seed': 1337,\n    'eta': 0.0123,\n    'subsample': 0.8,\n    'colsample_bytree': 0.85,\n    'tree_method': 'gpu_hist',\n    'device': 'gpu',\n    'silent': 1,\n}","527fe1e2":"def run_xgb(params, X_train, X_test):\n    n_splits = 10\n    verbose_eval = 1000\n    num_rounds = 60000\n    early_stop = 500\n\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1337)\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n\n    i = 0\n    for train_idx, valid_idx in kf.split(X_train, X_train['AdoptionSpeed'].values):\n\n        X_tr = X_train.iloc[train_idx, :]\n        X_val = X_train.iloc[valid_idx, :]\n\n        y_tr = X_tr['AdoptionSpeed'].values\n        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n\n        y_val = X_val['AdoptionSpeed'].values\n        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n\n        d_train = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n        d_valid = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n\n        valid_pred = model.predict(xgb.DMatrix(X_val, feature_names=X_val.columns), ntree_limit=model.best_ntree_limit)\n        test_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model.best_ntree_limit)\n\n        oof_train[valid_idx] = valid_pred\n        oof_test[:, i] = test_pred\n\n        i += 1\n    return model, oof_train, oof_test","a963d656":"model, oof_train, oof_test = run_xgb(xgb_params, X_train_non_null, X_test_non_null)","d6540453":"import seaborn as sns\n\ndef plot_pred(pred):\n    sns.distplot(pred, kde=True, hist_kws={'range': [0, 5]})\n    \nplot_pred(oof_train)\nplot_pred(oof_test.mean(axis=1))","c774f85b":"optR = OptimizedRounder()\noptR.fit(oof_train, X_train['AdoptionSpeed'].values)\ncoefficients = optR.coefficients()\nvalid_pred = optR.predict(oof_train, coefficients)\nqwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, valid_pred)\nprint(\"QWK = \", qwk)","be7bb3d2":"xgb.plot_importance(model)","a57c5339":"test_predictions = optR.predict(oof_test.mean(axis=1), coefficients).astype(np.int8)\nsubmission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","2430b364":"# Training a Model\nThese are the XGBoost parameters, check out [the documentation for more info](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html):","c6b68243":"Let's compute the Quadratic Weighted Kappa this gets us. Remember that this score is based on our test set. **It won't be the same as what appears on the leaderboard**, as the leadboard uses a secret validation set. It will also be different from the final competition score, which will be calculated based on yet another secret validation set.","49558c92":"We train the model using stratified k-fold cross-validation:","2b0634d0":"**Ok! this is great, we have built features, fit a regression model, and optimized the cutoff values.** Now, we want to add more features to hopefully improve our model.\n\n`# WIP`","cec42f76":"Let's see which features ended up being the **most importants** in this model:","848106ce":"The files `breed_labels.csv`, `color_labels.csv` and `state_labels.csv` are dictionaries mapping respectively breeds, colors and places to their full names. Additionally, **sentiment data** from Google's Natural Language API and **image metadata** from Google's Vision API are provided in the `*_sentiment.zip` and `*_metadata.zip` files respectively.\n\nWe will not do any EDA here, many good kernels can give you the information you need, eg:\n* https:\/\/www.kaggle.com\/artgor\/exploration-of-data-step-by-step\n* https:\/\/www.kaggle.com\/erikbruin\/petfinder-my-detailed-eda-and-xgboost-baseline\n* https:\/\/www.kaggle.com\/jaseziv83\/an-extensive-eda-of-petfinder-my-data\n\n**You are also allowed to use external data sources.** All the allowed sources are listed in [the Official External Data Disclosure Thread](https:\/\/www.kaggle.com\/c\/petfinder-adoption-prediction\/discussion\/75943). It is now to late to ask to use anything else.","e9fb9c38":"# Simplest Model\nWe start by making a copy of the original data so we leave the original `X` untouched. We drop all text columns for now. ","a6f50a14":"We split back the data into train and test sets, and remove `AdoptionSpeed` from the test set.","1f9cbfce":"We can now write the `submission.csv` file to submit it to the challenge.","ac970e3c":"Done! Let's compare the distribution of the train and test sets:","c7ee76a6":"You will then optimize the coefficients with:\n\n```\noptR = OptimizedRounder()\noptR.fit(regression_predictions, original_labels)\nprint(optR.coefficients)\n```","a4d7bce2":"# How are submissions compared?\n\n> Submissions are scored based on the **quadratic weighted kappa**, which measures the agreement between two ratings. This metric typically varies from 0 (random agreement between raters) to 1 (complete agreement between raters). In the event that there is less agreement between the raters than expected by chance, the metric may go below 0. The quadratic weighted kappa is calculated between the scores which are expected\/known and the predicted scores.\n\n[You can read more on Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Cohen%27s_kappa#Weighted_kappa) if you want, but you don't need to. Here's the implementation most kernels use:\n","71ebb026":"# Only a few days left to go! \ud83d\ude31\n**Just joining this challenge? You're at the right place.**\n\nThe goal is to predict ***pet adoptability***, ie given the profile of a cat or dog in a shelter, guess *how fast* he\/she will get adopted by a caring family.\n\nThe \"adoption speed\" has 5 possible values, between 0 and 4. From the [official data description](https:\/\/www.kaggle.com\/c\/petfinder-adoption-prediction\/data):\n\n> The value is determined by how quickly, if at all, a pet is adopted. The values are determined in the following way: \n* 0 - Pet was adopted on the same day as it was listed. \n* 1 - Pet was adopted between 1 and 7 days (1st week) after being listed. \n* 2 - Pet was adopted between 8 and 30 days (1st month) after being listed. \n* 3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed. \n* 4 - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days).\n\nPretty straightforward.","ccd6d636":"# Let's build some features!\n\nOk, we now have a basic understanding of the problem, let's extract some features and actually train a model. Most of the code below is taken from [the *single-xgboost-model* notebook](https:\/\/www.kaggle.com\/ranjoranjan\/single-xgboost-model\/notebook), which itself reuses lots of shared code snippets from different public kernels.\n\n**We will first build a simple model using only the basic data from `train.csv` and `test.csv`. Then you should improve it by adding features from the text, images or any other external source.**","ba158eb3":"# So, is this a classification or a regression task?\n\nIt can be seen as both. But participants are overwhelmingly using **regression models** as it provides better results:\n\n> I tried all \"optimal\" methods for classification I found in research articles over the last 20 years and they work less well than regression with cutoff searching on this dataset\u2026 -- [Amit Steinberg](https:\/\/www.kaggle.com\/c\/petfinder-adoption-prediction\/discussion\/81375#481535)\n\nSo let's go with regression. What is ***cutoff searching***? \n\nYou will first extract features from the data, then fit a regression model. Let's say for a given animal your model gives a value of `1.6`.  Should you consider it as part of **class 1**: \"pet was adopted between 1 and 7 days\"? or **class 2**: \"between 8 and 30 days\"?\n\nIt is not clear where the ***cutoffs*** between the classes should be! You want to optimize the cuttoffs so they give you the best **evaluation metric**. In practice, most kernels optimize the cutoff coefficients using the [Nelder-Mead method](https:\/\/en.wikipedia.org\/wiki\/Nelder%E2%80%93Mead_method). You can read more about this, and explore alternative optimization methods, in [Grandmaster Abhishek's post](https:\/\/www.kaggle.com\/c\/petfinder-adoption-prediction\/discussion\/76107).\n\n Here's an example implementation using Nelder-Mead:","a42aa136":"That's it! We're ready to train our first model.","c54f7c46":"# What Data is Available?\n\nThe challenge includes **text, tabular, and image data**. Let's load it to have a look."}}