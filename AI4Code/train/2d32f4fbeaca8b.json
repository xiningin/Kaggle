{"cell_type":{"fed17eb7":"code","659be772":"code","ab33c485":"code","8cca3cd8":"code","5bb64a79":"code","de37e84d":"code","cd567150":"code","f62cb273":"code","ca7d5864":"code","33420c73":"code","ab72d8b4":"code","d3fa80ee":"code","beff6ba0":"code","ee5b1e38":"code","80e0afb1":"code","1b7f6e4b":"code","f997b032":"code","3670dd99":"code","5771383f":"code","09590f27":"code","09134a36":"code","13e1f9ae":"markdown","4368c29a":"markdown","3dfc070b":"markdown","4455dad4":"markdown","7eb01140":"markdown","94a60999":"markdown","e3420f00":"markdown","2f09e5a8":"markdown","c11af50c":"markdown","4f93569a":"markdown","93573990":"markdown","81ddf492":"markdown","a8030252":"markdown","2a497ecf":"markdown","3edfb1e2":"markdown","68bb7319":"markdown","b21f3751":"markdown"},"source":{"fed17eb7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nplt.style.use('fivethirtyeight')","659be772":"# Import pandas\n#import pandas as pd\n#import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,7))\n# Read 'gapminder.csv' into a DataFrame: df\ndf = pd.read_csv('..\/input\/gm2008\/gm_2008_region.csv')\n\n# Print the columns of df\nprint(df.columns)\n\n# Create a boxplot of life expectancy per region\ndf.boxplot('life', 'Region', rot=60,ax=ax)\n\n# Show the plot\nplt.show()\n","ab33c485":"# Print the columns of df_region\nprint(\"Dataframe with Region:\\n \",df.info())\n\nprint(\"Dataframe head containing Region column :\\n\", df['Region'].head(10))\n\n# Create dummy variables: df_region\ndf_region1 = pd.get_dummies(df)\n\nprint(\"Dataframe after creating dummy columns without dropping region :\\n \",df_region1.info())\n\n# Create dummy variables with drop_first=True: df_region\ndf_region2 = pd.get_dummies(df,drop_first=True)\n\n# Print the new columns of df_region\nprint(\"Dataframe Region with Dummy Columns but dropping first dummy column : \\n\",df_region2.info())\n\nprint(\"Dataframe Region columns after dummification step :\\n \", df_region2.iloc[:10,9:11])","8cca3cd8":"from sklearn.datasets import load_boston\nboston = load_boston()\nX = boston.data\nY = boston.target\nprint(\"X Shape : \",X.shape)\nprint(\"Y Shape : \",Y.shape)","5bb64a79":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=1000)","de37e84d":"import numpy as np\nX = np.random.uniform(0.0, 1.0, size=(10, 2))\nY = np.random.choice(('Male','Female'), size=(10))\nprint(\"X : \",X)\nprint(\"Y : \",Y)","cd567150":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nyt = le.fit_transform(Y)\nprint(yt)","f62cb273":"from sklearn.preprocessing import LabelBinarizer\nlb = LabelBinarizer()\nYb = lb.fit_transform(Y)\nprint(\"Yb : \",Yb)\nprint(\"Inverse Transformation : \",lb.inverse_transform(Yb))","ca7d5864":"from sklearn.preprocessing import Imputer\ndata = np.array([[1, np.nan, 2], [2, 3, np.nan], [-1, 4, 2]])\nprint(\"data : \",data)\n# Mean Strategy\nimp = Imputer(strategy='mean')\ntrans_data = imp.fit_transform(data)\nprint(\"Transformed Data using mean strategy : \\n\",trans_data)\n# Median Strategy\nimp = Imputer(strategy='median')\ntrans_data = imp.fit_transform(data)\nprint(\"Transformed Data using median strategy : \\n\",trans_data)\n# Most Frequent\nimp = Imputer(strategy='most_frequent')\ntrans_data = imp.fit_transform(data)\nprint(\"Transformed Data using most frequent strategy : \\n\",trans_data)","33420c73":"# Import pandas\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Read 'gapminder.csv' into a DataFrame: df\ndf = pd.read_csv('..\/input\/housingvotes\/house-votes-84.csv',header=None)\n\n# Convert '?' to NaN\ndf[df == '?'] = np.nan\n\n# Print the number of NaNs\nprint(\"The number of NaNs :\\n\",df.isnull().sum())\n\n# Print shape of original DataFrame\nprint(\"Shape of Original DataFrame: {}\".format(df.shape))\n\n# Drop missing values and print shape of new DataFrame\ndf = df.dropna()\n\n# Print shape of new DataFrame\nprint(\"Shape of DataFrame After Dropping All Rows with Missing Values: {}\".format(df.shape))\n","ab72d8b4":"from pandas import read_csv\nfrom sklearn.preprocessing import Imputer\n#from sklearn.impute import SimpleImputer\nimport numpy as np\ndataset = read_csv('..\/input\/pimaindian\/pima-indians-diabetes.data.csv', header=None)\n# mark zero values as missing or NaN\ndataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, np.NaN)\n# fill missing values with mean column values\nvalues = dataset.values\n\nimputer = Imputer()\n\ntransformed_values = imputer.fit_transform(values)\n\n# count the number of NaN values in each column\nprint(\"NaN values count :- \",np.isnan(transformed_values).sum())\n\n","d3fa80ee":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_wine\n\nss = StandardScaler()\nfeatures, target = load_wine(return_X_y=True)\n\nscaled_data = ss.fit_transform(features)\nprint('Unscaled Data:\\n',features)\nprint(\"Scaled Data :\\n\",scaled_data)","beff6ba0":"from sklearn.preprocessing import Normalizer\nimport numpy as np\n\ndata = np.array([1.0, 2.0])\nn_max = Normalizer(norm='max')\nnorm_data = n_max.fit_transform(data.reshape(1, -1))\nprint(\"Norm Data(max) :\\n \",norm_data)\nn_l1 = Normalizer(norm='l1')\nnorm_data = n_l1.fit_transform(data.reshape(1, -1))\nprint(\"Norm Data(l1) :\\n \",norm_data)\nn_l2 = Normalizer(norm='l2')\nn_l2.fit_transform(data.reshape(1, -1))\nprint(\"Norm Data(l2) :\\n \",norm_data)","ee5b1e38":"from sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.DataFrame({\n    # positive skew\n    'x1': np.random.chisquare(8, 1000),\n    # negative skew \n    'x2': np.random.beta(8, 2, 1000) * 40,\n    # no skew\n    'x3': np.random.normal(50, 3, 1000)\n})\n\nscaler = MinMaxScaler()\nscaled_df = scaler.fit_transform(df)\nscaled_df = pd.DataFrame(scaled_df, columns=['x1', 'x2', 'x3'])\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5))\nax1.set_title('Before Scaling')\nsns.kdeplot(df['x1'], ax=ax1)\nsns.kdeplot(df['x2'], ax=ax1)\nsns.kdeplot(df['x3'], ax=ax1)\nax2.set_title('After Min-Max Scaling')\nsns.kdeplot(scaled_df['x1'], ax=ax2)\nsns.kdeplot(scaled_df['x2'], ax=ax2)\nsns.kdeplot(scaled_df['x3'], ax=ax2)\nplt.show()","80e0afb1":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = pd.DataFrame({\n    # Distribution with lower outliers\n    'x1': np.concatenate([np.random.normal(20, 1, 1000), np.random.normal(1, 1, 25)]),\n    # Distribution with higher outliers\n    'x2': np.concatenate([np.random.normal(30, 1, 1000), np.random.normal(50, 1, 25)]),\n})\n\nscaler = RobustScaler()\nrobust_scaled_df = scaler.fit_transform(x)\nrobust_scaled_df = pd.DataFrame(robust_scaled_df, columns=['x1', 'x2'])\n\nscaler = MinMaxScaler()\nminmax_scaled_df = scaler.fit_transform(x)\nminmax_scaled_df = pd.DataFrame(minmax_scaled_df, columns=['x1', 'x2'])\n\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(12, 5))\n\nax1.set_title('Before Scaling')\nsns.kdeplot(x['x1'], ax=ax1)\nsns.kdeplot(x['x2'], ax=ax1)\n\nax2.set_title('After Robust Scaling')\nsns.kdeplot(robust_scaled_df['x1'], ax=ax2)\nsns.kdeplot(robust_scaled_df['x2'], ax=ax2)\n\nax3.set_title('After Min-Max Scaling')\nsns.kdeplot(minmax_scaled_df['x1'], ax=ax3)\nsns.kdeplot(minmax_scaled_df['x2'], ax=ax3)\nplt.show()","1b7f6e4b":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import Normalizer\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.DataFrame({\n    'x1': np.random.randint(-100, 100, 1000).astype(float),\n    'y1': np.random.randint(-80, 80, 1000).astype(float),\n    'z1': np.random.randint(-150, 150, 1000).astype(float),\n})\n\nscaler = Normalizer()\nscaled_df = scaler.fit_transform(df)\nscaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n\nfig = plt.figure(figsize=(9, 5))\nax1 = fig.add_subplot(121, projection='3d')\nax2 = fig.add_subplot(122, projection='3d')\nax1.scatter(df['x1'], df['y1'], df['z1'])\nax2.scatter(scaled_df['x1'], scaled_df['y1'], scaled_df['z1'])\nplt.show()","f997b032":"#SelectKBest features\nfrom sklearn.datasets import load_boston, load_iris\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, chi2, f_regression\nregr_data = load_boston()\nprint(regr_data.data.shape)\nkb_regr = SelectKBest(f_regression)\nX_b = kb_regr.fit_transform(regr_data.data, regr_data.target)\nprint(X_b.shape)\nprint(kb_regr.scores_)","3670dd99":"class_data = load_iris()\nprint(class_data.data.shape)\nperc_class = SelectPercentile(chi2, percentile=15)\nX_p = perc_class.fit_transform(class_data.data, class_data.target)\nprint(X_p.shape)\nprint(perc_class.scores_)","5771383f":"from sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndigits = load_digits()\n\n# Show some random digits\nselection = np.random.randint(0, 1797, size=100)\n\nfig, ax = plt.subplots(10, 10, figsize=(10, 10))\n\nsamples = [digits.data[x].reshape((8, 8)) for x in selection]\n\nfor i in range(10):\n    for j in range(10):\n        ax[i, j].set_axis_off()\n        ax[i, j].imshow(samples[(i * 8) + j], cmap='gray')\n\nplt.show()\n","09590f27":"\n\npca = PCA(n_components=36, whiten=True)\nX_pca = pca.fit_transform(digits.data \/ 255)\nprint(pca.explained_variance_ratio_)\n\n# Plot the explained variance ratio\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\n\nax[0].set_xlabel('Component')\nax[0].set_ylabel('Variance ratio (%)')\nax[0].bar(np.arange(36), pca.explained_variance_ratio_ * 100.0)\n\nax[1].set_xlabel('Component')\nax[1].set_ylabel('Cumulative variance (%)')\nax[1].bar(np.arange(36), np.cumsum(pca.explained_variance_)[::-1])\n\nplt.show()","09134a36":"X_rebuilt = pca.inverse_transform(X_pca)\n# Rebuild from PCA and show the result\nfig, ax = plt.subplots(10, 10, figsize=(10, 10))\n\nsamples = [pca.inverse_transform(X_pca[x]).reshape((8, 8)) for x in selection]\n\nfor i in range(10):\n    for j in range(10):\n        ax[i, j].set_axis_off()\n        ax[i, j].imshow(samples[(i * 8) + j], cmap='gray')\n\nplt.show()\n","13e1f9ae":"# Dummy Variables\n\nAs we know,  scikit-learn does not accept non-numerical features. You saw in the graph above that the\u00a0'Region'\u00a0feature contains very useful information that can predict life expectancy. \nFor example, Sub-Saharan Africa has a lower life expectancy compared to Europe and Central Asia. Therefore, if you are trying to predict life expectancy,  it would be preferable to retain the\u00a0'Region'\u00a0feature. To do this, you need to binarize it by creating dummy variables, which is what you will do in this exercise.\n","4368c29a":"# Introduction\n\nData preparation is very important for all machine learning problems. Most of the real world datasets have dirty or unclean data, data may also be unsuitable for machine learning techniques that you want to apply. \n\nData may suffer from many defects, including :-\n    a). Missing Data\n    b). Duplications\n    c). Incorrect data types including categorical data\n    d). Outliers\n    e). Inconsistencies in data representation e.g. Rating was \u201c1,2,3\u201d, now rating \u201cA, B, C\u201d\n    \nThe purpose of data preparation is to transform data sets so that their information content is best exposed to solve given problem. \nImportant advantages of preprocessing include :-\n    a). Error prediction rate should be lower (or the same) after the preparation as before it\n    b). Less computation and higher performance is achieved because of preprocessing\n\nPreprocessing Techniques covered in this tutorial are :-\n\n1). Label Encoding\n\n2).  Handling Missing Features with Imputation\n\n3). Scaling and Normalization\n\n4). Dummy Variables for dealing with Categorical Values\n","3dfc070b":"# Handling Missing Features\n\nSometimes a dataset can contain missing features, so there are a few options that can be taken into account:\n\n1). Removing the whole line\n\n2). Creating sub-model to predict those features\n\n3). Using an automatic strategy to input them according to the other known values","4455dad4":"# Feature Selection\n\nTwo examples of feature selection that use the classes SelectKBest (which selects the best K high-score features) and SelectPercentile (which selects only a subset of features\nbelonging to a certain percentile) are shown next. It's possible to apply them both to regression and classification datasets, being careful to select appropriate score functions.","7eb01140":"# Normalizer\n\nThe normalizer scales each value by dividing each value by its magnitude in n-dimensional space for n number of features.\n\n","94a60999":"## Note\n\nPlease share, upvote and comment to help me create and share more content for the community.\nThank you all.","e3420f00":"# Label Encoding\n\nThe first option is to use the LabelEncoder class, which adopts a dictionary-oriented approach, associating to each category label a progressive integer number, that is an index of an instance array called classes_:","2f09e5a8":"# Imputing Missing Data\n\nReal-world data often has missing values.\n\nData can have missing values for a number of reasons such as observations that were not recorded and data corruption.\n\nHandling missing data is important as many machine learning algorithms do not support data with missing values.\n\nAs you've come to appreciate, there are many steps to building a model, from creating training and test sets, to fitting a classifier or regressor, to tuning its parameters, to evaluating its performance on new data. \n\nImputation allows you to specify the value to replace (it can be something other than NaN) and the technique used to replace it (such as mean, median, or mode). The Imputer class operates directly on the NumPy array instead of the DataFrame.\n\nImputation can be seen as the first step of this machine learning process, the entirety of which can be viewed within the context of a pipeline. Scikit-learn provides a pipeline constructor that allows you to piece together these steps into one process and thereby simplify your workflow.\n","c11af50c":"# Principal Component Analysis\n\nIn many cases, the dimensionality of the input dataset X is high and so is the complexity of every related machine learning algorithm. Moreover, the information is seldom spread\nuniformly across all the features and, as discussed in the previous chapter, there will be high entropy features together with low entropy ones, which, of course, don't contribute\ndramatically to the final outcome. \n\nIt's possible to project the original feature vectors into this new (sub-)space, where each component carries a portion of total variance and where the new covariance matrix is\ndecorrelated to reduce useless information sharing (in terms of correlation) among different features. In scikit-learn, there's the PCA class which can do all this in a very smooth way.\n","4f93569a":"# MinMax Scaler\n\nThe MinMaxScaler is the probably the most famous scaling algorithm. It essentially shrinks the range such that the range is now between 0 and 1 (or -1 to 1 if there are negative values).\n\nThis scaler works better for cases in which the standard scaler might not work so well. If the distribution is not Gaussian or the standard deviation is very small, the min-max scaler works better.","93573990":"As expected, the contribution to the total variance decreases dramatically starting from the fifth component, so it's possible to reduce the original dimensionality without an\nunacceptable loss of information, which could drive an algorithm to learn wrong classes. In the preceding graph, there are the same handwritten digits rebuilt using the first 36\ncomponents with whitening and normalization between 0 and 1.","81ddf492":"# Exploring Categorical Features\n\nThe Gapminder dataset also contains a categorical\u00a0'Region'\u00a0feature.  In this exercise we first explore this feature. Boxplots are particularly useful for visualizing categorical features such as this.\n","a8030252":"# Setting Up for Label Encoding\n\nAnother approach is to encode categorical values with a technique called \"label encoding\", which allows you to convert each value in a column to a number. Numerical labels are always between 0 and n_categories-1.\nSometimes, you might just want to encode a bunch of categories within a feature to some numeric value and encode all the other categories to some other numeric value.","2a497ecf":"Each image is a vector of 64 unsigned int (8 bit) numbers (0, 255), so the initial number of components is indeed 64. However, the total amount of black pixels is often predominant and the basic signs needed to write 10 digits are similar, so it's reasonable to assume both high cross-correlation and a low variance on several components. Trying with 36 principal components, we get:","3edfb1e2":"# Dropping Missing Data\n\nThe voting dataset contained a bunch of missing values. Now, it's time for you to take care of these !\n\nThe unprocessed dataset has been loaded into a DataFrame\u00a0df. Explore it in the IPython Shell with the\u00a0.head()\u00a0method. You will see that there are certain data points labeled with a\u00a0'?'. These denote missing values. As you saw in the video, different datasets encode missing values in different ways. Sometimes it may be a\u00a0'9999', other times a\u00a00\u00a0- real-world data can be very messy! If you're lucky, the missing values will already be encoded as\u00a0NaN. \n\nWe use\u00a0NaN\u00a0because it is an efficient and simplified way of internally representing missing data, and it lets us take advantage of pandas methods such as\u00a0.dropna()\u00a0and\u00a0.fillna(), as well as scikit-learn's Imputation transformer\u00a0Imputer().\n\nIn this exercise, we convert the\u00a0'?'s to NaNs, and then drop the rows that contain them from the DataFrame.\n","68bb7319":"# Data Scaling and Normalization\n\nA generic dataset (we assume here that it is always numerical) is made up of different values which can be drawn from different distributions, having different scales and, sometimes, there are also outliers. A machine learning algorithm isn't naturally able to\ndistinguish among these various situations, and therefore, it's always preferable to standardize datasets before processing them.","b21f3751":"# Robust Scaler\n\nThe RobustScaler uses a similar method to the Min-Max scaler but it instead uses the interquartile range, rathar than the min-max, so that it is robust to outliers. "}}