{"cell_type":{"11558671":"code","9c814b03":"code","b0883b76":"code","500cdaec":"code","5626794f":"code","3b91b66c":"code","d71c352b":"code","bac31c09":"code","1d8dce91":"code","61ceff4a":"code","ee790450":"code","478d33d2":"code","567e3b90":"code","92fbcb84":"markdown","4ba38d71":"markdown","a195276d":"markdown","4c9a1922":"markdown"},"source":{"11558671":"!pip install --upgrade razdel nltk rouge==0.3.1 summa sumy lexrank","9c814b03":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport json\nimport random\n\nimport numpy as np # linear algebra\nimport razdel # Russian tokenization and sentence splitting\nfrom rouge import Rouge\nfrom nltk.translate.bleu_score import corpus_bleu\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0883b76":"def read_gazeta_records(file_name, shuffle=False, sort_by_date=True):\n    assert shuffle != sort_by_date\n    records = []\n    with open(file_name, \"r\") as r:\n        for line in r:\n            records.append(json.loads(line))\n    if sort_by_date:\n        records.sort(key=lambda x: x[\"date\"])\n    if shuffle:\n        random.shuffle(records)\n    return records\n\ntrain_records = read_gazeta_records(\"\/kaggle\/input\/gazeta-summaries\/gazeta_train.jsonl\")\nval_records = read_gazeta_records(\"\/kaggle\/input\/gazeta-summaries\/gazeta_val.jsonl\")\ntest_records = read_gazeta_records(\"\/kaggle\/input\/gazeta-summaries\/gazeta_test.jsonl\")","500cdaec":"train_records[0]","5626794f":"!rm -f meteor-1.5.tar.gz\n!wget -nc https:\/\/www.cs.cmu.edu\/~alavie\/METEOR\/download\/meteor-1.5.tar.gz\n!tar -xzvf meteor-1.5.tar.gz","3b91b66c":"# Python wrapper for METEOR implementation, by Xinlei Chen\n# Acknowledge Michael Denkowski for the generous discussion and help\n# Based on https:\/\/github.com\/tylin\/coco-caption\/blob\/master\/pycocoevalcap\/meteor\/meteor.py\n# Modified by Ilya Gusev\n\nimport subprocess\nimport threading\n\n\nclass Meteor:\n    def __init__(self, meteor_jar, language):\n        # Used to guarantee thread safety\n        self.lock = threading.Lock()\n\n        self.meteor_cmd = ['java', '-jar', '-Xmx2G', meteor_jar, '-', '-', '-stdio', '-l', language, '-norm']\n        self.meteor_p = subprocess.Popen(self.meteor_cmd,\n                                         stdin=subprocess.PIPE,\n                                         stdout=subprocess.PIPE,\n                                         stderr=subprocess.STDOUT,\n                                         encoding='utf-8',\n                                         bufsize=0)\n\n    def compute_score(self, hyps, refs):\n        scores = []\n        self.lock.acquire()\n        for hyp, ref in zip(hyps, refs):\n            stat = self._stat(hyp, ref)\n            # EVAL ||| stats\n            eval_line = 'EVAL ||| {}'.format(\" \".join(map(str, map(int, map(float, stat.split())))))\n            self.meteor_p.stdin.write('{}\\n'.format(eval_line))\n            scores.append(float(self.meteor_p.stdout.readline().strip()))\n        self.lock.release()\n\n        return sum(scores) \/ len(scores)\n\n    def _stat(self, hypothesis_str, reference_list):\n        # SCORE ||| reference 1 words ||| reference n words ||| hypothesis words\n        hypothesis_str = hypothesis_str.replace('|||', '').replace('  ', ' ')\n        score_line = ' ||| '.join(('SCORE', ' ||| '.join(reference_list), hypothesis_str))\n        self.meteor_p.stdin.write('{}\\n'.format(score_line))\n        return self.meteor_p.stdout.readline().strip()\n\n    def __del__(self):\n        self.lock.acquire()\n        self.meteor_p.stdin.close()\n        self.meteor_p.kill()\n        self.meteor_p.wait()\n        self.lock.release()","d71c352b":"def calc_metrics(refs, hyps, language, metric=\"all\", meteor_jar=None):\n    metrics = dict()\n    metrics[\"count\"] = len(hyps)\n    metrics[\"ref_example\"] = refs[-1]\n    metrics[\"hyp_example\"] = hyps[-1]\n    many_refs = [[r] if r is not list else r for r in refs]\n    if metric in (\"bleu\", \"all\"):\n        metrics[\"bleu\"] = corpus_bleu(many_refs, hyps)\n    if metric in (\"rouge\", \"all\"):\n        rouge = Rouge()\n        scores = rouge.get_scores(hyps, refs, avg=True)\n        metrics.update(scores)\n    if metric in (\"meteor\", \"all\") and meteor_jar is not None and os.path.exists(meteor_jar):\n        meteor = Meteor(meteor_jar, language=language)\n        metrics[\"meteor\"] = meteor.compute_score(hyps, many_refs)\n    return metrics\n\n\ndef print_metrics(refs, hyps, language, metric=\"all\", meteor_jar=None):\n    metrics = calc_metrics(refs, hyps, language=language, metric=metric, meteor_jar=meteor_jar)\n\n    print(\"-------------METRICS-------------\")\n    print(\"Count:\\t\", metrics[\"count\"])\n    print(\"Ref:\\t\", metrics[\"ref_example\"])\n    print(\"Hyp:\\t\", metrics[\"hyp_example\"])\n\n    if \"bleu\" in metrics:\n        print(\"BLEU:     \\t{:3.1f}\".format(metrics[\"bleu\"] * 100.0))\n    if \"rouge-1\" in metrics:\n        print(\"ROUGE-1-F:\\t{:3.1f}\".format(metrics[\"rouge-1\"]['f'] * 100.0))\n        print(\"ROUGE-2-F:\\t{:3.1f}\".format(metrics[\"rouge-2\"]['f'] * 100.0))\n        print(\"ROUGE-L-F:\\t{:3.1f}\".format(metrics[\"rouge-l\"]['f'] * 100.0))\n    if \"meteor\" in metrics:\n        print(\"METEOR:   \\t{:3.1f}\".format(metrics[\"meteor\"] * 100.0))","bac31c09":"def postprocess(refs, hyps, tokenize_after=True, lower=True):\n    for i, (ref, hyp) in enumerate(zip(refs, hyps)):\n        ref = ref.strip()\n        hyp = hyp.strip()\n        if tokenize_after:\n            hyp = \" \".join([token.text for token in razdel.tokenize(hyp)])\n            ref = \" \".join([token.text for token in razdel.tokenize(ref)])\n        if lower:\n            hyp = hyp.lower()\n            ref = ref.lower()\n        refs[i] = ref\n        hyps[i] = hyp\n    return refs, hyps","1d8dce91":"def calc_method_score(records, predict_func, nrows=None):\n    references = []\n    predictions = []\n\n    for i, record in enumerate(records):\n        if nrows is not None and i >= nrows:\n            break\n        summary = record[\"summary\"]\n        text = record[\"text\"]\n        prediction = predict_func(text, summary)\n        references.append(summary)\n        predictions.append(prediction)\n    references, predictions = postprocess(references, predictions)\n    print_metrics(references, predictions, \"ru\", meteor_jar=\"meteor-1.5\/meteor-1.5.jar\")","61ceff4a":"import razdel\n\ndef predict_lead(text, summary, n):\n    sentences = [sentence.text for sentence in razdel.sentenize(text)]\n    prediction = \" \".join(sentences[:n])\n    return prediction","ee790450":"calc_method_score(test_records, lambda x, y: predict_lead(x, y, 3))","478d33d2":"from summa.summarizer import summarize\n\n\ndef predict_text_rank(text, summary, summary_part=0.1):\n    return summarize(text, ratio=summary_part, language='russian').replace(\"\\n\", \" \")\n\n\ncalc_method_score(test_records, predict_text_rank)","567e3b90":"import lexrank\nfrom lexrank import LexRank\nfrom lexrank.mappings.stopwords import STOPWORDS\n\n\ndef predict_lex_rank(text, summary, lxr, summary_size=3, threshold=None):\n    sentences = [s.text for s in razdel.sentenize(text)]\n    prediction = lxr.get_summary(sentences, summary_size=summary_size, threshold=threshold)\n    prediction = \" \".join(prediction)\n    return prediction\n    \n\nsentences = [[s.text for s in razdel.sentenize(r[\"text\"])] for r in test_records]\nlxr = LexRank(sentences, stopwords=STOPWORDS['ru'])\ncalc_method_score(test_records, lambda x, y: predict_lex_rank(x, y, lxr))","92fbcb84":"## LexRank\n* Original paper: https:\/\/arxiv.org\/pdf\/1109.2128.pdf\n* lexrank library: https:\/\/github.com\/crabcamp\/lexrank","4ba38d71":"## Metrics\n* ROUGE: https:\/\/github.com\/pltrdy\/rouge\n* METEOR: https:\/\/www.cs.cmu.edu\/~alavie\/METEOR\n* BLEU: https:\/\/www.nltk.org\/_modules\/nltk\/translate\/bleu_score.html","a195276d":"## Lead-3\nFirst 3 sentences of a text as a baseline.","4c9a1922":"## TextRank\n* Original paper: https:\/\/www.aclweb.org\/anthology\/W04-3252.pdf\n* Summa library: https:\/\/github.com\/summanlp\/textrank"}}