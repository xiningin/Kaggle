{"cell_type":{"2939af89":"code","0d07c225":"code","2268c280":"code","991c73f0":"code","17dc0c7c":"code","b642c161":"code","bcd46718":"code","a0ecfc8d":"code","e819682f":"code","b93257f1":"code","34b846a8":"code","a0f5a352":"code","9bed3e80":"code","145c5331":"code","1f7e63e0":"code","216c461b":"code","d53b0287":"code","f12ac2aa":"code","b2751bb5":"code","313c95f5":"code","ec9e86ce":"markdown","46b79d4c":"markdown","3cde5593":"markdown","6e104f6f":"markdown"},"source":{"2939af89":"from statsmodels.tsa.ar_model import AutoReg, ar_select_order\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.api import acf, pacf, graphics\nfrom typing import List, Tuple, Union, NoReturn\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.offline as py\nimport plotly.express as px\nimport cufflinks as cf\nimport plotly\nfrom statsmodels.robust import mad\nimport matplotlib.pyplot as plt\nfrom scipy.signal import butter\nfrom scipy import signal\nimport seaborn as sns\nfrom sklearn import *\nimport pandas as pd \nimport numpy as np\nimport warnings\nimport scipy\nimport pywt\nimport os\nimport gc\n\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error, f1_score, roc_auc_score\nfrom functools import partial\nimport scipy as sp\nimport datetime\n\ncf.go_offline()\npy.init_notebook_mode()\ncf.getThemes()\ncf.set_config_file(theme='ggplot')\nwarnings.simplefilter('ignore')\npd.plotting.register_matplotlib_converters()\nsns.mpl.rc('figure',figsize=(16, 6))\nplt.style.use('ggplot')\nsns.set_style('darkgrid')\n\nfrom sklearn.linear_model import Ridge, ElasticNet\nfrom sklearn.preprocessing import StandardScaler\n# from bayes_opt import BayesianOptimization","0d07c225":"nfolds = 5","2268c280":"def add_bathing_to_data(df : pd.DataFrame) -> pd.DataFrame :\n    batches = df.shape[0] \/\/ 500000\n    df['batch'] = 0\n    for i in range(batches):\n        idx = np.arange(i*500000, (i+1)*500000)\n        df.loc[idx, 'batch'] = i + 1\n    return df\n\ndef p5( x : pd.Series) -> pd.Series : return x.quantile(0.05)\ndef p95(x : pd.Series) -> pd.Series : return x.quantile(0.95)\n","991c73f0":"def maddest(d, axis=None):\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef high_pass_filter(x, low_cutoff=1000, sample_rate=10000):\n\n    nyquist = 0.5 * sample_rate\n    norm_low_cutoff = low_cutoff \/ nyquist\n    print(norm_low_cutoff)\n    sos = butter(10, Wn=[norm_low_cutoff], btype='highpass', output='sos')\n    filtered_sig = signal.sosfilt(sos, x)\n\n    return filtered_sig\n\ndef denoise_signal( x, wavelet='db4', level=1):\n    \n    coeff = pywt.wavedec( x, wavelet, mode=\"per\" )\n    sigma = (1\/0.6745) * maddest( coeff[-level] )\n    uthresh = sigma * np.sqrt( 2*np.log( len( x ) ) )\n    coeff[1:] = ( pywt.threshold( i, value=uthresh, mode='hard' ) for i in coeff[1:] )\n    return pywt.waverec( coeff, wavelet, mode='per' )","17dc0c7c":"def features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index \/\/ 25_000\n    df['batch_index'] = df.index  - (df.batch * 25_000)\n    df['batch_slices'] = df['batch_index']  \/\/ 2500\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n\n    for c in ['batch','batch_slices2']:\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal'].mean()\n        d['median'+c] = df.groupby([c])['signal'].median()\n        d['max'+c] = df.groupby([c])['signal'].max()\n        d['min'+c] = df.groupby([c])['signal'].min()\n        d['std'+c] = df.groupby([c])['signal'].std()\n        d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n        d['range'+c] = (d['max'+c] - d['min'+c])\n        d['abs_avg'+c] = ((d['abs_min'+c] + d['abs_max'+c]) \/ 2)\n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n\n        \n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]:\n        df[c] = df[c] - df['signal']\n       \n    # rolling features\n    windows = [50, 500]\n\n    for win in windows:\n        xroll = df.groupby(['batch'])['signal'].rolling(win, min_periods= 3)\n        df['min_r'+str(win)] = np.array(xroll.min().fillna(0))\n        df['max_r'+str(win)] = np.array(xroll.max().fillna(0))\n        df['mean_r'+str(win)] = np.array(xroll.max().fillna(0))\n        df['std_r'+str(win)] = np.array(xroll.std().fillna(0))\n        df['skew_r'+str(win)] = np.array(xroll.skew().fillna(0))\n        df['kurt_r'+str(win)] = np.array(xroll.kurt().fillna(0))\n        df['q01_r'+str(win)] = np.array(xroll.quantile(0.01).fillna(0))\n        df['q99_r'+str(win)] = np.array(xroll.quantile(0.99).fillna(0))\n    \n    \n\n            \n    return df","b642c161":"def MacroF1Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = metrics.f1_score(labels, preds, average = 'macro')\n    return ('MacroF1Metric', score, True)\n","bcd46718":"\nclass OptimizedRounder(object):\n\n    def __init__(self):\n        self.coef_ = 0\n\n    def loss(self, coef, X, y):\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        return -metrics.f1_score(y, X_p, average = 'macro')\n\n    def fit(self, X, y):\n        loss_partial = partial(self.loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        return (pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])).astype(np.int8)\n\n    def coefficients(self):\n        return self.coef_['x']\n","a0ecfc8d":"def optimize_predictions(preds, coeffs):\n    \n    preds[preds <= coeffs[0]] = 0\n    preds[np.where(np.logical_and(preds > coeffs[0], preds <= coeffs[1]))] = 1\n    preds[np.where(np.logical_and(preds > coeffs[1], preds <= coeffs[2]))] = 2\n    preds[np.where(np.logical_and(preds > coeffs[2], preds <= coeffs[3]))] = 3\n    preds[np.where(np.logical_and(preds > coeffs[3], preds <= coeffs[4]))] = 4\n    preds[np.where(np.logical_and(preds > coeffs[4], preds <= coeffs[5]))] = 5\n    preds[np.where(np.logical_and(preds > coeffs[5], preds <= coeffs[6]))] = 6\n    preds[np.where(np.logical_and(preds > coeffs[6], preds <= coeffs[7]))] = 7\n    preds[np.where(np.logical_and(preds > coeffs[7], preds <= coeffs[8]))] = 8\n    preds[np.where(np.logical_and(preds > coeffs[8], preds <= coeffs[9]))] = 9\n    preds[preds > coeffs[9]] = 10\n    preds = preds.astype(np.int8)\n    \n    return preds","e819682f":"def shrink_memory(df, xcols):\n    for colname in xcols:\n#        print(colname)\n        c_min = df[colname].min()\n        c_max = df[colname].max()\n        if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n            df[colname] = df[colname].astype(np.float16)\n        elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n            df[colname] = df[colname].astype(np.float32)\n        else:\n            df[colname] = df[colname].astype(np.float64)    \n    return df","b93257f1":"base = os.path.abspath('\/kaggle\/input\/liverpool-ion-switching\/')\n# base = os.path.abspath('..\/input\/')\n\ntrain = pd.read_csv(os.path.join(base + '\/train.csv'))\n","34b846a8":"# create features and shrink memory\n\ntrain = features(train)\n\nstart_memory = train.memory_usage(deep= True).sum() \/ 1024 ** 2\nprint(start_memory)\n\nxcols = [f for f in train.columns if f not in ['time', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]\n\ntrain = shrink_memory(train, xcols) \n\nend_memory = train.memory_usage().sum() \/ 1024**2\npercent = 100 * (start_memory - end_memory) \/ start_memory\nprint('Mem. usage decreased from {:5.2f} Mb to {:5.2f} Mb ({:.1f}% reduction)'.format(start_memory, end_memory, percent))  \n","a0f5a352":"test  = pd.read_csv(os.path.join(base + '\/test.csv'))\n\ntest = features(test)\n\nstart_memory = test.memory_usage(deep= True).sum() \/ 1024 ** 2\nprint(start_memory)\n\nxcols = [f for f in test.columns if f not in ['time', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]\n\ntest = shrink_memory(test, xcols) \n\nend_memory = test.memory_usage().sum() \/ 1024**2\npercent = 100 * (start_memory - end_memory) \/ start_memory\nprint('Mem. usage decreased from {:5.2f} Mb to {:5.2f} Mb ({:.1f}% reduction)'.format(start_memory, end_memory, percent))  \n","9bed3e80":"dropcols = ['batch', 'batch_index', 'batch_slices', 'batch_slices2']\n\nbatch_train = train['batch'] \n\ntrain.drop(dropcols, axis = 1, inplace = True)\ntest.drop(dropcols, axis = 1, inplace = True)\n","145c5331":"id_train = train['time']\nid_test = test['time']\nytrain = train['open_channels']\n\n\ntrain.drop(['time', 'open_channels'], axis = 1, inplace = True)\ntest.drop(['time'], axis = 1, inplace = True)","1f7e63e0":"train['is_test'] = 0\ntest['is_test'] = 1\n\nntr = train.shape[0]","216c461b":"xdat = pd.concat([train, test], axis = 0)\ndel train, test\nydat = xdat['is_test']\nxdat.drop('is_test', axis = 1, inplace = True)","d53b0287":"skf = StratifiedKFold(n_splits= nfolds, shuffle= True, random_state=42)\n\n\nmvalid = np.zeros((xdat.shape[0],1))\n","f12ac2aa":"# for fold, (tr_ind, val_ind) in enumerate(skf.split(ytrain, groups = batch_train)):\nfor (fold, (id0, id1)) in enumerate(skf.split(xdat, ydat)):\n    \n    # prepare split\n    x0, x1 = xdat.iloc[id0], xdat.iloc[id1]\n    y0, y1 = ydat.iloc[id0], ydat.iloc[id1]\n      \n    sc0 = StandardScaler()\n    sc0.fit(x0)\n    \n    x0 = sc0.transform(x0)\n    x1 = sc0.transform(x1)\n\n    model = Ridge(alpha = 30)\n    model.fit(x0, y0)\n            \n    mvalid[id1,0] = model.predict(x1)\n    \n    print(roc_auc_score(y1, mvalid[id1,0]))\n\n    del x0, x1, y0, y1\n    \n#     mvalid[id1,0] = model.predict(x1, num_iteration=model.best_iteration_)\n#     mfull[:,fold] = model.predict(test, num_iteration = model.best_iteration_)\n    ","b2751bb5":"df_out = pd.DataFrame()\ndf_out['time'] = id_train\ndf_out['wgt'] = mvalid[0:ntr]\ndf_out.to_csv('av_weights.csv', index = False)","313c95f5":"print(roc_auc_score(ydat, mvalid))","ec9e86ce":"# Data","46b79d4c":"# Model","3cde5593":"Original https:\/\/www.kaggle.com\/siavrez\/simple-eda-model\n\n","6e104f6f":"# Functions"}}