{"cell_type":{"13d6dc67":"code","4ec7fc2c":"code","f62ddc02":"code","2f7dc63d":"code","aecad11a":"code","a0d6a3ea":"code","39250015":"code","6e122cae":"code","d910cd92":"code","28b9f85f":"code","2217486d":"code","b3944efc":"code","69a004f3":"code","14d1a2f4":"code","17b8e8ed":"code","68e2c187":"code","ab8ef00e":"code","5b48ad2f":"code","723af952":"code","fce39ca3":"code","6c6e69bd":"code","c57c73c9":"markdown","34e44c00":"markdown","17a496d2":"markdown","6631b8b6":"markdown","636ec241":"markdown","3d4851fc":"markdown","6ba900d5":"markdown"},"source":{"13d6dc67":"import numpy as np\nimport pandas as pd\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.model_selection import KFold","4ec7fc2c":"%%time\ndata = pd.read_csv('..\/input\/santander-value-prediction-challenge\/train.csv')\ntarget = np.log1p(data['target'])\ndata.drop(['ID', 'target'], axis=1, inplace=True)","f62ddc02":"%%time\nleak = pd.read_csv('..\/input\/breaking-lb-fresh-start-with-lag-selection\/train_leak.csv')\ndata['leak'] = leak['compiled_leak'].values\ndata['log_leak'] = np.log1p(leak['compiled_leak'].values)","2f7dc63d":"# %%time\n# def rmse(y_true, y_pred):\n#     return mean_squared_error(y_true, y_pred) ** .5\n\n# reg = XGBRegressor(n_estimators=1000)\n# folds = KFold(4, True, 134259)\n# fold_idx = [(trn_, val_) for trn_, val_ in folds.split(data)]\n# scores = []\n\n# nb_values = data.nunique(dropna=False)\n# nb_zeros = (data == 0).astype(np.uint8).sum(axis=0)\n\n# features = [f for f in data.columns if f not in ['log_leak', 'leak', 'target', 'ID']]\n# for _f in features:\n#     score = 0\n#     for trn_, val_ in fold_idx:\n#         reg.fit(\n#             data[['log_leak', _f]].iloc[trn_], target.iloc[trn_],\n#             eval_set=[(data[['log_leak', _f]].iloc[val_], target.iloc[val_])],\n#             eval_metric='rmse',\n#             early_stopping_rounds=50,\n#             verbose=False\n#         )\n#         score += rmse(target.iloc[val_], reg.predict(data[['log_leak', _f]].iloc[val_], ntree_limit=reg.best_ntree_limit)) \/ folds.n_splits\n#     scores.append((_f, score))","aecad11a":"# report = pd.DataFrame(scores, columns=['feature', 'rmse']).set_index('feature')\n# report['nb_zeros'] = nb_zeros\n# report['nunique'] = nb_values\n# report.sort_values(by='rmse', ascending=True, inplace=True)","a0d6a3ea":"# report.to_csv('feature_report.csv', index=True)","39250015":"report = pd.read_csv('..\/input\/feature-report\/feature_report.csv', index_col='feature')","6e122cae":"report.head()","d910cd92":"good_features = report.loc[report['rmse'] <= 0.7955].index\nrmses = report.loc[report['rmse'] <= 0.7955, 'rmse'].values\ngood_features","28b9f85f":"test = pd.read_csv('..\/input\/santander-value-prediction-challenge\/test.csv')","2217486d":"%%time\ntst_leak = pd.read_csv('..\/input\/breaking-lb-fresh-start-with-lag-selection\/test_leak.csv')\ntest['leak'] = tst_leak['compiled_leak']\ntest['log_leak'] = np.log1p(tst_leak['compiled_leak'])","b3944efc":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb","69a004f3":"folds = KFold(n_splits=5, shuffle=True, random_state=1)\n\n# Use all features for stats\nfeatures = [f for f in data if f not in ['ID', 'leak', 'log_leak', 'target']]\ndata.replace(0, np.nan, inplace=True)\ndata['log_of_mean'] = np.log1p(data[features].replace(0, np.nan).mean(axis=1))\ndata['mean_of_log'] = np.log1p(data[features]).replace(0, np.nan).mean(axis=1)\ndata['log_of_median'] = np.log1p(data[features].replace(0, np.nan).median(axis=1))\ndata['nb_nans'] = data[features].isnull().sum(axis=1)\ndata['the_sum'] = np.log1p(data[features].sum(axis=1))\ndata['the_std'] = data[features].std(axis=1)\ndata['the_kur'] = data[features].kurtosis(axis=1)\n\ntest.replace(0, np.nan, inplace=True)\ntest['log_of_mean'] = np.log1p(test[features].replace(0, np.nan).mean(axis=1))\ntest['mean_of_log'] = np.log1p(test[features]).replace(0, np.nan).mean(axis=1)\ntest['log_of_median'] = np.log1p(test[features].replace(0, np.nan).median(axis=1))\ntest['nb_nans'] = test[features].isnull().sum(axis=1)\ntest['the_sum'] = np.log1p(test[features].sum(axis=1))\ntest['the_std'] = test[features].std(axis=1)\ntest['the_kur'] = test[features].kurtosis(axis=1)","14d1a2f4":"# Only use good features, log leak and stats for training\nfeatures = good_features.tolist()\nfeatures = features + ['log_leak', 'log_of_mean', 'mean_of_log', 'log_of_median', 'nb_nans', 'the_sum', 'the_std', 'the_kur']\ndtrain = lgb.Dataset(data=data[features], \n                     label=target, free_raw_data=False)\ntest['target'] = 0\n\ndtrain.construct()\noof_preds = np.zeros(data.shape[0])","17b8e8ed":"from skopt import BayesSearchCV\nfrom sklearn.model_selection import StratifiedKFold, KFold","68e2c187":"def status_print(optim_result):\n    \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n    \n    # Get all the models tested so far in DataFrame format\n    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n    \n    # Get current parameters and the best parameters    \n    best_params = pd.Series(bayes_cv_tuner.best_params_)\n    print('Model #{}\\nBest MSE: {}\\nBest params: {}\\n'.format(\n        len(all_models),\n        np.round(bayes_cv_tuner.best_score_, 4),\n        bayes_cv_tuner.best_params_\n    ))\n    \n    # Save all model results\n    clf_name = bayes_cv_tuner.estimator.__class__.__name__\n    all_models.to_csv(clf_name+\"_cv_results.csv\")","ab8ef00e":"import math\n\n#A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\ndef rmsle(y, y_pred):\n    assert len(y) == len(y_pred)\n    terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(terms_to_sum) * (1.0\/len(y))) ** 0.5","5b48ad2f":"%%time\nbayes_cv_tuner = BayesSearchCV(\n    estimator = lgb.LGBMRegressor(objective='regression', boosting_type='gbdt', subsample=0.6143), #colsample_bytree=0.6453, subsample=0.6143\n    search_spaces = {\n        'learning_rate': (0.01, 1.0, 'log-uniform'),\n        'num_leaves': (10, 100),      \n        'max_depth': (0, 50),\n        'min_child_samples': (0, 50),\n        'max_bin': (100, 1000),\n        'subsample_freq': (0, 10),\n        'min_child_weight': (0, 10),\n        'reg_lambda': (1e-9, 1000, 'log-uniform'),\n        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n        'scale_pos_weight': (1e-6, 500, 'log-uniform'),\n        'n_estimators': (50, 150),\n    },    \n    scoring = 'neg_mean_squared_log_error', #neg_mean_squared_log_error\n    cv = KFold(\n        n_splits=5,\n        shuffle=True,\n        random_state=42\n    ),\n    n_jobs = 1,\n    n_iter = 100,   \n    verbose = 0,\n    refit = True,\n    random_state = 42\n)\n\n# Fit the model\nresult = bayes_cv_tuner.fit(data[features], target, callback=status_print)","723af952":"pred = bayes_cv_tuner.predict(test[features])","fce39ca3":"test['target'] = np.expm1(pred)\ntest[['ID', 'target']].to_csv('my_submission.csv', index=False, float_format='%.2f')","6c6e69bd":"test.loc[test['leak'].notnull(), 'target'] = test.loc[test['leak'].notnull(), 'leak']\ntest[['ID', 'target']].to_csv('submission.csv', index=False, float_format='%.2f')","c57c73c9":"### Add leak to test","34e44c00":"### Add train leak","17a496d2":"### Train lightgbm","6631b8b6":"### Create dataframe","636ec241":"### Save submission","3d4851fc":"### Select some features (threshold is not optimized)","6ba900d5":"### Feature Scoring using XGBoost with the leak feature"}}