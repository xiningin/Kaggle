{"cell_type":{"967686ae":"code","132ec623":"code","610304cc":"code","a3729db2":"code","1a715cc5":"code","e63bf7c9":"code","468ccea5":"code","2f729a04":"code","05cbadc8":"code","b86ecdb6":"code","782c5532":"code","f1c340e4":"code","8d5ec42c":"code","6c5af0f2":"code","a693c694":"code","9141465a":"code","36376005":"code","1947e064":"code","90aed91d":"code","4927904d":"code","1ec771b1":"code","1831649a":"code","88162ac3":"code","062bb912":"code","96ecb2d5":"code","47423476":"code","465189ae":"code","951910e7":"code","1908f936":"code","10002a35":"code","4c65083c":"code","6c024bfa":"code","5b3f5143":"markdown","5ad8c7f3":"markdown","ab0e3c80":"markdown","c22e2e98":"markdown","bc77b774":"markdown","ac1e8936":"markdown","7c0bd809":"markdown","5d913a1d":"markdown","1f29030e":"markdown","115e1726":"markdown","da32d2ed":"markdown","a8b6ba0d":"markdown","64fe64f4":"markdown","16398775":"markdown"},"source":{"967686ae":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.subplots import make_subplots\nfrom sklearn.preprocessing import StandardScaler\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n","132ec623":"#raw_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques')\nraw_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nraw_data","610304cc":"raw_data.info()","a3729db2":"#Con la funcion .decribe se logra encontrar que hay columnas donde la mayoria de datos son Null.","1a715cc5":"# Paso 1: Eliminar las columnas donde mas de 15% de los datos son nulos. Estas columnas no aportan informacion al modelo.","e63bf7c9":"#data_NO_NaN = dataframe sin valores NaN\ndata_NO_NaN=raw_data.copy()\ndataframeDimention=data_NO_NaN[\"Id\"].describe()[0] #calculate the daraframe size\ndataframeDimention_85 =data_NO_NaN[\"Id\"].describe()[0]*0.85  #calculate the dataframe 85% size [to delete de 15% of the data]\n\nfor col in data_NO_NaN:\n    non_null=data_NO_NaN[col].describe()[0] #calculate the qty of non-null values in the colum\n    if(non_null<dataframeDimention_85):\n        #print(col)\n        data_NO_NaN.drop(col, axis=1, inplace=True)\n        \n        \n    \n \n   ","468ccea5":"data_NO_NaN.info()","2f729a04":"#Del paso 1: Quedaron 74 columnas. Se eliminaron 6 de las 80 originales","05cbadc8":"# Paso 2: Remplazar los valores NaN (adicional se cambia los datos tipo \"object\" por \"category\")","b86ecdb6":"for col in data_NO_NaN:\n    if(data_NO_NaN[col].dtype==\"int64\" or data_NO_NaN[col].dtype==\"float64\" and col!=\"Id\"):#Si son valores numercos sustituir por el promedio\n        data_NO_NaN[col].fillna((data_NO_NaN[col].mean()), inplace=True)\n    elif(data_NO_NaN[col].dtype==\"object\"):\n        data_NO_NaN[col].fillna((data_NO_NaN[col].value_counts().index[0]), inplace=True)#Si son valores categoricos sustituir por el valor mas comun\n        #data_NO_NaN[col]=data_NO_NaN[col].astype('category') #Cambiar el tipo de dato a categorico","782c5532":"sns.heatmap(data_NO_NaN.isnull(), cbar=False); #Se comprueba que no hay valores NaN","f1c340e4":"#Se muestra el histograma de los datos para entender la distribucion.\n\n#Usar Encode para poder visualizar las columnas categoricas en los histogramas\ndata_for_hist=data_NO_NaN.copy()#Se crea un nuevo dataframe con el unico fin de cambiar todos los datos categoricos a numericos.\nfor col in data_for_hist:\n    if(data_for_hist[col].dtype==\"object\"):\n        count = data_for_hist[col].value_counts()\n        data_for_hist[col] = data_for_hist[col].map(count)\n        \n\nfor col in data_for_hist:\n    if((data_for_hist[col].dtype==\"int64\" or data_for_hist[col].dtype==\"float64\") and col!=\"Id\"):#Si son valores numercos aplicar normalizacion\n        y = data_for_hist[col]\n        removed_outliers = y.between(y.quantile(.05), y.quantile(.95)) #eliminar valores entre Quartile 5% y 95%\n        index_names = data_for_hist[~removed_outliers].index # INVERT removed_outliers!!\n        data_for_hist.drop(index_names, inplace=True)\n        data_for_hist.reset_index()\n        \n        \n        \ndata_for_hist.hist(bins=50, figsize=(30,30))\nplt.show()","8d5ec42c":"#Conclusiones principales de los datos\n#-Algunas de las columnas acumulan todos sus valores en una misma region. Esto hace que la columna no agregue informacion util.","6c5af0f2":"#Paso3:Para solucional esto se usa la funcion \"value_counts\" para contar la cantidad de muestras con el mismo valor y determinar si en un mismo valor se acumulan muchos de los datos\n#Si la muestra mas frecuente se encuentra 5 veces mas que la segunda muestra mas frecuente. Entonces se elimina la columna\n\nbestData_1=data_NO_NaN.copy()\n\nfor col in bestData_1:\n    if(bestData_1[col].dtype==\"object\"):\n        count = bestData_1[col].value_counts()\n        bestData_1[col] = bestData_1[col].map(count)\n\n\nfor col in bestData_1:\n    mustFrecuentValue=list(bestData_1[col].value_counts())[0]\n    secondMmustFrecuentValue=list(bestData_1[col].value_counts())[1]\n    if((mustFrecuentValue\/secondMmustFrecuentValue)>5):\n        bestData_1=bestData_1.drop(col, axis=1)\nbestData_1","a693c694":"bestData_1.hist(bins=50, figsize=(30,30))\nplt.show()","9141465a":"#Del paso 3: Quedaron 39 columnas. Se eliminaron 35 de las 74 que quedaban del paso anterios","36376005":"#Para este paso se va a utiliza la herramienta de correlacion\nbestData_2=bestData_1.copy()\n\ncorr_matrix = bestData_2.corr()\ncorr_values = corr_matrix[\"SalePrice\"].sort_values(ascending=False)\n\ntop_15 = corr_values[:25]\nbottom_10 = corr_values[-10:]\n\n\n\n\nprint(f\"top_15\\n{top_15}\")\nprint(f\"bottom_10\\n{bottom_10}\")","1947e064":"best_cols = list(top_15.keys())\nprint(best_cols)","90aed91d":"# Se toman las 15 columnas con mayor correlacion\nbestData_2=bestData_2[best_cols]\nbestData_2","4927904d":"bestData_2.describe()","1ec771b1":"fig = make_subplots(\n    rows=5, \n    cols=2,\n    subplot_titles=best_cols\n)\n\nfig.update_layout(\n    autosize=False,\n    width=900,\n    height=1800,\n    showlegend=False\n)\n\nfor r in range(1, 6):\n    for c in range(1, 3):\n        fig.add_trace(go.Box(y=bestData_2[best_cols[(r-1)*2+(c-1)]], boxpoints=\"all\"), r, c)\n\nfig.show()","1831649a":"#drop outliers\nbestData_2_NoOutliers=bestData_2.copy()\n\nfor col in bestData_2_NoOutliers:\n        y = bestData_2_NoOutliers[col]\n        removed_outliers = y.between(y.quantile(.02), y.quantile(.97)) #eliminar valores entre Quartile 5% y 98%\n        index_names = bestData_2_NoOutliers[~removed_outliers].index # INVERT removed_outliers!!\n        bestData_2_NoOutliers.drop(index_names, inplace=True)\n        bestData_2_NoOutliers.reset_index()\nbestData_2_NoOutliers.describe()      ","88162ac3":"fig = make_subplots(\n    rows=5, \n    cols=2,\n    subplot_titles=best_cols\n)\n\nfig.update_layout(\n    autosize=False,\n    width=900,\n    height=1800,\n    showlegend=False\n)\n\nfor r in range(1, 6):\n    for c in range(1, 3):\n        fig.add_trace(go.Box(y=bestData_2_NoOutliers[best_cols[(r-1)*2+(c-1)]], boxpoints=\"all\"), r, c)\n\nfig.show()","062bb912":"standard_scaler = StandardScaler()\n\nbestData_2_Scale=bestData_2_NoOutliers.copy()\n\nfor col in bestData_2_Scale:\n    if(col!=\"SalePrice\"):\n        bestData_2_Scale[col] = standard_scaler.fit_transform(bestData_2_Scale[[col]])\nbestData_2_Scale\n","96ecb2d5":"cleanData=bestData_2_Scale.copy()\nhousing_labels = cleanData[\"SalePrice\"].copy()\nhousing_prepared = cleanData.drop(\"SalePrice\", axis=1)\nhousing_prepared\n#cleanData.info()","47423476":"lrmodel = LinearRegression()\nlrmodel.fit(housing_prepared, housing_labels)","465189ae":"housing_predictions = lrmodel.predict(housing_prepared)\nmse = mean_squared_error(housing_labels, housing_predictions)\nrmse_log=mean_squared_log_error(abs(housing_labels), abs(housing_predictions))\nrmse = np.sqrt(mse)\nprint(f\"mse: {mse}, rmse: {rmse}, rmse_log:{rmse_log}\")","951910e7":"model_nn = keras.Sequential([\n    layers.Dense(50, activation='relu', input_shape=[len(housing_prepared.keys())]),\n    layers.Dense(80, activation='relu'),\n    layers.Dense(50, activation='relu'),\n    #layers.Dense(50, activation='relu'),\n    #layers.Dense(100, activation='relu'),\n    layers.Dense(1)\n  ])\n\noptimizer = tf.keras.optimizers.RMSprop(0.001)\n\nmodel_nn.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae', 'mse'])\nmodel_nn.summary()","1908f936":"history = model_nn.fit(\n  housing_prepared, housing_labels,\n  epochs=200, validation_split = 0.2, verbose=0,\n  )","10002a35":"fig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(go.Scatter(\n    y=history.history['loss'],\n    mode='lines+markers',\n    name='training loss'\n), row=1, col=1)\n\nfig.add_trace(go.Scatter(\n    y=history.history['val_loss'],\n    mode='lines+markers',\n    name='validation loss'\n), row=1, col=1)\n\n\nfig.add_trace(go.Scatter(\n    y=history.history['mse'],\n    mode='lines+markers',\n    name='training mse'\n), row=1, col=2)\n\nfig.add_trace(go.Scatter(\n    y=history.history['val_mse'],\n    mode='lines+markers',\n    name='validation mse'\n), row=1, col=2)\n\nfig.update_xaxes(title_text='Epoch')\n\nfig.update_layout(\n    title_text=\"Training History Metrics\",\n    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1, xanchor=\"right\", x=1)\n)\n\nfig.show()","4c65083c":"housing_predictions_nn = model_nn.predict(housing_prepared)\nmse = mean_squared_error(housing_labels, housing_predictions_nn)\nrmse_log=mean_squared_log_error(abs(housing_labels), abs(housing_predictions_nn))\nrmse = np.sqrt(mse)\nprint(f\"mse: {mse}, rmse: {rmse}, rmse_log:{rmse_log}\")","6c024bfa":"housing_predictions = lrmodel.predict(housing_prepared)\nmse = mean_squared_error(housing_labels, housing_predictions)\nrmse_log=mean_squared_log_error(abs(housing_labels), abs(housing_predictions))\nrmse = np.sqrt(mse)\nprint(f\"Linar Regresion mse: {mse}, rmse: {rmse}, rmse_log:{rmse_log}\")","5b3f5143":"# Imports","5ad8c7f3":"# Load Data","ab0e3c80":"# Remove NaN","c22e2e98":"# Normalization","bc77b774":"# Check the data","ac1e8936":"# Handle outliers","7c0bd809":"## Parte B\n\nIntente mejorar la predicci\u00f3n de precios de las casas de boston con una red neuronal. Adjunte el link de kaggle y el la captura del score obtenido. (10 pts)\n\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques","5d913a1d":"# Model: Red Neuronal","1f29030e":"# Model: Linear Regression","115e1726":"# Filtrar los mejores valores","da32d2ed":"# Fit Model: Neural Network","a8b6ba0d":"# Measure the error (RMSE): Linear Regression","64fe64f4":"# Fit Model: Linear Regression","16398775":"# Tarea Semana 12"}}