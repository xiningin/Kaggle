{"cell_type":{"d3d58230":"code","16926b75":"code","0140a3a7":"code","3efd4e55":"code","51db1b42":"code","9c07651a":"code","3a149dd1":"code","bad4d4b9":"code","f7120801":"code","6a563200":"code","af84feb0":"code","9dd590f1":"code","0a577079":"code","3c17a2ed":"code","46d487ca":"code","101187ff":"code","e1a84794":"code","10fc8b5b":"code","df1fe8c4":"markdown","32f09a35":"markdown","1dca6950":"markdown","65a45049":"markdown","15a05e9c":"markdown","9e625c2e":"markdown"},"source":{"d3d58230":"import warnings\nwarnings.filterwarnings(\"ignore\") #Never displays warnings which match","16926b75":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nplt.style.use('fivethirtyeight')","0140a3a7":"from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","3efd4e55":"iris_ = load_iris()\niris = pd.DataFrame(iris_.data, columns = iris_.feature_names)\niris['species'] = iris_.target","51db1b42":"iris.head()","9c07651a":"iris.shape","3a149dd1":"X = iris.iloc[:, 0:4].values\ny = iris.iloc[:, 4].values","bad4d4b9":"# Feature Scaling\nsc = StandardScaler()\nX = sc.fit_transform(X)","f7120801":"pca = PCA(n_components=2)\nX_pca = pca.fit(X).transform(X)","6a563200":"X_pca.shape, X.shape","af84feb0":"colors = ['navy', 'turquoise', 'darkorange']\nlw=2\n\nfor color, i, target in zip(colors, [0, 1, 2], iris_.target_names):\n    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, alpha=.8, lw=lw,\n                label=target)\nplt.legend(loc='best', shadow=False, scatterpoints=1)\nplt.title('PCA of IRIS dataset')","9dd590f1":"X_train,X_test,y_train,y_test = train_test_split(X_pca,y,test_size=0.2, random_state = 0)\n\nmodel1 = RandomForestClassifier(max_depth=2,random_state=0,n_estimators =10)\nmodel1.fit(X_train,y_train)\npca_pred = model1.predict(X_test)","0a577079":"accuracy_score(y_test, pca_pred)","3c17a2ed":"lda = LinearDiscriminantAnalysis(n_components=2)\nX_lda = lda.fit(X, y).transform(X)","46d487ca":"colors = ['navy', 'turquoise', 'darkorange']\nlw = 2\n\nfor color, i, target in zip(colors, [0, 1, 2], iris_.target_names):\n    plt.scatter(X_lda[y == i, 0], X_lda[y == i, 1], alpha=.8, color=color, label=target)\nplt.legend(loc='best', shadow=False, scatterpoints=1)\nplt.title('LDA of IRIS dataset')","101187ff":"X_lda.shape, X.shape","e1a84794":"X_train, X_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, random_state=0)\n\nmodel2 = RandomForestClassifier(max_depth=2, random_state=0, n_estimators = 10)\n\nmodel2.fit(X_train, y_train)\nlda_pred = model2.predict(X_test)","10fc8b5b":"accuracy_score(y_test, lda_pred)","df1fe8c4":"## Performing PCA\n\n`Principal Component` PCs plays a vital role in Feature Extraction\n\nIn `Feature Selection` we take subset of the features, while in Feature Extraction we take the original features and map it to  a lower dimensional space and each feature obtained as a function of feature set.\n\n**PCA** is a dimensianlity reduction technique.\nAt the end of PCA, we get features of -\n* Large Variance\n    * such that features are able to distinguish between the different instances and\n* Uncorrelated\n    * features are not correlated to each other, such that they improves model.","32f09a35":"### Importing Desired Libraries","1dca6950":"We reduced feautures from 4 to 2.","65a45049":"## Cheers!!! LDA WINS OVER PCA","15a05e9c":"### Performing LDA\n\nWhile applying PCA for classification problem, two classes overlap and we loss class information which is very important for supervised learning. Here, we go for LDA.\n\nLDA uses two things for projecting to lower dimensional space.\n* Between-class distance\n    * Distance between the centroids of different classes\n* Within-class distance\n    * Accumulated distance of an instance to the centroid of its class\n\nThus, LDA finds most discriminant projection by\n* maximizing `between-class` distance,\n* minimzing `within-class` distance these leads to classes that are well seperated.","9e625c2e":"## Linear Discriminant Analysis over Principle Component Analysis Using IRIS Data"}}