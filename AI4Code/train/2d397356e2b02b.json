{"cell_type":{"4eb58e1b":"code","3053b201":"code","085798b7":"code","4de0f8d9":"code","dfe3c887":"code","2c3c226e":"code","9930eb67":"code","02e1f565":"code","01d57ba7":"code","64ae3936":"code","3a366038":"code","f67c9eb0":"code","7671b01e":"code","21e94c27":"code","59329b6b":"code","5d4daae1":"code","9683cf00":"code","b1cb7816":"code","4c6752d9":"code","81c130a2":"markdown","8da1dfff":"markdown","a2d42d40":"markdown","4781e41f":"markdown","06b56439":"markdown","c2e5bcd3":"markdown","4e8a29a0":"markdown","d3c0c402":"markdown","f0aaf505":"markdown"},"source":{"4eb58e1b":"import os\nimport glob\nfrom tqdm import tqdm_notebook as tqdm\nimport math\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom torchvision import transforms, utils\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nfrom sklearn.metrics import roc_auc_score\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3053b201":"import sys\nsys.path.append('..\/input\/efficientnetpyttorch3d\/EfficientNet-PyTorch-3D')\nfrom efficientnet_pytorch_3d import EfficientNet3D","085798b7":"path = '\/kaggle\/input\/rsna-miccai-brain-tumor-radiogenomic-classification'\ntrain_data = pd.read_csv(os.path.join(path, 'train_labels.csv'))\nprint('Num of train samples:', len(train_data))\ntrain_data.head()","4de0f8d9":"def dicom2array(path, voi_lut=True, fix_monochrome=True, remove_black_boundary=True):\n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to\n    # transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    if remove_black_boundary:\n        (x, y) = np.where(data > 0)\n        if len(x) > 0 and len(y) > 0:\n            x_mn = np.min(x)\n            x_mx = np.max(x)\n            y_mn = np.min(y)\n            y_mx = np.max(y)\n            if (x_mx - x_mn) > 10 and (y_mx - y_mn) > 10:\n                data = data[:,np.min(y):np.max(y)]\n    data = cv2.resize(data, (256, 256))\n    return data\n\ndef load_3d_dicom_images(scan_id, split = \"train\", channel_expand = True, remove_black_boundary=True):\n    \"\"\"\n    we will use some heuristics to choose the slices to avoid any numpy zero matrix (if possible)\n    \"\"\"\n    flair = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/FLAIR\/*.dcm\"))\n    t1w = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/T1w\/*.dcm\"))\n    t1wce = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/T1wCE\/*.dcm\"))\n    t2w = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/T2w\/*.dcm\"))\n    \n    \n    flair_img = np.array([dicom2array(a, remove_black_boundary = remove_black_boundary) for a in flair[len(flair)\/\/2 - 32:len(flair)\/\/2 + 32]]).T\n    \n    if len(flair_img) == 0:\n        flair_img = np.zeros((256, 256, 64))\n    elif flair_img.shape[-1] < 64:\n        n_zero = 64 - flair_img.shape[-1]\n        flair_img = np.concatenate((flair_img, np.zeros((256, 256, n_zero))), axis = -1)\n    #print(flair_img.shape)\n        \n    \n    \n    t1w_img = np.array([dicom2array(a, remove_black_boundary = remove_black_boundary) for a in t1w[len(t1w)\/\/2 - 32:len(t1w)\/\/2 + 32]]).T\n    \n    if len(t1w_img) == 0:\n        t1w_img = np.zeros((256, 256, 64))\n    elif t1w_img.shape[-1] < 64:\n        n_zero = 64 - t1w_img.shape[-1]\n        t1w_img = np.concatenate((t1w_img, np.zeros((256, 256, n_zero))), axis = -1)\n    #print(t1w_img.shape)\n    \n    \n    t1wce_img = np.array([dicom2array(a, remove_black_boundary = remove_black_boundary) for a in t1wce[len(t1wce)\/\/2 - 32:len(t1wce)\/\/2 + 32]]).T\n    \n    if len(t1wce_img) == 0:\n        t1wce_img = np.zeros((256, 256, 64))\n    elif t1wce_img.shape[-1] < 64:\n        n_zero = 64 - t1wce_img.shape[-1]\n        t1wce_img = np.concatenate((t1wce_img, np.zeros((256, 256, n_zero))), axis = -1)\n    #print(t1wce_img.shape)\n    \n    \n    t2w_img = np.array([dicom2array(a, remove_black_boundary = remove_black_boundary) for a in t2w[len(t2w)\/\/2 - 32:len(t2w)\/\/2 + 32]]).T\n    \n    if len(t2w_img) == 0:\n        t2w_img = np.zeros((256, 256, 64))\n    elif t2w_img.shape[-1] < 64:\n        n_zero = 64 - t2w_img.shape[-1]\n        t2w_img = np.concatenate((t2w_img, np.zeros((256, 256, n_zero))), axis = -1)\n    #print(t2w_img.shape)\n    \n    return np.concatenate((flair_img, t1w_img, t1wce_img, t2w_img), axis = -1) if not channel_expand else np.moveaxis(np.array((flair_img, t1w_img, t1wce_img, t2w_img)), 0, -1)","dfe3c887":"load_3d_dicom_images(\"00000\", channel_expand = False).shape","2c3c226e":"slices = load_3d_dicom_images(\"00000\", remove_black_boundary=False)\nprint(slices.shape)","9930eb67":"# doing a little more cleaning up\ns_slice = slices[:,:,0,0]\nplt.imshow(s_slice)\nplt.title(\"Lots of black pixels\")\nplt.show()\n(x, y) = np.where(s_slice > 0)\nns_slice = s_slice[np.min(x):np.max(x),np.min(y):np.max(y)]\nplt.title(\"Less black pixels\")\nplt.imshow(ns_slice)\nplt.show()","02e1f565":"# https:\/\/www.kaggle.com\/josepc\/rsna-effnet\/\n\nviews = ['FLAIR', 'T1w', 'T1wCE', 'T2w']\ndef load_imgs(idx):\n    slices = load_3d_dicom_images(idx)\n    imgs = {}\n    for i, view in enumerate(views):\n        imgs[view] = slices[:,:,:,i].swapaxes(0, -1)\n    return imgs\n\nfor i in range(10,32):\n    idx = str(i).zfill(5)\n    imgs = load_imgs(idx)","01d57ba7":"# the video play doesn't work, you can download it to view\n\nfrom IPython.display import HTML\nfrom base64 import b64encode\nimport matplotlib.animation as animation\n\ndef play(filename):\n    html = ''\n    video = open(filename,'rb').read()\n    src = 'data:video\/mp4;base64,' + b64encode(video).decode()\n    html += '<video width=500 controls autoplay loop><source src=\"%s\" type=\"video\/mp4\"><\/video>' % src \n    return HTML(html)\n\ndef create_video(imgs, output='\/kaggle\/working\/vis_video.mp4', duration=30, subplot=True, \n                frame_delay=200):\n    fig, ax = plt.subplots(figsize=(15, 10))\n    ims = []\n    if not subplot:\n        shape = imgs.shape[0]\n        for i in range(duration):\n            im = ax.imshow(imgs[i % shape], animated=True)\n            ims.append([im])\n        plt.close(fig)\n    else:\n        shapes = [imgs[views[0]].shape[0], imgs[views[1]].shape[0], \n                  imgs[views[2]].shape[0], imgs[views[3]].shape[0]]\n        fig, ax = plt.subplots(2,2, figsize=(10,10))\n        for k in range(duration):\n            im_ = []\n            for i in range(2):\n                for j in range(2):\n                    im = ax[i,j].imshow(imgs[views[2*i+j]][k % shapes[2*i+j]], animated=True)\n                    im_.append(im)\n                    ax[i,j].set_title(views[2*i+j])\n                    plt.close()\n            ims.append(im_)\n\n    ani = animation.ArtistAnimation(fig, ims, interval=frame_delay, blit=True, repeat_delay=1000)\n\n    ani.save(output)","64ae3936":"create_video(imgs, duration=60, subplot=True, frame_delay=300)\nplay('\/kaggle\/working\/vis_video.mp4')","3a366038":"# let's write a simple pytorch dataloader\n\n\nclass BrainTumor(Dataset):\n    def __init__(self, path = '\/kaggle\/input\/rsna-miccai-brain-tumor-radiogenomic-classification', split = \"train\", validation_split = 0.2):\n        # labels\n        train_data = pd.read_csv(os.path.join(path, 'train_labels.csv'))\n        self.labels = {}\n        brats = list(train_data[\"BraTS21ID\"])\n        mgmt = list(train_data[\"MGMT_value\"])\n        for b, m in zip(brats, mgmt):\n            self.labels[str(b).zfill(5)] = m\n            \n        if split == \"valid\":\n            self.split = split\n            self.ids = [a.split(\"\/\")[-1] for a in sorted(glob.glob(path + f\"\/train\/\" + \"\/*\"))]\n            self.ids = self.ids[:int(len(self.ids)* validation_split)] # first 20% as validation\n        elif split == \"train\":\n            self.split = split\n            self.ids = [a.split(\"\/\")[-1] for a in sorted(glob.glob(path + f\"\/{split}\/\" + \"\/*\"))]\n            self.ids = self.ids[int(len(self.ids)* validation_split):] # last 80% as train\n        else:\n            self.split = split\n            self.ids = [a.split(\"\/\")[-1] for a in sorted(glob.glob(path + f\"\/{split}\/\" + \"\/*\"))]\n            \n    \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        imgs = load_3d_dicom_images(self.ids[idx], self.split)\n        imgs = imgs - imgs.min()\n        imgs = (imgs + 1e-5) \/ (imgs.max() - imgs.min() + 1e-5)\n        \n        # imgs = imgs - imgs.mean()\n        # imgs = (imgs + 1e-5)\/(imgs.std() + 1e-5)\n        \n        if self.split != \"test\":\n            label = self.labels[self.ids[idx]]\n            return torch.tensor(imgs, dtype = torch.float32).permute(-1, 0, 1, 2), torch.tensor(label, dtype = torch.long)\n        else:\n            return torch.tensor(imgs, dtype = torch.float32).permute(-1, 0, 1, 2)","f67c9eb0":"# testing the dataloader\ntrain_dataset = BrainTumor()\ntrain_bs = 4\ntrain_loader = DataLoader(train_dataset, batch_size = train_bs, shuffle=True)\n\nval_dataset = BrainTumor(split = \"valid\")\nval_bs = 2\nval_loader = DataLoader(val_dataset, batch_size = val_bs, shuffle=True)","7671b01e":"for img, label in train_loader:\n    print(img.shape)\n    print(img.max())\n    print(img.mean())\n    print(img.min())\n    print(label.shape)\n    break\n\nfor img, label in val_loader:\n    print(img.shape)\n    print(label.shape)\n    break","21e94c27":"PATH = \"..\/input\/rsna-efficientnet3db0\/best_roc_0.29_loss_1826.83.pt\" # using a pretrained weight\n\nmodel = EfficientNet3D.from_name(\"efficientnet-b0\", override_params={'num_classes': 2}, in_channels=4)\nmodel.load_state_dict(torch.load(PATH))\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(),lr = 0.0007, weight_decay=0.08)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.25)\nn_epochs = 10","59329b6b":"print(model)\nmodel(torch.randn(1, 4, 256, 256, 64))","5d4daae1":"# helper\ndef one_hot(arr):\n    return [[1, 0] if a_i == 0 else [0, 1] for a_i in arr]","9683cf00":"# let's train\ngpu = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(gpu)\n\ntrain_loss = []\nval_loss = []\ntrain_roc = []\nval_roc = []\nbest_roc = 0.0\n\nfor epoch in range(n_epochs):  # loop over the dataset multiple times\n    y_all = []\n    outputs_all = []\n    running_loss = 0.0\n    roc = 0.0\n    \n    model.train()\n    for i, data in tqdm(enumerate(train_loader, 0)):\n        x, y = data\n        \n        # x = torch.unsqueeze(x, dim = 1)\n        x = x.to(gpu)\n        y = y.to(gpu)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(x)\n        loss = criterion(outputs, y)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        y_all.extend(y.tolist())\n        outputs_all.extend(outputs.tolist())\n    \n    roc += roc_auc_score(one_hot(y_all), outputs_all) \/ train_bs\n    print(f\"epoch {epoch+1} train loss: {running_loss} train roc: {roc}\")\n    \n    train_loss.append(running_loss)\n    train_roc.append(roc)\n\n    y_all = []\n    outputs_all = []\n    running_loss = 0.0\n    roc = 0.0 \n    \n    model.eval()\n    for i, data in tqdm(enumerate(val_loader, 0)):\n\n        x, y = data\n        \n        # x = torch.unsqueeze(x, dim = 1)\n        x = x.to(gpu)\n        y = y.to(gpu)\n\n        # forward\n        outputs = model(x)\n        loss = criterion(outputs, y)\n\n        # print statistics\n        running_loss += loss.item()\n        y_all.extend(y.tolist())\n        outputs_all.extend(outputs.tolist())\n    \n    roc += roc_auc_score(one_hot(y_all), outputs_all) \/ val_bs\n    scheduler.step(running_loss)\n        \n    print(f\"epoch {epoch+1} val loss: {running_loss} val roc: {roc}\")\n    \n    val_loss.append(running_loss)\n    val_roc.append(roc)\n    \n    if roc > best_roc:\n        best_roc = roc\n        torch.save(model.state_dict(), f'best_roc_{round(roc, 2)}_loss_{round(running_loss, 2)}.pt')","b1cb7816":"plt.plot(train_loss, label = 'train loss')\nplt.plot(val_loss, label = 'val loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend(['train loss', 'val loss'])\nplt.show()\n\nplt.plot(train_roc, label = 'train roc')\nplt.plot(val_roc, label = 'val roc')\nplt.xlabel('epochs')\nplt.ylabel('roc auc')\nplt.legend(['train roc', 'val roc'])\nplt.show()","4c6752d9":"submission = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv\")\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","81c130a2":"### **Model: EfficientNet-3D B0**","8da1dfff":"### **Inspecting Labels**","a2d42d40":"### **Data Loader**","4781e41f":"### **Importing EfficientNet-3D**","06b56439":"# RSNA-MICCAI Brain Tumor Radiogenomic Classificationn - **An approach with PyTorch EfficientNet 3D**\n\n## **Problem Description**:\n\nThere are structural multi-parametric MRI (mpMRI) scans for different subjects, in DICOM format. The exact mpMRI scans included are:\n\n* Fluid Attenuated Inversion Recovery (FLAIR)\n* T1-weighted pre-contrast (T1w)\n* T1-weighted post-contrast (T1Gd)\n* T2-weighted (T2)\n\n`train_labels.csv` - file contains the target **MGMT_value** for each subject in the training data **(e.g. the presence of MGMT promoter methylation)**.\n\nSo, it's a binary classification problem.\n\n## **An EfficientNet3D solution**:\n\n* For each patient, we consider 4 sequences (FLAIR, T1w, T1Gd, T2), and for each of those sequences we take 64 slices from the middle. We resize the slices in shape (256, 256).\n\n* Construct an efficientnet-3d in pytorch with input shape (256, 256, 256) or (4, 256, 256, 64).\n\n* Perform binary classification.\n\n\n### \u26a1 **Inference kernel:** https:\/\/www.kaggle.com\/furcifer\/torch-effnet3d-for-mri-no-inference\/\n","c2e5bcd3":"### **Training**","4e8a29a0":"### **Importing libraries**","d3c0c402":"### **MRI Slice Loading\/Processing**","f0aaf505":"### **Visualization**"}}