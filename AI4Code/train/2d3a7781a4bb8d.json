{"cell_type":{"91846df9":"code","4d8b2d3a":"code","d526c090":"code","d3e75d1a":"code","60759776":"code","26d7c1db":"code","df41316b":"code","f3916b76":"code","4fff446f":"code","eb12d0b0":"code","dc028ada":"code","e744cc43":"code","897ae950":"code","219dd13e":"code","758c1b1d":"code","d09c6e62":"code","f73a1f42":"code","1327ecf4":"code","c5180138":"code","0eacc2ac":"code","ff7c4eb4":"code","8abbf186":"code","ce64b6ca":"code","8b109aa4":"code","4be9cbd5":"code","2a0700fe":"code","ad21bfbf":"code","c799dab1":"code","90edcd6f":"code","23e3b2e9":"code","89882997":"code","757537bc":"markdown","a1448d42":"markdown","9cd369e7":"markdown","b5cd8812":"markdown","01b1e3d1":"markdown","ea39ac6d":"markdown","13be41a8":"markdown","be8b1012":"markdown","9c71cf2a":"markdown","3941db93":"markdown","fda3fb13":"markdown","0d2cd0ea":"markdown"},"source":{"91846df9":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport sklearn.model_selection as ms\nimport sklearn.metrics as m\nimport sklearn.tree as tree\nimport sklearn.ensemble as ensemble\nimport sklearn.svm as svm\nimport sklearn.linear_model as lm\nimport sklearn.preprocessing as pp\nimport sklearn.compose as compose\nimport sklearn.pipeline as pipe\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4d8b2d3a":"data = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\ndata.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\\\n           'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2', 'CLIENTNUM'], inplace=True, axis=1)","d526c090":"data.info()","d3e75d1a":"data.iloc[:, :11].head(5)","60759776":"data.iloc[:, 11:].head(5)","26d7c1db":"data['Attrition_Flag'].value_counts(normalize=True) * 100","df41316b":"data.isnull().sum()","f3916b76":"# display all unique values from all the columns\nfor col in data.columns:\n    print(col)\n    print(data[col].unique())\n    print()","4fff446f":"data.columns","eb12d0b0":"data.iloc[:, :11].head(1)","dc028ada":"data.iloc[:, 11:].head(1)","e744cc43":"data['Attrition_Flag'].replace({'Attrited Customer': 1, 'Existing Customer': 0}, inplace=True)\ndata['Gender'].replace({'M': 1, 'F': 0}, inplace=True) # do not need to one-hot encode","897ae950":"cat_attribute_2b_encoded = ['Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']\n\nnum_attribute_2b_transform = [i for i in data.columns if i not in cat_attribute_2b_encoded]\nnum_attribute_2b_transform = [i for i in num_attribute_2b_transform if i not in ['Attrition_Flag', 'Gender']]","219dd13e":"print(f'No of numercial attribute: {len(num_attribute_2b_transform)}')\nprint(f'No of categorical attribute: {len(cat_attribute_2b_encoded)}')","758c1b1d":"y = data['Attrition_Flag'].copy()\n\nX = data.copy()\nX.drop('Attrition_Flag', inplace=True, axis=1)","d09c6e62":"num_pipeline = pipe.Pipeline([\n                            ('scaler', pp.StandardScaler())\n])\n\n\nfull_pipeline = compose.ColumnTransformer([\n                            ('num', num_pipeline, num_attribute_2b_transform),\n                            ('cat', pp.OneHotEncoder(), cat_attribute_2b_encoded)\n                                        ], remainder='passthrough', sparse_threshold=0)","f73a1f42":"X_transform = full_pipeline.fit_transform(X)\n\nprint(X_transform.shape)\nprint(full_pipeline.named_transformers_)","1327ecf4":"cat_transform_feature_names = list(full_pipeline.named_transformers_['cat'].get_feature_names())\nlen(cat_transform_feature_names)","c5180138":"X_train_val, X_test, y_train_val, y_test = ms.train_test_split(\\\n                                    X_transform, y, train_size=0.75, random_state=42, stratify=y, shuffle=True)\nX_train, X_validation, y_train, y_validation = ms.train_test_split(\\\n                                    X_train_val, y_train_val, train_size=0.75, random_state=42, stratify=y_train_val, shuffle=True)","0eacc2ac":"print(f'Total number of Instances: {X_transform.shape[0]}')\nprint(f'Size of Training Dataset: {X_train.shape[0]}')\nprint(f'Size of Validation Dataset: {X_validation.shape[0]}')\nprint(f'Size of Testing Dataset: {X_test.shape[0]}')","ff7c4eb4":"rf_clf = ensemble.RandomForestClassifier(random_state=42)\ndt_clf = tree.DecisionTreeClassifier(random_state=42)\next_clf = ensemble.ExtraTreesClassifier(random_state=42)\ngb_clf = ensemble.GradientBoostingClassifier(random_state=42)\n\nvoting_classifier = ensemble.VotingClassifier([\n                    ('rf_clf', ensemble.RandomForestClassifier(random_state=42)),\n                    ('dt_clf', tree.DecisionTreeClassifier(random_state=42)),\n                    ('ext_clf', ensemble.ExtraTreesClassifier(random_state=42)),\n                    ('gb_clf', ensemble.GradientBoostingClassifier(random_state=42))\n                    ], voting='hard')\n\nestimators = [rf_clf, dt_clf, ext_clf, gb_clf, voting_classifier]","8abbf186":"cv = ms.RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n\nfor estimator in estimators:\n    estimator.fit(X_train, y_train)\n    cv_accuracy = ms.cross_val_score(estimator, X_train, y_train, cv=cv, n_jobs=-1, scoring='accuracy')\n    cv_recall = ms.cross_val_score(estimator, X_train, y_train, cv=cv, n_jobs=-1, scoring='recall')\n    cv_f1 = ms.cross_val_score(estimator, X_train, y_train, cv=cv, n_jobs=-1, scoring='f1')\n    \n    print(estimator.__class__.__name__)\n    print(f'Avg Accuracy: {round(np.mean(cv_accuracy) * 100,2)}')\n    print(f'Std Accuracy: {round(np.std(cv_accuracy) * 100,2)}')\n    \n    print(f'Avg Recall: {round(np.mean(cv_recall) * 100,2)}')\n    print(f'Std Rcall: {round(np.std(cv_recall) * 100,2)}')\n    \n    print(f'Avg F1: {round(np.mean(cv_f1) * 100,2)}')\n    print(f'Std F1: {round(np.std(cv_f1) * 100,2)}')\n    print()","ce64b6ca":"for estimator in estimators:\n    prediction = estimator.predict(X_validation)\n\n    print(estimator.__class__.__name__)\n    print(f'Accuracy score: {round(m.accuracy_score(y_validation, prediction) * 100,2)}')\n    print(f'Precision score: {round(m.precision_score(y_validation, prediction) * 100,2)}')\n    print(f'Recall score: {round(m.recall_score(y_validation, prediction) * 100,2)}')\n    print(f'F1 score: {round(m.f1_score(y_validation, prediction) * 100,2)}')\n    print()","8b109aa4":"# From a recall perspective, ExtraTreesClassifier does not seems to be performing well as compared to the other models\n# Let's remove ExtraTreesClassifier from the ensemble VotingClassifier and see if the performance of the voting classifier improves\n\nnew_voting_classifier = ensemble.VotingClassifier([\n                    ('rf_clf', ensemble.RandomForestClassifier(random_state=42)),\n                    ('dt_clf', tree.DecisionTreeClassifier(random_state=42)),\n                    ('gb_clf', ensemble.GradientBoostingClassifier(random_state=42))\n                    ], voting='hard')\n\nnew_voting_classifier_list = [new_voting_classifier]\n\ncv = ms.RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\nprint('Performance on Training Data')\nprint()\nfor estimator in new_voting_classifier_list:\n    estimator.fit(X_train, y_train)\n    cv_accuracy = ms.cross_val_score(estimator, X_train, y_train, cv=cv, n_jobs=-1, scoring='accuracy')\n    cv_recall = ms.cross_val_score(estimator, X_train, y_train, cv=cv, n_jobs=-1, scoring='recall')\n    cv_f1 = ms.cross_val_score(estimator, X_train, y_train, cv=cv, n_jobs=-1, scoring='f1')\n    \n    print(estimator.__class__.__name__)\n    print(f'Avg Accuracy: {round(np.mean(cv_accuracy) * 100,2)}')\n    print(f'Std Accuracy: {round(np.std(cv_accuracy) * 100,2)}')\n    print(f'Avg Recall: {round(np.mean(cv_recall) * 100,2)}')\n    print(f'Std Rcall: {round(np.std(cv_recall) * 100,2)}')\n    print(f'Avg F1: {round(np.mean(cv_f1) * 100,2)}')\n    print(f'Std F1: {round(np.std(cv_f1) * 100,2)}')\n    print()\n    \n    print('Performance on Validation Dataset')\n    prediction = estimator.predict(X_validation)\n    print(f'Accuracy score: {round(m.accuracy_score(y_validation, prediction) * 100,2)}')\n    print(f'Precision score: {round(m.precision_score(y_validation, prediction) * 100,2)}')\n    print(f'Recall score: {round(m.recall_score(y_validation, prediction) * 100,2)}')\n    print(f'F1 score: {round(m.f1_score(y_validation, prediction) * 100,2)}')","4be9cbd5":"gb_clf.get_params()","2a0700fe":"parameter_grid = [\n                {'n_estimators': [50,75,100,125,150],\n                'learning_rate': np.arange(0.1,1.0,0.1),\n                }, \n                {'ccp_alpha': np.arange(0.1,1.0,0.1)   \n                },\n                {'max_leaf_nodes': [25,50,75,100],\n                 'min_samples_split': [25,50,75,100],\n                 'min_samples_leaf': [25,50,75,100]\n                }, \n                {'max_depth': [3,5,10,20,25,50,75,100],\n                 'max_features': [None, 5, 10,15,20,25,36]\n                }\n                ]","ad21bfbf":"gb_clf_grid_search_cv = ms.GridSearchCV(gb_clf, parameter_grid, scoring=\"recall\", cv=3, return_train_score= True)\ngb_clf_grid_search_cv.fit(X_train, y_train)","c799dab1":"gb_clf_grid_search_cv.best_estimator_","90edcd6f":"gb_clf_grid_search_cv.best_score_","23e3b2e9":"best_gb_clf = gb_clf_grid_search_cv.best_estimator_\nbest_gb_clf.fit(X_train_val, y_train_val)","89882997":"# Performance on testing data\n\ntraining_prediction = best_gb_clf.predict(X_test)\n\nprint(m.accuracy_score(y_test, training_prediction) * 100)\nprint(m.precision_score(y_test, training_prediction) * 100)\nprint(m.recall_score(y_test, training_prediction) * 100)\nprint(m.f1_score(y_test, training_prediction) * 100)","757537bc":"# Data Processing","a1448d42":"# Candidates Models & Ensemble (BASELINE)","9cd369e7":"# Best Estimator Training and Performance on Test Data\n* Best Estimator will be trained on the full training set\n* We then proceed to test it on the testing data","b5cd8812":"# Performance on Training Data","01b1e3d1":"# Result Commentary on Models from Training and Validation Dataset\n* While the recall performance of the new voting classifier on the validation dataset improves from 77.1% to 85.9%, it is still lower than the performance result from GB Classifier\n\n\n* Based on the validation dataset, GB Classifier seems to the best performing estimator\n    * Highest recall score of 86.9% and 83.8% in validation and training dataset\n\n\n* Let's fine-tuned the GB Classifier model \n\n                                                             \n\n| Models              \t| Recall Score on Training Set \t| Recall Score on Validation Set \t|\n|---------------------\t|------------------------------\t|--------------------------------\t|\n| RandomForest        \t| 75.11                        \t| 80.66                          \t|\n| DecisionTree        \t| 79.12                        \t| 81.31                          \t|\n| ExtraTrees          \t| 58.07                        \t| 63.28                          \t|\n| GradientBoosting    \t| 83.79                        \t| 86.89                          \t|\n| Old Voting Ensemble \t| 70.13                        \t| 77.05                          \t|\n| New Voting Ensemble \t| 81.97                        \t| 85.9                           \t|","ea39ac6d":"# Performance on Validation Data","13be41a8":"# Basic EDA","be8b1012":"# Transformation Pipeline","9c71cf2a":"# New Voting Model and Performance on Training and Validation Data","3941db93":"# Objective\n* To develop a model that can identify all actual positive case, i.e. 100% recall rate is the ideal case\n* False positive is okay \n* False negative is NOT okay","fda3fb13":"# Splitting Data","0d2cd0ea":"# Hyperparameter Tuning"}}