{"cell_type":{"74d79b97":"code","a7001d54":"code","afbb1d29":"code","41deb562":"code","d283434e":"code","d876a8c4":"code","3e164da5":"code","1c54a36a":"code","65fef55e":"code","8e71b2a6":"code","e8bb6da7":"code","5afe06f1":"code","d3ab3e03":"code","64be2b4e":"code","5dc32cb1":"code","ac391b75":"code","aafb2b9b":"code","9f0cf9ae":"code","c792a561":"code","66ea6eda":"code","6fa87119":"code","cfe8d7c0":"code","4b0960f1":"code","70fe9b32":"code","49422166":"code","5c93195f":"code","33b95a74":"code","4b2580aa":"code","3d1f7f71":"code","7d10b7ac":"code","0190aaef":"code","63a3cab8":"code","e0160274":"code","56cf1c87":"code","68e4f148":"code","f2f41175":"code","22897bcd":"code","3989fbe9":"code","66baff07":"code","57bcae78":"code","9bdcf875":"code","06469e10":"code","4540b32f":"code","b8ef9cbf":"code","8d7d8d45":"code","462ea096":"code","eb899689":"code","f7002c15":"code","ab7901f4":"code","77ebada3":"code","c0037790":"code","13d06f0a":"markdown","b8ab4f0f":"markdown","2a032f7d":"markdown","a2defbd8":"markdown","65a9897b":"markdown","edaa8ae3":"markdown","a6bc369b":"markdown","475f2c82":"markdown","79049acc":"markdown","021139bc":"markdown","3ca55c1b":"markdown","9b3c8ad8":"markdown","ff87b9e8":"markdown","33d17cf4":"markdown","2a42bfce":"markdown","eba5f4a1":"markdown","5e97c020":"markdown","0b1a8808":"markdown","7c6d7c53":"markdown","9a7b7b46":"markdown","beb0c93f":"markdown","e890759f":"markdown","814baa2d":"markdown","eceb8002":"markdown","7aa5a128":"markdown","c73ebc0f":"markdown","37c4d2fc":"markdown"},"source":{"74d79b97":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport logging\n\nfrom sklearn import preprocessing, impute\n\nplt.style.use('ggplot')","a7001d54":"random_state = 42\n# np.random.seed = random_state\nrng = np.random.default_rng(random_state)","afbb1d29":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","41deb562":"fp = '\/kaggle\/input\/song-popularity-prediction\/'\n\ndf = pd.read_csv(fp+'train.csv', index_col=0)\ndf_sub = pd.read_csv(fp+'test.csv', index_col=0)\ndf_sample = pd.read_csv(fp+'sample_submission.csv', index_col=0)\n\ndf.info()\ndf_sub.info()","d283434e":"print(df.apply(lambda x: x.nunique()))\ndf.describe()","d876a8c4":"col_y = 'song_popularity'\n\nX = df.copy()\ny = X.pop(col_y)\n\ncol_cat = ['key', 'audio_mode', 'time_signature']\ncol_num = X.drop(columns=col_cat).columns","3e164da5":"X_clean = X.dropna(how='any')\ny_clean = y.loc[~X.isna().any(axis=1)]","1c54a36a":"fig, axs = plt.subplots(len(col_cat), figsize=(6, len(col_cat)*4))\n\nfor col, ax in zip(col_cat, axs):\n    sns.histplot(X[col], ax=ax)","65fef55e":"y.plot.hist()","8e71b2a6":"sns.histplot(x=y, stat='probability', color='r')\nsns.histplot(x=y_clean, stat='probability', color='b')\n\nfig, axs = plt.subplots(len(col_cat), figsize=(6, len(col_cat)*4))\n\nfor col, ax in zip(col_cat, axs):\n    sns.histplot(x=X[col], ax=ax, stat='probability', color='r')\n    sns.histplot(x=X_clean[col], ax=ax, stat='probability', color='b')","e8bb6da7":"fig, axs = plt.subplots(len(col_num), 2,\n                        figsize=(10, len(df.columns)*4))\n\nfor i, col in enumerate(col_num):\n    sns.histplot(X[col], ax=axs[i, 0])\n    sm.qqplot(X[col].dropna(), line=\"s\", ax=axs[i, 1], fmt='b')\n    axs[i, 1].set_title(col)","5afe06f1":"# def song_duration_ms_scaling(x):\n#     x = x \/ 60_000\n#     if x < 1:\n        \n\n# _ = sm.qqplot(\n#     X['song_duration_ms'].dropna().apply(\n#         lambda x: np.log1p(x)),\n#     line=\"s\", fmt='b'\n# )","d3ab3e03":"X['song_duration_ms'].dropna().apply(lambda x: x\/60000).plot.hist()\nplt.figure()\nX['song_duration_ms'].dropna().apply(lambda x: np.log1p(x\/60000)).plot.hist()","64be2b4e":"_ = sm.qqplot(\n    X['loudness'].dropna().apply(\n        lambda x: (-x)),\n    line=\"s\", fmt='b'\n)","5dc32cb1":"inv_sigmoid = lambda x: np.log(x \/ (1-x))\n\n_ = sm.qqplot(\n    preprocessing.power_transform(\n        X[['danceability']].dropna().apply(inv_sigmoid)\n    )[:, 0],\n    line=\"s\", fmt='b'\n)","ac391b75":"col_sig = [\n    'acousticness',\n    'danceability',\n    'energy',\n    'instrumentalness',\n    'liveness',\n    'speechiness',\n    'audio_valence',\n]\n\ncol_pow = [\n    'song_duration_ms',\n    'tempo',\n]\n\ndef scale_data(X):\n    inv_sigmoid = lambda x: np.log(x \/ (1-x))\n    \n    X = X.copy()\n    for col in col_sig:\n        X[col] = preprocessing.minmax_scale(X[col], feature_range=(0+1e-6, 1-1e-6))\n        X[col] = X[col].apply(inv_sigmoid)\n    X[col_pow+col_sig] = preprocessing.power_transform(X[col_pow+col_sig])\n    X['loudness'] = X['loudness'].apply(lambda x: np.log1p(-x))\n    return X\n    \nX_proc = scale_data(X_clean)","aafb2b9b":"fig, axs = plt.subplots(len(col_num), 2,\n                        figsize=(10, len(df.columns)*4))\n\nfor i, col in enumerate(col_num):\n    sns.histplot(X_proc[col], ax=axs[i, 0])\n    sm.qqplot(X_proc[col].dropna(), line=\"s\", ax=axs[i, 1], fmt='b')\n    axs[i, 1].set_title(col)","9f0cf9ae":"from sklearn.decomposition import PCA\n\npca = PCA(random_state=random_state).fit(X_proc)\n\nX_pca = pd.DataFrame(pca.transform(X_clean), index=X_proc.index)\n\npca_comp = pd.DataFrame(pca.components_, index=X.columns)\npca_comp.style.background_gradient(\n    vmin=-1, vmax=1, cmap=sns.color_palette(\"vlag\", as_cmap=True))","c792a561":"sns.clustermap(X_clean.corr(), center=0, cmap=\"vlag\")","66ea6eda":"row_colors = dict(zip(y.unique(), 'kw'))\nsns.clustermap(X_proc, row_colors=y_clean.map(row_colors))","6fa87119":"from umap import UMAP\nimport umap.plot\nfrom sklearn.manifold import TSNE\n\nproj = UMAP().fit_transform(X_proc)\n# proj = TSNE().fit_transform(X_proc)","cfe8d7c0":"# umap.plot.points(mapper, labels=y_clean)\n# umap.plot.points(mapper, labels=X_clean['time_signature'])\n\nsns.scatterplot(x=proj[:, 0], y=proj[:, 1], hue=X_clean['instrumentalness'] > 0.01)","4b0960f1":"from sklearn.feature_selection import mutual_info_classif\n\ndef test_mi(X, y):\n    mi = mutual_info_classif(X, y,\n                             discrete_features=X.columns.isin(col_cat),\n                             random_state=random_state)\n    return (pd.Series(mi, index=X.columns)\n            .sort_values(ascending=False)\n            .to_frame()\n            .style.bar())","70fe9b32":"test_mi(X_clean, y_clean)","49422166":"test_mi(X_proc, y_clean)","5c93195f":"N_SPLITS = 5","33b95a74":"missing_ind = impute.MissingIndicator(features='missing-only')\nmissing_values = missing_ind.fit_transform(X)\n\nmissing_values = pd.DataFrame(missing_values, index=X.index)","4b2580aa":"from sklearn.model_selection import StratifiedKFold, PredefinedSplit, train_test_split\n\n\n# all categorical except `key` because it contains too many classes\n# (split of less than 1 combination)\ncol_strat = [col_y, 'audio_mode', 'time_signature']\n\nstrat, _ = (\n    df[['audio_mode', 'time_signature', col_y]]\n    .join(missing_values.any(axis=1).rename('missing'))\n    .astype('str')\n    .apply(lambda row: '_'.join(row), axis=1)\n    .factorize()\n)\nstrat = pd.Series(strat, index=df.index)\n\n\n# Split dev\/test\nX_dev, X_test, y_dev, y_test = train_test_split(\n    X, y, test_size=0.1, random_state=random_state, shuffle=True, stratify=strat)\n\ndev_index = X.index.isin(X_dev.index).astype(int)\ntest_split = PredefinedSplit(dev_index)\n\nstrat_dev = strat.loc[X_dev.index]\nstrat_test = strat.loc[X_test.index]\n\n\n# Split train\/valid\ndef make_folds(X, strat):\n    strat_folds = StratifiedKFold(n_splits=N_SPLITS,\n                                  shuffle=True,\n                                  random_state=random_state)\n\n    cv_fold = np.empty([len(X), 1], dtype=np.int32)\n\n    for i, (_, test_index) in enumerate(strat_folds.split(X, strat)):\n        cv_fold[test_index] = i\n\n    return PredefinedSplit(cv_fold)\n    \ncv_split = make_folds(X_dev, strat_dev)\nfull_split = make_folds(X, strat)","3d1f7f71":"# Results\n[idx for idx, _ in test_split.split()], [idx for idx, _ in cv_split.split()]","7d10b7ac":"cmap_data = plt.cm.Paired\ncmap_cv = plt.cm.coolwarm\n\ndef plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n\n    # Generate the training\/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n        # Fill in indices with the training\/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(\n            range(len(indices)),\n            [ii + 0.5] * len(indices),\n            c=indices,\n            marker=\"_\",\n            lw=lw,\n            cmap=cmap_cv,\n            vmin=-0.2,\n            vmax=1.2,\n        )\n\n    # Plot the data classes and groups at the end\n    ax.scatter(\n        range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data\n    )\n\n    ax.scatter(\n        range(len(X)), [ii + 2.5] * len(X), c=group, marker=\"_\", lw=lw, cmap=cmap_data\n    )\n\n    # Formatting\n    yticklabels = list(range(n_splits)) + [\"class\", \"group\"]\n    ax.set(\n        yticks=np.arange(n_splits + 2) + 0.5,\n        yticklabels=yticklabels,\n        xlabel=\"Sample index\",\n        ylabel=\"CV iteration\",\n        ylim=[n_splits + 2.2, -0.2],\n        xlim=[0, 100],\n    )\n    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=15)\n    return ax\n\nfig, ax = plt.subplots(figsize=(10, 10))\nplot_cv_indices(test_split, X, strat, group=None, ax=ax, n_splits=2)\nax.set_title('Preparation - dev\/test split')\n\n# fig, ax = plt.subplots(figsize=(10, 10))\n# plot_cv_indices(strat_folds, X_dev, strat_dev, group=None, ax=ax, n_splits=N_SPLITS)\n# ax.set_title('Preparation - Stratification CV')\n\nfig, ax = plt.subplots(figsize=(10, 10))\nplot_cv_indices(cv_split, X_dev, y_dev, group=None, ax=ax, n_splits=N_SPLITS)\nax.set_title('Predefined CV splits')\n\nfig, ax = plt.subplots(figsize=(10, 10))\nplot_cv_indices(full_split, X, y, group=None, ax=ax, n_splits=N_SPLITS)\nax.set_title('Full split for submission model')\n\nplt.show()","0190aaef":"col_sig = [\n    'acousticness',\n    'danceability',\n    'energy',\n    'instrumentalness',\n    'liveness',\n    'speechiness',\n    'audio_valence',\n]\n\ncol_pow = [\n    'song_duration_ms',\n    'tempo',\n]\n\n# For comparison between pipeline and previous processing code:\n\n# def scale_data(X):\n#     inv_sigmoid = lambda x: np.log(x \/ (1-x))\n    \n#     X = X.copy()\n#     for col in col_sig:\n#         X[col] = preprocessing.minmax_scale(X[col], feature_range=(0+1e-6, 1-1e-6))\n#         X[col] = X[col].apply(inv_sigmoid)\n#     X[col_pow+col_sig] = preprocessing.power_transform(X[col_pow+col_sig])\n#     X['loudness'] = X['loudness'].apply(lambda x: np.log1p(-x))\n#     return X","63a3cab8":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.experimental import enable_hist_gradient_boosting\n\nfrom sklearn.ensemble import HistGradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom lightgbm import LGBMClassifier\n\n# Model Pipeline\ninv_sigmoid = Pipeline([\n    ('0', preprocessing.MinMaxScaler(feature_range=(1e-3, 1-1e-3))),\n    ('1', FunctionTransformer(lambda x: np.log(x \/ (1-x)))),\n    ('2', preprocessing.PowerTransformer())\n])\n\ntransformer = ColumnTransformer([\n    ('inv_sigmoid', inv_sigmoid, col_sig),\n    ('scale', preprocessing.PowerTransformer(), col_pow),\n    ('db_to_num', FunctionTransformer(lambda x: np.log1p(-x)), ['loudness']),\n], remainder='passthrough')\n\npipe = Pipeline([\n    ('transform', transformer),\n    ('imputer', impute.SimpleImputer(strategy='median')),\n    ('model', LogisticRegression())\n])\n\nparam_grid = [{\n    'model': [LogisticRegression(),\n              HistGradientBoostingClassifier(),\n              LGBMClassifier(),\n              ExtraTreesClassifier()],\n    'imputer': [impute.SimpleImputer(strategy='median'),\n                impute.SimpleImputer(strategy='mean'),\n               ],#impute.KNNImputer()],\n    'transform__inv_sigmoid__0': ['passthrough'],\n    'transform__inv_sigmoid__1': ['passthrough'],\n},{\n    'model': [LogisticRegression(),\n              HistGradientBoostingClassifier(),\n              LGBMClassifier(),\n              ExtraTreesClassifier()],\n    'imputer': [impute.SimpleImputer(strategy='median'),\n                impute.SimpleImputer(strategy='mean'),\n                ],#impute.KNNImputer()],\n},{\n    'model': [HistGradientBoostingClassifier(), LGBMClassifier()],\n    'imputer': ['passthrough',\n                impute.IterativeImputer(random_state=random_state),\n                impute.IterativeImputer(random_state=random_state, add_indicator=True)],\n},{\n    'model': [HistGradientBoostingClassifier(), LGBMClassifier()],\n    'imputer': ['passthrough'],\n    'transform__inv_sigmoid__0': ['passthrough'],\n    'transform__inv_sigmoid__1': ['passthrough'],\n}]\n\ngscv = GridSearchCV(\n    estimator=pipe,\n    param_grid=param_grid,\n    scoring=['roc_auc', 'neg_log_loss'],\n    n_jobs=4,\n    refit='roc_auc',\n    cv=cv_split,\n    verbose=1,\n    error_score='raise',\n    return_train_score=True\n)\n\n_ = gscv.fit(X_dev, y_dev)","e0160274":"r = pd.DataFrame(gscv.cv_results_)\\\n    .sort_values('rank_test_roc_auc')\n    #.apply(lambda x: x.abs() if np.issubdtype(x.dtype, np.number) else x)\nr.filter(regex='mean|param_').style.bar(\n    align='zero', color=['#5fba7d', '#d65f5f'])","56cf1c87":"from sklearn.model_selection import cross_val_score\n\nmodel = gscv.best_estimator_\nmodel.set_params(model__max_iter=1_000)\n\nprint(f\"full cv score: {cross_val_score(model, X, y, cv=full_split, scoring='roc_auc')}\")\nprint(f\"dev\/test score: {cross_val_score(model, X, y, cv=test_split, scoring='roc_auc')}\")\n\n# _ = model.fit(X, y)","68e4f148":"import optuna\nimport lightgbm as lgb\nfrom optuna.integration import lightgbm as opt_lgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","f2f41175":"X_lgb = transformer.fit_transform(X)\n\nprint(col_cat)\npd.DataFrame(X_lgb).describe()","22897bcd":"dtrain = lgb.Dataset(X_lgb, label=y, categorical_feature= [10, 11, 12],)\n\nparams = {\n    \"objective\": \"binary\",\n    \"num_iterations\": 1_000,\n    \"metric\": \"auc\",\n    \"verbosity\": -100,\n    #\"verbose_eval\": -1,\n    \"boosting_type\": \"gbdt\",\n}\n\ntuner = opt_lgb.LightGBMTunerCV(\n    params,\n    dtrain,\n    folds=full_split,\n    return_cvbooster=True,\n    optuna_seed=random_state,\n    #verbosity=False,\n    verbose_eval=False,\n    show_progress_bar=False,\n    #early_stopping_rounds=100,\n    callbacks=[lgb.early_stopping(100, verbose=False)], #lgb.log_evaluation(0)],\n)\ntuner.run()\n\nlgb_boosters = tuner.get_best_booster().boosters\n\nprint(\"Best score:\", tuner.best_score)\nprint('Best iteration:', tuner.get_best_booster().best_iteration)\ntuner.best_params","3989fbe9":"dtrain_raw = lgb.Dataset(X, label=y, categorical_feature=col_cat)\n\nparams = {\n    \"objective\": \"binary\",\n    \"num_iterations\": 1_000,\n    \"metric\": \"auc\",\n    \"verbosity\": -100,\n    #\"verbose_eval\": -1,\n    \"boosting_type\": \"gbdt\",\n}\n\ntuner_raw = opt_lgb.LightGBMTunerCV(\n    params,\n    dtrain_raw,\n    folds=full_split,\n    return_cvbooster=True,\n    optuna_seed=random_state,\n    #verbosity=False,\n    verbose_eval=False,\n    show_progress_bar=False,\n    #early_stopping_rounds=100,\n    callbacks=[lgb.early_stopping(100, verbose=False)], #lgb.log_evaluation(0)],\n)\ntuner_raw.run()\n\nlgb_boosters_raw = tuner_raw.get_best_booster().boosters\n\nprint(\"Best score:\", tuner_raw.best_score)\nprint('Best iteration:', tuner_raw.get_best_booster().best_iteration)\ntuner_raw.best_params","66baff07":"# dtrain = lgb.Dataset(X_lgb, label=y, categorical_feature= [10, 11, 12],)\ndtrain = lgb.Dataset(X, label=y, categorical_feature=col_cat, free_raw_data=False)\n# dtrain.construct()\n# _ = dtrain.get_label()\n\nparams = {\n    \"objective\": \"binary\",\n    \"num_iterations\": 1_000,\n    \"metric\": \"auc\",\n    \"verbose\": -1,\n    #\"verbose_eval\": -1,\n    \"boosting_type\": \"gbdt\",\n}\n\ndef fpreproc(dtrain, dtest, params):\n    \n    # Model Pipeline\n    inv_sigmoid = Pipeline([\n        ('0', preprocessing.MinMaxScaler(feature_range=(1e-3, 1-1e-3))),\n        ('1', FunctionTransformer(lambda x: np.log(x \/ (1-x)))),\n        ('2', preprocessing.PowerTransformer())\n    ])\n\n    transformer = ColumnTransformer([\n        ('inv_sigmoid', inv_sigmoid, col_sig),\n        ('scale', preprocessing.PowerTransformer(), col_pow),\n        ('db_to_num', FunctionTransformer(lambda x: np.log1p(-x)), ['loudness']),\n    ], remainder='passthrough')\n\n    pipe = Pipeline([\n        ('transform', transformer),\n        ('imputer', impute.IterativeImputer(random_state=random_state)),\n    ])\n    \n    dtrain.construct()\n    dtest.construct()\n    xtrain = dtrain.get_data()\n    xtest = dtest.get_data()\n    ytrain = dtrain.get_label()\n    ytest = dtest.get_label()\n    \n    pipe.fit(xtrain)\n    \n    xtrain = pipe.transform(xtrain)\n    xtest = pipe.transform(xtest)\n    \n    dtrain = opt_lgb.Dataset(xtrain, label=ytrain)\n    dtest = opt_lgb.Dataset(xtest, label=ytest, reference=dtrain)\n    \n    return dtrain, dtest, params\n\n\ntuner_w_imp = opt_lgb.LightGBMTunerCV(\n    params,\n    dtrain,\n    folds=full_split,\n    return_cvbooster=True,\n    optuna_seed=random_state,\n    fpreproc=fpreproc,\n    #verbosity=False,\n    verbose_eval=False,\n    show_progress_bar=False,\n    #early_stopping_rounds=100,\n    callbacks=[lgb.early_stopping(100, verbose=False)], #lgb.log_evaluation(0)],\n)\ntuner_w_imp.run()\n\nlgb_boosters_w_imp = tuner_w_imp.get_best_booster().boosters\n\nprint(\"Best score:\", tuner_w_imp.best_score)\nprint('Best iteration:', tuner_w_imp.get_best_booster().best_iteration)\ntuner_w_imp.best_params","57bcae78":"dtrain_raw = lgb.Dataset(X, label=y, categorical_feature=col_cat, free_raw_data=False)\n\ndef fpreproc_raw(dtrain, dtest, params):\n    dtrain.construct()\n    dtest.construct()\n    xtrain = dtrain.get_data()\n    xtest = dtest.get_data()\n    ytrain = dtrain.get_label()\n    ytest = dtest.get_label()\n    \n    imputer = impute.IterativeImputer(random_state=random_state).fit(xtrain)\n    \n    xtrain = imputer.transform(xtrain)\n    xtest = imputer.transform(xtest)\n    \n    dtrain = opt_lgb.Dataset(xtrain, label=ytrain)\n    dtest = opt_lgb.Dataset(xtest, label=ytest, reference=dtrain)\n    \n    return dtrain, dtest, params\n\n\nparams = {\n    \"objective\": \"binary\",\n    \"num_iterations\": 1_000,\n    \"metric\": \"auc\",\n    \"verbose\": -1,\n    #\"verbose_eval\": -1,\n    \"boosting_type\": \"gbdt\",\n}\n\ntuner_raw_w_imp = opt_lgb.LightGBMTunerCV(\n    params,\n    dtrain_raw,\n    folds=full_split,\n    return_cvbooster=True,\n    optuna_seed=random_state,\n    fpreproc=fpreproc_raw,\n    #verbosity=False,\n    verbose_eval=False,\n    show_progress_bar=False,\n    #early_stopping_rounds=100,\n    callbacks=[lgb.early_stopping(100, verbose=False)], #lgb.log_evaluation(0)],\n)\ntuner_raw_w_imp.run()\n\nlgb_boosters_raw_w_imp = tuner_raw_w_imp.get_best_booster().boosters\n\nprint(\"Best score:\", tuner_raw_w_imp.best_score)\nprint('Best iteration:', tuner_raw_w_imp.get_best_booster().best_iteration)\ntuner_raw_w_imp.best_params","9bdcf875":"inv_sigmoid2 = Pipeline([\n    ('0', preprocessing.MinMaxScaler(feature_range=(1e-3, 1-1e-3))),\n    ('1', FunctionTransformer(lambda x: np.log(x \/ (1-x)))),\n])\n\ntransformer2 = ColumnTransformer([\n    ('inv_sigmoid', inv_sigmoid2, col_sig),\n    #('scale', preprocessing.PowerTransformer(), col_pow),\n    ('db_to_num', FunctionTransformer(lambda x: np.log1p(-x)), ['loudness']),\n], remainder='passthrough')\n\nX_lgb2 = transformer2.fit_transform(X)\n\ndtrain2 = lgb.Dataset(X_lgb2, label=y, categorical_feature= [10, 11, 12],)\n\nparams = {\n    \"objective\": \"binary\",\n    \"num_iterations\": 1_000,\n    \"metric\": \"auc\",\n    \"verbosity\": -100,\n    #\"verbose_eval\": -1,\n    \"boosting_type\": \"gbdt\",\n}\n\ntuner2 = opt_lgb.LightGBMTunerCV(\n    params,\n    dtrain2,\n    folds=full_split,\n    return_cvbooster=True,\n    optuna_seed=random_state,\n    #verbosity=False,\n    verbose_eval=False,\n    show_progress_bar=False,\n    #early_stopping_rounds=100,\n    callbacks=[lgb.early_stopping(100, verbose=False)], #lgb.log_evaluation(0)],\n)\ntuner2.run()\n\nlgb_boosters2 = tuner2.get_best_booster().boosters\n\nprint(\"Best score:\", tuner.best_score)\nprint('Best iteration:', tuner.get_best_booster().best_iteration)\ntuner2.best_params","06469e10":"df_sample['song_popularity'] = model.predict(df_sub)\n\ndf_sample.to_csv('submission_sklearn.csv')","4540b32f":"df_sub_proc = transformer.transform(df_sub)\ny_sub = np.stack([m.predict(df_sub_proc) for m in lgb_boosters], axis=1)\ndf_sample['song_popularity'] = np.mean(y_sub, axis=1)\n\ndf_sample.to_csv('submission_lgb.csv')","b8ef9cbf":"y_sub = np.stack([m.predict(df_sub) for m in lgb_boosters_raw], axis=1)\ndf_sample['song_popularity'] = np.mean(y_sub, axis=1)\n\ndf_sample.to_csv('submission_lgb_raw.csv')","8d7d8d45":"dsub = lgb.Dataset(df_sub, categorical_feature=col_cat, free_raw_data=False)\n_, dsub, _ = fpreproc(dtrain, dsub, None)\ndsub.free_raw_data = False\ndsub.construct()\ndsub = dsub.get_data()\n\ny_sub = np.stack([m.predict(dsub) for m in lgb_boosters_w_imp], axis=1)\ndf_sample['song_popularity'] = np.mean(y_sub, axis=1)\n\ndf_sample.to_csv('submission_lgb_w_imp.csv')","462ea096":"dsub = lgb.Dataset(df_sub, categorical_feature=col_cat, free_raw_data=False)\n_, dsub, _ = fpreproc_raw(dtrain_raw, dsub, None)\ndsub.free_raw_data = False\ndsub.construct()\ndsub = dsub.get_data()\n\ny_sub = np.stack([m.predict(dsub) for m in lgb_boosters_raw_w_imp], axis=1)\ndf_sample['song_popularity'] = np.mean(y_sub, axis=1)\n\ndf_sample.to_csv('submission_lgb_raw_w_imp.csv')","eb899689":"df_sub_proc = transformer2.transform(df_sub)\ny_sub = np.stack([m.predict(df_sub_proc) for m in lgb_boosters2], axis=1)\ndf_sample['song_popularity'] = np.mean(y_sub, axis=1)\n\ndf_sample.to_csv('submission_lgb2.csv')","f7002c15":"from sklearn.model_selection import cross_val_predict\n\ndef get_oob(ys, cv):\n    y = ys[0]\n    for i, (_, test_idx) in enumerate(cv.split()):\n        y[test_idx] = ys[i][test_idx]\n    return y\n    \nX_proc = transformer.fit_transform(X)\nX_proc_test = transformer.transform(df_sub)\n\nX_stack_train = [\n    cross_val_predict(model, X, y, cv=full_split,\n                      method='predict_proba')[:, 1],\n    get_oob([m.predict(X_proc) for m in lgb_boosters], full_split),\n    get_oob([m.predict(X) for m in lgb_boosters_raw], full_split)\n]\nX_stack_train = np.stack(X_stack_train, axis=1)\n\nstack_model = LogisticRegression().fit(X_stack_train, y)","ab7901f4":"X_stack_test = [\n    model.predict_proba(df_sub)[:, 1],\n    np.mean([m.predict(X_proc_test) for m in lgb_boosters], axis=0),\n    np.mean([m.predict(df_sub) for m in lgb_boosters_raw], axis=0),\n]\nX_stack_test = np.stack(X_stack_test, axis=1)","77ebada3":"y_pred = stack_model.predict(X_stack_test)\n\ndf_sample['song_popularity'] = y_pred\ndf_sample.to_csv('submission_stack.csv')","c0037790":"cross_val_score(stack_model, X_stack_train, y, scoring='roc_auc')","13d06f0a":"**\/!\\ added solution after the competition end \/!\\**","b8ab4f0f":"# Submission","2a032f7d":"# Imports","a2defbd8":"Energy and loudness are correlated","65a9897b":"# Feature importance","edaa8ae3":"# Data collection","a6bc369b":"The only found pattern is the same as instrumentalness > 0.01\n\nMaybe adding this as a categorical feature could improve model performance.","475f2c82":"# Explanation Summary\n\n## Results\n\nThis notebook is the version 6 which got 1st place in the competition. The ranking surprised me.  \nI tried here to explain what I did which could have improved my score.\n\nThe submission result are the following:\n\n| output name | private score | public score |\n| :--- | ----: | ---: |\n| *lgb*      | **0.57703** | 0.60155 |\n| *lgb_raw*   | **0.57820** | 0.60516 |\n| *stacked* | 0.50466 | 0.50750 |\n| *sklearn* | 0.50590 | 0.51066 |\n\nThe stacking of *lgb_raw*, *lgb* and *sklearn* with LR did not work as intended.  \nOn V7 I tried to add the missing indicators but it did not change the score. I kept here below the V6 version without missing indicators.\n\nI had expected the *lgb* file to get the better private score. It is still high enough to achieve 1st place too.\n\n\n## Solution specificity\n\nI share what I found specific to my solution which could explain the score. (On top of the fact that I avoided the overfit with an unique model).  \nI had already worked with Spotify audio feature for a personal project.  \n(documentation here: https:\/\/developer.spotify.com\/documentation\/web-api\/reference\/#\/operations\/get-audio-features).\n\n### 1. Data Scaling\n**This transformation did not improve the public score compare to raw data.**\n\n[Link to specific code paragraph](#scaling)\n\nAs usual for me, I do a qq-plot. From the plots, I proposed to scale the data as follows:\n```\ncol_sig = ['acousticness', 'danceability', 'energy', 'instrumentalness',\n           'liveness', 'speechiness', 'audio_valence']\ncol_pow = ['song_duration_ms', 'tempo']\n```\n\nThe `col_sig` will get an inverse sigmoid transformation. I take as an hypothesis that they're the results of a NN and a sigmoid. With the inverse sigmoid I get the logits values.  \nI remember that the usual Spotify values from the API are always between 0 and 1, hence my hypothesis. Some values are outisde (0, 1), so I forced with the min-max scale because the data was maybe transformed for this competition.  \n\nThe `col_sig` and the `col_pow` columns goes through `PowerTransform` for a Box-Cox transformation.\n\nThe `loudness` column is scaled by `lambda x: np.log1p(-x)`\n\nThe function created in this codeblock was to test the output on qq-plots.  \nThe transformation used for the LGB models is done with sklearn `Pipeline` and `ColumnTransfomer` to avoid data leakage. See `transformer` in the following code-block: [sklearn pipeline](#sklearn)\n\n\n### 2. Data Splitting <-- probably what helped the most\n\n[Link to specific code paragraph](#split)\n\nI created 2 cv folds:\n* train\/valid\/test to as usual work on train\/valid but evaluate on test if necessary after CV optimization.\n* train\/valid with all the data for the final training. I only used this one for the LGB best solution (did not want to wait for 2 training)\n\n**I stratified the data on categorical columns, missing values, and target labels.**\n\ncategorical columns : `audio_mode`, `time_signature`  \nmissing values : `MissingIndicator` with an `any()` aggregation. So I would be sure to get the same **10%** of missing values in each folds.\n\nThe categorical columns because they're quite important to a song.  \nThe missing values because they are artificial, so it must be the competition focus. I wanted my model to generalize well with those missing features.\n\n\n### 3. Models\n\n#### 3.1 [GridSearchCV - benchmark for feature transformation](#sklearn)\nThe sklearn `GridSearchCV` benchmark my proposed feature transformation and which imputer was best between `SimpleImputer(strategy='median')`, `SimpleImputer(strategy='mean')`, `IterativeImputer()`.\n`IterativeImputer` was best but not used with LGB.\n\n#### 3.2 [Optimized LightGBM with Optuna `LightGBMTunerCV`](#lgb)\n\n`LightGBMTunerCV`:\nIt is a serial optimization (instead to try all combination, you find best for one parameter and optimized next parameter). It could be used as the basis for a second more thorough optimization.\n\nI tried:\n* `lgb`: input data is the transformed data (scaling only, no imputation).\n* `lgb_raw`: input data is not transformed (same as raw).\n* after the competition I tried without applying Box-Cox on `col_pow` columns: `lgb_transf`.\n\n**The LGB models (best solution) did not use any imputation method (I let LightGBM use its own method).**  \nTo add an imputer, I needed to apply it in CV inside the tuner. It is quite difficult with `LightGBMTunerCV`: need to add a f_preproc function...\n\n\n### 4. Other - failure\n\n*Dead-end feature engineering trials:*  \nPCA, U-MAP, and Mutual Information did not help. I just noticed that U-MAP found a pattern which is the same as instrumentalness > 0.01.\n\n*[Logistic Regression Blending](#lgb)*  \nIt did not work as intended.  \nFor the LGB-CV models I took care for each fold to predict the validation data (as with `cross_val_predict()`).  \nOn V7 I fitted a`QuadraticDiscriminantAnalysis`. But the result were not better than LGB either.\n\n### 5. Solution summary for `lgb_raw` (best result):\n* data split with target AND categorical AND missing indicators (aggregated with any())\n* LightGBMTunerCV with usual parameters\n* predict and aggregate with mean\n\n\n## Summary\n\nI was surprised to see missing values because from the Spotify API this never happen. It must have been artificial, and nothing could tell if the private\/public leaderboard was stratified on this artificial missing ratio. Adding the missing indicator (such as in V7 of this notebook) did not improve the score, which is maybe a clue for random missing imputation.\n\nAnother reason I was dubious of the public leaderboard is that my public score was much higher than my CV (usually they're quite close).\n\nI think what helped me most is the data splitting where I used the missing indicators (refer to section 2). I expected to get a better score on public than private leaderboard, but it is true I did not expect the 1st place. After searching for a job the previous weeks this competition result help me be more confident in my basics (data splitting, modelling).","79049acc":"# Modelling","021139bc":"# EDA","3ca55c1b":"The other columns seem to be outputs of sigmoid functions.\nIf we apply the inverse sigmoid we should get the logits values. Then we can box-cox transform.","9b3c8ad8":"## <div id=\"split\">Split<\/div>\n\nWe will make a test fold of 10%, and 5 CV splits (20% of remaining data)","ff87b9e8":"## <div id=\"scaling\">Processing - scaling<\/div>","33d17cf4":"## UMAP\n\nWith the processed data","2a42bfce":"**\/!\\ added solution after the competition end \/!\\**\n\n* tried with iterative imputer\n* tried without power transf.","eba5f4a1":"## <div id=\"lgb\">Optuna - LightGBM<\/div>","5e97c020":"Create a missing values table to ensure that the stratification takes into account the missing values distribution.","0b1a8808":"A Box-Cox transformation would be useful, especially for `loudness` and `song_duration_ms`","7c6d7c53":"# <div id=\"FeatureEng\">Feature Eng.<\/div>","9a7b7b46":"1. Make stratification series (based on categorical columns, missing values, and target labels)\n2. split dev and test\n3. CV split of dev set","beb0c93f":"# Stacking - Blending","e890759f":"Same distribution for categorical and target in right to missing values.\n\n--> So we can drop missing values for analysis if necessary.","814baa2d":"## qq-plot","eceb8002":"## PCA","7aa5a128":"## Correlation","c73ebc0f":"## <div id=\"sklearn\">Benchmark with Pipeline (LR \/ GB \/ RF)<\/div>","37c4d2fc":"**Tips for `fpreproc`:**\n* use `construct()` before `get_data()` and `get_label()`: https:\/\/github.com\/Microsoft\/LightGBM\/issues\/1173\n* use `reference=dtrain` when creating a test set: https:\/\/github.com\/microsoft\/LightGBM\/issues\/3170#issuecomment-644815236"}}