{"cell_type":{"7a66eada":"code","b7a93336":"code","455bcd41":"code","b79c1d62":"code","1650a6c5":"code","2f14c9b0":"code","b0385087":"code","79394b87":"code","20806f74":"code","d31bac14":"code","7f6c28a9":"markdown","6af19899":"markdown","3bfd3fea":"markdown"},"source":{"7a66eada":"import os\nimport gc\nimport sys\nimport cv2\nimport glob\nimport time\nimport signal\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\nimport matplotlib.pyplot as plt\nimport tensorboardX\nfrom tqdm.notebook import tqdm\n\nfrom collections import OrderedDict\nfrom sklearn import model_selection\n\nseed = 42\ntest_size = 0.2\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nbackbone = [1, 1, 1, 1]\nencoder_channels = np.array([64, 128, 256, 512, 1024])*2\ndecoder_channels = np.array([512, 256, 128, 64])*2\n\ntime_step = 4000 # a continuous batch is 500000!\ntime_step_test = 10000\nstride = 2 # for 4 times\nbatch_size = 8\n\nTRAIN = True\nPREDICT = True\n\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\nprint(device)","b7a93336":"# data loading \n\ndf_train = pd.read_csv(\"..\/input\/liverpool-ion-switching\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/liverpool-ion-switching\/test.csv\")","455bcd41":"# possible augmentations?\n# https:\/\/github.com\/iver56\/audiomentations\n\ndef do_identity(image, label):\n    return image, label\n\ndef do_flip(image, label):\n    image = np.ascontiguousarray(np.flip(image, axis=1))\n    label = np.ascontiguousarray(np.flip(label))\n    return image, label\n\ndef train_augment(image, label):\n    for op in np.random.choice([\n        lambda image, label: do_identity(image, label),\n        lambda image, label: do_flip(image, label),\n    ], 1):\n        image, label = op(image, label)\n    \n    return image, label\n\ndef valid_augment(image, label):\n    return image, label","b79c1d62":"class IonDataset(Dataset):\n    def __init__(self, data, labels=None, type='train', transform=None):\n        self.data = data\n        self.labels = labels\n        self.type = type\n        self.transform = transform\n        \n    def __getitem__(self, i):\n        signal = self.data[i].astype(np.float32) # [1, time_step]\n        if self.type == 'train':\n            label = self.labels[i].astype(np.int64) # [time_step]\n            if self.transform is not None:\n                signal, label = self.transform(signal, label)\n            return signal, label\n        else:\n            return signal\n    \n    def __len__(self):\n        return len(self.data)","1650a6c5":"class SEModule(nn.Module):\n    def __init__(self, in_channels, reduction=4):\n        super().__init__()\n        self.conv1 = nn.Conv1d(in_channels, in_channels\/\/reduction, kernel_size=1, padding=0)\n        self.conv2 = nn.Conv1d(in_channels\/\/reduction, in_channels, kernel_size=1, padding=0)\n        \n    def forward(self, x):\n        # x: [B, C, H]\n        s = F.adaptive_avg_pool1d(x, 1) # [B, C, 1]\n        s = self.conv1(s) # [B, C\/\/reduction, 1]\n        s = F.relu(s, inplace=True)\n        s = self.conv2(s) # [B, C, 1]\n        x = x + torch.sigmoid(s)\n        return x\n\nclass ConvBR1d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, padding=0, dilation=1, stride=1, groups=1, is_activation=True):\n        super().__init__()\n        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, dilation=dilation, stride=stride, groups=groups, bias=False)\n        self.bn = nn.BatchNorm1d(out_channels)\n        self.is_activation = is_activation\n        \n        if is_activation:\n            self.relu = nn.ReLU(inplace=True)\n        \n    def forward(self, x):\n        x = self.bn(self.conv(x))\n        if self.is_activation:\n            x = self.relu(x)\n        return x\n\n\nclass SENextBottleneck(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, groups=32, reduction=16, pool=None, is_shortcut=False):\n        super().__init__()\n        mid_channels = out_channels \/\/ 2\n        self.conv1 = ConvBR1d(in_channels, mid_channels, 1, 0, 1, )\n        self.conv2 = ConvBR1d(mid_channels, mid_channels, 3, 1, 1, groups=groups)\n        self.conv3 = ConvBR1d(mid_channels, out_channels, 1, 0, 1, is_activation=False)\n        self.se = SEModule(out_channels, reduction)\n        self.stride = stride\n        self.is_shortcut = is_shortcut\n        \n        if is_shortcut:\n            self.shortcut = ConvBR1d(in_channels, out_channels, 1, 0, 1, is_activation=False)\n        if stride > 1:\n            if pool == 'max':\n                self.pool = nn.MaxPool1d(stride, stride)\n            elif pool == 'avg':\n                self.pool = nn.AvgPool1d(stride, stride)\n    \n    def forward(self, x):\n        s = self.conv1(x)\n        s = self.conv2(s)\n        if self.stride > 1:\n            s = self.pool(s)\n        s = self.conv3(s)\n        s = self.se(s)\n        \n        if self.is_shortcut:\n            if self.stride > 1:\n                x = F.avg_pool1d(x, self.stride, self.stride) # avg\n            x = self.shortcut(x)\n        \n        x = x + s\n        x = F.relu(x, inplace=True)\n        \n        return x\n\n\nclass Encoder(nn.Module):\n    def __init__(self, num_features=1):\n        super().__init__()\n        self.block0 = nn.Sequential(\n            ConvBR1d(num_features, encoder_channels[0], kernel_size=5, stride=1, padding=2),\n            ConvBR1d(encoder_channels[0], encoder_channels[0], kernel_size=3, stride=1, padding=1),\n            ConvBR1d(encoder_channels[0], encoder_channels[0], kernel_size=3, stride=1, padding=1),\n        )\n        self.block1 = nn.Sequential(\n            SENextBottleneck(encoder_channels[0], encoder_channels[1], stride=stride, is_shortcut=True, pool='max'),\n          *[SENextBottleneck(encoder_channels[1], encoder_channels[1], stride=1, is_shortcut=False) for i in range(backbone[0])]\n        )\n        self.block2 = nn.Sequential(\n            SENextBottleneck(encoder_channels[1], encoder_channels[2], stride=stride, is_shortcut=True, pool='max'),\n          *[SENextBottleneck(encoder_channels[2], encoder_channels[2], stride=1, is_shortcut=False) for i in range(backbone[1])]\n        )\n        self.block3 = nn.Sequential(\n            SENextBottleneck(encoder_channels[2], encoder_channels[3], stride=stride, is_shortcut=True, pool='max'),\n          *[SENextBottleneck(encoder_channels[3], encoder_channels[3], stride=1, is_shortcut=False) for i in range(backbone[2])]\n        )\n        self.block4 = nn.Sequential(\n            SENextBottleneck(encoder_channels[3], encoder_channels[4], stride=stride, is_shortcut=True, pool='avg'),\n          *[SENextBottleneck(encoder_channels[4], encoder_channels[4], stride=1, is_shortcut=False) for i in range(backbone[3])]\n        )  \n        \n    def forward(self, x):\n        x0 = self.block0(x) # [B, 64, L]\n        x1 = self.block1(x0) # [B, 256, L\/\/2]\n        x2 = self.block2(x1) # [B, 512, L\/\/4]\n        x3 = self.block3(x2) # [B, 1024, L\/\/8]\n        x4 = self.block4(x3) # [B, 2048, L\/\/16]\n        \n        return [x0, x1, x2, x3, x4]\n\nclass SCSEModule(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super().__init__()\n        self.cSE = nn.Sequential(\n            nn.AdaptiveAvgPool1d(1),\n            nn.Conv1d(in_channels, in_channels \/\/ reduction, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(in_channels \/\/ reduction, in_channels, 1),\n            nn.Sigmoid(),\n        )\n        self.sSE = nn.Sequential(nn.Conv1d(in_channels, 1, 1), nn.Sigmoid())\n\n    def forward(self, x):\n        return x * self.cSE(x) + x * self.sSE(x)\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels):\n        super().__init__()\n        self.conv1 = ConvBR1d(in_channels + skip_channels, out_channels, kernel_size=3, padding=1)\n        self.conv2 = ConvBR1d(out_channels, out_channels, kernel_size=3, padding=1)\n        # att\n        #self.att1 = SCSEModule(in_channels + skip_channels)\n        #self.att2 = SCSEModule(out_channels)\n        \n    def forward(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=stride, mode=\"linear\", align_corners=True)\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n            #x = self.att1(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        #x = self.att2(x)\n        return x\n    \nclass Decoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block4 = DecoderBlock(encoder_channels[-1], encoder_channels[-2], decoder_channels[0])\n        self.block3 = DecoderBlock(decoder_channels[0], encoder_channels[-3], decoder_channels[1])\n        self.block2 = DecoderBlock(decoder_channels[1], encoder_channels[-4], decoder_channels[2])\n        self.block1 = DecoderBlock(decoder_channels[2], encoder_channels[-5], decoder_channels[3])\n\n    def forward(self, xs):\n\n        x = self.block4(xs[4], xs[3])\n        x = self.block3(x, xs[2])\n        x = self.block2(x, xs[1])\n        x = self.block1(x, xs[0])\n        \n        return x\n        \n    \nclass Unet(nn.Module):\n    def __init__(self, num_features=1, num_classes=11):\n        super().__init__()\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.encoder = Encoder(num_features=num_features)\n        self.decoder = Decoder()\n        self.segmentation_head = nn.Conv1d(decoder_channels[-1], num_classes, kernel_size=1, padding=0, stride=1)\n    \n    def forward(self, x):\n        features = self.encoder(x)\n        x = self.decoder(features)\n        x = self.segmentation_head(x)\n        return x\n","2f14c9b0":"# trainer\nclass Logger:\n    def __init__(self, workspace=None, flush=True, mute=False):\n        self.workspace = workspace\n        self.flush = flush\n        self.mute = mute\n        if workspace is not None:\n            os.makedirs(workspace, exist_ok=True)\n            self.log_file = os.path.join(workspace, \"log.txt\")\n            self.fp = open(self.log_file, \"a+\")\n        else:\n            self.fp = None\n\n    def __del__(self):\n        if self.fp: \n            self.fp.close()\n\n    def _print(self, text, use_pprint=False):\n        if not self.mute:\n            print(text) if not use_pprint else pprint(text)\n        if self.fp:\n            print(text, file=self.fp)\n        if self.flush:\n            sys.stdout.flush()\n\n    def log(self, text, level=0):\n        text = \"\\t\"*level + text\n        text.replace(\"\\n\", \"\\n\"+\"\\t\"*level)\n        self._print(text)\n\n    def log1(self, text):\n        self.log(text, level=1)\n\n    def info(self, text):\n        text = \"[INFO] \" + text\n        text.replace(\"\\n\", \"\\n\"+\"[INFO] \")\n        self._print(text)\n\n    def error(self, text):\n        text = \"[ERROR] \" + text\n        text.replace(\"\\n\", \"\\n\"+\"[ERROR] \")\n        self._print(text)\n\n\nclass DelayedKeyboardInterrupt(object):\n    def __enter__(self):\n        self.signal_received = False\n        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n\n    def handler(self, sig, frame):\n        self.signal_received = (sig, frame)\n        print('SIGINT received. Delaying KeyboardInterrupt.')\n\n    def __exit__(self, type, value, traceback):\n        signal.signal(signal.SIGINT, self.old_handler)\n        if self.signal_received: \n            self.old_handler(*self.signal_received)\n\n\ndef fix_random_seed(seed=42, cudnn=False):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if cudnn:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n\n# from torch_summary\ndef summary(model, input_size, batch_size=-1, device=\"cuda\", logger=None):\n    # redirect to write in file\n    if logger is not None:\n        print = logger._print\n\n    def register_hook(module):\n\n        def hook(module, input, output):\n            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n            module_idx = len(summary)\n\n            m_key = \"%s-%i\" % (class_name, module_idx + 1)\n            summary[m_key] = OrderedDict()\n            summary[m_key][\"input_shape\"] = list(input[0].size())\n            summary[m_key][\"input_shape\"][0] = batch_size\n            if isinstance(output, (list, tuple)):\n                summary[m_key][\"output_shape\"] = [\n                    [-1] + list(o.size())[1:] for o in output\n                ]\n            else:\n                summary[m_key][\"output_shape\"] = list(output.size())\n                summary[m_key][\"output_shape\"][0] = batch_size\n\n            params = 0\n            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n                summary[m_key][\"trainable\"] = module.weight.requires_grad\n            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n            summary[m_key][\"nb_params\"] = params\n\n        if (\n            not isinstance(module, nn.Sequential)\n            and not isinstance(module, nn.ModuleList)\n            and not (module == model)\n        ):\n            hooks.append(module.register_forward_hook(hook))\n\n    device = device.lower()\n    assert device in [\n        \"cuda\",\n        \"cpu\",\n    ], \"Input device is not valid, please specify 'cuda' or 'cpu'\"\n\n    if device == \"cuda\" and torch.cuda.is_available():\n        dtype = torch.cuda.FloatTensor\n    else:\n        dtype = torch.FloatTensor\n\n    # multiple inputs to the network\n    if isinstance(input_size, tuple):\n        input_size = [input_size]\n\n    # batch_size of 2 for batchnorm\n    x = [torch.rand(2, *in_size).type(dtype) for in_size in input_size]\n    # print(type(x[0]))\n\n    # create properties\n    summary = OrderedDict()\n    hooks = []\n\n    # register hook\n    model.apply(register_hook)\n\n    # make a forward pass\n    # print(x.shape)\n    model(*x)\n\n    # remove these hooks\n    for h in hooks:\n        h.remove()\n\n    print(\"----------------------------------------------------------------\")\n    line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer (type)\", \"Output Shape\", \"Param #\")\n    print(line_new)\n    print(\"================================================================\")\n    total_params = 0\n    total_output = 0\n    trainable_params = 0\n    for layer in summary:\n        # input_shape, output_shape, trainable, nb_params\n        line_new = \"{:>20}  {:>25} {:>15}\".format(\n            layer,\n            str(summary[layer][\"output_shape\"]),\n            \"{0:,}\".format(summary[layer][\"nb_params\"]),\n        )\n        total_params += summary[layer][\"nb_params\"]\n        total_output += np.prod(summary[layer][\"output_shape\"])\n        if \"trainable\" in summary[layer]:\n            if summary[layer][\"trainable\"] == True:\n                trainable_params += summary[layer][\"nb_params\"]\n        print(line_new)\n\n    # assume 4 bytes\/number (float on cuda).\n    total_input_size = abs(np.prod(input_size) * batch_size * 4. \/ (1024 ** 2.))\n    total_output_size = abs(2. * total_output * 4. \/ (1024 ** 2.))  # x2 for gradients\n    total_params_size = abs(total_params.numpy() * 4. \/ (1024 ** 2.))\n    total_size = total_params_size + total_output_size + total_input_size\n\n    print(\"================================================================\")\n    print(\"Total params: {0:,}\".format(total_params))\n    print(\"Trainable params: {0:,}\".format(trainable_params))\n    print(\"Non-trainable params: {0:,}\".format(total_params - trainable_params))\n    print(\"----------------------------------------------------------------\")\n    print(\"Input size (MB): %0.2f\" % total_input_size)\n    print(\"Forward\/backward pass size (MB): %0.2f\" % total_output_size)\n    print(\"Params size (MB): %0.2f\" % total_params_size)\n    print(\"Estimated Total Size (MB): %0.2f\" % total_size)\n    print(\"----------------------------------------------------------------\")\n    # return summary\n        \n\nclass Trainer(object):\n    \"\"\"Base trainer class. \n    \"\"\"\n    def __init__(self,\n                 device,\n                 workspace,\n                 model, \n                 optimizer, \n                 lr_scheduler, \n                 objective, \n                 dataloaders,\n                 metrics=[],\n                 model_name=None,\n                 input_shape=None,\n                 use_checkpoint=\"latest\",\n                 use_tensorboardX=True,\n                 max_keep_ckpt=1,\n                 eval_interval=1,\n                 report_step_interval=300,\n                 restart=False,\n                 ):\n        \n        self.device = device\n        self.workspace_path = workspace\n        self.model = model\n        self.model_name = model_name\n        self.restart = restart\n        self.optimizer = optimizer\n        self.lr_scheduler = lr_scheduler\n        self.objective = objective\n        self.dataloaders = dataloaders\n        self.metrics = metrics\n        self.log = Logger(workspace)\n        self.use_checkpoint = use_checkpoint\n        self.max_keep_ckpt = max_keep_ckpt\n        self.eval_interval = eval_interval\n        self.report_step_interval = report_step_interval\n        self.time_stamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.log.info(f'Time stamp is {self.time_stamp}')\n        self.use_tensorboardX = use_tensorboardX\n        self.writer = None\n\n        self.model.to(self.device)\n\n        if input_shape is not None:\n            summary(self.model, input_shape, logger=self.log)\n\n        self.log.info(f'Number of model parameters: {sum([p.numel() for p in model.parameters() if p.requires_grad])}')\n\n        self.epoch = 1\n        self.global_step = 0\n        self.local_step = 0\n        self.stats = {\n            \"EvalResults\": [],\n            \"Checkpoints\": [],\n            \"BestResult\": None,\n            }\n\n        if self.workspace_path is not None:\n            os.makedirs(self.workspace_path, exist_ok=True)\n            if self.use_checkpoint == \"latest\":\n                self.log.info(\"Loading latest checkpoint ...\")\n                self.load_checkpoint()\n            elif self.use_checkpoint == \"scratch\":\n                self.log.info(\"Train from scratch\")\n            elif self.use_checkpoint == \"best\":\n                self.log.info(\"Loading best checkpoint ...\")\n                model_name = type(self.model).__name__\n                best_path = f\"{self.workspace_path}\/{model_name}_best.pth.tar\"\n                self.load_checkpoint(best_path)\n            else: # path to ckpt\n                self.log.info(f\"Loading checkpoint {self.use_checkpoint} ...\")\n                self.load_checkpoint(self.use_checkpoint)\n    \n    ### ------------------------------\n    \n    def train_step(self, data):\n        image, mask = data\n\n        output = self.model(image)\n        loss = self.objective(output, mask)\n        pred = F.softmax(output, 1).detach().cpu().numpy().argmax(axis=1)\n\n        return pred, mask, loss \n\n    def eval_step(self, data):\n        image, mask = data\n        \n        # tta\n        output = (self.model(image) + torch.flip(self.model(torch.flip(image, dims=[2])), dims=[2]))\/2\n        #output = self.model(image)\n\n        loss = self.objective(output, mask)\n        pred = F.softmax(output, 1).detach().cpu().numpy().argmax(axis=1)\n\n        return pred, mask, loss \n\n    ### ------------------------------\n\n    def train(self, max_epochs=None):\n        \"\"\"\n        do the training process for max_epochs.\n        \"\"\"\n        if max_epochs is None:\n            max_epochs = self.conf.max_epochs\n        \n        \n        if self.use_tensorboardX:\n            logdir = os.path.join(self.workspace_path, \"run\")\n            self.writer = tensorboardX.SummaryWriter(logdir)\n        \n        for epoch in range(self.epoch, max_epochs+1):\n            self.epoch = epoch\n\n            if self.optimizer.param_groups[0]['lr'] < 1e-8:\n                self.log.info(\"Early stopping.\")\n            \n            self.train_one_epoch()\n\n            if self.epoch % self.eval_interval == 0:\n                self.evaluate_one_epoch()\n\n                if self.workspace_path is not None:\n                    self.save_checkpoint()\n                    \n        if self.use_tensorboardX:\n            self.writer.close()\n\n        self.log.info(\"Finished Training.\")\n\n    def evaluate(self):\n        self.log.info(f\"Evaluate at best epoch...\")\n\n        # load model\n        model_name = type(self.model).__name__\n        best_path = f\"{self.workspace_path}\/{model_name}_best.pth.tar\"\n        if not os.path.exists(best_path):\n            self.log.error(f\"Best checkpoint not found! {best_path}, not loading anything.\")\n        else:\n            self.load_checkpoint(best_path)\n\n        self.use_tensorboardX = False\n        self.evaluate_one_epoch()\n        \n    def get_time(self):\n        if torch.cuda.is_available(): \n            torch.cuda.synchronize()\n        return time.time()\n\n    def prepare_data(self, data):\n        \"\"\" ToTensor for various data format \"\"\"\n        if isinstance(data, list) or isinstance(data, tuple):\n            for i, v in enumerate(data):\n                if isinstance(v, np.ndarray):\n                    data[i] = torch.from_numpy(v).to(self.device)\n                if torch.is_tensor(v):\n                    data[i] = v.to(self.device)\n        elif isinstance(data, dict):\n            for k, v in data.items():\n                if isinstance(v, np.ndarray):\n                    data[k] = torch.from_numpy(v).to(self.device)\n                if torch.is_tensor(v):\n                    data[k] = v.to(self.device)\n        elif isinstance(data, np.ndarray):\n            data = torch.from_numpy(data).to(self.device)\n        else: # is_tensor\n            data = data.to(self.device)\n\n        return data\n\n    \n    def train_one_epoch(self):\n        self.log.log(f\"==> Start Training Epoch {self.epoch}, lr={self.optimizer.param_groups[0]['lr']} ...\")\n\n        for metric in self.metrics:\n            metric.clear()\n        total_loss = []\n        self.model.train()\n\n        pbar = tqdm(self.dataloaders[\"train\"])\n\n        self.local_step = 0\n        epoch_start_time = self.get_time()                     \n                     \n        for data in pbar:\n            start_time = self.get_time()\n            self.local_step += 1\n            self.global_step += 1\n            \n            data = self.prepare_data(data)\n\n            preds, truths, loss = self.train_step(data)\n            \n            loss.backward()\n            \n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n            for metric in self.metrics:\n                metric.update(preds, truths)\n\n            total_loss.append(loss.item())\n            total_time = self.get_time() - start_time\n\n            if self.report_step_interval > 0 and self.local_step % self.report_step_interval == 0:\n                self.log.log1(f\"step={self.epoch}\/{self.local_step}, loss={loss.item():.4f}, time={total_time:.2f}\")\n                for metric in self.metrics:\n                    self.log.log1(metric.report())\n\n            if self.use_tensorboardX:\n                self.writer.add_scalar(f\"train{self.model_name}\/loss\", loss.item(), self.global_step)\n\n        if self.report_step_interval < 0:\n            for metric in self.metrics:\n                self.log.log1(metric.report())\n                metric.clear()\n\n        epoch_end_time = self.get_time()\n        average_loss = np.mean(total_loss)\n\n        self.log.log(f\"==> Finished Epoch {self.epoch}, average_loss={average_loss:.4f}, time={epoch_end_time-epoch_start_time:.4f}\")\n\n\n    def evaluate_one_epoch(self):\n        self.log.log(f\"++> Evaluate at epoch {self.epoch} ...\")\n\n        for metric in self.metrics:\n            metric.clear()\n        self.model.eval()\n\n        pbar = tqdm(self.dataloaders['valid'])\n\n        epoch_start_time = self.get_time()\n        total_loss = []\n\n        with torch.no_grad():\n            self.local_step = 0\n            start_time = self.get_time()\n            \n            for data in pbar:    \n                self.local_step += 1\n                \n                data = self.prepare_data(data)\n                preds, truths, loss = self.eval_step(data)\n                total_loss.append(loss.item())\n                \n                for metric in self.metrics:\n                    metric.update(preds, truths)\n\n            total_time = self.get_time() - start_time\n            self.log.log1(f\"total_time={total_time:.2f}\")\n\n            average_loss = np.mean(total_loss)\n            \n            for metric in self.metrics:\n                self.log.log1(metric.report())\n                if self.use_tensorboardX:\n                    metric.write(self.writer, self.epoch, prefix=f\"evaluate{self.model_name}\")\n                metric.clear()\n            \n            if self.use_tensorboardX:\n                self.writer.add_scalar(f\"evaluate{self.model_name}\/loss\", average_loss, self.epoch)\n            \n            self.stats[\"EvalResults\"].append(self.metrics[0].measure())\n\n        # monitor val loss!!!\n        self.lr_scheduler.step(average_loss)\n\n        epoch_end_time = self.get_time()\n        self.log.log(f\"++> Evaluate Finished. time={epoch_end_time-epoch_start_time:.4f}, loss={average_loss:.4f}\")\n\n    def save_checkpoint(self):\n        with DelayedKeyboardInterrupt():\n            model_name = type(self.model).__name__ if self.model_name is None else self.model_name\n            file_path = f\"{self.workspace_path}\/{model_name}_ep{self.epoch:04d}.pth.tar\"\n            best_path = f\"{self.workspace_path}\/{model_name}_best.pth.tar\"\n            os.makedirs(self.workspace_path, exist_ok=True)\n\n            self.stats[\"Checkpoints\"].append(file_path)\n\n            if len(self.stats[\"Checkpoints\"]) > self.max_keep_ckpt:\n                old_ckpt = self.stats[\"Checkpoints\"].pop(0)\n                if os.path.exists(old_ckpt):\n                    os.remove(old_ckpt)\n                    self.log.info(f\"Removed old checkpoint {old_ckpt}\")\n\n            state = {\n                'epoch': self.epoch,\n                'global_step': self.global_step,\n                'model_name': model_name,\n                'model': self.model.state_dict(),\n                'optimizer' : self.optimizer.state_dict(),\n                'lr_scheduler': self.lr_scheduler.state_dict(),\n                'stats' : self.stats,\n            }\n            \n            torch.save(state, file_path)\n            self.log.info(f\"Saved checkpoint {self.epoch} successfully.\")\n            \n            if self.stats[\"EvalResults\"] is not None:\n                ### better function\n                if self.stats[\"BestResult\"] is None or self.stats[\"EvalResults\"][-1] > self.stats[\"BestResult\"]:\n                    self.stats[\"BestResult\"] = self.stats[\"EvalResults\"][-1]\n                    torch.save(state, best_path)\n                    self.log.info(f\"Saved Best checkpoint.\")\n            \n\n    def load_checkpoint(self, checkpoint=None):\n\n        model_name = self.model_name if self.model_name is not None else type(self.model).__name__\n        \n        if checkpoint is None:\n            # Load most recent checkpoint            \n            checkpoint_list = sorted(glob.glob(f'{self.workspace_path}\/{model_name}_ep*.pth.tar'))\n            if checkpoint_list:\n                checkpoint_path = checkpoint_list[-1]\n            else:\n                self.log.info(\"No checkpoint found, model randomly initialized.\")\n                return False\n        elif isinstance(checkpoint, int):\n            # Checkpoint is the epoch number\n            checkpoint_path = f'{self.workspace_path}\/{model_name}_ep{checkpoint:04d}.pth.tar'\n        elif isinstance(checkpoint, str):\n            # checkpoint is the path\n            checkpoint_path = os.path.expanduser(checkpoint)\n        else:\n            self.log.error(\"load_checkpoint: Invalid argument\")\n            raise TypeError\n\n        checkpoint_dict = torch.load(checkpoint_path)\n\n        self.model.load_state_dict(checkpoint_dict['model'])\n        if not self.restart:\n            self.log.info(\"Loading epoch and other status...\")\n            self.epoch = checkpoint_dict['epoch'] + 1\n            self.global_step = checkpoint_dict['global_step']\n            self.optimizer.load_state_dict(checkpoint_dict['optimizer'])\n            self.lr_scheduler.load_state_dict(checkpoint_dict['lr_scheduler'])\n            self.lr_scheduler.last_epoch = checkpoint_dict['epoch'] \n            self.stats = checkpoint_dict['stats']\n        else:\n            self.log.info(\"Only loading model parameters.\")\n        \n        self.log.info(\"Checkpoint Loaded Successfully.\")\n        return True\n\n\nclass Predictor(object):\n    def __init__(self, device, models):\n        self.device = device\n        self.models = models\n        self.log = Logger()\n        self.time_stamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.log.info(f'Time stamp is {self.time_stamp}')\n        for model in self.models:\n            model.to(self.device)\n            \n    def predict_step(self, data):\n        image = data\n\n        outputs = []\n        for model in self.models:\n\n            # tta: flip\n            output = (model(image) + torch.flip(model(torch.flip(image, dims=[2])), dims=[2]))\/2\n            #output = model(image)\n\n            outputs.append(F.softmax(output, 1))\n            \n        pred = torch.mean(torch.stack(outputs, 0), 0).detach().cpu().numpy()\n        pred = pred.argmax(axis=1).astype(np.int64) # [B, L]\n        \n        return pred\n\n    def predict(self, dataloader):\n        self.log.log(f\"++> Predict start\")\n        # predict\n        for model in self.models:\n            model.eval()\n        pbar = tqdm(dataloader)\n        res = []\n        epoch_start_time = self.get_time()\n        with torch.no_grad():\n            self.local_step = 0\n            start_time = self.get_time()\n            for data in pbar:    \n                self.local_step += 1\n                data = self.prepare_data(data)\n                pred = self.predict_step(data)\n                res.extend(pred.reshape(-1))\n                \n            total_time = self.get_time() - start_time\n            self.log.log1(f\"total_time={total_time:.2f}\")\n        epoch_end_time = self.get_time()\n        self.log.log(f\"++> Predict Finished. time={epoch_end_time-epoch_start_time:.4f}\")\n        \n        return res\n        \n    def get_time(self):\n        if torch.cuda.is_available(): \n            torch.cuda.synchronize()\n        return time.time()\n\n    def prepare_data(self, data):\n        \"\"\" ToTensor for various data format \"\"\"\n        if isinstance(data, list) or isinstance(data, tuple):\n            for i, v in enumerate(data):\n                if isinstance(v, np.ndarray):\n                    data[i] = torch.from_numpy(v).to(self.device)\n                if torch.is_tensor(v):\n                    data[i] = v.to(self.device)\n        elif isinstance(data, dict):\n            for k, v in data.items():\n                if isinstance(v, np.ndarray):\n                    data[k] = torch.from_numpy(v).to(self.device)\n                if torch.is_tensor(v):\n                    data[k] = v.to(self.device)\n        elif isinstance(data, np.ndarray):\n            data = torch.from_numpy(data).to(self.device)\n        else: # is_tensor\n            data = data.to(self.device)\n        return data\n\n    def load_checkpoint(self, checkpoint_path, idx=0):\n        checkpoint = torch.load(checkpoint_path)\n        self.models[idx].load_state_dict(checkpoint['model'])\n        self.log.info(\"Checkpoint Loaded Successfully.\")\n        return True","b0385087":"class ClassificationMeter:\n    \"\"\" statistics for classification \"\"\"\n    def __init__(self, nCls, eps=1e-5, names=None):\n        self.nCls = nCls\n        self.names = names\n        self.eps = eps\n        self.N = 0\n        self.table = np.zeros((self.nCls, 4), dtype=np.int32)\n        self._measure = None\n\n    def clear(self):\n        self.N = 0\n        self.table = np.zeros((self.nCls, 4), dtype=np.int32)\n\n    def prepare_inputs(self, outputs, truths):\n        \"\"\"\n        outputs and truths are pytorch tensors or numpy ndarrays.\n        \"\"\"\n        if torch.is_tensor(outputs):\n            outputs = outputs.detach().cpu().numpy()\n        if torch.is_tensor(truths):\n            truths = truths.detach().cpu().numpy()\n        \n        return outputs, truths\n\n    def update(self, preds, truths):\n        preds, truths = self.prepare_inputs(preds, truths)\n        self.N += np.prod(truths.shape)\n        for Cls in range(self.nCls):\n            true_positive = np.count_nonzero(np.bitwise_and(preds == Cls, truths == Cls))\n            true_negative = np.count_nonzero(np.bitwise_and(preds != Cls, truths != Cls))\n            false_positive = np.count_nonzero(np.bitwise_and(preds == Cls, truths != Cls))\n            false_negative = np.count_nonzero(np.bitwise_and(preds != Cls, truths == Cls))\n            self.table[Cls] += [true_positive, true_negative, false_positive, false_negative]\n\n    # call after report() !\n    def measure(self):\n        return self._measure\n\n    def better(self, A, B):\n        return A > B\n\n    def write(self, writer, global_step, prefix=\"\"):\n        writer.add_scalar(os.path.join(prefix, \"Accuracy\"), self.measure(), global_step)\n\n    def report(self, each_class=False):\n        precisions = []\n        recalls = []\n        f1s = []\n        for Cls in range(self.nCls):\n            recall = self.table[Cls,0] \/ (self.table[Cls,0] + self.table[Cls,3] + self.eps) # TP \/ (TP + FN)\n            precision = self.table[Cls,0] \/ (self.table[Cls,0] + self.table[Cls,2] + self.eps) # TP \/ (TP + FP)\n            f1 = 2 * precision * recall \/ (precision + recall + self.eps)\n            precisions.append(precision)\n            recalls.append(recall)\n            f1s.append(f1)\n            \n        total_TP = np.sum(self.table[:, 0]) # all true positives \n        \n        accuracy = total_TP\/self.N\n        accuracy_mean_class = np.mean(precisions)\n        \n        macro_f1 = np.mean(f1s)\n        \n        # use macro_f1 to measure performance\n        self._measure = macro_f1\n        \n        text =    f\"Macro      F1       = {macro_f1:.4f}\\n\"\n        text += f\"\\tOverall    Accuracy = {accuracy:.4f}({total_TP}\/{self.N})\\n\"\n        text += f\"\\tMean-class Accuracy = {accuracy_mean_class:.4f}\\n\"\n        \n        if each_class:\n            for Cls in range(self.nCls):\n                #if precisions[Cls] != 0 or recalls[Cls] != 0:\n                text += f\"\\tClass {str(Cls)+'('+self.names[Cls]+')' if self.names is not None else Cls}: precision = {precisions[Cls]:.3f} recall = {recalls[Cls]:.3f}\\n\"\n\n        return text","79394b87":"def five_type_remove_drift(train):\n    # CLEAN TRAIN BATCH 2\n    a=500000; b=600000 \n    train.loc[train.index[a:b],'signal'] = train.signal[a:b].values - 3*(train.time.values[a:b] - 50)\/10.\n    def f(x,low,high,mid): \n        return -((-low+high)\/625)*(x-mid)**2+high -low\n    # CLEAN TRAIN BATCH 7\n    batch = 7; a = 500000*(batch-1); b = 500000*batch\n    train.loc[train.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,-1.817,3.186,325)\n    # CLEAN TRAIN BATCH 8\n    batch = 8; a = 500000*(batch-1); b = 500000*batch\n    train.loc[train.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,-0.094,4.936,375)\n    # CLEAN TRAIN BATCH 9\n    batch = 9; a = 500000*(batch-1); b = 500000*batch\n    train.loc[train.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,1.715,6.689,425)\n    # CLEAN TRAIN BATCH 10\n    batch = 10; a = 500000*(batch-1); b = 500000*batch\n    train.loc[train.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,3.361,8.45,475)\n    return train","20806f74":"if TRAIN:\n    df_train = five_type_remove_drift(df_train)\n    # 1 slow\n    batch = 1; a = 500000*(batch-1); b = 500000*batch\n    batch = 2; c = 500000*(batch-1); d = 500000*batch\n    X_train_1s = np.concatenate([df_train.signal.values[a:b],df_train.signal.values[c:d]]).reshape((-1,1))\n    y_train_1s = np.concatenate([df_train.open_channels.values[a:b],df_train.open_channels.values[c:d]]).reshape((-1,1))\n    # 1 fast\n    batch = 3; a = 500000*(batch-1); b = 500000*batch\n    batch = 7; c = 500000*(batch-1); d = 500000*batch\n    X_train_1f = np.concatenate([df_train.signal.values[a:b],df_train.signal.values[c:d]]).reshape((-1,1))\n    y_train_1f = np.concatenate([df_train.open_channels.values[a:b],df_train.open_channels.values[c:d]]).reshape((-1,1))\n    # 3\n    batch = 4; a = 500000*(batch-1); b = 500000*batch\n    batch = 8; c = 500000*(batch-1); d = 500000*batch\n    X_train_3 = np.concatenate([df_train.signal.values[a:b],df_train.signal.values[c:d]]).reshape((-1,1))\n    y_train_3 = np.concatenate([df_train.open_channels.values[a:b],df_train.open_channels.values[c:d]]).reshape((-1,1))\n    # 5\n    batch = 6; a = 500000*(batch-1); b = 500000*batch\n    batch = 9; c = 500000*(batch-1); d = 500000*batch\n    X_train_5 = np.concatenate([df_train.signal.values[a:b],df_train.signal.values[c:d]]).reshape((-1,1))\n    y_train_5 = np.concatenate([df_train.open_channels.values[a:b],df_train.open_channels.values[c:d]]).reshape((-1,1))\n    # 10\n    batch = 5; a = 500000*(batch-1); b = 500000*batch\n    batch = 10; c = 500000*(batch-1); d = 500000*batch\n    X_train_10 = np.concatenate([df_train.signal.values[a:b],df_train.signal.values[c:d]]).reshape((-1,1))\n    y_train_10 = np.concatenate([df_train.open_channels.values[a:b],df_train.open_channels.values[c:d]]).reshape((-1,1))\n\n    for X, y, num_classes, model_name, max_epoch in zip(\n            [X_train_1s, X_train_1f, X_train_3, X_train_5, X_train_10], \n            [y_train_1s, y_train_1f, y_train_3, y_train_5, y_train_10],\n            np.array([1, 1, 3, 5, 10])+1,\n            ['1s', '1f', '3', '5', '10'],\n            [30, 30, 60, 60, 120],\n        ):\n\n        X = X.reshape(-1, 1, time_step)\n        y = y.reshape(-1, time_step)\n\n        # split\n        idx = np.arange(X.shape[0])\n        kf = model_selection.KFold(5, shuffle=True, random_state=seed)\n        \n        for fold in range(5):\n            # save in different folders\n            workspace = f'{fold}'\n            \n            train_idx, val_idx = list(kf.split(idx))[fold]\n            \n            print(\"fold:\", fold)\n            print(\"model_name:\", model_name)\n            print(\"train dataset shape:\", X[train_idx].shape, y[train_idx].shape)\n\n            train_dataset = IonDataset(X[train_idx], y[train_idx], 'train', train_augment)\n            valid_dataset = IonDataset(X[val_idx], y[val_idx], 'train', valid_augment)\n\n            loaders = {\n                'train': DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True),\n                'valid': DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True),\n            }\n\n            model = Unet(num_classes=num_classes)\n\n            loss_function = nn.CrossEntropyLoss()\n\n            optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n            scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1, min_lr=1e-8)\n\n            metrics = [ClassificationMeter(num_classes),]\n\n            trainer = Trainer(device, workspace, model, optimizer, scheduler, loss_function, loaders, metrics,\n                              model_name=model_name,\n                              report_step_interval=-1,\n                              #input_shape=(1, time_step),\n                             )\n\n            trainer.train(max_epoch)\n\n","d31bac14":"if PREDICT:\n    ### preprocess\n    # REMOVE BATCH 1 DRIFT\n    start=500\n    a = 0; b = 100000\n    df_test.loc[df_test.index[a:b],'signal'] = df_test.signal.values[a:b] - 3*(df_test.time.values[a:b]-start)\/10.\n    start=510\n    a = 100000; b = 200000\n    df_test.loc[df_test.index[a:b],'signal'] = df_test.signal.values[a:b] - 3*(df_test.time.values[a:b]-start)\/10.\n    start=540\n    a = 400000; b = 500000\n    df_test.loc[df_test.index[a:b],'signal'] = df_test.signal.values[a:b] - 3*(df_test.time.values[a:b]-start)\/10.\n    # REMOVE BATCH 2 DRIFT\n    start=560\n    a = 600000; b = 700000\n    df_test.loc[df_test.index[a:b],'signal'] = df_test.signal.values[a:b] - 3*(df_test.time.values[a:b]-start)\/10.\n    start=570\n    a = 700000; b = 800000\n    df_test.loc[df_test.index[a:b],'signal'] = df_test.signal.values[a:b] - 3*(df_test.time.values[a:b]-start)\/10.\n    start=580\n    a = 800000; b = 900000\n    df_test.loc[df_test.index[a:b],'signal'] = df_test.signal.values[a:b] - 3*(df_test.time.values[a:b]-start)\/10.\n    # REMOVE BATCH 3 DRIFT\n    def f(x):\n        return -(0.00788)*(x-625)**2+2.345 +2.58\n    a = 1000000; b = 1500000\n    df_test.loc[df_test.index[a:b],'signal'] = df_test.signal.values[a:b] - f(df_test.time[a:b].values)\n    \n    ## predict\n    sub = pd.read_csv(\"..\/input\/liverpool-ion-switching\/sample_submission.csv\", dtype={'time':str})\n\n    predictors = {}\n    for num_classes, model_name in zip(\n            np.array([1, 1, 3, 5, 10])+1,\n            ['1s', '1f', '3', '5', '10']\n        ):\n\n        # ensemble happens here\n        models = [\n                Unet(num_classes=num_classes),\n                Unet(num_classes=num_classes),\n                Unet(num_classes=num_classes),\n                Unet(num_classes=num_classes),\n                Unet(num_classes=num_classes),\n            ]\n        predictor = Predictor(device, models)\n        predictor.load_checkpoint(os.path.join(\"0\", f\"{model_name}_best.pth.tar\"), 0)\n        predictor.load_checkpoint(os.path.join(\"1\", f\"{model_name}_best.pth.tar\"), 1)\n        predictor.load_checkpoint(os.path.join(\"2\", f\"{model_name}_best.pth.tar\"), 2)\n        predictor.load_checkpoint(os.path.join(\"3\", f\"{model_name}_best.pth.tar\"), 3)\n        predictor.load_checkpoint(os.path.join(\"4\", f\"{model_name}_best.pth.tar\"), 4)\n\n        predictors[model_name] = predictor\n\n    for start, end, model_type in zip(\n            np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])*100000,\n            np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20])*100000,\n            ['1s', '3', '5', '1s', '1f', '10', '5', '10', '1s', '3', '1s'],\n        ):\n        X = df_test.signal.values[start:end].reshape(-1, 1, time_step_test)\n        test_dataset = IonDataset(X, type='test')\n        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n        res = predictors[model_type].predict(test_loader)\n        sub.iloc[start:end,1] = res\n\n    sub.to_csv(\"submission.csv\", index=False)\n    \n    plt.figure(figsize=(20,5))\n    res = 1000\n    let = ['A','B','C','D','E','F','G','H','I','J']\n    plt.plot(range(0,df_test.shape[0],res),sub.open_channels[0::res])\n    for i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\n    for i in range(21): plt.plot([i*100000,i*100000],[-5,12.5],'r:')\n    for k in range(4): plt.text(k*500000+250000,10,str(k+1),size=20)\n    for k in range(10): plt.text(k*100000+40000,7.5,let[k],size=16)\n    plt.title('Test Data Predictions',size=16)\n    plt.show()\n    ","7f6c28a9":"### U-Net with pytorch\n\n* Drift removal: https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930\/notebook\n* Five Unet models (1f, 1s, 3, 5, 10).","6af19899":"### Trainer and Predictor\nBasically ignore them.","3bfd3fea":"### The model"}}