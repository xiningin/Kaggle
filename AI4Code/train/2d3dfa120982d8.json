{"cell_type":{"3fdfaa09":"code","097a613e":"code","80e0e973":"code","fb42de74":"code","4f4ba000":"code","12beee64":"code","46dd4dfb":"code","9543b320":"code","f94ed3bf":"code","09376217":"code","1b988d71":"code","9e463f90":"code","b4e1d870":"code","7a5603dc":"code","31b7271a":"code","4836ebb1":"code","aeab8b9d":"code","898904d3":"code","cf4bfaac":"code","e4da4ac8":"markdown","cfdf9c86":"markdown","2815133b":"markdown","ea9a4b9a":"markdown","e6482672":"markdown","d061e4c5":"markdown","c87cd232":"markdown","55bc5b65":"markdown","aee4d10b":"markdown","4615045f":"markdown","8e927e89":"markdown","aef440b4":"markdown","0a108afa":"markdown","e44b16a0":"markdown","a2f823d2":"markdown","758a8002":"markdown","6b77bb18":"markdown"},"source":{"3fdfaa09":"import numpy as np \nimport pandas as pd \nfrom IPython.display import display_html\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.gridspec as gridspec\nimport matplotlib.cm as cm\nimport matplotlib_venn as vplt\nimport squarify  \nsns.set_style(\"ticks\")\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import silhouette_samples,silhouette_score\n\n#------ Unsupervised Machine Learning \nfrom sklearn.cluster import KMeans,DBSCAN,MeanShift,estimate_bandwidth,OPTICS\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.decomposition import PCA","097a613e":"from itertools import chain,cycle\ndef display_side_by_side(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.render()\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n    \n#### Visualization\ndef get_label_rotation(angle, offset):\n    rotation = np.rad2deg(angle + offset)\n    if angle <= np.pi:\n        alignment = \"right\"\n        rotation = rotation + 180\n    else: \n        alignment = \"left\"\n    return rotation, alignment\n\ndef add_labels(angles, values, labels, offset, ax):\n    # This is the space between the end of the bar and the label\n    padding =3\n    \n    # Iterate over angles, values, and labels, to add all of them.\n    for angle, value, label, in zip(angles, values, labels):\n        angle = angle\n        # Obtain text rotation and alignment\n        rotation, alignment = get_label_rotation(angle, offset)\n        # And finally add the text\n        ax.text(x=angle, y=value*0.02 + padding, fontsize=8,\n            s=label, ha=alignment, va=\"center\", \n            rotation=rotation, rotation_mode=\"anchor\")","80e0e973":"path = '..\/input\/customer-personality-analysis\/marketing_campaign.csv'\ndf = pd.read_csv(path, delimiter = '\\t')\n\ndf.head(10).sort_values(by ='Income').style.format({\"ID\":\"#{:}\",\"Income\":\"${:2,.0f}\"}).hide_index()\\\n.bar(subset = ['ID'], color='lightgreen')\\\n.bar(subset = ['Recency'], color='lightgreen')\\\n.bar(subset = ['MntMeatProducts'], color='lightgreen')\\\n.bar(subset = ['Year_Birth'], color='#FFA07A')\\\n.bar(subset = ['Income'], color ='#ee1f5f')\\\n.applymap(lambda x: f\"color:{'red' if isinstance(x,str) else 'black'}\")","fb42de74":"from IPython.display import display_html\n\n# --------------- Finding Nan, Null, dtype \nnan_= df.isna().sum().to_dict()\nnull_ = df.isnull().sum().to_dict()\ndtypes_ = df.dtypes.to_dict()\n\n# --------------- Creating Data Set and Foramting \nexplo_ = pd.DataFrame(data = [dtypes_,nan_,null_,], index =['dytypes','Null','Nan']).T.astype({'Null':int,'Nan':int})\n\n# --------------- Formatting Options \nexplo_ = explo_.sort_values(by='Nan', ascending=False).head(8).style\\\n.applymap(lambda x: f\"color:{'red' if isinstance(x,int) else 'black'}\")\\\n.set_table_attributes(\"style='display:inline'\")\\\n.set_caption('Nan-Non-dtype Values')\n\ndescribe_ = df.describe().iloc[:,:3].style.\\\nformat(lambda x:round(x)).bar(color= 'lightgreen', axis = 1)\\\n.highlight_null(null_color='yellow')\\\n.set_properties(**{'width': '1px'}).\\\nformat('{:}').set_table_attributes(\"style='display:inline'\")\\\n.set_caption('Summary Showing Stats attributes')\n\noriginal = df[df.isna().any(axis = 1)][['Education','Marital_Status','Income']].head(8).style\\\n.set_caption('Nan Values')\\\n.applymap(lambda x: f\"color:{'grey' if isinstance(x,str) else 'black'}\")\n\n\ndisplay_side_by_side(original,describe_,explo_)","4f4ba000":"# ------------------Grouping Nan Values by Education and Martial_status <-----------------\n\nsample = df[df.isna().any(axis = 1)].groupby(['Education','Marital_Status']).mean().reset_index()\n\n# Extracing index to show the results <-----------------\nindex_ = df[df.isna().any(axis = 1)].index\n\n# Extracting Unique Values <-----------------\nEdu_, Stat_ = list(sample.Education.unique()) , list(sample.Marital_Status.unique())\n\n#----------Grouping No-Nan Values by Education.,Martial_status and Income <-----------------\n\nvalues_ = df.groupby(['Education','Marital_Status']).mean().round(0).reset_index()\n\n#Filtering by unque values in Martial_Status and Education  <-----------------\nvalues_ = values_[values_.Education.isin(Edu_) & values_.Marital_Status.isin(Stat_)][['Education','Marital_Status','Income']]\n\n#Creating Zip : tuples  of 3 (Education level, Martial_status, Income) <-----------------\nzipped_ = list(zip(values_.Education,values_.Marital_Status,values_.Income))\n\n#---------------------Imputing--------------------------\n\ndef imputing(col):\n    Education_ = col[0]\n    Marital_Status_ = col[1]\n    Income_ = col[2]\n    \n    if pd.isnull(Income_):\n        for tuple_ in range(len(zipped_)):\n            if Education_ == zipped_[tuple_][0]:\n                if Marital_Status_ == zipped_[tuple_][1]:\n                    return zipped_[tuple_][2]\n    else:\n        return Income_\n\ndf['Income'] = df[['Education','Marital_Status','Income','Income','Kidhome']].apply(imputing,axis = 1)\n\n# --------------- Creating Data Set ---------------------------\nDF_1 = sample[['Education','Marital_Status','Income']].style.set_caption('Group Nan Values by Education and Marital_Status')\nDF_2 = values_.head(11).style.set_caption('Group No - Nan Values by Education').bar(subset = ['Income'], color='lightgreen')\nDF_3 = df.loc[index_,['Education','Marital_Status','Income']].head(11).sort_values(['Education','Marital_Status']).style.set_caption('Filling Missing Values')\n\ndisplay_side_by_side(DF_1,DF_2,DF_3)","12beee64":"sns.set_style(\"whitegrid\")\nfig = plt.figure(constrained_layout=True, figsize =(20,10))\ngrid = gridspec.GridSpec(ncols=6, nrows=4, figure=fig)\n\n\n# --------- Boxplot plot Vertical ---------\nax1 = fig.add_subplot(grid[:, 1:2])\nax1.set_title('Income Segmented by Education')\nax1.annotate('Outlier', xy = (0.1, 666666), xytext=(0.7, 666666),arrowprops=dict(facecolor='black', shrink=0.05))\nsns.swarmplot(y='Income', x='Education', data=df,size = 2,ax=ax1)\n\n\n# ----- Education Distribution ---------\nax2 = fig.add_subplot(grid[:2, 2:4])\nax2.set_title('Education Distribution')\nnames_= list(df['Education'].value_counts().index)\nsize_= list(df['Education'].value_counts().values)\nlabels = [f'{name}: {values}' for name,values in zip(names_,size_)]\n\n#-->Creating circle \nmy_circle = plt.Circle( (0,0), 0.7 , color='white')\n\n#-->customizing Wedges\nplt.pie(size_, labels= labels , wedgeprops = { 'linewidth' : 7,'edgecolor':'white'})\nax2= plt.gcf()\nax2.gca().add_artist(my_circle);\n\n# --------- Circular Bar plot ------------------\nax3 = fig.add_subplot(grid[2:4,2:4], projection='polar')\nax3.set_title('Marital_Status and Education Distirbution')\nINFO = df.groupby(['Education','Marital_Status']).sum()['NumDealsPurchases'].reset_index()\nGroups_ = list(INFO.Education.unique())\nGroups_size = list(INFO.Education.value_counts().values)\nLabels_ = list(INFO.Marital_Status.values)\nValues_ = list(INFO['NumDealsPurchases'].values)\n\nPAD = 3\nANGLES_N = len(Values_) + PAD * len(np.unique(Groups_))\nANGLES = np.linspace(0, 2 * np.pi, num=ANGLES_N, endpoint=False)\nWIDTH = (2 * np.pi) \/ len(ANGLES)\n\noffset,OFFSET = 0, np.pi \/ 2\nIDXS,GROUPS_SIZE = [],Groups_size\n\nfor size in GROUPS_SIZE:\n    IDXS += list(range(offset + PAD, offset + size + PAD))\n    offset += size + PAD\n\nax3.set_theta_offset(OFFSET)\nax3.set_ylim(-100, 100)\nax3.set_frame_on(False)\nax3.xaxis.grid(False)\nax3.yaxis.grid(False)\nax3.set_xticks([])\nax3.set_yticks([])\n\nCOLORS = [f\"C{i}\" for i, size in enumerate(GROUPS_SIZE) for _ in range(size)]\n\nax3.bar(ANGLES[IDXS], Values_, width=WIDTH, color=COLORS, edgecolor=\"white\", linewidth=2)\nadd_labels(ANGLES[IDXS], Values_, Labels_, OFFSET, ax3)\noffset = 0\n\nfor group, size in zip(Groups_, GROUPS_SIZE):  # Add line below bars\n    x1 = np.linspace(ANGLES[offset + PAD], ANGLES[offset + size + PAD - 1], num=50)\n    ax3.plot(x1, [-5] * 50, color=\"#333333\")\n    ax3.text(np.mean(x1), -30, group, color=\"#333333\", fontsize=8, fontweight=\"bold\", ha=\"center\", va=\"center\")\n    x2 = np.linspace(ANGLES[offset], ANGLES[offset + PAD - 1], num=50)\n    ax3.plot(x2, [20] * 50, color=\"#bebebe\", lw=0.8)\n    ax3.plot(x2, [40] * 50, color=\"#bebebe\", lw=0.8)\n    ax3.plot(x2, [60] * 50, color=\"#bebebe\", lw=0.8)\n    ax3.plot(x2, [80] * 50, color=\"#bebebe\", lw=0.8)\n    offset += size + PAD\n    \n# ---- Several Boxplot -----\nax = [fig.add_subplot(grid[i, 0:1]) for i in range(4)]\namount = ['MntWines', 'MntFruits','MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']\n\nfor axes, column in zip(ax,amount):\n    axes.annotate('Advice to be combined', xy = (df[column].max() - 100, 5),arrowprops=dict(facecolor='black', shrink=0.05))\n    sns.boxenplot(x = df[column] , ax=axes,  palette=\"viridis\")\n\n# --------- bar plot Vertical ---------\nax8 = fig.add_subplot(grid[:2, 4:6])\nax8.set_title('Marital_Status Distribution')\nax8.annotate('Advice to be combined', xy = (6, 120), xytext=(5, 220),arrowprops=dict(facecolor='black', shrink=0.05))\nbar = sns.countplot(x='Marital_Status', data = df, ax = ax8, orient='h',palette='Paired')\nbar.set_xticklabels(bar.get_xticklabels(),  rotation=90, horizontalalignment='right')\nsns.despine(left=True, bottom=True ,ax=ax8)\nax8.get_yaxis().set_visible(False)\ni=0\n\n\nfor p in bar.patches:\n    height = p.get_height()\n    bar.text(p.get_x()+p.get_width()\/2., height + 0.1,\n        df['Marital_Status'].value_counts()[i],ha=\"center\")\n    i += 1\n    \n    \n# --------- Complains Distribution  ---------  \nax9 = fig.add_subplot(grid[2: ,4:6])\nax9.set_title('Total Complains segmented by Marital_Status')\ncate = df.groupby(by = 'Marital_Status')['Complain'].sum().index\nvalues = df.groupby(by = 'Marital_Status')['Complain'].sum().values + 1\nlabels = [f\"{cate_}\\n{values_}\" for cate_,values_ in zip(cate,values)]\ncolor = ['#dd4124','#009473', '#b4b4b4', '#336b87','#A262A0']\nsquarify.plot(sizes=values, label=labels, alpha=.9 ,ax=ax9,color=color)\nax9.axis('off');","46dd4dfb":"int_ = df[['Income', 'Kidhome', 'Teenhome', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n            'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases','NumWebPurchases']]\ncluster = sns.clustermap(int_, standard_scale=1, metric=\"euclidean\", method=\"ward\", cmap=\"viridis\",figsize=(20,8))\ncluster.gs.update(left=0.10, right=0.50)\ncluster.cax.set_visible(False)\ngs2 = matplotlib.gridspec.GridSpec(1,1, left=0.65)\nax2 = cluster.fig.add_subplot(gs2[0])\nsns.heatmap(data=int_.corr() , ax = ax2, cmap='viridis')\nplt.show()","9543b320":"# -- Dropping Outlier from Income \ndf.drop(df[df['Income']>600000].index, inplace= True)\n\n#Identifying and Ensembling to Outliers-data set\nLOF = LocalOutlierFactor(novelty=False)\ndata = df.select_dtypes(['int64','float64'])\ndata['Outliers'] = LOF.fit_predict(data)\n\n#Finding Ourliers\noutliers = data[data['Outliers'] ==-1].index\nprint(f\"--------------------> Let's Visualize the outliers using Income~Education and  Marital_Status ~ Amount of Wine \\nIn here Number of outliers {len(outliers)}, showed in Red\")\n\n#Visualizations\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nout_liers = data.iloc[outliers]\n\nsns.boxenplot(x='Education' ,y ='Income' , data=df, ax=ax[0])\nsns.scatterplot(x=df['Education'] ,y ='Income' , data=out_liers, color ='r',ax =ax[0])\n\nsns.boxenplot(x='Marital_Status' ,y ='MntWines' , data=df, ax=ax[1])\nsns.scatterplot(x=df['Marital_Status'] ,y ='MntWines' , data=out_liers, color ='r',ax =ax[1]);\ndf.drop(outliers , inplace=True)","f94ed3bf":"data = df.copy() # copy for Visualization\n\n# ---------- Splitting Columns ----------\ncategorical = df.select_dtypes('object').columns[:-1] #-1 because datetime is not categorical \nTime_Data = df.select_dtypes('object').columns[-1] # only time \n\n#  ---------- Drop ID, Drop Z_Revenue which has no relevance ----------\ndf.drop(['ID','Z_CostContact','Z_Revenue'], axis = 1 , inplace= True)\n\n#  ---------- Extract customer Age and later create bins ---------- \nLatest_day = max(pd.to_datetime(df[Time_Data])) #Latest Time\ndf['Year_Birth'] = df['Year_Birth'].apply(lambda x: int(Latest_day.year) - int(x))\n\n# Creating bins \nlabels = ['Young', 'Adult', 'Mature', 'Senior']\nbins = [0, 30, 45, 65, 121]\ndf['Year_Birth'] = pd.cut(df['Year_Birth'],bins=bins , labels= labels).astype(\"object\")\n\n#  ---------- Reduce the dimmensionality in Education  ---------- \nnew_labels = {'PhD':'Postgraduate' , 'Master':'Postgraduate','Basic':'Undergraduate','2n Cycle':'Undergraduate'}\ndf['Education'].replace(new_labels, inplace=True)\n\n# ---------- Combine Martial_status -- Alone, Yolo and single-- into single ---------- \nnew_labels = {'Alone':'Single','YOLO':'Single'}\ndf['Marital_Status'].replace(new_labels, inplace=True)\n\n# ---------- Combine Kidhome and Teenhome ---------- \ndf['Family_members'] = df['Kidhome'] + df['Teenhome']\ndf.drop(['Kidhome','Teenhome'],axis=1, inplace= True)\n\n# ---------- Create bins from Mntxxx into new segments ----------\nMnt = ['MntWines', 'MntFruits', 'MntMeatProducts','MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\nlabels = ['Low consumer', 'Frequent consumer', 'Biggest consumer']\n\nfor col in Mnt:\n    df[col] = pd.qcut(df[df[col]>0][col],3, labels=labels).astype(\"object\")\n    df[col].replace(np.nan, \"Non consumer\",inplace=True)\n    \n# ---------- Sum up all the spent ----------\ndf['TotWebPurc'] = df['NumWebPurchases']+df['NumCatalogPurchases']+df['NumStorePurchases']+df['NumDealsPurchases']\n\n\ncolumns = ['Year_Birth','Education','Marital_Status','MntMeatProducts','MntFishProducts']\ndisplay_side_by_side(data[columns].head(5).style.set_caption('Original Dataset').applymap(lambda x: f\"color:{'red' if isinstance(x,str) else 'black'}\"),\n                     df[columns].head(5).style.set_caption('After Creating Bins').applymap(lambda x: f\"color:{'red' if isinstance(x,str) else 'black'}\"))","09376217":"# ---------- Data into date_time ----------\ncopy = df.copy()\nLatest_day = max(pd.to_datetime(df['Dt_Customer']))\nprint(f'The Latest day in the data set is: {Latest_day}')\n\n#  ---------- Substracting the Number of day ---------- \ndf_time = pd.DataFrame(pd.to_datetime(df['Dt_Customer']))\ndf_time['Dt_Customer'] =  df_time['Dt_Customer'].apply(lambda x:(Latest_day - x).days )\n\n# ---------- Applying Label Encoder ----------\ncategorical = [col for col in df.columns if df[col].dtype=='object' and col!= 'Dt_Customer' ]\n#Saving for Inverse_Transformation\nLabel = LabelEncoder()\nLabel_ = Label.fit(df[categorical].values.flatten())\n\ndf_enco= df[categorical].apply(LabelEncoder().fit_transform)# Applying Label encoder\n\n#---------- Putting all together ----------\nall_columns_ = list(df_time.columns.append(df_enco.columns))\ndf.drop(all_columns_, axis =1, inplace=True)\ndf = pd.concat([df,df_enco,df_time],axis = 1).reset_index(drop= True)\n\n\ncolumns= ['Year_Birth','Education','Marital_Status','Dt_Customer']\ndisplay_side_by_side(data.loc[:,columns].head(5).style.set_caption('Original Dataset').applymap(lambda x: f\"color:{'red' if isinstance(x,str) else 'black'}\"),\n                     df_time.loc[:,['Dt_Customer']].head(5).style.set_caption('Days being a Customer'),\n                     df[columns].head(5).style.set_caption('After Label Encoding').applymap(lambda x: f\"color:{'red' if isinstance(x,str) else 'black'}\"))","1b988d71":"#checking Skew \nlog_columns = df.skew().sort_values(ascending=False)\nlog_columns = log_columns.loc[log_columns>0.75]\nprint(f'Columns that need to be transform {list(log_columns.index)[:5]}...')\n\n# Customer Numpy Log \nfor i in list(log_columns.index):\n    df[i] = np.log1p(df[i])\n\n# MinMax\nMinMax =  MinMaxScaler()\nX = MinMax.fit_transform(df)\n\n\n# Convert the original data\ndf = pd.DataFrame(X, columns = df.columns)\ndf.head(5)","9e463f90":"X = df.values\n\n\n\ndef Best_Kmean(X,k,random_state=104):\n    \n    #taking Dataset copy to Find Clusters\n    df_cluster = df.copy\n    \n    # Creting K\n    silhouette_=[]\n    Inertia = []\n    range_n_clusters = list(range(2,k+1))\n    time = 0\n    \n    \n    for n_clusters in range_n_clusters:\n        fig,(ax1,ax2,ax3,ax4) = plt.subplots(1,4,figsize =(25,5))\n        ax1.set_xlim([-0.1, 1])\n        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n        \n        clusterer = KMeans(n_clusters=n_clusters, random_state=random_state,init = 'k-means++')\n        cluster_labels = clusterer.fit_predict(X)\n        silhouette_avg = silhouette_score(X, cluster_labels)\n        silhouette_.append(silhouette_avg)\n        Inertia.append(clusterer.inertia_)\n        unique, counts = np.unique(cluster_labels, return_counts=True)\n        #print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)\n        \n        # Compute the silhouette scores for each sample\n        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n        y_lower = 10\n        for i in range(n_clusters):\n            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n            ith_cluster_silhouette_values.sort()\n            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n            y_upper = y_lower + size_cluster_i\n            color = cm.nipy_spectral(float(i) \/ n_clusters)\n            ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,facecolor=color, edgecolor=color, alpha=0.7)\n\n            # Label the silhouette plots with their cluster numbers at the middle\n            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i+1))\n            # Compute the new y_lower for next plot\n            y_lower = y_upper + 10  # 10 for the 0 samples\n\n        ax1.set_title(\"The silhouette plot for the various clusters.\")\n        ax1.set_xlabel(\"The silhouette coefficient values\")\n        ax1.set_ylabel(\"Cluster label\")\n\n        # The vertical line for average silhouette score of all the values\n        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n        ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n    \n        #---------------------> Optimal K silhouette_\n        k_optimal = np.argmax(silhouette_) + 2\n        ax2.plot(range(2,n_clusters),silhouette_[:time],linestyle = '--',color = 'blue', linewidth =3, markersize= 8)\n        ax2.scatter(n_clusters,silhouette_avg)\n        time +=1\n        \n        #Label and More\n        ax2.set_title('Silhouette Curve for optimal Number of clusters' , family = 'Arial', fontsize = 8)\n        ax2.set_xlabel(f'K={n_clusters}', fontsize = 14, family = 'Arial') \n        ax2.set_ylabel('Silhouette_score', fontsize = 14, family = 'Arial')\n        ax2.axvline(x=k_optimal,label='Optimal number of cluster ({optimal})'.format(optimal=k_optimal),linestyle = '--', color ='red')\n        ax2.set_yticks(np.linspace(0.1,1,k))  \n        ax2.set_xticks(np.arange(2,k+1))\n        ax2.scatter(k_optimal ,silhouette_[k_optimal-2], color = 'red', s=100)\n        ax2.legend(fontsize = 'medium')\n        \n        #---------------------> K - Elbow Method\n        ax3.set_title('Elbow method for Optimal K' , family = 'Arial', fontsize = 10)\n        ax3.plot(np.arange(2,n_clusters+1),Inertia[:time],linestyle = '--',color = 'blue', linewidth =3)\n        ax3.scatter(np.arange(2,n_clusters+1),Inertia[:time],color = 'red') #dots\n        ax3.scatter(n_clusters,clusterer.inertia_,color = 'red', s=100) #dots\n        ax3.set_xlabel('Number of Clusters')\n        ax3.set_ylabel('Inertia')\n        \n        #---------------------> Bar Plot\n        ax4.set_title('Cluster Distribution',family = 'Arial', fontsize = 8)\n        bar = sns.barplot(x=unique+1, y = counts ,palette='Paired')\n        bar.set_xticklabels(bar.get_xticklabels(),  rotation=90, horizontalalignment='right');\n        sns.despine(left=True, bottom=True ,ax=ax4)\n        ax4.get_yaxis().set_visible(False)\n        i=0\n\n\n        for p in bar.patches:\n            height = p.get_height()\n            bar.text(p.get_x()+p.get_width()\/2., height + 0.1, counts[i],ha=\"center\")\n            i += 1\n\n    plt.show()\nBest_Kmean(X,10)","b4e1d870":"bandwidth = estimate_bandwidth(X, quantile=0.5 ,n_samples=600)\n\nms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(X)\nlabels = ms.labels_\ncluster_centers = ms.cluster_centers_\n\nlabels_unique = np.unique(labels)\nn_clusters_ = len(labels_unique)\n\nprint(\"number of estimated clusters : %d\" % n_clusters_)","7a5603dc":"clustering = OPTICS(min_samples =15, min_cluster_size=3).fit(X)\nunique, counts = np.unique(clustering.labels_, return_counts=True)\nprint(f\"Numbers of Cluster :{unique} where {unique[0]} means Annomaly\")\nprint(f\"Number of Annomaly :{counts[0]}\")","31b7271a":"pca_list = list()\nfeature_weight_list = list()\n\nfor n in range(1, 9):\n    \n    # Create and fit the model\n    PCAmod = PCA(n_components=n)\n    PCAmod.fit(X)\n    \n    # Store the model and variance\n    pca_list.append(pd.Series({'n':n, 'model':PCAmod,'var': PCAmod.explained_variance_ratio_.sum()}))\n    \n    # Calculate and store feature importances\n    weights = PCAmod.components_\n    abs_feature_values = np.abs(PCAmod.components_).sum(axis=0)\n    feature_weight_list.append(pd.DataFrame({'n':n, \n                                             'features': df.columns,\n                                             'values':abs_feature_values\/abs_feature_values.sum()}))\n    \npca_df = pd.concat(pca_list, axis=1).T.set_index('n')\nfeatures_df = (pd.concat(feature_weight_list)\n               .pivot(index='n', columns='features', values='values'))\nax = features_df.plot(kind='bar', figsize=(25,8))\nax.legend(loc='upper right',prop={'size': 6})\nax.set(xlabel='Number of dimensions',\n       ylabel='Relative importance',\n       title='Feature importance vs Dimensions');","4836ebb1":"from sklearn.decomposition import KernelPCA\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\n\ndef scorer(pcamodel, X, y=None):\n    data_inv = pcamodel.fit(X).transform(X)\n    data_inv = pcamodel.inverse_transform(data_inv)\n    mse = mean_squared_error(data_inv.ravel(), X.ravel())\n    return -1.0 * mse\n\n# The grid search parameters\nparam_grid = {'gamma':[0.001, 0.01, 0.05, 0.1, 0.5, 1.0],'n_components': [2, 3, 4, 5, 6, 8, 9]}\nkernelPCA_ = GridSearchCV(KernelPCA(kernel='rbf', fit_inverse_transform=True),\n                         param_grid=param_grid,scoring=scorer,n_jobs=-1)\n\n\nkernelPCA_ = kernelPCA_.fit(df.values)\nprint(f'Best estimator {kernelPCA_.best_estimator_}')\n\n\n#Initiating PCA to reduce dimentions aka features to 9\ncolumns_ =[f'col{n}' for n in np.arange(1,10)]\nPCA_DATA = pd.DataFrame(kernelPCA_.transform(df), columns=(columns_))\nPCA_DATA.head(5)","aeab8b9d":"Best_Kmean(PCA_DATA.values,10)","898904d3":"km = KMeans(n_clusters=4, init='k-means++')\nkm.fit(PCA_DATA.values)\ncopy['Cluster']=km.labels_+1","cf4bfaac":"import cv2\nsns.set_style(\"whitegrid\")\nfig = plt.figure(constrained_layout=True, figsize =(20,10))\ngrid = gridspec.GridSpec(ncols=4, nrows=2, figure=fig)\n\n\n# --------- Boxplot plot Vertical ---------\nax1 = fig.add_subplot(grid[0, 0])\nsns.kdeplot(x = 'Family_members' , data=copy , fill=True, hue='Cluster', palette=['#3b528b','#21918c','#5ec962','#fde725'], alpha=.7, linewidth=0 , ax=ax1)\nax1.get_yaxis().set_visible(False)\n\n\n# ------------ > Customer Segmentation by NumDealsPurchases\nax2 = fig.add_subplot(grid[0, 1])\nsns.kdeplot(x = 'TotWebPurc' , data=copy , fill=True, hue='Cluster', palette=['#3b528b','#21918c','#5ec962','#fde725'], alpha=.8, linewidth=0,ax=ax2)\nax2.get_yaxis().set_visible(False)\n\n# ------------ > Customer Segmentation by Income\nax3 = fig.add_subplot(grid[0, 2])\n# ---------> Vars \nCLI1 = sns.kdeplot(data=copy, x=copy[copy ['Cluster']==1]['Income'],  fill=True, alpha=1 , ax=ax3, color ='#3b528b' )\nCLI2 = sns.kdeplot(data=copy, x=copy[copy ['Cluster']==2]['Income'],  fill=True, alpha=1,  ax=ax3,  color ='#21918c' )\nCLI3 = sns.lineplot(x=x_range, y=kde_3(x_range) * -1, color='#5ec962',ax=ax3)\nCLI4 = sns.lineplot(x=x_range, y=kde_4(x_range) * -1, color='#fde725',ax=ax3)\n\n# ---------> Kde\nkde_3= gaussian_kde(copy[copy ['Cluster']==1]['Income'])\nkde_4 = gaussian_kde(copy[copy ['Cluster']==4]['Income'])\nx_range = np.linspace(min(copy[copy ['Cluster']==4]['Income']), max(copy[copy ['Cluster']==4]['Income']), len(copy[copy ['Cluster']==3]['Income']))\n\n#---------> Labels and more\nsns.despine(left=True, bottom=True ,ax=ax3)\nax3.get_yaxis().set_visible(False)\nax3.fill_between(x_range, kde_3(x_range) * -1, color='#5ec962')\nax3.fill_between(x_range, kde_4(x_range) * -1, color='#fde725')\nax3.legend(['CLI1','CLI2','CLI3','CLI4'])\n\n\n# ------------ > Customer Segmentation by last day after purchase\nax4 = fig.add_subplot(grid[0, 3])\nsns.kdeplot(x = 'Recency' , data=copy , fill=True, hue='Cluster', palette=['#3b528b','#21918c','#5ec962','#fde725'], alpha=.5, linewidth=0 , ax=ax4)\nax4.get_yaxis().set_visible(False)\n\n# ------------> Final \nax5 = fig.add_subplot(grid[1, :])\nimg= cv2.imread('..\/input\/cusomter-personality-clusters\/Woman Running Photo Twitter Header (2).png')\nax5.imshow(img)\nax5.get_yaxis().set_visible(False)\nax5.get_xaxis().set_visible(False)","e4da4ac8":"<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1619038\/2661735\/Woman%20Running%20Photo%20Twitter%20Header%20%281%29.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20211011%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211011T044916Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=4bba3d30eedfea172242c493e54061b42791e1aa0285fe60fc3bf46842961f2299265297deac197ca0ac6aead5b33f018354375ef9ce5e5a83ff4d84e3274c8085b2169a473ce33d870f31c3fdf42b76b018951b0517cee653ccd3dcff4504a9630bd1b4b686c2f74f9d514b8d5bbd1e5e2c51918268407b604f1647f394db9d473abeb12097eac7004c8aaedf85bdb65ef72c82c471af6e893bd3bea2ba59e3f3e3afa0f34a088c3ff4539b1af956b3456c6368405a13a4447a3f70b683ab973e3f514e2600fe5e2d8655bca306f42c8ea466a6d750db300c21eb58026ab456f0416c787f602a04aa777464da16931a262bc28c215d62e955720f219437576e\" style=\"float:right;\" alt=\"Drawing\"\/>","cfdf9c86":"- Here We can see again that 4 are the best way to cluster these customer , however we those -1 that means Anommaly are consider outlier or points that cannot be in any of those cluster , there fore we go to Our enxt step \n\n\n<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#07C2D5 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <a id = \"4\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Dimensionality Reduction | - PCA\n    <\/p>    \n<\/div>\n\nIn Simple Words PCA aims to reduce the dimmensionality , I meant try to Group the variable it is like grouping X1 and X2 information into a new Xn\nThe challenge in this part is to find the proper Numbers of n_components, but again sklearn can help with Randomseach and PCA algorithms\n\nAs rule of thumb It is very common to limit the number of n_component to half the total variables number in the Dataset\n- We Will first take a look of ***Features importance***\n- Calculate the ebst N_component using ***GridSearchCV***","2815133b":"#### ------> Exploring   \n##### Finding NAN , Null Values\n\n- In the following Dataframe you can see ***Income = 2216*** and the others Columns have ***2240*** that shows us that there might be some missing values.\n- In the next dataframe **one in red** you can see those 24 missing values \n- Last DataFrame shows the missing values\n- **So let's start Working on that**","ea9a4b9a":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#07C2D5 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <a id = \"a\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n       Prepocessing | Visualizations\n    <\/p>    \n<\/div>","e6482672":"- The problem with Meanshift is that Cluster are not well Delimited there for MeanShift will not be beneficial\n\n<h3 style=\"float:left;\">OPTICS | DBSCAN ?<\/h3>\n    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n    \n<p>  DBSCAN This is a algorithm that will count how many sample\/instances are located withing a small region known as <b>\u03b5-neighborhood.<\/b>  once that its done any instance that is not a core instance and does not have one inits neighbor hood is considered an anomaly. <b> WARNING <\/b> This algorithm works well if all the clusters are dense enough and if they\nare well separated by low-density regions.<\/p> and the problem is to find <b>\u03b5-neighborhood.<\/b> but luckly Sckit Learn has <b>OPTICS<\/b> which is closely related to DBSCAN because it deviates from the original OPTICS by first performing k-nearest-neighborhood searches on all points to identify core sizes\n","d061e4c5":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#07C2D5 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <a id = \"2\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n       Loading | - Data Cleaning and Pre-processing\n    <\/p>    \n<\/div>\n\n- From this point we must check column by column, **what are we looking for ?**, ***-Nan Values-***, ***-Inconsisntences-***, ***etc***\n- but before let's see a quick description of the data, [in here full details](https:\/\/www.kaggle.com\/imakash3011\/customer-personality-analysis):\n    - The first 7 columns are self-explanatorty **'ID', 'Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome','Teenhome'** except **'Dt_customer'** but the description highlights **'Date of customer's enrollment with the company'**\n        - ***Note*** : Sorted by Income, Red color means Other format that is not Number (ex Date time, string)","c87cd232":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#07C2D5 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 15px;\n          color:white;\">\n        Contents | - Step by step  \n    <\/p>    \n<\/div>\n    \n* [1. Importing Libraries and Loading data](#1) import all libraries needed it in this notebook <b> (Pandas ,numpy ,scikit-learn, etc) <\/b>\n    \n* [2. Data Cleaning,Pre-Processing and visualization](#2) Get familiar with the Data, Find inconsistencies, drop no significant variables etc.\n    \n* [3. Base Model](#3) Implement the first <b>model<\/b>\n    \n* [4. Dimensionality Reduction](#4) Using <b>PCA<\/b> and <b>AutoEncoders<\/b>   \n    \n* [5. Second model Selection](#5) <b>Combine<\/b> several Model to have better performance\n      \n* [6. Profiling Customers](#6) Interpretating Data <b>From numbers to Customers<\/b>\n    \n* [7. Conclusions](#7) Finall Thoughts\n\n<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#07C2D5 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <a id = \"1\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Python | - Libraries \n    <\/p>    \n<\/div>","55bc5b65":"### Object into Date-Time | Using Label Encoder","aee4d10b":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#07C2D5 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <a id = \"5\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Second Model Selecction | - Kmeans - PCA \n    <\/p>    \n<\/div>","4615045f":"***Interpretation***\n\nAs you can see, this visualization is mucher richer when we have many criterias , from left to right we had ***silhouette diagrams***, ***silhouette scores***, ***Elbow Method*** and ***Clustering Interpretation*** in a giving Time.\n- ***silhouette diagrams***: Each diagram contains one knife shape per cluster. The shape\u2019s height indicates the number of instances the cluster contains, and its width represents the sorted silhouette coefficients of the instances in the cluster (wider is better). The dashed line indicates the mean silhouette coefficient. The vertical dashed lines represent the silhouette score for each number of clusters.\n- ***silhouette scores*** It is just the the distance of each point comparing to the same point to another cluster , ex. the distance from A to Cl1, Vs A to CL2 , A to CLn and so on ... sum them up and get the average \n- ***Elbow Method*** and ***Clustering Interpretation*** are just representation of Inertia and Clustering Distribution - How many Customer\/items\/samples have each Cluster\n\n\n- so which one should I choose ...Well..!! it would be really bad say 'this or that answer without justifying why\n     - If you notice ***silhouette diagrams***, ***silhouette scores*** indicate that K = 2 it is a good one,However the distribution showed in ***Clustering Interpretation*** isnt that good\n     - ***silhouette scores*** showed that 3 is as good as 2 ,***Elbow Method*** showed that 3 would be the point that we are looking for (inflection point) and in ***Clustering Interpretation*** graph we have better distribution \n     - So far I would say K=3 is the best one , however I will run Means-shift DBSCAN and PCA to back up my asumptions\n     \n<h3 style=\"float:left;\">Mean shift?<\/h3>\n    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n    \n<p>  Mean Shift is an unsupervised clustering algorithm that aims to discover blobs in a smooth density of samples. which works by updating candidates for centroids to be the mean of the points within a given region.<\/p>","8e927e89":"### Fun facts:\nCheck this out:\n- Heatmap:\n    - Income is realted to the product that people buy, this is obvious but what is funny yo notice is that \"the amount that people buy is not related to how much money they do\".\n    - Kidhome and NumDealPruchase are strongly related like the more kids you have the more thing you buy and the same applies to Teenager at home.\n    - The others are simples ' there is a strong correlation between Fruits, Wines, Meat ,Fish that people are buying'\n- Clustermap:\n    - From left to right :People who buy wines also enjoy buying most of the other products - and if you notice in the heat map Income ~ Wine ~ Meat are strongly related, I would say that buying wine\/Meat is a symbols of status in this DataSet\n    - There are several customer who buys fruits and fish. - If you are the manger of this place you ***should*** put both products near to each other.\n    \n    \n<div style=\"color:black;\n       display:fill;\n       border-radius:10px;\n       background-color:#126F78 ;\n       font-size:90%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <a id = \"a\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n       working on Dimmensionality\n    <\/p>    \n<\/div>\n","aef440b4":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#07C2D5 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 15px;\n          color:white;\">\n        Customer Segmentation using Unsupervised Machine Learning | - Introduction \n    <\/p>    \n<\/div>\n<img src=\"https:\/\/i.pinimg.com\/originals\/63\/8c\/9d\/638c9de7c3d74d67b3e8e11abf5abc79.gif\" \n     style=\"float:right;\" \n     alt=\"Customer Segmentation\" width=\"550\" height=\"800\"\/>\n     \n<h3 style=\"float:left;\">What is Customer segmentation?<\/h3>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n    <p>Paraphrasing <a href = 'https:\/\/stats.stackexchange.com\/questions\/326065\/cross-entropy-vs-sparse-cross-entropy-when-to-use-oneover-the-other'> Wikipedia Definition,<\/a> and using simple words 'it is grouping elements in our case people &nbsp;into subgroups that share similars behaviours. For example: A group of 100 customers that buy Chocolates &nbsp;Vs another 100 Customers that buy Cereals and finally other group of customers that buy only natural products, in here we had 3 groups segmented by their buying behaviours.\n    <\/p>\n\n<h3 style=\"float:left;\">How Machine Learning can help us with that?<\/h3>\n    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n    <p>As we all know Machine learning algorithms look and identify patterns hidden in the data and essentially that is what we need, when it comes to Custoemr segmentation, we have to create and train a model able to indentify those patterns. We dont have to make those algorithm from Zero instead we will us the most famous ones<a href=\"https:\/\/towardsdatascience.com\/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a#:~:text=Kmeans%20algorithm%20is%20an%20iterative,belongs%20to%20only%20one%20group.&text=The%20less%20variation%20we%20have,are%20within%20the%20same%20cluster.\"> K-means Algorithm<\/a>, <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#mean-shift\"> Mean Shift <\/a>, <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#hierarchical-clustering\">Hierarchical clustering<\/a> and finally <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#dbscan\">DBSCAN<\/a> using scikit learn libraries<\/p>\n<h3 style=\"float:left;\">Coming next... it is time to get our hand Dirty...<\/h3>\n\n\n","0a108afa":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#07C2D5 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <a id = \"3\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n        Base Model | - K-menas , DBSCAN , Meanshift \n    <\/p>    \n<\/div>\n\n<img src=\"https:\/\/dashee87.github.io\/images\/kmeans.gif\" \n     style=\"float:right;\" \n     alt=\"Customer Segmentation\" width=\"550\" height=\"800\"\/>\n     \n<h3 style=\"float:left;\">What is Customer segmentation?<\/h3>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n\n\n<p>At this point, we understand what is in the Data , initially exists n groups showed in Education or m groups in Marital_status, So now We have to find a proper numbers of Clusters to encapsulate all those customers , <b>There is no<\/b> 1 special model that can tell us how to do it , instead we have to see the same problem from different point of views to find the most accurate answer, we do not know the proper numbers of clusters however , we know the limit <b>it cannot be 1 and it cannot be greter that Martial_status<\/b><\/p>\n\n<h3 style=\"float:left;\">K Means?<\/h3>\n    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n    \n<p> In Simple words <b>K Means<\/b> refers to a collection of data points aggregated together because of certain similarities. K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible.<\/p>\n\n","e44b16a0":"### Checking skewing and Applying MinMaxScaler","a2f823d2":"- From this point I will take ***7 columns*** each time to work on ...\n- I will Transform String values such as Education and Marital Status into Categorical Values\n- Apply transformation such as [StandarScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html) or [MinMax](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html) which are techniques to encapsulate values between 0-1 or any rank that the data scientist needs \n- Transform ***date*** and time into ***Data time***\n\n### Filling Missing Values - How are we gonna do it?\n\n- ***It will be easy to fill those values with mean of the columns*** , however I have another option:\n* [1. Grouping by Categories](#a) : Group by Education and Marital_status-Income with **Nan-Values** it make sense to assume that Phd and Master earn much more that **2n cycle**, and also make  sense to think the fact of bein married could affect their incomes, now based on that I will group them as mention to extrac all combinatino Education-Marital-Status.\n* [2. Interating](#a) Then I will repeat the process but with No-Nan Values, Create tuples and finally Impute using Pandas all combination match ex if it 2n cycle and Married then input the income \n* [3. Imputing](#a) Take a look of the following data frame (left : Nan values , Center : grouped and segemented by Income , Right : Filling missing Values)\n\n<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#07C2D5 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <a id = \"a\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n       Imputing | - Data Cleaning and Pre-processing\n    <\/p>    \n<\/div>\n","758a8002":"<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#07C2D5 ;\n       font-size:90%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <a id = \"a\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n       Recaping | Visualizations\n    <\/p>    \n<\/div>\n\n### Summary\n\n- Many outliers in the amount of products that customer buys : We must apply some kind of **Normalization**\n- Outlier in Salary - Someone from Graduate groups is earning a lot 600 K .. oh yeahh... bring the money...!!!\n- Marital_status and Educational no equally distributed\n- the Tree map is showing that People who is married are the one who complains the most..!! SUPRISE..!!\n\n\n\nSo let's explain what's next: **It is super important** to highlight that Outliers will affect Meanshift, K-means or any other algorithms that works with Euclidan distance or something similar, the reason: the algorithm must find the nearest point between them and if one of them is far form the other this will afect the cluster - Searching Area-, there fore I will use [LocalOutlierFactor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.LocalOutlierFactor.html)\n\n***From left to right*** We can see on those Boxplots (blue ones) several outliers But **how can we make sure** they are not outlier to the dataset itself, **How is that** here an example , let's assume that in your house everyone can run 1 km in 5 mins but you can make it in 15 mins .. right there **you are** an outlier because you need longer time and this will affect the average speed, however let's assume that now luckly most of people in your city need 20 mins to run the same #km , in that case you are not an outlier in respect to the city members ... so the same happens here \n\n- ***Salary*** is easy to fix, we will drop that guy who earns more than 600K\n- Both ***circular bar*** are telling us that the distribution of Education and Marital Status is not equally distributed there for we can Combine some of then in this case I will combine **Martial_status** --> ***Alone, Yolo and single*** into **single**\n- Reduce the dimmensionality in **Education** by grouping --Master and Phd-- into **Postgraduate**,---Basic and 2n cycle-- into **Undergraduate**\n- Drop **ID, Drop Z_Revenue** which has no relevance because it doesnt specify what is that about.\n- Extract customer **Age** and later create bins -- From the latest day in the data set\n- Combine **Kidhome and Teenhome** and encapsulate them into **Family_members**\n- Create bins from **Mntxxx** into new segments ***-- Non Buyer , Low Buyer, Frequent Buyer, Biggest Buyer --***\n- **Sum up** all the spent to have the total amount spent\n- Transform ***Dt_Customer*** into Date Time and substract the number of day that X customer has been purchasing = the lates date - the current day   \n- I will use [Label Encoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html) to transform non-numericla labels (Graduate, Phd, Single , etc)\n","6b77bb18":"***Conclusion*** Now 3  and also 4 doesnt Look that bad ...!! PCA helped a lot and now I am more confident to Select 4 as the ideal K\n\n<div style=\"color:white;\n       display:fill;\n       border-radius:10px;\n       background-color:#07C2D5 ;\n       font-size:120%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <a id = \"6\"><\/a>\n    <p style=\"padding: 15px;\n          color:white;\">\n       Profiling\n    <\/p>    \n<\/div>\n\n\n- Frist, add labels to each customer \n- Load the OriginalDataset and add values\n- Create kdplot and find some similitaries\n\n\n### Conclusion\n\n- It is posible to create more cluster however that is up to you\/company needs and overall your Manager , at this point you have many details how k means work and how we can cluster all those customer \n- The problem with Unsupervised Machine Learning is that no one know is you are right or wrong that is why I tried to use different techniques but it is up to you and your experience to decide which one is better\n- Please consider 'upvoting' leave a comment or feedback I really want to become Notebook GrandMaster \n- Coming up : AutoEncoders and Unsupervise Deep Learning "}}