{"cell_type":{"0dbf7d40":"code","7c338c15":"code","d91bab9b":"code","6ff582d3":"code","6e261369":"code","47efdce9":"code","31bcd738":"code","443fcb47":"code","5df34734":"code","51301076":"code","48574a40":"code","9276ef27":"code","2d2ab822":"code","89fed7ea":"code","da8276d1":"code","b2337d20":"code","4cc4b0bc":"code","56a6b562":"code","aaaf2eac":"code","6835123f":"code","bb5319cd":"code","ea50dc74":"code","3ca2528e":"code","86edef5f":"code","41cb316b":"code","82b95c2a":"code","c828d14f":"code","f9437950":"code","4706e269":"code","34c66d2a":"code","23326a01":"code","a9801ea0":"code","d1a5c7a9":"code","6ff0a925":"code","978b8944":"code","26867b97":"code","bc5a7165":"code","75555515":"code","fdd466b5":"code","a7690c54":"code","8af35b96":"code","e9c6dc52":"code","72860c36":"code","3e9d204d":"code","5a095244":"code","aea4c0d9":"code","cd6708bf":"code","2055cf23":"code","f7a332ef":"code","1561436d":"code","e69044ac":"code","6b62b821":"code","5a9709a3":"code","5385769e":"code","8a295c80":"code","96239bb7":"code","3cd90e4f":"code","8534cb61":"code","3b34458e":"code","9bd68190":"code","39d20232":"code","4a18a8cb":"code","9e704247":"code","bffcc6fd":"code","6031ab90":"code","30a3bee8":"code","51a9b914":"code","0db5ebb7":"code","7dfe1915":"code","82f940df":"code","8df30334":"code","8e1b4e58":"code","b1be54e7":"code","59acc457":"code","7ccd6a24":"code","c94cb7b9":"code","7289b692":"code","4dbf816c":"code","a0bae85d":"code","1ec9d77d":"code","dfb7bc8b":"code","51e8de10":"code","df7d993f":"code","52c772af":"code","9304dd44":"code","71fc6cd8":"code","851a19cc":"code","ee375811":"code","02cd3391":"code","c35a94fe":"code","6b2bb247":"code","7d545626":"code","728830bd":"code","274341a7":"code","e0b429fa":"code","aa9eee1a":"markdown","449c5d33":"markdown","608d5868":"markdown","84018841":"markdown","89854ae4":"markdown","25905e74":"markdown","fb787e96":"markdown","6d4cafd5":"markdown","e2c59e17":"markdown","9accb0c6":"markdown","029a721b":"markdown","3f567a87":"markdown","4b15ab69":"markdown","9324f381":"markdown","d8749304":"markdown","55f9fe94":"markdown","5d1aca9f":"markdown","31284233":"markdown","d6577e07":"markdown","5c949ed2":"markdown","6cff3b16":"markdown","11737870":"markdown","940c0721":"markdown","76484b53":"markdown","e3e62bde":"markdown","cfce0c94":"markdown","9a9188f9":"markdown","e4bcf859":"markdown","9c3d7826":"markdown","09f1782c":"markdown","5979d93d":"markdown","ac9d6455":"markdown","fd3cffb9":"markdown","42233bea":"markdown","b5f8a07f":"markdown","7ddccfb5":"markdown","3dacf2ef":"markdown","030c0e84":"markdown","8720d197":"markdown","7af1d47a":"markdown","549255ca":"markdown","1a259be7":"markdown","e1a9fa12":"markdown","6d70677c":"markdown","bdda8f70":"markdown","41271909":"markdown","ea2cbaaa":"markdown","bbfd0c6c":"markdown","48d40ef3":"markdown","503ae8e7":"markdown","f1750081":"markdown","6d87c917":"markdown","4ce26761":"markdown","2ec0d2e3":"markdown","04e68e94":"markdown"},"source":{"0dbf7d40":"# Housing Price Prediction using Linear Regression, Decision Tree Regression, Random Forest Regression, Support Vector Regression.","7c338c15":"import pandas as pd    #Pandas is used to analyze data\nimport numpy as np     #This library contains a large number of mathematical, algebraic, and transformation functions\nimport matplotlib.pyplot as plt    # Used for Visualization of data\nimport seaborn as sns    #Seaborn is a Python data visualization library based on matplotlib\n%matplotlib inline","d91bab9b":"# lOADING DATA IN DATA VARIABLE \n\ndata=pd.read_csv(\"..\/input\/housing-dataset-based-on-california\/housing.csv\")","6ff582d3":"# Return a tuple representing the dimensionality of the DataFrame\n\ndata.shape","6e261369":"# Printing First 5 Rows of the dataset\n\ndata.head()","47efdce9":"# Printing Last 5 Rows of the dataset\n\ndata.tail()","31bcd738":"# PRINTING WHOLE DATASET TO HAVE PREVIEW OF IT \n\ndata\n\n# print(data) and data will both print same dataset","443fcb47":"# The info() function is a  method that prints information about a DataFrame including the index dtype and column dtypes,\n# non-null values and memory usage.\n\n\ndata.info()","5df34734":"# The describe() method is used for calculating some statistical data like percentile, \n# mean and std of the numerical values of the Series or DataFrame\n\ndata.describe()","51301076":"#The value_counts() function is used to get a Series containing counts of unique values\n\ndata[\"ocean_proximity\"].value_counts() ","48574a40":"#matplotlib inline sets the backend of matplotlib to the 'inline' backend\n\n%matplotlib inline\n\ndata.hist(figsize=(20,15),bins=50,)\n\nplt.show()","9276ef27":"# Visualization on Ocean Proximity using hbar(Horizontal Barchart)\n\ndata[\"ocean_proximity\"].value_counts().plot(kind=\"barh\",color='y')\nplt.title(' Horizontal Barchart on Ocean Proximity')\nplt.show()","2d2ab822":"# Histogram on Median Income\n\ndata['median_income'].hist(color='r')\nplt.title(' Histogram on Median Income')\nplt.show()","89fed7ea":"data['income_cat']=pd.cut(data['median_income'],\n                    bins=[0.0,1.5,3.0,4.5,6.0,np.inf],\n                    labels=[1,2,3,4,5])\ndata['income_cat'].value_counts()","da8276d1":"# Creating Histogram on Income_cat\n\ndata['income_cat'].hist(color='purple')\nplt.title('Histogram on Income_cat')\nplt.show()","b2337d20":"# Creating X is independent variable \n# Creating y is dependent variable  \n\nX=data.drop('median_house_value',axis=1)\n\ny=data['median_house_value']","4cc4b0bc":"# Printing X\n\nX","56a6b562":"X.shape","aaaf2eac":"# Printing Y\n\ny","6835123f":"y.shape","bb5319cd":"# Spliting Data into training data and testing data \n# For this purpose we will use train_test_split of sklearn\n\nfrom sklearn.model_selection import train_test_split","ea50dc74":"# Creating X_train,y_train,X_test,y_test\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33)","3ca2528e":"X_train.shape","86edef5f":"y_train.shape","41cb316b":"X_test.shape","82b95c2a":"y_test.shape","c828d14f":"# Visualizing our income_cat column from Training dataset\n\nX_train['income_cat'].hist(color='brown')","f9437950":"# Visualizing our income_cat column from Actual dataset\n\ndata['income_cat'].hist(color='brown')","4706e269":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\nfor train_index, test_index in split.split(data,data['income_cat']):\n    strat_train_data=data.loc[train_index]\n    strat_test_data=data.loc[test_index]","34c66d2a":"# Verifying Proper Distribution of Income cat usinf Histogram\n\nstrat_train_data['income_cat'].hist()","23326a01":"strat_train_data['income_cat'].value_counts()\/len(strat_train_data)","a9801ea0":"strat_train_data.drop('income_cat',axis=1,inplace=True)\n\nstrat_test_data.drop('income_cat',axis=1,inplace=True)","d1a5c7a9":"# Creating copy of strat_train_data\n\nhousing=strat_train_data.copy()","6ff0a925":"# Visualizing housing \n\nhousing.hist(bins=50,figsize=(20,15),legend=\"Housing Data\",color='r')\nplt.show()","978b8944":"housing['median_income'].hist(bins=50)\nplt.show()","26867b97":"#ScatterPlot \nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\nplt.show()","bc5a7165":"housing.plot(kind='scatter',x='longitude',y='latitude',alpha=0.4,\n            s=housing['population']\/100,label=\"population\",figsize=(10,7),\n            c=\"median_house_value\",cmap=plt.get_cmap(\"jet\"),colorbar=True,\n            sharex=False)\nplt.show()","75555515":"import matplotlib.image as mpimg\nimport os\ncalifornia_img=mpimg.imread(os.path.join(\"..\/input\/california-area-image\",'california.png'))\nax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n                  s=housing['population']\/100, label=\"Population\",\n                   c=\"median_house_value\",cmap=plt.get_cmap(\"jet\"),\n                  colorbar=False, alpha=0.4)\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n           cmap=plt.get_cmap(\"jet\"))\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\n\nprices = housing[\"median_house_value\"]\ntick_values = np.linspace(prices.min(), prices.max(), 11)\ncbar = plt.colorbar(ticks=tick_values\/prices.max())\ncbar.ax.set_yticklabels([\"$%dk\"%(round(v\/1000)) for v in tick_values], fontsize=14)\ncbar.set_label('Median House Value', fontsize=16)\n\nplt.legend(fontsize=16)\nplt.show()","fdd466b5":"# corr() is used to find the pairwise correlation of all columns in the dataframe.\n\nhousing.corr()","a7690c54":"# Lets visualize correlation using seaborn\n\ncorr=housing.corr()\nmask=np.triu(np.ones_like(corr,dtype=bool))\n\nf,ax=plt.subplots(figsize=(11,9))\ncmap=sns.diverging_palette(230,20,as_cmap=True)\nsns.heatmap(corr,mask=mask,cmap=cmap,vmax=3,center=0,\n           square=True,linewidth=5,cbar_kws={\"shrink\":0.5})","8af35b96":"# from pandas.tools.plotting import scatter_matrix \n# For older versions of Pandas\n\nfrom pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))\nplt.show()","e9c6dc52":"# Droping median_house_value using drop()\n\nhousing=strat_train_data.drop(\"median_house_value\",axis=1)","72860c36":"# Verifying changes are made or not\n\nhousing","3e9d204d":"# Making a copy\n\nhousing_labels=strat_train_data['median_house_value'].copy()","5a095244":"# Visualizing Histogram \n\nhousing_labels.hist(bins=50)\nplt.show()","aea4c0d9":"sample_incomplete_rows=housing[housing.isnull().any(axis=1)].head()","cd6708bf":"sample_incomplete_rows","2055cf23":"sample_incomplete_rows.drop(\"total_bedrooms\",axis=1)","f7a332ef":"# Calculating meadian of total_bedrooms\n\nmedian=housing[\"total_bedrooms\"].median()","1561436d":"median","e69044ac":"sample_incomplete_rows['total_bedrooms'].fillna(median,inplace=True)\n","6b62b821":"sample_incomplete_rows.shape","5a9709a3":"# strategy = median : replace missing values using the median along each column\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='median')","5385769e":"housing_num=housing.select_dtypes(include=[np.number])\nhousing_num","8a295c80":"imputer.fit(housing_num)","96239bb7":"X=imputer.transform(housing_num)","3cd90e4f":"housing_tr=pd.DataFrame(X,columns=housing_num.columns, index=housing_num.index)\nhousing_tr","8534cb61":"imputer.strategy","3b34458e":"# Since Ocean Proximity contains cateogrical Value we need to change it to numerical value using Ordinal Encoding\n\nhousing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)","9bd68190":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\n\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n\n# Printing first 10 rows to verify\nhousing_cat_encoded[:10]","39d20232":"ordinal_encoder.categories_","4a18a8cb":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder=OneHotEncoder(sparse=False)\nhousing_cat_1hot=cat_encoder.fit_transform(housing_cat)","9e704247":"housing_cat_1hot","bffcc6fd":"def feature_engineering(data):\n    data['bedrooms_per_household']=data['total_bedrooms'] \/ data['households']\n    data['population_per_household']=data['population'] \/ data['households']\n    data['rooms_per_household']=data['total_rooms'] \/ data['households']\n    return data","6031ab90":"housing_feature_engineered=feature_engineering(housing_num)\nhousing_feature_engineered","30a3bee8":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nhousing_scaled=scaler.fit_transform(housing_feature_engineered)\nhousing_scaled","51a9b914":"housing=strat_train_data.drop(\"median_house_value\",axis=1)\n\nhousing_labels=strat_train_data['median_house_value'].copy()\n\ndef data_tranformation(datas):\n    if \"median_house_value\" in data.columns:\n        labels=datas['median_house_value']\n        datas=datas.drop('median_house_value',axis=1)\n    else:\n        labels=None\n        \n    feature_engineered_data=feature_engineering(datas)\n    features=list(feature_engineered_data.columns)\n    \n    from sklearn.impute import SimpleImputer\n    imputer=SimpleImputer(strategy='median')\n    \n    housing_num=feature_engineered_data.select_dtypes(include=[np.number])\n    imputed=imputer.fit_transform(housing_num)\n    \n    housing_cat=feature_engineered_data.select_dtypes(exclude=[np.number])\n    \n    from sklearn.preprocessing import OneHotEncoder\n    cat_encoder=OneHotEncoder(sparse=False)\n    housing_cat_1hot=cat_encoder.fit_transform(housing_cat)\n    features=features+cat_encoder.categories_[0].tolist()\n    features.remove(\"ocean_proximity\")\n    \n    from sklearn.preprocessing import StandardScaler\n    scaler=StandardScaler()\n    housing_scaled=scaler.fit_transform(imputed)\n    output=np.hstack([housing_scaled,housing_cat_1hot])\n    \n    return output,labels,features      ","0db5ebb7":"train_data,train_labels,features=data_tranformation(strat_train_data)\n\ntrain_data","7dfe1915":"test_data,test_labels,features=data_tranformation(strat_test_data)\n\ntest_data","82f940df":"# Final features to be considered while Regression\n\nfeatures","8df30334":"from sklearn.linear_model import LinearRegression","8e1b4e58":"lin_reg=LinearRegression()","b1be54e7":"lin_reg.fit(train_data,train_labels)","59acc457":"# Compare the Orginal Values and predicited value","7ccd6a24":"original_values= test_labels[:5]\n\npredicted_values=lin_reg.predict(test_data[:5])","c94cb7b9":"comparision_dataframe=pd.DataFrame(data={\"Original values\":original_values,\"Predicted values\":predicted_values})\ncomparision_dataframe","7289b692":"comparision_dataframe['Differences']= comparision_dataframe['Original values'] - comparision_dataframe['Predicted values']\n\ncomparision_dataframe","4dbf816c":"# MSE simply refers to the mean of the squared difference between the predicted parameter and the observed parameter.\n\nfrom sklearn.metrics import mean_squared_error\n\nlin_mse=mean_squared_error(original_values,predicted_values)\nlin_rmse=np.sqrt(lin_mse)\nlin_rmse","a0bae85d":"# The MAE measures the average magnitude of the errors in a set of forecasts, without considering their direction. \n# It measures accuracy for continuous variables.\n\nfrom sklearn.metrics import mean_absolute_error\n\nlin_mae=mean_absolute_error(original_values,predicted_values)\nlin_mae","1ec9d77d":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg=DecisionTreeRegressor(random_state=42)\n\ntree_reg.fit(train_data,train_labels)","dfb7bc8b":"train_prediciton=tree_reg.predict(train_data)\ntree_mse=mean_squared_error(train_labels,train_prediciton)\ntree_rmse=np.sqrt(tree_mse)\ntree_rmse","51e8de10":"# Creating display Function to display RMSE, mean and std\n\ndef display(scores):\n    print(\"RMSE:\",scores)\n    print(\"\\n Mean:\",scores.mean())\n    print('\\n SD:',scores.std())\n    ","df7d993f":"from sklearn.model_selection import cross_val_score","52c772af":"# First Lets Calulate for Linear Regression\n\nlin_scores = cross_val_score(lin_reg, train_data,train_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\n\ndisplay(lin_rmse_scores)","9304dd44":"# Lets now calculate for Decision Tree Regressor\n\nscores=cross_val_score(tree_reg,train_data,train_labels,scoring=\"neg_mean_squared_error\",cv=10)\n\ntree_rmse_scores=np.sqrt(-scores)\n\ndisplay(tree_rmse_scores)","71fc6cd8":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg=RandomForestRegressor(n_estimators=100,random_state=42)\nforest_reg.fit(train_data,train_labels)","851a19cc":"# Calculate RMSE\n\ntrain_prediction=forest_reg.predict(train_data)\n\nforest_mse=mean_squared_error(train_labels,train_prediction)\n\nforest_rmse=np.sqrt(forest_mse)\n\nforest_rmse","ee375811":"from sklearn.model_selection import cross_val_score\n\nforest_scores=cross_val_score(forest_reg,train_data,train_labels,scoring='neg_mean_squared_error',cv=10)\n\nforest_rmse_scores=np.sqrt(-forest_scores)\n\ndisplay(forest_rmse)","02cd3391":"from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel=\"linear\")\n\nsvm_reg.fit(train_data,train_labels)","c35a94fe":"housing_predictions = svm_reg.predict(train_data)","6b2bb247":"svm_mse = mean_squared_error(train_labels,train_prediction)\n\nsvm_rmse = np.sqrt(svm_mse)\n\nsvm_rmse","7d545626":"scores=cross_val_score(lin_reg,train_data,train_labels,scoring=\"neg_mean_squared_error\",cv=10)\npd.Series(np.sqrt(-scores)).describe()","728830bd":"scores=cross_val_score(forest_reg,train_data,train_labels,scoring=\"neg_mean_squared_error\",cv=10)\npd.Series(np.sqrt(-scores)).describe()","274341a7":"scores=cross_val_score(svm_reg,train_data,train_labels,scoring=\"neg_mean_squared_error\",cv=10)\npd.Series(np.sqrt(-scores)).describe()","e0b429fa":"scores=cross_val_score(tree_reg,train_data,train_labels,scoring=\"neg_mean_squared_error\",cv=10)\npd.Series(np.sqrt(-scores)).describe()","aa9eee1a":"## What is One Hot Encoding ? ","449c5d33":"# Why are we using Stratified Shuffle Split ?","608d5868":"## Syntax:\n\n### from sklearn.model_selection import StratifiedShuffleSplit\n\n###  sklearn.model_selection.StratifiedShuffleSplit(n_splits=10, *, test_size=None, train_size=None, random_state=None)","84018841":"# Implementing Random Forest Regressor","89854ae4":"## Linear regression is a basic and commonly used type of predictive analysis.  The overall idea of regression is to examine two things: \n\n- ## (1) does a set of predictor variables do a good job in predicting an outcome (dependent) variable  \n- ## (2) Which variables in particular are significant predictors of the outcome variable, and in what way do they\u2013indicated by the magnitude and sign of the beta estimates\u2013impact the outcome variable?  \n\n## These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables.  \n## The simplest form of the regression equation with one dependent and one independent variable is defined by the formula y = c + b*x, \n- ## where y = estimated dependent variable score, c = constant, b = regression coefficient, and x = score on the independent variable. ","25905e74":"###  Summary:\n\n- ### So, Here we had housing data of california so first we visulaized data then we look for missing value or null values and tried to replace them or remove them using various  technique and then we splited our data into training data and testing data then we moved to visualize our training data and to verify proper distribution and then we trained model and calculated RMSE STD , Mean,etc. And we were done ! \n\n- ### Here we just used Linear Regression, Decision Tree Regression , Random Forest Regression and Support Vector Regression. You can use any regression model checkout sklearn documentation for more information\n\n- ### Note: Please note these and get information about them because without these we cannot perform Regression on this dataset\n\n -  1. train_test_split\n -  2. Stratified Shuffle Split\n -  3.  Simple Imputer\n -  4. Ordinal Encoding\n -  5. One Hot Encoding\n -  6. Standard Scaler \n ","fb787e96":"<a name='dtr'><\/a>\n# 2.2. What is Decision Tree Regression ?\n","6d4cafd5":"# What is Ordinal Encoding ? ","e2c59e17":"### SimpleImputer is a scikit-learn class which is helpful in handling the missing data in the predictive model dataset. It replaces the NaN values with a specified placeholder. ","9accb0c6":"# What is Standard Scaler ?","029a721b":"# [1. WorkFlow](#wf)\n\n# [2.1. What is Linear Regression ?](#lr)\n\n# [2.2. What is Decision Tree Regression ?](#dtr)\n\n# [2.3. What is Random Forest Regression ?](#rfr)\n\n# [3. Importing Library](#ib)\n\n# [4. Loading Dataset](#ld)\n\n# [5. Model Building](#md)\n\n# [6. Conclusion](#c)\n<hr style=\"height: 5px\">","3f567a87":"<a name='ld'><\/a>\n# 4. Loading Dataset","4b15ab69":"### When we split our data into train data and test data we want that training data has same apporixmate disribution of values as the original set od data did ","9324f381":"###  Train Test Split : It is  used for classification or regression problems and is used for any supervised learning algorithm. The procedure involves taking a dataset and dividing it into two subsets( Training dataset and Testing dataset).","d8749304":"- ###  The idea behind StandardScaler is that it will transform your data such that its distribution will have a mean value 0 and standard deviation of 1. In case of multivariate data, this is done feature-wise (in other words independently for each column of the data)","55f9fe94":"<a name='ib'><\/a>\n# 3. Importing library","5d1aca9f":"<a name= lr><\/a>\n# 2.1. What is Linear Regression ?","31284233":"# Feature Engineering","d6577e07":"### Its better we can see distribution better, lets make it best","5c949ed2":"![images.jfif](attachment:images.jfif)","6cff3b16":"## What is Simple Imputer ?","11737870":"# ![download%20%281%29.jfif](attachment:download%20%281%29.jfif)","940c0721":"Lets Find Correlation in dataset","76484b53":"# Visualization of Dataset","e3e62bde":"- ### StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. \n- ### Unit variance means dividing all the values by the standard deviation.\n- ### StandardScaler makes the mean of the distribution 0. \n- ### About 68% of the values will lie be between -1 and 1.","cfce0c94":"# Implementing Decision Tree Regressor","9a9188f9":"<a name='wf'><\/a>\n # 1. Work Flow","e4bcf859":"# Preparing Data for Regression","9c3d7826":"# Implementing Linear Regression","09f1782c":"### Housing Data --->  Data Preprocessing ---> Train Test Split ---> Model Fiting ---> Prediction","5979d93d":"##### Note: since Scikit-Learn 0.22, you can get the RMSE directly by calling the mean_squared_error() function with squared=False. ","ac9d6455":"# What is Stratified Shuffle Split ? ","fd3cffb9":" - ### The best imputer strategy is most_frequent and apparently almost all features are useful (15 out of 16). The last one (ISLAND) seems to just add some noise.\n \n - ### Overall Model Perform well \n \n - ### If you liked my Work Do Upvote, Hope its  Useful.  ","42233bea":"#####  As this data is related to california, we used this california area img to plot  and we get better insight rather than above ones","b5f8a07f":"# Table of Content\n<hr style=\"height:2px\">","7ddccfb5":"### In ordinal encoding, each unique category value is assigned an integer value. \n### For example, \u201cred\u201d is 1, \u201cgreen\u201d is 2, and \u201cblue\u201d is 3. This is called an ordinal encoding or an integer encoding and is easily reversible. Often, integer values starting at zero are used. ","3dacf2ef":"#  We already used train test split then why Stratified Shuffle Split\/ what is difference between Stratified Shuffle Split vs train test split ?","030c0e84":"## Checking for null values  missing values and dropping null \/ missing values","8720d197":"### Decision tree regression observes features of an object and trains a model in the structure of a tree to predict data in the future to produce meaningful continuous output. Continuous output means that the output\/result is not discrete, i.e., it is not represented just by a discrete, known set of numbers or values.\n- ### Discrete output example: A weather prediction model that predicts whether or not there\u2019ll be rain in a particular day.\n- ### Continuous output example: A profit prediction model that states the probable profit that can be generated from the sale of a product.","7af1d47a":"# Creating ML model","549255ca":"# What is difference between normalization and standardization ?","1a259be7":"- ### Normalization typically means rescales the values into a range of [0,1].\n- ### Standardization typically means rescales data to have a mean of 0 and a standard deviation of 1 ","e1a9fa12":"# ![download.jfif](attachment:download.jfif)","6d70677c":"<a name='c'><\/a>\n# Conclusion","bdda8f70":"###  Stratified sampling aims at splitting a data set so that each split is similar with respect to something. In a classification setting, it is often chosen to ensure that the train and test sets have approximately the same percentage of samples of each target class as the complete set.","41271909":"# ![download.png](attachment:download.png)","ea2cbaaa":"### Stratified Shuffle Split: Using StratifiedShuffleSplit the proportion of distribution of class labels is almost even between train and test dataset.","bbfd0c6c":"### A Random Forest is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and a technique called Bootstrap and Aggregation, commonly known as bagging. The basic idea behind this is to combine multiple decision trees in determining the final output rather than relying on individual decision trees.\n\n### Random Forest has multiple decision trees as base learning models. We randomly perform row sampling and feature sampling from the dataset forming sample datasets for every model. This part is called Bootstrap.\n\n### We need to approach the Random Forest regression technique like any other machine learning technique\n\n-  Design a specific question or data and get the source to determine the required data.\n- Make sure the data is in an accessible format else convert it to the required format.\n- Specify all noticeable anomalies and missing data points that may be required to achieve the required data.\n- Create a machine learning model\n- Set the baseline model that you want to achieve\n- Train the data machine learning model.\n- Provide an insight into the model with test data\n- Now compare the performance metrics of both the test data and the predicted data from the model.\n- If it doesn\u2019t satisfy your expectations, you can try improving your model accordingly or dating your data or use another data modeling technique.\n- At this stage you interpret the data you have gained and report accordingly.","48d40ef3":"# Implementing Support Vector Regressor","503ae8e7":"### Not getting clear insight lets render some parameter","f1750081":"# Scaling Data","6d87c917":"- ###  Encode categorical integer features using a one-hot aka one-of-K scheme. The input to this transformer should be a matrix of integers, denoting the values taken on by categorical (discrete) features.\n- ### The output will be a sparse matrix where each column corresponds to one possible value of one feature.\n- ###  OneHotEncoder from SciKit library only takes numerical categorical values, hence any value of string type should be label encoded before one hot encoded. So taking the dataframe from the previous example, we will apply OneHotEncoder on column Bridge_Types_Cat.","4ce26761":"\"\"\"\nAbout data:\n\n No   Column              Non-Null Count  Dtype  \n----  ------             --------------- -------  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \n \n\"\"\"","2ec0d2e3":"# What does scaler transform do?","04e68e94":"<a name='rfr'><\/a>\n# 2.3. What is Random Forest Regression ?"}}