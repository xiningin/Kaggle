{"cell_type":{"37adcffc":"code","08266ef2":"code","9807e2e0":"code","aa39748f":"code","6aa28ea1":"code","7ec37c01":"code","b7d7972a":"code","e6c1f823":"code","27444287":"code","06134969":"code","18eb6604":"code","cc4987ef":"code","19076c1d":"code","8151818c":"code","7d75ba36":"code","16f1a7dc":"code","7177ab84":"code","15109200":"code","432e98d5":"code","18900b9e":"code","8f76ca6b":"code","d72ace43":"code","50c33796":"code","286b8fb9":"code","54107715":"code","edbebe1f":"code","65db0810":"code","5dba611a":"code","6a9f9f24":"code","a0845abe":"code","65f4ce5c":"code","521bdfb9":"code","f81ac8dd":"code","9ddea6eb":"markdown"},"source":{"37adcffc":"# import all modules needed for this task\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os","08266ef2":"# ignore Deprecation Warning\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","9807e2e0":"# Load training and test data\nprint(os.listdir(\"..\/input\"))\ndf_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")\n\n# combine training and test datasets\ndf = df_train.append(df_test,ignore_index = True,sort=False )\n\n","aa39748f":"# A quick look at the first 3 rows for train and test datasets\ndisplay(df_train.head(3))\n\ndisplay(df_test.head(3))\n","6aa28ea1":"# A quick look at the first and last 5 rows for combined dataset to see if it properly aligns \ndisplay(df.head())\n\ndisplay(df.tail())","7ec37c01":"# some quick inspections:check no.of rows and columns in train.csv,test.csv,df.. also check titles\/headers in df\ndf_train.shape, df_test.shape,df.shape, df.columns.values","b7d7972a":"# Feature processing: Pclass , count the number of NaN for Pclass\n# there is no null value in Pclass.\n\ndf[\"Pclass\"].notnull().value_counts()\n","e6c1f823":"# check the correlation between Pclass and Survived.\n\"\"\"Pclass seems to be a very useful feature....there seems to be a correlation\"\"\"\n\ndf[[\"Pclass\", \"Survived\"]].groupby([\"Pclass\"], as_index=False).mean()\n\n","27444287":"\"\"\"A more friendly approach with Neural network is to scale and normalize all variables between the range of 0 to 1.\nMore specifically if the output activation functon is sigmoid\"\"\" \n#Now we can use dummy encoding to Normalise Pclass and drop the main Pclass.\n\ndf = pd.concat([df, pd.get_dummies(df['Pclass'],prefix=\"Pclass\")], axis=1).drop(labels=\"Pclass\",axis=1)\ndf.head(3)","06134969":"# Feature processing:Name\n# count the number of NaN for Name\ndf[\"Name\"].notnull().value_counts()\n","18eb6604":"# see first 3 rows for column \"Name\"\ndf.Name.head(3)","cc4987ef":"\"\"\"Name itself does not gives any deduction, but the titles in the names is paramount since it contains information \nof gender\/status\"\"\"\n\n#Let's extract the titles from these names and assign it to a variable called \"Title\".\n\ndf['Title'] = df.Name.map( lambda x: x.split( ',' )[1].split(\".\")[0].strip())\ndf = df.drop(labels=\"Name\",axis=1)\n\ndf.head()","19076c1d":"# inspect the amount of people for each title\ndf['Title'].value_counts()","8151818c":"\"\"\"Looks like the main ones are \"Mr\", \"Miss\", \"Mrs\", \"Master\". \nSome of the others can be be merged into some of these four categories. For the rest, it will be called 'Others\"\"\"\n\ndf['Title'] = df['Title'].replace('Mlle', 'Miss')\ndf['Title'] = df['Title'].replace(['Mme','Lady','Ms'], 'Mrs')\ndf.Title.loc[ (df.Title !=  'Master') & (df.Title !=  'Mr') & (df.Title !=  'Miss') \n             & (df.Title !=  'Mrs')] = 'Others'\n\n# inspect the correlation between Title and Survived\ndf[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","7d75ba36":"#Now we can use dummy encoding to Normalise Title and drop the main Name\ndf = pd.concat([df, pd.get_dummies(df['Title'],prefix=\"Title\")], axis=1).drop(labels=\"Title\",axis=1)\ndf.head(3)","16f1a7dc":"# Feature processing:Sex , check if there is any NaN\n\ndf.Sex.isnull().sum(axis=0)","7177ab84":"df[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean()","15109200":"# Feature processing: sex, convert into binary\ndf['Sex'] = df['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\ndisplay(df.head(3))","432e98d5":"# Feature processing:Cabin, check if there is any NaN, and also the number of rows in Cabin\ndf.Cabin.isnull().sum(axis=0), df[\"Cabin\"].shape","18900b9e":"#Cabin: Too many NaN in Cabin,i.e 1014 out of 1309. so Cabin will also be dropped later","8f76ca6b":"#Feature processing:SibSp and Parch, check if there is any NAN\n\"\"\"SibSp and Parch already has values 0 and 1, no null values , so no further processing.\"\"\"\n\ndf.SibSp.isnull().sum(axis=0), df.Parch.isnull().sum(axis=0)","d72ace43":"#Feature processing:Embarked, check if there is any NAN\ndf.Embarked.isnull().sum(axis=0)","50c33796":"# check for most common categories of embarked\ndf.describe(include=['O']) ","286b8fb9":"# S is the most common category. fill the 2 NaN spaces with S.\ndf.Embarked.fillna('S' , inplace=True )\ndf.Embarked.isnull().sum(axis=0)","54107715":"#Now we can use dummy encoding for Embarked and drop the main Embarked\ndf = pd.concat([df, pd.get_dummies(df['Embarked'],prefix=\"Embarked\")], axis=1).drop(labels=\"Embarked\",axis=1)\ndf.head()","edbebe1f":"#Removing insignificant features that do not have predictive power:\n#name,ticket, passengerId, Fare\n#Cabin may have predictive power but has too many NAs (1014 out of 1309),and so have to be removed as well.\n\ndf = df.drop(['Ticket','PassengerId','Cabin','Fare'],axis = 1)\ndisplay(df.head(2))","65db0810":"# Feature processing: Age, Age is a very important feature\n\"\"\"there are many NaN value(263) in this feature so first we will replace them with a random number\n   generated between (mean - std) and (mean + std) \"\"\"\n\nage_avg   = df['Age'].mean()\nprint (age_avg)\nage_std    = df['Age'].std()\nprint (age_std)\nage_null_count = df['Age'].isnull().value_counts()[1]\nprint (age_null_count)\n    \nage_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\nprint(age_null_random_list)\ndf['Age'][np.isnan(df['Age'])] = age_null_random_list\n","5dba611a":"\"\"\"since the output layer uses a sigmoid activation function,it is in my opinion to scale and normalize the datasets\n   so that the learning does not shift towards the higher weight\"\"\"\n  \n# normalize the dataset\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf = pd.DataFrame(scaler.fit_transform(df))\ndf.head()","6a9f9f24":"df.head(2)\ndf.shape","a0845abe":"#Modelling and Prediction.drop survived from X_train dataset.\nX_train = df[0:891].drop([0], axis=1)\ny_train = df[0:891][0]\nX_test  = df[891:].drop([0], axis=1)\n\nX_train.shape","65f4ce5c":"# fix random seed for reproducibility\nnp.random.seed(7)\n# create model\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=15, activation='relu'))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# Fit the model\nmodel.fit(X_train, y_train, epochs=200,validation_split=0.33, batch_size=10)\n","521bdfb9":"# evaluate the model\nscores = model.evaluate(X_train,y_train)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","f81ac8dd":"out_pred = model.predict(X_test)\nprint(out_pred)\nout_final = (out_pred > 0.5).astype(int).reshape(X_test.shape[0])\noutput = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Survived': out_final})\nprint(output)\noutput.to_csv('ANN prediction.csv', index=False)","9ddea6eb":"#Competition Description\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy."}}