{"cell_type":{"1ddeb76c":"code","9180cae5":"code","d10a80b3":"code","71852bd4":"code","541953be":"code","7e587c6a":"code","b0eb4e0b":"code","058d9f12":"code","da99c770":"code","b04331fb":"code","6749bc63":"code","3f3690df":"code","727da815":"code","99ffd645":"code","9604a082":"code","52b9b18c":"code","bab947b8":"code","d334feec":"code","66ca99ac":"code","66139dd5":"code","d6003eac":"code","ed0baf31":"markdown","5d4b7c96":"markdown","2b10a83a":"markdown","d2708076":"markdown","603c9bb6":"markdown","ddef2d91":"markdown","194d3c2a":"markdown","91c994d4":"markdown","716c71fe":"markdown","17d03e0c":"markdown","3a71d908":"markdown"},"source":{"1ddeb76c":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9180cae5":"N_TRIALS = 2000\nN_ARMS = 16\nN_FEATURES = 5\nBEST_ARMS = [3, 7, 9, 15]","d10a80b3":"# Generating the context data for every for every arm for every trial (they are all random)\ndef make_design_matrix(n_trials, n_arms, n_features):\n    \"\"\"\n    Generate context vectors for all arms for each of the trial\n    Parameters:\n    -----------\n        n_trials: number of trials\n        n_arms: number of arms per trial\n        n_features: number of feature per context vector\n    Returns:\n    ----------\n        A matrix of size n_trials x n_arms x n_features\n    \"\"\"\n    available_arms = np.arange(n_arms)\n    X = np.array([[np.random.uniform(0, 1, size = n_features) for _ in np.arange(n_arms)] for _ in np.arange(n_trials)])\n#     X = np.array([[arm\/(np.sqrt(arm.dot(arm))) for arm in trial] for trial in orig_x])\n    # normalize it too.\n    \n    return X","71852bd4":"# Generating a theoretical true theta so we can have things to compare with. I.e., will compare the oracles vs. the rewards from the selected arms\ndef make_theta(n_arms, n_features, best_arms, bias = 1):\n    \"\"\"\n    Generate true theta for testing purpose.\n    Parameters:\n    ----------\n        n_arms: number of arms\n        n_features: number of features for the context vector\n        best_arms: arms in which we should give some bias values (for good)\n        bias: value to be added to the best arms\n    Returns:\n    ----------\n        A matrix of size n_arms x n_features, each value is a random value with mean = 0 and standard deviation of 1\/4. However, for the best arms, we will add the bias\n    \"\"\"\n    true_theta = np.array([np.random.normal(size=n_features, scale=1.0\/4.0) for _ in range(n_arms)])\n    true_theta[best_arms] += bias\n    return true_theta","541953be":"def generate_reward(arm, x, theta, scale_noise = 1.0\/10.0):\n    \"\"\"\n    Generate reward for an arm given a context\n    Parameters:\n    ----------\n        arm: this is the arm index (0 to number of arms - 1)\n        x: is the context that we are observing for the arm index (arm)\n        theta: is the theta (true or predicted) that are are using to estimate the reward for each arm\n        scale_noise: we may need to add some random noise (mean 0 and standard deviation as scale_noise)\n    Returns:\n    ----------\n        The estimated score for the arm (with the arm index and the context observed corresponding to the given theta)\n    \"\"\"\n    signal = theta[arm].dot(x)\n    noise = np.random.normal(scale=scale_noise)\n    return signal + noise","7e587c6a":"def make_regret(payoffs, oracles):\n    \"\"\"\n    Generate the cummulative regret over time.\n    Parameters:\n    -----------\n        payoffs: an array of T payoffs (for T number of trials)\n        oracles: an array of best values for T trials (oracles)\n    Returns:\n        Array of the cumulative sum over time (of size T = number of trials)\n    \"\"\"\n    return np.cumsum(oracles - payoffs)","b0eb4e0b":"X = make_design_matrix(n_trials=N_TRIALS, n_arms=N_ARMS, n_features=N_FEATURES)\ntrue_theta = make_theta(n_arms=N_ARMS, n_features=N_FEATURES, best_arms=BEST_ARMS)","058d9f12":"payoffs = [[generate_reward(arm=arm, x=X[t, arm], theta=true_theta) for arm in np.arange(N_ARMS)] for t in np.arange(N_TRIALS)]\nave_rewards = np.mean(payoffs, axis=0)","da99c770":"f, (theta_fig, avg_reward_fig) = plt.subplots(1, 2, figsize=(15, 10))\nf.suptitle(\"Theta and average rewards for each arm\", fontsize=20)\n# Visualizing true theta\ntheta_fig.matshow(true_theta)\nf.colorbar(theta_fig.imshow(true_theta), ax = theta_fig)\ntheta_fig.set_xlabel(\"feature number\")\ntheta_fig.set_ylabel(\"arm number\")\ntheta_fig.set_yticks(np.arange(N_ARMS))\ntheta_fig.set_title(\"True theta matrix\")\n\n# Visualizing avewrage reward\navg_reward_fig.bar(np.arange(N_ARMS), ave_rewards)\navg_reward_fig.set_title(\"Average reward per arm\")\navg_reward_fig.set_xlabel(\"arm number\")\navg_reward_fig.set_ylabel(\"average reward\")\nplt.show()\n","b04331fb":"# Inspired from: https:\/\/github.com\/etiennekintzler\/bandits_algorithm\/blob\/master\/linUCB.ipynb\ndef lin_ucb(alpha, X, generate_reward, true_theta):\n    \"\"\"\n    Simulate the LINUCB algorithm using the generated data X.\n    Parameters:\n    -----------\n        alpha: this is the ::math:`\\alpha = \\sqrt{ln(2\/\\sigma)\/2}`\n        X: is the observed data (contexts for all arms at every trial)\n        generate_reward: a function used to generate the reward for an arm given a context and a theta\n        true_theta: the true theta used to generate the oracles and compare the losses (regrets)\n    \"\"\"\n    # Data storages\n    n_trials, n_arms, n_features = X.shape\n    arm_choice = np.empty(n_trials) # used to store agent's choices for each trial\n    r_payoffs = np.empty(n_trials) # used to store the payoff for each trial (the payoff for the selected arm based on the true_theta)\n    theta = np.empty(shape=(n_trials, n_arms, n_features)) # used to store the predicted theta over each trial\n    p = np.empty(shape=(n_trials, n_arms)) # used to store predictions for reward of each arm for each trial\n    # Lin UCB Objects\n    A = np.array([np.diag(np.ones(shape=n_features)) for _ in np.arange(n_arms)]) # A is the matrix defined as :math:A_a = D_a^TD_a + I_d, and for the initialization it is I_d and will be updated after every trial\n    b = np.array([np.zeros(shape=n_features) for _ in np.arange(n_arms)]) # b is the matrix defined as response vectors (reward for each feature for each arm at each trial, initialized to zero for all features of all arms at every trial)\n    # The algorithm\n    for t in range(n_trials):\n        # compute the estimates (theta) and prediction (p) for all arms\n        for a in range(n_arms):\n            inv_A = np.linalg.inv(A[a])\n            theta[t, a] = inv_A.dot(b[a]) # estimate theta as from this formula :math:`\\hat{\\theta}_a = A_a^{-1}b_a`\n            p[t, a] = theta[t, a].dot(X[t, a]) + alpha * np.sqrt(X[t, a].dot(inv_A).dot(X[t, a])) # predictions is the expected mean + the confidence upper bound\n        # choosing the best arms\n        chosen_arm = np.argmax(p[t])\n        x_chosen_arm = X[t, chosen_arm]\n        r_payoffs[t] = generate_reward(arm=chosen_arm, x = x_chosen_arm, theta=true_theta) # This payoff is for the predicted chosen arm, and but the payoff is based on theoretical theta (true theta)\n        arm_choice[t] = chosen_arm\n        \n        # Update intermediate objects (A and b)\n        A[chosen_arm] += np.outer(x_chosen_arm, x_chosen_arm.T)\n        b[chosen_arm] += r_payoffs[t]*x_chosen_arm # update the b values for each features corresponding to the pay off and the features of the chosen_arm\n    return dict(theta=theta, p=p, arm_choice=arm_choice, r_payoffs = r_payoffs)","6749bc63":"# Defining oracle (best payoffs based on the true_theta)\noracles = np.array([np.max([generate_reward(arm=arm, x=X[t, arm], theta=true_theta) for arm in range(N_ARMS)]) for t in range(N_TRIALS)])","3f3690df":"len(oracles)","727da815":"# Define random payoff (see how if we just select an arm randomly, then how is the payoffs going to be)\nrandom_payoffs = np.array([generate_reward(arm=np.random.choice(N_ARMS), x = X[t, np.random.choice(N_ARMS)], theta=true_theta) for t in range(N_TRIALS)])","99ffd645":"# calcualte the regret for random policy\nrandom_regrets = make_regret(payoffs=random_payoffs, oracles=oracles)","9604a082":"alphas = [0, 1, 2.5, 5, 10, 20]\nresults_dict = {alpha: lin_ucb(alpha=alpha, X = X, generate_reward=generate_reward, true_theta=true_theta) for alpha in alphas}","52b9b18c":"def plot_regrets(results, oracles):\n    [plt.plot(make_regret(payoffs=x['r_payoffs'], oracles=oracles), label=\"alpha: \" + str(alpha)) for (alpha, x) in results.items()]","bab947b8":"plt.figure(figsize=(12.5, 7.5))\nplot_regrets(results_dict, oracles)\n# plot also the random one\nplt.plot(make_regret(random_payoffs, oracles), label='random', linestyle='--')\nplt.legend()\nplt.title(\"Regrets for various levels of alpha and also random policy\")\nplt.show()","d334feec":"def thompson_sampling(X, true_theta, R=0.01, epsilon=0.5, delta=0.5):\n    '''\n    Parameters:\n    --------------\n    delta: float, 0 < delta < 1\n        With probability 1 - delta, linear thompson sampling satisfies the theoretical regret bound.\n    R: float, R >= 0\n        Assume that the residual  :math:`ri(t) - bi(t)^T \\hat{\\mu}` is R-sub-gaussian. \n        In this case, R^2 represents the variance for residuals of the linear model :math:`bi(t)^T`.\n    epsilon: float, 0 < epsilon < 1\n        A  parameter  used  by  the  Thompson Sampling algorithm. If the total trials T is known, we can choose epsilon = 1\/ln(T).\n    '''\n    n_trials, n_arms, n_features = X.shape\n    r_payoffs = np.zeros(n_trials) # used to store the payoff for each trial (the payoff for the selected arm based on the true_theta)\n    v = R * np.sqrt(24 \/ epsilon * n_features * np.log(1 \/ delta))\n    \n    \n    # model initialization\n    B = np.identity(n_features) \n    mu_hat = np.zeros(shape=(n_features, 1))\n    f = np.zeros(shape=(n_features,1))    \n\n    for t in range(n_trials):\n        context = X[t]\n        mu_tilde = np.random.multivariate_normal(mu_hat.flat, v**2 * np.linalg.inv(B))[..., np.newaxis]\n        score_array = context.dot(mu_tilde)\n        chosen_arm = np.argmax(score_array)\n        context_t = context[chosen_arm]\n        reward = generate_reward(arm=chosen_arm, x=context_t, theta=true_theta)\n        r_payoffs[t] = reward\n        context_t = np.reshape(context_t, (-1, 1))\n        B += context_t.dot(context_t.T)\n        f += reward*context_t\n        mu_hat = np.linalg.inv(B).dot(f)\n    return dict(r_payoffs = r_payoffs)","66ca99ac":"# delta=0.61, r=0.01, epsilon=0.71\nR = 0.01\nepsilon = 0.5\ndelta = 0.5\nthompson_results = thompson_sampling(X=X, true_theta=true_theta, R = R, epsilon=epsilon, delta = delta)","66139dd5":"thompson_results['r_payoffs']","d6003eac":"plt.figure(figsize=(12.5, 7.5))\n# plot for LinUCB\nplot_regrets(results_dict, oracles)\n# plot also the random one\nplt.plot(make_regret(random_payoffs, oracles), label='random', linestyle='--')\n# plot for Thompson\nplt.plot(make_regret(thompson_results['r_payoffs'], oracles), label='thompson')\nplt.legend()\nplt.title(\"Regrets for various levels of alpha and also random policy\")\nplt.show()","ed0baf31":"Defining constants","5d4b7c96":"Problem settings","2b10a83a":"## Analyzing regrets, coefficients estimates and chosen arm","d2708076":"# 2. Thompson Sampling","603c9bb6":"## Testing for various alpha for LIN UCB","ddef2d91":"# The contextual bandit\nThe contextual bandit is just like MAB problem but now the true expected reward parameter $\\theta_k$ depends on external variables. Therefore, we add the notion of context (or state) to support our decision.\n\nThus, we're going to suppose that the probability of reward is now of the form:\n\\begin{equation}\n    \\theta_k(x) = \\frac{1}{1+exp(-f(x))}\n\\end{equation}\nwhere\n\\begin{equation}\n    f(x) = \\beta_0 + \\beta_1 . x + \\epsilon\n\\end{equation}\n\nand $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$. In other words, the expected reward parameters for each bandit linearly depends on an external variable $x$ with logistic link.","194d3c2a":"Learned from: A Contextual-Bandit Approach to Personalized News Article Recommendation (paper from Li et al.)\n## Problem formulation\n\nA contextual bandit A proceeds in discrete trials t = 1, 2, 3, ... In trial t\n1. The algorithm observes current user $u_t$ and a set $A_t$ of arms (actions). These two made up $x_{t, a}$ for every $a \\in A_t$, and is called the $context$.\n2. Based on the observed payoffs in previous trials, $A$ chooses an arm $a_t \\in A_t$, and receives a payoff $r_{t,a_t}$ whose expectation depends on both the user $u_t$ and the arm $a_t$.\n3. $A$ improves arm-selection strategy with the new observation $(x_{t, a_t}, a_t, r_{t, a_t})$.\n4. The total T-trial payoff of $A$ is defined as $\\sum_{t=1}^{T}r_{t, a_t}$\n5. Optimal expected T-trial payoff is defined as $E[\\sum_{t=1}^Tr_{t, a_t^*}]$, where $a_t^*$ is the arm with maximum expected payoff at trial $t$.\n6. The goal for $A$ is to maximize payoff or minimize the regret, which is defined as:\n\\begin{equation}\n    R_A(T) = E[\\sum_{t=1}^Tr_{t, a_t^*}] - E[\\sum_{t=1}^Tr_{t, a_t}]\n\\end{equation}\n\n## The algorithm\nAssume that the payoff of an arm $a$ is linear in its d-dimensional feature x_{t, a} with some unknown coefficient vector $\\theta_a^*$:\n\\begin{equation}\nE[r_{t, a} | x_{t, a}] = x_{t, a}^T\\theta_a^*\n\\end{equation}\n\nLet $D_a$ be a design matrix of dimension $m\\times d$ at trial $t$, $m$ is the training inputs (e.g., m contexts that are observed previously for article a), and $c_a \\in R^m$ be the corresponding response vector (e.g., the corresponding $m$ click\/no-click user feedback). Applying ridge regression to the training data ($D_a$, $b_a$) gives an estimate of the coefficients:\n\n\\begin{equation}\n\\hat\\theta_a = (D_a^TD_a + I_d)^{-1}D_a^Tb_a\n\\end{equation}\n\nIt can be proved that, with probability at least $1-\\delta$:\n\\begin{equation}\n|x_{t, a}^T\\hat\\theta_a - E[r_{t, a}|x_{t, a}]| \\leq \\alpha \\sqrt{x_{t, a}^T(D_a^TDa + I_d)^{-1}x_{t, a}}\n\\end{equation}\nfor any $\\delta>0$ and $x_{t, a} \\in R^d$, where $\\alpha = \\sqrt{ln(2\/\\delta)\/2}$ is a constant.\n\nIn other words, the inequality above gives a reasonably tight UCB for the expected payoff of arm a, from which a UCB-type arm-selection strategy can be derived: at each trial t, choose:\n\\begin{equation}\na_t = arg\\,\\underset{a\\in A_t}{max}(x_{t, a}^T + \\alpha\\sqrt{x_{t, a}^T A_a^{-1}x_{t, a}})\n\\end{equation}\nwhere $A_a = D_a^TD_a + I_d$\n\nAlso, the expected payoff $x_{t, a}^T\\theta_a^*$ can be calculated as $x_{t, a}^TA_a^{-1}$, and then $\\sqrt{x_{t, a}^TA_a^{-1}x_{t, a}}$ becomes the standard deviation.\n\nTherefore the upper bound should be:\n\\begin{equation}\n    \\hat\\theta_{t, a} = A_a^{-1}b_a \\\\\n    p_{t, a} = \\hat\\theta_{t, a}x_{t, a} + \\alpha*\\sqrt{x_{t, a}A^{-1}x_{t, a}}\n\\end{equation}","91c994d4":"# 1. LINUCB","716c71fe":"# The algorithm\nThompson Sampling for Contextual bandits\n> Set $B = I_d, \\hat\\mu=0_d, f = 0_d$\n>\n> **for all** $t=1, 2, ..., $ **do**\n>> Sample $\\tilde{\\mu}(t)$ from distribution $\\mathcal{N}(\\hat\\mu, v^2B^{-1})$\n>>\n>> Play arm $a(t) = arg\\,max_i b_i(t)^T \\tilde{\\mu}(t)$, and observe reward $r_t$\n>>\n>> Updates:\n>>>\n>>> $B = B + b_{a(t)}(t)b_{a(t)}(t)^T$\n>>> \n>>> $f = f + b_{a(t)}(t)r_t$\n>>>\n>>> $\\hat\\mu = B^{-1}f$\n>>\n>**end for**\n\nWhere $v = R\\sqrt{\\frac{24}{\\epsilon}d ln(\\frac{1}{\\delta})}$, with $\\epsilon \\in (0, 1)$ parameterizes the algorithm.\n\nFollowings are the initialized values:\ndelta $\\delta = 0.5$, $R=0.01$, $\\epsilon=0.5$","17d03e0c":"Learned from: https:\/\/gdmarmerola.github.io\/ts-for-contextual-bandits\/","3a71d908":"Visualization of average payoff per arm"}}