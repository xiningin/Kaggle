{"cell_type":{"806255d4":"code","63f1eccc":"code","fff7a2f9":"code","7433976c":"code","f1625cc2":"code","9d3b0b49":"code","910c885e":"code","585d6004":"code","10cb801b":"code","017dbcd6":"code","4bcfe218":"code","c98de473":"code","27316c5a":"code","b4f914b0":"code","64439b6a":"code","cca4231e":"code","38cf3a80":"code","753227bc":"code","adc3d39c":"markdown","3c75e534":"markdown","d3c78714":"markdown","fe3070db":"markdown","13221c15":"markdown","6e07a425":"markdown","56ad43bc":"markdown","8fadc26a":"markdown","ab6f0b48":"markdown","1e92ce64":"markdown","9e9564ea":"markdown","c6369c66":"markdown","a7f30c0f":"markdown","6bb740d2":"markdown","89da02a2":"markdown","736838e8":"markdown","b0cdfddc":"markdown","3089c59b":"markdown","55f4f580":"markdown","7fd59cd6":"markdown","8f4a2d1e":"markdown","2b7d4131":"markdown","4887badc":"markdown","8ec04f0f":"markdown"},"source":{"806255d4":"# The fast guys\nfrom fastai.imports import *\nimport pandas_profiling\nimport h2o\nfrom h2o.automl import H2OAutoML\nimport warnings\nwarnings.filterwarnings('ignore')\n%config InlineBackend.figure_format = 'retina'\n# List of files in the directory\nPATH = '..\/input\/titanic\/'\nlist(os.listdir(PATH))","63f1eccc":"train = pd.read_csv(PATH+'train.csv')\ntest = pd.read_csv(PATH+'test.csv')\nsubmission = pd.read_csv(PATH+'gender_submission.csv')","fff7a2f9":"%%time\npandas_profiling.ProfileReport(train)","7433976c":"train = train.drop(columns=['PassengerId', 'Ticket'])\ntest = test.drop(columns=['PassengerId', 'Ticket']);","f1625cc2":"train['Deck'] = train['Cabin'].astype(str).str[0]\ntrain['Deck'] = train['Deck'].map({'A': 1, 'B': 1, 'C': 1, 'D': 1, 'E':1, \n                                             'F':1, 'G': 1, 'T': 0, 'n' : 0}).astype(int)\ntest['Deck'] = test['Cabin'].astype(str).str[0]\ntest['Deck'] = test['Deck'].map({'A': 1, 'B': 1, 'C': 1, 'D': 1, 'E':1, \n                                           'F':1, 'G': 1, 'T': 0, 'n' : 0}).astype(int)\ntrain = train.drop(columns=['Cabin'])\ntest = test.drop(columns=['Cabin']);","9d3b0b49":"train = train.dropna(subset=['Embarked']).reset_index(drop=True)","910c885e":"missing = test.isnull().sum()\nmissing = missing[missing > 0]\nmissing","585d6004":"test['Fare'].hist();\nplt.title('Fare Distribution on Test Set')\nplt.xlabel('Fare');","10cb801b":"test['Fare'] = test['Fare'].fillna(test['Fare'].mode()[0])","017dbcd6":"train['Age'] = train['Age'].fillna(train['Age'].median())\ntest['Age'] = test['Age'].fillna(test['Age'].median())","4bcfe218":"train['Family_Name'] = train['Name'].apply(lambda x : x.split(',')[0])\ntrain['First_Name'] = train['Name'].apply(lambda x : x.split(',')[1])\ntrain['Honorific'] = train['First_Name'].apply(lambda x : x.split('.')[0])\ntrain = train.drop(columns=['Name', 'First_Name'])\ntest['Family_Name'] = test['Name'].apply(lambda x : x.split(',')[0])\ntest['First_Name'] = test['Name'].apply(lambda x : x.split(',')[1])\ntest['Honorific'] = test['First_Name'].apply(lambda x : x.split('.')[0])\ntest = test.drop(columns=['Name', 'First_Name'])","c98de473":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\ntrain[['Age', 'Fare']] = sc.fit_transform(train[['Age','Fare']])\ntest[['Age', 'Fare']] = sc.fit_transform(test[['Age','Fare']])","27316c5a":"display('Train Dataset')\ndisplay(train.head(3))\ndisplay('Test Dataset')\ndisplay(test.head(3))","b4f914b0":"h2o.init()\n# Load the DataFrames on H2O format\ntrain_frame = h2o.H2OFrame(train)\ntest_frame = h2o.H2OFrame(test)\ntrain_frame['Survived'] = train_frame['Survived'].asfactor()\n# Features to predict\nx = train_frame.columns\ny = 'Survived'\nx.remove(y)","64439b6a":"%%time\n# Build and Train the model\nmodel = H2OAutoML(max_models=10, seed=42, nfolds=5)\nmodel.train(x=x, y=y, training_frame=train_frame)\nlb = model.leaderboard\nlb","cca4231e":"m = h2o.get_model(lb[2, \"model_id\"])\nm.varimp_plot()","38cf3a80":"train_frame_b = train_frame[:, ['Family_Name', 'Honorific', 'Sex', 'Pclass', 'Fare', 'Deck', 'Survived']]\ntrain_frame_b['Survived'] = train_frame_b['Survived'].asfactor()\nx_b = train_frame_b.columns\ny_b = 'Survived'\nx_b.remove(y_b)\nmodel_b = H2OAutoML(max_models=10, seed=42, nfolds=5)\nmodel_b.train(x=x_b, y=y_b, training_frame=train_frame_b)\nlb_b = model_b.leaderboard\nlb_b","753227bc":"predictions = model.leader.predict(test_frame);\npredictions = predictions.as_data_frame()\npredictions = predictions.predict\nsubmission_frame = h2o.H2OFrame(submission)\npassenger_id = submission_frame['PassengerId'].as_data_frame()\nsubmission_final = pd.concat([passenger_id, predictions], axis=1, ignore_index=False)\nsubmission_final.columns = ['PassengerId', 'Survived']\nsubmission_final.to_csv('submission.csv', index=False)\nprint('Submission file saved!')","adc3d39c":"## 5.1 Model Interpretability\n\nIt's not just build the model and make it a black-box, it's necessary to make him interpretable to undestand how the features would contribute to the predictions, therefore, time to see the feature importance.","3c75e534":"Identify the Cabin Deck location with the first letter of `Cabin` feature. It's important to predict the survivors, because the deck location will measure the distance to the ship staircase.<br>\nRead this excellent [Kernel](https:\/\/www.kaggle.com\/gunesevitan\/advanced-feature-engineering-tutorial-with-titanic) for more information.\n","d3c78714":"# 4. Feature Engineering (FE)\n\nAfter analyzing all the variables in EDA, it's time to clean and extract more useful information to feed the model.<br>\nStarting dropping unworthy features: `PassengerId`, `Ticket`.","fe3070db":"Here's the magic of Pandas Profiling, make a good EDA just using the function `ProfileReport`, it give variable distribution, check of null values, variable types, correlation matrix and other useful things.<br>\nApplying it on Training data.","13221c15":"And the new Datasets are here.","6e07a425":"Make the predicitions with the model with all features and append the results to submission file.","56ad43bc":"Let's standardize `Age` and `Fare`.","8fadc26a":"Amazing, no?<br>\nWhat can be learned from this? Let's examine each variable (features that will feed the model) and get some insights.<br>\n\n* `Survived`: The Target variable. The plot indicates a small class imbalance but there's no need for resampling and accuracy seems good to evaluate the model.<br>\n* `Age`: It's a numeric type, has almost 20% of missing values (`NaN`), the distribution has a little right skewness but is near from Normal. There's a small negative correlation with `Survived`, indicating that youngers could have a better chance to survive than the oldest.  <br>\n* `Cabin`: Categorical feature, has a lot of missing values (77.1%) and some passengers have more than one cabin. Maybe the first letter could indicate the deck area, then it is considered as a relevant feature. <br>\n* `Embarked`: Another categorical feature, has only 2 missing values, drop these values on Feature Engineering step will not harm the model. <br>\n* `Fare`: Numerical feature with a right skewness. There's no missing values. Has a small positive correlation with `Survived`, indicating that the person owned a more expensive ticket and following the idea that a better Cabin\/Area\/Class has more chance to survive. Note that it has a mode with low values, hence the big part of passengers bought a more cheaper ticker. <br>\n* `Name`: Categorical feature. No missing values. The name structure is interesting, 'Family Name'+','+'Honorific'+'name'.\n* `Parch`: Numerical with right skewness. No missing values. Small positive correlation with `Survived`.\n* `PassengerId`: Numerical but not relevant for the prediction. Will be dropped in FE step.\n* `Pclass`: Categorical Feature. No Missing Values. There's more 3rd class passengers than others. Negative correlation with `Survived`, it reinforces the idea on `Fare`, expensive tickets allows better classes leading to most chance to survive.\n* `Sex`: Categorical Feature. The distribution indicates that have more men than women. No Missing Values.\n* `SibSp`: Numeric with a small negative correlation with `Survived`. No Missing Values.\n* `Ticket`: Categorical. No missing values but has a High Cardinality.\n\nWith all information gathered from the EDA, it's time to get insights on how to proceed in FE, that starts now.","ab6f0b48":"Drop the two `NaN` samples in `Embarked`.","1e92ce64":"# 3. Exploratory Data Analysis (EDA)","9e9564ea":"Don't get any improvements, althought the lesser AUC score, the model performed well with only the 6 top features.","c6369c66":"We can see that `Family_Name` give a good gain to model prediction, followed by `Honorific` and `Sex`. <br>\nLet's re-train with the 6 top features and see if we could got any improvements.","a7f30c0f":"Looks like for the distribution is better to fill the mode.","6bb740d2":"## 5.2 Predictions","89da02a2":"![](https:\/\/media.giphy.com\/media\/m2tOKbpjpFMvm\/giphy.gif)\n# 1. Introduction\nIn this Kernel is made an approach using three Python modules as the pillars: [fastai](https:\/\/github.com\/fastai\/fastai), [Pandas Profiling](https:\/\/github.com\/pandas-profiling\/pandas-profiling) and [H2O](https:\/\/www.h2o.ai\/products\/h2o\/), but why to use these modules? Because they can do so much work writing just a few lines of code speeding up the process, therefore the name \"Fastanic\".<br>\n\nThe fastai loads a lot of useful modules, as Pandas and Numpy, Pandas Profiling can generate a detailed Exploratory Data Analysis (EDA) with just one code line and H2O using its AutoML module can train different Machine Learning models and ensemble it to make good predictions, but remember there's [*No Free Lunch*](https:\/\/towardsdatascience.com\/a-blog-about-lunch-and-data-science-how-there-is-no-such-a-thing-as-free-lunch-e46fd57c7f27), fastai is good but maybe you will need other modules to make your EDA, Pandas Profiling is not good to report in large datasets (I tried to use on [Porto Seguro Safe Driver Prediction Dataset](https:\/\/www.kaggle.com\/c\/porto-seguro-safe-driver-prediction) without sucess) and H2O AutoML can take a big amount of time to train all the models.<br>\n\nThe Legendary Competition to predict the survivors of Titanic Disaster is a good start point to apply these great modules and understand how to work with them and know their pros and cons. This dataset is small enough to use these tools and give a good idea when to use they and we can dedicate more time to understand the data and apply Feature Engineering (FE) to it.\n\n### 1.1 The Challenge\n\nThe [sinking of the Titanic](https:\/\/en.wikipedia.org\/wiki\/Sinking_of_the_RMS_Titanic) is one of the most infamous shipwrecks in history.<br>\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.<br>\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.<br>\n\nIn this challenge, we ask you to build a predictive model that answers the question: \"what sorts of people were more likely to survive?\" using passenger data (ie name, age, gender, socio-economic class, etc).<br>\n\nIt is your job to predict if a passenger survived the sinking of the Titanic or not.<br>\nFor each in the test set, you must predict a 0 (Dead) or 1 (Survived)  value for the variable.<br>\nYour score is the percentage of passengers you correctly predict, the accuracy.<br>\n\n### 1.2 Data\nSome variables to take note for insights:<br>\n* **pclass**: A proxy for socio-economic status (SES)<br>\n1st = Upper<br>\n2nd = Middle<br>\n3rd = Lower<br>\n\n* **age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br>\n\n* **sibsp**: The dataset defines family relations in this way...<br>\nSibling = brother, sister, stepbrother, stepsister<br>\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)<br>\n\n* **parch**: The dataset defines family relations in this way...<br>\nParent = mother, father<br>\nChild = daughter, son, stepdaughter, stepson<br>\nSome children travelled only with a nanny, therefore parch=0 for them.<br>\n\n# 2. Load Modules and Data\nLet's start importing our Power Trio (fastai, Pandas Profiling and H2O), define the data path and load the available data.","736838e8":"Thinking about `NaN` values, it's good to check it on test set.","b0cdfddc":"About the categorical values left, I will not make a encode, this part I will let the job for the AutoML.<br>\nThe Deck `NaN` will be handled by H2O AutoML.","3089c59b":"And let's check the `Fare` distribution to decide how to fill `NaN` values.","55f4f580":"The model is built using the great [H2O AutoML](http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html), read about, it is a nice addition to your skills.<br>\nIt will train so many Machine Learning algorithms to search for a best fit and report it, even using ensemble in the models trained. There are so many parameters to configure and you need to be cautios and how many models you will train, because it will get so much time to train and get a lot of resources! Known your data and decide if you'll use H2O AutoML or any other AutoML module.","7fd59cd6":"# 6. Conclusion\n\nThe automation tools to speedup the procees worked well, making steps like EDA more smooth and allowing more time to make FE.\nAbout the AutoML, was possible to achieve a good result with little steps, allowing more time to research other solutions.\nThanks for your reading!","8f4a2d1e":"Train the model for 10 algorithms and 5-folds.\nThe sort will be made based on AUC Score, H2O AutoML doesn't have accuracy as sort metric.","2b7d4131":"Time to work with `Age`, since the distribution is near from normal, it's good to fill all `NaN` values with the median to not harm the distribution.","4887badc":"# 5. AutoML Model","8ec04f0f":"Ok, all `NaN` values were filled, we could proceed to build the model, but AutoML, as any Machine Learning Algorithm, is **NOT** a magic wand! You can feed an AutoML Model with no treated Data, but it will not operate a miracle, you as Data Scientist or any other job role is the miracle! You are the wizard that chant spells (coding) learned from your grimoire (your books, courses, etc.) and add your creativity (the miracle factor).<br>\nHence, let's work one more step, remember name? If you create a new feature called `Family_Name` to get the info if the Passengers are in the same family? I got this insight because it could solve the problem about Cabin missing values, let me explain my idea.<br>\n\nAs [the wikipedia article about Titanic Sink](https:\/\/en.wikipedia.org\/wiki\/Sinking_of_the_RMS_Titanic) say that the time of disaster was 02:20 AM, time when much people could be sleep, and family could be sleeping together in the same Cabin or in near Cabins, it could explain why some passengers owned more than one Cabin. Creating this feature will reduce the cardinality in `Name`. I will extract the honorific name too in new feature."}}