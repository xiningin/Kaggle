{"cell_type":{"25cbdf91":"code","801a3f67":"code","b6d4203b":"code","ac84e7db":"code","b4cd1133":"code","c5be28df":"code","a5f83680":"code","78c1d56a":"code","2a1d7f52":"code","a340b81c":"code","2e9b208e":"code","a0625969":"code","9e571efa":"code","4e51875c":"code","30afe37d":"code","95c19770":"code","cceb6b6b":"code","451b2119":"code","a296ad42":"code","f3790bab":"code","a48ab101":"code","7290408f":"code","2be704eb":"code","780f7a74":"code","e3c7e185":"code","f38d4a54":"code","3e59cfbb":"code","039158a2":"code","8d6a2212":"code","ab764190":"code","4faf30a2":"code","efe6be5b":"code","9ecefe7d":"code","1c272a2f":"code","6bdb8878":"code","4eb3c6fe":"code","ab9e7479":"code","656123fc":"code","b1daa5b7":"code","04c80ff1":"code","01fd87ed":"code","ee0c5dcc":"code","6664a0ba":"code","35979fb4":"code","c4635d06":"code","8f681375":"code","80c3f040":"code","27b62155":"code","11fc701a":"code","6b3e9c1f":"code","631457a4":"code","bb466d9f":"code","716e8f02":"code","be5f62df":"code","7153bfc6":"code","4d68d8ae":"code","f7584a5e":"code","1906e3f6":"code","a64bae8b":"code","570170af":"code","d9639635":"code","4a1e6a3d":"code","57c297c0":"code","bd570e74":"code","57b716ff":"code","036f4dd5":"code","b0c8f163":"code","283702a9":"code","eef7c1e7":"code","dc5a04f9":"code","662bcb25":"code","cf853553":"code","545b2f18":"code","de78b0a6":"code","44ff7c29":"code","5a217a4f":"code","b6fd7c44":"code","303259f8":"code","ee8590ee":"code","d76d0590":"code","79788947":"code","e1430e84":"code","5b514f19":"code","3e4727ba":"code","65761a3d":"code","b5b606dd":"code","230c5f65":"code","d41cfb34":"code","57aa3947":"code","7622f9bf":"code","b18bbcd2":"code","3f7f2ab0":"code","8bd4387d":"code","5b12013b":"code","07ed33bf":"code","a31010cd":"code","9f289445":"code","7d46e1e1":"code","190b35db":"code","1b3ffdca":"code","cb995d5f":"code","8be6b0c2":"code","26f6a1e4":"code","f1154b9a":"code","a5ce2c84":"code","686dd301":"code","66743c5f":"code","94d223a0":"code","d8aa2c40":"code","c84257a8":"code","1a5a1651":"code","946e671f":"code","28369cf2":"code","99f62f34":"code","1936cd2b":"code","b7824923":"code","7e9006be":"code","988ba06e":"markdown","68f1b40b":"markdown","04946e1d":"markdown","ae5e40db":"markdown","b7249744":"markdown","2bf7d4b8":"markdown","ff21fcf4":"markdown","e6d703d6":"markdown","9906ca33":"markdown","64493ac2":"markdown","674d5cf5":"markdown","7c7b0fa1":"markdown","686b8c79":"markdown","b2f607a5":"markdown","a1da0df5":"markdown","ba9bdcbb":"markdown","bcc7dd1c":"markdown","78a2830a":"markdown","67a21e18":"markdown","77e1f436":"markdown","53db7172":"markdown","6d59a5db":"markdown","4eae68d4":"markdown","1fa21fe8":"markdown","17950c8e":"markdown","ad580f0c":"markdown","67c9bc40":"markdown","3e4f0ef6":"markdown","b43dcb9f":"markdown","7be0a005":"markdown","13a18022":"markdown","b8c5dcd7":"markdown","763012d5":"markdown","c89e3cbe":"markdown","f20eda81":"markdown","65738488":"markdown","0e1bec2d":"markdown","867e0b9d":"markdown","1770eae5":"markdown","296a61fa":"markdown"},"source":{"25cbdf91":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","801a3f67":"import matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings \ndef warns(*args,**kwargs):\n    pass\nwarnings.warn=warns\nsns.set(rc={'figure.figsize':(16,5)})","b6d4203b":"test=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ng_s=pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\npassengerSurvived=train[train.Survived==1]\nId_test=test[\"PassengerId\"]\nprint(\"# train.csv =>\")\ntrain.head()","ac84e7db":"print(\"# test.csv =>\")\ntest.head()","b4cd1133":"print(\"Shape of train.csv :- \",train.shape)\nprint(\"Shape of test.csv :- \",test.shape)\nprint(\"Shape of gender_submission.csv :- \",g_s.shape)","c5be28df":"# Info of each column\nprint(train.info())","a5f83680":"# description for numerical datatype features\ntrain.describe()","78c1d56a":"# Value count in Target Column.\nprint(train.Survived.value_counts())","2a1d7f52":"sns.countplot(train[\"Survived\"])","a340b81c":"# Count of unique categories in Pclass ..\ntrain.Pclass.value_counts()","2e9b208e":"sns.countplot(train[\"Pclass\"])","a0625969":"# Percent of people survived in different Passenger Class ..\nprint(\"Number of Passengers Survived in Pclass => \\n\",passengerSurvived[[\"Pclass\",\"Survived\"]].groupby(\"Pclass\").count())\nprint(\"=\"*20)\nprint(\"Average of Passengers Survived in PClass => \\n\",train[[\"Pclass\",\"Survived\"]].groupby(\"Pclass\").mean())","9e571efa":"sns.barplot(train[\"Pclass\"],train[\"Survived\"])","4e51875c":"# Average Fare of Passenger Class ..\ntrain[[\"Pclass\",\"Fare\"]].groupby(\"Pclass\").mean()","30afe37d":"sns.set(rc={'figure.figsize':(16,5)})\npassengerSurvived=train[train.Survived==1]\nsns.barplot(x=\"Pclass\",y=\"Fare\",data=passengerSurvived)","95c19770":"# Check the Survival of different Age groups..\nplt.figure(figsize=[18,5])\nplt.scatter(x=\"Age\",y=\"Survived\",data=train)","cceb6b6b":"# Check the Survival of Fare..\nplt.figure(figsize=[18,5])\nsns.scatterplot(x='Age',y='Fare',hue='Survived',data=train)","451b2119":"# Count of unique categories in SibSp ..\ntrain.SibSp.value_counts()","a296ad42":"# Count of unique categories in SibSp ..\ntrain.Parch.value_counts()","f3790bab":"# Survival on basis of Fair..\nplt.figure(figsize=[16,5])\nplt.scatter(x=\"Fare\",y=\"Survived\",data=train)\nplt.xlabel(\"Fare\")\nplt.ylabel(\"Survived\")","a48ab101":"# for object datatype features\ntrain.describe(include=\"O\")","7290408f":"# Name Column..\ntrain[\"Name\"].head(10)","2be704eb":"# Sex Column..\npassengerSurvived.Sex.value_counts()","780f7a74":"sns.countplot(x=\"Sex\",hue=\"Survived\",data=train)","e3c7e185":"train.Embarked.value_counts()","f38d4a54":"print(\"Mean Fare of Embarked ports => \\n\",train[[\"Embarked\",\"Fare\"]].groupby([\"Embarked\"]).mean())","3e59cfbb":"print(\"Mean Survival Rate of Embarked ports => \\n\",train[[\"Embarked\",\"Survived\"]].groupby([\"Embarked\"]).mean())","039158a2":"plt.figure(figsize=(5,5))\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='magma')","8d6a2212":"print(\"Age =>  \\n\",\"Number of missing values => \",train.Age.isnull().sum())\nprint(\" Percentage of missing values => \",(train.Age.isnull().sum()\/891)*100)","ab764190":"# As 20% of values are missing, it can be replaced with the mean of Age column.\ntrain.Age.fillna(train.Age.mean(),inplace=True)\ntrain.Age.isnull().all()","4faf30a2":"print(\"Embarked => \\n\",\"Number of missing values => \",train.Embarked.isnull().sum())","efe6be5b":"# As only 2 values are missing, it can be replaced with the mode of Embarked column.\ntrain.Embarked=np.where(train.Embarked.isnull(),train.Embarked.mode(),train.Embarked)\ntrain.Embarked.isnull().all()","9ecefe7d":"plt.figure(figsize=(5,5))\nsns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='magma')","1c272a2f":"print(\"Age =>  \\n\",\"Number of missing values => \",test.Age.isnull().sum())\nprint(\" Percentage of missing values => \",(test.Age.isnull().sum()\/891)*100)","6bdb8878":"test.Age.fillna(train.Age.mean(),inplace=True)\ntest.Age.isnull().all()","4eb3c6fe":"print(\"Fare => \\n\",\"Number of missing values => \",test.Fare.isnull().sum())","ab9e7479":"test.Fare.fillna(test.Fare.mean(),inplace=True)\ntest.Fare.isnull().all()","656123fc":"train_new=train.drop([\"PassengerId\",\"Cabin\",\"Ticket\"],axis=1)\ntrain_new.columns","b1daa5b7":"test_df=test.drop([\"PassengerId\",\"Cabin\",\"Ticket\"],axis=1)\ntest_df.columns","04c80ff1":"train_new.Embarked.value_counts()","01fd87ed":"# encode Sex column\ntrain_new[\"Sex\"]=np.where(train_new[\"Sex\"]==\"male\",1,0)\n# encode Embarked column\ntrain_new[\"Embarked\"]=train_new[\"Embarked\"].replace({\"S\":1,\"C\":2,\"Q\":3})","ee0c5dcc":"# Check the datatype of encoded dataframe..\ntrain_new.head()","6664a0ba":"test_df[\"Sex\"]=np.where(test_df[\"Sex\"]==\"male\",1,0)\n# encode Embarked column\ntest_df[\"Embarked\"]=test_df[\"Embarked\"].replace({\"S\":1,\"C\":2,\"Q\":3})","35979fb4":"test_df.head()","c4635d06":"train_new[\"Name\"]","8f681375":"ls=[]\nfor row in train_new.Name:\n    start_index=row.find(\", \")\n    end_index=row.find(\".\")\n    ls.append(row[start_index+1:end_index])\nprint(\"length of list => \",len(ls))\nprint(\"\\nList =>\\n\",ls[:35])","80c3f040":"train_new['NameTitle'] = pd.Series(ls)\ntrain_new.head()","27b62155":"# Concating Name Titles having less value count to a single category..\ntrain_new['NameTitle'] = train_new['NameTitle'].replace([' Dr',' Rev',' Mlle', ' Col', ' Major',\n                                       ' Sir',' Ms', ' the Countess',' Lady',' Mme', ' Jonkheer',' Capt',' Don'],' Other')","11fc701a":"train_new.NameTitle.unique()","6b3e9c1f":"train_new.NameTitle.value_counts()","631457a4":"plt.figure(figsize=(5,5))\nsns.countplot(x=\"NameTitle\",hue=\"Survived\",data=train_new)","bb466d9f":"# Survival Rate of each category..\ntrain_new[[\"NameTitle\",\"Survived\"]].groupby(\"NameTitle\").mean()","716e8f02":"# Convert Catergorical to Numerical datatype..\ntrain_new[\"NameTitle\"].replace({\" Mr\":1,\" Miss\":2,\" Mrs\":3,\" Master\":4,\" Other\":5},inplace=True)\ntrain_new.head()","be5f62df":"# Removing Name column from datafrane..\ntrain_df=train_new.drop([\"Name\"],axis=1)\nprint(\"# Dimensions Of Data => \",train_df.shape)\ntrain_df.head()","7153bfc6":"ls=[]\nfor row in test_df.Name:\n    start_index=row.find(\", \")\n    end_index=row.find(\".\")\n    ls.append(row[start_index+1:end_index])\nprint(\"length of list => \",len(ls))\nprint(\"\\nList =>\\n\",ls)","4d68d8ae":"test_df['NameTitle'] = pd.Series(ls)\n# Concating Name Titles having less value count to a single category..\ntest_df['NameTitle'] = test_df['NameTitle'].replace([' Dr',' Rev',' Mlle', ' Col', ' Major',\n                                       ' Sir',' Ms', ' the Countess',' Lady',' Mme', ' Jonkheer',' Capt',' Don',' Dona'],' Other')","f7584a5e":"# Dropping Name Column..\ntest_df.drop([\"Name\"],axis=1,inplace=True)\n# Convert Catergorical to Numerical datatype..\ntest_df[\"NameTitle\"].replace({\" Mr\":1,\" Miss\":2,\" Mrs\":3,\" Master\":4,\" Other\":5},inplace=True)\ntest_df.NameTitle.value_counts()","1906e3f6":"# let's see how data is distributed for every column\nfrom scipy.stats import norm\nplt.figure(figsize=(15,15), facecolor='white')\nplotnumber = 1\n\nfor column in train_df: \n    if plotnumber<=len(train_df.columns):\n        ax = plt.subplot(3,3,plotnumber)\n        sns.distplot(train_df[column],rug=True)\n        plt.xlabel(column,fontsize=20)\n    plotnumber+=1\nplt.tight_layout()","a64bae8b":"# Age column..\nprint(\"Passengers with age less than 1 => \")\ntrain_df[train_df.Age<1]","570170af":"# if Age is less than 1 like 0.24 -> make it one, if not less than 1 => round off..\nfor index,value in enumerate(train_df.Age):\n    if value<1:\n        train_df.Age[index]=1      \n    else:\n        train_df.Age[index]=round(value)\n\n# Change datatype float to int..\ntrain_df.Age=train_df.Age.astype('int64')","d9639635":"print(\"Passengers with age less than 1 => \",len(train_df[train_df.Age<1]))","4a1e6a3d":"# Fare Column..\nprint(\"Number of Passenger with Zero Fare => \",len(train_df[train_df.Fare==0]))","57c297c0":"# As these are only 15 we can replace these with mean Fare..\ntrain_df.Fare.replace(0,train_df.Fare.mean(),inplace=True)\nprint(\"Number of Passenger with Zero Fare => \",len(train_df[train_df.Fare==0]))\n\nprint(\"Passengers with age less than 1 => \",len(train_df[train_df.Age<1]))","bd570e74":"train_df.head()","57b716ff":"train_df['Age']=np.log(train_df['Age']+1)\ntrain_df['Fare']=np.log(train_df['Fare']+1)","036f4dd5":"(train_df['Age']).plot(kind = 'density', title = 'Log Age distribution', fontsize=14, figsize=(8,5))","b0c8f163":"test_df['Age']=np.log(test_df['Age'])\ntest_df['Fare']=np.log(test_df['Fare']+1)","283702a9":"(test_df['Age']).plot(kind = 'density', title = 'Log Age distribution', fontsize=14, figsize=(8, 5))","eef7c1e7":"plt.figure(figsize=(10,10))\nsns.heatmap(train_df.corr(),annot=True,vmin=-1, vmax=1, center= 0)","dc5a04f9":"# Shifting target column at last ..\nsurvived_col=train_df.pop(\"Survived\")\ntrain_df[\"Survived\"]=survived_col \ntrain_df.head(3)","662bcb25":"X = train_df.drop(['Survived'],axis=True)\ny = train_df['Survived']","cf853553":"X","545b2f18":"from sklearn.preprocessing import StandardScaler\nscalar = StandardScaler()\nX_scaled = scalar.fit_transform(X)\nX_scaled","de78b0a6":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif[\"vif\"] = [variance_inflation_factor(X_scaled,i) for i in range(X_scaled.shape[1])]\nvif[\"Features\"] = X.columns\n#let's check the values\nvif","44ff7c29":"scaled_df=pd.DataFrame(X_scaled,columns=X.columns)\nscaled_df.head()","5a217a4f":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test=train_test_split(scaled_df,y,test_size=0.20,random_state=42)","b6fd7c44":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score","303259f8":"log_reg = LogisticRegression()\nlog_reg.fit(x_train,y_train)","ee8590ee":"y_pred = log_reg.predict(x_test)","d76d0590":"accuracy= accuracy_score(y_test,y_pred)\naccuracy","79788947":"conf_mat = confusion_matrix(y_test,y_pred)\nconf_mat","e1430e84":"predictions = log_reg.predict(x_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions))","5b514f19":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(log_reg, X_scaled, y, cv=5)\nscores","3e4727ba":"print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))","65761a3d":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV","b5b606dd":"clf=DecisionTreeClassifier()\nclf.fit(x_train,y_train)\nclf.score(x_train,y_train)","230c5f65":"clf_pred = clf.predict(x_test)\nclf.score(x_test,y_test)","d41cfb34":"# we are tuning three hyperparameters right now, we are passing the different values for both parameters\ngrid_param = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth' : range(2,32,1),\n    'min_samples_leaf' : range(1,10,1),\n    'min_samples_split': range(2,10,1),\n    'splitter' : ['best', 'random']    \n}","57aa3947":"grid_search = GridSearchCV(estimator=clf,\n                     param_grid=grid_param,\n                     cv=2, n_jobs =-1,verbose=3)\ngrid_search.fit(x_train,y_train)","7622f9bf":"best_parameters = grid_search.best_params_\nprint(best_parameters)\nprint(grid_search.best_score_)","b18bbcd2":"clf=DecisionTreeClassifier(criterion='entropy',max_depth=15, min_samples_leaf=8, min_samples_split=9,splitter=\"random\")\nclf.fit(x_train,y_train)","3f7f2ab0":"clf.score(x_test,y_test)","8bd4387d":"scores = cross_val_score(clf, X_scaled, y, cv=5)\nscores","5b12013b":"print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))","07ed33bf":"predictions = clf.predict(x_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions))","a31010cd":"y_pred = clf.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\nconf_mat","9f289445":"from sklearn.ensemble import RandomForestClassifier","7d46e1e1":"rand_clf = RandomForestClassifier(random_state=6)\nrand_clf.fit(x_train,y_train)","190b35db":"rand_clf.score(x_test,y_test)","1b3ffdca":"# we are tuning three hyperparameters right now, we are passing the different values for both parameters\ngrid_param = {\n    \"n_estimators\" : [90,100,115,130],\n    'criterion': ['gini', 'entropy'],\n    'max_depth' : range(2,20,1),\n    'min_samples_leaf' : range(1,10,1),\n    'min_samples_split': range(2,10,1),\n    'max_features' : ['auto','log2']\n}","cb995d5f":"grid_search = GridSearchCV(estimator=rand_clf,param_grid=grid_param,cv=5,n_jobs =-1,verbose = 3)\ngrid_search.fit(x_train,y_train)","8be6b0c2":"#let's see the best parameters as per our grid search\ngrid_search.best_params_","26f6a1e4":"rand_clf = RandomForestClassifier(criterion=\"entropy\",n_estimators=90, max_depth=7, min_samples_leaf=1,\n                                  min_samples_split=3, max_features='log2')\nrand_clf.fit(x_train,y_train)","f1154b9a":"rand_clf.score(x_test,y_test)","a5ce2c84":"scores = cross_val_score(rand_clf, X_scaled, y, cv=5)\nprint(scores)\nprint(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))","686dd301":"y_pred=rand_clf.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\nconf_mat","66743c5f":"predictions = rand_clf.predict(x_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions))","94d223a0":"from sklearn.svm import SVC","d8aa2c40":"svm_clf=SVC(random_state=0)\nsvm_clf.fit(x_train,y_train)\nsvm_clf.score(x_test,y_test)","c84257a8":"# Cross validation score\nscore_svm = cross_val_score(svm_clf, X_scaled, y, cv=5)\nprint(score_svm)\nprint(\"%0.2f accuracy with a standard deviation of %0.2f\" % (score_svm.mean(), score_svm.std()))","1a5a1651":"y_pred=svm_clf.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\nconf_mat","946e671f":"predictions = svm_clf.predict(x_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions))","28369cf2":"from xgboost import XGBClassifier\nmodel_7 = XGBClassifier()\nmodel_7.fit(x_train,y_train)\npred = model_7.predict(x_test)\nscore_7 = accuracy_score(y_test,pred)\nscore_7","99f62f34":"predictions_7 = model_7.predict(x_test)\nprint(classification_report(y_test,predictions_7))","1936cd2b":"score_xgb = cross_val_score(model_7, X_scaled, y, cv=5)\nscore_xgb","b7824923":"print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (score_xgb.mean(), score_xgb.std()))","7e9006be":"y_pred=model_7.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\nconf_mat","988ba06e":"### Removing Skewness in Test Data ..","68f1b40b":"#### Testing Data...\n\nCoverting for test aswell, for fitting it into our model.","04946e1d":"### Observation_2 ->\n1). PassengerId has all unique values -> so it can be removed.                            \n2). The <b>survival rate<\/b> in our data <b>is 38%<\/b> which was 32% in titanic tragedy.                              \n3). Most of the people were in 3rd Passenger Class. <br>\n4). <b>63%<\/b> people <b>survived in Pclass 1<\/b>, 47% in Pclass 2, 24% in Pclass 3.                                           \n5). The <b>average Fare of Pclass 1 is 84<\/b>, Pclass 2 is 20.6 and Pclass 3 is 13.6. <br>\n6). No clear cluster to say which Age group surives the most. <br>\n7). The highest aged person was 80 years old. <br>\n8). Most of the passengers had no Siblings\/Spouse and Parent-Child relation. <br>\n9). Hightest Fare was 512.\n\n#### Insights ->\n1). The Fare of upper and lower class varies too much, thus most number of passengers in 3rd Passenger_class.","ae5e40db":"### Missing Value Imputation ->\nFrom Observation 1, we are going to impute the missing values in Age and Embarked Column.","b7249744":"### Removing the Skewness In the Train Data Col","2bf7d4b8":"## Section-1). Analyzing the Data","ff21fcf4":"#### Testing data.","e6d703d6":"#### Observation ->\nThis confirm the fact that female survived the most, as Miss has highest survival rate.","9906ca33":"#### Observation ->\nThe Age and Fare cannot be zero as seen in the distplot. Let's deal with it.","64493ac2":"### Observations_3 ->\n1). Name contains all unique values , therefore cannot contribute much too our model thus can be removed.  <\/br>             \n2). We can <b>extract a new feature from name<\/b> which contains tittle of name (like - mrs. ,mr. ,dr. , etc.). <\/br>           \n3). 577 are men out of 891 (i.e. There were 65 % of men on the ship ), but survival rate of female was more than men. <\/br>                                          \n4). Ticket has large percentage of unique values , therfore can be removed (as contains many unique values).  <\/br>             \n5). Cabin has lots of missing values , therefore can be removed from our data.  <\/br>                         \n6). There were 3 Embarked ports where S was the most frequent one. <br>\n\n#### Insights ->\n1). As Fare for port C is highest, it can be said that C was the first port in path or most of the passenger in Pclass 1 embarked from port C.","674d5cf5":"### Classification Report","7c7b0fa1":"## 5). Xg Boost Classifier..","686b8c79":"### Feature Extraction ->\nAs mentioned in Observation_3, we can extract new feature from name column.","b2f607a5":"## Section-2). EDA and Preprocessing of Data ..","a1da0df5":"### Confusion Matrix","ba9bdcbb":"## Split the data in train and test","bcc7dd1c":"#### Observations ..\nNo value exceeds 5, therefore no collinearity found.","78a2830a":"### Training Data..","67a21e18":"### Correlation Checking..","77e1f436":"## Data loading ..","53db7172":"# Titanic Machine Learning From Disaster\n\n<img src=\"https:\/\/www.historic-uk.com\/wp-content\/uploads\/2017\/04\/the-sinking-of-the-rms-titanic.jpg\" alt =\"Titanic\" style='width: 1000px;'>\n\n## Problem Statement\nUse machine learning to create a model that predicts which passengers survived the Titanic shipwreck or not.\n\n\n## Dataset\n1). <b>train.csv:-<\/b> training dataset, contains all attributes required to make a predictive model along with the outcome attribute i.e. \"ground truth\".\n\n2). <b>test.csv:-<\/b> testing dataset, contains all attributes except outcome.\n\n3). <b>gender_submission.csv:-<\/b> a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n\n## Attributes Dictionary\n\n<b>PassengerID:-<\/b> Unique Id's of passenger\n\n<b>Survived:-<\/b> Target column <br>\n1 = Survived <br>\n0 = Not Survived.\n\n<b>Pclass:<\/b> Ticket Class<br>\n1 = Upper class <br>\n2 = Middle class <br>\n3 = Lower class\n\n<b>Name:-<\/b> Name of passenger.\n \n<b>Sex:-<\/b> Sex of passenger.\n\n<b>Age:-<\/b> Age of passenger in years.\n\n<b>Sibsp:-<\/b> The dataset defines family relations in this way. <br>\nSibling = brother, sister, stepbrother, stepsister <br>\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n<b>Parch:-<\/b> The dataset defines family relations in this way. <br>\nParent = mother, father <br>\nChild = daughter, son, stepdaughter, stepson <br>\nSome children travelled only with a nanny, therefore parch=0 for them.\n\n<b>Ticket:-<\/b> Ticket number of passenger.\n\n<b>Fare:-<\/b> Passenger Fare.\n\n<b>Cabin:-<\/b> Cabin Number.\n\n<b>Embarked:-<\/b> Port of Embarkation(port on which passenger started journey). <br>\nC = Cherbourg <br>\nQ = Queenstown <br>\nS = Southampton.","6d59a5db":"#### Training Data ..","4eae68d4":"## 4). SVM..","1fa21fe8":"## ..... MODEL SELECTION .....","17950c8e":"### Check Data Distribution Of Each Feature ->","ad580f0c":"## 1). Logistic Regression","67c9bc40":"### Encoding the Categorical Features ->\nAs observed from above Sex column has 2 categories and Embarked Column has 3.","3e4f0ef6":"### Check Multicolinearity ..","b43dcb9f":"### Standardization..","7be0a005":"## Import Packages ..","13a18022":"### Removing insignificant features ->\nFrom observation 2 and 3, Ticket and Cabin column can be removed.","b8c5dcd7":"## 2). Decision Tree","763012d5":"### Cross-Validation Score ..","c89e3cbe":"=> As per the correlation heatmap -> SibSp and Parch are low correlated, this basically means if someone has child onboard, may have spouce onboard too, or if sibling then may have parent too. <br>\n=> In overall low correlation (less than 0.5) is seen in some features but we can negelect it.","f20eda81":"#### Training data.","65738488":"### Observation_1 :-\n1). Titanic had 2224 passengers and crew .                                                \nOur train data has 891 rows (i.e. data of 891 passengers ) and 12 columns ( their information ) and test data has 418 rows and 11 columns.\n\n2). Age, Embarked and Cabin column has null\/missing values.","0e1bec2d":"### Split the features and Target column..","867e0b9d":"### Testing Data ..","1770eae5":"## 3). Random Forest..","296a61fa":"## Final Note => \n\nXgboost shows best performance results(f1 score and accuracy). <br>\n\nAny suggestions for improvement --? Always welcomed!"}}