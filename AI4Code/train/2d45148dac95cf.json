{"cell_type":{"2feedd77":"code","1959bf7d":"code","441c3657":"code","01eeef49":"code","b5e16663":"code","855a2b7d":"code","3f73e967":"code","d4799ab8":"code","96048e69":"code","1ad2cf6c":"code","1271bfb0":"code","eb07a631":"code","22af2206":"code","ade20f84":"code","45a87e94":"code","36d4c293":"code","e842d9e2":"code","c592494c":"code","132f7dcd":"code","d90c973f":"code","745db49f":"code","12cdaf77":"code","6c7e31c2":"code","62f3d6a0":"code","b9d42c90":"code","dfd763d2":"markdown","0e34a498":"markdown","f8799327":"markdown","77e00ae8":"markdown","a7bb611d":"markdown","5a4ea01c":"markdown","7d0f64bf":"markdown","ce25c840":"markdown","437e232f":"markdown","bbf52968":"markdown","2cfc77f6":"markdown","19b8ca17":"markdown","6635488a":"markdown","86362daf":"markdown","fd47a531":"markdown","55103899":"markdown","c76204ac":"markdown"},"source":{"2feedd77":"# Basics \nimport pandas as pd \nimport numpy as np \n# Data Viz \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n# Miscelaneous \nimport warnings \nimport os \nimport time \n# Machine Learning\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, KFold\nfrom sklearn.metrics import confusion_matrix \nfrom imblearn.over_sampling import SMOTE\nimport xgboost as xgb\n# Neural Networks \nimport tensorflow as tf\nprint(\"Tensorflow Version:\",tf.__version__)\nfrom tensorflow.keras.layers import Input, Dense, Dropout, LeakyReLU\nfrom tensorflow.keras.models import Model\n# Ignore pesky warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n%matplotlib inline ","1959bf7d":"##### EDA Helper Functions \n\ndef correlation_matrix(df):\n    '''\n    This function prints out a Pearson's Correlation Matrix \n    '''\n    corr = df.corr()\n    f, ax = plt.subplots(figsize=(10, 10))\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n    sns.heatmap(corr, mask=None, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n##### DATA PREPARATION HELPER FUNCTIONS #####\n\ndef make_dummy(df, feature):\n    '''\n    This feature makes dummy variables out of a data frame's feature.\n    '''\n    dummy = pd.get_dummies(df[feature])\n    return(dummy)\n\n##### MACHINE LEARNING HELPER FUNCTIONS #####\n\ndef classifier(model,X_train, y_train, X_test, y_test, printOut = False):\n    '''\n    This function trains a model and evaluates it with classification performance metrics.\n    '''\n    # Fit \/ train\n    from sklearn.metrics import accuracy_score\n    model.fit(X_train,y_train)\n    \n    # Predict\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    \n    accuracy_score_train = accuracy_score(y_train, y_pred_train)\n    accuracy_score_test = accuracy_score(y_test, y_pred_test)\n    \n    # Printing predictions \n    print(\"TRAIN Results:\\n\")\n    print(f\"Accuracy: {round(accuracy_score_train, 2)}%\")\n    print(\"\\n\")\n    print(\"TEST Results:\\n\")\n    print(f\"Accuracy: {round(accuracy_score_test, 2)}%\")\n    print(\"\\n\")\n\n    # , precision_score_test, recall_score_test, f1_score_test, roc_auc_test\n    return (accuracy_score_test)\n\ndef map_feature_importances(clf,X,top=20):\n    '''\n    This function prints out a graph representing the top 20 most important features of a RF or XGB model.\n    '''\n    import matplotlib as plt\n    feat_importances = pd.Series(clf.feature_importances_, index=X.columns.values)\n    feat_importances.nlargest(top).sort_values().plot(kind='barh', color='darkgrey', figsize=(20,10))\n        \n        \ndef plot_xgb(model,X_train,X_test,y_train,y_test):\n    '''\n    This function prints out the booster's Log Loss and Classification Errors over time. \n    '''\n    from sklearn.metrics import accuracy_score\n    from matplotlib import pyplot\n    eval_set = [(X_train, y_train), (X_test, y_test)]\n    # make predictions for test data\n    y_pred = model.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n\n    # evaluate predictions\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\n    # retrieve performance metrics\n    results = model.evals_result()\n    epochs = len(results['validation_0']['error'])\n    x_axis = range(0, epochs)\n\n    # plot log loss\n    fig, ax = pyplot.subplots(figsize=(8,8))\n    ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n    ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n    ax.legend()\n    pyplot.ylabel('Log Loss')\n    pyplot.title('XGBoost Log Loss')\n    pyplot.show()\n\n    # plot classification error\n    fig, ax = pyplot.subplots(figsize=(8,8))\n    ax.plot(x_axis, results['validation_0']['error'], label='Train')\n    ax.plot(x_axis, results['validation_1']['error'], label='Test')\n    ax.legend()\n    pyplot.ylabel('Classification Error')\n    pyplot.title('XGBoost Classification Error')\n    pyplot.show()","441c3657":"# Dealing with data frame \ndf = pd.read_csv(\"..\/input\/churn-for-bank-customers\/churn.csv\", index_col=False)\nprint(\"The dataset has \", df.shape[0], \" data points and \", df.shape[1], \" features.\\n\\n\")","01eeef49":"df.head()","b5e16663":"#I've comented the followig lines out but checking the dataset for missing values and data types is a must. \n#df.isna().sum()\n#df.dtypes\ndf.describe() # Finally, this prints out a lot of statistics. ","855a2b7d":"\ndf.boxplot(column=\"CreditScore\")","3f73e967":"df.Gender.value_counts().plot(kind='pie')","d4799ab8":"sns.pairplot(df, hue=\"Exited\", palette=\"coolwarm\")","96048e69":"# correlation table\n\ncorr = df.corr()\nf, ax = plt.subplots(figsize=(10, 10))\ncmap = sns.diverging_palette(220, 20, as_cmap=True)\nsns.heatmap(corr, mask=None, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","1ad2cf6c":"# Preparing data \ndata = df.drop(['RowNumber','CustomerId','Surname'], axis=1)\n\ndummy_feature_list = ['Gender','Geography'] \nfor f in dummy_feature_list:\n    dumm = make_dummy(data,f)\n    data = pd.concat([data,dumm], axis=1)\ndata = data.drop(dummy_feature_list, axis=1)\n","1271bfb0":"# ORIGINAL DATA\nX = data.drop(['Exited'],axis=1) # The features\ny = data.Exited # The target feature\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42) # Splitting data \n\n# RESAMPLED DATA\nseed = 42\nk = 1 \nsm = SMOTE(sampling_strategy = 'minority', k_neighbors = k, random_state = seed)\nres_X, res_y = sm.fit_resample(X, y)\nres_X_train, res_X_test, res_y_train, res_y_test = train_test_split(res_X, res_y,test_size=0.2, random_state=42)\n\n# RESCALED DATA \nscaler = StandardScaler()\nstd_X = scaler.fit_transform(res_X)\nstd_X_train,std_X_test,std_y_train,std_y_test = train_test_split(std_X,res_y,test_size=0.2,random_state=42) # Splitting data\n\n# Going to stick to rescaled data but rename it for convenience's sake\nX_train,X_test,y_train,y_test = std_X_train,std_X_test,std_y_train,std_y_test","eb07a631":"xgb_clf = xgb.XGBClassifier()\n\nxgb_params = {\"colsample_bytree\":[0.3,0.6,0.8],\n              \"gamma\":[0,0.01,0.1,0.5],\n              \"learning_rate\":[0.01,0.03,0.1],\n              \"max_depth\": [2,4,6,None],\n              \"n_estimators\": [100,200, 500, 1000],\n              \"subsample\":[0,0.5,1],\n              \"min_samples_split\": [2,5,10,20,30],\n              \"lambda\":[0,0.001,0.01,1],\n              \"alpha\":[0,0.001,0.01,1]}\n\nxgb_cv_model = RandomizedSearchCV(estimator= xgb_clf, \n                                 param_distributions = xgb_params, \n                                 n_iter=50, cv=10, scoring='accuracy', \n                                 n_jobs=-1, verbose=2).fit(std_X_train, std_y_train)","22af2206":"# These are the best hyperparameters that the search could give us. \nxgb_cv_model.best_params_","ade20f84":"# instantiate xgboost with best parameters\nbooster = xgb.XGBClassifier(subsample=0.5,colsample_bytree=0.6, alpha=0.001,gamma=0.5,learning_rate=0.03, \n                           max_depth=6, n_estimators=500, random_state=1, verbosity=1)\nbooster_metrics = classifier(booster,X_train, y_train, X_test, y_test, printOut=True)","45a87e94":"# create a baseline\neval_set = [(X_train, y_train), (X_test, y_test)]\nbooster.fit(X_train,y_train,eval_set=eval_set,eval_metric=[\"error\", \"logloss\"])","36d4c293":"plot_xgb(booster,X_train,X_test,y_train,y_test)","e842d9e2":"feat_importances = pd.Series(booster.feature_importances_, index=X.columns.values)\nfeat_importances.nlargest(20).sort_values().plot(kind='barh', color='darkgrey', figsize=(20,10))","c592494c":"# Build the model using the functional API\n# create model\ni = Input(shape=std_X_train[-1].shape)\nx = Dense(512,  activation='relu')(i)\nx = Dropout(0.2)(x)\nx = Dense(512,  activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(256,  activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(256,  activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(64, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(32, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(16, activation='relu')(x)\nx = Dropout(0.4)(x)\nx = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(i, x)\n\n# Compile and fit\n# Note: make sure you are using the GPU for this!\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nr = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50)","132f7dcd":"# Plot loss per iteration\nimport matplotlib.pyplot as plt\nplt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()","d90c973f":"# Plot accuracy per iteration\nplt.plot(r.history['accuracy'], label='acc')\nplt.plot(r.history['val_accuracy'], label='val_acc')\nplt.legend()","745db49f":"# Build the model using the functional API\n# create model\ni = Input(shape=std_X_train[-1].shape)\nx = Dense(1024,  activation='relu')(i)\nx = Dropout(0.2)(x)\nx = Dense(512,  activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(256,  activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(256,  activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(64, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(32, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(16, activation='relu')(x)\nx = Dropout(0.4)(x)\nx = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(i, x)\n\nimport keras\n# Choosing optimizer\nopt = keras.optimizers.Adam(learning_rate=0.001)\n\n# Compile and fit\nmodel.compile(optimizer=opt,\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nr = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15)","12cdaf77":"# Plot loss per iteration\nimport matplotlib.pyplot as plt\nplt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()","6c7e31c2":"# Plot accuracy per iteration\nplt.plot(r.history['accuracy'], label='acc')\nplt.plot(r.history['val_accuracy'], label='val_acc')\nplt.legend()","62f3d6a0":"# Build the model using the functional API\n# create model\ni = Input(shape=std_X_train[-1].shape)\nx = Dense(1024,  activation='relu')(i)\n#x = Dropout(0.1)(x)\nx = Dense(512,  activation='relu')(x)\n#x = Dropout(0.1)(x)\nx = Dense(256,  activation='relu')(x)\n#x = Dropout(0.1)(x)\nx = Dense(256,  activation='relu')(x)\n#x = Dropout(0.1)(x)\nx = Dense(64, activation='relu')(x)\n#x = Dropout(0.1)(x)\nx = Dense(32, activation='relu')(x)\n#x = Dropout(0.1)(x)\nx = Dense(16, activation='relu')(x)\n#x = Dropout(0.1)(x)\nx = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(i, x)\n\nimport keras\n# Choosing optimizer\nopt = keras.optimizers.Adam(learning_rate=0.001)\n\n# Compile and fit\nmodel.compile(optimizer=opt,\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nr = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=7)","b9d42c90":"# Plot accuracy per iteration\nplt.plot(r.history['accuracy'], label='acc')\nplt.plot(r.history['val_accuracy'], label='val_acc')\nplt.legend()","dfd763d2":"### Pearson's Correlation Matrix\n\nIdeally, we don't want our features to be correlated too strongly. This would make including both features of such pairs redundant, since they influence the result in similar fashion. The matrix shows us that:\n\n- Number of products indirectly correlated with Balance. (Remaining customers have max 1 product. This explains the negative correlation.)\n- High correlation between Exited and Age. (The older the client, the higher the probability of having left.)\n- Some small positive correlation between Exited and Balance. \n- Some negative correlation between Exited and Active\/Inactive.\n\nOverall, we have heteroscedasticity. The correlation indices are way to small to consider any feature for removal. Generally, we want to eliminate one feature in a pair which has correlation greater than modulo(0.5).\n","0e34a498":"# II. Helper Functions\n\nNext, I am going to write a series of funtions that will help along the way to plot some graphs or fit and evaluate some models. ","f8799327":"This tiny line of code is awesome for producing histograms and scatter plots for the whole dataset. It takes a bit of time to run but the end result is worth it. We can even color code the plot's hue by a feature of our choice. In this case, blue is remaining customers and orange is exited customers.\n\n### Insights \n\nExited customers are older, on average, than those still active. This kind of makes sense, as clients who have left must have been with the bank some time. The young ones have not really had the reason or the opportunity to yet leave. The bank should look out for middle aged clients who might be looking for alternatives. \n\nThe exited customers also had higher balances on average. Again, this makes more sense if we look back at the previous ovservation. Having younger clients naturally means that their savings are likely to be lower, and so should their income. Either way, what we can observe is basically two large groups (or sub-distributions) in each distribution (exited\/not-exited): those with low balance (actually most of it is absolute zero) and the other ones, which have a Gaussian Bell Shaped distribution of data points, as all data tends to conform to eventually. \n\nThe customers that remain mostly have 0 to 1 products. Those who have exited also held up to 4, in a small proportion. I believe there might be something unexplained in the data here. Perhaps it is because the bank used to have more products but now it doesn't, and older customers, with greater tenure, that have been with them for a long time, benefited from different products\/services that are no longer available. Or perhaps the tendency is for clients to not use those products anymore.  \n\nIndeed,there were more exited customers who didn't have a card. This is a sure sign of not sticking with the bank much. ","77e00ae8":"The dataset has no missing values. The statistics look allright. The datatypes are as expected of such features. I consider these features to be sufficient. Feature Engineering could be helpful, but nothing useful can come of the present ones, as demonstrated in <a href=\"https:\/\/www.kaggle.com\/ahalimzdemir\/churn-project\">this notebook<\/a>, where the newly created features were at the bottom of the models' importance charts. \n\n### Continuous variables\n- Credit Score: MAX  is 850. MIN is 350. AVG of 650; STD is 96,7 points. \n- Age: MAX is 92 yrs; MIN is 18 yrs; AVG is 38 yrs; STD is 10,5 yrs. \n- Tenure: MAX 10 years; MIN is 0; AVG is 5; STD is 2,89. \n- Balance: MAX is $ 250.898; MIN is $ 0; AVG is $ 77.000; STD is 62.000.\n- Num of Products: MAX is 4. MIN is 1. AVG is 1,53. \n- Estimated Salary: MAX is 199.992; MIN is 11; AVG is 100.000; STD is 57.000. (Quite high variance. Must further inspect this.)\n","a7bb611d":"# IV. Data Analysis\n\nThe way I would analyze the data is visual inspection using Tableau, as it makes the whole process much easier and pleasant. Perhaps I will come back to this section soon to uncover more of the data's hidden secrets. Stay tuned.\n\n### Further questions:\n- Are there any 0 Tenure customers who have already Exited?\n- Which Tenure group (there are 10 groups) has the highest Churn rate?\n- What about the gender pay gap?\n- Which country has the biggest problems in customer retention?\n- Are there any new features that could be engineered? \n\n# V. Machine Learning\n\nFirst, we need to prepare the data. We eliminate useless features and one-hot-encode categorical features.","5a4ea01c":"We can even check out some of the more interesting features using different data visualization techniques. \n\n#### Box Plot Insights\n- Some true outliers with very low credit scores.\n- Some very old customers are outliers.\n- Tenure perfectly distributed\n- Salaries perfectly distributed\n- Very few outliers have 4 products.\n- Too many customers seem to have very low balance.\n\n#### Histogram Insights\n- Normal(ish) distribution for credit scores.\n- Predominantly young clientele. Mostly working class.\n- Large number of old clients. Very few brad new clients.\n- Vast majority of clients have 1 or 2 products, rarely 3, almost never 4.\n- Estimated salary's distribution looks off. I should inspect that in detail.\n- Balance: if not for the vast majority having 0 balance, it would be a perfectly normal distribution.\n- Idea: Perhaps the 0 balance corresponds to only exited clients?\n\n#### Pie Plot insights \n- Slightly more males than females. Almost 50\/50. \n- Around 75% have a credit card. \n- 50 \/ 50 Active and Inactive members. \n- 25 % of clients are EXITED. \n- Half the customers are French. The rest is evenly split between Germans and Spaniards.\n\n#### Observation: \n- If I am to classify Exited or not, the classes are imbalanced. This will need to be addressed. ","7d0f64bf":"Here we can see our model's results. Pretty good.","ce25c840":"As can be seen from the charts, the model simply starts overfitting after 15 epochs, where it's reached its peak. After this, validation accuracy does not increase anymore and model just learns the training data by heart. \n\n## Question\n\nI haven't figured out why this NN behaves as such. Why is the validation accuracy higher and its loss lower than those of the test set? I welcome you all to comment and try to solve this. Perhaps I should try different train\/test splits?\n\n## Possible answers\n\nPrzemys\u0142aw Dolata had <a href=\"https:\/\/www.researchgate.net\/post\/When_can_Validation_Accuracy_be_greater_than_Training_Accuracy_for_Deep_Learning_Models\">this<\/a> to say about dropout (a regularization technique I am making heavy use of in this example): \"The training loss is higher because you've made it artificially harder for the network to give the right answers. However, during validation all of the units are available, so the network has its full computational power - and thus it might perform better than in training.\"\n","437e232f":"# III. Data Mining","bbf52968":"# VI. Deep Learning (ANN)\n\nNow let's attempt a simple neural network and see if it performs better than the XGBoost. Honestly, on this type of data, I assumed it won't (and it didn't). Plus, the cost (time, computational power) of training is be much greater. There's also the question of architecture choice and hyperparameter tuning which is more difficult with NNs.\n\nNote: I tried smaller architectures and they didn't perform as well. Also, without adding dropout layers, the NN overfits.","2cfc77f6":"### Plotting the training results \n\nHere we can see that the validation sticks close to the training in both loss and classification error. We can also see that there is room for a bit if improvement if we can perhaps let the Booster grow more trees. Albeit, the progress is going to be very slow after these 500 iterations. ","19b8ca17":"### Train\/Test splitting, SMOTE and Standardization \nXGBoost does not require the data to be standardized, but the ANN we will use later does. Either way, I noticed no difference in training the XGB with un-standardised data or otherwise. What is truly important and had a great effect on our performance is using the SMOTE algorithm to \"produce\" more artificial data points that become representative of our minority class, which is customers who are no longer with the bank, which are outnumbered 3 to 1. Ideally, we'd like to have data that is equally representative of all of our studied classes. When we do not have that, we can use sampling strategies.","6635488a":"# Conclusions \n\nThis type of Machine Learning task is rather trivial. The data is small and clean. The predicted outcome is binary. The data points are labeled and the learning is supervised. Still, there is much to be learned from the Exploratory Data Analysis and from basic Statistical Assumptions about the underlying structure of the data. When I say this, I am referring to the relationships that exists between the studied variables and even about their own individial characteristics such as distribution. \n\nThis type of work could come in handy to an organization that cares about customer retention. I would recommend that machine learning could be used to flag customers that are potentially willing to leave the bank. After this, a case worker cand pick up the person's profile and analyse it. A decision on how to proceed with the client is made by balancing the worker's expertize, experience and guts and insights that were gained through Data Analytics.\n\nWe saw here a brief taste of what it means to search for good hyperparameters, we saw what regularization techniques can do for the model's performance. We saw that re-sampling imbalanced datasets can be used to reduce bias. Basic data visualization on-the-go is great for testing assumptions and getting creative. \n\nI truly hope that you find this notebook useful and that it also finds you in your time of need. ^^ Please leave a comment if you liked it and don't be afraid to question, criticize or direct to my attention any peculiarity that you find here. ","86362daf":"### Eliminating dropout\n\nThis is what happens if we eliminate the regularization. Frankly, there is not much going on. The model is behaving in approximately the same way, only this time it gets a little worse and the training and validation performance cross over each other earlier, at the 6th epoch.","fd47a531":"# Customer Churn Prediction using XGBoost and ANN\n\n![](https:\/\/miro.medium.com\/max\/762\/1*X-oZNRw5Pnef-kR9CgLx1g.png)\n\n*(Awesome, evocative picture. Courtesy of <a href=\"https:\/\/medium.com\/@superalbert\">Super Albert<\/a>,  on <a href=\"https:\/\/medium.com\/diogo-menezes-borges\/predicting-banks-churn-with-artificial-neural-networks-f48393fb1f9c\">Medium<\/a>)*\n\n## Word Before \n\nBriefly, this is my firt official Notebook posted on Kaggle. Although I've been a user of this platform for 2 years now, searching for data, reading notebooks, browsing through users and competitions, I've never participated myself. But this platform has helped me tremendously during my studies and has been a fascinating digital refuge for me, in search of amazing things us humans have learned to do. It is why I also wish to start contributing more to this industry and its community. I will try to make my submisions as thorough as possible, so that others may learn from me, as I have also done. Also, without wanting to seem umprofesional, I would like to keep an easy-going, reader-friendly language and let you be the judges of that. \n\n\n## Objectives, tasks and questions\n\nAs per this <a href=\"https:\/\/en.wikipedia.org\/wiki\/Churn_rate\">Wikipedia Article<\/a>, churn \"is a measure of the number of individuals or items moving out of a collective group over a specific period. The general purpose or end-goal of this notebook is to predict if a bank's customer will churn, by making use of Supervised Machine Learning methods. I chose this dataset because I believe it to be an everyday industrial use of these technologies and techniques.Obviously, the dataset is very basic and real corporations from the financial sector probably have a much more thorough and extensive 'case file' on their clientele. But this data will do just fine. Below, I am going to enuntiate a  \n\n\n### Step 1: Data Mining\n- Check database for duplicates and missing values.\n- Check data types.\n- Check dispersion of data\n- Check Pearson's Correlation Matrix \n- Detect outliers \n\n### Step 2: Data analysis \n- What is the range of values for credit scores? \n- Where do our customers come from? \n- What is the gender distribution?\n- Our newest and oldest customers?\n- Active \/ inactive distribution. \n- Salary in relation to the balance. \n- Plot the results! \n- Come up up interesting numbers?\n- What are the actionable insights?\n\n### Step 3: Machine Learning:\n- Do we have enough data? Is there some way to use data augmentation?\n- Is this a Regression or a Classification problem?\n- Candidates for y: Exited (binary classification) & Credit Score (regression) & Gender (binary classification)\n- Candidates for algorithms: XGBoost & ANN.\n\n\n# I. Importing libraries\n\nHere we have quite a lot of imports to take in, but they are (I hope) neatly organized.","55103899":"### Hyperparameter choice \n\nI used a grid of hyperparameter combinations as input for a Randomized Grid Search with Cross Validation, which is faster and not as exhaustive as a full-on grid search strategy. Even so, it is still fairly slow but allows us to more easily come up with ideal hyperparameter choices. ","c76204ac":"### Plotting the feature importances \n\nRandom Forests and XGBoost both come equipped with these feature importance lists, which tells us which variables had the greatest predictive power. \n\nIt seems to me that the assumptions made by the uploader of this dataset were proven true. The country is important. But not as much as the gender of the client. Activity is the most important predictor."}}