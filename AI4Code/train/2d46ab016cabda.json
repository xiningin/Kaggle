{"cell_type":{"f4e5d46a":"code","c5281d09":"code","fdea1379":"code","435376b8":"code","6cc7e2c3":"code","968f3a12":"code","03b962fd":"code","dedf22a7":"code","a7244f5c":"code","d0950757":"code","86a7ee02":"code","fff6b142":"code","6ba19041":"code","6c29d144":"code","726e7d13":"code","d702a3f3":"code","078cec30":"code","05f4abde":"code","0a5d2e9e":"code","9be0a1cd":"code","16e180d8":"code","e66358de":"code","ef88ee5a":"code","d066316b":"code","8ba49aff":"code","c1fea5c7":"code","dae39ca7":"code","a67ebfbe":"code","65627789":"code","944f96ff":"code","59eb083b":"code","1ecc0991":"code","e930cfa8":"code","c9190c05":"code","e3c87e49":"markdown","43cb34b9":"markdown","bcdafa50":"markdown"},"source":{"f4e5d46a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport copy\nimport warnings\nimport graphviz\nfrom sklearn import tree\nwarnings.filterwarnings(\"ignore\") \nplt.style.use(\"bmh\")","c5281d09":"df = pd.read_csv(\"\/kaggle\/input\/predict-test-scores-of-students\/test_scores.csv\")","fdea1379":"df.head()","435376b8":"fig, axs = plt.subplots(2,2,figsize=(12,7))\nplt.suptitle(\"Distribution of posttest\")\nsns.kdeplot(data=df, x=\"posttest\", hue=\"gender\", ax=axs[0,0])\nsns.kdeplot(data=df, x=\"posttest\", hue=\"school_type\", ax=axs[0,1])\nsns.kdeplot(data=df, x=\"posttest\", hue=\"lunch\", ax=axs[1,0])\nsns.kdeplot(data=df, x=\"posttest\", hue=\"teaching_method\", ax=axs[1,1]);","6cc7e2c3":"sns.kdeplot(data=df, x=\"posttest\", hue=\"school_setting\");","968f3a12":"n_students = df.n_student.unique()\nn_students_mean_score = [df[df.n_student==x][\"posttest\"].mean() for x in n_students]\nplt.figure(figsize=(12,5))\nplt.title(\"Mean posttest score by n_student\")\nsns.barplot(x=n_students, y=n_students_mean_score)\nplt.ylim(45, 85);","03b962fd":"plt.figure(figsize=(12,5))\nsns.scatterplot(data=df, x=\"n_student\", y=\"posttest\");","dedf22a7":"plt.figure(figsize=(12,5))\nsns.scatterplot(data=df, x=\"posttest\", y=\"pretest\");","a7244f5c":"df.corr(method=\"spearman\")","d0950757":"from sklearn.preprocessing import OneHotEncoder","86a7ee02":"target_columns = [\"school_setting\",\"school_type\",\"teaching_method\",\"gender\",\"lunch\"]\nnew_column_names = []\nfor target_column in target_columns:\n    print(f\"{target_column} has {len(df[target_column].unique())} unique values: {df[target_column].unique()}\")\n    for x in range(len(df[target_column].unique())):\n        new_column_names.append(f\"{target_column}_{x}\")\nmyEncoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\nprint(\"New column names:\",new_column_names)\nmyEncoder.fit(df[target_columns])\ndf_Xy = pd.concat([df.drop(target_columns+[\"school\", \"classroom\", \"student_id\"], 1), pd.DataFrame(myEncoder.transform(df[target_columns]))], axis=1).reindex()\nj = 0\nfor i, column in enumerate(df_Xy.columns):\n    if type(column)==int:\n        df_Xy[column] = df_Xy[column].astype(\"int64\")\n        df_Xy.rename(columns={column: new_column_names[j]}, inplace=True)\n        j += 1","fff6b142":"df_Xy.head()","6ba19041":"#X = df_Xy[[\"n_student\", \"pretest\", \"school_setting_0\", \"school_setting_1\", \"school_setting_2\", \"school_type_0\", \"school_type_1\", \"teaching_method_0\", \"teaching_method_1\", \"gender_0\", \"gender_1\", \"lunch_0\", \"lunch_1\"]].to_numpy()\n# Removing \"pretest\" from our feature list\nX = df_Xy[[\"n_student\", \"school_setting_0\", \"school_setting_1\", \"school_setting_2\", \"school_type_0\", \"school_type_1\", \"teaching_method_0\", \"teaching_method_1\", \"gender_0\", \"gender_1\", \"lunch_0\", \"lunch_1\"]].to_numpy()\ny = df_Xy[[\"posttest\"]].to_numpy()\nprint(\"Shape of X:\", X.shape)\nprint(\"Shape of y:\", y.shape)","6c29d144":"from sklearn.model_selection import train_test_split","726e7d13":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","d702a3f3":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error","078cec30":"reg_dt = DecisionTreeRegressor(max_depth=3)\nreg_dt.fit(X_train, y_train)\ny_hat_dt = reg_dt.predict(X_test)","05f4abde":"print(\"Mean squared error:\", mean_squared_error(y_test, y_hat_dt))\nprint(\"Mean absolute error:\", mean_absolute_error(y_test, y_hat_dt))\nprint(\"Median squared error:\", median_absolute_error(y_test, y_hat_dt))","0a5d2e9e":"y_test_comp = np.ravel(y_test[-10:])\ny_hat_comp_dt = y_hat_dt[-10:]\nplt.figure(figsize=(12,5))\nplt.title(\"Comparison of some samples for decision trees\")\nsns.scatterplot(x=[x for x in range(len(y_test_comp))], y=y_test_comp, marker=\"_\", s=300)\nsns.scatterplot(x=[x for x in range(len(y_test_comp))], y=y_hat_comp_dt, marker=\"_\", s=300);","9be0a1cd":"dot_data = tree.export_graphviz(reg_dt, out_file=None, feature_names=[\"n_student\", \"school_setting_0\", \"school_setting_1\", \"school_setting_2\", \"school_type_0\", \"school_type_1\", \"teaching_method_0\", \"teaching_method_1\", \"gender_0\", \"gender_1\", \"lunch_0\", \"lunch_1\"], filled=True, rounded=True,  special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","16e180d8":"max_depths = [x for x in range(2,11)]\ntrain_score = []\ntest_score = []\nfor max_depth in max_depths:\n    reg_dt = DecisionTreeRegressor(max_depth=max_depth)\n    reg_dt.fit(X_train, y_train)\n    y_train_dt = reg_dt.predict(X_train)\n    y_test_dt = reg_dt.predict(X_test)\n    train_score.append(mean_squared_error(y_train, y_train_dt))\n    test_score.append(mean_squared_error(y_test, y_test_dt))","e66358de":"plt.figure(figsize=(14,4))\nplt.title(\"max_depth: MSE for train vs. test data\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"MSE\")\nsns.scatterplot(x=max_depths, y=train_score, marker=\"_\", s=300)\nsns.scatterplot(x=max_depths, y=test_score, marker=\"_\", s=300);","ef88ee5a":"from sklearn.ensemble import RandomForestRegressor","d066316b":"reg_rf = RandomForestRegressor(n_estimators=30, max_depth=3)\nreg_rf.fit(X_train, np.ravel(y_train))\ny_hat_rf = reg_rf.predict(X_test)","8ba49aff":"print(\"Mean squared error:\", mean_squared_error(y_test, y_hat_rf))\nprint(\"Mean absolute error:\", mean_absolute_error(y_test, y_hat_rf))\nprint(\"Median squared error:\", median_absolute_error(y_test, y_hat_rf))","c1fea5c7":"y_hat_comp_rf = y_hat_rf[-10:]\nplt.figure(figsize=(12,5))\nplt.title(\"Comparison of some samples for random forests\")\nsns.scatterplot(x=[x for x in range(len(y_test_comp))], y=y_test_comp, marker=\"_\", s=300)\nsns.scatterplot(x=[x for x in range(len(y_test_comp))], y=y_hat_comp_rf, marker=\"_\", s=300);","dae39ca7":"from sklearn.ensemble import GradientBoostingRegressor","a67ebfbe":"reg_gb = GradientBoostingRegressor(n_estimators=30, max_depth=3)\nreg_gb.fit(X_train, np.ravel(y_train))\ny_hat_gb = reg_gb.predict(X_test)","65627789":"print(\"Mean squared error:\", mean_squared_error(y_test, y_hat_gb))\nprint(\"Mean absolute error:\", mean_absolute_error(y_test, y_hat_gb))\nprint(\"Median squared error:\", median_absolute_error(y_test, y_hat_gb))","944f96ff":"y_hat_comp_gb = y_hat_gb[-10:]\nplt.figure(figsize=(12,5))\nplt.title(\"Comparison of some samples for gradient boosted trees\")\nsns.scatterplot(x=[x for x in range(len(y_test_comp))], y=y_test_comp, marker=\"_\", s=300)\nsns.scatterplot(x=[x for x in range(len(y_test_comp))], y=y_hat_comp_gb, marker=\"_\", s=300);","59eb083b":"from sklearn.ensemble import AdaBoostRegressor","1ecc0991":"reg_ab = AdaBoostRegressor(n_estimators=30)\nreg_ab.fit(X_train, np.ravel(y_train))\ny_hat_ab = reg_ab.predict(X_test)","e930cfa8":"print(\"Mean squared error:\", mean_squared_error(y_test, y_hat_ab))\nprint(\"Mean absolute error:\", mean_absolute_error(y_test, y_hat_ab))\nprint(\"Median squared error:\", median_absolute_error(y_test, y_hat_ab))","c9190c05":"y_hat_comp_ab = y_hat_ab[-10:]\nplt.figure(figsize=(12,5))\nplt.title(\"Comparison of some samples for ada gradient boosted trees\")\nsns.scatterplot(x=[x for x in range(len(y_test_comp))], y=y_test_comp, marker=\"_\", s=300)\nsns.scatterplot(x=[x for x in range(len(y_test_comp))], y=y_hat_comp_ab, marker=\"_\", s=300);","e3c87e49":"First let's break down what the features mean:\n\nschool_setting has 3 unique values:  \"Urban\", \"Suburban\", \"Rural\"<br>\nschool_type has 2 unique values: \"Non-public\", \"Public\"<br>\nteaching_method has 2 unique values: \"Standard\", \"Experimental\"<br>\ngender has 2 unique values: \"Female\", \"Male\"<br>\nlunch has 2 unique values: \"Does not qualify\", \"Qualifies for reduced\/free lunch\"\n\nAs all features are encoded using \"one hot encoding\" the feature \"school_seeting_1\" represents \"Suburban\" with the possible values 0 = False and 1 = True\n\nOne could expect the highest score if:\n* Does **not** qualify for reduced\/free lunch\n* school_setting: Suburban\n* n_student: <=26\n\nOther interesting influences:\n* school_type \"Public\" dramatically reduces the expected score\n* Fewer students per class = Higher score","43cb34b9":"Let's see when the decision tree model starts to overfit...","bcdafa50":"\"prestest\" and \"posttest\" are highly correllated. That means that a linear regression on \"pretest\" alone would be able to predict \"posttest\" with good precision.<br>\nIf we leave \"prestest\" in the dataset many (if not all) tree based algorithms would do exactly that - depend only on \"pretest\".<br>\nAs this is boring and we want to learn more about the influences of the other features we'll remove that feature before starting to fit our models."}}