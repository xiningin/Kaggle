{"cell_type":{"43740166":"code","163aab71":"code","b7f56a46":"code","8448e6d2":"code","aa87129c":"code","b8eb595f":"code","f5291bbb":"code","06303ef7":"code","f82ad0bf":"code","5b2c9c01":"code","26bd9ba2":"code","d7de8fc9":"code","96a5f9d6":"code","1d1f4bff":"code","97a6aaec":"code","8cfa9534":"code","c9c10732":"code","d96c8405":"code","7bd67e5a":"code","76bf3241":"code","cc4bf3a1":"code","a7d683d9":"code","523a89c5":"code","8b69ef74":"code","80a15016":"code","b317d736":"code","9d838f0b":"code","85c5c740":"code","5f5dffda":"code","52de5725":"code","b1c212e2":"code","d1a70d63":"code","a65e63a2":"code","7e030531":"code","15bcaed0":"code","61c450b3":"code","ada89b3c":"code","daf51e0c":"markdown","f69efd6a":"markdown","acdf20be":"markdown","05985031":"markdown","fd14e829":"markdown","3fc6ab68":"markdown","49af95f4":"markdown","78fc0670":"markdown","9f040778":"markdown","bb4a1a22":"markdown","223abfbb":"markdown","c9d33429":"markdown","ff83c60d":"markdown","48c164e0":"markdown","cf85bcd8":"markdown","40222f42":"markdown","07e79781":"markdown","8a139b50":"markdown","e90bfd12":"markdown","adb1d725":"markdown"},"source":{"43740166":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","163aab71":"df = pd.read_csv('\/kaggle\/input\/housesalesprediction\/kc_house_data.csv')\ndf.head()","b7f56a46":"df.isna().sum()","8448e6d2":"df.dtypes","aa87129c":"import matplotlib.pyplot as plt\ndf.hist(figsize=(8,6));","b8eb595f":"from sklearn.preprocessing import StandardScaler","f5291bbb":"## scale\nnum_cols = [c for c in df.columns if df[c].dtypes!='object' and c!='price']\nsc = StandardScaler()\nnum_col_scale = sc.fit_transform(df[num_cols])\n\n## get covariate matrix\ncov_matrix = np.cov(num_col_scale.T)\n\n## eigen value decomposition\neigval, eigvec  = np.linalg.eig(cov_matrix)\n\n## calculate explained variance\nexplained_var = eigval \/ np.sum(eigval)\n\n## plot each component\nplt.plot(np.arange(len(explained_var)), np.cumsum(explained_var), 'bo')","06303ef7":"eigval","f82ad0bf":"## sort the eigen values from the largest to the smallest\nn_components = 2\nidx = eigval.argsort()[::-1]\neigval_sorted = eigval[idx][:2]\neigvec_sorted = np.atleast_1d(eigvec[:, idx])[:, :n_components]","5b2c9c01":"## transform all the num_col\nnum_col_scale.dot(eigval_sorted.T[0]).shape","26bd9ba2":"## get the dot product\npc1 = num_col_scale.dot(eigvec_sorted.T[0])\npc2 = num_col_scale.dot(eigvec_sorted.T[1])\n\nimport seaborn as sns\nsns.scatterplot(pc1, pc2, hue=df.price);","d7de8fc9":"from sklearn.metrics import mean_squared_error\n\n\ndef gradient_descent(X, y, lr=0.001, epoch=20):\n    m, b= 0.3, 0.4\n    log, mse = [], []\n    \n    N = len(X)\n    \n    for _ in range(epoch):\n        f = y-(m*X + b)\n        \n        b -= lr*(-2*f.sum()\/N)\n        \n        m -= lr*(-2*X.dot(f).sum()\/N)\n        \n        log.append((m, b))\n        \n        mse.append(mean_squared_error(y, (m*X + b)))\n        \n    return m, b, log, mse","96a5f9d6":"X = [np.random.randint(1,10,1) for _ in range(10)]\nX = np.asarray(X).reshape(-1)\ny = 0.44*X + 0.6","1d1f4bff":"plt.plot(X, y, 'ro')","97a6aaec":"m, b, log, mse = gradient_descent(X, y, epoch=30)","8cfa9534":"plt.plot(np.arange(30), [i[0] for i in log])\nplt.xlabel('epoch')\nplt.ylabel('M value');","c9c10732":"plt.plot(np.arange(30), [i[1] for i in log])\nplt.ylabel('b value');","d96c8405":"plt.plot(np.arange(30), mse)\nplt.ylabel('MSE');","7bd67e5a":"df.columns","76bf3241":"plt.plot(df.lat, df.long, 'ro');","cc4bf3a1":"class KMeans:\n    \n    def __init__(self, k, tol= 0.0001, max_iter = 200):\n        self.k = k\n        self.tol = tol\n        self.max_iter = max_iter\n        \n    def fit(self, X, ):\n        \n        self.centroids = {}\n        \n        for i in range(self.k):\n            self.centroids[i] = X[random.choice(len(X))]\n            \n        for i in range(self.max_iter):\n            self.classes = {}\n            for j in range(self.k):\n                self.classes[j] = []\n                \n            for feature in X:\n                distances = [np.linalg.norm(feature - self.centroids[c]) for c in self.controids]\n                classes = distances.index(min(distances))\n            \n                self.classes[classes].append(feature)\n                \n            prev_centroids = dict(self.centroids)\n            \n            for class_ in self.classes:\n                self.centroids[class_] = np.average(self.classes[class_], axis=0)\n            \n            optimized = False\n            for c in self.centroids:\n                original_c = prev_centroids[c]\n                current_c = self.centroids[c]\n                \n                if np.sum((current_c - original_c)\/original_c*100)> self.tol:\n                    print(np.sum((current_c -original_c )\/original_c *100.0))\n                    optimized = False\n                    \n                    \n            if optimized:\n                break\n                \n    def predict(test_X):\n        distances = [np.linalg.norm(test_X - self.centroids[c]) for c in self.centroids]\n        \n        classes = distances.index(min(distances))\n        return classes","a7d683d9":"## this example is from : https:\/\/chrisalbon.com\/code\/machine_learning\/naive_bayes\/naive_bayes_classifier_from_scratch\/\nimport pandas as pd\n\ndf = pd.read_csv('\/kaggle\/input\/housesalesprediction\/kc_house_data.csv')","523a89c5":"df_nb = df[['bedrooms', 'floors', 'sqft_living', 'grade']]","8b69ef74":"## for simplicity we use two claases\ndf_nb['grade'] = df_nb.grade.apply(lambda x: True if x > 8 else False)","80a15016":"df_nb.dtypes","b317d736":"\ncondition_cols = df_nb.columns[:5]\ncondition_cols\n","9d838f0b":"priors = pd.DataFrame(df_nb.groupby('grade')['bedrooms'].count()\/df_nb.shape[0])","85c5c740":"priors","5f5dffda":"## to get the mean\ndf_mean = df_nb.groupby('grade').mean()\ndf_mean","52de5725":"df_variance = df_nb.groupby('grade').var()\ndf_variance","b1c212e2":"## now we calculate the mean and variance for each condition\n\n## mean for grade=True\nbed_true_mean = df_mean['bedrooms'][df_variance.index==True].values[0]\nfloor_true_mean = df_mean['floors'][df_variance.index==True].values[0]\nsqft_true_mean = df_mean['sqft_living'][df_variance.index==True].values[0]\n\n## mean for grade=False\nbed_false_mean = df_mean['bedrooms'][df_variance.index==False].values[0]\nfloor_false_mean = df_mean['floors'][df_variance.index==False].values[0]\nsqft_false_mean = df_mean['sqft_living'][df_variance.index==False].values[0]\n\n## var for grade=True\nbed_true_var = df_variance['bedrooms'][df_variance.index==True].values[0]\nfloor_true_var = df_variance['floors'][df_variance.index==True].values[0]\nsqft_true_var = df_variance['sqft_living'][df_variance.index==True].values[0]\n\n## var for grade = False\nbed_false_var = df_variance['bedrooms'][df_variance.index==False].values[0]\nfloor_false_var = df_variance['floors'][df_variance.index==False].values[0]\nsqft_false_var = df_variance['sqft_living'][df_variance.index==False].values[0]\n","d1a70d63":"import numpy as np\n# Create a function that calculates p(x | y):\ndef p_x_given_y(x, mean_y, variance_y):\n\n    # Input the arguments into a probability density function\n    p = 1\/(np.sqrt(2*np.pi*variance_y)) * np.exp((-(x-mean_y)**2)\/(2*variance_y))\n    \n    # return p\n    return p","a65e63a2":"df_nb.head(1)","7e030531":"df_nb.head(1)['bedrooms'].values[0]","15bcaed0":"## numerator of posterior if classified as True\n\npriors[priors.index==True].values[0][0] * \\\np_x_given_y(df_nb.head(1)['bedrooms'].values[0], bed_true_mean, bed_true_var) *\\\np_x_given_y(df_nb.head(1)['floors'].values[0], floor_true_mean, floor_true_var) *\\\np_x_given_y(df_nb.head(1)['sqft_living'].values[0],sqft_true_mean, sqft_true_var)","61c450b3":"## numerator of posterior if classified as false\n\npriors[priors.index==False].values[0][0] * \\\np_x_given_y(df_nb.head(1)['bedrooms'].values[0], bed_false_mean, bed_false_var) *\\\np_x_given_y(df_nb.head(1)['floors'].values[0], floor_false_mean, floor_false_var) *\\\np_x_given_y(df_nb.head(1)['sqft_living'].values[0],sqft_false_mean, sqft_false_var)","ada89b3c":"  \n# from __future__ import print_function, division\n# import numpy as np\n# from mlfromscratch.utils import euclidean_distance\n\n\n# class KNN:\n#     def __init__(self, k):\n#         self.k = k\n#     def _vote( self, neighbour_labels):\n#         counts = np.bincount(neighbour_labels.astype('int'))\n        \n#         return counts.argmax()\n    \n    \n#     def predict(self, X_train, X_test, y_train):\n        \n#         ## initialize\n        \n#         y_pred = np.empty(X_test.shape[0])\n        \n        \n#         ## get the neighbour label\n        \n#         for i, v in enumerate(X_test):\n#             ## sort the training samples by their distances\n#             idx = np.argsort([euclidean_distance(v, x) for x in X_train])[self.k:]\n            \n#             ## Extract the labels \n#             k_nearest = np.array([y_train[i] for i in idx])\n            \n#             y_pred[i] = self._vote(k_nearest)\n            \n#         return y_pred\n            \n            ","daf51e0c":"<a id='tag2'><\/a>\n## Gradient descent ","f69efd6a":"<a id='tag1'><\/a>\n## PCA from Scratch","acdf20be":"* For each data point, find the classes for the closesest N neighbours\n* Using majority vote to determine the classes","05985031":"$p(grade|condition) \\propto p(grade) p(condition|grade)$","fd14e829":"$p(grade)$ is prior ","3fc6ab68":"\n\n$$ p (grade| condition) = \\frac{p(grade)p(condition|grade)}{p(condition)}$$\n\n### calculate prior","49af95f4":"These are the gradients\n\nand to update m and b\n\n$m := m - \\lambda \\frac{ \\partial{f}}{\\partial{m}}$\n\n$b := b- \\lambda \\frac{\\partial{f}}{\\partial{b}}$","78fc0670":"Reference: https:\/\/towardsdatascience.com\/gradient-descent-from-scratch-e8b75fa986cc","9f040778":"Taking partial derivatives of m and b from the loss function of OLS:\n\n$$\\frac{\\partial f}{\\partial b} = \\frac{1}{n}\\Sigma^n_{i=1}[-2(y_i-(mx_i+b))]$$\n$$\\frac{\\partial f}{\\partial m} = \\frac{1}{n}\\Sigma^n_{i=1}[-2x_i(y_i-(mx+b))]$$","bb4a1a22":"To get the likehood $p(condition|grade)$, we first assume that each feature distributed normally, so that using the normal distribution pdf we get \n\nhttps:\/\/en.wikipedia.org\/wiki\/Normal_distribution\n\n$$f(x) = -\\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\theta})^2}$$","223abfbb":"Therefore, the $p(condition|grade=F)$ is higher than $p(condition|grade=T)$","c9d33429":"# TOC\n\n[PCA using numpy](#tag1)\n\n[Gradient descent](#tag2)\n\n[K-means](#tag3)\n\n[Naive Bayes](#tag4)\n\n[KNN](#tag5)","ff83c60d":"[ref1](https:\/\/github.com\/eriklindernoren\/ML-From-Scratch\/blob\/master\/mlfromscratch\/supervised_learning\/naive_bayes.py)\n\n[ref2](https:\/\/github.com\/tigju\/Naive-Bayes-Classifier-from-scratch\/blob\/main\/naive_bayes.ipynb)\n\n[ref3](https:\/\/chrisalbon.com\/code\/machine_learning\/naive_bayes\/naive_bayes_classifier_from_scratch\/)","48c164e0":"Reference https:\/\/medium.com\/@rishit.dagli\/build-k-means-from-scratch-in-python-e46bf68aa875","cf85bcd8":"<a id='tag4'><\/a>\n\n## Naive Bayes Classifier","40222f42":"* Scale the matrix\n* Get covariate matrix\n* Singular Value Decomposition from the cov_matrix \n* Sort and find the n largest eigen vector and corresponding eigen values\n* Get the dot product of the scaled metrix with the eigen vector ","07e79781":"### Putting together \n\nSince the denominator of the equation $p(condition)$ is the marginal probability, and sometimes we can ignore the denominator and assume that the posterior is proportional to the denominator, thus","8a139b50":"### Likelihood","e90bfd12":"<a id='tag5'><\/a>\n\n## KNN\n","adb1d725":"<a id='tag3'><\/a>\n## K means"}}