{"cell_type":{"0d81bcb5":"code","461641d5":"code","f5f9ba17":"code","7d361450":"code","7e725470":"code","ab49e5cd":"code","73ef3319":"code","77028a4e":"code","055f41ff":"code","b70fdf68":"code","75648958":"code","dbde14bc":"code","2b345d65":"code","94a98cbf":"code","067f6fa0":"code","ce6d8bd7":"code","f376ee0f":"code","f743e0cc":"code","e35c0fdf":"code","6f2077a6":"code","aea325f6":"code","bacfe479":"markdown"},"source":{"0d81bcb5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","461641d5":"train_t = pd.read_csv('\/kaggle\/input\/language-translation-englishfrench\/eng_-french.csv')","f5f9ba17":"train_t.columns\n","7d361450":"df = train_t[0:100000]\ndf","7e725470":"english_text = df['English words\/sentences']\nfrench_text = df['French words\/sentences']","ab49e5cd":"import re\nenglish = []\nfrench = []\nfor i in range(len(english_text)):\n    text = english_text[i].lower()\n    text = re.sub('[^a-zA-Z]',' ',text)\n    english.append(text)\n    \n\nfor i in range(len(french_text)):\n    ftext = french_text[i].lower()\n    ftext = (re.sub(\"[^a-zA-Z' \u00e0\u00e2\u00e4\u00e8\u00e9\u00ea\u00eb\u00ee\u00ef\u00f4\u0153\u00f9\u00fb\u00fc\u00ff\u00e7\u00c0\u00c2\u00c4\u00c8\u00c9\u00ca\u00cb\u00ce\u00cf\u00d4\u0152\u00d9\u00db\u00dc\u0178\u00c7]\",' ',ftext))\n    french.append(\"START_ \" + ftext + \" _END\")\n","73ef3319":"french","77028a4e":"#Vocabulary of English\nall_eng_words = set()\nfor i in english:\n    for j in i.split():\n        all_eng_words.add(j)\n\n#vocabulary of french\nall_fre_words = set()\nfor i in french:\n    for j in i.split():\n        all_fre_words.add(j)\n\n#maxlen of the source sequence\nmax_length_src = 0\nfor i in english:\n    a = len(i.split())\n    if a>max_length_src:\n        max_length_src = a\n        \n#maxlen of the target sequence\nmax_length_tar = 0\nfor j in french:\n    b = len(j.split())\n    if b>max_length_tar:\n        max_length_tar = b\n        \n\ninput_words = sorted(list(all_eng_words))\ntarget_words = sorted(list(all_fre_words))\n\n# Calculate Vocab size for both source and targe\nnum_encoder_tokens = len(all_eng_words)\nnum_decoder_tokens = len(all_fre_words)\n\n\n#indexs for input and target sequences\ninput_index = dict([(words,i) for i,words in enumerate(input_words)])\ntarget_index = dict([(word, i) for i, word in enumerate(target_words)])\n\nreverse_input_index = dict((i, word) for word, i in input_index.items())\nreverse_target_index = dict((i, word) for word, i in target_index.items())","055f41ff":"print(max_length_src)\nprint(max_length_tar)\nprint(num_encoder_tokens)\nprint(num_decoder_tokens)","b70fdf68":"encoder_input_data = np.zeros((100000, max_length_src, num_encoder_tokens),dtype='float32')\ndecoder_input_data = np.zeros((100000, max_length_tar, num_decoder_tokens),dtype='float32')\ndecoder_target_data = np.zeros((100000, max_length_tar, num_decoder_tokens),dtype='float32')","75648958":"for j in range(100000):\n    for i,text in enumerate(english[j].split()):\n        encoder_input_data[j,i,input_index[text]] = 1.\n\nfor j in range(100000):\n    for i,text in enumerate(french[j].split()):\n        decoder_input_data[j,i,target_index[text]] = 1.\n        if i>0:\n            decoder_target_data[j,i-1,target_index[text]] = 1.","dbde14bc":"import keras, tensorflow\nfrom keras.models import Model\nfrom keras.layers import Input, LSTM, Dense, Bidirectional","2b345d65":"batch_size = 64\nepochs = 100\nlatent_dim = 256 #size of the lstms hidden state","94a98cbf":"#Time to bulid the model\n\n#inputs for the encoder\nencoder_inputs = Input(shape=(None,num_encoder_tokens))\n#encoder lstm\nencod_lstm = (LSTM(latent_dim,return_state = True))\nencoder_output,state_h,state_c = encod_lstm(encoder_inputs)\n\n#hidden from encoder to pass to the decoder as initial hidden state\nencoder_states = [state_h,state_c]\n\n#inputs for the decoder\ndecoder_inputs = Input(shape=(None,num_decoder_tokens))\n#decoder lstm \ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_output,_,_= decoder_lstm(decoder_inputs,initial_state = encoder_states)\n#The decoder output is passed through the softmax layer that will learn to classify the correct french character\n#Activation functions are used to transform vectors before computing the loss in the training phase\n#for more on softmax https:\/\/gombru.github.io\/2018\/05\/23\/cross_entropy_loss\/\ndense_layer = Dense(num_decoder_tokens, activation='softmax')\ndecoder_output = dense_layer(decoder_output)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = Model([encoder_inputs, decoder_inputs], decoder_output)\n\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.summary()","067f6fa0":"from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model.png', show_shapes=True)","ce6d8bd7":"model.fit([encoder_input_data,decoder_input_data],decoder_target_data,batch_size= 64,epochs= 50,validation_split=0.2)","f376ee0f":"encoder_model = Model(encoder_inputs,encoder_states)\n\ndecoder_state_h = Input(shape=(latent_dim,))\ndecoder_state_c = Input(shape=(latent_dim,))\ndecode_state = [decoder_state_h,decoder_state_c]\n\ndecoder_outputs,state_h,state_c = decoder_lstm(decoder_inputs,initial_state = decode_state)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = dense_layer(decoder_outputs)\n\ndecoder_model = Model([decoder_inputs] + decode_state,[decoder_outputs] + decoder_states)","f743e0cc":"from keras.utils.vis_utils import plot_model\nplot_model(decoder_model, to_file='model.png', show_shapes=True)","e35c0fdf":"def decode_sequence(input_seq):\n    # encode the input sequence to get the internal state vectors.\n    states_value = encoder_model.predict(input_seq)\n  \n    # generate empty target sequence of length 1 with only the start character\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    target_seq[0, 0, target_index['START_']] = 1.\n  \n    # output sequence loop\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n    \n        # sample a token and add the corresponding character to the \n        # decoded sequence\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_index[sampled_token_index]\n        \n        if (sampled_char == \"_END\" or len(decoded_sentence) > max_length_tar):\n            stop_condition = True\n            break\n            \n        decoded_sentence += sampled_char\n        decoded_sentence +=' '\n      \n        # update the target sequence (length 1).\n        target_seq = np.zeros((1, 1, num_decoder_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.\n    \n        # update states\n        states_value = [h, c]\n    \n    return decoded_sentence\n            ","6f2077a6":"toks = ['i love you','run fast','she is the client','my name is tom']\nfor t in toks:\n    input_sentence = t\n    test_sentence_tokenized = np.zeros((1, max_length_src, num_encoder_tokens), dtype='float32')\n    for t, char in enumerate(input_sentence.split()):\n        test_sentence_tokenized[0, t, input_index[char]] = 1.\n    print(input_sentence)\n    print(decode_sequence(test_sentence_tokenized))\n    print(' ')","aea325f6":"#result je vous aime is i love you in english\n#result un fait vite is move fast in english\n#result elle est dans le tu is she\u2019s in the you \n#result mon nom est tom is my name is tom","bacfe479":"https:\/\/miro.medium.com\/max\/3240\/1*1I2tTjCkMHlQ-r73eRn4ZQ.png"}}