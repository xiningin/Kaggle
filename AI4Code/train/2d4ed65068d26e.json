{"cell_type":{"2afd5ee3":"code","2c7bc18b":"code","8e979b6b":"code","a13bfe95":"code","71b39c16":"code","5c9cfd3a":"code","a52e77d8":"code","197852f5":"code","95a92b9d":"code","bfbc9b8f":"code","2b23a0b3":"code","4fea1996":"code","1b183336":"code","8bae3a3f":"code","3eb45508":"code","d09a6adb":"code","e9cb40f1":"code","e92a7bc1":"code","3255a6a9":"code","21cd414e":"code","852a7288":"code","8ff0abf6":"code","8fc2720a":"code","3aa183a4":"code","e12b629d":"code","57fcebe2":"code","f72907db":"code","f6c23acb":"code","c6d31315":"code","24f67c4d":"code","78d0770e":"code","8153e7c8":"code","f5655b2c":"code","b8321dd1":"code","3a422448":"code","13aeaacb":"code","dbc3532a":"code","10795083":"code","b3a83efb":"code","041ffb45":"code","d3e34591":"code","1d96e590":"code","c13a04c4":"code","cd47807b":"code","8cbfcd78":"code","2804fe14":"code","5cb215d4":"code","f9f88064":"code","f46a1d24":"code","730aa3b5":"code","ed3b74ad":"code","942d4733":"code","5e8f5656":"code","6c45901c":"code","2a93f8b0":"code","00453d8d":"code","cd5cdafb":"code","086f1b8e":"markdown","54836da9":"markdown","795117b0":"markdown","758b0caa":"markdown","06bdef70":"markdown","940fac10":"markdown","5e5ea58f":"markdown","6bd5b68d":"markdown","8e0064ef":"markdown","eefba6c0":"markdown","7fb87e54":"markdown","27f0bbea":"markdown","ff11ef77":"markdown","24383c30":"markdown","882b9eb5":"markdown","e4ec2794":"markdown","6b34d058":"markdown","e93b19b0":"markdown","53ce0315":"markdown"},"source":{"2afd5ee3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc, math\n\nimport matplotlib.gridspec as gridspec # to do the grid of plots\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2c7bc18b":"sns.set(rc={'figure.figsize':(11,8)})\nsns.set(style=\"whitegrid\")","8e979b6b":"train_df = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/test.csv')\nsample_sub_df = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/sample_submission.csv')","a13bfe95":"print (f'Shape of training data: {train_df.shape}')\nprint (f'Shape of testing data: {test_df.shape}')","71b39c16":"train_df.head()","5c9cfd3a":"test_df.head()","a52e77d8":"train_df.columns","197852f5":"train_df.dtypes","95a92b9d":"not train_df.isna().sum().values.any()","bfbc9b8f":"not test_df.isna().sum().values.any()","2b23a0b3":"## Function to reduce the memory usage\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","4fea1996":"train_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)","1b183336":"total = len(train_df)\n\nax = sns.barplot(pd.unique(train_df['target']), train_df['target'].value_counts())\nax.set(xlabel='Target Type', ylabel='# of records', title='Tsrget Distribution')\nplt.show()","8bae3a3f":"def bin_feature_transform(df):\n    feature_map = {\n        'T': 1,\n        'Y': 1,\n        'F': 0,\n        'N': 0\n    }\n    df['bin_3'] = df['bin_3'].map(feature_map)\n    df['bin_4'] = df['bin_4'].map(feature_map)\n    return df","3eb45508":"train_df = bin_feature_transform(train_df)\ntest_df = bin_feature_transform(test_df)","d09a6adb":"train_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)","e9cb40f1":"grid = gridspec.GridSpec(3, 2) # The grid of chart\nplt.figure(figsize=(16,20)) # size of figure\n\nbin_cols = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']\nval_num_map = {0: 0, 1: 1}\n\n\nfor n, col in enumerate(train_df[bin_cols]):\n    ax = plt.subplot(grid[n]) # feeding the figure of grid\n    ax = sns.barplot(np.vectorize(val_num_map.get)(pd.unique(train_df[col])), train_df[col].value_counts())\n    ax.set(xlabel=f'Feature: {col}', ylabel='# of records', title=f'Binary feature {n} vs. # of records')\n    sizes = []\n    for p in ax.patches: # loop to all objects\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\", fontsize=14) \n\n    \nplt.show()","e92a7bc1":"train_df.groupby('nom_9')['id'].nunique().shape","3255a6a9":"grid = gridspec.GridSpec(5, 5) # The grid of chart\nplt.figure(figsize=(30,30)) # size of figure\n\nnom_cols = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4']\nval_num_map = {0: 0, 1: 1}\n\nfor n, col in enumerate(train_df[nom_cols]):\n    ax = plt.subplot(grid[n]) # feeding the figure of grid\n    ax = sns.barplot(train_df.groupby(col)['id'].nunique().keys(), train_df.groupby(col)['id'].nunique())\n    ax.set(xlabel = f'Feature: {col}', ylabel='# of records', title=f'Nominal feature {n} vs. # of records')\n    sizes = []\n    for p in ax.patches: # loop to all objects\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\", fontsize=14) \n\n    \nplt.show()","21cd414e":"low_card_nom_cols=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4']\ntrain_df = pd.get_dummies(train_df, columns=low_card_nom_cols)\ntest_df = pd.get_dummies(test_df, columns=low_card_nom_cols)","852a7288":"train_df.head()","8ff0abf6":"train_df.columns","8fc2720a":"test_df.columns","3aa183a4":"print (train_df.shape)\nprint (test_df.shape)","e12b629d":"high_card_nom_cols = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\nfor col in high_card_nom_cols:\n    train_df[f'hash_{col}'] = train_df[col].apply(lambda x: hash(str(x))%5000)\n    test_df[f'hash_{col}'] = test_df[col].apply(lambda x: hash(str(x))%5000)","57fcebe2":"train_df = train_df.drop(high_card_nom_cols, axis=1)\ntest_df = test_df.drop(high_card_nom_cols, axis=1)","f72907db":"train_df.head()","f6c23acb":"ord_features = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']\ntrain_df[ord_features].head()","c6d31315":"train_df[ord_features].nunique()","24f67c4d":"grid = gridspec.GridSpec(2, 3) # The grid of chart\nplt.figure(figsize=(40,40)) # size of figure\n\nfor n, col in enumerate(train_df[ord_features[:-3]]):\n    ax = plt.subplot(grid[n]) # feeding the figure of grid\n    ax = sns.barplot(train_df.groupby(col)['id'].nunique().keys(), train_df.groupby(col)['id'].nunique())\n    ax.set(xlabel = f'Feature: {col}', ylabel='# of records', title=f'Ordinal feature {n} vs. # of records')\n    sizes = []\n    for p in ax.patches: # loop to all objects\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\", fontsize=30) \n\n    \nplt.show()","78d0770e":"# Importing categorical options of pandas\nfrom pandas.api.types import CategoricalDtype \n\n# seting the orders of our ordinal features\nord_1 = CategoricalDtype(categories=['Novice', 'Contributor','Expert', \n                                     'Master', 'Grandmaster'], ordered=True)\nord_2 = CategoricalDtype(categories=['Freezing', 'Cold', 'Warm', 'Hot',\n                                     'Boiling Hot', 'Lava Hot'], ordered=True)\nord_3 = CategoricalDtype(categories=['a', 'b', 'c', 'd', 'e', 'f', 'g',\n                                     'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'], ordered=True)\nord_4 = CategoricalDtype(categories=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n                                     'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n                                     'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], ordered=True)","8153e7c8":"# Transforming ordinal Features for train dataset\ntrain_df.ord_1 = train_df.ord_1.astype(ord_1)\ntrain_df.ord_2 = train_df.ord_2.astype(ord_2)\ntrain_df.ord_3 = train_df.ord_3.astype(ord_3)\ntrain_df.ord_4 = train_df.ord_4.astype(ord_4)\n\n# Same test dataset\ntest_df.ord_1 = test_df.ord_1.astype(ord_1)\ntest_df.ord_2 = test_df.ord_2.astype(ord_2)\ntest_df.ord_3 = test_df.ord_3.astype(ord_3)\ntest_df.ord_4 = test_df.ord_4.astype(ord_4)","f5655b2c":"train_df['ord_1'].head()","b8321dd1":"# Geting the codes of ordinal categoy's - train\ntrain_df.ord_1 = train_df.ord_1.cat.codes\ntrain_df.ord_2 = train_df.ord_2.cat.codes\ntrain_df.ord_3 = train_df.ord_3.cat.codes\ntrain_df.ord_4 = train_df.ord_4.cat.codes\n\n# Geting the codes of ordinal categoy's - test\ntest_df.ord_1 = test_df.ord_1.cat.codes\ntest_df.ord_2 = test_df.ord_2.cat.codes\ntest_df.ord_3 = test_df.ord_3.cat.codes\ntest_df.ord_4 = test_df.ord_4.cat.codes","3a422448":"train_df.head()","13aeaacb":"from sklearn.preprocessing import OrdinalEncoder\n\noe = OrdinalEncoder()\noe.fit(train_df['ord_5'].values.reshape(-1, 1))\noe.categories_","dbc3532a":"encoded_train = oe.transform(train_df['ord_5'].values.reshape(-1, 1))\nencoded_test = oe.transform(test_df['ord_5'].values.reshape(-1, 1))","10795083":"train_df","b3a83efb":"train_df['ord_5'] = encoded_train\ntest_df['ord_5'] = encoded_test","041ffb45":"def encode_cyclic_feature(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]\/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]\/max_vals)\n    df = df.drop(col, axis=1)\n    return df\n\ntrain_df = encode_cyclic_feature(train_df, 'day', 7)\ntest_df = encode_cyclic_feature(test_df, 'day', 7) \n\ntrain_df = encode_cyclic_feature(train_df, 'month', 12)\ntest_df = encode_cyclic_feature(test_df, 'month', 12)","d3e34591":"# Drop ID columns from both train and test dataset as it's not a feature\ntrain_df = train_df.drop(['id'], axis=1)\ntest_df = test_df.drop(['id'], axis=1)","1d96e590":"Y_train = train_df['target']\nX_train = train_df.drop(['target'], axis=1)","c13a04c4":"from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold","cd47807b":"ntrain = train_df.shape[0]\nntest = test_df.shape[0]\nSEED = 666\nNFOLDS = 5\nkf = KFold(n_splits = NFOLDS, shuffle=True, random_state=SEED)\n\nclass StackingHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n    \n    def train(self, X_train, Y_train):\n        self.clf.fit(X_train, Y_train)\n        \n    def predict(self, X):\n        return self.clf.predict_proba(X)","8cbfcd78":"# Get out of fold predictions\ndef get_oof(clf, X_train, Y_train, X_test):\n    oof_train = np.zeros((ntrain, ))\n    oof_test = np.zeros((ntest, ))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n    \n    for i, (train_index, test_index) in enumerate(kf.split(X_train, Y_train)):\n        x_train = X_train.iloc[train_index]\n        y_train = Y_train.iloc[train_index]\n        x_test = X_train.iloc[test_index]\n        \n        clf.train(x_train, y_train)\n        \n        oof_train[test_index] = clf.predict(x_test)[:, 1]\n        oof_test_skf[i: ] = clf.predict(X_test)[:, 1]\n        \n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","2804fe14":"# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt'\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    'max_depth': 8,\n    'min_samples_leaf': 2\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n    'max_depth': 5,\n    'min_samples_leaf': 2\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n}","5cb215d4":"# Create 5 objects that represent our 4 models\nrf = StackingHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = StackingHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = StackingHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = StackingHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = StackingHelper(clf=SVC, seed=SEED, params=svc_params)","f9f88064":"# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, X_train, Y_train, test_df) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,X_train, Y_train, test_df) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, X_train, Y_train, test_df) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,X_train, Y_train, test_df) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")","f46a1d24":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()","730aa3b5":"from sklearn.metrics import roc_auc_score\nprint (roc_auc_score(Y_train, et_oof_train))\nprint (roc_auc_score(Y_train, rf_oof_train))\nprint (roc_auc_score(Y_train, ada_oof_train))\nprint (roc_auc_score(Y_train, gb_oof_train))","ed3b74ad":"print (gb.predict(test_df)[:, 1])\nprint (ada.predict(test_df)[:, 1])\nprint (rf.predict(test_df)[:, 1])\nprint (et.predict(test_df)[:, 1])\n","942d4733":"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test), axis=1)","5e8f5656":"import xgboost as xgb\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, Y_train)\npredictions = gbm.predict_proba(x_test)","6c45901c":"predictions","2a93f8b0":"sample_sub_df['target'] = predictions[:, 1]","00453d8d":"sample_sub_df.to_csv('submission.csv', index=False)\nsample_sub_df","cd5cdafb":"from IPython.display import FileLink, FileLinks\nFileLink('submission.csv')","086f1b8e":"First let's encode the binaty features. Features `bin_0`, `bin_1` and `bin_2` are already encoded. We need to convert `T`, `F`, `Y`, `N` etc. to numericals (i.e. 0 and 1).  ","54836da9":"### Exploratory Data Analysis\nExploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data.","795117b0":"Now let's make sure that there is no missing values in training and testing dataset","758b0caa":"Set the size and styles of graphs","06bdef70":"We only encode nominal features having lower cardinality with one hot encoding. Refer [this article](https:\/\/towardsdatascience.com\/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159) for the detailed explanation.","940fac10":"Ordinal features with only lower cardinality are plotted in above plot.","5e5ea58f":"Now let's use stacking to predict the final results. Reference: [this notebook](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python)","6bd5b68d":"### Encoding Binary Features","8e0064ef":"Now we'll encode higher cardinality nominal data using fearure hashing\/hashing trick. Refer [this](https:\/\/www.quora.com\/Can-you-explain-feature-hashing-in-an-easily-understandable-way) and [this](https:\/\/stats.stackexchange.com\/questions\/233262\/ohe-vs-feature-hashing\/233351) answers for more detail about how feature hashing works.","eefba6c0":"### Encoding cyclic features\nCyclic features like time (Day, month, seconds, hours, weekday etc.) shoul be encoded in such a way that conveys the nature of cyclic features (e.g. difference between time 23:55 and 00:05 is only 10 minutes and not 23 hours 50 minutes) to our model. See [this answer](https:\/\/datascience.stackexchange.com\/questions\/17759\/encoding-features-like-month-and-hour-as-categorial-or-numeric) for more details about encoding cyclic features.","7fb87e54":"`nom_5` to `nom_9` does not have meaningful details and hence not plotted in above plots.","27f0bbea":"### Distribution of Target variable","ff11ef77":"As mentioned in competition's description, there is no missing values in the given dataset. Now let's try to reduce the memory usage by determining appropriate datatypes for all the columns.","24383c30":"### Glimpse of training and testing dataset","882b9eb5":"### Read the dataset","e4ec2794":"### Distribution of each feature","6b34d058":"We have to predict the `target` column. Now let's see the columns and their datatypes.","e93b19b0":"### Encoding Ordinal Features","53ce0315":"### Encoding Nominal Features\nNominal values represent discrete units and are used to label variables, that have no quantitative value. So order of the values is not important for this type of feature. Now if we encode these features as numbers (also called label encoding), our model will learn unexpected behavior. For example, if feature value `Green` is encoded as `0` and `Red` is encoded as `1`, then roughly speaking, the model can learn `Green` < `Red` because 0 < 1 and this will lead to unexpeced results.\n\nTo avoid these kind to unexpected results, we can encode such features using one-hot encoding. Refer [this](https:\/\/machinelearningmastery.com\/why-one-hot-encode-data-in-machine-learning\/) article for detailed explanation."}}