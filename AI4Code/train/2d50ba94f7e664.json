{"cell_type":{"258039d4":"code","13fb7adf":"code","0f9cf1d8":"code","e88b4269":"code","98017d70":"code","04dc3d04":"code","79428cdf":"code","5e5af58c":"code","c66aa010":"code","0c7e46ea":"code","7b44eb71":"code","c974e467":"code","467c1978":"code","fe4ffcb8":"code","d9e8bf3c":"code","ebe9fdda":"code","2d0eff27":"code","81777a66":"code","ffac4205":"code","542e00b4":"code","3fdcbacf":"code","a22bf216":"code","e0ba81d7":"code","1a42a990":"code","58aacacf":"markdown","605553cd":"markdown","8775bd12":"markdown","7a3ba6c0":"markdown","164c396a":"markdown","51f49eb2":"markdown","bfc5531b":"markdown","bb8ddb4f":"markdown","53ea4aaf":"markdown","2034a670":"markdown","503960e6":"markdown","05a89ea2":"markdown","fce96c1a":"markdown","e11ed565":"markdown","b0c9dafc":"markdown"},"source":{"258039d4":"import numpy as np\nimport pandas as pd\nimport cv2 as cv\nimport cv2 as cv2\nfrom matplotlib import pyplot as plt\nimport math\nimport os\nimport shutil\nfrom pathlib import Path\nimport glob\nimport itertools\nfrom itertools import cycle\nfrom scipy import interp\n\nfrom statistics import mean\nfrom statistics import stdev\n\n!pip install pyradiomics\nimport radiomics\nimport SimpleITK as sitk\nimport six\nfrom skimage.feature import greycomatrix, greycoprops\nfrom array import *\nfrom scipy.cluster.vq import whiten\nfrom scipy.cluster.vq import kmeans\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.decomposition import PCA\n\nfrom sklearn import datasets\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img","13fb7adf":"# Remember to include '\/' at the end of the folder path only\n\n# Folder path for original images\noriBS = '..\/input\/tomato-leaves-original-dataset\/Dataset\/Bacterial_Spot\/'\noriEB = '..\/input\/tomato-leaves-original-dataset\/Dataset\/Early_Blight\/'\noriHL = '..\/input\/tomato-leaves-original-dataset\/Dataset\/Healthy\/'\noriLB = '..\/input\/tomato-leaves-original-dataset\/Dataset\/Late_Blight\/'\noriSLS = '..\/input\/tomato-leaves-original-dataset\/Dataset\/Septorial_leaf_spot\/'\noriYCV = '..\/input\/tomato-leaves-original-dataset\/Dataset\/Yellow_Curl_Virus\/'\n\n# Folder path for segmented images\nsegBS = '.\/Bacterial_Spot_Segmented\/'\nsegEB = '.\/Early_Blight_Segmented\/'\nsegHL = '.\/Healthy_Segmented\/'\nsegLB = '.\/Late_Blight_Segmented\/'\nsegSLS = '.\/Septorial_Leaf_Spot_Segmented\/'\nsegYCV = '.\/Yellow_Curl_Virus_Segmented\/'\n\n\n# File path for extracted features\nfeaBS = '.\/Bacterial_Spot_Features.csv'\nfeaEB = '.\/Early_Blight_Features.csv'\nfeaHL = '.\/Healthy_Features.csv'\nfeaLB = '.\/Late_Blight_Features.csv'\nfeaSLS = '.\/Septorial_Leaf_Spot_Features.csv'\nfeaYCV = '.\/Yellow_Curl_Virus_Features.csv'","0f9cf1d8":"# Create the folders to store segmented images\n# If the folders had been created, do not run this cell\nos.makedirs(segBS)\nos.makedirs(segEB)\nos.makedirs(segHL)\nos.makedirs(segLB)\nos.makedirs(segSLS)\nos.makedirs(segYCV)","e88b4269":"# Done by: Tan Li Xue\ndef segmentation(originalImageFolder, newFolderPath, cannyMin, cannyMax, numOfControusThreshold, lowerBoundDenominator, dilationIt, erodeIt, kernelDil, kernelEro):\n\n    fileList = []\n    fileIDList = []\n\n    fileIDList = next(os.walk(originalImageFolder), (None, None, []))[2] \n        \n    print('Number of Images: ', len(fileIDList))\n    i = 0\n    for file in fileIDList:\n        imgOriginal = cv.imread(originalImageFolder+file,1)\n        imgGray = cv.imread(originalImageFolder+file,0)\n        \n        \n        # Apply Gaussian Blur\n        imgGray = cv.GaussianBlur(imgGray,(3,3),0)\n\n        \n        # Apply Canny edge\n        edges = cv.Canny(imgGray,cannyMin,cannyMax)\n\n\n        # Apply filter contour by length\n        contours, hierarchy = cv2.findContours(edges,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n        contours = sorted(contours, key=lambda x: cv2.arcLength(x, True))\n        if len(contours) > numOfControusThreshold:\n            imgContours = np.zeros(imgGray.shape, dtype=np.uint8)\n            lowerBound = max(numOfControusThreshold, int(len(contours)\/lowerBoundDenominator))\n            for j in reversed(range(len(contours) - lowerBound,len(contours))):\n                length = cv2.arcLength(contours[j], True)\n                cv2.drawContours(imgContours, contours, j, (255), -1)\n        else:\n            imgContours = edges\n\n            \n        # Apply dilation\n        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernelDil,kernelDil))\n        dilation = cv2.dilate(imgContours,kernel,iterations = dilationIt)\n\n\n        # Find the max contours\n        contours, hierarchy = cv2.findContours(dilation,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n        maxContour = max(contours, key = len)\n\n\n        # Draw it on image\n        img = np.zeros( imgGray.shape, dtype=np.uint8)\n        img = cv2.drawContours(img, [maxContour], -1, (255),-1)\n\n\n        # Erosion after filling contour\n        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernelEro,kernelEro))\n        img = cv2.erode(img,kernel,iterations = erodeIt)\n        segmented = cv2.bitwise_and(imgOriginal,imgOriginal,mask = img)\n\n        \n        # Convert from RGB to LAB colour space\n        lab = cv2.cvtColor(segmented, cv2.COLOR_BGR2LAB)\n        lab_planes = cv2.split(lab)\n        \n        \n        # Apply CLAHE\n        clahe = cv2.createCLAHE(clipLimit=5.0)\n        lab_planes[0] = clahe.apply(lab_planes[0])\n        lab = cv2.merge(lab_planes)\n\n        \n        # Convert back from RGB to LAB colour space\n        bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n\n        \n        # Write image into the folder\n        cv2.imwrite(newFolderPath + fileIDList[i], bgr)\n        i+=1\n        \n    # Create a zip file fir the folder to be downloaded\n    shutil.make_archive(newFolderPath, 'zip', newFolderPath)","98017d70":"# Done by: Tan Li Xue\n\ncannyMin = 40\ncannyMax = 125\nnumOfControusThreshold = 70\nlowerBoundDenominator = 2\ndilationIt = 20\nerodeIt = 15\nkernelDil = 3\nkernelEro = 3\nsegmentation(oriBS, segBS, cannyMin, cannyMax, numOfControusThreshold, \\\n             lowerBoundDenominator, dilationIt, erodeIt, kernelDil, kernelEro)\n\n\ncannyMin = 80\ncannyMax = 125\nnumOfControusThreshold = 50\nlowerBoundDenominator = 2\ndilationIt = 20\nerodeIt = 15\nsegmentation(oriEB, segEB, cannyMin, cannyMax, numOfControusThreshold, \\\n             lowerBoundDenominator, dilationIt, erodeIt, kernelDil, kernelEro)\n\n\ncannyMin = 80\ncannyMax = 125\nnumOfControusThreshold = 30\nlowerBoundDenominator = 2\ndilationIt = 20\nerodeIt = 15\n\nsegmentation(oriHL, segHL, cannyMin, cannyMax, numOfControusThreshold, \\\n             lowerBoundDenominator, dilationIt, erodeIt, kernelDil, kernelEro)\n\n\ncannyMin = 60\ncannyMax = 125\nnumOfControusThreshold = 70\nlowerBoundDenominator = 2\ndilationIt = 20\nerodeIt = 15\n\nsegmentation(oriLB, segLB, cannyMin, cannyMax, numOfControusThreshold, \\\n             lowerBoundDenominator, dilationIt, erodeIt, kernelDil, kernelEro)\n\n\ncannyMin = 100\ncannyMax = 125\nnumOfControusThreshold = 100\nlowerBoundDenominator = 4\ndilationIt = 30\nerodeIt = 20\n\nsegmentation(oriSLS, segSLS, cannyMin, cannyMax, numOfControusThreshold, \\\n             lowerBoundDenominator, dilationIt, erodeIt, kernelDil, kernelEro)\n\n\ncannyMin = 40\ncannyMax = 125\nnumOfControusThreshold = 40\nlowerBoundDenominator = 4\ndilationIt = 30\nerodeIt = 20\n\nsegmentation(oriYCV, segYCV, cannyMin, cannyMax, numOfControusThreshold, \\\n             lowerBoundDenominator, dilationIt, erodeIt, kernelDil, kernelEro)","04dc3d04":"# Done by: Chen Zhe Sheng\ndef ColourHistogram(folderPath,filename):\n    \n    # Read image\n    img = cv.imread(folderPath+filename)\n    \n    # Resize image\n    width = int(256)\n    height = int(256)\n    dim = (width, height)\n    img = cv.resize(img, dim, interpolation = cv.INTER_AREA)\n    \n    # Create df\n    x = [i for i in range(0,45)]\n    df = pd.DataFrame(columns = x)\n    \n    # Colour Histogram\n    Blue = cv.calcHist([img], [0], None, [256], [0,256])\n    Green = cv.calcHist([img], [1], None, [256], [0,256])\n    Red = cv.calcHist([img], [2], None, [256], [0,256])\n\n    result_R = [i for i in range(0,15)]\n    result_G = [i for i in range(0,15)]\n    result_B = [i for i in range(0,15)]\n\n    start = 0\n    end = 17\n    for i in range(0,15):\n        r = np.sum(Red[start:end])\n        g = np.sum(Green[start:end])\n        b = np.sum(Blue[start:end])\n        start = end\n        end = end + 17\n        result_R[i] = r\n        result_G[i] = g\n        result_B[i] = b\n\n    finalResult = [filename]\n    finalResult = result_R + result_G + result_B + finalResult\n\n    df = df.append([finalResult])\n    df['ID'] = df[45]\n    df.drop(45, axis = 1, inplace = True)\n    df = df.set_index('ID')\n\n    return df","79428cdf":"# Done by: Toh Yue Xiang\ndef DominantColour(folderPath,filename):\n    \n    # Read image\n    img = cv.imread(folderPath+filename)\n    \n    \n    # Resize image\n    width = int(256)\n    height = int(256)\n    dim = (width, height)\n    img = cv.resize(img, dim, interpolation = cv.INTER_AREA)\n    \n    \n    # Create df\n    df = pd.DataFrame()\n    \n    \n    # RGB\n    r = []\n    g = []\n    b = []\n    for row in img:\n        for temp_r, temp_g, temp_b in row:\n            r.append(temp_r)\n            g.append(temp_g)\n            b.append(temp_b)\n            \n    dfKMeans = pd.DataFrame({'red' : r,\n                          'green' : g,\n                          'blue' : b})\n \n    dfKMeans['scaled_color_red'] = whiten(dfKMeans['red'])\n    dfKMeans['scaled_color_blue'] = whiten(dfKMeans['blue'])\n    dfKMeans['scaled_color_green'] = whiten(dfKMeans['green'])\n\n    cluster_centers, _ = kmeans(dfKMeans[['scaled_color_red',\n                                    'scaled_color_blue',\n                                    'scaled_color_green']], 3)\n\n    dominant_colors = [filename]\n\n    red_std, green_std, blue_std = dfKMeans[['red','green','blue']].std()\n\n    values = [filename]\n    for i, cluster_center in enumerate(cluster_centers):\n        red_scaled, green_scaled, blue_scaled = cluster_center\n        values.append(red_scaled * red_std \/ 255)\n        values.append(green_scaled * green_std \/ 255)\n        values.append(blue_scaled * blue_std \/ 255)\n\n    while len(values) < 10:\n        values.append(pd.NA)\n        \n    df = df.append([values])\n    df.columns = ['ID','Red1','Green1','Blue1','Red2','Green2','Blue2','Red3','Green3','Blue3']\n    df = df.set_index('ID')\n\n    return df","5e5af58c":"# Done by: Aaron Kong Kah Lun\ndef GLCM(folderPath,filename):\n    # Read image\n    img = cv.imread(folderPath+filename, 0)\n    \n    # Resize image\n    width = int(256)\n    height = int(256)\n    dim = (width, height)\n    img = cv.resize(img, dim, interpolation = cv.INTER_AREA)\n    \n    df = pd.DataFrame()\n    df['ID'] = [filename]\n\n    #Full image\n    #GLCM = greycomatrix(img, [1], [0, np.pi\/4, np.pi\/2, 3*np.pi\/4])\n    GLCM = greycomatrix(img, [1], [0])       \n    GLCM_Energy = greycoprops(GLCM, 'energy')[0]\n    df['Energy'] = GLCM_Energy\n    GLCM_corr = greycoprops(GLCM, 'correlation')[0]\n    df['Corr'] = GLCM_corr       \n    GLCM_diss = greycoprops(GLCM, 'dissimilarity')[0]\n    df['Diss_sim'] = GLCM_diss       \n    GLCM_hom = greycoprops(GLCM, 'homogeneity')[0]\n    df['Homogen'] = GLCM_hom       \n    GLCM_contr = greycoprops(GLCM, 'contrast')[0]\n    df['Contrast'] = GLCM_contr\n\n    GLCM2 = greycomatrix(img, [3], [0])       \n    GLCM_Energy2 = greycoprops(GLCM2, 'energy')[0]\n    df['Energy2'] = GLCM_Energy2\n    GLCM_corr2 = greycoprops(GLCM2, 'correlation')[0]\n    df['Corr2'] = GLCM_corr2       \n    GLCM_diss2 = greycoprops(GLCM2, 'dissimilarity')[0]\n    df['Diss_sim2'] = GLCM_diss2       \n    GLCM_hom2 = greycoprops(GLCM2, 'homogeneity')[0]\n    df['Homogen2'] = GLCM_hom2       \n    GLCM_contr2 = greycoprops(GLCM2, 'contrast')[0]\n    df['Contrast2'] = GLCM_contr2\n\n    GLCM3 = greycomatrix(img, [5], [0])       \n    GLCM_Energy3 = greycoprops(GLCM3, 'energy')[0]\n    df['Energy3'] = GLCM_Energy3\n    GLCM_corr3 = greycoprops(GLCM3, 'correlation')[0]\n    df['Corr3'] = GLCM_corr3       \n    GLCM_diss3 = greycoprops(GLCM3, 'dissimilarity')[0]\n    df['Diss_sim3'] = GLCM_diss3       \n    GLCM_hom3 = greycoprops(GLCM3, 'homogeneity')[0]\n    df['Homogen3'] = GLCM_hom3       \n    GLCM_contr3 = greycoprops(GLCM3, 'contrast')[0]\n    df['Contrast3'] = GLCM_contr3\n\n    GLCM4 = greycomatrix(img, [0], [np.pi\/4])       \n    GLCM_Energy4 = greycoprops(GLCM4, 'energy')[0]\n    df['Energy4'] = GLCM_Energy4\n    GLCM_corr4 = greycoprops(GLCM4, 'correlation')[0]\n    df['Corr4'] = GLCM_corr4       \n    GLCM_diss4 = greycoprops(GLCM4, 'dissimilarity')[0]\n    df['Diss_sim4'] = GLCM_diss4       \n    GLCM_hom4 = greycoprops(GLCM4, 'homogeneity')[0]\n    df['Homogen4'] = GLCM_hom4       \n    GLCM_contr4 = greycoprops(GLCM4, 'contrast')[0]\n    df['Contrast4'] = GLCM_contr4\n\n    GLCM5 = greycomatrix(img, [0], [np.pi\/2])       \n    GLCM_Energy5 = greycoprops(GLCM5, 'energy')[0]\n    df['Energy5'] = GLCM_Energy5\n    GLCM_corr5 = greycoprops(GLCM5, 'correlation')[0]\n    df['Corr5'] = GLCM_corr5       \n    GLCM_diss5 = greycoprops(GLCM5, 'dissimilarity')[0]\n    df['Diss_sim5'] = GLCM_diss5       \n    GLCM_hom5 = greycoprops(GLCM5, 'homogeneity')[0]\n    df['Homogen5'] = GLCM_hom5       \n    GLCM_contr5 = greycoprops(GLCM5, 'contrast')[0]\n    df['Contrast5'] = GLCM_contr5\n\n    GLCM6 = greycomatrix(img, [0], [3*np.pi\/4])       \n    GLCM_Energy6 = greycoprops(GLCM6, 'energy')[0]\n    df['Energy6'] = GLCM_Energy6\n    GLCM_corr6 = greycoprops(GLCM6, 'correlation')[0]\n    df['Corr6'] = GLCM_corr6       \n    GLCM_diss6 = greycoprops(GLCM6, 'dissimilarity')[0]\n    df['Diss_sim6'] = GLCM_diss6       \n    GLCM_hom6 = greycoprops(GLCM6, 'homogeneity')[0]\n    df['Homogen6'] = GLCM_hom6       \n    GLCM_contr6 = greycoprops(GLCM6, 'contrast')[0]\n    df['Contrast6'] = GLCM_contr6\n\n    df = df.set_index('ID')\n    return df","c66aa010":"# Done by: Tan Li Xue\ndef GLSZM(folderPath, filename):\n    # Read image\n    #img = cv.imread(filename)\n    img = cv.imread(folderPath+filename, 0)\n    # Resize image\n    width = int(256)\n    height = int(256)\n    dim = (width, height)\n    img = cv.resize(img, dim, interpolation = cv.INTER_AREA)\n    \n    height = img.shape[0]\n    width = img.shape[1]\n    tempMask = np.ones((height,width), np.uint8)\n\n    img = sitk.GetImageFromArray(img)\n    mask = sitk.GetImageFromArray(tempMask)\n    \n    # Create df\n    df = pd.DataFrame()\n    \n    # Generate Features\n    glszmFeatures = radiomics.glszm.RadiomicsGLSZM(img, mask)\n    glszmFeatures.enableAllFeatures()\n    result = glszmFeatures.execute()\n    \n    values = [filename]\n    for (key, val) in six.iteritems(result):\n        values.append(val.item())\n\n    # Append the value into the df and define the column names\n    df = df.append([values])\n    df.columns = ['ID', 'GrayLevelNonUniformity', 'GrayLevelNonUniformityNormalized',\n       'GrayLevelVariance', 'HighGrayLevelZoneEmphasis', 'LargeAreaEmphasis',\n       'LargeAreaHighGrayLevelEmphasis', 'LargeAreaLowGrayLevelEmphasis',\n       'LowGrayLevelZoneEmphasis', 'SizeZoneNonUniformity',\n       'SizeZoneNonUniformityNormalized', 'SmallAreaEmphasis',\n       'SmallAreaHighGrayLevelEmphasis', 'SmallAreaLowGrayLevelEmphasis',\n       'ZoneEntropy', 'ZonePercentage', 'ZoneVariance']\n    df = df.set_index('ID')\n    return df","0c7e46ea":"# Done by: Aaron Kong Kah Lun\ndef FeaturesExtractionsAll(folderPath):\n    df = pd.DataFrame()\n    for fname in sorted(os.listdir(folderPath)):\n        colourHist = ColourHistogram(folderPath,fname)\n        dominantCol = DominantColour(folderPath,fname)\n        glcm = GLCM(folderPath,fname)\n        glszm = GLSZM(folderPath,fname)\n        dfRow = colourHist.join([dominantCol, glcm, glszm])\n        df = df.append(dfRow)\n            \n    return df","7b44eb71":"# Done by: Aaron Kong Kah Lun\ndfBacterialSpot = FeaturesExtractionsAll(segBS)\ndfBacterialSpot.to_csv(feaBS)\n\ndfEarlyBlight = FeaturesExtractionsAll(segEB)\ndfEarlyBlight.to_csv(feaEB)\n\ndfHealthy = FeaturesExtractionsAll(segHL)\ndfHealthy.to_csv(feaHL)\n\ndfLateBlight = FeaturesExtractionsAll(segLB)\ndfLateBlight.to_csv(feaLB)\n\ndfSeptorial = FeaturesExtractionsAll(segSLS)\ndfSeptorial.to_csv(feaSLS)\n\ndfYellowCurl = FeaturesExtractionsAll(segYCV)\ndfYellowCurl.to_csv(feaYCV)","c974e467":"# Done by: Chen Zhe Sheng\ndef CombineAllDF(folderPaths, labels):\n    dfAll = pd.DataFrame()\n    i=0\n    for paths, label in zip(folderPaths, labels):\n        dfs = pd.read_csv(paths)\n        \n        dfs['Labels'] = labels[i]\n        dfs = dfs.set_index('ID')\n        \n        dfAll = dfAll.append(dfs)\n        i=i+1\n    dfAll = dfAll.reset_index()\n    return dfAll","467c1978":"# Done by: Chen Zhe Sheng\n\n# Combine all df from each class into one big df\n# So the final df contains a df with all classess, and all features\n\nbacterialSpot = feaBS\nearlyBlight = feaEB\nhealthy = feaHL\nlateBlight = feaLB\nseptorialSpot = feaSLS\nyellowCurl = feaYCV\n\n\nfolderPaths = [bacterialSpot,earlyBlight,healthy,lateBlight,septorialSpot,yellowCurl]\nclasses = ['Bacterial Spot','Early Blight','Healthy','Late Blight','Septorial Leaf Spot','Yellow Curl Virus']\ndfALL = CombineAllDF(folderPaths, classes)","fe4ffcb8":"# Done by: Toh Yue Xiang\n    \n# Drop ID and LABELS\n# Because this is to remove outliers only\ncols = dfALL.drop(['ID','Labels'], axis = 1).columns\n\n# Remove outliers\nQ1 = dfALL[cols].quantile(0.25)\nQ3 = dfALL[cols].quantile(0.75)\nIQR = Q3 - Q1\n\ndfFinal = dfALL[~((dfALL[cols] < (Q1 - 1.5 * IQR)) | (dfALL[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\ndfFinal = dfFinal.reset_index()\ndfFinal.drop('index',inplace = True, axis = 1)\ndfFinal['Red3'] = dfFinal['Red3'].replace(np.nan, 0.277547)\ndfFinal['Green3'] = dfFinal['Green3'].replace(np.nan, 0.34919)\ndfFinal['Blue3'] = dfFinal['Blue3'].replace(np.nan, 0.340418)\nprint(dfFinal.shape)\n\n# Generate X_train, without ID and Labels\n# Y_train only contains labels\nX = dfFinal.drop(['ID','Labels'], axis = 1)\ny = dfFinal.loc[:,'Labels']\n\n# split the dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\nprint('Training set shape: ', X_train.shape, y_train.shape)\nprint('Testing set shape: ', X_test.shape, y_test.shape)","d9e8bf3c":"# Done by: Toh Yue Xiang\n    \n# List out all files that have been removed\ndfOutliers = dfALL[((dfALL[cols] < (Q1 - 1.5 * IQR)) | (dfALL[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\nfor i in range(dfOutliers.shape[0]):\n    print(dfOutliers['ID'].iloc[i])","ebe9fdda":"# Done by: Tan Li Xue\n\n# Use 10-fold cross validation\nn_splits = 10\n\n# Number of components\/features after PCA\nn_components = int(len(cols)\/5)\n\nkf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n\n\n# Create a list to store all metrics for training and evaluating\naccuracies_tr = []\nprecisions_tr = []\nrecalls_tr = []\nf1s_tr = []\n\naccuracies_val = []\nprecisions_val = []\nrecalls_val = []\nf1s_val = []\n\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train)):\n    # split the training set into train and val sets\n    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, random_state = 42)\n\n    \n    # Mean Imputation\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n    imp.fit(X_tr)\n    imp.transform(X_tr)\n    imp.transform(X_val)\n    \n    \n    # Standard Scaler\n    scaler = StandardScaler()\n    scaler.fit(X_tr)\n    X_tr = scaler.transform(X_tr)\n    X_val = scaler.transform(X_val)\n    \n    \n    # PCA\n    pca = PCA(n_components = n_components)\n    pca.fit(X_tr)\n    X_tr = pca.transform(X_tr)\n    X_val = pca.transform(X_val)\n    \n    \n    # Classifier\n    #clf =  #svm.SVC()\n    clf = DecisionTreeClassifier(random_state = 42)\n    clf.fit(X_tr, y_tr)\n    y_pred_tr = clf.predict(X_tr)\n    y_pred_val = clf.predict(X_val)\n    \n    \n    # Print current split count\n    print('Current Split: ', i)\n    i+=1\n    \n    \n    # Evaluate Train\n    accuracy = accuracy_score(y_tr, y_pred_tr)\n    pre, rec, f1, _ = precision_recall_fscore_support(y_tr, y_pred_tr, average='macro', zero_division = 0)\n    print('Training')\n    print('Accuracy: ',accuracy)\n    print('Precision: ', pre)\n    print('Recall: ', rec)\n    print('F1: ', f1)\n    con_tr = confusion_matrix(y_tr, y_pred_tr, labels=classes)#Labels\n\n    \n    accuracies_tr.append(accuracy)\n    precisions_tr.append(pre)\n    recalls_tr.append(rec)\n    f1s_tr.append(f1)\n    \n    \n    # Evaluate Val\n    accuracy = accuracy_score(y_val, y_pred_val)\n    pre, rec, f1, _ = precision_recall_fscore_support(y_val, y_pred_val, average='macro', zero_division = 0)\n    print('\\n')\n    print('Validation')\n    print('Accuracy: ',accuracy)\n    print('Precision: ', pre)\n    print('Recall: ', rec)\n    print('F1: ', f1)\n    con_val = confusion_matrix(y_val, y_pred_val, labels=classes)#Labels\n\n    \n    accuracies_val.append(accuracy)\n    precisions_val.append(pre)\n    recalls_val.append(rec)\n    f1s_val.append(f1)\n    \n    \n    print('\\n')\n    print('\\n')\n    print('\\n')\n    \nprint('Average training accuracy: ', mean(accuracies_tr))\nprint('Average training precision: ', mean(precisions_tr))\nprint('Average training recall: ', mean(recalls_tr))\nprint('Average training f1: ', mean(f1s_tr))\nprint('\\n')\nprint('Average validation accuracy: ', mean(accuracies_val))\nprint('Average validation precision: ', mean(precisions_val))\nprint('Average validation recall: ', mean(recalls_val))\nprint('Average validation f1: ', mean(f1s_val))","2d0eff27":"# Done by: Aaron Kong Kah Lun\n\n# List out parameters to perform grid search\nparameters = {'kernel':['linear', 'poly','rbf', 'sigmoid'], \n      'C':[0.1, 0.5, 1, 5, 10],\n      'degree':[1, 3, 5, 7, 9],\n      'class_weight':[None, 'balanced']\n     }\n\n# This one list all the scoring that we wants\nscoring = ['accuracy','precision_macro','recall_macro','f1_macro']\n\n# Transform input data\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_tr = scaler.transform(X_train)\n\npca = PCA(n_components = n_components)\npca.fit(X_tr)\nX_tr = pca.transform(X_tr)\n\n# Start hyperparameter tuning\nclf = svm.SVC(decision_function_shape = 'ovr')\nclf = GridSearchCV(clf, parameters, cv = 5, scoring = scoring, refit = 'accuracy')\n\n# Fit the model on our train set\nclf.fit(X_tr, y_train)\n\n# Find the best score\nprint(clf.best_score_)\n\n# Get the hyperparameters with the best score\nprint(clf.best_params_)\nbest_clf = clf.best_params_\n\n# Record the best estimator\nprint(clf.best_estimator_)\n\n\n### THERE MIGHT BE WARNING FOR PRECISION ILL-DEFINED DUE TO DIVISION BY 0","81777a66":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    \n    \n# Done by: Chen Zhe Sheng\ndef show_roc_curve(labels,scores):\n    for i in range(labels.shape[1]):\n\n        cur_labels = labels[:,i]\n        cur_scores = scores[:,i]\n\n        #Calculate roc_auc score\n        fpr, tpr, thresholds = roc_curve(cur_labels, cur_scores)\n        roc_auc = auc(fpr,tpr) \n        \n        #+3 to ignore first colour set for background channel of masks\n        r = Colours[i*3+0]\/255\n        g = Colours[i*3+1]\/255\n        b = Colours[i*3+2]\/255\n        lw=2\n        plt.plot(fpr, tpr, color=(r, g, b),lw=lw, label='ROC-AUC: '+classes[i] + '(area = %0.2f)' % roc_auc)\n\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC AUC for all classes')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    plt.figure()\n    \n# Done by: Chen Zhe Sheng\ndef image_grid(array, ncols):\n    index, height, width, channels = array.shape\n    nrows = index\/\/ncols\n    \n    img_grid = (array.reshape(nrows, ncols, height, width, channels)\n              .swapaxes(1,2)\n              .reshape(height*nrows, width*ncols,channels))\n    \n    return img_grid","ffac4205":"# Done by: Aaron Kong Kah Lun\n# USe best param to fit the values again\nn_components = int(len(cols)\/5)\n\n# Standardize\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_tr = scaler.transform(X_train)\nX_test_tr = scaler.transform(X_test)\n\n# Feature selection\npca = PCA(n_components = n_components)\npca.fit(X_tr)\nX_tr = pca.transform(X_tr)\nX_test_tr = pca.transform(X_test_tr)\n\n# Train classifier\nclf = svm.SVC(C = 5, class_weight = None, decision_function_shape = 'ovr', degree = 1, kernel = 'rbf')\n#clf = DecisionTreeClassifier(random_state = 42, class_weight = None, max_depth = 15, max_features = 'auto'\n                             #, min_samples_split = 2, splitter = 'best')\nclf.fit(X_tr, y_train)\n\n# Predict\ny_pred_tr = clf.predict(X_tr)\ny_pred_test = clf.predict(X_test_tr)\n\n# Accuracy of training set\nprint('Training set accuracy: ', accuracy_score(y_train, y_pred_tr))\n\n# Accuracy of test set\nprint('Test set accuracy: ', accuracy_score(y_test, y_pred_test))\n\n# Confusion matrix of test set\nprint('Confusion matrix')\nplot_confusion_matrix(confusion_matrix(y_test, y_pred_test), classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues)\n\n# Classification report of test set\nprint('Classification report')\nprint(classification_report(y_test, y_pred_test))","542e00b4":"# Done by: Chen Zhe Sheng\n\n# Binarize labels\ny_tr_bin = label_binarize(y_train, classes=classes)\ny_test_bin = label_binarize(y_test, classes=classes)\n\n# Get y score of test set\ny_score = clf.decision_function(X_test_tr)\n\n# Compute macro-average ROC curve and ROC area\nfprMacro, tprMacro, _ = roc_curve(y_test_bin.ravel(), y_score.ravel())\nroc_auc = auc(fprMacro,tprMacro)\n\nlw = 2\nplt.figure()\n\n# Plot the curve\nplt.plot(fprMacro, tprMacro, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n\n# Limit axis\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\n\n# Add labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\n\n# Display ROC curve for all classes cobined\nplt.show()\n\n# Display ROC curve for each class\nColours = [\n           255,0,0,\n            0,255,0,\n           0,0,255,\n           0,255,255,\n           255,0,255,\n           255,255,0,\n          ]\nshow_roc_curve(y_test_bin, y_score)","3fdcbacf":"# Done by: Toh Yue Xiang\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(len(classes)):\n    r = Colours[i*3+0]\/255\n    g = Colours[i*3+1]\/255\n    b = Colours[i*3+2]\/255\n    precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i],y_score[:, i])\n    average_precision[i] = average_precision_score(y_test_bin[:, i], y_score[:, i])\n    plt.plot(recall[i], precision[i], color=(r, g, b), label='Pre-Rec: '+classes[i] + ' = ' + str(round(average_precision[i], 2)))\n    \nprecision[\"macro\"], recall[\"macro\"], _ = precision_recall_curve(y_test_bin.ravel(),y_score.ravel())\naverage_precision[\"macro\"] = average_precision_score(y_test_bin, y_score,average=\"macro\")\n\n# Plot Precision-Recall curve\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall')\nplt.legend(loc=\"lower left\")\nplt.show()","a22bf216":"# Done by: Tan Li Xue\n\nclasses = ['Bacterial Spot','Early Blight','Healthy','Late Blight','Septorial Leaf Spot','Yellow Curl Virus']\nalgos = ['KNN','DT','RF','SVM']\n\ncmKNN = [[54,1,0,2,0,0],\n     [5,55,4,3,1,1],\n     [0,1,72,1,1,0],\n     [3,5,1,23,3,3],\n     [3,3,5,4,33,4],\n     [2,1,1,1,1,34]]\n\ncmDT = [[45,3,4,1,0,4],\n     [6,46,4,4,3,6],\n     [2,5,58,5,5,0],\n     [2,6,1,22,2,5],\n     [6,7,5,7,24,3],\n     [3,1,1,1,2,32]]\n\ncmRF = [[51,3,0,1,1,1],\n     [4,57,2,1,4,1],\n     [0,2,73,0,0,0],\n     [1,7,0,24,4,2],\n     [2,0,3,2,43,2],\n     [1,1,0,0,0,38]]\n\ncmSVM = [[55,1,0,1,0,0],\n     [3,57,1,5,1,2],\n     [0,1,74,0,0,0],\n     [2,5,0,24,3,4],\n     [1,0,2,4,43,2],\n     [1,0,0,0,1,38]]\n\ncms = [cmKNN, cmDT, cmRF, cmSVM]\nsaveMR = [[],\n         [],\n         [],\n         [],\n         [],\n         []]\n\nfor i in range(6):\n    for j in range(i + 1, 6):\n        print(classes[i],'   ', classes[j])\n        avgRate = 0\n        for algo, cm in zip(algos, cms):\n            ii = cm[i][i]\n            jj = cm[j][j]\n            ij = cm[i][j]\n            ji = cm[j][i]\n            total = ii + jj + ij + ji\n            falseLabels = ij + ji\n            misclassificationRate = falseLabels\/total\n            avgRate += misclassificationRate\n            print(algo,': ',round(misclassificationRate, 4))\n        print('Avg: ',round(avgRate\/4, 4))\n        print('\\n')","e0ba81d7":"# Done by: Toh Yue Xiang\n    \n# To display the wrongly classified image\nX_temp = dfFinal\ny_temp = dfFinal\n\n# split the dataset into train and test sets\nX_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(X_temp, y_temp, random_state = 42)\nindices = [i for i in range(len(y_test)) if y_test.iloc[i] != y_pred_test[i]]\nwrong_prediction_fname = y_test_temp.iloc[indices,0]\nwrong_prediction_cat = y_test.iloc[indices]\n\nbs = []\neb = []\nhl = []\nlb = []\nsls = []\nycv = []\nfor i in range(len(wrong_prediction_fname)):\n\n    if wrong_prediction_cat.iloc[i] == 'Bacterial Spot':\n        image = cv2.imread(segBS + wrong_prediction_fname.iloc[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        bs.append(image)\n        \n    if wrong_prediction_cat.iloc[i] == 'Early Blight':\n        image = cv2.imread(segEB + wrong_prediction_fname.iloc[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        eb.append(image)\n        \n    if wrong_prediction_cat.iloc[i] == 'Healthy':\n        image = cv2.imread(segHL + wrong_prediction_fname.iloc[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        hl.append(image)\n        \n    if wrong_prediction_cat.iloc[i] == 'Late Blight':\n        image = cv2.imread(segLB + wrong_prediction_fname.iloc[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        lb.append(image)\n       \n    if wrong_prediction_cat.iloc[i] == 'Septorial Leaf Spot':\n        image = cv2.imread(segSLS + wrong_prediction_fname.iloc[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        sls.append(image)\n        \n    if wrong_prediction_cat.iloc[i] == 'Yellow Curl Virus':\n        image = cv2.imread(segYCV + wrong_prediction_fname.iloc[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        ycv.append(image)","1a42a990":"# Done by: Toh Yue Xiang\nresult = image_grid(np.array(bs),len(bs))\nfig = plt.figure(figsize=(20., 20.))\nplt.title(\"Bacterial Spot\")\nplt.imshow(result)\n\n\nresult = image_grid(np.array(eb),len(eb))\nfig = plt.figure(figsize=(20, 20))\nplt.title(\"Early Blight\")\nplt.imshow(result)\n\n\nresult = image_grid(np.array(hl),len(hl))\nfig = plt.figure(figsize=(20., 20.))\nplt.title(\"Healthy\")\nplt.imshow(result)\n\n\nresult = image_grid(np.array(lb),len(lb))\nfig = plt.figure(figsize=(20., 20.))\nplt.title(\"Late Blight\")\nplt.imshow(result)\n\n\nresult = image_grid(np.array(sls),len(sls))\nfig = plt.figure(figsize=(20., 20.))\nplt.title(\"Septorial Leaf Spot\")\nplt.imshow(result)\n\n\nresult = image_grid(np.array(ycv),len(ycv))\nfig = plt.figure(figsize=(20., 20.))\nplt.title(\"Yellow Curl Virus\")\nplt.imshow(result)","58aacacf":"## GLSZM","605553cd":"## Colour Histogram","8775bd12":"## K-Fold Cross Validation","7a3ba6c0":"# Folders and Files Paths","164c396a":"# Training","51f49eb2":"# Feature Extraction","bfc5531b":"## Dominant Colours","bb8ddb4f":"# Evaluation","53ea4aaf":"# Import","2034a670":"## Combine All Classes' Features and Remove Outliers","503960e6":"# ROI Segmentation and Image Pre-processing","05a89ea2":"## Extract All Features","fce96c1a":"## Hyperparameter Tuning","e11ed565":"## Error Analysis","b0c9dafc":"## GLCM"}}