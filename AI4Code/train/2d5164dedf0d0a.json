{"cell_type":{"b31f2f83":"code","7ce8bad3":"code","d81018ac":"code","b08a8bf8":"code","0a92f3e9":"code","ff78194c":"code","970ec041":"code","971e7454":"code","56ff6a6b":"code","54d34a6b":"code","1aaa0cf7":"code","dca64225":"code","7e2df84f":"code","36302473":"code","4eccea13":"code","5c2ae758":"code","c656ba45":"markdown","cff49906":"markdown","966e1acc":"markdown","97b4522f":"markdown","5cff0c3f":"markdown","0eeab53e":"markdown","7d80e4d8":"markdown","3e05566b":"markdown","75dce4de":"markdown","6fc7bb67":"markdown","e5e591dc":"markdown","99aea4e4":"markdown","c58e8cac":"markdown"},"source":{"b31f2f83":"import re\nimport json\nimport string\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import  CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score","7ce8bad3":"path ='..\/input\/news-category-dataset\/News_Category_Dataset_v2.json'","d81018ac":"list_ = []\nwith open(path) as files:\n    for file in files:\n        list_.append(json.loads(file))","b08a8bf8":"data = pd.DataFrame(list_)\ndata.head()","0a92f3e9":"data.info()","ff78194c":"data.isnull().sum()","970ec041":"plt.figure(figsize=(14,8))\ncount = data.category.value_counts()\nsns.barplot(x=count.index, y=count)\nplt.xlabel('Category')\nplt.ylabel('Count')\nplt.xticks(rotation=90);","971e7454":"def clean_text(text):\n    text = text.lower()                                  # lower-case all characters\n    text =  re.sub(r'@\\S+', '',text)                     # remove twitter handles\n    text =  re.sub(r'http\\S+', '',text)                  # remove urls\n    text =  re.sub(r'pic.\\S+', '',text) \n    text =  re.sub(r\"[^a-zA-Z+']\", ' ',text)             # only keeps characters\n    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text+' ')      # keep words with length>1 only\n    text = \"\".join([i for i in text if i not in string.punctuation])\n    words = nltk.tokenize.word_tokenize(text)\n    stopwords = nltk.corpus.stopwords.words('english')   # remove stopwords\n    text = \" \".join([i for i in words if i not in stopwords and len(i)>2])\n    text= re.sub(\"\\s[\\s]+\", \" \",text).strip()            # remove repeated\/leading\/trailing spaces\n    return text","56ff6a6b":"data['Text_cleaning'] = data.headline.apply(clean_text)\ndata.head()","54d34a6b":"vectorizer = CountVectorizer()\ndata_vectorizer = vectorizer.fit_transform(data['Text_cleaning'])","1aaa0cf7":"labels = data['category']","dca64225":"X_train, X_test, y_train, y_test = train_test_split(data_vectorizer, labels, test_size=0.2, random_state=42)","7e2df84f":"nb = MultinomialNB()\nnb.fit(X_train,y_train)","36302473":"y_pred = nb.predict(X_test)","4eccea13":"Acc_train = nb.score(X_train, y_train)\nacc_test = nb.score(X_test, y_test)\nprint('Train Accuracy : {:.2f}%'.format(Acc_train*100))\nprint('Test Accuracy  : {:.2f}%'.format(acc_test*100))","5c2ae758":"print(classification_report(y_test, y_pred))","c656ba45":"# \ud83e\uddf9 Cleaning Data","cff49906":"# \ud83d\udcdd Meta information of Dataframe","966e1acc":"# \ud83d\udcda Training model","97b4522f":"* Feel free to download Notebook and do experiments on it.\n* Comments if you find something inappropriate and will improve accordingly.\n* Upvote if you find this notebook useful.","5cff0c3f":"# \ud83d\udce5 Importing Libraries","0eeab53e":"# \u2728 Thanks","7d80e4d8":"# \u2702\ufe0f Train test split","3e05566b":"# \ud83d\udd22 The process of converting words into numbers","75dce4de":"# \ud83d\udd25 EDA & Visualization","6fc7bb67":"# \ud83d\uddc3\ufe0f Load Dataset","e5e591dc":"# \ud83d\udd0e Checking for NaN values","99aea4e4":"# \u2714\ufe0f Classification report","c58e8cac":"# \ud83e\uddea Test & Train Accuracy"}}