{"cell_type":{"36b931fe":"code","c578d10c":"code","7831aaae":"code","3f0658a5":"code","83c41d11":"code","7e24c898":"code","f4142212":"code","8d868a5d":"code","5bc54099":"code","5ae10716":"code","54752631":"code","82896004":"code","2472dc14":"code","36042d7c":"code","31f6411e":"code","352675a1":"code","bf031f02":"code","66ee7878":"code","631ac731":"code","f9167709":"code","22c128d7":"code","f87ec508":"code","7c0a089e":"code","4fce591e":"code","c0517eab":"code","4180660f":"code","949b788a":"code","26c410f9":"code","57e5a386":"code","03508e55":"markdown","c3cee892":"markdown","501cd937":"markdown","40eddf2b":"markdown","b14c3c14":"markdown","5b8bd4a3":"markdown","ffc5b5fb":"markdown","b67b131c":"markdown"},"source":{"36b931fe":"!pip install u8darts[torch]","c578d10c":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport datetime\nimport random\n\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.impute import SimpleImputer\n\nimport joblib\nimport pickle\nimport os\n\nfrom darts import TimeSeries\nfrom darts.metrics import mape, smape\nfrom darts.models import RNNModel, NBEATSModel\nfrom darts.utils.data.sequential_dataset import SequentialDataset\nfrom darts.utils.data.horizon_based_dataset import HorizonBasedDataset\n\nfrom torch.nn import L1Loss\nimport torch\nimport torch.optim as optim","7831aaae":"torch.manual_seed(1337)\nrandom.seed(1337)\nnp.random.seed(1337)","3f0658a5":"df = pd.read_csv('..\/input\/web-traffic-time-series-forecasting\/train_2.csv.zip', compression='zip', index_col='Page').fillna(0)","83c41d11":"df.columns = pd.DatetimeIndex(df.columns)","7e24c898":"LOOKBACK = 3\nCHUNK_OUTPUT_FINAL_LENGTH = (datetime.date(2017, 11, 13) - datetime.date(2017, 9, 10)).days\nCHUNK_OUTPUT_LENGTH = 32\nCHUNK_INPUT_LENGTH = CHUNK_OUTPUT_LENGTH*LOOKBACK","f4142212":"CACHEDIR = '.\/'\nmemory = joblib.Memory(CACHEDIR, verbose=0)","8d868a5d":"# Cleaning cache if changing the params\n!rm .\/training_sequence.pickle .\/test_sequence.pickle .\/validation_sequence.pickle","5bc54099":"def walk_forward_split(df: pd.DataFrame,\n                       chunk_input_length: int,\n                       chunk_output_length: int,\n                       validset_ratio: float = 0.2,\n                       testset_ratio: float = 0.1):\n    \n    dataset_length = df.shape[0]\n    timespan = df.shape[1]\n\n    testset_start_index = timespan - int(timespan * testset_ratio)\n    validset_start_index = testset_start_index - int(timespan * validset_ratio)\n\n    training_set = df.iloc[:, :validset_start_index]\n    validation_set = df.iloc[:, validset_start_index-chunk_input_length:testset_start_index]\n    test_set = df.iloc[:, testset_start_index-chunk_input_length:]\n\n    return training_set, validation_set, test_set\n\n\ndef array_to_seq(arr, date_index):\n    ts_sequence = []\n    for i in tqdm(range(len(arr))):\n        ts_sequence.append(TimeSeries.from_times_and_values(date_index, arr[i, :]))\n\n    return ts_sequence","5ae10716":"training_set, validation_set, test_set = walk_forward_split(df=df,\n                                                            chunk_input_length=CHUNK_INPUT_LENGTH,\n                                                            chunk_output_length=CHUNK_OUTPUT_LENGTH,\n                                                            validset_ratio=0.15,\n                                                            testset_ratio=0.0)                                                            ","54752631":"class LocalMinMaxScaler(TransformerMixin):\n\n    def __init__(self, minimum=None):\n        self.minimum = minimum\n\n    def fit(self, X, y=None):\n        if isinstance(X, pd.DataFrame):\n              X = X.values\n\n        self.min_ = X.min(axis=1).reshape(-1, 1) if self.minimum is None else self.minimum\n        self.max_ = X.max(axis=1).reshape(-1, 1)\n\n        return self\n\n    def transform(self, X, y=None):\n        return np.divide(X - self.min_, self.max_ - self.min_, out=np.zeros_like(X), where=(self.max_ - self.min_) != 0.0)\n\n    def inverse_transform(self, X):\n        return X * (self.max_ - self.min_) + self.min_","82896004":"class VariableSizeImputer(TransformerMixin):\n    def __init__(self, fill_value=0.0):\n        self.fill_value = fill_value\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill_value)\n\n    def inverse_transform(self, X):\n        return X","2472dc14":"imputer = VariableSizeImputer()\nscaler = LocalMinMaxScaler(minimum=0.1)\n#log_mapper = FunctionTransformer(func=np.log1p, inverse_func=np.expm1)","36042d7c":"pipe = make_pipeline(imputer,\n                     #log_mapper,\n                     scaler)\npipe.fit(training_set)","31f6411e":"CACHE_ON = False\n\ndef transform_and_cache(dataset, columns, fpath):\n    if CACHE_ON:\n        if os.path.exists(fpath):\n            return joblib.load(fpath)\n        else:\n            ret = array_to_seq(pipe.transform(dataset), columns)\n            joblib.dump(ret, fpath)\n            return ret\n    else:\n        return array_to_seq(pipe.transform(dataset), columns)\n        ","352675a1":"training_sequence = transform_and_cache(\n    training_set, \n    training_set.columns, \n    os.path.join(CACHEDIR, 'training_sequence.pickle'))","bf031f02":"validation_sequence = transform_and_cache(\n    validation_set, \n    validation_set.columns,\n    os.path.join(CACHEDIR, 'validation_sequence.pickle'))","66ee7878":"test_sequence = transform_and_cache(test_set, \n                                    test_set.columns,\n                                    os.path.join(CACHEDIR,'test_sequence.pickle'))","631ac731":"from darts.utils.data.horizon_based_dataset import HorizonBasedDataset\n\ntraining_dataset = HorizonBasedDataset(target_series = training_sequence,\n                                       output_chunk_length=CHUNK_OUTPUT_LENGTH,\n                                       lh=(1,2),\n                                       lookback=LOOKBACK)\n\nvalidation_dataset = HorizonBasedDataset(target_series = validation_sequence,\n                                         output_chunk_length=CHUNK_OUTPUT_LENGTH,\n                                         lh=(1,2),\n                                         lookback=LOOKBACK)","f9167709":"def divide_no_nan(a, b):\n    result = a \/ b\n    result[result != result] = .0\n    result[result == np.inf] = .0\n    return result\n\ndef smape_loss(forecast, target):\n    return 200 * torch.mean(divide_no_nan(torch.abs(forecast - target),\n                                          torch.abs(forecast.data) + torch.abs(target.data)))","22c128d7":"N_EPOCHS = 25\n\n\nNUM_STACKS=1\nNUM_BLOCKS=4\nNUM_LAYERS=4\nLAYER_WIDTH=64\nMODEL_NAME = 'NBEATS'\nBATCH_SIZE = 1024\n\nmodel = NBEATSModel(input_chunk_length=CHUNK_OUTPUT_LENGTH*LOOKBACK,\n                    output_chunk_length=CHUNK_OUTPUT_LENGTH,\n                    nr_epochs_val_period=1,\n                    num_stacks=NUM_STACKS,\n                    num_blocks=NUM_BLOCKS,\n                    num_layers=NUM_LAYERS,\n                    layer_widths=LAYER_WIDTH,\n                    generic_architecture=True,\n                    model_name=MODEL_NAME,\n                    batch_size=BATCH_SIZE,\n                    #log_tensorboard=True,\n                    n_epochs=N_EPOCHS,\n                    loss_fn=smape_loss)\n\n# model fitting\nprint(\"STARTING TRAINING..\")\nmodel.fit_from_dataset(train_dataset=training_dataset, val_dataset=validation_dataset,verbose=True)","f87ec508":"KEYS = '..\/input\/web-traffic-time-series-forecasting\/key_2.csv.zip'\ndef read_keys():\n    keys = pd.read_csv(KEYS, compression='zip')\n    id_dict = {}\n\n    for page, page_id in zip(keys['Page'], keys['Id']):\n        id_dict.update({page:page_id})\n\n    return id_dict","7c0a089e":"id_dict = read_keys()","4fce591e":"#model = torch.load('\/content\/drive\/MyDrive\/NBEATS_2_stacks\/checkpoint_24.pth.tar')\npred = model.predict(n=CHUNK_OUTPUT_FINAL_LENGTH, series=test_sequence)","c0517eab":"def map_page_and_time_to_id(page, date):\n    return id_dict[page+'_'+str(date)]","4180660f":"X = np.array([p.values().squeeze() for p in pred])","949b788a":"post_proc_pred = pipe.inverse_transform(X)","26c410f9":"submission = []\ntime_index = pred[0].time_index()\n\nfor page, ts in tqdm(zip(training_set.index, post_proc_pred)):\n    for timestamp, value in zip(time_index[2:], ts[2:]):\n        # value[0] since values() returns an array of arrays\n        submission.append([map_page_and_time_to_id(page, timestamp.date()), value])\n\nsubmission_df = pd.DataFrame.from_records(submission, columns=['Id', 'Visits'])\n\nassert len(submission_df) == 8993906\n\n# saving the output file\nsubmission_df.to_csv('submission.csv', index=False)\nprint('FILE SAVED')","57e5a386":"kaggle competitions submit -c web-traffic-time-series-forecasting -f submission.csv -m \"NBEATS 1 stack, 4 blocks, 4 layers, 64 layer width, smape loss\"","03508e55":"## Post-processing","c3cee892":"## Reading the mapping for the competition submission format","501cd937":"## Predicting","40eddf2b":"# Predictions and Post-Processing","b14c3c14":"### Fixing random seeds","5b8bd4a3":"# Loading data","ffc5b5fb":"# Training ","b67b131c":"# Config"}}