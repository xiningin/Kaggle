{"cell_type":{"39a0f5ac":"code","b148f66f":"code","e2a1742b":"code","ff9a2121":"code","d6409c61":"code","eb6bc76c":"code","49b7548a":"code","eeab9028":"code","e520e2da":"code","c19a2c25":"code","db83c2ca":"code","3ec85d4c":"code","51078a7c":"code","41cfc7e2":"code","055f1ef2":"code","a8b3e11f":"code","4a7c7008":"code","a7b0c24d":"code","a7be90cd":"code","7b823b4d":"code","3953805f":"code","88aacbf5":"code","8242fd24":"code","fc7577c9":"code","cdb0ba78":"code","12ed8f2d":"markdown","773a44d6":"markdown","34b96d31":"markdown","8972f4b2":"markdown","13d7aed9":"markdown","5d42c132":"markdown","67be83f0":"markdown","2a710b68":"markdown","0d00b422":"markdown","2f5935f5":"markdown","9a632937":"markdown"},"source":{"39a0f5ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b148f66f":"import pandas as pd\nsample_submission = pd.read_csv(\"..\/input\/allstate-claims-severity\/sample_submission.csv\")\ntest_data = pd.read_csv(\"..\/input\/allstate-claims-severity\/test.csv\")\ntrain_data = pd.read_csv(\"..\/input\/allstate-claims-severity\/train.csv\")","e2a1742b":"print(\"Train data dimensions: \", train_data.shape)\nprint(\"Test data dimensions: \", test_data.shape)","ff9a2121":"#Exploring the train_data\ntrain_data.head()","d6409c61":"print('Number of missing values', train_data.isnull().sum().sum())","eb6bc76c":"#Exploring the data stastically \ntrain_data.describe()","49b7548a":"#Exploring the columns of data\ntrain_data.columns","eeab9028":"test_data.columns","e520e2da":"train_data.info()","c19a2c25":"# Counting the Feature of train_data\ncont_Featureslist = []\nfor colName,x in train_data.iloc[1,:].iteritems():\n    #print(x)\n    if(not str(x).isalpha()):\n        cont_Featureslist.append(colName)","db83c2ca":"print(cont_Featureslist)","3ec85d4c":"cont_Featureslist.remove('id')","51078a7c":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline ","41cfc7e2":"# To find correlation between features and target feature\ncorrelationMatrix = train_data[cont_Featureslist].corr().abs()\n\nplt.subplots(figsize=(15, 10))\nsns.heatmap(correlationMatrix,annot=True)\n\n# Mask unimportant features\nsns.heatmap(correlationMatrix, mask=correlationMatrix < 1, cbar=False)\nplt.show()","055f1ef2":"plt.figure(figsize=(15,10))\nsns.distplot(train_data[\"loss\"])\nsns.boxplot(train_data[\"loss\"])","a8b3e11f":"plt.figure(figsize=(12,8))\nsns.distplot(np.log1p(train_data[\"loss\"]))","4a7c7008":"catCount = sum(str(x).isalpha() for x in train_data.iloc[1,:])\nprint(\"Number of categories: \",catCount)","a7b0c24d":"cat_Featureslist = []\nfor colName,x in train_data.iloc[1,:].iteritems():\n    if(str(x).isalpha()):\n        cat_Featureslist.append(colName)","a7be90cd":"print(train_data[cat_Featureslist].apply(pd.Series.nunique))","7b823b4d":"from sklearn.preprocessing import LabelEncoder\nfor cf1 in cat_Featureslist:\n    le = LabelEncoder()\n    le.fit(train_data[cf1].unique())\n    train_data[cf1] = le.transform(train_data[cf1])","3953805f":"train_data.head(5)","88aacbf5":"featureslist = []\nfor colName,x in train_data.iloc[1,:].iteritems():\n    #print(x)\n    if(not str(x).isalpha() or str(x).isalpha):\n        featureslist.append(colName)","8242fd24":"featureslist.remove('id')","fc7577c9":"# Import nessery models\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import train_test_split\n\nX = train_data[featureslist]\ny = train_data.loss\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\ndtr_model = DecisionTreeRegressor()\ndtr_model.fit(X_train, y_train)\ny_pred = dtr_model.predict(X_test)\nval_mae = mean_absolute_error(y_pred, y_test)\nmse_test = MSE(y_test, y_pred)\nrmse_test = mse_test**(1\/2)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\nprint('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))","cdb0ba78":"from sklearn.ensemble import RandomForestRegressor\nrf_model = RandomForestRegressor(random_state=1)\nrf_model.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\nval_mae = mean_absolute_error(y_pred, y_test)\nmse_test = MSE(y_test, y_pred)\nrmse_test = mse_test**(1\/2)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\nprint('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))","12ed8f2d":"There are 116 categories with non alphanumeric values, most of the machine learning algorithms doesn't work with alpha numeric values. So, lets convert it into numeric values.","773a44d6":"**Chicking the Root mean squared error for RandomForestRegressor method**","34b96d31":"After checking the model with DecisionTreeRegressor model we get mean absolute error as \"2\" and RMSE as \"122.531\" which is greater than LinearRegression Model.","8972f4b2":"So we got normal distribution by applying logarithm on loss function.\n\nFinally we got normal distribution, so we can train model using target feature as log of loss. So there is no need to remove outliers.","13d7aed9":"For RandomForestRegeressor model we got RMSE as \"64.548\".","5d42c132":"**Making Prediction by training model**","67be83f0":"**Finding Root mean squred error for DecisionTreeRegressor**","2a710b68":"Here, we can see loss is highly right skewed data. This happened because there are many outliers in the data. Lets apply log to see if we can get normal distribution. ","0d00b422":"Conver categorical string values to numeric values","2f5935f5":"**Plotting Corelation between countinues features and target **","9a632937":"By seeing both Train Data and Test Data there are 132 columns in train data and 131 column in test data. 'Loss' column is missing in test data indicating that it is the **target**."}}