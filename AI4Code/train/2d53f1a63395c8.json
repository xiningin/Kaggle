{"cell_type":{"e16a95c2":"code","2277787b":"code","ae6e2bf7":"code","a8dcd15e":"code","492ffb14":"code","c63b6dee":"code","7855b5a0":"code","ca5c5196":"code","c12fa23b":"code","c331b23f":"code","26f92bbd":"code","9e3ae66b":"code","5c19aa79":"code","70cb684e":"code","5e9ddf38":"code","4bdbed13":"code","0f63ee0d":"code","f1c11e1c":"code","5d00e27f":"code","5fc3002d":"code","59c5fb09":"code","326442e2":"code","6f4f9da5":"code","e2d951df":"code","d5597e7b":"code","683caa95":"code","b30f657c":"code","de41ba47":"code","94cb9804":"code","6c395644":"code","2cc2fe7a":"code","a4a5f708":"code","3376338f":"code","3508a576":"code","172893e4":"code","0244c497":"code","f1bea44f":"code","0bfce1be":"code","f31ed2ff":"code","6f46fe1b":"code","6d93168e":"code","8f8fa0ad":"code","feda5c89":"code","b5ba736e":"code","247d9148":"code","9b50ae93":"code","8091dc91":"code","3d255c5a":"code","a0042535":"code","5fa2326e":"code","3fa043b0":"code","d457dc8f":"code","db17a8d6":"code","bfe9ef31":"code","4015870b":"code","ade4f2b7":"code","f5521dec":"code","762c95c7":"code","1d6718ab":"code","38e24ae1":"code","4aeda7d9":"code","9f607456":"code","9b8ade6e":"code","dd8504b0":"code","eaa8e3b3":"code","87727aee":"code","00eae736":"code","e73e246c":"code","1f86b8c3":"code","85ddeec1":"code","7e912429":"code","74c9ab19":"code","cea882e0":"code","3da0670f":"code","4500c2bd":"code","dbb91ee9":"code","4cb29fc2":"code","2f621cff":"code","9dc5e6c2":"code","7267acaa":"code","8601db48":"code","08f57ac2":"code","98e25631":"code","d97af135":"code","e227eee1":"code","ab3614fa":"code","41519e2b":"code","751418a1":"code","e2aece63":"code","63462181":"code","121d61c8":"code","c5527aca":"code","3661d880":"code","8ac96fdb":"code","6c5c01f1":"code","bff70d81":"code","ea9213f7":"code","d6d93247":"code","13b3a9a3":"code","a0b61c76":"code","d2b96f94":"code","4a9023bd":"code","bfa80e6d":"code","51b41c19":"code","0ac83586":"code","9ced6683":"code","2a635376":"code","9aa1c878":"code","0aa7b5e3":"code","0c912fbc":"code","4ed56a94":"code","fe5eaa9b":"code","ead05c78":"code","b9906bd0":"code","b94812ab":"code","f48887f9":"code","f6aaeba4":"code","50287d31":"code","e7d795de":"code","717ff065":"code","7cd0eb7c":"code","2132142e":"code","942b9622":"code","266ebdfc":"code","c89e79cb":"code","5caeace5":"code","7d622ef7":"code","39ffe493":"code","68321b29":"code","e2bf7f8f":"code","6b29e651":"code","a2b05c41":"code","e45de5cf":"code","9fecc677":"code","cc2b2c2f":"code","ef70d099":"code","5c6f9d36":"code","73cfd5f9":"code","19697a23":"code","28029b21":"code","ea2e460e":"code","38b3b822":"code","7e9e72ce":"code","2297c46d":"code","b3c86e1e":"code","595ab53b":"code","e441056e":"code","8f955061":"code","82e28b1d":"code","4e1f0b48":"code","4a94c072":"code","18b23536":"code","b196884f":"code","3ba9151f":"code","b54b73ae":"code","660be23f":"code","614f5e4e":"code","e246c57d":"code","145f6c35":"code","459df224":"code","237951a1":"code","1faac632":"code","f9411cf8":"code","1bfab1e3":"code","c2f17ffc":"code","a4c76807":"code","bbad4b35":"code","1e9d4634":"code","ad26281d":"code","6084e02a":"code","10a48968":"code","fa2fd1a6":"code","e6110172":"code","186a4a5b":"code","b56bc2b2":"code","b57921a1":"code","e7d2bb7d":"code","5c5c4d49":"code","4f0017be":"code","980fa001":"code","624a44b1":"code","b198747a":"code","7e1f37cb":"code","bde1259a":"code","69219242":"code","403e735e":"code","b597f9fe":"code","8c8bf967":"code","4bce30a4":"code","3f518fb7":"code","ae1df866":"code","9aa3b89a":"code","91920f25":"code","12610194":"code","2da678a4":"code","466b923b":"code","741df0a7":"code","5114dac5":"code","e949efbb":"code","51b550be":"code","2248468b":"code","d5bd71ee":"code","ddbc9dc7":"code","d931eb4c":"code","e89005be":"code","6ce79c48":"code","b55cb96b":"code","f123082a":"code","992f6909":"code","7c94bb1b":"code","fafa54f3":"code","347519a8":"code","5c44afbc":"code","2e9e10f3":"code","2dbc094a":"code","ffafc031":"code","0c5012bd":"code","f8ec28d3":"code","131d01ee":"code","ea53d7db":"code","dd5248ee":"code","a56dc70f":"code","1c4942b6":"code","4149b84c":"code","cf59bdb1":"code","dfa0d0f5":"code","0ae20fcf":"code","be2e25cb":"code","e9fb7032":"code","098b61f1":"code","6e499c7a":"code","88b0509c":"code","5956edec":"code","d7b18eb5":"code","38c2fb6b":"code","3074e6b1":"code","60243676":"code","8e7db7ff":"code","aa773348":"code","cef1fa56":"code","10b77c22":"code","d0fc2641":"code","ae0499f3":"code","767655f5":"code","acbc5cf4":"code","32b62bf4":"code","18deffc9":"code","efcc105c":"code","2f0f0a1a":"code","992dce6a":"code","3559983b":"code","6525bc45":"code","5221cf6f":"code","16f51cf8":"code","31c422da":"code","3e506a5a":"code","04fe1e3a":"code","a351fa0d":"code","96846a77":"code","0229f6ec":"code","fa4f2381":"code","ca9ca6a9":"code","279e7222":"code","7128a6c6":"code","23416d16":"code","72a5a934":"code","f7b36bb2":"code","9c17ca10":"code","02fd6339":"code","81b6017a":"code","a5302dd0":"code","990e1741":"code","104cb80f":"code","7c779a20":"code","39cf86a6":"code","ac154c59":"code","aff4ca9d":"code","d00cf16b":"code","d5c0954e":"code","6be6ef17":"code","b4b5d05a":"code","36fd9a1a":"code","19790b59":"code","b8bf06af":"code","9a8c634c":"code","f1810a76":"code","5631a5e6":"code","f1e99ed1":"code","ccffc7ab":"code","5c86953c":"code","fe769591":"code","34096fa6":"code","28332230":"code","4275a3a6":"code","7c3633ce":"code","06e2c336":"code","029a2b68":"code","772810e7":"code","8e693803":"code","81f4b0eb":"code","5a814e07":"code","d0e0df98":"code","f5882fac":"code","c536103b":"code","2d18a801":"code","84de359e":"code","4cb4d620":"code","a36f3ce3":"code","f3461b32":"code","3807afe1":"code","310a8930":"code","2a91d12f":"code","62d0ed74":"code","04eb3364":"code","ab1dc285":"code","732028a3":"code","bdd51322":"code","0cad5d46":"code","ad1f35fa":"code","d5e0fa94":"code","181bb1ee":"code","a5df9437":"code","71e53471":"code","91134397":"code","2b64385b":"code","ef808ccf":"code","e6439214":"code","88688c6d":"code","da7afb10":"code","ca217754":"code","b30e7711":"code","c9177d3b":"code","0bcc2b69":"code","79fcb3e1":"code","9b27f42d":"code","478beb98":"code","e7cdcde5":"code","01ec47b2":"code","dce0993b":"code","2bd14447":"code","0b4138f2":"code","4f74df19":"code","33eaed10":"code","c5adfe25":"code","8f2d357a":"code","9369b4f6":"code","14889a02":"code","0ba90fff":"code","db0d838b":"code","846d1ebf":"code","d72fa30c":"code","221033dc":"code","3c803dce":"code","28b82bdc":"code","a826d2f0":"code","712b7020":"code","794ece2e":"code","ce39cf9e":"code","0d3e37ba":"code","83672c39":"code","fff83a82":"code","e4cb537d":"code","7a37b1fb":"code","c80eeb6e":"code","f2b16866":"code","db3a1d4b":"code","a2b42f3f":"code","d25fffaa":"code","e2c21c6a":"code","ad172fd3":"code","c8413d93":"code","26bc52e5":"code","b14cde4b":"code","57da019c":"code","97c6046c":"code","a76e2569":"code","ace0f841":"code","6df83c12":"code","ab1eed73":"code","3c34c22a":"code","2d787203":"code","551c54fb":"code","f61aaaef":"code","33333122":"code","4adbb21e":"code","1f562dd9":"code","38bc848d":"code","185245cc":"code","70af1378":"code","2e78d8c7":"code","f0786ed5":"code","12ad77f6":"code","369c7a8e":"code","8f1070b2":"code","576ad22e":"code","263b12c8":"code","57b9f562":"code","0c727426":"code","647d6404":"code","b8330cd3":"code","53b0caf2":"code","67b12b8f":"code","77e89f14":"code","52b65f17":"code","177e23da":"code","d42b7fa9":"code","c76c5a05":"code","d27f63b2":"code","d1abf158":"code","5695e39f":"code","9efe826a":"code","cc536312":"code","532a55ed":"code","df3eea33":"code","117df149":"code","bcbc1128":"code","4909483b":"code","81035b78":"code","2d92a600":"code","44e2e6ba":"code","8a3788fd":"code","8c9b8890":"code","65e2a2c5":"code","8f45e5ac":"code","2df4ed08":"code","62433521":"code","f5dd6b6b":"code","056bef9f":"code","1c8290b7":"code","f41b7992":"code","893ec28c":"code","8ceb567c":"code","92ca9b25":"code","ef4382a4":"code","36ca9d9e":"code","8dd46a3d":"code","cfdf6e5f":"code","0f667e50":"code","cae68c72":"code","50c7f43c":"code","76a7d87c":"code","4aaa11c6":"code","25eed12b":"code","44912421":"code","98c245e8":"code","c8d1404f":"code","de09f76d":"code","93f2197e":"code","faa1f581":"code","8d747b99":"code","2918a2a6":"markdown","9a5b1bb6":"markdown","364c6138":"markdown","124f86f5":"markdown","44dfaeba":"markdown","dfbf2077":"markdown","196326bf":"markdown","2c459aff":"markdown","81c5536b":"markdown","223f9e14":"markdown","ceb6f0c9":"markdown","6a0d270a":"markdown","37bacfb1":"markdown","c921b8a5":"markdown","cb57d3d4":"markdown","fb8f4c3e":"markdown","4f88bb3d":"markdown","d72c348b":"markdown","863c9814":"markdown","595ba866":"markdown","d1449e78":"markdown","740a3f03":"markdown","abc712fa":"markdown","3dc4ff89":"markdown","358f4c97":"markdown","ff0d2c5c":"markdown","d6e3de1d":"markdown","979a1ac8":"markdown","0eab3ffe":"markdown","5f64f065":"markdown","47150804":"markdown","fc72f9b0":"markdown","129ba8e2":"markdown","3eab1048":"markdown","20ce3ffa":"markdown","4476a297":"markdown","4eb9976f":"markdown","0bb74feb":"markdown","62b3963a":"markdown","abdb0558":"markdown","9dcb0d90":"markdown","6eef39c8":"markdown","7d6e5870":"markdown","c3230e27":"markdown","9b391d05":"markdown","73371514":"markdown","b1c7f4fd":"markdown","224b8245":"markdown","467342d9":"markdown","c1e0d290":"markdown","aa3069da":"markdown","fba6e300":"markdown","9ebab751":"markdown","268ac04b":"markdown","1f56d84b":"markdown","a3eadcaa":"markdown","2eabded4":"markdown","620a3a79":"markdown","6e44aa11":"markdown","840c6a62":"markdown","9f3ccd62":"markdown","9f3d0eb1":"markdown","5eb4ed9a":"markdown","802e8498":"markdown","56f9d526":"markdown","d9e8cb38":"markdown","fa54acc2":"markdown","ae958e0f":"markdown","c74087cc":"markdown","1ed8ae9c":"markdown","370fc679":"markdown","0c040574":"markdown","67689769":"markdown","a797d10d":"markdown","0c25816a":"markdown","c872f5eb":"markdown","4f5c6dda":"markdown","8871cb0e":"markdown","77f9b468":"markdown","d2861216":"markdown","e3daf718":"markdown","e23c4ed4":"markdown","4f80d717":"markdown","70676bba":"markdown","0c4209e3":"markdown","630d65e1":"markdown","70441a11":"markdown","2dabfef0":"markdown","69e47590":"markdown","e918b744":"markdown","6b28201a":"markdown","7c6fc2ba":"markdown","6930549d":"markdown","a358b334":"markdown","89b59f1c":"markdown","45d59b35":"markdown","269a3fda":"markdown","0bf05d28":"markdown","47ed3ca4":"markdown","5e421e21":"markdown","f676e3df":"markdown","fd0e77ea":"markdown","b269eb44":"markdown","e2663a7c":"markdown","05b271ac":"markdown","a9df3b1e":"markdown","f786b8d1":"markdown","03bbc88a":"markdown","57ce0c52":"markdown","4e74277c":"markdown"},"source":{"e16a95c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2277787b":"# Pandas is a newer package built on top of NumPy, and provides an efficient implementation of a DataFrame. DataFrames are essentially multidimen\u2010\n# sional arrays with attached row and column labels, and often with heterogeneous types and\/or missing data. \n\n# Once Pandas is installed, you can import it and check the version\n\nimport pandas\npandas.__version__\n\n# Just as we generally import NumPy under the alias np, we will import Pandas under the alias pd:\n\nimport pandas as pd","ae6e2bf7":"# The Pandas Series Object\n\n# A Pandas Series is a one-dimensional array of indexed data. It can be created from a list or array as follows\n\ndata=pd.Series([0.25,0.5,0.75,1])\ndata\n\n# As we see in the preceding output, the Series wraps both a sequence of values and a sequence of indices, which we can access with the values and index attributes. The\n# values are simply a familiar NumPy array\n","a8dcd15e":"data.values","492ffb14":"data.shape","c63b6dee":"data.index","7855b5a0":"# Like with a NumPy array, data can be accessed by the associated index via the familiar Python square-bracket notation\n\ndata[1]","ca5c5196":"data[3]","c12fa23b":"data[1:3]","c331b23f":"# Series as generalized NumPy array\n# From what we\u2019ve seen so far, it may look like the Series object is basically inter\u2010changeable with a one-dimensional NumPy array. The essential difference is the pres\u2010\n# ence of the index: while the NumPy array has an implicitly defined integer index used to access the values, the Pandas Series has an explicitly defined index associated with the values.\n\ndata1=pd.Series([0.75,.50,.75,1],index=['a','b','c','d'])\ndata1","26f92bbd":"data1['b']","9e3ae66b":"# We can even use noncontiguous or nonsequential indices\n\ndata2=pd.Series([0.25,.5,0.75,1],index=[2,5,7,3])\ndata2","5c19aa79":"# Series as specialized dictionary\n\nPopulation_dict={'California':123543,'Texas':87451,'Boston':986734,'Newyork':907856}\n\nPopulation=pd.Series(Population_dict)\n\nPopulation","70cb684e":"# By default, a Series will be created where the index is drawn from the sorted keys.\n# From here, typical dictionary-style item access can be performed\n\nPopulation['California']","5e9ddf38":"# Unlike a dictionary, though, the Series also supports array-style operations such as slicing\n\nPopulation['California':'Texas']","4bdbed13":"pd.Series([2,4,7])","0f63ee0d":"# data can be a scalar, which is repeated to fill the specified index\n\npd.Series(5, index=[100, 200, 300])","f1c11e1c":"# data can be a dictionary, in which index defaults to the sorted dictionary keys:\n\npd.Series({2:'a', 1:'b', 3:'c'})","5d00e27f":"# In each case, the index can be explicitly set if a different result is preferred:\n\npd.Series({2:'a',4:'b',5:'c'},index=[3,2])","5fc3002d":"# DataFrame can be thought of either as a generalization of a NumPy array, or as a specialization of a Python dictionary. \n\narea_dict={'California': 12345,'Boston':6745,'newyork':9078,'newtown':23126}\n\narea=pd.Series(area_dict)\n\narea","59c5fb09":"# Now that we have this along with the population Series from before, we can use a dictionary to construct a single two-dimensional object containing this information:\n\nstates=pd.DataFrame({'population': Population,'area': area})\nstates\n\n","326442e2":"# Like the Series object, the DataFrame has an index attribute that gives access to the index labels\nstates.index","6f4f9da5":"# Additionally, the DataFrame has a columns attribute, which is an Index object holding the column labels\nstates.columns\n\n# Thus the DataFrame can be thought of as a generalization of a two-dimensional NumPy array, where both the rows and columns have a generalized index for access\u2010ing the data.","e2d951df":"# DataFrame as specialized dictionary\n\n# Similarly, we can also think of a DataFrame as a specialization of a dictionary. Where a dictionary maps a key to a value, a DataFrame maps a column name to a Series of\n# column data. For example, asking for the 'area' attribute returns the Series object containing the areas we saw earlier\n\nstates['area']\n","d5597e7b":"# Notice the potential point of confusion here: in a two-dimensional NumPy array, data[0] will return the first row. For a DataFrame, data['col0'] will return the first column.\n\n# Constructing DataFrame objects\n\n# A Pandas DataFrame can be constructed in a variety of ways. Here we\u2019ll give several examples.\n\n# From a single Series object. A DataFrame is a collection of Series objects, and a singlecolumn DataFrame can be constructed from a single Series\n\npd.DataFrame(Population,columns=['Population'])\n","683caa95":"# From a list of dicts. Any list of dictionaries can be made into a DataFrame. We\u2019ll use a simple list comprehension to create some data\n\ndata=[{'a':i,'b':2*i} for i in range(3)]\npd.DataFrame(data)","b30f657c":"# Even if some keys in the dictionary are missing, Pandas will fill them in with NaN (i.e.,\u201cnot a number\u201d) values\n\npd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])","de41ba47":"# From a dictionary of Series objects, As we saw before, a DataFrame can be constructed from a dictionary of Series objects as well\n\npd.DataFrame({'Population': Population,'area': area})\n\n\n","94cb9804":"# From a two-dimensional NumPy array\n\n# Given a two-dimensional array of data, we can create a DataFrame with any specified column and index names. If omitted, an integer index will be used for each\n\npd.DataFrame(np.random.rand(3,2),columns=['foo','bar'],index=['a','b','c'])\n","6c395644":"# From a NumPy structured array\n\n# A Pandas DataFrame operates much like a structured array, and can be created directly from one\n\nA=np.zeros(3,dtype=[('A','i8'),('B','f8')])\n\npd.DataFrame(A)","2cc2fe7a":"# The Pandas Index Object\n\nind=pd.Index([2,3,5,7,11])\nind","a4a5f708":"# Index as immutable array\n\n# The Index object in many ways operates like an array. For example, we can use stan\u2010 dard Python indexing notation to retrieve values or slices\n\nind[1]","3376338f":"ind[::2]","3508a576":"# Index objects also have many of the attributes familiar from NumPy arrays:\n\nprint(ind.size,ind.shape,ind.ndim,ind.dtype)","172893e4":"# Data Indexing and Selection\n\nimport pandas as pd\ndata = pd.Series([0.25,0.50,0.75,1],index=['a','b','c','d'])\ndata","0244c497":"data['b']","f1bea44f":"# We can also use dictionary-like Python expressions and methods to examine the keys\/indices and values\n'a' in data","0bfce1be":"data.keys()","f31ed2ff":"list(data.items())","6f46fe1b":"# Series objects can even be modified with a dictionary-like syntax. Just as you can extend a dictionary by assigning to a new key, you can extend a Series by assigning to a new index value\n\ndata['e']=1.25\ndata","6d93168e":"# Series as one-dimensional array\n\n# A Series builds on this dictionary-like interface and provides array-style item selec\u2010 tion via the same basic mechanisms as NumPy arrays\u2014that is, slices, masking, and\n# fancy indexing. Examples of these are as follows\n\n# # slicing by explicit index\n\ndata['a':'c']","8f8fa0ad":"# slicing by implicit integer index\n\ndata[0:2]","feda5c89":"# # masking\ndata[(data>0.3) & (data<0.8)]","b5ba736e":"# # fancy indexing\n\ndata[['a', 'e']]","247d9148":"# Indexers: loc, iloc, and ix\n\ndata = pd.Series(['a', 'b', 'c'], index=[1, 3, 5])\ndata","9b50ae93":"# explicit index when indexing\n\ndata[1]","8091dc91":"# implicit index when slicing\ndata[1:3]","3d255c5a":"# First, the loc attribute allows indexing and slicing that always references the explicit index\n\ndata.loc[1]","a0042535":"data.loc[1:3]","5fa2326e":"# The iloc attribute allows indexing and slicing that always references the implicit Python-style index:\ndata.iloc[1:3]","3fa043b0":"data.iloc[1]","d457dc8f":"# DataFrame as a dictionary\n\narea=pd.Series({'California': 423967, 'Texas': 695662,'New York': 141297, 'Florida': 170312,'Illinois': 149995})\npop = pd.Series({'California': 38332521, 'Texas': 26448193,'New York': 19651127, 'Florida': 19552860,'Illinois': 12882135})\n\ndata=pd.DataFrame({'area':area,'pop':pop})\ndata\n\n","db17a8d6":"data['area']","bfe9ef31":"data.area","4015870b":"data.area is data['area']","ade4f2b7":"data.pop is data['pop']","f5521dec":"# Like with the Series objects discussed earlier, this dictionary-style syntax can also be used to modify the object, in this case to add a new column:\n\ndata['density']=data['pop']\/data['area']\ndata\n","762c95c7":"# DataFrame as two-dimensional array\n\n# As mentioned previously, we can also view the DataFrame as an enhanced twodimensional array. We can examine the raw underlying data array using the values attribute\n\ndata.values","1d6718ab":"# we can transpose the full DataFrame to swap rows and columns:\ndata.T\n","38e24ae1":"# When it comes to indexing of DataFrame objects, however, it is clear that the dictionary-style indexing of columns precludes our ability to simply treat it as a\n# NumPy array. In particular, passing a single index to an array accesses a row\n\ndata.values[0]","4aeda7d9":"data['area']","9f607456":"data.iloc[:3,:2]   # iloc= index location","9b8ade6e":"data.loc[:'Illinois', :'pop']\n","dd8504b0":"# in the loc indexer we can combine masking and fancy indexing as in the following:\n\ndata.loc[data.density>100,['pop','density']]","eaa8e3b3":"# Any of these indexing conventions may also be used to set or modify values; this is done in the standard way that you might be accustomed to from working with NumPy\n\ndata.iloc[0,2]=90\ndata","87727aee":"# Additional indexing conventions\n\n# while index\u2010ing refers to columns, slicing refers to rows:\n\ndata['Florida':'Illinois']","00eae736":"# Such slices can also refer to rows by number rather than by index:\n\ndata[1:3]","e73e246c":"# direct masking operations are also interpreted row-wise rather than column-wise:\n\ndata[data.density>100]","1f86b8c3":"# Ufuncs: Index Preservation\n# Let\u2019s start by defining a simple Series and DataFrame on which to demonstrate this:\n\nimport pandas as pd\nimport numpy as np\n\nrng = np.random.RandomState(42)\nser = pd.Series(rng.randint(0, 10, 4))\nser\n\n","85ddeec1":"df = pd.DataFrame(rng.randint(0, 10, (3, 4)),columns=['A', 'B', 'C', 'D'])\ndf","7e912429":"# If we apply a NumPy ufunc on either of these objects, the result will be another Pandas object with the indices preserved\n\nnp.exp(ser)","74c9ab19":"# Or, for a slightly more complex calculation\nnp.sin(df*np.pi\/4)","cea882e0":"# UFuncs: Index Alignment\n\n# For binary operations on two Series or DataFrame objects, Pandas will align indices in the process of performing the operation. This is very convenient when you are\n# working with incomplete data, \n\narea=pd.Series({'Alaska':23456,'California':89675,'Boston':89234,'Jaandar':12345},name='area')\npopulation=pd.Series({'California': 38332521, 'Texas': 26448193,'New York': 19651127}, name='population')\n\nprint(area,population)","3da0670f":"area\/population\n\n# The resulting array contains the union of indices of the two input arrays, which we could determine using standard Python set arithmetic on these indices","4500c2bd":"area.index","dbb91ee9":"population.index","4cb29fc2":"area.index | population.index   # union","2f621cff":"# Any item for which one or the other does not have an entry is marked with NaN, or \u201cNot a Number,\u201d which is how Pandas marks missing data\n\n#  This index matching is imple\u2010mented this way for any of Python\u2019s built-in arithmetic expressions; any missing values are filled in with NaN by default:\n\nA=pd.Series([2,4,6],index=[0,1,2])\nB=pd.Series([6,7,8],index=[1,2,3])\nprint(A+B)","9dc5e6c2":"A.add(B,fill_value=0)","7267acaa":"# Index alignment in DataFrame\n\n# A similar type of alignment takes place for both columns and indices when you are performing operations on DataFrames:\n\nA=pd.DataFrame(rng.randint(0,20),(2,2),columns=list('AB'))\nA","8601db48":"B = pd.DataFrame(rng.randint(0, 10, (3, 3)),columns=list('BAC'))\nB","08f57ac2":"A+B","98e25631":"fill=A.stack().mean()\nA.add(B,fill_value=fill)","d97af135":"# Ufuncs: Operations Between DataFrame and Series\n\n# When you are performing operations between a DataFrame and a Series, the index and column alignment is similarly maintained. Operations between a DataFrame and\n# a Series are similar to operations between a two-dimensional and one-dimensional NumPy array. Consider one common operation, where we find the difference of a two-dimensional array and one of its rows:\n\nA=rng.randint(10,size=(3,4))\nA","e227eee1":"A-A[0]","ab3614fa":"# In Pandas, the convention similarly operates row-wise by default:\n\ndf=pd.DataFrame(A,columns=list('QRST'))\ndf-df.iloc[0]\n\n# Note that these DataFrame\/Series operations, like the operations discussed before, will automatically align indices between the two elements:","41519e2b":"halfrow=df.iloc[0,::2]\nhalfrow","751418a1":"df-halfrow","e2aece63":"# None: Pythonic missing data\n\n# The first sentinel value used by Pandas is None, a Python singleton object that is often used for missing data in Python code. Because None is a Python object, it cannot be\n# used in any arbitrary NumPy\/Pandas array, but only in arrays with data type 'object' (i.e., arrays of Python objects):\n\nimport pandas as pd\nimport numpy as np\n\nvals1=np.array([1,None,3,4])\nvals1","63462181":"for dtype in ['object', 'int']:\n     print(\"dtype =\", dtype)\n     %timeit np.arange(1E6, dtype=dtype).sum()\n     print()","121d61c8":"# NaN: Missing numerical data\n# The other missing data representation, NaN (acronym for Not a Number), is different; it is a special floating-point value recognized by all systems that use the standard\n# IEEE floating-point representation:\n\nvals2=np.array([1,np.nan,3,4])\nvals2","c5527aca":"vals2.dtype\n\n# Notice that NumPy chose a native floating-point type for this array: this means that unlike the object array from before, this array supports fast operations pushed into\n# compiled code","3661d880":"#  You should be aware that NaN is a bit like a data virus\u2014it infects any other object it touches. Regardless of the operation, the result of arithmetic with NaN will be another NaN:\n\n1+np.nan\n","8ac96fdb":"0*np.nan","6c5c01f1":"# Note that this means that aggregates over the values are well defined (i.e., they don\u2019tresult in an error) but not always useful:\n\nvals2.sum(),vals2.min(),vals2.max()","bff70d81":"# NumPy does provide some special aggregations that will ignore these missing values:\n\nnp.nansum(vals2),np.nanmin(vals2),np.nanmax(vals2)\n\n# Keep in mind that NaN is specifically a floating-point value; there is no equivalent NaN value for integers, strings, or other types.\n","ea9213f7":"# NaN and None in Pandas\n\n# NaN and None both have their place, and Pandas is built to handle the two of them nearly interchangeably, converting between them where appropriate:\n\npd.Series([1,np.nan,4,None])","d6d93247":"x=pd.Series(range(2),dtype=int)\nx","13b3a9a3":"x[0]=None\nx","a0b61c76":"# Detecting null values : Pandas data structures have two useful methods for detecting null data: isnull() and notnull(). Either one will return a Boolean mask over the data. For example:\n\ndata=pd.Series([1,np.nan,None,4,8,'hello'])\ndata.isnull()","d2b96f94":"data[data.notnull()]","4a9023bd":"# Dropping null values\n# In addition to the masking used before, there are the convenience methods, dropna() (which removes NA values) and fillna() (which fills in NA values). For a Series, the result is straightforward:\n\ndata.dropna()","bfa80e6d":"# For a DataFrame, there are more options. Consider the following DataFrame:\n\ndf = pd.DataFrame([[1, np.nan, 2],\n                     [2, 3, 5],\n                 [np.nan, 4, 6]])","51b41c19":"df","0ac83586":"# We cannot drop single values from a DataFrame; we can only drop full rows or full columns. Depending on the application, you might want one or the other, so\n# dropna() gives a number of options for a DataFrame.\n\n# By default, dropna() will drop all rows in which any null value is present:\n\ndf.dropna()","9ced6683":"# Alternatively, you can drop NA values along a different axis; axis=1 drops all col\u2010umns containing a null value:\n\ndf.dropna(axis='columns')","2a635376":"# But this drops some good data as well; you might rather be interested in dropping rows or columns with all NA values, or a majority of NA values. This can be specified\n# through the how or thresh parameters, which allow fine control of the number of nulls to allow through.\n\n# The default is how='any', such that any row or column (depending on the axis key\u2010word) containing a null value will be dropped. You can also specify how='all', which\n# will only drop rows\/columns that are all null values:\n\ndf[3]=np.nan\ndf","9aa1c878":"df.dropna(axis='columns',how='all')","0aa7b5e3":"# For finer-grained control, the thresh parameter lets you specify a minimum number of non-null values for the row\/column to be kept:\n\ndf.dropna(axis='rows',thresh=3)\n\n# Here the first and last row have been dropped, because they contain only two nonnull values.\n","0c912fbc":"# Filling null values\n\n# Sometimes rather than dropping NA values, you\u2019d rather replace them with a valid value. This value might be a single number like zero, or it might be some sort of imputation or interpolation from the good values. You could do this in-place using\n# the isnull() method as a mask, but because it is such a common operation Pandas provides the fillna() method, which returns a copy of the array with the null values replaced.\n\ndata=pd.Series([1,np.nan,2,None,5],index=list('abcde'))\ndata","4ed56a94":"# We can fill NA entries with a single value, such as zero:\n\ndata.fillna(0)","fe5eaa9b":"# We can specify a forward-fill to propagate the previous value forward:\n\ndata.fillna(method='ffill')","ead05c78":"# Or we can specify a back-fill to propagate the next values backward:\ndata.fillna(method='bfill')","b9906bd0":"# For DataFrames, the options are similar, but we can also specify an axis along which the fills take place:\n\ndf.fillna(method='ffill',axis=1)\n\n# Notice that if a previous value is not available during a forward fill, the NA value remains.","b94812ab":"# 1 Dimensional data= stored in Series\n# 2 Dimensional data= stored in DataFrame\n# 3 or higher dimensional data (Data index more than one or two keys) = Stored in Multi Index so we need to create MultiIndex objects\n\n# A Multiply Indexed Series\n\n# Let\u2019s start by considering how we might represent two-dimensional data within a one-dimensional Series.\n\n# We can create a multi-index from the tuples as follows:\n\nindex = [('California', 2000), ('California', 2010),('New York', 2000), ('New York', 2010),('Texas', 2000), ('Texas', 2010)]\npopulations=[33871648, 37253956,18976457, 19378102,20851820, 25145561]\n\npop=pd.Series(populations,index=index)\npop","f48887f9":"index = pd.MultiIndex.from_tuples(index)\nindex","f6aaeba4":"# If we reindex our series with this MultiIndex, we see the hierarchical representation of the data:\n\npop=pop.reindex(index)\npop\n\n# Here the first two columns of the Series representation show the multiple index val\u2010 ues, while the third column shows the data. Notice that some entries are missing in\n# the first column: in this multi-index representation, any blank entry indicates the same value as the line above it.","50287d31":"# Now to access all data for which the second index is 2010, we can simply use the Pan\u2010das slicing notation:\npop[:,2010]","e7d795de":"# MultiIndex as extra dimension\n\n# The unstack() method will quickly convert a multiplyindexed Series into a conventionally indexed DataFrame:\n\npop_df=pop.unstack()\npop_df","717ff065":"# Naturally, the stack() method provides the opposite operation:\n\npop_df.stack()","7cd0eb7c":"# we can also use Multi Index to repre\u2010sent data of three or more dimensions in a Series or DataFrame. Each extra level in a multi-index represents an extra dimension of data; \n\npop_df = pd.DataFrame({'total': pop,'under18': [9267089, 9284094,4687374, 4318033,5906301, 6879014]})\npop_df","2132142e":"# Here we compute the fraction of people under 18 by year, given the above data:\n\nf_u18 = pop_df['under18'] \/ pop_df['total']\nf_u18.unstack()","942b9622":"# Methods of MultiIndex Creation\n\n# The most straightforward way to construct a multiply indexed Series or DataFrame is to simply pass a list of two or more index arrays to the constructor.\n\ndf = pd.DataFrame(np.random.rand(4, 2),index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],columns=['data1', 'data2'])\ndf","266ebdfc":"# Similarly, if you pass a dictionary with appropriate tuples as keys, Pandas will auto\u2010matically recognize this and use a MultiIndex by default:\n\ndata = {('California', 2000): 33871648,('California', 2010): 37253956,('Texas', 2000): 20851820,('Texas', 2010): 25145561,('New York', 2000): 18976457,('New York', 2010): 19378102}\npd.Series(data)","c89e79cb":"# Explicit MultiIndex constructors\n# For more flexibility in how the index is constructed, you can instead use the class method constructors available in the pd.MultiIndex.\n\npd.MultiIndex.from_arrays([['a','a','b','b'],['1','2','1','2']])\n","5caeace5":"# You can construct it from a list of tuples, giving the multiple index values of each point:\n\npd.MultiIndex.from_tuples([('a',1),('a',2),('b',1),('b',2)])","7d622ef7":"# You can even construct it from a Cartesian product of single indices:\n\npd.MultiIndex.from_product([['a', 'b'], [1, 2]])","39ffe493":"# MultiIndex level names\n\n# Sometimes it is convenient to name the levels of the MultiIndex\n\npop.index.names=['state','year']\npop","68321b29":"# MultiIndex for columns\n\n# In a DataFrame, the rows and columns are completely symmetric, and just as the rows can have multiple levels of indices, the columns can have multiple levels as well. \n\n# # hierarchical indices and columns\n\nindex = pd.MultiIndex.from_product([[2013,2014],[1,2]],names=['year','visit'])\nindex\ncolumns = pd.MultiIndex.from_product([['Bob', 'Guido', 'Sue'], ['HR', 'Temp']],names=['subject', 'type'])\n\n\n# mock some data\ndata=np.round(np.random.randn(4,6),1)\ndata[:,::2]*=10\ndata+=37","e2bf7f8f":"# create the DataFrame\n\nhealth_data=pd.DataFrame(data,index=index,columns=columns)\nhealth_data\n\n# Here we see where the multi-indexing for both rows and columns can come in very handy. This is fundamentally four-dimensional data, where the dimensions are the subject, the measurement type, the year, and the visit number. \n\n# 2 Dimensional in columns\n# 2 Dimensional in rows","6b29e651":"health_data['Guido']","a2b05c41":"# Indexing and Slicing a MultiIndex\n\n# Multiply indexed Series\n\npop","e45de5cf":"# We can access single elements by indexing with multiple terms:\n\npop['California',2000]","9fecc677":"# The MultiIndex also supports partial indexing, or indexing just one of the levels in the index.\npop['California']","cc2b2c2f":"# Partial slicing is available as well, as long as the MultiIndex is sorted\n\npop.loc['California':'New York']","ef70d099":"# With sorted indices, we can perform partial indexing on lower levels by passing an empty slice in the first index:\n\npop[:,2000]\n","5c6f9d36":"# Slicing,  selection based on Boolean masks:\n\npop[pop>220000]\n","73cfd5f9":"# Selection based on fancy indexing also works:\npop[['California','Texas']]","19697a23":"# Multiply indexed DataFrames\n\n# A multiply indexed DataFrame behaves in a similar manner. \n\nhealth_data","28029b21":"#  we can recover Guido\u2019s heart rate data with a simple operation:\n\nhealth_data['Guido','HR']","ea2e460e":"health_data.iloc[:2,:2]","38b3b822":"health_data.loc[:,('Bob','HR')]","7e9e72ce":"idx=pd.IndexSlice\nhealth_data.loc[idx[:, 1], idx[:, 'HR']]","2297c46d":"# Rearranging Multi-Indices\n\n# Sorted and unsorted indices\n\n#  Many of the MultiIndex slicing operations will fail if the index is not sorted. Let\u2019s take a look at this here.\n\nindex=pd.MultiIndex.from_product([['a','c','b'],['1','2']])\ndata=pd.Series(np.random.rand(6),index=index)\ndata.index.names=['char','int']\ndata","b3c86e1e":"# Pandas provides a number of convenience routines to perform this type of sorting; examples are the sort_index() and sortlevel() methods of the DataFrame. We\u2019ll use the simplest, sort_index(), here:\n\ndata=data.sort_index()\ndata","595ab53b":"# With the index sorted in this way, partial slicing will work as expected:\n\ndata['a':'b']","e441056e":"# Stacking and unstacking indices\n\n# As we saw briefly before, it is possible to convert a dataset from a stacked multi-index to a simple two-dimensional representation, optionally specifying the level to use:\n\npop.unstack(level=0)","8f955061":"pop.unstack(level=1)\n\n# The opposite of unstack() is stack(), which here can be used to recover the original series:","82e28b1d":"pop.unstack().stack()","4e1f0b48":"# Index setting and resetting\n\n# Another way to rearrange hierarchical data is to turn the index labels into columns; this can be accomplished with the reset_index method. \n\npop_flat=pop.reset_index(name='population')\npop_flat","4a94c072":"# it\u2019s useful to build a MultiIndex from the column values. This can be done with the set_index method of the DataFrame, which returns a multiply indexed Data Frame:\n\npop_flat.set_index(['state','year'])","18b23536":"# Data Aggregations on Multi-Indices\n\n# For hierarchically indexed data, these can be passed a level parameter that controls which subset of the data the aggregate is computed on.\n\nhealth_data\n","b196884f":"# Perhaps we\u2019d like to average out the measurements in the two visits each year. We can do this by naming the index level we\u2019d like to explore, in this case the year:\n\ndata_mean=health_data.mean(level='year')\ndata_mean","3ba9151f":"# By further making use of the axis keyword, we can take the mean among levels on the columns as well:\n\ndata_mean.mean(axis=1,level='type')\n","b54b73ae":"import pandas as pd\nimport numpy as np\n\ndef make_df(cols,ind):\n    \"\"\"Quickly make a DataFrame\"\"\"\n    data={c:[str(c)+str(i) for i in ind] for c in cols}\n    return pd.DataFrame(data,ind)","660be23f":"make_df('ABC',range(10))","614f5e4e":"# Recall: Concatenation of NumPy Arrays\n# Concatenation of Series and DataFrame objects is very similar to concatenation of NumPy arrays, which can be done via the np.concatenate function\nx = [1, 2, 3]\ny = [4, 5, 6]\nz = [7, 8, 9]\np=np.concatenate([x,y,z])\nprint(p)","e246c57d":"# The first argument is a list or tuple of arrays to concatenate. Additionally, it takes an axis keyword that allows you to specify the axis along which the result will be concatenated:\n\nx = [[1, 2],\n [3, 4]]\nnp.concatenate([x, x], axis=1)","145f6c35":"# Simple Concatenation with pd.concat\n# Pandas has a function, pd.concat(), which has a similar syntax to np.concatenate but contains a number of option\n\n# pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,keys=None, levels=None, names=None, verify_integrity=False,copy=True)\n# pd.concat() can be used for a simple concatenation of Series or DataFrame objects,\n\nser1=pd.Series(['A','B','C'],index=[1,2,3])\nser2=pd.Series(['D','E','F'],index=[4,5,6])\n\npd.concat([ser1,ser2])\n\n\n","459df224":"# It also works to concatenate higher-dimensional objects, such as DataFrames(2D)\n\ndf1=make_df('AB',[1,2])\ndf2=make_df('AB',[3,4])\n\nprint(df1); print(df2); print(pd.concat([df1, df2]))","237951a1":"# By default, the concatenation takes place row-wise within the DataFrame (i.e., axis=0). Like np.concatenate, pd.concat allows specification of an axis along which\n# concatenation will take place. Consider the following example:\ndf3 = make_df('AB', [0, 1])\ndf4 = make_df('CD', [0, 1])\nprint(df3); print(df4); print(pd.concat([df3, df4], axis=1))\n\n# We could have equivalently specified axis=1; here we\u2019ve used the more intuitive axis='col'\n","1faac632":"# Duplicate indices\n\n# One important difference between np.concatenate and pd.concat is that Pandas concatenation preserves indices, even if the result will have duplicate indices! Consider this simple example:\n\nx=make_df('AB',[0,1])\ny=make_df('AB',[2,3])\n\ny.index=x.index  # make duplicate indices!\nprint(x),print(y),print(pd.concat([x,y]))\n\n# Notice the repeated indices in the result. While this is valid within DataFrames, the outcome is often undesirable. pd.concat() gives us a few ways to handle it.\n\n","f9411cf8":"# Ignoring the index. \n\n# Sometimes the index itself does not matter, and you would prefer it to simply be ignored. You can specify this option using the ignore_index flag. With\n# this set to True, the concatenation will create a new integer index for the resulting Series:\n\nprint(x),print(y),print(pd.concat([x,y],ignore_index=True))\n","1bfab1e3":"# Adding MultiIndex keys\n\n# Another alternative is to use the keys option to specify a label for the data sources; the result will be a hierarchically indexed series containing the data:\n\nprint(x),print(y),print(pd.concat([x,y],keys=['x','y']))\n\n# The result is a multiply indexed DataFrame\n","c2f17ffc":"# Concatenation with joins\n\ndf5=make_df('ABC',[1,2])\ndf6=make_df('BCD',[3,4])\n\nprint(df5),print(df6),print(pd.concat([df5,df6]))\n\n","a4c76807":"# By default, the entries for which no data is available are filled with NA values. To change this, we can specify one of several options for the join and join_axes param\u2010\n# eters of the concatenate function. By default, the join is a union of the input columns (join='outer'), but we can change this to an intersection of the columns using join='inner':\n\nprint(df5),print(df6),print(pd.concat([df5,df6],join='inner'))","bbad4b35":"# Another option is to directly specify the index of the remaining colums using the join_axes argument, which takes a list of index objects. Here we\u2019ll specify that the\n# returned columns should be the same as those of the first input:\n\nprint(df5),print(df6)\n# print(pd.concat([df5, df6], join_axes=[df5.columns]))","1e9d4634":"# The append() method\n\n# Because direct array concatenation is so common, Series and DataFrame objects have an append method that can accomplish the same thing in fewer keystrokes. For\n# example, rather than calling pd.concat([df1, df2]), you can simply call df1.append(df2):\n\nprint(df1.append(df2))\n\n# Keep in mind that unlike the append() and extend() methods of Python lists, the append() method in Pandas does not modify the original object\u2014instead, it creates a\n# new object with the combined data. \n\n","ad26281d":"# Combining Datasets: Merge and Join\n\n# Categories of Joins\n\n# The pd.merge() function implements a number of types of joins: the one-to-one, many-to-one, and many-to-many joins. All three types of joins are accessed via an\n# identical call to the pd.merge() interface; the type of join performed depends on the form of the input data. \n\n# One-to-one joins\n\n# As a concrete example, consider the following two DataFrames, which contain information on several employees in a company:\n\ndf1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\n 'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n\ndf2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n 'hire_date': [2004, 2008, 2012, 2014]})\n\nprint(df1),print(df2)\n\n","6084e02a":"# To combine this information into a single DataFrame, we can use the pd.merge() function:\n\ndf3=pd.merge(df1,df2)\nprint(df3)","10a48968":"# Many-to-one joins\n\n# Many-to-one joins are joins in which one of the two key columns contains duplicate entries. For the many-to-one case, the resulting DataFrame will preserve those dupli\u2010\n# cate entries as appropriate. Consider the following example of a many-to-one join:\n\ndf4=pd.DataFrame({'group':['Accounting','Engineering','HR'],\n                 'supervisor':['carly','Guido','steve']})\n\nprint(df3),print(df4)","fa2fd1a6":"pd.merge(df3,df4)","e6110172":"# Many-to-many joins\n\n# Many-to-many joins are a bit confusing conceptually, but are nevertheless well defined. If the key column in both the left and right array contains duplicates, then\n# the result is a many-to-many merge.\n\n# By performing a many-to-many join, we can recover the skills associated with any individual person:\n\ndf5=pd.DataFrame({'group':['Accounting','accounting','Engineering','Engineering','HR','HR'],'skills':['math','spreadsheets','coding','linux','spreadsheets','organisation']})\n\nprint(df1),print(df5)","186a4a5b":"print(pd.merge(df1,df5))","b56bc2b2":"# Specification of the Merge Key\n\n# We\u2019ve already seen the default behavior of pd.merge(): it looks for one or more matching column names between the two inputs, and uses this as the key. However,\n# often the column names will not match so nicely, and pd.merge() provides a variety of options for handling this.\n\n# The on keyword\n\n# Most simply, you can explicitly specify the name of the key column using the on key\u2010word, which takes a column name or a list of column names:\n\nprint(df1),print(df2)","b57921a1":"print(pd.merge(df1,df2,on='employee'))","e7d2bb7d":"# The left_on and right_on keywords\n\n# At times you may wish to merge two datasets with different column names; for exam\u2010ple, we may have a dataset in which the employee name is labeled as \u201cname\u201d rather\n# than \u201cemployee\u201d. In this case, we can use the left_on and right_on keywords to specify the two column names:\n\ndf3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n 'salary': [70000, 80000, 120000, 90000]})\n\nprint(df1),print(df3)","5c5c4d49":"pd.merge(df1,df3,left_on='employee',right_on='name')","4f0017be":"# The result has a redundant column that we can drop if desired\u2014for example, by using the drop() method of DataFrames:\n\npd.merge(df1,df3,left_on='employee',right_on='name').drop('name',axis=1)","980fa001":"# The left_index and right_index keywords\n\n# Sometimes, rather than merging on a column, you would instead like to merge on an index. For example, your data might look like this:\n\ndf1a=df1.set_index('employee')\ndf2a = df2.set_index('employee')\nprint(df1a),print(df2a)\n","624a44b1":"# You can use the index as the key for merging by specifying the left_index and\/or right_index flags in pd.merge():\n\nprint(pd.merge(df1a, df2a, left_index=True, right_index=True))","b198747a":"# For convenience, DataFrames implement the join() method, which performs a merge that defaults to joining on indices:\n\n\nprint(df1a.join(df2a))","7e1f37cb":"# If you\u2019d like to mix indices and columns, you can combine left_index with right_on or left_on with right_index to get the desired behavior:\n\nprint(df1a),print(df3)\n\n","bde1259a":"pd.merge(df1a, df3, left_index=True, right_on='name')\n\n# All of these options also work with multiple indices and\/or multiple columns; the interface for this behavior is very intuitive.","69219242":"# Specifying Set Arithmetic for Joins\n\n# In all the preceding examples we have glossed over one important consideration in performing a join: the type of set arithmetic used in the join. This comes up when a\n# value appears in one key column but not the other. Consider this example:\n\ndf6 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],\n 'food': ['fish', 'beans', 'bread']},\n columns=['name', 'food'])\n\ndf7 = pd.DataFrame({'name': ['Mary', 'Joseph'],\n 'drink': ['wine', 'beer']},\n columns=['name', 'drink'])\n\n\nprint(df6); print(df7); print(pd.merge(df6, df7))","403e735e":"pd.merge(df6,df7)","b597f9fe":"# Here we have merged two datasets that have only a single \u201cname\u201d entry in common: Mary. By default, the result contains the intersection of the two sets of inputs; this is\n# what is known as an inner join. We can specify this explicitly using the how keyword, which defaults to 'inner':\n\npd.merge(df6,df7,how='inner')","8c8bf967":"# Other options for the how keyword are 'outer', 'left', and 'right'. An outer join returns a join over the union of the input columns, and fills in all missing values with NAs:\n\npd.merge(df6,df7,how='outer')","4bce30a4":"# The left join and right join return join over the left entries and right entries, respec\u2010tively. For example:\n\npd.merge(df6,df7,how='left')","3f518fb7":"pd.merge(df6,df7,how='right')","ae1df866":"# Overlapping Column Names: The suffixes Keyword\n\ndf8=pd.DataFrame({'name':['Bob','Jake','Lisa','Sue'],'rank':['1','4','3','7']})\ndf9=pd.DataFrame({'name':['Bob','Jake','Lisa','Sue'],'rank':['3','1','2','4']})\nprint(df8),print(df9),print(pd.merge(df8, df9, on=\"name\"))","9aa3b89a":"# Because the output would have two conflicting column names, the merge function automatically appends a suffix _x or _y to make the output columns unique. If these\n# defaults are inappropriate, it is possible to specify a custom suffix using the suffixes keyword:\n\nprint(pd.merge(df8, df9, on=\"name\", suffixes=[\"_L\", \"_R\"]))","91920f25":"# Let\u2019s take a look at the three datasets, using the Pandas read_csv() function:\n\npop=pd.read_csv('\/kaggle\/input\/us-states-data\/state-population.csv')\narea=pd.read_csv('\/kaggle\/input\/us-states-data\/state-areas.csv')\nabbrevs=pd.read_csv('\/kaggle\/input\/us-states-data\/state-abbrevs.csv')\n","12610194":"pop.head()","2da678a4":"area.head()","466b923b":"abbrevs.head()","741df0a7":"# We\u2019ll start with a many-to-one merge that will give us the full state name within the population DataFrame. We want to merge based on the state\/region column of pop,\n# and the abbreviation column of abbrevs. We\u2019ll use how='outer' to make sure no data is thrown away due to mismatched labels.\n\nmerged=pd.merge(pop,abbrevs,how='outer',left_on='state\/region',right_on='abbreviation')\nprint(merged)","5114dac5":"merged = merged.drop( columns='abbreviation') # drop duplicate info\nmerged.head()\n","e949efbb":"# Let\u2019s double-check whether there were any mismatches here, which we can do by looking for rows with nulls\nmerged.isnull().any()","51b550be":"# Some of the population info is null; let\u2019s figure out which these are!\n\nmerged[merged['population'].isnull()].head(10)","2248468b":"merged.loc[merged['state'].isnull(), 'state\/region'].unique()","d5bd71ee":"merged.loc[merged['state\/region']=='PR','state']='purto rico'\nmerged.loc[merged['state\/region'] == 'USA', 'state'] = 'United States'\nmerged.isnull().any()\n\n# No more nulls in the state column: we\u2019re all set!","ddbc9dc7":"# Now we can merge the result with the area data using a similar procedure. Examining our results, we will want to join on the state column in both:\n\nfinal = pd.merge(merged, area, on='state', how='left')\nfinal.head()\n","d931eb4c":"# Again, let\u2019s check for nulls to see if there were any mismatches:\n\nfinal.isnull().any()","e89005be":"# There are nulls in the area column; we can take a look to see which regions were ignored here:\nfinal['state'][final['area (sq. mi)'].isnull()].unique()\n","6ce79c48":"final.dropna(inplace=True)\nfinal.head()\n","b55cb96b":"data2010=final.query(\"year==2010 & ages=='total'\")\ndata2010.head()","f123082a":"data2010.set_index('state', inplace=True)\ndensity = data2010['population'] \/ data2010['area (sq. mi)']","992f6909":"density.sort_values(ascending=False, inplace=True)\ndensity.head()","7c94bb1b":"density.tail()","fafa54f3":"# Planets Data\n\nimport seaborn as sns\nplanets=sns.load_dataset('planets')\nplanets.head()","347519a8":"# Simple Aggregation in Pandas\n\nrng=np.random.RandomState(42)\nser=pd.Series(rng.rand(5))\nser","5c44afbc":"ser.sum()","2e9e10f3":"ser.mean()","2dbc094a":"df=pd.DataFrame({'A':rng.rand(5),\n                'B':rng.rand(5)})\ndf","ffafc031":"df.mean()","0c5012bd":"# By specifying the axis argument, you can instead aggregate within each row:\n\ndf.mean(axis='columns')","f8ec28d3":"#  there is a convenience method describe() that computes several common aggregates for each column and returns the result. Let\u2019s use this on the Planets data, for now drop\u2010\n# ping rows with missing values:\n\nplanets.dropna().describe()","131d01ee":"df=pd.DataFrame({'key':['A','B','C','D','E','F'],'data':['1','2','3','4','5','6']})\ndf","ea53d7db":"df.groupby('key')","dd5248ee":"df.groupby('key').sum()\n#The sum() method is just one possibility here; you can apply virtually any common Pandas or NumPy aggregation function, as well as virtually any valid DataFrame operation, ","a56dc70f":"# The GroupBy object\n# The GroupBy object is a very flexible abstraction. In many ways, you can simply treat it as if it\u2019s a collection of DataFrames, and it does the difficult things under the hood.\n\nplanets.head()\n","1c4942b6":"planets.groupby('method')['orbital_period']\n","4149b84c":"# As with the GroupBy object, no computation is done until we call some aggregate on the object:\nplanets.groupby('method')['orbital_period'].median()","cf59bdb1":"for (method, group) in planets.groupby('method'):\n    print(\"{0:30s} shape={1}\".format(method, group.shape))\n    \n# This can be useful for doing certain things manually, though it is often much faster to use the built-in apply functionality, which we will discuss momentarily.\n    ","dfa0d0f5":"# Dispatch methods. \nplanets.groupby('method')['year'].describe().unstack()","0ae20fcf":"# Aggregate, filter, transform, apply\n# In particular, GroupBy objects have aggregate(), filter(), transform(), and apply() methods that efficiently implement a variety of useful operations before combining the grouped data.\n\nrng=np.random.RandomState(0)\ndf = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],'data1': range(6),'data2': rng.randint(0, 10, 6)},columns = ['key', 'data1', 'data2'])\ndf","be2e25cb":"# Aggregation\n# We\u2019re now familiar with GroupBy aggregations with sum(), median(), and the like, but the aggregate() method allows for even more flexibility. It can take\n# a string, a function, or a list thereof, and compute all the aggregates at once. Here is a quick example combining all these:\n\ndf.groupby('key').aggregate([min,np.median,max])\n","e9fb7032":"# Another useful pattern is to pass a dictionary mapping column names to operations to be applied on that column:\n\ndf.groupby('key').aggregate({'data1': 'min','data2': 'max'})","098b61f1":"# Filtering\n# A filtering operation allows you to drop data based on the group proper\u2010 ties. For example, we might want to keep all groups in which the standard deviation is larger than some critical value:\n\ndef filter_fun(x):\n    return x['data2'].std()>4","6e499c7a":"print(df)","88b0509c":"print(df.groupby('key').std())","5956edec":"print(df.groupby('key').filter(filter_fun))\n\n# The filter() function should return a Boolean value specifying whether the group passes the filtering. Here because group A does not have a standard deviation greater than 4, it is dropped from the result.","d7b18eb5":"# Transformation\n\n# While aggregation must return a reduced version of the data, trans\u2010 formation can return some transformed version of the full data to recombine. For such a transformation, the output is the same shape as the input. A common example\n# is to center the data by subtracting the group-wise mean:\n\ndf.groupby('key').transform(lambda x:x-x.mean())\n","38c2fb6b":"# The apply() method\n# The apply() method lets you apply an arbitrary function to the group results. The function should take a DataFrame, and return either a Pandasobject (e.g., DataFrame, Series) or a scalar; the combine operation will be tailored to\n# the type of output returned.\n\n# For example, here is an apply() that normalizes the first column by the sum of the second:\n\ndef norm_by_data2(x):\n    # x is a DataFrame of group values\n    x['data1']=x['data2'].sum()\n    return x","3074e6b1":"print(df)","60243676":"print(df.groupby('key').apply(norm_by_data2))\n\n# apply() within a GroupBy is quite flexible: the only criterion is that the function takes a DataFrame and returns a Pandas object or scalar;","8e7db7ff":"# Specifying the split key\n# In the simple examples presented before, we split the DataFrame on a single column name. This is just one of many options by which the groups can be defined, and we\u2019ll\n# go through some other options for group specification here.\n\n# A list, array, series, or index providing the grouping keys. \n\nL = [0, 1, 0, 1, 2, 0]\nprint(df)","aa773348":"print(df.groupby(L).sum())","cef1fa56":"# Of course, this means there\u2019s another, more verbose way of accomplishing the df.groupby('key') from before:\n\nprint(df.groupby(df['key']).sum())\n","10b77c22":"# A dictionary or series mapping index to group. \n\n# Another method is to provide a dictionary that maps index values to the group keys:\n\ndf2=df.set_index('key')\nmapping={'A':'vowel','B':'consonant','C':'consonant'}\nprint(df2)\n","d0fc2641":"print(df2.groupby(mapping).sum())","ae0499f3":"# Any Python function. \n\n# Similar to mapping, you can pass any Python function that will input the index value and output the group:\n\nprint(df2.groupby(str.lower).mean())\n","767655f5":"# A list of valid keys. \n# Further, any of the preceding key choices can be combined to group on a multi-index:\n\ndf2.groupby([str.lower,mapping]).mean()\n","acbc5cf4":"# Grouping example\n# As an example of this, in a couple lines of Python code we can put all these together and count discovered planets by method and by decade:\n\ndecade = 10 * (planets['year'] \/\/ 10)\ndecade = decade.astype(str) + 's'\ndecade.name = 'decade'\nplanets.groupby(['method', decade])['number'].sum().unstack().fillna(0)\n","32b62bf4":"#  The pivot table takes simple columnwise data as input, and groups the entries into a two-dimensional table that provides\n# a multidimensional summarization of the data.\n# Motivating Pivot Tables\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\ntitanic=sns.load_dataset('titanic')\nprint(titanic.head(20))","18deffc9":"# Pivot Tables by Hand\n# let\u2019s look at survival rate by gender:\nprint(titanic.groupby('sex')['survived'].mean())","efcc105c":"#  look at survival by both sex and, say, class.we might proceed using something\n# like this: we group by class and gender, select survival, apply a mean aggregate, com\u2010\n# bine the resulting groups, and then unstack the hierarchical index to reveal the hidden\n# multidimensionality.\n\ntitanic.groupby(['sex', 'class'])['survived'].aggregate('mean').unstack()","2f0f0a1a":"# Pivot Table Syntax\n# Here is the equivalent to the preceding operation using the pivot_table method of DataFrames:\n\ntitanic.pivot_table('survived',index='sex',columns='class')","992dce6a":"# Multilevel pivot tables\n# Just as in the GroupBy, the grouping in pivot tables can be specified with multiple lev\u2010\n# els, and via a number of options. For example, we might be interested in looking at\n# age as a third dimension. We\u2019ll bin the age using the pd.cut function:\n\nage=pd.cut(titanic['age'],[0,18,80])\nprint(titanic.pivot_table('survived',['sex',age],'class'))","3559983b":"# We can apply this same strategy when working with the columns as well; let\u2019s add info on the fare paid using pd.qcut to automatically compute quantiles:\n\nfare=pd.qcut(titanic['fare'],2)\ntitanic.pivot_table('survived',['sex',age],[fare,'class'])","6525bc45":"# Additional pivot table options\n# The full call signature of the pivot_table method of DataFrames is as follows:\n\ntitanic.pivot_table(index='sex', columns='class',aggfunc={'survived':sum, 'fare':'mean'})","5221cf6f":"# At times it\u2019s useful to compute totals along each grouping. This can be done via the margins keyword:\n\ntitanic.pivot_table('survived',index='sex',columns='class',margins=True)","16f51cf8":"# Introducing Pandas String Operations\nx=np.array([2,3,6,4,8])\nx=x*2\nprint(x)","31c422da":"# This vectorization of operations simplifies the syntax of operating on arrays of data: we no longer have to worry about the size or shape of the array, but just about what\n# operation we want done. For arrays of strings, NumPy does not provide such simple access, and thus you\u2019re stuck using a more verbose loop syntax:\n\ndata = ['peter', 'Paul', 'MARY','gUIDO']\n[s.capitalize() for s in data]\nprint(data)","3e506a5a":"# Pandas includes features to address both this need for vectorized string operations\n# and for correctly handling missing data via the str attribute of Pandas Series and\n# Index objects containing strings. So, for example, suppose we create a Pandas Series\n# with this data:\n\nimport pandas as pd\nnames = pd.Series(data)\nnames","04fe1e3a":"names.str.capitalize()\n# Using tab completion on this str attribute will list all the vectorized string methods available to Pandas.\n","a351fa0d":"# Tables of Pandas String Methods\n\nmonte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam','Eric Idle', 'Terry Jones', 'Michael Palin'])","96846a77":"\n# Methods similar to Python string methods\n# Nearly all Python\u2019s built-in string methods are mirrored by a Pandas vectorized string\n# method. Here is a list of Pandas str methods that mirror Python string methods:\n\nprint(monte.str.lower())","0229f6ec":"monte.str.len()","fa4f2381":"# Or Boolean values:\nmonte.str.startswith('T')","ca9ca6a9":"# Still others return lists or other compound values for each element:\n\nmonte.str.split()\n# we can extract the first name from each by asking for a contiguous group of characters at the beginning of each element:\nprint(monte.str.extract('([A-Za-z]+)'))\n","279e7222":"\n# Or we can do something more complicated, like finding all names that start and end with a consonant, making use of the start-of-string (^) and end-of-string ($) regular expression characters:\n\nmonte.str.findall(r'^[^AEIOU].*[^aeiou]$')","7128a6c6":"# The get() and slice() operations, in particular, enable vectorized element access from each array. For example, we can get a slice of\n# the first three characters of each array using str.slice(0, 3). Note that this behav\u2010\n# ior is also available through Python\u2019s normal indexing syntax\u2014for example, df.str.slice(0, 3) is equivalent to df.str[0:3]:\nprint(monte.str[0:3])\n# Indexing via df.str.get(i) and df.str[i] is similar.","23416d16":"# These get() and slice() methods also let you access elements of arrays returned by\n# split(). For example, to extract the last name of each entry, we can combine split() and get():\n\nmonte.str.split().str.get(-1)","72a5a934":"# Indicator variables\n# Another method that requires a bit of extra explanation is the get_dummies() method. This is useful when your data has a column containing some\n# sort of coded indicator. For example, we might have a dataset that contains informa\u2010tion in the form of codes, such as A=\u201cborn in America,\u201d B=\u201cborn in the United King\u2010\n# dom,\u201d C=\u201clikes cheese,\u201d D=\u201clikes spam\u201d:\n\nfull_monte = pd.DataFrame({'name': monte,\n 'info': ['B|C|D', 'B|D', 'A|C', 'B|D', 'B|C',\n 'B|C|D']})\n\nprint(full_monte)","f7b36bb2":"# The get_dummies() routine lets you quickly split out these indicator variables into a DataFrame:\n\nprint(full_monte['info'].str.get_dummies('|'))","9c17ca10":"# Native Python dates and times: datetime and dateutil\nfrom datetime import datetime\nprint(datetime(year=2020,month=11,day=7))","02fd6339":"# Or, using the dateutil module, you can parse dates from a variety of string formats:\nfrom dateutil import parser\ndate=parser.parser(\"4th of July 2020\")\nprint(date)","81b6017a":"# Typed arrays of times: NumPy\u2019s datetime64\n# The datetime64 dtype encodes dates as 64-bit integers, and thus allows arrays of dates to be represented very compactly. The date\n# time64 requires a very specific input format:\n\nimport numpy as np\ndate=np.array('2015-07-04', dtype=np.datetime64)\ndate","a5302dd0":"# Once we have this date formatted, however, we can quickly do vectorized operations on it:\n\ndate+np.arange(12)","990e1741":"np.datetime64('2015-07-04')","104cb80f":"np.datetime64('2015-07-04 12:00')","7c779a20":"np.datetime64('2015-07-04 12:59:59.50', 'ns')","39cf86a6":"# Dates and times in Pandas: Best of both worlds\n\ndate = pd.to_datetime(\"4th of July, 2015\")\ndate","ac154c59":"print(date.strftime('%A'))","aff4ca9d":"# Additionally, we can do NumPy-style vectorized operations directly on this same object:\n\nprint(date + pd.to_timedelta(np.arange(12),'D'))","d00cf16b":"# Pandas Time Series: Indexing by Time\n# Where the Pandas time series tools really become useful is when you begin to index data by timestamps. For example, we can construct a Series object that has timeindexed data:\n\nindex = pd.DatetimeIndex(['2014-07-04', '2014-08-04','2015-07-04', '2015-08-04'])\ndata=pd.Series([0,1,2,3],index=index)\nprint(data)","d5c0954e":"\n# Now that we have this data in a Series, we can make use of any of the Series index\u2010ing patterns we discussed in previous sections, passing values that can be coerced into dates:\n\ndata['2014-07-04':'2015-07-04']","6be6ef17":"# There are additional special date-only indexing operations, such as passing a year to obtain a slice of all data from that year:\n\nprint(data['2015'])","b4b5d05a":"# Pandas Time Series Data Structures\ndates = pd.to_datetime([datetime(2015, 7, 3), '4th of July, 2015','2015-Jul-6', '07-07-2015', '20150708'])\ndates","36fd9a1a":"# Any DatetimeIndex can be converted to a PeriodIndex with the to_period() function with the addition of a frequency code; here we\u2019ll use 'D' to indicate daily frequency:\n\nprint(dates.to_period('D'))","19790b59":"\n# A TimedeltaIndex is created, for example, when one date is subtracted from another:\nprint(dates - dates[0])","b8bf06af":"# Regular sequences: pd.date_range()\n# To make the creation of regular date sequences more convenient, Pandas offers a few functions for this purpose: pd.date_range() for timestamps, pd.period_range() for\n# periods, and pd.timedelta_range() for time deltas. We\u2019ve seen that Python\u2019s Working with Time Series  range() and NumPy\u2019s np.arange() turn a startpoint, endpoint, and optional stepsize\n# into a sequence. Similarly, pd.date_range() accepts a start date, an end date, and an optional frequency code to create a regular sequence of dates. By default, the frequency is one day:\n\nprint(pd.date_range('2015-07-03', '2015-07-10'))","9a8c634c":"# Alternatively, the date range can be specified not with a start- and endpoint, but with a startpoint and a number of periods:\n\npd.date_range('2015-07-03', periods=8)","f1810a76":"# You can modify the spacing by altering the freq argument, which defaults to D. For example, here we will construct a range of hourly timestamps: pd.date_range('2015-07-03', periods=8, freq='H')\n\nprint(pd.date_range('2015-07-03', periods=8, freq='H'))","5631a5e6":"# To create regular sequences of period or time delta values, the very similar pd.period_range() and pd.timedelta_range() functions are useful. Here are some monthly periods:\n\nprint(pd.period_range('2015-07',periods=20,freq='M'))","f1e99ed1":"# And a sequence of durations increasing by an hour:\n\nprint(pd.timedelta_range(0, periods=10, freq='H'))","ccffc7ab":"print(pd.timedelta_range(0,periods=8,freq=\"2H30T\"))","5c86953c":"# All of these short codes refer to specific instances of Pandas time series offsets, which can be found in the pd.tseries.offsets module. For example, we can create a busi\u2010\n# ness day offset directly as follows:\n\nfrom pandas.tseries.offsets import BDay\npd.date_range('2015-07-01', periods=5, freq=BDay())","fe769591":"dtype = [('Col1','int32'), ('Col2','float32'), ('Col3','float32')]\nvalues = np.zeros(20, dtype=dtype)\nindex = ['Row'+str(i) for i in range(1, len(values)+1)]\n\ndf = pandas.DataFrame(values, index=index)\nprint(df)\n\ndf = pandas.DataFrame(values)\nprint(df)","34096fa6":"df=pd.read_csv(\"\/kaggle\/input\/usaccidentsdata\/US_Accidents_June20.csv\")","28332230":"print(\"The shape of the df is {}\".format(df.shape))","4275a3a6":"df = pd.read_csv(\"\/kaggle\/input\/usaccidentsdata\/US_Accidents_June20.csv\", skiprows = lambda x: x>0 and np.random.rand() > 0.01)\nprint(\"The shape of the df is {}. It has been reduced 10 times!\".format(df.shape))\n\n'''\nHow it works:\nskiprows accepts a function that is evaluated against the integer index.\nx > 0 makes sure that the headers is not skipped\nnp.random.rand() > 0.01 returns True 99% of the tie, thus skipping 99% of the time.\nNote that we are using skiprows\n\n'''","7c3633ce":"d={\\\n \"zip code\":[492005,245678,493006,236743],\n \"factory\":[100, 400, 500, 600],\n\"warehouse\":[200,300,400,500],\n\"retail\":[1,2,3,4]}\ndf=pd.DataFrame(d)\nprint(d)","06e2c336":"df = pd.DataFrame(d)\ndf\n\n# save to csv\ndf.to_csv(\"trick99data.csv\")\n\ndf = pd.read_csv(\"trick99data.csv\")\ndf\n","029a2b68":"df = pd.read_csv(\"trick99data.csv\", index_col=0)\n# or when saving df = pd.read_csv(\"trick99data.csv\", index = False)\ndf","772810e7":"d = {\\\n\"zip_code\": [12345, 56789, 101112, 131415],\n\"factory\": [100, 400, 500, 600],\n\"warehouse\": [200, 300, 400, 500],\n\"retail\": [1, 2, 3, 4]\n}\n\ndf = pd.DataFrame(d)\ndf","8e693803":"# location_type is generated automatically from the columns left after specifying id_vars (you can pass a list also)\ndf = df.melt(id_vars = \"zip_code\", var_name = \"location_type\", value_name = \"distance\")\ndf","81f4b0eb":"d = {\\\n\"year\": [2019, 2019, 2020],\n\"day_of_year\": [350, 365, 1]\n}\nprint(d)","5a814e07":"df = pd.DataFrame(d)\ndf\n","d0e0df98":"# Step 1: create a combined column\ndf[\"combined\"] = df[\"year\"]*1000 + df[\"day_of_year\"]\ndf","f5882fac":"# Step 2: convert to datetime\n# Step 2: convert to datetime\ndf[\"date\"] = pd.to_datetime(df[\"combined\"], format = \"%Y%j\")\ndf","c536103b":"print(pd.__version__)\n## Pandas version 0.25 or higher requiered and you need hvplot","2d18a801":"df=pd.read_csv(\"\/kaggle\/input\/drinksbycountrydata\/drinksbycountry.csv\")\ndf","84de359e":"df.plot()","4cb4d620":"# this one is not interactve\ndf.plot(kind = \"scatter\", x = \"spirit_servings\", y = \"wine_servings\")\n# run !pip install hvplot\n#pd.options.plotting.backend = \"hvplot\"\n","a36f3ce3":"d = {\\\n\"col1\": [2019, 2019, 2020],\n\"col2\": [350, 365, 1],\n\"col3\": [np.nan, 365, None]\n}\n\ndf = pd.DataFrame(d)\ndf","f3461b32":"# Solution 1\ndf.isnull().sum()","3807afe1":"# Solution 2\ndf.isna().sum()","310a8930":"# Solution 3\ndf.isna().any()\n","2a91d12f":"# Solution 4:\n#94\ndf.isna().any(axis = None)","62d0ed74":"df = pd.read_csv(\"\/kaggle\/input\/titanic-data\/train.csv\", usecols = [\"Pclass\", \"Sex\", \"Parch\", \"Cabin\"])\ndf\n\n# let's see how much our df occupies in memory\ndf.memory_usage(deep = True)\n\n# convert to smaller datatypes\ndf = df.astype({\"Pclass\":\"int8\",\n                \"Sex\":\"category\", \n                \"Parch\": \"Sparse[int]\", # most values are 0\n                \"Cabin\":\"Sparse[str]\"}) # most values are NaN\n\ndf.memory_usage(deep = True)\n","04eb3364":"d = {\"genre\": [\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"D\", \"E\", \"F\"]}\ndf = pd.DataFrame(d)\ndf\n\n# Step 1: count the frequencies\nfrequencies = df[\"genre\"].value_counts(normalize = True)\nfrequencies","ab1dc285":"# Step 2: establish your threshold and filter the smaller categories\nthreshold = 0.15\nsmall_categories = frequencies[frequencies < threshold].index\nsmall_categories","732028a3":"# Step 3: replace the values\ndf[\"genre\"] = df[\"genre\"].replace(small_categories, \"Other\")\ndf[\"genre\"].value_counts(normalize = True)","bdd51322":"d = {\"customer\": [\"A\", \"B\", \"C\", \"D\"], \"sales\":[1100, 950.75, \"$400\", \"$1250.35\"]}\ndf = pd.DataFrame(d)\ndf","0cad5d46":"# Step 1: check the data types\ndf[\"sales\"].apply(type)","ad1f35fa":"# Step 2: use regex\n# Step 2: use regex\ndf[\"sales\"] = df[\"sales\"].replace(\"[$,]\", \"\", regex = True).astype(\"float\")\ndf\ndf[\"sales\"].apply(type)","d5e0fa94":"# Solution 1\nnumber_or_rows = 365*24 # hours in a year\npd.util.testing.makeTimeDataFrame(number_or_rows, freq=\"H\")","181bb1ee":"# Solution 2\nnum_cols = 2\ncols = [\"sales\", \"customers\"]\ndf = pd.DataFrame(np.random.randint(1, 20, size = (number_or_rows, num_cols)), columns=cols)\ndf.index = pd.util.testing.makeDateIndex(number_or_rows, freq=\"H\")\ndf","a5df9437":"d = {\"A\":[15, 20], \"B\":[20, 25], \"C\":[30 ,40], \"D\":[50, 60]}\ndf = pd.DataFrame(d)\ndf","71e53471":"# Using insert\ndf.insert(3, \"C2\", df[\"C\"]*2)\ndf","91134397":"# Other solution\ndf[\"C3\"] = df[\"C\"]*3 # create a new columns, it will be at the end\ndf","2b64385b":"df = pd.Series([\"Geordi La al Forge\", \"Deanna Troi\", \"Data\"]).to_frame()\ndf.rename({0:\"names\"}, inplace = True, axis = 1)\ndf","ef808ccf":"# split on first space\ndf[\"first_name\"] = df[\"names\"].str.split(n = 1).str[0]\ndf[\"Last_name\"] = df[\"names\"].str.split(n = 1).str[1]\ndf","e6439214":"def generate_sample_data(): # creates a fake df for testing\n    number_or_rows = 20\n    num_cols = 7\n    cols = list(\"ABCDEFG\")\n    df = pd.DataFrame(np.random.randint(1, 20, size = (number_or_rows, num_cols)), columns=cols)\n    df.index = pd.util.testing.makeIntIndex(number_or_rows)\n    return df","88688c6d":"df = generate_sample_data()\ndf.head(25)","da7afb10":"# Solution 1\ndf[[\"A\", \"C\", \"D\", \"F\", \"E\", \"G\", \"B\"]].head() \n","ca217754":"# Solution 2\ncols_to_move = [\"A\", \"G\", \"B\"]\nnew_order = cols_to_move + [c for c in df.columns if c not in cols_to_move] # generate your new order\ndf[new_order].head()","b30e7711":"# Solutin 3: using index\ncols = df.columns[[0, 5 , 3, 4, 2, 1, 6]] # df.columns returns a series with index, we use the list to iorder the index as we like --> this way we order the columns\ndf[cols].head()","c9177d3b":"def generate_sample_data_datetime(): # creates a fake df for testing\n    number_or_rows = 365*24\n    num_cols = 2\n    cols = [\"sales\", \"customers\"]\n    df = pd.DataFrame(np.random.randint(1, 20, size = (number_or_rows, num_cols)), columns=cols)\n    df.index = pd.util.testing.makeDateIndex(number_or_rows, freq=\"H\")\n    return df","0bcc2b69":"# show several prints in one cell. This will allow us to condence every trick in one cell.\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n","79fcb3e1":"df = generate_sample_data_datetime()\ndf","9b27f42d":"df.shape","478beb98":"df.info","e7cdcde5":"df.describe","01ec47b2":"# Step 1: resample by \"D\" Daily Basically agregate by day and use to_frame() to convert it to frame\ndaily_sales = df.resample(\"D\")[\"sales\"].sum().to_frame()\ndaily_sales","dce0993b":"# Step 2: filter weekends\nweekends_sales = daily_sales[daily_sales.index.dayofweek.isin([5, 6])]\nweekends_sales\n\n'''\ndayofweek day\n0         Monday\n1         Tuesday\n2         Wednesday\n3         Thursday\n4         Friday\n5         Saturday\n6         Sunday\n'''","2bd14447":"df = pd.read_csv(\"\/kaggle\/input\/titanic-data\/train.csv\")\ndf.head()","0b4138f2":"print(\"The Problem relies on that we don't know the column name\")\ndf.groupby(\"Pclass\")[\"Age\"].agg([\"mean\", \"max\"])","4f74df19":"print(\"The Problem relies on that we have multiindex\")\ndf.groupby(\"Pclass\").agg({\"Age\":[\"mean\",\"max\"]})","33eaed10":"print(\"Now we have solved the previous problems by specifyig the column final name we want.\")\nprint(\"BUT IT ONLY WORKS WITH A COLUMN. TO THIS KIND OF OPERATIONS ON MULTIPLE COLUMNS CHECK THE NEXT CELL\")\ndf.groupby(\"Pclass\")[\"Age\"].agg(age_mean = \"mean\", age_max = \"max\")","c5adfe25":"def my_agg(x):\n    names = {\n        'age_mean': x['Age'].mean(),\n        'age_max':  x['Age'].max(),\n        'fare_mean': x['Fare'].mean(),\n        'fare_max': x['Fare'].max()\n    } # define you custom colum names and operations\n\n    return pd.Series(names, index=[ key for key in names.keys()]) # all the columns you create in the previous dictionary will be in this list comprehension\n\ndf.groupby('Pclass').apply(my_agg)","8f2d357a":"# Do some fast feature eng on the DF\nd = {\"gender\":[\"male\", \"female\", \"male\"], \"color\":[\"red\", \"green\", \"blue\"], \"age\":[25, 30, 15]}\ndf = pd.DataFrame(d)\ndf\n\n#Solution\n","9369b4f6":"# Solution\nmap_dict = {\"male\":\"M\", \"female\":\"F\"}\ndf[\"gender_mapped\"] = df[\"gender\"].map(map_dict) # using dictionaries to map values\ndf[\"color_factorized\"] = df[\"color\"].factorize()[0] # using factorize: returns a tuple of arrays (array([0, 1, 2]), Index(['red', 'green', 'blue'], dtype='object')) that's why we select [0]\ndf[\"age_compared_boolean\"] = df[\"age\"] < 18 # return a True False boolean value\n\ndf","14889a02":"print(\"This df occupies way too much space\")\ndf = generate_sample_data()\ndf","0ba90fff":"print(\"using set_option to save some screen space\")\npd.set_option(\"display.max_rows\", 6)\ndf","db0d838b":"pd.reset_option(\"all\")\ndf","846d1ebf":"# We will create a sample student dataset consisting of 5 columns \u2013 age, section, city, gender, and favorite color. This dataset will contain both numerical as well as categorical variables:\n\n# crete a sample dataframe\ndata = pd.DataFrame({\n    'age' :     [ 10, 22, 13, 21, 12, 11, 17],\n    'section' : [ 'A', 'B', 'C', 'B', 'B', 'A', 'A'],\n    'city' :    [ 'Gurgaon', 'Delhi', 'Mumbai', 'Delhi', 'Mumbai', 'Delhi', 'Mumbai'],\n    'gender' :  [ 'M', 'F', 'F', 'M', 'M', 'M', 'F'],\n    'favourite_color' : [ 'red', np.NAN, 'yellow', np.NAN, 'black', 'green', 'red']\n})\n\n# view the data\ndata\n","d72fa30c":"# Find all the rows based on any condition in a column\ndata.loc[data.age>15]","221033dc":"# Find all the rows with more than one condition\n# Similarly, we can also use multiple conditions to filter our data, such as finding all the rows where the age is greater than or equal to 12 and the gender is also male:\n\ndata.loc[(data.age>=12) & (data.gender==\"M\")]","3c803dce":"data.loc[(data.age>=12) & (data.gender!=\"M\")]","28b82bdc":"data.loc[1:3]","a826d2f0":"data.loc[(data.age>12,[\"city\",\"gender\"])]","712b7020":"# We often have to update values in our dataset based on a certain condition. For example, if the values in age are greater than equal to 12, then we want to update the values of the column section to be \u201cM\u201d.\n\n# update a column with condition\ndata.loc[(data.age >= 12), ['section']] = 'M'\ndata","794ece2e":"# In this example, if the value in the column age is greater than 20, then the loc function will update the values in the column section with \u201cS\u201d and the values in the column city with Pune:\n\ndata.loc[(data.age >= 20), ['section', 'city']] = ['S','Pune']\ndata","ce39cf9e":"data.iloc[[0,2]]","0d3e37ba":"data.iloc[[0,2],[1,3]]","83672c39":"data.iloc[1:3]","fff83a82":"data.iloc[1:3,1:3]","e4cb537d":"df = pd.read_csv(\"\/kaggle\/input\/drinksbycountrydata\/drinksbycountry.csv\")\ndf.head(20)","7a37b1fb":"# Step 1: Let's the datetype of the columns\ncol_types = df.dtypes.to_frame()\ncol_types.rename({0:\"type\"}, inplace = True, axis = 1)\ncol_types\ncol_types.to_csv(\"trick83data.csv\")","c80eeb6e":"# Step 2: Let's import the previous data and convert it to a dictionary\ncol_dict = pd.read_csv(\"trick83data.csv\", index_col = 0)[\"type\"].to_dict()","f2b16866":"# Step 3: Edit the dictionary with the correct data types\nprint(\"Original dictionary :\",col_dict)\n","db3a1d4b":"col_dict[\"country\"] = \"category\"\ncol_dict[\"continent\"] = \"category\"\nprint(\"Modified dictionary :\",col_dict)\n","a2b42f3f":"# Step 4: Use the dictionary to import the data\ndf = pd.read_csv(\"\/kaggle\/input\/drinksbycountrydata\/drinksbycountry.csv\", dtype=col_dict)\ndf.dtypes","d25fffaa":"# iloc is used to filter the rows and loc the columns\ndf = pd.read_csv(\"\/kaggle\/input\/drinksbycountrydata\/drinksbycountry.csv\", index_col=\"country\")\ndf.iloc[15:20, :].loc[:, \"beer_servings\":\"wine_servings\"]","e2c21c6a":"d = {\"customer\":[\"A\", \"B\", \"C\", \"D\", \"E\"], \"sales\":[100, \"100\", 50, 550.20, \"375.25\"]}\ndf = pd.DataFrame(d)\ndf","ad172fd3":"df.dtypes","c8413d93":"# everything seems  but this operation crashes df[\"sales\"].sum(). We have mixed data types\ndf.dtypes","26bc52e5":"df.dtypes\ndf[\"sales\"].apply(type) # Wow we can see that we have int, str, floats in one column\ndf[\"sales\"].apply(type).value_counts() # See the number of each value","b14cde4b":"df[\"sales\"] = df[\"sales\"].astype(float) # convert the data to float\ndf[\"sales\"].sum()\ndf[\"sales\"].apply(type).value_counts()","57da019c":"df = generate_sample_data().T\ndf","97c6046c":"df = generate_sample_data().T\ncols_str = list(map(str, list(df.columns))) # so that we can do df[\"0\"] as string for the example\ndf.columns = cols_str","a76e2569":"# Using pandas concatenation\n# if you are ever confused about axis = 1 or axis = 0, just put axis = \"columns\" or axis = \"rows\"\npd.concat([df.loc[:, \"0\":\"2\"], df.loc[:, \"6\":\"10\"], df.loc[:, \"16\":\"19\"]], axis = \"columns\") # ------------------> here we are selecting columns converted to strings","ace0f841":"# Using lists\n# please ntoe that df.columns is a series with index, so we are using index to filter # -------------------------> here we are selecting the index of columns\ndf[list(df.columns[0:3]) + list(df.columns[6:11]) + list(df.columns[16:20])]","6df83c12":"# Using numpy\ndf.iloc[:, np.r_[0:3, 6:11, 16:20]] # probably the most beautiful solution","ab1eed73":"df = generate_sample_data()\ndf.head()\ndf.shape","3c34c22a":"# absolute values\n(df[\"A\"] < 5).sum()\nprint(\"In the columns A we have {} of rows that are below 5\".format((df[\"A\"] < 5).sum()))\n","2d787203":"# percentage\n(df[\"A\"] < 5).mean()\nprint(\"In the columns A the values that are below 5 represent {}%\".format((df[\"A\"] < 5).mean()))","551c54fb":"df = pd.read_csv(\"\/kaggle\/input\/imdbmoviedata\/IMDB-Movie-Data.csv\", usecols=[\"Title\"])\ndf[\"Words\"] = df[\"Title\"].str.count(\" \") +1\ndf","f61aaaef":"df = pd.read_csv(\"..\/input\/imdbmoviedata\/IMDB-Movie-Data.csv\")\ndf.head()","33333122":"meta = df.pop(\"Metascore\").to_frame()\ndf.head()\nmeta.head()","4adbb21e":"df = pd.read_csv(\"..\/input\/imdbmoviedata\/IMDB-Movie-Data.csv\")\ndf.head()","1f562dd9":"# Using cut you can specify the bin edges\npd.cut(df[\"Metascore\"], bins = [0, 25, 50, 75, 99]).head()","38bc848d":"# Using qcut you can specify the number of bins and it fill generate of aproximate equal size\npd.qcut(df[\"Metascore\"], q = 3).head()","185245cc":"# cut and qcut accept label bin size\npd.qcut(df[\"Metascore\"], q = 4, labels = [\"awful\", \"bad\", \"average\", \"good\"]).head()","70af1378":"print(pd.__version__)\nprint(pd.show_versions())","2e78d8c7":"d = {\"A\":[1, 2, 3, 4,], \"B\":[1.0, 2.0, 3.0, 4.0], \"C\":[1.00000, 2.00000, 3.00000, 4.000003], \"D\":[1.0, 2.0, 3.0, 4.0], \"E\":[4.0, 2.0, 3.0, 1.0]}\ndf = pd.DataFrame(d)\ndf","f0786ed5":"df[\"A\"].equals(df[\"B\"]) # they requiere identical datatypes\ndf[\"B\"].equals(df[\"C\"])\ndf[\"B\"].equals(df[\"D\"])\ndf[\"B\"].equals(df[\"E\"]) # and the same order\n\nprint(pd.testing.assert_series_equal(df[\"A\"], df[\"B\"], check_names=False, check_dtype=False)) # assertion passes","12ad77f6":"df = pd.read_csv(\"..\/input\/drinksbycountrydata\/drinksbycountry.csv\", usecols=[\"continent\", \"beer_servings\"])\ndf.head()","369c7a8e":"(df.assign(continent = df[\"continent\"].str.title(),\n           beer_ounces = df[\"beer_servings\"]*12,#                                     this will allow yo set a title\n           beer_galons = lambda df: df[\"beer_ounces\"]\/128).query(\"beer_galons > 30\").style.set_caption(\"Average beer consumption\"))","8f1070b2":"d = {\"state\":[\"ny\", \"CA\", \"Tx\", \"FI\"], \"country\":[\"USA\", \"usa\", \"UsA\", \"uSa\"], \"pop\":[1000000, 2000000, 30000, 40000]}\ndf = pd.DataFrame(d)\ndf","576ad22e":"int_types = [\"int64\"]\n# creating new columns\nfor col in df.columns:\n    ctype = str(df[col].dtype)\n    if ctype in int_types:\n        df[f'{col}_millions'] = df[col]\/1000000\n    elif ctype == \"object\":\n        df[f'{col}_new'] = df[col].str.upper()\n        # you can also drop the columns\n        df.drop(col, inplace = True, axis = \"columns\")\n        \ndf","263b12c8":"df = pd.read_csv(\"..\/input\/drinksbycountrydata\/drinksbycountry.csv\")\ndf\n\ndrink = \"wine\"","57b9f562":"# allows us to iterate fast over columns\ndf[f'{drink}_servings'].to_frame()","0c727426":"df = pd.DataFrame({\"gender\":[\"Male\", \"Female\", \"Female\", \"Male\"]})\ndf","647d6404":"# Getting this nasty warning\nmales = df[df[\"gender\"] == \"Male\"]\nmales[\"abbreviation\"] = \"M\"\n# Fixing the error\nprint(\"Fixing the warning with print\")\nmales = df[df[\"gender\"] == \"Male\"].copy()\nmales[\"abbreviation\"] = \"M\"\nmales","b8330cd3":"d = {\"salesperson\":[\"Nico\", \"Carlos\", \"Juan\", \"Nico\", \"Nico\", \"Juan\", \"Maria\", \"Carlos\"], \"item\":[\"Car\", \"Truck\", \"Car\", \"Truck\", \"cAr\", \"Car\", \"Truck\", \"Moto\"]}\ndf = pd.DataFrame(d)\ndf","53b0caf2":"# Fixing columns\ndf[\"salesperson\"] = df[\"salesperson\"].str.title()\ndf[\"item\"] = df[\"item\"].str.title()","67b12b8f":"df[\"count_by_person\"] = df.groupby(\"salesperson\").cumcount() + 1","77e89f14":"df[\"count_by_item\"] = df.groupby(\"item\").cumcount() + 1","52b65f17":"df[\"count_by_both\"] = df.groupby([\"salesperson\",\"item\"]).cumcount() + 1","177e23da":"df","d42b7fa9":"df = pd.DataFrame({\"gender\":[\"Male\", \"Female\", \"Female\", \"Male\"]})\ndf","c76c5a05":"# Getting this nasty warning\ndf[df[\"gender\"] == \"Male\"][\"gender\"] = 1\ndf[df[\"gender\"] == \"Female\"][\"gender\"] = 0\n\nprint(\"Fix using loc\")\ndf.loc[df[\"gender\"] == \"Male\", \"gender\"] = 1\ndf.loc[df[\"gender\"] == \"Female\", \"gender\"] = 0\ndf","d27f63b2":"d = {\"salesperson\":[\"Nico\", \"Carlos\", \"Juan\", \"Nico\", \"Nico\", \"Juan\", \"Maria\", \"Carlos\"], \"item\":[10, 120, 130, 200, 300, 550, 12.3, 200]}\ndf = pd.DataFrame(d)\ndf","d1abf158":"df[\"running_total\"] = df[\"item\"].cumsum()\ndf[\"running_total_by_person\"] = df.groupby(\"salesperson\")[\"item\"].cumsum()\ndf","5695e39f":"d = {\"orderid\":[1, 1, 1, 2, 2, 3, 4, 5], \"item\":[10, 120, 130, 200, 300, 550, 12.3, 200]}\ndf = pd.DataFrame(d)\ndf","9efe826a":"print(\"This is the output we want to aggregate to the original df\")\ndf.groupby(\"orderid\")[\"item\"].sum().to_frame()\n\ndf[\"total_items_sold\"] = df.groupby(\"orderid\")[\"item\"].transform(sum)\ndf","cc536312":"# we have empty rows and bad data\ndf = pd.read_csv(\"..\/input\/tricks58data\/trick58data.csv\")\ndf","532a55ed":"# importing correct data\ndf = pd.read_csv(\"..\/input\/tricks58data\/trick58data.csv\", header = 2, skiprows = [3,4])\ndf","df3eea33":"df = pd.read_csv(\"..\/input\/imdbmoviedata\/IMDB-Movie-Data.csv\")\ndf","117df149":"gbdf = df.groupby(\"Genre\")\ngbdf.get_group(\"Horror\")","bcbc1128":"df = pd.DataFrame({\"A\":[\"Male\", \"Female\", \"Female\", \"Male\"], \"B\":[\"x\", \"y\", \"z\", \"A\"], \"C\":[\"male\", \"female\", \"male\", \"female\"], \"D\":[1, 2, 3, 4]})\ndf","4909483b":"# first let's use applymap to convert to standarize the text\ndf = df.applymap(lambda x: x.lower() if type(x) == str else x)","81035b78":"mapping = {\"male\":0, \"female\":1}","2d92a600":"print(\"PROBLEM: Applies to the whole df but retruns None\")\ndf.applymap(mapping.get)","44e2e6ba":"print(\"Get the correct result but you have to specify the colums. If you don't want to do this, check the next result\")\ndf[[\"A\", \"C\"]].applymap(mapping.get)","8a3788fd":"print(\"Condtional apply map: if can map --> map else return the same value\")\ndf = df.applymap(lambda x: mapping[x] if x in mapping.keys() else x)\ndf","8c9b8890":"df = pd.read_csv(\"..\/input\/drinksbycountrydata\/drinksbycountry.csv\")\ndf","65e2a2c5":"print(\"Classical filter hard to read and mantain.\")\ndf[(df[\"continent\"] == \"Europe\") & (df[\"beer_servings\"] > 150) & (df[\"wine_servings\"] > 50) & (df[\"spirit_servings\"] < 60)]","8f45e5ac":"print(\"You can split it across multiple lines to make it more readable. But it's still hard to read.\")\ndf[\n    (df[\"continent\"] == \"Europe\") & \n    (df[\"beer_servings\"] > 150) & \n    (df[\"wine_servings\"] > 50) & \n    (df[\"spirit_servings\"] < 60)\n]","2df4ed08":"print(\"Solution saving criteria as objects\")\n\ncr1 = df[\"continent\"] == \"Europe\"\ncr2 = df[\"beer_servings\"] > 150\ncr3 = df[\"wine_servings\"] > 50\ncr4 = df[\"spirit_servings\"] < 60\n\ndf[cr1 & cr2 & cr3 & cr4]","62433521":"print(\"Solution using reduce\")\nfrom functools import reduce\n\ncriteria = reduce(lambda x, y: x & y, (cr1, cr2, cr3, cr4))\ndf[criteria]","f5dd6b6b":"df = generate_sample_data()\ndf[\"A_diff\"] = df[\"A\"].diff() # calculate the difference between 2 rows\ndf[\"A_diff_pct\"] = df[\"A\"].pct_change()*100 # calculates the porcentual variation between 2 rows\n\n# add some style\ndf.style.format({\"A_diff_pct\":'{:.2f}%'})","056bef9f":"df = generate_sample_data()\n\ndf.sample(frac = 0.5, random_state = 2)\ndf.sample(frac = 0.5, random_state = 2).reset_index(drop = True) # reset index after shuffeling\n","1c8290b7":"df = generate_sample_data()\n\ndf.plot(kind = \"line\")\ndf.plot(kind = \"bar\")\ndf.plot(kind = \"barh\")\ndf.plot(kind = \"hist\")\ndf.plot(kind = \"box\")\ndf.plot(kind = \"kde\")\ndf.plot(kind = \"area\")","f41b7992":"# the following plots requiere x and y\ndf.plot(x = \"A\", y = \"B\", kind = \"scatter\")\ndf.plot(x = \"A\", y = \"B\", kind = \"hexbin\")\ndf.plot(x = \"A\", y = \"B\", kind = \"pie\") # here you can pass only x but you need to add subplots = True\n","893ec28c":"df = pd.read_csv(\"..\/input\/titanic-data\/train.csv\")","8ceb567c":"# Solution 1: using str.cat \ndf[\"Name\"].str.cat(df[\"Sex\"], sep = \", \").head()","92ca9b25":"# using + sign\ndf[\"Name\"] + \", \" + df[\"Sex\"].head()","ef4382a4":"df = pd.read_csv(\"..\/input\/titanic-data\/train.csv\")\ndf","36ca9d9e":"d = {\"A\": [100, 200, 300, 400, 100], \"W\":[10, 5, 0, 3, 8]}\ndf = pd.DataFrame(d)\ndf\n\n# with replacement\ndf.sample(n = 5, replace = True, random_state = 2)\n\n# adding weights\ndf.sample(n = 5, replace = True, random_state = 2, weights = \"W\")","8dd46a3d":"d = {\"A\": [100, 200, 300, 400, 100], \"W\":[10, 5, 0, 3, 8]}\ndf = pd.DataFrame(d)\ndf\n\n# with replacement\ndf.sample(n = 5, replace = True, random_state = 2)\n\n# adding weights\ndf.sample(n = 5, replace = True, random_state = 2, weights = \"W\")","cfdf6e5f":"print(\"Default series\")\nser1 = pd.Series([10, 20])\nser1\n\nprint(\"Let's add a NaN to an int64 series\")\nser1 = pd.Series([10, 20, np.nan])\nser1 # Notice it has been converted to float64\n\nprint(\"But if we use Int64 than everything will work\")\nser1 = pd.Series([10, 20, np.nan], dtype = \"Int64\")\nser1","0f667e50":"d = {\"Team\":[\"FC Barcelona\", \"FC Real Madrid\"], \n    \"Players\":[\"Ter Stegen, Semedo, Piqu\u00e9, Lenglet, Alba, Rakitic, De Jong, Sergi Roberto, Messi, Su\u00e1rez, Griezmann\",\n               \"Courtois, Carvajal, Varane, Sergio Ramos, Mendy, Kroos, Valverde, Casemiro, Isco, Benzema, Bale\"]}\n\nprint(\"Notice that we have a list of players for each team separated by commas. Let's generate a row for each player.\")\ndf = pd.DataFrame(d)\ndf\n\nprint(\"Notice that we have converted to something similar seen in example 47.\")\ndf.assign(Players = df[\"Players\"].str.split(\",\"))\n\nprint(\"Now add explode and done.\")\ndf.assign(Players = df[\"Players\"].str.split(\",\")).explode(\"Players\")","cae68c72":"df = generate_sample_data()\ndf\n\n# create a local variable mean\nmean = df[\"A\"].mean()\n\n# now let's use in inside a query of pandas using @\ndf.query(\"A > @mean\")","50c7f43c":"d = {\"Team\":[\"FC Barcelona\", \"FC Real Madrid\"], \n    \"Players\":[[\"Ter Stegen\", \"Semedo\", \"Piqu\u00e9\", \"Lenglet\", \"Alba\", \"Rakitic\", \"De Jong\", \"Sergi Roberto\", \"Messi\", \"Su\u00e1rez\", \"Griezmann\"], \\\n               [\"Courtois\", \"Carvajal\", \"Varane\", \"Sergio Ramos\", \"Mendy\", \"Kroos\", \"Valverde\", \"Casemiro\", \"Isco\", \"Benzema\", \"Bale\"]]}\n\nprint(\"Notice that we have a list of players for each team. Let's generate a row for each player.\")\ndf = pd.DataFrame(d)\ndf\n\nprint(\"Using explode to generate new rows for each player.\")\ndf1 = df.explode(\"Players\")\ndf1\n\nprint(\"Reverse this operation with groupby and agg\")\ndf[\"Imploded\"] = df1.groupby(df1.index)[\"Players\"].agg(list)\ndf","76a7d87c":"d = {\"patient\":[1, 2, 3, 1, 1, 2], \"visit\":[2015, 2016, 2014, 2016, 2017, 2020]}\ndf = pd.DataFrame(d)\ndf.sort_values(\"visit\")\n\nprint(\"Let's get the last visit for each patient\")\ndf.groupby(\"patient\")[\"visit\"].last().to_frame()","4aaa11c6":"import pandas as pd\nfrom pandas.api.types import CategoricalDtype\nd = {\"ID\":[100, 101, 102, 103], \"quality\":[\"bad\", \"very good\", \"good\", \"excellent\"]}\ndf = pd.DataFrame(d)\ndf\n\nprint(\"Let's create our own categorical order.\")\ncat_type = CategoricalDtype([\"bad\", \"good\", \"very good\", \"excellent\"], ordered = True)\ndf[\"quality\"] = df[\"quality\"].astype(cat_type)\ndf\n\nprint(\"Now we can use logical sorting.\")\ndf = df.sort_values(\"quality\", ascending = True)\ndf\n\nprint(\"We can also filter this as if they are numbers. AMAZING.\")\ndf[df[\"quality\"] > \"bad\"]","25eed12b":"df = generate_sample_data()\nprint(\"Original df\")\ndf\n\ndf.style.hide_index().set_caption(\"Styled df with no index and a caption\")","44912421":"df = generate_sample_data()\ndf","98c245e8":"# using loc --> labels\ndf.loc[0, \"A\"]","c8d1404f":"# using iloc --> position\ndf.iloc[0, 0]","de09f76d":"\n# mixing labels and position with loc\ndf.loc[0, df.columns[0]]","93f2197e":"# mixing labels and position with loc\ndf.loc[df.index[0], \"A\"]","faa1f581":"# mixing labels and position with iloc\ndf.iloc[0, df.columns.get_loc(\"A\")]","8d747b99":"# mixing labels and position with iloc\ndf.iloc[df.index.get_loc(0), 0]","2918a2a6":"<a id=\"section-twelve\"><\/a>\n# Section  12 - Explore with US states data\n[Go back to the Table of Contents](#table_of_contents)","9a5b1bb6":"<a id=\"section-twentyfour\"><\/a>\n# Section  24 - Count the missing values\n[Go back to the Table of Contents](#table_of_contents)","364c6138":"<a id=\"section-fiftyfour\"><\/a>\n# Section  54 - Print current version of pandas and it's dependencies\n[Go back to the Table of Contents](#table_of_contents)","124f86f5":"<a id=\"section-eighteen\"><\/a>\n# Section  18 - Create Numpy Array: Create three columns with Zero values\n[Go back to the Table of Contents](#table_of_contents)","44dfaeba":"![image.png](attachment:image.png)","dfbf2077":"<a id=\"section-fourtytwo\"><\/a>\n# Section  42 - Select rows with indices using iloc\n[Go back to the Table of Contents](#table_of_contents)","196326bf":"<a id=\"section-fiftytwo\"><\/a>\n# Section  52 - Remove a column and store it as a separate series\n[Go back to the Table of Contents](#table_of_contents)","2c459aff":"We have seen here that both the Series and DataFrame objects contain an explicit\nindex that lets you reference and modify data. This Index object is an interesting\nstructure in itself, and it can be thought of either as an immutable array or as an\nordered set (technically a multiset, as Index objects may contain repeated values).\nThose views have some interesting consequences in the operations available on Index\nobjects. As a simple example, let\u2019s construct an Index from a list of integers:\n","81c5536b":"<a id=\"section-fiftyeight\"><\/a>\n# Section  58 - Select columns using f-strings (new in pandas 3.6+)\n[Go back to the Table of Contents](#table_of_contents)","223f9e14":"<a id=\"section-twentyeight\"><\/a>\n# Section  28 - Creating a time series dataset for testing\n[Go back to the Table of Contents](#table_of_contents)","ceb6f0c9":"<a id=\"section-seventyseven\"><\/a>\n# Section  77 - Create one row for each item in a list (explode)\n[Go back to the Table of Contents](#table_of_contents)","6a0d270a":"<a id=\"section-fiftyfive\"><\/a>\n# Section  55 - Check if 2 series are \"similar\"\n[Go back to the Table of Contents](#table_of_contents)","37bacfb1":"<a id=\"section-twentyone\"><\/a>\n# Section  21 - Convert a wide DF into a long one\n[Go back to the Table of Contents](#table_of_contents)","c921b8a5":"<a id=\"section-twentysix\"><\/a>\n# Section  26 - Combine the small categories into a single category named \"Others\" (using frequencies)\n[Go back to the Table of Contents](#table_of_contents)","cb57d3d4":"<a id=\"section-two\"><\/a>\n# Section  2 - The Pandas Dataframe Object\n[Go back to the Table of Contents](#table_of_contents)","fb8f4c3e":"If using NaN values is not the desired behavior, we can modify the fill value using\nappropriate object methods in place of the operators. For example, calling A.add(B)\nis equivalent to calling A + B, but allows optional explicit specification of the fill value\nfor any elements in A or B that might be missing:","4f88bb3d":"<a id=\"section-fourteen\"><\/a>\n# Section  14 - GroupBy: Split, Apply, Combine\n[Go back to the Table of Contents](#table_of_contents)\n","d72c348b":"<a id=\"section-fiftyseven\"><\/a>\n# Section  57 - Create a bunch of new columns using a for loop and f-strings df[f'{col}_new']\n[Go back to the Table of Contents](#table_of_contents)","863c9814":"<a id=\"section-fourtyone\"><\/a>\n# Section  41 - Update the values of multiple columns on selected rows\n[Go back to the Table of Contents](#table_of_contents)","595ba866":"<a id=\"section-fiftynine\"><\/a>\n# Section  59 - Fixing \"SettingWithCopyWarning\" when creating a new columns\n[Go back to the Table of Contents](#table_of_contents)","d1449e78":"<a id=\"section-thirtyfive\"><\/a>\n# Section  35 - Convert one type of values to others\n[Go back to the Table of Contents](#table_of_contents)","740a3f03":"<a id=\"section-thirtythree\"><\/a>\n# Section  33 - Named aggregations - avoids multi index\n[Go back to the Table of Contents](#table_of_contents)","abc712fa":"![image.png](attachment:image.png)","3dc4ff89":"DataFrame as a generalized NumPy array\n\nIf a Series is an analog of a one-dimensional array with flexible indices, a DataFrame\nis an analog of a two-dimensional array with both flexible row indices and flexible\ncolumn names. Just as you might think of a two-dimensional array as an ordered\nsequence of aligned one-dimensional columns, you can think of a DataFrame as a\nsequence of aligned Series objects. Here, by \u201caligned\u201d we mean that they share the\nsame index.\n\nTo demonstrate this, let\u2019s first construct a new Series listing the area of each of the\nfive states discussed in the previous section:\n","358f4c97":"loc in Pandas: loc is label-based, which means that we have to specify the name of the rows and columns that we need to filter out.\niloc in Pandas:On the other hand, iloc is integer index-based. So here, we have to specify rows and columns by their integer index.","ff0d2c5c":"<a id=\"section-fourtyfive\"><\/a>\n# Section  45 - Select a range of rows and columns using iloc\n[Go back to the Table of Contents](#table_of_contents)","d6e3de1d":"<a id=\"section-four\"><\/a>\n# Section  4 - Indexers: loc, iloc, and ix\n[Go back to the Table of Contents](#table_of_contents)","979a1ac8":"\n# Introduction to Pandas\n\n-- Pandas is a package for data manipulation and analysis in Python. The name Pandas is derived from the econometrics term Panel Data. Pandas incorporates two additional data structures into Python, namely Pandas Series and Pandas DataFrame. These data structures allow us to work with labeled and relational data in an easy and intuitive manner.\nPandas Series and DataFrames are designed for fast data analysis and manipulation, as well as being flexible and easy to use. Below are just a few features that makes Pandas an excellent package for data analysis:\n\n-- Allows the use of labels for rows and columns\n\n-- Can calculate rolling statistics on time series data\n\n-- Easy handling of NaN values\n\n-- Is able to load data of different formats into DataFrames\n\n-- Can join and merge different datasets together\n\n-- It integrates with NumPy and Matplotlib\n\nDocumentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/","0eab3ffe":"<a id=\"section-thirteen\"><\/a>\n# Section  13 - Aggregation and Grouping\n[Go back to the Table of Contents](#table_of_contents)","5f64f065":"<a id=\"section-fourty\"><\/a>\n# Section  40 - Update the values of a particular column on selected rows\n[Go back to the Table of Contents](#table_of_contents)","47150804":"<a id=\"section-thirtyseven\"><\/a>\n# Section  37 - loc vs iloc in pandas\n[Go back to the Table of Contents](#table_of_contents)","fc72f9b0":"<a id=\"section-seventyeight\"><\/a>\n# Section  78 - New aggregation function --> last()\n[Go back to the Table of Contents](#table_of_contents)","129ba8e2":"<a id=\"section-Eight\"><\/a>\n# Section  8 - Hierarchical Indexing (Multi Indexing)\n[Go back to the Table of Contents](#table_of_contents)","3eab1048":"This dtype=object means that the best common type representation NumPy could infer for the contents of the array is that they are Python objects. While this kind of\nobject array is useful for some purposes, any operations on the data will be done at the Python level, with much more overhead than the typically fast operations seen for\narrays with native types:","20ce3ffa":"<a id=\"section-three\"><\/a>\n# Section  3 - The Pandas Index Object\n[Go back to the Table of Contents](#table_of_contents)","4476a297":"<a id=\"section-seventyfour\"><\/a>\n# Section  74 - Store NaN in an integer type with Int64\n[Go back to the Table of Contents](#table_of_contents)","4eb9976f":"<a id=\"section-sixtynine\"><\/a>\n# Section  69 - Shuffle rows of a df (df.sample())\n[Go back to the Table of Contents](#table_of_contents)","0bb74feb":"<a id=\"section-fiftythree\"><\/a>\n# Section  53 - Convert continuos variable to categorical (cut and qcut)\n[Go back to the Table of Contents](#table_of_contents)","62b3963a":"<a id=\"section-thirtynine\"><\/a>\n# Section  39 - Select only required columns with a condition\n[Go back to the Table of Contents](#table_of_contents)","abdb0558":"<a id=\"section-Six\"><\/a>\n# Section  6 - Handling Missing Data\n[Go back to the Table of Contents](#table_of_contents)","9dcb0d90":"# Constructing Series objects\nWe\u2019ve already seen a few ways of constructing a Pandas Series from scratch; all of\nthem are some version of the following:\n\npd.Series(data, index=index)\n\nwhere index is an optional argument, and data can be one of many entities.\nFor example, data can be a list or NumPy array, in which case index defaults to an\ninteger sequence:","6eef39c8":"<a id=\"section-seventy\"><\/a>\n# Section  70 - Making plots with pandas\n[Go back to the Table of Contents](#table_of_contents)","7d6e5870":"<a id=\"section-thirtyfour\"><\/a>\n# Section  34 - Named aggregations on multiple columns- avoids multiindex\n[Go back to the Table of Contents](#table_of_contents)","c3230e27":"<a id=\"section-seventyone\"><\/a>\n# Section  71 - Concatenate 2 column strings\n[Go back to the Table of Contents](#table_of_contents)","9b391d05":"We can use the asso\u2010ciated object\u2019s arithmetic method and pass any desired fill_value to be used in place\nof missing entries. Here we\u2019ll fill with the mean of all values in A (which we compute\nby first stacking the rows of A):","73371514":"<a id=\"section-twentytwo\"><\/a>\n# Section  22 - Convert year and day of year into a single datetime column\n[Go back to the Table of Contents](#table_of_contents)","b1c7f4fd":"<a id=\"section-thirtytwo\"><\/a>\n# Section  32 - Aggregate you datetime by and filter weekends\n[Go back to the Table of Contents](#table_of_contents)","224b8245":"As we have seen, Pandas treats None and NaN as essentially interchangeable for indi\u2010 cating missing or null values. To facilitate this convention, there are several useful\nmethods for detecting, removing, and replacing null values in Pandas data structures.They are:\n\n![image.png](attachment:image.png)","467342d9":"The difference between data found in many tutorials and data in the real world is that real-world data is rarely clean and homogeneous. In particular, many interesting\ndatasets will have some amount of data missing. To make matters even more compli\u2010cated, different data sources may indicate missing data in different ways.\n\nIn this section, we will discuss some general considerations for missing data, discuss how Pandas chooses to represent it, and demonstrate some built-in Pandas tools for\nhandling missing data in Python. Here and throughout the book, we\u2019ll refer to miss\u2010ing data in general as null, NaN, or NA values.\n\nA number of schemes have been developed to indicate the presence of missing data in a table or DataFrame. Generally, they revolve around one of two strategies: using a\nmask that globally indicates missing values, or choosing a sentinel value that indicates a missing entry.\n\nIn the masking approach, the mask might be an entirely separate Boolean array, or it may involve appropriation of one bit in the data representation to locally indicate the\nnull status of a value.\n\nIn the sentinel approach, the sentinel value could be some data-specific convention, such as indicating a missing integer value with \u20139999 or some rare bit pattern, or it\ncould be a more global convention, such as indicating a missing floating-point value with NaN (Not a Number), a special value which is part of the IEEE floating-point specification\n\nNone of these approaches is without trade-offs: use of a separate mask array requires allocation of an additional Boolean array, which adds overhead in both storage and\ncomputation. A sentinel value reduces the range of valid values that can be repre\u2010sented, and may require extra (often non-optimized) logic in CPU and GPU arith\u2010 metic. Common special values like NaN are not available for all data types.\n\nAs in most cases where no universally optimal choice exists, different languages and systems use different conventions. For example, the R language uses reserved bit pat\u2010\nterns within each data type as sentinel values indicating missing data, while the SciDB system uses an extra byte attached to every cell to indicate a NA state.\n\n\n\n","c1e0d290":"<a id=\"section-thirtyone\"><\/a>\n# Section  31 - Rearrange columns in a df\n[Go back to the Table of Contents](#table_of_contents)","aa3069da":"<a id=\"section-fourtysix\"><\/a>\n# Section  46 - Correct the data types while importing the df\n[Go back to the Table of Contents](#table_of_contents)","fba6e300":"![image.png](attachment:image.png)","9ebab751":"<a id=\"section-twentythree\"><\/a>\n# Section  23 - Interactive plots in pandas\n[Go back to the Table of Contents](#table_of_contents)","268ac04b":"<a id=\"section-ninteen\"><\/a>\n# Section  19 - Loading sample of big data\n[Go back to the Table of Contents](#table_of_contents)","1f56d84b":"<a id=\"section-fourtyfour\"><\/a>\n# Section  44 - Select rows with particular indices and particular columns\n[Go back to the Table of Contents](#table_of_contents)","a3eadcaa":"<a id=\"section-sixtyfour\"><\/a>\n# Section  64 - Use header and skiprows to get rid of bad data or empty rows while importing\n[Go back to the Table of Contents](#table_of_contents)","2eabded4":"<a id=\"section-fourtyeight\"><\/a>\n# Section  48 - Use apply(type) to see if you have mixed data types\n[Go back to the Table of Contents](#table_of_contents)","620a3a79":"![image.png](attachment:image.png)","6e44aa11":"<a id=\"section-thirtysix\"><\/a>\n# Section  36 - Show fewer rows in a df\n[Go back to the Table of Contents](#table_of_contents)","840c6a62":"<a id=\"section-fiftysix\"><\/a>\n# Section  56 - Create new columns or overwrite using assing and set a title for the df\n[Go back to the Table of Contents](#table_of_contents)","9f3ccd62":"<a id=\"section-fiftyone\"><\/a>\n# Section  51 - Count the number of words in a pandas series\n[Go back to the Table of Contents](#table_of_contents)","9f3d0eb1":"![image.png](attachment:image.png)","5eb4ed9a":"<a id=\"section-fifty\"><\/a>\n# Section  50 - Count of rows that match a condition\n[Go back to the Table of Contents](#table_of_contents)","802e8498":"<a id=\"section-thirty\"><\/a>\n# Section  30 - Split names into first and last name\n[Go back to the Table of Contents](#table_of_contents)","56f9d526":"In the simple examples we just looked at, we were mainly concatenating DataFrames\nwith shared column names. In practice, data from different sources might have differ\u2010\nent sets of column names, and pd.concat offers several options in this case. Consider\nthe concatenation of the following two DataFrames, which have some (but not all!)\ncolumns in common:","d9e8cb38":"<a id=\"section-seventysix\"><\/a>\n# Section  76 - Use a local variable within a query in pandas (using @)\n[Go back to the Table of Contents](#table_of_contents)","fa54acc2":"<a id=\"section-sixtytwo\"><\/a>\n# Section  62 - Creating running totals with cumsum function\n[Go back to the Table of Contents](#table_of_contents)","ae958e0f":"<a id=\"section-thirtyeight\"><\/a>\n# Section  38 - Select a range of rows using loc\n[Go back to the Table of Contents](#table_of_contents)","c74087cc":"<a id=\"section-sixteen\"><\/a>\n# Section  16 - Vectorized String Operations\n[Go back to the Table of Contents](#table_of_contents)","1ed8ae9c":"<a id=\"section-seventynine\"><\/a>\n# Section  79 - Ordered categories (from pandas.api.types import CategoricalDtypee)\n[Go back to the Table of Contents](#table_of_contents)","370fc679":"Notice that in addition to casting the integer array to floating point, Pandas automati\u2010cally converts the None to a NaN value\n\n![image.png](attachment:image.png)","0c040574":"Simple aggregations can give you a flavor of your dataset, but often we would prefer to aggregate conditionally on some label or index: this is implemented in the socalled groupby operation. \nSplit, apply, combine\n\nA canonical example of this split-apply-combine operation, where the \u201capply\u201d is a summation aggregation,\n\n\u2022 The split step involves breaking up and grouping a DataFrame depending on the value of the specified key.\n\u2022 The apply step involves computing some function, usually an aggregate, transfor\u2010mation, or filtering, within the individual groups.\n\u2022 The combine step merges the results of these operations into an output array.\n\n![image.png](attachment:image.png)\n","67689769":"<a id=\"section-sixtyfive\"><\/a>\n# Section  65 - Accesing the groups of a groupby object (get_group())\n[Go back to the Table of Contents](#table_of_contents)","a797d10d":"<a id=\"section-sixtyseven\"><\/a>\n# Section  67 - Filtering a df with multiple criteria using reduce\n[Go back to the Table of Contents](#table_of_contents)","0c25816a":"<a id=\"section-ten\"><\/a>\n# Section  10 - Combining Datasets: Merge and Join\n[Go back to the Table of Contents](#table_of_contents)","c872f5eb":"<a id=\"section-eleven\"><\/a>\n# Section  11 - Difference Between Concat, Merge and Join\n[Go back to the Table of Contents](#table_of_contents)","4f5c6dda":"<a id = \"table_of_contents\"><\/a>\n# Table of contents\n\n* [Section  1 - The Pandas Series Object](#section-one)  \n* [Section 2 - The Pandas DataFrame Object](#section-two)\n* [Section 3 - The Pandas Index Object](#section-three)\n* [Section 4 - Indexers: loc, iloc, and ix](#section-four)\n* [Section 5 - Operating on Data in Pandas](#section-five)\n* [Section 6 - Handling Missing Data](#section-Six)\n* [Section 7 - Operating on Null Values](#section-Seven)\n* [Section 8 - Hierarchical Indexing (Multi Indexing)](#section-Eight)\n* [Section 9 - Combining Datasets: Concat and Append](#section-Nine)\n* [Section 10 - Combining Datasets: Merge and Join](#section-ten)\n* [Section 11 - Difference Between Concat, Merge and Join](#section-eleven)\n* [Section 12 - Explore with US states data](#section-twelve)\n* [Section 13 - Aggregation and Grouping](#section-thirteen)\n* [Section 14 - GroupBy: Split, Apply, Combine](#section-fourteen)\n* [Section 15 - Introduction to Pivot Tables](#section-fifteen)\n* [Section 16 - Vectorized String Operations](#section-sixteen)\n* [Section 17 - Working with Time Series](#section-seventeen)\n* [Section 18 - Create Numpy Array: Create three columns with Zero values](#section-eighteen)\n* [Section 19 - Loading sample of big data](#section-ninteen)\n* [Section 20 - How to avoid Unnamed: 0 columns](#section-twenty)\n* [Section 21 - Convert a wide DF into a long one](#section-twentyone)\n* [Section 22 - Convert year and day of year into a single datetime column](#section-twentytwo)\n* [Section 23 - Interactive plots in pandas](#section-twentythree)\n* [Section 24 - Count the missing values](#section-twentyfour)\n* [Section 25 - Save memory by fixing your datatype](#section-twentyfive)\n* [Section 26 - Combine the small categories into a single category named \"Others\" (using frequencies)](#section-twentysix)\n* [Section 27 - Clean Object column with mixed data using regex](#section-twentyseven)\n* [Section 28 - Creating a time series dataset for testing](#section-twentyeight)\n* [Section 29 - Moving columns to a specific location](#section-twentynine)\n* [Section 30 - Split names into first and last name](#section-thirty)\n* [Section 31 - Rearrange columns in a df](#section-thirtyone)\n* [Section 32 - Aggregate you datetime by and filter weekends](#section-thirtytwo)\n* [Section 33 - Named aggregations - avoids multi index](#section-thirtythree)\n* [Section 34 - Named aggregations on multiple columns- avoids multiindex](#section-thirtyfour)\n* [Section 35 - Convert one type of values to others](#section-thirtyfive)\n* [Section 36 - Show fewer rows in a df](#section-thirtysix)\n* [Section 37 - loc vs iloc in pandas](#section-thirtyseven)\n* [Section 38 - Select a range of rows using loc](#section-thirtyeight)\n* [Section 39 - Select only required columns with a condition](#section-thirtynine)\n* [Section 40 - Update the values of a particular column on selected rows](#section-fourty)\n* [Section 41 - Update the values of multiple columns on selected rows](#section-fourtyone)\n* [Section 42 - Select rows with indices using iloc](#section-fourtytwo)\n* [Section 43 - Select rows with particular indices and particular columns](#section-fourtythree)\n* [Section 44 - Select rows with particular indices and particular columns](#section-fourtyfour)\n* [Section 45 - Select a range of rows and columns using iloc](#section-fourtyfive)\n* [Section 46 - Correct the data types while importing the df](#section-fourtysix)\n* [Section 47 - Select data by label and position (chained iloc and loc)](#section-fourtyseven)\n* [Section 48 - Use apply(type) to see if you have mixed data types](#section-fourtyeight)\n* [Section 49 - Select multiple slices of columns from a df](#section-fourtynine)\n* [Section 50 - Count of rows that match a condition](#section-fifty)\n* [Section 51 - Count the number of words in a pandas series](#section-fiftyone)\n* [Section 52 - Remove a column and store it as a separate series](#section-fiftytwo)\n* [Section 53 - Convert continuos variable to categorical (cut and qcut)](#section-fiftythree)\n* [Section 54 - Print current version of pandas and it's dependencies](#section-fiftyfour)\n* [Section 55 - Check if 2 series are \"similar\"](#section-fiftyfive)\n* [Section 56 - Create new columns or overwrite using assing and set a title for the df](#section-fiftysix)\n* [Section 57 - Create a bunch of new columns using a for loop and f-strings df[f'{col}_new']](#section-fiftyseven)\n* [Section 58 - Select columns using f-strings (new in pandas 3.6+)](#section-fiftyeight)\n* [Section 59 - Fixing \"SettingWithCopyWarning\" when creating a new columns](#section-fiftynine)\n* [Section 60 - Calculate running count with groups using cumcount() + 1](#section-sixty)\n* [Section 61 - Fixing \"SettingWithCopyWarning\" when changing columns using loc](#section-sixtyone)\n* [Section 62 - Creating running totals with cumsum function](#section-sixtytwo)\n* [Section 63 - Combine the output of an aggregation with the original df using transform](#section-sixtythree)\n* [Section 64 - Use header and skiprows to get rid of bad data or empty rows while importing](#section-sixtyfour)\n* [Section 65 - Accesing the groups of a groupby object (get_group())](#section-sixtyfive)\n* [Section 66 - Apply a mappings or functions to the whole df (applymap)](#section-sixtysix)\n* [Section 67 - Filtering a df with multiple criteria using reduce](#section-sixtyseven)\n* [Section 68 - Calculate the difference between each row and the previous (diff())](#section-sixtyeight)\n* [Section 69 - Shuffle rows of a df (df.sample())](#section-sixtynine)\n* [Section 70 - Making plots with pandas](#section-seventy)\n* [Section 71 - Concatenate 2 column strings](#section-seventyone)\n* [Section 72 - Named aggregation with multiple columns passing tupples (new in pandas 0.25)](#section-seventytwo)\n* [Section 73 - Sampling with pandas (with replacement and weights)](#section-seventythree)\n* [Section 74 - Store NaN in an integer type with Int64](#section-seventyfour)\n* [Section 75 - Create rows for values separated by commas in a cell (assing and explode)](#section-seventyfive)\n* [Section 76 - Use a local variable within a query in pandas (using @)](#section-seventysix)\n* [Section 77 - Create one row for each item in a list (explode)](#section-seventyseven)\n* [Section 78 - New aggregation function --> last()](#section-seventyeight)\n* [Section 79 - Ordered categories (from pandas.api.types import CategoricalDtypee)](#section-seventynine)\n* [Section 80 - Style you df fast with hide_index() and set_caption()](#section-eighty)\n* [Section 81 - Pandas slicing loc and iloc](#section-eightyone)\n\n","8871cb0e":"<a id=\"section-eighty\"><\/a>\n# Section  80 - Style you df fast with hide_index() and set_caption()\n[Go back to the Table of Contents](#table_of_contents)","77f9b468":"<a id=\"section-sixtyeight\"><\/a>\n# Section  68 - Calculate the difference between each row and the previous (diff())\n[Go back to the Table of Contents](#table_of_contents)","d2861216":"<a id=\"section-five\"><\/a>\n# Section  5 - Operating on Data in Pandas\n[Go back to the Table of Contents](#table_of_contents)","e3daf718":"<a id=\"section-seventytwo\"><\/a>\n# Section  72 - Named aggregation with multiple columns passing tupples (new in pandas 0.25)\n[Go back to the Table of Contents](#table_of_contents)","e23c4ed4":"<a id=\"section-eightyone\"><\/a>\n# Section  81 - Pandas slicing loc and iloc\n[Go back to the Table of Contents](#table_of_contents)","4f80d717":"<a id=\"section-twentynine\"><\/a>\n# Section  29 - Moving columns to a specific location\n[Go back to the Table of Contents](#table_of_contents)","70676bba":"<a id=\"section-one\"><\/a>\n# Section  1 - The Pandas Series Object\n[Go back to the Table of Contents](#table_of_contents)\n\n\n","0c4209e3":"![image.png](attachment:image.png)\n![image.png](attachment:image.png)","630d65e1":"<a id=\"section-fourtyseven\"><\/a>\n# Section  47 - Select data by label and position (chained iloc and loc)\n[Go back to the Table of Contents](#table_of_contents)","70441a11":"<a id=\"section-fifteen\"><\/a>\n# Section  15 - Introduction to Pivot Tables\n[Go back to the Table of Contents](#table_of_contents)\n\n","2dabfef0":"In this way, you can think of a Pandas Series a bit like a specialization of a Python\ndictionary. A dictionary is a structure that maps arbitrary keys to a set of arbitrary\nvalues, and a Series is a structure that maps typed keys to a set of typed values. This\ntyping is important: just as the type-specific compiled code behind a NumPy array\nmakes it more efficient than a Python list for certain operations, the type information\nof a Pandas Series makes it much more efficient than Python dictionaries for certain\noperations.\nWe can make the Series-as-dictionary analogy even more clear by constructing a\nSeries object directly from a Python dictionary","69e47590":"<a id=\"section-sixtyone\"><\/a>\n# Section  61 - Fixing \"SettingWithCopyWarning\" when changing columns using loc\n[Go back to the Table of Contents](#table_of_contents)","e918b744":"![image.png](attachment:image.png)","6b28201a":"<a id=\"section-seventyfive\"><\/a>\n# Section  75 - Create rows for values separated by commas in a cell (assing and explode)\n[Go back to the Table of Contents](#table_of_contents)","7c6fc2ba":"<a id=\"section-seventeen\"><\/a>\n# Section  17 - Working with Time Series\n[Go back to the Table of Contents](#table_of_contents)","6930549d":"<a id=\"section-Seven\"><\/a>\n# Section  7 - Operating on Null Values\n[Go back to the Table of Contents](#table_of_contents)","a358b334":"<a id=\"section-fourtythree\"><\/a>\n# Section  43 - Select rows with particular indices and particular columns\n[Go back to the Table of Contents](#table_of_contents)","89b59f1c":"![image.png](attachment:image.png)","45d59b35":"![image.png](attachment:image.png)\n","269a3fda":"<a id=\"section-seventythree\"><\/a>\n# Section  73 - Sampling with pandas (with replacement and weights)\n[Go back to the Table of Contents](#table_of_contents)","0bf05d28":"<a id=\"section-twentyseven\"><\/a>\n# Section  27 - Clean Object column with mixed data using regex\n[Go back to the Table of Contents](#table_of_contents)","47ed3ca4":"For types that don\u2019t have an available sentinel value, Pandas automatically type-casts when NA values are present. For example, if we set a value in an integer array to\nnp.nan, it will automatically be upcast to a floating-point type to accommodate the NA:","5e421e21":"<a id=\"section-twenty\"><\/a>\n# Section  20 - How to avoid Unnamed: 0 columns\n[Go back to the Table of Contents](#table_of_contents)\n","f676e3df":"<a id=\"section-fourtynine\"><\/a>\n# Section  49 - Select multiple slices of columns from a df\n[Go back to the Table of Contents](#table_of_contents)","fd0e77ea":"At the very basic level, Pandas objects can be thought of as enhanced versions of NumPy structured arrays in which the rows and columns are identified with labels\nrather than simple integer indices\n\nlet\u2019s introduce these three fundamental Pandas data structures: the Series, DataFrame, and Index.","b269eb44":"<a id=\"section-sixty\"><\/a>\n# Section  60 - Calculate running count with groups using cumcount() + 1\n[Go back to the Table of Contents](#table_of_contents)","e2663a7c":"![image.png](attachment:image.png)","05b271ac":"<a id=\"section-twentyfive\"><\/a>\n# Section  25 - Save memory by fixing your datatype\n[Go back to the Table of Contents](#table_of_contents)","a9df3b1e":"<a id=\"section-seventythree\"><\/a>\n# Section  73 - Sampling with pandas (with replacement and weights)\n[Go back to the Table of Contents](#table_of_contents)","f786b8d1":"Notice that what is returned is not a set of DataFrames, but a DataFrameGroupBy\nobject. This object is where the magic is: you can think of it as a special view of the\nDataFrame, which is poised to dig into the groups but does no actual computation\nuntil the aggregation is applied. This \u201clazy evaluation\u201d approach means that common\naggregates can be implemented very efficiently in a way that is almost transparent to\nthe user.\n\n\nTo produce a result, we can apply an aggregate to this DataFrameGroupBy object,\nwhich will perform the appropriate apply\/combine steps to produce the desired\nresult:","03bbc88a":"<a id=\"section-sixtythree\"><\/a>\n# Section  63 - Combine the output of an aggregation with the original df using transform\n[Go back to the Table of Contents](#table_of_contents)","57ce0c52":"<a id=\"section-sixtysix\"><\/a>\n# Section  66 - Apply a mappings or functions to the whole df (applymap)\n[Go back to the Table of Contents](#table_of_contents)","4e74277c":"<a id=\"section-Nine\"><\/a>\n# Section  9 - Combining Datasets: Concat and Append\n[Go back to the Table of Contents](#table_of_contents)\n"}}