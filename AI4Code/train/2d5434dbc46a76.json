{"cell_type":{"65c6a6f2":"code","7694dde4":"code","c06557e0":"code","8e247c63":"code","6b0f6e65":"code","15b9eef3":"code","9b6669e5":"code","b72a4451":"code","6e609a48":"code","5909dbdd":"code","f033f469":"code","12595f2d":"code","35372df4":"code","bda4206d":"code","671aaf94":"code","3fcdccf6":"code","82ad87c1":"code","f27e2589":"code","28458bab":"code","35cfea7c":"code","a7a8f0d4":"code","d7f12395":"code","618b9c23":"code","8a38fd27":"code","50cb131f":"code","79d769e5":"code","35820502":"code","9f6079cf":"code","e366a909":"code","adbbc5b5":"code","0f386b9e":"code","b01c278a":"code","0dc6cbfd":"code","fb5e2554":"code","2326d082":"code","1b10d421":"code","d37b7f24":"code","416ff95b":"code","bce24f86":"code","64de5848":"code","4cd17b36":"code","4d382bbe":"code","a19dd366":"code","a312c195":"code","626db594":"code","2777b49b":"code","ec064c09":"code","d6aaf2ee":"code","0218aae4":"code","365ac1e0":"code","b5634791":"code","f92db4eb":"code","e5c2ce89":"code","235ff099":"code","870a0ed4":"code","92651a07":"code","e7f2cfdc":"code","fdf47a82":"code","e3b17087":"code","71ebe49a":"code","81d70892":"markdown","eb2ceafb":"markdown","85e73a78":"markdown","cf046d09":"markdown","2a6df91c":"markdown","ba541da1":"markdown","37fe9615":"markdown","1f802b78":"markdown","ec557a5b":"markdown","0609790d":"markdown"},"source":{"65c6a6f2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\n\nprint(\"file path imported:\")\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7694dde4":"import json\nimport matplotlib.pyplot as plt\nimport sklearn","c06557e0":"!pip install efaqa-corpus-zh","8e247c63":"# \u4f7f\u7528github\u4e0a\u7684\u5e93\uff08kaggle\u4e0a\u7684\u53ef\u80fd\u662f\u65e7\u7248\u672c\uff09\nimport efaqa_corpus_zh\ndata = list(efaqa_corpus_zh.load())","6b0f6e65":"num_post = len(data)\nnum_sentence = sum([len(post[\"chats\"]) for post in data])\navg_num_word = sum(len(chat[\"value\"]) for post in data for chat in post[\"chats\"]) \/ num_sentence\n\nprint(\"\u5e16\u5b50\u6570\u91cf\", num_post)\nprint(\"\u6587\u672c\u6761\u6570\uff08\u4e0d\u8ba1title\uff09\", num_sentence)\nprint(\"\u5e16\u5747\u6587\u672c\u6761\u6570\", num_sentence \/ num_post)\nprint(\"\u6587\u672c\u5e73\u5747\u957f\u5ea6\uff08\u4e0d\u8ba1title\uff09\/\u5b57\", avg_num_word)","15b9eef3":"# \u6570\u636e\u96c6\u5305\u542b20000\u4e2a\u6837\u672c\n# \u6bcf\u4e2a\u6837\u672c\u5373\u4e3a\u4e00\u4e2a\u5e16\u5b50\uff0c\u5185\u542b\u82e5\u5e72\u8ddf\u5e16\u548c\u76f8\u5173\u4fe1\u606f\ndata[0]  # \u5c55\u793a\u4e00\u6761\u6570\u636e","9b6669e5":"# \u6bcf\u4e2a\u5e16\u5b50\u662f\u4e00\u4e2a\u5b57\u5178\u5bf9\u8c61\uff0c\u5305\u542b\u5bf9\u8bdd\uff08\u8ddf\u5e16\uff09\u3001\u53d1\u5e16\u4eba\u3001\u6807\u9898\u3001hash\u7801\u3001\u5fc3\u7406\u72b6\u6001\u6807\u7b7e\uff08label\uff09\u4e94\u4e2a\u5c5e\u6027\n# label\u6807\u7b7e\u662f\u91cd\u8981\u5c5e\u6027\nprint(type(data[0]))\ndata[0].keys()","b72a4451":"# chats\u5b57\u6bb5\u662f\u4e00\u4e2a\u5217\u8868\uff0c\u5305\u542b\u82e5\u5e72\u8ddf\u5e16\nprint(type(data[0][\"chats\"]))\n\n# \u8ddf\u5e16\u7684\u7ed3\u6784\u662f\u5b57\u5178\u5bf9\u8c61\uff0c\u5305\u542b\u8ddf\u5e16\u65f6\u95f4\u3001\u5185\u5bb9\u3001\u53d1\u9001\u8005\uff08\u662f\u697c\u4e3b\u8fd8\u662f\u5176\u4ed6\u7528\u6237\uff09\u3001\u5185\u5bb9\u7c7b\u578b\uff08\u6587\u672c\u8fd8\u662f..\uff09\u3001\u6807\u7b7e\uff08\u662f\u5426\u4e3a\u95ee\u53e5\uff0c\u662f\u5426\u77e5\u8bc6\uff0c\u662f\u5426\u4e3a\u6d88\u6781\u6d88\u606f\uff09\ndata[0][\"chats\"][0]","6e609a48":"# \u6587\u672c\u7c7b\u578b\u662f\u552f\u4e00\u7684\u56de\u5e16\u7c7b\u578b\ncontent_types = set([follow[\"type\"] for lt in data for follow in lt[\"chats\"] ])\ncontent_types","5909dbdd":"data_fields = set([tuple(chat.keys()) for post in data for chat in post[\"chats\"]])\ndata_fields","f033f469":"# y_s1 = [post['label']['s1'] for post in data]\n# plt.plot(y_s1)\n# plt.show()","12595f2d":"# y_s2 = [post['label']['s2'] for post in data]\n# plt.plot(y_s2)\n# plt.show()","35372df4":"# y_s3 = [post['label']['s3'] for post in data]\n# plt.plot(y_s3)\n# plt.show()","bda4206d":"# \u5fc3\u7406\u72b6\u6001\u6807\u7b7e\nfor post in data[:2]:\n    print(\"------------------\")\n    for item in post[\"label\"].items():\n        print(item)","671aaf94":"# !pip install bert-serving-server\n# !pip install bert-serving-client","3fcdccf6":"# from bert_serving.client import BertClient\n# bc = BertClient()\n# print(bc.encode(['\u4e2d\u56fd', '\u7f8e\u56fd']))","82ad87c1":"import jieba\nfrom gensim.models import Word2Vec\nfrom gensim.models.word2vec import LineSentence","f27e2589":"stop_word_path = \"\/kaggle\/input\/tipdmcup2020-data-and-scripts\/resources\/special-words\/stop_words.txt\"\n# \u8bfb\u53d6\u505c\u7528\u8bcd\nstopwords = [line.strip() for line in open(stop_word_path, 'r', encoding='utf-8').readlines()]\n\nstopwords += [\" \", \"\uff0c\", \"\u3002\"]\n\nlen(stopwords)","28458bab":"# \u5c06\u6bcf\u4e2a\u5e16\u5b50\u4e2d\u7684\u6587\u672c\u5206\u79bb\u51fa\u6765\nlines = [[post[\"title\"]] + [chat[\"value\"] for chat in post[\"chats\"]] for post in data]","35cfea7c":"lines[0]","a7a8f0d4":"import jieba\nfrom functools import reduce\nfrom tqdm import tqdm","d7f12395":"# \u5206\u8bcd\nx = []\nfor cluster in tqdm(lines):\n    x_line = []\n    for line in cluster:\n        tmp = [char for char in jieba.lcut(line) if char not in stopwords]\n        x_line.append(tmp)\n    x.append(reduce(lambda a, b: a+b, x_line))","618b9c23":"x[0]","8a38fd27":"# \u9884\u6d4b\u76ee\u6807y\uff08\u5fc3\u7406\u72b6\u6001\u6807\u7b7e\uff09\ny_s1_raw = []\ny_s2_raw = []\ny_s3_raw = []\n\nfor post in tqdm(data):\n    cluster = {item[0]: item[1] for item in post[\"label\"].items()} \n    y_s1_raw.append(cluster[\"s1\"])\n    y_s2_raw.append(cluster[\"s2\"])\n    y_s3_raw.append(cluster[\"s3\"])","50cb131f":"y_map = {}\nfor label in y_s1_raw + y_s2_raw + y_s3_raw:\n    if label not in y_map:\n        y_map[label] = len(y_map)","79d769e5":"y_map","35820502":"y_s1 = [y_map[label] for label in y_s1_raw]\ny_s2 = [y_map[label] for label in y_s2_raw]\ny_s3 = [y_map[label] for label in y_s3_raw]","9f6079cf":"import gensim\nfrom sklearn.model_selection import train_test_split","e366a909":"# \u5206\u5272\u6d4b\u8bd5\u96c6\u548c\u8bad\u7ec3\u96c6\nx_train, x_test, y_train, y_test \\\n        = train_test_split(x, y_s1, test_size=0.3)","adbbc5b5":"# \u83b7\u5f97\u8bcd\u5d4c\u5165\nword2vec_model = gensim.models.Word2Vec(x_train)","0f386b9e":"# \u6587\u6863\u5411\u91cf\u8ba1\u7b97\u65b9\u6cd5\uff1a\u8bcd\u5411\u91cf\u7684\u52a0\u6743\u5e73\u5747\ndef get_doc_vec(x, word2vec_model):\n    doc_vec_s1 = []\n    zero_count = 0\n    for doc in tqdm(x):\n        tmp = [word2vec_model[word] for word in doc if word in word2vec_model]\n        if len(tmp) == 0:\n            avg = np.zeros(len(doc_vec_s1[0]))\n            zero_count += 1\n        else:\n            avg = [item\/len(tmp) for item in reduce(lambda lt1, lt2: [lt1[index]+lt2[index] for index in range(len(lt1))], tmp)]\n        doc_vec_s1.append(avg)\n    print(\"\u96f6\u5411\u91cf\u5360\u6bd4\", zero_count\/len(doc_vec_s1))\n    return doc_vec_s1","b01c278a":"doc_vec_s1 = get_doc_vec(x_train, word2vec_model)","0dc6cbfd":"len(doc_vec_s1)","fb5e2554":"len(doc_vec_s1[0])","2326d082":"from sklearn.svm import SVC  # \u652f\u6301\u5411\u91cf\u673a\u5206\u7c7b\u5668","1b10d421":"svm_model = SVC()","d37b7f24":"svm_model.fit(doc_vec_s1, y_train)","416ff95b":"predicted = svm_model.predict(get_doc_vec(x_test, word2vec_model))","bce24f86":"from sklearn import metrics  # \u6a21\u578b\u8bc4\u4ef7\u5de5\u5177\nprint(metrics.classification_report(y_test, predicted))","64de5848":"# \u6362\u6a21\u578b\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()","4cd17b36":"rf.fit(doc_vec_s1, y_train)\npredicted = rf.predict(get_doc_vec(x_test, word2vec_model))\nprint(metrics.classification_report(y_test, predicted))","4d382bbe":"# \u5206\u5272\u6d4b\u8bd5\u96c6\u548c\u8bad\u7ec3\u96c6\nx_train, x_test, y_train, y_test \\\n        = train_test_split(x, y_s2, test_size=0.3)","a19dd366":"doc_vec_s2 = get_doc_vec(x_train, word2vec_model)\nsvm_model = SVC()\nsvm_model.fit(doc_vec_s2, y_train)\npredicted = svm_model.predict(get_doc_vec(x_test, word2vec_model))","a312c195":"print(metrics.classification_report(y_test, predicted))","626db594":"# \u5206\u5272\u6d4b\u8bd5\u96c6\u548c\u8bad\u7ec3\u96c6\nx_train, x_test, y_train, y_test \\\n        = train_test_split(x, y_s3, test_size=0.3)","2777b49b":"doc_vec_s3 = get_doc_vec(x_train, word2vec_model)\nsvm_model = SVC()\nsvm_model.fit(doc_vec_s3, y_train)\npredicted = svm_model.predict(get_doc_vec(x_test, word2vec_model))","ec064c09":"print(metrics.classification_report(y_test, predicted))","d6aaf2ee":"y_s3_bi_map = [0 if item is 27 else 1 for item in y_s3]","0218aae4":"# \u5206\u5272\u6d4b\u8bd5\u96c6\u548c\u8bad\u7ec3\u96c6\nx_train, x_test, y_train, y_test \\\n        = train_test_split(x, y_s3_bi_map, test_size=0.3)","365ac1e0":"doc_vec_s3 = get_doc_vec(x_train, word2vec_model)\nsvm_model = SVC()\nsvm_model.fit(doc_vec_s3, y_train)\npredicted = svm_model.predict(get_doc_vec(x_test, word2vec_model))","b5634791":"print(metrics.classification_report(y_test, predicted))","f92db4eb":"rf_model = RandomForestClassifier()\nrf_model.fit(doc_vec_s3, y_train)\npredicted = rf_model.predict(get_doc_vec(x_test, word2vec_model))\nprint(metrics.classification_report(y_test, predicted))","e5c2ce89":"x_train_0 = [x_train[i] for i in range(len(x_train)) if y_train[i] is 0]\nx_train_1 = [x_train[i] for i in range(len(x_train)) if y_train[i] is 1]","235ff099":"# \u4e24\u4e2a\u7c7b\u522b\u7684\u6837\u672c\u6570\u91cf\u60ac\u6b8a\nprint(len(x_train_0))\nprint(len(x_train_1))","870a0ed4":"import random\nx_train_0_balanced = random.sample(x_train_0, len(x_train_1))","92651a07":"print(len(x_train_0_balanced))\nprint(len(x_train_1))","e7f2cfdc":"# \u5408\u6210\u65b0\u7684x_train\u548cy_train\ntuples = [(item, 0) for item in x_train_0_balanced] + [(item, 1) for item in x_train_1]\nrandom.shuffle(tuples)  # \u6253\u4e71\u987a\u5e8f\nx_train = [item[0] for item in tuples]\ny_train = [item[1] for item in tuples]\n\nprint(len(x_train))\nprint(len(y_train))","fdf47a82":"rf_model = RandomForestClassifier()\nrf_model.fit(get_doc_vec(x_train, word2vec_model), y_train)\npredicted = rf_model.predict(get_doc_vec(x_test, word2vec_model))\nprint(metrics.classification_report(y_true=y_test, y_pred=predicted))","e3b17087":"doc_vec_s3 = get_doc_vec(x_train, word2vec_model)\nsvm_model = SVC()\nsvm_model.fit(doc_vec_s3, y_train)\npredicted = svm_model.predict(get_doc_vec(x_test, word2vec_model))","71ebe49a":"print(metrics.classification_report(y_true=y_test, y_pred=predicted))","81d70892":"# \u8bad\u7ec3\u6a21\u578b\uff08s2\uff09","eb2ceafb":"# \u4e2d\u6587\u5fc3\u7406\u54a8\u8be2\u6570\u636e\u96c6","85e73a78":"# \u83b7\u5f97\u8bcd\u5411\u91cf(s1)","cf046d09":"# \u4e8c\u3001\u6570\u636e\u96c6\u63cf\u8ff0\u6027\u6570\u636e\u5206\u6790","2a6df91c":"# \u8bad\u7ec3\u6a21\u578b\uff08s1\uff09","ba541da1":"# \u4e09\u3001\u6587\u672c\u9884\u5904\u7406","37fe9615":"# \u8f6c\u5316\u4e3a\u4e8c\u5206\u7c7b\u6765\u6539\u8fdb","1f802b78":"# \u8bad\u7ec3\u6a21\u578b\uff08s3\uff09","ec557a5b":"# \u5747\u8861\u89c4\u6a21","0609790d":"# \u4e00\u3001\u6570\u636e\u96c6\u7ed3\u6784"}}