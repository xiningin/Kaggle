{"cell_type":{"51fa686b":"code","5c100266":"code","26873194":"code","4ca3e51c":"code","6d7ed1d8":"code","6c2347b1":"code","e29c28c7":"code","fcaa9f85":"code","8ef2f522":"code","8effaf38":"code","65ba6aaf":"code","fab1ffc3":"code","c263f32b":"code","5549c538":"code","0acd1b9d":"code","2f65c071":"code","1e51523d":"code","704d9960":"code","ee23ce27":"code","422c0022":"code","ba542e5e":"markdown","ce81e931":"markdown","744c74dd":"markdown","2c1b2ebe":"markdown","39eb180a":"markdown","a187a53a":"markdown","c39e1c77":"markdown","97c6595b":"markdown","7959140d":"markdown","57db5700":"markdown","9af22d77":"markdown","48e8524a":"markdown","03474ab8":"markdown","021a476d":"markdown"},"source":{"51fa686b":"import numpy as np # linear algebra\nimport pandas as pd \n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5c100266":"from sklearn.preprocessing import LabelEncoder\n\ndef encode_data(df):\n    '''\n    The function does not return, but transforms the input pd.DataFrame\n    \n    Encodes the Costa Rican Household Poverty Level data \n    following studies in https:\/\/www.kaggle.com\/mlisovyi\/categorical-variables-in-the-data\n    and the insight from https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/discussion\/61403#359631\n    \n    The following columns get transformed: edjefe, edjefa, dependency, idhogar\n    The user most likely will simply drop idhogar completely (after calculating houshold-level aggregates)\n    '''\n    \n    yes_no_map = {'no': 0, 'yes': 1}\n    \n    df['dependency'] = df['dependency'].replace(yes_no_map).astype(np.float32)\n    \n    df['edjefe'] = df['edjefe'].replace(yes_no_map).astype(np.float32)\n    df['edjefa'] = df['edjefa'].replace(yes_no_map).astype(np.float32)\n    \n    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])","26873194":"def do_features(df):\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2')\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] \/ df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean', 'count'],\n                'escolari': ['min', 'max', 'mean', 'std']\n               }\n    aggs_cat = {'dis': ['mean']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean']\n    # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n    df.fillna(0, inplace=True)\n    # do something advanced above...\n    \n    # Drop SQB variables, as they are just squres of other vars \n    df.drop([f_ for f_ in df.columns if f_.startswith('SQB') or f_ == 'agesq'], axis=1, inplace=True)\n    # Drop id's\n    df.drop(['Id'], axis=1, inplace=True)\n    # Drop repeated columns\n    df.drop(['hhsize', 'female', 'area2'], axis=1, inplace=True)\n    return df","4ca3e51c":"train = pd.read_csv('..\/input\/train.csv')\n#We do not need the test sample for this exercise\n#test = pd.read_csv('..\/input\/test.csv')","6d7ed1d8":"train.info()","6c2347b1":"def process_df(df_):\n    # fix categorical features\n    encode_data(df_)\n    #fill in missing values based on https:\/\/www.kaggle.com\/mlisovyi\/missing-values-in-the-data\n    for f_ in ['v2a1', 'v18q1', 'meaneduc', 'SQBmeaned']:\n        df_[f_] = df_[f_].fillna(0)\n    df_['rez_esc'] = df_['rez_esc'].fillna(-1)\n    # do feature engineering and drop useless columns\n    return do_features(df_)\n\ntrain = process_df(train)\n#test = process_df(test)","e29c28c7":"train.info()","fcaa9f85":"X = train.query('parentesco1==1')#.sample(frac=0.2)\n\n# pull out the target variable\ny = X['Target'] - 1\nX = X.drop(['Target'], axis=1)","8ef2f522":"cols_2_drop=[]\n\nX.drop((cols_2_drop+['idhogar']), axis=1, inplace=True)\n#test.drop((cols_2_drop+['idhogar']), axis=1, inplace=True)","8effaf38":"from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler","65ba6aaf":"rs = RobustScaler().fit(X)\nss = StandardScaler().fit(X)\nmm = MinMaxScaler().fit(X)","fab1ffc3":"X_rs = pd.DataFrame(rs.transform(X), columns=X.columns)\nX_ss = pd.DataFrame(ss.transform(X), columns=X.columns)\nX_mm = pd.DataFrame(mm.transform(X), columns=X.columns)","c263f32b":"import time\nfrom sklearn.base import clone\ndef transform_data(tr_, X_, configs_, tr_name_):\n    X_tr = {}\n    for i,params in configs_.items():\n        print('---------- {} -----------'.format(params))\n        t_start = time.clock()\n        X_tr[i] = clone(tr_).set_params(**params).fit_transform(X_ss)\n        t_end = time.clock()\n        print('{} fitted in {} sec'.format(tr_name_, t_end-t_start))\n    return X_tr","5549c538":"colors=['r','b','y','g']\ndef plot_transformed_data(X_tr_, y_, configs_, tr_name_):\n    for j, X_ in X_tr_.items():\n        plt.figure(figsize=(6,4))\n        for i in [3,0,1, 2]:\n            plt.scatter(X_[y_==i,0], X_[y_==i,1], c=colors[i], s=5, label=i+1)\n        plt.legend()\n        plt.title('{}: {}'.format(tr_name_, configs_[j]))","0acd1b9d":"from sklearn.manifold import TSNE\ntsne_configs = {1: dict(init='random'),\n                2: dict(init='pca'),\n                3: dict(init='pca', n_iter=5000),\n                4: dict(init='pca', n_iter=500),\n                5: dict(init='pca', learning_rate=50),\n                6: dict(init='pca', learning_rate=500),\n                7: dict(init='pca', perplexity=15),\n                8: dict(init='pca', perplexity=50)}\nX_tsne = transform_data(TSNE(n_components=2, random_state=314), \n                        X_rs, \n                        tsne_configs, \n                        't-SNE')","2f65c071":"plot_transformed_data(X_tsne, y, tsne_configs, 't-SNE')","1e51523d":"from sklearn.manifold import MDS\n\nmds_configs = {1: dict(max_iter=100),\n               2: dict(max_iter=300),\n               3: dict(max_iter=500)}\nX_mds = transform_data(MDS(n_components=2, n_init=2, n_jobs=1, random_state=314), \n                       X_rs, \n                       mds_configs, \n                       'MDS')","704d9960":"plot_transformed_data(X_mds, y, mds_configs, 'MDS')","ee23ce27":"from sklearn.manifold import Isomap\n\niso_configs = {1: dict(n_neighbors=20),\n               2: dict(n_neighbors=50),\n               3: dict(n_neighbors=100)}\nX_isomap = transform_data(Isomap(n_components=2, n_jobs=4), \n                       X_rs, \n                       iso_configs, \n                       'Isomap')","422c0022":"plot_transformed_data(X_isomap, y, iso_configs, 'Isomap')","ba542e5e":"# MDS\nDefine several variants to see dependence on the input parameters","ce81e931":"# t-SNE\nDefine several variants to see dependence on the input parameters","744c74dd":"# Read in the data and clean it up","2c1b2ebe":"# Isomap\nDefine several variants to see dependence on the input parameters","39eb180a":"**There is also feature engineering magic happening here:**","a187a53a":"# VERY IMPORTANT\n> Note that ONLY the heads of household are used in scoring. All household members are included in test + the sample submission, but only heads of households are scored.","c39e1c77":"The following categorical mapping originates from [this kernel](https:\/\/www.kaggle.com\/mlisovyi\/categorical-variables-encoding-function).","97c6595b":"Note the change in the number of features of different type. What we did was:\n- encoded categorical variables appropreately into numerical values;\n- dropped a few irrelevant columns;\n- added several columns with household aggregates and cand-crafted ratio and subtraction features","7959140d":"As one can see, the green dots (class 4 = _non vulnerable households_) can be separated from the rest (in particular, for `max_iter=500`), while the other three classes are closely mixed together","57db5700":"Define helper functions  `transform_data` and `plot_transformed_data` that will be used by all following clustering algorithms.","9af22d77":"# Scale the training data\nData data has to be rescaled to mean of 0 and variance of 1, since most of the following algorithms use distance metric. We will initialise 3 different scalers: `StandardScaler`, `RobustScaler` and `MinMaxScaler` but only one will be used later on","48e8524a":"As one can see, the green dots (class 4 = _non vulnerable households_) can be separated from the rest, while the other three classes are closely mixed together","03474ab8":"# Study if classes form separable clusters\nThis kernel closely follows https:\/\/www.kaggle.com\/mlisovyi\/feature-engineering-lighgbm-with-f1-macro for data cleaning and feature engineering. \n\n# Short summary\nIt seems that the class 4 = _\"non vulnerable households\"_ form clusters that can be separated from the rest of the clusters. The rest of classes are all mixed together","021a476d":"As one can see, the green dots (class 4 = _non vulnerable households_) can be separated from the rest, while the other three classes are closely mixed together"}}