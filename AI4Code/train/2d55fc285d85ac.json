{"cell_type":{"41696be0":"code","669bce9a":"code","3a22c4e0":"code","af337062":"code","0976713b":"code","69734788":"code","1730e188":"code","6c7de5e8":"code","0b993df1":"code","7662ad7b":"code","4e8e8f51":"code","80b6020a":"code","38d5cab2":"code","fc3051fd":"code","0ffa69cf":"code","3cba4eb6":"code","375b3706":"code","5339f634":"code","95745fcf":"code","5a72402f":"code","361b1b8a":"code","016aa736":"code","3af2bcd5":"code","3d093a9e":"code","8ccd30b5":"code","7e6bad10":"code","79306e26":"code","02f571be":"code","7f22a470":"code","3cae3154":"code","6a96faa6":"code","2b87fdd1":"code","95399bbe":"code","b1b376ac":"markdown","9d27a2ab":"markdown","df0ef998":"markdown","d7d67c38":"markdown","a9c349ff":"markdown","5ee8c070":"markdown","0c5e2f1b":"markdown","0456f99e":"markdown","6c233340":"markdown","956dcd17":"markdown","8babe97b":"markdown","50804618":"markdown","f638aba3":"markdown","8da516a4":"markdown","e739bb6b":"markdown","6cbbc101":"markdown","1f639dee":"markdown","da7f70cf":"markdown","df309461":"markdown","8624d495":"markdown","4b796625":"markdown","2a943762":"markdown","6653c095":"markdown","f1dd5cca":"markdown","76823389":"markdown"},"source":{"41696be0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, random, cv2, pickle, json, itertools\nimport imgaug.augmenters as iaa\nimport imgaug.imgaug\n\nfrom IPython.display import SVG\nfrom tensorflow.keras.utils import plot_model, model_to_dot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\nfrom sklearn.utils import class_weight\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelBinarizer\n\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (Add, Input, Conv2D, Dropout, Activation, BatchNormalization, MaxPooling2D, ZeroPadding2D, AveragePooling2D, Flatten, Dense)\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, Callback\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.initializers import *","669bce9a":"def show_final_history(history):\n    \n    plt.style.use(\"ggplot\")\n    fig, ax = plt.subplots(1,2,figsize=(15,5))\n    ax[0].set_title('Loss')\n    ax[1].set_title('Accuracy')\n    ax[0].plot(history.history['loss'],label='Train Loss')\n    ax[0].plot(history.history['val_loss'],label='Validation Loss')\n    ax[1].plot(history.history['accuracy'],label='Train Accuracy')\n    ax[1].plot(history.history['val_accuracy'],label='Validation Accuracy')\n    \n    ax[0].legend(loc='upper right')\n    ax[1].legend(loc='lower right')\n    plt.show();\n    pass","3a22c4e0":"def plot_confusion_matrix(cm,classes,title='Confusion Matrix',cmap=plt.cm.Blues):\n    \n#     np.seterr(divide='ignore',invalid='ignore')\n    cm = cm.astype('float')\/cm.sum(axis=1)[:,np.newaxis]\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm,interpolation='nearest',cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes,rotation=45)\n    plt.yticks(tick_marks, classes)\n    \n    fmt = '.2f'\n    thresh = cm.max()\/2.\n    for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n        plt.text(j,i,format(cm[i,j],fmt),\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i,j] > thresh else \"black\")\n        pass\n    \n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    pass","af337062":"datasets = ['..\/input\/satellite-imagery-of-ships']\n\nclass_names = [\"no-ship\",\"ship\"]\n\nclass_name_labels = {class_name:i for i,class_name in enumerate(class_names)}\n\nnum_classes = len(class_names)\nclass_name_labels","0976713b":"def load_data():\n    images, labels = [], []\n    \n    for dataset in datasets:\n        \n        for folder in os.listdir(dataset):\n            label = class_name_labels[folder]\n            \n            for file in tqdm(os.listdir(os.path.join(dataset,folder))):\n                \n                img_path = os.path.join(dataset,folder,file)\n                \n                img = cv2.imread(img_path)\n                img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n                img = cv2.resize(img, (48,48))\n                \n                images.append(img)\n                labels.append(label)\n                pass\n            pass\n        \n        images = np.array(images,dtype=np.float32)\/255.0\n        labels = np.array(labels,dtype=np.float32)\n        pass\n    \n    return (images, labels)\n    pass","69734788":"(images, labels) = load_data()\nimages.shape, labels.shape","1730e188":"n_labels = labels.shape[0]\n\n_, count = np.unique(labels, return_counts=True)\n\ndf = pd.DataFrame(data = count)\ndf['Class Label'] = class_names\ndf.columns = ['Count','Class-Label']\ndf.set_index('Class-Label',inplace=True)\ndf","6c7de5e8":"df.plot.bar(rot=0)\nplt.title(\"distribution of images per class\");","0b993df1":"plt.pie(count,\n       explode=(0,0),\n       labels=class_names,\n       autopct=\"%1.2f%%\")\nplt.axis('equal');","7662ad7b":"AUGMENTATION = True","4e8e8f51":"def augment_add(images, seq, labels):\n    \n    augmented_images, augmented_labels = [],[]\n    for idx,img in tqdm(enumerate(images)):\n        \n        if labels[idx] == 1:\n            image_aug_1 = seq.augment_image(image=img)\n            image_aug_2 = seq.augment_image(image=img)\n            augmented_images.append(image_aug_1)\n            augmented_images.append(image_aug_2)\n            augmented_labels.append(labels[idx])\n            augmented_labels.append(labels[idx])\n        pass\n    \n    augmented_images = np.array(augmented_images, dtype=np.float32)\n    augmented_labels = np.array(augmented_labels, dtype=np.float32)\n    \n    return (augmented_images, augmented_labels)\n    pass","80b6020a":"seq = iaa.Sequential([\n    iaa.Fliplr(0.5),\n    iaa.Crop(percent=(0,0.1)),\n    iaa.LinearContrast((0.75,1.5)),\n    iaa.Multiply((0.8,1.2), per_channel=0.2),\n    iaa.Affine(\n        scale={'x':(0.8,1.2), \"y\":(0.8,1.2)},\n        translate_percent={\"x\":(-0.2,0.2),\"y\":(-0.2,0.2)},\n        rotate=(-25,25),\n        shear=(-8,8)\n    )\n], random_order=True)","38d5cab2":"if AUGMENTATION:\n    (aug_images, aug_labels) = augment_add(images, seq, labels)\n    images = np.concatenate([images, aug_images])\n    labels = np.concatenate([labels, aug_labels])","fc3051fd":"images.shape, labels.shape","0ffa69cf":"if AUGMENTATION:\n    _, count = np.unique(labels, return_counts=True)\n\n    plt.pie(count,\n           explode=(0,0),\n           labels=class_names,\n           autopct=\"%1.2f%%\")\n    plt.axis('equal');","3cba4eb6":"labels = to_categorical(labels)","375b3706":"np.random.seed(42)\nnp.random.shuffle(images)\n\nnp.random.seed(42)\nnp.random.shuffle(labels)","5339f634":"total_count = len(images)\ntotal_count\n\ntrain = int(0.7*total_count)\nval = int(0.2*total_count)\ntest = int(0.1*total_count)\n\ntrain_images, train_labels = images[:train], labels[:train]\nval_images, val_labels = images[train:(val+train)], labels[train:(val+train)]\ntest_images, test_labels = images[-test:], labels[-test:]\n\ntrain_images.shape, val_images.shape, test_images.shape","95745fcf":"if not AUGMENTATION:\n    count_labels = train_labels.sum(axis=0)\n\n    classTotals = train_labels.sum(axis=0)\n    classWeight = {}\n\n    for i in range(0,len(classTotals)):\n        classWeight[i] = classTotals.max()\/classTotals[i]\n        pass\n    print(classWeight)","5a72402f":"def conv_block(X,k,filters,stage,block,s=2):\n    \n    conv_base_name = 'conv_' + str(stage)+block+'_branch'\n    bn_base_name = 'bn_'+str(stage)+block+\"_branch\"\n    \n    F1 = filters\n    \n    X = Conv2D(filters=F1, kernel_size=(k,k), strides=(s,s),\n              padding='same',name=conv_base_name+'2a')(X)\n    X = BatchNormalization(name=bn_base_name+'2a')(X)\n    X = Activation('relu')(X)\n    \n    return X\n    pass","361b1b8a":"def basic_model(input_shape,classes):\n    \n    X_input = Input(input_shape)\n    \n    X = ZeroPadding2D((5,5))(X_input)\n    \n    X = Conv2D(16,(3,3),strides=(2,2),name='conv1',padding=\"same\")(X)\n    X = BatchNormalization(name='bn_conv1')(X)\n    \n    # stage 2\n    X = conv_block(X,3,32,2,block='A',s=1)\n    X = MaxPooling2D((2,2))(X)\n    X = Dropout(0.25)(X)\n\n#     Stage 3\n    X = conv_block(X,5,32,3,block='A',s=2)\n    X = MaxPooling2D((2,2))(X)\n    X = Dropout(0.25)(X)\n    \n#     Stage 4\n    X = conv_block(X,3,64,4,block='A',s=1)\n    X = MaxPooling2D((2,2))(X)\n    X = Dropout(0.25)(X)\n    \n#   Output Layer\n    X = Flatten()(X)\n    X = Dense(64)(X)\n    X = Dropout(0.5)(X)\n    \n    X = Dense(128)(X)\n    X = Activation(\"relu\")(X)\n    \n    X = Dense(classes,activation=\"softmax\",name=\"fc\"+str(classes))(X)\n    \n    model = Model(inputs=X_input,outputs=X,name='Feature_Extraction_and_FC')\n    \n    return model\n    pass","016aa736":"model = basic_model(input_shape=(48,48,3),classes=2)","3af2bcd5":"plot_model(model,to_file='basic_model.png')\nSVG(model_to_dot(model).create(prog='dot',format='svg'))\n\nmodel.summary()","3d093a9e":"opt = Adam(lr=1e-3)\nmodel.compile(optimizer=opt,loss='binary_crossentropy',metrics=['accuracy'])","8ccd30b5":"checkpoint = ModelCheckpoint(\"model_weights.h5\",monitor='val_accuracy',verbose=1,save_best_only=True,mode='max')\nlogs = TensorBoard(\"logs\")","7e6bad10":"epochs = 50\nbatch_size = 16\n\nhistory = model.fit(train_images,train_labels,\n                   steps_per_epoch=len(train_images)\/\/batch_size,\n                   epochs=epochs,\n                   verbose=1, \n                   validation_data=(val_images,val_labels),\n                   validation_steps=len(val_images)\/\/batch_size,\n                   callbacks=[checkpoint, logs]\n#                    class_weight=classWeight # Uncomment if AUGMENTATION is set to FALSE\n                   )","79306e26":"show_final_history(history)","02f571be":"val_pred = model.predict(val_images)\nval_pred = np.argmax(val_pred,axis=1)\nval_pred.shape","7f22a470":"val_actual = np.argmax(val_labels,axis=1)\n\ncnf_mat = confusion_matrix(val_actual, val_pred)\nnp.set_printoptions(precision=2)\n\nplt.figure()\nplot_confusion_matrix(cnf_mat,classes=class_names)\nplt.grid(None)\nplt.show();","3cae3154":"test_pred = model.predict(test_images)\ntest_pred = np.argmax(test_pred,axis=1)\ntest_pred.shape","6a96faa6":"test_actual = np.argmax(test_labels,axis=1)\n\ncnf_mat_test = confusion_matrix(test_actual, test_pred)\nnp.set_printoptions(precision=2)\n\nplt.figure()\nplot_confusion_matrix(cnf_mat_test,classes=class_names)\nplt.grid(None)\nplt.show()","2b87fdd1":"rnd_idx = random.sample(range(0,400),10)\n\nclass_labels = {i:class_name for (class_name,i) in class_name_labels.items()}\nclass_labels\n\n# fig, ax = plt.subplots(2,5,figsize=(5,5))\n\nfor i,idx in enumerate(rnd_idx):\n    \n    plt.imshow(test_images[idx])\n    plt.title(\"Actual: {}\\nPredicted: {}\".format(class_labels[test_actual[idx]],class_labels[test_pred[idx]]))\n    plt.grid(None)\n    plt.show()\n    pass","95399bbe":"model.save(\"ship-model.h5\")","b1b376ac":"# Uploading the images\n\nThe images from the **satellite-imagery-of-ships** is loaded into numpy arrays, with labels [0,1] corresponding to the classes *no-ship* and *ship*. The data was loaded into numpy arrays as data augmentation and upsampling\/downsampling is easier to perform. ","9d27a2ab":"# Augmenting Images of Minority Class\n\nThe images present in the *ship* class are augmented and then stored in the dataset, so that there is an equal representation of the classes. The current ratio of classes is 1:3, meaning that for every image present in the *ship* class there are 3 images present in the *no-ship* class. This will be countered by producing 2 augmented images per original image of the *ship* class. This will make the dataset balanced.","df0ef998":"# EDA of dataset\n\n* bar-plot - Bar plot is made to find the count of images per class\n* pie-plot - Pie plot is drawn to find the percentage of class distribution in the dataset","d7d67c38":"# Importing libraries\n\nThe necessary libraries required for implementing the Faster R-CNN model are imported in the code block below.","a9c349ff":"As it can be seen that the data is skewed towards the *no-ship* class with 75% of images belonging to that very specific class.","5ee8c070":"# Plotting Loss and Accuracy\n\nThe loss and accuracy for the training and validation datasets is plotted using the *show_final_history* function","0c5e2f1b":"# One Hot Encoding Variables\n\nThe labels numpy array is one hot encoded using *to_categorical* from keras. This removes any uncessary bias in the dataset, by keeping the class at equal footing, with respect to labels. ","0456f99e":"# Training, Validation and Testing\n\nInstead of using *train_test_split* the images and labels arrays are randomly shuffled using the same seed value set at *42*. This allows the images and their corresponding labels to remain linked even after shuffling. \n\nThis method allows the user to make all 3 datasets. The training and validation dataset is used for training the model while the testing dataset is used for testing the model on unseen data. Unseen data is used for simulating real-world prediction, as the model has not seen this data before. It allows the developers to see how robust the model is.","6c233340":"# Confusion Matrix for Testing Dataset","956dcd17":"## Creation of Model\n\n### Block 1\n* An input layer is initialised using the Input Keras layer, this defines the number of neurons present in the input layer\n* ZeroPadding is applied to the input image, so that boundary features are not lost.\n    \n### Block 2\n* First Convolutioanl Layer, it starts with 16 filters and kernel size with (3,3) and strides (2,2). Padding is maintaned same, so the image does not chaneg spatially, until the next block in which MaxPooling occurs\n        \n### Block 3 - 4\n* Similar structure in both with a convolutional layer followed by a MaxPooling and Dropout layers.\n\n### Output Block\n* The feature map produced by the previous convolutional layers is converted into a single column using Flatten Layer and the classified using a Dense layer(output layer) with the number of classes present in the dataset, and *sigmoid* as activation function.","8babe97b":"# Detection using Faster R-CNN\n\nThis notebook contains a process of creating an end to end Faster R-CNN model using tensorflow. First features are extracted from the images using a CNN model and then passed through a Region Proposal Network(RPN). All due credit for the algorithm goes to the authors of the paper, *Faster R-CNN: Towards Real-Time Object\nDetection with Region Proposal Networks*.\n\nIt is divided up into 3 notebooks:\n\n1. [Part 1](https:\/\/www.kaggle.com\/apollo2506\/ship-detection-using-faster-r-cnn-part-1): Creating a model for classifying the ROIs into the 2 classes, *ship* and *no-ship* .\n2. [Part 2](https:\/\/www.kaggle.com\/apollo2506\/ship-detection-using-faster-r-cnn-part-2): Using Selective Search for obtaining ROIs and testing of model on geospatial data.\n3. Part 3: Using Regional Proposal Networks to obtain the Region of Interests thereby decreasing computation time. Faster R-CNN","50804618":"Due to imbalance in dataset, upsampling is done on the minority class, by randomly duplicating images until the 2 classes have comparable distribution in the dataset. After this is done, the dataset will be split into the training, testing and validation sets by randomly shuffling them and then splitting.\n\nAnother way is to introduce class weights for each specific class. Each class is penalised with the specific class weight. Higher the class weight, greater the penalty. Classes with lower percentage have a higher penalty. This allows for the model to penalise itself heavily if class detected is incorrect.","f638aba3":"## Explanation of features\n\n* Conv2D - This is a 2 dimensional convolutional layer, the number of filters decide what the convolutional layer learns. Greater the number of filters, greater the amount of information obtained.\n\n![Conv2D Visualisation](https:\/\/www.pyimagesearch.com\/wp-content\/uploads\/2018\/12\/keras_conv2d_num_filters.png)\n\n* MaxPooling2D - This reduces the spatial dimensions of the feature map produced by the convolutional layer without losing any range information. This allows a model to become slightly more robust\n* Dropout - This removes a user-defined percentage of links between neurons of consecutive layers. This allows the model to be robust. It can be used in both fully convolutional layers and fully connected layers.\n* BatchNormalization - This layer normalises the values present in the hidden part of the neural network. This is similar to MinMax\/Standard scaling applied in machine learning algorithms\n* Padding- This pads the feature map\/input image with zeros allowing border features to stay.","8da516a4":"# Utility functions\n\n* show_final_history - For plotting the loss and accuracy of the training and validation datasets\n* plot_confusion_matrix - For plotting the percentage of true positives per class for a better feel of how the model predicted the data","e739bb6b":"### With Augmentation:\n\n* Precision:\n    - *no-ship* : 0.942\n    - *ship* : 0.989\n\n\n* Recall:\n    - *no-ship* : 0.99\n    - *ship* : 0.94\n    \n### With class weights\n\n* Precision:\n    - *no-ship* : 1.00\n    - *ship* : 0.99\n\n\n* Recall:\n    - *no-ship* : 0.99\n    - *ship* : 1.00","6cbbc101":"# Creation of model\n\n* **conv_block** - This function contains the convulotional layer, batch normalization and activation layers. The number of filters, kernel_size, strides to be taken are defined by the developer. This allows a developer to make the model without having to repeat the same lines continuously many times. It also uses the OOPs concepts of Python which is recommended instead of coding like it is *C*.\n\n* **basic_model** - This function creates the model using the aforementioned function, max pooling layers and dropouts. After the specified number of convolutional layers, a flatten layer is introduced, along with dense layers so that the image can be classified. The flatten layer converts the feature map produced by the convolutional layers into a single column for classification.\n\nFor image detection in images using Faster R-CNN the feature map produced by the convolutional layers is used, i.e., the model drops the layers after the final convolutional block and passes the generated feature map to a Regional Proposal Network, which can either uses a vanilla CNN model containing fully connected layers or a Logistic Regression, Support Vector machines or Random forests. Advised to use vanilla CNN as it uses less CPU and memory, plus slightly faster and capable of giving multiple outputs if required.","1f639dee":"Version 4: Using numpy array and custom data augmentation tool instead of ImageDataGenerator.\n\nVersion 5: Incorporated data augmentation of *ship* class, along with an option for using class weights instead.\n\nVersion 6: Training on augmented data for Parts *2 and 3*.","da7f70cf":"# Checking the model","df309461":"## Compiling the Model\n\nThe model is compiled using the *Adam* optimizer with learning rate set at 1e-3. *Binary Crossentropy* is used as a loss function as there are only two classes.","8624d495":"**Saving the model**","4b796625":"## Spliting of data\n\n* 70% - Training\n* 20% - Validation\n* 10% - Testing","2a943762":"If AUGMENTATION is set True, then the number of images per class is balanced. If AUGMENTATION is set to False, then compute the class weights given below and accordingly change the fit function of the Keras API when training.","6653c095":"If augmentation of dataset is required then set AUGMENTATION to *True*. This will balance the dataset via augmentation of minority classes. To train via class weights, then set AUGMENTATION to *False*.","f1dd5cca":"# Training model\n\nThe model is trained for 50 epochs with a batch size of 16. The best model weights are stored in the file *model_weight.h5* with TensorBoard logs being stored in the *logs* directory.","76823389":"# Confusion Matrix for Validation dataset\n\nThe confusion matrix"}}