{"cell_type":{"db2e1b10":"code","8b5a1f5b":"code","ba4ba663":"code","08d50953":"code","caf16df6":"code","8318d902":"code","da48bdfb":"code","d4f08f7b":"code","4b280af4":"code","9d229cf8":"code","d6e66dc9":"code","1e93ad67":"code","694063c4":"code","1dd26d64":"code","63420d64":"code","f22efeef":"code","ea27d3b0":"code","2f479f50":"code","9703ed6c":"code","60644d41":"code","e4c8a36f":"code","cdabcb9d":"code","309daf31":"code","dc1c1fee":"code","7bd64067":"code","4bd11a25":"code","759f330e":"code","35b68aea":"code","b59b4fa9":"code","1a8567bc":"code","9603ac00":"code","d9f9a6b1":"code","11640112":"code","4f71ffc7":"code","db17872a":"code","ff0dd6e9":"code","5bd1675b":"code","58e5887e":"code","d1895199":"code","fae5a6d9":"code","423b6aea":"code","4014693b":"code","40bb6be1":"code","035482e8":"code","2b91dc00":"code","6c0f577c":"code","8433be5e":"code","392331ed":"code","aa17ff15":"code","90696e9d":"code","e7704548":"code","c8f0835f":"code","0ad14cfe":"code","1f9e5460":"code","9500d6a9":"code","b3b2a458":"code","3eea6e8d":"code","81583da4":"code","48424e84":"code","3cd7ece5":"code","eb84a453":"code","b648b474":"code","d8bdbf99":"code","5f67b91f":"code","dfb5a0fa":"code","d2fb14ff":"code","a3f22225":"code","17f79135":"code","063b5107":"code","4d27ef16":"code","52b95cb7":"code","6b258801":"code","970fe4b1":"code","354a5029":"code","9c9096d8":"code","5b153fec":"code","019825b1":"code","c61de284":"code","1a6ee050":"code","45565fa2":"code","d02eb107":"code","7d5f893c":"code","121e7592":"code","6df4f16b":"code","d5f78c55":"code","3ff90627":"code","1871e242":"code","a8c6f2da":"code","9b39f5ed":"code","eb3bb307":"code","cbef5260":"code","ad493d43":"code","9e5a4d8e":"code","7f41045c":"code","8e55b910":"markdown","b1f8a904":"markdown","88b66848":"markdown","9efa633b":"markdown","a7b5954c":"markdown","067a10a9":"markdown","973200ba":"markdown","4a502365":"markdown","0400226c":"markdown","27dc8bd3":"markdown","983a4e7d":"markdown","97d1a981":"markdown","8181d430":"markdown","3bb9d591":"markdown","64146d35":"markdown","ad99d912":"markdown","a9deeb5c":"markdown","1b724089":"markdown","01be9687":"markdown","2cd0b914":"markdown","d809814d":"markdown","cde755f6":"markdown","38125a2e":"markdown","a678648d":"markdown","ee3c7ac5":"markdown","b6d6f78d":"markdown","096740d2":"markdown","807d909b":"markdown","1092a303":"markdown","a3d6a013":"markdown","6d564601":"markdown","ede81ebd":"markdown","f70cf154":"markdown","0b89f44d":"markdown","fe923c44":"markdown","cf6dfc63":"markdown","ceda62fe":"markdown"},"source":{"db2e1b10":"from fastai.tabular import *\nfrom isoweek import Week\n#import tarfile\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# path to external datasets\ntar = tarfile.open('\/kaggle\/input\/external-datasets\/rossmann.tgz', \"r:gz\")","8b5a1f5b":"# place holders\npath = \"\/kaggle\/input\/rossmann-store-sales\/\"\nbase_path=\"..\/output\"","ba4ba663":"# paths to kaggle data sets\ntrain = \"\/kaggle\/input\/rossmann-store-sales\/train.csv\"\ntest = \"\/kaggle\/input\/rossmann-store-sales\/test.csv\"\nstore = \"\/kaggle\/input\/rossmann-store-sales\/store.csv\"\n\n# paths to external tar file datasets\nstore_states = tar.extractfile('store_states.csv')\nstate_names = tar.extractfile('state_names.csv')\ngoogletrend = tar.extractfile('googletrend.csv')\nweather = tar.extractfile('weather.csv')","08d50953":"# read in kaggle and external datasets as dataframes\ntable_names = [train, store, store_states, state_names, googletrend, weather, test]\ntables = [pd.read_csv(fpath, low_memory=False) for fpath in table_names]\ntrain, store, store_states, state_names, googletrend, weather, test = tables\nlen(train),len(test)","caf16df6":"print(train.shape)\ntrain.head()","8318d902":"print(test.shape)\ntest.head()","da48bdfb":"print(store.shape)\nstore.head()","d4f08f7b":"print(store_states.shape)\nstore_states.head()","4b280af4":"print(googletrend.shape)\ngoogletrend.head()","9d229cf8":"print(weather.shape)\nweather.head()","d6e66dc9":"print(train.StateHoliday.unique())\nprint(test.StateHoliday.unique())","1e93ad67":"train.StateHoliday = train.StateHoliday!='0'\ntest.StateHoliday = test.StateHoliday!='0'","694063c4":"def join_df(left, right, left_on, right_on=None, suffix='_y'):\n    if right_on is None: right_on = left_on\n    return left.merge(right, how='left', left_on=left_on, right_on=right_on,\n                     suffixes=(\"\",suffix))","1dd26d64":"weather = join_df(weather, state_names, \"file\", \"StateName\")\nweather.head(3)","63420d64":"googletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0]\ngoogletrend['State'] = googletrend.file.str.split('_', expand=True)[2]\ngoogletrend.loc[googletrend.State=='NI', \"State\"] = 'HB,NI'","f22efeef":"googletrend.head(3)","ea27d3b0":"def add_datepart(df, fldname, drop=True, time=False):\n    \"Helper function that adds columns relevant to a date.\"\n    fld = df[fldname]\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) \/\/ 10 ** 9\n    if drop: df.drop(fldname, axis=1, inplace=True)","2f479f50":"add_datepart(googletrend,\"Date\", drop=False)\ngoogletrend.head(3)","9703ed6c":"# continue with all other tables\nadd_datepart(weather, \"Date\", drop=False)\nadd_datepart(train, \"Date\", drop=False)\nadd_datepart(test, \"Date\", drop=False)","60644d41":"trend_de = googletrend[googletrend.file == 'Rossmann_DE']\ntrend_de.head(3)","e4c8a36f":"store = join_df(store, store_states, \"Store\")\nlen(store[store.State.isnull()])","cdabcb9d":"joined = join_df(train, store, \"Store\")\njoined_test = join_df(test, store, \"Store\")\nlen(joined[joined.StoreType.isnull()]), len(joined_test[joined_test.StoreType.isnull()])","309daf31":"# join the joined df with googletrend with [\"State\",\"Year\",\"Week\"] as the index\n# this way the non matching day dates do not create issues.\njoined = join_df(joined, googletrend, [\"State\",\"Year\", \"Week\"])\njoined_test = join_df(joined_test, googletrend, [\"State\",\"Year\", \"Week\"])\nlen(joined[joined.trend.isnull()]),len(joined_test[joined_test.trend.isnull()])","dc1c1fee":"# now join the overal germany trend\njoined = joined.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\njoined_test = joined_test.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\nlen(joined[joined.trend_DE.isnull()]),len(joined_test[joined_test.trend_DE.isnull()])","7bd64067":"# finally join the weather data\njoined = join_df(joined, weather, [\"State\",\"Date\"])\njoined_test = join_df(joined_test, weather, [\"State\",\"Date\"])\nlen(joined[joined.Mean_TemperatureC.isnull()]),len(joined_test[joined_test.Mean_TemperatureC.isnull()])","4bd11a25":"# now we can drop duplicated columns\nfor df in (joined, joined_test):\n    for c in df.columns:\n        if c.endswith('_y'):\n            if c in df.columns: df.drop(c, inplace=True, axis=1)","759f330e":"for df in (joined,joined_test):\n    df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32)\n    df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32)\n    df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32)\n    df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)","35b68aea":"for df in (joined, joined_test):\n    df['CompetitionOpenSince'] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear,\n                                                     month=df.CompetitionOpenSinceMonth, \n                                                     day=15))\n    df['CompetitionDaysOpen'] = df.Date.subtract(df.CompetitionOpenSince).dt.days","b59b4fa9":"for df in (joined, joined_test):\n    df.loc[df.CompetitionDaysOpen<0, \"CompetitionDaysOpen\"] = 0\n    df.loc[df.CompetitionOpenSinceYear<1990, \"CompetitionDaysOpen\"] = 0","1a8567bc":"for df in (joined,joined_test):\n    df[\"CompetitionMonthsOpen\"] = df[\"CompetitionDaysOpen\"]\/\/30\n    df.loc[df.CompetitionMonthsOpen>24, \"CompetitionMonthsOpen\"] = 24\njoined.CompetitionMonthsOpen.unique()","9603ac00":"for df in (joined, joined_test):\n    df[\"Promo2Since\"] = pd.to_datetime(df.apply(\n        lambda x: Week(x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1))\n    df[\"Promo2Days\"] = df.Date.subtract(df['Promo2Since']).dt.days","d9f9a6b1":"for df in (joined,joined_test):\n    df.loc[df.Promo2Days<0, \"Promo2Days\"] = 0\n    df.loc[df.Promo2SinceYear<1990, \"Promo2Days\"] = 0\n    df[\"Promo2Weeks\"] = df[\"Promo2Days\"]\/\/7\n    df.loc[df.Promo2Weeks<0, \"Promo2Weeks\"] = 0\n    df.loc[df.Promo2Weeks>25, \"Promo2Weeks\"] = 25\n    df.Promo2Weeks.unique()","11640112":"#joined.to_pickle(PATH\/'joined')\n#joined_test.to_pickle(PATH\/'joined_test')","4f71ffc7":"def get_elapsed(fld, pre):\n    day1 = np.timedelta64(1, 'D')\n    last_date = np.datetime64()\n    last_store = 0\n    res = []\n    \n    for s,v,d in zip(df.Store.values, df[fld].values, df.Date.values):\n        if s != last_store:\n            last_date = np.datetime64()\n            last_store = s\n        if v: last_date = d\n        res.append(((d-last_date).astype('timedelta64[D]') \/ day1))\n    df[pre+fld] = res","db17872a":"columns = [\"Date\", \"Store\", \"Promo\", \"StateHoliday\", \"SchoolHoliday\"]","ff0dd6e9":"df = train[columns].append(test[columns])\ndf.head(3)","5bd1675b":"fld = 'SchoolHoliday'\ndf = df.sort_values(['Store', 'Date'])\nget_elapsed(fld, 'After')\ndf = df.sort_values(['Store', 'Date'], ascending=[True, False])\nget_elapsed(fld, 'Before')","58e5887e":"fld = 'StateHoliday'\ndf = df.sort_values(['Store', 'Date'])\nget_elapsed(fld, 'After')\ndf = df.sort_values(['Store', 'Date'], ascending=[True, False])\nget_elapsed(fld, 'Before')","d1895199":"fld = 'Promo'\ndf = df.sort_values(['Store', 'Date'])\nget_elapsed(fld, 'After')\ndf = df.sort_values(['Store', 'Date'], ascending=[True, False])\nget_elapsed(fld, 'Before')","fae5a6d9":"df = df.set_index('Date')","423b6aea":"columns = ['SchoolHoliday', 'StateHoliday', 'Promo']","4014693b":"for o in ['Before', 'After']:\n    for p in columns:\n        a = o+p\n        df[a] = df[a].fillna(0).astype(int)","40bb6be1":"bwd = df[['Store']+columns].sort_index().groupby(\"Store\").rolling(7, min_periods=1).sum()","035482e8":"fwd = df[['Store']+columns].sort_index(ascending=False\n                                      ).groupby(\"Store\").rolling(7, min_periods=1).sum()","2b91dc00":"bwd.drop('Store',1,inplace=True)\nbwd.reset_index(inplace=True)","6c0f577c":"fwd.drop('Store',1,inplace=True)\nfwd.reset_index(inplace=True)","8433be5e":"df.reset_index(inplace=True)","392331ed":"df = df.merge(bwd, 'left', ['Date', 'Store'], suffixes=['', '_bw'])\ndf = df.merge(fwd, 'left', ['Date', 'Store'], suffixes=['', '_fw'])","aa17ff15":"df.drop(columns,1,inplace=True)","90696e9d":"df.head()","e7704548":"#df.to_pickle(PATH\/'df')","c8f0835f":"df[\"Date\"] = pd.to_datetime(df.Date)","0ad14cfe":"df.columns","1f9e5460":"#joined = pd.read_pickle(PATH\/'joined')\n#joined_test = pd.read_pickle(PATH\/f'joined_test')","9500d6a9":"joined = join_df(joined, df, ['Store', 'Date'])","b3b2a458":"joined_test = join_df(joined_test, df, ['Store', 'Date'])","3eea6e8d":"joined = joined[joined.Sales!=0]","81583da4":"joined.reset_index(inplace=True)\njoined_test.reset_index(inplace=True)","48424e84":"#joined.to_pickle(path\/'train_clean')\n#joined_test.to_pickle(path\/'test_clean')","3cd7ece5":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","eb84a453":"train_df = joined\ntest_df = joined_test","b648b474":"train_df.head().T","d8bdbf99":"print(test_df.shape)\ntest_df.head()","5f67b91f":"n = len(train_df); n","dfb5a0fa":"idx = np.random.permutation(range(n))[:2000]\nidx.sort()\nsmall_train_df = train_df.iloc[idx[:1000]]\nsmall_test_df = train_df.iloc[idx[1000:]]\nsmall_cont_vars = ['CompetitionDistance','Mean_Humidity']\nsmall_cat_vars = ['Store','DayOfWeek','PromoInterval']\nsmall_train_df = small_train_df[small_cat_vars + small_cont_vars + ['Sales']]\nsmall_test_df = small_test_df[small_cat_vars + small_cont_vars + ['Sales']]","d2fb14ff":"small_train_df.head()","a3f22225":"small_test_df.head()","17f79135":"categorify = Categorify(small_cat_vars, small_cont_vars)\ncategorify(small_train_df)\ncategorify(small_test_df, test=True)","063b5107":"small_train_df.head()","4d27ef16":"small_train_df.PromoInterval.cat.categories","52b95cb7":"# we convert to categories then add 1 to -1 (NaNs) to turn it to zero because you can not look up 1 in an embedding matrix\nsmall_train_df['PromoInterval'].cat.codes[:5]","6b258801":"fill_missing = FillMissing(small_cat_vars, small_cont_vars)\nfill_missing(small_train_df)\nfill_missing(small_test_df, test=True)","970fe4b1":"# find any missing values, create a column called \"_na\" and set it to True any time it is missing\n# then replace the empty value with the median of CompetitionDistance because it needs to be a continues varaiable\nsmall_train_df[small_train_df['CompetitionDistance_na'] == True]","354a5029":"len(train_df),len(test_df)","9c9096d8":"# as seen above, create pre processers fill missing, categorify \n# and normalize (normalize: for any continous var subtract the mean and divide by std)\nprocs=[FillMissing, Categorify, Normalize]","5b153fec":"# name your category variables, keep some continues variables like \"day\" as cat because\n# as a cat var it will create an embedding matrix and the different days of the month will create different behavors\ncat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n    'SchoolHoliday_fw', 'SchoolHoliday_bw']\n\n# name your continues variables\ncont_vars = ['CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n   'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n   'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday']","019825b1":"# dependant var\ndep_var = 'Sales'\n\n# the final df to pass in will be the cat_vars, cont_vars, dep_var, and date, date will be used to create the validation set, \n#it will be the same number of records at the end of the time period as the test set from kaggle\ndf = train_df[cat_vars + cont_vars + [dep_var,'Date']].copy()","c61de284":"df.head()","1a6ee050":"test_df['Date'].min(), test_df['Date'].max()","45565fa2":"cut = train_df['Date'][(train_df['Date'] == train_df['Date'][len(test_df)])].index.max()\ncut","d02eb107":"valid_idx = range(cut)","7d5f893c":"# finally, lets look \ndf[dep_var].head()","121e7592":"# create databunch\ndata = (TabularList.from_df(df, path='.', cat_names=cat_vars, cont_names=cont_vars, procs=procs,)\n                .split_by_idx(valid_idx)\n                .label_from_df(cols=dep_var, label_cls=FloatList, log=True)\n                .add_test(TabularList.from_df(test_df, path=path, cat_names=cat_vars, cont_names=cont_vars))\n                .databunch())","6df4f16b":"max_log_y = np.log(np.max(train_df['Sales'])*1.2)\ny_range = torch.tensor([0, max_log_y], device=defaults.device)","d5f78c55":"# Learner\nlearn = tabular_learner(data, layers=[1000,500], ps=[0.001,0.01], emb_drop=0.04,\n                       y_range=y_range, metrics=exp_rmspe)","3ff90627":"learn.model","1871e242":"len(data.train_ds.cont_names)","a8c6f2da":"learn.lr_find()\nlearn.recorder.plot()","9b39f5ed":"learn.fit_one_cycle(5, 1e-3, wd=0.2)","eb3bb307":"learn.save('1')","cbef5260":"learn.recorder.plot_losses(skip_start=10000)","ad493d43":"learn.load('1');","9e5a4d8e":"learn.fit_one_cycle(5, 3e-4)","7f41045c":"test_preds=learn.get_preds(DatasetType.Test)\ntest_df[\"Sales\"]=np.exp(test_preds[0].data).numpy().T[0]\ntest_df[[\"Id\",\"Sales\"]]=test_df[[\"Id\",\"Sales\"]].astype(\"int\")\ntest_df[[\"Id\",\"Sales\"]].to_csv(\"rossmann_submission.csv\",index=False)","8e55b910":"Same process for Promo dates.","b1f8a904":"The authors also removed all instances where the store had zero sale \/ was closed. We speculate that this may have cost them a higher standing in the competition. One reason this may be the case is that a little exploratory data analysis reveals that there are often periods where stores are closed, typically for refurbishment. Before and after these periods, there are naturally spikes in sales that one might expect. By ommitting this data from their training, the authors gave up the ability to leverage information about these periods to predict this otherwise volatile behavior.","88b66848":"It is common when working with time series data to extract data that explains relationships across rows as opposed to columns, e.g.:\n* Running averages\n* Time until next event\n* Time since last event\n\nThis is often difficult to do with most table manipulation frameworks, since they are designed to work with relationships across columns. As such, we've created a class to handle this type of data.\n\nWe'll define a function `get_elapsed` for cumulative counting across a sorted dataframe. Given a particular field `fld` to monitor, this function will start tracking time since the last occurrence of that field. When the field is seen again, the counter is set to zero.\n\nUpon initialization, this will result in datetime na's until the field is encountered. This is reset every time a new store is seen. We'll see how to use this shortly.","9efa633b":"## Architecture\n\nthe most basic fully connected model","a7b5954c":"The following extracts particular date fields from a complete datetime for the purpose of constructing categoricals.\n\nYou should *always* consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can't capture any trend\/cyclical behavior as a function of time at any of these granularities. We'll add to every table with a date field.","067a10a9":"The Google trends data has a special category for the whole of the Germany - we'll pull that out so we can use it explicitly.","973200ba":"finally, name these new joined external data and original data dfs to your train and test sets","4a502365":"Next we want to drop the Store indices grouped together in the window function.\n\nOften in pandas, there is an option to do this in place. This is time and memory efficient when working with large datasets.","0400226c":"Lets look at the dep_var, sales, it is an int64. Usually fastAi is looking for a float for a regression problem\nIf it stays as int64 it will run a classification model by default\ninclude `lables_cls=FloatList` to convert the dep_var to a be read as a float\n","27dc8bd3":"Let's walk through an example.\n\nSay we're looking at School Holiday. We'll first sort by Store, then Date, and then call `add_elapsed('SchoolHoliday', 'After')`:\nThis will apply to each row with School Holiday:\n* A applied to every row of the dataframe in order of store and date\n* Will add to the dataframe the days since seeing a School Holiday\n* If we sort in the other direction, this will count the days until another holiday.","983a4e7d":"We'll replace some erroneous \/ outlying data.","97d1a981":"We'll be applying this to a subset of columns:","8181d430":"## Dropout:\nThe intermediate weight matrix `layers` will need to go from 1000 activation input to 500 activation output, \nwhich means the 500,000 elements in the weight matrix, which is a lot for a data set with only a couple 100k rows,\nso it will overfit.\nThe way to fix this is through regularization, use weight decay (which is used by default),\nand we will want more regularization through dropout `ps` and `emb_drop` will let you include dropout.\n\n**Dropout**\nSince we are using a fully connected network, we will throw away a certian percantage of activations.\nEverytime you have a mini batch going through you will at random remove some activations.\nDoing this will ensure that the model cannot find a specific activation that can memorize some part of the input (ie overfit)\nDropout is important to reduce overfitting.","3bb9d591":"# Submission","64146d35":"# Exploratory Data Analysis\nA quick analysis of the datasets","ad99d912":"# DEEP LEARNING Tabular Data Model\n\nNow we will begain our CNN","a9deeb5c":"find validation set indexes","1b724089":"`join_df` is a function for joining tables on specific fields. By default, we'll be doing a left outer join of `right` on the `left` argument using the given fields for each table.\n\nPandas does joins using the `merge` method. The `suffixes` argument describes the naming convention for duplicate fields. We've elected to leave the duplicate field names on the left untouched, and append a \"\\_y\" to those on the right.","01be9687":"We're going to set the active index to Date.","2cd0b914":"The evaluation metric is RMSE, if you first take the log of y then it becomes RMSE.\nPretty much anytime you are predicting population, or sales, you will usually want to take log as true then RMSE","d809814d":"# Data Preparation \/ Feature Engineering\n\nPre-proccessing steps to the Rossman train, test, store, and external data sets\nWe will also need to merge the external data sets into the Rossman train and test data sets.","cde755f6":"Now we can outer join all of our data into a single dataframe. Recall that in outer joins everytime a value in the joining field on the left table does not have a corresponding value on the right table, the corresponding row in the new table has Null values for all right table fields. One way to check that all records are consistent and complete is to check for Null values post-join, as we do here.\n\n*Aside*: Why not just do an inner join?\nIf you are assuming that all records are complete and match on the field you desire, an inner join will do the same thing as an outer join. However, in the event you are wrong or a mistake is made, an outer join followed by a null-check will catch it. (Comparing before\/after # of rows for inner join is equivalent, but requires keeping track of before\/after row #'s. Outer join is easier.)","38125a2e":"We add \"CompetitionMonthsOpen\" field, limiting the maximum to 2 years to limit number of unique categories.","a678648d":"Next we'll extract features \"CompetitionOpenSince\" and \"CompetitionDaysOpen\". Note the use of `apply()` in mapping a function across dataframe values.","ee3c7ac5":"Next we'll demonstrate window functions in pandas to calculate rolling quantities.\n\nHere we're sorting by date (`sort_index()`) and counting the number of events of interest (`sum()`) defined in `columns` in the following week (`rolling()`), grouped by Store (`groupby()`). We do the same in the opposite direction.","b6d6f78d":"## Model\n\ntake the max of sales and make it a little larger, \nthis well help the regression problem because the sigmoid needs to reach the max of the actual data, but it wont unless you make it a little larger\n","096740d2":"## Preparing the full data set","807d909b":"Then set null values from elapsed field calculation to 0.","1092a303":"In pandas you can add new columns to a dataframe by simply defining it. We'll do this for googletrends by extracting dates and state names from the given data and adding those columns.\n\nWe're also going to replace all instances of state name 'NI' to match the usage in the rest of the data: 'HB,NI'. This is a good opportunity to highlight pandas indexing. We can use `.loc[rows, cols]` to select a list of rows and a list of columns from the dataframe. In this case, we're selecting rows w\/ statename 'NI' by using a boolean list `googletrend.State=='NI'` and selecting \"State\".","a3d6a013":"## Durations","6d564601":"## Experimenting with a sample of the data","ede81ebd":"Next we'll fill in missing values to avoid complications with `NA`'s. `NA` (not available) is how Pandas indicates missing values; many models have problems when missing values are present, so it's always important to think about how to deal with them. In these cases, we are picking an arbitrary *signal value* that doesn't otherwise appear in the data.","f70cf154":"We can conver state holiday columns to booleans, this will make them move convenient for modeling.","0b89f44d":"Now we'll merge these values onto the df.","fe923c44":"# Rossman Sales Fastai Tabular Deep Learning Model with External Data Sets\n\nThis notebook focuses on extensive feature engineering of external datasets and a relativly simple deep learning model.\n\n* Feature Engineering from external datasets for googletrends, weather, and store state names.\n* Model Architecture: Basic fully connected network\n    * include dropout","cf6dfc63":"Join weather\/state names. This will allow us to merge other datasets.","ceda62fe":"It's usually a good idea to back up large tables of extracted \/ wrangled features before you join them onto another one, that way you can go back to it easily if you need to make changes to it."}}