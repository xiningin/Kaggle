{"cell_type":{"f2aedbb1":"code","a850e447":"code","45a58975":"code","279de8a5":"code","6df6bef2":"code","e098cecb":"code","2322f59b":"code","e51fa93a":"code","04117d0d":"code","785f4617":"code","9678c1fe":"code","02cb78c1":"code","1f7de4a7":"code","46a91410":"code","e3795019":"code","d03291c1":"code","ad6230d8":"code","808ea603":"code","cd941cf1":"code","04ec7131":"markdown","fabdbf50":"markdown","ac858198":"markdown","3950673d":"markdown","4dddd2ca":"markdown","7b38262d":"markdown","14475f9a":"markdown","6849a63d":"markdown","e159a407":"markdown","315121b2":"markdown","2211d160":"markdown","6cfb5506":"markdown","f91721b5":"markdown","d36d477c":"markdown","7f6da209":"markdown","45163efd":"markdown","125f5baa":"markdown","fc4b8ea0":"markdown","69b7887f":"markdown"},"source":{"f2aedbb1":"!pip install --upgrade scikit-learn","a850e447":"import pandas as pd\nimport numpy as np\nimport gc\n\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")","45a58975":"train[\"soiltype_label\"] = 0\ntest[\"soiltype_label\"] = 0\n\ntrain[\"soiltype_label\"] = train[\"soiltype_label\"].astype(np.int64)\ntest[\"soiltype_label\"] = test[\"soiltype_label\"].astype(np.int64)\n\nsoil_columns = [x for x in train.columns if x.startswith(\"Soil_Type\")]","279de8a5":"def make_40_bit_int_from_soiltype(row):\n    value = 0\n    for column in soil_columns:\n        value |= row[column]\n        value = value << 1\n    return value","6df6bef2":"train[\"soiltype_label\"] = train.apply(make_40_bit_int_from_soiltype, axis=1)\nprint(\": Number of unique labels: {:,d}\".format(train[\"soiltype_label\"].nunique()))","e098cecb":"# Drop Cover_Type 5, since we only have one example of it\ntrain = train[(train[\"Cover_Type\"] != 5)]\n\n# Retain all of our features\nfeatures = [\n    'Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n    'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n    'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n    'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', \n    'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n    'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', \n    'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n    'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', \n    'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n    'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']","2322f59b":"%%time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom lightgbm import LGBMClassifier\nfrom lightgbm import early_stopping\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\ntarget = train[\"Cover_Type\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\ntrain_preds = np.zeros(len(train.index), )\ntrain_probas = np.zeros(len(train.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[features], target)):\n    x_train = train[features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"softmax\",\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        callbacks=[early_stopping(50, verbose=False)],\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_preds[test_index] = train_oof_preds\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n    print(\"-- Accuracy: {}\".format(accuracy_score(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- Accuracy: {}\".format(accuracy_score(target, train_preds)))\n\ntrain[\"unmodified_preds\"] = train_preds\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"Cover_Type\"], train[\"unmodified_preds\"])\ncover_labels = [1, 2, 3, 4, 6, 7]\nfig, ax = plt.subplots(figsize=(15, 15))\nax = sns.heatmap(confusion, annot=True, fmt=\",d\", xticklabels=cover_labels, yticklabels=cover_labels)\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (Unmodified Dataset)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(confusion)\n_ = gc.collect()","e51fa93a":"from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\ntrain[\"soiltype_label_encoded\"] = encoder.fit_transform(train[\"soiltype_label\"])\ntrain[\"soiltype_label_encoded\"] = train[\"soiltype_label_encoded\"].astype(np.int16)","04117d0d":"%%time\ntarget = train[\"Cover_Type\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\nfeatures = [x for x in features if not x.startswith(\"Soil_Type\")]\nfeatures.insert(0, \"soiltype_label_encoded\")\n\ntrain_preds = np.zeros(len(train.index), )\ntrain_probas = np.zeros(len(train.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[features], target)):\n    x_train = train[features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"softmax\",\n        cat_feature=[0],\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        callbacks=[early_stopping(50, verbose=False)],\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_preds[test_index] = train_oof_preds\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n    print(\"-- Accuracy: {}\".format(accuracy_score(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- Accuracy: {}\".format(accuracy_score(target, train_preds)))\n\ntrain[\"collapsed_preds\"] = train_preds\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"Cover_Type\"], train[\"collapsed_preds\"])\ncover_labels = [1, 2, 3, 4, 6, 7]\nfig, ax = plt.subplots(figsize=(15, 15))\nax = sns.heatmap(confusion, annot=True, fmt=\",d\", xticklabels=cover_labels, yticklabels=cover_labels)\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (Collapsed Dataset)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(confusion)\n_ = gc.collect()","785f4617":"bar, ax = plt.subplots(figsize=(20, 10))\nax = sns.barplot(\n    x=[\"Unmodified\", \"40-bit Integer\"],\n    y=[\n        float(accuracy_score(target, train[\"unmodified_preds\"])),\n        accuracy_score(target, train[\"collapsed_preds\"]),\n    ],\n)\n_ = ax.set_title(\"Accuracy Score Based on Approach\", fontsize=15)\n_ = ax.set_xlabel(\"Approach\")\n_ = ax.set_ylabel(\"Accuracy Score\")\n_ = ax.set(ylim=(0.90, 1.0))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(\n        x=p.get_x()+(p.get_width()\/2),\n        y=height,\n        s=\"{:.4f}\".format(height),\n        ha=\"center\"\n    )","9678c1fe":"test[\"soiltype_label\"] = test.apply(make_40_bit_int_from_soiltype, axis=1)\nprint(\": Number of unique labels: {:,d}\".format(test[\"soiltype_label\"].nunique()))","02cb78c1":"from sklearn.preprocessing import OrdinalEncoder\n\nencoder = OrdinalEncoder(\n    dtype=np.int16,\n    handle_unknown=\"use_encoded_value\",\n    unknown_value=32766,\n)\ntrain[\"soiltype_label_ordinal\"] = encoder.fit_transform(train[[\"soiltype_label\"]])\ntest[\"soiltype_label_ordinal\"] = encoder.transform(test[[\"soiltype_label\"]])","1f7de4a7":"print(\": Number of mismatched rows: {:,d}\".format(\n    test[(test[\"soiltype_label_ordinal\"] == 32766)].shape[0]\n))","46a91410":"def make_5_8_bit_ints_from_soiltype(row):\n    integer1 = (np.int64(row[\"soiltype_label\"]) & 0xFF00000000) >> 30\n    integer2 = (np.int64(row[\"soiltype_label\"]) & 0x00FF000000) >> 24\n    integer3 = (np.int64(row[\"soiltype_label\"]) & 0x0000FF0000) >> 16\n    integer4 = (np.int64(row[\"soiltype_label\"]) & 0x000000FF00) >> 8\n    integer5 = (np.int64(row[\"soiltype_label\"]) & 0x00000000FF)\n    return integer1, integer2, integer3, integer4, integer5","e3795019":"train[[\"soiltype_int1\", \"soiltype_int2\", \"soiltype_int3\", \"soiltype_int4\", \"soiltype_int5\"]] = train.apply(make_5_8_bit_ints_from_soiltype, axis=1, result_type=\"expand\")\ntest[[\"soiltype_int1\", \"soiltype_int2\", \"soiltype_int3\", \"soiltype_int4\", \"soiltype_int5\"]] = test.apply(make_5_8_bit_ints_from_soiltype, axis=1, result_type=\"expand\")","d03291c1":"int_encoder = OrdinalEncoder(\n    dtype=np.int16,\n    handle_unknown=\"use_encoded_value\",\n    unknown_value=32766,\n)\ntrain[[\"soiltype_label_int1\", \"soiltype_label_int2\", \"soiltype_label_int3\", \"soiltype_label_int4\", \"soiltype_label_int5\"]] = int_encoder.fit_transform(train[[\"soiltype_int1\", \"soiltype_int2\", \"soiltype_int3\", \"soiltype_int4\", \"soiltype_int5\"]])\ntest[[\"soiltype_label_int1\", \"soiltype_label_int2\", \"soiltype_label_int3\", \"soiltype_label_int4\", \"soiltype_label_int5\"]] = int_encoder.transform(test[[\"soiltype_int1\", \"soiltype_int2\", \"soiltype_int3\", \"soiltype_int4\", \"soiltype_int5\"]])","ad6230d8":"def check_for_missing_values(row):\n    return 1 if row[\"soiltype_label_int1\"] == 32766 or row[\"soiltype_label_int2\"] == 32766 or row[\"soiltype_label_int3\"] == 32766 or row[\"soiltype_label_int4\"] == 32766 or row[\"soiltype_label_int5\"] == 32766 else 0\n\ntest[\"missing_8_bit_value\"] = test.apply(check_for_missing_values, axis=1)\nprint(\": Number of mismatched rows: {:,d}\".format(\n    test[\"missing_8_bit_value\"].sum()\n))","808ea603":"%%time\ntarget = train[\"Cover_Type\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\nfeatures.remove(\"soiltype_label_encoded\")\nfeatures.insert(0, \"soiltype_label_int1\")\nfeatures.insert(1, \"soiltype_label_int2\")\nfeatures.insert(2, \"soiltype_label_int3\")\nfeatures.insert(3, \"soiltype_label_int4\")\nfeatures.insert(4, \"soiltype_label_int5\")\n\ntrain_preds = np.zeros(len(train.index), )\ntrain_probas = np.zeros(len(train.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[features], target)):\n    x_train = train[features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"softmax\",\n        cat_feature=[0, 1, 2, 3, 4],\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        callbacks=[early_stopping(50, verbose=False)],\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_preds[test_index] = train_oof_preds\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n    print(\"-- Accuracy: {}\".format(accuracy_score(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- Accuracy: {}\".format(accuracy_score(target, train_preds)))\n\ntrain[\"integer_preds\"] = train_preds\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"Cover_Type\"], train[\"integer_preds\"])\ncover_labels = [1, 2, 3, 4, 6, 7]\nfig, ax = plt.subplots(figsize=(15, 15))\nax = sns.heatmap(confusion, annot=True, fmt=\",d\", xticklabels=cover_labels, yticklabels=cover_labels)\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (8-bit Integers)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(confusion)\n_ = gc.collect()","cd941cf1":"bar, ax = plt.subplots(figsize=(20, 10))\nax = sns.barplot(\n    x=[\"Unmodified\", \"40-bit Integer\", \"5 x 8-bit Integers\"],\n    y=[\n        float(accuracy_score(target, train[\"unmodified_preds\"])),\n        accuracy_score(target, train[\"collapsed_preds\"]),\n        accuracy_score(target, train[\"integer_preds\"]),\n    ],\n)\n_ = ax.set_title(\"Accuracy Score Based on Approach\", fontsize=15)\n_ = ax.set_xlabel(\"Approach\")\n_ = ax.set_ylabel(\"Accuracy Score\")\n_ = ax.set(ylim=(0.90, 1.0))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(\n        x=p.get_x()+(p.get_width()\/2),\n        y=height,\n        s=\"{:.4f}\".format(height),\n        ha=\"center\"\n    )","04ec7131":"# Encoding Using 5 x 8-bit Integers\n\nInstead of using a single 40-bit integer, we could instead encode the soil types using a series of five 8-bit integers instead. By breaking up the encoded values, we may lose less information about soil types, since we spread out single-bit differences across 5 numerics instead of 1.","fabdbf50":"A general observation can be made here:\n\n1. The performance of the 40-bit integer representation is better, again probably due to dimensionality reduction. We see accuracy improvements in `Cover_Type` classes of `4`, `6`, and `7`.\n\nThe question is however, is can we apply the same integer encoding to the test set and expect to see the same performance?","ac858198":"### Transform the rows","3950673d":"Given that there are 1,000,000 rows in the testing set, having 6,843 that are mismatched is not bad (~0.7% of the testing data are impacted by this). However, this competition is all about rare cases, so we should examine if we can improve upon this.","4dddd2ca":"### Check for Mismatches\n\nLet's use the `OrdinalEncoder` and check to see how many rows in the testing set have values we haven't seen before.","7b38262d":"### Transform Test Set\n\nFirst thing we need to do, is apply the same 40-bit integer transformer to the testing set.","14475f9a":"# Conclusions\n\nIt appears that we can reduce the overall dimensionality of the dataset significantly by encoding the `Soil_Type` variables as a series of five 8-bit integers. The encoding compresses the data while at the same time providing a boost to a vanilla LightGBM model's accuracy by nearly 0.5%. Presumably, the reduced dimensionality may also provide some benefit to other approaches, as we collapsed a 40-dimensional vector down to a 5-dimensional vector.\n\nIf you find this kernel useful, please consider upvoting it!","6849a63d":"# Convert To 40-Bit Integer\n\n### Setup Dataframes","e159a407":"We can compare the two runs to see what happened:","315121b2":"### New Column Definition\n\nHere we'll generate a new field for the `soiltype_label`, and store it in a new column.","2211d160":"The overall accuracy for the 5 x 8-bit integer approach is very slightly less than that of the 40-bit integer approach. However, overall, we only have a mismatch in data of 33 rows. It is probably very worthwhile to go with the 5 x 8-bit integer approach, as we have an impact on only a very small number of testing rows.","6cfb5506":"### Row Transformer\n\nThe function below will take a single row of data, and transform the `Soil_Type` fields into a 40-bit integer.","f91721b5":"Now let's see what happens if we drop all of those `Soil_Type` columns and focus on using our `soiltype_label` instead. We'll label encode it first however, since huge values of 64-bit integers are going to give LightGBM a problem.","d36d477c":"### Apply Transformation\n\nWith the function above, all we have to do is `apply` it to our dataframe.","7f6da209":"Let's check to see how many rows in our test set are now mismatched from the training set.","45163efd":"We've now reduced the number of mismatches to 33. Let's re-run the LightGBM model and see if there is any accuracy increase with the new encoding, or if the additional dimensions reduces accuracy.","125f5baa":"# Introduction\n\nThis kernel is a bit of an exploration to see if we can reduce our dimensionality in a way that tree-based ML algorithms can make use of. In particular, this was inspired by the kernel [TPS122021 Exploiting Sparsity for XGBoost](https:\/\/www.kaggle.com\/siukeitin\/tps122021-exploiting-sparsity-for-xgboost) that was discussed at [https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/294808](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/294808). \n\nIn this particular kernel, we'll make use of the fact that the `Soil_Type` fields are binary valued. Because of this fact, we can string all of them together and construct a 40-bit integer representation of the `Soil_Type` field. To explain with a simple example, assume for a moment that we have only 8 `Soil_Type` fields:\n\n```\nSoil_Type1, Soil_Type2, Soil_Type3, Soil_Type4, Soil_Type5, Soil_Type6, Soil_Type7, Soil_Type8\n```\n\nAll of the values are either `0` or `1` like so:\n\n```\n0, 0, 0, 0, 0, 0, 1, 1\n```\n\nIf we concatenate them together, and stuff them into an integer, we would have a value that ranges from 0 to 255. With the example above we would have:\n\n```\nbinary     decimal\n00000011 = 3\n```\n\nWe can do the same thing with all of the `Soil_Type` features, and generate 40-bit integer values. The benefit here is that we would retain all of the original information within the `Soil_Type` bits, but collapse a 40-dimensional space down to 1. Let's see how this looks.","fc4b8ea0":"# Train \/ Test Differences\n\nThe first thing to do is check to see if the set of integer representations between the two sets overlap one another. The motivation behind this check is that certain combinations of binary features may not occur in the training set, but occur in the testing set. This is problematic, since the integer encoding will result in two different numeric values. For example, assume again that we have only 8 `Soil_Type` features. In the training set and testing set, let's assume that we have only the following two rows of observations:\n\n| Data Set | Type 1 | Type 2 | Type 3 | Type 4 | Type 5 | Type 6 | Type 7 | Type 8 | Encoded |\n| :------- | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :-----: |\n| Training | 0      | 0      | 0      | 0      | 0      | 0      | 1      | 1      | 3       |\n| Testing  | 0      | 0      | 0      | 0      | 0      | 0      | 0      | 1      | 1       |\n\nThe problem is that while both the training set and testing set have `Soil_Type8` of `1`, the training set has `Soil_Type7` as `1`. As we can see, the overall encoded values differ because of that. This results in a loss of information - specifically, the loss of `Soil_Type8` being the same between the datasets. Even worse, if we attempt to label encode the two sets, the default SciKit Learn `LabelEncoder` will throw errors that the testing set contains values that were unseen in the original `fit` from the training set (and rightly so, since `LabelEncoder` is only meant to be used to transform `Y` variables, not features). \n\nWe can however, use `OrdinalEncoder` and scope out the size of the differences that occurs between the two datasets.","69b7887f":"# Comparing Information Representation Methods\n\nNow that we have a single label representing all of the `Soil_Type` features, we should check to make sure we aren't losing any performance from the new features. First, we'll generate a LightGBM model with all of the `Soil_Type` features as they originally appeared."}}