{"cell_type":{"38e24b00":"code","65831677":"code","ca02cd9b":"code","5b950a35":"code","6e5bf7aa":"code","68b7a261":"code","8aafd063":"code","05114aaf":"code","07e92f1c":"code","18686047":"code","a5f4366d":"code","b1759b45":"code","b425fa7b":"code","dca8da5a":"code","18bc8f2c":"code","65210c40":"code","53ecfcd4":"code","0e53a991":"code","b9774590":"code","74f4b865":"code","b16e85a8":"code","12976bd9":"code","be0e66e6":"code","df2f8b6c":"code","94d2fddb":"code","d63bee22":"markdown","a3d7a09c":"markdown","a1ba58c6":"markdown","ec4fe77e":"markdown","4ae79a66":"markdown","e0d1c46d":"markdown","d6061cbc":"markdown","e910b4f6":"markdown","01bd401a":"markdown"},"source":{"38e24b00":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport seaborn as sns\nimport transformers\n\nimport nltk\nimport re\n\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\nplt.style.use('seaborn')","65831677":"print(tf.__version__)\nprint(tf.config.list_physical_devices('GPU'))","ca02cd9b":"#nltk.download('stopwords')","5b950a35":"PATH_CSV_TRAIN = '..\/input\/nlp-getting-started\/train.csv'\nPATH_CSV_TEST = '..\/input\/nlp-getting-started\/test.csv'\nPATH_CSV_SUBMISSION = '..\/input\/nlp-getting-started\/submission.csv'\n\ndataf = pd.read_csv(PATH_CSV_TRAIN)\ndataf_test = pd.read_csv(PATH_CSV_TEST)","6e5bf7aa":"def clean_text(text):\n    #Remove emojis and special chars\n    clean=text\n    #reg = re.compile('\\\\.+?(?=\\B|$)')\n    #clean = text.apply(lambda r: re.sub(reg, string=r, repl=''))\n    #reg = re.compile('\\x89\u00db_')\n    #clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n    reg = re.compile('\\&amp')\n    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='&'))\n    reg = re.compile('\\\\n')\n    clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n\n    #Remove hashtag symbol (#)\n    #clean = clean.apply(lambda r: r.replace('#', ''))\n\n    #Remove user names\n    reg = re.compile('@[a-zA-Z0-9\\_]+')\n    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='@'))\n\n    #Remove URLs\n    reg = re.compile('https?\\S+(?=\\s|$)')\n    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='www'))\n\n    #Lowercase\n    #clean = clean.apply(lambda r: r.lower())\n    return clean","68b7a261":"dataf['clean'] = clean_text(dataf['text'])\ndataf_test['clean'] = clean_text(dataf_test['text'])","8aafd063":"dataf.head(3)","05114aaf":"from transformers import TFXLNetModel, XLNetTokenizer","07e92f1c":"# This is the identifier of the model. The library need this ID to download the weights and initialize the architecture\n# here is all the supported ones:\n# https:\/\/huggingface.co\/transformers\/pretrained_models.html\nxlnet_model = 'xlnet-large-cased'\nxlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_model)","18686047":"def create_xlnet(mname):\n    \"\"\" Creates the model. It is composed of the XLNet main block and then\n    a classification head its added\n    \"\"\"\n    # Define token ids as inputs\n    word_inputs = tf.keras.Input(shape=(120,), name='word_inputs', dtype='int32')\n\n    # Call XLNet model\n    xlnet = TFXLNetModel.from_pretrained(mname)\n    xlnet_encodings = xlnet(word_inputs)[0]\n\n    # CLASSIFICATION HEAD \n    # Collect last step from last hidden state (CLS)\n    doc_encoding = tf.squeeze(xlnet_encodings[:, -1:, :], axis=1)\n    # Apply dropout for regularization\n    doc_encoding = tf.keras.layers.Dropout(.1)(doc_encoding)\n    # Final output \n    outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(doc_encoding)\n\n    # Compile model\n    model = tf.keras.Model(inputs=[word_inputs], outputs=[outputs])\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n\n    return model","a5f4366d":"xlnet = create_xlnet(xlnet_model)","b1759b45":"xlnet.summary()","b425fa7b":"tweets = dataf['clean']\nlabels = dataf['target']\n\nX_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.15, random_state=196)","dca8da5a":"def get_inputs(tweets, tokenizer, max_len=120):\n    \"\"\" Gets tensors from text using the tokenizer provided\"\"\"\n    inps = [tokenizer.encode_plus(t, max_length=max_len, pad_to_max_length=True, add_special_tokens=True) for t in tweets]\n    inp_tok = np.array([a['input_ids'] for a in inps])\n    ids = np.array([a['attention_mask'] for a in inps])\n    segments = np.array([a['token_type_ids'] for a in inps])\n    return inp_tok, ids, segments\n\ndef warmup(epoch, lr):\n    \"\"\"Used for increasing the learning rate slowly, this tends to achieve better convergence.\n    However, as we are finetuning for few epoch it's not crucial.\n    \"\"\"\n    return max(lr +1e-6, 2e-5)\n\ndef plot_metrics(pred, true_labels):\n    \"\"\"Plots a ROC curve with the accuracy and the AUC\"\"\"\n    acc = accuracy_score(true_labels, np.array(pred.flatten() >= .5, dtype='int'))\n    fpr, tpr, thresholds = roc_curve(true_labels, pred)\n    auc = roc_auc_score(true_labels, pred)\n\n    fig, ax = plt.subplots(1, figsize=(8,8))\n    ax.plot(fpr, tpr, color='red')\n    ax.plot([0,1], [0,1], color='black', linestyle='--')\n    ax.set_title(f\"AUC: {auc}\\nACC: {acc}\");\n    return fig","18bc8f2c":"inp_tok, ids, segments = get_inputs(X_train, xlnet_tokenizer)","65210c40":"callbacks = [\n    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, min_delta=0.02, restore_best_weights=True),\n    tf.keras.callbacks.LearningRateScheduler(warmup, verbose=0),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=1e-6, patience=2, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=1e-6)\n]","53ecfcd4":"hist = xlnet.fit(x=inp_tok, y=y_train, epochs=15, batch_size=16, validation_split=.15, callbacks=callbacks)","0e53a991":"inp_tok, ids, segments = get_inputs(X_test, xlnet_tokenizer)","b9774590":"preds = xlnet.predict(inp_tok, verbose=True)","74f4b865":"plot_metrics(preds, y_test);","b16e85a8":"pred_analysis_df = pd.DataFrame({'tweet':X_test.values, 'pred':preds.flatten(), 'real':y_test})\npred_analysis_df['rounded'] = np.array(pred_analysis_df['pred'] > 0.5, dtype='int')\ndiff = pred_analysis_df[pred_analysis_df['real'] != pred_analysis_df['rounded']]","12976bd9":"#change to see other examples\nidx = 44\n\ntweet, real, pred = diff.iloc[idx, [0,2,3]]\nprint(tweet)\nprint(\"PRED: \" + str(pred))\nprint(\"REAL: \" + str(real))","be0e66e6":"tweets = dataf_test['clean']\n\ninp_tok, ids, segments = get_inputs(tweets, xlnet_tokenizer)","df2f8b6c":"preds = xlnet.predict(inp_tok, verbose=True)","94d2fddb":"dataf_test['target'] = preds\ndataf_test['target'] = np.array(dataf_test['target'] >= 0.5, dtype='int')\ndataf_test[['id', 'target']].to_csv('submission.csv', index=False)","d63bee22":"### Training","a3d7a09c":"### Testing","a1ba58c6":"**Generate submission file**","ec4fe77e":"Create the input data (tensors)","4ae79a66":"### Why the fails?\n\nStudying the examples our NN failed to classify could give us some hints to improve","e0d1c46d":"# Using XLNet to classify tweets\n\n## About XLNET\nXLNet was a step-forward from BERT last year. I thought I could give you an example on how to use HuggingFace's library to quickly build a NN based on this model, as I haven't seen many XLNets here :P.\n\nXLNet was already overpassed by other architectures on the GLUE benchmark [2]. I recommed you to use Roberta (by Facebook), which still is in the near SOTA.\n\nThis architecture is fully explained in [1], but briefly, it is BERT but better pre-trained. The original creators realized that the Masking approximation of BERT was not optimal because on final tasks, BERT is not going to see the ``[MASK]`` token. Instead of corrupting the input, the researchers simply permutated the target outputs, which is more natural with respect to final tasks. This led them to achieve SOTA and outperform BERT. (They used a greater amount of data, BTW).\n\n## Huggingface??\nHuggingface it's a company that, IMO, has brought a great value to the NLP community. They basically adapt and pretrain the current SOTA models and put them inside a Python package which is very user friendly and easy to use. In few lines (as you'll see) you can have a state-of-the-art model working!. \n\nThe package it's called ``transformers`` and it's already preinstalled on Kaggle.  It contains implementations for PyTorch and Tensorflow 2.0, and the tokenizers which are needed for converting the outputs, so you can use it even if you are using a third party implementation of a model.\n\nInteresting Sources:\n\n[1] XLNet: Generalized Autoregressive Pretraining for Language Understanding. https:\/\/arxiv.org\/abs\/1906.08237\n\n[2] GLUE LB. https:\/\/gluebenchmark.com\/leaderboard\n\n[3] Huggingface's Github https:\/\/github.com\/huggingface\/transformers","d6061cbc":"** These are the classes we're going to use **","e910b4f6":"** Text cleanning: **\n- remove strange characters\n- remove URLs (they doesn't tell us pretty much)\n- replace usernames for \"@\" character\n- remove line breaks","01bd401a":"Clean and split the data"}}