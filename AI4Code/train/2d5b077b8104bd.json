{"cell_type":{"cf551739":"code","befe45cb":"code","6bfdde52":"code","b9dc889e":"code","cf902817":"code","d8a34f42":"code","6928bdff":"code","54e92c14":"code","a0a83f54":"code","d7a5658c":"code","eeb71114":"code","70a69c81":"code","abc41e83":"code","92cd4d30":"code","402531d3":"code","d121dde5":"code","db397430":"code","477e43f7":"code","dbb271df":"code","e20cf8b5":"code","6fcc3f52":"code","1bb6e91b":"code","f5dd8ed2":"code","7aaa1cea":"code","0a8ff0fe":"code","65de21ee":"code","789528b2":"code","9d09ba7a":"code","1a573777":"code","f4fb23c2":"markdown","29b6d85a":"markdown","f5f5020d":"markdown","86b26ddd":"markdown","890acd24":"markdown","8fb2f091":"markdown","068a62e9":"markdown","cc60dd08":"markdown","1f871fa8":"markdown","b93d115f":"markdown","ef411071":"markdown","da0d4c66":"markdown","225bbb11":"markdown","3b6808dd":"markdown","05c1895b":"markdown","3a2ff408":"markdown","fb5bc21a":"markdown","ef1f6304":"markdown","ecd32640":"markdown","e1e5f582":"markdown","814b2a00":"markdown","2df2cd9d":"markdown","d4fb72c8":"markdown","7a8ef69d":"markdown","44a5dc65":"markdown","a02fc9e6":"markdown","871df31a":"markdown","7cdc57c1":"markdown","0f9ad2a5":"markdown","991d15cc":"markdown","03793734":"markdown","911a58bb":"markdown","741db9e0":"markdown","743af9b4":"markdown","ad68fc9c":"markdown","df8af018":"markdown","e2d6a0ba":"markdown","d4ff5a48":"markdown","aee1134e":"markdown"},"source":{"cf551739":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","befe45cb":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\nprint(\"Shape of train dataset is\", train.shape)\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nprint(\"Shape of test dataset is\", test.shape)","6bfdde52":"# with below method we can display maximum number of rows and columns we want to display.\n\npd.set_option('display.max_rows', 10)\npd.set_option('display.max_columns', 10)\ntrain.head(10)","b9dc889e":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef bar_plot(feature):\n    sns.set(style=\"darkgrid\")\n    ax = sns.countplot(x=feature , data=train)\n    \nprint(\"Total number of different target categories is\", train.target.value_counts().count())\ncount_0 = train.target.value_counts()[0]\ncount_1 = train.target.value_counts()[1]\nprint(\"target with count 1 is {}\".format(count_1))\nprint(\"target with count 0 is {}\".format(count_0))\nbar_plot(\"target\")","cf902817":"print(\"Total different categories in keyword is :\", train.keyword.value_counts().count())\nprint(\"Total different categories in location is :\", train.location.value_counts().count())","d8a34f42":"train.isna().sum()","6928bdff":"train[~train[\"location\"].isna()][\"location\"].tolist()[0:40]","54e92c14":"import geopy\nimport numpy as np\nimport pycountry\n\nfrom geopy.geocoders import Nominatim\ngeolocator = Nominatim(\"navneet\")\ndef get_location(region=None):\n \n    if region:\n        try:    \n            return geolocator.geocode(region)[0].split(\",\")[-1] \n        except:\n            return region\n    return None\n\ntrain[\"country\"] = train[\"location\"].apply(get_location)","a0a83f54":"train[~train[\"country\"].isna()][\"country\"].tolist()[30:50]","d7a5658c":"train[~train[\"country\"].isna()][\"country\"].nunique()","eeb71114":"train[~train[\"country\"].isna()][\"country\"].head()","70a69c81":"set(train[~train[\"keyword\"].isna()][\"keyword\"].tolist())","abc41e83":"def split_keywords(keyword):\n    try:\n        return keyword.split(\"%20\")\n    except:\n        return [keyword]\n    \n\ntrain[\"keyword\"] = train[\"keyword\"].apply(split_keywords)\n","92cd4d30":"train[~train[\"keyword\"].isna()][\"keyword\"].tolist()[100:110]","402531d3":"def count_keywords_in_text(keywords, text):\n    if not keywords[0]:\n        return 0\n    count = 0\n    for keyword in keywords:\n        each_keyword_count = text.count(str(keyword))\n        count = count + each_keyword_count\n    return count\n\ntrain[\"keyword_count_in_text\"] = train.apply(lambda row: count_keywords_in_text(row[\"keyword\"] , row['text']), axis=1)","d121dde5":"train.tail()","db397430":"train[\"text\"].tolist()[0:100]","477e43f7":"def get_count_of_hash(text):\n    if not text:\n        return -1\n    return text.count(\"#\")\n\ntrain[\"count_#\"] = train[\"text\"].apply(get_count_of_hash)","dbb271df":"def get_count_of_at_rate(text):\n    if not text:\n        return -1\n    return text.count(\"@\")\n\ntrain[\"count_@\"] = train[\"text\"].apply(get_count_of_at_rate)","e20cf8b5":"train[\"count_@\"].to_list()[100:110]","6fcc3f52":"train.head()","1bb6e91b":"import re\n\nprint(\"Before---------\")\nprint(train[\"text\"].tolist()[31])\n\ntrain['text'] = train['text'].str.replace('http:\\S+', '', case=False)\ntrain['text'] = train['text'].str.replace('https:\\S+', '', case=False)\nprint(\"After----------\")\nprint(train[\"text\"].tolist()[31])\n","f5dd8ed2":"import string\nexclude = set(string.punctuation)\nexclude_hash = {\"#\"}\nexclude = exclude - exclude_hash\nprint(\"Length of punctuations to be excluded :\",len(exclude))\n\nprint(\"Before---------\")\nprint(train[\"text\"].tolist()[0])\n\nfor punctuation in exclude:\n  train['text'] = train['text'].str.replace(punctuation, '', regex=True)\n\nprint(\"After----------\")\nprint(train[\"text\"].tolist()[0])","7aaa1cea":"import nltk\nnltk.download('stopwords')\nfrom stop_words import get_stop_words\nfrom nltk.corpus import stopwords\n\nstop_words = list(get_stop_words('en'))         #About 900 stopwords\nnltk_words = list(stopwords.words('english')) #About 179 stopwords\nstop_words = sorted(set(stop_words).union(set(nltk_words)) - exclude_hash)  # removing hash from stop words\n\nprint(\"total stop words to be removed :\", len(stop_words))","0a8ff0fe":"\nprint(\"Before--------\")\nprint(train[\"text\"].tolist()[0])\npreprocessed_text = []\n# tqdm is for printing the status bar\nfor sentance in train['text'].values:\n    sent = ' '.join(e for e in sentance.split() if e not in stop_words)\n    preprocessed_text.append(sent.lower().strip())\n\ntrain[\"text\"] = preprocessed_text\nprint(\"After----------\")\nprint(train[\"text\"].tolist()[0])","65de21ee":"import spacy\n\n# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\nprint(\"Before--------\")\nprint(train[\"text\"].tolist()[2])\n\nlemet_text = []\n# tqdm is for printing the status bar\nfor sentance in train['text'].values:\n    sent = \" \".join([token.lemma_ for token in nlp(sentance)])\n    lemet_text.append(sent.lower().strip())\n\ntrain[\"text\"] = lemet_text\n\ntrain[\"text\"] = lemet_text\nprint(\"After----------\")\nprint(train[\"text\"].tolist()[2])","789528b2":"nltk.download('punkt')\n\ntrain['text'] = train.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\ntrain[\"text\"].tolist()[2]","9d09ba7a":"from gensim.models import Word2Vec\n# train model\nmodel = Word2Vec(train.text.values, min_count=1, size = 300)\n\n# summarize vocabulary\nwords = list(model.wv.vocab)\n#print(words)\n\n# save model\nmodel.save('model.bin')\n# load model\nnew_model = Word2Vec.load('model.bin')\nprint(new_model)","1a573777":"print(model.most_similar('disaster', topn = 20))","f4fb23c2":"Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster\n\n> In this competition, you\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t\n\nRead more about this here --> https:\/\/www.kaggle.com\/c\/nlp-getting-started","29b6d85a":"> From the above we can see that  location contains 2533 null values followed by keywords with 61 null values","f5f5020d":"# Removing all the stop words","86b26ddd":"> Since this is twitter text, so counting number of hashes becomes more important","890acd24":"# Since dataset is very less so lets create out own w2v embedding","8fb2f091":"> In the keywords we can see that few of the words are concatenated with \"%20\". Let's seperate these words","068a62e9":"**Remove website links**","cc60dd08":"**Future work**\n\n> different types of text embedding like countvectorizer, tfidf etc.\n\n> some more feature engineering and cleaning\n\n> differnt types of models like naive bayes, logistic, lightgbm","1f871fa8":"# Extract country from location","b93d115f":"> Checking not null locations, below I have limited it to 40, you can try will all","ef411071":"**Why lemmatisation ?**\n\n> lemmatisation is done on text data to get the lemma of that word.. Ex : stops -- > stop","da0d4c66":"# Function to check if keywords exist in text or not","225bbb11":"> form above we can see that we have #, ==>, ... and a lot of unnecessary words like to, is, are [stopwords], links that needs to be removed","3b6808dd":"> from above we can see that all the stop words like [are, of, this] has been removed.","05c1895b":" # Count number of @(at the rate) in a text","3a2ff408":"> this way we can remove all website links","fb5bc21a":" # Count number of #(hash) in a text","ef1f6304":"> In the below codes we are removing all the website links starting with http: or https:","ecd32640":"> tokenization is needed for making w2v models.\n\n> \"my name is navneet\" --> after tokenization --> [\"my\", \"name\", \"is\", \"navneet\"]","e1e5f582":"# Before diving deep into text data lets explore categorical data","814b2a00":"> punctuations should be removed because it doesnot add much value ","2df2cd9d":"# Checking for null values ","d4fb72c8":"**Lemmatise the words with spacy**","7a8ef69d":"# Check for target distribution","44a5dc65":"# Problem Statement","a02fc9e6":"# Let's play with keyword","871df31a":"> there are 86 unique country in dataframe.","7cdc57c1":"**Hello Friends,\n In this kernel my main aim is to make you guys familar with basic nlp techniques and feature engineering with codes and theory.**","0f9ad2a5":"> here i am making an extra column named country using geopy,\n\n>  you can play with this geopy library to get latitude and longitude also..\n\n> Comment down how you want to use geopy for this competion ?","991d15cc":"> Analysing first 100 rows","03793734":"**converting the data into vector forms using Word2Vec with vector size of 300**","911a58bb":"> it always recommended to check total number of null values\n\n> and then we have to decide where we should delete all the null rows or repalce by mean, median, mode or total number of counts","741db9e0":"**Removing all punctuations except hash**","743af9b4":"**Toekenizing the data**","ad68fc9c":"> here i am adding stop words from two different package.\n\n> you can check all the stop words by running below code.","df8af018":"**let's extract the country name from given location**","e2d6a0ba":"# Work in progress, please upvote this kernel if you like my work and comment if i made any mistake.","d4ff5a48":"# Let's start doing analysis on text data","aee1134e":"**future pending work to be done below**"}}