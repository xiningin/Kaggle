{"cell_type":{"8d9accbd":"code","4a87595e":"code","2f154fbc":"code","c70b4f24":"code","5baa40fd":"code","4b134721":"code","3f103d8b":"code","7bef3d32":"code","6c584a1f":"code","d5874ee6":"code","2b33bf34":"code","26d9fb05":"markdown","fa7009f9":"markdown","52d44ca0":"markdown","8f343f03":"markdown","12349d18":"markdown","145b22a4":"markdown","5fe7e387":"markdown","9f5fff60":"markdown","28429f22":"markdown","a2a2ba1a":"markdown","5a73c873":"markdown","c377828a":"markdown","ec04888d":"markdown","882eb048":"markdown","9897185c":"markdown"},"source":{"8d9accbd":"import keras\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils.vis_utils import plot_model","4a87595e":"# Load the raw csv data directly from Kaggle database\ndata = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndata.info()\ndata.head()","2f154fbc":"# Split the training and testing data\ninputs = data.values[:,0:8]\nlabels = data.values[:,8].reshape((-1, 1))\nsampleSize = inputs.shape[0]\n# testInputs = inputs[0:int(sampleSize\/3), :] # 1\/3 for testing\n# testLabels = labels[0:int(sampleSize\/3), :]\n# trainInputs = inputs[-int(sampleSize*2\/3):, :] # 2\/3 for training\n# trainLabels = labels[-int(sampleSize*2\/3):, :]","c70b4f24":"# Set up the network architecture\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=8, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","5baa40fd":"# Compile the network\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","4b134721":"# Visualize the neural network's architecture\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n\nimport matplotlib.image as img\nim = img.imread('model_plot.png')\nplt.imshow(im)","3f103d8b":"history = model.fit(inputs, labels, validation_split=0.33,epochs=150, batch_size=10, verbose=0)","7bef3d32":"# Plot the learning curve (quantified by loss)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train','Test'], loc='upper left')\nplt.show()","6c584a1f":"model = Sequential()\nmodel.add(Dense(24, input_dim=8, activation='relu'))\nmodel.add(Dense(12, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(inputs, labels, validation_split=0.33,epochs=10, batch_size=10, verbose=0)\n\n# Plot the training loss curve\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","d5874ee6":"model = Sequential()\nmodel.add(Dense(12, input_dim=8, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(inputs, labels, validation_split=0.33,epochs=5000, batch_size=15, verbose=0) \n\n# Plot the training loss curve\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","2b33bf34":"K, bestAccuracy = 3, 0 \n\naccuracies = np.zeros((K, 1))\nfor i in range(K):\n    # Split the data for training and testing\n    if i == 0:\n        kthTestData = inputs[0:int(sampleSize\/K), :]\n        kthTestLabels = labels[0:int(sampleSize\/K), :]\n        kthTrainData = inputs[-int(sampleSize*(K-1)\/K):, :]\n        kthTrainLabels = labels[-int(sampleSize*(K-1)\/K):, :]\n    elif i == K-2:\n        kthTestData = inputs[-int(sampleSize\/K):, :]\n        kthTestLabels = labels[-int(sampleSize\/K):, :]\n        kthTrainData = inputs[0:int(sampleSize*(K-1)\/K), :];\n        kthTrainLabels = labels[0:int(sampleSize*(K-1)\/K), :]\n    else:\n        kthTestData = inputs[i*int(sampleSize\/K):(i+1)*int(sampleSize\/K),:]\n        kthTestLabels = labels[i*int(sampleSize\/K):(i+1)*int(sampleSize\/K), :]  \n        kthTrainDataFront = inputs[0:i*int(sampleSize\/K),:]\n        kthTrainDataBack = inputs[(i+1)*int(sampleSize\/K):, :] \n        kthTrainLabelsFront = labels[0:i*int(sampleSize\/K),:]\n        kthTrainLabelsBack = labels[(i+1)*int(sampleSize\/K):, :] \n        kthTrainData = np.vstack((kthTrainDataFront, kthTrainDataBack))   # Stack arrays in sequence vertically (row wise)\n        kthTrainLabels = np.vstack((kthTrainLabelsFront, kthTrainLabelsBack))\n    # Configure and compile the kth network for training\n    kthModel = Sequential()\n    kthModel.add(Dense(12, input_dim=8, activation='relu'))\n    kthModel.add(Dense(8, activation='relu'))\n    kthModel.add(Dense(1, activation='sigmoid'))\n    kthModel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # Train the kth network\n    kthModel.fit(kthTrainData, kthTrainLabels, epochs=150, batch_size=10, verbose=0) \n    # Obtain the prediction accuracy for the kth network\n    predictions = kthModel.predict_classes(kthTestData)\n    accuracy = np.sum(predictions == kthTestLabels) \/ kthTestLabels.shape[0]\n    accuracies[i] = accuracy\n    # Or we can directly use the 'evaluate' method\n    _, accuracy = kthModel.evaluate(kthTestData, kthTestLabels)\n    if accuracy > bestAccuracy:\n        bestAccuracy = accuracy\n        bestNN = kthModel\n# Show the mean and std of the k accuracies\nprint('The averaged accuracy =', np.mean(accuracies), '; standard deviation = ', np.std(accuracies))","26d9fb05":"# K-Fold Cross Validation\nThis method evaluates the performance of a neural network with less variance than a single train-test set split. Base on the k different performances, you can summarize the performance using a mean and a standard deviation of the k accuracies, which is a more comprehensive and reliable estimation of the network's performance.\n* The original sample is randomly partitioned into k equal sized subsamples. \n* A single subsample is retained as the validation data for testing the network.\n* The remaining k \u2212 1 subsamples are used as training data. \n\nVisualization of this validation procedure:\n\n![kfoldvalidation.PNG](attachment:kfoldvalidation.PNG)\n\nFig. source: Wikipedia, Cross-validation (statistics)","fa7009f9":"# Step 2. Configure the Neural Network","52d44ca0":"# Overfitting and Underfitting\n![image.png](attachment:image.png)\n\nFig. source: AI Wiki, Overfitting vs Underfitting\n\nUnderfitting Characteristics: perform poorly on the training data; generalize poorly to other new test data. \n\nLearning curve indicates underfitting if:\n* The training loss remains flat regardless of training.\n* The training loss continues to decrease until the end of training.","8f343f03":"# Step 3. Train the Neural Network","12349d18":"![image.png](attachment:af292cd0-8be9-4e4f-86d9-5acf5daeaff2.png)","145b22a4":"Tutorial 9 Artificial Neural Network\n\nMECH 305\/306 April 7th\n\nDataset\n\nhttps:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database\n\n* This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. \n* The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. \n* The datasets consists of several medical predictor variables and one outcome variable. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, etc. The outcome variable is either 1: have diabetes or 0: does not have diabetes.\n* Our goal: build an artificial neural network model to predict whether the patients have diabetes or not.","5fe7e387":"Cross Entropy Loss\n\n$D(\\vec{predicted}, \\vec{label}) = -\\sum_i \\vec{label}(i)log(\\vec{predicted}(i))$","9f5fff60":"# Reference\/Acknowledgement\n* https:\/\/machinelearningmastery.com\/\n* Recommend this website for further self-learning.","28429f22":"![image.png](attachment:e741313b-52c4-4992-abcd-15c2d7b3ea75.png)","a2a2ba1a":"Key parameter explanation\n* Sample: a single row of data.\n* Epoch: the number of times that the network will work through the entire training dataset.\n* Batch size: the number of samples to work through before updating the network's parameters.\n* Iteration: the number of batches needed to complete one epoch.\n* For example, we split a dataset of 5000 samples into batches of 500. Thus, it will take 10 iterations to complete 1 epoch.\n* Validation_split: the ratio of the data used for validation (here in the example, 1\/3 used for validation and 2\/3 used for training).\n* Verbose: display the training progress; 1: display vs. 0: not display.","5a73c873":"Overfitting characteristics: perform well on the training data, but generliaze poorly to other new test data. \n\nLearning curve indicates overfitting if:\n* The plot of training loss continues to decrease with experience.\n* The plot of validation loss decreases to a point and begins increasing again.","c377828a":"# Step 4: Evaluate the Neural Network\n\nWe can use the learning curve to evaluate the performance of the neural network. Learning curve indicates a good fit if:\n* The plot of training loss decreases to stability.\n* The plot of validation loss decreases to stability and has a small gap with the training loss.","ec04888d":"# Step 1: Load the Data for Training\n\nInput Variables for Training:\n* Number of times pregnant\n* Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* Diastolic blood pressure (mm Hg)\n* Triceps skin fold thickness (mm)\n* 2-Hour serum insulin (mu U\/ml)\n* Body mass index (weight in kg\/(height in m)^2)\n* Diabetes pedigree function\n* Age (years)\n\nLabels (outcomes) for Training:\n* Class variable (0 or 1)\n* 0: no diabetes; 1: diabetes","882eb048":"![image.png](attachment:8f156473-94fa-4b73-98c2-cad188d04c37.png)","9897185c":"![sigmoid.png](attachment:70b72291-7840-44f5-adf5-bc3a049c2004.png)\n\nFormula: $f(x) =  \\frac{1}{1 + e^{-x}}$  \nTypically used for binary classification."}}