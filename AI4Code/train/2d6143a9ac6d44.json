{"cell_type":{"d1407cbe":"code","909aa0f4":"code","ad32f8b0":"code","92907e3f":"code","a40ed868":"code","a9c345e3":"code","6f804826":"code","63a96f1a":"code","1d55f1c6":"code","5c3493b7":"code","414a2b9b":"code","6895b7e8":"code","bc162b91":"code","9dced208":"code","d797675a":"code","ed86efe6":"code","91c70c23":"code","68d6b1d7":"code","487a8514":"code","539949da":"code","0aaaaa13":"code","18f4f702":"code","bd8c7dde":"code","9cfe6a38":"code","8cd4051b":"code","478cbaa3":"code","d4828c6b":"code","cb383ebc":"code","1bac32b9":"code","36de42f5":"code","be8570c6":"code","5d4434a7":"code","25c72da2":"code","bc67d40b":"code","cd9b2466":"code","8225e061":"markdown","4577bd90":"markdown","76d707d5":"markdown","bae3bb49":"markdown","3cdc8c90":"markdown","d23faffd":"markdown","e7eb962b":"markdown","34c3867c":"markdown","b1219565":"markdown","342bb33a":"markdown","0d71fe75":"markdown","26325baf":"markdown","6356d4da":"markdown","2259507b":"markdown","7a0bad8f":"markdown","79eaa17f":"markdown"},"source":{"d1407cbe":"import logging\nlogger = logging.getLogger(\"spacy\")\nlogger.setLevel(logging.ERROR)\n\nimport dask.bag as db\nimport json\nimport pandas as pd\n\ndocs = db.read_text('..\/input\/arxiv\/arxiv-metadata-oai-snapshot.json').map(json.loads)","909aa0f4":"#Total number of documents: 1872765\ndocs.count().compute()","ad32f8b0":"# Looking at one document:\ndocs.take(1)","92907e3f":"# The dataset is very huge. Not sure if the whole set can be used. I start prototyping with a subset of the data so it's easyer to handel:\n# This procedure was recommended in the ArXiv dataset itself\n\nget_latest_version = lambda x: x['versions'][-1]['created']\n\n\n# get only necessary fields of the metadata file\ntrim = lambda x: {'id': x['id'],\n                  'authors': x['authors'],\n                  'title': x['title'],\n                  'doi': x['doi'],\n                  'category':x['categories'].split(' '),\n                  'abstract':x['abstract'],}\n# filter for papers published on or after 2019-01-01\ncolumns = ['id','category','abstract']\ndocs_df = (docs.filter(lambda x: int(get_latest_version(x).split(' ')[3]) > 2018)\n           .map(trim).\n           compute())\n\n# convert to pandas\ndocs_df = pd.DataFrame(docs_df)","a40ed868":"#save trimmed dataset for later use so we can skip the dataset trimming later:\ndocs_df.to_csv(\"trimmed_arxiv_docs.csv\", index=False)","a9c345e3":"#Let's have a look at the first 5 rows:\ndocs_df.head()","6f804826":"df = pd.read_csv(\".\/trimmed_arxiv_docs.csv\")","63a96f1a":"df.info()","1d55f1c6":"df.shape","5c3493b7":"#Addint word counts of each abstract could be a usefull feature\ndf['abstract_word_count'] = docs_df['abstract'].apply(lambda x: len(x.strip().split())) ","414a2b9b":"df['abstract'].describe(include='all')","6895b7e8":"#There are duplicated abstracts, would be the best to get rid of these\ndf.drop_duplicates(['abstract',], inplace=True)\ndf['abstract'].describe(include='all')","bc162b91":"# the dataframe contains still hugh amount of data. The process the data faster I reduce the df to 10000 rows\n# The scope of the notebook is not to analyze all data\ndf = df.sample(10000, random_state=42)","9dced208":"from tqdm import tqdm\n# I discoverd that it's possible to download models for the specific purpose to preprocess scientific texts\n# In the spacy docs I found a specific model for this : https:\/\/spacy.io\/universe\/project\/scispacy\n#Downloading en_core_sci_lg model to preprocess abstracts\nfrom IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.4.0\/en_core_sci_lg-0.4.0.tar.gz","d797675a":"#Import NLP librarys and the spacy package to preprocess the abstract text\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS #import commen list of stopword\nimport en_core_sci_lg  # import downlaoded model","ed86efe6":"# Parser\nparser = en_core_sci_lg.load()\nparser.max_length = 7000000 #Limit the size of the parser\n\ndef spacy_tokenizer(sentence):\n    ''' Function to preprocess text of scientific papers \n        (e.g Removing Stopword and puntuations)'''\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ] # transform to lowercase and then split the scentence\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ] #remove stopsword an punctuation\n    mytokens = \" \".join([i for i in mytokens]) \n    return mytokens","91c70c23":"import string\n\npunctuations = string.punctuation #list of punctuation to remove from text\nstopwords = list(STOP_WORDS)\nstopwords[:10]","68d6b1d7":"tqdm.pandas()\ndf[\"processed_text\"] = df[\"abstract\"].progress_apply(spacy_tokenizer)","487a8514":"# Import vectorizer and define vec function\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef vectorize(text, maxx_features):\n    \n    vectorizer = TfidfVectorizer(max_features=maxx_features)\n    X = vectorizer.fit_transform(text)\n    return X","539949da":"#vectorize each processed abstract\ntext = df['processed_text'].values\nX = vectorize(text, 2 ** 12) #arbitrary max feature -_> Hyperpara. for optimisation (?)\nX.shape","0aaaaa13":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95, random_state=42) #Keep 95% of the variance\nX_reduced= pca.fit_transform(X.toarray())\nX_reduced.shape","18f4f702":"from sklearn.cluster import KMeans\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom scipy.spatial.distance import cdist\nfrom sklearn import metrics","bd8c7dde":"# find optimal k value\n#r_seed = 24\n#cluster_errors = []\n\n#for i in range(1, 50):\n    #n_clusters = i\n    #pipe_pca_kmean = Pipeline([(\"cluster\", KMeans(n_clusters=n_clusters, random_state=r_seed, verbose=0, n_jobs=1))]\n    #)\n\n    #pipe_pca_kmean.fit(X_reduced)\n    #pipe_pca_kmean.predict(X_reduced)\n    #cluster_errors.append(pipe_pca_kmean.named_steps[\"cluster\"].inertia_) ","9cfe6a38":"#plt.clf()\n#plt.plot(cluster_errors, \"o-\")\n#plt.xlabel(\"k_clusters\")\n#plt.ylabel(\"sum sq distances from mean\")\n#plt.show()","8cd4051b":"k = 20 # optimal k found in elbow plot\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(X_reduced)\ndf['kmean_clusters'] = y_pred","478cbaa3":"!pip install umap-learn","d4828c6b":"from umap import UMAP","cb383ebc":"# UMAP Definition:\numap_embeddings = UMAP(n_neighbors=100, min_dist=0.3, n_components=2)","1bac32b9":"X_umap = umap_embeddings.fit_transform(X_reduced)","36de42f5":"from sklearn.manifold import TSNE\n\ntsne = TSNE(verbose=1, perplexity=100, random_state=42)\nX_embedded = tsne.fit_transform(X.toarray())","be8570c6":"import seaborn as sns\n\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"bright\", 1)\n\n# plot\nsns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], palette=palette)\nplt.title('t-SNE without Labels')\nplt.savefig(\"t-sne_arxvid.png\")\nplt.show()","5d4434a7":"# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"bright\", 1)\n\n# plot\nsns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], palette=palette)\nplt.title('umap without Labels')\nplt.savefig(\"umap_arxvid.png\")\nplt.show()","25c72da2":"%matplotlib inline\n\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.hls_palette(20, l=.4, s=.9)\n\n# plot\nsns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\nplt.title('t-SNE with Kmeans Labels')\nplt.savefig(\"cluster_tsne.png\")\nplt.show()","bc67d40b":"# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.hls_palette(20, l=.4, s=.9)\n\n# plot\nsns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], hue=y_pred, legend='full', palette=palette)\nplt.title('umap with Kmeans Labels')\nplt.savefig(\"cluster_umap_kmeans_labels.png\")\nplt.show()","cd9b2466":"import plotly.express as px\nfig = px.scatter(df, x=X_embedded[:,0], y=X_embedded[:,1], color=y_pred.astype(str),\n                 hover_data=['id', 'authors', 'title',],\n                 height= 1000, width=1000,\n                title = \"t-SNE with Kmeans Labels\")\nfig.show()","8225e061":"# Goal of this Project\n\nThe goal of this project is to cluster scientifical papers based on theirs abstract with unsupervised ML methods.\n\nBy using clustering for labelling in combination with dimensionality reduction for visualization, the collection of papers can be represented as a scatter plot. On this plot, papers of highly similar topics will share a label and will be plotted near each other. As a bonus (if clustering works well) I try to find meaning in the clusters by using topic modelling to find the keywords of each cluster.\nThis can help to find publications with similar research backgrounds and to compare similar research and publications.\n\nThe data must be prepared accordingly so that they can be processed with a clustering model. \nI use NLP (Natural Language Processing) to prepare the data. With this notebook I also want to test how certain preprocessing steps affect the quality of the clusters (stops word removing, handling different languages, different vectorization strategies).\n\nThis project will be done with the [arXiv Dataset](https:\/\/www.kaggle.com\/Cornell-University\/arxiv) featured on kaggle.com","4577bd90":"# Vectorization of the abstracts and dimensionality reduction with PCA","76d707d5":"# ","bae3bb49":"# Plot interactive scatter plot bases on t-SNE","3cdc8c90":"# Compare t-SNE and umap","d23faffd":"The raw text of the abstracts can't be processed by a model. Next step now is to transform the data in two steps:\n\n- Use NLP to restructur the abstract text --> Remove Stop words an punctuation\n- Vectorize the abstact's of each paper\n\nThe NLP wasn't cover in the course. Most of the strategies use in the next steps are based on the kaggle NLP Course https:\/\/www.kaggle.com\/learn\/natural-language-processing","e7eb962b":"# t-SNE and umap (Using umap to see difference to t-SNE)","34c3867c":"# Plot Clusters","b1219565":"### About the ArXiv dataset:\n> For nearly 30 years, ArXiv has served the public and research communities by providing open access to scholarly articles, from the vast branches of physics to the many subdisciplines of computer science to everything in between, including math, statistics, electrical engineering, quantitative biology, and economics. This rich corpus of information offers significant, but sometimes overwhelming depth.\nIn these times of unique global challenges, efficient extraction of insights from data is essential. To help make the arXiv more accessible, we present a free, open pipeline on Kaggle to the machine-readable arXiv dataset: a repository of 1.7 million articles, with relevant features such as article titles, authors, categories, abstracts, full text PDFs, and more.\nOur hope is to empower new use cases that can lead to the exploration of richer machine learning techniques that combine multi-modal features towards applications like trend analysis, paper recommender engines, category prediction, co-citation networks, knowledge graph construction and semantic search interfaces. \n\n>#### Cite: https:\/\/www.kaggle.com\/Cornell-University\/arxiv","342bb33a":"# Clustering\n\nUsing Kmeans for Clustering","0d71fe75":"The labeled plot gives insights of how the papers are grouped.\nIt is difficult to say which dimension reduction performs better for this data. The performance would have to be evaluated in further steps.\n\nThe location of each paper on the plot was determined by umap \/ t-SNE while the labels (colors) was determined by k-means. \nIf we look at a particular parts of the plots where t-SNE and umap have grouped many articles forming a cluster, it is likely that k-means is uniform in the labeling of this cluster.\n\nIn other cases the labels (k-means) are more spread out on the plot (umap \/t-SNE). This means that umap \/ t-SNE and k-means found differences in the higher dimensional data. \n\nThis could be because certain contents in the papers overlaping, so it's hard to clearly separate them. This effect can be observed in the formation of subclusters on the plot.\n\nThe algorithms may find connections that were unnaparent to humans. This may highlight hidden shared information and advance further research.","26325baf":"# Approach:\n\n- Load data\n- Parse the text from the abstract of each document using Natural Language Processing (NLP).\n- Turn each abstract into a feature vector using Term Frequency\u2013inverse Document Frequency (TF-IDF).\n- Use Principal Component Analysis (PCA) for dimensionality reduction\n- Apply Dimensionality Reduction to each feature vector using t-Distributed Stochastic Neighbor Embedding (t-SNE) and umap.\n- Apply k-means clustering\n- Visualize clusters as a (interactive) scatterplot \n- **Bonus:** Apply Topic Modeling for each cluster ","6356d4da":"# Short EDA and some basic data-cleaning \/ feature engineering","2259507b":"# Load the data\nIf the whole dataset is to be loaded, it is recommended in the footnote of the dataset to load the dataset with the library [dask](http:\/\/dask.org).\nDask makes it possible to load lager datasets an process large amount of data on personal computers or VM's with limited resources\n\nAbout dask:\n\n> Dask is a flexible library for parallel computing in Python.\n\n> From dask.org","7a0bad8f":"Content:\n1. Load data\n2. Short EDA and some basic data-cleaning \/ feature engineering\n3. NLP data preprocessing\n4. Vectorization of the abstracts and dimensionality reduction with PCA\n5. Clustering\n6. t-SNE and umap (Using umap to see difference to t-SNE)\n7. Compare t-SNE and umap\n8. Plots an interactive scatter plot","79eaa17f":"# NLP data preprocessing"}}