{"cell_type":{"9dadf0d4":"code","e5130ea7":"code","099cad1a":"code","3f9cc008":"code","999b786d":"code","4ee96a8a":"code","0ff2fec0":"code","dffdae7e":"code","98b2d060":"code","2b9a76ce":"code","c9c7aa1c":"code","a03b723c":"code","90717063":"code","f6e7152e":"code","8e4189e4":"code","4c27d2f1":"code","773ebcdd":"code","a3b72419":"code","9fc9f138":"code","44c10e37":"code","b613b493":"code","e7ab2ad1":"code","f131b8aa":"code","848b63d1":"code","8a92da9e":"code","935d5936":"code","ff2bd5ad":"code","85502404":"code","f52b24f0":"code","a933ed8d":"code","b52e2f51":"code","d96f5bbb":"code","518561bc":"code","5f49e3ab":"code","c1f5d0ab":"code","f5c5d2c6":"code","f53bcf8c":"code","fe79c4a7":"code","bdde9ecb":"code","1c3d1c5a":"code","b1216a76":"code","dbf057fe":"code","d1191d26":"code","67ce5dd6":"code","e54ab872":"code","602c3ca6":"code","3e2d9bc0":"code","26d1e1e3":"code","2cfa2877":"code","46209e5f":"code","29ee707e":"code","9367657f":"code","5232618a":"code","293026d0":"code","a335b807":"code","c88139c0":"code","53c1d398":"code","7b69b562":"code","a4e5cefa":"code","342dd6e1":"code","1323bd8e":"code","f745f141":"code","4ab10097":"code","b6aca28f":"code","ead82e42":"code","b5c48b06":"code","d6c8a8d8":"code","30973558":"code","2fd8b5ca":"markdown","ca293565":"markdown","3ed45af3":"markdown","0c0266e2":"markdown","a494b295":"markdown","a4335620":"markdown","25fc370d":"markdown","20443822":"markdown","d3582d0a":"markdown","a2f91dd9":"markdown","17c9595f":"markdown","5eb0fb75":"markdown","32c35125":"markdown","f43d92e1":"markdown","29b6a9c4":"markdown","3830dc7a":"markdown","957247dc":"markdown","7f847704":"markdown","23aa8f3e":"markdown","4789cc1d":"markdown","0f07da17":"markdown"},"source":{"9dadf0d4":"# Import built-in modules\nimport sys\nimport platform\nimport os\nfrom pathlib import Path\nimport warnings\n\n# Import external modules\nfrom IPython import __version__ as IPy_version\nimport IPython.display as ipyd\nimport numpy as np\nimport pandas as pd\nfrom sklearn import __version__ as skl_version\nfrom sklearn.model_selection import train_test_split\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom bokeh import __version__ as bk_version\nimport geopandas as gpd\nimport geoviews as gv\n\n# Check they have loaded and the versions are as expected\nassert platform.python_version_tuple() == ('3', '6', '6')\nprint(f\"Python version:\\t\\t{sys.version}\")\nassert IPy_version == '6.4.0'\nprint(f'IPython version:\\t{IPy_version}')\nassert np.__version__ == '1.16.2'\nprint(f'numpy version:\\t\\t{np.__version__}')\nassert pd.__version__ == '0.23.4'\nprint(f'pandas version:\\t\\t{pd.__version__}')\nassert skl_version == '0.20.3'\nprint(f'sklearn version:\\t{skl_version}')\nassert mpl.__version__ == '2.2.3'\nprint(f'matplotlib version:\\t{mpl.__version__}')\nassert bk_version == '1.0.4'\nprint(f'bokeh version:\\t\\t{bk_version}')\nassert gpd.__version__ == '0.4.0'\nprint(f'geopandas version:\\t{gpd.__version__}')\nassert gv.__version__ == '1.6.1'\nprint(f'geoviews version:\\t{gv.__version__}')","e5130ea7":"# Set warning messages\n# Show all warnings in IPython\nwarnings.filterwarnings('always')\n# Ignore specific numpy warnings (as per <https:\/\/github.com\/numpy\/numpy\/issues\/11788#issuecomment-422846396>)\nwarnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n# Additional warning that sometimes pops up\nwarnings.filterwarnings(\"ignore\", message=\"`nbconvert.exporters.exporter_locator` is deprecated\")\nwarnings.filterwarnings(\"ignore\", message=\"Traits should be given as instances\")","099cad1a":"# Load Bokeh for use in a notebook\nfrom bokeh.io import output_notebook\noutput_notebook()","3f9cc008":"# Initialise geoviews\ngv.extension('bokeh')","999b786d":"# Configuration variables\nclaims_data_filepath = Path('\/kaggle\/input\/french-motor-claims-datasets-fremtpl2freq\/freMTPL2freq.csv')\nadditional_data_folderpath = Path('\/kaggle\/input\/additionaldataforfrenchmotorclaims')","4ee96a8a":"# Output exact environment specification, in case it is needed later\nprint(\"Capturing full pip environment spec\")\nprint(\"(But note that not all these packages are required)\")\n!pip freeze > requirements_Kaggle.txt","0ff2fec0":"# Load data\nnrows_sample = 1000\ndf_small_sample = pd.read_csv(claims_data_filepath, delimiter=',', nrows = nrows_sample)\n\n# Check it has loaded OK\nassert df_small_sample.shape == (1000, 12)\nprint(\"Correct: Shape of DataFrame is as expected\")","dffdae7e":"#  Look at the first few rows\ndf_small_sample.head()","98b2d060":"# Look at data types of the columns\ndtypes_sample = df_small_sample.dtypes\nprint(dtypes_sample)  # Check these look reasonable","2b9a76ce":"assert df_small_sample.isna().sum().sum() == 0\nprint(\"Correct: There are no missing values in the sample dataset\")","c9c7aa1c":"# Load data\ndf_raw = pd.read_csv(\n    claims_data_filepath, \n    delimiter=',', \n    dtype={'IDpol': np.int64}  # Without this, IDpol is cast as a float, but not sure why\n)","a03b723c":"# Check it has loaded OK\nnRows, nCols = (678013, 12)\nassert df_raw.shape == (nRows, nCols)\nprint(f\"Correct: Shape of DataFrame is as expected: {nRows} rows, {nCols} cols\")\nassert (df_raw.dtypes == dtypes_sample).all()\nprint(\"Correct: Data types are as expected\")\nassert df_raw.isna().sum().sum() == 0\nprint(\"Correct: There are no missing values in the raw dataset\")","90717063":"def get_df_extra(df):\n    \"\"\"\n    Given a DataFrame of that contains the raw data columns (and possibly additional columns), \n    return the DataFrame with additional pre-processed columns\n    \"\"\"\n    df_extra = df.copy()\n    \n    # Exposure rounded to 4 dps\n    df_extra['Exp_4dps'] = df_extra.Exposure.round(4)\n    \n    # Calculate frequency per year on each row\n    df_extra['freq_pyr'] = df_extra['ClaimNb'] \/ df_extra['Exp_4dps']\n    \n    return(df_extra)  ","f6e7152e":"# Run pre-processing to get a new DataFrame\ndf_extra = get_df_extra(df_raw)","8e4189e4":"# Import modules specific for this section\nfrom sklearn.model_selection import train_test_split","4c27d2f1":"# Get index sorted with ascending IDpol, just in case it is out or order\ndf_all = df_raw.sort_values('IDpol').reset_index(drop=True)\n\n# Split out training data\ndf_train, df_not_train = train_test_split(\n    df_all, test_size=0.3, random_state=51, shuffle=True\n)\n# Split remaining data between validation and holdout\ndf_validation, df_holdout = train_test_split(\n    df_not_train, test_size=0.5, random_state=13, shuffle=True\n)","773ebcdd":"# Check resulting split looks reasonable\ndf_raw.assign(  \n    # Add indicator columns for whether each row is in_train, in_validation, in_holdout\n    in_train=df_raw.IDpol.isin(df_train.IDpol),\n    in_validation=df_raw.IDpol.isin(df_validation.IDpol),\n    in_holdout=df_raw.IDpol.isin(df_holdout.IDpol),\n    # Add column of which subset each row is in\n    subset=lambda x: np.select(\n        [x.in_train, x.in_validation, x.in_holdout],\n        ['train', 'validation', 'holdout'],\n        default='no_subset')\n).groupby(  # Group rows by which subset they are in\n    ['in_train', 'in_validation', 'in_holdout', 'subset']\n).agg({  # Calculate stats for each group\n    'IDpol': 'size', 'Exposure': 'sum', 'ClaimNb': 'sum'\n}).rename(columns={'IDpol': 'num_of_rows'}).assign(\n    # Add additional stats\n    Claim_freq_overall=lambda x: x.ClaimNb \/ x.Exposure,\n    num_of_rows_prop=lambda x: x.num_of_rows \/ x.num_of_rows.sum(),\n    Exposure_prop=lambda x: x.Exposure \/ x.Exposure.sum(),\n    ClaimNb_prop=lambda x: x.ClaimNb \/ x.ClaimNb.sum(),\n).pipe(lambda df: df.append(pd.DataFrame.from_dict({\n    # Add totals row. It is the sum for every column except 'Claim_freq_overall'\n    # where it is the overall claims frequency of the entire data set\n    ('Total','','',''): [\n        df.ClaimNb.sum() \/ df.Exposure.sum() if col_name == 'Claim_freq_overall'\n        else df.loc[:,col_name].sum() for col_name in df\n    ]}, orient='index', columns=df.columns\n))) \\\n.style.format(  # Format the output so it looks reasonable when printed\n    '{:.2%}'  # Default number format\n).format({  # Specific number formats where we want to override the default\n    **{col: '{:,.0f}' for col in ['num_of_rows', 'ClaimNb']},\n    'Exposure': '{:,.1f}'\n}).apply(  # Separate totals row by adding a line\n    lambda x: ['border-top-style: double'] * x.shape[0], subset=pd.IndexSlice[\"Total\",:]\n)","a3b72419":"assert (df_raw.duplicated('IDpol') == False).shape[0] == df_raw.shape[0]\nprint(\"Correct: There are no duplicates of IDpol\")\nassert (df_raw.sort_values('IDpol').index == df_raw.index).all()\nprint(\"Correct: Rows are sorted in order of ascending IDpol\")","9fc9f138":"df_raw.Exposure.plot.hist(bins=50)\nplt.title(\"Histogram of Exposure (years)\")\nplt.show()","44c10e37":"# In fact, the values close to 1 are all to 2 dps\n# Look at the frequency close to 1. Does not look unreasonable\ndf_extra[(df_extra.Exposure > 0.95) & (df_raw.Exposure <= 1.05)].Exposure.value_counts().sort_index()","b613b493":"# Not *all* policies have Exposure to 2 dps. The following shows that:\n# - Values below 0.01 are split further\n# - Values greater than or equal to 0.01 are given to 2 dps\ndf_extra.assign(Exp_100=lambda x: (x.Exposure * 100).round(6)).assign(\n    Exp_100_int_part=lambda x: np.trunc(x.Exp_100)\n).assign(\n    Exp_100_frac_part=lambda x: x.Exp_100 - x.Exp_100_int_part,\n    Exp_100_int_part_gte_1=lambda x: x.Exp_100_int_part >= 1\n).groupby(['Exp_100_int_part_gte_1', 'Exp_100_frac_part']).size(\n).to_frame('num_of_policies').reset_index().style.format(\n    {'Exp_100_frac_part': '{:.8f}', 'num_of_policies': '{:,}'})","e7ab2ad1":"def display_side_by_side(*args):\n    \"\"\"\n    Print the display of multiple DataFrames side-by-side\n    \n    *args: DataFrames or Stylers to be displayed side-by-side\n    Return: No return value\n    Adapted from: <https:\/\/stackoverflow.com\/a\/44923103>\n    \"\"\"\n    html_str=''\n    for df_styler in args:\n        if isinstance(df_styler, pd.DataFrame):\n            df_styler = df_styler.style\n        html_str += df_styler.set_table_attributes(\n            \"style='display:inline'\"  # Side-by-side\n        )._repr_html_()\n    ipyd.display_html(html_str,raw=True)\n\n# Example usage\n# df1 = pd.DataFrame(np.arange(12).reshape((3,4)),columns=['A','B','C','D',])\n# df2 = pd.DataFrame(np.arange(16).reshape((4,4)),columns=['A','B','C','D',])\n\n# df1_styler = df1.style.set_table_attributes(\"style='display:inline'\").set_caption('Caption table 1')\n# df2_styler = df2.style.set_table_attributes(\"style='display:inline'\").set_caption('Caption table 2')\n\n# display_side_by_side(df1, df2_styler)","f131b8aa":"# In fact, the following shows that some Exposure values that \n# are rounded (to 8 dps), and some that are not.\nexposure_cnts_df = df_extra.Exposure.value_counts().sort_index(\n).to_frame('num_of_policies').reset_index(\n).rename(columns={'index': 'Exposure'}).assign(\n    Exposure_to_8_dp=lambda x: x.Exposure.round(8),\n)[['Exposure', 'Exposure_to_8_dp', 'num_of_policies']]  # Re-order columns\n\n# We can compare these to the Exposure value calculated for a given number of days \n# (and assumed number of days in the year). By comparing these to the above, \n# we see that the Exp_100 values below one are highly likely to equate to a number of days. \ndays_in_yr_df = pd.DataFrame({'num_of_days': np.arange(1, 5)}).assign(\n    In_365d_yr=lambda x: x.num_of_days \/ 365,\n    In_366d_yr=lambda x: x.num_of_days \/ 366,\n)\nnum_dps = 15\ncol_format = f'{{:.{num_dps}f}}'\n\ndisplay_side_by_side(\n    exposure_cnts_df[exposure_cnts_df.Exposure < 0.01].style.format(\n        {'Exposure': '{:.20f}'}\n    ).set_caption(\"Exposure values below 0.01 in the data\").hide_index(),\n    days_in_yr_df.style.format(\n        {'In_365d_yr': col_format, 'In_366d_yr': col_format}\n    ).set_caption(\"Calculated Exposure for given num of days\").hide_index()\n)","848b63d1":"def plot_exposure_hist_zoom(sers=df_extra.Exp_4dps, lims=(None, None), width=0.01):\n    \"\"\"Zoom in on a specific region of the histogram of Exposure\"\"\"\n    lims = list(lims)\n    if lims[0] is None:\n        lims[0] = 0\n    if lims[1] is None:\n        lims[1] = sers.max()\n    n_bins = int((lims[1] - lims[0]) \/ width)\n    sers[(sers >= lims[0]) & (sers <= lims[1])].plot.hist(bins=n_bins)\n    plt.title(f\"Histogram of Exposure (years) between {lims[0]} and {lims[1]}\")","8a92da9e":"# No obvious pattern in the values above 1\nplot_exposure_hist_zoom(lims=(1.05, None))","935d5936":"# Some very low values\nplot_exposure_hist_zoom(lims=(None, 0.3), width=0.002)\nprint(\n    \"The peak at 0.08 is equivalent to one month because:\\n\"\n    f\"  - Upper bound: 31 \/ 365 = {31 \/ 365 :.5f}\\n\"\n    f\"  - Lower bound: 28 \/ 366 = {28 \/ 366 :.5f}\"\n)","ff2bd5ad":"expl_var_names = df_raw.columns[3:].tolist()\nprint('Explanatory variables:\\t' + '  '.join(expl_var_names))","85502404":"# Are there any duplicates? Yes, a lot!\ndup_summary1 = df_extra.duplicated(\n    expl_var_names, keep='first'  # Unique values are marked as False, so reverse this in the next operation\n).ne(True).value_counts().to_frame('num_of_rows').rename_axis(\n    'expl_vars_is_unique', axis=0\n)\ndup_summary1.style.format(\"{:,}\")","f52b24f0":"# But there are a lot fewer if we add Exp_4dps\ndf_extra.duplicated(\n    ['Exp_4dps'] + expl_var_names, keep='first'\n).ne(True).value_counts().to_frame('num_of_rows').rename_axis(\n    'expl_vars_and_exp_is_unique', axis=0\n).style.format(\"{:,}\")","a933ed8d":"# Add columns to identify duplicates\ndf_with_dups = df_extra.assign(\n    is_dup=lambda x: x.duplicated(expl_var_names, keep=False)\n).reset_index().rename(columns={'index': 'CSV_order'}).sort_values(\n    expl_var_names + ['CSV_order']  # CSV_order ensures the order is unique in case of tie-break\n).reset_index(drop=True).assign(\n    occurence_1st=lambda x: ~x.duplicated(expl_var_names, keep='first'),\n    d_CSV_order=lambda x: x.CSV_order - x.CSV_order.shift(1),  # change in CSV_order (current minus previous)\n    IDpol_unique=lambda x: x.occurence_1st.cumsum(),\n    d_CSV_order_dups=lambda x: np.where(~x.occurence_1st, x.d_CSV_order, 0).astype(int)\n)","b52e2f51":"# Check this matches the above\ndup_summary2 = df_with_dups.groupby(['occurence_1st', 'is_dup']).size(\n).to_frame('num_of_rows')\nassert dup_summary1.loc[False][0] == dup_summary2.loc[False].sum()[0]\nprint(f'Correct: Agrees number of repeated explanatory variable combinations')\nassert dup_summary1.loc[True][0] == dup_summary2.loc[True].sum()[0]\nprint(f'Correct: Agrees number of unique explanatory variable combinations')\ndup_summary2.style.format(\"{:,}\")","d96f5bbb":"# Many of the duplicated rows are consecutive IDpol numbers (i.e. d_CSV_order is 1)\ndf_with_dups.assign(\n    d_CSV_order_dups_grpd=lambda x: np.where(x.d_CSV_order_dups >= 5, '5+', x.d_CSV_order_dups)\n).query(\"is_dup == True\").groupby(\n    ['occurence_1st', 'd_CSV_order_dups_grpd']\n).size().to_frame('num_of_rows').style.format(\n    \"{:,}\").set_caption(\"Break down includes duplicated rows only\")","518561bc":"# Look at an example\nCSV_order_gap = 5  # Try amending these to get another example\nrand_example = 4\nview_range = 2\n\nIDpol_unique_example = df_with_dups.IDpol_unique[\n    df_with_dups.d_CSV_order_dups == CSV_order_gap].iloc[rand_example,]\nCSV_order_range = df_with_dups.CSV_order[df_with_dups.IDpol_unique == IDpol_unique_example].values\n\ndisplay(\n    df_with_dups.query(\n        \"IDpol_unique >= @IDpol_unique_example - @view_range & IDpol_unique <= @IDpol_unique_example + @view_range\"\n    ).style.apply(lambda row_sers: np.repeat(\n        np.select(\n            [row_sers.IDpol_unique == IDpol_unique_example], \n            ['background-color: yellow'], \n            default=''\n        ), row_sers.shape[0]\n    ), axis=1).set_caption(\"Example highlighted from table ordered by duplicates\")\n)\n\ndisplay(\n    df_extra.loc[CSV_order_range[0]-2:CSV_order_range[1]+2,:].style.apply(\n        lambda row_sers: np.repeat(\n            np.select([\n                (row_sers.name == CSV_order_range[0]) | (row_sers.name == CSV_order_range[1]),\n                (row_sers.name >= CSV_order_range[0]) & (row_sers.name <= CSV_order_range[1])\n            ],[\n                'background-color: yellow',\n                'background-color: red'\n            ], default=''\n            ), row_sers.shape[0]\n        ), axis=1\n    ).set_caption(\"Same example from table ordered by IDpol\")\n)","5f49e3ab":"# Get DataFrame with duplicates aggregated into one row each\ndf_de_duped = df_with_dups.groupby(expl_var_names + ['IDpol_unique']).agg({\n    'IDpol': 'size',\n    'Exp_4dps': 'sum',\n    'ClaimNb': 'sum',\n}).rename(columns={'IDpol': 'num_of_rows'}).reset_index().set_index('IDpol_unique')\n\n# Check it worked\nassert df_de_duped.shape[0] == dup_summary1.loc[True][0]\nprint(\"Correct: Number of rows matches number of unique explanatory variable combinations\")\nassert (df_de_duped.Exp_4dps.sum() - df_extra.Exp_4dps.sum()) < 1e-6\nprint(\"Correct: Sum of exposure agrees\")\nassert df_de_duped.ClaimNb.sum() == df_extra.ClaimNb.sum()\nprint(\"Correct: Sum of number of claims agrees\")","c1f5d0ab":"# Look at first few rows\ndf_de_duped.head()","f5c5d2c6":"# Look at the Exposure histogram now\ndf_de_duped.Exp_4dps.plot.hist(bins=50)\nplt.title(\"Histogram of Exposure (years)\")\nplt.show()","f53bcf8c":"# In fact, if we look back at a graph of the original (not aggregated) data\n# and compare it to the de-duped data, we see the peak at 0.08 (= one month)\n# has been largely removed, but little else noticeable has changed.\nfig, _ = plt.subplots(2, 2, figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplot_exposure_hist_zoom(width=0.01, lims=(None, 2))\nplt.subplot(1, 2, 2)\nplot_exposure_hist_zoom(df_de_duped.Exp_4dps, width=0.01, lims=(None, 2))","fe79c4a7":"# That change is even more pronounced when you zoom in on values below 1\nfig, _ = plt.subplots(2, 2, figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplot_exposure_hist_zoom(width=0.01, lims=(None, 0.98))\nplt.subplot(1, 2, 2)\nplot_exposure_hist_zoom(df_de_duped.Exp_4dps, width=0.01, lims=(None, 0.98))","bdde9ecb":"# Above 1, neither have very many policies, although the aggregated data \n# clearly has a peak at 2.\nfig, _ = plt.subplots(2, 2, figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplot_exposure_hist_zoom(width=0.01, lims=(1.02, None))\nplt.subplot(1, 2, 2)\nplot_exposure_hist_zoom(df_de_duped.Exp_4dps, width=0.01, lims=(1.02, 3))","1c3d1c5a":"# How many ID_pols have gone in to each IDpol_unique? \n# This shows the distribution. We see there is a sharp drop after 12.\ndf_de_duped.groupby('num_of_rows').agg({\n    'Area': 'size',\n    'Exp_4dps': ['sum', 'mean', 'min', 'max'],\n    'ClaimNb': ['sum', 'mean', 'min', 'max']\n}).rename(columns={'Area': 'num_of_IDpol_unique'}).assign(\n    Per_row_exp=lambda x: x[('Exp_4dps', 'mean')] \/ x.index,\n    Per_row_ClaimNb=lambda x: x[('ClaimNb', 'mean')] \/ x.index,\n).style.bar(subset=['Per_row_exp', 'Per_row_ClaimNb'], color='violet')","b1216a76":"# Look at ClaimNb distribution in the aggregated data\ndf_de_duped.ClaimNb.value_counts().sort_index(\n).to_frame(\"num_of_IDpol_unique\").rename_axis(\"ClaimNb\").style.format('{:,}')","dbf057fe":"# Investigate high number of claims on these particular aggregations\ndf_de_duped.query(\"num_of_rows == 15\")","d1191d26":"# Look at the one example that shows a *lot* of claims\nIDpol_unique_example = 313686\ndf_with_dups.loc[\n    df_with_dups.IDpol_unique == IDpol_unique_example,\n    [col for col in df_with_dups.columns if col not in expl_var_names]\n].style.bar(subset=['ClaimNb'], color='violet')","67ce5dd6":"# The next most extreme example\ndf_de_duped.query(\"ClaimNb == 24\")","e54ab872":"# Look at this example\nIDpol_unique_example = 314815\ndf_with_dups.loc[\n    df_with_dups.IDpol_unique == IDpol_unique_example,\n    [col for col in df_with_dups.columns if col not in expl_var_names]\n].style.bar(subset=['ClaimNb'], color='violet')","602c3ca6":"# Look at stats of number of claims and frequency per year for IDpol\ndisplay_side_by_side(\n    df_extra[['ClaimNb', 'freq_pyr']].describe(\n        percentiles=[.5, .9, .99, .999]).style.set_caption(\"All IDpol rows\"),\n    df_extra.query(\"ClaimNb > 0\")[['ClaimNb', 'freq_pyr']].describe(\n        percentiles=[.5, .9, .99, .999]).style.set_caption(\"Only rows with claims\"),\n    df_extra.ClaimNb.value_counts().sort_index(\n    ).to_frame(\"num_of_rows\").rename_axis(\"ClaimNb\").style.format('{:,}'),\n)","3e2d9bc0":"# Look at claim frequency\nassert (df_extra.freq_pyr == 0).sum() == 643953  # Check the number of zeros is consistent\ndf_extra.query(\"freq_pyr > 0.02\").freq_pyr.plot.hist(bins=50)\nplt.show()","26d1e1e3":"def get_bin_width_rdd(\n    lims_exact, nbins_target=30,\n    split_prop_candidates=None\n):\n    \"\"\"Get a sensible width of intervals to split a range given a target number of bins\n    lims_exact: tuple (min, max) of the range (with min < max)\n    nbins_target: positive int, number of intervals to target\n    split_prop_candidates: Defines what are 'sensible' width sizes. Can be:\n        - str: 'strict' or 'expanded' to choose pre-defined version\n        - None: same as 'strict'\n        - np.array of values between 0 and 1\n    Returns: The calculated sensible width\n    \"\"\"\n    split_prop_candidates_dict = {\n        'one_dp': np.array([.1, .2, .5]),\n        'strict': np.array([.1, .2, .25, .5]),\n        'expanded': np.concatenate((\n            np.arange(10, 40, 5) \/ 100, \n            np.arange(4, 6, 1) \/ 10, \n            np.array([.6, .8, 1.])\n        ))\n    }\n    if split_prop_candidates is None:\n        split_prop_candidates = 'strict'\n    if isinstance(split_prop_candidates, str):\n        split_prop_candidates = split_prop_candidates_dict[split_prop_candidates]\n    width_exact = (lims_exact[1] - lims_exact[0]) \/ float(nbins_target)\n    width_oom = np.ceil(np.log10(width_exact)).astype(np.int64)  # oom = order of magnitude\n    split_prop_best = split_prop_candidates[(np.abs(\n        split_prop_candidates - np.around(  # Attempt to avoid rounding instability\n            width_exact \/ (10. ** width_oom), 6\n        )\n    )).argmin()]\n    return(split_prop_best * (10. ** width_oom))\n\ndef get_breaks_rdd(\n    lims_exact, nbins_target=30, \n    boundary=0.,\n    split_prop_candidates=None\n):\n    \"\"\"Split a range by a sensible width intervals given a target number of bins\n    boundary: float, all breaks will be `boundary + width * n` for some n\n    Other arguments: see get_bin_width_rdd()\n    Returns: Equally spaced breaks\n    \"\"\"\n    width_rdd = get_bin_width_rdd(lims_exact, nbins_target, split_prop_candidates)\n    lims_expd = (\n        np.floor((lims_exact[0] - boundary) \/ width_rdd) * width_rdd + boundary,\n        np.ceil((lims_exact[1] - boundary) \/ width_rdd) * width_rdd + boundary\n    )    \n    return(np.arange(lims_expd[0], lims_expd[1] + width_rdd \/ 2, width_rdd))","2cfa2877":"df_extra.head()","46209e5f":"x_col = 'DrivAge'\nweight_col = 'Exp_4dps'\nline_cols = ['freq_pyr']","29ee707e":"x_sers = df_extra[x_col]\nbreak_points = get_breaks_rdd(\n    (x_sers.min(), x_sers.max()), nbins_target=30, split_prop_candidates='one_dp')\nbin_labels = [f'[{break_points[0]}, {break_points[1]}]'] + [\n    f'({lwr}, {upr}]' for lwr,upr in zip(break_points[1:-1], break_points[2:])]\ncut_obj = pd.cut(\n    x_sers, bins=break_points,\n    right=True, include_lowest=True, labels=None\n)","9367657f":"cols_needed = [x_col, weight_col] + line_cols\ndf_plot = df_extra.copy()[cols_needed].rename(columns={\n    weight_col: 'Weight'\n}).assign(num_of_rows=1)","5232618a":"agg_commands = {**{\n    'num_of_rows': 'size',\n    'Weight': 'sum',\n}, **{\n    line_col: ['mean', 'max', 'min'] for line_col in line_cols\n}}\ndf_plot_agg = df_plot.groupby(cut_obj).agg(agg_commands).set_index(\n    pd.IntervalIndex.from_breaks(break_points)\n).assign(\n    bin_labels=bin_labels,\n    left=lambda x: x.index.left,\n    right=lambda x: x.index.right,\n    mid=lambda x: x.index.mid,\n)\ndf_plot_agg.head()","293026d0":"# Import specific modules for this section\nfrom bokeh.layouts import gridplot\nfrom bokeh.plotting import figure, output_file, show, output_notebook\nfrom bokeh.models.ranges import Range1d\nfrom bokeh.models.axes import LinearAxis","a335b807":"bkplt = figure(\n    title=\"title\", x_axis_label=x_col, y_axis_label=\"Exposure (yrs)\", \n    tools=\"reset,box_zoom,pan,wheel_zoom,save\", background_fill_color=\"#fafafa\",\n    plot_width=800, plot_height=500\n)\nbkplt.quad(\n    top=df_plot_agg.Weight['sum'], bottom=0, left=df_plot_agg.left, right=df_plot_agg.right,\n    fill_color=\"khaki\", line_color=\"white\"#, alpha=0.5\n)\nbkplt.y_range=Range1d(0, df_plot_agg.Weight['sum'].max() \/ 0.5)\n\ny_range2_name = 'y_range2_name'\nbkplt.extra_y_ranges[y_range2_name] = Range1d(0, 0.6)\nax_new = LinearAxis(y_range_name=y_range2_name, axis_label=\"Response\")\nbkplt.add_layout(ax_new, 'right')\n\nfor line_col in line_cols:\n    bkplt.circle(\n        df_plot_agg.mid, df_plot_agg[line_col]['mean'], \n        color=\"purple\", size=4,\n        y_range_name=y_range2_name,\n    )\n    \nbkplt.grid.grid_line_color=\"white\"\nshow(bkplt)","c88139c0":"deps_sf = gpd.read_file(additional_data_folderpath \/ 'departements-version-simplifiee.geojson')\nassert deps_sf.shape == (96, 3)\nprint(\"Correct: Shape of the DataFrame is as expected\")","53c1d398":"# Look at first few rows\ndeps_sf.head()","7b69b562":"# Note that accents and hats on characters have loaded correctly\ndeps_sf.loc[[\n    region_name in [\"Bouches-du-Rh\u00f4ne\", \"Pyr\u00e9n\u00e9es-Orientales\", \"Finist\u00e8re\"] \n    for region_name in deps_sf.nom\n],:]","a4e5cefa":"# Check data types look OK\ndeps_sf.applymap(type).groupby(deps_sf.columns.tolist(), sort=False).size(\n).to_frame(\"num_of_rows\").rename_axis(\"data_type\", 1)","342dd6e1":"# In a notebook, a Polygon object automatically prints out\ndeps_sf.geometry[27]","1323bd8e":"regs_sf = gpd.read_file(additional_data_folderpath \/ 'regions-avant-redecoupage-2015.geojson')\nassert regs_sf.shape == (22, 3)\nprint(\"Correct: Shape of the DataFrame is as expected\")\nassert regs_sf.code.duplicated().sum() == 0\nprint(\"Correct: All values in the field 'code' are unique\")","f745f141":"# Look at first few rows\nregs_sf.head()","4ab10097":"# Load lookup tables for data\nreg_lookup_df = pd.read_csv(additional_data_folderpath \/ 'regions.csv', sep=';', encoding='latin1')\nassert reg_lookup_df.shape == (22, 3)\nprint(\"Correct: Shape of the DataFrame is as expected\")\nassert reg_lookup_df.isna().sum().sum() == 0\nprint(\"Correct: There are no missing values\")\nassert reg_lookup_df.Region.duplicated().sum() == 0\nprint(\"Correct: All values in the field 'Region' are unique\")","b6aca28f":"# Look at first few rows\nreg_lookup_df.head()","ead82e42":"# Merge the Regions lookup and sf tables\nreg_combined_df = regs_sf.merge(\n    reg_lookup_df.assign(code=lambda x: x.Region.str[1:]),\n    how='outer', on=['code']\n)\nassert reg_combined_df.isna().sum().sum() == 0\nprint(\"Correct: Every row has been matched exactly once\")","b5c48b06":"# We also see that any differences in `Name` and `nom` are explainable\nreg_combined_df.query(\"Name != nom\")[['Region', 'Name', 'nom']]","d6c8a8d8":"# Data fields available to color the chloropleth\nagg_region_df = df_extra.assign(num_of_rows=1).groupby('Region').agg({\n    'num_of_rows': 'size',\n    'Exp_4dps': 'sum',\n    'ClaimNb': 'sum',\n    'Density': 'mean',\n    'DrivAge': 'mean'\n}).reset_index()\nagg_region_df.head()","30973558":"# Plot chloropleth\nregs_chloro_plt = gv.Polygons(\n    reg_combined_df.merge(agg_region_df, on='Region'), \n    vdims=['nom','DrivAge']\n)\nregs_chloro_plt.opts(\n    width=520, height=500, toolbar='above', color_index='DrivAge',\n    colorbar=True, tools=['hover']#, aspect='equal'\n)","2fd8b5ca":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n# Resulting preprocessing\nThis section creates a function that implements the decisions that are justified below. We put this section here (and not at the end) so that the resulting DataFrame can be used throughout the analysis.","ca293565":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n# Load data\n## Small sample","3ed45af3":"<!-- This table of contents is updated *manually* -->\n# Contents\n1. [Setup](#Setup)\n1. [Load data](#Load-data): Small sample, Full dataset\n1. [Resulting preprocessing](#Resulting-preprocessing)\n1. [Data subsets for modelling](#Data-subsets-for-modelling)\n1. [Exploratory analysis](#Exploratory-analysis): \n    - [Identifier](#Identifier)\n    - [Exposure](#Exposure)\n    - [Explanatory variables: duplicates](#Explanatory-variables:-duplicates)\n    - [Response](#Response)\n    - [One-ways](#One-ways)\n1. [Geographic factors](#Geographic-factors)","0c0266e2":"**Question**: Are there underlying individuals who have multiple `IDpol`s?\n\n**Answer**: The data is inconclusive. For instance:\n- **In favour**: The Exposure distribution in the aggregated data shows a higher peak at 1, and less of a peak at one month. That might suggest that the \n- **Against**: The Exposure distribution in the aggregated data shows a higher peak at 2, but the explanatory variables include `DrivAge`, and you'd expect that to increase \n\n**Decision**: Decided *not* to use the aggregate data, and to stick with the original data, as identified by `IDpol`. However, when modelling, we should consider that multiple `IDpol`s could belong to the same individual, so they may not be independent samples. In fact, each `IDpol` should be considered a *policy period*, rather than a policy. \n\n*Further idea*: It might be interesting to test any model produced to see how it is affected by considering original (`IDpol`) or aggregated (`\u00ccDpol_unique`) data.","a494b295":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n## Explanatory variables: duplicates","a4335620":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n## Exposure\n`Exposure` = total exposure in yearly units","25fc370d":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n# Exploratory analysis","20443822":"**Summary**: The two `IDpol_unique`s (of `313686` and `314815`) are certainly outliers, and look very suspiciously like either poor data or fraudulent activity. Given the fields in the input data, we'll never be able to fully ascertain the true cause. The options are:\n1. Leave the data and hope any resulting model is unaffected. It may be a question of selecting a model specifically to be immune to this type of data (e.g. model claim propensity, and then number of claims). \n1. Remove one or both from the data.\n1. Keep them but manually alter the data, e.g. cap the number of claims on any `IDpol` row at 3 or 4, say.\n\n**Decision**: For the time being, I will leave them unaltered in the data (opion 1 above), but bear in mind when I come to look at frequency and considering model techniques. In fact, it'll be interesting to fit and assess any model with and without these extreme observations, to see what difference it makes.","d3582d0a":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n# Setup","a2f91dd9":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n## Response","17c9595f":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n# Data subsets for modelling","5eb0fb75":"**Decision**: Therefore, we choose to round Exposure to 4 dps so that:\n- The rounded and unrounded Exposure values round to the same value\n- Exposure values that represent x days in a year of 365 or 366 days round to the same value","32c35125":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n## Identifier","f43d92e1":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n## One-ways","29b6a9c4":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n# Geographic factors","3830dc7a":"**NOT COMPLETE**","957247dc":"## Full dataset","7f847704":"Observations: \n- A low proportion of policies have `Exposure` above 1. Could be poor data quality?\n- A significant number of policies have low `Exposure`, i.e. close to zero.\n- Range of values is reasonable.","23aa8f3e":"This is a *lot* of claims in one unique bucket. Even if you took out the one row with 16 claims, there is still 6, 8 and 9, which are very extreme values in the data. That is, even excluding the 16 row, we'd still have 32 in this bucket.","4789cc1d":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>","0f07da17":"# Exploratory data analysis of French motor insurance claims\nUsing the French Motor Claims dataset as an example to work through predictive modelling considerations. It is designed to be run on a Kaggle Kernel here: <https:\/\/www.kaggle.com\/btw78jt\/french-motor-claims-eda>"}}