{"cell_type":{"853aea53":"code","5cbdf155":"code","686078f9":"code","1e57204a":"code","512f7a52":"code","96dcbb2a":"code","4501553f":"code","365cd667":"code","6dcf01d2":"code","7cca273d":"code","8b884cd6":"code","fcfa045a":"code","8e65fe1b":"code","1cecb318":"code","57a7cc9d":"code","c1268801":"markdown","a024eca4":"markdown","730af649":"markdown","3e90fbcc":"markdown","b3d9bcfb":"markdown","4295daa2":"markdown","fb6effd7":"markdown","c5a4a0bc":"markdown","4657a14a":"markdown","7bbb2eb4":"markdown","00f70e69":"markdown","2a714b25":"markdown","be595032":"markdown"},"source":{"853aea53":"# importing torch \n# nn is a module in pytorch package which helps and provide us various \nimport torch\nimport torch.nn as nn \nimport numpy as np \nimport pandas as pd \n\n# train_test_split\n# TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# scipy to convert .. sparse matrix to dense \nimport scipy \n# improting optimizers \nfrom torch import optim\n\n# for ploting loss and train& val prediction accuracy \nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\ntorch.__version__","5cbdf155":"df1 = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')\ndf2 = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')\ndf1['label'] = 1\ndf2['label'] = 0\ndf = pd.concat([df1, df2], axis=0)\n\ndel df1 \ndel df2\ndf.head()","686078f9":"print(f'Shape of the dataset: {df.shape}')\nprint(f'\\nSum of nulls:\\n{df.isna().sum()}')","1e57204a":"df.info()","512f7a52":"# Text preprocessing \n\ndef normalise_text(text):\n    \n    text = text.str.lower()\n    text = text.str.replace(r\"\\#\", \"\" )\n    text = text.str.replace(r'http\\S+', \"URL\")\n    text = text.str.replace(r\"@\", \"\")\n    text = text.str.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\",\" \")\n    text = text.str.replace(\"\\s{2,}\",\" \")\n    \n    return text","96dcbb2a":"df['text']= df['title']+\" \"+df['text']","4501553f":"df['text'] = normalise_text(df['text'])\ndel df['title']\ndel df['subject']\ndel df['date']","365cd667":"# let's split the test and train \nX_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.3)","6dcf01d2":"# creating vectors for the sentence \nvectorizer = TfidfVectorizer(ngram_range=(1,1), max_features=10000)\n\n# Learn vocabulary from training texts and vectorize training texts.\nX_train = vectorizer.fit_transform(X_train)\n\n# Vectorize test texts.\nX_test = vectorizer.transform(X_test)","7cca273d":"#X_train = torch.tensor(scipy.sparse.csr_matrix.todense(X_train)).float()\n#X_test = torch.tensor(scipy.sparse.csr_matrix.todense(X_test)).float()\n\nX_train = torch.tensor(X_train.todense()).float()\nX_test = torch.tensor(X_test.todense()).float()","8b884cd6":"y_train = torch.tensor(y_train.values)\ny_test = torch.tensor(y_test.values)","fcfa045a":"model = nn.Sequential(\n                nn.Linear(X_train.shape[1],128 ),\n                nn.ReLU(),\n                nn.Dropout(0.1),\n                nn.Linear(128, df['label'].nunique()),\n                nn.LogSoftmax(dim=1)\n)\n","8e65fe1b":"# defining the loss \ncriterion = nn.NLLLoss()\n\n# Forward pass, get our logits\nlogps = model(X_train)\n\n# Calculate the loss with the logits and the labels\nloss = criterion(logps, y_train)\n\n\nloss.backward()\n\n# Optimizers require the parameters to optimize and a learning rate\noptimizer = optim.Adam(model.parameters(), lr=0.002)","1cecb318":"%%time\ntrain_losses = []\ntest_losses = []\ntest_accuracies = []\n\nepochs = 100\nfor e in range(epochs):\n    optimizer.zero_grad()\n\n    output = model.forward(X_train)\n    loss = criterion(output, y_train)\n    loss.backward()\n    train_loss = loss.item()\n    train_losses.append(train_loss)\n    \n    optimizer.step()\n    \n    \n    # Turn off gradients for validation, saves memory and computations\n    with torch.no_grad():\n        model.eval()\n        log_ps = model(X_test)\n        test_loss = criterion(log_ps, y_test)\n        test_losses.append(test_loss)\n\n        ps = torch.exp(log_ps)\n        top_p, top_class = ps.topk(1, dim=1)\n        equals = top_class == y_test.view(*top_class.shape)\n        test_accuracy = torch.mean(equals.float())\n        test_accuracies.append(test_accuracy)\n\n    model.train()\n    if (e+1)%10==0:\n        print(f\"Epoch: {e+1}\/{epochs}.. \",\n              f\"Training Loss: {train_loss:.3f}.. \",\n              f\"Test Loss: {test_loss:.3f}.. \",\n              f\"Test Accuracy: {test_accuracy:.3f}\")\n\n    ","57a7cc9d":"plt.figure(figsize=(12, 5))\nax = plt.subplot(121)\nplt.xlabel('epochs')\nplt.ylabel('negative log likelihood loss')\nplt.plot(train_losses, label='Training loss')\nplt.plot(test_losses, label='Validation loss')\nplt.legend(frameon=False);\nplt.subplot(122)\nplt.xlabel('epochs')\nplt.ylabel('test accuracy')\nplt.plot(test_accuracies);","c1268801":"# CONVERTING INTO TENSORS \n- Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model\u2019s parameters.\n\n- Tensors are similar to NumPy\u2019s ndarrays, except that tensors can run on GPUs or other specialized hardware to accelerate computing. If you\u2019re familiar with ndarrays, you\u2019ll be right at home with the Tensor API. If not, follow along in this quick API walkthrough.","a024eca4":"# \ud83d\ude0e\ud83d\ude0eDO UPVOTE FOR CHANDLER\ud83d\ude0e\ud83d\ude0e \n![Chandler](https:\/\/media.tenor.com\/images\/b7b886de0e04e771361f730414f52919\/tenor.gif)","730af649":"\n <img align=centre src = \"https:\/\/i.pinimg.com\/originals\/28\/18\/74\/2818749911c12b7f854d45e250c0b6d1.gif\" width=\"1000\" height=\"700\">\n\n","3e90fbcc":"# Training Loop","b3d9bcfb":"![PytorchGIF](https:\/\/images.ctfassets.net\/rc8q7tcpu9y3\/7JrqmBOgQcEQWGuKEiSuow\/4f2e59badb8cecb518fb38752feebf61\/Facebook-PyTorch-Conference-Experience-Design-Brand-Logo-Construction.gif)\nsource : https:\/\/images.ctfassets.net\/rc8q7tcpu9y3\/7JrqmBOgQcEQWGuKEiSuow\/4f2e59badb8cecb518fb38752feebf61\/Facebook-PyTorch-Conference-Experience-Design-Brand-Logo-Construction.gif","4295daa2":"# Loss, Optimzer","fb6effd7":"### What is torch?\nThe torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities.(copied from documentation) ","c5a4a0bc":"# TEXT CLASSIFICATION USING PYTORCH \n\n- As most of the users are prefering pytorch over tensorflow and I am also finding pytorch more of like a maths that we learn as basic of machine learning. In tensorflow the beauty of Maths is abstract, So I am here trying to understand\/Learn pytorch as much possible as I can. I am mostly working on TEXTUAL data. As my area of focus is NLP. \n\n- In this tutorial I am gonna build a text classfier using Linear Model. \n- I am also gonna keep updating in future notebooks . \n- I hope this will help anyone who's trying to use\/ learn pytorch. \n\n\n","4657a14a":"## Let's look in data\n\n- here we got the data in two different files. so I used df1 & df2 to import the data \n- then I used pandas concat() function doing axis=0 so the rows will get add from second dataset(df2)\n- deleted the df1 and df2 \n","7bbb2eb4":"# Train Test Splitting","00f70e69":"# Text preprocessing \n","2a714b25":"# Building Model ","be595032":"# Let's Plot the Learning and Loss Curve"}}