{"cell_type":{"271b6a6e":"code","634e774f":"code","c6801bf9":"code","ad39b376":"code","2145d5e4":"code","f734b17d":"code","e20cdf57":"code","c13f033c":"code","425e7a01":"code","6cbfd5b0":"code","4dd583c5":"code","63448533":"code","b355d182":"code","23fcb44a":"code","cc95b76f":"code","d457d54f":"code","3f6704ed":"code","f2b718c0":"code","452d9667":"code","eae914a3":"code","274b2988":"code","08a7201d":"code","1ff36b44":"code","c47d2563":"code","8534b7ba":"code","ad38a72f":"code","2c921b71":"code","5347cfa0":"code","0cda669d":"code","b0e41b30":"code","59272990":"code","53cab695":"code","c022d305":"code","497168a5":"code","0e63a3ad":"code","937975e3":"code","0a7192bf":"code","0bf6bed7":"code","ac0f8e43":"code","fd11d24d":"code","96e3ae23":"code","e41d4199":"code","e4d9d06c":"code","5ab38036":"code","b40e28a7":"code","4d25dbf3":"code","5d378cc4":"code","16adc3ff":"code","18e164ba":"code","11074c37":"code","a227218c":"code","71d5dc8e":"code","082d8daa":"code","34e6079c":"code","33718cc6":"code","40cfd0fa":"code","7542e6f0":"code","0d946a47":"code","20149a4c":"code","6d7243e1":"code","372b6f83":"code","7fd06e14":"code","19bd76c4":"code","da45ea5d":"code","08577460":"code","5886e010":"code","8826d923":"markdown","08b8be4e":"markdown","a42cf23a":"markdown"},"source":{"271b6a6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","634e774f":"train=pd.read_csv('..\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv')\ntest=pd.read_csv('..\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv')","c6801bf9":"import re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport warnings","ad39b376":"train.head()","2145d5e4":"test.head()","f734b17d":"train.info()","e20cdf57":"test.info()","c13f033c":"def remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n    return input_txt","425e7a01":"# remove twitter handles (@user)\ntrain['clean_tweet'] = np.vectorize(remove_pattern)(train['tweet'], \"@[\\w]*\")\ntrain.head()","6cbfd5b0":"train['clean_tweet'] = train['clean_tweet'].apply(lambda x: x.lower())\ntrain.head()","4dd583c5":"train['clean_tweet'] = train['clean_tweet'].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\ntrain.head(10)","63448533":"train['clean_tweet'] = train['clean_tweet'].apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ',x))\ntrain.head(10)","b355d182":"train['clean_tweet'] = train['clean_tweet'].apply(lambda x: re.sub(r'[^a-zA-Z]',' ',x))\ntrain.head(10)","23fcb44a":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize","cc95b76f":"# Creating token for the clean tweets\ntrain ['tweet_token'] = train['clean_tweet'].apply(lambda x: word_tokenize(x))\n\n## Fully formated tweets & there tokens\ntrain.head(10)","d457d54f":"stop_words = set(stopwords.words('english'))\nstop_words","3f6704ed":"train['tweet_token_filtered'] = train['tweet_token'].apply(lambda x: [word for word in x if not word in stop_words])\n\n## Tokens columns with stop words and without stop words\ntrain.head(10)","f2b718c0":"# Importing library for stemming\nfrom nltk.stem import PorterStemmer\nstemming = PorterStemmer()","452d9667":"# Created one more columns tweet_stemmed it shows tweets' stemmed version\ntrain['tweet_stemmed'] = train['tweet_token_filtered'].apply(lambda x: ' '.join([stemming.stem(i) for i in x]))\ntrain.head(10)","eae914a3":"# Importing library for lemmatizing\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlemmatizing = WordNetLemmatizer()","274b2988":"train['tweet_lemmatized'] = train['tweet_token_filtered'].apply(lambda x: ' '.join([lemmatizing.lemmatize(i) for i in x]))\ntrain.head(10)","08a7201d":"# Importing library\nfrom sklearn.feature_extraction.text import CountVectorizer\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\nbow_vectorizer","1ff36b44":"# bag-of-words feature matrix -\ntrainbow_stem = bow_vectorizer.fit_transform(train['tweet_stemmed'])\ntrainbow_stem","c47d2563":"trainbow_stem.toarray()","8534b7ba":"trainbow_lemm = bow_vectorizer.fit_transform(train['tweet_lemmatized'])\ntrainbow_lemm.toarray()","ad38a72f":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\ntfidf_vectorizer","2c921b71":"traintfidf_stem = tfidf_vectorizer.fit_transform(train['tweet_stemmed'])\ntraintfidf_stem.toarray()","5347cfa0":"traintfidf_lemm = tfidf_vectorizer.fit_transform(train['tweet_lemmatized'])\ntraintfidf_lemm.toarray()","0cda669d":"test","b0e41b30":"test['clean_tweet'] = np.vectorize(remove_pattern)(test['tweet'], \"@[\\w]*\")\ntest.head()","59272990":"test['clean_tweet'] = test['clean_tweet'].apply(lambda x: x.lower())\ntest.head(10)","53cab695":"test['clean_tweet'] = test['clean_tweet'].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\ntest.head()","c022d305":"test['clean_tweet'] = test['clean_tweet'].apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ',x))\ntest.head(10)","497168a5":"test['clean_tweet'] = test['clean_tweet'].apply(lambda x: re.sub(r'[^a-zA-Z]',' ',x))\ntest.head(10)","0e63a3ad":"test ['tweet_token'] = test['clean_tweet'].apply(lambda x: word_tokenize(x))\ntest.head(10)","937975e3":"\ntest['tweet_token_filtered'] = test['tweet_token'].apply(lambda x: [word for word in x if not word in stop_words])\ntest.head(10)","0a7192bf":"test['tweet_stemmed'] = test['tweet_token_filtered'].apply(lambda x: ' '.join([stemming.stem(i) for i in x]))\ntest['tweet_stemmed'].head(10)","0bf6bed7":"test['tweet_lemmatized'] = test['tweet_token_filtered'].apply(lambda x: ' '.join([lemmatizing.lemmatize(i) for i in x]))\ntest.head(10)","ac0f8e43":"from sklearn.feature_extraction.text import CountVectorizer\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\nbow_vectorizer","fd11d24d":"testbow_stem = bow_vectorizer.fit_transform(test['tweet_stemmed'])\ntestbow_stem.toarray()","96e3ae23":"testbow_lemm = bow_vectorizer.fit_transform(test['tweet_lemmatized'])\ntestbow_lemm.toarray()","e41d4199":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\ntfidf_vectorizer","e4d9d06c":"testtfidf_stem = tfidf_vectorizer.fit_transform(test['tweet_stemmed'])\ntesttfidf_stem.toarray()","5ab38036":"testtfidf_lemm = tfidf_vectorizer.fit_transform(test['tweet_lemmatized'])\ntesttfidf_lemm.toarray()","b40e28a7":"# Importing Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score","4d25dbf3":"X=traintfidf_lemm\ny=train['label']","5d378cc4":"xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=.3,random_state=42)","16adc3ff":"lr=LogisticRegression()\nlr.fit(xtrain,ytrain)","18e164ba":"predict_lr=lr.predict(xtest)","11074c37":"print(accuracy_score(predict_lr,ytest))\nprint(confusion_matrix(predict_lr,ytest))\nprint(classification_report(predict_lr,ytest))","a227218c":"svc=SVC()\nsvc.fit(xtrain,ytrain)\npredict_svc=svc.predict(xtest)","71d5dc8e":"print(accuracy_score(predict_svc,ytest))\nprint(confusion_matrix(predict_svc,ytest))\nprint(classification_report(predict_svc,ytest))","082d8daa":"nb=GaussianNB()\nnb.fit(xtrain.toarray(),ytrain)\npredict_nb=nb.predict(xtest.toarray())","34e6079c":"print(accuracy_score(predict_nb,ytest))\nprint(confusion_matrix(predict_nb,ytest))\nprint(classification_report(predict_nb,ytest))","33718cc6":"X1=traintfidf_stem\ny1=train['label']","40cfd0fa":"x1train,x1test,y1train,y1test=train_test_split(X1,y1,test_size=.3,random_state=42)","7542e6f0":"lr1=LogisticRegression()\nlr1.fit(x1train,y1train)","0d946a47":"predict_lr1=lr1.predict(x1test)","20149a4c":"print(accuracy_score(predict_lr1,y1test))\nprint(confusion_matrix(predict_lr1,y1test))\nprint(classification_report(predict_lr1,y1test))","6d7243e1":"svc1=SVC()\nsvc1.fit(x1train,y1train)\npredict_svc1=svc1.predict(x1test)","372b6f83":"print(accuracy_score(predict_svc1,y1test))\nprint(confusion_matrix(predict_svc1,y1test))\nprint(classification_report(predict_svc1,y1test))","7fd06e14":"nb1=GaussianNB()\nnb1.fit(x1train.toarray(),y1train)\npredict_nb1=nb1.predict(x1test.toarray())","19bd76c4":"print(accuracy_score(predict_nb1,y1test))\nprint(confusion_matrix(predict_nb1,y1test))\nprint(classification_report(predict_nb1,y1test))","da45ea5d":"test_predict_lr=lr.predict(testtfidf_lemm)\ntest_predict_lr","08577460":"test_predict_svc=svc.predict(testtfidf_lemm)\ntest_predict_svc","5886e010":"test_predict_nb=nb.predict(testtfidf_lemm.toarray())\ntest_predict_nb","8826d923":"# Test Data","08b8be4e":"---------------------------------------------------","a42cf23a":"---------------------------------------------------"}}