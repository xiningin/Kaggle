{"cell_type":{"6e6f94a0":"code","977a7613":"code","7f4c8efd":"code","807b772f":"code","eb702400":"code","b8cad30f":"code","1a783a42":"code","aa3f4b42":"code","6390fe14":"code","25ddae09":"code","1b8ff48d":"code","fccff082":"code","cb9850a6":"code","2d8adf83":"markdown","98a07d2b":"markdown","38a0b954":"markdown","bdfb8ea5":"markdown","29a3c5e4":"markdown","3fdd9544":"markdown","6b6057b0":"markdown","7c1f9036":"markdown","281bcef2":"markdown","a36862bf":"markdown","ac21f906":"markdown","828a0697":"markdown","09519175":"markdown","c3a9c622":"markdown","cc74bbfe":"markdown","16ec2141":"markdown"},"source":{"6e6f94a0":"# load some important modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# load the inspections data and split into train\/test\nd = pd.read_csv('..\/input\/cuny-data-challenge-2019\/inspections_train.csv', parse_dates=['inspection_date'])\nx_train, x_test = train_test_split(d, test_size=0.25)\n\n# load the venue data\nvenue_stats = pd.read_csv('..\/input\/cuny-data-challenge-2019\/venues.csv').set_index('camis')\n\n# load the violations data\nviolations = pd.read_csv('..\/input\/cuny-data-challenge-2019\/violations.csv', parse_dates=['inspection_date'])\n\n# merge the venue stats with the inspection data\nx_train = x_train.merge(venue_stats, 'left', left_on='camis', right_index=True)\nx_test = x_test.merge(venue_stats, 'left', left_on='camis', right_index=True)\n\n# create the violation count feature\nviolation_counts = violations.groupby(['camis', 'inspection_date']).size()\nviolation_counts = violation_counts.reset_index().set_index(['camis', 'inspection_date'])\nviolation_counts.columns = ['n_violations']\n\n# add the violation counts to the main data set\nx_train = x_train.merge(violation_counts, 'left', left_on=['camis', 'inspection_date'], right_index=True)\nx_test = x_test.merge(violation_counts, 'left', left_on=['camis', 'inspection_date'], right_index=True)\n\n# add the inspect \/ re-inspect feature to our train \/ test data\nx_train['re_inspect'] = x_train.inspection_type.str.contains('re-', regex=False, case=False).map(int)\nx_train['initial_inspect'] = x_train.inspection_type.str.contains('initial', regex=False, case=False).map(int)\n\nx_test['re_inspect'] = x_test.inspection_type.str.contains('re-', regex=False, case=False).map(int)\nx_test['initial_inspect'] = x_test.inspection_type.str.contains('initial', regex=False, case=False).map(int)\n\n# add our borough specific encoding\nboro_dict = {\n    'Missing': 0,\n    'STATEN ISLAND': 0,\n    'BROOKLYN': 1,\n    'MANHATTAN': 1,\n    'BRONX': 2,\n    'QUEENS': 2    \n}\n\nx_train['boro_idx'] = [boro_dict[_] for _ in x_train.boro]\nx_test['boro_idx'] = [boro_dict[_] for _ in x_test.boro]\n\n# add the inspection month\nx_train['inspection_month'] = (x_train.inspection_date.dt.strftime('%m').map(int) + 6) % 12\nx_test['inspection_month'] = (x_test.inspection_date.dt.strftime('%m').map(int) + 6) % 12\n\n# add the cuisine hitrates\ncuisine_hitrates = x_train.groupby(['cuisine_description']).agg({'passed':'mean', 'id':'count'}).\\\n        rename(columns={'id':'ct'}).sort_values('passed')[['passed']]\ncuisine_hitrates.columns = ['cuisine_hr']\n\nx_train = x_train.merge(cuisine_hitrates, 'left', left_on='cuisine_description', right_index=True)\nx_test = x_test.merge(cuisine_hitrates, 'left', left_on='cuisine_description', right_index=True)\n\nmodel_features = ['n_violations', 'inspection_month', 'cuisine_hr', 'boro_idx', 're_inspect', 'initial_inspect']","977a7613":"# we should now see a directory for our CUNY data as well as a new folder!\nos.listdir('..\/input\/')","7f4c8efd":"zip_data = pd.read_csv('..\/input\/zip-codes-and-stats\/medzip.csv').set_index('Zipcode')\nzip_data.Median.median()","807b772f":"zip_data.Median.head()","eb702400":"zip_data = pd.read_csv('..\/input\/zip-codes-and-stats\/medzip.csv', thousands=',').set_index('Zipcode')\nzip_data.columns = ['zip', 'med', 'population']\nzip_data.head()","b8cad30f":"x_train = x_train.merge(zip_data[['med', 'population']], 'left', left_on='zipcode', right_index=True)\nx_test = x_test.merge(zip_data[['med', 'population']], 'left', left_on='zipcode', right_index=True)","1a783a42":"new_features = model_features + ['med', 'population']\ntarget = ['passed']\nx_train[new_features + target].head()","aa3f4b42":"x_train[new_features].isna().sum()","6390fe14":"x_train['med'] = x_train.med.fillna(x_train.med.mean())\nx_train['population'] = x_train.population.fillna(x_train.population.mean())","25ddae09":"# for filter method\nfrom sklearn.feature_selection import f_classif, SelectKBest\n\n# for wrapper method\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import LogisticRegression\n\n# for embedded method\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\n\nnumeric_features = ['n_violations', 'cuisine_hr', 'med', 'population']\ncategorical_features = ['inspection_month', 'boro_idx', 're_inspect', 'initial_inspect']","1b8ff48d":"# create the selector object with scoring metric.\n# we are specifying the 4 best features here, but in reality we'd probably want fewer!\nselector = SelectKBest(f_classif, 4)\n\n# fit the selector on our numeric features and our target feature\n# note: 'ravel' is a way to flatten a dataframe into a vector\nselector.fit(x_train[numeric_features], x_train[target].values.ravel())\n\n# convert the scores into a dataframe object\nfeature_scores = pd.DataFrame({'features': numeric_features,\n                               'scores': selector.scores_})\nfeature_scores","fccff082":"# set up the model\nmod = LogisticRegression(solver='sag', penalty='l2', max_iter=500)\n\n# set up the selector\nselector = RFECV(mod, cv=5, scoring='neg_log_loss')\n\n# we're going to get a convergence warning, so we'll filter those out here\nimport warnings\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    # fit the selector\n    selector.fit(x_train[numeric_features + categorical_features], x_train[target].values.ravel())\n\n# convert the scores into a dataframe object\nfeature_scores = pd.DataFrame({'features': numeric_features + categorical_features,\n                               'scores': selector.grid_scores_*-1})\nfeature_scores.sort_values('scores')","cb9850a6":"mod = RandomForestClassifier(n_estimators=200, max_depth=8, min_samples_leaf=20)\nmod.fit(x_train[numeric_features + categorical_features], x_train[target].values.ravel())\n\nfeature_scores = pd.DataFrame({'features': numeric_features + categorical_features,\n                               'scores': mod.feature_importances_})\nfeature_scores.sort_values('scores', ascending=False)\n","2d8adf83":"Luckily, I went to the `pandas` [**documentation for the `read_csv()` function**](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html). Looks like there's a specific function argument, `thousands`, to tackle this issue. Looks like it's working now:","98a07d2b":"Let's load this data and find out the median of the `Median` field, which appears to be the median household income for each zipcode.","38a0b954":"#### Wrapper methods\n\nNext we'll try a feature subset search algorithm called [**Recursive Feature Elimination**](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV). We're going to be using a variant of it called **RFECV** that applies the algorithm with cross-validation automatically. If you're not familiar with cross-validation, check out the kernel on it; it's really important!","bdfb8ea5":"### Let's use MORE data!\nI wanted to see if we could potentially enrich our analysis with some additional information, so I clicked on the **\"+ Add Dataset\"** button at the top of this screen and typed in \"NYC data\". I found some open source IRS data that another user had already uploaded with a \"Public Domain\" license. It looks reasonable- check it out [**here**](https:\/\/www.kaggle.com\/jakerohrer\/zip-codes-and-stats\/).\n\nI added this data set to this kernel, and now it appears in my `input` folder. Take a look:","29a3c5e4":"### Creating a starting feature set\nFirst, we're going to repeat many of the steps from [**kernel_3**](https:\/\/www.kaggle.com\/nicknormandin\/cuny-data-challenge-kernel3). If this looks new, I recommend starting with the kernel instead.","3fdd9544":"#### Embedded methods\n\nWe'll try training a `RandomForestClassifier` on the data next. This model is fairly standard for embedded feature selection because the construction process involves repeated random sampling of features and bootstrapping of observations. ","6b6057b0":"Now that we have well-behaved zip data, let's merge it with our training data. We'll merge on zip code, and we really only want the median income and population data.","7c1f9036":"It looks like `n_violations` is by far our most important feature. The `med` and `population` features we recently added might not even be important at all.","281bcef2":"### Feature importance\n\nThere are many ways of determining the value of a feature, but we can separate them broadly into a few categories.\n\n- **filter methods**: This is a form of *univariate* feature selection (meaning we only look at one feature at a time). In using a filter method, we'll evaluate a feature and the value we're trying to predict using a scoring method (eg: evaluating the relationship between `n_violations` and `passed`).\n- **wrapper methods**: A wrapper method is a *multivariate* selection methodology where we follow a series of steps to train a model on a set of features and then add or remove features to improve our loss function.\n- **embedded methods**: Embedded methods are also *multivariate*, and leverage the fact that some machine learning models will attempt to *do their own feature selection*. We can evaluate feature importance using one of these models (and maybe even use it with our other models!)\n\nNow let's import some tools, and then separate our features into numeric and categorical types.","a36862bf":"## tutorial on feature importance, feature selection methods, and finding alternative features!\nIn this notebook we're going to:\n\n1. Build a feature set using data provided in the competition\n2. Find some new data and enrich our analysis with it\n3. Try to figure out which features are helpful and which aren't\n4. Discuss methods for determining a good 'feature set'\n\n","ac21f906":"Let's add the new features to our list and check out our dataframe.","828a0697":"Oh no- that broke! How come? The error message says it couldn't convert string to float... why is that? Take a look at the `Median` field. That looks like a comma in the value, and the `dtype` listed says `object` when we should really be seeing a numeric value.","09519175":"### Summary\nIt's  clear that each of our feature selection methods yielded slightly different results, but we're able to see some patterns emerging. This part of data science is really more art than science; you should work to blend your understanding of the feature importance scores with your knowledge of the domain. If a feature you think should be important isn't getting a high score, are you sure you're encoding it properly? Should your normalize it somehow? Should you remove outliers? Is it in only important in *combination* with some other feature or aspect of the problem?\n\nIn addition to locating your most useful features, you should also use this as an opportunity to remove your features that don't look useful- especially if this conforms to your view of the problem.|","c3a9c622":"#### Filter methods\n\nFor now, let's just focus on the numeric features. This problem contains a lot of categorical variables, and there's no one right way to handle those. There are many options, which means you get to be creative in how you communicate the value of those features to the model.\n\nFirst, let's use `SelectKBest` to pick to rank our features according to the `f_classif` score.","cc74bbfe":"Whenever we add new features, it's import to evaluate them to make sure we don't have missing values or outliers. Let's take a look","16ec2141":"Ok, it looks like we're missing some zip codes. It's only about 3\\% of our data, but we shouldn't drop it unless we have to. Handling missing data is an important step and there isn't always a right answer. In this case, let's fill them with the average values for each field."}}