{"cell_type":{"f4de8554":"code","f76ca5e8":"code","06c5d378":"code","a49f7510":"code","20f9a0d3":"code","d093811f":"code","ea68969e":"code","f144430c":"code","91de7592":"code","61bdb854":"code","3b4fdaa1":"code","f7abd533":"code","e653e9ac":"code","c89c44bc":"code","55ab3086":"code","280e9f4d":"code","d3d13012":"code","5d422b6c":"code","b50d1baa":"code","904f2959":"code","6bf20bcc":"code","cdd83629":"code","fc881570":"code","0ce79b5d":"code","129762c7":"code","7c90640d":"code","4cf6c324":"code","1678f082":"code","b3103a9d":"code","40b568ed":"code","ee9084d8":"code","97eea76c":"code","9e0f2dc5":"code","a1df5cd2":"code","02426d20":"code","f8b81136":"code","e12e93da":"markdown","17ae1e66":"markdown","619a9825":"markdown","6caae95a":"markdown","37e6060d":"markdown","604c0cc7":"markdown","430bcbf0":"markdown","416cfe25":"markdown","0af7beb0":"markdown","28ab4c30":"markdown","89fea4bb":"markdown","98dcd03a":"markdown","5991ab38":"markdown"},"source":{"f4de8554":"!pip -q install pyspellchecker optuna","f76ca5e8":"import os\nimport optuna\nimport pandas as pd\nimport numpy as np\nimport random\nimport re\nfrom scipy import sparse\nfrom spellchecker import SpellChecker\nimport string\nimport warnings\n\nfrom IPython.display import HTML\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score","06c5d378":"optuna.logging.set_verbosity(0)\nwarnings.filterwarnings(\"ignore\")","a49f7510":"sns.set_style(\"darkgrid\")","20f9a0d3":"SEED = 42\nNUM_SPLITS = 10\nNUM_TRIALS = 100","d093811f":"# setting seed\nos.environ['PYTHONHASHSEED']=str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)","ea68969e":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","f144430c":"_ = plt.figure(figsize=(6, 4))\nsns.countplot(x=\"target\", data=train)\n_ = plt.title(\"Target Distribution\")","91de7592":"test[\"target\"] = -1\ndf = pd.concat([train, test])","61bdb854":"print(\"NaN Distribution\\n\")\nfor col in df.columns:\n    print(f\"{col}: {((df[col].isna().sum()\/df.shape[0])*100):.2f}\")","3b4fdaa1":"HTML('<div style=\"position:relative;height:0;padding-bottom:56.25%\"><iframe src=\"https:\/\/youtu.be\/3w92peJtYNQ\" width=\"640\" height=\"360\" frameborder=\"0\" style=\"position:absolute;width:100%;height:100%;left:0\" allowfullscreen><\/iframe><\/div>')","f7abd533":"# loading augmented training data\ntrain = pd.read_csv(\"..\/input\/twitter-sentiment-easy-data-augmentation\/train_augmented.csv\")","e653e9ac":"_ = plt.figure(figsize=(6, 4))\nsns.countplot(x=\"target\", data=train)\n_ = plt.title(\"Augmented Data - Target Distribution\")","c89c44bc":"df = pd.concat([train, test])","55ab3086":"train.shape, test.shape, df.shape","280e9f4d":"df[\"text\"] = df[\"text\"].str.lower()","d3d13012":"PUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndf[\"text\"] = df[\"text\"].apply(lambda text: remove_punctuation(text))","5d422b6c":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndf[\"text\"] = df[\"text\"].apply(lambda text: remove_emoji(text))","b50d1baa":"def remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    try:\n        return url_pattern.sub(r'', text)\n    except:\n        print(text)\n    \ndf[\"text\"] = df[\"text\"].apply(lambda text: remove_urls(text))","904f2959":"def remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\ndf[\"text\"] = df[\"text\"].apply(lambda text: remove_html(text))","6bf20bcc":"with open(\"..\/input\/slangtext\/slang.txt\", \"r\") as file:\n    chat_words_str = file.read()\n\nchat_words_map_dict = {}\nchat_words_list = []\nfor line in chat_words_str.split(\"\\n\"):\n    if line != \"\" and \"=\" in line:\n        cw = line.split(\"=\")[0]\n        cw_expanded = line.split(\"=\")[1]\n        chat_words_list.append(cw)\n        chat_words_map_dict[cw] = cw_expanded\nchat_words_list = set(chat_words_list)","cdd83629":"def chat_words_conversion(text):\n    new_text = []\n    for w in text.split():\n        if w.upper() in chat_words_list:\n            new_text.append(chat_words_map_dict[w.upper()])\n        else:\n            new_text.append(w)\n    return \" \".join(new_text)\n\ndf[\"text\"] = df[\"text\"].apply(lambda text: chat_words_conversion(text))","fc881570":"spell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n\ndf[\"text\"] = df[\"text\"].apply(lambda text: chat_words_conversion(text))","0ce79b5d":"class NBSVMClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, C=1.0, max_iter=100, dual=False, n_jobs=1):\n        self.C = C\n        self.dual = dual\n        self.n_jobs = n_jobs\n        self.max_iter = max_iter\n\n    def predict(self, x):\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict(x.multiply(self._r))\n\n    def fit(self, x, y):\n        y = y.values\n        x, y = check_X_y(x, y, accept_sparse=True)\n\n        def pr(x, y_i, y):\n            p = x[y==y_i].sum(0)\n            return (p+1) \/ ((y==y_i).sum()+1)\n\n        self._r = sparse.csr_matrix(np.log(pr(x,1,y) \/ pr(x,0,y)))\n        x_nb = x.multiply(self._r)\n        self._clf = LogisticRegression(C=self.C, dual=self.dual, \n                                       max_iter=self.max_iter, \n                                       n_jobs=self.n_jobs).fit(x_nb, y)\n        return self","129762c7":"X_train, X_valid, y_train, y_valid = train_test_split(train[\"text\"], train[\"target\"],\n                                                      test_size=0.2, random_state=SEED,\n                                                      stratify=train[\"target\"])","7c90640d":"vec = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.9, \n                      strip_accents='unicode', use_idf=1,\n                      smooth_idf=1, sublinear_tf=1)","4cf6c324":"def objective(trial):\n    C = trial.suggest_float(name=\"C\", low=1e-3, high=1e3, log=True)\n    max_iter = trial.suggest_discrete_uniform(name=\"max_iter\", low=50, high=500, q=50)\n    nbsvm = NBSVMClassifier(C=C, max_iter=max_iter)\n    \n    train_term_doc = vec.fit_transform(X_train)\n    valid_term_doc = vec.transform(X_valid)\n    nbsvm.fit(train_term_doc, y_train)\n    \n    preds = nbsvm.predict(valid_term_doc)\n    preds[preds>=0.5] = 1\n    preds[preds<0.5] = 0\n    \n    acc = accuracy_score(y_valid, preds)\n    return acc","1678f082":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=NUM_TRIALS, show_progress_bar=True)","b3103a9d":"print(f\"Best Value: {study.best_trial.value}\")\nprint(f\"Best Params: {study.best_params}\")","40b568ed":"kwargs = study.best_params","ee9084d8":"train = df[df['target']!=-1]\ntest = df[df['target']==-1]","97eea76c":"def print_metrics(y_true, y_pred):\n    print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n    print(f\"MCC: {matthews_corrcoef(y_true, y_pred)}\")\n    print(f\"F1: {f1_score(y_true, y_pred)}\\n\")","9e0f2dc5":"final_preds = np.zeros((len(test)))\nkfold = StratifiedKFold(n_splits=NUM_SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (train_index, valid_index) in enumerate(kfold.split(train[\"text\"], train[\"target\"])):\n    print(\"*\"*60)\n    print(\"*\"+\" \"*26+f\"FOLD {fold+1}\"+\" \"*26+\"*\")\n    print(\"*\"*60, end=\"\\n\")    \n    \n    X_train = train.iloc[train_index, :].reset_index(drop=True)\n    X_valid = train.iloc[valid_index, :].reset_index(drop=True)\n    \n    y_train = X_train['target']\n    y_valid = X_valid['target']\n    \n    train_term_doc = vec.fit_transform(X_train[\"text\"])\n    valid_term_doc = vec.transform(X_valid[\"text\"])\n    test_term_doc = vec.transform(test[\"text\"])\n    \n    # using best hyperparameters selected above\n    nbsvm = NBSVMClassifier(**kwargs)\n    nbsvm.fit(train_term_doc, y_train)\n    \n    valid_preds = nbsvm.predict(valid_term_doc)\n    print_metrics(y_valid, valid_preds)\n    \n    test_preds = nbsvm.predict(test_term_doc)\n    final_preds += test_preds","a1df5cd2":"submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","02426d20":"submission[\"target\"] = final_preds\/NUM_SPLITS\nsubmission[\"target\"] = submission[\"target\"].apply(lambda x: 1 if x>=0.5 else 0)","f8b81136":"submission.to_csv(\"submission.csv\", index=False)","e12e93da":"## [Optuna Hyperparameter Tuning](https:\/\/optuna.readthedocs.io\/en\/stable\/)\n\nOptuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. Optuna enjoys high modularity, and the user of Optuna can dynamically construct the search spaces for the hyperparameters.\n\nOptuna has modern functionalities as follows -\n* Lightweight, versatile, and platform agnostic architecture\n* Parallel distributed optimization\n* Pruning of unpromising trials","17ae1e66":"## Easy Data Augmentation(EDA)\n\nEDA provides techniques for boosting performance on text classification tasks and was introduced in [this paper](https:\/\/arxiv.org\/abs\/1901.11196). This technique showed improvements on five NLP classification tasks, with substantial improvements on datasets of size `N < 500` i.e number of training samples < 500. The authors found that simple text editing operations using EDA result in good performance gains. It performs the following operations -\n\n* **Synonym Replacement (SR):** Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.\n* **Random Insertion (RI):** Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times.\n* **Random Swap (RS):** Randomly choose two words in the sentence and swap their positions. Do this n times.\n* **Random Deletion (RD):** For each word in the sentence, randomly remove it with probability p.\n\n***Example*** -\n* Original - A sad, superior human comedy played out on the back roads of life.\n* SR - A *lamentable*, superior human comedy played out on the *backward* road of life.\n* RI - A sad, superior human comedy played out on *funniness* the back roads of life.\n* RS - A sad, superior human comedy played out on *roads* back *the* of life.\n* RD - A sad, superior human out on the roads of life.\n\nHenry AI labs has provided a detailed explanation for this augmentation technique on his youtube channel [here](https:\/\/youtu.be\/3w92peJtYNQ).","619a9825":"Location has about 1\/3rd data empty.","6caae95a":"## Importing Libraries","37e6060d":"## Submission","604c0cc7":"If you find my kernel helpful, kindly consider upvoting it!!","430bcbf0":"## Text Cleaning\n\nTaken from SRK's excellent [kernel](https:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-text-preprocessing)\n* Lowercasing\n* Removing Punctuation\n* Removing Emojis\n* Remove URLs\n* Remove HTML tags\n* Chat words conversion\n* Spelling Correction","416cfe25":"## Training","0af7beb0":"## NBSVM\n\nTaken from Alex Sanchez's comment in Jeremy's [NB-SVM strong linear baseline](https:\/\/www.kaggle.com\/jhoward\/nb-svm-strong-linear-baseline) kernel","28ab4c30":"Convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way. Not helpful for sentiment classification. Since Jeremy uses TF-IDF for his NB-SVM classifier, we can lowercase text","89fea4bb":"## Constants","98dcd03a":"To use this technique you only need to take care of 2 hyperparameters -\n* `alpha` - percent of words in the sentence that will be changed (default is 0.1 or 10%)\n* `num_aug` - number of generated augmented sentences per original sentence (default is 9)\n\nThe authors suggest the following -\n\n| Ntrain | alpha | num_aug |\n|--------|-------|---------|\n| 500  | 0.05 | 16 |\n| 2000 | 0.05 | 8 |\n| 5000 | 0.1 | 4 |\n| More | 0.1 | 4 |\n\n\nOfficial implementation of EDA is available on [github](https:\/\/github.com\/jasonwei20\/eda_nlp)\n\nUsing the same technique, I have created an augmented dataset & shared publicly.\n* [Twitter Sentiment - Easy Data Augmentation]()\n\nTo generate the augmented training data, I've used `alpha=0.1` and `num_aug=4`","5991ab38":"## Basic Exploration"}}