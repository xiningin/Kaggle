{"cell_type":{"1b36653d":"code","d1259160":"code","12c0cf9f":"code","59a884db":"code","c2c129a0":"code","23f121fa":"code","61a99846":"code","93240d66":"code","73f68395":"code","b0417f33":"code","f10d057a":"code","a52fc710":"code","05282c63":"code","4db36fed":"code","9bd3024e":"code","e7297e74":"code","8a99690e":"code","cfca3f74":"code","21ca15c8":"code","a3167bcf":"code","e4073e54":"code","c13a293f":"code","2021ba20":"code","68dd1aef":"code","50d3f11b":"code","5a05450c":"code","6e66e4b8":"code","033f1391":"code","01d165f8":"code","0df4362a":"code","1f3da273":"code","90c78353":"code","3830567b":"code","3fb71647":"code","1c281dad":"code","54e18f01":"code","b82939a4":"code","22d3cc50":"code","3772fe98":"code","a4de2e12":"code","ed5b8810":"code","457166e5":"code","585fefe1":"code","1d1f9c68":"code","d1785016":"code","defa96ac":"code","cb935243":"code","a4c68218":"code","cc9a7909":"code","d5eae0e9":"code","9ab3440f":"code","cc4903ec":"code","248e7188":"code","4fc2343a":"code","48b8d0f0":"code","fa086dbd":"code","00bbba10":"code","b496b6a8":"code","513f5130":"code","2cbf605f":"code","c58d364f":"markdown","5944ecda":"markdown","aa97316f":"markdown","ad751dd1":"markdown","13469742":"markdown"},"source":{"1b36653d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score as cv\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.metrics import f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1259160":"train_df = pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","12c0cf9f":"train_df.info()","59a884db":"test_df.info()","c2c129a0":"d = {'na_count':train_df.isna().sum()}\ntrain_na = pd.DataFrame(data =d)\ntrain_na['percentage'] = train_df.isna().sum()\/19158 * 100\ntrain_na","23f121fa":"sns.heatmap(train_df.isnull(), cbar=False)","61a99846":"d = {'na_count':test_df.isna().sum()}\ntest_na = pd.DataFrame(data=d)\ntest_na['percentage'] = test_df.isna().sum() \/ 2129 * 100\ntest_na","93240d66":"sns.heatmap(test_df.isnull(), cbar=False)","73f68395":"train_df.head()","b0417f33":"train_df.last_new_job.unique()","f10d057a":"train_df.company_type.unique()","a52fc710":"train_df.company_size.unique()","05282c63":"train_df.city.unique()","4db36fed":"# Firstly, we observed that the city column has so many unique value and problem can be occur during one hot encoder process","9bd3024e":"train_df = train_df.drop('city',axis=1)\ntest_df = test_df.drop('city',axis=1)","e7297e74":"# If any column has more than 15% Null values, it might be drop\ntrain_df = train_df.drop(['company_size','company_type'],axis=1)\ntest_df = test_df.drop(['company_size','company_type'],axis=1)\n","8a99690e":"def distribution_plot(data,column):\n    sns.countplot(data=data, x=column)\n    plt.show()","cfca3f74":"object_list = list(train_df.select_dtypes(include=['object']).columns)\nobject_list\nfor i in object_list:\n    distribution_plot(train_df,i)","21ca15c8":"train_df['relevent_experience'] = train_df['relevent_experience'].replace({'Has relevent experience':1,'No relevent experience':0})\ntest_df['relevent_experience'] = test_df['relevent_experience'].replace({'Has relevent experience':1,'No relevent experience':0})","a3167bcf":"train_df['last_new_job'] = train_df['last_new_job'].replace({'never':0,'>4':5}).astype('float')\ntest_df['last_new_job'] = test_df['last_new_job'].replace({'never':0,'>4':5}).astype('float')","e4073e54":"# We have handled information regarding experience and last_new_job, how often they are changing job ?\ntrain_df['experience']= train_df['experience'].replace({'<1':0,'>20':21}).astype('float')\ntest_df['experience'] = test_df['experience'].replace({'<1':0,'>20':21}).astype('float')\n\ntrain_df['experience_per_job'] = train_df['experience'] \/ [x + 1 for x in train_df['last_new_job']]\ntest_df['experience_per_job'] = test_df['experience'] \/ [x + 1 for x in test_df['last_new_job']]","c13a293f":"numerical_cols = test_df.select_dtypes(exclude = ['object']).columns\nnumerical_cols = numerical_cols[1:-1]\ncategorical_cols = test_df.select_dtypes(include = ['object'] ).columns\n\nimp_mean_numerical = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp_most_frequent_categorical = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n\nnumerical_transformer_imputer = imp_mean_numerical\n\ncategorical_transformer_simple = Pipeline(steps=[\n    ('imputer',imp_most_frequent_categorical),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\ndata_transformer_simple = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer_imputer, numerical_cols),\n        ('cat', categorical_transformer_simple, categorical_cols)\n    ])\n\n","2021ba20":"test_df = test_df.iloc[:,1:]\ntest_df","68dd1aef":"X = train_df.loc[:,train_df.columns != 'target']\ny = train_df.loc[:,train_df.columns == 'target']","50d3f11b":"X = X.drop('enrollee_id',axis=1)","5a05450c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","6e66e4b8":"target_pipeline_xgbclas = Pipeline(steps=[\n                                    ('preprocessor',data_transformer_simple),\n                                    ('model',XGBClassifier())\n])","033f1391":"params = {\n    'model__colsample_bytree': [0.3, 0.7],\n    'model__n_estimators': [25,50,100],\n    'model__max_depth': range(4,8),\n    \n}","01d165f8":"randomized_model = RandomizedSearchCV(target_pipeline_xgbclas,params,cv=3,n_jobs=-1,verbose=2)","0df4362a":"randomized_model.fit(X_train,y_train)","1f3da273":"print(\"Best parameters found: \", randomized_model.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_model.best_score_)))","90c78353":"cv_results = cv(randomized_model.best_estimator_,X,y,cv=5)","3830567b":"print('XGBOOST Mean of Cross validation is = ' + str(cv_results.mean()))","3fb71647":"test_predictions = randomized_model.best_estimator_.predict(X_test)\n\n# Create and print the confusion matrix\ncm = confusion_matrix(y_test, test_predictions)\nprint(cm)\n\n# Print the true positives (actual 1s that were predicted 1s)\nprint(\"The XGBOOST number of true positives is: {}\".format(cm[1,1]))","1c281dad":"score = precision_score(y_test, test_predictions)\n\n# Print the final result\nprint(\"The XGBOOST precision value is {0:5f}\".format(score))","54e18f01":"# Check train and test mean absolute error, if there is equality there is no underfitting\n\nprint('The training error is {0:.5f}'.format(\n  mae(y_train, randomized_model.best_estimator_.predict(X_train))))\nprint('The XGBOOST testing error is {0:.5f}'.format(\n  mae(y_test, randomized_model.best_estimator_.predict(X_test))))","b82939a4":"# Let the check f1 score \nprint('The XGBOOST f1_score  is {0:.5f}'.format(f1_score(y_test,test_predictions)\n  ))","22d3cc50":"target_pipeline_rf = Pipeline(steps=[\n                                    ('preprocessor',data_transformer_simple),\n                                    ('model',RandomForestClassifier())\n])\n\n","3772fe98":"rf_params = {'model__max_depth' :[2,5,8,19],\n            'model__max_features':[2,5,8],\n            'model__n_estimators':[10,500,1000],\n            'model__min_samples_split':[2,5,10]}","a4de2e12":"random_forest_randomized = RandomizedSearchCV(target_pipeline_rf,rf_params,cv=3,n_jobs=-1,verbose=2)","ed5b8810":"random_forest_randomized.fit(X_train,y_train)","457166e5":"print(\"Best parameters found: \", random_forest_randomized.best_params_)\nprint(\"Lowest  rmse found: \", np.sqrt(np.abs(random_forest_randomized.best_score_)))","585fefe1":"cv_results = cv(random_forest_randomized.best_estimator_,X,y,cv=10)","1d1f9c68":"print('RandomForest Mean of Cross validation is = ' + str(cv_results.mean()))","d1785016":"test_predictions = random_forest_randomized.best_estimator_.predict(X_test)\n\n# Create and print the confusion matrix\ncm = confusion_matrix(y_test, test_predictions)\nprint(cm)\n\n# Print the true positives (actual 1s that were predicted 1s)\nprint(\"The RandomForest number of true positives is: {}\".format(cm[1,1]))","defa96ac":"score = precision_score(y_test, test_predictions)\n\n# Print the final result\nprint(\"The RandomForest precision value is {0:.5f}\".format(score))","cb935243":"# Check train and test mean absolute error, if there is equality there is no underfitting\n\nprint('The training error is {0:.5f}'.format(\n  mae(y_train, random_forest_randomized.best_estimator_.predict(X_train))))\nprint('The RandomForest testing error is {0:.5f}'.format(\n  mae(y_test, random_forest_randomized.best_estimator_.predict(X_test))))","a4c68218":"# Let the check f1 score \nprint('The RandomForest f1_score  is {0:.5f}'.format(f1_score(y_test,test_predictions)\n  ))","cc9a7909":"# XGboost and RandomForest looks very similar","d5eae0e9":"target = randomized_model.best_estimator_.predict(test_df).astype('int')","9ab3440f":"test_df = pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","cc4903ec":"test_df['target'] = target","248e7188":"submission_csv =test_df.iloc[:,[0,-1]]","4fc2343a":"submission_csv.to_csv(\"\/kaggle\/working\/submission_csv\", index = False)","48b8d0f0":"print(submission_csv.shape)\nprint(submission_csv.isnull().sum())","fa086dbd":"submission_csv\n","00bbba10":"feature_importance = randomized_model.best_estimator_._final_estimator.feature_importances_\nfeature_imp = pd.DataFrame(sorted(zip(feature_importance,X.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(50, 40))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('GBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()","b496b6a8":"feature_imp = feature_imp.rename(columns={'Value':'Percentage'})","513f5130":"feature_imp","2cbf605f":"# As you know we created new column As 'experience_per_job',then it become our 3rd important feature","c58d364f":"**RandomForest**","5944ecda":"**Preprocessing**","aa97316f":"**Pipeline**","ad751dd1":"**XGBOOST**","13469742":"**Introduction**"}}