{"cell_type":{"265ef308":"code","b45d15d6":"code","1aa6ff63":"code","e0ef815d":"code","198d15fc":"code","5af9f6bb":"code","a9a1d9d4":"code","c9188731":"code","41592368":"code","385155a3":"code","52ac6ac5":"code","631ac144":"code","dbe0a228":"code","2164f4ad":"code","dc2e2111":"code","b35be66a":"code","c32669e6":"code","318c264c":"code","bc096e0b":"code","1663141c":"code","363f77c4":"code","db16eb7e":"code","c2f9a7ec":"code","b9ff9420":"code","d46b341a":"code","256f0900":"code","c9f39e98":"code","a487925f":"code","d98fae71":"code","41d0c9d0":"code","96591e1f":"code","f57d3c12":"code","3015dc7f":"code","2713fd8f":"code","2a493f71":"code","5fca8605":"code","b82c9def":"code","8d86955a":"code","ec5478c1":"code","5601d43d":"code","8ce93cf5":"code","48d53022":"code","a7ef52fb":"markdown","299d9f45":"markdown","446cb216":"markdown","3e33f028":"markdown","9dc80fce":"markdown","c244c3fe":"markdown","4f965301":"markdown","c14dd564":"markdown","cf3f6d9f":"markdown","ca6d3e39":"markdown","c875619e":"markdown","9ef62666":"markdown","d2393e1b":"markdown","be7a0127":"markdown","dde113e3":"markdown","f40615e4":"markdown","864a25cb":"markdown","33db91b0":"markdown"},"source":{"265ef308":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b45d15d6":"import pandas as pd\nimport numpy as np\nimport spacy\nimport re\nimport string\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom warnings import filterwarnings\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom wordcloud import WordCloud\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom textblob import TextBlob","1aa6ff63":"filterwarnings(action='ignore')","e0ef815d":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","198d15fc":"train_df","5af9f6bb":"train_df.info()","a9a1d9d4":"train_df.describe()","c9188731":"# num of missing values\n\nprint('num of NaNs in the keywords: ', train_df.keyword.isna().sum())\nprint('num of NaNs in the locations: ', train_df.location.isna().sum())\nprint('num of NaNs in the text: ', train_df.text.isna().sum())","41592368":"for df in [train_df, test_df]:\n    df['has_location'] = 1\n    df.loc[(df['location'].isna()), 'has_location'] = 0\n\n    df['has_keyword'] = 1\n    df.loc[(df['keyword'].isna()), 'has_keyword'] = 0","385155a3":"# target distribution\n\nresults = train_df.groupby('target', as_index=False)['id'].count()\ng = sns.barplot(['Not Disaster', 'Disaster'], results['id'], palette='muted')\ng.set(ylabel='count', title='target')","52ac6ac5":"# non disasters\n\ng = sns.distplot(train_df[train_df['target'] == 0]\n                 ['text'].str.replace(' ', '').map(lambda x: len(x)))\ng.set(title='Non Disaster Tweets', xlabel='Character Count')\nplt.show()","631ac144":"# non disasters\n\ng = sns.distplot(train_df[train_df['target'] == 1]\n                 ['text'].str.replace(' ', '').map(lambda x: len(x)))\ng.set(title='Disaster Tweets', xlabel='Character Count')\nplt.show()","dbe0a228":"# non disasters\n\ng = sns.distplot(train_df[train_df['target'] == 0]\n                 ['text'].str.split().map(lambda x: len(x)))\ng.set(title='Non Disaster Tweets', xlabel='Word Count')\nplt.show()","2164f4ad":"# disasters\n\ng = sns.distplot(train_df[train_df['target'] == 1]\n                 ['text'].str.split().map(lambda x: len(x)))\ng.set(title='Disaster Tweets', xlabel='Word Count')\nplt.show()","dc2e2111":"def spell_corrector(text):\n    corrected = str(TextBlob(text).correct())\n    return corrected","b35be66a":"def stopword_remover(text):\n    sw_nltk = set(stopwords.words('english'))\n    sw_clean = [word for word in text if word not in sw_nltk]\n    return sw_clean","c32669e6":"def url_remover(text):\n    # urls\n    text = re.sub(\n        r'https?:\/\/\\S+|www\\.\\S+', '', text)\n\n    # html tags\n    text = re.sub(r'<.*?>', '', text)\n\n    # numbers\n    text = re.sub(r'\\d+', '', text)\n\n    return text","318c264c":"def punctuation_remover(text):\n    import string\n\n    punc_clean = [words for words in text if words not in string.punctuation]\n    return ''.join(punc_clean)","bc096e0b":"def emoji_remover(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","1663141c":"def tokenizer(text):\n    tokens = word_tokenize(text)\n    return tokens","363f77c4":"def lemmatizer(text):\n    lem = [WordNetLemmatizer().lemmatize(word) for word in text]\n    return lem","db16eb7e":"def token_to_text(token):\n    return ' '.join(token)","c2f9a7ec":"def other_clean(text):\n        \"\"\"\n            Other manual text cleaning techniques\n        \"\"\"\n        # Typos, slang and other\n        sample_typos_slang = {\n                                \"w\/e\": \"whatever\",\n                                \"usagov\": \"usa government\",\n                                \"recentlu\": \"recently\",\n                                \"ph0tos\": \"photos\",\n                                \"amirite\": \"am i right\",\n                                \"exp0sed\": \"exposed\",\n                                \"<3\": \"love\",\n                                \"luv\": \"love\",\n                                \"amageddon\": \"armageddon\",\n                                \"trfc\": \"traffic\",\n                                \"16yr\": \"16 year\"\n                                }\n\n        # Acronyms\n        sample_acronyms =  { \n                            \"mh370\": \"malaysia airlines flight 370\",\n                            \"okwx\": \"oklahoma city weather\",\n                            \"arwx\": \"arkansas weather\",    \n                            \"gawx\": \"georgia weather\",  \n                            \"scwx\": \"south carolina weather\",  \n                            \"cawx\": \"california weather\",\n                            \"tnwx\": \"tennessee weather\",\n                            \"azwx\": \"arizona weather\",  \n                            \"alwx\": \"alabama weather\",\n                            \"usnwsgov\": \"united states national weather service\",\n                            \"2mw\": \"tomorrow\"\n                            }\n\n        \n        # Some common abbreviations \n        sample_abbr = {\n                        \"$\" : \" dollar \",\n                        \"\u20ac\" : \" euro \",\n                        \"4ao\" : \"for adults only\",\n                        \"a.m\" : \"before midday\",\n                        \"a3\" : \"anytime anywhere anyplace\",\n                        \"aamof\" : \"as a matter of fact\",\n                        \"acct\" : \"account\",\n                        \"adih\" : \"another day in hell\",\n                        \"afaic\" : \"as far as i am concerned\",\n                        \"afaict\" : \"as far as i can tell\",\n                        \"afaik\" : \"as far as i know\",\n                        \"afair\" : \"as far as i remember\",\n                        \"afk\" : \"away from keyboard\",\n                        \"app\" : \"application\",\n                        \"approx\" : \"approximately\",\n                        \"apps\" : \"applications\",\n                        \"asap\" : \"as soon as possible\",\n                        \"asl\" : \"age, sex, location\",\n                        \"atk\" : \"at the keyboard\",\n                        \"ave.\" : \"avenue\",\n                        \"aymm\" : \"are you my mother\",\n                        \"ayor\" : \"at your own risk\", \n                        \"b&b\" : \"bed and breakfast\",\n                        \"b+b\" : \"bed and breakfast\",\n                        \"b.c\" : \"before christ\",\n                        \"b2b\" : \"business to business\",\n                        \"b2c\" : \"business to customer\",\n                        \"b4\" : \"before\",\n                        \"b4n\" : \"bye for now\",\n                        \"b@u\" : \"back at you\",\n                        \"bae\" : \"before anyone else\",\n                        \"bak\" : \"back at keyboard\",\n                        \"bbbg\" : \"bye bye be good\",\n                        \"bbc\" : \"british broadcasting corporation\",\n                        \"bbias\" : \"be back in a second\",\n                        \"bbl\" : \"be back later\",\n                        \"bbs\" : \"be back soon\",\n                        \"be4\" : \"before\",\n                        \"bfn\" : \"bye for now\",\n                        \"blvd\" : \"boulevard\",\n                        \"bout\" : \"about\",\n                        \"brb\" : \"be right back\",\n                        \"bros\" : \"brothers\",\n                        \"brt\" : \"be right there\",\n                        \"bsaaw\" : \"big smile and a wink\",\n                        \"btw\" : \"by the way\",\n                        \"bwl\" : \"bursting with laughter\",\n                        \"c\/o\" : \"care of\",\n                        \"cet\" : \"central european time\",\n                        \"cf\" : \"compare\",\n                        \"cia\" : \"central intelligence agency\",\n                        \"csl\" : \"can not stop laughing\",\n                        \"cu\" : \"see you\",\n                        \"cul8r\" : \"see you later\",\n                        \"cv\" : \"curriculum vitae\",\n                        \"cwot\" : \"complete waste of time\",\n                        \"cya\" : \"see you\",\n                        \"cyt\" : \"see you tomorrow\",\n                        \"dae\" : \"does anyone else\",\n                        \"dbmib\" : \"do not bother me i am busy\",\n                        \"diy\" : \"do it yourself\",\n                        \"dm\" : \"direct message\",\n                        \"dwh\" : \"during work hours\",\n                        \"e123\" : \"easy as one two three\",\n                        \"eet\" : \"eastern european time\",\n                        \"eg\" : \"example\",\n                        \"embm\" : \"early morning business meeting\",\n                        \"encl\" : \"enclosed\",\n                        \"encl.\" : \"enclosed\",\n                        \"etc\" : \"and so on\",\n                        \"faq\" : \"frequently asked questions\",\n                        \"fawc\" : \"for anyone who cares\",\n                        \"fb\" : \"facebook\",\n                        \"fc\" : \"fingers crossed\",\n                        \"fig\" : \"figure\",\n                        \"fimh\" : \"forever in my heart\", \n                        \"ft.\" : \"feet\",\n                        \"ft\" : \"featuring\",\n                        \"ftl\" : \"for the loss\",\n                        \"ftw\" : \"for the win\",\n                        \"fwiw\" : \"for what it is worth\",\n                        \"fyi\" : \"for your information\",\n                        \"g9\" : \"genius\",\n                        \"gahoy\" : \"get a hold of yourself\",\n                        \"gal\" : \"get a life\",\n                        \"gcse\" : \"general certificate of secondary education\",\n                        \"gfn\" : \"gone for now\",\n                        \"gg\" : \"good game\",\n                        \"gl\" : \"good luck\",\n                        \"glhf\" : \"good luck have fun\",\n                        \"gmt\" : \"greenwich mean time\",\n                        \"gmta\" : \"great minds think alike\",\n                        \"gn\" : \"good night\",\n                        \"g.o.a.t\" : \"greatest of all time\",\n                        \"goat\" : \"greatest of all time\",\n                        \"goi\" : \"get over it\",\n                        \"gps\" : \"global positioning system\",\n                        \"gr8\" : \"great\",\n                        \"gratz\" : \"congratulations\",\n                        \"gyal\" : \"girl\",\n                        \"h&c\" : \"hot and cold\",\n                        \"hp\" : \"horsepower\",\n                        \"hr\" : \"hour\",\n                        \"hrh\" : \"his royal highness\",\n                        \"ht\" : \"height\",\n                        \"ibrb\" : \"i will be right back\",\n                        \"ic\" : \"i see\",\n                        \"icq\" : \"i seek you\",\n                        \"icymi\" : \"in case you missed it\",\n                        \"idc\" : \"i do not care\",\n                        \"idgadf\" : \"i do not give a damn fuck\",\n                        \"idgaf\" : \"i do not give a fuck\",\n                        \"idk\" : \"i do not know\",\n                        \"ie\" : \"that is\",\n                        \"i.e\" : \"that is\",\n                        \"ifyp\" : \"i feel your pain\",\n                        \"IG\" : \"instagram\",\n                        \"iirc\" : \"if i remember correctly\",\n                        \"ilu\" : \"i love you\",\n                        \"ily\" : \"i love you\",\n                        \"imho\" : \"in my humble opinion\",\n                        \"imo\" : \"in my opinion\",\n                        \"imu\" : \"i miss you\",\n                        \"iow\" : \"in other words\",\n                        \"irl\" : \"in real life\",\n                        \"j4f\" : \"just for fun\",\n                        \"jic\" : \"just in case\",\n                        \"jk\" : \"just kidding\",\n                        \"jsyk\" : \"just so you know\",\n                        \"l8r\" : \"later\",\n                        \"lb\" : \"pound\",\n                        \"lbs\" : \"pounds\",\n                        \"ldr\" : \"long distance relationship\",\n                        \"lmao\" : \"laugh my ass off\",\n                        \"lmfao\" : \"laugh my fucking ass off\",\n                        \"lol\" : \"laughing out loud\",\n                        \"ltd\" : \"limited\",\n                        \"ltns\" : \"long time no see\",\n                        \"m8\" : \"mate\",\n                        \"mf\" : \"motherfucker\",\n                        \"mfs\" : \"motherfuckers\",\n                        \"mfw\" : \"my face when\",\n                        \"mofo\" : \"motherfucker\",\n                        \"mph\" : \"miles per hour\",\n                        \"mr\" : \"mister\",\n                        \"mrw\" : \"my reaction when\",\n                        \"ms\" : \"miss\",\n                        \"mte\" : \"my thoughts exactly\",\n                        \"nagi\" : \"not a good idea\",\n                        \"nbc\" : \"national broadcasting company\",\n                        \"nbd\" : \"not big deal\",\n                        \"nfs\" : \"not for sale\",\n                        \"ngl\" : \"not going to lie\",\n                        \"nhs\" : \"national health service\",\n                        \"nrn\" : \"no reply necessary\",\n                        \"nsfl\" : \"not safe for life\",\n                        \"nsfw\" : \"not safe for work\",\n                        \"nth\" : \"nice to have\",\n                        \"nvr\" : \"never\",\n                        \"nyc\" : \"new york city\",\n                        \"oc\" : \"original content\",\n                        \"og\" : \"original\",\n                        \"ohp\" : \"overhead projector\",\n                        \"oic\" : \"oh i see\",\n                        \"omdb\" : \"over my dead body\",\n                        \"omg\" : \"oh my god\",\n                        \"omw\" : \"on my way\",\n                        \"p.a\" : \"per annum\",\n                        \"p.m\" : \"after midday\",\n                        \"pm\" : \"prime minister\",\n                        \"poc\" : \"people of color\",\n                        \"pov\" : \"point of view\",\n                        \"pp\" : \"pages\",\n                        \"ppl\" : \"people\",\n                        \"prw\" : \"parents are watching\",\n                        \"ps\" : \"postscript\",\n                        \"pt\" : \"point\",\n                        \"ptb\" : \"please text back\",\n                        \"pto\" : \"please turn over\",\n                        \"qpsa\" : \"what happens\", #\"que pasa\",\n                        \"ratchet\" : \"rude\",\n                        \"rbtl\" : \"read between the lines\",\n                        \"rlrt\" : \"real life retweet\", \n                        \"rofl\" : \"rolling on the floor laughing\",\n                        \"roflol\" : \"rolling on the floor laughing out loud\",\n                        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n                        \"rt\" : \"retweet\",\n                        \"ruok\" : \"are you ok\",\n                        \"sfw\" : \"safe for work\",\n                        \"sk8\" : \"skate\",\n                        \"smh\" : \"shake my head\",\n                        \"sq\" : \"square\",\n                        \"srsly\" : \"seriously\", \n                        \"ssdd\" : \"same stuff different day\",\n                        \"tbh\" : \"to be honest\",\n                        \"tbs\" : \"tablespooful\",\n                        \"tbsp\" : \"tablespooful\",\n                        \"tfw\" : \"that feeling when\",\n                        \"thks\" : \"thank you\",\n                        \"tho\" : \"though\",\n                        \"thx\" : \"thank you\",\n                        \"tia\" : \"thanks in advance\",\n                        \"til\" : \"today i learned\",\n                        \"tl;dr\" : \"too long i did not read\",\n                        \"tldr\" : \"too long i did not read\",\n                        \"tmb\" : \"tweet me back\",\n                        \"tntl\" : \"trying not to laugh\",\n                        \"ttyl\" : \"talk to you later\",\n                        \"u\" : \"you\",\n                        \"u2\" : \"you too\",\n                        \"u4e\" : \"yours for ever\",\n                        \"utc\" : \"coordinated universal time\",\n                        \"w\/\" : \"with\",\n                        \"w\/o\" : \"without\",\n                        \"w8\" : \"wait\",\n                        \"wassup\" : \"what is up\",\n                        \"wb\" : \"welcome back\",\n                        \"wtf\" : \"what the fuck\",\n                        \"wtg\" : \"way to go\",\n                        \"wtpa\" : \"where the party at\",\n                        \"wuf\" : \"where are you from\",\n                        \"wuzup\" : \"what is up\",\n                        \"wywh\" : \"wish you were here\",\n                        \"yd\" : \"yard\",\n                        \"ygtr\" : \"you got that right\",\n                        \"ynk\" : \"you never know\",\n                        \"zzz\" : \"sleeping bored and tired\"\n                        }\n            \n        sample_typos_slang_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_typos_slang.keys()) + r')(?!\\w)')\n        sample_acronyms_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_acronyms.keys()) + r')(?!\\w)')\n        sample_abbr_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_abbr.keys()) + r')(?!\\w)')\n        \n        text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)\n        text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)\n        text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)\n        \n        return text","b9ff9420":"def pipline(data):\n    data['clean_text'] = data['text'].apply(lambda x: str.lower(x))\n    data['clean_text'] = data['clean_text'].apply(lambda x: url_remover(x))\n    data['clean_text'] = data['clean_text'].apply(lambda x: emoji_remover(x))\n    data['clean_text'] = data['clean_text'].apply(\n        lambda x: punctuation_remover(x))\n    data['clean_text'] = data['clean_text'].apply(lambda x: other_clean(x))\n    data['clean_text'] = data['clean_text'].apply(lambda x: spell_corrector(x))\n    data['clean_text'] = data['clean_text'].apply(lambda x: tokenizer(x))\n    data['clean_text'] = data['clean_text'].apply(\n        lambda x: stopword_remover(x))\n    data['clean_text'] = data['clean_text'].apply(lambda x: lemmatizer(x))\n    data['clean_text'] = data['clean_text'].apply(lambda x: token_to_text(x))","d46b341a":"pipline(train_df)\npipline(test_df)","256f0900":"train_df","c9f39e98":"def most_used_words(df, key):\n    plt.figure(figsize=(8, 6))\n    title = 'Disaster' if key == 1 else 'Non Disaster'\n\n    pd.Series(' '.join(df[train_df['target'] == key]['clean_text']).split(\n    )).value_counts()[:20].plot.bar(title=title)\n    plt.show()","a487925f":"most_used_words(train_df, 0)","d98fae71":"most_used_words(train_df, 1)","41d0c9d0":"most_used_words(test_df, 0)","96591e1f":"most_used_words(test_df, 1)","f57d3c12":"def wordcloud_plot(df, target):\n    words = []\n    for x in df[df['target'] == target]['text'].str.split():\n        for i in x:\n            words.append(i)\n\n    # plotting\n    plt.figure(figsize=(12, 8))\n    word_cloud = WordCloud(\n        background_color='white',\n        max_font_size=80\n    ).generate(\" \".join(words[:50]))\n    plt.imshow(word_cloud)\n    plt.axis('off')\n    plt.show()","3015dc7f":"print('Disaster')\nwordcloud_plot(train_df, 1)","2713fd8f":"print('Not A Disaster')\nwordcloud_plot(train_df, 0)","2a493f71":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import VotingClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier","5fca8605":"X_train, X_val, y_train, y_val = train_test_split(train_df['clean_text'].to_numpy(\n), train_df['target'].to_numpy(), test_size=0.2, random_state=42)","b82c9def":"tf_transformer = TfidfVectorizer().fit(X_train)\ntrain_features = tf_transformer.transform(X_train)\n\nval_features = tf_transformer.transform(X_val)","8d86955a":"nv = MultinomialNB()\nrf = RandomForestClassifier(random_state=42)\nlgbm = LGBMClassifier(random_state=42)\nxgb = XGBClassifier(random_state=42)\ncat = CatBoostClassifier(random_state=42)\nvoting_clf = VotingClassifier(\n    estimators=[('lgbm', lgbm), ('rf', rf), ('nv', nv), ('xgb', xgb)], voting='soft')\n\nfor clf in (nv, rf, lgbm, xgb, voting_clf):\n    clf.fit(train_features, y_train)\n    print(clf.__class__.__name__, accuracy_score(\n        y_val, clf.predict(val_features)))","ec5478c1":"test = test_df['clean_text'].to_numpy()","5601d43d":"test_features = tf_transformer.transform(test)","8ce93cf5":"y_pred = voting_clf.predict(test_features)","48d53022":"submission = pd.DataFrame(columns=['id', 'target'])\nsubmission['id'] = test_df['id']\nsubmission['target'] = y_pred\n\nsubmission.to_csv('sub.csv', index=False)\nsubmission","a7ef52fb":"## Word cloud","299d9f45":"### Not disaster","446cb216":"## Removing stop words","3e33f028":"## Text cleaner pipeline","9dc80fce":"### Disaster","c244c3fe":"## Other text cleanings","4f965301":"### Word distribution","c14dd564":"### Character distribution","cf3f6d9f":"## Remove Emojies","ca6d3e39":"## Train and test split","c875619e":"## Spell correction","9ef62666":"### Checking NaNs","d2393e1b":"## Lemmatization","be7a0127":"## Converting array to text","dde113e3":"## Tokenization","f40615e4":"## Bag of words","864a25cb":"## Remove URL and HTML tags and numbers","33db91b0":"## Remove punctuation marks"}}