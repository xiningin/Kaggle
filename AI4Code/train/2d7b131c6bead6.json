{"cell_type":{"bdf63db2":"code","aef80e35":"code","f5c8ed4d":"code","247a308d":"code","37c56cc5":"code","7b7a65db":"code","3137da2a":"code","45d9bf41":"code","ca8561e1":"code","ee9dd698":"code","2621be18":"code","0f4aa2ad":"code","ae6ee61c":"code","e044208f":"code","558e4fb3":"code","5c231d62":"code","dd42e412":"code","1afee1db":"code","37075916":"code","ddde08f0":"code","5ddef10b":"code","65e1eb92":"code","d3ceaef2":"code","148dc6af":"code","0d51c0ed":"code","7d908dce":"code","e78f6596":"code","3a6cea9f":"code","95e12c8a":"code","d4728925":"code","670dd3b2":"code","b3c1a250":"code","b46dc74a":"code","2b551cc1":"markdown","8150eed2":"markdown","059f6706":"markdown","6877bc1a":"markdown","07efab4d":"markdown","3ba3e62e":"markdown","b135170b":"markdown","6bece458":"markdown","192f15cc":"markdown","21a9f6f2":"markdown","39f8a6a5":"markdown","74e8f7a2":"markdown","00c41298":"markdown","14bbbe01":"markdown","1afebb39":"markdown"},"source":{"bdf63db2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import Isomap\nfrom sklearn.manifold import TSNE\n\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","aef80e35":"df_wine = pd.read_csv('..\/input\/Wine.csv')\n","f5c8ed4d":"df_wine.describe()","247a308d":"df_wine.head(5)","37c56cc5":"df_wine.info()","7b7a65db":"df_wine['Customer_Segment'].unique()","3137da2a":"# seperate the label and featuers\nX = df_wine.drop('Customer_Segment', axis=1)\ny = df_wine['Customer_Segment']","45d9bf41":"wine_correlation = X.corr()\nplt.figure(figsize=(25,25))\nsns.heatmap(wine_correlation, annot=True, cmap =\"RdBu_r\");","ca8561e1":"# Based on heat map Flavanoids is linearly related to Total_Phenols\nsns.scatterplot(x='Flavanoids', y='Total_Phenols', data=df_wine);","ee9dd698":"# Based on heat map Flavanoids is linearly related to OD280\nsns.scatterplot(x='Flavanoids', y='OD280', data=df_wine);","2621be18":"pca_wine = PCA(n_components=2)\n\nprincipalComp_wine = pca_wine.fit_transform(X)\nprint('Explained variation per principal component: {}'.format(pca_wine.explained_variance_ratio_))","0f4aa2ad":"principal_wine_Df = pd.DataFrame(data = principalComp_wine, columns = ['principal component 1', 'principal component 2'])\nprincipal_wine_Df.head(5)","ae6ee61c":"plt.figure(figsize=(10,8))\nplt.scatter(principal_wine_Df['principal component 1'],principal_wine_Df['principal component 2'], c=y);","e044208f":"embedding = Isomap(n_components=2,n_neighbors=40)\nX_isomap = embedding.fit_transform(X)\n\nplt.figure(figsize=(10, 8))\nplt.scatter(X_isomap[:, 0], X_isomap[:, 1], c=y);","558e4fb3":"tsneModel = TSNE(n_components=2, random_state=0, perplexity=50, n_iter=5000)\ntsne_wine_data =  tsneModel.fit_transform(X)\n\ntsne_wine_data = np.vstack((tsne_wine_data.T, y)).T\n\ntsne_wine_df = pd.DataFrame(data=tsne_wine_data, columns=('Dim_1', 'Dime_2', 'label'))\n\nsns.FacetGrid(tsne_wine_df, hue='label', size=6).map(plt.scatter,  'Dim_1', 'Dime_2')\nplt.show();","5c231d62":"X.head()","dd42e412":"from sklearn import preprocessing","1afee1db":"minmaxScaler = preprocessing.MinMaxScaler()\nX_scaled_df = minmaxScaler.fit_transform(X)","37075916":"X_scaled_df = pd.DataFrame(X_scaled_df, columns=X.columns)","ddde08f0":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6,5))\nax1.set_title('Before Scaling')\nsns.kdeplot(X['Alcohol'], ax=ax1)\nsns.kdeplot(X['Magnesium'], ax=ax1)\nsns.kdeplot(X['Proline'], ax=ax1)\n\nax2.set_title('After Min-Max Scaling')\nsns.kdeplot(X_scaled_df['Alcohol'], ax=ax2)\nsns.kdeplot(X_scaled_df['Magnesium'], ax=ax2)\nsns.kdeplot(X_scaled_df['Proline'], ax=ax2)\n\nplt.show();","5ddef10b":"from sklearn import metrics\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans","65e1eb92":"X_scaled_df.shape","d3ceaef2":"X_scaled_df.describe()","148dc6af":"sScores = []\nfor n_clusters in range(2,30):\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n    kmeans.fit(X_scaled_df)\n    clusters = kmeans.predict(X_scaled_df)\n    silhouette_avg = silhouette_score(X_scaled_df, clusters)\n    sScores.append(silhouette_avg)\n\nsilhouette_df = pd.DataFrame({'No_Clusters':range(2,30), 'sScore':sScores})","0d51c0ed":"sns.set()\nplt.figure(figsize=(10, 6))\nax = sns.lineplot(x='No_Clusters', y='sScore', data=silhouette_df);\nax.set(xticks=silhouette_df['No_Clusters']);\n# from the plot below we can conclude that 3 is a good cluster number as Silhouette Score is the highest.","7d908dce":"wss = []\nfor n_clusters in range(2,30):\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n    kmeans.fit(X_scaled_df)\n    wss.append(kmeans.inertia_)\n    \n\nelbow_df = pd.DataFrame({'No_Clusters':range(2,30), 'sScore':wss})\n    ","e78f6596":"plt.figure(figsize=(10, 6))\nax = sns.lineplot(x='No_Clusters', y='sScore', data=elbow_df);\nax.set(xticks=elbow_df['No_Clusters']);\n# from the plot below we can conclude that 3 is a good cluster number as the  elbow is at 3.","3a6cea9f":"# as per the elbow-method or Silhouette plot the number of clusters is => 3\nkmeans_tsne_2d_df = tsne_wine_df.drop('label', axis=1)\nkmeans_tsne = KMeans(n_clusters=3, random_state=0)\nkmeans_tsne_labels = kmeans_tsne.fit_predict(kmeans_tsne_2d_df)\nkmeans_tsne_centers = kmeans_tsne.cluster_centers_\nkmeans_tsne_2d_df['label'] = kmeans_tsne_labels.tolist()","95e12c8a":"kmeans_tsne_2d_df.head()","d4728925":"from scipy.spatial.distance import cdist\n\nfig, ax = plt.subplots(figsize=(10, 10))\nax.axis('equal')\nsns.scatterplot(x='Dim_1', y='Dime_2', hue='label', data=kmeans_tsne_2d_df, palette=['red','blue','green'], ax=ax)\nplt.scatter(kmeans_tsne_centers[:,0], kmeans_tsne_centers[:, 1], s = 300, c = 'black' , label = 'centeroid', marker='*')\nradii = [cdist(kmeans_tsne_2d_df[kmeans_tsne_labels == i].drop('label', axis=1), [center]).max() for i, center in enumerate(kmeans_tsne_centers)]\nfor c, r in zip(kmeans_tsne_centers, radii):\n    ax.add_patch(plt.Circle(c, r, alpha=0.3))\n        \nplt.show();","670dd3b2":"from sklearn.mixture import GaussianMixture\n\ngmm_tsne_2d_df = tsne_wine_df.drop('label', axis=1)\n\ngmm = GaussianMixture(n_components=3, random_state=42, covariance_type='full')\ngmm_tsne_labels = gmm.fit(gmm_tsne_2d_df).predict(gmm_tsne_2d_df)\ngmm_tsne_centers = gmm.means_\ngmm_tsne_2d_df['label'] = gmm_tsne_labels.tolist()","b3c1a250":"gmm_tsne_2d_df.head()","b46dc74a":"from matplotlib.patches import Ellipse\n\nfig, ax = plt.subplots(figsize=(10, 10))\nax.axis('equal')\nsns.scatterplot(x='Dim_1', y='Dime_2', hue='label', data=gmm_tsne_2d_df, palette=['red','blue','green'], ax=ax)\n\nplt.scatter(gmm_tsne_centers[:,0], gmm_tsne_centers[:, 1], s = 300, c = 'black' , label = 'centeroid', marker='*')\nax.add_patch(Ellipse(gmm_tsne_centers[0], 2, 5.5, 150, alpha=0.3));\nax.add_patch(Ellipse(gmm_tsne_centers[1], 2, 6.6, 145, alpha=0.3));\nax.add_patch(Ellipse(gmm_tsne_centers[2], 2, 7, 145, alpha=0.3));\n\nplt.show();","2b551cc1":"# **1. Clustering Introduction**\nClustering is an unsupervised Machine Learning method that is used to learn structure in a set of data points. There are no labels that can be used to train the model. Clustering tries to provide an answer to question \"How to  make sense of data\". Clustering is performed  on a set of N numbers of point present in a n-dimensonal hypercube. \n\nDistance between points indicates how similar points are. Below two points are the **Objectives** of clustering\n1. Between points in different clusters ditances should be large.\n2. Between points in same cluster distances should be small.\n\nPoints in the same group are very similar and points in different groups are very different\n\nTwo types of unsupervised machine learning\n\n![image.png](attachment:image.png)\n\nTwo types of clustering \n1. Density Based\n    1. KMeans\n    2. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n    3. Mean Shift\n    4. GMM (Gaussian Mixture Models)\n2. Hierarchical Based\n    1. Top-down\n    2. Bottom-up\n    \n    \nReference:\n\nI have added reference to indiviual sections. \n","8150eed2":"**3.1 PCA - Principal Component Analysis**\n\n1. Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction in data. \n2. Most basic and simplest dimensionality reduction\n3. Visualizaing high n-dimensaonality data by reducing to 2\/3-dimensaion.\n4. Using PCA to reduce n-dimensional data into 2 dimension, so that data can be better understood.\n5. PCA tries to preserve the global shape\/structure of data","059f6706":"### **5.1.3 Visualizing K-Means in 2D**\nTo visualize in 2D, we have to reduce the n-dimesion to 2D. For this let's use t-SNE dataframe we had generated in step 3.3.\n\nOne way to think about the k-means model is that it places a circle (or, in higher dimensions, a hyper-sphere) at the center of each cluster, with a radius defined by the most distant point in the cluster. This radius acts as a hard cutoff for cluster assignment within the training set: any point outside this circle is not considered a member of the cluster.\n","6877bc1a":"# 4. Standardization\nNormalization typically means rescales the values into a range of [0,1]. \nStandardization typically means rescales data to have a mean of 0 and a standard deviation of 1 (unit variance).","07efab4d":"### **5.1.1 Silhouette Method**\n1. What is Silhouette Score\n\nThis doesn't need labeled data thus gives a huge advantage. The Silhouette score is dervied  from Silhouette Coeff, where-in Silhouette Coeff is calculated from  each data point. \nThe Silhouette Score  ranges from '-1' to '+1'. A score of '+1' indicates the  data point is well within the cluster. A score of '-1' indicates that the  data point is assigned to wrong cluster. A score of '0' indicates that the  data point is on or  near to  border  of cluster.\n2. Math behind Silhouette Score\n\n2.1 Calculate the Silhouette Coeff for  each data point\n![image.png](attachment:image.png)\n\nb(i) => Mean distance of point i from all the points in **next nearest** cluster\n\na(i) => Mean distance of point i from all the points in **same** cluster\n\n2.2 Calculate the  Silhouette Score by averaging Silhouette Coeff of all data points\n\n\nReference:\n\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html\n","3ba3e62e":"# **3.Dimensionality reduction for Data Visualization**\n\n1. Principal component analysis (PCA)\n2. Isometric Mapping\n3. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n\nBelow is a 8 part collection on Dimensionality Reduction\n\n[https:\/\/blog.paperspace.com\/tag\/series\/](http:\/\/)","b135170b":"** 3.3 t-Distributed Stochastic Neighbor Embedding (t-SNE) **\n\n1. t-SNE is capable of capturing much of the local structure of the high-dimensional\ndata very well, while also revealing global structure such as the presence of clusters at several scales.\n2. t-SNE preserve the local strucutre\n\nReference: https:\/\/www.youtube.com\/watch?v=FQmCzpKWD48","6bece458":"# Content\n    1.Clustering Introduction\n    2.EDA\n    3.Dimensionality reduction for visualization\n    4.Standardization\n    5.Clustering Techniques","192f15cc":"**3.2. Isometric Mapping**\n\nIsomap is a non-linear dimensionality reduction method based on the spectral theory which tries to preserve the geodesic distances in the lower dimension.\n![image.png](attachment:image.png)\nReference: https:\/\/blog.paperspace.com\/dimension-reduction-with-isomap\/","21a9f6f2":"### **5.1.2 Elbow Method**\nThe Elbow Method is one of the most popular methods to determine this optimal value of k. The basic idea is to define clusters such that the total intra-cluster variation i.e. WSS (within-cluster sum of square) is minimized. The total WSS measures the compactness of the cluster. \n\nThe desired number of clusters should be such that adding another cluster doesn't improve the WSS. If the line plot looks like an arm, then the \"elbow\" on the arm is the value of k that is the best cluster number.","39f8a6a5":"**EDA - exploratory data analysis**\n1. Descriptive Statistics\n    1. Pandas Describe Func\n2. Correlation","74e8f7a2":"# **5. Clustering Techniques**\n\n","00c41298":" **Min-Max Scaler**\n\nThe MinMaxScaler is the probably the most famous scaling algorithm, and follows the following formula for each feature:\n\n![image.png](attachment:image.png)\n\nIt essentially shrinks the range such that the range is now between 0 and 1 (or -1 to 1 if there are negative values).\n\nThis scaler works better for cases in which the standard scaler might not work so well. If the distribution is not Gaussian or the standard deviation is very small, the min-max scaler works better.\n\nReference: http:\/\/benalexkeen.com\/feature-scaling-with-scikit-learn\/","14bbbe01":"## **5.2 Gaussian Mixture Models (GMMs)**\n\nA Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. GMM has two parameters to describe the shape of the clusters i.e. Mean and Standard Deviation. Due to this the cluster can take any eliptical shape. GMM does soft assignments to each cluster as compared to hard assignments done by K-Means. Expectation\u2013Maximization (EM) algorithm is used to fit clusters.\n\nThe hyperparameter (covariance_type), controls the degree of freedom in the shape of cluster. The default is covariance_type=\"diag\", which means that the size of the cluster along each dimension can be set independently, with the resulting ellipse constrained to align with the axes.\n\n\nReference:\n\nhttps:\/\/drrahilasheikh.wordpress.com\/2017\/12\/22\/featured-content\/comment-page-1\/\n\nhttps:\/\/towardsdatascience.com\/gaussian-mixture-modelling-gmm-833c88587c7f\n\nhttps:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.12-gaussian-mixtures.html\n","1afebb39":"## **5.1 K-means**\nIs an example of *centroid* based algorithm, where every cluster can be  represnted by a centroid or reference vector.\nK-means clustering is top-down approach, in the sense, we decide the number of clusters (k) and then group the data points into k clusters. k-means is limited to linear cluster boundaries.\n\nBelow are the two methods to pick the right value of 'K'. Meaning the right number of clusters.\n1. Silhouette Method\n2. Elbow Method\n\n*Note:*\nThe elbow method doesn't always work well, if the data is not clustered  well. If the line plot doesn't plot an 'arm' like plot, and instead the plot is fairly smooth curve. Then it's unclear what is the best value of k to  choose. In this case try out 'Silhouette Method' or reevaluate whether clustering is the right thing\n\n<img src=\"https:\/\/drrahilasheikh.files.wordpress.com\/2017\/12\/92d25-1krczk0xygta4qfrvr0fo2w.gif\">\n\n*Reference:*\n\nhttps:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.11-k-means.html\n\nhttps:\/\/drrahilasheikh.wordpress.com\/2017\/12\/22\/featured-content\/comment-page-1\/"}}