{"cell_type":{"bd3d9e0b":"code","a1cfa078":"code","23c28185":"code","b4133f03":"code","2402a5d1":"code","3a768b35":"code","d0372dae":"code","aa18b6f5":"code","6f28b426":"markdown","6eceeb6f":"markdown","59d8ea9c":"markdown","b8c682b8":"markdown","da122f52":"markdown","ab12b354":"markdown","a6eb5b92":"markdown","8dd6130e":"markdown","ef9c9599":"markdown","bf384263":"markdown","c174b44d":"markdown","fc0e46fe":"markdown","23d6c8c9":"markdown","88f0d3ae":"markdown","f1b054c1":"markdown","94d643c1":"markdown","ccd25556":"markdown","01b4d814":"markdown","e2f03bc0":"markdown","02ab1b80":"markdown","1c6c78eb":"markdown","6f5f060f":"markdown","ee2a6f19":"markdown","cc42f860":"markdown","7e90b6b4":"markdown","1e5c1cc0":"markdown","3f9f40fd":"markdown","cbf4b244":"markdown","c0c2555f":"markdown","1a829f62":"markdown","65eb2a41":"markdown","14b816c2":"markdown","bcd4b1a4":"markdown","d71a578b":"markdown","cfaac70e":"markdown","6a4436e6":"markdown","aad7dd57":"markdown","dbb4e7e8":"markdown"},"source":{"bd3d9e0b":"import gym # openAi gym\nfrom gym import envs\nfrom IPython.display import Image\nimport os\nImage(\"\/kaggle\/input\/week9dataset\/Guide of OpenAI Gym1.png\")","a1cfa078":"Image(\"\/kaggle\/input\/week9dataset\/Quality_Based_Reinforcement_Learning_Methods2.jpeg\")","23c28185":"import gym\nenv = gym.make('FrozenLake-v0')\nenv.action_space# View the number of actions available in the environment","b4133f03":"env.reset() # Reset the environment\nenv.render() # Render the environment\n\naction = env.action_space.sample() # Use sample to indicate the random actions\nenv.step(action) # Take random actions","2402a5d1":"observation = env.reset() \n\nfor t in range(20):\n    env.render()\n    action = env.action_space.sample()\n    observation, reward, done, info = env.step(action) \n    print(observation, reward, done) # Output\n    if done: # If the game is terminated\n        print(\"Episode finished after {} timesteps\".format(t+1)) # Print the timesteps\n        break","3a768b35":"from IPython import display\nimport time\n\nobservation = env.reset() \nfor t in range(20):\n    env.render() \n    action = env.action_space.sample()\n    observation, reward, done, info = env.step(action) \n    print(observation, reward, done)\n    display.clear_output(wait=True) # Clear the Output\n    time.sleep(1) # Delay 1s to execute\n    print(\"step:\", t)\n    if done:\n        print(\"Episode finished after {} timesteps\".format(t+1))\n        break","d0372dae":"!pip install gym[atari] \nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nenv = gym.make(\"Breakout-v0\")\nrgb_array = env.render(mode='rgb_array') # Render as arrays\nplt.imshow(rgb_array) # Display","aa18b6f5":"env.reset()\nfor t in range(100): # Set maximum timesteps as 100\n    plt.imshow(env.render(mode='rgb_array'))\n    display.display(plt.gcf())\n    action = env.action_space.sample()\n    observation, reward, done, info = env.step(action) # Take random actions\n    print(reward, done)\n    display.clear_output(wait=True)","6f28b426":"First we need to load the environment. The method is very simple:","6eceeb6f":"<div style=\"color: #999;font-size: 12px;font-style: italic;\">Congratulations! You've completed the Guide of OpenAI Gym Lab.<\/div>","59d8ea9c":"### Environment","b8c682b8":"The biggest feature of reinforcement learning is that the agent can interact with the environment and accept rewards from the environment. When building a reinforcement learning algorithm, setting the environment up is a very troublesome process. Today OpenAI provides a library Gym that makes it easy for us to learn and debug in some interesting environment. This chapter is mainly to introduce the Gym environment and to familiarize you with the use of Gym with the help of several demos.","da122f52":"Besides the `FrozenLake-v0` environment, we will demonstrate the `Breakout-v0` under the **Atari** category, which is a brick-breaking game.","ab12b354":"The introduction of Gym environment is up to here. You may feel that Gym is very easy to use. As we say, Gym has actually lowered the threshold for beginners. Review the experimental knowledge points of this chapter:<br>\n<br>\n- Concept of reinforcement learning<br>\n- Using OpenAi Gym environments<br>\n- Composition of Gym environment<br>\n- Basic use of Gym environment ","a6eb5b92":"As you can see from the output, after the `env.step()` action occurs, the environment returns four values which correspond to:<br>\n<br>\n- `observation (object)`: The state of the ice surface, that is, the observation of the Agent.<br>\n- `reward (float)`: Reward, the sum of all returns obtained from previous actions.<br>\n- `done (boolean)`: Determine whether you want to reset the environment. For example, if you drop into the ice hole or get the frisbee, reset the environment.<br>\n- `info (dict)`: Debug diagnostic information.","8dd6130e":"Now, with OpenAI's Gym environment, this will no longer be a hassle, especially for beginners. We can quickly learn algorithms and conduct experiments in several fun game environments.","ef9c9599":"<h1 style=\"color:brown\">Reinforcement Learning<\/h1>","bf384263":"In the previous chapters, we gave the reinforcement learning flowchart shown below. It can be clearly seen that the environment is the basis of reinforcement learning. The agent always interacts with the environment and gets rewards from it. It is also an important characteristics for which reinforcement learning is different from supervised learning or unsupervised learning.","c174b44d":"---","fc0e46fe":"---","23d6c8c9":"The entire environment provided by Gym mainly includes the following major categories:","88f0d3ae":"Here `S` indicates a start block, `G` indicates a goal block where the disk is located, `F` indicates a safe block (frozen surface) and `H` indicates the dangerous hole. Every time the Agent departs from `S`, if it falls into the ice hole, the game ends; while reaching `G` will reward it with `1`. In addition, the ice surface is very slippery, so it does not necessarily move in the expected direction.","f1b054c1":"### Introduction","94d643c1":"## Summary","ccd25556":"---","01b4d814":"Next we use the word game called `FrozenLake-v0` to get started with Gym. `FrozenLake-v0` is a game of flying discs on the ice, defaulting to a 4x4 grid. e.g.:","e2f03bc0":"## Guide of OpenAI Gym","02ab1b80":"**At present, our Kaggle Kernel environment has a complete OpenAI Gym environment and there is no need to perform any aditional installations during online experiments.**","1c6c78eb":"If the agent takes a random action, the environment will reward it and the agent will then move to the next state:","6f5f060f":"In fact, we can also use the `IPython.display` module to make the process dynamic in Notebook:","ee2a6f19":"- `Atari`: Dozens of small games, such as squares, table tennis etc.<br>\n- `Box2D`: Continuous tasks in a two-dimensional environment, such as driving a car, walking a robot etc.<br>\n- `Classic control`: Classic control tasks such as simple robotic arm control or trolley climbing missions.<br>\n- `MuJoCo`: Continuous tasks in a three-dimensional environment, such as a villain walking on the floor.<br>\n- `Robotics`: Robotic arm simulation mission.<br>\n- `Toy text`: Very simple word game.","cc42f860":"### `FrozenLake-v0` Environment","7e90b6b4":"The OpenAI Gym environment comes preinstalled on Kaggle. There are **NO** limitations of Notebook, we just need to **import gym** # openAi gym amd **from gym import envs**.<br>\n<br>","1e5c1cc0":"- Concept of reinforcement learning<br>\n- Installation of Gym environment<br>\n- Composition of Gym environment<br>\n- Basic use of Gym environment ","3f9f40fd":"- Python 3.6<br>\n- Gym 0.10.5","cbf4b244":"**Detailed information about these environments can be found by visiting [this link](https:\/\/gym.openai.com\/envs).**","c0c2555f":"`Discrete (4)` means that there are `4` actions you can think of: `up`, `down`, `left` and `right`.","1a829f62":"Just because reinforcement learning requires the environment, when we want to debug or compare algorithms, we need to set the environment up at first (such as designing and building a maze, determining rewards etc.), which is often a complicated process. Therefore, Q-Learning can only use the text maze game in our previous attempt.","65eb2a41":"$$\n\\begin{bmatrix}\nSFFF\n\\\\ \nFHFH\n\\\\ \nFFFH\n\\\\ \nHFFG\n\\end{bmatrix}\n$$","14b816c2":"### Key Points","bcd4b1a4":"As you can see, the top is the scoreboard and the right is the initial location of the Agent. Again, we can animate the process of random actions:","d71a578b":"**Reading Material**<br>\n<br>\n- [OpenAI Gym](https:\/\/gym.openai.com\/)","cfaac70e":"## Using Gym in Kaggle Kernels","6a4436e6":"## Use of Gym","aad7dd57":"### `Breakout-v0` Environment","dbb4e7e8":"Again, let's load the environment first. We need to use Matplotlib to help visualize the initial location of the Agent:"}}