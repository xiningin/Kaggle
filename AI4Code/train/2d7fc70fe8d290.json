{"cell_type":{"3d1aa99e":"code","3728ef61":"code","3c4777e8":"code","e91d6ee1":"code","90a0aca7":"code","dc03606d":"code","dcf9de67":"code","8dc5b66b":"code","36e9a846":"code","b2adc775":"code","e070ce53":"code","184458a8":"code","31827332":"code","cd9f27ee":"code","a1acfaaf":"code","f1463ad5":"code","0166c2aa":"code","55239bc7":"code","db142b8d":"code","f571289c":"code","b32aaea2":"code","df8f7b88":"code","a6dc1b0d":"code","991f229c":"code","3d272529":"code","240eb129":"code","d8e2059b":"code","d7f62b31":"code","3646ff20":"code","7fa33264":"code","95d190de":"code","fc9ec471":"code","5446adf8":"code","f5486ceb":"code","3f965f6b":"code","1640fff9":"code","bb8d8474":"code","cb844b47":"code","ff158aee":"code","ab974cf9":"code","19464f35":"code","402b99e8":"code","932ed926":"code","d77a4c2b":"code","ee5d0a84":"markdown","ea535e09":"markdown","c28ea2f9":"markdown","a1452a88":"markdown"},"source":{"3d1aa99e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns # prettier plotting\n\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3728ef61":"# loading the data\ntrain_df=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n# setting this to see all columns\npd.set_option('display.max_columns', 1000)","3c4777e8":"# lets view the data\ntrain_df.head()","e91d6ee1":"# some information about the data, and the scale of the features\ntrain_df.describe().T","90a0aca7":"# a bit more information\ntrain_df.info()","dc03606d":"# missing count and percentge of feature\n# TODO make this a function\nmost_missing = train_df.isnull().sum().sort_values(ascending = False)\ntotal = most_missing[most_missing != 0]\npercent = total \/ len(train_df)\n\npd.concat([total, percent], axis=1, keys=['Total','Percent'])","dcf9de67":"# Checking correlation of features to the target variable\n(train_df.corr() ** 2)[\"SalePrice\"].sort_values(ascending = False)","8dc5b66b":"plt.subplots(figsize = (15,10))\nsns.scatterplot(train_df.OverallQual, train_df.SalePrice)","36e9a846":"plt.subplots(figsize = (15,10))\nsns.scatterplot(train_df.GrLivArea, train_df.SalePrice)","b2adc775":"# The two point to the bottom right are obvious outliers, so it will be good to remove them\ntrain_df = train_df.drop(train_df[train_df['GrLivArea']>4500].index).reset_index(drop=True)","e070ce53":"plt.subplots(figsize = (15,10))\nsns.scatterplot(train_df.GarageCars, train_df.SalePrice)","184458a8":"plt.subplots(figsize = (15,10))\nsns.scatterplot(train_df.GarageArea, train_df.SalePrice)","31827332":"plt.subplots(figsize = (15,10))\nsns.scatterplot(train_df.TotalBsmtSF, train_df.SalePrice)","cd9f27ee":"# Now lets check the skew of the target variable\n\nimport matplotlib.gridspec as gridspec\nfrom scipy import stats\n\nfig = plt.figure(constrained_layout=True, figsize=(10,5))\ngrid = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)\n\nax1 = fig.add_subplot(grid[0, 0])\nax1.set_title('Histogram')\nsns.distplot(train_df.SalePrice, norm_hist=True, ax = ax1)\n \nax2 = fig.add_subplot(grid[0, 1])\nax2.set_title('Prop_plot') \nstats.probplot(train_df.SalePrice, plot = ax2)","a1acfaaf":"# Since it is right skewed we will use log transform to normalize it\ntrain_df['SalePrice'] = np.log1p(train_df['SalePrice'])\n\nfig = plt.figure(constrained_layout=True, figsize=(15,5))\ngrid = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)\n\nax1 = fig.add_subplot(grid[0, 0])\nax1.set_title('Histogram')\nsns.distplot(train_df.SalePrice, norm_hist=True, ax = ax1)\n \nax2 = fig.add_subplot(grid[0, 1])\nax2.set_title('Prop_plot') \nstats.probplot(train_df.SalePrice, plot = ax2)","f1463ad5":"# Now lets check the correlation between our independent variables\n\nplt.subplots(figsize = (30,30))\nsns.heatmap(train_df.corr(), \n            annot=True, \n            center = 0, \n           );","0166c2aa":"# ok, now since we did all of this we will merge all the data together to transform it\n# this is not good for real-world applications, since it wont give the most accurate result, since it causes data leaking\n# but this is a competition and we have all the data, so we can ignore that, since this way we will get better results\n\nall_df = pd.concat((train_df, test_df)).reset_index(drop = True)\nall_df.drop(['SalePrice', 'Id'], axis = 1, inplace = True)","55239bc7":"all_df.head()","db142b8d":"# How many missing values\n\nmost_missing = all_df.isnull().sum().sort_values(ascending = False)\ntotal = most_missing[most_missing != 0]\npercent = total \/ len(all_df)\n\npd.concat([total, percent], axis=1, keys=['Total','Percent'])","f571289c":"# This cell will impute None, where the missing data actualy has a meaning\nfill_with_none = ['Alley', \n                  'PoolQC', \n                  'MiscFeature',\n                  'Fence',\n                  'FireplaceQu',\n                  'GarageType',\n                  'GarageFinish',\n                  'GarageQual',\n                  'GarageCond',\n                  'BsmtQual',\n                  'BsmtCond',\n                  'BsmtExposure',\n                  'BsmtFinType1',\n                  'BsmtFinType2',\n                  'MasVnrType']\n\nfor col in fill_with_none:\n    all_df[col] = all_df[col].fillna('None')","b32aaea2":"# Same as the cell above, but for numeric features\nfill_with_zero = ['BsmtFinSF1',\n                  'BsmtFinSF2',\n                  'BsmtUnfSF',\n                  'TotalBsmtSF',\n                  'BsmtFullBath', \n                  'BsmtHalfBath', \n                  'GarageYrBlt',\n                  'GarageArea',\n                  'GarageCars',\n                  'MasVnrArea']\n\nfor col in fill_with_zero:\n    all_df[col] = all_df[col].fillna(0)","df8f7b88":"# Imputing the most occurring value, there where the missing value is a missing value\nfill_with_mode = ['MSZoning', \n                     'Functional',\n                     'Utilities',\n                     'Exterior1st',\n                     'Exterior2nd',\n                     'KitchenQual',\n                     'SaleType', \n                     'Electrical',\n                    ]\n\nfor col in fill_with_mode:\n    all_df[col] = all_df[col].fillna(all_df[col].mode()[0])     ","a6dc1b0d":"# Converting some numerical variables to categorical, as they should have been that from the start\nconvert_to_string = ['MSSubClass',\n                     'OverallCond',\n                     'OverallQual',\n                     'YrSold', \n                     'MoSold']\n\nfor col in convert_to_string:\n    all_df[col] = all_df[col].astype(str)  ","991f229c":"# Lot frontage has a high dependency on Neighborhoods, so we impute with groupby\nall_df['LotFrontage'] = all_df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))","3d272529":"# Check for missing data\nmost_missing = all_df.isnull().sum().sort_values(ascending = False)\ntotal = most_missing[most_missing != 0]\npercent = total \/ len(all_df)\n\npd.concat([total, percent], axis=1, keys=['Total','Percent'])","240eb129":"# Now lets normalize the features\nfrom scipy.stats import skew\n\nnumeric_feats = all_df.dtypes[all_df.dtypes != \"object\"].index\nskewed_feats = all_df[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nskewed_feats","d8e2059b":"# Normalizing if skew is greater than 0.75\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nhigh_skew = skewed_feats[abs(skewed_feats) > 0.75].index\n\nfor col in high_skew:\n    all_df[col] = boxcox1p(all_df[col], boxcox_normmax(all_df[col] + 1))\n\n","d7f62b31":"# Check skew again\nskewed_feats = all_df[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nskewed_feats","3646ff20":"# Adding some true or false features(Has no effect on normal linear regresion, but does od modified versions, and other models)\nall_df['haspool'] = all_df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_df['has2ndfloor'] = all_df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_df['hasgarage'] = all_df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_df['hasbsmt'] = all_df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_df['hasfireplace'] = all_df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","7fa33264":"# Adding features for total SF and porch SF\nall_df['TotalSF'] = (all_df['TotalBsmtSF'] + all_df['1stFlrSF'] + all_df['2ndFlrSF'])\nall_df['Total_porch_sf'] = (all_df['OpenPorchSF'] \n                              + all_df['3SsnPorch'] \n                              + all_df['EnclosedPorch'] \n                              + all_df['ScreenPorch'] \n                              + all_df['WoodDeckSF']\n                             )","95d190de":"# Checking if there are any caterorical features that cant help us\ncategorical_feats = all_df.dtypes[all_df.dtypes == \"object\"].index\nfor col in categorical_feats:\n    print(col)\n    print(all_df[col].value_counts())","fc9ec471":"# Removing these because they dont give much info\nall_df = all_df.drop(['Street', 'Utilities', 'PoolQC',], axis=1)","5446adf8":"# Adding dummy variables(OHE)\nall_df = pd.get_dummies(all_df).reset_index(drop=True)\nall_df.shape","f5486ceb":"# Splitting the data\ny = train_df['SalePrice'].reset_index(drop=True)\nX = all_df.iloc[:len(y), :]\nX_final = all_df.iloc[len(y):, :]","3f965f6b":"X.head()","1640fff9":"# Basic linear regression\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .25)\n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(y_pred, y_test)\n\n","bb8d8474":"# Ridge\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n## Assiging different sets of alpha values to explore which can be the best fit for the model. \nalpha_ridge = [-3,-2,-1,1e-15, 1e-10, 1e-8,1e-5,1e-4, 1e-3,1e-2,0.5,1,1.5, 2,3,4, 5, 10, 20, 30, 40]\ntemp_rss = {}\ntemp_mse = {}\nfor i in alpha_ridge:\n    ## Assigin each model. \n    ridge = Ridge(alpha= i, normalize=True)\n    ## fit the model. \n    ridge.fit(X_train, y_train)\n    ## Predicting the target value based on \"Test_x\"\n    y_pred = ridge.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    rss = sum((y_pred-y_test)**2)\n    temp_mse[i] = mse\n    temp_rss[i] = rss\n\n\nfor key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))\n\n\nprint('x'*10)\n    \nfor key, value in sorted(temp_rss.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","cb844b47":"# Lasso\n\nfrom sklearn.linear_model import Lasso\ntemp_rss = {}\ntemp_mse = {}\nfor i in alpha_ridge:\n    ## Assigin each model. \n    lasso = Lasso(alpha= i, normalize=True)\n    ## fit the model. \n    lasso.fit(X_train, y_train)\n    ## Predicting the target value based on \"Test_x\"\n    y_pred = lasso.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    rss = sum((y_pred-y_test)**2)\n    temp_mse[i] = mse\n    temp_rss[i] = rss\n\n\nfor key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))\n\n\nprint('x'*10)\n    \nfor key, value in sorted(temp_rss.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","ff158aee":"# Elastic Net\n\nfrom sklearn.linear_model import ElasticNet\ntemp_rss = {}\ntemp_mse = {}\nfor i in alpha_ridge:\n    ## Assigin each model. \n    el_net = ElasticNet(alpha= i, normalize=True)\n    ## fit the model. \n    el_net.fit(X_train, y_train)\n    ## Predicting the target value based on \"Test_x\"\n    y_pred = el_net.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    rss = sum((y_pred-y_test)**2)\n    temp_mse[i] = mse\n    temp_rss[i] = rss\n\n\nfor key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))\n\n\nprint('x'*10)\n    \nfor key, value in sorted(temp_rss.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","ab974cf9":"# Gradient boosting\n\nfrom xgboost import XGBRegressor\n\ntemp_rss = {}\ntemp_mse = {}\nfor i in [1e-11, 5e-10, 1e-10, 5e-9]:\n    ## Assigin each model. \n    xgb = XGBRegressor(learning_rate=0.01,n_estimators=3000,\n                                     max_depth=3, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:squarederror', nthread=-1,\n                                     reg_alpha=abs(i))\n    ## fit the model. \n    xgb.fit(X_train, y_train)\n    ## Predicting the target value based on \"Test_x\"\n    y_pred = xgb.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    rss = sum((y_pred-y_test)**2)\n    temp_mse[i] = mse\n    temp_rss[i] = rss\n    print(i, sep=' ')\nprint()\n\n\nfor key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))\n\n\nprint('x'*10)\n    \nfor key, value in sorted(temp_rss.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","19464f35":"# Blending\n\nridge = Ridge(alpha= 0.5, normalize=True)\nridge.fit(X_train, y_train)\ny_pred_ridge = ridge.predict(X_test)\n\nlasso = Lasso(alpha= 0.0001, normalize=True)\nlasso.fit(X_train, y_train)\ny_pred_lasso = lasso.predict(X_test)\n\nelnet = ElasticNet(alpha= 0.0001, normalize=True)\nelnet.fit(X_train, y_train)\ny_pred_elnet = ridge.predict(X_test)\n\nxgb = XGBRegressor(learning_rate=0.01,n_estimators=3000,\n                                     max_depth=3, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:squarederror', nthread=-1,\n                                     reg_alpha=1e-10)\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)\n\ny_pred = (y_pred_ridge + y_pred_lasso + y_pred_elnet + y_pred_xgb) \/ 4","402b99e8":"# RMSE\nnp.sqrt(mean_squared_error(y_test, y_pred))","932ed926":"# Learning on entire dataset\n\nridge = Ridge(alpha= 0.5, normalize=True)\nridge.fit(X, y)\ny_pred_ridge = ridge.predict(X_final)\n\nlasso = Lasso(alpha= 0.0001, normalize=True)\nlasso.fit(X, y)\ny_pred_lasso = lasso.predict(X_final)\n\nelnet = ElasticNet(alpha= 0.0001, normalize=True)\nelnet.fit(X, y)\ny_pred_elnet = ridge.predict(X_final)\n\nxgb = XGBRegressor(learning_rate=0.01,n_estimators=3000,\n                                     max_depth=3, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:squarederror', nthread=-1,\n                                     reg_alpha=1e-10)\nxgb.fit(X, y)\ny_pred_xgb = xgb.predict(X_final)\n\ny_pred = (y_pred_ridge + y_pred_lasso + y_pred_elnet + y_pred_xgb) \/ 4","d77a4c2b":"# Creating submit file\n\ndata = {'Id': test_df['Id'],\n        'SalePrice': np.exp(y_pred)\n}\nsubmission_df = pd.DataFrame(data)\nsubmission_df.to_csv('houses.csv', mode='w', index=False)","ee5d0a84":"# Training a model","ea535e09":"# Transforming the data","c28ea2f9":"# Now we plot some of the most related features","a1452a88":"y_pred = (y_pred_ridge + y_pred_lasso + y_pred_elnet + y_pred_xgb) \/ 4 \n\ngives\n\n0.13156882915541787\n"}}