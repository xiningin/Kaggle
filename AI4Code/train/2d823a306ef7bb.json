{"cell_type":{"4ae1a1ba":"code","cd670747":"code","19521dd0":"code","fa2973e8":"code","60d09e97":"code","6f9f37e9":"code","861db29e":"code","64fbd216":"code","8378a359":"code","96ef213e":"code","c95b2dc1":"code","2d02fac4":"code","3cd23f2f":"code","502a3892":"code","a325036a":"code","e4f05cb0":"code","a80a248b":"code","5e713686":"code","8105b27f":"code","195dcee5":"code","d5e0183b":"code","81032647":"markdown","e7faa516":"markdown","424b5273":"markdown","0220fc97":"markdown","d2a701b5":"markdown","c59eca45":"markdown","b00cc2ce":"markdown","7540279f":"markdown","dd2e241a":"markdown","97892af3":"markdown","3d0d956d":"markdown","d7949845":"markdown","ea64dd37":"markdown","ab60f63a":"markdown","96fbf5df":"markdown","0f2076fb":"markdown","dc147866":"markdown","26298173":"markdown","8eaf5112":"markdown","0a6ba52a":"markdown","bbd1da34":"markdown","f5b7b4c5":"markdown","cd07b51e":"markdown","f50189ad":"markdown","da8f08f7":"markdown","71d9f433":"markdown","1b160f55":"markdown","be5cf943":"markdown","64b16b92":"markdown","5ef09595":"markdown","0981ef29":"markdown","6908df91":"markdown","6cd9cfd9":"markdown"},"source":{"4ae1a1ba":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt #data visualisation\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nprint(\"All libraries are imported\")","cd670747":"df= pd.read_csv(\"..\/input\/twitter-airline-sentiment\/Tweets.csv\")\ndf.head()","19521dd0":"print(\"Shape of the dataframe is\",df.shape)\nprint(\"The number of nulls in each column are \\n\", df.isna().sum())","fa2973e8":"print(\"Percentage null or na values in df\")\n((df.isnull() | df.isna()).sum() * 100 \/ df.index.size).round(2)","60d09e97":"del df['tweet_coord']\ndel df['airline_sentiment_gold']\ndel df['negativereason_gold']\ndf.head()","6f9f37e9":"print(\"Total number of tweets for each airline \\n \",df.groupby('airline')['airline_sentiment'].count().sort_values(ascending=False))\nairlines= ['US Airways','United','American','Southwest','Delta','Virgin America']\nplt.figure(1,figsize=(15,15))\nfor i in airlines:\n    indices= airlines.index(i)\n    plt.subplot(2,3,indices+1)\n    new_df=df[df['airline']==i]\n    count=new_df['airline_sentiment'].value_counts()\n    Index = [1,2,3]\n    plt.bar(Index,count, color=['blue', 'green', 'red'])\n    plt.xticks(Index,['negative','neutral','positive'])\n    plt.ylabel('Mood Count')\n    plt.xlabel('Mood')\n    plt.title('Count of Moods of '+i)","861db29e":"from wordcloud import WordCloud,STOPWORDS\n","64fbd216":"new_df=df[df['airline_sentiment']=='negative']\nwords = ' '.join(new_df['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\nplt.figure(1,figsize=(15, 15))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","8378a359":"new_df=df[df['airline_sentiment']=='positive']\nwords = ' '.join(new_df['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\nplt.figure(1,figsize=(15,15))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","96ef213e":"# Calculate highest frequency words in positive tweets\ndef freq(str): \n  \n    # break the string into list of words  \n    str = str.split()          \n    str2 = [] \n  \n    # loop till string values present in list str \n    for i in str:              \n  \n        # checking for the duplicacy \n        if i not in str2: \n  \n            # insert value in str2 \n            str2.append(i)  \n              \n    for i in range(0, len(str2)): \n        if(str.count(str2[i])>50): \n            print('Frequency of', str2[i], 'is :', str.count(str2[i]))\n        \nprint(freq(cleaned_word))","c95b2dc1":"#get the number of negative reasons\ndf['negativereason'].nunique()\n\n\nNR_Count=dict(df['negativereason'].value_counts(sort=False))\ndef NR_Count(Airline):\n    if Airline=='All':\n        a=df\n    else:\n        a=df[df['airline']==Airline]\n    count=dict(a['negativereason'].value_counts())\n    Unique_reason=list(df['negativereason'].unique())\n    Unique_reason=[x for x in Unique_reason if str(x) != 'nan']\n    Reason_frame=pd.DataFrame({'Reasons':Unique_reason})\n    Reason_frame['count']=Reason_frame['Reasons'].apply(lambda x: count[x])\n    return Reason_frame\n\n\n\ndef plot_reason(Airline):\n    \n    a=NR_Count(Airline)\n    count=a['count']\n    Index = range(1,(len(a)+1))\n    plt.bar(Index,count, color=['blue','yellow','red','orange','black','brown','gray','cyan','purple','green'])\n    plt.xticks(Index,a['Reasons'],rotation=90)\n    plt.ylabel('Count')\n    plt.xlabel('Reason')\n    plt.title('Count of Reasons for '+Airline)\n    \nplot_reason('All')  \nplt.figure(2,figsize=(15,15))\nfor i in airlines:\n    indices= airlines.index(i)\n    plt.subplot(2,3,indices+1)\n    plt.subplots_adjust(hspace=0.9)\n    plot_reason(i)","2d02fac4":"date = df.reset_index()\n#convert the Date column to pandas datetime\ndate.tweet_created = pd.to_datetime(date.tweet_created)\n#Reduce the dates in the date column to only the date and no time stamp using the 'dt.date' method\ndate.tweet_created = date.tweet_created.dt.date\ndate.tweet_created.head()\ndf = date\nday_df = df.groupby(['tweet_created','airline','airline_sentiment']).size()\n# day_df = day_df.reset_index()\nday_df","3cd23f2f":"day_df = day_df.loc(axis=0)[:,:,'negative']\n\n#groupby and plot data\nax2 = day_df.groupby(['tweet_created','airline']).sum().unstack().plot(kind = 'bar', color=['blue', 'green', 'red','yellow','orange','purple'], figsize = (15,8), rot = 70)\nlabels = ['American','Delta','Southwest','US Airways','United','Virgin America']\nax2.legend(labels = labels)\nax2.set_xlabel('Date')\nax2.set_ylabel('Negative Tweets')\nplt.show()","502a3892":"def tweet_to_words(tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",tweet) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return( \" \".join( meaningful_words )) ","a325036a":"df['clean_tweet']=df['text'].apply(lambda x: tweet_to_words(x))\n","e4f05cb0":"train,test = train_test_split(df,test_size=0.2,random_state=42)\n","a80a248b":"train_clean_tweet=[]\nfor tweet in train['clean_tweet']:\n    train_clean_tweet.append(tweet)\ntest_clean_tweet=[]\nfor tweet in test['clean_tweet']:\n    test_clean_tweet.append(tweet)","5e713686":"from sklearn.feature_extraction.text import CountVectorizer\nv = CountVectorizer(analyzer = \"word\")\ntrain_features= v.fit_transform(train_clean_tweet)\ntest_features=v.transform(test_clean_tweet)","8105b27f":"Classifiers = [\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=200)]","195dcee5":"dense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,train['airline_sentiment'])\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(dense_features,train['airline_sentiment'])\n        pred = fit.predict(dense_test)\n    accuracy = accuracy_score(pred,test['airline_sentiment'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))\n    print(classification_report(pred,test['airline_sentiment']))\n    cm=confusion_matrix(pred , test['airline_sentiment'])\n    plt.figure()\n    \n    plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\n    plt.xticks(range(3), ['Negative', 'Neutral','Positive'], fontsize=16,color='blue')\n    plt.yticks(range(3), ['Negative', 'Neutral','Positive'], fontsize=16,color='blue')\n    plt.show()","d5e0183b":"Index = [1,2,3]\nplt.figure(figsize=(15,10))\nplt.bar(Index,Accuracy,color=['orange','red','blue'])\nplt.xticks(Index, Model,rotation=45)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Accuracies of Models')","81032647":"- **Customer Service Issue** is the main neagtive reason for US Airways,United,American,Southwest,Virgin America\n- **Late Flight** is the main negative reason for Delta  \n- Interestingly, Virgin America has the least count of negative reasons (all less than 60)\n- Contrastingly to Virgin America, airlines like US Airways,United,American have more than 500 negative reasons (Late flight, Customer Service Issue)","e7faa516":"### Data Preprocessing","424b5273":"- The goal is to firstly get an idea of the most frequent words in negative tweets.\n- Get idea about most frequent words in positive tweets.","0220fc97":"* Words like **Thanks**, **best**, **customer** , **love**, **flying** , **good** are understandably present in the **most frequent** words of positive tweets. \n* However, other than these, most of the words are stop words and need to be filtered. We will do so later.\n* Lets try and visualize the reasons for negative tweets first !!","d2a701b5":"This shows the sentiments of tweets for each date from **2015-02-17** to **2015-02-24** for every airline in our dataframe.\n\nOur next step will be to plot this and get better visualization for negative tweets.","c59eca45":"Our dataframe has data from **2015-02-17** to **2015-02-24**\n\nIt will be interesting to see if the date has any effect on the sentiments of the tweets(*especially negative !*). We can draw various conclusions by visualizing this.","b00cc2ce":"Here **tweet_coord , airline_sentiment_gold, negativereason_gold**  have more than 90% missing data. It will be better to delete these columns as they will not provide any constructive information.\n\n","7540279f":"**The data is split in the standard 80,20 ratio.**","dd2e241a":"### Preprocessing the tweet text data","97892af3":"### Airline sentiments for each airline\n","3d0d956d":"To get a better idea, lets calculate the percentage of nulls or NA values in each column","d7949845":"The first step should be to check the shape of the dataframe and then check the number of null values in each column.\n\nIn this way we can get an idea of the redundant columns in the data frame depending on which columns have the highest number of null values.","ea64dd37":"### Wordcloud for Negative sentiments of tweets","ab60f63a":"- SVM(Support Vector Machine)\n- Decision Tree Classifier\n- Random Forest Classifier","96fbf5df":"- Interestingly, **American** has a sudden upsurge in negative sentimental tweets on **2015-02-23**, which reduced to half the very next day **2015-02-24**. (*I hope American is doing better these days and resolved their Customer Service Issue as we saw before*)\n- **Virgin America** has the least number of negative tweets throughout the weekly data that we have. It should be noted that the total number of tweets for **Virgin America** was also significantly less as compared to the rest airlines, and hence the least negative tweets.\n- The negative tweets for all the rest airlines is slightly skewed towards the end of the week !","0f2076fb":"### Lets try and calculate the highest frequency words in postive sentimental tweets","dc147866":"### Importing the libraries and loading the data","26298173":"### What are the reasons for negative sentimental tweets for each airline ?","8eaf5112":"**Breakdown of this notebook:**\n\n1. Loading the dataset: Load the data and import the libraries.\n2. Data Preprocessing:\n     - Analysing missing data. \n     - Removing redundant columns.\n3. Visualising and counting sentiments of tweets for each airline.\n4. Wordcloud plots for **positive** and **negative** tweets to visualise most frequent words for each.\n5. Analysing the reasons for **negative tweets** for each airline.\n6. Visualising negative tweet-sentiment relationship with dates.\n7. Predicting the tweet sentiments with tweet text data with:\n      - SVM(Support Vector Machine)\n      - Decision Tree Classifier\n      - Random Forest Classifier\n8. Calculating accuracies, plotting the confusion matrix and comparing the models.","0a6ba52a":"### Is there a relationship between negative sentiments and date ?","bbd1da34":"### Prediciting sentiments from tweet text data ","f5b7b4c5":"### Most used words in Positive and Negative tweets ","cd07b51e":"Wordcloud is a great tool for visualizing nlp data. The larger the words in the wordcloud image , the more is the frequency of that word in our text data.","f50189ad":"### References:- \nI learnt a lot from this blog which shows you how to handle nlp data and implement data preprocessing and explanatory visualization for better understanding.\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2018\/07\/hands-on-sentiment-analysis-dataset-python\/","da8f08f7":"- As we you can see above we have plotted the **Confusion Matrix** for predicted sentiments and actual sentiments (negative,neutral and positive)\n- **Random Forest Classifier** gives us the best accuracy score, precision scores according to the classification report.\n- The confusion matrix shows the TP,TN,FP,FN for all the 3 sentiments(negative,neutral and positive)\n  Here also **Random Forest Classifier** gives **better** results than the **Decision Tree Classifier** and **SVM**.\n  \n  ","71d9f433":"- Firstly lets calculate the total number of tweets for each airline\n- Then, we are going to get the barplots for each airline with respect to sentiments of tweets (positive,negative or neutral).\n- This will give us a clear idea about the airline sentiments and airlines relationship. ","1b160f55":"# Sentiment Analysis of Airline users using thier tweets.","be5cf943":"## Thanks for stay tuned....","64b16b92":"#### We will explore the **negative reason** column of our dataframe to extract conclusions about negative sentiments in the tweets by the customers ","5ef09595":"Now, we will clean the tweet text data and apply classification algorithms on it","0981ef29":" - United, US Airways, American substantially get negative reactions.\n - Tweets for Virgin America are the most balanced.","6908df91":"The code for getting positive sentiments is completely same with the one for negative sentiments. Just replace negative with positive in the first line.","6cd9cfd9":"### Wordcloud for positive reasons"}}