{"cell_type":{"5c9a1aba":"code","de33e353":"code","58949c56":"code","e9d8eb83":"code","493f924a":"code","c10ce1d2":"code","44760c15":"code","3e95dbdf":"code","f2c3e2f0":"code","20460e6c":"code","81f90ff9":"code","4fa9c8a2":"code","691946ae":"code","406ce790":"code","2a9905c3":"code","b2f9fd73":"code","c0348fd5":"code","e516d9e7":"code","cd52ec0f":"code","13e7c5ad":"code","73be4a0b":"code","7f5b1b89":"markdown","f64f12ad":"markdown","e4657bf7":"markdown","299df274":"markdown","1abb2204":"markdown","3aa907cc":"markdown","d69a0a7c":"markdown","ff3dcc19":"markdown","20e2145b":"markdown","12d9a3bc":"markdown","8d8a7207":"markdown","78f6cd85":"markdown","d8e2a093":"markdown","eb3b6cb2":"markdown","8a97d9ea":"markdown","ca6ca1c0":"markdown","0ad8d9e1":"markdown","192b3e74":"markdown","bae4c61d":"markdown","87f576b8":"markdown","8290c61d":"markdown","5ef854c2":"markdown","38635fef":"markdown","2c2f4af3":"markdown","c8185de8":"markdown","9373377d":"markdown","4ad83240":"markdown","7cbaaf8e":"markdown","2009432c":"markdown","65efcd60":"markdown","072d3796":"markdown","78065d5e":"markdown","b583d616":"markdown","03a4aade":"markdown","15d9dbc5":"markdown","f20ab743":"markdown","c21bc39c":"markdown","032e341e":"markdown","63fff367":"markdown","68533918":"markdown","95a6ff9d":"markdown","1ac62e45":"markdown","9ccd4224":"markdown","9ac749e1":"markdown","47bed993":"markdown","9a669ac3":"markdown","56701fb2":"markdown","6a62653b":"markdown","ae2442ca":"markdown","00c83fa4":"markdown","78a1e53f":"markdown","75eec084":"markdown","24e2b5cf":"markdown","69fc5d1b":"markdown","ef198732":"markdown","1c50329c":"markdown","97a1537b":"markdown","027b1fa4":"markdown"},"source":{"5c9a1aba":"# import the require libraries \nfrom bs4 import BeautifulSoup\nfrom urllib import request\nimport re\nimport numpy as np\nimport time\nimport os\n\n# uncomment if you are using this on Colab\n# from google.colab import files\n\n\n# the domain name of the website we are crawling is a global variable used by the crawler\ndomain_name = 'https:\/\/www.gsmarena.com\/'","de33e353":"# if using Google Colab, I use a list to collect the log of any issues\n# you give give a shot at refactoring this function to use the logging module\ndebug_collector = []\n\ndef collect_debug(error):\n  '''A function for logging any unexpected behaviour'''\n  global debug_collector\n#   print(error)\n  debug_collector.append(error)","58949c56":"# STEP 1\n\n# get meta data and links to all the makers in GSMArena  \ndef get_maker_links(url):\n  '''A function for getting links to all makers from the GSMArena makers list.\n \n     Takes in the url of the list of makers and returns a list of lists of the form\n     [[maker_name_1, maker_name_1, num_devices_1, maker_link_1],.....]\n  '''\n  # get the maker page and read it\n  page = request.urlopen(url)\n  html = page.read()\n  \n   # create the BeautifulSoup(bs) object \n  bs = BeautifulSoup(html, 'html.parser')\n\n  # find the div-tag which contains the table\n  table = bs.findChild('div', class_='st-text').table\n\n  # if there is no table in the seed page, log it \n  if not table:\n    error = 'Maker Page Error: has no brands table| function_name: {}| url: {}'.format(function_name, url)\n    collect_debug(error)\n  # if the table is present then get the the maker information from it\n  else:\n    # inside each table, the data in is maker is stored under td-tag which we collect using a list\n    rows = table.findChildren('td')\n    rows_collector = []\n    # takes in data as [index, maker_name, link, #phones]\n    for maker_id, row in enumerate(rows):\n    # get the maker name and maker link. if there is no a-tag, collect a log\n      row_a_tag = row.a\n      if not row_a_tag:\n        error = 'Maker Page Error: no row_a_tag| function_name: {}| url: {}| row_num: {}'.format(function_name, url, n)\n        collect_debug(error)\n      else:\n        maker_link = domain_name + row_a_tag['href']\n        # use the stripped_strings generator to get a tuple of the maker name and num of devices \n        # if you are wondering what is going in the line above, please check out comprehensions in Python\n        maker_name, num_devices = (item for item in row_a_tag.stripped_strings)\n\n        # extract the numerical portion of num_devices and convert it into an integer\n        num_devices = re.findall(re.compile('\\d+'), num_devices)[0]\n        num_devices = int(num_devices)\n        # append all of the maker data to the rows_collector and return it\n        rows_collector.append([maker_id, maker_name, num_devices, maker_link])\n  return rows_collector","e9d8eb83":"seed_path = 'makers.php3'\nseed_url = domain_name + seed_path \n\n# test out the function that we just created\nmaker_list = get_maker_links(seed_url)\nprint(maker_list)","493f924a":"# define a function for giving us the nav_page_num 1 of a maker given their name\ndef get_makers_link(maker_name):\n  '''A function for getting the link to a maker given the maker's name.\n  \n     Takes in a maker's name, say 'Samsung' returns 'https:\/\/www.gsmarena.com\/samsung-phones-9.php'\n     Maker name is case insenstive.\n  '''\n  # go through the maker_list and find list_item[1], i.e maker_name\n  global maker_list\n  if not maker_list:\n      error = 'No maker_list!'\n      collect_debug(error)\n  else:\n    for list_item in maker_list:\n      if maker_name.lower() == list_item[1].lower():\n        print('maker_link called for {} \\n{}'.format(maker_name, list_item[-1]))\n        \n  \n# test it out\nmaker_name = 'Samsung'\nget_makers_link(maker_name)\n\nmaker_name = 'samSung'\nget_makers_link(maker_name)","c10ce1d2":"# since we call a webpage and get the bs object of the page a lot, we can define a function to make it easier  \ndef get_bs(url, parser='html.parser'):\n  '''A function for returning the BeautifulSoup object of a webpage given its url\n  \n     Uses 'html.parser' by defualt and can be modified using the optional argument 'parser'\n  '''\n  # return the bs onject for a given webpage\n  page = request.urlopen(url)\n  html = page.read()\n  bs = BeautifulSoup(html, parser)\n  return bs","44760c15":"# since Samsung is the maker we want from maker_ist, we write a function to get the largest maker\ndef get_largest_maker():\n  '''A function that returns the maker data corresponding to largest maker from the maker_list'''\n  global maker_list\n  # compare and set the num_devices under each maker against this variable if num_devices > is_largest\n  is_largest = 0\n  maker_id = None\n  # iterate through all the maker's in maker_list and return the largest maker\n  for maker in maker_list:\n    num_devices = maker[2]\n    if num_devices > is_largest:\n      is_largest = num_devices\n      maker_id = maker[0]\n  return maker_list[maker_id]\n\n# test it out\nmaker = get_largest_maker()\nprint(maker)","3e95dbdf":"# STEP 2\n\n# iterate through each maker in the maker list and apply this function over the maker to get the nav_page_links\ndef get_nav_page_links(maker):\n  '''A function for getting all the nav pages under a given maker\n  \n    This function takes in a maker_list item of the form [maker_id, maker_name, num_devices, maker_link]\n    Returns a dict of the form {nav_page_num:nav_page_link} for the maker\n  '''\n  # unpack the items in the list\n  maker_id, maker_name, num_devices, maker_link = maker\n  # a dictionary that will be used to collect all the nav_pages for a given maker\n  maker_nav_pages = {}\n  # first add the landing page as nav_page_num = 1\n  maker_nav_pages[1] = maker_link\n  # get the maker's page\n  bs = get_bs(maker_link)\n\n  # find the div-tag containing the nav_pages\n  nav_pages = bs.findChild('div', class_='nav-pages')\n  # if the maker has no nav_pages, which is possible, collect it for logging\n  if not nav_pages:\n    error = '{} does not have nav_pages| maker_link: {}'.format(maker_name, maker_link)\n    collect_debug(error)\n  # otherwise we can get a list of all the nav_pages \n  else:\n    # insde this div tag, the pages we want are inside a-tags\n    nav_pages = nav_pages.findChildren('a', recursive=False)\n    for nav_page_num, nav_page in enumerate(nav_pages):\n        # nav_page_num needs to be offset by 2 before using as a key to add the nav page link\n        maker_nav_pages[nav_page_num + 2] = domain_name + nav_page['href']\n  return maker_nav_pages\n\n# test it out\nnav_page_links = get_nav_page_links(maker)  \nprint(nav_page_links)  ","f2c3e2f0":"# STEP 3\n\n# get the information about the devices present in a nav_page by iterating through the all the devices on that page\n# all the devices by a given maker are collected as elements in dictionary of the form\n# {Samsung:[device_1_data, device_2_data,.......], Acer:[device_1_data, device_2_data,....],....}\n\n# we also need to define a couple of global variables, which are results from the earlier functions\ndevices_collector = {}\nmaker_name = maker[1]\nmaker_link = maker[-1]\n\n# for nav_page in nav_pages, we will iterate through this function\nnav_page_links = get_nav_page_links(maker) \n\n# collect all devices by the makers in the devices_collector dict using maker_name's as the key\ndevices_collector[maker_name] = [] \ndef get_device_links(nav_page_link, devices_collector, maker_name):\n  '''A function to get the device links and device info for all devices in a nav page\n     This function will be called for every device in GSMArena when used with the crawler\n  '''\n  # unpack the \n#   global devices_collector, maker_name\n  # get the nav_page\n  bs = get_bs(nav_page_link)\n\n  # get the list items under the div-tag with the class name 'makers'\n  devices = bs.findChild('div', class_='makers').ul\n  devices = devices.findChildren('li', recrusive=False)\n\n  # iterate through each device and collect the device_name, device_info, device_img_link, device_link\n  page_device_collector = []\n  for device_num, device in enumerate(devices):\n    device_name = device.get_text()\n    # we cannot collect the link for a device if it does not have an a-tag\n    if not device.a:\n        error = \"{} does not have a link : nav_page: {}| maker_name {}: \".format(device_name, nav_page_link , maker_name)\n        collect_debug.append(error)\n    else:\n      device_link = domain_name + device.a['href']\n      # img_link, and title are stored in the img tag\n      img_tag = device.a.findChild('img')\n      if not img_tag:\n        error = \"{} does not have a img_tag| nav_page: {}| maker_name: {}\".format(device_name, nav_page_link , maker_name)\n        collect_debug.append(error)\n      else:\n        device_img_link = img_tag['src']\n        device_info = img_tag['title']\n    page_device_collector.append([device_name, device_info, device_img_link, device_link]) \n  # concat the device info from this nav page onto what is already present on the list\n  devices_collector[maker_name] += page_device_collector\n\n# test it out \nfor nav_page_num, nav_page_link in nav_page_links.items():\n  get_device_links(nav_page_link, devices_collector, maker_name)\n  \nprint(devices_collector.keys())\nprint(devices_collector['Samsung'])\nprint(devices_collector['Samsung'].__len__())","20460e6c":"# get the data for Samsung Galaxy S10 (devices[3])so that we can build a sample crawler for a device \ndevices = devices_collector['Samsung']\ndevice = devices[3]\n\n# to get the device information, the functions take in each of these device info as attributes\ndevice_link = device[-1]\ndevice_name = device[0]\n\n# a collector dict which consolidates all features, including the banner for a device\nspecs_collector = {}","81f90ff9":"# get the spec_sheet for a device from the device_link\ndef get_device_specs(bs, specs_collector, device_name, device_link):\n  '''A function for findinf the specs tabele of a device given a bs object of the device webpage'''\n  # the specs are stored inside individual tables, so find them all\n  specs_tables = bs.findChildren('table')\n  if not specs_tables:\n    error = '{} has no specs_tables| device_link: {}'.format(device_name, device_link)\n    collect_debug.append(error)\n  # if the phone does have a spec-list\n  else:\n    # get the spec category like 'Network', 'Launch', 'Memory', 'Battery', ...\n    for table in specs_tables:\n      # find all the rows in the table \n      table_rows = table.findChildren('tr', recursive=False)\n      # each table will only hacve one child th-tag, i.e a header\n      # this header of the table is the name of the spec\n      table_header = table.findChild('th').get_text(strip=True)\n\n      # for row in tables: if the class = 'ttl' or 'nfo', it is a column in the table\n      # 'ttl' tags correspond to a potential feature that we could extract such as Dimension, Weight, Date Announced, etc..\n      # 'nfo' coresponds to a actual data point corresponding to the 'ttl' feature\n      ttl_collector = {}\n      for row_num, row in enumerate(table_rows):\n        ttl_tag = row.findChild('td', class_='ttl')\n        nfo_tag = row.findChild('td', class_='nfo')\n\n        # if neither the ttl_tag or nfo tag are present, we want to log it\n        if (not ttl_tag) or (not nfo_tag):\n          error = '{} has ttl-tag OR nfo-tag| device_link:{}'.format(device_name, device_link)\n          collect_debug(error)\n          # we also want to set the text to NaN if a column is empty so that it can later be processed easily \n          ttl_tag_text = np.NaN\n          nfo_tag_text = np.NaN\n        # if either the ttl_tag or nfo tag are present, we want to collect them and log any missing values\n        else:\n          if not ttl_tag:\n            error = '{} has no ttl-tag| device_link:{}'.format(device_name, device_link)\n            collect_debug(error)\n          else:\n            ttl_tag_text = ttl_tag.get_text(strip=True)\n            if ttl_tag_text == '\\xa0' or ttl_tag_text == '':\n              ttl_tag_text = np.NaN\n\n          if not nfo_tag:\n            error = 'No nfo-tag: {}: {}: {}'.format(n, link, row)\n            collect_debug(error_mess)\n          else:\n            nfo_tag_text = nfo_tag.get_text(strip=True)\n            if nfo_tag_text == '\\xa0' or nfo_tag_text == '':\n              nfo_tag_text = np.NaN\n        # add the values of the ttl-tag and nfo-tag as key value pairs\n        ttl_collector.setdefault(ttl_tag_text, nfo_tag_text)\n      # add the table header and the collected attribute value pairs to the specs_collector\n      specs_collector.setdefault(table_header, ttl_collector)\n      \n      \n# test it out\nbs = get_bs(device_link)\nget_device_specs(bs, specs_collector, device_name, device_link)\nfor key, value in specs_collector.items():\n  print('{} : {}'.format(key, value))\n","4fa9c8a2":"# ge the device banner data and add it to the specs_collector with the key 'Banner'\ndef get_device_banner(bs, specs_collector, device_name, device_link):\n  '''A function to scrape data from the banner of a a device'''\n  # get the unordered list with the class name 'specs-spotlight-features'\n  banner = bs.findChild('ul', class_='specs-spotlight-features')\n  # if a banner is not present, collect the information for dbugging\n  if not banner:\n    error = '{} has no banner| device_link:{}'.format(device_name, device_link)\n    collect_debug(error)\n  # else get all the list items and find the data stored in the banner\n  else:\n    banner_items = banner.findChildren('li')\n    banner_specs_collector = {}\n    for list_item in banner_items:\n      # find all the items in the list falling into the data-spec category, such as battery-hl, screen-hl, etc...\n      banner_specs = list_item.findChildren(['span', 'div'], {'data-spec':re.compile('.*')})\n      # for each spec in the banner iterate through the key value pairs and add it to banner_spec_collector\n      for banner_spec in banner_specs:\n        banner_spec_name = banner_spec['data-spec']\n        if banner_spec_name:\n          # setting strip = True removes any white space space characters\n          banner_spec_value = banner_spec.get_text(strip=True)\n          if banner_spec_value:\n            banner_specs_collector[banner_spec_name] = banner_spec_value\n\n      # we now need to find the device popularity and hits from the webpage\n      if 'help-popularity' in list_item['class']:\n        # get information about the device's popularity and collect debug if it does not have the attribute\n        device_popularity = list_item.findChild('strong')\n        if not device_popularity:\n          error = '{} has no device_popularity| device_link:{}'.format(device_name, device_link)\n          collect_debug(error)\n        else:\n          device_popularity = device_popularity.get_text()\n          # do no capture the Unicode white space character '\\xa0'\n          if device_popularity == '\\xa0' or device_popularity == '' :\n            device_popularity = None        \n        # collect information about the device's popularity and collect debug if it does not have the attribute          \n        device_hits = list_item.findChild('span')\n        if not device_hits:\n          error = '{} has no device_hits| device_link:{}'.format(device_name, device_link)\n          collect_debug(error)\n        else:\n          device_hits = device_hits.get_text()\n          if device_hits == '\\xa0'or device_hits == '':\n            device_hits = None\n\n        # add device_popularity and divice_hits to the banner_specs_collector if they are present\n        if device_popularity:\n          banner_specs_collector['device_popularity'] = device_popularity\n        if device_hits:\n          banner_specs_collector['device_hits'] = device_hits\n    specs_collector['Banner'] = banner_specs_collector\n\n# test it out\nget_device_banner(bs, specs_collector, device_name, device_link)\nfor key, value in specs_collector.items():\n  print('{} : {}'.format(key, value))\n  \n# we can see that the Banner has now been successfully added to the specs_collector","691946ae":"# the last thing we want to grab from the device page is the 'Total user opinions' at the bottom of the page\ndef get_device_opinions(bs,specs_collector, device_name, device_link):\n  '''A function to get the 'Total user opionions' for a device form the devie pages bs object'''\n  opinions = bs.findChild('div', id='opinions-total')\n  if not opinions:\n    error = '{} has no Total user opinions| device_link:{}'.format(devie_name, device_link)\n    collect_debug(error)\n  else:\n    num_opinions = opinions.b.get_text(strip=True)\n    specs_collector.setdefault('Opinions', num_opinions)\n  \n\n# test it out\nget_device_opinions(bs, specs_collector, device_name, device_link)\nget_device_banner(bs, specs_collector, device_name, device_link)\nfor key, value in specs_collector.items():\n  print('{} : {}'.format(key, value))","406ce790":"# put everything we have made so far for collecting the specs of a device into a single function\ndef get_device_data(device_link):\n  '''A function to get the banner data and spec-sheet from a device on GSMArena\n  \n     Takes in a device's url and returns a dict with all the specs\n  '''\n  specs_collector = {}\n  # get the devie bs object\n  bs = get_bs(device_link)\n  device_name = bs.findChild('h1', class_='specs-phone-name-title').get_text()\n  # get te device_specs\n  specs = get_device_specs(bs,specs_collector, device_name, device_link)\n  # get the banner using the get_device_banner method, defined below\n  banner = get_device_banner(bs, specs_collector, device_name, device_link)\n  # get the user opinions for the device\n  opinions = get_device_opinions(bs,specs_collector, device_name, device_link)\n  # get the banner spec_sheet using the get_device_specs method, defined below \n  return specs_collector\n\n    \n# test it out\ndevice_link = 'https:\/\/www.gsmarena.com\/samsung_galaxy_s10-9536.php'\nget_device_data(device_link)","2a9905c3":"# helper function that allows returns the maker_id of of maker\ndef get_maker_id(name_of_maker, maker_list):\n  '''A function for returning the maker_id of a maker given the maker_name.\n     \n     This function is case insensitive.\n  '''\n  for maker in maker_list:\n    name = maker[1]\n    maker_id = maker[0]\n    if name_of_maker.lower() == name.lower():\n      return maker_id\n  # if a name is not found in the maker list, we want to throw an exception and collect it for log\n  raise NameError('GSMArena has no maker \\'{}\\''.format(maker_name))\n  \n  \ndef switch(maker_id, name_of_maker, maker_list):\n  '''A function for returning a bool which tells the crawler which maker(s) to scrape for data.'''\n  # if no name_of_maker is given, return true in all cases\n  if name_of_maker is None:\n    return True\n  # else get maker_id for the given maker name and return True only when current maker_id == given maker_id\n  else:\n    given_maker_id = get_maker_id(name_of_maker, maker_list)\n    if given_maker_id == maker_id:\n      return True\n    else:\n      return False","b2f9fd73":"# assemble the functions we built earlier in the right format in order to get the functionality we want\n\ndef GSMCrawler(seed_url, name_of_maker=None):\n  '''A crawler to return device data from GSMArena\n      \n     Takes in the seed_url 'https:\/\/www.gsmarena.com\/makers.php3'.\n     If name_of_maker is specified, device info for will be collected only for that maker.\n  '''\n  # we want to measure how long the crawling took to excecute\n  start_time = time.time()\n# STEP 1: get the links to all the makers in GSMArena\n  print('Starting GSMArena Crawler...\\n')\n  maker_list = get_maker_links(seed_url)\n#   maker_list = get_maker_links(seed_url)\n  print('Successfully retrived maker_list!\\n')\n  \n  # tell us if we the crawl is being done for a single maker or all makers\n  if name_of_maker is None:\n    print('Crawling for devices by ALL makers...\\n')\n  else:\n    print('Crawling for devices by {}...\\n'.format(name_of_maker))\n  \n# STEP 2: iterate trough each maker and get the device links and device info from all the nav pages\n  devices_collector = {}\n  for maker_id, maker in enumerate(maker_list):\n    if switch(maker_id, name_of_maker, maker_list):\n      maker_link = maker[-1]\n      maker_name = maker[1]\n\n# STEP 3: the first thing we want to do on the makers page is to get a list of all nav links\n      nav_pages_links = get_nav_page_links(maker)\n      # for each nav page in a maker's nav_pages, get the device info for all devices by that maker\n      print('Getting nav_page_links for {}...\\n'.format(maker_name))\n      devices_collector[maker_name] = []\n      for nav_page_num, nav_page_link in nav_pages_links.items():\n        get_device_links(nav_page_link, devices_collector, maker_name)\n      print('Successfully collected all device info for {}!\\n'.format(maker_name))\n      \n  # notify us of how many devices were collected in total \n  total_num_devices = 0\n  for maker, devices_info in devices_collector.items():\n    total_num_devices += devices_info.__len__()\n  print('Successfully collected info for all devices! {} devices were collected\\n'.format(total_num_devices))\n\n# STEP 4: go through each each device_link in the devices_collector and pass it onto get_device_data\n  print('Collecting spec sheets for all devices. This could take a while. Sit back and relax...\\n')\n# WARNING: This loop will scrape the spec sheet of every device in GSM Arena.\n # it is good practice to put this under a try block; in case some thing we want to collect some debug info\n  try:\n    for maker, devices_info in devices_collector.items():\n      print('Getting spec sheets for {} devices by {}\\...n'.format(devices_info.__len__(), maker))\n      for device_num, device in enumerate(devices_info):\n        device_link = device[-1]\n        # get the device data using the get_device_data function we defined earlier\n        device_specs =  get_device_data(device_link)\n        device.append(device_specs)\n# WARNING END\n      print('Successfully scraped info for all devices by {}\\n!'.format(maker))\n  \n  except Exception as e :\n      error = 'Device crawl exception: {}| device_name: {}| device_links:{}\\n'.format(e, device_link)\n      collect_debug(error)\n  # if nothing went wrong, let us know that all has gone well\n  else:\n      end_time = time.time()\n      print('GSMCrawler has completed excecuting! All credits for this data goes to the GSMArena team\\n')\n      print('Time time required to excecute for {}: {} seconds'.format(end_time - start_time))\n      print('Time time per : {} seconds'.format(end_time - start_time))\n      print('='*50)\n  finally:\n    # finally return the data_collector\n    return devices_collector","c0348fd5":"# try out our newly built crawler\n\nseed_path = 'makers.php3'\nseed_url = domain_name + seed_path \n\n# due to Colab's limitations, I will run the crawler only for Samsung\n# you can find the data the full set of devices on my GitHub page under the name devices_data.json\ndevices_collector = GSMCrawler(seed_url,'Samsung')\n\n# uncomment the code below to run the crawler for the full site\n# devices_collector = GSMCrawler(seed_url)\n\nprint(devices_collector.keys())\nprint(devices_collector['Samsung'])","e516d9e7":"# we want to convert the data that we just collected into a JSON oject to interact with later \ndef make_devices_json(devices_collector, save_json=False):\n  '''A function for coverting devices_collector text into a JSON obj and optionally saving the file\n     If save_json is True, a file called devices_data.txt will be made in your current working directory\n  '''\n  json_dict = {}\n  for maker, devices_info in devices_collector.items():\n    maker_dict = {}\n    for device_id, device in enumerate(devices_info):\n      device_dict = {}\n      device_name, device_info, device_img_link, device_link, device_specs = device\n\n      # start adding data as key value pairs into the device_dict\n      device_dict['device_name'] = device_name\n      device_dict['device_info'] = device_info\n      device_dict['device_img_link'] = device_img_link\n      device_dict['device_link'] = device_link\n      device_dict['device_specs'] = device_specs\n\n\n      # use the device_id as key to to set the device\n      maker_dict[device_id] = device_dict\n    # set the maker id to the json_dict with the maker name as key\n    json_dict[maker] = maker_dict\n    \n  # if save json is true, then save the devices collected by the crawler in the working directory as a json file\n  if save_json:\n    cwd = os.getcwd()\n    save_file_name = cwd + '\/devices_data.txt'\n    with open(save_file_name, 'w', encoding='utf-8') as file:\n      json.dump(json_dict, file, ensure_ascii=False)\n    # notify us where the file was saved\n    print('Successfully saved device data as a JSON file at {}'.format(save_file_name))\n    \n  return json.dumps(json_dict, ensure_ascii=False)\n  \n# test it out\ndevices_json = make_devices_json(devices_collector, save_json=True)\ndevices_json","cd52ec0f":"# verify that the newly created json file is present in you local directory\n!ls","13e7c5ad":"# if you are using Colab and want to download the file we just created\nfiles.download('devices_data.txt')","73be4a0b":"def read_devices_json(file_path):\n  '''A function for reading in a JSON obj of the devices data i.e devices.txt\n     \n     Takes in the string file_path\n  '''\n  with open(file_path, 'r', encoding='utf-8') as file:\n    return json.load(file)\n\n# test it out\ncwd = os.getcwd()\nfile_path = cwd + '\/devices_data.txt'\njson_dict = read_devices_json(file_path)\n\n# double check that we have a dictionary \njson_dict.keys()","7f5b1b89":"In this tutorial, we will work on actually gathering the data before passing it on for processing process which will be covered in next week's tutorial. Our aim here is to develop a web crawler, which can traverse the website [GSMArena](https:\/\/www.gsmarena.com\/) and gather the spec sheets for all devices stored by them in over 9000+ webpages.\n\n**Disclaimer**\n1. All the information that we gather here is made available through the hard work of the GSMArena team and all credits go to them for it. \n2. **ALWAYS** check the ['robot.txt'](https:\/\/en.wikipedia.org\/wiki\/Robots_exclusion_standard) of the website that you are trying to crawl. GSMArena does not forbid crawling for the information that we collecting here, but I request that you do not strain their servers by creating unnecessary requests.\n3. I am breaking my own rule. A quick search reveals that [others](hhttps:\/\/www.google.com\/search?rlz=1CAWOMZ_enUS810&ei=VN2EXJP7EOSN5wKx-q-oCQ&q=gsm+arena+dataset&oq=gsm+arena+data&gs_l=psy-ab.1.0.35i39.893.2883..5425...0.0..0.99.1026.13......0....1..gws-wiz.......0i71j0j0i22i30j0i22i10i30j0i131i10j0i20i263j0i131i20i263.ICV5YZv6D1I) have already assembled an almost identical dataset, but I still decided to go ahead with it any way because learning how to build a web crawler is an indespensable tool to have in my arsenal and I could not find any acessible tutorials online that went as deep as we are going to in a building a crawler. As a personal rule, I refrained from taking a look at these projects project and the dataset they collected, until I was done building this crawler since I wanted to give the problem a shot without an *answer key* already given to me.","f64f12ad":"Once the **nav_pages** for a maker (Samsung in this case) have been collected, we can collect the device info and most importantly the **device_link** for every device by that maker.","e4657bf7":"Now that we have the functionality to find all the device information as well the **device_links** for the devices under a maker, the last component that we need is for the crawler to be able to visit a device and collect all the information for a device from its **banner** and **spec_sheet**. First, let us use *Inspect Elements* to examine the device page for the [Samsung Galaxy S10](https:\/\/www.gsmarena.com\/samsung_galaxy_s10-9536.php)","299df274":"1. Some of the \"phones\" listed in GSMArena are smart-watches, tablets, and other devices. Our crawler will be device agnostic and grab data on all the devices.\n2. Another convenient reason for why I chose Samsung is that it has the most number of devices in **maker_list**. Below we will define a function to check and verify this, but is is not necessary for the crawler to work.\n3. The 1174 devices listed under Samsung are spread across 14 pages. We can see this at the very bottom of the webpage (*see image below*). The crawler we make has to traverse each of those pages. Ideally, you have to account for a clickable JavaScript element to get a list of all the pages. Since Samsung has, by far, more devices than other manufacturers, and consequently the maximum number of such **nav_pages** all listed on its **maker_page**, we do not have to worry about this. A quick examination using *Inspect Elements* shows us that all the **nav_page** links are clearly structured in the page HTML and hence are easy to extract.","1abb2204":"### Gathering Data From the Endpoint","3aa907cc":"The way things are set up now, there is a lot of scope for expanding the functionality of the crawler. Building all the base functions required for the crawler to work took up more time than I had anticipated. But I think it was well worth it because there is a lot of potential in this rich dataset. One obvious next step is to implement **classes** for makers and devices. I will follow up on this soon, but we have work to do before that.\n\nNext week, it will be function city as we will be writing a **LOT** of functions to parse the data we just collected to extract meaningful information from it. The **re** module will be invaluable for this process, so please read up on it if you are following along. We will also be using [**pandas**](https:\/\/pandas.pydata.org\/) to handle and manipulate the data and [**matplotlib**](https:\/\/matplotlib.org\/) to visualize it before moving on to [exploratory data analysis](https:\/\/towardsdatascience.com\/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184) and hardcore ML in subsequent weeks.\n\nThe last thing I have for you are a few lessons which I learned along the way. I am leaving these here in the hopes that they will \"[be a light to you in dark places, when all other lights go out\"](https:\/\/www.goodreads.com\/quotes\/140704-may-it-be-a-light-to-you-in-dark-places).","d69a0a7c":"### Motivation Behind This Series","ff3dcc19":"Taking a look at the maker page on Screen 2, we see that the information regarding the maker name and the link to their page (example of [Samsung's](https:\/\/www.gsmarena.com\/samsung-phones-9.php) page) is stored as a table under a *div-tag* with a *class* name 'st-text'. There are three pieces of information that we can grab from each row in the table (*tr-tag*).\n1. **maker_name** (Samsung)\n2. **num_devices** (1174 devices)\n3. **maker_link**('samsung-phones-9.php') | visit [this](https:\/\/doepud.co.uk\/blog\/anatomy-of-a-url) to get an understanding of how an url is structured\n\nTo get these three values we need to iterate over the rows of the table. The maker link is stored as the value of the attribute *href* of the \"*dictionary*\" *a-tag*. The maker name is stored as the text of the *a-tag*. Finally, the number of devices are stored as the text of a *span-tag*.\n\nWe first make an HTTP request to get the maker page (seed page) using the **urllib.request** module and then use **BeautifulSoup** to find the tags where the information we want is stored. To parse and extract specific information from a tag's text, we use the Python Regular Expression (RegEx) module **re**. Since there have already been great tutorials on using all these libraries, I direct you to their documentation and also to tutorials that I found useful to gain an understanding of their attributes and methods.\n\n- Regular Expression\n  - [re Documentation](https:\/\/docs.python.org\/3\/library\/re.html)\n  - A very through [tutorial](https:\/\/www.youtube.com\/watch?v=sa-TUpSx1JA) tutorial on RegEx in Python by Corey Schafer. All his tutorials are great and I highly recommend that you check his content out.\n- BeautifulSoup \n  - [BeautifulSoup Documentation](https:\/\/www.crummy.com\/software\/BeautifulSoup\/bs4\/doc\/#)\n  - Corey's [video](https:\/\/www.youtube.com\/watch?v=ng2o98k983k) on BeautifulSoup\n- urllib\n  - [urllib.request Documentation](https:\/\/docs.python.org\/3\/library\/urllib.request.html#module-urllib.request)\n  - [Video] on urllib by Socratica\n  \n  \nIf you do not have familiarity with the above modules, my suggestion is to watch the videos at 1.5x speed and then refer back to this tutorial. I know that this is a lot of prerequisite  to cover, but learning this stuff is well worth it. In particular the BeautifulSoup documentation is very accessible and it will be worth your time to go through it even though we will only be using a handful of its methods and features. **Note** that **re**, [**os**](https:\/\/www.youtube.com\/watch?v=tJxcKyFMTGo&t=301s), [**numpy**](http:\/\/www.numpy.org\/) and [**time**](https:\/\/docs.python.org\/3\/library\/time.html) are not very important for this tutorial, but we will be using them a lot for the upcoming tutorials and I suggest that you take the opportunity to learn about them.","20e2145b":"![Seed page insepct element](https:\/\/i.imgur.com\/r67Gy60.jpg)","12d9a3bc":"### First Steps Towards Building A Web Crawler","8d8a7207":"###Putting it All Together","78f6cd85":"If you are interested in reading in **devices_data** form a JSON file on your device, use the following function.","d8e2a093":"Some of the thing that I want to point out so that you can get the best out these tutorials are:\n1. You must have a beginner to an intermediate level understanding of Python to follow along. I do not cover the basics since there are those who are much more qualified than me to teach you about it. [YouTube](https:\/\/www.youtube.com\/watch?v=YYXdXT2l-Gg&list=PL-osiE80TeTt2d9bfVyTiXJA-UTHn6WwU) and [Medium](https:\/\/medium.com\/topic\/programming) are great to get you started.\n2. In the case of libraries, I will refer to their documentation, GitHub posts on any specific issues that I faced while using them and also any tutorials which I found helpful to gain a good understanding of what I needed to get this project going. Please refer to them if you need help understanding the methods and attributes in a library.\n3. Although I encourage you to follow along with this tutorial, my suggestion is that you read through it first to understand how to think through the problem, and then use the skills that you learn here to solve a problem that **you find interesting**.\n4. I will refer to other channels, pages, blogs, etc whenever possible. To learn the skills you need to solve a large problem, there will be a lot of Googling and downtime involved to learn the necessary concepts and tools.\n5. Everything that I have written here is only meant as a template on how this particular problem can be approached. There are numerous thing which could be done to my code to improve its speed and readability. Since I want to put up tutorials on a weekly basis, I will not be refactoring my code here, but you can always find the latest version of this crawler on my [GitHub](https:\/\/github.com\/vigvisw) page.\n\n**If you are an experienced developer and you see any mistakes in my code or see places where I can make improvements**, I request you to please point them out to me.\n","eb3b6cb2":"### Defining the Data","8a97d9ea":"**A NOTE ON THE CODE**","ca6ca1c0":"### Framing The Problem","0ad8d9e1":"Go through a few more examples until you feel comfortable with the information that you have to collect. But, do not look at more than a few examples. We are all human and get a little lost in exploring data which interests us. But,  **observation of the data biases the result**. It is easy enough to follow best practices such as the creation of test, train and development sets of the data such that you only see the development set when you have well-structured data (such as one from a CSV file). However, when web crawling, you inevitably have to take a close look at at least a few examples of your data to understand the best way to parse it from the webpage. This is a very deep topic that could take an entire module to cover and I urge you to do you own reserach about it. To understand more about bias in ML models and how to avoid it, check out [this](https:\/\/towardsdatascience.com\/preventing-machine-learning-bias-d01adfe9f1fa) article.\n\nIn order to avoid observing the specifc data points in too much detail, we will use the S10 example for building most of the crawlers features.","192b3e74":"We want to collect all the data we can about a phone from GSMArena. Cleaning and making sense of the data is the next step and will be covered in detail in the next tutorial.\n\nTo understand what we are dealing with, let us look at an example of the webpage (i.e the spec sheet) which we will be crawling. We will take a look at the GSMArena entry for a phone. The example that we will use for the tutorial is the newly released [Samsung Galaxy S10](https:\/\/www.gsmarena.com\/samsung_galaxy_s10-9536.php). ","bae4c61d":"![Total user opinions](https:\/\/i.imgur.com\/m54SzpW.jpg)","87f576b8":"Examining the maker page, we can see that the list of all brands are arranged alphabetically on a single webpage. In most cases (as we will see), when a page has a number of items (say phones from a particular maker or search results for a product on Amazon), they are split and distributed across multiple webpages and stored as separate links, which can be found on a *nag page* tab. The crawler will have to be able to find all such *nav page links*. We will not worry about it in the case of the maker page, since all the information we want is listed on a single page.\n\nJust like an explorer in an unknown terrain, we need to assess the HTML structure of the weppage before we attempt to traverse it. I will assume that you are using Chrome (highly recommended) for this and all other tutorials, but I am sure that a simple Google search can help you figure out how to do the inspect the elements of the page for your browser.\n\nOn the list of all makers page linked above, use **Crtl + Shift + I** if you are using Chrome on Windows or ChromeOS and **Option + Cmd + I** if you are on a Mac. Check out [this](https:\/\/www.wikihow.com\/Inspect-Element-on-Chrome) Wikihow link for more information on how to inspect the elements of your webpage.\n\nUsing the keyboard shortcuts should prompt your browser to go into this cool split screen mode. To make things unabmiguous, I will refer to the normally displayed human readable half of the screen as Screen 1 and the screen which is displaying the html elements as Screen 2.","8290c61d":"![A typical Machine Learning project (Courtesy: Western Digitial)](https:\/\/2s7gjr373w3x22jf92z99mgm5w-wpengine.netdna-ssl.com\/wp-content\/uploads\/2018\/09\/WD_3.png)","5ef854c2":"Use the same procedure that I described in the above section to dig deeper into the HTML tags to find the information that you need. \n\nThe device data stored in the banner is mostly a repeat of what is in the spec sheet, but I will grab it regardless because calling the webpage itself is the most time-consuming process. The **banner** also contains the **web_hits** and the **popularity** of given device. I did not find the 'Become a Fan' attribute interesting or rich enough for analysis, so I did not grab it. \n\nUse *Inspect Elements* and the comments in the code to help you break down the HTML structure for the **banner** and the **spec_sheet**.","38635fef":"When it comes to a large project like this and what will follow, there will be a lot of lines of code involved. In the beginning, it was really difficult for me to wrap my head around all the functions that the program would have to perform. This is an area where Jupyter Notebooks became very useful. You can build up your program function by function and line by line without having to execute everything from scratch. Especially in the case of a web crawler, sending out HTTP requests and getting back responses can be the most time-consuming block of the code. In such cases, using the *cells* of a Jupyter Notebook becomes very handy. \n\nWe want to build up our program in pieces. If we put everything inside a **for** loop, we will eventually find it very difficult to understand what is going on and debugging the code becomes a nightmare. The strategy I use is to split up the problem into sensible blocks (1. get a list of all makers, 2. get a list of all nav pages for a maker, etc..) and then define functions which perform the vebose tasks that needs to be performed on the blocks. This makes the process of debugging, refactoring and repurposing your code much easier. The next step would be to create a Python **class** that incorporates all these functions. To keep the length and scope of this tutorial manageable, we will not be defining a class for the crawler. I leave it up to the motivated reader to do so. Note that I will, however, be following up on this over at my [GitHub](https:\/\/github.com\/vigvisw) page in the near future. \n\nA few useful videos to get you started are:\n1. Functions\n  - [Python Tutorial for Beginners 8: Functions](https:\/\/www.youtube.com\/watch?v=9Os0o3wzS_I) by Corey Schafer\n  - [Python Functions](https:\/\/www.youtube.com\/watch?v=NE97ylAnrz4) by Socratica\n2. Classes\n  - [Python OOP Tutorial 1: Classes and Instances](https:\/\/www.youtube.com\/watch?v=ZDa-Z5JzLYM) by Corey Schafer\n  - [Python Classes and Objects](https:\/\/www.youtube.com\/watch?v=apACNr7DC_s) by Socratica","2c2f4af3":"**A NOTE ON BIAS**","c8185de8":"Just like everyone else, once I had completed my *Introduction to Python* on an MOOC, I was eager to apply what I had learned on a real-world machine learning problem. However, when it actually came time to build something useful, I realized that the toy projects and purely conceptual lessons that I had learned in the Python course did not help much. I have come to realize that this is because, while most of the tutorials for Python and its application on YouTube and other sources are great for learning stand-alone aspects of a project, they seldom give a complete picture of how all the pieces fit together. Since then, through reading, trial and experimentation, and a lot of sleepless nights of debugging, I started to learn how different blocks can be put together to form a coherent project and began realizing that with a little foresight, programming can be used to perfrom anything that you need. These series of tutorials titled, **Truly End-to-End Machine Learning** are meant as a way for me to document my approach to solving a problem and also serve as a guide for you to learn new skills and apply them in a truly end-to-end ML project\n","9373377d":"![Maker structure](https:\/\/i.imgur.com\/GzcUk3X.jpg)","4ad83240":"Keep in mind that in this world where data is the new gold, you can always find data for interesting analysis wherever you look. As an example, along with or instead of scrapping just the specs, a motivated reader of this tutorial could add the functionality to scrape all the user opinions (i.e comments) for a given phone and perform sentiment analysis on it to test whether the public opinion of a phone is positive or negative and compare it with comments from YouTube video of the phone. However, for the purposes of this project, we will only be analyzing the specs and leave the comments alone. Always remember that the possibilities are endless.","7cbaaf8e":"1. Do not try achieve too much too fast. Always try to create a prototype of the function first before attempting to expand your code's functionality to multiple items.\n\n2. Do not change too much too fast. Making too many changes before you run a test of the code can make it difficult to track down the culprit when errors will occur.\n\n3. Errors **WILL** inevitably occur. While it can be frustrating, it is still a problem that you can think your way through.\n\n5. There will be times when your program will not work the way you want it to. If you have hit a wall during the debugging process, sometimes walking away is the best strategy. Leaving the mind to its own devices and then getting to the block of code has been doing wonders for me and I suggest that you try this out.\n\n3. Endless improvements are possible over time, but always try to develop the minimum viable product (MVP) first.\n\n4. Do not shy away from learning new things. I do not come from a background in Computer Science. My expertise are in Nanotechnology and Materials Science. Your background can be in anything, but programming is a tool just like any other tool you can learn to use with mastery. However, it is something that can be used to build things faster than most tools due to its accessibility. \n\n6. Programming is not magic. When observing from a third party's perspective any new field can seem overwhelming and hard to break into. Keep chipping away at it until one day, it finally cracks.\n\n7. Split time between learning and practicing. I have often been given the advice that practice is the best way to learn how to code. While I agree with this, I think that it is equally important to continuously learn new concepts and ideas just for the sake of it. Being knowledgeable about multiple fields helps you to come up with novel, never before seen, solutions to hard problems.","2009432c":"**A NOTE ON BUILDING IN PARTS**","65efcd60":"**Examine the Speed Page**","072d3796":"I refer to the process of building the web crawler and everything else to follow as a problem because, in a realistic setting, a problem is exactly what it will turn out to be. But I do not use 'problem' in the negative context of the word. A problem is and should always be treated as an opportunity to learn from your mistakes and acquire new skills. At the end of the problem-solving process, seeing results in front of you can be one of the most exhilarating feelings in the world.\n\nWith philosophy out of the way, imagine yourself in the shoes of an aspiring data scientist who is working for a battery manufacturer. Let's say that your employer wants to study the distribution of battery capacity in commercial electronic products in the hopes of gaining some insight which could be used to gain a competitive advantage. This is a huge project, which depending on the project manager, could be split up into any number of smaller projects. For the sake of simplicity, let us assume that your project manager decided to split commercial electronics with batteries into segments such as  phones, IoT devices, kitchen devices, etc. It just so happens that you are given the task of analyzing the capacity of batteries in phones to gain any potential insight.\n\nThis is one of many reasons for why you would want to build a crawler assembling this dataset. The spec sheet for a device on GSMArena is incredibly rich and goes far beyond battery capacity and the above scenario is just one of many examples of what we can do with it. ","78065d5e":"**Lessons Learned**","b583d616":"### Concluding Remarks","03a4aade":"**A NOTE ON THE WEBPAGE HTML**","15d9dbc5":"### The Big Picture","f20ab743":"You can download the full list of specs that I downloaded for all the makers [here](https:\/\/drive.google.com\/open?id=1rpefi8CrQMUgs14U_H5rPEpkeYNqCvio).","c21bc39c":"And just like that, our crawler is complete!\n\nThe last step is to convert the data in **devices_collector** into a JSON object and save or download it (if you are on Colab)","032e341e":"Looking through this example, it is very apparent what type of information we want to grab. We pretty much want the crawler to scrape whatever is available in the banner (the green region in the above image) and the specifications (specs) sheet (the red region in the above image). Along with this data, we also want to grab the *Total user opinions* (*see image below*) at the bottom of the webpage.","63fff367":"I also defined two functions which helps us get a list of devices under a particular maker because the code was taking too long to run as descriibed earlier. You can try to crawl for all the sites, but this might fail for you on the two hour mark if your using Colab. Give it a shot and let me know how it does.\n\nYou can also download this file as an [IPython](https:\/\/ipython.org\/) notebook by using *File > Download .ipynb*. You can then run this notebook on you local Jupyter Notebook environment or try connecting Colab to a [local runtime](https:\/\/research.google.com\/colaboratory\/local-runtimes.html).\n","68533918":"\n1. Always test out a function before passing it along to something else. It can become extremely difficult to debug when you are a few functions deep.\n2. The code I use in the tutorials might not represent the optimal solution since the goal here is the get the task done and then worry about improving the speed and readability. I encourage you to recycle and refactor what is written here for your own projects.","95a6ff9d":"**Phone**\n\n1. Inside a maker's webpage, the data about the device is stored inside a *div-tag* with the class name 'makers' in the form on an unordered list. \n\n2. Each device on the page is a list item with an *a-tag* with the link (which we want) to the device. The *a-tag* has two child tags. \n\n3. The thumbnail for the device is stored inside an *img-tag*. The *img-tag* has two attributes *'scr'* and *'title'*, which we want.\n \n4. The text of the **strong-tab** is name of the device, which we want.\n\n\n**Nav Pages**\n1. All the information we want about the **nav_pages** is stored inside a *div-tag* with the class name 'nav-pages'.\n2. The landing page for the maker is 'Page 1' and hence has no hyperlink.\n3. All the other **nav_pages** for a given maker are stored inside the *div-tag* as child *a-tags* with the link accessible as the value of the *'href'* attribute, which we want.\n\nThinking your way through the HTML tree of the webpages that you are trying to crawl across and gaining familiarity with the layout of the information on the page is very important and I advise that you spend a few minutes on this before you write any code.\n\nMy approach to developing this crawler was to first visit a given maker in the **maker_list** and then create a dictionary with all the **nav_page** numbers and their links, including the maker's landing page. We will then use this dictionary to visit all the **nav_pages** and collect the information that we want about the phones. You are more than welcome to add your own twist to this. \n\nWe will prototype this portion of the crawler using Samsung and then iterate through all the makers in **maker_list**","1ac62e45":"![Nav links html](https:\/\/i.imgur.com\/dU3F1pq.jpg)","9ccd4224":"Most of this tutorial will be written and can be completed by you in Google's amazing tool [Colaboratory](https:\/\/colab.research.google.com\/notebooks\/welcome.ipynb#scrollTo=xitplqMNk_Hc). Please visit the attached link to learn more about Colab and how to use its numerous features. I also want to share my personal reasons for using it and hope that I can convince you to explore it further.\n\n1. Colab requires absolutely no setup to get you started. You can just login within your Google account and get started right away.\n2. You have access to a [Jupyuter Notebook](https:\/\/jupyter.org\/). Need I say more? But in all seriousness, I think it is a great way for beginners and even advanced users to prototype code and take notes using [Markdown](https:\/\/colab.research.google.com\/notebooks\/markdown_guide.ipynb#scrollTo=JtBxirFReX5n).\n3. There are **LOT** of libraries pre-built into your virtual environment, and hence there is minimal setup required to get you started. \n4. Cloud functionality is great when switching between devices. You can code on your laptop and instantly switch over to another machine at work and connect to a local runtime, if needed.\n5. Finally, any potato device that can run a browser has the ability to access a powerful virtual machine. I split my time between my beloved Chromebook Pro and my aging Lenovo Y50-70. Colab can be seamlessly run on both these machines.\n\nBefore you start using Colab, I **highly suggest** that you go under *Tools*, click on *Preferences* and check **'Show line numbers'**. This makes it much easier to debug your code when (**not if**) it throws an exception. You might also want to use the **dark mode** to ease the strain on your eyes for a long coding session and change any other settings as you see fit. Also check out [this](https:\/\/www.kdnuggets.com\/2018\/02\/essential-google-colaboratory-tips-tricks.html) article from KDNuggets to learn a few more of Colab's features\n\n**NOTE** :\nDespite all its merits, Colab caused me one major issue when writing this tutorial. I was unable to keep a hosted Colab runtime alive for more than two hours when using the crawler. With over 9000+ links to crawl across, we are looking at a crawl time in excess of three hours. I ended up having to connect to a local Jupyter Notebook runtime to get it to run for the full length of time needed. The code can be definitely be refactored to get over this issue and if you know of any method to keep Colabs hosted runtime for longer, please let me know.","9ac749e1":"![Samsung page](https:\/\/i.imgur.com\/r6e3BiR.jpg)","47bed993":"![S10 html](https:\/\/i.imgur.com\/q1ixL1Q.jpg)","9a669ac3":"This was a fun project to do, and remember that we are only just getting started with this series. Documenting the process certainly changed the way I approached the problem and I had a lot of fun (and sleepless nights) doing it. Regardless of how much attention this tutorial gets, I will be documenting the entire **Truly End-to-End Machine Learning** series and you will be able to find them on my [GitHub](https:\/\/github.com\/vigvisw) page, as soon as I can finish writing them. I acknowledge that this was a long read, but I wanted to share as much as I could to help any beginners' who are the same shoes that I was in. If you have any questions, queries, or ideas, please feel free to reach out to me at vigvisw@gmail.com.\n\nThank you for reading up until the very end and I hope you use the information provided here to build something incredible.","56701fb2":"** A NOTE ON THE RENDERED PAGE**","6a62653b":"When it comes to crawling webpages for data, the name of the game is iteration. If we can figure out how to crawl across one of the maker's in **maker_list**, we should be able to iterate through every maker in the list. We will use the maker [**Samsung**](https:\/\/www.gsmarena.com\/samsung-phones-9.php) for this example. (*No they are not sponsoring me. I just really like the S10's design and hence chose it for this example*). \n\nLet us quickly design a few useful functions. There are many ways to expand on the functionality of the crawler and modularize certain functions. For the sake of keeping the tutorial at a reasonable length, I leave it up to you to experiment further.","ae2442ca":"**Find the Seed Page**","00c83fa4":"![Galaxy S10 spec sheet ](https:\/\/i.imgur.com\/DT6KEQp.jpg )","78a1e53f":"The above pictograph (Courtesy: Western Digital) illustrates what is considered the typical pipeline for an ML project. This version of the pipeline works great when presented with well-structured SQL databases, CSV files or JSON data. However, in the real world, data is hard to collect, messy and presents a number of challenges. Your problem statement might be to build a model that takes in information collected using sensors (accelerometers, gyroscopes, and thermometers for example) and make predictions about a dependent variable. If data does not already exist for your project, it can be a challenge to devise your own methods to collect such data. \n\nMy first suggestion is that you take a look at sites like [UC Irvine Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/index.php), [Kaggle](https:\/\/www.kaggle.com\/) and, [Google Dataset Search](https:\/\/toolbox.google.com\/datasetsearch). Many a time, you can find the data that you need through a simple search and save a lot of effort. Failing that try checking if the data that you need can be accessed using an API. May interesting APIs exist, which can make your job a lot simpler. I would like to point out a few interesting ones for those who want to build a dataset using an existing API.\n\n1. [Materials Project](https:\/\/materialsproject.org\/): \"Provides open web-based access to computed information on known and predicted materials as well as powerful analysis tools to inspire and design novel materials\". If anyone is interested, I will be happy to make a tutorial on how to build a materials dataset using the Python wrapper for the API in the future. \n2. [PRAW: The Python Reddit API Wrapper](https:\/\/praw.readthedocs.io\/en\/latest\/) and [Pushshift](https:\/\/pushshift.io\/): Easy to use APIs used for accessing data from Reddit. \n\nThe worst case and the most probable scenario is that that data you want does not exist and you have to collect it yourself. Depending on your problem, the approach and the skill set you need to learn will differ. In the context of a computer vision problem involving collecting and labeling an image data set, you may have to set up an imaging system and use computer vision tools such as [OpenCV](https:\/\/opencv.org\/) to get the task done. In other cases, you may have to crawl through a website and extract the data that you need. This leads to the infamous [80\/20 split](https:\/\/www.infoworld.com\/article\/3228245\/the-80-20-data-science-dilemma.html) where up to 80% of your time in an ML project can be sunk into finding, cleaning and restructuring data.\n\nThe closest I have come to seeing a truly end-to-end ML project is [A Complete Machine Learning Project Walk-Through in Python](https:\/\/towardsdatascience.com\/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420) by [Will Koehrsen](https:\/\/towardsdatascience.com\/@williamkoehrsen). Even this incredibly detailed tutorial starts with a pre-assembled dataset which is often **not indicative** of the real world. The point I am trying to get across is that the most mission critical aspect of an ML project, which more often than not, is the data itself can be hard to find and it is always a good idea to set aside the time to learn the skills require to collect it. ","75eec084":"The function above represents one of the blocks of code that we want the crawler to execute. It takes in the url for the seed page and returns a list of all the makers in GSMArena.  ","24e2b5cf":"There are a few important things to that I want to point out here which I wish I had paid attention to when I started.\n1. Learn the basics of HTML. You do not need frontend web developer levels skills to develop your first crawler. But a cursory understanding of basic HTML elements like * tags *  and *attributes* will go a long when in helping you to find the data that you need when you are a dozen HTML tags deep. After only about an hour of studying HTML, it felt as though a fog has been lifted and ideas immediately started pouring in on how to work with BeautifulSoup elements. Please refer to the [W3 School HTML Tutorial](https:\/\/www.w3schools.com\/html\/default.asp) for all your HTML needs. They explain everything in noob friendly language and I suggest that **at the very least** you refer to pages on [HTML Introduction](https:\/\/www.w3schools.com\/html\/html_intro.asp), [HTML Basic Examples](https:\/\/www.w3schools.com\/html\/html_basic.asp), [HTML Elements](https:\/\/www.w3schools.com\/html\/html_elements.asp) and [HTML Attributes](https:\/\/www.w3schools.com\/html\/html_attributes.asp). If time permits take a look at [HTML Links](https:\/\/www.w3schools.com\/html\/html_links.asp), [HTML Tables](https:\/\/www.w3schools.com\/html\/html_tables.asp) and [HTML Lists](https:\/\/www.w3schools.com\/html\/html_lists.asp), since we will be using these tags A LOT to find the information we want from webpages. This was enough to get me started but we will be revisiting this topic in future modules.\n\n- **USE** the *Inspect Element* tool. When this toggle is turned on, and you hover the mouse over an element that you want to inspect on Screen 1, the parent tag corresponding to that element will be automatically opened on the HTML tree displayed on Screen 2. This mode can be toggled by clicking the first icon (the one which looks like an arrow in front of a screen) on the top right of Screen 2. You can also toggle it using the keyboard shortcut **Crtl + Shift  + C**. I cannot emphasize how much I wish I had learned to use this tool sooner for finding the HTML structure of the element that I want from Screen 1.\n\n- Using the *toggle device toolbar*, if needed. The toggle device toolbar allows us to change the view between desktop mode and mobile device. Throughout this tutorial, I will be using the desktop view, but your needs may change if the webpage that you are looking to crawl for information supports certain mobile only functions. This mode can be toggled by clicking the second icon (the one which looks like a mobile phone and tablet) on the top right of Screen 2. You can also toggle it using the keyboard shortcut **Crtl + Shift  + M**.\n\n**NOTE:** From this point forward, I will assume that you are referring back to the Webpage HTML using *Inspect Elements* as and when needed to understand the structure of the elements.","69fc5d1b":"The seed page is the page that we will be providing to the crawler. Ideally, you only need to provide the crawler with the link for the seed page in order to start the crawler off on its merry way. But depending on your needs, you may have to provide a list, dictionary, DataFrame, etc. In such cases, all the same ideas and concepts apply and all you have to do is modify your approach to meet the problem.\n\nThe seed page is usually the home page of the website. Each page on the website is stored as a directory or file within that home page. In our case '\/samsung_galaxy_s10-9536.php' is a webpage within the \"directory\" 'www.gsmarena.com'.  **However**, I used GSMArena's [list of all mobile brands](https:\/\/www.gsmarena.com\/makers.php3) as my seed page since I found it to be a lot easier as the starting point. You are more than welcome to write a crawler which uses a different seed page for this same problem. If you are using this tutorial as a template for crawling a different site, look around until you get the right seed page which can link your crawler to all the other pages which may be needed.","ef198732":"We now have all the components required to build the final crawler. Two things to decide before we can put everything together are:\n1. **Crawl Strategy**: This is how we actually want to proceed with the crawl. We do not want to go full Inception on the crawler and go a link within a link within a link. This will eventually lead to us getting lost in nested loops with no way to debug our code. My preferred crawl strategy for this particular crawler is as follows.\n\n> Get the **maker_list** from the seed page.\n\n>For each **maker** in the **maker_list**.\n\n\n>> For each **device** in a **maker**. \n\n>>> Get all the device information on a device.\n\n>> Return a **dict** with the device info and **device_link** to all devices on GSMArena using the **maker_name** as key.\n\n> For **all** devices in dict above find the spec sheet, banner, and opinions.\n\n>> Append the spec sheet to the device info in the **dict** from above.\n\n> Return the dict as **devices_collector**.\n\n2. **Data Storage**: This is how the information about all the phones will be stored. You are welcome to develop your own strategy for this, but my preferred method is to use the structure of the form **{maker_1_name: [device info, spec_collector], ...], maker_2_name:[.......]., ....}**. This will also allow for easy conversion of the data we scrape into any format we want such as a JSON object.","1c50329c":"**Step 1: Get the list of makers from the seed page**","97a1537b":"### Expanding The Capability Of The Web Crawler","027b1fa4":"### A Note On Colaboratory"}}