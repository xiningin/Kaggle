{"cell_type":{"90210f58":"code","8b29fda8":"code","8473f330":"code","d9ae5d70":"code","dcf7ca62":"code","df06a6c8":"code","92f49fc8":"code","c1cbbae0":"code","087ef889":"code","6cb69c68":"code","9c3ecd96":"code","3b9d97cf":"code","a2936aa8":"code","b4733b37":"code","92ccc40e":"code","47161f3b":"code","b16db668":"code","c5def51c":"code","2d69fa55":"code","1599ee2e":"code","1e5abd24":"code","67b43e6d":"code","b8f664ed":"code","e4ea964a":"code","a1225a33":"code","209489ed":"markdown","6f545826":"markdown","f79b7be7":"markdown","00270534":"markdown","457df883":"markdown","192f5dda":"markdown","28791078":"markdown"},"source":{"90210f58":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndata = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\n\n# Remove Last 2 Columns\ndata = data.iloc[:, :-2]\ndata = data.iloc[:, 1:]","8b29fda8":"data.columns","8473f330":"sns.countplot(x='Attrition_Flag', data=data)","d9ae5d70":"# Checking if we have some NaN values in our dataset\ndata.isna().sum()","dcf7ca62":"plt.figure(figsize=(8,6))\nsns.boxplot(x='Income_Category', \n            y='Credit_Limit',\n            order=['Unknown', 'Less than $40K', '$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +'],\n            data=data).set_title('Income x Credit_Limit Boxplot')","df06a6c8":"plt.figure(figsize=(8,6))\nsns.boxplot(x='Education_Level', \n            y='Credit_Limit',\n            order=['Unknown', 'Uneducated', 'High School', 'College', 'Graduate', 'Doctorate', 'Post-Graduate'],\n            data=data).set_title('Boxplot')","92f49fc8":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(data.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","c1cbbae0":"# Mapping target feature 'Attrition Flag' to binary\ndata['Attrition_Flag'] = data['Attrition_Flag'].map({'Existing Customer': 0, 'Attrited Customer': 1})","087ef889":"corr = data.corr()\ncorr.sort_values('Attrition_Flag', ascending=False, inplace=True)\nprint(corr.Attrition_Flag)","6cb69c68":"# One Hot Encoding Categorical Features\ndata = pd.get_dummies(data)","9c3ecd96":"# Churns vs Credit Card Category (Blue, Silver, Gold, Platinum)\nprint('Card        % of Customers     % of Churns')\nprint('------------------------------------------')\nprint('Blue: %15.2f %17.2f' % (data['Card_Category_Blue'].mean()*100, \n                               (data['Card_Category_Blue'] == data['Attrition_Flag']).mean()*100))\n\nprint('Silver: %13.2f %17.2f' % (data['Card_Category_Silver'].mean()*100, \n                                 (data['Card_Category_Silver'] == data['Attrition_Flag']).mean()*100))\n\nprint('Gold: %15.2f %17.2f' % (data['Card_Category_Gold'].mean()*100, \n                               (data['Card_Category_Gold'] == data['Attrition_Flag']).mean()*100))\n\nprint('Platinum: %11.2f %17.2f' % (data['Card_Category_Platinum'].mean()*100, \n                                   (data['Card_Category_Platinum'] == data['Attrition_Flag']).mean()*100))","3b9d97cf":"# Separating features into x and target y Attrition_Flag (churns)\nx = data.iloc[:, 1:].values\ny = data.iloc[:, 0].values\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=5)","a2936aa8":"# Scaling features\nscaler = StandardScaler()\nscaler.fit(x_train)\n\nscaled_x_train = scaler.transform(x_train)\nscaled_x_test = scaler.transform(x_test)","b4733b37":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\npc = pca.fit_transform(scaled_x_train)\n\nprint('Components Variance:')\nprint('1st: %.3f' % pca.explained_variance_ratio_[0])\nprint('2nd: %.3f' % pca.explained_variance_ratio_[1])\nprint('3rd: %.3f' % pca.explained_variance_ratio_[2])","92ccc40e":"plt.scatter(pc[:, 0], pc[:, 1], c=y_train)\nplt.title(\"PCA\")\nplt.xlabel(\"1st Component\")\nplt.ylabel(\"2nd Component\")\nplt.show()","47161f3b":"from mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(8,8))\nax = Axes3D(fig)\nax.scatter(pc[:, 0], pc[:, 1], pc[:, 2], c=y_train)\nax.set_xlabel('1st Component')\nax.set_ylabel('2nd Component')\nax.set_zlabel('3rd Component')\nax.set_title('PCA')","b16db668":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=3)\ntsne_comp = tsne.fit_transform(scaled_x_train)","c5def51c":"plt.scatter(tsne_comp[:, 0], tsne_comp[:, 1], c=y_train)\nplt.title(\"t-SNE\")\nplt.xlabel(\"1st Component\")\nplt.ylabel(\"2nd Component\")\nplt.show()","2d69fa55":"fig = plt.figure(figsize=(8,8))\nax = Axes3D(fig)\nax.scatter(tsne_comp[:, 0], tsne_comp[:, 1], tsne_comp[:, 2], c=y_train)\nax.set_xlabel('1st Component')\nax.set_ylabel('2nd Component')\nax.set_zlabel('3rd Component')\nax.set_title('t-SNE')","1599ee2e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# Evaluate Models\n\nn_folds = 10\nmodels = []\n\nsvc_clf = LinearSVC(random_state=0, tol=1e-5, dual=False)\nsvc_clf.fit(scaled_x_train, y_train)\n\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('Tree', DecisionTreeClassifier()))\nmodels.append(('SVC', LinearSVC(dual=False)))\nmodels.append(('Forest', RandomForestClassifier()))\nmodels.append(('XGB', XGBClassifier(use_label_encoder=False, eval_metric='logloss')))\nmodels.append(('NB', GaussianNB()))\n\nfor name, model in models:\n    kfold = KFold(n_splits=n_folds)\n    cv_results = cross_val_score(model, scaled_x_train, y_train, cv=kfold, scoring='accuracy')\n    print(\"%6s %.3f %.3f \" % (name, cv_results.mean(), cv_results.std()))","1e5abd24":"import xgboost as xgb\n\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [5, 6, 7, 8],\n        }\n\nxgb_clf = xgb.XGBClassifier(learning_rate=0.3, n_estimators=600, objective='binary:logistic',\n                            nthread=1, use_label_encoder=False, eval_metric='logloss')","67b43e6d":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfolds = 3\nparam_comb = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state = 25)\n\nsearch = RandomizedSearchCV(xgb_clf, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(x_train, y_train), random_state=25)\n\nsearch.fit(x_train, y_train)\n\nprint('Best hyperparameters:')\nprint(search.best_params_)","b8f664ed":"params = {\n        'min_child_weight': 1,\n        'gamma': 2,\n        'subsample': 1,\n        'colsample_bytree': 0.8,\n        'max_depth': 8,\n        }\n\nxgb_clf = xgb.XGBClassifier(learning_rate=0.3, n_estimators=600, objective='binary:logistic',\n                            nthread=1, use_label_encoder=False, eval_metric='logloss')\n\nxgb_clf.fit(x_train, y_train)","e4ea964a":"xgb_predictions = xgb_clf.predict(x_test)\nxgb_predictions = np.round(xgb_predictions)\nprint('XGBoost Test Set')\nprint('Accuracy: %.2f' % ((xgb_predictions == y_test).mean()*100))","a1225a33":"xgb_clf.get_booster().feature_names = list(data.columns[1:])\nxgb_clf.get_booster().get_score(importance_type=\"gain\")\n\nfig, ax = plt.subplots(1,1,figsize=(10,10))\nxgb.plot_importance(xgb_clf, max_num_features=35, ax=ax)\nplt.show()","209489ed":"**Pearson Correlation between features and target variable Attrtion_Flag**","6f545826":"**Model accuracy in our test set**","f79b7be7":"**Grid Search Hyperparameter Tuning for XGBoost**\n\nNow we will use sklearn's grid search to find the best parameters for our XGBoost model","00270534":"# Exploratory Data Analysis\n\n* Doing some visualization with our data;\n\n* Correlation between features.","457df883":"**Visualizing our with some Dimensionality Reduction Algorithms**\n\nUsing PCA decomposition and t-SNE to see if we can find some clusters","192f5dda":"# Testing Classifiers\n\nWe'll test some classifiers using cross validation to determine which one we will fine tune and test it with our test set.\n\n* Logistic Regression\n* Decision Tree\n* LinearSVC\n* Random Forest\n* XGBoost\n* Gaussian Naive Bayes","28791078":"# **Abstract**\n* The objective of this notebook is to predict customers churns (cancellation) among credit card customers"}}