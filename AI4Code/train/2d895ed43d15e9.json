{"cell_type":{"b9bbe4ef":"code","8d1c6957":"code","3f74b710":"code","95befe37":"code","f0a28312":"code","30505f70":"code","594d7ff0":"code","8705dfe4":"code","1d31d5cb":"code","cab0d3f2":"code","259fffef":"code","95da370c":"code","94d29fd8":"code","7eb20af2":"code","c3b1948a":"code","fa2c27c0":"code","c001e4be":"code","2b4a0119":"code","1468a35e":"code","029826a9":"code","e27c56dc":"code","b2f02c75":"code","1a815793":"code","e248b40d":"code","0e47680e":"code","27fc8070":"code","1980fb1b":"code","b3c1394b":"code","4c4a2a8d":"code","c0bdd0e7":"code","a19ead37":"code","068e8765":"code","f8cb2e49":"code","cc0ca51e":"code","4d54f2a0":"code","a51b6899":"code","3bb21270":"code","6b23d557":"code","e44fc0e8":"code","c75e4458":"code","664f0dfd":"code","77a9da27":"code","b4c20731":"code","d0ffd27e":"code","92db4e97":"code","33378c68":"markdown","0a14ec99":"markdown","3da3b525":"markdown","263beb94":"markdown","533fe820":"markdown","fe9c0482":"markdown","ced74d06":"markdown","5629a25e":"markdown","17189018":"markdown","c47027cb":"markdown","e0962d7b":"markdown","5b7e66b6":"markdown","806d733f":"markdown","b73f0b26":"markdown","746cb34a":"markdown","d3532d9e":"markdown","9195a86a":"markdown","d40bc3fe":"markdown","d265d024":"markdown","c08e95c5":"markdown","5a6c247d":"markdown","cf051e6d":"markdown"},"source":{"b9bbe4ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\n%matplotlib inline\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader,Dataset\nimport matplotlib.pyplot as plt\nimport torchvision.utils\nimport numpy as np\nimport random\nfrom PIL import Image\nimport torch\nfrom torch.autograd import Variable\nimport PIL.ImageOps    \nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport itertools\nimport os\nimport pandas as pd\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8d1c6957":"print(os.listdir(\"..\/input\/pytorch-pretrained-models-for-face-detection\/\"))\nprint(os.listdir(\"..\/input\/recognizing-faces-in-the-wild\/\"))","3f74b710":"#Checking if CUDA is available or not\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","95befe37":"def imshow(img,text=None,should_save=False):\n    npimg = img.numpy()\n    plt.axis(\"off\")\n    if text:\n        plt.text(75, 8, text, style='italic',fontweight='bold',\n            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()    \n\ndef show_plot(iteration,loss):\n    plt.plot(iteration,loss)\n    plt.show()","f0a28312":"df = pd.read_csv(\"..\/input\/recognizing-faces-in-the-wild\/train_relationships.csv\")\ndf.head()","30505f70":"new = df[\"p1\"].str.split(\"\/\", n = 1, expand = True)\n\n# making separate first name column from new data frame \ndf[\"Family1\"]= new[0]\n# making separate last name column from new data frame \ndf[\"Person1\"]= new[1]\n\n# Dropping old Name columns\ndf.drop(columns =[\"p1\"], inplace = True)\n\nnew = df[\"p2\"].str.split(\"\/\", n = 1, expand = True)\n\n# making separate first name column from new data frame \ndf[\"Family2\"]= new[0]\n# making separate last name column from new data frame \ndf[\"Person2\"]= new[1]\n\n# Dropping old Name columns\ndf.drop(columns =[\"p2\"], inplace = True)\ndf.head()","594d7ff0":"TRAIN_ZIP = '..\/input\/recognizing-faces-in-the-wild\/train.zip'\nTEST_ZIP='..\/input\/recognizing-faces-in-the-wild\/test.zip'","8705dfe4":"import zipfile\nprint(\"unzipping train set\")\nwith zipfile.ZipFile(TRAIN_ZIP, 'r') as zip_ref:\n    zip_ref.extractall(\"..\/output\/kaggle\/working\/train\")\n\nprint(\"unzipping test set\")\nwith zipfile.ZipFile(TEST_ZIP, 'r') as zip_ref:\n    zip_ref.extractall(\"..\/output\/kaggle\/working\/test\")\nprint(\"unzipping complete!! \")","1d31d5cb":"root_dir = '..\/output\/kaggle\/working\/train\/'\ntemp = []\nfor index, row in df.iterrows():\n    path1 = root_dir+row.Family1+'\/'+row.Person1\n    path2 = root_dir+row.Family2+'\/'+row.Person2\n    if os.path.exists(path1) and os.path.exists(path2):\n        len1 = len([name for name in os.listdir(path1) if os.path.isfile(os.path.join(path1, name))])\n        len2 = len([name for name in os.listdir(path2) if os.path.isfile(os.path.join(path2, name))])\n\n        if len1 == 0 or len2 == 0: \n            temp.append(index)\n        continue\n    else:\n        temp.append(index)\n        \nprint(len(temp))\ndf = df.drop(temp, axis=0)","cab0d3f2":"len(df)","259fffef":"#A new column in the existing dataframe with all values as 1, since these people are all related\ndf['Related'] = 1\n\n#Creating a dictionary, and storing members of each family\ndf_dict = {}\nfor index, row in df.iterrows():\n    if row['Family1'] in df_dict:\n        df_dict[row['Family1']].append(row['Person1'])\n    else:\n        df_dict[row['Family1']] = [row['Person1']]\n        \n#For each family in this dictionary, we'll first make pairs of people\n#For each pair, we'll check if they're related in our existing Dataset\n#If they're not in the dataframe, means we'll create a row with both persons and related value 0\ni=1\nfor key in df_dict:\n    pair = list(itertools.combinations(df_dict[key], 2))\n    for item in pair:\n        if len(df[(df['Family1']==key)&(df['Person1']==item[0])&(df['Person2']==item[1])])==0 \\\n        and len(df[(df['Family1']==key)&(df['Person1']==item[1])&(df['Person2']==item[0])])==0:\n            new = {'Family1':key,'Person1':item[0],'Family2':key,'Person2':item[1],'Related':0}\n            df=df.append(new,ignore_index=True)\n        \n#Storing rows only where Person1 and Person2 are not same\ndf = df[(df['Person1']!=df['Person2'])]\n\n#len(df[(df['Related']==1)])\n\nprint(df['Related'].value_counts())","95da370c":"extra = df['Related'].value_counts()[1]-df['Related'].value_counts()[0]\nwhile extra>=0:\n    rows = df.sample(n=2)\n    first = rows.iloc[0,:]\n    second = rows.iloc[1,:]\n    \n    if first.Family1!=second.Family1 and first.Family2!=second.Family2:\n        new1 = {'Family1':first.Family1,'Person1':first.Person1,'Family2':second.Family1,'Person2':second.Person1,'Related':0}\n        extra=extra-1\n        if extra==0:\n            break\n        new2 = {'Family1':first.Family2,'Person1':first.Person2,'Family2':second.Family2,'Person2':second.Person2,'Related':0}\n        extra=extra-1\n        \n        df=df.append(new1,ignore_index=True)\n        df=df.append(new2,ignore_index=True)","94d29fd8":"df = df.sample(frac=1).reset_index(drop=True)","7eb20af2":"df['Related'].value_counts()","c3b1948a":"df.head()","fa2c27c0":"from PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nclass FamilyDataset(Dataset):\n    \"\"\"Family Dataset.\"\"\"\n\n    def __init__(self, df, root_dir, transform=None):\n        \"\"\"\n        Args:\n            df (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.relations = df\n        self.root_dir = root_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.relations)\n    \n    def __getpair__(self,idx):\n        pair = self.root_dir+self.relations.iloc[idx,0] + '\/' + self.relations.iloc[idx,1],\\\n        self.root_dir+self.relations.iloc[idx,2] + '\/' + self.relations.iloc[idx,3]\n        return pair\n    \n    def __getlabel__(self,idx):\n        return self.relations.iloc[idx,4]\n    \n    def __getitem__(self, idx):\n        try:\n            pair =  self.__getpair__(idx)\n            label = self.__getlabel__(idx)\n\n            first = random.choice(os.listdir(pair[0]))\n            second = random.choice(os.listdir(pair[1]))\n\n            img0 = Image.open(pair[0] + '\/' + first)\n            img1 = Image.open(pair[1] + '\/'  + second)\n    #         img0 = img0.convert(\"L\")\n    #         img1 = img1.convert(\"L\")\n\n            if self.transform is not None:\n                img0 = self.transform(img0)\n                img1 = self.transform(img1)\n\n            return idx,img0,img1,label\n        except Exception as e:\n            print(e)\n            pass","c001e4be":"train_df,valid_df = np.split(df, [int(.8*len(df))])","2b4a0119":"train_transform = transforms.Compose([transforms.Resize(255),\n    transforms.ColorJitter(0.9, 0.9, 0.9, 0.1),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])])\n\nvalid_transform = transforms.Compose([transforms.Resize(255),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])])\n\ntrain_dataset= FamilyDataset(df=train_df,root_dir=\"..\/output\/kaggle\/working\/train\/\",transform=train_transform)\nvalid_dataset = FamilyDataset(df=valid_df,root_dir=\"..\/output\/kaggle\/working\/train\/\",transform=valid_transform)","1468a35e":"vis_dataloader = DataLoader(train_dataset,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=8)\ndataiter = iter(vis_dataloader)\n\nexample_batch = next(dataiter)\nconcatenated = torch.cat((example_batch[1],example_batch[2]),0)\nimshow(torchvision.utils.make_grid(concatenated))\nprint(example_batch[3].numpy())","029826a9":"class Vgg_face_dag(nn.Module):\n\n    def __init__(self):\n        super(Vgg_face_dag, self).__init__()\n        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n                     'std': [1, 1, 1],\n                     'imageSize': [224, 224, 3]}\n        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu1_1 = nn.ReLU(inplace=True)\n        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu1_2 = nn.ReLU(inplace=True)\n        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu2_1 = nn.ReLU(inplace=True)\n        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu2_2 = nn.ReLU(inplace=True)\n        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu3_1 = nn.ReLU(inplace=True)\n        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu3_2 = nn.ReLU(inplace=True)\n        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu3_3 = nn.ReLU(inplace=True)\n        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu4_1 = nn.ReLU(inplace=True)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu4_2 = nn.ReLU(inplace=True)\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu4_3 = nn.ReLU(inplace=True)\n        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu5_1 = nn.ReLU(inplace=True)\n        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu5_2 = nn.ReLU(inplace=True)\n        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu5_3 = nn.ReLU(inplace=True)\n        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n        self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True)\n        self.relu6 = nn.ReLU(inplace=True)\n        self.dropout6 = nn.Dropout(p=0.5)\n        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n        self.relu7 = nn.ReLU(inplace=True)\n        self.dropout7 = nn.Dropout(p=0.5)\n        self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True)\n\n    def forward_once(self, x0):\n        x1 = self.conv1_1(x0)\n        x2 = self.relu1_1(x1)\n        x3 = self.conv1_2(x2)\n        x4 = self.relu1_2(x3)\n        x5 = self.pool1(x4)\n        x6 = self.conv2_1(x5)\n        x7 = self.relu2_1(x6)\n        x8 = self.conv2_2(x7)\n        x9 = self.relu2_2(x8)\n        x10 = self.pool2(x9)\n        x11 = self.conv3_1(x10)\n        x12 = self.relu3_1(x11)\n        x13 = self.conv3_2(x12)\n        x14 = self.relu3_2(x13)\n        x15 = self.conv3_3(x14)\n        x16 = self.relu3_3(x15)\n        x17 = self.pool3(x16)\n        x18 = self.conv4_1(x17)\n        x19 = self.relu4_1(x18)\n        x20 = self.conv4_2(x19)\n        x21 = self.relu4_2(x20)\n        x22 = self.conv4_3(x21)\n        x23 = self.relu4_3(x22)\n        x24 = self.pool4(x23)\n        x25 = self.conv5_1(x24)\n        x26 = self.relu5_1(x25)\n        x27 = self.conv5_2(x26)\n        x28 = self.relu5_2(x27)\n        x29 = self.conv5_3(x28)\n        x30 = self.relu5_3(x29)\n        x31_preflatten = self.pool5(x30)\n        x31 = x31_preflatten.view(x31_preflatten.size(0), -1)\n        x32 = self.fc6(x31)\n        x33 = self.relu6(x32)\n        x34 = self.dropout6(x33)\n        x35 = self.fc7(x34)\n        x36 = self.relu7(x35)\n        x37 = self.dropout7(x36)\n        x38 = self.fc8(x37)\n        return x38\n\n    def forward(self,input1,input2):\n        output1 = self.forward_once(input1)\n        output2 = self.forward_once(input2)\n# #         print(\"shapes and sizes \", output1.shape, output2.shape)\n#         len1 = (output1.shape)[0]\n#         len2 = (output2.shape)[0]\n#         output1 = output1.reshape(len1,-1, 2622)\n#         output2 = output2.reshape(len2,-1, 2622)\n#         cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n        difference = output1 - output2\n        return difference\n\ndef vgg_face_dag(weights_path=None, **kwargs):\n    \"\"\"\n    load imported model instance\n\n    Args:\n        weights_path (str): If set, loads model weights from the given path\n    \"\"\"\n    model = Vgg_face_dag()\n    if weights_path:\n        state_dict = torch.load(weights_path)\n        model.load_state_dict(state_dict)\n    return model","e27c56dc":"A= torch.randn(32,2622).reshape(32,-1, 2622)\nB = torch.randn(32,2622).reshape(32,-1, 2622)\nc = Vgg_face_dag()\n# d = (A - B).pow(2).sum(3).sqrt()\nd = torch.nn.functional.pairwise_distance(A, B)\ncos = nn.CosineSimilarity(dim=1, eps=1e-6)\noutput = cos(A, B)\nprint(output.shape)","b2f02c75":"def triplet_loss(y_true, y_pred, alpha = 400,N=5):\n    \"\"\"\n    Implementation of the triplet loss function\n    Arguments:\n    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n    y_pred -- python list containing three objects:\n            anchor -- the encodings for the anchor data\n            positive -- the encodings for the positive data (similar to anchor)\n            negative -- the encodings for the negative data (different from anchor)\n    Returns:\n    loss -- real number, value of the loss\n    \"\"\"\n#     print((y_pred[0]))\n    N=y_pred.shape[1]\/\/3\n    anchor = y_pred[:,0:N]\n    positive = y_pred[:,N:N*2]\n    negative = y_pred[:,N*2:N*3]\n\n    # distance between the anchor and the positive\n    pos_dist = math.sqrt(math.sum(math.square(anchor-positive),axis=1)+.01)\n\n    # distance between the anchor and the negative\n    neg_dist = K.sqrt(K.sum(K.square(anchor-negative),axis=1)+.01)\n\n    # compute loss\n    basic_loss = (pos_dist-neg_dist+alpha)\n    loss = K.maximum(basic_loss,0.0)\n \n    return loss\n\ndef Neg_Dist(y_true, y_pred):\n    \"\"\"\n    Implementation of the triplet loss function\n    Arguments:\n    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n    y_pred -- python list containing three objects:\n            anchor -- the encodings for the anchor data\n            positive -- the encodings for the positive data (similar to anchor)\n            negative -- the encodings for the negative data (different from anchor)\n    Returns:\n    loss -- real number, value of the loss\n    \"\"\"\n#     print((y_pred[0]))\n    N=y_pred.shape[1]\/\/3\n    anchor = y_pred[:,0:N]\n    negative = y_pred[:,N*2:N*3]\n\n\n\n    # distance between the anchor and the negative\n    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n\n \n    return neg_dist","1a815793":"vggnet = vgg_face_dag(weights_path=\"..\/input\/pytorch-pretrained-models-for-face-detection\/VGG Face\")\nvggnet = vggnet.cuda()\nfor param in vggnet.parameters(): \n    param.requires_grad = False","e248b40d":"# if train_on_gpu:\n#     net = SiameseNetwork().cuda()\n# else:\n#     net= SiameseNetwork()","0e47680e":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Sequential(\n            nn.Linear(2622, 1024),\n            nn.ReLU(inplace=True),\n            \n            nn.Linear(1024, 512),\n            nn.ReLU(inplace=True),\n            \n            nn.Linear(512, 128),\n            nn.ReLU(inplace=True),\n            \n            nn.Linear(128, 2))\n#             nn.ReLU(inplace=True),\n            \n#             nn.Linear(512, 128),\n#             nn.ReLU(inplace=True),\n            \n#             nn.Linear(128, 128),\n#             nn.ReLU(inplace=True),\n\n#             nn.Linear(128, 2))\n  \n    def forward(self, x):\n        x = self.fc1(x)\n        return x","27fc8070":"#model = Model().cuda()\n#net = nn.Sequential(vgg_face_dag(weights_path=\"..\/input\/pytorch-pretrained-models-for-face-detection\/VGG Face\").cuda(),Model().cuda())\nnet = Model().cuda()","1980fb1b":"# specify loss function (categorical cross-entropy)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.008, momentum=0.9)\n# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n# optimizer = optim.Adam(net.parameters(), lr=0.0001)","b3c1394b":"class Config():\n    training_dir = \"..\/output\/kaggle\/working\/train\/\"\n    testing_dir = \"..\/output\/kaggle\/working\/test\/\"\n    batch_size = 32\n    train_number_epochs = 150\n    num_workers = 4","4c4a2a8d":"# # percentage of training set to use as validation\n# valid_size = 0.2\n\n# # obtain training indices that will be used for validation\n# num_train = len(train_dataset)\n# indices = list(range(num_train))\n# np.random.shuffle(indices)\n# split = int(np.floor(valid_size * num_train))\n# train_idx, valid_idx = indices[split:], indices[:split]\n\n# # define samplers for obtaining training and validation batches\n# train_sampler = SubsetRandomSampler(train_idx)\n# valid_sampler = SubsetRandomSampler(valid_idx)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset,shuffle=True,num_workers=Config.num_workers, batch_size = Config.batch_size)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset,shuffle=True,num_workers=Config.num_workers, batch_size = Config.batch_size)","c0bdd0e7":"len(train_dataset)","a19ead37":"train_counter = []\ntrain_loss_history = []\ntrain_iteration_number= 0\n\nvalid_counter = []\nvalid_loss_history = []\nvalid_iteration_number= 0\n\n# initialize tracker for minimum validation loss\nvalid_loss_min = np.Inf # set initial \"min\" to infinity\n\ntrain_class_correct = list(0 for i in range(2))\ntrain_class_total = list(0 for i in range(2))\n\nvalid_class_correct = list(0 for i in range(2))\nvalid_class_total = list(0 for i in range(2))","068e8765":"for epoch in range(0,Config.train_number_epochs):\n    train_loss = 0.0\n    valid_loss = 0.0\n    net.train()\n    for i, data in enumerate(train_loader):\n#         print(\"here\", row)\n        row, img0, img1 , label = data\n        row, img0, img1 , label = row.cuda(), img0.cuda(), img1.cuda() , label.cuda()\n        \n        optimizer.zero_grad()\n#         print(\"shape of images 1, 2 \", img0.shape, img1.shape)\n        output1= vggnet(img0,img1)\n#         print(\"size of output1 \", output1.shape)\n        output = net(output1)\n#         print(\"size of output 2 \", output.shape)\n        _, pred= torch.max(output,1)\n\n#         loss = criterion(output,label)\n        loss = triplet_loss(label, output)\n        loss.backward()\n        \n        optimizer.step()\n        \n        correct = pred.eq(label.view_as(pred))\n        for j in range(len(label)):\n                        target = label[j].data\n                        train_class_correct[target] += correct[j].item()\n                        train_class_total[target] += 1\n                        \n        if i%30 == 0:\n            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch+1,loss.item()))\n            train_iteration_number +=30\n            train_counter.append(train_iteration_number)\n            train_loss_history.append(loss.item())\n\n            for i in range(2):\n                if train_class_total[i] > 0:\n                        print('\\nTraining Accuracy of %5s: %2d%% (%2d\/%2d)' % (\n                            str(i), 100 * train_class_correct[i] \/ train_class_total[i],\n                            np.sum(train_class_correct[i]), np.sum(train_class_total[i])))\n\n            print('\\nTraining Accuracy (Overall): %2d%% (%2d\/%2d)' % (\n                100. * np.sum(train_class_correct) \/ np.sum(train_class_total),\n                np.sum(train_class_correct), np.sum(train_class_total)))\n    \n    net.eval()\n    for i, data in enumerate(valid_loader,0):\n        row, img0, img1 , label = data\n        row, img0, img1 , label = row.cuda(), img0.cuda(), img1.cuda() , label.cuda()\n        \n        output1= vggnet(img0,img1)\n        output = net(output1)\n        #combined = torch.cat([vgg,res1,sen1],1)\n        #output= net(combined)\n        _, pred= torch.max(output,1)\n\n#         loss = criterion(output,label)\n        loss = triplet_loss(label, output)\n        \n\n\n        \n        correct = pred.eq(label.view_as(pred))\n        for j in range(len(label)):\n                        target = label[j].data\n                        valid_class_correct[target] += correct[j].item()\n                        valid_class_total[target] += 1\n        valid_loss += loss.item()\n#         scheduler.step(loss)\n        if i%30 == 0:\n            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch+1,loss.item()))\n            valid_iteration_number +=30\n            valid_counter.append(valid_iteration_number)\n            valid_loss_history.append(loss.item())\n\n            for i in range(2):\n                if train_class_total[i] > 0:\n                        print('\\nValdiation Accuracy of %5s: %2d%% (%2d\/%2d)' % (\n                            str(i), 100 * valid_class_correct[i] \/ valid_class_total[i],\n                            np.sum(valid_class_correct[i]), np.sum(valid_class_total[i])))\n\n            print('\\nValdiation Accuracy (Overall): %2d%% (%2d\/%2d)' % (\n                100. * np.sum(valid_class_correct) \/ np.sum(valid_class_total),\n                np.sum(valid_class_correct), np.sum(valid_class_total)))\n            \n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(net.state_dict(), 'model.pt')\n        valid_loss_min = valid_loss","f8cb2e49":"print('\\n Training Loss History')\nshow_plot(train_counter,train_loss_history)\n\nprint('\\n Validation Loss History')\nshow_plot(valid_counter,valid_loss_history)","cc0ca51e":"net.load_state_dict(torch.load('model.pt'))","4d54f2a0":"sample_submission = pd.read_csv(\"..\/input\/recognizing-faces-in-the-wild\/sample_submission.csv\")\nsample_submission.head()","a51b6899":"new = sample_submission[\"img_pair\"].str.split(\"-\", n = 1, expand = True)\n\n# making separate first name column from new data frame \nsample_submission[\"Person1\"]= new[0]\n# making separate last name column from new data frame \nsample_submission[\"Person2\"]= new[1]\n\n# Dropping old Name columns\nsample_submission.head()","3bb21270":"class FamilyTestDataset(Dataset):\n    \"\"\"Family Dataset.\"\"\"\n\n    def __init__(self, df, root_dir, transform=None):\n        \"\"\"\n        Args:\n            df (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.relations = df\n        self.root_dir = root_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.relations)\n    \n    def __getpair__(self,idx):\n        pair = self.root_dir+self.relations.iloc[idx,2],\\\n        self.root_dir+self.relations.iloc[idx,3]\n        return pair\n    \n    def __getlabel__(self,idx):\n        return self.relations.iloc[idx,4]\n    \n    def __getitem__(self, idx):\n        try:\n            pair =  self.__getpair__(idx)\n\n            img0 = Image.open(pair[0])\n            img1 = Image.open(pair[1])\n    #         img0 = img0.convert(\"L\")\n    #         img1 = img1.convert(\"L\")\n\n            if self.transform is not None:\n                img0 = self.transform(img0)\n                img1 = self.transform(img1)\n            \n            return idx,img0,img1\n        except Exception as e:\n            print(e)","6b23d557":"transform = transforms.Compose([transforms.Resize(255),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])])\n\ntest_dataset= FamilyTestDataset(df=sample_submission,root_dir=\"..\/output\/kaggle\/working\/test\/\",transform=transform)","e44fc0e8":"test_loader = torch.utils.data.DataLoader(test_dataset,shuffle=True,num_workers=Config.num_workers, batch_size = Config.batch_size)","c75e4458":"net.eval()\nfor i, data in enumerate(test_loader,0):\n    row, img0, img1 = data\n    row, img0, img1 = row.cuda(), img0.cuda(), img1.cuda()\n   \n    output1= vggnet(img0,img1)\n    output = net(output1)\n    #output= net(img0,img1)\n    _, pred= torch.max(output,1)\n     \n    count=0\n    for item in row:\n        sample_submission.loc[item,'is_related'] = pred[count].item()\n        count+=1","664f0dfd":"sample_submission.drop(columns =[\"Person1\",\"Person2\"], inplace = True)","77a9da27":"sample_submission['is_related'].value_counts()","b4c20731":"output= sample_submission.to_csv('output.csv',index=False)","d0ffd27e":"# import the modules we'll need\nfrom IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)","92db4e97":"create_download_link(sample_submission)","33378c68":"It is essential to check if all the folders present in the above DataFrame exist, else it'll be an error while processing the dataset. We should remove the rows that are duplicate and the rows that don't have any folder existing corresponding to their values","0a14ec99":"Now that our model is trained, we'll see how the sample submissions are","3da3b525":"# Defining Neural Network","263beb94":"# Training the model","533fe820":"# Loading Saved Model","fe9c0482":"# Visualising Data","ced74d06":"From value_counts() it is visible that data is imbalanced and there are many more instances of 1 than 0. So, let's create some more instances of class 0 between two families, such that dataframe becomes balanced","5629a25e":"# Configuration Class","17189018":"Now all rows with 1 are together and all rows with 0 are together, Rows should be shuffled","c47027cb":"# Classification Model","e0962d7b":"# Helper Functions","5b7e66b6":"# Optimizer","806d733f":"The dataset is now balanced","b73f0b26":"# VGG Face Net","746cb34a":"We can see the data has two columns of people that are related. Let's split these columns into three i.e. Family, Person1 and Person2","d3532d9e":"We can also create some more data from Families for people that are not related. Then we can use this data to train our model on classes 'Related' and 'Not Related'. 'Not Related' instances can be numerous since there are many families that are not correlated. However, that would give a lot of data, We can create new data from people within families that are not related. Eg. A son and daughter would be related to their father and their mother, However, the mother and father won't be related themselves","9195a86a":"# Training and Validation Loss","d40bc3fe":"There are 236 rows that don't have any folders corresponding to them so they have been removed","d265d024":"# Pre processing relationships Data","c08e95c5":"# Custom Dataset Class","5a6c247d":"# Custom Test Dataset\n","cf051e6d":"# Training and Validation Dataloader"}}