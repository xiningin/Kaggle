{"cell_type":{"69436e4c":"code","14fac904":"code","d6bc63c9":"code","865d83c1":"code","f660d2bd":"code","8ded3c5a":"code","021d4b02":"code","7b324300":"code","776cc030":"code","59a257b8":"code","8d2b8c77":"code","b5f7c35d":"code","5054b31a":"code","0184f182":"code","679878cf":"code","07249fce":"code","e951520c":"code","5d47d59c":"code","4de3f9ce":"code","aea0f4a6":"code","98b2a523":"code","a31e0e87":"code","766fa98a":"code","9845690e":"code","9b0c7c6c":"code","2c4860d4":"code","81c380b0":"code","25d78341":"code","5cf2cb86":"code","2ccd6839":"code","7b728558":"code","f6956ec7":"code","f993fcb8":"code","3ddf183b":"code","401695f1":"code","0c632aa1":"code","6d660db7":"code","6d9fa0ae":"code","8e79d72b":"code","10a0c702":"code","d48ebcb0":"code","fb822ab3":"code","c4ad5e38":"code","31429025":"code","36b9c85a":"code","2bfd05a3":"code","39b4029e":"code","31dcd625":"code","e0353b75":"code","bba6fef8":"code","6d5809ba":"code","1f0d8731":"code","762ad5a1":"code","3b88b67e":"code","4142abbd":"code","0cfcca9f":"code","37de272f":"code","b622125b":"code","ccbfce26":"code","2f0cea85":"code","23931c36":"code","49a31712":"code","543e3eaf":"code","3041a8f9":"code","5efe87f6":"code","a9376777":"code","35e40429":"code","13b2f22b":"code","a16a6c17":"code","76a22a8f":"code","344b65b4":"code","573820a3":"code","e1c79646":"code","87c6acf0":"code","3145a556":"code","633dd80f":"code","633fe296":"code","536d69b8":"code","74878de7":"code","0ab5ff0d":"code","a3c81401":"code","6eb27573":"code","289ffe37":"code","56ddaf22":"code","8f38a308":"code","928f1c5a":"code","97c229a6":"code","9a6b97a3":"code","a9efcd3d":"code","981004bb":"code","4451e604":"code","bd1aa739":"code","cadc185e":"code","6ccb54c7":"code","a5bc0b4a":"code","cd76a442":"code","e37cd1b6":"code","19388571":"code","46df47ca":"code","e3d0b8ca":"code","62ed11d5":"code","ed5f7e12":"code","f79b8ea6":"code","7a7c3a67":"code","a7402df0":"code","d4c233fb":"code","401698dd":"markdown","38cc5757":"markdown","794916ca":"markdown","cf0dc963":"markdown","214d1f48":"markdown","58a2ff77":"markdown","4a13fe01":"markdown","7ace8d40":"markdown","a88c2185":"markdown","b78dcebf":"markdown","8db39031":"markdown","31b36530":"markdown","75f895b6":"markdown","70282810":"markdown","5555eb7b":"markdown","37a87286":"markdown","015934c5":"markdown","8204d873":"markdown","2f80f5d4":"markdown","831a8d5b":"markdown","f0a46ede":"markdown","afc81870":"markdown"},"source":{"69436e4c":"#@Rita","14fac904":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","d6bc63c9":"df = pd.read_csv('..\/input\/heart.csv')","865d83c1":"df.head()","f660d2bd":"# We must clean the NaN values, since we cannot train\n# our model with unknown values.\n# Luckily, there is nothing to clean.\ndf.info() , df.isna().sum()","8ded3c5a":"# We have no need to encode labels since \n# the categories are fine (integers).","021d4b02":"# GOAL: presence of heart disease in a patient given the explanatory variables\n# which are age, sex, and so on.\n# Therefore, target is YES or NO (1\/0) depending whether a patient has got the disease or not.\ndf.target.value_counts()","7b324300":"# The best way to explore data is via plots\n# so we gonna plot some stuff","776cc030":"# First of all let's see how many zeros and ones do we have...\nnegative_target = len(df[df.target == 0])\npositive_target = len(df[df.target == 1])\nprint(\"Percentage of Patients that do not have Heart Disease: {:.3f}%\".format((negative_target \/ (len(df.target))*100)))\nprint(\"Percentage of Patients that have Heart Disease: {:.3f}%\".format((positive_target \/ (len(df.target))*100)))\nsns.countplot(x = \"target\", data = df, palette = \"pastel\")\nplt.xlabel(\"Target (0 = no, 1= yes)\")\nplt.ylabel(\"count\")\nplt.show()","59a257b8":"# Males vs Females\nfemale_patient = len(df[df.sex == 0])\nmale_patient = len(df[df.sex == 1])\nprint(\"Percentage of Female Patients: {:.3f}%\".format((female_patient \/ (len(df.sex))*100)))\nprint(\"Percentage of Male Patients: {:.3f}%\".format((male_patient \/ (len(df.sex))*100)))\nsns.countplot(x = 'sex', data = df, palette = \"pastel\")\nplt.xlabel(\"Sex (0 = female, 1= male)\")\nplt.ylabel(\"count\")\nplt.show()","8d2b8c77":"for x in range (0, 4):\n    print(\"% of patients that show positivity with chest pain {}: {:.3f}%\".format(x, 100*(((df['cp'] == x) & df['target'] == 1).sum())\/((df['target'] == 1).sum())))","b5f7c35d":"for x in range (0, 4):\n    print(\"% of patients that show negativity with chest pain {}: {:.3f}%\".format(x, 100*(((df['cp'] == x) & (df['target'] == 0)).sum())\/((df['target'] == 0).sum())))","5054b31a":"print(\"% of patients that show positivity with trestbps > 130: {:.3f}%\".format\n      (100*(((df['trestbps'] > 130) & df['target'] == 1).sum())\n       \/((df['target'] == 1).sum())))\nprint(\"% of patients that show positivity with trestbps <= 130: {:.3f}%\".format\n      (100*(((df['trestbps'] <= 130) & df['target'] == 1).sum())\n       \/((df['target'] == 1).sum())))","0184f182":"print(\"% of patients that show positivity with chol > 200: {:.3f}%\".format\n      (100*(((df['chol'] > 200) & df['target'] == 1).sum())\n       \/((df['target'] == 1).sum())))\nprint(\"% of patients that show positivity with chol <= 200: {:.3f}%\".format\n      (100*(((df['chol'] <= 200) & df['target'] == 1).sum())\n       \/((df['target'] == 1).sum())))","679878cf":"for x in range (0, 2):\n    print(\"% of patients that show positivity with fbs {}: {:.3f}%\".format(x, 100*(((df['fbs'] == x) & df['target'] == 1).sum())\/((df['target'] == 1).sum())))","07249fce":"for x in range (0, 3):\n    print(\"% of patients that show positivity with restecg {}: {:.3f}%\".format(x, 100*(((df['restecg'] == x) & df['target'] == 1).sum())\/((df['target'] == 1).sum())))","e951520c":"print(\"% of patients that show positivity with thalach > 100: {:.3f}%\".format\n      (100*(((df['thalach'] > 100) & df['target'] == 1).sum())\n       \/((df['target'] == 1).sum())))\nprint(\"% of patients that show positivity with thalach <= 100: {:.3f}%\".format\n      (100*(((df['thalach'] <= 100) & df['target'] == 1).sum())\n       \/((df['target'] == 1).sum())))","5d47d59c":"for x in range (0, 2):\n    print(\"% of patients that show positivity with exang {}: {:.3f}%\".format(x, 100*(((df['exang'] == x) & df['target'] == 1).sum())\/((df['target'] == 1).sum())))","4de3f9ce":"for x in range (0, 3):\n    print(\"% of patients that show positivity with slope {}: {:.3f}%\".format(x, 100*(((df['slope'] == x) & df['target'] == 1).sum())\/((df['target'] == 1).sum())))\nprint(\"We can suppose that Slope = 2 is likely to sign Heart disease.\")   ","aea0f4a6":"for x in range (0, 3):\n    print(\"% of patients that show positivity with ca {}: {:.3f}%\".format(x, 100*(((df['ca'] == x) & df['target'] == 1).sum())\/((df['target'] == 1).sum())))","98b2a523":"# We have got more positive samples than negative,\n# And we have more females than males in the dataset...\n# but what is the proportion of positivity by sex?","a31e0e87":"pd.crosstab(df.sex,df.target).plot(kind=\"bar\",figsize=(20,6), color=['#99ccff','#ffcc99'])\nplt.title('Heart Disease Frequency for Sex')\nplt.xlabel('Sex')\nplt.ylabel('Frequency')\nplt.show()","766fa98a":"# Intuitively, age must be an important factor:\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.countplot(x = 'age', data = df, palette = \"pastel\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"count\")\nplt.show()","9845690e":"pd.crosstab(df.age, df.target).plot(kind = \"bar\", figsize = (20,6), color=['#99ccff','#ffcc99'])\nplt.title('Heart Disease Frequency by Age')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","9b0c7c6c":"# Now, let's dive into more medical categories: \n# What type of chest pain is likely to show a positive result?","2c4860d4":"pd.crosstab(df.cp,df.target).plot(kind=\"bar\",figsize=(20,6), color=['#99ccff','#ffcc99'])\nplt.title('Heart Disease Frequency for Chest Pain Type')\nplt.xlabel('Chest Pain Type')\nplt.ylabel('Frequency')\nplt.show()","81c380b0":"# Chest pain by age?\npd.crosstab(df.age, df.cp).plot(kind = \"bar\", figsize = (20,6), color=['#99ccff','#ffcc99', '#ffccff','#99ff99'])\nplt.title('Chest Pain type by Age')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","25d78341":"pd.crosstab(df.ca,df.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for # of major vessels')\nplt.xlabel('Number of major vessels')\nplt.ylabel('Frequency')\nplt.show()","5cf2cb86":"# The plot below can be used to determine the correlation between the different features of the dataset. \n# From the above set we can also find out the features which have the most and the least effect on the target \n# feature (whether the patient have heart diseases or not).","2ccd6839":"sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)\n_ = plt.title('Correlation')","7b728558":"# target is highly correlated with CP, THALACH, EXANG, OLDPEAK.\n# Secondly with CA, SLOPE, THAL.","f6956ec7":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score","f993fcb8":"# It is a classification problem, and more precisely a BINARY CLASSIFICATION.\n# we divide into two subsets: the training data and the testing data\n# X are the explanatory variables, Y is the response variable ('target'):\nX = df.drop('target', axis=1)  # everything except target.\nY = df['target']               # only target.\n\n\n# Since the amount of data is not extremely large, we will use a small test_size (0.10-0.15).\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15) ","3ddf183b":"# Firstly, let's take Random Forest Classifier\nrfc = RandomForestClassifier(n_estimators=1000)","401695f1":"rfc.fit(X_train, Y_train)\nrfc.score(X_test, Y_test)","0c632aa1":"rfc_predictions = rfc.predict(X_test)\ntest_error = accuracy_score(rfc_predictions, Y_test)\n# test_error == rfc.score(X_test, Y_test)\ntest_error","6d660db7":"# Cross validation: see how it works in average:\ncross_validation_scores = cross_val_score(rfc, X, Y, cv=10)\nprint('Average accuracy for Random Forest: %0.4f' %cross_validation_scores.mean())\nprint(cross_validation_scores)","6d9fa0ae":"# no overfitting!\n# since test_error is lower than test_train (= cross_validation_scores.mean()).\n# Let's try a smaller test_size:\nrfc_2 = RandomForestClassifier(n_estimators=1000)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05) \nrfc_2.fit(X_train, Y_train)\nrfc_2.score(X_test, Y_test), cross_val_score(rfc_2, X, Y, cv=10).mean()\n# overfitting.","8e79d72b":"# Tuning of random forest classifier:\n# 1. n_estimators HIGH.  The more estimators you give it, the better it will do.\n# 2. max_features.  It may have a large impact on the behavior of the RF because it decides \n# how many features each tree in the RF considers at each split. Default is 'sqrt'.\ntuned_rfc = RandomForestClassifier(n_estimators = 5000, oob_score = True,\n                                   max_depth = None, max_features = 'sqrt')\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05) \ntuned_rfc.fit(X_train, Y_train)\ntuned_rfc.score(X_test, Y_test), cross_val_score(tuned_rfc, X, Y, cv=10).mean()\n\n# I have tried max_features = 'log2' and it does not improve, so the best we can do in this\n# case is increasing n_estimators.","10a0c702":"# KNN Model\nfrom sklearn.neighbors import KNeighborsClassifier\nfor i in range (1,10):\n    knn = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn.fit(X_train, Y_train)\n    prediction = knn.predict(X_train)\n\n    print(\"{} NN Score: {:.4f}%\".format(i, knn.score(X_test, Y_test)*100))","d48ebcb0":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(X_train, Y_train)\n\nprint(\"Test Accuracy of SVM Algorithm: {:.4f}%\".format(svm.score(X_test, Y_test)*100))","fb822ab3":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, Y_train)\nprint(\"Accuracy of Naive Bayes: {:.4f}%\".format(nb.score(X_train, Y_train)*100))","c4ad5e38":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, Y_train)\nprint(\"Decision Tree Test Accuracy {:.4f}%\".format(dtc.score(X_train, Y_train)*100))","31429025":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 1000, random_state = 1)\nrf.fit(X_train, Y_train)\nprint(\"Random Forest Algorithm Accuracy Score : {:.4f}%\".format(rf.score(X_train, Y_train)*100))","36b9c85a":"from sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15)\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(probability=True,gamma='scale'),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=100),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression(solver='lbfgs')]\n\nscores = []\nfor clf in classifiers:\n    clf.fit(X_train, Y_train)\n    cv_results = cross_validate(clf, X_test, Y_test, cv=5, return_train_score=True)\n    scores.append(np.mean(cv_results['test_score']))\n    \nsns.barplot(y=[n.__class__.__name__  for n in classifiers], x=scores, orient='h')","2bfd05a3":"scores","39b4029e":"# We observe that logistics regression is the best BUT! \ud83d\ude48 \n# If we use a LINEAR kernel in SVC it should work better!\nsvm = SVC(probability=True, kernel='linear',gamma='scale')\nsvm.fit(X_train, Y_train)\n\nprint(\"Test Accuracy of SVM Algorithm: {:.4f}%\".format(svm.score(X_test, Y_test)*100))\ncv_result = cross_val_score(svm, X, Y, cv=10)\nprint(cv_result.mean())","31dcd625":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nX = df.drop('target', axis=1) # everything except target.\nY = df['target']               # only target.\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15)\nnb.fit(X_train, Y_train)\n\nprint(\"Accuracy of Naive Bayes: {:.4f}%\".format(nb.score(X_train, Y_train)*100))","e0353b75":"# Which SVC kernel is better?\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05)\nsvm_linear = SVC(probability=True, kernel='linear',gamma='scale')\nsvm_linear.fit(X_train, Y_train)\n\nprint(\"Test Accuracy of SVM Algorithm: {:.4f}%\".format(svm_linear.score(X_test, Y_test)*100))\ncv_result_linear = cross_val_score(svm_linear, X, Y, cv=10)","bba6fef8":"svm_poly = SVC(probability=True, kernel='poly',degree=3,gamma='scale')\nsvm_poly.fit(X_train, Y_train)\n\nprint(\"Test Accuracy of SVM Algorithm: {:.4f}%\".format(svm_poly.score(X_test, Y_test)*100))\ncv_result_poly = cross_val_score(svm_poly, X, Y, cv=3)","6d5809ba":"svm_rbf = SVC(probability=True, kernel='rbf',gamma='scale')\nsvm_rbf.fit(X_train, Y_train)\n\nprint(\"Test Accuracy of SVM Algorithm: {:.4f}%\".format(svm_rbf.score(X_test, Y_test)*100))\ncv_result_rbf = cross_val_score(svm_rbf, X, Y, cv=10)","1f0d8731":"# order by\nprint(cv_result_linear.mean(), cv_result_poly.mean(), cv_result_rbf.mean())","762ad5a1":"X_test.head()","3b88b67e":"Y_test.head()","4142abbd":"print(nb.predict((X_test.loc[[227]]).values.tolist()),\nnb.predict((X_test.loc[[269]]).values.tolist()),\nnb.predict((X_test.loc[[262]]).values.tolist()),\nnb.predict((X_test.loc[[300]]).values.tolist()),\nnb.predict((X_test.loc[[192]]).values.tolist()))","0cfcca9f":"# \ud83d\ude2e","37de272f":"pairs = sns.pairplot(df)\npairs","b622125b":"pairs.savefig('a')\n# age with chol.\n# trestbps with chol.\n# thalach with chol.","ccbfce26":"from sklearn.decomposition import PCA\npca = PCA()","2f0cea85":"df_pca = pca.fit_transform(df)\ny_variance = pca.explained_variance_ratio_\npd.DataFrame(pca.components_, columns=df.columns)","23931c36":"# See that 0 has 0.99 of CHOL\n# 1 has -0.97 of THALACH\n# 2 has 0.98 of TRESTBPS\n# recall:\nsns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)\n# Possibly others had high values due to multicollinearity.","49a31712":"sns.barplot(x=[i for i in range(len(y_variance))], y=y_variance)\nplt.title(\"PCA\")","543e3eaf":"X = df.drop('target', axis=1) \npca = PCA()\npca.fit(X)\ndf_pca = pca.transform(X)\nprint(\"original shape:   \", X.shape)\nprint(\"transformed shape:\", df_pca.shape)","3041a8f9":"plt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\n\n# we can observe the cumulative explained variance.","5efe87f6":"np.cumsum(pca.explained_variance_ratio_)[2], np.cumsum(pca.explained_variance_ratio_)[3]\n# With 3 components it's more than enough, even with 2 we can explain the 98% of the\n# variability in the data! \ud83d\ude2e","a9376777":"sns.barplot(np.arange(0,14),pca.explained_variance_ratio_)","35e40429":"from sklearn.datasets import make_blobs\nfrom sklearn import decomposition","13b2f22b":"# I decided to take 2 components:\npca = decomposition.PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\ndf_pca = pd.DataFrame(data = X_pca , \n        columns = ['PC1', 'PC2'])\ndf_pca['Cluster'] = Y\ndf_pca.head()","a16a6c17":"pca.explained_variance_ratio_","76a22a8f":"pc_df = pd.DataFrame({'var': pca.explained_variance_ratio_,\n             'PC':['PC1','PC2']})\nsns.barplot(x='PC',y=\"var\", \n           data=pc_df, color=\"c\");","344b65b4":"sns.lmplot( x=\"PC1\", y=\"PC2\",\n  data=df_pca, \n  fit_reg=False, \n  hue='Cluster', # color by cluster\n  legend=True,\n  scatter_kws={\"s\": 80}) # specify the point size","573820a3":"from sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, Y)\nmodel = SelectFromModel(lsvc, prefit=True)\nfeature_index = model.get_support()\nfeature_index # what features have an explicit influence","e1c79646":"# We take them \nX.head()\nX_up = X[X.columns[feature_index]]\nX[X.columns[feature_index]].head()","87c6acf0":"# First, let's see if our tuned RFC improves...\ntuned_rfc = RandomForestClassifier(n_estimators = 1000, oob_score = True,\n                                   max_depth = None, max_features = 'sqrt')\nX_train, X_test, Y_train, Y_test = train_test_split(X_up, Y, test_size=0.05) \ntuned_rfc.fit(X_train, Y_train)\ntuned_rfc.score(X_test, Y_test), cross_val_score(tuned_rfc, X_up, Y, cv=10).mean()","3145a556":"# Seems like not \ud83d\ude22","633dd80f":"# Let's try our best option SVC: \nX_train, X_test, Y_train, Y_test = train_test_split(X_up, Y, test_size=0.05) \n\nsvm = SVC(probability=True, kernel='linear',gamma='scale')\nsvm.fit(X_train, Y_train)\n\nprint(\"Test Accuracy of SVM Algorithm: {:.4f}%\".format(svm.score(X_test, Y_test)*100))\ncv_result = cross_val_score(svm, X, Y, cv=10)\nprint(cv_result.mean())","633fe296":"# Not better \ud83d\ude11 OK.","536d69b8":"# First, let's see an example with 3 clusters:\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)","74878de7":"kmeans.labels_","0ab5ff0d":"kmeans.inertia_ # that's the cost, the lower - the better.","a3c81401":"df['KMEANS'] = kmeans.labels_","6eb27573":"df.head()","289ffe37":"plt.scatter(x=df_pca['PC1'], y=df_pca['PC2'], c=kmeans.labels_*2 )","56ddaf22":"from sklearn import metrics\nfrom sklearn.metrics import pairwise_distances\nkmeans_cost = np.array([])\ncalinskis = np.array([])\nfor k in range(2,20):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(X)\n    kmeans_cost = np.append(kmeans_cost, kmeans.inertia_)\n    calinskis = np.append(calinskis, metrics.calinski_harabaz_score(pc, kmeans.labels_))\nplt.scatter(x=df_pca['PC1'], y=df_pca['PC2'], c=kmeans.labels_*2 )","8f38a308":"x = np.arange(2,20)\ny = kmeans_cost\nsns.barplot(x,y)","928f1c5a":"plt.scatter(x, calinskis), calinskis","97c229a6":"# We take the best k: where the cost does not vary much. See it with calinski\n# where do we have the lowest value? 11 groups:\nkmeans = KMeans(n_clusters=11)\nkmeans.fit(X)\nkmeans_cost = np.append(kmeans_cost, kmeans.inertia_)\nplt.scatter(x=df_pca['PC1'], y=df_pca['PC2'], c=kmeans.labels_*2 )","9a6b97a3":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.utils import to_categorical\nimport keras","a9efcd3d":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10)\n\naccuracies = []\nlosses = []\nfor i in range(0, 5):\n    model = Sequential()\n    model.add(Dense(5, input_dim=X_train.shape[1], activation='relu'))    \n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n    model.fit(X_train, Y_train, epochs=400, batch_size=32, verbose=1)\n\n    loss, acc = model.evaluate(X_test, Y_test, batch_size=32)\n\n","981004bb":"accuracies = []\nlosses = []\n# It is better for internal nodes to have linear and rectified activations.\n# In binary classification, the last level has to have just ONE node.\n# And it's better to have there a sigmoid-like activation to distinguish between the\n# 2 states (0 and 1).\ninternal = ['relu', 'tanh', 'linear']\nlast = ['sigmoid', 'tanh', 'softsign']\noptimizer = ['RMSprop', 'SGD', 'adam']\n\nfor internal_activation_func in range(len(internal)):\n    for last_activation_func in range(len(last)):\n        for j in range(0,5):\n            print(\"INTERNAL: \" + internal[internal_activation_func])\n            print(\"LAST: \" + last[last_activation_func])\n            \n            model = Sequential()\n            model.add(Dense(5, input_dim=X_train.shape[1], activation=internal[internal_activation_func]))    \n            model.add(Dense(1, activation=last[last_activation_func]))\n            model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n\n            # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n            model.fit(X_train, Y_train, epochs=400, batch_size=32, verbose=1)\n\n            loss, acc = model.evaluate(X_test, Y_test, batch_size=32)\n\n            accuracies.append(acc)\n            losses.append(loss)\n\n","4451e604":"# After 20 minutes... ","bd1aa739":"print(\"NEURAL NETWORK 1:\")\nprint(\"\\tINTERNAL: \" + internal[0])\nprint(\"\\tLAST: \" + last[0])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[0:5])))\n\nprint(\"NEURAL NETWORK 2:\")\nprint(\"\\tINTERNAL: \" + internal[0])\nprint(\"\\tLAST: \" + last[1])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[5:10])))\n\nprint(\"NEURAL NETWORK 3:\")\nprint(\"\\tINTERNAL: \" + internal[0])\nprint(\"\\tLAST: \" + last[2])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[10:15])))","cadc185e":"print(\"NEURAL NETWORK 4:\")\nprint(\"\\tINTERNAL: \" + internal[1])\nprint(\"\\tLAST: \" + last[0])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[15:20])))\n\nprint(\"NEURAL NETWORK 5:\")\nprint(\"\\tINTERNAL: \" + internal[1])\nprint(\"\\tLAST: \" + last[1])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[20:25])))\n\nprint(\"NEURAL NETWORK 6:\")\nprint(\"\\tINTERNAL: \" + internal[1])\nprint(\"\\tLAST: \" + last[2])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[25:30])))","6ccb54c7":"print(\"NEURAL NETWORK 7:\")\nprint(\"\\tINTERNAL: \" + internal[2])\nprint(\"\\tLAST: \" + last[0])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[20:25])))\n\nprint(\"NEURAL NETWORK 8:\")\nprint(\"\\tINTERNAL: \" + internal[2])\nprint(\"\\tLAST: \" + last[1])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[25:30])))\n\nprint(\"NEURAL NETWORK 9:\")\nprint(\"\\tINTERNAL: \" + internal[2])\nprint(\"\\tLAST: \" + last[2])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[35:40])))","a5bc0b4a":"# Horrible neural networks, bye \ud83e\udd2f","cd76a442":"# Let's do a single neural network with several layers and with act functions\n# which have given the best results in the previous attempt, which are TANH and SIGMOID:","e37cd1b6":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05)\naccuracies = []\nlosses = []\nfor j in range(0,5):\n    model = Sequential()\n    model.add(Dense(5, input_dim=X_train.shape[1], activation='tanh'))  \n    model.add(Dense(5, activation='tanh'))    \n    model.add(Dense(5, activation='tanh'))    \n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n\n    # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n    model.fit(X_train, Y_train, epochs=400, batch_size=32, verbose=1)\n\n    loss, acc = model.evaluate(X_test, Y_test, batch_size=32)\n\n    accuracies.append(acc)\n    losses.append(loss)","19388571":"np.mean(accuracies) # well...","46df47ca":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05)\naccuracies = []\nlosses = []\nfor j in range(0,5):\n    model = Sequential()\n    model.add(Dense(10, input_dim=X_train.shape[1], activation='tanh'))  \n    model.add(Dense(2, activation='sigmoid'))     \n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n\n    # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n    model.fit(X_train, Y_train, epochs=400, batch_size=32, verbose=1)\n\n    loss, acc = model.evaluate(X_test, Y_test, batch_size=32)\n\n    accuracies.append(acc)\n    losses.append(loss)","e3d0b8ca":"np.mean(accuracies) ","62ed11d5":"model.summary()","ed5f7e12":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05)\naccuracies = []\nlosses = []\nfor j in range(0,5):\n    model = Sequential()\n    model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))  \n    model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n\n    # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n    model.fit(X_train, Y_train, epochs=400, batch_size=32, verbose=1)\n\n    loss, acc = model.evaluate(X_test, Y_test, batch_size=32)\n\n    accuracies.append(acc)\n    losses.append(loss)","f79b8ea6":"# approx 0.5x","7a7c3a67":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05)\naccuracies = []\nlosses = []\nfor j in range(0,5):\n    model = Sequential()\n    model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))  \n    model.compile(loss= 'binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n    \n\n    # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n    model.fit(X_train, Y_train, epochs=400, batch_size=32, verbose=1)\n\n    loss, acc = model.evaluate(X_test, Y_test, batch_size=32)\n\n    accuracies.append(acc)\n    losses.append(loss)","a7402df0":"np.mean(accuracies) #pff","d4c233fb":"# that's all","401698dd":"# Neural Network \ud83d\udd78\ud83d\udd77","38cc5757":"# 6 --> PCA","794916ca":"# 1--> GET THE DATA","cf0dc963":"# 5 --> PREDICT THE DATA","214d1f48":"7. Principal components' analysis","58a2ff77":"# 2--> CLEAN THE DATA\n####  and make it easier to work with...","4a13fe01":"2. Analysis of the problem:","7ace8d40":"3. Cleaning ","a88c2185":"# 3--> EXPLORE THE DATA\n#### and find the explanatory and the response variables...","b78dcebf":"9. Analysis of obtained predictions","8db39031":"#### RANDOM FOREST","31b36530":"5. Dataset analysis","75f895b6":"4. New features","70282810":"1. Identification of the problem:","5555eb7b":"# 4 --> TRAIN THE DATA\n#### and don't forget about CROSS VALIDATION","37a87286":"**AGE**: age in years.\n\n**SEX**: (1 = male; 0 = female).\n\n**CP**: chest pain type.\n+ 0, Typical angina: chest pain related to the decrease of blood supply to the heart.\n+ 1, Atypical angina: chest pain not related to the heart.\n+ 2, Non-anginal pain: esophageal spasms (not related to the heart).\n+ 3, Asymptomatic: chest pain not showing signs of disease.\n\n**TRESTBPS**: resting blood pressure (in mmHg on admission to the hospital).\n+ Anything above 130-140 is typically cause for concern.\n\n**CHOL**: serum cholestoral level in mg\/dl.\n+ serum = LDL + HDL + 0.2*triglycerides.\n+ Above 200 is cause for concern.\n\n**FBS**: (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false).\n+ fbs > 126 mg\/dL signals diabetes.\n\n**RESTECG**: resting electrocardiographic results (0 - 2).\n+ 0, Nothing to note.\n+ 1, ST-T Wave abnormality. Signals non-normal heart beat \u2764\ufe0f.\n+ 2, Possible or definite Left ventricular hypertrophy.\n\n**THALACH**: maximum heart rate achieved.\n+ Rate above 100 is cause for concern.\n\n**EXANG**: exercise induced angina (1 = yes; 0 = no).\n\n**OLDPEAK**: ST depression induced by exercise relative to rest.\n+ Looks at stress of heart during exercise.\n+ Unhealthy heart will stress more.\n\n**SLOPE**: the slope of the peak exercise ST segment.\n+ 0, Upsloping: better heart rate with exercise (uncommon).\n+ 1, Flatsloping: minimal change (typical healthy heart).\n+ 2, Downsloping: signs of unhealthy heart.\n\n**CA**: number of major vessels (0 - 3) colored by flourosopy.\n\n**THAL**: thalium stress test result. Sees how blood moves through the heart while exercising.\n+ 1, 3 = normal; \n+ 6 = fixed defect; Used to be defect, but now it is okay.\n+ 7 = reversable defect. Not proper blood movement when exercising.\n\n**TARGET**: heart disease (1 or 0).","015934c5":"#### KNN Model","8204d873":"10. Clustering","2f80f5d4":"6. Feature selection","831a8d5b":"12. Neural network","f0a46ede":"8. Modelos de ML","afc81870":"11. SVC"}}