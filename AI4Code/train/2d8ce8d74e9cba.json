{"cell_type":{"c8933f13":"code","2dedc046":"code","d62f33f3":"code","6f3bc06f":"code","8b3c5a24":"code","aa792003":"code","c3b3fc1e":"code","53c41281":"code","1454c295":"code","866c6d55":"code","02abf6e4":"code","6c7df6e6":"code","b464f671":"code","56b4ed65":"code","a17b00f9":"code","fc40a89b":"code","ecaaf1b8":"code","e8f2f96f":"code","bdc6c317":"code","d9b484b3":"code","594bb092":"markdown","0a98bc61":"markdown","ffb98576":"markdown","2235d7f4":"markdown","a232b036":"markdown","9301885b":"markdown","c020ac10":"markdown","44db2783":"markdown","b42a27e4":"markdown","2d061103":"markdown","bb3d8f1c":"markdown","e7c78c16":"markdown","9bf050eb":"markdown","f3902ddf":"markdown","2392965e":"markdown","92c652f4":"markdown","e2894df7":"markdown","004264cf":"markdown","9dcc2163":"markdown","90e5a548":"markdown","ee2d9feb":"markdown","3aa8cefc":"markdown","27ba637e":"markdown","778c34af":"markdown","fb58dd83":"markdown","f16d7926":"markdown","db67710c":"markdown","1254cdef":"markdown","5421d21f":"markdown","d5200898":"markdown","6df658d8":"markdown"},"source":{"c8933f13":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Standard plotly imports\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\nimport cufflinks as cf\nimport plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\ncufflinks.go_offline(connected=True)\n\nimport os\nprint(os.listdir(\"..\/input\"))","2dedc046":"df_train_id = pd.read_csv(\"..\/input\/train_identity.csv\", nrows=50000)\ndf_train_trans = pd.read_csv(\"..\/input\/train_transaction.csv\", nrows=50000)\n#df_test_id = pd.read_csv(\"..\/input\/test_identity.csv\")\n#df_test_trans = pd.read_csv(\"..\/input\/test_transaction.csv\")\n","d62f33f3":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\ndef plot_distribution(df, var_select=None, title=None, bins=1.0): \n    # Calculate the correlation coefficient between the new variable and the target\n    tmp_fraud = df[df['isFraud'] == 1]\n    tmp_no_fraud = df[df['isFraud'] == 0]    \n    corr = df['isFraud'].corr(df[var_select])\n    corr = np.round(corr,3)\n    tmp1 = tmp_fraud[var_select].dropna()\n    tmp2 = tmp_no_fraud[var_select].dropna()\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['Fraud', 'No Fraud']\n    colors = ['seagreen','indianred', ]\n\n    fig = ff.create_distplot(hist_data,\n                             group_labels,\n                             colors = colors, \n                             show_hist = True,\n                             curve_type='kde', \n                             bin_size = bins\n                            )\n    \n    fig['layout'].update(title = title+' '+'(corr target ='+ str(corr)+')')\n\n    iplot(fig, filename = 'Density plot')\n    \ndef plot_dist_churn(df, col, binary=None):\n    tmp_churn = df[df[binary] == 1]\n    tmp_no_churn = df[df[binary] == 0]\n    tmp_attr = round(tmp_churn[col].value_counts().sort_index() \/ df[col].value_counts().sort_index(),2)*100\n    print(f'Distribution of {col}: ')\n    trace1 = go.Bar(\n        x=tmp_churn[col].value_counts().sort_index().index,\n        y=tmp_churn[col].value_counts().sort_index().values, \n        name='Fraud',opacity = 0.8, marker=dict(\n            color='seagreen',\n            line=dict(color='#000000',width=1)))\n\n    trace2 = go.Bar(\n        x=tmp_no_churn[col].value_counts().sort_index().index,\n        y=tmp_no_churn[col].value_counts().sort_index().values,\n        name='No Fraud', opacity = 0.8, \n        marker=dict(\n            color='indianred',\n            line=dict(color='#000000',\n                      width=1)\n        )\n    )\n\n    trace3 =  go.Scatter(   \n        x=tmp_attr.sort_index().index,\n        y=tmp_attr.sort_index().values,\n        yaxis = 'y2', \n        name='% Fraud', opacity = 0.6, \n        marker=dict(\n            color='black',\n            line=dict(color='#000000',\n                      width=2 )\n        )\n    )\n    \n    layout = dict(title =  f'Distribution of {str(col)} feature by %Fraud',\n              xaxis=dict(type='category'), \n              yaxis=dict(title= 'Count'), \n              yaxis2=dict(range= [0, 15], \n                          overlaying= 'y', \n                          anchor= 'x', \n                          side= 'right',\n                          zeroline=False,\n                          showgrid= False, \n                          title= 'Percentual Fraud Transactions'\n                         ))\n\n    fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n    iplot(fig)\n    \n## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n## REducing memory\ndf_train_trans = reduce_mem_usage(df_train_trans)\ndf_train_id = reduce_mem_usage(df_train_id)","6f3bc06f":"## REducing memory\ndf_train_trans = reduce_mem_usage(df_train_trans)\ndf_train_id = reduce_mem_usage(df_train_id)","8b3c5a24":"resumetable(df_train_id)","aa792003":"resumetable(df_train_trans)[:25]","c3b3fc1e":"print(\"Transactions % Fraud:\")\nprint(round(df_train_trans[['isFraud', 'TransactionID']]['isFraud'].value_counts(normalize=True) * 100,2))\n# df_train.groupby('Churn')['customerID'].count().iplot(kind='bar', title='Churn (Target) Distribution', \n#                                                      xTitle='Customer Churn?', yTitle='Count')\n\ntrace0 = go.Bar(\n    x=df_train_trans[['isFraud', 'TransactionID']].groupby('isFraud')['TransactionID'].count().index,\n    y=df_train_trans[['isFraud', 'TransactionID']].groupby('isFraud')['TransactionID'].count().values,\n    marker=dict(\n        color=['indianred', 'seagreen']),\n)\n\ndata = [trace0] \nlayout = go.Layout(\n    title='Fraud (Target) Distribution <br>## 0: No Fraud | 1: Is Fraud ##', \n    xaxis=dict(\n        title='Transaction is fraud', \n        type='category'),\n    yaxis=dict(\n        title='Count')\n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","53c41281":"def print_trans(tmp, num_col='TransactionAmt'):\n    print(f\"The mininum value in Transaction Amount is {tmp[num_col].min()}, median is {round(tmp[num_col].median(),2)}, and the maximum is {df_train_trans[num_col].max()}\")\n    print(f\"The mean Transaction Amount of Fraudulent Transactions is {round(tmp[tmp['isFraud'] == 1][num_col].median(),2)}\\\n          \\nThe mean Transaction Amount of No-Fraudulent Transactions is {round(tmp[tmp['isFraud'] == 0][num_col].median(),2)}\")\n    \nprint_trans(df_train_trans[['isFraud', 'TransactionAmt']], 'TransactionAmt')","1454c295":"print(\"Transaction Amount Quantiles: \")\nprint(df_train_trans['TransactionAmt'].quantile([0.01, .025, .1, .25, .5, .75, .975, .99]))","866c6d55":"# df_train_trans['TransactionAmt_log'] = df_train_trans['TransactionAmt'].apply(np.log)\ntmp = df_train_trans[['TransactionAmt', 'isFraud']]\ntmp['TransactionAmt_log'] = tmp['TransactionAmt'].apply(np.log)\n## Calling the function\nplot_distribution(tmp[(tmp['TransactionAmt'] <= 800)], 'TransactionAmt', 'Transaction Amount Distribution', bins=10.0,)\nplot_distribution(tmp[(tmp['TransactionAmt'] <= 800)], 'TransactionAmt_log', 'Transaction Amount Log Distribution', bins=0.1)","02abf6e4":"plot_dist_churn(df_train_trans[['ProductCD', 'isFraud']], 'ProductCD', 'isFraud')","6c7df6e6":"for col in ['card4', 'card6']:\n    df_train_trans[col] = df_train_trans[col].fillna('NoInf')\n    plot_dist_churn(df_train_trans, col, 'isFraud')","b464f671":"print(\"Card Features Quantiles: \")\nprint(df_train_trans[['card1', 'card2', 'card3', 'card5']].quantile([0.01, .025, .1, .25, .5, .75, .975, .99]))","56b4ed65":"for col in ['card1', 'card2', 'card3', 'card5']:\n    df_train_trans[str(col)+'_log'] = np.log(df_train_trans[col])","a17b00f9":"## Calling the function\nplot_distribution(df_train_trans[['isFraud','card1_log']], 'card1_log', 'Card 1 Feature Log Distribution by Target', bins=0.05,)","fc40a89b":"## Calling the function\nplot_distribution(df_train_trans[['isFraud','card2_log']], 'card2_log', 'Card 2 Feature Log Distribution by Target', bins=0.05)\n","ecaaf1b8":"df_train_trans.loc[df_train_trans.card3.isin(df_train_trans['card3'].value_counts()[df_train_trans['card3'].value_counts() < 10].index), 'card3'] = -99","e8f2f96f":"plot_dist_churn(df_train_trans[['card3', 'isFraud']], 'card3', 'isFraud')","bdc6c317":"df_train_trans.loc[df_train_trans.card5.isin(df_train_trans['card5']\\\n                                             .value_counts()\\\n                                             [df_train_trans['card5']\\\n                                              .value_counts() < 20]\\\n                                             .index), 'card5'] = -99\n\nplot_dist_churn(df_train_trans[['card5', 'isFraud']], 'card5', 'isFraud')","d9b484b3":"tmp = df_train_trans[['M1','M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'isFraud']]\nfor col in ['M1','M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']:\n    tmp[col] = tmp[col].fillna('NoInf')\n    plot_dist_churn(tmp, col, 'isFraud')","594bb092":"## Knowning the Identity dataset\n- What type of data we have on our data?","0a98bc61":"We can see that fraudulent transactions has a higher mean than No-Fraudulent Transactions","ffb98576":"We don't have a high correlation between Transaction Amount and Fraud Transactions. <br>\nAlso, we can see many cases of fraud transactions with values between 5 to 14 and other peak in 75 -  85.\n\nLet's keep investigating this data","2235d7f4":"## Exploring Card Features \nWe have 6 columns that are about the Card of the transaction.<br>\nI will start by the categoricals and after it, I will explore the continuous ","a232b036":"Wow, We have a bizarre high dimension. The shape of Transactions is: 506691, 393<br>\nI will need some time to explore it further. The first aim is start simple. ","9301885b":"To avoid us of outliers and a better view of distribution, I will filter the data and get only values equal or lower than 800","c020ac10":"## Card3 feature by Target\n- As we have many values with low frequency, I will set all values with frequency lower than 10 as -99","44db2783":"## ScatterPollar of Binary Features","b42a27e4":"## Exploring the M2-M9 features\n- Seen","2d061103":"## Knowing the transactions\n- What type of data we have on our data?","bb3d8f1c":"from sklearn.preprocessing import LabelEncoder\ntmp = df_train_trans[]\n#Label encoding Binary columns\nle = LabelEncoder()\n\ntmp_churn = df_train_trans[df_train_trans['isFraud'] == 1]\ntmp_no_churn = df_train_trans[df_train_trans['isFraud'] == 0]\n\nbi_cs = df_train_trans.nunique()[df_train_trans.nunique() == 2].keys()\ndat_rad = df_train_trans[bi_cs]\n\nfor cols in bi_cs :\n    tmp_churn[cols] = le.fit_transform(tmp_churn[cols])\n    \ndata_frame_x = tmp_churn[bi_cs].sum().reset_index()\ndata_frame_x.columns  = [\"feature\",\"yes\"]\ndata_frame_x[\"no\"]    = tmp_churn.shape[0]  - data_frame_x[\"yes\"]\ndata_frame_x  = data_frame_x[data_frame_x[\"feature\"] != \"Churn\"]\n\n#count of 1's(yes)\ntrace1 = go.Scatterpolar(r = data_frame_x[\"yes\"].values.tolist(), \n                         theta = data_frame_x[\"feature\"].tolist(),\n                         fill  = \"toself\",name = \"Fraud 1's\",\n                         mode = \"markers+lines\", visible=True,\n                         marker = dict(size = 5)\n                        )\n\n#count of 0's(No)\ntrace2 = go.Scatterpolar(r = data_frame_x[\"no\"].values.tolist(),\n                         theta = data_frame_x[\"feature\"].tolist(),\n                         fill  = \"toself\",name = \"Fraud 0's\",\n                         mode = \"markers+lines\", visible=True,\n                         marker = dict(size = 5)\n                        ) \nfor cols in bi_cs :\n    tmp_no_churn[cols] = le.fit_transform(tmp_no_churn[cols])\n    \ndata_frame_x = tmp_no_churn[bi_cs].sum().reset_index()\ndata_frame_x.columns  = [\"feature\",\"yes\"]\ndata_frame_x[\"no\"]    = tmp_no_churn.shape[0]  - data_frame_x[\"yes\"]\ndata_frame_x  = data_frame_x[data_frame_x[\"feature\"] != \"Churn\"]\n\n#count of 1's(yes)\ntrace3 = go.Scatterpolar(r = data_frame_x[\"yes\"].values.tolist(),\n                         theta = data_frame_x[\"feature\"].tolist(),\n                         fill  = \"toself\",name = \"NoFraud 1's\",\n                         mode = \"markers+lines\", visible=False,\n                         marker = dict(size = 5)\n                        )\n\n#count of 0's(No)\ntrace4 = go.Scatterpolar(r = data_frame_x[\"no\"].values.tolist(),\n                         theta = data_frame_x[\"feature\"].tolist(),\n                         fill  = \"toself\",name = \"NoFraud 0's\",\n                         mode = \"markers+lines\", visible=False,\n                         marker = dict(size = 5)\n                        ) \n\ndata = [trace1, trace2, trace3, trace4]\n\nupdatemenus = list([\n    dict(active=0,\n         x=-0.15,\n         buttons=list([  \n            dict(\n                label = 'Fraud Dist',\n                 method = 'update',\n                 args = [{'visible': [True, True, False, False]}, \n                     {'title': 'Transaction Fraud Binary Counting Distribution'}]),\n             \n             dict(\n                  label = 'No-Fraud Dist',\n                 method = 'update',\n                 args = [{'visible': [False, False, True, True]},\n                     {'title': 'Transaction No-Fraud Binary Counting Distribution'}]),\n\n        ]),\n    )\n])\n\nlayout = dict(title='ScatterPolar Distribution of Fraud and No-Fraud Transactions (Select from Dropdown)', \n              showlegend=False,\n              updatemenus=updatemenus)\n\nfig = dict(data=data, layout=layout)\n\niplot(fig)","e7c78c16":"# Competition Objective is to detect fraud in transactions; \n\n## Data\n\n\nIn this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target ```isFraud```.\n\nThe data is broken into two files **identity** and **transaction**, which are joined by ```TransactionID```. \n\n> Note: Not all transactions have corresponding identity information.\n\n**Categorical Features - Transaction**\n\n- ProductCD\n- emaildomain\n- card1 - card6\n- addr1, addr2\n- P_emaildomain\n- R_emaildomain\n- M1 - M9\n\n**Categorical Features - Identity**\n\n- DeviceType\n- DeviceInfo\n- id_12 - id_38\n\n**The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n\n## Questions\nI will start exploring based on Categorical Features and Transaction Amounts.\nThe aim is answer some questions like:\n- What type of data we have on our data?\n- How many cols, rows, missing values we have?\n- Whats the target distribution?\n- What's the Transactions values distribution of fraud and no fraud transactions?\n- We have predominant fraudulent products? \n- What features or target shows some interesting patterns? \n- And a lot of more questions that will raise trought the exploration. \n\n\nI hope you enjoy my kernel and if it be useful for you, <b>upvote<\/b> the kernel","9bf050eb":"Cool!! Again, we can clearly see that card4 we can't see different patterns, but in Card6 we can note that Credit has higher incidence of fraud than Debit payment","f3902ddf":"## Understanding the Target Distribution","2392965e":"## Card1 feature by Target","92c652f4":"## Importing Libraries","e2894df7":"# NOTE: THIS KERNEL IS NOT FINISHED. I WILL KEEP EXPLORING IT. ","004264cf":"## Welcome to my fraud detection Kernel. \n\n","9dcc2163":"## Reading dataset","90e5a548":"## As we have a high dimensional data, I will reduce the memory usage","ee2d9feb":"Cool!!! I think that this chart is very insightful. <br>\nAltought the W is the most frequent Product we can see higher values in C, R and S products altought we have many lowest values in these categories. ","3aa8cefc":"We have shape 144.2 rows by 41 columns. <br>\nAlso, we can see that almost all features has missing values. We will need to work with that. <br>\nLet's see the transactions table and see the details ","27ba637e":"Nice. <br>\nWe have only 3.5% of positive values in our target. It's an unbalanced data and we will keep investiganting the data to find some insight.","778c34af":"\n![](http:\/\/technosavvy.co.ke\/wp-content\/uploads\/2015\/12\/fraud.jpg)","fb58dd83":"## Card5 feature by Target\n- Again, as we have many values with low frequency I will set all values with frequency lower than 20 as -99","f16d7926":"## Ploting and Knowing Transaction Amount distribution \n","db67710c":"## <font color=\"red\">Please if this kernel were useful for you, please <b>UPVOTE<\/b> the kernel and give me your feedback =)<\/font>\n","1254cdef":"## Functions to epxlore the data","5421d21f":"## Knowing the Product feature\n- We have predominant fraudulent products? ","d5200898":"I will transform Card1 and Card2 to Logarithm scale to we better understand the distribution ","6df658d8":"## Card2 feature by Target"}}