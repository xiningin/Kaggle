{"cell_type":{"07a70608":"code","087b3c20":"code","fce95b2d":"code","4ce14450":"code","e9a41b49":"code","a6bcb880":"code","22ff6ee3":"code","a2fade39":"code","3d357b8e":"code","78be9875":"code","e9981d91":"code","e8ab672e":"code","46dcfd85":"code","ff9d4a80":"code","de321768":"code","125464d5":"code","c15439e6":"code","a3daa386":"code","6c8e6294":"code","64d24838":"code","4a4a7688":"code","33c1b81d":"code","c34bed08":"code","c16122c6":"code","e18cc60b":"code","e14cbb54":"code","b0ac5099":"code","5c489c9b":"code","d5146f77":"code","e125cbca":"code","13d61d07":"code","192b8cd9":"code","d625d99e":"code","e910da1b":"code","039b03b5":"code","935d7a0e":"code","24396775":"code","6e3d907c":"code","9c8fd043":"code","7c0e5dae":"markdown","673a979f":"markdown","61a55099":"markdown","a321f8a4":"markdown","090f248a":"markdown","45769b30":"markdown","51b1ee97":"markdown","83104eb3":"markdown","5811a2e8":"markdown","29d38f89":"markdown","9bb92c7b":"markdown","845a2d8c":"markdown","b7a51384":"markdown","3c32d964":"markdown","8e772129":"markdown","73e7adbd":"markdown","d68af830":"markdown","b2b0aed5":"markdown","a75f5587":"markdown","a3ab46dc":"markdown","8c885d95":"markdown","b41294e0":"markdown","d9c5cae1":"markdown","3587c7ad":"markdown"},"source":{"07a70608":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","087b3c20":"import seaborn as sns\nimport matplotlib.pyplot as plt","fce95b2d":"data = pd.read_csv('..\/input\/customer-personality-analysis\/marketing_campaign.csv', delimiter='\\t')\ndata","4ce14450":"list(data.columns)","e9a41b49":"data.info()","a6bcb880":"data['Dt_Customer'] = pd.to_datetime(data['Dt_Customer'])\ndata","22ff6ee3":"data.describe().T","a2fade39":"data.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True)","3d357b8e":"data.isna().sum()","78be9875":"data = data[data['Income'].notnull()]\ndata.reset_index(drop=True, inplace=True)\ndata","e9981d91":"data.describe(exclude='number')","e8ab672e":"data['Year'] = data['Dt_Customer'].apply(lambda row: row.year)\ndata","46dcfd85":"data[['Year']].describe()","ff9d4a80":"sns.countplot(data['Year'])","de321768":"data['Age'] = data['Year'] - data['Year_Birth']\ndata","125464d5":"#from sklearn.preprocessing import StandardScaler\n#place_col = ['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n#prod_col = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n\n#data[place_col] = np.argsort(data[place_col]) + 1\n\n#data[prod_col] = StandardScaler().fit_transform(data[prod_col])\n\n#data[prod_col] = np.argsort(data[prod_col]) + 1\n\n#data","c15439e6":"vis_data = data.copy()\nvis_data","a3daa386":"vis_data.drop([\n    'ID',\n    'Year_Birth',\n    'Education',\n    'Marital_Status',\n    'Dt_Customer',\n    'Year'\n],axis=1, inplace=True)\n\nvis_data.rename({\n    'MntWines': 'WineRank',\n    'MntFruits': 'FruitsRank',\n    'MntMeatProducts': 'MeatRank',\n    'MntFishProducts': 'FishRank',\n    'MntSweetProducts': 'SweetRank',\n    'MntGoldProds': 'GoldRank',\n    'NumWebPurchases': 'WebRank',\n    'NumCatalogPurchases': 'CatalogRank',\n    'NumStorePurchases': 'StoreRank'\n}, axis=1, inplace=True)\n\nvis_data","6c8e6294":"sns.displot(vis_data['Recency']) # Fairly Uniform. We don't really need this","64d24838":"vis_data.drop(['Recency'], axis=1, inplace=True)\nvis_data","4a4a7688":"sns.kdeplot(data['Income'])","33c1b81d":"outlier_idx = vis_data[vis_data['Income'] > 150000].index\nvis_data.drop(outlier_idx, inplace=True)\n\nsns.kdeplot(vis_data['Income'])","c34bed08":"sns.kdeplot(data['Age'])","c16122c6":"outlier_age = vis_data.loc[vis_data['Age'] > 90].index\nvis_data.drop(outlier_age, inplace=True)\nvis_data.reset_index(drop=True, inplace=True)\n\nsns.kdeplot(vis_data['Age'])","e18cc60b":"sns.countplot(vis_data['Kidhome'])","e14cbb54":"sns.countplot(vis_data['Teenhome'])","b0ac5099":"sns.countplot(vis_data['NumDealsPurchases'])","5c489c9b":"vis_data['Kidhome'] = vis_data['Kidhome'].apply(lambda row: 1 if row >= 1 else 0)\nvis_data['Teenhome'] = vis_data['Teenhome'].apply(lambda row: 1 if row >= 1 else 0)\n\nvis_data","d5146f77":"from sklearn.manifold import TSNE, LocallyLinearEmbedding, MDS, Isomap\nfrom sklearn.preprocessing import StandardScaler\n\nc_al_data = vis_data.copy()\nc_al_data","e125cbca":"num_col = ['Income', 'Age', 'NumDealsPurchases', 'NumWebVisitsMonth']\ncat_col = ['Kidhome', 'Teenhome', 'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response', 'Complain']","13d61d07":"c_al_data['Accepted'] = c_al_data[['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']].sum(axis=1) > 0\nc_al_data['Accepted'] = c_al_data['Accepted'].apply(lambda row: 1 if row else 0)\n\nc_al_data.drop(['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response'], axis=1, inplace=True)\n\nc_al_data","192b8cd9":"c_al_data = pd.DataFrame(StandardScaler().fit_transform(c_al_data), columns=c_al_data.columns)","d625d99e":"for i in c_al_data.columns:\n    c_al_data[i].astype(dtype=float)\n\nc_al_data.info()","e910da1b":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=10)\n\npca_data = pd.DataFrame(pca.fit_transform(c_al_data))\n\nplt.plot(pca.explained_variance_ratio_.cumsum())\n\nc_al_data = pd.concat([c_al_data, pca_data], axis=1)\n\nc_al_data","039b03b5":"tsne = TSNE(learning_rate=50)\n\ntsne_results = c_al_data.copy()\n\ntsne_results[['TSNE1', 'TSNE2']] = pd.DataFrame(tsne.fit_transform(c_al_data[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]), columns=['TSNE1', 'TSNE2'])\n\ntsne_results","935d7a0e":"plt.figure(figsize=(10,10))\nsns.scatterplot(x='TSNE1', y='TSNE2', hue='Accepted', data=tsne_results)","24396775":"from scipy.cluster.vq import kmeans, vq\nimport random\n\nrandom.seed(1000)\n\n# All Data\ncluster_data = c_al_data.copy()\n\ndistortions = []\nfor k in range(1, 15):\n    _, distortion = kmeans(cluster_data, k)\n    distortions.append(distortion)\n    \nplt.plot(distortions, label='All Data')\n\n# Some Meaningful only\ncluster_data = c_al_data.copy()[[\n    'Kidhome', 'Teenhome', 'Complain', 'Accepted'\n]]\n\ndistortions = []\nfor k in range(1, 15):\n    _, distortion = kmeans(cluster_data, k)\n    distortions.append(distortion)\n    \nplt.plot(distortions, label='Home Only')\n\n# PCA Only\ncluster_data = c_al_data.copy()[[\n    0, 1, 2, 3, 4, 5, 6, 7\n]]\n\ndistortions = []\nfor k in range(1, 15):\n    _, distortion = kmeans(cluster_data, k)\n    distortions.append(distortion)\n    \nplt.plot(distortions, label='PCA Only')\n\n\n\n\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","6e3d907c":"random.seed(500)\n\ncluster_data = c_al_data.copy()[['Kidhome', 'Teenhome', 'Complain', 'Accepted']]\n\ncluster_centers, _ = kmeans(cluster_data, 5)\n\ncluster_data['cluster_labels'], _ = vq(cluster_data, cluster_centers)\n\ntsne = TSNE(learning_rate=100)\n\ntsne_results = cluster_data.copy()\n\ntsne_results[['TSNE1', 'TSNE2']] = pd.DataFrame(tsne.fit_transform(c_al_data), columns=['TSNE1', 'TSNE2'])\n\nplt.figure(figsize=(10,10))\n\nsns.scatterplot(x='TSNE1', y='TSNE2', hue='cluster_labels', cmap=sns.color_palette(), data=tsne_results)","9c8fd043":"random.seed(500)\n\ncluster_data = c_al_data.copy()[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]\n\ncluster_centers, _ = kmeans(cluster_data, 5)\n\ncluster_data['cluster_labels'], _ = vq(cluster_data, cluster_centers)\n\ntsne = TSNE(learning_rate=100)\n\ntsne_results = cluster_data.copy()\n\ntsne_results[['TSNE1', 'TSNE2']] = pd.DataFrame(tsne.fit_transform(c_al_data), columns=['TSNE1', 'TSNE2'])\n\nplt.figure(figsize=(10,10))\n\nsns.scatterplot(x='TSNE1', y='TSNE2', hue='cluster_labels', cmap=sns.color_palette(), data=tsne_results)","7c0e5dae":"From there we can describe the Year now.","673a979f":"First we check if the all of the columns have the correct format","61a55099":"Next we describe the non-numerical features.","a321f8a4":"As we can see, there is a lot of outliers. We remove these in order to get a better plot.","090f248a":"Let's now plot the distributions to see the time analytics","45769b30":"Next, we check missing values","51b1ee97":"We need to visualize the Distributions of all of the things","83104eb3":"It would be nice to combine TeenHome and KidHome to just having a Kid or Having a Teen","5811a2e8":"We can see that it too is approximately normal. Next, we will look into the distributions of discrete variables","29d38f89":"First, we need to visualize the clusters before performing KMeans Clustering and setting the seed.","9bb92c7b":"Let's remove the outliers that have an age greater than 90.","845a2d8c":"As we can see, the Income is fairly normal after removing the outliers. Next, we will look into the Age.","b7a51384":"Missing Count: 24\n-----------------\n\nThis could easily be Imputed. For our analysis, we would remove these first.","3c32d964":"<h2>Univariate Analysis<\/h2>","8e772129":"we see that Z_CostContact and Z_Revenue does not have any variance. Because of this, we can easily remove it from the dataset.","73e7adbd":"We see that we have clustered the Customers quite well. However, there may be better things to do this clustering. We cannot determine how many we want and just guess segmentation to 5 categories. We see that the clustering with just 4 Features is quite amazing. We keep this clustering. I suggest for a better performing model to be put in an sklearn pipeline with just the Kidhome, Teenhome, Complain, and Accepted Features. ","d68af830":"# Final Results","b2b0aed5":"We can sort of see some clusters","a75f5587":"It seems that it would be best to engineer the month and year from Dt_Customer. The Day doesn't seem important.","a3ab46dc":"# Data Visualization","8c885d95":"We convert Dt_Customer into a datetime format.","b41294e0":"To get some more descriptive features, we can calculate the age of the customer at that time they purchased something","d9c5cae1":"# Cluster Analysis","3587c7ad":"# Preprocessing"}}