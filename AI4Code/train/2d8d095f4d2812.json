{"cell_type":{"ccb57292":"code","3c9c1587":"code","4a87d4ff":"code","58884ad2":"code","9ac50387":"code","b08a88ad":"code","7936bfb1":"code","a3060c09":"code","30613296":"code","0d9b2820":"code","b4751d74":"code","85212203":"code","b7bb8ecf":"code","9fda1a00":"code","52b8911a":"code","7561eaef":"code","14c20960":"code","f75f8913":"code","f32100d2":"code","285aaeea":"code","47620b98":"code","89658cf5":"code","24c4d7ea":"code","cfb7e002":"code","84ede294":"code","6431c40a":"code","b29e18f9":"code","d2cc1d48":"code","4da9624b":"code","a8171f63":"markdown","4069ddb7":"markdown","d735673c":"markdown","296766d4":"markdown","0f65f6c7":"markdown","44b3ff6f":"markdown","f937fd92":"markdown","5db0ac0d":"markdown","043976a6":"markdown","ae06fde3":"markdown","4bdc7f8b":"markdown","e905a64e":"markdown","dffe8587":"markdown","854fc6d0":"markdown","bb5b4b64":"markdown","239bf8fc":"markdown","e143ce2c":"markdown","43fd0655":"markdown","a7704689":"markdown","99278078":"markdown"},"source":{"ccb57292":"!pip install bert-for-tf2\n!pip install sentencepiece","3c9c1587":"import os\nimport pandas as pd\nimport numpy as np\nimport re\nimport random\nimport math\n\ntry:\n    %tensorflow_version 2.x\nexcept Exception as ex:\n    print(ex)\n    pass\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nimport bert","4a87d4ff":"movie_reviews = pd.read_csv(\"\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")\nprint(f\"Null: {movie_reviews.isnull().values.any()}\")\nprint(f\"shape: {movie_reviews.shape}\")","58884ad2":"TAG_RE = re.compile(r'<[^>]+>')\ndef remove_tags(text):\n    \"\"\"\n    Remove html tags\n    \"\"\"\n    return TAG_RE.sub('', text)","9ac50387":"def preprocess_text(sen):\n    \"\"\"\n    Remove html tags\n    Remove punctuations and numbers\n    Remove single character words\n    Remove multiple spaces\n    \"\"\"\n    # Removing html tags\n    sentence = remove_tags(sen)\n\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence","b08a88ad":"reviews = []\nsentences = list(movie_reviews['review'])\nfor sen in sentences:\n    reviews.append(preprocess_text(sen))","7936bfb1":"print(movie_reviews.columns.values)","a3060c09":"movie_reviews.sentiment.unique()","30613296":"y = movie_reviews['sentiment']\ny = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))","0d9b2820":"print(f\"Review sample:\\n {reviews[10]}\")\nprint(f\"Review sentiment: {y[10]}\")","b4751d74":"BertTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\n                            trainable=False)\nvocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nto_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = BertTokenizer(vocabulary_file, to_lower_case)","85212203":"print(tokenizer.tokenize(\"don't try to be so sentimental or so judgemental\"))","b7bb8ecf":"print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"don't try to be so sentimental or so judgemental\")))","9fda1a00":"def tokenize_reviews(text_reviews):\n    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))","52b8911a":"tokenized_reviews = [tokenize_reviews(review) for review in reviews]","7561eaef":"reviews_with_len = [[review, y[i], len(review)]\n                 for i, review in enumerate(tokenized_reviews)]","14c20960":"random.shuffle(reviews_with_len)","f75f8913":"reviews_with_len.sort(key=lambda x: x[2])","f32100d2":"sorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]","285aaeea":"processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32))","47620b98":"BATCH_SIZE = 32\nbatched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))","89658cf5":"next(iter(batched_dataset))","24c4d7ea":"TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) \/ BATCH_SIZE)\nTEST_BATCHES = TOTAL_BATCHES \/\/ 10\nbatched_dataset.shuffle(TOTAL_BATCHES)\ntest_data = batched_dataset.take(TEST_BATCHES)\ntrain_data = batched_dataset.skip(TEST_BATCHES)","cfb7e002":"class TextClassificationModel(tf.keras.Model):\n    \n    def __init__(self,\n                 vocabulary_size,\n                 embedding_dimensions=128,\n                 cnn_filters=50,\n                 dnn_units=512,\n                 model_output_classes=2,\n                 dropout_rate=0.1,\n                 training=False,\n                 name=\"text_model\"):\n        super(TextClassificationModel, self).__init__(name=name)\n        \n        self.embedding = layers.Embedding(vocabulary_size,\n                                          embedding_dimensions)\n        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=2,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=3,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=4,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.pool = layers.GlobalMaxPool1D()\n        \n        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n        self.dropout = layers.Dropout(rate=dropout_rate)\n        if model_output_classes == 2:\n            self.last_dense = layers.Dense(units=1,\n                                           activation=\"sigmoid\")\n        else:\n            self.last_dense = layers.Dense(units=model_output_classes,\n                                           activation=\"softmax\")\n    \n    def call(self, inputs, training):\n        l = self.embedding(inputs)\n        l_1 = self.cnn_layer1(l) \n        l_1 = self.pool(l_1) \n        l_2 = self.cnn_layer2(l) \n        l_2 = self.pool(l_2)\n        l_3 = self.cnn_layer3(l)\n        l_3 = self.pool(l_3) \n        \n        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n        concatenated = self.dense_1(concatenated)\n        concatenated = self.dropout(concatenated, training)\n        model_output = self.last_dense(concatenated)\n        \n        return model_output","84ede294":"VOCAB_LENGTH = len(tokenizer.vocab)\nEMB_DIM = 200\nCNN_FILTERS = 100\nDNN_UNITS = 256\nOUTPUT_CLASSES = 2\n\nDROPOUT_RATE = 0.2\n\nNB_EPOCHS = 5","6431c40a":"text_model = TextClassificationModel(vocabulary_size=VOCAB_LENGTH,\n                        embedding_dimensions=EMB_DIM,\n                        cnn_filters=CNN_FILTERS,\n                        dnn_units=DNN_UNITS,\n                        model_output_classes=OUTPUT_CLASSES,\n                        dropout_rate=DROPOUT_RATE)","b29e18f9":"if OUTPUT_CLASSES == 2:\n    text_model.compile(loss=\"binary_crossentropy\",\n                       optimizer=\"adam\",\n                       metrics=[\"accuracy\"])\nelse:\n    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n                       optimizer=\"adam\",\n                       metrics=[\"sparse_categorical_accuracy\"])","d2cc1d48":"text_model.fit(train_data, epochs=NB_EPOCHS)","4da9624b":"results = text_model.evaluate(test_data)\nprint(f\"Test evaluation results: {results}\")","a8171f63":"## Tokenization using BERT","4069ddb7":"### Aplly tokenizer to data","d735673c":"### Check text and target features","296766d4":"## Train model","0f65f6c7":"We use `BertTokenizer` (BERT uncased) from bert.\nWe initialize BertTokenizer with vocabulary file and option to lower case.","44b3ff6f":"### Check tokenizer","f937fd92":"## Install and import libraries","5db0ac0d":"## Validation using test set","043976a6":"## Define model","ae06fde3":"## Train-test split\n\n\nWe split the dataset in train-test, reserving 10% of the data for test, 90% in used for train.","4bdc7f8b":"We apply the preprocessing to all sentences.","e905a64e":"## Input data","dffe8587":"We prepare the text for the classification. The text preprocessing includes the following:\n* Remove html tags;  \n* Remove punctuations and numbers;  \n* Remove single character words;  \n* Remove multiple spaces.  ","854fc6d0":"After tokenization, we add to the transformed data as well the length for each review.","bb5b4b64":"We transform the target feature, from {'positive', 'negative'} to {1, 0}","239bf8fc":"# Analysis preparation","e143ce2c":"# Model","43fd0655":"# Data preprocessing","a7704689":"# Introduction\n\n\nThis Notebook introduces use of BERT for tokenization task in a solution for text classification.\n\nThe dataset used here is [IMDB Dataset of 50K Movie Reviews](https:\/\/www.kaggle.com\/lakshmi25npathi\/imdb-dataset-of-50k-movie-reviews)","99278078":"Sort reviews on length."}}