{"cell_type":{"fd1f276b":"code","62ab32b0":"code","753aa7d3":"code","3e9574a5":"code","5780d542":"code","d6374f2b":"code","e70b890e":"code","c69af75d":"code","94a266cf":"code","df00b1a0":"code","b85de24d":"code","dba98d79":"code","ebbe67ad":"code","967f805d":"code","b3a85c3c":"code","8d893a15":"code","73d2468e":"code","f6722b4c":"code","6a51e0f8":"code","925bc873":"code","bc07d29a":"code","a55f29cb":"markdown","73222b8b":"markdown","767b21b8":"markdown","7bcad978":"markdown","476280f7":"markdown","748916d1":"markdown","ae93e611":"markdown","73982adf":"markdown","8f953dd5":"markdown","c68620ee":"markdown","bf051b84":"markdown","f0cd45af":"markdown","c9ebd15a":"markdown","ecbfeeaa":"markdown","40ff4f84":"markdown","4305d6cf":"markdown","971a2a1d":"markdown","88a93f5a":"markdown","c1b782d7":"markdown","05a669a8":"markdown","dcdcc8f1":"markdown","0d774171":"markdown"},"source":{"fd1f276b":"import pandas as pd\nimport numpy as np\n\n\n# Bokeh\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import HoverTool, CustomJS, ColumnDataSource, Slider, Range1d\nfrom bokeh.layouts import column\nfrom bokeh.palettes import all_palettes\noutput_notebook()","62ab32b0":"df = pd.read_csv(\"..\/input\/nips-papers\/papers.csv\")\nprint(df.paper_text[0][:500] + ' ...')","753aa7d3":"%%time\nimport spacy\n\nnlp = spacy.load('en', disable=['parser', 'ner'])\ndf['paper_text_lemma'] = df.paper_text.map(lambda x: [token.lemma_ for token in nlp(x) if token.lemma_ != '-PRON-' and token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV'}])\n\n# Final cleaning\ndf['paper_text_lemma'] = df.paper_text_lemma.map(lambda x: [t for t in x if len(t) > 1])\n\n# Example\nprint(df['paper_text_lemma'][0][:25], end='\\n\\n')","3e9574a5":"%%time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nnp.random.seed(42)\nn_features=2000\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, ngram_range=(1,2), stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(df.paper_text_lemma.map(lambda x: ' '.join(x)))","5780d542":"%%time\nimport umap\n\numap_embr = umap.UMAP(n_neighbors=10, metric='cosine', min_dist=0.1, random_state=42)\nembedding = umap_embr.fit_transform(tfidf.todense())\nembedding = pd.DataFrame(embedding, columns=['x','y'])","d6374f2b":"source = ColumnDataSource(\n        data=dict(\n            x = embedding.x,\n            y = embedding.y,\n            title = df.title,\n            year = df.year,\n        )\n    )\nhover_emb = HoverTool(names=[\"df\"], tooltips=\"\"\"\n    <div style=\"margin: 10\">\n        <div style=\"margin: 0 auto; width:300px;\">\n            <span style=\"font-size: 12px; font-weight: bold;\">Title:<\/span>\n            <span style=\"font-size: 12px\">@title<\/span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Year:<\/span>\n            <span style=\"font-size: 12px\">@year<\/span>\n        <\/div>\n    <\/div>\n    \"\"\")\ntools_emb = [hover_emb, 'pan', 'wheel_zoom', 'reset']\nplot_emb = figure(plot_width=600, plot_height=600, tools=tools_emb, title='Papers')\nplot_emb.circle('x', 'y', size=5, fill_color='green',\n                alpha=0.7, line_alpha=0, line_width=0.01, source=source, name=\"df\")\n\nplot_emb.x_range = Range1d(-8, 6)\nplot_emb.y_range = Range1d(-8, 7)\n\nlayout = column(plot_emb)\nshow(layout)","e70b890e":"%%time\nfrom gensim import corpora, models\nnp.random.seed(42)\n\n# Create a corpus from a list of texts\ntexts = df['paper_text_lemma'].values\ndictionary = corpora.Dictionary(texts, prune_at=2000)\ncorpus = [dictionary.doc2bow(text) for text in texts]","c69af75d":"%%time\nfrom gensim.models.nmf import Nmf\nfrom gensim.models.coherencemodel import CoherenceModel\n\ncoh_list = []\nfor n_topics in range(3,50+1):\n    # Train the model on the corpus\n    nmf = Nmf(corpus, num_topics=n_topics, id2word=dictionary, random_state=42)\n    # Estimate coherence\n    cm = CoherenceModel(model=nmf, texts=texts, dictionary=dictionary, coherence='u_mass')\n    coherence = cm.get_coherence_per_topic() # get coherence value\n    coh_list.append(coherence)","94a266cf":"# Coherence scores:\ncoh_means = np.array([np.mean(l) for l in coh_list])\ncoh_stds = np.array([np.std(l) for l in coh_list])\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.xticks(np.arange(3, 50+1, 3.0));\nplt.plot(range(3,50+1), coh_means);\nplt.fill_between(range(3,50+1), coh_means-coh_stds, coh_means+coh_stds, color='g', alpha=0.05);\nplt.vlines([6, 12, 23], -1.1, 0, color='red', linestyles='dashed',  linewidth=1);\nplt.hlines([-0.645], 3, 50, color='black', linestyles='dotted',  linewidth=0.5);\nplt.ylim(-1.1,0);","df00b1a0":"%%time\nfrom sklearn.decomposition import NMF\n\nn_topics=6\nn_top_words = 15\nnmf = NMF(n_components=n_topics, random_state=42, alpha=.1, l1_ratio=.5).fit(tfidf)\nnmf_embedding = nmf.transform(tfidf)\nfeature_names = tfidf_vectorizer.get_feature_names()\nprint(\"Topics found via NMF:\")\nfor topic_idx, topic in enumerate(nmf.components_):\n    print(\"\\nTopic {}:\".format(topic_idx))\n    print(\" \".join(['[{}]'.format(feature_names[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]))\nprint()","b85de24d":"topics = ['Optimization Algorithms',\n          'Artificial Neurons',\n          'Game Theory\/Reinf. Learn.',\n          'Neural Networks',\n          'Bayesian Methods',\n          'Kernel Methods'          \n         ]","dba98d79":"centroids = umap_embr.transform(nmf.components_)\nembedding['hue'] = nmf_embedding.argmax(axis=1)\nmy_colors = [all_palettes['Category20'][20][i] for i in embedding.hue]\nsource = ColumnDataSource(\n        data=dict(\n            x = embedding.x,\n            y = embedding.y,\n            colors = my_colors,\n            topic = [topics[i] for i in embedding.hue],\n            title = df.title,\n            year = df.year,\n            alpha = [0.7] * embedding.shape[0],\n            size = [7] * embedding.shape[0]\n        )\n    )\nhover_emb = HoverTool(names=[\"df\"], tooltips=\"\"\"\n    <div style=\"margin: 10\">\n        <div style=\"margin: 0 auto; width:300px;\">\n            <span style=\"font-size: 12px; font-weight: bold;\">Topic:<\/span>\n            <span style=\"font-size: 12px\">@topic<\/span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Title:<\/span>\n            <span style=\"font-size: 12px\">@title<\/span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Year:<\/span>\n            <span style=\"font-size: 12px\">@year<\/span>\n        <\/div>\n    <\/div>\n    \"\"\")\ntools_emb = [hover_emb, 'pan', 'wheel_zoom', 'reset']\nplot_emb = figure(plot_width=700, plot_height=700, tools=tools_emb, title='Papers')\nplot_emb.circle('x', 'y', size='size', fill_color='colors', \n                 alpha='alpha', line_alpha=0, line_width=0.01, source=source, name=\"df\", legend='topic')\n\nfor i in range(n_topics):\n    plot_emb.cross(x=centroids[i,0], y=centroids[i,1], size=15, color='black', line_width=2, angle=0.79)\nplot_emb.legend.location = \"bottom_left\"\nplot_emb.legend.label_text_font_size= \"8pt\"\nplot_emb.legend.spacing = -5\nplot_emb.x_range = Range1d(-9, 7)\nplot_emb.y_range = Range1d(-9, 7)\n\ncallback = CustomJS(args=dict(source=source), code=\n    \"\"\"\n    var data = source.data;\n    var f = cb_obj.value\n    x = data['x']\n    y = data['y']\n    colors = data['colors']\n    alpha = data['alpha']\n    title = data['title']\n    year = data['year']\n    size = data['size']\n    for (i = 0; i < x.length; i++) {\n        if (year[i] <= f) {\n            alpha[i] = 0.9\n            size[i] = 7\n        } else {\n            alpha[i] = 0.05\n            size[i] = 4\n        }\n    }\n    source.change.emit();\n    \"\"\")\n\nslider = Slider(start=df.year.min()-1, end=df.year.max(), value=2016, step=1, title=\"Before year\")\nslider.js_on_change('value', callback)\n\nlayout = column(slider, plot_emb)\nshow(layout)","ebbe67ad":"import matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\nlegend_list = []\nfor color in all_palettes['Category20'][20][:n_topics]:   \n    legend_list.append(mpatches.Ellipse((0, 0), 1, 1, fc=color))\n    \nfig,ax = plt.subplots(figsize=(12,13))\nax.scatter(embedding.x, embedding.y, c=my_colors, alpha=0.7)\nax.scatter(centroids[:,0], centroids[:,1], c='black', s=100, alpha=0.7, marker='x')\nax.set_title('6 topics found via NMF');\nfig.legend(legend_list, topics, loc=(0.18,0.87), ncol=3)\nplt.subplots_adjust(top=0.82)\nplt.suptitle(\"NIPS clustered by topic\", **{'fontsize':'14','weight':'bold'});\nplt.figtext(.51,0.95, 'topic modeling with NMF + 2D-embedding with UMAP', \n            **{'fontsize':'12','weight':'light'}, ha='center');","967f805d":"%%time\nfrom sklearn.decomposition import NMF\nn_topics=12\nn_top_words = 15\nnmf = NMF(n_components=n_topics, random_state=42, alpha=.1, l1_ratio=.5).fit(tfidf)\nnmf_embedding = nmf.transform(tfidf)\nfeature_names = tfidf_vectorizer.get_feature_names()\nprint(\"Topics found via NMF:\")\nfor topic_idx, topic in enumerate(nmf.components_):\n    print(\"\\nTopic {}:\".format(topic_idx))\n    print(\" \".join(['[{}]'.format(feature_names[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]))\nprint()","b3a85c3c":"topics = ['Optimization Algorithms',\n          'Neural Networks',\n          'Reinforcement Learning',\n          'Image Recognition',\n          'Bayesian Methods',\n          'Visual Neurons',\n          'Graph\/Tree Methods',\n          'Classification Problems',\n          'Kernel Methods',\n          'Clastering Methods',\n          'Game Theory',\n          'Artificial Neurons'\n         ]","8d893a15":"centroids = umap_embr.transform(nmf.components_)\nembedding['hue'] = nmf_embedding.argmax(axis=1)\nmy_colors = [all_palettes['Category20'][20][i] for i in embedding.hue]\nsource = ColumnDataSource(\n        data=dict(\n            x = embedding.x,\n            y = embedding.y,\n            colors = my_colors,\n            topic = [topics[i] for i in embedding.hue],\n            title = df.title,\n            year = df.year,\n            alpha = [0.7] * embedding.shape[0],\n            size = [7] * embedding.shape[0]\n        )\n    )\nhover_emb = HoverTool(names=[\"df\"], tooltips=\"\"\"\n    <div style=\"margin: 10\">\n        <div style=\"margin: 0 auto; width:300px;\">\n            <span style=\"font-size: 12px; font-weight: bold;\">Topic:<\/span>\n            <span style=\"font-size: 12px\">@topic<\/span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Title:<\/span>\n            <span style=\"font-size: 12px\">@title<\/span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Year:<\/span>\n            <span style=\"font-size: 12px\">@year<\/span>\n        <\/div>\n    <\/div>\n    \"\"\")\ntools_emb = [hover_emb, 'pan', 'wheel_zoom', 'reset']\nplot_emb = figure(plot_width=700, plot_height=700, tools=tools_emb, title='Papers')\nplot_emb.circle('x', 'y', size='size', fill_color='colors', \n                 alpha='alpha', line_alpha=0, line_width=0.01, source=source, name=\"df\", legend='topic')\n\nfor i in range(n_topics):\n    plot_emb.cross(x=centroids[i,0], y=centroids[i,1], size=15, color='black', line_width=2, angle=0.79)\nplot_emb.legend.location = \"bottom_left\"\nplot_emb.legend.label_text_font_size= \"8pt\"\nplot_emb.legend.spacing = -5\nplot_emb.x_range = Range1d(-9, 7)\nplot_emb.y_range = Range1d(-9, 7)\n\ncallback = CustomJS(args=dict(source=source), code=\n    \"\"\"\n    var data = source.data;\n    var f = cb_obj.value\n    x = data['x']\n    y = data['y']\n    colors = data['colors']\n    alpha = data['alpha']\n    title = data['title']\n    year = data['year']\n    size = data['size']\n    for (i = 0; i < x.length; i++) {\n        if (year[i] <= f) {\n            alpha[i] = 0.9\n            size[i] = 7\n        } else {\n            alpha[i] = 0.05\n            size[i] = 4\n        }\n    }\n    source.change.emit();\n    \"\"\")\n\nslider = Slider(start=df.year.min()-1, end=df.year.max(), value=2016, step=1, title=\"Before year\")\nslider.js_on_change('value', callback)\n\nlayout = column(slider, plot_emb)\nshow(layout)","73d2468e":"import matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\nlegend_list = []\nfor color in all_palettes['Category20'][20][:n_topics]:   \n    legend_list.append(mpatches.Ellipse((0, 0), 1, 1, fc=color))\n    \nfig,ax = plt.subplots(figsize=(12,13))\nax.scatter(embedding.x, embedding.y, c=my_colors, alpha=0.7)\nax.scatter(centroids[:,0], centroids[:,1], c='black', s=100, alpha=0.7, marker='x')\nax.set_title('11 topics found via NMF');\nfig.legend(legend_list, topics, loc=(0.09,0.87), ncol=4)\nplt.subplots_adjust(top=0.82)\nplt.suptitle(\"NIPS clustered by topic\", **{'fontsize':'14','weight':'bold'});\nplt.figtext(.51,0.95, 'topic modeling with NMF + 2D-embedding with UMAP', \n            **{'fontsize':'12','weight':'light'}, ha='center');","f6722b4c":"%%time\nfrom sklearn.decomposition import NMF\nn_topics=23\nn_top_words = 15\nnmf = NMF(n_components=n_topics, random_state=42, alpha=.1, l1_ratio=.5).fit(tfidf)\nnmf_embedding = nmf.transform(tfidf)\nfeature_names = tfidf_vectorizer.get_feature_names()\nprint(\"Topics found via NMF:\")\nfor topic_idx, topic in enumerate(nmf.components_):\n    print(\"\\nTopic {}:\".format(topic_idx))\n    print(\" \".join(['[{}]'.format(feature_names[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]))\nprint()","6a51e0f8":"topics = ['Optimization Algorithms',\n          'Neural Networks',\n          'Reinforcement Learning',\n          'Image Recognition', \n          'Probabilistic Methods',\n          'Visual Neurons',\n          'Graph\/Networks',\n          'Classification Problems',          \n          'Kernel Methods',\n          'Bayesian Methods',\n          'Multiiarm Bandits',\n          'General Neurons',          \n          'Clastering Methods',\n          'Matrix Decompositions',\n          'Control Theory',\n          'Topic Modeling',          \n          'Tree Methods',\n          'Greedy Algorithms',\n          'Speech Recognition',\n          'Dimensionality Reduction',          \n          'Chips\/Circuit',\n          'Game Theory',\n          'Feature Engineering'\n         ]","925bc873":"centroids = umap_embr.transform(nmf.components_)\nembedding['hue'] = nmf_embedding.argmax(axis=1)\nmy_colors = [(all_palettes['Category20'][20] + all_palettes['Category20'][20])[i] for i in embedding.hue]\nsource = ColumnDataSource(\n        data=dict(\n            x = embedding.x,\n            y = embedding.y,\n            colors = my_colors,\n            topic = [topics[i] for i in embedding.hue],\n            title = df.title,\n            year = df.year,\n            alpha = [0.7] * embedding.shape[0],\n            size = [7] * embedding.shape[0]\n        )\n    )\nhover_emb = HoverTool(names=[\"df\"], tooltips=\"\"\"\n    <div style=\"margin: 10\">\n        <div style=\"margin: 0 auto; width:300px;\">\n            <span style=\"font-size: 12px; font-weight: bold;\">Topic:<\/span>\n            <span style=\"font-size: 12px\">@topic<\/span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Title:<\/span>\n            <span style=\"font-size: 12px\">@title<\/span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Year:<\/span>\n            <span style=\"font-size: 12px\">@year<\/span>\n        <\/div>\n    <\/div>\n    \"\"\")\ntools_emb = [hover_emb, 'pan', 'wheel_zoom', 'reset']\nplot_emb = figure(plot_width=700, plot_height=700, tools=tools_emb, title='Papers')\nplot_emb.circle('x', 'y', size='size', fill_color='colors', \n                 alpha='alpha', line_alpha=0, line_width=0.01, source=source, name=\"df\", legend='topic')\n\nfor i in range(n_topics):\n    plot_emb.cross(x=centroids[i,0], y=centroids[i,1], size=15, color='black', line_width=2, angle=0.79)\nplot_emb.legend.location = \"bottom_left\"\nplot_emb.legend.label_text_font_size= \"8pt\"\nplot_emb.legend.spacing = -5\nplot_emb.x_range = Range1d(-9, 7)\nplot_emb.y_range = Range1d(-9, 7)\n\ncallback = CustomJS(args=dict(source=source), code=\n    \"\"\"\n    var data = source.data;\n    var f = cb_obj.value\n    x = data['x']\n    y = data['y']\n    colors = data['colors']\n    alpha = data['alpha']\n    title = data['title']\n    year = data['year']\n    size = data['size']\n    for (i = 0; i < x.length; i++) {\n        if (year[i] <= f) {\n            alpha[i] = 0.9\n            size[i] = 7\n        } else {\n            alpha[i] = 0.05\n            size[i] = 4\n        }\n    }\n    source.change.emit();\n    \"\"\")\n\nslider = Slider(start=df.year.min()-1, end=df.year.max(), value=2016, step=1, title=\"Before year\")\nslider.js_on_change('value', callback)\n\nlayout = column(slider, plot_emb)\nshow(layout)","bc07d29a":"import matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\nlegend_list = []\nfor color in (all_palettes['Category20'][20] + all_palettes['Category20'][20])[:n_topics]:   \n    legend_list.append(mpatches.Ellipse((0, 0), 1, 1, fc=color))\n    \nfig,ax = plt.subplots(figsize=(12,13))\nax.scatter(embedding.x, embedding.y, c=my_colors, alpha=0.7)\nax.scatter(centroids[:,0], centroids[:,1], c='black', s=100, alpha=0.7, marker='x')\nax.set_title('23 topics found via NMF');\nfig.legend(legend_list, topics, loc=(0.075,0.835), ncol=4)\nplt.subplots_adjust(top=0.82)\nplt.suptitle(\"NIPS clustered by topic\", **{'fontsize':'14','weight':'bold'});\nplt.figtext(.51,0.95, 'topic modeling with NMF + 2D-embedding with UMAP', \n            **{'fontsize':'12','weight':'light'}, ha='center');","a55f29cb":"### 5.1 NMF-6","73222b8b":"### 5.3. NMF-23","767b21b8":"#### 5.1.3. Static Picture","7bcad978":"#### 5.3.3. Static Picture","476280f7":"#### 5.2.3. Static Picture","748916d1":"## 1. Loading data\nWe load docs from [NIPS Papers](https:\/\/www.kaggle.com\/benhamner\/nips-papers) dataset.","ae93e611":"#### 5.1.2. Bokeh interactive plot","73982adf":"Let plot the coherence scores and guess the number of topics. First, we calculate mean score and the standard deviation for each model. The blue line shows the means and the green region represents the standard deviations.","8f953dd5":"Now, we'll build an embedding of our `n_feature`-dimesional space into 2D using `UMAP` ([Uniform Manifold Approximation and Projection for Dimension Reduction](https:\/\/umap-learn.readthedocs.io\/en\/latest\/)) packege for visualization.","c68620ee":"## 5. NMF models in details","bf051b84":"## 2. Lemmatization\n\nApply lemmatization `spaCy` [framework](https:\/\/spacy.io\/). **Lemmatization** is the redusing a word to its \"dictionary form\" (word's *lemma*). ","f0cd45af":"### 5.2. NMF-12","c9ebd15a":"## 3. TFIDF and UMAP\n\nConstructing [TFIDF-matrix](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf).","ecbfeeaa":"Training the `NMF` models. Here, we'll train approximately $50$ models (for the numbers of topics (`n_topics`) between $3$ and $50$). For each model we calculate the *coherence score* (coherence score is cculated for each topic within a particular module). All those scores will be saved into `coh_list` (a list of coherence scores for every model). For example, the first element of the list is a list consisting of $3$ scores (since the first model will have only $3$ topics. The second element is the list of length $4$, and so on.\n\nWe are using the coherence metric called `UMass` (aka *intrinsic measure*).","40ff4f84":"#### 5.2.1. Topics","4305d6cf":"It seems we have some structure there, let's investigate further.","971a2a1d":"As \"good candidates\" for the number of topics we'll chose a few local minima of the graph. Those are `n_topic=6` and `n_topic=12`. Also, the mean coherence plot seems to have a starting plato around `n_topic=23`. We'll investigate those values below.","88a93f5a":"#### 5.1.1. Topics\n\nFor further investigation we'll use NMF algorithm from another packege (`NMF` from `sklearn.decomposition`).","c1b782d7":"#### 5.2.2. Bokeh interactive plot","05a669a8":"## 4. Gensim NMF model and Coherence\n\nLet's organize the text into a datastructure sutable for `gensim` [non-negative matrix factorization](https:\/\/en.wikipedia.org\/wiki\/Non-negative_matrix_factorization) model.","dcdcc8f1":"#### 5.3.2. Bokeh interactive plot","0d774171":"So, let's see what we have..."}}