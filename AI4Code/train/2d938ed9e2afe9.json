{"cell_type":{"9bba9938":"code","5889e168":"code","d9b6811c":"code","3b264626":"code","0a1b689b":"code","4a038972":"code","d56fb546":"code","887adb3b":"code","b41be831":"code","774f8a49":"code","7ceef5e2":"code","25868109":"code","07e6107f":"code","de74907a":"code","84a7d1d3":"code","91a00520":"code","1521400a":"code","93ffa7c7":"code","2e3c8ed4":"code","253eb195":"code","b36de95e":"code","08c8e3b4":"code","9e1ecefc":"code","b68642d6":"code","edd4d1eb":"code","d64e1b68":"code","5805da19":"code","84639cd8":"code","43b03c0c":"code","f57f4555":"code","c7dddb68":"code","26d31c7f":"code","87850d38":"code","46de1506":"code","10dc083e":"code","b8dacce0":"code","3bdb9a6c":"code","31459e4e":"code","a50c5a4d":"code","5e2fd44b":"code","d12fa3ab":"code","5c96f789":"code","588b5a96":"code","4e94b8df":"code","f35fc887":"code","2711f827":"code","6af8c59b":"code","d6399e32":"code","1cb386e7":"code","fbc90a90":"code","fbd7e370":"code","83dad7b3":"code","a5977094":"code","dc88f1f7":"code","ddf8e5b3":"code","4d857b7e":"code","8cc4811d":"code","5bc790ba":"code","1576ad55":"code","98ebe2ce":"code","3a6cc9da":"code","fe1ef853":"code","132d7a01":"code","5020d19b":"markdown","4fccdb71":"markdown","bf05bb93":"markdown","276a2bef":"markdown","38229c48":"markdown","dba07dd4":"markdown","ab7f695b":"markdown","700389da":"markdown","81b38508":"markdown","1e0ca4fb":"markdown","cd12a2d9":"markdown","134ce40b":"markdown","01dc9fc2":"markdown","31ede5d5":"markdown","cb488934":"markdown","dc902a1c":"markdown","c90ca98e":"markdown","b2b23cf3":"markdown","9fa05af2":"markdown","36caae00":"markdown","e0617842":"markdown","bdd63089":"markdown","685eab9a":"markdown","1fcba3c7":"markdown","a3e908e1":"markdown","42d65ca3":"markdown","4960b80f":"markdown","faae3231":"markdown","2f94aed9":"markdown","9771aa3f":"markdown","7c09f8af":"markdown","895ed922":"markdown","814fbad2":"markdown","292356db":"markdown","02bfc6a9":"markdown","58e948d8":"markdown","9df5951d":"markdown","de9e058a":"markdown","87e09a4b":"markdown","266cfaef":"markdown","9351888f":"markdown","b8e01d3e":"markdown","9385a7eb":"markdown","543449f6":"markdown","816e0966":"markdown","45336eff":"markdown","efae7a5b":"markdown","165c216e":"markdown","95ba952d":"markdown","deb5b53a":"markdown","afbf04e0":"markdown"},"source":{"9bba9938":"# Import Libraries\nimport warnings\nwarnings.filterwarnings('ignore')\nimport time\nimport numpy as np\nimport pandas as pd\npd.options.display.float_format = '{:.2f}'.format\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\n# Print versions of libraries\nprint(f\"Numpy version : Numpy {np.__version__}\")\nprint(f\"Pandas version : Pandas {pd.__version__}\")\nprint(f\"Seaborn version : Seaborn {sns.__version__}\")\nprint(f\"SkLearn version : SkLearn {sklearn.__version__}\")\n# Magic Functions for In-Notebook Display\n%matplotlib inline\n# Setting seabon style\nsns.set(style='darkgrid', palette='colorblind')","5889e168":"cd '..\/input\/medical-masks-part1'","d9b6811c":"train = pd.read_csv('df.csv', encoding='latin_1')\n# Converting all column names to lower case\ntrain.columns = train.columns.str.lower()\ntrain","3b264626":"train.info()\ntrain.describe()","0a1b689b":"train['id'].nunique()","4a038972":"train['user_id'].nunique()","d56fb546":"uniqueUID = pd.DataFrame(list(train['user_id'].value_counts().index))\nuniqueUID['user_id'] = list(train['user_id'].value_counts())\nax = sns.scatterplot(uniqueUID['user_id'], uniqueUID[0])\nax.set(xlabel=\"Number of images given to a single person\", ylabel = \"USER ID\")","887adb3b":"train","b41be831":"a = pd.DataFrame(train.query('gender != \"NONE\"'))","774f8a49":"a['type'].value_counts()","7ceef5e2":"a","25868109":"q = pd.DataFrame(a.query('user_id == 5228'))\nq","07e6107f":"cd images","de74907a":"import cv2\nimgs = []\nfor i in list(np.array(q[\"name\"])):\n    im = cv2.imread(i)\n    imgs.append(im)\n    \nplt.figure(figsize=(20,20))\nfor n , i in enumerate(list(np.random.randint(0,len(imgs),16))) : \n    plt.subplot(6,6,n+1)\n    plt.imshow(imgs[i])   \n    plt.axis('off')\n","84a7d1d3":"#LET'S GO BACK TO PREVIOUS DIRECTORY\ncd ..\n","91a00520":"newdf = []\ncuid = []\nfor i in np.array(a):\n    if cuid.count(i[2]) < 4 or len(cuid) == 0:\n        newdf.append(i)\n        cuid.append(i[2])\n        ","1521400a":"c = a.columns","93ffa7c7":"newdf = pd.DataFrame(np.array(newdf), columns = c)","2e3c8ed4":"newdf","253eb195":"newdf.info()","b36de95e":"uniqueUID = pd.DataFrame(list(newdf['user_id'].value_counts().index))\nuniqueUID['user_id'] = list(newdf['user_id'].value_counts())\nax = sns.scatterplot(uniqueUID['user_id'], uniqueUID[0])\nax.set(xlabel=\"Number of images given to a single person\", ylabel = \"USER ID\")","08c8e3b4":"newdf['type'].value_counts()","9e1ecefc":"sns.boxplot(newdf['size_mb'])","b68642d6":"slicer = pd.DataFrame(newdf.query('size_mb > 4'))\nslicer = list(slicer['user_id'].unique())\nlen(slicer)","edd4d1eb":"for i in slicer:\n    indexNames = newdf[newdf['user_id'] == i].index\n    # Delete these row indexes from dataFrame\n    newdf.drop(indexNames , inplace=True)","d64e1b68":"sns.boxplot(newdf['size_mb'])","5805da19":"newdf['id'] = newdf['id'].astype(int)\nnewdf['user_id'] = newdf['user_id'].astype(int)\nnewdf['age'] = newdf['age'].astype(int)\nnewdf['size_mb'] = newdf['size_mb'].astype(float)","84639cd8":"newdf.describe() ","43b03c0c":"sns.boxplot(newdf['age'])","f57f4555":"slicer = pd.DataFrame(newdf.query('age > 50'))\nslicer = list(slicer['age'].unique())\nlen(slicer)\nfor i in slicer:\n    indexNames = newdf[newdf['age'] == i].index\n    # Delete these row indexes from dataFrame\n    newdf.drop(indexNames , inplace=True)","c7dddb68":"sns.boxplot(newdf['age'])","26d31c7f":"newdf['type'].value_counts()","87850d38":"newdf.describe()","46de1506":"newdf.shape","10dc083e":"training = pd.DataFrame(newdf[:4000])\ntesting = pd.DataFrame(newdf[8000:])","b8dacce0":"training","3bdb9a6c":"cd images","31459e4e":"import glob as gb\nimport cv2\nfrom tqdm import tqdm\nX = []\ny = []\ns = 299\nfor i in tqdm(list((training['name']))):\n    image = cv2.imread(i)\n    image_array = cv2.resize(image , (s,s))\n    X.append(list(image_array))\n    y.append(int(i.split(\"_\")[1])-1)\n    ","a50c5a4d":"plt.figure(figsize=(20,20))\nfor n , i in enumerate(list(np.random.randint(0,len(X),36))) : \n    plt.subplot(6,6,n+1)\n    plt.imshow(X[i])   \n    plt.axis('off')\n    plt.title(y[i])","5e2fd44b":"X = np.array(X)\ny = np.array(y)\n\nprint(X.shape)\nprint(y.shape)","d12fa3ab":"import itertools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom keras import Sequential\nfrom keras.applications import Xception #For Transfer Learning\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import SGD,Adam\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout\nfrom keras.utils import to_categorical","5c96f789":"X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=.3)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_val.shape)\nprint(y_val.shape)","588b5a96":"from keras.utils import to_categorical\n#One Hot Encoding\ny_train=to_categorical(y_train)\ny_val=to_categorical(y_val)\n#Verifying the dimension after one hot encoding\nprint((X_train.shape,y_train.shape))\nprint((X_val.shape,y_val.shape))","4e94b8df":"import tensorflow as tf\nimport keras\nfrom keras.applications.xception import Xception\nmodel=Xception(include_top = False, weights = 'imagenet', input_shape = (299,299,3))\nflattened = tf.keras.layers.Flatten()(model.output)\n\nfc1 = tf.keras.layers.Dense(4, activation='softmax', name=\"AddedDense2\")(flattened)\n\nmodel = tf.keras.models.Model(inputs=model.input, outputs=fc1) ","f35fc887":"model.compile(optimizer ='adamax',loss='categorical_crossentropy',metrics=['accuracy'])\n\nprint('Model Details are : ')\nprint(model.summary())","2711f827":"\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping   \ncallbacks_list = [  \n    ModelCheckpoint('\/kaggle\/working\/AMD.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True),\n]  \n\nThisModel = model.fit(X_train, y_train, epochs=10, callbacks=callbacks_list, verbose=1, validation_data=(X_val,y_val))","6af8c59b":"# summarize history for accuracy\nplt.plot(ThisModel.history['accuracy'])\nplt.plot(ThisModel.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(ThisModel.history['loss'])\nplt.plot(ThisModel.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","d6399e32":"from keras.models import load_model\nmodel.save(\"\/kaggle\/working\/AMD.h5\")\n#LOADING THE BEST MODEL\nmodel = load_model(\"\/kaggle\/working\/AMD.h5\")","1cb386e7":"y_pred1a = model.predict(X_val, verbose=1)      \ny_pred_bool1a = np.round(np.array([list(i) for i in y_pred1a]))\nfrom sklearn.metrics import accuracy_score\nprint(\"THE ACCURACY SCORE IS: {}\".format(accuracy_score(y_val, y_pred_bool1a)))\nfrom sklearn.metrics import roc_auc_score\nprint(\"THE AUCROC SCORE IS: {}\".format(roc_auc_score(y_val, y_pred_bool1a)))\nfrom sklearn.metrics import average_precision_score     \nprint(\"THE PRAUC SCORE IS: {}\".format(average_precision_score(y_val, y_pred_bool1a))) ","fbc90a90":"def predictor(a):\n    image = cv2.imread(a)\n    image_array = cv2.resize(image , (299,299))\n    image = list(image_array)\n    im = []\n    im.append(image)\n    a = np.array(im)\n    one = \"The mask is worn correctly, covers the nose and mouth.\"\n    two = \"The mask covers the mouth, but does not cover the nose.\"\n    three =  \"The mask is on, but does not cover the nose and mouth.\"\n    four = \"There is no mask on the face.\"\n    a = model.predict(a)\n    a = [int(np.round(j)) for i in a for j in i]\n    if a == [1, 0, 0, 0]:\n#         s = 0\n        s = one\n        return s\n    elif a == [0, 1, 0, 0]:\n#         s = 1\n        s=two\n        return s\n    elif a == [0, 0, 1, 0]:\n#         s = 2\n        s=three\n        return s\n    elif [0, 0, 0, 1]:\n#         s = 3\n        s=four\n        return s","fbd7e370":"image = predictor('000001_1_000001_MALE_25.jpg')\nimage","83dad7b3":"image = predictor('000006_2_000006_MALE_28.jpg')\nimage","a5977094":"image = predictor('000010_3_000010_MALE_24.jpg')\nimage","dc88f1f7":"image = predictor('000015_4_000015_FEMALE_31.jpg')\nimage","ddf8e5b3":"testing\n","4d857b7e":"actualvalues = np.array(testing[\"type\"])\nactualvalues","8cc4811d":"images = np.array(testing[\"name\"])\nimages","5bc790ba":"predictedvalues = []\nfor i in images:\n    op = predictor(i)\n    predictedvalues.append(op)","1576ad55":"predictedvalues = np.array(predictedvalues)","98ebe2ce":"predictedvalue = []\nfor a in tqdm(predictedvalues):\n    if a == 'The mask is worn correctly, covers the nose and mouth.':\n        s = 1\n        predictedvalue.append(s)\n    elif a == 'The mask covers the mouth, but does not cover the nose.':\n        s = 2\n        predictedvalue.append(s)\n    elif a == 'The mask is on, but does not cover the nose and mouth.':\n        s = 3\n        predictedvalue.append(s)\n    elif a == 'There is no mask on the face.':\n        s = 4\n        predictedvalue.append(s)","3a6cc9da":"predictedvalue = np.array(predictedvalue)","fe1ef853":"actualvalues=to_categorical(actualvalues)\npredictedvalue=to_categorical(predictedvalue)","132d7a01":"from sklearn.metrics import accuracy_score\nprint(\"THE ACCURACY SCORE IS: {}\".format(accuracy_score(actualvalues, predictedvalue)))","5020d19b":"**NOW WE ARE DONE WITH EDA AND DATA PRE-PROCESSING. LET'S BEGIN THE TRAINING. WE WILL BE USING ONLY 4000 OF THE IMAGES SO THAT LESS COMPUTATION TIME IS TAKEN FOR TRAINING AND READING THE IMAGES AND REMAINING WE WILL USE FOR TESTING**","4fccdb71":"**CONVERTING TO NUMPY ARRAY FOR TRAINING**","bf05bb93":"**CONVERTING BOTH THE ARRAYS TO CATEGORICAL**","276a2bef":"**LET'S FIRST HAVE A LOOK OF DATA SET WITHOUT NONE VALUES**","38229c48":"**IF YOU NOTICE THAT THE LAST FEW ROWS YOU WILL SEE THAT USER_ID IS 5228 AND ID IS 9998 THAT MEANS THAT 5228 HAS MORE THEN 4 IMAGES SO LET'S HAVE A LOOK**","dba07dd4":"**DEFINING THE PRE-TRAINED MODEL XCEPTION AND ADDING IT THE LAST DENSE LAYER TO IT FOR OUR FOUR CLASSES**","ab7f695b":"**WE CAN SEE A LOT OF OUTLIER'S HERE! LET'S JUST GET RID OF THESE. WE WILL PANDAS QUERY TO ACHIEVE THIS:**","700389da":"## LET'S GET SOME INSIGHTS AND STATISTICS OF THE DATA WE HAVE","81b38508":"**LET'S CHECK AGAIN IF THE DISTRIBUTION IS EVEN OR NOT.**","1e0ca4fb":"## GETTING INSIDE THE DIRECTORY","cd12a2d9":"**OUR GUESS WAS RIGHT AGAIN, IT HAS 16 COUNTS. LETS SEE IF THEY ALL ARE SAME PERSON OR NOT.**","134ce40b":"**AFTER QUERING, WE CAN SEE THE BOXPLOT BELOW THAT WE ARE FREE FROM OUTLIERS IN AGE.**","01dc9fc2":"**NOW, THE SIZE_MB IS FREE FROM OUTLIERS. LET'S DEAL WITH AGE.**","31ede5d5":"**WE CAN SEE THAT OUR OBSERVATION WAS RIGHT. THERE ARE 7 CASES IN DATA WHERE A SINGLE PERSON HAS MORE THEN 30 IMAGES. SO, BEFORE WE FILTER OUT THOSE DATA, IF YOU LOOK AT THE DATAFRAME AGAIN *(PRINTED BELOW)* YOU WILL NOTICE THAT THERE ARE SOME DATA WITH NONE IN GENDER. SO, LETS GET RID OF THOSE FIRST.**","cb488934":"**LET'S LOOK AT PROGRESS:**","dc902a1c":"**LET'S GO IN IMAGE DIRECTORY TO GET OUR DATA**","c90ca98e":"**LETS CHECK ON ID AND USER ID:**","b2b23cf3":"**MAKING ARRAY OF PREDICTED VALUES**","9fa05af2":"**FITTING THE MODEL**","36caae00":"**DEIFINING AN ARRAY WITH THE CORRECT VALUES OF CLASS FROM TYPE COLUMN**","e0617842":"**COMPILING THE MODEL**","bdd63089":"## LET'S PREDICT FOR THESE IMAGES","685eab9a":"**SPLITTING THE DATA**","1fcba3c7":"**AGE HAS OUTLIERS TOO. LET'S WORK ON IT.**","a3e908e1":"**WE CAN SEE THAT THEY ARE ALL SAME PERSON. SO, LET'S DROP OF THIS EXTRA DATA SO WE ONLY HAVE MEANINGFULL DATA TO WORK WITH**","42d65ca3":"## NOW LET'S PREDICT THE TESTING DATA:","4960b80f":"**ALL THE CLASSES ARE DISTRIBUTED EVENLY. NOW, LETS DEAL WITH USER_ID.**","faae3231":"## IMPORTING LIBRARIES","2f94aed9":"**CHECKING ACCURACY**","9771aa3f":"**LET'S CHECK HOW MODEL PERFORMS ON VALIDATION DATA**","7c09f8af":"**AFTER ANY KIND OF DATA OPERATION PERFORM WE NEED TO CHECK IF THE DATA IS BALANCED FOR EVERY CLASS OR NOT.**","895ed922":"**HERE WE WILL USE THE IN BUILT QUERY FEATURE OF PANDAS DATA FRAME TO GET RID OF THOSE NONE DATA:**","814fbad2":"![image.png](attachment:6618e16e-80bb-41db-a42f-e5e492f02a06.png) ![image.png](attachment:c5973cd4-bffe-487f-9fe2-704d2cefbf07.png) ![image.png](attachment:cefd12e7-bc20-4c51-bde1-2d724010816b.png) ![image.png](attachment:f0d74476-f176-46bf-b6aa-5acb152571aa.png)","292356db":"**TYPES**\n\n*TYPE 1 - There is no mask on the face.*\n\n*TYPE 2 - The mask is on, but does not cover the nose or mouth.*\n\n*TYPE 3 - The mask covers the mouth, but does not cover the nose.*\n\n*TYPE 4 - The mask is worn correctly, covers the nose and mouth.*","02bfc6a9":"**DEFINING A PREDICTOR FUNCTION**","58e948d8":"**WE HERE FILTER OUT EXTRA IMAGES**","9df5951d":"**LET'S HAVE A LOOK AT DATA WITH LABELS**","de9e058a":"**ENCODING OUR LABELS**","87e09a4b":"**GETTING THE NAMES OF THE IMAGES**","266cfaef":"**GETTING IMAGES**","9351888f":"## TRAINING","b8e01d3e":"## EXPLORATORY DATA ANALYSIS & DATA PRE-PROCESSING","9385a7eb":"**DATA IS BALANCED, LET'S HAVE A LOOK AT SIZE_MB**","543449f6":"**SO, ON TESTING DATA THE ACCURACY IS 97.5%**","816e0966":"**NOW THE PROBLEM OF USER_ID IS SOLVED! LET'S LOOK AT ANOTHER FEATURE, AND BEFORE WE DO THAT LET'S CHECK AGAIN IF THE DATA IS BALANCED OR NOT.**","45336eff":"**GETTING LIBRARIES FOR TRAINING OUR DEEP LEARNING MODEL**","efae7a5b":"**THE NEW DATAFRAME**","165c216e":"The largest dataset with people wearing face masks 250,000 images, 4 types of mask worn, 60,000 unique faces. \n\nAll images were collected using the Toloka.ai crowdsourcing service and validated by TrainingData.ru. \n\nEach object contains image size, photo type, person's age, gender, user ID.","95ba952d":"## MAKING THE DATA FRAME","deb5b53a":"**SAVING THE MODEL**","afbf04e0":"**WE CAN SEE THAT ID AND USER_ID COUNTS VARY THAT MEANS THAT WE HAVE SOME DUPLICATE DATA OR WE HAVE MORE THEN 4 IMAGES OF A SINGLE PERSON. LET'S SEE HOW THESE DUPLICATES ARE DISTRIBUTED:**"}}