{"cell_type":{"14928b08":"code","a933777b":"code","a673b653":"code","c2087b78":"code","95aaf47a":"code","dc117027":"code","f34e1952":"code","eb18119b":"code","4bc3d495":"code","cd7c2c56":"code","30fbf937":"code","d1e5c131":"code","a6138ac3":"code","8364e56e":"code","398583df":"code","085e6925":"code","86db0ec5":"code","f05ba60a":"code","f4848fd7":"code","dfe2bfd0":"code","bd7942a6":"code","eb692453":"code","3ceb5fc5":"code","91229d21":"code","9852a5c8":"code","6afa8c20":"code","79077b27":"code","aa74c666":"code","11a4e23c":"code","71bc5b96":"code","7d843454":"code","685429ff":"code","63d481e8":"code","0579eb65":"code","29bed677":"code","d5e4d6c2":"code","b9a7e4da":"code","3351c39b":"code","15396181":"code","4ffc2ac4":"code","00489001":"code","5223ed17":"code","b6118b0b":"code","5ecad43d":"code","c39f5d95":"code","ae4eb838":"code","c1367e2e":"code","e50b3b00":"code","2c8760dd":"code","cb91abad":"markdown","0bd85afd":"markdown","f369dcb9":"markdown","b362c0c4":"markdown","b6f899e6":"markdown","18e838c1":"markdown","eca730d2":"markdown","11346c99":"markdown","3d4e33ad":"markdown","da7c3386":"markdown","f2aa59ea":"markdown","91171d83":"markdown","26ee9876":"markdown","97935166":"markdown","5d5ffc1a":"markdown","87a1c866":"markdown","f746cb99":"markdown","45be0a4c":"markdown","91af7c98":"markdown","0f05d299":"markdown","b361265e":"markdown","550ee947":"markdown","8ca22d5a":"markdown","4d5940b7":"markdown","6909c8f7":"markdown","47d9b9f4":"markdown","4395b00a":"markdown"},"source":{"14928b08":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np, os \nimport pandas as pd, gc \nfrom tqdm import tqdm\n\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification,LukeForEntitySpanClassification,RobertaTokenizer,RobertaTokenizerFast,LukeTokenizer,DataCollatorForTokenClassification,TrainingArguments, Trainer, EarlyStoppingCallback\nfrom datasets import Dataset as ds, DatasetDict, load_metric\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom sklearn.metrics import accuracy_score\nfrom torch import cuda\nfrom torch import nn\nimport itertools\n\n!pip install seqeval\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a933777b":"config = {'model_name': 'studio-ousia\/luke-large-finetuned-conll-2003',   \n         'max_length': 512,\n         'train_batch_size':2,\n         'valid_batch_size':4,\n         'epochs':1,\n         'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n         'max_grad_norm':10,\n         'device': 'cuda' if cuda.is_available() else 'cpu'}","a673b653":"spans_tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n\ntokenizer = LukeTokenizer.from_pretrained(config['model_name'])\nmodel = LukeForEntitySpanClassification.from_pretrained(config['model_name'])","c2087b78":"text = \"Beyonc\u00e9 lives in Los Angeles\"\nout=spans_tokenizer(text,return_offsets_mapping=True, padding='max_length', truncation=True, max_length=512)","95aaf47a":"def get_spans(offset_mapping):\n    spans=[]\n    for ix,offset in enumerate(offset_mapping):\n        \n        if offset==(0,0):\n            continue\n\n        if offset[0]==0 and offset[1]!=0:\n            start=offset[0]\n            end=offset[1]\n\n        elif offset_mapping[ix-1][1]!=offset_mapping[ix][0]:\n            spans.append((start,end))\n            start=offset[0]\n\n        end=offset[1]\n\n    if start!=0 and end!=0:\n        spans.append((start,end))\n        \n    return spans","dc117027":"spans=get_spans(out['offset_mapping'])\n\n# List all possible entity spans in the text\nword_start_positions = list(list(zip(*spans))[0])  # character-based start positions of word tokens\nword_end_positions = list(list(zip(*spans))[1])  # character-based end positions of word tokens\nentity_spans = []\nfor i, start_pos in enumerate(word_start_positions):\n    for end_pos in word_end_positions[i:]:\n        entity_spans.append((start_pos, end_pos))\n\ninputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_indices = logits.argmax(-1).squeeze().tolist()","f34e1952":"for span, predicted_class_idx in zip(entity_spans, predicted_class_indices):\n    if predicted_class_idx != 0:\n         print(text[span[0]:span[1]], model.config.id2label[predicted_class_idx])","eb18119b":"predicted_class_indices","4bc3d495":"model.config.id2label","cd7c2c56":"train_df = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\ntrain_df['discourse_start']=train_df['discourse_start'].apply(lambda x:int(x))\ntrain_df['discourse_end']=train_df['discourse_end'].apply(lambda x:int(x))\ntrain_df=train_df.groupby('id').agg({'discourse_start':list,'discourse_end':list,'discourse_type':list,'predictionstring':list})\n\ntrain_df['spans']=train_df[['discourse_start','discourse_end']].apply(lambda x: list(zip(x[0],x[1])),axis=1)\ntrain_df['spans']=train_df[['discourse_start','discourse_end']].apply(lambda x: list(zip(x[0],x[1])),axis=1)","30fbf937":"def create_chunks(\n    uniqueid: int = 0,\n    content: str = \"\",\n    tokenizer: RobertaTokenizer = None,\n    max_length: int = 512,\n):\n    \"\"\"\n    Chunk the given text\n\n    :param uniqueid: Unique id for the record\n    :param content: Text to chunk\n    :param tokenizer: Tokenizer from huggingface library\n    :param max_length: Max length the chunked sentence has to be, after tokenizing\n    :return: Returns a list of chunked sentences.\n    \"\"\"\n\n    try:\n        all_chunks = []\n        splits = tokenizer.encode_plus(\n            content,\n            add_special_tokens=False,\n            return_tensors=\"pt\",\n            return_offsets_mapping=True,\n        )\n        offsets = splits[\"offset_mapping\"][0].split(max_length)\n\n        if len(offsets) > 1:\n            for ix, offset in enumerate(offsets):\n\n                if ix == 0:\n                    all_chunks.append(\n                        (uniqueid, 0, content[0 : offset[-1][1]])\n                    )\n                else:\n                    all_chunks.append(\n                        (\n                            uniqueid,\n                            int(offset[0][0]),\n                            content[offset[0][0] : offset[-1][1]],\n                        )\n                    )\n\n        else:\n            all_chunks.append((uniqueid, 0, content[0 : offsets[0][-1][1]]))\n\n        return all_chunks\n\n    except Exception as e:\n        print(f\"Chunking failed : {str(e)}\")\n        print(f\"Chunk text: {content}\")\n        return [(-100, 0, \"\")]","d1e5c131":"def correct_spans(offsets):\n    new_offsets=[]\n    for ix,span in enumerate(offsets):\n        \n        if ix==0:\n            new_offsets.append(span)\n        elif offsets[ix-1][1]==offsets[ix][0]:\n            new_offsets.append((span[0]+1,span[1]))\n        else:\n            new_offsets.append(span)\n            \n    return new_offsets\n\ndef fill_spans(offsets,labels):\n    new_offsets=[]\n    new_labels=[]\n    \n    for ix,span in enumerate(offsets):\n        \n        if ix==0 and span[0]==0:\n            new_offsets.append(span)\n            new_labels.append(labels[ix])\n            \n        elif ix==0 and span[0]!=0:\n            new_offsets.append((0,span[0]-1))\n            new_labels.append('Nil')\n            \n            new_offsets.append(span)\n            new_labels.append(labels[ix])\n            \n        elif offsets[ix][0]-offsets[ix-1][1]>1:\n            new_offsets.append((offsets[ix-1][1]+1,offsets[ix][0]-1))\n            new_offsets.append(span)\n            \n            new_labels.append('Nil')\n            new_labels.append(labels[ix])\n        else:\n            new_offsets.append(span)\n            new_labels.append(labels[ix])\n            \n    return (new_offsets,new_labels)\n\n\ndef fix_chars(text,span):\n    span[-1]=(span[-1][0],len(text.strip()))\n        \n    return span","a6138ac3":"train_df['spans']=train_df['spans'].apply(lambda x:correct_spans(x))\ntrain_df[['spans','discourse_type']]=train_df[['spans','discourse_type']].apply(lambda x:fill_spans(x[0],x[1]),axis=1,result_type='expand')\ntrain_df['spans']=train_df['spans'].apply(lambda x:correct_spans(x))","8364e56e":"######### Unit test to make sure the spans are correct\nfor i in range(len(train_df)):\n    assert len(train_df.iloc[i]['spans'])==len(train_df.iloc[i]['discourse_type'])\n    \nfor i in range(len(train_df)):\n\n    spans=train_df.iloc[i]['spans']\n    for ix, span in enumerate(spans):\n        if ix==0:\n            pass\n        else:\n            assert (spans[ix][0]-spans[ix-1][1])==1","398583df":"# https:\/\/www.kaggle.com\/raghavendrakotala\/fine-tunned-on-roberta-base-as-ner-problem-0-533\ntest_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/train'))):\n    test_names.append(f.replace('.txt', ''))\n    train_texts.append(open('..\/input\/feedback-prize-2021\/train\/' + f, 'r').read())\ntrain_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\ntrain_text_df.head()","085e6925":"train_df=train_text_df.merge(train_df,on='id')\n\ntrain_df['tok_len']=train_df['text'].apply(lambda x: len(spans_tokenizer.tokenize(x)))\ntrain_df=train_df[train_df['tok_len']<450].head(500)\ntrain_df.head()","86db0ec5":"train_df['spans']=train_df[['text','spans']].apply(lambda x:fix_chars(x[0],x[1]),axis=1)\ntrain_df['text']=train_df['text'].apply(lambda x:x.strip())","f05ba60a":"# CREATE DICTIONARIES THAT WE CAN USE DURING TRAIN AND INFER\noutput_labels = ['Nil', 'Lead', 'Position','Claim', 'Counterclaim','Rebuttal', 'Evidence', 'Concluding Statement']\n\nlabels_to_ids = {v:k for k,v in enumerate(output_labels)}\nids_to_labels = {k:v for k,v in enumerate(output_labels)}","f4848fd7":"train_df['discourse_type']=train_df['discourse_type'].apply(lambda x: [labels_to_ids[label] for label in x])","dfe2bfd0":"train_df[:2]","bd7942a6":"class luke_dataset(Dataset):\n  def __init__(self, dataframe, tokenizer, max_len):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n  def __getitem__(self, index):\n    \n        # GET TEXT AND WORD LABELS \n        text = self.data.text[index]  \n        spans = self.data.spans[index]\n        \n        print(\"1111\",spans)\n        # TOKENIZE TEXT\n        inputs = self.tokenizer(text, entity_spans=spans)\n        \n        inputs = {key: torch.as_tensor(val) for key, val in inputs.items()}\n        inputs['labels'] = torch.as_tensor(spans)\n        \n        return inputs\n\n  def __len__(self):\n        return self.len","eb692453":"# CHOOSE VALIDATION INDEXES (that match my TF notebook)\nIDS = train_df.id.unique()\nprint('There are',len(IDS),'train texts. We will split 90% 10% for validation.')\n\n# TRAIN VALID SPLIT 90% 10%\nnp.random.seed(42)\ntrain_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\nvalid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\nnp.random.seed(None)","3ceb5fc5":"# CREATE TRAIN SUBSET AND VALID SUBSET\ndata = train_df[['id','text', 'spans','discourse_type']]\ntrain_dataset = data.loc[data['id'].isin(IDS[train_idx])].reset_index(drop=True)\ntest_dataset = data.loc[data['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(data.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntraining_set = luke_dataset(train_dataset, tokenizer, config['max_length'])\ntesting_set = luke_dataset(test_dataset, tokenizer,config['max_length'])","91229d21":"# TRAIN DATASET AND VALID DATASET\ntrain_params = {'batch_size': config['train_batch_size'],\n                'shuffle': False,\n                'num_workers': 2,\n                'pin_memory':True\n                }\n\ntest_params = {'batch_size': config['valid_batch_size'],\n                'shuffle': False,\n                'num_workers': 2,\n                'pin_memory':True\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","9852a5c8":"# https:\/\/www.kaggle.com\/raghavendrakotala\/fine-tunned-on-roberta-base-as-ner-problem-0-533\ndef train(df,epoch):\n    tr_loss, tr_accuracy = 0, 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    batch_size=config['train_batch_size']\n    pad_token=-100\n    \n    # put model in training mode\n    model.train()\n    \n    for batch_start_idx in range(0, len(df), batch_size):\n        \n        batch_examples = df.iloc[batch_start_idx:batch_start_idx + batch_size]\n        \n        # GET TEXT AND WORD LABELS \n        text = batch_examples['text'].values.tolist()  \n        spans = batch_examples['spans'].values.tolist() \n\n        labels=batch_examples['discourse_type'].values.tolist() \n        labels = zip(*itertools.zip_longest(*labels, fillvalue=pad_token))\n        \n        # TOKENIZE TEXT\n        inputs = tokenizer(text, entity_spans=spans,padding=True)\n        inputs = {key: torch.as_tensor(val).cuda() for key, val in inputs.items()}\n\n        inputs['labels']=torch.as_tensor(list(labels)).cuda()\n        \n        output=model(**inputs,return_dict=False)\n    \n        loss=output[0]\n        logits=output[1]\n        \n        tr_loss += loss.item()\n\n        nb_tr_steps += 1\n        nb_tr_examples += inputs['labels'].size(0)\n        \n        if batch_start_idx % 200==0:\n            loss_step = tr_loss\/nb_tr_steps\n            print(f\"Training loss after {batch_start_idx:4d} training steps: {loss_step}\")\n           \n        # compute training accuracy\n        flattened_targets = inputs['labels'].view(-1) # shape (batch_size * seq_len,)\n        active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n        \n        # only compute accuracy at active labels\n        active_accuracy = inputs['labels'].view(-1) != -100 # shape (batch_size, seq_len)\n        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n        \n        labels = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        #tr_labels.extend(labels)\n        #tr_preds.extend(predictions)\n\n        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n        tr_accuracy += tmp_tr_accuracy\n    \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(\n            parameters=model.parameters(), max_norm=config['max_grad_norm']\n        )\n        \n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    epoch_loss = tr_loss \/ nb_tr_steps\n    tr_accuracy = tr_accuracy \/ nb_tr_steps\n    print(f\"Training loss epoch: {epoch_loss}\")\n    print(f\"Training accuracy epoch: {tr_accuracy}\")","6afa8c20":"# CREATE MODEL\nmodel = LukeForEntitySpanClassification.from_pretrained(config['model_name'])","79077b27":"model.classifier=nn.Linear(3072,len(labels_to_ids))\nmodel.num_labels=len(labels_to_ids)\nmodel.config.id2label=ids_to_labels\nmodel.config.label2id=labels_to_ids","aa74c666":"model.to(config['device'])\noptimizer = torch.optim.Adam(params=model.parameters(), lr=config['learning_rates'][0])","11a4e23c":"# LOOP TO TRAIN MODEL\nfor epoch in range(config['epochs']):\n\n    print(f\"### Training epoch: {epoch + 1}\")\n    for g in optimizer.param_groups: \n        g['lr'] = config['learning_rates'][epoch]\n    lr = optimizer.param_groups[0]['lr']\n    print(f'### LR = {lr}\\n')\n\n    train(train_df,epoch)\n    torch.cuda.empty_cache()\n    gc.collect()\n\ntorch.save(model.state_dict(), 'luke.pt')","71bc5b96":"train_df = pd.read_csv('..\/input\/train-ner\/train_NER.csv',nrows=1000)\n\ntrain_df['tokens']=train_df['text'].apply(lambda x: x.split())\ntrain_df['entities']=train_df['entities'].apply(lambda x: eval(x))","7d843454":"train_df.head(2)","685429ff":"# CHOOSE VALIDATION INDEXES (that match my TF notebook)\nIDS = train_df.id.unique()\nprint('There are',len(IDS),'train texts. We will split 90% 10% for validation.')\n\n# TRAIN VALID SPLIT 90% 10%\nnp.random.seed(42)\ntrain_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\nvalid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\nnp.random.seed(None)","63d481e8":"# CREATE TRAIN SUBSET AND VALID SUBSET\ndata = train_df[['id','tokens', 'entities']]\ntrain_dataset = data.loc[data['id'].isin(IDS[train_idx]),['tokens', 'entities']].reset_index(drop=True)\nval_dataset = data.loc[data['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(data.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(val_dataset.shape))\n","0579eb65":"label_all_tokens=True\n\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(examples[\"tokens\"], padding=True, truncation=True, \n                                 is_split_into_words=True, return_offsets_mapping=True)\n\n    labels = []\n    for i, label in enumerate(examples[\"entities\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n            # ignored in the loss function.\n            if word_idx is None:\n                label_ids.append(-100)\n            # We set the label for the first token of each word.\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            # For the other tokens in a word, we set the label to either the current label with I-tag or -100, depending on\n            # the label_all_tokens flag.\n            else:\n                if label_all_tokens:\n                    new_label = id2tag[label[word_idx]].replace('B-', 'I-')\n                    label_ids.append(tag2id[new_label])\n                else:\n                    label_ids.append(-100)\n            previous_word_idx = word_idx\n\n        labels.append(label_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","29bed677":"def compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","d5e4d6c2":"# mappings\nall_tags = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\nunique_tags = sorted(set(all_tags))\ntag2id = {tag: id for id, tag in enumerate(unique_tags)}\nid2tag = {id: tag for tag, id in tag2id.items()}","b9a7e4da":"# encode labels\ntrain_dataset['entities'] = train_dataset['entities'].apply(lambda labels: [tag2id[x] for x in labels])\nval_dataset['entities'] = val_dataset['entities'].apply(lambda labels: [tag2id[x] for x in labels])","3351c39b":"train_dataset = ds.from_pandas(train_dataset[['tokens', 'entities']])\nval_dataset = ds.from_pandas(val_dataset[['tokens', 'entities']])","15396181":"datasets = DatasetDict({\n    'train': train_dataset,\n    'val': val_dataset\n})\n\ndatasets","4ffc2ac4":"label_list = list(unique_tags)\nprint(sorted(label_list))","00489001":"class config:\n    batch_size=4\n    model_checkpoint = 'roberta-base'\n    num_classes=len(label_list)\n    label_all_tokens = True\n    lr_rate=3.3374982585607736e-05\n    epochs=5\n    weight_decay=0.013998996632720116\n    device='cuda' if cuda.is_available() else 'cpu'\n    \nconfig=config()","5223ed17":"tokenizer = AutoTokenizer.from_pretrained(config.model_checkpoint, add_prefix_space=True, do_lower_case=False)\nmodel = AutoModelForTokenClassification.from_pretrained(config.model_checkpoint, num_labels=len(label_list)).to(config.device)","b6118b0b":"model.num_labels = config.num_classes\nmodel.config.num_labels = config.num_classes\nmodel.config.id2label=id2tag\nmodel.config.label2id=tag2id","5ecad43d":"data_collator = DataCollatorForTokenClassification(tokenizer)","c39f5d95":"# align labels with subword tokens\ntokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\ntokenized_datasets","ae4eb838":"metric = load_metric(\"seqeval\")","c1367e2e":"if '\/' in config.model_checkpoint:\n    model_name = config.model_checkpoint.split(\"\/\")[-1]\nelse:\n    model_name = config.model_checkpoint\n\nargs = TrainingArguments(\n    output_dir=f\"{model_name}-finetuned-ner\",\n    logging_dir = f\"{model_name}-finetuned-ner\/runs\",\n    overwrite_output_dir=True,\n    evaluation_strategy = 'epoch',\n    save_strategy='epoch',\n    logging_strategy='epoch',\n#     save_steps=100,\n#     eval_steps=100,\n#     logging_steps=100,\n    #log_level='info',\n    learning_rate=config.lr_rate,\n    per_device_train_batch_size=config.batch_size,\n    per_device_eval_batch_size=config.batch_size,\n    num_train_epochs=config.epochs,\n    weight_decay=config.weight_decay,\n    push_to_hub=False,\n    load_best_model_at_end=True,\n    report_to=\"none\"\n)","e50b3b00":"trainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"val\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n)","2c8760dd":"#trainer.train()","cb91abad":"- The main contributions of this paper are summarized as follows:\n    - Contextualized representations specifically designed to address entity related tasks. LUKE is trained to predict randomly masked words and entities using a large amount of entity-annotated corpus obtained from Wikipedia.\n    - Entity-aware self-attention\n    mechanism, an effective extension of the original mechanism of transformer. The proposed mechanism considers the type of the tokens (words or entities) when computing attention scores.\n    - LUKE achieves strong empirical performance and obtains state-of-the-art results on five popular datasets: Open Entity, TACRED, CoNLL2003, ReCoRD, and SQuAD 1.1.\n","0bd85afd":"# <font color='brown' size=4>5. Acknowledgements<\/font>","f369dcb9":"> **\ud83d\udccc Note**:Architecture of LUKE using the input sentence \u201cBeyonce lives in Los Angeles. \u00b4 \u201d LUKE outputs contextualized representation for each word and entity in the text. The model is trained to predict randomly masked words\n(e.g., lives and Angeles in the figure) and entities (e.g., Los Angeles in the figure). Downstream tasks are solved\nusing its output representations with linear classifiers.","b362c0c4":"<p>Get span function gets token level spans from offset mapping <\/p>","b6f899e6":"<p>LUKE is based\non a transformer trained\nusing a large amount of entity-annotated corpus obtained from Wikipedia. An important difference\nbetween LUKE and existing CWRs(Context Word Representations) is that it treats\nnot only words, but also entities as independent tokens, and computes intermediate and output representations for all tokens using the transformer. Since entities are treated as tokens, LUKE can directly model the relationships between entities<\/p>","18e838c1":"# Table of Contents\n\n- 1. LUKE Demo\n\n- 2. LUKE Overview\n   - 2.1 Summary\n   - 2.2 Architecture\n   - 2.3 Benchmark results\n      \n- 3. LUKE Training\n   - 3.1 Helpers\n   - 3.2 Dataset\n   - 3.3 Engine\n\n- 4. HF trainer\n      \n- 5. Acknowledgements","eca730d2":"Uncomment the above line for training","11346c99":"## <font color='brown' size=4>3.1 Helpers<\/font>","3d4e33ad":"![image.png](attachment:ab8279b4-ede5-48f3-a616-97916119880d.png)","da7c3386":"> **\ud83d\udccc Note**: Brief overview of the above helper functions\n- correct_spans - fixes spans which are continuous \n    - eg: [(3,4),(4,5)] -> [(3,4),(5,6)]\n- fill_spans - fills the inbetween spans and also labels them as nil entity \n    - eg: [(5,8),(11,12)] ['claim','claim'] - > [(0,4),(5,8),(9,10),(11,12)] ['Nil','claim','Nil','claim']","f2aa59ea":"# <font color='brown' size=4>3. LUKE Training<\/font>","91171d83":"## <font color='brown' size=4>3.2 Dataset<\/font>","26ee9876":"## <font color='brown' size=4>3.3 Engine<\/font>","97935166":"## <font color='brown' size=4>2.3 Benchmark results<\/font>","5d5ffc1a":"<p>We run only on limited set of examples for demo and also on records which are less than 512 token length<\/p>","87a1c866":"<p>Author proposes a new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. LUKE model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. Author also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores.<\/p> \n\n<p>The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering).<\/p>","f746cb99":"# <font color='brown' size=4>2. LUKE Overview<\/font>","45be0a4c":"## <font color='brown' size=4>2.1 Summary<\/font>","91af7c98":"> **\ud83d\udccc Note**: Here we initialize two tokenizers.\n>1. To create entity spans\n>2. Luke specific tokenizer for the model inference\n\n>LUKE is based on RoBERTa and adds entity embeddings as well as an entity-aware self-attention mechanism, which helps improve performance on various downstream tasks involving reasoning about entities such as named entity recognition, extractive and cloze-style question answering, entity typing, and relation classification.","0f05d299":"<p>Tada. We have our entities from the spans.<\/p>\n\n> **\ud83d\udccc Note**:We need to pass all combination of token spans during the inference time to get the predictions for each spans. This is highly computational heavy","b361265e":"# <font color='brown' size=4>4. HF trainer<\/font>","550ee947":"# <font color='brown' size=4>Objective:<\/font> \n        \n<p> This notebook approaches this problem using NER but not as tokens classification but using spans to classify entities. Introducing <b>LUKE: Deep Contextualized Entity Representations with\nEntity-aware Self-attention<\/b> We will formulate the inputs accordingly to feed the data to LUKE model. Lets dig in further<\/p>","8ca22d5a":"<img src='https:\/\/d3i71xaburhd42.cloudfront.net\/eedf2748a9a1ba2779cde95fd8bad9c2260d5317\/2-Figure1-1.png' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","4d5940b7":"> **\ud83d\udccc Note**:The above function can be used to break down the longer text into chunks. It is not used as part of training process. We can use it in inference pipeline","6909c8f7":"## <font color='brown' size=4>2.2 Architecture<\/font>","47d9b9f4":"1. https:\/\/arxiv.org\/pdf\/2010.01057.pdf\n2. https:\/\/huggingface.co\/docs\/transformers\/model_doc\/luke\n3. https:\/\/www.kaggle.com\/cdeotte\/pytorch-bigbird-ner-cv-0-615","4395b00a":"# <font color='brown' size=4>1. LUKE Demo<\/font>"}}