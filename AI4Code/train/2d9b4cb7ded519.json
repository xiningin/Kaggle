{"cell_type":{"9aae4530":"code","2259f278":"code","31968cb1":"code","07e7d81a":"code","43f68c78":"code","2a9b0c43":"code","8a15d0b8":"code","94c087d8":"code","2da37d68":"code","02258d5f":"code","a95d9382":"code","02615670":"code","c68f7476":"code","1c98263c":"code","10e6a074":"code","99e3c6a1":"code","ef9b06bb":"code","a56ee0e4":"code","5bcdcd2e":"code","da7ca5f9":"code","35d647f2":"code","5b17c6a6":"code","d898f45f":"code","dfd1a39d":"code","04356f20":"code","41975fe3":"code","4f1bbf46":"code","1ff05806":"code","040da404":"code","aa82407f":"code","9e3c16fd":"code","00a71b57":"code","519fef2c":"code","b1f41f04":"code","47aaa7fb":"code","da940dd2":"code","e7253243":"code","1568c48f":"code","be711c1b":"code","96404070":"code","6f7d8692":"code","268e657f":"code","07be7e1c":"code","96e02cf6":"code","81c0a84b":"code","596554c2":"code","5b5acab3":"code","ebd54e10":"code","7478e8a6":"code","f95a4f18":"code","62786fa3":"code","5bc7f0ab":"code","7d6d39e5":"code","acc6816d":"code","26a9ac52":"code","d05eb3e1":"code","a5514bfe":"markdown","037bd31c":"markdown","05d8ea0a":"markdown","12a26c2e":"markdown","5e16b1f2":"markdown","c1b020f8":"markdown","0960d64e":"markdown","966643d8":"markdown","1e498e7f":"markdown","855a168e":"markdown","1fd73659":"markdown"},"source":{"9aae4530":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n \nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.decomposition import PCA\n \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n \nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom fastai.tabular.all import *","2259f278":"data_dir = '..\/input\/lish-moa\/'\nos.listdir(data_dir)","31968cb1":"train_features = pd.read_csv(data_dir + 'train_features.csv')\ntrain_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\ntrain_drug = pd.read_csv(data_dir + 'train_drug.csv')\ntest_features = pd.read_csv(data_dir + 'test_features.csv')\nsample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n\nprint('train_features: {}'.format(train_features.shape))\nprint('train_targets_scored: {}'.format(train_targets_scored.shape))\nprint('train_targets_nonscored: {}'.format(train_targets_nonscored.shape))\nprint('train_drug: {}'.format(train_drug.shape))\nprint('test_features: {}'.format(test_features.shape))\nprint('sample_submission: {}'.format(sample_submission.shape))","07e7d81a":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nprint('GENES: {}'.format(GENES[:10]))\nprint('CELLS: {}'.format(CELLS[:10]))","43f68c78":"!pip install \/kaggle\/input\/iterative-stratification\/iterative-stratification-master\/\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","2a9b0c43":"for col in (GENES + CELLS):\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","8a15d0b8":"SEED_VALUE = 42\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=SEED_VALUE)","94c087d8":"# GENES\nn_comp = 600\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=SEED_VALUE).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","2da37d68":"# CELLS\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=SEED_VALUE).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","02258d5f":"from sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","a95d9382":"train_drug.head()","02615670":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_nonscored, on='sig_id')\ntrain = train.merge(train_drug, on='sig_id')\ntrain = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)","c68f7476":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","1c98263c":"train.head()","10e6a074":"target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\naux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\nall_target_cols = target_cols + aux_target_cols\n\nnum_targets = len(target_cols)\nnum_aux_targets = len(aux_target_cols)\nnum_all_targets = len(all_target_cols)\n\nprint('num_targets: {}'.format(num_targets))\nprint('num_aux_targets: {}'.format(num_aux_targets))\nprint('num_all_targets: {}'.format(num_all_targets))","99e3c6a1":"print(train.shape)\nprint(test.shape)\nprint(sample_submission.shape)","ef9b06bb":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n        }\n        \n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n\n        return dct","a56ee0e4":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    return preds","5bcdcd2e":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            \n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","da7ca5f9":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model, self).__init__()\n        self.hidden_size = [1500, 1250, 1000, 750]\n        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_value[3])\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n            \n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","35d647f2":"class FineTuneScheduler:\n    def __init__(self, epochs):\n        self.epochs = epochs\n        self.epochs_per_step = 0\n        self.frozen_layers = []\n\n    def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n        self.frozen_layers = []\n\n        model_new = Model(num_features, num_targets)\n        model_new.load_state_dict(model.state_dict())\n\n        # Freeze all weights\n        for name, param in model_new.named_parameters():\n            layer_index = name.split('.')[0][-1]\n\n            if layer_index == 5:\n                continue\n\n            param.requires_grad = False\n\n            # Save frozen layer names\n            if layer_index not in self.frozen_layers:\n                self.frozen_layers.append(layer_index)\n\n        self.epochs_per_step = self.epochs \/\/ len(self.frozen_layers)\n\n        # Replace the top layers with another ones\n        model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n        model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n        model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n        model_new.to(DEVICE)\n        return model_new\n\n    def step(self, epoch, model):\n        if len(self.frozen_layers) == 0:\n            return\n\n        if epoch % self.epochs_per_step == 0:\n            last_frozen_index = self.frozen_layers[-1]\n            \n            # Unfreeze parameters of the last frozen layer\n            for name, param in model.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == last_frozen_index:\n                    param.requires_grad = True\n\n            del self.frozen_layers[-1]  # Remove the last layer as unfrozen","5b17c6a6":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","d898f45f":"feature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\nnum_features = len(feature_cols)\nnum_features","dfd1a39d":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 24\nBATCH_SIZE = 128\n\nWEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\nMAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\nDIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\nPCT_START = 0.1","04356f20":"# Show model architecture\nmodel = Model(num_features, num_all_targets)\nmodel","41975fe3":"from sklearn.model_selection import KFold\n\ndef make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n    vc = train.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n    for seed_id in range(SEEDS):\n        kfold_col = 'kfold_{}'.format(seed_id)\n        \n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        train[kfold_col] = train.drug_id.map(dct1)\n        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n        train[kfold_col] = train[kfold_col].astype('int8')\n        \n    return train\n\nSEEDS = 7\nNFOLDS = 7\nDRUG_THRESH = 18\n\ntrain = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\ntrain.head()","4f1bbf46":"def run_training(fold_id, seed_id):\n    seed_everything(seed_id)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    kfold_col = f'kfold_{seed_id}'\n    trn_idx = train_[train_[kfold_col] != fold_id].index\n    val_idx = train_[train_[kfold_col] == fold_id].index\n    \n    train_df = train_[train_[kfold_col] != fold_id].reset_index(drop=True)\n    valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n    \n    def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):\n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n        \n        train_dataset = MoADataset(x_train, y_train)\n        valid_dataset = MoADataset(x_valid, y_valid)\n\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                                  steps_per_epoch=len(trainloader),\n                                                  pct_start=PCT_START,\n                                                  div_factor=DIV_FACTOR[tag_name], \n                                                  max_lr=MAX_LR[tag_name],\n                                                  epochs=EPOCHS)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing=0.001)\n\n        oof = np.zeros((len(train), len(target_cols_now)))\n        best_loss = np.inf\n        \n        for epoch in range(EPOCHS):\n            if fine_tune_scheduler is not None:\n                fine_tune_scheduler.step(epoch, model)\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            print(f\"SEED: {seed_id}, FOLD: {fold_id}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n\n            if np.isnan(valid_loss):\n                break\n            \n            if valid_loss < best_loss:\n                best_loss = valid_loss\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), f\"{tag_name}_FOLD{fold_id}_.pth\")\n\n        return oof\n\n    fine_tune_scheduler = FineTuneScheduler(EPOCHS)\n\n    pretrained_model = Model(num_features, num_all_targets)\n    pretrained_model.to(DEVICE)\n\n    # Train on scored + nonscored targets\n    train_model(pretrained_model, 'ALL_TARGETS', all_target_cols)\n\n    # Load the pretrained model with the best loss\n    pretrained_model = Model(num_features, num_all_targets)\n    pretrained_model.load_state_dict(torch.load(f\"ALL_TARGETS_FOLD{fold_id}_.pth\"))\n    pretrained_model.to(DEVICE)\n\n    # Copy model without the top layer\n    final_model = fine_tune_scheduler.copy_without_top(pretrained_model, num_features, num_all_targets, num_targets)\n\n    # Fine-tune the model on scored targets only\n    oof = train_model(final_model, 'SCORED_ONLY', target_cols, fine_tune_scheduler)\n\n    # Load the fine-tuned model with the best loss\n    model = Model(num_features, num_targets)\n    model.load_state_dict(torch.load(f\"SCORED_ONLY_FOLD{fold_id}_.pth\"))\n    model.to(DEVICE)\n\n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    predictions = np.zeros((len(test_), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n    return oof, predictions","1ff05806":"def run_k_fold(NFOLDS, seed_id):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold_id in range(NFOLDS):\n        oof_, pred_ = run_training(fold_id, seed_id)\n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","040da404":"from time import time\n\n# Averaging on multiple SEEDS\nSEED = [0, 1, 2, 3, 4, 5, 6]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\ntime_begin = time()\n\nfor seed_id in SEED:\n    oof_, predictions_ = run_k_fold(NFOLDS, seed_id)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\ntime_diff = time() - time_begin\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","aa82407f":"from datetime import timedelta\nstr(timedelta(seconds=time_diff))","9e3c16fd":"train_targets_scored.head()","00a71b57":"len(target_cols)","519fef2c":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\n\nfor i in range(len(target_cols)):\n    score += log_loss(y_true[:, i], y_pred[:, i])\n\nprint(\"CV log_loss: \", score \/ y_pred.shape[1])","b1f41f04":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","47aaa7fb":"sub.shape","da940dd2":"freq_drugs_MoA = [[],\n                 ['egfr_inhibitor'],\n                 ['nfkb_inhibitor', 'proteasome_inhibitor'],\n                 ['cdk_inhibitor'],\n                 ['hmgcr_inhibitor'],\n                 ['tubulin_inhibitor'],\n                 ['raf_inhibitor'],\n                 ['flt3_inhibitor', 'kit_inhibitor', 'pdgfr_inhibitor']]","e7253243":"pred_cols=[f'pred{i}' for i in range(9)]\neps=1e-5 #numerical stability\nclass_df=pd.read_csv('..\/input\/moa-drug-classification-fastai\/drug_predictions.csv')\nclass_df[pred_cols]=class_df[pred_cols].clip(upper=1-eps) ","1568c48f":"#clip predictions using drug classification confidence\ntrain_copy=train.copy()\ntrain_copy=train_copy.merge(class_df[['sig_id']+pred_cols],on='sig_id')\nfor i in range(len(freq_drugs_MoA)):\n    clipping_1=train_copy[f'pred{i}']\n    train_copy[freq_drugs_MoA[i]]=train_copy[freq_drugs_MoA[i]].clip(lower=clipping_1,axis=0)\n    zero_cols=[col for col in target_cols if col not in freq_drugs_MoA[i]]\n    clipping_0=(1-train_copy[f'pred{i}']).pow(2)\n    train_copy[zero_cols]=train_copy[zero_cols].clip(upper=clipping_0,axis=0)","be711c1b":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train_copy[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\n\nfor i in range(len(target_cols)):\n    score += log_loss(y_true[:, i], y_pred[:, i])\n\nprint(\"CV log_loss: \", score \/ y_pred.shape[1])","96404070":"train_features=pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets=pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest_df=pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ndrug_df=pd.read_csv('..\/input\/lish-moa\/train_drug.csv')","6f7d8692":"#Minor trick - using categorical features both as categorical and continious \ntrain_features['cp_dose_cont']=train_features.cp_dose.apply(lambda x:int(x[1]))\ntrain_features['cp_time_cont']=train_features.cp_time\ntest_df['cp_dose_cont']=test_df.cp_dose.apply(lambda x:int(x[1]))\ntest_df['cp_time_cont']=test_df.cp_time","268e657f":"features=train_features.columns.tolist()\ncat_names=features[2:4]\ncont_names=features[4:]","07be7e1c":"y_names=train_targets.columns.tolist()[1:]","96e02cf6":"train_df=train_features.merge(train_targets,how='left',on='sig_id')\ntrain_df.head()","81c0a84b":"# remove control rows\ntrain_df=train_df[train_df.cp_type!='ctl_vehicle']\ntrain_df.reset_index(drop=True,inplace=True)\ntest_df=test_df[test_df.cp_type!='ctl_vehicle']\ntest_df.reset_index(drop=True,inplace=True)","596554c2":"train_df.shape,test_df.shape","5b5acab3":"#Build classification target - one of the 8 most frequent drugs or 'misc' for everything else\ntrain_df=train_df.merge(drug_df)\ntrain_df['vc']=train_df.groupby('drug_id')['sig_id'].transform('count')\ntrain_df.loc[train_df.vc>19,'drug']=train_df[train_df.vc>19].drug_id\ntrain_df.loc[train_df.vc<=19,'drug']='misc'","ebd54e10":"train_df.drug.value_counts().sort_index()","7478e8a6":"#fastai transform implementing Rank Gauss\nclass RankGauss(TabularProc):\n    def setups(self, to:Tabular,n_quantiles=200):\n        self.cont_names=to.cont_names\n        self.transformer = QuantileTransformer(n_quantiles=n_quantiles, output_distribution=\"normal\")\n        self.transformer.fit(to.train[self.cont_names])\n        return self(to)\n\n    def encodes(self, to:Tabular):\n        to[self.cont_names]=self.transformer.transform(to[self.cont_names])\n        return to","f95a4f18":"#Rank Gauss and embedding categorical features as the only data pre-processing steps\nprocs = [Categorify,RankGauss]","62786fa3":"splits=[L(range(len(train_df))),[]]\nto=TabularPandas(train_df,procs=procs, cat_names=cat_names, cont_names=cont_names, y_names='drug',\n                 y_block=CategoryBlock(),splits=splits)","5bc7f0ab":"bs=50\nnum_layers=2\nhidden_units=525\nlayers=[hidden_units]*num_layers\ndropout_rate=0.3\nconfig=tabular_config(ps=[dropout_rate]*num_layers,embed_p=dropout_rate,\n                      act_cls=nn.LeakyReLU(negative_slope=0.1,inplace=True))\ndls = to.dataloaders(bs=bs,val_bs=10000)\nlearn = tabular_learner(dls, layers=layers,\n                    config=config,\n                    cbs=[],\n                    opt_func = Lamb,\n                    metrics = [])","7d6d39e5":"#loading pretrained models and running inference on test data\npath_models=Path('..\/input\/moa-drug-classification-fastai')\nfnames=[fname for fname in (path_models\/'models').ls() if not str(fname).endswith('model.pth')]\nn_runs=len(fnames)\npreds=None\nfor fname in fnames:    \n    learn.path=path_models\n    learn.load(fname.stem);\n    learn.path=Path('.')\n    dl=learn.dls.test_dl(test_df)\n    with learn.no_bar():\n        pred=learn.get_preds(dl=dl)[0]\n    if preds is None: preds = pred\/n_runs\n    else:            preds += pred\/n_runs  ","acc6816d":"#clip test predictions using drug classification confidence\n\ntest[pred_cols]=preds.numpy()\ntest[pred_cols]=test[pred_cols].clip(upper=1-eps)\nfor i in range(len(freq_drugs_MoA)):\n    clipping_1=test[f'pred{i}']\n    test[freq_drugs_MoA[i]]=test[freq_drugs_MoA[i]].clip(lower=clipping_1,axis=0)\n    zero_cols=[col for col in y_names if col not in freq_drugs_MoA[i]]\n    clipping_0=(1-test[f'pred{i}']).pow(2.)\n    test[zero_cols]=test[zero_cols].clip(upper=clipping_0,axis=0)","26a9ac52":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","d05eb3e1":"sub.shape","a5514bfe":"This notebook is copied from https:\/\/www.kaggle.com\/thehemen\/pytorch-transfer-learning-with-k-folds-by-drug-ids. I have only added a few sections to the bottom of the notebook to demonstrate how post-processing based on drug classification can be applied (as explained here https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/200609).","037bd31c":"# Post-Processing Based on Drug Classification","05d8ea0a":"# RankGauss","12a26c2e":"# Model","5e16b1f2":"# Preprocessing steps","c1b020f8":"# Single fold training","0960d64e":"## Submission\nSubmission requires repeating data preparation steps from classification notebook.","966643d8":"## CV","1e498e7f":"# PCA features + Existing features","855a168e":"# feature Selection using Variance Encoding","1fd73659":"# Dataset Classes"}}