{"cell_type":{"2847848d":"code","252e9b2a":"code","dfe23381":"code","73189b9c":"code","e9ae8937":"markdown","d7886d06":"markdown"},"source":{"2847848d":"!pip install pytorch_fid\n\nimport os\nimport sys\nimport math\nimport json\nfrom tqdm import tqdm\nfrom math import floor, log2\nfrom random import random\nfrom shutil import rmtree\nfrom functools import partial\nimport multiprocessing\nfrom contextlib import contextmanager, ExitStack\nimport numpy as np\nimport torch\nfrom torch import nn, einsum\nfrom torch.utils import data\nfrom torch.optim import Adam\nimport torch.nn.functional as F\nfrom torch.autograd import grad as torch_grad\nfrom kornia.filters import filter2D\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport gc\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntorch.backends.cudnn.benchmark = True","252e9b2a":"image_paths = os.listdir(\"..\/input\/flickrfaceshq-dataset-ffhq\")\nimage_paths = [f\"..\/input\/flickrfaceshq-dataset-ffhq\/{el}\" for el in image_paths]\n\nclass ImageDataset(Dataset):\n    def __init__(self, image_paths):\n        super().__init__()\n        self.image_paths = image_paths\n        self.transform = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.RandomGrayscale(0.5),\n            transforms.RandomHorizontalFlip(0.5), \n            transforms.ToTensor()\n        ])\n        self.cutout = transforms.Compose([\n            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.1)),\n            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.1)),\n            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.1)),\n            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.1)),\n            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.05)),\n            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.05)),\n            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.05)),\n            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.05)),\n            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.05)),\n            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.05)),\n            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.05)),\n            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.05)),\n        ])\n        self.normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        self.normalize_mask = transforms.Normalize((0.5, ), (0.5, ))\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        image = Image.open(self.image_paths[index])\n        image = self.transform(image)\n        mask = torch.zeros((1, 256, 256))\n        mask = self.cutout(mask)\n        image_cutout = torch.where(mask.expand_as(image) == 1.0, torch.ones_like(image), image)\n        return self.normalize(image), self.normalize(image_cutout), self.normalize_mask(mask), mask","dfe23381":"NUM_CORES = multiprocessing.cpu_count()\nEXTS = ['jpg', 'jpeg', 'png']\n\nclass NanException(Exception):\n    pass\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.reshape(x.shape[0], -1)\n\nclass RandomApply(nn.Module):\n    def __init__(self, prob, fn, fn_else = lambda x: x):\n        super().__init__()\n        self.fn = fn\n        self.fn_else = fn_else\n        self.prob = prob\n    def forward(self, x):\n        fn = self.fn if random() < self.prob else self.fn_else\n        return fn(x)\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x):\n        return self.fn(x) + x\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = ChanNorm(dim)\n\n    def forward(self, x):\n        return self.fn(self.norm(x))\n\nclass ChanNorm(nn.Module):\n    def __init__(self, dim, eps = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n\n    def forward(self, x):\n        std = torch.var(x, dim = 1, unbiased = False, keepdim = True).sqrt()\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) \/ (std + self.eps) * self.g + self.b\n\nclass PermuteToFrom(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x):\n        x = x.permute(0, 2, 3, 1)\n        out, loss = self.fn(x)\n        out = out.permute(0, 3, 1, 2)\n        return out, loss\n\nclass Blur(nn.Module):\n    def __init__(self):\n        super().__init__()\n        f = torch.Tensor([1, 2, 1])\n        self.register_buffer('f', f)\n    def forward(self, x):\n        f = self.f\n        f = f[None, None, :] * f [None, :, None]\n        return filter2D(x, f, normalized=True)\n\nclass DepthWiseConv2d(nn.Module):\n    def __init__(self, dim_in, dim_out, kernel_size, padding = 0, stride = 1, bias = True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, dim_in, kernel_size = kernel_size, padding = padding, groups = dim_in, stride = stride, bias = bias),\n            nn.Conv2d(dim_in, dim_out, kernel_size = 1, bias = bias)\n        )\n    def forward(self, x):\n        return self.net(x)\n\ndef exists(val):\n    return val is not None\n\n@contextmanager\ndef null_context():\n    yield\n\ndef combine_contexts(contexts):\n    @contextmanager\n    def multi_contexts():\n        with ExitStack() as stack:\n            yield [stack.enter_context(ctx()) for ctx in contexts]\n    return multi_contexts\n\ndef default(value, d):\n    return value if exists(value) else d\n\ndef cycle(iterable):\n    while True:\n        for i in iterable:\n            yield i\n\ndef cast_list(el):\n    return el if isinstance(el, list) else [el]\n\ndef is_empty(t):\n    if isinstance(t, torch.Tensor):\n        return t.nelement() == 0\n    return not exists(t)\n\ndef gradient_penalty(images, output, weight = 10):\n    batch_size = images.shape[0]\n    gradients = torch_grad(outputs=output, inputs=images,\n                           grad_outputs=torch.ones(output.size(), device=images.device),\n                           create_graph=True, retain_graph=True, only_inputs=True)[0]\n\n    gradients = gradients.reshape(batch_size, -1)\n    return weight * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n\ndef calc_pl_lengths(styles, images):\n    device = images.device\n    num_pixels = images.shape[2] * images.shape[3]\n    pl_noise = torch.randn(images.shape, device=device) \/ math.sqrt(num_pixels)\n    outputs = (images * pl_noise).sum()\n\n    pl_grads = torch_grad(outputs=outputs, inputs=styles,\n                          grad_outputs=torch.ones(outputs.shape, device=device),\n                          create_graph=True, retain_graph=True, only_inputs=True)[0]\n\n    return (pl_grads ** 2).sum(dim=2).mean(dim=1).sqrt()\n\ndef noise(n, latent_dim, device):\n    return torch.randn(n, latent_dim).cuda(device)\n\ndef noise_list(n, layers, latent_dim, device):\n    return [(noise(n, latent_dim, device), layers)]\n\ndef mixed_list(n, layers, latent_dim, device):\n    tt = int(torch.rand(()).numpy() * layers)\n    return noise_list(n, tt, latent_dim, device) + noise_list(n, layers - tt, latent_dim, device)\n\ndef latent_to_w(style_vectorizer, latent_descr):\n    \"\"\"print()\n    for z, num_layers in latent_descr:\n        print(z.shape)\n        print(num_layers)\n    print()\"\"\"\n    return [(style_vectorizer(z), num_layers) for z, num_layers in latent_descr]\n\ndef image_noise(n, im_size, device):\n    return torch.FloatTensor(n, im_size, im_size, 1).uniform_(0., 1.).cuda(device)\n\ndef leaky_relu(p=0.2):\n    return nn.LeakyReLU(p, inplace=True)\n\ndef evaluate_in_chunks(max_batch_size, model, *args):\n    split_args = list(zip(*list(map(lambda x: x.split(max_batch_size, dim=0), args))))\n    chunked_outputs = [model(*i) for i in split_args]\n    if len(chunked_outputs) == 1:\n        return chunked_outputs[0]\n    return torch.cat(chunked_outputs, dim=0)\n\ndef styles_def_to_tensor(styles_def):\n    return torch.cat([t[:, None, :].expand(-1, n, -1) for t, n in styles_def], dim=1)\n\ndef set_requires_grad(model, bool):\n    for p in model.parameters():\n        p.requires_grad = bool\n\ndef slerp(val, low, high):\n    low_norm = low \/ torch.norm(low, dim=1, keepdim=True)\n    high_norm = high \/ torch.norm(high, dim=1, keepdim=True)\n    omega = torch.acos((low_norm * high_norm).sum(1))\n    so = torch.sin(omega)\n    res = (torch.sin((1.0 - val) * omega) \/ so).unsqueeze(1) * low + (torch.sin(val * omega) \/ so).unsqueeze(1) * high\n    return res\n\ndef d_logistic_loss(fake_pred, real_pred):\n    real_loss = F.softplus(-real_pred)\n    fake_loss = F.softplus(fake_pred)\n    return real_loss.mean() + fake_loss.mean()\n\ndef g_nonsaturating_loss(fake_pred):\n    loss = F.softplus(-fake_pred).mean()\n    return loss\n\ndef gen_hinge_loss(fake, real):\n    return fake.mean()\n\ndef hinge_loss(fake):\n    return (F.relu(1 + real) + F.relu(1 - fake)).mean() \n\n# stylegan2 classes\nclass EqualLinear(nn.Module):\n    def __init__(self, in_dim, out_dim, lr_mul = 1, bias = True):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))\n        if bias:\n            self.bias = nn.Parameter(torch.zeros(out_dim))\n\n        self.lr_mul = lr_mul\n\n    def forward(self, input):\n        return F.linear(input, self.weight * self.lr_mul, bias=self.bias * self.lr_mul)\n\nclass MappingNet(nn.Module):\n    def __init__(self, emb, depth, lr_mul = 0.1):\n        super().__init__()\n        layers = []\n        for i in range(depth):\n            layers.extend([EqualLinear(emb, emb, lr_mul), leaky_relu()])\n\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = F.normalize(x, dim=1)\n        return self.net(x)\n\nclass RGBBlock(nn.Module):\n    def __init__(self, latent_dim, input_channel, upsample, rgba = False):\n        super().__init__()\n        self.input_channel = input_channel\n        self.to_style = nn.Linear(latent_dim, input_channel)\n\n        out_filters = 3 if not rgba else 4\n        self.conv = Conv2DMod(input_channel, out_filters, 1, demod=False)\n\n        self.upsample = nn.Sequential(\n            nn.Upsample(scale_factor = 2, mode='bilinear', align_corners=False),\n            Blur()\n        ) if upsample else None\n\n    def forward(self, x, prev_rgb, istyle):\n        b, c, h, w = x.shape\n        style = self.to_style(istyle)\n        x = self.conv(x, style)\n\n        if exists(prev_rgb):\n            x = x + prev_rgb\n\n        if exists(self.upsample):\n            x = self.upsample(x)\n\n        return x\n\nclass Conv2DMod(nn.Module):\n    def __init__(self, in_chan, out_chan, kernel, demod=True, stride=1, dilation=1, eps = 1e-8, **kwargs):\n        super().__init__()\n        self.filters = out_chan\n        self.demod = demod\n        self.kernel = kernel\n        self.stride = stride\n        self.dilation = dilation\n        self.weight = nn.Parameter(torch.randn((out_chan, in_chan, kernel, kernel)))\n        self.eps = eps\n        nn.init.kaiming_normal_(self.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n\n    def _get_same_padding(self, size, kernel, dilation, stride):\n        return ((size - 1) * (stride - 1) + dilation * (kernel - 1)) \/\/ 2\n\n    def forward(self, x, y):\n        b, c, h, w = x.shape\n\n        w1 = y[:, None, :, None, None]\n        w2 = self.weight[None, :, :, :, :]\n        weights = w2 * (w1 + 1)\n\n        if self.demod:\n            d = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4), keepdim=True) + self.eps)\n            weights = weights * d\n\n        x = x.reshape(1, -1, h, w)\n\n        _, _, *ws = weights.shape\n        weights = weights.reshape(b * self.filters, *ws)\n\n        padding = self._get_same_padding(h, self.kernel, self.dilation, self.stride)\n        x = F.conv2d(x, weights, padding=padding, groups=b)\n\n        x = x.reshape(-1, self.filters, h, w)\n        return x\n\nclass GeneratorBlock(nn.Module):\n    def __init__(self, latent_dim, input_channels, filters, upsample = True, upsample_rgb = True, rgba = False):\n        super().__init__()\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False) if upsample else None\n\n        self.to_style1 = nn.Linear(latent_dim, input_channels * 2)\n        self.to_noise1 = nn.Linear(1, filters)\n        self.conv1 = Conv2DMod(input_channels * 2, filters, 3)\n        \n        self.to_style2 = nn.Linear(latent_dim, filters)\n        self.to_noise2 = nn.Linear(1, filters)\n        self.conv2 = Conv2DMod(filters, filters, 3)\n\n        self.activation = leaky_relu()\n        self.to_rgb = RGBBlock(latent_dim, filters, upsample_rgb, rgba)\n\n    def forward(self, x, skip, prev_rgb, istyle, inoise):\n        if exists(self.upsample):\n            x = self.upsample(x)\n            \n        x = torch.cat([x, skip], dim=1)\n\n        inoise = inoise[:, :x.shape[2], :x.shape[3], :]\n        noise1 = self.to_noise1(inoise).permute((0, 3, 2, 1))\n        noise2 = self.to_noise2(inoise).permute((0, 3, 2, 1))\n\n        style1 = self.to_style1(istyle)\n        x = self.conv1(x, style1)\n        x = self.activation(x + noise1)\n\n        style2 = self.to_style2(istyle)\n        x = self.conv2(x, style2)\n        x = self.activation(x + noise2)\n\n        rgb = self.to_rgb(x, prev_rgb, istyle)\n        return x, rgb\n\nclass DiscriminatorBlock(nn.Module):\n    def __init__(self, input_channels, filters, downsample=True, with_skip=False):\n        super().__init__()\n        self.conv_res = nn.Conv2d(input_channels, filters, 1, stride = (2 if downsample else 1))\n\n        self.net = nn.Sequential(\n            nn.Conv2d(input_channels, filters, 3, padding=1),\n            leaky_relu(),\n            nn.Conv2d(filters, filters, 3, padding=1),\n            leaky_relu()\n        )\n        \n        self.downsample = nn.Sequential(\n            Blur(),\n            nn.Conv2d(filters, filters, 3, padding = 1, stride = 2)\n        ) if downsample else None\n\n        self.with_skip = with_skip\n        \n    def forward(self, x):\n        res = self.conv_res(x)\n        x = self.net(x)\n        skip = x\n        if exists(self.downsample):\n            x = self.downsample(x)\n        x = (x + res) * (1 \/ math.sqrt(2))\n        if self.with_skip: \n            return x, skip\n        return x\n\nclass Generator(nn.Module):\n    def __init__(self, image_size, latent_dim, network_capacity = 16, transparent = False, fmap_max = 512):\n        super().__init__()\n        self.image_size = image_size\n        self.latent_dim = latent_dim\n        self.num_layers = int(log2(image_size) - 2)\n\n        filters = [network_capacity * (2 ** (i + 1)) for i in range(self.num_layers)][::-1]\n\n        set_fmap_max = partial(min, fmap_max)\n        filters = list(map(set_fmap_max, filters))\n        init_channels = filters[0]\n        filters = [init_channels, *filters]\n\n        in_out_pairs = zip(filters[:-1], filters[1:])\n\n        self.upsample = nn.Upsample(scale_factor = 2, mode='bilinear', align_corners=False)\n        \n        self.initial_conv = nn.Conv2d(filters[0], filters[0], 3, padding=1)\n        self.blocks = nn.ModuleList([])\n\n        for ind, (in_chan, out_chan) in enumerate(in_out_pairs):\n            not_first = ind != 0\n            not_last = ind != (self.num_layers - 1)\n            num_layer = self.num_layers - ind\n\n\n            block = GeneratorBlock(\n                latent_dim * 2,\n                in_chan,\n                out_chan,\n                upsample = not_first,\n                upsample_rgb = not_last,\n                rgba = transparent\n            )\n            self.blocks.append(block)\n\n    def forward(self, x, skips, styles_mapping, styles_encoder, input_noise):\n        batch_size = styles_mapping.shape[0]\n        image_size = self.image_size\n        \n        styles_encoder = styles_encoder.unsqueeze(1)\n        styles_encoder = styles_encoder.expand(\n            styles_encoder.shape[0], \n            styles_mapping.shape[1], \n            styles_encoder.shape[2]\n        )\n        \n        styles = torch.cat([styles_mapping, styles_encoder], dim=2)\n\n        rgb = None\n        styles = styles.transpose(0, 1)\n        x = self.initial_conv(x)\n        x = self.upsample(x)\n\n        for style, block, skip in zip(styles, self.blocks, skips[::-1]):\n            x, rgb = block(x, skip, rgb, style, input_noise)\n\n        return rgb\n    \nclass Encoder(nn.Module):\n    def __init__(self, image_size, network_capacity = 32, transparent = False, fmap_max = 512):\n        super().__init__()\n        num_layers = int(log2(image_size) - 2)\n        num_init_filters = 4\n\n        blocks = []\n        filters = [num_init_filters] + [(network_capacity * 4) * (2 ** i) for i in range(num_layers + 1)]\n\n        set_fmap_max = partial(min, fmap_max)\n        filters = list(map(set_fmap_max, filters))\n        chan_in_out = list(zip(filters[:-1], filters[1:]))\n\n        blocks = []\n\n        for ind, (in_chan, out_chan) in enumerate(chan_in_out):\n            num_layer = ind + 1\n            is_not_last = ind != (len(chan_in_out) - 1)\n\n            block = DiscriminatorBlock(in_chan, out_chan, downsample = is_not_last, with_skip=True)\n            blocks.append(block)\n\n        self.blocks = nn.ModuleList(blocks)\n\n        chan_last = filters[-1]\n        latent_dim = 4 * 4 * chan_last\n\n        self.final_conv = nn.Conv2d(chan_last, chan_last, 3, padding=1)\n        self.flatten = Flatten()\n        self.to_latent = nn.Linear(latent_dim, 512)\n        self.dropout = nn.Dropout(p=0.5)\n        \n    def forward(self, x,  mask_norm_batch):\n        b, *_ = x.shape\n        \n        x = torch.cat([x, mask_norm_batch], dim=1)\n        skips = []\n        for i, block in enumerate(self.blocks):\n            x, skip = block(x)\n            if i != len(self.blocks) - 1:\n                skips.append(skip)\n\n        x = self.final_conv(x)\n        latents = self.flatten(x)\n        latents = self.to_latent(latents)\n        latents = self.dropout(latents)\n        latents = latents.squeeze()\n        return x, latents, skips\n\nclass Discriminator(nn.Module):\n    def __init__(self, image_size, network_capacity = 16, transparent = False, fmap_max = 512):\n        super().__init__()\n        num_layers = int(log2(image_size) - 2)\n        num_init_filters = 4\n\n        blocks = []\n        filters = [num_init_filters] + [(network_capacity * 4) * (2 ** i) for i in range(num_layers + 1)]\n\n        set_fmap_max = partial(min, fmap_max)\n        filters = list(map(set_fmap_max, filters))\n        chan_in_out = list(zip(filters[:-1], filters[1:]))\n\n        blocks = []\n        for ind, (in_chan, out_chan) in enumerate(chan_in_out):\n            num_layer = ind + 1\n            is_not_last = ind != (len(chan_in_out) - 1)\n\n            block = DiscriminatorBlock(in_chan, out_chan, downsample = is_not_last)\n            blocks.append(block)\n\n        self.blocks = nn.ModuleList(blocks)\n\n        chan_last = filters[-1]\n        latent_dim = 4 * 4 * chan_last\n\n        self.final_conv = nn.Conv2d(chan_last, chan_last, 3, padding=1)\n        self.flatten = Flatten()\n        self.to_logit = nn.Linear(latent_dim, 1)\n\n    def forward(self, x, mask):\n        b, *_ = x.shape\n        \n        x = torch.cat([x, mask], dim=1)\n    \n        for block in self.blocks:\n            x = block(x)\n\n\n        x = self.final_conv(x)\n        x = self.flatten(x)\n        x = self.to_logit(x)\n        return x.squeeze()\n\nclass StyleGAN2(nn.Module):\n    def __init__(self, image_size, latent_dim = 512, fmap_max = 512, style_depth = 8, network_capacity = 16, transparent = False, steps = 1, lr = 1e-4, ttur_mult = 2, lr_mlp = 0.1, rank = 0):\n        super().__init__()\n        self.lr = lr\n        self.steps = steps\n\n        self.S = MappingNet(latent_dim, style_depth, lr_mul = lr_mlp)\n        self.G = Generator(image_size, latent_dim, network_capacity, transparent = transparent, fmap_max = fmap_max)\n        self.D = Discriminator(image_size, network_capacity, transparent = transparent, fmap_max = fmap_max)\n        self.E = Encoder(image_size, network_capacity,  transparent = transparent, fmap_max = fmap_max)\n        \n        print(self.S)\n        print(self.D)\n        print(self.G)\n        print(self.E)\n        \n        # init optimizers\n        generator_params = list(self.G.parameters()) + list(self.S.parameters()) + list(self.E.parameters())\n        self.G_opt = Adam(generator_params, lr = self.lr, betas=(0.5, 0.9))\n        self.D_opt = Adam(self.D.parameters(), lr = self.lr * ttur_mult, betas=(0.5, 0.9))\n\n        # init weights\n        self._init_weights()\n        self.cuda(rank)\n    \n\n    def _init_weights(self):\n        for m in self.modules():\n            if type(m) in {nn.Conv2d, nn.Linear}:\n                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n\n        for block in self.G.blocks:\n            nn.init.zeros_(block.to_noise1.weight)\n            nn.init.zeros_(block.to_noise2.weight)\n            nn.init.zeros_(block.to_noise1.bias)\n            nn.init.zeros_(block.to_noise2.bias)\n\n    def forward(self, x):\n        return x\n    \ndef reconstruct_image(real_images, fake_images, mask):\n    return (mask * fake_images + (1 - mask) * real_images).cuda()\n\nclass Trainer():\n    def __init__(\n        self,\n        name = 'default',\n        results_dir = 'results',\n        models_dir = 'models',\n        base_dir = '.\/',\n        image_size = 128,\n        network_capacity = 32,\n        fmap_max = 512,\n        transparent = False,\n        batch_size = 4,\n        mixed_prob = 0.9,\n        lr = 2e-4,\n        lr_mlp = 0.1,\n        ttur_mult = 2,\n        num_workers = None,\n        save_every = 1000,\n        evaluate_every = 1000,\n        num_image_tiles = 8,\n        trunc_psi = 0.6,\n        no_pl_reg = False,\n        aug_types = ['translation', 'cutout'],\n        generator_top_k_gamma = 0.99,\n        generator_top_k_frac = 0.5,\n        calculate_fid_every = None,\n        calculate_fid_num_images = 12800,\n        clear_fid_cache = False,\n        rank = 0,\n        world_size = 1,\n        *args,\n        **kwargs\n    ):\n        self.GAN_params = [args, kwargs]\n        self.GAN = None\n\n        self.name = name\n\n        base_dir = Path(base_dir)\n        self.base_dir = base_dir\n        self.results_dir = base_dir \/ results_dir\n        self.models_dir = base_dir \/ models_dir\n        self.fid_dir = base_dir \/ 'fid' \/ name\n        self.config_path = self.models_dir \/ name \/ '.config.json'\n\n        assert log2(image_size).is_integer(), 'image size must be a power of 2 (64, 128, 256, 512, 1024)'\n        self.image_size = image_size\n        self.network_capacity = network_capacity\n        self.fmap_max = fmap_max\n        self.transparent = transparent\n\n        self.aug_types = aug_types\n\n        self.lr = lr\n        self.lr_mlp = lr_mlp\n        self.ttur_mult = ttur_mult\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.mixed_prob = mixed_prob\n\n        self.num_image_tiles = num_image_tiles\n        self.evaluate_every = evaluate_every\n        self.save_every = save_every\n        self.steps = 0\n\n        self.av = None\n        self.trunc_psi = trunc_psi\n\n        self.no_pl_reg = no_pl_reg\n        self.pl_mean = None\n\n        self.d_loss = 0\n        self.d_loss_ema = 0\n        self.g_loss = 0\n        self.g_loss_ema = 0\n        self.last_gp_loss = None\n        self.last_cr_loss = None\n        self.last_fid = None\n\n        self.init_folders()\n\n        self.loader = None\n\n        self.calculate_fid_every = calculate_fid_every\n        self.calculate_fid_num_images = calculate_fid_num_images\n        self.clear_fid_cache = clear_fid_cache\n\n        self.generator_top_k_gamma = generator_top_k_gamma\n        self.generator_top_k_frac = generator_top_k_frac\n\n        self.is_main = rank == 0\n        self.rank = rank\n        self.world_size = world_size\n\n    @property\n    def image_extension(self):\n        return 'jpg' if not self.transparent else 'png'\n\n    @property\n    def checkpoint_num(self):\n        return floor(self.steps \/\/ self.save_every)\n\n    @property\n    def hparams(self):\n        return {'image_size': self.image_size, 'network_capacity': self.network_capacity}\n        \n    def init_GAN(self):\n        args, kwargs = self.GAN_params\n        self.GAN = StyleGAN2(lr = self.lr, lr_mlp = self.lr_mlp, ttur_mult = self.ttur_mult, image_size = self.image_size, network_capacity = self.network_capacity, fmap_max = self.fmap_max, transparent = self.transparent, rank = self.rank, *args, **kwargs)\n\n\n    def write_config(self):\n        self.config_path.write_text(json.dumps(self.config()))\n\n    def load_config(self):\n        config = self.config() if not self.config_path.exists() else json.loads(self.config_path.read_text())\n        self.image_size = config['image_size']\n        self.network_capacity = config['network_capacity']\n        self.transparent = config['transparent']\n        self.fmap_max = config.pop('fmap_max', 512)\n        self.lr_mlp = config.pop('lr_mlp', 0.1)\n        del self.GAN\n        self.init_GAN()\n\n    def config(self):\n        return {'image_size': self.image_size, 'network_capacity': self.network_capacity, 'lr_mlp': self.lr_mlp, 'transparent': self.transparent}\n\n    def set_data_src(self, folder):\n        self.dataset = ImageDataset(image_paths)\n        num_workers = NUM_CORES \n        dataloader = data.DataLoader(\n            self.dataset, \n            num_workers = num_workers, \n            batch_size = self.batch_size,\n            shuffle = True, \n            drop_last = True, \n            pin_memory = True\n        )\n        self.loader = cycle(dataloader)\n\n    def train(self):\n        assert exists(self.loader), 'You must first initialize the data source with `.set_data_src(<folder of images>)`'\n\n        if not exists(self.GAN):\n            self.init_GAN()\n\n        self.GAN.train()\n        total_disc_loss = torch.tensor(0.).cuda(self.rank)\n        total_gen_loss = torch.tensor(0.).cuda(self.rank)\n\n        batch_size = self.batch_size\n\n        image_size = self.GAN.G.image_size\n        latent_dim = self.GAN.G.latent_dim\n        num_layers = self.GAN.G.num_layers\n\n        aug_types  = self.aug_types\n        aug_kwargs = {'types': aug_types}\n\n        apply_gradient_penalty = self.steps % 4 == 0\n        \n        S = self.GAN.S\n        G = self.GAN.G\n        D = self.GAN.D\n        E = self.GAN.E\n\n        # setup losses\n        D_loss_fn = d_logistic_loss\n        G_loss_fn = g_nonsaturating_loss\n        G_requires_reals = False\n\n\n        # train discriminator\n\n        avg_pl_length = self.pl_mean\n        self.GAN.D_opt.zero_grad()\n\n        get_latents_fn = mixed_list if random() < self.mixed_prob else noise_list\n        style = get_latents_fn(batch_size, num_layers, latent_dim, device=self.rank)\n        noise = image_noise(batch_size, image_size, device=self.rank)\n\n        w_space = latent_to_w(S, style)\n        w_styles_mapping = styles_def_to_tensor(w_space)\n\n        image_batch, image_cut_batch, mask_norm_batch, mask_batch = next(self.loader)\n        image_batch, image_cut_batch, mask_norm_batch, mask_batch = image_batch.cuda(), image_cut_batch.cuda(), mask_norm_batch.cuda(), mask_batch.cuda()\n        \n        with torch.no_grad():\n            images, w_styles_encoder, skips = E(image_cut_batch, mask_norm_batch)\n            generated_images = G(images, skips, w_styles_mapping, w_styles_encoder, noise)\n        fake_output = D(reconstruct_image(image_batch, generated_images.clone().detach(), mask_batch), mask_norm_batch)\n\n        image_batch.requires_grad_()\n        real_output = D(image_batch, mask_norm_batch)\n        divergence = D_loss_fn(fake_output, real_output)\n        disc_loss = divergence\n\n        if apply_gradient_penalty:\n            gp = gradient_penalty(image_batch, real_output)\n            self.last_gp_loss = gp.clone().detach().item()\n            disc_loss = disc_loss + gp\n\n        disc_loss.backward()\n\n        total_disc_loss += divergence.detach().item()\n\n        self.d_loss = float(total_disc_loss)\n        self.d_loss_ema = 0.98  * self.d_loss_ema + 0.02 * self.d_loss\n        \n        nn.utils.clip_grad_norm_(D.parameters(), 1.0)\n        self.GAN.D_opt.step()\n\n        # train generator\n        self.GAN.G_opt.zero_grad()\n\n        style = get_latents_fn(batch_size, num_layers, latent_dim, device=self.rank)\n        noise = image_noise(batch_size, image_size, device=self.rank)\n\n        w_space = latent_to_w(S, style)\n        w_styles_mapping = styles_def_to_tensor(w_space)\n\n        images, w_styles_encoder, skips = E(image_cut_batch, mask_norm_batch)\n        generated_images = G(images, skips, w_styles_mapping, w_styles_encoder, noise)\n        fake_output = D(reconstruct_image(image_batch, generated_images, mask_batch), mask_norm_batch)\n\n        real_output = None\n        if G_requires_reals:\n            image_batch = next(self.loader).cuda(self.rank)\n            real_output = D(image_batch.detach())\n            real_output = real_output.detach()\n\n        loss = G_loss_fn(fake_output)\n        gen_loss = loss\n\n        gen_loss.backward()\n\n        total_gen_loss += loss.detach().item()\n\n        self.g_loss = float(total_gen_loss)\n        self.g_loss_ema = 0.98  * self.g_loss_ema + 0.02 * self.g_loss\n        \n        nn.utils.clip_grad_norm_(list(G.parameters()) + list(S.parameters()) + list(E.parameters()), 1.0)\n        self.GAN.G_opt.step()\n\n        # save from NaN errors\n\n        if any(torch.isnan(l) for l in (total_gen_loss, total_disc_loss)):\n            print(f'NaN detected for generator or discriminator. Loading from checkpoint #{self.checkpoint_num}')\n            self.load(self.checkpoint_num)\n            raise NanException\n        \n        if self.steps % 10 == 0:\n            gc.collect()\n        \n        # periodically save results\n        if self.steps  % 10 == 0:\n            print(f\"gen_loss: {round(self.g_loss_ema, 4)} disc_loss: {round(self.d_loss_ema, 4)} last_gp_loss: {round(self.last_gp_loss \/ 4, 4)}\")\n        if self.steps % 100 == 0 and self.steps != 0:\n            \"\"\"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 20))\n            ax[0, 0].imshow((image_cut_batch[0].cpu().detach().permute(1, 2, 0).numpy() * 0.5) + 0.5)\n            ax[0, 1].imshow((generated_images[0].cpu().detach().permute(1, 2, 0).numpy() * 0.5) + 0.5)\n            ax[1, 0].imshow(mask_batch[0].cpu().permute(1, 2, 0).numpy(), cmap=\"gray\")\n            ax[1, 1].imshow((reconstruct_image(image_batch, generated_images.clone().detach(), mask_batch)[0].detach().cpu().permute(1, 2, 0).numpy() * 0.5) + 0.5)\"\"\"\n            fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 20))\n            ax[0].imshow((image_cut_batch[0].cpu().detach().permute(1, 2, 0).numpy() * 0.5) + 0.5)\n            ax[0].set_title(\"input\")\n            ax[1].imshow((reconstruct_image(image_batch, generated_images.clone().detach(), mask_batch)[0].detach().cpu().permute(1, 2, 0).numpy() * 0.5) + 0.5)\n            ax[1].set_title(\"output\")\n            plt.show()\n\n        if self.steps % self.save_every == 0:\n            self.save(self.checkpoint_num)\n\n        \"\"\"if self.steps % self.evaluate_every == 0 or (self.steps % 100 == 0 and self.steps < 2500):\n            self.evaluate(floor(self.steps \/ self.evaluate_every))\n\n        if exists(self.calculate_fid_every) and self.steps % self.calculate_fid_every == 0 and self.steps != 0:\n            num_batches = math.ceil(self.calculate_fid_num_images \/ self.batch_size)\n            fid = self.calculate_fid(num_batches)\n            self.last_fid = fid\n\n            with open(str(self.results_dir \/ self.name \/ f'fid_scores.txt'), 'a') as f:\n                f.write(f'{self.steps},{fid}\\n')\"\"\"\n\n        self.steps += 1\n        self.av = None\n\n    @torch.no_grad()\n    def evaluate(self, num = 0, trunc = 1.0):\n        self.GAN.eval()\n        ext = self.image_extension\n        num_rows = self.num_image_tiles\n    \n        latent_dim = self.GAN.G.latent_dim\n        image_size = self.GAN.G.image_size\n        num_layers = self.GAN.G.num_layers\n\n        # latents and noise\n\n        latents = noise_list(num_rows ** 2, num_layers, latent_dim, device=self.rank)\n        n = image_noise(num_rows ** 2, image_size, device=self.rank)\n\n        # regular\n\n        generated_images = self.generate_truncated(self.GAN.S, self.GAN.G, latents, n, trunc_psi = self.trunc_psi)\n        torchvision.utils.save_image(generated_images, str(self.results_dir \/ self.name \/ f'{str(num)}.{ext}'), nrow=num_rows)\n        \n        # moving averages\n\n        generated_images = self.generate_truncated(self.GAN.SE, self.GAN.GE, latents, n, trunc_psi = self.trunc_psi)\n        torchvision.utils.save_image(generated_images, str(self.results_dir \/ self.name \/ f'{str(num)}-ema.{ext}'), nrow=num_rows)\n\n        # mixing regularities\n\n        def tile(a, dim, n_tile):\n            init_dim = a.size(dim)\n            repeat_idx = [1] * a.dim()\n            repeat_idx[dim] = n_tile\n            a = a.repeat(*(repeat_idx))\n            order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])).cuda(self.rank)\n            return torch.index_select(a, dim, order_index)\n\n        nn = noise(num_rows, latent_dim, device=self.rank)\n        tmp1 = tile(nn, 0, num_rows)\n        tmp2 = nn.repeat(num_rows, 1)\n\n        tt = int(num_layers \/ 2)\n        mixed_latents = [(tmp1, tt), (tmp2, num_layers - tt)]\n\n        generated_images = self.generate_truncated(self.GAN.SE, self.GAN.GE, mixed_latents, n, trunc_psi = self.trunc_psi)\n        torchvision.utils.save_image(generated_images, str(self.results_dir \/ self.name \/ f'{str(num)}-mr.{ext}'), nrow=num_rows)\n\n    @torch.no_grad()\n    def calculate_fid(self, num_batches):\n        from pytorch_fid import fid_score\n        latent_dim = self.GAN.G.latent_dim\n        image_size = self.GAN.G.image_size\n        num_layers = self.GAN.G.num_layers\n        torch.cuda.empty_cache()\n\n        real_path = self.fid_dir \/ 'real'\n        fake_path = self.fid_dir \/ 'fake'\n        \n        # remove any existing files used for fid calculation and recreate directories\n\n        rmtree(real_path, ignore_errors=True)\n        rmtree(fake_path, ignore_errors=True)\n        os.makedirs(real_path)\n        os.makedirs(fake_path)\n\n        for batch_num in tqdm(range(num_batches), desc='calculating FID - saving reals'):\n            real_batch, _, _, _ = next(self.loader)\n            for k, image in enumerate(real_batch.unbind(0)):\n                filename = str(k + batch_num * self.batch_size)\n                torchvision.utils.save_image(image, str(real_path \/ f'{filename}.png'))\n\n        # generate a bunch of fake images in results \/ name \/ fid_fake\n        self.GAN.eval()\n        ext = self.image_extension\n\n        latent_dim = self.GAN.G.latent_dim\n        image_size = self.GAN.G.image_size\n        num_layers = self.GAN.G.num_layers\n\n        for batch_num in tqdm(range(num_batches), desc='calculating FID - saving generated'):\n            images, image_cutout, mask_normalised, mask = next(self.loader)\n            images, image_cutout, mask_normalised, mask = images.cuda(), image_cutout.cuda(), mask_normalised.cuda(), mask.cuda()\n            generated_images = self.generate(images, image_cutout, mask_normalised, mask)\n            for j, image in enumerate(generated_images.unbind(0)):\n                torchvision.utils.save_image(image, str(fake_path \/ f'{str(j + batch_num * self.batch_size)}-ema.{ext}'))\n\n        score =  fid_score.calculate_fid_given_paths(\n            paths=[str(real_path), str(fake_path)], \n            batch_size=64, \n            device=\"cuda\", \n            dims=2048\n        )\n        rmtree(real_path, ignore_errors=True)\n        rmtree(fake_path, ignore_errors=True)\n        return score\n    \n    @torch.no_grad()\n    def generate(self, images, image_cut, mask_norm, mask):\n        batch_size = self.batch_size\n        image_size = self.GAN.G.image_size\n        latent_dim = self.GAN.G.latent_dim\n        num_layers = self.GAN.G.num_layers\n        \n        style = noise_list(batch_size, num_layers, latent_dim, device=self.rank)\n        noise = image_noise(batch_size, image_size, device=self.rank)\n        w_space = latent_to_w(self.GAN.S, style)\n        w_styles_mapping = styles_def_to_tensor(w_space)\n        \n        images_latent, w_styles_encoder, skips = self.GAN.E(image_cut, mask_norm)\n        generated_images = self.GAN.G(images_latent, skips, w_styles_mapping, w_styles_encoder, noise)\n        reconstruct_image(images, generated_images, mask)\n        return generated_images\n\n    @torch.no_grad()\n    def truncate_style(self, tensor, trunc_psi = 0.75):\n        S = self.GAN.S\n        batch_size = self.batch_size\n        latent_dim = self.GAN.G.latent_dim\n\n        if not exists(self.av):\n            z = noise(2000, latent_dim, device=self.rank)\n            samples = evaluate_in_chunks(batch_size, S, z).cpu().numpy()\n            self.av = np.mean(samples, axis = 0)\n            self.av = np.expand_dims(self.av, axis = 0)\n\n        av_torch = torch.from_numpy(self.av).cuda(self.rank)\n        tensor = trunc_psi * (tensor - av_torch) + av_torch\n        return tensor\n\n    @torch.no_grad()\n    def truncate_style_defs(self, w, trunc_psi = 0.75):\n        w_space = []\n        for tensor, num_layers in w:\n            tensor = self.truncate_style(tensor, trunc_psi = trunc_psi)            \n            w_space.append((tensor, num_layers))\n        return w_space\n\n    @torch.no_grad()\n    def generate_truncated(self, S, G, style, noi, trunc_psi = 0.75, num_image_tiles = 8):\n        w = map(lambda t: (S(t[0]), t[1]), style)\n        w_truncated = self.truncate_style_defs(w, trunc_psi = trunc_psi)\n        w_styles = styles_def_to_tensor(w_truncated)\n        generated_images = evaluate_in_chunks(self.batch_size, G, w_styles, noi)\n        return generated_images.clamp_(0., 1.)\n\n    @torch.no_grad()\n    def generate_interpolation(self, num = 0, num_image_tiles = 8, trunc = 1.0, num_steps = 100, save_frames = False):\n        self.GAN.eval()\n        ext = self.image_extension\n        num_rows = num_image_tiles\n\n        latent_dim = self.GAN.G.latent_dim\n        image_size = self.GAN.G.image_size\n        num_layers = self.GAN.G.num_layers\n\n        # latents and noise\n\n        latents_low = noise(num_rows ** 2, latent_dim, device=self.rank)\n        latents_high = noise(num_rows ** 2, latent_dim, device=self.rank)\n        n = image_noise(num_rows ** 2, image_size, device=self.rank)\n\n        ratios = torch.linspace(0., 8., num_steps)\n\n        frames = []\n        for ratio in tqdm(ratios):\n            interp_latents = slerp(ratio, latents_low, latents_high)\n            latents = [(interp_latents, num_layers)]\n            generated_images = self.generate_truncated(self.GAN.SE, self.GAN.GE, latents, n, trunc_psi = self.trunc_psi)\n            images_grid = torchvision.utils.make_grid(generated_images, nrow = num_rows)\n            pil_image = transforms.ToPILImage()(images_grid.cpu())\n            \n            if self.transparent:\n                background = Image.new(\"RGBA\", pil_image.size, (255, 255, 255))\n                pil_image = Image.alpha_composite(background, pil_image)\n                \n            frames.append(pil_image)\n\n        frames[0].save(str(self.results_dir \/ self.name \/ f'{str(num)}.gif'), save_all=True, append_images=frames[1:], duration=80, loop=0, optimize=True)\n\n        if save_frames:\n            folder_path = (self.results_dir \/ self.name \/ f'{str(num)}')\n            folder_path.mkdir(parents=True, exist_ok=True)\n            for ind, frame in enumerate(frames):\n                frame.save(str(folder_path \/ f'{str(ind)}.{ext}'))\n\n    def model_name(self, num):\n        return str(self.models_dir \/ self.name \/ f'model_{num}.pt')\n\n    def init_folders(self): \n        (self.results_dir \/ self.name).mkdir(parents=True, exist_ok=True)\n        (self.models_dir \/ self.name).mkdir(parents=True, exist_ok=True)\n\n    def clear(self):\n        rmtree(str(self.models_dir \/ self.name), True)\n        rmtree(str(self.results_dir \/ self.name), True)\n        rmtree(str(self.fid_dir), True)\n        rmtree(str(self.config_path), True)\n        self.init_folders()\n\n    def save(self, num):\n        save_data = {\n            'GAN': self.GAN.state_dict(),\n        }\n        torch.save(save_data, self.model_name(num))\n        self.write_config()\n\n    def load(self):\n        self.load_config()\n        \n        load_data = torch.load(\"..\/input\/stylegan-2\/models\/default\/model_3.pt\")\n        if 'version' in load_data:\n            print(f\"loading from version {load_data['version']}\")\n\n        try:\n            self.GAN.load_state_dict(load_data['GAN'])\n        except Exception as e:\n            print('unable to load save model. please try downgrading the package to the version specified by the saved model')\n            raise e","73189b9c":"def cast_list(el):\n    return el if isinstance(el, list) else [el]\n\ndef timestamped_filename(prefix = 'generated-'):\n    now = datetime.now()\n    timestamp = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n    return f'{prefix}{timestamp}'\n\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    random.seed(seed)\n\ndef run_training(rank, world_size, model_args, data, load_from, new, num_train_steps, name, seed):\n    is_main = rank == 0\n\n    model_args.update(\n        rank = rank,\n        world_size = world_size\n    )\n\n    model = Trainer(**model_args)\n\n    if not new:\n        model.load(load_from)\n    else:\n        model.clear()\n\n    model.set_data_src(data)\n\n    for step in tqdm(range(1, num_train_steps + 1)):\n        model.train()\n        # Outside of training training function to save memory\n        if exists(model.calculate_fid_every) and (step == 1 or step % model.calculate_fid_every == 0 and step != 0):\n            num_batches = math.ceil(model.calculate_fid_num_images \/ model.batch_size)\n            fid = model.calculate_fid(num_batches)\n            model.last_fid = fid\n            print(f\"FID: {fid}\")\n\n    model.save(model.checkpoint_num)\n\n\ndef train_from_folder(\n    data = '..\/input\/flickrfaceshq-dataset-ffhq',\n    results_dir = '.\/results',\n    models_dir = '.\/models',\n    name = 'default',\n    new =  True,\n    load_from = -1,\n    image_size = 256,\n    network_capacity = 16,\n    fmap_max = 512, \n    transparent = False,\n    batch_size = 11,\n    num_train_steps = 13000,\n    learning_rate = 2e-4,\n    lr_mlp = 0.1,\n    ttur_mult = 1.5,\n    num_workers =  None,\n    save_every = 1000,\n    evaluate_every = 1000,\n    generate = False,\n    num_generate = 1,\n    generate_interpolation = False,\n    interpolation_num_steps = 100,\n    save_frames = False,\n    num_image_tiles = 8,\n    trunc_psi = 0.75,\n    mixed_prob = 0.9,\n    no_pl_reg = False,\n    aug_types = ['translation', 'cutout'],\n    generator_top_k_gamma = 0.99,\n    generator_top_k_frac = 0.5,\n    calculate_fid_every = 1000,\n    calculate_fid_num_images = 1000,\n    clear_fid_cache = False,\n    seed = 42,\n):\n    model_args = dict(\n        name = name,\n        results_dir = results_dir,\n        models_dir = models_dir,\n        batch_size = batch_size,\n        image_size = image_size,\n        network_capacity = network_capacity,\n        fmap_max = fmap_max,\n        transparent = transparent,\n        lr = learning_rate,\n        lr_mlp = lr_mlp,\n        ttur_mult = ttur_mult,\n        num_workers = num_workers,\n        save_every = save_every,\n        evaluate_every = evaluate_every,\n        num_image_tiles = num_image_tiles,\n        trunc_psi = trunc_psi,\n        no_pl_reg = no_pl_reg,\n        aug_types = cast_list(aug_types),\n        generator_top_k_gamma = generator_top_k_gamma,\n        generator_top_k_frac = generator_top_k_frac,\n        calculate_fid_every = calculate_fid_every,\n        calculate_fid_num_images = calculate_fid_num_images,\n        clear_fid_cache = clear_fid_cache,\n        mixed_prob = mixed_prob,\n    )\n\n    if generate:\n        model = Trainer(**model_args)\n        model.load(load_from)\n        samples_name = timestamped_filename()\n        for num in tqdm(range(num_generate)):\n            model.evaluate(f'{samples_name}-{num}', num_image_tiles)\n        \n        print(f'sample images generated at {results_dir}\/{name}\/{samples_name}')\n        return\n\n    if generate_interpolation:\n        model = Trainer(**model_args)\n        model.load(load_from)\n        samples_name = timestamped_filename()\n        model.generate_interpolation(samples_name, num_image_tiles, num_steps = interpolation_num_steps, save_frames = save_frames)\n        print(f'interpolation generated at {results_dir}\/{name}\/{samples_name}')\n        return\n\n\n    run_training(0, 1, model_args, data, load_from, new, num_train_steps, name, seed)\n        \n\ntrain_from_folder()","e9ae8937":"## Models","d7886d06":"## Dataset"}}