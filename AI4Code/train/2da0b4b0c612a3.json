{"cell_type":{"9ef3562f":"code","d788c62b":"code","01032625":"code","16b3d68e":"code","b366a06f":"code","bcd7850b":"code","c1b27a41":"code","3215366a":"code","8d5e6fa9":"code","fbbd528c":"code","caeadd0f":"code","25ecd756":"code","c8fda493":"code","90a34d63":"code","f342c7ac":"code","e99a8ebb":"code","85e7857b":"code","a0946387":"code","a11faf35":"code","761556ca":"code","d28c9876":"code","047e3806":"code","47df1ee4":"code","e64f5720":"code","e22e83d0":"code","3307b5f9":"code","91f7e88d":"code","5b649a4e":"code","27842b19":"code","113b9b5e":"code","8bc95dd7":"code","8e955b73":"code","66ec8195":"code","ebf94ee0":"code","932947a7":"code","d3ce077d":"markdown","c5965d44":"markdown","8e22ca23":"markdown","f0ac6aa7":"markdown","b2f85185":"markdown","d81c4490":"markdown","549a59f9":"markdown","7169864a":"markdown","16e9243f":"markdown","62109c1e":"markdown","c1f1a28b":"markdown","3d1c7360":"markdown","f449e5af":"markdown","59585428":"markdown","c78ed9a7":"markdown","c4bad370":"markdown","e1ad2e82":"markdown","cc761a6b":"markdown","95031ff9":"markdown","c741de68":"markdown","6619e3a5":"markdown","0b6d0ec9":"markdown","98d0c5a0":"markdown","9d6d12f0":"markdown","3a8f416e":"markdown","e329ec35":"markdown","65580daf":"markdown","801fa15d":"markdown","973c28b5":"markdown","18e74e75":"markdown","a02866e9":"markdown","9bb64c59":"markdown","28577a9f":"markdown","e2239f00":"markdown","c5790499":"markdown","9176a7c6":"markdown","9e4188a6":"markdown","d3ab84dd":"markdown"},"source":{"9ef3562f":"#data analysis libraries \nimport numpy as np\nimport pandas as pd\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","d788c62b":"#import train and test CSV files\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n#take a look at the training data\ntrain.describe(include=\"all\")","01032625":"# save PassengerId for a future output \nids = test['PassengerId']","16b3d68e":"#get a list of the features within the dataset\nprint(train.columns)","b366a06f":"#see a sample of the dataset to get an idea of the variables\ntrain.sample(5)","bcd7850b":"#see a summary of the training dataset\ntrain.describe(include = \"all\")","c1b27a41":"#check for any other unusable values\nprint(pd.isnull(train).sum())","3215366a":"features = ['Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']\nfig, saxis = plt.subplots(1, len(features),figsize=(len(features) * 6,6))\nfor ind, x in enumerate(features):\n    print('Survival Correlation by:', x)\n    print(train[[x, \"Survived\"]].groupby(x, as_index=False).mean()) \n    print('-'*10, '\\n')\n    #draw a bar plot of survival by sex\n    sns.barplot(x, y=\"Survived\", data=train, ax = saxis[ind])\n    ","8d5e6fa9":"#sort the ages into logical categories\nbins = [0, 2, 12, 17, 60, np.inf]\nlabels = ['baby', 'child', 'teenager', 'adult', 'elderly']\nage_groups = pd.cut(train.Age, bins, labels = labels)\ntrain['AgeGroup'] = age_groups\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train)\nplt.show()","fbbd528c":"train[\"CabinBool\"] = (train[\"Cabin\"].notnull().astype('int'))\ntest[\"CabinBool\"] = (test[\"Cabin\"].notnull().astype('int'))\n\n#calculate percentages of CabinBool vs. survived\n\nprint('Survival Correlation by: Cabin')\nprint(train[[\"CabinBool\", \"Survived\"]].groupby(\"CabinBool\", as_index=False).mean()) \n\n#draw a bar plot of CabinBool vs. survival\nsns.barplot(x=\"CabinBool\", y=\"Survived\", data=train)\nplt.show()","caeadd0f":"test.describe(include=\"all\")","25ecd756":"# all_data = pd.concat([train.drop(columns='Survived'), test], ignore_index=True)\nall_data = pd.concat([train, test], ignore_index=True)\nprint(all_data.shape)","c8fda493":"#complete embarked with mode\nall_data['Embarked'].fillna(all_data['Embarked'].mode()[0], inplace = True)\n\n#complete missing fare with median\nall_data['Fare'].fillna(all_data['Fare'].median(), inplace = True)\n","90a34d63":"#extract a title for each Name \nall_data['Title'] = all_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\nall_data['Title'].value_counts()","f342c7ac":"frequent_titles = all_data['Title'].value_counts()[:5].index.tolist()\nfrequent_titles","e99a8ebb":"# keep only the most frequent titles\nall_data['Title'] = all_data['Title'].apply(lambda x: x if x in frequent_titles else 'Other')\n# all_data.head()","85e7857b":"# fill missing age with median age group for each title\nmedian_ages = {}\n# calculate median age for different titles\nfor title in frequent_titles:\n    median_ages[title] = all_data.loc[all_data['Title'] == title]['Age'].median()\nmedian_ages['Other'] =  all_data['Age'].median()\nall_data.loc[all_data['Age'].isnull(), 'Age'] = all_data[all_data['Age'].isnull()]['Title'].map(median_ages)","a0946387":"all_data.head()","a11faf35":"from sklearn.preprocessing import LabelEncoder\n\nCat_Features = ['Sex', 'Embarked', 'Title']\nfor feature in Cat_Features:\n    label = LabelEncoder()\n    all_data[feature] = label.fit_transform(all_data[feature])","761556ca":"Cont_Features = ['Age', 'Fare']\nnum_bins = 5\nfor feature in Cont_Features:\n    bin_feature = feature + 'Bin'\n    all_data[bin_feature] = pd.qcut(all_data[feature], num_bins)\n    label = LabelEncoder()\n    all_data[bin_feature] = label.fit_transform(all_data[bin_feature])","d28c9876":"all_data.head()","047e3806":"all_data['Surname'] = all_data.Name.str.extract(r'([A-Za-z]+),', expand=False)\nall_data['TicketPrefix'] = all_data.Ticket.str.extract(r'(.*\\d)', expand=False)\nall_data['Surname_Ticket'] = all_data['Surname'] + all_data['TicketPrefix']\nall_data['IsFamily'] = all_data.Surname_Ticket.duplicated(keep=False).astype(int)","47df1ee4":"all_data['Child'] = all_data.Age.map(lambda x: 1 if x <=16 else 0)\nFamilyWithChild = all_data[(all_data.IsFamily==1)&(all_data.Child==1)]['Surname_Ticket'].unique()\nlen(FamilyWithChild)","e64f5720":"all_data['FamilyId'] = 0\nfor ind, identifier in enumerate(FamilyWithChild):\n all_data.loc[all_data.Surname_Ticket==identifier, ['FamilyId']] = ind + 1","e22e83d0":"all_data['FamilySurvival'] = 1 \nSurvived_by_FamilyId = all_data.groupby('FamilyId').Survived.sum()\nfor i in range(1, len(FamilyWithChild)+1):\n   if Survived_by_FamilyId[i] >= 1:\n      all_data.loc[all_data.FamilyId==i, ['FamilySurvival']] = 2\n   elif Survived_by_FamilyId[i] == 0:\n      all_data.loc[all_data.FamilyId==i, ['FamilySurvival']] = 0\nsns.barplot(x='FamilySurvival', y='Survived', data=all_data)\nplt.show()","3307b5f9":"train = all_data[: len(train)]\ntest = all_data[len(train):]\ntrain.shape","91f7e88d":"train.columns","5b649a4e":"# keep only some columns\nX_train = train[['Pclass', 'Sex', 'Parch', 'Embarked', 'CabinBool', 'Title', 'AgeBin', 'FareBin', 'FamilySurvival']]\ny_train = train['Survived']","27842b19":"# we start with this very powerful classifier \nfrom catboost import CatBoostClassifier\n\nmodel = CatBoostClassifier(verbose=False)","113b9b5e":"model.fit(X_train,y_train)\nimportance = pd.DataFrame({'feature':X_train.columns, 'importance': model.feature_importances_})\nimportance.sort_values('importance', ascending=False).set_index('feature').plot(kind='barh')\nplt.show()","8bc95dd7":"X_train.columns","8e955b73":"main_features = ['Sex', 'FamilySurvival', 'FareBin', 'Pclass', 'Title']\n\n\nX_test = test[main_features]\nX_train = train[main_features]","66ec8195":"from sklearn.model_selection import cross_val_score\n\ncross_val_score(estimator=model, X=X_train, y=y_train, cv=5).mean()","ebf94ee0":"from sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn import svm, neighbors\nfrom xgboost import XGBClassifier\n\nensemble = [CatBoostClassifier(verbose=False), RandomForestClassifier(), svm.NuSVC(probability=True), neighbors.KNeighborsClassifier()]\n\nclassifiers_with_names = []\n_ = [classifiers_with_names.append((clf.__class__.__name__, clf)) for clf in ensemble]\nvoting = VotingClassifier(classifiers_with_names, voting='hard')\n\ncv_results = cross_validate(voting, X_train, y_train, cv=5)\nprint(cv_results['test_score'].mean())\n\nvoting.fit(X_train, y_train)\npredictions = voting.predict(X_test)\n","932947a7":"output = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions.astype(int)})\noutput.to_csv('submission_new_session.csv', index=False)","d3ce077d":"<a id=\"ch4\"><\/a>\n## 4. Cleaning and Transforming the Data\nTime to clean our data to account for missing values and unnecessary information!","c5965d44":"#### Age Feature","8e22ca23":"Identifying the most important features","f0ac6aa7":"Choose the most important features based on their importance and the cross validation score ","b2f85185":"### Creating frequency bins for continuous variables and encoding\nUse qcut and LabelEncoder for continuous variable bins","d81c4490":"People with a recorded Cabin number are, in fact, more likely to survive. (66.6% vs 29.9%)","549a59f9":"### Content:\n\n[1. Import Libraries](#ch1)\n\n[2. Read In and Explore the Data](#ch2)\n\n[3. Data Analysis and Visualisation](#ch3)\n\n[4. Cleaning and Transforming the Data](#ch4)\n\n[5. Feature Creation](#ch5)\n\n[6. Model and Feature Selection](#ch6)\n\n[7. Creating Submission File](#ch7)\n","7169864a":"* We have a total of 418 passengers.\n* 1 value from the Fare feature is missing.\n* Around 20.5% of the Age feature is missing, we will need to fill that in.","16e9243f":"<a id=\"ch3\"><\/a>\n## 3. Data Analysis and Visualisation\nIt's time to analize our data so we can see whether our predictions were accurate! ","62109c1e":"### Some Predictions:\n* Sex: Females are more likely to survive.\n* SibSp\/Parch: People traveling alone are more likely to survive.\n* Age: Young children are more likely to survive.\n* Pclass: People of higher socioeconomic class are more likely to survive.","c1f1a28b":"### Looking at the Test Data\nLet's see how our test data looks!","3d1c7360":"### Splitting back Train and Test data","f449e5af":"<a id=\"ch5\"><\/a>\n## 5. Feature Creation\n\nWe are going create one additional feature, which can imrove the model. As it was observed in many other notebooks the members of families with children have higer probability to survive.","59585428":"<a id=ch7><\/a>\n## 7. Creating Submission File","c78ed9a7":"Next we'll fill in the missing values in the Age feature. Since a higher percentage of values are missing, it would be illogical to fill all of them with the same value (as we did with Embarked). Instead, let's try to find a way to predict the missing ages. ","c4bad370":"### Cabin Feature\nThe idea here is that people with recorded cabin numbers are of higher socioeconomic class, and thus more likely to survive.","e1ad2e82":"<a id=\"ch2\"><\/a>\n## 2. Read in and Explore the Data  <div id=\"part2\"> <\/div>\nIt's time to read in our training and testing data using `pd.read_csv`, and take a first look at the training data using the `describe()` function.","cc761a6b":"### Filling simple missing features","95031ff9":"For each family of above, if there is at least one survived, we assume the others can survive too.","c741de68":"First, we are going to identify families. \n\nIt appears that passengers with same surnames have the same Ticket names.\nLet\u2019s extract the surnames and tickets name and find out duplicate ones. There may be passengers from the same families.","6619e3a5":"## Sources:\n* [**Titanic Survival Predictions (Beginner)**](https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner)\n* [**A Data Science Framework: To Achieve 99% Accuracy**](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy)\u00a0\n* [Titanic Survival Prediction](https:\/\/www.kaggle.com\/vaishnavikhilari\/titanic-survival-prediction)\n\nYour feedback is very welcome! ","0b6d0ec9":"<a id=ch6><\/a>\n## 6. Model and Feature Selection","98d0c5a0":"### Combining Training and Test data for cleaning and transforming","9d6d12f0":"### Voting Classifier","3a8f416e":"As predicted, \n* females have a much higher chance of survival than males. The Sex feature is essential in our predictions\n* people with higher socioeconomic class had a higher rate of survival. (62.9% vs. 47.3% vs. 24.2%)\n\nIn general, it's clear that people with more siblings or spouses aboard were less likely to survive. However, contrary to expectations, people with no siblings or spouses were less to likely to survive than those with one or two. (34.5% vs 53.4% vs. 46.4%)\n\nPeople with less than four parents or children aboard are more likely to survive than those with four or more. Again, people traveling alone are less likely to survive than those with 1-3 parents or children.","e329ec35":"### Encoding categorical features with non-numerical values\nUse LabelEncoder for categorical features","65580daf":"There are 66 families which have 1 or more children.\n\nEncode each family with children and assign 0 for others.","801fa15d":"#### Some Observations:\n* There are a total of 891 passengers in our training set.\n* The Age feature is missing approximately 19.8% of its values. I'm guessing that the Age feature is pretty important to survival, so we should probably attempt to fill these gaps. \n* The Cabin feature is missing approximately 77.1% of its values. Since so much of the feature is missing, it would be hard to fill in the missing values. We'll probably drop these values from our dataset.\n* The Embarked feature is missing 0.22% of its values, which should be relatively harmless.","973c28b5":"Indeed, we can see that chances to survive are higher for passagiers in families.","18e74e75":"We can see that except for the abovementioned missing values, no NaN values exist.","a02866e9":"<a id=\"ch1\"><\/a>\n## 1. Import Libraries\nFirst, we need to import several Python libraries such as numpy, pandas, matplotlib and seaborn.","9bb64c59":"### Features to keep in train data \nWe can drop now some original features that we don't need or that we have used already to create new features. ","28577a9f":"Next, we find the families with children.","e2239f00":"### Filling missing features using other features","c5790499":"# Titanic: simple ensemble voting (Top 3%)\n\nThis notebook is a successful attempt to improve the result of my previous notebook:\n\n\n#### [Titanic: simple voting based on cross-validation](https:\/\/www.kaggle.com\/alexanderossipov\/titanic-simple-voting-based-on-cross-validation)\n\n\n### List of main changes:\n\n1. One additional\u00a0feature is introduced to the model and only the most important features are selected.\n2. The number of models is reduced to 4.\n3. An ensemble\u00a0is constructed in the standard way using\u00a0VotingClassifier.\u00a0 \n\nIt's amazing that such a good result can be achieved\u00a0without any hyperparameter tuning. If you will be able to improve\u00a0it further by\u00a0fine tuning the parameters, please\u00a0 let me know!\u00a0\u00a0\n\n\n**Your feedback is very welcome!**","9176a7c6":"### Age Feature","9e4188a6":"Create an ensemble of models showing the best performance in the previous notebook.","d3ab84dd":"The survival probability deacreases with age. "}}