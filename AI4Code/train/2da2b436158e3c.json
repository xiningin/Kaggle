{"cell_type":{"63d585b3":"code","6bda8005":"code","b874cb0b":"code","2bc02118":"code","81474e1e":"code","b8fa9071":"code","df0d9c3f":"code","d67932b0":"code","efb5fcf8":"code","ddf180c7":"code","b16bbff4":"markdown","2884ea52":"markdown","5655ac2a":"markdown","cb845c57":"markdown","299c786a":"markdown","c7375fe5":"markdown","87810ba8":"markdown"},"source":{"63d585b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, f_classif, chi2\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6bda8005":"df = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\n\nbins = (2, 6.5, 8)\ngroup_names = [0, 1]\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)\n\ndf.head()","b874cb0b":"df.describe()","2bc02118":"plt.figure(figsize=(12,10))\nax = sns.heatmap(df.corr(), annot=True)","81474e1e":"features = df.iloc[:,:-1]\nsns.pairplot(features)\nplt.show()","b8fa9071":"x = df.quality.value_counts()\nsns.barplot(['Bad','Good'],x.values)\nplt.show()","df0d9c3f":"X = df.iloc[:,:-1].values\ny = df.iloc[:,-1:].values\n\nscaler = StandardScaler()\nX_scaled = scaler.fit(X).transform(X)","d67932b0":"## Using ANOVA test for feature selection\nfeature_selector = SelectKBest(f_classif, k=6)\nX_scaled = feature_selector.fit_transform(X_scaled, y.flatten())\n\nbest_features = feature_selector.get_support()\nprint(\"Best Features: {}\".format(list(df.iloc[:,:-1].iloc[:,best_features].columns)))","efb5fcf8":"X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)\n\nprint(\"X_train Shape: {}\".format(X_train.shape))\nprint(\"y_train Shape: {}\".format(Y_train.shape))\nprint(\"X_test Shape: {}\".format(X_test.shape))\nprint(\"y_test Shape: {}\".format(Y_test.shape))","ddf180c7":"clf = KNeighborsClassifier()\nclf.fit(X_train,Y_train.flatten())\ny_expect = Y_test.flatten()\ny_pred = clf.predict(X_test)\n\nprint(\"Report:\")\nprint(metrics.classification_report(y_expect, y_pred, zero_division=1))","b16bbff4":"### 4. KNeighbors Classifier","2884ea52":"## Correlation Between Features","5655ac2a":"# Baseline Model","cb845c57":"## Class Distribution","299c786a":"### 1. Standardizing Data","c7375fe5":"### 2. Selecting Best Features","87810ba8":"### 3. Train test split"}}