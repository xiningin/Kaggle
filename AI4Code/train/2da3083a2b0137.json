{"cell_type":{"bc3f169b":"code","df407318":"code","d4332f89":"code","e3013e8c":"code","acc3e348":"code","ef5fe92a":"code","ed0d3d41":"code","49565f5e":"code","d8930779":"code","5da144ab":"code","fa44b03a":"code","0a95dbd3":"code","083d8706":"code","edddeb78":"code","64b818a6":"code","b85517b9":"code","c1265eff":"code","f6dfee87":"code","06f21dff":"code","6f7afaad":"code","fd38367b":"code","01d6c6fa":"code","8cde6c58":"code","cf9ab17c":"code","8d778185":"code","5a539238":"code","c58b338f":"code","08f4e81b":"code","0881cc4d":"code","4c6d56bc":"code","a9fbc7cf":"code","892b5c16":"markdown","1b067a19":"markdown","e29d78eb":"markdown","38b1b927":"markdown","296d74f5":"markdown"},"source":{"bc3f169b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","df407318":"#importing required library\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import  LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import make_classification","d4332f89":"#reading the  train data\ndf_train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_train.head()","e3013e8c":"#read the test data\ndf_test=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n","acc3e348":"#checking the number of nan values\ndf_train.isnull().sum()","ef5fe92a":"#understanding outlairs\ndf_train.plot(kind ='box', subplots = True, layout =(3, 3),  \n                       sharex = False, sharey = False,figsize=(20,20)) ","ed0d3d41":"#understanding no of survived persons\nsns.countplot(x='Survived', data=df_train);\n","49565f5e":"#plotting between sex and survived\nsns.factorplot(x='Survived', col='Sex', kind='count', data=df_train)","d8930779":"#understanding the no; male and female survived\ndf_train.groupby(['Sex']).Survived.sum()","5da144ab":"#plotting between pclass and survived\nsns.factorplot(x='Survived', col='Pclass', kind='count', data=df_train);","fa44b03a":"#plotting between embarked and survived\nsns.factorplot(x='Survived', col='Embarked', kind='count', data=df_train);","0a95dbd3":"#understanding the correlation\nplt.figure(figsize=(12, 8))\nsns.heatmap(df_train.corr(), cmap=plt.cm.RdBu, annot=True)","083d8706":"#checking the unique values in the sex column\nnp.unique(df_train.Sex)\n#repalce the string value of sex with a numerical value\na= [1,2]\nb=['female', 'male']\ndf_train=df_train.replace(b,a)\ndf_train.head()","edddeb78":"#replace the catogorical value with numerical\ndf_test=df_test.replace(b,a)\ndf_train = df_train[pd.notnull(df_train['Embarked'])]\n#undersanding unique values\nnp.unique(df_train.Embarked)\n#applying label encoding\nlabel = LabelEncoder()\n\ndf_train['Embarked']= label.fit_transform(df_train['Embarked']) \n\n    \n","64b818a6":"#replace nan value with mean of the feature\ndf_train['Age']=df_train['Age'].fillna(df_train['Age'].mean()) \n#understanding nan value\ndf_train.isnull().sum()\n","b85517b9":"x_train=df_train[['Pclass','Sex','Age','SibSp','Parch',\"Embarked\"]]\ny_train=df_train['Survived']\ndf_test.isnull().sum()","c1265eff":"#replace nan value with mean of the feature\ndf_test['Age']=df_test['Age'].fillna(df_test['Age'].mean()) \n#replace nan value with mean of the feature\ndf_train['Fare']=df_train['Fare'].fillna(df_train['Fare'].mean()) \nlabel = LabelEncoder()\n\ndf_test['Embarked']= label.fit_transform(df_test['Embarked']) ","f6dfee87":"x_test=df_test[['Pclass','Sex','Age','SibSp','Parch','Embarked']]\nscaler = MinMaxScaler()\nscaler.fit(x_train)","06f21dff":"gbrt = GradientBoostingClassifier()\nlearning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\nparam_grid = dict(learning_rate=learning_rate)\ngbrt1 = GridSearchCV(gbrt, param_grid)\ngbrt1.fit(x_train,y_train)\n","6f7afaad":"gbrt2=GradientBoostingClassifier(learning_rate= 0.1)\nscores = cross_val_score(gbrt2, x_train, y_train, cv=5)\nprint(scores.mean() )","fd38367b":"ada=AdaBoostClassifier()\nlearning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\nparam_grid = dict(learning_rate=learning_rate)\nada1 = GridSearchCV(ada, param_grid)\nada1.fit(x_train, y_train)\nada1.best_params_\n","01d6c6fa":"ada2=AdaBoostClassifier(learning_rate= 0.1)\nscores0 = cross_val_score(ada2, x_train, y_train, cv=5)\nprint(scores0.mean() ) ","8cde6c58":"lg=LogisticRegression()\npenalty = ['l1', 'l2']\n\n# Create regularization hyperparameter space\nC = np.logspace(0, 4, 10)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)\nlg1= GridSearchCV(lg, hyperparameters, cv=5, verbose=0)\n\nlg1.fit(x_train, y_train)  \nlg1.best_params_","cf9ab17c":"lg2=LogisticRegression(C=1,penalty='l2')\nscores1 = cross_val_score(lg2, x_train, y_train, cv=5)\nprint(scores1.mean() )             ","8d778185":"param_gridc = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  \n  \ngrid = GridSearchCV(SVC(), param_gridc, refit = True, verbose = 3) \n\ngrid.fit(x_train, y_train)  \ngrid.best_params_","5a539238":"grid1=GridSearchCV(SVC(C=1000,gamma=.001,kernel='rbf'), param_gridc, refit = True, verbose = 3)\ngrid1.fit(x_train, y_train)\nscore2=cross_val_score(grid1, x_train, y_train, cv=5)\nprint(score2.mean())","c58b338f":"rf=RandomForestClassifier(random_state=42)\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nCV_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv= 5)\nCV_rf.fit(x_train, y_train)\nCV_rf.best_params_\n","08f4e81b":"rf1=RandomForestClassifier(random_state=42, max_features= 'auto',n_estimators= 200,max_depth= 4,criterion= 'entropy')\nrf1.fit(x_train, y_train)\nscore3=cross_val_score(rf1, x_train, y_train, cv=5)\nprint(score3.mean())","0881cc4d":"#here random forest is more accurate\ny_pred=rf1.predict(x_test)","4c6d56bc":"gender_submission = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Survived':y_pred})\n\ngender_submission.to_csv('gender_submission.csv')","a9fbc7cf":"gender_submission.head(5)","892b5c16":"logistic regression with tuning\n****","1b067a19":"adaboost with tuning******","e29d78eb":"random forest with tuning","38b1b927":"Gradient boosting with tuning****","296d74f5":"svm with tuning"}}