{"cell_type":{"78e62e23":"code","2d709daa":"code","c521641d":"code","41afadd0":"code","b92559b4":"code","159d5d95":"code","674fed76":"code","fe31a24e":"code","91d96ae6":"code","0e42c34d":"code","eaec162d":"code","31b9868b":"code","a133984d":"code","bf5af5fe":"code","7bf4b381":"code","0d34d37f":"code","5d5e282c":"code","ae266222":"code","beb1ce8b":"code","22b60aab":"markdown","798a5f42":"markdown","6617d52b":"markdown","4f3de984":"markdown","34cbd422":"markdown","7eb055dd":"markdown","a6f5b2e2":"markdown","bb05f553":"markdown","dbddacb8":"markdown","7f8661c5":"markdown","462d27ca":"markdown","149bbdb3":"markdown","565fe4a1":"markdown","f7db8d57":"markdown","62c8fbb1":"markdown","4dede7a6":"markdown","2177e288":"markdown","e4d7fca2":"markdown","146b5081":"markdown","9d3f404b":"markdown"},"source":{"78e62e23":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2d709daa":"data_2C = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")\ndata_2C.info()\n\n#data_3C = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_3C_weka.csv\")\n#data_3C.info()","c521641d":"print(data_2C.columns)\nprint(data_2C[\"class\"].unique())\ndata_2C[\"class\"] = [1 if (i==\"Abnormal\" or i==1) else 0 if (i==\"Normal\" or i==0) else None for i in data_2C[\"class\"]]","41afadd0":"x = data_2C.drop([\"class\"], axis=1)\ny = data_2C[\"class\"].values","b92559b4":"x = (x - np.min(x))\/(np.max(x) - np.min(x))\nprint(x.head())","159d5d95":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=4)\n","674fed76":"result_dictionary = {}","fe31a24e":"from sklearn.neighbors import KNeighborsClassifier\n\nn = 11 # neighbor number\n\nknn = KNeighborsClassifier(n_neighbors=n)\nknn.fit(x_train,y_train)\n\nknn_score = knn.score(x_test,y_test)\n\nresult_dictionary[\"KNearestNeighbor\"] = knn_score\n\nprint(\"KNN score for n={} is => {:.1f}%\".format(n, 100*knn_score))","91d96ae6":"neighbor_number_list = list(range(1,30))\nscore_list = []\n\nfor n in neighbor_number_list:\n    knn_new = KNeighborsClassifier(n_neighbors=n)\n    knn_new.fit(x_train, y_train)\n    score_list.append(knn_new.score(x_test,y_test))\n    \nhighest_score = max(score_list)\nhighest_score_index = score_list.index(highest_score)\n\nbest_neighbor_number = neighbor_number_list[highest_score_index]\nprint(\"Best value for neighbor number n is => {}\".format(best_neighbor_number))","0e42c34d":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(10,6))\n\nplt.plot(neighbor_number_list, score_list, color=\"black\", lw=1.2, label=\"Score Graph\")\n\nplt.axhline(y=highest_score, color=\"red\", ls=\"--\", lw=1, label=\"Highest Score ({:.2f})\".format(highest_score))\nplt.axvline(x=best_neighbor_number, color=\"blue\", ls=\"--\", lw=1, label=\"Best n ({})\".format(best_neighbor_number))\n\nplt.xlabel(\"Neighbor Number\")\nplt.ylabel(\"Fit Score\")\n\nplt.legend()\nplt.grid(True)\nplt.show()","eaec162d":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(x_train, y_train)\n\nlr_score = lr.score(x_test,y_test)\n\nresult_dictionary[\"LogisticRegression\"] = lr_score\n\nprint(\"Logistic Regression Test Score: {:.1f} %\".format(100*lr_score))","31b9868b":"from sklearn.svm import SVC\n\nsvm = SVC(random_state=1)\nsvm.fit(x_train,y_train)\n\nsvm_score = svm.score(x_test,y_test)\n\nresult_dictionary[\"SupportVectorMachine\"] = svm_score\n\nprint(\"Support Vector Machine Test Score: {:.1f} %\".format(100*svm_score))","a133984d":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\nnb_score = nb.score(x_test,y_test)\n\nresult_dictionary[\"NaiveBayes\"] = nb_score\n\nprint(\"Naive Bayes Test Score: {:.1f} %\".format(100*nb_score))","bf5af5fe":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\ndt_score = dt.score(x_test,y_test)\n\nresult_dictionary[\"DecisionTree\"] = dt_score\n\nprint(\"Decision Tree Test Score: {:.1f} %\".format(100*dt_score))","7bf4b381":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=5, random_state=4)\nrf.fit(x_train, y_train)\n\nrf_score = rf.score(x_test,y_test)\n\nresult_dictionary[\"RandomForest\"] = rf_score\n\nprint(\"Decision Tree Test Score: {:.1f} %\".format(100*rf_score))","0d34d37f":"estimator_number_list = list(range(1,21))\nscore_list = []\n\nfor n in estimator_number_list:\n    rf_new = RandomForestClassifier(n_estimators=n, random_state=4)\n    rf_new.fit(x_train, y_train)\n    score_list.append(rf_new.score(x_test,y_test))\n    \nhighest_score = max(score_list)\nhighest_score_index = score_list.index(highest_score)\n\nbest_estimator_number = estimator_number_list[highest_score_index]\nprint(\"Best value for estimator number n is => {}\".format(best_estimator_number))","5d5e282c":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(10,6))\n\nplt.plot(estimator_number_list, score_list, color=\"black\", lw=1.2, label=\"Score Graph\")\n\nplt.axhline(y=highest_score, color=\"red\", ls=\"--\", lw=1, label=\"Highest Score ({:.2f})\".format(highest_score))\nplt.axvline(x=best_estimator_number, color=\"blue\", ls=\"--\", lw=1, label=\"Best n ({})\".format(best_estimator_number))\n\nplt.xlabel(\"Estimator Number\")\nplt.ylabel(\"Fit Score\")\n\nplt.legend()\nplt.grid(True)\nplt.show()","ae266222":"keys,values = zip(*result_dictionary.items())\n\nbest_score = max(values)\nbest_model = \"\"\n\nfor k,v in result_dictionary.items():\n    print(\"Fit Score of {}: {:.3f}\".format(k,v))\n    if v==best_score:\n        best_model = k\n        \nprint(\"#\"*45)\nprint(\"Best Model: {}\".format(best_model))\nprint(\"Fit Score of Best Model: {:.5f}\".format(best_score))\nprint(\"#\"*45)","beb1ce8b":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ny_predict = rf.predict(x_test)\ny_true = y_test\n\ncm = confusion_matrix(y_true, y_predict)\n\nf, ax = plt.subplots(figsize=(7,7))\nsns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n\nplt.xlabel(\"y_predict\")\nplt.ylabel(\"y_true\")\n\nplt.title(\"Confusion Matrix\")\nplt.show()","22b60aab":"# Naive Bayes","798a5f42":"Splitting the data into train and test sets","6617d52b":"# Comparing Different Algorithms\n\nI will create an empty dictionary and add the results of each algoritm to make a comparison.","4f3de984":"# Decision Tree","34cbd422":"\"x\" will be input parameters, \"y\" will be result values.\nSo \"class\" column should be in y, the rest should be x.","7eb055dd":"# Confusion Matrix","a6f5b2e2":"Finding the best neigbor number by trying for different values and comparing the fit scores.","bb05f553":"Drawing a plot for \"n\" vs \"score\"","dbddacb8":"# Results and Comparison","7f8661c5":"Drawing a plot for \"n\" vs \"score\"","462d27ca":"Parameter values in the x should be normalised.\n(differ between 0 and 1)\n","149bbdb3":"# Supervised Learning Practice\n\nThis notebook is a practice about some supervised learning algorithms.","565fe4a1":"# Logistic Regression","f7db8d57":"# Support Vector Machine","62c8fbb1":"# Data Preparation","4dede7a6":"There are two files in this data set. I will use \"column_2C_weka.csv\" first.\n\n(Size of the dataset is small. I will find a better dataset to improve my practice later.)","2177e288":"Finding the best estimator number by trying for different values and comparing the fit scores.","e4d7fca2":"# **KNN Classification**","146b5081":"\"class\" column will be our classifier.\nThere are two different results in it: \"Abnormal\" and \"Normal\"\nI will change the into numeric values: \"Abnormal\" = 1 and \"Normal\" = 0","9d3f404b":"# Random Forest"}}