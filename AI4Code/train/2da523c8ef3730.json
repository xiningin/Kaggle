{"cell_type":{"59bd3b86":"code","d35b5088":"code","6c4be84e":"code","86849bc0":"code","bad9833d":"code","dd9de672":"code","8cfc1d59":"code","0230a722":"code","1c7374e3":"code","2cfbe902":"code","3320b2f8":"code","8f91884f":"code","e2b756bb":"code","5608d1e0":"code","0ae495bd":"code","401e1819":"code","f8ac28b9":"markdown","d6367bb5":"markdown"},"source":{"59bd3b86":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.spatial import distance\nfrom nltk.tokenize import sent_tokenize\nimport scipy.spatial\n\n!pip install -U sentence-transformers\n\n# Library from here: https:\/\/github.com\/UKPLab\/sentence-transformers\nfrom sentence_transformers import SentenceTransformer","d35b5088":"CLEAN_DATA_PATH = \"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/\"\n\nbiorxiv_df = pd.read_csv(CLEAN_DATA_PATH + \"biorxiv_clean.csv\")\nclean_pmc = pd.read_csv(CLEAN_DATA_PATH + \"clean_pmc.csv\")\npapers_df = pd.concat([clean_pmc, biorxiv_df], axis=0).reset_index(drop=True)\n\npapers_df.dropna(inplace=True)\npapers_df.drop_duplicates(subset=['title'], keep=False, inplace=True)\n\n# Load Sentence Embedding Model.\nmodel = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')","6c4be84e":"papers_df.head()","86849bc0":"sentences = sent_tokenize(papers_df.iloc[0]['text'])\nsentences_df = pd.DataFrame({'id':np.zeros(len(sentences)).astype(int), 'sentences':sentences},index=None)\n\nfor i in range(1, len(papers_df)):\n    paper_sentences = sent_tokenize(papers_df.iloc[i]['text'])\n    paper_sentences_df = pd.DataFrame({'id':(np.ones(len(paper_sentences))*i).astype(int), 'sentences':paper_sentences},index=None)\n    sentences_df = pd.concat([sentences_df, paper_sentences_df], axis=0).reset_index(drop=True)\n    \nsentences = sentences_df['sentences'].str.lower().tolist()\nsentence_embeddings = model.encode(sentences)","bad9833d":"question_1 = 'medical health care covid-19 coronavirus'\nquestion_1_embedding = model.encode(question_1)\n\nquestion_sentence_similarity_scores = []\nfor i in range(len(sentence_embeddings)):\n    question_sentence_similarity_scores.append(scipy.spatial.distance.cdist([question_1_embedding[0]], [sentence_embeddings[i]], \"cosine\")[0])\n    \nsentences_df['cosine_score'] = question_sentence_similarity_scores\nsentences_df.head()","dd9de672":"for index, row in sentences_df[sentences_df['cosine_score'] > 1.3].iterrows():\n    print(f\"ID: {index}\\nSentence: {row['sentences']}\", '\\n')","8cfc1d59":"# Load Data and Library\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom subprocess import check_output\n! pip install wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\n\npapers_df=papers_df.reset_index(drop=True)\n\nMETA_DATA_PATH = \"..\/input\/CORD-19-research-challenge\/\"\nmeta_df = pd.read_csv(META_DATA_PATH + \"metadata.csv\")\n\n","0230a722":"# Data Source\n\ndf_sy=meta_df['source_x'].value_counts(dropna=False)\nprint(df_sy)\n\nplt.barh(df_sy.index, df_sy.values, align='center', alpha=0.5,color='orange')\nplt.ylabel('Quantity')\nplt.title('Number of Literature by Sources')\n\nplt.show()\n\n","1c7374e3":"# Literature Text\n\npapers_df.shape\n\nmeta_df.shape\n\npapers_df.shape[0]\/meta_df.shape[0]\n\ndf_sy=meta_df['has_pmc_xml_parse'].value_counts(dropna=False)\nprint(df_sy)\n\nplt.pie(df_sy.values,labels=df_sy.index,autopct='%1.1f%%',colors=['lightblue','orange'], shadow = True)\nplt.title('Whether have PMC XML')\nplt.axis('equal')\nplt.show()\n\n\ndf_sy=meta_df['has_pdf_parse'].value_counts(dropna=False)\nprint(df_sy)\n\nplt.pie(df_sy.values,labels=df_sy.index,autopct='%1.1f%%',colors=['orange','lightblue'], shadow = True)\nplt.title('Whether have PDF')\nplt.axis('equal')\nplt.show()\n\n\ndf_sy=meta_df['full_text_file'].value_counts(dropna=False)\nprint(df_sy)\ndf_sy.index = pd.Series(df_sy.index).replace(np.nan, 'NaN')\n\n\nplt.pie(df_sy.values,labels=df_sy.index,autopct='%1.1f%%',shadow = True)\nplt.title('Full Text Source File')\nplt.axis('equal')\nplt.show()\n\nplt.barh(df_sy.index, df_sy.values, align='center', alpha=0.5,color='orange')\nplt.ylabel('Quantity')\nplt.title('Number of Literatures with Full Text File')\n\nplt.show()\n\n### Comments:\n### paper_df only have 38% data of metadata, should we use meta as well?\n### PDF file have more info than PMC, seems like paper_df use PMC\n\n","2cfbe902":"# License\n\ndf_sy=meta_df['license'].value_counts(dropna=False)\nprint(df_sy)\n\nplt.barh(df_sy.index, df_sy.values, align='center', alpha=0.5,color='orange')\nplt.ylabel('Quantity')\nplt.title('Number of Literatures by License')\n\nplt.show()\n\n","3320b2f8":"# Authors\n\nauthors_count=meta_df['authors'].str.count(\";\")+1\n\ndf_sy=authors_count.isna().value_counts(dropna=False)\nprint(df_sy)\n\nplt.pie(df_sy.values,labels=df_sy.index,autopct='%1.1f%%',colors=['lightblue','orange'], shadow = True)\nplt.title('Whether Authors is NA')\nplt.axis('equal')\nplt.show()\n\n\nplot_df_sy = authors_count.dropna()\n\nprint('Top Number of Authors:')\nplot_df_sy.sort_values().tail()\n\nsns.distplot(plot_df_sy, hist = False, kde = True,\n                 kde_kws = {'shade': True, 'linewidth': 3}\n            ).set_title(\"Number of Authors Distribution\")\n\n","8f91884f":"# Affiliations\n\nstopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(papers_df['affiliations']))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"wordcloud_affiliations.png\", dpi=1024)\n\n","e2b756bb":"# Bibliography\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(papers_df['bibliography']))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word_bibliography.png\", dpi=1024)\n\n","5608d1e0":"# Time\n\ndf_sy=meta_df['publish_time'].str.slice(0, 4).value_counts(dropna=True)\n\ndf_sy=df_sy.sort_index()\n\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(8, 12), dpi=80, facecolor='w', edgecolor='k')\n\nplt.barh(df_sy.index, df_sy.values)\nplt.title('Number of Literatures by Year')\nplt.ylabel('Year');\nplt.show() \n\n### Comments:\n### Most of the literatures are published in 2020, but there are still a large amount of papers published long time ago, should be cautious how reliable they are as for now\n\n","0ae495bd":"#journal\nwordcloud = WordCloud(\n                          background_color='white',\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(meta_df['journal']))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word_journal.png\", dpi=1024)\n\nmeta_df['journal'].value_counts(dropna=False).sort_values().tail()","401e1819":"### Comments:\n### paper_df only have 38% data of metadata, we should use meta as well\n### PDF file have more info than PMC, seems like paper_df only use PMC\n\n\n### Comments:\n### Most of the literatures are published in 2020, but there are still a large amount of papers published long time ago, should be cautious how reliable they are as for now\n\n","f8ac28b9":"## This is a Developement Notebook for the COVID-19 Dataset. Feel free to post your ideas, text or code here and we can clean and aggregate the work into another Final Notebook in the end. ","d6367bb5":"Load DataFrame of Cleaned Documents"}}