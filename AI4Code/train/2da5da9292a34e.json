{"cell_type":{"3bfe78c1":"code","7a4d7649":"code","cc4718dc":"code","f757bb65":"code","89d59669":"code","0636fdbb":"code","a2f08448":"code","7f667af5":"code","974b0f69":"code","86276bbe":"code","5a1d3a5a":"code","2c6f8305":"code","15339f38":"code","cae388ec":"code","363cc51e":"code","446739d9":"code","275797a8":"code","aa345e8b":"code","e241886e":"code","40cb85d5":"code","e6194a9f":"code","b8a7ebbe":"code","b6700bf1":"code","884a8a8d":"code","4ea47164":"code","5b44fdf4":"code","423d16b8":"code","60b8190d":"code","1130180f":"code","53019625":"code","28ab0f8f":"code","0a705439":"code","4a652d0a":"code","dd04750d":"code","5ec89bb4":"code","9a3935b5":"code","e8ecdb7b":"code","038cb83f":"code","272596df":"code","c519e3ed":"code","3b20a6ba":"code","974d7dae":"code","35046c9b":"code","f7b8b1a4":"code","b34ec4b6":"code","a3b2c881":"code","0eac4627":"code","25b5e2dd":"code","c8ad6536":"code","4ac1c677":"code","606366da":"code","f0e38d09":"code","800a85d2":"code","f9d853a0":"code","082124b9":"code","86b01b04":"code","f1f7114d":"markdown","1a0aa5d1":"markdown","9061af76":"markdown","ab2ee6d5":"markdown","70d12f42":"markdown","215fc65e":"markdown","c0c83604":"markdown","9685524e":"markdown","218bb18a":"markdown","00ab6570":"markdown","55b97727":"markdown","0770ffdd":"markdown","34d61fdd":"markdown","941df814":"markdown","58a53fa1":"markdown","07f56d4d":"markdown","a5d07752":"markdown","16dd6675":"markdown","19044569":"markdown","2829b2d0":"markdown","8be331ae":"markdown","d5823504":"markdown","bc2c8f12":"markdown","d53564cb":"markdown","aeba55eb":"markdown","3d1f023e":"markdown","53b1a45e":"markdown","523b1a9d":"markdown","0a7cb942":"markdown","fb5f91c9":"markdown"},"source":{"3bfe78c1":"import pandas as pd ## Dealing with dataframe ,excel and csv file ..\nimport numpy as np ##for linear algebra\nimport matplotlib.pyplot as plt ## ploting graph etc..\nimport seaborn as sns  ##same as matplotlib with extra features","7a4d7649":"!ls ..\/input","cc4718dc":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","f757bb65":"train.head(5)","89d59669":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['ID']\ntest_ID = test['ID']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"ID\", axis = 1, inplace = True)\ntest.drop(\"ID\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","0636fdbb":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.Segmentation.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['Segmentation'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","a2f08448":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(1)","7f667af5":"#Correlation map to see how features are correlated with Segmentation\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","974b0f69":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","86276bbe":"all_data.dtypes","5a1d3a5a":"all_data[\"Family_Size\"] = all_data[\"Family_Size\"].fillna(0)","2c6f8305":"all_data[\"Var_1\"].value_counts().plot(kind='barh')","15339f38":"all_data[\"Var_1\"] = all_data[\"Var_1\"].fillna(\"Cat_6\")","cae388ec":"all_data[\"Profession\"].value_counts().plot(kind=\"barh\")","363cc51e":"all_data[\"Profession\"] = all_data[\"Profession\"].fillna(\"Artist\")","446739d9":"sns.countplot(all_data[\"Ever_Married\"]).set_title('Married or not married ?')","275797a8":"## calculate the total number of missing value in the EVER_married variable\nall_data[\"Ever_Married\"] =all_data[\"Ever_Married\"].fillna(\"Yes\")","aa345e8b":"all_data.Work_Experience.value_counts().plot(kind=\"barh\")","e241886e":"all_data.Work_Experience = all_data.Work_Experience.fillna(1)","40cb85d5":"all_data.Graduated.value_counts().plot(kind=\"barh\")","e6194a9f":"all_data.Graduated = all_data.Graduated.fillna(\"Yes\")","b8a7ebbe":"all_data.head()","b6700bf1":"from sklearn.preprocessing import LabelEncoder\ncols = (\"Gender\",\"Ever_Married\",\"Graduated\")\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))\n","884a8a8d":"from sklearn.preprocessing import StandardScaler\nk_mean_var = all_data[[\"Age\",\"Work_Experience\",\"Family_Size\"]]\nstc = StandardScaler()\nk_mean_var = stc.fit_transform(k_mean_var)\n","4ea47164":"from sklearn.cluster import KMeans\nk=5\nkmeans = KMeans(n_clusters=k) \nkmean_cluster = kmeans.fit_predict(k_mean_var)","5b44fdf4":"all_data[\"kmean_cluster\"] =kmean_cluster","423d16b8":"all_data =pd.get_dummies(all_data)\nprint(all_data.shape)","60b8190d":"train = all_data[:ntrain]\ntest = all_data[ntrain:]\n","1130180f":"train.shape","53019625":"y_train = pd.DataFrame(y_train)\ny_train.columns = [\"target\"]","28ab0f8f":"quality_mapping = { \"A\":\"3\" ,\n    \"B\": \"2\",\n    \"C\": \"1\",\n    \"D\": \"0\"}\ny_train = y_train.target.map(quality_mapping)","0a705439":"from sklearn.ensemble import RandomForestClassifier,  GradientBoostingClassifier,ExtraTreesClassifier,AdaBoostClassifier\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold,StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport sklearn\n","4a652d0a":"X = train.values\ncol_names = train.columns\ny = y_train\n\n\nmodel_xgb = xgb.XGBClassifier(\n                             learning_rate=0.05, max_depth=30\n                             )\nmodel = model_xgb\n# fit the model\nmodel.fit(X, y)\n\nimport matplotlib.pyplot as plt\nimportances = model.feature_importances_\nidxs = np.argsort(importances)\nplt.title('Feature Importances')\nplt.barh(range(len(idxs)), importances[idxs], align='center') \nplt.yticks(range(len(idxs)), [col_names[i] for i in idxs]) \nplt.xlabel('Random Forest Feature Importance')\nplt.show()\n\nxgb_features_extract = []\nfor i in range(importances.shape[0]):\n  if importances[i] > 0.05: ##tune this\n    xgb_features_extract.append(col_names[i])","dd04750d":"## install optimizer\n!pip3 install optuna","5ec89bb4":"from sklearn import ensemble \nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\nimport numpy as np\nimport optuna\nfrom functools import partial\n\ndef optimize(trial,x,y,regressor):\n\n  if (regressor==\"random_forest\"):\n\n    criterion = trial.suggest_categorical(\"criterion\", [\"entropy\",\"gini\"])\n    n_estimators = trial.suggest_int(\"n_estimators\",100,1000)\n    max_depth = trial.suggest_int(\"max_depth\",3,30)\n    max_features = trial.suggest_categorical(\"max_features\",[\"sqrt\",\"auto\",\"log2\"])\n\n    model = ensemble.RandomForestClassifier(\n      criterion=criterion,\n      n_estimators=n_estimators,\n      max_depth=max_depth,\n      max_features=max_features,\n  )\n  elif (regressor==\"SVM\"): ##SVM\n    kernel = trial.suggest_categorical(\"kernel\", [\"rbf\",\"linear\",\"poly\"])\n    gamma = trial.suggest_categorical(\"gamma\", [\"scale\",\"auto\"])\n    coef0 = trial.suggest_int(\"coef0\",1,50)\n    degree = trial.suggest_int(\"degree\",1,4)\n\n\n    model = SVC(\n      kernel=kernel,\n      gamma=gamma,\n      coef0=coef0,\n      degree=degree\n  )\n    \n  else:\n    max_depth = trial.suggest_int(\"max_depth\",3,30)\n\n    n_estimators = trial.suggest_int(\"n_estimators\",100,3000)\n\n    #max_leaves= trial.suggest_int(\"max_leaves\",1,10)\n  # Loguniform parameter\n    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.09)\n  # Uniform parameter\n    colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.0, 1.0) \n\n\n    #min_child_weight = trial.suggest_uniform('min_child_weight',1,3)\n\n  \n\n    model = xgb.XGBClassifier(\n      objective ='reg:squarederror',\n      \n      n_estimators=n_estimators,\n      max_depth=max_depth,\n      learning_rate=learning_rate,\n      colsample_bytree=colsample_bytree,\n     # gamma=gamma,\n     # min_child_weight=min_child_weight,\n      #reg_lambda=reg_lambda,\n     # max_leaves=max_leaves\n\n  ) \n\n  kf=model_selection.KFold(n_splits=5)\n  accuracies=[]\n  for idx in kf.split(X=x , y=y):\n    train_idx , test_idx= idx[0],idx[1]\n    xtrain=x[train_idx]\n    ytrain=y[train_idx]\n    xtest=x[test_idx]\n    ytest=y[test_idx]   \n    model.fit(x,y)\n    y_pred = model.predict(xtest)\n\n    fold_accuracy = metrics.accuracy_score( ytest,\n    y_pred )\n    accuracies.append(fold_accuracy)\n    # return negative accuracy\n    return -1 * np.mean(accuracies)\n","9a3935b5":"\toptimization_function = partial(optimize , x=train.values,y=y_train.values,regressor=\"random_forest\")\n\tstudy = optuna.create_study(direction=\"minimize\")\n\tstudy.optimize(optimization_function,n_trials=10)","e8ecdb7b":"#\toptimization_function = partial(optimize , x=train.values,y=y_train.values,regressor=\"xgb\")\n#\tstudy = optuna.create_study(direction=\"minimize\")\n#\tstudy.optimize(optimization_function,n_trials=10)","038cb83f":"#\ndef acc(y, y_pred):\n    return sklearn.metrics.accuracy_score(y,y_pred)","272596df":"model_xgb = xgb.XGBClassifier(colsample_bytree=0.8718834395682377, \n                             learning_rate=0.0394107485633355, max_depth=30,max_leaves=4, \n                             min_child_weight=2.978747882768758, n_estimators=2000\n                            )","c519e3ed":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = model_xgb.predict(test)\n","3b20a6ba":"print(acc(y_train, xgb_train_pred))","974d7dae":"GBoost = GradientBoostingClassifier(n_estimators=333, learning_rate=0.07177674296965386,\n                                   max_depth=5,\n                                   min_samples_leaf=7)\n","35046c9b":"GBoost.fit(train, y_train)\ngboost_train_pred = GBoost.predict(train)\ngboost_pred = GBoost.predict(test)\nprint(acc(y_train, gboost_train_pred))","f7b8b1a4":"#'criterion': 'entropy', 'n_estimators': 190, 'max_depth': 21, 'max_features': 'sqrt'\nrf = RandomForestClassifier(n_estimators=500,max_depth=20,max_features='sqrt',criterion=\"entropy\")","b34ec4b6":"rf.fit(train.values,y_train)\nrf_train_pred = rf.predict(train)\nrf_pred = rf.predict(test)\nprint(acc(y_train, rf_train_pred))","a3b2c881":"etc = ensemble.ExtraTreesClassifier(max_depth=20,n_estimators=1000,min_samples_split=3)\netc.fit(train.values,y_train)\netc_train_pred = etc.predict(train)\netc_pred = etc.predict(test)\nprint(acc(y_train, etc_train_pred))","0eac4627":"def max_voting(preds):\n  idxs=np.argmax(preds,axis=1)\n  return np.take_along_axis(preds,idxs[:,None],axis=1)","25b5e2dd":"pred_voting_test = [xgb_pred,rf_pred,gboost_pred,etc_pred,etc_pred]\npred_voting_train = [xgb_train_pred,rf_train_pred,gboost_train_pred,etc_train_pred]","c8ad6536":"numpy_array = np.array(pred_voting_train)\ntranspose = numpy_array.T\npred_voting_train_t = transpose.tolist()\n\nnumpy_array = np.array(pred_voting_test)\ntranspose = numpy_array.T\npred_voting_test_t = transpose.tolist()\n\nprint(pred_voting_test_t)","4ac1c677":"pred_voting_test_t = np.array(pred_voting_test_t)\n\npred_voting_train_t = np.array(pred_voting_train_t)","606366da":"voting_pred_train = max_voting(pred_voting_train_t)\nvoting_pred_train = np.concatenate( voting_pred_train, axis=0 )","f0e38d09":"voting_pred_test = max_voting(pred_voting_test_t)\nvoting_pred_test = np.concatenate( voting_pred_test, axis=0 )","800a85d2":"acc(voting_pred_train,y_train)","f9d853a0":"sub = pd.DataFrame()\nsub['ID'] = test_ID\nsub['Segmentation'] = voting_pred_test\n","082124b9":"test_mapping = { \"3\":\"A\" ,\n    \"2\": \"B\",\n    \"1\": \"C\",\n    \"0\": \"D\"}\n ","86b01b04":"sub[\"Segmentation\"] = sub[\"Segmentation\"].astype(\"str\")\nsub[\"Segmentation\"] =sub[\"Segmentation\"].map(test_mapping)\nsub.to_csv('submission.csv',index=False)","f1f7114d":"I suggest that NaN values on this series represents a non-worker ,I fill it with None","1a0aa5d1":"## Family Size","9061af76":"## Kmeans to create new  clusters as variables","ab2ee6d5":"## Imputing missing values\n","70d12f42":"## Features engineering","215fc65e":"## Gboost","c0c83604":"## Graduated","9685524e":"## ExtraTreeClassifier","218bb18a":"## Random forest","00ab6570":"## Ever_married","55b97727":"## Profession","0770ffdd":"### Scaling numerical data","34d61fdd":"### Accuracy of the 4 classifiers votes","941df814":"### Final submission","58a53fa1":"### Loading data","07f56d4d":"## Metrics function","a5d07752":"## Work Experience","16dd6675":"## Max votings of the 4 predictions","19044569":"## Xgboost","2829b2d0":"## Predictions","8be331ae":"## Features Importances ","d5823504":"types of variables \nObject => categorical\n\n\nfloat,int .. => numerical","bc2c8f12":"## Modeling","d53564cb":"## Optimization using Optuna","aeba55eb":"## Var_1 ","3d1f023e":"## mapping the target ","53b1a45e":"I concatenate the train and the test data into one entity to apply the transformation and clean the data on both the dataframe","523b1a9d":"## Split the dataset","0a7cb942":"# Load libraries","fb5f91c9":"## Label Encoder"}}