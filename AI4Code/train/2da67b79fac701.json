{"cell_type":{"ce40b701":"code","63ad9df5":"code","de4ad602":"code","61352d60":"code","11cb2a74":"code","d4d1e82f":"code","ee231bb7":"code","7ba5a758":"code","166854ef":"code","b236f866":"code","f17b958e":"code","1f6c5c1b":"code","e4abe9c5":"code","e74feb14":"code","b062d0ae":"code","b2805fb2":"code","cb14e39f":"code","e1773826":"code","32ca3fbe":"code","ad8f3b88":"code","2352e77a":"code","9744d957":"code","4842c7b2":"code","b4d8b4fb":"code","2eeeb9a1":"code","a9639068":"code","f51a5d78":"code","b316bc01":"code","3a412f4a":"code","b0bfc5f8":"code","bfd12b9a":"code","53060c15":"code","c7ed9889":"code","14e56827":"code","98d0500c":"code","85f4c6c7":"code","cee6e74c":"code","ce8777ae":"code","48d94422":"code","e4d1a81a":"code","450dd8e6":"code","969b17ea":"code","cb56f977":"code","327b3c99":"code","99658ab3":"code","e6cc0a3c":"code","4775db3e":"code","6a072b55":"markdown","8f530576":"markdown","e6009c64":"markdown","dbb32b05":"markdown","a72c0ede":"markdown","71a922c9":"markdown","98aa4084":"markdown","48379487":"markdown","1cac07dc":"markdown","4d8611be":"markdown","dd252088":"markdown","d0829457":"markdown"},"source":{"ce40b701":"import numpy as np\nimport pandas as pd \n\nimport pickle\nfrom collections import namedtuple, defaultdict\nimport datetime\nfrom math import sin, cos, sqrt, atan2, radians\nfrom lightgbm import LGBMClassifier\nfrom tqdm import tqdm_notebook\nimport folium\n%matplotlib inline\nimport sklearn\nimport scipy.sparse\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\nfrom sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\nfrom multiprocessing import Pool\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom itertools import product\nimport warnings \nwarnings.simplefilter('ignore')","63ad9df5":"Y_train =  pd.read_csv('..\/input\/ozonmasters-ml2-2020-c1\/1_data\/train_target.csv')\nX_train = pd.read_csv('..\/input\/ozonmasters-ml2-2020-c1\/1_data\/train_data.csv', parse_dates=['due'])\nX_test = pd.read_csv('..\/input\/ozonmasters-ml2-2020-c1\/1_data\/test_data.csv', parse_dates=['due'])","de4ad602":"X_train['target'] = Y_train.astype(int)\nX_test['target'] = np.nan\nX_train['isTrain'] = True\nX_test['isTrain'] = False\n\nX = pd.concat([X_train, X_test])\n\nX['old_index'] = X.index \nX.reset_index(drop=True, inplace=True)","61352d60":"cities = pd.DataFrame([['moscow', 55.755814, 37.617635],\n                       ['kazan',  55.796289, 49.108795],\n                       ['nnovgorod',  56.326797, 44.006516],\n                       ['spb',  59.939095, 30.315868],\n                       ['voronezh',  51.660781, 39.200269]], columns = ['city', 'lat', 'lon'] ) \n\nNN = NearestNeighbors(n_neighbors=5, metric='euclidean', n_jobs=-1)\nNN.fit(cities[['lat', 'lon']])","11cb2a74":"def get_city_features(X):\n    dist, neighbours = NN.kneighbors(X[['lat', 'lon']])\n    X['centr_dist'] = dist[:, 0]\n    X['city'] = neighbours[:, 0]\n    X['city'] = X['city'].map(cities.city)","d4d1e82f":"get_city_features(X)","ee231bb7":"X['time'] = X.due.dt.floor(\"H\")","7ba5a758":"def add_weather(X):\n    result = []\n    for city in ['moscow', 'kazan', 'nnovgorod', 'spb', 'voronezh']:\n        city_weather = pd.read_json(f'..\/input\/ozonmasters-ml2-2020-c1\/weather_data\/weather_data\/group-city-{city}.jsonl', lines=True)\n        city_weather.time = pd.to_datetime(city_weather.time)\n        city_weather['city'] = city \n        result.append(city_weather)\n    result = pd.concat(result)\n    return pd.merge(X, result, on=['city', 'time'], how='left')    ","166854ef":"X = add_weather(X)","b236f866":"def get_time_features(df):\n    df['date'] = df.due.dt.date\n    df['dow'] = df.due.dt.dayofweek\n    df['hour'] = df.due.dt.hour\n    df['minute'] = df.due.dt.minute\n    df['second'] = df.due.dt.second","f17b958e":"get_time_features(X)  ","1f6c5c1b":"def get_atipic_hour(X):\n    \n    # \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u044b\u0437\u043e\u0432\u043e\u0432 \u0432 \u0434\u0430\u043d\u043d\u044b\u0439 \u0434\u0435\u043d\u044c \u0438 \u0447\u0430\u0441 \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0433\u043e\u0440\u043e\u0434\u0435 \n    ctc = X.sort_values(by=['due']).groupby(['city', 'time']).due.count().rename('counts_per_hour').reset_index()\n\n    \n    # c\u0440\u0435\u0434\u043d\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u044b\u0437\u043e\u0432\u043e\u0432 \u0432 \u0434\u0430\u043d\u043d\u044b\u0439 \u0447\u0430\u0441 \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0433\u043e\u0440\u043e\u0434\u0435 \n    ctc['hour'] = ctc.time.dt.hour\n    ctc['mean_count'] = ctc.groupby(['city', 'hour']).counts_per_hour.transform('mean')\n\n    ctc['atipic_hour'] = ctc.counts_per_hour\/ctc.mean_count\n\n    return pd.merge(X, ctc[['city', 'time', 'atipic_hour']], on = ['city', 'time'], how = 'left')\n    ","e4abe9c5":"X = get_atipic_hour(X)","e74feb14":"def get_atipic_day(X):\n    \n    # \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u044b\u0437\u043e\u0432\u043e\u0432 \u0432 \u0434\u0430\u043d\u043d\u044b\u0439 \u0434\u0435\u043d\u044c \u0438 \u0447\u0430\u0441 \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0433\u043e\u0440\u043e\u0434\u0435 \n    ctc = X.sort_values(by=['due']).groupby(['city', 'date']).due.count().rename('counts_per_day').reset_index()\n\n    \n    # c\u0440\u0435\u0434\u043d\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u044b\u0437\u043e\u0432\u043e\u0432 \u0432 \u0434\u0430\u043d\u043d\u044b\u0439 \u0447\u0430\u0441 \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0433\u043e\u0440\u043e\u0434\u0435 \n    ctc['mean_count'] = ctc.groupby('city').counts_per_day.transform('mean')\n\n    ctc['atipic_day'] = ctc.counts_per_day\/ctc.mean_count\n\n    return pd.merge(X, ctc[['city', 'date', 'atipic_day', 'counts_per_day']], on = ['city', 'date'], how = 'left')","b062d0ae":"X = get_atipic_day(X)","b2805fb2":"lags = [1, 7]","cb14e39f":"def lag_feature(df, lags):\n    for i in lags:\n        shifted = df[['city', 'date', 'counts_per_day']].drop_duplicates()\n        feature = 'ratio_counts_lag_'+str(i)\n        shifted.columns = ['city', 'date', feature]   \n        shifted['date'] = shifted['date'].apply(lambda x: x + datetime.timedelta(days=i))\n        df = pd.merge(df, shifted, on=['city', 'date'], how='left')\n        df[feature] = df['counts_per_day']\/df[feature] \n    return df","e1773826":"X = lag_feature(X, lags)","32ca3fbe":"X_train = X[X.isTrain] \nX_test = X[~X.isTrain] ","ad8f3b88":"knc = KNeighborsClassifier(metric='euclidean', n_neighbors=100)","2352e77a":"prediction  = cross_val_predict(knc, X_train[['lat', 'lon']], X_train.target, cv=10, method='predict_proba')","9744d957":"X_train['knn_prediction'] = prediction[:,0]","4842c7b2":"n = round(X_train.shape[0] * 0.9)\nX_train_sample = X_train.sample(n, random_state=42)\nknc.fit(X_train_sample[['lat', 'lon']], X_train_sample.target)","b4d8b4fb":"test_prediction = knc.predict_proba(X_test[['lat', 'lon']])\n\nX_test['knn_prediction'] = test_prediction[:,0]","2eeeb9a1":"class NearestNeighborsFeats(BaseEstimator, ClassifierMixin):\n    ''' \n        \u042d\u0442\u043e\u0442 \u043a\u043b\u0430\u0441\u0441 \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 KNN \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.\n    '''\n\n    def __init__(self, k_list, metric, n_jobs = 4,  n_classes=None, n_neighbors=None, eps=1e-10):\n        self.n_jobs = n_jobs\n        self.k_list = k_list\n        self.metric = metric\n        self.n_neighbors = n_neighbors or max(k_list)\n        self.eps = eps\n        self.n_classes_ = n_classes\n\n    def fit(self, X, y):\n        # \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u0430-\u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430  \n        self.NN = NearestNeighbors(n_neighbors=max(self.k_list),\n                                   metric=self.metric,\n                                   n_jobs=-1,\n                                   algorithm='brute' if self.metric == 'cosine' else 'auto')\n        self.NN.fit(X)\n\n        # \u0421\u043e\u0445\u0440\u0430\u043d\u0438\u0435\u043d\u0438\u0435 \u043c\u0435\u0442\u043e\u043a \n        self.y_train = y.values\n\n        # \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \n        self.n_classes = len(np.unique(y)) if self.n_classes_ is None else self.n_classes_\n\n    def predict(self, X):\n        '''\n            \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u0432 \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\n        '''\n        result = []\n        for k in self.k_list:\n            \n            neighs_dist, neighs = self.NN.kneighbors(X)\n            neighs_dist, neighs = neighs_dist[:, :k], neighs[:, :k] \n\n            neighs_y = self.y_train[neighs]\n\n            # 1. \u0414\u043e\u043b\u044f \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u0441\u0440\u0435\u0434\u0438 \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0445 \u0441\u043e\u0441\u0435\u0434\u0435\u0439\n            fraction = np.mean(neighs_y, axis = 1)\n\n            # 2. \u041c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0434\u0438\u0441\u0442\u0430\u043d\u0446\u0438\u044f \u0434\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u043a\u043b\u0430\u0441\u0441\u043e\u0432\n\n            # \u0433\u0434\u0435 y=1  \u043d\u0435 \u0442\u0440\u043e\u0433\u0430\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0434\u0438\u0441\u0442\u0430\u043d\u0446\u0438\u0438, \u0433\u0434\u0435 y=0 \u043f\u0440\u0438\u0431\u0430\u043b\u0432\u044f\u0435\u043c \u043a \u0434\u0438\u0441\u0442\u0430\u043d\u0446\u0438\u0438 np.inf\n            ones = np.min(neighs_dist + np.where(neighs_y, 0, np.inf), axis =1)\n            zeros = np.min(neighs_dist + np.where(neighs_y, np.inf, 0), axis =1)\n\n            # 3. \u0421\u0440\u0435\u0434\u043d\u044f\u044f \u0434\u0438\u0441\u0442\u0430\u043d\u0446\u0438\u044f \n\n            mean_distance = np.median(neighs_dist, axis=1)\n\n            # 4. \u041c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0434\u0438\u0441\u0442\u0430\u043d\u0446\u0438\u044f \u0434\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u0434\u0435\u043b\u0435\u043d\u043d\u0430\u044f \u043d\u0430 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u0434\u043e \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430\n\n            norm_ones = ones\/(mean_distance + self.eps)\n\n            norm_zeros = zeros\/(mean_distance + self.eps)\n\n            # 6. \u0421\u0440\u0435\u0434\u043d\u044f\u044f \u0434\u0438\u0441\u0442\u0430\u043d\u0446\u0438\u044f \u0434\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u0438\u0437 k \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0445 \u0441\u043e\u0441\u0435\u0434\u0435\u0439\n\n            ones_mean = (np.sum(neighs_dist*neighs_y, axis=1) + self.eps) \/ np.sum(neighs_y, axis=1)\n\n            mask = 1 * ~neighs_y.astype(bool)\n\n            zeros_mean = (np.sum(neighs_dist*mask, axis=1) + self.eps) \/ np.sum(mask, axis=1)\n            \n            column_names = ['fraction_ones', 'min_distance_one', 'min_distance_zero',\n                            'mean_distance', 'norm_min_distance_one', 'norm_min_distance_zero',\n                            'mean_distance_one', 'min_distance_zero']\n            \n            result.append(pd.DataFrame(data = np.c_[[fraction, ones, zeros, mean_distance,\n                                                      norm_ones, norm_zeros, ones_mean, zeros_mean]].T, \n                                        columns = column_names, \n                                        index = X.index).add_suffix(f'_{k}'))\n\n        return pd.concat(result, axis=1)","a9639068":"nnf = NearestNeighborsFeats(n_jobs = 10, k_list=[10, 50], metric='euclidean')","f51a5d78":"prediction = cross_val_predict(nnf, X_train[['lat', 'lon']], X_train.target, cv=10, method='predict')","b316bc01":"X_train = pd.concat([X_train, pd.DataFrame(prediction, index=X_train.index).add_prefix('nnf_')], axis=1)","3a412f4a":"n = round(X_train.shape[0]*0.9)\n\nX_train_sample = X_train.sample(n, random_state=42)\n\nnnf.fit(X_train_sample[['lat', 'lon']], X_train_sample.target)","b0bfc5f8":"prediction_test = nnf.predict(X_test[['lat', 'lon']])\n\nX_test = pd.concat([X_test, pd.DataFrame(prediction_test.values, index=X_test.index).add_prefix('nnf_')], axis=1)","bfd12b9a":"sample = X_train.sample(3000)\nMap = folium.Map(location=(55.751244, 37.618423), zoom_start=12)\nfor lat, long, target in zip(sample.lat, sample.lon, sample.target):\n    folium.Circle((lat, long),\n                   radius=5,\n                   color='blue' if target else 'red',\n                   fill_color='#3186cc',\n                   ).add_to(Map)","53060c15":"Map","c7ed9889":"class ThrColumnEncoder:\n    def __init__(self, thr=0.5):\n        self.thr = thr\n        self.categories = defaultdict(lambda: -1)\n    def fit(self, x):\n        values = x.value_counts(dropna=False)\n        values = values*100\/len(x) if self.thr < 1 else values\n        for value, key in enumerate(values[values >= self.thr].index):\n            self.categories[key] = value\n        for value, key in enumerate(values[values < self.thr].index):\n            self.categories[key] = -1\n            \n    def transform(self, x):   \n        return x.apply(self.categories.get)\n    \n    def fit_transform(self, x):\n        self.fit(x)\n        return self.transform(x)","14e56827":"class ThrLabelEncoder:\n    \"\"\"\n    \u0420\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0441 pd.DataFrame.\n    \"\"\"\n    def __init__(self, thr=0.5):\n        self.thr = thr\n        self.column_encoders = {}\n        self.features = None\n        \n    def fit(self, X, features):\n        self.features = features\n        for feature in self.features:\n            ce = ThrColumnEncoder(thr=self.thr)\n            ce.fit(X.loc[:, feature])\n            self.column_encoders[feature] = ce\n            \n    def transform(self, X):\n        for feature in self.features: \n            ce = self.column_encoders[feature]\n            X.loc[:, feature] = ce.transform(X[feature]).values\n\n    def fit_transform(self, X, features):\n        self.features = features\n        self.fit(X, features)\n        self.transform(X)","98d0500c":"cat_features = ['f_class', 's_class', 't_class', 'city', 'summary', 'icon', 'precip_type'] ","85f4c6c7":"tle = ThrLabelEncoder()\n\ntle.fit_transform(X_train, features=cat_features)\n\ntle.transform(X_test)","cee6e74c":"class MetaFeatureConstructor:\n    def __init__(self, n_folds=10, model=None, eval_metric='auc', early_stopping_rounds=100):\n        self.random_state = np.random.randint(1e7)\n        self.n_folds = n_folds\n        self.model = model\n        self.eval_metric = eval_metric\n        self.early_stopping_rounds = early_stopping_rounds\n        self.kf = StratifiedKFold(\n            n_folds, random_state=self.random_state, shuffle=True)\n        self.models = list()\n        self.feature_importance_ = pd.DataFrame()\n\n    def cross_val_predict(self, X, Y):\n        result = []\n        # \u043e\u0441\u0442\u043e\u0440\u043e\u0436\u043d\u043e \u0441\u0442\u0440\u0430\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u043f\u043e \u0432\u044b\u0431\u0440\u043e\u0441\u0430\u043c\n        for train_index, test_index in tqdm_notebook(self.kf.split(X, Y)):\n            X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n            Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n            train_data = lgb.Dataset(X_train, label=Y_train)\n            valid_data = lgb.Dataset(X_test, label=Y_test)\n            one_fold_model = lgb.train(params = self.model.get_params(), train_set = train_data, num_boost_round =10000,\n                                       valid_sets=[train_data, valid_data],\n                                       verbose_eval=100, early_stopping_rounds=self.early_stopping_rounds)\n            Y_predict = pd.Series(\n                one_fold_model.predict(X_test), index=Y_test.index)\n\n            fold_importance = pd.DataFrame(np.c_[np.array(X.columns), one_fold_model.feature_importance()],\n                                           columns=['feature', 'importance'])\n            self.feature_importance_ = pd.concat(\n                [self.feature_importance_, fold_importance], axis=0)\n\n            result.append(Y_predict[:])\n            self.models.append(one_fold_model)\n\n        return pd.concat(result, sort=False, axis=0)\n\n    def predict(self, X_test):\n        result = pd.concat([pd.Series(model.predict(\n            X_test), index=X_test.index) for model in self.models], axis=1)\n        return result.mean(axis=1)\n\n    @property\n    def feature_importance(self):\n        return self.feature_importance_.groupby(\"feature\").sum().reset_index()\n\n    @property\n    def scores(self):\n        return {'training': [model.best_score['training'][self.eval_metric] for model in self.models],\n                'valid': [model.best_score['valid_1'][self.eval_metric] for model in self.models]\n                }","ce8777ae":"params = {'task': 'train',\n          'boosting': 'gbdt',\n          'n_estimators': 10000,\n          'class_weight': 'balanced',\n          'metric': {'logloss', 'auc'},\n          'learning_rate': 0.1,\n          'feature_fraction': 0.66, \n          'max_depth': 8,\n          'num_leaves': 64,\n          'min_data_in_leaf': 25,\n          'verbose': -1,\n          'seed': None,\n          'bagging_seed': None,\n          'drop_seed': None\n          }","48d94422":"X_train.columns","e4d1a81a":"train_columns = ['dist',  'f_class', 'lat', 'lon', 's_class', 't_class', \n                 'centr_dist', 'city', 'temperature', 'apparent_temperature',\n                 'wind_speed', 'wind_gust', 'wind_bearing', 'cloud_cover',  'visibility',\n                 'dow', 'hour', 'minute', 'second', 'atipic_hour', 'atipic_day',\n                 'counts_per_day', 'ratio_counts_lag_1', 'ratio_counts_lag_7',\n                 'knn_prediction', 'nnf_0', 'nnf_1', 'nnf_2', 'nnf_3', 'nnf_4', 'nnf_5',\n                 'nnf_6', 'nnf_7', 'nnf_8', 'nnf_9', 'nnf_10', 'nnf_11', 'nnf_12',\n                  'nnf_13', 'nnf_14', 'nnf_15']","450dd8e6":"mfc = MetaFeatureConstructor(model=lgb.LGBMClassifier(**params), n_folds=5)\n\nprediction = mfc.cross_val_predict(X_train.loc[:, train_columns], X_train.loc[:, 'target'])","969b17ea":"mfc.feature_importance.sort_values('importance')","cb56f977":"mfc.scores","327b3c99":"y_train_prediction = mfc.predict(X_test.loc[:, train_columns])","99658ab3":"X_test.index = X_test.old_index","e6cc0a3c":"X_test['target'] = y_train_prediction.values","4775db3e":"target = pd.read_csv('..\/input\/ozonmasters-ml2-2020-c1\/1_data\/sample_submission.csv')\n\ntarget.target = X_test['target'] \n\ntarget.to_csv('weather_5folds.csv', index=False)","6a072b55":"#### \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f","8f530576":"### \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0430 train\/test","e6009c64":"#### \u041e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0432\u044b\u0437\u043e\u0432\u043e\u0432 \u0432 \u0434\u0435\u043d\u044c \u043a \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u0432\u044b\u0437\u043e\u0432\u043e\u0432 \u0432\u0447\u0435\u0440\u0430, \u043d\u0430 \u043f\u0440\u043e\u0448\u043b\u043e\u0439 \u043d\u0435\u0434\u0435\u043b\u0435 \u0432 \u044d\u0442\u043e\u0442 \u0436\u0435 \u0434\u0435\u043d\u044c \u043d\u0435\u0434\u0435\u043b\u0438","dbb32b05":"#### \u0412\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438","a72c0ede":"### KNN features","71a922c9":"#### \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043f\u043e\u0433\u043e\u0434\u0443","98aa4084":"####  labling \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445","48379487":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0433\u043e\u0440\u043e\u0434\u0430","1cac07dc":"### \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438","4d8611be":"#### \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043c\u0435\u0442\u043e\u043a** \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0445 \u0441\u043e\u0441\u0435\u0434\u0435\u0439","dd252088":"#### \u041e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0432\u044b\u0437\u043e\u0432\u043e\u0432 \u0432 \u0434\u0435\u043d\u044c \u043a \u0441\u0440\u0435\u0434\u043d\u0435\u043c\u0443 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u0432\u044b\u0437\u043e\u0432\u043e\u0432 \u0432 \u0434\u0435\u043d\u044c \u0432 \u0433\u043e\u0440\u043e\u0434\u0435","d0829457":"#### \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u044b\u0437\u043e\u0432\u043e\u0432 \u0432 \u0442\u0435\u043a\u0443\u0449\u0438\u0439 \u0447\u0430\u0441 \u043f\u043e \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044e \u043a \u0441\u0440\u0435\u0434\u043d\u0435\u043c\u0443 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u0432\u044b\u0437\u043e\u0432 \u0432 \u044d\u0442\u043e\u0442 \u0447\u0430\u0441 \u0432 \u044d\u0442\u043e\u043c \u0433\u043e\u0440\u043e\u0434\u0435"}}