{"cell_type":{"a7bb431a":"code","f47a71e8":"code","784eca6a":"code","1e510195":"code","ccf975f4":"code","ecc8cb09":"code","bb005f5c":"code","b61f22b2":"code","ddeefa72":"code","655c18fa":"code","8d13691f":"code","88d01a75":"code","1d07adab":"code","00d02393":"code","424520b9":"code","349b849b":"code","e41fe5cb":"code","a7b1f7dd":"code","05fcb15b":"code","6039c3ee":"code","168a51a4":"code","f79a58be":"code","786eb3be":"code","a7d1e79a":"code","870d39a4":"markdown","f1d63bb3":"markdown","4f8d2f45":"markdown","4a16ca86":"markdown","b3ebe164":"markdown"},"source":{"a7bb431a":"'''IMPORTING Libraries'''\nimport pandas as pd\nimport os,time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.combine import SMOTEENN\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.model_selection import train_test_split,cross_val_score, StratifiedKFold,KFold\nfrom sklearn.metrics import f1_score,ConfusionMatrixDisplay\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score,plot_confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom category_encoders import TargetEncoder\nimport imblearn","f47a71e8":"'''Loading Data '''\ntest=pd.read_csv('..\/input\/analytics-vidhya-loan-prediction\/train.csv')\ntrain=pd.read_csv('..\/input\/analytics-vidhya-loan-prediction\/train.csv')\n\n\n'''Checking the data '''\nprint(train.head())\nprint(test.head())\n\n","784eca6a":"'''setting random seed for reproducability'''\nseed=7\ndef set_seed(seed):\n    np.random.seed(seed)\n    os.PYTHONHASHSEED.set_seed(str(seed))\n\nplt.style.use('Solarize_Light2')\nplt.rcParams['font.size']=7\n\n","1e510195":"'''Checking for null values and dtypes'''\n# print(train.isnull().sum())\n# print(test.isnull().sum())\n\n#info()\n# print(train.info())\n# print(test.info())\n\n","ccf975f4":"'''Imputing the missing values and encoding the categorical values'''\n# Target variable: 'Loan_Status'\ntrain['Loan_Status']=train['Loan_Status'].replace({'Y':1,'N':0})\n\n# 1) missing numerical values\nnumerical_cols=[col for col in train.columns if train[col].dtype in ['float','int']]\nnumerical_cols.remove('Credit_History')\nnumerical_cols.remove('Loan_Status')\n#imputing with the mean value.\nimp_mn=SimpleImputer(strategy='mean')  #impute with mean\ntrain[numerical_cols]=imp_mn.fit_transform(train[numerical_cols])\ntest[numerical_cols]=imp_mn.fit_transform(test[numerical_cols])\n\n#imputing with most_frequent values:\nimp_mf=SimpleImputer(strategy='most_frequent')  #impute with most frequent\ntrain['Credit_History']= imp_mf.fit_transform(np.array(train['Credit_History']).reshape(-1,1))\ntest['Credit_History']= imp_mf.fit_transform(np.array(test['Credit_History']).reshape(-1,1))\n\n","ecc8cb09":"# 2) missing categorical values:\n# 2,1) Ordinal Variables: 'Dependents','Education','Property_Area'\nord=['Dependents','Education','Property_Area']\ntrain[ord]=imp_mf.fit_transform(train[ord])\ntest[ord]=imp_mf.fit_transform(test[ord])\n\n#encoding Education\n# train['Education']=train['Education'].replace({'Graduate':1,'Not Graduate':0})\n# test['Education']=test['Education'].replace({'Graduate':1,'Not Graduate':0})\n\n#\n# train['Property_Area']=train['Property_Area'].replace({'Urban':3,'Semiurban':2,'Rural':1})\n# test['Property_Area']=test['Property_Area'].replace({'Urban':3,'Semiurban':2,'Rural':1})\n\n#2,2)Nominal Variables: 'Gender' ,'Married','Self_Employed'\nnom=['Gender' ,'Married','Self_Employed']\ntrain[nom]=imp_mf.fit_transform(train[nom])\ntest[nom]=imp_mf.fit_transform(test[nom])\n\n# train['Married']=train['Married'].replace({'Yes':1 ,'No':0})\n# test['Married']=test['Married'].replace({'Yes':1 ,'No':0})\n#\n# train['Self_Employed']=train['Self_Employed'].replace({'Yes':1,'No':0})\n# test['Self_Employed']=test['Self_Employed'].replace({'Yes':1,'No':0})\n\n'''creating new features '''\ntrain['DtIR']=((train['ApplicantIncome'] +train['CoapplicantIncome']) \/(train['LoanAmount'] * 10e+3))   #debt to income ratio\ntest['DtIR']=((test['ApplicantIncome'] + train['CoapplicantIncome'])\/(test['LoanAmount']))\n\n","bb005f5c":"'''dropping features and making data ready for testing'''\n#setting target and X:\n#reset index\ntrain.reset_index(inplace=True,drop=True)\ntest.reset_index(inplace=True,drop=True)\n\nX1=train.drop(['Loan_ID','Loan_Status','Gender'],axis=1)\n\ny_true= test['Loan_Status']\nX_test=test.drop(['Loan_ID','Gender','Loan_Status'],axis=1)\n\nX_test_ID=test.Loan_ID\ny=train['Loan_Status']\n\n","b61f22b2":"#target encoding\ntr=TargetEncoder()\nX=tr.fit_transform(X1,y)\nX_test=tr.transform(X_test)\n\n#one hot encoding\n# X=pd.get_dummies(X,drop_first=True)\n# X_test=pd.get_dummies(X_test,drop_first=True)\n","ddeefa72":"assert X.shape==X_test.shape","655c18fa":"#correlation matrix\nplt.figure(figsize=(12,6))\nplt.rcParams['font.size']=10\nX_c=X.copy()\nX_c['Loan_Status']=y\ncorrel=X_c.corr()\nsns.heatmap(correl)\nplt.title('Correlation Matrix')\nplt.show()\n","8d13691f":"'''checking the class balance'''\nplt.figure(figsize=(14,7))\nsns.countplot(y)\nplt.title('class_frequency')\nplt.show()\n","88d01a75":"def plot_dist(df,col):\n    df1=df[df['Loan_Status']==1]\n    df0=df[df['Loan_Status']==0]\n    plt.figure(figsize=(14,7))\n    plt.title(f'{col} distribution')\n    sns.distplot(df1[col],hist=False,color='g',label='Loan Accepted')\n    sns.distplot(df0[col],hist=False,color='r',label='Loan Declined')\n    plt.legend()\n    plt.show()\n\n#ApplicantIncome:\nplot_dist(train,'ApplicantIncome')\n","1d07adab":"\n# #CoapplicantIncome:\nplot_dist(train,'CoapplicantIncome')","00d02393":"# #LoanAmount:\nplot_dist(train,'LoanAmount')\n","424520b9":"#Term:\nplot_dist(train,'Loan_Amount_Term')\n","349b849b":"cat_cols=['Gender','Married','Self_Employed','Dependents','Education','Property_Area','Credit_History']\n\ndef plot_countplot(df,cols):\n    plt.figure(figsize=(20,20))\n    for i,col in enumerate(cols):\n        plt.subplot(3,3,i+1)\n        sns.countplot(x=col,data=df,hue='Loan_Status')\n        plt.title(f'{col}',size=10,loc='right')\n    plt.tight_layout()\n    plt.show()\n\nplot_countplot(train,cat_cols)\n","e41fe5cb":"def plot_probs(df,cols):\n    plt.figure(figsize=(20,15))\n    for i,col in enumerate(cols):\n        grp_mean=df.groupby(col)['Loan_Status'].mean()\n        plt.subplot(3,3,i+1)\n        plt.bar(grp_mean.index,grp_mean)\n        plt.title(f'{col}',size=10,loc='right')\n    plt.tight_layout()\n    plt.show()\n\nplot_probs(train,cat_cols)\n\n\n","a7b1f7dd":"'''Scaling'''\n\nscaler=MinMaxScaler()\nscaler.fit(np.vstack((X,X_test)))\nX = scaler.transform(X)\nX_test=scaler.transform(X_test)\nprint(X.shape,X_test.shape)\n","05fcb15b":"'''Cross Validation on diffrent models'''\n\nskf=KFold(n_splits=7,random_state=seed,shuffle=True)\n\n#logistic:\nlog_reg=LogisticRegression(random_state=seed ,C= 0.01, penalty= 'l1',solver='liblinear')\n\n#ensembles:\nRandom_forest=RandomForestClassifier(n_estimators= 500,min_samples_split=8,\nbootstrap= True, max_depth= 80, max_features= 3, min_samples_leaf= 5,random_state=seed)\nAdaBoost=AdaBoostClassifier(n_estimators=300,random_state=seed)\nGBMC=GradientBoostingClassifier(n_estimators=300,random_state=seed)\n\n#KnearestKneighbors:\nKNNC=KNeighborsClassifier()\n\n\n#CROSS VAL SCORE:\n\nmodels=[log_reg,KNNC,Random_forest,GBMC,AdaBoost]\n\nfor model in models:\n    score=cross_val_score(model,X,y,cv=skf)\n    print(f'Mean Cross val Score with {model} is {score.mean()} +\/- {score.std()}')\n\n\n","6039c3ee":"'''hyper parameter tuning for random forest '''\n# params= {'bootstrap': [True],\n#         'max_depth': [80, 90, 100, 110],\n#         'max_features': [2, 3],\n#         'min_samples_leaf': [3, 4, 5],\n#         'min_samples_split': [8, 10, 12],\n#         'n_estimators': [300, 600, 1000]}\n\n# grid_search = GridSearchCV(estimator = Random_forest, param_grid = params,\n#                           cv = 5, n_jobs = -1, verbose = 2)\n#\n# grid_search.fit(X,y)\n#\n# best_params=grid_search.best_params_\n#\n# print(best_params)\n\n#  Hyperparameter tuning for logistic regression\n\n# params={'penalty':['l1','l2'] , 'C':[1e-3,1e-2,1e-1, 1,1e+1,1e+2,1e+3] }\n\n# grid_search=GridSearchCV(estimator=log_reg,param_grid=params,cv=5,n_jobs=-1,verbose=2)\n\n# grid_search.fit(X,y)\n\n# best_params=grid_search.best_params_\n# best_params\n\n","168a51a4":"'''Predictions'''\n\n\n\n\n'''training model on 5 folds and taking the mean of predictions '''\npreds=[]\nscore=[]\n\nfor train_index, test_index in skf.split(X,y):\n      X_train, x_test = X[train_index], X[test_index]\n      y_train, y_test = y[train_index], y[test_index]\n      Random_forest.fit(X_train,y_train)\n      KNNC.fit(X_train,y_train)\n      log_reg.fit(X_train,y_train)\n      score.append((Random_forest.score(x_test,y_test)+log_reg.score(x_test,y_test)+KNNC.score(x_test,y_test))\/3)\n      \n\n      #prediction\n      rf=Random_forest.predict(X_test)\n      lr=log_reg.predict(X_test)\n      knn=KNNC.predict(X_test)\n      \n      mean_pred=[(rf[i]+lr[i] + knn[i])\/3 for i in range(len(rf))]\n      preds.append(mean_pred)\nprint(np.mean(score))\npreds=np.array(preds)\nmean_preds=np.mean(a=preds,axis=0)\n\nprint(mean_preds)\n\n","f79a58be":"y_preds=[]\n# '''predicting Y if mean > threshold ,N other wise'''\nfor x in mean_preds:\n    if x >.80:\n        y_preds.append('Y')\n    else :\n        y_preds.append('N')\n\n        \n        \n#SUBMISSION:\n\nsub=pd.DataFrame({'Loan_ID':X_test_ID , 'Loan_Status':y_preds})\nprint(sub['Loan_Status'].value_counts())\nprint(sub.head(10))\nsub.to_csv('loan_predictions.csv',index=False)\n","786eb3be":"y_preds=sub.Loan_Status.replace({'Y':1,'N':0})\ny_true=y_true.replace({'Y':1,'N':0})\n\nassert y_preds.shape==y_true.shape\n\nprint(f'Accuracy score on test set is {f1_score(y_preds,y_true)}')","a7d1e79a":"from sklearn.metrics import confusion_matrix\n\n\ncm=confusion_matrix(y_true,y_preds)\ndisp=ConfusionMatrixDisplay(cm)\ndisp.plot()","870d39a4":"# HyperParameter Optimization ","f1d63bb3":"# Predictions","4f8d2f45":"# EDA","4a16ca86":"# Training:","b3ebe164":"# Importing Libraries"}}