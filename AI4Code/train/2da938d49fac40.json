{"cell_type":{"91a05fef":"code","2b8040f8":"code","fcad7615":"code","33d44acf":"code","5c90ebc0":"code","d213b47c":"code","a275931b":"code","53bd804c":"code","a8142c24":"code","d703bb3f":"code","12a51ad8":"code","4c1739f2":"code","837ab7e0":"code","25d84286":"code","5f09b54b":"markdown","1877c96f":"markdown","b40e4b28":"markdown"},"source":{"91a05fef":"import os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\n\nfrom sklearn.model_selection import KFold\n\nimport gc\ngc.enable()","2b8040f8":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 10\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"roberta-large\"\nTOKENIZER_PATH = \"roberta-large\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","fcad7615":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True","33d44acf":"train_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")\n\n# Remove incomplete entries if any.\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")","5c90ebc0":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","d213b47c":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","a275931b":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention1 = nn.Sequential(            \n            nn.Linear(1024, 256),            \n            nn.Tanh(),                       \n            nn.Linear(256, 1),\n            nn.Softmax(dim=1)\n        )\n        self.attention2 = nn.Sequential(            \n            nn.Linear(1024, 256),            \n            nn.Tanh(),                       \n            nn.Linear(256, 1),\n            nn.Softmax(dim=1)\n        )\n        self.attention4 = nn.Sequential(            \n            nn.Linear(1024, 256),            \n            nn.Tanh(),                       \n            nn.Linear(256, 1),\n            nn.Softmax(dim=1)\n        )\n        self.attention8 = nn.Sequential(            \n            nn.Linear(1024, 256),            \n            nn.Tanh(),                       \n            nn.Linear(256, 1),\n            nn.Softmax(dim=1)\n        )\n        \n        self.attention16 = nn.Sequential(            \n            nn.Linear(1024,256),\n            nn.Tanh(),                       \n            nn.Linear(256, 1),\n            nn.Softmax(dim=1)\n        )\n        self.attention20 = nn.Sequential(            \n            nn.Linear(1024,256),\n            nn.Tanh(),                       \n            nn.Linear(256, 1),\n            nn.Softmax(dim=1)\n        )\n        \n        self.attention_sm = nn.Sequential(            \n            nn.Linear(1024, 6),\n            nn.Tanh(),                       \n            nn.Linear(6, 1),\n            nn.Softmax(dim=1)\n        )\n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n        \n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights_1 = self.attention1(last_layer_hidden_states)\n        context_vector_1 = torch.sum(weights_1 * last_layer_hidden_states, dim=1)\n        \n        last2_layer_hidden_states = roberta_output.hidden_states[-2]\n        weights_2 = self.attention2(last2_layer_hidden_states)\n        context_vector_2 = torch.sum(weights_2 * last2_layer_hidden_states, dim=1)\n        \n        last4_layer_hidden_states = roberta_output.hidden_states[-3]\n        weights_4 = self.attention4(last4_layer_hidden_states)\n        context_vector_4 = torch.sum(weights_4 * last4_layer_hidden_states, dim=1)\n        \n        last8_layer_hidden_states = roberta_output.hidden_states[-4]\n        weights_8 = self.attention8(last8_layer_hidden_states)\n        context_vector_8 = torch.sum(weights_8 * last8_layer_hidden_states, dim=1)\n        \n        last16_layer_hidden_states = roberta_output.hidden_states[-16]\n        weights_16 = self.attention16(last16_layer_hidden_states)\n        context_vector_16 = torch.sum(weights_16 * last16_layer_hidden_states, dim=1)\n        \n        last20_layer_hidden_states = roberta_output.hidden_states[-20]\n        weights_20 = self.attention20(last20_layer_hidden_states)\n        context_vector_20 = torch.sum(weights_20 * last20_layer_hidden_states, dim=1)\n        \n#         print(context_vector_1.shape)\n        con_context_vectors = torch.stack([context_vector_1, context_vector_2, context_vector_4, context_vector_8, context_vector_16, context_vector_20], dim=1)\n#         print(con_context_vectors.shape)\n        layer_weights = self.attention_sm(con_context_vectors)\n#         print(layer_weights.shape)\n        final_context_vector = torch.sum(layer_weights * con_context_vectors, dim=1)\n        \n#         print(final_context_vector.shape)\n        ans = self.regressor(final_context_vector)\n#         print(ans.shape)\n        \n        # Now we reduce the context vector to the prediction score.\n        return ans","53bd804c":"def eval_mse(model, data_loader):\n    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n    model.eval()            \n    mse_sum = 0\n\n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)                        \n            target = target.to(DEVICE)           \n            \n            pred = model(input_ids, attention_mask)                       \n\n            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n                \n\n    return mse_sum \/ len(data_loader.dataset)","a8142c24":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n            pred = model(input_ids, attention_mask)                        \n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n    return result","d703bb3f":"def train(model, model_path, train_loader, val_loader,\n          optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n    best_val_rmse = None\n    best_epoch = 0\n    step = 0\n    last_eval_step = 0\n    eval_period = EVAL_SCHEDULE[0][1]    \n    start = time.time()\n    for epoch in range(num_epochs):                           \n        val_rmse = None         \n        for batch_num, (input_ids, attention_mask, target) in enumerate(train_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)            \n            target = target.to(DEVICE)                        \n            optimizer.zero_grad()\n            model.train()\n            pred = model(input_ids, attention_mask)\n            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n            mse.backward()\n            optimizer.step()\n            if scheduler:\n                scheduler.step()\n            if step >= last_eval_step + eval_period:\n                elapsed_seconds = time.time() - start\n                num_steps = step - last_eval_step\n                last_eval_step = step\n                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n                for rmse, period in EVAL_SCHEDULE:\n                    if val_rmse >= rmse:\n                        eval_period = period\n                        break                               \n                if not best_val_rmse or val_rmse < best_val_rmse:                    \n                    best_val_rmse = val_rmse\n                    best_epoch = epoch\n                    torch.save(model.state_dict(), model_path)\n                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n                start = time.time()                       \n            step += 1\n    return best_val_rmse","12a51ad8":"def create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    roberta_parameters = named_parameters[:389]    \n    attention_parameters = named_parameters[391:395]\n    regressor_parameters = named_parameters[395:]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n\n        lr = 1.2e-5\n\n        if layer_num >= 133:        \n            lr = 3e-5\n\n        if layer_num >= 261:\n            lr = 7.5e-5\n\n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n\n    return AdamW(parameters)","4c1739f2":"# del model\ngc.collect()\ntorch.cuda.empty_cache()","837ab7e0":"gc.collect()\n\nSEED = 1000\nlist_val_rmse = []\n\nkfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n\nfor fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n    print(f\"\\nFold {fold + 1}\/{NUM_FOLDS}\")\n    model_path = f\"model_{fold + 1}.pth\"\n        \n    set_random_seed(SEED + fold)\n    \n    train_dataset = LitDataset(train_df.loc[train_indices])    \n    val_dataset = LitDataset(train_df.loc[val_indices])    \n        \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              drop_last=True, shuffle=True, num_workers=2)    \n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n                            drop_last=False, shuffle=False, num_workers=2)    \n        \n    set_random_seed(SEED + fold)    \n    \n    model = LitModel().to(DEVICE)\n    \n    optimizer = create_optimizer(model)                        \n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_training_steps=NUM_EPOCHS * len(train_loader),\n        num_warmup_steps=50)    \n    \n    list_val_rmse.append(train(model, model_path, train_loader,\n                               val_loader, optimizer, scheduler=scheduler))\n\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    \nprint(\"\\nPerformance estimates:\")\nprint(list_val_rmse)\nprint(\"Mean:\", np.array(list_val_rmse).mean())","25d84286":"# 0.474","5f09b54b":"# Overview\nMy Idea: 2D attention\n\n\nProvide attention to hidden states for multiple layers and create a context vector based on attention weights.\nthen provide attention to context vectors of multiple layers\n\nand \n\n\nAcknowledgments: some ideas were taken from kernels by [Torch](https:\/\/www.kaggle.com\/rhtsingh) and [Maunish](https:\/\/www.kaggle.com\/maunish) and [Andrey Tuganov](https:\/\/www.kaggle.com\/andretugan).","1877c96f":"# Model\nThe model is inspired by the one from [Maunish](https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm).","b40e4b28":"# Dataset"}}