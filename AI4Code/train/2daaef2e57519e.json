{"cell_type":{"840adf15":"code","ab40ae0d":"code","3e74c5d4":"code","3a31c55f":"code","0fd8ac3c":"code","3309b368":"code","1d08201a":"code","4a910525":"code","c8b18fd9":"code","c042653b":"code","6860df29":"code","5cc5a445":"code","f3c88ea4":"code","b7442039":"code","fb235794":"code","a5b3a447":"code","13480986":"code","c3ec59fa":"code","25363acf":"code","96e65236":"code","6cb93148":"code","7f9e456a":"code","deb006d5":"code","e7b36a04":"code","18c3dfd0":"code","aa455f55":"code","97a4dda9":"code","f81fe6ff":"code","585fccd4":"code","d1249e19":"code","7ceea6c1":"code","65e29da3":"code","87628fed":"code","10c101e9":"code","eaea0bc0":"code","c93b1c45":"code","281e732b":"code","1be69c63":"code","4eef8059":"code","94149492":"code","3650fa9d":"markdown","c48059d4":"markdown","4ac29f61":"markdown","2f4b7817":"markdown","e10ff614":"markdown","8b212557":"markdown","f8a3b88d":"markdown","46f30884":"markdown","72e10594":"markdown","24e6aa6e":"markdown","087db463":"markdown","0423c2b1":"markdown","7795020f":"markdown","b0629687":"markdown"},"source":{"840adf15":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"..\/input\"))","ab40ae0d":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","3e74c5d4":"# First, lets count how much data we have!\ntrain_len, test_len = len(train_df.index), len(test_df.index)\nprint(f'train size: {train_len}, test size: {test_len}')","3a31c55f":"# also, lets take a quick look at what we have \ntrain_df.head(20)","0fd8ac3c":"# its always a good idea to count the amount of missing values before diving into any analysis\n# Lets also see how many missing values (in percentage) we are dealing with\nmiss_val_train_df = train_df.isnull().sum(axis=0) \/ train_len\nmiss_val_train_df = miss_val_train_df[miss_val_train_df > 0] * 100\nmiss_val_train_df","3309b368":"# lets create a list of all the identities tagged in this dataset. This list given in the data section of this competition. \nidentities = ['male','female','transgender','other_gender','heterosexual','homosexual_gay_or_lesbian',\n              'bisexual','other_sexual_orientation','christian','jewish','muslim','hindu','buddhist',\n              'atheist','other_religion','black','white','asian','latino','other_race_or_ethnicity',\n              'physical_disability','intellectual_or_learning_disability','psychiatric_or_mental_illness',\n              'other_disability']","1d08201a":"# getting the dataframe with identities tagged\ntrain_labeled_df = train_df.loc[:, ['target'] + identities ].dropna()\n# changing the value of identity to 1 or 0 only\nidentity_label_count = train_labeled_df[identities].where(train_labeled_df == 0, other = 1).sum()\n# dividing the time each identity appears by the total number of comments\nidentity_label_pct = identity_label_count \/ len(train_labeled_df.index)","4a910525":"# now we will use seaborn to do a horizontal bar plot and visualize our result. \n# since it would be nicer to have it ordered by most frequent to least, we do a simple sort\nidentity_label_pct = identity_label_pct.sort_values(ascending=False)\n# seaborn is more of a wrapper around matplotlib. So to edit size and give x, y labels; we use the plt that we imported earlier\nplt.figure(figsize=(30,20))\nsns.set(font_scale=3)\nax = sns.barplot(x = identity_label_pct.values * 100, y = identity_label_pct.index, alpha=0.8)\nplt.ylabel('Demographics')\nplt.xlabel('Total Percentage')\nplt.title('Most Frequent Identities')\nplt.show()","c8b18fd9":"# First we multiply each identity with the target\nweighted_toxic = train_labeled_df.iloc[:, 1:].multiply(train_labeled_df.iloc[:, 0], axis=\"index\").sum() \n# then we divide the target weighted value by the number of time each identity appears\nweighted_toxic = weighted_toxic \/ identity_label_count\nweighted_toxic = weighted_toxic.sort_values(ascending=False)\n# plot the data using seaborn like before\nplt.figure(figsize=(30,20))\nsns.set(font_scale=3)\nax = sns.barplot(x = weighted_toxic.values , y = weighted_toxic.index, alpha=0.8)\nplt.ylabel('Demographics')\nplt.xlabel('Weighted Toxicity')\nplt.title('Weighted Analysis of Most Frequent Identities')\nplt.show()","c042653b":"# total length of characters in the comment\ntrain_df['total_length'] = train_df['comment_text'].apply(len)","6860df29":"# number of capital letters\ntrain_df['capitals'] = train_df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))","5cc5a445":"# ratio of capital characters vs length of the comment\ntrain_df['caps_vs_length'] = train_df.apply(lambda row: float(row['capitals'])\/float(row['total_length']),axis=1)","f3c88ea4":"# count of special characters\ntrain_df['num_exclamation_marks'] = train_df['comment_text'].apply(lambda comment: comment.count('!'))\ntrain_df['num_question_marks'] = train_df['comment_text'].apply(lambda comment: comment.count('?'))\ntrain_df['num_punctuation'] = train_df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\ntrain_df['num_symbols'] = train_df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))","b7442039":"# word count\ntrain_df['num_words'] = train_df['comment_text'].apply(lambda comment: len(comment.split()))","fb235794":"# number of unique words in the comment\ntrain_df['num_unique_words'] = train_df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))","a5b3a447":"# ratio of unique words and number of words\ntrain_df['words_vs_unique'] = train_df['num_unique_words'] \/ train_df['num_words']","13480986":"# number of smiley faces\ntrain_df['num_smilies'] = train_df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))","c3ec59fa":"# let's create a list of columns with the new features we have just created\nnew_features = ['total_length', 'capitals', 'caps_vs_length', 'num_exclamation_marks','num_question_marks', \n            'num_punctuation', 'num_words', 'num_unique_words','words_vs_unique', 'num_smilies', 'num_symbols']\n# the dataset is labeled with more information alongside the target. lets collect them in a list as well\nlabels = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'funny', 'wow', 'sad', \n           'likes', 'disagree', 'sexual_explicit','identity_annotator_count', 'toxicity_annotator_count']","25363acf":"# in pandas calculating correlation is simple. You can calculate the correlation value between two columns in the following way\ntrain_df['total_length'].corr(train_df['funny'])","96e65236":"# lets loop over each feature and calculate its correlation with a label\nrows = [{label:train_df[feature].corr(train_df[label]) for label in labels} for feature in new_features]\ntrain_correlations = pd.DataFrame(rows, index=new_features)","6cb93148":"# now we have our beautiful correlation matrix\ntrain_correlations","7f9e456a":"# heatmaps are great for visualizing correlation matrix. and its very simple to do so in seaborn\nplt.figure(figsize=(10, 6))\nsns.set(font_scale=1)\nax = sns.heatmap(train_correlations, vmin=-0.1, vmax=0.1, center=0.0)","deb006d5":"# imports for topic modeling and loading stopwords\nimport nltk; nltk.download('stopwords')\nimport re\nimport numpy as np\nimport pandas as pd\nfrom pprint import pprint\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nimport spacy\nimport pyLDAvis\nimport pyLDAvis.gensim\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use', 'people', 'may', 'think'])","e7b36a04":"#convert to list\n#data = train_df.comment_text.values.tolist()\n#covert to list while subsetting\ndata_select = train_df[['comment_text',\"severe_toxicity\"]]\ndata_select.head(5)\ndata_query = data_select[data_select[\"severe_toxicity\"] > 0.01]\ndata_query2 = data_query[['comment_text']]\ndata = data_query2.comment_text.values.tolist()","18c3dfd0":"#tokenize words and Clean-up text\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:31])","aa455f55":"#9. Creating Bigram and Trigram Models\n#Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\nprint(bigram_mod[bigram_mod[data_words[0]]])\nprint(trigram_mod[trigram_mod[data_words[0]]])\n","97a4dda9":"# Define functions for stopwords, bigrams, trigrams and lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out\n# call the functions in right order\n\n# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[:1])","f81fe6ff":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1])","585fccd4":"# Human readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","d1249e19":"#Building the Topic Model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=10, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","7ceea6c1":"#Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","65e29da3":"# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","87628fed":"from matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.colors as mcolors\n\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\ncloud = WordCloud(stopwords=stop_words,\n                  background_color='white',\n                  width=2500,\n                  height=1800,\n                  max_words=10,\n                  colormap='tab10',\n                  color_func=lambda *args, **kwargs: cols[i],\n                  prefer_horizontal=1.0)\n\ntopics = lda_model.show_topics(formatted=False)\n\nfig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    fig.add_subplot(ax)\n    topic_words = dict(topics[i][1])\n    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n    plt.gca().imshow(cloud)\n    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n    plt.gca().axis('off')\n\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()","10c101e9":"def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n     # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_lemmatized)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\ndf_dominant_topic.head(10)","eaea0bc0":"# lets take the dataset with identitiy tags, created date, and target column\nwith_date_df = train_df.loc[:, ['created_date', 'target'] + identities].dropna()\n# next we will create a weighted dataframe for each identity tag (like we did before)\n# first we divide each identity tag with the total value it has in the dataset\nweighted_df = with_date_df.iloc[:, 2:] \/ with_date_df.iloc[:, 2:].sum()\n# then we multiplty this value with the target \ntarget_weighted_df = weighted_df.multiply(with_date_df.iloc[:, 1], axis=\"index\")\n# lets add a column to count the number of comments\ntarget_weighted_df['comment_count'] = 1\n# now we add the date to our newly created dataframe (also parse the text date as datetime)\ntarget_weighted_df['created_date'] = pd.to_datetime(with_date_df['created_date']).values.astype('datetime64[M]')\n# now we can do a group by of the created date to count the number of times a identity appears for that date\nidentity_weight_per_date_df = target_weighted_df.groupby(['created_date']).sum().sort_index()","c93b1c45":"# importing plotly\nimport plotly\nimport plotly.plotly as py\nimport cufflinks as cf\nimport plotly.graph_objs as go\nplotly.tools.set_credentials_file(username='ekhtiar', api_key='NUzf7CKPlCmChi5aFEdY')\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)","281e732b":"races = ['black','white','asian','latino','other_race_or_ethnicity']\nidentity_weight_per_date_df[races].iplot(title = 'Time Series Toxicity & Race', filename='Time Series Toxicity & Race' )","1be69c63":"religions = ['atheist', 'buddhist', 'christian', 'hindu', 'muslim', 'other_religion']\nidentity_weight_per_date_df[religions].iplot(title = 'Time Series Toxicity & Religion', filename='Time Series Toxicity & Religion')","4eef8059":"sexual_orientation = ['heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation']\nidentity_weight_per_date_df[sexual_orientation].iplot(title = 'Time Series Toxicity & Sexual Orientation', filename='Time Series Toxicity & Sexual Orientation')","94149492":"identity_weight_per_date_df['comment_count'].iplot(title = 'Time Series Total Tagged Comments', filename='Time Series Total Tagged Comments')","3650fa9d":"On 2018, Kaggle launched a competition in association with Jigsaw\/Conversation AI to classify toxic comments. The original competition can be viewed [here](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge). A toxic comment is a comment that is rude, disrespectful or otherwise likely to make someone leave a discussion. The goal of this competition was to build a multi-headed model that\u2019s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate.\n\nHowever, the models developed in this competition unfortunately associated the targetted group with toxicity, i.e. \"gay\". For example, a comment like \"I am a gay woman\" would be classified as toxic. This happened as the examples of identities associated with toxicity outnumbered neutral comments regarding the same identity.\n\nTherefore, the same team launched a new competition to recognize unintended bias towards identities. We are asked to use a dataset labeled with the associated identity. Let's look into the dataset and understand it first, so we can create a model that can better deal with bias.\n\nThis notebook is heavily based on the kernel:\nhttps:\/\/www.kaggle.com\/nz0722\/simple-eda-text-preprocessing-jigsaw. The purpose of this Kernel is to walk Kaggle newbies through the process of data exploration and visualization. In this notebook, we will use Pandas to do a little bit of data wrangling and Plotly and Seaborn to visualize the result of our wrangling. ","c48059d4":"### Challenge: Can you do a stacked horizontal bar plot to show which portion of this data is toxic vs not toxic for the first graph? \n### ps. use a threshold (i.e. >.5) to turn the target into binary values.","4ac29f61":"As we can see from the graph above, the two race based identities (White and Black) and religion based identities (Muslim and Jews) are heavily associated with toxic comments. ","2f4b7817":"## Section 2: Identity Analysis and Horizontal Barplot\n\nNow that we know a large portion of our dataset is doesn't have the group identity, (for now) we can drop it before we do any basic analysis. First simple and interesting question to answer would be which identity appears the most in the dataset. ","e10ff614":"If you want to understand each of the field, it is a good idea to read the data section of the Kaggle competition (https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/data). Let's take a couple of minutes to read this section first.","8b212557":"The diagram above is certainly one way of looking at our data. However, it is missing the complete picture because we are not using two things. First, for each example we have a score (target) of how toxic the comment is. Second, each identity also has a value between 0 to 1 to identify how much they have been targeted. We can use this two aspects to our advantage and see which identities are more frequently related to toxic comments.","f8a3b88d":"## Section 4: Time series analysis with Plotly\n**\nThe dataset also has a 'created_date' field, which tells us when the comment was made. We can use this field to do some time series analysis. In this section, we will also use plotly for simplifying our visualization needs.**","46f30884":"We can certainly see some interesting patterns. For example, funny comments seems to have negative correlation with the size of the comment and also the number of capital characters in the comments.","72e10594":"## Section 3: Data Wrangling and Correlation Plotting with Heatmap\n\nWe can also do a little data wrangling to create our own features and see how they correlate to different labels within our data.","24e6aa6e":"### Challenge: Can you create your own feature and visualize the correlation?","087db463":"### Challenge: what can you do to give us a better perspective?****","0423c2b1":"## Section 1: Understanding The Shape of Data","7795020f":"> As you can see from the table above, a large portion of the data is missing the identity tag. However, as the number is the same for the tags, I assume that the data is complete for the part which has identity tags.","b0629687":"We will use plotly to visualize our time series data. Plotly is a great tool that allows us to draw interactive graphs within this notebook. Though for commercial purposes plotly isn't free but for our educational and open source puposes it is. It is also free to sign up. Please sign up [here](http:\/\/plot.ly) to get your username and api_key. You can also use my one for the time being or replace my one with your own if you would like. Once you have created your account and logged in, you can go to your username on the top right corner > setting > API keys to get your username and generate an API key. Also for this part, The Kaggle notebook needs to have internet access. You can enable it under the settings on the right hand side of this notebook."}}