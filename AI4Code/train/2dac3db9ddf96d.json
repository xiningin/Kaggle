{"cell_type":{"8b47c601":"code","9fd68f02":"code","8b0922b5":"code","5bebdf74":"code","e31e2c3c":"code","c9a04f70":"code","41b838db":"code","159df44b":"code","446456fa":"code","3cabf2c0":"code","2a6a1522":"code","7d8856f1":"code","1b351e85":"code","6c73f56e":"code","49874708":"code","fe0ab308":"code","2e8a75fa":"code","703f25a5":"code","54dbef44":"code","3fe34c17":"code","8355d656":"code","08cae36a":"code","1ffe02f0":"code","27a3787f":"code","876691cb":"code","25af5c6c":"code","a8a71aa4":"code","96b5a3e5":"code","fd2a9300":"code","5071c5b2":"code","7361b5ee":"markdown","79dd0038":"markdown","b7117591":"markdown","d59a20c2":"markdown","1898ff3e":"markdown","2e95e091":"markdown","be078aed":"markdown","edb9c072":"markdown","681af407":"markdown","277a223c":"markdown","74cc65c4":"markdown","a27cf8dc":"markdown","431bdd3a":"markdown","8240ec82":"markdown"},"source":{"8b47c601":"import numpy as np\nimport pandas as pd","9fd68f02":"# weight of RoBERTa Large predicts\nRoB_L_W = [.2]*5\nRoB_B_W = [.2]*5\n# weight of SVM predicts\nSVM_W = [.2]*5\n\nweight_M_dict = {'RoBL':.3, 'RoBB':.3, 'SVM':.4}\n\n\nassert np.sum(SVM_W)==1\nassert np.sum(RoB_L_W)==1","8b0922b5":"import os\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport optuna\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor, Pool, CatBoost\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts, CosineAnnealingLR, \n                                      ReduceLROnPlateau)\n\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification,get_constant_schedule_with_warmup)\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL","5bebdf74":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\ntarget = train_data['target'].to_numpy()\n\n#for kfold  \nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\nbins = train_data.bins.to_numpy()\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","e31e2c3c":"config = {\n    'batch_size':128,\n    'max_len':256,\n    'seed':42,\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])","c9a04f70":"class CLRPDataset(nn.Module):\n    def __init__(self,df,tokenizer,max_len=128):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)  \n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)","41b838db":"def get_embeddings(df,path,plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    MODEL_PATH = path\n    model = AutoModel.from_pretrained(MODEL_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n    model.to(device)\n    model.eval()\n\n    ds = CLRPDataset(df,tokenizer,config['max_len'])\n    dl = DataLoader(ds,\n                  batch_size = config[\"batch_size\"],\n                  shuffle=False,\n                  num_workers = 4,\n                  pin_memory=True,\n                  drop_last=False\n                 )\n        \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs[0][:,0].detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)","159df44b":"!pip3 install ..\/input\/frozendict\/frozendict-2.0.2-py3-none-any.whl\n!pip3 install ..\/input\/neuraltangent\/neural_tangents-0.3.6-py2.py3-none-any.whl","446456fa":"from jax import random\nfrom neural_tangents import stax\nimport neural_tangents as nt\n\ndef get_preds_svm(X,y,X_test,bins=bins,nfolds=5,C=10,kernel='rbf'):\n    kfold = StratifiedKFold(n_splits=nfolds)\n    scores = list()\n    preds = np.zeros((X_test.shape[0]))\n    for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n        X_train,y_train = X[train_idx], y[train_idx]\n        X_valid,y_valid = X[valid_idx], y[valid_idx]\n\n        ResBlock = stax.serial(\n                        stax.FanOut(2),\n                        stax.parallel(\n                            stax.serial(\n                                stax.Erf(),\n                                stax.Dense(1, W_std=1.25, b_std=0.0),\n                                stax.Erf(),\n                                stax.Dense(1, W_std=1.25, b_std=0.0),\n                                stax.Erf(),\n                                stax.Dense(1, W_std=1.25, b_std=0.0),\n                            ),\n                            stax.Identity(),\n                        ),\n                        stax.FanInSum()\n                    )\n\n        init_fn, apply_fn, kernel_fn = stax.serial(\n                stax.Dense(1, W_std=1.0, b_std=0),\n                ResBlock, ResBlock, stax.Erf(),\n                stax.Dense(1, W_std=2.5, b_std=0.1)\n        )\n\n        key = random.PRNGKey(10)\n        _, params = init_fn(key, input_shape=X_train.shape)\n        predict_fn = nt.predict.gradient_descent_mse_ensemble(kernel_fn,\n                                                                  X_train,\n                                                                  y_train[:,np.newaxis],\n                                                                  diag_reg=1e-1,\n                                                                  lr=1)\n        prediction = predict_fn(x_test=X_valid, get='nngp', t=None)#model.predict(X_valid)\n        score = rmse_score(prediction,y_valid)\n        print(f'Fold {k} , rmse score: {score}')\n        scores.append(score)\n        preds += predict_fn(x_test=X_test, get='nngp', t=None)#model.predict(X_test)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return np.array(preds)\/nfolds","3cabf2c0":"train_embeddings1 =  get_embeddings(train_data,'..\/input\/modelf1')\ntest_embeddings1 = get_embeddings(test_data,'..\/input\/modelf1')\nsvm_p1 = get_preds_svm(train_embeddings1,target,test_embeddings1).mean(axis=1)\ndel train_embeddings1,test_embeddings1\n\ntrain_embeddings2 =  get_embeddings(train_data,'..\/input\/modelf2')\ntest_embeddings2 = get_embeddings(test_data,'..\/input\/modelf2')\nsvm_p2 = get_preds_svm(train_embeddings2,target,test_embeddings2).mean(axis=1)\ndel train_embeddings2,test_embeddings2\n\n\ntrain_embeddings3 =  get_embeddings(train_data,'..\/input\/modelf3')\ntest_embeddings3 = get_embeddings(test_data,'..\/input\/modelf3')\nsvm_p3 = get_preds_svm(train_embeddings3,target,test_embeddings3).mean(axis=1)\ndel train_embeddings3,test_embeddings3\n\ntrain_embeddings4 =  get_embeddings(train_data,'..\/input\/modelf4')\ntest_embeddings4 = get_embeddings(test_data,'..\/input\/modelf4')\nsvm_p4 = get_preds_svm(train_embeddings4,target,test_embeddings4).mean(axis=1)\ndel train_embeddings4,test_embeddings4\n\ntrain_embeddings5 =  get_embeddings(train_data,'..\/input\/modelf5')\ntest_embeddings5 = get_embeddings(test_data,'..\/input\/modelf5')\nsvm_p5 = get_preds_svm(train_embeddings5,target,test_embeddings5).mean(axis=1)\ndel train_embeddings5,test_embeddings5\n\ndel train_data, test_data","2a6a1522":"svm_preds = svm_p1*SVM_W[0] + svm_p2*SVM_W[1] + svm_p3*SVM_W[2] + svm_p4*SVM_W[3] + svm_p5*SVM_W[4]","7d8856f1":"svm_pred_df = pd.DataFrame()\nsvm_pred_df['svm'] = svm_preds\nsvm_pred_df.head(10)","1b351e85":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","6c73f56e":"%matplotlib inline\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt\nimport json\nfrom collections import defaultdict\nimport gc\ngc.enable()","49874708":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import (\n    Dataset, DataLoader, \n    SequentialSampler, RandomSampler\n)\nfrom transformers import RobertaConfig\nfrom transformers import (\n    get_cosine_schedule_with_warmup, \n    get_cosine_with_hard_restarts_schedule_with_warmup\n)\nfrom transformers import RobertaTokenizer\nfrom transformers import RobertaModel\nfrom IPython.display import clear_output\nfrom tqdm import tqdm, trange","fe0ab308":"def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n    data = data.replace('\\n', '')\n    tok = tokenizer.encode_plus(\n        data, \n        max_length=max_len, \n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=True\n    )\n    curr_sent = {}\n    padding_length = max_len - len(tok['input_ids'])\n    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n        ([0] * padding_length)\n    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n        ([0] * padding_length)\n    return curr_sent","2e8a75fa":"class DatasetRetriever(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.values.tolist()\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt, label = self.excerpts[item], self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.double),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }","703f25a5":"class CommonLitModel(nn.Module):\n    def __init__(\n        self, \n        model_name, \n        config,  \n        multisample_dropout=False,\n        output_hidden_states=False\n    ):\n        super(CommonLitModel, self).__init__()\n        self.config = config\n        self.roberta = RobertaModel.from_pretrained(\n            model_name, \n            output_hidden_states=output_hidden_states\n        )\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.regressor)\n \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n \n    def forward(\n        self, \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        labels=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n \n        # multi-sample dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.regressor(dropout(sequence_output))\n            else:\n                logits += self.regressor(dropout(sequence_output))\n        \n        logits \/= len(self.dropouts)\n \n        # calculate loss\n        loss = None\n        if labels is not None:\n            loss_fn = torch.nn.MSELoss()\n            logits = logits.view(-1).to(labels.dtype)\n            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n        \n        output = (logits,) + outputs[1:]\n        return ((loss,) + output) if loss is not None else output","54dbef44":"def make_model(model_name='roberta-large', num_labels=1):\n    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n    config = RobertaConfig.from_pretrained(model_name)\n    config.update({'num_labels':num_labels})\n    model = CommonLitModel(model_name, config=config)\n    return model, tokenizer\n\ndef make_loader(\n    data, \n    tokenizer, \n    max_len,\n    batch_size,\n):\n    \n    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n    test_sampler = SequentialSampler(test_dataset)\n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=batch_size \/\/ 2, \n        sampler=test_sampler, \n        pin_memory=False, \n        drop_last=False, \n        num_workers=0\n    )\n\n    return test_loader","3fe34c17":"class Evaluator:\n    def __init__(self, model, scalar=None):\n        self.model = model\n        self.scalar = scalar\n\n    def evaluate(self, data_loader, tokenizer):\n        preds = []\n        self.model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for batch_idx, batch_data in enumerate(data_loader):\n                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n                    batch_data['attention_mask'], batch_data['token_type_ids']\n                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n                    attention_mask.cuda(), token_type_ids.cuda()\n                \n                if self.scalar is not None:\n                    with torch.cuda.amp.autocast():\n                        outputs = self.model(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids\n                        )\n                else:\n                    outputs = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        token_type_ids=token_type_ids\n                    )\n                \n                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n                preds += logits\n        return preds","8355d656":" def config(fold, model_name, load_model_path):\n    torch.manual_seed(2021)\n    torch.cuda.manual_seed(2021)\n    torch.cuda.manual_seed_all(2021)\n    \n    max_len = 250\n    batch_size = 8\n\n    model, tokenizer = make_model(\n        model_name=model_name, \n        num_labels=1\n    )\n    model.load_state_dict(\n        torch.load(f'{load_model_path}\/model{fold}.bin')\n    )\n    test_loader = make_loader(\n        test, tokenizer, max_len=max_len,\n        batch_size=batch_size\n    )\n\n    if torch.cuda.device_count() >= 1:\n        print('Model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n\n    # scaler = torch.cuda.amp.GradScaler()\n    scaler = None\n    return (\n        model, tokenizer, \n        test_loader, scaler\n    )","08cae36a":"def run(fold=0, model_name=None, load_model_path=None):\n    model, tokenizer, \\\n        test_loader, scaler = config(fold, model_name, load_model_path)\n    \n    import time\n\n    evaluator = Evaluator(model, scaler)\n\n    test_time_list = []\n\n    torch.cuda.synchronize()\n    tic1 = time.time()\n\n    preds = evaluator.evaluate(test_loader, tokenizer)\n\n    torch.cuda.synchronize()\n    tic2 = time.time() \n    test_time_list.append(tic2 - tic1)\n    \n    del model, tokenizer, test_loader, scaler\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return preds","1ffe02f0":"RBWRTbase_pred_df = pd.DataFrame()\nRBWRTlarge_pred_df = pd.DataFrame()\nfor fold in tqdm(range(5)):\n    RBWRTbase_pred_df[f'fold{fold}'] = run(fold, '..\/input\/roberta-base\/', '..\/input\/commonlit-roberta-base-i\/')\n    RBWRTlarge_pred_df[f'fold{fold}'] = run(fold, '..\/input\/robertalarge\/', '..\/input\/roberta-large-itptfit\/')","27a3787f":"RoBL_preds = RBWRTlarge_pred_df['fold0']*0\nRoBB_preds = RBWRTbase_pred_df['fold0']*0\nfor i in range(FOLDS):\n    RoBL_preds += RBWRTlarge_pred_df[f'fold{i}']*RoB_L_W[i]\n    RoBB_preds += RBWRTbase_pred_df[f'fold{i}']*RoB_B_W[i]     \n    \n","876691cb":"RoBL_preds[:7]","25af5c6c":"RoBB_preds[:7]","a8a71aa4":"svm_preds[:7]","96b5a3e5":"W_M = weight_M_dict\npds1 = RoBL_preds\npds2 = RoBB_preds\npds3 = svm_preds","fd2a9300":"sub = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsub['target'] = ((RBWRTbase_pred_df.mean(axis=1) + RBWRTlarge_pred_df.mean(axis=1))\/2).values.tolist()\n# sub['target'] = pds1*W_M['RoBL'] + pds2*W_M['RoBB'] + pds3*W_M['SVM']\nsub.to_csv('submission.csv', index=False)","5071c5b2":"sub","7361b5ee":"## A simple average of my two previous kernels\n\n1. RoBERTa-Base -> [CommonLit Readability Prize - RoBERTa Torch|Infer](https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer)\n\n2. RoBERTa-Large -> [CommonLit Readability Prize-RoBERTa Torch|Infer 2](https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-2)\n\n### P.S. What's Coming Soon -   \n1. [CommonLit Readability Prize - RoBERTa Torch|FIT](https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-fit) fixed version.  \n2. **CommonLit Readability Prize - RoBERTa Torch|FIT 2** Dynamic Padding + Sequence Bucketing, Stochastic Weight Averaging, MIXOUT, Utilizing Different Transformer Layers (LSTM, Attention) **all in one kernel maybe**\n3. **Multi-Task Learning**  ","79dd0038":"### Dataset Retriever","b7117591":"### Import Dependencies","d59a20c2":"### Make Submission","1898ff3e":"### Config","2e95e091":"## RoBERT lager and base","be078aed":"### Convert Examples `(Excerpt)` to Features","edb9c072":"### Please upvote before Fork ;)","681af407":"### Run","277a223c":"### Evaluator","74cc65c4":"## SVM","a27cf8dc":"### Load Test Dataset","431bdd3a":"### Utils","8240ec82":"### Model"}}