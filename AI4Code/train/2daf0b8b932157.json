{"cell_type":{"7a6a4c7e":"code","8948afea":"code","11f9d93a":"code","6b063745":"code","0238a719":"code","3d9e381c":"code","de7fb3a7":"code","5cfbad61":"code","b53f9fef":"code","6fb36a07":"code","f3c0398c":"code","b2562bca":"code","95320e71":"code","3f64a2ad":"code","708173ad":"code","af9f14c0":"code","636a050d":"markdown","e46de5ea":"markdown","faa8bfc1":"markdown","e3c72ba6":"markdown","c4525c26":"markdown","e1a77f2d":"markdown","883cfd72":"markdown","ff1e1e4c":"markdown"},"source":{"7a6a4c7e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport math\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8948afea":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntrain_data.head()","11f9d93a":"train_data.info()","6b063745":"y=train_data['Survived'].copy().values","0238a719":"# Create a new dummy columns with person sex and Embarked type \n\nsex_df=pd.get_dummies(train_data['Sex'])\nembarked_df=pd.get_dummies(train_data['Embarked'],prefix='Embarked')\n\ntrain_data = pd.concat([train_data,sex_df,embarked_df],axis=1)\ntrain_data.drop(['Sex','Embarked','PassengerId','Name','Cabin','Survived','Ticket'],axis=1,inplace=True)","3d9e381c":"# Age column has null vaules, Fill NAN values with the Mean of the age column\ntrain_data.Age.fillna(train_data.Age.mean(),inplace=True)\n\ntrain_data.head()","de7fb3a7":"features = train_data.iloc[:len(train_data)].values\nfeatures.shape","5cfbad61":"class Logistic_Regression:\n    \n    def sigmoid(self,m,x):\n#         model can output values much greater than  and much smaller than. \n#         This is unnatural and not desired\u2014we would like to get values between  and  so we can interpret the output as probabilities.\n#         To achieve this we use the sigmoid function\n        z = np.dot(x,m)\n        return 1\/(1+ np.exp(-z))\n    \n    def cost_function(self,m,x,y):\n        preds=self.sigmoid(m,x)\n        loss = ((-y*np.log(preds))-((1-y)*np.log(1-preds))).mean()\n        return loss\n    \n    def gredient_descent(self,m,x,y,learning_rate):\n        preds=self.sigmoid(m,x)\n        derivative = np.dot(x.T,(preds-y))\/len(y)\n        m=m-learning_rate*derivative\n        return m\n    ","b53f9fef":"learning_rate=0.001\nepoch = 100000\n\n#  Coefficent for each independent variable \nmvals = np.zeros(features.shape[1])\n\nlogistic_reg =Logistic_Regression()\n\nfor i in range(epoch):\n    mvals=logistic_reg.gredient_descent(mvals,features,y,learning_rate)\n    error = logistic_reg.cost_function(mvals,features,y)","6fb36a07":"predicted_data = logistic_reg.sigmoid(mvals,features)\n\nfor j in range(len(predicted_data)):\n    if (predicted_data[j] > .5):\n        predicted_data[j]=1\n    else:\n        predicted_data[j]=0","f3c0398c":"correct_predictions = 0\nsurvived_predictions=0\n\nfor j in range(len(predicted_data)):\n    if predicted_data[j] ==1:\n        survived_predictions+=1\n        \n    if (predicted_data[j] == y[j]):\n            correct_predictions+=1\n            \naccuracy = correct_predictions\/len(predicted_data)\n\nprint(\"Number of correct predictions\", correct_predictions)\nprint(\"Number of predicted survivers\", survived_predictions)\nprint(\"Accuracy\", accuracy)","b2562bca":"test_data.info()","95320e71":"test_sex_df=pd.get_dummies(test_data['Sex'])\ntest_embarked_df=pd.get_dummies(test_data['Embarked'],prefix='Embarked')\npassenger_ids=test_data['PassengerId'].copy().values\n\ntest_data.Age.fillna(test_data.Age.mean(),inplace=True)\ntest_data.Fare.fillna(test_data.Age.mean(),inplace=True)\n\ntest_data = pd.concat([test_data,test_sex_df,test_embarked_df],axis=1)\ntest_data.drop(['Sex','Embarked','PassengerId','Name','Cabin','Ticket'],axis=1,inplace=True)\n","3f64a2ad":"passenger_ids.shape\ntest_data.shape\ntest_data.info()","708173ad":"# Predicting the test data\ntest_features = test_data.iloc[:len(test_data)].values\ntest_predicted = logistic_reg.sigmoid(mvals,test_features)\n\ntest_survived_count =0\n\nfor i in range(len(test_predicted)):\n    if test_predicted[i]>0.5:\n        test_predicted[i]=1\n        test_survived_count+=1\n    else:\n        test_predicted[i]=0\n        \nprint(\"Number of people survived in test data\",test_survived_count)","af9f14c0":"test_predicted = test_predicted.astype(int)\nsubmission = pd.DataFrame({\n        \"PassengerId\": passenger_ids,\n        \"Survived\": test_predicted\n    })\n\nsubmission.to_csv('gender_submission.csv', index=False)","636a050d":"**Prediction** \n\npredict the number of people survied for the train data with the trained coefficent values.\n","e46de5ea":"**Accuracy**\n\nAccuracy is the proportion of correct predictions over total predictions. This is how we can find the accuracy with logistic regression:\n> accuracy = correct_predictions \/ total_predictions","faa8bfc1":"Target data ","e3c72ba6":"**Feature\/Independent variables**","c4525c26":"**Logistic Regression**\n\nIn this problem we use the logistic regression to train the model and predict the number of survived people\n","e1a77f2d":"**Model Training**","883cfd72":"**Titanic Disaster Logistic Regression From Scratch using Python**","ff1e1e4c":"**Apply Trained model to Test Data**\n\nIn this section we would clean up the test data and apply the trained cofficents to test data to predict the survived people."}}