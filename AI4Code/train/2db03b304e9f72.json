{"cell_type":{"e6ca1de5":"code","c39d4c6e":"code","331d3df3":"code","2df0d59d":"code","c4b7e2fc":"code","2b1e6776":"code","7a479ea7":"code","2cc6e52c":"code","ea0670e2":"code","dc2ce47f":"code","d9463390":"code","3b551c38":"code","a38d6621":"code","eed6480a":"code","bbaea9f8":"code","b816d16d":"code","64cd9c7c":"code","4daf6280":"markdown","69931d77":"markdown","d3d1e1d8":"markdown","05628f22":"markdown","a528ab04":"markdown","94f8fdcf":"markdown","a1411e33":"markdown","3c10ba23":"markdown","8d44b8f8":"markdown","dbcb329b":"markdown","2f7af9ff":"markdown","e152b949":"markdown","7922ae9a":"markdown","d9a01450":"markdown","f234620d":"markdown"},"source":{"e6ca1de5":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","c39d4c6e":"(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\nprint(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)","331d3df3":"train_images = train_images.reshape((60000, 28 * 28))\ntrain_images = train_images.astype(\"float32\") \/ 255 ","2df0d59d":"test_images = test_images.reshape((10000, 28 * 28))\ntest_images = test_images.astype(\"float32\") \/ 255 ","c4b7e2fc":"num_epochs = 30\nbatch_size = 256\nvalidation_split = 0.3","2b1e6776":"def get_shallow_model():\n    model = keras.Sequential([\n        layers.Dense(10, activation = \"softmax\"),\n        \n    ])\n    model.compile(optimizer = 'rmsprop',\n                  loss = 'sparse_categorical_crossentropy',\n                  metrics = ['accuracy'])\n    return model\n\nshallow_model = get_shallow_model()\nhistory_shallow = shallow_model.fit(train_images, train_labels,\n                                   epochs = num_epochs,\n                                   batch_size = batch_size,\n                                   validation_split = validation_split)\n","7a479ea7":"def plot(history_obj,type_nw = \"\"):\n    t_loss = history_obj.history[\"loss\"]\n    v_loss = history_obj.history[\"val_loss\"]\n    t_acc = history_obj.history[\"accuracy\"]\n    v_acc = history_obj.history[\"val_accuracy\"]\n    print(f\"Min val loss: {round(min(v_loss),3)} at {np.argmin(np.array(v_loss))} epoch\")\n    print(f\"Max val acc: {round(max(v_acc),3)}\")\n    epochs = range(1, num_epochs+1)\n    plt.plot(epochs, t_loss, \"b-\",\n             label=\"training loss\")\n    plt.plot(epochs, v_loss, \"b--\",\n             label=\"val loss\")\n    plt.plot(epochs, t_acc, \"r-\",\n             label=\"training acc\")\n    plt.plot(epochs, v_acc, \"r--\",\n             label=\"val acc\")\n    plt.title(\"loss with {} network\".format(type_nw))\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"loss \/ accuracy\")\n    plt.legend()\n","2cc6e52c":"\nplot(history_obj = history_shallow,\n     type_nw=\"shallow\")","ea0670e2":"def get_med_model():\n    model = keras.Sequential([\n        layers.Dense(64, activation = \"relu\"),\n        layers.Dense(10, activation = \"softmax\"),\n        \n    ])\n    model.compile(optimizer = 'rmsprop',\n                  loss = 'sparse_categorical_crossentropy',\n                  metrics = ['accuracy'])\n    return model\n\nmed_model = get_med_model()\nhistory_med = med_model.fit(train_images, train_labels,\n                                   epochs = num_epochs,\n                                   batch_size = batch_size,\n                                   validation_split = validation_split)\n","dc2ce47f":"plot(history_obj = history_med,\n     type_nw=\"med\")","d9463390":"def get_large_model():\n    model = keras.Sequential([\n        layers.Dense(512, activation = \"relu\"),\n        layers.Dense(512, activation = \"relu\"),\n        layers.Dense(10, activation = \"softmax\"),\n        \n    ])\n    model.compile(optimizer = 'rmsprop',\n                  loss = 'sparse_categorical_crossentropy',\n                  metrics = ['accuracy'])\n    return model\n\nlarge_model = get_large_model()\nhistory_large = large_model.fit(train_images, train_labels,\n                                   epochs = num_epochs,\n                                   batch_size = batch_size,\n                                   validation_split = validation_split)\n","3b551c38":"plot(history_obj = history_large,\n     type_nw=\"large\")","a38d6621":"\ndef get_large_reg_model():\n    model = keras.Sequential([\n        layers.Dense(512, \n                     kernel_regularizer=keras.regularizers.l2(0.001), \n                     activation = \"relu\"),\n        layers.Dense(512, \n                     kernel_regularizer=keras.regularizers.l2(0.001), \n                     activation = \"relu\"),\n        layers.Dense(10, activation = \"softmax\"),\n        \n    ])\n    model.compile(optimizer = 'rmsprop',\n                  loss = 'sparse_categorical_crossentropy',\n                  metrics = ['accuracy'])\n    return model\n\nlarge_reg_model = get_large_reg_model()\nhistory_large_reg = large_reg_model.fit(train_images, train_labels,\n                                   epochs = num_epochs,\n                                   batch_size = batch_size,\n                                   validation_split = validation_split)\n","eed6480a":"plot(history_obj = history_large_reg,\n     type_nw=\"large regularized\")","bbaea9f8":"\ndef get_large_drp_model():\n    model = keras.Sequential([\n        layers.Dense(512,  \n                     activation = \"relu\"),\n        layers.Dropout(0.5),\n        layers.Dense(512, \n                     activation = \"relu\"),\n        layers.Dropout(0.5),\n        layers.Dense(10, activation = \"softmax\"),\n        \n    ])\n    model.compile(optimizer = 'rmsprop',\n                  loss = 'sparse_categorical_crossentropy',\n                  metrics = ['accuracy'])\n    return model\n\nlarge_drp_model = get_large_drp_model()\nhistory_large_drp = large_drp_model.fit(train_images, train_labels,\n                                   epochs = num_epochs,\n                                   batch_size = batch_size,\n                                   validation_split = validation_split)\n","b816d16d":"plot(history_obj = history_large_drp,\n     type_nw=\"large w Dropout(0.5)\")","64cd9c7c":"s_loss = history_shallow.history[\"val_loss\"]\nm_loss = history_med.history[\"val_loss\"]\nl_loss = history_large.history[\"val_loss\"]\nlr_loss = history_large_reg.history[\"val_loss\"]\nld_loss = history_large_drp.history[\"val_loss\"]\n\nepochs = range(1, num_epochs+1)\n\nplt.figure(figsize=(14,8))\nplt.plot(epochs, s_loss, \"b--\",\n         label=\"val loss Shallow\")\nplt.plot(epochs, m_loss, \"r--\",\n         label=\"val loss Medium\")\nplt.plot(epochs, l_loss, \"g--\",\n         label=\"val loss Large\")\nplt.plot(epochs, lr_loss, \"p--\",\n         label=\"val loss Large Regularized\")\nplt.plot(epochs, ld_loss, \"o--\",\n         label=\"val loss Large Dropout\")\nplt.title(\"loss with different network\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"loss \")\nplt.legend()\n","4daf6280":"## Loss is continously goes down till the end and hovers around 0.278 (which seems high) \n","69931d77":"# Large model - 3 layer model with 512 neurons each ","d3d1e1d8":"# Large model with L2 regularization - Helps to reduce overfitting","05628f22":"# Download MNIST","a528ab04":"# Large model with added Dropout - This also reduce over-fitting by randomly removing x% of neurons from previous layers while training - which leads to less memorization","94f8fdcf":"# Reshape data","a1411e33":"* Shallow network is under-fitted with small capacity hence higher val loss\n* Medium network with more capacity has reduced val loss but still doesnt overfit\n* Medium network with more capacity and regularization does bettwe than medium without regularization (less erratic than large with reg)\n* Large network quickly goes down till epoch 2 and then overfits and loss increases\n* Adding l2 regularizer makes the network loss more erratic but it doesnt overfit the model\n* Adding Dropout seems to work well, network overfits after 10 epochs ","3c10ba23":"# Shallow model - 1 layer model","8d44b8f8":"# Lets check out the concept of under-fitting and over-fitting on MNIST dataset","dbcb329b":"# Medium sized model - 2 layer network","2f7af9ff":"# Fix initial parameters","e152b949":"## With added regularization, model doesnt overfit quickly and val loss goes down for longer - but becomes erratic","7922ae9a":"# Comparison of val loss across different networks","d9a01450":"## With increased model capacity, val loss goes down to 0.112\n## Means shallow network was under-fitted","f234620d":"## For large network, can see that model overfits quickly and then loss goes up after 2 epochs - Models starts to overfit"}}