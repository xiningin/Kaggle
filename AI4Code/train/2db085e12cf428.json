{"cell_type":{"f801dfa2":"code","d057cbd9":"code","2be52f69":"code","1f699836":"code","0247e91c":"code","c1ebf59b":"code","d03ce6f6":"code","fd2a2a08":"code","26b1c92e":"code","26652ab5":"code","a1ed32b4":"code","97b495ba":"code","eee0a7a4":"code","3ca4e8f8":"code","e3ae0458":"code","584ef1e7":"code","d20e51c5":"code","03ccd760":"code","98a7dab9":"code","a5872f87":"code","8847a53d":"code","8df6a3d0":"code","c3f1d01c":"markdown","bd970e10":"markdown","e41f1153":"markdown","44e64f87":"markdown","df67fe13":"markdown","5847f784":"markdown","c62f84e5":"markdown","f228598e":"markdown"},"source":{"f801dfa2":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.metrics import accuracy_score\n\n","d057cbd9":"#Importing Train and Test\ntrain_df=pd.read_csv('\/kaggle\/input\/customer-segmentation\/Train_aBjfeNk.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/customer-segmentation\/Test_LqhgPWU.csv')","2be52f69":"train_df.info()","1f699836":"test_df.info()","0247e91c":"sns.pairplot(train_df)","c1ebf59b":"combine_set=pd.concat([train_df,test_df],ignore_index=True)\ncombine_set['Ever_Married'].fillna('unknown',inplace=True)\ncombine_set['Graduated'].fillna('unknown',inplace=True)\ncombine_set['Profession'].fillna('unknown',inplace=True)\ncombine_set['Work_Experience'].fillna(combine_set['Work_Experience'].mode()[0], inplace=True)\ncombine_set['Family_Size'].fillna(0,inplace=True)\ncombine_set['Var_1'].fillna('Cat_0',inplace=True)\ncombine_set.head(5)","d03ce6f6":"#Adding more Features\ncombine_set['Unique_profession_per_agegroup']=combine_set.groupby(['Age'])['Profession'].transform('nunique')\ncombine_set['Unique_agegroup_per_profession']=combine_set.groupby(['Profession'])['Age'].transform('nunique')\ncombine_set['Age_Family_size']=combine_set.groupby(['Age'])['Family_Size'].transform('nunique')\ncombine_set.head(5)","fd2a2a08":"combine_set['Var_1'].value_counts()","26b1c92e":"combine_set_ann=pd.get_dummies(combine_set,columns=['Gender','Ever_Married','Graduated','Spending_Score','Profession','Var_1'],drop_first=True)\nprint(combine_set_ann.shape)\ncombine_set_ann.head(10)","26652ab5":"#Encoding Category Variables\ndef frequency_encoding(col):\n    fe=combine_set.groupby(col).size()\/len(combine_set)\n    combine_set[col]=combine_set[col].apply(lambda x: fe[x])\n#     le=LabelEncoder()\n#     combine_set[col]=le.fit_transform(combine_set[col])","a1ed32b4":"for col in list(combine_set.select_dtypes(include=['object']).columns):\n    if col!='Segmentation':\n        frequency_encoding(col)\n    \n    \ncombine_set.head(5)\n    ","97b495ba":"train_df=combine_set[combine_set['Segmentation'].isnull()==False]\ntrain_ann=combine_set_ann[combine_set_ann['Segmentation'].isnull()==False]\ntest_df=combine_set[combine_set['Segmentation'].isnull()==True]\ntest_ann=combine_set_ann[combine_set_ann['Segmentation'].isnull()==True]\n\n\n# 90% Train Data is repeated in Test set so seperating the ID's which are common both in test and train set\nsubmission_df=pd.merge(train_df,test_df,on='ID',how='inner')\nsubmission_df=submission_df[['ID','Segmentation_x']]\nsubmission_df.columns=['ID','Segmentation']\nsubmission_ann=pd.merge(train_ann,test_ann,on='ID',how='inner')\nsubmission_ann=submission_ann[['ID','Segmentation_x']]\nsubmission_ann.columns=['ID','Segmentation']\n\n\n# le=LabelEncoder()\n# train_df['Segmentation']=le.fit_transform(train_df['Segmentation'])\nprint(submission_df.shape)\nsubmission_df.head(5)\n","eee0a7a4":"# Creating Train and Test Data\nX=train_df.drop(['Segmentation'],axis=1)\nX_ann=train_ann.drop(['ID','Segmentation'],axis=1)\nY=train_df['Segmentation']\nY_ann=pd.get_dummies(Y)\n\nmd_df=pd.concat([pd.DataFrame(submission_df['ID']),pd.DataFrame(test_df['ID'])]).drop_duplicates(keep=False)\n\ntest_df=pd.merge(md_df,test_df,on='ID',how='inner')\n\nmd_ann=pd.concat([pd.DataFrame(submission_ann['ID']),pd.DataFrame(test_ann['ID'])]).drop_duplicates(keep=False)\n\ntest_ann=pd.merge(md_ann,test_ann,on='ID',how='inner')\nX_main_test=test_df.drop(['Segmentation'],axis=1)\nX_main_ann=test_ann.drop(['ID','Segmentation'],axis=1)\n\n\nX_train,X_val,Y_train,Y_val=train_test_split(X,Y,test_size=0.2,random_state=100)","3ca4e8f8":"X_ann.head(5)","e3ae0458":"from keras.models import Sequential\nfrom keras.layers import Dense\n\nclassifier=Sequential()\nclassifier.add(Dense(256,activation='relu',input_shape=(X_ann.shape[1],)))\nclassifier.add(Dense(128,activation='relu'))\nclassifier.add(Dense(32,activation='relu'))\nclassifier.add(Dense(4,activation='softmax'))\nclassifier.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n\nclassifier.fit(X_ann,Y_ann,epochs=50,batch_size=32)\n\n# preds=classifier.predict(X_main_test)","584ef1e7":"preds=classifier.predict(X_main_ann)\n\nlis=[]\nfor item in preds:\n    index,value=max(enumerate(item),key=lambda x: x[1])\n    if index==0:\n        lis.append('A')\n    elif index==1:\n        lis.append('B')\n    elif index==2:\n        lis.append('C')\n    else:\n        lis.append('D')\n","d20e51c5":"lg=LGBMClassifier(boosting_type='gbdt', max_depth=10, learning_rate=0.09, objective='multiclass', reg_alpha=0,\n                  reg_lambda=1, n_jobs=-1, random_state=100, n_estimators=1000)\n\nlg.fit(X,Y)\n\n# print(accuracy_score(Y_val,lg.predict(X_val)))","03ccd760":"from catboost import CatBoostClassifier\n\ncb=CatBoostClassifier(learning_rate=0.05,depth=8,boosting_type='Plain',eval_metric='Accuracy',n_estimators=1000,random_state=294)\ncb.fit(X,Y)\n# print(accuracy_score(Y_val,xg.predict(X_val)))","98a7dab9":"xg=XGBClassifier(booster='gbtree',verbose=0,learning_rate=0.07,max_depth=8,objective='multi:softmax',\n                  n_estimators=1000,seed=294)\nxg.fit(X,Y)\n# print(accuracy_score(Y_val,xg.predict(X_val)))\n","a5872f87":"perm = PermutationImportance(xg,random_state=100).fit(X_val, Y_val)\neli5.show_weights(perm,feature_names=X_val.columns.tolist())","8847a53d":"d=pd.DataFrame()\nd=pd.concat([d,pd.DataFrame(cb.predict(X_main_test)),pd.DataFrame(xg.predict(X_main_test)),pd.DataFrame(lg.predict(X_main_test))],axis=1)\nd.columns=['1','2','3']\n\nre=d.mode(axis=1)[0]\nre.head(5)","8df6a3d0":"submission_dataframe=pd.DataFrame()\n\nsubmission_dataframe['ID']=test_df['ID']\nsubmission_dataframe['Segmentation']=np.array(re)\nsubmission_dataframe=pd.concat([submission_df,submission_dataframe])\nsubmission_dataframe.to_csv('\/kaggle\/working\/main_test.csv', index=False)","c3f1d01c":"# Using ANN","bd970e10":"# Catboost","e41f1153":"# Ensembling LightGBM,Xgboost,Catboost","44e64f87":"# XGBOOST","df67fe13":"**Problem Statement**:\n    An automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4 and P5). After intensive market research, they\u2019ve deduced that the behavior of new market is similar to their existing market.\n\nIn their existing market, the sales team has classified all customers into 4 segments (A, B, C, D ). Then, they performed segmented outreach and communication for different segment of customers. This strategy has work exceptionally well for them. They plan to use the same strategy on new markets and have identified 2627 new potential customers. \n\nYou are required to help the manager to predict the right group of the new customers.\n\n","5847f784":"# Checking the Permutation Importance of Features","c62f84e5":"# Customer Segmentation","f228598e":"# LightGBM"}}