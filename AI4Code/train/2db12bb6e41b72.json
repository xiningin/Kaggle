{"cell_type":{"63393de5":"code","9d699a2a":"code","d085f6d2":"code","49ede10f":"code","2099b5ab":"code","d3578619":"code","a7062e50":"code","b445a0ac":"code","faf558e5":"code","feb24861":"code","443939b5":"code","1d5b2ad5":"code","3ef08152":"code","467d1f21":"code","b402b5bf":"code","60ddc68c":"markdown","11c14761":"markdown","2f37d2dd":"markdown","84e3de7a":"markdown","59b6a3c0":"markdown","f24eef9e":"markdown","c735672b":"markdown","ccb5804e":"markdown"},"source":{"63393de5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9d699a2a":"import numpy as np\nimport pandas as pd\n\nimport pandas_profiling as pp\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d085f6d2":"dat1 = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict.csv')\nprint('First Data Set ---------------------------------------------------')\nprint(dat1.describe())\nprint(dat1.info())\n\ndat2 = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')\nprint('\\nSecond Data Set ------------------------------------------------')\nprint(dat2.describe())\nprint(dat2.info())","49ede10f":"dat = pd.concat([dat1, dat2], ignore_index=True)\nprint('Combining the two data sets without knowledge of duplication')\nprint(dat.describe())\n\ndf = dat.drop_duplicates()\nprint('\\nDropping duplicates across the data sets')\nprint(df.describe())\n\ndf = df.set_index('Serial No.')\ndf.head()","2099b5ab":"pp.ProfileReport(df)","d3578619":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.metrics import mean_squared_error, r2_score","a7062e50":"np.random.seed(0)\n# df = df.reindex(np.random.permutation(df.index))\n\ncol = df.columns\nprint(col)\ny = df['Chance of Admit ']\nX = df.drop(['Chance of Admit '], axis=1)\n\nprint(X.info())\n\nX.describe()","b445a0ac":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=2)\n# check distributions across X_train and X_test\nlcol = X_train.columns\nncol = len(lcol)\nnbins = 10\n\nfig, axs = plt.subplots(2,4,figsize=(20,5))\nfor i,col in enumerate(lcol):\n    r,c = divmod(i,4)\n    X_train.hist(density=1,column=col, bins=nbins, ax=axs[r,c],color = 'blue',alpha=0.7)\n    X_test.hist(density=1, column=col, bins=nbins, ax=axs[r,c], color = 'red', alpha=0.7)\n\nsns.histplot(y_train, bins=nbins, ax=axs[1,3], color= 'blue', alpha=0.7,linewidth=0, stat='density')\nsns.histplot(y_test, bins=nbins, ax=axs[1,3], color= 'red', alpha=0.7,linewidth=0, stat='density')","faf558e5":"# Scaling the data\n# sc = StandardScaler()\nsc = MinMaxScaler()\nX_train_sc = sc.fit_transform(X_train)\nX_test_sc = sc.transform(X_test)\n\nfig, axs = plt.subplots(2,4,figsize=(20,5))\nfor i in range(X_train_sc.shape[1]):\n    r,c = divmod(i,4)    \n    sns.histplot(X_train_sc[:,i], bins=nbins, ax=axs[r,c], color= 'blue', alpha=0.7,linewidth=0, stat='density').set_title(lcol[i])\n    sns.histplot(X_test_sc[:,i], bins=nbins, ax=axs[r,c], color= 'red', alpha=0.7,linewidth=0, stat='density')","feb24861":"# Scale only part of the feature set\n# features_to_scale = lcol # should replicate above cell\nfeatures_to_scale = ['GRE Score', 'TOEFL Score']\nct = ColumnTransformer([('Scale',MinMaxScaler(),features_to_scale)],remainder='passthrough')\nX_train_ct = ct.fit_transform(X_train)\nX_test_ct = ct.transform(X_test)\n\nfig, axs = plt.subplots(2,4,figsize=(20,5))\nfor i in range(X_train_sc.shape[1]):\n    r,c = divmod(i,4)    \n    sns.histplot(X_train_ct[:,i], bins=nbins, ax=axs[r,c], color= 'blue', alpha=0.7,linewidth=0, stat='density').set_title(lcol[i])\n    sns.histplot(X_test_ct[:,i], bins=nbins, ax=axs[r,c], color= 'red', alpha=0.7,linewidth=0, stat='density')","443939b5":"def get_cv_score(model, X, y):\n    scores = -1*cross_val_score(model,X,y,cv=5,scoring='neg_mean_absolute_error')\n    return scores.mean()","1d5b2ad5":"def get_summary(model):\n    sc = {}\n    sc['Unscaled'] = get_cv_score(model, X_train, y_train)\n    sc['Scaled'] = get_cv_score(model, X_train_sc, y_train)\n    sc['PartScaled'] = get_cv_score(model, X_train_ct, y_train)\n\n    model.fit(X_train_ct,y_train)\n    y_predict = model.predict(X_test_ct)\n    sc['RMSE'] = mean_squared_error(y_test,y_predict,squared=False)\n    sc['R2'] = r2_score(y_test,y_predict)\n        \n    return sc, y_predict","3ef08152":"pd.set_option('precision',1)\nzz, ylr = get_summary(LinearRegression())\ndd = pd.DataFrame({'LinearRegression':zz.values()}, index = zz.keys())\n\nzz, yrf = get_summary(RandomForestRegressor())\ndd['RandomForest'] = zz.values()\nzz, ysvr = get_summary(SVR(kernel='linear'))\ndd['SupportVector'] = zz.values()\n\npp = Pipeline([('poly',PolynomialFeatures(degree=2)),('lr',LinearRegression(fit_intercept=False))])\nzz, ypp = get_summary(pp)\ndd['2ndPoly'] = zz.values()\n\nprint(dd)","467d1f21":"plt.figure(figsize=(10,10))\nplt.scatter(y_test,ylr,alpha=0.9,marker='o',s=150)\nplt.scatter(y_test,yrf,alpha=0.7,marker='s',s=120)\nplt.scatter(y_test,ysvr,alpha=0.6,marker='p',s=80)\nplt.scatter(y_test,ypp,alpha=0.5,marker='d',s=50)\nplt.legend(dd.keys())\nplt.xlim([0.4,1])\nplt.ylim([0.4,1])\nplt.grid()\nplt.show()","b402b5bf":"print('RMSE from linear regression for last 100 rows is:\\n')\nprint(dd.loc['RMSE','LinearRegression'])","60ddc68c":"**Preprocessing**","11c14761":"**Profiling the data**","2f37d2dd":"# About the data set\n\n**Reference:** \nMohan S Acharya, Asfia Armaan, Aneeta S Antony : A Comparison of Regression Models for Prediction of Graduate Admissions, IEEE International Conference on Computational Intelligence in Data Science 2019\n\nThe dataset contains several parameters which are considered important during the application for Masters Programs.\nThe parameters included are :\n\n* GRE Scores ( out of 340 )\n* TOEFL Scores ( out of 120 )\n* University Rating ( out of 5 )\n* Statement of Purpose and Letter of Recommendation Strength ( out of 5 )\n* Undergraduate GPA ( out of 10 )\n* Research Experience ( either 0 or 1 )\n* Chance of Admit ( ranging from 0 to 1 )","84e3de7a":"# Knowing the data","59b6a3c0":"**Build functions**","f24eef9e":"**Scaling train and test data**","c735672b":"**Read in the data sets**","ccb5804e":"# The infamous brain drain \n\nOver the past multiple decades, India has continually experienced human capital migration out of the country - with the most popular destination being the Middle East, the United States, the United Kingdom, Canada and Asia Pac among other regions\/countries [1]. The figure below, from Pew Research, shows the emigration numbers for India as of 2017. Even though the emigration rate for India is low at about 1%, the number of migrants has more than doubled over the past 25 years, growing twice as fast as the world\u2019s total migrant population.\n\n![Pew_India_Emigration_2017.PNG](attachment:3e1ec2d2-a0d4-49d9-9252-29c78d9e0dbd.PNG)\n\nWhile the Middle East is a popular destination country for the working class, the West is a magnet for those pursuing higher education as a means to better opportunities outside of home. Among those coming to the US for higher education, the most popular fields of study happen to be engineering, math & computational sciences and social sciences. The recent digital boom in India did see a reversal in the emigration trends - but the trends seem to be reversing back.\n\nThis analysis is hoping to identify trends between admission to a Master's level graduate program in the US given various admission metrics.\n\n**Reference:**\n1. https:\/\/www.pewresearch.org\/global\/interactives\/global-migrant-stocks-map\/"}}