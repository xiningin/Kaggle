{"cell_type":{"aa689f03":"code","558848b3":"code","041a9b86":"code","c0e75fea":"code","3b50413d":"code","aeea72fb":"code","17315423":"code","d061bbfc":"code","c67906a0":"code","c19a7a69":"code","ae292c81":"code","eb45bd4a":"code","af9be9cc":"code","3d780279":"code","b2337de9":"markdown"},"source":{"aa689f03":"import math\nimport torch\nimport torchvision\nfrom torch import nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as T\nfrom scipy.optimize import linear_sum_assignment\nfrom torchvision.models._utils import IntermediateLayerGetter","558848b3":"class PositionEmbeddingSine(nn.Module):\n    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n        self.normalize = normalize\n        if scale is not None and normalize is False:\n            raise ValueError(\"normalize should be True if scale is passed\")\n        if scale is None:\n            scale = 2 * math.pi\n        self.scale = scale\n\n    def forward(self, x, mask):\n        assert mask is not None\n        not_mask = ~mask\n        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n        if self.normalize:\n            eps = 1e-6\n            y_embed = y_embed \/ (y_embed[:, -1:, :] + eps) * self.scale\n            x_embed = x_embed \/ (x_embed[:, :, -1:] + eps) * self.scale\n\n        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n        dim_t = self.temperature ** (2 * (dim_t \/\/ 2) \/ self.num_pos_feats)\n\n        pos_x = x_embed[:, :, :, None] \/ dim_t\n        pos_y = y_embed[:, :, :, None] \/ dim_t\n        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n        return pos\n\nclass MLP(nn.Module):\n    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n        super().__init__()\n        self.num_layers = num_layers\n        h = [hidden_dim] * (num_layers - 1)\n        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n\n    def forward(self, x):\n        for i, layer in enumerate(self.layers):\n            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n        return x\n\nclass FrozenBatchNorm2d(torch.nn.Module):\n    def __init__(self, n):\n        super(FrozenBatchNorm2d, self).__init__()\n        self.register_buffer(\"weight\", torch.ones(n))\n        self.register_buffer(\"bias\", torch.zeros(n))\n        self.register_buffer(\"running_mean\", torch.zeros(n))\n        self.register_buffer(\"running_var\", torch.ones(n))\n\n    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                              missing_keys, unexpected_keys, error_msgs):\n        num_batches_tracked_key = prefix + 'num_batches_tracked'\n        if num_batches_tracked_key in state_dict:\n            del state_dict[num_batches_tracked_key]\n\n        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n            state_dict, prefix, local_metadata, strict,\n            missing_keys, unexpected_keys, error_msgs)\n\n    def forward(self, x):\n        # move reshapes to the beginning\n        # to make it fuser-friendly\n        w = self.weight.reshape(1, -1, 1, 1)\n        b = self.bias.reshape(1, -1, 1, 1)\n        rv = self.running_var.reshape(1, -1, 1, 1)\n        rm = self.running_mean.reshape(1, -1, 1, 1)\n        eps = 1e-5\n        scale = w * (rv + eps).rsqrt()\n        bias = b - rm * scale\n        return x * scale + bias","041a9b86":"class BoxUtils(object):\n    @staticmethod\n    def box_cxcywh_to_xyxy(x):\n        x_c, y_c, w, h = x.unbind(-1)\n        b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n             (x_c + 0.5 * w), (y_c + 0.5 * h)]\n        return torch.stack(b, dim=-1)\n\n    @staticmethod\n    def box_xyxy_to_cxcywh(x):\n        x0, y0, x1, y1 = x.unbind(-1)\n        b = [(x0 + x1) \/ 2, (y0 + y1) \/ 2,\n             (x1 - x0), (y1 - y0)]\n        return torch.stack(b, dim=-1)\n\n    @staticmethod\n    def rescale_bboxes(out_bbox, size):\n        img_h, img_w = size\n        b = BoxUtils.box_cxcywh_to_xyxy(out_bbox)\n        b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n        return b\n\n    @staticmethod\n    def box_area(boxes):\n        \"\"\"\n        Computes the area of a set of bounding boxes, which are specified by its\n        (x1, y1, x2, y2) coordinates.\n        Arguments:\n            boxes (Tensor[N, 4]): boxes for which the area will be computed. They\n                are expected to be in (x1, y1, x2, y2) format\n        Returns:\n            area (Tensor[N]): area for each box\n        \"\"\"\n        return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n        \n    @staticmethod\n    # modified from torchvision to also return the union\n    def box_iou(boxes1, boxes2):\n        area1 = BoxUtils.box_area(boxes1)\n        area2 = BoxUtils.box_area(boxes2)\n\n        lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n        rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n\n        wh = (rb - lt).clamp(min=0)  # [N,M,2]\n        inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n\n        union = area1[:, None] + area2 - inter\n\n        iou = inter \/ union\n        return iou, union\n\n    @staticmethod\n    def generalized_box_iou(boxes1, boxes2):\n        \"\"\"\n        Generalized IoU from https:\/\/giou.stanford.edu\/\n        The boxes should be in [x0, y0, x1, y1] format\n        Returns a [N, M] pairwise matrix, where N = len(boxes1)\n        and M = len(boxes2)\n        \"\"\"\n        # degenerate boxes gives inf \/ nan results\n        # so do an early check\n        assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n        assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n        iou, union = BoxUtils.box_iou(boxes1, boxes2)\n\n        lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n        rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n\n        wh = (rb - lt).clamp(min=0)  # [N,M,2]\n        area = wh[:, :, 0] * wh[:, :, 1]\n\n        return iou - (area - union) \/ area","c0e75fea":"class HungarianMatcher(nn.Module):\n    \"\"\"This class computes an assignment between the targets and the predictions of the network\n    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n    while the others are un-matched (and thus treated as non-objects).\n    \"\"\"\n\n    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):\n        \"\"\"Creates the matcher\n        Params:\n            cost_class: This is the relative weight of the classification error in the matching cost\n            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n        \"\"\"\n        super().__init__()\n        self.cost_class = cost_class\n        self.cost_bbox = cost_bbox\n        self.cost_giou = cost_giou\n        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n\n    @torch.no_grad()\n    def forward(self, outputs, targets):\n        \"\"\" Performs the matching\n        Params:\n            outputs: This is a dict that contains at least these entries:\n                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n                           objects in the target) containing the class labels\n                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n        Returns:\n            A list of size batch_size, containing tuples of (index_i, index_j) where:\n                - index_i is the indices of the selected predictions (in order)\n                - index_j is the indices of the corresponding selected targets (in order)\n            For each batch element, it holds:\n                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n        \"\"\"\n        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n\n        # We flatten to compute the cost matrices in a batch\n        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n\n        # Also concat the target labels and boxes\n        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n\n        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n        # but approximate it in 1 - proba[target class].\n        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n        cost_class = -out_prob[:, tgt_ids]\n\n        # Compute the L1 cost between boxes\n        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n\n        # Compute the giou cost betwen boxes\n        cost_giou = -BoxUtils.generalized_box_iou(\n            BoxUtils.box_cxcywh_to_xyxy(out_bbox),\n            BoxUtils.box_cxcywh_to_xyxy(tgt_bbox)\n        )\n\n        # Final cost matrix\n        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n        C = C.view(bs, num_queries, -1).cpu()\n\n        sizes = [len(v[\"boxes\"]) for v in targets]\n        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n\nclass SetCriterion(nn.Module):\n    \"\"\" This class computes the loss for DETR.\n    The process happens in two steps:\n        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n        2) we supervise each pair of matched ground-truth \/ prediction (supervise class and box)\n    \"\"\"\n    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n        \"\"\" Create the criterion.\n        Parameters:\n            num_classes: number of object categories, omitting the special no-object category\n            matcher: module able to compute a matching between targets and proposals\n            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n            eos_coef: relative classification weight applied to the no-object category\n            losses: list of all the losses to be applied. See get_loss for list of available losses.\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.matcher = matcher\n        self.weight_dict = weight_dict\n        self.eos_coef = eos_coef\n        self.losses = losses\n        empty_weight = torch.ones(self.num_classes + 1)\n        empty_weight[-1] = self.eos_coef\n        self.register_buffer('empty_weight', empty_weight)\n\n    def loss_labels(self, outputs, targets, indices, num_boxes):\n        \"\"\"Classification loss (NLL)\n        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n        \"\"\"\n        assert 'pred_logits' in outputs\n        src_logits = outputs['pred_logits']\n\n        idx = self._get_src_permutation_idx(indices)\n        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n                                    dtype=torch.int64, device=src_logits.device)\n        target_classes[idx] = target_classes_o\n\n        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n        losses = {'loss_ce': loss_ce}\n        return losses\n\n    @torch.no_grad()\n    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n        \"\"\"\n        pred_logits = outputs['pred_logits']\n        device = pred_logits.device\n        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n        losses = {'cardinality_error': card_err}\n        return losses\n\n    def loss_boxes(self, outputs, targets, indices, num_boxes):\n        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n        \"\"\"\n        assert 'pred_boxes' in outputs\n        idx = self._get_src_permutation_idx(indices)\n        src_boxes = outputs['pred_boxes'][idx]\n        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n\n        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n\n        losses = {}\n        losses['loss_bbox'] = loss_bbox.sum() \/ num_boxes\n\n        loss_giou = 1 - torch.diag(BoxUtils.generalized_box_iou(\n            BoxUtils.box_cxcywh_to_xyxy(src_boxes),\n            BoxUtils.box_cxcywh_to_xyxy(target_boxes))\n        )\n        losses['loss_giou'] = loss_giou.sum() \/ num_boxes\n        return losses\n\n    def _get_src_permutation_idx(self, indices):\n        # permute predictions following indices\n        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n        src_idx = torch.cat([src for (src, _) in indices])\n        return batch_idx, src_idx\n\n    def _get_tgt_permutation_idx(self, indices):\n        # permute targets following indices\n        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n        return batch_idx, tgt_idx\n\n    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n        loss_map = {\n            'labels': self.loss_labels,\n            'cardinality': self.loss_cardinality,\n            'boxes': self.loss_boxes,\n        }\n        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n\n    def forward(self, outputs, targets):\n        \"\"\" This performs the loss computation.\n        Parameters:\n             outputs: dict of tensors, see the output specification of the model for the format\n             targets: list of dicts, such that len(targets) == batch_size.\n                      The expected keys in each dict depends on the losses applied, see each loss' doc\n        \"\"\"\n        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n\n        # Retrieve the matching between the outputs of the last layer and the targets\n        indices = self.matcher(outputs_without_aux, targets)\n\n        # Compute the average number of target boxes accross all nodes, for normalization purposes\n        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n\n        # Compute all the requested losses\n        losses = {}\n        for loss in self.losses:\n            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n\n        return losses","3b50413d":"class Backbone(nn.Module):\n    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n    def __init__(self, name: str,\n                train_backbone = False,\n                pretrained = True,\n                return_interm_layers = False,\n                dilation = False,\n                norm_layer = None,\n                hidden_dim = 256):\n        super().__init__()\n        backbone = getattr(torchvision.models, name)(\n            replace_stride_with_dilation = [False, False, dilation],\n            pretrained = pretrained,\n            norm_layer = norm_layer or FrozenBatchNorm2d\n        )\n        self.num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n\n        for name, parameter in backbone.named_parameters():\n            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n                parameter.requires_grad_(False)\n        if return_interm_layers:\n            return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n        else:\n            return_layers = {'layer4': \"0\"}\n        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n\n        N_steps = hidden_dim \/\/ 2\n        self.position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n\n    def forward(self, x, mask = None):\n        mask = mask or torch.zeros_like(x[:,0], dtype = torch.bool)\n        xs = self.body(x)\n        out: Dict[str, NestedTensor] = {}\n        for name, x in xs.items():\n            m = F.interpolate(mask[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n            pos = self.position_embedding(x, m).to(x.dtype)\n            out[name] = (x, m, pos)\n        return out[self.body.return_layers[\"layer4\"]]","aeea72fb":"class DETR(nn.Module):\n    \"\"\" This is the DETR module that performs object detection \"\"\"\n    def __init__(self, num_classes, num_queries, backbone = None, transformer = None):\n        super().__init__()\n        self.num_queries = num_queries\n        self.transformer = transformer or nn.Transformer(\n            d_model = 256, #hidden_dim\n            nhead = 8,\n            num_encoder_layers = 6,\n            num_decoder_layers = 6,\n            dim_feedforward = 2048,\n            dropout = 0.1,\n            activation = \"gelu\", \n        )\n        hidden_dim = self.transformer.d_model\n        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n\n        self.backbone = backbone or Backbone(\n            name = \"resnet34\",\n            train_backbone = True,\n            pretrained = True,\n            return_interm_layers = True,\n            hidden_dim = hidden_dim\n        )\n        self.input_proj = nn.Conv2d(self.backbone.num_channels, hidden_dim, kernel_size=1)\n\n    def forward(self, x):\n        features, mask, pos_embed = self.backbone(x)\n        src = self.input_proj(features).flatten(2).permute(2, 0, 1)\n        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n        mask = mask.flatten(1)\n \n        query_embed = self.query_embed.weight.unsqueeze(1).repeat(1, x.size(0), 1)\n        tgt = torch.zeros_like(query_embed)\n        hs = self.transformer(\n            src = src + pos_embed,\n            src_key_padding_mask = mask,\n            memory_key_padding_mask = mask,\n            tgt = tgt + query_embed\n        )\n        outputs_class = self.class_embed(hs)\n        outputs_coord = self.bbox_embed(hs).sigmoid()\n        out = {'pred_logits': outputs_class.permute(1, 0, 2), 'pred_boxes': outputs_coord.permute(1, 0, 2)}\n        return out","17315423":"num_classes = 5\nnum_queries = 100\nmodel = DETR(num_classes, num_queries)\nmatcher = HungarianMatcher(cost_class = 1, cost_bbox = 5, cost_giou = 2)\nweight_dict = {'loss_ce': 1, 'loss_bbox': 5, 'loss_giou': 2}\nlosses = ['labels', 'boxes', 'cardinality']\ncriterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict, eos_coef=0.1, losses=losses)","d061bbfc":"imgs = torch.randn((2,3,256,256))\noutputs = model(imgs)","c67906a0":"targets = [\n    {\n        \"boxes\":torch.tensor([[0.1,0.2,0.3,0.6], [0.5,0.6,0.7,0.8]]+[[0.0, 0.0, 1.0, 1.0] for i in range(98)]),\n        \"labels\": torch.tensor([1, 1]+[num_classes for i in range(98)])\n    },\n    {\n        \"boxes\":torch.tensor([[0.1,0.2,0.3,0.6], [0.5,0.6,0.7,0.8]]+[[0.0, 0.0, 1.0, 1.0] for i in range(98)]),\n        \"labels\": torch.tensor([1, 1]+[num_classes for i in range(98)])\n    }\n]","c19a7a69":"losses = criterion(outputs, targets)\nsum(losses.values()), losses","ae292c81":"class AttentionVisualizer:\n    def __init__(self, model):\n        self.model = model\n        self.model.eval()\n        self.transforms = T.Compose([\n            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n\n    def set_hooks(self, conv_features, enc_attn_weights, dec_attn_weights):\n        hooks = [\n            model.backbone.body.register_forward_hook(\n                lambda self, input, output: conv_features.append(output)\n            ),\n            model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n                lambda self, input, output: enc_attn_weights.append(output[1])\n            ),\n            model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n                lambda self, input, output: dec_attn_weights.append(output[1])\n            )\n        ]\n        return hooks\n\n    def compute_features(self, image):\n        # use lists to store the outputs via up-values\n        conv_features, enc_attn_weights, dec_attn_weights = [], [], []\n        # set up hooks\n        hooks = self.set_hooks(conv_features, enc_attn_weights, dec_attn_weights)\n        with torch.no_grad():\n            # propagate through the model\n            outputs = self.model(self.transforms(image)[None])\n        for hook in hooks:\n            hook.remove()\n        # don't need the list anymore\n        conv_features = conv_features[0]\n        enc_attn_weights = enc_attn_weights[0]\n        dec_attn_weights = dec_attn_weights[0]\n        # get the HxW shape of the feature maps of the CNN\n        conv_features = conv_features[self.model.backbone.body.return_layers[\"layer4\"]]\n        shape = conv_features.shape[-2:]\n        # and reshape the self-attention to a more interpretable shape\n        enc_attn_weights = enc_attn_weights.reshape((-1,) + shape + shape)\n        dec_attn_weights = dec_attn_weights.reshape((-1, dec_attn_weights.size(1)) + shape)\n        boxes, scores = outputs[\"pred_boxes\"][0], outputs[\"pred_logits\"][0].softmax(-1)\n        boxes = BoxUtils.rescale_bboxes(boxes, image.shape[-2:])\n        return conv_features[0], enc_attn_weights[0], dec_attn_weights[0], boxes, scores\n\n    def visualize_image_with_bboxes(self, ax, image, boxes, scores, loc, query_idx):\n        row = int(loc[0] * image.size(1))-1\n        col = int(loc[1] * image.size(2))-1\n        image = image.permute(1, 2, 0).numpy() # [H,W,C]\n        ax.imshow(image)\n        ax.add_patch(plt.Circle((row, col), 16, color='r'))\n        for idx, (box, score) in enumerate(zip(boxes, scores), start = 1):\n            color = 'blue'if idx == query_idx else \"red\"\n            xmin, ymin, xmax, ymax = box\n            box = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color =color, linewidth=3)\n            ax.add_patch(box)\n            ax.text(xmin, ymin, f\"{score.max():.2f}\", fontsize=15, bbox=dict(facecolor='yellow', alpha=0.5))\n        ax.axis('off')\n        ax.set_title(\"Image\")\n\n    def visualize_self_attention(self, ax, sattn, loc, size, sattn_direction = False):\n        # convert reference point to absolute coordinates\n        row = int(loc[0] * size[0])\n        col = int(loc[1] * size[1])\n        # compute the downsampling factor for the model\n        fact = 2 ** round(math.log2(size[-1] \/ sattn.shape[-1]))\n        # round the position at the downsampling factor\n        idx = (row \/\/ fact-1, col \/\/ fact-1)\n        sattn_map = sattn[idx[0], idx[1], ...] if sattn_direction else sattn[..., idx[0], idx[1]]\n        ax.imshow(sattn_map, cmap='cividis', interpolation='nearest')\n        ax.add_patch(plt.Circle((idx[1],idx[0]), 0.5, color='r'))\n        ax.axis('off')\n        ax.set_title(f'Self Attention {(row, col)}')\n\n    def visualize_multihead_attention(self, ax, mattn, query_idx):    \n        ax.imshow(mattn[query_idx-1])\n        ax.axis('off')\n        ax.set_title(f'Query Id: {query_idx}')","eb45bd4a":"viz = AttentionVisualizer(model)","af9be9cc":"conv_features, enc_attn_weights, dec_attn_weights, boxes, scores = viz.compute_features(imgs[0])","3d780279":"fig, axs = plt.subplots(ncols=3, nrows=1, figsize=(5*3, 5*1), dpi = 100)\nloc = (0.5, 0.5); query = 100\nviz.visualize_image_with_bboxes(axs[0], imgs[0], boxes, scores, loc, query)\nviz.visualize_self_attention(axs[1], enc_attn_weights, loc, (256, 256))\nviz.visualize_multihead_attention(axs[2], dec_attn_weights, query)\nfig.tight_layout()\nplt.show()","b2337de9":"[DETR Facebook Github](https:\/\/github.com\/facebookresearch\/detr.git)"}}