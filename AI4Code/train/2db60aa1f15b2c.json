{"cell_type":{"9a233e3d":"code","b82196bb":"code","02c296f7":"code","01cc3af2":"code","d0cd797d":"code","0b186831":"code","1ca45ff0":"code","50cb58f1":"code","0e76170d":"code","20cc7e6d":"code","381ef394":"code","e73917f0":"code","136d5d3a":"code","976ce2bd":"code","d67af245":"code","8d24db01":"code","56a97991":"code","b6617fdf":"code","7fff0a39":"code","3e7328f9":"code","81b09bc6":"code","9479ea50":"code","2de30b88":"code","a61b86d0":"code","478d4cb0":"code","408cf3b4":"code","2cad5f4c":"code","cc75b35f":"code","77c4a21f":"code","8259a011":"code","aaedcc9f":"code","a9de4d5e":"code","29c38d0e":"code","2f2ca769":"code","81c849f2":"code","2474695a":"code","d9f58462":"code","99b40a1a":"code","2c27a4e0":"code","c8579d4f":"code","12da8e99":"code","f22cf019":"code","4f4649d1":"code","4f0fc735":"code","937b3da6":"markdown","385c4107":"markdown","2dbe4e11":"markdown","2169a482":"markdown","b6fa13eb":"markdown","8131abd4":"markdown","3d231656":"markdown","b11a8d13":"markdown","0f267bf7":"markdown","4792f54d":"markdown","461773ca":"markdown","a36a5f15":"markdown","c27c0782":"markdown","706427ce":"markdown","e6387fea":"markdown","82f0643d":"markdown","831f3b9a":"markdown","e12a6ebe":"markdown","57c9f17f":"markdown","2c747a6f":"markdown","bafc4c07":"markdown","ddf99cf6":"markdown","aa58923e":"markdown","1ef33fe0":"markdown","9e509909":"markdown","66eb15f0":"markdown","f1e204e9":"markdown","d474fa09":"markdown","ec08096e":"markdown","3bc03ad7":"markdown","158a8fd3":"markdown","2c54e7d8":"markdown","f5fd5081":"markdown","cb39f2d5":"markdown","e44c42e8":"markdown","acd2507f":"markdown","f2aad6c3":"markdown","782ec28e":"markdown","5e4f6f5e":"markdown","58fdfb4d":"markdown","b2e60587":"markdown","bdc0a2cb":"markdown","f85acb31":"markdown","dacfcb0b":"markdown","6cb72fe3":"markdown","c2b5f847":"markdown","4be6572f":"markdown","107f4de5":"markdown","6d3c0b12":"markdown"},"source":{"9a233e3d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy\nfrom seaborn import load_dataset\nimport seaborn as sns","b82196bb":"# Loading the data set\n#diamonds = sns.load_dataset(\"diamonds\")\n# issues with Kaggle, use their data sets\ndiamonds = pd.read_csv(\"..\/input\/diamonds\/diamonds.csv\")\ndiamonds.head()","02c296f7":"#?diamonds","01cc3af2":"diamonds.rename(columns = {'x': 'length', 'y': 'width', 'z': 'height'}, inplace = True)\ndiamonds.head()","d0cd797d":"diamonds.describe()","0b186831":"diamonds.groupby(\"cut\").mean()","1ca45ff0":"# use vectorised cython functions instead.\ndiamonds.groupby([\"cut\", \"color\"]).mean()","50cb58f1":"diamonds.groupby(\"clarity\").mean()","0e76170d":"# unnamed must be removed, leftover from rownames ie index\n#diamonds.apply(lambda x: sum(x.notnull())\/len(x) * 100)\n# rewrite with transform\n#diamonds.dtypes\n# I conclude that there is no(as far as I know and have researched) more concise alternative to using apply\n#diamonds[diamonds.notnull()].groupby([\"cut\", \"color\", \"clarity\"]).transform(\"sum\")\ndiamonds.notnull().sum() \/ len(diamonds) * 100","20cc7e6d":"%matplotlib inline\nplt.figure()\nplt.hist(x = \"price\", data = diamonds, color = \"indianred\", alpha = 0.8)\nplt.title(\"Histogram of Prices\", fontsize = 15)\nplt.ylabel(\"Frequency\", fontsize = 15)\nplt.xlabel(\"Prices(USD)\", fontsize = 15)","381ef394":"from scipy.stats import skew\nskew(diamonds[\"price\"])","e73917f0":"from IPython.display import display\ndisplay(np.mean(diamonds[\"price\"]))\ndisplay(np.median(diamonds[\"price\"]))","136d5d3a":"sns.catplot(x=\"cut\", y=\"price\",kind=\"bar\", data = diamonds)\nplt.title(\"Prices by cut\", fontsize = 15)","976ce2bd":"sorted_data = diamonds.sort_values(by = [\"price\"] , ascending = False)\n#sorted_data.head()\n# probably a better way exists that leverages col_order\n# This sorts the x-axis in alphabetical order which is a bit less informative\nsns.catplot(x = \"cut\", y=\"price\", data = sorted_data, kind = \"bar\")\n","d67af245":"plt.scatter(x = \"carat\", y = \"price\", data = diamonds, color = \"indianred\")\nplt.title(\"Prices by carat\", fontsize = 15)\nplt.xlabel(\"Carat\", fontsize = 14)\nplt.ylabel(\"Price\", fontsize = 14)","8d24db01":"diamonds[diamonds[\"carat\"] >=5]","56a97991":"diamonds.query(\"carat >=5\")","b6617fdf":"sns.boxplot(x=\"cut\", y=\"price\", data = diamonds, palette = \"RdBu\")\nplt.title(\"Price Distribution by Cut\", fontsize = 15, fontweight = \"bold\")\nplt.xlabel(\"cut\",size=14)\nplt.ylabel(\"price\", size = 15)","7fff0a39":"filtered = diamonds.query(\"price > 10000\")\n\nsns.boxplot(x = \"cut\", y = \"price\", data = filtered, palette = \"RdBu\")","3e7328f9":"facets = sns.FacetGrid(col = \"cut\", row = \"clarity\", data = diamonds)\nfacets.map(plt.hist, \"price\")\nplt.title(\"Price Distribution Plot by Cut and Clarity\")\n","81b09bc6":"column_order = sorted(diamonds.clarity.unique())\nsns.countplot(x=\"clarity\", data = diamonds.sort_values([\"price\", \"depth\"]), order = column_order)","9479ea50":"sns.violinplot(x=\"clarity\", y=\"price\", data = diamonds.query(\"price > 10000\").sort_values(\"price\"),\n              order = column_order)","2de30b88":"diamonds.groupby(\"clarity\").mean()","a61b86d0":"#diamonds.clarity, order = [\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\",\"SI2\",\"I1\"])\n# convert to categorical\n#pd.Categorical(diamonds[\"clarity\"], categories= [\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\",\"SI2\",\"I1\"] )\n# This is probbaly computationally expensive\ncategorize_data = pd.CategoricalDtype(categories= [\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\",\"SI2\",\"I1\"])\ndiamonds[\"clairty\"] = pd.Series(diamonds.clarity, dtype = categorize_data)\ndiamonds.groupby(\"clairty\").mean()","478d4cb0":"# Ok, seaborn's x axis really needs an upgrade. It is currently in my opinion less flexible.\nsns.catplot(x = \"clarity\", y=\"price\", data =  diamonds, kind = \"bar\",\n          col_order = [\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\",\"SI2\",\"I1\"])","408cf3b4":"from sklearn import linear_model\nfrom sklearn.model_selection import train_test_split","2cad5f4c":"# Make dependent and predictor variables\ndependent_variable = diamonds[\"price\"]\npredictors = diamonds.drop(\"price\", axis = 1)\ndisplay(dependent_variable.describe())\n#display(predictors.columns)\ndisplay(predictors.head(5))","cc75b35f":"# split our data into test\/train\nx_train, x_test, y_train, y_test = train_test_split(predictors, \n                                                    dependent_variable, test_size = 0.2, random_state = 101)\nprint(len(x_train),len(x_test), len(y_train), len(y_test) )\n","77c4a21f":"from sklearn.preprocessing import LabelEncoder\nx_train.dtypes","8259a011":"# apply the encoder on cut, color and clarity\n# Instantiate the Encoder\nencoder = LabelEncoder()\n# select_if(is.object, col)\nto_encode = x_train.select_dtypes(exclude = 'float').columns.values\nx_train[list(to_encode)] = x_train[to_encode].apply(encoder.fit_transform)\nx_test[list(to_encode)] = x_test[to_encode].apply(encoder.fit_transform)\nx_train.head()","aaedcc9f":"diamonds[to_encode].astype(\"category\").apply(lambda x: x.cat.codes).head()","a9de4d5e":"# make a model object\nregressor = linear_model.LinearRegression()\n# Use the regressor to fit our model\nregressor.fit(x_train, y_train)\n","29c38d0e":"# predict on x_test\npredicted_price = regressor.predict(x_test)","2f2ca769":"display(\"R Squared value(Train): \", regressor.score(x_train, y_train) * 100)\ndisplay(\"R Squared value(Test): \", regressor.score(x_test, y_test) * 100)","81c849f2":"y_hat = predicted_price\n# sum squared residuals(sse)\nssr = sum((y_test - y_hat)**2)\n# sum squared total\nsst = sum((y_test - np.mean(y_test)) **2 )\nr_squared = 1-(ssr\/sst)\n# compute the adjusted r squared\nadj_r_squared =    1 - (1-regressor.score(x_test, y_test))*(len(y_test)-1)\/(len(y_test)- x_test.shape[1]-1)\nprint(r_squared, adj_r_squared)","2474695a":"from sklearn import metrics","d9f58462":"display(\"MSE: \", metrics.mean_squared_error(predicted_price, y_test))\ndisplay(\"MAE: \", metrics.mean_absolute_error(predicted_price, y_test))\n","99b40a1a":"corrs = diamonds.corr()\n\ncorrs","2c27a4e0":"sns.heatmap(data = corrs, square = True, cmap = [\"dodgerblue\", \"lightgreen\", \"gray\"], annot = True)","c8579d4f":"from sklearn.feature_selection import f_regression","12da8e99":"f_value, p_value = f_regression(x_train, y_train)\ndata = pd.DataFrame([f_value, x_train.columns.values, p_value]).T\ndata.columns = [\"f_value\", \"predictor\", \"p_value\"]\nsorted_data = data.sort_values(by = \"p_value\", ascending = False)\nsorted_data","f22cf019":"sorted_data[\"signif\" ] = np.where(sorted_data[\"p_value\"] > 0.05 , \"ns\", \"sf\")\nsorted_data[np.where(sorted_data[\"signif\"] == \"sf\", True, False)].drop([10,0])","4f4649d1":"to_index = list(sorted_data[\"predictor\"].values)\n# cannot use drop_dupes for some reason\ndel to_index[2]\n# display correlations with price\ncorrs[[x for x in to_index if x in corrs.columns]].loc[\"price\"]","4f0fc735":"# Points clustered together. Might be best to reduce this clustering\n# Decided to filter for only highly priced diamonds\n# perhaps add some noise(jitter?)\n\nsns.lmplot(x = \"carat\", y = \"price\", data = diamonds.query(\"price >= 10000\"), hue = \"clarity\")","937b3da6":"There are several ways to find missing data, but we will represent it as the percentage of each column that is missing.","385c4107":"We can now proceed to use our model for predictions.","2dbe4e11":"When I first made this plot, one could see that the internally flawless diamonds had a lower mean price than other levels of clarity. It is interesting however and as one might probably have expected to see that if we zoom in on the dataset to include higher prices, the mean price of the innternally flawless is higher than that of included(I1) and almost equal to that of the slightly included. \n\nThis therefore suggests that the price of a diamond is(logically) determined by more than just its clarity. \n\n## Flawless but cheap?\n","2169a482":"The above is probably a better approach since we now have our data sorted as per an expert in the diamond mining business would expect. Let's see what a plot of this data would look like.","b6fa13eb":"# Linear regression in the diamond mines\n\nIn order to carry out(build a model) regression analysis on our data, we need to import a popular Machine Learning library(package) known as [scikit-learn](https:\/\/scikit-learn.org\/stable\/index.html)(**sklearn**). In our case, we are more interested in ordinary least squares regression which is [documented here](https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#ordinary-least-squares). \n\n## Data Split","8131abd4":"Next, we need to split our dataset into a training and test\/validation set. We shall then split these into a y(dependent variable) and x(predictor variable(s)). In this case, our y will be the price of a diamond while for starters, we'll build a model that uses all our other attributes for predicting the price of a diamond.","3d231656":"From the above, we can see that the mean price was high. However, looking at these statistics is less informative since these may vary by cut, color, clarity or depth. Therefore looking at these statistics by group could be more informative.\n\n## Mean Prices by Cut","b11a8d13":"## Prices by cut\n\nFor this part, we shall uses `seaborn`'s awesome functions that deal with categorical plots.\n","0f267bf7":"## Towards Reality\n\nFrom the above, one can easily see a major flaw in this approach. Surely, not all the dataset's features will have a major impact on the model. We can turn to looking at correlations between the variables to see which are more correlated and use these to build more realistic models.","4792f54d":"We can plot the correlations based on one to one or one to many correlation.","461773ca":"From the above plot, we see a generally highly linear relationship between price and clarity\/carat. It is therefore possible that a model with these two features alone would likely provide strong predictive power for a dimaond's price. \n\n**It is worth mentioning however that linear regression does not mean a linear relationship between the target and the variables but rather the target and the coefficients of the variables ie in the equation y = mx + c, y is linearly correlated to m and not necessarily x.**\n\n\n## Future Steps\n\nIt would be close to impossible if not impossible to fully analyse and cover all important parts of linear regression. I therefore conclude this kernel(notebook) by thanking you for your time. As a bonus, here are a few interesting future steps that lack of space unfortunately prevents us from exploring.\n\n1. Build Models based on the most highly correlated variables\n\n2. Compare the above models with a model that uses all variables in the dataset.\n\n3. Use a Bayesian approach to statistical modeling to see how well this would compare to what we have.\n\n4. Improve the visualizations.\n\n5. Experiment with more \"advanced models\"(knns, svms, random forests, xgboost, adaboost, etc)\n\n6. Deploy  a real world model and see how this works on an actual real world problem.\n\n\n**Thank you for reading and do provide feeback on what works and what doesn't.** You can contact me via my Github [repo](https:\/\/github.com\/Nelson-Gon\/sweetpy) or by email(via [Kaggle](https:\/\/www.kaggle.com\/gonnel)'s) contact me .\n","a36a5f15":"The above plot gives a more detailed account of the price distribution with respect to cut and clarity. The majority of the diamonds appear to fall in the ideal and VS2 category.\n\nInteresting are the very low numbers of diamonds that fit I1 and good cut criterion. It is also surprising to see that clarity IF and fair cut almost has no diamonds. \n\nThis is surprising because IF(Internally flawless) diamonds would be expected to be more even with a fair cut. This might however imply that these are extremely rare diamonds.\n\nTo get a clearer picture, we could visualize the distribution by clarity.\n\n## Are some diamonds rare?","c27c0782":"## The Model\nHaving encoded our data, we can proceed with building our model. There are a number of libraries that have been written to achieve this but as alluded to earlier, we shall use `sklearn`. First we need to instantiate a regressor that we shall then use to fit our model. ","706427ce":"## Feature Importance and Selection\n\nHaving obtained these correlations, we can use them to decide which features are more highly correlated and therefore build our models based on these. We can utilise `sklearn`'s [`feature_selection`](https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html) module to help us better get feature importance. This is \"nice\" since it will provide us with p values and their respective significance. From the documentation, `f_regression` attempts to:\n```\nUnivariate linear regression tests.\n\nLinear model for testing the individual effect of each of many regressors.\nThis is a scoring function to be used in a feature selection procedure, not\na free standing feature selection procedure.\n\n\n1. The correlation between each regressor and the target is computed,\n   that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) \/ (std(X[:, i]) *\n   std(y)).\n2. It is converted to an F score then to a p-value.\n\n```","e6387fea":"Looking at the above table, we can  see that carat, length, width and height happen to have a high correlation with  price. Therefore, we can build our models around these features and see how well it performs. Let us visualize these and see. ","82f0643d":"## Histogram of Prices\n\nTo visualize the prices, we can either use `matplotlib`, `pandas` or `seaborn` amongst other libraries. We shall use `matplotlib`'s `hist`.","831f3b9a":"## Modules Used\n","e12a6ebe":" ## Zooming In\n \n We therefore take a look at these by \"zooming in\" on the data set as follows. It is interesting to note that the mean prices for this data set do not appear to follow a linear relationship. ","57c9f17f":"## Box Plots\nHaving looked at a few plots, perhaps looking at box and whiskers plots might provide more insight about the data set. What we are interested in is the distirbution of our data across various categories. Let's look at the clarity.","2c747a6f":"For completeness, we shall show a more pandas like way to achieve the above result. This achieves the same purpose in arguably more user-friendly code. This is especially useful if one is used to querying data bases.","bafc4c07":"As shown in [Mean Prices by Cut](#Mean-Prices-by-Cut), the prices do vary by cut and as already stated the relatioship appears to be linear ie as the quality(cut) of the diamond increases, so does its price. There will however always be outliers.  Let us take a look at the prices by carat.\n\n## Prices by carat","ddf99cf6":"Having taken a peek at our data set, there are a few things that need to be done. \nFirst, we need to have a general understanding of what the columns stand for. For instance, it would be hard to understand what x, y and z stood for unless you had prior knowledge about the data set.[](http:\/\/)","aa58923e":"## Basic Data Stats\n\nBefore, taking a look at the data's basic statistics, it might be useful to give the data more meanignful names. To do that, we shall use a data frame(`DataFrame`)'s `rename` method with a `dictionary` mapping.  Since the default is to set `inplace` to `False`, we can set that to `True`.","1ef33fe0":"Alternatively, we can simply convert our columns to panda's Categorical data type.","9e509909":"The above box plot does give a general idea of how the prices are distributed across the different cuts. As noted previously, there appears to be a linear relationship between cut and price. We however, see a few points that do not exactly fall within the range ie they are above the upper quartile(outliers).","66eb15f0":"From the above we it appears(as one might expect), the distribution of prices is **skewed to the right** ie as the prices increase, the number of diamonds decrease. We can carry out a skewness test using `scipy`'s `stats` module to verify this.\n\n## Skewness Test\n","f1e204e9":"The focus of our analysis has upto now been on the mean prices. It is worth noting what the clarity actually [means](https:\/\/ggplot2.tidyverse.org\/reference\/diamonds.html): **a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))**. With that in mind, the mean prices appear to follow a non linear relationship with respect to clarity. We will verify this as the notebook progresses statistically. \n\nHaving looked at a few basic stats about the data(remember we focused entirely on the mean), we now take a different route. We need to visualize our data which might reveal more trends and perhaps provide a more informative picture. However, important for modeling purposes is whether we have missing data.\n\n## Where's the missing data?","d474fa09":" Since we are now more comfortable with the dataset's column names since they are more informative, we can proceed to look at the basic stats.","ec08096e":"The nature of the display does not allow for a great interpretation of the data. However, from the few fields shown, we can seee a clear difference in prices with respect to color. \n## What about clarity?","3bc03ad7":"However, a web [search](https:\/\/rstudio-pubs-static.s3.amazonaws.com\/316651_5c92e58ef8a343e4b3f618a7b415e2ad.html) does identify what the columns mean and x, y, z are as guessed above.\n","158a8fd3":"## Model Evaluation\n\nTo understand why we need to evaluate our models and why metrics are important, one ought to go back in time to a rather famous [quote](https:\/\/stats.stackexchange.com\/questions\/57407\/what-is-the-meaning-of-all-models-are-wrong-but-some-are-useful\/57414):\n\n>\n\n    \"Essentially, all models are wrong, but some are useful.\"\n\n    --- Box, George E. P.; Norman R. Draper (1987). Empirical Model-Building and Response Surfaces, p. 424, Wiley. ISBN 0471810339.\n \n \n Having said that, let us take a look at our model's R<sup>2<\/sup> on both the train and test datasets. This is known as the `score` in `sklearn`'s linear_model module. ","2c54e7d8":"Next, we need to split our data into a training and testing\/validation dataset  as shown below. This will enable us to test our model on \"unseen\" data and evaluate how well it performs. \n\nTo do the split, we can use `sklearn`'s `train_test_split` method that makes it quite easy to split the data.","f5fd5081":"From the above, we can see that all our columns in fact have 100% of the data which is great. We can then proceed with data visualization.\n\n## Data Visualization","cb39f2d5":"In general, the distribution of the prices is such that most of the diamonds fall in the lower half. On the issue of outliers, there are visible outliers for instance 5 carat diamonds which is a bit unusual. To get a better understanding of these outliers, we shall filter our data set for these outliers and get more info about them.\n\n## Outliers by carat\nWe see that there is only a single diamond with a carat greater than or equal to 5. Interesting is the fact that the price of this diamond is comparatively high especially given it is of just fair cut. ","e44c42e8":"From the above, our skew is a positive value(greater than 1) indicating a positive skewness which means that our mean is greater than the median. Let us look at our data's median and mean prices to verify.","acd2507f":"From the above, it is clearer that the mean prices for instance as one might have expected vary by cut. There is an almost linear relationship between cut and price. It is also interesting to note that the mean depth does not vary that much across the different cut(s).\n\nWe can take the above a step further by looking at how prices for instance vary when the data is grouped by both cut and color.\n\n## Mean Prices by Cut and Color","f2aad6c3":"However, to make the above plot more \"attractive\"\/informative\/intuitive, we could go a step further and sort the values before plotting them.","782ec28e":"Indeed, we see that internally flawless(IF) diamonds occupy a relatively small portion of the data set. This could perhaps be due to mining techniques. Indeed, [these are extremely rare diamonds](https:\/\/beyond4cs.com\/clarity\/if-and-fl\/).\n\n## Violin Plots\nWe can take this a step further to visualize price distribution by clarity. In this example, we are essentially zooming in on our data to get the distribution of diamonds whose price is greater than or equal to 10000 USD.","5e4f6f5e":"# Exploring the diamonds data set.\n\nThis IPython notebook takes a look at the diamonds data set. We first do some exploratory data analysis followed by some visualization and modeling. This notebook is also available at my GitHub repository [sweetpy](https:\/\/www.github.com\/Nelson-Gon\/sweetpy).\n\n* **Contents**\n   - [Modules Used](#Modules-Used)\n   - [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n      * [Data Attributes](#Data-Attributes)\n      * [Basic Data Stats](#Basic-Data-Stats)\n         * [Mean Prices by Cut](#Mean-Prices-by-Cut)\n         * [Mean Prices by Cut and Color](#Mean-Prices-by-Cut-and-Color)\n         * [What about clarity?](#What-about-clarity?)\n         * [Where's the missing data?](#Where's-the-missing-data?)\n   - [Data Visualization](#Data-Visualization)\n      * [Histogram of Prices](#Histogram-of-Prices)\n      * [Skewness Test](#Skewness-Test)\n      * [Prices by cut](#Prices-by-cut)\n      * [Prices by carat](#Prices-by-carat)\n      * [Outliers by carat](#Outliers-by-carat)\n      * [Box Plots](#Box-Plots)\n         * [Zooming In](#Zooming-In)\n      * [Prrices by cut and clarity](#Prices-by-cut-and-clarity)\n      * [Are some diamonds rare?](#Are-some-diamonds-rare?)\n      * [Violin Plots](#Violin-Plots)\n      * [Flawless but cheap?](#Flawless-but-cheap?)\n   - Modeling\n      * [Linear regression](#Linear-regression-in-the-diamond-mines)\n        * [Train-Test-Split](#Data-Split)\n        * [Label Encoding](#Label-Encoding)\n        * [The Model](#The-Model)\n        * [Model Evaluation](#Model-Evaluation)\n          * [What's in the R<sup>2<\/sup>?!](#What's-in-the-R-squared?!)\n        * [Approaching Reality](#Towards-Reality)\n            * [Feature Selection](#Feature-Importance-and-Selection)\n         \n    - [Future Steps and Feedback](#Future-Steps)","58fdfb4d":"Having split our data, we next fit our model. The approach we have taken is to predict the price using all features in the dataset. Such a model may or may not best explain our data. We shall see how to take a more informed \"guess\" later. \n\nBefore we can proceed with building our model, we need to solve one more isssue. Some of our columns are of type `object` which is not suitable for regression. We therefore need to encode this data. Again, we take advantage of `sklearn`'s prepocessing features to help us in doing so.\n\n## Label Encoding","b2e60587":"We can take the above a step further by using `sklearn`'s builtin metrics.","bdc0a2cb":"## Exploratory Data Analysis\n\nTo begin our analysis, we need to first get a general idea of what our  dataset looks like. To do this, we shall first take a look at a few common procedures in exploratory data analysis.\n","f85acb31":"### Data Attributes\n\nThere are a couple of ways to find out what the column mean: One could simply guess that x, y and z refer to the lengths, widths and heights of the diamonds. One could also do a simple web search. \n\nPreferably, one can take a look at the dataset's documentation as shown next. Regrettably, the data set is not well documented by the package authors.","dacfcb0b":"From the above, it appears that the most likely(highly correlated) variables with price are cut,clarity, table and color. We shall therefore fit a new model with only these predictors and see how well it works, but first how does this result relate to the correlations we obtained above? ","6cb72fe3":"To take the above a step further, we can sort the clarity in increasing or decreasing order as per the industry standards. ","c2b5f847":"## Prices by cut and clarity","4be6572f":"Having sorted our f_values and p_values, we need to make a choice on what our cut off point is for a statistically significant p_value. Due to convention(Bayesians will likely disagree), I will set the cut off point to 0.05 and choose only those predictors with a p_value less than 0.05 as being statistically significant. We can make this fancy as follows.","107f4de5":"We have seen(visually) how price is influence by cut but what happens when we add clarity to the story?","6d3c0b12":"# What's in the R squared?!\nHowever, [what does the R<sup>2<\/sup> tell us](http:\/\/people.duke.edu\/%7Ernau\/rsquared.htm)?! This value while not conclusive gives us an understanding of how much variance can be explained by our model. It is often assumed that the higher the R<sup>2<\/sup> value, the better the model. However, as you probably now know from the above link, this is often not the case.  A better metric is the adjusted R squared which we can calculate as shown here:"}}