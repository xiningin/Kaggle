{"cell_type":{"9f1229a5":"code","3a76072c":"code","60d0dd39":"code","15f4bcf7":"code","98a2cea3":"code","926e76bf":"code","2cdfeae3":"code","32663eb4":"code","3a8cf83f":"code","7b38a581":"code","004613e7":"code","04b6bd9d":"code","764d378c":"code","c3bb981e":"code","cc8aedb8":"code","09ddc81f":"code","49f56d9f":"code","db5cd59e":"code","bdadf189":"code","3bd2270a":"code","a9b6387c":"code","37956bcd":"code","aabc3504":"code","9701b048":"code","985b02f8":"code","0682fb90":"code","180034ef":"code","cf9d67cc":"code","f964fc21":"code","dacdcfb0":"code","35fffa3d":"code","56c7230f":"code","62f8c8e8":"code","b74797e5":"code","19a905e3":"code","5e4438bb":"code","946025b0":"code","788fc846":"code","df57dc67":"code","a1734f61":"code","4838b724":"code","7e055bf1":"code","4de83609":"code","ea974cc6":"code","e5c38db2":"code","18039dc3":"code","3c2b04a2":"code","61eb0778":"code","611597d6":"code","75dd3deb":"code","ab25f993":"code","d0e239de":"code","801ae222":"code","8e89e8d2":"code","492de3ea":"code","11d2c351":"code","77f0dcd9":"code","c7cc9e43":"code","3edb0739":"code","25f013fb":"code","25ae7177":"code","ad8c2953":"code","c89195c5":"code","e9c645aa":"code","f4853a7c":"code","405c826d":"markdown","5a43e8da":"markdown","34f386a0":"markdown","66ef27b9":"markdown","8552a92c":"markdown","4a659588":"markdown","852b7e1b":"markdown","dd2f8136":"markdown","5d2141a3":"markdown","141e9103":"markdown","6631d1c2":"markdown","3522df15":"markdown","0673e456":"markdown","c2031990":"markdown","ce154474":"markdown","36ed2a36":"markdown","bc98d7dc":"markdown","b97a9493":"markdown","6623a941":"markdown","0d5a1e8e":"markdown","78e08717":"markdown","a410b53b":"markdown","ae925d78":"markdown","f752a9fd":"markdown","18c79531":"markdown","73f2ee23":"markdown","48dc08b4":"markdown","a5febfb5":"markdown","6b822f03":"markdown","a2d4cc68":"markdown","fdcedd92":"markdown","69a79f0a":"markdown","e7c2d098":"markdown","f2ec8630":"markdown","09381b3d":"markdown","6e94e69c":"markdown","56ae46af":"markdown","654e87ce":"markdown","d9b91732":"markdown","b625dcc8":"markdown","c5ad3b2a":"markdown","12a39931":"markdown","3c5944d7":"markdown","e66b4f37":"markdown","4ac1685e":"markdown","5f4128ea":"markdown","c3b6e573":"markdown","ae355eae":"markdown","62cd51f3":"markdown","9fc97a18":"markdown","43f2fe08":"markdown","ac12c14e":"markdown","61d40526":"markdown","6d6878c6":"markdown","b2e34e08":"markdown","80d1f3b2":"markdown","02c164d7":"markdown","fd29f24b":"markdown","4cb9e3b3":"markdown"},"source":{"9f1229a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3a76072c":"# usual imports \nimport pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport random\n\n# assert the python version\nassert sys.version_info >= (3,5)\n\n# visualization imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# consistent plot size unless specified \nfrom pylab import rcParams\nrcParams['figure.figsize'] = 12,7\nrcParams['xtick.labelsize'] = 12\nrcParams['ytick.labelsize'] = 12\n\n# handle unwanted warnings\nimport warnings\nwarnings.filterwarnings(action='ignore',category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore',category=FutureWarning)\n\n# NLP \/ Text specific imports\nimport nltk\nimport spacy\nnlp = spacy.load('en_core_web_sm',disable=['parser','tagger','ner'])\n\n# NLP - Text Cleaning\nfrom nltk.tokenize import word_tokenize\nfrom nltk import sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\n# Word Cloud\nfrom wordcloud import WordCloud\n\n# text vectorization\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# regular expression\nimport re\n\n# string operations\nimport string\n\n# ML Models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n# cross validation\nfrom sklearn.model_selection import cross_val_score,cross_val_predict,KFold,RepeatedStratifiedKFold\n\n# Model Fine Tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# Pipeline\nfrom sklearn.pipeline import Pipeline\n\n# keras\nfrom keras.utils import to_categorical\n\n# Model evaluation metrics\nfrom sklearn.metrics import classification_report,f1_score,accuracy_score,precision_score,recall_score","60d0dd39":"# check the subdirectories under the root folder\ngen = os.walk('\/kaggle\/input')\nnext(gen)","15f4bcf7":"%%time\n# load the train and test data\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nprint(f'Train Data Size {train.shape}')","98a2cea3":"# check few rows \ntrain.head()","926e76bf":"train.tail()","2cdfeae3":"train.columns","32663eb4":"train.info()","3a8cf83f":"# Print out the full tweet of a few rows\nimport random\nfor i in range(3):\n    row = random.randint(0,len(train))\n    print(f'Tweet at Row {row}')\n    print(train['text'].loc[row])\n    print('\\n')","7b38a581":"train['target'].value_counts()","004613e7":"sns.countplot(train['target'])\nplt.title('Disaster vs Non Disaster Tweets');","04b6bd9d":"# Add a new column to make the target more explicit \ntrain['disaster'] =  train['target'].apply(lambda x: 'real' if x ==1 else 'not real')","764d378c":"# Plot the same graph as earlier but on the newly created column --> more intuitive\nsns.countplot(train['disaster'])\nplt.title('Disaster vs Non Disaster Tweets')","c3bb981e":"# Code section to create a lis of indices where there are empty tweets\nblanks = []\n\nfor idx,id_,kwd,loc,txt,tgt,dis in train.itertuples():\n    if type(txt) == str:\n        if txt.isspace():\n            blanks.append(idx)\nprint (f'The list of indices where there are blank tweets is {blanks}')","cc8aedb8":"# heatmap to visualize the missing values\nsns.heatmap(train.isnull(),cbar=False,yticklabels=False,cmap='viridis');","09ddc81f":"# missing values in train set\ntrain.isna().sum()","49f56d9f":"# check for missing values in test set\ntest.isna().sum()\n","db5cd59e":"# Randomly pick a few rows and print the keyword and the corresponding tweet \nfor i in range(5): # check any 5 random keywords\n    row = random.randint(0,len(train))\n    \n    if train['keyword'].loc[row].isspace():  # to handle the missing keywords\n        print('Nothing to print')\n        \n    else:  # print the keyword and the corresponding text\n        print(f'keyword at Row {row}:-')\n        print(train['keyword'].loc[row])\n        print('The full tweet text is :-')\n        print(train['text'].loc[row])\n        print('\\n')","bdadf189":"train[train['keyword'].isna()]['disaster'].value_counts()","3bd2270a":"# get the keywords  --> top_kwd will be the series containing all the keywords sorted in the descending order of occurrence\ntop_kwd = train['keyword'].value_counts()","a9b6387c":"sns.barplot(y=top_kwd[:20].index,x=top_kwd[:20],palette='winter')\nplt.title('Top Keywords in the Tweets');","37956bcd":"sns.barplot(y=top_kwd[-20:].index,x=top_kwd[-20:],palette='winter')\nplt.title('Least Keywords in Tweets');","aabc3504":"# total unique keywords\nlen(train['keyword'].unique())","9701b048":"num_ = 3\nfor i in range(num_):\n    print('Example {}'.format(i+1))\n    print('Tweet of Missing keyword and Real Disaster')\n    print(train[train['keyword'].isna() & train['target']==1]['text'].iloc[i])\n    print('Tweet of Missing keyword and Not Real Disaster')\n    print(train[train['keyword'].isna() & train['target']==0]['text'].iloc[i])\n    print('\\n')","985b02f8":"# function for word count \ndef tweet_len(string):\n    if type(string)==str:\n        string_list = string.split()\n        return len(string_list)","0682fb90":"train['tweet_length'] = train['text'].apply(tweet_len)","180034ef":"# Histogram of the tweet length\nfig,(ax1,ax2)= plt.subplots(1,2)\ntweet_length_1 =  train[train['target']==1]['tweet_length']\nax1.hist(tweet_length_1,color='red');\nax1.set_title('Disaster Tweet Length')\n\ntweet_length_0 =  train[train['target']==0]['tweet_length']\nax2.hist(tweet_length_0,color='green')\nax2.set_title('Non Disaster Tweet Length')\nfig.suptitle('Tweet Length');","cf9d67cc":"# function for word count \ndef avg_word_len(text):\n     # split the tweet text per row on space and create a word list   \n    if type(text)==str:\n        word_list = text.split()\n        word_len = []\n        # compute the average length of word per tweet text --> text is uncleaned, so all characters will be counted\n        for word in word_list:            \n            word_len.append(len(word))\n            avg_word_len_text = sum(word_len)\/len(word_list)\n        return avg_word_len_text    ","f964fc21":"# create a new column to store the average word length per tweet\ntrain['avg_word_len'] = train['text'].apply(avg_word_len)\ntrain['avg_word_len'] = round(train['avg_word_len'],1)","dacdcfb0":"train.head()","35fffa3d":"# Histogram of the average character length --> at this point counts special character and digits\nfig,(ax1,ax2)= plt.subplots(1,2)\nword_length_1 =  train[train['target']==1]['avg_word_len']\nax1.hist(word_length_1,color='red');\nax1.set_title('Disaster Tweet Avg Word Length')\n\nword_length_0 =  train[train['target']==0]['avg_word_len']\nax2.hist(word_length_0,color='green')\nax2.set_title('Non Disaster Tweet Avg Word Length')\nfig.suptitle('Average Word Length');","56c7230f":"# top 10 locations\ntrain['location'].value_counts()[:10]","62f8c8e8":"# plot the top 10 locations\nsns.barplot(y=train['location'].value_counts()[:10].index,x=train['location'].value_counts()[:10],palette='summer')\nplt.title('Top 10 Locations');","b74797e5":"train['location']= train['location'].apply(lambda x: 'USA' if x=='United States' else x)","19a905e3":"train['location'].value_counts()[:10] # we should not see United States now","5e4438bb":"# plot the top 10 locations\nsns.barplot(y=train['location'].value_counts()[:10].index,x=train['location'].value_counts()[:10],palette='summer')\nplt.title('Top 10 Locations');","946025b0":"train['location'].unique()","788fc846":"train['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, DC\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                           'Sacramento, CA':'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                           \"Dallas, TX\":'USA',\n                           \"Nashville, TN\":'USA',\n                           'Denver, Colorado':'USA',\n                            \"Mumbai\":\"India\"},inplace=True)","df57dc67":"# plot the top 10 locations\nsns.barplot(y=train['location'].value_counts()[:7].index,x=train['location'].value_counts()[:7],palette='summer')\nplt.title('Top 7 Locations');","a1734f61":"def rem_url(text):\n    '''This function removes the urls from the text. Alternatively use lambda functions'''\n    new_text = re.sub(r'http\\S+','',text) \n    new_text = re.sub(r'@\\S+','',new_text) # get rid of all the name referrals as in @prakhar\n    return new_text ","4838b724":"def sent_text(text):\n    '''This function will return the sentences from the tweets'''\n    sentences = sent_tokenize(text)\n    return sentences","7e055bf1":"def to_lower(text):\n    ''' Convert the string to lower case'''\n    ## first split on spaces \n    text_split = text.split()\n    word_list = []\n    \n    for word in text_split:        \n        if type(word) == str:\n            word_ = word.lower()\n            word_list.append(word_)\n        new_text = ' '.join(word_list)\n    return new_text     \n    ","4de83609":"def split_words(text):\n    '''This function splits the sentences into words and filters out the punctuations'''\n    tokens = word_tokenize(text)\n    # filter out the punctuations\n    words = [word for word in tokens if word.isalpha()]\n    return words","ea974cc6":"def stop_word(text):\n    '''This function will filter out the stop words from the word tokens'''\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in text if not word in stop_words]\n    return words","e5c38db2":"def stem_word(text):\n    '''This function performs stemming -- Returns the root form of the word'''\n    porter = PorterStemmer()\n    stemmed = [porter.stem(word) for word in text]\n    return stemmed","18039dc3":"def lemma_word(text):\n    '''This function performs lemmatization'''\n    lemmatizer = WordNetLemmatizer()\n    word_lemma = [lemmatizer.lemmatize(word) for word in text]\n    return word_lemma","3c2b04a2":"def join_list(text):\n    '''This function will perform the join operation'''\n    join_text = ' '.join(text)\n    return join_text","61eb0778":"clean_fun_list =  [to_lower,rem_url,split_words,stop_word,lemma_word,join_list]\n\nfor function in clean_fun_list:\n    train['text'] = train['text'].apply(function)\n    test['text'] = test['text'].apply(function)\n    \n# print a sample of cleaned text \nprint(train['text'][random.randint(0,len(train))])\n","611597d6":"real_disaster = train[train['target']==1]['text']\nnon_disaster = train[train['target']==0]['text']\n\ndisaster_wordcloud = WordCloud( background_color='white',\n                        width=900,\n                        height=900).generate(' '.join(real_disaster))\n\nplt.title('Disaster Tweets',fontsize=30)\nplt.axis('off')\nplt.imshow(disaster_wordcloud);","75dd3deb":"non_disaster_wordcloud = WordCloud( background_color='white',\n                        width=900,\n                        height=900).generate(' '.join(non_disaster))\n\nplt.title('Non Disaster Tweets',fontsize=30)\nplt.axis('off')\nplt.imshow(non_disaster_wordcloud);\n","ab25f993":"X = train['text']\ny = train['target']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)","d0e239de":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","801ae222":"tfidf = TfidfVectorizer(min_df=2,max_df=0.95,ngram_range=(1,2))","8e89e8d2":"X_train = tfidf.fit_transform(X_train)\n#y_train = to_categorical(y_train)","492de3ea":"X_test = tfidf.transform(X_test)\n#y_test = to_categorical(y_test)","11d2c351":"def cross_validate(X=X_train,y=y_train):\n    '''This function will test various models using cross validation and \n       prints out the mean and standard deviation. The best performing model will be \n       used for further fine tuning using Grid search. Due to time constraint, ensemble techniques have been\n       excluded.'''\n    \n    warnings.filterwarnings(action='ignore',message='')\n    \n    # create a list of models to be cross validated\n    models = []\n    models.append(('Naive Bayes',MultinomialNB()))\n    models.append(('Logistic Regression',LogisticRegression()))\n    #models.append(('Random Forest',RandomForestClassifier()))\n    #models.append(('Gradient Boosting',GradientBoostingClassifier()))\n    #models.append(('XG Boost',XGBClassifier()))\n    models.append(('Linear SVC',LinearSVC()))\n    models.append(('Decision Tree',DecisionTreeClassifier()))\n    \n    results = []\n    names = []\n    scoring = 'f1'\n    \n    # Cross validate all the models in the list and print the mean and std deviation of the f1 score. \n    for name,model in models:\n        kfold = RepeatedStratifiedKFold(random_state=42,n_repeats=10,n_splits=5)\n        cv_results = cross_val_score(model,X,y,cv=kfold,scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        print (f'Model:{name},Mean:{cv_results.mean()},Std Dev: {cv_results.std()}')      \n    ","77f0dcd9":"cross_validate(X_train,y_train)","c7cc9e43":"nb_model = MultinomialNB()\nnb_model.fit(X_train,y_train)","3edb0739":"y_test_predict = nb_model.predict(X_test)","25f013fb":"print(classification_report(y_test,y_test_predict))","25ae7177":"text_nb_clf = Pipeline([('tfidf',TfidfVectorizer(max_df=0.95,min_df=2,ngram_range=(1,2))),\n                       ('Naive Bayes',MultinomialNB())])","ad8c2953":"text_nb_clf.fit(X,y)","c89195c5":"test_vector = test['text']\ny_pred = text_nb_clf.predict(test_vector)","e9c645aa":"submission =  test.copy()\nsubmission['target'] = y_pred\nsubmission.drop(['keyword','location','text'],inplace=True,axis=1)","f4853a7c":"submission.to_csv('submission.csv',index=False)","405c826d":"There are more data points for not real disaster tweets than the real disaster tweets. ","5a43e8da":"### Join the list to form the cleaned text","34f386a0":"### Model evaluation using various classification metrics","66ef27b9":"USA tops the list in disaster tweets. India is at the 9th position but Mumbai city in India has its own number of disaster tweets. If we combine the cities and their countries, the overall position would differ from what we have now. USA and United States are the same. Perhaps we should combine them into one.","8552a92c":"## Cross Validation\nWe will make use of the RepeatedStratifiedKFold.\nFor more information check this link https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RepeatedStratifiedKFold.html","4a659588":"## Standardize the location \nHere I will make use of the code block from https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp-a-general-intro \n\nI added a few more places Washington, DC instead of Washington, D.C. and Sacramento, CA to USA and many others. This is also not a complete replacement. ","852b7e1b":"## WordCloud\nAn NLP project without a word cloud always seems incomplete. Let's see how the word cloud looks like after the text cleaning has been applied.\n\nFrom the wordcloud,it is not very clear what the word amp means. It occurs in both the disaster as well as in non disaster tweets. Probably a result of the cleaning of the text where the context is broken. Probably  it is a short hand for words like 'amplified', 'amplifier'. ","dd2f8136":"## Further Improvements\n- There is no end to the text cleaning approach. Many of the notebooks I referred to have applied various text cleaning. Sometimes indicating reading out quite a few tweets to understand the usage. \n\n- Apply ensemble techniques. This version of the notebook score 80% f1 score on the test data with Naive Bayes model with no hyper parameter tuning. I am sure trying out various other models with fine tuned hyperparameters can stretch the score to higher levels. \n\n-  Try out with TfidfVectorizer with higer values for max_df and min_df to punish the tokens which occur higher number of times across the documents. \n\n- Feature engineering: One of the consideration would be make use of the keyword column that itself can be encoded or added to the full text column.","5d2141a3":"Now India climbs to the 3rd position. I have no clue which place is represented by \"ss\"","141e9103":"The result is similar in the test dataset. The missing keyword is relatively very low compared to the total tweets. Perhaps dropping them off from the dataset wouldn't cause much damage. The location is missing for over 30% of the total tweets in the test data. Anyway, we do not want to explore the test dataset and keep it as hidden as possible. All transformation, vectorization applied on the training data would be applied on the test data later on.","6631d1c2":"## Load the Libraries\nLoad the usual ones that would be used. Specific library imports done at the place where it is used. That way it helps in maintaining the flow. ","3522df15":"### Create pipeline for vectorizer and model","0673e456":"### Which keywords are repeated more often ? ","c2031990":"Cool, there are no blank tweets or empty strings in the text column. Had it been there, I would choose to drop them from analysis.","ce154474":"The heatmap makes the missing values transparent by highlighting them with a separate color. Location has a lot of missing values. Keyword also has a lot of missing values but have to look closely. Let's check as well numerically.","36ed2a36":"### Check for data balance","bc98d7dc":"# Natural Language Processing - Disaster Tweets","b97a9493":"## Data Exploration\nUnderstand and visualize the data","6623a941":"### Filter out the Stop Words","0d5a1e8e":"### Which tweets have missing keywords more - real or not real ? ","78e08717":"## Text Vectorization\nWe will make use of the TfidfVectorizer which combines the steps of Count vectorizer and TfidfTransformer. ","a410b53b":"A quick glance at the top keywords clearly indicates the vocab used while tweeting  for a disaster. However, it does not reflect whether it is associated with a real disaster or not. ","ae925d78":"## Fit the model on the entire dataset\nEarlier we had fit on 80% of the training data and validated on the 20% of the training data which we called as X_test. Now it is time to fit the Naive Bayes model on the entire train dataset and then predict on the actual test dataset provided for the competition. \n","f752a9fd":"### Check for empty tweets","18c79531":"### Stemming","73f2ee23":"### Clean the text in the train and test dataset \nAfter having defined these functions, it is time to apply them to the text of train and test dataset. The advantage of these functions is that I can reuse them for my other NLP projects. In many of the cases, this could have been realized in a single line of code using the capabilities of lamdba expressions in Python. \n\nWe will do this by creating a list of all the cleaning functions we would like to apply on the text. I chose not to include stemming but rather picked up lemmatization. ","48dc08b4":"Both the disaster and non disaster tweets have similar number of words in their tweets, between 15 to 20.","a5febfb5":"As expected, the tweets contain hashtags, links etc. There is a pattern across all tweets. All the tweets contain one of the strong words which represent or highlight a special situation. A few of the examples of these words are 'emergency', 'electrocute', 'bleeding','guilty', 'crash' , 'windstorm', 'snowstorm' and a lot more. ","6b822f03":"Define functions to take care of the following. \n\n- Remove the urls and weblinks\n- Split into sentences\n- Split into words\n- Filter out Punctuation\n- Filter out Stop Words\n- Stem Words\n- Lemmatization","a2d4cc68":"### Cross validation summary\nNaive Bayes is the best performance on the f1 score compared to logistic regression,decision tree and linear svc. Due to the execution time, I have not included ensemble methods in the cross validation. Another approach would be to stack multiple models. ","fdcedd92":"### Which keywords are the least used ? ","69a79f0a":"### Check for missing values","e7c2d098":"### Number of words in the tweets\nHere I must mention that I took inspiration from the notebook https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove for the two visualizations below. ","f2ec8630":"### Plot the location after cleaning\n","09381b3d":"## Introduction\n\nThis is a beginner's approach and key must have steps for a NLP project. The style is very generic for text cleaning and vectorization. The choice of model is based on cross validation and how a simple model with no bells and whistles can lead to a good f1-score. Try out various other models if you find this approach suitable. The key is not to be overwhelmed by the amount of regex operations one would like to do. This is kept minimal in this notebook where regex is applied only to get rid of the urls. Lastly, I am very grateful to some of the kagglers who went on to share their work on the same project.","6e94e69c":"## Prediction on the validation set\nNext we will fine tune the best model to predict on the validation set X_test","56ae46af":"## Prediction on the test dataset","654e87ce":"### Print out the full tweet for a few samples where the keyword is missing","d9b91732":"Disaster tweets seem to look longer than the non disaster ones. Lets visualize this out.","b625dcc8":"#### Classification Report","c5ad3b2a":"### Average word length in the tweet\nGenerally, in tweets there are repetitions of the same character, e.g woooooowwww, LOOOOOOOOOOOOOOOOL etc. Usually for disaster tweets, I expect people would try to stress on the urgency by stretching a particular character. ","12a39931":"There are only 61 rows with missing keyword out of over 7000 tweets. The location is missing for roughly 40% of the tweets. ","3c5944d7":"The keyword column contains the strong words that signify disaster. The word exists in the text of the tweets and the real context of the word can only be ascertained from the full tweets. ","e66b4f37":"There is no clear cut distinction in terms of the avg length of the characters in the real and non real disaster tweets. Both of them have median value between 5 and 7.5 character length. ","4ac1685e":"## Text Cleaning","5f4128ea":"### Data Exploration summary\n- The dataset has missing values. Location column has roughly 40% missing values. \n- Keywords are the strong words which are typically used in disaster or a catastrophe. It need not be associated     with a natural real occurence of a disaster. \n- There are a few missing keywords, totally 61 with real disaster tweets with 42 and remaining to non real           disaster. \n- The exploration of the number of words and avg word per tweet --> no clear distinction for a real or a non real tweet. These features are added as additional columns and might be handy for improving the predictability of the model.\n- Location clean up and plot indicated the countries from where the maximum tweets came from. ","c3b6e573":"### Exploring the location feature","ae355eae":"## Load the data\nThe dataset consists of train csv and test csv","62cd51f3":"### Remove URL's","9fc97a18":"### Understand the keyword and the relation with tweets","43f2fe08":"The dataset is fairly balanced. Will reserve synthetic generation of data using techniques like SMOTE or ADASYN for performance tuning if needed.","ac12c14e":"### Load as Pandas Dataframe","61d40526":"## Split into train and vaidation dataset\nIn this step, before applying the text vectorization it is better to split the training dataset into train and validation set and test the various models. The best model will be selected for predicting on the test dataset. ","6d6878c6":"Real has comparatively more missing keywords than the non real ones. ","b2e34e08":"### See the full tweets randomly\n","80d1f3b2":"### Split into words\nBefore splitting into words, we need to make the text in lower case. Additionally, the punctuations will also be filtered out.","02c164d7":"### Split into Sentences","fd29f24b":"## Final Submission","4cb9e3b3":"222 unique keywords is quite a lot. By the way, there are also occurences where there are multiple keywords. "}}