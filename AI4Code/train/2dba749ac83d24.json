{"cell_type":{"74d7c601":"code","3e7de52e":"code","fd9d8536":"code","d9d4fb92":"code","8b42b1a4":"code","a9b60114":"code","c82530c4":"code","bbe113fe":"code","0c741d09":"code","a8378cc7":"code","64b51b2d":"code","05830ba7":"code","3e12b510":"code","c63d25f2":"code","4811481d":"code","1f834208":"code","af257985":"code","3e58d96e":"code","372b4514":"code","66152e28":"code","9e5593b6":"code","35bc37eb":"code","e2f8f21c":"code","47f37e70":"code","3ae75a20":"code","b42c6776":"code","03a5fb01":"code","a761f411":"code","37dbf89f":"code","38627bae":"code","da7af110":"code","c151c477":"code","7f09a5fd":"code","dd28cfc2":"code","74e453cd":"code","feadb81d":"code","c581cf39":"code","7763e8fb":"markdown","d6eeab08":"markdown","99af1456":"markdown","af660600":"markdown","7fbaf7fa":"markdown","1ebe829e":"markdown","6347131c":"markdown","8997c572":"markdown","44cf9612":"markdown","8196607e":"markdown","5b92d6da":"markdown","68280d2b":"markdown","3f735afe":"markdown","6693efad":"markdown","f724d64a":"markdown","96719cbd":"markdown","30cf92ef":"markdown","933eb36e":"markdown","2b6a2da7":"markdown","c51e769e":"markdown","b267fc2d":"markdown","5b429ae7":"markdown","242d7960":"markdown","126fe6df":"markdown","164eec5a":"markdown","fa9ca31f":"markdown","62f4e626":"markdown","0a55a8d7":"markdown"},"source":{"74d7c601":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom keras.layers import Dense, BatchNormalization, Dropout, LSTM\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score","3e7de52e":"#loading data\ndata = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\ndata.head()","fd9d8536":"#Prevalence of outcome event\nsns.set_theme(context='poster')\nplt.figure(figsize=(10,7))\nplt.title('Disease status \\n (Survived (0), Death (1))', fontsize=20)\ncols= [\"#7cd16d\",\"#eb2009\"]\nsns.countplot(x= data[\"DEATH_EVENT\"], palette= cols)\nplt.show()","d9d4fb92":"#finding missing values\ndata.isnull().sum()","8b42b1a4":"#correlation between the variables in the study\ndata.corr().style.background_gradient(cmap='Spectral').set_precision(2)","a9b60114":"#Age distribution\nsns.set_theme(context='poster')\nplt.figure(figsize=(20,20))\nplt.title('Distribution of age', color=\"Green\",fontsize=40)\nDays_of_week=sns.countplot(x=data['age'])\nDays_of_week.set_xticklabels(Days_of_week.get_xticklabels(), rotation=40, ha=\"right\",fontsize=20)\nplt.tight_layout()\nplt.show()","c82530c4":"# Boxen and swarm plot of some non binary features.\nfeature = [\"age\",\"creatinine_phosphokinase\",\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\", \"time\"]\nfor i in feature:\n    plt.figure(figsize=(8,8))\n    sns.swarmplot(x=data[\"DEATH_EVENT\"], y=data[i], color=\"black\", alpha=0.5)\n    sns.violinplot(x=data[\"DEATH_EVENT\"], y=data[i], palette=cols)\n    plt.show()","bbe113fe":"sns.set_theme(context='poster')\nplt.figure(figsize=(15,10))\nplt.title('Kernel density plot for age based on follow up (time)', color=\"Green\",fontsize=30)\nsns.kdeplot(x=data[\"time\"], y=data[\"age\"], hue =data[\"DEATH_EVENT\"], palette=cols)\nplt.tight_layout()\nplt.show()","0c741d09":"data.describe().T","a8378cc7":"#Defining the target X and Y variable\nX=data.drop([\"DEATH_EVENT\"],axis=1)\ny=data[\"DEATH_EVENT\"]","64b51b2d":"#Standard scaler features of the dataset\ncol_names = list(X.columns)\ns_scaler = preprocessing.StandardScaler()\nX_df= s_scaler.fit_transform(X)\nX_df = pd.DataFrame(X_df, columns=col_names)   \nX_df.describe().T","05830ba7":"#Examining the scaled features\nsns.set_theme(context='poster')\nplt.figure(figsize=(20,15))\nplt.title('Examining the scaled features (of columns)', color=\"Green\",fontsize=30)\n#colours =[\"#774571\",\"#b398af\",\"#f1f1f1\" ,\"#afcdc7\", \"#6daa9f\"]\nsns.violinplot(data = X_df,palette = 'Set2')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()","3e12b510":"#Spliting test and training sets\nX_train, X_test, y_train,y_test = train_test_split(X_df,y,test_size=0.3,random_state=42)","c63d25f2":"# 1. Initialising the NN\nmodel = Sequential()\n\n# 2. layers\nmodel.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))\nmodel.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 7, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# 3. Compiling the ANN\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# 4. Train the ANN\nhistory = model.fit(X_train, y_train, batch_size = 32, epochs = 500, validation_split=0.2)","4811481d":"val_accuracy = np.mean(history.history['val_accuracy'])\nprint(\"\\n%s: %.2f%%\" % ('val_accuracy', val_accuracy*100))","1f834208":"# Predicting from the test set results\ny_pred = model.predict(X_test)\ny_pred = (y_pred > 0.5)\nnp.set_printoptions()","af257985":"# Confusion matrix for prediction results \ncmap1 = sns.diverging_palette(275,150,  s=40, l=65, n=6)\nplt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix\/np.sum(cf_matrix), cmap = 'magma', annot = True, annot_kws = {'size':15})","3e58d96e":"ac_ann = accuracy_score(y_test,y_pred)","372b4514":"#Print the classification test results\nprint(classification_report(y_test, y_pred))","66152e28":"# Confusion Matrix & accuracy score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nmodel = LogisticRegression()\n\n#Fit the model\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nmylist = []\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\n# accuracy score\nacc_logreg = accuracy_score(y_test, y_pred)\n\nmylist.append(acc_logreg)\nprint(cm)\nprint(acc_logreg)","9e5593b6":"#Finding the optimum number of n_estimators\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,30):\n    classifier = RandomForestClassifier(n_estimators = estimators, random_state=0, criterion='entropy')\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#Figure\nsns.set_theme(context='poster')\nplt.figure(figsize=(15,10))\nplt.title('Number of estimators', color=\"Green\",fontsize=30)\nplt.plot(list(range(10,30)), list1)\nplt.show()","35bc37eb":"# Training the RandomForest Classifier on the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 15, criterion='entropy', random_state=0)\nclassifier.fit(X_train,y_train)","e2f8f21c":"# Predicting the test set results\ny_pred = classifier.predict(X_test)\nprint(y_pred)","47f37e70":"#Confusion matrix and accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_randomforest = accuracy_score(y_test, y_pred)\nmylist.append(acc_randomforest)\nprint(cm)\nprint(acc_randomforest)","3ae75a20":"# Confusion matrix for prediction results \ncmap1 = sns.diverging_palette(275,150,  s=40, l=65, n=6)\nplt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix\/np.sum(cf_matrix), cmap = 'magma', annot = True, annot_kws = {'size':15})","b42c6776":"from xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,30,1):\n    classifier = XGBClassifier(n_estimators = estimators, max_depth=12, subsample=0.7)\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    list1.append(accuracy_score(y_test,y_pred))\n##Figure\nsns.set_theme(context='poster')\nplt.figure(figsize=(15,10))\nplt.title('Number of estimators', color=\"Green\",fontsize=30)\nplt.plot(list(range(10,30,1)), list1)\nplt.show()","03a5fb01":"from xgboost import XGBClassifier\nclassifier = XGBClassifier(n_estimators = 10, max_depth=12, subsample=0.7)\nclassifier.fit(X_train,y_train)","a761f411":"y_pred = classifier.predict(X_test)\nprint(y_pred)","37dbf89f":"# Making the confusion matrix and calculating the accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac_xgboost = accuracy_score(y_test, y_pred)\nmylist.append(ac_xgboost)\nprint(cm)\nprint(ac_xgboost)","38627bae":"# Confusion matrix for prediction results \ncmap1 = sns.diverging_palette(275,150,  s=40, l=65, n=6)\nplt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix\/np.sum(cf_matrix), cmap = 'magma', annot = True, annot_kws = {'size':15})","da7af110":"from catboost import CatBoostClassifier\nclassifier = CatBoostClassifier()\nclassifier.fit(X_train, y_train)","c151c477":"y_pred = classifier.predict(X_test)\nprint(y_pred)","7f09a5fd":"#Confusion matrix and accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac_catboost = accuracy_score(y_test, y_pred)\nmylist.append(ac_catboost)\nprint(cm)\nprint(ac_catboost)","dd28cfc2":"# Confusion matrix for prediction results \ncmap1 = sns.diverging_palette(275,150,  s=40, l=65, n=6)\nplt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix\/np.sum(cf_matrix), cmap = 'magma', annot = True, annot_kws = {'size':15})","74e453cd":"#Summary of all model classifiers","feadb81d":"models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', \n              'Random Forest','xgboost','catboost'],\n    'Score': [acc_logreg, \n              acc_randomforest, ac_ann, ac_xgboost,ac_catboost\n              ]})\nmodels.sort_values(by='Score', ascending=False)","c581cf39":"#Figure for all the classifier models\nplt.rcParams['figure.figsize']=15,8 \nsns.set_style(\"darkgrid\")\nax = sns.barplot(x=models.Model, y=models.Score, palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Classifier Models\", fontsize = 20 )\nplt.ylabel(\"% of Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 25)\nplt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","7763e8fb":"> Last not the least, I would like to thank the fellow Kaggler [@karnikakapoor](https:\/\/www.kaggle.com\/karnikakapoor\/heart-failure-prediction-ann) and [@midouazerty](https:\/\/www.kaggle.com\/midouazerty\/heart-disease-using-8-machine-learning-algorithms) for providing the template for building a ANN model. I further compared the ANN approach with other traditional and ML based models using the template provided by [@midouazerty](https:\/\/www.kaggle.com\/midouazerty\/heart-disease-using-8-machine-learning-algorithms). ","d6eeab08":"<a id=\"3\"><\/a><h1 style='background:#7ad16d; border:0; color:black'><center> Basic model building <\/center><\/h1>","99af1456":"We build our model using artificial nural network,which involves the following steps:\n1. Initialising the ANN\n1. Defining the added layers\n1. Compiling the ANN\n1. Train the ANN","af660600":"<center><img \nsrc=\"https:\/\/cdn.dribbble.com\/users\/1277402\/screenshots\/4180449\/heartwalk.gif\" width=\"900\" height=\"900\"><\/img><\/center>\n\n<br>","7fbaf7fa":"1. [Introduction](#1)\n1. [Data cleaning, exploration and preprocessing](#2)\n1. [Basic model building](#3)\n1. [Comparison: ANN vs rest](#4)\n1. [Acknowledgements](#5)","1ebe829e":"**2. Random forrest classification**","6347131c":"<a id=\"4\"><\/a><h1 style='background:#7ad16d; border:0; color:black'><center> Table of contents <\/center><\/h1>","8997c572":"**4. Catboost**","44cf9612":"The model I have developed has slightly underperformed compared to other models developed by fellow Kaggler [@karnikakapoor](https:\/\/www.kaggle.com\/karnikakapoor\/heart-failure-prediction-ann) and [@midouazerty](https:\/\/www.kaggle.com\/midouazerty\/heart-disease-using-8-machine-learning-algorithms). I will continuousuly updating and training the model - hoping to get a better score in subsequent iteraions.","8196607e":"What the data tell us:\n<br>\n    1. Serum creatiine (r=0.29) is postively and sodium (-0.20) is negatively correlated with risk of death.\n    1. Interestingly lifestyle factors such as smoking (-0.01) and diabetes (0.00) were either not correlated or weakly correlated with risk of deaths. ","5b92d6da":"Cardiovascular disease (CVD) is the most common cause of morbidity and mortality among men and women globally. Heart failure is a commong CVD condition. The Heart Foundation defines Heart failure as \"A condition where your heart isn\u2019t pumping as well as it should be.\" The signs and symptoms of heart failure commonly include shortness of breath, excessive tiredness and leg swelling. \n\nCommon causes of heart failure:\n    1. Coronary artery disease\n    2. Myocardial infraction (heart attack)\n    3. High blood pressure\n    4. Arterial fibrillation\n    5. Cardiomyopathy\n    6. Valvular heart disease \n    7. Infections ","68280d2b":"**1. Logistic regression**","3f735afe":"**3. Xboost**","6693efad":"<a id=\"1\"><\/a><h1 style='background:#7ad16d; border:0; color:black'><center> Introduction <\/center><\/h1>","f724d64a":"# Data preprocessing","96719cbd":"> **Objective**: In this notebook, I will build a series of classifier model and compare that with ANN.","30cf92ef":"Despite the model's results here, key driver's of cardiovascular health lies indisputably in our lifestyle habits (e.g. sedentary habits, consumption of junk and energy dense food, smoking etc.), changes in these habits are essential for reducing the risk of cardiovascular events (e.g. HF).","933eb36e":"Following building the ANN model we will compare the results with similar models build using:\n    1. Catboost\n    1. Random Forest\n    1. Xgboost\n    1. Logistic regression\n    1. KNN","2b6a2da7":"<center><img \nsrc=\"https:\/\/d1nakyqvxb9v71.cloudfront.net\/wp-content\/uploads\/2020\/01\/heart-health-tips-animation-thumbnail.gif\" width=\"900\" height=\"900\"><\/img><\/center>\n\n<br>","c51e769e":"**Variables in the dataset:**\n\n* **Age**: Age of the patient\n* **Anaemia**: If the patient had the haemoglobin below the normal range\n* **Creatinine_phosphokinase**: The level of the creatine phosphokinase in the blood in mcg\/L\n* **Diabetes**: If the patient was diabetic\n* **Ejection_fraction**: Ejection fraction is a measurement of how much blood the left ventricle pumps out with each contraction\n* **High_blood_pressure**: If the patient had hypertension\n* **Platelets**: Platelet count of blood in kiloplatelets\/mL\n* **Serum_creatinine**: The level of serum creatinine in the blood in mg\/dL\n* **Serum_sodium**: The level of serum sodium in the blood in mEq\/L\n* **Sex**: The sex of the patient\n* **Smoking**: If the patient smokes actively or ever did in past\n* **Time**: It is the time of the patient's follow-up visit for the disease in months\n* **Death_event**: If the patient deceased during the follow-up period","b267fc2d":"The major steps invovled in preprocessing:\n<br>\n    1. Outlier detection and correction.\n    1. If necessary feature engineering for the dependent and independent variables.\n    1. Dividing the dataset for training and test sets.","5b429ae7":"<a id=\"2\"><\/a><h1 style='background:#7ad16d; border:0; color:black'><center> Data cleaning, exploration and preprocessing  <\/center><\/h1>","242d7960":"What the data tell us:\n<br>\n    1. Outlier observations are detected for the variables above.\n    1. This might be due to measurement error or due to some factors unique to the study population.","126fe6df":"Here, we show the testing results as well as the classification report and the confusion matrix.","164eec5a":"<a id=\"5\"><\/a><h1 style='background:#7ad16d; border:0; color:black'><center> Acknowledgements <\/center><\/h1>","fa9ca31f":"<a id=\"4\"><\/a><h1 style='background:#6daa9f; border:3; color:white'><center> Machine Learning For Heart Failure Prediction: ANN Endgame <\/center><\/h1>","62f4e626":"# Loading the dataset","0a55a8d7":"<a id=\"4\"><\/a><h1 style='background:#7ad16d; border:0; color:black'><center> Comparison: ANN vs rest <\/center><\/h1>\n"}}