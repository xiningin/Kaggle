{"cell_type":{"6540ae4f":"code","3f6efd67":"code","4ef47c81":"code","080557e0":"code","33570967":"code","08c7bf02":"code","dc16bc48":"code","da65a006":"code","4afd5518":"code","17b882a9":"code","f4bfda35":"code","8ee047cf":"code","37344fc4":"code","95c022e3":"code","92bdf7dc":"code","2e52aae9":"code","796d6a49":"code","7dd0560e":"code","bee6f103":"code","9e605aa5":"code","85e89eff":"code","ee68659d":"code","341c6d18":"code","6b7eee4d":"code","17e2739f":"code","e66c318a":"code","53be1598":"code","158ccb3e":"code","baf61861":"code","f8289875":"markdown","c1b7dd97":"markdown","45a40dbc":"markdown","c23f4990":"markdown","f94cd449":"markdown","f48c52d4":"markdown","87547e45":"markdown"},"source":{"6540ae4f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3f6efd67":"train_data = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\")\ntrain_data.head()","4ef47c81":"train_data[train_data[\"toxic\"]==1]","080557e0":"X_train = train_data[\"comment_text\"]","33570967":"import tensorflow as tf\nfrom tensorflow import keras","08c7bf02":"X_train","dc16bc48":"y_train = train_data.iloc[:, 2:]\ny_train","da65a006":"y_train = y_train.values\ny_train","4afd5518":"import re","17b882a9":"def clean_text(text):\n    text = re.sub(r\"\\n\", \" \", text)\n    return text","f4bfda35":"X_train = X_train.apply(clean_text)\nX_train","8ee047cf":"tokenizer = keras.preprocessing.text.Tokenizer(num_words = 100000, oov_token='<oov>')","37344fc4":"tokenizer.fit_on_texts(X_train)","95c022e3":"from keras.preprocessing.sequence import pad_sequences\n\nmaxlen = max([len(x) for x in np.array(X_train)])\n\ndef preprocess_to_sequences(dataset, fitted_tokenizer, maxlen):\n    dataset = tokenizer.texts_to_sequences(dataset)\n    dataset = pad_sequences(dataset, padding=\"pre\", truncating=\"pre\", maxlen=maxlen)\n    return dataset","92bdf7dc":"maxlen","2e52aae9":"X_train_tokenized = preprocess_to_sequences(X_train, tokenizer, maxlen)","796d6a49":"from sklearn.model_selection import train_test_split\nX_train_tokenized, X_val_tokenized, y_train, y_val = train_test_split(X_train_tokenized, y_train, test_size=0.2)","7dd0560e":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip -q glove.6B.zip","bee6f103":"path_to_glove_file = \".\/glove.6B.100d.txt\"\n\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))","9e605aa5":"num_tokens = len(tokenizer.word_index) + 1\nembedding_dim = 100\nhits = 0\nmisses = 0\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","85e89eff":"from tensorflow.keras.layers import Embedding\nembedding_layer = Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n    trainable=False,\n)","ee68659d":"# Bidirectional LSTM with Conv1D, pre-trained 100d GloVe Embedding\n'''tf.random.set_seed(17)\nmodel = tf.keras.Sequential([embedding_layer,\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True)),\n    tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform'),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(6, activation='sigmoid')])'''","341c6d18":"# Bidirectional LSTM with Conv1D\n'''tf.random.set_seed(17)\nmodel = tf.keras.Sequential([tf.keras.layers.Embedding(150000, 50),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True)),\n    tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform'),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(6, activation='sigmoid')])\n'''","6b7eee4d":"# 2 GRU Layers\n'''tf.random.set_seed(17)\nmodel = keras.models.Sequential([\n    tf.keras.layers.Embedding(150000, 50),\n    keras.layers.GRU(128, return_sequences=True),\n    keras.layers.GRU(128),\n    keras.layers.Dense(6, activation=\"sigmoid\")\n])'''","17e2739f":"keras.backend.clear_session()","e66c318a":"# Bidirectional GRU\ntf.random.set_seed(17)\nmodel = keras.models.Sequential([\n    embedding_layer,\n    keras.layers.Bidirectional(keras.layers.GRU(64, return_sequences=True)),\n    keras.layers.GRU(128),\n    keras.layers.Dense(6, activation=\"sigmoid\")\n])\n","53be1598":"model.summary()","158ccb3e":"model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"AUC\"])\nmodel.fit(X_train_tokenized, y_train, epochs=2, batch_size=200)","baf61861":"sample_text = \"You've got the nerve to tell me that some of them are fine. Wishy washy, not even seasoned, and you know what, more importantly, they're boiled. You donkey!\"\n# Quote from Gordon Ramsay, Hell's Kitchen\n\n# function that prints the probabilities of 6 labels of one sample text\ndef predict_print_text(sample_text):\n    sample_text = preprocess_to_sequences([sample_text], tokenizer, maxlen) # preprocesses text into number representations using a tokenizer\n    prediction = model.predict(sample_text) # predict using the trained model, consists of a GloVe 100d Embedding, a bidirectional GRU, a regular GRU and sigmoid unit\n    labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"] # 6 labels from the data\n    for idx, label in enumerate(labels):\n        print(f\"{label} = {prediction[0][idx]:.2f}\") # print probabilities for each label\n    return\n\npredict_print_text(sample_text)","f8289875":"# Pad sequences with 0's so that all sequences are of the same length","c1b7dd97":"# Now to turn English letters into numbers (word vectors) so we can work with them","45a40dbc":"# Let's predict some text and see what the model thinks of Gordon Ramsay","c23f4990":"# Split 80% train 20% validation","f94cd449":"# The text contains some line breaks \"\\n\" so we remove it with regular expressions","f48c52d4":"# Results\n\nModel|Embedding|AUC|Total parameters|Time per epoch (seconds)|\n-----|-----|-----|-----|-----|\nBidirectional LSTM with Conv1D|300d|0.9848|45,542,054|889\nBidirectional LSTM with Conv1D|100d|0.9847|15,337,254|808\nBidirectional LSTM with Conv1D|50d|0.9842|7,786,054|659\nBidirectional LSTM with Conv1D|GloVe 100d|0.9718|21,371,154|625\nBidirectional LSTM with Conv1D|GloVe 50d|0.9720|10,803,004|584\n2 GRU Layers|300d|0.9877|45,264,966|1178\n2 GRU Layers|100d|0.9873|15,188,166|660\n2 GRU Layers|50d|0.9871|7,668,966|527\n2 GRU Layers|GloVe 100d|0.9780|21,222,066|471\n2 GRU Layers|GloVe 50d|0.9749|10,685,916|455\nBidirectional GRU|300d|0.9879|45,240,390|1303\nBidirectional GRU|100d|0.9877|15,163,590|755\nBidirectional GRU|50d|0.9871|7,644,390|599\nBidirectional GRU|GloVe 100d|0.9797|21,197,490|565\nBidirectional GRU|GloVe 50d|0.9761|10,661,340|527\n","87547e45":"# Download pretrained GloVe embeddings to use them as the first layer in our models\n# Embeddings map the word vectors into a vector space, similar words are grouped closely together in this space. They also apply a mask onto the padded sequences in order to ignore the 0's after padding"}}