{"cell_type":{"fa00c267":"code","c91bebd6":"code","4514a101":"code","174b60d7":"code","c3bf1dc6":"code","c5019cfd":"code","11827134":"code","d9df3e59":"code","27207cb2":"code","34e2ac90":"code","90526ffc":"code","9a85694a":"code","972c8b0b":"code","2ea4c7d9":"code","97be88ca":"code","c85e7888":"code","cd557181":"code","9fcd1bfa":"code","77efd8f3":"code","f29b1884":"code","20bdd5d0":"code","880ac875":"markdown","87b754aa":"markdown","858aa720":"markdown","f68383c6":"markdown","fb322e8d":"markdown","803240eb":"markdown","77689083":"markdown","a4748d98":"markdown","9dbefc3f":"markdown","41bb38b8":"markdown","8999c922":"markdown","b1ff14f1":"markdown","3392fd9f":"markdown","04c17feb":"markdown","68cfb85f":"markdown"},"source":{"fa00c267":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","c91bebd6":"plt.style.use(['ggplot'])","4514a101":"X = 2 * np.random.rand(100,1)\ny = 4 +3 * X+np.random.randn(100,1)","174b60d7":"\nplt.plot(X,y,'b.')\nplt.xlabel(\"$x$\", fontsize=18)\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)\n_ =plt.axis([0,2,0,15])","c3bf1dc6":"X_b = np.c_[np.ones((100,1)),X]\ntheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\nprint(theta_best)","c5019cfd":"X_new = np.array([[0],[2]])\nX_new_b = np.c_[np.ones((2,1)),X_new]\ny_predict = X_new_b.dot(theta_best)\ny_predict","11827134":"plt.plot(X_new,y_predict,'r-')\nplt.plot(X,y,'b.')\nplt.xlabel(\"$x_1$\", fontsize=18)\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)\nplt.axis([0,2,0,15])","d9df3e59":"\ndef  cal_cost(theta,X,y):\n    '''\n    \n    Calculates the cost for given X and Y. The following shows and example of a single dimensional X\n    theta = Vector of thetas \n    X     = Row of X's np.zeros((2,j))\n    y     = Actual y's np.zeros((2,1))\n    \n    where:\n        j is the no of features\n    '''\n    \n    m = len(y)\n    \n    predictions = X.dot(theta)\n    cost = (1\/2*m) * np.sum(np.square(predictions-y))\n    return cost\n","27207cb2":"def gradient_descent(X,y,theta,learning_rate=0.01,iterations=100):\n    '''\n    X    = Matrix of X with added bias units\n    y    = Vector of Y\n    theta=Vector of thetas np.random.randn(j,1)\n    learning_rate \n    iterations = no of iterations\n    \n    Returns the final theta vector and array of cost history over no of iterations\n    '''\n    m = len(y)\n    cost_history = np.zeros(iterations)\n    theta_history = np.zeros((iterations,2))\n    for it in range(iterations):\n        \n        prediction = np.dot(X,theta)\n        \n        theta = theta -(1\/m)*learning_rate*( X.T.dot((prediction - y)))\n        theta_history[it,:] =theta.T\n        cost_history[it]  = cal_cost(theta,X,y)\n        \n    return theta, cost_history, theta_history","34e2ac90":"lr =0.01\nn_iter = 1000\n\ntheta = np.random.randn(2,1)\n\nX_b = np.c_[np.ones((len(X),1)),X]\ntheta,cost_history,theta_history = gradient_descent(X_b,y,theta,lr,n_iter)\n\n\nprint('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\nprint('Final cost\/MSE:  {:0.3f}'.format(cost_history[-1]))","90526ffc":"fig,ax = plt.subplots(figsize=(12,8))\n\nax.set_ylabel('J(Theta)')\nax.set_xlabel('Iterations')\n_=ax.plot(range(n_iter),cost_history,'b.')","9a85694a":"\nfig,ax = plt.subplots(figsize=(10,8))\n_=ax.plot(range(200),cost_history[:200],'b.')","972c8b0b":"def plot_GD(n_iter,lr,ax,ax1=None):\n     \"\"\"\n     n_iter = no of iterations\n     lr = Learning Rate\n     ax = Axis to plot the Gradient Descent\n     ax1 = Axis to plot cost_history vs Iterations plot\n\n     \"\"\"\n     _ = ax.plot(X,y,'b.')\n     theta = np.random.randn(2,1)\n\n     tr =0.1\n     cost_history = np.zeros(n_iter)\n     for i in range(n_iter):\n        pred_prev = X_b.dot(theta)\n        theta,h,_ = gradient_descent(X_b,y,theta,lr,1)\n        pred = X_b.dot(theta)\n\n        cost_history[i] = h[0]\n\n        if ((i % 25 == 0) ):\n            _ = ax.plot(X,pred,'r-',alpha=tr)\n            if tr < 0.8:\n                tr = tr+0.2\n     if not ax1== None:\n        _ = ax1.plot(range(n_iter),cost_history,'b.')  ","2ea4c7d9":"fig = plt.figure(figsize=(30,25))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nit_lr =[(2000,0.001),(500,0.01),(200,0.05),(100,0.1)]\ncount =0\nfor n_iter, lr in it_lr:\n    count += 1\n    \n    ax = fig.add_subplot(4, 2, count)\n    count += 1\n   \n    ax1 = fig.add_subplot(4,2,count)\n    \n    ax.set_title(\"lr:{}\".format(lr))\n    ax1.set_title(\"Iterations:{}\".format(n_iter))\n    plot_GD(n_iter,lr,ax,ax1)","97be88ca":"_,ax = plt.subplots(figsize=(14,10))\nplot_GD(100,0.1,ax)","c85e7888":"def stocashtic_gradient_descent(X,y,theta,learning_rate=0.01,iterations=10):\n    '''\n    X    = Matrix of X with added bias units\n    y    = Vector of Y\n    theta=Vector of thetas np.random.randn(j,1)\n    learning_rate \n    iterations = no of iterations\n    \n    Returns the final theta vector and array of cost history over no of iterations\n    '''\n    m = len(y)\n    cost_history = np.zeros(iterations)\n    \n    \n    for it in range(iterations):\n        cost =0.0\n        for i in range(m):\n            rand_ind = np.random.randint(0,m)\n            X_i = X[rand_ind,:].reshape(1,X.shape[1])\n            y_i = y[rand_ind].reshape(1,1)\n            prediction = np.dot(X_i,theta)\n\n            theta = theta -(1\/m)*learning_rate*( X_i.T.dot((prediction - y_i)))\n            cost += cal_cost(theta,X_i,y_i)\n        cost_history[it]  = cost\n        \n    return theta, cost_history","cd557181":"lr =0.5\nn_iter = 50\n\ntheta = np.random.randn(2,1)\n\nX_b = np.c_[np.ones((len(X),1)),X]\ntheta,cost_history = stocashtic_gradient_descent(X_b,y,theta,lr,n_iter)\n\n\nprint('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\nprint('Final cost\/MSE:  {:0.3f}'.format(cost_history[-1]))","9fcd1bfa":"fig,ax = plt.subplots(figsize=(10,8))\n\nax.set_ylabel('{J(Theta)}',rotation=0)\nax.set_xlabel('{Iterations}')\ntheta = np.random.randn(2,1)\n\n_=ax.plot(range(n_iter),cost_history,'b.')","77efd8f3":"def minibatch_gradient_descent(X,y,theta,learning_rate=0.01,iterations=10,batch_size =20):\n    '''\n    X    = Matrix of X without added bias units\n    y    = Vector of Y\n    theta=Vector of thetas np.random.randn(j,1)\n    learning_rate \n    iterations = no of iterations\n    \n    Returns the final theta vector and array of cost history over no of iterations\n    '''\n    m = len(y)\n    cost_history = np.zeros(iterations)\n    n_batches = int(m\/batch_size)\n    \n    for it in range(iterations):\n        cost =0.0\n        indices = np.random.permutation(m)\n        X = X[indices]\n        y = y[indices]\n        for i in range(0,m,batch_size):\n            X_i = X[i:i+batch_size]\n            y_i = y[i:i+batch_size]\n            \n            X_i = np.c_[np.ones(len(X_i)),X_i]\n           \n            prediction = np.dot(X_i,theta)\n\n            theta = theta -(1\/m)*learning_rate*( X_i.T.dot((prediction - y_i)))\n            cost += cal_cost(theta,X_i,y_i)\n        cost_history[it]  = cost\n        \n    return theta, cost_history","f29b1884":"lr =0.1\nn_iter = 200\n\ntheta = np.random.randn(2,1)\n\n\ntheta,cost_history = minibatch_gradient_descent(X,y,theta,lr,n_iter)\n\n\nprint('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\nprint('Final cost\/MSE:  {:0.3f}'.format(cost_history[-1]))","20bdd5d0":"fig,ax = plt.subplots(figsize=(10,8))\n\nax.set_ylabel('{J(Theta)}',rotation=0)\nax.set_xlabel('{Iterations}')\ntheta = np.random.randn(2,1)\n\n_=ax.plot(range(n_iter),cost_history,'b.')","880ac875":"## Let us build a function which can show the effects together and also show how gradient decent actually is working","87b754aa":"Check how with small learning rates it takes long to converge to the solution whereas with with larger learning rates it is quicker.\n\nA note of caution the actual learning rate for your problem would depend on the data and there is no general formula to set it right. However there are sophisticated optimization algorithms which start with a larger learning rates and then slowly reduce the learning rate as we approach the solution e.g. Adam optimizer.\n\n# Stochastic Gradient Descent (SGD)\nYou may have heard of this term and may be wondering what is this. It is very simple to understand this, in our gradient descent algorithm we did the gradients on each observation one by one,in stochastic gradient descent we can chose the random observations randomly. It is called stochastic because samples are selected randomly (or shuffled) instead of as a single group (as in standard gradient descent) or in the order they appear in the training set.\n\n![img](https:\/\/miro.medium.com\/max\/875\/1*fzZ_BvRP6RZsY0ADk4nvGQ.png)\n\n# Mini Batch Gradient Descent\nIn actual practice we use an approach called Mini batch gradient descent. This approach uses random samples but in batches. What this means is that we do not calculate the gradients for each observation but for a group of observations which results in a faster optimization.A simple way to implement is to shuffle the observations and then create batches and then proceed with gradient descent using batches.\n\n![img](https:\/\/miro.medium.com\/max\/875\/1*JuN8CaAmMN7O6qyPb3bsvQ.png)\n\nWe implemented the gradient descent for linear regression but you can do it for logistic regression or any other algorithm. What would change is the cost function and the way you calculate gradients. So we need to define our cost function and gradient calculation.\n\nThis was an simplified explanation of gradient descent but in practice you do not need to write your own gradient descent. There are numerous sophisticated algorithms available.\n\n","858aa720":"![img](https:\/\/miro.medium.com\/max\/875\/1*hGkkgGpdDAl33rL2cRyqgQ.png)\nIt takes three mandatory inputs X,y and theta. You can adjust the learning rate and iterations. As I said previously we are calling the cal_cost from the gradient_descent function.\n\nLet us try to solve the problem we defined earlier using gradient descent.\n\nWe need to find theta0 and theta1 and but we need to pass some theta vector in gradient descent. We can start with random values of theta from Gaussian distribution and may be 1000 iterations and learning rate of 0.01. The code snippet is self explanatory.\n\n\n![img](https:\/\/miro.medium.com\/max\/875\/1*xFdocKwzkv1wecE_DWYA5A.png)\n\nWe get theta0 = 4.11 and theta1 =2.899 which very close to our actual values of 4 and 3 for theta0 and theta1 respectively. But do we need to iterate 1000 times and use a learning rate of 0.01? To answer this we need to look at the how the cost varies with iterations so let\u2019s plot cost_history against iterations.\n\n![img](https:\/\/miro.medium.com\/max\/875\/1*03karRL6U5_qFeX0kRm4pw.png)\n\nLooking at this graph it is apparent that the cost after around 180 iterations does not reduce which means we can use only 200 iterations. If we zoom in the graph we can notice this.\n\n![img](https:\/\/miro.medium.com\/max\/875\/1*8HSuDgmhWjiFahZUtU7szw.png)\n\nAlso we can notice that cost reduces faster initially and then slows down.\n\nYou can try and play with different learning rate and iteration combinations.\n\nIt would be great to see how the gradient descent actually converges to the solution with different learning rates and iterations. And this would make more sense if we can see everything in one go.\n\nSo why wait let\u2019s do it, let us plot the graphs for convergence and cost vs iterations for the four combinations of iterations and learning rates\n\nit_lr =[(2000,0.001),(500,0.01),(200,0.05),(100,0.1)]\n\nFor brevity I am not pasting the code here but only the graphs, please feel free to check out the full code on my GitHub link.\n\n\n![img](https:\/\/miro.medium.com\/max\/875\/1*oGW47KB3qnvoTNd9EB12SQ.png)","f68383c6":"## Create Data","fb322e8d":"## Plot the graphs for different iterations and learning rates combination","803240eb":"## Let's plot the cost history over iterations","77689083":"## You can always plot Indiviual graphs to zoom in","a4748d98":"## Analytical way of Linear Regression","9dbefc3f":"![img](https:\/\/i.ytimg.com\/vi\/sDv4f4s2SB8\/maxresdefault.jpg)\n\nWhen you venture into machine learning one of the fundamental aspects of your learning would be to understand \u201cGradient Descent\u201d. Gradient descent is the backbone of an machine learning algorithm. In this article I am going to attempt to explain the fundamentals of gradient descent using python code. Once you get hold of gradient descent things start to be more clear and it is easy to understand different algorithms.Much has been already written on this topic so it is not going to be a ground breaking one. To follow along and build your own gradient descent you will need some basic python packages viz. numpy and matplotlib to visualize.\n\nLet us start with some data, even better let us create some data. We will create a linear data with some random Gaussian noise.\n\n X = 2 * np.random.rand(100,1)\n\ny = 4 +3 * X+np.random.randn(100,1)\n\nNext let\u2019s visualize the data\n\n![img](https:\/\/miro.medium.com\/max\/606\/1*Kax6xe_MlOaoe429JC8yLg.png)\n\nIt is evident that Y has a nice linear relationship with X. This data is very simple and has only one independent variable X.\n\nYou may studied at college level that a line can be expressed as\n\n![img](https:\/\/miro.medium.com\/max\/240\/1*pgsqDZm0HmOm-U7JSpq3vw.png)\n\nAnd then you can solve the equation for b and m as follows:\n\n![img](https:\/\/miro.medium.com\/max\/343\/1*KgOIGsHIU4PlLw6CP1gOBA.png)","41bb38b8":"# Stochastic Gradient Descent","8999c922":"## Gradient Descent","b1ff14f1":"Remember that I have added a bias unit to X that is 1 for every vector in X. This if for ease of matrix multiplication to solve for Theta.\n\nYou can see that if the number of features in X starts increasing then the load on CPU\/GPU to do the matrix multiplication would start increasing and if the number of features as really huge like a million features then it would almost become infeasible for your computer to solve this. This is where gradient descent comes to the rescue.\n\nTo explain in brief about gradient descent, imagine that you are on a mountain and are blindfolded and your task is to come down from the mountain to the flat land without assistance. The only assistance you have is a gadget which tells you the height from sea-level. What would be your approach be. You would start to descend in some random direction and then ask the gadget what is the height now. If the gadget tells you that height and it is more than the initial height then you know you started in wrong direction. You change the direction and repeat the process. This way in many iterations finally you successfully descend down.\n\nWell here is the analogy with machine learning terms now:","3392fd9f":"This is called the analytical method of solving the equation. Nothing wrong in this however remember machine learning is about programming in matrices. For sake of machine learning I can express the equation for a line in terms of machine learning in a different way. I would call y as my hypothesis and represent it as J(theta) and call b as theta0 and m as theta1. I can write same equation as :\n![img](https:\/\/miro.medium.com\/max\/408\/1*RACzPsWmZvxrv7-V5Auu4Q.png)\n\nTo solve for the Theta0 and Theta1 analytical way I would have to write the following program:\n\n![img](https:\/\/miro.medium.com\/max\/408\/1*RACzPsWmZvxrv7-V5Auu4Q.png)\n\n> theta_best = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n","04c17feb":"## Mini Batch Gradient Descent","68cfb85f":"Size of Steps took in any direction = Learning rate\n\nGadget tells you height = Cost function\n\nThe direction of your steps = Gradients\n\nLooks simple but mathematically how can we represent this. Here is the maths:\n![img](https:\/\/miro.medium.com\/max\/528\/1*zGZwS27JjfCQG4TXV25XQQ.png)\n> Where m = Number of observations\n\nI am taking an example of linear regression.You start with a random Theta vector and predict the h(Theta), then derive cost using the above equation which stands for Mean Squared Error(MSE). Remember you try to minimize cost you would need to know your next step (or Theta). The partial derivative is something that can help to find the Theta for next iteration.\n\nBut wait currently we need to calculate Theta0 and Theta1 how to do that and what if we had multiple features then we would have multiple Theta. Don\u2019t worry here is a generalized form to calculate Theta:\n![img](https:\/\/miro.medium.com\/max\/621\/1*VwTuB8HQs5gzwotbUXZMQw.png)\n\nwhere alpha = Learning Rate\n\nAll right we are all set to write our own gradient descent, although it might look overwhelming to begin with, with matrix programming it is just a piece of cake, trust me.\n\nWhat are the things we need, a cost function which calculates cost, a gradient descent function which calculates new Theta vector that\u2019s it, really that it.\n\nLet start with cost function and here is the code:\n![img](https:\/\/miro.medium.com\/max\/875\/1*b1gKG3RdC9wIDGofiItG7A.png)\n\nI would share my GitHub gist at the end of this article so you can download and run the code but for now let us understand the cost function. It takes theta,X and y where theta is a vector , X is row vector and y is vector. This is how generally your data is X is a matrix of row vectors while y is a vector.\n\nRemember you do not need to call this function explicitly our gradient descent method will call it internally so let head to our gradient descent function."}}