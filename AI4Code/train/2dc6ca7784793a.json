{"cell_type":{"c22d4162":"code","eebe8fec":"code","38e0d5c7":"code","6d3138fe":"code","c53841a5":"code","84a25d92":"code","3f5b81df":"code","1f6bc914":"code","5f6ff685":"code","80941888":"code","8150fb38":"code","0ee7d431":"code","27c0d66e":"code","73929ebe":"code","a0634378":"code","312558b7":"code","8b676dcf":"code","3ba59a52":"code","0aa4bbe2":"code","f23f7e57":"code","5f666191":"code","acd756e0":"code","a66c33b0":"code","dd9574c8":"code","527ba0b4":"code","b015a981":"code","c0fd0613":"code","173ad7e0":"code","c77a490d":"code","4a1b0c8a":"code","6aa9543b":"code","e4021b56":"code","395f7cd0":"code","f8f93c85":"code","2dac595c":"code","ff67b6ea":"code","3ba5083d":"code","15518eb6":"code","50be90e6":"code","117650da":"code","4235372e":"code","9ec6502b":"code","f9b2d6a5":"code","f21d8dde":"code","0bedc9da":"code","ac4100c4":"code","818d0132":"code","dcddde52":"code","72432fb3":"code","0196f7aa":"code","07ab9869":"code","3a6c99ab":"code","f5f3d6cd":"code","9b6403e3":"code","8ac9497d":"code","93331ec8":"code","f9d527b6":"code","80d034c4":"code","02c91ba9":"code","9336240a":"markdown","f98574f6":"markdown","48ad4431":"markdown","ff6ebef0":"markdown","8628ffbd":"markdown","75362c5c":"markdown","3bd5eca4":"markdown","75027d24":"markdown","37832261":"markdown","0df7afb9":"markdown","4b55dcf3":"markdown","8ff676ea":"markdown","106548e3":"markdown","993a85a6":"markdown","188daca7":"markdown","466046a3":"markdown","17262f5b":"markdown","38295bf6":"markdown","5a0d9907":"markdown","269a16b5":"markdown","ee7c6376":"markdown","d14be372":"markdown","99b330d1":"markdown","d510e6ad":"markdown","2018d2bc":"markdown","1f1d7936":"markdown","95507a33":"markdown","0ad05819":"markdown","62d7030e":"markdown","31b82c66":"markdown","66f78c0f":"markdown","0d193e1f":"markdown","f160a673":"markdown"},"source":{"c22d4162":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelBinarizer,LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('Display.max_columns',500)\npd.set_option('Display.max_rows',500)","eebe8fec":"app_train = pd.read_csv('..\/input\/application_train.csv')\napp_test = pd.read_csv('..\/input\/application_test.csv')","38e0d5c7":"app_train.head()","6d3138fe":"app_train.isnull().sum()","c53841a5":"app_train[app_train.isnull().any(axis=1)].shape","84a25d92":"app_train[~app_train.isnull().any(axis=1)].shape","3f5b81df":"app_train.select_dtypes(include=object).dtypes","1f6bc914":"app_test.select_dtypes(include=object).isnull().sum()","5f6ff685":"app_train.select_dtypes(include=object).isnull().sum()","80941888":"app_train.NAME_TYPE_SUITE = app_train.NAME_TYPE_SUITE.fillna('Unaccompanied')\napp_test.NAME_TYPE_SUITE = app_test.NAME_TYPE_SUITE.fillna('Unaccompanied')","8150fb38":"app_train.OCCUPATION_TYPE = app_train.OCCUPATION_TYPE.fillna('Others')\napp_test.OCCUPATION_TYPE = app_test.OCCUPATION_TYPE.fillna('Others')","0ee7d431":"app_train.FONDKAPREMONT_MODE = app_train.FONDKAPREMONT_MODE.fillna('not specified')\napp_test.FONDKAPREMONT_MODE = app_test.FONDKAPREMONT_MODE.fillna('not specified')","27c0d66e":"app_train.HOUSETYPE_MODE = app_train.HOUSETYPE_MODE.fillna('Others')\napp_test.HOUSETYPE_MODE = app_test.HOUSETYPE_MODE.fillna('Others')","73929ebe":"app_train.WALLSMATERIAL_MODE = app_train.WALLSMATERIAL_MODE.fillna('Others')\napp_test.WALLSMATERIAL_MODE = app_test.WALLSMATERIAL_MODE.fillna('Others')","a0634378":"app_train = app_train.drop(columns=['EMERGENCYSTATE_MODE'])\napp_test = app_test.drop(columns=['EMERGENCYSTATE_MODE'])","312558b7":"min_ext_1 = app_train.EXT_SOURCE_1.min()\nmin_ext_2 = app_train.EXT_SOURCE_2.min()\nmin_ext_3 = app_train.EXT_SOURCE_3.min()","8b676dcf":"app_train.EXT_SOURCE_1 = app_train.EXT_SOURCE_1.fillna(min_ext_1)\napp_train.EXT_SOURCE_2 = app_train.EXT_SOURCE_2.fillna(min_ext_2)\napp_train.EXT_SOURCE_3 = app_train.EXT_SOURCE_3.fillna(min_ext_3)\n\napp_test.EXT_SOURCE_1 = app_test.EXT_SOURCE_1.fillna(min_ext_1)\napp_test.EXT_SOURCE_2 = app_test.EXT_SOURCE_2.fillna(min_ext_2)\napp_test.EXT_SOURCE_3 = app_test.EXT_SOURCE_3.fillna(min_ext_3)\n\napp_train = app_train.fillna(0)\napp_test = app_test.fillna(0)","3ba59a52":"print(app_train.isnull().sum().sum())\nprint(app_test.isnull().sum().sum())","0aa4bbe2":"x_cat = app_train[app_train.select_dtypes(include=object).columns].columns\nx_num = app_train[app_train.select_dtypes(exclude=object).columns].columns.drop(['SK_ID_CURR','TARGET'])","f23f7e57":"def plot_hist(x):\n    plt.rcParams[\"figure.figsize\"] = (10,8)\n    ax = sns.countplot(x=x,data=app_train)\n    plt.xlabel(str(x))\n    plt.title('Histogram of '+str(x))\n    plt.xticks(rotation=70)\n    plt.show()","5f666191":"plot_hist('TARGET')","acd756e0":"for x in x_cat:\n    plot_hist(x)","a66c33b0":"def plot_dist(x):\n    plt.rcParams[\"figure.figsize\"] = (10,8)\n    ax = sns.distplot(app_train[x])\n    plt.xlabel(str(x))\n    plt.title('Distribution of '+str(x))\n    plt.show()","dd9574c8":"for x in x_num:\n    plot_dist(x)","527ba0b4":"def plot_hist(x):\n    plt.rcParams[\"figure.figsize\"] = (10,8)\n    ax = sns.countplot(x=x,data=app_test)\n    plt.xlabel(str(x))\n    plt.title('Histogram of '+str(x))\n    plt.xticks(rotation=70)\n    plt.show()","b015a981":"for x in x_cat:\n    plot_hist(x)","c0fd0613":"def plot_dist(x):\n    plt.rcParams[\"figure.figsize\"] = (10,8)\n    ax = sns.distplot(app_test[x])\n    plt.xlabel(str(x))\n    plt.title('Distribution of '+str(x))\n    plt.show()","173ad7e0":"for x in x_num:\n    plot_dist(x)","c77a490d":"categorical = app_train[app_train.select_dtypes(include=object).columns]\nx_cat = categorical.columns\ncategorical.head()","4a1b0c8a":"numerical = app_train[app_train.select_dtypes(exclude=object).columns]\nnumerical = numerical.drop(columns=['SK_ID_CURR','TARGET'])\nx_num = numerical.columns\nnumerical.head()","6aa9543b":"target = app_train.TARGET\ntarget.head()","e4021b56":"scaller = MinMaxScaler()\napp_train[x_num] = scaller.fit_transform(app_train[x_num])\napp_test[x_num] = scaller.transform(app_test[x_num])\napp_train[x_num].head()","395f7cd0":"for x in x_cat:\n    lb = LabelEncoder()\n    app_train[x] = lb.fit_transform(app_train[x])\n    app_test[x] = lb.transform(app_test[x])","f8f93c85":"app_train[x_cat].head()","2dac595c":"lb = LabelBinarizer()\napp_train['TARGET'] = lb.fit_transform(app_train.TARGET)","ff67b6ea":"app_train.TARGET.head()","3ba5083d":"app_train.head()","15518eb6":"x_call = app_train.columns[2:]","50be90e6":"app_test.head()","117650da":"corr = app_train[x_num].corr()\ncmap=sns.diverging_palette(5, 250, as_cmap=True)\n\ndef magnify():\n    return [dict(selector=\"th\",\n                 props=[(\"font-size\", \"7pt\")]),\n            dict(selector=\"td\",\n                 props=[('padding', \"0em 0em\")]),\n            dict(selector=\"th:hover\",\n                 props=[(\"font-size\", \"12pt\")]),\n            dict(selector=\"tr:hover td:hover\",\n                 props=[('max-width', '200px'),\n                        ('font-size', '12pt')])\n]\n\ncorr.style.background_gradient(cmap, axis=1)\\\n    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n    .set_caption(\"Hover to magify\")\\\n    .set_precision(2)\\\n    .set_table_styles(magnify())","4235372e":"fig,ax = plt.subplots(figsize=(12,10))\ncorr = app_train[x_call].corr()\nhm = sns.heatmap(corr,ax=ax,vmin=-1,vmax=1,annot=False,cmap='coolwarm',square=True,fmt='.2f',linewidths=.05)","9ec6502b":"for x in x_call:\n    msg = \"%s : %.3f\" % (x,np.corrcoef(app_train[x],app_train.TARGET)[0,1])\n    print(msg)","f9b2d6a5":"train_df, test_df = train_test_split(app_train,test_size=0.33,shuffle=True,stratify=app_train.TARGET,\n                                     random_state=217)","f21d8dde":"from sklearn.model_selection import StratifiedKFold,cross_validate,cross_val_score\nfrom sklearn.metrics import classification_report,confusion_matrix,precision_score,recall_score,log_loss,roc_curve\nfrom sklearn.metrics import accuracy_score,f1_score,roc_auc_score,average_precision_score,brier_score_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier,ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.utils import compute_sample_weight,compute_class_weight\nfrom sklearn.calibration import calibration_curve,CalibratedClassifierCV\nfrom sklearn import model_selection\n","0bedc9da":"x_calls = train_df.columns[2:]","ac4100c4":"scorer = ('accuracy','roc_auc','f1_weighted','average_precision')","818d0132":"models = []\nmodels.append(('LR', LogisticRegression(class_weight='balanced')))\nmodels.append(('CART', DecisionTreeClassifier(class_weight='balanced')))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('RFC', RandomForestClassifier(class_weight='balanced')))\nmodels.append(('ETC', ExtraTreesClassifier(class_weight='balanced')))\nmodels.append(('XGBC', XGBClassifier(scale_pos_weight=189399\/16633)))\nmodels.append(('GBM', LGBMClassifier(class_weight='balanced')))","dcddde52":"for name, model in models:\n    kfold = StratifiedKFold(n_splits=10, random_state=217, shuffle=True)\n    cv_results = cross_validate(model, train_df[x_calls], train_df.TARGET,cv=kfold, scoring=scorer)\n    cv_results1=cv_results['test_accuracy']\n    cv_results2=cv_results['test_roc_auc']\n    cv_results3=cv_results['test_f1_weighted']\n    cv_results4=cv_results['test_average_precision']\n    msg = \"%s by Accuracy: %f(%f), by ROC_AUC: %f(%f), by F1-score: %f(%f), PR_AUC: %f(%f)\" % (name, np.mean(cv_results1),\n        np.std(cv_results1),np.mean(cv_results2),np.std(cv_results2),np.mean(cv_results3),np.std(cv_results3),\n        np.mean(cv_results4),np.std(cv_results4))\n    print(msg)","72432fb3":"model_gbm = LGBMClassifier(boosting_type='gbdt', class_weight='balanced',\n        colsample_bytree=1.0, importance_type='split',\n        learning_rate=0.09275695087706179, max_depth=3669,\n        min_child_samples=60, min_child_weight=0.001, min_data=6,\n        min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=64,\n        objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n        silent=True, sub_feature=0.7757070409332384, subsample=1.0,\n        subsample_for_bin=200000, subsample_freq=0)\nmodel_gbm.fit(train_df[x_calls], train_df.TARGET)","0196f7aa":"predictions = model_gbm.predict(test_df[x_calls])\nprob = model_gbm.predict_proba(test_df[x_calls])[:,1]\ntest_df['TARGET_hat']=predictions\ntest_df['TARGET_prob']=prob\nY_validation = test_df.TARGET\nprint(\"Accuracy Score: %f\" % accuracy_score(Y_validation, predictions))\nprint(\"ROC_AUC: %f\" % roc_auc_score(Y_validation, prob,average='weighted'))\nprint(\"PR_AUC: %f\" % average_precision_score(Y_validation, prob,average='weighted'))\nprint(\"F1: %f\" % f1_score(Y_validation, predictions,average='weighted'))\nprint(\"Recall: %f\" % recall_score(Y_validation, predictions,average='weighted'))\nprint(\"Precision: %f\" % precision_score(Y_validation, predictions,average='weighted'))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))","07ab9869":"fpr_rf_lm, tpr_rf_lm, _ = roc_curve(test_df.TARGET, test_df.TARGET_prob)\n\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_rf_lm, tpr_rf_lm, label='RT + LR')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()","3a6c99ab":"def plot_calibration_curve(est, name, X_train, y_train, X_test, y_test):\n    isotonic = CalibratedClassifierCV(est, cv='prefit', method='isotonic')\n    sigmoid = CalibratedClassifierCV(est, cv='prefit', method='sigmoid')\n    lr = LogisticRegression(C=1., solver='lbfgs',class_weight='balanced')\n    fig = plt.figure(1, figsize=(10, 10))\n    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n    ax2 = plt.subplot2grid((3, 1), (2, 0))\n    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n    for clf, name in [(lr, 'Logistic'),(est, name),(isotonic, name + ' + Isotonic'),(sigmoid, name + ' + Sigmoid')]:\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        if hasattr(clf, \"predict_proba\"):\n            prob_pos = clf.predict_proba(X_test)[:, 1]\n        else:  # use decision function\n            prob_pos = clf.decision_function(X_test)\n            prob_pos = (prob_pos - prob_pos.min()) \/ (prob_pos.max() - prob_pos.min())\n        clf_score = brier_score_loss(y_test, prob_pos, pos_label=y_train.max())\n        L2_score = log_loss(y_test, prob_pos)\n        print(\"%s:\" % name)\n        print(\"\\tBrier: %.3f\" % (clf_score))\n        print(\"\\tLog Loss: %.3f\" % (L2_score))\n        print(\"\\tAUC: %.3f\" % roc_auc_score(y_test, prob_pos,average='weighted'))\n        print(\"\\tF1: %.3f\\n\" % f1_score(y_test, y_pred,average='weighted'))\n        fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=10)\n        ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",label=\"%s (%1.3f)\" % (name, clf_score))\n        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,histtype=\"step\", lw=2)\n    ax1.set_ylabel(\"Fraction of positives\")\n    ax1.set_ylim([-0.05, 1.05])\n    ax1.legend(loc=\"lower right\")\n    ax1.set_title('Calibration plots  (reliability curve)')\n    ax2.set_xlabel(\"Mean predicted value\")\n    ax2.set_ylabel(\"Count\")\n    ax2.legend(loc=\"upper center\", ncol=2)\n    plt.tight_layout()","f5f3d6cd":"plot_calibration_curve(model_gbm,'LGBM',train_df[x_calls], train_df['TARGET'],\n                       test_df[x_calls], test_df['TARGET'])","9b6403e3":"model_fix = LGBMClassifier(boosting_type='gbdt', class_weight='balanced',\n        colsample_bytree=1.0, importance_type='split',\n        learning_rate=0.09275695087706179, max_depth=3669,\n        min_child_samples=60, min_child_weight=0.001, min_data=6,\n        min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=64,\n        objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n        silent=True, sub_feature=0.7757070409332384, subsample=1.0,\n        subsample_for_bin=200000, subsample_freq=0)\nmodel_fix.fit(app_train[x_calls],app_train.TARGET)","8ac9497d":"importances = model_fix.feature_importances_\nindices = np.argsort(importances)[::-1]","93331ec8":"def variable_importance(importance, indices,x):\n    print(\"Feature ranking:\")\n    importances = []\n    for f in range(len(x)):\n        i = f\n        t=0\n        print(\"%d. The feature '%s' has a Mean Decrease in Gini of %f\" % (f + 1,x[indices[i]],importance[indices[f]]))\n        importances.append([x[indices[i]],importance[indices[f]]])\n    importances = pd.DataFrame(importances,columns=['Features','Gini'])\n    return importances\n\nimportance = variable_importance(importances, indices,x_calls)","f9d527b6":"model_fix = CalibratedClassifierCV(model_fix, cv='prefit', method='sigmoid')\nmodel_fix.fit(app_train[x_calls],app_train.TARGET)","80d034c4":"TARGET = model_fix.predict_proba(app_test[x_calls])[:,1]\nsubmission = pd.DataFrame({'SK_ID_CURR':app_test['SK_ID_CURR'],'TARGET':TARGET})","02c91ba9":"submission.to_csv('submission.csv', index=False)\nsubmission.head()","9336240a":"## Numerical data normalization\n\nFor numerical columns, we normalized using MinMaxScaller to remove outlier.","f98574f6":"### 1. Heatmap correlation","48ad4431":"<hr>","ff6ebef0":"## Categorical data, numerical data, and target data separation\n\nIn this stage, we separated catagerical data, numerical data, and target data to do different treatment.","8628ffbd":"# Fix Data","75362c5c":"<hr>","3bd5eca4":"## Feature impotances of fix model","75027d24":"We can see that, the dataset is imbalanced.","37832261":"## Feature correlation analysis\n\nIn this stage, we try to analyze the correlation among features.","0df7afb9":"## Distribution of application_train data","4b55dcf3":"# Imported Dataset\n\nLoad data for classification. In this moment, we just used application_train and application_test data to build the model.","8ff676ea":"# Choosing Model\n\nIn this stage, we try to build model using several kinds of model. To choose the best model, we using several metrics, those are accuracy_score to see the accuracy, roc_auc_acore, average_precision or PR_AUC, and f1_score. However, we give more attention to roc_auc.","106548e3":"## Categorical data encoding\n\nFor Categorical data, we converted to numerical using Label Encoder. By this method, all categorical data are sorted by alphabetically.","993a85a6":"## Calculating the probability of application_test data","188daca7":"<hr>","466046a3":"By the model, we got 71.83% accuracy score, 75.93% roc_auc, and 78.07% F1_score. The roc_auc curve can be seen below.","17262f5b":"# Imported Library\n\nLoad all libraries for prepocessing data.","38295bf6":"## Distribution of application_test data","5a0d9907":"There are 298909 rows contain missing data in application_train and 8602 in application_test.","269a16b5":"# Exploratory Data Analysis","ee7c6376":"# Building Fix Model","d14be372":"We can see that, there is no correlation between each features and the target. However, some features have high correlation among them. By this condition, we decided to use tree model rather than Ordinary Least Squared Model.","99b330d1":"## Target data preprocessing\n\nWe just make sure the target is binarizer using Label Binarizer method.","d510e6ad":"As we can see above, the calibration result using isotonic funstion got the same score with that of sigmoid function in Brier Loss Score. Nevertheless, the loss score of sigmoid function is slighly better than that of Isotonic function. Therefore, we decided to use Sigmoid function to calibrate the score of model to be probability of chossing the positif target.","2018d2bc":"# Null value analysis\n\nIn this stage, we try to handle the missing data in application_train and application_test.","1f1d7936":"# Calibration to get probability\n\nIn this stage we convert the score of model to probability using CalibrationClassifierCV. There are two functions that can be used to calibrate the model, those are Sigmoid Function and Isotonic function. To choose the best calibration, we used Brier_score_loss and log_loss as metrics. The lower the both score, the better model.","95507a33":"# Data Preprocessing","0ad05819":"# Train Test Split\n\nIn this stage, we separate data to be 67% train_df and 33% test_df.","62d7030e":"In this stage, we made some assumptions and filled missing data with the assumptions. For column NAME_TYPE_SUITE, we decided to fill missing data with 'Unaccompanied'; OCCUPATION_TYPE, HOUSE_TYPE, and WALLSMATERIAL_MODE with 'Others'; FONDKAPREMONT_MODE with 'not specified'; EXT_SOURCE with the minimum value of each column; and others with zero.","31b82c66":"# Evaluating the best model\n\nBefore evaluating, we used RandomSearchCV to choose the better parameters for the model. However, we don't put the method in this script to handle the long running time.","66f78c0f":"<hr>","0d193e1f":"As we can see above, according to Gini Score the most impotant feature is EXT_SOURCE_3 followed by AMT_CREDIT, EXT_SOURCE_1, AMT_ANNUITY, EXT_SOURCE_2, and so on. However, there are nine features that got zero gini score.","f160a673":"By the test above, we choose LightGBM to build the model."}}