{"cell_type":{"43c25802":"code","54ceb5b7":"code","7b242f7b":"code","e64cd5c0":"code","80b05898":"code","760d1ec7":"code","b82af930":"code","787574a1":"code","efc5d525":"code","93d24447":"code","bc38957f":"code","68ebe851":"code","497edce7":"code","041fae59":"code","5df35024":"code","f833f8c7":"code","ddbd9ce7":"code","9f47ec89":"code","a0223a5c":"code","df94271b":"code","ba6ef812":"code","4d35619d":"code","f4febacc":"code","346abda7":"code","02a934ae":"code","69de5cbb":"code","ba50b5c3":"code","aac5a852":"code","c190cfa7":"code","d2ba454d":"code","e60ab0c5":"code","37beb625":"code","9f0e2fbf":"code","4d1c0fb4":"code","9403ad97":"code","69d653e3":"code","7d56d1f2":"code","9478be7c":"code","ec055797":"code","f986831d":"code","c79c9814":"code","e897bf6c":"code","71f22c42":"code","87dddfd5":"code","4b5deacf":"code","09deeca8":"code","104d7fd4":"code","38372e96":"code","2d4af5f3":"code","6b4a794f":"code","196fe177":"code","6b41400f":"code","e3e7219c":"code","a62aa924":"code","66eacfb2":"code","47d96137":"code","ccd9533b":"code","3044329e":"code","aabbe35d":"code","0eb4f8ef":"code","a57c311a":"code","65b69ccd":"code","ca9538a7":"code","3bb0e61c":"code","dd7e9ae9":"code","9c37a9e1":"code","59662afb":"code","5eeb8fe6":"code","1329cd9f":"code","6229d2af":"code","2e86839a":"code","fca1f8bf":"code","a614bbbe":"code","22c08926":"code","53168305":"code","2ed7d87a":"code","aa6744b3":"code","5e638ff1":"code","22a1b92a":"code","802d7363":"code","33221bab":"code","645de341":"code","bfb25f01":"code","909e9259":"code","380b2603":"code","d74bb332":"code","c1a8f12f":"code","33748a01":"code","bbdc01db":"code","bf3cc652":"code","fcd4574a":"code","dd99c2e4":"code","8243c467":"code","477d4820":"markdown","8ef8f6a8":"markdown","dcdd1a17":"markdown","5151f005":"markdown","4ae6e709":"markdown","efdb33ae":"markdown","123cf49c":"markdown","228a2e19":"markdown","8fb65c6c":"markdown","b77401c6":"markdown","87fbef4e":"markdown","c6ce384f":"markdown","e0ae363b":"markdown","aa431de2":"markdown","d0b04c44":"markdown","b7ae506b":"markdown","7e442926":"markdown","5274c0bb":"markdown","94628db2":"markdown","ff496351":"markdown","b15b7d98":"markdown","040eb1d4":"markdown","305b58cf":"markdown","142cf9e7":"markdown","e7050332":"markdown","8e369fd9":"markdown","e239eabc":"markdown","58216c01":"markdown","4af30e53":"markdown","71ebb46f":"markdown","59c82e81":"markdown","93c2249c":"markdown","d5230c0d":"markdown","a3d16902":"markdown","2bc964c0":"markdown","31b7d293":"markdown","d6211071":"markdown","55017701":"markdown","7d41e53a":"markdown","24674afa":"markdown"},"source":{"43c25802":"!nvidia-smi","54ceb5b7":"!pip install --upgrade pip","7b242f7b":"!pip install tensorflow-gpu","e64cd5c0":"!pip install --upgrade grpcio","80b05898":"!pip install tqdm","760d1ec7":"!pip install bert-for-tf2","b82af930":"!pip install sentencepiece","787574a1":"import os\nimport math\nimport datetime\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport bert\nfrom bert import BertModelLayer\nfrom bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\nfrom bert.tokenization.bert_tokenization import FullTokenizer\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nfrom matplotlib import rc\nfrom sklearn.metrics import confusion_matrix, classification_report\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\nsns.set(font_scale=1.2)\nplt.style.use('ggplot')\nrcParams['figure.figsize'] = 12, 8\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)","efc5d525":"import re\nimport matplotlib.image as image\nimport matplotlib.colors\nfrom collections import defaultdict\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport squarify as sq\nfrom colorama import Fore, Back, Style","93d24447":"df = pd.read_csv('..\/input\/wine-reviews\/winemag-data-130k-v2.csv')\ndf.head()","bc38957f":"df.shape","68ebe851":"# Checking the data for duplicates\ndf[df.duplicated('description',keep=False)].sort_values('description').head(5)","497edce7":"# Dropping all duplicates\ndf.drop_duplicates(('description', 'title'), inplace=True)\ndf[pd.notnull(df.price)]\ndf.shape","041fae59":"# Missing values\ntotal = df.isnull().sum().sort_values(ascending = False)\npercent = (df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending = False)\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.style.background_gradient(cmap='seismic')","5df35024":"# Imputing missing values\nfor col in ('region_2', 'designation', 'taster_twitter_handle', 'taster_name', 'region_1'):\n    df[col]=df[col].fillna('Unknown')\ndf['province'] = df['province'].fillna(df['province'].mode())\ndf['price'] = df['price'].fillna(df['price'].mean())","f833f8c7":"from plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go","ddbd9ce7":"data = df['country'].replace(\"US\", \"United States\").value_counts()\niplot([go.Choropleth(\n    locationmode='country names',\n    locations=data.index.values,\n    text=data.index,\n    z=data.values,\n    colorscale='portland'\n)])","9f47ec89":"# Countries with the most wine reviews\ncountries = df.country.value_counts()\n# Limit top countries to those with more than 500 reviews\ntemp_dict = countries[countries>500].to_dict()\ntemp_dict['Other'] = countries[countries<501].sum()\nless_countries = pd.Series(temp_dict)\nless_countries.sort_values(ascending=False, inplace=True)\n# Turn Series into DataFrame for display purposes\ndf1 = less_countries.to_frame()\ndf1.columns=['Number of Reviews']\ndf1.index.name = 'Country'\ndf1.style.background_gradient(cmap='coolwarm')","a0223a5c":"# Tree map \ncmap = plt.cm.gist_rainbow_r\nnorm = matplotlib.colors.Normalize(vmin=0, vmax=15)\ncolors = [cmap(norm(value)) for value in range(15)]\nnp.random.shuffle(colors)\nfig,ax = plt.subplots(1,1,figsize=(16, 10))\nsq.plot(sizes=less_countries.values, label=less_countries.index.values, alpha=0.5, ax=ax, color=colors)\nplt.axis('off')\nplt.title('Countries by Number of Wine Reviews')","df94271b":"fig = iplot([go.Scatter(x=df.head(1000)['points'],\n                  y=df.head(1000)['price'],\n                  mode='markers', marker_color='darkred')])","ba6ef812":"data = df.assign(n=0).groupby(['points', 'price'])['n'].count().reset_index()\ndata = data[data[\"price\"] < 100]\nv = data.pivot(index='price', columns='points', values='n').fillna(0).values.tolist()\niplot([go.Surface(z=v)])","4d35619d":"w = df.groupby(['country','points'])['price'].agg(['count','min','max','mean']).sort_values(by='mean',ascending=False)[:10]\nw.reset_index(inplace=True)\nw.style.background_gradient(cmap='Wistia', high=0.5)","f4febacc":"print(Fore.YELLOW + 'Number of variety of wines', df['variety'].nunique())\nfig,ax = plt.subplots(1,2,figsize=(16,8))\nax1,ax2 = ax.flatten()\nw = df.groupby(['variety'])['price'].max().sort_values(ascending=False).to_frame()[:15]\nsns.barplot(x = w['price'], y = w.index, color='brown',ax=ax1)\nax1.set_title('The grapes used for most expensive wine')\nax1.set_ylabel('Variety')\nax1.set_xlabel('')\nw = df.groupby(['variety'])['points'].max().sort_values(ascending=False).to_frame()[:15]\nsns.barplot(x = w['points'], y = w.index, color='brown',ax=ax2)\nax2.set_title('The grapes used for most rated wine')\nax2.set_ylabel('')\nax2.set_xlabel('')\nplt.subplots_adjust(wspace=0.3);","346abda7":"fig,ax = plt.subplots(1,2,figsize=(16,8))\nax1,ax2 = ax.flatten()\nw = df.groupby(['variety'])['price'].min().sort_values(ascending=True).to_frame()[:15]\nsns.barplot(x = w['price'], y = w.index, color='y',ax=ax1)\nax1.set_title('The grapes used for least priced wine')\nax1.set_xlabel('')\nax1.set_ylabel('Variety')\nw = df.groupby(['variety'])['points'].min().sort_values(ascending=True).to_frame()[:15]\nsns.barplot(x = w['points'], y = w.index, color='y', ax=ax2)\nax2.set_title('The grapes used for least rated wine')\nax2.set_xlabel('')\nax2.set_ylabel('')\nplt.subplots_adjust(wspace=0.4);","02a934ae":"fig,ax = plt.subplots(1,2,figsize=(16,8))\nax1,ax2 = ax.flatten()\nw = df.groupby(['country'])['price'].max().sort_values(ascending=False).to_frame()[:15]\nsns.barplot(x = w['price'], y = w.index, color='purple',ax=ax1)\nax1.set_title('Most expensive wine by country')\nax1.set_ylabel('Variety')\nax1.set_xlabel('')\nw = df.groupby(['country'])['price'].min().sort_values(ascending=True).to_frame()[:15]\nsns.barplot(x = w['price'], y = w.index, color='purple',ax=ax2)\nax2.set_title('Least priced wine by country')\nax2.set_ylabel('')\nax2.set_xlabel('')\nplt.subplots_adjust(wspace=0.3);","69de5cbb":"fig,ax = plt.subplots(1,2,figsize=(16,8))\nax1,ax2 = ax.flatten()\nw = df.groupby(['country'])['points'].max().sort_values(ascending=False).to_frame()[:15]\nsns.barplot(x = w['points'], y = w.index, color='yellow',ax=ax1)\nax1.set_title('Most rated wine by country')\nax1.set_ylabel('Variety')\nax1.set_xlabel('')\nw = df.groupby(['country'])['points'].min().sort_values(ascending=True).to_frame()[:15]\nsns.barplot(x = w['points'], y = w.index, color='yellow',ax=ax2)\nax2.set_title('Least rated wine by country')\nax2.set_ylabel('')\nax2.set_xlabel('')\nplt.subplots_adjust(wspace=0.3);","ba50b5c3":"print(Fore.BLUE + Style.BRIGHT + 'Number of province list in data:', df['province'].nunique())\nplt.figure(figsize=(14,10))\nw = df['province'].value_counts().to_frame()[0:20]\n#plt.xscale('log')\nsns.barplot(x= w['province'], y =w.index, data=w, color='crimson', orient='h')\nplt.title('Distribution of Wine Reviews by Top 20 Provinces');","aac5a852":"print(Fore.RED + Style.BRIGHT + 'Number of vineyard designation', df['designation'].nunique())\nw = df.groupby(['designation'])['price'].mean().to_frame().sort_values(by='price',ascending=False)[:15]\nf,ax = plt.subplots(1,2,figsize= (14,6))\nax1,ax2 = ax.flatten()\nsns.barplot(w['price'], y = w.index, color='cyan', ax = ax1)\nax1.set_xlabel('')\nax1.set_ylabel('Designation(Vineyard)')\nax1.set_title('Most expensive wine prepared in the vineyard')\nw = df.groupby(['designation'])['points'].mean().to_frame().sort_values(by = 'points', ascending = False)[:15]\nsns.barplot(w['points'], y = w.index, color='cyan', ax = ax2)\nax2.set_xlabel('')\nax2.set_ylabel('')\nax2.set_title('Most rated wine prepared in the vineyard')\nplt.subplots_adjust(wspace=0.3)","c190cfa7":"print(Fore.RED + Style.BRIGHT + 'Number of wineries:', df['winery'].nunique())\nf,ax = plt.subplots(1,2,figsize=(16,6))\nax1,ax2 = ax.flatten()\nw = df.groupby(['winery'])['price'].max().to_frame().sort_values(by='price',ascending=False)[:15]\nsns.barplot(w['price'],y = w.index, color='black',ax = ax1)\nax1.set_title('Wineries with the most expensive wines')\nw = df.groupby(['winery'])['points'].max().to_frame().sort_values(by = 'points', ascending = False)[:15]\nsns.barplot(w['points'], y = w.index, color='black')\nplt.title('Wineries with the most rated wines');","d2ba454d":"stopwords = set(STOPWORDS)\nnewStopWords = ['fruit', \"Drink\", \"black\", 'wine', 'drink']\nstopwords.update(newStopWords)\nwordcloud = WordCloud(\n    stopwords=stopwords,\n    colormap='Set1',\n    max_words=300,\n    max_font_size=200, \n    width=1000, height=800,\n    random_state=42,\n).generate(\" \".join(df['description'].astype(str)))\nprint(wordcloud)\nfig = plt.figure(figsize = (12,14))\nplt.imshow(wordcloud)\nplt.title(\"WORD CLOUD - DESCRIPTION\",fontsize=25)\nplt.axis('off')","e60ab0c5":"wordcloud = WordCloud(\n    background_color='white',\n    stopwords=stopwords,\n    colormap='autumn_r',\n    max_words=300,\n    max_font_size=200, \n    width=1000, height=800,\n    random_state=42,\n).generate(\" \".join(df['variety'].astype(str)))\nprint(wordcloud)\nfig = plt.figure(figsize = (12,14))\nplt.imshow(wordcloud)\nplt.title(\"WORD CLOUD - VARIETY\",fontsize=25)\nplt.axis('off')","37beb625":"!python -m spacy download en_core_web_lg\nimport spacy\nnlp = spacy.load('en_core_web_lg')\ndef normalize_text(text):\n    tm1 = re.sub('<pre>.*?<\/pre>', '', text, flags=re.DOTALL)\n    tm2 = re.sub('<code>.*?<\/code>', '', tm1, flags=re.DOTALL)\n    tm3 = re.sub('<[^>]+>\u00a9', '', tm1, flags=re.DOTALL)\n    return tm3.replace(\"\\n\", \"\")","9f0e2fbf":"# Removing code syntax from text \ndf['description_Cleaned_1'] = df['description'].apply(normalize_text)","4d1c0fb4":"print(Fore.MAGENTA + 'Before normalizing text-----\\n')\nprint(df['description'])\nprint(Fore.YELLOW + Style.DIM + '\\nAfter normalizing text-----\\n')\nprint(df['description_Cleaned_1'])","9403ad97":"from spacy import displacy\nabout_interest_text = ('I like different types of wine')\nabout_interest_doc = nlp(about_interest_text)\ndisplacy.render(about_interest_doc, style='dep')","69d653e3":"# Stop words\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\nlen(spacy_stopwords)\nfor stop_word in list(spacy_stopwords)[:10]:\n    print(Fore.CYAN + stop_word)","7d56d1f2":"doc = nlp(df[\"description\"][3])","9478be7c":"review = str(\" \".join([i.lemma_ for i in doc]))","ec055797":"doc = nlp(review)\nspacy.displacy.render(doc, style='ent', jupyter=True)","f986831d":"# Part of Speech Tagging\nimport string\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\npunctuations = string.punctuation\nstopwords = STOP_WORDS","c79c9814":"# POS tagging\nfor i in nlp(review):\n    print(i, Fore.GREEN + \"=>\",i.pos_)","e897bf6c":"# Parser for reviews\nparser = English()\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens","71f22c42":"tqdm.pandas()\ndf[\"processed_description\"] = df[\"description\"].progress_apply(spacy_tokenizer)","87dddfd5":"# Topic Modeling\n# Creating a vectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\ndata_vectorized = vectorizer.fit_transform(df[\"processed_description\"])","4b5deacf":"NUM_TOPICS = 10","09deeca8":"# Latent Dirichlet Allocation Model\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nlda = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online',verbose=True)\ndata_lda = lda.fit_transform(data_vectorized)","104d7fd4":"# Non-Negative Matrix Factorization Model\nnmf = NMF(n_components=NUM_TOPICS)\ndata_nmf = nmf.fit_transform(data_vectorized)","38372e96":"# Latent Semantic Indexing Model using Truncated SVD\nlsi = TruncatedSVD(n_components=NUM_TOPICS)\ndata_lsi = lsi.fit_transform(data_vectorized)","2d4af5f3":"# Functions for printing keywords for each topic\ndef selected_topics(model, vectorizer, top_n=10):\n    for idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (idx))\n        print([(vectorizer.get_feature_names()[i], topic[i])\n                        for i in topic.argsort()[:-top_n - 1:-1]])","6b4a794f":"# Keywords for topics clustered by Latent Dirichlet Allocation\nprint(Back.RED + \"LDA Model:\")\nselected_topics(lda, vectorizer)","196fe177":"# Keywords for topics clustered by Latent Semantic Indexing\nprint(Back.BLUE + \"NMF Model:\")\nselected_topics(nmf, vectorizer)","6b41400f":"# Keywords for topics clustered by Non-Negative Matrix Factorization\nprint(Back.MAGENTA + \"LSI Model:\")\nselected_topics(lsi, vectorizer)","e3e7219c":"# Transforming an individual sentence\ntext = spacy_tokenizer(\"Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripened apple, citrus and dried sage alongside brisk acidity.\")\nx = lda.transform(vectorizer.transform([text]))[0]\nprint(x)","a62aa924":"# Description and variety of grapes\ndf = df[['description', 'variety']]\ndf.head()","66eacfb2":"# Getting top 8 most described variety\ntemp_df = df.variety.value_counts()\ntemp_df.head(8)","47d96137":"# For this project we are taking top 8 variety only\nmask = df['variety'].isin(['Pinot Noir', 'Chardonnay', 'Cabernet Sauvignon', 'Red Blend',\n                           'Bordeaux-style Red Blend', 'Riesling', 'Sauvignon Blanc',\n                           'Syrah'])\ndf = df[mask]\ndf.head()","ccd9533b":"chart = sns.countplot(df.variety, color='darkred')\nplt.title(\"Number of descriptions per Variety\")\nchart.set_xticklabels(chart.get_xticklabels(), rotation=30, horizontalalignment='right');","3044329e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.description, df.variety, test_size = 0.2, random_state = 42)","aabbe35d":"train = { 'text': X_train, 'intent': y_train }\ntrain_df = pd.DataFrame(train)\ntest = { 'text': X_test, 'intent': y_test }\ntest_df = pd.DataFrame(test)","0eb4f8ef":"train_df.head()","a57c311a":"# Making the training dataset uniform - taking the wine with least number of count (i.e. Syrah)\nsyrah_df = train_df[train_df['intent']=='Syrah']\n# Selecting other varities of wines\nriesling_df = train_df[train_df['intent']=='Riesling']\npinot_noir_df = train_df[train_df['intent']=='Pinot Noir']\nchardonnay_df = train_df[train_df['intent']=='Chardonnay']\ncabernet_sauvignon_df = train_df[train_df['intent']=='Cabernet Sauvignon']\nred_blend_df = train_df[train_df['intent']=='Red Blend']\nbordeaux_style_red_blend_df = train_df[train_df['intent']=='Bordeaux-style Red Blend']\nsauvignon_blanc_df = train_df[train_df['intent']=='Sauvignon Blanc']","65b69ccd":"# Setting their count equal to that of Syrah\npinot_noir_df = pinot_noir_df.sample(n=len(syrah_df), random_state=RANDOM_SEED)\nchardonnay_df = chardonnay_df.sample(n=len(syrah_df), random_state=RANDOM_SEED)\ncabernet_sauvignon_df = cabernet_sauvignon_df.sample(n=len(syrah_df), random_state=RANDOM_SEED)\nred_blend_df = red_blend_df.sample(n=len(syrah_df), random_state=RANDOM_SEED)\nbordeaux_style_red_blend_df = bordeaux_style_red_blend_df.sample(n=len(syrah_df), random_state=RANDOM_SEED)\nriesling_df = riesling_df.sample(n=len(syrah_df), random_state=RANDOM_SEED)\nsauvignon_blanc_df = sauvignon_blanc_df.sample(n=len(syrah_df), random_state=RANDOM_SEED)","ca9538a7":"# Adding all the data together\nsyrah_df = syrah_df.append(pinot_noir_df).reset_index(drop=True)\nsyrah_df = syrah_df.append(chardonnay_df).reset_index(drop=True)\nsyrah_df = syrah_df.append(cabernet_sauvignon_df).reset_index(drop=True)\nsyrah_df = syrah_df.append(red_blend_df).reset_index(drop=True)\nsyrah_df = syrah_df.append(bordeaux_style_red_blend_df).reset_index(drop=True)\nsyrah_df = syrah_df.append(riesling_df).reset_index(drop=True)\nsyrah_df = syrah_df.append(sauvignon_blanc_df).reset_index(drop=True)\ntrain_df = syrah_df\ntrain_df.shape","3bb0e61c":"chart = sns.countplot(train_df.intent, color='darkred')\nplt.title(\"Number of descriptions per Variety (Resampled)\")\nchart.set_xticklabels(chart.get_xticklabels(), rotation=30, horizontalalignment='right');","dd7e9ae9":"# Shuffling the data\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)","9c37a9e1":"!wget https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-12_H-768_A-12.zip","59662afb":"!unzip uncased_L-12_H-768_A-12.zip","5eeb8fe6":"os.makedirs(\"model\", exist_ok=True)","1329cd9f":"!mv uncased_L-12_H-768_A-12\/ model","6229d2af":"bert_model_name=\"uncased_L-12_H-768_A-12\"\nbert_ckpt_dir = os.path.join(\"model\/\", bert_model_name)\nbert_ckpt_file = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\nbert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")","2e86839a":"class IntentDetectionData:\n    DATA_COLUMN = \"text\"\n    LABEL_COLUMN = \"intent\"\n    def __init__(self, train, test, tokenizer: FullTokenizer, classes, max_seq_len=192):\n        self.tokenizer = tokenizer\n        self.max_seq_len = 0\n        self.classes = classes\n        train, test = map(lambda df: df.reindex(df[IntentDetectionData.DATA_COLUMN].str.len().sort_values().index), [train, test])\n        ((self.train_x, self.train_y), (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n        print(\"max seq_len\", self.max_seq_len)\n        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n        self.train_x, self.test_x = map(self._pad, [self.train_x, self.test_x])\n    def _prepare(self, df):\n        x, y = [], []\n        for _, row in tqdm(df.iterrows()):\n            text, label = row[IntentDetectionData.DATA_COLUMN], row[IntentDetectionData.LABEL_COLUMN]\n            tokens = self.tokenizer.tokenize(text)\n            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n            x.append(token_ids)\n            y.append(self.classes.index(label))\n        return np.array(x), np.array(y)\n    def _pad(self, ids):\n        x = []\n        for input_ids in ids:\n            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n            x.append(np.array(input_ids))\n        return np.array(x)","fca1f8bf":"tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))","a614bbbe":"tokenizer.tokenize(\"I like red wine more than white wine\")","22c08926":"tokens = tokenizer.tokenize(\"Wines from some countries are very underrated!\")\ntokenizer.convert_tokens_to_ids(tokens)","53168305":"def create_model(max_seq_len, bert_ckpt_file):\n    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n        bc = StockBertConfig.from_json_string(reader.read())\n        bert_params = map_stock_config_to_params(bc)\n        bert_params.adapter_size = None\n        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n    input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name=\"input_ids\")\n    bert_output = bert(input_ids)\n    print(\"bert shape\", bert_output.shape)\n    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n    cls_out = keras.layers.Dropout(0.5)(cls_out)\n    logits = keras.layers.Dense(units=768, activation=\"swish\")(cls_out)\n    logits = keras.layers.Dropout(0.5)(logits)\n    logits = keras.layers.Dense(units=len(classes), activation=\"softmax\")(logits)\n    model = keras.Model(inputs=input_ids, outputs=logits)\n    model.build(input_shape=(None, max_seq_len))\n    load_stock_weights(bert, bert_ckpt_file)\n    return model","2ed7d87a":"test_df.head()","aa6744b3":"classes = train_df.intent.unique().tolist()\ndata = IntentDetectionData(train_df, test_df, tokenizer, classes, max_seq_len=128)","5e638ff1":"data.train_x.shape","22a1b92a":"data.train_x[0]","802d7363":"data.train_y[0]","33221bab":"data.max_seq_len","645de341":"model = create_model(data.max_seq_len, bert_ckpt_file)","bfb25f01":"model.summary()","909e9259":"model.compile(\n  optimizer=keras.optimizers.Adam(1e-5),\n  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n)","380b2603":"log_dir = \"log\/intent_detection\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\")\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\nhistory = model.fit(\n  x=data.train_x, \n  y=data.train_y,\n  validation_split=0.1,\n  batch_size=16,\n  shuffle=True,\n  epochs=10,\n  callbacks=[tensorboard_callback]\n)","d74bb332":"ax = plt.figure().gca()\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\nax.plot(history.history['loss'])\nax.plot(history.history['val_loss'])\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'])\nplt.title('Loss over training epochs')","c1a8f12f":"ax = plt.figure().gca()\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\nax.plot(history.history['acc'])\nax.plot(history.history['val_acc'])\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'])\nplt.title('Accuracy over training epochs')","33748a01":"_, train_acc = model.evaluate(data.train_x, data.train_y)\n_, test_acc = model.evaluate(data.test_x, data.test_y)\nprint(Fore.RED + \"train acc\", train_acc)\nprint(Fore.BLUE + \"test acc\", test_acc)","bbdc01db":"y_pred = model.predict(data.test_x).argmax(axis=-1)","bf3cc652":"print(classification_report(data.test_y, y_pred, target_names=classes))","fcd4574a":"cm = confusion_matrix(data.test_y, y_pred)\ndf_cm = pd.DataFrame(cm, index=classes, columns=classes)","dd99c2e4":"hmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap='hot')\nhmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\nhmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\nplt.ylabel('True label')\nplt.xlabel('Predicted label');","8243c467":"sentences = [\n  \"Strong wine made of red grapes\",\n  \"Grapy plummy and juicy taste\"\n]\npred_tokens = map(tokenizer.tokenize, sentences)\npred_tokens = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\npred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\npred_token_ids = map(lambda tids: tids +[0]*(data.max_seq_len-len(tids)),pred_token_ids)\npred_token_ids = np.array(list(pred_token_ids))\npredictions = model.predict(pred_token_ids).argmax(axis=-1)\nfor text, label in zip(sentences, predictions):\n    print(\"text:\", text, \"\\nintent:\", classes[label])\n    print()","477d4820":"\nSpaCy comes with a built-in visualizer called **DisplaCy**","8ef8f6a8":"<a id=\"section-two\"><\/a>\n# 2. Visualizations","dcdd1a17":"# Table of Contents\n\n* [1. Importing Libraries and Data Cleaning](#section-one)\n* [2. Visualizations](#section-two)\n    - [2.1 Wine Reviews by Country](#subsection-one)\n    - [2.2 Price and Points](#subsection-two)\n    - [2.3 Variety of Wines](#subsection-three)\n    - [2.4 Wine Price by Country](#subsection-four)\n    - [2.5 Highest and Least Rated Wines](#subsection-five)\n    - [2.6 Wine Reviews by Province](#subsection-six)\n    - [2.7 Vineyards and Wineries](#subsection-seven)\n    - [2.8 Word Clouds](#subsection-eight)\n* [3. Preprocessing with SpaCy](#section-three)\n    - [3.1 Removing Code Syntax from Text and Using DisplaCy](#sub-one)\n    - [3.2 Stop Words and Lemmatization](#sub-two)\n    - [3.3 Part of Speech Tagging (POS)](#sub-three)\n    - [3.4 Topic Modeling](#sub-four)\n* [4. Preparing Data for the Model](#section-four)\n* [5. BERT](#section-five)","5151f005":"California has the highest number of wine reviews, followed by Washington state ","4ae6e709":"<a id=\"subsection-five\"><\/a>\n## 2.5 Highest and Least Rated Wines","efdb33ae":"We will use **BERT** to extract embeddings from each review in the dataset and then use these embeddings to train a text classification model","123cf49c":"The quest for learning language representations by pre-training models on large unlabelled text data started from word embeddings like **Word2Vec** and **GloVe**. One limitation of these embeddings was the use of very shallow Language Models. This meant there was a limit to the amount of information they could capture and this motivated the use of deeper and more complex language models (layers of **LSTMs** and **GRUs**). Another key limitation was that these models did not take the context of the word into account.","228a2e19":"<a id=\"subsection-six\"><\/a>\n## 2.6 Wine Reviews by Province","8fb65c6c":"<a id=\"section-four\"><\/a>\n# 4. Preparing Data for the Model","b77401c6":"**Part of speech** or **POS** is a grammatical role that explains how a particular word is used in a sentence. There are eight parts of speech: noun, pronoun, adjective, verb, adverb, preposition, conjunction, interjection. **Part of speech tagging** is the process of assigning a POS tag to each token depending on its usage in the sentence. POS tags are useful for assigning a syntactic category like noun or verb to each word. In spaCy, POS tags are available as an attribute on the Token object:","87fbef4e":"**Topic Modeling** is the process of extracting the main topics from a collection of text data or documents. Essentially, it's a form of Dimensionality Reduction","c6ce384f":"<a id=\"subsection-two\"><\/a>\n## 2.2 Price and Points","e0ae363b":"<a id=\"section-five\"><\/a>\n# 5. BERT","aa431de2":"<a id=\"subsection-eight\"><\/a>\n## 2.8 Word Clouds","d0b04c44":"**Lemmatization** is the process of reducing inflected forms of a word while still ensuring that the reduced form belongs to the language. This reduced form or root word is called a **lemma**","b7ae506b":"**References:**\n1. [https:\/\/www.analyticsvidhya.com\/blog\/2019\/09\/demystifying-bert-groundbreaking-nlp-framework\/](http:\/\/)\n2. https:\/\/www.kaggle.com\/thebrownviking20\/topic-modelling-with-spacy-and-scikit-learn\n3. https:\/\/www.kaggle.com\/kshitijmohan\/wine-recommendation-system-based-on-bert\n4. https:\/\/realpython.com\/natural-language-processing-spacy-python\/\n5. https:\/\/www.kaggle.com\/sudhirnl7\/wine-recommender","7e442926":"Google\u2019s **BERT** is a NLP framework. **BERT** stands for **Bidirectional Encoder Representations from Transformers**. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks.","5274c0bb":"We defined some stop words, but we won't remove them. Stop words can contain useful information, and discarding them is not always helpful","94628db2":"<a id=\"sub-three\"><\/a>\n## 3.3 Part of Speech Tagging (POS)","ff496351":"<a id=\"section-one\"><\/a>\n# 1. Importing Libraries and Data Cleaning","b15b7d98":"<a id=\"sub-two\"><\/a>\n## 3.2 Stop Words and Lemmatization","040eb1d4":"<a id=\"subsection-three\"><\/a>\n## 2.3 Variety of Wines","305b58cf":"# <img src=\"https:\/\/asset1.cxnmarksandspencer.com\/is\/image\/mands\/Maisie_igital_Mothers-Day_Group-Generic-Wines_V2_MP_1200x1200_BMNP-3729?wid=900&qlt=70&fmt=pjpeg\">","142cf9e7":"<a id=\"subsection-seven\"><\/a>\n## 2.7 Vineyards and Wineries","e7050332":"As we can see from the barplots the highest rated wines are from Italy and the least rated ones are from Argentina","8e369fd9":"### In this notebook I will build a classifier for predicting the grape variety based on the expert description of wines","e239eabc":"**Stop words** are the most common words in a language","58216c01":"<a id=\"section-three\"><\/a>\n# 3. Preprocessing with SpaCy","4af30e53":"**SpaCy** is a powerful and advanced library used for NLP tasks. Its main pros are speed, ease of use, accuracy and extensibility ","71ebb46f":"<img src=\"https:\/\/bs-uploads.toptal.io\/blackfish-uploads\/uploaded_file\/file\/40127\/image-1567089150930-365a40370d26388997f678560fb7eb77.png\" width=\"1000\">","59c82e81":"Generally the more expensive the wine is the more points it gets","93c2249c":"<a id=\"subsection-one\"><\/a>\n## 2.1 Wine Reviews by Country","d5230c0d":"There's a large variety of wines in the dataset. However, there's an exponential decline in the number of observations for each wine type, so we are going to use only wine types with high number of observations ","a3d16902":"<a id=\"subsection-four\"><\/a>\n## 2.4 Wine Price by Country","2bc964c0":"<a id=\"sub-four\"><\/a>\n## 3.4 Topic Modeling","31b7d293":"The highest number of wine reviews is from the US","d6211071":"The highest number of missing values is in region_2","55017701":"France generally produces the most expensive wines while Argentina produces the cheapest wines","7d41e53a":"# Project Goal","24674afa":"<a id=\"sub-one\"><\/a>\n## 3.1 Removing Code Syntax from Text and Using DisplaCy"}}