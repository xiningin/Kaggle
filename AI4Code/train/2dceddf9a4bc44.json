{"cell_type":{"a2268ead":"code","131bd453":"code","80e2df5d":"code","1b015e33":"code","254a7b81":"code","fc92d833":"code","0878a87e":"code","30cfe0db":"code","666b90b4":"code","6df9a0d9":"code","d2c228f7":"code","5e916e14":"code","14292090":"code","4f87d31a":"code","00fe51f4":"code","e20cdb12":"code","9ff22cc5":"code","50a00c97":"code","e4d24617":"code","4a5afffd":"code","4917744e":"code","336814bf":"code","0fafab7a":"code","0126db5e":"code","6783189d":"code","973b2424":"code","73b4dac3":"code","e428cf8b":"code","fdc4a085":"code","107a1fc2":"code","dbf0195e":"code","a0e43f2b":"code","484b29f4":"code","944d0727":"code","e6162c60":"code","b5f84674":"code","53094835":"code","63035a1e":"code","43ae41e8":"code","6a169f5b":"code","74c2c302":"code","e3274b60":"code","ffebf393":"code","99e906db":"code","4897b69b":"code","64a0837d":"code","5397f0df":"code","f1573b22":"code","74a42764":"code","9e3cca5b":"code","048cd121":"code","d6025881":"code","44d2758c":"markdown","368b0bd9":"markdown","94af1621":"markdown","bf805bfc":"markdown","b40bedfe":"markdown","6a317472":"markdown","1c7e249b":"markdown","9e6db02a":"markdown","f038868e":"markdown","69096128":"markdown","a5823b55":"markdown","7a970482":"markdown","165b2510":"markdown","f7a2cbea":"markdown","393546e2":"markdown","4119a7f3":"markdown"},"source":{"a2268ead":"#machine_learning\n#SVM\n#KNN\n#LogisticRegression\n#RandomForest\n#DecisionTree","131bd453":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix\nfrom sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, plot_roc_curve\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nimport matplotlib.image as pltimg\n%matplotlib inline","80e2df5d":"df = pd.read_csv(r'\/kaggle\/input\/heart-failure-prediction\/heart.csv')","1b015e33":"df.head()","254a7b81":"df.info()","fc92d833":"df.describe()","0878a87e":"df.shape","30cfe0db":"print(df['Sex'].unique())\nprint(df['ChestPainType'].unique())\nprint(df['RestingECG'].unique())\nprint(df['ExerciseAngina'].unique())\nprint(df['ST_Slope'].unique())","666b90b4":"df['Sex'].value_counts()\nsns.countplot(data=df, x='Sex')","6df9a0d9":"df['ChestPainType'].value_counts()\nsns.countplot(data=df, x='ChestPainType')","d2c228f7":"df['RestingECG'].value_counts()\nsns.countplot(data=df, x='RestingECG')","5e916e14":"df['ExerciseAngina'].value_counts()\nsns.countplot(data=df, x='ExerciseAngina')","14292090":"df['ST_Slope'].value_counts()\nsns.countplot(data=df, x='ST_Slope')","4f87d31a":"df.isnull().sum()","00fe51f4":"df = pd.get_dummies(df, columns = ['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope'])\nprint(df)","e20cdb12":"df.corr()['HeartDisease'].sort_values()","9ff22cc5":"plt.figure(figsize=(16,9))\nsns.heatmap(df.corr(), annot=True,cmap=\"coolwarm\")","50a00c97":"#checking for outliers\nplt.figure(figsize=(20,15))\n\nfor i in range(6):\n  plt.subplot(2,3, i+1)\n  sns.boxplot(data=df, x='HeartDisease', y=df[df.columns[i]])","e4d24617":"X = df.drop('HeartDisease', axis = 1)\ny = df['HeartDisease']","4a5afffd":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)","4917744e":"scaler=StandardScaler()\nscaler.fit(X_train)","336814bf":"scaler_train=scaler.transform(X_train)\nscaler_test=scaler.transform(X_test)","0fafab7a":"Lr_model = LogisticRegression()\nLr_model.fit(scaler_train,y_train)","0126db5e":"y_pred= Lr_model.predict(scaler_test)","6783189d":"plot_confusion_matrix(Lr_model, scaler_test, y_test)","973b2424":"print(classification_report(y_test, y_pred))","73b4dac3":"test_error_rate=[]\nfor k in range(1,30):\n    knn_midel=KNeighborsClassifier(n_neighbors=k)\n    knn_midel.fit(scaler_train,y_train)\n    y_p_test=knn_midel.predict(scaler_test)\n    test_error=1-accuracy_score(y_test,y_p_test)\n    test_error_rate.append(test_error)","e428cf8b":"test_error_rate","fdc4a085":"plt.figure(figsize=(12,6))\nplt.plot(range(1,30),test_error_rate,label='test_error')\nplt.legend()\nplt.xlabel('k Value')\nplt.ylabel('Error')","107a1fc2":"knn_model=KNeighborsClassifier(n_neighbors=3)\nknn_model.fit(scaler_train,y_train)","dbf0195e":"y_pred=knn_model.predict(scaler_test)","a0e43f2b":"plot_confusion_matrix(Lr_model, scaler_test, y_test)","484b29f4":"print(classification_report(y_test, y_pred))","944d0727":"svc = SVC()\nsvc.fit(scaler_train, y_train)\ny_pred = svc.predict(scaler_test)","e6162c60":"plot_confusion_matrix(Lr_model, scaler_test, y_test)","b5f84674":"print(classification_report(y_test, y_pred))","53094835":"kernels = ['Polynomial', 'RBF', 'Sigmoid','Linear']\n#A function which returns the corresponding SVC model\ndef getClassifier(ktype):\n    if ktype == 0:\n        # Polynomial kernal\n        return SVC(kernel='poly', degree=8, gamma=\"auto\")\n    elif ktype == 1:\n        # Radial Basis Function kernal\n        return SVC(kernel='rbf', gamma=\"auto\")\n    elif ktype == 2:\n        # Sigmoid kernal\n        return SVC(kernel='sigmoid', gamma=\"auto\")\n    elif ktype == 3:\n        # Linear kernal\n        return SVC(kernel='linear', gamma=\"auto\")","63035a1e":"kernels = ['Polynomial', 'RBF', 'Sigmoid','Linear']","43ae41e8":"for i in range(4):\n    svclassifier = getClassifier(i) \n    svclassifier.fit(scaler_train, y_train)# Make prediction\n    y_pred = svclassifier.predict(scaler_test)# Evaluate our model\n    print(\"Evaluation:\", kernels[i], \"kernel\")\n    print(classification_report(y_test,y_pred))","6a169f5b":"def train_using_gini(scaler_train,y_train):\n  \n    # Creating the classifier object\n    clf_gini = DecisionTreeClassifier(criterion = \"gini\",\n            random_state = 100,max_depth=3, min_samples_leaf=5)\n  \n    # Performing training\n    clf_gini.fit(scaler_train, y_train)\n    return clf_gini","74c2c302":"def tarin_using_entropy(scaler_train, y_train):\n  \n    # Decision tree with entropy\n    clf_entropy = DecisionTreeClassifier(\n            criterion = \"entropy\", random_state = 100,\n            max_depth = 3, min_samples_leaf = 5)\n  \n    # Performing training\n    clf_entropy.fit(scaler_train, y_train)\n    return clf_entropy","e3274b60":"def prediction(scaler_test, clf_object):\n  \n    # Predicton on test with giniIndex\n    y_pred = clf_object.predict(scaler_test)\n    print(\"Predicted values:\")\n    print(y_pred)\n    return y_pred","ffebf393":"def cal_accuracy(y_test, y_pred):\n      \n    print(\"Confusion Matrix: \",\n        confusion_matrix(y_test, y_pred))\n      \n    print (\"Accuracy : \",\n    accuracy_score(y_test,y_pred)*100)\n      \n    print(\"Report : \",\n    classification_report(y_test, y_pred))","99e906db":"    clf_gini = train_using_gini(scaler_train, y_train)\n    clf_entropy = tarin_using_entropy(scaler_train, y_train)\n      \n    # Operational Phase\n    print(\"Results Using Gini Index:\")\n      \n    # Prediction using gini\n    y_pred_gini = prediction(X_test, clf_gini)\n    cal_accuracy(y_test, y_pred_gini)\n      \n    print(\"Results Using Entropy:\")\n    # Prediction using entropy\n    y_pred_entropy = prediction(scaler_test, clf_entropy)\n    cal_accuracy(y_test, y_pred_entropy)","4897b69b":"RF = RandomForestClassifier(n_estimators = 100) ","64a0837d":"RF.fit(scaler_train, y_train)","5397f0df":"plot_confusion_matrix(Lr_model, scaler_test, y_test)","f1573b22":"print(classification_report(y_test, y_pred))","74a42764":"# group \/ ensemble of models\nestimator = []\nestimator.append(('LR', \n                  LogisticRegression(solver ='lbfgs', \n                                     multi_class ='multinomial', \n                                     max_iter = 200)))\nestimator.append(('SVC', SVC(gamma ='auto', probability = True)))\nestimator.append(('DTC', DecisionTreeClassifier()))\n  \n# Voting Classifier with hard voting\nvot_hard = VotingClassifier(estimators = estimator, voting ='hard')\nvot_hard.fit(scaler_train, y_train)\ny_pred = vot_hard.predict(scaler_test)\n  \n# using accuracy_score metric to predict accuracy\nplot_confusion_matrix(Lr_model, scaler_test, y_test)\nprint(classification_report(y_test, y_pred))\nprint(\"Hard Voting\")\n  \n","9e3cca5b":"# Voting Classifier with soft voting\nvot_soft = VotingClassifier(estimators = estimator, voting ='soft')\nvot_soft.fit(scaler_train, y_train)\ny_pred = vot_soft.predict(scaler_test)\n  \n# using accuracy_score\nplot_confusion_matrix(Lr_model, scaler_test, y_test)\nprint(classification_report(y_test, y_pred))\nprint(\"Soft Voting \")","048cd121":"result=pd.DataFrame(data=[86,87,87,84,86,86],index=['Logistic Regression','KNN','SVM','Tree','Random Forest','Ensemble VotingClassifier'],columns=['Accuracy'])","d6025881":"result","44d2758c":"# Import the required libraries","368b0bd9":"# KNN and SVM have the best accuracy","94af1621":"# Split Data into train,test","bf805bfc":"# 1- Logistic Regression","b40bedfe":"# Fit and test all machin learning models","6a317472":"# Missing Values","1c7e249b":"# Exploratory Data Analysis","9e6db02a":"# 4- Tree","f038868e":"# Result","69096128":"# Import and print the dataset","a5823b55":"# 6- Ensemble VotingClassifier","7a970482":"# 2- KNN","165b2510":"# 3- SVM","f7a2cbea":"# One-Hot Encoding","393546e2":"# 5- Random Forest","4119a7f3":"# Normalization Data"}}