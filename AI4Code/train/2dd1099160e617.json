{"cell_type":{"2dc43f99":"code","8645924d":"code","4d29510a":"code","84df0493":"code","6154acd0":"code","9cd7b2aa":"code","b499e73b":"code","15ec72cf":"code","b227ea35":"code","a796749e":"code","52d0b5be":"code","2561fa74":"code","8bb6a559":"code","6cc165e3":"code","0fe8519b":"code","8b0612d3":"code","a7569c9e":"code","0fba4337":"code","394eb7af":"code","f4551940":"code","75bb9a7b":"code","3b913b84":"code","4b2b44b6":"code","202de155":"code","1a3c62c7":"code","7c495e5c":"code","e7ff9e34":"code","6ab351ec":"code","4e7d3542":"code","f0507c04":"code","c1600fa3":"code","dcbceb4b":"code","94a5faba":"code","ea2cf911":"code","2a1bce90":"code","512679ac":"code","073f11b0":"code","2ba41479":"code","53bfb2cb":"code","c5a5d6c8":"code","c24ab95b":"code","ae065f15":"code","881cb2af":"code","2a25f1bc":"code","aefe8a4c":"code","576fdae4":"code","017f75f9":"code","f9f9e150":"code","28c5a481":"code","c8cd568b":"code","9e021b5a":"code","970414cd":"code","2b974914":"code","d1a9648b":"markdown","712c07f1":"markdown","93ceef1b":"markdown","f36bcb2c":"markdown","cc9f0922":"markdown","f0a50253":"markdown","4c64b84c":"markdown"},"source":{"2dc43f99":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8645924d":"#import modules\nfrom sklearn.metrics import r2_score\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\n\nimport os\nimport glob\n\nfrom multiprocessing import Pool\n\nfrom pandarallel import pandarallel\npandarallel.initialize()\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nimport time\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\npath_to_files = \"..\/input\/optiver-realized-volatility-prediction\"\n\nbook_train_files =  path_to_files + '\/book_train.parquet\/stock_id={}'\ntrade_train_files =  path_to_files + '\/trade_train.parquet\/stock_id={}'\n\nbook_test_files =  path_to_files + '\/book_test.parquet\/stock_id={}'\ntrade_test_files =  path_to_files + '\/trade_test.parquet\/stock_id={}'\n\nSMALL_F = 0.00000001","4d29510a":"!pip -q install ..\/input\/pytorchtabnet\/pytorch_tabnet-3.1.1-py3-none-any.whl","84df0493":"import os\nimport glob\nfrom joblib import Parallel, delayed\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nimport lightgbm as lgb\nimport warnings\nimport random\nfrom collections import Counter, defaultdict\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.preprocessing import MinMaxScaler, QuantileTransformer, LabelEncoder, StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\nimport gc\n\nimport optuna.integration.lightgbm as lgbo\nimport numpy.matlib\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.decomposition import FastICA, PCA\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom joblib import Parallel, delayed\n\nimport shutil\n\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nimport torch\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n\n# setting some globl config\n\nplt.style.use('ggplot')\norange_black = [\n    '#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820'\n]\nplt.rcParams['figure.figsize'] = (16,9)\nplt.rcParams[\"figure.facecolor\"] = '#FFFACD'\nplt.rcParams[\"axes.facecolor\"] = '#FFFFE0'\nplt.rcParams[\"axes.grid\"] = True\nplt.rcParams[\"grid.color\"] = orange_black[3]\nplt.rcParams[\"grid.alpha\"] = 0.5\nplt.rcParams[\"grid.linestyle\"] = '--'\n\nwarnings.filterwarnings('ignore')","6154acd0":"from numpy.random import seed\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nfrom keras import backend as K\nfrom keras.engine.topology import Layer","9cd7b2aa":"tf.random.set_seed(42)\nnp.random.seed(42)","b499e73b":"#Configuration\ncfg = dict(\n    isCollectDataOnly = True,\n    isStockIdUsed = False,\n    isTFModelUsed = False,\n    trainNotUsedCols = ['row_id', 'target', 'time_id', 'stock_id'],\n    predictNotUsedCols = ['row_id', 'time_id', 'stock_id'],\n    useHyperOpt = False,\n    useLabelTransformation = False,\n    volumeBarThreshold = 500.0\n)\n\n\ncfg","15ec72cf":"# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\ndef log_return(prob):\n    prob += SMALL_F\n    return np.log(prob).diff()\n\ndef log_return2(x):\n    return np.log1p(x.pct_change())\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\n\ndef getOscStoch(x):\n    return (x[-1] - np.min(x))\/(np.max(x)-np.min(x)+SMALL_F)\n\ndef getNormVal(x):\n    return (x[-1])\/(np.mean(x)+SMALL_F)\n\ndef getBreath(ret, size):\n    f = ret > 0\n    upside = size[f]\n    downside = size[~f]\n    \n    di = np.sum(upside)\/(np.sum(downside)+SMALL_F)\n    ado = np.sum(upside) - np.sum(downside)\n    \n    return di, ado\n    \ndef rateLastFirst(x):\n    return np.mean(x)\/(np.sum(x)+SMALL_F)\n\ndef rolling_windows_vectorized(array, sub_window_size=2):\n    start = 0\n    max_time = len(array)-sub_window_size\n    sub_windows = (\n        start +\n        # expand_dims are used to convert a 1D array to 2D array.\n        np.expand_dims(np.arange(sub_window_size), 0) +\n        np.expand_dims(np.arange(max_time + 1), 0).T\n    )\n    \n    return array[sub_windows]\n\ndef rolling_windows_vectorized_v2(array, sub_window_size, stride_size):\n    start = 0\n    max_time = len(array)-sub_window_size\n    sub_windows = (\n        start + \n        np.expand_dims(np.arange(sub_window_size), 0) +\n        # Create a rightmost vector as [0, V, 2V, ...].\n        np.expand_dims(np.arange(max_time + 1, step=stride_size), 0).T\n    )\n    \n    return array[sub_windows]    \n\ndef getVolumaBars(i_data, threshold=1000.0):\n    o, h, l, c, v  = 0.0, 0.0, 1000000.0, 0.0, 0.0\n    res_array = []\n    isNewBar = True\n    bar_index = 0.0\n    cum_volume = 0.0\n    data_len = i_data.shape[0]\n    for i in range(data_len):\n        #print(i_data[i])\n        cur_price = i_data[i][0]\n        #print(cur_price)\n        if True == isNewBar:\n            bar_index = i\n            o = cur_price\n            c = 0.0\n            h = 0.0\n            l = 10000000.0\n            v = 0.0\n            \n            isNewBar = False\n\n        if cur_price > h:\n            h = cur_price\n        if cur_price < l:\n            l = cur_price\n\n        v += i_data[i][1]\n        \n        if (v >= threshold) or (i == data_len-1):\n            isNewBar = True\n            #bar_index = i\n            c = cur_price\n            res_array.append([bar_index, o, h, l, c, v])\n\n\n    return pd.DataFrame(res_array, columns=['bar_index', 'open', 'high', 'low', 'close', 'volume'])\n\n\ndef _get_beta(high, low, window):\n\n    beta_r = np.empty(high.shape)\n    beta_r[:] = np.NaN\n\n    ret = np.log(high \/ low)\n    high_low_ret = ret ** 2\n    beta = rolling_windows_vectorized(high_low_ret, 2).sum(axis=1)\n    beta = rolling_windows_vectorized(beta, window).mean(axis=1)\n    beta_r[len(beta_r)-len(beta):] = beta    \n\n    return beta_r\n\n\ndef _get_gamma(high, low):\n    gamma_r = np.empty(high.shape)\n    gamma_r[:] = np.NaN\n\n    high_max = rolling_windows_vectorized(high, 2).max(axis=1)\n    low_min = rolling_windows_vectorized(low, 2).min(axis=1)\n    gamma = np.log(high_max \/ low_min) ** 2\n    gamma_r[len(gamma_r)-len(gamma):] = gamma\n\n    return gamma_r    \n\ndef get_bekker_parkinson_vol2(high, low, window: int = 20):\n\n    beta = _get_beta(high, low, window)\n    gamma = _get_gamma(high, low)\n\n    k2 = (8 \/ np.pi) ** 0.5\n    den = 3 - 2 * 2 ** .5\n    sigma = (2 ** -0.5 - 1) * beta ** 0.5 \/ (k2 * den)\n    sigma += (gamma \/ (k2 ** 2 * den)) ** 0.5\n    sigma[sigma < 0] = 0\n\n    return sigma\n\ndef get_garman_class_vol2(open, high, low, close, window):\n    ret_value = np.empty(high.shape)\n    ret_value[:] = np.NaN\n    ret = np.log(high \/ low)  # High\/Low return\n    close_open_ret = np.log(close \/ open)  # Close\/Open return\n    estimator = 0.5 * ret ** 2 - (2 * np.log(2) - 1) * close_open_ret ** 2\n    ret_v = rolling_windows_vectorized(estimator, window).mean(axis=1)\n    ret_value[len(ret_value)-len(ret_v):] = np.sqrt(ret_v)\n    return ret_value\n\ndef getWindows(n_bars, isSpecialWindow=False, min_bar_length=3):\n    if(True==isSpecialWindow):\n        window_size = n_bars\/\/2\n    else:\n        window_size = n_bars-(min_bar_length-1)\n\n    if(window_size<=0):\n        if(n_bars-1 > 0):\n            window_size = n_bars - 1\n        else:\n            window_size=1\n\n    return window_size\n\ndef getMicrostructuralFeatures(input_df, output_df, col_prefix = '', col_postfix = '', min_bar_length = 3, volumeThreshold = cfg['volumeBarThreshold'], isParkinson=True, isGarman=True, isYyang=True, isBekker=True, isMicro=True, isSpecialWindow=False, micro_cols=[]):\n    \n    v_sum = np.sum(input_df.loc[:, 'size']).astype(np.float64)\n    thres = volumeThreshold if v_sum >= volumeThreshold else v_sum\n    volume_bars = getVolumaBars(input_df.loc[:,['price','size']].to_numpy(), thres)\n\n    window_size = getWindows(len(volume_bars), isSpecialWindow=isSpecialWindow, min_bar_length=min_bar_length)\n\n    if(True == isGarman):\n        col_name = col_prefix+'garman_class_vol'+col_postfix\n        pv = get_garman_class_vol2(volume_bars.open.to_numpy(), volume_bars.high.to_numpy(), volume_bars.low.to_numpy(), volume_bars.close.to_numpy(), window=window_size)\n        pv = pv[~np.isnan(pv)]\n        if(len(pv)>0):\n            output_df.loc[:,col_name] = np.median(pv)\n        else:\n            output_df.loc[:,col_name] = 0.0\n\n    if(True == isBekker):\n        col_name = col_prefix+'bekker_parkinson_vol'+col_postfix\n        pv = get_bekker_parkinson_vol2(volume_bars.high.to_numpy(), volume_bars.low.to_numpy(), window=window_size)\n        pv = pv[~np.isnan(pv)]\n        if(len(pv)>0):\n            output_df.loc[:,col_name] = np.median(pv)\n        else:\n            output_df.loc[:,col_name] = 0.0\n\n    return output_df\n\ndef sqsum(series):\n    return np.sum(np.square(series))\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef calc_wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x \/ y) = logb(x) - logb(y)\ndef log_return_new(series):\n    return np.log(series).diff()\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\ndef realized_quarticity(series):\n    return np.sum(series**4)*series.shape[0]\/3\n\ndef realized_1(series):\n    return np.sqrt(np.sum(series**4)\/(6*np.sum(series**2)))\n\ndef ffill(data_df):\n    data_df=data_df.set_index(['time_id', 'seconds_in_bucket'])\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0, 600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()\n\ndef fix_offsets(data_df):\n    offsets = data_df.groupby(['time_id']).agg({'seconds_in_bucket':'min'})\n    offsets.columns = ['offset']\n    data_df = data_df.join(offsets, on='time_id')\n    data_df.seconds_in_bucket = data_df.seconds_in_bucket - data_df.offset\n    return data_df","b227ea35":"# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\ndef log_return(prob):\n    prob += SMALL_F\n    return np.log(prob).diff()\n\ndef log_return2(x):\n    return np.log1p(x.pct_change())\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\n\ndef getOscStoch(x):\n    return (x[-1] - np.min(x))\/(np.max(x)-np.min(x)+SMALL_F)\n\ndef getNormVal(x):\n    return (x[-1])\/(np.mean(x)+SMALL_F)\n\ndef getBreath(ret, size):\n    f = ret > 0\n    upside = size[f]\n    downside = size[~f]\n    \n    di = np.sum(upside)\/(np.sum(downside)+SMALL_F)\n    ado = np.sum(upside) - np.sum(downside)\n    \n    return di, ado\n    \ndef rateLastFirst(x):\n    return np.mean(x)\/(np.sum(x)+SMALL_F)\n\ndef rolling_windows_vectorized(array, sub_window_size=2):\n    start = 0\n    max_time = len(array)-sub_window_size\n    sub_windows = (\n        start +\n        # expand_dims are used to convert a 1D array to 2D array.\n        np.expand_dims(np.arange(sub_window_size), 0) +\n        np.expand_dims(np.arange(max_time + 1), 0).T\n    )\n    \n    return array[sub_windows]\n\ndef rolling_windows_vectorized_v2(array, sub_window_size, stride_size):\n    start = 0\n    max_time = len(array)-sub_window_size\n    sub_windows = (\n        start + \n        np.expand_dims(np.arange(sub_window_size), 0) +\n        # Create a rightmost vector as [0, V, 2V, ...].\n        np.expand_dims(np.arange(max_time + 1, step=stride_size), 0).T\n    )\n    \n    return array[sub_windows]    \n\ndef getVolumaBars(i_data, threshold=1000.0):\n    o, h, l, c, v  = 0.0, 0.0, 1000000.0, 0.0, 0.0\n    res_array = []\n    isNewBar = True\n    bar_index = 0.0\n    cum_volume = 0.0\n    data_len = i_data.shape[0]\n    for i in range(data_len):\n        #print(i_data[i])\n        cur_price = i_data[i][0]\n        #print(cur_price)\n        if True == isNewBar:\n            bar_index = i\n            o = cur_price\n            c = 0.0\n            h = 0.0\n            l = 10000000.0\n            v = 0.0\n            \n            isNewBar = False\n\n        if cur_price > h:\n            h = cur_price\n        if cur_price < l:\n            l = cur_price\n\n        v += i_data[i][1]\n        \n        if (v >= threshold) or (i == data_len-1):\n            isNewBar = True\n            #bar_index = i\n            c = cur_price\n            res_array.append([bar_index, o, h, l, c, v])\n\n\n    return pd.DataFrame(res_array, columns=['bar_index', 'open', 'high', 'low', 'close', 'volume'])\n\n\ndef _get_beta(high, low, window):\n\n    beta_r = np.empty(high.shape)\n    beta_r[:] = np.NaN\n\n    ret = np.log(high \/ low)\n    high_low_ret = ret ** 2\n    beta = rolling_windows_vectorized(high_low_ret, 2).sum(axis=1)\n    beta = rolling_windows_vectorized(beta, window).mean(axis=1)\n    beta_r[len(beta_r)-len(beta):] = beta    \n\n    return beta_r\n\n\ndef _get_gamma(high, low):\n    gamma_r = np.empty(high.shape)\n    gamma_r[:] = np.NaN\n\n    high_max = rolling_windows_vectorized(high, 2).max(axis=1)\n    low_min = rolling_windows_vectorized(low, 2).min(axis=1)\n    gamma = np.log(high_max \/ low_min) ** 2\n    gamma_r[len(gamma_r)-len(gamma):] = gamma\n\n    return gamma_r    \n\ndef get_bekker_parkinson_vol2(high, low, window: int = 20):\n\n    beta = _get_beta(high, low, window)\n    gamma = _get_gamma(high, low)\n\n    k2 = (8 \/ np.pi) ** 0.5\n    den = 3 - 2 * 2 ** .5\n    sigma = (2 ** -0.5 - 1) * beta ** 0.5 \/ (k2 * den)\n    sigma += (gamma \/ (k2 ** 2 * den)) ** 0.5\n    sigma[sigma < 0] = 0\n\n    return sigma\n\ndef get_garman_class_vol2(open, high, low, close, window):\n    ret_value = np.empty(high.shape)\n    ret_value[:] = np.NaN\n    ret = np.log(high \/ low)  # High\/Low return\n    close_open_ret = np.log(close \/ open)  # Close\/Open return\n    estimator = 0.5 * ret ** 2 - (2 * np.log(2) - 1) * close_open_ret ** 2\n    ret_v = rolling_windows_vectorized(estimator, window).mean(axis=1)\n    ret_value[len(ret_value)-len(ret_v):] = np.sqrt(ret_v)\n    return ret_value\n\ndef getWindows(n_bars, isSpecialWindow=False, min_bar_length=3):\n    if(True==isSpecialWindow):\n        window_size = n_bars\/\/2\n    else:\n        window_size = n_bars-(min_bar_length-1)\n\n    if(window_size<=0):\n        if(n_bars-1 > 0):\n            window_size = n_bars - 1\n        else:\n            window_size=1\n\n    return window_size\n\ndef getMicrostructuralFeatures(input_df, output_df, col_prefix = '', col_postfix = '', min_bar_length = 3, volumeThreshold = cfg['volumeBarThreshold'], isParkinson=True, isGarman=True, isYyang=True, isBekker=True, isMicro=True, isSpecialWindow=False, micro_cols=[]):\n    \n    v_sum = np.sum(input_df.loc[:, 'size']).astype(np.float64)\n    thres = volumeThreshold if v_sum >= volumeThreshold else v_sum\n    volume_bars = getVolumaBars(input_df.loc[:,['price','size']].to_numpy(), thres)\n\n    window_size = getWindows(len(volume_bars), isSpecialWindow=isSpecialWindow, min_bar_length=min_bar_length)\n\n    if(True == isGarman):\n        col_name = col_prefix+'garman_class_vol'+col_postfix\n        pv = get_garman_class_vol2(volume_bars.open.to_numpy(), volume_bars.high.to_numpy(), volume_bars.low.to_numpy(), volume_bars.close.to_numpy(), window=window_size)\n        pv = pv[~np.isnan(pv)]\n        if(len(pv)>0):\n            output_df.loc[:,col_name] = np.median(pv)\n        else:\n            output_df.loc[:,col_name] = 0.0\n\n    if(True == isBekker):\n        col_name = col_prefix+'bekker_parkinson_vol'+col_postfix\n        pv = get_bekker_parkinson_vol2(volume_bars.high.to_numpy(), volume_bars.low.to_numpy(), window=window_size)\n        pv = pv[~np.isnan(pv)]\n        if(len(pv)>0):\n            output_df.loc[:,col_name] = np.median(pv)\n        else:\n            output_df.loc[:,col_name] = 0.0\n\n    return output_df\n\ndef sqsum(series):\n    return np.sum(np.square(series))\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef calc_wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x \/ y) = logb(x) - logb(y)\ndef log_return_new(series):\n    return np.log(series).diff()\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\ndef realized_quarticity(series):\n    return np.sum(series**4)*series.shape[0]\/3\n\ndef realized_1(series):\n    return np.sqrt(np.sum(series**4)\/(6*np.sum(series**2)))\n\ndef ffill(data_df):\n    data_df=data_df.set_index(['time_id', 'seconds_in_bucket'])\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0, 600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()\n\ndef fix_offsets(data_df):\n    offsets = data_df.groupby(['time_id']).agg({'seconds_in_bucket':'min'})\n    offsets.columns = ['offset']\n    data_df = data_df.join(offsets, on='time_id')\n    data_df.seconds_in_bucket = data_df.seconds_in_bucket - data_df.offset\n    return data_df\ndef getDataFromBidAsk_numpy(df, ci):\n    a = 0\n    b = 0\n    spread  = {}\n    for k in [1,2]:\n        #k = i+1\n        bidp = 'bid_price{}'.format(k)\n        askp = 'ask_price{}'.format(k)\n        bids = 'bid_size{}'.format(k)\n        asks = 'ask_size{}'.format(k)\n        #calculate comulative wap\n        a += (df[:,ci[bidp]] * df[:,ci[asks]] + df[:,ci[askp]] * df[:,ci[bids]])\n        b += df[:,ci[bids]] + df[:,ci[asks]]\n\n        #wap 1 and 2\n        spread[f'fb_w_{k}'] = (df[:,ci[bidp]] * df[:,ci[asks]] + df[:,ci[askp]] * df[:,ci[bids]] ) \/ (df[:,ci[bids]] + df[:,ci[asks]] + SMALL_F)\n        spread[f'fb_mid_point_{k}'] = (df[:,ci[askp]]) + (df[:,ci[bidp]]) \/ 2\n        spread[f'fb_volume_total_{k}'] = (df[:,ci[asks]]) + (df[:,ci[bids]])\n\n    \n    # mean wap\n    spread['fb_w'] = (a\/(b+SMALL_F))\n    # rates\n    spread['fb_w_rate'] = (spread['fb_w_1']) \/ (spread['fb_w_2']+SMALL_F) \n    spread['fb_mid_point_rate'] = (spread['fb_mid_point_1']) \/ (spread['fb_mid_point_2']+SMALL_F)\n    #sum volume\n    spread['fb_volume_total'] = spread['fb_volume_total_1'] + spread['fb_volume_total_2']\n    \n    ################# test ##################\n    spread['ask_1'] = df[:,ci['ask_price1']]\n    spread['bid_1'] = df[:,ci['bid_price1']]\n    spread['ask_2'] = df[:,ci['ask_price2']]\n    spread['bid_2'] = df[:,ci['bid_price2']]\n    #########################################\n    \n    return spread\n\n\n\ndef Fx(group, stock_id=0, n=10):\n    new_df = pd.DataFrame()\n    name = int(group.time_id.unique()[0])\n    tmp = pd.DataFrame()\n\n    #calculate log return from the following features:\n    cols = [\n        'fb_w', \n        'fb_w_1', \n        'fb_w_2',\n        'fb_mid_point_1',\n        'fb_mid_point_rate',\n        'fb_w_rate',\n    ]\n\n    new_cols = [s + '_lr' for s in cols]\n    group.loc[:,new_cols] = log_return2(group[cols]).to_numpy()\n    group = group[~group['fb_w'].isnull()]\n    \n    #calculate realized volatility\n    cols = [\n        'fb_mid_point_1_lr',\n        'fb_mid_point_rate_lr',\n        'fb_w_rate_lr',\n    ]\n    new_cols = [s + '_vola' for s in cols]\n    tmp = pd.concat([tmp, pd.DataFrame(realized_volatility(group.loc[:,cols]).to_numpy().reshape(1,-1), columns=new_cols)], axis=1)\n\n    #calculate sum of log return\n    new_cols = [s + '_sum' for s in cols]\n    tmp = pd.concat([tmp, pd.DataFrame(np.sum(group.loc[:,cols]).to_numpy().reshape(1,-1), columns=new_cols)], axis=1)\n\n    #calclulate market microstructural features\n    c = '1'\n    cols_1 = []\n    new_df = pd.DataFrame({'price': group.loc[:,'fb_mid_point_'+c].to_numpy().flatten(), 'size': group.loc[:,'fb_volume_total_'+c].to_numpy().flatten()}).reset_index()\n    tmp = getMicrostructuralFeatures(new_df, tmp, col_prefix = 'fb_', col_postfix = '_'+c, micro_cols=cols_1)\n    c = '2'\n    cols_2 = []  \n    new_df = pd.DataFrame({'price': group.loc[:,'fb_mid_point_'+c].to_numpy().flatten(), 'size': group.loc[:,'fb_volume_total_'+c].to_numpy().flatten()}).reset_index()\n    tmp = getMicrostructuralFeatures(new_df, tmp, col_prefix = 'fb_', col_postfix = '_'+c, micro_cols=cols_2)\n    \n    ############ test idea ################\n    col_name = \"Test_1\"\n    window_size = getWindows(len(group), isSpecialWindow=False, min_bar_length=3)\n    pv = get_bekker_parkinson_vol2(group.ask_1.to_numpy(), group.bid_1.to_numpy(), window=window_size)\n    pv = pv[~np.isnan(pv)]\n    if(len(pv)>0):\n        tmp.loc[:,col_name] = np.median(pv)\n    else:\n        tmp.loc[:,col_name] = 0.0\n\n    col_name = \"Test_2\"\n    window_size = getWindows(len(group), isSpecialWindow=False, min_bar_length=3)\n    pv = get_bekker_parkinson_vol2(group.ask_2.to_numpy(), group.bid_2.to_numpy(), window=window_size)\n    pv = pv[~np.isnan(pv)]\n    if(len(pv)>0):\n        tmp.loc[:,col_name] = np.median(pv)\n    else:\n        tmp.loc[:,col_name] = 0.0\n    #######################################\n    \n    tmp.loc[:,'row_id'] = str(stock_id) + '-' + str(name)\n    #tmp.loc[:,'time_id'] = int(name)\n    return tmp\ndef getFeaturesFromBookData(df, stock_id, n=10):\n    results = df.groupby(['time_id']).apply(Fx, stock_id=stock_id, n=n).reset_index(drop=True)\n    return results","a796749e":"def getDataFromTrade(df):\n    log_ret = log_return(df.price).dropna()\n    rz_vol = realized_volatility(log_ret)\n    \n    tmp = pd.DataFrame()\n    \n    tmp.loc[:,'p_lr_rate'] = [rateLastFirst(log_ret)]\n    \n    tmp.loc[:,'p_price_count'] = count_unique(df['price'].to_numpy())\n    tmp.loc[:,'p_sec_count'] = count_unique(df['seconds_in_bucket'].to_numpy())\n\n    cols_p = []  \n    tmp = getMicrostructuralFeatures(df.loc[:, ['price', 'size']], tmp, col_prefix = 'p_', col_postfix = '', isParkinson=False, isGarman=False, isYyang=False, micro_cols=cols_p)\n\n    tmp.loc[:,'p_size_mean'] = np.mean(df['size']) \n\n    time_id = df.time_id.unique()[0]\n    tmp.loc[:,'time_id'] = time_id\n    return tmp\n\ndef getFeaturesFromTradeData(df):\n    return df.groupby(['time_id']).apply(getDataFromTrade).reset_index(drop=True)","52d0b5be":"# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\n# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\ndef calc_price(df):\n    diff = abs(df.filter(like='price').diff())\n    min_diff = np.nanmin(diff.where(lambda x: x > 0))\n    n_ticks = (diff \/ min_diff).round()\n    return 0.01 \/ np.nanmean(diff \/ n_ticks)\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    stock_id = file_path.split('=')[1]\n    spread = getDataFromBidAsk_numpy(df.to_numpy(),{k: v for v, k in enumerate(df.columns.values)})\n    df_book_data = pd.concat([df, pd.DataFrame(spread)], axis=1)\n    df_book_datar = getFeaturesFromBookData(df_book_data, stock_id, 10)\n    df_prices = df.groupby(['time_id']).apply(calc_price).to_frame('prices').reset_index()\n    \n    df_fill = fix_offsets(df)\n    df_fill = ffill(df_fill)\n    df_fill['ask1_bid1_spread'] = df_fill['ask_price1'] \/ df_fill['bid_price1'] - 1\n    \n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    df['wap3'] = calc_wap3(df)\n    df['wap4'] = calc_wap4(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return_new)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return_new)\n    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return_new)\n    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return_new)\n    # Calculate wap balance\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    #add\n    df['ask1_bid1_spread'] = df['ask_price1'] \/ df['bid_price1'] - 1\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return1': [realized_volatility, realized_quarticity, realized_1, np.std, np.sum],\n        'log_return2': [realized_volatility, realized_quarticity, realized_1, np.std, np.sum],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n        'wap_balance': [np.sum, np.max],\n        'bid_spread':[np.sum, np.max],\n        'ask_spread':[np.sum, np.max],\n        'price_spread': [np.sum, np.max],\n        'total_volume':[np.sum, np.max],\n        'ask1_bid1_spread': [np.sum, np.max],\n        'price_spread2':[np.sum, np.max],\n        'wap_balance': [np.sum, np.max],\n        'volume_imbalance':[np.sum, np.max],\n        'wap1': [np.sum, np.std],\n        'wap2': [np.sum, np.std],\n        'wap3': [np.sum, np.std],\n        'wap4': [np.sum, np.std]\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict, seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    create_feature_dict_time = {\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility]\n    }\n    \n    def aggregateBookData(df_, interval):\n        df = df_.copy()\n        df['interval'] = df['seconds_in_bucket'] \/\/ interval\n\n        df_agg = df.groupby(['time_id', 'interval']).agg(\n                            ask1_bid1_spread_avg = pd.NamedAgg(column='ask1_bid1_spread', aggfunc=np.mean) ).reset_index()\n\n        df_wide = pd.pivot_table(df_agg, values=['ask1_bid1_spread_avg'], \n                                         index='time_id', columns='interval').reset_index().fillna(0)\n        df_wide.columns = ['_'.join(str(e) for e in col) for col in df_wide.columns]\n        df_wide = df_wide.add_suffix(f'_{interval}s_wide').rename(columns={f'time_id__{interval}s_wide' : 'time_id'})\n\n        return df_wide\n    \n    df_interval = aggregateBookData(df_fill, 30)\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n    \n    # Create row_id so we can merge\n    df_interval['row_id'] = df_interval['time_id'].apply(lambda x: f'{stock_id}-{x}')\n    df_interval.drop(['time_id'], axis = 1, inplace = True)\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    df_prices['row_id'] = df_prices['time_id'].apply(lambda x: f'{stock_id}-{x}')\n    df_prices.drop(['time_id'], axis = 1, inplace = True)\n    df_feature = pd.merge(df_feature, df_prices, how='left', on='row_id')\n    df_feature = pd.merge(df_feature, df_book_datar, how='left', on='row_id')\n    df_feature = pd.merge(df_feature, df_interval, how='left', on='row_id')\n    \n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df_trade_datar = getFeaturesFromTradeData(df)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return_new)\n    df['amount']=df['price']*df['size']\n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility, realized_1],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.std, np.sum, np.max, np.min],\n        'order_count':[np.sum,np.max],\n        'amount':[np.sum,np.max,np.min]\n    }\n    create_feature_dict_time = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'order_count':[np.sum],\n        'size':[np.sum]\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff\/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # vol vars\n        \n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    df_feature = df_feature.merge(df_trade_datar, on = ['time_id'], how = 'left')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500', 'time_id__400', 'time_id__300', 'time_id__200', 'time_id__100', 'time_id'], axis = 1, inplace = True)\n    \n    \n    df_feature['vega'] = np.sum(df['price'].values)\/(df_feature['log_return_realized_volatility']+SMALL_F)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 10)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    #df = [for_joblib(stock_id) for stock_id in list_stock_ids]\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df","2561fa74":"def getDataFromTransformedDataFx(df, prefix=''):\n    used_cols = ['log_return1_sum',\n     'log_return1_realized_volatility',\n     'log_return1_realized_quarticity',\n     'log_return1_realized_1',\n     'trade_log_return_realized_volatility',\n     'price_spread_sum',\n     'trade_abs_diff_v',\n     'trade_p_size_mean']\n    for c in used_cols:\n        df.loc[:, prefix+c+'_rate'] = df.loc[:,c] \/ (np.mean(df.loc[:,c]) + SMALL_F)\n        df.loc[:, prefix+c+'_diff'] = df.loc[:,c] - (np.mean(df.loc[:,c]) )\n\n    return df\n\ndef getDataFromTransformedData(df):\n    tmp1 = df.groupby(['stock_id']).apply(getDataFromTransformedDataFx, prefix='stock_id_').reset_index(drop=True)\n    cols_tmp1 = [x for x in tmp1.columns if 'stock_id_' in x]\n    tmp1['row_id'] = tmp1['stock_id'].astype(str) + '-' + tmp1['time_id'].astype(str)\n    cols_tmp1.append('row_id')\n    tmp2 = df.groupby(['time_id']).apply(getDataFromTransformedDataFx, prefix='time_id_').reset_index(drop=True)\n    cols_tmp2 = [x for x in tmp2.columns if 'time_id_' in x]\n    tmp2['row_id'] = tmp2['stock_id'].astype(str) + '-' + tmp2['time_id'].astype(str)\n    cols_tmp2.append('row_id')\n\n    df = df.merge(tmp1.loc[:,cols_tmp1], on = ['row_id'], how = 'left')\n    df = df.merge(tmp2.loc[:,cols_tmp2], on = ['row_id'], how = 'left')\n    \n    print(df.shape)\n\n    return df","8bb6a559":"train, test = read_train_test()\n# Get unique stock ids \n#train_stock_ids = train['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\n#train_ = preprocessor(train_stock_ids, is_train = True)\n#train = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')","6cc165e3":"train = pd.read_csv('..\/input\/optivertraincsvshigeriatmp\/train_v23.csv', index_col=0)","0fe8519b":"train.drop(['trade_vega'], axis=1, inplace=True)\ntest.drop(['trade_vega'], axis=1, inplace=True)","8b0612d3":"train = getDataFromTransformedData(train)\ntest = getDataFromTransformedData(test)","a7569c9e":"# Get group stats of time_id and stock_id\ntrain = get_time_stock(train)\ntest = get_time_stock(test)","0fba4337":"train['size_tau2'] = np.sqrt( 1\/ train['trade_order_count_sum'] )\ntest['size_tau2'] = np.sqrt( 1\/ test['trade_order_count_sum'] )\ntrain['size_tau2_d'] = np.sqrt( 0.33\/ train['trade_order_count_sum'] ) - train['size_tau2']\ntest['size_tau2_d'] = np.sqrt( 0.33\/ test['trade_order_count_sum'] ) - test['size_tau2']","394eb7af":"train.head()","f4551940":"test.head()","75bb9a7b":"train.to_csv('train_tmp.csv')\ntest.to_csv('test_tmp.csv')","3b913b84":"target_name = 'target2'\nscores_folds = {}","4b2b44b6":"def root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred)\/ y_true )))\n    \nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=20, verbose=0,\n    mode='min',restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n    mode='min')","202de155":"# making agg features\n\ntrain_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=42).fit(corr.values)\nprint(kmeans.labels_)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\nmatTest = []\n\nfor i, ind in enumerate(l):\n    print(ind)\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(i)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(i)+'c1'\n    matTest.append ( newDf )\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()","1a3c62c7":"matTest = []\nmat = []\nkmeans = []\n\nmat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","7c495e5c":"nnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',\n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',\n     'log_return1_realized_volatility_5c1',\n     'total_volume_sum_0c1',\n     'total_volume_sum_1c1', \n     'total_volume_sum_3c1',\n     'total_volume_sum_4c1', \n     'total_volume_sum_5c1',\n     'trade_order_count_sum_0c1',\n     'trade_order_count_sum_1c1',\n     'trade_order_count_sum_3c1',\n     'trade_order_count_sum_4c1',\n     'trade_order_count_sum_5c1',\n     'bid_spread_sum_0c1',\n     'bid_spread_sum_1c1',\n     'bid_spread_sum_3c1',\n     'bid_spread_sum_4c1',\n     'bid_spread_sum_5c1',       \n     'ask_spread_sum_0c1',\n     'ask_spread_sum_1c1',\n     'ask_spread_sum_3c1',\n     'ask_spread_sum_4c1',\n     'ask_spread_sum_5c1',\n     'ask1_bid1_spread_sum_0c1',\n     'ask1_bid1_spread_sum_1c1',\n     'ask1_bid1_spread_sum_3c1',\n     'ask1_bid1_spread_sum_4c1',\n     'ask1_bid1_spread_sum_5c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_5c1'] ","e7ff9e34":"train = pd.merge(train,mat1[nnn],how='left',on='time_id')","6ab351ec":"test = pd.merge(test,mat2[nnn],how='left',on='time_id')","4e7d3542":"del mat1,mat2\ngc.collect()","f0507c04":"train.head()","c1600fa3":"test.head()","dcbceb4b":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\ndef train_and_evaluate(train, test):\n    # Hyperparammeters (optimized)\n    seed = 42\n    params = {\n            'learning_rate': 0.1,        \n            'lambda_l1': 2,\n            'lambda_l2': 7,\n            'num_leaves': 800,\n            'min_sum_hessian_in_leaf': 20,\n            'feature_fraction': 0.8,\n            'feature_fraction_bynode': 0.8,\n            'bagging_fraction': 0.9,\n            'bagging_freq': 42,\n            'min_data_in_leaf': 700,\n            'max_depth': 4,\n            'seed': seed,\n            'feature_fraction_seed': seed,\n            'bagging_seed': seed,\n            'drop_seed': seed,\n            'data_random_seed': seed,\n            'objective': 'rmse',\n            'boosting': 'gbdt',\n            'verbosity': -1,\n            'n_jobs':-1,\n            'device': 'gpu'\n        }\n    \n    # Split features and target\n    groups = train['time_id'].astype(int)\n    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n    y = train['target']\n    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n    # Transform stock id to a numeric value\n    x['stock_id'] = x['stock_id'].astype(int)\n    x_test['stock_id'] = x_test['stock_id'].astype(int)\n    # Create out of folds array\n    oof_predictions = np.zeros(x.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(x_test.shape[0])\n    # Create a KFold object\n    kfold = GroupKFold(n_splits = 10)\n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x, y, groups)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        print(x_train.shape, x_val.shape, x.shape)\n        # Root mean squared percentage error weights\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n        model = lgb.train(params = params, \n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          num_boost_round = 10000, \n                          early_stopping_rounds = 100, \n                          verbose_eval = 200,\n                          feval = feval_rmspe)\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val, num_iteration=model.best_iteration)\n        # Predict the test set\n        test_predictions += model.predict(x_test, num_iteration=model.best_iteration) \/ 10\n        \n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    # Return test predictions\n    return test_predictions","94a5faba":"# Traing and evaluate\ntest_predictions = train_and_evaluate(train, test)","ea2cf911":"def rmspe(y_true, y_pred):\n    # Function to calculate the root mean squared percentage error\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\nclass RMSPE(Metric):\n    def __init__(self):\n        self._name = \"rmspe\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_score):\n        \n        return np.sqrt(np.mean(np.square((y_true - y_score) \/ y_true)))\n    \ndef RMSPELoss(y_pred, y_true):\n    return torch.sqrt(torch.mean( ((y_true - y_pred) \/ y_true) ** 2 )).clone()","2a1bce90":"train.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\n\nfill_0_features = ['log_return1_realized_volatility_500',\n'log_return2_realized_volatility_500',\n'log_return3_realized_volatility_500',\n'log_return4_realized_volatility_500',\n'trade_log_return_realized_volatility',\n'trade_log_return_realized_1',\n'trade_seconds_in_bucket_count_unique',\n'trade_size_sum',\n'trade_size_amax',\n'trade_size_amin',\n'trade_order_count_sum',\n'trade_order_count_amax',\n'trade_amount_sum',\n'trade_amount_amax',\n'trade_amount_amin',\n'trade_energy_v',\n'trade_log_return_realized_volatility_500',\n'trade_seconds_in_bucket_count_unique_500',\n'trade_order_count_sum_500',\n'trade_size_sum_500',\n'trade_log_return_realized_volatility_400',\n'trade_seconds_in_bucket_count_unique_400',\n'trade_order_count_sum_400',\n'trade_size_sum_400',\n'trade_log_return_realized_volatility_300',\n'trade_seconds_in_bucket_count_unique_300',\n'trade_order_count_sum_300',\n'trade_size_sum_300',\n'trade_log_return_realized_volatility_200',\n'trade_seconds_in_bucket_count_unique_200',\n'trade_order_count_sum_200',\n'trade_size_sum_200',\n'trade_log_return_realized_volatility_100',\n'trade_seconds_in_bucket_count_unique_100',\n'trade_order_count_sum_100',\n'trade_size_sum_100',\n'trade_p_price_count',\n'stock_id_trade_log_return_realized_volatility_rate',\n'stock_id_trade_log_return_realized_volatility_diff',\n'time_id_trade_log_return_realized_volatility_rate',\n'time_id_trade_log_return_realized_volatility_diff']\n\ntrain[fill_0_features] = train[fill_0_features].fillna(0)\ntest[fill_0_features] = test[fill_0_features].fillna(0)\n\nfeatures = [col for col in list(train) if col not in ['time_id','target','row_id', 'stock_id']]\ntrain[features] = train[features].fillna(train[features].mean())\ntest[features] = test[features].fillna(train[features].mean())","512679ac":"nunique = train[features].nunique()\ntypes = train[features].dtypes\n\ncategorical_columns = []\ncategorical_dims =  {}\n\nqt_lis = []\n\nfor col in features:\n    if  col == 'stock_id':\n        l_enc = LabelEncoder()\n        train[col] = l_enc.fit_transform(train[col].values)\n        test[col] = l_enc.transform(test[col].values)\n        categorical_columns.append(col)\n        categorical_dims[col] = len(l_enc.classes_)\n    else:\n        scaler = StandardScaler()\n        train[col] = scaler.fit_transform(train[col].values.reshape(-1, 1))\n        test[col] = scaler.transform(test[col].values.reshape(-1, 1))\n        \ntrain[features] = train[features].fillna(train[features].mean())\ntest[features] = test[features].fillna(train[features].mean())\n\ncat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n\ncat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]","073f11b0":"tabnet_params = dict(\n    cat_idxs=cat_idxs,\n    cat_dims=cat_dims,\n    cat_emb_dim=1,\n    n_d = 16,\n    n_a = 16,\n    n_steps = 2,\n    gamma = 2,\n    n_independent = 2,\n    n_shared = 2,\n    lambda_sparse = 0,\n    optimizer_fn = Adam,\n    optimizer_params = dict(lr = (0.02)),\n    mask_type = \"entmax\",\n    scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n    scheduler_fn = CosineAnnealingWarmRestarts,\n    seed = 42,\n    verbose = 10   \n)","2ba41479":"groups = train['time_id'].astype(int)\nX = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nX_test = test.drop(['time_id', 'row_id'], axis=1)\nkfold = GroupKFold(n_splits = 5)\n# Create out of folds array\noof_predictions = np.zeros((X.shape[0], 1))\ntest_predictions_tabnet = np.zeros(X_test.shape[0])\n\nfor fold, (trn_ind, val_ind) in enumerate(kfold.split(X, y, groups)):\n    print(f'Training fold {fold + 1}')\n    X_train, X_val = X.iloc[trn_ind].values, X.iloc[val_ind].values\n    y_train, y_val = y.iloc[trn_ind].values.reshape(-1,1), y.iloc[val_ind].values.reshape(-1,1)\n\n\n    clf = TabNetRegressor(**tabnet_params)\n    clf.fit(\n      X_train, y_train,\n      eval_set=[(X_val, y_val)],\n      max_epochs = 200,\n      patience = 50,\n      batch_size = 1024*20, \n      virtual_batch_size = 128*20,\n      num_workers = 4,\n      drop_last = False,\n      eval_metric=[RMSPE],\n      loss_fn=RMSPELoss\n      )\n    \n    saving_path_name = f\".\/fold{fold}\"\n    saved_filepath = clf.save_model(saving_path_name)\n    \n    oof_predictions[val_ind] = clf.predict(X_val)\n    test_predictions_tabnet += clf.predict(X_test.values).flatten()\/5\n    \nprint(f'OOF score across folds: {rmspe(y, oof_predictions.flatten())}')","53bfb2cb":"del train, test\ngc.collect()","c5a5d6c8":"train = pd.read_csv('.\/train_tmp.csv', index_col=0)\ntest = pd.read_csv('.\/test_tmp.csv', index_col=0)","c24ab95b":"colNames = list(train)\n\ncolNames.remove('time_id')\ncolNames.remove('target')\ncolNames.remove('row_id')\ncolNames.remove('stock_id')","ae065f15":"train.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\nqt_train = []\n\nfor col in colNames:\n    #print(col)\n    qt = QuantileTransformer(random_state=42,n_quantiles=4000, output_distribution='normal')\n    train[col] = qt.fit_transform(train[[col]])\n    test[col] = qt.transform(test[[col]])    \n    qt_train.append(qt)","881cb2af":"# making agg features\n\ntrain_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=42).fit(corr.values)\nprint(kmeans.labels_)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\nmatTest = []\n\nn = 0\nfor i, ind in enumerate(l):\n    print(ind)\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(i)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(i)+'c1'\n    matTest.append ( newDf )\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()\n\nmatTest = []\nmat = []\nkmeans = []\n\nmat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","2a25f1bc":"train = pd.merge(train,mat1[nnn],how='left',on='time_id')\ntest = pd.merge(test,mat2[nnn],how='left',on='time_id')","aefe8a4c":"test['target1'] = test_predictions\ntest['target3'] = test_predictions_tabnet","576fdae4":"# kfold based on the knn++ algorithm\n\nout_train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\nout_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n\n#out_train[out_train.isna().any(axis=1)]\nout_train = out_train.fillna(out_train.mean())\nout_train.head()\n\n# code to add the just the read data after first execution\n\n# data separation based on knn ++\nnfolds = 10 # number of folds\nindex = []\ntotDist = []\nvalues = []\n# generates a matriz with the values of \nmat = out_train.values\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\nmat = scaler.fit_transform(mat)\n\nnind = int(mat.shape[0]\/nfolds) # number of individuals\n\n# adds index in the last column\nmat = np.c_[mat,np.arange(mat.shape[0])]\n\n\nlineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n\nlineNumber = np.sort(lineNumber)[::-1]\n\nfor n in range(nfolds):\n    totDist.append(np.zeros(mat.shape[0]-nfolds))\n\n# saves index\nfor n in range(nfolds):\n    \n    values.append([lineNumber[n]])    \n\n\ns=[]\nfor n in range(nfolds):\n    s.append(mat[lineNumber[n],:])\n    \n    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n\nfor n in range(nind-1):    \n\n    luck = np.random.uniform(0,1,nfolds)\n    \n    for cycle in range(nfolds):\n         # saves the values of index           \n\n        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n\n        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n        totDist[cycle] += sumDist        \n                \n        # probabilities\n        f = totDist[cycle]\/np.sum(totDist[cycle]) # normalizing the totdist\n        j = 0\n        kn = 0\n        for val in f:\n            j += val        \n            if (j > luck[cycle]): # the column was selected\n                break\n            kn +=1\n        lineNumber[cycle] = kn\n        \n        # delete line of the value added    \n        for n_iter in range(nfolds):\n            \n            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n            j= 0\n        \n        s[cycle] = mat[lineNumber[cycle],:]\n        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n\n\nfor n_mod in range(nfolds):\n    values[n_mod] = out_train.index[values[n_mod]]","017f75f9":"#https:\/\/bignerdranch.com\/blog\/implementing-swish-activation-function-in-keras\/\nfrom keras.backend import sigmoid\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\n\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\nget_custom_objects().update({'swish': Activation(swish)})","f9f9e150":"hidden_units = (64, 32, 16)\nstock_embedding_size = 24\n\ncat_data = train['stock_id']\n\ndef base_model():\n    \n    # Each instance will consist of two inputs: a single user id, and a single movie id\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(222,), name='num_data')\n\n    #embedding, flatenning and concatenating\n    x = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    y = tf.expand_dims(num_input, axis=1)\n    x = keras.layers.Concatenate(axis=-1)([x, y])\n    x = keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(256, return_sequences=True))(x)\n    x = keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNGRU(128, return_sequences=True))(x)\n    avg_pool = keras.layers.GlobalAveragePooling1D()(x)\n    max_pool = keras.layers.GlobalMaxPooling1D()(x)\n    conc = keras.layers.concatenate([avg_pool, max_pool])\n    out = keras.layers.Flatten()(conc)\n    \n    # Add one or more hidden layers\n    for n_hidden in hidden_units:\n\n        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n        \n\n    #out = keras.layers.Concatenate()([out, num_input])\n\n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n    \n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = out,\n    )\n    \n    return model","28c5a481":"model_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\n\nn_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\nscores_folds[model_name] = []\ncounter = 1\n\nfeatures_to_consider = list(train)\n\nfeatures_to_consider.remove('time_id')\nfeatures_to_consider.remove('target')\nfeatures_to_consider.remove('row_id')\ntry:\n    features_to_consider.remove('pred_NN')\nexcept:\n    pass\n\ntrain[fill_0_features] = train[fill_0_features].fillna(0)\ntest[fill_0_features] = test[fill_0_features].fillna(0)\n\ntrain[features_to_consider] = train[features_to_consider].fillna(train[features_to_consider].mean())\ntest[features_to_consider] = test[features_to_consider].fillna(train[features_to_consider].mean())\n\ntrain[pred_name] = 0\ntest['target2'] = 0","c8cd568b":"for n_count in range(n_folds):\n    print('CV {}\/{}'.format(counter, n_folds))\n    \n    indexes = np.arange(nfolds).astype(int)    \n    indexes = np.delete(indexes,obj=n_count, axis=0) \n    \n    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]],values[indexes[4]],values[indexes[5]],values[indexes[6]],values[indexes[7]], values[indexes[8]]]\n    \n    X_train = train.loc[train.time_id.isin(indexes), features_to_consider]\n    y_train = train.loc[train.time_id.isin(indexes), 'target']\n    X_test = train.loc[train.time_id.isin(values[n_count]), features_to_consider]\n    y_test = train.loc[train.time_id.isin(values[n_count]), 'target']\n    \n    #############################################################################################\n    # NN\n    #############################################################################################\n    \n    model = base_model()\n    \n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.006), #0.005\n        loss=root_mean_squared_per_error\n    )\n    \n    try:\n        features_to_consider.remove('stock_id')\n    except:\n        pass\n    \n    num_data = X_train[features_to_consider]\n    \n    scaler = MinMaxScaler(feature_range=(-1, 1))         \n    num_data = scaler.fit_transform(num_data.values)    \n    \n    cat_data = X_train['stock_id'] \n    target =  y_train\n    \n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.values)\n    cat_data_test = X_test['stock_id']\n\n    model.fit([cat_data, num_data], \n              target,               \n              batch_size=2048,\n              epochs=200, # 1000\n              validation_data=([cat_data_test, num_data_test], y_test),\n              callbacks=[es, plateau],\n              validation_batch_size=len(y_test),\n              shuffle=True,\n             verbose = 1)\n\n    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n    \n    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    \n    tt = scaler.transform(test[features_to_consider].values)\n    train.loc[train.time_id.isin(values[n_count]), pred_name] += preds\n    test[target_name] += model.predict([test['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n       \n    counter += 1\n    features_to_consider.append('stock_id')\n    del model\n    gc.collect()","9e021b5a":"test[target_name] = test[target_name] \/ n_folds\nscore = round(rmspe(y_true = train['target'].values, y_pred = train[pred_name].values),5)\nprint('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))","970414cd":"test['target'] = test['target1'] * 0.2 + test['target2'] * 0.4 + test['target3'] * 0.4\n\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","2b974914":"test[['row_id', 'target', 'target1', 'target2', 'target3']].head(2)","d1a9648b":"## construct features across dataset","712c07f1":"## Model Definition","93ceef1b":"## Constract \\[stock_id,time_id\\] features\n","f36bcb2c":"## collect and preprocess data","cc9f0922":"## Useful functions","f0a50253":"## Prepare test data","4c64b84c":"# add data"}}