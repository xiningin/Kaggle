{"cell_type":{"7b6234d9":"code","3eca7495":"code","0df70bc7":"code","e8a8b95a":"code","f1fbf972":"code","a0540667":"code","d35e70d1":"code","26fdab3c":"code","e9285680":"code","f3b188e4":"code","c5027390":"code","6d123bcb":"code","46c67875":"code","9d41b834":"code","356f448c":"code","d9940921":"code","cfe53ac0":"code","c4d6274f":"code","7d54d8dc":"code","90f0fb48":"code","6128c65d":"markdown"},"source":{"7b6234d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3eca7495":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")","0df70bc7":"! pip install wandb --upgrade","e8a8b95a":"import wandb\nwandb.login(key=secret_value_0)\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","f1fbf972":"import random, torch\n\ndef torch_seed(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)","a0540667":"import transformers, tokenizers\ntransformers.__version__, tokenizers.__version__","d35e70d1":"import sys\n\ndef override_sys_breakpoint(frame=None):\n    from IPython.core.debugger import set_trace\n    set_trace(frame=frame)\n\nsys.breakpointhook = override_sys_breakpoint","26fdab3c":"import pandas as pd\nimport datasets\nfrom transformers import RobertaTokenizerFast, RobertaForSequenceClassification#, Trainer, TrainingArguments\nimport torch.nn as nn\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom tqdm import tqdm\nimport os\nimport re","e9285680":"PERCENTILES = [0.25, 0.5, 0.75, 0.9, 0.95, 0.98]\nMAX_WORDS = 750\nMAX_CHARS = 3_000\nBS_TRAIN = 32\nBS_EVAL = 64\nOVERFLOW_APPROACH = \"chunked\" # \"head\" or \"chunked\"\nCONCAT = \"concat_pool\" # \"mean_pool\", \"max_pool\", \"concat_pool\"\nSAMPLE_TO_1K = False\nfrac = None if SAMPLE_TO_1K else 1\nsample_size = 1_000 if SAMPLE_TO_1K else None\nSEED = 88\n\ntorch_seed(SEED)","f3b188e4":"from sklearn.datasets import fetch_20newsgroups\ndata_home =\"\/kaggle\/working\/newsgroups-marc\/\"\nnewsgroups_train = fetch_20newsgroups(data_home=data_home, subset='train')\nnewsgroups_test = fetch_20newsgroups(data_home=data_home, subset='test')","c5027390":"data_df = (\n    pd.DataFrame(\n        {\n            \"text\": newsgroups_train.data + newsgroups_test.data,\n            \"label\": np.concatenate([newsgroups_train.target, newsgroups_test.target]),\n            \"is_dev\": [False] * len(newsgroups_train.data) + [True] * len(newsgroups_test.data),\n        }\n    ).\n    assign(\n        text=lambda df: (\n            df.text\n            \n            # Remove Emails preambles\n            .replace(re.compile(r\"From: \\S*@\\S*\\s?\"), \"\")\n\n            # Remove extra space\n            .replace(re.compile('\\s+'), \" \")\n\n            # Remove distracting single quotes\n            .replace(re.compile(\"\\'\"), \"\")\n        ),\n        # number of characters\n        n_chars=lambda df: df.text.apply(len),\n        # number of words\n        n_words=lambda df: df.text.str.split().apply(len),\n        # create binary labels\n        blabel=lambda df: np.where(df.label < 10, 0, 1)\n    )\n    # remove very long texts\n    .loc[lambda df: df.n_words.lt(MAX_WORDS) & df.n_chars.lt(MAX_CHARS)]\n    .sample(frac=frac, n=sample_size, random_state=88)\n    .reset_index(drop=True)\n)\nprint(len(data_df))\nprint(\"\\nLabels\")\ndisplay(data_df.label.value_counts())\nprint(\"\\nBinary Labels\")\ndisplay(data_df.blabel.value_counts())\nprint(\"\\nNumber of characters\")\ndisplay(data_df.n_chars.describe(percentiles=PERCENTILES))\nprint(\"\\nNumber of words\")\ndisplay(data_df.n_words.describe(percentiles=PERCENTILES))","6d123bcb":"from transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained(\n    'distilbert-base-uncased',\n    cache_dir=\"\/kaggle\/working\/newsgroups-marc\/\",\n)","46c67875":"# def get_encodings(data):\n#     return tokenizer(data, truncation=True, padding=True)\n\ndef get_data_from_mask(data, mask):\n    return data[mask].reset_index(drop=True)\n\n# train_encodings = get_encodings(list(get_data_from_mask(data_df, ~data_df.is_dev).text))\n# dev_encodings = get_encodings(list(get_data_from_mask(data_df, data_df.is_dev).text))","9d41b834":"\"\"\"\nInspired by https:\/\/github.com\/helmy-elrais\/RoBERT_Recurrence_over_BERT\/blob\/master\/Custom_Dataset_Class.py\nUpdated to new version, where chunking is automatically done\n\"\"\"\nfrom enum import Enum\nfrom typing import Sequence\n\nimport torch\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport re\nfrom torch.utils.data import Dataset\n\n\nclass Approach(Enum):\n    HEAD = \"head\"\n    CHUNKED = \"chunked\"\n\n\nclass NgChunkedDataset(Dataset):\n\n    \"\"\"Make preprocessing, tokenization and transform\n    NG dataset into pytorch DataLoader instance.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer,\n        texts: pd.Series,\n        labels: pd.Series,\n        chunk_len=512,\n        overlap_len=8,\n        approach: Approach = Approach.CHUNKED,\n    ):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.overlap_len = overlap_len\n        self.chunk_len = chunk_len\n        self.approach = approach\n\n    def __getitem__(self, idx):\n        \"\"\"Return a single tokenized sample at a given position [idx] from data\"\"\"\n\n        text = self.texts[idx]\n        label = self.labels[idx]\n        # tokenize text, and if there are more than `self.chunk_len` tokens,\n        # tokens will be an [N x self.chunk_len] tensor\n        tokens = self.tokenizer.encode_plus(\n            text,\n            max_length=self.chunk_len,\n            truncation=True,\n            stride=self.overlap_len,\n            padding=\"max_length\",\n            add_special_tokens=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_overflowing_tokens=(self.approach == Approach.CHUNKED),\n            return_tensors=\"pt\",\n        )\n\n        # flatten `tokens` + return RoBERTa-like dict\n        return self.chunked_tokenize(tokens, label)\n\n    def __len__(self):\n        \"\"\"Return data length\"\"\"\n        return self.labels.shape[0]\n\n    def chunked_tokenize(self, tokens, label):\n        \"\"\"Flatten tokens and return RoBERTa-like dict.\n\n        Parameters\n        ----------\n        tokens: BatchEncoding\n            a tokenized result of one sample from tokenizer.encode_plus method.\n        targets: array\n            labels of each sample.\n\n        Returns\n        -------\n        dict\n            a dictionary that contains\n             - [ids]  token ids\n             - [mask] attention mask of each token\n             - [target_list] list of all sample labels after adding overlap tokens as samples according to the approach used\n             - [num_chunks] length of target_list = number of chunks for sample\n        \"\"\"\n\n        num_chunks = tokens[\"input_ids\"].shape[0]\n\n        return dict(\n            input_ids=torch.stack([input_ids for input_ids in tokens[\"input_ids\"]]),\n            attention_mask=torch.stack(\n                [attention_mask for attention_mask in tokens[\"attention_mask\"]]\n            ),\n            labels=torch.LongTensor([label]),\n            num_chunks=torch.LongTensor([num_chunks]),\n        )\n\n\ndef collate_chunks(batch):\n    return {\n        key: torch.stack([\n            chunk_value\n            for sample_dict in batch\n            for chunk_value in sample_dict[key]\n        ])\n        for key in batch[0].keys()\n    }\n\n\ntrain_dataset = NgChunkedDataset(\n    tokenizer,\n    texts=get_data_from_mask(data_df, ~data_df.is_dev).text,\n    labels=get_data_from_mask(data_df, ~data_df.is_dev).blabel,\n    approach=Approach(OVERFLOW_APPROACH),\n)\ndev_dataset = NgChunkedDataset(\n    tokenizer,\n    texts=get_data_from_mask(data_df, data_df.is_dev).text,\n    labels=get_data_from_mask(data_df, data_df.is_dev).blabel,\n    approach=Approach(OVERFLOW_APPROACH),\n)\n","356f448c":"# for sample in tqdm(train_dataset):\n#     if sample[\"num_chunks\"][0] == 71:\n#         break","d9940921":"text = data_df.text.iloc[32]\nprint(len(text.split()))\ntokens = tokenizer.encode_plus(\n    text,\n    max_length=512,\n    truncation=True,\n    stride=8,\n    padding=\"max_length\",\n    add_special_tokens=True,\n    return_attention_mask=True,\n    return_token_type_ids=False,\n    return_overflowing_tokens=True,\n    return_tensors=\"pt\",\n)\n# print(tokens[\"input_ids\"].shape, tokens.keys(), tokens[\"overflow_to_sample_mapping\"])\nprint(tokens[\"input_ids\"].shape, tokens.keys())\n# tokens[\"input_ids\"]","cfe53ac0":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)  # these are not probas, they're logits (!)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels,\n        preds,\n        average=\"binary\",\n    )\n    acc = accuracy_score(labels, preds)\n    return {\n        \"accuracy\": acc,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n    }","c4d6274f":"# Model\nfrom enum import Enum\n\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom torch.nn import CrossEntropyLoss\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\n\nclass PoolingMethod(Enum):\n    MEAN_POOL = \"mean_pool\"\n    MAX_POOL = \"max_pool\"\n    CONCAT_POOL = \"concat_pool\"\n\n\nEMBEDDING_SIZE = 768\n\n\nclass HierarchicalModel(nn.Module):\n    def __init__(\n        self,\n        cache_dir=\"\/kaggle\/working\/newsgroups-marc\/\",\n        pooling_method: PoolingMethod = PoolingMethod.MEAN_POOL,\n    ):\n        super(HierarchicalModel, self).__init__()\n\n        self.pooling_method = pooling_method\n        self.num_labels = 2\n        self.distilbert = transformers.DistilBertModel.from_pretrained(\n            \"distilbert-base-uncased\",\n            cache_dir=cache_dir,\n        )\n        # TODO marc: move numbers to config\n        concat_pool_on = 1 + (self.pooling_method == PoolingMethod.CONCAT_POOL)\n        self.mixer = nn.Linear(concat_pool_on * EMBEDDING_SIZE, concat_pool_on * EMBEDDING_SIZE)\n        self.dropout = nn.Dropout(0.2)\n        self.classifier = nn.Linear(concat_pool_on * EMBEDDING_SIZE, 2)\n\n    def forward(\n        self,\n        input_ids,\n        attention_mask,\n        num_chunks,\n        labels=None,\n    ):\n        r\"\"\"\n        Simplified from https:\/\/huggingface.co\/transformers\/_modules\/transformers\/models\/distilbert\/modeling_distilbert.html#DistilBertForSequenceClassification\n        and combined with https:\/\/github.com\/helmy-elrais\/RoBERT_Recurrence_over_BERT\/blob\/master\/BERT_Hierarchical.py\n\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the sequence classification\/regression loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n\n        # input_ids: (bs * chunk, seq_len)\n\n        try:\n            distilbert_output = self.distilbert(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n            )\n        except Exception as e:\n            print(input_ids.shape)\n            print(num_chunks)\n            raise RuntimeError(e)\n\n        hidden_state = distilbert_output[0]  # (bs * chunk, seq_len, emb)\n        pooled_output = hidden_state[:, 0]  # (bs * chunk, emb)\n\n        # (bs * chunk, emb) --> tuple of size bs, each with size (chunk, emb)\n        pooled_output = pooled_output.split(num_chunks.tolist())\n\n        # Different methods but all do: tuples --> (bs, emb)\n        # Note: for concat pool is (1 + concat) * emb, but will write just emb hereafter\n        if self.pooling_method == PoolingMethod.MEAN_POOL:\n            pooled_output = torch.stack([torch.mean(sample, 0) for sample in pooled_output])\n        elif self.pooling_method == PoolingMethod.MAX_POOL:\n            pooled_output = torch.stack([torch.max(sample, 0)[0] for sample in pooled_output])\n        elif self.pooling_method == PoolingMethod.CONCAT_POOL:\n            pooled_output = torch.cat(\n                (\n                    torch.stack([torch.mean(sample, 0) for sample in pooled_output]),\n                    torch.stack([torch.max(sample, 0)[0] for sample in pooled_output]),\n                ),\n                dim=1,  # concat over embedding dimension\n            )\n        else:\n            raise NotImplementedError(f\"pooling method: {self.pooling_method}\")\n\n        pooled_output = self.mixer(pooled_output)  # (bs, emb)\n        pooled_output = nn.ReLU()(pooled_output)  # (bs, emb)\n        pooled_output = self.dropout(pooled_output)  # (bs, emb)\n        logits = self.classifier(pooled_output)  # (bs, num_labels)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=distilbert_output.hidden_states,\n            attentions=distilbert_output.attentions,\n        )\n","7d54d8dc":"from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='\/kaggle\/working\/results',          # output directory\n    num_train_epochs=5,              # total number of training epochs\n    per_device_train_batch_size=BS_TRAIN,  # batch size per device during training\n    per_device_eval_batch_size=BS_EVAL,   # batch size for evaluation\n    evaluation_strategy = \"epoch\", #\"epoch\", # \"epoch\", \"steps\"\n    save_strategy= \"epoch\",\n    disable_tqdm = False, \n    load_best_model_at_end=True,\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='\/kaggle\/working\/logs',            # directory for storing logs\n    logging_steps=10,\n    fp16 = True,\n    seed=SEED,\n#     report_to=\"none\", # defaults to \"all\". In this case will do wandb. Use \"none\" to disable\n    run_name = f'distilbert-{OVERFLOW_APPROACH}-{CONCAT}-classification',\n)\n\ntorch.cuda.empty_cache()\nmodel = HierarchicalModel(\n    pooling_method = PoolingMethod(CONCAT),\n    cache_dir=\"\/kaggle\/working\/newsgroups-marc\/\",    \n)\n# model = DistilBertForSequenceClassification.from_pretrained(\n#     \"distilbert-base-uncased\",\n#     cache_dir=\"\/kaggle\/working\/newsgroups-marc\/\"\n# )","90f0fb48":"trainer = Trainer(\n    model=model,                         # the instantiated \ud83e\udd17 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=dev_dataset,             # evaluation dataset\n    compute_metrics=compute_metrics,\n    data_collator=collate_chunks,\n)\ntrainer.train()","6128c65d":"## TODO\n\n* Generate predictions\n* Plot PR curves\n* Go to multi-class and compare to https:\/\/marctorsoc.github.io\/posts\/intro-to-text-classification\/ (probs a random stratified split is enough to compare)\n* Use Fnet"}}