{"cell_type":{"335b26b7":"code","c74df139":"code","6d899705":"code","47c6b202":"code","58258dea":"code","777a3496":"code","39b9ba18":"code","49e18d72":"code","701a29a0":"code","4b590f72":"code","d1be2852":"code","8b6dff17":"code","6031b433":"code","abfa9236":"code","7ac3e33c":"code","5902cdb3":"code","1cf6561c":"markdown","d75a7342":"markdown","8ba8389a":"markdown","927fcc52":"markdown","0090c329":"markdown","4a454b25":"markdown","a1a736bf":"markdown"},"source":{"335b26b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c74df139":"# For deleting any pre-existing models\nfor filename in os.listdir():\n    if filename.endswith('.hdf5'):\n        os.unlink(filename)\nos.listdir()","6d899705":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils\nimport numpy as np","47c6b202":"SEQ_LENGTH = 100","58258dea":"test_x = np.array([1, 2, 0, 4, 3, 7, 10])\n\n# one hot encoding\ntest_y = np_utils.to_categorical(test_x)\nprint(test_x)\nprint(test_y)","777a3496":"# Using keras functional model\ndef create_functional_model(n_layers, input_shape, hidden_dim, n_out, **kwargs):\n    drop        = kwargs.get('drop_rate', 0.2)\n    activ       = kwargs.get('activation', 'softmax')\n    mode        = kwargs.get('mode', 'train')\n    hidden_dim  = int(hidden_dim)\n\n    inputs      = Input(shape = (input_shape[1], input_shape[2]))\n    model       = LSTM(hidden_dim, return_sequences = True)(inputs)\n    model       = Dropout(drop)(model)\n    model       = Dense(n_out)(model)","39b9ba18":"# Using keras sequential model\ndef create_model(n_layers, input_shape, hidden_dim, n_out, **kwargs):\n    drop        = kwargs.get('drop_rate', 0.2)\n    activ       = kwargs.get('activation', 'softmax')\n    mode        = kwargs.get('mode', 'train')\n    hidden_dim  = int(hidden_dim)\n    model       = Sequential()\n    flag        = True \n\n    if n_layers == 1:   \n        model.add( LSTM(hidden_dim, input_shape = (input_shape[1], input_shape[2])) )\n        if mode == 'train':\n            model.add( Dropout(drop) )\n\n    else:\n        model.add( LSTM(hidden_dim, input_shape = (input_shape[1], input_shape[2]), return_sequences = True) )\n        if mode == 'train':\n            model.add( Dropout(drop) )\n        for i in range(n_layers - 2):\n            model.add( LSTM(hidden_dim, return_sequences = True) )\n            if mode == 'train':\n                model.add( Dropout(drop) )\n        model.add( LSTM(hidden_dim) )\n\n    model.add( Dense(n_out, activation = activ) )\n\n    return model","49e18d72":"def train(model, X, Y, n_epochs, b_size, vocab_size, **kwargs):    \n    loss            = kwargs.get('loss', 'categorical_crossentropy')\n    opt             = kwargs.get('optimizer', 'adam')\n    \n    model.compile(loss = loss, optimizer = opt)\n\n    filepath        = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n    checkpoint      = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n    callbacks_list  = [checkpoint]\n    X               = X \/ float(vocab_size)\n    model.fit(X, Y, epochs = n_epochs, batch_size = b_size, callbacks = callbacks_list)","701a29a0":"def generate_text(model, X, filename, ix_to_char, vocab_size):\n    \n    # Load the weights from the epoch with the least loss\n    model.load_weights(filename)\n    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n\n    start   = np.random.randint(0, len(X) - 1)\n    pattern = np.ravel(X[start]).tolist()\n\n    # We seed the model with a random sequence of 100 so it can start predicting\n    print (\"Seed:\")\n    print (\"\\\"\", ''.join([ix_to_char[value] for value in pattern]), \"\\\"\")\n    output = []\n    for i in range(250):\n        x           = np.reshape(pattern, (1, len(pattern), 1))\n        x           = x \/ float(vocab_size)\n        prediction  = model.predict(x, verbose = 0)\n        index       = np.argmax(prediction)\n        result      = index\n        output.append(result)\n        pattern.append(index)\n        pattern = pattern[1 : len(pattern)]\n\n    print(\"Predictions\")\n    print (\"\\\"\", ''.join([ix_to_char[value] for value in output]), \"\\\"\")","4b590f72":"filename    = '..\/input\/game_of_thrones.txt'\ndata        = open(filename).read()\ndata        = data.lower()\n# Find all the unique characters\nchars       = sorted(list(set(data)))\nchar_to_int = dict((c, i) for i, c in enumerate(chars))\nix_to_char  = dict((i, c) for i, c in enumerate(chars))\nvocab_size  = len(chars)\n\nprint(\"List of unique characters : \\n\", chars)\n\nprint(\"Number of unique characters : \\n\", vocab_size)\n\nprint(\"Character to integer mapping : \\n\", char_to_int)","d1be2852":"list_X      = []\nlist_Y      = []\n\n# Python append is faster than numpy append. Try it!\nfor i in range(0, len(data) - SEQ_LENGTH, 1):\n    seq_in  = data[i : i + SEQ_LENGTH]\n    seq_out = data[i + SEQ_LENGTH]\n    list_X.append([char_to_int[char] for char in seq_in])\n    list_Y.append(char_to_int[seq_out])\n\nn_patterns  = len(list_X)\nprint(\"Number of sequences in data set : \\n\", n_patterns)\nprint(list_X[0])\nprint(list_X[1])","8b6dff17":"X           = np.reshape(list_X, (n_patterns, SEQ_LENGTH, 1)) # (n, 100, 1)\n# Encode output as one-hot vector\nY           = np_utils.to_categorical(list_Y)\n\nprint(X[0])\nprint(Y[0])","6031b433":"print(\"Shape of input data \", X.shape, \"\\nShape of output data \", Y.shape)","abfa9236":"model   = create_model(1, X.shape, 256, Y.shape[1], mode = 'train')","7ac3e33c":"train(model, X[:1024], Y[:1024], 2, 512, vocab_size)","5902cdb3":"# Iterating through each model and generating text\nfor filename in os.listdir():\n    if filename.endswith('.hdf5'):\n        print(\"Model Name:\", filename)\n        generate_text(model, X, filename, ix_to_char, vocab_size)","1cf6561c":"Now we add a function to create our LSTM.","d75a7342":"Now we're ready to either train or test our model.","8ba8389a":"Training our model","927fcc52":"This functions returns an array of sequences from the input text file and the corresponding output for each sequence encoded as a one-hot vector.","0090c329":"The fit function will run the input batchwase n_epochs number of times and it will save the weights to a file whenever there is an improvement. This is taken care of through the callback. <br><br>\nAfter the training is done or once you find a loss that you are happy with, you can test how well the model generates text.","4a454b25":"Now that we've imported everything we need form Keras, we're all set to go!\n\nFirst, we load our data.","a1a736bf":"What np_utils.to_categorical does"}}