{"cell_type":{"2d917e2a":"code","9724b2d1":"code","0f044ed0":"code","3bd9b2cc":"code","f8d038d0":"code","9ac796fd":"code","e06d66af":"code","add9558c":"code","8ad724b4":"code","d3531053":"code","0f0aa345":"code","994eb467":"code","f3db2b28":"code","7e3fde04":"code","e736b7f8":"code","f89cf956":"markdown","bdf70dee":"markdown","5c6c7d25":"markdown","e969ff46":"markdown","a2ab4b0e":"markdown","a2f43420":"markdown","6f5ef017":"markdown","93fe41a6":"markdown"},"source":{"2d917e2a":"from collections import Counter\n\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm","9724b2d1":"config = {\n    \"TRAIN_PATH\" : \"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\"\n}","0f044ed0":"train = pd.read_csv(config[\"TRAIN_PATH\"])","3bd9b2cc":"train.head(3)","f8d038d0":"train.shape","9ac796fd":"train_texts = train[\"less_toxic\"].tolist() + train[\"more_toxic\"].tolist()\nprint(\"Train texts: {} | unique: {}\".format(len(train_texts), len(set(train_texts))))","e06d66af":"train_texts_counter = Counter(train_texts)\ntrain_texts_counter.most_common()[:4]","add9558c":"sns.histplot([count for text, count in train_texts_counter.most_common()],\n             discrete=True)","8ad724b4":"anker_set = set(train[\"more_toxic\"].tolist()) & set(train[\"less_toxic\"].tolist())","d3531053":"print(\"anker: {}\".format(len(anker_set)))","0f0aa345":"anker_pair = {\"less_toxic\": [], \"more_toxic\": []}\nfor anker in tqdm(anker_set):\n    less_texts = set(train.loc[train[\"more_toxic\"]==anker, \"less_toxic\"])\n    more_texts = set(train.loc[train[\"less_toxic\"]==anker, \"more_toxic\"])\n    for less_text in less_texts:\n        for more_text in more_texts:\n            if less_text == more_text:\n                continue\n            anker_pair[\"less_toxic\"].append(less_text)\n            anker_pair[\"more_toxic\"].append(more_text)\nanker_pair[\"worker\"] = [-1] * len(anker_pair[\"less_toxic\"])","994eb467":"anker_pair = pd.DataFrame(anker_pair)\nanker_pair.head(5)","f3db2b28":"train_augmented = pd.concat([train, anker_pair]).reset_index(drop=True)\nprint(\"Train: {} + Augment: {} -> All: {}\".format(len(train), len(anker_pair), len(train_augmented)))","7e3fde04":"train_augmented.head(5)","e736b7f8":"train_augmented.to_csv(\"train_augmented.csv\", index=False)","f89cf956":"There are duplicates texts in the training data pairs.\nIt may help to augment training data.","bdf70dee":"# Configs","5c6c7d25":"# Import","e969ff46":"# Augmentation","a2ab4b0e":"- I tried to augment the data using duplicates.","a2f43420":"- There are pairs where A<B and B<A.\n- So, maybe more data will only confuse the model.","6f5ef017":"# About Data","93fe41a6":"- If A<B and B<C, then A<C  (Ideally)"}}