{"cell_type":{"989e880f":"code","eed788cf":"code","7d51fdf0":"code","05a67b4c":"code","ad2e18dc":"code","e1c01d38":"code","f5872f0a":"code","6895f344":"code","da0ab34c":"code","e97c32a7":"code","26aa282f":"code","f08e9b73":"code","08a850ab":"code","8086d21f":"code","7ce696d6":"code","a55439bd":"code","55405a1d":"code","929700cd":"code","c08a58c1":"code","7c1ef567":"code","dc4e175e":"code","926474b1":"code","32291d34":"code","e2f3ef86":"code","1d9227db":"code","ddfdc817":"code","94ef70bc":"code","2a54748a":"code","12ef807e":"code","eda23e57":"code","fdbd36fa":"code","8e55c1ce":"code","3fbd675f":"code","5d666917":"code","575f057e":"code","090c0fed":"code","e3803d53":"code","da544ef0":"code","8e77b8cb":"code","f90b1edc":"code","81b9df8a":"code","02788e40":"code","28ce8507":"code","037d71c6":"code","5914e77c":"code","810dae0e":"code","f8a3795c":"code","9bff80d1":"code","251964c1":"code","1d2825ea":"code","26bb4f85":"code","561a46c9":"code","17310c6e":"code","bd134289":"code","16f973e2":"code","208bdf7f":"code","f4b682fb":"code","b7a945d9":"code","78474d75":"code","a93d65b5":"code","e8119c2a":"code","1ce70482":"code","ae6d24d7":"code","dbe8d38e":"code","c22e3d97":"code","a5f8cd81":"code","652685e9":"code","ce2ec8e1":"code","5e9410df":"code","55c50b8c":"code","05b556b1":"code","2bf702ed":"code","c070255b":"code","a5e23975":"code","d31df36d":"code","1f66c859":"code","96d60a90":"code","5a956eb6":"code","527546ad":"code","f1a49013":"code","7d132a0e":"code","4d1a87bc":"code","548b91c8":"code","8effe96e":"code","bfedab65":"code","78b73700":"code","fa18c724":"code","90807776":"code","9f91f453":"code","edf0a183":"code","61b424a6":"code","891a4af7":"code","896089a6":"code","9f3e682b":"code","01b6ec82":"code","c4874a0b":"code","bdabb648":"code","aa6f1478":"code","d32fe86b":"code","e36accfe":"code","e10bbdaf":"code","dba6abad":"code","9e69038c":"code","1b2ea827":"code","a72bda1d":"code","bba01f75":"code","c64d5f91":"code","9a9a4f00":"code","0a8ac04c":"code","5e798c53":"code","ca7ca9f4":"code","080a4449":"code","7a078d00":"code","738a2f32":"code","a250c40d":"code","52d886f5":"code","ef980332":"code","0bcb6afa":"code","5e94ae0c":"code","241fef61":"code","1a265f92":"code","b5034b84":"code","f67c6fda":"code","58eef090":"code","feff5a04":"code","cbadfb4d":"code","b9ec1009":"code","8b5db2ab":"code","04ada3c8":"code","22c438ef":"code","d522712e":"markdown","0651d65d":"markdown","594abdf4":"markdown","3ca0634f":"markdown","d51ca0e4":"markdown","a6477349":"markdown","ff5d7bb2":"markdown","ff788054":"markdown","367c777f":"markdown","dc51fe07":"markdown","83741f50":"markdown","2deb269f":"markdown","80efcaa6":"markdown","9278e9ec":"markdown","7cb616b4":"markdown"},"source":{"989e880f":"\"\"\"\n1) Prepare Problem\na) Load libraries\n\"\"\"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Lets see what other libraries I will be using\n# Keras\n# sklearn","eed788cf":"\"\"\"\n1) Prepare Problem\nb) Load dataset\n\"\"\"\nitems=pd.read_csv('..\/input\/items.csv')\nitem_categories=pd.read_csv('..\/input\/item_categories.csv')\nshops=pd.read_csv('..\/input\/shops.csv')\n\n# What about these *.gz files?\n# It is a compressed format: \"For on-the-fly decompression of on-disk data\"\ntest=pd.read_csv('..\/input\/test.csv.gz',compression='gzip')\nsample_submission=pd.read_csv('..\/input\/sample_submission.csv.gz',compression='gzip')\nsales_train=pd.read_csv('..\/input\/sales_train.csv.gz',compression='gzip')","7d51fdf0":"items.info()","05a67b4c":"items.head()","ad2e18dc":"items.describe()","e1c01d38":"item_categories.info()","f5872f0a":"item_categories.head()","6895f344":"item_categories.describe()\n# Gives descriptive statistics on quantitative features","da0ab34c":"shops.info()","e97c32a7":"shops.head()","26aa282f":"shops.describe()","f08e9b73":"test.info()","08a850ab":"test.head()","8086d21f":"test.describe()","7ce696d6":"sample_submission.info()","a55439bd":"sample_submission.head()","55405a1d":"sample_submission.describe()","929700cd":"sales_train.info()","c08a58c1":"sales_train.head()","7c1ef567":"sales_train.describe()","dc4e175e":"sales_train.info()","926474b1":"sales_train.describe()","32291d34":"sales_train.item_price.hist()","e2f3ef86":"sales_train.item_price.value_counts()","1d9227db":"sales_train.item_price.nunique()","ddfdc817":"sales_train.item_price.max()","94ef70bc":"print(sales_train[sales_train.item_price==sales_train.item_price.max()])","2a54748a":"print(sales_train[sales_train.item_price==sales_train.item_price.max()].item_id)","12ef807e":"print(items[items.item_id==6066])","eda23e57":"# Radmin is a remote control software - dont think that it is that expensive. Let's check if it was sold for \"normal\" prices\nprint(sales_train[sales_train.item_id==6066])","fdbd36fa":"# Only this one time. Interesting. Then maybe it is right. One huge license?\n# Let's see if there are other Radmin versions\n# and if this is the only outlier in price\nprint(sales_train[sales_train.item_price>50000])","8e55c1ce":"# ok lets leave it in for now.\nsales_train[sales_train.item_price<60000].item_price.hist()","3fbd675f":"sales_train[sales_train.item_price<30000].item_price.hist()","5d666917":"sales_train[sales_train.item_price<15000].item_price.hist()","575f057e":"sales_train[sales_train.item_price<5000].item_price.hist()","090c0fed":"sales_train[sales_train.item_price<3000].item_price.hist()","e3803d53":"sales_train[sales_train.item_price<1000].item_price.hist()","da544ef0":"# So definetly lets build some categories on price. There seems to be mayority is small B2C business but there are also big B2B deals.\n\n# In general I should understand more what actually the products are:\nprint(item_categories.head(300))","8e77b8cb":"# Playstation, X-Box and kyrillic things.\n# Lets translate the column (and also the shop column - check if we can see cities)\n\"\"\"from textblob import TextBlob\n\nitem_categories['english'] = item_categories['item_category_name'].str.encode('cp437', 'ignore').apply(lambda x:TextBlob(x.strip()).translate(to='en'))\n\"\"\"\n\"\"\"\nfrom unidecode import unidecode\nitem_categories['english'] = unidecode(item_categories['item_category_name'])\n\"\"\"\n# 3rd solution worked out: https:\/\/stackoverflow.com\/questions\/14173421\/use-string-translate-in-python-to-transliterate-cyrillic\nsymbols=(u\"\u0430\u0431\u0432\u0433\u0434\u0435\u0451\u0436\u0437\u0438\u0439\u043a\u043b\u043c\u043d\u043e\u043f\u0440\u0441\u0442\u0443\u0444\u0445\u0446\u0447\u0448\u0449\u044a\u044b\u044c\u044d\u044e\u044f\u0410\u0411\u0412\u0413\u0414\u0415\u0401\u0416\u0417\u0418\u0419\u041a\u041b\u041c\u041d\u041e\u041f\u0420\u0421\u0422\u0423\u0424\u0425\u0426\u0427\u0428\u0429\u042a\u042b\u042c\u042d\u042e\u042f\", u\"abvgdeejzijklmnoprstufhzcss_y_euaABVGDEEJZIJKLMNOPRSTUFHZCSS_Y_EUA\")\nenglish = {ord(a):ord(b) for a, b in zip(*symbols)}\n\nitem_categories['items_english'] = item_categories['item_category_name'].apply(lambda x: x.translate(english))\n\nprint(item_categories.items_english.head(100))\n\n# Observations:\n# In categories are meta-categories: Accessories, Console, PC, programs, music...\n# Added to to-do list: Take these meta-categories as features\n","f90b1edc":"#Split the metacategories with the \"-\"\nitem_categories[\"meta_category\"]=item_categories.items_english.apply(lambda x:x.split(\" - \")[0])\nprint(item_categories.meta_category.head(100))","81b9df8a":"item_categories.head()","02788e40":"print(item_categories.meta_category.unique())\nprint(item_categories.meta_category.nunique())\n#Great! Only 20 makro-categories\nprint(item_categories.meta_category.value_counts())\nprint(item_categories.meta_category)\n# Of course: I need to put the makro-categories into the data","28ce8507":"shops.info()\n\n# Translate shop names\nshops['shops_english'] = shops['shop_name'].apply(lambda x: x.translate(english))\nprint(shops.shops_english.head(100))\n\n# YES! First word is the city! Great feature to extract! Another \"Makro-category\"\n\"\"\"\n# And because it is only 60 objects this can even be done and checked manually\nshops[\"town\"] =[\"Yakutsk\",\"Yakutsk\",\"Adygea\",\"Balasiha\",\"Volzhskij\",\"Vologda\",\"Voronej\",\"Voronej\",]\n\"\"\"\n# No this was to stupid:\nshops[\"town\"]=shops.shops_english.apply(lambda x:x.split()[0])\nprint(shops.town)\n\n# While doing this and researching cities next idea: Another makro feature of \"regions\" eg Balashiha belongs to moscow region\nshops[\"region\"]=[\"Sakha\",\"Sakha\",\"Adygea\",\"Moscow\",\"Volgograd\", \"Vologda\", \"voronezh\",\"voronezh\",\"voronezh\",\"Vyezdnaa\", \"Moscow\", \"Moscow\",\"Internet\", \"tatarstan\", \"tatarstan\",\"Kaluga\", \"Moscow\", \"Moscow\", \"Moscow\", \"Kursk\", \"Moscow\", \"Moscow\", \"Moscow\", \"Moscow\", \"Moscow\", \"Moscow\", \"Moscow\", \"Moscow\", \"Moscow\", \"Moscow\", \"Moscow\", \"Moscow\", \"Moscow\",\"Moscow\",\"novgorod\",\"novgorod\",\"novosibirsk\",\"novosibirsk\",\"omsk\",\"rostov\",\"rostov\",\"rostov\",\"Saint Peterburg\",\"Saint Peterburg\",\"samara\",\"samara\",\"moscow\",\"Khanty-Mansi\",\"Tomsk\",\"Tyumen\",\"tyumen\",\"tyumen\",\"Bashkortostan\",\"Bashkortostan\",\"moscow\",\"zifrovoj\",\"moscow\",\"sakha\",\"sakha\",\"yaroslavl\"]\nprint(shops.town.nunique())\nprint(shops.region.nunique())\n# hmmm didn't help much - only 6 towns that belong to Moscow region\n\nshops.to_csv('final_shops.csv',index=False)","037d71c6":"# Always print (parts of) data that you are examining just to get an idea\n# done\nprint(test.shape)\nprint(sales_train.shape)\n\n# 3 features missing in test\nprint(test.columns)\nprint(sales_train.columns)\n\nprint(test.head())\n# ok test is really only the form I need to fill. per shop per item forecast revenue for the specific month\n# Therefore need to split later training data into train & validation set\n# Feedback on test-set will be the evaluation via Kaggle and\/or coursera","5914e77c":"sales_train.columns\n# Let's start with the dates column\nsales_train['day'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y').dt.day\nsales_train['month'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y').dt.month\nsales_train['year'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y').dt.year\nsales_train['weekday'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y').dt.dayofweek\nsales_train.columns\nprint(sales_train.head())","810dae0e":"# Dates look ordered and shops also\nsales_train.date_block_num.plot()","f8a3795c":"sales_train.shop_id.plot(figsize=(20,4))\n# Interesting. There is a rythm to it","9bff80d1":"sales_train.weekday[0:100].plot(figsize=(20,4))","251964c1":"sales_train.head(100)\n\"\"\"\nAha. Order of train set is by\n- month\n- shops per month\n- item_id per shop\n- dates of items per shop\n\"\"\"","1d2825ea":"sales_train[3000:4000]\n# No hypotheses from above is wrong. Shop 25 appears again after shop 24","26bb4f85":"sales_train.item_id[1:10000].plot(figsize=(20,4))\n# There seem to be groups. Maybe it has to do with categories?","561a46c9":"sales_train=sales_train.merge(items, how='left')\nsales_train=sales_train.merge(item_categories,how=\"left\")\nsales_train=sales_train.merge(shops,how=\"left\")","17310c6e":"sales_train.head()","bd134289":"sales_train.drop(\"item_name\",axis=1,inplace=True)\nsales_train.drop(\"shop_name\",axis=1,inplace=True)\nsales_train.drop(\"item_category_name\",axis=1,inplace=True)\nsales_train.head()","16f973e2":"sales_train.isnull().values.any()\n# No values NaN\n# Maybe NaN values have been replaced by a number? Carefully check for outliers","208bdf7f":"sales_train.head(200)\n# Can't see a strict order. Kind of per shop. Kind of per item_id. ","f4b682fb":"sales_train[\"revenue\"]=sales_train.item_cnt_day * sales_train.item_price","b7a945d9":"sales_train.groupby(\"year\").sum()\n# Interesting: 2015 lower revenue - no! We dont have full years. Reaches from beginning of 2013 to October 2015","78474d75":"sales_train.groupby(\"date_block_num\").sum()[\"revenue\"].plot()\n# Very clearly typical retail pattern: Peak at christmas and low in Summer","a93d65b5":"sales_train.groupby(\"weekday\").sum()[\"revenue\"].plot()\n# looks like Friday, Saturday and Sunday are the busiest days","e8119c2a":"sales_train.groupby(\"shop_id\").sum()[\"revenue\"].plot.bar()\n# Significant differences\n# But careful: Could be that shops opened later or closed earlier\n# And what is with the online shop?","1ce70482":"sales_train[sales_train[\"shops_english\"]==\"Internet-magazin CS\"][\"revenue\"].sum()\n# 1.1 in above scale so not the most big one","ae6d24d7":"sales_train[sales_train[\"shops_english\"]==\"Internet-magazin CS\"][\"revenue\"].plot()\n# funny spikes\n# I expected steady and increasing sales if it would be an online shop","dbe8d38e":"sales_train[sales_train[\"shops_english\"]==\"Internet-magazin CS\"].groupby(\"date_block_num\").sum()[\"revenue\"].plot()","c22e3d97":"# How does this look for other shops?\nsales_train[sales_train[\"shops_english\"]==\"Vyezdnaa Torgovla\"][\"revenue\"].plot()\n# Spikes seem to be rather normal when larger things are being sold.\n# These \"things\" need to be looked into much deeper and when they occure. They have a mayor impact! \n# How can tihs be modelled? Is this yearly licenses? Or random occurence and I should model a random? ","a5f8cd81":"sales_train[sales_train[\"shops_english\"]==\"Vyezdnaa Torgovla\"].groupby(\"date_block_num\").sum()[\"revenue\"].plot()\n# This looks very strange\n# Has it to do with a test-train split? No data for monthes 23-31?","652685e9":"sales_train.groupby(\"shop_id\").sum()[\"revenue\"]","ce2ec8e1":"# Understand the training data structure a bit better\nsales_train.shop_id.plot(figsize=(20,4))\n# Why so irregular?","5e9410df":"sales_train.groupby(\"date_block_num\").count().town.plot()\n# The number of transactions is shrinking!","55c50b8c":"sales_train.groupby(\"date_block_num\").mean().item_price.plot()\n# But because average price is increasing stronger the revenue is increasing","05b556b1":"sales_train.groupby(\"date_block_num\").mean().revenue.plot()","2bf702ed":"sales_train.groupby(\"date_block_num\").sum().item_cnt_day.plot()\n#Just doublechecking it is not only number of transactions but also total items","c070255b":"# https:\/\/tradingeconomics.com\/russia\/inflation-rate-mom\n# Inflation rate in russia over the period was oscillating between 0 and 1%\n# except one huge peak (to 4% per month very shortly) end of 2014, beginning 2015\n# Due to lower oil prices and Western sanctions imposed over Ukraine\n\n# https:\/\/www.quora.com\/Economy-of-Russia-What-caused-the-high-inflation-in-Russia-in-2014-and-2015\n# It translates to a yearly inflation rate of 17%    ","a5e23975":"sns.pairplot(sales_train)\n\n# if you work in the kernel you should de-activate this as it takes a long time\n\n\"\"\"Observations:\n- one extreme outlier in price\n- one extreme outlier in cnt_day\n- one month (~10 date_block_num) has a lot of high revenue items\n- certain item_ids have wide range of revenues, some have outliers\n\"\"\"","d31df36d":"sales_train.groupby([\"shop_id\",\"date_block_num\"]).sum()\n# Definetly big difference in how long shops are on the market","1f66c859":"shop_life=pd.DataFrame(columns=[\"shop_id\",\"Start\", \"Stop\"])\nshop_life[\"shop_id\"]=np.arange(60)\nshop_life[\"Start\"]=sales_train.groupby(\"shop_id\")[\"date_block_num\"].min()\nshop_life[\"Stop\"]=sales_train.groupby(\"shop_id\")[\"date_block_num\"].max()\nshop_life.merge(shops, how=\"left\").drop(\"shop_name\",axis=1)\nprint(shop_life)\n\"\"\"\nObservations:\n- shops 10 and 11 have the same name, just ^2 and ? -> Check if shop 10 is empty at month 25 (a)\n- shops 39 and 40 seem to be the same? (b)\n- definetly need to check what shops are in the test-set (c)\n- should closed shops be considered? (d)\n\"\"\"","96d60a90":"# (a)\nsales_train[(sales_train[\"shop_id\"]==10) & (sales_train[\"date_block_num\"]==25)]","5a956eb6":"sales_train[(sales_train[\"shop_id\"]==11) & (sales_train[\"date_block_num\"]==25)]","527546ad":"sales_train[(sales_train[\"shop_id\"]==10) & (sales_train[\"date_block_num\"]==24)]","f1a49013":"sales_train[(sales_train[\"shop_id\"]==10) & (sales_train[\"date_block_num\"]==26)]","7d132a0e":"sales_train[(sales_train[\"shop_id\"]==11) & (sales_train[\"date_block_num\"]==24)]","4d1a87bc":"sales_train[(sales_train[\"shop_id\"]==11) & (sales_train[\"date_block_num\"]==26)]","548b91c8":"# Good. Brute-force but clear.\n# Let's have a 100% picture:\nsales_train[(sales_train[\"shop_id\"]==10) | (sales_train[\"shop_id\"]==11)].groupby([\"shop_id\",\"date_block_num\"]).sum()\n# Yes, definetly.","8effe96e":"sales_train.loc[sales_train[\"shop_id\"]==11,\"shop_id\"]=10\nsales_train[sales_train[\"shop_id\"]==11]","bfedab65":"sales_train[(sales_train[\"shop_id\"]==10)&(sales_train[\"date_block_num\"]==25)]","78b73700":"sales_train.to_csv('sales_train.csv',index=False)","fa18c724":"# Good. Next one:\n# b) shops 39 and 40 seem to be the same?\nsales_train[(sales_train[\"shop_id\"]==39) | (sales_train[\"shop_id\"]==40)].groupby([\"shop_id\",\"date_block_num\"]).sum()\n# No, seems to be two separate shops. Both opened in month 14, one closed earlier than the other","90807776":"#c) Check what shops are in the test-set\nprint(sorted(test.shop_id.unique()))\ntest_list=list(test.shop_id.unique())\ncomplete_list=list(range(60))\nout_of_test=[x for x in complete_list if x not in test_list]\nprint(out_of_test)\nprint(shop_life[shop_life[\"Stop\"]<33])\n# 9, 11, 20, are not in test but were active in time_period 33\n# What could be the reason?\n# Maybe they closed then?\n# A good question is whether the train model should also look at the shops that are in test!?\nprint(shops.loc[9])\n#print(shops.loc[11])\n# Yes of course this one I deleted manually\nprint(shops.loc[20])\nsales_train[(sales_train[\"shop_id\"]==9) | (sales_train[\"shop_id\"]==20)].groupby([\"shop_id\",\"date_block_num\"]).sum()[\"revenue\"]\n# Aha, yet another trick. There is data only for limited periods for these shops. \n# Lets check if this is the reason and others are consistently in business\nsales_train[(sales_train[\"shop_id\"]==3) | (sales_train[\"shop_id\"]==24)].groupby([\"shop_id\",\"date_block_num\"]).sum()[\"revenue\"]\n# Yes looks fine\n\n# I think it depends what I want to achieve whether i include these shops or not.\n# Definetly a kind of different distribution in train and test\n","9f91f453":"# I want to see KPIs over time (prices, revenue per shop per month)\n# Let's start with prices\nsales_train.groupby(\"item_id\").sum()\n# Clearly to be seen some items only very short time in sale","edf0a183":"sales_train.groupby(\"item_id\").sum()[\"revenue\"].hist(figsize=(20,4),bins=100)\n# many many items with little revenue","61b424a6":"sales_train.groupby(\"item_category_id\").sum()","891a4af7":"sales_train.groupby(\"item_category_id\").sum()[\"revenue\"].hist(figsize=(20,4),bins=100)\n# many many items with little revenue","896089a6":"sales_train.groupby([\"date_block_num\",\"item_category_id\"]).sum()[\"revenue\"].unstack()\n# unstack is a great function!\n# https:\/\/scentellegher.github.io\/programming\/2017\/07\/15\/pandas-groupby-multiple-columns-plot.html\n# https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.unstack.html\n# https:\/\/nikgrozev.com\/2015\/07\/01\/reshaping-in-pandas-pivot-pivot-table-stack-and-unstack-explained-with-pictures\/\n\nsales_train.groupby([\"date_block_num\",\"item_category_id\"]).sum()[\"revenue\"].unstack().plot(figsize=(20,20))","9f3e682b":"# Let's just look at growth rates of categories (CAGR's)","01b6ec82":"sales_train.groupby([\"date_block_num\",\"meta_category\"]).sum()[\"revenue\"].unstack().plot(figsize=(20,20))\n# Consoles with 2 big spikes\n# Exactly at Christmas! :-) Playstations from Santa Claud: Or Father Frost as it is in Russia I think","c4874a0b":"sales_train.groupby([\"date_block_num\",\"shop_id\"]).sum()[\"revenue\"].unstack().plot(figsize=(20,20))\n# of course same spikes. Let's normalize","bdabb648":"sales_train.groupby([\"date\",\"shop_id\"]).sum()[\"revenue\"].unstack().plot(figsize=(20,20))\n# Very spiky (see above where I saw spikes for first time)\n# Looks difficult to predict\n# Probably a good idea to modell low-cost predictable items separately and then model random big sales","aa6f1478":"# What is total revenue of company?\nsales_train.groupby([\"date_block_num\"]).sum()[\"revenue\"].plot(figsize=(20,5))","d32fe86b":"# I am working through the question list now - so maybe a bit random\n# Slowly I start to think more about how actually to do the modelling\n# At the moment I have lost a bit the track on how this will look like and how this actually works.\n\n# Products out of sale:\n#All item ids that are being sold in the last month in the train data\nsales_train[sales_train.date_block_num==33][\"item_id\"]\nprint(sales_train[sales_train.date_block_num==33][\"item_id\"].nunique())\n# All item ids from all times\nprint(sales_train.item_id.nunique())\n\n# Wow: more than 3\/4 of items out of sale. But makes sense. Music titles, old programs, old consoles and PCs\n\n# How about the test data?\nprint(test.item_id.nunique())\n# 300 less than was sold in the month before\n# Are there items that are new?\na=set(test.item_id)\nb=set(sales_train[sales_train.date_block_num==33][\"item_id\"])","e36accfe":"new_in_test_items=[x for x in a if x not in b]\nprint(len(new_in_test_items))\nprint(new_in_test_items[0:100])\nprint(sales_train[sales_train[\"item_id\"]==8214])\n# ok this is an item that is not often sold (only 2 times in dataset)\nprint(sales_train[sales_train[\"item_id\"]==4893]) #(18 times in dataset)\n\n# a) Let's check how many items are just sold <100 times - maybe different categories? FMCC vs. B2B\n\n# Really new items:\nc=set(sales_train[\"item_id\"])\nprint(len(c))\n\nnew_in_test_items2=[x for x in a if x not in c]\nprint(len(new_in_test_items2))\nprint(new_in_test_items2[0:100])\n\nprint(sales_train[sales_train[\"item_id\"]==83])\nprint(sales_train[sales_train[\"item_id\"]==430])\n\n# ok it is working. But funny that such low IDs appear for the first time. \n# b) Aren't the IDs an ever growing item? Or are certain ID-number-blocks reserved for categories?","e10bbdaf":"# How many items are only sold rarely?","dba6abad":"# Aren't the IDs an ever growing item? Or are certain ID-number-blocks reserved for categories?\nsales_train[sales_train.item_category_id==11]\n# Interesting, the same item (PS3) has many different product ids","9e69038c":"print(sales_train[sales_train.items_english==\"Igrovye konsoli - PS3\"])","1b2ea827":"print(sales_train[sales_train.items_english==\"Igrovye konsoli - PS3\"][\"item_id\"].unique())\nprint(sales_train[sales_train.items_english==\"Igrovye konsoli - PS3\"][\"item_id\"].nunique())\n\nprint(sales_train[sales_train.items_english==\"Igrovye konsoli - PS3\"][\"item_price\"].unique())\nprint(sales_train[sales_train.items_english==\"Igrovye konsoli - PS3\"][\"item_price\"].nunique())","a72bda1d":"prices_PS3=sales_train[sales_train.items_english==\"Igrovye konsoli - PS3\"][\"item_price\"]\n\nplt.figure(figsize=(20, 8), dpi=80)\nplt.scatter(prices_PS3.index, prices_PS3,s=0.1)\n# quite a spread\n# Why so many item_ids? Does this correlate price per ID?","bba01f75":"sales_train[\"value\"]=1\npivot=pd.pivot_table(sales_train[sales_train.items_english==\"Igrovye konsoli - PS3\"], values=\"value\", index=[\"item_id\"], columns=\"item_price\", fill_value=0) \n# No, it is not one item id per price\n","c64d5f91":"print(new_in_test_items)","9a9a4f00":"# Now examine price changes and price developments of items\nsales_train.groupby([\"item_id\",\"item_price\"]).sum()","0a8ac04c":"# Now examine the categories\n\n# How many items in each category\n#sales_train.groupby(\"category_id\",\"item_id\").count()","5e798c53":"# Save the final dataset (to not always calculate everything above when restarting the Kernel)\nsales_train.to_csv('mycsvfile.csv',index=False)","ca7ca9f4":"print(os.listdir(\"..\/\"))","080a4449":"print(os.listdir(\"..\/working\"))","7a078d00":"# Load the pre-processed dataset to continue from here without always calculating everything above\ntrain=pd.read_csv('..\/working\/mycsvfile.csv')","738a2f32":"sample_submission.head(100)\n# Do I have to predict only the amount, not the revenue \/ price???\n# Indeed \"We are asking you to predict total sales for every product and store in the next month.\"\n# So price information is helpful only as a feature","a250c40d":"test.head()","52d886f5":"# submission.to_csv('submission.csv',index=False)\n# Very first submission resulted in a score of 1,8 something - an extremely bad score place 863 of 950\n# I had the sum of items per month completely wrong","ef980332":"interim= sales_train[sales_train[\"date_block_num\"]==33].groupby([\"shop_id\", \"item_id\"],as_index=False).sum()[[\"shop_id\",\"item_id\",\"item_cnt_day\"]]\ninterim[\"item_cnt_day\"].clip(0,20,inplace=True)\ninterim","0bcb6afa":"# the item_cnt_month are not properly entered into the grid\ninterim2=pd.merge(test, interim, how=\"left\", left_on=[\"shop_id\",\"item_id\"], right_on = [\"shop_id\",\"item_id\"])\ninterim2.info()\ninterim2=interim2[[\"ID\",\"item_cnt_day\"]]\ninterim2.columns=[\"ID\",\"item_cnt_month\"]\ninterim2.fillna(0,inplace=True)\ninterim2","5e94ae0c":"interim2.to_csv('submission2.csv',index=False)\n# \"Your submission scored 16.05675\" ? -> Forgot to clip values\n# 1.96214: Still worse than before and lower than mentioned!? -> I had the summing up completely wrong\n\n# v2: Yes: 1.02172, place 376","241fef61":"#Let's try November values from last year\ninterim= sales_train[sales_train[\"date_block_num\"]==22].groupby([\"shop_id\", \"item_id\"],as_index=False).sum()[[\"shop_id\",\"item_id\",\"item_cnt_day\"]]\ninterim[\"item_cnt_day\"].clip(0,20,inplace=True)\ninterim2=pd.merge(test, interim, how=\"left\", left_on=[\"shop_id\",\"item_id\"], right_on = [\"shop_id\",\"item_id\"])\ninterim2=interim2[[\"ID\",\"item_cnt_day\"]]\ninterim2.columns=[\"ID\",\"item_cnt_month\"]\ninterim2.fillna(0,inplace=True)\ninterim2.to_csv('submission3.csv',index=False)\n\n# Interesting: 1.60233: Much worse. So October-November seasonal effect smaller than November-November between 2 years","1a265f92":"# Before starting with more complicated methods lets model something meaningful for items that are sold for the first time\n# And lets check if there werer items in the data sold in September, but not in October, but then in November again\n\n# Data that was for the first time in test:\nnew_in_test_items2\nprint(test[test.item_id.isin(new_in_test_items2)])\n# No further information on the items. Can categories be learned from ids? At the example of 5320\nsales_train[sales_train.item_id.isin(range(5310,5330))].groupby(\"item_id\").max()\n# Not really.\n# One more example 3405-3408\nsales_train[sales_train.item_id.isin(range(3400,3415))].groupby(\"item_id\").max()\n# Here the category seems to be Igry PC. But prices and counts vary very much\n\n# Idea could be:\n# - if category before and after the ID is the same use the average of this category\n# - if they do not match take some average (eg of both categories or of all categories)","b5034b84":"# Now for the next idea: items that were sold in september but not october\nseptember = set(sales_train[sales_train.date_block_num==32].item_id)\noctober = set(sales_train[sales_train.date_block_num==33].item_id)\nnovember = set(test.item_id)\nsep_but_not_oct=[x for x in november if x not in october and x not in new_in_test_items2]\nsep_but_not_oct\nprint(len(september))\nprint(len(october))\nprint(len(november))\nprint(len(sep_but_not_oct))\n# 746 items were we could use the september figures","f67c6fda":"interim= sales_train[sales_train[\"date_block_num\"]==33].groupby([\"shop_id\", \"item_id\"],as_index=False).sum()[[\"shop_id\",\"item_id\",\"item_cnt_day\"]]\ninterim[\"item_cnt_day\"].clip(0,20,inplace=True)\n\ninterim2=sales_train[sales_train[\"date_block_num\"]==32].groupby([\"shop_id\", \"item_id\"],as_index=False).sum()[[\"shop_id\",\"item_id\",\"item_cnt_day\"]]\ninterim2=interim2[interim2.item_id.isin(sep_but_not_oct)]\ninterim2[\"item_cnt_day\"].clip(0,20,inplace=True)\n\ninterim3=pd.merge(test, interim, how=\"left\", left_on=[\"shop_id\",\"item_id\"], right_on = [\"shop_id\",\"item_id\"])\ninterim3=pd.merge(interim3, interim2, how=\"left\", left_on=[\"shop_id\",\"item_id\"], right_on = [\"shop_id\",\"item_id\"])\n\ninterim3.fillna(0,inplace=True)\n\ninterim3[\"item_cnt_month\"]=interim3[[\"item_cnt_day_x\",\"item_cnt_day_y\"]].max(axis=1) \n\nprint(interim3)\ninterim4=interim3[[\"ID\",\"item_cnt_month\"]]\nprint(interim4)\ninterim4.to_csv('submission4.csv',index=False)\n\n# Scored worse: 1.16602 - but I am not sure that the operations above did what I want them to do. \n# Have to check more closely in next working session","58eef090":"sales_train.groupby([\"item_id\",\"shop_id\",\"date_block_num\"],as_index=False).sum()[[\"item_id\",\"shop_id\",\"date_block_num\",\"item_cnt_day\"]]\n\n# Need to figure out how to only include the latest date_block_num per item per shop in table as lookup\n# value for test","feff5a04":"abc=sales_train.groupby([\"item_id\",\"shop_id\",\"date_block_num\"],as_index=False).sum()[[\"item_id\",\"shop_id\",\"date_block_num\",\"item_cnt_day\"]].head(10)\nabc","cbadfb4d":"# Now find out how to get only the rows where item_id and shop_id are the same and date_block_num is max\n# I.e. I want to have a table with the most recent item_cnt of a specific item per shop as lookup table to fill this\n# most recent item_cnt into the test.\nabc.groupby([\"item_id\",\"shop_id\"]).last()\n#YES!","b9ec1009":"interim5=sales_train.groupby([\"item_id\",\"shop_id\",\"date_block_num\"],as_index=False).sum()[[\"item_id\",\"shop_id\",\"date_block_num\",\"item_cnt_day\"]].groupby([\"item_id\",\"shop_id\"],as_index=False).last()\ninterim5=interim5[[\"item_id\",\"shop_id\",\"item_cnt_day\"]]\ninterim5","8b5db2ab":"interim5[\"item_cnt_day\"].clip(0,20,inplace=True)\ninterim6=pd.merge(test, interim5, how=\"left\", left_on=[\"shop_id\",\"item_id\"], right_on = [\"shop_id\",\"item_id\"])\ninterim6=interim6[[\"ID\",\"item_cnt_day\"]]\ninterim6.columns=[\"ID\",\"item_cnt_month\"]\ninterim6.fillna(0,inplace=True)\ninterim6.to_csv('submission5.csv',index=False)\ninterim6\n# scored 1.38739, better than september and october together but worse than october alone. Probably some items way to old\n# lets restrict age","04ada3c8":"interim=sales_train.groupby([\"item_id\",\"shop_id\",\"date_block_num\"],as_index=False).sum()[[\"item_id\",\"shop_id\",\"date_block_num\",\"item_cnt_day\"]].groupby([\"item_id\",\"shop_id\"],as_index=False).last()\ninterim=interim[interim[\"date_block_num\"]<25]\ninterim=interim[[\"item_id\",\"shop_id\",\"item_cnt_day\"]]\ninterim[\"item_cnt_day\"].clip(0,20,inplace=True)\ninterim7=pd.merge(test, interim, how=\"left\", left_on=[\"shop_id\",\"item_id\"], right_on = [\"shop_id\",\"item_id\"])\ninterim7=interim7[[\"ID\",\"item_cnt_day\"]]\ninterim7.columns=[\"ID\",\"item_cnt_month\"]\ninterim7.fillna(0,inplace=True)\ninterim7.to_csv('submission6.csv',index=False)\ninterim7\n# Only slightly better: 1.32017\n# So this is a dead end. Lets start with the modelling","22c438ef":"sales_train.to_csv('sales_train.csv',index=False)","d522712e":"Exporting the data to upload in this second kernel:","0651d65d":"Ok in week 2 of the course it was said that score should be 1,16777\n\n\"A good exercise is to reproduce previous_value_benchmark. As the name suggest - in this benchmark for the each shop\/item pair our predictions are just monthly sales from the previous month, i.e. October 2015.\n\nThe most important step at reproducing this score is correctly aggregating daily data and constructing monthly sales data frame. You need to get lagged values, fill NaNs with zeros and clip the values into [0,20] range. If you do it correctly, you'll get precisely 1.16777 on the public leaderboard.\n\nGenerating features like this is a necessary basis for more complex models. Also, if you decide to fit some model, don't forget to clip the target into [0,20] range, it makes a big difference.\"","594abdf4":"Observations:\n    - So this is what I have to deliver in the end. An item_cnt_month per ID from test-set. I.e. how many items (item_id) are sold in a specific shop (shop_id) in this month? (November 2015) (Added seasonalization to list of open to dos)","3ca0634f":"From data desciption in competition:\n\"the test set. You need to forecast the sales for these shops and products for November 2015\"\n\nObservations:\n    - 214,200 items in test\n    - So I need to make predictions for 60 shops for certain items sold per month\n    - There are not all item_ids in each shop: 60 shops * 22,167 items would make 1,330,000 predictions","d51ca0e4":"\n- A lot around revenues\n    - differences per shop: how many items, how many categories, how much revenue\n    - differences of items: how many sold, how different price\n    - price changes and price spread\n    - check especially the online channel\/shop: strongly increasing sales I assume\n- categories: How are they distributes? How many items per category? prices per category?\n- seasonalization of sales? Difference per shop? per price? per category? per product?\n- how much does price change? Over time? between shops?\n- how do weekdays differ in sales? How do months differ? Check for number of weekends specifically to see how in November 2015 this compares\n- some items will surely be out of catalogue in Nov 2015 and others will have increasing sales (old models vs. new models of e.g Playstations)\n- should closed shops be considered? (shops ids: )\n- should shops out of test be considered in training?\n- should products that are not sold anymore be considered in training?\n- Should I include negative revenues in the training? Or should I better cancel the sales out and only model\/train net sales? I guess returns are rather random so would be good to cancel out\n- looking at the revenue spikes per shops definetely groups of prices needed (mass product \/ b2b contracts \/ ...)\n- correlation between targeted month and product categories or price\/categories will be interesting\n\n* Check seasonalization in data. Forecast is for November. Maybe seasonalization works different per shop? Or per item? + what is the overall trajectory (probably over time linearly increasing sales but, around this, seasonalization e.g. lower in november (but compared to ever increasing base) due to upcoming christmas business.\n* What do I loose when I aggregate train data to months? Is there a daily \/ weekly pattern that is relevant to our monthly sales predictions? Maybe number of weekends in November? # of working days!\n\n* Categorize price into categories. Maybe some shops have larger presence in B2B deals\n* Does test price distribution look similar? Very small number of very large prices?\n\n- Is it true that the train data does not show all data? That random rows (30% or so) are missing? Maybe it is an idea to construct these rows and fill these NaN values with something meaningul? A moving average or something? These values for train could be learned through a model to then feed a complete dataset into the final algorithm... (easily written - already scared thinking of implementing it :-) )\n\n-----------------------------------------------------------------------------------\nFor model building part\n- setup a test and validation set from the train_sales set\n- shuffle rows in training as they are ordered\n- Validate with a same month in the year (to ensure same bank holidays etc.)\n- we need to check item distribution in test and match with validation set\n- the problem is a time series one. I need to forecast the next month of specific shops and have the past data for these shops\n    - LSTM problem, isnt it?\n    - or linear model\n    - NN? (how would this work? What would it be \/ model?\n- Try one model where rows \/ items with high prices\/revenues are deleted \/ capped\n------------------------------------------------------------------------------------\nDone:\n* split date\n* put weekdays into data\n* put test and train together (if same columns) (doesnt apply here)\n* plot revenue over time per shop\n* make first steps of analysis (NaNs)\n* analyze for ordinal, categorical values\n* Take meta-categories as features - problem with solution now is that if there is no meta-category it takes same value as category\n","a6477349":"Further notes:\n- new_in_test_items: list of items that are new in the test-set but havent been in train\n- shop_life: Dataframe with columns: shop_id, opening month, closing month","ff5d7bb2":"Observations:\n- So there are 22.170 different items  in the catalogue\n- Each one has a unique ID\n- There are 84 (0 is included as can be seen below) different categories and each product is categorized. No NaN's.","ff788054":"* Ok. I have now a bit of understanding of the data.\n\nLet's start with step 3\n\n3. Prepare Data\na) Data Cleaning\nb) Feature Selection\nc) Data Transforms (Normalize,...)","367c777f":"Now we get to Phase 2:\n2. Summarize Data\na) Descriptive statistics\n\n= EDA = Exploratory data analysis (= week 2 of coursera course)\n\nPrimarily interest here of course the train_dataset","dc51fe07":"Let's get started!\n\nI) I got used to work through such kernels and data science flows by using the \"Feynman\" methodology (https:\/\/mattyford.com\/blog\/2014\/1\/23\/the-feynman-technique-model). \n![](https:\/\/cdn.britannica.com\/84\/19184-004-AE04C440.jpg)\nHe famously said that you only really understand a concept if you are able to explain it to a 5-year-old. Therefore I go though my work and comment \/ annotate every step and learning (and especially mistake) I do along the way. That forces me to really understand the topic and to look at my work later and again understand what worked how. Additionally I imagine it could be interesting for fellow learners.\n\nII)  From earlier work on Kaggle I saved the following workflow and will use it for structuring the project:\n\n 1. Prepare Problem\na) Load libraries\nb) Load dataset\n\n2. Summarize Data\na) Descriptive statistics\nb) Data visualizations\n\n3. Prepare Data\na) Data Cleaning\nb) Feature Selection\nc) Data Transforms (Normalize,...)\n\nThese steps go into a second kernel [here](https:\/\/www.kaggle.com\/dennise\/coursera-competition-modelling\/edit)\n\n4. Evaluate Algorithms\na) Split-out validation dataset\nb) Test options and evaluation metric\nc) Spot Check Algorithms\nd) Compare Algorithms\n\n5. Improve Accuracy\na) Algorithm Tuning\nb) Ensembles\n\n6. Finalize Model\na) Predictions on validation dataset\nb) Create standalone model on entire training dataset\nc) Save model for later use","83741f50":"Observations:\n- There are 84 different categories (=IDs) (starting at 0 with \"PC\")\n- Each category has a name","2deb269f":"The Kernel seemd to have reached some capacity bottleneck. I'll continue the modelling [here](https:\/\/www.kaggle.com\/dennise\/coursera-competition-modelling?scriptVersionId=7573203):\n","80efcaa6":"It took me a while to figure out how to get such an output file from one kernel into another kernel. Here is how it goes:\n- Go to your new Kernel\n- Edit it\n- Check on the right-hand side: There is an \"Add Data\" button where you can link to your previous work\n\n![](https:\/\/i.imgur.com\/ClP9kLb.png)\n\nAnd pay attention. In this second Kernel there are now subfolders in the \"input\" folder:\n![](https:\/\/i.imgur.com\/rMZ3Kb8.png)","9278e9ec":"Observations:\n- nearly 3 million items in train-set\n- the transaction history of 60 shops over 33 months\n- per shop, per item, total sales\/shop\/item\/day\n- revenue\/item\/shop\/day = item_price*item_cnt_day (as learned in the coursera hosted pandas introduction session)\n- looks like there are no NaNs in the data","7cb616b4":"Observations:\n- There are 60 shops (again starting at 0) = IDs\n-  Each shop has a name\n    - maybe the names refer to places and that could be meaningul for analysis? (Added to open questions\/ideas)"}}