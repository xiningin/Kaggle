{"cell_type":{"0bad1587":"code","c4bf252b":"code","ef367b94":"code","580dad59":"code","f3292554":"code","554ec33e":"code","3ebd5f75":"code","e738fbdd":"code","c815200f":"code","7a05b5ec":"code","3a911099":"code","b680b25c":"code","4f8e2d78":"code","33e48518":"code","df96d582":"code","d2eea0aa":"code","e4722905":"code","ce9231b0":"code","506a308b":"code","bee27a72":"code","2746e251":"code","de6a168d":"code","4f5596d5":"code","bfcc1fde":"code","4cfb9822":"code","95bf623c":"code","f0a276f4":"code","3bd55c08":"code","2c881e52":"code","3589e4ec":"code","c736d337":"code","81bbfcf3":"code","10ca49a4":"code","f45fd35a":"code","75d66233":"code","971b4d0d":"code","f4e9d80b":"code","1250f3d8":"code","1444775c":"code","8a125918":"code","ba2144fc":"code","9fc17e9d":"code","783df76c":"code","96e0bd1e":"code","fd5c7297":"code","89847493":"code","2c3ca336":"code","bed40397":"code","4a3dbda3":"markdown","241a7e1f":"markdown","e43164bd":"markdown","07503299":"markdown","6a734889":"markdown","017db71c":"markdown","b698f54a":"markdown","dc0748a3":"markdown","17a9a270":"markdown","39b2c3b6":"markdown","56f85b3d":"markdown","3b97d9e9":"markdown","be4ab842":"markdown","7d518aeb":"markdown"},"source":{"0bad1587":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport re\nimport string","c4bf252b":"import tensorflow as tf \nimport tensorflow_datasets as tfds\n\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras import losses\nfrom keras import utils\n#from keras.layers.experimental.preprocessing import TextVectorization\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import layers\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Dropout\nfrom tensorflow.keras.models import load_model","ef367b94":"import nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer","580dad59":"from sklearn.datasets import fetch_20newsgroups\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, plot_confusion_matrix, confusion_matrix, f1_score, ConfusionMatrixDisplay","f3292554":"# load data and remove very promiment features (remove headers, signature blocks, and quotation blocks) \n# see also https:\/\/scikit-learn.org\/0.19\/datasets\/twenty_newsgroups.html\ndataset_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\ndataset_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)","554ec33e":"dataset_train.target_names","3ebd5f75":"def reallocate_classes(df):\n        # replace to politics\n    df['label'].replace({'talk.politics.misc':'politics','talk.politics.guns':'politics',\n                         'talk.politics.mideast':'politics'},inplace=True)\n\n    # replace to sport\n    df['label'].replace({'rec.sport.hockey':'sport','rec.sport.baseball':'sport'},inplace=True)\n\n    # replace to religion\n    df['label'].replace({'soc.religion.christian':'religion','talk.religion.misc':'religion'},inplace=True)\n\n    # replace to computer\n    df['label'].replace({'comp.windows.x':'computer','comp.sys.ibm.pc.hardware':'computer',\n                        'comp.os.ms-windows.misc':'computer','comp.graphics':'computer',\n                        'comp.sys.mac.hardware':'computer'},inplace=True)  \n    # replace to sales\n    df['label'].replace({'misc.forsale':'sales'},inplace=True)\n\n    # replace to automobile\n    df['label'].replace({'rec.autos':'automobile','rec.motorcycles':'automobile'},inplace=True)\n\n    # replace to science\n    df['label'].replace({'sci.crypt':'science','sci.electronics':'science','sci.space':'science'},inplace=True)\n\n    # replace to medicine\n    df['label'].replace({'sci.med':'medicine'},inplace=True)\n    return df","e738fbdd":"def clean_text(text):\n    # lowercase the each word\n    text = text.lower()\n    # remove text in square brackets, links and punctuation \n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","c815200f":"# remove stopwords\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words ","7a05b5ec":"def lem_word(x):\n    lem = WordNetLemmatizer()\n    return [lem.lemmatize(w) for w in x]","3a911099":"# create clean text \ndef combine_text(list_of_text):\n    return ' '.join(list_of_text)","b680b25c":"def data_preprocessing(dataset, reallocate=False):\n    df_ = pd.DataFrame()\n    df_['text'] = dataset['data']\n    df_['source'] = dataset['target'] \n    \n    label = [dataset['target_names'][i] for i in df_['source']]\n    df_['label'] = label\n    df_.drop(['source'], axis=1, inplace=True)\n    \n    # combine classes to bigger cluster of classes (e.g. politics, sports, etc.)\n    if reallocate:\n        print(\"Reallocating classes!\")\n        df_ = reallocate_classes(df_)\n    \n    # delete all rows with no entries \n    df_['number_of_words'] = df_['text'].apply(lambda x:len(str(x).split()))\n    df_.drop(df_[df_['number_of_words']==0].index, inplace=True)\n    \n    # clean text from noise \n    df_['clean_text'] = df_['text'].apply(lambda x: clean_text(x))\n    df_['clean_text'].head()\n    \n    # tokenize text\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    df_['tokens'] = df_['clean_text'].apply(lambda x:tokenizer.tokenize(x))\n    \n    # remove stop words \n    df_['clean_tokens'] = df_['tokens'].apply(lambda x: remove_stopwords(x))\n    # lemmatizaion \n    df_['lemmatized_tokens'] = df_['clean_tokens'].apply(lambda x: lem_word(x))\n    # combine all texts to one document\/corpus \n    df_['clean_text'] = df_['lemmatized_tokens'].apply(lambda x: combine_text(x))\n    \n    df_.drop(df_[df_['clean_text']==0].index, inplace=True)\n    \n    # convert labels into numeric values \n    label_encoder = preprocessing.LabelEncoder()\n    df_['target'] = label_encoder.fit_transform(df_['label'])\n        \n    return df_","4f8e2d78":"df_train = data_preprocessing(dataset_train, reallocate=True)\ndf_test = data_preprocessing(dataset_test, reallocate=True)","33e48518":"X_sequence = df_train['clean_text']\ny = df_train['label']\nX_test_sequence = df_test['clean_text'] \ny_test = df_test['label']\n\nlabel_names = df_train['label'].unique()\nnum_classes = len(label_names)\nprint(f'We have {num_classes} classes in our dataset.')","df96d582":"# get tfidf representation\ntfidf_vectorizer = TfidfVectorizer(min_df = 2, max_df = 0.5)\nX_train_lg = tfidf_vectorizer.fit_transform(X_sequence)\nX_test_lg = tfidf_vectorizer.transform(X_test_sequence)\nY_train_lg = np.array(df_train.target)\nY_test_lg = np.array(df_test.target)\nprint(f'Number of documents: {len(X_sequence)}')\nprint(f'Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}')\nprint(f'Shape of td-idf matrix: {X_train_lg.toarray().shape}')","d2eea0aa":"x = X_train_lg.toarray()\nprint(f'Document 10: Number of words with non-zero entry {len(np.where(x[9] != 0)[0])}')\nprint(f'Document 100: Number of words with non-zero entry {len(np.where(x[99] != 0)[0])}')\nprint(f'Document 1000: Number of words with non-zero entry {len(np.where(x[999] != 0)[0])}')\nprint(f'Document 10000: Number of words with non-zero entry {len(np.where(x[9999] != 0)[0])}')","e4722905":"# plot class distribution \nfig, axes = plt.subplots(1, 2, figsize=(20, 5))\nsns.countplot(ax=axes[0], x=Y_train_lg)\naxes[0].set_title('Class distribution train dataset')\nsns.countplot(ax=axes[1], x=Y_test_lg)\naxes[1].set_title('Class distribution test dataset')\nplt.show()","ce9231b0":"# train the logistic regression model \nlg = LogisticRegression(C = 1.0, solver='liblinear')\nlg.fit(X_train_lg, Y_train_lg)\n\ntrain_pred_lg = lg.predict(X_train_lg)\nlg_train_accuracy =  accuracy_score(Y_train_lg, train_pred_lg) * 100\nlg_train_f1 =  f1_score(Y_train_lg, train_pred_lg, average='weighted') * 100\ntest_pred_lg = lg.predict(X_test_lg)\nlg_test_accuracy =  accuracy_score(Y_test_lg, test_pred_lg) * 100\nlg_test_f1 =  f1_score(Y_test_lg, test_pred_lg, average='weighted') * 100\nprint(f\"Train accuracy score: {lg_train_accuracy:.2f}%\")\nprint(f\"Train F1 score: {lg_train_f1:.2f}%\")\nprint(f\"Test accuracy score: {lg_test_accuracy:.2f}%\")\nprint(f\"Test F1 score: {lg_test_f1:.2f}%\")","506a308b":"cm = confusion_matrix(df_test.target, test_pred_lg)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=label_names)\n\nfig, ax = plt.subplots(figsize=(15,10))\ndisp = disp.plot(xticks_rotation='vertical', ax=ax, cmap='summer')\nplt.show()","bee27a72":"idx = 120\nprint('--> Sample test text:')\nprint(X_sequence[idx], \"\\n\")\nprint(f'--> Predicted class: {label_names[test_pred_lg[idx]]}')\nprint(f'--> True class: {label_names[Y_test_lg[idx]]}')","2746e251":"import tensorflow as tf \nimport tensorflow_datasets as tfds\n\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras import losses\nfrom keras import utils\n#from keras.layers.experimental.preprocessing import TextVectorization\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import layers\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Dropout","de6a168d":"max_text = 0\nfor i, text in enumerate(X_sequence):\n    if len(text.split()) > max_text:\n        max_text = len(text.split())\nprint(f'Maximum number of words in a document: {max_text}') ","4f5596d5":"max_features = 6433 # maximum number of words to keep based on word frequency \ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_sequence.values)\nword_index = tokenizer.word_index\nprint(f'Size of the vocabulary: {len(word_index)}')","bfcc1fde":"# train validation split \nY = keras.utils.to_categorical(df_train.target, num_classes)\nX_train, X_val, Y_train, Y_val = train_test_split(X_sequence, Y, test_size=0.1, random_state = 42, stratify = y)","4cfb9822":"X_train = tokenizer.texts_to_sequences(X_train.values)\nX_train = pad_sequences(X_train, padding='post', maxlen=max_features)\n\nX_val = tokenizer.texts_to_sequences(X_val.values)\nX_val = pad_sequences(X_val, padding='post', maxlen=max_features)\n\nX_test = tokenizer.texts_to_sequences(X_test_sequence.values)\nX_test = pad_sequences(X_test, padding='post', maxlen=max_features)\n#Y_test = pd.get_dummies(y_test).values\nY_test = keras.utils.to_categorical(df_test.target, num_classes)","95bf623c":"print(X_train.shape,Y_train.shape)\nprint(X_val.shape,Y_val.shape)\nprint(X_test.shape,Y_test.shape)","f0a276f4":"# check class distribution in train, val and test set \nfig, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n_, idx_val = np.where(Y_val == 1)\ndf_val = pd.DataFrame({'target': idx_val})\n_, idx_train = np.where(Y_train == 1)\ndf_train = pd.DataFrame({'target': idx_train})\n\nsns.countplot(ax=axes[0], data=df_train, x=\"target\")\naxes[0].set_title('Class distribution train dataset')\nsns.countplot(ax=axes[1], data=df_val, x=\"target\")\naxes[1].set_title('Class distribution val dataset')\nsns.countplot(ax=axes[2], data=df_test, x='target')\naxes[2].set_title('Class distribution test dataset')\nplt.show()","3bd55c08":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip -q glove.6B.zip","2c881e52":"embedding_dim = 300\npath_to_glove_file = (f'\/kaggle\/working\/glove.6B.{embedding_dim}d.txt')\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))","3589e4ec":"vocab_size = len(word_index) + 1\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector","c736d337":"embedding_layer_pretrained = Embedding(vocab_size,\n                            embedding_dim,\n                            weights=[embedding_matrix],\n                            input_length=max_features,\n                            trainable=False)\n\nembedding_layer_scratch = Embedding(vocab_size,\n                                    embedding_dim,\n                                    input_length=max_features,\n                                    trainable=True)","81bbfcf3":"def plot_loss(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(len(acc))\n\n    fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n    axes[0].plot(epochs, acc, 'r', label='Training accuracy')\n    axes[0].plot(epochs, val_acc, 'b', label='Validation accuracy')\n    axes[0].set_title('Training and validation accuracy')\n    axes[0].legend()\n\n    axes[1].plot(epochs, loss, 'r', label='Training Loss')\n    axes[1].plot(epochs, val_loss, 'b', label='Validation Loss')\n    axes[1].set_title('Training and validation loss')\n    axes[1].legend()\n\n    plt.show()","10ca49a4":"def cnn_model(emb_layer=embedding_layer_scratch):\n    model = keras.Sequential()\n    model.add(emb_layer)\n    model.add(layers.Conv1D(filters=50, kernel_size=3, activation=\"relu\"))\n    model.add(layers.MaxPooling1D(5))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(units=64, activation=\"relu\"))\n    model.add(layers.Dropout(0.1))\n    model.add(layers.Dense(num_classes, activation = 'softmax'))\n    \n    return model ","f45fd35a":"model = cnn_model(emb_layer=embedding_layer_pretrained)\nmodel.summary()","75d66233":"batch_size = 32\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n\noptimizer = keras.optimizers.Adam(lr=0.0003)\nmodel.compile(loss = 'categorical_crossentropy', optimizer=optimizer, metrics = ['accuracy'])\nhistory = model.fit(\n    X_train, Y_train, epochs = 10, batch_size=batch_size, verbose = 1, \n    validation_data= (X_val, Y_val),callbacks=[earlystop]\n)","971b4d0d":"plot_loss(history)","f4e9d80b":"# final evluation on the test set \nresults = model.evaluate(X_test, Y_test, batch_size=batch_size)\nprint(results)","1250f3d8":"# compute predictions for all test sequences \nprediction = model.predict(X_test).argmax(axis=1)","1444775c":"# let's check some of the test sequences and our predictions \nidx = 1430\nprint(f'Test sequence:\\n{df_test.clean_text[idx]}\\n')\nprint(f'--> Predicted class: {label_names[prediction[idx]]}')\nprint(f'--> True class: {y_test[idx]}')","8a125918":"cm = confusion_matrix(df_test.target, prediction)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=label_names)\n\nfig, ax = plt.subplots(figsize=(15,10))\ndisp = disp.plot(xticks_rotation='vertical', ax=ax, cmap='summer')\nplt.show()","ba2144fc":"def lstm_model(emb_layer=embedding_layer_scratch):\n    model = keras.Sequential()\n    model.add(emb_layer)\n    model.add(Bidirectional(LSTM(32)))\n    model.add(Dropout(0.2))\n    model.add(Dense(32, activation = 'relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(num_classes, activation = 'softmax'))\n\n    return model    ","9fc17e9d":"model_lstm = lstm_model(emb_layer=embedding_layer_pretrained)\nmodel_lstm.summary()","783df76c":"batch_size = 64\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n\noptimizer = keras.optimizers.Adam(lr=0.0003)\nmodel_lstm.compile(loss = 'categorical_crossentropy', optimizer=optimizer, metrics = ['accuracy'])\nhistory_lstm = model_lstm.fit(\n    X_train, Y_train, epochs = 10, batch_size=batch_size, verbose = 1, \n    validation_data= (X_val, Y_val),callbacks=[earlystop]\n)","96e0bd1e":"plot_loss(history_lstm)","fd5c7297":"# final evluation on the test set \nresults_lstm = model_lstm.evaluate(X_test, Y_test, batch_size=batch_size)\nprint(results_lstm)","89847493":"# compute predictions for all test sequences \nprediction_lstm = model_lstm.predict(X_test).argmax(axis=1)","2c3ca336":"# let's check some of the test sequences and our predictions \nidx = 902\nprint(f'Test sequence:\\n{df_test.clean_text[idx]}\\n')\nprint(f'--> Predicted class: {label_names[prediction_lstm[idx]]}')\nprint(f'--> True class: {y_test[idx]}')","bed40397":"cm = confusion_matrix(df_test.target, prediction_lstm)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=label_names)\n\nfig, ax = plt.subplots(figsize=(15,10))\ndisp = disp.plot(xticks_rotation='vertical', ax=ax, cmap='summer')\nplt.show()","4a3dbda3":"With our plain logistic regression model we achieve a test accuracy of **66.64%** and a F1 score of **65.89%**.","241a7e1f":"In this section we will utilize deep learning to predict the right class of news articles. The main difference to the previous approach is that we use word embeddings that are not based on frequency of words but are learned during the training. In order to do that we predefine the dimension of the embeddings (= vector representation of our words) where each entry of one vector corresponds to an abstract feature of this word. Thus, we are able to learn much more generalizable representations of the words giving us the possibility to reuse them in other models as well. \n\nThe following steps are performed to prepare our data:\n1. Generate a vocabulary index \n2. Transform each text (train and test) into a sequence of integers \n3. Apply padding on the sequence of integers so each input sequence will have the same length \n4. Transform the labels into a one-hot encoded matrix ","e43164bd":"We can see that the data distribution of classes in our train and test set looks very similar. The class distribution itself is pretty balanced, only classes 0, 18 and 19 have a little bit less samples than the rest. So for now, we don't have to make any changes here and can start the training. ","07503299":"# Define our deep learning models \n\nWe first define our embedding layer:\n* pre-trained: Here we use the pre-trained embeddings that we just loaded. We set the parameter \"trainable\" to false since we won't change our embedding weights during training. This will significantly reduce the number of parameters that the model has to learn. \n* scratch: Here we learn our own embeddings from scratch during training. This results in a much bigger number of trainable parameters.","6a734889":"# Tokenization","017db71c":"### LSTM model ","b698f54a":"We start with a simple logitstic regression model to get a feeling for the data. \n\nIn NLP we need a method to vectorize our words so we can feed them as numerical values into our model. Such a method is **TF-IDF** (Term frequency inverse-document frequency).\\\nTF-IDF assigns each word in the vocabulary of the corpus a weight based on its relevance to the text. If you want learn a little bit more in detail about TF-IDF, I recommend this [article](https:\/\/www.geeksforgeeks.org\/understanding-tf-idf-term-frequency-inverse-document-frequency\/).","dc0748a3":"# Logistic regression ","17a9a270":"In the confusion matrix above, we can see the number of predictions for each class (the ones on the diagonal are correct predictions and everything else are wrong predictions). \nWe can see that class \"sci.crypt\" is performing the worst compared with all other classes. This can be explained by its small number of samples in the training set compared with the rest of the classes. ","39b2c3b6":"# Loading pretrained embeddings \n\nHere we load pre-trained glove embeddings from Stanford and map them to our word indices from the tokenization step. We are going to use an embedding dimension of 300. In other words, each token in our vocabulary will be represented by a vector of dimension 300. ","56f85b3d":"We can see that in each document each unique word has its own weight encoded. \\\nHowever the matrix is very sparse as we can see in the following output:","3b97d9e9":"# Loading the data","be4ab842":"# Preprocessing the data ","7d518aeb":"### CNN Model"}}