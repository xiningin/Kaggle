{"cell_type":{"6ec89218":"code","9a4eb28b":"code","f0e77228":"code","596783a4":"code","0570a4f8":"code","d6480130":"code","57c6d05b":"code","e422ab02":"code","157b7af9":"code","77f5fdc4":"code","5eda5276":"code","985ebc21":"code","0231c41f":"code","68201d9a":"code","e7f1270a":"code","0d7d5e3c":"code","df0260ae":"code","6821f228":"code","645ad9fa":"code","95f70276":"code","0f5cec66":"code","54524664":"code","9b309951":"code","f5d962a5":"code","8fe36769":"code","c254043c":"code","d776a7ea":"code","26a10a9d":"code","937e5c96":"code","e8e9d3f8":"code","fa87e1e1":"code","d4806516":"code","071e55f8":"code","ee50bcd0":"code","d28c4ab9":"code","21405384":"code","3ec7f006":"code","cedc6324":"code","168fbb2d":"code","32e0de6b":"code","20078c4a":"markdown","b157df7b":"markdown","476a4595":"markdown","2e4f58fa":"markdown","c6a720ea":"markdown","99f07c89":"markdown","6b0c03f0":"markdown"},"source":{"6ec89218":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# plt.style.use('g')\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a4eb28b":"train_df = pd.read_csv(\"..\/input\/janatahack-independence-day-2020-ml-hackathon\/train.csv\")\ntrain_df.head()","f0e77228":"train_df.shape","596783a4":"test_df = pd.read_csv(\"..\/input\/janatahack-independence-day-2020-ml-hackathon\/test.csv\")\ntest_df.head()","0570a4f8":"test_df.shape","d6480130":"sample_df = pd.read_csv(\"..\/input\/janatahack-independence-day-2020-ml-hackathon\/sample_submission_UVKGLZE.csv\")\nsample_df.head()","57c6d05b":"test_id = test_df.ID.values\ntest_id","e422ab02":"plt.style.use('ggplot')","157b7af9":"categories = list(train_df.columns.values)[3:]\nvalues = list(train_df.iloc[:,3:].sum(axis =0).values)\n\nsns.set(font_scale =1)\nfig = plt.figure(figsize =(10,8))\n\nax = sns.barplot(categories,values)\nax.set_xlabel(\"categories\")\nax.set_ylabel(\"number of topics and abstracts\")\nax.set_title(\"total topics in each category\")\nplt.xticks(rotation=90)\n\n\nrects = ax.patches\nlabels = values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n    \nplt.show()","77f5fdc4":"from wordcloud import WordCloud, STOPWORDS","5eda5276":"subset = train_df[train_df['Computer Science'] == True]\nsubset = subset.TITLE.values","985ebc21":"stopwords = set(STOPWORDS)\ncse_wc = WordCloud(\n            background_color='black',\n            max_words=3000,\n            stopwords= stopwords,\n            width =600,height = 400).generate(\" \".join(subset))\n\nfig  = plt.figure(figsize=(14,8))\nplt.imshow(cse_wc)\nplt.axis('off')\nplt.show()","0231c41f":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport re\nimport warnings\nimport sys\n\ndata_train = train_df.copy()\ndata_test = test_df.copy()\n\n\ndata_train['TITLE'] = data_train[['TITLE','ABSTRACT']].apply(lambda x: \" \".join(x),axis =1)\ndata_test['TITLE'] = data_test[['TITLE','ABSTRACT']].apply(lambda x: \" \".join(x),axis =1)\n\ndel data_train['ABSTRACT']\ndel data_test['ABSTRACT']\n\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n        \n        ","68201d9a":"data_train.columns.values","e7f1270a":"def cleanPunctuation(sentence):\n    cl_sentence  = re.sub(r'[?|!|\\'|\"|#|\\_]',r'',sentence)\n    cl_sentence = re.sub(r'[.|,|)|(|\\|\/]',r'',cl_sentence)\n    cl_sentence = re.sub(r'\\\\',r'',cl_sentence)\n    cl_sentence = cl_sentence.strip()\n    cl_sentence = cl_sentence.replace(\"\\n\",\" \")\n    return cl_sentence\n\ndef keepAlphabet(sentence):\n    alpha_sent = \"\"\n    for word in sentence.split():\n        alpha_word = re.sub('[^a-z A-z]+',' ',word)\n        alpha_sent +=alpha_word\n        alpha_sent+=\" \"\n    alpha_sent =alpha_sent.strip()\n    \n    return alpha_sent","0d7d5e3c":"data_train['TITLE'] = data_train['TITLE'].str.lower()\ndata_train['TITLE'] = data_train['TITLE'].str.lower()\n\ndata_train['TITLE'] = data_train['TITLE'].apply(cleanPunctuation)\ndata_test['TITLE'] = data_test['TITLE'].apply(cleanPunctuation)\n\ndata_train['TITLE'] = data_train['TITLE'].apply(keepAlphabet)\ndata_test['TITLE'] = data_test['TITLE'].apply(keepAlphabet)\n\nprint(\"Train data \\n\",data_train['TITLE'].values[0],\"\\n\")\n\nprint(\"Test data \\n\",data_test['TITLE'].values[0])\n\n","df0260ae":"stopwords = set(stopwords.words('english'))\n# print(stopwords)\nre_stop_words = re.compile(r\"\\b(\"+\"|\".join(stopwords) + \")\\\\W\", re.I)\ndef removeStopWords(sentence):\n    global re_stop_words\n    return re_stop_words.sub(\" \", sentence)\n\ndata_train['TITLE'] = data_train['TITLE'].apply(removeStopWords)\ndata_test['TITLE'] = data_test['TITLE'].apply(removeStopWords)\n\nprint(\"Train data \\n\",data_train['TITLE'].values[0])\n\nprint(\"Test data \\n\",data_test['TITLE'].values[0])\n\n# data_train['TITLE'].values[:10]","6821f228":"from nltk.stem import wordnet\nfrom nltk.stem import WordNetLemmatizer\nword_len  = WordNetLemmatizer()","645ad9fa":"data_new_train = data_train.copy()\ndata_new_test = data_test.copy()\n\ndef stemming(sentence):\n    stemSentence =[]\n    \n    for word in sentence.split():\n        stem  = word_len.lemmatize(word)\n        stemSentence.append(stem)\n        \n\n    return \" \".join(stemSentence)\n\ndata_new_train['TITLE'] = data_new_train['TITLE'].apply(stemming)\ndata_new_test['TITLE'] = data_new_test['TITLE'].apply(stemming)\n\nprint(\"Train data \\n\",data_train['TITLE'].values[0])\n\nprint(\"Test data \\n\",data_test['TITLE'].values[0])\n\n# data_new['TITLE'].head().values[:10]","95f70276":"data2_train = data_new_train.copy()","0f5cec66":"from sklearn.model_selection import train_test_split\ntrain,dev = train_test_split(data2_train,random_state =4,test_size = 0.20,shuffle = True)","54524664":"train.shape","9b309951":"from sklearn.feature_extraction.text import TfidfVectorizer ","f5d962a5":"col = data2_train.columns.values\ncol","8fe36769":"categories = col[2:]\nprint(categories)","c254043c":"train_text = train['TITLE']\ndev_text = dev['TITLE']\ntest_text = data_new_test['TITLE']","d776a7ea":"train_text","26a10a9d":"vectorizor = TfidfVectorizer(strip_accents='unicode',analyzer='word',ngram_range=(1,2),norm = 'l2')\nvectorizor.fit(train_text)\nvectorizor.fit(test_text)\nvectorizor.fit(dev_text)\n","937e5c96":"\nx_train= vectorizor.transform(train_text)\nx_dev= vectorizor.transform(dev_text)\nx_test = vectorizor.transform(test_text)\n\ny_train = train.drop(labels=['ID','TITLE'],axis = 1)\ny_dev = dev.drop(labels=['ID','TITLE'],axis = 1)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(x_dev.shape)\nprint(y_dev.shape)\n\n# x_train.toarray()\n# x_dev = x_dev.toarray()\n# x_test = x_test.toarray()","e8e9d3f8":"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score,accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import MultinomialNB","fa87e1e1":"# categories = [data]\ny_train.shape","d4806516":"logReg_pipeline = Pipeline([('oneVrest',OneVsRestClassifier(LogisticRegression(solver ='sag'),n_jobs = -1)),])\npred_arr = []\nfinal_pred = []\nfor cat in categories:\n    print(\".....processing.....{} category...\".format(cat))\n#     print(cat)\n    logReg_pipeline.fit(x_train,train[cat])\n    pred = logReg_pipeline.predict(x_dev)\n    f_pred = logReg_pipeline.predict(x_test)\n    pred_arr.append(pred)\n    final_pred.append(f_pred)\n    \n    print(\"f1 score = {}\".format(accuracy_score(dev[cat],pred)))\n    print()","071e55f8":"y_pred = np.array(pred_arr).T\ny_pred.shape\ny_dev.shape\nprint(\"f1 score for oneVRest classifier = \",f1_score(y_dev,y_pred,average ='micro'))","ee50bcd0":"lst = np.array(final_pred).T\nprint(lst)","d28c4ab9":"del train_df\ndel data_new_train\ndel data_train\ndel test_df\ndel data_test\n\n","21405384":"from skmultilearn.problem_transform import LabelPowerset\nclassifier = LabelPowerset(LogisticRegression(solver= 'sag',n_jobs = -1,class_weight ='balanced'))","3ec7f006":"# classifier.fit(x_train, y_train)\n# pred_lp = classifier.predict(x_dev)\n# lst = classifier.predict(x_test)\n# print(\"f1 score =\",f1_score(y_dev,pred_lp,average =\"micro\"))\n# lst =lst.toarray()","cedc6324":"ID = np.asanyarray(test_id)\ndf_submit = pd.DataFrame(lst,columns =categories)\ndf_submit['ID'] = ID\ndf_submit.head()","168fbb2d":"col = df_submit.columns.tolist()\ncol_n = col[-1:]\n\nfor ele in range(len(col)-1):\n    col_n.append(col[ele])\n    \ndf_submit = df_submit[col_n]\ndf_submit.head()","32e0de6b":"compression_opts = dict(method='zip',\n                        archive_name='submission4.csv')  \ndf_submit.to_csv('out13.zip', index=False,\n          compression=compression_opts)","20078c4a":"1. # Use of oneVRest classifier","b157df7b":"# Level Powerset","476a4595":"Uncomment the below code to run it","2e4f58fa":"# Stemming","c6a720ea":"## Exploratory data analysis","99f07c89":"### Let's visualize the word cloud for Computer Sciecne category","6b0c03f0":"# Data Preprocessing "}}