{"cell_type":{"cb00ebb4":"code","d103024b":"code","0c3ab787":"code","701d4087":"code","0877a8fb":"code","40d8b07b":"code","da9fa044":"code","dab861f2":"code","d81db8a7":"code","2d9a3188":"code","93a3b149":"code","d30c9f98":"code","ee5a68f8":"code","21f03fbc":"code","a0890412":"code","4580ae9a":"code","9513ce2b":"code","a35a4779":"code","06c58bd1":"code","1e454584":"code","12c29ba7":"code","612c7d66":"code","f55fb6ad":"code","dd6efa98":"code","7b0ee52c":"markdown","c1690b35":"markdown","3e75ebb5":"markdown"},"source":{"cb00ebb4":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.dummy import DummyClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom itertools import combinations\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d103024b":"df = pd.read_csv('..\/input\/loan-prediction\/train_loan.csv')","0c3ab787":"df.head()","701d4087":"#I see some 'object' type and need to deal with them\ndf.info()","0877a8fb":"df.describe()","40d8b07b":"#Loan_ID: unuseful fot the model, can be removed","da9fa044":"# gender: There are some NaN but there is no way to infer this, so unfortunately I remove the related rows\ndf = df[df['Gender'].notna()]\ndf.Gender.unique()","dab861f2":"#Married: I assume NaN for this feature means \"Divorced\"\n\nmissing_married = len(df[ df.Married.isna()])\nprint('Missing married before: %d' %(missing_married))\n\nnan_married = df[df.Married.isna()].index\ndf.loc[nan_married,'Married'] = 'Divorced'\n\nmissing_married = len(df[df.Married.isna()])\nprint('Missing married after : %d' %(missing_married))\ndf.Married.unique()","d81db8a7":"# I assume that when Self_Employed is set to nan it means the guy lives with a revenue\ndef fix_self_employed(self_employed):\n    ret = self_employed\n    if str(self_employed) == 'nan':\n        ret = 'Revenue'\n    #print(ret)\n    return ret\n\ndf['Self_Employed'] = df.apply(lambda row : fix_self_employed(row['Self_Employed']),axis=1)","2d9a3188":"# Dependent (number): I tried different setup here but nothing changed that much the results.\n# I keep this where I set category '1' for more than 3 dependents, 0 for less than '3', -1 when the value is not applicable\n\ndef fix_dependents(num_dependents):\n    \n    map = {'3+' : 1, '3': 0, '2': 0, '1': 0, '0': 0, 'nan': -1 }\n    return map[str(num_dependents)]\n           \ndf['Dependents'] = df.apply(lambda row : fix_dependents(row['Dependents']),axis=1)","93a3b149":"# Education: all values are there. I do not see any issue in this column\ndf['Education'].unique()","d30c9f98":"# ApplicantIncome: all values are there. I do not see any issue in this column\nlen(df[df['ApplicantIncome'].isna()]) == 0","ee5a68f8":"# CoapplicantIncome: when there is no coapplicant this value is set to zero\ncoapplicant_income_set_to_zero = len(df[df['CoapplicantIncome'] == 0])\ncoapplicant_income_set_to_nan = len(df[df['CoapplicantIncome'].isna()])\nprint(coapplicant_income_set_to_zero,coapplicant_income_set_to_nan)\n\n# create a new feature as the sum of ApplicantIncome and CoapplicantIncome\ndf['TotalApplicantIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']","21f03fbc":"# loanAmount : there are 22 rows without any value \n# (the dataset is already tiny so better replace them with mean rather than get rid of them)\n\nprint(len(df[df['LoanAmount'].isna()]))\n\nloan_amount_mean = df['LoanAmount'].mean()\n\ndef replace_NaN_loan_Amount(loan_amount, replace_value):\n    ret = loan_amount\n    if str(loan_amount) == 'nan':\n        #print(str(loan_amount))\n        ret = replace_value\n    return ret\n\ndf['LoanAmount'] = df['LoanAmount'].fillna(loan_amount_mean)\n\nprint(len(df[df['LoanAmount'].isna()]))","a0890412":"# Loan_Amount_Term : there are 14 rows without any value\n# (the dataset is already tiny so better replace them with mean rather than get rid of them)\n\nprint(len(df[df['Loan_Amount_Term'].isna()]))\ndf.Loan_Amount_Term.unique()\n\nloan_amount_term_mean = df['Loan_Amount_Term'].mean()\n\ndef replace_NaN_Loan_Amount_Term(loan_amount, replace_value):\n    ret = loan_amount\n    if str(loan_amount) == 'nan':\n        #print(str(loan_amount))\n        ret = replace_value\n    return ret\n\ndf['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna(loan_amount_term_mean)\n\nprint(len(df[df['Loan_Amount_Term'].isna()]))\n","4580ae9a":"#Credit_History : some people (50) don't have a credit history. I assign them a new category = 2. (unrated)\n    \nprint(len(df[df['Credit_History'].isna()]))\n\ndf['Credit_History'] = df['Credit_History'].fillna(2.)\n\nprint(len(df[df['Credit_History'].isna()]))\ndf['Credit_History'].unique()","9513ce2b":"#Property_Area: this column seems fine: all values set and no NaN\nlen(df[df.Property_Area.isna()])\ndf.Property_Area.unique()","a35a4779":"# Encode all values\nfor column in df.columns:\n    if df[column].dtype == 'O':\n        # object values are label-encoded\n        le = LabelEncoder()\n        df[column] = le.fit_transform(df[column].apply(str))\n    # numerical values are scaled\n    scaler = MinMaxScaler()\n    df[column] = scaler.fit_transform(np.array(df[column]).reshape(-1, 1))","06c58bd1":"# drop unuseful\/meaningless features\nfeatures_to_be_dropped = ['Loan_ID','ApplicantIncome', 'CoapplicantIncome']\ndf.drop(features_to_be_dropped,inplace=True, axis=1)","1e454584":"# split in train and test\nY = df['Loan_Status'].values\nX = df.drop(['Loan_Status'], axis=1).values\n\nX_train, X_test, Y_train, Y_test =  train_test_split(X,Y, test_size=0.3, random_state=1)","12c29ba7":"# try a Dummy Classifier to see what is the reference accuracy for my model\ndc = DummyClassifier()\ndc.fit(X_train,Y_train)\nY_train_pred = dc.predict(X_train)\nY_test_pred = dc.predict(X_test)\n\nprint('Dummy ACCURACY train: %.4f, test: %.4f' %(accuracy_score(Y_train,Y_train_pred), accuracy_score(Y_test,Y_test_pred)))","612c7d66":"# try with RandomForest\nrfc = RandomForestClassifier(n_estimators=30)\nrfc.fit(X_train,Y_train)\n\nY_train_pred = rfc.predict(X_train)\nY_test_pred = rfc.predict(X_test)\n\nprint('RandomForest ACCURACY train: %.4f, test: %.4f' %(accuracy_score(Y_train,Y_train_pred), accuracy_score(Y_test,Y_test_pred)))","f55fb6ad":"# try with XGBoost\nxg_reg = xgb.XGBClassifier(n_estimators=30)\n\nxg_reg.fit(X_train,Y_train)\n\nY_train_pred = xg_reg.predict(X_train)\nY_test_pred = xg_reg.predict(X_test)\n\nprint('XGBoost ACCURACY train: %.4f, test: %.4f' %(accuracy_score(Y_train,Y_train_pred), accuracy_score(Y_test,Y_test_pred)))","dd6efa98":"# We have a lot of feature and I wonder whether a differnt combination of them can lead to better result,\n#so here I try all the feature's combinations and see what happens\n\nfeature_list = []\nfor column in df.columns:\n    if column == 'Loan_Status' or column == 'Loan_ID':\n        continue\n    feature_list.append(column)\nprint(feature_list)\n\nfor num_features_step in range(1,len(feature_list)+1):\n\n    combs = combinations(feature_list, num_features_step) \n    features_for_model = []\n    \n    for comb in combs:\n        features_for_model = [feature for feature in comb]\n        print(features_for_model)\n        # drop unuseful\/meaningless features\n        Y = df['Loan_Status'].values\n        X = df[features_for_model].values\n        X_train, X_test, Y_train, Y_test =  train_test_split(X,Y, test_size=0.3, random_state=1)\n        rfc = RandomForestClassifier(n_estimators=30)\n        rfc.fit(X_train,Y_train)\n        Y_train_pred = rfc.predict(X_train)\n        Y_test_pred = rfc.predict(X_test)\n        print('---ITERATION---')\n        print(features_for_model)\n        print('RandomForestClassifier ACCURACY train: %.4f, test: %.4f' %(accuracy_score(Y_train,Y_train_pred), accuracy_score(Y_test,Y_test_pred)))\n        xg_reg = xgb.XGBClassifier(n_estimators=30)\n        xg_reg.fit(X_train,Y_train)\n        Y_train_pred = xg_reg.predict(X_train)\n        Y_test_pred = xg_reg.predict(X_test)\n        print('XGBClassifier          ACCURACY train: %.4f, test: %.4f' %(accuracy_score(Y_train,Y_train_pred), accuracy_score(Y_test,Y_test_pred)))","7b0ee52c":"On my PC, with the features set as described above, I see that a relatively high score is reached by using only the 'Credit History' feature\n\n**Features: ['Credit_History']**\n\n**RandomForestClassifier ACCURACY train: 0.8190, test: 0.7845**\n\n**XGBClassifier          ACCURACY train: 0.8190, test: 0.7845**\n\n\nWith the following two features only I already reach the max accuracy seen, with quite balanced values between train and test set:\n\n**Features: ['Self_Employed', 'Credit_History']**\n\n**RandomForestClassifier ACCURACY train: 0.8214, test: 0.8011**\n\n**XGBClassifier          ACCURACY train: 0.8214, test: 0.8011**\n\n\nAdding more features to the model does not improve my results :-|\n\n\nI ran out of ideas on this dataset: if you have any comments\/suggestion\/criticism\/ideas please share them in the comments","c1690b35":"At this point the dataset is consistent","3e75ebb5":"For this dataset I will comment my choices for each column in the related cell"}}