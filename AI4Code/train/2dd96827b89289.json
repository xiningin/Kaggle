{"cell_type":{"88d9dd76":"code","785ac5f4":"code","06ef0263":"code","3ce65ca8":"code","93be19a7":"code","d4c6a0dd":"code","c8763ba3":"code","6d5b8cc2":"code","a5120425":"code","0495537b":"code","94909a57":"code","df1cdc32":"code","d6f89c9b":"code","45702fc9":"code","bb386d38":"code","6d29eff1":"code","c91b3d0c":"code","655a2a69":"code","cdec1963":"code","70ca7efb":"code","e94a8280":"code","eb10ffe3":"code","9d097484":"markdown","00677e0e":"markdown","8a529d60":"markdown","e3e6f0ba":"markdown","47f10118":"markdown","015ec706":"markdown","5d74ce4d":"markdown","2023ae85":"markdown"},"source":{"88d9dd76":"!pip install git+https:\/\/github.com\/goolig\/dsClass.git","785ac5f4":"# imports:\nimport pandas as pd\nfrom dsClass.path_helper import *\nimport sklearn","06ef0263":"theft_data_path = get_file_path('theft_data_bgu.csv')\ntheft_data = pd.read_csv(theft_data_path)\nholiday_data_path = get_file_path('holidays_data_bgu.csv')\nholidays_data_bgu = pd.read_csv(holiday_data_path)","3ce65ca8":"theft_data.head()","93be19a7":"# check the columns:\ntheft_data.columns","d4c6a0dd":"#Q1:\n\n# Filter only important columns:\n# keep only the following columns in your dataframe- Date, District, Count(our label), Year\n# Overwrite theft_data\n\n\n\n#### insert your code here:\n#overwriting theft_data with filtered columns\ntheft_data = theft_data.filter(items=['Date', 'District', 'Count', 'Year'])\n\n\n####\n\ntheft_data.columns","c8763ba3":"#Q2:\n\n# Change the Date feature from string to timestamp, in theft_data & holidays_data_bgu:\n# Hint - look at pandas.to_datetime\n\n#### insert your code here:\n#overwrite the dfs with identical dfs with to_datetime in Date columns\ntheft_data[\"Date\"]= pd.to_datetime(theft_data[\"Date\"])\nholidays_data_bgu[\"Date\"]= pd.to_datetime(holidays_data_bgu[\"Date\"])\n\n\n####\n","6d5b8cc2":"# Q3:\n# Add time information:\n\n\n\n# Create day of week column:\n# Name the column Week_day, it will contain a number for each day of week: \n# Monday=0, ...., Sunday=6\n# Hint - pandas.Series.dt.weekday\n\n\n#### insert your code here:\n#create week day column based on date column\nresult = theft_data[\"Date\"].dt.day_name()\n#create column named week_day and assign the dt.weekday we created\ntheft_data[\"Week_day\"] = result\ntheft_data.head()\n####\n\n","a5120425":"# Q4:\n\n# Create season column:\n# Name the column Season, it will contain a string which tells us the season based on the Date: \n# Hint - use the following package to indicate specific dates and write function that gets the date and returns the corresponding season:\n# Any reasnoable assumption about the seasons in chicago will be ok.\nfrom datetime import date\n\n#### insert your code here:\ntoday = date.today()\n#assume summer is between 2\/6 til 1\/9\nsummer_begin = date(today.year, 6, 2)\nsummer_end = date(today.year, 9, 1)\n#assume autumn is between 2\/9 til 31\/12\nautumn_begin = date(today.year, 9, 2)\nautumn_end = date(today.year, 12, 1)\n#assume winter is betweeen 1\/1 til 1\/4\nwinter_begin = date(today.year, 12, 2)\nwinter_end = date(today.year, 4, 1)\n#assume spring is between 2\/4 til 1\/6\nspring_begin = date(today.year, 4, 2)\nspring_end = date(today.year, 6, 1)\n\n\ndef get_season(given):\n    #make year this year to simplify the calculations \n    parsed = given.replace(year=today.year)\n    if parsed >= summer_begin and parsed<= summer_end:\n        return \"Summer\"\n    elif parsed >= autumn_begin and parsed<= autumn_end:\n        return \"Autumn\"\n    elif parsed >= winter_begin or parsed<= winter_end:\n        return \"Winter\"\n    elif  parsed >= spring_begin and parsed<= spring_end:\n        return \"Spring\"\n\n#create list of seasons and make it a column in df\nresult = []\nfor date in theft_data[\"Date\"]:\n    result.append(get_season(date))\ntheft_data[\"Season\"] = result\ntheft_data.head()\n####","0495537b":"# Q5:\n\n# Add information from other sources\n# Create column is_holiday that will take 1 if its an holiday and 0 otherwise.\n# Holiday data can be found in the dataframe holidays_data_bgu\n\n#### insert your code here:\nresult= []\nfor date in theft_data[\"Date\"]:\n    #check if date is holiday\n    if date in holidays_data_bgu[\"Date\"].values:\n        result.append(1)\n    #add 0 only if not found\n    else:\n        result.append(0)\n    \ntheft_data[\"is_holiday\"] = result\n\n\n####","94909a57":"# now you suppose to have the following columns: Date, is_holiday, district, count(our label), year, week_day, season\ntheft_data.columns\n\n","df1cdc32":"# Print statistics of all of the variables: describe or value_counts select the most appropriate for each feature.","d6f89c9b":"#Q6:\n# Create dummy variables for the categorical features:\n# Hint - use the pandas function 'get_dummies'\n# Do not change the date feature\n#### insert your code here:\n\none_hot = pd.get_dummies(theft_data['Season'])\n# Drop column model as it is now encoded\ntheft_data = theft_data.drop('Season',axis = 1)\n#join tables\ntheft_data = theft_data.join(one_hot)\n\none_hot = pd.get_dummies(theft_data['Week_day'])\n# Drop column model as it is now encoded\ntheft_data = theft_data.drop('Week_day',axis = 1)\n#join tables\ntheft_data = theft_data.join(one_hot)\n\none_hot = pd.get_dummies(theft_data['District'])\n# Drop column model as it is now encoded\ntheft_data = theft_data.drop('District',axis = 1)\n#join tables\ntheft_data = theft_data.join(one_hot)\ntheft_data.head()\n\n####","45702fc9":"# Now you suppose to have the following columns: Date, is_holiday, count, year,\n# Dummies for season, week_day and district\ntheft_data.columns","bb386d38":"#Q7:\n\n# Remove the original date columns: (we won't use it for modelling)\n# Print the list of features.\n#### insert your code here:\ntheft_data = theft_data.drop('Date',axis = 1)\ntheft_data.columns\n\n\n####","6d29eff1":"# choose years for train and test\ntrain_start_year = 2014\ntrain_end_year = 2015\ntest_year = 2016","c91b3d0c":"# split the data into train\/test\ndataTrain = theft_data[(theft_data[\"Year\"] >= train_start_year) & (theft_data[\"Year\"] <= train_end_year)]\nlabelsTrain = dataTrain.Count\ndataTrain = dataTrain.drop('Count', axis=1)\n\ndataTest = theft_data[(theft_data[\"Year\"] == test_year)]\nlabelsTest = dataTest.Count\ndataTest = dataTest.drop('Count', axis=1)\n\n# Remove unnecessary columns:\ndataTrain = dataTrain.drop('Year', axis=1)\ndataTest = dataTest.drop('Year', axis=1)\n\nprint(\"Train data shape: \" , dataTrain.shape)\nprint(\"Test data shape: \" , dataTest.shape)\n","655a2a69":"# check for null values (should print 0)\nprint(theft_data.isnull().sum().sum())\n","cdec1963":"from sklearn.linear_model import LinearRegression\n\nmlModel = LinearRegression()\n\nmlModel.fit(dataTrain, labelsTrain)\n","70ca7efb":"predTest = mlModel.predict(dataTest)\n\n# print the Rsquare:\nprint(\"Test set R^2: \", mlModel.score(dataTest, labelsTest))","e94a8280":"#Q8:\n# Train the model on years 2013-2015 and test it on 2016.\n# How does the model results change?\n# The results are more accurate (graeter R^2) whene we train the model over 2014-2015 data (we tested the modle on 2016 data in both cases). \n \n\n#Test set R^2:  0.5595970144026327 is for 2013-2015, tested on 2016\n#Test set R^2:  0.5649622632097961 is for 2014-2015, tested on 2016\n#Test set R^2:  0.5696382817516925 is for 2015 alone, tested on 2016\n\n\n","eb10ffe3":"#Q9:\n# Now read the documentation and explanation of cross-validation:\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html\n# Use it (sklearn.model_selection.cross_validate) to evaluate the model with all of the avialable data\n# Print the result\n\n# Explain the difference in resulst between cross-validation and year-based train-test split:\n\n# We got better results for year-based train-test split. The data we are working on, is based on human behavior and  \n# tend to change over the years. Therefor, when the model is trained over \"older\" data, its calculating data that could\n# be irrelevant to the present and reduce its accuracy as a result. \n\n# Explain which evaluation scheme (cross-validation or year-based) is more approriate in this case. Why?\n# Based on the claim above, we believe that year-based scheme is a better approach because it will be more accurate in \n#regards to the spefic time we are evaluating. \n\ntheft_data_no_count = theft_data.drop(\"Count\",axis = 1)\n\n# By default, the score computed at each CV iteration is the score method of the estimator.\nresult = sklearn.model_selection.cross_validate(LinearRegression(), theft_data_no_count, theft_data[\"Count\"])\nprint(result[\"test_score\"])\n\n#calculate average score\navg = sum(result[\"test_score\"])\/len(result[\"test_score\"])\nprint(\"average score: \", avg)\n","9d097484":"\u05d4\u05ea\u05e8\u05d2\u05d9\u05dc \u05e2\u05d5\u05e9\u05d4 \u05e9\u05d9\u05de\u05d5\u05e9 \u05d1\u05e9\u05ea\u05d9 \u05d8\u05d1\u05dc\u05d0\u05d5\u05ea: holidays_data_bgu, \u05d4\u05de\u05db\u05d9\u05dc \u05de\u05d9\u05d3\u05e2 \u05dc\u05d2\u05d1\u05d9 \u05d4\u05d9\u05de\u05d9\u05dd \u05d1\u05e9\u05e0\u05d4 \u05d0\u05e9\u05e8 \u05e0\u05d7\u05e9\u05d1\u05d9\u05dd \u05d7\u05d2\u05d9\u05dd. \u05d1\u05e0\u05d5\u05e1\u05e3, theft_data_bgu \u05d4\u05de\u05db\u05d9\u05dc \u05de\u05d9\u05d3\u05e2 \u05dc\u05d2\u05d1\u05d9 \u05d4\u05e4\u05e9\u05d9\u05e2\u05d4 \u05d1\u05e9\u05d9\u05e7\u05d2\u05d5. \u05e0\u05d9\u05ea\u05df \u05dc\u05e7\u05e8\u05d5\u05d0 \u05e2\u05d5\u05d3 \u05d1\u05e7\u05d9\u05e9\u05d5\u05e8 [\u05d4\u05d6\u05d4](https:\/\/data.cityofchicago.org\/Public-Safety\/Crimes-2001-to-present\/ijzp-q8t2) (\u05d0\u05e0\u05d7\u05e0\u05d5 \u05de\u05e9\u05ea\u05de\u05e9\u05d9\u05dd \u05d1\u05d7\u05dc\u05e7 \u05de\u05d4\u05d3\u05d0\u05d8\u05d0 \u05e9\u05d1\u05e7\u05d9\u05e9\u05d5\u05e8).\n\n\u05d0\u05d7\u05e8\u05d9 \u05db\u05dc \u05de\u05e9\u05d9\u05de\u05d4, \u05d4\u05d3\u05e4\u05d9\u05e1\u05d5 \u05dc\u05de\u05e1\u05da \u05d0\u05ea \u05d4\u05e4\u05dc\u05d8 \u05d0\u05d5 \u05e4\u05dc\u05d8\u05d9\u05dd \u05e9\u05dc \u05d4\u05e0\u05ea\u05d5\u05e0\u05d9\u05dd \u05d4\u05e8\u05dc\u05d5\u05d5\u05e0\u05d8\u05d9\u05dd\n\u05dc\u05d0 \u05dc\u05e9\u05db\u05d5\u05d7 \u05dc\u05db\u05dc\u05d5\u05dc \u05d4\u05e2\u05e8\u05d5\u05ea \u05d4\u05de\u05e1\u05d1\u05d9\u05e8\u05d5\u05ea \u05de\u05d4 \u05d1\u05d9\u05e6\u05e2\u05ea\u05dd \u05de\u05d1\u05d7\u05d9\u05e0\u05d4 \u05d8\u05db\u05e0\u05d9\u05ea \u05d5\u05d4\u05df \u05d0\u05ea \u05d4\u05e1\u05d9\u05d1\u05d4 \u05e9\u05d1\u05d4 \u05d4\u05e9\u05ea\u05de\u05e9\u05ea\u05dd \u05d1\u05e4\u05e7\u05d5\u05d3\u05d5\u05ea","00677e0e":"# BGU Data Science Course","8a529d60":"### Modelling","e3e6f0ba":"### Feature extraction:","47f10118":"### Train-Test Splitting:","015ec706":"### Question:","5d74ce4d":"### Evaluation","2023ae85":"### Load the data:"}}