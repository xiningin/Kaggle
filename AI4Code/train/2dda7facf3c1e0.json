{"cell_type":{"07d4cc7d":"code","437cc780":"code","6db3d1c3":"code","d1d6c6fe":"code","a7f3cf7d":"code","9501d624":"code","9c4778d2":"code","ad6a7e16":"code","169c333f":"code","93cb6e60":"code","e8c0f4fb":"code","bdecec5e":"code","dd1caaea":"code","f6912d1d":"code","be180e45":"code","6907f0b0":"code","085a27f8":"code","b38fb5e3":"code","261426ba":"code","560931b0":"code","a81f3b52":"code","9b64cdf5":"code","1dad218b":"code","b96b9da1":"markdown","723441bb":"markdown","8a07698c":"markdown","1a0ed2b7":"markdown","7a19b348":"markdown","be2d000c":"markdown","ed5dd9e2":"markdown","8d6bbe1c":"markdown","0ccac0e1":"markdown","395e6631":"markdown","cdc1e36d":"markdown","346a2afa":"markdown","e2796fcf":"markdown","f59e9948":"markdown"},"source":{"07d4cc7d":"#@title Environment and Dependencies Setup. Please be patient as it may take a while. You may also need to run this section twice or trice if there are any errors. And in some cases, Runtime also needs to be restarted\n# The MIT-Zero License\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so.\n\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n\n\n# Create the environment and install required packages\n!git clone https:\/\/github.com\/asigalov61\/aws-deepcomposer-samples.git\n%cd \/content\/aws-deepcomposer-samples\/ar-cnn\n%tensorflow_version 1.x\n!pip install -r requirements.txt\n!pip install tensorflow-gpu==1.15.3\n!pip install pyFluidSynth\n!apt install fluidsynth\n!pip install midi2audio\n!pip install utils\n!cp \/usr\/share\/sounds\/sf2\/FluidR3_GM.sf2 \/content\/font.sf2","437cc780":"#@title Import all modules and other necessary code\n\n# Imports\n\nfrom google.colab import output\nfrom IPython.display import display, Javascript, HTML, Audio\nimport os\nimport glob\nimport json\nimport numpy as np\nimport keras\nfrom enum import Enum\nfrom keras.models import Model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, Dropout\nfrom keras.optimizers import Adam, RMSprop\nfrom keras import backend as K\nfrom random import randrange\nimport random\nimport math\nimport pypianoroll\nfrom utils.midi_utils import play_midi, plot_pianoroll, get_music_metrics, process_pianoroll, process_midi\n\nfrom augmentation import AddAndRemoveAPercentageOfNotes\nfrom data_generator import PianoRollGenerator\nfrom utils.generate_training_plots import GenerateTrainingPlots\n#from inference import Inference\nfrom model import OptimizerType\nfrom model import ArCnnModel\nfrom midi2audio import FluidSynth\n\nclass Constants():\n    # Make it a multiple of the batch size for best (balanced) performance\n    samples_per_ground_truth_data_item = 8\n    training_validation_split = 0.9\n    # Number of Bars\n    bars = 16\n    # Number of Beats Per Bar\n    beats_per_bar = 4\n    beat_resolution = 4\n    # number of bars to be shifted\n    bars_shifted_per_sample = 16\n    # Total number of pitches in a Pianoroll\n    number_of_pitches = 128\n    # Total number of Tracks\n    number_of_channels = 1\n    output_file_path = \"output_midi.mid\"\n    tempo = 100  # 100 bpm\n\n!mkdir \/content\/aws-deepcomposer-samples\/ar-cnn\/input     ","6db3d1c3":"#@title (The Best Choice\/Works best stand-alone) Super Piano Original 2500 MIDIs \n%cd \/content\/aws-deepcomposer-samples\/ar-cnn\/input\n!wget 'https:\/\/github.com\/asigalov61\/SuperPiano\/raw\/master\/Super_Piano_2_MIDI_DataSet_CC_BY_NC_SA.zip'\n!unzip -j 'Super_Piano_2_MIDI_DataSet_CC_BY_NC_SA.zip'","d1d6c6fe":"#@title (Second Best Choice\/Works best stand-alone) Alex Piano Only Drafts Original 1500 MIDIs \n%cd \/content\/aws-deepcomposer-samples\/ar-cnn\/input\n!wget 'https:\/\/github.com\/asigalov61\/AlexMIDIDataSet\/raw\/master\/AlexMIDIDataSet-CC-BY-NC-SA-All-Drafts-Piano-Only.zip'\n!unzip -j 'AlexMIDIDataSet-CC-BY-NC-SA-All-Drafts-Piano-Only.zip'","a7f3cf7d":"#!unzip data\/JSB\\ Chorales.zip -d data\n#@title (Alternative Choice) Alex Piano Only Original 450 MIDIs \n!mkdir \/content\/aws-deepcomposer-samples\/ar-cnn\/input \n%cd \/content\/aws-deepcomposer-samples\/ar-cnn\/input\n!wget 'https:\/\/github.com\/asigalov61\/AlexMIDIDataSet\/raw\/master\/AlexMIDIDataSet-CC-BY-NC-SA-Piano-Only.zip'\n!unzip -j 'AlexMIDIDataSet-CC-BY-NC-SA-Piano-Only.zip'","9501d624":"#@title Super Piano 2 Pre-Trained Model (floss=1.11 - 12.5k training steps)\n%cd \/content\/Performance-RNN-PyTorch\/save\n!wget 'https:\/\/superpiano.s3-us-west-1.amazonaws.com\/trained-model-checkpoint-floss0-966-5ksteps.h5'","9c4778d2":"#@title Import the MIDI files from the data_dir and save them with the midi_files variable  \ndata_dir = '\/content\/aws-deepcomposer-samples\/ar-cnn\/input\/*.*'\nmidi_files = glob.glob(data_dir)\n\n#Finds our random MIDI file from the midi_files variable and then plays it\n#Note: To listen to multiple samples from the Bach dataset, you can run this cell over and over again. \nrandom_midi = randrange(len(midi_files))\nplay_midi(midi_files[random_midi])\n#print(midi_files)\nprint(midi_files[random_midi])","ad6a7e16":"#@title Generate MIDI file samples and shuffle the DataSet\ndef generate_samples(midi_files, bars, beats_per_bar, beat_resolution, bars_shifted_per_sample):\n    \"\"\"\n    dataset_files: All files in the dataset\n    return: piano roll samples sized to X bars\n    \"\"\"\n    timesteps_per_nbars = bars * beats_per_bar * beat_resolution\n    time_steps_shifted_per_sample = bars_shifted_per_sample * beats_per_bar * beat_resolution\n    samples = []\n    for midi_file in midi_files:\n        pianoroll = process_midi(midi_file, beat_resolution) # Parse the MIDI file and get the piano roll\n        samples.extend(process_pianoroll(pianoroll, time_steps_shifted_per_sample, timesteps_per_nbars))\n    return samples\n\n# Saving the generated samples into a dataset variable \ndataset_samples = generate_samples(midi_files, Constants.bars, Constants.beats_per_bar,Constants.beat_resolution, Constants.bars_shifted_per_sample)\n# Shuffle the dataset\nrandom.shuffle(dataset_samples);\nrandom_pianoroll = dataset_samples[randrange(len(dataset_samples))]\n\nplot_pianoroll(random_pianoroll, 4)","169c333f":"#@title Sampling from a uniform distribution\nsampling_lower_bound_remove = 0 \nsampling_upper_bound_remove = 100\nsampling_lower_bound_add = 1\nsampling_upper_bound_add = 1.5","93cb6e60":"#@title Calculate Training and Validation sampling lengths\ndataset_size = len(dataset_samples)\ndataset_split = math.floor(dataset_size * Constants.training_validation_split) \n\ntraining_samples = dataset_samples[0:dataset_split]\nprint(\"training samples length: {}\".format(len(training_samples)))\nvalidation_samples = dataset_samples[dataset_split + 1:dataset_size]\nprint(\"validation samples length: {}\".format(len(validation_samples)))","e8c0f4fb":"#@title Specifying training hyperparameters. NOTE If you want to test that your model is training on your custom dataset, you can decrease the number of epochs down to 1 in this cell.\n\n# Piano Roll Input Dimensions\ninput_dim = (Constants.bars * Constants.beats_per_bar * Constants.beat_resolution, \n             Constants.number_of_pitches, \n             Constants.number_of_channels)\n# Number of Filters In The Convolution\nnum_filters = 32\n# Growth Rate Of Number Of Filters At Each Convolution\ngrowth_factor = 2\n# Number Of Encoder And Decoder Layers\nnum_layers = 5\n# A List Of Dropout Values At Each Encoder Layer\ndropout_rate_encoder = [0, 0.5, 0.5, 0.5, 0.5]\n# A List Of Dropout Values At Each Decoder Layer\ndropout_rate_decoder = [0.5, 0.5, 0.5, 0.5, 0]\n# A List Of Flags To Ensure If batch_normalization Should be performed At Each Encoder\nbatch_norm_encoder = [True, True, True, True, False]\n# A List Of Flags To Ensure If batch_normalization Should be performed At Each Decoder\nbatch_norm_decoder = [True, True, True, True, False]\n# Path to Pretrained Model If You Want To Initialize Weights Of The Network With The Pretrained Model\npre_trained = False\n# Learning Rate Of The Model\nlearning_rate = 0.001\n# Optimizer To Use While Training The Model\noptimizer_enum = OptimizerType.ADAM\n# Batch Size\nbatch_size = 32\n# Number Of Epochs\nepochs = 3","bdecec5e":"#@title Calculate the number of Batch Iterations Before A Training Epoch Is Considered Finished\nsteps_per_epoch = int(\n    len(training_samples) * Constants.samples_per_ground_truth_data_item \/ int(batch_size))\n\nprint(\"The Total Number Of Steps Per Epoch Are: \"+ str(steps_per_epoch))\n\n# Total Number Of Time Steps\nn_timesteps = Constants.bars * Constants.beat_resolution * Constants.beats_per_bar","dd1caaea":"#@title Creating the data generators that perform data augmentation. To create the input piano rolls during training, we need data generators for both the training and validation samples. For our purposes, we use a custom data generator to perform data augmentation.\n# Training Data Generator\ntraining_data_generator = PianoRollGenerator(sample_list=training_samples,\n                                             sampling_lower_bound_remove = sampling_lower_bound_remove,\n                                             sampling_upper_bound_remove = sampling_upper_bound_remove,\n                                             sampling_lower_bound_add = sampling_lower_bound_add,\n                                             sampling_upper_bound_add = sampling_upper_bound_add,\n                                             batch_size = batch_size,\n                                             bars = Constants.bars,\n                                             samples_per_data_item = Constants.samples_per_ground_truth_data_item,\n                                             beat_resolution = Constants.beat_resolution,\n                                             beats_per_bar = Constants.beats_per_bar,\n                                             number_of_pitches = Constants.number_of_pitches,\n                                             number_of_channels = Constants.number_of_channels)\n# Validation Data Generator\nvalidation_data_generator = PianoRollGenerator(sample_list = validation_samples,\n                                               sampling_lower_bound_remove = sampling_lower_bound_remove,\n                                               sampling_upper_bound_remove = sampling_upper_bound_remove,\n                                               sampling_lower_bound_add = sampling_lower_bound_add,\n                                               sampling_upper_bound_add = sampling_upper_bound_add,\n                                               batch_size = batch_size, \n                                               bars = Constants.bars,\n                                               samples_per_data_item = Constants.samples_per_ground_truth_data_item,\n                                               beat_resolution = Constants.beat_resolution,\n                                               beats_per_bar = Constants.beats_per_bar, \n                                               number_of_pitches = Constants.number_of_pitches,\n                                               number_of_channels = Constants.number_of_channels)","f6912d1d":"#@title Creating callbacks for the model and performing initial intitialization\n# Callback For Loss Plots \nplot_losses = GenerateTrainingPlots()\n## Checkpoint Path\ncheckpoint_filepath =  '\/content\/aws-deepcomposer-samples\/ar-cnn\/trained-model-checkpoint.h5'\n\n# Callback For Saving Model Checkpoints \nmodel_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True)\nlogdir = \"\/content\/aws-deepcomposer-samples\/ar-cnn\"\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n# Create A List Of Callbacks\ncallbacks_list = [plot_losses, model_checkpoint_callback, tensorboard_callback]\n\n\n# Create A Model Instance\nMusicModel = ArCnnModel(input_dim = input_dim,\n                        num_filters = num_filters,\n                        growth_factor = growth_factor,\n                        num_layers = num_layers,\n                        dropout_rate_encoder = dropout_rate_encoder,\n                        dropout_rate_decoder = dropout_rate_decoder,\n                        batch_norm_encoder = batch_norm_encoder,\n                        batch_norm_decoder = batch_norm_decoder,\n                        pre_trained = pre_trained,\n                        learning_rate = learning_rate,\n                        optimizer_enum = optimizer_enum)\n\nmodel = MusicModel.build_model()","be180e45":"#@title Tensorboard Graphs and Stats\n# Load the TensorBoard notebook extension\n%reload_ext tensorboard\nimport tensorflow as tf\nimport datetime, os\n%tensorboard --logdir '\/content\/aws-deepcomposer-samples\/ar-cnn'","6907f0b0":"#@title Main Training Loop. In the following cell, you start training your model. NOTE: Training times can vary greatly based on the parameters that you have chosen and the notebook instance type that you chose when launching this notebook.\n# Start Training\n#logdir = \"\/content\/aws-deepcomposer-samples\/ar-cnn\"\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\nhistory = model.fit_generator(training_data_generator,\n                              use_multiprocessing = True,\n                              validation_data = validation_data_generator,\n                              steps_per_epoch = steps_per_epoch,\n                              epochs = epochs,\n                              callbacks = callbacks_list) #callbacks_list\nmodel.save('\/content\/aws-deepcomposer-samples\/ar-cnn\/trained-model-checkpoint.h5') ","085a27f8":"#@title (Optional) Save your model manually if necessary\nmodel.save('\/content\/aws-deepcomposer-samples\/ar-cnn\/trained-model-checkpoint.h5') ","b38fb5e3":"#@title Inference Code Initialization Routine\nimport os\nimport logging\nimport pypianoroll\nimport keras\nimport numpy as np\nfrom losses import Loss\nimport copy\n\nlogger = logging.getLogger(__name__)\n\n\nclass Inference:\n    def __init__(self, model=None):\n        self.model = model\n        self.number_of_timesteps = (Constants.beat_resolution *\n                                    Constants.beats_per_bar * Constants.bars)\n\n    def load_model(self, model_path):\n        \"\"\"\n        Loads a trained keras model\n        Parameters\n        ----------\n        model_path : string\n            Full file path to the trained model\n        Returns\n        -------\n        None\n        \"\"\"\n        self.model = keras.models.load_model(model_path,\n                                             custom_objects={\n                                                 'built_in_softmax_kl_loss':\n                                                 Loss.built_in_softmax_kl_loss\n                                             },\n                                             compile=False)\n\n    @staticmethod\n    def convert_tensor_to_midi(tensor, tempo, output_file_path):\n        \"\"\"\n        Writes a pianoroll tensor to a midi file\n        Parameters\n        ----------\n        tensor : 2d numpy array\n            pianoroll to be converted to a midi\n        tempo : float\n            tempo to output\n        output_file_path : str\n            output midi file path\n        Returns\n        -------\n        None\n        \"\"\"\n\n        single_track = pypianoroll.Track(pianoroll=tensor)\n        multi_track = pypianoroll.Multitrack(\n            tracks=[single_track],\n            tempo=tempo,\n            beat_resolution=Constants.beat_resolution)\n        output_file_index = 0\n        while os.path.isfile(output_file_path.format(output_file_index)):\n            output_file_index += 1\n        multi_track.write(output_file_path.format(output_file_index))\n\n    @staticmethod\n    def get_indices(input_tensor, value):\n        \"\"\"\n        Parameters\n        ----------\n        input_tensor : 2d numpy array\n        value : int (either 1 or 0)\n        Returns\n        -------\n        indices_with_value : 2d array of indices in the input_tensor where the pixel value equals value (1 or 0).\n        \"\"\"\n        indices_with_value = np.argwhere(input_tensor.astype(np.bool_) == value)\n        return set(map(tuple, indices_with_value))\n\n    @staticmethod\n    def get_softmax(input_tensor, temperature):\n        \"\"\"\n        Gets the softmax of a tensor with temperature\n        Parameters\n        ----------\n        input_tensor : numpy array\n            original tensor (e.g. original predictions)\n        temperature : int\n            softmax temperature\n        Returns\n        -------\n        tensor : numpy array\n            softmax of input tensor with temperature\n        \"\"\"\n        tensor = input_tensor \/ temperature\n        tensor = np.exp(tensor)\n        tensor = tensor \/ np.sum(tensor)\n        return tensor\n\n    @staticmethod\n    def get_sampled_index(input_tensor):\n        \"\"\"\n        Gets a randomly chosen index from the input tensor\n        Parameters\n        ----------\n        input_tensor : numpy array\n            original tensor\n        Returns\n        -------\n        tensor : numpy array\n            softmax of input tensor with temperature\n        \"\"\"\n\n        sampled_index = np.random.choice(range(input_tensor.size),\n                                         1,\n                                         p=input_tensor.ravel())\n        sampled_index = np.unravel_index(sampled_index, input_tensor.shape)\n        return sampled_index\n\n    def generate_composition(self, input_midi_path, inference_params):\n        \"\"\"\n        Generates a new composition based on an old midi\n        Parameters\n        ----------\n        input_midi_path : str\n            input midi path\n        inference_params : json\n            JSON with inference parameters\n        Returns\n        -------\n        None\n        \"\"\"\n        try:\n            input_tensor = self.convert_midi_to_tensor(input_midi_path)\n            output_tensor = self.sample_multiple(\n                input_tensor, inference_params['temperature'],\n                inference_params['maxPercentageOfInitialNotesRemoved'],\n                inference_params['maxNotesAdded'],\n                inference_params['samplingIterations'])\n            self.convert_tensor_to_midi(output_tensor, Constants.tempo,\n                                        Constants.output_file_path)\n        except Exception:\n            logger.error(\"Unable to generate composition.\")\n            raise\n\n    def convert_midi_to_tensor(self, input_midi_path):\n        \"\"\"\n        Converts a midi to pianoroll tensor\n        Parameters\n        ----------\n        input_midi_path : string\n            Full file path to the input midi\n        Returns\n        -------\n        2d numpy array\n            2d tensor that is a pianoroll\n        \"\"\"\n\n        multi_track = pypianoroll.Multitrack(\n            beat_resolution=Constants.beat_resolution)\n        try:\n            multi_track.parse_midi(input_midi_path,\n                                   algorithm='custom',\n                                   first_beat_time=0)\n        except Exception as e:\n            logger.error(\"Failed to parse the MIDI file.\")\n\n        if len(multi_track.tracks) > 1:\n            logger.error(\"Input MIDI file has more than 1 track.\")\n\n        multi_track.pad_to_multiple(self.number_of_timesteps)\n        multi_track.binarize()\n        pianoroll = multi_track.tracks[0].pianoroll\n\n        if pianoroll.shape[0] > self.number_of_timesteps:\n            logger.error(\"Input MIDI file is longer than 8 bars.\")\n\n        # truncate\n        tensor = pianoroll[0:self.number_of_timesteps, ]\n        tensor = np.expand_dims(tensor, axis=0)\n        tensor = np.expand_dims(tensor, axis=3)\n\n        return tensor\n\n    def mask_not_allowed_notes(self, current_input_indices, output_tensor):\n        \"\"\"\n        Masks notes in output tensor that cannot be added or removed\n        Parameters\n        ----------\n        current_input_indices : 2d numpy array\n          indices to be masked based on the current input that was fed to model\n        output_tensor : 2d numpy array\n          consists of probabilities that are predicted by the model\n        Returns\n        -------\n        2d numpy array - output tensor with not allowed notes masked\n        \"\"\"\n\n        if len(current_input_indices) != 0:\n            output_tensor[tuple(np.asarray(list(current_input_indices)).T)] = 0\n            if np.count_nonzero(output_tensor) != 0:\n                output_tensor = output_tensor \/ np.sum(output_tensor)\n        return output_tensor\n\n    def sample_multiple(self, input_tensor, temperature,\n                        max_removal_percentage, max_notes_to_add,\n                        number_of_iterations):\n        \"\"\"\n        Samples multiple times from an tensor.\n        Returns the final output tensor after X number of iterations.\n        Parameters\n        ----------\n        input_tensor : 2d numpy array\n            original tensor (i.e. user input melody)\n        temperature : float\n            temperature to apply before softmax during inference\n        max_removal_percentage : float\n            maximum percentage of notes that can be removed from the original input\n        max_notes_to_add : int\n            maximum number of notes that can be added to the original input\n        number_of_iterations : int\n            number of iterations to sample from the model predictions\n        Returns\n        -------\n        2d numpy array\n            output tensor (i.e. new composition)\n        \"\"\"\n\n        max_original_notes_to_remove = int(\n            max_removal_percentage * np.count_nonzero(input_tensor) \/ 100)\n        notes_removed_count = 0\n        notes_added_count = 0\n\n        original_input_one_indices = self.get_indices(input_tensor, 1)\n        original_input_zero_indices = self.get_indices(input_tensor, 0)\n\n        current_input_one_indices = copy.deepcopy(original_input_one_indices)\n        current_input_zero_indices = copy.deepcopy(original_input_zero_indices)\n\n        for _ in range(number_of_iterations):\n            input_tensor, notes_removed_count, notes_added_count = self.sample_notes_from_model(\n                input_tensor, max_original_notes_to_remove, max_notes_to_add,\n                temperature, notes_removed_count, notes_added_count,\n                original_input_one_indices, original_input_zero_indices,\n                current_input_zero_indices, current_input_one_indices)\n\n        return input_tensor.reshape(self.number_of_timesteps,\n                                    Constants.number_of_pitches)\n\n    def sample_notes_from_model(self,\n                                input_tensor,\n                                max_original_notes_to_remove,\n                                max_notes_to_add,\n                                temperature,\n                                notes_removed_count,\n                                notes_added_count,\n                                original_input_one_indices,\n                                original_input_zero_indices,\n                                current_input_zero_indices,\n                                current_input_one_indices,\n                                num_notes=1):\n        \"\"\"\n        Generates a sample from the tensor and return a new tensor\n        Modifies current_input_zero_indices, current_input_one_indices, and input_tensor\n        Parameters\n        ----------\n        input_tensor : 2d numpy array\n            input tensor to feed into the model\n        max_original_notes_to_remove : int\n            maximum number of notes to remove from the original input\n        max_notes_to_add : int\n            maximum number of notes that can be added to the original input\n        temperature : float\n            temperature to apply before softmax during inference\n        notes_removed_count : int\n            number of original notes that have been removed from input\n        notes_added_count : int\n            number of new notes that have been added to the input\n        original_input_one_indices : set of tuples\n            indices which have value 1 in original input\n        original_input_zero_indices : set of tuples\n            indices which have value 0 in original input\n        current_input_zero_indices : set of tuples\n            indices which have value 0 and were not part of the original input\n        current_input_one_indices : set of tuples\n            indices which have value 1 and were part of the original input\n        Returns\n        -------\n        input_tensor : 2d numpy array\n            output after samping from the model prediction\n        notes_removed_count : int\n            updated number of original notes removed\n        notes_added_count : int\n            updated number of new notes added\n        \"\"\"\n\n        output_tensor = self.model.predict([input_tensor])\n\n        # Apply temperature and softmax\n        output_tensor = self.get_softmax(output_tensor, temperature)\n\n        if notes_removed_count >= max_original_notes_to_remove:\n            # Mask all pixels that both have a note and were once part of the original input\n            output_tensor = self.mask_not_allowed_notes(current_input_one_indices, output_tensor)\n\n        if notes_added_count > max_notes_to_add:\n            # Mask all pixels that both do not have a note and were not once part of the original input\n            output_tensor = self.mask_not_allowed_notes(current_input_zero_indices, output_tensor)\n\n        if np.count_nonzero(output_tensor) == 0:\n            return input_tensor, notes_removed_count, notes_added_count\n\n        sampled_index = self.get_sampled_index(output_tensor)\n        sampled_index_transpose = tuple(np.array(sampled_index).T[0])\n\n        if input_tensor[sampled_index]:\n            # Check if the note being removed is from the original input\n            if notes_removed_count < max_original_notes_to_remove and (\n                sampled_index_transpose in original_input_one_indices):\n                notes_removed_count += 1\n                current_input_one_indices.remove(sampled_index_transpose)\n            elif tuple(sampled_index_transpose) not in original_input_one_indices:\n                notes_added_count -= 1\n                current_input_zero_indices.add(sampled_index_transpose)\n            input_tensor[sampled_index] = 0\n        else:\n            # Check if the note being added is not in original input\n            if sampled_index_transpose not in original_input_one_indices:\n                notes_added_count += 1\n                current_input_zero_indices.remove(sampled_index_transpose)\n            else:\n                notes_removed_count -= 1\n                current_input_one_indices.add(sampled_index_transpose)\n            input_tensor[sampled_index] = 1\n        input_tensor = input_tensor.astype(np.bool_)\n        return input_tensor, notes_removed_count, notes_added_count\n","261426ba":"#@title Loading a saved checkpoint file. To use your trained model, you will need to update the PATH variable in the cell below.\nfull_path_to_checkpoint_file = \"\/content\/aws-deepcomposer-samples\/ar-cnn\/trained-model-checkpoint-floss0-966-5ksteps.h5\" #@param {type:\"string\"}\n\n%cd \/content\/aws-deepcomposer-samples\/ar-cnn\n# Create An Inference Object\ninference_obj = Inference()\n# Load The Checkpoint\ncheckpoint_var = full_path_to_checkpoint_file\ninference_obj.load_model(checkpoint_var)","560931b0":"#@title MODEL INFERENCE\/GENERATION CONTROLS { run: \"auto\" }\nmodel_temperature = 6 #@param {type:\"slider\", min:0.1, max:6, step:0.1}\npercentage_of_max_removed_notes = 50 #@param {type:\"slider\", min:0, max:100, step:1}\npercentage_of_max_added_notes = 100 #@param {type:\"slider\", min:0, max:100, step:1}\noutput_refinement_iterations = 100 #@param {type:\"slider\", min:0, max:100, step:1}\n#with open('inference_parameters.json') as json_file:\n#    inference_params = json.load(json_file)\n#print(inference_params)\n\nimport json\n\ndata = {\n  u\"temperature\": float(model_temperature),\n  u\"maxNotesAdded\": int(percentage_of_max_added_notes),\n  u\"maxPercentageOfInitialNotesRemoved\": int(percentage_of_max_removed_notes),\n  u\"samplingIterations\": int(output_refinement_iterations)\n}\n\ndata = json.dumps(data) # dict to string\ninference_params = json.loads(data) # string to json\n#print (inference_params)","a81f3b52":"#@title Main Generation Loop\ndesired_midi_file_full_path = \"\/content\/aws-deepcomposer-samples\/ar-cnn\/seed3.mid\" #@param {type:\"string\"}\n# Generate The Composition\n%cd \/content\/aws-deepcomposer-samples\/ar-cnn\n!rm output_wav.wav output_midi.mid\ninput_melody = '\/content\/aws-deepcomposer-samples\/ar-cnn\/sample_inputs\/ode_to_joy.midi'  \ninference_obj.generate_composition(desired_midi_file_full_path, inference_params)\nFluidSynth(\"\/content\/font.sf2\").midi_to_audio('output_midi.mid','output_wav.wav')\n# set the src and play\nfrom google.colab import files\nfiles.download('\/content\/aws-deepcomposer-samples\/ar-cnn\/output_midi.mid')\nAudio(\"output_wav.wav\")","9b64cdf5":"#@title Plot and Graph the Output :) Only first batch MIDI file is plotted and displayed \ngraphs_length_inches = 18 #@param {type:\"slider\", min:0, max:20, step:1}\nnotes_graph_height = 6 #@param {type:\"slider\", min:0, max:20, step:1}\nhighest_displayed_pitch = 100 #@param {type:\"slider\", min:1, max:128, step:1}\nlowest_displayed_pitch = 10 #@param {type:\"slider\", min:1, max:128, step:1}\n\nimport librosa\nimport numpy as np\nimport pretty_midi\nimport pypianoroll\nfrom pypianoroll import Multitrack, Track\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.use('SVG')\n# For plotting\nimport mir_eval.display\nimport librosa.display\n%matplotlib inline\noutput_melody = '\/content\/aws-deepcomposer-samples\/ar-cnn\/output_midi.mid'\n\nmidi_data = pretty_midi.PrettyMIDI('\/content\/aws-deepcomposer-samples\/ar-cnn\/output_midi.mid')\n\ndef plot_piano_roll(pm, start_pitch, end_pitch, fs=100):\n    # Use librosa's specshow function for displaying the piano roll\n    librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n                             hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n                             fmin=pretty_midi.note_number_to_hz(start_pitch))\n\n\n\nroll = np.zeros([int(graphs_length_inches), 128])\n# Plot the output\n\ntrack = Multitrack('\/content\/aws-deepcomposer-samples\/ar-cnn\/output_midi.mid', name='track')\nplt.figure(figsize=[graphs_length_inches, notes_graph_height])\nfig, ax = track.plot()\nfig.set_size_inches(graphs_length_inches, notes_graph_height)\nplt.figure(figsize=[graphs_length_inches, notes_graph_height])\nax2 = plot_piano_roll(midi_data, lowest_displayed_pitch, highest_displayed_pitch)\nplt.show(block=False)","1dad218b":"#@title Additional Input\/Output Comparison Metrics of the specified MIDI file\n# Input Midi Metrics:\nprint(\"The input midi metrics are:\")\nget_music_metrics(input_melody, beat_resolution=4)\n\nprint(\"\\n\")\n# Generated Output Midi Metrics:\nprint(\"The generated output midi metrics are:\")\nget_music_metrics(output_melody, beat_resolution=4)\n\n# Convert The Input and Generated Midi To Tensors (a matrix)\ninput_pianoroll = process_midi(input_melody, beat_resolution=4)\noutput_pianoroll = process_midi(output_melody, beat_resolution=4)\n\n# Plot Input Piano Roll\nplot_pianoroll(input_pianoroll, beat_resolution=4)\n\n# Plot Output Piano Roll\nplot_pianoroll(output_pianoroll, beat_resolution=4)","b96b9da1":"### How to change the *inference parameters* when you perform inference \n\nThe model performs inference by sampling from its predicted probability distribution across the entire piano roll. \n\nInference is an iterative process. After adding or removing a note from the input, the model feeds this new input back into itself. The model has been trained to both remove and add notes, so it can improve the input melody and correct mistakes that it may have made in earlier iterations.\n\nYou also can change the *inference parameters* to observe differences in the quality of the music generated: \n\n- Sampling iterations (`samplingIterations`): The number of iterations performed during inference. A higher number of sampling iterations gives the model more time to improve the input melody.\n\n- Maximum notes to remove (`maxPercentageOfInitialNotesRemoved`): The maximum percentage of notes that can be removed during inference. Setting this value to 0% prevents the model from removing notes from your input melody.\n\n- Maximum notes to add (`maxNotesAdded`): The maximum percentage of notes that can be added during inference. Setting this value to 0% means no notes will be added to your input melody\n\n>**NOTE:** If you restrict your model's ability to add and remove notes, you risk creating poor compositions. \n\n- Creativity, `temperature`: To create the output probability distribution, the final layer uses a softmax activation. You can change the temperature for the softmax to produce different levels of creativity in the outputs generated by the model.\n","723441bb":"<a href=\"https:\/\/colab.research.google.com\/github\/asigalov61\/Amazon-Deep-Composer\/blob\/master\/Amazon_Deep_Composer.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","8a07698c":"### Augment the data for better results and training\n\nWe are going to do the following things in this section:\n\n1) Adding or removing notes during training\n\n2) Removing random notes from a target piano roll to create input piano rolls\n\n3) Adding random notes to the target piano roll to create input piano rolls \n","1a0ed2b7":"#Create, Setup the Model. Setup all (Hyper)Parameters","7a19b348":"## Installing dependencies\nFirst, let's install and import all of the Python packages that we will use in this tutorial.","be2d000c":"###(Optional) Download and Unzip pre-trained model","ed5dd9e2":"#Generate and Evaluate\/Plot from the resulting model","8d6bbe1c":"## Preprocessing the data into the *piano roll* format\n\n### Reviewing sample piano rolls \n\n### Why do we use 128 timesteps?\nIn this tutorial, we use 8-[bar](https:\/\/en.wikipedia.org\/wiki\/Bar_(music)) samples from the dataset. We subdivide those 8 bars into 128 timesteps. That's because each of the 8 bars contains 4 beats. We further divide each beat into 4 timesteps. \n\nThis yields 128 timesteps:\n\n$$ \\frac{4\\;timesteps}{1\\;beat} * \\frac{4\\;beats}{1\\;bar} * \\frac{8\\;bars}{1} = 128\\;timesteps $$\n\nWe found that this level of resolution is sufficient to capture the musical details in our dataset.\n\n### Creating samples of uniform size (shape) for model training \n\nFor model training, the *input piano rolls* must be the same size. As you saw when we used the `play_midi` function, each sample isn't the same length. We use two functions to create *target piano rolls* that are the same size: `process_midi` and `process_pianoroll`. These functions are wrapped in a larger function, `generate_samples`, which also takes in constants that are related to subdividing the .mid files.\n\n#### In the code cells below:\n- `generate_samples` is a function used to ingest the midi files and break the files down into a uniform shape\n- `plot_pianoroll` uses a built in function `plot_track` from the  [`pypianoroll`](https:\/\/salu133445.github.io\/pypianoroll\/visualization.html) library to plot a piano roll track from the dataset.","0ccac0e1":"#Start Here","395e6631":"## Performing inference \n\nCongratulations! You have now trained your very own AR-CNN model to generate music. Now you can see how well your model will perform with an input melody. \n","cdc1e36d":"# Amazon Deep Composer\n\n## Training a custom AR-CNN model \nIn this Jupyter notebook, we guide you through several steps of the data science life cycle. We explain how to acquire the data that you use for this project, \nprovide some exploratory data analysis (**EDA**), and show how we augment the data during training. \n\n\n### The AWS DeepComposer approach to generating music  \nAutoregressive-based approaches are prone to accumulate errors during training. To help mitigate this problem, we train our AR-CNN model so that it can detect and then fix mistakes, including those made by the model itself.\n\nWe do this by treating music generation as a series of *edit events*, which can be either the addition or removal of a note. An *edit sequence* is a series of edit events. Every edit sequence can directly correspond to a piano roll.\n\nBy training our model to view the problem as edit events rather than as an entire image or just the addition of notes, we found that it can offset the accumulation of errors and generate higher quality music.\n\nNow that you understand the basic theory behind our approach, let\u2019s dive into the code. In the next section, we show examples of the piano roll format that we use for training the model.","346a2afa":"# Preprocess and Parse Training DataSet","e2796fcf":"## Submitting to the *Spin the Model* Chartbusters challenge\n\nTo submit your composition(s) and model to the *Spin the model* chartbusters challenge you will first need to create a public repository on [GitHub](https:\/\/github.com\/). Then download your notebook, checkpoint files, and compositions from SageMaker, and upload them to your public repository. Use the link from your public repository to make your submission to the Chartbusters challenge! ","f59e9948":"### Download and Unzip Training DataSet\n"}}