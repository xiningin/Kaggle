{"cell_type":{"578e23eb":"code","370adeab":"code","d1559fb6":"code","f47bc2c9":"code","8e00db15":"code","8ec2e0b6":"code","de1fde17":"code","60023a50":"code","eadaa503":"code","1c5b6923":"code","b4368d40":"code","3015ca56":"markdown","9dc02657":"markdown","de1804f5":"markdown","2768115a":"markdown","d41d2886":"markdown","3e38944e":"markdown","ed14633f":"markdown","9615cea1":"markdown","8ff7fdda":"markdown","0e30380a":"markdown","c7afe5f7":"markdown"},"source":{"578e23eb":"import pandas as pd\n\ndata = pd.read_csv('..\/input\/microsoftchallenge4\/clusters.csv')\ndata.head()","370adeab":"data.describe()","d1559fb6":"data.isnull().sum()","f47bc2c9":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\n\nscaled_data = MinMaxScaler().fit_transform(data)\n\npca = PCA(n_components=2).fit(scaled_data)\nprint(pca)\nprint(pca.explained_variance_ratio_)","8e00db15":"features = pca.transform(scaled_data)\nfeatures[0:9]","8ec2e0b6":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.scatter(features[:,0],features[:,1])\n\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\nplt.title('Data')\n\nplt.show()","de1fde17":"import numpy as np\nfrom sklearn.cluster import KMeans\n\n# I got this from references\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i)\n    kmeans.fit(scaled_data)\n    wcss.append(kmeans.inertia_)\n\nplt.plot(range(1, 11), wcss)\n\nplt.title('WCSS by Clusters')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\n\nplt.show()","60023a50":"model = KMeans(n_clusters = 4, init = 'k-means++', n_init = 500, max_iter = 1500)\ndata_clusters = model.fit_predict(scaled_data)\ndata_clusters[0:9]","eadaa503":"def plot_clusters(samples, clusters):\n    col_dic = {0:'blue',1:'green',2:'orange', 3:'cyan'}\n    mrk_dic = {0:'*',1:'x',2:'+', 3:'.'}\n    colors = [col_dic[x] for x in clusters]\n    markers = [mrk_dic[x] for x in clusters]\n    for sample in range(len(clusters)):\n        plt.scatter(samples[sample][0], samples[sample][1], color = colors[sample], marker=markers[sample], s=100)\n    plt.xlabel('Dimension 1')\n    plt.ylabel('Dimension 2')\n    plt.title('Assignments')\n    plt.show()\n\nplot_clusters(features, data_clusters)","1c5b6923":"from sklearn.cluster import AgglomerativeClustering\n\nmodel = AgglomerativeClustering(n_clusters = 4)\ndata_clusters = model.fit_predict(scaled_data)\ndata_clusters[0:9]","b4368d40":"def plot_clusters(samples, clusters):\n    col_dic = {0:'blue',1:'green',2:'orange', 3:'cyan'}\n    mrk_dic = {0:'*',1:'x',2:'+', 3:'.'}\n    colors = [col_dic[x] for x in clusters]\n    markers = [mrk_dic[x] for x in clusters]\n    for sample in range(len(clusters)):\n        plt.scatter(samples[sample][0], samples[sample][1], color = colors[sample], marker=markers[sample], s=100)\n    plt.xlabel('Dimension 1')\n    plt.ylabel('Dimension 2')\n    plt.title('Assignments')\n    plt.show()\n\nplot_clusters(features, data_clusters)","3015ca56":"# Answer","9dc02657":"### Visualize Data","de1804f5":"### K-Means Clusterring","2768115a":"# Clustering Challenge\n\nBy: Muhammad Alwy Shihab (Fresh Graduate of Statistics, Unpad)\n\nClustering is an *unsupervised* machine learning technique in which you train a model to group similar entities into clusters based on their features.\n\nIn this exercise, you must separate a dataset consisting of three numeric features (**A**, **B**, and **C**) into clusters. Run the cell below to load the data.","d41d2886":"### Agglomerative Clustering","3e38944e":"### Principal Component Analysis","ed14633f":"Mulai landai di n = 4","9615cea1":"### How many cluster?","8ff7fdda":"Your challenge is to identify the number of discrete clusters present in the data, and create a clustering model that separates the data into that number of clusters. You should also visualize the clusters to evaluate the level of separation achieved by your model.\n\nAdd markdown and code cells as required to create your solution.","0e30380a":"### Data Exploration","c7afe5f7":"Terlihat lebih rapih menggunakan K-Means Clustering dimana dengan pemetaan tersebut bisa menjelaskan 93% informasi dari data."}}