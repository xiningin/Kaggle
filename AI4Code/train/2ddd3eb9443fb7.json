{"cell_type":{"8589539a":"code","0ea9ef1a":"code","5458d684":"code","a0970573":"code","c8088059":"code","a06ba91a":"code","0529b60b":"code","f134402a":"code","1d61ed57":"code","356d711b":"code","81609415":"code","758365e0":"code","e72ea455":"code","ea1c4893":"markdown","197bd58b":"markdown","e3fc6d02":"markdown","e2376d57":"markdown","41f89301":"markdown","c79430c1":"markdown"},"source":{"8589539a":"import numpy as np\nimport pandas as pd\n\nfrom nltk.corpus import stopwords\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","0ea9ef1a":"data = pd.read_csv('..\/input\/mbti-type\/mbti_1.csv')","5458d684":"data","a0970573":"data.info()","c8088059":"data['type'].unique()","a06ba91a":"def preprocess_inputs(df):\n    \n    texts = df['posts'].copy()\n    labels = df['type'].copy()\n    \n    # Process text data\n    stop_words = stopwords.words('english')\n    \n    texts = [text.lower() for text in texts]\n    texts = [text.split() for text in texts]\n    texts = [[word.strip() for word in text] for text in texts]\n    texts = [[word for word in text if word not in stop_words] for text in texts]\n    \n    vocab_length = 10000\n    \n    tokenizer = Tokenizer(num_words=vocab_length)\n    tokenizer.fit_on_texts(texts)\n    \n    texts = tokenizer.texts_to_sequences(texts)\n    \n    max_seq_length = np.max([len(text) for text in texts])\n    \n    texts = pad_sequences(texts, maxlen=max_seq_length, padding='post')\n    \n    # Process label data\n    label_values = [\n        'INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ'\n    ]\n    \n    label_mapping = {label: np.int(label[0] == 'E') for label in label_values}\n    \n    labels = labels.replace(label_mapping)\n    labels = np.array(labels)\n    \n    return texts, labels, max_seq_length, vocab_length, label_mapping","0529b60b":"texts, labels, max_seq_length, vocab_length, label_mapping = preprocess_inputs(data)","f134402a":"print(\"Text sequences:\\n\", texts.shape)\nprint(\"\\nLabels:\\n\", labels.shape)\nprint(\"\\nMax sequence length:\\n\", max_seq_length)\nprint(\"\\nVocab length:\\n\", vocab_length)\nprint(\"\\nLabel mapping:\\n\", label_mapping)","1d61ed57":"texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, train_size=0.7, random_state=123)","356d711b":"texts","81609415":"embedding_dim = 512\n\ninputs = tf.keras.Input(shape=(max_seq_length,))\n\nembedding = tf.keras.layers.Embedding(\n    input_dim=vocab_length,\n    output_dim=embedding_dim,\n    input_length=max_seq_length\n)(inputs)\n\ngru = tf.keras.layers.Bidirectional(\n    tf.keras.layers.GRU(\n        units=256,\n        return_sequences=True\n    )\n)(embedding)\n\nflatten = tf.keras.layers.Flatten()(gru)\n\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(flatten)\n\n\nmodel = tf.keras.Model(inputs, outputs)\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.AUC(name='auc')\n    ]\n)\n\n\nhistory = model.fit(\n    texts_train,\n    labels_train,\n    validation_split=0.2,\n    batch_size=32,\n    epochs=5,\n    callbacks=[\n        tf.keras.callbacks.ModelCheckpoint('.\/model.h5', save_best_only=True, save_weights_only=True)\n    ]\n)","758365e0":"model.load_weights('.\/model.h5')","e72ea455":"model.evaluate(texts_test, labels_test)","ea1c4893":"# Training","197bd58b":"# Task for Today  \n\n***\n\n## Personality Type Prediction  \n\nGiven *data about posts people have made*, let's try to predict the **personality type** of a given person.  \n  \nWe will use a TensorFlow RNN to make our predictions.","e3fc6d02":"# Results","e2376d57":"# Preprocessing","41f89301":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/s3g0MJcJZyA","c79430c1":"# Getting Started"}}