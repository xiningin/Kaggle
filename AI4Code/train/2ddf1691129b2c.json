{"cell_type":{"cb945ffd":"code","30cf5c8e":"code","78ede64d":"code","6480c789":"code","42680c13":"code","8082db9b":"code","9a9085df":"code","1a23f063":"code","d8dc5fac":"code","5c34bb86":"code","d76e5d8b":"code","51b1bf7c":"code","027d71c4":"code","e1b1679e":"code","50df1603":"code","d69cf5cb":"markdown","11ada619":"markdown","9a50afea":"markdown","6101bb51":"markdown","1f7c28e0":"markdown","a9fa4ba6":"markdown","7cff8375":"markdown","f4baeeb0":"markdown","da6b42cd":"markdown","a2c438d2":"markdown","44d1fca5":"markdown"},"source":{"cb945ffd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","30cf5c8e":"%matplotlib inline\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader,Dataset\nimport matplotlib.pyplot as plt\nimport torchvision.utils\nimport numpy as np\nimport random\nfrom PIL import Image\nimport torch\nfrom torch.autograd import Variable\nimport PIL.ImageOps    \nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F","78ede64d":"def imshow(img,text=None,should_save=False):\n    npimg = img.numpy()\n    plt.axis(\"off\")\n    if text:\n        plt.text(75, 8, text, style='italic',fontweight='bold',\n            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()    \n\ndef show_plot(iteration,loss):\n    plt.plot(iteration,loss)\n    plt.show()","6480c789":"class Config():\n    training_dir = \"..\/input\/facerecognitionusingsiamese\/data\/faces\/training\"\n    testing_dir = \"..\/input\/facerecognitionusingsiamese\/data\/faces\/testing\"\n    train_batch_size = 64\n    train_number_epochs = 100","42680c13":"class SiameseNetworkDataset(Dataset):\n    \n    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n        self.imageFolderDataset = imageFolderDataset    \n        self.transform = transform\n        self.should_invert = should_invert\n        \n    def __getitem__(self,index):\n        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n        #we need to make sure approx 50% of images are in the same class\n        should_get_same_class = random.randint(0,1) \n        if should_get_same_class:\n            while True:\n                #keep looping till the same class image is found\n                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n                if img0_tuple[1]==img1_tuple[1]:\n                    break\n        else:\n            while True:\n                #keep looping till a different class image is found\n                \n                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n                if img0_tuple[1] !=img1_tuple[1]:\n                    break\n\n        img0 = Image.open(img0_tuple[0])\n        img1 = Image.open(img1_tuple[0])\n        img0 = img0.convert(\"L\")\n        img1 = img1.convert(\"L\")\n        \n        if self.should_invert:\n            img0 = PIL.ImageOps.invert(img0)\n            img1 = PIL.ImageOps.invert(img1)\n\n        if self.transform is not None:\n            img0 = self.transform(img0)\n            img1 = self.transform(img1)\n        \n        return img0, img1 , torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n    \n    def __len__(self):\n        return len(self.imageFolderDataset.imgs)","8082db9b":"folder_dataset = dset.ImageFolder(root=Config.training_dir)","9a9085df":"siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,\n                                        transform=transforms.Compose([transforms.Resize((100,100)),\n                                                                      transforms.ToTensor()\n                                                                      ])\n                                       ,should_invert=False)","1a23f063":"vis_dataloader = DataLoader(siamese_dataset,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=8)\ndataiter = iter(vis_dataloader)\n\n\nexample_batch = next(dataiter)\nconcatenated = torch.cat((example_batch[0],example_batch[1]),0)\nimshow(torchvision.utils.make_grid(concatenated))\nprint(example_batch[2].numpy())","d8dc5fac":"class SiameseNetwork(nn.Module):\n    def __init__(self):\n        super(SiameseNetwork, self).__init__()\n        self.cnn1 = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(1, 4, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(4),\n            \n            nn.ReflectionPad2d(1),\n            nn.Conv2d(4, 8, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(8),\n\n\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(8, 8, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(8),\n\n\n        )\n\n        self.fc1 = nn.Sequential(\n            nn.Linear(8*100*100, 500),\n            nn.ReLU(inplace=True),\n\n            nn.Linear(500, 500),\n            nn.ReLU(inplace=True),\n\n            nn.Linear(500, 5))\n\n    def forward_once(self, x):\n        output = self.cnn1(x)\n        output = output.view(output.size()[0], -1)\n        output = self.fc1(output)\n        return output\n\n    def forward(self, input1, input2):\n        output1 = self.forward_once(input1)\n        output2 = self.forward_once(input2)\n        return output1, output2","5c34bb86":"class ContrastiveLoss(torch.nn.Module):\n    \"\"\"\n    Contrastive loss function.\n    Based on: http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/hadsell-chopra-lecun-06.pdf\n    \"\"\"\n\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, output1, output2, label):\n        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n\n\n        return loss_contrastive","d76e5d8b":"train_dataloader = DataLoader(siamese_dataset,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=Config.train_batch_size)","51b1bf7c":"net = SiameseNetwork().cuda()\ncriterion = ContrastiveLoss()\noptimizer = optim.Adam(net.parameters(),lr = 0.0005 )","027d71c4":"counter = []\nloss_history = [] \niteration_number= 0","e1b1679e":"for epoch in range(0,Config.train_number_epochs):\n    for i, data in enumerate(train_dataloader,0):\n        img0, img1 , label = data\n        img0, img1 , label = img0.cuda(), img1.cuda() , label.cuda()\n        optimizer.zero_grad()\n        output1,output2 = net(img0,img1)\n        loss_contrastive = criterion(output1,output2,label)\n        loss_contrastive.backward()\n        optimizer.step()\n        if i %10 == 0 :\n            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.item()))\n            iteration_number +=10\n            counter.append(iteration_number)\n            loss_history.append(loss_contrastive.item())\nshow_plot(counter,loss_history)","50df1603":"folder_dataset_test = dset.ImageFolder(root=Config.testing_dir)\nsiamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,\n                                        transform=transforms.Compose([transforms.Resize((100,100)),\n                                                                      transforms.ToTensor()\n                                                                      ])\n                                       ,should_invert=False)\n\ntest_dataloader = DataLoader(siamese_dataset,num_workers=6,batch_size=1,shuffle=True)\ndataiter = iter(test_dataloader)\nx0,_,_ = next(dataiter)\n\nfor i in range(10):\n    _,x1,label2 = next(dataiter)\n    concatenated = torch.cat((x0,x1),0)\n    \n    output1,output2 = net(Variable(x0).cuda(),Variable(x1).cuda())\n    euclidean_distance = F.pairwise_distance(output1, output2)\n    imshow(torchvision.utils.make_grid(concatenated),'Dissimilarity: {:.2f}'.format(euclidean_distance.item()))\n\n","d69cf5cb":"## Configuration Class\nA simple class to manage configuration","11ada619":"## Custom Dataset Class\nThis dataset generates a pair of images. 0 for geniune pair and 1 for imposter pair","9a50afea":"## Helper functions\nSet of helper functions","6101bb51":"# One Shot Learning with Siamese Networks\n\nThis is the jupyter notebook that accompanies","1f7c28e0":"## Contrastive Loss","a9fa4ba6":"## Visualising some of the data\nThe top row and the bottom row of any column is one pair. The 0s and 1s correspond to the column of the image.\n1 indiciates dissimilar, and 0 indicates similar.","7cff8375":"## Training Time!","f4baeeb0":"## Using Image Folder Dataset","da6b42cd":"## Neural Net Definition\nWe will use a standard convolutional neural network","a2c438d2":"## Imports\nAll the imports are defined here","44d1fca5":"## Some simple testing\nThe last 3 subjects were held out from the training, and will be used to test. The Distance between each image pair denotes the degree of similarity the model found between the two images. Less means it found more similar, while higher values indicate it found them to be dissimilar."}}