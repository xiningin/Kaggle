{"cell_type":{"9e9ccd59":"code","1d653bbb":"code","fb011eb5":"code","060c2b8d":"code","43a6209e":"code","e98f5cab":"code","983d0448":"code","6b318de6":"code","24ebc0b5":"code","5e90a87f":"code","9231fe73":"code","8e583099":"code","737e7360":"code","0efedec9":"code","b640af08":"code","6708f5d2":"markdown","03db52b8":"markdown","b386c7a4":"markdown","89e4464f":"markdown","9e19590a":"markdown","856a8b63":"markdown","f77ddfa8":"markdown","c1cdb7d0":"markdown","89f16621":"markdown"},"source":{"9e9ccd59":"from pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport random\nimport time\nimport os\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nfrom scipy.optimize import minimize\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as ctb\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.linear_model import LinearRegression\n\nfrom scipy.optimize import minimize\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.options.display.max_rows = 120\npd.options.display.max_columns = 100\n\nimport warnings\nwarnings.simplefilter('ignore')","1d653bbb":"N_SPLITS = 10\nN_ESTIMATORS = 10000\nEARLY_STOPPING_ROUNDS = 200\nVERBOSE = 1000\nSEED = 299792458","fb011eb5":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(SEED)","060c2b8d":"INPUT = Path(\"..\/input\/tabular-playground-series-aug-2021\")\nINPUT_PRED = Path(\"..\/input\/tps08-pred\")\n\ntrain = pd.read_csv(INPUT \/ \"train.csv\")\ntrain['pred'] = np.load(INPUT_PRED \/ \"oof.npy\")\n\ntest = pd.read_csv(INPUT \/ \"test.csv\")\ntest['pred'] = np.load(INPUT_PRED \/ \"pred.npy\")\n\nsubmission = pd.read_csv(INPUT \/ \"sample_submission.csv\")\n\nscale_features = [col for col in test.columns if 'f' in col]\nfeatures = scale_features + ['pred']\ntarget = 'loss'","43a6209e":"mul_list = [('f13', 'f31'),\n            ('f13', 'f46'),\n            ('f13', 'f74'),\n            ('f25', 'f34'),\n            ('f25', 'f58'),\n            ('f25', 'f63'),\n            ('f25', 'f66'),\n            ('f25', 'f73'),\n            ('f25', 'f96')\n           ]\n\ndiv_list = [('f13', 'f58'),\n            ('f25', 'f9'),\n            ('f25', 'f12'),\n            ('f25', 'f30'),\n            ('f25', 'f46'),\n            ('f25', 'f64'),\n            ('f25', 'f78'),\n            ('f25', 'f79'),\n            ('f25', 'f84')\n           ]\n\nfor cols in mul_list:\n    train[f'mul_{cols[0]}_{cols[1]}'] = train[cols[0]] * train[cols[1]]\n    test[f'mul_{cols[0]}_{cols[1]}'] = test[cols[0]] * test[cols[1]]\n    \nfor cols in div_list:\n    train[f'div_{cols[0]}_{cols[1]}'] = train[cols[0]] \/ train[cols[1]]\n    test[f'div_{cols[0]}_{cols[1]}'] = test[cols[0]] \/ test[cols[1]]\n    \nfe_features = [col for col in test.columns if ('mul_' in col) or ('div_' in col)]\n\nscale_features += fe_features\nfeatures = scale_features + ['pred']","e98f5cab":"ss = StandardScaler()\ntrain[scale_features] = ss.fit_transform(train[scale_features])\ntest[scale_features] = ss.transform(test[scale_features])","983d0448":"train.shape, test.shape","6b318de6":"lgb_params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'n_estimators': N_ESTIMATORS,\n    'random_state': SEED,\n    'learning_rate': 5e-3,\n    'subsample': 0.8,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.6,\n    'reg_alpha': 6.4,\n    'reg_lambda': 1.8,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'importance_type': 'gain',\n    'device': 'gpu',\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0\n}\n\nxgb_params = {\n    'objective': 'reg:squarederror',\n    'learning_rate': 5e-3,\n    'seed': SEED,\n    'subsample': 0.8,\n    'colsample_bytree': 0.6,\n    'n_estimators': N_ESTIMATORS,\n    'max_depth': 11,\n    'alpha': 20,\n    'lambda': 9,\n    'min_child_weight': 256,\n    'importance_type': 'total_gain',\n    'tree_method': 'gpu_hist'\n}\n\nctb_params = {\n    'bootstrap_type': 'Poisson',\n    'loss_function': 'RMSE',\n    'eval_metric': 'RMSE',\n    'random_seed': SEED,\n    'task_type': 'GPU',\n    'max_depth': 8,\n    'learning_rate': 5e-3,\n    'n_estimators': N_ESTIMATORS,\n    'max_bin': 280,\n    'min_data_in_leaf': 64,\n    'l2_leaf_reg': 0.01,\n    'subsample': 0.8\n}","24ebc0b5":"mlp_oof = np.zeros(train.shape[0])\nlgb_oof = np.zeros(train.shape[0])\nxgb_oof = np.zeros(train.shape[0])\nctb_oof = np.zeros(train.shape[0])\n\nmlp_pred = np.zeros(test.shape[0])\nlgb_pred = np.zeros(test.shape[0])\nxgb_pred = np.zeros(test.shape[0])\nctb_pred = np.zeros(test.shape[0])\n\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X=train[features], y=train[target])):\n    print(f\"===== fold {fold} =====\")\n    X_train, y_train = train[features].iloc[trn_idx], train[target].iloc[trn_idx]\n    X_valid, y_valid = train[features].iloc[val_idx], train[target].iloc[val_idx]\n    X_test = test[features]\n    \n    start = time.time()\n    model = MLPRegressor(\n        hidden_layer_sizes=50,\n        early_stopping=True,\n        n_iter_no_change=100,\n        solver='adam',\n        shuffle=True,\n        random_state=SEED\n    )\n    model.fit(X_train,y_train)\n\n    mlp_oof[val_idx] = model.predict(X_valid)\n    mlp_pred += model.predict(X_test) \/ N_SPLITS\n    \n    elapsed = time.time() - start\n    \n    rmse = mean_squared_error(y_valid, mlp_oof[val_idx], squared=False)\n    print(f\"fold {fold} - mlp rmse: {rmse:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n    \n    start = time.time()\n    model = lgb.LGBMRegressor(**lgb_params)\n    model.fit(\n        X_train, \n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        eval_metric='rmse',\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n        verbose=VERBOSE,\n    )\n    \n    lgb_oof[val_idx] = model.predict(X_valid)\n    lgb_pred += model.predict(X_test) \/ N_SPLITS\n    \n    elapsed = time.time() - start\n    rmse = mean_squared_error(y_valid, lgb_oof[val_idx], squared=False)\n    print(f\"fold {fold} - lgb rmse: {rmse:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n\n    start = time.time()\n    model = xgb.XGBRegressor(**xgb_params)\n    model.fit(\n        X_train, \n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        eval_metric='rmse',\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n        verbose=VERBOSE\n    )\n\n    xgb_oof[val_idx] = model.predict(X_valid)\n    xgb_pred += model.predict(X_test) \/ N_SPLITS\n\n    elapsed = time.time() - start\n    rmse = mean_squared_error(y_valid, xgb_oof[val_idx], squared=False)\n    print(f\"fold {fold} - xgb rmse: {rmse:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n\n    start = time.time()\n    model = ctb.CatBoostRegressor(**ctb_params)\n    model.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        use_best_model=True,\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n        verbose=VERBOSE\n    )\n\n    ctb_oof[val_idx] = model.predict(X_valid)\n    ctb_pred += model.predict(X_test) \/ N_SPLITS\n\n    elapsed = time.time() - start\n    rmse = mean_squared_error(y_valid, ctb_oof[val_idx], squared=False)\n    print(f\"fold {fold} - ctb rmse: {rmse:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n        \nprint(f\"oof mlp_rmse = {mean_squared_error(train[target], mlp_oof, squared=False)}\")\nprint(f\"oof lgb_rmse = {mean_squared_error(train[target], lgb_oof, squared=False)}\")\nprint(f\"oof xgb_rmse = {mean_squared_error(train[target], xgb_oof, squared=False)}\")\nprint(f\"oof ctb_rmse = {mean_squared_error(train[target], ctb_oof, squared=False)}\")\n\nnp.save(\"mlp_oof.npy\", mlp_oof)\nnp.save(\"mlp_pred.npy\", mlp_pred)\nnp.save(\"lgb_oof.npy\", lgb_oof)\nnp.save(\"lgb_pred.npy\", lgb_pred)\nnp.save(\"xgb_oof.npy\", xgb_oof)\nnp.save(\"xgb_pred.npy\", xgb_pred)\nnp.save(\"ctb_oof.npy\", ctb_oof)\nnp.save(\"ctb_pred.npy\", ctb_pred)","5e90a87f":"def class_optimizer(X, a0, a1, a2, a3):\n    oof = X[0]*a0 + X[1]*a1 + X[2]*a2 + (1-X[0]-X[1]-X[2])*a3\n    return mean_squared_error(train[target], oof, squared=False)\n\nres = minimize(\n    fun=class_optimizer,\n    x0=[0.1, 0.3, 0.3],\n    args=tuple([mlp_oof, lgb_oof, xgb_oof, ctb_oof]),\n    method='Nelder-Mead',\n    options={'maxiter': 300})\n\nprint(res)\nprint(f\"coef0 {res.x[0]}, coef1 {res.x[1]}, coef2 {res.x[2]}, coef3 {1-res.x[0]-res.x[1]-res.x[2]}\")","9231fe73":"ensemble_oof = res.x[0] * mlp_oof + res.x[1] * lgb_oof + res.x[2] * xgb_oof + (1-res.x[0]-res.x[1]-res.x[2]) * ctb_oof\nensemble_pred = res.x[0] * mlp_pred + res.x[1] * lgb_pred + res.x[2] * xgb_pred + (1-res.x[0]-res.x[1]-res.x[2]) * ctb_pred","8e583099":"train_npy = np.concatenate([mlp_oof.reshape(-1, 1),\n                            lgb_oof.reshape(-1, 1),\n                            xgb_oof.reshape(-1, 1),\n                            ctb_oof.reshape(-1, 1),\n                            ensemble_oof.reshape(-1, 1)], axis=1)\ny_npy = train[target].to_numpy()\ntest_npy = np.concatenate([mlp_pred.reshape(-1, 1),\n                           lgb_pred.reshape(-1, 1),\n                           xgb_pred.reshape(-1, 1),\n                           ctb_pred.reshape(-1, 1),\n                           ensemble_pred.reshape(-1, 1)], axis=1)","737e7360":"stack_oof = np.zeros(train.shape[0])\nstack_pred = np.zeros(test.shape[0])\n\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X=train_npy, y=y_npy)):\n    print(f\"===== fold {fold} =====\")\n    X_train, y_train = train_npy[trn_idx], y_npy[trn_idx]\n    X_valid, y_valid = train_npy[val_idx], y_npy[val_idx]\n    X_test = test_npy\n\n    start = time.time()\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    with open(f\"stacking_model{fold}.pkl\", 'wb') as f:\n        pickle.dump(model, f)\n    \n    stack_oof[val_idx] = model.predict(X_valid)\n    stack_pred += model.predict(X_test) \/ N_SPLITS\n    \n    elapsed = time.time() - start\n    rmse = mean_squared_error(y_valid, stack_oof[val_idx], squared=False)\n    print(f\"fold {fold} - stack rmse: {rmse:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n        \nprint(f\"oof stack_rmse = {mean_squared_error(train[target], stack_oof, squared=False)}\")\n\nnp.save(\"stack_oof.npy\", stack_oof)\nnp.save(\"stack_pred.npy\", stack_pred)","0efedec9":"best_pred = np.load(INPUT_PRED \/ \"best_pred.npy\")\n\nfinal_pred = 0.7 * best_pred + 0.3 * stack_pred","b640af08":"submission['loss'] = final_pred\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission","6708f5d2":"## Ensemble","03db52b8":"# Submission\n---","b386c7a4":"## Blending","89e4464f":"# Libraries\n---","9e19590a":"# Post process\n---","856a8b63":"# LGBM\/XGB\/CatBoost\n---","f77ddfa8":"## Stacking","c1cdb7d0":"# Datasets\n---","89f16621":"# Parameters\n---"}}