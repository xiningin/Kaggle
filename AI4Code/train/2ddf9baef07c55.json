{"cell_type":{"bd0a6f83":"code","10314e89":"code","d7af6ac4":"code","caf7da45":"code","abb18abe":"code","4f46a1b9":"code","572b1727":"code","4675edb3":"code","8d63bdce":"code","b958ce46":"code","b3b9f68a":"code","c8d671da":"code","735a2a0f":"code","acf06641":"code","e91e9c30":"code","9af46ccb":"code","379123f9":"code","dad1a3f3":"code","b4284a6b":"code","999982c2":"code","9a6024fd":"code","784db2a1":"code","13a69af7":"code","d7de05f5":"code","3fdd93a2":"code","c9c7cada":"code","57fa1003":"code","5899770e":"code","862cbb14":"code","1fbd856f":"code","3da30e2a":"code","3c558498":"markdown","56811b03":"markdown","15c4b525":"markdown","7b9ac056":"markdown","bc4f66f2":"markdown","96189fea":"markdown","020f5758":"markdown","b7e6d291":"markdown","40b0afbe":"markdown","7d4a06dd":"markdown","bdf81071":"markdown","256864da":"markdown","2933db59":"markdown","52cbcdc8":"markdown","9d80c6b1":"markdown","b9c5c8e7":"markdown","46cf3934":"markdown","120157bf":"markdown","1dc2b71b":"markdown","e6c7d6f5":"markdown","76a90472":"markdown","af438784":"markdown","b9800c4a":"markdown","ea78990c":"markdown","f9f09525":"markdown","b35f8b69":"markdown","be7866dd":"markdown","d9e1b280":"markdown","c58be117":"markdown","e5ceed84":"markdown","de91e13f":"markdown","05cbe7cd":"markdown","62462d25":"markdown","c3c4c104":"markdown","b0b4cdda":"markdown","5f0929a6":"markdown","15efb90e":"markdown","91f3a210":"markdown","4bc80a0d":"markdown","d6714d15":"markdown","e16c0398":"markdown","c0e96b1f":"markdown","8572efae":"markdown","bbb662ae":"markdown","f3c7d01f":"markdown","b88e1a8e":"markdown","7a6b6998":"markdown","884f5996":"markdown","0bdc60c1":"markdown","dcec2ce1":"markdown","e892942e":"markdown","9e178544":"markdown","6ef9bcec":"markdown"},"source":{"bd0a6f83":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\npd.set_option('display.max_columns', None) #This displays all the columns instead of the default 20 ","10314e89":"# train.csv contains the predictor variables and target variable, Attrition \ntrain_df = pd.read_csv('https:\/\/raw.githubusercontent.com\/thepankj\/IIT-G-Summer-Analytics-2020\/master\/train.csv')\n\ntrain_df.head()","d7af6ac4":"train_df.shape","caf7da45":"train_df.dtypes.sort_values()","abb18abe":"train_df.describe(include = 'int').T","4f46a1b9":"train_df.describe(include = 'object').T","572b1727":"no_unique_values = train_df.nunique().sort_values()\nno_unique_values","4675edb3":"#This checks for missing values \nsns.heatmap(train_df.isnull());","8d63bdce":"plt.figure(figsize = (5,5))\nsns.countplot(train_df.Attrition);","b958ce46":"train_features = train_df.drop(['Id','Attrition', 'Behaviour'], axis = 1)\ntrain_features.hist(figsize = (20,20), grid = False);","b3b9f68a":"train_categorical = train_features.select_dtypes('object')\nplt.figure(figsize = (20, 20))\nfor _ in range(train_categorical.shape[1]):\n    x = train_categorical.iloc[:, _]\n    plt.subplot(7, 4, _+1)\n    plt.subplots_adjust(hspace=2)\n    plt.xticks(rotation=60)\n    sns.countplot(x)","c8d671da":"nom_catg_col = list(train_df.select_dtypes(['object']).columns)\nnom_catg_col","735a2a0f":"ord_catg_col = [\"Education\", \"EnvironmentSatisfaction\", \"JobInvolvement\",\"JobSatisfaction\",\n                    \"PerformanceRating\",\"StockOptionLevel\",\"CommunicationSkill\", \"Behaviour\"]\nord_catg_col","acf06641":"catg_col_list = nom_catg_col + ord_catg_col\ncatg_col_df = train_df[catg_col_list]\ncatg_col_df.head()","e91e9c30":"df_hot_enc_catg = pd.get_dummies(catg_col_df, columns = nom_catg_col, drop_first = True) #one hot encoding the nominal variables\ndf_hot_enc_catg.head()","9af46ccb":"num_col_list = list(set(train_df.columns)-set(catg_col_list)-{'Attrition'})\nnum_col_df = train_df[num_col_list]\nnum_col_df.head()","379123f9":"from sklearn.preprocessing import MinMaxScaler\nscalar = MinMaxScaler(feature_range=(0, 1))\ndf_min_max_num = pd.DataFrame(scalar.fit_transform(num_col_df), columns = num_col_df.columns)\ndf_min_max_num.head()","dad1a3f3":"standar_df = pd.concat([df_hot_enc_catg, df_min_max_num], axis=1)\nstandar_df.head()","b4284a6b":"standar_df.shape","999982c2":"from sklearn.ensemble import ExtraTreesClassifier\n\nfeatures = ExtraTreesClassifier()\nfeatures.fit(standar_df, train_df.Attrition)\n\nscore = features.feature_importances_\nscore","9a6024fd":"feature_scores = pd.Series(score, index = standar_df.columns).sort_values(ascending = False)\nplt.figure(figsize=(10,20))\nfeature_scores.plot(kind='barh')\nplt.show()","784db2a1":"feature_scores","13a69af7":"best_features = list(feature_scores.index[0:30])\nbest_features","d7de05f5":"standar_df[best_features].apply(lambda x: x.corr(train_df.Attrition)).sort_values()","3fdd93a2":"top_feat_df = standar_df[best_features].drop(['Id'], axis = 1)\ntop_feat_df.head()","c9c7cada":"X = top_feat_df\ny = train_df.Attrition","57fa1003":"#Splitting the dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=4)","5899770e":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score","862cbb14":"from sklearn.linear_model import LogisticRegression\nlogModel = LogisticRegression()\nparam_dist = {'C':list(range(1, 5)),\n              'random_state':list(range(0,5)),\n              'max_iter':[50, 100, 150, 200, 250]}\nlog_search = RandomizedSearchCV(logModel, param_distributions=param_dist, n_iter=50)\nlog_search.fit(X_train, y_train)","1fbd856f":"pred = log_search.predict(X_val)\naccuracy_score(y_val, pred)","3da30e2a":"from sklearn.svm import SVC\nsvcModel = SVC(gamma=\"auto\")\nsvcModel.fit(X_train, y_train)\npred = svcModel.predict(X_val)\naccuracy_score(y_val, pred)","3c558498":"Let's look at the datatypes of the various attributes.","56811b03":"Before looking into the data, I've imported some libraries, which will be helpful for the exploration.","15c4b525":"<hr>","7b9ac056":"### Using Extra Tree Classifier","bc4f66f2":"## Importing Dataset","96189fea":"7 out of 29 attributes are 'object' type. We need to convert them to integer type to feed in our ML algos.","020f5758":"**Numerical Data**","b7e6d291":"<hr>","40b0afbe":"## EDA and Feature Engineering","7d4a06dd":"From the above graphs, we can see that some features like DistanceFromHome and MonthlyIncome are highly skewed. We will take care of these later.","bdf81071":"<hr>","256864da":"Next, let's look at the number of unique values in each of the columns (attributes)","2933db59":"# Logistic Regression","52cbcdc8":"Checking the number of rows and columns","9d80c6b1":"### Data Description","b9c5c8e7":"Finding the correlation b\/w the variables. Continuous-continuous, categorical-categorical, continuous-categorical. We need to scale the continuous variables before calculating the correlations.","46cf3934":"## One Hot Encoding for the Nominal Variables","120157bf":"Select the top 30 features","1dc2b71b":"<hr>","e6c7d6f5":"Next the dataset is Standardized using MinMaxScalar ","76a90472":"A descriiption of the 'int' and 'object' type of attributes ","af438784":"# \nTo check for skewness in the data and apply the required transformation, I plotted the graph of the original data along with Square Root transformed and Log Transformed","b9800c4a":"# Feature Selection","ea78990c":"# Model Building","f9f09525":"<hr>","b35f8b69":"**Categorical Data**","be7866dd":"# \nThe first model I tried is Logistic Regression. I used RandomizedSearchCV to get the best parameters. The accuracy score is around 77%.","d9e1b280":"By looking at the above we can know that 'Behaviour' and 'Id' are redundant. Behaviour has a constant value and Id is, well just Id which is unique for all rows. So these two columns can be dropped.","c58be117":"\n## Importing Required libraries","e5ceed84":"The dataset is almost balanced","de91e13f":"### Correlation between the target and features","05cbe7cd":"## Scaling the Numerical Variables","62462d25":"## Taking care of the skewed features","c3c4c104":"**Labels**","b0b4cdda":"<hr>","5f0929a6":"I left the features as selected above since the correlation between the features and target variable is above 0.01. \nIt's surprising that for Id the correlation is >0.69. Still, it should be dropped.","15efb90e":"<hr>","91f3a210":"I have uploaded the train and test datasets on Github. The target variable is separated from the train set.","4bc80a0d":"#### Separating the Nominal and Ordinal Categorical features from the dataset","d6714d15":"<hr>","e16c0398":"We see that JobRole, Gender, OverTime, EducationField, Department, BusinessTravel, MaritalStatus are categorical variables. And the rest are numeric. It's better to One Hot Encode the categorical variables.","c0e96b1f":"The above plot shows that there are no missing values in our dataset.","8572efae":"The nominal variables, which are 'BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime' are One Hot Encoded.","bbb662ae":"**Checking for null values**\n\nNext, inorder to check for missing values (if any), I plotted the Heatmap of null values.","f3c7d01f":"# Employee Attrition Prediction\n\n\nThis project was a part of IIT Guwahati's Summer Analytics program which concluded on June 25, 2020. Apart from learning a about Meachine Learning and some Deep Learning, I learnt about procrastination.\n\nYou see, I have been meaning to put up this project since a month. But as Mark Twain said -\n    'Never put off till tomorrow what may be done day after tomorrow just as well.'\nand I followed it.\nUntil today, when I wresteled myself to finalise this project. \n\nWell enought of digression and let's dive into the project.\n\nThe aim of this project is to predict Employee Attrition given some attributes (duh!). For the uninitiated, employee attrition is 'the natural process by which employees leave the workforce for example, through resignation for personal reasons or retirement and are not immediately replaced.'","b88e1a8e":"<hr>","7a6b6998":"<hr>","884f5996":"<hr>","0bdc60c1":"**Checking for imbalance in the dataset attributes**","dcec2ce1":"# SVC","e892942e":"<hr>","9e178544":"## Taking Care of the Categorical and Numerical variables","6ef9bcec":"## Final dataset after One Hot Encoding and Standardization"}}