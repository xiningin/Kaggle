{"cell_type":{"8b311134":"code","ef418703":"code","5e1f4a0d":"code","3e45695d":"code","6697ca2b":"code","fb5099d8":"code","05e25b39":"code","7de7344a":"code","a60dce22":"code","edbc7dbd":"code","dff6bbee":"code","0f0050ec":"code","37dac7f9":"code","de1c55b6":"code","5ae8fe82":"code","f2a1f2dc":"code","2102481a":"code","3032ceba":"code","4e2a13af":"code","6f3eeaa0":"code","bfe4fdbb":"code","c038a9b5":"code","7bb90502":"code","2a0ad9dc":"code","2fbe9ff2":"markdown","507ea33b":"markdown","c36fe61a":"markdown","4ccc2d71":"markdown","ec076fdf":"markdown","2e15b8d7":"markdown","6a51bf96":"markdown","9daa02db":"markdown","c7f82049":"markdown","6d4a579f":"markdown","363ec01e":"markdown","48c51640":"markdown","efec21d1":"markdown","07d3038a":"markdown","cbe01bbf":"markdown","82aa8a91":"markdown","597f7148":"markdown","c18a7d8b":"markdown","0d1c34dd":"markdown","2eb14f56":"markdown","3d102e18":"markdown","79c2609f":"markdown","6deb64bd":"markdown","94e2c865":"markdown","ac82b2af":"markdown"},"source":{"8b311134":"import pandas as pd\n#import autosklearn.classification\nimport featuretools as ft\nfrom featuretools.primitives import *\nfrom featuretools.variable_types import Numeric\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\nimport os\nprint(os.listdir(\"..\/input\"))","ef418703":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nanswers = pd.read_csv('..\/input\/gender_submission.csv')","5e1f4a0d":"print(train_df.columns.values)","3e45695d":"combine = train_df.append(test_df)\n\npassenger_id=test_df['PassengerId']\n#combine.drop(['PassengerId'], axis=1, inplace=True)\ncombine = combine.drop(['Ticket', 'Cabin'], axis=1)\n\ncombine.Fare.fillna(combine.Fare.mean(), inplace=True)\n\ncombine['Sex'] = combine.Sex.apply(lambda x: 0 if x == \"female\" else 1)\n\nfor name_string in combine['Name']:\n    combine['Title']=combine['Name'].str.extract('([A-Za-z]+)\\.',expand=True)\n    \n#replacing the rare title with more common one.\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\ncombine.replace({'Title': mapping}, inplace=True)\n\ncombine = combine.drop(['Name'], axis=1)\n\ntitles=['Mr','Miss','Mrs','Master','Rev','Dr']\nfor title in titles:\n    age_to_impute = combine.groupby('Title')['Age'].median()[titles.index(title)]\n    combine.loc[(combine['Age'].isnull()) & (combine['Title'] == title), 'Age'] = age_to_impute\ncombine.isnull().sum()\n\nfreq_port = train_df.Embarked.dropna().mode()[0]\ncombine['Embarked'] = combine['Embarked'].fillna(freq_port)\n    \ncombine['Embarked'] = combine['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ncombine['Title'] = combine['Title'].map( {'Mr': 0, 'Mrs': 1, 'Miss': 2, 'Master': 3, 'Rev': 4, 'Dr': 5} ).astype(int)\ncombine.fillna(0, inplace=True)","6697ca2b":"combine.info()","fb5099d8":"es = ft.EntitySet(id = 'titanic_data')\n\nes = es.entity_from_dataframe(entity_id = 'combine', dataframe = combine.drop(['Survived'], axis=1), \n                              variable_types = \n                              {\n                                  'Embarked': ft.variable_types.Categorical,\n                                  'Sex': ft.variable_types.Boolean,\n                                  'Title': ft.variable_types.Categorical\n                              },\n                              index = 'PassengerId')\n\nes","05e25b39":"es = es.normalize_entity(base_entity_id='combine', new_entity_id='Embarked', index='Embarked')\nes = es.normalize_entity(base_entity_id='combine', new_entity_id='Sex', index='Sex')\nes = es.normalize_entity(base_entity_id='combine', new_entity_id='Title', index='Title')\nes = es.normalize_entity(base_entity_id='combine', new_entity_id='Pclass', index='Pclass')\nes = es.normalize_entity(base_entity_id='combine', new_entity_id='Parch', index='Parch')\nes = es.normalize_entity(base_entity_id='combine', new_entity_id='SibSp', index='SibSp')\nes","7de7344a":"primitives = ft.list_primitives()\npd.options.display.max_colwidth = 100\nprimitives[primitives['type'] == 'aggregation'].head(primitives[primitives['type'] == 'aggregation'].shape[0])","a60dce22":"primitives[primitives['type'] == 'transform'].head(primitives[primitives['type'] == 'transform'].shape[0])","edbc7dbd":"features, feature_names = ft.dfs(entityset = es, \n                                 target_entity = 'combine', \n                                 max_depth = 2)","dff6bbee":"feature_names","0f0050ec":"len(feature_names)","37dac7f9":"features[features['Age'] == 22][[\"Title.SUM(combine.Age)\",\"Age\",\"Title\"]].head()","de1c55b6":"# Threshold for removing correlated variables\nthreshold = 0.95\n\n# Absolute value correlation matrix\ncorr_matrix = features.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head(50)","5ae8fe82":"# Select columns with correlations above threshold\ncollinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d features to remove.' % (len(collinear_features)))","f2a1f2dc":"features_filtered = features.drop(columns = collinear_features)\n\nprint('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])","2102481a":"features_positive = features_filtered.loc[:, features_filtered.ge(0).all()]\n\ntrain_X = features_positive[:train_df.shape[0]]\ntrain_y = train_df['Survived']\n\ntest_X = features_positive[train_df.shape[0]:]","3032ceba":"lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(train_X, train_y)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(train_X)\nX_selected_df = pd.DataFrame(X_new, columns=[train_X.columns[i] for i in range(len(train_X.columns)) if model.get_support()[i]])\nX_selected_df.shape","4e2a13af":"X_selected_df.columns","6f3eeaa0":"random_forest = RandomForestClassifier(n_estimators=2000,oob_score=True)\nrandom_forest.fit(X_selected_df, train_y)","bfe4fdbb":"X_selected_df.shape","c038a9b5":"Y_pred = random_forest.predict(test_X[X_selected_df.columns])","7bb90502":"print(Y_pred)","2a0ad9dc":"my_submission = pd.DataFrame({'PassengerId': passenger_id, 'Survived': Y_pred})\nmy_submission.to_csv('submission.csv', index=False)","2fbe9ff2":"## 1. Introduction <a name=\"introduction\"><\/a>","507ea33b":"Finally, we will create a basic random forest classifier with 2000 estimators. Please notice that I skip essential steps such as crossvalidation, the analysis of learning curves, etc.","c36fe61a":"If you have ever manually created hundreds of features for your ML project (I am sure you did it), then you will be happy to find out how the Python package called \"featuretools\" can help out with this task. The good news is that this package is very easy to use. It is aimed at automated feature engineering. **Of course human expertise cannot be substituted**, but nevertheless **\"featuretools\" can automate a large amount of routine work**. For the exploration purpose I will use a well-known Titanic dataset. The achieved resuls will be compared to the results obtained with handcrafted feature engineering and manual hyperparameter optimisation.\n\nThe main takeaways from this notebook are:\n\n* Firstly, going from 11 total features to 146 using automated feature engineering (\"featuretools\" package).\n* Secondly, applying the feature reduction and selection methods to select X most relevant features out of 146 features.\n* The accuracy of 0.74162 on the public leaderboard with a basic random forest classifier.\n\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated.**","4ccc2d71":"By using \"featuretools\", we were able to **generate 146 features just in a moment**.\n\nThe \"featuretools\" is a powerful package that allows saving time to create new features from multiple tables of data. However, it does not completely subsitute the human domain knowledge. Additionally, now we are facing another problem known as the \"curse of dimensionality\".","ec076fdf":"Once the data is cleaned, we can proceed to the cake in our party - i.e. automated feature engineering. To work with \"featuretools\" package, we should specify our dataframes \"train_df\" and \"test_df\" as entities of the entity set. The entity is just a table with a uniquely identifying column known as an index. The \"featuretools\" can automatically infer the variable types (numeric, categorical, datetime) of the columns, but it's a good idea to also pass in specific datatypes to override this behavior.","2e15b8d7":"## 4. Perform automated feature engineering <a name=\"afe\"><\/a> ","6a51bf96":"Collinearity means high intercorrelations among independent features. If we maintain such features in the mode, it might be difficult to assess the effect of independent features on target variable. Therefore we will detect these features and delete them, though applying a manual revision before removal.","9daa02db":"### 5.1 Determine collinear features","c7f82049":"This is a list of new features. For example, \"Title.SUM(combine.Age\" means the sum of Age values for each unique value of Title.","6d4a579f":"If we use 20 selected features, we get the accuracy of 0.74162 on public leaderboard. Indeed it is not high. However, if we compare it to the accuracy that we would receive with the same random forest classfier using 83 features from \"features_positive\", then we would get the accuracy of 0.73205.\n\nThe recommended approach is to combine outputs of \"featuretools\" with the human expert knowledge, and use crossvalidation in order to analyze learning curves and pick up the most efficient model.","363ec01e":"Since the number of features is smaller than the number of observations in \"train_X\", the parameter \"dual\" is equal to False.","48c51640":"## 2. Load data <a name=\"load_data\"><\/a>","efec21d1":"1. Now we will apply a **deep feature synthesis (dfs)** function that will generate new features by automatically applying suitable aggregations, I selected a depth of 2. Higher depth values will stack more primitives. ","07d3038a":"As we can see, the most of \"transformation\" functions are applied to datetime or time-dependent variables. In our dataset we do not have such variables. Therefore these functions will not be used.","cbe01bbf":"**Be aware, however, that it is not a good idea to remove features only by correlation without understanding the removal process**. Features that have very high correlation (for example, Embarked.SUM(combine.Age) and Embarked.SUM(combine.Fare)) with significant difference between may require additional inve. Therefore manual guidance is necessary. But this topic is outside of the scope of this Kernel.","82aa8a91":"The next step is to use linear models penalized with the L1 norml. ","597f7148":"### 5.2 Detect the most relevant features using linear models penalized with the L1 norm","c18a7d8b":"# Automated Feature Engineering and Selection for Titanic Dataset","0d1c34dd":"## 5. \"Curse of dimensionality\": Feature reduction and selection <a name=\"frs\"><\/a> ","2eb14f56":"***Liana Napalkova***\n\n***3 September 2018***","3d102e18":"To deal with the \"curse of dimensionality\", it's necessary to apply the feature reduction and selection, which means removing low-value features from the data. But keep in mind that feature selection can hurt the performance of ML models. The tricky thing is that the design of ML models contains an artistic component. It's definitely not the deterministic process with strict rules that should be followed to achieve success. In order to come up with an accurate model, it is necessary to apply, combine and compare dozens of methods. In this notebook, I will not explain all possible approaches to deal with the \"curse of dimensionality\". I will rather concentrate on the following methods:\n\n   * Determine collinear features\n   * Detect the most relevant features using linear models penalized with the L1 norm","79c2609f":"# Table of contents\n1. [Introduction](#introduction)\n2. [Load data](#load_data)\n3. [Clean data](#clean_data)\n4. [Automated feature engineering](#afe)\n5. [\"Curse of dimensionality\": Feature reduction and selection](#frs)\n6. [Training and testing the simple model](#ttm)","6deb64bd":"Once the entity set is created, it is possible to generate new features using so called **feature primitives**. A feature primitive is an operation applied to data to create a new feature. Simple calculations can be stacked on top of each other to create complex features. Feature primitives fall into two categories:\n\n* **Aggregation**: these functions group together child datapoints for each parent and then calculate a statistic such as mean, min, max, or standard deviation. The aggregation works across multiple tables using relationships between tables.\n* **Transformation**: these functions work on one or multiple columns of a single table.\n\nIn our case we do not have different tables linked between each other. However, we can create dummy tables using \"normalize_entity\" function. What will it give us? Well, this way we will be able to apply both aggregation and transformation functions to generate new features. To create such tables, we will use categorical, boolean and integer variables.","94e2c865":"## 6. Training and testing the simple model <a name=\"ttm\"><\/a> ","ac82b2af":"## 3. Clean data <a name=\"clean_data\"><\/a>\n\nFirst of all, it is necessary to clean the data. Since the main focus of this notebook is to explore \"featuretools\", we will not re-invent the wheel in data cleaning. Therefore, we will apply the code of feature cleaning taken from one of existing Kernels - [Best Titanic Survival Prediction for Beginners](https:\/\/www.kaggle.com\/vin1234\/best-titanic-survival-prediction-for-beginners), where an interested reader can find a very detailed explanation of each steps of the data cleaning procedure."}}