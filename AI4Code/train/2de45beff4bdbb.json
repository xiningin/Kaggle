{"cell_type":{"a66555cb":"code","20a5212d":"code","68204837":"code","8989aca8":"code","af925d45":"code","2832fb88":"code","d54201c8":"code","b7cb5164":"code","25475dcd":"code","530113be":"code","53c0303f":"code","652b89d3":"code","c566b6f9":"markdown","ee01684f":"markdown","f87cb5d8":"markdown","31bee8b2":"markdown","275b1871":"markdown","4a33a3e7":"markdown","4cdda606":"markdown","f91c784c":"markdown","a138ff78":"markdown","09e7f0d2":"markdown","f3f6f085":"markdown","f96e12bf":"markdown","266fd130":"markdown"},"source":{"a66555cb":"class Config:\n    vocab_size = 15000 # Vocabulary Size\n    sequence_length = 100 # Length of sequence\n    batch_size = 1024\n    embed_dim = 256\n    latent_dim = 256\nconfig = Config()","20a5212d":"import pandas as pd\nimport tensorflow as tf\nimport pathlib\nimport random\nimport string\nimport re\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import TweetTokenizer \nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom scipy.stats import rankdata\nimport json\nimport sys\nsys.setrecursionlimit(100000)","68204837":"class Tokenizer:\n    \n    stopwords = set([\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ])\n    \n    tweet_tokenizer = TweetTokenizer() \n    \n    stemmer = PorterStemmer()\n    \n    lemmatizer = WordNetLemmatizer()\n    \n    def __init__(self, vocab_size = None, oov_token = None, bos_token = None, eos_token = None, max_length = 10000):\n        self.vocab_size = vocab_size\n        self.oov_token = oov_token\n        self.max_length = max_length\n        self.bos_token = bos_token\n        self.eos_token = eos_token\n    \n    @staticmethod\n    def preprocess_string(text):\n        # Convert sentences to lowercase.\n        text = text.lower()\n        # Remove puntuations, but ? and ! are usually enmotional so I won't remove it.\n        text = re.sub(r'[\\n| |.|\\\"|,|:|\\(|\\)|#|\\'|\\{|\\}|\\*|\\\/|\\$|\\\u2014|~|;|=|\\[\uff5c\\]|\\-]+', \" \", text)\n        # Remove Digits\n        text = re.sub(\"[0-9]+\", \" \", text)\n        text = re.sub(\"[ ]+\", \" \", text)\n        text = text.strip(\" \")\n        # Convert sentences to tokens\n        items = Tokenizer.tweet_tokenizer.tokenize(text)\n        # Remove stop words\n        new_items = []\n        for item in items:\n            if item not in Tokenizer.stopwords:\n                new_item = Tokenizer.lemmatizer.lemmatize(item)\n                new_item = Tokenizer.stemmer.stem(new_item)\n                new_items.append(new_item)\n        return new_items\n        \n    def fit_transform(self, texts):\n        current_index = 1\n        word_index = {self.oov_token: current_index}\n        if self.bos_token != None:\n            current_index += 1\n            word_index[self.bos_token] = current_index\n        if self.eos_token != None:\n            current_index += 1\n            word_index[self.eos_token] = current_index\n        word_count = {}\n        for i in range(len(texts)):\n            text = texts[i]\n            for item in text:\n                if item in word_count:\n                    word_count[item] += 1\n                else:\n                    word_count[item] = 1\n        word_count_df = pd.DataFrame({\"key\": word_count.keys(), \"count\": word_count.values()})\n        word_count_df.sort_values(by=\"count\", ascending=False, inplace=True)\n        vocab = list(word_index.keys())\n        vocab += list(word_count_df[\"key\"][0: self.vocab_size - len(word_index)])\n        vocab = set(vocab)\n        self.vocab = vocab\n        \n        sentences = []\n        offset = 1 if self.eos_token != None else 0\n        for i in range(len(texts)):\n            text = texts[i]\n            sentence = []\n            if self.bos_token != None:\n                sentence.append(word_index[self.bos_token])\n            for item in text:\n                if item in self.vocab:\n                    if item in word_index:\n                        sentence.append(word_index[item])\n                    else:\n                        current_index += 1\n                        word_index[item] = current_index\n                        sentence.append(word_index[item])\n                else:\n                    sentence.append(word_index[self.oov_token])\n            if len(sentence) <= self.max_length - offset:\n                if self.eos_token != None:\n                    sentence.append(word_index[self.eos_token])\n                sentence += [0] * (self.max_length - len(sentence))\n            elif len(sentence) > self.max_length - offset:\n                sentence = sentence[:self.max_length - offset]\n                if self.eos_token != None:\n                    sentence.append(word_index[self.eos_token])\n            sentences.append(sentence)\n        self.word_index = word_index\n        self.index_word = dict({word_index[key]: key for key in word_index.keys()})\n        return sentences\n    \n    def save(self, path):\n        dic = {\n            \"vocab_size\": self.vocab_size,\n            \"oov_token\": self.oov_token,\n            \"max_length\":  self.max_length,\n            \"vocab\": list(self.vocab),\n            \"index_word\": self.index_word,\n            \"word_index\": self.word_index\n        }\n        \n        if self.bos_token is not None:\n            dic[\"bos_token\"] = self.bos_token\n            \n        if self.eos_token is not None:\n            dic[\"eos_token\"] = self.eos_token\n            \n        res = json.dumps(dic)\n        \n        with open(path, \"w+\") as f:\n            f.write(res)\n            \n    def load(self, path):\n        with open(path, \"r\") as f:\n            dic = json.load(f)\n        self.vocab_size = dic[\"vocab_size\"]\n        self.oov_token = dic[\"oov_token\"]\n        self.max_length = dic[\"max_length\"]\n        self.vocab = set(dic[\"vocab\"])\n        self.index_word = dic[\"index_word\"]\n        self.word_index = dic[\"word_index\"]\n        if \"bos_token\" in dic:\n            self.bos_token = dic[\"bos_token\"]\n        if \"eos_token\" in dic:\n            self.eos_token = dic[\"eos_token\"]\n            \n    def transform(self, texts):\n        sentences = []\n        offset = 1 if self.eos_token != None else 0\n        for i in range(len(texts)):\n            text = texts[i]\n            sentence = []\n            if self.bos_token != None:\n                sentence.append(self.word_index[self.bos_token])\n            for item in text:\n                if item in self.vocab:\n                    sentence.append(self.word_index[item])\n                else:\n                    sentence.append(self.word_index[self.oov_token])\n            if len(sentence) == self.max_length - offset:\n                if self.eos_token != None:\n                    sentence.append(self.word_index[self.eos_token])\n            elif len(sentence) < self.max_length - offset:\n                if self.eos_token != None:\n                    sentence.append(self.word_index[self.eos_token])\n                sentence += [0] * (self.max_length - len(sentence))\n            elif len(sentence) > self.max_length - offset:\n                sentence = sentence[:self.max_length - offset]\n                if self.eos_token != None:\n                    sentence.append(self.word_index[self.eos_token])\n            sentences.append(sentence)\n        return sentences","8989aca8":"tokenizer = Tokenizer()\ntokenizer.load(\"..\/input\/jigsaw-toxicity-fnet\/tokenizer.json\")","af925d45":"class FNetEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, dropout_rate=0.1, **kwargs):\n        super(FNetEncoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.dense_proj = keras.Sequential(\n            [\n                layers.Dense(dense_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self, inputs):\n        # Casting the inputs to complex64\n        inp_complex = tf.cast(inputs, tf.complex64)\n        # Projecting the inputs to the frequency domain using FFT2D and\n        # extracting the real part of the output\n        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n        proj_input = self.layernorm_1(inputs + fft)\n        proj_output = self.dense_proj(proj_input)\n       \n        layer_norm = self.layernorm_2(proj_input + proj_output)\n        output = self.dropout(layer_norm)\n        return output","2832fb88":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super(PositionalEmbedding, self).__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n","d54201c8":"def get_fnet_classifier(config):\n    inputs = keras.Input(shape=(config.sequence_length), dtype=\"int64\", name=\"encoder_inputs\")\n    x = PositionalEmbedding(config.sequence_length, config.vocab_size, config.embed_dim)(inputs)\n    x = FNetEncoder(config.embed_dim, config.latent_dim)(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0.3)(x)\n    for i in range(3):\n        x = layers.Dense(100, activation=\"relu\")(x)\n        x = layers.Dropout(0.3)(x)\n    output = layers.Dense(1, activation=\"sigmoid\")(x)\n    fnet = keras.Model(inputs, output, name=\"fnet\")\n    return fnet","b7cb5164":"fnet = get_fnet_classifier(config)","25475dcd":"fnet.load_weights(\"..\/input\/jigsaw-toxicity-fnet\/model_latest.h5\")","530113be":"fnet.summary()","53c0303f":"keras.utils.plot_model(fnet, show_shapes=True)","652b89d3":"test = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv\")\ntest[\"text_preprocess\"] = test[\"text\"].apply(Tokenizer.preprocess_string)\ntest_sequences = tokenizer.transform(list(test[\"text_preprocess\"]))\nprint(test_sequences[0])\ntest_ds = tf.data.Dataset.from_tensor_slices((test_sequences)).batch(config.batch_size).prefetch(1)\nscore = fnet.predict(test_ds).reshape(-1)\nsample_submission[\"score\"] = rankdata(score, method='ordinal')\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","c566b6f9":"<a id=\"2.\"><\/a>\n## 2. Setup","ee01684f":"<a id=\"3.\"><\/a>\n## 3. Tokenzier","f87cb5d8":"\n<a id=\"6.\"><\/a>\n## 6. References\n- [FNet: Mixing Tokens with Fourier Transforms](https:\/\/arxiv.org\/abs\/2105.03824v3)\n- [Attention Is All You Need](https:\/\/arxiv.org\/abs\/1706.03762v5)\n- [Text Generation using FNet](https:\/\/keras.io\/examples\/nlp\/text_generation_fnet\/)\n- [English-Spanish Translation: FNet](https:\/\/www.kaggle.com\/lonnieqin\/english-spanish-translation-fnet)","31bee8b2":"This is inference notebook. For training notebook, visit [here](https:\/\/www.kaggle.com\/lonnieqin\/jigsaw-toxicity-prediction-with-fnet).","275b1871":"### 4.1 FNet Encoder","4a33a3e7":"<a id=\"4.\"><\/a>\n## 4. FNet Model","4cdda606":"<font color=\"red\" size=\"3\">If you found it useful and would like to back me up, just upvote.<\/font>","f91c784c":"# Jigsaw Toxicity Inference with FNet\n## Table of Contents\n* [1. Configuration](#1.)\n* [2. Setup](#2.)\n* [3. Tokenzier](#3.)\n* [4. FNet Model](#4.)\n* [5. Submission](#5.)","a138ff78":"<a id=\"4.\"><\/a>\n### 4.3 FNet Classification Model","09e7f0d2":"<a id=\"4.2\"><\/a>\n### 4.2 Positional Embedding","f3f6f085":"<a id=\"1.\"><\/a>\n## 1. Configuration","f96e12bf":"Let's visualize the Model.","266fd130":"<a id=\"5.\"><\/a>\n## 5. Submission"}}