{"cell_type":{"df87a2e8":"code","d228421d":"code","e051a4da":"code","4d0ebd41":"code","67841881":"code","85e1d297":"code","e384602b":"code","cc516c1f":"code","20755696":"code","f52e6ad4":"code","a21e58ae":"code","39735888":"code","e0fa3605":"code","53edd141":"code","c3ef7662":"code","8f354802":"code","ed275469":"code","39c8168c":"code","873244d1":"code","b9c7102c":"code","b8e2aef8":"code","2ce753e6":"code","e1e05645":"code","a7f9ab2a":"code","32a899d0":"code","b84963a0":"code","7b24d666":"code","87b2c127":"code","0af19c0a":"code","8cdf313e":"code","96fce8d5":"code","7286fb7b":"code","0eb6c9a8":"code","c0090925":"code","8bd40a9c":"code","7de526ca":"code","d19ecea9":"code","4cbadfd4":"code","fb738d26":"markdown"},"source":{"df87a2e8":"import pandas as pd\nimport numpy as np","d228421d":"PATHS = [\n    '..\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Monday-WorkingHours.pcap_ISCX.csv',\n    '..\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Tuesday-WorkingHours.pcap_ISCX.csv',\n    '..\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Wednesday-workingHours.pcap_ISCX.csv',\n    '..\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n    '..\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n    '..\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Morning.pcap_ISCX.csv',\n    '..\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n    '..\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n    \n]\ndf = pd.read_csv(PATHS[0])\nfor i in range(1,len(PATHS)):\n    temp = pd.read_csv(PATHS[i])\n    df = pd.concat([df,temp])","e051a4da":"# df = pd.get_dummies(df)","4d0ebd41":"m = df.loc[df[' Flow Packets\/s'] != np.inf,' Flow Packets\/s'].max()\ndf[' Flow Packets\/s'].replace(np.inf,m,inplace=True)\nm = df.loc[df['Flow Bytes\/s'] != np.inf,'Flow Bytes\/s'].max()\ndf['Flow Bytes\/s'].replace(np.inf,m,inplace=True)","67841881":"dtypes = df.dtypes\nprint(f\"Number of columns with Int {len(dtypes[dtypes == int])}\")\nprint(f\"Number of columns with float {len(dtypes[dtypes == float])}\")\nprint(f\"Number of columns with object {len(dtypes[dtypes == object])}\")","85e1d297":"null_values = df.isna().sum()\nnull_values[null_values >0]","e384602b":"null_index = np.where(df['Flow Bytes\/s'].isnull())[0]\ndf.dropna(inplace = True)","cc516c1f":"# labels = df[' Label'].unique()\n# label_enc  = dict()\n# for i in range(len(labels)):\n#     label_enc[labels[i]] = i\n# df[' Label'] = df[' Label'].map(label_enc)\n# import plotly.express as px\n# fig = px.imshow(df.corr())\n# fig.show()\n","20755696":"# import plotly.express as px\n# fig = px.imshow(df.corr())\n# fig.show()","f52e6ad4":"df[' Destination Port']","a21e58ae":"temp = df[df[' Label'] == 'BENIGN']\ntemp[' Destination Port'].describe()\ntemp = temp.sample(frac = 0.1)","39735888":"df = df[df[' Label'] != 'BENIGN']\ndf = pd.concat([df,temp])","e0fa3605":"def sample_class(x,benign_sample,malicious_sample):\n    res = 0\n    print(x.shape)\n    try:\n        x = x.reset_index(drop = True)\n        check = x[' Label'][0]\n        if check == 'BENIGN':\n            res = x.sample(benign_sample)\n        else:\n            res = x.sample(malicious_sample)\n    except:\n        pass\n    return res","53edd141":"t = df.groupby(' Label').apply(lambda x:  sample_class(x,benign_sample = 10000,malicious_sample = 5000) if x.shape[0]>5000 else x ) # sample_class(x,benign_sample = 10000,malicious_sample = 5000) if x.shape[0]>5000 else","c3ef7662":"t = t.reset_index(drop = True)\nt.to_csv(\"CICIDS2017_w_o_oversample.csv\",index = False)","8f354802":"t = pd.get_dummies(t)","ed275469":"col = [' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n       ' Total Backward Packets', 'Total Length of Fwd Packets',\n       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n       ' Bwd Packet Length Std', 'Flow Bytes\/s', ' Flow Packets\/s',\n       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags',\n       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length',\n       ' Bwd Header Length', 'Fwd Packets\/s', ' Bwd Packets\/s',\n       ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean',\n       ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count',\n       ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count',\n       ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count',\n       ' ECE Flag Count', ' Down\/Up Ratio', ' Average Packet Size',\n       ' Avg Fwd Segment Size', ' Avg Bwd Segment Size',\n       ' Fwd Header Length.1', 'Fwd Avg Bytes\/Bulk', ' Fwd Avg Packets\/Bulk',\n       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes\/Bulk', ' Bwd Avg Packets\/Bulk',\n       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes',\n       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n       ' Init_Win_bytes_backward', ' act_data_pkt_fwd',\n       ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max',\n       ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min']","39c8168c":"train_df= df[df['folds'] != 5]\nvalid_df = df[df['folds'] == 5]","873244d1":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ntrain_df[col] = scaler.fit_transform(train_df[col])\nvalid_df[col] = scaler.transform(valid_df[col])","b9c7102c":"\ndef create_ae_mlp(num_columns, num_labels, hidden_units, dropout_rates, ls = 1e-2, lr = 1e-3):\n    \n    inp = tf.keras.layers.Input(shape = (num_columns, ))\n    x0 = tf.keras.layers.BatchNormalization()(inp)\n    \n    #Encoder\n    encoder = tf.keras.layers.GaussianNoise(dropout_rates[0])(x0)\n    encoder = tf.keras.layers.Dense(hidden_units[0])(encoder)\n    encoder = tf.keras.layers.BatchNormalization()(encoder)\n    encoder = tf.keras.layers.Activation('swish')(encoder)\n    \n    #Decoder\n    decoder = tf.keras.layers.Dropout(dropout_rates[1])(encoder)\n    decoder = tf.keras.layers.Dense(num_columns, name = 'decoder')(decoder)\n    \n    \n    x_ae = tf.keras.layers.Dense(hidden_units[1])(decoder)\n    x_ae = tf.keras.layers.BatchNormalization()(x_ae)\n    x_ae = tf.keras.layers.Activation('swish')(x_ae)\n    x_ae = tf.keras.layers.Dropout(dropout_rates[2])(x_ae)\n\n    out_ae = tf.keras.layers.Dense(num_labels, activation = 'sigmoid', name = 'ae_action')(x_ae)\n    \n    #Multi Layer perceptron\n    x = tf.keras.layers.Concatenate()([x0, encoder])\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(dropout_rates[3])(x)\n    \n    for i in range(2, len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation('swish')(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 2])(x)\n        \n    out = tf.keras.layers.Dense(num_labels, activation = 'sigmoid', name = 'action')(x)\n    \n    model = tf.keras.models.Model(inputs = inp, outputs = [out_ae, out])\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr),\n                  loss = {\n                          'ae_action': tf.keras.losses.BinaryCrossentropy(label_smoothing = ls),\n                          'action': tf.keras.losses.BinaryCrossentropy(label_smoothing = ls), \n                         },\n                  metrics = { \n                             'ae_action': tf.keras.metrics.AUC(name = 'AUC'), \n                             'action': tf.keras.metrics.AUC(name = 'AUC'), \n                            }, \n                 )\n    \n    return model","b8e2aef8":"train_df.iloc[:,-15:].columns","2ce753e6":"params = {'num_columns': len(col), \n          'num_labels': 15, \n          'hidden_units': [96, 96, 896, 448, 448, 256], \n          'dropout_rates': [0.03527936123679956, 0.038424974585075086, 0.42409238408801436, 0.10431484318345882, 0.49230389137187497, 0.32024444956111164, 0.2716856145683449, 0.4379233941604448], \n          'ls': 0, \n          'lr':1e-3, \n         }","e1e05645":"import tensorflow as tf\ntf.random.set_seed(42)\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping","a7f9ab2a":"batch_size = 64","32a899d0":"epochs = 10","b84963a0":"fold = 5\nckp_path = f'JSModel_{fold}.hdf5'\nmodel = create_ae_mlp(**params)\nckp = ModelCheckpoint(ckp_path, monitor = 'val_action_AUC', verbose = 0, \n                      save_best_only = True, save_weights_only = True, mode = 'max')\nes = EarlyStopping(monitor = 'val_action_AUC', min_delta = 1e-4, patience = 20, mode = 'max', \n                   baseline = None, restore_best_weights = True, verbose = 0)\nhistory = model.fit(train_df[col], [train_df.iloc[:,-15:],train_df.iloc[:,-15:]], \n                    validation_data = (valid_df[col], [ valid_df.iloc[:,-15:],valid_df.iloc[:,-15:]]), \n                    epochs = epochs, batch_size = batch_size, callbacks = [ckp, es], verbose = True)\nhist = pd.DataFrame(history.history)\n","7b24d666":"from sklearn.metrics import precision_recall_fscore_support,confusion_matrix\nfrom sklearn.metrics import roc_auc_score","87b2c127":"res = model.predict(valid_df[col])\nres = np.argmax(res[1],axis =1)\n\ntruth = np.argmax(valid_df.iloc[:,-15:].values,axis = 1)\n\ntruth = np.argmax(valid_df.iloc[:,-15:].values,axis = 1)\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(truth, res)\nprecision,recall,_,_ = precision_recall_fscore_support(truth,res)","0af19c0a":"columns","8cdf313e":"roc_auc_score(valid_df.iloc[:,-15:], model.predict(valid_df[col])[1], multi_class='ovr')","96fce8d5":"df_cm = confusion_matrix(truth, res)\ndf_cm = pd.DataFrame(df_cm, columns = columns,index = columns)","7286fb7b":"columns = valid_df.columns[-15:]","0eb6c9a8":"import seaborn as sn\nplot = sn.heatmap(df_cm, annot=True,cmap='Blues')","c0090925":"plot.savefig(\"output.png\")","8bd40a9c":"import plotly.graph_objects as go\nimport json\n!pip install -U kaleido\nimport kaleido","7de526ca":"hist['epochs'] = [ i+1 for i in range(epochs)]","d19ecea9":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n                            x=hist['epochs'], y=hist['val_loss'],\n                            mode='lines+markers',\n                            name = \"Validation Loss\"\n                        )\n            )\n\nfig.add_trace(go.Scatter(\n                            x=hist['epochs'], y=hist['loss'],\n                            mode='lines+markers',\n                            name = \"Train Loss\"\n                        )\n            )\nfig.update_layout(title=\"Loss Versus Epochs for Autoencoder-MLP\",\n                   xaxis_title='Epoch',\n                   yaxis_title='Loss')\nfig.write_image(\"ae_loss.jpeg\")\nfig.show()","4cbadfd4":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n                            x=hist['epochs'], y=hist['val_AUC'],\n                            mode='lines+markers',\n                            name = \"Validation AUC\"\n                        )\n            )\n\nfig.add_trace(go.Scatter(\n                            x=hist['epochs'], y=hist['AUC'],\n                            mode='lines+markers',\n                            name = \"Train AUC\"\n                        )\n            )\nfig.update_layout(title=\"AUC Versus Epochs for Autoencoder-MLP\",\n                   xaxis_title='Epoch',\n                   yaxis_title='AUC')\nfig.write_image(\"ae_auc.jpeg\")\nfig.show()","fb738d26":"# Model "}}