{"cell_type":{"a454fccf":"code","b74aca74":"code","38e8b34a":"code","ffd4a7f9":"code","07380d87":"code","def5de07":"code","de433010":"code","8787f1ec":"code","2b9aa6ec":"code","c9b363ef":"code","aac7cdfd":"code","6fe7c9b0":"code","bcd84b77":"code","f4635d5c":"code","603fab7b":"code","43258171":"code","53bbe782":"code","aa2a8fc5":"code","6dceb174":"code","889a5180":"code","95d2706a":"code","a938b76f":"code","571b17ac":"code","6b46538d":"code","a9520cbd":"code","0aa67506":"code","244648e3":"code","fa1c1066":"code","4512b278":"code","8629fe00":"code","3af8a078":"code","9b1d0359":"code","412ab8d7":"code","5d7efaaa":"code","8170f44b":"code","5739e5b0":"code","35799cf3":"code","1c97253b":"code","51501c74":"code","a78309f4":"markdown","74c525b0":"markdown","350a5f0d":"markdown","505e3adb":"markdown","77e48417":"markdown","1159d77b":"markdown","9c5efba1":"markdown","5dd8ffca":"markdown","7867d3ba":"markdown","a0292ad9":"markdown","ace5758c":"markdown"},"source":{"a454fccf":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nfrom pathlib import Path\nimport random","b74aca74":"# util function to randomly view some pairs\ndef view_pairs(data1, data2, n_items):\n    indices = random.choices(range(len(data1)), k=n_items)\n    for idx in indices:\n        print(data1[idx])\n        print(data2[idx])\n        print(\"\\n----------------------\")","38e8b34a":"train_gu1 = open(Path(\"\/kaggle\/input\/pm-india-mann-ki-baat\/CVIT Mann Ki Baat\/PMINDIA-cvit-mono-bi__training_dataset\/pmindia\/train.gu-en.gu\"), \"r\").read().split(\"\\n\")\ntrain_en1 = open(Path(\"\/kaggle\/input\/pm-india-mann-ki-baat\/CVIT Mann Ki Baat\/PMINDIA-cvit-mono-bi__training_dataset\/pmindia\/train.gu-en.en\"), \"r\").read().split(\"\\n\")\nval_gu1 = open(Path(\"\/kaggle\/input\/pm-india-mann-ki-baat\/CVIT Mann Ki Baat\/cvit-mkb\/dev.gu-en.gu\"), \"r\").read().split(\"\\n\")\nval_en1 = open(Path(\"\/kaggle\/input\/pm-india-mann-ki-baat\/CVIT Mann Ki Baat\/cvit-mkb\/dev.gu-en.en\"), \"r\").read().split(\"\\n\")\ntest_gu1 = open(Path(\"\/kaggle\/input\/pm-india-mann-ki-baat\/CVIT Mann Ki Baat\/cvit-mkb\/test.gu-en.gu\"), \"r\").read().split(\"\\n\")\ntest_en1 = open(Path(\"\/kaggle\/input\/pm-india-mann-ki-baat\/CVIT Mann Ki Baat\/cvit-mkb\/test.gu-en.en\"), \"r\").read().split(\"\\n\")","ffd4a7f9":"view_pairs(test_gu1, test_en1,10)","07380d87":"# Size of data\nprint(f' Train en: {len(train_en1)}, Train gu: {len(train_gu1)} \\n \\\nVal en: {len(val_gu1)}, Val gu: {len(val_gu1)}\\n \\\nTest en: {len(test_en1)}, Test gu: {len(test_gu1)}')","def5de07":"data_en1 = train_en1 + val_en1 + test_en1\ndata_gu1 = train_gu1 + val_gu1 + test_gu1","de433010":"data_gu2 = open(Path(\"\/kaggle\/input\/gujarati-to-english-translation\/train.gu\"), \"r\").read().split(\"\\n\")\ndata_en2 = open(Path(\"\/kaggle\/input\/gujarati-to-english-translation\/train.en\"), \"r\").read().split(\"\\n\")","8787f1ec":"view_pairs(data_gu2, data_en2, 10)","2b9aa6ec":"def load_wmt_data(data_path):\n    lines = open(Path(data_path), \"r\").read().split(\"\\n\")\n    data = [tuple(x.split(\"\\t\")) for x in lines]\n    data = [x for x in data if len(x)==2]\n    data_gu = [x[0] for x in data]\n    data_en = [x[1] for x in data]\n    return data_gu, data_en","c9b363ef":"# Download the data\n! wget http:\/\/data.statmt.org\/wmt19\/translation-task\/bible.gu-en.tsv.gz\n# extract\n! gunzip -d .\/bible.gu-en.tsv.gz","aac7cdfd":"data_gu3, data_en3 = load_wmt_data(\".\/bible.gu-en.tsv\")\nview_pairs(data_gu3, data_en3, 5)","6fe7c9b0":"! wget http:\/\/data.statmt.org\/wmt19\/translation-task\/govin-clean.gu-en.tsv.gz\n! gunzip -d .\/govin-clean.gu-en.tsv.gz","bcd84b77":"data_gu4, data_en4 = load_wmt_data(\".\/govin-clean.gu-en.tsv\")\nview_pairs(data_gu4, data_en4, 5)","f4635d5c":"! wget http:\/\/data.statmt.org\/wmt19\/translation-task\/opus.gu-en.tsv.gz\n! gunzip -d .\/opus.gu-en.tsv.gz","603fab7b":"data_gu5, data_en5 = load_wmt_data(\".\/opus.gu-en.tsv\")\nview_pairs(data_gu5, data_en5, 5)","43258171":"! wget http:\/\/data.statmt.org\/wmt19\/translation-task\/wikipedia.gu-en.tsv.gz\n!gunzip -d wikipedia.gu-en.tsv.gz","53bbe782":"data_gu6, data_en6 = load_wmt_data(\".\/opus.gu-en.tsv\")\nview_pairs(data_gu6, data_en6, 5)","aa2a8fc5":"def remove_punct_en(eng_sentence):\n    # remove all punctuations\n    eng_sentence = eng_sentence.lower()\n    clean_sentence = re.sub(r'[^a-z0-9\\s]', \"\", eng_sentence)\n  \n    return clean_sentence","6dceb174":"# test remove_punct_en\ntest_cases = [\"hi, check: this\", \",and$ this also\", \"why not 88?\", \"motorcycle coming up to a stop sign on the street...\"]\nans = [\"hi check this\", \"and this also\", \"why not 88\", \"motorcycle coming up to a stop sign on the street\"]\nfor idx in range(len(test_cases)):\n    op = remove_punct_en(test_cases[idx])\n    expected_op = ans[idx]\n    assert op==expected_op","889a5180":"def remove_punct_guj(guj_sentence):\n    # remove using regex\n    clean_guj = re.sub(r'[~`!@#$%^&*()-_=+,.\u2022<>?\/;\"\u201c\u201d\\']', \"\", guj_sentence)\n    return clean_guj","95d2706a":"# test remove_punct_guj\ntest_cases = [\"\u0ab6\u0acd\u0ab0\u0ac0 \u0ab8\u0abe\u0ab0\u0acd\u0ab5\u0a9c\u0aa8\u0abf\u0a95 \u0aaa\u0acd\u0ab0\u0abe\u0aaf\u0aae\u0ab0\u0ac0 \u0a9f\u0ac0\u0a9a\u0ab0\u0acd\u0ab8 \u0a95\u0acb\u0ab2\u0ac7\u0a9c , \u0a85\u0ab0\u0ab5\u0abf\u0a82\u0aa6 \u0aac\u0abe\u0a97 \u0aaa\u0abe\u0ab8\u0ac7, \u0aae\u0ac1.\u0aa4\u0abe.\u0a9c\u0abf.\u0aae\u0ab9\u0ac7\u0ab8\u0abe\u0aa3\u0abe\", \n              \"\u0a88\u0ab8\u0ac1\u0a8f' \u0a95\u0ab9\u0acd\u0aaf\u0ac1\u0a82 \u0a95\u0ac7, \u201c\u0aa4\u0aae\u0aa8\u0ac7 \u0ab6\u0abe\u0a82\u0aa4\u0abf \u0aa5\u0abe\u0a93.\u201d\",\n             \"(\u0aac\u0ac0\u0a9c\u0ac0 \u0aae\u0acb\u0a9f\u0ac0 \u0a86\u0aaa\u0aa4\u0acd\u0aa4\u0abf \u0aaa\u0ac2\u0ab0\u0ac0 \u0aa5\u0a88 \u0a9b\u0ac7. \u0ab9\u0ab5\u0ac7 \u0aa4\u0acd\u0ab0\u0ac0\u0a9c\u0ac0 \u0aae\u0acb\u0a9f\u0ac0 \u0a86\u0aaa\u0aa4\u0acd\u0aa4\u0abf \u0a9c\u0ab2\u0aa6\u0ac0\u0aa5\u0ac0 \u0a86\u0ab5\u0ac0 \u0ab0\u0ab9\u0ac0 \u0a9b\u0ac7.)\"]\nans = [\"\u0ab6\u0acd\u0ab0\u0ac0 \u0ab8\u0abe\u0ab0\u0acd\u0ab5\u0a9c\u0aa8\u0abf\u0a95 \u0aaa\u0acd\u0ab0\u0abe\u0aaf\u0aae\u0ab0\u0ac0 \u0a9f\u0ac0\u0a9a\u0ab0\u0acd\u0ab8 \u0a95\u0acb\u0ab2\u0ac7\u0a9c  \u0a85\u0ab0\u0ab5\u0abf\u0a82\u0aa6 \u0aac\u0abe\u0a97 \u0aaa\u0abe\u0ab8\u0ac7 \u0aae\u0ac1\u0aa4\u0abe\u0a9c\u0abf\u0aae\u0ab9\u0ac7\u0ab8\u0abe\u0aa3\u0abe\", \n       \"\u0a88\u0ab8\u0ac1\u0a8f \u0a95\u0ab9\u0acd\u0aaf\u0ac1\u0a82 \u0a95\u0ac7 \u0aa4\u0aae\u0aa8\u0ac7 \u0ab6\u0abe\u0a82\u0aa4\u0abf \u0aa5\u0abe\u0a93\",\n      \"\u0aac\u0ac0\u0a9c\u0ac0 \u0aae\u0acb\u0a9f\u0ac0 \u0a86\u0aaa\u0aa4\u0acd\u0aa4\u0abf \u0aaa\u0ac2\u0ab0\u0ac0 \u0aa5\u0a88 \u0a9b\u0ac7 \u0ab9\u0ab5\u0ac7 \u0aa4\u0acd\u0ab0\u0ac0\u0a9c\u0ac0 \u0aae\u0acb\u0a9f\u0ac0 \u0a86\u0aaa\u0aa4\u0acd\u0aa4\u0abf \u0a9c\u0ab2\u0aa6\u0ac0\u0aa5\u0ac0 \u0a86\u0ab5\u0ac0 \u0ab0\u0ab9\u0ac0 \u0a9b\u0ac7\"]\nfor idx in range(len(test_cases)):\n    op = remove_punct_guj(test_cases[idx])\n    expected_op = ans[idx]\n    assert op==expected_op","a938b76f":"def other_cleaning(sentence):\n    \n    # remove multiple spaces\n    sentence = re.sub(r'\\s+', \" \", sentence)\n    return sentence","571b17ac":"# test other_cleaning\ntest_cases = [\"\u0a9f\u0ac0\u0a9a\u0ab0\u0acd\u0ab8 \u0a95\u0acb\u0ab2\u0ac7\u0a9c  \u0a85\u0ab0\u0ab5\u0abf\u0a82\u0aa6 \u0aac\u0abe\u0a97\"]\nexpected_op = [\"\u0a9f\u0ac0\u0a9a\u0ab0\u0acd\u0ab8 \u0a95\u0acb\u0ab2\u0ac7\u0a9c \u0a85\u0ab0\u0ab5\u0abf\u0a82\u0aa6 \u0aac\u0abe\u0a97\"]\nfor idx in range(len(test_cases)):\n    op = other_cleaning(test_cases[idx])\n    assert op == expected_op[idx]\n","6b46538d":"def clean_en_dataset(dataset):\n    dataset = [other_cleaning(remove_punct_en(x)) for x in dataset]\n    return dataset\ndef clean_gu_dataset(dataset):\n    dataset = [other_cleaning(remove_punct_guj(x)) for x in dataset]\n    return dataset","a9520cbd":"data_en = [data_en1, data_en2, data_en3, data_en4, data_en5, data_en6]\ndata_gu = [data_gu1, data_gu2, data_gu3, data_gu4, data_gu5, data_gu6]\nfor idx in range(len(data_en)):\n    data_en[idx] = clean_en_dataset(data_en[idx])\n    data_gu[idx] = clean_gu_dataset(data_gu[idx])","0aa67506":"view_pairs(data_en[5], data_gu[5], 5)","244648e3":"import pandas as pd","fa1c1066":"def convert_csv(data_en, data_gu,name):\n    data_csv = pd.DataFrame(zip(data_en, data_gu), columns=[\"en\", \"gu\"])\n    data_csv = data_csv.sample(frac=1).reset_index(drop=True)\n    data_csv[\"n_words_en\"] = data_csv[\"en\"].apply(lambda x: len(x.split()))\n    data_csv[\"n_words_gu\"] = data_csv[\"gu\"].apply(lambda x: len(x.split()))\n    data_csv[\"dataset_name\"] = name\n    return data_csv","4512b278":"list_csv = []\ndataset_names = [\"man ki baat\", \"mscoco\", \"bible\", \"govin\", \"opus\", \"wikipedia\"]\nfor idx in range(len(data_en)):\n    csv = convert_csv(data_en[idx], data_gu[idx], dataset_names[idx])\n    list_csv.append(csv)","8629fe00":"list_csv[2].head()","3af8a078":"def plot_hist(csv):\n    csv[[\"n_words_en\", \"n_words_gu\"]].plot.hist(bins=10, title=csv.dataset_name[0])","9b1d0359":"plot_hist(list_csv[0])","412ab8d7":"plot_hist(list_csv[1])","5d7efaaa":"plot_hist(list_csv[2])","8170f44b":"plot_hist(list_csv[3])","5739e5b0":"plot_hist(list_csv[4])","35799cf3":"plot_hist(list_csv[5])","1c97253b":"train,val,test=None,None,None\nidx=0\nfor csv in list_csv:\n    dataset_name = csv.dataset_name[0]\n    l1 = len(csv)\n    csv = csv.drop_duplicates()\n    csv = csv[(csv[\"en\"]!=\"\") & (csv[\"gu\"]!=\"\")]\n    l2 = len(csv)\n    print(f'dataset: {dataset_name}, Before: {l1} After: {l2}')\n    train_i, validate_i, test_i = \\\n              np.split(csv.sample(frac=1, random_state=42), \n                       [int(.70*len(csv)), int(.85*len(csv))])\n    if idx==0:\n        idx=1\n        train, val, test = train_i, validate_i, test_i\n    else:\n        train = pd.concat([train, train_i])\n        val = pd.concat([val, validate_i])\n        test = pd.concat([test, test_i])\nprint(f'train: {len(train)}, val: {len(val)}, test: {len(test)}')","51501c74":"! mkdir gu_en_data\ntrain.to_csv(\"gu_en_data\/train_gu_en.csv\")\nval.to_csv(\"gu_en_data\/val_gu_en.csv\")\ntest.to_csv(\"gu_en_data\/test_gu_en.csv\")","a78309f4":"## Dataset1 : Mann ki baat","74c525b0":"## Dataset6: [wikipedia](http:\/\/data.statmt.org\/wmt19\/translation-task\/wikipedia.gu-en.tsv.gz)","350a5f0d":"## Create train-val-test split","505e3adb":"## Dataset3: Bible gu-en [link](http:\/\/data.statmt.org\/wmt19\/translation-task\/bible.gu-en.tsv.gz)","77e48417":"## Clean all the datasets","1159d77b":"## Important news\/links related to translation of Indian Languages\n* [https:\/\/www.cfilt.iitb.ac.in\/](https:\/\/www.cfilt.iitb.ac.in\/)\n* [https:\/\/indiaai.gov.in\/missions\/national-mission-on-natural-language-translation](https:\/\/indiaai.gov.in\/missions\/national-mission-on-natural-language-translation)\n\n","9c5efba1":"# If you are aware of any dataset that is not included here, please comment and I will add it.","5dd8ffca":"## Dataset5: [opus](http:\/\/data.statmt.org\/wmt19\/translation-task\/opus.gu-en.tsv.gz)","7867d3ba":"## Data Cleaning","a0292ad9":"## Dataset2: eng_guj_parallel_corpus [Github](https:\/\/github.com\/shahparth123\/eng_guj_parallel_corpus), MSCOCO","ace5758c":"## Dataset4: [govin - clean](http:\/\/data.statmt.org\/wmt19\/translation-task\/govin-clean.gu-en.tsv.gz)"}}