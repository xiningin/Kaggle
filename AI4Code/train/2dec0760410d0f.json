{"cell_type":{"50b7ca78":"code","f9146700":"code","5248bbce":"code","590905b3":"code","e486a4f8":"code","66f9fad0":"code","03efa88e":"code","1d7c7c62":"code","67aebaf8":"code","abf2b255":"code","407decb3":"code","152dc894":"code","d5bebc23":"code","4f8a51bf":"code","9e3aea99":"code","5852d9d9":"code","5bd904fc":"code","9cb39cfc":"code","f411440a":"code","3d584b2a":"code","00cc97b9":"code","8d87f0f0":"code","21ccb504":"code","1f39eaba":"code","df2a8964":"code","1a923514":"code","a90dd0d8":"code","316231b8":"code","804c5b2e":"code","1ee868fc":"code","b09eba00":"code","64479216":"code","9666d9af":"code","35280c9e":"markdown","1a919fd4":"markdown","a9a94ced":"markdown","35ece0ba":"markdown","0814b90a":"markdown","76158b23":"markdown","0c6e1b25":"markdown","2327d836":"markdown","948aaaa6":"markdown","a6855004":"markdown","83b73dd3":"markdown","0dab73c3":"markdown","f92502bd":"markdown","8452c296":"markdown","4085a828":"markdown","e5485978":"markdown","a33cf470":"markdown","762ce280":"markdown","630cbb42":"markdown","2d70f6af":"markdown","176db95c":"markdown","7b2be394":"markdown","a842223a":"markdown","316d95a8":"markdown","cc58a279":"markdown","f8f5822a":"markdown","5752b704":"markdown","c2f30939":"markdown","d1e124f8":"markdown","b8aa6b60":"markdown","40d43bd2":"markdown","3bf91990":"markdown","d7124ac0":"markdown","42073726":"markdown","311f1756":"markdown"},"source":{"50b7ca78":"import pandas as pd\nimport numpy as np\n\nfrom scipy.stats import ttest_ind\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","f9146700":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","5248bbce":"data_df = [train_df, test_df]","590905b3":"train_df.head()","e486a4f8":"train_df.dtypes","66f9fad0":"for df in data_df:\n    df.crew = df.crew.astype('category')\n    df.experiment = df.experiment.astype('category')\n    df.seat = df.seat.astype('category')\n    \ntrain_df.event = train_df.event.astype('category')","03efa88e":"train_df.isnull().sum().sum(), test_df.isnull().sum().sum()","1d7c7c62":"train_df[['event', 'crew']].groupby(['event', 'crew'], as_index=False).size()","67aebaf8":"train_df.experiment.unique().categories, test_df.experiment.unique().categories","abf2b255":"train_df[['event', 'seat']].groupby(['event', 'seat'], as_index=False).size()","407decb3":"#Dropping categorical columns\nfor df in data_df:\n    df.drop(columns=['crew'], inplace = True)\n    df.drop(columns=['experiment'], inplace = True)\n    df.drop(columns=['seat'], inplace = True)","152dc894":"for predictor in test_df.columns[1:]:\n    g = sns.FacetGrid(train_df, col='event')\n    g.map(plt.hist, predictor, bins=100)","d5bebc23":"#Dropping numerical columns\nfor df in data_df:\n    df.drop(columns=['time'], inplace = True)\n    df.drop(columns=['ecg'], inplace = True)\n    df.drop(columns=['r'], inplace = True)\n    df.drop(columns=['gsr'], inplace = True)","4f8a51bf":"for predictor in test_df.columns[1:]:\n    g = sns.FacetGrid(train_df, col='event')\n    g.set(yscale=\"log\")\n    g.map(plt.hist, predictor, bins=100)","9e3aea99":"for predictor in test_df.columns[1:]:\n    train_df.boxplot(column=predictor, by='event', showmeans=True, showfliers=False)","5852d9d9":"#Helper function\ndef get_subset(event, predictor):\n    return train_df.loc[train_df.event == event, predictor]\n\n#Helper lists\nall_predictors = test_df.columns[1:]\nevents = ['A', 'B', 'C', 'D']","5bd904fc":"distinctive_A_predictors = []\n\nfor predictor in all_predictors:\n    p_vals = [ttest_ind(get_subset('A', predictor), get_subset(e, predictor), equal_var = False)[1] for e in events[1:]]\n    is_distinct = [p < 0.05 for p in p_vals]\n    if (all(is_distinct)):\n        distinctive_A_predictors.append(predictor)\n\ndistinctive_A_predictors","9cb39cfc":"distinctive_predictors = []\n\nfor predictor in all_predictors:\n    p_vals = [ttest_ind(get_subset(events[e1], predictor), get_subset(events[e2], predictor), equal_var = False)[1] for e1 in range(3) for e2 in range(e1+1,4)]\n    is_distinct = [p < 0.05 for p in p_vals]\n    if (all(is_distinct)):\n        distinctive_predictors.append(predictor)\n\ndistinctive_predictors","f411440a":"corrmat = train_df.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","3d584b2a":"abs_corrmat = train_df[distinctive_predictors].corr().abs()\ncorrlist = abs_corrmat.unstack().sort_values(ascending=False).iloc[len(distinctive_predictors)::2]\ncorrlist[corrlist > 0.7]","00cc97b9":"uncorrelated_predictors = distinctive_predictors.copy()\nuncorrelated_predictors.remove('eeg_p4')\nuncorrelated_predictors.remove('eeg_fp2')","8d87f0f0":"selected_predictors = uncorrelated_predictors","21ccb504":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import log_loss","1f39eaba":"X = train_df[selected_predictors]\nY = train_df.event\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=.2, random_state=35)\nX_train.shape, Y_train.shape, X_val.shape, Y_val.shape ","df2a8964":"losses = pd.DataFrame(columns=['log_loss','model'])","1a923514":"qda = QuadraticDiscriminantAnalysis()\nqda.fit(X_train, Y_train)\nqda_prob = qda.predict_proba(X_val)\nqda_loss = log_loss(Y_val, qda_prob, labels=qda.classes_)\nlosses.loc[\"QDA\"] = [qda_loss, qda]\n\nqda_loss","a90dd0d8":"lda = LinearDiscriminantAnalysis()\nlda.fit(X_train, Y_train)\nlda_prob = lda.predict_proba(X_val)\nlda_loss = log_loss(Y_val, lda_prob, labels=lda.classes_)\nlosses.loc[\"LDA\"] = [lda_loss, lda]\n\nlda_loss","316231b8":"logreg = LogisticRegression(multi_class = 'multinomial', solver='lbfgs', C=3, max_iter=500)\nlogreg.fit(X_train, Y_train)\nlogreg_prob = logreg.predict_proba(X_val)\nlogreg_loss = log_loss(Y_val, logreg_prob, labels=logreg.classes_)\nlosses.loc[\"Log Regression\"] = [logreg_loss, logreg]\n\nlogreg_loss","804c5b2e":"rforest = RandomForestClassifier(n_estimators = 100 , class_weight=\"balanced\")\nrforest.fit(X_train, Y_train)\nrforest_prob = rforest.predict_proba(X_val)\nrforest_loss = log_loss(Y_val, rforest_prob, labels=rforest.classes_)\nlosses.loc[\"R-forest\"] = [rforest_loss, rforest]\n    \nrforest_loss","1ee868fc":"losses.head()","b09eba00":"losses.log_loss.sort_values(ascending=True)","64479216":"best_model = losses.model[losses.log_loss.argmin()]","9666d9af":"test_res = best_model.predict_proba(test_df[selected_predictors])\nres_df = pd.DataFrame({\"A\" : test_res[:,0], \n                       \"B\" : test_res[:,1], \n                       \"C\" : test_res[:,2], \n                       \"D\" : test_res[:,3]})\nres_df.to_csv('submission.csv', index_label='id')","35280c9e":"Logistic Regression for different values of penalty.","1a919fd4":"## Analyzing obtained results","a9a94ced":"Approximating probability density functions of numerical predictors by plotting histograms of data divided into bins.","35ece0ba":"Selecting final set of features","0814b90a":"### Check highly correlated features","76158b23":"# Data Analysis","0c6e1b25":"Converting _object_ and _int64_ data type to _categorical_ for __crew__, __experiment__, __seat__, and __event__ columns. <br>\nThese columns are categorical by the description of data on Kaggle. ","2327d836":"Building correlation matrix","948aaaa6":"Predictors which remain can be better seen on the log-y scale.","a6855004":"# Model & predict","83b73dd3":"Selecting the most correlated pairs of features","0dab73c3":"There is no significant relationship between __event__ and __crew__ columns. __Crew__ column is a candidate for deletion.","f92502bd":"## General analysis","8452c296":"Building box plots of each predictor by the __event__. Plots visualize skewness of data by comparing mean, median and quartiles. ","4085a828":"### Check if feature predicts event","e5485978":"### Categorical columns","a33cf470":"Checking data types of loaded data","762ce280":"## Modeling","630cbb42":"## Saving results","2d70f6af":"## Analyze by column","176db95c":"Testing a hypothesis that values for different __events__ are drown from the same distribution for each predictor. \n\nNull hypothesis:\n- values of a predictor for event A and another event are taken from the same distribution (they have the same expected value) \n\nAlternative hypothesis:\n- values are taken from different distributions (different expected values)\n\nWe will reject the null hypothesis if the probability of the null hypothesis is less than 5%. scypi.stats.ttest_ind performs such testing.","7b2be394":"There is no significant relationship between __event__ and __seat__ columns. __Seat__ column is a candidate for deletion.","a842223a":"For such columns as __time__, __ecg__, __r__ and __gsr__ it is hard to see some pattern. And it would be hard to capture it if it exists. Therefore, we will delete these columns.","316d95a8":"Some of the _predictors_ distributions look the same for all of the __event__ types, which means that they won't likely be good predictors for event classification. But there are some other _predictors_ that show different mean for different events, which probably can be used for classifying __events__. We will investigate this means differences further in the next section.","cc58a279":"## Selecting predictors","f8f5822a":"__Experiment__ column is just a description column and it does not hold significant information. As it could not be used as a predictor it will be deleted.","5752b704":"Chosen models:\n- Linear and Quadratic Discriminant Analysis \n- Logistic Regression\n- Random Forrest","c2f30939":"Quadratic Discriminant Analysis","d1e124f8":"## Loading data","b8aa6b60":"Random Forrest","40d43bd2":"## Data preparation","3bf91990":"Checking for missing values","d7124ac0":"### Numerical columns","42073726":"*distinctive_predictors* contains only those predictors that have a statistically significant difference in expected values for all of the events. We will use only *distinctive_predictors* features in modeling.","311f1756":"Linear Discriminant Analysis"}}