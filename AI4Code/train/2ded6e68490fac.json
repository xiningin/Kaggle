{"cell_type":{"81632717":"code","c6a0e14b":"code","8374139e":"code","69dd6000":"code","8e7d17a2":"code","fdf2b206":"code","1eccf7d4":"code","44098428":"code","af043536":"code","cddd1564":"code","e50cf59d":"code","954b706e":"code","680d8def":"code","e955cd0d":"code","b582bb47":"code","e49c39b0":"code","671e10fe":"code","87f90772":"code","161d21ab":"code","2653d4e9":"code","aa31c39c":"markdown","2c016fd5":"markdown","21229dff":"markdown","f1b16198":"markdown","09be4c12":"markdown","d48fa906":"markdown","b2175755":"markdown","e51d96ea":"markdown","0c030f97":"markdown","e3f3ae5c":"markdown","6fbb0958":"markdown","287b40bd":"markdown","1ae8c05b":"markdown","5b04a4b5":"markdown","3925f9b7":"markdown","088bc68d":"markdown","9d763c9c":"markdown","53686c02":"markdown","cf968c72":"markdown","978fd1c3":"markdown","e62392ae":"markdown"},"source":{"81632717":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2 # image processing\nimport time # benchmarking\nimport bisect\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport math\n\n\n\ntraining_images = glob('..\/input\/cat-and-dog\/training_set\/training_set\/**\/*.jpg')\nplt.figure(figsize=(16, 16))\nfor i in range(1, 10):\n    training_image = np.random.choice(training_images)\n    plt.subplot(3, 3, i)\n    plt.imshow(cv2.imread(training_image)) \n    plt.axis('off')\n\n\n#im = cv2.imread(\"..\/input\/cat-and-dog\/training_set\/training_set\/cats\/cat.1.jpg\")","c6a0e14b":"print(\"Training Set:\")\nprint(\"Cat images: \" + str(len(glob('..\/input\/cat-and-dog\/training_set\/training_set\/cats\/*.jpg'))))\nprint(\"Dog images: \" + str(len(glob('..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/*.jpg'))))\nprint(\"Test Set:\")\nprint(\"Cat images: \" + str(len(glob('..\/input\/cat-and-dog\/test_set\/test_set\/cats\/*.jpg'))))\nprint(\"Dog images: \" + str(len(glob('..\/input\/cat-and-dog\/test_set\/test_set\/dogs\/*.jpg'))))","8374139e":"def jpg_to_arr(filename, shape=(1080, 1080)):\n    '''\n    Convert a JPG to a NumPy array of size (1080, 1080, 3).\n    The (x, y)th pixel in the original JPG will be located \n    at jpg_to_arr(inp)[x][y], and will have three dimensions\n    for its red, blue, and green color values, respectively.\n    That is, if the pixel at (x, y) had the color #A51822,\n    then the resulting array `arr` would have:\n    arr[x][y] == [165 \/ 255, 24 \/ 255, 34 \/ 255]\n    \n    :param string filename: The path to the filename containing a JPG image. Undefined behavior otherwise\n    :param tuple(int, int) shape: The resolution of the resulting NumPy array.\n    '''\n    \n    img = cv2.imread(filename)\n    resized_img = cv2.resize(img, shape) \/ 255\n    return resized_img.astype(float)\n\nprint(\"Value of pixel (15, 27): \" + str(jpg_to_arr(glob('..\/input\/cat-and-dog\/training_set\/training_set\/**\/*.jpg')[0])[15][27]))","69dd6000":"def dist(x, y):\n    '''\n    Computes the euclidean distance between two 3-dimensional vectors\n    x and y.\n    '''\n    assert x.shape == y.shape  # This is why the reshaping in jpg_to_arr() is necessary\n    \n    total_dist = 0\n    \n    for i in range(x.shape[0]):\n        for j in range(x.shape[1]):\n            for k in range(x.shape[2]):\n                total_dist += (x[i][j][k] - y[i][j][k])**2\n    return np.sqrt(total_dist)\n\ndef faster_dist(x, y):\n    '''\n    Same as the above, but much faster\n    '''\n    return np.linalg.norm(x - y)\n\n            \ncat_img1 = jpg_to_arr(glob('..\/input\/cat-and-dog\/training_set\/training_set\/cats\/*.jpg')[0])\ncat_img2 = jpg_to_arr(glob('..\/input\/cat-and-dog\/training_set\/training_set\/cats\/*.jpg')[1])\ndog_img1 = jpg_to_arr(glob('..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/*.jpg')[0])\n\nt1 = time.time()\nprint(\"Euclidean Distance between two cat images: \" + str(dist(cat_img1, cat_img2)))\nprint(\"Euclidean Distance between a cat and a dog image: \" + str(dist(cat_img1, dog_img1)))\nprint(\"Execution Time: \" + str(time.time() - t1))\nt2 = time.time()\nprint(\"Euclidean Distance between two cat images: \" + str(faster_dist(cat_img1, cat_img2)))\nprint(\"Euclidean Distance between a cat and a dog image: \" + str(faster_dist(cat_img1, dog_img1)))\nprint(\"Execution Time: \" + str(time.time() - t2))","8e7d17a2":"class KNN:\n    def __init__(self, k, shape=(64, 64), debug=False):\n        self.k = k\n        self.points = []  # A list of tuples, where `first == vector` and `second == class_val`.\n        self.shape = shape\n        self.img_read_time = 0\n        self.img_resize_time = 0\n        self.debug = debug\n        \n        \n    def distance(self, x, y):\n        '''\n        Computes the euclidean distance between two 3-dimensional vectors\n        x and y.\n        '''\n        \n        return np.linalg.norm(x - y)\n    \n    \n    def jpg_to_arr(self, filename):\n        '''\n        Convert a JPG to a NumPy array of size (dim1, dim2, 3).\n        The (x, y)th pixel in the original JPG will be located \n        at jpg_to_arr(inp)[x][y], and will have three dimensions\n        for its red, blue, and green color values, respectively.\n        That is, if the pixel at (x, y) had the color #A51822,\n        then the resulting array `arr` would have:\n        arr[x][y] == [165 \/ 255, 24 \/ 255, 34 \/ 255]\n\n        :param string filename: The path to the filename containing a JPG image. Undefined behavior otherwise\n        :param tuple(int, int) shape: The resolution of the resulting NumPy array.\n        '''\n    \n        t1 = time.time()\n        img = cv2.imread(filename)\n        t2 = time.time()\n        resized_img = cv2.resize(img, self.shape) \/ 255\n        t3 = time.time()\n        self.img_read_time += t2 - t1\n        self.img_resize_time += t3 - t2\n        return resized_img.astype(np.float16)\n    \n    \n    def classify_point_from_vector(self, vector):\n        '''\n        Classifies `vector` using the stored list of known points, `self.points`\n        \n        :param vector: A vector matching the dimensions of the vectors the KNN model was trained on.\n        '''\n        t1 = time.time()\n        distances = []\n        for index, point in enumerate(self.points):\n            point_vector = point[0]\n            dist = self.distance(vector, point_vector)\n            bisect.insort(distances, [dist, index])\n        class_count = {}\n        for i in range(self.k):\n            class_val = self.points[distances[i][1]][1]\n            class_count[class_val] = class_count.get(class_val, 0) + 1\n        if self.debug:\n            t2 = time.time()\n            print(\"Time to classify point: \" + str(t2 - t1))\n        return max(class_count, key=class_count.get)\n    \n    \n    def classify_point_from_filename(self, filename):\n        '''\n        Classifies `filename` using the stored list of known points, `self.points`\n        \n        :param filename: A filename pointing to a JPG image.\n        '''\n        vector = self.jpg_to_arr(filename)\n        return self.classify_point_from_vector(vector)\n    \n    \n    def classify_points_from_filenames(self, filename_lists, class_names):\n        '''\n        Evalutes the model on a set of test images, where `filename_lists` is a list of lists where\n        the Nth list contains all of the example image filenames for the Nth class in `class_names`\n        \n        :param filename_lists: A list of lists, where each list contains examples for a specific class\n        :param class_names: A list of the class names\n        '''\n\n        assert len(filename_lists) == len(class_names)\n                \n        conf_matrix = [[0, 0], [0, 0]]\n        for i in range(len(filename_lists)):\n            for filename in filename_lists[i]:\n                prediction = self.classify_point_from_filename(filename)\n                prediction_index = class_names.index(prediction)\n                conf_matrix[i][prediction_index] += 1\n        return conf_matrix\n        \n    def classify_points_from_vectors(self, vectors_lists, class_names):\n        '''\n        Evalutes the model on a set of test images, where `vectors_lists` is a list of lists where\n        the Nth list contains all of the example image filenames for the Nth class in `class_names`\n        \n        :param vectors_lists: A list of lists, where each list contains examples for a specific class\n        :param class_names: A list of the class names\n        '''\n\n        assert len(vectors_lists) == len(class_names)\n                \n        conf_matrix = [[0, 0], [0, 0]]\n        for i in range(len(vectors_lists)):\n            for vector in vectors_lists[i]:\n                prediction = self.classify_point_from_vector(vector)\n                prediction_index = class_names.index(prediction)\n                conf_matrix[i][prediction_index] += 1\n        return conf_matrix\n    \n    \n    def add_point(self, vector, class_val):\n        '''\n        Adds the data point `vector` to self.points\n        '''\n        \n        self.points.append((vector, class_val))\n\n\n    def load_dataset(self, training_examples, class_names):\n        '''\n        Populates `self.points` using `training_examples` and `class_names` to populate\n        the first and second values in each tuple in `self.points`, respectively. \n        \n        That is, training_examples is a list of lists, where the Nth list is a list of filenames\n        that serve as examples for the Nth class_name. Thus, the length of `training_examples` must\n        be equal to the length of `class_names`.\n        \n        :param training_examples: A list of lists, where each list contains examples for a specific class\n        :param class_names: A list of the class names\n        '''\n        \n        assert len(training_examples) == len(class_names)\n        for i in range(len(class_names)):\n            for example in training_examples[i]:\n                self.add_point(self.jpg_to_arr(example), class_names[i])\n        if self.debug:\n            print(\"Total Image Read Time: \" + str(self.img_read_time))\n            print(\"Total Image Resize Time: \" + str(self.img_resize_time))\n    \n    def get_distance_to_nearest_neighbor(self, vector):\n        '''\n        Get the distance between a point `vector` and its nearest neighbor in `self.points`.\n        '''\n        min_dist = math.inf\n        for point,  _ in self.points:\n            min_dist = min(min_dist, self.distance(vector, point))\n        return min_dist\n    \n    def get_distances_to_n_nearest_neighbors(self, vector, n):\n        '''\n        Get the distances between a point `vector` and its nth nearest neighbors in `self.points`.\n        '''\n        dists = [self.distance(vector, point) for point, _ in self.points]\n        return sorted(dists)[:n]\n    \n    def get_sorted_distances_to_neighbors_with_class_data(self, vector, class_val):\n        '''\n        Returns a sorted list of tuples, where each tuple is a pair of (distance, bool),\n        where bool is True if the corresponding point belongs to class `class_val` or False\n        otherwise.\n        '''\n        return sorted([(self.distance(vector, point), class_val == point_class_val) for point, point_class_val in self.points])\n\n\nmodel = KNN(5, debug=True)\nmodel.load_dataset([glob('..\/input\/cat-and-dog\/training_set\/training_set\/cats\/*.jpg'),\n                  glob('..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/*.jpg')],\n                  [\"cats\", \"dogs\"])\nprint(\"The model predicts the picture of a cat belongs to the class \" + str(model.classify_point_from_filename(glob('..\/input\/cat-and-dog\/test_set\/test_set\/cats\/*.jpg')[0])))","fdf2b206":"'''\nmodel = KNN(5, shape=(128, 128))\nmodel.load_dataset([glob('..\/input\/cat-and-dog\/training_set\/training_set\/cats\/*.jpg'),\n                  glob('..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/*.jpg')],\n                  [\"cats\", \"dogs\"])\nconf_matrix = model.classify_points([glob('..\/input\/cat-and-dog\/test_set\/test_set\/cats\/*.jpg'),\n                  glob('..\/input\/cat-and-dog\/test_set\/test_set\/dogs\/*.jpg')],\n                  [\"cats\", \"dogs\"])\nprint(\"Confusion Matrix: \" + str(conf_matrix))\n'''","1eccf7d4":"class MetricConstructor:\n    def __init__(self, conf_matrix, class_names):\n        self.cm = np.array(conf_matrix)\n        self.class_names = class_names\n        self.total = conf_matrix[0][0] + conf_matrix[0][1] + conf_matrix[1][0] + conf_matrix[1][1]\n        \n    \n    def accuracy(self):\n        return (self.cm[0][0] + self.cm[1][1]) \/ self.total\n        \n    \n    def precision(self, invert=False):\n        if invert:\n            return self.cm[1][1] \/ (self.cm[1][1] + self.cm[0][1])\n        else:\n            return self.cm[0][0] \/ (self.cm[0][0] + self.cm[1][0])\n    \n    \n    def recall(self, invert=False):\n        if invert:\n            return self.cm[1][1] \/ (self.cm[1][1] + self.cm[1][0])\n        else:\n            return self.cm[0][0] \/ (self.cm[0][0] + self.cm[0][1])\n    \n    \n    def f1(self, invert=False):\n        return (2 * self.precision(invert=invert) * self.recall(invert=invert)) \/ (self.precision(invert=invert) + self.recall(invert=invert))\n    \n    \n    def print_metrics(self):\n        \n        print(\"Accuracy: \" + str(self.accuracy()))\n        print(\"Precision for {}: {}\".format(self.class_names[0], self.precision()))\n        print(\"Precision for {}: {}\".format(self.class_names[1], self.precision(invert=True)))\n        print(\"Recall for {}: {}\".format(self.class_names[0], self.recall()))\n        print(\"Recall for {}: {}\".format(self.class_names[1], self.recall(invert=True)))\n        print(\"F1 Score for {}: {}\".format(self.class_names[0], self.f1()))\n        print(\"F1 Score for {}: {}\".format(self.class_names[1], self.f1(invert=True)))\n        \n        return self.accuracy()\n    \n    \nconf_matrix = [[772, 239], [681, 331]]\nmetrics = MetricConstructor(conf_matrix, [\"cats\", \"dogs\"])\nmetrics.print_metrics()\n","44098428":"def train_and_test_knn(k_arr, shape_arr):\n    '''\n    Trains and prints the evaluation metrics of KNN models tried on all image resolutions\n    provided by `shape_arr` for all k-values provided by `k_arr`. That is, this function is\n    O(n*m), where len(k_arr) == n and len(shape_arr) == m.\n    \n    The model is trained on the cat-and-dog training set, and then evaluated using the cat-and-dog\n    test set, for each combination of values in `k_arr` and `shape_arr`.\n    \n    :param k_arr: A list of k-values to test the model with.\n    :param shape_arr: A list of shape tuples to use as image resolutions for training the model on.\n    '''\n    for k in k_arr:\n        print(\"k == \" + str(k))\n        for shape in shape_arr:\n            print(\"shape == \" + str(shape))\n            model = KNN(k, shape=shape)\n            model.load_dataset([glob('..\/input\/cat-and-dog\/training_set\/training_set\/cats\/*.jpg'),\n                      glob('..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/*.jpg')],\n                      [\"cats\", \"dogs\"])\n            conf_matrix = model.classify_points_from_filenames([glob('..\/input\/cat-and-dog\/test_set\/test_set\/cats\/*.jpg'),\n                      glob('..\/input\/cat-and-dog\/test_set\/test_set\/dogs\/*.jpg')],\n                      [\"cats\", \"dogs\"])\n            MetricConstructor(conf_matrix, [\"cats\", \"dogs\"]).print_metrics()\n\n            \n#train_and_test_knn(range(1, 5), [(128, 128)])\n'''\nk == 1\nAccuracy: 0.5289174493326743\nPrecision for cats: 0.5216417910447761\nPrecision for dogs: 0.5431918008784773\nRecall for cats: 0.6913946587537092\nRecall for dogs: 0.366600790513834\nF1 Score for cats: 0.5946405784772437\nF1 Score for dogs: 0.43775811209439525\nk == 2\nAccuracy: 0.5289174493326743\nPrecision for cats: 0.5216417910447761\nPrecision for dogs: 0.5431918008784773\nRecall for cats: 0.6913946587537092\nRecall for dogs: 0.366600790513834\nF1 Score for cats: 0.5946405784772437\nF1 Score for dogs: 0.43775811209439525\nk == 3\nAccuracy: 0.5516559565002471\nPrecision for cats: 0.536723163841808\nPrecision for dogs: 0.586490939044481\nRecall for cats: 0.751730959446093\nRecall for dogs: 0.35177865612648224\nF1 Score for cats: 0.6262875978574372\nF1 Score for dogs: 0.43977764051883883\nk == 4\nAccuracy: 0.5437469105289174\nPrecision for cats: 0.5308555399719496\nPrecision for dogs: 0.5745393634840871\nRecall for cats: 0.7487636003956478\nRecall for dogs: 0.33893280632411066\nF1 Score for cats: 0.621255642183012\nF1 Score for dogs: 0.42635177128651336\n'''","af043536":"#train_and_test_knn([i * 10 for i in range(1, 5)], [(128, 128)])\n'''\nk == 10\nAccuracy: 0.5719228868017795\nPrecision for cats: 0.548953409858204\nPrecision for dogs: 0.6346863468634686\nRecall for cats: 0.8041543026706232\nRecall for dogs: 0.33992094861660077\nF1 Score for cats: 0.6524879614767255\nF1 Score for dogs: 0.44272844272844275\nk == 20\nAccuracy: 0.569451309935739\nPrecision for cats: 0.5461741424802111\nPrecision for dogs: 0.6390532544378699\nRecall for cats: 0.8189910979228486\nRecall for dogs: 0.3201581027667984\nF1 Score for cats: 0.6553225168183616\nF1 Score for dogs: 0.42659644502962474\nk == 30\nAccuracy: 0.5580820563519525\nPrecision for cats: 0.5382103200522534\nPrecision for dogs: 0.6199186991869918\nRecall for cats: 0.8150346191889218\nRecall for dogs: 0.30138339920948615\nF1 Score for cats: 0.6483084185680568\nF1 Score for dogs: 0.40558510638297873\nk == 40\nAccuracy: 0.5659911023232822\nPrecision for cats: 0.5444221776887107\nPrecision for dogs: 0.6273764258555133\nRecall for cats: 0.8061325420375866\nRecall for dogs: 0.32608695652173914\nF1 Score for cats: 0.6499202551834131\nF1 Score for dogs: 0.4291287386215865\n'''","cddd1564":"#train_and_test_knn([i * 100 for i in range(1, 5)], [(128, 128)])\n'''\nk == 100\nAccuracy: 0.5892239248640633\nPrecision for cats: 0.5645624103299857\nPrecision for dogs: 0.643879173290938\nRecall for cats: 0.7784371909000989\nRecall for dogs: 0.40019762845849804\nF1 Score for cats: 0.6544698544698544\nF1 Score for dogs: 0.49360146252285186\nk == 200\nAccuracy: 0.606030647553139\nPrecision for cats: 0.5874183006535948\nPrecision for dogs: 0.6345431789737171\nRecall for cats: 0.7111770524233432\nRecall for dogs: 0.5009881422924901\nF1 Score for cats: 0.643400447427293\nF1 Score for dogs: 0.5599116510215351\nk == 300\nAccuracy: 0.6109738012852199\nPrecision for cats: 0.6014492753623188\nPrecision for dogs: 0.6224156692056583\nRecall for cats: 0.6567754698318496\nRecall for dogs: 0.5652173913043478\nF1 Score for cats: 0.6278959810874705\nF1 Score for dogs: 0.5924391506991196\nk == 400\nAccuracy: 0.6050420168067226\nPrecision for cats: 0.6043307086614174\nPrecision for dogs: 0.605759682224429\nRecall for cats: 0.6073194856577646\nRecall for dogs: 0.6027667984189723\nF1 Score for cats: 0.605821410952146\nF1 Score for dogs: 0.6042595344229816\n'''","e50cf59d":"#train_and_test_knn([5, 50, 250, 400, 1000], [(2 ** i, 2 ** i) for i in [4, 5, 6, 7, 8]])\n\nk_5 = [.563, .551, .547, .545, .546]\nk_50 = [.583, .572, .570, .582, .581]\nk_250 = [.615, .608, .613, .616, .616]\nk_400 = [.617, .603, .597, .605, .600]\nk_1000 = [.576, .584, .576, .577, .578]\nk_accuraries = [(5, k_5), (50, k_50), (250, k_250), (400, k_400), (1000, k_1000)]\n\nresolutions = [16, 32, 64, 128, 256]\n\nplt.figure(figsize=(16, 16))\nplt.rcParams.update({'font.size': 22})\nfor k, accuracies in k_accuraries:\n    plt.plot(resolutions, accuracies, label=\"k == \" + str(k))\n\nplt.title(\"Model Performance\")\nplt.xlabel(\"Image Resolution\")\nplt.ylabel(\"Test Set Accuracies\")\nplt.grid()\nplt.legend(title=\"KNN k value\")\nplt.show()","954b706e":"#train_and_test_knn([5, 50, 250, 400, 1000], [(2 ** i, 2 ** i) for i in range(1, 4)])\n\nk_5 = [.532, .557, .581, .563, .551, .547, .545, .546]\nk_50 = [.533, .574, .603, .583, .572, .570, .582, .581]\nk_250 = [.531, .580, .595, .615, .608, .613, .616, .616]\nk_400 = [.519, .571, .597, .617, .603, .597, .605, .600]\nk_1000 = [.519, .567, .571, .576, .584, .576, .577, .578]\nk_accuraries = [(5, k_5), (50, k_50), (250, k_250), (400, k_400), (1000, k_1000)]\n\nresolutions = [2, 4, 8, 16, 32, 64, 128, 256]\n\nplt.figure(figsize=(16, 16))\nplt.rcParams.update({'font.size': 22})\nfor k, accuracies in k_accuraries:\n    plt.plot(resolutions, accuracies, label=\"k == \" + str(k))\n\nplt.title(\"Model Performance\")\nplt.xlabel(\"Image Resolution\")\nplt.ylabel(\"Test Set Accuracies\")\nplt.grid()\nplt.legend(title=\"KNN k value\")\nplt.show()","680d8def":"img = glob('..\/input\/cat-and-dog\/training_set\/training_set\/**\/*.jpg')[0]\nplt.figure(figsize=(16, 16))\nfor i in range(1, 10):\n    data = jpg_to_arr(img, shape=(2 ** i, 2 ** i))\n    plt.subplot(3, 3, i)\n    plt.title(\"{}x{}x3\".format(2 ** i, 2 ** i))\n    plt.imshow(data) \n    plt.axis('off')","e955cd0d":"class CatOrDog:\n    '''\n    A simple data class.\n    tail_length - {0: no tail, 1: short tail, 2: long tail}\n    fur_color - {0: brown, 1: grey, 2: white, 3: black, 4: orange}\n    pupil_type - {0: round, 1: vertical-slit}\n    whiskers - {0: false, 1: true}\n    class_val - {0: dog, 1: cat}\n    '''\n    \n    def __init__(self, weight, tail_length, fur_color, age, pupil_type, whiskers, litter_size, class_val):\n        self.weight = weight\n        self.tail_length = tail_length\n        self.fur_color = fur_color\n        self.age = age\n        self.pupil_type = pupil_type\n        self.whiskers = whiskers\n        self.litter_size = litter_size\n        self.class_val = class_val\n    \n    \n    def get_vector(self):\n        '''\n        Get a vector of the x values corresponding to this object (every value except for class_val).\n        '''\n        tail_length_one_hot = [self.tail_length == i for i in range(3)]\n        fur_color_one_hot = [self.fur_color == i for i in range(5)]\n        return np.array([self.weight, self.age, self.pupil_type, self.whiskers, self.litter_size] + tail_length_one_hot + fur_color_one_hot)\n    \n    \n    def get_class(self):\n        return self.class_val\n        \n\ndef generate_dataset(cats_n, dogs_n):\n    '''\n    Populates a dummy dataset of cats and dogs. Note that this dataset may create dogs or cats that aren't realistic.\n    This method only exists for instructional purposes.\n    \n    :param cats_n: Number of cat examples to generate.\n    :param dogs_n: Number of dog examples to generate.\n    '''\n    \n    dataset = []\n    for i in range(cats_n):\n        dataset.append(CatOrDog(np.random.uniform(0, 0.25), 1, np.random.randint(0, 5), np.random.uniform(0, 1), 1, 1, np.random.randint(2, 12) \/ 11, 1))\n    for i in range(dogs_n):\n        dataset.append(CatOrDog(np.random.uniform(0, 1), np.random.randint(0, 3), np.random.randint(0, 5), np.random.uniform(0, 0.8), 0, 1, np.random.randint(5, 8) \/ 11, 0))\n    return dataset\n    \n\n    \ntraining_set = generate_dataset(1000, 1000)\ntest_set = [[animal.weight for animal in generate_dataset(200, 0)], [animal.weight for animal in generate_dataset(0, 200)]]\n\nmodel = KNN(5)\nfor animal in training_set:\n    model.add_point(animal.weight, animal.get_class())\n\nconf_matrix = model.classify_points_from_vectors(test_set, [1, 0])\nMetricConstructor(conf_matrix, [\"cats\", \"dogs\"]).print_metrics()","b582bb47":"training_set = generate_dataset(1000, 1000)\ntest_set = [[np.array([animal.weight, animal.pupil_type]) for animal in generate_dataset(200, 0)], [np.array([animal.weight, animal.pupil_type]) for animal in generate_dataset(0, 200)]]\n\nmodel = KNN(5)\nfor animal in training_set:\n    model.add_point(np.array([animal.weight, animal.pupil_type]), animal.get_class())\n\nconf_matrix = model.classify_points_from_vectors(test_set, [1, 0])\nMetricConstructor(conf_matrix, [\"cats\", \"dogs\"]).print_metrics()","e49c39b0":"training_set = generate_dataset(1000, 1000)\ntest_set = [[animal.get_vector() for animal in generate_dataset(200, 0)], [animal.get_vector() for animal in generate_dataset(0, 200)]]\n\nmodel = KNN(5)\nfor animal in training_set:\n    model.add_point(animal.get_vector(), animal.get_class())\n\nconf_matrix = model.classify_points_from_vectors(test_set, [1, 0])\nMetricConstructor(model.classify_points_from_vectors(test_set, [1, 0]), [\"cats\", \"dogs\"]).print_metrics()","671e10fe":"accuracies = []\navg_dists = []\nnoisy_dimensions = []\n\nfor i in range(2, 6):\n    print(\"Adding {} noisy dimensions\".format(10 ** i))\n    training_set = generate_dataset(1000, 1000)\n    test_set = [[np.append(animal.get_vector(), np.random.random_sample(10 ** i)) for animal in generate_dataset(200, 0)], [np.append(animal.get_vector(), np.random.random_sample(10 ** i)) for animal in generate_dataset(0, 200)]]\n\n    model = KNN(5)\n    for animal in training_set:\n        model.add_point(np.append(animal.get_vector(), np.random.random_sample(10 ** i)), animal.get_class())\n\n    conf_matrix = model.classify_points_from_vectors(test_set, [1, 0])\n    accuracies.append(MetricConstructor(conf_matrix, [\"cats\", \"dogs\"]).print_metrics())\n    \n    total_dist = 0\n    for test_point in test_set[0] + test_set[1]:\n        total_dist += model.get_distance_to_nearest_neighbor(test_point)\n    print(\"Average distance for any testing point to the nearest neighbor: \" + str(total_dist \/ (len(test_set[0]) + len(test_set[1]))))\n    \n    avg_dists.append(total_dist \/ (len(test_set[0]) + len(test_set[1])))\n    noisy_dimensions.append(10 ** i)\n    \nplt.figure(figsize=(6, 6))   \nplt.plot(noisy_dimensions, accuracies)\nplt.title(\"KNN when k == 5\")\nplt.xlabel(\"Number of Noisy Dimensions\")\nplt.ylabel(\"Test Set Accuracy\")\nplt.grid()\nplt.show()\nplt.figure(figsize=(6, 6))\nplt.plot(noisy_dimensions, avg_dists)\nplt.title(\"KNN when k == 5\")\nplt.xlabel(\"Number of Noisy Dimensions\")\nplt.ylabel(\"Average Distance to Nearest Neighbor\")\nplt.grid()\nplt.show()","87f90772":"'''\nnearest_any_dist = []\nnearest_same_dist = []\nnearest_diff_dist = []\nfurthest_any_dist = []\nfurthest_same_dist = []\nfurthest_diff_dist = []\nfor i in range(1, 8):\n    print(\"Shape: {}x{}x3\".format(2 ** i, 2 ** i))\n    model = KNN(5, shape=(2 ** i, 2 ** i))\n    model.load_dataset([glob('..\/input\/cat-and-dog\/training_set\/training_set\/cats\/*.jpg'),\n              glob('..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/*.jpg')],\n              [\"cats\", \"dogs\"])\n    training_set_size = len(glob('..\/input\/cat-and-dog\/training_set\/training_set\/cats\/*.jpg')) + len(glob('..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/*.jpg'))\n    test_set_cats = glob('..\/input\/cat-and-dog\/test_set\/test_set\/cats\/*.jpg')\n    test_set_dogs = glob('..\/input\/cat-and-dog\/test_set\/test_set\/dogs\/*.jpg')\n    nearest_neighbors_avg = np.zeros(training_set_size)\n    nearest_same_class_neighbors_avg = np.zeros(4000)\n    nearest_diff_class_neighbors_avg = np.zeros(4000)\n    for filename in test_set_cats:\n        vector = model.jpg_to_arr(filename)\n        nearest_neighbors_avg += np.array(model.get_distances_to_n_nearest_neighbors(vector, training_set_size))\n        nearest_neighbors_pairs = model.get_sorted_distances_to_neighbors_with_class_data(vector, \"cats\")\n        nearest_neighbors_same, nearest_neighbors_diff = [dist for dist, same_class in nearest_neighbors_pairs if same_class],  [dist for dist, same_class in nearest_neighbors_pairs if not same_class]\n        nearest_same_class_neighbors_avg += np.array(nearest_neighbors_same[:4000])\n        nearest_diff_class_neighbors_avg += np.array(nearest_neighbors_diff[:4000])\n    for filename in test_set_dogs:\n        vector = model.jpg_to_arr(filename)\n        nearest_neighbors_avg += np.array(model.get_distances_to_n_nearest_neighbors(vector, training_set_size))\n        nearest_neighbors_pairs = model.get_sorted_distances_to_neighbors_with_class_data(vector, \"dogs\")\n        nearest_neighbors_same, nearest_neighbors_diff = [dist for dist, same_class in nearest_neighbors_pairs if same_class],  [dist for dist, same_class in nearest_neighbors_pairs if not same_class]\n        nearest_same_class_neighbors_avg += np.array(nearest_neighbors_same[:4000])\n        nearest_diff_class_neighbors_avg += np.array(nearest_neighbors_diff[:4000])\n    nearest_neighbors_avg \/= len(test_set_cats) + len(test_set_dogs)\n    nearest_same_class_neighbors_avg \/= len(test_set_cats) + len(test_set_dogs)\n    nearest_diff_class_neighbors_avg \/= len(test_set_cats) + len(test_set_dogs)\n    nearest_any_dist.append(nearest_neighbors_avg[0])\n    nearest_same_dist.append(nearest_same_class_neighbors_avg[0])\n    nearest_diff_dist.append(nearest_diff_class_neighbors_avg[0])\n    furthest_any_dist.append(nearest_neighbors_avg[-1])\n    furthest_same_dist.append(nearest_same_class_neighbors_avg[-1])\n    furthest_diff_dist.append(nearest_diff_class_neighbors_avg[-1])\n    print(\"Average distance to nearest neighbor: \" + str(nearest_neighbors_avg[0]))\n    print(\"Average distance to {}th neighbor: {}\".format(training_set_size, nearest_neighbors_avg[-1]))\n    print(\"Average distance to nearest same-class neighbor: \" + str(nearest_same_class_neighbors_avg[0]))\n    print(\"Average distance to 4000th same-class neighbor: \" + str(nearest_same_class_neighbors_avg[-1]))\n    print(\"Average distance to nearest diff-class neighbor: \" + str(nearest_diff_class_neighbors_avg[0]))\n    print(\"Average distance to 4000th diff-class neighbor: \" + str(nearest_diff_class_neighbors_avg[-1]))\n\nplt.figure(figsize=(8, 8))   \ndims = [2 ** i for i in range(1, 8)]\nplt.plot(dims, nearest_any_dist, label=\"Nearest Neighbor (Any class)\")\nplt.plot(dims, nearest_same_dist, label=\"Nearest Neighbor (Same class)\")\nplt.plot(dims, nearest_diff_dist, label=\"Nearest Neighbor (Diff class)\")\nplt.plot(dims, furthest_any_dist, label=\"Furthest Neighbor (Any class)\")\nplt.plot(dims, furthest_same_dist, label=\"Furthest Neighbor (Same class)\")\nplt.plot(dims, furthest_diff_dist, label=\"Furthest Neighbor (Diff class)\")\nplt.title(\"Distance to Nearest Neighbors\")\nplt.xlabel(\"Width\/Height of Resized Images (Pixels)\")\nplt.ylabel(\"Distance to Neighbor\")\nplt.legend()\nplt.grid\\\nplt.show()\n'''","161d21ab":"class ImprovedKNN(KNN):\n    def __init__(self, k, shape=(64, 64), debug=False, color_flag=cv2.IMREAD_COLOR):\n        '''\n        :param int color_flag: Passed as a flag to `iv2.imread()`, denoting the method in which images should be read.\n        '''\n        KNN.__init__(self, k, shape, debug)\n        self.color_flag = color_flag\n        \n        \n    def run_pca(self, min_explained_var):\n        '''\n        Performs PCA to calculate a lower-dimensional representation space for the associated data stored\n        in `self.points`.\n        \n        :param min_explained_var: The minimum variance that has to be explained by the lower-dimensional \n        representation space. More plainly, the explained variance of the selected principal components\n        will be at least `min_explained_var`.\n        '''\n        self.mean = np.mean([np.array(el[0]).flatten() for el in self.points], axis=0)\n        covariance_matrix = np.cov((np.array([np.array(el[0]).flatten() for el in self.points]) - self.mean).T)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n        del covariance_matrix\n        self.explained_variances = eigenvalues \/ np.sum(eigenvalues)\n        self.eigenvectors = np.array([])\n        explained_var = 0\n        i = 0\n        while explained_var < min_explained_var:\n            explained_var += self.explained_variances[i]\n            i += 1\n        self.eigenvectors = eigenvectors[:i]\n        print(\"{} components are needed to explain {} variance in the dataset\".format(i, explained_var))\n        \n        \n    def transform_point(self, vector):\n        '''\n        Transforms the vector to the lower-dimensional respresentational space calculated by the PCA. As such,\n        `self.run_pca()` must be called before invoking this method.\n        '''\n        return np.dot(np.array(vector).flatten() - self.mean, self.eigenvectors.T)\n    \n    \n    def transform_points(self):\n        self.points = [(self.transform_point(point), class_val) for point, class_val in self.points]\n            \n    \n    def classify_point_from_vector(self, vector):\n        t1 = time.time()\n        distances = []\n        vector = self.transform_point(vector)\n        for index, point in enumerate(self.points):\n            point_vector = point[0]\n            dist = self.distance(vector, point_vector)\n            bisect.insort(distances, [dist, index])\n        class_count = {}\n        for i in range(self.k):\n            class_val = self.points[distances[i][1]][1]\n            class_count[class_val] = class_count.get(class_val, 0) + 1\n        if self.debug:\n            t2 = time.time()\n            print(\"Time to classify point: \" + str(t2 - t1))\n        return max(class_count, key=class_count.get)\n    \n    def jpg_to_arr(self, filename):\n        '''\n        Convert a JPG to a NumPy array of size (dim1, dim2, 3).\n        The (x, y)th pixel in the original JPG will be located \n        at jpg_to_arr(inp)[x][y], and will have three dimensions\n        for its red, blue, and green color values, respectively.\n        That is, if the pixel at (x, y) had the color #A51822,\n        then the resulting array `arr` would have:\n        arr[x][y] == [165 \/ 255, 24 \/ 255, 34 \/ 255]\n\n        :param string filename: The path to the filename containing a JPG image. Undefined behavior otherwise\n        :param tuple(int, int) shape: The resolution of the resulting NumPy array.\n        '''\n    \n        t1 = time.time()\n        img = cv2.imread(filename, self.color_flag)\n        t2 = time.time()\n        resized_img = cv2.resize(img, self.shape) \/ 255\n        t3 = time.time()\n        self.img_read_time += t2 - t1\n        self.img_resize_time += t3 - t2\n        return resized_img.astype(np.float16)\n    \n    \n    def graph_data(self, class_color_map):\n        '''\n        Graph the data points in `self.points` by using PCA to reduce the dimensionality of the data to two.\n        :param class_color_map: A dict mapping the names of classes to the color that points belonging to the\n        classes should have on the scatter plot.\n        '''\n        mean = np.mean([np.array(el[0]).flatten() for el in self.points], axis=0)\n        covariance_matrix = np.cov((np.array([np.array(el[0]).flatten() for el in self.points]) - mean).T)\n        eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n        del covariance_matrix\n        explained_var = np.sum(eigenvalues[:2] \/ np.sum(eigenvalues))\n        print(\"{} of the variance is explained\".format(explained_var))\n        points = [(np.dot(np.array(point).flatten() - mean, eigenvectors[:2].T), class_val) for point, class_val in self.points]\n        for point, class_val in points:\n            plt.scatter(point[0], point[1], color=class_color_map[class_val])\n        plot_title = \", \".join([k + \" == \" + v for (k, v) in class_color_map.items()])\n        plt.title(plot_title)\n        plt.grid()\n        plt.show()\n        \n    \ndef train_and_test_improved_knn(k_arr, shape_arr, explained_var_arr, color_flag_arr):\n    '''\n    Same as `train_and_test_knn()`, but for the ImprovedKNN. Takes two additional parameters.\n    Like `train_and_test_knn()`, this method evaluates models for every\n    combination of input parameters - as such, it has O(n * m * o * p) runtime, where the constants\n    are the length of the input parameter arrays, respectively.\n    \n    :param k_arr: A list of k-values to test the model with.\n    :param shape_arr: A list of shape tuples to use as image resolutions for training the model on.\n    :param explained_var_arr: A list of minimum explained variances to use to run the PCA with. \n    :param color_flag_arr: A list of color_flags to construct the ImprovedKNN model with.\n    '''\n    for k in k_arr:\n        print(\"k == \" + str(k))\n        for shape in shape_arr:\n            print(\"shape == \" + str(shape))\n            for explained_var in explained_var_arr:\n                print(\"min_explained_var == \" + str(explained_var))\n                for color_flag in color_flag_arr:\n                    print(\"color_flag == \" + str(color_flag))\n                    model = ImprovedKNN(k, shape=shape, color_flag=color_flag)\n                    model.load_dataset([glob('..\/input\/cat-and-dog\/training_set\/training_set\/cats\/*.jpg'),\n                              glob('..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/*.jpg')],\n                              [\"cats\", \"dogs\"])\n                    model.run_pca(explained_var)\n                    model.transform_points()\n                    conf_matrix = model.classify_points_from_filenames([glob('..\/input\/cat-and-dog\/test_set\/test_set\/cats\/*.jpg'),\n                              glob('..\/input\/cat-and-dog\/test_set\/test_set\/dogs\/*.jpg')],\n                              [\"cats\", \"dogs\"])\n                    MetricConstructor(conf_matrix, [\"cats\", \"dogs\"]).print_metrics()\n\nmodel = ImprovedKNN(k, shape=(64, 64))\nmodel.load_dataset([glob('..\/input\/cat-and-dog\/training_set\/training_set\/cats\/*.jpg'),\n          glob('..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/*.jpg')],\n          [\"cats\", \"dogs\"])\nmodel.graph_data({\"cats\": \"red\", \"dogs\": \"blue\"})\n\ntrain_and_test_improved_knn([5, 25], [(64, 64)], [0.9, 0.75], [cv2.IMREAD_COLOR, cv2.IMREAD_GRAYSCALE])\n","2653d4e9":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\n\nclass SKLearnKNN(KNN):\n    def fit(self):\n        self.knn = KNeighborsClassifier(self.k)\n        self.knn.fit([np.array(el[0]).flatten() for el in self.points], [el[1] for el in self.points])\n    \n    def classify_point_from_vector(self, vector):\n        t1 = time.time()\n        prediction = self.knn.predict(vector.flatten().reshape(1, -1))\n        if self.debug:\n            t2 = time.time()\n            print(\"Time to classify point: \" + str(t2 - t1))\n        return prediction\n    \nclass SKLearnKNNwithPCA(KNN):\n    def fit(self, n_components):\n        self.pca = PCA(n_components)\n        self.knn = KNeighborsClassifier(self.k)\n        transformed_points = self.pca.fit_transform([np.array(el[0]).flatten() for el in self.points])\n        self.points = [(point, el[1]) for point, el in zip(transformed_points, self.points)]\n        self.knn.fit([np.array(el[0]).flatten() for el in self.points], [el[1] for el in self.points])\n        \n    def classify_point_from_vector(self, vector):\n        t1 = time.time()\n        transformed_vector = self.pca.transform(vector.flatten().reshape(1, -1))\n        prediction = self.knn.predict(transformed_vector)\n        if self.debug:\n            t2 = time.time()\n            print(\"Time to classify point: \" + str(t2 - t1))\n        return prediction\n    \n    def graph_data(self, class_color_map):\n        pca = PCA(2)\n        transformed_points = pca.fit_transform([np.array(el[0]).flatten() for el in self.points])\n        for transformed_point, el in zip(transformed_points, self.points):\n            plt.scatter(transformed_point[0], transformed_point[1], color=class_color_map[el[1]][1])\n        plot_title = \", \".join([k + \" == \" + v for (k, v) in class_color_map])\n        plt.title(plot_title)\n        plt.grid()\n        plt.show()\n            \n        \n\nmodel = SKLearnKNN(5, shape=(64, 64))\nmodel.load_dataset([glob('..\/input\/cat-and-dog\/training_set\/training_set\/cats\/*.jpg'),\n          glob('..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/*.jpg')],\n          [0, 1])\nmodel.fit()\nconf_matrix = model.classify_points_from_filenames([glob('..\/input\/cat-and-dog\/test_set\/test_set\/cats\/*.jpg'),\n                          glob('..\/input\/cat-and-dog\/test_set\/test_set\/dogs\/*.jpg')],\n                          [0, 1])\nMetricConstructor(conf_matrix, [\"cats\", \"dogs\"]).print_metrics()\n\nmodel = SKLearnKNNwithPCA(5, shape=(64, 64))\nmodel.load_dataset([glob('..\/input\/cat-and-dog\/training_set\/training_set\/cats\/*.jpg'),\n          glob('..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/*.jpg')],\n          [0, 1])\nmodel.fit(300)\nconf_matrix = model.classify_points_from_filenames([glob('..\/input\/cat-and-dog\/test_set\/test_set\/cats\/*.jpg'),\n                          glob('..\/input\/cat-and-dog\/test_set\/test_set\/dogs\/*.jpg')],\n                          [0, 1])\nMetricConstructor(conf_matrix, [\"cats\", \"dogs\"]).print_metrics()\nmodel.graph_data([(\"cats\", \"red\"), (\"dogs\", \"blue\")])","aa31c39c":"The graph above illustrates two important trends:\n\n1. There is only a miniscule difference between distances between a vector and its nth same-class nearest neighbor and a vector and its nth different-class nearest neighbor. In other words, the nth nearest neighbor of a given vector is, effectively, jsut as likely to belong to the same class as it is the opposite class.\n2. The distances between a vector and its nearest neighbor grows linearly with the image resolution the vector represents.\n\nAs stated previously, the KNN algorithm excels when similar points are located in close proximity in their representational space. But that claim does not hold for this dataset - all points seem to be equidistant, regardless of any resizing.\n\nIt'd be premature to conclude that a KNN cannot model this dataset, however. KNNs have been shown to be successful at many simiar image classification tasks, so intuitively they should be able to achieve better than 60% accuracy at classifying images as being cats or dogs. To attempt to achieve better performance, the cat and dog images will be preprocessed by performing dimensionality reduction to reduce the number of irrelevant dimensions. More specifically, PCA will be used to engineer new features in a much smaller representational space, with hopefully very little resulting information loss.\n\nA basic PCA method will be implemented below.","2c016fd5":"The performance of the model is negligibly better than a model which classified images randomly, which could be expected to achieve around 50% accuracy. The model also seems biased towards classifying images as cats, with a 76% recall for cats and only a 33% recall for dogs.\n\nWhat's not clear is whether these poor results are due to the poor resolution size of the images, the value of `k` that was chosen, or the inability of a KNN to properly model this dataset. So, to elimate at least one of these possibilities, the model will be retrained on additional values of k to see how performance is then affected.","21229dff":"The performance of the model when the image resolution is `2x2x3` drops significantly, with model performance peaking with image resolutions of either `8x8x3` or `16x16x3`. This peak in performance near the middle of resolution values is comparable to how model accuracy correlates with `k`, with performance peaking when `k == 250` and dropping as `k` increases or decreases past that value.\n\nBut why is model performance often better, depending on the value of `k`, when images have a resolution of `8x8x3` compared with resolutions of `256x256x3`? The difference in accuracy is admittedly negligible - the greatest absolute difference between test set accuracies for any of the evaluated values of `k` between the `8x8x3` and `256x256x3` instances is less than 4% - but the difference being negligible is, in itself, significant.\n\nLet's take a look at a sample picture from the training set when resized to different resolutions to better visualize the data the model is being trained on.","f1b16198":"With PCA, the model ends up performing worse than without PCA, and the scatter plot partially elucidates why.\n\nTwo dimensions are able to explain ~30% of the variance of the data in the training set, but a scatter plot of the training set data transformed to those two dimensions reveals an amorphous blob of the two classes mixed together.\n\nOf course, PCA isn't a panacea. It works simply by engineering new features which explain the most variance of the data, which each feature being perpendicular to the other features. The below image helps to explain the process.\n\n![image.png](attachment:image.png)\n\nTypically, PCA is done by standardizing the input matrix, centering the input matrix on the mean (though this step is not always necessary), computing the covariance matrix of the input matrix, computing the eigenvectors of the covariance matrix, and then transforming the input matricies using the selected eigenvectors. PCA will result in as many eigenvectors as there are dimensions of the input dataset, but `n` of those eigenvectors will explain `x` variance of the training set. For example, the \"most predictive\" eigenvector might explain 25% of the variance, while the second explains 12%, the third explains 8%, and so on.  As this notebook concerns mostly KNNs, an in depth analysis and explanation of PCA won't be provided, but there are countless resources online which cover the algorithm effectively.\n\nWith PCA, the dataset could be reduced from over `12_000` dimensions to under `400` dimensions that could account for over 90% of the variance of the training set, and yet the performance of the algorithm remained the same. Thus, the dimensionality of the dataset isn't the only obstacle preventing the KNN from properly modeling the data. If this is the case, it seems futile to try to preprocess the dataset to boost the accuracy of the model - it'd be better to just use a model better suited to the dataset.\n\nAs a bit of a sanity check, the KNN and PCA implementation from SKLearn will be used, to see if their implementations of the algorithms are somehow able to achieve better results.","09be4c12":"After running the above, the following results are obtained:\n\n```\nconf_matrix = [[772, 239], [681, 331]]\n```","d48fa906":"The sampled pictures all align with reasonable expectations, but it's worth noting how noisy many of the images appear to be. Obscured foreground, poor resolution, and bad lighting are just some of the obstacles present in this dataset that the model will have to overcome in order to properly classify images, although these issues are far from exclusive to this particular dataset. \n\nLet's also take a look at how many images are present in the training and testing sets for each category.","b2175755":"There is negligible class imbalance in the dataset, and it appears the data has been partionined using an 80\/20 split - that is, 80% of the samples for each category were used to build the training set, and the other 20% of the samples were used to build the testing set.\n\nSince the classes are balanced and the dataset has already been partitioned, this notebook can be dedicated to the construction and evaluation of the KNN model.\n\nBut first, it's worth giving an overview of how KNN works and why it is often able to build highly accurate models.\n\nKNN, or K-Nearest Neighbors, is an algorithm that classifies a novel data point `x` by comparing its feature vector to those of already classified data points. The `k` data points which are closest to `x` are determined, and the mode of the classes that those `k` data points belong to is the predicted class for `x`. That is, if `k == 5`, and for a given input `x` the nearest neighbors are `{\"red\": 1, \"blue\": 3, \"green\": 1} \/\/ class: occurrence`, then `x` is predicted to be `\"blue\"`, as three neighbors were `\"blue\"` while only two were `\"red\"` or `\"green\"`.\n\nThis explanation is basic, though, and fails to address many questions. How is the value for `k` chosen? How are images mapped to feature vectors? How is \"nearness\" calculated for feature vectors? What happens in the case of a tie between nearest neighbors (`k == 4`, and `{\"red\": 2, \"blue\": 2}`)?\n\nAll of these questions will be addressed below, as the model is constructed in Python. ","e51d96ea":"Still, the model achieves 100% accuracy, even with the addition of irrelevant or uniformative dimensions such as `whiskers` (all cats and dogs have whiskers). From these results it's not clear why the main assumption of the KNN model - that similar points are located in close proximity in their representational space - is relevant to the poor results of the KNN model when applied to graphical data.\n\n","0c030f97":"Both of the functions above perform the same calculation, but NumPy and its use of C-level code allows for much faster performance than the naive looping solution.\n\nA simple implementation of a KNN would require comparing a novel data point to every other existing data point to determine what the `k` nearest neighbors to that point are, even if `k == 1`. Thus, in the case of this dataset with `~8_000` entries in the training set, classifying an image in the testing set would require comparing the image to `~8_000` other images, which would take `~8_000 images * 10 seconds per image -> ~80_000 seconds`, or `~22 hours`, if using the nomral `dist(x, y)` function. `faster_dist(x, y)`, however, would only require `8_000 images * 0.009 seconds per image -> ~72 seconds` to compare a single image to every image in the training set. This means that classifying all of the images in the test set using `dist(x, y)` would be expected to take __over 5 years__, while classifying all of the test set images would be expcted to take \"just\" 40 hours. \n\nThere are several data structures that can be used to store the data points which would greatly reduce the number of points the algorithm would have to check to calculate the k nearest neighbors (from being an `O(N)` task to only `O(log(N))`), but before any further optimization the actual KNN class will be constructed.","e3f3ae5c":"The results of the model when `k == [1, 5]` are all comparable to each other, as can be seen in the commented out code above. \n\nPerhaps the model is able to achieve better results with much larger values for `k`.","6fbb0958":"The performance of the SKLearn KNN and KNN with PCA is no better than that of the custom implementation declared in this notebook. None of the KNN models have been able to achieve better than ~60% accuracy, which is poor for a cat\/dog image classifier. ","287b40bd":"Again, the results are still poor, and the model continues to be biased towards classifying images as cats even when `k == 40`, even though the classes in the training set were balanced.\n\nIt's unlikely that simply increasing the resolution of the images from 128x128 to something higher will result in an extreme increase in performance - 128x128 should be a high enough resolution to achieve better results than 56% accuracy, and it'd be astonishing if increasing the resolution from 128x128 to something like 512x512 would result in an accuracy boost of `56% -> ~80%`.\n\nOne final time, the model will be evaluated with different values for `k` - this time, the values will be much larger than before.","1ae8c05b":"In past notebooks, I have investigated building both Decision Tree models and Random Forest models using just numpy and pandas. Both algorithms have their merits and are able to model complex data, but tend to achieve poor results when trained on visual data. \n\nConvolutional Neural Networks (CNNs) are often the first algorithm that comes to mind concerning the task of image classification. They're typically able achieve impressive results, but they require a large dataset, take a substantial amount of time to train, and require care to avoid overfitting.\n\nK-Nearest Neighbors is a simple algorithm that is often able to achieve comparable results to a CNN. It has an intuitive algorithm, only one hyperparameter, and most would agree it forms a model with far greater interprability than that of a CNN. In this notebook, I'll be explaining, building and evaluating a KNN model that predicts whether a given image more closely resembles a cat or a dog.\n\nFirst, we'll take a look at a few of the images in the dataset to better understand the data that the model will be trained on.","5b04a4b5":"For a certain cat image in the test set, the KNN model where `k == 5` correctly classifies the image as being a cat. The classifcaton process takes several minutes, however, so further optimizations would be ideal before running the model on all `~2_000` images in the test set.\n\nApart from a linear search of all of the data points, it is common to see a k-d tree used to store the data points to speed up the classifcation process. This can reduce the complexity of the nearest neighbor search from `O(n) -> O(log(n))`, although the optimization is most significant when the dimensionality of the dataset is small; in particular, when the number of dimensions `d` of the dataset is far greater than the number of data points `n`, `(n >> 2**k)`. Since `n ~= d` for this dataset, the k-d tree optimization won't be very helpful, as the algorithm will likely need to make as many comparisons as it did with the naive approach. Dimesionality reduction would have to be used before using this approach with this dataset, but the potential lossy compression from doing so makes it less favorable than other optimizations that wouldn't require dimensionality reduction. Additionally, if the dimensions are to be reduced such that `n >> 2**k`, then the naive linear algorithm will likely perform fast enough anyways.\n\nThe main bottleneck of the algorithm comes from reading in the images. It takes around `~1.25` seconds to classify a single image, but `~33` seconds to load in all of the images in the training set as vectors. This may not seem bad, but currently memory limitations are preventing the images from being stored in a higher quality. It would be easy to refactor the code so `self.points` stores tuples of `(filename, class_name)`, but this would mean classifying a point would require loading in every training set image each time. This would increase the classification time from `~1.25` seconds to `~34.25` seconds, which would increase the total time to classify all `~2_000` test set images from `~45` minutes to `~19` hours. \n\nBefore any additional preprocessing or optimizations, let's evaluate the model with image resolutions of 128x128 - close to the maximum resolution possible without the model crashing due to memory constraints. If the model is able to perform well, then there may be no need to worry about any of the obstacles that occur with higher image resolutions.","3925f9b7":"When `pupil_type` is added as a second dimension to the KNN model, all of the metrics of the model - accuracy, precision, recall, and F1 score - reach 100%. This makes sense, given that all cats have vertical-slit pupils and all dogs have round pupils, so the KNN is able to correctly classify all animals properly. That is, with this new metric both cats and dogs will always be in close proximity to at least some members of the same class.\n\nWhat happens when all of the remaining dimensions are provided to the KNN model?","088bc68d":"When `k == 5`, and when the KNN model is trained only on a single dimension - the weight of the animal - the model is able to achieve ~90% accuracy on the test set. Big dogs are always classified correctly, as the heaviest cat is only 25% the weight of the heaviest dog, but small dogs and cats are sometimes classified incorrectly.\n\nLet's add the type of the pupil to see how performance is affected.","9d763c9c":"Without any additional noisy dimensions, the model is able to obtain 100% accuracy on the test set and was shown previously. However, as additional noisy dimensions are added the performance of the model start to suffer until eventually the model has only ~50% accuracy. With `100_000` noisy dimensions, the KNN model barely performs better on the test set than a random coinflip could be expected to. \n\nAs the number of noisy dimensions increase, the average distance between all of the points in the test set to their nearest neighbor increases, from ~0 when there is no irrelevant noise to ~130 when there are `100_000` noisy dimensions.\n\nThe above observation is a key flaw that can lead to failure when using a KNN model, resulting from the main assumption of the model that was stated earlier: that __similar points are located in close proximity in their representational space.__\n\nUltimately, KNNs struggle to model sparse datasets - they are especially prone to the curse of dimensionality. Unlike a basic Decision Tree model which would be able to properly model this particular dataset by splitting the two classes based on the variable which lead to the most information gain - `pupil_type` - a KNN model struggles as the introduction of noisy features increases the likelihood that points belonging to the same class are further apart from each other. In fact, as the number of noisy dimensions increases, the query vector and the vectors in the training set become increasingly equidistant. \n\nTo understand this, imagine the above dataset trained only on `pupil_type`. Vectors belonging to the same class would have exactly 0 distance between them, while vectors of opposite classes would have exactly 1 distance between them. Now, imagine a single noisy dimension is added, where values are picked randomly for both classes from a uniform distribution between 0.0 and 1.0. On average, the expected distance between two vectors of the same class would now be 0.5, while the expected distance between two vectors of opposite classes would be 1.5. As this increases several orders of magnitude, a point will eventually be reached where, due to the sparsity of the dataset, the probability that the nearest neighbor of a query vector is of the same class is effiectively equal to the probability hat it belongs to the opposite class.\n\nBut is this particular issue the problem that the KNN model is encountering when trained on the original dataset? Note that when images are resized to a resolution of 128x128x3 or 256x256x3, the dimensionality of the dataset is `128*128*3=49_152` and` 256*256*3=196_608` respectively, both comparable to the `~100_000` dimensions that were evaluated in the above dataset. The size of both datasets is comparable, also. What's unclear is whether the main dataset contains a comparable number of irrelevant features as the mock dataset.\n\nTo provide some insight to answer this question, the average Euclidean distance between images in the test set to their nearest neighbors will be calculated. ","53686c02":"Given the above images it's astonishing that the model achieves comparable accuracy when presented with an amorphous blob of pixels than when it's presented with an image that a human could be expected to quite confidently classify as a cat. Even with a resolution of `16x16x3` the image is difficult to decipher, and only when the image has a resolutuon of `64x64x3` would many people be able to confidently identify the object in the image as a cat.\n\nThus, it's not immediately evident (at least for those without much experience with KNNs) why a KNN seems to be as capable at properly classifying a pixelated mess as it is an unambiguous picture of an animal. The KNN is able to form _some_ model of the data at both levels of resolutions, but why is it unable to achieve a satisfactory model?\n\nFirst, it's important to understand the straightforward, but fundamental assumption that the KNN algorithm makes: __Similar points are located in close proximity in their representational space.__\n\nAt first, that might not seem like a problem. This model converts images into their RGB-pixel representations, and in theory these vector representations will be located in their representational space closer to images of the same class than those of the opposite class. A novel picture in the test set can be reasonably expected to look similar to at least one picture that was provided in the training set, and these images will have a relatively small Euclidean distance between them, as many of their pixels will have similar values. Perhaps there will be a few pictures provided in the test set that aren't similar in apperance to any pictures in the training set, but that should be an overwhelming minority, so the accuracy of the model should be much higher than ~60%. So why is the performance so bad?\n\nLet's take a look at an imaginary dataset that contains examples of cats and dogs, but instead of images the dataset contains written features.","cf968c72":"Interestingly, image resolution doesn't seem to have a significant effect on the performance of the KNN model. When `k == 400`, for example, the model performs best when the images are resized to `16x16x3`, with an accuracy of 62% compared to the 60% accuracy the model achieves when the images are resized to `256x256x3`. Out of the five evaluated values for `k`, the accuracy of the model differs by less than 1% between when the resolution is `16x16x3` versus `256x256x3` for 3 of the `k` values, and accuracy drops by more than 1% for the other two `k` values. That is, none of the five `k` values have an increase in accuracy from `16x16x3 -> 256x256x3` of 1% or greater. \n\nGiven these findings it would be erroneous to assume that further increasing the reoslution of the images provided to the model will lead to better performance. In fact, a more sound assumption based on the data would be that further decreasing the image resolution will lead to better model performance. \n\nThe KNN models evaluated above will be trained again, but this time on images of resolutions of `2x2x3`, `4x4x3`, and `8x8x3`.","978fd1c3":"When `k` is equal to 200 and greater, the accuracy of the model is able to reach 61%, and when `k == 400`, the model is no longer significantly biased towards classifying images as cats. While 61% accuracy is far from ideal for a model trained on this dataset, it's at least better than the 56% accuracy that was seen with lower values of `k`, and the elimination of the classification bias is promising as well. \n\nNow, with `k == 400`, it will be interesting to see how performance metrics are affected by altering the resolution of the images provided to the model. If the model isn't able to achieve satisfactory accuracy even after being provided with much higher resolution of images, it could be a sign that there is a serious flaw somewhere in the model, or it could simply mean that KNN isn't well-suited for this particular dataset.","e62392ae":"Above, a basic function for converting a JPG filename to a NumPy array is provided, `jpg_to_arr()`. It uses the OpenCV library to read in the image as an array, as creating a utility to do that from scratch would introduce an excessive amount of boilerplate code.\n\nThe function is simple to understand from the provided documentation, but it is important to understand the necessity of resizing the image to be an array of a certain size. The distance function below will help to elucidate the purpose."}}