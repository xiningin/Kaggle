{"cell_type":{"bd5b4afd":"code","c47efd58":"code","89630994":"code","ae480a91":"code","c0acfa5b":"code","6e4abf8a":"code","f1194e9c":"code","65d61bad":"code","f2967ac2":"code","3f36e0dd":"code","fee443d9":"code","bf890128":"code","4150024b":"code","fe341005":"code","ba10c217":"code","50cc1941":"code","9b3d99ed":"code","674228bc":"code","9bb2341e":"code","82550815":"code","a079b88b":"code","9d2b0dd3":"code","384103bc":"code","1fe9295c":"code","a82095c8":"code","972a093f":"code","2f8c6054":"code","63d67d7d":"code","8692c405":"code","f8751757":"code","61c2abb8":"code","70ee2122":"code","9c5c7209":"code","23ef08ba":"code","a13bb5f9":"code","8bc01499":"code","4679a2a1":"code","495c0abb":"code","5526cbfd":"code","af8f4940":"code","4cc31135":"code","08029bd6":"code","3cd6e07c":"code","f1ebcaf5":"code","a555f77a":"code","8048a3db":"code","159ed4d4":"code","10b845ba":"code","3f41c67e":"code","b4cec521":"code","5587366a":"code","25e4fc74":"code","56f13749":"code","e1c10af2":"code","b0b42875":"code","b98dee68":"code","9f2eba73":"code","6b5c434e":"code","1a504c68":"code","ecbcd874":"markdown","5ba5706d":"markdown","84c74078":"markdown","ca965605":"markdown","f4a73502":"markdown","6227b692":"markdown","a680d7ac":"markdown","5d437413":"markdown","0191383b":"markdown","e1317852":"markdown","34d82b6c":"markdown","039bea6f":"markdown","3ae35777":"markdown","026a3e0d":"markdown","4da6c273":"markdown","1c06d12a":"markdown","4b30d95d":"markdown","781cf4c2":"markdown","fec3ef5c":"markdown","fb33fb98":"markdown","0feedd1a":"markdown","7fb8e6b1":"markdown","194cbf3c":"markdown","11541fd9":"markdown","d4829e0c":"markdown","058012fe":"markdown","5177e148":"markdown","ee5b50ad":"markdown","2f2d9b85":"markdown","dcbcf24c":"markdown","a95959c0":"markdown","52362121":"markdown","8f9b0f0c":"markdown","9104fbcb":"markdown","d453510b":"markdown","b0d7afe8":"markdown","5f8d5533":"markdown","16d7ef0c":"markdown","ecb80859":"markdown"},"source":{"bd5b4afd":"import numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c47efd58":"# import moduls\nimport seaborn as sns\nsns.set_style('darkgrid') # set grid for all graphs\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import Counter","89630994":"data = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","ae480a91":"data.head()","c0acfa5b":"data.shape","6e4abf8a":"data.info()","f1194e9c":"data.isnull().sum()","65d61bad":"fig = plt.figure()\nax = fig.add_subplot()\nax.set(title='Distribution of target variable')\ndata.output.value_counts().plot(kind='pie', autopct=\"%.2f\")\nplt.show()","f2967ac2":"for col in data.columns:\n    n = data[col].nunique()\n    print(col + \" has %s unique values\" %n)","3f36e0dd":"num_cols = [col for col in data.columns if data[col].nunique() > 5]\ncat_cols = [col for col in data.columns if data[col].nunique() <= 5]\nprint('Numeric columns are:', num_cols, 'Categorical columns are:', cat_cols, sep='\\n')","fee443d9":"data[num_cols].describe()","bf890128":"sns.pairplot(data, vars=num_cols, hue='output', corner=True)\nplt.show()","4150024b":"sns.histplot(data=data, x=\"thalachh\", hue=\"output\", kde=True)\nplt.show()","fe341005":"cat_data = data.copy()","ba10c217":"cat_data.head()","50cc1941":"# function for calculation max heart rate ('thalachh') normal\/critical for person\n# it calculates as: 220 - age, and then compare with value in 'thalachh' columns\n\ndef calc_max_pulse(col_max_heart_rate, col_age):\n    x = np.array(col_max_heart_rate)\n    y = np.array(col_age)\n    changed_array = []\n    for n in range(len(x)):\n        if (x[n]) <= (220 - y[n]):\n            changed_array.append('normal')\n        else: \n            changed_array.append('critical')\n    return changed_array","9b3d99ed":"## binning of numerical variables\n\ncat_data['trtbps'] = pd.cut(cat_data['trtbps'], bins = [0, 120, 129, 139, 159, 179, 10000], labels = ['Optimal', 'Normal', 'High normal', \n                                                                                              'Grade 1 hypertension', 'Grade 2 hypertension',\n                                                                                              'Grade 3 hypertension'])\n\ncat_data['thalachh'] = calc_max_pulse(cat_data['thalachh'], cat_data['age'])\n\ncat_data['age'] = pd.cut(cat_data['age'], bins = [0,45,60,200], labels = ['Adults','Mid Adults','Elderly'])\n\ncat_data['chol'] = pd.cut(cat_data['chol'], bins = [0, 200, 239, 600], labels = ['Ideal', 'Borderline high', 'High'])","674228bc":"cat_data['sex'].replace({0: 'female', 1: 'male'},inplace=True)\ncat_data['cp'].replace({1: 'typical angina', 2: 'atypical angina', 3: 'non-anginal pain', 0: 'asymptomatic'},inplace=True)\ncat_data['output'].replace({0: 'less risk', 1: 'risk'},inplace=True)\ncat_data['fbs'].replace({0: 'blood sugar less 120 mg\/dl', 1: 'blood sugar more 120 mg\/dl'},inplace=True)\ncat_data['restecg'].replace({1: 'normal', 2: 'having ST-T wave abnormality', 0: 'hypertrophy'},inplace=True)\ncat_data['exng'].replace({0: 'no exercise', 1: 'exercise induced angina'},inplace=True)\ncat_data['slp'].replace({0: 'downsloping', 1: 'flat', 2: 'upsloping'},inplace=True)\ncat_data['caa'].replace({0: '0 major vessel', 1: '1 major vessel', 2: '2 major vessels', 3: '3 major vessels', 4: '4 major vessels'},inplace=True)\ncat_data['thall'].replace({1: 'fixed defect', 2: 'normal', 3: 'reversable defect'},inplace=True)","9bb2341e":"fig = plt.figure(figsize=(25,25))\nn = 1\nfor column in cat_data.drop('oldpeak', axis=1):\n    ax = plt.subplot(4,4,n)\n    sns.countplot(x='output', hue=column, data=cat_data)\n    n += 1\nplt.show()","82550815":"cat_data[(cat_data['thall'] == 0)]","a079b88b":"cat_data.groupby(['output', 'thall']).agg({'output': 'count'})","9d2b0dd3":"cat_data.loc[((cat_data['thall'] == 0) & (cat_data['output'] == 'less risk')), ('thall')] = 'reversable defect'","384103bc":"cat_data.loc[((cat_data['thall'] == 0) & (cat_data['output'] == 'risk')), ('thall')] = 'normal'","1fe9295c":"cat_data.groupby(['output', 'thall']).agg({'output': 'count'})","a82095c8":"fig = plt.figure(figsize=(15,15))\nsns.heatmap(data.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.show()","972a093f":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder","2f8c6054":"Counter(data.output)","63d67d7d":"X = data.drop('output', axis=1)\ny = data.output","8692c405":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","f8751757":"scaled_X_train = X_train.copy()","61c2abb8":"scaled_X_train.head()","70ee2122":"array = ['age', 'trtbps', 'chol', 'thalachh']\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(scaled_X_train[array])\nnormalize_results = pd.DataFrame(X_train_scaled, index=scaled_X_train.index, columns=array)\nscaled_X_train.drop(array, axis=1, inplace=True)\nscaled_X_train = scaled_X_train.join(normalize_results)","9c5c7209":"scaled_X_train.head()","23ef08ba":"encodered_X_train = X_train.copy()","a13bb5f9":"encodered_X_train['trtbps'] = pd.cut(encodered_X_train['trtbps'], bins = [0, 120, 129, 139, 159, 179, 10000], labels = ['Optimal', 'Normal', 'High normal', \n                                                                                              'Grade 1 hypertension', 'Grade 2 hypertension',\n                                                                                              'Grade 3 hypertension'])\n\nencodered_X_train['thalachh'] = calc_max_pulse(encodered_X_train['thalachh'], encodered_X_train['age'])\n\nencodered_X_train['age'] = pd.cut(encodered_X_train['age'], bins = [0,45,60,200], labels = ['Adults','Mid Adults','Elderly'])\n\nencodered_X_train['chol'] = pd.cut(encodered_X_train['chol'], bins = [0, 200, 239, 600], labels = ['Ideal', 'Borderline high', 'High'])","8bc01499":"encoder = LabelEncoder()\nfor title in array:\n    encodered_X_train[title] = encoder.fit_transform(encodered_X_train[title])","4679a2a1":"# IT CAN BE USED INSTEAD OF LabelEncoder()\n\n# col_for_dummy = ['age', 'cp', 'trtbps', 'chol', 'thalachh', 'slp', 'caa', 'thall']\n# dummy_df = pd.get_dummies(encodered_X_train[col_for_dummy])\n# encodered_X_train.drop(col_for_dummy, axis=1, inplace=True)\n# dummy_X_train = pd.concat([encodered_X_train, dummy_df], axis=1)\n# encodered_X_train = dummy_X_train","495c0abb":"encodered_X_train.head()","5526cbfd":"X_train.head()","af8f4940":"# models\nfrom sklearn.ensemble import (RandomForestClassifier, \n                              AdaBoostClassifier, \n                              GradientBoostingClassifier)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# metrics\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, \n                             f1_score, roc_auc_score, confusion_matrix, \n                             precision_recall_curve, roc_curve, \n                             plot_precision_recall_curve, plot_confusion_matrix)\n\n# for regular expressions\nimport re ","4cc31135":"models = [RandomForestClassifier(), GradientBoostingClassifier(), LogisticRegression(), KNeighborsClassifier(), AdaBoostClassifier(),\n         DecisionTreeClassifier(), SVC(probability = True), XGBClassifier(eval_metric = 'logloss'), LGBMClassifier()]\n\nmodel_names = []\ntrain_data = [X_train, scaled_X_train, encodered_X_train]\n\nfor mod_name in models:\n    r = re.findall((r'\\w*'), str(mod_name))\n    model_names.append(r[0])\n    \nfor n in range(len(models)):\n    clf = models[n]\n    scores = cross_val_score(clf, train_data[0], y_train, cv=5)\n    print('Algorithm is: %s' %model_names[n])\n    print('Raw data accuracy:', scores.mean().round(3))\n    scores = cross_val_score(clf, train_data[1], y_train, cv=5)\n    print('Scaled data accuracy:', scores.mean().round(3))\n    scores = cross_val_score(clf, train_data[2], y_train, cv=5)\n    print('Encodered data accuracy:', scores.mean().round(3))\n    print()","08029bd6":"def cross_valid_scores(models_array, X_tr, y_tr):\n    \n    accuracy = []\n    precision = []\n    recall = []\n    f1 = []\n    auc = []\n        \n    scoring = ['precision_macro', 'recall_macro', 'f1_macro', 'accuracy', 'roc_auc']\n\n    for n in models:\n        clf = n\n        scores = cross_validate(clf, X_tr, y_tr, cv=5, scoring=scoring)\n\n        acc_mean = scores['test_accuracy'].mean().round(3)\n        prec_mean = scores['test_precision_macro'].mean().round(3)\n        rec_mean = scores['test_recall_macro'].mean().round(3)\n        f1_mean = scores['test_f1_macro'].mean().round(3)\n        roc_mean = scores['test_roc_auc'].mean().round(3)\n\n        accuracy.append(acc_mean*100)\n        precision.append(prec_mean*100)\n        recall.append(rec_mean*100)\n        f1.append(f1_mean*100)\n        auc.append(roc_mean*100)\n        \n    results_df = pd.DataFrame({\"Accuracy Score\":accuracy,\"Precision Score\":precision,\n                        \"Recall Score\":recall, \"f1 Score\":f1,\"AUC Score\":auc,\n                        \"Algorithm\": model_names})\n    \n    results_df = (results_df.sort_values(by = 'AUC Score', ascending = False)\n                  .reset_index(drop =  True))\n    \n    return results_df","3cd6e07c":"scaled_X_test = X_test.copy()\nX_test_scaled = scaler.transform(scaled_X_test[array])\nnormalize_results = pd.DataFrame(X_test_scaled, index=scaled_X_test.index, columns=array)\nscaled_X_test.drop(array, axis=1, inplace=True)\nscaled_X_test = scaled_X_test.join(normalize_results)","f1ebcaf5":"scaled_X_test.head()","a555f77a":"cross_valid_scores(models, scaled_X_train, y_train)","8048a3db":"param_grid = {'C': np.arange(1e-05, 3, 0.1)}\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss', 'Recall': 'recall'}\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), return_train_score=True,\n                  param_grid=param_grid, scoring=scoring, cv=5, refit='Log_loss')\n\ngrid_log_reg.fit(scaled_X_train, y_train)","159ed4d4":"grid_log_reg.cv_results_","10b845ba":"grid_log_reg.best_params_","3f41c67e":"best_clf = grid_log_reg.best_estimator_","b4cec521":"best_clf.score(scaled_X_train, y_train).round(2)*100","5587366a":"scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n\nfor n in scoring:\n    scores = cross_val_score(best_clf, scaled_X_train, y_train, cv=5, scoring=n).mean()\n    print(n, round((scores),2)*100)","25e4fc74":"prediction = best_clf.predict(scaled_X_test)\nprobability = best_clf.predict_proba(scaled_X_test)","56f13749":"probability_train = best_clf.predict_proba(scaled_X_train)","e1c10af2":"print('Accuracy: ', round(accuracy_score(y_test, prediction),2)*100)\nprint('Precision: ', round(precision_score(y_test, prediction),2)*100)\nprint('Recall: ', round(recall_score(y_test, prediction),2)*100)\nprint('F1-score: ', round(f1_score(y_test, prediction),2)*100)\nprint()\nprint('Confusion matrix: ', confusion_matrix(y_test, prediction), sep='\\n')","b0b42875":"train_auc = roc_auc_score(y_train, probability_train[:, 1])\ntest_auc = roc_auc_score(y_test, probability[:, 1])\n\nplt.figure()\nplt.plot(*roc_curve(y_train, probability_train[:, 1])[:2], label='train AUC={:.4f}'.format(train_auc))\nplt.plot(*roc_curve(y_test, probability[:, 1])[:2], label='train AUC={:.4f}'.format(test_auc))\n\nlegend_box = plt.legend(fontsize='large', framealpha=1).get_frame()\nlegend_box.set_facecolor('white')\nlegend_box.set_edgecolor('black')\nplt.plot(np.linspace(0,1,100), np.linspace(0,1,100))\n\nplt.show()","b98dee68":"pd.Series(probability[:, 1]).hist()\nplt.show()","9f2eba73":"plot_precision_recall_curve(best_clf, scaled_X_test, y_test)","6b5c434e":"prediction_with_threshold = np.where(probability[:, 1] > 0.4, 1, 0)","1a504c68":"print('Accuracy with adjusted threshold: ', round(accuracy_score(y_test, prediction_with_threshold),2)*100)\nprint('Precision with adjusted threshold: ', round(precision_score(y_test, prediction_with_threshold),2)*100)\nprint('Recall with adjusted threshold: ', round(recall_score(y_test, prediction_with_threshold),2)*100)\nprint('F1-score with adjusted threshold: ', round(f1_score(y_test, prediction_with_threshold),2)*100)\nprint()\nprint('Confusion matrix with adjusted threshold: ', confusion_matrix(y_test, prediction_with_threshold), sep='\\n')","ecbcd874":"# Logistic Regression tune with Grid Search","5ba5706d":"# Preparation data for modeling","84c74078":"Historgam of predicted probabilities. We can see the highest area between 0.4-0.5 in flat middle. I will adjust threshold = 0.4 to include this area in posotive class","ca965605":"Import modeling moduls","f4a73502":"# Discription of variables in data set","6227b692":"We have 2 observations. One in risk col and another in less risk col. I will change \"thall = 0\" values to most frequent in \"risk\" and \"less risk\" groups","a680d7ac":"**Here I would like to examine dependance \"chance of heart attack\" and other features. But at first I have to convert some numeric data to categorical (do binning)**","5d437413":"***We do scaling after splitting for excluding data leakage in test data!!***","0191383b":"# Modeling","e1317852":"# Binning and rename values","34d82b6c":"Do the same I did before in binning section","039bea6f":"* The strongest correlation between features \"slp\" and \"oldpeak\". And it's negative correlation\n* Target feature \"output\" has stronger possitive correlation with \"cp\", \"thalach\" and stronger negative correlation with \"exng\", \"oldpeak\".","3ae35777":"> [](http:\/\/)","026a3e0d":"# Loading data and first sight at data frame","4da6c273":"1. age - age in years\n\n2. sex - sex (1 = male; 0 = female)\n\n3. cp - chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 0 = asymptomatic)\n\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n\n5. chol - serum cholestoral in mg\/dl\n\n6. fbs - fasting blood sugar > 120 mg\/dl (1 = true; 0 = false)\n\n7. restecg - resting electrocardiographic results (1 = normal; 2 = having ST-T wave abnormality; 0 = hypertrophy)\n\n8. thalach - maximum heart rate achieved\n\n9. exang - exercise induced angina (1 = yes; 0 = no)\n\n10. oldpeak - ST depression induced by exercise relative to rest\n\n11. slope - the slope of the peak exercise ST segment (2 = upsloping; 1 = flat; 0 = downsloping)\n\n12. ca - number of major vessels (0-3) colored by flourosopy\n\n13. thal - 2 = normal; 1 = fixed defect; 3 = reversable defect\n\n14. num - the predicted attribute - diagnosis of heart disease (angiographic disease status) (Value 0 = < diameter narrowing; Value 1 = > 50% diameter narrowing)\n\nThanks to [jaykumar1607](http:\/\/www.kaggle.com\/jaykumar1607)","1c06d12a":"**Conclusions:**\n* It doesn't look like any variables have correlation. Maybe only 'thalachh' and 'age' have light negative correlation.\n* Interesting histigrams of 'oldpeak'. Most of observations have low 'oldpeak' (< 2) but most of all have chance of heart attack.\n* Interesting histigrams of 'thalachh': we can say that maximum heart rate can increase chance of heart attack.","4b30d95d":"Check correlations between features (I use main data without binning)","781cf4c2":"**Rename features for more informative graphs**","fec3ef5c":"About this dataset\nAge : Age of the patient\n\nSex : Sex of the patient\n\nexang: exercise induced angina (1 = yes; 0 = no)\n\nca: number of major vessels (0-3)\n\ncp : Chest Pain type chest pain type (Value 1: typical angina\nValue 2: atypical angina\nValue 3: non-anginal pain\nValue 4: asymptomatic)\n\ntrtbps : resting blood pressure (in mm Hg)\n\nchol : cholestoral in mg\/dl fetched via BMI sensor\n\nfbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\nrest_ecg : resting electrocardiographic results\n\nValue 0: normal\nValue 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\nValue 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\nthalach : maximum heart rate achieved\n\ntarget : 0= less chance of heart attack 1= more chance of heart attack","fb33fb98":"We see here that some of variables have 2-5 unique values. I will treat it like categorical variables. Collect numeric and categorical columns separetly","0feedd1a":"KNN and SVC - sensitive for preparing data but in other cases deference is not significant. I prefer use scaled data","7fb8e6b1":"# Prediction","194cbf3c":"# Gain Recall to 95% by specifying threshold for positive class","11541fd9":"As we can see target variable is balanced and we haven't to do \"undersampling\" or \"oversampling\" things. Just separate our data with \"train_test_split\"","d4829e0c":"# Visialization","058012fe":"# Summary:\n* loaded few modules and looked through dataset\n* binned some features\n* visulaization of few relationships bewteen variables and individual variables.\n* feature correaltion visualization\n* prepared and compared dataset with different scaling\n* chose algorithm with cross-validation\n* tuned model with GridSearchCV\n* gained recall to the specified value (95%).","5177e148":"**Conclusions:** \n* Women more vulnerable than men\n* Chest pain (cp) angina (typical\/atypical) can indicate people who have a risk\n* A bit confusing: we have more risk obsertation in \"Optimal\" blood pressure (trtbps) group and \"Normal\" group in esting electrocardiographic results (restecg). thall = \"normal\" the same\n* Cholesterol and blood sugar don't informative for us\n* \"Exng\", \"slp\", \"caa\" have one strong tendence in risk group\n* Some strange value for feature 'thall' = 0. It wasn't described anywhere.\n\n\nI would say this plots a bit confusing. I thought that it would be some correlation between high rate of heart rate and cholesterol or elderly people would have more chance to be in risk but it doesn't observed.","ee5b50ad":"**Thanks for reading. Upvote, if this notebook was useful or interesting for you**","2f2d9b85":"Normalized test set","dcbcf24c":"Distribution of numeric data","a95959c0":"Data doesn't have missing values","52362121":"This is precision-recall curve where we can see dependence between these variables","8f9b0f0c":"I will maximaze \"Recall score\" because in my opinion, we don't have to miss positvite class (heart risk). Below I'll use Logistic Regression for final model. ","9104fbcb":"**Not all variables were described on dataset page, I took this one from [disscusions](http:\/\/www.kaggle.com\/rashikrahmanpritom\/heart-attack-analysis-prediction-dataset\/discussion\/234843):**","d453510b":"We have only numeric (int, float) types","b0d7afe8":"**I would like check difference between raw data, normalized and binning data. I will do 2 new data frame. First (\"scaled_X_train\") with normalized train data (using Standart Scaler), second (\"encodered_X_train\") with transformed numeric columns to categorical (binning like I did above and then use Label Encoder). And compare the results.**","5f8d5533":"Now we have 3 dataset for modeling","16d7ef0c":"Firstly, let's check which dataset fits more","ecb80859":"\n[Binning blood pressure](http:\/\/en.wikipedia.org\/wiki\/Blood_pressure)\n\n[Age binnig](http:\/\/kidspicturedictionary.com\/english-through-pictures\/people-english-through-pictures\/age-physical-description\/) thanks to [bhuvanchennoju](http:\/\/www.kaggle.com\/bhuvanchennoju) and his [great notebook](https:\/\/www.kaggle.com\/bhuvanchennoju\/data-stroytelling-auc-focus-on-strokes#Summary)\n\n[Cholesterol level](http:\/\/www.cholesterolmenu.com\/cholesterol-levels-chart\/)\nI suspect that our data['chol'] is 'Total cholesterol'"}}