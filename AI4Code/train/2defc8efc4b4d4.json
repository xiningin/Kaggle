{"cell_type":{"dde88a39":"code","97c5cc3a":"code","7f9fd80f":"code","69d4adf2":"code","d0a6ab76":"code","7645bbc4":"code","346318a1":"code","4dcb73e6":"code","f5e03cba":"code","83af17fa":"code","fbebbe8d":"code","7fa35ca2":"code","4594a238":"code","098e2d5e":"code","57100a9e":"code","4e481baf":"code","5053abe9":"code","fe21f28f":"code","4eefb4d9":"code","ab824c02":"code","454d1d86":"code","55ce3d72":"code","ef189e42":"markdown","51fbd540":"markdown","458c5511":"markdown","d4fa9f2c":"markdown","300ad83e":"markdown","3c6babb5":"markdown","eb202491":"markdown","9ab744fd":"markdown"},"source":{"dde88a39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom pandas import datetime\nimport math, time\nimport itertools\nfrom sklearn import preprocessing\nimport datetime\nfrom operator import itemgetter\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import load_model\nimport keras\nimport h5py\nimport requests\nimport os\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","97c5cc3a":"data_df =  pd.read_csv(\"..\/input\/prices-split-adjusted.csv\", index_col = 0)\ndata_df.head()","7f9fd80f":"df2 = pd.read_csv(\"..\/input\/fundamentals.csv\")\ndf2.head()","69d4adf2":"data_df = data_df[data_df.symbol == 'AAPL']\ndata_df.drop(['symbol'],1,inplace=True)\ndata_df.head()","d0a6ab76":"data_df.shape","7645bbc4":"data_df.tail()","346318a1":"plt.plot(data_df['close'])\nplt.show()","4dcb73e6":"data_df['date'] = data_df.index\ndata_df.head()","f5e03cba":"data_df['date'] = pd.to_datetime(data_df['date'])","83af17fa":"data_df.head()","fbebbe8d":"min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\ndataset = min_max_scaler.fit_transform(data_df['close'].values.reshape(-1, 1))","7fa35ca2":"dataset[0:10]","4594a238":"# split into train and test sets\ntrain_size = int(len(dataset) * 0.7)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\nprint(len(train), len(test))","098e2d5e":"print(len(data_df))\nprint(1233 + 529)","57100a9e":"# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=15):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)","4e481baf":"x_train, y_train = create_dataset(train, look_back=15)\nx_test, y_test = create_dataset(test, look_back=15)","5053abe9":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","fe21f28f":"x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\nx_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))","4eefb4d9":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","ab824c02":"# create and fit the LSTM network\nlook_back = 15\nmodel = Sequential()\nmodel.add(LSTM(20, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(x_train, y_train, epochs=20, batch_size=1, verbose=2)","454d1d86":"trainPredict = model.predict(x_train)\ntestPredict = model.predict(x_test)\n# invert predictions\ntrainPredict = min_max_scaler.inverse_transform(trainPredict)\ntrainY = min_max_scaler.inverse_transform([y_train])\ntestPredict = min_max_scaler.inverse_transform(testPredict)\ntestY = min_max_scaler.inverse_transform([y_test])\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","55ce3d72":"# shift train predictions for plotting\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n# shift test predictions for plotting\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(min_max_scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","ef189e42":">reference: https:\/\/machinelearningmastery.com\/time-series-prediction-lstm-recurrent-neural-networks-python-keras\/","51fbd540":"**LSTM**\n\nThe Long Short-Term Memory network, or LSTM network, is a recurrent neural network that is trained using Backpropagation Through Time and overcomes the vanishing gradient problem.\n\nAs such, it can be used to create large recurrent networks that in turn can be used to address difficult sequence problems in machine learning and achieve state-of-the-art results.\n\nInstead of neurons, LSTM networks have memory blocks that are connected through layers.\n\nA block has components that make it smarter than a classical neuron and a memory for recent sequences. A block contains gates that manage the block\u2019s state and output. A block operates upon an input sequence and each gate within a block uses the sigmoid activation units to control whether they are triggered or not, making the change of state and addition of information flowing through the block conditional.\n\nThere are three types of gates within a unit:\n\n* Forget Gate: conditionally decides what information to throw away from the block.\n* Input Gate: conditionally decides which values from the input to update the memory state.\n* Output Gate: conditionally decides what to output based on input and the memory of the block.\n\nEach unit is like a mini-state machine where the gates of the units have weights that are learned during the training procedure.","458c5511":"The type of Neural Network designed to handle sequence dependence is called recurrent neural networks(RNN). \n\nThe Long Short-Term Memory network or LSTM network is a type of recurrent neural network used in deep learning because very large architectures can be successfully trained","d4fa9f2c":"The LSTM network expects the input data (X) to be provided with a specific array structure in the form of: [samples, time steps, features].\n\nCurrently, this data is in the form: [samples, features] and we are framing the problem as one time step for each sample. We can transform the prepared train and test input data into the expected structure using numpy.reshape() as follows:","300ad83e":"I set the look back date as 15 days, which is the number of previous time steps to use as input variables to predict the next time period","3c6babb5":"> LSTMs are sensitive to the scale of the input data, specifically when the sigmoid (default) or tanh activation functions are used. It can be a good practice to rescale the data to the range of 0-to-1, also called normalizing. ","eb202491":"I am extracting the apple stocks only: AAPL","9ab744fd":">The network has a visible layer with 1 input, a hidden layer with 20 LSTM blocks or neurons, and an output layer that makes a 15 value prediction. The default sigmoid activation function is used for the LSTM blocks. The network is trained for 20 epochs and a batch size of 1 is used."}}