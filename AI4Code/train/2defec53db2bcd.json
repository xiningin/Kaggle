{"cell_type":{"e3e0083d":"code","703d66fb":"code","36412ada":"code","7b2bc349":"code","b78d8117":"code","a2f604e9":"code","5307fc38":"code","32270dba":"code","fb0288e2":"code","6eb4699b":"code","fa214c76":"code","8124672e":"code","a4d376bb":"code","83439edf":"code","1da193b1":"code","6cdcc1fa":"code","97016d46":"code","ae028ede":"code","09595537":"code","ed43facd":"code","f1cc194c":"code","5d712e6a":"code","6740d515":"code","8065775a":"code","1aa81d30":"code","cdd3d42c":"code","d37d9d9e":"code","c40f0476":"code","60459960":"code","eeea2f6f":"code","497c5211":"code","628c8f66":"code","8e870da7":"code","e26fe575":"code","59890077":"code","85983d44":"code","213d7dea":"code","19bacb89":"code","60d47395":"markdown","024b1964":"markdown","70e1c82b":"markdown","1c50358d":"markdown","01541b7c":"markdown","55ec5ba4":"markdown","8500106b":"markdown","b82dab93":"markdown","81282657":"markdown","430d931f":"markdown","a300219c":"markdown","b3ce1736":"markdown","9b58f511":"markdown","ce2ecc67":"markdown","13c95036":"markdown","2b4093ea":"markdown","ad2a1dd7":"markdown","7fa31f5d":"markdown","4eb383bf":"markdown","f7472421":"markdown","f6ff805f":"markdown","99bdd201":"markdown","c9259d70":"markdown","1da52955":"markdown","8ef904b5":"markdown","c9afb0cd":"markdown","dca4bd57":"markdown","4c5578d2":"markdown","888745e0":"markdown","1e695d87":"markdown","c8953b8e":"markdown"},"source":{"e3e0083d":"import tensorflow as tf\nimport numpy as np\nimport os\nimport time","703d66fb":"path_to_file = tf.keras.utils.get_file('The Jungle Book.txt', 'https:\/\/www.gutenberg.org\/files\/236\/236-0.txt')\ntext = open(path_to_file, 'rb').read().decode(encoding='utf-8')","36412ada":"start_index = text.find(\"START OF THIS PROJECT GUTENBERG\")\nend_index = text.find('End of the Project Gutenberg')\ntext = text[start_index : end_index]","7b2bc349":"text[:1000]","b78d8117":"# The unique characters in the file\nvocab = sorted(set(text))\nprint('Length of text: {} characters.'.format(len(text)))\nprint('Unique characters: {}'.format(len(vocab)))","a2f604e9":"char2idx = {char:i for i, char in enumerate(vocab)}\nidx2char = np.array(vocab)","5307fc38":"text_encoded = [char2idx[c] for c in text]\ntext_encoded = np.array(text_encoded)","32270dba":"# Show how the first 31 characters from the text are mapped to integers\nprint('Text: {} \\n==> Encoded as : {}'.format(text[:31], text_encoded[:31]))","fb0288e2":"# The maximum length sentence we want for a single input in characters\nseq_length = 100\nexample_per_epoch = len(text)\/\/seq_length # as we have 1 example of seq_length characters.","6eb4699b":"# Create training examples \/ targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_encoded)","fa214c76":"for i in char_dataset.take(5):\n    print(idx2char[i.numpy()])","8124672e":"sequences = char_dataset.batch(batch_size=seq_length+1, drop_remainder=True)","a4d376bb":"for item in sequences.take(5):\n    print(repr(''.join(idx2char[item.numpy()]))) # repr function print string representation of an object.","83439edf":"# Always try to make preprocessing function for one input and after apply it on whole list by using map or apply.\ndef split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text","1da193b1":"dataset = sequences.map(split_input_target)","6cdcc1fa":"for input_ex, target_ex in dataset.take(1):\n    print('Input data: ', repr(''.join(idx2char[input_ex.numpy()])))\n    print('Output data:', repr(''.join(idx2char[target_ex.numpy()])))","97016d46":"for i, (input_idx, target_idx) in enumerate(zip(input_ex[:5], target_ex[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"Input : {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n    print(\"Expected output : {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))","ae028ede":"# Batch size \nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000","09595537":"dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\ndataset","ed43facd":"# Length of vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units\nrnn_units = 1024","f1cc194c":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(input_dim=vocab_size,\n                                  output_dim = embedding_dim,\n                                  batch_input_shape = [batch_size, None]),\n        tf.keras.layers.GRU(units = rnn_units,\n                            return_sequences= True,\n                            stateful=True,\n                            recurrent_initializer='glorot_uniform'),\n        tf.keras.layers.Dense(vocab_size)\n    ])\n    \n    return model","5d712e6a":"model = build_model(vocab_size = len(vocab),\n                    embedding_dim = embedding_dim,\n                    rnn_units = rnn_units,\n                    batch_size = BATCH_SIZE)","6740d515":"model.summary()","8065775a":"# First check the shape of the output:\nfor input_ex_batch, target_ex_batch in dataset.take(1):\n    ex_batch_prediction = model(input_ex_batch) # simply it takes input and calculate output with initial weights.\n    print(ex_batch_prediction.shape, \"# (batch_size, sequence_length, vocab_size)\")","1aa81d30":"sampled_indices = tf.random.categorical(ex_batch_prediction[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()","cdd3d42c":"sampled_indices","d37d9d9e":"print('Input: \\n', repr(\"\".join(idx2char[input_ex_batch[0].numpy()])))\nprint('\\nPredicted sequence for next characters is: \\n', repr(\"\".join(idx2char[sampled_indices])))","c40f0476":"def loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nex_batch_loss = loss(target_ex_batch, ex_batch_prediction)\nprint(\"Prediction shape: \", ex_batch_prediction.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"Scaler loss: \", ex_batch_loss.numpy().mean())","60459960":"model.compile(optimizer='adam', loss=loss)","eeea2f6f":"# Directory where the checkpoints will be saved\ncheckpoint_dir = '.\/training_checkpoints'\n\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint( filepath=checkpoint_prefix, save_weights_only=True)","497c5211":"EPOCHS = 10","628c8f66":"history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","8e870da7":"tf.train.latest_checkpoint(checkpoint_dir)","e26fe575":"model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nmodel.build(tf.TensorShape([1, None])) # Builds the model based on input shapes received.","59890077":"model.summary()","85983d44":"def generate_text(model, start_string, num_generate, temperature):\n    \n    # Converting our start string to numbers (vectorizing)\n    input_eval = [char2idx[s] for s in start_string]\n    # convert (x,y) shaped matrix to (1,x,y).\n    input_eval = tf.expand_dims(input_eval, axis=0) \n    \n    # Empty string to store our results\n    text_generated = []\n    \n    # Here batch size == 1\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        \n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n        \n        # using a categorical distribution to predict the \n        # character returned by the model\n        predictions = predictions \/ temperature\n        \n        # We got the predictions for every timestep but we \n        # want only last so first we take [-1] to consider on last \n        # predictions distribution only and after we try to get id \n        # from 1D array. Ex. we got '2' from a=['2'] by a[0].\n        predicted_id = tf.random.categorical(predictions, \n                                             num_samples=1\n                                            )[-1,0].numpy()\n        \n        # We pass the predicted character as the next input to the \n        # model along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n        \n        text_generated.append(idx2char[predicted_id])\n        \n    return (start_string + ''.join(text_generated))","213d7dea":"print(generate_text(model, \n                    start_string=u\"The moon was sinking behind the\", \n                    num_generate=500, temperature=1.0))","19bacb89":"print(generate_text(model, \n                    start_string=u\"The moon was sinking behind the\", \n                    num_generate=500, temperature=0.5))","60d47395":"Let's compile the model:","024b1964":"**`Each index`** of these vectors(input vector and output vector of integers) are processed as **`one time step`**. Means we have `total timesteps` in `one example` is equal to `sequence length`. For the input at time step 0, the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the RNN considers the previous step context in addition to the current input character.","70e1c82b":"### Attach an optimizer, and a loss function\n\nThe standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n\nBecause our model returns logits, we need to set the `from_logits` flag.","1c50358d":"## The prediction loop:\n\nThe following code block generates the text:\n\n* It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n* Get the prediction distribution of the next character using the start string and the RNN state.\n* Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n* The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters.","01541b7c":"Build char mapping:","55ec5ba4":"The value of temperature determines how random the generated text is (lower is less random, meaning more predictable).","8500106b":"Now convert whole text to int:","b82dab93":"Let\u2019s check the length of the total characters in this textfile and how many unique characters are there.","81282657":"### Configure checkpoints\nUse a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:","430d931f":"There are not good and relative sentence but if we tuned our model then we can get. Thank you.","a300219c":"Let\u2019s check the output for temperature=0.5:","b3ce1736":"We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches.","9b58f511":"## Build The Model","ce2ecc67":"Now let\u2019s open our text file and apply the `\u2018utf-8\u2019` encode technique on it. After that, we are trying to remove some extra stuff given in starting of the textfile.","13c95036":"## Create training batches","2b4093ea":"`model.build()` : This method only exists for users who want to call `model.build()` in a\nstandalone way (as a substitute for calling the model on real data to\nbuild it). It will never be called by the framework (and thus it will\nnever throw unexpected errors in an unrelated workflow).\n","ad2a1dd7":"Here we are using text file [The Jungle Book](https:\/\/www.gutenberg.org\/files\/236\/236-0.txt) to generate next scene of it.","7fa31f5d":"Here not confused about shape of `input_ex_batch` because we are taking every single batch from dataset so now we have input_ex_batch has shape of (number of sequence, sequence length) and below we are taking 0th sequence.","4eb383bf":"Now we have an integer representation for each character. Notice that we mapped the character as indexes from `0` to `len(unique)`.","f7472421":"This gives us, at each timestep, a prediction of the next character index:","f6ff805f":"means we have 64 examples(sequences) in a batch and every example has 100 timesteps or we can say 100 characters or in lemon term 100 input features.","99bdd201":"## Run the model","c9259d70":"In the above example the sequence length of the input is 100 but the model can be run on inputs of any length.\n\nTo get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n**Note:** It is important to sample from this distribution because if we take the argmax of the distribution it can easily get the model stuck in a loop.\n\nLet first try it for first batch:","1da52955":"The batch method lets us easily convert these individual characters to sequences of the desired size.  \n\n**drop_remainder**: representing whether the last batch should be dropped in the case it has fewer than `batch_size` elements; the default behavior is not to drop the smaller batch.  \n\n**batch_size:** representing the number of consecutive elements of this dataset to combine in a single batch.  \n\nHere every character act as one elements so here 1 batch creates one sequences. so batch size is equal to `(seq_length+1)`.","8ef904b5":"For each sequence, duplicate and shift it to form the input and target text by using the map method to apply a simple function to each batch:","c9afb0cd":"Here , we have finite batch_size so give value of `batch_input_shape` parameter, instead if we have variable length batch_size as case if we give `drop_remainder=False` then we can also define another parameter `batch_size`.\n\n**Embedding layer:**  \nEmbedding layer turns positive integers (indexes) into dense vectors of fixed size. Ex. It take one word and represent it as an vector of output_dim. If we converted our text into one hot encoded and then give input then this function should not need but for millions of words its not beneficial for us to give that much shape of array as input. so we just apply word embedding algorithm to our word or character (here act as integer) and make vector representation of that word or character or integer which takes lower dimensions and eventually less memory.  \nHere `input_dim` corresponds to the `vocabulary size (number of distinct words)` of your dataset, so layer can decide how it should have convert every word in vector space.  \n`output_dim` is an arbitrary hyperparameter that indicates the dimension of your embedding space. In other words, if you set `output_dim=x`, each word in the sentence will be characterized with x features.  \n`input_length` should be set to `SEQ_LENGTH` (an integer indicating the length of each sentence), assuming that all the sentences have the same length. Here we not define this parameter as it is not mandatory.  \nThis layer can only be used as the first layer in a model. One constrain in using this layer is that the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size). For visulization this image can be helpful \n![image](https:\/\/www.tensorflow.org\/tutorials\/text\/images\/text_generation_training.png)\n<p style=\"text-align:center;\">image from the Tensorflow RNN blog<\/p>\n\n**GRU Layer:**  \nhere `units` defines dimensionality of the output shape. Now as seen above image we can say that for each word or character process there is being one GRU layer with shared weights. so for one character or word our embedding layer gives us output as vector of `embedding_dim` shape. so it takes input as this vector and give output as `rnn_unit` shape of vector which contains more information. \n\n**Dense Layer:**  \nas seen from image we have seperate dense layer for all single character, we can say that output of GRU layer is connected to Dense Layer and from that we want to get some extra features. Now consider this problem as classification problem as we want to predict out next character so we need output neuron for single character is size of vocabulary so that it can predict probability of every character for next word and the character or word with max probability we take it as next word prediction. so here we give dense layer `units` equal to `vocab_size`.","dca4bd57":"## Generate text\n\n### Restore the latest checkpoint\n\nTo keep this prediction step simple, use a batch size of 1.\n\nBecause of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n\nTo run the model with a different batch_size, we need to rebuild the model and restore the weights from the checkpoint.","4c5578d2":"## Train the model\n\nAt this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character.","888745e0":"## Create training examples and targets\n\nNext divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n\nFor each input sequence, the corresponding `targets` contain the same length of text, except shifted one character to the right.\n\nSo break the text into chunks of `seq_length+1`. For example, say `seq_length` is `4` and our text is `\"Hello\"`. The `input` sequence would be `\"Hell\"`, and the `target` sequence `\"ello\"`.\n\nTo do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the `text vector` into a `stream of character indices`.","1e695d87":"Decode these to see the text predicted by this untrained model:","c8953b8e":"Now from first `.batch()` function we created sequences of character and make dataset from it. and as we know one sequence is act as one example for input. so for batch training we create a batch of more than one example. so again we are using `.batch()` function with `batch_size`, note that here every sequence is act as one element means one batch of batch_size number of sequences."}}