{"cell_type":{"8b4b0373":"code","7cb32e7f":"code","505e7715":"code","35d8c962":"code","a02d4bc8":"code","f429eef6":"code","3bda7560":"code","3d211de1":"code","0b45541a":"code","4844d528":"code","fc91b677":"code","e0085d72":"code","417e34cd":"code","7aa0df28":"code","a53d2d9b":"code","539a9b7d":"code","2edc9fc8":"code","843d9acb":"code","ba5e449c":"code","cd6fc31c":"markdown","0792507c":"markdown","1e9dc441":"markdown","bdde583d":"markdown","0fbb3d1b":"markdown","566431b2":"markdown","d31d4ac0":"markdown","ab7938d6":"markdown","41fda9e5":"markdown","9d32ca0b":"markdown","264ad8fb":"markdown","9fb78503":"markdown","d6281e76":"markdown"},"source":{"8b4b0373":"import pandas as pd\nimport numpy as np\ntrain = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\ntrain['target']","7cb32e7f":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train['target'].unique())\nle.classes_","505e7715":"target = le.transform(train['target'])\ntarget","35d8c962":"def submit(test_pred, filename):\n    submission = pd.DataFrame(test_pred, columns=le.classes_)\n    submission.insert(0, 'id', test['id'])\n    submission.to_csv(filename, index=False)","a02d4bc8":"from sklearn.metrics import log_loss","f429eef6":"import numpy as np\nlabels, frequencies = np.unique(target, return_counts=True)\n(labels, frequencies)","3bda7560":"frequencies\/len(train)","3d211de1":"pred = [[0.0,1.0,0.0,0.0]]*len(train)","0b45541a":"import math\nprint(f'logloss:{-math.log(10**-15)*(1-frequencies[1]\/len(train))}')","4844d528":"log_loss(target, pred, labels=[0,1,2,3])","fc91b677":"import math\nprint(f'logloss:{-math.log(10**-7)*(1-frequencies[1]\/len(train))}')","e0085d72":"from tensorflow.keras.losses import SparseCategoricalCrossentropy\nscc = SparseCategoricalCrossentropy()\nscc(target, pred).numpy()","417e34cd":"test_pred = [[0.0,1.0,0.0,0.0]]*len(test)\nsubmit(test_pred, 'baseline_majority.csv')","7aa0df28":"pred = frequencies\/len(train)","a53d2d9b":"print(f'logloss:{-np.sum(pred * np.log(pred))}')","539a9b7d":"pred = np.tile(frequencies\/len(train), (len(train),1))\npred","2edc9fc8":"log_loss(target, pred, labels=[0,1,2,3])","843d9acb":"from tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.losses import Reduction\nscc = SparseCategoricalCrossentropy()\nscc(target, pred).numpy()","ba5e449c":"test_pred = pred = np.tile(frequencies\/len(test), (len(test),1))\nsubmit(test_pred, 'baseline_apriori.csv')","cd6fc31c":"## Check with leaderboard","0792507c":"Calculated value:\n* y = class 0 (8.49% of cases): logloss=log(0.0849)=-2.46\n* y = class 1 (57.5% of cases): logloss = log(0.57497)=-0.55\n* etc\n\nThis is the best single-point estimator.","1e9dc441":"# Multiclass logarithmic loss\n\nA quick look into the behavior and definition of the logloss as used in the Kaggle Tabular Data competition of May 2021. The logloss uses natural logs rather than log2 (as in https:\/\/en.wikipedia.org\/wiki\/Cross_entropy). In addition to that an epsilon value is used to prevent returning infinite loss values. The definition used in the competition is exactly the one from sklearn. The Tensorflows implementation is similar (with a higher epsilon), as is the one in LightGMB (https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/src\/metric\/multiclass_metric.hpp).","bdde583d":"# Loading data","0fbb3d1b":"# Baseline model: majority class","566431b2":"## Check with leaderboard","d31d4ac0":"Calculate with sklearn","ab7938d6":"Leaderboard score is 1.11634, seems to match","41fda9e5":"Calculate with tensorflow, uses a non-configurable(?) epsilon of 10^-7.","9d32ca0b":"Calculated value:\n* y = class 1 (57.5% of cases): logloss = 0\n* y != class 1 (42.5% of cases): logloss = log(10^-15) = -34.5","264ad8fb":"# Submission","9fb78503":"Score is 14.62209, seems to match","d6281e76":"# Baseline model: a priori probabilities"}}