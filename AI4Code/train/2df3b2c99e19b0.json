{"cell_type":{"28d360f7":"code","7e831035":"code","6549a4e0":"code","cb536915":"code","503ee2b7":"code","a5996ed9":"code","9e7bcb0b":"code","5a42350f":"code","c2f6d8ba":"code","276e01dc":"code","18d7a12a":"code","d6ff9a0a":"code","3b20b027":"code","8ded8eb6":"code","14b40bf4":"code","13f91757":"code","b695b115":"code","5bf2d112":"code","2e9fcdd3":"code","a76a4234":"code","0b32d0e7":"code","bba50c43":"code","d02d65ec":"code","4726d48d":"code","47fe0789":"code","77d4a4f7":"code","b833ba10":"code","e5e7bb07":"code","211403ec":"code","ad2c2ea4":"code","e9a636b4":"code","f6913522":"code","56ccaf56":"code","9644ed51":"code","5039cba6":"code","1a0240f2":"code","17fb27ad":"code","df3e2dee":"code","4c166011":"code","3bb60454":"code","6b927535":"markdown","7d27977b":"markdown","aeda22c5":"markdown","1120e975":"markdown","2eed3282":"markdown","741b237f":"markdown","0281f46a":"markdown","65b70461":"markdown","1d78b4e4":"markdown","baffa732":"markdown","8df034eb":"markdown","2c1c2621":"markdown","ab3b966c":"markdown","2a2ba60a":"markdown","dbbb91a4":"markdown"},"source":{"28d360f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7e831035":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Binarizer, KBinsDiscretizer, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n\nurl = '..\/input\/league-of-legends\/games.csv'\n\ndata = pd.read_csv(url, index_col=0, encoding = \"ISO-8859-1\")","6549a4e0":"#On vire les variables qui ne me serviront \u00e0 rien, connaissant le jeu (et pour un premier projet, surtout)\ndf = data.copy()\ndf = df.drop(['creationTime', 'seasonId'], axis=1)\ndf = df.drop(df.columns[8: df.shape[1]], axis=1)\ndf.head()","cb536915":"#Modifier notre index.\ndf['Index'] = range(0, df.shape[0])\ndf = df.set_index('Index')\ndf.head()","503ee2b7":"#Modifier les valeurs 1 et 2 en 0 et 1\ndf['winner'] = df['winner'].replace(1, 0)\ndf['winner'] = df['winner'].replace(2, 1)\n\ndf['winner'].unique()","a5996ed9":"df.shape","9e7bcb0b":"#Compter le nombre de fois o\u00f9 la team 1 a gagn\u00e9.\ndf['winner'].value_counts(normalize=True)","5a42350f":"#Choisir notre target et notre data\nX = df.drop(['winner'], axis=1)\ny = df['winner']","c2f6d8ba":"#Cr\u00e9er des sous-ensembles quand l'\u00e9quipe 1 gagne et quand elle perd\nteam1_win = df[df['winner'] == 0]\nteam1_lose = df[df['winner'] == 1]","276e01dc":"#Tracer les histogrammes des colonnes\nfor col in df.select_dtypes('int'):\n    plt.figure()\n    sns.distplot(df[col])","18d7a12a":"#Cr\u00e9ation petits groupes pour partager les gameDuration par index \ndf['1st_group_gd'] = df.index < 10000\ndf['2nd_group_gd'] = (df.index >= 10000) & (df.index < 20000)\ndf['3rd_group_gd'] = (df.index >= 20000) & (df.index < 30000)\ndf['4th_group_gd'] = (df.index >= 30000) & (df.index < 40000)\ndf['5th_group_gd'] = (df.index >= 40000) & (df.index < df.shape[0])\n\n#Remplacer les true & false par 1 & 0 -> M\u00e9thode 1\ndf = df.applymap(lambda x: 1 if x == True else x)\ndf = df.applymap(lambda x: 0 if x == False else x)\n\ndf.head()","d6ff9a0a":"#Afficher les variables corell\u00e9es.\nsns.clustermap(df.corr())","3b20b027":"#V\u00e9rifier si y'a une corr\u00e9lation avec des %\ndf.corr()['winner'].sort_values()","8ded8eb6":"#Cr\u00e9er les set\ntrainset, testset = train_test_split(df, test_size=0.2, random_state=0) ","14b40bf4":"#V\u00e9rifier le nombre dans les set\ntrainset['winner'].value_counts()","13f91757":"#Les proportions sont bien gard\u00e9es\ntestset['winner'].value_counts()","b695b115":"#Autre m\u00e9thode pour les true\/false en 1\/0 -> Encodage\ndef encodage(df):\n#    code = {\n#        'True':1,\n#        'False':0\n#    }\n#    \n#    for col in df.select_dtypes('bool').columns:\n#        df.loc[col] = df[col].map(code)\n#        \n    return df","5bf2d112":"#Plus tard, si nous voulons supprimer quelques colonnes pour tester l'efficatit\u00e9 de notre model, on le fera ici\ndef imputation(df):\n    df = df.drop(df.columns[8: df.shape[1]], axis=1)\n    df = df.drop(['gameDuration'], axis=1) #D'ailleurs, apr\u00e8s plusieurs tentatives, en supprimant la colonne gameDuration, les r\u00e9sultats sont meilleurs\n    return df","2e9fcdd3":"def preprocessing(df):\n    \n    df = encodage(df)\n    df = imputation(df)\n    \n    X = df.drop('winner', axis=1)\n    y = df['winner']\n    \n    print(y.value_counts())\n    \n    return X, y","a76a4234":"#Cr\u00e9er nos X_train et y_train automatis\u00e9s.\nX_train, y_train = preprocessing(trainset)","0b32d0e7":"#Cr\u00e9er nos variables de test automatis\u00e9es\nX_test, y_test = preprocessing(testset)","bba50c43":"from sklearn.metrics import f1_score, confusion_matrix, classification_report, recall_score\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler","d02d65ec":"def evaluation(model):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))\n    \n    # Savoir si il est en overfitting ou en underfitting\n    #N, train_score, val_score = learning_curve(model, X_train, y_train, \n    #                                           cv=4, scoring='f1',\n    #                                    train_sizes=np.linspace(0.1, 1, 10))\n    \n    #plt.figure(figsize=(12, 8))\n    #plt.plot(N, train_score.mean(axis=1), label='Train_score')\n    #plt.plot(N, val_score.mean(axis=1), label='Val_score')\n    #plt.legend()","4726d48d":"#Cr\u00e9ation d'un premier model basique mais qui donne des r\u00e9sultats finalement assez int\u00e9ressants.\nmodel = KNeighborsClassifier(n_neighbors=5)\nevaluation(model)","47fe0789":"#V\u00e9rifier les variables les plus importantes\n#pd.DataFrame(model.feature_importances_, index=X_train.columns).plot.bar(figsize=(12, 8))","77d4a4f7":"#Il faut d'abord cr\u00e9er une pipeline\npreprocessor = make_pipeline(SelectKBest(f_classif, k='all'))","b833ba10":"#D\u00e9finir les mod\u00e8les qu'on utilisera. Liste non exhaustive, on choisit 4 algo connus\nRandomForest = make_pipeline(preprocessor, \n                             RandomForestClassifier(random_state=0))\nAdaBoost = make_pipeline(preprocessor, \n                             AdaBoostClassifier(random_state=0))\nSVM = make_pipeline(preprocessor, StandardScaler(),\n                             SVC(random_state=0))\nKNN = make_pipeline(preprocessor, StandardScaler(),\n                             KNeighborsClassifier())\n\nlist_of_models = {'RandomForest':RandomForest, \n                  'AdaBoost':AdaBoost,\n                  'SVM': SVM,\n                  'KNN': KNN\n                 }","e5e7bb07":"#Automatiser chaque essai\nfor name, model in list_of_models.items():\n    print(name)\n    evaluation(model)","211403ec":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","ad2c2ea4":"#Cr\u00e9ation d'une liste d'hyper_param\u00e8tres\nhyper_params = {'svc__C': [1, 2],  \n                'svc__kernel': ['linear', 'poly', 'rbf']\n}","e9a636b4":"#Trouver les meilleurs param\u00e8tres.\ngrid = GridSearchCV(SVM, hyper_params, scoring='recall', cv=4)\n\n#Entra\u00eener la grille\ngrid.fit(X_train, y_train)\n\n#V\u00e9rifier les meilleurs param\u00e8tres\nprint(grid.best_params_)\n\n#Pr\u00e9dire y\ny_pred = grid.predict(X_test)\n\nprint(classification_report(y_test, y_pred))","f6913522":"#Puis on entra\u00eene enfin notre model avec les meilleurs param\u00e8tres possibles\nevaluation(grid.best_estimator_)","56ccaf56":"from sklearn.metrics import precision_recall_curve","9644ed51":"#Utilisation du model qui fonctionne bien sans toucher aux hyper_param\u00e8tres\nmodel = SVM","5039cba6":"from sklearn.metrics import precision_recall_curve","1a0240f2":"#R\u00e9cup\u00e9rer les donn\u00e9es des variables\nprecision, recall, threshold = precision_recall_curve(y_test, grid.best_estimator_.decision_function(X_test))","17fb27ad":"#Afficher les courbes de precision et de recall\nplt.plot(threshold, precision[:-1], label='precision')\nplt.plot(threshold, recall[:-1], label='recall')\nplt.legend()","df3e2dee":"#D\u00e9cider du point de d\u00e9cision\ndef final_model(model, X, threshold=0):\n    return model.decision_function(X) > threshold\n\ny_pred = final_model(grid.best_estimator_, X_test, threshold=0)","4c166011":"#V\u00e9rifier le score F1\nf1_score(y_test, y_pred)","3bb60454":"#V\u00e9rifier le recall score\nrecall_score(y_test, y_pred)","6b927535":"Pour le moment nous avons simplement filtr\u00e9 ce qui ne m'allait pas \u00eatre utile pour mon premier mod\u00e8le. En effet, conna\u00eetre quel champion a \u00e9t\u00e9 choisi ou encore les bannissements effectu\u00e9s ne m'int\u00e9ressent gu\u00e8re pour les premi\u00e8res versions.\n\nEnsuite on a aussi modifi\u00e9 l'index pour une meilleure lisibilit\u00e9.","7d27977b":"### La variable \"GameDuration\" m'int\u00e9resse et j'aimerais donc en faire ungraphique plus joli. Et compter \u00e9ventuellement les valeurs. Cependant, en faisant mes petits tests je me suis rendu compte qu'\u00e9tant donn\u00e9 les valeurs de gameDuration beaucoup trop h\u00e9t\u00e9rog\u00e8nes, j'arrivais \u00e0 peu de r\u00e9sultat. Je suis donc dit qu'il \u00e9tait plus int\u00e9ressant de diviser en petits groupes les donn\u00e9es de gameDuration. Je verrai s'ils sont utiles plus tard.","aeda22c5":"## Notre job maintenant est de v\u00e9rifier parmis plusieurs algo, quel est le meilleur","1120e975":"## Automatisation","2eed3282":"# Optimisation","741b237f":"# Proc\u00e9dure d'\u00e9valuation","0281f46a":"### Pour la suite on gardera SVM mais on aurait pu garder AdaBoost qui est aussi tr\u00e8s bon !","65b70461":"# Optimisation","1d78b4e4":"**D'apr\u00e8s** mon analyse, je remarque donc plusieurs choses. Premi\u00e8rement, on a environ 50% de victoires des deux \u00e9quipes. Ratio qu'il faudra garder quand on cr\u00e9era nos sets. Ensuite, les variables ne sont pas \u00e9norm\u00e9ment corr\u00e9l\u00e9es \u00e0 part l'objectif \"first inhibitor\" corr\u00e9l\u00e9e \u00e0 53.6%. Ce qui m'\u00e9tonne c'est le gameDuration qui n'est pas \u00e9norm\u00e9ment corr\u00e9l\u00e9e mais je pense que c'est \u00e0 cause de l'\u00e9norme diversit\u00e9 des valeurs qui ne permet pas de faire une bonne corr\u00e9lation.\n\n","baffa732":"# Analyse de fond & de forme","8df034eb":"## V\u00e9rifier les relations entre target et les diff\u00e9rentes variables","2c1c2621":"# Preprocessing\n\n## Trainset \/ Nettoyage \/ Encodage","ab3b966c":"### La Grid nous permet d'obtenir de bons r\u00e9sultats sans transcender les r\u00e9sultats initiaux. On essaiera \u00e0 l'avenir plus de param\u00e8tres pour am\u00e9liorer ce point-ci.","2a2ba60a":"### Je vais essayer de cr\u00e9er un mod\u00e8le pr\u00e9dictif capable de dire si l'\u00e9quipe 1 va gagner en fonction de plusieurs param\u00e8tres inscrits dans le dataset trouv\u00e9.","dbbb91a4":"## Precision Recall Curve"}}