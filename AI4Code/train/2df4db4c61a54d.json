{"cell_type":{"43643668":"code","79e75f7a":"code","0aa4b1bc":"code","4b2629de":"code","1bd2c672":"code","4baed54d":"code","18f06ca6":"code","453900a1":"code","ede083d7":"code","4318894f":"code","bd3aba6f":"code","2255cb18":"code","5968d85f":"code","91d9bd40":"code","1e2fee9b":"code","5cb3f731":"code","972dacee":"code","3e8f8af8":"code","613ce584":"code","a6e55c6a":"code","7c2cdd4c":"code","3bd16785":"code","edcc4e50":"code","6441796c":"code","373d6aa8":"code","d2bcb8f3":"code","4bf2f3fb":"code","65844244":"code","bf067491":"code","a52d18c7":"markdown","56cf01ef":"markdown","f4eed58e":"markdown","36159d29":"markdown","d4a55040":"markdown","16f0dd12":"markdown","9782e24c":"markdown","9524024d":"markdown","8c969b10":"markdown","c2087b46":"markdown","3eaee4c7":"markdown","343dc4b3":"markdown","66cf2d64":"markdown","7fb90aca":"markdown","b2f8b01b":"markdown"},"source":{"43643668":"import pandas as pd\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntrain.head(5)","79e75f7a":"import seaborn as sns\nX = train.drop(labels=[\"label\"], axis = 1)\ny = train[\"label\"]\nsns.countplot(y)","0aa4b1bc":"X = X \/ 255.0\ntest = test \/ 255.0","4b2629de":"X = X.values.reshape(-1, 28, 28, 1)\ntest = test.values.reshape(-1, 28, 28, 1)","1bd2c672":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","4baed54d":"import matplotlib.pyplot as plt\nplt.imshow(X_train[15][:, :, 0], cmap = \"Greys\")","18f06ca6":"from tensorflow import keras\ndatagen = keras.preprocessing.image.ImageDataGenerator( rotation_range= 10, zoom_range=0.1, \n                                                       width_shift_range=0.1, height_shift_range=0.1)\n\ndatagen.fit(X_train)\ny_train_c = keras.utils.to_categorical(y_train)\ny_test_c = keras.utils.to_categorical(y_test)\ntrain_data = datagen.flow(x = X_train, y = y_train_c, batch_size=64)\ntest_data = datagen.flow(x = X_test, y = y_test_c, batch_size = 64)","453900a1":"from keras import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n\ninput_shape = (28, 28, 1)\ndef cnn1():\n    return Sequential([\n        Conv2D(8, kernel_size=(3, 3), activation='relu',padding='same',input_shape=input_shape),\n        MaxPooling2D(pool_size=(2, 2),strides=2),\n        Conv2D(16, kernel_size=(5, 5), activation='relu'),\n        MaxPooling2D(pool_size=(2, 2),strides=2),\n        Flatten(),\n        Dense(120, activation='relu'),\n        Dense(84, activation='relu'),\n        Dense(10, activation='softmax')\n    ])\n\ndef cnn2():\n    return Sequential([\n        Conv2D(32, kernel_size=(3, 3), activation='relu',input_shape=input_shape),\n        MaxPooling2D(pool_size=(2, 2),strides=2),\n        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n        MaxPooling2D(pool_size=(2, 2),strides=2),\n        Conv2D(128, kernel_size=(3, 3), activation='relu'),\n        MaxPooling2D(pool_size=(2, 2),strides=2),\n        Flatten(),\n        Dense(64, activation='relu'),\n        Dense(10, activation='softmax')\n    ])\n\ndef cnn3():\n    return Sequential([\n        Conv2D(16, kernel_size=(3, 3), activation='relu',padding='same',input_shape=input_shape),\n        Conv2D(16, kernel_size=(3, 3), activation='relu',padding='same'),\n        MaxPooling2D(pool_size=(2, 2),strides=2),\n        Conv2D(32, kernel_size=(3, 3), activation='relu'),\n        Conv2D(32, kernel_size=(3, 3), activation='relu'),\n        MaxPooling2D(pool_size=(2, 2),strides=2),\n        Flatten(),\n        Dense(512, activation='relu'),\n        Dense(10, activation='softmax')\n    ])","ede083d7":"model = cnn1()\nmodel.compile(optimizer=\"Adam\", loss=[\"categorical_crossentropy\"], metrics = [\"accuracy\"])\nhistory = model.fit(train_data ,epochs=3, validation_data=test_data, verbose=2)","4318894f":"import numpy as np\ntrain_data = datagen.flow(x = X_train, y = y_train_c, batch_size=1)\ntest_data = datagen.flow(x = X_test, y = y_test_c, batch_size = 1)\ndef get_array_from_datagen(train_generator):\n  x=[]\n  y=[]\n  train_generator.reset()\n  for i in range(train_generator.__len__()):\n    a,b=train_generator.next()\n    x.append(a)\n    y.append(b)\n  x=np.array(x)\n  y=np.array(y)\n  print(x.shape)\n  print(y.shape)\n  return x,y\n\nX_train_2, y_train_2 = get_array_from_datagen(train_data)\nX_test_2, y_test_2 = get_array_from_datagen(test_data)\n\nX_train_2 = X_train_2.reshape(-1, 28, 28, 1)\ny_train_2 = y_train_2.reshape(-1, 10)\n\nX_test_2 = X_test_2.reshape(-1, 28, 28, 1)\ny_test_2 = y_test_2.reshape(-1, 10)\n\nX_train_2.shape, y_train_2.shape","bd3aba6f":"model = cnn1()\nmodel.compile(optimizer=\"Adam\", loss=[\"categorical_crossentropy\"], metrics = [\"accuracy\"])\nhistory = model.fit(X_train_2, y_train_2 ,epochs=3, validation_data=(X_test_2, y_test_2), verbose=2)","2255cb18":"import matplotlib.pyplot as plt\ndef plot_history(history):\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","5968d85f":"model = cnn1()\nmodel.compile(optimizer=\"Adam\", loss=[\"categorical_crossentropy\"], metrics = [\"accuracy\"])\nhistory = model.fit(X_train_2, y_train_2 ,epochs=25, validation_data=(X_test_2, y_test_2), verbose=2)","91d9bd40":"plot_history(history)","1e2fee9b":"model = cnn2()\nmodel.compile(optimizer=\"Adam\", loss=[\"categorical_crossentropy\"], metrics = [\"accuracy\"])\nhistory = model.fit(X_train_2, y_train_2 ,epochs=25, validation_data=(X_test_2, y_test_2), verbose=2)","5cb3f731":"plot_history(history)","972dacee":"model = cnn3()\nmodel.compile(optimizer=\"Adam\", loss=[\"categorical_crossentropy\"], metrics = [\"accuracy\"])\nhistory = model.fit(X_train_2, y_train_2 ,epochs=25, validation_data=(X_test_2, y_test_2), verbose=2)","3e8f8af8":"plot_history(history)","613ce584":"import gc\nbest_accuracy = 0\nbest_batch_size = -1\nfor batch_size in [32, 64, 128, 256]:\n    model = cnn3()\n    model.compile(optimizer=\"Adam\", loss=[\"categorical_crossentropy\"], metrics = [\"accuracy\"])\n    history = model.fit(X_train_2, y_train_2 ,epochs=10, batch_size = batch_size, validation_data=(X_test_2, y_test_2), verbose=0)\n    acc = history.history[\"val_accuracy\"][-1]\n    if acc > best_accuracy:\n        best_accuracy = acc\n        best_batch_size = batch_size\n    print(batch_size, acc)\n    gc.collect() # Clear RAM\nprint(\"\\nBEST -> \", best_batch_size, best_accuracy)","a6e55c6a":"learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(\n    monitor = \"val_accuracy\",\n    factor = 0.5,\n    patience = 3,\n    verbose = 0,\n    min_lr = 0.00001\n)","7c2cdd4c":"learning_rate = [0.01, 0.001, 0.0001]\nmomentum = [0, 0.2, 0.4, 0.6, 0.8, 0.99]\nnesterov = [True, False]\nbest_accuracy = 0\nbest_sgd_params = {\"learning_rate\" : 0, \"momentum\" : 0, \"nesterov\" : True}\nfor l_rate in learning_rate:\n  for m in momentum:\n    for n in nesterov:\n      \n      optimizer = SGD(learning_rate=l_rate, momentum=m, nesterov=n)\n      \n      model = cnn3()\n      model.compile(optimizer=optimizer, loss=[\"categorical_crossentropy\"], metrics = [\"accuracy\"])\n      \n      history = model.fit(X_train_2, y_train_2, batch_size = 32, epochs=10, \n                         validation_data=(X_test_2, y_test_2), verbose=0, callbacks = [learning_rate_reduction])\n      acc = history.history[\"val_accuracy\"][-1]\n      if acc > best_accuracy:\n        best_sgd_params[\"learning_rate\"] = l_rate\n        best_sgd_params[\"momentum\"] = m\n        best_sgd_params[\"nesterov\"] = n\n        best_accuracy = acc\n      print(\"SGD : \"+ f\"{l_rate}\/{m}\/{n} -> \", acc)\n      gc.collect() # Clear RAM\nprint(\"\\n Best -> \" + str(best_sgd_params) + \" -> \" + str(best_accuracy))","3bd16785":"learning_rate = [0.01, 0.001, 0.0001]\nmomentum = [0.8, 0]\ncentered = [True, False]\nepsilon = [10e-7, 10e-6, 10e-8]\nrho = [0.9, 0.45]\nbest_accuracy = 0\nbest_rms_params = {\"learning_rate\" : 0, \"momentum\" : 0, \"centered\" : False, \"epsilon\": 0, \"rho\" : 0}\nfor l_rate in learning_rate:\n  for m in momentum:\n    for c in centered:\n        for e in epsilon:\n            for r in rho:\n              optimizer = RMSprop(learning_rate=l_rate, momentum=m, centered=c, epsilon = e, rho = r)\n\n              model = cnn3()\n              model.compile(optimizer=optimizer, loss=[\"categorical_crossentropy\"], metrics = [\"accuracy\"])\n\n              history = model.fit(X_train_2, y_train_2, batch_size = 32, epochs=10, \n                                 validation_data=(X_test_2, y_test_2), verbose=0, callbacks = [learning_rate_reduction])\n              acc = history.history[\"val_accuracy\"][-1]\n              if acc > best_accuracy:\n                best_rms_params[\"learning_rate\"] = l_rate\n                best_rms_params[\"momentum\"] = m\n                best_rms_params[\"centered\"] = c\n                best_rms_params[\"epsilon\"] = e\n                best_rms_params[\"rho\"] = r\n                best_accuracy = acc\n              print(\"RMSProp : \"+ f\"{l_rate}\/{m}\/{c}\/{e}\/{r} -> \", acc)\n              gc.collect() # Clear RAM\n      \n      \nprint(\"\\n Best -> \" + str(best_rms_params) + \" -> \" + str(best_accuracy))","edcc4e50":"learning_rate = [0.01, 0.001, 0.0001]\nbeta_1 = [0.9, 0.99]\nbeta_2 = [0.9, 0.99]\nepsilon = [1e-7, 1e-5, 1e-3]\namsgrad = [False, True]\nbest_accuracy = 0\nbest_adam_params = {\"learning_rate\" : 0, \"beta_1\" : 0, \"beta_2\" : 0, \"epsilon\" : 0, \"amsgrad\" : True}\nfor l in learning_rate:\n    for b1 in beta_1:\n        for b2 in beta_2:\n            for e in epsilon:\n                for a in amsgrad:\n                  optimizer = Adam(learning_rate=l, beta_1 = b1, beta_2 = b2, epsilon = e, amsgrad = a)\n\n                  model = cnn3()\n                  model.compile(optimizer=optimizer, loss=[\"categorical_crossentropy\"], metrics = [\"accuracy\"])\n\n                  history = model.fit(X_train_2, y_train_2, batch_size = 32, epochs=10, \n                                     validation_data=(X_test_2, y_test_2), verbose=0, callbacks = [learning_rate_reduction])\n                  acc = history.history[\"val_accuracy\"][-1]\n                  if acc > best_accuracy:\n                    best_adam_params[\"learning_rate\"] = l\n                    best_adam_params[\"beta_1\"] = b1\n                    best_adam_params[\"beta_2\"] = b2\n                    best_adam_params[\"epsilon\"] = e\n                    best_adam_params[\"amsgrad\"] = a\n                    best_accuracy = acc\n                  print(\"Adam : \"+ f\"{l}\/{b1}\/{b2}\/{e}\/{a} -> \", acc)\n                  gc.collect() # Clear RAM\n      \n      \nprint(\"\\n Best -> \" + str(best_adam_params) + \" -> \" + str(best_accuracy))","6441796c":"accuricies = {}\nparams = {\"SGD\" : best_sgd_params, \"Adam\": best_adam_params, \"RMSprop\": best_rms_params}","373d6aa8":"p = best_sgd_params\noptimizer = SGD(learning_rate = p[\"learning_rate\"], momentum = p[\"momentum\"], nesterov = p[\"nesterov\"])\nmodel = cnn3()\nmodel.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\nhistory = model.fit(X_train_2, y_train_2, epochs = 25, batch_size = 32, callbacks = [learning_rate_reduction], validation_data=(X_test_2, y_test_2))\naccuricies[\"SGD\"] = history.history[\"val_accuracy\"][-1]","d2bcb8f3":"p = best_rms_params\noptimizer = RMSprop(learning_rate = p[\"learning_rate\"], momentum = p[\"momentum\"], \n                    centered = p[\"centered\"], epsilon = p[\"epsilon\"], rho=p[\"rho\"])\nmodel = cnn3()\nmodel.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\nhistory = model.fit(X_train_2, y_train_2, epochs = 25, batch_size = 32, callbacks = [learning_rate_reduction], validation_data=(X_test_2, y_test_2))\naccuricies[\"RMSprop\"] = history.history[\"val_accuracy\"][-1]","4bf2f3fb":"p = best_adam_params\noptimizer = Adam(learning_rate = p[\"learning_rate\"], beta_1 = p[\"beta_1\"], beta_2 = p[\"beta_2\"],\n                 epsilon = p[\"epsilon\"], amsgrad=p[\"amsgrad\"])\nmodel = cnn3()\nmodel.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\nhistory = model.fit(X_train_2, y_train_2, epochs = 25, batch_size = 32, callbacks = [learning_rate_reduction], validation_data=(X_test_2, y_test_2))\naccuricies[\"Adam\"] = history.history[\"val_accuracy\"][-1]","65844244":"max_ = 0\nmax_key = \"\"\nfor key,value in accuricies.items():\n    if value > max_:\n        max_key = key\n        max_ = value\nprint(max_key, accuricies[max_key], str(params[max_key]))","bf067491":"results = model.predict(test)\n\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","a52d18c7":"## Choose Batch Size","56cf01ef":"As you can see above numpy array type increased speed to 3s\/epoch from 12s\/epoch. This will save a lot of time beacuse we will apply parameter tuning in the later codes","f4eed58e":"## Normalization","36159d29":"Best Optimizer is","d4a55040":"CNN-3 Model gave better results other two model. We will continue with this model. And we can break learning at tenth epoch","16f0dd12":"## Train Model","9782e24c":"We will use LearnigRateReduction callback. This callback function reducing learning rate if validation loss is increasing","9524024d":"To avoiding overfitting and increasing learning speed we need to apply normalization","8c969b10":"## Data Augmentation","c2087b46":"The DataGenerator type apply CPU operations in learning but we want to use only GPU for increase learn speed","3eaee4c7":"## Read Data","343dc4b3":"First we try 3 trend in CNN algorithms and continue with model that has best accuracy","66cf2d64":"We will choose 32 for batch size","7fb90aca":"We will apply rotating, zooming, shifting some images to learn better our CNN algorithm","b2f8b01b":"## Choose Best Optimizer"}}