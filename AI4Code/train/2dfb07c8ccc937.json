{"cell_type":{"526877b3":"code","50247a84":"code","015444b1":"code","089746d9":"code","61350c1a":"code","82643a97":"code","6b0b501c":"code","fc034ba8":"code","ad27c127":"code","9f99e897":"code","d48df0bc":"code","658a5ce3":"code","22436ad6":"code","2d9908d3":"code","40f2397d":"code","77679961":"code","bcfb38e7":"code","a9c7484f":"code","905b14f8":"code","64ca9821":"markdown","52cca8e0":"markdown","61f02e63":"markdown","f073eff4":"markdown","7be7628f":"markdown","898a5da3":"markdown","fd668488":"markdown","9d3ec843":"markdown","135fb9a8":"markdown","da2b4575":"markdown","4c62ddd9":"markdown","7b8a97f9":"markdown","db4996ba":"markdown","bc697c9d":"markdown","80f72d9d":"markdown","dd0e90ae":"markdown"},"source":{"526877b3":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport tensorflow as tf\nimport os\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.applications.xception import preprocess_input\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom keras import backend as K\nfrom keras import optimizers\nfrom datetime import datetime\nfrom keras.callbacks import ReduceLROnPlateau\n\n%matplotlib inline","50247a84":"data_dir = '..\/input\/intel-image-classification' \ntrain_dir = data_dir + '\/seg_train\/seg_train'\ntest_dir = data_dir + '\/seg_test\/seg_test'\n","015444b1":"# parameters\nimg_width, img_height = 150, 150  # dimensions of the images \nbatch_size = 32\nseed = 50\ncategories = [ 'buildings', 'forest', 'glacier', 'mountain', 'sea', 'street' ]\ncategory_id = {categories:i for i, categories in enumerate(categories)}\n\nclasses = len(categories)\n","089746d9":"train_data_gen = ImageDataGenerator(\n    preprocessing_function = preprocess_input,\n    rotation_range = 10,\n    width_shift_range = 0.1,\n    height_shift_range = 0.1,\n    shear_range = 0.1,\n    horizontal_flip = True,\n    zoom_range = 0.2,\n    validation_split=0.2,\n    fill_mode = 'nearest')\ntest_data_gen = ImageDataGenerator(\n    preprocessing_function = preprocess_input)","61350c1a":"train_data = train_data_gen.flow_from_directory(\n    train_dir\n    ,target_size = (img_width, img_height)\n    ,batch_size = batch_size\n    ,class_mode = \"categorical\"\n    ,subset = 'training'\n)\n\nvalid_data = train_data_gen.flow_from_directory(\n    train_dir \n    ,target_size = (img_width, img_height)\n    ,batch_size = batch_size \n    ,class_mode = \"categorical\"\n    ,subset = 'validation'\n)\n\ntest_data = test_data_gen.flow_from_directory(\n    test_dir \n    ,target_size = (img_width, img_height)\n    ,batch_size = batch_size\n    ,class_mode = \"categorical\"\n)\n","82643a97":"def timer(start_time=None):\n  #function to track time \n    if not start_time:\n        print(datetime.now())\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n\ndef plot_acc_loss(result):\n    # function to plot the accuracy and loss graphs\n    acc = result.history['accuracy']    \n    val_acc = result.history['val_accuracy']\n    loss = result.history['loss']\n    val_loss = result.history['val_loss']\n\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1, 2, 1)\n    plt.title(\"Training and Validation Accuracy\")\n    plt.plot(acc,color = 'green',label = 'Training Acuracy')\n    plt.plot(val_acc,color = 'red',label = 'Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.subplot(1, 2, 2)\n    plt.title('Training and Validation Loss')\n    plt.plot(loss,color = 'blue',label = 'Training Loss')\n    plt.plot(val_loss,color = 'purple',label = 'Validation Loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(loc='upper right')\n    plt.show()\n\ndef plot_imgs(item_dir, top=10):\n    all_item_dirs = os.listdir(item_dir)\n    item_files = [os.path.join(item_dir, file) for file in all_item_dirs][:6]\n  \n    plt.figure(figsize=(10, 10))\n  \n    for idx, img_path in enumerate(item_files):\n        plt.subplot(5, 5, idx+1)        \n        img = plt.imread(img_path)\n        plt.tight_layout() \n        plt.axis('off')\n        plt.imshow(img, cmap='gray')","6b0b501c":"for k in  train_data.class_indices.keys():\n    plot_imgs(train_dir+'\/' + k)","fc034ba8":"if K.image_data_format() == 'channels_first':\n    input_shape = (3, img_width, img_height)\nelse:\n    input_shape = (img_width, img_height, 3)","ad27c127":"xception_model = Xception(\n    include_top = False,\n    weights = \"imagenet\",\n    input_shape = input_shape\n)\n","9f99e897":"learning_rate_reduction = ReduceLROnPlateau(\n    monitor = 'val_accuracy', \n    patience = 3, \n    verbose = 1, \n    factor = 0.5, \n    min_lr = 1e-5)","d48df0bc":"model=tf.keras.models.Sequential()\nmodel.add(xception_model)\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(.25))\nmodel.add(Dense(classes, activation='softmax'))","658a5ce3":"model.summary()","22436ad6":"optimizer = optimizers.SGD(lr = 1e-3, momentum = 0.9)\nmodel.compile(\n    optimizer = optimizer,\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)","2d9908d3":"start_time=timer(None)\nresult = model.fit(\n    train_data, \n    epochs = 100, \n    steps_per_epoch = 32, \n    batch_size = batch_size, \n    validation_data = valid_data,\n    shuffle=True,\n    callbacks = [learning_rate_reduction])\n\ntimer(start_time)","40f2397d":"plot_acc_loss(result)","77679961":"y_pred = model.predict(test_data)\ny_pred = np.argmax(y_pred, axis=1)","bcfb38e7":"print(classification_report(test_data.classes, y_pred))","a9c7484f":"cm = confusion_matrix(test_data.classes, y_pred)\ncm","905b14f8":"print(\"Transfer Learning Testing Accuracy: \", accuracy_score(test_data.classes,y_pred))","64ca9821":"### Visualizing some of the data","52cca8e0":"## Custom functions","61f02e63":"### Accuracy Score","f073eff4":"### Image Data Augmentation","7be7628f":"### Fitting Model","898a5da3":"### Compiling Model","fd668488":"### Classification Report","9d3ec843":"## Importing Libraries","135fb9a8":"## Access the data in the folder","da2b4575":"### For this implementation chose the Xception pre trained model","4c62ddd9":"### Visualizing Result","7b8a97f9":"### Confiusion Matrix","db4996ba":"# Regex CNN MASTERCLASS TASK - Intel Scene Classification Using Transfer Learning","bc697c9d":"#### We can see from above the training Accuracy is around 95% and validation accuracy is around 92%","80f72d9d":"### Predictions","dd0e90ae":"#### As per the Xception documentation at [Keras](https:\/\/keras.io\/api\/applications\/xception\/) : input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (299, 299, 3). It should have exactly 3 inputs channels, and width and height should be no smaller than 71. E.g. (150, 150, 3) would be one valid value."}}