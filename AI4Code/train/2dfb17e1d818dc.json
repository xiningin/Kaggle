{"cell_type":{"0bb2323a":"code","654eada5":"code","66391ca3":"code","f74a6a39":"code","5ee60c77":"code","33998813":"code","419d1277":"code","34073034":"code","8f440a0d":"code","b6f9111d":"code","46520617":"code","ac6566a4":"code","aa7cf897":"code","d4b5ade6":"code","da26d36f":"code","6f0b0f49":"code","096ca6d0":"code","88283f98":"code","2c64ff30":"code","49aef8fd":"code","30e4e0d8":"code","fc63c101":"code","004475f2":"code","86fe7c4f":"code","50bedabf":"code","3194e5b1":"code","cedd0c2e":"code","9c8b89e2":"code","352ac0cb":"code","a6b22594":"code","ee51c269":"code","d6cef10a":"code","69ca1306":"code","8ca4d318":"code","c1630f2d":"code","b4cde36e":"code","6f5a6e70":"code","9782680b":"code","4ec738aa":"code","588030cd":"code","5b6aa844":"code","19259a53":"code","4985d07a":"code","374cfa97":"code","c502bcd1":"code","f9685e16":"code","ba9830cd":"code","0faaf9b2":"code","9cd2a2cf":"code","4875be2a":"code","0d4a90b8":"code","92fe0335":"code","a3e75441":"code","6683a0dd":"code","39536ec2":"code","72c7ced5":"code","d6dd7e5e":"code","33342fb0":"code","97beb166":"code","a0b1b65c":"code","5c4bd961":"code","41f9cfee":"code","52515b00":"code","0f140be8":"code","08a121af":"code","91671fd5":"code","4d630330":"code","95908a78":"code","3cb75559":"code","47583d69":"code","553aa1d9":"code","a06f3859":"code","e09d39c2":"code","02c7d83c":"code","6fe55147":"code","3a0ffc57":"code","ed8ea41d":"code","c7914a66":"code","a1cb60e9":"code","e654cff4":"code","54fdeb20":"code","24de08a7":"code","fba6d620":"code","333c5dac":"code","269b3dd6":"code","36a81589":"code","f48fda2d":"code","98cf9376":"code","f87a57fa":"code","0ade02d9":"code","a112bca8":"code","d56a02c9":"code","b879f4c8":"code","5cedd676":"code","daae58f5":"code","5f3bd80f":"code","b763d236":"code","be39328e":"code","5d97f14d":"code","b51f8597":"code","110bf645":"code","34ecfef6":"code","27c175f6":"code","c34eda39":"code","82b7f2be":"code","3e2f7364":"code","881822e2":"code","dda08e3d":"code","00f31a47":"code","5f9ac02e":"code","6e709314":"code","f6779ca8":"code","ecb5ee98":"code","31f526c0":"code","e8a5b90b":"markdown","de9f776f":"markdown","af2104cb":"markdown","820a0004":"markdown","5a5d2add":"markdown","6b119322":"markdown","7af52ff9":"markdown","05ad552f":"markdown","1ce821a6":"markdown","b9ecf891":"markdown","409763fa":"markdown","feb39b38":"markdown","0f20672e":"markdown"},"source":{"0bb2323a":"!pip install beautifulsoup4","654eada5":"\nimport re \nimport pandas as pd\npd.set_option('display.max_colwidth', 200)\n\nimport numpy as np\nfrom bs4 import BeautifulSoup\n\nimport matplotlib.pyplot as plt  \n\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nimport random\n\nimport torch\nfrom torch import nn\nfrom torch.nn import Sequential, Linear,  ReLU, Sigmoid, Dropout, BCELoss, Embedding, RNN, LSTM\nfrom torchtext import data ","66391ca3":"# load the stackoverflow questions dataset\nquestions_df = pd.read_csv('..\/input\/statsquestions\/Questions.csv',encoding='latin-1')\n\n# load the tags dataset\ntags_df = pd.read_csv('..\/input\/statsquestions\/Tags.csv')","f74a6a39":"questions_df.head()","5ee60c77":"questions_df.shape","33998813":"tags_df.head()","419d1277":"# No. of unique tags\nlen(tags_df['Tag'].unique())","34073034":"# remove \"-\" from the tags\ntags_df['Tag'] = tags_df['Tag'].apply(lambda x:re.sub(\"-\",\" \",x))","8f440a0d":"# group tags Id wise\ntags_df = tags_df.groupby('Id').apply(lambda x:x['Tag'].values).reset_index(name='tags')\ntags_df.head()","b6f9111d":"# merge tags and questions\ndf = pd.merge(questions_df,tags_df,how='inner',on='Id')","46520617":"# fetch required columns\ndf = df[['Id','Body','tags']]","ac6566a4":"#first 5 rows\ndf.head()","aa7cf897":"#shape of the dataset\ndf.shape","d4b5ade6":"# check occurence of each tag\nfreq={}\nfor i in df['tags']:\n  for j in i:\n    if j in freq.keys():\n      freq[j] = freq[j] + 1\n    else:\n      freq[j] = 1","da26d36f":"# sort the dictionary in descending order\nfreq = dict(sorted(freq.items(), key=lambda x:x[1],reverse=True))","6f0b0f49":"# Top 10 most frequent tags\ncommon_tags = list(freq.keys())[:10]\nprint(common_tags)","096ca6d0":"#finding queries associated with common tags\nx=[]\ny=[]\n\nfor i in range(len(df['tags'])):  \n\n  temp=[]\n  for j in df['tags'][i]:\n    if j in common_tags:\n      temp.append(j)\n  \n  #if common tags are more than 1\n  if(len(temp)>1):\n    x.append(df['Body'][i])\n    y.append(temp)","88283f98":"# number of questions left\nlen(x)","2c64ff30":"#first 5 tags\ny[:5]","49aef8fd":"#combining the labels by space\ny = [ \",\".join([str(j) for j in i ]) for i in y]","30e4e0d8":"#labels after converting to string\ny[:5]","fc63c101":"#save to dataframe\ndframe = pd.DataFrame({'query':x,'tags':y})","004475f2":"#first 5 rows\ndframe.head()","86fe7c4f":"#save to csv\ndframe.to_csv('stack.csv',index=False)","50bedabf":"def cleaner(text):\n\n  text = BeautifulSoup(text).get_text()\n  \n  # fetch alphabetic characters\n  text = re.sub(\"[^a-zA-Z]\", \" \", text)\n\n  # convert text to lower case\n  text = text.lower()\n\n  # split text into tokens to remove whitespaces\n  tokens = text.split()\n  \n  return tokens","3194e5b1":"#define field object for query\nmax_len = 100\nTEXT = data.Field(tokenize=cleaner, batch_first=True, fix_length=max_len)","cedd0c2e":"#define field object for label\nLABEL = data.LabelField(batch_first=True)","9c8b89e2":"#define a list of tuple with field objects\nfields = [('query',TEXT),('tags', LABEL)]","352ac0cb":"#reading the dataset\ntraining_data = data.TabularDataset(path = 'stack.csv', format = 'csv', fields = fields, skip_header = True)","a6b22594":"print(training_data)","ee51c269":"#print preprocessed text\nprint(vars(training_data.examples[0]))","d6cef10a":"train_data, valid_data = training_data.split(split_ratio=0.8, random_state = random.seed(32))","69ca1306":"#preparing the vocabulary for the text\nTEXT.build_vocab(train_data, min_freq=3)","8ca4d318":"#No. of unique words\nlen(TEXT.vocab)","c1630f2d":"#word index\nlist(TEXT.vocab.stoi.items())[:10]","b4cde36e":"def fetch_text(examples):\n\n  text=[]\n  for example in examples:\n    query = vars(example)['query']\n    text.append(query)\n    \n  return text","6f5a6e70":"train_text = fetch_text(train_data)\nvalid_text = fetch_text(valid_data)","9782680b":"def convert2seq(text):\n  \n  #padding\n  text = TEXT.pad(text)\n  \n  #converting to numbers\n  text = TEXT.numericalize(text)\n  \n  return text","4ec738aa":"X_train = convert2seq(train_text)\nX_valid = convert2seq(valid_text)","588030cd":"X_train[0]","5b6aa844":"X_train.shape, X_valid.shape","19259a53":"def fetch_tags(data):\n  tags=[]\n  for example in data.examples:\n    tags.append(vars(example)['tags'])\n  return tags","4985d07a":"train_tags = fetch_tags(train_data)\nvalid_tags = fetch_tags(valid_data)","374cfa97":"train_tags[:5]","c502bcd1":"#preparing the output labels \ntrain_tags_list=[i.split(\",\") for i in train_tags]\nvalid_tags_list=[i.split(\",\") for i in valid_tags]","f9685e16":"mlb= MultiLabelBinarizer()\nmlb.fit(train_tags_list)","ba9830cd":"mlb.classes_","0faaf9b2":"y_train  = mlb.transform(train_tags_list)\ny_valid  = mlb.transform(valid_tags_list)","9cd2a2cf":"y_train.shape, y_valid.shape","4875be2a":"type(y_train)","0d4a90b8":"y_train = torch.FloatTensor(y_train)\ny_valid = torch.FloatTensor(y_valid)","92fe0335":"type(y_train)","a3e75441":"# define embedding layer\nemb = Embedding(num_embeddings=len(TEXT.vocab), embedding_dim=50)","6683a0dd":"X_train[:1].shape","39536ec2":"# check sample input\nsample_embedding = emb(X_train[:1])","72c7ced5":"sample_embedding.shape","d6dd7e5e":"#define a rnn\nrnn = RNN(input_size=50, hidden_size=128, batch_first=True, nonlinearity='relu')","33342fb0":"#pass the input to rnn\nhidden_states,last_hidden_state = rnn(sample_embedding)","97beb166":"#Hidden state of every timestep (Batch, seq_len, no. of hidden neurons)\nhidden_states.shape","a0b1b65c":"#output shape of last hidden timestep\nlast_hidden_state.shape","5c4bd961":"#reshaping the hidden states\nreshaped = hidden_states.reshape(hidden_states.size(0),-1)\nreshaped.shape","41f9cfee":"class Net(nn.Module):\n    \n    #define all the layers used in model\n    def __init__(self):\n        \n        #Constructor\n        super(Net, self).__init__()   \n        \n        self.rnn_layer = nn.Sequential(\n            \n            #embedding layer [batch_size,vocab_size]\n            Embedding(num_embeddings=len(TEXT.vocab), embedding_dim=50),\n        \n            #rnn layer [batch_size,100,128]\n            RNN(input_size=50, hidden_size=128, nonlinearity='relu',batch_first=True)\n          \n            )\n\n        self.dense_layer = nn.Sequential(\n            \n            #[batch_size,100*128]\n            Linear(12800, 128),\n\n            ReLU(),\n\n            #[batch_size,128]\n            Linear(128,10),\n            \n            #[batch_size,10]\n            Sigmoid()\n\n        )\n\n    def forward(self, x):\n        \n        #rnn layer\n        hidden_states, last_hidden_state = self.rnn_layer(x)\n\n        #reshaping\n        hidden_states = hidden_states.reshape(hidden_states.size(0),-1)\n\n        #dense layer\n        outputs=self.dense_layer(hidden_states)\n        \n        return outputs","52515b00":"#define the model\nmodel = Net()","0f140be8":"#model layers\nmodel","08a121af":"with torch.no_grad():\n  pred = model(X_train[:1])\n  print(pred)","91671fd5":"#define optimizer and loss\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = BCELoss()\n\n# checking if GPU is available\nif torch.cuda.is_available():\n    model = model.cuda()\n    criterion = criterion.cuda()","4d630330":"# define training function\ndef train(X,y,batch_size):\n\n  #activate training phase\n  model.train()\n  \n  #initialization\n  epoch_loss= 0\n  no_of_batches = 0\n\n  #randomly create indices\n  indices= torch.randperm(len(X))\n  \n  #loading in batches\n  for i in range(0,len(indices),batch_size):\n    \n    #indices for a batch\n    ind = indices[i:i+batch_size]\n  \n    #batch  \n    batch_x=X[ind]\n    batch_y=y[ind]\n    \n    #push to cuda\n    if torch.cuda.is_available():\n        batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n\n    #clear gradients\n    optimizer.zero_grad()\n          \n    #forward pass\n    outputs = model(batch_x)\n\n    #converting to a 1 dimensional tensor\n    outputs = outputs.squeeze()\n\n    #calculate loss and accuracy\n    loss = criterion(outputs, batch_y)\n    \n    #Backward pass\n    loss.backward()\n    \n    #Update weights\n    optimizer.step()\n\n    #Keep track of the loss and accuracy of a epoch\n    epoch_loss = epoch_loss + loss.item()\n\n    #No. of batches\n    no_of_batches = no_of_batches+1\n\n  return epoch_loss\/no_of_batches","95908a78":"# define evaluation function\ndef evaluate(X,y,batch_size):\n\n  #deactivate training phase\n  model.eval()\n\n  #initialization\n  epoch_loss = 0\n  no_of_batches = 0\n\n  #randomly create indices\n  indices= torch.randperm(len(X))\n\n  #deactivates autograd\n  with torch.no_grad():\n    \n    #loading in batches\n    for i in range(0,len(indices),batch_size):\n      \n      #indices for a batch\n      ind = indices[i:i+batch_size]\n  \n      #batch  \n      batch_x= X[ind]\n      batch_y= y[ind]\n\n      #push to cuda\n      if torch.cuda.is_available():\n          batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n        \n      #Forward pass\n      outputs = model(batch_x)\n\n      #converting the output to 1 Dimensional tensor\n      outputs = outputs.squeeze()\n\n      # Calculate loss and accuracy\n      loss = criterion(outputs, batch_y)\n      \n      #keep track of loss and accuracy of an epoch\n      epoch_loss = epoch_loss + loss.item()\n\n      #no. of batches\n      no_of_batches = no_of_batches + 1\n\n    return epoch_loss\/no_of_batches","3cb75559":"# define prediction function\ndef predict(X,batch_size):\n  \n  #deactivate training phase\n  model.eval()\n\n  # initialization \n  predictions = []\n\n  # create indices\n  indices = torch.arange(len(X))\n\n  #deactivates autograd\n  with torch.no_grad():\n      \n      for i in range(0, len(X), batch_size):\n        \n        #indices for a batch\n        ind = indices[i:i+batch_size]\n\n        # batch\n        batch_x = X[ind]\n\n        #push to cuda\n        if torch.cuda.is_available():\n            batch_x = batch_x.cuda()\n\n        #Forward pass\n        outputs = model(batch_x)\n\n        #converting the output to 1 Dimensional tensor\n        outputs = outputs.squeeze()\n\n        # convert to numpy array\n        prediction = outputs.data.cpu().numpy()\n        predictions.append(prediction)\n    \n  # convert to single numpy array\n  predictions = np.concatenate(predictions, axis=0)\n    \n  return predictions","47583d69":"N_EPOCHS = 10\nbatch_size = 32\n\n# intialization\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n     \n    #train the model\n    train_loss   = train(X_train, y_train, batch_size)\n    \n    #evaluate the model\n    valid_loss   = evaluate(X_valid, y_valid, batch_size)\n\n    print('\\nEpoch :',epoch,\n          'Training loss:',round(train_loss,4),\n          '\\tValidation loss:',round(valid_loss,4))\n\n    #save the best model\n    if best_valid_loss >= valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'saved_weights.pt') \n        print(\"\\n----------------------------------------------------Saved best model------------------------------------------------------------------\")   ","553aa1d9":"#load weights of best model\npath='saved_weights.pt'\nmodel.load_state_dict(torch.load(path))","a06f3859":"#predict probabilities\ny_pred_prob = predict(X_valid, batch_size)","e09d39c2":"y_pred_prob[0]","02c7d83c":"#actual tags\ny_true = y_valid.cpu().numpy()","6fe55147":"#define candidate threshold values\nthreshold  = np.arange(0,0.5,0.01)\nprint(threshold)","3a0ffc57":"# convert probabilities into classes or tags based on a threshold value\ndef classify(y_pred_prob, thresh):\n  \n  y_pred = []\n\n  for i in y_pred_prob:\n    temp=[]\n      \n    for j in i:\n      if j>=thresh:\n        temp.append(1)\n      else:\n        temp.append(0)\n    \n    y_pred.append(temp)\n\n  return np.array(y_pred)","ed8ea41d":"score=[]\n\nfor thresh in threshold:\n    \n    #classes for each threshold\n    y_pred = classify(y_pred_prob, thresh) \n\n    #convert to 1d array\n    y_pred_1d    =  y_pred.ravel()\n    y_true_1d    =  y_true.ravel()\n \n    score.append(metrics.f1_score(y_true_1d, y_pred_1d))","c7914a66":"# find the optimal threshold\nopt = threshold[score.index(max(score))]\nprint(opt)","a1cb60e9":"#predictions for optimal threshold\ny_pred = classify(y_pred_prob, opt)","e654cff4":"#converting to 1D\ny_pred_1d = y_pred.ravel()\n\n#Classification report\nprint(metrics.classification_report(y_true_1d, y_pred_1d))","54fdeb20":"#convert back to tags\ny_pred_label = mlb.inverse_transform(np.array(y_pred))\ny_true_label = mlb.inverse_transform(np.array(y_true))\n\n# get all validation text\nqueries = [\" \".join(i) for i in valid_text]\n\n# create a dataframe to show the data and prediction side by side\ndf = pd.DataFrame({'Questions':queries,'Actual Tags':y_true_label,'Predicted Tags':y_pred_label})\n\n# print first five rows\ndf.head()","24de08a7":"#raw text\ntext = \"For example, in the case of logistic regression, the learning function is a Sigmoid function that tries to separate the 2 classes\"","fba6d620":"#cleaning text\ntokens = cleaner(text)\ntokens[:5]","333c5dac":"#first argument to the model is no. of samples\ntokens = np.array(tokens).reshape(-1,len(tokens))\ntokens.shape","269b3dd6":"#converting text to integer sequences\nseq = convert2seq(tokens)\nseq","36a81589":"#predictions\nwith torch.no_grad():\n  if torch.cuda.is_available():\n    seq = seq.cuda()\n  pred_prob= model(seq)\n  print(pred_prob)","f48fda2d":"#classify\npred = classify(pred_prob,opt)\npred","98cf9376":"tags  = mlb.inverse_transform(pred)[0]\ntags","f87a57fa":"def predict_tags(text):\n  \n  tokens = cleaner(text)\n  \n  tokens = np.array(tokens).reshape(-1,len(tokens))\n  \n  seq = convert2seq(tokens)\n  \n  with torch.no_grad():\n    if torch.cuda.is_available():\n      seq = seq.cuda()\n\n  pred_prob= model(seq)\n  pred = classify(pred_prob,opt)\n  \n  tags  = mlb.inverse_transform(pred)[0]\n  \n  return tags","0ade02d9":"text = \"For example, in the case of logistic regression, the learning function is a Sigmoid function that tries to separate the 2 classes\"\n\ntags = predict_tags(text)\nprint(\"Query: \", text)\nprint(\"Predicted tags:\",tags)","a112bca8":"sample_embedding.shape","d56a02c9":"#define an LSTM\nlstm_layer = LSTM(input_size=50, hidden_size=128, batch_first=True)","b879f4c8":"#pass the input to LSTM\nhidden_states, (last_hidden_state,last_cell_state) = lstm_layer(sample_embedding)","5cedd676":"#Hidden state of every timestep (Batch, seq_len, no. of hidden neurons)\nhidden_states.shape","daae58f5":"#output shape of last hidden timestep\nlast_hidden_state.shape","5f3bd80f":"#output shape of last cell state\nlast_cell_state.shape","b763d236":"#reshaping the hidden states\nreshaped = hidden_states.reshape(hidden_states.size(0),-1)\nreshaped.shape","be39328e":"class Net(nn.Module):\n    \n    #Constructor\n    def __init__(self):\n\n        #Constructor\n        super(Net, self).__init__()   \n  \n        #rnn block\n        self.lstm_layer = Sequential(\n            \n            #embedding layer\n            Embedding(num_embeddings=len(TEXT.vocab), embedding_dim=100),\n        \n            #lstm layer\n            LSTM(input_size=100, hidden_size=128, batch_first=True)\n          \n            )\n\n        #dense block\n        self.dense_layer = Sequential(\n            \n            Linear(12800,128),\n\n            ReLU(),\n\n            Linear(128,10),\n            \n            Sigmoid()\n\n        )\n    \n    #forward pass\n    def forward(self, x):\n        \n        #rnn layer\n        hidden_states, (last_hidden_state,last_cell_state) = self.lstm_layer(x)\n\n        #flattening\n        hidden_states = hidden_states.reshape(hidden_states.size(0),-1)\n        \n        #dense layer\n        outputs=self.dense_layer(hidden_states)\n        \n        return outputs","5d97f14d":"#define the model\nmodel = Net()","b51f8597":"#layers of the model\nmodel","110bf645":"with torch.no_grad():\n  pred = model(X_train[:1])\n  print(pred)","34ecfef6":"#define optimizer and loss\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = BCELoss()\n\n# checking if GPU is available\nif torch.cuda.is_available():\n    model = model.cuda()\n    criterion = criterion.cuda()","27c175f6":"N_EPOCHS = 10\nbatch_size = 32\n\n# intialization\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n     \n    #train the model\n    train_loss   = train(X_train, y_train, batch_size)\n    \n    #evaluate the model\n    valid_loss   = evaluate(X_valid, y_valid, batch_size)\n\n    print('\\nEpoch :',epoch,\n          'Training loss:',round(train_loss,4),\n          '\\tValidation loss:',round(valid_loss,4))\n\n    #save the best model \n    if best_valid_loss >= valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'saved_weights.pt') \n        print(\"\\n----------------------------------------------------Saved best model------------------------------------------------------------------\")   \n\n","c34eda39":"#load weights of best model\npath='saved_weights.pt'\nmodel.load_state_dict(torch.load(path))","82b7f2be":"#predict probabilities\ny_pred_prob = predict(X_valid, batch_size)","3e2f7364":"y_pred_prob[0]","881822e2":"score=[]\n\nfor thresh in threshold:\n    \n    #classes for each threshold\n    y_pred = classify(y_pred_prob, thresh) \n\n    #convert to 1d array\n    y_pred_1d    =  y_pred.ravel()\n    y_true_1d    =  y_true.ravel()\n \n    score.append(metrics.f1_score(y_true_1d, y_pred_1d))","dda08e3d":"# find the optimal threshold\nopt = threshold[score.index(max(score))]\nprint(opt)","00f31a47":"#predictions for optimal threshold\ny_pred = classify(y_pred_prob, opt)","5f9ac02e":"#converting to 1D\ny_pred_1d = y_pred.ravel()\n\n#Classification report\nprint(metrics.classification_report(y_true_1d, y_pred_1d))","6e709314":"y_pred_label = mlb.inverse_transform(np.array(y_pred))","f6779ca8":"df = pd.DataFrame({'comment':queries,'actual':y_true_label,'predictions':y_pred_label})","ecb5ee98":"df.head()","31f526c0":"text = \"For example, in the case of logistic regression, the learning function is a Sigmoid function that tries to separate the 2 classes\"\n\ntags = predict_tags(text)\nprint(\"Query: \",text)\nprint(\"Predicted tags:\",tags)","e8a5b90b":"# Dataset Preparation","de9f776f":"# Text Preprocessing","af2104cb":"# Please upvote the notebook if you find it insightful!","820a0004":"We will use only those questions\/queries that are associated with the top 10 tags.","5a5d2add":"![alt text](https:\/\/cdn.sstatic.net\/Sites\/stackoverflow\/company\/img\/logos\/se\/se-logo.svg?v=d29f0785ebb7)\n\nThe objective of notebook is to build a model to automatically predict tags for a given a StackExchange question by using the text of the question in PyTorch using TorchText.\n\nDataset:Over 85,000 questions and over 1300 unique tags\n\nThe question-answering site StackOverflow allows users to assign tags to questions in order to make them easier for other people to find. Further experts on a certain topic can subscribe to tags to receive digests of new questions for which they might have an answer. Therefore it is both in the interest of the original poster and in the interest of people who are interested in the answer that a question gets assigned appropriate tags.\n","6b119322":"# Model Evaluation","7af52ff9":"# Show Inference","05ad552f":"# Load Data and Import Libraries","1ce821a6":"# Model Building for LSTM","b9ecf891":"# Model Building for RNN","409763fa":"# Model Evaluation","feb39b38":"Next we are going to create a list of tuples where first value in every tuple contains a column name and second value is a field object.","0f20672e":"# Model Training"}}