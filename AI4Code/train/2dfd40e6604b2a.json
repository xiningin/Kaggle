{"cell_type":{"2b03acbf":"code","e5a522cd":"code","d164f03f":"code","c005d9df":"code","2c3e9f79":"code","d79d7bfa":"code","6440093e":"code","b7742e7b":"markdown","a6b2e8fd":"markdown","e0b98355":"markdown","ff146583":"markdown"},"source":{"2b03acbf":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version \"nightly\"","e5a522cd":"!pip install -U pytorch_lightning","d164f03f":"import os\nfrom pathlib import Path\n\nimport albumentations as A\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations.augmentations.transforms import Blur, RandomBrightness\nfrom torch.utils.data import DataLoader, Dataset\n\n\nclass ChineseMNISTDataset(Dataset):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        image_root: Path,\n        transform: A.BasicTransform = None,\n    ) -> None:\n        super().__init__()\n        self.df = df\n        self.image_root = image_root\n        self.transform = transform\n\n    def __getitem__(self, idx: int):\n        row = self.df.loc[idx, :]\n        suite_id, code, sample_id = row.suite_id, row.code, row.sample_id\n        filename = self.image_root \/ f\"input_{suite_id}_{sample_id}_{code}.jpg\"\n        assert os.path.isfile(filename), f\"{filename} is not a file\"\n        image = cv2.imread(str(filename))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        image = image[:, np.newaxis]\n        if self.transform is not None:\n            image = self.transform(image=image)[\"image\"]\n        return image, code - 1\n\n    def __len__(self):\n        return len(self.df)\n\n\nclass ChineseMNISTDataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        data_root: Path,\n        all_df: pd.DataFrame,\n        train_indices: pd.Index,\n        val_indices: pd.Index,\n        batch_size: int = 64\n    ) -> None:\n        super().__init__()\n        self.data_root = data_root\n        self.df = all_df\n        self.image_root = self.data_root \/ \"data\" \/ \"data\"\n        self.train_df = self.df.loc[train_indices, :].copy().reset_index()\n        self.train_transform = A.Compose(\n            [\n                Blur(),\n                RandomBrightness(),\n                ToTensorV2(),\n            ]\n        )\n        self.val_df = self.df.loc[val_indices, :].copy().reset_index()\n        self.val_transform = A.Compose(\n            [\n                ToTensorV2(),\n            ]\n        )\n        self.batch_size = batch_size\n\n    def train_dataloader(self):\n        ds = ChineseMNISTDataset(self.train_df, self.image_root, self.train_transform)\n        return DataLoader(\n            ds,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=4,\n            pin_memory=True,\n        )\n\n    def val_dataloader(self):\n        ds = ChineseMNISTDataset(self.val_df, self.image_root, self.val_transform)\n        return DataLoader(\n            ds,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=4,\n            pin_memory=True,\n        )\n\n    def test_dataloader(self):\n        return self.val_dataloader()\n    \n# sanity check\nis_kaggle = os.path.isdir(\"\/kaggle\")\ndata_root = Path(\"\/kaggle\/input\/chinese-mnist\" if is_kaggle else \"archive\")\nassert os.path.isdir(data_root), f\"{data_root} is not a dir\"\ndf = pd.read_csv(data_root \/ \"chinese_mnist.csv\")\n\ndata_module = ChineseMNISTDataModule(data_root, df, df.index[:20], df.index[20:30])","c005d9df":"import os\nfrom pathlib import Path\n\nimport pandas as pd\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\nfrom pytorch_lightning.metrics import Accuracy\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch import nn, optim\nfrom torchvision.models import resnet18\n\n\nclass ChineseMNISTResnetModel(pl.LightningModule):\n    def __init__(self, learning_rate=1e-3):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.num_classes = 15\n        resnet = resnet18(pretrained=True, progress=True)\n        resnet.conv1 = nn.Conv2d(\n            in_channels=1,\n            out_channels=resnet.conv1.out_channels,\n            kernel_size=resnet.conv1.kernel_size,\n            stride=resnet.conv1.stride,\n            dilation=resnet.conv1.dilation,\n            bias=resnet.conv1.bias,\n        )\n        resnet.fc = nn.Linear(512, self.num_classes)\n        self.resnet = resnet\n        self.accuracy = Accuracy()\n        self.criterion = nn.CrossEntropyLoss()\n\n    def forward(self, image):\n        image = image.permute(0, 3, 1, 2).contiguous().float()\n        return self.resnet(image)\n\n    def training_step(self, batch, batch_idx: int):\n        image, y = batch\n        yhat = self(image)\n        loss = self.criterion(yhat, y)\n        acc = self.accuracy(yhat, y)\n        return loss\n\n    def validation_step(self, batch, batch_idx: int, log: bool = True):\n        image, y = batch\n        yhat = self(image)\n        loss = self.criterion(yhat, y)\n        acc = self.accuracy(yhat, y)\n        if log:\n            self.log('val_loss', loss, prog_bar=True, on_epoch=True, on_step=False)\n            self.log('val_acc', acc, prog_bar=True, on_epoch=True, on_step=False)        \n        return {'val_loss': loss, 'val_acc': acc}\n\n    def test_step(self, batch, batch_idx):\n        metrics = self.validation_step(batch, batch_idx, log = False)\n        self.log('test_loss', metrics[\"val_loss\"], on_epoch=True, on_step=False)\n        self.log('test_acc', metrics[\"val_acc\"], on_epoch=True, on_step=False)    \n        return {\"test_acc\": metrics[\"val_acc\"], \"test_loss\": metrics[\"val_loss\"]}\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n","2c3e9f79":"is_kaggle = os.path.isdir(\"\/kaggle\")\ndata_root = Path(\"\/kaggle\/input\/chinese-mnist\" if is_kaggle else \"archive\")\nall_df = pd.read_csv(data_root \/ \"chinese_mnist.csv\")\n\nskf = StratifiedKFold(n_splits=5, shuffle=True)\n\ncheckpoint_callback = ModelCheckpoint(\n    filepath=os.getcwd(),\n    save_top_k=1,\n    verbose=True,\n    monitor=\"val_loss\",\n    mode=\"min\",\n)\ntrainer = pl.Trainer(\n    # gpus=1,\n    tpu_cores=8,\n    max_epochs=20,\n    precision=16,\n    val_check_interval=1.,\n    callbacks=[checkpoint_callback]\n)\n\nfor train_indices, val_indices in skf.split(all_df, all_df.code):\n    data_module = ChineseMNISTDataModule(\n        data_root=data_root,\n        all_df=all_df,\n        train_indices=train_indices,\n        val_indices=val_indices,\n        batch_size=32\n    )\n    model = ChineseMNISTResnetModel()\n    trainer.fit(model, datamodule=data_module)\n    break","d79d7bfa":"model.load_state_dict(torch.load(checkpoint_callback.best_model_path)[\"state_dict\"])\ntrainer.test(test_dataloaders=data_module.train_dataloader(),ckpt_path=None)","6440093e":"trainer.test(test_dataloaders=data_module.val_dataloader(),ckpt_path=None)","b7742e7b":"## Define dataset and dataloader via pytorch-lightning datamodule","a6b2e8fd":"Note from Ceshine:\n\nThis is a fork of [Chinese MNIST with pytorch-lightning and Resnet](https:\/\/www.kaggle.com\/etareduce\/chinese-mnist-with-pytorch-lightning-and-resnet). The motivation behind this fork is that I couldn't make PyTorch Lightning to train on TPU with a more complicated setup, so I wanted to try a much simpler scenario and then add more stuffs one-by-one. Hopefully this can help me figure out which part of code is blocking the training.\n\nI also updated some of the code so it is compatible with PyTorch Lightning 1.0+.\n\n# Overview\n\n\nThis notebook will demonstrates solving Chinese MNIST classification task using:\n\n1. `torch` and `torchvision` with pretrained resnet 18\n2. `pytorch-lightning` for code formation (you can see easily setup GPU training and half precision flag)\n3. `albumentations` for augmentations\n4. sklearn's stratified k-fold cross-validation for equally distribute labels (`code`s) amongst different folds\n\nAlthought it can achieve 1.0 or at least 0.98 accuracy, there is still room for improvement, _for which I don't have more time_.","e0b98355":"## Model definition","ff146583":"## Install dependencies"}}