{"cell_type":{"e4b43f53":"code","b49ba09d":"code","44f05ece":"code","98704c7b":"code","7e402e7f":"code","e79d4cdc":"code","210bcc90":"code","a7155f18":"code","8825368b":"code","8e926839":"code","a7f8a9b5":"code","549ca176":"code","5533f28a":"code","3b3c7f81":"code","a139b67e":"code","b0322cf4":"code","a7ba0133":"code","26acd421":"code","3d01b102":"code","ee80e103":"code","655b4ff2":"code","2e341a94":"code","ccfb1351":"code","cdf887d0":"code","079f1919":"code","7688e4c7":"code","5181a8d0":"code","15407fe1":"code","23c196ba":"code","328f4bc6":"code","51e23d1f":"code","1f4d1257":"code","45d24499":"code","59a38070":"code","2e07a8de":"code","66f8a925":"code","7a8c5e85":"code","9702a0e8":"code","f92afa57":"code","f7385d23":"code","bc38c5c4":"code","031b1e1f":"code","c3ebd188":"code","c01e6307":"code","3bf49584":"code","1fc07c5d":"code","5f9da144":"code","aad495a9":"code","b588e1ff":"code","9f4e51c8":"markdown","718f3f33":"markdown","c72e41f5":"markdown","ebf1dd67":"markdown","07641886":"markdown","a2b7d844":"markdown","dc4b41a2":"markdown","097dbaff":"markdown","f415c8bd":"markdown","cf69edc5":"markdown","5a2a2aa4":"markdown","fc3efba1":"markdown","704c94a7":"markdown","86cf1552":"markdown","9e7249ae":"markdown","1e00ba45":"markdown","84d75127":"markdown","14bca1fb":"markdown","47c8469e":"markdown","3762de48":"markdown","c302910a":"markdown","9c51614b":"markdown","91564f19":"markdown","7a030bfa":"markdown","0f800b75":"markdown","e5ca6b5d":"markdown","54c5bbfc":"markdown"},"source":{"e4b43f53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b49ba09d":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ngender_submission = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","44f05ece":"train.head()","98704c7b":"train.info()","7e402e7f":"train['Age'].fillna(value = round(np.mean(train['Age'])), inplace=True)\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(16, 8))\nwomen = train[train['Sex']=='female']\nmen = train[train['Sex']=='male']\nax = sns.distplot(women[women.Survived == 1].Age, bins =18, label = 'Survived', ax=axes[0], kde=False, color='green')\nax = sns.distplot(women[women.Survived == 0].Age, bins=40, label = 'Not Survived', ax = axes[0], kde =False, color=\"red\")\nax.legend()\nax.set_title('Female')\nax1 = sns.distplot(men[men.Survived == 1].Age, bins = 18, label = 'Survived', ax = axes[1], kde = False, color = 'green')\nax1 = sns.distplot(men[men.Survived == 0].Age, bins = 40, label = 'Not Survived', ax = axes[1], kde = False, color = 'red')\nax1.legend()\nax1.set_title('Male')","e79d4cdc":"sns.barplot(data=train, x='Pclass', y='Survived')","210bcc90":"train['relatives'] = train['SibSp'] + train['Parch']\ntrain.loc[train['relatives'] > 0, 'travelled_alone'] = 'No'\ntrain.loc[train['relatives'] == 0, 'travelled_alone'] = 'Yes'\nsns.factorplot(data = train, x ='relatives', y='Survived', aspect = 2.5)","a7155f18":"cont_var = train[['SibSp', 'Parch', 'Fare']]","8825368b":"women = train.loc[train.Sex == 'female']['Survived']\nrate_women = sum(women)\/len(women)\nprint('% of women who survived :', rate_women )","8e926839":"men = train.loc[train.Sex == 'male']['Survived']\nrate_men = sum(men) \/ len(men)\nprint('% of women who survived :', rate_men)","a7f8a9b5":"plot = sns.FacetGrid(train, row='Embarked', size=4.5, aspect=1.6)\nplot.map(sns.pointplot, 'Pclass', 'Survived', 'Sex')\nplot.add_legend()","549ca176":"sns.set(style='darkgrid')\nsns.countplot(data=train, x='Survived', hue='Embarked', palette='Set2')","5533f28a":"disc_var = train[['Ticket', 'Cabin']]","3b3c7f81":"train['cabin_multiple'] = train.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\ntrain['cabin_multiple'].value_counts()","a139b67e":"train['cabin_adv'] = train.Cabin.apply(lambda x: str(x)[0])\nprint(train.cabin_adv.value_counts())","b0322cf4":"pd.pivot_table(train, index='Survived', columns='cabin_adv', values='Ticket', aggfunc='count')","a7ba0133":"#pd.set_option('max_rows', None)\ntrain['Ticket'].value_counts()","26acd421":"train['numeric_ticket'] = train.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntrain['ticket_letters'] = train.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) > 0 else 0)","3d01b102":"train['numeric_ticket'].value_counts()","ee80e103":"pd.set_option('max_rows', None)\ntrain['ticket_letters'].value_counts()","655b4ff2":"pd.pivot_table(train, index='Survived', columns='numeric_ticket', values='Ticket', aggfunc='count')","2e341a94":"train['Name'].head(10)","ccfb1351":"train['name_title'] = train.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntrain['name_title'] = train['name_title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona', 'the Countess'], 'Rare')\ntrain['name_title'] = train['name_title'].replace('Mlle', 'Miss')\ntrain['name_title'] = train['name_title'].replace('Ms', 'Miss')\ntrain['name_title'] = train['name_title'].replace('Mme', 'Mrs')","cdf887d0":"train['name_title'] = train['name_title'].map({'Mr':1, 'Miss':2, 'Mrs':3, 'Master':4, 'Rare':5})","079f1919":"train['name_title'].value_counts()","7688e4c7":"train['Sex'] = train['Sex'].replace(['male'], 0)\ntrain['Sex'] = train['Sex'].replace(['female'], 1)\ntrain['FirstClass'] = train['Pclass'].apply(lambda x:1 if x == 1 else 0)\ntrain['SecondClass'] = train['Pclass'].apply(lambda x:1 if x == 2 else 0)","5181a8d0":"print(train['Embarked'].head())","15407fe1":"train.dropna(subset=['Embarked'], inplace=True)","23c196ba":"train['Embarked'] = train['Embarked'].replace(['C'], 1)\ntrain['Embarked'] = train['Embarked'].replace(['Q'], 2)\ntrain['Embarked'] = train['Embarked'].replace(['S'], 3)","328f4bc6":"print(train['Embarked'].isna().sum())","51e23d1f":"features = train[['Sex', 'Age','FirstClass', 'SecondClass', 'Embarked', 'relatives']]\nsurvival = train['Survived']","1f4d1257":"#for col in features.columns[6:]:\n    #features = pd.get_dummies(features, columns = [col], prefix = [col])","45d24499":"pd.set_option('max_columns', None)\nfeatures.head()","59a38070":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(features, survival, test_size = 0.8)","2e07a8de":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntrain_features = scaler.fit_transform(x_train)\ntest_features = scaler.transform(x_test)","66f8a925":"test.head(10)","7a8c5e85":"test.info()","9702a0e8":"test['cabin_multiple'] = test.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\ntest['cabin_adv'] = test.Cabin.apply(lambda x: str(x)[0])\ntest['numeric_ticket'] = test.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntest['ticket_letters'] = test.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) > 0 else 0)\ntest['name_title'] = test.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntest['Sex'] = test['Sex'].replace(['male'], 0)\ntest['Sex'] = test['Sex'].replace(['female'], 1)\ntest['Age'].fillna(value = round(np.mean(test['Age'])), inplace=True)\ntest['FirstClass'] = test['Pclass'].apply(lambda x:1 if x == 1 else 0)\ntest['SecondClass'] = test['Pclass'].apply(lambda x:1 if x == 2 else 0)\ntest['Embarked'] = test['Embarked'].replace(['C'], 1)\ntest['Embarked'] = test['Embarked'].replace(['Q'], 2)\ntest['Embarked'] = test['Embarked'].replace(['S'], 3)\ntest['relatives'] = test['SibSp'] + test['Parch']","f92afa57":"datatest_features = test[['Sex', 'Age','FirstClass', 'SecondClass', 'Embarked', 'relatives']]\nprint(datatest_features.head())","f7385d23":"#for col in datatest_features.columns[10:]:\n    #datatest_features = pd.get_dummies(datatest_features, columns = [col], prefix = [col])\n    \ntest_passenger = scaler.transform(datatest_features)","bc38c5c4":"from sklearn.linear_model import LinearRegression\nlr_model = LinearRegression()\nlr_model.fit(train_features, y_train)","031b1e1f":"print(lr_model.score(train_features, y_train))\nprint(lr_model.score(test_features, y_test))","c3ebd188":"print(lr_model.coef_)","c01e6307":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier(n_neighbors = 2).fit(train_features, y_train)\nprint(knn_model.score(train_features, y_train))\nprint(knn_model.score(test_features, y_test))","3bf49584":"from sklearn.linear_model import LogisticRegression\n\nlogr_model = LogisticRegression().fit(train_features, y_train)\n\nprint(logr_model.score(train_features, y_train))\nprint(logr_model.score(test_features, y_test))","1fc07c5d":"from sklearn.svm import SVC\n\nsvc_model = SVC(C = 50).fit(train_features, y_train)\n\nprint(svc_model.score(train_features, y_train))\nprint(svc_model.score(test_features, y_test))","5f9da144":"from sklearn.tree import DecisionTreeClassifier\n\ntree_model = DecisionTreeClassifier().fit(train_features, y_train)\n\nprint(tree_model.score(train_features, y_train))\nprint(tree_model.score(test_features, y_test))","aad495a9":"from sklearn.ensemble import RandomForestClassifier\n\nrforest_model = RandomForestClassifier(n_estimators = 150).fit(train_features, y_train)\n\nprint(rforest_model.score(train_features, y_train))\nprint(rforest_model.score(test_features, y_test))","b588e1ff":"logr_survived = logr_model.predict(test_passenger)\npassengerID = test['PassengerId']\nresult = list(zip(passengerID, logr_survived))\nlogr_result = pd.DataFrame(result, columns = ['PassengerId', 'Survived'])\nfl = logr_result.to_csv('submission.csv', index = False)\nprint(fl)","9f4e51c8":"### Random Forest","718f3f33":"### Age\n\nAge between 15 - 20 years old are more likely to survive","c72e41f5":"## Discrete Variable","ebf1dd67":"### Embarked\nWomen will survive more if they embarked from port \u2018Southampton\u2019 or \u2018 Queenstown\u2019. While men will survive more from the port 'Cherbourg'. Passengers from port \u2018Southampton\u2019 have a low survival rate of 34%, while those from the port \u2018Cherbourg\u2019 have a survival rate of 55%. Over 72% of the passengers embarked from the port 'Southampton\u2019, 18% from the port \u2018Cherbourg\u2019 and the rest from the port \u2018Queenstown\u2019.","07641886":"# Data Exploration","a2b7d844":"### Pclass\nYou have a higher chance of surviving if you have a first class ticket than having a second or thir.d","dc4b41a2":"### Decision Trees","097dbaff":"### Splitting Data","f415c8bd":"### KNN ","cf69edc5":"### Relatives\nYou are more likely to survive if you are travels with 1 to 3 people and if you have 0 or more than three you have a less chance.","5a2a2aa4":"### Dummy Variable\nDummy Variables are created to avoid categorical variables, so the dummy variable will change the category per variable to its own binary identifier.","fc3efba1":"### Cabin","704c94a7":"### Scaling Data","86cf1552":"## Model","9e7249ae":"### Support Vector Machines","1e00ba45":"### Person's titles","84d75127":"# Data Preprocessing for Model\n1) Drop null values from Embarked (only 2) and other null value from another feature\n\n2) Include only relevant variables (Since we have limited data, I wanted to exclude things like name and passanger ID so that we could have a reasonable number of features for our models to deal with)\n\nVariables: 'Pclass', 'Sex','Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'cabin_adv', 'cabin_multiple', 'numeric_ticket', 'name_title'\n\n3) Do categorical transforms on all data. Usually we would use a transformer, but with this approach we can ensure that our traning and test data have the same colums. We also may be able to infer something about the shape of the test data through this method. I will stress, this is generally not recommend outside of a competition (use onehot encoder).\n\n4) Impute data with mean for fare and age (Should also experiment with median)\n\n5) Normalized fare using logarithm to give more semblance of a normal distribution\n\n6) Scaled data 0-1 with standard scaler","14bca1fb":"The sinking of the Titanic is an event that shocked the world, almost everyone knows this incident. This is a dataset analysis project from Titanic containing passenger data. We will analyze whether a passenger survived or not in this incident by using the existing features.\n\nHere there are 2 datasets, namely train and test in .csv format.","47c8469e":"# Test Dataset","3762de48":"### Logistic Regression","c302910a":"The first one we will analyze is the train dataset. The train data set consists of the following columns:\n\nSurvived => 0 : no \/ 1 : yes\n\nPclass (Class Ticket) => 1 : 1st \/ 2 : 2nd \/ 3 : 3rd\n\nName\n\nSex => 0 : man \/ 1 : woman\n\nAge\n\nSibsp (of siblings \/ spouses aboard the Titanic)\n\nParch (of parents \/ children aboard the Titanic)\n\nTicket (Ticket Number)\n\nFare (Passenger Fare)\n\nCabin (Kabin Number) \n\nEmbarked (Port of Embarkation) => C : Cherbourg \/ Q = Queenstown \/ S = Southampton","9c51614b":"### Sex\n\nWomen and Children were the first to board the titanic which means they are more likely to survive than man.","91564f19":"## Conclusion\n\u200b\nAfter we create several machine learning models using the train dataset, here are the predicted results:\n\u200b\n###### Linear Regression : 42%\n###### KNN : 83%\n###### Logistic Regression : 85%\n###### Support Vector Machines : 94%\n###### Decision Trees : 99%\n###### Random Forest : 99%","7a030bfa":"# Feature Engineering\n1) Cabin - simplify cabins (evaluated if cabin letter (cabin_adv) or the purchase of tickets across multiple cabins (cabin_multiple) impacted survival)\n\n2) Tickets - Do different ticket types impact survival rates?\n\n3) Does a person's title realte to survival rates?","0f800b75":"### Ticket","e5ca6b5d":"### Linear Regression","54c5bbfc":"## Contionus Variable "}}