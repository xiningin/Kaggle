{"cell_type":{"85db3aec":"code","7df9742d":"code","91482ae7":"code","4b438403":"code","d5ac43f5":"code","59b251dc":"code","b2e220d5":"code","0b83c12e":"code","c4c5f8de":"code","869bafcb":"code","7b90f191":"code","f793315c":"code","add49e07":"code","51061cba":"code","aa69515c":"code","1fb6b11a":"code","894c9e13":"code","d5ae19b0":"code","f142dc52":"code","92fa366b":"code","3eb58667":"code","e35cb1c0":"code","498395b8":"code","d539f293":"code","8499fa91":"code","73734f26":"code","3ff85aa9":"code","f0d4d002":"code","cf97b5f0":"code","a5dddc39":"code","3c456632":"code","4f1421d8":"code","028f3fc1":"code","d4292c6c":"markdown","9678004d":"markdown","7a7f9910":"markdown","e55c8039":"markdown","d568ccf5":"markdown","4e120d09":"markdown","11da10d2":"markdown","8523ac32":"markdown","e3b4e326":"markdown","e77c9cb9":"markdown","5be8316b":"markdown","7c78baa7":"markdown","03cd2a94":"markdown","a095ea5b":"markdown","e37e121a":"markdown","f7f62272":"markdown"},"source":{"85db3aec":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","7df9742d":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","91482ae7":"df.describe()","4b438403":"df.info()","d5ac43f5":"df.shape","59b251dc":"# Time VS Amount, red - fraud, blue - non-fraud. \nfraud_df = df[df.Class == 1]\nplt.scatter(df.Time, df.Amount, color = 'blue')\nplt.xlabel('Time')\nplt.ylabel('Amount')\n# plt.show()\nplt.scatter(fraud_df.Time, fraud_df.Amount, color = 'red')\nplt.xlabel('Time')\nplt.ylabel('Amount')\nplt.show()","b2e220d5":"# correlation maatrix\n\ncorr = df.corr()\nround(corr,2)","0b83c12e":"sns.heatmap(corr);","c4c5f8de":"# look potential statistical distrbution in different features based on fraud or not. \n\nfig, axes = plt.subplots(7, 4, figsize=(24, 16))\nfig.suptitle('Density Plot for each feature')\n\nsns.kdeplot(ax=axes[0,0],x='V1', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[0,1],x='V2', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[0,2],x='V3', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[0,3],x='V4', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[1,0],x='V5', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[1,1],x='V6', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[1,2],x='V7', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[1,3],x='V8', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[2,0],x='V9', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[2,1],x='V10', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[2,2],x='V11', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[2,3],x='V12', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[3,0],x='V13', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[3,1],x='V14', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[3,2],x='V15', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[3,3],x='V16', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[4,0],x='V17', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[4,1],x='V18', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[4,2],x='V19', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[4,3],x='V20', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[5,0],x='V21', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[5,1],x='V22', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[5,2],x='V23', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[5,3],x='V24', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[6,0],x='V25', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[6,1],x='V26', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[6,2],x='V27', hue='Class', data= df, shade=True)\nsns.kdeplot(ax=axes[6,3],x='V28', hue='Class', data= df, shade=True)\n","869bafcb":"sns.distplot(df.Time.values)\nsns.distplot(fraud_df.Time.values)","7b90f191":"sns.distplot(df.Amount.values)\nsns.distplot(fraud_df.Amount.values)","f793315c":"# sns.set_theme(style=\"darkgrid\")\nsns.countplot('Class', hue = 'Class', data=df)\nplt.title('Data Class Distributions  \\n (0: No Fraud, 1: Fraud)', fontsize=14)","add49e07":"# rescale \"Time\" and \"Amount\"\nfrom sklearn.preprocessing import RobustScaler\n\nrobust_scaler = RobustScaler()\ndf['scaled_amount'] = robust_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = robust_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)","51061cba":"df.head()","aa69515c":"# Split data\nfrom sklearn.model_selection import train_test_split \n\n# train, test_ = train_test_split(df, train_size=0.8, random_state=2021, shuffle=True )\n# train_df, validation = train_test_split(train, test_size=0.5, random_state=2021, shuffle=True )\n\nX = df.drop('Class', axis = 1)\ny = df['Class']\ntrain_X,test_X, train_y, test_y = train_test_split(X,y, test_size=0.3, random_state=2021)\n\n# train, validation = train_test_split(train, test_size=0.223, random_state=2021) # 0.777 x 0.9 = 0.7 \n\n# train --> 0.7, validation --> 0.2,  test --> 0.1","1fb6b11a":"print(train_X.shape, train_y.shape)\nprint(test_X.shape, test_y.shape)","894c9e13":"# Handle imbalanced data\nfrom imblearn.over_sampling import SMOTE\n# from imblearn.under_sampling import RandomUnderSampler\n\n\noversample = SMOTE()\ntrain_X, train_y = oversample.fit_resample(train_X, train_y)\n\nprint(sum(train_y==0))\nprint(sum(train_y==1))\n# Now the class is balanced. ","d5ae19b0":"# from sklearn.metrics import roc_auc_score\n# from sklearn.model_selection import cross_val_predict\n# from sklearn.model_selection import cross_val_score\n\n# # Classifier Libraries\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.svm import SVC\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.tree import DecisionTreeClassifier\n\n\n# classifiers = {\n#     \"LogisiticRegression\": LogisticRegression(),\n#     \"KNearest\": KNeighborsClassifier(),\n# #     \"Support Vector Classifier\": SVC(),\n# #     \"DecisionTreeClassifier\": DecisionTreeClassifier()\n# }\n\n\n# for _, classifier in classifiers.items():\n#     classifier.fit(train_X, train_y)\n#     accuracy = cross_val_score(classifier, train_X, train_y, cv=5)\n#     print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(accuracy.mean(), 2) * 100, \"% accuracy score\")\n#     pred = cross_val_predict(classifier, train_X, train_y, cv=5) #,\n#                              #method=\"decision_function\")\n#     print(classifier.__class__.__name__, roc_auc_score(train_y, pred))\n    \n","f142dc52":"from sklearn.svm import SVC\nsvm_model = SVC()\nfrom sklearn.model_selection import GridSearchCV\nsvm_hyparam = {\"C\": np.arange(1,5), \"kernel\":[\"linear\", \"rbf\"]}\nsvm_cv_model = GridSearchCV(svm_model, svm_hyparam, cv=5).fit(train_X, train_y)","92fa366b":"svm_cv_model.best_score_","3eb58667":"best_param = svm_cv_model.best_params_\nprint(best_param)","e35cb1c0":"from sklearn.svm import SVC\n\n# predict the result \n# svm = SVC(C = best_params['C'], kernel=best_params['kernel'], probability=True).fit(train_X, train_y)\nsvm = SVC(C = 3, kernel='rbf', probability=True).fit(train_X, train_y)","498395b8":"svm_y_predict = svm.predict(test_X)\nprint('accuracy_score', accuracy_score(y_test, svm_y_predict))","d539f293":"from sklearn.metrics import accuracy_score\nprint('accuracy_score', accuracy_score(test_y, svm_y_predict))","8499fa91":"from sklearn.metrics import classification_report\n\nprint(classification_report(test_y, svm_y_predict))","73734f26":"from sklearn.linear_model import LogisticRegression\nlog_hyparam={\"C\":np.logspace(-5,5,6), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nlog=LogisticRegression()\nlog_cv=GridSearchCV(logreg,log_hyparam,cv=10)\nlog_cv.fit(train_X,train_y)\n\nprint(log_cv.best_params_)\nprint(\"accuracy \",log_cv.best_score_)\nbest_param = log_cv.best_params_\n","3ff85aa9":"log = LogisticRegression(C = best_params['C'], penalty=best_params['penalty'], probability=True).fit(train_X, train_y)","f0d4d002":"\nlog_y_predict = log.predict(test_X)\nprint('accuracy_score', accuracy_score(y_test, log_y_predict))","cf97b5f0":"from sklearn.metrics import classification_report\n\nprint(classification_report(test_y, log_y_predict))","a5dddc39":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_hyparam={\"n_neighbors\":[2,4,6,8,10], \"metric\":[\"euclidean\", \"manhattan\"]}\nKNN=KNeighborsClassifier()\nKNN_cv=GridSearchCV(KNN,KNN_hyparam,cv=10)\nKNN_cv.fit(train_X,train_y)\n\nprint(KNN_cv.best_params_)\nprint(\"accuracy \",KNN_cv.best_score_)\nbest_param = log_cv.best_params_","3c456632":"KNN = LogisticRegression(n_neighbors = best_params['n_neighbors'], metric=best_params['metric'], probability=True).fit(train_X, train_y)","4f1421d8":"KNN_y_predict = KNN.predict(test_X)\nprint('accuracy_score', accuracy_score(y_test, KNN_y_predict))","028f3fc1":"from sklearn.metrics import classification_report\n\nprint(classification_report(test_y, KNN_y_predict))","d4292c6c":"Synthetic Minority Oversampling Technique\n\nReference: https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/","9678004d":"## SVM","7a7f9910":"From above graphs, we find that the values for Time and Amount are in large scale than other 28 features, we need to rescale \"Time\" and \"Amount\" otherwise the model we built may inaccurate. \nMeanwhile, we have also found there is imbalanced class data. The number of fraud transactions are much less than the number of normal transactions. And our object is to detect those fraud credit card transactions, therefore we need to handle these imbalanced data. ","e55c8039":"# Introduction\nCredit card have been used a lot in our daily life. However, But credit card fraud has been a long-standing problem, costing both customers and banks a lot of money. In this project, we would like to analyze the credit card fraud data, find any potential pattern of fraud happened, and detect any transactions are fraud. ","d568ccf5":"**Acknowledgements**\n\nThe dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http:\/\/mlg.ulb.ac.be) of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection.\nMore details on current and past projects on related topics are available on https:\/\/www.researchgate.net\/project\/Fraud-detection-5 and the page of the DefeatFraud project\n\nPlease cite the following works:\n\nAndrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n\nDal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon\n\nDal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE\n\nDal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n\nCarcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-A\u00ebl; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier\n\nCarcillo, Fabrizio; Le Borgne, Yann-A\u00ebl; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing\n\nBertrand Lebichot, Yann-A\u00ebl Le Borgne, Liyun He, Frederic Obl\u00e9, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019\n\nFabrizio Carcillo, Yann-A\u00ebl Le Borgne, Olivier Caelen, Frederic Obl\u00e9, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019\n\nYann-A\u00ebl Le Borgne, Gianluca Bontempi Machine Learning for Credit Card Fraud Detection - Practical Handbook","4e120d09":"This dataset only contains numerical variables, and due to confidential issues, original features and more background information about the data are removed. The only 28 features V1, V2, ... V28 are the principal components obtained with PCA. Features \"Time\", \"Amount\", \"Class\" are remained the same. Feature \"Class\" is the target variable, it uses 1 to represent fraud and 0 for other cases. Feature \"Amount\" is the transaction amount, and feature \"Time\" is the time difference in seconds between current transaction time and the first transaction time. These two features could help analyze the fraud transaction amount and any seasonal pattern within fraud transactions [1].    \n\n\nThere is no missing value. \nAnd from the plot, we see most fraud transactions do not have the large transaction amount. And it happened during most time stamp. There is no obvious time seasonality trend found. \n\nFrom the corerlation matrix and map, we found there is no obvious co-linear relationship between data features. \n\n\n\n**Reference** \n1. https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\n\n\n\n","11da10d2":"Outline: \n* intro\n* data\n* data cleaning \n* build model\n* analysis result","8523ac32":"## KNN","e3b4e326":"## Data Cleaning","e77c9cb9":"## import packages","5be8316b":"## Logistc Regression","7c78baa7":"Reference: \n1. https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets\n\n2. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html","03cd2a94":"### Understand the data","a095ea5b":"# Data","e37e121a":"# Build model\n\n1. logistic regression\n2. KNN\n3. SVM","f7f62272":"# ECE 572 Project "}}