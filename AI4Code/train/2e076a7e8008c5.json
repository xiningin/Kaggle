{"cell_type":{"ecd307c4":"code","d48d8317":"code","8ef316fe":"code","4081f78a":"code","eaa3085e":"code","04cd08cc":"code","3a76da71":"code","4b62f87c":"code","71176c96":"code","71f61a2b":"code","ec19fab8":"code","23d847c3":"code","788e8eef":"code","c1385a59":"code","5aefbe71":"code","c7b76794":"code","92242cad":"code","a781db72":"code","f0e5ce67":"code","aa784f90":"code","229061ed":"code","514ffbcb":"code","36ba6bbe":"code","0e661097":"code","4f62fd16":"code","3ca78395":"code","7a3c596d":"code","d655b15c":"code","98044978":"code","6c01a223":"code","46af7c3a":"code","0609c5b9":"code","fbdec6bc":"code","81dfd65f":"code","1f0093ef":"code","aacd2c57":"code","6eea5918":"code","1bffc7c3":"code","40cc8f9c":"code","0de98217":"code","4a6e8170":"code","78058e31":"code","8dc9b370":"code","d2bbc01f":"code","113626bd":"code","de4c0b4a":"code","0c78e902":"code","57498bba":"code","72c8f6ab":"code","0f5bdace":"code","943fa7e5":"code","a4ba5c83":"code","384a3d1e":"code","f9bcd9c7":"code","f6728a38":"code","593cc9e3":"code","ed866e8f":"markdown","bfde447c":"markdown","2a5a8010":"markdown","67b63699":"markdown","c7f77bd5":"markdown","c7d9b383":"markdown","54f143ca":"markdown","5e26ae4c":"markdown","709df7e0":"markdown","29d15a17":"markdown","190f648b":"markdown","62c262c2":"markdown","3a18ae01":"markdown","6cf1c4b2":"markdown","129fca60":"markdown","1aa6eeb5":"markdown","cf3ed62b":"markdown","f6322610":"markdown","2e523161":"markdown","848ef86c":"markdown","3825e120":"markdown","6170d92c":"markdown","852541c7":"markdown","f1284146":"markdown","5d4caa24":"markdown","0c1e2c4b":"markdown","3ff87431":"markdown","bcd3d2b8":"markdown","96789ee8":"markdown","976cf767":"markdown","ff4b9a6a":"markdown","31fb2942":"markdown","da15c5ca":"markdown"},"source":{"ecd307c4":"# import libraries\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport zipfile\n\n# visualization libararies\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# waffle charts\nfrom pywaffle import Waffle\n\n# map visualization\nimport folium\nfrom folium import plugins\n\n# import files\nimport os","d48d8317":"# read input datasets\n\n#read .csv files\ndistricts = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv') # information about school districts\nproducts = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv')   # information about the top 372 products with most users in 2020\n\n#create list of all files in 'engagement_data' folder\nfiles = next(os.walk('..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data'))[2]\n\nengagement_list = []\n\nfor file in files:\n    district_id = file.split(\".\")[0]\n    file = '..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/'+ str(file)\n    df = pd.read_csv(file)\n    df[\"district_id\"] = district_id\n    engagement_list.append(df)\n    \nengagement = pd.concat(engagement_list)  # information about page load events\nengagement = engagement.reset_index(drop=True)\n","8ef316fe":"# missing values\n\ndef print_missing_values(df):\n    # function calculates and displays amount of missing values in the dataset \n    # df - dataframe to analyse\n    \n    print('Percentage of missing values per column before pre-processing:\\n')\n    for column in df.columns:\n        print('{} - {}%'.format(column,\n                                round((df[column].isna().sum() \/ len(df)) * 100, 3)))\n        \nprint_missing_values(engagement)   ","4081f78a":"# engagement_index = NaN when pct_access = 0, so, I replace NaN values with 0.\nengagement.loc[engagement.pct_access == 0, 'engagement_index'] = engagement.loc[\n                                                                engagement.pct_access == 0, 'engagement_index'].fillna(0)\n\n# drop lines with missing values in 'pct_access', 'lp_id' columns\n# After this step there are no more missing values in the dataframe.\nengagement.dropna(subset=['pct_access', 'lp_id'], inplace = True)\n\n\n#change column types to int and date format\nengagement['lp_id'] = engagement['lp_id'].astype(int)\nengagement['district_id'] = engagement['district_id'].astype(int)\nengagement['time'] = pd.to_datetime(engagement['time'], format = '%Y-%m-%d')","eaa3085e":"# missing values\nprint_missing_values(districts)   ","04cd08cc":"# only districts with information about state can be valuable for analysis,\n# rows with missing 'state' are empty in other columns too.\ndistricts = districts.loc[districts.state.isna() == False]\n\n# drop non informative column: 162 non-null values out of which 161 - '0.18, 1', and 1 - '1, 2'\ndistricts.drop(columns = 'county_connections_ratio', inplace = True)\n\n# replace comma ',' with hyphen '-' for easier interpretation \n# drop punctuation that makes information less interpretable\n\nfor column in districts.columns[-3:]:\n    districts[column] = districts[column].str.replace(', ', ' - ')\n    districts[column] = districts[column].str.replace('[', '')","3a76da71":"# fill in per-pupil total expenditure NaN values using Edunomics Lab website: https:\/\/edunomicslab.org\/nerds\/\n\n#identify states with missing information about per-pupil total expenditure\nstates_missing_exp = list(districts.loc[districts['pp_total_raw'].isna() == True]['state'].unique())\n\n# delete New Hampshire from the list because there is no available information about this state.\nstates_missing_exp.remove('New Hampshire')\n\n# states abbreviations\nstates_abb = zip(states_missing_exp, ['CT', 'OH','CA','AZ','ND','NY'])\n\n# as I don't have access to information about district locale status (city, suburb etc),\n# I will use median total expenditure per state and the range of 2000$ around this median value.\n\nfor state, abb in states_abb:\n    state_name = state\n    file = 'https:\/\/raw.githubusercontent.com\/ElinaAizenberg\/Kaggle-competition\/main\/'+ abb + '_1819_final_August_1st_21.csv'\n    state = pd.read_csv(file, sep = ';', \n                        error_bad_lines=False,\n                        encoding= 'unicode_escape')\n    \n    # files with financial information per state are standardized and column names are the same + state abbreviation\n    column_name = 'pp_total_raw_' + abb                  # the same column name as in the given dataset\n    state = state[[column_name]].dropna()\n    state = state[[column_name]].replace(',','.', regex=True).astype(float)\n    \n    # there are negative values in the columns but in documentation it's noticed that these values are mistakes.\n    state = state.loc[state[column_name] > 0]            # choose the rows with positive values\n    median = round(state[[column_name]].median(), -3)    # round to the nearest thousand\n        \n    median_left = int(median[0] - 1000)    # start of the range\n    median_right = int(median[0] + 1000)   # end of the range\n    \n    median_range = str(median_left) + ' - ' + str(median_right)  # range to fill in cells with missing information\n    \n    districts.loc[districts.state == state_name, 'pp_total_raw'] = districts.loc[\n                                                                   districts.state == state_name,\n                                                                   'pp_total_raw'].fillna(median_range)","4b62f87c":"# missing values\nprint_missing_values(products)   ","71176c96":"def fill_missing_sector_function(df, company):\n    # function identifies the most frequent sector\/function of products of a given company\n    # and fills in missing values of 'Sector(s)', 'Primary Essential Function' columns filtered for this company\n    # if there is only one product of a given company, then Sector = 'Higher Ed; Corporate',\n    # Primary Essential Function = 'LC\/CM\/SDO - Other'.\n    \n    # df - dataframe with inormation about poducts\n    # company - given company with missing values in 'Sector(s)' column\n    \n    df_company = df.loc[df['Provider\/Company Name'] == company]\n\n    if len(df_company) > 1:\n        sector_value = df.loc[df['Provider\/Company Name'] == company, 'Sector(s)'].mode()[0]\n        function_value = df.loc[df['Provider\/Company Name'] == company, 'Primary Essential Function'].mode()[0]\n    \n    else:\n        sector_value = 'Higher Ed; Corporate'\n        function_value = 'LC\/CM\/SDO - Other'\n        \n    df.loc[df['Provider\/Company Name'] == company, 'Sector(s)'] = df.loc[\n                                                                df['Provider\/Company Name'] == company, 'Sector(s)'\n                                                                ].fillna(sector_value)\n\n    df.loc[df['Provider\/Company Name'] == company, 'Primary Essential Function'] = df.loc[\n                                                            df['Provider\/Company Name'] == company, 'Primary Essential Function'\n                                                            ].fillna(function_value)\n    \n    \n# create a list of companies with missing info about their products\ncompany_list = list(\n                products.loc[products['Sector(s)'].isna() == True, 'Provider\/Company Name']\n                .dropna().unique())\n\n# apply function to fill in missing values\nfor company in company_list:\n    fill_missing_sector_function(products, company)","71f61a2b":"# instead of sectors I find that users' category is more interpretable and create a new feature based on Sector(s).\nsectors = {'PreK-12':'Children under 18 y.o.',\n           'PreK-12; Higher Ed': 'Students, all ages',\n           'PreK-12; Higher Ed; Corporate': 'Universal products',\n           'Higher Ed; Corporate':'Uni.students and corporate users',\n            'Corporate': 'Corporate users'}\n\n# create new feature\nproducts['Users'] = products['Sector(s)'].copy()\nproducts.replace({\"Users\": sectors}, inplace = True)\n\n# split Primary Essential Function column on 2 columns: Function(abb) + Function(details)\nproducts['Function(abb)'] = products['Primary Essential Function'].str.split(' - ').str[0]\nproducts['Function(details)'] = products['Primary Essential Function'].str.split(' - ').str[1]\n\n# drop Primary Essential Function, because it duplicates information\nproducts.drop(columns = 'Primary Essential Function', inplace = True)\n\n# in Function(details) column there are 2 same categories: 'Sites, Resources & Reference' and\n# 'Sites, Resources & References'. They should be merged into 1 category.\nproducts.replace({\"Function(details)\":\n                 {'Sites, Resources & Reference':'Sites, Resources & References'}},\n                 inplace = True)\n","ec19fab8":"# parameters for visualizations\n# fonts\nfont_title = 18\nfont_text = 14\nfont_labels = 12\nfont_ticks = 12\nfont_legend = 10\nfontname = 'Times New Roman'\n\n#colors\ncolors_group_1 =  ['darkcyan', 'limegreen', \"gold\"]\ncolors_group_2 =  [\"#782B9D\", \"#EA4F88\", \"#F98477\"]\n\ncmap_1 = 'viridis'\ncmap_2 = 'plasma'\nwaffle_cmap = 'Set1'\n\n#labels\nfunction_abb_labels = {'LC':'Learning & Curriculum',\n                       'CM':'Classroom Management',\n                       'SDO':'School & District Operations',\n                       'LC\/CM\/SDO':'Universal products'}","23d847c3":"# waffle chart to identify the distribution of products' sectors or users categories\ndf_users = products.Users.value_counts()\n\nfig_0 = plt.figure(\n    FigureClass = Waffle, \n    rows = 10,\n    values = list(df_users.values),\n    interval_ratio_x = 0.5,\n    interval_ratio_y = 0.6,\n    figsize = (12, 7),\n    icons = 'file-alt',\n    title = {\n        'label': \"Products' users\",\n        'loc': 'left',\n        'fontdict': {\n            'fontsize': font_title,\n            'fontname': fontname\n        }},\n    labels = [f\"{k} ({int(v \/ sum(df_users.values) * 100)}%)\" for k, v in df_users.items()],\n    legend = {\n        \n        'loc': 'lower left',\n        'bbox_to_anchor': (0, -0.35),\n        'ncol': 3,\n        'framealpha': 0,\n        'fontsize': font_labels\n    },\n    vertical = True,\n    cmap_name = waffle_cmap\n)","788e8eef":"# bar chart that displays how intersect products accrding to their sectors\n\n# count how many products per each category, including mixed sectors\ndf_sectors = products['Sector(s)'].value_counts()\n\n# create dictionary to calculate how time each sector is mentioned in the 'Sector(s)' column\nsectors = {'PreK-12':0, 'Higher Ed':0, 'Corporate':0}\n\nfor key, value in sectors.items():\n    for index, count in df_sectors.items():\n        if key in index:\n            sectors[key] += count\n\n# get list of values from the dictionary            \nstacked_sectors = list(sectors.values())\n\n#labels for the chart\nsectors_names = ['PreK-12 (school students)', 'Higher Education', 'Corporate users' ]\n\n# create a chart\nfig_2 = plt.figure(figsize = (15,1.8))\nfig_2_ax = fig_2.add_subplot()\n\n# white spaces are necessary to visualze intersection of sectors\nwhite_spaces = [0, 176, 241]\nfig_2_ax.barh(sectors_names, white_spaces, color = \"white\", height = 1)\n\n# colored bars\nfor i in range(len(stacked_sectors)):\n    fig_2_ax.barh(sectors_names[i], stacked_sectors[i],\n                  left=white_spaces[i], color=colors_group_1[i],\n                  height=1)\n\n# add vertical lines and labels for better understanding how sectors intersect\n# 'PreK-12 (school students)'\nfig_2_ax.vlines(df_sectors['PreK-12'], -1, 3, colors = 'black', linestyles = '--')\nfig_2_ax.text(75, 0.8, str(df_sectors['PreK-12']), fontsize=font_labels, color = 'black')\n\n# 'PreK-12; Higher Ed'\nfig_2_ax.vlines(241, -0.8, 3, colors='black', linestyles='--')\nfig_2_ax.text(200, 1.6, str(df_sectors['PreK-12; Higher Ed']), fontsize=font_labels, color = 'black')\n\n# 'PreK-12; Higher Ed; Corporate'\nfig_2_ax.vlines(363, -1, 3, colors='black', linestyles='--')\nfig_2_ax.text(292, 2.6, str(df_sectors['PreK-12; Higher Ed; Corporate']), fontsize=font_labels, color = 'black')\n\n# 'Higher Ed; Corporate'\nfig_2_ax.vlines(370, -1, 3, colors='black', linestyles='--')\nfig_2_ax.text(364, 2.6, str(df_sectors['Higher Ed; Corporate']), fontsize=font_labels, color = 'black')\n\n# 'Corporate'\nfig_2_ax.text(372, 2.6, str(df_sectors['Corporate']), fontsize = font_labels, color = 'black')\n    \n        \nfig_2_ax.set_xlim([0, 400])     # set x-scale\nfig_2_ax.yaxis.tick_right()     # put y-ticks on the right side\nfig_2_ax.set_yticklabels(sectors_names, fontsize = font_labels)\n\nfig_2_ax.set(frame_on = False)  # get rid of the frame\nfig_2_ax.set_xticks([])         # get rid of x-ticks\n\nplt.show()","c1385a59":"# waffle chart to identify the distribution of products' functions\ndf_functions = products['Function(abb)'].value_counts()\ndf_functions.rename(function_abb_labels, inplace = True)\n\nfig_1 = plt.figure(\n    FigureClass = Waffle, \n    rows = 6,\n    columns = 21,\n    rounding_rule = 'floor',\n    values = list(df_functions.values),\n    interval_ratio_x = 0.5,\n    interval_ratio_y = 0.8,\n    figsize = (13, 7),\n    icons = 'file-alt',\n    title = {\n        'label': \"Products' functions\",\n        'loc': 'left',\n        'fontdict': {\n            'fontsize': font_title,\n            'fontname': fontname\n        }},\n    labels = [f\"{k} ({int(v \/ sum(df_functions.values) * 100)}%)\" for k, v in df_functions.items()],\n    legend = {\n        \n        'loc': 'upper right',\n        'bbox_to_anchor': (1.45, 1.05),\n        'ncol': 1,\n        'framealpha': 0,\n        'fontsize': font_labels\n    },\n    vertical = False,\n    cmap_name= waffle_cmap\n)","5aefbe71":"# merge engagement and products dataframes\nproduct_engagement = pd.merge(engagement[['time', 'lp_id', 'engagement_index']],\n                              products[['LP ID', 'Product Name', 'Provider\/Company Name',\n                                        'Users', 'Function(abb)', 'Function(details)']],\n                              how = 'left', left_on = 'lp_id', right_on = 'LP ID')\n# drop mutual column 'LP ID'\nproduct_engagement.drop(columns = ['LP ID'], inplace = True)\n\n# calculate total engagement index for ALL PRODUCTS per Users' category per day\n# (w\/o split per districts and products\/ names)\nusers_df = product_engagement.groupby(\n                               by = [product_engagement.time, product_engagement.Users]\n                               )['engagement_index'].sum().reset_index()\n\n# calculate average engagement index per week in 2020, per Users' category\nusers_df = users_df.groupby(by = [users_df.time.dt.week, users_df.Users]\n                            )['engagement_index'].mean(\n                            ).unstack(level = 1).reset_index()","c7b76794":"def create_season_chart(title, y_lim, y_text):\n    # function create a sample of chart with colored seasons and weeks of 2020 on x-axis\n    \n    # title - title of the chart\n    # y_lim - maximum of y_axis, should be slightly more than maximum value of the Users' column\n    # y_text - y-coordinate of seasons' names, should be less than y_lim\n    \n    # create chart\n    plt.style.use('ggplot')\n    fig = plt.figure(figsize = (17.5,5.5))\n    ax = fig.add_subplot()\n    \n    #tisks\n    x_ticks = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 53]\n    plt.xticks(x_ticks)\n    ax.set_xlim(1, 53)\n    ax.set_ylim(1, y_lim)\n    \n    #background color - seasons\n    plt.axvspan(1, 9, facecolor='blue', alpha=0.2)\n    plt.axvspan(9, 22, facecolor='limegreen', alpha=0.2)\n    plt.axvspan(22, 35, facecolor='gold', alpha=0.2)\n    plt.axvspan(35, 48, facecolor='coral', alpha=0.2)\n    plt.axvspan(48, 53, facecolor='blue', alpha=0.2)\n    \n    #text seasons\n    ax.text(3.5, y_text, 'Winter', fontsize=14, color = 'blue', alpha = 0.4, fontweight = 'semibold')\n    ax.text(14, y_text, 'Spring', fontsize=14, color = 'limegreen', alpha = 0.4, fontweight = 'semibold')\n    ax.text(27, y_text, 'Summer', fontsize=14, color = 'orange', alpha = 0.4, fontweight = 'semibold')\n    ax.text(40, y_text, 'Autumn', fontsize=14, color = 'coral', alpha = 0.4, fontweight = 'semibold')\n    ax.text(49, y_text, 'Winter', fontsize=14, color = 'blue', alpha = 0.4, fontweight = 'semibold')\n    \n    #grid\n    ax.grid(axis='x')\n    \n    #labels\n    ax.set_ylabel('Av.am. of page-load events per 1000 students,.000', fontsize = font_labels)\n    ax.set_xlabel('Weeks, 2020 year', fontsize = font_labels)\n\n    #title\n    ax.set_title(title, fontsize = font_title, fontname = fontname)\n       \n    return fig, ax","92242cad":"def top_products(df, user_categoty, n_products, axis, y_lim, x_text_coordinate, color_line = 'black'):\n    # function calculates average engagement index for each product in a given Users' group\n    # for the whole year, sorts products by avg.engagement index and makes a list of n-top used products.\n    # Afterwards, function displays the list on a given plot.\n    \n    # df - merged dataframe to calculate avg. engagement index\n    # user_categoty - given Users' category\n    # n_products - how many top used products to display\n    # axis - axis of the plot, where to display\n    # y_lim - feature used in create_season_chart function, use the same value\n    # x_text_coordinate - x-coordinate on the list of products on the plot\n    # color_line - the same color as Users' category plot or darker color, defualt = 'black'\n    \n    top_product = df.loc[df.Users == user_categoty]. \\\n                  groupby(by= 'Product Name')['engagement_index']. \\\n                  mean().reset_index()\n            \n    top_product_list = list(top_product.sort_values(\n                       by = 'engagement_index', ascending = False).head(n_products)['Product Name'])\n    \n    title_coordinate = y_lim * (3\/4)\n    axis.text(x_text_coordinate, title_coordinate, 'Top used products in 2020:',\n              fontsize = font_text,\n              color = color_line,\n              alpha = 0.8,\n              fontname = fontname)\n        \n    for i in range(0, n_products):\n        product_coordinate = title_coordinate - (i+1) * y_lim * (1\/15)\n        \n        axis.text(x_text_coordinate, product_coordinate, str(str(i+1) + ' - ' + top_product_list[i]),\n                  fontsize = font_text,\n                  color = color_line,\n                  alpha = 0.9,\n                  fontname = fontname)","a781db72":"fig_3, fig_3_ax = create_season_chart('Universal products for all users, all states in 2020',\n                                      y_lim = 12000, y_text = 11000)\n\nfig_3_ax.plot(users_df.time, users_df['Universal products'] \/1000,\n              linewidth = 1.5, color = 'blue',\n              label = 'Universal products')\n\ntop_products(product_engagement, 'Universal products',\n             n_products = 6, axis = fig_3_ax,\n             y_lim = 12000, x_text_coordinate = 24)\n\nfig_3_ax.legend(fontsize = font_legend,\n                loc = 'lower left')\n\nplt.show()","f0e5ce67":"fig_4, fig_4_ax = create_season_chart('Products that can be used by children under 18 y.o. and university students, all states in 2020',\n                                      y_lim = 3000, y_text = 2750)\n\nfig_4_ax.plot(users_df.time, users_df['Children under 18 y.o.'] \/1000,\n              linewidth = 1.5, color = 'red',\n              label = 'Children under 18 y.o.')\n\nfig_4_ax.plot(users_df.time, users_df['Students, all ages'] \/1000,\n              linewidth = 1.5, color = 'green',\n              label = 'Students, all ages')\n\ntop_products(product_engagement, 'Children under 18 y.o.',\n             n_products = 5,\n             axis = fig_4_ax, y_lim = 3000,\n             x_text_coordinate = 17,\n             color_line = 'darkred')\n\ntop_products(product_engagement, 'Students, all ages',\n             n_products = 5,\n             axis = fig_4_ax, y_lim = 3000,\n             x_text_coordinate = 30,\n             color_line = 'darkgreen')\n\nfig_4_ax.legend(fontsize = font_legend,\n                loc = 'lower left')\n\nplt.show()","aa784f90":"fig_5, fig_5_ax = create_season_chart('Products that can be used by corporate users and university students, all states in 2020',\n                                      y_lim = 25, y_text = 22.5)\n\nfig_5_ax.plot(users_df.time, users_df['Uni.students and corporate users'] \/1000,\n              linewidth = 1.5, color = 'darkviolet',\n              label = 'Uni.students and corporate users')\n\nfig_5_ax.plot(users_df.time, users_df['Corporate users'] \/1000,\n              linewidth = 1.5, color = 'orange',\n              label = 'Corporate users')\n\ntop_products(product_engagement, 'Uni.students and corporate users',\n             n_products = 5,\n             axis = fig_5_ax, y_lim = 25,\n             x_text_coordinate = 17,\n             color_line = 'rebeccapurple')\n\ntop_products(product_engagement, 'Corporate users',\n             n_products = 1,\n             axis = fig_5_ax, y_lim = 25,\n             x_text_coordinate = 30,\n             color_line = 'darkgoldenrod')\n\nfig_5_ax.legend(fontsize = font_legend,\n                loc = 'lower left')\n\nplt.show()","229061ed":"def function_extreme_std(df):\n    # function calculates average engagement index per Function(deatils) category per month,\n    # identifies top-3 categories which usage mostly fluctuated during 2020.\n    # function returns dictionary with name of function and list of average engagement indexes per month. \n    \n    function_details = list(df['Function(details)'].unique())  #list of all Functions(details)\n    \n    functions_to_vis = {}\n    y_df = df.groupby(by = ['time', 'Function(details)'])['engagement_index'].sum().reset_index()\n    \n    for category in function_details:\n        y = y_df.loc[y_df['Function(details)'] == category].groupby(by = y_df.time.dt.month)['engagement_index'].mean()\n        y_std = np.std(y\/y.mean())  #scale means per each month by dividing by the total mean\n        \n        if y_std > 0.9:             # y_std = 0.9 is manually chosen value\n            y = list(y)\n            functions_to_vis[category] = y\n        \n    return functions_to_vis\n\n\ndef function_extreme_average(df):\n    # function calculates average engagement index per Function(deatils) category per the whole 2020,\n    # identifies top-3 categories with the highest average engagement index.\n    # function returns dictionary with name of function and list of average engagement indexes per month.\n    \n    y = df.groupby(by = ['time', 'Function(details)'])['engagement_index'].sum().unstack(level = 1).reset_index()\n    y_mean = y.mean()\n    \n    function_details = list(y_mean.sort_values(ascending = False)[:3].index)\n    functions_to_vis = {}\n    \n    for category in function_details:\n        y_df = df.groupby(by = ['time', 'Function(details)'])['engagement_index'].sum().reset_index()\n        y_category = y_df.loc[y_df['Function(details)'] == category]. \\\n                                          groupby(by = y_df.time.dt.month)['engagement_index'].mean()\n        y_category = list(y_category \/ 1000)\n        functions_to_vis[category] = y_category\n\n    return functions_to_vis\n\n\ndef function_extreme_chart(dictionary, figure, colors, *axis):\n    # function creates set of charts from dictionary\n    # on given a figure, axises using a list of colors.\n    \n    months = ['January','February','March', 'April','May','June','July', 'August', 'September', 'October', 'November','December']\n    \n    for ax, item, color in zip(axis, dictionary.items(), colors):\n        \n        sns.barplot(months, item[1], ax = ax, color = color)\n        ax.set_xticklabels(months, rotation = 45, fontsize = 10)\n        ax.set_xlabel('')\n        ax.set_title(item[0], fontsize = font_title, fontname = fontname)\n        ax.locator_params(axis='y', nbins=6) \n        \n     ","514ffbcb":"# create dictionaries with the most fluctuating and the most used categories in Function(details)\nfunctions_std = function_extreme_std(product_engagement)\nfunctions_average = function_extreme_average(product_engagement)","36ba6bbe":"fig_6, fig_6_ax = plt.subplots(nrows=3, ncols=2, sharex=True, sharey=False, figsize=(18, 15))\nplt.style.use('ggplot')\n\nf6_ax1, f6_ax2, f6_ax3 =  fig_6_ax[:,0]\nf6_ax4, f6_ax5, f6_ax6 =  fig_6_ax[:,1]\n\n# charts with top-3 fluctuating categories\nfunction_extreme_chart(functions_std, fig_6, colors_group_1, f6_ax1, f6_ax2, f6_ax3)\nfig_6.text(0.2, 0.93, 'The most fluctuating functions in 2020',\n           fontsize = font_title, fontname = fontname)\n\n# y-label for top-3 fluctuating categories\nfig_6.text(0.08, 0.4, 'Average amount of page-load events per 1000 students',\n           fontsize = font_labels, alpha = 0.8,\n           rotation = 'vertical')\n\n# charts with top-3 used categories\nfunction_extreme_chart(functions_average, fig_6, colors_group_2, f6_ax4, f6_ax5, f6_ax6)\nfig_6.text(0.63, 0.93, 'The most used functions in 2020',\n           fontsize = font_title, fontname = fontname)\n\n# y-label for top-3 used categories\nfig_6.text(0.5, 0.4, 'Average amount of page-load events per 1000 students, .000',\n           fontsize = font_labels, alpha = 0.8,\n           rotation = 'vertical')\n\n\n\nplt.show()","0e661097":"# merge engagement and district dataframes\nlocale_engagement = pd.merge(engagement[['engagement_index','district_id','lp_id', 'time']],\n                         districts[['district_id', 'locale']],\n                         how = 'left', on = 'district_id')\n\n# merge new dataframe and products dataframe\nlocale_engagement = locale_engagement.merge(products[['LP ID', 'Function(abb)', 'Users']],\n                                   how = 'left', left_on = 'lp_id', right_on = 'LP ID')\n\n# it is crucial to drop rows with missing information about products and districts for further manual calculations\nlocale_engagement.dropna(inplace = True)\n","4f62fd16":"# calculate total engagement index for ALL PRODUCTS per day and per locale category\n# (w\/o split per products\/ names)\nlocale_engage_function = locale_engagement.groupby(by = ['time','locale']) \\\n                                                    ['engagement_index'].sum().reset_index()\n\n# calculate average engagement index for the whole 2020 per \nlocale_group = locale_engage_function.groupby(by = ['locale']) \\\n                                                    ['engagement_index'].mean().reset_index()","3ca78395":"# list of all unique categories in Function(abb) feature\nfunctions_abb = products['Function(abb)'].unique()[:-1]\n\n# caluclate manually partion of each Function(abb) engagement index per locale category\nfor function in functions_abb:\n    locale_group[function] = locale_group.apply(lambda row:\n                                                locale_engagement.loc[(locale_engagement['Function(abb)'] == function)&\n                                                (locale_engagement.locale == row.locale),\n                                                'engagement_index'].sum() \/\n                                                locale_engagement.loc[locale_engagement.locale == row.locale,\n                                                'time'].nunique(), axis = 1)\n\n# create a column for custom sorting: City, Town, Suburb, Rural    \nlocale_group['locale_index'] = locale_group['locale'].map({'City': 0, 'Town':1, 'Suburb':2, 'Rural':3})\nlocale_group = locale_group.sort_values(by = 'locale_index', ascending = True)\n\n# caluculate percentages of Function(abb)'s engagement indexes in total index per locale category \npercentages_locale = []                      # empty list for lists of percentages\nfor index, row in locale_group.iterrows():\n    perc_locale = []\n    columns = locale_group.columns[2:6]\n    \n    for col in columns:\n        p = round((row[col] \/ row['engagement_index'])*100)\n        perc_locale.append(p)\n    \n    percentages_locale.append(perc_locale)","7a3c596d":"def engagement_group(figsize, results, category_names, labels, cmap, percentages):\n    # function creates stacked horizontal bar chart displaying percentages per each stack\n    # and returns figure, axis\n    \n    # figsize - size of the figure\n    # results - dataframe with numberical data to visualize\n    # category_names - labels for stacked bars: locale names\n    # labels - catgories that form each bar\n    # cmap - color map\n    # percentages - values to display on each stack (calculate separately before applying the function)\n\n    data = np.array(results)\n    data_cum = data.cumsum(axis=1)\n    category_colors = plt.get_cmap(cmap)(np.linspace(0.15, 0.85, data.shape[1]))\n    \n\n    fig, ax = plt.subplots(figsize=figsize)  \n    \n    \n    for i, (colname, color, label) in enumerate(zip(category_names, category_colors, labels)):\n        widths = data[:, i]\n        starts = data_cum[:, i] - widths\n        xcenters = starts + widths \/ 2\n        rects = ax.barh(category_names, widths, left=starts, height=0.7, label=label, color=color)\n\n      \n        r, g, b, _ = color\n        text_color = 'white' if r * g * b < 0.1 else 'black'\n        for y, (x, c) in enumerate(zip(xcenters, percentages)):\n            if c[i] > 4:\n                ax.text(x, y, str(c[i])+'%', ha='center', va='center', color=text_color, fontsize = 12)\n    \n    ax.legend(labels, fontsize = 12, loc='lower right')\n    plt.xticks(fontsize = font_ticks)\n    plt.yticks(fontsize = font_ticks)\n\n    \n    return fig, ax\n","d655b15c":"# numerical data to visualize\nresults = locale_group.iloc[:, [2,3,4,5]] \/1000\n\n# names of locale categories\nlocale_labels = list(locale_group.locale.unique())\n\n# labels of categories in Function(abb)\nfunction_labels = ['Learning & Curriculum', 'Classroom Management',\n                    'School & District Operations', 'Universal products']\n\nfig_7, fig_7_ax = engagement_group(figsize = (18, 5.5), results = results,\n                                   category_names = locale_labels,\n                                   labels = function_labels,\n                                   cmap = cmap_1,\n                                   percentages = percentages_locale)\n\nfig_7_ax.set_xlabel('Average amount of page-load events per 1000 students per 1 day in 2020,.000', fontsize = font_labels)\n\nfig_7_ax.set_title(\"Engagement in digital learning products in various locations - Products' functions\",\n                   fontsize = font_title, fontname = fontname)\nfig_7_ax.ticklabel_format(axis='x', style='plain')\n\nplt.show()","98044978":"# calculate total engagement index for ALL PRODUCTS per day and per locale category\n# (w\/o split per products\/ names)\nlocale_engage_user = locale_engagement.groupby(by = ['time','locale']) \\\n                                                    ['engagement_index'].sum().reset_index()\n\n# calculate average engagement index for the whole 2020 per \nlocale_group_user = locale_engage_user.groupby(by = ['locale']) \\\n                                                    ['engagement_index'].mean().reset_index()","6c01a223":"# list of all unique categories in Users feature\nusers = products['Users'].unique()[:-1]\n\n# caluclate manually partion of each User engagement index per locale category\nfor user in users:\n    locale_group_user[user] = locale_group.apply(lambda row:\n                                                locale_engagement.loc[(locale_engagement['Users'] == user)&\n                                                (locale_engagement.locale == row.locale),\n                                                'engagement_index'].sum() \/\n                                                locale_engagement.loc[locale_engagement.locale == row.locale,\n                                                'time'].nunique(), axis = 1)\n\n# create a column for custom sorting: City, Town, Suburb, Rural    \nlocale_group_user['locale_index'] = locale_group_user['locale'].map({'City': 0, 'Town':1, 'Suburb':2, 'Rural':3})\nlocale_group_user = locale_group_user.sort_values(by = 'locale_index', ascending = True)\n\n# caluculate percentages of Users' engagement indexes in total index per locale category \npercentages_locale_user = []                      # empty list for lists of percentages\nfor index, row in locale_group_user.iterrows():\n    perc_locale = []\n    columns = locale_group_user.columns[2:7]\n    \n    for col in columns:\n        p = round((row[col] \/ row['engagement_index'])*100)\n        perc_locale.append(p)\n    \n    percentages_locale_user.append(perc_locale)","46af7c3a":"# numerical data to visualize\nresults_2 = locale_group_user.iloc[:, [2,3,4,5, 6]]\/1000\n\nfig_8, fig_8_ax = engagement_group(figsize = (18, 5.5), results = results_2,\n                                   category_names = locale_labels,\n                                   labels = users,\n                                   cmap = cmap_2,\n                                   percentages = percentages_locale_user)\n\nfig_8_ax.set_xlabel('Average amount of page-load events per 1000 students per 1 day in 2020,.000', fontsize = font_labels)\n\nfig_8_ax.set_title(\"Engagement in digital learning products in various locations - Products' users\",\n                   fontsize = font_title, fontname = fontname)\nfig_8_ax.ticklabel_format(axis='x', style='plain')\n\nplt.show()","0609c5b9":"# calculate average engagement index per week in 2020 per locale category\nlocale_group_time = locale_engage_function.groupby(by = [locale_engage_function.time.dt.week, locale_engage_function.locale]) \\\n                                         ['engagement_index'].mean().reset_index()\n\n# list of color\ncategory_colors = plt.get_cmap(cmap_1)(np.linspace(0.15, 0.85, 4))\n\n","fbdec6bc":"fig_9 = plt.figure(figsize = (16,5.5))\nfig_9_ax = fig_9.add_subplot()\nplt.style.use('ggplot')\n\n\nfor local, color in zip(locale_labels, category_colors):\n    local_df = locale_group_time.loc[locale_group_time.locale == local]\n    fig_9_ax.plot(local_df.time, local_df.engagement_index, label = local, color = color)\n \nfig_9_ax.legend(fontsize = 12, loc='upper left')\n\n#tisks\n\n\n#labels & ticks\nx_ticks = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 53]\nplt.xticks(x_ticks)\nfig_9_ax.set_xlim(1, 53)\nfig_9_ax.set_ylim(0, 7000000)\nfig_9_ax.set_yticklabels([0, 1000, 2000, 3000, 4000, 5000, 6000, 7000])\n\nfig_9_ax.set_xlabel('Weeks, 2020 year', fontsize = font_labels)\nfig_9_ax.set_ylabel('Average amount of page-load events per 1000 students, .000', fontsize = 10)\n\n#title\nfig_9_ax.set_title('Engagement in digital learning products in various locations',\n                   fontsize = font_title, fontname = fontname)\n\n#grid\nfig_9_ax.grid(axis='x')\n\nplt.show()","81dfd65f":"# merge engagement and district dataframes\nstate_engagement = pd.merge(engagement[['engagement_index','district_id', 'time']],\n                         districts[['district_id', 'locale', 'state']],\n                         how = 'left', on = 'district_id')\n\nstate_engagement.dropna(inplace = True)\n\n# repeat the same states that were done to analyse engagement per locale category, but with analytics per state\n# sum engagement index for all products per each date, per state\nstate_group = state_engagement.groupby(by = ['time','state'])['engagement_index'].sum().reset_index()\n\n# calculate average engagement index per state\nstate_group = state_group.groupby(by = 'state')['engagement_index'].mean().reset_index()\n","1f0093ef":"# sort states from smallest to biggest average engagement index in 2020\nstate_group = state_group.sort_values(by = 'engagement_index', ascending = True)\n\nfor locale in locale_labels:\n    state_group[locale] = state_group.apply(lambda row:\n                                                state_engagement.loc[(state_engagement.locale == locale)&\n                                                (state_engagement.state == row.state)]['engagement_index'].sum() \/\n                                                state_engagement.loc[state_engagement.state == row.state,\n                                                'time'].nunique(), axis = 1)\n    \n","aacd2c57":"# caluculate percentages of locale engagement indexes in total index per each state  \npercentages_states_locale = []\nfor index, row in state_group.iterrows():\n    perc_locale = []\n    columns = state_group.columns[2:6]\n    \n    for col in columns:\n        p = round((row[col] \/ row['engagement_index'])*100)\n        perc_locale.append(p)\n    \n    percentages_states_locale.append(perc_locale)","6eea5918":"# Because of the significant differences between largest and smallest engagement indexes,\n# I split the list of state in 2 parts:\n\n# part I - top-8 states with highest average engagement index\nstates_1 = list(state_group.state.unique()[14:])\nresults_states_1 = state_group[state_group.columns[2:6]][14:]\npercentages_states_1 = percentages_states_locale[14:]\n\nfig_10, f10_ax1 = engagement_group((15, 7), results_states_1,\n                                   states_1, locale_labels,\n                                    cmap_1, percentages_states_1)\n#title\nf10_ax1.set_title('Top-8 states with highest engagement in digital learning products',\n                   fontsize = font_title, fontname = fontname)\n\n#ticks\nf10_ax1.set_xticklabels([0, 200, 400, 600, 800, 1000, 1200, 1400])\nf10_ax1.set_xlabel('Average amount of page-load events per 1000 students per 1 day in 2020, .000', fontsize = font_labels)\n\n# part II - states with lowest average engagement index\nstates_2 = list(state_group.state.unique()[:14])\nresults_states_2 = state_group[state_group.columns[2:6]][:14]\npercentages_states_2 = percentages_states_locale[:14]\n\n_, f10_ax2 = engagement_group((15, 9), results_states_2,\n                                   states_2, locale_labels,\n                                    cmap_1, percentages_states_2)\n#title\nf10_ax2.set_title('States with lowest engagement in digital learning products',\n                   fontsize = font_title, fontname = fontname)\n\n#ticks\nf10_ax2.set_xticklabels([0, 20, 40, 60, 80, 100, 120, 140])\nf10_ax2.set_xlabel('Average amount of page-load events per 1000 students per 1 day in 2020, .000', fontsize = font_labels)\n\nplt.show()","1bffc7c3":"def correct_add_data(df):\n    # correct 'District of Columbia' to upper case\n    df['state'] = df['state'].replace({'District of Columbia':'District Of Columbia'})\n    \n    # drop white spaces before and after names of the states\n    df['state'] =[\"\".join(string.rstrip().lstrip()) for string in df['state']]\n    \n    return df    \n    ","40cc8f9c":"# elections\nelections = pd.read_csv(r'https:\/\/raw.githubusercontent.com\/ElinaAizenberg\/Kaggle-competition\/main\/Popular%20vote%20backend%20-%20Sheet1.csv',\n                       usecols = [0,1], skiprows = [0,1,2,3], header = 0, names = ['state', 'called'])\n\n# drop sub-states like 'Maine 2nd District'\nelections = elections.drop(index = [4, 7, 15, 32, 39, 40])\n\nelections = correct_add_data(elections)\n\n# replace D with Democrats and R with Republicans\nelections['called'] = ['Democrats' if x == 'D' else 'Republicans' for x in elections['called']]\n\n# covid_cases\ncovid_cases = pd.read_csv(r'https:\/\/raw.githubusercontent.com\/ElinaAizenberg\/Kaggle-competition\/main\/United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv',\n                         usecols = [0,1, 5], names = ['submission_date', 'state_abb', 'new_case'], header = 0)\n\n#replace NYC with NY\ncovid_cases = covid_cases.replace({\"state_abb\": {'NYC':'NY'}})\n\n# abbreviation of states\nstate_abb = pd.read_csv(r'https:\/\/raw.githubusercontent.com\/ElinaAizenberg\/Vision-Zero-in-USA---project\/main\/data\/name-abbr.csv',\n                       header = None, names = ['state', 'state_abb'])\n\n# merge covid_cases with states abbreviations\ncovid_cases = covid_cases.merge(state_abb, how = 'left', on = 'state_abb')\n\n# drop rows: Republic of Marshall Islands, GU - Guam, VI - Virgin Islands, PR - Puerto Rico,\n# MP - Northern Mariana Islands, AS - American Samoa, PW - Palau, FSM - Federated States of Micronesia\ncovid_cases = covid_cases.loc[~covid_cases.state_abb.isin(['RMI', 'GU', 'VI', 'PR', 'MP', 'AS', 'PW','FSM'])]\n\n# number of universities per state\nuniversities = pd.read_csv(r'https:\/\/raw.githubusercontent.com\/ElinaAizenberg\/Kaggle-competition\/main\/tabn317.20.csv',\n                           header = 1, sep = ';', names = ['state', 'amount_uni'])\nuniversities.dropna(inplace = True)\n\nuniversities = correct_add_data(universities)\n\n# coordinates of states\n\nstates_coordinates = pd.read_csv('https:\/\/raw.githubusercontent.com\/ElinaAizenberg\/Kaggle-competition\/main\/statelatlong.csv',\n                                names = ['Abb', 'Latitude', 'Longitude', 'state'])\n\nstates_coordinates = correct_add_data(states_coordinates)","0de98217":"# teachers' salaries\nteachers_salaries = pd.read_csv('https:\/\/raw.githubusercontent.com\/ElinaAizenberg\/Kaggle-competition\/main\/tabn211.60.csv',\n                               header = 0, sep = ';')\n\nteachers_salaries = teachers_salaries.iloc[:,[0, 7]]\nteachers_salaries = teachers_salaries.rename(columns = {'Unnamed: 0':'state', '2019-20':'salary_2020'})\n\nteachers_salaries = correct_add_data(teachers_salaries)\nteachers_salaries.dropna(inplace = True)\n\n# replace 'Columbia' with 'District of Columbia'\nteachers_salaries['state'] = teachers_salaries['state'].replace({'Columbia':'District Of Columbia'})\n\n# states' population\n\npopulation = pd.read_csv('https:\/\/raw.githubusercontent.com\/ElinaAizenberg\/Kaggle-competition\/main\/nst-est2019-01.csv',\n                               header = 0, sep = ';')\n\npopulation.dropna(inplace = True)\n\npopulation['state'] = population['state'].str.replace('.', '')\npopulation['population_2019'] = population['population_2019'].str.replace(' ','')\n\n\npopulation['population_2019'] = population['population_2019'].astype(int)\npopulation = correct_add_data(population)","4a6e8170":"# the base of the new dataframe - state_group with information about engagement index\nstate_data = state_group[['state', 'engagement_index']]\nstate_data = pd.merge(state_data, state_abb, how = 'left', on = 'state')\n\n# merge all dataframes: universities, elections '20 results, population, teachers' salaries, coordinates of the states\nstate_data = pd.merge(state_data, universities, how = 'left', on = 'state')\nstate_data = pd.merge(state_data, elections, how = 'left', on = 'state')\nstate_data = pd.merge(state_data, population, how = 'left', on = 'state')\nstate_data = pd.merge(state_data, teachers_salaries, how = 'left', on = 'state')\nstate_data = pd.merge(state_data, states_coordinates[['Latitude','Longitude','state']], how = 'left', on = 'state')\n\n# calculate how many citizens per 1 university in each state\nstate_data['citizens_per_uni'] = round(state_data['population_2019'] \/ state_data['amount_uni'])\n\n# chage data type of salary column to integer\nstate_data['salary_2020'] = state_data['salary_2020'].str.replace(' ','')\nstate_data['salary_2020'] = state_data['salary_2020'].astype(int)","78058e31":"# visualize geo-data with choropleth\nurl = (\n    \"https:\/\/raw.githubusercontent.com\/python-visualization\/folium\/master\/examples\/data\"\n)\nstate_geo = f\"{url}\/us-states.json\"\nm = folium.Map(location=[40, -102], zoom_start=4, zoom_control=False,\n               scrollWheelZoom=False)\n\n#popup_text = \"State: {}, \\nElections in 2020: {}, Amount of universities: {}, Citizens per 1 university: {}, Average annual salary of teachers in '19: {}%\"\n\nfolium.Choropleth(\n    geo_data=state_geo,\n    name=\"choropleth\",\n    data=state_data,\n    columns=[\"state_abb\", \"engagement_index\"],\n    key_on=\"feature.id\",\n    fill_color=\"OrRd\",\n    threshold_scale=[0, 200000, 400000,600000, 800000, 1000000, 1200000, 1400000],\n    fill_opacity=0.7,\n    line_opacity=0.2,\n    legend_name=\"Average number of page-load events per 1000 students in 2020\",\n).add_to(m)\n\nfolium.LayerControl().add_to(m)\n\nfor index, row in state_data.iterrows():\n    text = 'State: '+ str(row.state) + '<br>' + 'Elections in 2020: ' + str(row.called) \\\n            + '<br>' + 'Amount of universities: ' + str(row.amount_uni) + '<br>' \\\n            + 'Citizens per 1 university: ' + str(row.citizens_per_uni) + '<br>' \\\n            + \"Average annual salary of teachers in '19: \" + str(row.salary_2020) + ' $'\n\n    iframe = folium.IFrame(text, width=290, height=130)\n    popup = folium.Popup(iframe, max_width=290)\n    \n    folium.Marker(\n    location=[row.Latitude, row.Longitude],\n    popup = popup,\n    icon=folium.Icon(color=\"green\")).add_to(m)\n\nm","8dc9b370":"# Prepare dataframes for analysis engagement_index VS Covid-19 cases\n\n# change data type of date column to DateTime\ncovid_cases['submission_date'] = pd.to_datetime(covid_cases['submission_date'], format = '%m\/%d\/%Y')\n\n# select only 2020 cases\ncovid_cases = covid_cases.loc[covid_cases.submission_date.dt.year == 2020]\n\n# sum new cases per day w\/o split on states\ncovid_cases_total = covid_cases.groupby(by = 'submission_date')['new_case'].sum().reset_index()\n\n# sort by date\ncovid_cases_total = covid_cases_total.sort_values(by = 'submission_date')\n\n# sum engagement index for all products and all districts per each day in 2020\nengagement_per_day = engagement.groupby(by = 'time')['engagement_index'].sum().reset_index()","d2bbc01f":"fig_11 = plt.figure(figsize = (16,7))\nfig_11_ax = fig_11.add_subplot()\n\n#covid-19 cases each day(bar chart) and rolling average number of cases per 7 days(line chart)\nfig_11_ax.bar(covid_cases_total.submission_date, covid_cases_total.new_case,\n            color = 'midnightblue', label='New COVID-19 cases', alpha = 0.5)\nfig_11_ax.plot(covid_cases_total.submission_date, covid_cases_total.new_case.rolling(7).mean(),\n            color = 'midnightblue', label='New COVID-19 cases', linewidth = 2.0)\n\n# twin axis\nfig_11_ax2 = fig_11_ax.twinx()\n\n# rolling average engagement index per 7 days\nfig_11_ax2.plot(engagement_per_day.time, engagement_per_day.engagement_index.rolling(7).mean(),\n            color = 'red', label='Amount of page-load events per 1000 students', linewidth = 2.0)\n\n# labels & ticks\nfig_11_ax2.set_xlim([datetime.date(2020, 1, 22), datetime.date(2021, 1, 1)])\n\nfig_11_ax.set_ylabel('Amount of new COVID-19 cases', fontsize = font_labels)\nfig_11_ax2.set_ylabel('Av. amount of page-load events per 1000 students', fontsize = font_labels)\nfig_11_ax2.set_yticklabels([])\n\n# legend\nfig_11_ax2.legend(loc = 'upper left', fontsize = 11)\nfig_11_ax.legend(loc = 'upper left', fontsize = 11)\n\n# grid\nfig_11_ax2.grid(b = False)\nfig_11_ax.grid(axis='x')\n\n# title\nfig_11_ax.set_title('COVID-19 cases VS engagement in digital learning products, 2020',\n                   fontsize = font_title, fontname = fontname)\n\nplt.show()","113626bd":"# CSV files downloaded from Google trends\ne_learning = pd.read_csv(r'https:\/\/raw.githubusercontent.com\/ElinaAizenberg\/Kaggle-competition\/main\/E_learning.csv',\n                        parse_dates = [0])\ndigital_learning = pd.read_csv(r'https:\/\/raw.githubusercontent.com\/ElinaAizenberg\/Kaggle-competition\/main\/Digital%20learning.csv',\n                              parse_dates = [0])\n\n\nfig_19, fig_19_ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 10))\n\nfig_19_ax_1 = fig_19_ax[0]\nfig_19_ax_2 = fig_19_ax[1]\n\n#E-learning topic\nfig_19_ax_1.plot(e_learning.Week, e_learning['E-Learning: (United States)'],\n            color = 'blue', linewidth = 2.0, label = 'E-learning')\n#Digital learning topic\nfig_19_ax_2.plot(digital_learning.Week, digital_learning['Digital learning: (United States)'],\n            color = 'darkcyan', linewidth = 2.0, label = 'Digital learning')\n\nfig_19_ax_1.axvspan(pd.Timestamp('2020-01-01'), pd.Timestamp('2021-01-01'),\n                    alpha=0.2, color='red')\n\nfig_19_ax_2.axvspan(pd.Timestamp('2020-01-01'), pd.Timestamp('2021-01-01'),\n                    alpha=0.2, color='red')\n\n#lines to separate years\nyears = ['2017-01-01','2018-01-01','2019-01-01','2020-01-01','2021-01-01']\n\nfor year in years:\n    fig_19_ax_1.vlines(pd.Timestamp(year), 0, 100,\n                       colors='black', linestyles='-',\n                       linewidth = 1.0, alpha = 0.5)\n    fig_19_ax_2.vlines(pd.Timestamp(year), 0, 100,\n                       colors='black', linestyles='-',\n                       linewidth = 1.0, alpha = 0.5)\n\n# legend    \nfig_19_ax_1.legend(fontsize = 12,\n                loc = 'upper right')\nfig_19_ax_2.legend(fontsize = 12,\n                loc = 'upper right')\n\n# labels & ticks\nfig_19_ax_1.set_ylabel('Interest rate', fontsize = font_labels)\nfig_19_ax_2.set_ylabel('Interest rate', fontsize = font_labels)\n\n# grid\nfig_19_ax_1.grid(axis='x')\nfig_19_ax_2.grid(axis='x')\n\n# title\nfig_19.suptitle('Interest in online education topics in the USA, 2017-2021',\n                   fontsize = font_title, fontname = fontname)\n\nplt.show()","de4c0b4a":"# merge engagement and districts dataframes\nsocial_engagement = pd.merge(engagement[['engagement_index','district_id', 'time']],\n                         districts[['district_id', 'state', 'pct_black\/hispanic','pct_free\/reduced', 'pp_total_raw']],\n                         how = 'left', on = 'district_id')\n\n# merge new dataframe and information about each state\nsocial_engagement = pd.merge(social_engagement,\n                             state_data[['state', 'called', 'salary_2020', 'citizens_per_uni']],\n                            how = 'left', on = 'state')\n\n# drop only the rows which has missing information about the state\nsocial_engagement = social_engagement.dropna(subset = ['state'], axis=0, how='any')","0c78e902":"def prepare_social_df(df, social_column):\n    #this function calculates average engagegment index per each catogory of the feature chosen for analysis\n    #df - social_engagement dataframe\\\n    #social_column - feature for analysis\n    \n    democrats = df.loc[df.called == 'Democrats'].groupby(\n                by = ['district_id', social_column])['engagement_index'].sum().reset_index()\n    \n    republicans = df.loc[df.called == 'Republicans'].groupby(\n                by = ['district_id',social_column])['engagement_index'].sum().reset_index()\n    \n    democrats.dropna(inplace = True)\n    republicans.dropna(inplace = True)\n\n    democrats = democrats.groupby(by = social_column)['engagement_index'].mean().reset_index()\n    republicans = republicans.groupby(by = social_column)['engagement_index'].mean().reset_index()\n\n    social_df = pd.merge(democrats, republicans, how = 'outer', on = social_column)\n    social_df = social_df.rename(columns = {'engagement_index_x':'engagement_D',\n                                                    'engagement_index_y':'engagement_R'})\n\n    social_df['engagement_D'] = social_df['engagement_D'] \/ 1000\n    social_df['engagement_R'] = social_df['engagement_R'] \/ 1000\n    \n    return social_df\n    ","57498bba":"# create categorical feature based on continious feature of teachers' salaries\nsocial_engagement['salary_2020_range'] = pd.cut(social_engagement['salary_2020'],\n                                                bins=[0, 50000, 60000, 70000, 80000, 90000],\n                                                labels=['40-50K $', '50-60K $', '60-70K $', '70-80K $', '80-90K $'])\n","72c8f6ab":"def butterfly_chart(df, social_column, yticklabels, ylabel):\n    # this function creates a butterfly chart with split for republican and democratic states\n\n    hfont = {'fontname':'Calibri'}\n    color_red = '#fd625e'\n    color_blue = '#01b8aa'\n    index = df[social_column]\n    column0 = df['engagement_R']\n    column1 = df['engagement_D']\n    title0 = 'Republicans, .000 per-load events'\n    title1 = 'Democrats, .000 per-load events'\n\n    fig, ax = plt.subplots(figsize=(10.5,5), ncols=2, sharey=True)\n    fig.tight_layout()\n\n    ax[0].barh(index, column0, align='center', color=color_red, zorder=10)\n    ax[0].set_title(title0, fontsize=14, pad=14, color=color_red, **hfont)\n    ax[1].barh(index, column1, align='center', color=color_blue, zorder=10)\n    ax[1].set_title(title1, fontsize=14, pad=14, color=color_blue, **hfont)\n\n    # If you have positive numbers and want to invert the x-axis of the left plot\n    ax[0].invert_xaxis() \n\n    # To show data from highest to lowest\n    plt.gca().invert_yaxis()\n\n    ax[0].set(yticks=df[social_column],\n                    yticklabels=yticklabels,\n                    ylabel = ylabel)\n\n\n    ax[0].set_xticks([0, 5000, 10000, 15000, 20000])\n    ax[1].set_xticks([0, 5000, 10000, 15000, 20000])\n\n    plt.subplots_adjust(wspace=0, top=0.85, bottom=0.1, left=0.18, right=0.95)\n    \n    fig.suptitle('Average engagement rate per school district in democratic and republican states',\n                 fontsize=font_title, fontname = fontname)\n\n    return fig, ax","0f5bdace":"ethnics_group = prepare_social_df(social_engagement, 'pct_black\/hispanic')\n\nyticklabels_ethnics = ['0-20%', '20-40%', '40-60%', '60-80%', '80-100%']\nylabel_ethnics = '% black\/hispanic students'\n\nfig_12, fig_12_ax = butterfly_chart(ethnics_group, 'pct_black\/hispanic',\n                                    yticklabels_ethnics, ylabel_ethnics)\n","943fa7e5":"lunch_group = prepare_social_df(social_engagement, 'pct_free\/reduced')\n\nyticklabels_lunch = ['0-20%', '20-40%', '40-60%', '60-80%', '80-100%']\nylabel_lunch = '% of students receiving free\/reduced lunches'\n\nfig_13, fig_13_ax = butterfly_chart(lunch_group, 'pct_free\/reduced',\n                                    yticklabels_lunch, ylabel_lunch)","a4ba5c83":"fig_15 = plt.figure(figsize = (17,7))\nfig_15_ax = fig_15.add_subplot()\n\nfig_15_ax.bar(state_data.loc[state_data.called == 'Republicans']['state'],\n        state_data.loc[state_data.called == 'Republicans']['salary_2020'],\n        color = '#fd625e', label = 'Republicans')\n\nfig_15_ax.bar(state_data[state_data.called == 'Democrats']['state'],\n        state_data[state_data.called == 'Democrats']['salary_2020'],\n        color = '#01b8aa', label = 'Democrats')\n\n# ticks\nplt.xticks(rotation = 80, fontsize = font_ticks)\nfig_15_ax.set_ylim(20000, 90000)\n\n#labels\nfig_15_ax.set_ylabel('Average salary, $', fontsize = font_labels)\n\n# display the line of average salary of school teacher in 2019 - 2020: 63 645 $\nplt.axhline(y=63645, color=\"black\", linestyle=\"--\")\nfig_15_ax.text(6.5, 65000,'Average salary across the states - $63 645 per year', fontsize = font_text)\n\n# legend\nfig_15_ax.legend(loc = 'upper left', fontsize = font_legend)\n\n#grid\nfig_15_ax.grid(axis='x')\n\n# title\nfig_15_ax.set_title('Average salary of school teachers in USA, 2019 - 2020 ', fontsize = font_title, fontname = fontname)\n\nplt.show()","384a3d1e":"#manual application of the function prepare_social_df\ndemocrats = social_engagement.loc[social_engagement.called == 'Democrats'].groupby(\n                by = ['district_id', 'salary_2020_range'])['engagement_index'].sum().reset_index()\n    \nrepublicans = social_engagement.loc[social_engagement.called == 'Republicans'].groupby(\n            by = ['district_id','salary_2020_range'])['engagement_index'].sum().reset_index()\n\ndemocrats = democrats.loc[democrats['engagement_index'] > 0]\nrepublicans = republicans.loc[republicans['engagement_index'] > 0]\n\ndemocrats = democrats.groupby(by = 'salary_2020_range')['engagement_index'].mean().reset_index()\nrepublicans = republicans.groupby(by = 'salary_2020_range')['engagement_index'].mean().reset_index()\n\nsalary_group = pd.merge(democrats, republicans, how = 'outer', on = 'salary_2020_range')\nsalary_group = salary_group.rename(columns = {'engagement_index_x':'engagement_D',\n                                                'engagement_index_y':'engagement_R'})\n\nsalary_group['engagement_D'] = salary_group['engagement_D'] \/ 1000\nsalary_group['engagement_R'] = salary_group['engagement_R'] \/ 1000\n    \n    \nyticklabels_salary = ['40-50K $', '50-60K $', '60-70K $', '70-80K $', '80-90K $']\nylabel_salary = \"Average teacher's salary in 2020\"\n\nfig_14, fig_14_ax = butterfly_chart(salary_group, 'salary_2020_range',\n                                    yticklabels_salary, ylabel_salary)\n\nplt.show()","f9bcd9c7":"districts_finance = pd.merge(districts[['district_id','state', 'pp_total_raw']],\n                            state_data[['state', 'called']],\n                            how = 'left', on = 'state')\n\ndistricts_finance = districts_finance.groupby(\n                    by = ['pp_total_raw', 'called'])['district_id'].count(). \\\n                    unstack(level = 1).reset_index()\n\n# fill missing values with '0'\ndistricts_finance.fillna(0, inplace = True)\n\n# change order of indexes to order costs per students in ascending order\ndistricts_finance = districts_finance.reindex(index = [10,11,12,13,0,1,2,3,4,5,6,7,8,9])","f6728a38":"fig_16 = plt.figure(figsize = (10,7))\nfig_16_ax = fig_16.add_subplot()\n\nfig_16_ax.barh(districts_finance.pp_total_raw, districts_finance.Republicans,\n                label='Republicans', color = '#fd625e')\nfig_16_ax.barh(districts_finance.pp_total_raw, districts_finance.Democrats,\n                left = districts_finance.Republicans,\n                label='Democrats', color = '#01b8aa')\n\n\nfig_16_ax.set_yticklabels(labels = districts_finance.pp_total_raw, fontsize = 11)\nfig_16_ax.set_xlim(0, 50)\n\n# To show data from highest to lowest\nplt.gca().invert_yaxis()\n\n# legend\nfig_16_ax.legend(loc = 'upper right', fontsize = font_legend)\n\n#grid\nfig_16_ax.grid(axis='y')\n\n# labels\nfig_16_ax.set_xlabel('Number of districts with the corresponding per-pupil expenditure range', fontsize = font_labels)\nfig_16_ax.set_ylabel('Per-pupil expenditure range, $', fontsize = font_labels)\n\n# title\nfig_16_ax.set_title('Per-pupil total expenditure in districts', fontsize = font_title, fontname = fontname)\n\n\nplt.show()","593cc9e3":"expenditure_group = prepare_social_df(social_engagement, 'pp_total_raw')\nexpenditure_group = expenditure_group.reindex(index = [11,12,10,13,0,1,2,3,4,5,6,7,8,9])\n\nylabel_expenditure = 'Per-pupil expenditure range, $'\n\nfig_17, fig_17_ax = butterfly_chart(expenditure_group, 'pp_total_raw',\n                                    expenditure_group.pp_total_raw, ylabel_expenditure)\n\nfig_17.set_size_inches(12, 7.5)\n\nfig_17_ax[0].set_xticks([0, 5000, 10000, 15000, 20000, 25000, 30000])\nfig_17_ax[1].set_xticks([0, 5000, 10000, 15000, 20000, 25000, 30000])\n\nplt.show()","ed866e8f":"**Missing values:**\n- 'engagement_index' equals NaN when 'pct_access' equals 0 or NaN, therefore, in the first case NaN values can be replaced with 0;\n- lines with missing information about 'lp_id' and 'engagement_index' are useless for the following analysis, therefore, they can be dropped.","bfde447c":"# Table of contents\n\n**Introduction**\n\n**1 Datasets pre-processing**\n\n     1.1 Engagement dataset\n     1.2 Districts dataset\n     1.3 Products dataset\n     \n**2 Top digital learning products in 2020**\n\n     2.1 Users of the digital learning products\n     2.2 Functions of the digital learning products\n     \n**3 What is the picture of engagement in digital learning in 2020?**\n\n     3.1 Engagement in digital learning products for each Users' category\n     3.2 Engagement in digital learning products with various functionality\n     \n**4 City, Town, Suburb, Rural**\n\n**5 Additional information about the states**\n\n**6 Engagement in digital learning products vs COVID-19 pandemic**\n\n**7 Influence of social \/ political \/ financial aspects on online engagement**\n\n     7.1 Social context\n     7.2 Financial context\n\n**Personal view**\n\n**Data**\n\n\n\n# Introduction\n\nLot of words have already been said about digital learning and the influence of the global pandemic on education. And hundreds of articles and research work will be written in the next few years to evaluate the effect of COVID-19 pandemic on education and other social institutions worldwide.\n\nSo, I wish to proceed with analysis immediately and not to bore you with the long introduction.\nLet's see what was the state of digital learning in the USA in the outstanding and frightening 2020.\n","2a5a8010":"**Missing values:**\n'Sector(s)' and  'Primary Essential Function' - I decided to fill in missing values with the most common sector for a company-producer. Usually a product with missing information about its sector or function details is a version or part of products' group in the dataset. In the situations when company has only 1 product and there is no information about its  sector or function, I decided to choose following characteristics:\n- Sector = 'Higher Ed; Corporate';\n- Primary Essential Function = 'LC\/CM\/SDO - Other'.\n\n**New features:**\n- split 'Primary Essential Function' into 2 features: 'Function(abb)' and 'Function(details)'. 'Function(abb)' consists of 4 possible categories, while 'Function(details)' consists of 18 different categories. The split into 2 features will provide 2 levels for analysis of DL products' usage and facilitate analysing process;\n- feature 'Users' is created based on feature 'Sector(s)' and does not add new information, but increases interpretability of 'Sector(s)' column, displaying groups of students who are target-users of each product.   \n\n","67b63699":"**Key insights:**\n- on average we can notice increase in engagement rate along with increase in expenditure per pupil in schools dicticts;\n- the highest engagement rate is achieved in the third from the last category in both democratic and republican states. **Probably, there is some point in financial support of the school when its efficiency stops rising**;\n- another possible explanation is that schools with high level of expenditure per pupil try to engage students more in offline activities than in online;\n- in all categories where democratic and republican districts have their schools, average engagement in democratic districts is higher than in republican ones. ","c7f77bd5":"Due to anonymization procedures there are a lot of missing values in the dataset that creates limitations for analysis. Nevertheless, some information gaps can be eliminated based on information about the state where the school district is located.\n\n**Missing values:**\n- 'state' - only districts with information about state can be valuable for analysis, rows with missing 'state' are empty in other columns too, therefore, rows with 'state' equals NaN should be dropped;\n- 'county_connections_ratio' - non-informative column, because out of 70% non-missing values, 99% of values (161 out of 162) equal '0.18-1', therefore, the entire column can not be used to analyze impact of connections ratio on learning process.\n\n**'pp_total_raw'** column deserves special attention. This information was taken from Edunomics Lab's National Education Resource Database on Schools (NERD$) project. The column contains the median value or more specifically the range around median value of the expenditure of a given school district.\n\nEven though I cannot find missing expenditure values for the specific districts, I used the same source [Edunomics Lab](https:\/\/edunomicslab.org\/nerds\/) to download files with per-state information about total expenditure for every school in a state and calculate median values for each state with missing data. The only state with no available information is New Hampshire.\n\n**Interpretation**:\n'pct_black\/hispanic', 'pct_free\/reduced' and 'pp_total_raw' are columns with categorical information. For easier interpretation I performed some changes to make them cleaner.\n\n","c7d9b383":"### 2.2 Functions of the digital learning products\n\n**How digital learning products are used by students\/teachers\/users in educational process?**\n\nIn my view, the chart below not only answers the question, but also demonstrate why schools and universities all over the world were not prepared to enforced switch to online-lessons.\nThe products that are designed to organize educational process online and teach effectively, safely constitute together only 17% (9% CM + 8% SDO).\nTeachers have faced new reality with little technical support, not speaking about methodological background.\n\nThe vast majority of products are from 'Learning & Curriculum' segment which might indicate that educational content is valued higher than smooth and effective educational process.\n\n**Some interesting point to note:**\n- only 1% of 'Learning & Curriculum' products is dedicated to 'Career Planning & Job Search', although, nowadays it is a topic of high demand.","54f143ca":"**Key insights:**\n- in republican states school districts varies from 4000-6000 USD per pupil to 11000-13000 USD per pupil;\n- the **most frequent expenditure range in republican districts is 8000-10000 USD per pupil**;\n- in democratic states school districts the smallest expenditure range is 2 times higher than in republicans - 8000-10000 USD\n- as well as the most frequent expenditure range **in democratic districts is 2 times higher than in republicans - 16000-18000 USD**;","5e26ae4c":"# 7 Influence of social \/ political \/ financial aspects on online engagement\n\nIn this section I would like to analyze how engagement in digital learning products relates to political orientation of the state where the school district is located, whether there is correlation between ethnicity distribution in school district or financial status of students in a district and online engagement.\n\nAlso, I believe that there might be a correlation between average salaries of teachers in a state, per-pupil expenditure and level of online engagement because the better is financial support of the school, the higher teachers' salaries are, the more opportunities to organize online education are at disposal.\n\nI decided to analyze the impact of social and financial aspects along with the political split of the states because I believe that in many cases each state of the USA can be considered as a distinct country. And one aspect that can help in clustering school districts is the political orientation of each state. I use the results of the latest presidential election to split school districts on democratic and republican.\n\n### 7.1 Social context\n\n*Methodology*:\n1. sum up total engagement index for ALL products for the whole year per each school district and per each category in social\/financial feature separately in democratic and republican states;\n2. calculate average engagement index per 1 district for the whole year per each category.","709df7e0":"I have an assumption that the level of average engagement per state might relate to the number of big and famous universities in the state. For instance, Connecticut - Yale, Illinois - University of Chicago, Massachusetts - MIT and Harvard, California - Stanford, New York - Columbia University and Cornell University etc. Although Princeton in New Jersey didn't bring this state one of the top places, I assume it is the result of missing information in the districts dataset.","29d15a17":"# 2 Top digital learning products in 2020\n\nThis section covers an overview of the top Digital Learning products with the most users in 2020. First of all, the state of the market should be analyzed. I wondered **who are the main users of digital learning products and if there is any free niche for those who wish to enter this growing market in the near future.**\n\n### 2.1 Users of the digital learning products\n\nBased on my personal experience of using various learning platforms and educational sources I got an impression that main users of the rapidly growing digital education market are working specialists who wish to change careers and college\/university students who wish to expand knowledge and skills they receive during official education. Surprisingly for me, the results of the analysis showed an absolutely different picture.\n","190f648b":"# Personal view\n\nOrganizers offered several questions to answer using the data they provided. The most vague question that cannot be answered with this analysis is **how might online and distance learning evolve in the future?** \n\nI want to disclose my personal opinion on this problem that I formed in discussions with secondary school teachers and university professors in Moscow.\n\nOnline learning will definitely develop in the near future and even though after the pandemic the level of skepsis towards digital learning products increases, it will not stop the process of transferring from offline to online education.\noffline education has its own limits that cannot be overcome and that are successfully beaten by online education:\n- cost of education (primarily for higher education);\n- strict discipline rules that young students and their parents may not approve;\n- state defined educational programs.\n\nOnline education offers a candy in the form of a more free timetable, ability to study from any corner of the Earth, lower prices for higher education degrees and more freedom in choosing the syllabus. What is more, nowadays there is a widespread opinion that children should not learn handwriting, history or basic math because everything can be found in Google, calculated on a laptop and they will not write by hand at all. And this tendency has no geographical borders. A lesson with a laptop seems better than a lesson with a textbook and whiteboard. To my mind, the lower the level of education in society, the more this society will support online education. \n\nThis will lead to a bigger education gap: offline education for the rich, online education for the poor. I see that during  the pandemic the opposite view emerged: poorer and more vulnerable with limited access to digital products, bad Internet connection suffer more from distance education. But this situation is temporary: Internet connection will be more and more accessible for everybody, while real schools with educated teachers which are expensive to maintain will shorten in amount.\nUniversities and schools where teachers\/professors will continue to communicate with students face to face and teach them to research, defend their views and develop critical thinking will become a privilege with a highly competitive entrance examination.\n\nI hope my predictions and personal view do not seem too pessimistic because there is also a bright side. Even though I assume that the quality of online education is significantly lower in comparison with offline learning, its broad application might offer great opportunities in those countries and regions where there are no or very limited alternatives. ","62c262c2":"**Key insights:**\n- **3 year before the pandemic interest of Google users in E-learning topic followed the same pattern**: increase at the beginning of the year, reduction  by half during summer months with following rise at the beginning of a new academic year and decrease by Christmas;\n- 2020 demonstrated outstanding results in absolute values, but the same pattern in general;\n- **the most important thing to note: in 2021 interest rate returns on the level of 2017-2019**;\n- Digital learning topic demonstrates a less clear pattern each year, except for a surge during the first weeks every year, but it is easy to notice a dramatic increase in the middle of 2020;\n- once again, **in 2021 interest rate of Digital learning topic is on the pre-pandemic level**.\n","3a18ae01":"**Some interesting points to note:**\n- there is **no obvious and uniform correlation during the year between the progress of the pandemic and engagement in digital learning products**;\n- the significant growth of engagement rate at the beginning of the year is seen before pandemic declaration and schools' closure;\n- there might be **some relationship between the increase of COVID-19 cases and drop of engagement in digital learning during the summer months**. Probably, large number of children and students who were not occupied with distance learning might be one of the reasons of the higher 'second wave of the pandemic';\n- it seems that with the beginning of the new academic year and increase in usage of digital learning products the severity of pandemic slightly decreased;\n- even though we can notice sharp decrease both in engagement and COVID-19 cases during Thanksgiving week, it resulted in subsequent increase, probably, due to celebrating national holiday with families;\n- situation **during Christmas holidays remindes summer months: decrease in engagement in digital learning was accompanied by a surge in new COVID-19 cases**.\n\nSpeaking about the future of distance learning after the global pandemic, I would like to note that education is a social process and social institution and, as\u00a0[political scientist Dr. Ekaterina Schulmann](https:\/\/en.wikipedia.org\/wiki\/Ekaterina_Schulmann) said, **crises do not bring fundamental changes to the world. They accelerate the processes that were growing and kills those that were dying**. Therefore, to predict the vague future of distance learning we should know what was the trend before the pandemic.\n\nAs we have no data from previous years, I decided to get a general understanding of people's interest in digital or online education using Google Trends. I chose **2 topics in Google Web search in the USA and downloaded the data for the last 5 years**:\n1. E-learning;\n2. Digital learning.\n\nThe analysis is based on the value of 'interest over time' which represents search interest relative to the highest point on the chart for the given region (USA) and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means there was not enough data for this term.\n\n","6cf1c4b2":"### 3.2 Engagement in digital learning products with various functionality\n\nSpeaking about functionality of digital learning products, there are 18 different categories of functions and analysing each of them seems inefficient to me. So, I decided to compare monthly engagement in products with functions that were mostly used in 2020 across all states and monthly engagement in products that fluctuated over average engagement index more than others.\n\n*Methodology for identifying the most fluctuating functions:*\n1. sum up total engagement index (number of page-load events per one 1000 students) for ALL PRODUCTS per Function category per day;\n2. calculate average engagement index per month in 2020, per Function category;\n3. scale monthly means with the total average of engagement index per year and calculate standard deviation of each month;\n4. choose the top-3 mostly fluctuating functions and display monthly average engagement index for these functions.\n\n*Methodology for identifying the most used functions:*\n1. sum up total engagement index (number of page-load events per one 1000 students) for ALL PRODUCTS per Function category per day;\n2. calculate average engagement index per day in 2020, per Function category;\n3. choose the top-3 functions with the largest average engagement index and display monthly average engagement index for these functions.\n","129fca60":"# 4 City, Town, Suburb, Rural\n\nCoronavirus moving study conducted by [MYMOVE](https:\/\/www.mymove.com\/moving\/covid-19\/coronavirus-moving-trends\/) showed that pandemic affected tremendously the moving trends and people's preferences in terms of type of location where to live. It is exciting to compare data about engagement in digital learning products and information about relocation trends in 2020.\n\nEven though we cannot observe if there were any changes in engagement from various locations in comparison with the last few years, the charts below can partially confirm some facts about migration from cities to suburbs and rural areas.\n\nFirst of all, it is obvious that **suburbs are leading in interest in digital education, while cities and towns are several times less active in using digital learning products**. It corresponds to the moving trend of pandemic times: from cities to suburbs, from crowded spaces to less densely populated areas.\n\nAlthough absolute numbers of engagement rate differ significantly depending on the type of location, relative numbers of engagement in products with various functionality are quite similar for all types.","1aa6eeb5":"**% of black \/ hispanic students in a school district:**\n\n- as we can see, average engagement in democratic and republican states differs: in all categories of ethnic feature **engagement in digital learning is higher in democratic states than in republican**;\n- the highest engagement in democratic states is achieved in districts with simultaneously highest and lowest percentage of black \/ hispanic students;\n- I assume that quite similar situation can be observed in republican states, but due to the lack of information the highest engagement rate is achieved in 0-20% and 60-80% groups, while 80-100% category shows the lowest result.\n\n\n","cf3ed62b":"\n### 7.2 Financial context\n\nFor analysis I will consider 2 financial features:\n- average salary of school teachers per state;\n- total per-pupil expenditure per school district.\n\nI suppose there are no doubts that teachers with higher salaries will work better, have more opportunities to study new teaching methods with usage of digital products, can be more involved and interested in organizing engaging educational processes.\n\nIn the states included into the dataset we can see the following:\n\n- **in all republican states average teachers' salaries are significantly lower than the national average** and it might be related to the lower average engagement in digital learning that we observe in republican states;\n- in 29% of democratic states teachers' salaries are significantly lower than the national average (< 60 000 USD), but in the rest 50% of the democratic states salaries are significantly higher than the national average (> 70 000 USD) and on average in democratic states teachers earn much better.","f6322610":"Analysing engagement in products for different Users' groups, we can see surprising results:\n- even though **products for children under 18 y.o. constitute almost 50% of all digital learning products, their part in total engagement rate does not exceed 20% in all locations**;\n- vice versa with universal  products (from users perspective): 74% - 84% of total engagement is constituted by 32% of products. **Nice visualization of [Pareto principle](https:\/\/en.wikipedia.org\/wiki\/Pareto_principle)**;\n- rural areas and suburbs have similar distribution of engagement in products, as well as towns and cities;\n- in all location types part of engagement in products for students of all ages (this category includes university\/college students) does not exceed 5%.\n\nAnalysing the behavior of weekly average engagement rate in various location types, the prime thing to notice is that Suburb - line fluctuates much more than others. **Starting  and ending with the same level of engagement in digital learning products, Suburbs show dramatic rise\/drops during the year and responds to events like end of academic year, holidays or declaration of pandemic much harder than other locations.**\n\nPresumably, the growth after the 12th week and higher engagement in 2020-2021 academic year in suburbs and rural areas might be related to the mentioned migration from cities to less densely popuated areas. On the other hand, during the first 10 weeks of 2020 total engagement in Suburbs is obviously much higher than in other locations.","2e523161":"# 3 What is the picture of engagement in digital learning in 2020?\n\nThe most exciting part of the presented analysis is to see how people in the USA studied during such an unusual and difficult year when online education became not a choice, but an obligation.\n\n**Did they actually use 'the gift of time'?**\n\n**What digital learning products were the most popular among users in different categories?**\n\n**Will we see some patterns in users' behavior?**\n\n\n### 3.1 Engagement in digital learning products for each Users category \n\n*Methodology:*\n1. sum up total engagement index (number of page-load events per one 1000 students) for ALL PRODUCTS per Users' category per day;\n2. calculate average engagement index per week in 2020, per Users' category;\n3. display the behavior of weekly engagement index;\n3. for each product in every category of Users' feature calculate average engagement index, sort the list and choose n-most popular products.","848ef86c":"**Average teacher's salary:**\n- unfornunately, there is no obviuos positive correlation between average salary in a state and engagement in digital learning. In both political categories we can notice the decrease in engagement and I assume that is a interesting issue to explore in cooperation in teaching methodologists and teachers;\n\n- at the same time, in the 50-60K USD category which is the only mutual category for democratic and republican states, average engagement in democratic states' districts is higher than in republicans.\n\nThe chart below shows the distribution of school districts according to per-pupil expenditure range.","3825e120":"# 5 Additional information about states\n\nThere are multiple factors that might influence students' engagement in digital learning process:\n- organization of educational process;\n- teacher's qualification, background and personal interest in doing its best for students;\n- educational system in country\/state and support of teachers from authorities (financial\/methodological\/technical);\n- current trends among students concerning value of education, especially higher education;\n- current trends on job market;\n- cultural traditions, family background and so on.\n\nIn my view, analyzing all states together is not entirely correct because in many issues the states can be considered as different 50 countries. To get a better understanding of the states from the districts dataset, I decided to create visualization with the following information per state:\n\n1. choropleth (colored map according to certain statistical metric) based on average engagement rate per day in each state;\n2. political orientation of the state based on presidential elections 2020 - I truly believe that political views of the majority in the state and educational issues are closely related;\n3. average annual salary of school teachers - not only per-pupil funding matters in education, but also per-teacher funding;\n4. number of universities and ratio 'citizens per 1 university' in the state - to my mind, higher education institutions are like centers of gravity in cities and towns. They play a significant role in the cultural life of citizens and can set cultural trends especially among young people.\n\nUsing this map, you can see how different the states are and, what is more important for analysis on the country level, how many blank spaces are on this map. Almost half of the states are neglected which makes results unreliable and the whole analysis quite limited.\n\nNevertheless, there might be interesting correlations between level of engagement and the features I chose to display.","6170d92c":"**Key insights:**\n- almost **half of all products are designed for children under 18 years old** (kindergarten, primary and secondary education);\n- specifically corporate users or working specialists not enrolled in universities are almost neglected: less than 1% of all products;\n- almost **1\/3 of digital learning products can be used universally by users of all ages**;\n- university students experience little support from providers of digital learning products: 17% of products can be used by students of all ages including both kindergarten and universities.\n\n\nThere are 3 categories of users or products' sectors: PreK-12 (school students), Higher education (college\/university students) and corporate users. I wondered **how these categories intersect and who are major clients of digital education**.\n\nThe chart below shows that school students can use almost all digital learning products from the provided list and apparently are seen as the major group of users of digital learning, while older users have 2 times less options.","852541c7":"# Data\n\n1. United States COVID-19 Cases and Deaths by State over Time ([link](https:\/\/catalog.data.gov\/dataset\/united-states-covid-19-cases-and-deaths-by-state-over-time));\n2. Edunomics Lab's National Education Resource Database on Schools (NERD) project ([link](https:\/\/edunomicslab.org\/nerds\/));\n3. \tEstimated average annual salary of teachers in public elementary and secondary schools, Digest of Education Statistics (NCES) ([link](https:\/\/nces.ed.gov\/programs\/digest\/d20\/tables\/dt20_211.60.asp));\n4. How Teacher Pay Compares to Average Salary in Each State 2021 ([link](https:\/\/www.business.org\/hr\/workforce-management\/best-us-states-for-teachers\/));\n5. 2020 National Popular Vote Tracker ([link](https:\/\/cookpolitical.com\/2020-national-popular-vote-tracker));\n6. Degree-granting postsecondary institutions, by control and classification of institution and state or jurisdiction, Digest of Education Statistics (NCES) ([link](https:\/\/nces.ed.gov\/programs\/digest\/d20\/tables\/dt20_317.20.asp));\n7. State Population Totals: 2010-2019, US Census Buro ([link](https:\/\/www.census.gov\/data\/datasets\/time-series\/demo\/popest\/2010s-state-total.html#par_textimage_500989927));\n8. USA lat,long for state abbreviations, Kaggle dataset ([link](https:\/\/www.kaggle.com\/washimahmed\/usa-latlong-for-state-abbreviations));\n9. Google Trends, Web search topic 'E-learning' ([link](https:\/\/trends.google.com\/trends\/explore?date=today%205-y&geo=US&q=%2Fg%2F121tbbp8));\n10. Google Trends, Web search topic 'Digital learning' ([link](https:\/\/trends.google.com\/trends\/explore?date=today%205-y&geo=US&q=%2Fm%2F0113h59z)).\n","f1284146":"**Some interesting points to note:**\n- there are literally no products only for university students, although, to my mind this category of digital learning users have a direct interest in gaining knowledge before they enter highly competitive job market and they have more skills to study independently without guidance from teachers (in comparison with school students) and have more free time (in comparison with corporate users);\n- there is only 1 product ([Weebly](https:\/\/weebly.com\/) - web-hosting service) designed specifically for corporate users, although, there are multiple spheres and industries where constant learning is of paramount importance for professionals and can be provided with digital products.\n\n**Overall, the charts above demonstrate outdated attitude towards education in general:**\n\n- people study mostly in schools (for more than 10 years);\n\n- less than half of them continue to study in colleges\/universities  ([37.5% of the U.S. population who were aged 25 and above had graduated from college or another higher education institution](https:\/\/www.statista.com\/statistics\/184260\/educational-attainment-in-the-us\/));\n\n- after graduation people stop educating themselves or use the same sources as university students.\n\nI consider this view of education and people's attitude towards it to be outdated because nowadays people tend to change careers and specializations during their lives, more and more secondary school graduates and even university students value education and try to get new skills online to be more versatile in our competitive world. This tendency has not started with COVID-19 pandemic, but this worldwide crisis might reinforce it.\n\n","5d4caa24":"# 1 Datasets pre-processing\n\n### 1.1 Engagement dataset\n","0c1e2c4b":"### 1.3 Products dataset","3ff87431":"**Key insights:**\n- presumably, all three **most fluctuating functions experienced strong influence of the pandemic**;\n- almost zero engagement in Virtual Classroom products in January, February with subsequent increase in March (after COVID-19 pandemic declaration) and tripled engagement index after summer months shows that the **need in Virtual Classroom products was created by circumstances** and entrenched at the beginning of the new academic year in 2020;\n- the absolutely opposite situation can be observed in Admission, Enrollment & Rostering category. With the start of the pandemic engagement in these products dropped to almost zero;\n- **EHS Compliance products demonstrated increase in March, April** when schools and universities across all states were closed and transferred to online-education;\n- the most used functions\u00a0demonstrate the same pattern: increase at the beginning of the year till the spring with the subsequent decrease to the lowest engagement level in July. Engagement started to rise quickly at the beginning of the new academic year (2020-2021) and slightly decrease by Christmas holidays;\n- I assume that **pandemic might influence the absolute numbers of engagement in the products of the most used functions, but had little effect on the pattern itself**. High usage during the academic year and low interest during holidays seem natural.\n","bcd3d2b8":"# 6  Engagement in digital learning products vs COVID-19 pandemic\n\nOne of the main questions that were offered to disclose in the description of this analytical challenge and undoubtedly was discussed in teachers' communities all over the word is **'How did COVID-19 influence online education?'**. More general and more unpredictable issue to discuss is **'What is the future of online education after the pandemic?'**.\n\nOnce again I would like to underline that we cannot make any conclusions based solely on the data of 2020. First of all, this year should be considered as an outlier because of the circumstances that made transfer from traditional learning to distance learning obligatory, rapid and unprepared. Secondly, this transfer revealed all disadvantages of distance learning that might result in further resistance no matter how many great distance learning products will be on the market. And finally, the pandemic hasn't finished in December 2020. It is not over even now, in September 2021. Therefore, it is too early to make any conclusions about the effect of the pandemic on distance learning.\n\nNevertheless, we might evaluate if there were any relationships between the engagement in digital learning products and number of COVID-19 cases or severity of pandemic in the USA.\n\n*Methodology:*\n1. sum all new Covid-19 cases from all states per day;\n2. calculate 7-days rolling mean of the new cases;\n3. sum up total engagement index (number of page-load events per one 1000 students) for ALL PRODUCTS and all districts per day;\n4. calculate 7-days rolling mean of the engagement index.\n\n","96789ee8":"Learning & Curriculum products constitute 65-68% in cities, suburbs and rural areas. While users who live in towns express more interest in this type of products - 74%.\n\nThe similar situation is observed with School & District Operations products: 20-21% in all locations except for towns where these products constitute 18% on the engagement rate.\n\n**Interesting point to note**:\n- even though universal products (from functions perspective) include Facebook and some of the Microsoft Office and Google products, total engagement in them in all location types does not exceed 5%.\n\n","976cf767":"**Key insights:**\n- the common patterns for all categories are, first of all, **dramatic decrease in engagement during summer months** with subsequent increase starting from the end of August, and **sharp fall in engagement during holidays: Thanksgiving day and Christmas**;\n- universal products that include Google services and YouTube were used much more intensively than digital learning products for students of all ages, not speaking about corporate users' products;\n- products designed for all students experienced less fluctuation in engagement in comparison with other categories;\n- we can notice a more or less **sharp decrease in the weekly engagement index on the 12th week, the next week after WHO declared COVID-19 a Pandemic and President Trump declared COVID-19 a National Emergency**;\n- the behavior of engagement index of products for university students and corporate users differs from the rest categories. I assume it might be related to the pandemic and high level of uncertainty for university students and those who were going to apply to colleges in 2020;\n- in the second half of the year engagement in products for children under 18 y.o. repeated the behavior of the universal products line quite closely.\n\nTo my mind, national holidays influence overall engagement in digital learning products more than crises like global pandemic or political events like presidential elections.\n\n**I want to point out that we cannot evaluate the influence of COVID-19 pandemic based solely on the data of 2020. To identify whether there was any impact of pandemic and to measure it we should analyze data for the several previous years and at least one academic year after the pandemic.**\n","ff4b9a6a":"### 1.2 Districts dataset","31fb2942":"**% of students receiving free \/ reduced lunches:** \n\nIt should be noted that the percentage of students in a school who are eligible for free or reduced-price lunch is an indicator of the amount of low-income students in a school according to [National Center for Education Statistics (NCES)](https:\/\/nces.ed.gov\/fastfacts\/display.asp?id=898). The higher the percentage of students who are eligible for free or reduced-price lunch the higher is the level of school poverty.\n\nI initially expected to see a strong correlation between level of engagement in digital learning and percentage of students receiving free lunches. As far as I know, it has been discussed by social scientists and teachers that transfer to online education might affect the poorest and most financially vulnerable families because they are initially in worse circumstances and have less opportunities to fully participate in the education process.\n\nNevertheless, in republican states the chart does not show a clear relationship between a social feature and level of engagement, the result is more or less the same in all categories. While **in democratic states in the first 4 categories from 0% to 80% of students with free\/reduced lunches we can observe negative correlation**, but the last category of 80-100% students breaks the tendency.\n\n**Interesting thing to notice:**\n\n- in the category 60-80% average engagement rates in democratic and republican districts are very close.\n","da15c5ca":"The last, but not least in this section is analysis of engagement in digital learning in different states and distribution of 4 location types in total engagement rate per state.\n\nI have splitted the chart into 2 parts because of the high variation in absolute numbers of average engagement per 1 day in 2020 across the states. So, the first chart demonstrates the top-8 states with the highest engagement in digital learning, and the second chart shows the rest of the states.\n\nI should mention that obviously the dataset with information about school districts has a lot of gaps. Arizona shows 100% of city population, while Florida - 100% of suburb population, which is far from reality. Nevertheless, let's see the picture based on available information."}}