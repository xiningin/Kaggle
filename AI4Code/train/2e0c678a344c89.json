{"cell_type":{"180863b9":"code","dfc9aeea":"code","a1204aab":"code","7bc5e2b0":"code","1c791985":"code","49e1f1c1":"code","bbe9f5f4":"code","14601770":"code","945e980e":"code","4edfb2bc":"code","e118fe1e":"code","6b113911":"code","58dd205e":"code","91381305":"code","70a27e2a":"code","92fed675":"markdown","22d2779b":"markdown","e37c4302":"markdown","2ea0a289":"markdown","ac7f43b3":"markdown","a2514b39":"markdown","6b144b11":"markdown","2c24189f":"markdown","64277159":"markdown","90bac242":"markdown"},"source":{"180863b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import train_test_split\n#constants\n\nIMG_HEIGHT=28\nIMG_WIDTH=28\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","dfc9aeea":"loaded_images=pd.read_csv('..\/input\/train.csv')\nloaded_images.head()","a1204aab":"images=loaded_images.iloc[:,1:]\nlabels=loaded_images.iloc[:,:1]   # for the labels to be a dataframe . iloc[:,0] returns a Series  iloc[:,:1] returns a Dataframe\nlabels.head()","7bc5e2b0":"train_images,test_images,train_labels,test_labels=train_test_split(images,labels,test_size=0.2,random_state=13)","1c791985":"train_images.describe()","49e1f1c1":"tree=DecisionTreeClassifier(criterion='gini',random_state=1)\ntree.fit(train_images,train_labels)","bbe9f5f4":"tree.score(train_images,train_labels.values.ravel())","14601770":"tree.score(test_images,test_labels.values.ravel())","945e980e":"figr,axes=plt.subplots(figsize=(10,10),ncols=3,nrows=3)\naxes=axes.flatten()\nfor i in range(0,9):\n    jj=np.random.randint(0,test_images.shape[0])          #pick a random image\n    axes[i].imshow(test_images.iloc[[jj]].values.reshape(IMG_HEIGHT,IMG_WIDTH))\n    axes[i].set_title('predicted: '+str(tree.predict(test_images.iloc[[jj]])[0]))\n\n\n","4edfb2bc":"new_data=pd.read_csv('..\/input\/test.csv')\nnew_data.head(n=3)","e118fe1e":"y_pred=tree.predict(new_data)","6b113911":"y_pred.shape","58dd205e":"submissions=pd.DataFrame({\"ImageId\":list(range(1,len(y_pred)+1)), \"Label\":y_pred})\nsubmissions.head()","91381305":"submissions.to_csv(\"mnist_decision_tree_submit.csv\",index=False,header=True)","70a27e2a":"!ls","92fed675":"**Loading the data**  \nusing pandas.read_csv to load in the training data. ","22d2779b":"Since out of all the features that are used to split a node, the one that maximizes the Information Gain is the one that the node is split  on.\n\n*IG is defined in terms of the impurity measure \"I\" which could be defined in terms of either entropy or the gini index. Each of these indices describes how pure a node is . So entropy of 1 implies a very impure node and an entropy of 0 implies that all members of the node belong to the same class i.e a pure node. similar with the gini index. Scaling is not an issue here since the impurity functions are defined in terms of probabilities which are values between 0 and 1*  \n\nTherefore features need not be scaled when dealing with DTs.  \n  \n**Classification using a Decision Tree**  \nusing the [DecisionTreeClassifier in sklearn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html)","e37c4302":"get both the train and the test scores.","2ea0a289":"In this notebook I  have  performed digit  classification using a  decision tree","ac7f43b3":"Split into images and labels.  ","a2514b39":"**Submission **  \n\nload the data in test.csv  \npredict using the model","6b144b11":"An 85% accuracy on the test data.\nSpot check to see if the predictions are correct . Plotting the predictions as labels.","2c24189f":"further split into training and test sets","64277159":"create a dataframe which will then be exported as a csv file for submissions.","90bac242":"possible overfitting on the training data."}}