{"cell_type":{"6aebfc48":"code","4ab74e25":"code","15dd253c":"code","be47551e":"code","1e37a34c":"code","103c4c09":"code","7ecb9a45":"code","69f1d48b":"code","b78fc4d7":"code","238f471b":"code","68755628":"code","30f54cf8":"code","1ba9eb6f":"code","f2182ec6":"code","dfc575e1":"code","bb90148f":"code","a2a0dca9":"code","8a3ca80d":"code","37119500":"code","11902dd4":"code","39a8fb3f":"code","90812eae":"code","0eb8e1cd":"code","86916cdc":"code","9607c321":"code","afaf8072":"code","2eb80275":"code","bd708dd1":"code","3f063266":"code","9b70d54e":"code","937bc336":"code","77c46faf":"code","07d97bd1":"code","cf534d4a":"code","08489e9d":"code","70f51bfe":"code","521acd0e":"code","565553bc":"code","cd9e7c08":"code","4613a724":"code","55971c96":"code","7234f901":"code","2062b550":"code","b15ac180":"code","68230009":"code","e2371579":"code","f1d030a9":"code","b5f96030":"code","0f5dd5ae":"code","0b514cf9":"code","de7ba7c2":"code","90c00fa4":"code","ad1002f7":"code","df3bd618":"code","491f7ef2":"code","e2afe367":"code","fa4cefc6":"code","74159655":"code","17eba210":"code","1695821e":"code","b6ba2094":"code","16871f8a":"code","65f3a3ca":"code","57d7d9fe":"markdown","9b428c58":"markdown","95460282":"markdown","20a7ce9d":"markdown","08b521ef":"markdown","660e4977":"markdown","8ab52661":"markdown","3bfc505b":"markdown","650fe56f":"markdown","626705f5":"markdown","2637e1e5":"markdown","fe753487":"markdown","b34b1340":"markdown","41fff426":"markdown","6a2824fd":"markdown","ac3cab02":"markdown","5090f081":"markdown","3ff0d7c5":"markdown","9e118426":"markdown","75f5f3ca":"markdown","3df9bab1":"markdown","f007ce1e":"markdown","e0b9a45f":"markdown","ae5d786e":"markdown","8fda9636":"markdown","69120546":"markdown","e790fa1c":"markdown","8842dc83":"markdown","038033de":"markdown","d23b3c42":"markdown","07be032f":"markdown","8ad3fdef":"markdown","eed1e538":"markdown","82850496":"markdown","04363dae":"markdown","0f8a29c7":"markdown","5270adb8":"markdown","fc8cfcc4":"markdown","114a3cce":"markdown","213638cd":"markdown","4fd274fc":"markdown","b351e04e":"markdown","95933494":"markdown","ff70ecaa":"markdown","6490eacc":"markdown","3fe32ab8":"markdown","f567624d":"markdown","35d47ef2":"markdown","65413d2a":"markdown","8c294b84":"markdown","9d951571":"markdown","faee682c":"markdown","db4dcaa0":"markdown","9b4e7868":"markdown","14e5e5a5":"markdown","c642355b":"markdown","7f615a27":"markdown","47b2c77b":"markdown","1872592b":"markdown","d5faf4b0":"markdown","63bb3322":"markdown","92bfb692":"markdown","39408d00":"markdown","dcd39546":"markdown","cea60a2a":"markdown","d7f8a108":"markdown","70df548e":"markdown","cada8cd3":"markdown","db20b7fd":"markdown","b0b3d2fa":"markdown","8031da81":"markdown","e702d433":"markdown","f3e935b7":"markdown","ef3387d6":"markdown","2a6e6395":"markdown","30d6c4b3":"markdown"},"source":{"6aebfc48":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4ab74e25":"%matplotlib inline    \n# To make data visualisations display in Jupyter Notebooks \n\nimport numpy as np    # linear algebra \nimport pandas as pd    # Data processing, Input & Output load    \nimport matplotlib.pyplot as plt    # Visualization & plotting\nimport datetime\n\nimport xgboost as xgb\nfrom sklearn.ensemble import GradientBoostingClassifier    # GBM algorithm\nfrom sklearn.ensemble import RandomForestClassifier    # Random Forest Algorithm\nfrom sklearn.linear_model import LogisticRegression    # Logistic Regression Algorithm\n\nfrom xgboost.sklearn import XGBClassifier    # Extreme Gradient Boosting\nfrom xgboost import plot_importance    # Plotting Important Variables\n\nimport joblib  #Joblib is a set of tools to provide lightweight pipelining in Python (Avoid computing twice the same thing)\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n                                    # GridSearchCV - Implements a \u201cfit\u201d and a \u201cscore\u201d method\n                                    # train_test_split - Split arrays or matrices into random train and test subsets\n                                    # cross_val_score - Evaluate a score by cross-validation     \n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import f1_score, precision_score, accuracy_score, roc_auc_score, recall_score, roc_curve\nfrom sklearn.metrics import make_scorer, confusion_matrix, classification_report   # Differnt metrics to evaluate the model\nimport pandas_profiling as pp    # simple and fast exploratory data analysis of a Pandas Dataframe\n\nimport warnings    # To avoid warning messages in the code run\nwarnings.filterwarnings('ignore')","15dd253c":"def plot_roc_auc_curve(y_train_actual, train_pred_prob, y_test_actual, test_pred_prob, *args):\n    '''\n    Generate train and test roc curve\n    '''\n      \n    AUC_Train = roc_auc_score(y_train_actual, train_pred_prob)\n    AUC_Test = roc_auc_score(y_test_actual, test_pred_prob)\n    \n    if len(args) == 0:\n        print(\"Train AUC = \", AUC_Train)\n        print(\"Test AUC = \", AUC_Test)\n        fpr_train, tpr_train, thresholds = roc_curve(y_train_actual, train_pred_prob)\n        fpr_test, tpr_test, thresholds = roc_curve(y_test_actual, test_pred_prob)\n        roc_plot(fpr_train, tpr_train, fpr_test, tpr_test)\n        \n    else:\n        AUC_Valid = roc_auc_score(args[0], args[1])\n        print(\"Train AUC = \", AUC_Train)\n        print(\"Test AUC = \", AUC_Test)\n        print(\"Validation AUC = \", AUC_Valid)\n        fpr_train, tpr_train, thresholds = roc_curve(y_train_actual, train_pred_prob)\n        fpr_test, tpr_test, thresholds = roc_curve(y_test_actual, test_pred_prob)\n        fpr_val, tpr_val, thresholds = roc_curve(args[0], args[1])\n        roc_plot(fpr_train, tpr_train, fpr_test, tpr_test, fpr_val, tpr_val)        ","be47551e":"def roc_plot(fpr_train, tpr_train, fpr_test, tpr_test, *args):\n    '''\n    Generate roc plot\n    '''\n    \n    fig = plt.plot(fpr_train, tpr_train, label = 'Train')\n    fig = plt.plot(fpr_test, tpr_test, label = 'Test')\n    \n    if len(args) == 0:\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.0])\n        plt.title(\"ROC curve using \")\n        plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n        plt.ylabel(\"True Positive Rate (Sensitivity)\")\n        plt.legend(loc = 'lower right')\n        plt.grid(True)\n        plt.show()\n    \n    else:\n        fig = plt.plot(args[0], args[1], label = 'Validation')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.0])\n        plt.title(\"ROC curve using \")\n        plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n        plt.ylabel(\"True Positive Rate (Sensitivity)\")\n        plt.legend(loc = 'lower right')\n        plt.grid(True)\n        plt.show()","1e37a34c":"data = pd.read_csv('..\/input\/hepatitis-c-dataset\/HepatitisCdata.csv')\n\n# Copying the original data into a new python variable object data_new\ndata_new = data.copy()\n\nprint(\"Data Shape - \", data_new.shape)\n\ndata_new.head()","103c4c09":"data_new.describe()","7ecb9a45":"data_new.describe(include = np.object)","69f1d48b":"data_new.info()","b78fc4d7":"pp.ProfileReport(data_new)","238f471b":"data_new = data_new.drop('Unnamed: 0', axis = 1)","68755628":"data_new['Category'].loc[data_new['Category'].isin(['0=Blood Donor', '0s=suspect Blood Donor'])] = 0\ndata_new['Category'].loc[data_new['Category'].isin(['1=Hepatitis', '2=Fibrosis', '3=Cirrhosis'])] = 1","30f54cf8":"data_new['Category'] = data_new['Category'].astype(int)","1ba9eb6f":"Target = 'Category'\npd.crosstab(data_new[Target], columns = 'Normalized', normalize = True)","f2182ec6":"data_new.isnull().sum()","dfc575e1":"pp.ProfileReport(data_new)","bb90148f":"print(\"Unique values Sex count: \", data_new['Sex'].nunique())\nprint(\"Sex values: \", data_new['Sex'].unique())\npd.value_counts(data_new['Sex'])","a2a0dca9":"print(\"Unique values Category count: \", data_new['Category'].nunique())\nprint(\"Category values: \", data_new['Category'].unique())\npd.value_counts(data_new['Category'])","8a3ca80d":"plt.figure(figsize = (10, 8))\nplt.pie(pd.value_counts(data_new['Sex']), \n        labels = ['m', 'f'],\n        autopct = '%.2f%%',\n        textprops = {'size' : 'x-large',\n                     'fontweight' : 'bold', \n                     'rotation' : '30',\n                     'color' : 'w'})\n\nplt.legend()\nplt.title('Percentage Of Sex Types', fontsize = 18, fontweight = 'bold')\nplt.show()","37119500":"plt.figure(figsize = (10, 8))\nplt.pie(pd.value_counts(data_new['Category']), \n        labels = [0, 1],\n        autopct = '%.2f%%',\n        textprops = {'size' : 'x-large',\n                     'fontweight' : 'bold', \n                     'rotation' : '30',\n                     'color' : 'w'})\n\nplt.legend()\nplt.title('Percentage Of Category Types', fontsize = 18, fontweight = 'bold')\nplt.show()","11902dd4":"num_cols = data_new.select_dtypes(include = [np.number]).columns.tolist()\nobj_cols = data_new.select_dtypes(exclude = [np.number]).columns.tolist()","39a8fb3f":"num_cols = data_new.drop(['Category'], axis = 1).select_dtypes(include = [np.number]).columns.tolist()","90812eae":"print('Numeric Columns \\n', num_cols)\nprint('Non-Numeric Columns \\n', obj_cols)","0eb8e1cd":"num_cols_viz = ['Age', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT']\n\nfig, axes = plt.subplots(1, 1, sharex = False, sharey = False, figsize = (15, 15))\ndata_new.loc[:, [Target]+num_cols_viz].boxplot(by = Target, ax = axes, return_type = 'axes');","86916cdc":"pd.crosstab(data_new['Sex'], data_new[Target]).plot(kind = 'bar', stacked = True, grid = False)","9607c321":"print(\"Missing Data Percentage: \", (31\/615)*100, \"%\")","afaf8072":"data_new = data_new.dropna(axis = 0)\ndata_new.head()","2eb80275":"encoding_list = ['Sex']\n\nlabel_encoding_list = []\none_hot_encoding_list = []\n\nfor i in range (0, len(encoding_list)):\n    if(len(data_new[f'{encoding_list[i]}'].unique()) == 2):\n        label_encoding_list.append(encoding_list[i])\n    else:\n        one_hot_encoding_list.append(encoding_list[i])\n        \n    print(f'Unique Values for {encoding_list[i]}', data_new[f'{encoding_list[i]}'].unique())","bd708dd1":"# Numerical columns data\ndata_new_num = data_new[num_cols + ['Category']]\n\n# Categorical columns data\ndata_new_cat = data_new[obj_cols]\n\n# Creating dummies\ndata_new_cat_dummies = pd.get_dummies(data_new_cat)\nprint(data_new_cat_dummies.shape)\ndata_new_cat_dummies.head()","3f063266":"data_new_final = pd.concat([data_new_num, data_new_cat_dummies], axis = 1)\nprint(data_new_final.shape)\ndata_new_final.head()","9b70d54e":"data_new_final.isnull().sum(axis = 0)","937bc336":"X = data_new_final.drop(['Category'], axis = 1)\ny = data_new_final['Category']","77c46faf":"X_train, X_test, y_train, y_test = tts(X, y, test_size = 0.3, random_state = 100) \n\nprint('Train Shape: ', X_train.shape)\nprint('Test Shape: ', X_test.shape)","07d97bd1":"model_parameters = {'n_estimators': [10, 50, 100, 200, 500, 750, 1000], 'max_depth': [3, 5, 10],\n                    'min_samples_leaf': [np.random.randint(1,10)], 'max_features': [None, 'sqrt', 'log2']}","cf534d4a":"model = GradientBoostingClassifier(random_state = 10)\ngscv_GBM = GridSearchCV(estimator = model, \n                        param_grid = model_parameters, \n                        cv = 5, \n                        verbose = 1, \n                        n_jobs = -1,\n                        scoring = 'roc_auc')\n\ngscv_GBM.fit(X_train, y_train)","08489e9d":"print('The best parameters are -', gscv_GBM.best_params_)","70f51bfe":"final_mod_GBM = GradientBoostingClassifier(**gscv_GBM.best_params_)\nfinal_mod_GBM.fit(X_train, y_train)","521acd0e":"train_pred = final_mod_GBM.predict(X_train)\ntest_pred = final_mod_GBM.predict(X_test)","565553bc":"print('Classification report for train data is : \\n',\n      classification_report(y_train, train_pred))\nprint('Classification report for test data is : \\n',\n      classification_report(y_test, test_pred))","cd9e7c08":"final_mod_GBM.variables = X_train.columns","4613a724":"joblib.dump(final_mod_GBM, 'best_model_GBM.joblib')","55971c96":"plt.subplots(figsize = (10, 5))\ntrain_prob = final_mod_GBM.predict_proba(X_train)[:, 1]\ntest_prob = final_mod_GBM.predict_proba(X_test)[:, 1]\n\nplot_roc_auc_curve(y_train, train_prob, y_test, test_prob)","7234f901":"y_pred = final_mod_GBM.predict(X_test)\npredictions = [round(value) for value in y_pred]","2062b550":"accuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","b15ac180":"log_reg = LogisticRegression(solver = 'liblinear')\nlog_reg.fit(X_train, y_train)","68230009":"train_pred = log_reg.predict(X_train)\ntest_pred = log_reg.predict(X_test)","e2371579":"print('Classification report for train data is : \\n',\n      classification_report(y_train, train_pred))\nprint('Classification report for test data is : \\n',\n      classification_report(y_test, test_pred))","f1d030a9":"log_reg.variables = X_train.columns","b5f96030":"joblib.dump(log_reg, 'best_model_log_reg.joblib')","0f5dd5ae":"plt.subplots(figsize = (10, 5))\ntrain_prob = log_reg.predict_proba(X_train)[:, 1]\ntest_prob = log_reg.predict_proba(X_test)[:, 1]\n\nplot_roc_auc_curve(y_train, train_prob, y_test, test_prob)","0b514cf9":"y_pred = log_reg.predict(X_test)\npredictions = [round(value) for value in y_pred]","de7ba7c2":"accuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","90c00fa4":"model_parameters = {'n_estimators': [10, 50, 100, 200, 500, 750, 1000], 'max_depth': [3, 5, 10],\n                    'min_samples_leaf': [np.random.randint(1,10)], 'max_features': [None, 'sqrt', 'log2']}","ad1002f7":"model = RandomForestClassifier(random_state = 10)\ngscv_randfor = GridSearchCV(estimator = model, \n                        param_grid = model_parameters, \n                        cv = 5, \n                        verbose = 1, \n                        n_jobs = -1,\n                        scoring = 'roc_auc')\n\ngscv_randfor.fit(X_train, y_train)","df3bd618":"print('The best parameters are -', gscv_randfor.best_params_)","491f7ef2":"final_mod_randfor = GradientBoostingClassifier(**gscv_randfor.best_params_)\nfinal_mod_randfor.fit(X_train, y_train)","e2afe367":"train_pred = final_mod_randfor.predict(X_train)\ntest_pred = final_mod_randfor.predict(X_test)","fa4cefc6":"print('Classification report for train data is : \\n',\n      classification_report(y_train, train_pred))\nprint('Classification report for test data is : \\n',\n      classification_report(y_test, test_pred))","74159655":"final_mod_randfor.variables = X_train.columns","17eba210":"joblib.dump(final_mod_randfor, 'best_model_randfor.joblib')","1695821e":"plt.subplots(figsize = (10, 5))\ntrain_prob = log_reg.predict_proba(X_train)[:, 1]\ntest_prob = log_reg.predict_proba(X_test)[:, 1]\n\nplot_roc_auc_curve(y_train, train_prob, y_test, test_prob)","b6ba2094":"y_pred = log_reg.predict(X_test)\npredictions = [round(value) for value in y_pred]","16871f8a":"accuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","65f3a3ca":"print('The best model is Gradient Boosting model')","57d7d9fe":"### d) Saving the best model","9b428c58":"* Let's check if there are any null variables in the <b>data_new<\/b> dataset.","95460282":"### g) Evaluating prediction accuracy for test data","20a7ce9d":"* We have approximately <b>88%<\/b> of <b>0's<\/b> and <b>12%<\/b> of <b>1's<\/b> in our data.","08b521ef":"### e) Model Evaluation","660e4977":"### g) Saving the best model","8ab52661":"### f) Saving the variables used in the model","3bfc505b":"### i) Making predictions for test data","650fe56f":"### b) Creating Dummy Variables","626705f5":"### c) Saving the variables used in the model","2637e1e5":"### b) Using GridSearch Cross Validation to find out the best parameters using L2 penalty","fe753487":"## 6.1)  Updated Data Profiling Report","b34b1340":"### Following are the insights gathered from the boxplots\n\n* <b>The \"ALP\" boxplot shows that lower the ALP, higher the chance of a person experiencing Hepatitis C<\/b>.\n* <b>The \"AST\" boxplot shows that higher the AST, higher the chance of a person experiencing Hepatitis C<\/b>.\n* <b>The \"CHE\" boxplot shows that higher the CHE, higher the chance of a person experiencing Hepatitis C<\/b>.\n* <b>The \"BIL\" boxplot shows that higher the BIL, higher the chance of a person experiencing Hepatitis C<\/b>.","41fff426":"### e) Displaying model prediction and classification report","6a2824fd":"### h) Model Evaluation","ac3cab02":"### a) Applying logistic regression","5090f081":"### a) Sex ","3ff0d7c5":"### b) Displaying model prediction and classification report","9e118426":"## 2. Analysis of each category of the numerical variables of num_cols dataframe w.r.t Target variable - Category.","75f5f3ca":"## 4. Let's Understand Our Data","3df9bab1":"### i) Making predictions for test data","f007ce1e":"* Let's drop the columns which we won't be using.","e0b9a45f":"### d) Refitting the model with best parameters","ae5d786e":"* We shall first drop the variable <b>Unnamed: 0<\/b> as it's just an id variable.","8fda9636":"### b) Category","69120546":"## 7.1) Creating Model Dataset","e790fa1c":"* From the Updated Data Profiling report, we got to know that,\n  1. Variable <b>ALB<\/b> has <b>1<\/b> missing value.\n  2. Variable <b>ALP<\/b> has <b>18<\/b> missing values.\n  3. Variable <b>ALT<\/b> has <b>1<\/b> missing value.\n  4. Variable <b>CHOL<\/b> has <b>10<\/b> missing values.\n  5. Variable <b>PROT<\/b> has <b>1<\/b> missing value.\n  6. So, overall there are <b>31<\/b> missing value records.","8842dc83":"### d) Refitting the model with best parameters","038033de":"### a) Define model parameters to be tuned","d23b3c42":"### f) Saving the variables used in the model","07be032f":"* Let's first plot the boxplot of each numerical variable w.r.t our target variable.","8ad3fdef":"### b) Using GridSearch Cross Validation to find out the best parameters using L2 penalty","eed1e538":"## 8.1) Model 1 - GBM (Gradient Boosting)","82850496":"* Now we shall first do the <b>Univariate Analysis<\/b> by analysing the data w.r.t our <b>Target Variable - Category<\/b>.","04363dae":"## 6.2) Univariate Analysis ","0f8a29c7":"## 8.2) Model 2 - Logistic Regression","5270adb8":"### b) Performing Train, Test & Split","fc8cfcc4":"## 7. Feature Engineering","114a3cce":"## 1. Data Categorization","213638cd":"## 8.3) Model 3 - Random Forest Classifier","4fd274fc":"## 6. EDA(Exploratory Data Analysis)","b351e04e":"## a) Analysis of unique values & their counts for categorical variables of the data_new dataset.","95933494":"### Following are the insights gathered from the stacked bar charts\n\n* <b>Males are more prone to suffer from Hepatitis C as compared to females<\/b>.","ff70ecaa":"### g) Saving the best model","6490eacc":"### c) Concatenating columns - numeric and dummies","3fe32ab8":"## 7.3) Splitting the newly created model data into train and test data","f567624d":"## 6.4) Missing Value Treatment","35d47ef2":"## 3. Importing Dataset","65413d2a":"* Let's find out how much percentage of data is missing.","8c294b84":"### j) Evaluating prediction accuracy for test data","9d951571":"### d) Null value check in the final dataset before model run","faee682c":"* Converting the datatype of <b>Category<\/b> column from <b>object<\/b> to <b>int<\/b>.","db4dcaa0":"### a) Define model parameters to be tuned","9b4e7868":"## 5. Data Profiling Report","14e5e5a5":"* We would categorize the existing variables of our existing dataframe into <b>numerical<\/b> and <b>categorical<\/b> variables.","c642355b":"* We shall recategorize the categories of the variable <b>Category<\/b> for easy and simple identification of <b>Hepatitis C<\/b> categories.","7f615a27":"## 2. Defining Functions For Plotting ROC_AUC Curve & ROC_Plot","47b2c77b":"2. Now, let's get the summary for categorical data","1872592b":"## 3. Analysis of each category of the categorical variables of obj_cols dataframe w.r.t Target variable - Category.","d5faf4b0":"### c) Displaying the best parameters","63bb3322":"1. First, let's get the summary of the numerical data","92bfb692":"* As there's only 5% of the missing data, we can drop this missing data.","39408d00":"## 6.3) Bivariate Analysis","dcd39546":"### a) Separating the target variable - Category from the data_new_final dataframe","cea60a2a":"## 1. Importing Necessary Libraries","d7f8a108":"### c) Displaying the best parameters","70df548e":"## 8) Applying Different Models On Train & Test Data","cada8cd3":"### j) Evaluating prediction accuracy for test data","db20b7fd":"## b) Analysis of percentage unique values for categorical variables of the data_new dataset.","b0b3d2fa":"### Following are the insights gathered from the data_new dataframe\n\n1. <b>Maximum entries<\/b> are of <b>males<\/b> as compared to <b>females<\/b>.\n2. <b>12.20%<\/b> of the patients have chances to suffer from <b>Hepatitis C<\/b>.\n3. <b>87.80%<\/b> of the patients don't have any chances to suffer from <b>Hepatitis C<\/b>.","8031da81":"* The entire dataset contains <b>615<\/b> rows and <b>14<\/b> columns.","e702d433":"### a) Finding unique values of each object variable of data_new dataframe","f3e935b7":"## 9) Displaying Best Model","ef3387d6":"### f) Making predictions for test data","2a6e6395":"### e) Displaying model prediction and classification report","30d6c4b3":"### h) Model Evaluation"}}