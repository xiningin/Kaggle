{"cell_type":{"99e2ed68":"code","d4af712b":"code","3c5e1c33":"code","1214f4bb":"code","407be9d7":"code","6a161a3c":"code","3c84901d":"code","402a4835":"code","3832a12b":"code","2c0d76bb":"code","d1bd18a4":"code","a7a02a0b":"code","da6cc663":"code","27efa7ce":"code","d31f329a":"code","d168f753":"code","8cd38081":"code","f789943e":"code","db3a9fe6":"markdown","9b29c701":"markdown"},"source":{"99e2ed68":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4af712b":"!pip install ipython-autotime\n%load_ext autotime","3c5e1c33":"df = pd.read_csv('\/kaggle\/input\/topic-balaned-dataset\/topic_balanced_aug.csv')\ndf","1214f4bb":"df['topic'].value_counts() # checking data balance","407be9d7":"import string\nfrom gensim.parsing.preprocessing import remove_stopwords\n# remove stopwords and punctuation\ndef preprocess_sentence(sentence):\n    return remove_stopwords(sentence.lower()).translate(str.maketrans('', '', string.punctuation)).strip()","6a161a3c":"# testing the string preprocessing function\ndic = {'title': [\"Kareem-buggy sentence . , large full of punct?u@ations\", 'another given?.,*^% sentence! is am are']}\npd.DataFrame(dic)['title'].apply(preprocess_sentence)","3c84901d":"df['title'] = df['title'].apply(preprocess_sentence)\ndf['topic'] = df['topic'].apply(str.lower) \ndf.head()","402a4835":"import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\ndef cosine(u, v):\n    return np.dot(u, v) \/ (np.linalg.norm(u) * np.linalg.norm(v))","3832a12b":"sentences = df['title'].tolist()\n# Tokenization of each document\ntokenized_sentences = []\nfor s in sentences:\n    tokenized_sentences.append(word_tokenize(s.lower()))\ntokenized_sentences[0]","2c0d76bb":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument\ntagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sentences)]\ntagged_data[0:3]","d1bd18a4":"# ## Train doc2vec model\n# model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100, workers=4)\n\n# '''\n# vector_size = Dimensionality of the feature vectors.\n# window = The maximum distance between the current and predicted word within a sentence.\n# min_count = Ignores all words with total frequency lower than this.\n# alpha = The initial learning rate.\n# '''\n","a7a02a0b":"# model.save('topic_doc2vec.model')","da6cc663":"# load pretrained model made using cells above in some older notebook\nmodel = Doc2Vec.load('\/kaggle\/input\/news-topic-classification-doc2vec-model\/topic_doc2vec.model')","27efa7ce":"#transform training data\ntrain_vectors = []\nfor t in tokenized_sentences:\n    train_vectors.append(model.infer_vector(t))","d31f329a":"train_vectors[0:3]","d168f753":"embeddings = pd.DataFrame(train_vectors)\nembeddings['topic'] = df['topic']\nembeddings","8cd38081":"# save embeddings to be able to use them later\nembeddings.to_csv('topic_doc2vec_embeddings.csv', index=False, header=True)","f789943e":"from sklearn import linear_model, datasets\nlogreg = linear_model.LogisticRegression()\nlogreg.fit(train_vectors, df['topic'])\nprint(\n    \"Logistic Regression classification accuracy on training data:\\n\",\n    logreg.score(train_vectors,df['topic'])*100,\"%\")","db3a9fe6":"### Doc2vec embeddings","9b29c701":"### Data preprocessing"}}