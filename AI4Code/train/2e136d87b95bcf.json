{"cell_type":{"ed774ce4":"code","81b11b04":"code","2b19f885":"code","d69b9fbb":"code","017e1d2f":"code","ac800e0f":"code","8d2b7e4c":"code","884379f8":"code","90c0d64e":"code","7729dc8b":"code","4e3a372e":"code","ac1e06bf":"code","7409fe59":"code","60bee782":"markdown","3ae00b82":"markdown","9ceef910":"markdown","f2ffeebc":"markdown","cce06719":"markdown","b7536d38":"markdown","1539e098":"markdown","84219673":"markdown","40e0de3a":"markdown","6b061ebf":"markdown","9f9f2fc0":"markdown","4a4b6c4e":"markdown","97f82acf":"markdown","92190f5a":"markdown","d71548eb":"markdown","7710472c":"markdown"},"source":{"ed774ce4":"import os\nimport pandas as pd\n\ndata = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')\ndata.info()","81b11b04":"data.describe().round(2)","2b19f885":"data.isnull().sum()","d69b9fbb":"print(data.columns)","017e1d2f":"data.columns = (['serial_no','GRE','TOEFL','university_rating','SOP','LOR','CGPA','research','COA'])\ndata = data.drop('serial_no',axis=1)\ndata.head()","ac800e0f":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='darkgrid')\n\nplt.figure(figsize=(20,20))\n\nplt.subplot(4,2,1)\nsns.distplot(data['GRE'])\nplt.subplot(4,2,2)\nsns.distplot(data['TOEFL'])\nplt.subplot(4,2,3)\nsns.distplot(data['university_rating'], kde=False)\nplt.subplot(4,2,4)\nsns.distplot(data['SOP'], kde=False)\nplt.subplot(4,2,5)\nsns.distplot(data['LOR'], kde=False)\nplt.subplot(4,2,6)\nsns.distplot(data['CGPA'])\nplt.subplot(4,2,7)\nsns.distplot(data['research'], kde=False)\nplt.subplot(4,2,8)\nsns.distplot(data['COA'])\nplt.show()","8d2b7e4c":"con_feat = ['GRE','TOEFL','CGPA', 'COA']\nax = sns.pairplot(data[con_feat])","884379f8":"correlation_matrix = data.corr()\nf, x = plt.subplots(figsize=(10,10))\nsns.heatmap(correlation_matrix, vmax=1, annot=True)","90c0d64e":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfeatures = ['GRE','TOEFL','university_rating','SOP','LOR','CGPA','research']\nvif = pd.DataFrame()\nvif['Features'] = data[features].columns\nvif['VIF'] = [variance_inflation_factor(data[features].values, i) for i in range(data[features].shape[1])] \nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = 'VIF', ascending = False)\nvif","7729dc8b":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(0)\ndata_train, data_test = train_test_split(data, train_size=.8, test_size=.2, random_state=100)\ndata_train.head()","4e3a372e":"X_train = data_train[features] \ny_train = data_train['COA']\nX_test = data_test[features] \ny_test = data_test['COA']","ac1e06bf":"from sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\nevaluation = pd.DataFrame({'Model': [],\n                          'RMSE': [],\n                          '5-Fold Cross Validation': []})","7409fe59":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\n\nmodel1 = LinearRegression()\nmodel1.fit(X_train,y_train)\n\npredict1 = model1.predict(X_test)\nrmse1 = float(format(np.sqrt(metrics.mean_squared_error(y_test,predict1)),\n                   '.3f'))\ncv1 = float(format(cross_val_score(model1, data[features], data['COA'],\n                                 cv=5).mean(), '.3f'))\n\nmodel2 = RandomForestRegressor()\nmodel2.fit(X_train,y_train)\n\npredict2 = model2.predict(X_test)\nrmse2 = float(format(np.sqrt(metrics.mean_squared_error(y_test,predict2)),\n                   '.3f'))\ncv2 = float(format(cross_val_score(model2, data[features], data['COA'],\n                                 cv=5).mean(), '.3f'))\n\nmodel3 = RandomForestRegressor()\nmodel3.fit(X_train,y_train)\n\npredict3 = model3.predict(X_test)\nrmse3 = float(format(np.sqrt(metrics.mean_squared_error(y_test,predict3)),\n                   '.3f'))\ncv3 = float(format(cross_val_score(model3, data[features], data['COA'],\n                                 cv=5).mean(), '.3f'))\n\nmodel4 = RandomForestRegressor()\nmodel4.fit(X_train,y_train)\n\npredict4 = model4.predict(X_test)\nrmse4 = float(format(np.sqrt(metrics.mean_squared_error(y_test,predict4)),\n                   '.3f'))\ncv4 = float(format(cross_val_score(model4, data[features], data['COA'],\n                                 cv=5).mean(), '.3f'))\n\nr = evaluation.shape[0]\nevaluation.loc[r] = ['Multiple Regression', rmse1, cv1]\nevaluation.loc[r+1] = ['Random Forest', rmse2, cv2]\nevaluation.loc[r+2] = ['K-Neighbours', rmse3, cv3]\nevaluation.loc[r+3] = ['SVR', rmse4, cv4]\nevaluation.sort_values(by = '5-Fold Cross Validation', ascending = False)","60bee782":"<p>VIFs range from 1 upwards. The numerical value for VIF tells you (in decimal form) what percentage the variance (i.e. the standard error squared) is inflated for each coefficient. For example, a VIF of 1.9 tells you that the variance of a particular coefficient is 90% bigger than what you would expect if there was no multicollinearity \u2014 if there was no correlation with other predictors.<\/p>\n\n<p>A rule of thumb for interpreting the variance inflation factor:<\/p>\n\n- 1 = not correlated.\n- Between 1 and 5 = moderately correlated.\n- Greater than 5 = highly correlated.\n\nAs you can see the VIF result are rather high. So we can confirm that a strong multicollinearity occur within the data.  ","3ae00b82":"There are indeed a strong correlation between `CGPA`, `TOEFL`, and `GRE` (above 0.8). Second way to confirm the multicollinearity is to check the variance inflation factor. ","9ceef910":"I would begin to try to understanding the data by running the `.describe()` and `.info()` to get an idea what kind of data I will be handle.","f2ffeebc":"# VIF\n\nVariance Inflation Factor (VIF) estimates on how much the variance of a regression coefficient is inflated due to multicollinearity in the model. VIFs are calculated by taking a predictor, and regressing it against every other predictor in the model. This gives you the R-squared values, which can then be plugged into the VIF formula. \u201ci\u201d is the predictor you\u2019re looking at (e.g. x1 or x2):\n\n### $ VIF_i = \\frac{1}{1 - {R_i}^2} $","cce06719":"It would also useful to plot the relation between each variable.","b7536d38":"# Splitting and Training the Data","1539e098":"By the graphs above, we can tell that `CGPA` and `TOEFL` are correlate with each other and therefore should expect the mulicollinearity within the data. There are multiple ways to identify the multicollinearity. First, I would just dig into the correlation numbers between the features.  ","84219673":"I deciced to change the column names, and drop `Serial No.` for the simplicity-sake.","40e0de3a":"## Table of Content\n\n1. Step 1: Reading and Understanding the Data\n2. Step 2: Cleaning the Data\n3. Step 3: Data Visualization\n4. Step 4: Splitting and Training the Data\n5. Step 5: Building the Models and Evaluation\n6. Step 6: Conclusion","6b061ebf":"No missing data found. We can proceed to the next step. ","9f9f2fc0":"# Overview\n\nHowdy!\n\nWelcome to my kernel! In the kernel I would us the Graduation Admission data to predict the chances of admit value. It is a regressional problem, and there are multiple methods to adress it. I choose to perform couple of different machine learning model and will pick the the model with the best prediction accuracy.\n\nI hope you can take something from my kernel and please leave some feedback if you have any :). ","4a4b6c4e":"# Reading and Understanding the Data","97f82acf":"# Cleaning the Data","92190f5a":"# Conclusion\n\nIf we look at the evaluation table, **linear regression** results in the best RMSE. Do mind that involving the strong multicollinearity as a consideration of choosing the model, linear regression might not be the most suitable model as it will reduces the precision of the estimate coefficients, which weakens the statistical power of your regression model. You might not be able to trust the p-values to identify independent variables that are statistically significant.\n\nChoosing how to address the multicollinearity is just a matter of your primary goal of analysis. While multicollinearity strongly affects the statistical power of your model, it would not affects your accuracy of prediction. If you want to identify which feature is the best predictor, you might want to choose other-than-linear-regression model. If what you are looking for is a model with the best accuracy, then it is ok to use linear regression. For this case, I would just to choose **multipe regression** since the goal is only to generate a model with the lowest RMSE.\n\n: It is my first machine learning learning model so please write a comment if you find any errors or valuable feedback, thank you!  ","d71548eb":"## Visualizing the Data\n\n<p>Firstly I just want to see the distribution for each variable. <\/p>","7710472c":"# Building the Models and Evaluation\n\nHere I would use 4 models for the regressional prediction (**Multiple Regression**, **Random Forest**, **KNN**, and **Suport Vector Regression**) and evaluate each of the prediction result. "}}