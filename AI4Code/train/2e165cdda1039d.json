{"cell_type":{"07ba8d1e":"code","087e5319":"code","6e9cbaef":"code","499ffad3":"code","7886f923":"code","0b2f623e":"code","4e1162b6":"code","19c43d3d":"code","59397875":"code","05eec15f":"code","b8791425":"code","401fe0de":"code","a8022cc9":"code","b0a6cf83":"code","070cb31a":"code","23753520":"code","2ca6cee8":"code","c14a8f89":"code","5b483e68":"code","0808bcb5":"code","21af12d7":"code","44c12eee":"code","b117db4e":"code","5cf21045":"code","6dec1cca":"code","f12caf5b":"markdown","ea8c5a39":"markdown","9705e325":"markdown","ff6e63ab":"markdown","9bfeff1c":"markdown","ad7edf05":"markdown","eb1ac2ba":"markdown","b1aeacca":"markdown"},"source":{"07ba8d1e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom datetime import date\nfrom datetime import time\nfrom datetime import datetime\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","087e5319":"# import the modules we'll need\nfrom IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"time_data.csv\"):  \n    csv = df.to_csv(index= False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)","6e9cbaef":"train_data = pd.read_csv(\"\/kaggle\/input\/train-data\/Train_SU63ISt.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/test-data\/Test_0qrQsBZ.csv\")\n\ntrain_original=train_data.copy() \ntest_original=test_data.copy()","499ffad3":"train_data.head()","7886f923":"train_data['Datetime'] = pd.to_datetime(train_data.Datetime,format='%d-%m-%Y %H:%M') \ntest_data['Datetime'] = pd.to_datetime(test_data.Datetime,format='%d-%m-%Y %H:%M') \ntest_original['Datetime'] = pd.to_datetime(test_original.Datetime,format='%d-%m-%Y %H:%M') \ntrain_original['Datetime'] = pd.to_datetime(train_original.Datetime,format='%d-%m-%Y %H:%M')","0b2f623e":"for i in (train_data, test_data, test_original, train_original):\n    i['year']=i.Datetime.dt.year \n    i['month']=i.Datetime.dt.month \n    i['day']=i.Datetime.dt.day\n    i['Hour']=i.Datetime.dt.hour ","4e1162b6":"train_data['dayOfWeek']=train_data['Datetime'].dt.dayofweek ","19c43d3d":"def applyer(row):\n    if row.dayofweek == 5 or row.dayofweek == 6:\n        return 1\n    else:\n        return 0 \ntemp2 = train_data['Datetime'].apply(applyer) \ntrain_data['weekend']=temp2","59397875":"train_data.index = train_data['Datetime'] # indexing the Datetime to get the time period on the x-axis. \ndf=train_data.drop('ID',1)           # drop ID variable to get only the Datetime on x-axis. \nts = df['Count'] \nplt.figure(figsize=(16,8)) \nplt.plot(ts, label='Passenger Count') \nplt.title('Time Series') \nplt.xlabel(\"Time(year-month)\") \nplt.ylabel(\"Passenger count\") \nplt.legend(loc='best')","05eec15f":"train_data.groupby(\"year\")['Count'].mean().plot.bar()","b8791425":"train_data.groupby(\"month\")['Count'].mean().plot.bar()","401fe0de":"train_data.groupby(\"day\")['Count'].mean().plot.bar()","a8022cc9":"train_data.groupby(\"Hour\")['Count'].mean().plot.bar()","b0a6cf83":"train_data.groupby(\"weekend\")['Count'].mean().plot.bar()","070cb31a":"Train=train_data.loc['2012-08-25':'2014-06-24'] \nvalid=train_data.loc['2014-06-25':'2014-09-25']","23753520":"def submission(model):\n    predict=model.forecast(len(test_data))\n    submission = test_data[[\"ID\"]]\n    submission[\"Count\"] = predict \n    return submission","2ca6cee8":"from sklearn.metrics import mean_squared_error\n\ndf = np.asarray(Train.Count)\ny_pred = valid.copy()\ny_pred['naive'] = df[len(df)-1]\n\nrmse = np.sqrt(mean_squared_error(valid['Count'],y_pred['naive']))\nprint(rmse)","c14a8f89":"plt.figure(figsize=(12,8)) \nplt.plot(Train.index,Train['Count'],label = \"Train\")\nplt.plot(valid.index,valid['Count'],label = \"Valid\")\nplt.plot(y_pred.index,y_pred['naive'],label = \"Naive\")\nplt.legend(loc='best') \nplt.title(\"Naive Forecast\")\nplt.show()","5b483e68":"y_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(5).mean().iloc[-1]\n\nplt.figure(figsize=(12,8)) \nplt.plot(Train.index,Train['Count'],label = \"Train\")\nplt.plot(valid.index,valid['Count'],label = \"Valid\")\nplt.plot(y_hat_avg.index,y_hat_avg['moving_avg_forecast'],label = \"average\")\nplt.legend(loc='best') \nplt.title(\"Moving Averages\")\nplt.show()\n\nrmse = np.sqrt(mean_squared_error(valid['Count'],y_hat_avg['moving_avg_forecast']))\nprint(rmse)","0808bcb5":"from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\nmodel = SimpleExpSmoothing(np.asarray(Train['Count'])).fit(smoothing_level=0.6,optimized=False)\n\npred1 = valid.copy()\npred1[\"SES\"] = model.forecast(len(valid))\n\nplt.figure(figsize=(12,8)) \nplt.plot(Train.index,Train['Count'],label = \"Train\")\nplt.plot(valid.index,valid['Count'],label = \"Valid\")\nplt.plot(pred1.index,pred1[\"SES\"],label = \"smothing\")\nplt.legend(loc='best') \nplt.title(\"Moving Averages\")\nplt.show()\n\nrmse = np.sqrt(mean_squared_error(valid['Count'],pred1['SES']))\nprint(rmse)","21af12d7":"fit2 = Holt(np.asarray(Train['Count'])).fit()\ny_holt = valid.copy()\ny_holt[\"Holt\"] = fit2.forecast(len(valid))\n\nplt.figure(figsize=(12,8)) \nplt.plot(Train.index,Train['Count'],label = \"Train\")\nplt.plot(valid.index,valid['Count'],label = \"Valid\")\nplt.plot(y_holt.index,y_holt[\"Holt\"],label = \"Holt\")\nplt.legend(loc='best') \nplt.title(\"Moving Averages\")\nplt.show()\n\nrmse = np.sqrt(mean_squared_error(valid['Count'],y_holt['Holt']))\nprint(rmse)","44c12eee":"#Submission\n\ndf =submission(fit2)\npredict=fit2.forecast(len(test_data))\ntest_data['prediction']=predict\n\ncreate_download_link(df)","b117db4e":"train_original['ratio'] = train_original['Count']\/train_original['Count'].sum() \n\ntemp=train_original.groupby(['Hour'])['ratio'].sum()\npd.DataFrame(temp, columns=['Hour','ratio']).to_csv('GROUPby.csv') \n\ntemp2=pd.read_csv(\"GROUPby.csv\") \ntemp2=temp2.drop('Hour.1',1) ","5cf21045":"# Predicting by merging merge and temp2 \nprediction=pd.merge(test_data, temp2, on='Hour', how='left') \n\nprediction['Count']=prediction['prediction']*prediction['ratio']*24 \nsubmission=prediction.drop(['Datetime', 'year','month','day','prediction','Hour', 'ratio'],axis=1) \ncreate_download_link(submission)\n","6dec1cca":"y_hat_avg = valid.copy() \nexpSm = ExponentialSmoothing(np.asarray(Train['Count']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit() \ny_hat_avg['Holt_Winter'] = expSm.forecast(len(valid)) \n\nplt.figure(figsize=(16,8)) \nplt.plot(Train['Count'],label = 'Train')\nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')\nplt.show()\n\nrms = np.sqrt(mean_squared_error(valid.Count, y_hat_avg.Holt_Winter)) \nprint(rms)","f12caf5b":"Holt's Linear Trend on daily Time Series","ea8c5a39":"Data Visualization","9705e325":"Holt's Winter Trend","ff6e63ab":"Holt\u2019s Linear Trend Model","9bfeff1c":"Model 3 : Simple Exponential Smoothing\n","ad7edf05":"Model 1 : Using Naive Approach","eb1ac2ba":"**MODEL DEVELOPMENT**\n\nBefore the development we will split the data into training, testing and validation data sets. In time series splitting the data is not done randomly but a certain time period is taken for validation and training data sets. In this case I am taking last 3 months data as validation data set.","b1aeacca":"Model 2 : Moving Averages"}}