{"cell_type":{"bfd1b311":"code","b4da5332":"code","63e82296":"code","b376e1e1":"code","50aa61c3":"code","738008ab":"code","f4062927":"code","10322429":"code","676a368f":"code","75942b4a":"code","4cd8aa78":"code","86713451":"code","de771b6a":"code","0dd1a5fa":"code","b38b9da7":"code","c8713d6f":"code","b6601513":"code","cf92059f":"code","a1a6aead":"code","d5f0f113":"code","852bd680":"code","041ae646":"code","b66673a4":"code","527390fc":"code","1929df7d":"markdown","142385e9":"markdown","66a9b5e6":"markdown","0487d8c4":"markdown","3a40eba0":"markdown","4ce956b1":"markdown","729269eb":"markdown","bd1b4a3a":"markdown","303b701c":"markdown"},"source":{"bfd1b311":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.preprocessing import OrdinalEncoder \nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport os\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly\nimport plotly.express as px\nfrom numpy import mean\nfrom numpy import std\nfrom numpy import absolute\nfrom pandas import read_csv\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nroot = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/'\n\n# Any results you write to the current directory are saved as output.","b4da5332":"df_price = pd.read_csv(\n    os.path.join(root, 'train.csv'))   \nprice_df = df_price.copy()\nprice_df.shape","63e82296":"df_price['SalePrice'].describe()","b376e1e1":"#Understading sales prices\nsns.distplot( df_price['SalePrice'], color=\"skyblue\")\nprint(\"Skewness: %f\" % df_price['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_price['SalePrice'].kurt())\nprint(\"Mean: %f\" % df_price['SalePrice'].mean())","50aa61c3":"#Finding NaN values\nprice_df.describe()","738008ab":"price_df.columns","f4062927":"#thanks to https:\/\/www.kaggle.com\/suprematism\/advanced-feature-engineering-utility-functions\n\nnumeric_columns = ['LotArea', 'BsmtFinSF1', \n            'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n            '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', \n            'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch', 'MiscVal', \n            '3SsnPorch' , 'PoolArea' , 'LowQualFinSF']","10322429":"#Function to calculate k-means clusters\ndef find_kmeans_cluster(price_df, numeric_columns, max_cluster = 10):\n    \n    #Scaling columns to bring all data to same numeric scale\n    scaler = StandardScaler()\n    scaled_df = scaler.fit_transform(price_df[numeric_columns])\n    \n    #Calcuating sum of suquared error for each feature\n    squared_err = []\n    for num in range(1, max_cluster):\n        kmeans = KMeans(n_clusters = num, init = 'k-means++', random_state = 64)\n        kmeans.fit(scaled_df)\n        squared_err.append(kmeans.inertia_)\n    df_res = pd.DataFrame({'Num_of_cluster': range(1, max_cluster), 'squared_err': squared_err})\n    return(df_res)","676a368f":"#Plotting error graph - result shows that squred error decreases significanlty for 13 clusters\ndf_res = find_kmeans_cluster(price_df, numeric_columns, 20)\nsns.set_style('darkgrid')\nsns.lineplot(x = df_res['Num_of_cluster'],  y = df_res['squared_err'],  markers=True )","75942b4a":"#Dropping irrelavant columns and NaN values\nprice_df.drop(columns = ['LotFrontage', 'Alley', 'MasVnrArea', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n                         'BsmtFinType1','FireplaceQu', 'BsmtFinType2',  'GarageType', 'GarageYrBlt', 'GarageFinish', \n                         'GarageArea','TotalBsmtSF','YearRemodAdd', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature'], inplace= True)\n\n#Removing the NaN of the Electrical from the dataset\nprice_df.drop(price_df[price_df['Electrical'].isnull()].index, inplace= True)\n\n","4cd8aa78":"import seaborn as sns\ncorr = price_df.corr(method = 'pearson')\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr, vmax=1, square=True);\ncorr = corr.loc['SalePrice':, :].T\ncorr = corr[(corr['SalePrice'] >0.5) | (corr['SalePrice'] < -0.5)]\ncorr","86713451":"#Comparing a variable with the SalePrice\nprice_df.drop(price_df[price_df['GrLivArea'] > 4000].index, inplace = True)\n#price_df.drop(price_df[price_df['TotalBsmtSF'] > 3000].index, inplace = True)\nprice_df.drop(price_df[price_df['1stFlrSF'] > 2800].index, inplace = True)\n\n# Uncomment below to plot the graph\n# fig = make_subplots(rows=5, cols=2, start_cell=\"top-left\")\n\n# for i, var in enumerate(corr.index):\n#     if not var == 'SalePrice':\n#         i = i+1\n#         m = 1 if i%2==1 else 2\n#         k = int(i\/2) + 2 - m\n#         fig.add_trace(go.Scatter(x = price_df[var].values, y = price_df['SalePrice'].values, mode = 'markers'), row=k, col=m)\n#         fig.update_xaxes(title_text= var, row=k, col=m)\n# fig.update_layout(height=1200, width=800, title_text=\"Correlation with SalesPrice\")\n# fig.show()\n","de771b6a":"#Distinguishing numerical and categorical data\nfrom pandas.api.types import is_numeric_dtype\nimport category_encoders as ce\nfrom sklearn.preprocessing import LabelEncoder\n\nnum = []\ncateg = []\nfor row in price_df.columns: \n    numeric = is_numeric_dtype(price_df[row])\n    if row not in num and numeric == False:\n        categ.append(row)\n    elif row not in categ and numeric == True:\n        num.append(row)\n    \n#Identifying non-correlated categorical data\n# Uncomment below to plot the graph\n# for row in categ:\n#     ce_ord = ce.OrdinalEncoder(cols=[row])\n#     price_df[row] = ce_ord.fit_transform(price_df[row], price_df['SalePrice'])\n# for row in categ:\n#     var = row\n#     plt.figure()\n#     plt.scatter(price_df[var],price_df['SalePrice'])\n#     plt.xlabel(var), plt.ylabel('SalePrice')\n# corr_all = price_df.corr(method = 'pearson')\n# corr_all = corr_all.loc['SalePrice':, :].T\n# corr_all = corr_all[(corr_all['SalePrice'] >0.5) | (corr_all['SalePrice'] < -0.5)]\n# corr_all\n\n# corr1 = corr_all.merge(corr, left_index = True, right_index = True, how= 'outer')\n# corr1\n\n#no categorical data is strongly correlated to the SalePrice\n","0dd1a5fa":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\ntrain_df = pd.DataFrame(price_df, columns = corr.index)\nSalePrice = train_df.pop('SalePrice')\nX_train, X_test, y_train, y_test = train_test_split(train_df, SalePrice, random_state=0)\n\n\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\nprint('R2 value for linear regression train case {}'.format(r2_score(y_train, y_pred_train)))\nprint('R2 value for linear regression test case {}'.format(r2_score(y_test, y_pred_test)))\nprint('MSE value for linear regression test case {}'.format(mean_absolute_error(y_test, y_pred_test)))\n\nmodel_poly = LinearRegression()\npolynomial_features= PolynomialFeatures(degree=2)\nx_poly_train = polynomial_features.fit_transform(X_train)\nx_poly_test = polynomial_features.fit_transform(X_test)\nmodel_poly.fit(x_poly_train,y_train)\ny_pred_train = model_poly.predict(x_poly_train)\ny_pred_test = model_poly.predict(x_poly_test)\nprint('R2 value for polynomial regression train case {}'.format(r2_score(y_train, y_pred_train)))\nprint('R2 value for polynomial regression test case {}'.format(r2_score(y_test, y_pred_test)))\nprint('MSE value for polynomial regression test case {}'.format(mean_absolute_error(y_test, y_pred_test)))","b38b9da7":"#Using regularization to reduce the overfitiing\n# define model\nmodel = Ridge(alpha=20.0)\n# define model evaluation method\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores = cross_val_score(model, X_train, y_train, scoring='r2', cv=cv, n_jobs=1)\n# force scores to be positive\nscores = absolute(scores)\nprint('Mean R2: {} and {}'.format(mean(scores), std(scores)))","c8713d6f":"#fiting the model on the training data\nmodel.fit(X_train,y_train)\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\nprint('R2 value for linear regression train case {}'.format(r2_score(y_train, y_pred_train)))\nprint('R2 value for linear regression test case {}'.format(r2_score(y_test, y_pred_test)))\nprint('MSE value for linear regression test case {}'.format(mean_absolute_error(y_test, y_pred_test)))","b6601513":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Define model\nmelbourne_model = DecisionTreeRegressor(min_samples_split = 20, max_features = 'auto', criterion = 'mse')\n\n# define model evaluation method\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n# evaluate model\nscores = cross_val_score(melbourne_model, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=1)\n\n# force scores to be positive\nscores = absolute(scores)\nprint('Mean MSE: {} and {}'.format(mean(scores), std(scores)))\n","cf92059f":"#fiting the model on the training data\nmelbourne_model.fit(X_train,y_train)\ny_pred_train = melbourne_model.predict(X_train)\ndf_test = pd.read_csv(\n    os.path.join(root, 'test.csv'))  \ndf_test = df_test[['OverallQual','YearBuilt','1stFlrSF','GrLivArea','FullBath','TotRmsAbvGrd','GarageCars']]\ndf_test['GarageCars']  = df_test['GarageCars'].fillna(df_test['GarageCars'].mode()[0])\ny_pred_test = melbourne_model.predict(df_test)\ny_result = pd.DataFrame({'Id' : df_test.index, 'SalePrice' : y_pred_test})\ny_result.to_csv('submission.csv', index = False)\n","a1a6aead":"#Prediction\nparam_df = pd.DataFrame([{'OverallQual' : 7 , 'YearBuilt' : 2005, '1stFlrSF':850,\n       'GrLivArea' : 1600, 'FullBath': 2 , 'TotRmsAbvGrd': 6 , 'GarageCars' : 2}])\n\nprint(melbourne_model.predict(param_df))","d5f0f113":"from keras.models import Sequential\nfrom keras.layers import Dense , Dropout , Lambda, Flatten\nfrom keras.optimizers import Adam ,RMSprop\nfrom keras.callbacks import EarlyStopping","852bd680":"# setting random seed\nseed = 43\nnp.random.seed(seed)","041ae646":"model= Sequential()\nmodel.add(Dense(10, activation='sigmoid'))","b66673a4":"model.compile(optimizer=RMSprop(lr=0.001),\nloss='categorical_crossentropy',\nmetrics=['r2'])","527390fc":"X_train = np.array\nbatches = model.fit(X_train, y_train, batch_size=64)\ntest_batches=model.predict(X_test, y_test, batch_size=64)","1929df7d":"***Decision Tree Regressor***","142385e9":"**K-means clustering to identify dependent variable**\n\nFinding optimal number of clusters are challenging, hence, I used elbow curve to determine the no. of clusters. ","66a9b5e6":"***Cleaning Data***","0487d8c4":"***Linear regression with Regularization***","3a40eba0":"Cleaning the data, data with outliers are removed after analyzing the variable plot with the salesprice.","4ce956b1":"**Neural Networks**","729269eb":"Dealing with Categorical data","bd1b4a3a":"Finding correlations between  feature and result","303b701c":"***linear and polynomial regression***"}}