{"cell_type":{"4a990736":"code","d39f9daf":"code","66fd626e":"code","8cb51c0f":"code","ec29323c":"code","40a2b1d7":"code","8381f6a6":"code","21230b04":"code","ec4b658a":"code","264e0413":"code","b4c041e6":"code","5be63d96":"code","875f3a13":"code","efd328dc":"code","27aa04f9":"code","3f0622da":"code","d0c5c8a9":"code","5b23b431":"code","7963f7a2":"code","4e62799b":"code","d282fc39":"code","b00e6b15":"code","5d5e142c":"code","211f6a6d":"code","ca8a4d9e":"code","cec581a1":"code","f289ad43":"code","2c54c673":"code","264276fb":"code","76e245fe":"code","2d74af86":"code","02cb5d61":"code","9569b06b":"code","108b0849":"code","7927203b":"code","9fd36e0e":"code","5bd30152":"code","56341750":"code","7e4a8163":"code","967d2136":"code","7cbc69c4":"code","838ba43e":"code","cb6012c3":"code","1f1d912f":"code","3125dc30":"code","b553f9b4":"code","1d45c6b0":"markdown","15995c37":"markdown","ef3e1192":"markdown","f58c9fbf":"markdown","4d864ceb":"markdown","c5d52336":"markdown","2bdb8a30":"markdown","2f33dacb":"markdown","a16d3c8e":"markdown","0e208c98":"markdown","98ea06f4":"markdown","9e8c3826":"markdown","8617ee06":"markdown","88b5d584":"markdown","34dbb3ae":"markdown","9d2f33c0":"markdown","31b6d5a6":"markdown","a9e77648":"markdown","dd3593cb":"markdown"},"source":{"4a990736":"!pip install mtcnn","d39f9daf":"# Imports\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom PIL import Image\n\n# Confirm mtcnn was installed correctly\nimport mtcnn\nfrom mtcnn.mtcnn import MTCNN\nfrom matplotlib.patches import Rectangle\n\nfrom os import listdir\nfrom tqdm import tqdm\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns","66fd626e":"DIRECTORY = \"..\/input\/yale-face-database\/\"","8cb51c0f":"filename = \"..\/input\/yale-face-database\/subject01.centerlight\"\npixels = plt.imread(filename)\n\nrgb_pixels = np.stack((pixels, pixels, pixels), axis=2)\nprint(rgb_pixels.shape)\nplt.imshow(pixels)\nplt.show()","ec29323c":"# Create the detector, using default weights\ndetector = MTCNN()\n# detect faces in the image\nresults = detector.detect_faces(rgb_pixels)\nresults","40a2b1d7":"# draw an image with detected objects\ndef draw_image_with_boxes(data, result_list):\n    # plot the image\n    plt.imshow(data)\n    # get the context for drawing boxes\n    ax = plt.gca()\n    # plot each box\n    for result in result_list:\n        # get coordinates\n        x, y, width, height = result['box']\n        # create the shape\n        rect = Rectangle((x, y), width, height, fill=False, color='red')\n        # draw the box\n        ax.add_patch(rect)\n    # show the plot\n    plt.show()\n\n# display faces on the original image\ndraw_image_with_boxes(rgb_pixels, results)","8381f6a6":"# extract a single face from a given photograph\ndef extract_face_from_file(filename, required_size=(160, 160)):\n    # load image from file\n    image = Image.open(filename)\n    \n    return extract_face(image, required_size)\n\ndef extract_face(image, required_size=(160, 160)):\n    # convert to RGB, if needed\n    image = image.convert('RGB')\n    # convert to array\n    pixels = np.asarray(image)\n    # detect faces in the image\n    results = detector.detect_faces(pixels)\n    # extract the bounding box from the first face\n    x1, y1, width, height = results[0]['box']\n    # bug fix\n    x1, y1 = abs(x1), abs(y1)\n    x2, y2 = x1 + width, y1 + height\n    # extract the face\n    face = pixels[y1:y2, x1:x2]\n    # resize pixels to the model size\n    image = Image.fromarray(face)\n    image = image.resize(required_size)\n    face_array = np.asarray(image)\n    gray_face = cv2.cvtColor(face_array, cv2.COLOR_BGR2GRAY)\n    \n    return gray_face\n\n\n# Create the detector, using default weights\ndetector = MTCNN()\n\n# load the photo and extract the face\nface_pixels = extract_face_from_file(\"..\/input\/yale-face-database\/subject01.centerlight\")\n\nplt.imshow(face_pixels)","21230b04":"def list_files(directory, contains):\n    return list(f for f in listdir(directory) if contains in f)","ec4b658a":"i = 1\nfaces = list()\nfor filename in tqdm(list_files(DIRECTORY, \"subject\")[0:16]):\n    # path\n    path = DIRECTORY + filename\n    # get face\n    face = extract_face_from_file(path)\n    # plot\n    plt.subplot(4, 4, i)\n    plt.axis('off')\n    plt.imshow(face)\n    faces.append(face)\n    i += 1\nplt.show()","264e0413":"# list filenames\nfilenames = pd.DataFrame(list_files(DIRECTORY, \"subject\"))\n\n# generate split \ndf = filenames[0].str.split(\".\", expand=True)\ndf[\"filename\"] = filenames\n\n# # tidy columns\ndf = df.rename(columns = {0:\"subject\", 1:\"category\"})\ndf['subject'] = df.subject.str.replace('subject' , '')\ndf.apply(pd.to_numeric, errors='coerce').dropna()\ndf['subject'] = pd.to_numeric(df[\"subject\"])\ndf","b4c041e6":"# Print classnames\ndf['subject'].unique()","5be63d96":"PER_CLASS = 8 # 11 images (3 test & 8 train)\nNO_CLASSES = 15\nDS_SIZE = df[\"subject\"].count()\nTEST_SIZE = 1 - (PER_CLASS * NO_CLASSES \/ DS_SIZE)\n\n# # list files for each group\n# # df.groupby(['subject'])['filename'].apply(list)\ny = df['subject']\nX = df.drop('subject',axis=1)\n\n# # subject\nX_train_info, X_test_info, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=45, stratify=y)\n\ny_train = y_train.tolist()\ny_test = y_test.tolist()","875f3a13":"detector = MTCNN()\n\ndef load_dataset(dataset):\n    faces = list()\n    for filename in tqdm(dataset[\"filename\"]):\n        path = DIRECTORY + filename\n        # get face\n        face = extract_face_from_file(path)\n        faces.append(face)\n    return np.asarray(faces)","efd328dc":"X_test = load_dataset(X_test_info)\nX_train = load_dataset(X_train_info)\n\nprint(X_test.shape)\nprint(X_train.shape)","27aa04f9":"# develop a classifier for faces\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport random","3f0622da":"# Normalize input vectors\nin_encoder = Normalizer(norm='l2')\n\nX_train_reshaped = X_train.reshape(X_train.shape[0],X_train.shape[1]*X_train.shape[2])\nprint('Reshaped X_train', X_train_reshaped.shape)\nX_train_reshaped = in_encoder.transform(X_train_reshaped)\n\nX_test_reshaped = X_test.reshape(X_test.shape[0],X_test.shape[1]*X_test.shape[2])\nprint('Reshaped X_test', X_test_reshaped.shape)\nX_test_reshaped = in_encoder.transform(X_test_reshaped)\n\n# Label encode targets\nout_encoder = LabelEncoder()\nout_encoder.fit(y_train)\n\n# Fit model\nmodel = SVC(kernel='linear', probability=True)\nmodel.fit(X_train_reshaped, y_train)\n\n# Predict\nyhat_train = model.predict(X_train_reshaped)\nyhat_test = model.predict(X_test_reshaped)\n\n# Score\nscore_train = accuracy_score(y_train, yhat_train)\nscore_test = accuracy_score(y_test, yhat_test)\n# Summarize\nprint('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))","d0c5c8a9":"svm_predictions = model.predict(X_test_reshaped)\nprint(classification_report(y_test,svm_predictions))\n\n# Display the confusion matrix:\n# [TP,FP]\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test,svm_predictions))","5b23b431":"# Test model on a random example from the test dataset\nselection = random.randint(1, X_test.shape[0])\nrandom_face_emb = X_test_reshaped[selection]\nrandom_face_class = y_test[selection]\nrandom_face_name = out_encoder.inverse_transform([random_face_class]) - 1\n\n# prediction for the face\nsamples = np.expand_dims(random_face_emb, axis=0)\nyhat_class = model.predict(samples)\nyhat_prob = model.predict_proba(samples)\n\n# get name\nclass_index = yhat_class[0]\n\nclass_probability = yhat_prob[0,class_index] * 100\npredict_names = out_encoder.inverse_transform(yhat_class) - 1\nprint('Predicted: %s (%.3f)' % (predict_names[0], class_probability))\nprint('Expected: %s' % random_face_name[0])\n\n# Show the image\nplt.imshow(X_train[selection])\ntitle = 'Predicted: %s (%.3f)' % (predict_names[0], class_probability)\nplt.title(title)\nplt.show()","7963f7a2":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train_reshaped, y_train)","4e62799b":"rf_predictions = rfc.predict(X_test_reshaped)\nprint(classification_report(y_test,rf_predictions))\n\n# Display the confusion matrix:\n# [TP,FP]\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test,rf_predictions))","d282fc39":"y_test","b00e6b15":"# Test model on a random example from the test dataset\nselection = random.randint(1, X_test.shape[0])\nrandom_face_emb = X_test_reshaped[selection]\nrandom_face_class = y_test[selection]\nrandom_face_name = out_encoder.inverse_transform([random_face_class]) - 1\n\n# Prediction for the face\nsamples = np.expand_dims(random_face_emb, axis=0)\nyhat_class = rfc.predict(samples)\nyhat_prob = rfc.predict_proba(samples)\n\n# Get name\nclass_index = yhat_class[0]\n\n# Calculate results\nclass_probability = yhat_prob[0,class_index] * 100\npredict_names = out_encoder.inverse_transform(yhat_class) - 1\nprint('Predicted: %s (%.3f)' % (predict_names[0], class_probability))\nprint('Expected: %s' % random_face_name[0])\n\n# Show the image\nplt.imshow(X_test[selection])\ntitle = 'Predicted: %s (%.3f)' % (predict_names[0], class_probability)\nplt.title(title)\nplt.show()","5d5e142c":"# Options \n\nTRAINING_DATA_DIRECTORY = \"data\/train\"\nTESTING_DATA_DIRECTORY = \"data\/test\"\nNUM_CLASSES = 15\nEPOCHS = 25\nBATCH_SIZE = 20\nNUMBER_OF_TRAINING_IMAGES = 120\nNUMBER_OF_TESTING_IMAGES = 45\nIMAGE_HEIGHT = 160\nIMAGE_WIDTH = 160","211f6a6d":"import os \n\ndef save_keras_dataset(setname, dataset, labels, per_class):\n    # combine labels and images to generate files\n    data = sorted(list(zip(labels, dataset)), key=lambda x: x[0])\n\n    # Save images\n    j = 0\n    for label, gray_img in tqdm(data):\n        j = (j% per_class) + 1\n        # Create directory\n        directory = f\"data\/{setname}\/class_{label}\/\"\n        if not os.path.exists(directory):\n                os.makedirs(directory)\n        cv2.imwrite(f\"{directory}class_{label}_{j}.png\",gray_img)","ca8a4d9e":"y_train","cec581a1":"# clear directory if it already exists\nimport shutil\nshutil.rmtree(r'data', ignore_errors=True)\n\n# Save datasets\nsave_keras_dataset(\"test\", X_test, y_test, 3)\nsave_keras_dataset(\"train\", X_train, y_train, 8)","f289ad43":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ndef data_generator():\n    return ImageDataGenerator(\n        rescale=1.\/255,\n        # horizontal_flip=True,\n#         fill_mode=\"nearest\",\n#         zoom_range=0.1,\n#         width_shift_range=0.1,\n#         height_shift_range=0.1,\n#         rotation_range=10,\n        # preprocessing_function=add_noise\n    )\n\ndef add_noise(img):\n    \"\"\"Add random noise to an image\"\"\"\n    VARIABILITY = 35\n    deviation = VARIABILITY*random.random()\n    noise = np.random.normal(0, deviation, img.shape)\n    img += noise\n    np.clip(img, 0., 255.)\n    return img","2c54c673":"# Setup Data Generators\ntraining_generator = data_generator().flow_from_directory(\n    TRAINING_DATA_DIRECTORY,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    color_mode='grayscale'\n)\n\ntesting_generator = data_generator().flow_from_directory(\n    TESTING_DATA_DIRECTORY,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    class_mode='categorical',\n    color_mode='grayscale'\n)\n\nvalidation_generator = data_generator().flow_from_directory(\n    TESTING_DATA_DIRECTORY,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    class_mode='categorical',\n    color_mode='grayscale',\n    shuffle=False # IMPORTANT: to ensure classes line up with batches\n)","264276fb":"sample_images = testing_generator.next()[0]\n\nf, xyarr = plt.subplots(3,3)\nxyarr[0,0].imshow(sample_images[0])\nxyarr[0,1].imshow(sample_images[1])\nxyarr[0,2].imshow(sample_images[2])\nxyarr[1,0].imshow(sample_images[3])\nxyarr[1,1].imshow(sample_images[4])\nxyarr[1,2].imshow(sample_images[5])\nxyarr[2,0].imshow(sample_images[6])\nxyarr[2,1].imshow(sample_images[7])\nxyarr[2,2].imshow(sample_images[8])\nplt.show()","76e245fe":"import keras\nclass MCDropout(keras.layers.Dropout):\n    def call(self, inputs):\n        return super().call(inputs, training=True)","2d74af86":"from tensorflow.keras import models\nfrom tensorflow.keras.layers import Activation, ZeroPadding2D, MaxPooling2D, Conv2D, Flatten, Dense, Dropout\nfrom tensorflow.keras import regularizers, constraints\n\n# Define a sequential keras model\nmodel = models.Sequential()\n\n# 1st Convolution layer\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='linear', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 1), padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\n# 2nd Convolution layer\nmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(l2=0.01)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# 3rd Convolution layer\nmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(l2=0.01)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Flatten the Convolution\nmodel.add(Flatten())\n\n# Define a dense layer with l2 regularizer to reduce overfitting\nmodel.add(Dense(512, activation='relu', kernel_initializer=\"glorot_uniform\", kernel_regularizer=regularizers.l2(l2=0.01)))\n\n# Define a drop layer to reduce overfitting\nmodel.add(MCDropout(rate=0.5))\n\n# Final output layer\nmodel.add(Dense(NUM_CLASSES, activation='softmax', kernel_initializer=\"glorot_uniform\"))\n","02cb5d61":"model.summary()","9569b06b":"from keras.utils.vis_utils import plot_model\nplot_model(model, show_shapes=True, show_layer_names=True)","108b0849":"from tensorflow.keras import optimizers, losses\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping()\n\nmodel.compile(\n    loss=losses.CategoricalCrossentropy(from_logits=True),\n    optimizer=optimizers.Adam(learning_rate=0.0003),\n    metrics=[\"accuracy\"]\n)\n\nhistory = model.fit(\n    training_generator,\n    steps_per_epoch=(NUMBER_OF_TRAINING_IMAGES\/\/BATCH_SIZE ),\n    epochs=EPOCHS,\n    validation_data=testing_generator,\n    shuffle=True,\n    validation_steps=(NUMBER_OF_TESTING_IMAGES\/\/BATCH_SIZE),\n#     callbacks=[early_stopping]\n)","7927203b":"plot_folder = \"plot\"\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.1, 1])\nplt.legend(loc='lower right')","9fd36e0e":"plot_folder = \"plot\"\nplt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Val Loss')\nplt.legend(loc='lower right')","5bd30152":"from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n\nY_pred = model.predict(validation_generator)\ny_pred = np.argmax(Y_pred, axis=1)\nprint(classification_report(validation_generator.classes, y_pred))\nprint(validation_generator.classes)\nprint(y_pred)\nprint('Confusion Matrix')\nprint(confusion_matrix(validation_generator.classes, y_pred))","56341750":"MODEL_NAME = \"keras_face_recognition.h5\"\nmodel_path = \".\/model\"\nif not os.path.exists(model_path):\n    os.mkdir(model_path)\n\nmodel.save(os.path.join(model_path, MODEL_NAME))\nclass_names = training_generator.class_indices\nclass_names_file_reverse = MODEL_NAME[:-3] + \"_class_names_reverse.npy\"\nclass_names_file = MODEL_NAME[:-3] + \"_class_names.npy\"\nnp.save(os.path.join(model_path, class_names_file_reverse), class_names)\nclass_names_reversed = np.load(os.path.join(model_path, class_names_file_reverse), allow_pickle=True).item()\nclass_names = dict([(value, key) for key, value in class_names_reversed.items()])\nnp.save(os.path.join(model_path, class_names_file), class_names)","7e4a8163":"def get_sample_test_image():\n    \"\"\"Chooses a random image from the testing set\"\"\"\n    \n    # Choose image sample\n    expected_class = random.randint(1, NUM_CLASSES)\n    random_sample = random.randint(1, 3)\n\n    # Build image path\n    image_path = f\"data\/train\/class_{expected_class}\/class_{expected_class}_{random_sample}.png\"\n\n    # Read the file\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Return the results\n    return img, expected_class","967d2136":"def preprocess_image(img):\n    \"\"\"Ensures the image is the correct shape and normalises the pixels\"\"\"\n\n    image = Image.fromarray(img)\n    image = image.resize((160,160))\n    face_array = np.asarray(image)\n    # expands the dimensions\n    face_array = face_array.reshape(160,160,1)\n\n    face_array = face_array.astype('float32')\n    scaled_image = np.expand_dims(face_array, axis=0)\n\n    return scaled_image","7cbc69c4":"def prediction(image, debug=True): \n    # show the image\n    plt.imshow(image)\n    plt.show()\n\n    # Process the sample\n    input_sample = preprocess_image(img) \n\n    # Prediction\n    results = model.predict(input_sample)\n    result = np.argmax(results, axis=1)\n    index = result[0]\n\n    # Calculate Confidence\n    confidence = results[0][index] * 100\n    classes = np.load(os.path.join(\"model\", class_names_file), allow_pickle=True).item()\n    # Get class name\n    if type(classes) is dict:\n        for k, v in classes.items():\n            if k == index:\n                class_name = v\n    if debug:\n        print(results)\n        print(\"Detected class is {} with {:.2f}% confidence\".format(class_name, round(confidence, 2)))\n    \n    # Return results\n    return class_name, confidence","838ba43e":"# Choose an image\nimg, expected_class = get_sample_test_image()\n\nprint(f\"Expected class: {expected_class}\")\n\nprediction(img), f\"expected:{expected_class}\"","cb6012c3":"def monte_carlo_prediction(image, debug=True): \n    # show the image\n    plt.imshow(image)\n    plt.show()\n    \n    # Process the sample\n    input_sample = preprocess_image(img)\n\n    # Prediction\n    results = np.stack([model(input_sample, training=True) for _ in range(100)])\n\n    # Calucate Results\n    results_mean = results.mean(axis=0)\n    results_std = results.std(axis=0)\n    index = np.argmax(results_mean,axis=1)\n\n    # Calculate Confidence\n    confidence = results_mean[0][index][0] * 100\n\n    classes = np.load(os.path.join(\"model\", class_names_file), allow_pickle=True).item()\n    if type(classes) is dict:\n        for k, v in classes.items():\n            if k == index:\n                class_name = v\n\n    if (debug):\n        print(f'Mean = {np.round(results_mean[:1],2)}')\n        print(f'Std  = {np.round(results_std[:1],2)}')\n        print(confidence)\n        print(\"detected class is {} with {:.2f}% confidence\".format(class_name, round(confidence, 2)))\n        \n    return class_name, confidence","1f1d912f":"# Choose an image\nimg, expected_class = get_sample_test_image()\nprint(f\"expected class {expected_class}\")\n\nmonte_carlo_prediction(img)","3125dc30":"THRESHOLD = 80\n\nALLOWED_USERS = [\"class_01\", \"class_03\", \"class_05\", \"class_07\", \"class_09\", \"class_11\", \"class_13\", \"class_15\"]\n\ndef authenticate(img, debug=False):\n    classname, confidence = monte_carlo_prediction(img, debug)\n\n    if (confidence < THRESHOLD):\n        # Not authenticated\n        print(\"Face not recognised\")\n    elif (classname in ALLOWED_USERS):\n        print(\"Welcome {}\".format(classname))\n    else:\n        print(\"You are not permitted {}\".format(classname))\n\n    return classname, confidence","b553f9b4":"img, expected_class = get_sample_test_image()\nprint(f\"expected class {expected_class}\")\n\nauthenticate(img)","1d45c6b0":"# Setup the test train data","15995c37":"### Monte Carlo Dropout","ef3e1192":"## Extract the faces","f58c9fbf":"The result achieves 75% accuracy on the test set which is a fairly good result, testing other models will give us a better benchmark.","4d864ceb":"# Training a Convolutional Neural Network","c5d52336":"## Creating the authentication mechanism\n\nWe can filter against this by ensuring that a person is not allowed into the building if the classification is below a certain threshold. ","2bdb8a30":"# Sample Testing","2f33dacb":"## CNN Model Evaluation\n\nAfter training, we can evaluate the model to test its accuracy.\n\nWe need to test for overfitting which occurs when the accuracy of the training set is sigificantly higher than the testing set due to the loss function.\n\nImplementing l2 regularization attempts to reduce overfitting by penalizing large weights in the network. Due to the small size of the dataset, there has to be a trade off between overfitting and accuracy.","a16d3c8e":"# Testing Random Forrest","0e208c98":"# Building the dataset\nWe need to extract the faces for all of the images so that we can create our dataset for training\/testing.","98ea06f4":"## Train the SVM Model\n\nAn SVM model gives us a good baseline as to the performance of the dataset that we can expect","9e8c3826":"## Random Sample testing","8617ee06":"### Save the dataset to disk to load into keras\n\nKeras has built in features to load data from disk, so we will save the dataset.","88b5d584":"In each of the images, the face is positioned in a different region. The first task is to normalise all of the face positions so that they can reliably be fed into a classifier. There are are few techniques to achieve this. Haar Cascades provide a simple and fast method for detecting faces but they can be unreliable. They are ideally suited to real time detection. Facial identification can afford to spend longer processing the image. Therefore we will use MTCNN which is a face detection algorithm using a CNN, it achieves much higher accuracy than other techniques.","34dbb3ae":"## Save the model\n\nThe model can be saved for future processing","9d2f33c0":"## Test Train Split\n\nThere are a limited number of samples per class, although we need to a suitable number of classes in order to benchmark the models appropriately. I decided on a test train split of ~ 70\/30 (enough for 3 test images).","31b6d5a6":"The next step is to extract and normalise the face pixels so that they can reliably be used for classifying.","a9e77648":"# Exploratory Data Analysis\nThe Yale Face Database is made from 165 grayscale images of 15 people. There are 11 images per person with different expressions. ","dd3593cb":"## Configure data generators with random gaussian noise, zooming and rotation \n\nKeras includes an ImageDataGenerator which can automatically augment the dataset."}}