{"cell_type":{"3c122e8f":"code","18b4832f":"code","6efe6f5a":"code","3c77100b":"code","903db667":"code","d644fbc6":"code","ec93cc49":"code","520b3b6d":"code","438cb733":"code","dd64b7e9":"code","39ec8b51":"code","2ffa6f85":"code","a4b60c5f":"code","ed1a9e57":"code","d3583a75":"code","5928ef8d":"code","e745403b":"code","753ed215":"markdown","738b3da5":"markdown"},"source":{"3c122e8f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.datasets import cifar10\n\n(x_train,y_train),(x_test,y_test) = cifar10.load_data() #load cifar 10 dataset from keras\n","18b4832f":"# let's see the dimen of our training data!\nprint(x_train.shape)\nprint(y_train.shape)\n#Looks like x_train has 50000 entries of dimen 32*32*3 and y_train has 50000 entries of dimen 1","6efe6f5a":"# let's see the dimen of our testing data!\nprint(x_test.shape)\nprint(y_test.shape)\n# As expected, similar to the shape of testing data","3c77100b":"# how about we try to see the contents. Let's look at the first element of x_train\nx_train[0]\n# It is simply an array of numbers - Note that these numbers denote the pixel values(0-255)","903db667":"y_train\n# We know that all the images are labelled over 10 categories. \n#So, the y_train is a number between 0 to 10 where each number depicts one category.","d644fbc6":"# time to re-scale so that all the pixel values lie within 0 to 1\nx_train = x_train.astype('float32')\/255\nx_test = x_test.astype('float32')\/255","ec93cc49":"# Let's see how it looks after re-scale\nx_train[0]","520b3b6d":"no_of_classes = len(np.unique(y_train))\nno_of_classes","438cb733":"import keras\n# here, we are transforming y_train and y_test to be an array of size 10.\n# The value of y_train\/y_test as we saw earlier was a number from 0 to 9 each depicting one category.\n# The value of y is represented by 1 in the corresponding array position and others are set to 0. \n# So, each row has only one item whose value will be 1 which depicts the category.\ny_train = keras.utils.to_categorical(y_train,no_of_classes)\ny_test = keras.utils.to_categorical(y_test,no_of_classes)\ny_test","dd64b7e9":"# we are going to divide our training set into 2 sets - train and validation.\nx_train,x_valid = x_train[5000:],x_train[:5000]\ny_train,y_valid = y_train[5000:],y_train[:5000]\nprint(x_train.shape)\nprint(y_train.shape)","39ec8b51":"print(x_valid.shape)\nprint(y_valid.shape)","2ffa6f85":"#let's visualize the first 50 images of training set\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(25,5))\nfor i in range(50):\n    ax = fig.add_subplot(5,10,i+1,xticks=[],yticks=[])\n    ax.imshow(np.squeeze(x_train[i]))\n    ","a4b60c5f":"# Time to create our model ! Simple use of convolutional and max pooling layers.\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,Dense,Flatten,Dropout,MaxPooling2D\n\nmodel = Sequential()\nmodel.add(Conv2D(filters=16, kernel_size = 2, padding = 'same',activation = 'relu',input_shape=(32,32,3)))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(Conv2D(filters=32, kernel_size = 2, padding = 'same',activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(Conv2D(filters=64, kernel_size = 2, padding = 'same',activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\nmodel.add(Dense(500,activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(10,activation='softmax'))\n\nmodel.summary()","ed1a9e57":"#Compile the model\nmodel.compile(optimizer = 'rmsprop', loss ='categorical_crossentropy',metrics=['accuracy'])\nprint('compiled!')","d3583a75":"# start training\nfrom keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint(filepath = 'best_model.h5',save_best_only = True,verbose=1)\n\nhistory = model.fit(x_train,y_train,batch_size=32, epochs = 100,\n          validation_data=(x_valid,y_valid),\n          callbacks=[checkpoint],\n          verbose=2, shuffle=True)","5928ef8d":"#Let's check the accuracy score of the best model on our test set\nmodel.load_weights('best_model.h5')\nscore = model.evaluate(x_test,y_test,verbose=0)\nscore[1]\n# Not bad ! we have an accuracy score of 68% on our test set.","e745403b":"#Lets try to visualize the accuracy and loss over the epochs.\nplt.figure(1)  \n   \n # summarize history for accuracy  \n   \nplt.subplot(211)  \nplt.plot(history.history['acc'])  \nplt.plot(history.history['val_acc'])  \nplt.title('model accuracy')  \nplt.ylabel('accuracy')  \nplt.xlabel('epoch')  \nplt.legend(['train', 'test'], loc='upper left')  \n   \n # summarize history for loss  \n   \nplt.subplot(212)  \nplt.plot(history.history['loss'])  \nplt.plot(history.history['val_loss'])  \nplt.title('model loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')  \nplt.legend(['train', 'test'], loc='upper left')  \nplt.show()  ","753ed215":"In this kernel, I will be building a CNN from scratch for CIFAR 10 dataset loaded from keras. It is a dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images. We will try to find the best CNN model for this dataset.","738b3da5":"We can see that the accuracy starts increasing till it reaches around epoch 10 and then starts decreasing and just the opposite happens with loss which decreases till epoch 10 and then increases. Somewhere, around apoch 10 we have found our best model."}}