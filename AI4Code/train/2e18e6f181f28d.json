{"cell_type":{"228769bd":"code","42f2f145":"code","afbd5d3a":"code","e9d7d65a":"code","cd617cd5":"code","4a1ac6c6":"code","8d18790f":"code","91e9f7ec":"code","d9a636f7":"code","48503540":"code","483723d2":"code","0be07395":"code","4375ced3":"code","f7c3e637":"code","eb162e4c":"code","171dc45c":"code","e6f3c1d7":"code","5ef07cca":"code","8fedbbc6":"code","ac918917":"code","31e12342":"code","061bde7f":"code","86d2ddb9":"code","7f9f6567":"code","3840ff3c":"code","181da408":"code","0097dbe5":"code","a0a215fa":"code","b4d1b1c5":"code","b1d6b92c":"code","dd8c2988":"code","64446173":"code","5e0c13d9":"code","701d5242":"code","7e05ad21":"code","ef606dd6":"code","af454c75":"code","b225baf2":"code","8d64582f":"code","4dd954ac":"code","d8244825":"code","d0f396c0":"code","61701d5c":"code","d64a1422":"code","6e5cb2aa":"code","ca2169f5":"code","70bb638f":"code","2aec89a2":"code","6fba9fc5":"code","a1c97342":"code","eeb31f46":"code","7801d7ce":"code","821adb7a":"code","085bacc9":"code","4e8c7b05":"code","981b7731":"code","79eb6481":"code","1476e8b6":"code","296daf34":"code","d6f7f09a":"code","664a044b":"code","410e07a5":"code","6e088610":"code","fe4e76c5":"code","9f53f47e":"code","12dcd595":"code","57cc91fe":"code","01487017":"code","533abadd":"code","1e0c0f1a":"code","f86ba0a7":"code","00be9102":"markdown","e35b2240":"markdown","8079740a":"markdown","860b18a6":"markdown","25c7c74c":"markdown","69af9e49":"markdown","348d7692":"markdown","43431eec":"markdown","6f18a5cf":"markdown","f69c767e":"markdown","f07144dd":"markdown","7cbb3565":"markdown","f3fa422f":"markdown","176c694e":"markdown","ac845433":"markdown","0961a545":"markdown","9bd8a6cb":"markdown","40ed9105":"markdown","81b78f32":"markdown","67dfccc6":"markdown","722a55e4":"markdown","eac32d21":"markdown","683973dc":"markdown","e7569f2c":"markdown","547173e3":"markdown","ddab16a7":"markdown","eb20ba49":"markdown","d79a2700":"markdown","ba521030":"markdown","bca353e9":"markdown","96d0b988":"markdown","643d96d8":"markdown","6b621892":"markdown","1edb30dd":"markdown","90af2a43":"markdown","3a1a46fe":"markdown","b68e517b":"markdown","15f1abad":"markdown","944cd646":"markdown","4541d2c3":"markdown","a52ed423":"markdown","3920a124":"markdown","8bbfbe19":"markdown","912b7521":"markdown","a6e4587d":"markdown","a2f995c4":"markdown","64a7fa1a":"markdown","6075a3e7":"markdown","ceb2128b":"markdown","019690f1":"markdown","b01a9fd0":"markdown","2c2ffb07":"markdown","d4a1bd07":"markdown","da070792":"markdown","767f45b5":"markdown","7a2910a0":"markdown","7d02793b":"markdown","7051ae46":"markdown","a884789d":"markdown","5bf5e73b":"markdown","ec2eb989":"markdown","9386d1fa":"markdown","83a1bacf":"markdown","bc09c23e":"markdown","f9dbc531":"markdown","8bef3d3f":"markdown","3afd4458":"markdown","5c200079":"markdown","484db116":"markdown","4a7f7a56":"markdown","653057ca":"markdown","a1192634":"markdown","93a2137a":"markdown","2eb4196c":"markdown","deac950f":"markdown","ddb3b1f5":"markdown","0d084027":"markdown","1ab0da23":"markdown","d309b657":"markdown","5142ea96":"markdown","85b6ce1b":"markdown","82186380":"markdown"},"source":{"228769bd":"import numpy as np\nfrom bs4 import BeautifulSoup\nimport re\nfrom tqdm import tqdm\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom matplotlib import rcParams\nfrom wordcloud import WordCloud\nfrom matplotlib import rc_params\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, classification_report\nfrom sklearn.metrics import precision_recall_curve, precision_score, recall_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom gensim.models import Word2Vec\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom prettytable import PrettyTable","42f2f145":"%matplotlib inline\nrcParams[\"figure.figsize\"] = 8,8\nsns.set_style(\"darkgrid\")\n# plt.style.use('ggplot')","afbd5d3a":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\nprint(train.info())\ntrain.head()","e9d7d65a":"test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ny_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\n# Droping id column from y_test data-set as that same id is present in test data-set in same order.\ny_test = y_test.drop(\"id\", axis=1)\n\n# Joining test with y_test data-set to make a complete new test data-set.\ntest = pd.DataFrame.join(test, y_test)\nprint(test.info())\ntest.head()","cd617cd5":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))","4a1ac6c6":"fig, axes = plt.subplots(1, 2, figsize=(10,10))\nsns.heatmap(train.isnull(), ax=axes[0]).set_title(\"Train Data-Frame\")\nsns.heatmap(test.isnull(), ax=axes[1]).set_title(\"Test Data-Frame\")\n\nplt.suptitle(\"Heatmap For Finding Both Data-Frame's Missing Values\", fontsize=25)\n# plt.tight_layout()\nplt.show()","8d18790f":"fig, axes = plt.subplots(1, 2, figsize=(10,10))\ncommon_locations_train = train.location.value_counts()[:10]\ncommon_locations_test = test.location.value_counts()[:10]\nsns.barplot(x=common_locations_train, y=common_locations_train.index, ax=axes[0]).set_title(\"Train Data-Frame\")\nsns.barplot(x=common_locations_test, y=common_locations_test.index, ax=axes[1]).set_title(\"Test Data-Frame\")\nplt.suptitle(\"Top 10 Locations From Both Data Tweet's\", fontsize=20)\nplt.tight_layout(pad=6.0)\nplt.show()","91e9f7ec":"test = test.drop(\"location\", axis=1)\n\n# Droping the Missing Values \ntest = test.dropna(axis=0)\n\n# As Data's indexs are not in order so :\ntest = test.reset_index()\n\n# Now, droping the old indexs as it became a column\ntest = test.drop(\"index\", axis=1)\nprint(test.info())\ntest.head()","d9a636f7":"train = train.drop(\"location\", axis=1)\n\n# Droping the Missing Values\ntrain = train.dropna(axis=0)\n\n# As Data's indexs are not in order so :\ntrain = train.reset_index()\n\n# Now, droping the old indexs as it became a column\ntrain = train.drop(\"index\", axis=1)\nprint(train.info())\ntrain.head()","48503540":"fig, axes = plt.subplots(1, 2, figsize=(10,10))\nsns.barplot(x=train.target.value_counts().index, y=train.target.value_counts(), ax=axes[0]).set_title(\"Train Data-Frame\")\nsns.barplot(x=test.target.value_counts().index, y=test.target.value_counts(), ax=axes[1]).set_title(\"Test Data-Frame\")\nplt.suptitle(\"Distribution Of Classes\", fontsize=20)\nplt.tight_layout(pad=6.0)\nplt.show()","483723d2":"fig, axes = plt.subplots(1, 2, figsize=(10,10))\ncommon_keywords_train = train.keyword.value_counts()[:10]\ncommon_keywords_test = test.keyword.value_counts()[:10]\nsns.barplot(x=common_keywords_train, y=common_keywords_train.index, ax=axes[0]).set_title(\"Train Data-Frame\")\nsns.barplot(x=common_keywords_test, y=common_keywords_test.index, ax=axes[1]).set_title(\"Test Data-Frame\")\nplt.suptitle(\"Top 10 Keywords From Both Data Tweets\", fontsize=20)\nplt.tight_layout(pad=6.0)\nplt.show()","0be07395":"print(train.text[20])\nprint(\"=\"*100)\nprint(train.text[120])\nprint(\"=\"*100)\nprint(train.text[220])\nprint(\"=\"*100)\nprint(train.text[320])","4375ced3":"stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", \"like\", \"via\", \"u\", \"video\", \"would\", \"one\"]","f7c3e637":"# Pre_processing all the train data :-\nlemmatizer = WordNetLemmatizer()\ntrain.text = train.text.apply(lambda a: a.lower())\npreprocessed_train = []\nfor sentance in tqdm(train.text.values):\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    \n    sentance = ' '.join(lemmatizer.lemmatize(e) for e in sentance.split() if e not in stopwords)\n    preprocessed_train.append(sentance.strip())","eb162e4c":"print(preprocessed_train[20])\nprint(\"=\"*100)\nprint(preprocessed_train[120])\nprint(\"=\"*100)\nprint(preprocessed_train[220])\nprint(\"=\"*100)\nprint(preprocessed_train[320])","171dc45c":"print(test.text[20])\nprint(\"=\"*100)\nprint(test.text[120])\nprint(\"=\"*100)\nprint(test.text[220])\nprint(\"=\"*100)\nprint(test.text[320])","e6f3c1d7":"# Pre_processing all the test data :-\ntest.text = test.text.apply(lambda a: a.lower())\npreprocessed_test = []\nfor sentance in tqdm(test.text.values):\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    \n    sentance = ' '.join(lemmatizer.lemmatize(e) for e in sentance.split() if e not in stopwords)\n    preprocessed_test.append(sentance.strip())","5ef07cca":"print(preprocessed_test[20])\nprint(\"=\"*100)\nprint(preprocessed_test[120])\nprint(\"=\"*100)\nprint(preprocessed_test[220])\nprint(\"=\"*100)\nprint(preprocessed_test[320])","8fedbbc6":"# Converting preprocessed_train List into a Series to join it back in Train Data : \nfinal_text_train = pd.Series(preprocessed_train)\nfinal_text_train.name = \"final_text\"\n\n# Joining Train Data with preprocessed_train Series & droping the old text(Tweet) column :\ntrain = pd.DataFrame.join(train, final_text_train)\ntrain = train.drop(\"text\", axis=1)\ntrain.info()","ac918917":"# Converting preprocessed_test List into a Series to join it back in Test Data :\nfinal_text_test = pd.Series(preprocessed_test)\nfinal_text_test.name = \"final_text\"\n\n# Joining Test Data with preprocessed_test Series & droping the old text(Tweet) column :\ntest = pd.DataFrame.join(test, final_text_test)\ntest = test.drop(\"text\", axis=1)\ntest.info()","31e12342":"df = train.append(test, ignore_index=True)\n\n# Id & Keyword columns are not of any use for Creating Model or for any Prediction's :\ndf = df.drop([\"id\", \"keyword\"], axis=1)\n\nprint(df.info())\ndf.head()","061bde7f":"print('Now there are {} rows & {} columns in train.'.format(train.shape[0],train.shape[1]))\nprint('Now there are {} rows & {} columns in test.'.format(test.shape[0],test.shape[1]))\nprint('Now there are {} rows & {} columns in df(The Final Data-Frame).'.format(df.shape[0],df.shape[1]))","86d2ddb9":"words = []\nfor sentences in tqdm(df.final_text.values):\n    sentences = \"\".join(sentences.lower())\n    words.append(sentences)\nwords = ''.join(word for word in words)\nwords = nltk.word_tokenize(words)\nwords = pd.Series(words)","7f9f6567":"plt.figure(figsize=(10,10))\nsns.barplot(x=words.value_counts()[:15], y=words.value_counts()[:15].index)\nplt.title(\"Top 15 Frequent Words In Tweet's\", fontsize=20)\nplt.show()","3840ff3c":"# Defining Input & Output :\nX = df[\"final_text\"]\ny = df[\"target\"]\n\n# Spliting Final Data-Frame Into Train, Cross-Validation, Test :\nX_1, X_test, y_1, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nX_train, X_cv, y_train, y_cv = train_test_split(X_1, y_1, test_size=0.2)","181da408":"fig, axes = plt.subplots(2, 2, figsize=(10,10))\nsns.barplot(x=df.target.value_counts().index, y=df.target.value_counts(), ax=axes[0,0]).set_title(\"Final Data-Frame\")\nsns.barplot(x=y_train.value_counts().index, y=y_train.value_counts(), ax=axes[0,1]).set_title(\"Y_Train\")\nsns.barplot(x=y_cv.value_counts().index, y=y_cv.value_counts(), ax=axes[1,1]).set_title(\"Y_CV\")\nsns.barplot(x=y_test.value_counts().index, y=y_test.value_counts(), ax=axes[1,0]).set_title(\"Y_Test\")\nplt.suptitle(\"Distribution Of Classes\", fontsize=20)\nplt.tight_layout(pad=6.0)\nplt.show()","0097dbe5":"print(\"Length Of X_train :-\", X_train.shape[0])\nprint(\"Length Of y_train :-\", y_train.shape[0])\nprint(\"Length Of X_test :-\", X_test.shape[0])\nprint(\"Length Of y_test :-\", y_test.shape[0])\nprint(\"Length Of X_cv :-\", X_cv.shape[0])\nprint(\"Length Of y_cv :-\", y_cv.shape[0])","a0a215fa":"bow = CountVectorizer(ngram_range=(1,2), min_df=2)\nX_train_bow = bow.fit_transform(X_train).toarray()\nX_cv_bow = bow.transform(X_cv).toarray()\nX_test_bow = bow.transform(X_test).toarray()","b4d1b1c5":"log = LogisticRegression()\nlog.fit(X_train_bow, y_train)","b1d6b92c":"pre_cv_bow_log = log.predict(X_cv_bow)\npre_test_bow_log = log.predict(X_test_bow)","dd8c2988":"print(\"BOW CV Classification Report by Logistic Regression\")\nprint(classification_report(y_cv, pre_cv_bow_log))\nprint(\"=\"*100)\nprint(\"BOW Test Classification Report by Logistic Regression\")\nprint(classification_report(y_test, pre_test_bow_log))","64446173":"preproba_cv_bow_log = log.predict_proba(X_cv_bow)[:,1]\npreproba_test_bow_log = log.predict_proba(X_test_bow)[:,1]\npreproba_train_bow_log = log.predict_proba(X_train_bow)[:,1]\n\nfpr_cv_bow_log_roc, tpr_cv_bow_log_roc, threshold_cv_bow_log_roc = roc_curve(y_cv, preproba_cv_bow_log)\nfpr_test_bow_log_roc, tpr_test_bow_log_roc, threshold_test_bow_log_roc = roc_curve(y_test, preproba_test_bow_log)\nfpr_train_bow_log_roc, tpr_train_bow_log_roc, threshold_train_bow_log_roc = roc_curve(y_train, preproba_train_bow_log)\n\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_bow_log_roc,tpr_test_bow_log_roc, label='Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_log)*100))\nax.plot(fpr_cv_bow_log_roc, tpr_cv_bow_log_roc, label='CV ROC AUC ='+str(roc_auc_score(y_cv, preproba_cv_bow_log)*100))\nax.plot(fpr_train_bow_log_roc,tpr_train_bow_log_roc, label='Train ROC AUC ='+str(roc_auc_score(y_train,preproba_train_bow_log)*100))\nplt.title('ROC', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()\n\n","5e0c13d9":"precision_test_bow_log_pr, recall_test_bow_log_pr, threshold_test_bow_log_pr = precision_recall_curve(y_test, preproba_test_bow_log)\nprecision_cv_bow_log_pr, recall_cv_bow_log_pr, threshold_cv_bow_log_pr = precision_recall_curve(y_cv, preproba_cv_bow_log)\nprecision_train_bow_log_pr, recall_train_bow_log_pr, threshold_train_bow_log_pr = precision_recall_curve(y_train, preproba_train_bow_log)\nfig_1 = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_bow_log_pr, precision_test_bow_log_pr, label=\"Test\")\nax_1.plot(recall_cv_bow_log_pr, precision_cv_bow_log_pr, label=\"CV\")\nax_1.plot(recall_train_bow_log_pr, precision_train_bow_log_pr, label=\"Train\")\nplt.title('Precision-Recall Curve', fontsize=20)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nax_1.legend()\nplt.show()","701d5242":"conf_test_bow_log = confusion_matrix(y_test, pre_test_bow_log)\nclass_label = [\"1 (Positive)\", \"0 (Negative)\"]\ndf = pd.DataFrame(conf_test_bow_log, index = class_label, columns = class_label)\nsns.heatmap(df, annot = True,fmt=\"d\")\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.ylabel(\"Predicted Label\")\nplt.xlabel(\"True Label\")\nplt.show()\n","7e05ad21":"mnb = MultinomialNB()\nmnb.fit(X_train_bow, y_train)","ef606dd6":"pre_cv_bow_mnb = mnb.predict(X_cv_bow)\npre_test_bow_mnb = mnb.predict(X_test_bow)","af454c75":"print(\"BOW CV Classification Report by Multi-Nomial Naiye Bayes\")\nprint(classification_report(y_cv, pre_cv_bow_mnb))\nprint(\"=\"*100)\nprint(\"BOW Test Classification Report by Multi-Nomial Naiye Bayes\")\nprint(classification_report(y_test, pre_test_bow_mnb))","b225baf2":"preproba_cv_bow_mnb = mnb.predict_proba(X_cv_bow)[:,1]\npreproba_test_bow_mnb = mnb.predict_proba(X_test_bow)[:,1]\npreproba_train_bow_mnb = mnb.predict_proba(X_train_bow)[:,1]\n\nfpr_cv_bow_mnb_roc, tpr_cv_bow_mnb_roc, threshold_cv_bow_mnb_roc = roc_curve(y_cv, preproba_cv_bow_mnb)\nfpr_test_bow_mnb_roc, tpr_test_bow_mnb_roc, threshold_test_bow_mnb_roc = roc_curve(y_test, preproba_test_bow_mnb)\nfpr_train_bow_mnb_roc, tpr_train_bow_mnb_roc, threshold_train_bow_mnb_roc = roc_curve(y_train, preproba_train_bow_mnb)\n\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_bow_mnb_roc,tpr_test_bow_mnb_roc, label='Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_mnb)*100))\nax.plot(fpr_cv_bow_mnb_roc, tpr_cv_bow_mnb_roc, label='CV ROC AUC ='+str(roc_auc_score(y_cv, preproba_cv_bow_mnb)*100))\nax.plot(fpr_train_bow_mnb_roc, tpr_train_bow_mnb_roc, label='Train ROC AUC ='+str(roc_auc_score(y_train, preproba_train_bow_mnb)*100))\nplt.title('ROC', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","8d64582f":"precision_test_bow_mnb_pr, recall_test_bow_mnb_pr, threshold_test_bow_mnb_pr = precision_recall_curve(y_test, preproba_test_bow_mnb)\nprecision_cv_bow_mnb_pr, recall_cv_bow_mnb_pr, threshold_cv_bow_mnb_pr = precision_recall_curve(y_cv, preproba_cv_bow_mnb)\nprecision_train_bow_mnb_pr, recall_train_bow_mnb_pr, threshold_train_bow_mnb_pr = precision_recall_curve(y_train, preproba_train_bow_mnb)\nfig_1 = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_bow_mnb_pr, precision_test_bow_mnb_pr, label=\"Test\")\nax_1.plot(recall_cv_bow_mnb_pr, precision_cv_bow_mnb_pr, label=\"CV\")\nax_1.plot(recall_train_bow_mnb_pr, precision_train_bow_mnb_pr, label=\"Train\")\nplt.title('Precision-Recall Curve', fontsize=20)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nax_1.legend()\nplt.show()","4dd954ac":"conf_test_bow_mnb = confusion_matrix(y_test, pre_test_bow_mnb)\nclass_label = [\"1 (Positive)\", \"0 (Negative)\"]\ndf = pd.DataFrame(conf_test_bow_mnb, index = class_label, columns = class_label)\nsns.heatmap(df, annot = True,fmt=\"d\")\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.ylabel(\"Predicted Label\")\nplt.xlabel(\"True Label\")\nplt.show()\n","d8244825":"dtc = DecisionTreeClassifier()\ndtc.fit(X_train_bow, y_train)","d0f396c0":"pre_cv_bow_dtc = dtc.predict(X_cv_bow)\npre_test_bow_dtc = dtc.predict(X_test_bow)","61701d5c":"print(\"BOW CV Classification Report by Decision-Tree Classifier\")\nprint(classification_report(y_cv, pre_cv_bow_dtc))\nprint(\"=\"*100)\nprint(\"BOW Test Classification Report by Decision-Tree Classifier\")\nprint(classification_report(y_test, pre_test_bow_dtc))","d64a1422":"preproba_cv_bow_dtc = dtc.predict_proba(X_cv_bow)[:,1]\npreproba_test_bow_dtc = dtc.predict_proba(X_test_bow)[:,1]\npreproba_train_bow_dtc = dtc.predict_proba(X_train_bow)[:,1]\n\nfpr_cv_bow_dtc_roc, tpr_cv_bow_dtc_roc, threshold_cv_bow_dtc_roc = roc_curve(y_cv, preproba_cv_bow_dtc)\nfpr_test_bow_dtc_roc, tpr_test_bow_dtc_roc, threshold_test_bow_dtc_roc = roc_curve(y_test, preproba_test_bow_dtc)\nfpr_train_bow_dtc_roc, tpr_train_bow_dtc_roc, threshold_train_bow_dtc_roc = roc_curve(y_train, preproba_train_bow_dtc)\n\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_bow_dtc_roc,tpr_test_bow_dtc_roc, label='Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_dtc)*100))\nax.plot(fpr_cv_bow_dtc_roc, tpr_cv_bow_dtc_roc, label='CV ROC AUC ='+str(roc_auc_score(y_cv, preproba_cv_bow_dtc)*100))\nax.plot(fpr_train_bow_dtc_roc, tpr_train_bow_dtc_roc, label='Train ROC AUC ='+str(roc_auc_score(y_train, preproba_train_bow_dtc)*100))\nplt.title('ROC', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","6e5cb2aa":"precision_test_bow_dtc_pr, recall_test_bow_dtc_pr, threshold_test_bow_dtc_pr = precision_recall_curve(y_test, preproba_test_bow_dtc)\nprecision_cv_bow_dtc_pr, recall_cv_bow_dtc_pr, threshold_cv_bow_dtc_pr = precision_recall_curve(y_cv, preproba_cv_bow_dtc)\nprecision_train_bow_dtc_pr, recall_train_bow_dtc_pr, threshold_train_bow_dtc_pr = precision_recall_curve(y_train, preproba_train_bow_dtc)\nfig_1 = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_bow_dtc_pr, precision_test_bow_dtc_pr, label=\"Test\")\nax_1.plot(recall_cv_bow_dtc_pr, precision_cv_bow_dtc_pr, label=\"CV\")\nax_1.plot(recall_train_bow_dtc_pr, precision_train_bow_dtc_pr, label=\"Train\")\nplt.title('Precision-Recall Curve', fontsize=20)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nax_1.legend()\nplt.show()","ca2169f5":"conf_test_bow_dtc = confusion_matrix(y_test, pre_test_bow_dtc)\nclass_label = [\"1 (Positive)\", \"0 (Negative)\"]\ndf = pd.DataFrame(conf_test_bow_dtc, index = class_label, columns = class_label)\nsns.heatmap(df, annot = True,fmt=\"d\")\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.ylabel(\"Predicted Label\")\nplt.xlabel(\"True Label\")\nplt.show()","70bb638f":"fig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_bow_log_roc,tpr_test_bow_log_roc, label='Logistic Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_log)*100))\nax.plot(fpr_test_bow_mnb_roc,tpr_test_bow_mnb_roc, label='Multi-Nomial Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_mnb)*100))\nax.plot(fpr_test_bow_dtc_roc,tpr_test_bow_dtc_roc, label='Decision-Tree Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_dtc)*100))\nplt.title('ROC Comparison of Logistic Vs Multi-Nomial Vs Decision-Tree in BOW', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","2aec89a2":"fig = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_bow_log_pr, precision_test_bow_log_pr, label=\"Logistic Test PR Curve\")\nax_1.plot(recall_test_bow_mnb_pr, precision_test_bow_mnb_pr, label=\"Multi-Nomial Test PR Curve\")\nax_1.plot(recall_test_bow_dtc_pr, precision_test_bow_dtc_pr, label=\"Decision-Tree Test PR Curve\")\nplt.title('PR Curve Comparison of Logistic Vs Multi-Nomial Vs Decision-Tree in BOW', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax_1.legend()\nplt.show()","6fba9fc5":"tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2)\nX_train_tfidf = tfidf.fit_transform(X_train).toarray()\nX_cv_tfidf = tfidf.transform(X_cv).toarray()\nX_test_tfidf = tfidf.transform(X_test).toarray()","a1c97342":"log = LogisticRegression()\nlog.fit(X_train_tfidf, y_train)","eeb31f46":"pre_cv_tfidf_log = log.predict(X_cv_tfidf)\npre_test_tfidf_log = log.predict(X_test_tfidf)","7801d7ce":"print(\"TF-IDF CV Classification Report by Logistic Regresion\")\nprint(classification_report(y_cv, pre_cv_tfidf_log))\nprint(\"=\"*100)\nprint(\"TF-IDF Test Classification Report by Logistic Regresion\")\nprint(classification_report(y_test, pre_test_tfidf_log))","821adb7a":"preproba_cv_tfidf_log = log.predict_proba(X_cv_tfidf)[:,1]\npreproba_test_tfidf_log = log.predict_proba(X_test_tfidf)[:,1]\npreproba_train_tfidf_log = log.predict_proba(X_train_tfidf)[:,1]\n\nfpr_cv_tfidf_log_roc, tpr_cv_tfidf_log_roc, threshold_cv_tfidf_log_roc = roc_curve(y_cv, preproba_cv_tfidf_log)\nfpr_test_tfidf_log_roc, tpr_test_tfidf_log_roc, threshold_test_tfidf_log_roc = roc_curve(y_test, preproba_test_tfidf_log)\nfpr_train_tfidf_log_roc, tpr_train_tfidf_log_roc, threshold_train_tfidf_log_roc = roc_curve(y_train, preproba_train_tfidf_log)\n\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_tfidf_log_roc,tpr_test_tfidf_log_roc, label='Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_log)*100))\nax.plot(fpr_cv_tfidf_log_roc, tpr_cv_tfidf_log_roc, label='CV ROC AUC ='+str(roc_auc_score(y_cv, preproba_cv_tfidf_log)*100))\nax.plot(fpr_train_tfidf_log_roc,tpr_train_tfidf_log_roc, label='Train ROC AUC ='+str(roc_auc_score(y_train,preproba_train_tfidf_log)*100))\nplt.title('ROC', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","085bacc9":"precision_test_tfidf_log_pr, recall_test_tfidf_log_pr, threshold_test_tfidf_log_pr = precision_recall_curve(y_test, preproba_test_tfidf_log)\nprecision_cv_tfidf_log_pr, recall_cv_tfidf_log_pr, threshold_cv_tfidf_log_pr = precision_recall_curve(y_cv, preproba_cv_tfidf_log)\nprecision_train_tfidf_log_pr, recall_train_tfidf_log_pr, threshold_train_tfidf_log_pr = precision_recall_curve(y_train, preproba_train_tfidf_log)\nfig_1 = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_tfidf_log_pr, precision_test_tfidf_log_pr, label=\"Test\")\nax_1.plot(recall_cv_tfidf_log_pr, precision_cv_tfidf_log_pr, label=\"CV\")\nax_1.plot(recall_train_tfidf_log_pr, precision_train_tfidf_log_pr, label=\"Train\")\nplt.title('Precision-Recall Curve', fontsize=20)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nax_1.legend()\nplt.show()","4e8c7b05":"conf_test_tfidf_log = confusion_matrix(y_test, pre_test_tfidf_log)\nclass_label = [\"1 (Positive)\", \"0 (Negative)\"]\ndf = pd.DataFrame(conf_test_tfidf_log, index = class_label, columns = class_label)\nsns.heatmap(df, annot = True,fmt=\"d\")\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.ylabel(\"Predicted Label\")\nplt.xlabel(\"True Label\")\nplt.show()\n","981b7731":"mnb = MultinomialNB()\nmnb.fit(X_train_tfidf, y_train)","79eb6481":"pre_cv_tfidf_mnb = mnb.predict(X_cv_tfidf)\npre_test_tfidf_mnb = mnb.predict(X_test_tfidf)","1476e8b6":"print(\"TF-IDF CV Classification Report by Multi-Nomial Naive Bayes\")\nprint(classification_report(y_cv, pre_cv_tfidf_mnb))\nprint(\"=\"*100)\nprint(\"TF-IDF Test Classification Report by Multi-Nomial Naive Bayes\")\nprint(classification_report(y_test, pre_test_tfidf_mnb))","296daf34":"preproba_cv_tfidf_mnb = mnb.predict_proba(X_cv_tfidf)[:,1]\npreproba_test_tfidf_mnb = mnb.predict_proba(X_test_tfidf)[:,1]\npreproba_train_tfidf_mnb = mnb.predict_proba(X_train_tfidf)[:,1]\n\nfpr_cv_tfidf_mnb_roc, tpr_cv_tfidf_mnb_roc, threshold_cv_tfidf_mnb_roc = roc_curve(y_cv, preproba_cv_tfidf_mnb)\nfpr_test_tfidf_mnb_roc, tpr_test_tfidf_mnb_roc, threshold_test_tfidf_mnb_roc = roc_curve(y_test, preproba_test_tfidf_mnb)\nfpr_train_tfidf_mnb_roc, tpr_train_tfidf_mnb_roc, threshold_train_tfidf_mnb_roc = roc_curve(y_train, preproba_train_tfidf_mnb)\n\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_tfidf_mnb_roc,tpr_test_tfidf_mnb_roc, label='Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_mnb)*100))\nax.plot(fpr_cv_tfidf_mnb_roc, tpr_cv_tfidf_mnb_roc, label='CV ROC AUC ='+str(roc_auc_score(y_cv, preproba_cv_tfidf_mnb)*100))\nax.plot(fpr_train_tfidf_mnb_roc,tpr_train_tfidf_mnb_roc, label='Train ROC AUC ='+str(roc_auc_score(y_train,preproba_train_tfidf_mnb)*100))\nplt.title('ROC', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","d6f7f09a":"precision_test_tfidf_mnb_pr, recall_test_tfidf_mnb_pr, threshold_test_tfidf_mnb_pr = precision_recall_curve(y_test, preproba_test_tfidf_mnb)\nprecision_cv_tfidf_mnb_pr, recall_cv_tfidf_mnb_pr, threshold_cv_tfidf_mnb_pr = precision_recall_curve(y_cv, preproba_cv_tfidf_mnb)\nprecision_train_tfidf_mnb_pr, recall_train_tfidf_mnb_pr, threshold_train_tfidf_mnb_pr = precision_recall_curve(y_train, preproba_train_tfidf_mnb)\nfig_1 = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_tfidf_mnb_pr, precision_test_tfidf_mnb_pr, label=\"Test\")\nax_1.plot(recall_cv_tfidf_mnb_pr, precision_cv_tfidf_mnb_pr, label=\"CV\")\nax_1.plot(recall_train_tfidf_mnb_pr, precision_train_tfidf_mnb_pr, label=\"Train\")\nplt.title('Precision-Recall Curve', fontsize=20)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nax_1.legend()\nplt.show()","664a044b":"conf_test_tfidf_mnb = confusion_matrix(y_test, pre_test_tfidf_mnb)\nclass_label = [\"1 (Positive)\", \"0 (Negative)\"]\ndf = pd.DataFrame(conf_test_tfidf_mnb, index = class_label, columns = class_label)\nsns.heatmap(df, annot = True,fmt=\"d\")\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.ylabel(\"Predicted Label\")\nplt.xlabel(\"True Label\")\nplt.show()\n","410e07a5":"dtc = DecisionTreeClassifier()\ndtc.fit(X_train_tfidf, y_train)","6e088610":"pre_cv_tfidf_dtc = dtc.predict(X_cv_tfidf)\npre_test_tfidf_dtc = dtc.predict(X_test_tfidf)","fe4e76c5":"print(\"TF-IDF CV Classification Report by Decision-Tree Classifier\")\nprint(classification_report(y_cv, pre_cv_tfidf_dtc))\nprint(\"=\"*100)\nprint(\"TF-_IDF Test Classification Report by Decision-Tree Classifier\")\nprint(classification_report(y_test, pre_test_tfidf_dtc))","9f53f47e":"preproba_cv_tfidf_dtc = dtc.predict_proba(X_cv_tfidf)[:,1]\npreproba_test_tfidf_dtc = dtc.predict_proba(X_test_tfidf)[:,1]\npreproba_train_tfidf_dtc = dtc.predict_proba(X_train_tfidf)[:,1]\n\nfpr_cv_tfidf_dtc_roc, tpr_cv_tfidf_dtc_roc, threshold_cv_tfidf_dtc_roc = roc_curve(y_cv, preproba_cv_tfidf_dtc)\nfpr_test_tfidf_dtc_roc, tpr_test_tfidf_dtc_roc, threshold_test_tfidf_dtc_roc = roc_curve(y_test, preproba_test_tfidf_dtc)\nfpr_train_tfidf_dtc_roc, tpr_train_tfidf_dtc_roc, threshold_train_tfidf_dtc_roc = roc_curve(y_train, preproba_train_tfidf_dtc)\n\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_tfidf_dtc_roc,tpr_test_tfidf_dtc_roc, label='Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_dtc)*100))\nax.plot(fpr_cv_tfidf_dtc_roc, tpr_cv_tfidf_dtc_roc, label='CV ROC AUC ='+str(roc_auc_score(y_cv, preproba_cv_tfidf_dtc)*100))\nax.plot(fpr_train_tfidf_dtc_roc, tpr_train_tfidf_dtc_roc, label='Train ROC AUC ='+str(roc_auc_score(y_train, preproba_train_tfidf_dtc)*100))\nplt.title('ROC', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","12dcd595":"precision_test_tfidf_dtc_pr, recall_test_tfidf_dtc_pr, threshold_test_tfidf_dtc_pr = precision_recall_curve(y_test, preproba_test_tfidf_dtc)\nprecision_cv_tfidf_dtc_pr, recall_cv_tfidf_dtc_pr, threshold_cv_tfidf_dtc_pr = precision_recall_curve(y_cv, preproba_cv_tfidf_dtc)\nprecision_train_tfidf_dtc_pr, recall_train_tfidf_dtc_pr, threshold_train_tfidf_dtc_pr = precision_recall_curve(y_train, preproba_train_tfidf_dtc)\nfig_1 = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_tfidf_dtc_pr, precision_test_tfidf_dtc_pr, label=\"Test\")\nax_1.plot(recall_cv_tfidf_dtc_pr, precision_cv_tfidf_dtc_pr, label=\"CV\")\nax_1.plot(recall_train_tfidf_dtc_pr, precision_train_tfidf_dtc_pr, label=\"Train\")\nplt.title('Precision-Recall Curve', fontsize=20)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nax_1.legend()\nplt.show()","57cc91fe":"conf_test_tfidf_dtc = confusion_matrix(y_test, pre_test_tfidf_dtc)\nclass_label = [\"1 (Positive)\", \"0 (Negative)\"]\ndf = pd.DataFrame(conf_test_tfidf_dtc, index = class_label, columns = class_label)\nsns.heatmap(df, annot = True,fmt=\"d\")\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.ylabel(\"Predicted Label\")\nplt.xlabel(\"True Label\")\nplt.show()","01487017":"fig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr_test_tfidf_log_roc,tpr_test_tfidf_log_roc, label='Logistic Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_log)*100))\nax.plot(fpr_test_tfidf_mnb_roc,tpr_test_tfidf_mnb_roc, label='Multi-Nomial Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_mnb)*100))\nax.plot(fpr_test_tfidf_dtc_roc,tpr_test_tfidf_dtc_roc, label='Decision-Tree Test ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_dtc)*100))\nplt.title('ROC Comparison of Logistic Vs Multi-Nomial Vs Decision-Tree in TF-IDF', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","533abadd":"fig = plt.figure()\nax_1 = plt.subplot(111)\nax_1.plot(recall_test_tfidf_log_pr, precision_test_tfidf_log_pr, label=\"Logistic Test PR Curve\")\nax_1.plot(recall_test_tfidf_mnb_pr, precision_test_tfidf_mnb_pr, label=\"Multi-Nomial Test PR Curve\")\nax_1.plot(recall_test_tfidf_dtc_pr, precision_test_tfidf_dtc_pr, label=\"Decision-Tree Test PR Curve\")\nplt.title('PR Curve Comparison of Logistic Vs Multi-Nomial Vs Decision-Tree in TF-IDF', fontsize=20)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax_1.legend()\nplt.show()","1e0c0f1a":"fig_2 = plt.figure(figsize=(15,15))\n\n# Ploting fig_2\nax_5 = plt.subplot(221)\nax_5.set_title(\"Comparing Logistic's\", fontsize=20)\nax_5.plot(fpr_test_tfidf_log_roc,tpr_test_tfidf_log_roc, label='Logistic TF-IDF ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_log)*100))\nax_5.plot(fpr_test_bow_log_roc,tpr_test_bow_log_roc, label='Logistic BOW ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_log)*100))\n\nax_6 = plt.subplot(222)\nax_6.set_title(\"Comparing Multi-Nomial Naive Baye's\", fontsize=20)\nax_6.plot(fpr_test_tfidf_mnb_roc,tpr_test_tfidf_mnb_roc, label='Multi-Nomial TF-IDF ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_mnb)*100))\nax_6.plot(fpr_test_bow_mnb_roc,tpr_test_bow_mnb_roc, label='Multi-Nomial BOW ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_mnb)*100))\n\nax_7 = plt.subplot(212)\nax_7.set_title(\"Finally Comparing Top ROC AUC Curve's\", fontsize=20)\nax_7.plot(fpr_test_tfidf_log_roc,tpr_test_tfidf_log_roc, label='Logistic TF-IDF ROC AUC ='+str(roc_auc_score(y_test,preproba_test_tfidf_log)*100))\nax_7.plot(fpr_test_bow_mnb_roc,tpr_test_bow_mnb_roc, label='Multi-Nomial BOW ROC AUC ='+str(roc_auc_score(y_test,preproba_test_bow_mnb)*100))\n\n\nax_5.legend()\nax_6.legend()\nax_7.legend()\nplt.suptitle('Final ROC AUC Comparison Of Logistic & Multi-Nomial In BOW Vs TF-IDF', fontsize=25)\nplt.show()\n","f86ba0a7":"x = PrettyTable()\nx.field_names = [\"Vectorizer\", \"Model\", \"AUC (in %)\", \"Precision Score (in %)\", \"Recall Score (in %)\"]\nx.add_row([\"BOW\", \"Logistic Regression\", round(roc_auc_score(y_test,preproba_test_bow_log)*100), round(precision_score(y_test,pre_test_bow_log)*100), round(recall_score(y_test,pre_test_bow_log)*100)])\nx.add_row([\"BOW\", \"Multi-Nomial Naive Bayes\", round(roc_auc_score(y_test,preproba_test_bow_mnb)*100), round(precision_score(y_test,pre_test_bow_mnb)*100), round(recall_score(y_test,pre_test_bow_mnb)*100)])\nx.add_row([\"BOW\", \"Decision-Tree Classifier\", round(roc_auc_score(y_test,preproba_test_bow_dtc)*100), round(precision_score(y_test,pre_test_bow_dtc)*100), round(recall_score(y_test,pre_test_bow_dtc)*100)])\nx.add_row([\"TF-IDF\", \"Logistic Regression\", round(roc_auc_score(y_test,preproba_test_tfidf_log)*100), round(precision_score(y_test,pre_test_tfidf_log)*100), round(recall_score(y_test,pre_test_tfidf_log)*100)])\nx.add_row([\"TF-IDF\", \"Multi-Nomial Naive Bayes\", round(roc_auc_score(y_test,preproba_test_tfidf_mnb)*100), round(precision_score(y_test,pre_test_tfidf_mnb)*100), round(recall_score(y_test,pre_test_tfidf_mnb)*100)])\nx.add_row([\"TF-IDF\", \"Decision-Tree Classifier\", round(roc_auc_score(y_test,preproba_test_tfidf_dtc)*100), round(precision_score(y_test,pre_test_tfidf_dtc)*100), round(recall_score(y_test,pre_test_tfidf_dtc)*100)])\nprint(x)\n","00be9102":"Here I have used **Bag-Of-Words** for **converting** all the **tweet's** from **text** to **vectors**. ","e35b2240":"# 2. Loading The Data-Set's :-","8079740a":"In the above line of code we plotted **Precision-Recall Curve** from **Test, Train and Cross-Validation Data**.","860b18a6":"# 5.1.4. - Comaprison of Bag Of Words Test's :-","25c7c74c":"After creating a **New Test Data** we can see through **test.info()** & through **test.head()** that there are columns having **missing values(NaN)**.","69af9e49":"The above line of code ***Pre-Processed*** all the ***Test Data*** and ***stored*** it all in a list named ***preprocessed_test***.","348d7692":"# 3.4.2. Top 15 Frequent Words In Tweet's :-","43431eec":"# 2.1. Loading Train Data :-","6f18a5cf":"# 3. Exploratory Data Analysis (EDA) :-<br>\n\n***Following are the parts of Exploratory Data Analysis (EDA) :-***<br><br>\n**3.1.** Heatmap For Finding Both Data-Frame's Missing Values<br>\n**3.2.** Top 10 Locations From Both Data Tweet's<br>\n**3.3.** Data Cleaning<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**3.3.1** Distribution Of Classes<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**3.3.2** Top 10 Keywords From Both Data Tweets<br>\n**3.4.** Data Pre-Processing<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**3.4.1.** Creating Final Data-Frame (Inclusive Of Train & Test Both)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**3.4.2.** Top 15 Frequent Words In Tweet's<br><br>\nIn the following section I have tried to understand what data is about and what answers can I generate through some visual representations before & after Data Cleaning \/ Data Pre-Processing.","f69c767e":"After **training the model** with the help of **TF-IDF & Multi-Nomial Naive Bayes** here in the above line of code I have stored **predictions** of **Cross-Validation & Test Data** for **further analysis**.","f07144dd":"# 5.1. - Bag Of Words :-","7cbb3565":"# 3.3.1. Distribution Of Classes :-","f3fa422f":"**Through Heatmap** we can see that there are many **missing values** in **location column** of both **Train & Test Data** and a few are in **keyword column** as well.<br><br> We will handle them in **Data Cleaning part.**","176c694e":"# 3.3. Data Cleaning :-","ac845433":"In the above line of code we plotted **Receiver Operating Characteristic (ROC) Curve** on all the **threshold's** possible and then drew this above diagram from **Test, Train and Cross-Validation Data**.","0961a545":"We can see many things from above **Classification Report's** like **Precision**, **Recall**, **F1-score**, **Accuracy** of both the predictions which we got from **Cross-Validation & Test Data**.","9bd8a6cb":"In the above line of code we plotted **Precision-Recall Curve** from **Test, Train and Cross-Validation Data**.","40ed9105":"So, we can see here the **words** which are widely used in our **Data of Tweet's**.","81b78f32":"There are **7613 rows** and **5 columns** in **Train**<br>\nThere are **3263 rows** and **5 columns** in **Test**","67dfccc6":"From the above diagram's we can see the most frequent locations from where the tweet's are actually posted on **Twitter** related to **Natural Disaster's**.<br><br>\nAnd we can easily conclude that in both of the data-set's **New York**, **USA**, **United States** & **Canada** are in the Top Five List.","722a55e4":"# 5.2.2. - Naive Bayes {Multi-Nomial Naive Bayes} :-","eac32d21":"# 5.2.1. - Logisctic Regression :-","683973dc":"***So, by all these Comparison's and by this Conclusion Table we can select the Best Algorithm with Best Featurization Technique which suit's our priorities \/ requirements {Like :- Some gives more priority to ROC-AUC or some give priority to Precision-Recall}.***","e7569f2c":"Here I have used **TF-IDF (Term Frequency - Inverse Term Frequency)** for **converting** all the **tweet's** from **text** to **vectors**. ","547173e3":"Here I have used **train_test_split()** function to **break** my **Data of Tweet's** into **3 categories equally proportional to each class {0,1}** namely **:-**<br> {**Train** (For Training the Model), **Cross-Validation** (For cross checking the predictions) **&** **Test** (For final testing of our model)}.","ddab16a7":"In the above line of code we plotted **Receiver Operating Characteristic (ROC) Curve** on all the **threshold's** possible and then drew this above diagram from **Test, Train and Cross-Validation Data**.","eb20ba49":"As we saw above that the **location** column in **Train Data** has too much missing value's.<br><br>\nSo, I am **droping** that column because that column doesn't also help us in any way to possibly predict whether a tweet is talking about a **real disaster or not**.","d79a2700":"# 5.2. - TF-IDF :-","ba521030":"***There are three files to load :-***<br><br>\n**2.1.** train.csv (consisting of {id, keyword, location, text, target} as columns)<br>\n**2.2.** test.csv  (consisting of {id, keyword, location, text} as columns)<br>\n**2.3.** y_test.csv  (consisting of target column of test.csv data)<br>","bca353e9":"# 5.3. - Final Comparison :-","96d0b988":"From the above diagram's we can easily notice that there are **barely 2 to 3 keywords** which are **common** in both data-set's **Top 10 Keywords List**.<br><br>\nAnd these **Keywords** are even not playing any important role in predicting that whether a tweet is talking about a **real disaster or not**.","643d96d8":"In the above line of code we plotted **Receiver Operating Characteristic (ROC) Curve** on all the **threshold's** possible and then drew this above diagram from **Test, Train and Cross-Validation Data**.","6b621892":"As we can see above all the **3 categories** are now having **equal proportion of {0,1} Classes.**<br><br>\nThis tells us that now our **Data** is all set to go for **Featurization** & then for **Creating Model by various Algorithms**.","1edb30dd":"# 3.2. Top 10 Locations From Both Data Tweet's :-","90af2a43":"Now we can see all our **Test Data** is **cleaned** and **Pre-Processed**.","3a1a46fe":"<h1>Twitter Tweet's Model For Identifying Real Disaster Tweet's<\/h1>","b68e517b":"![title](http:\/\/pluspng.com\/img-png\/thanks-png-hd-images-simple-graphic-tnku0195-640.png \"Header\")","15f1abad":"**Now** there are **7552 rows** & **4 columns** in **train**.<br>\n**Now** there are **3237 rows** & **4 columns** in **test**.<br><br>\n**And** there are **10789 rows** & **2 columns** in **df(The Final Data-Frame)**.","944cd646":"# 3.4.1. Creating Final Data-Frame (Inclusive Of Train & Test Both):-","4541d2c3":"By loading **train.csv** we can see through **train.info()** & through **train.head()** that there are columns having **missing values(NaN).**","a52ed423":"# 1. Importing Libraries :-","3920a124":"After **training the model** with the help of **Bow & Logistic Regression** here in the above line of code I have stored **predictions** of **Cross-Validation & Test Data** for **further analysis**.","8bbfbe19":"***Following are the parts of my Featurization & Applying Algorithms section :-***<br>\n# 5.1.\nIn this part I am going to convert all my **Tweet's text Data into Vectors** using **Bag-Of-Words** represented by **bow** and then will apply following **Algorithms :-**<br><br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.1.1.** - **First** will apply **Logistic Regression** which is **represented** here by **log**.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.1.2.** - **Second** will apply **Multi-Nomial Naive Bayes** which is **represented** here by **mnb**.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.1.3.** - **Third** will apply **Decision-Tree Classifier** which is **represented** here by **dtc**.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.1.4.** - **Fourth** there will be **Comaprison of Bag-Of-Words Test's** {To **find out** which **Algorithm** worked **best with Bag-Of-Words**}.\n\n# 5.2.\nIn this part I am going to convert all my **Tweet's text Data into Vectors** using **TF-IDF (Term Frequency - Inverse Document Frequency)** represented by **tfidf** and then will apply following **Algorithms :-**<br><br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.2.1.** - **First** will apply **Logistic Regression** which is **represented** here by **log**.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.2.2.** - **Second** will apply **Multi-Nomial Naive Bayes** which is **represented** here by **mnb**.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.2.3.** - **Third** will apply **Decision-Tree Classifier** which is **represented** here by **dtc**.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**5.2.4.** - **Fourth** there will be **Comaprison of TF-IDF Test's** {To **find out** which **Algorithm** worked **best with TF-IDF**}.\n\n# 5.3\nHere in **Final Comparison** I will compare all the **selected Algorithms** which i will get from **both Featurization techniques {Bag-Of-Words & TF-IDF}** by **ROC & AUC Curve's**.\n\n# 5.4\nNow comes the **Conclusion** part where I will create a **Conclusion Table** with the help of **prettytable library** and will show the **results** of all of the **Algorithms** to **easily compare**.","912b7521":"Here is the **list of words** which are **most frequently** used in any **english paragraph** or in a **group of sentences**, and which **doesn't** play any important role in **creating a model to predict anything**.<br> **So**, we are simply naming them as **stopwords** and in the **following line of codes** I will be **removing these stopwords** from our **Data of Tweet's**.","a6e4587d":"**As** we can see here all the tweet's of **Test Data** are in a need of some **Polishing(Pre-Processing)** because there are various **stopwords, http:\/\/ tags & various punchuations** which aren't required while **predicting** that whether a **tweet** is talking about a **real disaster or not**.","a2f995c4":"After **training the model** with the help of **TF-IDF & Decision-Tree Classifier** here in the above line of code I have stored **predictions** of **Cross-Validation & Test Data** for **further analysis**.","64a7fa1a":"We can see many things from above **Classification Report's** like **Precision**, **Recall**, **F1-score**, **Accuracy** of both the predictions which we got from **Cross-Validation & Test data**.","6075a3e7":"# 5.2.3. - Decision-Tree Classifier :-","ceb2128b":"After **training the model** with the help of **Bow & Decision-Tree Classifier** here in the above line of code I have stored **predictions** of **Cross-Validation & Test Data** for **further analysis**.","019690f1":"The above line of code ***Pre-Processed*** all the ***Train Data*** and ***stored*** it all in a list named ***preprocessed_train***.","b01a9fd0":"# 5.4. - Conclusion :-","2c2ffb07":"In the above line of code we plotted **Receiver Operating Characteristic (ROC) Curve** on all the **threshold's** possible and then drew this above diagram from **Test, Train and Cross-Validation Data**.","d4a1bd07":"In the above line of code as we want to **plot a barplot** of some of the **most frequent words** in our whole **Data of Tweet's**.<br> **First** we will have to **join all the tweet's** with **lowering all the words and alphabet's** so that our system will easily find out the **words** which are **most frequently repeated**.<br> **Second**, we will have to **convert all the tweets** to **single-single words** to know the **frequency** of them, this can be done by using **nltk.word_tokenize()** and in the **last** we will convert all **single-single words** from a **list** to a **column** to apply a function called **value_counts()**.","da070792":"We can see many things from above **Classification Report's** like **Precision**, **Recall**, **F1-score**, **Accuracy** of both the predictions which we got from **Cross-Validation & Test data**.","767f45b5":"We can see many things from above **Classification Report's** like **Precision**, **Recall**, **F1-score**, **Accuracy** of both the predictions which we got from **Cross-Validation & Test data**.","7a2910a0":"Now we can see all our **Train Data** is **cleaned** and **Pre-Processed**.","7d02793b":"After **training the model** with the help of **TF-IDF & Logistic Regression** here in the above line of code I have stored **predictions** of **Cross-Validation & Test Data** for **further analysis**.","7051ae46":"As we saw above that the **location** column in **Test Data** has too much missing value's.<br><br>\nSo, I am **droping** that column because that column doesn't also help us in any way to possibly predict whether a tweet is talking about a **real disaster or not**.","a884789d":"We can see many things from above **Classification Report's** like **Precision**, **Recall**, **F1-score**, **Accuracy** of both the predictions which we got from **Cross-Validation & Test data**.","5bf5e73b":"We can see many things from above **Classification Report's** like **Precision**, **Recall**, **F1-score**, **Accuracy** of both the predictions which we got from **Cross-Validation & Test data**.","ec2eb989":"# 5. Featurization & Applying Algorithms :- \n","9386d1fa":"# 3.4. Data Pre-Processing :-","83a1bacf":"![title](https:\/\/pngriver.com\/wp-content\/uploads\/2018\/04\/Download-Twitter-PNG-HD-1-768x289.png \"Header\")","bc09c23e":"In the above line of code we plotted **Precision-Recall Curve** from **Test, Train and Cross-Validation Data**.","f9dbc531":"# 5.1.1. - Logisctic Regression :-","8bef3d3f":"# 3.3.2. Top 10 Keywords From Both Data Tweet's :-","3afd4458":"In the above line of code we plotted **Receiver Operating Characteristic (ROC) Curve** on all the **threshold's** possible and then drew this above diagram from **Test, Train and Cross-Validation Data**.","5c200079":"**Data Source :-** https:\/\/www.kaggle.com\/c\/nlp-getting-started\/data <br>\n\n**Twitter Tweet's Data Overview :-**<br>\n\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster or what.\n\nSo, to indentify this I have build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t.\n\n***Number of Tweet's in Train Data-Set :-*** 7,613<br> \n***Number of Tweet's in Test Data-Set :-*** 3263<br>\n***Total Number of Tweet's :-*** 10,876<br>\n\n**Attribute's Information :-**<br>\n\n***1.*** id - a unique identifier for each tweet<br>\n***2.*** keyword - a particular keyword from the tweet (may be blank)<br>\n***3.*** location - the location the tweet was sent from (may be blank)<br>\n***4.*** text - the text of the tweet<br>\n***5.*** target - this denotes whether a tweet is about a real disaster (1) or not (0)<br>\n","484db116":"# 5.1.3. - Decision-Tree Classifier :-","4a7f7a56":"In the above line of code we plotted **Precision-Recall Curve** from **Test, Train and Cross-Validation Data**.","653057ca":"Here we can see that our **classes are not much imbalanced** in **Train Data** & we can conclude that in our **Train Data only there are two classes like {0,1} whereas in our Test Data only 0 value target tweet's are stored.**<br><br>\nWe will handle that as well a little later after **Data Pre-Processing part.**","a1192634":"In the above line of code we plotted **Receiver Operating Characteristic (ROC) Curve** on all the **threshold's** possible and then drew this above diagram from **Test, Train and Cross-Validation Data**.","93a2137a":"# 4. Spliting Final Data-Frame Into Train, Cross-Validation,  Test :-","2eb4196c":"In the above line of code we plotted **Precision-Recall Curve** from **Test, Train and Cross-Validation Data**.","deac950f":"# 3.1. Heatmap For Finding Both Data-Frame's Missing Values :-","ddb3b1f5":"# 5.1.2. - Naive Bayes {Multi-Nomial Naive Bayes}:-","0d084027":"In the above line of code we plotted **Precision-Recall Curve** from **Test, Train and Cross-Validation Data**.","1ab0da23":"# 2.2. & 2.3. Loading Test Data with y_test Data :-","d309b657":"# 5.2.4. - Comparison of TF-IDF Test's :-","5142ea96":"**As** we can see here all the tweet's of **Train Data** are in a need of some **Polishing(Pre-Processing)** because there are various **stopwords, http:\/\/ tags & various punchuations** which aren't required while **predicting** that whether a **tweet** is talking about a **real disaster or not**.","85b6ce1b":"After **training the model** with the help of **Bow & Multi-Nomial Naive Bayes** here in the above line of code I have stored **predictions** of **Cross-Validation & Test Data** for **further analysis**.","82186380":"# 4.1. Equal Distribution Of Classes :-"}}