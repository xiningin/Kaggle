{"cell_type":{"8e010ee3":"code","b9191d15":"code","dbad2eb3":"code","8d8ef3f6":"code","aa7d88cf":"code","0bde70a1":"code","08bacaf0":"code","eeeeb8a5":"code","be1daff2":"code","ac680081":"code","30d9ff4b":"code","4dde914a":"code","5dc33ff0":"code","d673e6e7":"code","db4ab085":"code","36c5a4c7":"code","980ae0a7":"markdown","b359aa01":"markdown","ad4dace9":"markdown","3cdb935a":"markdown","2b64b439":"markdown"},"source":{"8e010ee3":"# In Google Colab, uncomment this:\n!wget https:\/\/bit.ly\/2FMJP5K -O setup.py && bash setup.py","b9191d15":"# XVFB will be launched if you run on a server\nimport os\nif type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n    !bash ..\/xvfb start\n    os.environ['DISPLAY'] = ':1'","dbad2eb3":"import gym\nimport numpy as np\nimport pandas as pd\n\nenv = gym.make(\"Taxi-v3\")\nenv.reset()\nenv.render()","8d8ef3f6":"n_states = env.observation_space.n\nn_actions = env.action_space.n\n\nprint(\"n_states=%i, n_actions=%i\" % (n_states, n_actions))","aa7d88cf":"policy = np.ones((n_states, n_actions))\/(n_actions)","0bde70a1":"assert type(policy) in (np.ndarray, np.matrix)\nassert np.allclose(policy, 1.\/n_actions)\nassert np.allclose(np.sum(policy, axis=1), 1)","08bacaf0":"def generate_session(policy, t_max=10**4):\n    \"\"\"\n    Play game until end or for t_max ticks.\n    :param policy: an array of shape [n_states,n_actions] with action probabilities\n    :returns: list of states, list of actions and sum of rewards\n    \"\"\"\n    states, actions = [], []\n    total_reward = 0.\n\n    s = env.reset()\n\n    for t in range(t_max):\n\n        #a = <sample action from policy(hint: use np.random.choice) >\n        a = np.random.choice(n_actions, 1, p=policy[s])[0]\n\n        new_s, r, done, info = env.step(a)\n\n        # Record state, action and add up reward to states,actions and total_reward accordingly.\n        states.append(s)\n        actions.append(a)\n        total_reward += r\n\n        s = new_s\n        if done:\n            break\n    return states, actions, total_reward","eeeeb8a5":"s, a, r = generate_session(policy)\nassert type(s) == type(a) == list\nassert len(s) == len(a)\nassert type(r) in [float, np.float]","be1daff2":"# let's see the initial reward distribution\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsample_rewards = [generate_session(policy, t_max=1000)[-1] for _ in range(200)]\n\nplt.hist(sample_rewards, bins=20)\nplt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\nplt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\nplt.legend()","ac680081":"def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n    \"\"\"\n    Select states and actions from games that have rewards >= percentile\n    :param states_batch: list of lists of states, states_batch[session_i][t]\n    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n    :param rewards_batch: list of rewards, rewards_batch[session_i]\n\n    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n\n    Please return elite states and actions in their original order \n    [i.e. sorted by session number and timestep within session]\n\n    If you are confused, see examples below. Please don't assume that states are integers\n    (they will become different later).\n    \"\"\"\n\n#     reward_threshold = <Compute minimum reward for elite sessions. Hint: use np.percentile >\n    reward_threshold = np.percentile(rewards_batch, percentile)\n    elite_states = []\n    elite_actions = []\n    for i in range(len(rewards_batch)):\n        if rewards_batch[i] >= reward_threshold:\n            elite_states = elite_states + states_batch[i]\n            elite_actions = elite_actions +actions_batch[i]\n    return elite_states, elite_actions","30d9ff4b":"states_batch = [\n    [1, 2, 3],     # game1\n    [4, 2, 0, 2],  # game2\n    [3, 1],        # game3\n]\n\nactions_batch = [\n    [0, 2, 4],     # game1\n    [3, 2, 0, 1],  # game2\n    [3, 3],        # game3\n]\nrewards_batch = [\n    3,  # game1\n    4,  # game2\n    5,  # game3\n]\n\ntest_result_0 = select_elites(\n    states_batch, actions_batch, rewards_batch, percentile=0)\ntest_result_40 = select_elites(\n    states_batch, actions_batch, rewards_batch, percentile=30)\ntest_result_90 = select_elites(\n    states_batch, actions_batch, rewards_batch, percentile=90)\ntest_result_100 = select_elites(\n    states_batch, actions_batch, rewards_batch, percentile=100)\n\nassert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1])  \\\n    and np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]),\\\n    \"For percentile 0 you should return all states and actions in chronological order\"\nassert np.all(test_result_40[0] == [4, 2, 0, 2, 3, 1]) and \\\n    np.all(test_result_40[1] == [3, 2, 0, 1, 3, 3]),\\\n    \"For percentile 30 you should only select states\/actions from two first\"\nassert np.all(test_result_90[0] == [3, 1]) and \\\n    np.all(test_result_90[1] == [3, 3]),\\\n    \"For percentile 90 you should only select states\/actions from one game\"\nassert np.all(test_result_100[0] == [3, 1]) and\\\n    np.all(test_result_100[1] == [3, 3]),\\\n    \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\nprint(\"Ok!\")","4dde914a":"def update_policy(elite_states, elite_actions):\n    \"\"\"\n    Given old policy and a list of elite states\/actions from select_elites,\n    return new updated policy where each action probability is proportional to\n\n    policy[s_i,a_i] ~ #[occurences of si and ai in elite states\/actions]\n\n    Don't forget to normalize policy to get valid probabilities and handle 0\/0 case.\n    In case you never visited a state, set probabilities for all actions to 1.\/n_actions\n\n    :param elite_states: 1D list of states from elite sessions\n    :param elite_actions: 1D list of actions from elite sessions\n\n    \"\"\"\n\n    new_policy = np.zeros([n_states, n_actions])\n\n#     <Your code here: update probabilities for actions given elite states & actions >\n    # Don't forget to set 1\/n_actions for all actions in unvisited states.\n    for e_i in range(len(elite_states)):\n        # Add 1 for every elite state and action\n        new_policy[elite_states[e_i], elite_actions[e_i]] += 1\n    # check for each state\n    for s_i in range(n_states):\n        total_actions = sum(new_policy[s_i])\n        if total_actions != 0:\n            new_policy[s_i] = new_policy[s_i]\/total_actions\n        else:\n            new_policy[s_i] = np.ones(n_actions)\/n_actions\n    return new_policy","5dc33ff0":"elite_states = [1, 2, 3, 4, 2, 0, 2, 3, 1]\nelite_actions = [0, 2, 4, 3, 2, 0, 1, 3, 3]\n\nnew_policy = update_policy(elite_states, elite_actions)\n\nassert np.isfinite(new_policy).all(\n), \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\nassert np.all(\n    new_policy >= 0), \"Your new policy can't have negative action probabilities\"\nassert np.allclose(new_policy.sum(\n    axis=-1), 1), \"Your new policy should be a valid probability distribution over actions\"\nreference_answer = np.array([\n    [1.,  0.,  0.,  0.,  0.],\n    [0.5,  0.,  0.,  0.5,  0.],\n    [0.,  0.33333333,  0.66666667,  0.,  0.],\n    [0.,  0.,  0.,  0.5,  0.5]])\nassert np.allclose(new_policy[:4, :5], reference_answer)\nprint(\"Ok!\")","d673e6e7":"from IPython.display import clear_output\n\ndef show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n    \"\"\"\n    A convenience function that displays training progress. \n    No cool math here, just charts.\n    \"\"\"\n\n    mean_reward = np.mean(rewards_batch)\n    threshold = np.percentile(rewards_batch, percentile)\n    log.append([mean_reward, threshold])\n\n    clear_output(True)\n    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n    plt.figure(figsize=[8, 4])\n    plt.subplot(1, 2, 1)\n    plt.plot(list(zip(*log))[0], label='Mean rewards')\n    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n    plt.legend()\n    plt.grid()\n\n    plt.subplot(1, 2, 2)\n    plt.hist(rewards_batch, range=reward_range)\n    plt.vlines([np.percentile(rewards_batch, percentile)],\n               [0], [100], label=\"percentile\", color='red')\n    plt.legend()\n    plt.grid()\n\n    plt.show()","db4ab085":"# reset policy just in case\npolicy = np.ones([n_states, n_actions]) \/ n_actions","36c5a4c7":"n_sessions = 250  # sample this many sessions\npercentile = 50  # take this percent of session with highest rewards\nlearning_rate = 0.5  # add this thing to all counts for stability\n\nlog = []\n\nfor i in range(100):\n\n#     %time sessions = [ < generate a list of n_sessions new sessions > ]\n    %time sessions = [generate_session(policy) for _ in range(n_sessions)]\n    \n    states_batch, actions_batch, rewards_batch = zip(*sessions)\n\n#     elite_states, elite_actions = <select elite states\/actions >\n    elite_states, elite_actions = select_elites(states_batch=states_batch, actions_batch=actions_batch, rewards_batch=rewards_batch, percentile=percentile)\n\n#     new_policy = <compute new policy >\n    new_policy = update_policy(elite_states=elite_states, elite_actions=elite_actions)\n    \n    policy = learning_rate*new_policy + (1-learning_rate)*policy\n\n    # display results on chart\n    show_progress(rewards_batch, log, percentile)","980ae0a7":"# Training loop\nGenerate sessions, select N best and fit to those.","b359aa01":"# Play the game\nJust like before, but we also record all states and actions we took.","ad4dace9":"# Crossentropy method steps","3cdb935a":"# Create stochastic policy\nThis time our policy should be a probability distribution\n$policy[s, a]=P(\\text{take action a | in state s})$\n\nSince we still use int3eger state and action representations, you can use a 2-dimensional array to represent the policy.\n\nPlease initialize policy uniformly, that is, probabilities of all actions sh9oould be equal.","2b64b439":"# Crossentropy method\nThis notebook will teach you to solve reinforcement learning problems with crossentropy method. We'll follow-up by scaling everything up and using . neural network policy."}}