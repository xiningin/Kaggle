{"cell_type":{"e981e129":"code","51b0a8db":"code","dbdf3bc4":"code","48db7d36":"code","a094a1f3":"code","b001b53f":"code","4e966492":"code","6ad71915":"code","5f7cdf3f":"code","42f71bd8":"code","e67ee265":"code","71e64948":"code","0cae4c92":"code","11f1e346":"code","22c2a77f":"code","6652afd0":"code","1f0cc9c0":"code","0b399aca":"code","60937c1c":"code","58b5c38c":"markdown","5064c100":"markdown","84f5d8da":"markdown","c044f369":"markdown","d5daebd5":"markdown","b7fd2440":"markdown","c766f4f3":"markdown","4b6488f0":"markdown","811319a0":"markdown","e9d2a22d":"markdown","edd181ec":"markdown","e70ba86b":"markdown"},"source":{"e981e129":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use('fivethirtyeight')","51b0a8db":"data = pd.read_csv(\"\/kaggle\/input\/college-data\/data.csv\")\ndata.columns = data.columns.str.lower()\ndata.head()","dbdf3bc4":"pd.set_option('display.float_format', '{:.4}'.format)\ndata.describe()","48db7d36":"data.info()","a094a1f3":"data.isnull().sum()","b001b53f":"plt.figure(figsize=(10, 8))\nsns.scatterplot(x='room_board', y='grad_rate', data=data, hue='private')","4e966492":"plt.figure(figsize=(10, 8))\nsns.scatterplot(x='outstate', y='f_undergrad', data=data, hue='private')","6ad71915":"plt.figure(figsize=(12, 8))\n\ndata.loc[data.private == 'Yes', 'outstate'].hist(label=\"Private College\", bins=30)\ndata.loc[data.private == 'No', 'outstate'].hist(label=\"Non Private College\", bins=30)\n\nplt.xlabel('Outstate')\nplt.legend()","5f7cdf3f":"plt.figure(figsize=(12, 8))\n\ndata.loc[data.private == 'Yes', 'grad_rate'].hist(label=\"Private College\", bins=30)\ndata.loc[data.private == 'No', 'grad_rate'].hist(label=\"Non Private College\", bins=30)\n\nplt.xlabel('Graduation Rate')\nplt.legend()","42f71bd8":"data.loc[data.grad_rate > 100]","e67ee265":"data.loc[data.grad_rate > 100, 'grad_rate'] = 100","71e64948":"plt.figure(figsize=(12, 8))\n\ndata.loc[data.private == 'Yes', 'grad_rate'].hist(label=\"Private College\", bins=30)\ndata.loc[data.private == 'No', 'grad_rate'].hist(label=\"Non Private College\", bins=30)\n\nplt.xlabel('Graduation Rate')\nplt.legend()","0cae4c92":"data.private.value_counts()","11f1e346":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nX = data.drop(['private'], axis=1)\ny = data.private\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nscalar = StandardScaler()\n\nX_train = scalar.fit_transform(X_train)\nX_test = scalar.transform(X_test)","22c2a77f":"from sklearn.model_selection import cross_val_predict, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\ndef evaluate(model, X_train, X_test, y_train, y_test):\n    y_test_pred = model.predict(X_test)\n    y_train_pred = model.predict(X_train)\n\n    print(\"TRAINIG RESULTS: \\n===============================\")\n    clf_report = pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))\n    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_train, y_train_pred)}\")\n    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_train, y_train_pred):.4f}\")\n    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n\n    print(\"TESTING RESULTS: \\n===============================\")\n    clf_report = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))\n    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_test_pred)}\")\n    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_test_pred):.4f}\")\n    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")","6652afd0":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClassifier(n_neighbors=2)\nknn_clf.fit(X_train, y_train)\nevaluate(knn_clf, X_train, X_test, y_train, y_test)","1f0cc9c0":"scores = []\n\nfor n in range(2, 40):\n    knn = KNeighborsClassifier(n_neighbors=n)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    score = accuracy_score(y_test, y_pred)\n    scores.append(score)","0b399aca":"plt.figure(figsize=(10, 8))\nplt.plot(range(2, 40), scores)\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"K nearest neighbors\")","60937c1c":"knn_clf = KNeighborsClassifier(n_neighbors=7)\nknn_clf.fit(X_train, y_train)\n\nevaluate(knn_clf, X_train, X_test, y_train, y_test)","58b5c38c":"# Exploratory Data Analysis (EDA)","5064c100":"# Choosing a K Value\nLet's go ahead and use the elbow method to pick a good K Value!\n\n**Create a for loop that trains various KNN models with different k values, then keep track of the error_rate for each of these models with a list.**","84f5d8da":"# K Nearest Neighbors Project (Universities classification - Private vs Public)\n\n![alignement-p%C3%A9dagogique-universit%C3%A9-1440x564_c.jpg](attachment:alignement-p%C3%A9dagogique-universit%C3%A9-1440x564_c.jpg)\n\nClassification and Regression\n\n![k-nearest.jfif](attachment:k-nearest.jfif)\n\n* Lazy learner\n\n  * [Instance Based](https:\/\/en.wikipedia.org\/wiki\/Instance-based_learning)\n  * Lazy because it does not try to learn a function from the training data. \n  * It memorise the pattern from the dataset\n\n\n* [Nonparametric model](http:\/\/blog.minitab.com\/blog\/adventures-in-statistics-2\/choosing-between-a-nonparametric-test-and-a-parametric-test)\n\n  * distribution-free tests because no assumption of the data needing to follow a specific distribution\n  * [wikipedia](https:\/\/en.wikipedia.org\/wiki\/Nonparametric_statistics)\n  * Other examples - Decision Tree, Random Forest\n\n\n\nUsed for:\n\n* Predict cancer is malignant or benign\n* Pattern recognition\n* Recommender Systems\n* Computer Vision\n* Gene Expression\n* Protein-Protein Interaction and 3D Structure Prediction\n\n\n## Disadvantages\n\n* Not efficient on big data\n* Curse of dimensionality. Very susceptible to overfitting\n\n\n## Steps:\n\n* Choose the number of $k$\n* Select a distance metric\n* Find the k nearest neighbors of the sample\n* Assign the class label by majority vote\n\n# Making Predictions with KNN\nKNN makes predictions using the training dataset directly.\n\nPredictions are made for a new instance (x) by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances.\n\nTo determine which of the K instances in the training dataset are most similar to a new input a distance measure is used.\n\nMetrics intended for real-valued vector spaces:\n\n| identifier\t| class name\t| args\t| distance function |\n|:- |:- |:- |:- |\n|\u201ceuclidean\u201d | \tEuclideanDistance  |   | $\\sqrt{\\sum(x - y)^2)}$  |  \n|\u201cmanhattan\u201d | ManhattanDistance |  | $\\sum\\big|x - y\\big|$|  \n|\u201cchebyshev\u201d | ChebyshevDistance |  | max${\\big|x - y\\big|}$ |  \n|\u201cminkowski\u201d | MinkowskiDistance\t | p\t | $\\sum(\\big|x - y\\big|^p)^{\\frac{1}{p}}$     | \n|\u201cwminkowski\u201d | WMinkowskiDistance\t | p, w\t | $\\sum(w\\big|x - y\\big|^p)^{\\frac{1}{p}}$     |\n|\u201cseuclidean\u201d | SEuclideanDistance\t | V\t | $\\sqrt{\\sum\\frac{(x - y)^2}{V})}$     | \n\n# Best Prepare Data for KNN\n- **Rescale Data:** KNN performs much better if all of the data has the same scale. Normalizing your data to the range [0, 1] is a good idea. It may also be a good idea to standardize your data if it has a Gaussian distribution.\n- **Address Missing Data:** Missing data will mean that the distance between samples can not be calculated. These samples could be excluded or the missing values could be imputed.\n- **Lower Dimensionality:** KNN is suited for lower dimensional data. You can try it on high dimensional data (hundreds or thousands of input variables) but be aware that it may not perform as well as other techniques. KNN can benefit from feature selection that reduces the dimensionality of the input feature space.","c044f369":"# Summary\nIn this post you discovered the KNN machine learning algorithm. You learned that:\n\n- KNN stores the entire training dataset which it uses as its representation.\n- KNN does not learn any model.\n- KNN makes predictions just-in-time by calculating the similarity between an input sample and each training instance.\n- There are many distance measures to choose from to match the structure of your input data.\n- That it is a good idea to rescale your data, such as using normalization, when using KNN.\n- If you have any questions about this post or the KNN algorithm ask in the comments and I will do my best to answer.","d5daebd5":"Set that school's graduation rate to 100 so it makes sense. You may get a warning not an error) when doing this operation, so use dataframe operations or just re-do the histogram visualization to make sure it actually went through.","b7fd2440":"**Now create the following plot using the information from your for loop.**","c766f4f3":"# Standardize the Variables","4b6488f0":"Notice how there seems to be a private school with a graduation rate of higher than 100%.What is the name of that school?","811319a0":"## Get the Data","e9d2a22d":"# Train test split","edd181ec":"# Predictions and Evaluations\nLet's evaluate our KNN model!","e70ba86b":"## Retrain with new K Value\n\n**Retrain your model with the best K value (up to you to decide what you want) and re-do the classification report and the confusion matrix.**"}}