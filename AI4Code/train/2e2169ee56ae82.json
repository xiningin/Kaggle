{"cell_type":{"c3e114bf":"code","7a1862df":"code","44cf58b0":"code","1e2f68b4":"code","39023257":"code","98919d91":"code","52babc12":"code","11ae0db8":"markdown","9c7fb71d":"markdown","ea49d738":"markdown","b262e351":"markdown","555ba5e1":"markdown","977628c7":"markdown","51d469f6":"markdown","aba537ff":"markdown"},"source":{"c3e114bf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, Activation, Dense, Input, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Flatten, MaxPooling2D, GlobalAveragePooling2D, LeakyReLU\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nwarnings.filterwarnings(action='ignore')\n%matplotlib inline\n\ntrain_df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest_df = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\ntrain_label = train_df['label']\ntrain_label = to_categorical(train_label)\ntrain_data = (train_df.iloc[:, 1:].values.astype('float32') \/ 255.0 ).reshape(-1, 28, 28, 1)\ntest_data = (test_df.iloc[:,:].values.astype('float32') \/ 255.0 ).reshape(-1, 28, 28, 1)\n\nprint('train_df shape : ',train_data.shape, 'train_label : ',train_label.shape, 'test_data : ',test_data.shape)","7a1862df":"count = pd.value_counts(train_df['label'].values)\ncount.plot.bar(figsize=(10,8), fontsize=15)","44cf58b0":"# Build CNN(Convolutional Neural Network)\noptimizer = RMSprop(learning_rate=0.0025,\n                    rho=0.9,\n                    momentum=0.1,\n                    epsilon=1e-07,\n                    centered=True,\n                    name='RMSprop')\n\nnets = 15                # Number of Reapeat\nmodel = [0] *nets        # Making Ensemble Model List\n\n\nfor i in range(nets):\n    model[i] = tf.keras.models.Sequential([\n        tf.keras.layers.Conv2D(64, (3,3), padding='same', input_shape=(28, 28, 1)),\n        tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n        tf.keras.layers.LeakyReLU(alpha=0.1),\n        \n        tf.keras.layers.Conv2D(64,  (3,3), padding='same'),\n        tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n        tf.keras.layers.LeakyReLU(alpha=0.1),\n        \n        tf.keras.layers.Conv2D(64,  (3,3), padding='same'),\n        tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n        tf.keras.layers.LeakyReLU(alpha=0.1),\n\n        tf.keras.layers.MaxPooling2D(2, 2),\n        tf.keras.layers.Dropout(0.25),\n\n        tf.keras.layers.Conv2D(128, (3,3), padding='same'),\n        tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n        tf.keras.layers.LeakyReLU(alpha=0.1),\n        \n        tf.keras.layers.Conv2D(128, (3,3), padding='same'),\n        tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n        tf.keras.layers.LeakyReLU(alpha=0.1),\n        \n        tf.keras.layers.Conv2D(128, (3,3), padding='same'),\n        tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n        tf.keras.layers.LeakyReLU(alpha=0.1),\n\n        tf.keras.layers.MaxPooling2D(2,2),\n        tf.keras.layers.Dropout(0.25),    \n\n        tf.keras.layers.Conv2D(256, (3,3), padding='same'),\n        tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n        tf.keras.layers.LeakyReLU(alpha=0.1),\n        \n        tf.keras.layers.Conv2D(256, (3,3), padding='same'),\n        tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n        tf.keras.layers.LeakyReLU(alpha=0.1),\n\n        tf.keras.layers.MaxPooling2D(2,2),\n        tf.keras.layers.Dropout(0.25),\n\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(256),\n        tf.keras.layers.LeakyReLU(alpha=0.1),\n\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ])\n    \n    model[i].compile(loss='categorical_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])","1e2f68b4":"# if you want see model summary()\n# model.summary()","39023257":"annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n\ndatagen = ImageDataGenerator(\n        rotation_range=10,  \n        zoom_range = 0.10,  \n        width_shift_range=0.1, \n        height_shift_range=0.1)\n\nhistory = [0] * nets\nepochs = 45\nfor j in range(nets):\n    X_train2, X_val2, Y_train2, Y_val2 = train_test_split(train_data, train_label, test_size = 0.1, stratify = train_label)\n    \n    history[j] = model[j].fit_generator(datagen.flow(X_train2,Y_train2, batch_size=64),\n        epochs = epochs, steps_per_epoch = X_train2.shape[0]\/\/64,  \n        validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        j+1,epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))","98919d91":"fig, ax = plt.subplots(2,1)\nax[0].plot(history[14].history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history[14].history['val_loss'], color='r', label=\"Validation loss\",axes =ax[0])\nax[0].grid(color='black', linestyle='-', linewidth=0.25)\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history[14].history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history[14].history['val_accuracy'], color='r',label=\"Validation accuracy\")\nax[1].grid(color='black', linestyle='-', linewidth=0.25)\nlegend = ax[1].legend(loc='best', shadow=True)","52babc12":"# ENSEMBLE PREDICTIONS AND SUBMIT\nresults = np.zeros( (test_data.shape[0],10) ) \nfor j in range(nets):\n    results = results + model[j].predict(test_data)\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"submission.csv\",index=False)","11ae0db8":"# Visualization about Loss and Accuracy","9c7fb71d":"# DIGIT MNIST Classification Ensemble(KOR, ENG)\n\nToday I learning about CNN Ensemble(Bagging). It is very amazting performance about accuracy.<br>\n**Ensemble(Bagging) technique is a method of solving a diffcult problem using the collective intelligence of several people rather than one expert.**<br>\n(but i think this model is Geneius of several people!!)<br>\n**This NoteBook Guideline is easy to understand for anyone. I hope it helps many people.**<br>\nI will also add the link referenced below, so it will be more useful if you check it out!<br>\n**If you have any question. plz comment!! and Don't forget Upvote!**\n<hr>\n\uae08\uc77c CNN\uc744 \uc559\uc0c1\ube14 \uae30\ubc95\uc758 \uc801\uc6a9\uc6d0\ub9ac\uc5d0 \ub300\ud558\uc5ec \ud559\uc2b5\ud558\uc600\uc2b5\ub2c8\ub2e4.<br>\uc774 Notebook\uc740 \uc27d\uac8c \uc774\ud574\ud560 \uc218 \uc788\uac8c \uc791\uc131\ub418\uc5b4 \uc788\uc73c\uba70,\n\uac1c\uc778\uc801\uc778 \ubc14\ub7a8\uc774\uc9c0\ub9cc, \ub9ce\uc740 \uc0ac\ub78c\ub4e4\uc774 \uc27d\uac8c \uc774\ud574\ud558\uc600\uc73c\uba74 \uc88b\uaca0\uc2b5\ub2c8\ub2e4.<br>\n\ucd94\uac00\ub85c \ud558\ub2e8\uc5d0 \ucc38\uc870\ud588\uc73c\uba74 \ud558\ub294 \ub9c1\ud06c\ub97c \ub2ec\uc544\ub450\uc5c8\uc73c\ub2c8, \ub0b4\uc6a9\uc744 \ud655\uc778\ud558\uc2dc\uba74 \uc870\uae08 \ub354 \ub3c4\uc6c0\ub420 \uac83\uc785\ub2c8\ub2e4.\n\n\n# Contents\n- **Load Data and Preprocessing Data**\n- **Check Values Count**\n- **Create Model**\n- **Learning Ensemble(Very Importance)**\n- **Visualization about Loss and Accuracy**\n- **Reference Link**\n","ea49d738":"# Upvote!","b262e351":"# Reference Link\n<hr>\n   - It's very useful kernel!!(I recommand below kernel)\n   \n- reference link : https:\/\/www.kaggle.com\/benanakca\/kannada-mnist-cnn-tutorial-with-app-top-2\n- reference link : https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist\/notebook\n","555ba5e1":"# Load Data and Preprocessing Data\n- **train_data compose label and pixel value.**\n    - **train_data Pixel be transformation of 28*28**\n    - **Pixel value normalization**\n    - **label data drop and make train_label**\n        - **(KOR) 28*28 \uc0ac\uc774\uc988\ub85c \ubcc0\ud615 \ubc0f \uc815\uaddc\ud654 \uacfc\uc815\uc744 \uac70\uce58\uba70, label \uc14b\uc744 \ub530\ub85c \ub9cc\ub4e4\uc5b4 \uc90d\ub2c8\ub2e4.**\n<\/br>\n<\/br>\n- **test_data**\n    - **Pixel be transformation of 28*28**\n    - **Pixel value normalization**    \n        - **(KOR) 28*28 \uc0ac\uc774\uc988\ub85c \ubcc0\ud658\ub9cc \ud574\uc8fc\uba74 \ub429\ub2c8\ub2e4.**","977628c7":"# Learning Ensemble for Bagging\n<img src = 'https:\/\/blog.kakaocdn.net\/dn\/PC6mD\/btrh2i38SMg\/VtEFCCGAyvknFrTVkZQ76K\/img.webp'><\/src>","51d469f6":"# Create Ensemble Model\n- Please note that I will comment below Code!!!","aba537ff":"# Check values count\n- **Let's Check evenly the distribution of the label data**"}}