{"cell_type":{"1747848c":"code","b31a20f2":"code","635922c5":"code","74bc5f6c":"code","f0fdb4a0":"code","69fbe023":"code","61c00be5":"code","cf3c253b":"code","b4a51e70":"code","6b5acee5":"code","effb5986":"code","caf210aa":"code","cc8e8fca":"code","7d3df613":"code","f36c93a3":"code","e57e6329":"code","6af663dd":"code","25d5a136":"code","c5027a87":"code","4eb74c91":"code","9acd02bc":"code","b35bf1f1":"code","b5b6239c":"code","c0812313":"code","853c7dc7":"code","48f91f16":"code","5f87009d":"code","84c3363f":"code","9a5856d7":"code","742111a3":"code","0b2c9ac9":"code","e9494eb7":"code","e5de6668":"code","14689cfc":"code","6f830d72":"code","6b3f0d85":"code","80da8b08":"code","06eb5646":"code","61901022":"code","63dd0089":"code","620bab4e":"code","4bbf4bed":"code","36d22b9a":"code","4fd29b6f":"code","2a7bc025":"code","e9109329":"code","fb96cb6b":"code","04e801ee":"code","ce95054f":"code","b5676010":"code","f6db350c":"markdown","9a324ff1":"markdown","759d2d08":"markdown","d7f91af4":"markdown","e79aacc5":"markdown","e4c4f542":"markdown","ba194ec6":"markdown","5e0b8c51":"markdown","55e594ae":"markdown","fcea8dd3":"markdown"},"source":{"1747848c":"from IPython.display import clear_output\n!pip install imutils\n!pip install -q efficientnet\nclear_output()","b31a20f2":"import numpy as np \nimport tensorflow as tf\nfrom tqdm import tqdm\nimport cv2\nimport os\nimport shutil\nimport itertools\nimport imutils\nimport matplotlib.pyplot as plt\nimport sklearn as sk\nimport tensorflow\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, f1_score\n\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping, Callback\n\n\ninit_notebook_mode(connected=True)\nRANDOM_SEED = 333\nclear_output()","635922c5":"!apt-get install tree\nclear_output()\n# create new folders\n!mkdir TRAIN TEST VAL TEST_BMI TEST_BDB TEST_BMD TRAIN\/YES TRAIN\/NO TEST\/YES TEST\/NO VAL\/YES VAL\/NO TEST_BMI\/YES TEST_BMI\/NO TEST_BDB\/YES TEST_BDB\/NO TEST_BMD\/YES TEST_BMD\/NO \n!tree -d","74bc5f6c":"CLASS_list = ['no','yes']\nIMG_PATH = '..\/input\/brain-tumor-detection'\nprint(os.listdir(IMG_PATH))\nprint(CLASS_list)","f0fdb4a0":"IMG_PATH = '..\/input\/brain-tumor-detection\/'\n\nfor CLASS in CLASS_list:\n    if (not CLASS.startswith('.') and (not CLASS.startswith('pred'))):\n        print(IMG_PATH + '     ' + CLASS)\n        IMG_NUM = len(os.listdir(IMG_PATH + CLASS))\n        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n            img = IMG_PATH + CLASS + '\/' + FILE_NAME\n            if n < 200:\n                shutil.copy(img, 'TEST\/' + CLASS.upper() + '\/' + FILE_NAME)\n            elif n < 1200:\n                shutil.copy(img, 'TRAIN\/'+ CLASS.upper() + '\/' + FILE_NAME)\n            else:\n                shutil.copy(img, 'VAL\/'+ CLASS.upper() + '\/' + FILE_NAME)","69fbe023":"IMG_PATH = '..\/input\/brain-mri-images-for-brain-tumor-detection'\nfor CLASS in CLASS_list:\n    if not CLASS.startswith('.'):\n        IMG_NUM = len(os.listdir(IMG_PATH + '\/' + CLASS))\n        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + '\/' + CLASS)):\n            img = IMG_PATH + '\/' + CLASS + '\/' + FILE_NAME\n            if n < 500:\n                shutil.copy(img, 'TEST_BMI\/' + CLASS.upper() + '\/' + FILE_NAME)","61c00be5":"IMG_PATH = '..\/input\/brain-tumor\/Brain2'\nfor CLASS in os.listdir(IMG_PATH):\n    if not CLASS.startswith('.'):\n        IMG_NUM = len(os.listdir(IMG_PATH + '\/' + CLASS))\n        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + '\/' + CLASS)):\n            img = IMG_PATH + '\/' + CLASS + '\/' + FILE_NAME\n            if n < 500 and CLASS == 'yes' :\n                shutil.copy(img, 'TEST_BDB\/' + 'YES' + '\/' + FILE_NAME)\n            elif n < 500 and CLASS == 'no' :\n                shutil.copy(img, 'TEST_BDB\/' + 'NO' + '\/' + FILE_NAME)","cf3c253b":"IMG_PATH = '..\/input\/brain-mri-dataset\/Brain'\nfor CLASS in os.listdir(IMG_PATH):\n    if not CLASS.startswith('.'):\n        IMG_NUM = len(os.listdir(IMG_PATH + '\/' + CLASS))\n        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + '\/' + CLASS)):\n            img = IMG_PATH + '\/' + CLASS + '\/' + FILE_NAME\n            if n < 50 and CLASS == 'yes' :\n                shutil.copy(img, 'TEST_BMD\/' + 'YES' + '\/' + FILE_NAME)\n            elif n < 50 and CLASS == 'no' :\n                shutil.copy(img, 'TEST_BMD\/' + 'NO' + '\/' + FILE_NAME)","b4a51e70":"def load_data(dir_path, img_size=(100,100)):\n    \"\"\"\n    Load resized images as np.arrays to workspace\n    \"\"\"\n    X = []\n    y = []\n    i = 0\n    labels = dict()\n    for path in tqdm(sorted(os.listdir(dir_path))):\n        if not path.startswith('.'):\n            labels[i] = path\n            for file in os.listdir(dir_path + path):\n                if not file.startswith('.'):\n                    img = cv2.imread(dir_path + path + '\/' + file)\n                    X.append(img)\n                    y.append(i)\n            i += 1\n    X = np.array(X)\n    y = np.array(y)\n    print(f'{len(X)} images loaded from {dir_path} directory.')\n    return X, y, labels\n\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    cm = np.round(cm,2)\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","6b5acee5":"TRAIN_DIR = 'TRAIN\/'\nTEST_DIR = 'TEST\/'\nVAL_DIR = 'VAL\/'\nTEST_BMI_DIR = 'TEST_BMI\/'\nTEST_BMD_DIR = 'TEST_BMD\/'\nTEST_BDB_DIR = 'TEST_BDB\/'\nIMG_SIZE = (224,224)\n\n# use predefined function to load the image data into workspace\nX_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\nX_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\nX_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)\nX_test_bmi, y_test_bmi, _ = load_data(TEST_BMI_DIR, IMG_SIZE)\nX_test_bmd, y_test_bmd, _ = load_data(TEST_BMD_DIR, IMG_SIZE)\nX_test_bdb, y_test_bdb, _ = load_data(TEST_BDB_DIR, IMG_SIZE)","effb5986":"y = dict()\ny[0] = []\ny[1] = []\nfor set_name in (y_train, y_val, y_test, y_test_bmi, y_test_bmd, y_test_bdb):\n    y[0].append(np.sum(set_name == 0))\n    y[1].append(np.sum(set_name == 1))\n\ntrace0 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set', 'Test_BMI Set', 'Test_BMD Set', 'Test_BDB Set'],\n    y=y[0],\n    name='No',\n    marker=dict(color='#33cc33'),\n    opacity=0.7\n)\ntrace1 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set', 'Test_BMI Set', 'Test_BMD Set', 'Test_BDB Set'],\n    y=y[1],\n    name='Yes',\n    marker=dict(color='#ff3300'),\n    opacity=0.7\n)\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title='Count of classes in each set',\n    xaxis={'title': 'Set'},\n    yaxis={'title': 'Count'}\n)\nfig = go.Figure(data, layout)\niplot(fig)","caf210aa":"def plot_samples(X, y, labels_dict, n=50):\n    \"\"\"\n    Creates a gridplot for desired number of images (n) from the specified set\n    \"\"\"\n    for index in range(len(labels_dict)):\n        imgs = X[np.argwhere(y == index)][:n]\n        j = 10\n        i = int(n\/j)\n\n        plt.figure(figsize=(15,6))\n        c = 1\n        for img in imgs:\n            plt.subplot(i,j,c)\n            plt.imshow(img[0])\n\n            plt.xticks([])\n            plt.yticks([])\n            c += 1\n        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n        plt.show()","cc8e8fca":"plot_samples(X_test_bdb, y_test_bdb, labels, 10)","7d3df613":"plot_samples(X_train, y_train, labels, 10)","f36c93a3":"RATIO_LIST = []\nfor set in (X_train, X_test, X_val, X_test_bmi, X_test_bdb, X_test_bmd):\n    for img in set:\n        RATIO_LIST.append(img.shape[1]\/img.shape[0])\n        \nplt.hist(RATIO_LIST)\nplt.title('Distribution of Image Ratios')\nplt.xlabel('Ratio Value')\nplt.ylabel('Count')\nplt.show()","e57e6329":"# Homomorphic filter class\nclass HomomorphicFilter:\n    \"\"\"Homomorphic filter implemented with diferents filters and an option to an external filter.\n    \n    High-frequency filters implemented:\n        butterworth\n        gaussian\n    Attributes:\n        a, b: Floats used on emphasis filter:\n            H = a + b*H\n     \n       .\n    \"\"\"\n\n    def __init__(self, a = 0.5, b = 1.5):\n        self.a = float(a)\n        self.b = float(b)\n\n    # Filters\n    def __butterworth_filter(self, I_shape, filter_params):\n        P = I_shape[0]\/2\n        Q = I_shape[1]\/2\n        U, V = np.meshgrid(range(I_shape[0]), range(I_shape[1]), sparse=False, indexing='ij')\n        Duv = (((U-P)**2+(V-Q)**2)).astype(float)\n        H = 1\/(1+(Duv\/filter_params[0]**2)**filter_params[1])\n        return (1 - H)\n\n    def __gaussian_filter(self, I_shape, filter_params):\n        P = I_shape[0]\/2\n        Q = I_shape[1]\/2\n        H = np.zeros(I_shape)\n        U, V = np.meshgrid(range(I_shape[0]), range(I_shape[1]), sparse=False, indexing='ij')\n        Duv = (((U-P)**2+(V-Q)**2)).astype(float)\n        H = np.exp((-Duv\/(2*(filter_params[0])**2)))\n        return (1 - H)\n\n    # Methods\n    def __apply_filter(self, I, H):\n        H = np.fft.fftshift(H)\n        I_filtered = (self.a + self.b*H)*I\n        return I_filtered\n\n    def filter(self, I, filter_params, filter='butterworth', H = None):\n        \"\"\"\n        Method to apply homormophic filter on an image\n        Attributes:\n            I: Single channel image\n            filter_params: Parameters to be used on filters:\n                butterworth:\n                    filter_params[0]: Cutoff frequency \n                    filter_params[1]: Order of filter\n                gaussian:\n                    filter_params[0]: Cutoff frequency\n            filter: Choose of the filter, options:\n                butterworth\n                gaussian\n                external\n            H: Used to pass external filter\n        \"\"\"\n\n        #  Validating image\n        if len(I.shape) is not 2:\n            raise Exception('Improper image')\n\n        # Take the image to log domain and then to frequency domain \n        I_log = np.log1p(np.array(I, dtype=\"float\"))\n        I_fft = np.fft.fft2(I_log)\n\n        # Filters\n        if filter=='butterworth':\n            H = self.__butterworth_filter(I_shape = I_fft.shape, filter_params = filter_params)\n        elif filter=='gaussian':\n            H = self.__gaussian_filter(I_shape = I_fft.shape, filter_params = filter_params)\n        elif filter=='external':\n            print('external')\n            if len(H.shape) is not 2:\n                raise Exception('Invalid external filter')\n        else:\n            raise Exception('Selected filter not implemented')\n        \n        # Apply filter on frequency domain then take the image back to spatial domain\n        I_fft_filt = self.__apply_filter(I = I_fft, H = H)\n        I_filt = np.fft.ifft2(I_fft_filt)\n        I = np.exp(np.real(I_filt))-1\n        return np.uint8(I)\n# End of class HomomorphicFilter","6af663dd":"kernel = np.array([[0, -1, 0],\n                   [-1, 5,-1],\n                   [0, -1, 0]])\nkernel_2 = np.array([[-1, -1, -1],\n                    [-1, 9,-1],\n                    [-1, -1, -1]])\n\nkernel_3 = np.array([[1, 1, 1],\n                    [1, -7, 1],\n                    [1, 1, 1]])\n\nkernel_4 = np.array([[-1, -1, -1, -1, -1],\n                    [-1, 2, 2, 2, -1],\n                    [-1, 2, 8, 2, -1],\n                    [-1, 2, 2, 2, -1],\n                    [-1, -1, -1, -1, -1]]) \/ 8.0","25d5a136":"def preprocessing_imgs(set_name, add_pixels_value=0):\n    \"\"\"\n    Finds the extreme points on the image and crops the rectangular out of them\n    \n    \"\"\"\n    set_new = []\n    for img in set_name:\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # threshold the image, then perform a series of erosions +\n        # dilations to remove any small regions of noise\n        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n        thresh = cv2.erode(thresh, None, iterations=2)\n        thresh = cv2.dilate(thresh, None, iterations=2)\n\n        # find contours in thresholded image, then grab the largest one\n        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts = imutils.grab_contours(cnts)\n        c = max(cnts, key=cv2.contourArea)\n\n        # find the extreme points\n        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n        extRight = tuple(c[c[:, :, 0].argmax()][0])\n        extTop = tuple(c[c[:, :, 1].argmin()][0])\n        extBot = tuple(c[c[:, :, 1].argmax()][0])\n        \n        ADD_PIXELS = add_pixels_value\n        new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n        new_img = cv2.cvtColor(new_img, cv2.COLOR_RGB2GRAY) #-transfer to grayscale\n        #homo_filter = HomomorphicFilter(a = 0.9, b = 1.4) \n        #new_img = homo_filter.filter(I=new_img, filter_params=[30,2])\n        #new_img = cv2.filter2D(src=img, ddepth=-1, kernel=kernel)\n        new_img = cv2.equalizeHist(new_img) #-equalizing the intensity histogram\n        new_img = cv2.cvtColor(new_img,cv2.COLOR_GRAY2RGB)\n        set_new.append(new_img)\n\n    return np.array(set_new)","c5027a87":"\nimg2 = '..\/input\/brain-tumor\/Brain2\/yes\/continvY114.JPG'\nimg = cv2.imread(img2)\nimg = cv2.resize(\n            img,\n            dsize=IMG_SIZE,\n            interpolation=cv2.INTER_CUBIC\n        )\ngray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\ngray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n# threshold the image, then perform a series of erosions +\n# dilations to remove any small regions of noise\nthresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\nthresh = cv2.erode(thresh, None, iterations=2)\nthresh = cv2.dilate(thresh, None, iterations=2)\n\n# find contours in thresholded image, then grab the largest one\ncnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\nc = max(cnts, key=cv2.contourArea)\n\n# find the extreme points\nextLeft = tuple(c[c[:, :, 0].argmin()][0])\nextRight = tuple(c[c[:, :, 0].argmax()][0])\nextTop = tuple(c[c[:, :, 1].argmin()][0])\nextBot = tuple(c[c[:, :, 1].argmax()][0])\n\n# add contour on the image\nimg_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n\n# add extreme points\nimg_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\nimg_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n\n# crop\nADD_PIXELS = 0\nnew_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n#new_img = cv2.cvtColor(new_img, cv2.COLOR_RGB2GRAY) #-transfer to grayscale\n#homo_filter = HomomorphicFilter(a = 0.9, b = 1.4) \n#new_img = homo_filter.filter(I=new_img, filter_params=[30,2])\n#new_img = cv2.filter2D(src=img, ddepth=-1, kernel=kernel)\n#new_img = cv2.equalizeHist(new_img)\n#new_img = cv2.cvtColor(new_img,cv2.COLOR_GRAY2RGB)\n","4eb74c91":"plt.figure(figsize=(15,6))\nplt.subplot(141)\nplt.imshow(img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 1. Get the original image')\nplt.subplot(142)\nplt.imshow(img_cnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 2. Find the biggest contour')\nplt.subplot(143)\nplt.imshow(img_pnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 3. Find the extreme points')\nplt.subplot(144)\nplt.imshow(new_img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 4. Crop the image')\nplt.show()","9acd02bc":"# apply this for each set\nX_train_crop = preprocessing_imgs(set_name=X_train)\nX_val_crop = preprocessing_imgs(set_name=X_val)\nX_test_crop = preprocessing_imgs(set_name=X_test)\nX_test_bmi_crop = preprocessing_imgs(set_name=X_test_bmi)\nX_test_bmd_crop = preprocessing_imgs(set_name=X_test_bmd)\nX_test_bdb_crop = preprocessing_imgs(set_name=X_test_bdb )","b35bf1f1":"def save_new_images(x_set, y_set, folder_name):\n    i = 0\n    for (img, imclass) in zip(x_set, y_set):\n        if imclass == 0:\n            cv2.imwrite(folder_name+'NO\/'+str(i)+'.jpg', img)\n        else:\n            cv2.imwrite(folder_name+'YES\/'+str(i)+'.jpg', img)\n        i += 1","b5b6239c":"# saving new images to the folder\n!mkdir TRAIN_CROP TEST_CROP VAL_CROP TEST_BMI_CROP TEST_BDB_CROP TEST_BMD_CROP TRAIN_CROP\/YES TRAIN_CROP\/NO TEST_CROP\/YES TEST_CROP\/NO VAL_CROP\/YES VAL_CROP\/NO TEST_BMI_CROP\/YES TEST_BMI_CROP\/NO TEST_BDB_CROP\/YES TEST_BDB_CROP\/NO TEST_BMD_CROP\/YES TEST_BMD_CROP\/NO\n\nsave_new_images(X_train_crop, y_train, folder_name='TRAIN_CROP\/')\nsave_new_images(X_val_crop, y_val, folder_name='VAL_CROP\/')\nsave_new_images(X_test_crop, y_test, folder_name='TEST_CROP\/')\nsave_new_images(X_test_bmi_crop, y_test_bmi, folder_name='TEST_BMI_CROP\/')\nsave_new_images(X_test_bmd_crop, y_test_bmd, folder_name='TEST_BMD_CROP\/')\nsave_new_images(X_test_bdb_crop, y_test_bdb, folder_name='TEST_BDB_CROP\/' ) ","c0812313":"def resize_imgs(set_name, img_size):\n    set_new = []\n    for img in set_name:\n        img = cv2.resize(\n            img,\n            dsize=img_size,\n            interpolation=cv2.INTER_CUBIC\n        )\n        set_new.append(preprocess_input(img))\n    return np.array(set_new)","853c7dc7":"X_train_crop_prep = resize_imgs(set_name=X_train_crop, img_size=IMG_SIZE)\nX_test_crop_prep = resize_imgs(set_name=X_test_crop, img_size=IMG_SIZE)\nX_val_crop_prep = resize_imgs(set_name=X_val_crop, img_size=IMG_SIZE)\nX_test_bmi_crop_prep = resize_imgs(set_name=X_test_bmi_crop, img_size=IMG_SIZE)\nX_test_bdb_crop_prep = resize_imgs(set_name=X_test_bdb_crop, img_size=IMG_SIZE)\nX_test_bmd_crop_prep = resize_imgs(set_name=X_test_bmd_crop, img_size=IMG_SIZE)","48f91f16":" plot_samples(X_train_crop_prep, y_train, labels, 30)","5f87009d":"# set the paramters we want to change randomly\ndemo_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    rescale=1.\/255,\n    shear_range=0.05,\n    brightness_range=[0.1, 1.5],\n    horizontal_flip=False,\n    vertical_flip=False\n)","84c3363f":"os.mkdir('preview')\nx = X_train_crop[0]  \nx = x.reshape((1,) + x.shape) \n\ni = 0\nfor batch in demo_datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='aug_img', save_format='jpg'):\n    i += 1\n    if i > 20:\n        break ","9a5856d7":"plt.imshow(X_train_crop[0])\nplt.xticks([])\nplt.yticks([])\nplt.title('Original Image')\nplt.show()\n\nplt.figure(figsize=(15,6))\ni = 1\nfor img in os.listdir('preview\/'):\n    img = cv2.cv2.imread('preview\/' + img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.subplot(3,7,i)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    i += 1\n    if i > 3*7:\n        break\nplt.suptitle('Augemented Images')\nplt.show()","742111a3":"!rm -rf preview\/","0b2c9ac9":"TRAIN_DIR = 'TRAIN_CROP\/'\nVAL_DIR = 'VAL_CROP\/'\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    brightness_range=[0.5, 1.5],\n    horizontal_flip=False,\n    vertical_flip=False,\n    preprocessing_function=preprocess_input\n)\n\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input\n)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=32,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)\n\n\nvalidation_generator = test_datagen.flow_from_directory(\n    VAL_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=16,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)","e9494eb7":"from tensorflow.keras.applications import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.preprocessing.image import *\nfrom tensorflow.keras.utils import *\nfrom keras import optimizers\n\nbase_Neural_Net= ResNet50(input_shape=(224,224,3), weights='imagenet', include_top=False)\nmodel=Sequential()\nmodel.add(base_Neural_Net)\nmodel.add(Flatten())\nmodel.add(BatchNormalization())\nmodel.add(Dense(256,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1,activation='sigmoid'))\n\nfor layer in base_Neural_Net.layers:\n    layer.trainable = False\n\n    \nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy' , 'AUC']\n)\n\nmodel.summary()","e5de6668":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,TensorBoard, ReduceLROnPlateau\nes = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=5)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.3, patience=2, min_denta=0.0001, mode='auto', verbose=1)\ntensorboard = TensorBoard(log_dir='logs')\ncheckpoint = ModelCheckpoint(\"ResNet_cropping+equalization.h5\", monitor='val_accuracy', save_best_only=True, mode='auto', verbose=1)","14689cfc":"import time\n\nstart = time.time()\n\nefficientnet_history = model.fit_generator(\n    train_generator,\n    epochs=30,\n    validation_data=validation_generator,\n    validation_steps=30,\n    callbacks=[es,tensorboard, checkpoint, reduce_lr]\n    )\n\nend = time.time()\nprint(end - start)","6f830d72":"# plot model performance\nacc = efficientnet_history.history['accuracy']\nval_acc = efficientnet_history.history['val_accuracy']\nloss = efficientnet_history.history['loss']\nval_loss = efficientnet_history.history['val_loss']\nepochs_range = range(1, len(efficientnet_history.epoch) + 1)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Train Set')\nplt.plot(epochs_range, val_acc, label='Val Set')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Train Set')\nplt.plot(epochs_range, val_loss, label='Val Set')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Model Loss')\n\nplt.tight_layout()\nplt.show()","6b3f0d85":"from tensorflow.keras.models import load_model\nResNet50V2 = load_model('.\/EfficientNetB2_cropping+equalization.h5')\nResNet50V2.summary()","80da8b08":"# validate on val set\npredictions = efficientnet.predict(X_val_crop_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_val, predictions)\nprint('Val Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_val, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","06eb5646":"# validate on test set\npredictions = efficientnet.predict(X_test_crop_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_test, predictions)\nprint('Test Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_test, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","61901022":"ind_list = np.argwhere((y_test == predictions) == False)[:, -1]\nprint\nif ind_list.size == 0:\n    print('There are no missclassified images.')\nelse:\n    for i in ind_list:\n        plt.figure()\n        plt.imshow(X_test_crop[i])\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(f'Actual class: {y_val[i]}\\nPredicted class: {predictions[i]}')\n        plt.show()","63dd0089":"# validate on test_bmi set with pre-processing methods\npredictions = efficientnet.predict(X_test_bmi_crop_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_test_bmi, predictions)\nprint('Val Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_test_bmi, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","620bab4e":"ind_list = np.argwhere((y_test_bmi == predictions) == False)[:, -1]\nprint\nif ind_list.size == 0:\n    print('There are no missclassified images.')\nelse:\n    for i in ind_list:\n        plt.figure()\n        plt.imshow(X_test_bmi_crop[i])\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(f'Actual class: {y_val[i]}\\nPredicted class: {predictions[i]}')\n        plt.show()","4bbf4bed":"# validate on test_bdd set with pre-processing methods\npredictions = efficientnet.predict(X_test_bdb_crop_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_test_bdb, predictions)\nprint('Val Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_test_bdb, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","36d22b9a":"ind_list = np.argwhere((y_test_bdb == predictions) == False)[:, -1]\nprint\nif ind_list.size == 0:\n    print('There are no missclassified images.')\nelse:\n    for i in ind_list:\n        plt.figure()\n        plt.imshow(X_test_bdb_crop[i])\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(f'Actual class: {y_val[i]}\\nPredicted class: {predictions[i]}')\n        plt.show()","4fd29b6f":"# validate on test_bmd set with pre-processing methods\npredictions = efficientnet.predict(X_test_bmd_crop_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_test_bmd, predictions)\nprint('Val Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_test_bmd, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","2a7bc025":"ind_list = np.argwhere((y_test_bmd == predictions) == False)[:, -1]\nprint\nif ind_list.size == 0:\n    print('There are no missclassified images.')\nelse:\n    for i in ind_list:\n        plt.figure()\n        plt.imshow(X_test_bmd_crop[i])\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(f'Actual class: {y_val[i]}\\nPredicted class: {predictions[i]}')\n        plt.show()","e9109329":"class GradCAM:\n    # Adapted with some modification from https:\/\/www.pyimagesearch.com\/2020\/03\/09\/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning\/\n    def __init__(self, model, layerName=None):\n        \"\"\"\n        model: pre-softmax layer (logit layer)\n        \"\"\"\n        self.model = model\n        self.layerName = layerName\n            \n        if self.layerName == None:\n            self.layerName = self.find_target_layer()\n    \n    def find_target_layer(self):\n        for layer in reversed(self.model.layers):\n            if len(layer.output_shape) == 4:\n                return layer.name\n        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM\")\n            \n    def compute_heatmap(self, image, classIdx, upsample_size, eps=1e-5):\n        gradModel = Model(\n            inputs = [self.model.inputs],\n            outputs = [self.model.get_layer(self.layerName).output, self.model.output]\n        )\n        # record operations for automatic differentiation\n        \n        with tf.GradientTape() as tape:\n            inputs = tf.cast(image, tf.float32)\n            (convOuts, preds) = gradModel(inputs) # preds after softmax\n            loss = preds[:,classIdx]\n        \n        # compute gradients with automatic differentiation\n        grads = tape.gradient(loss, convOuts)\n        # discard batch\n        convOuts = convOuts[0]\n        grads = grads[0]\n        norm_grads = tf.divide(grads, tf.reduce_mean(tf.square(grads)) + tf.constant(eps))\n        \n        # compute weights\n        weights = tf.reduce_mean(norm_grads, axis=(0,1))\n        cam = tf.reduce_sum(tf.multiply(weights, convOuts), axis=-1)\n        \n        # Apply reLU\n        cam = np.maximum(cam, 0)\n        cam = cam\/np.max(cam)\n        cam = cv2.resize(cam, upsample_size,interpolation=cv2.INTER_LINEAR)\n        \n        # convert to 3D\n        cam3 = np.expand_dims(cam, axis=2)\n        cam3 = np.tile(cam3, [1,1,3])\n        \n        return cam3\n    \ndef overlay_gradCAM(img, cam3):\n    cam3 = np.uint8(255*cam3)\n    cam3 = cv2.applyColorMap(cam3, cv2.COLORMAP_JET)\n    \n    new_img = 0.3*cam3 + 0.5*img\n    \n    return (new_img*255.0\/new_img.max()).astype(\"uint8\")","fb96cb6b":"@tf.custom_gradient\ndef guidedRelu(x):\n    def grad(dy):\n        return tf.cast(dy>0,\"float32\") * tf.cast(x>0, \"float32\") * dy\n    return tf.nn.relu(x), grad\n\n# Reference: https:\/\/github.com\/eclique\/keras-gradcam with adaption to tensorflow 2.0  \nclass GuidedBackprop:\n    def __init__(self,model, layerName=None):\n        self.model = model\n        self.layerName = layerName\n        self.gbModel = self.build_guided_model()\n        \n        if self.layerName == None:\n            self.layerName = self.find_target_layer()\n\n    def find_target_layer(self):\n        for layer in reversed(self.model.layers):\n            if len(layer.output_shape) == 4:\n                return layer.name\n        raise ValueError(\"Could not find 4D layer. Cannot apply Guided Backpropagation\")\n\n    def build_guided_model(self):\n        gbModel = Model(\n            inputs = [self.model.inputs],\n            outputs = [self.model.get_layer(self.layerName).output]\n        )\n        layer_dict = [layer for layer in gbModel.layers[1:] if hasattr(layer,\"activation\")]\n        for layer in layer_dict:\n            if layer.activation == tf.keras.activations.relu:\n                layer.activation = guidedRelu\n        \n        return gbModel\n    \n    def guided_backprop(self, images, upsample_size):\n        \"\"\"Guided Backpropagation method for visualizing input saliency.\"\"\"\n        with tf.GradientTape() as tape:\n            inputs = tf.cast(images, tf.float32)\n            tape.watch(inputs)\n            outputs = self.gbModel(inputs)\n\n        grads = tape.gradient(outputs, inputs)[0]\n\n        saliency = cv2.resize(np.asarray(grads), upsample_size)\n\n        return saliency\n\ndef deprocess_image(x):\n    \"\"\"Same normalization as in:\n    https:\/\/github.com\/fchollet\/keras\/blob\/master\/examples\/conv_filter_visualization.py\n    \"\"\"\n    # normalize tensor: center on 0., ensure std is 0.25\n    x = x.copy()\n    x -= x.mean()\n    x \/= (x.std() + K.epsilon())\n    x *= 0.25\n\n    # clip to [0, 1]\n    x += 0.5\n    x = np.clip(x, 0, 1)\n\n    # convert to RGB array\n    x *= 255\n    if K.image_data_format() == 'channels_first':\n        x = x.transpose((1, 2, 0))\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x","04e801ee":"def show_gradCAMs(model, gradCAM, GuidedBP, im_ls, n=3, decode={}):\n    \"\"\"\n    model: softmax layer\n    \"\"\"\n    random.shuffle(im_ls)\n    plt.subplots(figsize=(30, 10*n))\n    k=1\n    for i in range(n):\n        img = cv2.imread(os.path.join(IMAGE_DIR,im_ls[i]))\n        upsample_size = (img.shape[1],img.shape[0])\n        if (i+1) == len(df):\n            break\n        # Show original image\n        plt.subplot(n,3,k)\n        plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n        plt.title(\"Filename: {}\".format(im_ls[i]), fontsize=20)\n        plt.axis(\"off\")\n        # Show overlayed grad\n        plt.subplot(n,3,k+1)\n        im = img_to_array(load_img(os.path.join(IMAGE_DIR,im_ls[i]), target_size=(W,H)))\n        x = np.expand_dims(im, axis=0)\n        x = preprocess_input(x)\n        preds = model.predict(x)\n        idx = preds.argmax()\n        if len(decode)==0:\n            res = decode_predictions(preds)[0][0][1:]\n        else:\n            res = [decode[idx],preds.max()]\n        cam3 = gradCAM.compute_heatmap(image=x, classIdx=idx, upsample_size=upsample_size)\n        new_img = overlay_gradCAM(img, cam3)\n        new_img = cv2.cvtColor(new_img, cv2.COLOR_BGR2RGB)\n        plt.imshow(new_img)\n        plt.title(\"GradCAM - Pred: {}. Prob: {}\".format(res[0],res[1]), fontsize=20)\n        plt.axis(\"off\")\n        \n        # Show guided GradCAM\n        plt.subplot(n,3,k+2)\n        gb = GuidedBP.guided_backprop(x, upsample_size)\n        guided_gradcam = deprocess_image(gb*cam3)\n        guided_gradcam = cv2.cvtColor(guided_gradcam, cv2.COLOR_BGR2RGB)\n        plt.imshow(guided_gradcam)\n        plt.title(\"Guided GradCAM\", fontsize=20)\n        plt.axis(\"off\")\n        \n        k += 3\n    plt.show()","ce95054f":"gradCAM = GradCAM(model=ResNet50V2, layerName=\"conv5_block3_out\")\nguidedBP = GuidedBackprop(model=ResNet50V2,layerName=\"conv5_block3_out\")","b5676010":"show_gradCAMs(resnet50, gradCAM,guidedBP,dogs, n=5)","f6db350c":"#### Data Import and Preprocessing","9a324ff1":"#### Data augmentation","759d2d08":"#### Example of image preprocessing","d7f91af4":"#### Distribution of classes among sets","e79aacc5":"#### All images have a different ratio of height and width and may look strange when resized to (224, 224) later. Histogram of ratio distributions (ratio = width\/height):","e4c4f542":"#### Splitting the data into train, val and test folders","ba194ec6":"#### Kernels to sharpen the image","5e0b8c51":"#### Model Building","55e594ae":"#### Homomorphic filter - used to sharpen the image, equalize its \"illumination\" and normalize","fcea8dd3":"#### Setting up the Environment"}}