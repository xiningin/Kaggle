{"cell_type":{"bc52e182":"code","9839ea62":"code","2f86ce12":"code","1dd692f5":"code","b07eb50d":"code","c4589e99":"code","de53cffa":"code","81e5ad40":"code","963e18de":"code","387a0443":"code","5fbd3203":"code","95db6de1":"code","c2b0ec94":"code","2982bfdf":"code","14c7131b":"code","a4667135":"code","587a6c5c":"code","34c72318":"code","8985e69a":"code","d5d8002e":"code","7f150b8f":"code","32f06c88":"code","60fb41ce":"code","1b43cd45":"code","8baf0f0c":"code","9f856162":"code","622e7187":"code","868dc3ef":"code","c39e04f0":"code","8f0fc20e":"code","ee2bd95d":"code","a170d23a":"code","e346f25e":"code","9c3634c2":"code","c92a3bcb":"code","7392e67e":"code","fc8b5df7":"markdown","428be858":"markdown","50ab00e7":"markdown","b370c8b4":"markdown","01674b9b":"markdown","574eaf51":"markdown","86120789":"markdown","d5981061":"markdown","eb19883d":"markdown","d1c93606":"markdown","d92a5c86":"markdown","c166bffc":"markdown","6d205783":"markdown"},"source":{"bc52e182":"# Load in libraries \nimport re\nimport csv\nimport codecs\nimport numpy as np\nimport pandas as pd\nimport operator\nimport string\n\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\nfrom collections import defaultdict\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\neng_stopwords = set(stopwords.words(\"english\"))\nimport sys\n\nimport gensim\nfrom gensim import corpora, models\nfrom gensim.corpora.dictionary import Dictionary","9839ea62":"#Read in metadata \ndata = pd.read_csv(\"..\/input\/CORD-19-research-challenge\/metadata.csv\") \n\n#Select the columns that you need \nmeta_data = data[[\"cord_uid\",\"sha\", \"title\", \"abstract\"]]\n#check na\nmeta_data.isna().sum()\n\n#Remove NA \ncomplete_cases = meta_data.dropna()\nprint(complete_cases.isna().sum())\ncomplete_cases.shape","2f86ce12":"def tokenize(text):\n    '''\n    Convert the text corpus to lower case, remove all punctuations and numbers which lead to\n    a final cleaned corpus with only tokens where all characters in the string are alphabets.\n    '''\n    # convert the text to lower case and replace all new line characters by an empty string\n    lower_text = text.lower().replace('\\n', ' ')\n    # replace all the punctuations in text by an empty string\n    table = str.maketrans('', '', string.punctuation)\n    punct_text = lower_text.translate(table)\n    # use NLTK's word tokenization to tokenize the text \n    # remove numbers and empty tokens, only keep tokens with all characters in the string are alphabets\n    tokens = [word for word in word_tokenize(punct_text) if word.isalpha()]\n    return tokens","1dd692f5":"def remove_stopwords(word_list, sw=stopwords.words('english')):\n    \"\"\" \n    Filter out all stop words from the text corpus.\n    \"\"\"\n    # It is important to keep words like no and not. Since the meaning of the text will change oppositely\n    # if they are removed.\n    if 'not' in sw:\n        sw.remove('not')\n    if 'no' in sw:\n        sw.remove('no')\n    \n    cleaned = []\n    for word in word_list:\n        if word not in sw:\n            cleaned.append(word)\n    return cleaned","b07eb50d":"def stem_words(word_list):\n    stemmer = SnowballStemmer('english')\n    stemmed_words = [stemmer.stem(word) for word in word_list]\n    text = \" \".join(stemmed_words)\n    return stemmed_words, text","c4589e99":"def preprocess(text):\n    \"\"\"\n    Combine all preprocess steps together.\n    Clean each text into tokenized stemmed word list and also return concatenated string\n    \"\"\"\n    tokenized = tokenize(text)\n    stopword_removed = remove_stopwords(tokenized)\n    tokenized_str, cleaned_str = stem_words(stopword_removed)\n    return stopword_removed, tokenized_str, cleaned_str","de53cffa":"# clean titles\ntitles = complete_cases[\"title\"].values.tolist()\ntokenized_titles = []\nstr_titles = []\nword_dict = {}\nfor title in titles:\n  result = preprocess(title)\n  tokenized = result[0]\n  stemmed = result[1]\n  for i in range(0,len(stemmed)):\n    if stemmed[i] not in word_dict:\n      word_dict[stemmed[i]] = tokenized[i]\n  tokenized_titles.append(stemmed)\n  str_titles.append(result[2])\n    \nprint(tokenized_titles[0])\nprint(str_titles[0])\nprint(titles[0])\nprint(len(word_dict))  # there are 23209 unique words in titles ","81e5ad40":"# clean abstracts\nabstracts = complete_cases[\"abstract\"].values.tolist()\ntokenized_abstracts = []\nstr_abstracts = []\nword_dict_abstract = {}\nfor abstract in abstracts:\n  result = preprocess(abstract)\n  tokenized = result[0]\n  stemmed = result[1]\n  for i in range(0,len(stemmed)):\n    if stemmed[i] not in word_dict_abstract:\n      word_dict_abstract[stemmed[i]] = tokenized[i]\n  tokenized_abstracts.append(stemmed)\n  str_abstracts.append(result[2])\n    \nprint(tokenized_abstracts[0])\nprint(str_abstracts[0])\nprint(abstracts[0])\nprint(len(word_dict_abstract))  # there are 92667 unique words in abstracts","963e18de":"complete_cases[\"tokenized_abstract\"] = tokenized_abstracts\ncomplete_cases[\"cleaned_abstracts\"] = str_abstracts\n# append to data\ncomplete_cases[\"tokenized_title\"] = tokenized_titles\ncomplete_cases[\"cleaned_titles\"] = str_titles","387a0443":"# Use the Vectorizer to identify the keywords with highest frequency\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')\n%matplotlib inline\n\n# Helper function\ndef plot_10_most_common_words(count_data, count_vectorizer):\n    import matplotlib.pyplot as plt\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts+=t.toarray()[0]\n    \n    count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words)) \n    \n    plt.figure(2, figsize=(15, 15\/1.6180))\n    plt.subplot(title='10 most common words')\n    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n    sns.barplot(x_pos, counts, palette='husl')\n    plt.xticks(x_pos, words, rotation=90) \n    plt.xlabel('words')\n    plt.ylabel('counts')\n    plt.show()\n    \n# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n\n# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(complete_cases['cleaned_titles'])\n\n# Visualise the 10 most common words\nplot_10_most_common_words(count_data, count_vectorizer)","5fbd3203":"del_list = ('virus', 'infect','respiratori','coronavirus','diseas','cell','protein','viral','influenza')\ntokenized_titles = complete_cases['tokenized_title']\ntokenized_titles2 = [[ele for ele in sub if ele not in del_list] for sub in tokenized_titles]","95db6de1":"dictionary = gensim.corpora.Dictionary(tokenized_titles2)\nbow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_titles2]","c2b0ec94":"tfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]\nfrom pprint import pprint\nfor doc in corpus_tfidf:\n    pprint(doc)\n    break","2982bfdf":"lda_model_tfidf = gensim.models.ldamodel.LdaModel(corpus_tfidf, num_topics=10, id2word=dictionary)\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","14c7131b":"for idx, topic in lda_model_tfidf.show_topics(formatted=False, num_words= 10):\n    print('Topic: {} \\nWords: {}'.format(idx, [w[0] for w in topic]))","a4667135":"from gensim.models import CoherenceModel\nlda_cm = CoherenceModel(model = lda_model_tfidf, corpus = corpus_tfidf, dictionary= dictionary, texts = tokenized_titles2, coherence = \"c_v\")\nlda_CM = lda_cm.get_coherence()\nlda_CM","587a6c5c":"wordDict = Dictionary(tokenized_titles2)\nabCorpus = [wordDict.doc2bow(text) for text in tokenized_titles2]","34c72318":"from gensim.models import HdpModel\nhdp = HdpModel(corpus = abCorpus, id2word = wordDict)\nhdp_topics = hdp.print_topics()\nfor topic in hdp_topics: \n    print(topic)","8985e69a":"for idx, topic in hdp.show_topics(formatted=False, num_words= 10):\n    print('Topic: {} \\nWords: {}'.format(idx, [w[0] for w in topic]))","d5d8002e":"from gensim.models import CoherenceModel\nhdp_cm = CoherenceModel(model = hdp, corpus = abCorpus, dictionary= wordDict, texts = tokenized_titles2, coherence = \"c_v\")\nhdp_CM = hdp_cm.get_coherence()\nhdp_CM","7f150b8f":"# Keyword Search\nselection = ['contagion','contagious','shedding','transmission','transmittable','airbourne','incubation','asymptomatic', 'seasonality']\nselection_stemmed = stem_words(selection)\n# 'detection','antibody','bolvine','immunizaton','infection','hydrophilic','hydrophobic','calves','environmental' \n    \n# Scan with abstract\naScan = complete_cases.tokenized_abstract.apply(lambda x: any(item for item in selection if item in x))\ncomplete_cases3 = complete_cases[aScan]\nprint(complete_cases3.shape)\n\n# Scan with title \ntScan = complete_cases.tokenized_title.apply(lambda x: any(item for item in selection if item in x))\ncomplete_cases2 = complete_cases[tScan]\nprint(complete_cases2.shape)","32f06c88":"\"\"\"Read in the full text and filter to the selected list based on keyword search\"\"\" \n\"\"\"posted code to generate csv: https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\"\"\"\ntokenized_txt = pd.read_csv(\"..\/input\/tokenized-text\/text_tokens.csv\") \nsmList = pd.DataFrame(complete_cases3)\nfiltered_df = pd.DataFrame(pd.merge(tokenized_txt,smList, left_on = \"paper_id\", right_on = \"sha\"))\nfiltered_df.shape","60fb41ce":"\"\"\"Create extended stopwords list (Ref- https:\/\/www.kaggle.com\/vrushank12\/covid-analysis-using-doc2vec-and-sent2vec)\"\"\" \n# pos_words and extend words are some common words to be removed from abstract\nstop_words = nltk.corpus.stopwords.words('english')\npos_words = ['highest','among','either','seven','six','plus','strongest','worst','doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n,'greatest','every','better','per','across','throughout','except','fewer','trillion','fewest','latest','least','manifest','unlike','eight','since','toward','largest','despite','via','finest','besides','easiest','must','million','oldest','behind','outside','smaller','nest','longest','whatever','stronger','worse','two','another','billion','best','near','nine','around','nearest','wechat','lowest','smallest','along','higher','three','older','greater','neither','inside','newest','lower','may','although','though','earlier','upon','five','ca','larger','us','whether','beyond','onto','might','one','out','unless','four','whose','can','fastest','without','ecobooth','broadest','easier','within','like', 'could','biggest','bigger','would','thereby','yet','timely','thus','also','avoid','know','usually','time','year','go','welcome','even','date',\n             'used', 'following', 'go', 'instead', 'fundamentally', 'first', 'second', 'alone',\n               'everything', 'end', 'also', 'year', 'made', 'many', 'towards', 'truly', 'last','introduction', 'abstract', 'section', 'edition', 'chapter','and', 'the', 'is', 'any', 'to', 'by', 'of', 'on','or', 'with', 'which', 'was','be','we', 'are', 'so',\n                    'for', 'it', 'in', 'they', 'were', 'as','at','such', 'no', 'that', 'there', 'then', 'those',\n                    'not', 'all', 'this','their','our', 'between', 'have', 'than', 'has', 'but', 'why', 'only', 'into',\n                    'during', 'some', 'an', 'more', 'had', 'when', 'from', 'its', \"it's\", 'been', 'can', 'further',\n                    'above', 'before', 'these', 'who', 'under', 'over', 'each', 'because', 'them', 'where', 'both',\n                     'just', 'do', 'once', 'through', 'up', 'down', 'other', 'here', 'if', 'out', 'while', 'same',\n                    'after', 'did', 'being', 'about', 'how', 'few', 'most', 'off', 'should', 'until', 'will', 'now',\n                    'he', 'her', 'what', 'does', 'itself', 'against', 'below', 'themselves','having', 'his', 'am', 'whom',\n                    'she', 'nor', 'his', 'hers', 'too', 'own', 'ma', 'him', 'theirs', 'again', 'doing', 'ourselves',\n                     're', 'me', 'ours', 'ie', 'you', 'your', 'herself', 'my', 'et', 'al', 'may', 'due', 'de',\n                     'one','two', 'three', 'four', 'five','six','seven','eight','nine','ten', 'however',\n                     'i', 'ii', 'iii','iv','v', 'vii', 'viii', 'ix', 'x', 'xi', 'xii','xiii', 'xiv' \n               'often', 'called', 'new', 'date', 'fully', 'thus', 'new', 'include', 'http', \n               'www','doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et',\n               'al', 'author', 'figure','rights', 'reserved', 'permission', 'used', 'using', 'biorxiv',\n               'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI','-PRON-']\n\nstop_words.extend(pos_words)\n","1b43cd45":"filtered_df['body_text_processed'] = filtered_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\nfiltered_df['body_text_processed_1'] = filtered_df['body_text_processed'].str.replace('[^\\w\\s]','')","8baf0f0c":"filtered_tokenized_doc = []\nfor d in filtered_df['body_text_processed_1']:\n    filtered_tokenized_doc.append(word_tokenize(d.lower()))","9f856162":"# find most similar doc \n#Read in model \nimport numpy\nimport gensim\nfrom gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\ntagged_data = [gensim.models.doc2vec.TaggedDocument(d, [i]) for i, d in enumerate(filtered_tokenized_doc)]","622e7187":"max_epochs = 100\nvec_size = 100\nalpha = 0.025\n\nmodel = Doc2Vec(vector_size=vec_size,\n                alpha=alpha, \n                min_alpha=0.00005,\n                min_count=1,\n                dm =0,\n                dbow_words = 1)\n  \nmodel.build_vocab(tagged_data)\n\nfor epoch in range(max_epochs):\n    print('iteration {0}'.format(epoch))\n    model.train(tagged_data,\n                total_examples=model.corpus_count,\n                epochs=model.iter)\n    # decrease the learning rate\n    model.alpha -= 0.0002\n    # fix the learning rate, no decay\n    model.min_alpha = model.alpha","868dc3ef":"\"\"\"Subtopic 1: Transmission\"\"\"\n\nimport heapq\nimport re\nmod = model\ntest_doc = word_tokenize(\"transmission\".lower())\nsimilar=mod.docvecs.most_similar(positive=[mod.infer_vector(test_doc)],topn=10)\n\ndef Extract(lst): \n    return list(list(zip(*lst))[0])\ntemp=Extract(similar)\n\ntreated_text = filtered_df.iloc[temp].iloc[0:,5].str.cat(sep=', ')\ntreated_text = re.sub(r'\\[[0-9]*\\]', ' ', treated_text)\ntreated_text = re.sub(r'\\s+', ' ', treated_text)\nformatted_article_text = re.sub('[^a-zA-Z]', ' ', treated_text)\nformatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\nsentence_list = nltk.sent_tokenize(treated_text)\n\nstopwords = nltk.corpus.stopwords.words('english')\n\nword_frequencies = {}\nfor word in nltk.word_tokenize(formatted_article_text):\n    if word not in stopwords:\n        if word not in word_frequencies.keys():\n            word_frequencies[word] = 1\n        else:\n            word_frequencies[word] += 1\nmaximum_frequncy = max(word_frequencies.values())\n\nfor word in word_frequencies.keys():\n    word_frequencies[word] = (word_frequencies[word]\/maximum_frequncy)\nsentence_scores = {}\nfor sent in sentence_list:\n    for word in nltk.word_tokenize(sent.lower()):\n        if word in word_frequencies.keys():\n            if len(sent.split(' ')) < 30:\n                if sent not in sentence_scores.keys():\n                    sentence_scores[sent] = word_frequencies[word]\n                else:\n                    sentence_scores[sent] += word_frequencies[word]\n\nsummary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n\nsummary = ' '.join(summary_sentences)\nsummary = summary.replace(\"Wang et al.\", \"\").replace(\"Chen et al.\", \"\").replace(\"Liu et al.\", \"\").replace(\"Prakash et al.\", \"\")\n\nprint(summary)","c39e04f0":"\"\"\"Subtopic 1: Incubation\"\"\"\ntest_doc = word_tokenize(\"contagious\".lower())\nsimilar=mod.docvecs.most_similar(positive=[mod.infer_vector(test_doc)],topn=10)\n\ndef Extract(lst): \n    return list(list(zip(*lst))[0])\ntemp=Extract(similar)\n\ntreated_text = filtered_df.iloc[temp].iloc[0:,5].str.cat(sep=', ')\ntreated_text = re.sub(r'\\[[0-9]*\\]', ' ', treated_text)\ntreated_text = re.sub(r'\\s+', ' ', treated_text)\nformatted_article_text = re.sub('[^a-zA-Z]', ' ', treated_text)\nformatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\nsentence_list = nltk.sent_tokenize(treated_text)\n\nstopwords = nltk.corpus.stopwords.words('english')\n\nword_frequencies = {}\nfor word in nltk.word_tokenize(formatted_article_text):\n    if word not in stopwords:\n        if word not in word_frequencies.keys():\n            word_frequencies[word] = 1\n        else:\n            word_frequencies[word] += 1\nmaximum_frequncy = max(word_frequencies.values())\n\nfor word in word_frequencies.keys():\n    word_frequencies[word] = (word_frequencies[word]\/maximum_frequncy)\nsentence_scores = {}\nfor sent in sentence_list:\n    for word in nltk.word_tokenize(sent.lower()):\n        if word in word_frequencies.keys():\n            if len(sent.split(' ')) < 30:\n                if sent not in sentence_scores.keys():\n                    sentence_scores[sent] = word_frequencies[word]\n                else:\n                    sentence_scores[sent] += word_frequencies[word]\n\nsummary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n\nsummary = ' '.join(summary_sentences)\nsummary = summary.replace(\"Wang et al.\", \"\").replace(\"Chen et al.\", \"\").replace(\"Liu et al.\", \"\").replace(\"Prakash et al.\", \"\")\nprint(summary)","8f0fc20e":"\"\"\"Subtopic 3: Infection\"\"\"\ntest_doc = word_tokenize(\"infection\".lower())\nsimilar=mod.docvecs.most_similar(positive=[mod.infer_vector(test_doc)],topn=10)\n\ndef Extract(lst): \n    return list(list(zip(*lst))[0])\ntemp=Extract(similar)\n\ntreated_text = filtered_df.iloc[temp].iloc[0:,5].str.cat(sep=', ')\ntreated_text = re.sub(r'\\[[0-9]*\\]', ' ', treated_text)\ntreated_text = re.sub(r'\\s+', ' ', treated_text)\nformatted_article_text = re.sub('[^a-zA-Z]', ' ', treated_text)\nformatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\nsentence_list = nltk.sent_tokenize(treated_text)\n\nstopwords = nltk.corpus.stopwords.words('english')\n\nword_frequencies = {}\nfor word in nltk.word_tokenize(formatted_article_text):\n    if word not in stopwords:\n        if word not in word_frequencies.keys():\n            word_frequencies[word] = 1\n        else:\n            word_frequencies[word] += 1\nmaximum_frequncy = max(word_frequencies.values())\n\nfor word in word_frequencies.keys():\n    word_frequencies[word] = (word_frequencies[word]\/maximum_frequncy)\nsentence_scores = {}\nfor sent in sentence_list:\n    for word in nltk.word_tokenize(sent.lower()):\n        if word in word_frequencies.keys():\n            if len(sent.split(' ')) < 30:\n                if sent not in sentence_scores.keys():\n                    sentence_scores[sent] = word_frequencies[word]\n                else:\n                    sentence_scores[sent] += word_frequencies[word]\n\nsummary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n\nsummary = ' '.join(summary_sentences)\nprint(summary)","ee2bd95d":"from nltk.corpus import stopwords\nfrom nltk.cluster.util import cosine_distance\nimport numpy as np\nimport networkx as nx\n\ndef sentence_similarity(sent1, sent2, stopwords=None):\n    if stopwords is None:\n        stopwords = []\n \n    sent1 = [w.lower() for w in sent1]\n    sent2 = [w.lower() for w in sent2]\n \n    all_words = list(set(sent1 + sent2))\n \n    vector1 = [0] * len(all_words)\n    vector2 = [0] * len(all_words)\n \n    # build the vector for the first sentence\n    for w in sent1:\n        if w in stopwords:\n            continue\n        vector1[all_words.index(w)] += 1\n \n    # build the vector for the second sentence\n    for w in sent2:\n        if w in stopwords:\n            continue\n        vector2[all_words.index(w)] += 1\n \n    return 1 - cosine_distance(vector1, vector2) ","a170d23a":"def build_similarity_matrix(sentences, stop_words):\n    # Create an empty similarity matrix\n    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n \n    for idx1 in range(len(sentences)):\n        for idx2 in range(len(sentences)):\n            if idx1 == idx2: #ignore if both are same sentences\n                continue \n            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n            return similarity_matrix","e346f25e":"def generate_summary(sentences, top_n=5):\n    stop_words = stopwords.words('english')\n    summarize_text = []\n    # Step 1 - Generate Similary Martix across sentences\n    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n    # Step 2 - Rank sentences in similarity martix\n    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n    scores = nx.pagerank(sentence_similarity_graph)\n    # Step 3 - Sort the rank and pick top sentences\n    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n    for i in range(top_n):\n        summarize_text.append(\"\".join(ranked_sentence[i][1]))\n        # Step 4 - print summarized text\n    print(\"Summarize Text: \\n\", \".\".join(summarize_text))","9c3634c2":"\"\"\"Subtopic: Transmission\"\"\"\ntest_doc = word_tokenize(\"transmission channel\".lower())\nsimilar=mod.docvecs.most_similar(positive=[mod.infer_vector(test_doc)],topn=10)\ntemp=Extract(similar)\n\ntreated_text = filtered_df.iloc[temp].iloc[0:,5].str.cat(sep=', ')\ntreated_text = re.sub(r'\\[[0-9]*\\]', ' ', treated_text)\ntreated_text = re.sub(r'\\s+', ' ', treated_text)\nformatted_article_text = re.sub('[^a-zA-Z]', ' ', treated_text)\nformatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\nsentence_list = nltk.sent_tokenize(treated_text)\n\n\ngenerate_summary(sentence_list, 10)","c92a3bcb":"\"\"\"Subtopic: Incubation\"\"\"\ntest_doc = word_tokenize(\"duration of incubation period\".lower())\nsimilar=mod.docvecs.most_similar(positive=[mod.infer_vector(test_doc)],topn=10)\ntemp=Extract(similar)\n\ntreated_text = filtered_df.iloc[temp].iloc[0:,5].str.cat(sep=', ')\ntreated_text = re.sub(r'\\[[0-9]*\\]', ' ', treated_text)\ntreated_text = re.sub(r'\\s+', ' ', treated_text)\nformatted_article_text = re.sub('[^a-zA-Z]', ' ', treated_text)\nformatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\nsentence_list = nltk.sent_tokenize(treated_text)\n\n\ngenerate_summary(sentence_list, 10)","7392e67e":"\"\"\"Subtopic: Contagious\"\"\"\ntest_doc = word_tokenize(\"contagious\".lower())\nsimilar=mod.docvecs.most_similar(positive=[mod.infer_vector(test_doc)],topn=10)\ntemp=Extract(similar)\n\ntreated_text = filtered_df.iloc[temp].iloc[0:,5].str.cat(sep=', ')\ntreated_text = re.sub(r'\\[[0-9]*\\]', ' ', treated_text)\ntreated_text = re.sub(r'\\s+', ' ', treated_text)\nformatted_article_text = re.sub('[^a-zA-Z]', ' ', treated_text)\nformatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\nsentence_list = nltk.sent_tokenize(treated_text)\n\n\ngenerate_summary(sentence_list, 10)","fc8b5df7":"### Experiment shows that keyword search allow us to filter down to articles most related to our task\n- Scanned both title and abstract to see which one yields better result \n- Result shows that we get more articles through abstract.","428be858":"# What is known about transmission, incubation, and environmental stability?\n\n1. Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\n2. Prevalence of asymptomatic shedding and transmission (e.g., particularly children).\n3. Seasonality of transmission.\n4. Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\n5.  Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\n6. Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\n7. Natural history of the virus and shedding of it from an infected person\n8. Implementation of diagnostics and products to improve clinical processes\n9. Disease models, including animal models for infection, disease and transmission\n10. Tools and studies to monitor phenotypic change and potential adaptation of the virus\n11. Immune response and immunity\n12. Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\n13. Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\n14. Role of the environment in transmission","50ab00e7":"#### Note\n- Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic.\n- C_v measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity\n- LDA outperforms in this instance ","b370c8b4":"# LDA Model \n- Filter out the most common words that might create noise in LDA model \n- build tfidf model to ensure the LDA model we build uses words that are important instead of just words that are of high frequency\n- We also tried building an LDA model with abstract but that yields worse result","01674b9b":"# Plot the 10 most common words within titles \n\n- This can help us identify any words we want to remove to reduce noise in topic modeling \n- reference: https:\/\/towardsdatascience.com\/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0","574eaf51":"### Lets look for summary in articles related to transmission \n- We are trying 2 methods: 1) Weighted word frequency ; 2) Cosine Similarity","86120789":"# HDP Model \n- https:\/\/medium.com\/@oyewusiwuraola\/exploring-topic-modelling-with-gensim-on-the-essential-science-indicators-journals-list-1dc4d9f96d9c","d5981061":"** Created by a TransUnion data scientist that believes that information can be used to change our world for the better. #InformationForGood**","eb19883d":"### Use the functions to clean but titles and abstracts","d1c93606":"### Approach 1: Weighted Word Frequency","d92a5c86":"### Approach 2: Cosine Similarity\n-https:\/\/towardsdatascience.com\/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70","c166bffc":"## Read in meta data and write preprocessing functions for it","6d205783":"# Train Word2Vec Model"}}