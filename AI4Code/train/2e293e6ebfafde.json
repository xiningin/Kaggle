{"cell_type":{"c6c8327b":"code","1db44c0d":"code","e192539d":"code","a7159efe":"code","3a176e4a":"code","dd013b1a":"code","45a973f6":"code","0f11d018":"code","6ef339cf":"code","8e253452":"code","0cd83b57":"code","cfa7f92e":"code","14d922db":"code","e67c2851":"code","109b8599":"code","63ed889c":"code","a2d80705":"code","67c7a693":"code","2c9bfb9c":"code","48a75d9d":"code","9af7e28d":"code","7d9fbab8":"markdown","6f2aff4d":"markdown","d046516b":"markdown","3bc8de76":"markdown","e662d8cf":"markdown","4d87f9f7":"markdown","7354cea7":"markdown","c8782246":"markdown","ab006367":"markdown","3f6fc646":"markdown","cc8109cf":"markdown","076578af":"markdown","54b9ff72":"markdown","3c8aa928":"markdown","ce43099f":"markdown","71e1c9c3":"markdown","f8c6a437":"markdown","6f4f577a":"markdown","bd9f0dda":"markdown","216374c1":"markdown","37494e9e":"markdown","e165ee41":"markdown"},"source":{"c6c8327b":"#import some necessary librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom hyperopt import hp\nfrom hyperopt import tpe\nfrom hyperopt import Trials\nfrom hyperopt import fmin\nfrom hyperopt import STATUS_OK\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom xgboost import XGBRegressor\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)","1db44c0d":"df_train  = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","e192539d":"df_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)","a7159efe":"#We make the transformation on both train and test sets\ndf = pd.concat([df_train.drop('SalePrice', axis=1),df_test])\ndf = df.set_index('Id')\n\n#When no pool, we put 'na'\ndf[[\"PoolQC\"]] = df[[\"PoolQC\"]].fillna('na')\n\n#When no miscFeature, we put 'na'\ndf[[\"MiscFeature\"]] = df[[\"MiscFeature\"]].fillna('na')\n\n#When noAlley, we put 'na'\ndf[[\"Alley\"]] = df[[\"Alley\"]].fillna('na')\n\n#When noFence, we put 'na'\ndf[[\"Fence\"]] = df[[\"Fence\"]].fillna('na')\n\n#When noFirePlace, we put 'na'\ndf[[\"FireplaceQu\"]] = df[[\"FireplaceQu\"]].fillna('na')\n\n#Linear regressions for \"Lot Frontage\" based on 'LotArea'\nlfront = df[[\"LotArea\",\"LotFrontage\"]].dropna()\nmodel = LinearRegression().fit(lfront[[\"LotArea\"]],lfront[\"LotFrontage\"])\nclfront = model.coef_\nilfront = model.intercept_\nu = df[\"LotFrontage\"]\nv = ilfront + clfront * df.loc[u.isnull(),[\"LotArea\"]]\ndf.loc[u.isnull(),[\"LotFrontage\"]].index\nv[\"LotFrontage\"] = v[\"LotArea\"]\ndf.update(v[\"LotFrontage\"])\n\n#Update for the nogarage types, with 'na' and 0, as explained in the data description\ndf[[\"GarageCond\"]] = df[[\"GarageCond\"]].fillna('na')\ndf[[\"GarageQual\"]] = df[[\"GarageQual\"]].fillna('na')\ndf[[\"GarageYrBlt\"]] = df[[\"GarageYrBlt\"]].fillna('na')\ndf[[\"GarageFinish\"]] = df[[\"GarageFinish\"]].fillna('na')\ndf[[\"GarageType\"]] = df[[\"GarageType\"]].fillna('na')\ndf[[\"GarageCars\"]] = df[[\"GarageCars\"]].fillna(0)\ndf[[\"GarageArea\"]] = df[[\"GarageArea\"]].fillna(0)\n\n#Basement conditions - Again we can fill 'na' as explained in the text\ndf[[\"BsmtExposure\"]] = df[[\"BsmtExposure\"]].fillna('na')\ndf[[\"BsmtCond\"]] = df[[\"BsmtCond\"]].fillna('na')\ndf[[\"BsmtQual\"]] = df[[\"BsmtQual\"]].fillna('na')\ndf[[\"BsmtFinType2\"]] = df[[\"BsmtFinType2\"]].fillna('na')\ndf[[\"BsmtFinType1\"]] = df[[\"BsmtFinType1\"]].fillna('na')\n\n#Massonery type\ndf[[\"MasVnrType\"]] = df[[\"MasVnrType\"]].fillna('na')\ndf[[\"MasVnrArea\"]] = df[[\"MasVnrArea\"]].fillna(0)\n\n\n\ndf['MSZoning'] = df['MSZoning'].fillna(df['MSZoning'].mode()[0])\n\ndf['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])\n\ndf['KitchenQual'] = df['KitchenQual'].fillna(df['KitchenQual'].mode()[0])\n\ndf['Exterior1st'] = df['Exterior1st'].fillna(df['Exterior1st'].mode()[0])\ndf['Exterior2nd'] = df['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0])\n\n\n\n#For the other nan values, that are a small minority, I just put 0\ndf = df.fillna(value=0)\n\n#We will also change some categorical feature with a notion of order into integer. This will reduce the number of features \n#in the one-hot-encoding and improve the speed of our XGBoost algorithm.\nchange_dict = {'LotShape':{'Reg':0,\n                           'IR1':1,\n                           'IR2':2,\n                           'IR3':3},\n              'LandSlope':{'Gtl':0,\n                           'Mod':1,\n                           'Sev':2},\n              'ExterQual':{'Ex':5,\n                           'Gd':4,\n                           'TA':3,\n                           'Fa':2,\n                           'Po':1},\n               'ExterCond':{'Ex':5,\n                           'Gd':4,\n                           'TA':3,\n                           'Fa':2,\n                           'Po':1},\n               'BsmtQual':{'Ex':5,\n                           'Gd':4,\n                           'TA':3,\n                           'Fa':2,\n                           'Po':1,\n                           'na':0},\n               'BsmtCond':{'Ex':5,\n                           'Gd':4,\n                           'TA':3,\n                           'Fa':2,\n                           'Po':1,\n                           'na':0},\n               'BsmtExposure':{'Gd':5,\n                           'Av':4,\n                           'Mn':3,\n                           'No':2,\n                           'na':1,\n                            0:0},\n               'BsmtFinType1':{'GLQ':7,\n                               'ALQ':6,\n                               'BLQ':5,\n                               'Rec':4,\n                               'LwQ':3,\n                               'Unf':2,\n                               'na':1},\n               'BsmtFinType2':{'GLQ':7,\n                               'ALQ':6,\n                               'BLQ':5,\n                               'Rec':4,\n                               'LwQ':3,\n                               'Unf':2,\n                               'na':1},\n               'HeatingQC':{'Ex':5,\n                           'Gd':4,\n                           'TA':3,\n                           'Fa':2,\n                           'Po':1},\n               'CentralAir':{'N':0,\n                             'Y':1},\n               'KitchenQual':{'Ex':5,\n                           'Gd':4,\n                           'TA':3,\n                           'Fa':2,\n                           'Po':1,\n                           'na':0,\n                             0:0},\n               'Functional':{'Typ':7,\n                             'Min1':6,\n                             'Min2':5,\n                             'Mod':4,\n                             'Maj1':3,\n                             'Maj2':2,\n                             'Sev':1,\n                             'Sal':0,\n                              0:0},\n               'FireplaceQu':{'Ex':5,\n                           'Gd':4,\n                           'TA':3,\n                           'Fa':2,\n                           'Po':1,\n                           'na':0,\n                           'no':0},\n              'GarageFinish':{'Fin':4,\n                              'RFn':3,\n                              'Unf':2,\n                              'na':1},\n              'GarageQual':{'Ex':5,\n                           'Gd':4,\n                           'TA':3,\n                           'Fa':2,\n                           'Po':1,\n                           'na':0,\n                           'no':0},\n              'GarageCond':{'Ex':5,\n                           'Gd':4,\n                           'TA':3,\n                           'Fa':2,\n                           'Po':1,\n                           'na':0,\n                           'no':0},\n              'PoolQC':{'Ex':5,\n                           'Gd':4,\n                           'TA':3,\n                           'Fa':2,\n                           'Po':1,\n                           'na':0,\n                           'no':0},\n               'Fence':{'GdPrv':4,\n                        'MnPrv':3,\n                        'GdWo':2,\n                        'MnWw':1,\n                        'na':0},\n              }\n\n\nfor k in change_dict.keys():\n    df[k] = df[k].apply(lambda x:change_dict[k][x])\n\n#OverallQual and OverallCond is an a quality score, we can keep it as an integer\ndf['OverallQual'] = df['OverallQual'].apply(lambda x:int(x))\ndf['GarageYrBlt'] = df['GarageYrBlt'].apply(lambda x:int(x) if x!='na' else 1900)\n\n\n#Changing OverallCond into a categorical variable\ndf['OverallCond'] = df['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\ndf['YrSold'] = df['YrSold'].astype(str)\ndf['MoSold'] = df['MoSold'].astype(str)\n\n\n\ndf = pd.get_dummies(df)\ndf.shape\n","3a176e4a":"from scipy import stats\nfrom scipy.stats import skew\n\n#All numerical features\nnumeric_feats = df.dtypes[(df.dtypes == np.int64) | (df.dtypes == np.float64)].index\n\n# Check the skew of all numerical features\nskewed_feats = df[numeric_feats].apply(lambda x: np.abs(skew(x))).sort_values(ascending=False)\nskew_df = pd.DataFrame({'skew_val' :skewed_feats})\nskew_df.head(10)\n\nplt.figure(figsize=(15,10))\nplt.title(\"Skewness of numerical variables\")\nplt.xticks(rotation=90)\nplt.bar(skew_df.index,skew_df.skew_val.values)\nplt.show()\n\n#sk_level can be an hyperparameter of the problem, let put it to one\nsk_level = 0.75\n\n#We log-transform features with a skew level above 1\nskew_feats = skew_df[skew_df.skew_val > sk_level].index\n\nfor feat in skew_feats:\n    df[feat] = np.log1p(df[feat])","dd013b1a":"X = df[:len(df_train)].values\ny = np.log(df_train['SalePrice'].values)","45a973f6":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\ndef cv(model, X, y, cv = 5, seed=1989):\n    \"\"\"Our cross-validation function\"\"\"\n    \n    #List of losses\n    losses = []\n    \n    #Define the kfold object, with a number of split cv, a random_seed to keep always same splits and with shuffle = True\n    #meaning the sets are shuffled before splitting\n    kf = KFold(n_splits=cv, shuffle=True, random_state=seed)\n    pred = np.zeros(len(y))\n    #kf returns a list of (train\/test) indexes. We will loop over this list, train a model, and get each score\n    for train_index, test_index in kf.split(X):\n        \n        #We define X_train, X_test, y_train and y_test\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        #We train our model. We evaluate it for this fold with X_test and y_test. \n        #Once the model didn't improve for 100 rounds, we stop the model and take the best iteration\n        model.fit(X_train,y_train, verbose=False)\n        \n        pred[test_index] = model.predict(X_test)\n        \n    return np.sqrt(mean_squared_error(pred,y))\n        ","0f11d018":"import time\n#We generate our model. We can take a huge n_estimators as there will be early stopping\n#We also use our custom loss function for the evaluation of the model\n\n#To evaluate the time taken to train the model\nt1 = time.time()\n\nxgbr1 = XGBRegressor()\n\n#We set a seed in the parameters to have consistent results\nparams = {'seed' : 1989,\n          'n_estimators' :  2000,\n          'feval' : 'rmse',\n          'n_jobs': 4}\nxgbr1.set_params(**params)\n\n#We check the score using our crossval function\nrmse_score = np.round(cv(xgbr1,X,y,5),4)\n\nt2 = time.time()\n\nprint(f\"Score of XGBoost vanilla : {rmse_score}\")\nprint(f'Model trained in : {np.round(t2-t1)} s')","6ef339cf":"# Define the domain space to search for global minimum\nspace = {\n    'boosting_type': hp.choice('boosting_type', \n                               [{'boosting_type': 'gbdt', \n                                    'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n                                 {'boosting_type': 'goss'}]),\n    'num_leaves': hp.quniform('num_leaves', 10, 400, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 1000),\n    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 1),\n    'reg_alpha': hp.loguniform('reg_alpha', np.log(0.001), np.log(1.0)),\n    'reg_lambda': hp.loguniform('reg_lambda', np.log(0.001), np.log(1.0)),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n    'n_jobs':4,\n    'n_estimators': 2000,\n    'seed':1989\n}","8e253452":"trials = Trials()","0cd83b57":"# Algorithm\ntpe_algorithm = tpe.suggest","cfa7f92e":"def objective(params, n_folds = 5):\n    \"\"\"Objective function for Gradient Boosting Hyperparameter Tuning\"\"\"\n    \n    #Our XGBRegressor\n    model = XGBRegressor()\n    \n    #We set the parameters of the model\n    model.set_params(**params)\n    \n    #We calculate the loss with our cv function\n    loss = cv(model , X , y , n_folds)\n    \n    #We return a dictionnary for hyperopt framework\n    return {'loss': loss, 'params': params, 'status': STATUS_OK}","14d922db":"# Optimize\nt1=time.time()\n\noptimized = fmin(fn = objective,\n                 space = space,\n                 algo = tpe.suggest,\n                 max_evals = 100,\n                 trials = trials)\n\nt2 = time.time()\n\nprint(f'Model trained in : {np.round(t2-t1)} s')","e67c2851":"#We get back the parameters from our optimization\nparams = trials.best_trial['result']['params']\nprint(params)","109b8599":"xgbr2 = XGBRegressor()\n\nxgbr2.set_params(**params)\n\n#We check the score using our crossval function\nrmse_score = np.round(cv(xgbr2,X,y,5),4)\n\nprint(f\"Score of XGBoost after optimization : {rmse_score}\")","63ed889c":"#I modify my dictionnary a bit here, with 3 keys, I will define the spaces from my xgboost, my lasso and my ponderation\nspace2 = {'xgb':{'boosting_type': hp.choice('boosting_type',\n                                           [{'boosting_type': 'gbdt', \n                                            'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n                                            {'boosting_type': 'goss'}]),\n                'num_leaves': hp.quniform('num_leaves', 10, 400, 1),\n                'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n                'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 1000),\n                'min_child_samples': hp.quniform('min_child_samples', 20, 500, 1),\n                'reg_alpha': hp.loguniform('reg_alpha', np.log(0.001), np.log(1.0)),\n                'reg_lambda': hp.loguniform('reg_lambda', np.log(0.001), np.log(1.0)),\n                'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n                'n_jobs':4,\n                'n_estimators': 2000,\n                'seed':1989},\n         'lasso':{'alpha': hp.loguniform('alpha', np.log(0.001), np.log(1.0))},\n         'pond' : hp.uniform('pond',0,1)\n        }\n\n# Algorithm\ntpe_algorithm2 = tpe.suggest\n\n# History\ntrials2 = Trials()","a2d80705":"def objective2(space, n_folds = 5):\n    \"\"\"Objective function for Gradient Boosting Hyperparameter Tuning\"\"\"\n    \n    #Our XGBRegressor\n    xgb = XGBRegressor()\n    #Lasso\n    lasso = Lasso()\n    \n    #We set the parameters of the xgb that are stored in space['xgb']\n    xgb.set_params(**space['xgb'])\n    \n    #Same with lasso\n    lasso.set_params(**space['lasso'])\n    \n    #We also take back our ponderation coefficient\n    pond = space['pond']\n\n    #We redefine manually the crossvalidation are we are using several models here\n    kf = KFold(n_splits=n_folds, shuffle=True, random_state=1989)\n    pred = np.zeros(len(y))\n    \n    #kf returns a list of (train\/test) indexes. We will loop over this list, train a model, and get each score\n    for train_index, test_index in kf.split(X):\n        \n        #We define X_train, X_test, y_train and y_test\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        #We train the two models. And we make the prediction using our ponderation. \n        xgb.fit(X_train,y_train, verbose=False)\n        lasso.fit(X_train,y_train)\n\n        #We ponderate the prediction of the two models\n        pred[test_index] += pond*xgb.predict(X_test)\n        pred[test_index] += (1-pond)*lasso.predict(X_test)\n    \n    #Once our prediction vector is calcutated for all the folds, we can evaluate the loss\n    loss = np.sqrt(mean_squared_error(pred,y))\n\n    \n    #We return a dictionnary for hyperopt framework\n    return {'loss': loss, 'params': space, 'status': STATUS_OK}\n","67c7a693":"# Optimize\nt1=time.time()\n\noptimized = fmin(fn = objective2, space = space2, algo = tpe_algorithm2, \nmax_evals = 500, trials = trials2)\n\nt2 = time.time()\n\nprint(f'Model trained in : {np.round(t2-t1)} s')","2c9bfb9c":"#We get back the parameters from our optimization\nparams = trials2.best_trial['result']['params']","48a75d9d":"X_test = df[len(df_train):].values\n\n#Our XGBRegressor\nxgb = XGBRegressor()\n#Lasso\nlasso = Lasso()\n\n#We set the parameters of the xgb that are stored in space['xgb']\nxgb.set_params(**params['xgb'])\n\n#Same with lasso\nlasso.set_params(**params['lasso'])\n\n#We also take back our ponderation coefficient\npond = params['pond']\n\npred = np.zeros(len(X_test))\n\n#We train the two models. And we make the prediction using our ponderation. \nxgb.fit(X,y, verbose=False)\nlasso.fit(X,y)\n\n#We ponderate the prediction of the two models\npred += pond*xgb.predict(X_test)\npred += (1-pond)*lasso.predict(X_test)\n\n#We remove the logarithm form of the prediction\npred = np.exp(pred)","9af7e28d":"my_submission = pd.DataFrame({'Id': df_test.Id, 'SalePrice': pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","7d9fbab8":"# Hyperparameter search with model combination \n\nWith this method, it is also possible to had extra hyper-parameters. Let's say we want to combine the prediction of our XGBoost with a Lasso Linear Regression and ponderate the two predictions. The ponderation will be also an hyper-parameter of the problem","6f2aff4d":"## Data import\n\nWe are using Pandas to import the datas","d046516b":"### K-fold preparation\n\nLet's prepare a custom function for our cross validation based on KFold from sklearn.\nThis function will make the cross-validation for a custom XGBRegression. As XGBRegression allows it, we will use the early_stopping method to prevent overfitting ","3bc8de76":"# Bayesian Optimization\n\nBayesian optimization algorithms are finding the next set of hyperparameter bases on what they saw in the past and shall converge much faster to an optimum.\n\nI will implement the solution presented in this article using the hyperopt library.\nhttps:\/\/towardsdatascience.com\/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a\n\nIn order to use this optimisation algorithm, we need for things : \n* An objective function, that the algorithm will try to minimize\n* A dictionnary where we store all the ranges for each parameters in the hyperopt format, in other words, the domain space\n* A optimization algorithm\n* An history dictionnary to store the results of the different iterations\n\n","e662d8cf":"## Basic XGBRegressor scoring\n\nLet's start by training a XGBRegressor without doing any hyper-parameters tunin","4d87f9f7":"## History storage\n\nTo store the result, we use the Trials object from hyperopt.","7354cea7":"This will be the final set of parameters I will use here for my prediction.","c8782246":"## XGBoost Training","ab006367":"With only 100 tryied, we already improved the model a bit. Just as a recall, with 100 tryied, we could have perform a gridsearch on 4 parameters with only 3 values for each...","3f6fc646":"## Domain Space\n\nWe can define the domain space using predifine distributions from hyperopt. The domain space has to be stored in a dictionnary according to hyperopt documentation. Later we will be able to acces or store this dictionnary with the optimized parametered","cc8109cf":"## Objective function\n\nIn this function we will set up an XGBoost model that we will run as before. We calculate the loss as before.\n","076578af":"# Dealing with skewed features","54b9ff72":"# Final prediction","3c8aa928":"## Outlier removing\n\nAs explained in Serigne Kernel, there is 2 big outliers that we shall remove as they might have a huge impact on the predictions. You can refer to its Kernel for more informations.\nhttps:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard","ce43099f":"## Feature Engineering \n\nAnother part that has been explored on a lot of kernel, so I will not spend so much time on it here","71e1c9c3":"# Hyper-parameter tuning with Bayesian method\n\nTunning the hyper-parameters of your ML algorithms can be very time and ressources consumming. \n\nUntil recently, I was mainly using GridSearch or RandomizedSearch in order to fine-tune my algorithms, but these approaches are not optimal, and especially not when the number of hyper-parameters  and the training time increase. \n\nIn this Kernel you will find another method, based on Bayesian methods.\nIf you want to go deeper in the subject, I found all the information to build my model in this great article :\nhttps:\/\/towardsdatascience.com\/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a\n\nAlso, i'd like to thanks Serigne for its awesome Kernel that helped me a lot on this problem at first. \nhttps:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n\nAs many work has been done on visualisation or Feature Engineering, I will not focus on that here.","f8c6a437":"Let's also transform some categorical values with a ranking into numerical values","6f4f577a":"Dealing with skew values is very important for some ML models such that NN or Linear Regression that are bases on normal distribution hypothesis. \n\nFor Boosting algorithms, this step is not mandatory","bd9f0dda":"Once finished, or even if you interrupt the process in the middle, you will find the best parameters stored in your Trial object.","216374c1":"We got a score of 0.1181 without any cross_validation, which is really not bad !\nIt took us 29s to go through all the cross validation process.\n\nWe could for sure improve the score by doing some hyper-parameter optimisation.\nFor this, the first thing we can think about is the **GridSearch**\n\nThe GridSearch is a kind of \"bruteforce\" method. You give it all the range of parameters you want to try, and it tries them all, keeping the one giving the best scoring at the end.\n\nFor quick algorithms with few hyper-parameters like the Lasso, it will work very well. But for XGboost it will not be the case because of the high number of parameters. ","37494e9e":"## Optimization algorithm\n\nWe use the built-in function Tree Parzen Estimator algorithm to minimize our objective function","e165ee41":"## Optimization\n\nNow we can just run the algorithm using the fmin function of hyperopt to find our hyperparameters :)\nWe will try 100 models. Given that it takes approx 30s to run a 2000 iteration model, it will take approximativaly 1hr to go through all the loop.\nTo get even better accuracy, we would have to train the algorithm over more iterations."}}