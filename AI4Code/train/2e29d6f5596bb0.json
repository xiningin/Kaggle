{"cell_type":{"451bebb3":"code","544a888a":"code","474d3517":"code","0257f9d8":"code","cb08324f":"code","ba93e861":"code","a83625ef":"code","9ca2d791":"code","0fee58e1":"code","d4e8cb37":"code","aa52cb2c":"code","ea906dac":"code","ad38902b":"code","087c7a13":"code","544bad34":"code","5bc0bffe":"code","397f0a16":"code","ebf73bd2":"code","0af19046":"code","5794bc25":"code","fbcfbbbd":"code","b29321e7":"code","a240b0eb":"code","19878c61":"code","4eaf9c07":"code","626bc9ff":"code","3c99d73a":"code","8057ac0c":"code","d1c5c57e":"code","08adf0f1":"code","f0f419f8":"code","637c1f24":"code","15a8fedb":"code","b9b55ee4":"code","9902aafe":"code","bea414bc":"code","2468d538":"code","d86f5252":"code","f8a166e3":"markdown","235c3121":"markdown","eafcf344":"markdown","a819ce24":"markdown","92994531":"markdown","418278c4":"markdown","825ee23b":"markdown","34ed224a":"markdown","976c672b":"markdown","998275a3":"markdown","94fd01bc":"markdown","b41fadd0":"markdown","ce497e43":"markdown","ced985ce":"markdown","1cd57045":"markdown","fd66a28f":"markdown","136e131b":"markdown","be4bc894":"markdown","f7c8a1bd":"markdown","68c0f386":"markdown","ad978bef":"markdown"},"source":{"451bebb3":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport numpy as np \nimport pandas as pd \nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB,BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score,ConfusionMatrixDisplay,precision_score,recall_score,f1_score,classification_report,roc_curve,plot_roc_curve,auc,precision_recall_curve,plot_precision_recall_curve,average_precision_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder","544a888a":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()","474d3517":"df.info()","0257f9d8":"df.isnull().sum()","cb08324f":"f, ax = plt.subplots(nrows = 1, ncols = 1, figsize=(16,5))\n\nsns.heatmap(df.T.isna(), cmap='Blues')\nax.set_title('Missing Values')\n\nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\nplt.show()","ba93e861":"import missingno as msno\nmsno.bar(df)\nplt.show()","a83625ef":"df.bmi.replace(to_replace=np.nan, value=df.bmi.mean(),inplace=True)","9ca2d791":"df.describe()","0fee58e1":"# compute the corr matrix\n\ncorr = df.corr()\n\n# generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr,dtype=bool))\n\n# set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11,9))\n\n# generate a custom diverging colormap\ncmap = sns.diverging_palette(230,20,as_cmap=True)\n\n#draw the heatpmap with the mask and correct aspect ratio\nsns.heatmap(corr,mask=mask,cmap=cmap,vmax=.3,center=0,square=True,linewidths=.5,cbar_kws={'shrink':.5})","d4e8cb37":"print(df.gender.value_counts())\nsns.set_theme(style='darkgrid')\nax = sns.countplot(data=df,x='gender')","aa52cb2c":"print(df.ever_married.value_counts())\nsns.set_theme(style='darkgrid')\nax = sns.countplot(data=df, x='ever_married')","ea906dac":"print(df.work_type.value_counts())\nsns.set_theme(style='darkgrid')\nax = sns.countplot(data=df,x='work_type')","ad38902b":"print(df.Residence_type.value_counts())\nsns.set_theme(style='darkgrid')\nax = sns.countplot(data=df,x='Residence_type')","087c7a13":"print(df.smoking_status.value_counts())\nsns.set_theme(style='darkgrid')\nax = sns.countplot(data=df,x='smoking_status')","544bad34":"print(df.stroke.value_counts())\nsns.set_theme(style='darkgrid')\nax = sns.countplot(data=df,x='stroke')","5bc0bffe":"fig = plt.figure(figsize=(7,7))\nsns.distplot(df.avg_glucose_level,color='green',label='avg_glucose_level',kde=True)\nplt.legend()","397f0a16":"fig = plt.figure(figsize=(7,7))\nsns.distplot(df.bmi,color='orange',label='bmi',kde=True)\nplt.legend()","ebf73bd2":"plt.figure(figsize=(7,7))\nsns.distplot(df[df['stroke'] == 0]['bmi'],color='green')\nsns.distplot(df[df['stroke'] == 1]['bmi'],color='red')\n\nplt.title('No Stroke vs Stroke by BMI',fontsize=15)\nplt.xlim([10,100])","0af19046":"plt.figure(figsize=(12,10))\nsns.distplot(df[df['stroke'] == 0]['avg_glucose_level'],color='green')\nsns.distplot(df[df['stroke'] == 1]['avg_glucose_level'],color='red')\nplt.title('No Stroke vs Stroke by Avg Glucose Level',fontsize=15)\nplt.xlim([30,330])","5794bc25":"plt.figure(figsize=(12,10))\nsns.distplot(df[df['stroke'] == 0]['age'],color='green')\nsns.distplot(df[df['stroke'] == 1]['age'],color='red')\nplt.title('No Stroke vs Stroke by Age',fontsize=15)\nplt.xlim([18,100])","fbcfbbbd":"fig = plt.figure(figsize=(7,7))\ngraph = sns.scatterplot(data=df,x='age',y='bmi',hue='gender')\ngraph.axhline(y=25, linewidth=4,color='r',linestyle= '--')","b29321e7":"fig = plt.figure(figsize=(7,7))\ngraph = sns.scatterplot(data=df,x='age',y='avg_glucose_level',hue='gender')\ngraph.axhline(y=150,linewidth=4,color='r',linestyle='--')","a240b0eb":"plt.figure(figsize=(13,13))\nsns.set_theme(style='darkgrid')\nplt.subplot(2,3,1)\nsns.violinplot(x='gender',y='stroke',data=df)\nplt.subplot(2,3,2)\nsns.violinplot(x='hypertension',y='stroke',data=df)\nplt.subplot(2,3,3)\nsns.violinplot(x='heart_disease',y='stroke',data=df)\nplt.subplot(2,3,4)\nsns.violinplot(x='ever_married',y='stroke',data=df)\nplt.subplot(2,3,5)\nsns.violinplot(x='ever_married',y='stroke',data=df)\nplt.subplot(2,3,6)\nsns.violinplot(x='Residence_type',y='stroke',data=df)\nplt.show()","19878c61":"fig = plt.figure(figsize=(10,10))\nsns.pairplot(df)\nplt.show()","4eaf9c07":"df.head()","626bc9ff":"le = LabelEncoder()\ndf['gender'] = le.fit_transform(df['gender'])\ndf['ever_married'] = le.fit_transform(df['ever_married'])\ndf['work_type'] = le.fit_transform(df['work_type'])\ndf['Residence_type'] = le.fit_transform(df['Residence_type'])\ndf['smoking_status'] = le.fit_transform(df['smoking_status'])","3c99d73a":"x = df.iloc[:,1:-1].values\ny = df.iloc[:,-1].values\n\nprint('X Shape', x.shape)\nprint('Y Shape',y.shape)","8057ac0c":"ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0,5,9])],remainder='passthrough')\nx = np.array(ct.fit_transform(x))","d1c5c57e":"X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)\n\nprint('Number transations x_train df',X_train.shape)\nprint('Number transations x_test df',X_test.shape)\nprint('Number transations y_train df',y_train.shape)\nprint('Number transations y_test df',y_test.shape)","08adf0f1":"# SMOTE \n# pip install imblearn\n# from imblearn.over_sampling import SMOTE\n\nprint('Before OverSampling, counts of label 1: {}'.format(sum(y_train==1)))\nprint('Before OverSampling, counts of label 0: {} \\n'.format(sum(y_train==0)))","f0f419f8":"sm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_resample(X_train,y_train.ravel())\n\nprint('After OverSampling, the shape of train_x: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {}'.format(y_train_res.shape))\n\nprint('After OverSampling, counts of label 1: {}'.format(sum(y_train_res == 1)))\nprint('After OverSampling, counts of label 0: {}'.format(sum(y_train_res == 0)))","637c1f24":"models = []\nmodels.append(['XGBClassifier',XGBClassifier(learning_rate=0.1,objective='binary:logistic',random_state=0,eval_metric='mlogloss')])\nmodels.append(['Logistic Regression',LogisticRegression(random_state=0)])\nmodels.append(['SVM',SVC(random_state=0)])\nmodels.append(['KNeigbors',KNeighborsClassifier()])\nmodels.append(['GaussianNB',GaussianNB()])\nmodels.append(['BernoulliNB',BernoulliNB()])\nmodels.append(['DecisionTree',DecisionTreeClassifier(random_state=0)])\nmodels.append(['RandomForest',RandomForestClassifier(random_state=0)])\nmodels.append(['AdaBoostClassifier',AdaBoostClassifier()])","15a8fedb":"lst_1 = []\nfor m in range(len(models)):\n    lst_2 = []\n    model = models[m][1]\n    model.fit(X_train_res,y_train_res)\n    y_pred = model.predict(X_test)\n    cm = confusion_matrix(y_test,y_pred)\n    accuracies = cross_val_score(estimator= model, X = X_train_res,y = y_train_res, cv=10)\n\n# k-fOLD Validation\n    roc = roc_auc_score(y_test,y_pred)\n    precision = precision_score(y_test,y_pred)\n    recall = recall_score(y_test,y_pred)\n    f1 = f1_score(y_test,y_pred)\n    print(models[m][0],':')\n    print(cm)\n    print('Accuracy Score: ',accuracy_score(y_test,y_pred))\n    print('')\n    print('K-Fold Validation Mean Accuracy: {:.2f} %'.format(accuracies.mean()*100))\n    print('')\n    print('Standard Deviation: {:.2f} %'.format(accuracies.std()*100))\n    print('')\n    print('ROC AUC Score: {:.2f} %'.format(roc))\n    print('')\n    print('Precision: {:.2f} %'.format(precision))\n    print('')\n    print('Recall: {:.2f} %'.format(recall))\n    print('')\n    print('F1 Score: {:.2f} %'.format(f1))\n    print('-'*40)\n    print('')\n    lst_2.append(models[m][0])\n    lst_2.append(accuracy_score(y_test,y_pred)*100)\n    lst_2.append(accuracies.mean()*100)\n    lst_2.append(accuracies.std()*100)\n    lst_2.append(roc)\n    lst_2.append(precision)\n    lst_2.append(recall)\n    lst_2.append(f1)\n    lst_1.append(lst_2)","b9b55ee4":"df2 = pd.DataFrame(lst_1,columns=['Model','Accuracy','K-Fold Mean Accuracy','Std.Deviation','ROC_AUC','Precision','Recall','F1 Score'])\n\ndf2.sort_values(by=['Accuracy','K-Fold Mean Accuracy'],inplace=True,ascending=False)\ndf2\n\n# COMPARE","9902aafe":"sns.barplot(x='Accuracy',y='Model',data=df2,color='b')\nplt.title('Model Compare Graphic');","bea414bc":"grid_models = [(XGBClassifier(), [{'learning_rate': [0.01, 0.05, 0.1], 'eval_metric': ['error']}]),\n               (KNeighborsClassifier(),[{'n_neighbors':[5,7,8,10], 'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}]), \n               (DecisionTreeClassifier(),[{'criterion':['gini','entropy'],'random_state':[0]}]), \n               (RandomForestClassifier(),[{'n_estimators':[100,150,200],'criterion':['gini','entropy'],'random_state':[0]}])]","2468d538":"for i,j in grid_models:\n    grid = GridSearchCV(estimator=i,param_grid = j, scoring = 'accuracy',cv = 10)\n    grid.fit(X_train_res,y_train_res)\n    best_accuracy = grid.best_score_\n    best_param = grid.best_params_\n    print(' {}: \\n Best Accuracy: {:.2f} %'.format(i,best_accuracy*100))\n    print('')\n    print('-'*25)\n    print('')","d86f5252":"classifier = XGBClassifier(eval_metric= 'error', learning_rate= 0.1)\nclassifier.fit(X_train_res, y_train_res)\ny_pred = classifier.predict(X_test)\ny_prob = classifier.predict_proba(X_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","f8a166e3":"#### 2.6 PairPlot","235c3121":"#### 3.3 Column Transformator and OneHotEncoder ","eafcf344":"#### 3.2 X and Y Splitting","a819ce24":"## 3. Data Preprocessing","92994531":"<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h2>Stroke Prediction<\/h2>\n\n<h3>1) Introduction<\/h3>\n<li>1.1. Importing Libraries<\/li>\n<li>1.2. Importing Dataset<\/li>\n<li>1.3. Missing Values<\/li>\n\n<h3>2) Data Visualization<\/h3>\n<li>2.1. Corr Heat Map<\/li>\n<li>2.2. Count Plot<\/li>\n<li>2.3. Distibution Plot<\/li>\n<li>2.4. Scatter Plot<\/li>\n<li>2.5. Violin Plot<\/li>\n<li>2.6. Pair Plot<\/li>\n    \n<h3>3) Data Preprocessing<\/h3>\n<li>3.1. Label Encoder<\/li>\n<li>3.2. X and Y Splitting<\/li>\n<li>3.3. Column Transformator and OneHotEncoder<\/li>\n<li>3.4. Train Test Split<\/li>\n<li>3.5. Smote<\/li>\n\n<h3>4) Model Selection<\/h3>\n<h3>5) Model Tuning<\/h3>\n<\/div>","418278c4":"#### 2.4 Scatter Plot","825ee23b":"#### 3.4 Train Test Split","34ed224a":"*We fill the missing values in the Body Mass Index variable with the average value.*","976c672b":"#### 1.2 Importing Dataset","998275a3":"## 4. Model Selection","94fd01bc":"#### 3.1 Label Encoder","b41fadd0":"#### 2.3 Distribution Plot","ce497e43":"## 2. Data Visualization","ced985ce":"## 1) Introduction\n#### 1.1 Importing Libraries","1cd57045":"#### 1.3 Missing Values","fd66a28f":"####  2.1 Corr Heat Map [](http:\/\/)","136e131b":"## 5. Model Tuning","be4bc894":"#### 3.5 Smote","f7c8a1bd":"#### 2.5 Violin Plot","68c0f386":"***Thank you for looking. Please provide your comments.***","ad978bef":"#### 2.2 Count Plot"}}