{"cell_type":{"c791e15d":"code","e337145a":"code","631f80fa":"code","16a2b5f1":"code","f8a3e01b":"code","5e383f20":"code","c9578e2e":"code","87aff6b3":"code","b87e33cf":"code","fb21d3f9":"code","0191e67f":"code","0d1fa5fe":"code","32c81c81":"code","3c151b14":"code","5dfc68b0":"code","87298710":"code","8b21d0d0":"code","98255ea8":"code","27e34f22":"code","f7785ff8":"code","f16014a7":"markdown","19ff9318":"markdown","fe58bd96":"markdown","24a78f76":"markdown","a8732adf":"markdown","24307559":"markdown","b33ef708":"markdown","0358655e":"markdown","4cd99834":"markdown","0ae86be5":"markdown","eaf5f0ba":"markdown","66d73561":"markdown","3c8c85d1":"markdown","86cf4e8b":"markdown"},"source":{"c791e15d":"# import sys\n# !cp ..\/input\/rapids\/rapids.0.18.0 \/opt\/conda\/envs\/rapids.tar.gz\n# !cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\n# sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\n# sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\n# sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n# !cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","e337145a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport cv2\nfrom joblib import dump, load\nfrom tqdm.notebook import tqdm\nimport re\nimport nltk\nfrom nltk.stem.porter import *\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom tqdm.notebook import tqdm\n# from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom cuml.neighbors import NearestNeighbors","631f80fa":"path = '..\/input\/shopee-product-matching'\ntrain_path = '..\/input\/shopee-product-matching\/train_images'\ntest_path = '..\/input\/shopee-product-matching\/test_images'\ncleaned_data_path = '..\/input\/cleaned-shopee-data-with-ocr\/cleaned_title_and_ocr_sw.csv'","16a2b5f1":"data = pd.read_csv(cleaned_data_path)\ndata.drop('Unnamed: 0',axis=1,inplace=True)\ndata.head()","f8a3e01b":"s_stemmer = SnowballStemmer(language='english')\nwords = data['cleaned_title'].iloc[4587].split()\nlemmatizer = WordNetLemmatizer()\nfor word in words:\n    print(word+' --> '+s_stemmer.stem(word))\n    print(word+' --> '+lemmatizer.lemmatize(word))","5e383f20":"data_train = pd.read_csv(cleaned_data_path)","c9578e2e":"tqdm.pandas()\n","87aff6b3":"data_train['len_title'] = data_train['cleaned_title'].progress_apply(lambda x: len(x))\ndata_train['word_count_title'] = data_train['cleaned_title'].progress_apply(lambda x: len(x.split()))\ndata_train['len_ocr'] = data_train['cleaned_ocr_text'].progress_apply(lambda x: len(str(x)))\ndata_train['word_count_ocr'] = data_train['cleaned_ocr_text'].progress_apply(lambda x: len(str(x).split()))","b87e33cf":"data_train['avg_word_length_title'] = data_train['len_title']\/data_train['word_count_title']\ndata_train['avg_word_length_ocr'] = data_train['len_ocr']\/data_train['word_count_ocr']","fb21d3f9":"def n_gram_count(text,n):\n    word_vectorizer = CountVectorizer(ngram_range=(n,n), analyzer='word', stop_words=None,max_df=0.8)\n    if len(text.split()) == 1:\n        return 1\n    if len(text.split()) == 0:\n        return 0\n    print(text)\n    sparse_matrix = word_vectorizer.fit_transform([text])\n    frequencies = len(sum(sparse_matrix).toarray()[0])\n    return frequencies","0191e67f":"data_train = cudf.DataFrame.from_pandas(data_train)","0d1fa5fe":"import gc\n_ = gc.collect()","32c81c81":"tfidf_vec = TfidfVectorizer(stop_words='english', \n                            binary=True, \n#                             max_df = 0.5,\n#                             min_df = 2\n                           )\ntitle_embeddings = tfidf_vec.fit_transform(data_train['cleaned_title']).toarray().astype(np.float32)\ntitle_embeddings.shape","3c151b14":"tfidf_vec_2 = TfidfVectorizer(stop_words='english', \n                            binary=True, \n#                             max_df = 0.5,\n#                             min_df = 2\n                           )\nocr_embeddings = tfidf_vec_2.fit_transform(data_train['cleaned_ocr_text']).toarray().astype(np.float32)\nocr_embeddings.shape","5dfc68b0":"# def cosine(v1, v2):\n#     v1 = np.array(v1)\n#     v2 = np.array(v2)\n\n#     return np.dot(v1, v2) \/ (np.sqrt(np.sum(v1**2)) * np.sqrt(np.sum(v2**2)))\n# sims = []\n# for i in tqdm(range(len(title_embeddings))):\n#     sims_temp = []\n#     for j in range(i,len(title_embeddings)):\n#         sim = cosine(title_embeddings[i],title_embeddings[j])\n#         if sim >= 0.5:\n#             sims_temp.append(data_train['posting_id'].iloc[j])\n#     sims.append(sims_temp)","87298710":"title_embeddings[0]","8b21d0d0":"preds = []\nCHUNK = 1024\n\nprint('Finding similar titles...')\nCTS = len(title_embeddings)\/\/CHUNK\nif len(title_embeddings)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(title_embeddings))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    cts = cupy.matmul(title_embeddings, title_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        IDX = cupy.where(cts[k,]>0.7)[0]\n        o = data.iloc[cupy.asnumpy(IDX)]['posting_id'].values\n        preds.append(o)\n        \n# del tfidf_vec, text_embeddings\n# _ = gc.collect()\ndata_train['title_cos_sim>0.7'] = preds","98255ea8":"preds_ocr = []\nCHUNK = 1024\n\nprint('Finding similar titles...')\nCTS = len(ocr_embeddings)\/\/CHUNK\nif len(ocr_embeddings)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(ocr_embeddings))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    cts = cupy.matmul(ocr_embeddings, ocr_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        IDX = cupy.where(cts[k,]>0.7)[0]\n        o = data.iloc[cupy.asnumpy(IDX)].posting_id.values\n        preds_ocr.append(o)\n        \n# del tfidf_vec, text_embeddings\n# _ = gc.collect()\ndata_train['ocr_cos_sim>0.7'] = preds_ocr","27e34f22":"data_train.head()\ndata_train = data_train.to_pandas()\ndata_train.to_csv('features_till_cos_sim.csv',index=False)","f7785ff8":"data_train = pd.read_csv('features_till_cos_sim.csv')","f16014a7":"**I have decided not to do lemmatization\/Stemming as most of the words are not native english words, there are other language words in english that might not get stemmed properly, better to leave it for now, I will come back here if I need to fine tune model and if this helps.**","19ff9318":"# Work in Progress\n\nI am working on implementing more features.","fe58bd96":"OCR Text Cosine similarity","24a78f76":"### Import Required Packages","a8732adf":"### Text Features\n\n+ **Length & Word count of titles and OCR text**","24307559":"<h1 style=\"font-size:60px\"><center>Shopee Product Matching<\/center><\/h1>\n\n![shopee logo](https:\/\/i.imgur.com\/GvmrZK0.png)","b33ef708":"### Approaches :\n\n+ **Euclidean Distannces between title\/ocr text using word2vec or tfidf vectors.**\n+ **Image Similarity using the phash value, setting a threshold for the hamming distance might be a good idea(subject to choosing the best hyperparamter). we will do this in modelling phase**\n+ **Use all features to find nearest neighbours and group them together (more resource and time intensive,selecting k neighbours will be key)**\n+ **Maybe we can turn this into a clasification problem with number of classes equal to the number of unique label_groups, I am a bit confused on this though.**","0358655e":"### Let's load in the data and have a quick look","4cd99834":"Title Embeddings Cosine Similarity","0ae86be5":"**Not using the above code, It works perfectly but is very slow as it runs on CPU, using RAPIDS is the only alternative for now it seems. the codde from the cell below has been taken from chris Deotte's notebook** [ Check it Out Here](https:\/\/www.kaggle.com\/cdeotte\/part-2-rapids-tfidfvectorizer-cv-0-700)","eaf5f0ba":"### Add required paths so as to avoid confusion later on","66d73561":"+ **Average Word lengths in title and OCR text**","3c8c85d1":"## Feature Engineering Notebook\n\n\n    \nHere I try to create as many features as possible so that in the modelling phase I can get good results. I am using the cleaned title and OCR data that I did in my previous [Notebook](https:\/\/www.kaggle.com\/mohneesh7\/shopee-challenge-eda-nlp-on-title-ocr). If you haven't followed it, please check it out.\n\nI haven't lemmatized the title text there, I will try to use lemmatized\/stemmed words as another feature. Let's see if it gives good results.\n    \n","86cf4e8b":"**To be on the safe side lets save all features till now (also I dont want to waste GPU time)**"}}