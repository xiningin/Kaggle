{"cell_type":{"652b7d23":"code","46ddb540":"code","93b42ded":"code","cc901b51":"code","3d19ea97":"code","dbd10a80":"code","8fb1ef5c":"code","7f20efef":"code","79c0519b":"code","04f87e83":"code","5d07d36b":"code","918ea98b":"code","8d0e00c1":"code","31b3daef":"code","3d77bad1":"code","e331ff80":"code","82c753c1":"code","738bef0d":"code","30f43a20":"code","2799db42":"code","26d18943":"code","70b439a8":"code","b68d5657":"code","14833862":"code","2cc7510e":"code","93ef5649":"code","58875dd1":"markdown"},"source":{"652b7d23":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","46ddb540":"import numpy as np","93b42ded":"import pandas as pd\ndata = pd.read_csv(\"..\/input\/data.csv\")\ntrain_encoded = pd.read_csv(\"..\/input\/train_encoded.csv\")","cc901b51":"# df=pd.read_csv('data.csv')\ndf=pd.read_csv(\"..\/input\/data.csv\")","3d19ea97":"# y=df['label']\nnp.random.seed(42)\ndf","dbd10a80":"df.rename(columns={'UNRATE': 'target'}, inplace=True)\ndate=df['sasdate']\ndf=df.drop(columns='sasdate')\ncol=df.columns[df.isna().any()].tolist()\ndf=df.drop(columns=col)","8fb1ef5c":"df","7f20efef":"n_val=1","79c0519b":"from sklearn.preprocessing import MinMaxScaler\ndef create_dataset(dataset,target_index, look_back=20):\n    dataX, dataY = [], []\n    for i in range(len(dataset) - look_back - 1-n_val):\n        #print(i)\n        a = dataset[i:(i + look_back)]\n        dataX.append(a)\n        b = dataset[(i+look_back):(i + look_back+n_val),target_index]\n        dataY.append(b)\n        #print(dataY)\n    return np.array(dataX), np.array(dataY)\n\n# y_scaler = MinMaxScaler(feature_range=(0, 1))\n# t_y = df['target'].values.astype('float32')\n# yy=t_y\n# t_y = np.reshape(t_y, (-1, 1))\n# y_scaler = y_scaler.fit(t_y)\n# df['target']=y_scaler.transform(t_y)\n# Scale and create datasets\ntarget_index = df.columns.tolist().index('target')\nprint(target_index)\ndataset = df.values.astype('float32')\n\ny_scaler = MinMaxScaler(feature_range=(0, 1))\nt_y = df['target'].values.astype('float32')\nyy=t_y\nt_y = np.reshape(t_y, (-1, 1))\ny_scaler = y_scaler.fit(t_y)\n\n# Scale the data\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)\n\n#Create y_scaler to inverse it later\n\n#print(y)\nX, y = create_dataset(dataset,target_index, look_back=24)\n# y_scaler = MinMaxScaler(feature_range=(0, 1))\n# y = y_scaler.fit_transform(y)\n# X_scaler= MinMaxScaler(feature_range=(0, 1))\n# X=X_scaler.fit_transform(X)\nprint(y.shape)\nprint(X.shape)\n#y = y[:, target_index]\n#print(y[0])\ntrain_size = int(len(X) * 0.95)\ntrainX = X[:train_size]\ntrainY = y[:train_size]\ntestX = X[train_size:]\ntestY = y[train_size:]","04f87e83":"trainX.shape","5d07d36b":"\nsum=0\nsum1=0\nfor i in range(len(yy)):\n    if(i>0):\n        sum+=abs(yy[i]-yy[i-1])\n        sum1+=(yy[i]-yy[i-1])**2\nprint(sum\/(len(yy)-1))\nprint(sum1\/(len(yy)-1))","918ea98b":"yy","8d0e00c1":"y[1,:]","31b3daef":"print(y.shape)\nprint(X.shape)\nX[0,0]\ny=y.reshape(-1,1)\nprint(y.shape)","3d77bad1":"import keras\nimport tensorflow\nfrom keras import Model","e331ff80":"from keras.models import Sequential\nfrom keras.layers import *\nfrom keras.regularizers import l2\n# model = Sequential()\n# model.add(Dense(12, input_dim=20,activation='relu',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n# model.add(Dropout(0.2))\n# model.add(Dense(4,activation='relu',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n# model.add(Dropout(0.2))\n# model.add(Dense(1,activation='linear'))\n# rms=keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n# model.compile(optimizer=rms,\n#               loss='mse')\nmodel = Sequential()\n\n# model.add(TimeDistributed(Conv1D(filters=16, kernel_size=2, activation='relu', padding='same'), input_shape=(None, n_length, n_features)))\n# model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n# model.add(TimeDistributed(Flatten()))\n# model.add(\n#     Bidirectional(LSTM(10, input_shape=(X.shape[1], X.shape[2]),\n#                        return_sequences=True),\n#                   merge_mode='sum',\n#                   weights=None,\n#                   input_shape=(X.shape[1], X.shape[2])))\nmodel.add(LSTM(30, input_shape=(X.shape[1], X.shape[2]),\n                       return_sequences=False))\nmodel.add(Dropout(0.2))\n# model.add(LSTM(15, return_sequences=False))\n# model.add(Dropout(0.3))\n\n# model.add(LSTM(4, return_sequences=False))\nmodel.add(Dense(30, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(n_val, kernel_initializer='uniform', activation='linear'))\nrms=keras.optimizers.RMSprop(lr=0.0001, rho=0.9)\nmodel.compile(loss='mean_squared_error', optimizer=rms, metrics=['mae', 'mse'])\nprint(model.summary())","82c753c1":"sum=0\nfor i in range(len(y)):\n    if(i>0):\n        sum+=(y[i]-y[i-1])**2\nsum\/(len(y)-1)","738bef0d":"from keras.callbacks import LearningRateScheduler\nimport keras.backend as K\n\ndef scheduler(epoch):\n    if epoch%20==0 and epoch!=0:\n        lr = K.get_value(model.optimizer.lr)\n        K.set_value(model.optimizer.lr, lr*.95)\n        print(\"lr changed to {}\".format(lr*.95))\n    return K.get_value(model.optimizer.lr)\nlr_decay = LearningRateScheduler(scheduler)\n","30f43a20":"filepath=\".\/..\/weights-{epoch:02d}-{val_mae:.5f}.hdf5\"\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.callbacks import ModelCheckpoint\ncheckpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint,lr_decay]\nmodel.fit(trainX,trainY,validation_data=[testX,testY],epochs=2000,batch_size=64,callbacks=callbacks_list)","2799db42":"# from keras.models import load_model\n# model=load_model('.\/..\/save_3\/weights-09-0.01477.hdf5') ","26d18943":"y=y.reshape(len(X),n_val)","70b439a8":"import matplotlib.pyplot as plt\nplt.plot(y_scaler.inverse_transform(model.predict(X)))\nplt.plot(y_scaler.inverse_transform(y))\n#plt.savefig('lstm_train_encoded_zoomed.png')\nplt.show()","b68d5657":"trainX.shape","14833862":"pred=(model.predict(X))\npred.shape\n#y=y.reshape(len(X)-n_val,n_val)\nnp.mean(np.abs(y_scaler.inverse_transform(y)-y_scaler.inverse_transform(pred)))\nplt.plot(y_scaler.inverse_transform(y)[666:701,:],label='actual')\nplt.plot(y_scaler.inverse_transform(pred)[666:701,:],label='predicted')\nplt.legend()\nplt.show()\n#y_scaler.inverse_transform(model.predict(X)[696,:].reshape(-1,1))\n#y_scaler.inverse_transform(y_scaler.inverse_transform(y[696,:].reshape(-1,1)))","2cc7510e":"testPred=model.predict(testX)\nprint(np.mean(np.abs(y_scaler.inverse_transform(testY)-y_scaler.inverse_transform(testPred))))\ntrainPred=model.predict(trainX)\nprint(np.mean(np.abs(y_scaler.inverse_transform(trainY)-y_scaler.inverse_transform(trainPred))))\n","93ef5649":"import matplotlib.pyplot as plt\nplt.plot(y_scaler.inverse_transform(model.predict(trainX)),label='true')\nplt.plot(y_scaler.inverse_transform(trainY),label='predicted')\nplt.legend()\n#plt.savefig('lstm_test.png')\n#plt.legend()\nplt.show()","58875dd1":"z=np.array(y_scaler.inverse_transform(y)).reshape(y.shape[0],n_val)-y_scaler.inverse_transform(model.predict(X))\nz=z**2\nprint(np.sum(z)\/len(y))\n"}}