{"cell_type":{"a3008f08":"code","c32db9c9":"code","593f3dc6":"code","d546b5af":"code","815c56c9":"code","6bfdd291":"code","6ec0b1cb":"code","3f64e4c8":"code","4b20726e":"code","ab59eeac":"code","64b2380a":"code","924c6833":"code","7a56694c":"code","4862e041":"code","c8e9cb0f":"code","103e5e56":"code","01a88b6d":"code","b966497f":"code","300b436d":"code","f42ca087":"code","6b7d589a":"code","844fc767":"code","6eff3f15":"code","f32aa936":"code","737b3328":"code","22f3d189":"code","90e60386":"code","38843ac9":"code","6c233810":"code","ca28c545":"code","7cc85d62":"code","16bac406":"code","79d4a461":"code","74e3592a":"code","e5572394":"code","fdaa46e6":"code","78a05635":"code","43b8b328":"code","7cdb2507":"code","f424c4a8":"code","847dc6d2":"code","0a0e1f05":"markdown","2c298c79":"markdown","4c90ef2b":"markdown","33cf277a":"markdown","2d74f433":"markdown","55c4f3dc":"markdown","29eec722":"markdown","176be511":"markdown","97e7ea61":"markdown","3319dc61":"markdown","c706d32c":"markdown","e5803653":"markdown","80ac1b44":"markdown","4c63a2bc":"markdown","0307f175":"markdown","b966b4c4":"markdown"},"source":{"a3008f08":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c32db9c9":"df = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')\ndf.columns = ['SerialNo', 'GRE_Score', 'TOEFL_Score', 'Uni_Rating', 'SOP', 'LOR', 'CGPA', 'Research', 'Chance_of_Admit']\ndf.set_index('SerialNo', inplace = True)\ndf.head()","593f3dc6":"df.shape","d546b5af":"import missingno\nmissingno.matrix(df)","815c56c9":"plt.figure(figsize = (14,7))\n\nsns.heatmap(df.corr(), annot = True)\n","6bfdd291":"# Chance of admission shows strong correlation with CGPA, Toefl score and GRE score","6ec0b1cb":"plt.figure(figsize = (12,6))\nplt.title('Correlation of Each feature with Chance of Admission', fontsize = 14)\nsns.barplot(x = df.corr()['Chance_of_Admit'], y = df.columns)","3f64e4c8":"for i, col in enumerate(df.columns):\n    print(i, col, '-', df[col].dtype)","4b20726e":"print('Counting the number of students in each category')\nplt.figure(figsize = (10,6))\nsns.countplot(x = 'Research', data = df)","ab59eeac":"plt.figure(figsize = (12,6))\nsns.swarmplot(x = df['Research'], y = df['Chance_of_Admit'])\nplt.xlabel('Research', fontsize=16)\nplt.ylabel('Chance of Admission', fontsize = 16)\n","64b2380a":"plt.figure(figsize = (10,6))\nsns.barplot(x = df['Research'], y = df['Chance_of_Admit'])\nplt.xlabel('Research', fontsize=16)\nplt.ylabel('Chance of Admission', fontsize = 16)","924c6833":"df[['Research','Uni_Rating', 'Chance_of_Admit']].groupby(['Research'], as_index = True).mean()","7a56694c":"plt.figure(figsize = (10,6))\nsns.scatterplot(x = df['CGPA'], y = df['Chance_of_Admit'])","4862e041":"\nsns.lmplot(data = df, x = 'CGPA', y = 'Chance_of_Admit', hue = 'Research', height = 8)\n","c8e9cb0f":"bins = np.linspace(min(df['CGPA']), max(df['CGPA']), 5)\ngroup_names = [1,2,3,4]\ndf['CGPA_binned'] = pd.cut(df['CGPA'], bins, labels = group_names, include_lowest = True)","103e5e56":"plt.figure(figsize = (12,6))\nsns.barplot(x = df['CGPA_binned'], y = df['Chance_of_Admit'])\nplt.xlabel('CGPA_binned', fontsize = 16)\nplt.ylabel('Chance of admission', fontsize = 16)","01a88b6d":"df['CGPA_binned'] = df['CGPA_binned'].astype(int)\nprint(df['CGPA_binned'].dtype)","b966497f":"df[['LOR', 'Chance_of_Admit']].groupby('LOR').mean()","300b436d":"plt.figure(figsize = (12,6))\nsns.barplot(x = df['LOR'], y = df['Chance_of_Admit'])\nplt.xlabel('Letter of Recommendation Strength', fontsize = 16)\nplt.ylabel('Chance of admission', fontsize = 16)","f42ca087":"df[['SOP', 'Chance_of_Admit']].groupby('SOP').mean()","6b7d589a":"plt.figure(figsize = (12,6))\nsns.barplot(x = df['SOP'], y = df['Chance_of_Admit'])\nplt.xlabel('Statement of Purpouse Strength', fontsize = 16)\nplt.ylabel('Chance of admission', fontsize = 16)","844fc767":"plt.figure(figsize = (12, 6))\nsns.swarmplot(x = df['Uni_Rating'] ,y = df['Chance_of_Admit'], hue = df['CGPA_binned'])\nplt.xlabel('University Rating')\nplt.ylabel('Chance of Admission')","6eff3f15":"plt.figure(figsize = (12, 6))\nsns.swarmplot(x = df['Uni_Rating'] ,y = df['Chance_of_Admit'], hue = df['Research'])\nplt.xlabel('University Rating')\nplt.ylabel('Chance of Admission')","f32aa936":"df[['Uni_Rating', 'Chance_of_Admit', 'CGPA_binned']].corr()","737b3328":"plt.figure(figsize = (12,6))\nplt.title('TOEFL score vs Chances of Admission')\nsns.regplot(x = df['TOEFL_Score'], y = df['Chance_of_Admit'])","22f3d189":"plt.figure(figsize = (12,6))\nsns.scatterplot(x = df['GRE_Score'], y = df['Chance_of_Admit'], hue = df['CGPA_binned'], palette = sns.color_palette('bright', 4))","90e60386":"plt.figure(figsize = (12,6))\nsns.regplot(x = df['GRE_Score'], y = df['Chance_of_Admit'])","38843ac9":"from sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import RandomizedSearchCV","6c233810":"features = ['GRE_Score', 'TOEFL_Score', 'Uni_Rating', 'SOP', 'LOR', 'Research', 'CGPA']\nX = df[features].copy()\nX.head()\n","ca28c545":"scalar = preprocessing.StandardScaler()\nscalar.fit(X)\nX = scalar.transform(X)\nX","7cc85d62":"def check_model_accuracy(model, X, y):\n    scores = cross_val_score(model, X, y, cv = 5, scoring = 'r2')\n    return scores.mean()","16bac406":"def check_model_error(model, X, y):\n    scores = cross_val_score(model, X, y, cv = 5, scoring = 'neg_root_mean_squared_error')\n    return scores.mean()","79d4a461":"parameters_knn = {\n    'n_neighbors':[i for i in range(5,25)],\n    'weights':['uniform','distance'],\n    'metric':['euclidean', 'manhattan']\n}\nrandm_knn = RandomizedSearchCV(estimator = KNeighborsRegressor(), param_distributions = parameters_knn, cv = 5, n_iter = 15, n_jobs = -1, scoring = 'r2')\nrandm_knn.fit(X,df.Chance_of_Admit)\n","74e3592a":"print('Best KNN parameters:', randm_knn.best_params_)\nprint('Best KNN Accuracy:', randm_knn.best_score_)\nfinal_knn_model = KNeighborsRegressor(weights = randm_knn.best_params_['weights'], n_neighbors = randm_knn.best_params_['n_neighbors'], metric = randm_knn.best_params_['metric'] )\nprint('Error:', -1*check_model_error(final_knn_model, X, df.Chance_of_Admit))","e5572394":"parameters_rf = {\n    'n_estimators' : [ i for i in range(100,1001,100)],\n    'max_features' : ['auto', 'sqrt', 'log2'],\n    'min_samples_leaf' : [1,2,5,10] #The minimum number of samples required to be at a leaf node.\n}\nrandm_rf = RandomizedSearchCV(estimator = RandomForestRegressor(), param_distributions = parameters_rf, cv = 5, n_iter = 15, n_jobs = -1, scoring = 'r2')\nrandm_rf.fit(X, df.Chance_of_Admit)","fdaa46e6":"print('Best RandomForests Parameters:', randm_rf.best_params_)\nprint('Best RandomForests Score:', randm_rf.best_score_)\nfinal_randomforests_model = RandomForestRegressor(n_estimators = randm_rf.best_params_['n_estimators'], min_samples_leaf = randm_rf.best_params_['min_samples_leaf'], max_features = randm_rf.best_params_['max_features'])\nprint('Error:', -1*check_model_error(final_randomforests_model, X, df.Chance_of_Admit))","78a05635":"linearRegression = LinearRegression()\nprint('Best linear Regression score:', check_model_accuracy(linearRegression, X, df.Chance_of_Admit))\nprint('Error:', -1*check_model_error(linearRegression, X, df.Chance_of_Admit))","43b8b328":"parameters_gb = {\n    'learning_rate' : np.arange(0.1,0.91,0.1),\n    'n_estimators' : range(100,1001,100),\n    'max_depth' : range(2,6),\n    'max_features' : ['auto', 'sqrt', 'log2',None]\n    \n}\nrandm_gb = RandomizedSearchCV(estimator = GradientBoostingRegressor(), param_distributions = parameters_gb, cv = 5, n_iter = 15, n_jobs = -1, scoring = 'r2')\nrandm_gb.fit(X, df.Chance_of_Admit)","7cdb2507":"print('Best GB parameters:', randm_gb.best_params_)\nprint('Best GB score:', randm_gb.best_score_)\nfinal_GB_model = GradientBoostingRegressor(n_estimators = randm_gb.best_params_['n_estimators'], max_features = randm_gb.best_params_['max_features'], max_depth = randm_gb.best_params_['max_depth'], learning_rate = randm_gb.best_params_['learning_rate'])\nprint('Error:', -1*check_model_error(final_GB_model, X, df.Chance_of_Admit))","f424c4a8":"scores = []\nmodels = [final_knn_model, final_randomforests_model, linearRegression, final_GB_model]\nfor model in models:\n    scores.append(check_model_accuracy(model, X, df.Chance_of_Admit))\n    ","847dc6d2":"plt.figure(figsize = (14,7))\nplt.title('Comparison of Models')\nX = ['KNN', 'RandomForests', 'LinearRegression', 'GradientBoosting']\nplt.ylabel('Accuracy')\nsns.barplot(x=X, y=scores)","0a0e1f05":"Students with Research opt have graduated from better rated universities and have higher chances of admission","2c298c79":"4. SOP","4c90ef2b":"# **Exploratory Data Analysis**","33cf277a":"3. LOR","2d74f433":"5. University Rating","55c4f3dc":"2. Random Forests","29eec722":"7. GRE","176be511":"**1. Research**","97e7ea61":"6. TOEFL","3319dc61":"# Model Development","c706d32c":"Students graduating from better rated universities show greater chances of admission","e5803653":"1. KNN","80ac1b44":"3. Linear Regression","4c63a2bc":"4. Gradient Boosting Regressor","0307f175":"**2. CGPA**","b966b4c4":"Linear regression has highest accuracy and lowest rmse."}}