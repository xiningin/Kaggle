{"cell_type":{"e7c228df":"code","afb9f3c3":"code","46e2ff7c":"code","afaae03e":"code","22a4cadc":"code","fdf3b444":"code","3cb9d079":"code","aa6e51c0":"code","ab6ca305":"code","0a39bb73":"code","07b3c0be":"code","e7973926":"code","68eb7b93":"code","3653c781":"code","78d5882f":"code","6ef61f4f":"code","4ddca58a":"code","64674e35":"code","4528246d":"code","8c2246f5":"code","fb063613":"code","f93dcb5e":"code","60941980":"code","e1ffd6a7":"code","f5198293":"code","53da1e61":"code","19088629":"code","08e8cfe9":"code","1eb28a14":"code","5f46b800":"code","c6753890":"code","4f397b66":"code","a8ddcd98":"code","1f9a7368":"code","165e13ac":"code","1c707233":"code","952374a8":"markdown","42d7043e":"markdown","3d61ebd0":"markdown","55daa5bc":"markdown","4621dc73":"markdown","590657d5":"markdown","4941ecaf":"markdown","7d862ca3":"markdown","d8e850d0":"markdown","5df30499":"markdown","39d3909b":"markdown","08431f0a":"markdown","fabded4e":"markdown"},"source":{"e7c228df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","afb9f3c3":"## check if all the required dependencies are already present in your system\n## use the init_notebook() method to install all the dependencies, if needed\ndef dependencies_check():\n\n    print ('python version details..')\n    ! python --version\n    ! pip --version\n\n    print ('\\ninstallation check: tensorflow')\n    ! pip list | grep tensorflow\n\n    print ('\\ninstallation check: tensorflow-gpu')\n    ! pip list | grep tensorflow-gpu\n\n    print ('\\ninstallation check: opencv')\n    ! pip list | grep opencv\n\n    print ('\\ninstallation check: keras')\n    ! pip list | grep keras\n\n    print ('\\ninstallation check: imageai')\n    ! pip list | grep imageai\n\n    print ('\\ninstalltion check: hyperdash')\n    ! pip list | grep hyperdash","46e2ff7c":"def init_notebook():\n    print ('python version details..')\n    ! python --version\n    ! pip --version\n    \n    ## imageai dependencies \n    ## https:\/\/imageai.readthedocs.io\/en\/latest\/\n    # basic requirements\n    ! pip install --upgrade tensorflow==1.14.0\n    ! pip install opencv-python==4.1.2.30\n    ! pip install keras==2.3.1\n    ! pip install --upgrade imageai==2.1.5\n    \n    ## for training a custom yolo with a gpu\n    ! pip install tensorflow-gpu==1.14.0\n\n    ## hyperdash is to monitor ur model training from ur mobile phone\n    ## https:\/\/hyperdash.io\/\n    ! pip install hyperdash\n    \n    ! pip install dicttoxml","afaae03e":"dependencies_check()","22a4cadc":"init_notebook()","fdf3b444":"# checking dependencies after init_notebook\ndependencies_check()","3cb9d079":"from IPython.display import Image as IPythonImage\n\nfrom PIL import Image\nimport cv2\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\n\nfrom dicttoxml import dicttoxml\nimport pprint\n\nimport urllib\n\nfrom hyperdash import monitor","aa6e51c0":"# # mount ur google drive to store ur model weights during the training process\n# from google.colab import drive\n# drive.mount('\/content\/gdrive')\n\n# # why should we do this?\n# ## whenever the colab runtime expires in 12 hours, the last available weights of ur training process is saved to ur drive\n# ## this helps to restart ur training with the available weights. don't have to start it all over again!","ab6ca305":"# check if GPU is enabled if you are gonna train an NN\n\n# if you installed CUDA correctly you can list ur graphics cards using nvidia-smi\n\n# and if you are using Google Colab, before you compile with GPU, ensure that you change your runtime to GPU\n## Runtime -> Change runtime type -> GPU\n\n! nvidia-smi","0a39bb73":"def display_img(img_path):\n    img = IPythonImage(filename=img_path)\n    display(img)","07b3c0be":"# downloading the pre-trained weights of a yolov3 trained on the COCO dataset\n## the model is capable of detecting 80 categories of images\n\n! wget https:\/\/github.com\/OlafenwaMoses\/ImageAI\/releases\/download\/1.0\/yolo.h5","e7973926":"datasource = '\/kaggle\/input\/vehicle-number-plate-detection\/Indian_Number_plates.json'\nnumber_plates_json = pd.read_json(datasource, lines=True)\nnumber_plates_json.head()","68eb7b93":"## bounding box coordinates in the json file are scaled between 0 to 1\n## where the original image is assumed to be transformed as a square image,\n## with the starting point - top left pixel as (0, 0) and ending point - bottom right pixel as (1, 1)\npprint.pprint (number_plates_json['annotation'][0])","3653c781":"## imageai expects annotation data to be stored in pascal voc format\n## and this method creates an xml file in pascal voc format for the given image attributes for object detection with yolo\ndef get_annotation_xml(filename, top_x=0, top_y=0, bottom_x=1, bottom_y=1, width=224, height=224):\n    annotation = {\n        \"folder\": \"images\",\n        \"filename\": filename,\n        \"path\": \"\/content\/gdrive\/My Drive\/alpr\/images\/\" + filename,\n        \"source\": { \"database\": \"Unknown\" },\n        \"size\": { \"width\": width, \"height\": height, \"depth\": 3 },\n        \"segmented\": 0,\n        \"object\": {\n            \"name\": \"number_plate\",\n            \"pose\": \"Unspecified\",\n            \"truncated\": 0,\n            \"difficult\": 0,\n            \"bndbox\": {\n                \"xmin\": int(top_x * width),\n                \"ymin\": int(top_y * height),\n                \"xmax\": int(bottom_x * width),\n                \"ymax\": int(bottom_y * height)\n            }\n        }\n    }\n    \n    # xml as bytes\n    xml = dicttoxml(annotation, custom_root='annotation', attr_type=False)\n    \n    # xml as string\n    xml_string = xml.decode(\"utf-8\")\n    \n    # remove first line\n    xml_string = xml_string.replace('<?xml version=\"1.0\" encoding=\"UTF-8\" ?>', '')\n    \n    return xml_string","78d5882f":"## sample annotation xml in pascal voc\nsample_annotation = \"\"\"{\n        \"folder\": \"images\",\n        \"filename\": \"car_1.jpg\",\n        \"path\": \"\/content\/gdrive\/My Drive\/number_plate\/images\/car_1.jpg\", \/\/ this path is just a placeholder, can give any value\n        \"source\": { \"database\": \"Unknown\" },\n        \"size\": { \"width\": 806, \"height\": 466, \"depth\": 3 }, \/\/ original dimensions of the image\n        \"segmented\": 0,\n        \"object\": {\n            \"name\": \"number_plate\", \/\/ class label; all the annotation files and the image files should placed in a dir with the same name as the class label\n            \"pose\": \"Unspecified\",\n            \"truncated\": 0,\n            \"difficult\": 0,\n            \"bndbox\": { \/\/ bounding box co-ordinates with respect to the original dimensions of the image\n                \"xmin\": 581,\n                \"ymin\": 273,\n                \"xmax\": 700,\n                \"ymax\": 320\n            }\n        }\n    }\n\"\"\"\n\n# print (sample_annotation)","6ef61f4f":"# downloading car images and creating annotation xml files\n## downloaded images will be stored under 'number_plate\/images' dir\n## and the corresponding annotation xml files will be stored under 'number_plate\/annotations'\n\n# this method additionally creates a dataset to view the image dimesions later\n## feel free to remove the dataset creation code in this method if not needed\n\ndef prepare_data():\n    ! mkdir 'number_plate'\n    \n    images_dir = 'number_plate\/images\/'\n    ! mkdir $images_dir\n    \n    annotations_dir = 'number_plate\/annotations\/'\n    ! mkdir $annotations_dir\n    \n    dataset = {\n        'img_file': [], 'img_width': [], 'img_height': [],\n        'top_x': [], 'top_y': [], 'bottom_x': [], 'bottom_y': []\n    }\n    \n    for index, row in number_plates_json.iterrows():\n        print (f'making files for row #{index+1}..')\n        \n        file_name = f'car_{index + 1}'\n        img_file = f'{file_name}.jpg'\n        txt_file = f'{file_name}.txt'\n        \n        img_url = row['content']\n        img = urllib.request.urlopen(img_url)\n        img = Image.open(img).convert('RGB')\n        img.save(f'{images_dir}{img_file}', 'JPEG')\n        \n        annotation = row['annotation'][0]\n        img_width = annotation['imageWidth']\n        img_height = annotation['imageHeight']\n        top_x = annotation['points'][0]['x']\n        top_y = annotation['points'][0]['y']\n        bottom_x = annotation['points'][1]['x']\n        bottom_y = annotation['points'][1]['y']\n        \n        dataset['img_file'].append(img_file)\n        dataset['img_width'].append(img_width)\n        dataset['img_height'].append(img_height)\n        dataset['top_x'].append(top_x)\n        dataset['top_y'].append(top_y)\n        dataset['bottom_x'].append(bottom_x)\n        dataset['bottom_y'].append(bottom_y)\n        \n        annotation_xml = get_annotation_xml(img_file, top_x, top_y, bottom_x, bottom_y, img_width, img_height)\n        annotation_file = open(f'{annotations_dir}{file_name}.xml','w+')\n        annotation_file.write(annotation_xml)\n        annotation_file.close()\n        \n    print(f'Number of car images downloaded - {len(dataset[\"img_file\"])}\\n')\n    print ('\\n')\n    ! ls -ltrh $images_dir | grep .jpg | head -10\n    \n    print ('\\n')\n    ! ls -ltrh $annotations_dir | grep .xml | head -10\n    \n    return pd.DataFrame(dataset)","4ddca58a":"df = prepare_data()","64674e35":"# viewing the prepared dataset\ndf.head()","4528246d":"# saving the dataset to file system\ndf.to_csv(\"indian_license_plates.csv\", index=False)","8c2246f5":"WIDTH = 224\nHEIGHT = 224\nCHANNEL = 3\n\ndef display_car_image(index, scale=True):\n    images_dir = 'number_plate\/images\/'\n    \n    img = cv2.imread(images_dir + df['img_file'].iloc[index])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, dsize=(WIDTH, HEIGHT))\n\n    top_x = int(df['top_x'].iloc[index] * WIDTH)\n    top_y = int(df['top_y'].iloc[index] * HEIGHT)\n    bottom_x = int(df['bottom_x'].iloc[index] * WIDTH)\n    bottom_y = int(df['bottom_y'].iloc[index] * HEIGHT)\n    center_x = 20\n    center_y = 20\n\n    # adding bounding box to the number plate in the car\n    bbox_width = 1\n    bbox_color = (255, 0, 0)\n    car = cv2.rectangle(img, (top_x, top_y), (bottom_x, bottom_y), bbox_color, bbox_width)\n    car = cv2.circle(car, (center_x, center_x), radius=0, color=(0, 0, 255), thickness=-1)\n\n    plt.figure(figsize=(20, 10))\n    plt.imshow(car)\n    plt.show()\n\n    # cropping the number plate to display\n    number_plate = Image.fromarray(img).crop((top_x + bbox_width, top_y + bbox_width, bottom_x, bottom_y))\n    plt.figure(figsize=(5, 5))\n    plt.imshow(number_plate)\n    plt.show()","fb063613":"display_car_image(6)","f93dcb5e":"img_files = df['img_file'].tolist()\nimg_files[:5]","60941980":"file_tuples = df['img_file'].apply(lambda img_file: (img_file, img_file.replace('.jpg', '')+'.xml')).tolist()\nfile_tuples[:5]","e1ffd6a7":"# example for shuffling the image files list\nnp.random.seed(6)\nnp.random.shuffle(file_tuples)\nfile_tuples[:5]","f5198293":"def test_train_split(file_tuples, test_size=0.05, random_seed=6):\n    np.random.seed(random_seed)\n    np.random.shuffle(img_files)\n    \n    test_end = int(test_size * len(file_tuples))\n    return file_tuples[:test_end], file_tuples[test_end:]","53da1e61":"test_files, train_files = test_train_split(file_tuples)\nlen(test_files), len(train_files)","19088629":"def move_data_files(file_tuples, dest_dir):\n    counter = 0\n    for tup in file_tuples:\n        counter += 1\n        print ('moving image file #' + str(counter))\n        \n        file_name = tup[0].replace('number_plate\/images\/', '').replace('.jpg', '')\n        \n        img_src = 'number_plate\/images\/' + tup[0]\n        annotation_src = 'number_plate\/annotations\/' + tup[1]\n        \n        img_dest = dest_dir + 'images\/' + file_name + '.jpg'\n        annotation_dest = dest_dir + 'annotations\/' + file_name + '.xml'\n        \n        ! mv $img_src $img_dest\n        ! mv $annotation_src $annotation_dest\n    print ('\\ntotal number of images moved: ' + str(counter))","08e8cfe9":"## this notebook cell took 5 mins to complete in kaggle\n\n# deleting image file directories if already present\n! rm -rf 'number_plate\/train\/' 'number_plate\/validation\/'\n! mkdir 'number_plate\/train\/' 'number_plate\/train\/images\/' 'number_plate\/train\/annotations\/'\n! mkdir 'number_plate\/validation\/' 'number_plate\/validation\/images\/' 'number_plate\/validation\/annotations\/'\n\n# moving image files from download location to expected train\/validation locations\nprint ('\\npopulating \"train\" dir..')\nmove_data_files(train_files, 'number_plate\/train\/')\n\nprint ('\\npopulating \"validation\" dir..')\nmove_data_files(test_files, 'number_plate\/validation\/')\n\n# removing the empty directories in the download location\n! rm -rf 'number_plate\/images\/' 'number_plate\/annotations\/'","1eb28a14":"! ls -ltrh *\n\nprint ('\\n')\n! ls -ltrh *\/*","5f46b800":"# training for 5 epochs took 5h 47m in kaggle (cpu mode)\n\nfrom imageai.Detection.Custom import DetectionModelTrainer\n\nBATCH_SIZE = 4\nNUMBER_OF_EPOCHS = 5 # increase the number of epochs for better detection accuracy\n\nIOU_THRESHOLD = 0.5\nNMS_THRESHOLD = 0.5\nOBJECT_THRESHOLD = 0.3\n\ndef get_api_key():\n    return '*****' # your hyperdash api key here\n\n# hyperdash_exp_title = 'alpr_kaggle:yolo' # uncomment this line if you are not using hyperdash\n# @monitor(hyperdash_exp_title, api_key_getter=get_api_key) # uncomment this line if you are not using hyperdash\n# def train_model(exp, gdrive=False, pretrained_model='yolo.h5'): # uncomment this line if you are not using hyperdash\ndef train_model(gdrive=False, pretrained_model='yolo.h5'): # comment this line if you are not using hyperdash\n    trainer = DetectionModelTrainer()\n    trainer.setModelTypeAsYOLOv3()\n    trainer.setDataDirectory(data_directory='number_plate')\n    trainer.setTrainConfig(object_names_array=['number_plate'], batch_size=BATCH_SIZE, num_experiments=NUMBER_OF_EPOCHS, train_from_pretrained_model=pretrained_model)\n    trainer.trainModel()\n    \n    if gdrive:\n        ! cp -r 'number_plate\/models\/' '\/content\/gdrive\/My Drive\/darknet\/number_plate\/models\/'\n        ! cp -r 'number_plate\/json\/' '\/content\/gdrive\/My Drive\/darknet\/number_plate\/json\/'\n        \ndef print_metrics(data_directory='', model_path='', json_path=''):\n    print ('\\n\\nmodel evaluation..')\n    trainer = DetectionModelTrainer()\n    trainer.setModelTypeAsYOLOv3()\n    trainer.setDataDirectory(data_directory=data_directory)\n    metrics = trainer.evaluateModel(model_path=model_path, json_path=json_path, iou_threshold=IOU_THRESHOLD, object_threshold=OBJECT_THRESHOLD, nms_threshold=NMS_THRESHOLD)\n    print (metrics)\n    \n    return metrics","c6753890":"### model training ###\ntrain_model()","4f397b66":"### model evaluation ###\nroot_dir = 'number_plate'\nmodels_dir = 'number_plate\/models'\nconfig_json = 'number_plate\/json\/detection_config.json'\n\n## uncomment the following lines if you are using colab\n# root_dir = 'number_plate'\n# models_dir = '\/content\/gdrive\/My Drive\/darknet\/number_plate\/models'\n# config_json = '\/content\/gdrive\/My Drive\/darknet\/number_plate\/json\/detection_config.json'\n\nmetrics = print_metrics(root_dir, models_dir, config_json)","a8ddcd98":"print ('-- listing sample files --') \nprint ('\\ntrain images..')\n! ls -ltrh number_plate\/train\/images\/ | head -5\n\nprint ('\\nvalidation images')\n! ls -ltrh number_plate\/validation\/images\/ | head -5","1f9a7368":"from imageai.Detection.Custom import CustomObjectDetection\n\n# choose the best weights you have found in your model evaluation for inference\n## in my case, mAP: 0.7931 was the best whose corresponding weights are stored as detection_model-ex-005--loss-0011.243.h5\nmodel_path = 'number_plate\/models\/detection_model-ex-005--loss-0011.243.h5'\njson_path = 'number_plate\/json\/detection_config.json'\n\n## uncomment the following lines if you are using colab\n# model_path = '\/content\/gdrive\/My Drive\/darknet\/' + model_path\n# json_path = '\/content\/gdrive\/My Drive\/darknet\/' + json_path\n\ndetector = CustomObjectDetection()\ndetector.setModelTypeAsYOLOv3()\ndetector.setModelPath(model_path)\ndetector.setJsonPath(json_path)\ndetector.loadModel()","165e13ac":"detections = detector.detectObjectsFromImage(input_image='number_plate\/train\/images\/car_4.jpg', output_image_path='nplate1-detected.jpg')\n\nfor obj in detections:\n    print (obj['name'])\n    print (obj['percentage_probability'])\n    print (obj['box_points'])\n    print ('\\n')\n\ndisplay_img('nplate1-detected.jpg')","1c707233":"detections = detector.detectObjectsFromImage(input_image='number_plate\/validation\/images\/car_85.jpg', output_image_path='nplate2-detected.jpg')\n\nfor obj in detections:\n    print (obj['name'])\n    print (obj['percentage_probability'])\n    print (obj['box_points'])\n    print ('\\n')\n\ndisplay_img('nplate2-detected.jpg')","952374a8":"### Inference","42d7043e":"# Check if GPU is enabled","3d61ebd0":"## Model Training and Model Evaluation","55daa5bc":"# Installing imageai and other dependencies","4621dc73":"## Downloading the pre-trained weights","590657d5":"### Downloading images and making annotation xml files","4941ecaf":"# Importing Libraries","7d862ca3":"### Test Train Split\n\nimageai expects the data for the object detection model to be present as in the following pattern\n\n<pre>\n    number_plate:\n    total 8.0K\n    drwxr-xr-x 4 root root 4.0K Jun  1 14:00 train\n    drwxr-xr-x 4 root root 4.0K Jun  1 14:00 validation\n\n    number_plate\/train:\n    total 24K\n    drwxr-xr-x 2 root root 12K Jun  1 14:05 images\n    drwxr-xr-x 2 root root 12K Jun  1 14:05 annotations\n\n    number_plate\/validation:\n    total 8.0K\n    drwxr-xr-x 2 root root 4.0K Jun  1 14:06 images\n    drwxr-xr-x 2 root root 4.0K Jun  1 14:06 annotations\n<\/pre>","d8e850d0":"# Method Definitions","5df30499":"# Mount  Google Drive (if you are using Colab)","39d3909b":"## Data Preparation","08431f0a":"### Looking at a sample image in the training set","fabded4e":"# License Plate Detection"}}