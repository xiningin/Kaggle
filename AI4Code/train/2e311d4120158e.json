{"cell_type":{"36002ae1":"code","c47c1156":"code","e93fbfa9":"code","89b037c5":"code","960407d3":"code","90379455":"code","2f83a5f9":"code","f4fd8940":"code","b8497984":"code","2675d48c":"code","12b1dc10":"code","d1bd30de":"code","eeea4527":"code","d6eb4251":"code","8c0dc866":"code","d9051b07":"code","953aa133":"code","a1cf92c1":"code","8711a159":"code","6f4e0a5a":"code","570582bb":"code","cd9fec8e":"code","0d78a608":"code","dcbeb79d":"code","ecac0c52":"code","e45882f9":"code","86c78a07":"code","738f78f4":"markdown","10f6a672":"markdown","a3d58776":"markdown","18981698":"markdown","20e1c3ff":"markdown"},"source":{"36002ae1":"#load libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\n\nimport lightgbm as lgb\nprint(\"LightGBM version:  {}\".format(lgb.__version__))\n\nimport matplotlib as plt\nprint(\"Matplotlib version:  {}\".format(plt.__version__))\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c47c1156":"# read input files\ndf_train = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\n\nfeature_cols = [col for col in df_train.columns if col.startswith(\"f\")]\ntarget=df_train.claim","e93fbfa9":"# define preprocessing pipelines, different pipelines are used for the different steps\ncat_like_features = ['f40','f42','f65','f70','f75']\nnumeric_features = list(set(feature_cols).difference(cat_like_features)) # all features that are not in cat_like_features\n\nnumeric_transformer = Pipeline(steps=[\n       ('imputer', SimpleImputer(strategy='mean')), # replace all Nan with mean\n       ('scaler', PowerTransformer()) # scale the features, remove\/change the scaler here for Steps 1 and 2\n        ]) \n\ncat_like_transformer = Pipeline(steps=[\n       ('imputer', SimpleImputer(strategy='mean')), \n       ('scaler', StandardScaler())\n        ]) \n\npreprocessor = ColumnTransformer(\n   transformers=[\n    ('numeric', numeric_transformer, numeric_features)\n   ,('cat_like', cat_like_transformer, cat_like_features)\n]) ","89b037c5":"# dividing X, y into train and test data\nX = df_train[feature_cols]\ny = df_train.claim\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state = 29, stratify=y)\ndisplay(X_train.shape)","960407d3":"X_train1 = numeric_transformer.fit_transform(X_train)\nX_val1 = numeric_transformer.transform(X_val)","90379455":"# most of the parameters I borrowed from BIZENs great notebook: https:\/\/www.kaggle.com\/hiro5299834\/tps-sep-2021-single-lgbm\n# this parameterset is kept fixed for my experiment\nlgb_params = {\n    'objective': 'binary',\n    'n_estimators': 1000,\n    'random_state': 29,\n    'learning_rate': 0.1,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n}","2f83a5f9":"model = lgb.LGBMClassifier(**lgb_params)\nmodel.fit( \n        X_train1, \n        y_train,\n        eval_set=[(X_val1, y_val)],\n        eval_names=['val'],\n        eval_metric='auc',\n        #early_stopping_rounds=30, # no early stopping for my experiment\n        verbose=100)","f4fd8940":"# LGBM Results\n# Part 1\n# LGBM with no missing values (SimpleImputer(strategy='mean'))\n#[1000]\tval's auc: 0.790365\tval's binary_logloss: 0.576905\n\n# Part 2\n# LGBM with no missing values (SimpleImputer(strategy='mean')) and all features standard scaled\n#[1000]\tval's auc: 0.793513\tval's binary_logloss: 0.574752\n# LGBM with no missing values (SimpleImputer(strategy='mean')) and all features MinMax scaled\n#[1000]\tval's auc: 0.790635\tval's binary_logloss: 0.576583\n# LGBM with no missing values (SimpleImputer(strategy='mean')) and all features robust scaled\n#[1000]\tval's auc: 0.790691\tval's binary_logloss: 0.576554\n# LGBM with no missing values (SimpleImputer(strategy='mean')) and all features power transformed\n#[1000]\tval's auc: 0.791065\tval's binary_logloss: 0.576157\n# LGBM with no missing values (SimpleImputer(strategy='mean')) and all features quantile transformed, normal distribution\n#[1000]\tval's auc: 0.790998\tval's binary_logloss: 0.576303","b8497984":"#pred_val = model.predict_proba(X_val)[:,1]\n#pred_val[0:5] \n#roc_auc_score(y_val, pred_val) ","2675d48c":"X_train2 = preprocessor.fit_transform(X_train)\nX_val2 = preprocessor.transform(X_val)","12b1dc10":"model = lgb.LGBMClassifier(**lgb_params)\nmodel.fit( \n        X_train2, \n        y_train,\n        eval_set=[(X_val2, y_val)],\n        eval_names=['val'],\n        eval_metric='auc',\n        #early_stopping_rounds=30, # no early stopping for my experiment\n        verbose=100)","d1bd30de":"# LGBM Results\n#[1000]\tval's auc: 0.790365\tval's binary_logloss: 0.576905 (from Part1)\n#[1000]\tval's auc: 0.791065\tval's binary_logloss: 0.576157 (from Part2, all power transformed)\n#[1000]\tval's auc: 0.790866\tval's binary_logloss: 0.576427 Power Transform on all \"non cat_like_features\"","eeea4527":"# fill means before, otherwise there would be Nans in qcut\ndf_train[feature_cols] = df_train[feature_cols].fillna(df_train[feature_cols].mean())","d6eb4251":"# Steps 4a, 4b: make categorical features, made auc worse, don't use\ndf_train['f40c'] = pd.cut(df_train['f40'], bins=2, labels=[0,1]).astype(int)\ndf_train['f42c'] = pd.cut(df_train['f42'], bins=3, labels=[0,1,2]).astype(int)\ndf_train['f65c'] = pd.cut(df_train['f65'], bins=2, labels=[0,1]).astype(int)\ndf_train['f70c'] = pd.cut(df_train['f70'], bins=2, labels=[0,1]).astype(int)\ndf_train['f75c'] = pd.cut(df_train['f75'], bins=2, labels=[0,1]).astype(int)\n# renew feature col list\nfeature_cols = [col for col in df_train.columns if col.startswith(\"f\")]","8c0dc866":"# dividing X, y into train and test data\nX = df_train[feature_cols]\ny = df_train.claim\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state = 29, stratify=y)\ndisplay(X_train.shape)","d9051b07":"model = lgb.LGBMClassifier(**lgb_params)\nmodel.fit( \n        X_train, \n        y_train,\n        eval_set=[(X_val, y_val)],\n        eval_names=['val'],\n        eval_metric='auc',\n        #early_stopping_rounds=30, # no early stopping for my experiment\n        verbose=100)","953aa133":"# LGBM Results\n#[1000]\tval's auc: 0.790365\tval's binary_logloss: 0.576905 (from Part 1)\n#[1000]\tval's auc: 0.789538\tval's binary_logloss: 0.577241 (adding categorical version of f40,f42,f65,f70,f75)\n#[1000]\tval's auc: 0.787255\tval's binary_logloss: 0.579993 (making f40,f42,f65,f70,f75 categorical)","a1cf92c1":"#pred_val = model.predict_proba(X_val)[:,1]\n#pred_val[0:5]  ","8711a159":"from sklearn import datasets\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","6f4e0a5a":"dataset = datasets.load_breast_cancer()\nX = dataset.data\ny = dataset.target","570582bb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","cd9fec8e":"clf_tree = RandomForestClassifier()\nclf_tree.fit(X_train, y_train) \ny_score1 = clf_tree.predict_proba(X_test)[:,1]","0d78a608":"print(y_score1[0:7]) # how confident the classifier is that the class is 1\nprint(y_test[0:7]) # the actual classes","dcbeb79d":"false_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_test, y_score1)","ecac0c52":"print(false_positive_rate1)\nprint(true_positive_rate1)\nprint(threshold1)","e45882f9":"# Ploting ROC curves\nplt.subplots(1, figsize=(10,10))\nplt.title('Receiver Operating Characteristic - RandomForest')\nplt.plot([0, 1], ls=\"--\")\nplt.plot(false_positive_rate1, true_positive_rate1)\n\n#plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","86c78a07":" print('roc_auc_score for Random Forest: ', roc_auc_score(y_test, y_score1))","738f78f4":"## Steps 1 and 2","10f6a672":"# Understanding ROC curve\nadapted from: https:\/\/www.projectpro.io\/recipes\/plot-roc-curve-in-python","a3d58776":"## Step 4: make a few features categorical","18981698":"# How do different input data transformations affect model outcome?\nMy goal with this notebook is to find out how different data preprocessing techniques affect the model's evaluation metric.\n\n**My experiment:**\n* Use one model with a fixed set of hyperparameters. I chose LightGBM with these parameters:\n> lgb_params = {\n    'objective': 'binary',\n    'n_estimators': 1000,\n    'random_state': 29,\n    'learning_rate': 0.1,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n}\n* Use exactly the same features\n* Fix the random state\n* Vary the preprocessing technique\n    * Step 1: replace missing values with column mean (rmv)\n    * Step 2: rvm + different scalers\/transformers applied to all features\n    * Step 2a: rmv + StandardScaler \n    * Step 2b: rmv + MinMaxScaler\n    * Step 2c: rmv + RobustScaler\n    * Step 2d: rmv + PowerTransformer\n    * Step 2e: rmv + QuantileTransformer\n    * Step 3: rmv + different Scalers on different features\n    * Step 4a: rmv + adding categorical version of f40,f42,f65,f70,f75\n    * Step 4b: rmv + making f40,f42,f65,f70,f75 categorical\n   \n**My conclusions:**\n\n* My experiment did not have a big impact on the validation score. The best result gave using PowerTransformer on all features. The more experimental Steps 3 and 4 led to a worse score. \n* On the bright side, I finally learned how to use Pipelines for preprocessing.\n\n----\n**I'd appreciate your feedback and ideas. Let's share our knowledge so that we can all learn something new!**\n","20e1c3ff":"## Step 3: Changing the preprocessing, different preprocessing for different features"}}