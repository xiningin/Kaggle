{"cell_type":{"8b3596eb":"code","901dcea8":"code","2a70d6dd":"code","14fdffea":"code","3334fbba":"code","b5738fa3":"code","f7b539ce":"code","31bbcb3e":"code","56ae30a5":"code","14e8b8e2":"code","df569d2b":"code","e8ce8b33":"code","e91aa3f1":"code","98a5efd6":"code","5a39e3af":"code","7f048c12":"code","3cc5e4bf":"code","01d07212":"code","fcd41606":"code","0578ab33":"code","4b9c506b":"code","44e82a3c":"code","349637cd":"code","d8b90162":"code","38e2da73":"code","75dac5b5":"code","64521888":"code","8d1ea16e":"code","bef0b24d":"code","10e7670c":"code","c74ed8b3":"code","c92cfbfc":"code","41f94fa6":"code","9c307d20":"code","86099e9e":"code","4c6fac1c":"code","4732c7a2":"code","e4dfcae3":"code","fac7316c":"code","83b766ae":"code","a5dc9318":"markdown","5fbad407":"markdown","64237475":"markdown","83d6f251":"markdown","143e2bdd":"markdown","bb54eee4":"markdown","fb723e68":"markdown","d9ec9f71":"markdown","57842e06":"markdown","8d398fab":"markdown","a33eb8f5":"markdown","bd836dd5":"markdown","42002440":"markdown","4c407a88":"markdown","1de80d35":"markdown","90e03281":"markdown","3630e9b2":"markdown","0a494bc7":"markdown","0931fac9":"markdown","bf59926a":"markdown","f771f1f0":"markdown","4ff1f8f5":"markdown","c679a6c6":"markdown","402ccb97":"markdown","f5272c76":"markdown","62281e72":"markdown","a133e0ce":"markdown","24b4cf2f":"markdown","fe34b8e6":"markdown","0dc89177":"markdown","5ef2e201":"markdown","d7d7c831":"markdown","5642d84a":"markdown","a41e5b6f":"markdown","6ab69a02":"markdown","049cbed8":"markdown","d8c7231f":"markdown","8d6b4043":"markdown","8db8793d":"markdown","1f496e92":"markdown","91f68894":"markdown","01d18d6f":"markdown","3cec85ba":"markdown","b7737f52":"markdown","a9477225":"markdown","ec772316":"markdown","003df3c1":"markdown","5a974461":"markdown","2da30449":"markdown","130285b4":"markdown","a4002d5f":"markdown","1d8ab888":"markdown","2e6ba536":"markdown","d3bde8a8":"markdown","ea38fb27":"markdown","d0901bde":"markdown","bbc98a90":"markdown","983d1253":"markdown","c7099981":"markdown","f03db3c8":"markdown","3e477851":"markdown","5da5c6c4":"markdown","a903a77a":"markdown","5660e118":"markdown"},"source":{"8b3596eb":"# Import the neccessary modules for data manipulation and visual representation\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as matplot\nimport seaborn as sns\n%matplotlib inline","901dcea8":"#Read the analytics csv file and store our dataset into a dataframe called \"df\"\ndf = pd.DataFrame.from_csv('..\/input\/hr-analytics\/turnover.csv', index_col=None)","2a70d6dd":"# Check to see if there are any missing values in our data set\ndf.isnull().any()","14fdffea":"# Get a quick overview of what we are dealing with in our dataset\ndf.head()","3334fbba":"# Renaming certain columns for better readability\ndf = df.rename(columns={'satisfaction_level': 'satisfaction', \n                        'last_evaluation': 'evaluation',\n                        'number_project': 'projectCount',\n                        'average_montly_hours': 'averageMonthlyHours',\n                        'time_spend_company': 'yearsAtCompany',\n                        'Work_accident': 'workAccident',\n                        'promotion_last_5years': 'promotion',\n                        'sales' : 'department',\n                        'left' : 'turnover'\n                        })","b5738fa3":"# Move the reponse variable \"turnover\" to the front of the table\nfront = df['turnover']\ndf.drop(labels=['turnover'], axis=1,inplace = True)\ndf.insert(0, 'turnover', front)\ndf.head()","f7b539ce":"# The dataset contains 10 columns and 14999 observations\ndf.shape","31bbcb3e":"# Check the type of our features. \ndf.dtypes","56ae30a5":"# Looks like about 76% of employees stayed and 24% of employees left. \n# NOTE: When performing cross validation, its important to maintain this turnover ratio\nturnover_rate = df.turnover.value_counts() \/ len(df)\nturnover_rate","14e8b8e2":"# Display the statistical overview of the employees\ndf.describe()","df569d2b":"# Overview of summary (Turnover V.S. Non-turnover)\nturnover_Summary = df.groupby('turnover')\nturnover_Summary.mean()","e8ce8b33":"#Correlation Matrix\ncorr = df.corr()\ncorr = (corr)\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\n\ncorr","e91aa3f1":"# Let's compare the means of our employee turnover satisfaction against the employee population satisfaction\n#emp_population = df['satisfaction'].mean()\nemp_population = df['satisfaction'][df['turnover'] == 0].mean()\nemp_turnover_satisfaction = df[df['turnover']==1]['satisfaction'].mean()\n\nprint( 'The mean satisfaction for the employee population with no turnover is: ' + str(emp_population))\nprint( 'The mean satisfaction for employees that had a turnover is: ' + str(emp_turnover_satisfaction) )","98a5efd6":"import scipy.stats as stats\nstats.ttest_1samp(a=  df[df['turnover']==1]['satisfaction'], # Sample of Employee satisfaction who had a Turnover\n                  popmean = emp_population)  # Employee Who Had No Turnover satisfaction mean","5a39e3af":"degree_freedom = len(df[df['turnover']==1])\n\nLQ = stats.t.ppf(0.025,degree_freedom)  # Left Quartile\n\nRQ = stats.t.ppf(0.975,degree_freedom)  # Right Quartile\n\nprint ('The t-distribution left quartile range is: ' + str(LQ))\nprint ('The t-distribution right quartile range is: ' + str(RQ))\n","7f048c12":"# Set up the matplotlib figure\nf, axes = plt.subplots(ncols=3, figsize=(15, 6))\n\n# Graph Employee Satisfaction\nsns.distplot(df.satisfaction, kde=False, color=\"g\", ax=axes[0]).set_title('Employee Satisfaction Distribution')\naxes[0].set_ylabel('Employee Count')\n\n# Graph Employee Evaluation\nsns.distplot(df.evaluation, kde=False, color=\"r\", ax=axes[1]).set_title('Employee Evaluation Distribution')\naxes[1].set_ylabel('Employee Count')\n\n# Graph Employee Average Monthly Hours\nsns.distplot(df.averageMonthlyHours, kde=False, color=\"b\", ax=axes[2]).set_title('Employee Average Monthly Hours Distribution')\naxes[2].set_ylabel('Employee Count')","3cc5e4bf":"f, ax = plt.subplots(figsize=(15, 4))\nsns.countplot(y=\"salary\", hue='turnover', data=df).set_title('Employee Salary Turnover Distribution');","01d07212":"# Employee distri\n# Types of colors\ncolor_types = ['#78C850','#F08030','#6890F0','#A8B820','#A8A878','#A040A0','#F8D030',  \n                '#E0C068','#EE99AC','#C03028','#F85888','#B8A038','#705898','#98D8D8','#7038F8']\n\n# Count Plot (a.k.a. Bar Plot)\nsns.countplot(x='department', data=df, palette=color_types).set_title('Employee Department Distribution');\n \n# Rotate x-labels\nplt.xticks(rotation=-45)","fcd41606":"f, ax = plt.subplots(figsize=(15, 5))\nsns.countplot(y=\"department\", hue='turnover', data=df).set_title('Employee Department Turnover Distribution');","0578ab33":"ax = sns.barplot(x=\"projectCount\", y=\"projectCount\", hue=\"turnover\", data=df, estimator=lambda x: len(x) \/ len(df) * 100)\nax.set(ylabel=\"Percent\")","4b9c506b":"# Kernel Density Plot\nfig = plt.figure(figsize=(15,4),)\nax=sns.kdeplot(df.loc[(df['turnover'] == 0),'evaluation'] , color='b',shade=True,label='no turnover')\nax=sns.kdeplot(df.loc[(df['turnover'] == 1),'evaluation'] , color='r',shade=True, label='turnover')\nax.set(xlabel='Employee Evaluation', ylabel='Frequency')\nplt.title('Employee Evaluation Distribution - Turnover V.S. No Turnover')","44e82a3c":"#KDEPlot: Kernel Density Estimate Plot\nfig = plt.figure(figsize=(15,4))\nax=sns.kdeplot(df.loc[(df['turnover'] == 0),'averageMonthlyHours'] , color='b',shade=True, label='no turnover')\nax=sns.kdeplot(df.loc[(df['turnover'] == 1),'averageMonthlyHours'] , color='r',shade=True, label='turnover')\nax.set(xlabel='Employee Average Monthly Hours', ylabel='Frequency')\nplt.title('Employee AverageMonthly Hours Distribution - Turnover V.S. No Turnover')","349637cd":"#KDEPlot: Kernel Density Estimate Plot\nfig = plt.figure(figsize=(15,4))\nax=sns.kdeplot(df.loc[(df['turnover'] == 0),'satisfaction'] , color='b',shade=True, label='no turnover')\nax=sns.kdeplot(df.loc[(df['turnover'] == 1),'satisfaction'] , color='r',shade=True, label='turnover')\nplt.title('Employee Satisfaction Distribution - Turnover V.S. No Turnover')","d8b90162":"#ProjectCount VS AverageMonthlyHours [BOXPLOT]\n#Looks like the average employees who stayed worked about 200hours\/month. Those that had a turnover worked about 250hours\/month and 150hours\/month\n\nimport seaborn as sns\nsns.boxplot(x=\"projectCount\", y=\"averageMonthlyHours\", hue=\"turnover\", data=df)","38e2da73":"#ProjectCount VS Evaluation\n#Looks like employees who did not leave the company had an average evaluation of around 70% even with different projectCounts\n#There is a huge skew in employees who had a turnover though. It drastically changes after 3 projectCounts. \n#Employees that had two projects and a horrible evaluation left. Employees with more than 3 projects and super high evaluations left\nimport seaborn as sns\nsns.boxplot(x=\"projectCount\", y=\"evaluation\", hue=\"turnover\", data=df)","75dac5b5":"sns.lmplot(x='satisfaction', y='evaluation', data=df,\n           fit_reg=False, # No regression line\n           hue='turnover')   # Color by evolution stage","64521888":"ax = sns.barplot(x=\"yearsAtCompany\", y=\"yearsAtCompany\", hue=\"turnover\", data=df, estimator=lambda x: len(x) \/ len(df) * 100)\nax.set(ylabel=\"Percent\")","8d1ea16e":"# Import KMeans Model\nfrom sklearn.cluster import KMeans\n\n# Graph and create 3 clusters of Employee Turnover\nkmeans = KMeans(n_clusters=3,random_state=2)\nkmeans.fit(df[df.turnover==1][[\"satisfaction\",\"evaluation\"]])\n\nkmeans_colors = ['green' if c == 0 else 'blue' if c == 2 else 'red' for c in kmeans.labels_]\n\nfig = plt.figure(figsize=(10, 6))\nplt.scatter(x=\"satisfaction\",y=\"evaluation\", data=df[df.turnover==1],\n            alpha=0.25,color = kmeans_colors)\nplt.xlabel(\"Satisfaction\")\nplt.ylabel(\"Evaluation\")\nplt.scatter(x=kmeans.cluster_centers_[:,0],y=kmeans.cluster_centers_[:,1],color=\"black\",marker=\"X\",s=100)\nplt.title(\"Clusters of Employee Turnover\")\nplt.show()","bef0b24d":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (12,6)\n\n# Renaming certain columns for better readability\ndf = df.rename(columns={'satisfaction_level': 'satisfaction', \n                        'last_evaluation': 'evaluation',\n                        'number_project': 'projectCount',\n                        'average_montly_hours': 'averageMonthlyHours',\n                        'time_spend_company': 'yearsAtCompany',\n                        'Work_accident': 'workAccident',\n                        'promotion_last_5years': 'promotion',\n                        'sales' : 'department',\n                        'left' : 'turnover'\n                        })\n\n# Convert these variables into categorical variables\ndf[\"department\"] = df[\"department\"].astype('category').cat.codes\ndf[\"salary\"] = df[\"salary\"].astype('category').cat.codes\n\n# Create train and test splits\ntarget_name = 'turnover'\nX = df.drop('turnover', axis=1)\n\n\ny=df[target_name]\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15, random_state=123, stratify=y)\n\ndtree = tree.DecisionTreeClassifier(\n    #max_depth=3,\n    class_weight=\"balanced\",\n    min_weight_fraction_leaf=0.01\n    )\ndtree = dtree.fit(X_train,y_train)\n\n## plot the importances ##\nimportances = dtree.feature_importances_\nfeat_names = df.drop(['turnover'],axis=1).columns\n\n\nindices = np.argsort(importances)[::-1]\nplt.figure(figsize=(12,6))\nplt.title(\"Feature importances by DecisionTreeClassifier\")\nplt.bar(range(len(indices)), importances[indices], color='lightblue',  align=\"center\")\nplt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical',fontsize=14)\nplt.xlim([-1, len(indices)])\nplt.show()","10e7670c":"# Import the neccessary modules for data manipulation and visual representation\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as matplot\nimport seaborn as sns\n%matplotlib inline\n#Read the analytics csv file and store our dataset into a dataframe called \"df\"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, confusion_matrix, precision_recall_curve\nfrom sklearn.preprocessing import RobustScaler\ndf = pd.DataFrame.from_csv('..\/input\/HR_comma_sep.csv', index_col=None)\n\n# Renaming certain columns for better readability\ndf = df.rename(columns={'satisfaction_level': 'satisfaction', \n                        'last_evaluation': 'evaluation',\n                        'number_project': 'projectCount',\n                        'average_montly_hours': 'averageMonthlyHours',\n                        'time_spend_company': 'yearsAtCompany',\n                        'Work_accident': 'workAccident',\n                        'promotion_last_5years': 'promotion',\n                        'sales' : 'department',\n                        'left' : 'turnover'\n                        })\n\n# Convert these variables into categorical variables\ndf[\"department\"] = df[\"department\"].astype('category').cat.codes\ndf[\"salary\"] = df[\"salary\"].astype('category').cat.codes\n\n\n# Move the reponse variable \"turnover\" to the front of the table\nfront = df['turnover']\ndf.drop(labels=['turnover'], axis=1,inplace = True)\ndf.insert(0, 'turnover', front)\n\n# Create an intercept term for the logistic regression equation\ndf['int'] = 1\nindep_var = ['satisfaction', 'evaluation', 'yearsAtCompany', 'int', 'turnover']\ndf = df[indep_var]\n\n# Create train and test splits\ntarget_name = 'turnover'\nX = df.drop('turnover', axis=1)\n\ny=df[target_name]\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15, random_state=123, stratify=y)\n\nX_train.head()\n#","c74ed8b3":"import statsmodels.api as sm\niv = ['satisfaction','evaluation','yearsAtCompany', 'int']\nlogReg = sm.Logit(y_train, X_train[iv])\nanswer = logReg.fit()\n\nanswer.summary\nanswer.params","c92cfbfc":"# Create function to compute coefficients\ncoef = answer.params\ndef y (coef, Satisfaction, Evaluation, YearsAtCompany) : \n    return coef[3] + coef[0]*Satisfaction + coef[1]*Evaluation + coef[2]*YearsAtCompany\n\nimport numpy as np\n\n# An Employee with 0.7 Satisfaction and 0.8 Evaluation and worked 3 years has a 14% chance of turnover\ny1 = y(coef, 0.7, 0.8, 3)\np = np.exp(y1) \/ (1+np.exp(y1))\np","41f94fa6":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, confusion_matrix, precision_recall_curve\nfrom sklearn.preprocessing import RobustScaler\n","9c307d20":"# Create base rate model\ndef base_rate_model(X) :\n    y = np.zeros(X.shape[0])\n    return y","86099e9e":"# Create train and test splits\ntarget_name = 'turnover'\nX = df.drop('turnover', axis=1)\n#robust_scaler = RobustScaler()\n#X = robust_scaler.fit_transform(X)\ny=df[target_name]\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15, random_state=123, stratify=y)","4c6fac1c":"# Check accuracy of base rate model\ny_base_rate = base_rate_model(X_test)\nfrom sklearn.metrics import accuracy_score\nprint (\"Base rate accuracy is %2.2f\" % accuracy_score(y_test, y_base_rate))","4732c7a2":"# Check accuracy of Logistic Model\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(penalty='l2', C=1)\n\nmodel.fit(X_train, y_train)\nprint (\"Logistic accuracy is %2.2f\" % accuracy_score(y_test, model.predict(X_test)))","e4dfcae3":"# Using 10 fold Cross-Validation to train our Logistic Regression Model\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nkfold = model_selection.KFold(n_splits=10, random_state=7)\nmodelCV = LogisticRegression(class_weight = \"balanced\")\nscoring = 'roc_auc'\nresults = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\nprint(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))","fac7316c":"# Compare the Logistic Regression Model V.S. Base Rate Model V.S. Random Forest Model\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n\nprint (\"---Base Model---\")\nbase_roc_auc = roc_auc_score(y_test, base_rate_model(X_test))\nprint (\"Base Rate AUC = %2.2f\" % base_roc_auc)\nprint(classification_report(y_test, base_rate_model(X_test)))\n\n# NOTE: By adding in \"class_weight = balanced\", the Logistic Auc increased by about 10%! This adjusts the threshold value\nlogis = LogisticRegression(class_weight = \"balanced\")\nlogis.fit(X_train, y_train)\nprint (\"\\n\\n ---Logistic Model---\")\nlogit_roc_auc = roc_auc_score(y_test, logis.predict(X_test))\nprint (\"Logistic AUC = %2.2f\" % logit_roc_auc)\nprint(classification_report(y_test, logis.predict(X_test)))\n\n# Decision Tree Model\ndtree = tree.DecisionTreeClassifier(\n    #max_depth=3,\n    class_weight=\"balanced\",\n    min_weight_fraction_leaf=0.01\n    )\ndtree = dtree.fit(X_train,y_train)\nprint (\"\\n\\n ---Decision Tree Model---\")\ndt_roc_auc = roc_auc_score(y_test, dtree.predict(X_test))\nprint (\"Decision Tree AUC = %2.2f\" % dt_roc_auc)\nprint(classification_report(y_test, dtree.predict(X_test)))\n\n# Random Forest Model\nrf = RandomForestClassifier(\n    n_estimators=1000, \n    max_depth=None, \n    min_samples_split=10, \n    class_weight=\"balanced\"\n    #min_weight_fraction_leaf=0.02 \n    )\nrf.fit(X_train, y_train)\nprint (\"\\n\\n ---Random Forest Model---\")\nrf_roc_auc = roc_auc_score(y_test, rf.predict(X_test))\nprint (\"Random Forest AUC = %2.2f\" % rf_roc_auc)\nprint(classification_report(y_test, rf.predict(X_test)))\n\n\n# Ada Boost\nada = AdaBoostClassifier(n_estimators=400, learning_rate=0.1)\nada.fit(X_train,y_train)\nprint (\"\\n\\n ---AdaBoost Model---\")\nada_roc_auc = roc_auc_score(y_test, ada.predict(X_test))\nprint (\"AdaBoost AUC = %2.2f\" % ada_roc_auc)\nprint(classification_report(y_test, ada.predict(X_test)))","83b766ae":"# Create ROC Graph\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, logis.predict_proba(X_test)[:,1])\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rf.predict_proba(X_test)[:,1])\ndt_fpr, dt_tpr, dt_thresholds = roc_curve(y_test, dtree.predict_proba(X_test)[:,1])\nada_fpr, ada_tpr, ada_thresholds = roc_curve(y_test, ada.predict_proba(X_test)[:,1])\n\nplt.figure()\n\n# Plot Logistic Regression ROC\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n\n# Plot Random Forest ROC\nplt.plot(rf_fpr, rf_tpr, label='Random Forest (area = %0.2f)' % rf_roc_auc)\n\n# Plot Decision Tree ROC\nplt.plot(dt_fpr, dt_tpr, label='Decision Tree (area = %0.2f)' % dt_roc_auc)\n\n# Plot AdaBoost ROC\nplt.plot(ada_fpr, ada_tpr, label='AdaBoost (area = %0.2f)' % ada_roc_auc)\n\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],label='Base Rate' 'k--')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Graph')\nplt.legend(loc=\"lower right\")\nplt.show()","a5dc9318":"**Edit 5:** Added decision tree classifier feature importance. Added visualization for decision tree. 9\/30\/2017","5fbad407":"**Edit 3**: Added pd.get_dummies for 'salary' and 'department' features. This increased the AUC score by 2% (76%-78%) 9\/23\/2017","64237475":"# Feature Importance\n***\n**Summary:**\n\nBy using a decision tree classifier, it could rank the features used for the prediction. The top three features were employee satisfaction, yearsAtCompany, and evaluation. This is helpful in creating our model for logistic regression because it\u2019ll be more interpretable to understand what goes into our model when we utilize less features. \n\n**Top 3 Features:**\n1. Satisfaction\n2. YearsAtCompany\n3. Evaluation\n","83d6f251":"## 3b2. Statistical Test for Correlation\n***\n\n### One-Sample T-Test (Measuring Satisfaction Level)\nA one-sample t-test checks whether a sample mean differs from the population mean. Since satisfaction has the highest correlation with our dependent variable turnover, let's test to see whether the average satisfaction level of employees that had a turnover differs from the those that had no turnover.\n\n**Hypothesis Testing:** Is there significant difference in the **means of satisfaction level** between employees who had a turnover and temployees who had no turnover?\n\n - **Null Hypothesis:** *(H0: pTS = pES)* The null hypothesis would be that there is **no** difference in satisfaction level between employees who did turnover and those who did not..\n\n - **Alternate Hypothesis:** *(HA: pTS != pES)* The alternative hypothesis would be that there **is** a difference in satisfaction level between employees who did turnover and those who did not..","143e2bdd":"## 3n. K-Means Clustering of Employee Turnover\n***\n**Cluster 1 (Blue):** Hard-working and Sad Employees\n\n**Cluster 2 (Red):** Bad and Sad Employee \n\n**Cluster 3 (Green):** Hard-working and Happy Employee \n\n**Clustering PROBLEM:**\n    - How do we know that there are \"3\" clusters?\n    - We would need expert domain knowledge to classify the right amount of clusters\n    - Hidden uknown structures could be present","bb54eee4":"## Potential Solution\n***\n**Binary Classification**: Turnover V.S. Non Turnover\n\n**Instance Scoring**: Likelihood of employee responding to an offer\/incentive to save them from leaving.\n\n**Need for Application**: Save employees from leaving\n\nIn our employee retention problem, rather than simply predicting whether an employee will leave the company within a certain time frame, we would much rather have an estimate of the probability that he\/she will leave the company. \nWe would rank employees by their probability of leaving, then allocate a limited incentive budget to the highest probability instances. \n\nConsider employee turnover domain where an employee is given treatment by Human  Resources because they think the employee will leave the company within a month, but the employee actually does not. This is a false positive. This mistake could be expensive, inconvenient, and time consuming for both the Human Resources and employee, but is a good investment for relational growth. \n\nCompare this with the opposite error, where Human Resources does not give treatment\/incentives to the employees and they do leave. This is a false negative. This type of error is more detrimental because the company lost an employee, which could lead to great setbacks and more money to rehire. \nDepending on these errors, different costs are weighed based on the type of employee being treated. For example, if it\u2019s a high-salary employee then would we need a costlier form of treatment? What if it\u2019s a low-salary employee? The cost for each error is different and should be weighed accordingly. \n \n **Solution 1:** \n - We can rank employees by their probability of leaving, then allocate a limited incentive budget to the highest probability instances.\n - OR, we can allocate our incentive budget to the instances with the highest expected loss, for which we'll need the probability of turnover.\n\n**Solution 2:** \nDevelop learning programs for managers. Then use analytics to gauge their performance and measure progress. Some advice:\n - Be a good coach\n - Empower the team and do not micromanage\n - Express interest for team member success\n - Have clear vision \/ strategy for team\n - Help team with career development    ","fb723e68":"**\"Yeah, they all said that to me...\"**, *Bob replied as we were at Starbucks sipping on our dark roast coffee. Bob is a friend of mine and was the owner of a multi-million dollar company, that's right, \"m-i-l-l-i-o-n\". He used to tell me stories about how his company's productivity and growth has sky rocketed from the previous years and everything has been going great. But recently, he's been noticing some decline within his company. In a five month period, he lost one-fifth of his employees. At least a dozen of them throughout each department made phone calls and even left sticky notes on their tables informing him about their leave. Nobody knew what was happening. In that year, he was contemplating about filing for bankruptcy. Fast-forward seven months later, he's having a conversation with his co-founder of the company. The conversation ends with, **\"I quit...\"**","d9ec9f71":"**Edit 6:** Added more information about precision\/recall and class imbalance solutions. Updated potential solution section and included a new section: evaluating model. 10\/1\/2017","57842e06":"**Edit 1**: Added Hypothesis testing for employee turnover satisfaction and entire employee satisfaction 8\/29\/2017","8d398fab":"## Using Logistic Regression Coefficients \n***\nWith the elimination of the other variables, I\u2019ll be using the three most important features to create our model: Satisfaction, Evaluation, and YearsAtCompany.\n\nFollowing overall equation was developed: \n\n**Employee Turnover Score** = Satisfaction*(**-3.769022**) + Evaluation*(**0.207596**) + YearsAtCompany*(**0.170145**) + **0.181896**\n","a33eb8f5":"**Note: Evaluating the Model**\n***\n**Precision and Recall \/ Class Imbalance**\n\nThis dataset is an example of a class imbalance problem because of the skewed distribution of employees who did and did not leave. More skewed the class means that accuracy breaks down. \n\nIn this case, evaluating our model\u2019s algorithm based on **accuracy** is the **wrong** thing to measure. We would have to know the different errors that we care about and correct decisions. Accuracy alone does not measure an important concept that needs to be taken into consideration in this type of evaluation: **False Positive** and **False Negative** errors. \n\n**False Positives (Type I Error)**: You predict that the employee will leave, but do not\n\n**False Negatives (Type II Error)**: You predict that the employee will not leave, but does leave\n\nIn this problem, what type of errors do we care about more? False Positives or False Negatives?\n","bd836dd5":"# Part 3: Exploring the Data\n*** \n <img  src=\"https:\/\/s-media-cache-ak0.pinimg.com\/originals\/32\/ef\/23\/32ef2383a36df04a065b909ee0ac8688.gif\"\/>","42002440":"### Conducting the T-Test\n***\nLet's conduct a t-test at **95% confidence level** and see if it correctly rejects the null hypothesis that the sample comes from the same distribution as the employee population. To conduct a one sample t-test, we can use the **stats.ttest_1samp()** function:","4c407a88":"**Edit 2**: Added Turnover VS Satisfaction graph 9\/14\/2017 ","1de80d35":"## Retention Plan Using Logistic Regression\n***\n\n**Reference:** http:\/\/rupeshkhare.com\/wp-content\/uploads\/2013\/12\/Employee-Attrition-Risk-Assessment-using-Logistic-Regression-Analysis.pdf\n\nWith the logistic regression model, we can now use our scores and evaluate the employees through different scoring metrics. Each zone is explain here:\n\n1.\t**Safe Zone (Green)** \u2013 Employees within this zone are considered safe. \n2.\t**Low Risk Zone (Yellow)** \u2013 Employees within this zone are too be taken into consideration of potential turnover. This is more of a long-term track.\n3.\t**Medium Risk Zone (Orange)** \u2013 Employees within this zone are at risk of turnover. Action should be taken and monitored accordingly. \n4.\t**High Risk Zone (Red)** \u2013 Employees within this zone are considered to have the highest chance of turnover. Action should be taken immediately. \n\nSo with our example above, the employee with a **14%** turnover score will be in the **safe zone**. \n\n<img src=\"http:\/\/i64.tinypic.com\/somk9s.jpg\"\/>\n","90e03281":"That is the last thing anybody wants to hear from their employees. In a sense, it\u2019s the employees who make the company. It\u2019s the employees who do the work. It\u2019s the employees who shape the company\u2019s culture. Long-term success, a healthy work environment, and high employee retention are all signs of a successful company. But when a company experiences a high rate of employee turnover, then something is going wrong. This can lead the company to huge monetary losses by these innovative and valuable employees.","3630e9b2":"##  3c. Distribution Plots (Satisfaction - Evaluation - AverageMonthlyHours)\n***\n**Summary:** Let's examine the distribution on some of the employee's features. Here's what I found:\n - **Satisfaction** - There is a huge spike for employees with low satisfaction and high satisfaction.\n - **Evaluation** - There is a bimodal distrubtion of employees for low evaluations (less than 0.6) and high evaluations (more than 0.8)\n - **AverageMonthlyHours** - There is another bimodal distribution of employees with lower and higher average monthly hours (less than 150 hours & more than 250 hours)\n - The evaluation and average monthly hour graphs both share a similar distribution. \n - Employees with lower average monthly hours were evaluated less and vice versa.\n - If you look back at the correlation matrix, the high correlation between evaluation and averageMonthlyHours does support this finding.\n \n**Stop and Think:** \n - Is there a reason for the high spike in low satisfaction of employees?\n - Could employees be grouped in a way with these features?\n - Is there a correlation between evaluation and averageMonthlyHours?","0a494bc7":"## ROC Graph\n***","0931fac9":"##  3g. Turnover V.S. Evaluation \n***\n**Summary:** \n - There is a biomodal distribution for those that had a turnover. \n - Employees with **low** performance tend to leave the company more\n - Employees with **high** performance tend to leave the company more\n - The **sweet spot** for employees that stayed is within **0.6-0.8** evaluation","bf59926a":"<img src=\"https:\/\/cdn.dribbble.com\/users\/20727\/screenshots\/2118641\/video-producer-motion-graphics-designer-animator-jobs-manchester-uk.gif\"\/>","f771f1f0":"## Logistic Regression V.S. Random Forest V.S. Decision Tree V.S. AdaBoost Model\n***","4ff1f8f5":"# Edits:\n***\n\n**To Do's:** \n1. Define \"high performers\". It's ambiguous and is normally determined by relationships. Could be inconsistent. To verify, do a **Evaluation V.S. Department**.\n\n2. Create Expected Value Model. Cost and Benefits. Understand the cost of targeting and cost of employee leaving. Known as Cost Matrix.\n\n3. Create a tableu dashboard for relevant\/important information and highlight ","c679a6c6":"##  3e. Department V.S. Turnover \n***\n**Summary:** Let's see more information about the departments. Here's what I found:\n - The **sales, technical, and support department** were the top 3 departments to have employee turnover\n - The management department had the smallest amount of turnover\n \n**Stop and Think:** \n - If we had more information on each department, can we pinpoint a more direct cause for employee turnover?","402ccb97":"# Part 2: Scrubbing the Data \n***","f5272c76":"##  3l. Satisfaction VS Evaluation\n***\n**Summary:** This is by far the most compelling graph. This is what I found:\n - There are **3** distinct clusters for employees who left the company\n \n**Cluster 1 (Hard-working and Sad Employee):** Satisfaction was below 0.2 and evaluations were greater than 0.75. Which could be a good indication that employees who left the company were good workers but felt horrible at their job. \n - **Question:** What could be the reason for feeling so horrible when you are highly evaluated? Could it be working too hard? Could this cluster mean employees who are \"overworked\"?\n\n**Cluster 2 (Bad and Sad Employee):** Satisfaction between about 0.35~0.45 and evaluations below ~0.58. This could be seen as employees who were badly evaluated and felt bad at work.\n - **Question:** Could this cluster mean employees who \"under-performed\"?\n\n**Cluster 3 (Hard-working and Happy Employee):** Satisfaction between 0.7~1.0 and evaluations were greater than 0.8. Which could mean that employees in this cluster were \"ideal\". They loved their work and were evaluated highly for their performance. \n - **Question:** Could this cluser mean that employees left because they found another job opportunity?","62281e72":"## Intepretation of Score\n***\nIf you were to use these employee values into the equation:\n- **Satisfaction**: 0.7\n- **Evaluation**: 0.8\n- **YearsAtCompany**: 3\n\nYou would get:\n\n**Employee Turnover Score** = (**0.7**)*(-3.769022) + (**0.8**)*(0.207596) + (**3**)*(0.170145) + 0.181896 = 0.14431 = **14%***\n\n**Result**: This employee would have a **14%** chance of leaving the company. This information can then be used to form our retention plan.","a133e0ce":"<img src=\"https:\/\/content.linkedin.com\/content\/dam\/brand\/site\/img\/logo\/logo-tm.png\"\/>","24b4cf2f":"##  3h. Turnover V.S. AverageMonthlyHours \n***\n**Summary:** \n - Another bi-modal distribution for employees that turnovered \n - Employees who had less hours of work **(~150hours or less)** left the company more\n - Employees who had too many hours of work **(~250 or more)** left the company \n - Employees who left generally were **underworked** or **overworked**.","fe34b8e6":"### T-Test Result\n***\nThe test result shows the **test statistic \"t\" is equal to -51.33**. This test statistic tells us how much the sample mean deviates from the null hypothesis. If the t-statistic lies **outside** the quantiles of the t-distribution corresponding to our confidence level and degrees of freedom, we reject the null hypothesis. We can check the quantiles with **stats.t.ppf()**:","0dc89177":"##  3a. Statistical Overview \n***\nThe dataset has:\n - About 15,000 employee observations and 10 features \n - The company had a turnover rate of about 24%\n - Mean satisfaction of employees is 0.61","5ef2e201":"## Client\n***\n*Bob the Boss*","d7d7c831":"## About This Kernel\n***\n**Feel free to use this kernel as a reference as a template for your analysis :)**\n\nFor those that are in or interested in **Human Resources** and would like a detailed guide on how to approach an **employee retention** problem through a **data science** point of view, feel free to check this notebook out.. \n\nI will be covering my analysis and approach through different process flows in the data science pipeline, which includes statistical inference and exploratory data analysis. The main goal is to understand the reasoning behind employee turnover and to come up with a model to classify an employee\u2019s risk of attrition. A recommendation for a retention plan was created, which incorporates some best practices for employee retention at different risk levels of attrition.\n\n*Hopefully the kernel added some new insights\/perspectives to the data science community! If there are any suggestions\/changes you would like to see in the Kernel please let me know :). Appreciate every ounce of help!* \n\n*This notebook will always be a work in progress. Please leave any comments about further improvements to the notebook! Any feedback or constructive criticism is greatly appreciated!. Thank you guys!*\n\n## UPDATE: R Version\n***\n**R Users:**\n\n**Thanks to Ragul, he has created a similar kernel but using R. Check it out if you are an R user!**\n\nhttps:\/\/www.kaggle.com\/ragulram\/hr-analytics-exploration-and-modelling-with-r","5642d84a":"##  3m. Turnover V.S. YearsAtCompany \n***\n**Summary:** Let's see if theres a point where employees start leaving the company. Here's what I found:\n - More than half of the employees with **4 and 5** years left the company\n - Employees with **5** years should **highly** be looked into \n \n**Stop and Think:** \n - Why are employees leaving mostly at the **3-5** year range?\n - Who are these employees that left?\n - Are these employees part-time or contractors? ","a41e5b6f":"# Human Resources Analytics # All in one\n***","6ab69a02":"**Edit 7:** Added an in-depth interpretation of logistic regression model. Using this for a more interpretable classifier. Showing how the coefficents are computed and how each variable is presented in the algoirthm. Added a retention plan as a metric to evaluate our model. 10\/11\/2017","049cbed8":"## Objective\n***\n*The company wants to understand what factors contributed most to employee turnover and to create a model that can predict if a certain employee will leave the company or not. The goal is to create or improve different retention strategies on targeted employees. Overall, the implementation of this model will allow management to create better decision-making actions. *","d8c7231f":" **Note: Different Ways to Evaluate Classification Models**\n ***\n   1.  **Predictive Accuracy:** How many does it get right?\n   2. **Speed:** How fast does it take for the model to deploy? \n   3. **Scalability:** Can the model handle large datasets?\n   4. **Robustness:** How well does the model handle outliers\/missing values?\n   5. **Interpretability:** Is the model easy to understand?","8d6b4043":"## What would you do?\n***\n\n**Reddit Commentor (DSPublic):** I worked in HR for a couple of years and here's a few questions I have:\nPeople that have HIGH salary and not been promoted, did they leave? If so, could it be a signal that we're not developing people or providing enough opportunities?\n\nHow would you define a 'high performer' without using their last evaluation rating? Evaluations tend to be inconsistently applied across departments and highly dependent on your relationship with the person doing that evaluation. Can we do an Evaluation Vs. Departments (see if there are actual differences)?\nOnce defined, did these high performers leave? If so, why? Are we not providing opportunities or recognizing these high performers? Is it a lack of salary?\n\nTo add some additional context, 24% turnover rate is high in general but do we know what industry this is from? If the industry norm is 50%, this company is doing great! I see you've done Turnover by dept which is great. If only we have more info and classify these turnovers.\n\nWe have voluntary and involuntary turnovers as well. Also, who are these employees - is it part timers, contract workers that turn over? We don't worry about those, they're suppose to go. I'd like to see Turnover vs. Years of service. In real life, we found a cluster \/ turning point where people 'turn sour' after about 5 years at the company. Can we see satisfaction vs. years at company?","8db8793d":"<img src=\"http:\/\/cliparts.co\/cliparts\/6iy\/oBb\/6iyoBbdpT.gif\"\/>","1f496e92":"*Typically, cleaning the data requires a lot of work and can be a very tedious procedure. This dataset from Kaggle is super clean and contains no missing values. But still, I will have to examine the dataset to make sure that everything else is readable and that the observation values match the feature names appropriately.*","91f68894":"Companies that maintain a healthy organization and culture are always a good sign of future prosperity. Recognizing and understanding what factors that were associated with employee turnover will allow companies and individuals to limit this from happening and may even increase employee productivity and growth. These predictive insights give managers the opportunity to take corrective steps to build and preserve their successful business.","01d18d6f":"** \"You don't build a business. You build people, and people build the business.\" - Zig Ziglar**\n***\n\n<img src=\"https:\/\/static1.squarespace.com\/static\/5144a1bde4b033f38036b7b9\/t\/56ab72ebbe7b96fafe9303f5\/1454076676264\/\"\/>","3cec85ba":"### T-Test Quantile\n***\nIf the t-statistic value we calculated above **(-51.33)** is outside the quantiles, then we can reject the null hypothesis","b7737f52":"# Part 1: Obtaining the Data \n***","a9477225":"##  3d. Salary V.S. Turnover\n***\n**Summary:** This is not unusual. Here's what I found:\n - Majority of employees who left either had **low** or **medium** salary.\n - Barely any employees left with **high** salary\n - Employees with low to average salaries tend to leave the company.\n \n**Stop and Think:** \n - What is the work environment like for low, medium, and high salaries?\n - What made employees with high salaries to leave?","ec772316":"##  3b. Correlation Matrix & Heatmap\n***\n**Moderate Positively Correlated Features:** \n- projectCount vs evaluation: 0.349333\n- projectCount vs averageMonthlyHours:  0.417211\n- averageMonthlyHours vs evaluation: 0.339742\n\n**Moderate Negatively Correlated Feature:**\n - satisfaction vs turnover:  -0.388375\n\n**Stop and Think:**\n- What features affect our target variable the most (turnover)?\n- What features have strong correlations with each other?\n- Can we do a more in depth examination of these features?\n\n**Summary:**\n\nFrom the heatmap, there is a **positive(+)** correlation between projectCount, averageMonthlyHours, and evaluation. Which could mean that the employees who spent more hours and did more projects were evaluated highly. \n\nFor the **negative(-)** relationships, turnover and satisfaction are highly correlated. I'm assuming that people tend to leave a company more when they are less satisfied. ","003df3c1":"# Let's Connect!\nIf anybody would like to discuss any other projects or just have a chat about data science topics, I'll be more than happy to connect with you on:\n\n**LinkedIn:**\nhttps:\/\/www.linkedin.com\/in\/randylaosat\/\n\n**My Website:**\nhttp:\/\/randylaosat.strikingly.com\/\n\n**This notebook will always be a work in progress. Please leave any comments about further improvements to the notebook! Any feedback or constructive criticism is greatly appreciated. Thank you guys!**","5a974461":"# 4a. Modeling the Data: Logistic Regression Analysis\n***\n**NOTE:** This will be an in-depth analysis of using logistic regression as a classifier. I do go over other types of models in the other section below this. **This is more of a use-case example of what can be done and explained to management in a company.**\n\nLogistic Regression commonly deals with the issue of how likely an observation is to belong to each group. This model is commonly used to predict the likelihood of an event occurring. In contrast to linear regression, the output of logistic regression is transformed with a logit function. This makes the output either 0 or 1. This is a useful model to take advantage of for this problem because we are interested in predicting whether an employee will leave (0) or stay (1). \n\nAnother reason for why logistic regression is the preferred model of choice is because of its interpretability. Logistic regression predicts the outcome of the response variable (turnover) through a set of other explanatory variables, also called predictors. In context of this domain, the value of our response variable is categorized into two forms: 0 (zero) or 1 (one). The value of 0 (zero) represents the probability of an employee not leaving the company and the value of 1 (one) represents the probability of an employee leaving the company.\n\n**Logistic Regression models the probability of \u2018success\u2019 as: **\n<img src=\"http:\/\/i64.tinypic.com\/m95a9k.jpg\"\/>\n \nThe equation above shows the relationship between, the dependent variable (success), denoted as (\u03b8) and independent variables or predictor of event, denoted as xi. Where \u03b1 is the constant of the equation and, \u03b2 is the coefficient of the predictor variables\n\n","2da30449":"##  3k. ProjectCount VS Evaluation\n***\n**Summary:** This graph looks very similar to the graph above. What I find strange with this graph is with the turnover group. There is an increase in evaluation for employees who did more projects within the turnover group. But, again for the non-turnover group, employees here had a consistent evaluation score despite the increase in project counts. \n\n**Questions to think about:**\n - **Why is it that employees who left, had on average, a higher evaluation than employees who did not leave, even with an increase in project count? **\n - Shouldn't employees with lower evaluations tend to leave the company more? ","130285b4":"<img src=\"http:\/\/i68.tinypic.com\/qsts7k.jpg\"\/>","a4002d5f":"# Google Docs Report \n***\nhttps:\/\/docs.google.com\/document\/d\/1E1oBewdQuX0f_LW26vKV_jcyUCNZqlivICss-ORZFtw\/edit?usp=sharing","1d8ab888":"# 5. Interpreting the Data\n***\n\n**Summary:** \nWith all of this information, this is what Bob should know about his company and why his employees probably left:\n 1. Employees generally left when they are **underworked** (less than 150hr\/month or 6hr\/day)\n 2. Employees generally left when they are **overworked** (more than 250hr\/month or 10hr\/day)\n 3. Employees with either **really high or low evaluations** should be taken into consideration for high turnover rate\n 4. Employees with **low to medium salaries** are the bulk of employee turnover\n 5. Employees that had **2,6, or 7 project count** was at risk of leaving the company\n 6. Employee **satisfaction** is the highest indicator for employee turnover.\n 7. Employee that had **4 and 5 yearsAtCompany** should be taken into consideration for high turnover rate\n 8. Employee **satisfaction**, **yearsAtCompany**, and **evaluation** were the three biggest factors in determining turnover.","2e6ba536":"### One-Sample T-Test Summary\n***\n#### **T-Test = -51.33**       |        **P-Value = 0.000_**       |       **Reject Null Hypothesis**\n\n## Question: How come the P-Value is literally 0.0? Can anybody answer this?\n\n**Reject the null hypothesis because:**\n - T-Test score is outside the quantiles\n - P-value is lower than confidence level of 5%\n\nBased on the statistical analysis of a one sample t-test, there seems to be some significant difference between the mean satisfaction of employees who had a turnover and the entire employee population. The super low P-value of **0.00_** at a 5% confidence level is a good indicator to **reject the null hypothesis**. \n\nBut this does not neccessarily mean that there is practical significance. We would have to conduct more experiments or maybe collect more data about the employees in order to come up with a more accurate finding.\n\n<img src=\"https:\/\/static1.squarespace.com\/static\/5144a1bde4b033f38036b7b9\/t\/56714b05c647ad9f555348fa\/1450265419456\/PresentWIP.gif?format=500w\"\/>","d3bde8a8":"# 4b. Using Other Models\n***\n**NOTE:** I'll be using four other models in this section to measure the accuracy of the different models\n\n The best model performance out of the four (Decision Tree Model, AdaBoost Model, Logistic Regression Model, Random Forest Model) was **Random Forest**! \n \n **Remember:** Machines can predict the future, as long as the future doesn't look too different from the past.\n \n **Note: Base Rate** \n ***\n - A **Base Rate Model** is a simple model or heuristic used as reference point for comparing how well a model is performing. A baseline helps model developers quantify the minimal, expected performance on a particular problem. In this dataset, the majority class that will be predicted will be **0's**, which are employees who did not leave the company. \n - If you recall back to *Part 3: Exploring the Data*, 24% of the dataset contained 1's (employee who left the company) and the remaining 76% contained 0's (employee who did not leave the company). The Base Rate Model would simply predict every 0's and ignore all the 1's. \n - **Example**: The base rate accuracy for this data set, when classifying everything as 0's, would be 76% because 76% of the dataset are labeled as 0's (employees not leaving the company).","ea38fb27":"## Recommended Websites:\n***\n\nStatiscal Concepts: https:\/\/www.youtube.com\/user\/BCFoltz\/playlists\n\nCommon Machine Learning Algorithms: https:\/\/www.linkedin.com\/pulse\/machine-learning-whats-inside-box-randy-lao\/\n\nBasics of Machine Learning: https:\/\/www.linkedin.com\/pulse\/machine-learning-fresh-bloods-randy-lao\/\n\nData Science Pipeline (OSEMN): https:\/\/www.linkedin.com\/pulse\/life-data-science-osemn-randy-lao\/\n\n","d0901bde":"##  3f. Turnover V.S. ProjectCount \n***\n**Summary:** This graph is quite interesting as well. Here's what I found:\n - More than half of the employees with **2,6, and 7** projects left the company\n - Majority of the employees who did not leave the company had **3,4, and 5** projects\n - All of the employees with **7** projects left the company\n - There is an increase in employee turnover rate as project count increases\n \n**Stop and Think:** \n - Why are employees leaving at the lower\/higher spectrum of project counts?\n - Does this means that employees with project counts 2 or less are not worked hard enough or are not highly valued, thus leaving the company?\n - Do employees with 6+ projects are getting overworked, thus leaving the company?\n\n","bbc98a90":"**Edit 4:** Added Random Forest Model and updated the ROC Curve. Added Base Rate Model explanation. Added AdaBoost Model. Added Decision Tree Model  9\/27\/2017","983d1253":"## What Now?\n***\nThis problem is about people decision. When modeling the data, we should not be using this predictive metric as a solution decider. But, we can use this to arm people with much better relevant information for better decision making.\n\nWe would have to conduct more experiments or collect more data about the employees in order to come up with a more accurate finding. I would recommend to gather more variables from the database that could have more impact on determining employee turnover and satisfaction such as their distance from home, gender, age, and etc.\n\n**Reverse Engineer the Problem**\n***\nAfter trying to understand what caused employees to leave in the first place, we can form another problem to solve by asking ourselves \n1. **\"What features caused employees stay?** \n2. **\"What features contributed to employee retention?**\n**\nThere are endless problems to solve!","c7099981":"##  3j. ProjectCount VS AverageMonthlyHours \n***\n\n**Summary:**\n - As project count increased, so did average monthly hours\n - Something weird about the boxplot graph is the difference in averageMonthlyHours between people who had a turnver and did not. \n - Looks like employees who **did not** have a turnover had **consistent** averageMonthlyHours, despite the increase in projects\n - In contrast, employees who **did** have a turnover had an increase in averageMonthlyHours with the increase in projects\n\n**Stop and Think:** \n - What could be the meaning for this? \n - **Why is it that employees who left worked more hours than employees who didn't, even with the same project count?**","f03db3c8":"##  3i. Turnover V.S. Satisfaction \n***\n**Summary:** \n - There is a **tri-modal** distribution for employees that turnovered\n - Employees who had really low satisfaction levels **(0.2 or less)** left the company more\n - Employees who had low satisfaction levels **(0.3~0.5)** left the company more\n - Employees who had really high satisfaction levels **(0.7 or more)** left the company more","3e477851":"## Business Problem\n***\n*Bob's multi-million dollar company is about to go bankrupt and he wants to know why his employees are leaving.*","5da5c6c4":"## Explanation of Coefficients\n*** \n**Employee Turnover Score** = Satisfaction*(**-3.769022**) + Evaluation*(**0.207596**) + YearsAtCompany*(**0.170145**) + **0.181896**\n\nThe values above are the coefficient assigned to each independent variable. The **constant** 0.181896 represents the effect of all uncontrollable variables. \n\n","a903a77a":"## OSEMN Pipeline\n****\n\n*I\u2019ll be following a typical data science pipeline, which is call \u201cOSEMN\u201d (pronounced awesome).*\n\n1. **O**btaining the data is the first approach in solving the problem.\n\n2. **S**crubbing or cleaning the data is the next step. This includes data imputation of missing or invalid data and fixing column names.\n\n3. **E**xploring the data will follow right after and allow further insight of what our dataset contains. Looking for any outliers or weird data. Understanding the relationship each explanatory variable has with the response variable resides here and we can do this with a correlation matrix. \n\n4. **M**odeling the data will give us our predictive power on whether an employee will leave. \n\n5. I**N**terpreting the data is last. With all the results and analysis of the data, what conclusion is made? What factors contributed most to employee turnover? What relationship of variables were found? \n\n**Note:** *The data was found from the \u201cHuman Resources Analytics\u201d dataset provided by Kaggle\u2019s website. https:\/\/www.kaggle.com\/ludobenistant\/hr-analytics*\n\n**Note:** THIS DATASET IS **SIMULATED**.","5660e118":"**\"You don't build a business. You build people, and people build the business.\" - Zig Ziglar**\n***\n<img src=\"http:\/\/www.goldbeck.com\/hrblog\/wp-content\/uploads\/2015\/11\/giphy-3.gif\"\/>"}}