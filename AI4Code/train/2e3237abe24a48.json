{"cell_type":{"9d6091bc":"code","33bbe2e5":"code","c7e87127":"code","d2e58845":"code","3896b8b0":"code","38bd0ba0":"code","7299336f":"code","1605629a":"code","472c3cf8":"code","05e78f83":"code","4046716c":"code","6012ea5b":"code","3ffc24cc":"code","6e781eaa":"code","a6002ff8":"code","1db63323":"code","4d526b38":"code","e45437fd":"code","f5b3bdeb":"code","4cb484fc":"code","b066060e":"code","57219763":"code","b5e35335":"code","e9b8a852":"code","c31cd77a":"code","dfc78650":"code","e0921324":"code","eedba435":"code","d91400ae":"code","fbd5937f":"code","e2cc16a1":"code","d0ee0337":"code","46255553":"code","6701f60b":"code","c1571045":"code","858ef02b":"code","1ed7c349":"code","b2ae7214":"code","2d71a468":"code","05485ddc":"code","eabfb74c":"code","9ba8617d":"code","9a2a1171":"markdown","954553e9":"markdown","8f8a2ebd":"markdown","e2f855f7":"markdown","19015b16":"markdown","3c6bafd7":"markdown","921b2c5a":"markdown","f32f68fa":"markdown","5f050594":"markdown","fd3e2cc3":"markdown","bb349fe1":"markdown","47873632":"markdown","f798cf06":"markdown","49710604":"markdown","7dea0a78":"markdown","c8d6ef69":"markdown","c4bd4bea":"markdown","0d1a5880":"markdown","50979f81":"markdown","99eb8db1":"markdown","10026064":"markdown","399e013a":"markdown","a15fdd24":"markdown","fc7af61d":"markdown","0af1efdb":"markdown","a1db4c37":"markdown","1c7b4b5e":"markdown","ed4a1f25":"markdown","127695d2":"markdown","f645ea3f":"markdown","6cedec0c":"markdown","b9ac01ed":"markdown","48eeb46b":"markdown","a64d4ba9":"markdown","f2b54892":"markdown","eaa6713f":"markdown"},"source":{"9d6091bc":"#Importing the libraries used for the first couple of steps\nimport pandas as pd\nimport numpy as np\n\n#AND suppressing warning; they tend to ruin an easy overview\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)","33bbe2e5":"def import_data(data):\n    df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/{}.csv'\n                     .format(data))\n    return df\n\ntrain = import_data('train')\ntrain = train.set_index('Id')\ntest = import_data('test')\nID = test[['Id']]\ntest = test.set_index('Id')","c7e87127":"print(\n    'Train dataset has {} rows and {} columns\\nTest dataset has {} rows and {} columns'\n      .format(train.shape[0],\n              train.shape[1],\n              test.shape[0],\n              test.shape[1]\n             )\n    )","d2e58845":"#defining qualitative and quantitative columns\nqual_cols = [col for col in train.columns if train[col].dtype in ['object', 'str']]\nquan_cols = [col for col in train.columns if train[col].dtype not in ['object', 'str']]\n\nprint('Qualitative columns ({} columns):\\n{}\\n'.format(len(qual_cols), train[qual_cols].columns))\nprint('Quanitative columns ({} columns):\\n{}'.format(len(quan_cols), train[quan_cols].columns))","3896b8b0":"train.head()","38bd0ba0":"from scipy import stats\n\n# defining top correlated features\n\ncorrmat = train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.6]\n\nfor kol in top_corr_features:\n    z = np.abs(stats.zscore(train[kol]))\n    train['z'] = z\n    #train = train[train.z < 3.5]\n    train = train.drop('z', axis = 1)","7299336f":"#Importing libraries for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Setting style to 'darkgrid'\nsns.set_style('darkgrid')\n\nplt.figure(figsize=(6,6))\ng = sns.heatmap(\n    train[top_corr_features].corr(), \n    annot = True, cmap = \"Blues\", \n    cbar = False, vmin = .5, \n    vmax = .7, square=True\n    )","1605629a":"sns.pairplot(train[top_corr_features], diag_kind='kde')","472c3cf8":"#train = train[(train['SalePrice']<450000) & (train['GrLivArea']<4000) & (train.TotalBsmtSF<3000)]\ntrain = train[(train['GrLivArea']<4500)]","05e78f83":"from scipy.stats import probplot\n\nprint('The sales prices are skewed quite a bit')\nsns.distplot(train.SalePrice, bins=20)\nplt.suptitle('Prices before normalizing')\nplt.show()\n\nprobplot(train.SalePrice, plot=plt)\nplt.show()\n\nprint('We therefore apply log1p to normalize the sales prices for prediction')\n#Normalizing sales price\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#let's also just defined our target as y:\ny = train['SalePrice'].values\n\nsns.distplot(train.SalePrice, bins=20)\nplt.suptitle('Normalized prices')\nplt.show()\n\nprobplot(train.SalePrice, plot=plt)\nplt.show()","4046716c":"df = pd.concat([train.drop(['SalePrice'], axis=1), test], axis=0)","6012ea5b":"def null_view(df):\n    null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = False)\n    null_view = pd.DataFrame(null_view, columns=['n_nulls'])\n    null_view['pct_null'] = null_view.n_nulls.apply(lambda x: str(round((x\/len(df))*100, 1)) +'%')\n    print(null_view.head(10))\nnull_view(test)\nprint('')\nnull_view(train)","3ffc24cc":"#Interpretations of null values\n#Credit: see link in the beginning of the kernel\n\ndf['MSSubClass'] = df['MSSubClass'].apply(str)\ndf['YrSold'] = df['YrSold'].astype(str)\ndf['MoSold'] = df['MoSold'].astype(str)\n\n#Fill out values with most common value\ncommonNa = [\n    'MSZoning', 'Electrical',\n    'KitchenQual', 'Exterior1st',\n    'Exterior2nd', 'SaleType',\n    'LotFrontage', 'Functional'\n    ]\n\n#Fill with zero value\ntoZero = [\n    'MasVnrArea', 'GarageYrBlt',\n    'BsmtHalfBath', 'BsmtFullBath',\n    'GarageArea', 'GarageCars',\n    'BsmtFinSF1', 'BsmtFinSF2',\n    'BsmtUnfSF', 'TotalBsmtSF'\n    ]\n\n#Fill with No data\ntoNoData = [\n    'PoolQC', 'MiscFeature', \n    'Fence','MasVnrType',\n    'FireplaceQu', 'GarageType', \n    'GarageFinish', 'GarageQual', \n    'GarageCond', 'BsmtQual', \n    'BsmtCond', 'BsmtExposure', \n    'BsmtFinType1', 'BsmtFinType2',\n    'Alley'\n    ]\n\n#Function fill missing values\ndef fillNa_fe(df):\n    df['Functional'] = df['Functional'].fillna('Typ')\n    for i in commonNa:\n        df[i] = df[i].fillna(df[i].mode()[0])\n    for i in toNoData:\n        df[i] = df[i].fillna('None')\n    for i in toZero:\n        df[i] = df[i].fillna(0)\n    \n    df.drop(['Utilities', 'PoolQC', 'MiscFeature'], axis=1, inplace=True)\n    \n    return df\n\n\ndf = fillNa_fe(df)","6e781eaa":"#Skew is normally >.5, but I have chosen 0.7 in this case\ndef skew_(df):\n    #columns which are skew-candidates\n    colls = [col for col in df.columns if df[col].dtype in ['int64','float']]\n    skews_df = [col for col in df[colls].columns if abs(df[col].skew()) > .7]\n\n    #function to correct skew\n    def skewfix(data, data2):\n        for i in data2:\n            data[i] = np.log1p(data[i])\n            return data\n    return skewfix(df, skews_df)\n\ndf = skew_(df)","a6002ff8":"def feat_eng(df):\n    #A little bit of festure engineering\n    df['totalSF'] = df['TotalBsmtSF'] \\\n                   + df['1stFlrSF'] \\\n                   + df['2ndFlrSF'] \\\n                   + df['GrLivArea']\n    \n    df['YrBltAndRemod']= df['YearBuilt'] + df['YearRemodAdd']\n    \n    df['Porch_SF'] = (df['OpenPorchSF'] \\\n                      + df['3SsnPorch'] \\\n                      + df['EnclosedPorch'] \\\n                      + df['ScreenPorch'] \\\n                      + df['WoodDeckSF'])\n    \n    df['Total_Bathrooms'] = (df['FullBath'] \\\n                             + (0.5 * df['HalfBath']) \\\n                             + df['BsmtFullBath'] \\\n                             + (0.5 * df['BsmtHalfBath']))   \n    \n    df['hpool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    df['h2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    df['hgarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    df['hbsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    df['hfireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    \n    return df\n\ndf = feat_eng(df)","1db63323":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n#We need to encode variables with categorical data:\nencoder = LabelEncoder()\nsc = StandardScaler()\n\ndef encode(df):\n    cat_df = [col for col in df.columns if df[col].dtype not in ['int','float']]\n    for col in cat_df:\n        df[col] = encoder.fit_transform(df[col])\n    df_ = sc.fit_transform(df)\n    df = pd.DataFrame(data=df_, columns = df.columns)\n    return df\n\n\n'''\nTesting dummies in this commit; scores better than encoding!\n\n'''\n\ndf = pd.get_dummies(df)\n\n#df = encode(df)","4d526b38":"train = df.iloc[:train.shape[0],:]\ntest  = df.iloc[train.shape[0]:,:]","e45437fd":"#Splitting dataset into test and train\nfrom sklearn.model_selection import train_test_split\n\nX = train.copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","f5b3bdeb":"import datetime\n\n#Import library for Gridsearch\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","4cb484fc":"#Importing regressor for each model, as below\nfrom lightgbm import LGBMRegressor\n\nprint('####################################################\\n{}\\tLightBGM'\n      .format(datetime.datetime.now().strftime('%H:%M')))\n\nparameters = {\n    'objective':['regression'],\n    'num_leaves':[5],\n    'learning_rate':[0.05], #0.005, \n    'n_estimators':[720], #, 4000, 5000 \n    'max_bin':[55], #, 300, 500\n    'max_depth':[2, 3],\n    'bagging_fraction':[.5, .8],\n    'bagging_freq':[5],\n    'bagging_seed':[9], #7\n    'feature_fraction':[0.2319],#0.2, \n    'feature_fraction_seed':[9] #5\n    }\n\nlight = LGBMRegressor()\nclf = GridSearchCV(light, parameters, verbose=0, iid=False)\nclf.fit(X_train, y_train)\nlightgbm = LGBMRegressor(**clf.best_params_)\n\nprint('\\nRegressor: \\n', lightgbm, '\\n')\nprint('{}\\tDone!\\n####################################################'\n      .format(datetime.datetime.now().strftime('%H:%M')))","b066060e":"from xgboost import XGBRegressor\nprint('####################################################\\n{}\\tXGBoost'\n      .format(datetime.datetime.now().strftime('%H:%M')))\n\nparams = {\n    'colsample_bytree': [0.4],\n    'gamma': [0.0],\n    'learning_rate': [0.01], \n    'max_depth': [3],\n    'min_child_weight': [2],\n    'n_estimators': [3460],\n    'seed': [36],\n    'subsample': [0.2],\n    'objective':['reg:squarederror'],\n    'reg_alpha':[0.00006],\n    'cale_pos_weight':[1],\n    'verbosity':[0]\n    }\n\ngbm = XGBRegressor()\nclf = GridSearchCV(gbm, params, verbose=0, iid=False)\nclf.fit(X_train,y_train)\nXGBoost = XGBRegressor(**clf.best_params_)\n\nprint('\\nRegressor: \\n', XGBoost, '\\n')\nprint('{}\\tDone!\\n####################################################'\n      .format(datetime.datetime.now().strftime('%H:%M')))","57219763":"from sklearn.ensemble import GradientBoostingRegressor\nprint('####################################################\\n{}\\tGradient Boost'\n      .format(datetime.datetime.now().strftime('%H:%M')))\n\nparameters = {\n    'n_estimators':[8000,],\n    'learning_rate':[0.01], \n    'max_depth':[2], \n    'max_features':['sqrt'], \n    'min_samples_leaf':[10], \n    'min_samples_split':[5], \n    'loss':['huber'], \n    'random_state' :[42],\n    }\n\ngbr = GradientBoostingRegressor()\nclf = GridSearchCV(gbr, parameters, verbose=0, iid=False)\nclf.fit(X_train, y_train)\ngbr = GradientBoostingRegressor(**clf.best_params_)\n\nprint('\\nRegressor: \\n', gbr, '\\n')\nprint('{}\\tDone!\\n####################################################'\n      .format(datetime.datetime.now().strftime('%H:%M')))","b5e35335":"from sklearn.neighbors import KNeighborsRegressor\nprint('####################################################\\n{}\\tK-nearest neighbor'\n      .format(datetime.datetime.now().strftime('%H:%M')))\n\nparameters = {\n    'algorithm':['auto'],\n    'leaf_size':[2],\n    'weights':['uniform', 'distance'],\n    'metric':['euclidean', 'manhattan'],\n    'n_neighbors':[10],\n    }\n\nknn = KNeighborsRegressor()\nclf = GridSearchCV(knn, parameters, verbose=0, iid=False)\nclf.fit(X_train, y_train)\nknn = KNeighborsRegressor(**clf.best_params_)\n\nprint('\\nRegressor: \\n', knn, '\\n')\nprint('{}\\tDone!\\n####################################################'\n      .format(datetime.datetime.now().strftime('%H:%M')))","e9b8a852":"from sklearn.ensemble import RandomForestRegressor\nprint('####################################################\\n{}\\tRandom forest'\n      .format(datetime.datetime.now().strftime('%H:%M')))\n\nparameters = {\n    'bootstrap': [True],\n    #'max_depth': [15],\n    'max_features': ['auto'],\n    'min_samples_leaf': [2],\n    'min_samples_split': [4],\n    'n_estimators': [1500],\n    'n_jobs':[-1],\n    'oob_score':[True]\n    }\n\nr_forest = RandomForestRegressor()\nclf = GridSearchCV(r_forest, parameters, verbose=0, iid=False)\nclf.fit(X_train, y_train)\nknn = RandomForestRegressor(**clf.best_params_)\n\nprint('\\nRegressor: \\n', knn, '\\n')\nprint('{}\\tDone!\\n####################################################'\n      .format(datetime.datetime.now().strftime('%H:%M')))","c31cd77a":"from sklearn.model_selection import KFold\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n#Imporing library for makign pipeline\nfrom sklearn.pipeline import make_pipeline","dfc78650":"from sklearn.linear_model import RidgeCV\n\nalphas_ridge = [\n    14.5, 14.6, 14.7, 14.8, 14.9, 15, \n    15.1, 15.2, 15.3, 15.4, 15.5, 15.6\n    ]\n\nridge = make_pipeline(\n    RobustScaler(), \n    RidgeCV(\n        alphas=alphas_ridge, \n        cv=kfolds\n        )\n    )\n\nprint('############################################################\\nRidge: \\n', \n      ridge, \n      '\\n############################################################\\n')","e0921324":"from sklearn.linear_model import LassoCV\n\nalphas_lasso = [\n    5e-05, 0.0001, 0.0002, 0.0003, \n    0.0004, 0.0005, 0.0006, 0.0007\n    ]\n\nlasso = make_pipeline(\n    RobustScaler(), \n    LassoCV(\n        alphas=alphas_lasso, \n        max_iter=1e7, \n        random_state=42, \n        cv=kfolds\n        )\n    )\n\nprint('############################################################\\nLasso: \\n', \n      lasso,\n      '\\n############################################################\\n')","eedba435":"from sklearn.linear_model import ElasticNetCV\n\nalphas_ela = [\n    0.0001, 0.0002, 0.0003, \n    0.0004, 0.0005, 0.0006, 0.0007\n    ]\n\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nelasticnet = make_pipeline(\n    RobustScaler(), \n    ElasticNetCV(\n        max_iter=1e7, \n        alphas=alphas_ela, \n        cv=kfolds, \n        l1_ratio=e_l1ratio\n        )\n    ) \n\nprint('############################################################\\nElastic: \\n',\n      elasticnet, \n      '\\n############################################################\\n')","d91400ae":"%%time\nfrom sklearn.svm import SVR\n\nparameters = {\n   # 'kernel': ['sigmoid'],\n    'C':     [120],\n    'gamma': [0.0003]\n    }\n\nsvr_ = SVR()\nclf = GridSearchCV(svr_, parameters, verbose=0, iid=False)\nclf.fit(X_train, y_train)\n\nsvr = make_pipeline(\n    RobustScaler(), \n    SVR(**clf.best_params_)\n    )\n\nprint('############################################################\\nSVR: \\n', \n      svr, \n      '\\n############################################################\\n')","fbd5937f":"from sklearn.kernel_ridge import KernelRidge\nkrr = KernelRidge()","e2cc16a1":"#Importing library for stacking\nfrom mlxtend.regressor import StackingCVRegressor\n\n#SVR, KNN and Random Forest removed!\n\nprint('Stacking: \\t stack_gen')\nstack_gen = StackingCVRegressor(\n    regressors=(ridge, lasso, elasticnet, lightgbm, XGBoost, gbr),\n    meta_regressor=XGBoost, \n    use_features_in_secondary=True\n    )\nprint('Done!\\n')\n\nprint('Stacking: \\t stack_gen2')\nstack_gen2 = StackingCVRegressor(\n    regressors=(ridge, lasso, elasticnet, lightgbm, XGBoost, gbr),\n    meta_regressor=lightgbm, \n    use_features_in_secondary=True\n    )\nprint('Done!\\n')\n\nprint('Stacking: \\t stack_gen3')\nstack_gen3 = StackingCVRegressor(\n    regressors=(ridge, lasso, elasticnet, lightgbm, XGBoost, gbr),\n    meta_regressor=ridge, \n    use_features_in_secondary=True\n    )\nprint('Done!')","d0ee0337":"models = {\n    'svr':svr,\n    'KernelRidge':krr,\n    'Ridge':ridge,\n    'Lightgbm':lightgbm,\n    'XGBoost':XGBoost,\n    'GradientBoost':gbr,\n    'Lasso':lasso,\n    'Elasticnet':elasticnet,\n    'Random forest':r_forest,\n    'KNN':knn\n    }\n\n#Importing metrics for calculation of error\nfrom sklearn import metrics\n\ndef calc(model): \n    return int(np.sqrt(metrics.mean_squared_error(\n        np.expm1(y_test), np.expm1(model.predict(X_test)))))\n\nliste = []\nfor one, two in models.items():\n    print('Fitting:\\t{}'.format(one))\n    two.fit(X_train, y_train)\n    print('Done! Error:\\t{}\\n'.format(calc(two)))\n    df = pd.DataFrame(\n        data=[calc(two)], \n        index=[one], \n        columns=['error']\n        )\n    liste.append(df)","46255553":"resList = pd.concat(liste, axis=0)","6701f60b":"print('stack_gen: \\t fitting...')\nstack_gen_model = stack_gen.fit(np.array(X_test), np.array(y_test))\nsg_error = int(np.sqrt(metrics.mean_squared_error(np.expm1(y_test), np.expm1(stack_gen_model.predict(np.array(X_test))))))\nprint('stack_gen: \\t done!')\nprint('error:\\t\\t', sg_error, '\\n')\n\nprint('stack_gen2: \\t fitting...')\nstack_gen_model2 = stack_gen2.fit(np.array(X_test), np.array(y_test))\nsg_error2 = int(np.sqrt(metrics.mean_squared_error(np.expm1(y_test), np.expm1(stack_gen_model2.predict(np.array(X_test))))))\nprint('stack_gen2: \\t done!')\nprint('error:\\t\\t', sg_error2, '\\n')\n\nprint('stack_gen3: \\t fitting...')\nstack_gen_model3 = stack_gen3.fit(np.array(X_test), np.array(y_test))\nsg_error3 = int(np.sqrt(metrics.mean_squared_error(np.expm1(y_test), np.expm1(stack_gen_model3.predict(np.array(X_test))))))\nprint('stack_gen3: \\t done!')\nprint('error:\\t\\t', sg_error3)\n\nsg = pd.DataFrame(index=['stack_gen'], data=sg_error, columns=['error'])\nsg2 = pd.DataFrame(index=['stack_gen2'], data=sg_error2, columns=['error'])\nsg3 = pd.DataFrame(index=['stack_gen3'], data=sg_error3, columns=['error'])","c1571045":"resList = resList.append(sg)\nresList = resList.append(sg2)\nresList = resList.append(sg3)","858ef02b":"def blend(data):\n    return (\n        #(0.01 * svr.predict(data)) +\n        #(0.03 * krr.predict(data)) +\n        (0.08 * ridge.predict(data)) +\n        (0.11 * lightgbm.predict(data)) +\n        (0.13 * XGBoost.predict(data)) +\n        (0.10 * gbr.predict(data)) +\n        (0.15 * stack_gen_model.predict(np.array(data))) +\n        (0.16 * stack_gen_model2.predict(np.array(data))) +\n        (0.16 * stack_gen_model3.predict(np.array(data))) +\n        (0.06 * lasso.predict(data)) +\n        #(0.03 * knn.predict(data)) + \n        #(0.02 * r_forest.predict(data)) +\n        (0.05 * elasticnet.predict(data)) \n        )\n\nblended_predictions = blend(test)","1ed7c349":"def createFrame(data):\n    '''\n    Compare results\n    '''\n    index = index=import_data('test').reset_index()['Id']\n    df_ = {\n        'lightgbm':lightgbm.predict(data),\n        'XGBoost':XGBoost.predict(data),\n        'GBR':gbr.predict(data),\n        'svr':svr.predict(data),\n        'ridge':ridge.predict(data),\n        'KernelRidge':krr.predict(data),\n        'elastic':elasticnet.predict(data),\n        'lasso':lasso.predict(data),\n        'KNN':knn.predict(data),\n        'Random forest':r_forest.predict(data),\n        'stack_gen':stack_gen_model.predict(np.array(data)),\n        'stack_gen2':stack_gen_model2.predict(np.array(data)),\n        'stack_gen3':stack_gen_model3.predict(np.array(data)),\n        'blended':blend(data)\n        }\n    df = pd.DataFrame(df_, index=index)\n    df = df.applymap(lambda x: np.expm1(x))\n    return df\n\npreds = createFrame(test)\n\nfor model in preds.columns:\n    df = preds[[model]]\n    df.rename(columns={model:'SalePrice'}, inplace=True)\n    df.to_csv('{}.csv'.format(model))\n\npreds.round(1).head(10)","b2ae7214":"x = blend(X_test)\n\nbl = pd.DataFrame(\n    index=['blend'], \n    data=int(np.sqrt(metrics.mean_squared_error(np.expm1(y_test), np.expm1(x)))),\n    columns=['error']\n    )\nresList = resList.append(bl)","2d71a468":"resList2 = resList.reset_index()\n#resList2.sort_values('error', inplace=True)\nplt.figure(figsize=(13,6))\nax1 = sns.barplot(\n    x=resList2['index'], \n    y=resList2.error, \n    color='#00616f'\n    )\n\nfor i, rows in resList2.iterrows():\n    ax1.annotate(\n        round(rows['error'],1), \n        xy=(i, rows['error']), \n        xycoords='data', \n        ha='center', \n        color='black'\n        )","05485ddc":"res = pd.DataFrame(data=np.expm1(blend(test)), index=ID.Id, columns=['SalePrice'])\n\nq1 = res['SalePrice'].quantile(0.005)\nq2 = res['SalePrice'].quantile(0.995)\n\nres['SalePrice'] = res['SalePrice'].apply(lambda x: x if x > q1 else x*0.78)\nres['SalePrice'] = res['SalePrice'].apply(lambda x: x if x < q2 else x*1.10)\n\nres.to_csv('1_submission.csv')","eabfb74c":"res.reset_index().head(10)","9ba8617d":"plt.figure(figsize=(10,10))\nsns.heatmap(\n    preds.corr().round(4), \n    annot=True, \n    cmap=\"Blues\", \n    cbar=False, \n    square=True\n   )","9a2a1171":"## 2.2 Handling skew for target","954553e9":"## 1.2 EDA (light)","8f8a2ebd":"## 2.1 Outliers","e2f855f7":"The train dataset has one more columns than the test dataset - this is the target value, SalePrice!  \n\nThe next step is to combine the two datasets for consistency, when I handle nulls and skew values and when encoding and scaling.  \n\nLet's also have a look at the data; which columns do we have, split into qualitative and quantitative columns.","19015b16":"Using a pairplot to check that all obvious outliers are in fact removed","3c6bafd7":"## 4.3 Ensambling  \n**Ensambling for a bleded prediction model**","921b2c5a":"**Pipelines for following models defined:**  \n>Ridge  \n>Lasso  \n>Elasticnet  \n>SVR","f32f68fa":"# 1 Data","5f050594":"**Fitting the models**","fd3e2cc3":"**I have been going through several possibilities in gridsearch, below is only the latest \"best parameters\"**","bb349fe1":"We set a couple of filters based on the above (see also other kernels for in depth review)","47873632":"I am combining the datasets for consistency in the cleaning process.  \nSalePrice is dropped from the combined dataset.","f798cf06":"# 5 Predictions  \n## 5.1 Comparing models  \n**Comparing the first 20 results for each model**","49710604":"Let's first look into our target variable: SalePrice.  \n\nWhen skewed, we apply np.log1p to: *\"Return the natural logarithm of one plus the input array, element-wise.\"*","7dea0a78":"## 1.1 Importing data","c8d6ef69":"Credit goes to (they deserve an upvote!):   \nhttps:\/\/www.kaggle.com\/shaygu\/house-prices-begginer-top-7  \nhttps:\/\/www.kaggle.com\/niteshx2\/top-50-beginners-stacking-lgb-xgb   \nhttps:\/\/www.kaggle.com\/iamprateek\/my-submission-to-predict-sale-price\/data   \nhttps:\/\/www.kaggle.com\/jesucristo\/1-house-prices-solution-top-1#Blending-Models","c4bd4bea":"## 2.3 Null values","0d1a5880":"# 4 Model fit","50979f81":"**Let's get an overview of the first 5 rows of the dataset:**","99eb8db1":"## 4.1 Hyper parameter tuning with GridSearch\n\n","10026064":"Top correlated features:","399e013a":"# Predicting house prices\n\n![](https:\/\/cdn.pixabay.com\/photo\/2017\/04\/14\/17\/17\/houses-2230817_960_720.png)\n\n### About the kernel\n\n>**Note!** *My purpose with this kernel is primarily to try different ML algorithms and to learn about hyperparameter tuning, stacking and ensambling.*\n>*I am therefore not including much EDA in this kernel and I am not describing the data cleaning- and feature engineering (FE) in details. I will refer to other kernels instead, as many others have already done an outstanding job doing in these regards!*  \n\n\n### Contents:    \n* **1 Data**\n  * 1.1 Importing data\n  * 1.2 Exploratory Data Analysis (light)\n* **2 Data cleaning**  \n  * 2.1 Outliers  \n  * 2.2 Handling skew and null   \n  * 2.3 Light feature engineering  \n* **3 Data preparation: Encode and scale**   \n* **4 Model fit**  \n  * 4.1 Hyper parameter tuning with GridSearch  \n  * 4.2 Stacking  \n  * 4.3 Emsambling  \n* **5 Predictions**  \n  * 5.1 Comparison of models","a15fdd24":"# 2 Data cleaning","fc7af61d":"*Getting to know the dataset!!!*  \n\nWe want to get an overview of the dataset, so we know which measures to take in order to get a nice and clean dataset to feed our machine learning models with.  \n\nI am starting with answering questions such as the shape (rows, columns) of both the training and the testing dataset, how many null values are in each row etc. The purpose of this dataset is not EDA, so if you are looking for a comprehensive overview, I will recommend moving on to another kernel :-)","0af1efdb":"All models with error","a1db4c37":"**Identifying outliers**  \n\nIdentifying most correlated features and using z-score to remove outliers.  \nOne solution is to remove outliers with a z-score > 3.  \nThis does not improve score, so it is not integrated, but the code is still found below.\n\nInstead I use data vizualization, a pairplot, to identify outliers and then filter them out.\n\n\nZ-scores:","1c7b4b5e":"In order to use qualitative variables I am encoding and scaling the variables.","ed4a1f25":"**Stacking the models:**  ","127695d2":"## 2.4 Feature engineering, light  \nInspired by other kernels, I am creating several new variables by combining\/altering elements of existing variables.","f645ea3f":"**Overview of null values in both test and train; top 10 for each**","6cedec0c":"# 3 Data preparation","b9ac01ed":"How are the predictions correlated?","48eeb46b":"**Five models with gradient boost first defined:**  \n>Gradient Boost  \n>Light GBM  \n>XGBoost  \n>K-nearest neighbors  \n>Random forest","a64d4ba9":"### Skew for rest of dataset\nFollowing SalePrice, I will \"fix\" skew for all other quantitative columns with skre > 0.7.","f2b54892":"How many rows and columns are we dealing with?","eaa6713f":"## 4.2 Stacking"}}