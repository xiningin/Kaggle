{"cell_type":{"4852c907":"code","da17d71d":"code","4877c0f0":"code","e861b434":"code","87330dc4":"code","c1bda286":"code","9eeb161b":"code","3c927f7b":"code","a06e6321":"code","bd3701f6":"code","1e86f9b2":"code","9571a832":"code","e0782f17":"code","b8d3bf7b":"code","158c27b4":"code","bbd1b7f1":"code","108988b1":"code","c7477dd4":"code","e50eb0b7":"code","f84a1611":"code","d56084ed":"code","200885a7":"code","3f83a923":"code","69a3c7ed":"code","3cd359dd":"code","f122d726":"code","df77968e":"code","49a1d639":"code","6fbde5ac":"code","f3ae35bd":"code","8a79a4b1":"code","30ee43b0":"code","e8e5e7af":"code","c02acbac":"code","cb7505ed":"code","de57c7c0":"code","a7a029be":"code","1464dc5f":"code","a3bd8f32":"code","7a6c8690":"code","0d44c27d":"code","ce924b74":"code","cdeabecf":"code","e171bfc7":"code","7a628369":"code","83ff009d":"code","0f54d8ed":"code","6dedba1d":"code","2918d966":"code","d70e2a8c":"markdown","f31f9137":"markdown","1de82922":"markdown","9ff95ff2":"markdown","2c281242":"markdown","f0ae069a":"markdown","bad7efd3":"markdown","ab44f279":"markdown","3cc9ef60":"markdown","d3d021ea":"markdown","78c9f71f":"markdown","6353638d":"markdown","225bef63":"markdown","780b0ce0":"markdown","dd000d8c":"markdown","0bec8256":"markdown","6d62857a":"markdown","2806721b":"markdown","5683fc06":"markdown","9d811622":"markdown","6af968a6":"markdown","173fef5f":"markdown","7b2ceb89":"markdown","351cdb5b":"markdown","ae12b3c8":"markdown"},"source":{"4852c907":"# import the usual suspects ...\nimport pandas as pd\nimport numpy as np\nimport glob\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# suppress all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","da17d71d":"accidents = pd.read_csv('..\/input\/Accident_Information.csv')\nprint('Records:', accidents.shape[0], '\\nColumns:', accidents.shape[1])\naccidents.head()","4877c0f0":"#accidents.info()","e861b434":"#accidents.describe().T","87330dc4":"#accidents.isna().sum()","c1bda286":"vehicles = pd.read_csv('..\/input\/Vehicle_Information.csv', encoding='ISO-8859-1')\nprint('Records:', vehicles.shape[0], '\\nColumns:', vehicles.shape[1])\nvehicles.head()","9eeb161b":"#vehicles.info()","3c927f7b":"#vehicles.describe().T","a06e6321":"#vehicles.isna().sum()","bd3701f6":"accidents['Date']= pd.to_datetime(accidents['Date'], format=\"%Y-%m-%d\")\n\n# check\naccidents.iloc[:, 5:13].info()","1e86f9b2":"# slice first and second string from time column\naccidents['Hour'] = accidents['Time'].str[0:2]\n\n# convert new column to numeric datetype\naccidents['Hour'] = pd.to_numeric(accidents['Hour'])\n\n# drop null values in our new column\naccidents = accidents.dropna(subset=['Hour'])\n\n# cast to integer values\naccidents['Hour'] = accidents['Hour'].astype('int')","9571a832":"# define a function that turns the hours into daytime groups\ndef when_was_it(hour):\n    if hour >= 5 and hour < 10:\n        return \"1\"\n    elif hour >= 10 and hour < 15:\n        return \"2\"\n    elif hour >= 15 and hour < 19:\n        return \"3\"\n    elif hour >= 19 and hour < 23:\n        return \"4\"\n    else:\n        return \"5\"","e0782f17":"# create a little dictionary to later look up the groups I created\ndaytime_groups = {1: 'Morning: Between 5 and 10', \n                  2: 'Office Hours: Between 10 and 15', \n                  3: 'Afternoon Rush: Between 15 and 19', \n                  4: 'Evening: Between 19 and 23', \n                  5: 'Night: Between 23 and 5'}","b8d3bf7b":"# apply this function to our temporary hour column\naccidents['Daytime'] = accidents['Hour'].apply(when_was_it)\naccidents[['Time', 'Hour', 'Daytime']].head()","158c27b4":"# drop old time column and temporary hour column\naccidents = accidents.drop(columns=['Time', 'Hour'])","bbd1b7f1":"print('Proportion of Missing Values in Accidents Table:', \n      round(accidents.isna().sum().sum()\/len(accidents), 3), '%')","108988b1":"#accidents.isna().sum()","c7477dd4":"# drop columns we don't need\naccidents = accidents.drop(columns=['2nd_Road_Class', '2nd_Road_Number', 'Did_Police_Officer_Attend_Scene_of_Accident',\n                                    'Location_Easting_OSGR', 'Location_Northing_OSGR', \n                                    'Longitude', 'Latitude', 'LSOA_of_Accident_Location',\n                                    'Pedestrian_Crossing-Human_Control', 'Pedestrian_Crossing-Physical_Facilities',\n                                    'InScotland'])\n\n# drop remaining records with NaN's\naccidents = accidents.dropna()\n\n# check if we have no NaN's anymore\naccidents.isna().sum().sum()","e50eb0b7":"print('Proportion of Missing Values in Vehicles Table:', \n      round(vehicles.isna().sum().sum()\/len(vehicles),3), '%')","f84a1611":"#vehicles.isna().sum()","d56084ed":"# combine the accidents with the vehicles table\ndf = pd.merge(accidents[['Accident_Index', 'Accident_Severity', 'Daytime', 'Speed_limit', 'Urban_or_Rural_Area']], \n              vehicles[['Accident_Index', 'Age_Band_of_Driver', 'Age_of_Vehicle', 'Sex_of_Driver', \n                        'Engine_Capacity_.CC.', 'Vehicle_Manoeuvre']], \n              on='Accident_Index')\n\ndf.isna().sum()","200885a7":"df = df.dropna()\ndf.isna().sum().sum()","3f83a923":"df.info()    ","69a3c7ed":"# cast categorical features - currently stored as string data - to their proper data format\nfor col in ['Accident_Severity', 'Daytime', 'Speed_limit', 'Urban_or_Rural_Area',\n            'Age_Band_of_Driver', 'Sex_of_Driver', 'Vehicle_Manoeuvre']:\n    df[col] = df[col].astype('category')\n    \ndf.info()","3cd359dd":"# define numerical columns\nnum_cols = ['Age_of_Vehicle', 'Engine_Capacity_.CC.']","f122d726":"# plotting boxplots\nsns.set(style='darkgrid')\nfig, axes = plt.subplots(2,1, figsize=(10,4))\n\nfor ax, col in zip(axes, num_cols):\n    df.boxplot(column=col, grid=False, vert=False, ax=ax)\n    plt.tight_layout();","df77968e":"df['Engine_Capacity_.CC.'].describe()","49a1d639":"# phrasing condition\ncondition = (df['Engine_Capacity_.CC.'] < 20000)\n\n# keep only records that meet the condition and don't fall within extreme outliers\ndf = df[condition]","6fbde5ac":"df['Age_of_Vehicle'].describe()","f3ae35bd":"age_of_vehicle_bins = {1: '1 to <2 years', \n                       2: '2 to <3 years', \n                       3: '3 to <7 years', \n                       4: '7 to <10 years', \n                       5: '>=10 years'}","8a79a4b1":"# arguments in bins parameter denote left edge of each bin\ndf['Age_of_Vehicle'] = np.digitize(df['Age_of_Vehicle'], bins=[1,2,3,7,10])\n\n# convert into categorical column\ndf['Age_of_Vehicle'] = df['Age_of_Vehicle'].astype('category')\n\n# check the count within each bucket\ndf['Age_of_Vehicle'].value_counts().sort_index()","30ee43b0":"# re-define numerical feature columns - only one left\nnum_cols = ['Engine_Capacity_.CC.']","e8e5e7af":"# define categorical feature columns\ncat_cols = ['Daytime', 'Speed_limit', 'Urban_or_Rural_Area',\n            'Age_Band_of_Driver', 'Age_of_Vehicle', 'Sex_of_Driver', 'Vehicle_Manoeuvre']\n\n# define target col\ntarget_col = ['Accident_Severity']\n\ncols = cat_cols + num_cols + target_col\n\n# copy dataframe - just to be safe\ndf_model = df[cols].copy()\ndf_model.shape","c02acbac":"# create dummy variables from the categorical features\ndummies = pd.get_dummies(df_model[cat_cols], drop_first=True)\ndf_model = pd.concat([df_model[num_cols], df_model[target_col], dummies], axis=1)\ndf_model.shape","cb7505ed":"df_model.isna().sum().sum()","de57c7c0":"# define our features \nfeatures = df_model.drop(['Accident_Severity'], axis=1)\n\n# define our target\ntarget = df_model[['Accident_Severity']]","a7a029be":"from sklearn.model_selection import train_test_split\n\n# split our data\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)","1464dc5f":"df_model['Accident_Severity'].value_counts(normalize=True)","a3bd8f32":"# import classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# import evaluation tools\nfrom sklearn.model_selection import KFold, cross_val_score","7a6c8690":"# instantiate RandomForestClassifier with entropy and class_weight\nforest_1 = RandomForestClassifier(random_state=4, criterion='entropy', n_jobs=-1, class_weight='balanced')\n\n# train\nforest_1.fit(X_train, y_train)\n\n# predict\ny_test_preds  = forest_1.predict(X_test)\n\n# evaluate\nreport = classification_report(y_test, y_test_preds)\nprint('Classification Report Random Forest - with Entropy and class_weight Parameter: \\n', report)","0d44c27d":"# cross-validation with F1 score (more appropriate to imbalanced classes)\ncross_val_score(forest_1, X_train, y_train, scoring='f1_macro', n_jobs=-1)","ce924b74":"# create confusion matrix# create confusion matrix\nmatrix = confusion_matrix(y_test, y_test_preds)\n\n# create dataframe\nclass_names = df_model.Accident_Severity.values\ndataframe = pd.DataFrame(matrix, index=['Fatal', 'Serious', 'Slight'], \n                         columns=['Fatal', 'Serious', 'Slight'])\n\n# create heatmap\nsns.heatmap(dataframe, annot=True, cbar=None, cmap='Blues')\nplt.title('Confusion Matrix')\nplt.tight_layout(), plt.xlabel('True Values'), plt.ylabel('Predicted Values')\nplt.show()","cdeabecf":"from imblearn.over_sampling import SMOTE","e171bfc7":"# view previous class distribution\nprint('Before Upsampling with SMOTE:'), print(target['Accident_Severity'].value_counts())\n\n# resample data ONLY using training data\nX_resampled, y_resampled = SMOTE().fit_sample(X_train, y_train) \n\n# view synthetic sample class distribution\nprint('\\nAfter Upsampling with SMOTE:'), print(pd.Series(y_resampled).value_counts())","7a628369":"# then perform ususal train-test-split\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=0)","83ff009d":"# instantiate second RandomForestClassifier with entropy and SMOTE\nforest_2 = RandomForestClassifier(random_state=4, criterion='entropy', n_jobs=-1)\n\n# train\nforest_2.fit(X_train, y_train)\n\n# predict\ny_test_preds = forest_2.predict(X_test)\n\n# evaluate\nreport = classification_report(y_test, y_test_preds)\nprint('Classification Report Random Forest - with Entropy and SMOTE Upsampling: \\n', report)","0f54d8ed":"# cross-validation with F1 score (more appropriate to imbalanced classes)\ncross_val_score(forest_2, X_train, y_train, scoring='f1_macro', n_jobs=-1)","6dedba1d":"# create confusion matrix\nmatrix = confusion_matrix(y_test, y_test_preds)\n\n# create dataframe\nclass_names = df_model.Accident_Severity.values\ndataframe = pd.DataFrame(matrix, index=['Fatal', 'Serious', 'Slight'], \n                         columns=['Fatal', 'Serious', 'Slight'])\n\n# create heatmap\nsns.heatmap(dataframe, annot=True, cbar=None, cmap='Blues')\nplt.title('Confusion Matrix')\nplt.tight_layout(), plt.xlabel('True Values'), plt.ylabel('Predicted Values')\nplt.show()","2918d966":"# plot the important features\nfeat_importances = pd.Series(forest_2.feature_importances_, index=features.columns)\nfeat_importances.nlargest(10).sort_values().plot(kind='barh', color='darkgrey', figsize=(10,5))\nplt.xlabel('Relative Feature Importance with Random Forest');","d70e2a8c":"We had our `Date` columnwith values not properly stored in the correct format. Let's do this now:","f31f9137":"*Back to: <a href='#Table of Contents'> Table of Contents<\/a>*\n### 3. Modeling the Data\n<a id='3. Modeling the Data'><\/a>","1de82922":"#### 2.1. Handling Date and Time\n<a id='2.1. Handling Date and Time'><\/a>","9ff95ff2":"#### 2.2. Handling Missing Values\n<a id='2.2. Handling Missing Values'><\/a>","2c281242":"*Handling Outliers*","f0ae069a":"*Encoding categorical features*","bad7efd3":"... and we can't apply the best strategy and simply can't collect more data, especially from the minority class, we need to find other ways to deal with imbalanced classes.\n\n- A second strategy is to use a model evaluation metric better suited to imbalances classes: confusion matrices, precision, recall, F1 scores, or ROC curves instead of accuracy.\n- A third strategy is to use the class weighing parameter included in implementations of some models. This allows us to have the algorithm adjust for imbalanced classes.\n- The fourth and fifth strategies are related: downsampling and upsampling. Several of these resampling stratgies are well summarized in this [blog post by Chris Remmel](https:\/\/calremmel.github.io\/fraud-detection-part-one.html).\n\nLet's focus on class **weight parameters**: The Random Forest Classifier we will use now is a popular classification algorithm and includes a `class_weight` parameter.","ab44f279":"#### 3.2. Handling Imbalanced Classes\n<a id='3.2. Handling Imbalanced Classes'><\/a>\n\nIf we have a target vector with highly imbalanced classes ...","3cc9ef60":"### 1. Obtaining and Viewing the Data\n<a id='1. Obtaining and Viewing the Data'><\/a>","d3d021ea":"The Random Forest using the `weight_class` parameter did not perform very well on classifying the severity. So let's try one of the resampling strategies to deal properly with our imbalances target classes: **Synthetic Minority Over-sampling Technique (SMOTE)**. Here we're repeatedly sample with replacement from the minority class to make it of equal size as the majority class. To be more specific: We're creating new synthetic data for the minority class - that is representative but not exact duplicate - using K-Nearest Neighbors.","78c9f71f":"> **Accidents**","6353638d":"*Back to: <a href='#Table of Contents'> Table of Contents<\/a>*\n### 2. Preprocessing the Data\n<a id='2. Preprocessing the Data'><\/a>","225bef63":"*Back to: <a href='#Table of Contents'> Table of Contents<\/a>*\n#### 2.3. Merging Dataframes\n<a id='2.3. Merging Dataframes'><\/a>","780b0ce0":"*Back to: <a href='#Table of Contents'> Table of Contents<\/a>*\n#### 2.5. Handling Categorical Data\n<a id='2.5. Handling Categorical Data'><\/a>","dd000d8c":"**Vehicles**","0bec8256":"#### 3.1. Train-Test-Split\n<a id='3.1. Train-Test-Split'><\/a>","6d62857a":"*Binning Age_of_Vehicle Feature*","2806721b":"*Back to: <a href='#Table of Contents'> Table of Contents<\/a>*\n#### 3.4. Training and Evaluating Random Forest Classifier with SMOTE\n<a id='3.4. Training and Evaluating Random Forest Classifier with SMOTE'><\/a>","5683fc06":"*Back to: <a href='#Table of Contents'> Table of Contents<\/a>*\n#### 3.3. Training and Evaluating Random Forest Classifier with class_weight\n<a id='3.3. Training and Evaluating Random Forest Classifier with class_weight'><\/a>","9d811622":"Next, let's define a new column that groups the `Time` the accidents happened into one of five options:\n- Morning Rush from 5am to 10am --> value 1\n- Office Hours from 10am to 3pm (or: 10:00 - 15:00) --> value 2\n- Afternoon Rush from 3pm to 7pm (or: 15:00 - 19:00) --> value 3\n- Evening from 7pm to 11pm (or: 19:00 - 23:00) --> value 4\n- Night from 11pm to 5am (or: 23:00 - 05:00) --> value 5","6af968a6":"# Road Safety Data for the UK","173fef5f":"*Detecting Outliers*","7b2ceb89":"# Table of Contents\n<a id='Table of Contents'><\/a>\n\n### <a href='#1. Obtaining and Viewing the Data'>1. Obtaining and Viewing the Data<\/a>\n\n### <a href='#2. Preprocessing the Data'>2. Preprocessing the Data<\/a>\n\n* <a href='#2.1. Handling Date and Time'>2.1. Handling Date and Time<\/a>\n* <a href='#2.2. Handling Missing Values'>2.2. Handling Missing Values<\/a>\n* <a href='#2.3. Merging Dataframes'>2.3. Merging Dataframes<\/a>\n* <a href='#2.4. Handling Numerical Data'>2.4. Handling Numerical Data<\/a>\n* <a href='#2.5. Handling Categorical Data'>2.5. Handling Categorical Data<\/a>\n\n### <a href='#3. Modeling the Data'>3. Modeling the Data<\/a>\n\n* <a href='#3.1. Train-Test-Split'>3.1. Train-Test-Split<\/a>\n* <a href='#3.2. Handling Imbalanced Classes'>3.2. Handling Imbalanced Classes<\/a>\n* <a href='#3.3. Training and Evaluating Random Forest Classifier with class_weight'>3.3. Training and Evaluating Random Forest Classifier with class_weight<\/a>\n* <a href='#3.4. Training and Evaluating Random Forest Classifier with SMOTE'>3.4. Training and Evaluating Random Forest Classifier with SMOTE<\/a>","351cdb5b":"*Back to: <a href='#Table of Contents'> Table of Contents<\/a>*\n#### 2.4. Handling Numerical Data \n<a id='2.4. Handling Numerical Data'><\/a>","ae12b3c8":"*Feature Scaling*\n\nIf you use a distance based algorithm and your numerical features\u2019 range vary widely, the algorithm won\u2019t work properly unless the range of all features is normalized.\n\nTree based models, which we will use here, are not distance based and can handle varying ranges of features. Therefore scaling is not required."}}