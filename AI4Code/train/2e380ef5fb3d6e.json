{"cell_type":{"10bab450":"code","880c8bed":"code","80172b7c":"code","658c3779":"code","efe97e43":"code","ec247987":"code","d55d02f0":"code","f9b7ebbf":"code","0cad263e":"code","e995f839":"code","d35e8e0e":"code","f4ebc186":"code","d3c1e4b8":"code","bd37f225":"code","acf56ca5":"code","d0f9a447":"code","caf06ef7":"code","7d873b29":"code","8abb0675":"code","3f917609":"code","8283774a":"code","642f8826":"code","17e12a40":"code","29943b79":"code","297e3524":"code","4e01b5a0":"markdown","a0e6fee1":"markdown","89910fc3":"markdown","00fb52b9":"markdown"},"source":{"10bab450":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/amazon-fine-food-reviews'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","880c8bed":"import sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os","80172b7c":"con = sqlite3.connect('..\/input\/amazon-fine-food-reviews\/database.sqlite') \n\nfiltered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews \n                                     WHERE Score != 3 LIMIT 5000\"\"\", con) \n\n# filtered_data.head()","658c3779":"\n# Rating Score>3 a positive rating, and score<3 a negative rating.\ndef partition(x):\n    if x < 3:\n        return 0\n    return 1\n\nactual_score = filtered_data['Score']\nPosNeg = actual_score.map(partition)\nfiltered_data['Score'] = PosNeg\n\nprint(\"Data-Points in data\", filtered_data.shape)\nfiltered_data.head(3)\n\n","efe97e43":"display = pd.read_sql_query(\"\"\"\n\nselect * from Reviews\nwhere Score!=3 AND UserId=\"AR5J8UI46CURR\"\norder by ProductId\n\"\"\", con)\n\ndisplay.head()","ec247987":"# Sorting data according to ProductId in Asc. Order\nsorted_data = filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, \n                                       kind='quicksort', na_position='last')","d55d02f0":"# drop duplicate entries\nfinal = sorted_data.drop_duplicates(subset={\"UserId\", \"ProfileName\", \"Time\", \"Text\"}, keep='first', inplace=False)\nfinal.shape","f9b7ebbf":"display = pd.read_sql_query(\"\"\"select * from Reviews\n                            where Score!=3 AND Id=44737 OR Id=64422\n                            order by ProductId\"\"\", con)","0cad263e":"display.head()","e995f839":"final = final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]","d35e8e0e":"print(final.shape)\n\n# number of +ve and -ve reviews \nfinal['Score'].value_counts()","f4ebc186":"import re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n\nstop = set(stopwords.words('english'))\nsno = nltk.stem.SnowballStemmer('english')\n\n\nprint(stop)\nprint(\"=\"*50)\n\nprint(sno.stem('tasty'))","d3c1e4b8":"# FUnction to clean HTML\ndef cleanHTML(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', sentence)\n    return cleantext\n\n# Function to clean Punctuation\ndef cleanPunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]', r'', sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]', r' ', cleaned)\n    return cleaned\n","bd37f225":"i = 0\nstr= ' '\n\nfinal_string=[]\nall_positive_words= []\nall_negative_words=[]\n\ns=''\n\nfor sent in final['Text'].values:\n    filtered_sentence=[]\n    \n    \n    sent = cleanHTML(sent)\n    for word in sent.split():\n        for cleaned_words in cleanPunc(word).split():\n            \n            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):\n                if(cleaned_words.lower() not in stop):\n                    s = (sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    \n                    \n                    if(final['Score'].values)[i] == 'positive':\n                        all_positive_words.appned(s)\n                    if(final['Score'].values)[i] == 'negative':\n                        all_negative_words.appned(s)\n                else:\n                    continue\n            else:\n                continue\n                \n                \n\n                        \n    str1 = b\" \".join(filtered_sentence)\n        \n    final_string.append(str1)\n    i+=1","acf56ca5":"# adding a Cleaned_text column\n\nfinal['CleanedText'] = final_string","d0f9a447":"final.head()\n","caf06ef7":"# storing this in table\n\nconn = sqlite3.connect('final.sqlite')\nc=conn.cursor()\nconn.text_factory = str\nfinal.to_sql('Reviews', conn, schema=None, if_exists='replace')","7d873b29":"# Bag_of_Words\ncount_vec = CountVectorizer() # in Scikit \nfinal_counts = count_vec.fit_transform(final['CleanedText'].values)\n# type(final_counts)\nprint(count_vec.get_feature_names()[:100])\nfinal_counts.get_shape()","8abb0675":"count_vec = CountVectorizer(ngram_range=(1, 2))\nfinal_bigram_counts = count_vec.fit_transform(final['Text'].values)","3f917609":"final_bigram_counts.get_shape()","8283774a":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\nfinal_tf_idf = tf_idf_vect.fit_transform(final['Text'].values)","642f8826":"final_tf_idf.get_shape()","17e12a40":"feature = tf_idf_vect.get_feature_names()\nlen(feature)","29943b79":"feature[140000:140010]","297e3524":"print(final_tf_idf[3,:].toarray()[0])","4e01b5a0":"# Text PreProcessing \n","a0e6fee1":"# Another Observation - \"HelpfulnessNum > HelpfulDenominator\"","89910fc3":"# TF-IDF","00fb52b9":"# Data Cleaning\n"}}