{"cell_type":{"a8b6cf1f":"code","405d8985":"code","185cdabd":"code","86c392a0":"code","d9f7f3c4":"code","9eace9c7":"code","71c56b20":"code","01d0d736":"code","9f255aeb":"code","60b5a3de":"code","61b0846f":"code","9f6e4516":"code","aec34e78":"code","f0282bef":"code","5171b698":"code","2e6929cc":"code","57e598bb":"code","943c4118":"code","24dbadc2":"code","5932d1f8":"code","e45fdd66":"code","45cbff77":"code","703d3b02":"code","f4f2a06f":"code","4e383814":"code","7ea49e0a":"code","35dd6bb1":"code","11f27407":"code","02eb6824":"code","4dc210b0":"code","80643c41":"markdown","a6debef9":"markdown","12ee820b":"markdown","320ffb51":"markdown","3e0f9309":"markdown","cd4f4ee7":"markdown","a92cef8a":"markdown","2d790ee0":"markdown","eb09b65f":"markdown","afd02a6d":"markdown","1db69c35":"markdown","7b02140e":"markdown","2ceb0bd3":"markdown","d9274958":"markdown","2ea17a09":"markdown","304a217f":"markdown","5570a0c6":"markdown","054119a0":"markdown","92b1a688":"markdown"},"source":{"a8b6cf1f":"import pandas as pd\nimport numpy as np\nimport random\nimport re\nimport nltk\nimport string \nfrom nltk.corpus import stopwords  \nfrom nltk.stem import PorterStemmer  \nfrom nltk.tokenize import TweetTokenizer\nimport emoji\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('stopwords')\n\n# dictionary for lemmatization\nnltk.download('wordnet')","405d8985":"\ndata = pd.read_csv(\"..\/input\/large-random-tweets-from-pakistan\/Random \"\n                   \"Tweets from Pakistan- Cleaned- Anonymous.csv\",encoding_errors = 'ignore')\n","185cdabd":"data.head()","86c392a0":"data.shape","d9f7f3c4":"# As we are doing NLP related tasks, I'll keep only text column\ndata = data['full_text']","9eace9c7":"data = data.dropna()","71c56b20":"# regular exp to filter out urdu text\nreg = re.compile(r'[\\u0600-\\u06ff]+', re.UNICODE)\ndata = data.apply(lambda x: re.sub(reg, \"\", x))","01d0d736":"\n# removing extra spaces\ndata = data.apply(lambda x: re.sub(r'[  ]+', \" \", x))\n\n# converting to lowercase letters \ndata = data.apply(lambda x: x.strip().lower())\n\n# remove hyperlinks\ndata = data.apply(lambda x: re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', x))\n\n# removing @Mentions\ndata = data.apply(lambda x:re.sub(r'@.+?\\s', '', x))\n\n# removing extra symbols\ndata = data.apply(lambda x: re.sub(r'#', '', x))\ndata = data.apply(lambda x: re.sub(r'rt : ', '', x))\ndata = data.apply(lambda x: re.sub(r'\\n', ' ', x))\n","9f255aeb":"data.shape","60b5a3de":"# removing duplicated data\ndata = data.drop_duplicates()","61b0846f":"data.shape","9f6e4516":"tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                           reduce_len=True)\ndata = data.apply(tokenizer.tokenize)","aec34e78":"stopwords_english = stopwords.words('english')\n\ndef clean(x):\n    return [y for y in x if y not in stopwords_english and y not in string.punctuation\n          and (len(y) > 1 or emoji.is_emoji(y)) ]\n\ndate = data.apply(clean)","f0282bef":"stemmer = PorterStemmer()\ndef stem(x):\n    return [stemmer.stem(y) for y in x]\n\n#stemming\nstemmed_tweets = data.apply(stem)\n","5171b698":"\nlematizer = WordNetLemmatizer()\ndef lema(x):\n    return [lematizer.lemmatize(y) for y in x]\n\n#lema\nlemmatized_tweets = data.apply(lema)","2e6929cc":"from nltk.corpus import twitter_samples\n\nnltk.download('twitter_samples')\n\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')","57e598bb":"def process_tweet(tweet,use_lemma =False):\n    if use_lemma:\n        stemmer = WordNetLemmatizer()\n    else:\n        stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            # tweets_clean.append(word)\n            if use_lemma:\n                stem_word = stemmer.lemmatize(word)  # stemming word\n            else:\n                stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n\n    return tweets_clean\n\n","943c4118":"\ndef build_freqs(tweets, ys):\n    yslist = np.squeeze(ys).tolist()\n\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n    return freqs\n\n","24dbadc2":"train_pos  = all_positive_tweets[:4000]\ntest_pos  = all_positive_tweets[4000:]\ntrain_neg  = all_negative_tweets[:4000]\ntest_neg  = all_negative_tweets[4000:]\n\ntrain_x = train_pos + train_neg\ntest_x = test_pos + test_neg\n\ntrain_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\ntest_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))","5932d1f8":"freqs = build_freqs(train_x, train_y)","e45fdd66":"theta = np.array([[7e-08, 0.0005239, -0.00055517]])\n\ndef logits(tweet):\n\n    i=0\n    x = np.zeros([1, 3])\n    x[0, 0] = 1\n    for token in tweet:\n\n        if (token,1) in freqs.keys():\n            x[0,1] += freqs[(token,1)]\n        else: i+=1\n            \n        if (token,0) in freqs.keys():\n            x[0,2] += freqs[(token,0)]\n        else: i+=1\n\n#     print(i,\" unknown tokens were discarded, not found in vocabulary\")\n    return x\n\ndef log_predict(tweet):\n    z = logits(tweet)\n    z = np.dot(theta,z.flatten())\n    if (1 \/ (1 + np.exp(-z))) > 0.5:    #Sigmoid\n        return 1\n    else: return 0\n\n","45cbff77":"predicted_y_LogReg = []\nfor x in test_x:\n    predicted_y_LogReg.append(log_predict(process_tweet(x)))\n\npredicted_y_LogReg[:5]","703d3b02":"total_tweets = len(test_y)\nright_preds = sum(test_y==predicted_y_LogReg)\naccuracy_log = right_preds\/total_tweets\nprint(\"Logistic regression accuracy is: \",accuracy_log)","f4f2a06f":"def train_naive_bayes(freqs, train_x, train_y):\n    \n    loglikelihood = {}\n    logprior = 0\n\n    # calculate V, the number of unique words in the vocabulary\n    vocab = set([pair[0] for pair in freqs.keys()])\n    V = len(vocab)\n\n    # calculate N_pos and N_neg\n    N_pos = N_neg = 0\n    for pair in freqs.keys():\n        # if the label is positive (greater than zero)\n        if pair[1] > 0:\n\n            # Increment the number of positive words by the count for this (word, label) pair\n            N_pos += freqs[pair]\n\n        # else, the label is negative\n        else:\n\n            # increment the number of negative words by the count for this (word,label) pair\n            N_neg += freqs[pair]\n\n    # Calculate D, the number of documents\n    D = len(train_y)\n\n    # Calculate D_pos, the number of positive documents (*hint: use sum(<np_array>))\n    D_pos = (len(list(filter(lambda x: x > 0, train_y))))\n\n    # Calculate D_neg, the number of negative documents (*hint: compute using D and D_pos)\n    D_neg = (len(list(filter(lambda x: x <= 0, train_y))))\n\n    # Calculate logprior\n    logprior = np.log(D_pos) - np.log(D_neg)\n\n    # For each word in the vocabulary...\n    for word in vocab:\n        # get the positive and negative frequency of the word\n        freq_pos = lookup(freqs,word,1)\n        freq_neg = lookup(freqs,word,0)\n\n        # calculate the probability that each word is positive, and negative\n        p_w_pos = (freq_pos + 1) \/ (N_pos + V)\n        p_w_neg = (freq_neg + 1) \/ (N_neg + V)\n\n        # calculate the log likelihood of the word\n        loglikelihood[word] = np.log(p_w_pos\/p_w_neg)\n        \n    return logprior, loglikelihood\n\ndef lookup(freqs, word, label):\n    n = 0  # freqs.get((word, label), 0)\n\n    pair = (word, label)\n    if (pair in freqs):\n        n = freqs[pair]\n\n    return n\n\nlogprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)","4e383814":"def NB_predict(tweet):\n    p = 0\n    p += logprior\n\n    for token in tweet:\n        if token in loglikelihood:\n            p += loglikelihood[token]\n            \n    # returning both raw probability and labels\n    if p>0:\n        return p,1\n    else: return p,0","7ea49e0a":"predicted_y_NB = []\nfor x in test_x:\n    predicted_y_NB.append(NB_predict(process_tweet(x))[1])","35dd6bb1":"total_tweets = len(test_y)\nright_preds = sum(test_y==predicted_y_NB)\naccuracy_NB = right_preds\/total_tweets\nprint(\"Accuracy of Naive Nayes is: \",accuracy_NB)","11f27407":"sample_tweet = data[5048]\nprint(\"Logisitic regression prediciton: \", log_predict(sample_tweet))\nprint(\"Naive Bayes prediciton: \", NB_predict(sample_tweet)[1])\nsample_tweet","02eb6824":"sample_tweet = data[5047]\nprint(\"Logisitic regression prediciton: \", log_predict(sample_tweet))\nprint(\"Naive Bayes prediciton: \", NB_predict(sample_tweet)[1])\nsample_tweet","4dc210b0":"predicted_y_LogReg = []\nfor x in test_x:\n    predicted_y_LogReg.append(log_predict(process_tweet(x)))\n\ntotal_tweets = len(test_y)\nright_preds = sum(test_y==predicted_y_LogReg)\naccuracy_log = right_preds\/total_tweets\nprint(\"Logistic regression accuracy with stemming: \",accuracy_log)\n\n# passing use_lemma =True \npredicted_y_LogReg = []\nfor x in test_x:\n    predicted_y_LogReg.append(log_predict(process_tweet(x,True)))\n\ntotal_tweets = len(test_y)\nright_preds = sum(test_y==predicted_y_LogReg)\naccuracy_log = right_preds\/total_tweets\nprint(\"Logistic regression accuracy with Lemmatization: \",accuracy_log)","80643c41":"#### *we can see, using Lemma decreases performance a bit*","a6debef9":"## 5. Applying trained models to predict the sentiments of our tweets data","12ee820b":"## Steps:\n#### 1. Simple EDA and preprocessing of Unlabeled tweets\n#### 2. Clean and Preprocess Labled tweets\n#### 3. Fit a Logistic regression model on labled tweets, so to use it on unlabeled tweets afterwords\n#### 4. Also fitting a Naive Bayes model and comparing its accuracy to Logistic regression\n#### 5. Applying trained models to predict the sentiments of our tweets data\n#### 6. Comparing Stemming and Lemmatization results\n\n","320ffb51":"#### LogReg accuracy","3e0f9309":"# Objective :\n##   Using Naive Bayes and Logistic regression to find the sentiment of tweets","cd4f4ee7":"### *So as we can see, accuracy of logistic regression is slightly higher than Naive bayes. This finding may not be generalized because the labeled sample of tweets was small*","a92cef8a":"## 2. Clean and Preprocess Labled tweets","2d790ee0":"## 1. Simple EDA and preprocessing of Unlabeled tweets","eb09b65f":"#### Removing stop words","afd02a6d":"#### Filtering & Cleaning","1db69c35":"#### Stemming","7b02140e":"## 6. Lastly we assess whether using Lemma or stemming make any difference","2ceb0bd3":"## 3. Fit a Naive Bayes model on labled tweets","d9274958":"#### Building Positive and Negative word frequencies dictionary","2ea17a09":"#### Tokenizing tweets","304a217f":"## 3. Fit a Logistic regression model on labled tweets","5570a0c6":"#### Lemma","054119a0":"#### Dividing data into train and test sets","92b1a688":"#### NBayes accuracy"}}