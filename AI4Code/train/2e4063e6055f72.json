{"cell_type":{"9ac6d20d":"code","562d3cf1":"code","c6b12555":"code","ea3eb729":"code","0516bb0e":"code","3a708fcd":"code","d0a38a71":"code","07dfe1f4":"code","0223cb07":"code","b17494cd":"markdown","38257a66":"markdown","f459ccdb":"markdown","0fdfa7eb":"markdown","0cd435cc":"markdown","0349a744":"markdown","faa7ea9f":"markdown","622077a2":"markdown"},"source":{"9ac6d20d":"import math\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nimport torch\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\n\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification)\n","562d3cf1":"class TextDataset(Dataset):\n    def __init__(self, sentences_lst, target, tokenizer):\n        \"\"\" Define anything as long as you have enough information to process and output data in __getitem__\n        for example, in nlp, if you provide raw texts, you need to tokenize your data before output tensor \n        so you may provide tokenizer which has ability to tokenize text in the way you want, e.g., \n        Huggingface tokenizer object\n        \n        \"\"\"\n        self.sentences_lst = sentences_lst\n        self.targets = target\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.sentences_lst)\n\n    def __getitem__(self, idx):\n        \"\"\" \n        Args:\n        idx: index to the instance\n        \n        Return:\n        input tensor and label of the indexed sample.  \n        \"\"\"\n        encode = self.tokenizer.encode(self.sentences_lst[idx])\n        \n        target = torch.tensor(self.targets[idx],dtype=torch.float)\n        return encode, target\n\n# How dataset-dataloader paradigm works with huggingface tokenizers for constructing input for huggingface model?\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\ntext = ['When the young people returned to the ballroom, it presented a decidedly changed appearance.', \n        'Instead of an interior scene, it was a winter landscape.\\n',\n        'The floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks',\n        'Each player was given a small flag which they were to plant on reaching the Pole.\\nThis would have been an easy matter, but each traveller was obliged to wear snowshoes.']\n\ntrain_ds = TextDataset(text,[1,2,1,1],tokenizer)\n\ntrain_ds[0]","c6b12555":"\n# train_dl = DataLoader(train_ds,\n#                           batch_size = config[\"batch_size\"],\n#                           shuffle=True,\n#                           num_workers = 4,\n#                           pin_memory=True,\n#                           drop_last=False\n#                          )\n\n# for inp, target in train_dl:\n# #     print(inp, target)\n#     print(inp['input_ids'].shape)\n#     break\n# torch.utils.data.DataLoader which can load multiple samples parallelly using torch.multiprocessing workers","ea3eb729":"# now we load data into pytorch Dataset, here I use predefined API from `torchvision` to load MNIST image dataset .\n# If someone has already provided API for you, no hesitate to directly load datasets predefined\n# Normally, these predefined datasets are subclasses of torch.utils.data.Dataset\ndataset = datasets.MNIST(\".\",\n                train=True,\n                download=True,\n              )\n\nimage, label = next(iter(dataset))\nprint(image)\nprint(label)\nimage","0516bb0e":"dataset = datasets.MNIST(\".\",\n                train=True,\n                download=True,\n                transform=transforms.Compose(\n                    [transforms.Resize(14)]\n                )\n              )\n\n# Check one sample\nimage, label = next(iter(dataset))\nprint(image)\nprint(label)\nimage","3a708fcd":"dataset = datasets.MNIST(\".\",\n                train=True,\n                download=True,\n                transform=transforms.Compose(\n                    [transforms.Resize(14), transforms.ToTensor()]\n                )\n              )\n\n# Check one sample\nimage, label = next(iter(dataset))\nprint(type(image))\nprint(image.size())\nprint(label)\n","d0a38a71":"plt.style.use(\"ggplot\")\nplt.hist(data[0].flatten())\nplt.axvline(data[0].mean())\nplt.show()\n\ndataset = datasets.MNIST(\".\",\n                train=True,\n                download=True,\n                transform=transforms.Compose(\n                    [transforms.Resize(28), transforms.ToTensor()]\n                )\n              )\n\nimgs_lbls = list(iter(dataset))\nimgs = [sample[0] for sample in imgs_lbls]\nimgs_tensor = torch.Tensor(len(imgs), *(imgs[0].size()))\ntorch.cat(imgs, out=imgs_tensor)\nprint('Training imgs', imgs_tensor.size())\n\n\nmean, std = imgs_tensor.mean(), imgs_tensor.std()\nmean, std","07dfe1f4":"# # dataloader could be used easily to compute mean and std \n# # It is especially useful if dataset is too large, then we can divide it into many batches for aggregating multiple times\n# dataloader = DataLoader(dataset,\n#                         batch_size=len(dataset),\n#                         shuffle=True)\n# data = next(iter(dataloader))\n# data[0].mean(), data[0].std()","0223cb07":"dataset = datasets.MNIST(\".\",\n                train=True,\n                download=True,\n                transform=transforms.Compose(\n                    [transforms.Resize(28), transforms.ToTensor(), transforms.Normalize(mean, std)]\n                )\n              )\n\n\nimgs_lbls = list(iter(dataset))\nimgs = [sample[0] for sample in imgs_lbls]\nimgs_tensor = torch.Tensor(len(imgs), *(imgs[0].size()))\ntorch.cat(imgs, out=imgs_tensor)\nprint('Training imgs', imgs_tensor.size())\n\nplt.style.use(\"ggplot\")\nplt.hist(data[0].flatten())\nplt.axvline(data[0].mean())\nplt.show()\n\n\nmean, std = imgs_tensor.mean(), imgs_tensor.std()\nmean, std\n\n","b17494cd":"# Pytorch `Dataset` and `Dataloader` API\nSimply speaking, `Dataset` reads data and output data in tensor as output to `Dataloader`. Then `Dataloader` provides input for training models.\n\n`torch.utils.data.Dataset` has two styles: map and iterable. Normally, we use map style and take care of 3 methods: `__init__`, `__getitem__`, `__len__`. Just **read the following code and command in details** since I provide flexible and simple template which at least suits all my own use case. [Here is the detail.](https:\/\/pytorch.org\/docs\/stable\/data.html)","38257a66":"Originally, it returns images as PIL format. Luckily, `torchvision` defines transform pipeline for dealing dataset using `transform.Compose()` directly when loading it. The following ones are the most commonly used.","f459ccdb":"Firstly, we need calculate mean and standard deviation","0fdfa7eb":"# Another example for vision","0cd435cc":"# Dataloader: Pytorch built-in support for batch iteration\nDifferent to normal implementation of Batch Training as below, in Pytorch, we could use built-in `DataSet` and `DataLoader` in Pytorch to load the training and validation data which would do the batch iteration for us. \n\n\n\n## Basic knowledge of Batch training for newies\n\nFor somebody who is not familiar with concepts like iteration, batch, epochs, here is the **basic background knowledge of Batch Training**.\n\n\n* Iteration: Normally, this concept is used in the Mini-batch training where iteration # is how many times passes(include forward propagation and backpropagation) are needed to go through the whole dataset once. Each iteration train <b>one-batch-size<\/b> samples. During one iteration, you usually pass a subset of the data set, which is called ***mini-batch***. In batch training, you pass all data in one iteration which also makes one epoch.\n    * **<span style=\"color:red\">Forward pass<\/span>**: refers to calculation process, values of the output layers from the inputs data. It's traversing through all neurons from first to last layer. A **<span style=\"color:cyan\">loss function<\/span>** is calculated from the output values.\n\n    * **<span style=\"color:red\">Backward pass<\/span>**: refers to process of computation from the last layer backward to the first layer to count changes in weights (de facto learning), using **<span style=\"color:cyan\">gradient descent<\/span>** algorithm (or similar) and update weights.\n\n\n\n* Epoch: One \"Epoch\" would go through the entire data set once. \n\n* BATCH # = Iteration # = Total training # \/ batch_size\n\n* Naive implementation of Batch Training (for complete and clear understanding, Epoch loop is also added)\n\n```\nfor epoch in range(EPOCH):\n    # Iteration: loop over all the batches\n    for i in range(BATCH_NUM):\n        x_batch, y_batch = ..\n```\n\nThe process could be visualized as below.\n\n\n![image-20200920145308194](https:\/\/i.loli.net\/2020\/09\/20\/JFehHBSq3o64gVl.png)\n\nNormally, `DataLoader` could be directly instantiated using `Dataset` instance. Furthermore, `batch_size` could be specified for batch training so that you do not need to explicitly loop each batch. More details about how to draw samples for each batch refer to: \nhttps:\/\/pytorch.org\/docs\/stable\/data.html#torch.utils.data.DataLoader\n    \nThe `Dataloader` should have the length as the iteration(the total number of samples \/ batch size). Let's check that.(it has 1 difference. I think it is because `Dataloader` at default will discard the left few samples which cannot make it the batch size).\n","0349a744":"`Resize()`","faa7ea9f":"`Normalize()`","622077a2":"`ToTensor()`"}}