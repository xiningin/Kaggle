{"cell_type":{"85f21b48":"code","4227bf2b":"code","e058d9fd":"code","cf89ba92":"code","5f4e8ba8":"code","4a3d5a6d":"code","e2eb2ca2":"code","7074bc03":"code","bf61dce2":"code","d929558d":"code","785134f9":"code","fb119b62":"code","0d7c9595":"code","83f2acf3":"code","29f6aac8":"code","d228ce9d":"code","abf7e659":"code","cae27d49":"code","d229c9d1":"code","368017bc":"code","722e9da7":"code","c242e071":"code","45b29aad":"code","a9c5c846":"code","9fd19ac2":"code","369c2d72":"code","2fa74f73":"code","717a87df":"code","c5e6af3e":"code","b2e1ea4b":"code","121b1e57":"code","04dde1f4":"markdown","f3a43cd1":"markdown","da2d8f00":"markdown","a290878b":"markdown","1de484ec":"markdown","41c81403":"markdown","e771484a":"markdown","90e6bbf7":"markdown","812ce3dd":"markdown","7932f56c":"markdown","fb8291e3":"markdown","9e139967":"markdown","2710af4c":"markdown","f39c11d1":"markdown","7c593c82":"markdown","aa0e719e":"markdown","d7d678d6":"markdown","a41aa2e6":"markdown","539759d3":"markdown","7e57d20f":"markdown","f13083ec":"markdown","7d31f8dc":"markdown","0ccce703":"markdown","4a218bf3":"markdown"},"source":{"85f21b48":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n","4227bf2b":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","e058d9fd":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","cf89ba92":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","5f4e8ba8":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","4a3d5a6d":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# creditcard.csv may have more rows in reality, but we are only loading\/previewing the first 1000 rows\ndf1 = pd.read_csv('\/kaggle\/input\/creditcard.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'creditcard.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","e2eb2ca2":"df1.head(5)","7074bc03":"plotPerColumnDistribution(df1, 10, 5)","bf61dce2":"plotCorrelationMatrix(df1, 8)\n","d929558d":"plotScatterMatrix(df1, 27, 10)  # signature(data, plotSize, textSize) #uses 10 columns at most, this is a computation intensive plot","785134f9":"from sklearn.metrics import accuracy_score, auc, balanced_accuracy_score, confusion_matrix, f1_score, precision_score, average_precision_score, roc_auc_score,  recall_score,  precision_recall_curve #some scoring functions\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, IsolationForest, RandomForestClassifier # Some classifiers\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate, train_test_split #Cross validation tools, and a train\/test split utility\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV #Hyper parameter search tools\nfrom hyperopt import hp, tpe, STATUS_OK, fmin, Trials #Hyperparameter search using a loss function \n","fb119b62":"# creditcard.csv may have more rows in reality, but we are only loading\/previewing the first 1000 rows\ndf = pd.read_csv('\/kaggle\/input\/creditcard.csv', delimiter=',')\ndf.dataframeName = 'creditcard.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')","0d7c9595":"X = df.iloc[:, 1:30]\ny = df.iloc[:, 30:31]\nX.head(), y.head()","83f2acf3":"# Let's split our dataset and see what it looks like:\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.25)\ntraining_fraud = sum(y_train.values)\ntraining_fraud_pct = sum(y_train.values)\/len(y_train.values)*100\ntest_fraud = sum(y_test.values)\ntest_fraud_pct = sum(y_test.values)\/len(y_test.values)*100\nprint(\"X train: {}\\nX test:  {}\\ny_train: {}\\ny test:  {} \\nFraud in train set: {},   {:2f}%\\nFraud in test set:  {},  {:2f}%\\n\".format( X_train.shape, \n                                                                                                                      X_test.shape, \n                                                                                                                      y_train.shape, \n                                                                                                                      y_test.shape, \n                                                                                                                      training_fraud[0], training_fraud_pct[0],\n                                                                                                                      test_fraud[0], test_fraud_pct[0]))","29f6aac8":"RandomForestClassifier()","d228ce9d":"help(RandomForestClassifier)","abf7e659":"clf = RandomForestClassifier()\nclf.fit(X_train, y_train.values.ravel())","cae27d49":"y_pred = clf.predict(X_test) # Using our test set, let's try and make some predictions","d229c9d1":"confusion_matrix(y_test, y_pred) # How did we do?  \n\n# TN FN\n# FP TP                               ","368017bc":"def print_scores(y_t, y_p):\n    print(f'Accuracy  :{accuracy_score(y_t, y_p):.2f}' )\n    print(f'Balanced  :{balanced_accuracy_score(y_t, y_p):.2f}' )\n    print(f'F1        :{f1_score(y_t, y_p):.2f}' )\n    print(f'Precision :{precision_score(y_t, y_p):.2f}' )\n    print(f'Recall    :{recall_score(y_t, y_p):.2f}' )\n    print(f'roc auc   :{roc_auc_score(y_t, y_p):.2f}' )\n    print(f'pr)auc    :{average_precision_score(y_t, y_p):.2f}' )\nprint_scores(y_test, y_pred)\n","722e9da7":"#let's start over...\nclf = RandomForestClassifier(n_jobs=-1,n_estimators=10, verbose=1)\n# ... but this time, let's fit our model using sklearn.model_selection.cross_val_score\ncvs = cross_val_score(clf,X=X_train, y=y_train, scoring='average_precision')\n","c242e071":"print(f'Using {len(cvs)} trials:\\n {cvs}')\nprint(f'Average:{np.mean(cvs)}')","45b29aad":"#let's start over...\nclf = RandomForestClassifier(n_jobs=-1, verbose=0) # these settings use multiple cores, and provide more command line feedback\n# ... but this time, let's fit our model using sklearn.model_selection.cross_validate\ncvs = cross_validate(clf,X=X_train, y=y_train, scoring=['accuracy','average_precision','balanced_accuracy','f1','precision','recall','roc_auc'])\n","a9c5c846":"for k, v in cvs.items():\n    print(f'{k.replace(\"test_\",\"\"):23}{v}   Avg:{\"\":4}{np.mean(v):.2f}' )","9fd19ac2":"#let's start over again...\nclf = RandomForestClassifier(n_jobs=-1, n_estimators=10, verbose=0) # these settings use multiple cores, and provide more command line feedback\n# ... but this time, let's fit our model using sklearn.model_selection.cross_validate\ncvs = cross_validate(clf,X=X_train, y=y_train.values.ravel(), cv=10, scoring=['accuracy','average_precision','balanced_accuracy','f1','precision','recall','roc_auc'])\n","369c2d72":"k_formatter = lambda k: k.replace('test_','')         # formatter for striping out the test prefix from our CV score names\nv_formatter = lambda val: str(val)[:6].ljust(6, '0')  # formatter for displaying our values to 4 significant digits. \n\nfor k, v in cvs.items():\n    v_print = ', '.join(map(v_formatter, v)) \n    print(f'{k_formatter(k):23} {v_print}     Avg:  {np.mean(v):.4f}    SDev:  {np.std(v):.4f}' )\n ","2fa74f73":"# let's start over again...\n# but this time, our parameter grid will look a bit different. \n\nparam_grid = {'max_depth':[ 5, 15, None], \n              'max_features': [None, 'sqrt'],\n              'n_estimators':[100, 500],\n              'min_samples_split':[2,3,5]}\nclf = RandomForestClassifier(n_jobs=-1, verbose=1,oob_score=True)       # We declare an instance of our classifier\n                                                                        # But instead of fitting it, We pass it, \nclf_cv = GridSearchCV(clf, param_grid, scoring=\"average_precision\",     # (and our parameter grid) to a new instance\n                      n_jobs=-1, verbose=1)                             # of a grid search object","717a87df":"# this will take a long time. 3*3*2*2= 36 fits, with 3-fold cross-validation = 108 fits\n\n# clf_cv.fit(X_train, y_train)","c5e6af3e":"# let's start over again...\n# but this time, our parameter grid will look a bit different. \nfrom scipy.stats import randint as sp_rand_int\n\nparam_grid = {'max_depth': sp_rand_int(5,30), \n              'max_features': sp_rand_int(5,30),\n              'n_estimators':sp_rand_int(100,500),\n              'min_samples_split':sp_rand_int(2,5)}\nclf = RandomForestClassifier(n_jobs=-1, verbose=1,oob_score=True) \nclf_cv =RandomizedSearchCV(clf, param_grid, scoring=\"average_precision\",     # (and our parameter grid) to a new instance\n                      n_jobs=-1, verbose=1, n_iter=10)","b2e1ea4b":"# This will run 10 times with 3-fold validation, for 30 fits. \nclf_cv.fit(X_train, y_train)","121b1e57":"clf_cv.best_\n","04dde1f4":"There is 1 csv file in the current version of the dataset:\n","f3a43cd1":"The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.","da2d8f00":"What other metrics are out there?","a290878b":"## Cross-Validation, Hyperparameters and Modeling\n\nI put this together as a learning exercise for myself, but I hope that others can benefit from the examples that I've strung together. \n\nIf anyone finds it useful, please let me know.  \n\n","1de484ec":"### Let's check 1st file: \/kaggle\/input\/creditcard.csv","41c81403":"Ok. It ran. What next?\n\nLet's see how well it predicts in our testing set. ","e771484a":"Scatter and density plots:","90e6bbf7":"## Conclusion\nThis concludes your starter analysis! To go forward from here, click the blue \"Edit Notebook\" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!","812ce3dd":"Let's try RandomForest. What parameters are available?","7932f56c":"1. ## Introduction\n1. Greetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue \"Edit Notebook\" or \"Fork Notebook\" button at the top of this kernel to begin editing.","fb8291e3":"So, cross validation uses samples of our data, and can give better scores without using our test data. The test data is reserved to prevent overfitting. \n\nHow can we use this information to get the best parameters for our model?\n\nOne way is using GridSearchCV. Grid search uses each combination of the parameters provided. It's an exhaustive search, and can be time consuming.\n\nThe parameter tuning guide for random forests notes that increasing n_estimators and max_features should always improve results, but that it may be computationally intensive with diminishing returns.\nIt also explains that max_depth and min_samples_split have reasonable defaults, but may be suboptimal. \n\n> 1.11.2.3. Parameters\n>\n> The main parameters to adjust when using these methods is n_estimators and max_features. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are max_features=None (always considering all features instead of a random subset) for regression problems, and max_features=\"sqrt\" (using a random subset of size sqrt(n_features)) for classification tasks (where n_features is the number of features in the data). Good results are often achieved when setting max_depth=None in combination with min_samples_split=2 (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default (bootstrap=True) while the default strategy for extra-trees is to use the whole dataset (bootstrap=False). When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by setting oob_score=True.\nMore here: https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#random-forest-parameters\n\nGridSearchCV ( and RandomSearchCV ), let you define a set of parameters, and a scoring method, and then run the fit repeatedly, trying to get the best combination of parameters. ","9e139967":"So, why might cross validation matter? The metric suggested by the providers of the dataset suggests that average precision be used. \n\n> Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n\n![image.png](attachment:image.png)\n\nWe have an average of .82, but our answers range from .78 to .87,  That seems like fairly wide range. \n\nLet's try again, but with more folds in our cross validation samples. ","2710af4c":"Let's take a quick look at what the data looks like:","f39c11d1":"are these accurate? Probably. But we can get a better picture of how are model is using our training data by trying cross validation.","7c593c82":"Distribution graphs (histogram\/bar graph) of sampled columns:","aa0e719e":"## Notes!\nI started this using Kaggle's starter notebook. I made minor tweaks to the code, but it is largely pre-built. If you're interested in seeing how some of the notebook based visualization tools work, it's a good place to start. \nIf you're just here to see what I made, skip ahead to the headline \"Cross-Validation, Hyperparameters and Modeling.\"\n","d7d678d6":"RandomSearchCV is a bit different. Instead of testing every combination of parameters, it tests random combinations parameters a preset number of times. \n\nFor more information, SciKit Learn has a [more detailed explanation.](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py)\n\n","a41aa2e6":"This gives us a better sense of how accurate our model by making better use of our training set. \nFor now, just realize, cross-validation uses a repeated sampling strategy to get a more accurate score (see also, k-fold). \n\nMoving on, we can also use cross validation to return multiple scores:\n\n","539759d3":"The stratify=y parameter of train_test_split gives an even ratio of our imbalanced class in each of our splits.  \n","7e57d20f":"Cool. But for now, let's just try and run it with the basic settings.","f13083ec":"Now you're ready to read in the data and use the plotting functions to visualize the data.","7d31f8dc":"Correlation matrix:","0ccce703":"That's neat, but what do the parameters actually do?","4a218bf3":"## Exploratory Analysis\nTo begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)"}}