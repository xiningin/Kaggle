{"cell_type":{"e84af866":"code","b7d59b38":"code","f78e400e":"code","c212bb12":"code","609f73d1":"code","2cd43169":"code","a54426b4":"code","1731a957":"code","4c7a3011":"code","8bbe8a8d":"code","73d67708":"code","d04fe295":"code","a5e57c37":"code","8762ae45":"code","ad28b372":"code","853d287b":"code","97dd2408":"code","38f2daa3":"code","e0f4a9b4":"code","3b8490f9":"code","47e5d694":"code","d42c20fd":"code","01a55e75":"code","873520c0":"code","f19350ca":"code","e70aee53":"code","9b9b0b3c":"code","d6ad557d":"code","203f3a59":"code","d5c5d361":"code","c38e1219":"code","2b383b10":"code","da4296ea":"code","9be1536c":"code","9ef1c386":"markdown","a85feb48":"markdown","0824e9b9":"markdown","446bfadf":"markdown","3324624f":"markdown","56a57d04":"markdown","f1f2d6fa":"markdown","dcd622db":"markdown","7bcf110e":"markdown","576683a0":"markdown","b056015e":"markdown","e82002d1":"markdown","fe2daa6b":"markdown","4c51b5e6":"markdown","67d06285":"markdown","b8d41053":"markdown","1cb0f187":"markdown","eb85f748":"markdown","3b801317":"markdown","4ccc9720":"markdown","eaf76922":"markdown","2e0bb1c7":"markdown","c2366e0b":"markdown","bfaaa1b3":"markdown","5fa06869":"markdown","c9b8ed9f":"markdown","3830edf2":"markdown","926ea81f":"markdown","b2e7c7eb":"markdown","7dc05455":"markdown","9fa829c4":"markdown","9406a89b":"markdown","8f471c7f":"markdown","809417d2":"markdown","ba093807":"markdown","b9ef6293":"markdown","3309f8f0":"markdown","cad0d9c8":"markdown","5a7e15e1":"markdown","824308a9":"markdown","0d44dd20":"markdown","d376117b":"markdown","15304a32":"markdown"},"source":{"e84af866":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(color_codes = True)\n\n# used to supress display of warnings\nimport warnings\n\nimport itertools\n\n# pre-processing methods\nfrom sklearn.preprocessing import StandardScaler\n\n# evaluation methods\nfrom sklearn.pipeline import Pipeline","b7d59b38":"# suppress display of warnings\nwarnings.filterwarnings('ignore')\n\n# display all dataframe columns\npd.options.display.max_columns = None\n\n# to set the limit to 3 decimals\npd.options.display.float_format = '{:.7f}'.format\n\n# display all dataframe rows\npd.options.display.max_rows = None","f78e400e":"INPUT = '..\/input\/tabular-playground-series-oct-2021\/'\n\ntrain = pd.read_csv(INPUT + 'train.csv')\ntest = pd.read_csv(INPUT + 'test.csv')\nsubmission = pd.read_csv(INPUT + 'sample_submission.csv')","c212bb12":"print('Number of rows = {0} and Number of columns = {1} in train set'.format(train.shape[0], train.shape[1]))\nprint('Number of rows = {0} and Number of columns = {1} in test set'.format(test.shape[0], test.shape[1]))\nprint('Number of rows = {0} and Number of columns = {1} in sample solution set'.format(submission.shape[0], submission.shape[1]))","609f73d1":"# Get train set - top 5 rows\ntrain.head()","2cd43169":"# Get test set - top 5 rows\ntest.head()","a54426b4":"# Get submission set - top 5 rows\nsubmission.head()","1731a957":"# Check train set info\ntrain.info()","4c7a3011":"# Reference - https:\/\/www.kaggle.com\/maksymshkliarevskyi\/tps-sep-all-for-start-eda-xgb-catboost-baseline\nmy_colors = ['#DC143C', '#FF1493', '#FF7F50', '#FFD700', '#32CD32', '#4ddbff', '#1E90FF', '#663399', '#708090']\n\ndtypes = train.dtypes.value_counts().reset_index()\n\nplt.figure(figsize = (12, 1))\nplt.title('Data types\\n')\nplt.barh(str(dtypes.iloc[0, 0]), dtypes.iloc[0, 1], label = str(dtypes.iloc[0, 0]), color = my_colors[4])\nplt.barh(str(dtypes.iloc[0, 0]), dtypes.iloc[1, 1], left = dtypes.iloc[0, 1], label = str(dtypes.iloc[1, 0]), color = my_colors[5])\nplt.legend(loc = 'upper center', ncol = 3, fontsize = 13, bbox_to_anchor = (0.5, 1.45), frameon = False)\nplt.yticks('')\nplt.text(110, -0.9, '', size = 8)\nplt.show()","8bbe8a8d":"# Check test set info\ntest.info()","73d67708":"# Drop id column in train and test set\ntrain.drop('id', axis = 1, inplace = True)\ntest.drop('id', axis = 1, inplace = True)","d04fe295":"# Check duplicates in train set\ntrain.duplicated().sum()","a5e57c37":"# Check duplicates in test set\ntest.duplicated().sum()","8762ae45":"# Number of missing values in each column in train set\ntrain.isnull().sum().sort_values(ascending = False).to_frame().rename({0:'Counts'}, axis = 1).T.style.background_gradient('crest')","ad28b372":"# Number of missing values in each column in test set\ntest.isnull().sum().sort_values(ascending = False).to_frame().rename({0:'Counts'}, axis = 1).T.style.background_gradient('crest')","853d287b":"# Reference: https:\/\/www.kaggle.com\/suharkov\/sep-2021-playground-eda-no-model-for-now\ndf_plot = ((train.iloc[:,0:242] - train.iloc[:,0:242].min())\/(train.iloc[:,0:242].max() - train.iloc[:,0:242].min()))\n\nfig, ax = plt.subplots(9, 1, figsize = (25,50))\nsns.boxplot(data = df_plot.iloc[:, 0:29], ax = ax[0])\nsns.boxplot(data = df_plot.iloc[:, 29:59], ax = ax[1])\nsns.boxplot(data = df_plot.iloc[:, 59:89], ax = ax[2])\nsns.boxplot(data = df_plot.iloc[:, 89:119], ax = ax[3])\nsns.boxplot(data = df_plot.iloc[:, 119:149], ax = ax[4])\nsns.boxplot(data = df_plot.iloc[:, 149:179], ax = ax[5])\nsns.boxplot(data = df_plot.iloc[:, 179:209], ax = ax[6])\nsns.boxplot(data = df_plot.iloc[:, 209:239], ax = ax[7])\nsns.boxplot(data = df_plot.iloc[:, 239:242], ax = ax[8])","97dd2408":"# Reference: https:\/\/www.kaggle.com\/suharkov\/sep-2021-playground-eda-no-model-for-now\ndf_plot = ((test.iloc[:,0:242] - test.iloc[:,0:242].min())\/(test.iloc[:,0:242].max() - test.iloc[:,0:242].min()))\n\nfig, ax = plt.subplots(9, 1, figsize = (25,50))\nsns.boxplot(data = df_plot.iloc[:, 0:29], ax = ax[0])\nsns.boxplot(data = df_plot.iloc[:, 29:59], ax = ax[1])\nsns.boxplot(data = df_plot.iloc[:, 59:89], ax = ax[2])\nsns.boxplot(data = df_plot.iloc[:, 89:119], ax = ax[3])\nsns.boxplot(data = df_plot.iloc[:, 119:149], ax = ax[4])\nsns.boxplot(data = df_plot.iloc[:, 149:179], ax = ax[5])\nsns.boxplot(data = df_plot.iloc[:, 179:209], ax = ax[6])\nsns.boxplot(data = df_plot.iloc[:, 209:239], ax = ax[7])\nsns.boxplot(data = df_plot.iloc[:, 239:242], ax = ax[8])","38f2daa3":"num_cols = train.columns[0:242]","e0f4a9b4":"cols = num_cols[0:110]\nlength = len(cols)\ncs = [\"b\",\"r\",\"g\",\"c\",\"m\",\"k\",\"lime\",\"c\",\"b\",\"r\"]*11\nfig = plt.figure(figsize=(20, 100))\n\nfor i,j,k in itertools.zip_longest(cols, range(length),cs):\n    plt.subplot(22,5,j+1)\n    fig.tight_layout(pad=2.0)\n    ax = sns.kdeplot(train[i], color=k)\n    ax.set_facecolor(\"w\")\n    plt.axvline(train[i].mean(), linestyle=\"dashed\", label=\"mean\", color=\"k\")\n    plt.legend(loc=\"best\")\n    plt.title(i,color=\"navy\")\n    plt.xlabel(\"\")","3b8490f9":"cols = num_cols[110:220]\nlength = len(cols)\ncs = [\"b\",\"r\",\"g\",\"c\",\"m\",\"k\",\"lime\",\"c\",\"b\",\"r\"]*11\nfig = plt.figure(figsize=(20, 100))\n\nfor i,j,k in itertools.zip_longest(cols, range(length),cs):\n    plt.subplot(22,5,j+1)\n    fig.tight_layout(pad=2.0)\n    ax = sns.kdeplot(train[i], color=k)\n    ax.set_facecolor(\"w\")\n    plt.axvline(train[i].mean(), linestyle=\"dashed\", label=\"mean\", color=\"k\")\n    plt.legend(loc=\"best\")\n    plt.title(i,color=\"navy\")\n    plt.xlabel(\"\")","47e5d694":"cols = num_cols[220:240]\nlength = len(cols)\ncs = [\"b\",\"r\",\"g\",\"c\",\"m\",\"k\",\"lime\",\"c\",\"b\",\"r\"]*2\nfig = plt.figure(figsize=(20, 100))\n\nfor i,j,k in itertools.zip_longest(cols, range(length),cs):\n    plt.subplot(22,5,j+1)\n    fig.tight_layout(pad=2.0)\n    ax = sns.kdeplot(train[i], color=k)\n    ax.set_facecolor(\"w\")\n    plt.axvline(train[i].mean(), linestyle=\"dashed\", label=\"mean\", color=\"k\")\n    plt.legend(loc=\"best\")\n    plt.title(i,color=\"navy\")\n    plt.xlabel(\"\")","d42c20fd":"cols = num_cols[240:242]\nlength = len(cols)\ncs = [\"b\",\"r\"]\nfig = plt.figure(figsize=(20, 5))\n\nfor i,j,k in itertools.zip_longest(cols, range(length),cs):\n    plt.subplot(1,2,j+1)\n    fig.tight_layout(pad=2.0)\n    ax = sns.kdeplot(train[i], color=k)\n    ax.set_facecolor(\"w\")\n    plt.axvline(train[i].mean(), linestyle=\"dashed\", label=\"mean\", color=\"k\")\n    plt.legend(loc=\"best\")\n    plt.title(i,color=\"navy\")\n    plt.xlabel(\"\")","01a55e75":"plt.figure(figsize = (10,6))\ntrain['target'].value_counts().plot.pie(autopct = '%.2f', colors = ['powderblue', 'slateblue'])\nplt.title(\"target vlaue distribution\", pad = 20, fontdict = {'size' : 15, 'color' : 'brown', 'weight' : 'bold'})\nplt.show()","873520c0":"target_yes = len(train.loc[train['target'] == 1])\ntarget_no = len(train.loc[train['target'] == 0])\n\nprint(\"Number of positive class: {0} ({1:2.2f}%)\".format(target_yes, (target_yes \/ (target_yes + target_no)) * 100 ))\nprint(\"Number of negative class: {0} ({1:2.2f}%)\".format(target_no, (target_no \/ (target_yes + target_no)) * 100))","f19350ca":"cols = num_cols[0:110]\nlength = len(cols)\ncs = [\"b\",\"r\",\"g\",\"c\",\"m\",\"k\",\"lime\",\"c\",\"b\",\"r\"]*11\nfig = plt.figure(figsize=(20, 100))\n\nfor i,j,k in itertools.zip_longest(cols, range(length),cs):\n    plt.subplot(22,5,j+1)\n    fig.tight_layout(pad=2.0)\n    ax = sns.kdeplot(test[i], color=k)\n    ax.set_facecolor(\"w\")\n    plt.axvline(test[i].mean(), linestyle=\"dashed\", label=\"mean\", color=\"k\")\n    plt.legend(loc=\"best\")\n    plt.title(i,color=\"navy\")\n    plt.xlabel(\"\")","e70aee53":"cols = num_cols[110:220]\nlength = len(cols)\ncs = [\"b\",\"r\",\"g\",\"c\",\"m\",\"k\",\"lime\",\"c\",\"b\",\"r\"]*11\nfig = plt.figure(figsize=(20, 100))\n\nfor i,j,k in itertools.zip_longest(cols, range(length),cs):\n    plt.subplot(22,5,j+1)\n    fig.tight_layout(pad=2.0)\n    ax = sns.kdeplot(test[i], color=k)\n    ax.set_facecolor(\"w\")\n    plt.axvline(test[i].mean(), linestyle=\"dashed\", label=\"mean\", color=\"k\")\n    plt.legend(loc=\"best\")\n    plt.title(i,color=\"navy\")\n    plt.xlabel(\"\")","9b9b0b3c":"cols = num_cols[220:240]\nlength = len(cols)\ncs = [\"b\",\"r\",\"g\",\"c\",\"m\",\"k\",\"lime\",\"c\",\"b\",\"r\"]*2\nfig = plt.figure(figsize=(20, 100))\n\nfor i,j,k in itertools.zip_longest(cols, range(length),cs):\n    plt.subplot(22,5,j+1)\n    fig.tight_layout(pad=2.0)\n    ax = sns.kdeplot(test[i], color=k)\n    ax.set_facecolor(\"w\")\n    plt.axvline(test[i].mean(), linestyle=\"dashed\", label=\"mean\", color=\"k\")\n    plt.legend(loc=\"best\")\n    plt.title(i,color=\"navy\")\n    plt.xlabel(\"\")","d6ad557d":"cols = num_cols[240:242]\nlength = len(cols)\ncs = [\"b\",\"r\"]\nfig = plt.figure(figsize=(20, 5))\n\nfor i,j,k in itertools.zip_longest(cols, range(length),cs):\n    plt.subplot(1,2,j+1)\n    fig.tight_layout(pad=2.0)\n    ax = sns.kdeplot(test[i], color=k)\n    ax.set_facecolor(\"w\")\n    plt.axvline(test[i].mean(), linestyle=\"dashed\", label=\"mean\", color=\"k\")\n    plt.legend(loc=\"best\")\n    plt.title(i,color=\"navy\")\n    plt.xlabel(\"\")","203f3a59":"# Summary statistics for Train set\ntrain.iloc[:,0:242].describe().style.background_gradient(cmap='magma_r')","d5c5d361":"# Summary statistics for Train set\ntrain.iloc[:,242:].describe().style.background_gradient(cmap='magma_r')","c38e1219":"# Summary statistics for Test set\ntest.iloc[:,0:242].describe().style.background_gradient(cmap='inferno_r')","2b383b10":"# Summary statistics for Train set\ntest.iloc[:,242:].describe().style.background_gradient(cmap='magma_r')","da4296ea":"# Create a copy of Train set\ntrain_corr = train.copy()","9be1536c":"# Correlation between target and all other features\ntrain_corr.corr()[['target']].T.style.background_gradient('magma_r')","9ef1c386":"**Observations**\n\n* 240 columns are decimals (float) and 47 columns are binary columns.\n* Id column is numerical column\n* target column is categorical (binary) column","a85feb48":"<a id = '4.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.1 Check Duplicates <\/strong><\/p> ","0824e9b9":"<a id = '1.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 1. Overview <\/h2> ","446bfadf":"#### Setting Options","3324624f":"<p style = \"font-size:20px; color: #007580 \"><strong> Study Correlation in Train Set <\/strong><\/p>","56a57d04":"**Observations**\n\n* **All above features are skewed to higher values.**","f1f2d6fa":"<a id = '4.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 4. Data Cleaning <\/h2> ","dcd622db":"**Observations**\n\n* **f131, f132, f135, f136, f139, f143, f147, f148, f151, f156,  - Left skewed distribution - all these features are skewed to lower values.**\n* **Except all above features, rest of the features are skewed to higher values.**","7bcf110e":"#### Data type of each attribute","576683a0":"<a id = '4.4'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.4 Check Outliers in Test Set <\/strong><\/p> ","b056015e":"<a id = '5.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.2 Univariate analysis - Target (Categorical) column <\/strong><\/p>","e82002d1":"<a id = '6.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 6. Future work <\/h2> \n\n1. Data preprocessing.\n2. Feature engineering.\n3. Model building.\n4. Hyper parameter tuning.\n5. Final submission.\n\n<p style = \"font-size:30px; color: #007580 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Finally, please let me know your suggestions. Thanks for reading \ud83d\ude42<\/strong><\/p>","fe2daa6b":"<a id = '0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #007580; color : #fed049; border-radius: 5px 5px; text-align:center; font-weight: bold\" >Table of Contents<\/h2> \n\n1. [Overview](#1.0)\n2. [Import the necessary libraries](#2.0)\n3. [Data Collection](#3.0)\n4. [Data Cleaning](#4.0)\n\t- [4.1 Check Duplicates](#4.1)\n\t- [4.2 Check Missing Values](#4.2)\n\t- [4.3 Check Outliers in Train Set](#4.3)\n\t- [4.4 Check Outliers in Test Set](#4.4)\n\t- [4.5 Data Cleaning Summary](#4.5)\n5. [EDA (Data Analysis and Preparation)](#5.0)\n\t- [5.1 Univariate Analysis - Train set - Numerical columns](#5.1)\n\t- [5.2 Univariate analysis - Target (Categorical) column](#5.2)\n\t- [5.3 Univariate analysis - Test set - Numerical columns](#5.3)\n    - [5.4 Study Summary Statistics](#5.4)\n\t- [5.5 Study Correlation](#5.5)\n\t- [5.6 EDA (Exploratory Data Analysis) Summary](#5.6)\n6. [Future work](#6.0)","4c51b5e6":"**Observations**\n\n* Except these features - f22, f44, f138, f139, f142, f144, f146, f157, f158  all other features are having outliers in test set.","67d06285":"### Files\n\n- train.csv - the training data with the target column\n- test.csv - the test set; you will be predicting the target for each row in this file (the probability of the binary target)\n- sample_submission.csv - a sample submission file in the correct format","b8d41053":"**Observations**\n\n* **f241 - Moderately right skewed distribution.**\n* **f240 feature is skewed to higher values.**","1cb0f187":"[![EDA-Image.png](https:\/\/i.postimg.cc\/Nj1tLsHM\/EDA-Image.png)](https:\/\/postimg.cc\/v4HkNdXF)","eb85f748":"<h2 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> TPS Oct 2021 - EDA<\/h2> ","3b801317":"<a id = '5.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.1 Univariate Analysis - Train set - Numerical columns<\/strong><\/p>","4ccc9720":"### Data Description:\n\nFor this competition, we will be predicting a binary target based on a number of feature columns given in the data. The columns are a mix of scaled continuous features and binary features.\n\nThe data is synthetically generated by a GAN that was trained on real-world molecular response data.","eaf76922":"### Calculate target ratio from target variable","2e0bb1c7":"**Observations**\n\n* Except these features - f22, f44, f138, f139, f142, f144, f146, f157, f158  all other features are having outliers in train set.","c2366e0b":"<a id = '5.5'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.5 Study Correlation <\/strong><\/p>","bfaaa1b3":"<a id = '5.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 5. EDA (Data Analysis and Preparation) <\/h2> ","5fa06869":"<a id = '4.3'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.3 Check Outliers in Train Set <\/strong><\/p> ","c9b8ed9f":"**Observations**\n\n* **Target feature is having moderately negative correlation with f22 feature.**\n* **All other independent featuers are having negligible correlation with target feature.**","3830edf2":"**Observations**\n\n* **f131, f132, f135, f136, f139, f143, f147, f148, f151, f156,  - Left skewed distribution - all these features are skewed to lower values.**\n* **Except all above features, rest of the features are skewed to higher values.**","926ea81f":"<a id = '4.5'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.5 Data Cleaning Summary <\/strong><\/p>\n\n1. No duplicate instances in both train and test set.\n2. No missing values in both train and test set.\n3. Except these features - f22, f44, f138, f139, f142, f144, f146, f157, f158 all other features are having outliers in train set.\n4. Except these features - f22, f44, f138, f139, f142, f144, f146, f157, f158 all other features are having outliers in test set.\n\n    **Note: Need to handle Outliers in both Train and Test set.**","b2e7c7eb":"**Observations**\n\n* **f39 feature - Normally distributed.**\n* **f6, f8, f13, f91 - Left skewed distribution - all these features are skewed to lower values.**\n* **f85 - seems to normally distributed.**\n* **f15, f21, f44, f62, f77, f79, f81, f86 - Moderately left skewed distribution.**\n* **f1, f3, f4, f5, f7, f12, f16, f17, f20, f24, f26, f40, f65, f88, f89 - Moderately right skewed distribution.**\n* **Except all above features, rest of the features are skewed to higher values.**","7dc05455":"<a id = '4.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.2 Check Missing Values <\/strong><\/p> ","9fa829c4":"**Observations**\n\n* **f241 - Moderately right skewed distribution.**\n* **f240 feature is skewed to higher values.**","9406a89b":"<a id = '2.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 2. Import the necessary libraries <\/h2> ","8f471c7f":"<a id = '5.3'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.3 Univariate analysis - Test set - Numerical columns <\/strong><\/p>","809417d2":"<a id = '5.6'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.6 EDA (Exploratory Data Analysis) Summary <\/strong><\/p>\n\n**1. Univariate analysis in both Train and Test set.**\n* **f39 feature - Normally distributed.**\n* **f6, f8, f13, f91, f131, f132, f135, f136, f139, f143, f147, f148, f151, f156 - Left skewed distribution - all these features are skewed to lower values.**\n* **f85 - seems to normally distributed.**\n* **f15, f21, f44, f62, f77, f79, f81, f86 - Moderately left skewed distribution.**\n* **f1, f3, f4, f5, f7, f12, f16, f17, f20, f24, f26, f40, f65, f88, f89, f241 - Moderately right skewed distribution.**\n* **Except all above features, rest of the features are skewed to higher values.**\n\n\t**Note: Train and Test set distribution looks similar, that means they come from same population distribution.**\n\n**2. We have balanced target data which is 50% in both classes (1 & 0).**\n\n**3. Correlation analysis:**\n* Target feature is having moderately negative correlation with f22 feature and negligible correlation with all other features.","ba093807":"<p style = \"font-size:30px; color: #007580 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Here are my other notebooks, please have a look and definitely you will find it useful. Happy reading \ud83d\ude42<\/strong><\/p>\n<ol>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/industrial-safety-complete-solution\">Industrial Safety - Complete Solution<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/eda-statistical-analysis-hypothesis-testing\">EDA - Statistical Analysis - Hypothesis Testing<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/random-forest-with-bootstrap-sampling-for-beginner\">Random Forest with Bootstrap Sampling for beginner<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/amazon-electronics-eda-recommender-system\">Amazon Electronics - EDA - Recommender System<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/personal-loan-logistic-reg-accuracy-90-41\">Personal Loan - Logistic Reg - Accuracy = 90.41%<\/a><\/li>\n<\/ol>","b9ef6293":"* **Above plot shows, target distribution is almost equal.**","3309f8f0":"**Observations**\n\n* 240 columns are decimals (float) and 47 columns are binary columns.","cad0d9c8":"<a id = '3.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 3. Data Collection <\/h2> ","5a7e15e1":"### Evaluation\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.","824308a9":"<p style = \"font-size:20px; color: #007580 \"><strong> Data Collection Summary <\/strong><\/p>\n\n1. There are about 1000000 rows and 287 columns in train dataset.\n2. There are about 500000 rows and 286 columns in test dataset.\n3. We see there are total 240 float64 types and 47 int64 types (binary) variables present in train dataset.\n4. We see there are total 240 float64 types and 46 int64 types (binary) variables present in test dataset.\n\n* **Binary Categorical columns** - From f242 to f284\n\n* **Numerical columns** - From f0 to f241","0d44dd20":"**Observations**\n\n* **All above features are skewed to higher values.**","d376117b":"<a id = '5.4'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.4 Study Summary Statistics <\/strong><\/p>","15304a32":"**Observations**\n\n* **f39 feature - Normally distributed.**\n* **f6, f8, f13, f91 - Left skewed distribution - all these features are skewed to lower values.**\n* **f85 - seems to normally distributed.**\n* **f15, f21, f44, f62, f77, f79, f81, f86 - Moderately left skewed distribution.**\n* **f1, f3, f4, f5, f7, f12, f16, f17, f20, f24, f26, f40, f65, f88, f89 - Moderately right skewed distribution.**\n* **Except all above features, rest of the features are skewed to higher values.**"}}