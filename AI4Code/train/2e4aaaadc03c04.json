{"cell_type":{"81444730":"code","08444cdb":"code","7faaf0c5":"code","e0c70bb0":"code","839a70b7":"code","1fe90400":"code","1d98a25e":"code","cf96e44e":"code","a24cfa8c":"code","346fec25":"code","59734a4c":"code","de020909":"code","9ffff1cd":"code","86f1d8ed":"code","600fe49b":"code","2f0a5797":"code","804a91a0":"code","ac562238":"code","82e793cb":"code","dd08123f":"code","61c205ed":"code","adbda90a":"code","1582071c":"code","138c8306":"code","f8a8c95c":"code","42214a4b":"code","73dfa53b":"markdown","d2399755":"markdown","188d7e24":"markdown","e2f30aaa":"markdown","956cff2e":"markdown","7fb70fb5":"markdown","20de3d8c":"markdown","5c883fba":"markdown","05ed4e5a":"markdown","4d482e7a":"markdown","ab209d98":"markdown","5011dc95":"markdown","d286ba38":"markdown","099734e1":"markdown","f0c71e8d":"markdown","12373087":"markdown","9c1e5edb":"markdown","c5c2e815":"markdown","6b2ffbf4":"markdown","7a51a8b9":"markdown","1e49ab93":"markdown","cf5e1cd7":"markdown","34d2754c":"markdown"},"source":{"81444730":"#Data manipulation libraries\nimport pandas as pd\nimport numpy as np\n\n#Visualizations\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"dark\")\nimport squarify\nimport matplotlib\n\n#for market basket analysis (using apriori)\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\n\n#for preprocessing\nfrom mlxtend.preprocessing import TransactionEncoder\n\n\n#to print all the interactive output without resorting to print, not only the last result.\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","08444cdb":"data = pd.read_csv('..\/input\/market.csv',header=None)","7faaf0c5":"#shape\ndata.shape","e0c70bb0":"#head of data\n\ndata.head()\n\n\n#tail of data\n\ndata.tail()","839a70b7":"#converting into required format of TransactionEncoder()\ntrans=[]\nfor i in range(0,7501):\n    trans.append([str(data.values[i,j]) for j in range(0,20)])\n\ntrans=np.array(trans)\n\nprint(trans.shape)","1fe90400":"### Using TransactionEncoder\n\nt=TransactionEncoder()\ndata=t.fit_transform(trans)\ndata=pd.DataFrame(data,columns=t.columns_,dtype=int)\n\ndata.shape","1d98a25e":"##here we also find nan as one of the columns so lets drop that column\n\ndata.drop('nan',axis=1,inplace=True)","cf96e44e":"#now lets check shape\ndata.shape\n\n#lets verify whether nan is present in columns\n'nan' in data.columns\n#so its proved that nan is not in columns ","a24cfa8c":"data.head()","346fec25":"##Lets consider the top 20 items purchased freequently\nr=data.sum(axis=0).sort_values(ascending=False)[:20]\n#altering the figsize\nplt.figure(figsize=(20,10))\ns=sns.barplot(x=r.index,y=r.values)\ns.set_xticklabels(s.get_xticklabels(), rotation=90)","59734a4c":"# create a color palette, mapped to these values\nmy_values=r.values\ncmap = matplotlib.cm.Blues\nmini=min(my_values)\nmaxi=max(my_values)\nnorm = matplotlib.colors.Normalize(vmin=mini, vmax=maxi)\ncolors = [cmap(norm(value)) for value in my_values]\n\n\n#treemap of top 20 frequent items\nplt.figure(figsize=(10,10))\nsquarify.plot(sizes=r.values, label=r.index, alpha=.7,color=colors)\nplt.title(\"Tree map of top 20 items\")\nplt.axis('off')\n","de020909":"#let us return items and ietmsets with atleast 5% support:\nfreq_items=apriori(data,min_support=0.05,use_colnames=True)","9ffff1cd":"freq_items","86f1d8ed":"#Now let's generate association rules\n\nres=association_rules(freq_items,metric=\"lift\",min_threshold=1.3)","600fe49b":"res","2f0a5797":"frequent_itemsets = apriori(data, min_support = 0.05, use_colnames=True)\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets","804a91a0":"# getting th item sets with length = 2 and support more han 10%\n\nfrequent_itemsets[ (frequent_itemsets['length'] == 2) &\n                   (frequent_itemsets['support'] >= 0.01) ]","ac562238":"# getting th item sets with length = 2 and support more han 10%\n\nfrequent_itemsets[ (frequent_itemsets['length'] == 1) &\n                   (frequent_itemsets['support'] >= 0.01) ]","82e793cb":"#Importing Libraries\n\nfrom mlxtend.frequent_patterns import fpgrowth","dd08123f":"#running the fpgrowth algorithm\nres=fpgrowth(data,min_support=0.05,use_colnames=True)","61c205ed":"res","adbda90a":"res=association_rules(res,metric=\"lift\",min_threshold=1)","1582071c":"res","138c8306":"import time\nl=[0.01,0.02,0.03,0.04,0.05]\nt=[]\nfor i in l:\n    t1=time.time()\n    apriori(data,min_support=i,use_colnames=True)\n    t2=time.time()\n    t.append((t2-t1)*1000)","f8a8c95c":"l=[0.01,0.02,0.03,0.04,0.05]\nf=[]\nfor i in l:\n    t1=time.time()\n    fpgrowth(data,min_support=i,use_colnames=True)\n    t2=time.time()\n    f.append((t2-t1)*1000)","42214a4b":"sns.lineplot(x=l,y=f,label=\"fpgrowth\")\nsns.lineplot(x=l,y=t,label=\"apriori\")\nplt.xlabel(\"Min_support Threshold\")\nplt.ylabel(\"Run Time in ms\")","73dfa53b":"We can find that mineral water is the most purchased item from the store, we may advice that mineral water must be always in the stock not only that mostly we can see from the above graph what 20 items are being frequently purchased.","d2399755":"Here we can find that data needs a lot of preprocessing.So let's preprocess the data.We use the TransactionEncoder() of mlxtend.preprocessing to do this work for us, if needed even we can implement the function i will provide the alternative as well.The TransactionEncoder() is an Encoder class for transaction data in Python list.It finds out what are all the different products in the transactions and will assign each transaction a list which contains a boolean array where each index represnts the corresponding product whether purchased in the transaction or not i.e. True or False.\n\nIt needs input as a python list of lists, where the outer list stores the n transactions and the inner list stores the items.\n\nIt returns the one-hot encoded boolean array of the input transactions, where the columns represent the unique items found in the input array in alphabetic order. \nFor further details you can refer its documentation:<a href=\"http:\/\/rasbt.github.io\/mlxtend\/user_guide\/preprocessing\/TransactionEncoder\/\">TransactionEncoder<\/a>\n\n\n","188d7e24":"## ECLAT ALGORITHM\n\n\nBottleNecks of Apriori:\n<ul>\n    <li>Candidate generation can result in huge candidate sets<\/li>\n    <li>Multiple Scans of Database--- needs (n+1) scans,n is the longest pattern<\/li>\n<\/ul>\n\nTo solve some of the above problems,Eclat has been introduced.\n\n\nThe ECLAT algorithm stands for Equivalence Class Clustering and bottom-up Lattice Traversal. It is one of the popular methods of Association Rule mining. It is a more efficient and scalable version of the Apriori algorithm. While the Apriori algorithm works in a horizontal sense imitating the Breadth-First Search of a graph, the ECLAT algorithm works in a vertical manner just like the Depth-First Search of a graph. This vertical approach of the ECLAT algorithm makes it a faster algorithm than the Apriori algorithm.\n\n\nHow it works:\n\nThe basic idea is to use Transaction Id Sets(tidsets) intersections to compute the support value of a candidate and avoiding the generation of subsets which do not exist in the prefix tree. In the first call of the function, all single items are used along with their tidsets. Then the function is called recursively and in each recursive call, each item-tidset pair is verified and combined with other item-tidset pairs. This process is continued until no candidate item-tidset pairs can be combined\n\n\nEg:\n\nt1={a,b,c}\nt2={a,b}\nt3={a}\n\nnow above is horizontal layout where t1,t2,t3 are transactions a,b,c are products.now let's make it into vertical layout....\n\n\nk=1,min_support=0.5\na={t1,t2,t3},sup=1\nb={t1,t2},sup=0.66\nc={t1},sup=0.33\n\n\nnow we eliminate c as is supp<min_support and them generate itemsets of length k=2\n\n{a,b}={t1,t2} supp=0.5\n\nand we can't generate anymore sets so we end up with only {a,b}.\n\n<b>This method has an advantage over Apriori as it does not require scanning the database to find the support of k+1 itemsets. This is because the Transaction set will carry the count of occurrence of each item in the transaction (support). The bottleneck comes when there are many transactions taking huge memory and computational time for intersecting the sets.<\/b>\n\nIf you want further reference you can visit : <a href=\"https:\/\/www.geeksforgeeks.org\/ml-eclat-algorithm\/\">Eclat Algo<\/a>\n\n\n<b>Advantages over Apriori algorithm:-<\/b>\n<ul>\n<li>Memory Requirements: Since the ECLAT algorithm uses a Depth-First Search approach, it uses less memory than Apriori algorithm.<\/li>\n    <li>Speed: The ECLAT algorithm is typically faster than the Apriori algorithm.<\/li>\n<li>Number of Computations: The ECLAT algorithm does not involve the repeated scanning of the data to compute the individual support values.<\/li>","e2f30aaa":"### Selecting and Filtering the Results","956cff2e":"Above we can see the 4 rules generated with lift greater than 1.3\n\nIntuition we can get is that:<pre>\n    22% of transactions containing mineral water also contain chocolate\n    32% of transactions containing chocolate also contain mineral water\n    34% of transactions containing spaghetti also contain mineral water\n    25% of transactions containing mineral water also contain spaghetti\n<\/pre>\n\n\nThere is more chance of the transaction {spaghetti,mineral water} than {chocolate,mineral water} as we can find the interesting nature of rule by comparing lift,leverage and conviction of {spaghetti,mineral water} and {chocolate,mineral water}.","7fb70fb5":"## FP GROWTH(Frequent Pattern Growth)\n\n","20de3d8c":"Yeah,bored with theory now let's implement the fp growth algorithm using mlxtend","5c883fba":"A rule is said to be interesting if it is unexpected(suprising to user) and\/or actionable(user can do something with it).It's a subjective measure.","05ed4e5a":"### Importing data ","4d482e7a":"Uffo...are you worried by looking at the Title ... Let's have a detailed explaination about what is <b>Market Basket Analysis<\/b>.Market basket analysis is generally done by the retailers to check the combination of two or more items that the customers are likely to buy. So let's see what is Market Basket and what does its analysis mean?","ab209d98":"This algorithm is an improvement to the Apriori method. A frequent pattern is generated without the need for candidate generation. FP growth algorithm represents the database in the form of a tree called a frequent pattern tree or FP tree.\n\n\nThis tree structure will maintain the association between the itemsets. The database is fragmented using one frequent item. This fragmented part is called \u201cpattern fragment\u201d. The itemsets of these fragmented patterns are analyzed. Thus with this method, the search for frequent itemsets is reduced comparatively.\n\n#### FP TREE\n\nFrequent Pattern Tree is a tree-like structure that is made with the initial itemsets of the database. The purpose of the FP tree is to mine the most frequent pattern. Each node of the FP tree represents an item of the itemset.\n\nThe root node represents null while the lower nodes represent the itemsets. The association of the nodes with the lower nodes that is the itemsets with the other itemsets are maintained while forming the tree.\n\n\n### Frequent Pattern Algorithm Steps\n\nThe frequent pattern growth method lets us find the frequent pattern without candidate generation.\n\nLet us see the steps followed to mine the frequent pattern using frequent pattern growth algorithm:\n\n1) The first step is to scan the database to find the occurrences of the itemsets in the database. This step is the same as the first step of Apriori. The count of 1-itemsets in the database is called support count or frequency of 1-itemset.\n\n2) The second step is to construct the FP tree. For this, create the root of the tree. The root is represented by null.\n\n3) The next step is to scan the database again and examine the transactions. Examine the first transaction and find out the itemset in it. The itemset with the max count is taken at the top, the next itemset with lower count and so on. It means that the branch of the tree is constructed with transaction itemsets in descending order of count.\n\n4) The next transaction in the database is examined. The itemsets are ordered in descending order of count. If any itemset of this transaction is already present in another branch (for example in the 1st transaction), then this transaction branch would share a common prefix to the root.\n\nThis means that the common itemset is linked to the new node of another itemset in this transaction.\n\n5) Also, the count of the itemset is incremented as it occurs in the transactions. Both the common node and new node count is increased by 1 as they are created and linked according to transactions.\n\n6) The next step is to mine the created FP Tree. For this, the lowest node is examined first along with the links of the lowest nodes. The lowest node represents the frequency pattern length 1. From this, traverse the path in the FP Tree. This path or paths are called a conditional pattern base.\n\nConditional pattern base is a sub-database consisting of prefix paths in the FP tree occurring with the lowest node (suffix).\n\n7) Construct a Conditional FP Tree, which is formed by a count of itemsets in the path. The itemsets meeting the threshold support are considered in the Conditional FP Tree.\n\n8) Frequent Patterns are generated from the Conditional FP Tree.\n\n\nFor example refer to <a href=\"https:\/\/www.softwaretestinghelp.com\/fp-growth-algorithm-data-mining\/\">link<\/a>","5011dc95":"Resources:\n<ul>\n    <li>https:\/\/webfocusinfocenter.informationbuilders.com\/wfappent\/TLs\/TL_rstat\/source\/marketbasket49.htm<\/li>\n    <li>https:\/\/www.hackerearth.com\/blog\/developers\/beginners-tutorial-apriori-algorithm-data-mining-r-implementation\/<\/li>\n    <li>https:\/\/www.newgenapps.com\/blog\/application-of-market-basket-analysis<\/li>\n    <li>http:\/\/rasbt.github.io\/mlxtend\/user_guide\/frequent_patterns\/association_rules\/<\/li>\n    \n    <li>https:\/\/en.wikipedia.org\/wiki\/Association_rule_learning<\/li>\n    <li>https:\/\/towardsdatascience.com\/association-rules-2-aa9a77241654<\/li>\n    <li>https:\/\/www.youtube.com\/watch?v=yCbankIouUU<\/li>\n    <\/ul>","d286ba38":"Now Lets do some Data Visualizations...\n\n\n### Data Visualizations\n","099734e1":"### Importing Libraries","f0c71e8d":"## Apriori Vs FP Growth\n\nSince FP-Growth doesn't require creating candidate sets explicitly, it can be magnitudes faster than the alternative Apriori algorithm. FP-Growth is about 5 times faster.Let's look at it.","12373087":"<img src=\"https:\/\/thumbor.forbes.com\/thumbor\/960x0\/https%3A%2F%2Fblogs-images.forbes.com%2Fmarciaturner%2Ffiles%2F2018%2F01%2FWegmans-Produce-1.jpg\" width=\"800px\">","9c1e5edb":"We can gain the required insights from the above graph about the run time comparision between the apriori and fpgrowth.","c5c2e815":"We could observe that {spaghetti}->{mineral water} is mostly like to occur as we can observe it from the lift.","6b2ffbf4":"## Market Basket Analysis","7a51a8b9":"Shortcomings Of Apriori Algorithm\n\n-Using Apriori needs a generation of candidate itemsets. These itemsets may be large in number if the itemset in the database is huge.\n\n\n-Apriori needs multiple scans of the database to check the support of each itemset generated and this leads to high costs.\nThese shortcomings can be overcome using the FP growth algorithm.","1e49ab93":"Let's now work on apriori \n","cf5e1cd7":"This tell us that there are 27 frequent itemsets of different lengths , so the first step of our apriori algorithm is finished","34d2754c":"### Market Basket\n\n\nThe market basket is a list of some fixed items that are used to track the inflation and overall price movements of a specific market in an economy. In other words, it is a basket that contains a set of standard goods or services that people commonly buys. \n\n\n### Market Basket Analysis\n\n\nMarket basket analysis is a method or technique of data analysis for retail and marketing purpose. Market basket analysis is done to understand the purchasing behavior of customers. MBA (Market Business Analysis) is used to uncover what items are frequently brought together by the customer. Market basket analysis leads to effective sales and marketing.Market basket analysis measures the co-occurrence of products and services. Market basket analysis is only considered when there is a transaction between two or more items. \n\n\n<b>Eg:<\/b>if a customer is buying bread then he is likely to buy butter, jam or milk to compliment bread. \n\n\n### Applications of Market Basket Analysis\n\n\nMarket basket analysis is applied to various fields of the retail sector in order to boost sales and generate revenue by identifying the needs of the customers and make purchase suggestions to them.\n\n\n<ul>\n    <li>Cross-selling is basically a sales technique in which seller suggests some related product to a customer after he buys a product.<\/li>\n    <li>Product Placement: It refers to placing the complimentary (pen and paper)and substitute goods (tea and coffee) together so that the customer addresses the goods and will buy both the goods together. <\/li>\n    <li>MBA has also been used in the field of healthcare for the detection of adverse drug reactions. It produces association rules that indicates what all combinations of medications and patient characteristics lead to ADRs.<\/li>\n    <li>Fraud Detection: Market basket analysis is also applied to fraud detection. It may be possible to identify purchase behavior that can associate with fraud on the basis of market basket analysis data that contain credit card usage<\/li>\n<\/ul>\n\n### How Market Based Analysis Works\n\n\nIn order to make it easier to understand, think of Market Basket Analysis in terms of shopping at a supermarket. Market Basket Analysis takes data at transaction level, which lists all items bought by a customer in a single purchase. The technique determines relationships of what products were purchased with which other product(s). These relationships are then used to build profiles containing If-Then rules of the items purchased.\n\n\n\nThe rules are written as :\n\n\n<b>if {A} then {B} i.e. {A} => {B}<\/b>\n\n\nThe If part of the rule (the {A} above) is known as the antecedent and the THEN part of the rule is known as the consequent (the {B} above). The antecedent is the condition and the consequent is the result. \n\n### Assosciation Rules\n    \nAssociation Rules are widely used to analyze retail basket or transaction data, and are intended to identify strong rules discovered in transaction data using measures of interestingness, based on the concept of strong rules.\n\n\nLet I={i1,i2,i3,\u2026,in} be a set of n attributes called items and D={t<sub>1<\/sub>,t<sub>2<\/sub>,\u2026,t<sub>n<\/sub>} be the set of transactions. It is called database. Every transaction, t<sub>i<\/sub> in D has a unique transaction ID, and it consists of a subset of itemsets in I.\n\nAssosciation rules are produced using algorithms like :\n<ul>\n    <li>Apriori Algorithm<\/li>\n    <li>Eclat Algorithm<\/li>\n    <li>FP-growth Algorithm<\/li>\n<\/ul>\n\n\nA rule can be defined as an implication, <b>X\u27f6Y<\/b> where X and Y are subsets of I(X,Y\u2286I), and they have no element in common. X and Y are the antecedent and the consequent of the rule, respectively.\n\n\nEg: {Bread,Egg}=> {Milk}    ItemSet={Bread,Egg,Milk}\n\nThere are various metrics in place to help us understand the strength of assosciation between antecedent and consequent:\n\n<ul>\n    <li>Support<\/li>\n    <li>Confidence<\/li>\n    <li>Lift or Correlation or interest<\/li>\n    <li>Leverage<\/li>\n    <li>Conviction<\/li>\n <\/ul>\n \n \n #### Support\n \n \n It gives an idea of how frequent an itemset is in all the transactions.To say in formal terms it's the fraction of total no. of transactions in which the itemset occurs.We refer to an itemset as a \"frequent itemset\" if you support is larger than a specified minimum-support threshold.\n \n     \n \\begin{equation*}\n supp(X->Y) =\\frac{(Transactions containing both X and Y)}{(Total No. of transactions)}\n\\end{equation*}\n \n <b>Range:[0,1]<\/b>\n Value of support helps us identifying the rules worth for future analysis.\n \n \n \n #### Confidence\n \n \n It defines the likelihood of occurence of consequent on the cart given that cart already has antecedent.It signifies the likelihood of item Y being purchased when item X is purchased.\n \n \n \\begin{equation*}\n confidence(X->Y) =\\frac{support(X->Y)}{support(X)}\n\\end{equation*}\n \n \n  <b>Range:[0,1]<\/b>\n \n If confidence is 0.75 then that imples that 75%of transactions containing X also contain Y .It can also be interpreted as the conditional probability P(Y|X), i.e, the probability of finding the itemset Y in transactions given the transaction already contains X.\n \n \nIt has a major drawback i.e. It only takes into account the popularity of the itemset X and not the popularity of Y. If Y is equally popular as X then there will be a higher probability that a transaction containing X will also contain Y thus increasing the confidence. To overcome this drawback there is another measure called lift.\n\n\n\n#### Lift\n\nLift gives the rise in the probability of having {Y} on the cart with the knowlede of {X} being present over the probability of having {Y} on the cart without knowlede about  presence of {X}.\n\n\n\\begin{equation*}\n Lift(X->Y) =\\frac{confidence(X->Y)}{support(Y)}\n\\end{equation*}\n\n  <b>Range:[0,Infinity]<\/b>\n  \n  It can simply be considered as correlation between the antecedent and consequent.If the value of lift is greater than 1, it means that the itemset Y is likely to be bought with itemset X, while a value less than 1 implies that itemset Y is unlikely to be bought if the itemset X is bought.\n  \n  \n#### Levarage or Piatetsky-Snapiro\n\nIt computes the difference between the observed frequency of X & Y appearing together and the frequency that we would expect if A and C are independent.\n\n\n\\begin{equation*}\n Leverage(X->Y) = support(X->Y) - support(X)*support(Y)\n\\end{equation*}\n\n\n<b>Range:[-1,1]<\/b>\n\n\nIf X,Y are positively correlated then we get leverage>0 ,we need such type of rules.<br>\nIf X,Y are negatively correlated then we get leverage<0.<br>\nIf X,y are independent , then we get leverage = 0.<br>\n\n\n#### Conviction\n\nIt can be interpreted as the ratio of the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions. \n\n\\begin{equation*}\n Conviction(X->Y) = \\frac{support(\\mathbf{Y})}{confidence(X->\\mathbf{Y})}\n\\end{equation*}\n <b>Please mark in the above equation Y means it is Y bar i.e. a bar on Y<\/b>\n \n\n<b>Range:[0,Infinity]<\/b>\n\n\nA high conviction value means that the consequent is highly depending on the antecedent. For instance, in the case of a perfect confidence score, the denominator becomes 0 (due to 1 - 1) for which the conviction score is defined as 'inf'. Similar to lift, if items are independent, the conviction is 1.\n\n\n## Apriori Algorithm\n\n\nApriori algorithm is a classical algorithm in data mining. It is used for mining frequent itemsets and relevant association rules. It is devised to operate on a database containing a lot of transactions, for instance, items brought by customers in a store.Association rule learning is a prominent and a well-explored method for determining relations among variables in large databases.\n\n\nRule - generation is a two step process. First is to generate frequent item set and second is to generate rules from the considered itemset.\n\n##### 1.Generating Frequent Itemset:\n\nOne approach to find the frequent itemsets  is to check all possible subsets of the given item set and check the support value of each itemset and consider only those that have support values greater than the minimum threshold support value.\n\n\nHere the Apriori uses the result of antimontone property of support and makes the generation of frequent Item set faster by reducing the search space.It has two principles:\n<ol>\n    <li>All subsets of a frequent itemset must be frequent<\/li>\n    <li>Similarly, for any infrequent itemset, all its supersets must be infrequent too<\/li>\n<\/ol>\n\nApriori principle allows us to prune all supersets of an itemset which does nogt satisfy the minimum threshold condition for support.For example if {Milk,Bread} does not satify our threshold value , then the superset of {Milk,Bread} will also not cross the threshold value there by we can just prune them away i.e. do not consider the itemsets that wil be generated from the {Milk,Bread}.\n\n\nTotally 3 major steps are involved here:\n\n<ol>\n    <li>Generate all frequent itemsets each satisfying the minimum threshold and having only one item let it be L<sub>1<\/sub>. Next use self join and generate all possible combinations of L<sub>1<\/sub> and now let the result be L<sub>2<\/sub>.<\/li>\n    <li>At each step as we keep on generating candidate itemsets, for each candidate we scan entire database so as to know its support and remove the candidates that do not satisfy minimum threshold<\/li>\n    <b>Here To reduce the no of comparisions, store the generated candidate items in a Hash Tree,Instead of matching each of candidate itemsets against each transaction,match each transaction with the candidates in hash tree(there by we can enhance the speed of apriori using this method)<\/b>\n    <br>\n    <li>In similar way create L<sub>k<\/sub> from L<sub>k-1<\/sub> until the point where we are unable to apply selfjoin.<\/li>\n    \n<\/ol>    \n    \n    \n   This approach of extending a frequent itemset one at a time is called the \u201cbottom up\u201d approach.\n    \n    \n<img src=\"https:\/\/blog-c7ff.kxcdn.com\/blog\/wp-content\/uploads\/2017\/03\/Apriori-Algorithm.jpg\" width=\"500px\">\n\n\n\n#### 2.Generating all possible rules from Frequent Itemsets\n\nIf n items are in set I , no of possible assosciation rules possible are 3<supn<\/sup>- 2<sup>n+1<\/sup> + 1.It becomes computationally expensive to generate all the rules and there is no meaning in genearting all that many no. of rules.\n\nSo apriori simplifies this approach by following some methodology,\n\n\nRules are formed by binary partition of each itemset.From a list of all possible candidate rules, we aim to identify rules that fall above the minimum confidence level.Just like antimontone property of support, confidence of rules generated from same itemset also follow the anti montone property. It's antimontone w.r.t no. of elements in consequent.\n\n=> CONF(A,B,C -> D) >=  CONF(B,C -> A,D) >= CONF(C -> A,B,D)\n\n\nOn the basis of this rules are generated.\n\n\nIf you want to refer further on Advanced Apriori Algorithms, Please refer to <a href=\"https:\/\/www.ijser.org\/paper\/Advanced-Apriori-Algorithms.html\">Advanced Apriori Algorithms.<\/a>\n\n\n\nApriori uses a breadth-first search strategy to count the support of itemsets and uses a candidate generation function which exploits the downward closure property of support.\n\nPros of the Apriori algorithm:\n<ul>\n    <li>It is an easy-to-implement and easy-to-understand algorithm.<\/li>\n    <li>It can be used on large itemsets.<\/li>\n\n\nCons of the Apriori Algorithm:\n<ul>\n<li>Sometimes, it may need to find a large number of candidate rules which can be computationally expensive.<\/li>\n<li>Calculating support is also expensive because it has to go through the entire database.<\/li>\n <\/ul>\n \n \nNow let's work on a dataset ..."}}