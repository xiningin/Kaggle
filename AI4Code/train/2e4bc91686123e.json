{"cell_type":{"7cc3d9d8":"code","f8624118":"code","d76a080c":"code","11e5017e":"code","29fd7bd2":"code","b554ccc8":"code","c5201994":"code","caba5332":"code","bb05929e":"code","ae848f08":"code","cb129f06":"code","aa6ac9c7":"code","5d330db0":"code","6002ada3":"code","e3b6f9dc":"code","c9b301ef":"code","35fa9a83":"code","66a9696b":"code","ee287b0c":"code","19975a58":"code","be865ad4":"code","2a420ec7":"code","97f7a8d1":"code","271e58ca":"code","f30d7272":"code","d342e0ea":"code","633e7527":"code","97d0d1ea":"code","be9a0561":"code","53e693c6":"code","ffb1613d":"code","945a86d5":"code","d95479f8":"code","4492d691":"code","44d3a463":"code","6301e7ad":"code","7ac6769a":"code","246d3401":"code","55126beb":"code","57340a8a":"code","b72689dd":"code","7bd88d2a":"code","ed3a2448":"code","2b09ac9b":"code","3e03a413":"code","2ce8ef01":"code","3e5c5f58":"code","fc17d289":"code","b2907f5a":"code","41f7f295":"markdown","fea0721e":"markdown","c6db088a":"markdown","99df50ab":"markdown","013489c6":"markdown","51981e1e":"markdown","ac06f71a":"markdown","e3964e42":"markdown","4074b837":"markdown","60476330":"markdown"},"source":{"7cc3d9d8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pdb # Python debugger\nfrom IPython.display import Image, display\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import export_graphviz\nfrom sklearn.impute import KNNImputer\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, learning_curve, train_test_split, GridSearchCV\n\nsns.set()\n\n%config InlineBackend.figure_format = 'retina' # Increase the figures' resolution in jupyter notebook\npd.set_option('display.max_colwidth', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","f8624118":"df_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_data = df_train.append(df_test).reset_index(drop=True)\ndf_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","d76a080c":"df_train.head()","11e5017e":"print('Training null values\\n')\nprint(df_train.isnull().sum()) \nprint('-'*30)\nprint('Testing null values\\n')\nprint(df_test.isnull().sum())","29fd7bd2":"print('Training info\\n')\nprint(df_train.info())\nprint('-'*30)\nprint('Testing info\\n')\nprint(df_test.info())","b554ccc8":"df_data.describe()","c5201994":"display(df_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean())","caba5332":"display(df_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean())\ndisplay(df_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())\ndisplay(df_data[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean())\ndisplay(df_data[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean())\ndisplay(df_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())","bb05929e":"sns.countplot(x=df_data['Embarked'], hue=df_data['Survived'])\n","ae848f08":"sns.countplot(x=df_data['Sex'], hue=df_data['Survived'])","cb129f06":"#mapping the sex feature\ndf_data['Sex#'] = df_data['Sex'].map({'male': 0, 'female': 1})","aa6ac9c7":"# Create new feature for Family\ndf_data['Fsize'] = df_data['Parch'] + df_data['SibSp'] + 1 #Family = Self + Sib + Parents\/Children","5d330db0":"# Create new feature for IsAlone\ndf_data['IsAlone']  = 0\ndf_data.loc[df_data.Fsize == 1, 'IsAlone'] = 1\n\n# Plot\nsns.countplot(x=df_data['IsAlone'], hue=df_data['Survived'])\nplt.show()\n\n#This looks like important data ","6002ada3":"# Making bins\ndf_data['Fare'].fillna(80.0, inplace = True)\ndf_data.isna().sum()\n\ndf_data['FareBin'] = pd.qcut(df_data['Fare'], 6)\n\n\n\n# Mapping the bins\nlabel_encoder = LabelEncoder()\ndf_data['FareBin'] = label_encoder.fit_transform(df_data['FareBin'])\n","e3b6f9dc":"# splits again beacuse we just engineered new feature\ndf_train = df_data[:len(df_train)]\ndf_test = df_data[len(df_train):]\n\n# Training set and labels\nx_train = df_train.drop(labels=['Survived','PassengerId'], axis=1)\ny_train = df_train['Survived']\n\n# show columns\nx_train.columns","c9b301ef":"#Extracting Family name by Regex - might help later\ndf_data['Fname'] = df_data['Name'].str.extract('([A-Za-z]+.[A-Za-z]+)\\,', expand=True)","35fa9a83":"# Assuming the cause of duplicate tickets is because the passengers knew each other. \n\nduplicates = []\n\nfor uniq in df_data['Ticket'].unique():\n    temp = df_data.loc[df_data['Ticket'] == uniq, 'Name']\n    if temp.count() > 1:\n        duplicates.append(df_data.loc[df_data['Ticket'] == uniq, ['Name', 'Ticket', 'Fare', 'FareBin', 'Fsize', 'Survived']])\nduplicates = pd.concat(duplicates)\nduplicates.head(20)","66a9696b":"df_friend = duplicates.loc[(duplicates.Fsize == 1) & (duplicates.Survived.notnull())]\ndf_family = duplicates.loc[(duplicates.Fsize > 1) & (duplicates.Survived.notnull())]\ndisplay(df_friend.head(), df_family.head())","ee287b0c":"print('The Duplicates: ', duplicates['Name'].count())\nprint('Family: ', df_family['Name'].count())\nprint('Friend: ', df_friend['Name'].count())\nprint('Other: ', duplicates['Name'].count() - df_family['Name'].count() - df_friend['Name'].count())","19975a58":"## Making a column for just Connected Survival\ndf_data['Connected_Survival'] = 0.5\n\nfor ticket_num, df_grp in df_data.groupby('Ticket'):\n    if len(df_grp) > 1: # Duplicates in Ticket\n            for index, row in df_grp.iterrows():\n                smax = df_grp.drop(index).Survived.max()\n                smin = df_grp.drop(index).Survived.min()\n                pid = row.PassengerId\n                if smax == 1.0:\n                    df_data.loc[df_data['PassengerId'] == pid, 'Connected_Survival'] = 1\n                elif smin == 0.0:\n                    df_data.loc[df_data['PassengerId'] == pid, 'Connected_Survival'] = 0","be865ad4":"# Embarked Filling by checking Fare\ndf_data[df_data['Embarked'].isnull()][['Embarked', 'Pclass', 'Fare']]","2a420ec7":"# Check their relation in groups\ndf_data.groupby(['Embarked', 'Pclass'])[['Fare']].median()","97f7a8d1":"#80 is closest to C1 - Assigning C and mapping\n\n# Filling missing values with the value that has greatest frequency\ndf_data['Embarked'] = df_data['Embarked'].fillna('C')\n\n# Mapping\ndf_data['Embarked#'] = df_data['Embarked'].map({'S': 1, 'C': 2, 'Q': 3})\ndf_data.head()","271e58ca":"## Extracting Titles from Name - might help in filling Age\ndf_data['Title'] = df_data['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\ndf_data['Title'] = df_data['Title'].replace(['Capt', 'Col', 'Rev', 'Don', 'Countess', 'Jonkheer', 'Dona', 'Sir', 'Dr', 'Major', 'Dr'], 'Rare')\ndf_data['Title'] = df_data['Title'].replace(['Mlle', 'Mme', 'Ms'], 'Miss')\ndf_data['Title'] = df_data['Title'].replace(['Lady'], 'Mrs')\ndf_data['Title'] = df_data['Title'].map({\"Mr\":0, \"Rare\" : 1, \"Master\" : 2,\"Miss\" : 3, \"Mrs\" : 4 })","f30d7272":"df_data.head()","d342e0ea":"display(df_data.Age.describe())\ndisplay(df_train['Age'].isna().sum())\ndisplay(df_test['Age'].isna().sum())","633e7527":"#By Title - \"Mr\":0, \"Rare\" : 1, \"Master\" : 2,\"Miss\" : 3, \"Mrs\" : 4 \ndf_data.groupby('Title')['Age'].median().values\n","97d0d1ea":"title_age = df_data.groupby('Title')['Age'].median().values\ndf_data['Age_pred1'] = df_data['Age']\nfor i in range(5):\n    df_data.loc[(df_data['Title'] == i) & (df_data['Age'].isnull()), 'Age_pred1'] = title_age[i]","be9a0561":"#By Linear Regression\nx_train = df_data[df_data.Age.notnull()]\ny_train = df_data[df_data.Age.notnull()]['Age']\nx_test = df_data[df_data.Age.isnull()]\n\n#select_feature = ['Sex#', 'Pclass', 'Title', 'FareBin','Embarked#', 'IsAlone']\nselect_feature = ['Sex#', 'Pclass', 'Title', 'FareBin']","53e693c6":"reg = LinearRegression()\nreg.fit(x_train[select_feature], y_train)\nreg.score(x_train[select_feature], y_train)","ffb1613d":"df_data['Age_pred3'] = df_data['Age']\ndf_data.loc[df_data['Age'].isnull(), 'Age_pred2'] = reg.predict(x_test[select_feature]).astype('int')","945a86d5":"xgb = XGBRegressor()\nxgb.fit(x_train[select_feature], y_train)\nxgb.score(x_train[select_feature], y_train)","d95479f8":"df_data['Age_pred3'] = df_data['Age']\ndf_data.loc[df_data['Age'].isnull(), 'Age_pred3'] = xgb.predict(x_test[select_feature]).astype('int')","4492d691":"#Higher survival rate for age <16, we put filter for predicting minors\n\ndf_data['Minor_pred1'] = ((df_data['Age_pred1']) < 16)*1\ndf_data['Minor_pred2'] = ((df_data['Age_pred2']) < 16)*1\ndf_data['Minor_pred3'] = ((df_data['Age_pred3']) < 16)*1\n","44d3a463":"# Bucketing Age like Fare\n\n\ndf_data['AgeBin_pred1'] = pd.qcut(df_data['Age_pred1'], 5)\n#df_data['AgeBin_pred2'] = pd.qcut(df_data['Age_pred2'], 5)\ndf_data['AgeBin_pred3'] = pd.qcut(df_data['Age_pred3'], 5)\n\ndf_data['AgeBin_pred1'] = label_encoder.fit_transform(df_data['AgeBin_pred1'])\n#df_data['AgeBin_pred2'] = label_encoder.fit_transform(df_data['AgeBin_pred2'])\ndf_data['AgeBin_pred3'] = label_encoder.fit_transform(df_data['AgeBin_pred3'])\n","6301e7ad":"## Assumption - What if a missing cabin value means that the passenger was not assigned a premium cabin?\n\ndf_data['Cabin'] = df_data['Cabin'].fillna(0)","7ac6769a":"def cabin(x):\n    try:\n        if x != 0:\n            return 1\n        else:\n            return 0\n    except:\n            return 0","246d3401":"df_data['Cabin'] = df_data['Cabin'].apply(cabin)","55126beb":"df_data[['PassengerId', 'Pclass', 'Sex#']] = df_data[['PassengerId', 'Pclass', 'Sex#']].astype('int32')\ndf_data.head()","57340a8a":"df_train = df_data[:len(df_train)]\ndf_test = df_data[len(df_train):]","b72689dd":"train_features = ['Survived', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Sex#', 'Fsize', 'IsAlone', 'FareBin', 'Connected_Survival', 'Embarked#', 'Age_pred1', 'Age_pred3', 'Minor_pred1', 'Minor_pred3', 'AgeBin_pred1', 'AgeBin_pred3', 'Cabin']","7bd88d2a":"corr_mat = df_train[train_features].astype(float).corr()\ncorr_mat_fil = corr_mat.loc[:, 'Survived'].sort_values(ascending=False)\ncorr_mat_fil = pd.DataFrame(data=corr_mat_fil[1:])","ed3a2448":"plt.figure(figsize=(20,12))\nbar = sns.barplot(x=corr_mat_fil.Survived.abs(), y=corr_mat_fil.index, data=corr_mat_fil, palette='deep')","2b09ac9b":"train_features = ['Survived', 'Sex#', 'Connected_Survival', 'FareBin', 'Minor_pred3', 'Embarked#', 'AgeBin_pred3', 'Parch', 'Age_pred3','IsAlone', 'Pclass', 'Cabin']\ncorr_mat = df_train[train_features].astype(float).corr()\n\nplt.figure(figsize=(20,10))\nsns.heatmap(corr_mat.abs(), annot=True)\nplt.show()","3e03a413":"#selected_features = ['Sex#', 'Pclass', 'FareBin', 'Connected_Survival', 'Minor_pred3', 'Embarked#', 'Cabin']\n#selected_features = ['Sex#', 'Pclass', 'FareBin', 'Connected_Survival', 'Cabin']\nselected_features = ['Sex#', 'Pclass', 'FareBin', 'Connected_Survival', 'Minor_pred3', 'Embarked#', 'IsAlone']\n\n\ndf_train = df_data[:len(df_train)]\ndf_test = df_data[len(df_train):]\n\nx_train = df_train[selected_features]\ny_train = df_train['Survived']\nx_test = df_test[selected_features]","2ce8ef01":"model = RandomForestClassifier(random_state=2)\n\ngrid_parameters = {'n_estimators': [i for i in range(300, 601, 50)], 'min_samples_split' : [10, 20, 30, 40]}\ngrid = GridSearchCV(estimator=model, param_grid=grid_parameters)\ngrid_result = grid.fit(x_train, y_train)\n\n# summarize results\nprint('Best: {} using {}'.format(grid_result.best_score_, grid_result.best_params_))","3e5c5f58":"n_estimator = grid_result.best_params_['n_estimators']\nmin_samples_split = grid_result.best_params_['min_samples_split']\n\nRFC = RandomForestClassifier(random_state=2, n_estimators=300, min_samples_split=40)\nRFC.fit(x_train, y_train)\ny_pred = RFC.predict(x_test)\n\noutput = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Survived': y_pred})\noutput = output.astype('int')\noutput.to_csv('predictionnocwisalone1.csv', index=False)\n#print('Your file was successfully saved!')","fc17d289":"selected_features = ['Sex#', 'Pclass', 'FareBin', 'Connected_Survival', 'Minor_pred3', 'Embarked#', 'Cabin']\n\n\ndf_train = df_data[:len(df_train)]\ndf_test = df_data[len(df_train):]\n\nx_train = df_train[selected_features]\ny_train = df_train['Survived']\nx_test = df_test[selected_features]","b2907f5a":"#xgbc = XGBClassifier(random_state=2)\n\n#xgbc.fit(x_train, y_train)\n#y_pred = xgbc.predict(x_test)\n\n#output = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Survived': y_pred})\n#output = output.astype('int')\n##print('Your file was successfully saved!')","41f7f295":"## Assumtions based on data analysis\n\n\n### Correlating.\n\n - Will use heatmaps and other visual methods to figure out which data correlates to survival the most.\n\n### Completing.\n\n- Fill Age value - A vital component \n- Embarked value \n- Cabin value - Has a lot of missing values and might not have high correlation. Potential Drop.\n\n### Correcting.\n\n- Name and ID have no correlation with survival and can be dropped for the model. \n- Some duplicates in the Ticket value is an interesting discovery. We can use this piece of information later on with a simple assumption - Passengers with same ticket values may be acquainted with each other.\n- Cabin can be dropped since we cannot assume that people with missing cabin entry were any different. \n\n### Creating.\n\n- We could build a feature known as Family - Using the columns Sibsp and Parch. \n- We could extract the title from Name \n- Turning the Age and Fair features into ordinal categorical features with Age Bands and Fair Bins\n\n### Classifying.\n\n- Some Assumptions: \n- Women (Sex=female) and Children (Age<16) had higher survival rate.\n- The upper-class passengers (Pclass=1) had higher survival rate.","fea0721e":"#### Filling Values of Age\n1. Linear Regression\/XGBRegressor\n2. Using Title\n3. Using Pclass and Sex","c6db088a":"## Feature Engineering and Data Cleaning","99df50ab":"## Exporatory Data Analysis (EDA)","013489c6":"## Building Models","51981e1e":"### Introduction\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nAim: Predicting the survival status of the passengers in test.csv","ac06f71a":"## XGBClassifier","e3964e42":"## Random Forest","4074b837":"# Titanic-Machine Learning from Disaster - Ishan Saksena","60476330":"## Some Observations from the Data\n\n### Features\n    PassengerId\n    Survived\n    Pclass (Ticket class)\n    Name\n    Sex\n    Age\n    SibSp (# of siblings \/ spouses aboard the Titanic)\n    Parch (# of parents \/ children aboard the Titanic)\n    Ticket\n    Fare\n    Cabin\n    Embarked (Port of Embarkation)\n    \n### Missing Values\n    Age and Cabin have a number of missing values, Embarked has some\n\n### Type\n    Categorical features: Survived, Sex, Embarked, and Pclass(ordinal).\n    Numercial features: Age, Fare. Discrete: SibSp, Parch.\n    \n### Distribution\n    Only Name has 100% unique values"}}