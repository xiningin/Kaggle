{"cell_type":{"bea85ed6":"code","fbc1b67f":"code","84a96efd":"code","4b440806":"code","594346f7":"code","42e0753d":"code","5d84ce58":"code","ca6b4cad":"code","4d1a00cd":"code","e97f0732":"code","2e2e5cbf":"code","6d653915":"code","77f2416b":"code","f17fff4a":"code","5b0823bc":"markdown","b7302736":"markdown","e405e830":"markdown","319e142e":"markdown","de19ae07":"markdown","d7aa0d6d":"markdown","29062a88":"markdown","41ab8574":"markdown","ab174e79":"markdown","9c46db73":"markdown"},"source":{"bea85ed6":"import os\nimport io\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport numpy as np\nimport pandas as pd\nimport cudf\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nfrom sklearn.preprocessing import MinMaxScaler","fbc1b67f":"%%time\ntrain = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv').set_index(\"id\")\ntest = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv').set_index(\"id\")\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")\n\nfeature_cols = test.columns.tolist()","84a96efd":"sc = MinMaxScaler()\ntrain[feature_cols] = sc.fit_transform(train[feature_cols])\ntest[feature_cols] = sc.transform(test[feature_cols])","4b440806":"train.hist(figsize=(20,15), grid=False, ylabelsize=5, xlabelsize=5, bins=30)\nplt.show()","594346f7":"dist1 = [ 'f1', 'f3', 'f5','f6','f7','f8', 'f10','f11', 'f13','f14', 'f15', 'f17','f18','f21','f22','f25','f26','f29','f34','f36','f37',\\\n 'f38','f40','f41','f43','f45', 'f46',  'f50','f54','f55','f57', 'f75','f76', 'f77','f80','f82','f85','f86','f91','f96','f97']\n\ndist2 = []\nfor col in feature_cols:\n    if col not in dist1:\n        dist2.append(col)","42e0753d":"%%time\nbins = 128\nn = 0\nbins_list = []\n\nbins_list.append(-np.inf)\nfor i in range(1,bins):\n    n += 1.\/bins\n    bins_list.append(n)\nbins_list.append(np.inf)\n\nlabels = [i for i in range(bins)]\nfor col in dist1:\n    train[col] = pd.cut(train[col], bins=bins_list, labels=labels)\n    test[col] = pd.cut(test[col], bins=bins_list, labels=labels)\n    \ntrain.head()","5d84ce58":"train[dist1] = train[dist1].astype('uint8')\ntest[dist1] = test[dist1].astype('uint8')","ca6b4cad":"x1 = train[dist1].values\nx2 = train[dist2].values\ny  = train['target'].values","4d1a00cd":"def get_model():\n    AF = \"relu\"\n    input_1 = layers.Input(shape=(x1.shape[-1]), name=\"continuous\")\n    x_1 = layers.Embedding(input_dim=bins, output_dim=4)(input_1)\n    x_1 = layers.TimeDistributed(layers.Dense(64, activation=AF))(x_1)\n    x_1 = layers.TimeDistributed(layers.Dense(64, activation=AF))(x_1)\n    x_1 = layers.Flatten()(x_1)\n    x_1 = layers.Dense(128, activation=AF)(x_1)\n    x_1 = layers.Dense(128, activation=AF)(x_1)\n    \n    input_2 = layers.Input(shape=x2.shape[-1], name=\"categories\")\n    x_2 = layers.Dense(128, activation=AF)(input_2)\n    x_2 = layers.Dense(128, activation=AF)(x_2)\n\n    x = layers.Concatenate()([x_1,x_2])\n    x = layers.Dense(64, activation=AF)(x)\n    x = layers.Dense(128, activation=AF)(x)\n    output = layers.Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n\n    model = tf.keras.Model([input_1,input_2], output)\n    return model\n\n\nmodel = get_model()\nmodel.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(), metrics=[\"AUC\"])\n    \ntf.keras.utils.plot_model(model, show_shapes=True)","e97f0732":"cb_es = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=4, mode=\"max\", restore_best_weights=True, verbose=1)\ncb_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", factor=0.5, patience=2, mode=\"max\", min_lr=0.0001, verbose=1)\n\nhistory = model.fit((x1,x2), \n                    y, \n                    epochs=40, \n                    validation_split=0.2, \n                    batch_size=512, \n                    validation_batch_size=512,\n                    callbacks=[cb_es, cb_lr])","2e2e5cbf":"e = model.layers[1]\nweights = e.get_weights()[0]\nprint(weights.shape)\n\nwords = [f\"{i} ({np.round(bins_list[i],3)}-{np.round(bins_list[i+1],3)})\" for i in labels]\n\nvecs = io.open('vecs.tsv', 'w', encoding='utf-8')\nmeta = io.open('meta.tsv', 'w', encoding='utf-8')\nfor i in range(bins):\n    vecs.write(words[i] + \"\\n\")\n    meta.write('\\t'.join([str(x) for x in weights[i]]) + \"\\n\")\nvecs.close()\nmeta.close()","6d653915":"preds = model.predict((test[dist1].values, test[dist2].values))","77f2416b":"plt.figure(figsize=(15,8))\nsns.histplot(x=preds.reshape(-1), kde=True, color=\"blue\")\nplt.title(\"Predictions Distribution\")\nplt.xlabel(\"Prediction\")\nplt.show()","f17fff4a":"sample_submission['target'] = np.squeeze(preds)\nsample_submission.to_csv(\"submission.csv\", index=False)","5b0823bc":"# Plot Features","b7302736":"# Scale Data","e405e830":"# Embeddings Projection","319e142e":"# Plot Predictions","de19ae07":"# Neural Network Model","d7aa0d6d":" You can uppload these two files (`vecs.tsv` and `meta.tsv`) on http:\/\/projector.tensorflow.org\/ to visualize embedding layer","29062a88":"# Submission","41ab8574":"# Categorize Data","ab174e79":"# Load Data","9c46db73":"# Predict"}}