{"cell_type":{"2d18a7e5":"code","c58c9c83":"code","fdd85130":"code","3dc18442":"code","b5d50c65":"code","65703d4d":"code","77515c22":"code","7fd48371":"code","052aebd7":"code","7f3eb423":"code","fa2421d4":"code","9fea305e":"code","bbe3ef4f":"code","aea97770":"code","7f1c04e2":"code","0426d8c0":"code","82ae69b4":"code","9e3463b6":"code","17782687":"code","b30eb966":"code","d29c6a4b":"code","05474eb2":"code","5f414515":"code","6d492cf0":"code","e2ff0183":"code","fbc71be3":"code","b0e57a6f":"markdown","da3fe31c":"markdown","021fba42":"markdown","71e0fe64":"markdown","81dfe02c":"markdown","083632be":"markdown","ab4ee2b5":"markdown","e6c14cee":"markdown","d808cb37":"markdown"},"source":{"2d18a7e5":"from distutils.dir_util import copy_tree\n\ncopy_tree(src = \"\/kaggle\/input\/m5-forecasting\/src\", \n          dst = \"\/kaggle\/working\/src\")\n","c58c9c83":"import os\nimport sys \nimport gc \nimport warnings \nimport random\nfrom pathlib import Path\n\nimport numpy as np\nimport torch \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsys.path.append(os.getcwd())\n\nseed = 0\nrandom.seed(seed)  \nnp.random.seed(seed)  \ntorch.manual_seed(seed) \n\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 5000)\n\n%matplotlib inline\nplt.tick_params(colors='white')\nsns.set_style(\"darkgrid\")\n\nwarnings.simplefilter('ignore', FutureWarning)\nwarnings.simplefilter('ignore', pd.core.common.SettingWithCopyWarning)\nwarnings.simplefilter('ignore', RuntimeWarning)\n","fdd85130":"from src.data.make_dataset import (\n    read_dataset, \n    get_date_cols, \n    merge_dataset\n)\nfrom src.features.build_dataset import (\n    TSdatasets, \n    setting_dataloader\n)\n\nfrom src.features.build_features import (\n    select_activate_items, \n    complement_missing,\n    add_base_features,\n    feature_enginearing\n)\nfrom src.models.eval_model import (\n    WRMSSEEvaluator, \n    root_mean_squared_error, \n    eval_quantity\n)\nfrom src.models.setting_model import (\n    AdaBound, \n    GradualWarmupScheduler\n)\nfrom src.models.architecture import TransformerModel\nfrom src.models.loss import RMSELoss\nfrom src.models.setting_model import setting_model\nfrom src.models.train_model import RecursiveModel\nfrom src.visualization.visualize import (\n    plot_sales,\n    plot_lr_and_sr,\n    plot_losses,\n    plot_eval, \n    plot_eval_per_group,\n    plot_prediction\n)\nfrom src.models.predict_model import (\n    output_inverse, \n    to_submission\n)\n","3dc18442":"private_sub = False","b5d50c65":"model_path = 'models\/transformer.model'\nvis_dir = 'reports\/figures'\nreports_dir = 'reports'","65703d4d":"# dataset\nN_IDS = 5000\n# N_IDS = 30490\nmax_train_size = 364 * 5\n\ntest_size = 28\nd_col = 'd'\n\n# features\ndiff_trans = False\n\npow_trans = False\nseason_diff_interval = 0\n# season_diff_interval = 28\nstd_trans = True\nminmax_trans = False\n\nbase_cols = [\n    'date', \n    'weekday', \n    'month', \n    'year', \n    'event_name_1', \n    'event_type_1', \n    'event_name_2', \n    'event_type_2', \n    'snap_CA', \n    'snap_TX', \n    'snap_WI'\n]\n\nnum_cols = [\n    'sell_price', \n    'is_snap'\n]\nsales_cat_cols = [\n    'id', \n    'item_id', \n    'dept_id', \n    'cat_id', \n    'store_id', \n    'state_id'\n]\ncat_cols = [\n    'quarter', \n    'is_weekend', \n    'part_of_month', \n    'event_name_1', \n    'event_type_1'\n]\n\nall_cat_cols = sales_cat_cols + cat_cols \n\n# if 0, onehot\ncat_emb = {\n    'id': 80,\n    'item_id': 30, \n    'dept_id': 0,\n    'cat_id': 0, \n    'store_id': 0, \n    'state_id': 0,\n    'quarter': 0,\n    'is_weekend': 0,\n    'part_of_month': 0,\n    'weekday': 3,\n    'month': 4,\n    'event_name_1': 10,\n    'event_type_1':0,\n    'event_name_2': 10,\n    'event_type_2':0\n}\n\n","77515c22":"# model\nepochs = 30\nbatch_size = 1024\/\/4\nclipping_value = 0.5\nlog_interval = 1\n\nbptt_x = 28 * 3\nbptt_y = test_size\nlags = [(1, 6), (7, 28 * 3)] # (tau, period)\n\nall_lags = [lag for tau, period in lags for lag in range(tau, period+1, tau)]\nt_emb = sum([lag[1] \/\/ lag[0] for lag in lags])\nmax_tau, max_lag  = lags[-1]\n\nscheduled_sampling = True\nsrc_mask = False\nmemory_mask = False\nnhid = 2048 \/\/ 2 # the dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 6 \/\/ 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 8 \/\/ 2 # the number of heads in the multiheadattention models\ndropout = 0.2 # the dropout value\n# fc_dims = [128, 64]\nfc_dims = []\nactivation = 'relu'\n\nlr = 1e-3 # learning rate\n\n","7fd48371":"sell_prices, sample_submission, calendar, sales_train = read_dataset(private_sub)","052aebd7":"level_cols, train_d_cols, test_d_cols, d2F_map = get_date_cols(\n    sales_train, \n    sample_submission, \n    max_train_size, \n    max_lag, \n    private_sub=private_sub\n)\n    ","7f3eb423":"sales_train = sales_train.iloc[:N_IDS, :]\n\nX = merge_dataset(\n    sales_train, \n    calendar, \n    sell_prices, \n    base_cols,\n    level_cols,\n    train_d_cols,\n    test_d_cols\n)\n\n","fa2421d4":"X = select_activate_items(X, train_d_cols, test_size, bptt_x, max_lag)\nX = complement_missing(X)\nX = add_base_features(X)\n","9fea305e":"plot_sales(\n    X.groupby('d')['sales'].sum(), \n    X['sales'], \n    train_d_cols\n)","bbe3ef4f":"X, all_cat_cols, all_num_cols, list_dtrans, id_enc = feature_enginearing(\n    X, \n    train_d_cols[max_lag+1:-test_size], \n    test_d_cols, \n    lags, \n    max_lag,\n    sales_cat_cols,\n    cat_cols,\n    num_cols, \n    diff_trans,\n    dtrans_map={'sales':[pow_trans, std_trans, minmax_trans, season_diff_interval], \n                'sell_price':[False, True, False, 0]}, \n    clipping_range={'sales': (0.0, 1.0), \n                    'sell_price': (0.0, 1.0)}\n)\n\ntrainloader, validloader, validmaskloader, testloader = setting_dataloader(\n    X, \n    train_d_cols,\n    test_d_cols,\n    all_num_cols,\n    all_cat_cols,\n    bptt_x,\n    bptt_y,\n    max_lag,\n    test_size,\n    batch_size,\n)\n","aea97770":"plot_sales(\n    X.groupby(['id', 'd'])['sales'].sum().reset_index().pivot(\n        index='id', columns='d', values='sales'\n    ).sum(0), \n    X['sales'], \n    train_d_cols\n)\n","7f1c04e2":"model_params = dict(\n    src_seq_len=bptt_x,\n    d_model=len(all_num_cols),\n    nhead=nhead,\n    nhid=nhid,\n    nlayers=nlayers,\n    dropout=dropout,\n    fc_dims=fc_dims,\n    activation=activation,\n    use_src_mask=src_mask,\n    use_memory_mask=memory_mask,\n)\n\nopt_params = dict(lr=lr, weight_decay=1e-4, amsgrad=False)\n\nlr_params = dict(T_max=epochs - (epochs \/\/ 10), eta_min=1e-5)\n\nwarmup_params = dict(multiplier=1, total_epoch=epochs \/\/ 10)\n\nsr_params = dict(\n    decay_schedules=\"inverse_sigmoid_decay\",\n    k=epochs,\n    start=1.0,\n    end=0.01,\n    slope=0.3,\n)\n","0426d8c0":"_, optimizer, _, lr_scheduler, sr_scheduler = setting_model(\n    X,\n    cat_emb,\n    all_cat_cols,\n    all_num_cols,\n    model_params,\n    opt_params,\n    lr_params,\n    warmup_params,\n    sr_params,\n)\n\nplot_lr_and_sr(epochs, optimizer, lr_scheduler, sr_scheduler)","82ae69b4":"model = RecursiveModel(bptt_y, all_lags)\nmodel.setting_model(\n    *setting_model(\n        X,\n        cat_emb,\n        all_cat_cols,\n        all_num_cols,\n        model_params,\n        opt_params,\n        lr_params,\n        warmup_params,\n        sr_params,\n    )\n)","9e3463b6":"model.model","17782687":"print('Training...')\nlosses = model.train(model_path, trainloader, validloader, epochs)\n\ndel trainloader\ngc.collect()\n\nplot_losses(model, losses)","b30eb966":"pred_idx = testloader.dataset.en_cat_i[:, 0, 0].argsort()\ntrain_pred_d_cols = train_d_cols[-test_size*2:-test_size]\nvalid_pred_d_cols = train_d_cols[-test_size:]\n\ntrainmaskset = TSdatasets(\n    X, \n    train_d_cols[-((test_size * 2) + max_lag):-test_size], \n    all_num_cols, \n    all_cat_cols, \n    bptt_x, \n    bptt_y, \n    max_lag,\n    mask=True\n)\ntrainmaskloader = torch.utils.data.DataLoader(\n    trainmaskset, batch_size=batch_size, shuffle=False\n)\n\nprint('Predicting...')\n\noutput_train = output_inverse(\n    model.predict(trainmaskloader)[pred_idx], \n    list_dtrans[0], \n    id_enc.classes_, \n    train_pred_d_cols,\n    diff_trans,\n    sales_train.loc[\n        :N_IDS, train_d_cols[-test_size*2-1]].values,    \n)\n\noutput_valid = output_inverse(\n    model.predict(validmaskloader)[pred_idx], \n    list_dtrans[0], \n    id_enc.classes_, \n    valid_pred_d_cols,\n    diff_trans,\n    sales_train.loc[\n        :N_IDS, train_d_cols[-test_size-1]].values\n)\n\noutput_test = output_inverse(\n    model.predict(testloader)[pred_idx], \n    list_dtrans[0], \n    id_enc.classes_, \n    test_d_cols,\n    diff_trans,\n    sales_train.loc[\n        :N_IDS, train_d_cols[-1]].values\n)\n\n\noutput_train = pd.concat(\n    [sales_train[level_cols].set_index('id'), output_train], \n    axis=1\n)\n\noutput_valid = pd.concat(\n    [sales_train[level_cols].set_index('id'), output_valid], \n    axis=1\n)\n\ntrue_train = sales_train[level_cols + train_pred_d_cols[-test_size:]].set_index('id')\ntrue_valid = sales_train[level_cols + valid_pred_d_cols].set_index('id')\n","d29c6a4b":"plot_eval(\n    Path(vis_dir, 'eval_train'), \n    output_train[train_pred_d_cols[-test_size:]], \n    true_train[train_pred_d_cols[-test_size:]]\n)\nplot_eval(\n    Path(vis_dir, 'eval_valid'), \n    output_valid[valid_pred_d_cols], \n    true_valid[valid_pred_d_cols]\n)","05474eb2":"plot_eval_per_group(\n    Path(vis_dir, 'eval_valid'), output_valid, true_valid, 'dept_id', valid_pred_d_cols\n)\nplot_eval_per_group(\n    Path(vis_dir, 'eval_valid'), output_valid, true_valid, 'store_id', valid_pred_d_cols\n)","5f414515":"plot_prediction(\n    Path(vis_dir, 'pred_test'), \n    sales_train.loc[: N_IDS - 1, train_d_cols], \n    output_test\n)","6d492cf0":"eval_quantity(\n    Path(reports_dir, 'eval_results.json'),\n    output_valid,\n    sales_train,\n    calendar,\n    sell_prices,\n    valid_pred_d_cols,\n)\n","e2ff0183":"my_submission = to_submission(\n    output_test,\n    sales_train,\n    sample_submission,\n    test_d_cols,\n    d2F_map,\n    private_sub=private_sub,\n\n)\n\nmy_submission.to_csv(\"submission.csv\")","fbc71be3":"my_submission","b0e57a6f":"# Config","da3fe31c":"# Evaluation","021fba42":"# Read","71e0fe64":"# upload modules","81dfe02c":"# Submission","083632be":"# import modules","ab4ee2b5":"# Modeling","e6c14cee":"# Feature engineering","d808cb37":"# Transformer\u306b\u3088\u308b\u6642\u7cfb\u5217\u4e88\u6e2c"}}