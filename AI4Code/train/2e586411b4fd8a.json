{"cell_type":{"60381029":"code","9f92cd85":"code","0f972e77":"code","14cc12c7":"code","fcf21d59":"code","5043b992":"code","fff8cb61":"code","faea3cd3":"code","4b926126":"code","25fb2795":"code","b8624afd":"code","a223c9f1":"code","0e088c0f":"code","96b71f6a":"code","d0bfef06":"code","fd62b6ec":"code","8e539890":"code","b6804d33":"code","d7175c58":"code","27ef1e5b":"code","adac9381":"code","d7a97f8a":"code","dfbf8e8a":"code","704c499a":"code","a7d64d9c":"code","8d2f3b12":"code","54eca70f":"code","dd9d8188":"code","0345ea64":"code","2264f1c1":"code","22451b41":"code","705b2e82":"code","531ec1c7":"code","53de1075":"code","6d25424d":"code","fa814765":"code","7a316469":"code","86e344d8":"code","bcb57722":"code","9f7489b2":"markdown","cca00d24":"markdown","5a4e1052":"markdown","99bc730f":"markdown","c11b4d8d":"markdown","90d3626d":"markdown","74621bac":"markdown","64d591d2":"markdown","e9103e32":"markdown","83008ac5":"markdown","e0a9ff92":"markdown"},"source":{"60381029":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score\n\nfrom fastai import *\nfrom fastai.vision import *\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport cv2\n\nfrom tqdm import tqdm\nfrom skmultilearn.model_selection import iterative_train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%load_ext autoreload\n%autoreload","9f92cd85":"model_path='.'\npath='..\/input\/'\ntrain_folder=f'{path}train'\ntest_folder=f'{path}test'\ntrain_lbl=f'{path}train_labels.csv'\nORG_SIZE=96\n\nbs=64\nnum_workers=None # Apprently 2 cpus per kaggle node, so 4 threads I think\nsz=96","0f972e77":"df_trn=pd.read_csv(train_lbl)","14cc12c7":"tfms = get_transforms(do_flip=True, flip_vert=True, max_rotate=.0, max_zoom=.1,\n                      max_lighting=0.05, max_warp=0.)","fcf21d59":"data = ImageDataBunch.from_csv(path,csv_labels=train_lbl,folder='train', ds_tfms=tfms, size=sz, suffix='.tif',test=test_folder,bs=bs);\nstats=data.batch_stats()        \ndata.normalize(stats)","5043b992":"data.show_batch(rows=5, figsize=(12,9))","fff8cb61":"from sklearn.metrics import roc_auc_score","faea3cd3":"def auc_score(y_pred,y_true,tens=True):\n    score=roc_auc_score(y_true,torch.sigmoid(y_pred)[:,1])\n    if tens:\n        score=tensor(score)\n    else:\n        score=score\n    return score","4b926126":"from torchvision.models import *","25fb2795":"learn = create_cnn(\n    data,\n    densenet201,\n    path='.',    \n    metrics=[auc_score], \n    ps=0.5\n)","b8624afd":"#print(learn.summary())","a223c9f1":"learn.lr_find()\nlearn.recorder.plot()","0e088c0f":"lr = 1e-04","96b71f6a":"learn.fit_one_cycle(1,lr)\nlearn.recorder.plot()\nlearn.recorder.plot_losses()","d0bfef06":"learn.unfreeze()\nlearn.lr_find()","fd62b6ec":"learn.recorder.plot()","8e539890":"learn.fit_one_cycle(1,slice(1e-4,1e-3)) #10","b6804d33":"learn.recorder.plot()","d7175c58":"learn.recorder.plot_losses()","27ef1e5b":"m = learn.model.eval()","adac9381":"idx = 4","d7a97f8a":"x,y = data.valid_ds[idx]\nx.show()\ndata.valid_ds.y[idx]","dfbf8e8a":"xb,_ = data.one_item(x) #takes all the settings from our previously created data object\nxb_im = Image(data.denorm(xb)[0])\nxb = xb.cuda()","704c499a":"from fastai.callbacks.hooks import *","a7d64d9c":"def hooked_backward(cat=y):\n    with hook_output(m[0]) as hook_a: #Get activations from the convolution layers\n        with hook_output(m[0], grad = True) as hook_g: #Get gradient from convolution layers\n            preds = m(xb) #DO foreward pass through model\n            preds[0,int(cat)].backward()\n    return hook_a, hook_g\n","8d2f3b12":"hook_a, hook_g = hooked_backward()","54eca70f":"#hook_a.stored","dd9d8188":"acts = hook_a.stored[0].cpu()\nacts.shape #Now we see our 512 chanels over the 11x11 sections of the image","0345ea64":"avg_acts = acts.mean(0)\navg_acts.shape","2264f1c1":"def show_heatmap(hm):\n    _,ax = plt.subplots()\n    xb_im.show(ax) #fastai function to show the image\n    ax.imshow(hm, alpha = 0.6, extent = (0,92,92, 0), #extent expands the 11x11 image to 352,352\n             interpolation = 'bilinear', cmap = 'magma')","22451b41":"show_heatmap(avg_acts)","705b2e82":"preds,y=learn.get_preds()\npred_score=auc_score(preds,y)\npred_score","531ec1c7":"preds,y=learn.TTA()\npred_score_tta=auc_score(preds,y)\npred_score_tta","53de1075":"preds_test,y_test=learn.get_preds(ds_type=DatasetType.Test)","6d25424d":"preds_test_tta,y_test_tta=learn.TTA(ds_type=DatasetType.Test)","fa814765":"sub=pd.read_csv(f'{path}\/sample_submission.csv').set_index('id')\nsub.head()","7a316469":"clean_fname=np.vectorize(lambda fname: str(fname).split('\/')[-1].split('.')[0])\nfname_cleaned=clean_fname(data.test_ds.items)\nfname_cleaned=fname_cleaned.astype(str)","86e344d8":"sub.loc[fname_cleaned,'label']=to_np(preds_test[:,1])\nsub.to_csv(f'submission_{pred_score}.csv')","bcb57722":"sub.loc[fname_cleaned,'label']=to_np(preds_test_tta[:,1])\nsub.to_csv(f'submission_{pred_score_tta}.csv')","9f7489b2":"For now I only did standard stuff and used the new suggested learning rate methods of the 1 cycle learning policy as described here:\n    \n1. [blog post by Sylvain Gugger summarizing the following papers](https:\/\/sgugger.github.io\/the-1cycle-policy.html#the-1cycle-policy)\n2. [original papers by leslie smith on hyperparameter tuning](https:\/\/arxiv.org\/pdf\/1803.09820.pdf) \n3. [ and Superconvergence, the 1 cycle policy learning](https:\/\/arxiv.org\/pdf\/1708.07120.pdf)\n\nNext things I planned would be to properly crop the images so it only includes the 32x32 sized patch that is the important part of the image, and check the augmentation settings.\nI already set up some functionality to use hyperopt to optimiize the hyperparameters of the one cycle parameters, this will come in another kernel.\nI also still need to check how many augmentation in the TTA  as used [here](https:\/\/towardsdatascience.com\/augmentation-for-image-classification-24ffcbc38833)\n\nI would also like check how the accuracy (or in this case ROC-AUC) changes with the different resnet18\/34\/50","cca00d24":"# Heatmap","5a4e1052":"In Case I want to run quick tests use a subsample:","99bc730f":"Defining a metric so after epoch I get the validation ROC-AUC score","c11b4d8d":"## I add the score to the name of the file so I can later plot the leaderboard score versus my validation score\nIn the fastai course Jeremy mentions that if you have a monotonic relation between validation and LB score the way you set up your validation set matches what the test set consists of.","90d3626d":"### Predit the validation data using TTA\nHere for every image we want to predict on, n_augs images are augmented form the original image.\nWe can then compare the predictions on for example the image and the image flipped \/ roated \/ slightly different crop\/ lighting\/stretched etc. \nFor now only the diherdral and rotations are used. THis gives a nice extra percent or two when compared to the auc above after training where not TTA is used. \nI also test if mean or max is better to use on the image and its augments but it can't conclude anything yet.","74621bac":"### Now predict on test set","64d591d2":"https:\/\/www.kaggle.com\/cuberti\/fastai-dl1-2019-lesson-6","e9103e32":"Sometimes its important in which order the ids in the submissions are so to make sure I don't mess up I put them in the same order. My first submission had a 50% score so I somewhere messed up the order oder the matching of id to label.\nsince fname_clean is the id we can just use that as index when adding the correct label in our dataframe. ","83008ac5":"### Warm up with frozen weight is done on a subset so we dont have to waste an entire epoch","e0a9ff92":"### prepare submission\nI now load in the sample submission and put my predictions in the label column and save to a new file."}}