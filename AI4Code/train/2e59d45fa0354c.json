{"cell_type":{"bf7daefe":"code","9f5c45e2":"code","417aaea6":"code","30515b83":"code","e7f47022":"code","ecda1ac8":"code","be8739e4":"code","ed0b31c0":"code","bfd5a0d0":"code","0afba429":"code","b9408891":"code","b91a3109":"code","2b0544d3":"code","d7b4ba76":"code","989badb6":"code","4fd343de":"code","1a86eb4a":"code","58046418":"code","af75b64c":"code","6831fcf3":"code","42ef21cd":"code","52c5c429":"code","f14b3591":"code","c1cc1f47":"code","79157174":"code","c416413f":"code","342277af":"code","20a74f35":"code","5a5fdab3":"code","53ac0e76":"markdown","092871cf":"markdown","6bce9fae":"markdown","0706fe50":"markdown","31111108":"markdown","866cbf7b":"markdown","06f35420":"markdown","3e01148a":"markdown","ff355b26":"markdown","80496f3c":"markdown","7a359de3":"markdown","e1c1d188":"markdown","c4af49e4":"markdown","c7810b65":"markdown","505e9d79":"markdown","86ec80f3":"markdown","71a0b0b4":"markdown","551de3ed":"markdown","9c622946":"markdown","c0dda04b":"markdown"},"source":{"bf7daefe":"import numpy as np\nimport pandas as pd\nimport os \nfrom collections import Counter\nfrom tqdm import tqdm\nfrom IPython.display import display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\ndata_path = \"..\/input\/lish-moa\"\n!ls -l --block-size=M $data_path","9f5c45e2":"%%time\ntrain_features = pd.read_csv(os.path.join(data_path, \"train_features.csv\"))\ntrain_targets_s= pd.read_csv(os.path.join(data_path, \"train_targets_scored.csv\"))\ntrain_targets_n= pd.read_csv(os.path.join(data_path, \"train_targets_nonscored.csv\"))\ntest_features  = pd.read_csv(os.path.join(data_path, \"test_features.csv\"))\nsample_submission = pd.read_csv(os.path.join(data_path, \"sample_submission.csv\"))\n\ntrain_features.set_index('sig_id', inplace=True)\ntrain_targets_s.set_index('sig_id', inplace=True)\ntest_features.set_index('sig_id', inplace=True)\nsample_submission.set_index('sig_id', inplace=True)\n\nassert np.all(test_features.index == sample_submission.index)","417aaea6":"print(f\"train shape {train_features.shape}\")\nprint(f\"test  shape {test_features.shape}\")\nprint(f\"train targets scored shape {train_targets_s.shape}\")\nprint(f\"train targets nonscored shape {train_targets_n.shape}\")","30515b83":"# features \nfeatures = list(train_features.columns)\nshort_features = Counter([f[:2] for f in features])\nshort_features","e7f47022":"train_targets_s.sum(axis=1).value_counts().sort_index().plot(kind=\"bar\", title=\"targets per record\", figsize=(7,4));","ecda1ac8":"# detect pairs \"agonist-antagonist\"\npairs = Counter([t.replace(\"_antagonist\", \"\").replace(\"_agonist\", \"\") for t in train_targets_s.columns])\npairs = [t for t, count in list(pairs.items()) if count == 2]\n\n# check if pairs \"agonist-antagonist\" is crossing\ndef filter_columns(df, s):\n    columns = [c for c in df.columns if s in c]\n    return df[columns]\n\nfor p in pairs:\n    df = filter_columns(train_targets_s, p)\n    boolean = np.all(df.sum(axis=1) < 2)\n    if not boolean:\n        print(p, len(df[df.sum(axis=1) == 2]))","be8739e4":"train_targets_s.mean(axis=0)[1:].plot(kind=\"hist\", bins=50, title=\"classes distribution\", figsize=(8,5));","ed0b31c0":"display(train_features.cp_type.value_counts(normalize=True))\n\ntrain_features['targets_total'] = train_targets_s.sum(axis=1)\ntrain_features.groupby('cp_type')['targets_total'].sum()","bfd5a0d0":"display(train_features.groupby('cp_time')['targets_total'].sum())\ntrain_features.groupby('cp_dose')['targets_total'].sum()","0afba429":"# 1. Features are continuous\nassert np.all(filter_columns(train_features, \"g-\").dtypes == \"float64\")\nassert np.all(filter_columns(train_features, \"c-\").dtypes == \"float64\")\n# 2. Gene features belong range [-10, 10]\nassert filter_columns(train_features, \"g-\").max().max() == 10\nassert filter_columns(train_features, \"g-\").min().min() == -10\n# 3. Cell features belong range [-10, <10]\nassert filter_columns(train_features, \"c-\").max().max() < 10\nassert filter_columns(train_features, \"c-\").min().min() == -10","b9408891":"# means distribution\nsns.distplot(filter_columns(train_features, \"c-\").mean(), label=\"cell\")\nsns.distplot(filter_columns(train_features, \"g-\").mean(), label=\"gene\")\nplt.legend();","b91a3109":"def distplots(df, columns, nrows=1, ncols=5, title=\"\", figsize=(15,3)):\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n    axes = axes.flatten()\n    fig.suptitle(title)\n    for i, column in enumerate(columns):\n        sns.distplot(df[column], ax=axes[i])\n        axes[i].axes.yaxis.set_visible(False)\n    # sns.despine(left=True)\n    plt.show()\n\ncolumns = filter_columns(train_features, \"c-\").iloc[:, [0,10,20,30,42,45,77]].columns.tolist()\ndistplots(train_features, columns, ncols=len(columns), figsize=(len(columns)*3, 3), title=\"Cell features example\")\ncolumns = filter_columns(train_features, \"g-\").iloc[:, [0,12,42,101,202,303,424]].columns.tolist()\ndistplots(train_features, columns, ncols=len(columns), figsize=(len(columns)*3, 3), title=\"Gene features example\")","2b0544d3":"def columnwise_corrcoef(m, v):\n    \"\"\" \n    Vectorized columnwise pearson correlation\n    Means of all columns should be equal 0\n    m: matrix \n    v: vector\n    \"\"\"\n    return (v @ m) \/ np.sqrt(np.sum(m ** 2, 0) * np.sum(v ** 2))\n\ncorr_features = train_features.columns[3:]\ndf_corr = train_features.copy().loc[train_features.cp_type=='trt_cp', corr_features].astype(np.float64)\ndf_corr = df_corr - df_corr.mean()\ndf_corr = df_corr.values","d7b4ba76":"corr = np.eye(len(corr_features), dtype=np.float32)\nn = len(corr_features)\nfor i in tqdm(range(n)):\n    corr[i] = columnwise_corrcoef(df_corr, df_corr[:, i])\nassert np.all(np.diag(corr) == 1)\nassert np.min(corr) >= -1\nassert np.max(corr) <= 1\nnp.fill_diagonal(corr, 0)","989badb6":"sns.distplot(corr[:, -1], bins=40)\nsns.despine()\nplt.xlim(-0.5, 0.5)\nplt.title(\"target correlation\");","4fd343de":"# help(sns.heatmap)\nplt.figure(figsize=(20, 15))\nsns.heatmap(corr, \n            xticklabels=[],\n            yticklabels=[],\n            cmap=\"cividis\"\n           );","1a86eb4a":"cell_corr = corr[-100:, -100:].copy()\nnp.fill_diagonal(cell_corr, 1)\nprint(\"cell features R =\", cell_corr.mean())","58046418":"targets_sum = train_targets_s.sum(axis=0)\nrare_targets = targets_sum[targets_sum < 5].index.tolist()\ntrain_targets_s.drop(rare_targets, axis=1, inplace=True)\nrare_targets, train_targets_s.shape","af75b64c":"train_features.drop('targets_total', axis=1, inplace=True) # remove targets from train set","6831fcf3":"assert np.all(train_features.index == train_targets_s.index)\ntest_features['cp_type'].value_counts(normalize=True)","42ef21cd":"# onehot encoding experiment features\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse=False)\ncategorical_features = ['cp_type', 'cp_time', 'cp_dose']\ncategories = encoder.fit_transform(train_features[categorical_features])\ncategories = categories[:, 1:] # keeps only 1 column for cp_type\n\ndef join_columns(c1,c2,sep='__'):\n    return sep.join([str(c1),str(c2)])\noh_features = [join_columns(f, v) for f, c in zip(categorical_features, encoder.categories_) for v in c][1:]\n\ntrain_features.drop(categorical_features, axis=1, inplace=True)\ntrain_features = pd.concat([\n    pd.DataFrame(categories, index=train_features.index, columns=oh_features), \n    train_features\n], axis=1)","52c5c429":"# make holdout fold\nfrom sklearn.model_selection import train_test_split\nX_train, X_hold, y_train, y_hold = train_test_split(\n    train_features, train_targets_s, \n    test_size=0.25, \n    random_state=171, shuffle=True\n)\nassert y_hold.loc[:, y_hold.sum(axis=0)<1].shape[1] == 0\n\nctype_train, ctype_hold = X_train.iloc[:, 0].values, X_hold.iloc[:, 0].values\nctype_train= ctype_train.astype(bool)\nctype_hold = ctype_hold.astype(bool)\nassert y_hold[~ctype_hold].sum().sum() == 0\n\ny_hold = y_hold.values","f14b3591":"X_train.drop('cp_type__trt_cp', axis=1, inplace=True)\nX_hold.drop('cp_type__trt_cp', axis=1, inplace=True)","c1cc1f47":"# metrics \nfrom sklearn.metrics import log_loss\n\ndef sklearn_multilabel_logloss(y_true, y_pred):\n    \"\"\" Sanity check \"\"\"\n    return np.mean([log_loss(y_true[:, i], y_pred[:, i]) for i in range(y_pred.shape[1])])\n\ndef logloss(y_true, y_pred, eps=1e-15):\n    y_pred[y_pred==0] = eps\n    y_pred[y_pred==1] = 1 - eps\n    return - (y_true * np.log(y_pred) + (1 - y_true) * (np.log(1 - y_pred))).mean()","79157174":"for c in np.arange(0.1, 0.51, 0.2):\n    y_pred = np.full_like(y_hold, c, dtype=np.float64)\n    y_pred[~ctype_hold] = 0\n    print(f\"constant={c:.1f} logloss={logloss(y_hold, y_pred):.5f}\")\n\nassert np.allclose(sklearn_multilabel_logloss(y_hold, y_pred) - logloss(y_hold, y_pred), 0)","c416413f":"class MultilabelMeanEstimator:\n    def __init__(self):\n        self._means = None\n    def fit(self, y_train):\n        self._means = y_train.mean(axis=0).to_dict()\n    def predict(self, test, zero_mask=None):\n        assert self._means, \"estimator isn't fitted\"\n        pred = np.zeros((len(test), len(self._means)), dtype=np.float64)\n        for i, c in enumerate(test.columns):\n            pred[:, i] = self._means[c]\n        if zero_mask is not None:\n            pred[zero_mask] = 0\n        return pred\n    ","342277af":"estimator = MultilabelMeanEstimator()\nestimator.fit(y_train)\ntest = pd.DataFrame(y_hold, columns=y_train.columns)\ny_pred = estimator.predict(test, ~ctype_hold)\nprint(f\"Means logloss = {logloss(y_hold, y_pred): .5f}\")","20a74f35":"# return rare targets\ntrain_targets_s[rare_targets] = 0\n\n# fit on full train data\nestimator = MultilabelMeanEstimator()\nestimator.fit(train_targets_s)","5a5fdab3":"zero_mask = (test_features.cp_type == 'ctl_vehicle').values # set control group to 0\nprediction = estimator.predict(sample_submission, zero_mask)\nsubmission = pd.DataFrame(prediction, columns=sample_submission.columns, index=sample_submission.index)\nsubmission.to_csv(\"submission.csv\")","53ac0e76":"## Feature interactions","092871cf":"## Baseline models","6bce9fae":"### There's not much linear correlation between targets and features","0706fe50":"### As it mentioned in competition data overview, features consist of 2 huge groups (related to genes & cell)","31111108":"## Data exploration","866cbf7b":"### Test and train `cp_type` distributions are similar: no need in upsampling\/downsampling by experiment type","06f35420":"### Preprocessing","3e01148a":"### Cell features have huge correlation between each other","ff355b26":"### Means","80496f3c":"### Pairs \"agonist-antagonist\" are not always mutually exclusive","7a359de3":"### Labels are very unbalanced","e1c1d188":"## Prediction","c4af49e4":"* Looks like distributions of features close to normal (with small left and right tails)\n* Cell features skewed to left a little bit; perhaps, it would be useful to create the bucket of left tail for most important cell features","c7810b65":"### Continuous features exploration","505e9d79":"### Rare targets (<5 records) will always set to 0 in prediction","86ec80f3":"## Validation split","71a0b0b4":"### Experiment features have no obvious connection with targets","551de3ed":"### Inside control groups all targets is 0","9c622946":"### Constant","c0dda04b":"### It's a multilabel task"}}