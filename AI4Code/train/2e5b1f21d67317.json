{"cell_type":{"282dd1a4":"code","56ad6a05":"code","88f994de":"code","5749714c":"code","cca7a381":"code","60535968":"code","4c190a19":"code","ad9e273f":"code","ad249665":"code","90897e18":"code","07e01a48":"code","72c65de3":"code","1da6a2c3":"code","7bd22612":"code","1869fd10":"code","d2bc157b":"code","c652c085":"code","0f098222":"code","b574561c":"code","9cd19ec7":"code","d7f74472":"code","84d9ee7b":"code","1f0a3dff":"code","fdf6586e":"code","5977158b":"code","54b1dc1e":"code","eeeaf82c":"code","f81e078d":"code","0860afce":"markdown","6591692a":"markdown","2d72e8f1":"markdown","35d44cce":"markdown","a9dbbab0":"markdown","4d5476d6":"markdown","bd42a435":"markdown","e5721feb":"markdown"},"source":{"282dd1a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","56ad6a05":"import re\nimport string\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm, trange\nimport os","88f994de":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torchtext.data import Field, BucketIterator, Example, Dataset, Iterator\nfrom torch.utils.data import DataLoader, random_split\n\nimport spacy\nfrom nltk.corpus import stopwords\n# from nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.porter import PorterStemmer\n# from nltk.stem import WordNetLemmatizer\n\nimport random\nimport math\nimport time","5749714c":"class News_Dataset(Dataset):\n\n    def __init__(self,path, fields, **kwargs):\n        \n        # path of directory containing inputs\n        self.path = path\n        # initialize fileds\n        if not isinstance(fields[0], (tuple, list)):\n            fields = [('src', fields[0]), ('trg', fields[1])]\n        \n        # read Articles and summaries into pandas dataframe\n        self.news_list = self._read_data()\n        # load articles as torch text examples\n        # I am not doing text pre-processing although I have written code for that\n        examples = [Example.fromlist(list(item), fields) for item in self.news_list] \n        # initialize\n        super().__init__(examples, fields, **kwargs)\n        \n\n    def __len__(self):\n        # return length of examples\n        try:\n            return len(self.examples)\n        except TypeError:\n            return 2**32\n\n    def __getitem__(self, index):\n        # get items from examples\n        return self.examples[index]\n    \n    \n    def __iter__(self):\n        # iterator for batch processing\n        for x in self.examples:\n            yield x\n\n            \n    def __getattr__(self, attr):\n        if attr in self.fields:\n            for x in self.examples:\n                yield getattr(x, attr)\n    \n    \n    # function to read text files into pandas data frame\n    def _read_data(self):\n        # initialize variables\n        Articles=[]\n        Summaries=[]\n        \n        # loop over all files and read them into lists\n        for d,path,filenames in tqdm(os.walk(self.path)):\n            for file in filenames:\n                if os.path.isfile(d+'\/'+file):\n                    if('Summaries' in d+'\/'+file):\n                        with open(d+'\/'+file,'r',errors='ignore') as f:\n                            summary=' '.join([i.rstrip() for i in f.readlines()])\n                            Summaries.append(summary)\n                    else:\n                        with open(d+'\/'+file,'r',errors='ignore') as f:\n                            Article=' '.join([i.rstrip() for i in f.readlines()])\n                            Articles.append(Article)\n        \n        return zip(Articles, Summaries)\n    \n\n    # functions for pre-processing data\n    # clean text data\n    def _clean_data(self, text):\n        # remove links\n        text = self._remove_links(text)\n        # remove numbers\n        text = self._remove_numbers(text)\n        # remove punctuations\n        text = self._remove_punct(text)\n        # word_list = self.tokenizer(text)\n        # word_list = self._get_root(word_list)\n\n        return text.lower()\n    \n    # remove punctuations\n    def _remove_punct(self, text):\n        nopunct = ''\n        for c in text:\n            if c not in string.punctuation:\n                nopunct = nopunct + c\n        return nopunct\n\n    # remove numbers\n    def _remove_numbers(self, text):\n        return re.sub(r'[0-9]', '', text)\n\n    # remove links\n    def _remove_links(self, text):\n        return re.sub(r'http\\S+', '', text)\n    \n    # stemming\n    def _get_root(self, word_list):\n        ps = PorterStemmer()\n        return [ps.stem(word) for word in word_list]","cca7a381":"spacy_en = spacy.load('en')\n\ndef tokenize_en(text):\n    # spacy tokenizer\n    return [tok.text for tok in spacy_en.tokenizer(text)]\n\n# fields for processing text data\n# source field\nSRC = Field(tokenize = tokenize_en, \n            init_token = '<sos>', \n            eos_token = '<eos>',\n            fix_length= 500,\n            lower = True)\n# target field\nTRG = Field(tokenize = tokenize_en, \n            init_token = '<sos>', \n            eos_token = '<eos>',\n            fix_length= 200,\n            lower = True)\n\n# you can set batch_first parameter in Fields to True\n# if you want first dimention to be batch dimension\n# I'm new to this library so I don't have any preference\n# So I'm just sticking to the tutorial mentioned in the reference section","60535968":"# create data set instance\nnews_data = News_Dataset(path='\/kaggle\/input', fields=[SRC,TRG])","4c190a19":"# split data into train, validation and test set\ntrain_data, valid_data, test_data = news_data.split(split_ratio=[0.8,0.1,0.1], random_state=random.seed(21))","ad9e273f":"# get length of each data set\nprint(f\"Number of training examples: {len(train_data.examples)}\")\nprint(f\"Number of validation examples: {len(valid_data.examples)}\")\nprint(f\"Number of testing examples: {len(test_data.examples)}\")","ad249665":"SRC.build_vocab(train_data, min_freq = 2)\nTRG.build_vocab(train_data, min_freq = 2)","90897e18":"len(SRC.vocab), len(TRG.vocab)","07e01a48":"# TRG.numericalize(news_data.df.loc[1, 'Summaries'])","72c65de3":"print(vars(train_data.examples[-1]))","1da6a2c3":"def get_length_of_tokens(data):\n    src = []\n    trg = []\n    for item in data.examples:\n        src.append(len(vars(item)['src']))\n        trg.append(len(vars(item)['trg']))\n        \n    return src, trg","7bd22612":"src_len, trg_len = get_length_of_tokens(train_data)","1869fd10":"min(src_len), max(src_len), min(trg_len), max(trg_len)","d2bc157b":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","c652c085":"BATCH_SIZE = 100\n\ntrain_iterator, valid_iterator, test_iterator = Iterator.splits(\n    (train_data, valid_data, test_data), \n    batch_size = BATCH_SIZE, \n    device = device,\n    sort_key= lambda x: len(x.src))","0f098222":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        \n        # initializations\n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        # we will use 2 layers for both encoder and decoder\n        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, src):\n        \n        #src = [src len, batch size]\n        \n        embedded = self.dropout(self.embedding(src))\n        \n        #embedded = [src len, batch size, emb dim]\n        \n        outputs, (hidden, cell) = self.rnn(embedded)\n        \n        #outputs = [src len, batch size, hid dim * n directions]\n        #hidden = [n layers * n directions, batch size, hid dim]\n        #cell = [n layers * n directions, batch size, hid dim]\n        \n        #outputs are always from the top hidden layer\n        \n        return hidden, cell\n \n\n\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        \n        # initialize\n        self.output_dim = output_dim\n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        # for decoder we will use n_directions 1\n        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n        # fully connected layer to predict words\n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, trg, hidden, cell):\n        \n        #trg = [batch size]\n        #hidden = [n layers * n directions, batch size, hid dim]\n        #cell = [n layers * n directions, batch size, hid dim]\n        \n        #n directions in the decoder will always be 1, therefore:\n        #hidden = [n layers, batch size, hid dim]\n        #context = [n layers, batch size, hid dim]\n        \n        trg = trg.unsqueeze(0)\n        \n        #trg = [1, batch size]\n        \n        embedded = self.dropout(self.embedding(trg))\n        \n        #embedded = [1, batch size, emb dim]\n                \n        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        \n        #output = [seq len, batch size, hid dim * n directions]\n        #hidden = [n layers * n directions, batch size, hid dim]\n        #cell = [n layers * n directions, batch size, hid dim]\n        \n        #seq len and n directions will always be 1 in the decoder, therefore:\n        #output = [1, batch size, hid dim]\n        #hidden = [n layers, batch size, hid dim]\n        #cell = [n layers, batch size, hid dim]\n        \n        prediction = self.fc_out(output.squeeze(0))\n        \n        #prediction = [batch size, output dim]\n        \n        return prediction, hidden, cell\n\n    \n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        \n        assert encoder.hid_dim == decoder.hid_dim, \"Hidden dimensions of encoder and decoder must be equal!\"\n        assert encoder.n_layers == decoder.n_layers, \"Encoder and decoder must have equal number of layers!\"\n        \n    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n        \n        #src = [src len, batch size] where src_len is number of tokens in source sentence\n        #trg = [trg len, batch size] same for trg_len\n        #teacher_forcing_ratio is probability to use teacher forcing\n        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n        \n        batch_size = trg.shape[1]\n        trg_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim # we don't have trg.shape[-1] here\n        \n        #tensor to store decoder outputs\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        \n        #last hidden state of the encoder is used as the initial hidden state of the decoder\n        hidden, cell = self.encoder(src)\n        \n        #first input to the decoder is the <sos> tokens\n        dec_input = trg[0,:]\n        \n        for t in range(1, trg_len):\n            \n            #insert input token embedding, previous hidden and previous cell states\n            #receive output tensor (predictions) and new hidden and cell states\n            output, hidden, cell = self.decoder(dec_input, hidden, cell)\n            \n            #place predictions in a tensor holding predictions for each token\n            outputs[t] = output\n            \n            #decide if we are going to use teacher forcing or not\n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            #get the highest predicted token from our predictions\n            top1 = output.argmax(1) \n            \n            #if teacher forcing, use actual next token as next input\n            #if not, use predicted token\n            dec_input = trg[t] if teacher_force else top1\n        \n        return outputs","b574561c":"# seq2seq model's config variables\nINPUT_DIM = len(SRC.vocab)\nOUTPUT_DIM = len(TRG.vocab)\nENC_EMB_DIM = 128\nDEC_EMB_DIM = 128\nHID_DIM = 256\nN_LAYERS = 2\nENC_DROPOUT = 0.1\nDEC_DROPOUT = 0.1\n\n# initialize seq2seq model\nenc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\ndec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\nmodel = Seq2Seq(enc, dec, device)","9cd19ec7":"class Seq2Seq_trainer(object):\n    def __init__(self, model, train_iterator, valid_iterator, pad_index, device, clip, learning_rate):\n        # initialize config variables\n        self.model = model.to(device)\n        self.train_iterator = train_iterator\n        self.valid_iterator = valid_iterator\n        self.clip = clip\n        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n        # TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n        self.criterion = nn.CrossEntropyLoss(ignore_index = pad_index)\n        self.model.apply(self.init_weights)\n        print(f'The model has {self.count_parameters(self.model):,} trainable parameters')\n\n        \n    \n    def init_weights(self,m):\n        for name, param in m.named_parameters():\n            nn.init.uniform_(param.data, -0.08, 0.08)\n        \n    \n    def count_parameters(self, model):\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    \n    def train(self):\n\n        self.model.train()\n\n        epoch_loss = 0\n\n        for i, batch in enumerate(self.train_iterator):\n\n            src = batch.src\n            trg = batch.trg\n\n            self.optimizer.zero_grad()\n\n            output = self.model(src, trg)\n\n            #trg = [trg len, batch size]\n            #output = [trg len, batch size, output dim]\n\n            output_dim = output.shape[-1]\n\n            output = output[1:].view(-1, output_dim)\n            trg = trg[1:].view(-1)\n\n            #trg = [(trg len - 1) * batch size]\n            #output = [(trg len - 1) * batch size, output dim]\n\n            loss = self.criterion(output, trg)\n\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n\n            self.optimizer.step()\n\n            epoch_loss += loss.item()\n\n        return epoch_loss \/ len(self.train_iterator)\n    \n    \n    def evaluate(self, iterator):\n\n        self.model.eval()\n\n        epoch_loss = 0\n\n        with torch.no_grad():\n\n            for i, batch in enumerate(iterator):\n\n                src = batch.src\n                trg = batch.trg\n\n                output = self.model(src, trg, 0) #turn off teacher forcing\n\n                #trg = [trg len, batch size]\n                #output = [trg len, batch size, output dim]\n\n                output_dim = output.shape[-1]\n\n                output = output[1:].view(-1, output_dim)\n                trg = trg[1:].view(-1)\n\n                #trg = [(trg len - 1) * batch size]\n                #output = [(trg len - 1) * batch size, output dim]\n\n                loss = self.criterion(output, trg)\n\n                epoch_loss += loss.item()\n\n        return epoch_loss \/ len(iterator)\n    \n    \n    def epoch_time(self, start_time, end_time):\n        elapsed_time = end_time - start_time\n        elapsed_mins = int(elapsed_time \/ 60)\n        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n        return elapsed_mins, elapsed_secs\n    \n    \n    def fit(self, nepochs):\n        best_valid_loss = float('inf')\n\n        for epoch in tqdm(range(nepochs)):\n\n            start_time = time.time()\n\n            train_loss = self.train()\n            valid_loss = self.evaluate(self.valid_iterator)\n\n            end_time = time.time()\n\n            epoch_mins, epoch_secs = self.epoch_time(start_time, end_time)\n\n            if valid_loss < best_valid_loss:\n                best_valid_loss = valid_loss\n                # torch.save(model.state_dict(), 'tut1-model.pt')\n                print(f'Epoch with best validation loss: {epoch+1:02}')\n\n            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n            print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n\n            \n    def predict(self, iterator):\n        self.model.eval()\n\n        with torch.no_grad():\n\n            for i, batch in enumerate(tqdm(iterator)):\n\n                src = batch.src\n                trg = batch.trg\n\n                output = self.model(src, trg, 0) #turn off teacher forcing\n\n                #trg = [trg len, batch size]\n                #output = [trg len, batch size, output dim]\n                \n                if i == 0:\n                    outputs = torch.argmax(output, -1)\n                else:\n                    outputs = torch.cat((outputs, torch.argmax(output, -1)), -1)\n                \n                # outputs = [trg_len, len(iterator)]\n        return torch.transpose(outputs, 0, 1)","d7f74472":"# print(test_data.examples[0].trg)","84d9ee7b":"# config vaiables\npad_index = TRG.vocab.stoi[TRG.pad_token]\n# initialize trainer\ntrainer = Seq2Seq_trainer(model, train_iterator, valid_iterator, pad_index, device, 1, 1e-3)","1f0a3dff":"trainer.fit(30)","fdf6586e":"# evaluate on test data\ntest_loss = trainer.evaluate(test_iterator)\nprint(f'\\t Test. Loss: {test_loss:.3f} |  Test. PPL: {math.exp(test_loss):7.3f}')","5977158b":"test_tensor = trainer.predict(test_iterator)","54b1dc1e":"test_out = test_tensor.to('cpu').numpy()","eeeaf82c":"test_out[74]","f81e078d":"TRG.vocab.itos[4], TRG.vocab.itos[0], TRG.vocab.itos[6], TRG.vocab.itos[3]","0860afce":"## Import Libraries","6591692a":"## Build Model","2d72e8f1":"## Text data Prepration","35d44cce":"## References\n\n*  https:\/\/github.com\/bentrevett\/pytorch-seq2seq\/blob\/master\/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n*  https:\/\/github.com\/Mjkim88\/Pytorch-Torchtext-Seq2Seq\n*  https:\/\/torchtext.readthedocs.io\/en\/latest\/\n*  https:\/\/www.kaggle.com\/mallaavinash\/text-summarization","a9dbbab0":"## Generate Summary","4d5476d6":"## Train Seq2Seq","bd42a435":"* that's funny","e5721feb":"# Text Summarization using Seq2Seq model"}}