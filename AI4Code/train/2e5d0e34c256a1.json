{"cell_type":{"37fa6bad":"code","5df4ad4c":"code","701272ed":"code","d1533a37":"code","6871fbd0":"code","1b1b6fad":"code","a48f8194":"code","e90fecdb":"code","19092c94":"code","c67be746":"code","b2957147":"code","f2569e60":"code","2c155c84":"code","3dd70f17":"code","04be3518":"code","6a40acc0":"code","ecfa1ffa":"code","aa24ed2a":"code","4e3d8a2d":"code","12899b3b":"code","3b2792c9":"code","b83ae194":"code","9c8b90d0":"markdown","b665ee8d":"markdown","d1d26edb":"markdown","8580175f":"markdown","8872c91c":"markdown","ddeb40c9":"markdown","3fe72571":"markdown","bcdaa834":"markdown","856fcc27":"markdown","9eeb870d":"markdown","43219635":"markdown","0a71bf78":"markdown","a369b7ca":"markdown","4b6055fb":"markdown","4dd56227":"markdown","94eb04fd":"markdown","25ad099d":"markdown","cd661388":"markdown","fbfa6f0c":"markdown","39d56ca4":"markdown","b23bbac2":"markdown","60622e0d":"markdown","17c6828e":"markdown","780e855d":"markdown","6bb35452":"markdown","4695edcf":"markdown","ae6c8538":"markdown"},"source":{"37fa6bad":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Splitting Data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\n\n# Modeling\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, recall_score\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import plot_tree","5df4ad4c":"cancer = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ncancer","701272ed":"cancer.info()","d1533a37":"cancer.drop(columns=['id', 'Unnamed: 32'], inplace = True)","6871fbd0":"cancer.isna().sum()\/len(cancer.index)*100","1b1b6fad":"cancer","a48f8194":"cancer['diagnosis'] = np.where(cancer['diagnosis'] == 'M', 1, 0)\ncancer['diagnosis'].value_counts()\/cancer.shape[0]*100","e90fecdb":"X = cancer.drop('diagnosis', axis = 1)\ny = cancer['diagnosis']","19092c94":"robust = RobustScaler()\nX_scaled = robust.fit_transform(X)","c67be746":"X.shape","b2957147":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y,\n                                                   stratify = y,\n                                                    test_size = 0.3,\n                                                   random_state = 3030)","f2569e60":"k = range(1,100,2)\ntesting_accuracy = []\ntraining_accuracy = []\nscore = 0\n\nfor i in k:\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train, y_train)\n    \n    y_predict_train = knn.predict(X_train)\n    training_accuracy.append(accuracy_score(y_train, y_predict_train))\n    \n    y_predict_test = knn.predict(X_test)\n    acc_score = accuracy_score(y_test,y_predict_test)\n    testing_accuracy.append(acc_score)\n    \n    if score < acc_score:\n        score = acc_score\n        best_k = i\n\nsns.lineplot(k, training_accuracy)\nsns.scatterplot(k, training_accuracy)\nsns.lineplot(k, testing_accuracy)\nsns.scatterplot(k, testing_accuracy)\nplt.legend(['training accuracy', 'testing accuracy'])","2c155c84":"print('This is the best K for KNeighbors Classifier: ', best_k, '\\nAccuracy score is: ', score)","3dd70f17":"depth = range(1,25)\ntesting_accuracy = []\ntraining_accuracy = []\nscore = 0\n\nfor i in depth:\n    tree = DecisionTreeClassifier(max_depth = i, criterion = 'entropy')\n    tree.fit(X_train, y_train)\n    \n    y_predict_train = tree.predict(X_train)\n    training_accuracy.append(accuracy_score(y_train, y_predict_train))\n    \n    y_predict_test = tree.predict(X_test)\n    acc_score = accuracy_score(y_test,y_predict_test)\n    testing_accuracy.append(acc_score)\n    \n    if score < acc_score:\n        score = acc_score\n        best_depth = i\n        \nsns.lineplot(depth, training_accuracy)\nsns.scatterplot(depth, training_accuracy)\nsns.lineplot(depth, testing_accuracy)\nsns.scatterplot(depth, testing_accuracy)\nplt.legend(['training accuracy', 'testing accuracy'])","04be3518":"print('This is the best depth for Decision Tree Classifier: ', best_depth, '\\nAccuracy score is: ', score)","6a40acc0":"knn = KNeighborsClassifier(n_neighbors = 3)\ntree = DecisionTreeClassifier(max_depth = 3, random_state = 3030)","ecfa1ffa":"def model_evaluation(model, metric):\n    model_cv = cross_val_score(model, X_train, y_train, cv = StratifiedKFold(n_splits = 5), scoring = metric)\n    return model_cv\n\nknn_cv = model_evaluation(knn, 'recall')\ntree_cv = model_evaluation(tree, 'recall')\n\nfor model in [knn, tree]:\n    model.fit(X_train, y_train)\n\nscore_cv = [knn_cv.round(5), tree_cv.round(5)]\nscore_mean = [knn_cv.mean(), tree_cv.mean()]\nscore_std = [knn_cv.std(), tree_cv.std()]\nscore_recall_score = [recall_score(y_test, knn.predict(X_test)), \n            recall_score(y_test, tree.predict(X_test))]\nmethod_name = [ 'KNN Classifier', 'Decision Tree Classifier']\ncv_summary = pd.DataFrame({\n    'method': method_name,\n    'cv score': score_cv,\n    'mean score': score_mean,\n    'std score': score_std,\n    'recall score': score_recall_score\n})\ncv_summary","aa24ed2a":"tree = DecisionTreeClassifier(max_depth = 3, random_state = 3030)\n\nhyperparam_space = {\n    'criterion': ['gini', 'entropy'],\n    'splitter': ['best', 'random'],\n    'max_depth': [3, 5, 7, 9, 11],\n    'min_samples_leaf': [3, 9, 13, 15, 17],\n    'class_weight': ['list', 'dict', 'balanced'],\n    'random_state': [3030]\n}\n\ngrid = GridSearchCV(\n                tree,\n                param_grid = hyperparam_space,\n                cv = StratifiedKFold(n_splits = 5),\n                scoring = 'recall',\n                n_jobs = -1)\n\ngrid.fit(X_train, y_train)\n\nprint('best score', grid.best_score_)\nprint('best param', grid.best_params_)","4e3d8a2d":"tree.fit(X_train, y_train)\ntree_recall = (recall_score(y_test, tree.predict(X_test)))\n\ngrid.best_estimator_.fit(X_train, y_train)\ngrid_recall = (recall_score(y_test, grid.predict(X_test)))\n\nscore_list = [tree_recall, grid_recall]\nmethod_name = ['Decision Tree Classifier Before Tuning', 'Decision Tree Classifier After Tuning']\nbest_summary = pd.DataFrame({\n    'method': method_name,\n    'score': score_list\n})\nbest_summary","12899b3b":"plt.figure(figsize=(15,8))\nplot_tree(grid.best_estimator_, feature_names = list(X), class_names = ['Benign','Malignant'], filled = True)\nplt.title('Tree Plot')\nplt.show()","3b2792c9":"importance_table = pd.DataFrame({\n    'imp': grid.best_estimator_.feature_importances_\n}, index = X.columns)\nimportance_table.sort_values('imp', ascending = False)","b83ae194":"importance_table.sort_values('imp', ascending = True).plot(kind = 'barh', figsize = (15,8))","9c8b90d0":"### *Define Target Data*\n* If the cancer is Benign, it will be 0\n* If the cancer is Malignant, it will be 1","b665ee8d":"- In the first step, I did **scaling at X data using Robust Scaler** because I believe there are so many outliers.\n- I only use **KNeighbor Classifier (KNN) and the Decision Tree Classifier (Tree)** in this prediction. I try to find the best K score and best depth for each model and see how the training and testing data on both models either.\n- From the cross-validation process, the KNN model has the highest score with 0.9 but after model evaluation using recall metric, the **Tree model has the highest score with 0.92**. Even the Tree model **indicated overfitting**, I still choose to use this score to continue the process.\n- I decide to get the best parameter for the Tree model by Tuning with the best score of 0.95 which is increasing, then compare the Tree model score before and after tuning. The comparison results prove that the **Tree model after the Tuning process is higher than before with 0.9375**.\n- I check again to see the data using the Feature Importance process. Surprisingly, from 30 features (columns), **only 4 features that is important** to prediction.","d1d26edb":"* From the cross validation and model evaluation processes, I decide to continue with **Decision Tree Classifier** even the score is indicated overfitting. Let's tune the model.","8580175f":"* Data is imbalanced.","8872c91c":"* I use 0.3 as default score for test_size and X.shape for random_state so the data will be devided equally.","ddeb40c9":"# Comparison Between Before & After Tuning","3fe72571":"# Modeling","bcdaa834":"### *Data Splitting*","856fcc27":"# Summary","9eeb870d":"**KNeighbors Classifier**","43219635":"# Feature Importance","0a71bf78":"* The results suggest perhaps 4 of the 30 features as being important to prediction.","a369b7ca":"* I use **KNeighbors Classifier** with best K score and **Decision Tree Classifier** with best depth score.","4b6055fb":"# HyperParam Tuning","4dd56227":"*Missing Value*","94eb04fd":"*Drop Columns*","25ad099d":"### *Define Model*","cd661388":"**Decision Tree Classifier**","fbfa6f0c":"* This is the comparison between before tuning score and after tuning score using Decision Tree Classifier. **I choose to use Decision Tree Classifier after tuning** score in this section.","39d56ca4":"# PreProcessing","b23bbac2":"# Decision Tree Classifier Plot","60622e0d":"* In the case of breast cancer, I want to reduce predictions to people who are misdiagnosed, diagnosed as benign, but it turns out to be malignant, that is, the person we predict is not the default (FN). Evaluation metrics used: **Recall**","17c6828e":"# Final Dataset","780e855d":"**Let's check if the cancer is Malignant or Benign**","6bb35452":"* This model indicates **underfitting** because training accuracy and testing accuracy are both decreases.","4695edcf":"# Data Cleaning","ae6c8538":"* This model indicates **overfitting** because training accuracy is good and the testing accuracy is decreased."}}