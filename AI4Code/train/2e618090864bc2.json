{"cell_type":{"99708eb3":"code","ed0e0968":"code","358a4d03":"code","63c31ef7":"code","e227f40f":"code","1246571d":"code","0bd1185f":"code","a6b447b8":"code","36f046b9":"code","52112b6c":"code","8745b0f2":"code","4d4f1c20":"code","0ad42a4c":"code","7995c816":"code","b6908b71":"code","d40b4e6e":"code","04069133":"code","bf90cfdf":"code","51272dd4":"code","0310baf1":"code","46a356bb":"code","3278bc76":"code","e8f3cf27":"markdown","2d86eb7c":"markdown","40d47add":"markdown","3bdf0541":"markdown","e73ba7da":"markdown","134a225f":"markdown","3194f3ed":"markdown","87371724":"markdown","f1209fa6":"markdown","379215d0":"markdown","583fe113":"markdown","967735d3":"markdown","3e3ef8ad":"markdown","4ed5da12":"markdown","993f0a8e":"markdown"},"source":{"99708eb3":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport os\n\nfrom tqdm import tqdm_notebook as tqdm\ntqdm().pandas()\n\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom keras.datasets import fashion_mnist\n\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense, Input, Embedding\nfrom keras.layers import Conv2D, Conv2DTranspose\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Reshape\nfrom keras.layers import Concatenate\n\nfrom keras.utils.vis_utils import plot_model","ed0e0968":"(trainX, trainy), (testX, testy) = fashion_mnist.load_data()","358a4d03":"print('X - Shape', trainX.shape)\n\nprint('Y- Shape', trainy.shape)\n\nprint('Classes : ', np.unique(trainy))","63c31ef7":"def plot_images(images, n):\n    plt.figure(figsize=(13,8))\n    for i in range(n * n):\n        plt.subplot(n, n, i+1)\n        plt.axis('off')\n        plt.imshow(images[i], cmap='gray_r')\n    plt.show()\n\nplot_images(trainX, 5)","e227f40f":"def define_discriminator(in_shape=(28,28,1), n_classes=10):\n    \n    # Label Input\n    in_label = Input(shape=(1,))    \n    # embedding for categorical input\n    li = Embedding(n_classes, 50)(in_label)\n    # scale up to image dimensions with linear activation\n    n_nodes = in_shape[0] * in_shape[1]\n    \n    li = Dense(n_nodes)(li)\n    li = Reshape((in_shape[0], in_shape[1], 1))(li)\n    \n    \n    # Image Input 28x28\n    in_image = Input(shape=in_shape)\n    merge = Concatenate()([in_image, li])\n    \n    # downsample 14x14\n    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)\n    fe = LeakyReLU(alpha=0.2)(fe)\n    \n    # downsample 7x7\n    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n    fe = LeakyReLU(alpha=0.2)(fe)\n    \n    #Flatten Feature maps\n    fe = Flatten()(fe)\n    fe = Dropout(0.4)(fe)\n    \n    out_layer = Dense(1, activation='sigmoid')(fe)\n    \n    model = Model([in_image, in_label], out_layer)\n    \n    opt = Adam(lr=0.0002, beta_1=0.5)\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n    return model\n    \n    \ndiscriminator = define_discriminator()\n    \nplot_model(discriminator, to_file='discriminator_plot.png', show_shapes=True, show_layer_names=True)","1246571d":"def define_generator(latent_dim, n_classes=10):\n    \n    # Label input\n    in_label = Input(shape=(1,))\n    \n    li = Embedding(n_classes, 50)(in_label)\n    \n    n_nodes = 7 * 7\n    li = Dense(n_nodes)(li)\n    \n    li = Reshape((7, 7, 1))(li)\n    \n    \n    # Image generator input\n    in_lat = Input(shape=(latent_dim,))\n    # foundation for 7x7 image\n    n_nodes = 128 * 7 * 7\n    gen = Dense(n_nodes)(in_lat)\n    gen = LeakyReLU(alpha=0.2)(gen)\n    gen = Reshape((7, 7, 128))(gen)\n    \n    # merge image gen and label input\n    merge = Concatenate()([gen, li])\n    \n    # upsample to 14x14\n    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge) \n    gen = LeakyReLU(alpha=0.2)(gen)\n    \n    # upsample to 28x28\n    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen) \n    gen = LeakyReLU(alpha=0.2)(gen)\n    \n    \n    out_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n    \n    model = Model([in_lat, in_label], out_layer)\n    return model\n\nlatent_dim = 100\n\ngenerator = define_generator(latent_dim)\n\nplot_model(generator, to_file='generator_plot.png', show_shapes=True, show_layer_names=True)\n","0bd1185f":"# define the combined generator and discriminator model, for updating the generator\n\ndef define_gan(generator, discriminator):\n    discriminator.trainable = False\n    \n    gen_noise, gen_label = generator.input\n    \n    gen_output = generator.output\n    \n    \n    # connect image output and label input from generator as inputs to discriminator\n    gan_output = discriminator([gen_output, gen_label])\n    \n    model = Model([gen_noise, gen_label], gan_output)\n    \n    opt = Adam(lr=0.0002, beta_1=0.5)\n    model.compile(loss='binary_crossentropy', optimizer=opt)\n    \n    return model\n\nmodel = define_gan(generator, discriminator)\n\nplot_model(model, to_file='gan_plot.png', show_shapes=True, show_layer_names=True)","a6b447b8":"print(trainX.shape)\nprint(np.expand_dims(trainX, axis=-1).shape)","36f046b9":"def load_real_samples():\n    (trainX, trainy), (_, _) = fashion_mnist.load_data()\n    \n    X = np.expand_dims(trainX, axis=-1)\n    \n    X = X.astype('float32')\n    X = (X - 127.5) \/ 127.5\n    \n    return [X, trainy]\n\n\ndef generate_real_samples(dataset, n_samples):\n    images, labels = dataset\n    ix = np.random.randint(0, images.shape[0], n_samples)\n    \n    X, labels = images[ix], labels[ix]\n    \n    y = np.ones((n_samples, 1))\n    \n    return [X, labels], y","52112b6c":"def generate_latent_points(latent_dim, n_samples, n_classes=10):\n    x_input = np.random.randn(latent_dim * n_samples)\n    # reshape into a batch of inputs for the network\n    x_input = x_input.reshape(n_samples, latent_dim)\n    \n    labels = np.random.randint(0, n_classes, n_samples)\n    return [x_input, labels]\n\ndef generate_fake_samples(generator, latent_dim, n_samples):\n    x_input, labels = generate_latent_points(latent_dim, n_samples)\n    images = generator.predict([x_input, labels])\n    \n    y = np.zeros((n_samples, 1))\n    \n    return [images, labels], y","8745b0f2":"def save_plot(examples, epoch, n=10):\n  # plot images\n    plt.figure(figsize=(10,5))\n    for i in range(n * n):\n        # define subplot\n        plt.subplot(n, n, 1 + i)\n        # turn off axis\n        plt.axis('off')\n        # plot raw pixel data\n        plt.imshow(examples[i], cmap='gray_r')\n        # save plot to file\n    filename = 'generated_plot_e%03d.png' % (epoch+1) \n    plt.savefig(filename)\n    plt.close()\n\ndef summarize_performance(epoch, generator, discriminator, dataset, latent_dim, n_samples=100):\n    [X_real, labels_real], y_real = generate_real_samples(dataset, n_samples)\n    _, acc_real = discriminator.evaluate([X_real, labels_real], y_real, verbose=0)\n    \n    [X_fake, labels], y_fake = generate_fake_samples(generator, latent_dim, n_samples)\n    _, acc_fake = discriminator.evaluate([X_fake, labels], y_fake, verbose=0)\n\n    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n    \n    save_plot(X_fake, epoch)\n    \n    # save model to file\n    filename = 'generator_model_%03d.h5' % (epoch + 1)\n    generator.save(filename)","4d4f1c20":"def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128):\n    bat_per_epo = int(dataset[0].shape[0] \/ n_batch)\n    half_batch = int(n_batch \/ 2)\n    \n    for i in range(n_epochs):\n        g_losses, d_losses = list(), list()\n        for j in range(bat_per_epo):\n            [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n            \n            #Train & Update Discriminator weights for real data\n            d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)           \n            \n            [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n            #Train & Update Discriminator weights for fake data\n            d_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n            \n            #X, y = np.vstack(([X_real, labels_real], [X_fake, labels])), np.vstack((y_real, y_fake))\n            d_loss = (d_loss1 + d_loss2) \/ 2.0\n            \n            # prepare points in latent space as input for the generator\n            [x_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n\n            # create inverted labels for the fake samples\n            y_gan = np.ones((n_batch, 1))\n\n            # update the generator via the discriminator's error\n            g_loss = gan_model.train_on_batch([x_input, labels_input], y_gan)\n            \n            g_losses.append(g_loss)\n            d_losses.append(d_loss)\n\n        print('>%d, d=%.3f, g=%.3f' % (i+1, np.mean(d_losses), np.mean(g_losses)))\n        \n        # evaluate the model every n_eval epochs\n        if (i+1) % 10 == 0:\n            summarize_performance(i, g_model, d_model, dataset, latent_dim)","0ad42a4c":"latent_points, labels = generate_latent_points(100, 100)\n\nlabels = np.asarray([x for _ in range(10) for x in range(10)])\n\ndiscriminator = define_discriminator()\n\ngenerator = define_generator(latent_dim)\n\ngan_model = define_gan(generator, discriminator)\n\ndataset = load_real_samples()\n\ntrain(generator, discriminator, gan_model, dataset, latent_dim)","7995c816":"PATH = '\/kaggle\/input\/output-data\/'\n\nplt.figure(figsize=(20,15))\nplt.axis('off')\n\nplt.imshow(plt.imread(PATH +'generated_plot_e010.png'))","b6908b71":"plt.figure(figsize=(20,15))\nplt.axis('off')\n\nplt.imshow(plt.imread(PATH +'generated_plot_e030.png'))","d40b4e6e":"PATH = '\/kaggle\/input\/output-data\/'\n\nplt.figure(figsize=(20,15))\nplt.axis('off')\n\nplt.imshow(plt.imread(PATH +'generated_plot_e100.png'))","04069133":"def show_plot(examples, n=10):\n  # plot images\n    plt.figure(figsize=(30,20))\n    for i in range(n * n):\n        # define subplot\n        plt.subplot(n, n, 1 + i)\n        # turn off axis\n        plt.axis('off')\n        # plot raw pixel data\n        plt.imshow(examples[i], cmap='gray_r')\n    plt.show()","bf90cfdf":"np.asarray([x for _ in range(10) for x in range(10)])","51272dd4":"from keras.models import load_model\n\nlatent_points, labels = generate_latent_points(100, 100)\n\nlabels = np.asarray([x for _ in range(10) for x in range(10)])\n\nmodel = load_model(PATH + 'generator_model_100.h5')\n\nX = model.predict([latent_points, labels])\n\n\n# scale from [-1,1] to [0,1]\nX = (X + 1) \/ 2.0\n\n# plot the result\nshow_plot(X, 10)","0310baf1":"latent_points, labels = generate_latent_points(100, 1)\nlabel = np.asarray([2])\n\nX = model.predict([latent_points, label])\n\n\n# scale from [-1,1] to [0,1]\nX = (X + 1) \/ 2.0\n\n# plot the result\nplt.imshow(X[0], cmap='gray_r')","46a356bb":"latent_points, labels = generate_latent_points(100, 1)\n\nlabel = np.asarray([8])\n\nX = model.predict([latent_points, label])\n\n\n# scale from [-1,1] to [0,1]\nX = (X + 1) \/ 2.0\n\n# plot the result\nplt.imshow(X[0], cmap='gray_r')","3278bc76":"latent_points, labels = generate_latent_points(100, 1)\n\nlabel = np.asarray([5])\n\nX = model.predict([latent_points, label])\n\n\n# scale from [-1,1] to [0,1]\nX = (X + 1) \/ 2.0\n\n# plot the result\nplt.imshow(X[0], cmap='gray_r')","e8f3cf27":"<h3>2. After 30 EPOCHS<\/h3>","2d86eb7c":"<h3><center>3. Define Descriminator<\/center><\/h3>\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nThe class label is passed through an Embedding layer with the size of 50. This means that each of the 10 classes for the Fashion-MNIST dataset (0 through 9) will map to a different 50-element vector representation that will be learned by the discriminator model. The output of the embedding is then passed to a fully connected layer with a linear activation.\n    <\/div>","40d47add":"<h3><center>4. Define Generator<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\n    \n <\/div>","3bdf0541":"<h3>Purse<\/h3>","e73ba7da":"<h3><center>6. Generate Samples<\/center><\/h3>","134a225f":"<h3>3. After 100 EPOCHS<\/h3>","3194f3ed":"<h3><center>Introduction<\/center><\/h3>\n\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\n    Developing a GAN for gener- ating images requires both a discriminator convolutional neural network model for classifying whether a given image is real or generated and a generator model that uses inverse convolutional layers to transform an input to a full two-dimensional image of pixel values.   \n<br><br>\nAdditional information that is correlated with the input images, such as class labels, can be used to improve the GAN. This improvement may come in the form of more stable training, faster training, and\/or generated images that have better quality. Class labels can also be used for the deliberate or targeted generation of images of a given type.\n<br><br>\nA GAN can be trained in such a way that both the generator and the discriminator models are conditioned on the class label. This means that when the trained generator model is used as a standalone model to generate images in the domain, images of a given type, or class label, can be generated.\n\n<\/div>\n\n<br><br>\n<h3>Tutorial Link of \"Detailed working on DCGAN network from scratch\"<\/h3>\n<a href=\"https:\/\/www.kaggle.com\/ashrafkhan94\/tutorial-deep-convolutional-gans-on-mnist?scriptVersionId=55785402\">https:\/\/www.kaggle.com\/ashrafkhan94\/tutorial-deep-convolutional-gans-on-mnist?scriptVersionId=55785402<\/a>","87371724":"<h3><center>1. Importing Libraries<\/center><\/h3>","f1209fa6":"<h3>Full Sleeve Tshirt<\/h3>","379215d0":"<h3><center>8. Plot IMAGES vs Epochs<\/center><\/h3>\n\n<h3>1. After 10 Epochs<\/h3>","583fe113":"<h3><center>2. Loading & Exploring dataset<\/center><\/h3>","967735d3":"<h3><center>5. Define GAN<\/center><\/h3>","3e3ef8ad":"<h3><center>7. Train GAN<\/center><\/h3>","4ed5da12":"<h3>Shoes<\/h3>","993f0a8e":"<h3><center>8. Conditional Clothing Generation<\/center><\/h3>"}}