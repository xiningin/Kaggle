{"cell_type":{"fbbf398d":"code","2296a9cd":"code","b943f98d":"code","7e7c314c":"code","48c1151b":"code","515fd188":"code","8dea3e78":"code","3c44591a":"code","35729000":"code","cb155604":"code","c8fea17d":"code","8d78bf38":"code","fd9a53bf":"code","0fed02ae":"code","4b15892f":"code","a4aec353":"code","e8b9042f":"code","72dbdede":"code","e962d352":"code","de59bc58":"markdown","ff48113a":"markdown","7fe225fd":"markdown","ac2f139b":"markdown","ceca32e3":"markdown","cd761b2b":"markdown","3e2092b9":"markdown","01e9533d":"markdown","fc40819c":"markdown","04464437":"markdown","cf82415d":"markdown"},"source":{"fbbf398d":"# DF\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n# Model\nfrom lightgbm import LGBMRegressor\n#Data\nimport gresearch_crypto\nimport traceback\nimport time\nfrom datetime import datetime\n# Plotters\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","2296a9cd":"# Load Datasets\n\npath = \"\/kaggle\/input\/g-research-crypto-forecasting\/\"\ndf_train = pd.read_csv(path + \"train.csv\")\ndf_test = pd.read_csv(path + \"example_test.csv\")\ndf_asset_details = pd.read_csv(path + \"asset_details.csv\")\ndf_supp_train = pd.read_csv(path + \"supplemental_train.csv\")","b943f98d":"df_train","7e7c314c":"df_test","48c1151b":"df_asset_details","515fd188":"df_supp_train","8dea3e78":"df_train.describe()","3c44591a":"df_train.isna().sum()","35729000":"df_train.timestamp\n# Time stamps are int, lets convert these to timestamps type for better indexing","cb155604":"timestamper = lambda s: np.int32(time.mktime(datetime.strptime(s, \"%d\/%m\/%Y\").timetuple()))","c8fea17d":"btc = df_train[df_train[\"Asset_ID\"]==1].set_index(\"timestamp\") # Asset_ID = 1 for Bitcoin\neth = df_train[df_train[\"Asset_ID\"]==6].set_index(\"timestamp\") # Asset_ID = 6 for Ethereum\nbnb = df_train[df_train[\"Asset_ID\"]==0].set_index(\"timestamp\") # Asset_ID = 0 for Binance Coin\nada = df_train[df_train[\"Asset_ID\"]==3].set_index(\"timestamp\") # Asset_ID = 3 for Cardano\n\nbeg_btc = datetime.fromtimestamp(btc.index[0]).strftime(\"%A, %B %d, %Y %I:%M:%S\") \nend_btc = datetime.fromtimestamp(btc.index[-1]).strftime(\"%A, %B %d, %Y %I:%M:%S\") \nbeg_eth = datetime.fromtimestamp(eth.index[0]).strftime(\"%A, %B %d, %Y %I:%M:%S\") \nend_eth = datetime.fromtimestamp(eth.index[-1]).strftime(\"%A, %B %d, %Y %I:%M:%S\")\nbeg_bnb = datetime.fromtimestamp(eth.index[0]).strftime(\"%A, %B %d, %Y %I:%M:%S\") \nend_bnb = datetime.fromtimestamp(eth.index[-1]).strftime(\"%A, %B %d, %Y %I:%M:%S\")\nbeg_ada = datetime.fromtimestamp(eth.index[0]).strftime(\"%A, %B %d, %Y %I:%M:%S\") \nend_ada = datetime.fromtimestamp(eth.index[-1]).strftime(\"%A, %B %d, %Y %I:%M:%S\")\n\nprint('Bitcoin from ', beg_btc, ' to ', end_btc) \nprint('Ethereum from ', beg_eth, ' to ', end_eth)\nprint('Binance from ', beg_bnb, ' to ', end_bnb) \nprint('Cardano from ', beg_ada, ' to ', end_ada)","8d78bf38":"plt.figure(figsize=(8,6))\nsns.heatmap(btc[['Count','Open','High','Low','Close','Volume','VWAP','Target']].corr(), \n            vmin=-1.0, vmax=1.0, annot=True, cmap='coolwarm', linewidths=0.1)\nplt.show()","fd9a53bf":"plt.figure(figsize=(8,6))\nsns.heatmap(eth[['Count','Open','High','Low','Close','Volume','VWAP','Target']].corr(), \n            vmin=-1.0, vmax=1.0, annot=True, cmap='coolwarm', linewidths=0.1)\nplt.show()","0fed02ae":"plt.figure(figsize=(8,6))\nsns.heatmap(bnb[['Count','Open','High','Low','Close','Volume','VWAP','Target']].corr(), \n            vmin=-1.0, vmax=1.0, annot=True, cmap='coolwarm', linewidths=0.1)\nplt.show()","4b15892f":"def hlco_ratio(df): \n    return (df['High'] - df['Low'])\/(df['Close']-df['Open'])\ndef upper_shadow(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\ndef lower_shadow(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']\n\ndef get_features(df):\n    df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].copy()\n    df_feat['Upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['hlco_ratio'] = hlco_ratio(df_feat)\n    df_feat['Lower_Shadow'] = lower_shadow(df_feat)\n    return df_feat","a4aec353":"# train test split df_train into 70% train rows and 30% valid rows\ntrain_data = df_train\n\ndef get_Xy_and_model_for_asset(df_train, asset_id):\n    df = df_train[df_train[\"Asset_ID\"] == asset_id]\n    \n    df = df.sample(frac=0.3)\n    df_proc = get_features(df)\n    df_proc['y'] = df['Target']\n    df_proc.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df_proc = df_proc.dropna(how=\"any\")\n    \n    \n    X = df_proc.drop(\"y\", axis=1)\n    y = df_proc[\"y\"]   \n    model = LGBMRegressor()\n    model.fit(X, y)\n    return X, y, model\n\nXs = {}\nys = {}\nmodels = {}\n\nfor asset_id, asset_name in zip(df_asset_details['Asset_ID'], df_asset_details['Asset_Name']):\n    print(f\"Training model for {asset_name:<16} (ID={asset_id:<2})\")\n    X, y, model = get_Xy_and_model_for_asset(train_data, asset_id)       \n    try:\n        Xs[asset_id], ys[asset_id], models[asset_id] = X, y, model\n    except: \n        Xs[asset_id], ys[asset_id], models[asset_id] = None, None, None ","e8b9042f":"models","72dbdede":"parameters = {\n    'num_leaves': range(21, 161, 10),\n    'learning_rate': [0.1, 0.01, 0.05]\n}\n\nnew_models = {}\nfor asset_id, asset_name in zip(df_asset_details['Asset_ID'], df_asset_details['Asset_Name']):\n    print(\"GridSearchCV for: \" + asset_name)\n    grid_search = GridSearchCV(\n        estimator=get_Xy_and_model_for_asset(df_train, asset_id)[2],\n        param_grid=parameters,\n        n_jobs = -1,\n        cv = 5,\n        verbose=True\n    )\n    grid_search.fit(Xs[asset_id], ys[asset_id])\n    new_models[asset_id] = grid_search.best_estimator_\n    grid_search.best_estimator_","e962d352":"env = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\nfor i, (df_test, df_pred) in enumerate(iter_test):\n    for j , row in df_test.iterrows():        \n        if new_models[row['Asset_ID']] is not None:\n            try:\n                model = new_models[row['Asset_ID']]\n                x_test = get_features(row)\n                y_pred = model.predict(pd.DataFrame([x_test]))[0]\n                df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = y_pred\n            except:\n                df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = 0\n                traceback.print_exc()\n        else: \n            df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = 0  \n    \n    env.predict(df_pred)","de59bc58":"#### 6. Submission","ff48113a":"#### 3. Feature Engineering","7fe225fd":"#### 4. Training","ac2f139b":"### Index\n\n1. About the Data\n2. EDA\n3. Feature Engineering\n4. Training\n5. HyperP Tuning\n6. Submission","ceca32e3":"#### 5. HyperP Tuning","cd761b2b":"- VWAP has 9 Nans","3e2092b9":"### Introduction \n\nOver $40 billion worth of cryptocurrencies are traded every day. They are among the most popular assets for speculation and investment, yet have proven wildly volatile. Fast-fluctuating prices have made millionaires of a lucky few, and delivered crushing losses to others. Could some of these price movements have been predicted in advance?\n\nIn this competition, you'll use your machine learning expertise to forecast short term returns in 14 popular cryptocurrencies. We have amassed a dataset of millions of rows of high-frequency market data dating back to 2018 which you can use to build your model. Once the submission deadline has passed, your final score will be calculated over the following 3 months using live crypto data as it is collected.\n\nThe simultaneous activity of thousands of traders ensures that most signals will be transitory, persistent alpha will be exceptionally difficult to find, and the danger of overfitting will be considerable. In addition, since 2018, interest in the cryptomarket has exploded, so the volatility and correlation structure in our data are likely to be highly non-stationary. The successful contestant will pay careful attention to these considerations, and in the process gain valuable insight into the art and science of financial forecasting.\n\nG-Research is Europe\u2019s leading quantitative finance research firm. We have long explored the extent of market prediction possibilities, making use of machine learning, big data, and some of the most advanced technology available. Specializing in data science and AI education for workforces, Cambridge Spark is partnering with G-Research for this competition.","01e9533d":"Data ranges from 2018 to 2021","fc40819c":"#### 2. EDA","04464437":"#### 1. About the Data\n\n**train.csv**\n\n- timestamp: All timestamps are returned as second Unix timestamps (the number of seconds elapsed since 1970-01-01 00:00:00.000 UTC). Timestamps in this dataset are multiple of 60, indicating minute-by-minute data.\n- Asset_ID: The asset ID corresponding to one of the crytocurrencies (e.g. Asset_ID = 1 for Bitcoin). The mapping from Asset_ID to crypto asset is contained in asset_details.csv.\n- Count: Total number of trades in the time interval (last minute).\n- Open: Opening price of the time interval (in USD).\n- High: Highest price reached during time interval (in USD).\n- Low: Lowest price reached during time interval (in USD).\n- Close: Closing price of the time interval (in USD).\n- Volume: Quantity of asset bought or sold, displayed in base currency USD.\n- VWAP: The average price of the asset over the time interval, weighted by volume. VWAP is an aggregated form of trade data.\n- Target: Residual log-returns for the asset over a 15 minute horizon.\n\n**supplemental_train.csv** \n\nAfter the submission period is over this file's data will be replaced with cryptoasset prices from the submission period. In the Evaluation phase, the train, train supplement, and test set will be contiguous in time, apart from any missing data. The current copy, which is just filled approximately the right amount of data from train.csv is provided as a placeholder.\n\n**asset_details.csv** \n\nProvides the real name and of the cryptoasset for each Asset_ID and the weight each cryptoasset receives in the metric. Weights are determined by the logarithm of each product's market cap (in USD), of the cryptocurrencies at a fixed point in time. Weights were assigned to give more relevance to cryptocurrencies with higher market volumes to ensure smaller cryptocurrencies do not disproportionately impact the models.\n\n**example_sample_submission.csv**\n\nAn example of the data that will be delivered by the time series API. The data is just copied from train.csv.\n\n**example_test.csv** \n\nAn example of the data that will be delivered by the time series API.","cf82415d":"Heat maps of 5 coins"}}