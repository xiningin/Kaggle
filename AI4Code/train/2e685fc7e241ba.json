{"cell_type":{"d40ce02f":"code","7f9d4cfb":"code","b339af0c":"code","87b0ebec":"code","5b5e1b6c":"code","c2818765":"code","6ee88d46":"code","d2faf98c":"code","a8b026bf":"code","5414bf5b":"code","095dd62f":"code","1977f752":"code","b63d540e":"code","ec0a01d0":"code","6e907d06":"code","1a066313":"code","5ffbe7e5":"code","8510a730":"code","a3f4838d":"code","3fe93e83":"code","d446d1a6":"code","af147ffc":"code","bdbdf918":"code","e5627914":"code","89f9ce0a":"code","0ae44968":"code","ea1227cd":"code","a30da9ad":"code","f0caa46c":"code","5fe70de2":"code","05699aec":"markdown","1786973b":"markdown","8b7514ed":"markdown","e9ecf7ab":"markdown","627e85eb":"markdown","5e98ea19":"markdown","c158ee9f":"markdown","d7b4c633":"markdown","d3b726a2":"markdown","7925815c":"markdown","4d2a90e6":"markdown","d70ad0a3":"markdown","951b8271":"markdown","558364cd":"markdown","5d69ce18":"markdown","0a33467f":"markdown","86c9d2dc":"markdown","4e093e60":"markdown","968981f3":"markdown","41f3b006":"markdown","6ae17519":"markdown","beac439b":"markdown","095f5159":"markdown","97b174f8":"markdown","b3149765":"markdown","b4c89b00":"markdown","b976d064":"markdown","f05517fa":"markdown","8ddc9c02":"markdown","648f484b":"markdown","b7501473":"markdown"},"source":{"d40ce02f":"import os\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\n\nplt.style.use(\"ggplot\")","7f9d4cfb":"# Load Data\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b339af0c":"PATH = \"\/kaggle\/input\/automobile-customer-multiclassification\/Automobile_customers.csv\"\ndata = pd.read_csv(PATH)","87b0ebec":"data","5b5e1b6c":"data.info()","c2818765":"print(f\"There are {data.shape[0]} rows in dataframe.\")\nprint(f\"And {data.shape[1]} columns. \\n\")\n\nprint(f\"Columns: {data.columns} \\n\")\n\nprint(f\"Percentage of Null values: \\n {data.isnull().sum() \/ data.shape[0]} \\n\")\nprint(f\"Percentage of NA values: \\n {data.isna().sum() \/ data.shape[0]}\")","6ee88d46":"data","d2faf98c":"data = data.dropna(axis = 0)\n\nprint(data.isnull().sum() \/ data.shape[0])\nprint(data.isna().sum() \/ data.shape[0])\n\ndata","a8b026bf":"# Here some strange values\ndata[\"Region\"].unique()\n\nmap_dict = {'Sub-urban#' : 'Sub-urban',\n            'Urban#' : 'Urban',\n            'Urban%' : 'Urban',\n            'Sub-urban%' : 'Urban',\n            'Sub-urban' : 'Sub-urban',\n            'Urban' : 'Urban',\n            'Rural' : 'Rural'}\n\ndata[\"Region\"] = data[\"Region\"].map(map_dict)\ndata[\"Region\"].unique()","5414bf5b":"gender_dict = {\"M\" : \"M\",\n               \"MA\" : \"M\",\n               \"!M\" : \"M\",\n               \"F\" : \"F\"}\n\ndata[\"Gender\"] = data[\"Gender\"].map(gender_dict)","095dd62f":"sns.pairplot(data = data, corner = True, kind = \"scatter\", diag_kind = \"kde\")","1977f752":"# all unique age variable\nages = sorted(data[\"Age\"].unique())\n\n# age periods\nyoung_costumers = len([year for year in data[\"Age\"] if year >= 18 and year <= 25])\nmiddle_age_costumers = len([year for year in data[\"Age\"] if year > 25 and year <= 50])\neldelrly_costumers = len([year for year in data[\"Age\"] if year > 50])","b63d540e":"fig = px.pie(labels = [\"18 - 25 years\", \"25 - 50 years\", \"Elderly costumers\"], values = [young_costumers, middle_age_costumers, eldelrly_costumers],\n             names = [\"18 - 25 years Count\", \"25 - 50 years Count\", \"Elderly costumers Count\"], title = \"Distributions of costumers age\",\n             hole = 0.1,\n             color = [\"Royal Blue\", \"Dark Blue\", \"Cyan\"],\n             color_discrete_map = {\"Cyan\" : \"cyan\",\n                                   \"Dark Blue\" : \"darkblue\",\n                                   \"Royal Blue\" : \"royalblue\"})\n\nfig.update_traces(textposition = \"inside\", textinfo = \"label+percent+value\", hoverinfo = \"label+percent\", textfont_size = 13)\nfig.update_layout(legend_title = \"Legend\")\n\nfig.show()","ec0a01d0":"labels = data[\"Gender\"].unique()\n\nincome_b3 = data[\"Gender\"][data[\"Income_Bucket\"] == 3].value_counts().values\nincome_b2 = data[\"Gender\"][data[\"Income_Bucket\"] == 2].value_counts().values\nincome_b1 = data[\"Gender\"][data[\"Income_Bucket\"] == 1].value_counts().values\n\nfig = go.Figure(data = [\n    go.Bar(name = \"High Salary\", x = labels, y = income_b3, marker_color = \"royalblue\"),\n    go.Bar(name = \"Medium\", x = labels, y = income_b2, marker_color = \"cyan\"), \n    go.Bar(name = \"Low Salary\", x = labels, y = income_b1, marker_color = \"darkblue\")])\n\nfig.update_layout(title = \"Gender and Income Bucket\", xaxis_title = \"Gender\", yaxis_title = \"Count\", legend_title=\"Legend\")\nfig.update_layout(barmode = \"group\")\n\nfig.show()","6e907d06":"# labels and values collecting\nregion_labels = data[\"Region\"].unique()\nvenicle_labels = data[\"Vehicle_Segment\"].value_counts().keys()\n\nlabels = [\"Regions\"]\nvalues = [3]\n\nfor region_type in region_labels:\n    labels.append(region_type)\n    for n in range(len(venicle_labels)):\n        labels.append(data[\"Vehicle_Segment\"][data[\"Region\"] == region_type].value_counts().keys()[n])\n    values.append(len(data[\"Vehicle_Segment\"][data[\"Region\"] == region_type]))\n    for n in range(4):\n        values.append(data[\"Vehicle_Segment\"][data[\"Region\"] == region_type].value_counts().values[n])\n\nprint(f\"Labels for Sunburst plot: {labels}\\n\")\nprint(f\"Values for Sunburst plot: {values}\")","1a066313":"parents = [\"\", \"Regions\", \"Sub-urban\", \"Sub-urban\", \"Sub-urban\", \"Sub-urban\", \n           \"Regions\", \"Urban\", \"Urban\", \"Urban\", \"Urban\", \n           \"Regions\", \"Rural\", \"Rural\", \"Rural\", \"Rural\"]\n\ncolors = [\"white\", \"royalblue\", \"cyan\", \"darkblue\", \"blue\", \"darkblue\", \"cyan\", \"royalblue\"]\nfig = go.Figure(go.Sunburst(labels = labels,\n                            parents = parents,\n                            values = values,\n                            maxdepth = 2, marker = dict(colors = colors)))\n\nfig.update_layout(title = \"Regions and Venicles types\")\n\nfig.show()","5ffbe7e5":"modelling_data = data.drop(columns = [\"Customer ID\", \"No_of_months\", \"On_Call_Offer\"], axis = 1)\n\nmodelling_data","8510a730":"from sklearn import preprocessing\n\nobject_type_columns = [column for column in modelling_data if modelling_data[column].dtype == \"object\"]\n\nencoder = preprocessing.LabelEncoder()\n\nfor column in object_type_columns:\n    modelling_data[column] = encoder.fit_transform(modelling_data[column])\n\nmodelling_data","a3f4838d":"list_of_columns = modelling_data.columns\ncorrelation_list = modelling_data[list_of_columns].corr()\n\nfig = go.Figure(data = go.Heatmap(z = correlation_list,\n                                  x = list_of_columns,\n                                  y = list_of_columns,\n                                  hoverongaps = True))\n\nfig.update_layout(title = \"Correlation heatmap\")\n\nfig.show()","3fe93e83":"!pip install skfeature-chappers\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.model_selection import train_test_split","d446d1a6":"X = modelling_data.drop(columns = [\"Vehicle_Segment\"], axis = 1)\nY = modelling_data[\"Vehicle_Segment\"]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 20)\n\nranks = mutual_info_classif(X_train, Y_train)\n\nselection_data = pd.Series(ranks, modelling_data.columns[0:len(modelling_data.columns) - 1])\n\nfig = go.Figure()\n\nfig = px.bar(selection_data, x = X_train.columns, y = ranks)\n\nfig.update_layout(title = \"Information Gain\",\n                  xaxis_title = \"Columns\", yaxis_title = \"Count\")\n\nfig.show()             ","af147ffc":"modelling_data = modelling_data.drop(columns = [\"Gender\", \"Marital_Status\", \"State\", \"No_of_children\"], axis = 1)","bdbdf918":"modelling_data.head(10)","e5627914":"X = modelling_data.drop(columns = [\"Vehicle_Segment\"], axis = 1)\nY = modelling_data[\"Vehicle_Segment\"]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)","89f9ce0a":"# shapes of train and test sets\nprint(f\"Shape of X_train and Y_train sets: {X_train.shape, Y_train.shape}\")\nprint(f\"Shape of X_test and Y_test sets: {X_test.shape, Y_test.shape}\")","0ae44968":"# sklearn for ml\nimport sklearn\nfrom sklearn.metrics import classification_report, jaccard_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm","ea1227cd":"from sklearn import linear_model\n\nlr_model = LogisticRegression(random_state = 42, solver=\"saga\", multi_class=\"multinomial\", n_jobs = 4)\n\nlr_model.fit(X_train, Y_train)\n\n# scores\nprint(f\"Accuracy score: {lr_model.score(X_test, Y_test)}\")\nprint(f\"Jaccard score: {jaccard_score(Y_test, lr_model.predict(X_test), average = 'macro')}\")","a30da9ad":"tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 9)\n\ntree.fit(X_train, Y_train)\nprint(f\"Score is {tree.score(X_test, Y_test)}\")\nprint(f\"Jaccard score: {jaccard_score(Y_test, tree.predict(X_test), average = 'macro')}\")","f0caa46c":"forest = RandomForestClassifier(n_estimators = 300, criterion = \"entropy\", max_depth = 12)\n\nforest.fit(X_train, Y_train)\nprint(f\"Score is {forest.score(X_test, Y_test)}\")\nprint(f\"Jaccard score: {jaccard_score(Y_test, forest.predict(X_test), average = 'macro')}\")","5fe70de2":"knn = KNeighborsClassifier(n_neighbors = 15, algorithm = \"kd_tree\", n_jobs = 4)\n\nknn.fit(X_train, Y_train)\nprint(f\"Score is {knn.score(X_test, Y_test)}\")\nprint(f\"Jaccard score: {jaccard_score(Y_test, knn.predict(X_test), average = 'macro')}\")","05699aec":"**1) Logistic Regression**","1786973b":"**4.1) Review visualization of all numeric values in data:**","8b7514ed":"**4.3) Gender and Income Bucket columns.**","e9ecf7ab":"**Region and Venicle type**","627e85eb":"# 6) Conclusion\n\nOn feature selection step we saw that corellation between variables and information gains rating were negligible. It means that there is simply no connection between the variables in this dataset, thats why we did not get good result or even score more than 50% with this data.\n\nI have tried many different variants of ml models (models which you can see in modeling part, after that models that I did not include in this notebook like SVM, XGboostClassifier. I`ve tried even stacking method) and changed a lot of parameters, but nothing improved situation.\n\nAfter all of this **I can tell for sure that this dataset is broken and notify you about this!**","5e98ea19":"# 5) Modelling\n\n1) Feature selection;\n\n2) Train, Test, Val sets creating;\n\n3) Work with ML models.","c158ee9f":"# 4) Analyzing","d7b4c633":"# 2) Fast looking on dataset\nLet`s see head of our data frame, list of columns, size and nan\/null values in this dataset.","d3b726a2":"**3) Information Gain**\n\n\nCalculates the entropy reduction resulting from transforming a dataset.","7925815c":"From this pie plot we learned that:\n\n1) Count of Young costumers is only 14.1%;\n\n2) Costumers in 25 - 50 age period count is 42.7%;\n\n3) The biggest number of costumers is Elderly people.\n\nWe can make a small conclusion about age distribution in our dataframe. Most customers are 25 to 50 years old and over.","4d2a90e6":"**Firstly, let`s drop columns which useless for modelling. Customer ID, No_of_months, On_Call_Offer.**","d70ad0a3":"# 1) Import Libraries and Load Data\nFirstly, lets import all useful libraries. Secondly, load data.","951b8271":"**I wont even ask you about comments and your upvotes because of this.\nThank you everyone who check this notebook!**","558364cd":"**Conclusion of feature selection step.\nFinally, we can make a decision that we should drop columns: Gender, State and Marital_Status because this columns are not important for modelling.**","5d69ce18":"**Nan, Null and Na values in columns.**\n\nHow we saw upper there are only 0.001677% (17 rows from 10000+) of Nan, Null and Na values in State column. This number is too small, thats why we can just drop these rows.","0a33467f":"We can see that:\n\n1) Low and Medium salaries are more common among men than high salaries\n\n2) The amount of data on the salaries of menis much higher than that of women\n\n3) High salaries are more common among women","86c9d2dc":"**Dataset prepared for machine learning actions:**","4e093e60":"Unfortunately, this score is awful. Next model.\n\n**2) Decision Tree Classifier**","968981f3":"# 3) Data preprocessing\n\nIn this step our goals are:\n\n1) Work with Null and Na values in State column;\n\n2) Fix Region column problem.","41f3b006":"**2) Correlation part**\n\nIf you want to learn more about correlation you can check my notebook about calculating the correlation of a youtube dataset on this link: https:\/\/www.kaggle.com\/artemborzenko\/calculating-the-correlation-of-a-youtube-dataset","6ae17519":"**This sunburst plot is interactive. To find out more info you can click on plot sections.*\n\nHere we can see that:\n\n1) Sub - urban is the most widespread region in this dataframe. Urban is on second place and Rural is less mentioned in data;\n\n2) Compact SUV and SUV are most popular than other venicle types in sub - urban region, but cars are more spread in urban and rural region.\n\n3) Count of Pick up trucks are almost always average ","beac439b":"For learn more from data, in my opinion, we should analyze every column from dataset (exception ID).\n\n**4.2) Age. Let`s see distribution of costumers age.**","095f5159":"Here we can see that:\n\n* There are 10733 rows in dataframe and 13 columns. Columns are Customer ID - IDs of customers, Age - Age of each customer, Income_Bucket - customers salaries, Gender, State, Region - location types, Maritial_Status, no_of_children - number of children in customers family, occupation - profession of each customer, Venicle_Segment - type of car (our target), No_of_monts, Hobbies, On_call_Offer;\n\n* There are columns which dtypes are Object, so we have to use Encoders to change it to integer (example - Urban: 1, Sub-urban: 2);\n\n* Unfortunately, there are Null and NA values in State column, we must work with that.\n","97b174f8":"**Feature selection**\n\n1) Changing dtype of columns using Encoder\n","b3149765":"3) Modelling\n\nOur task is classification, so we will use these models:\n\n* Logistic Regression;\n\n* Decision Tree Classifier;\n\n* Random Forest Classifier;\n\n* KNeighbors Classifier.\n\nAlso, we will tune our models to get best results.","b4c89b00":"Score is terrible too. Next model.\n\n**3) Random Forest Classifier**","b976d064":"**2) Train and Test sets creating**","f05517fa":"**Fix column problem**","8ddc9c02":"# Automobile \u0421ustomer \u0421lassification\n\nHello everyone! In this notebook we will analyze dataset which was created by one of the largest automotive companies and fit ML model for prediction of type of car \nsuitable for a specific client according to his needs and their qualities. Let`s start!","648f484b":"**Other libraries for working with machine learning we will import in Model Building steps.**","b7501473":"Again and again. Next model.\n\n**4) KNN**"}}