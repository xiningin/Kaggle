{"cell_type":{"6d80e2d5":"code","f1874bd3":"code","01d49112":"code","a8efc697":"code","5b933fa1":"code","6cfaf682":"code","cc44cf57":"code","1586187d":"code","fc3fbe78":"code","9ab2ef58":"code","8c1edc26":"code","f6a5cb74":"code","cae37a5f":"code","bd58067e":"code","879c638e":"code","67c3840b":"code","a621e644":"code","6b85f7d2":"code","9f5fdef1":"code","0533e0b8":"code","2b3351b9":"markdown","f55e7d66":"markdown","208d1c29":"markdown","c2355548":"markdown"},"source":{"6d80e2d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gresearch_crypto\nimport matplotlib.pyplot as plt\nenv = gresearch_crypto.make_env()\nfrom datetime import datetime\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f1874bd3":"from lightgbm import LGBMRegressor\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom sklearn.model_selection import train_test_split","01d49112":"def reduce_mem_usage(train_data):\n    start_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    for col in train_data.columns:\n        col_type = train_data[col].dtype\n        if col_type != object:\n            c_min = train_data[col].min()\n            c_max = train_data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    train_data[col] = train_data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    train_data[col] = train_data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    train_data[col] = train_data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    train_data[col] = train_data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    train_data[col] = train_data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    train_data[col] = train_data[col].astype(np.float32)\n                else:\n                    train_data[col] = train_data[col].astype(np.float64)\n        else:\n            train_data[col] = train_data[col].astype('category')\n\n    end_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return train_data","a8efc697":"data_dir = \"\/kaggle\/input\/g-research-crypto-forecasting\/\"\ncrypto_df = pd.read_csv(data_dir+\"train.csv\")","5b933fa1":"crypto_df = reduce_mem_usage(crypto_df)","6cfaf682":"crypto_df.head()","cc44cf57":"crypto_name = pd.read_csv(data_dir+\"asset_details.csv\")\ncrypto_name","1586187d":"#crypto_df.loc[crypto_df.Asset_ID == 2].plot(x = \"timestamp\",y = \"Target\")","fc3fbe78":"plot_df = crypto_df.merge(crypto_name,on = \"Asset_ID\")\nplot_df = plot_df.sort_values(by = ['Asset_Name','timestamp'])\nplot_df.head()","9ab2ef58":"assets = plot_df.Asset_Name.unique()\nassets","8c1edc26":"plot_df['change'] = plot_df['Close']\/plot_df['Open']\nplot_df['variation'] = plot_df['High']\/plot_df['Low']\nplot_df['change_diff'] = plot_df['Close'] - plot_df['Open']\nplot_df['variation_diff'] = plot_df['High'] - plot_df['Low']\ndef hlco_ratio(df): return (df['High'] - df['Low'])\/(df['Close']-df['Open'])\ndef upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\ndef lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\nplot_df['Upper_Shadow'] = upper_shadow(plot_df)\nplot_df['Lower_Shadow'] = lower_shadow(plot_df)\nplot_df['hlco_ration'] = hlco_ratio(plot_df)","f6a5cb74":"del crypto_df","cae37a5f":"#def reindex(df):\n#    df = df.reindex(range(df.index[0],df.index[-1]+60,60),method='nearest')\n#    return df\nplot_df = plot_df.sort_values(by = [\"Asset_ID\",'timestamp'])\n#plot_df = plot_df.set_index(['timestamp',\"Asset_ID\"])\n#plot_df = reindex(plot_df)","bd58067e":"plot_df2 = pd.DataFrame()\ndef reindex(df):\n    df = df.reindex(range(df.index[0],df.index[-1]+60,60),method='nearest')\n    df = df.assign(filt = (df.index - df.index[0])\/(df.index[-1]+60 - df.index[0]))\n    return df\nfor asset_id,asset_name in enumerate(assets):\n    df = plot_df.loc[plot_df[\"Asset_ID\"] == asset_id]\n    df = df.set_index(['timestamp'])\n    df = reindex(df)\n    plot_df2 = plot_df2.append(df)\nplot_df2.head()","879c638e":"plot_df2.shape","67c3840b":"plot_df2_train = plot_df2.loc[plot_df2[\"filt\"]<=0.7]\nplot_df2_test = plot_df2.loc[plot_df2[\"filt\"]>0.7]\n[plot_df2_train.shape,plot_df2_test.shape]","a621e644":"#del plot_df2\n[x_train,y_train,x_test,y_test] = [plot_df2_train.drop([\"Target\",\"Asset_Name\",\"Weight\"],axis=1),\n                                  plot_df2_train.Target,\n                                  plot_df2_test.drop([\"Target\",\"Asset_Name\",\"Weight\"],axis=1),\n                                  plot_df2_test.Target]\n\n\n[x_train.shape,y_train.shape,x_test.shape,y_test.shape] \n","6b85f7d2":"weight_train,weight_test = plot_df2_train.Weight,plot_df2_test.Weight\n","9f5fdef1":"x_train.to_csv(\"x_train.csv\")","0533e0b8":"x_test.to_csv(\"x_test.csv\")\ny_train.to_csv(\"y_train.csv\")\ny_test.to_csv(\"y_test.csv\")\nweight_train.to_csv(\"weight_train.csv\")\nweight_test.to_csv(\"weight_test.csv\")\n\n","2b3351b9":"#  Adding features","f55e7d66":"# Data Creation\n\nI have been trying to build models on this dataset, but due to the size I felt it would be more comfortable to break it up into two parts. In this notebook I will create datasets with additional variables, and I will later use them in a separate notebook to create predictions and submit. \n\nThe second part can be found here - [https:\/\/www.kaggle.com\/craniket\/g-research-lgbm-basic-model](http:\/\/)","208d1c29":"#  Re-indexing\n\nThe timestamps are not continuous for each Crypto. We need to reindex each individually.","c2355548":"Here we define a function to reduce the size of the data."}}