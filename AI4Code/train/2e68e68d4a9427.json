{"cell_type":{"8c3df83a":"code","bfb77eaa":"code","147b0e36":"code","2a56ecd2":"code","b65cf2c0":"code","9fbc6fea":"code","96a73660":"code","5828edec":"code","8920bf78":"code","bffe38af":"code","4081d5fc":"code","925938ab":"code","40f4cab1":"code","f35da140":"markdown","424a336e":"markdown","c2e561bb":"markdown"},"source":{"8c3df83a":"# Importing the libraries and reading the data as always\nimport pandas as pd\nimport hashlib\nimport copy\nimport numpy as np\nfrom datetime import datetime\n\nfrom sklearn.svm import SVC\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import cross_validate, StratifiedKFold\nfrom sklearn.metrics import f1_score, make_scorer, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom matplotlib import pyplot as plt","bfb77eaa":"data = pd.read_csv('..\/input\/malicious-and-benign-websites\/dataset.csv')\nlen_data = len(data)\ndata","147b0e36":"# First of all, it's necessary to check our data for missing values and fill NAs with mode\n# There are 'None' values in some columns, but I decided to keep them as these values say\n# that the domain server didn't provide this information\nfor col in data.drop(columns=['Type']).columns:\n  if data[col].isna().any():\n    mode = data[col].mode().iloc[0]\n    print(f\"{col} contains NAs, replacing with {mode}\")\n    data[col] = data[col].fillna(mode)","2a56ecd2":"# Let's replace datetime features with their UNIX Time representation;\n# it's more native to the classifiers and easier for feature engineering as it has numerical type\n# instead of datetime string\n# There are two datetime formats that are processed with different masks\n# and some incorrect values ('None', 'b', '0') that we replace with -1\nREGDATE_TS = []\n\nfor date in data.WHOIS_REGDATE.values:\n    if date not in ['None', 'b', '0']:\n        try:\n            REGDATE_TS.append(int(datetime.strptime(date, '%d\/%m\/%Y %H:%M').timestamp()))\n        except ValueError:\n            REGDATE_TS.append(int(datetime.strptime(date, '%Y-%m-%dT%H:%M:%S.0Z').timestamp()))\n    else:\n        REGDATE_TS.append(-1)\n        \nUPD_TS = []\n\nfor date in data.WHOIS_UPDATED_DATE.values:\n    if date not in ['None', 'b', '0']:\n        try:\n            UPD_TS.append(int(datetime.strptime(date, '%d\/%m\/%Y %H:%M').timestamp()))\n        except ValueError:\n            UPD_TS.append(int(datetime.strptime(date, '%Y-%m-%dT%H:%M:%S.0Z').timestamp()))\n    else:\n        UPD_TS.append(-1)","b65cf2c0":"data['REGDATE_TS'] = REGDATE_TS\ndata['UPD_TS'] = UPD_TS\n\ndata = data.drop(columns=['WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']) # dropping original string features\ndata","9fbc6fea":"# We have some columns with Object dtype; let's apply one hot encoding\n# (if the number of unique values is relatively small)\n# or hashing if there are many uniques\ncols_to_drop = []\n\nfor col in data.drop(columns=['Type']).columns:\n  if data[col].dtype == 'object':\n    print(f'Column {col} has {data[col].nunique()} values among {len_data}')\n\n    if data[col].nunique() < 25:\n      print(f'One-hot encoding of {col}')\n      one_hot_cols = pd.get_dummies(data[col])\n      for ohc in one_hot_cols.columns:\n        data[col + '_' + ohc] = one_hot_cols[ohc]\n    else:\n      print(f'Hashing of {col}')\n      data[col + '_hash'] = data[col].apply(lambda row: int(hashlib.sha1((col + \"_\" + str(row)).encode('utf-8')).hexdigest(), 16) % len_data)\n\n    cols_to_drop.append(col)\n    \ndata = data.drop(columns=cols_to_drop)","96a73660":"# We will fit the models on two different sets of features, so the fitting process is moved to\n# this function\ndef corr_clean_fit_validate(data: pd.DataFrame):\n    # Cleaning the data from highly correlated features is also performed here,\n    # as the sets of features and thus, the correlation coefficient values\n    # are different for each fit\n    corr = data.drop(columns=[\"Type\"]).corr()\n    corr_top = corr.abs().unstack().sort_values(kind='quicksort')\n    corr_top = corr_top[corr_top > 0.9][corr_top < 1]\n\n    cols_to_drop = [corr_top.index[i][0] for i in range(0, len(corr_top), 2)]\n    print(f\"Highly correlated features: {cols_to_drop}\")\n    data = data.drop(columns=cols_to_drop)\n    \n    # It's crucial for SVM to have data in small range, so let's scale it\n    ss = StandardScaler()\n    data_scaled = pd.DataFrame(columns=data.drop(columns=['Type']).columns,\n                                   data=ss.fit_transform(data.drop(columns=['Type']), data.Type))\n    data_scaled['Type'] = data.Type\n    \n    X = data_scaled.drop(columns=['Type'])\n    Y = data_scaled.Type\n    \n    # Fit and cross-validate SVC and CBC\n    svc = SVC(random_state=42, verbose=0)\n    cbc = CatBoostClassifier(random_state=42, verbose=0)\n    \n    f1_scorer = make_scorer(f1_score)\n    auc_scorer = make_scorer(roc_auc_score)\n    \n    cv_svc = cross_validate(svc, X, Y, cv=StratifiedKFold(random_state=42, shuffle=True), scoring=['f1_weighted', 'roc_auc'],\n                           n_jobs=-1, verbose=0)\n    print(f\"Average SVM F1 on CV is {round(np.mean(cv_svc['test_f1_weighted']), 4)}\")\n    print(f\"Average SVM AUC on CV is {round(np.mean(cv_svc['test_roc_auc']), 4)}\")\n    \n    cv_cbc = cross_validate(cbc, X, Y, cv=StratifiedKFold(random_state=42, shuffle=True), scoring=['f1_weighted', 'roc_auc'],\n                           n_jobs=-1, verbose=0)\n    print(f\"Average CatBoost F1 on CV is {round(np.mean(cv_cbc['test_f1_weighted']), 4)}\")\n    print(f\"Average CatBoost AUC on CV is {round(np.mean(cv_cbc['test_roc_auc']), 4)}\")","5828edec":"corr_clean_fit_validate(data)","8920bf78":"# Let's check the feature distributions for legal and malicious sites separately\ndata_legal = data[data.Type == 0]\ndata_fraud = data[data.Type == 1]","bffe38af":"# The distributions of all other features can be visualized in similar way,\n# but I found these two the most appropriate to construct new features\nfor col in ['URL_LENGTH', 'NUMBER_SPECIAL_CHARACTERS']:\n    print(f\"Showing {col}\")\n    plt.figure(figsize=(20, 10))\n    range_min = min(data_legal[col].min(), data_fraud[col].min())\n    range_max = max(data_legal[col].max(),data_fraud[col].max())\n    plt.hist(data_legal[col].values, bins=20, range=(range_min, range_max))\n    plt.hist(data_fraud[col].values, bins=20, range=(range_min, range_max))\n    plt.xticks(range(0, int(range_max), int(range_max \/ 20)))\n    plt.legend(['Legal sites', 'Malicious sites'])\n    plt.show()","4081d5fc":"# We can see that there are almost no legal sites with URLs longer than 120\n# and 24+ special characters in the URL; let's add the flags to tell the classifier about it directly\ndata['LONG_URL'] = [1 if url_len > 120 else 0 for url_len in data.URL_LENGTH.values]\ndata['MANY_SPEC_CHARS'] = [1 if n_specs > 24 else 0 for n_specs in data.NUMBER_SPECIAL_CHARACTERS.values]","925938ab":"# Let's add timestamp-related features assuming that the last timestamp in dataset equals to data acquisition time\n# We'll find the timestamp difference of the last time point and timestamp in URL's data\n# and add flags that mark recently registered\/updated domains\n# (they are potentially more malicious from the point of view of common sense)\nmax_reg_ts = data.REGDATE_TS.max()\nmax_upd_ts = data.UPD_TS.max()\n\ndata['REG_TS_DIFF'] = [max_reg_ts - reg_ts if reg_ts != -1 else max_reg_ts for reg_ts in data.REGDATE_TS.values]\ndata['REG_30DAYS'] = [1 if reg_ts != -1 and (max_reg_ts - reg_ts) <= 2592000 else 0 for reg_ts in data.REGDATE_TS.values]\ndata['UPD_TS_DIFF'] = [max_upd_ts - upd_ts if upd_ts != -1 else max_upd_ts for upd_ts in data.UPD_TS.values]\ndata['UPD_30DAYS'] = [1 if upd_ts != -1 and (max_upd_ts - upd_ts) <= 2592000 else 0 for upd_ts in data.UPD_TS.values]","40f4cab1":"corr_clean_fit_validate(data)","f35da140":"In this notebook, we will solve a web domains binary classification task using Sklearn's SVM, CatBoost and some basic feature engineering.","424a336e":"Our additional features slightly increased the classifiers' performance, making it even better from the baseline's good results, so they seem to be informative and may be applied in practic-oriented problems.\n\nThanks for your attention :)","c2e561bb":"Both classifiers perform well, but SVM's score looks like a couple of metrics to improve.\n\nLet's try to enhance our solution by adding some new features."}}