{"cell_type":{"b23ddd6b":"code","9eefdf5e":"code","7660e146":"code","3625c712":"code","25a3f28f":"code","91cd2cef":"code","85b628da":"code","71f5f111":"code","ac9c1372":"code","24f7d95b":"code","c5f8fce0":"code","4f926b50":"code","5e8d25b7":"code","3f61bc48":"code","05194fc0":"code","26f95ebb":"markdown","304882a2":"markdown","3d0c0220":"markdown"},"source":{"b23ddd6b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9eefdf5e":"# Read datasets to pandas dataframe\ndf_train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/test.csv')\ndf_sample_submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')","7660e146":"# Checking out df_train\ndf_train.describe()","3625c712":"# Lets see if we have any missing values\nmissing_values_train = df_train.isna().any().sum()\nmissing_values_test = df_test.isna().any().sum()\nprint(f'There are {missing_values_train} missing values in the train dataset')\nprint(f'There are {missing_values_test} missing values in the test dataset')","25a3f28f":"# Lets see which features are the most correlated with target\ndf_train.corr()['Cover_Type'].sort_values()","91cd2cef":"# Lets establish a baseline if we just always predict the target's most common class\n# AKA: null accuracy\ndf_train['Cover_Type'].value_counts(normalize=True).head(1)","85b628da":"# How imbalanced are the class distrubutions in our target variable?\ndf_train.groupby('Cover_Type').size()","71f5f111":"df_train = df_train[df_train['Cover_Type']!=5]","ac9c1372":"train=df_train\ntest=df_test","24f7d95b":"# Get train data without the target and ids\nX = train.iloc[:, 1:-1].copy()\n# Get the target\ny = train.Cover_Type.copy()\n\n# Create test X, drop ids.\ntest_X = test.iloc[:, 1:].copy()","c5f8fce0":"from sklearn.model_selection import train_test_split\n\nX_train, X_validate, Y_train, Y_validate = train_test_split( X, y, test_size=0.2, random_state=2)\nprint ('Train set:', X_train.shape,  Y_train.shape)\nprint ('Validation set:', X_validate.shape,  Y_validate.shape)","4f926b50":"X_train","5e8d25b7":"# Create LightGBM model\nfrom lightgbm import LGBMClassifier\n\nlgb_params = {\n    'objective' : 'multiclass',\n    'metric' : 'multi_logloss',\n    'device' : 'gpu',\n}\n\nlgbmmodel = LGBMClassifier(**lgb_params) \n\nlgbmmodel.fit(X_train,Y_train,\n               early_stopping_rounds=200,\n               eval_set=[(X_validate,Y_validate)],\n               verbose=True)\n\n# R^2 for training data\nlgbmmodel.score(X_train,Y_train)","3f61bc48":"# View sample submission\ndf_sample_submission","05194fc0":"# Rename df and replace the cover type column with our predictions\ndf_lgbm_submission = df_sample_submission\ndf_lgbm_submission['Cover_Type'] = lgbmmodel.predict(test_X).astype('int')\ndf_lgbm_submission.to_csv(\"submission.csv\",index=False)","26f95ebb":"## LGBM","304882a2":"# Modeling","3d0c0220":"# Data Preprocessing"}}