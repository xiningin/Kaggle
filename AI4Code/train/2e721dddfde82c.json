{"cell_type":{"bcf9ce87":"code","35e5fe30":"code","0e797ead":"code","3c5fc596":"code","165e782b":"code","d6c04d6b":"code","13b9f627":"code","b3c10262":"code","de995daf":"code","c752917a":"code","a25aa29c":"code","19350dc6":"code","c00063dd":"code","ae97b1bf":"code","132a5520":"code","b2d9c8ba":"code","dddfc83e":"code","eb530204":"code","5cb31823":"code","110088db":"code","b827c3f4":"code","bcbc5b4a":"code","f61baaf8":"code","97ced381":"code","0ea4d23b":"code","61a5434b":"code","fe6c4d3f":"code","dbcca828":"code","33113eb9":"code","6b0212b5":"markdown","a2770bdf":"markdown","5457b649":"markdown","12abb2a3":"markdown","acd82656":"markdown","472be35c":"markdown","41180472":"markdown","7f496aca":"markdown","39fc9652":"markdown","a2018f6c":"markdown","af0133c4":"markdown","7e7a833f":"markdown"},"source":{"bcf9ce87":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Display Image\nfrom PIL import Image\n\n# computer vision package to read dataset\nimport cv2\nimport os","35e5fe30":"# check gpu device\nimport tensorflow as tf\ntf.test.gpu_device_name()","0e797ead":"train_folder = \"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/\"\ntrain_n = \"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL\/\"\ntrain_p = \"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\/\"","3c5fc596":"norm_image = Image.open(train_n +'IM-0115-0001.jpeg')\npneumonia_image = Image.open(train_p + 'person1000_bacteria_2931.jpeg')\n\n# plot images\n# subplotting image data\nfig = plt.figure(figsize=(12,10))\na1 = fig.add_subplot(1,2,1)\nimg_plot = plt.imshow(norm_image, cmap = plt.cm.bone)\na1.set_title('Normal X-Ray', fontsize=12)\n\na1 = fig.add_subplot(1,2,2)\nimg_plot = plt.imshow(pneumonia_image, cmap = plt.cm.bone)\na1.set_title('Pneumonia X-Ray', fontsize=12)","165e782b":"train_folder = \"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/\"\ntest_folder = \"..\/input\/chest-xray-pneumonia\/chest_xray\/test\/\"\nval_folder = \"..\/input\/chest-xray-pneumonia\/chest_xray\/val\/\"","d6c04d6b":"from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom numpy import expand_dims","13b9f627":"img = load_img(train_n +\"IM-0115-0001.jpeg\")\ndata = img_to_array(img)\nsamples = expand_dims(data,0)","b3c10262":"samples # RGB values of pixels","de995daf":"datagen = ImageDataGenerator(zoom_range=0.3) # applying transformation of zoom range\nIDG = datagen.flow(samples, batch_size=1)\nfig, ax = plt.subplots(1,5,figsize= (20,15))\n\nfor i in range(5):\n    fig = plt.figure()\n    batch = IDG.next()\n    image = batch[0].astype('uint8')\n    ax[i].imshow(image)\nplt.show()","c752917a":"datagen = ImageDataGenerator(brightness_range= [0.8, 1.3])\nIDG = datagen.flow(samples, batch_size=1)\nfig, ax = plt.subplots(1,5,figsize= (20,15))\n\nfor i in range(5):\n    fig = plt.figure()\n    batch = IDG.next()\n    image = batch[0].astype('uint8')\n    ax[i].imshow(image)\nplt.show()","a25aa29c":"datagen = ImageDataGenerator(shear_range= 0.2)\nIDG = datagen.flow(samples, batch_size=1)\nfig, ax = plt.subplots(1,5,figsize= (20,15))\n\nfor i in range(5):\n    fig = plt.figure()\n    batch = IDG.next()\n    image = batch[0].astype('uint8')\n    ax[i].imshow(image)\nplt.show()","19350dc6":"datagen = ImageDataGenerator(width_shift_range= 0.1, height_shift_range=0.1)\nIDG = datagen.flow(samples, batch_size=1)\nfig, ax = plt.subplots(1,5,figsize= (20,15))\n\nfor i in range(5):\n    fig = plt.figure()\n    batch = IDG.next()\n    image = batch[0].astype('uint8')\n    ax[i].imshow(image)\nplt.show()","c00063dd":"train_datagen = ImageDataGenerator(rescale= 1.\/255, shear_range=0.2, zoom_range= 0.2 , height_shift_range= 0.1, width_shift_range= 0.1, \n                                   fill_mode='nearest')\ntest_datagen = ImageDataGenerator (rescale=1.\/255) # for testing, we neever make any modifications, only rescaling can be done","ae97b1bf":"train_set = train_datagen.flow_from_directory(train_folder, target_size=(224,224), batch_size= 32, class_mode='categorical')\ntest_set = test_datagen.flow_from_directory(test_folder, target_size=(224,224), batch_size= 32, class_mode='categorical')\nval_set = test_datagen.flow_from_directory(val_folder, target_size=(224,224), batch_size= 32, class_mode='categorical')","132a5520":"# checking image shape\ntrain_set.image_shape","b2d9c8ba":"from keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout, BatchNormalization, ZeroPadding2D #average pooling, global average pooling can be taken as well, \nfrom keras.optimizers import Adam","dddfc83e":"def build_model():\n    model = Sequential()\n\n    # Convolutional layer - I\n    model.add(ZeroPadding2D((1,1), input_shape = train_set.image_shape))\n    model.add(Conv2D(filters= 64, kernel_size=(3,3), activation= 'relu'))\n    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\n    # Convolutional layer - II\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Conv2D(filters= 128, kernel_size=(3,3), activation= 'relu'))\n    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\n    # Convolutional layer - III\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Conv2D(filters= 256, kernel_size=(3,3), activation= 'relu'))\n    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\n    # Convolutional layer - IV\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Conv2D(filters= 512, kernel_size=(3,3), activation= 'relu'))\n    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\n    model.add(Flatten())\n\n    # Fully Connected Layer\n    model.add(Dense(units = 256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(units= 2, activation='softmax'))\n\n    # Learning Rate, optimizers-Adam\n    adam_optimizer = Adam(learning_rate= 0.0001)\n\n    # loss = categorical_crossentropy\n    model.compile(optimizer = adam_optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","eb530204":"model = build_model()\nmodel.summary()","5cb31823":"from keras import callbacks\nfilepath = \".\/chest_xray_cnn.hdf5\"\ncheckpoint = callbacks.ModelCheckpoint(filepath, monitor= 'val_loss', save_best_only=True, mode ='min', verbose=1)\ncheckpoint","110088db":"import datetime\nimport keras\nlogdir = os.path.join(\".\/logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = keras.callbacks.TensorBoard(logdir)","b827c3f4":"# fit_generator() is used if images are generated by ImageDataGenerator()\nhistory = model.fit_generator(generator=train_set, epochs=15, shuffle=True, \n                              validation_data= val_set, steps_per_epoch=163, \n                              callbacks =[checkpoint, tensorboard_callback], verbose=1) # steps_per_epoch = number of samples\/ batch_size 5216\/32","bcbc5b4a":"%reload_ext tensorboard\n%tensorboard --logdir logs","f61baaf8":"model.load_weights('.\/chest_xray_cnn.hdf5')","97ced381":"model.evaluate(test_set)","0ea4d23b":"yhat = np.argmax(model.predict(test_set), axis=1)","61a5434b":"yhat","fe6c4d3f":"test_set.class_indices","dbcca828":"ytest = test_set.class_indices","33113eb9":"from sklearn.metrics import confusion_matrix, classification_report\n\nconfusion_matrix(ytest, yhat)","6b0212b5":"### Display Chest X-Ray Image","a2770bdf":"### Brigthness of the Image","5457b649":"### Using saved model","12abb2a3":"### Data Augmentation\n* All RGB Image data will be used to form new training samples using Image Data Generator\n* It creates complete new samples by transforming the images by shear range, zoom range, horizontal flip, rotation of image, etc","acd82656":"### Classification Report","472be35c":"**Train and Test Images Data Augementation**","41180472":"### Hieight and  Width Shift","7f496aca":"### Shear Range****","39fc9652":"### Tensorboard","a2018f6c":"### Tensorboard - Logs","af0133c4":"### Applying Zoom on Image Data","7e7a833f":"### CNN Modelling"}}