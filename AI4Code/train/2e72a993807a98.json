{"cell_type":{"fc3930da":"code","adac8f3f":"code","31171ace":"code","81dfcbe7":"code","01f16c21":"code","3c0790ec":"code","a566e813":"code","648ef49a":"code","49cc81b5":"code","bee9d5f6":"code","85156bfd":"code","5d00c14d":"code","fec477fc":"code","7fd7020a":"code","639cfc76":"code","00ece940":"code","c366ce40":"code","dd24034d":"code","6a7a71ec":"code","c5d21fea":"code","da36a63d":"code","3a26780d":"code","6c376fca":"code","64edb912":"code","575cd7db":"code","8db26bc4":"code","af992e74":"code","7872f3c7":"code","74a58175":"code","64463f8f":"code","ac05b655":"code","6b567f68":"code","9abede60":"code","4d498441":"code","152dc7df":"code","cddc8110":"code","236976a9":"code","e1edfbf2":"code","ca2e8349":"code","d6943658":"code","8694bf23":"code","b6c107a9":"markdown","3d692dee":"markdown","097fb6f7":"markdown","b3174a75":"markdown","6828a2a6":"markdown","9cbc1225":"markdown","a4d1d780":"markdown","a6ca7781":"markdown","cd5ed054":"markdown","c246a41d":"markdown","c9faa069":"markdown","e4dc8e2b":"markdown","8db48578":"markdown","ab6643e1":"markdown"},"source":{"fc3930da":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\ncolor = sns.color_palette()\n%matplotlib inline\nfrom sklearn.experimental import enable_iterative_imputer # to impute logerror\nfrom sklearn.impute import IterativeImputer","adac8f3f":"prop2016 = pd.read_csv(\"..\/input\/group-9-data-set\/2016prop.csv\", low_memory = False)\ntrain2016 = pd.read_csv(\"..\/input\/group-9-data-set\/train_2016_v2.csv\", parse_dates=[\"transactiondate\"], low_memory = False)\npd.set_option('display.max_columns', None)","31171ace":"prop2016.head()","81dfcbe7":"train2016.head()","01f16c21":"# To find the distribution of logerror \nulimit = np.percentile(train2016.logerror.values, 99)\nllimit = np.percentile(train2016.logerror.values, 1)\n\nplt.figure(figsize=(12,4))\nsns.distplot(train2016.logerror.values, bins=500, kde=False)\nplt.xlabel(\"Log Error\", fontsize=12)\nmu, std = norm.fit(train2016.logerror.values)\nxmin,xmax=plt.xlim()\nx = np.linspace(-5,5,1000)\np = norm.pdf(x, 0, 0.025)\nplt.plot(x, p*(x[1]-x[0])*90811, 'r', linewidth=2)\nplt.axis([-0.5,0.5,0,20000])\nplt.ylabel(\"Number of Parcels\", fontsize=12)\nplt.title(\"Train 2016 Dataset\")","3c0790ec":"\nid_feature=['airconditioningtypeid','architecturalstyletypeid','buildingclasstypeid',\n           'buildingqualitytypeid','decktypeid','hashottuborspa','heatingorsystemtypeid',\n           'pooltypeid10','pooltypeid2','pooltypeid7','propertylandusetypeid',\n            'storytypeid','typeconstructiontypeid','fireplaceflag','taxdelinquencyflag',\n            'taxdelinquencyyear']\ncnt_feature=['bathroomcnt','bedroomcnt','calculatedbathnbr','fireplacecnt','fullbathcnt',\n            'garagecarcnt','garagetotalsqft','poolcnt','roomcnt','threequarterbathnbr',\n            'unitcnt','yearbuilt','numberofstories','assessmentyear']\nsize_feature=['basementsqft','finishedfloor1squarefeet','calculatedfinishedsquarefeet',\n             'finishedsquarefeet12','finishedsquarefeet13','finishedsquarefeet15',\n              'finishedsquarefeet50','finishedsquarefeet6','lotsizesquarefeet',\n             'poolsizesum','yardbuildingsqft17','yardbuildingsqft26','structuretaxvaluedollarcnt','taxvaluedollarcnt',\n             'landtaxvaluedollarcnt','taxamount','latitude','longitude']\nlocation_feature=['fips','propertycountylandusecode','rawcensustractandblock',\n                 'regionidcity','regionidcounty','regionidneighborhood','regionidzip','censustractandblock']\nstr_feature=['propertyzoningdesc','propertycountylandusecode']","a566e813":"# data type analysis\ndtype_df16 = prop2016.dtypes.reset_index()\ndtype_df16.columns = [\"Feature\", \"Column Type\"]\ndtype_df16.groupby(\"Column Type\").aggregate('count').reset_index()","648ef49a":"# feature as object \n# look into non-numeric features\ndtype_df16[dtype_df16['Column Type']=='object']['Feature']","49cc81b5":"# values counts for propertycountylandusecode\nprint(prop2016['propertycountylandusecode'].value_counts())","bee9d5f6":"# values counts for propertyzoningdesc\nprint(prop2016['propertyzoningdesc'].value_counts())","85156bfd":"prop2016['propertytype'] = 'NR'\nprop2016.loc[prop2016['propertyzoningdesc'].str.contains('r', na=False, case=False), 'propertytype'] = 'R'","5d00c14d":"prop2016.head()","fec477fc":"plt.pie\nprop2016.propertytype.value_counts().plot(kind='pie', autopct='%1.0f%%', fontsize=12)\nplt.axis('equal')\nplt.title('Residential (R) and NonResidential (NR) for year 2016')","7fd7020a":"#https:\/\/thispointer.com\/pandas-count-rows-in-a-dataframe-all-or-those-only-that-satisfy-a-condition\/\nTotalNumOfRows16 = len(prop2016.index)\nseriesObj16 = prop2016.apply(lambda x: True if x['propertytype'] == 'R' else False, axis=1)\nnumOfRows16 = len(seriesObj16[seriesObj16 == True].index)\nprint('Property 2016 Dataset')\nprint('Number of Residential Properties: ', numOfRows16)\nprint (\"Number of Non-Residential Properties: \", TotalNumOfRows16 - numOfRows16)","639cfc76":"# value count for object features\nprint(prop2016['hashottuborspa'].value_counts())\nprint(prop2016['fireplaceflag'].value_counts())\nprint(prop2016['taxdelinquencyflag'].value_counts())","00ece940":"# drop the five object columns\nprop2016.drop(dtype_df16[dtype_df16['Column Type']=='object']['Feature'].values.tolist(), axis=1,inplace=True)","c366ce40":"prop2016.shape","dd24034d":"prop2016.head()","6a7a71ec":"# check the missing percentage\nmissing_df16 = prop2016.isnull().sum(axis=0).reset_index()\nmissing_df16.columns = ['column_name', 'missing_count']\nmissing_df16 = missing_df16.ix[missing_df16['missing_count']>0]\nmissing_df16 = missing_df16.sort_values(by='missing_count')\nind16 = np.arange(missing_df16.shape[0])\nwidth16 = 0.9\nfig, ax = plt.subplots(figsize=(12,14))\nrects16 = ax.barh(ind16, missing_df16.missing_count.values\/2985217*100, color='g')\nax.set_yticks(ind16)\nax.set_yticklabels(missing_df16.column_name.values, rotation='horizontal',fontsize=12)\nax.set_xlabel(\"Percentage of missing values\",fontsize=12)\nax.set_title('Property 2016 Data Set Missing Values')","c5d21fea":"# missing rate of the data\nmissing_df16['missing_rate']=missing_df16['missing_count']\/2985217\ncutoff=0.9\nprint(missing_df16[missing_df16.missing_rate<cutoff].shape)\nprint('There are',missing_df16[missing_df16.missing_rate<cutoff].shape[0],'features of which the percentage of missing values is less than',cutoff*100,'% in the 2016 property dataset.')\nmissing_df16[(missing_df16.missing_rate<cutoff)].column_name.values","da36a63d":"# drop feature missing rate>0.9\nprop2016.drop(missing_df16[(missing_df16.missing_rate>=cutoff)].column_name.values.tolist(),\n                    axis=1,inplace=True)","3a26780d":"# fill missing values\n# for id_feature, fill the missing values with most frequent value\n# for cnt_feature, fill the missing values with median value\n# for size_feature, fill the missing values with mean values\n# for location_feature, fill the missing values with the nearest values\n# categorize the left feature\nfeature_left16=prop2016.columns.tolist()\nid_feature_left16=list()\ncnt_feature_left16=list()\nsize_feature_left16=list()\nlocation_feature_left16=list()\nfor x in feature_left16:\n    if x in id_feature:\n        id_feature_left16.append(x)\n    elif x in cnt_feature:\n        cnt_feature_left16.append(x)\n    elif x in size_feature:\n        size_feature_left16.append(x)\n    elif x in location_feature:\n        location_feature_left16.append(x)\n\n# fill missing values\n# for id_feature, fill the missing values with most frequent value\n# for cnt_feature, fill the missing values with median value\n# for size_feature, fill the missing values with mean values\n# for location_feature, fill the missing values with the most frequent values\nfill_missing_value16=dict()\n# for id_feature\nfor x in id_feature_left16:\n    fill_missing_value16[x]=0    # prop2016[x].value_counts().index.tolist()[0]\n# for cnt_feature\nfor x in cnt_feature_left16:\n    fill_missing_value16[x]=prop2016[x].median()\n# for size_feature\nfor x in size_feature_left16:\n    fill_missing_value16[x]=prop2016[x].mean()\n# for size_feature\nfor x in location_feature_left16:\n    fill_missing_value16[x]=0     #prop2016[x].value_counts().index.tolist()[0]\nfor x in fill_missing_value16:\n    prop2016[x].fillna(fill_missing_value16[x],inplace=True)","6c376fca":"clean2016 = prop2016[['parcelid', 'calculatedfinishedsquarefeet', 'regionidzip', 'structuretaxvaluedollarcnt', 'yearbuilt']].copy()","64edb912":"clean2016.head()","575cd7db":"#https:\/\/stackoverflow.com\/questions\/44593284\/python-pandas-dataframe-merge-and-pick-only-few-columns\nclean2016=clean2016.merge(train2016[['parcelid', 'logerror']], on = 'parcelid', how='outer')","8db26bc4":"imp = IterativeImputer(max_iter=5, random_state=0)\nimp.fit(clean2016)  \nIterativeImputer(add_indicator=False, estimator=None,\n                 imputation_order='random', initial_strategy='median',\n                 max_iter=5, max_value=0.5, min_value=-0.5,\n                 missing_values='nan', n_nearest_features=None,\n                 sample_posterior=False, verbose=0)\nX_test16 = clean2016\n# the model learns that the second feature is double the first\nnp.round(imp.transform(X_test16))","af992e74":"clean2016i = imp.fit_transform(clean2016)\nprint(clean2016i)","7872f3c7":"clean2016i = pd.DataFrame(clean2016i, columns=['parcelid', 'calculatedfinishedsquarefeet', 'regionidzip',\n                                               'structuretaxvaluedollarcnt', 'yearbuilt', 'logerror'])","74a58175":"clean2016i.head()","64463f8f":"prop2016=prop2016.merge(clean2016i[['parcelid', 'logerror']], on = 'parcelid', how='outer')","ac05b655":"prop2016.head()","6b567f68":"## Read the distribution of logerror after imputation into prop2016 dataset\nulimit = np.percentile(train2016.logerror.values, 99)\nllimit = np.percentile(train2016.logerror.values, 1)\n\nplt.figure(figsize=(12,4))\nplt.title(\"Training 2016 Dataset Logerror\")\nsns.distplot(train2016.logerror.values, bins=500, kde=False)\nplt.xlabel(\"Log Error\", fontsize=12)\nmu, std = norm.fit(train2016.logerror.values)\nxmin,xmax=plt.xlim()\nx = np.linspace(-5,5,1000)\np = norm.pdf(x, 0, 0.025)\nplt.plot(x, p*(x[1]-x[0])*90811, 'r', linewidth=2)\nplt.axis([-0.5,0.5,0,20000])\nplt.ylabel(\"Number of Parcels\", fontsize=12)\nulimit = np.percentile(prop2016.logerror.values, 99)\nllimit = np.percentile(prop2016.logerror.values, 1)\n\nplt.figure(figsize=(12,4))\nsns.distplot(prop2016.logerror.values, bins=500, kde=False)\nplt.title(\"Imputed logerror into Property 2016 Dataset\")\nplt.xlabel(\"Log Error\", fontsize=12)\nmu, std = norm.fit(prop2016.logerror.values)\nxmin,xmax=plt.xlim()\nx = np.linspace(-5,5,1000)\np = norm.pdf(x, 0, 0.025)\nplt.plot(x, p*(x[1]-x[0])*90811, 'r', linewidth=2)\nplt.axis([-0.50,0.50,0.00,30000])\nplt.ylabel(\"Number of Parcels\", fontsize=12)","9abede60":"train2016v2=train2016.merge(prop2016[['parcelid', 'propertytype']], on = 'parcelid', how='inner')","4d498441":"train2016v2.head()","152dc7df":"sns.barplot(y=\"logerror\", x=\"propertytype\", data=train2016v2, order=[\"R\", \"NR\"]).set_title('Train 2016')","cddc8110":"sns.barplot(y=\"logerror\", x=\"propertytype\", data=prop2016, order=[\"R\", \"NR\"]).set_title('Prop2016')","236976a9":"sns.stripplot(x='propertytype', y='logerror', data=train2016v2, order=[\"R\", \"NR\"]).set_title('Train2016')","e1edfbf2":"sns.stripplot(x='propertytype', y='logerror', data=prop2016, order=[\"R\", \"NR\"]).set_title('Prop2016')","ca2e8349":"sns.stripplot(x='propertytype', y='logerror', data=train2016v2, order=[\"R\", \"NR\"]).set_title('Combined 2016')\nsns.stripplot(x='propertytype', y='logerror', data=prop2016, order=[\"R\", \"NR\"])","d6943658":"#Find the mean of NR and R properties, https:\/\/stackoverflow.com\/questions\/30482071\/how-to-calculate-mean-values-grouped-on-another-column-in-pandas\nprint(\"Train2016 dataset mean of logerror is: \", train2016['logerror'].mean(axis=0))\nprint(\"Prop2016 dataset mean of logerror is: \", prop2016['logerror'].mean(axis=0))","8694bf23":"print(\"Train2016 dataset mean of logerror for NR and R properties are: \", train2016v2.groupby('propertytype', as_index=False)['logerror'].mean())\nprint(\"\")\nprint(\"Prop2016 dataset mean of logerror for NR and R properties are: \", prop2016.groupby('propertytype', as_index=False)['logerror'].mean())","b6c107a9":"### Reviewing percentages of NR (non-residential) and R (residential) property types.","3d692dee":"## Reviewing logerror data after imputation.","097fb6f7":"### Categorizing features based on the definition of different features.","b3174a75":"## Imputing Logerror","6828a2a6":"# Group 9 - Imputation of Logerror","9cbc1225":"### Importing modules","a4d1d780":"### Read in Datasets\n","a6ca7781":"#### First part partially forked from Sukanya's notebook.","cd5ed054":"### Creating a new column for property type, based on propertyzoningdesc","c246a41d":"### Initial look at data","c9faa069":"### Checking the distribution of log error.","e4dc8e2b":"### Counting the number of rows in the property data sets.","8db48578":"### Dropping features with missing rate greater than 90%","ab6643e1":"### Comparring logerror by property type."}}