{"cell_type":{"cc216f5b":"code","a2e1c2f7":"code","7985b6a3":"code","4d07b8db":"code","53ee1cdd":"code","22904e98":"code","93ce5eaa":"code","4effdf8e":"code","3f3b5c35":"code","c4eb490e":"code","05aa279d":"code","1883af9a":"markdown","360ba1a1":"markdown","afda824c":"markdown","9f17a57f":"markdown","9eaf36b0":"markdown","d8324607":"markdown","476a8158":"markdown","fe5b8b14":"markdown","04a0008d":"markdown","a5e03d0b":"markdown","8192d42a":"markdown","a8dcdb66":"markdown","c2a5e076":"markdown","4b45eaff":"markdown","aa0a671a":"markdown","2dde14c8":"markdown","4f3f93a4":"markdown","50a1e3bd":"markdown","2c1f3c0b":"markdown","07c811d7":"markdown","88b60afa":"markdown","d5a0c766":"markdown","a5665313":"markdown","336063a4":"markdown","4dda6058":"markdown","ea16beb6":"markdown"},"source":{"cc216f5b":"#Numpy is used so that we can deal with array's, which are necessary for any linear algebra\n# that takes place \"under-the-hood\" for any of these algorithms.\n\nimport numpy as np\n\n\n#Pandas is used so that we can create dataframes, which is particularly useful when\n# reading or writing from a CSV.\n\nimport pandas as pd\n\n\n#Matplotlib is used to generate graphs in just a few lines of code.\n\nimport matplotlib.pyplot as plt\n\n\n#LinearRegression is the class of the algorithm we will be using.\n\nfrom sklearn.tree import DecisionTreeRegressor\n","a2e1c2f7":"#read the data from csv\ndataset = pd.read_csv('..\/input\/position-salaries\/Position_Salaries.csv')\n\n#set independent variable by using all rows, but just column 1.\nX = dataset.iloc[:, 1:2].values\n\n#set the dependent variable using all rows but only the last column. \ny = dataset.iloc[:, 2].values\n\n#take a look at our dataset\ndataset","7985b6a3":"#create an object of the DecisionTree class.\nregressor = DecisionTreeRegressor(random_state = 0)\n\n#fit it on the data, do not need to fit_transform her\nregressor.fit(X, y) ","4d07b8db":"#Create a grid, necessary because of the veritical jumps.\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\n\n#create a scatter plot\nplt.scatter(X, y, color = 'red')\n\n#plot the X values and the predictions \nplt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\n\n#Titles and labels.\nplt.title('Decision Tree Regression')\nplt.xlabel('Position level')\nplt.ylabel('Salary')\n\n#Show the plot.\nplt.show()","53ee1cdd":"# Predicting a new result\ny_pred = regressor.predict([[6.5]])\nprint(\"Predicted Salary: $\"+str(y_pred[0]))","22904e98":"#read the data from csv\ndataset = pd.read_csv('..\/input\/50-startups\/50_Startups.csv')\n\n#take a look at our dataset.  head() gives the first 5 lines. \ndataset.head()","93ce5eaa":"#drop the columns.\ndataset = dataset.drop(columns = ['Administration', 'State'])\n\n#look at the changes\ndataset.head()","4effdf8e":"#set independent variable by using all rows, but just column 1.\nX = dataset.iloc[:, :-1].values\n\n#set the dependent variable using all rows but only the last column. \ny = dataset.iloc[:, -1].values","3f3b5c35":"#create an object of the DecisionTree class.\nregressor = DecisionTreeRegressor(random_state = 0)\n\n#fit it on the data, do not need to fit_transform her\nregressor.fit(X, y) ","c4eb490e":"budgets = [[300000, 200000], [200000,300000], [100000,400000]]\n\nfor budget in budgets:\n    y_pred = regressor.predict([budget])\n    print(\"BUDGET:\", budget,\n          \"\\nPredicted Profit:\", y_pred)","05aa279d":"# First well create the 3d figure.\nfig = plt.figure(figsize = (10,10))\nax = plt.axes(projection='3d')\n\n#Next well pull out the datapoints for each axis\nzdata = y\nxdata = X[:, 0]\nydata = X[:, 1]\n#Now we plot the points \nax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='Reds', s = 50);\n\n#Next we need to make x and y dimensions for the data.\nxline = np.linspace(min(X[:, 0]), max(X[:, 0]), 50)\nyline = np.linspace(min(X[:, 1]), max(X[:, 1]), 50)\n#combine those back into a dataset to apply the prediction on \nz = np.concatenate((xline.reshape(-1,1),yline.reshape(-1,1)), axis = 1)\n#call the predictions \nzline = regressor.predict(z)\n#plot the resulting line. \nax.plot3D(xline, yline, zline, 'black')","1883af9a":"With the splitting complete, an average value is calculated for each **\u201cleaf node\u201d.**\n\n![image.png](attachment:image.png)","360ba1a1":"Now this obviously is not very accurate, and we only did it because its easier to visualize.\n\nLets repeat this with a more complex dataset, using info on start ups.","afda824c":"To keep this conceptually simple and avoid abstraction, we are using a very small dataset here.  \n\nBecause of this we will not be splitting part of it out into a training\/validation\/test set.\n\nIf we had data on 1000's of employees, we would absolutely complete this step.\n\nSo now we can fit the model in just 2 lines of code.","9f17a57f":"Now that I am happy with the data, I can name my x and y variables. ","9eaf36b0":"That's all it takes to fit the model.  \n\nNow we can visualize what it has predicted. \n","d8324607":"Once the algorithm determines the first split, this becomes the **\u201croot node\u201d.**\n\n![image.png](attachment:image.png)","476a8158":"## Conceptual Overview\n\nDecision Trees are very flexible algorithm and can be used in supervised or unsupervised contexts, for both classification and regression.\n\n![image.png](attachment:image.png)","fe5b8b14":"Here we see that either other the first two budgets are predicted to give a much higher profit than the third. \n\nNow lets try to visualize this, but be warned itll be a little more complicated.","04a0008d":"> ### This notebook is separated into two parts:\n\n**1) Conceptual Overview:**  I will introduce the topic in 200 words or less.\n\n**2) Implementation:**  I will implement the algorithm in as few lines as possible.","a5e03d0b":"**Regression** meaning we predict a numerical value, instead of a **\u201cclass\u201d.**\n\n![image.png](attachment:image.png)","8192d42a":"> # Enough to be Dangeous: Decision Tree Regression\n\n> ### This is the 4th notebook of my **\"Enough to be Dangeous\"** notebook series\n\nSee the other notebooks here:\n\n[Simple Linear regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangeous-simple-linear-regression)\n\n[Multiple Linear Regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-multiple-linear-regression)\n\n[Polynomial Regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-polynomial-regression)","a8dcdb66":"Now we can fit the new model.","c2a5e076":"Next, it evaluates the resulting subsets of data, and creates another split based on information gain.\n\n![image.png](attachment:image.png)","4b45eaff":"Decision trees split our data into subcategories which we use to predict some output variable.\n\n![image.png](attachment:image.png)","aa0a671a":"Now we can pass in new examples and predict their output.\n\n![image.png](attachment:image.png)","2dde14c8":"Now lets predict some values.\n\nTo make this more interesting, lets imagine the company has a fixed budget of $500,000\n\nThey are considering 3 different spending options:\n\nR&D: 300,000,  Marketing: 200,000\n\nR&D: 200,000,  Marketing: 300,000\n\nR&D: 100,000,  Marketing: 400,000","4f3f93a4":"They shine when working with datasets that have multiple \u201cpredictor\u201d variables, as these are notoriously hard to visualize.\n\n![image.png](attachment:image.png)","50a1e3bd":"I will be focusing on the \u201csupervised\u201d \u201cregression\u201d version.\n\n**Supervised** meaning we used labeled data to train the model.\n\n![image.png](attachment:image.png)","2c1f3c0b":"This is good, but I want to keep this even more simple, so I am going to drop the admin spend and state columns. ","07c811d7":"It continues with this process until hitting some predefined stopping point, such as a maximum depth or minimum sample size per subset.\n\n![image.png](attachment:image.png)","88b60afa":"With our imports complete, we now read in the data using Pandas.\n\nWe will set a independent variable (X) and a dependent variable (y).","d5a0c766":"Overall, Decision Trees are highly flexible and intuitive, but not the most accurate on their own. However, they are the foundation of other incredibly powerful algorithms.","a5665313":"![image.png](attachment:image.png)\n","336063a4":"To create these separations, the algorithm tests various splits, attempting to maximize **\u201cinformation gain\u201d** by reducing **\u201centropy\u201d**.\n\n![image.png](attachment:image.png)\n","4dda6058":"## Implementation\n\nIn this section I will implement the code in its simplest verison so that it is understandable if you are brand new to machine learning. \n\nBelow we will predict salary based on the current role someone is in.  This will demonstrate how it works and allow us to visualize it.  However, its performance shines on datasets with multiple independent variables, so we will apply it to one of those situations as well, but we will not be able to visualize the results. \n\n**The independent variables is**:\n* Position Level\n    \n**The dependent variable is** \n* Salary\n    \nThe first step is to start with \"imports\". These are \"libraries\" of pre-written code that will help us significantly.","ea16beb6":"Now we can use the trained model to predict new values. "}}