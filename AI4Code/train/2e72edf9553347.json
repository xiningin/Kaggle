{"cell_type":{"0183882b":"code","29a5c497":"code","320b8843":"code","5e68cfa6":"code","8fdd109d":"code","953f7ece":"code","eeb5e72e":"code","991ef437":"code","07b4dcf6":"code","7a355551":"code","228ab38a":"code","d07a02e4":"code","b22631fb":"code","be30ff49":"code","7f0c9f83":"code","96866fcf":"code","c29633a8":"code","67c6e1b1":"code","de1630af":"code","f26ae392":"code","7249e901":"code","ddda29a1":"code","27b3974a":"code","0878c623":"code","f8a8e682":"code","b4faf5cc":"code","7fd025e6":"code","c9038a2a":"code","2f3d6caf":"code","bd86b4a0":"code","9efb0c0e":"code","feef1f2c":"code","dcf7b19c":"code","be996a22":"code","44357f06":"code","e64d53c8":"code","8833e10e":"code","aa978a27":"code","ae742df2":"code","762525fa":"code","9daa11c8":"code","1e0e6795":"code","642862ce":"code","62520cb0":"markdown","b27412a5":"markdown","5ed9ca1f":"markdown","fc78cc30":"markdown","71115c3b":"markdown","14c268f7":"markdown","c7d631d5":"markdown","c4604a33":"markdown","8ebb1185":"markdown","d1a36392":"markdown","83cad589":"markdown","4180b880":"markdown","f6d09417":"markdown","5f93d069":"markdown","4adfe677":"markdown","7d0667ba":"markdown","eb2fc178":"markdown","c7692141":"markdown","de0b9834":"markdown","d6842348":"markdown","9aef5710":"markdown","159c2755":"markdown","5a1fe45d":"markdown","57686b81":"markdown","894af45a":"markdown","cbc0077e":"markdown"},"source":{"0183882b":"import numpy as np\nimport pandas as pd\nimport scipy.special\nimport matplotlib.pyplot as plt\nimport os\nimport random","29a5c497":"from keras.utils import Sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, LSTM, Embedding\nfrom keras.optimizers import RMSprop,Adam\nimport keras.backend as K","320b8843":"from sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler","5e68cfa6":"import warnings\nwarnings.filterwarnings(\"ignore\")","8fdd109d":"path_in = '..\/input\/ashrae-energy-prediction\/'\nprint(os.listdir(path_in))","953f7ece":"train_data = pd.read_csv(path_in+'train.csv', parse_dates=['timestamp'])\ntrain_weather = pd.read_csv(path_in+'weather_train.csv', parse_dates=['timestamp'])\nbuilding_data = pd.read_csv(path_in+'building_metadata.csv')","eeb5e72e":"def plot_bar(data, name):\n    fig = plt.figure(figsize=(16, 9))\n    ax = fig.add_subplot(111)\n    data_label = data[name].value_counts()\n    dict_train = dict(zip(data_label.keys(), ((data_label.sort_index())).tolist()))\n    names = list(dict_train.keys())\n    values = list(dict_train.values())\n    plt.bar(names, values)\n    ax.set_xticklabels(names, rotation=45)\n    plt.grid()\n    plt.show()","991ef437":"cols_with_missing_train_weather = [col for col in train_weather.columns if train_weather[col].isnull().any()]\ncols_with_missing_building = [col for col in building_data.columns if building_data[col].isnull().any()]","07b4dcf6":"print(cols_with_missing_train_weather)\nprint(cols_with_missing_building)","7a355551":"imp_most = SimpleImputer(strategy='most_frequent')\ntrain_weather[cols_with_missing_train_weather] = imp_most.fit_transform(train_weather[cols_with_missing_train_weather])\nbuilding_data[cols_with_missing_building] = imp_most.fit_transform(building_data[cols_with_missing_building])","228ab38a":"train_data['meter_reading'] = np.log1p(train_data['meter_reading'])","d07a02e4":"train_data['month'] = train_data['timestamp'].dt.month\ntrain_data['day'] = train_data['timestamp'].dt.weekday\ntrain_data['year'] = train_data['timestamp'].dt.year\ntrain_data['hour'] = train_data['timestamp'].dt.hour","b22631fb":"train_data['weekend'] = np.where((train_data['day'] == 5) | (train_data['day'] == 6), 1, 0)","be30ff49":"train_weather['wind_direction'+'_sin'] = np.sin((2*np.pi*train_weather['wind_direction'])\/360)\ntrain_weather['wind_direction'+'_cos'] = np.cos((2*np.pi*train_weather['wind_direction'])\/360)\ntrain_weather = train_weather.drop(['wind_direction'], axis=1)","7f0c9f83":"train_data = pd.get_dummies(train_data, columns=['meter'])","96866fcf":"features_cyc = {'month' : 12, 'day' : 7, 'hour' : 24}\nfor feature in features_cyc.keys():\n    train_data[feature+'_sin'] = np.sin((2*np.pi*train_data[feature])\/features_cyc[feature])\n    train_data[feature+'_cos'] = np.cos((2*np.pi*train_data[feature])\/features_cyc[feature])\ntrain_data = train_data.drop(features_cyc.keys(), axis=1)","c29633a8":"plot_bar(building_data, 'primary_use')","67c6e1b1":"map_use = dict(zip(building_data['primary_use'].value_counts().sort_index().keys(),\n                     range(1, len(building_data['primary_use'].value_counts())+1)))","de1630af":"building_data['primary_use'] = building_data['primary_use'].replace(map_use)","f26ae392":"#building_data = pd.get_dummies(building_data, columns=['primary_use'])","7249e901":"weather_scale = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'sea_level_pressure', 'wind_speed']","ddda29a1":"mean = train_weather[weather_scale].mean(axis=0)\ntrain_weather[weather_scale] = train_weather[weather_scale].astype('float32')\ntrain_weather[weather_scale] -= train_weather[weather_scale].mean(axis=0)\nstd = train_weather[weather_scale].std(axis=0)\ntrain_weather[weather_scale] \/= train_weather[weather_scale].std(axis=0)","27b3974a":"building_scale = ['square_feet', 'year_built', 'floor_count']","0878c623":"mean = building_data[building_scale].mean(axis=0)\nbuilding_data[building_scale] = building_data[building_scale].astype('float32')\nbuilding_data[building_scale] -= building_data[building_scale].mean(axis=0)\nstd = building_data[building_scale].std(axis=0)\nbuilding_data[building_scale] \/= building_data[building_scale].std(axis=0)","f8a8e682":"train_data = pd.merge(train_data, building_data, on='building_id', right_index=True)\ntrain_data = train_data.sort_values(['timestamp'])\ntrain_data = pd.merge_asof(train_data, train_weather, on='timestamp', by='site_id', right_index=True)\ndel train_weather","b4faf5cc":"class DataGenerator(Sequence):\n    \"\"\" A data generator based on the template\n        https:\/\/stanford.edu\/~shervine\/blog\/keras-how-to-generate-data-on-the-fly\n        \"\"\"\n    \n    def __init__(self, data, list_IDs, features, batch_size, shuffle=False):\n        self.data = data.loc[list_IDs].copy()\n        self.list_IDs = list_IDs\n        self.features = features\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.on_epoch_end()\n    \n    \n    def __len__(self):\n        return int(np.floor(len(self.list_IDs)\/self.batch_size))\n    \n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        return X, y\n    \n    \n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n        \n    def __data_generation(self, list_IDs_temp):        \n        X = np.empty((len(list_IDs_temp), len(self.features)), dtype=float)\n        y = np.empty((len(list_IDs_temp), 1), dtype=float)\n        X = self.data.loc[list_IDs_temp, self.features].values\n        \n        if 'meter_reading' in self.data.columns:\n            y = self.data.loc[list_IDs_temp, 'meter_reading'].values\n        # reshape\n        X = np.reshape(X, (X.shape[0], 1, X.shape[1]))\n        return X, y","7fd025e6":"train_size = int(len(train_data.index)*0.75)\nval_size = len(train_data.index) - train_size\ntrain_list, val_list = train_data.index[0:train_size], train_data.index[train_size:train_size+val_size]\nprint(train_size, val_size)\n","c9038a2a":"no_features = ['building_id', 'timestamp', 'meter_reading', 'year']\nfeatures = train_data.columns.difference(no_features)","2f3d6caf":"batch_size = 1024\ntrain_generator = DataGenerator(train_data, train_list, features, batch_size)\nval_generator = DataGenerator(train_data, val_list, features, batch_size)","bd86b4a0":"input_dim = len(features)\nprint(input_dim)","9efb0c0e":"model = Sequential()\n#model.add(Embedding(input_length=input_dim))\nmodel.add(LSTM(units=4, activation = 'relu', input_shape=(1, input_dim)))\n#model.add(LSTM(units=64, activation = 'relu'))\n#model.add(Dense(128, activation='relu', input_dim=input_dim))\n#model.add(Dense(256, activation='relu'))\n#model.add(Dense(512, activation='relu'))\nmodel.add(Dense(1, activation='relu'))","feef1f2c":"def rmse(y_true, y_pred):\n    \"\"\" root_mean_squared_error \"\"\"\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))","dcf7b19c":"model.compile(optimizer = Adam(lr=1e-4),\n              loss='mse',\n              metrics=[rmse])","be996a22":"model.summary()","44357f06":"epochs = 1","e64d53c8":"history = model.fit_generator(generator=train_generator,\n                              validation_data=val_generator,\n                              epochs = epochs)","8833e10e":"loss = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1, len(loss)+1)\nplt.plot(epochs, loss, 'bo', label='loss_train')\nplt.plot(epochs, loss_val, 'b', label='loss_val')\nplt.title('value of the loss function')\nplt.xlabel('epochs')\nplt.ylabel('value of the loss function')\nplt.legend()\nplt.grid()\nplt.show()","aa978a27":"acc = history.history['rmse']\nacc_val = history.history['val_rmse']\nepochs = range(1, len(loss)+1)\nplt.plot(epochs, acc, 'bo', label='accuracy_train')\nplt.plot(epochs, acc_val, 'b', label='accuracy_val')\nplt.title('accuracy')\nplt.xlabel('epochs')\nplt.ylabel('value of accuracy')\nplt.legend()\nplt.grid()\nplt.show()","ae742df2":"del train_data","762525fa":"nrows = 1667904\nbatch_size = 1022\nsteps = 25\ny_test = np.empty(())\ntest_weather = pd.read_csv(path_in+'weather_test.csv', parse_dates=['timestamp'])\ncols_with_missing_test_weather = [col for col in test_weather.columns if test_weather[col].isnull().any()]\ntest_weather[cols_with_missing_test_weather] = imp_most.fit_transform(test_weather[cols_with_missing_test_weather])\n\nmean = test_weather[weather_scale].mean(axis=0)\ntest_weather[weather_scale] = test_weather[weather_scale].astype('float32')\ntest_weather[weather_scale] -= test_weather[weather_scale].mean(axis=0)\nstd = test_weather[weather_scale].std(axis=0)\ntest_weather[weather_scale] \/= test_weather[weather_scale].std(axis=0)\n\ntest_weather['wind_direction'+'_sin'] = np.sin((2*np.pi*test_weather['wind_direction'])\/360)\ntest_weather['wind_direction'+'_cos'] = np.cos((2*np.pi*test_weather['wind_direction'])\/360)\ntest_weather = test_weather.drop(['wind_direction'], axis=1)\n\nfor i in range(0, steps):\n    print('work on step ', (i+1))\n    test_data = pd.read_csv(path_in+'test.csv', skiprows=range(1,i*(nrows)+1), nrows=nrows, parse_dates=['timestamp'])\n    test_data['month'] = test_data['timestamp'].dt.month\n    test_data['day'] = test_data['timestamp'].dt.weekday\n    test_data['year'] = test_data['timestamp'].dt.year\n    test_data['hour'] = test_data['timestamp'].dt.hour\n    test_data['weekend'] = np.where((test_data['day'] == 5) | (test_data['day'] == 6), 1, 0)\n    for feature in features_cyc.keys():\n        test_data[feature+'_sin'] = np.sin((2*np.pi*test_data[feature])\/features_cyc[feature])\n        test_data[feature+'_cos'] = np.cos((2*np.pi*test_data[feature])\/features_cyc[feature])\n    test_data = test_data.drop(features_cyc.keys(), axis=1)\n    test_data = pd.get_dummies(test_data, columns=['meter'])\n    test_data = pd.merge(test_data, building_data, on='building_id', right_index=True)\n    test_data = test_data.sort_values(['timestamp'])\n    test_data = pd.merge_asof(test_data, test_weather, on='timestamp', by='site_id', right_index=True)\n    test_data = test_data.sort_values(['row_id'])\n    for feature in features:\n        if feature not in test_data:\n            #print('   not in:', feature)\n            test_data[feature] = 0\n    test_generator = DataGenerator(test_data, test_data.index, features, batch_size)\n    predict = model.predict_generator(test_generator, verbose=1, workers=1)\n    predict = np.expm1(predict)\n    y_test = np.vstack((y_test, predict))\n    del test_data\n    del test_generator","9daa11c8":"y_test = np.delete(y_test, 0, 0)","1e0e6795":"del test_weather\ndel building_data","642862ce":"output = pd.DataFrame({'row_id': range(0, len(y_test)),\n                       'meter_reading': y_test.reshape(len(y_test))})\noutput = output[['row_id', 'meter_reading']]\noutput.to_csv('submission.csv', index=False)","62520cb0":"## Weather data\nThe feature wind_direction is cyclic.","b27412a5":"# Split the random input data into train and val\nSince it's a timeseries problem, we split the train and validation data by timestamp and not with a random split.","5ed9ca1f":"# Delete data","fc78cc30":"# Scale building and weather data\n## Weather data","71115c3b":"## Building data\nThe feature primary_use is a categorical feature with 16 categories. For the first we use a simple mapping.","14c268f7":"# Build the data generator","c7d631d5":"# Load Data","c4604a33":"# Load Libraries","8ebb1185":"# Predict test data\n* We following the steps above to prepare the data\n* Build data generator\n* Predict subdate\n* Write data in an array","d1a36392":"# Encoding\nThere is a greate encoding competition: https:\/\/www.kaggle.com\/drcapa\/categorical-feature-encoding-challenge-xgb\n## Train data\n### Feature meter\nThere are 4 types of meters: <br>\n0 = electricity, 1 = chilledwater, 2 = steam, 3 = hotwater <br>\nWe use the one hot encoding for this 4 feature.","83cad589":"Additionally we create the feature weekend: 5 = saturday and 6 = sunday.","4180b880":"### Features month, day and hour\nWe created the features month, day and hour which are cyclic.","f6d09417":"# Define Recurrent Neural Network\nWe use a simple recurrent neural network for train and prediction. Later we will improve.","5f93d069":"# Write output for submission","4adfe677":"# Analyse results\nA short analysis of the train results.","7d0667ba":"# Create new features\n## Train data\nBased on the timestamp we create new features which are cyclic.","eb2fc178":"# Welcome to the ASHRAE - Great Energy Predictor Competition\nThis notebook is a starter code for all beginners and easy to understand. The train and test data are very large so we will work with a data generator based on the template to generate the data on the fly <br>\nhttps:\/\/stanford.edu\/~shervine\/blog\/keras-how-to-generate-data-on-the-fly\n\nAdditionally we follow an efficient workflow. <br>\nWe also use categorical feature encoding techniques, compare <br>\nhttps:\/\/www.kaggle.com\/drcapa\/categorical-feature-encoding-challenge-xgb\n\nFor the first step we will take a simple neural network based on the keras library. After that we will use a RNN.<br>\nCurrent status of the kernel: The workflow is complete.<br>\nNext steps: \n* Improve the LSTM.\n* Expand the feature engineering based on the kernel: https:\/\/www.kaggle.com\/drcapa\/ashrae-feature-engineering","c7692141":"# Delete train data","de0b9834":"# Scale objective label","d6842348":"# Train model","9aef5710":"# Define train and validation data via Data Generator","159c2755":"# Handle missing values of building and weather data\nThe missing data are numerical values. So for the first step we can use a simple imputer of the sklearn library.","5a1fe45d":"# Help function","57686b81":"## Building data","894af45a":"# Merge data","cbc0077e":"# Define the features"}}