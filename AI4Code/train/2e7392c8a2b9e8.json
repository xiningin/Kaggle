{"cell_type":{"ad6d64c1":"code","64bf9237":"code","7d3f9b39":"code","905e0c19":"code","f0a324bc":"code","e161691a":"code","731ee841":"code","81fdedeb":"code","1085d152":"code","818e571c":"code","5edbdac4":"code","272cdf44":"code","bd46e72c":"code","df851764":"code","07312411":"code","71873f4d":"code","e1152886":"code","b02ad7ec":"code","0fed8950":"code","add8a2e2":"code","f219c6b0":"code","f33d72cc":"code","a9c2697e":"code","7eb3c5ae":"code","5256cb60":"code","326e473d":"code","cd11a656":"code","f3f42429":"code","32ab64c9":"code","99068087":"code","1acfb3c2":"code","76f48af5":"code","283a7d99":"code","814a993f":"code","eabda25e":"code","5ddc003f":"code","b0898320":"code","a5ad50a6":"code","70a923e0":"code","a974aaa8":"code","e676151d":"code","f18e7088":"code","1e6c91f0":"code","f923e439":"code","fab87b80":"code","364d2077":"code","cf2be618":"code","b44d2e61":"code","757c81c3":"code","9af585dc":"code","bb51c8db":"code","ac74b357":"code","2f5f7d2a":"code","0898d572":"code","568e87e2":"code","f53d0e04":"code","5b2cd00f":"code","e9deb029":"code","1d848876":"code","21172395":"code","9314f623":"code","ea686a8d":"code","0ce79821":"code","959a218d":"code","7e2fd415":"code","a88f6528":"code","376bdcf3":"code","c67efe8f":"code","dbcff129":"code","789eb2c0":"code","b6c2025e":"markdown","fb0f0359":"markdown","bfe67492":"markdown","18e328b6":"markdown","d0ca6a43":"markdown","b3382c6f":"markdown","f673f68c":"markdown","103d9874":"markdown","1b678b21":"markdown","c17b03e6":"markdown","87d80a4e":"markdown","ad7e39a4":"markdown","6b9ab75a":"markdown","3cfb06a0":"markdown","9276909c":"markdown","91ce563b":"markdown","ebd13e65":"markdown","7bf3ff57":"markdown","448ec64f":"markdown","1d49401e":"markdown","c393bef5":"markdown"},"source":{"ad6d64c1":"import numpy as np \nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\nimport seaborn as sns\nimport datetime\n%config InlineBackend.figure_format = 'retina'\n\n# \u5f15\u5165\u673a\u5668\u5b66\u4e60\u5e93\n\n\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn import svm\nfrom sklearn import model_selection\nfrom sklearn import metrics \n\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\nimport torch\n\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, KFold\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nimport os\n\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\nimport torch\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n\n# \u5bfc\u5165\u8bcd\u4e91\u5e93\nfrom wordcloud import WordCloud,ImageColorGenerator","64bf9237":"ds = pd.read_csv('..\/input\/2021proc\/2021MCMProblemC_DataSet.csv')\npic = pd.read_csv(\"..\/input\/2021proc\/2021MCM_ProblemC_ Images_by_GlobalID.csv\")","7d3f9b39":"from sklearn.model_selection import train_test_split","905e0c19":"ds.info()\nds.describe()","f0a324bc":"ds.head(5)\n","e161691a":"from sklearn.cluster import KMeans\n\ndef cluster(data):\n  '''\n  input: dataframe containing Latitude(x) and Longitude(y) coordinates\n  output: series of cluster labels that each row of coordinates belongs to.\n  '''\n  model = KMeans(n_clusters=50)\n  labels = model.fit_predict(data)\n  return labels","731ee841":"plot = ds.iloc[:4440].plot.scatter('Longitude', 'Latitude')","81fdedeb":"ds.plot(kind =\"scatter\", x=\"Longitude\",y=\"Latitude\",alpha=0.1)","1085d152":"keywords_vc = pd.DataFrame({\"Count\": ds[\"Detection Date\"].value_counts()})\nsns.barplot(y=keywords_vc[0:30].index, x=keywords_vc[0:30][\"Count\"], orient='h')\nplt.title(\"Top 30 Keywords\")\nplt.show()","818e571c":"ds['Detection Date'] = pd.to_datetime(ds['Detection Date'],errors='coerce',infer_datetime_format=True)\nds.head(5)","5edbdac4":"# plot the day of the month\nsns.distplot(ds['Detection Date'].dt.day, kde=False, bins=31)","272cdf44":"ds.loc[ds['Lab Status'] == 'Negative ID', 'Lab Status'] = 0\nds.loc[ds['Lab Status'] == 'Positive ID', 'Lab Status'] = 1\nds.loc[ds['Lab Status'] == 'Unverified', 'Lab Status'] = 2\nds.loc[ds['Lab Status'] == 'Unprocessed', 'Lab Status'] = 3","bd46e72c":"ds = ds.drop(index=ds.loc[(ds['Lab Status'] == 3)].index)","df851764":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(ds[ds['Lab Status'] == 0][\"Detection Date\"], alpha = 0.6, bins=bins, label='Not')\n\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim()\nplt.grid()\nplt.show()","07312411":"plt.hist(ds[ds['Lab Status'] == 1][\"Detection Date\"], alpha = 0.8, bins=bins, label='Real')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim()\nplt.grid()\nplt.show()","71873f4d":"def remove_punctuation(text):\n    '''a function for removing punctuation'''\n    import string\n    # replacing the punctuations with no space, \n    # which in effect deletes the punctuation marks \n    translator = str.maketrans('', '', string.punctuation)\n    # return the text stripped of punctuation marks\n    return text.translate(translator)\n\nds['Notes'] = ds['Notes'].apply(remove_punctuation)\nds.head(10)","e1152886":"# extracting the stopwords from nltk library\nsw = stopwords.words('english')\n# displaying the stopwords\nnp.array(sw)","b02ad7ec":"def stopwords(text):\n    '''a function for removing the stopword'''\n    # removing the stop words and lowercasing the selected words\n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    # joining the list of words with space separator\n    return \" \".join(text)","0fed8950":"ds['Notes'] =ds['Notes'].apply(stopwords)\nds.head(10)","add8a2e2":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","f219c6b0":"plt.figure(figsize=(16,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(ds[\"Notes\"])[:20]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","f33d72cc":"# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(ds[\"Notes\"][:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","a9c2697e":"def cv(data):\n    count_vectorizer = CountVectorizer()\n\n    emb = count_vectorizer.fit_transform(data)\n\n    return emb, count_vectorizer\n\nlist_corpus = ds[\"Notes\"].tolist()\nlist_labels = ds[\"Lab Status\"].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.4, \n                                                                                random_state=42)\n\nX_train_counts, count_vectorizer = cv(X_train)\nX_test_counts = count_vectorizer.transform(X_test)","7eb3c5ae":"def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n        lsa = TruncatedSVD(n_components=2)\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue']\n        if plot:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            orange_patch = mpatches.Patch(color='orange', label='Not')\n            blue_patch = mpatches.Patch(color='blue', label='Real')\n            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})\n\nfig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_counts, y_train)\nplt.show()","5256cb60":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n\n    train = tfidf_vectorizer.fit_transform(data)\n\n    return train, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)","326e473d":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_tfidf, y_train)\nplt.show()","cd11a656":"def create_corpus_new(df):\n    corpus=[]\n    for tweet in tqdm(df['Notes']):\n        words=[word.lower() for word in word_tokenize(tweet)]\n        corpus.append(words)\n    return corpus   ","f3f42429":"corpus=create_corpus_new(ds)","32ab64c9":"embedding_dict={}\nwith open(\"..\/input\/glove6b100dtxt\/glove.6B.100d.txt\",'r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","99068087":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","1acfb3c2":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","76f48af5":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec      ","283a7d99":"tweet_pad[0][0:]","814a993f":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=3e-4)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","eabda25e":"model.summary()","5ddc003f":"train=tweet_pad[:ds.shape[0]]\ntest=tweet_pad[ds.shape[0]:]","b0898320":"X_train,X_test,y_train,y_test=train_test_split(train,ds['Lab Status'].values,test_size=0.2)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","a5ad50a6":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(train,ds[\"Lab Status\"])\nplt.show()","70a923e0":"X_train = X_train.astype('float64')\ny_train = y_train.astype('float64')\nX_test = X_test.astype('float64')\ny_test = y_test.astype('float64')","a974aaa8":"# Recomended 10-20 epochs\n\nhistory=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)","e676151d":"train_result=history.history\n\nacc=train_result['accuracy']\nval_acc=train_result['val_accuracy']\nepochs=range(1,16)\nplt.plot(epochs,acc,'b-')\nplt.plot(epochs,val_acc,'r')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.show()\n\n#t=model.predict(y_train)\n#resultsss=model.evaluate(y_train,y_test)\n#resultsss","f18e7088":"train_pred_GloVe = model.predict(train)\ntrain_pred_GloVe_int = train_pred_GloVe.round().astype('int')","1e6c91f0":"pred = pd.DataFrame(train_pred_GloVe, columns=['preds'])\npred.plot.hist()","f923e439":"train_pred_GloVe","fab87b80":"dsc=ds.copy()","364d2077":"dsc.Notes = train_pred_GloVe","cf2be618":"dsc = pd.DataFrame(dsc,columns=[\"Notes\",'Latitude','Longitude','Lab Status'])\ndsc","b44d2e61":"dsc.to_csv(\"cleaning\")","757c81c3":"plt.subplots(figsize=(8, 5))\nsns.heatmap(dsc.corr(), annot=True, cmap=\"RdYlGn\")\nplt.show()","9af585dc":"y = dsc[\"Lab Status\"] \n\nX = dsc.drop(['Lab Status'],axis=1)","bb51c8db":"from sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)","ac74b357":"X_train.astype(\"int\"), X_test.astype(\"int\")","2f5f7d2a":"y_train.astype(\"int\")\ny_test.astype(\"int\")","0898d572":"from sklearn.tree import DecisionTreeClassifier\n\n# instantiate the DecisionTreeClassifier model with criterion gini index\n\nclf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)\n\n\n# fit the model\nclf_gini.fit(X_train.astype(\"int\"), y_train.astype(\"int\"))\n\n","568e87e2":"y_pred_gini = clf_gini.predict(X_test.astype(\"int\"))","f53d0e04":"y_pred_gini=y_pred_gini.astype(\"int\")","5b2cd00f":"from sklearn.metrics import accuracy_score\n\nprint('Model accuracy score with criterion gini index: {0:0.4f}'. format(accuracy_score(y_test.astype(\"int\"), y_pred_gini.astype(\"int\"))))","e9deb029":"y_pred_train_gini = clf_gini.predict(X_train)\n\ny_pred_train_gini","1d848876":"print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train.astype(\"int\"), y_pred_train_gini)))","21172395":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(clf_gini.score(X_train.astype(\"int\"), y_train.astype(\"int\"))))\n\nprint('Test set score: {:.4f}'.format(clf_gini.score(X_test.astype(\"int\"), y_test.astype(\"int\"))))","9314f623":"plt.figure(figsize=(15,15))\n\nfrom sklearn import tree\n\ntree.plot_tree(clf_gini.fit(X_train.astype(\"int\"), y_train.astype(\"int\"))) ","ea686a8d":"from sklearn import tree\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split,cross_val_score\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import export_graphviz\n\n# \u6784\u5efa\u6a21\u578b\nclf=tree.DecisionTreeClassifier()\nclf.fit(X_train.astype(\"int\"),y_train.astype(\"int\"))\n#\u8bc4\u4f30\u6a21\u578b\u4f7f\u7528\u5341\u6b21\u4ea4\u53c9\u9a8c\u8bc1\nscore = cross_val_score(clf, X, y, cv=10, scoring='accuracy')\nprint(np.mean(score))\nprint(clf.feature_importances_)\n \n# \u53ef\u89c6\u5316\u51b3\u7b56\u6811\n# feature_name = ['\u9152\u7cbe','\u82f9\u679c\u9178','\u7070','\u7070\u7684\u78b1\u6027','\u9541','\u603b\u915a','\u7c7b\u9ec4\u916e','\u975e\u9ec4\u70f7\u7c7b\u915a\u7c7b','\u82b1\u9752\u7d20','\u989c\u8272\u5f3a\u5ea6','\u8272\u8c03','od280\/od315\u7a00\u91ca\u8461\u8404\u9152','\u812f\u6c28\u9178']\nprint(export_graphviz(clf,feature_names=X_train.columns))\n","0ce79821":"t = tree.export_graphviz(clf_gini,feature_names=X_train.columns )\npic= graphviz.Source(t)\nprint(pic)","959a218d":"pic","7e2fd415":"from sklearn.svm import SVC\nfrom sklearn import metrics\n\nmodel_svc = svm.SVC()\nmodel_svc.fit(X_train.astype('int'), y_train.astype('int'))\nprediction=model_svc.predict(X_test.astype('int'))\nprint('The accuracy of the SVM is: {0}'.format(metrics.accuracy_score(prediction,y_test.astype('int'))))","a88f6528":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix  \nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb\n\nmodel = xgb.XGBClassifier(max_depth=6, n_estimators=100, learning_rate=0.1, \n                          colsample_bytree=.7, gamma=0.0, reg_alpha=4, objective='binary:logistic', \n                          eta=0.3, silent=1, subsample=1).fit(X_train, y_train) \n\nprediction = model.predict(X_test.astype('int')).astype('int')\ncm = confusion_matrix(y_test.astype('int'), prediction).astype('int')\nprint(cm)  \nprint(accuracy_score(y_test.astype('int'), prediction.astype('int')))\nprint(classification_report(y_test.astype('int'), prediction.astype('int')))\n","376bdcf3":"\nfrom xgboost import plot_importance\n# \u7ed8\u5236\u7279\u5f81\u91cd\u8981\u6027\nplot_importance(model)\nplt.show();","c67efe8f":"model","dbcff129":"from xgboost import plot_tree\nplot_tree(model,num_trees=0)\nplt.show()","789eb2c0":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","b6c2025e":" .astype(\"int\")","fb0f0359":"y = dsc[\"Lab Status\"]\ndsc_features=[\"Notes\", \"Latitude\",\"Longitude\"]\nX = dsc[dsc_features]\n\nX_train,X_test,y_train,y_test=model_selection.train_test_split(X,y,random_state=101,test_size=0.3)\nprint(\"split_train_data 70%:\", X_train.shape, \"split_train_target 70%:\",y_train.shape, \"split_test_data 30%\", X_test.shape, \"split_test_target 30%\",y_test.shape)\n\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Define model. Specify a number for random_state to ensure same results each run\ndsc_model = DecisionTreeRegressor(random_state=1)\n\n# Fit model\ndsc_model.fit(X, y)\n\nprint(\"Making predictions for the following 25 places:\")\nprint(X.head(25))\nprint(\"The predictions are\")\nprint(dsc_model.predict(X.head(25)))\n\n\nfrom sklearn.metrics import mean_absolute_error\n\npredicted_data = dsc_model.predict(X)\nmean_absolute_error(y, predicted_data)\n\nfrom sklearn.model_selection import train_test_split\n\n# split data into training and validation data, for both features and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n# Define model\nmelbourne_model = DecisionTreeRegressor()\n# Fit model\nmelbourne_model.fit(train_X, train_y)\n\n# get predicted prices on validation data\nval_predictions = melbourne_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\n# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [2,4,6]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","bfe67492":"\n# \u521b\u5efa\u6210lgb\u7279\u5f81\u7684\u6570\u636e\u96c6\u683c\u5f0f\nlgb_train = lgb.Dataset(X_train.astype('int'), y_train.astype('int')) # \u5c06\u6570\u636e\u4fdd\u5b58\u5230LightGBM\u4e8c\u8fdb\u5236\u6587\u4ef6\u5c06\u4f7f\u52a0\u8f7d\u66f4\u5feb\nlgb_eval = lgb.Dataset(X_test.astype('int'), y_test.astype('int'), reference=lgb_train)  # \u521b\u5efa\u9a8c\u8bc1\u6570\u636e\n \n# \u5c06\u53c2\u6570\u5199\u6210\u5b57\u5178\u4e0b\u5f62\u5f0f\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',  # \u8bbe\u7f6e\u63d0\u5347\u7c7b\u578b\n    'objective': 'regression', # \u76ee\u6807\u51fd\u6570\n    'metric': {'l2', 'auc'},  # \u8bc4\u4f30\u51fd\u6570\n    'num_leaves': 31,   # \u53f6\u5b50\u8282\u70b9\u6570\n    'learning_rate': 0.05,  # \u5b66\u4e60\u901f\u7387\n    'feature_fraction': 0.9, # \u5efa\u6811\u7684\u7279\u5f81\u9009\u62e9\u6bd4\u4f8b\n    'bagging_fraction': 0.8, # \u5efa\u6811\u7684\u6837\u672c\u91c7\u6837\u6bd4\u4f8b\n    'bagging_freq': 5,  # k \u610f\u5473\u7740\u6bcf k \u6b21\u8fed\u4ee3\u6267\u884cbagging\n    'verbose': 1 # <0 \u663e\u793a\u81f4\u547d\u7684, =0 \u663e\u793a\u9519\u8bef (\u8b66\u544a), >0 \u663e\u793a\u4fe1\u606f\n}\n","18e328b6":"import json\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import  make_classification\n","d0ca6a43":"print('Start training...')\n# \u8bad\u7ec3 cv and train\ngbm = lgb.train(params,lgb_train,num_boost_round=20,valid_sets=lgb_eval,early_stopping_rounds=5) # \u8bad\u7ec3\u6570\u636e\u9700\u8981\u53c2\u6570\u5217\u8868\u548c\u6570\u636e\u96c6\n \nprint('Save model...') \n \ngbm.save_model('model.txt')   # \u8bad\u7ec3\u540e\u4fdd\u5b58\u6a21\u578b\u5230\u6587\u4ef6\n \nprint('Start predicting...')\n# \u9884\u6d4b\u6570\u636e\u96c6\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration) #\u5982\u679c\u5728\u8bad\u7ec3\u671f\u95f4\u542f\u7528\u4e86\u65e9\u671f\u505c\u6b62\uff0c\u53ef\u4ee5\u901a\u8fc7best_iteration\u65b9\u5f0f\u4ece\u6700\u4f73\u8fed\u4ee3\u4e2d\u83b7\u5f97\u9884\u6d4b\n# \u8bc4\u4f30\u6a21\u578b\nprint('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5) # \u8ba1\u7b97\u771f\u5b9e\u503c\u548c\u9884\u6d4b\u503c\u4e4b\u95f4\u7684\u5747\u65b9\u6839\u8bef\u5dee","b3382c6f":"data = pd.merge(ds,pic)\ndata = pd.DataFrame(data,columns=['GlobalID','FileName','Detection Date',\"Notes\",'Lab Status','Lab Comments','Latitude','Longitude'])","f673f68c":"ds.plot(kind=\"scatter\",x=\"Longitude\",y=\"Latitude\",alpha=0.4,s=ds[\"Lab Status\"], label=\"Detection Date\",figsize=(10,7),c=\"Lab Status\",cmap=plt.get_cmap(\"jet\"),colorbar=True,)\nplt.legend()","103d9874":"from math import log\nimport operator\n \ndef cal_entropy(dataSet):\n    num=len(dataSet)\n    label_count={}\n    for fea in dataSet:\n        current_label=fea[-1]#\u7edf\u8ba1\u6bcf\u6761\u6570\u636e\u7684\u7c7b\n        if current_label not in label_count.keys():\n            label_count[current_label]=0\n        label_count[current_label]+=1 #\u8ba1\u7b97\u6bcf\u4e2a\u7c7b\u4e2d\u6709\u591a\u5c11\u6570\u636e\n    entropy=0.0\n    for i in label_count:#\u8ba1\u7b97\u7ecf\u9a8c\u71b5\n        Pi=float(label_count[i])\/num\n        entropy-=Pi*log(Pi,2)\n    return entropy\n \ndef remove_feature(dataSet,axis,feature):#\u53bb\u9664\u67d0\u4e2a\u7279\u5f81\n    retdataset=[]\n    for featVec in dataSet:\n        if featVec[axis]==feature:\n            reducedata=featVec[:axis]#\u67d0\u4e2a\u7279\u5f81\u524d\u6570\u636e\n            reducedata.extend(featVec[axis+1:])#\u67d0\u4e2a\u7279\u5f81\u540e\u6570\u636e\n            #\u53bb\u6389\u4e86axis\n            retdataset.append(reducedata)\n    return retdataset\n \ndef choose_best_feature(dataSet):\n    entropy=cal_entropy(dataSet)\n    feature_num=len(dataSet[0])-1\n    max_mutual_info=0\n    best_feature=-1\n    for i in range(feature_num):\n        feature_list=[example[i] for example in dataSet]\n        feature_class=set(feature_list)#\u5f97\u5230\u8be5\u7279\u5f81\u7684\u6240\u6709\u53ef\u80fd\u53d6\u503c\n        conditional_entropy=0\n        for value in feature_class:\n            retdataset=remove_feature(dataSet, i, value)\n            Pi=len(retdataset)\/float(len(dataSet))\n            conditional_entropy+=Pi*cal_entropy(retdataset)#\u6c42\u6761\u4ef6\u71b5\n        mutual_info=entropy-conditional_entropy#\u4e92\u4fe1\u606f\u91cf\n        if (mutual_info>max_mutual_info):\n            max_mutual_info=mutual_info\n            best_feature=i\n    return best_feature\n \ndef majority_vote(class_list):\n    class_count={}\n    for vote in class_list:\n        if vote not in class_count.keys():\n            class_count[vote]=0\n        class_count[vote]+=1\n    sort_class_count=sorted(class_count.items(),key=operator.itemgetter(1),reverse=True)\n    #\u6392\u5e8f\u6765\u51b3\u5b9a\u8be5\u8282\u70b9\u7684\u7c7b\n    return sort_class_count[0][0]\n \ndef create_tree(dataSet,labels):\n    class_list=[example[-1] for example in dataSet]\n    if class_list.count(class_list[0])==len(class_list):\n        return class_list[0]\n    if len(dataSet[0])==1:\n        #\u8282\u70b9\u5df2\u6ca1\u6709\u7279\u5f81\u53ef\u4ee5\u7ee7\u7eed\u5206\u89e3\n        return majority_vote(class_list)\n    best_feature=choose_best_feature(dataSet)\n    best_feature_label=labels[best_feature]\n    my_tree={best_feature_label:{}}\n    del(labels[best_feature])\n    #\u5220\u6389\u5df2\u9009\u62e9\u7684\u7279\u5f81\n    feature=[example[best_feature] for example in dataSet]\n    feature_class=set(feature)\n    for value in feature_class:\n        sublabels=labels[:]\n        my_tree[best_feature_label][value]=create_tree(remove_feature(dataSet,best_feature,value),sublabels)\n        #\u8fed\u4ee3\u751f\u6210\u51b3\u7b56\u6811\n    return my_tree\n","1b678b21":"data = dsc\n\npositive = data[data[\"Lab Status\"]==1]  # 1\nnegetive = data[data[\"Lab Status\"]==0]  # 0\n\nfig, ax = plt.subplots(figsize=(6,5))\nax.scatter(positive['Latitude'],positive['Longitude'],c='r', label='Positive')\nax.scatter(negetive['Latitude'],negetive['Longitude'],s=50, c='g', marker='x', label='Negative')\n# \u8bbe\u7f6e\u56fe\u4f8b\u663e\u793a\u5728\u56fe\u7684\u4e0a\u65b9\nbox = ax.get_position()\nax.set_position([box.x0, box.y0, box.width , box.height* 0.8])\nax.legend(loc='center left', bbox_to_anchor=(0.2, 1.12),ncol=3)\n# \u8bbe\u7f6e\u6a2a\u7eb5\u5750\u6807\u540d\nax.set_xlabel('Latitude')\nax.set_ylabel('Longitude')\nplt.show()\n#negetive['Latitude']\n#positive['Latitude'],","c17b03e6":"\nThese embeddings don't look very cleanly separated. Let's see if we can still fit a useful model on them.\n\n","87d80a4e":"Baseline Model with GloVe results","ad7e39a4":"from sklearn.metrics import mean_absolute_error\n\npredictions = model.predict(X_test)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_test)))","6b9ab75a":"TF IDF","3cfb06a0":"st_amt = data.groupby('Lab Status').size().sort_values(ascending=False).head(100)\nst_amt.plot(kind='bar')","9276909c":"These two values are quite comparable. So, there is no sign of overfitting.","91ce563b":"-------------","ebd13e65":"data[\"Detection Date\"].value_counts().head(10)","7bf3ff57":"Now that we've created embeddings, let's visualize them and see if we can identify some structure. In a perfect world, our embeddings would be so distinct that are two classes would be perfectly separated. Since visualizing data in 20k dimensions is hard, let's project it down to 2.","448ec64f":"-----------","1d49401e":" **Split the dataset into training and testing.**\n\nThe training set will be used to build the machine learning models. \nThe model will be based on the features like \"Latitude\",\"Longitude\",\"Notes\" but also on the known \"Lab Status\" flag.\n\nThe testing set should be used to see how well the model performs on unseen data. \nFor each report in the test set, I use the model trained to predict whether or not the insect mentioned in the reports belong to Hornet,then will be compared with the actual\"Lab Status\"flag.","c393bef5":"data.groupby(\"Lab Status\").count().sort_values(\"Detection Date\",ascending=False).head(5)"}}