{"cell_type":{"93b592b3":"code","4ee21efe":"code","c812fc2b":"code","52006c1e":"code","0c4f2424":"code","839fe447":"code","bf9c7a8a":"code","543bc88d":"code","f7a16b79":"code","e80dcda4":"code","f6fb1f05":"code","fba6b6ac":"code","82f2ba48":"code","7510a556":"code","e099847a":"code","03d768db":"code","44156e7e":"code","39acd3a8":"code","40249445":"code","f0a44147":"code","c8502705":"code","3333f9d7":"code","324aef96":"code","54b55b28":"code","936344da":"code","6db5ac93":"code","869064e7":"code","ac74590d":"code","0eff8e29":"code","6369113e":"code","f89cdda8":"code","ab223766":"code","c25267d5":"code","d8eed4ac":"code","fd1d8f19":"code","9cb7f386":"code","7d0809f2":"code","26337b81":"code","ee6a057a":"code","beb98e0c":"code","9e8b36da":"code","f2047410":"code","256c6bef":"code","63bcd337":"code","2b2b7770":"code","1431414f":"code","4e6c9e72":"code","7947734a":"code","1711a89f":"code","0daad210":"code","8ac1b859":"code","5aad8df5":"code","3d207e47":"markdown","2c8f947b":"markdown","95eee9ab":"markdown","1ef0d8f4":"markdown","9978ec8d":"markdown","505761a0":"markdown","02dc5cf4":"markdown","831637ad":"markdown","022340a3":"markdown","916075e5":"markdown","5e61bada":"markdown","839ef035":"markdown","73473730":"markdown"},"source":{"93b592b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in list(filenames)[:25]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4ee21efe":"from tqdm import tqdm","c812fc2b":"import json","52006c1e":"with open('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/test\/8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60.json') as sample:\n    sample_json = json.load(sample)","0c4f2424":"sample_json[:3]","839fe447":"# [{}, {}, {}]\nfor i in sample_json:\n    print(i.keys())","bf9c7a8a":"## ","543bc88d":"import nltk\nfrom typing import List\nfrom nltk.tokenize import word_tokenize","f7a16b79":"nltk.download('stopwords')","e80dcda4":"from nltk.corpus import stopwords\nenglish_stopwords = stopwords.words('english')","f6fb1f05":"def remove_stopwords(input_text: str, stopwords: List[str]) -> str:\n    \"\"\"Given an input text, removes all stopwords and punctuation marks\"\"\"\n    text_tokens = word_tokenize(input_text)\n    return [word.lower() for word in text_tokens if not word.lower() in stopwords and word.lower().isalnum()]","fba6b6ac":"remove_stopwords(sample_json[0]['text'], english_stopwords)[:10]","82f2ba48":"from sklearn.feature_extraction.text import CountVectorizer","7510a556":"sample_text = remove_stopwords(sample_json[0]['text'], english_stopwords)\n\nvectorizer = CountVectorizer()","e099847a":"corpus = [\" \".join(remove_stopwords(text_body['text'] ,english_stopwords)) for text_body in sample_json]","03d768db":"corpus[0][:100]","44156e7e":"X = vectorizer.fit_transform(corpus)","39acd3a8":"import pandas as pd","40249445":"pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())","f0a44147":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport re","c8502705":"def get_string(filename) -> str:\n    ...\n#     Load file from memory\n    with open(filename, 'r') as f:\n        doc_json = json.load(f)\n    body = ' '.join([dictionary['text'] for dictionary in doc_json])\n    return ' '.join([word for word in body.split(' ') if (word.isalpha() and word.isascii())])","3333f9d7":"tfidf_vectorizer = TfidfVectorizer(stop_words=english_stopwords, max_features=5000)","324aef96":"train_files = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/train\/'):\n    for filename in filenames:\n        if filename.endswith('.json'):\n            train_files += [os.path.join(dirname, filename)]\n\n# train_files = train_files[:1000]\n            \ndoc_strings = [get_string(file_path) for file_path in tqdm(train_files)]\n    \n    \nX = tfidf_vectorizer.fit_transform(doc_strings)\n","54b55b28":"tfidf_df = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names())\ntfidf_df","936344da":"def calculate_similarity(X, vectorizor, query, top_k=20):\n    \"\"\" Vectorizes the `query` via `vectorizor` and calculates the cosine similarity of\n    the `query` and `X` (all the documents) and returns the `top_k` similar documents.\"\"\"\n\n    # Vectorize the query to the same length as documents\n    query_vec = vectorizor.transform(query)\n    # Compute the cosine similarity between query_vec and all the documents\n    cosine_similarities = cosine_similarity(X,query_vec).flatten()\n    # Sort the similar documents from the most similar to less similar and return the indices\n    most_similar_doc_indices = np.argsort(cosine_similarities, axis=0)[:-top_k-1:-1]\n    return (most_similar_doc_indices, cosine_similarities)\n\ndef show_similar_documents(df, cosine_similarities, similar_doc_indices, query):\n    \"\"\" Prints the most similar documents using indices in the `similar_doc_indices` vector.\"\"\"\n    counter = 1\n    for index in similar_doc_indices:\n        print('Top-{}, Similarity = {}'.format(counter, cosine_similarities[index]))\n        filename = train_files[index].split('\/')[-1].split('.')[0]\n        print(f\"Filename: {filename}\")\n        cleaned_label = list(train[train['Id'] == filename]['cleaned_label'])\n        print(f\"cleaned_label: {cleaned_label}\")\n        print('body: {}, '.format(df[index])[:100])\n        print()\n        counter += 1\n        if bool([c for c in cleaned_label if c == query])","6db5ac93":"# query = ['national science foundation survey of industrial research and development']\n# sim_vecs, cosine_similarities = calculate_similarity(X, tfidf_vectorizer, query)\n\nfor query in unique_dataset_labels:\n    sim_vecs, cosine_similarities = calculate_similarity(X, tfidf_vectorizer, query)\n    show_similar_documents(doc_strings, cosine_similarities, sim_vecs, query)","869064e7":"train","ac74590d":"train = pd.read_csv('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/train.csv')","0eff8e29":"unique_dataset_labels = train['cleaned_label'].unique()\nprint(unique_dataset_labels)","6369113e":"train_results = pd.read_csv('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/train.csv')","f89cdda8":"train_results.groupby('Id').count().sort_values(by='pub_title',ascending=False)","ab223766":"train_results[train_results['Id']=='170113f9-399c-489e-ab53-2faf5c64c5bc']","c25267d5":"sample_submission = pd.read_csv('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')","d8eed4ac":"with open('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv', 'r') as f:\n    sample_submission = f.read()\n","fd1d8f19":"print(sample_submission)","9cb7f386":"def get_string(filename) -> str:\n    ...\n#     Load file from memory\n    with open(filename, 'r') as f:\n        doc_json = json.load(f)\n    return ' '.join([dictionary['text'] for dictionary in doc_json])\n        ","7d0809f2":"get_string('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/test\/8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60.json')[:100] + '...'","26337b81":"train = pd.read_csv('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\nprint(len(train['dataset_title'].unique()))\nprint(len(train['dataset_label'].unique()))\nprint(len(train['cleaned_label'].unique()))\nprint(train.shape)","ee6a057a":"list(train.groupby('dataset_title'))[0][1]","beb98e0c":"def get_dataset_map() -> dict:\n    label_map = {}\n\n    for ndx, row in train.iterrows():\n        if not label_map.get(row['dataset_label'],False):\n            label_map[row['dataset_label']] = []\n\n        if not label_map.get(row['dataset_title'],False):\n            label_map[row['dataset_title']] = []\n\n        label_map[row['dataset_label']] += [row['cleaned_label']]\n        label_map[row['dataset_title']] += [row['cleaned_label']]\n\n    for k, v in label_map.items():\n        label_map[k] = list(set(v))\n\n    return label_map","9e8b36da":"label_map = get_dataset_map()\nlabel_map","f2047410":"def find_datasets_in_string(corpus: str, label_map: dict) -> list:\n    predictions = []\n    for k, v in label_map.items():\n        if k in corpus:\n            predictions+=v\n    predictions_set = set(predictions)\n    return '|'.join(predictions_set)\n    \n    \n    ","256c6bef":"sample_doc = get_string('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/test\/8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60.json')\nprint(find_datasets_in_string(sample_doc, label_map))","63bcd337":"train[train['Id'] =='8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60']","2b2b7770":"files = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if filename.endswith('.json'):\n            files += [os.path.join(dirname, filename)]","1431414f":"results = {}\nfor doc in tqdm(files):\n    results[doc.split('\/')[-1].split('.')[0]] = find_datasets_in_string(get_string(doc), label_map)","4e6c9e72":"list(results.items())[:4]","7947734a":"get_csv_df(results)","1711a89f":"files = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/test\/'):\n    for filename in filenames:\n        if filename.endswith('.json'):\n            files += [os.path.join(dirname, filename)]\nresults = {}\nfor doc in tqdm(files):\n    results[doc.split('\/')[-1].split('.')[0]] = find_datasets_in_string(get_string(doc), label_map)","0daad210":"submission_df = get_csv_df(results)\nsubmission_df","8ac1b859":"submission_df.to_csv('submission.csv',index=False)","5aad8df5":"with open('submission.csv' , 'r') as f:\n    print(f.read())","3d207e47":"## Preprocessing","2c8f947b":"# TF-IDF Vectorizer","95eee9ab":"## Create Test Set Submission","1ef0d8f4":"### Create set of all possible datasets","9978ec8d":"## Submission format\n| Id | PredictionString |\n| --- | --- | \n| 2100032a-7c33-4bff-97ef-690822c43466 |adni|alzheimer s disease neuroimaging initiati |\n| 2f392438-e215-4169-bebf-21ac4ff253e1 |common core of data\\|nces common core of data|t |\n| 3f316b38-1a24-45a9-8d8c-4e05a42257c6 |slosh model\\|noaa storm surge inundation|sea la |\n| 8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60 |rural urban continuum codes |","505761a0":"## Format Submission","02dc5cf4":"### Search Document for all datasets in set","831637ad":"## Modeling","022340a3":"## Import the dataset","916075e5":"def get_csv_df(results):\n#     csv_string = \"Id,PredictionString\\n\"\n    csv_string = []\n    for k, v in results.items():\n#         csv_string+= f\"{k},{v}\\n\"\n        csv_string.append([k,v])\n    return pd.DataFrame(csv_string,columns=['Id','PredictionString'])","5e61bada":"## Load dataset values","839ef035":"### Turn document into single string","73473730":"## Train 130 different classifiers\nOne for each dataset label"}}