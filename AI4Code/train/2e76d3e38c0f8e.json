{"cell_type":{"a41b6ff9":"code","99d2fbbb":"code","b151ae70":"code","ebd3596c":"code","ea8844e6":"code","931adaf0":"markdown","7a2c3210":"markdown","df829063":"markdown","1611d8a2":"markdown"},"source":{"a41b6ff9":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns ","99d2fbbb":"dataset = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')","b151ae70":"X = dataset.iloc[:, : -1].values  #Features \ny = dataset.iloc[:,  -1].values   #Targegts","ebd3596c":"class NaiveBayes:\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self._classes = np.unique(y)\n        n_classes = len(self._classes)\n\n        # calculate mean, var, and prior for each class\n        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n        self._priors =  np.zeros(n_classes, dtype=np.float64)\n\n        for idx, c in enumerate(self._classes):\n            X_c = X[y==c]\n            self._mean[idx, :] = X_c.mean(axis=0)\n            self._var[idx, :] = X_c.var(axis=0)\n            self._priors[idx] = X_c.shape[0] \/ float(n_samples)\n\n    def predict(self, X):\n        y_pred = [self._predict(x) for x in X]\n        return np.array(y_pred)\n\n    def _predict(self, x):\n        posteriors = []\n\n        # calculate posterior probability for each class\n        for idx, c in enumerate(self._classes):\n            prior = np.log(self._priors[idx])\n            posterior = np.sum(np.log(self._pdf(idx, x)))\n            posterior = prior + posterior\n            posteriors.append(posterior)\n            \n        # return class with highest posterior probability\n        return self._classes[np.argmax(posteriors)]\n            \n\n    def _pdf(self, class_idx, x):\n        mean = self._mean[class_idx]\n        var = self._var[class_idx]\n        numerator = np.exp(- (x-mean)**2 \/ (2 * var))\n        denominator = np.sqrt(2 * np.pi * var)\n        return numerator \/ denominator","ea8844e6":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n\n\ndef accuracy(y_true, y_pred):\n    accuracy = np.sum(y_true == y_pred) \/ len(y_true)\n    return accuracy\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nnb = NaiveBayes()\nnb.fit(X_train, y_train)\npredictions = nb.predict(X_test)\n\nprint(\"Naive Bayes classification accuracy\", accuracy(y_test, predictions))","931adaf0":"#### 2- import Dataset","7a2c3210":"#### 1- Import Libraries","df829063":"# Naive Bayes Classificationfrom sklearn in Python - Iris Data set","1611d8a2":"### Built From Scratch..."}}