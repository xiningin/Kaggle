{"cell_type":{"7ffc70c9":"code","e268f136":"code","30e47e37":"code","51029916":"code","d627cc9f":"code","31fd4446":"code","69f47cfc":"code","321e2754":"code","e9846a3a":"code","c8984f18":"code","9f0b8815":"code","0f85e9fb":"code","415d5ea3":"code","de8f5838":"code","e034ea77":"markdown","22a5fa21":"markdown","3a4f11bc":"markdown","f55e31bc":"markdown","c43dd321":"markdown","c030ca2b":"markdown","b23c13c6":"markdown","4212e450":"markdown","7cd593f8":"markdown","e0ac1798":"markdown","d782c9e9":"markdown","fd301107":"markdown","25307649":"markdown","f7448da7":"markdown","b1c532b2":"markdown","3d48faa5":"markdown","aadd27b5":"markdown","078ea327":"markdown","be1f8c2f":"markdown","ba3e5cb4":"markdown","6d0a4ef9":"markdown","9a187262":"markdown","17a69553":"markdown","de0d1fab":"markdown","9e02640e":"markdown","561cda2a":"markdown","2aacff45":"markdown","0c9af772":"markdown"},"source":{"7ffc70c9":"# Imports\n\n# Import basic libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\nfrom collections import OrderedDict\nfrom PIL import Image\n\n# Import PyTorch\nimport torch # import main library\nfrom torch.autograd import Variable\nimport torch.nn as nn # import modules\nfrom torch.autograd import Function # import Function to create custom activations\nfrom torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\nfrom torch import optim # import optimizers for demonstrations\nimport torch.nn.functional as F # import torch functions\nfrom torchvision import transforms # import transformations to use for demo\nfrom torch.utils.data import Dataset, DataLoader ","e268f136":"# Define a transform\ntransform = transforms.Compose([transforms.ToTensor()])","30e47e37":"class FashionMNIST(Dataset):\n    '''\n    Dataset clas to load Fashion MNIST data from csv.\n    Code from original kernel:\n    https:\/\/www.kaggle.com\/arturlacerda\/pytorch-conditional-gan\n    '''\n    def __init__(self, transform=None):\n        self.transform = transform\n        fashion_df = pd.read_csv('..\/input\/fashion-mnist_train.csv')\n        self.labels = fashion_df.label.values\n        self.images = fashion_df.iloc[:, 1:].values.astype('uint8').reshape(-1, 28, 28)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        label = self.labels[idx]\n        img = Image.fromarray(self.images[idx])\n        \n        if self.transform:\n            img = self.transform(img)\n\n        return img, label\n\n# Load the training data for Fashion MNIST\ntrainset = FashionMNIST(transform=transform)\n# Define the dataloader\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)","51029916":"def train_model(model):\n    '''\n    Function trains the model and prints out the training log.\n    '''\n    #setup training\n    \n    #define loss function\n    criterion = nn.NLLLoss()\n    #define learning rate\n    learning_rate = 0.003\n    #define number of epochs\n    epochs = 5\n    #initialize optimizer\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    #run training and print out the loss to make sure that we are actually fitting to the training set\n    print('Training the model. Make sure that loss decreases after each epoch.\\n')\n    for e in range(epochs):\n        running_loss = 0\n        for images, labels in trainloader:\n            images = images.view(images.shape[0], -1)\n            log_ps = model(images)\n            loss = criterion(log_ps, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        else:\n            # print out the loss to make sure it is decreasing\n            print(f\"Training loss: {running_loss}\")","d627cc9f":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\nfig = plt.figure(figsize=(10,7))\nax = plt.axes()\n\nplt.title(\"SiLU\")\n\nx = np.linspace(-10, 10, 1000)\nax.plot(x, x * sigmoid(x), '-g');","31fd4446":"# simply define a silu function\ndef silu(input):\n    '''\n    Applies the Sigmoid Linear Unit (SiLU) function element-wise:\n\n        SiLU(x) = x * sigmoid(x)\n    '''\n    return input * torch.sigmoid(input) # use torch.sigmoid to make sure that we created the most efficient implemetation based on builtin PyTorch functions\n\n# create a class wrapper from PyTorch nn.Module, so\n# the function now can be easily used in models\nclass SiLU(nn.Module):\n    '''\n    Applies the Sigmoid Linear Unit (SiLU) function element-wise:\n    \n        SiLU(x) = x * sigmoid(x)\n\n    Shape:\n        - Input: (N, *) where * means, any number of additional\n          dimensions\n        - Output: (N, *), same shape as the input\n\n    References:\n        -  Related paper:\n        https:\/\/arxiv.org\/pdf\/1606.08415.pdf\n\n    Examples:\n        >>> m = silu()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    '''\n    def __init__(self):\n        '''\n        Init method.\n        '''\n        super().__init__() # init the base class\n\n    def forward(self, input):\n        '''\n        Forward pass of the function.\n        '''\n        return silu(input) # simply apply already implemented SiLU","69f47cfc":"# use SiLU with model created with Sequential\n\n# initialize activation function\nactivation_function = SiLU()\n\n# Initialize the model using nn.Sequential\nmodel = nn.Sequential(OrderedDict([\n                      ('fc1', nn.Linear(784, 256)),\n                      ('activation1', activation_function), # use SiLU\n                      ('fc2', nn.Linear(256, 128)),\n                      ('bn2', nn.BatchNorm1d(num_features=128)),\n                      ('activation2', activation_function), # use SiLU\n                      ('dropout', nn.Dropout(0.3)),\n                      ('fc3', nn.Linear(128, 64)),\n                      ('bn3', nn.BatchNorm1d(num_features=64)),\n                      ('activation3', activation_function), # use SiLU\n                      ('logits', nn.Linear(64, 10)),\n                      ('logsoftmax', nn.LogSoftmax(dim=1))]))\n\n# Run training\ntrain_model(model)","321e2754":"# create class for basic fully-connected deep neural network\nclass ClassifierSiLU(nn.Module):\n    '''\n    Demo classifier model class to demonstrate SiLU\n    '''\n    def __init__(self):\n        super().__init__()\n\n        # initialize layers\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        # make sure the input tensor is flattened\n        x = x.view(x.shape[0], -1)\n\n        # apply silu function\n        x = silu(self.fc1(x))\n\n        # apply silu function\n        x = silu(self.fc2(x))\n        \n        # apply silu function\n        x = silu(self.fc3(x))\n        \n        x = F.log_softmax(self.fc4(x), dim=1)\n\n        return x\n\n# Create demo model\nmodel = ClassifierSiLU()\n    \n# Run training\ntrain_model(model)","e9846a3a":"# Soft Exponential\ndef soft_exponential_func(x, alpha):\n    if alpha == 0.0:\n        return x\n    if alpha < 0.0:\n        return - np.log(1 - alpha * (x + alpha)) \/ alpha\n    if alpha > 0.0:\n        return (np.exp(alpha * x) - 1)\/ alpha + alpha\n\nfig = plt.figure(figsize=(10,7))\nax = plt.axes()\n\nplt.title(\"Soft Exponential\")\n\nx = np.linspace(-5, 5, 1000)\nax.plot(x, soft_exponential_func(x, -1.0), '-g', label = 'Soft Exponential, alpha = -1.0', linestyle = 'dashed');\nax.plot(x, soft_exponential_func(x, -0.5), '-g', label = 'Soft Exponential, alpha = -0.5');\nax.plot(x, soft_exponential_func(x, 0), '-b', label = 'Soft Exponential, alpha = 0');\nax.plot(x, soft_exponential_func(x, 0.5), '-r', label = 'Soft Exponential, alpha = 0.5');\n\nplt.legend();","c8984f18":"class soft_exponential(nn.Module):\n    '''\n    Implementation of soft exponential activation.\n\n    Shape:\n        - Input: (N, *) where * means, any number of additional\n          dimensions\n        - Output: (N, *), same shape as the input\n\n    Parameters:\n        - alpha - trainable parameter\n\n    References:\n        - See related paper:\n        https:\/\/arxiv.org\/pdf\/1602.01321.pdf\n\n    Examples:\n        >>> a1 = soft_exponential(256)\n        >>> x = torch.randn(256)\n        >>> x = a1(x)\n    '''\n    def __init__(self, in_features, alpha = None):\n        '''\n        Initialization.\n        INPUT:\n            - in_features: shape of the input\n            - aplha: trainable parameter\n            aplha is initialized with zero value by default\n        '''\n        super(soft_exponential,self).__init__()\n        self.in_features = in_features\n\n        # initialize alpha\n        if alpha == None:\n            self.alpha = Parameter(torch.tensor(0.0)) # create a tensor out of alpha\n        else:\n            self.alpha = Parameter(torch.tensor(alpha)) # create a tensor out of alpha\n            \n        self.alpha.requiresGrad = True # set requiresGrad to true!\n\n    def forward(self, x):\n        '''\n        Forward pass of the function.\n        Applies the function to the input elementwise.\n        '''\n        if (self.alpha == 0.0):\n            return x\n\n        if (self.alpha < 0.0):\n            return - torch.log(1 - self.alpha * (x + self.alpha)) \/ self.alpha\n\n        if (self.alpha > 0.0):\n            return (torch.exp(self.alpha * x) - 1)\/ self.alpha + self.alpha","9f0b8815":"# create class for basic fully-connected deep neural network\nclass ClassifierSExp(nn.Module):\n    '''\n    Basic fully-connected network to test Soft Exponential activation.\n    '''\n    def __init__(self):\n        super().__init__()\n\n        # initialize layers\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 10)\n\n        # initialize Soft Exponential activation\n        self.a1 = soft_exponential(256)\n        self.a2 = soft_exponential(128)\n        self.a3 = soft_exponential(64)\n\n    def forward(self, x):\n        # make sure the input tensor is flattened\n        x = x.view(x.shape[0], -1)\n\n        # apply Soft Exponential unit\n        x = self.a1(self.fc1(x))\n        x = self.a2(self.fc2(x))\n        x = self.a3(self.fc3(x))\n        x = F.log_softmax(self.fc4(x), dim=1)\n\n        return x\n    \nmodel = ClassifierSExp()\ntrain_model(model)","0f85e9fb":"# ReLU function\ndef relu(x):\n    return (x >= 0) * x\n# inversed ReLU\ndef inv_relu(x):\n    return - relu(- x)\n\nfig = plt.figure(figsize=(10,7))\nax = plt.axes()\n\nplt.title(\"BReLU\")\n\nx = np.linspace(-5, 5, 1000)\nax.plot(x, relu(x), '-g', label = 'BReLU, xi mod 2 = 0');\nax.plot(x, inv_relu(x), '-b', label = 'BReLU, xi mod 2 != 0', linestyle='dashed');\n\nplt.legend();","415d5ea3":"class brelu(Function):\n    '''\n    Implementation of BReLU activation function.\n\n    Shape:\n        - Input: (N, *) where * means, any number of additional\n          dimensions\n        - Output: (N, *), same shape as the input\n\n    References:\n        - See BReLU paper:\n        https:\/\/arxiv.org\/pdf\/1709.04054.pdf\n\n    Examples:\n        >>> brelu_activation = brelu.apply\n        >>> t = torch.randn((5,5), dtype=torch.float, requires_grad = True)\n        >>> t = brelu_activation(t)\n    '''\n    #both forward and backward are @staticmethods\n    @staticmethod\n    def forward(ctx, input):\n        \"\"\"\n        In the forward pass we receive a Tensor containing the input and return\n        a Tensor containing the output. ctx is a context object that can be used\n        to stash information for backward computation. You can cache arbitrary\n        objects for use in the backward pass using the ctx.save_for_backward method.\n        \"\"\"\n        ctx.save_for_backward(input) # save input for backward pass\n\n        # get lists of odd and even indices\n        input_shape = input.shape[0]\n        even_indices = [i for i in range(0, input_shape, 2)]\n        odd_indices = [i for i in range(1, input_shape, 2)]\n\n        # clone the input tensor\n        output = input.clone()\n\n        # apply ReLU to elements where i mod 2 == 0\n        output[even_indices] = output[even_indices].clamp(min=0)\n\n        # apply inversed ReLU to inversed elements where i mod 2 != 0\n        output[odd_indices] = 0 - output[odd_indices] # reverse elements with odd indices\n        output[odd_indices] = - output[odd_indices].clamp(min = 0) # apply reversed ReLU\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"\n        In the backward pass we receive a Tensor containing the gradient of the loss\n        with respect to the output, and we need to compute the gradient of the loss\n        with respect to the input.\n        \"\"\"\n        grad_input = None # set output to None\n\n        input, = ctx.saved_tensors # restore input from context\n\n        # check that input requires grad\n        # if not requires grad we will return None to speed up computation\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.clone()\n\n            # get lists of odd and even indices\n            input_shape = input.shape[0]\n            even_indices = [i for i in range(0, input_shape, 2)]\n            odd_indices = [i for i in range(1, input_shape, 2)]\n\n            # set grad_input for even_indices\n            grad_input[even_indices] = (input[even_indices] >= 0).float() * grad_input[even_indices]\n\n            # set grad_input for odd_indices\n            grad_input[odd_indices] = (input[odd_indices] < 0).float() * grad_input[odd_indices]\n\n        return grad_input","de8f5838":"class ClassifierBReLU(nn.Module):\n    '''\n    Simple fully-connected classifier model to demonstrate BReLU activation.\n    '''\n    def __init__(self):\n        super(ClassifierBReLU, self).__init__()\n\n        # initialize layers\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 10)\n\n        # create shortcuts for BReLU\n        self.a1 = brelu.apply\n        self.a2 = brelu.apply\n        self.a3 = brelu.apply\n\n    def forward(self, x):\n        # make sure the input tensor is flattened\n        x = x.view(x.shape[0], -1)\n\n        # apply BReLU\n        x = self.a1(self.fc1(x))\n        x = self.a2(self.fc2(x))\n        x = self.a3(self.fc3(x))\n        x = F.log_softmax(self.fc4(x), dim=1)\n        \n        return x\n    \nmodel = ClassifierBReLU()\ntrain_model(model)","e034ea77":"## Implement Activation Function with Learnable Parameters\n\nThere are lot's of activation functions with parameters, which can be trained with gradient descent while training the model. A great example for one of these is [SoftExponential](https:\/\/arxiv.org\/pdf\/1602.01321.pdf) function:\n\n$$SoftExponential(x, \\alpha) = \\left\\{\\begin{matrix} - \\frac{log(1 - \\alpha(x + \\alpha))}{\\alpha}, \\alpha < 0\\\\  x, \\alpha = 0\\\\  \\frac{e^{\\alpha * x} - 1}{\\alpha} + \\alpha, \\alpha > 0 \\end{matrix}\\right.$$","22a5fa21":"## Improvement\nWhile building a lot of custom activation functions, I noticed, that they often consume much more GPU memory. Creation of inplace implementations of custom activations using PyTorch inplace methods will improve this situation.","3a4f11bc":"To impement custom activation function with backward step we should:\n* create a class which, inherits Function from torch.autograd,\n* override static forward and backward methods. Forward method just applies the function to the input. Backward method should compute the gradient of the loss function with respect to the input given the gradient of the loss function with respect to the output.\n\nLet's see an example for BReLU:","f55e31bc":"We can also use silu function in model class as follows:","c43dd321":"Create a simple classifier model for a demonstration and run training:[](http:\/\/)","c030ca2b":"## Conclusion\nIn this tutorial I demonstrated:\n* How to create a simple custom activation function,\n* How to create an activation function with learnable parameters, which can be trained using gradient descent,\n* How to create an activation function with custom backward step.","b23c13c6":"## PS\n![echo logo](https:\/\/github.com\/Lexie88rus\/Activation-functions-examples-pytorch\/blob\/master\/assets\/echo_logo.png?raw=true)\n\nI participate in implementation of a __Echo package__ with mathematical backend for neural networks, which can be used with most popular existing packages (TensorFlow, Keras and [PyTorch](https:\/\/pytorch.org\/)). We have done a lot for activation functions for PyTorch so far. Here is a [link to a repository on GitHub](https:\/\/github.com\/digantamisra98\/Echo\/tree\/Dev-adeis), __I will highly appreciate your feedback__ on that.","4212e450":"Let's make a small demo: create a simple model, which uses Soft Exponential activation:","7cd593f8":"The most efficient way to transform the input data is to use buil-in PyTorch transformations:","e0ac1798":"## Seeting Up The Demo\nIh this section I will prepare everything for the demonstration:\n* Load Fashion MNIST dataset from PyTorch,\n* Introduce transformations for Fashion MNIST images using PyTorch,\n* Prepare model training procedure.\n\nIf you are familiar with PyTorch basics, just skip this part and go straight to implementation of the activation functions.","d782c9e9":"To load the data I used standard Dataset and Dataloader classes from PyTorch and [FashionMNIST class code from this kernel](https:\/\/www.kaggle.com\/arturlacerda\/pytorch-conditional-gan):","fd301107":"The implementation of SiLU:","25307649":"## Implement Activation Function with Custom Backward Step\nThe perfect example of an activation function, which needs implementation of a custom backward step is [BReLU](https:\/\/arxiv.org\/pdf\/1709.04054.pdf) (Bipolar Rectified Linear Activation Unit):\n\n$$BReLU(x_i) = \\left\\{\\begin{matrix} f(x_i), i \\mod 2 = 0\\\\  - f(-x_i), i \\mod 2 \\neq  0 \\end{matrix}\\right.$$\n\nThis function is not differenciable at 0, so automatic gradient computation may fail.\n\nPlot:","f7448da7":"To implement an activation function with trainable parameters we have to:\n* derive a class from nn.Module and make alpha one of its members,\n* wrap alpha as a Parameter and set requiresGrad to True.\n\nSee an example:","b1c532b2":"![soft exponential plot](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/b5\/Activation_soft_exponential.svg\/2880px-Activation_soft_exponential.svg.png)","3d48faa5":"## Implementing Simple Activation Functions\nThe most simple activation functions\n* are differentiable and don't need the manual implementation of the backward step,\n* don't have any trainable parameters, all their parameters are set in advance.\n\nOne of the examples of such simple functions is Sigmoid Linear Unit or just [SiLU](https:\/\/arxiv.org\/pdf\/1606.08415.pdf) also known as Swish-1:\n\n$$SiLU(x) = x * \\sigma(x) = x * \\frac{1}{1 + e^{-x}}$$\n\nPlot:","aadd27b5":"The [activation function](https:\/\/www.analyticsvidhya.com\/blog\/2017\/10\/fundamentals-deep-learning-activation-functions-when-to-use-them\/) is an essential building block for every neural network. We can choose from a huge list of popular activation functions, which are already implemented in Deep Learning frameworks, like [ReLU](https:\/\/en.wikipedia.org\/wiki\/Rectifier_(neural_networks), [Sigmoid](https:\/\/en.wikipedia.org\/wiki\/Sigmoid_function), [Tanh](https:\/\/en.wikipedia.org\/wiki\/Hyperbolic_function) and many others.\n\nBut to create a state of the art model, customized particularly for your task, you may need to use a custom activation function, which is not yet implemented in Deep Learning framework you are using. Activation functions can be roughly classified into the following groups by complexity:\n\n1. Simple activation functions like [SiLU](https:\/\/arxiv.org\/pdf\/1606.08415.pdf), [Inverse square root unit (ISRU)](https:\/\/arxiv.org\/pdf\/1710.09967.pdf). These functions can be easily implemented in any Deep Learning framework.\n2. Activation functions with __trainable parameters__ like [SoftExponential](https:\/\/arxiv.org\/pdf\/1602.01321.pdf) or [S-shaped rectified linear activation unit (SReLU)](https:\/\/arxiv.org\/pdf\/1512.07030.pdf). \n3. Activation functions, which are not differentiable at some points and require __custom implementation of backward step__, for example [Bipolar rectified linear unit (BReLU)](https:\/\/arxiv.org\/pdf\/1709.04054.pdf).\n\nIn this kernel I will try to cover implementation and demo examples for all of these types of functions using [Fashion MNIST dataset](https:\/\/www.kaggle.com\/zalando-research\/fashionmnist).","078ea327":"### Load the Data","be1f8c2f":"![image](https:\/\/github.com\/Lexie88rus\/Activation-functions-examples-pytorch\/raw\/master\/assets\/blur-blurry-close-up-167259.jpg)","ba3e5cb4":"## Activation Functions\n","6d0a4ef9":"## Additional References\nLinks to the additional resources and further reading:\n1. [Activation functions wiki page](https:\/\/en.wikipedia.org\/wiki\/Activation_function)\n2. [Tutorial on extending PyTorch](https:\/\/pytorch.org\/docs\/master\/notes\/extending.html)\n3. [Implementation of Maxout in PyTorch](https:\/\/github.com\/Usama113\/Maxout-PyTorch\/blob\/master\/Maxout.ipynb)\n4. [PyTorch Comprehensive Overview](https:\/\/medium.com\/@layog\/a-comprehensive-overview-of-pytorch-7f70b061963f)\n5. [PyTorch and Fashion MNIST Kernel](https:\/\/www.kaggle.com\/arturlacerda\/pytorch-conditional-gan) I copied some code to load the data","9a187262":"Now it's time for a small demo _(don't forget to enable GPU in kernel settings to make training faster)_","17a69553":"## Introduction\nToday deep learning is viral and applied to a variety of machine learning problems such as image recognition, speech recognition, machine translation, etc. There is a wide range of highly customizable neural network architectures, which can suit almost any problem when given enough data. Each neural network should be customized to suit the given problem well enough. You have to fine tune the hyperparameters for the network for each task (the learning rate, dropout coefficients, weight decay, etc.) as well as number of hidden layers, number of units in layers. __Choosing the right activation function for each layer is also crucial and may have a significant impact on learning speed.__","de0d1fab":"Plot (image from wikipedia):","9e02640e":"# Extending PyTorch with Custom Activation Functions","561cda2a":"Here is a small example of building a model with nn.Sequential and out custom SiLU class:","2aacff45":"### Setup Training Procedure\nI wrote a small training procedure, which runs 5 training epochs and prints the loss for each epoch, so we make sure that we are fitting the training set:","0c9af772":"### Introduce Transformations"}}