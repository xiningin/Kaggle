{"cell_type":{"8a0d5152":"code","9482fba7":"code","0fae8187":"code","0aee9c25":"code","6e8b04d2":"code","a207098a":"code","c8ab492d":"code","72f48dd4":"code","1de00b27":"code","816e6e81":"code","33d2935f":"code","1e26e153":"code","138afa35":"code","1565c3c9":"code","f547d26a":"code","918cf49b":"code","f5c62373":"code","24bfffef":"markdown","eaba7d60":"markdown","c0ed96c4":"markdown","61b7e677":"markdown","87d58bbf":"markdown","3e57d30f":"markdown","49fed6b5":"markdown","05dbbf0c":"markdown"},"source":{"8a0d5152":"import os\nfrom pathlib import Path\n\nimport tensorflow as tf\n\n\nos.listdir(\"..\/input\/chinese-mnist\")","9482fba7":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\nIMG_HEIGHT = 32\nIMG_WIDTH = 32\nCHANNELS = 1\n\nBATCH_SIZE = 32","0fae8187":"data_dir = Path('..\/input\/chinese-mnist\/data\/data')\nlist_ds = tf.data.Dataset.list_files(str(data_dir \/ '*.jpg'), shuffle=False)\nimage_count = len(list_ds)\nlist_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=False)","0aee9c25":"# check if list_ds is populated\nfor f in list_ds.take(5):\n    print(f.numpy())","6e8b04d2":"train_size = int(image_count * 0.75)\nval_size = int(image_count * 0.15)\ntest_size = int(image_count * 0.1)\n\ntrain_ds = list_ds.take(train_size)\ntest_ds = list_ds.skip(train_size)\nval_ds = test_ds.take(val_size)\ntest_ds = test_ds.skip(val_size)\n\nprint(tf.data.experimental.cardinality(train_ds).numpy())\nprint(tf.data.experimental.cardinality(val_ds).numpy())\nprint(tf.data.experimental.cardinality(test_ds).numpy())","a207098a":"def get_label(file_path):\n    # Get the class\n    label_str = tf.strings.split(tf.strings.split(file_path, \"_\")[3], \".\")[0]\n    # Start at 0\n    label_number = tf.strings.to_number(label_str, out_type=tf.dtypes.int32, name=None) - 1\n    return label_number\n\ndef decode_img(img):\n    # convert the compressed string to a 1D uint8 tensor\n    img = tf.image.decode_jpeg(img, channels=CHANNELS)\n    # resize the image to the desired size\n    return tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n\ndef process_path(file_path):\n    label = get_label(file_path)\n    # load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return img, label","c8ab492d":"train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\nval_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.map(process_path, num_parallel_calls=AUTOTUNE)","72f48dd4":"for image, label in train_ds.take(1):\n    print(\"Image shape: \", image.numpy().shape)\n    print(\"Label: \", label.numpy())","1de00b27":"def configure_for_performance(ds):\n    ds = ds.cache()\n    ds = ds.shuffle(buffer_size=1000)\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds\n\ntrain_ds = configure_for_performance(train_ds)\nval_ds = configure_for_performance(val_ds)\ntest_ds = configure_for_performance(test_ds)","816e6e81":"import matplotlib.pyplot as plt\n\nimage_batch, label_batch = next(iter(test_ds))\n\nplt.figure(figsize=(10, 10))\nfor i in range(16):\n    ax = plt.subplot(4, 4, i + 1)\n    plt.imshow(image_batch[i,:,:,0].numpy().astype(\"uint8\"))\n    label = label_batch[i]\n    plt.title(str(label.numpy()))\n    plt.axis(\"off\")","33d2935f":"from tensorflow.keras import layers\nfrom tensorflow.keras import Input\n\nnum_classes = 15\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS)),\n    # Preprocessing\n    layers.experimental.preprocessing.Rescaling(1.\/255),\n    \n    # Augmentation\n    #layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n    layers.experimental.preprocessing.RandomRotation(0.05),\n    \n    # Conv Maxpool model\n    layers.Conv2D(64, 3, activation='relu'),\n    layers.MaxPooling2D(pool_size=(2,2)),\n    layers.Conv2D(32, 3, activation='relu'),\n    layers.MaxPooling2D(pool_size=(2,2)),\n    layers.Conv2D(32, 3, activation='relu'),\n    layers.MaxPooling2D(pool_size=(2,2)),\n    layers.Flatten(),\n    \n    # Regularization\n    layers.Dropout(0.25),\n    \n    layers.Dense(128, activation='relu'),\n    layers.Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(\n  optimizer='adam',\n  loss=\"sparse_categorical_crossentropy\",\n  metrics=['accuracy'])\n\nmodel.summary()","1e26e153":"from tensorflow.keras.callbacks import ModelCheckpoint\ncheckpoint_filepath = '\/tmp\/checkpoint'\ncb_checkpoint = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)","138afa35":"history = model.fit(train_ds, epochs=40, validation_data=val_ds, batch_size=32, callbacks=[cb_checkpoint])","1565c3c9":"best_model = model\n_ = best_model.load_weights(checkpoint_filepath)","f547d26a":"best_model.evaluate(test_ds)","918cf49b":"import numpy as np\neval_lists = []\n# Predict in batches\nfor images, labels in test_ds.take(-1):  # only take first element of dataset\n    y_pred = best_model.predict(images)\n    y_pred_bool = np.argmax(y_pred, axis=1)\n    eval_lists.append(list(zip(y_pred_bool, labels.numpy())))\n\n# Place in format we can use\nimport itertools\neval_list = list(itertools.chain.from_iterable(eval_lists))\neval_t = list(zip(*eval_list))","f5c62373":"from sklearn.metrics import classification_report, confusion_matrix\n\nprint(classification_report(*eval_t))\nprint(confusion_matrix(*eval_t, labels=range(15)))","24bfffef":"Compose dataset from filenames. Generate labels and decode image.","eaba7d60":"## Dataset Visual","c0ed96c4":"## Constants","61b7e677":"### Train val test split","87d58bbf":"## Keras Model Creation\n\nCreating a simple CNN model with a callback for early stopping.","3e57d30f":"## Model evaluation","49fed6b5":"# CPU Keras CNN with TF dataset solution\n\nWe import the datast via the [tensorflow dataset API ](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset) and provide a CNN solution in Keras that runs in a reasonable ammount of time on the CPU (and very fast on a GPU). The solution uses 1\/5th the paramters of the GPU solution that can be found [here](https:\/\/www.kaggle.com\/gpreda\/tensorflow-keras-gpu-for-chinese-mnist-prediction)","05dbbf0c":"## Dataset Generation"}}