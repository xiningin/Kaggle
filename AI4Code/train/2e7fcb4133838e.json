{"cell_type":{"2b2c01ae":"code","b266ec6b":"code","0d371e7a":"code","df0c843a":"code","fa7452f1":"code","15819093":"code","af673f12":"code","1567dca8":"code","86d4fce0":"code","d5c3329f":"code","8a54e78a":"code","63fa977d":"code","ed49c562":"code","be9606af":"code","cfd9ea31":"code","bafcaca6":"code","898c3ed5":"code","0fead977":"code","c9c88567":"code","67eca798":"code","44cd680a":"code","6fe05c4d":"code","22eda936":"code","f13cebe0":"code","375809df":"code","532259fc":"code","b3f0cefd":"markdown","07592a04":"markdown","8bc116d6":"markdown"},"source":{"2b2c01ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b266ec6b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer,ColumnTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0d371e7a":"df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf.head()","df0c843a":"df.isna().sum().sort_values(ascending = False)","fa7452f1":"df['Age'] = df['Age'].fillna(df['Age'].mean())\ndf['Embarked'] = df['Embarked'].fillna('S')","15819093":"cols = ['Pclass','Name','Sex','Age','SibSp','Parch','Fare','Embarked']\nX = df[cols]\ny = df['Survived']","af673f12":"ohe = OneHotEncoder()\nvect = CountVectorizer()\n\nct = make_column_transformer(\n            (ohe,['Sex','Embarked']),\n            (vect,'Name'),\n            remainder = 'passthrough'\n)","1567dca8":"logreg = LogisticRegression(solver='liblinear',random_state=21)","86d4fce0":"pipe = make_pipeline(ct,logreg)","d5c3329f":"pipe.fit(X,y)","8a54e78a":"cross_val_score(pipe,X,y,cv=10,scoring='accuracy').mean()","63fa977d":"params = {}\nparams['logisticregression__penalty'] = ['l1','l2']\nparams['logisticregression__C'] = [0.1,1.0,10]","ed49c562":"rand_logreg = RandomizedSearchCV(pipe,param_distributions=params,cv = 10,n_iter=1000,n_jobs=-1,verbose=2)","be9606af":"rand_logreg.fit(X,y)","cfd9ea31":"rand_logreg.best_score_","bafcaca6":"ohe = OneHotEncoder()\nvect = CountVectorizer()\n\nct = make_column_transformer(\n            (ohe,['Sex','Embarked']),\n            (vect,'Name'),\n            remainder = 'passthrough'\n)\n\nxg_boost = xgb.XGBClassifier()\n\npipe = make_pipeline(ct,xg_boost)\n\npipe.fit(X,y)\n\ncross_val_score(pipe,X,y,cv = 10,scoring='accuracy').mean()","898c3ed5":"params = {}\nparams['xgbclassifier__learning_rate'] = [0.005, 0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ]\nparams['xgbclassifier__max_depth'] = [ 3, 4, 5, 6, 8, 10, 12, 15]\nparams['xgbclassifier__min_child_weight'] = [ 1, 3, 5, 7 ]\nparams['xgbclassifier__gamma'] = [ 0.0, 0.1, 0.2 , 0.3, 0.4 ]\nparams['xgbclassifier__colsample_bytree'] = [ 0.3, 0.4, 0.5 , 0.7 ]","0fead977":"rand_xgb = RandomizedSearchCV(pipe,param_distributions=params,cv=5,n_iter=10,n_jobs=-1,verbose=2)","c9c88567":"rand_xgb.fit(X,y)","67eca798":"rand_xgb.best_score_","44cd680a":"ohe = OneHotEncoder()\nvect = CountVectorizer()\n\nct = make_column_transformer(\n            (ohe,['Sex','Embarked']),\n            (vect,'Name'),\n            remainder = 'passthrough'\n)\n\nrf = RandomForestClassifier()\n\npipe = make_pipeline(ct,rf)\n\npipe.fit(X,y)\n\ncross_val_score(pipe,X,y,cv = 10,scoring='accuracy').mean()","6fe05c4d":"params = {}\nparams['randomforestclassifier__bootstrap'] = ['True','False']\nparams['randomforestclassifier__n_estimators'] = [int(x) for x in np.linspace(start=200,stop=2000,num=10)]\nparams['randomforestclassifier__max_features'] = ['auto','sqrt']\nparams['randomforestclassifier__min_samples_leaf'] = [1,2,4]\nparams['randomforestclassifier__min_samples_split'] = [2,5,10]\nparams['randomforestclassifier__max_depth'] = [int(x) for x in np.linspace(10, 110, num = 11)]\nparams['randomforestclassifier__max_depth'].append(None)","22eda936":"rand_rf = RandomizedSearchCV(pipe,param_distributions=params,cv=5,n_iter=10,n_jobs=-1,verbose=2)","f13cebe0":"rand_rf.fit(X,y)","375809df":"rand_rf.best_score_","532259fc":"print(f\"accuracy in Logistic Regression :{rand_logreg.best_score_*100:.3f}% \")\nprint(f\"accuracy in XG Boosting :{rand_xgb.best_score_*100:.3f}% \")\nprint(f\"accuracy in Random Forest :{rand_rf.best_score_*100:.3f}% \")","b3f0cefd":"## Random Forest","07592a04":"## Logistic Regression","8bc116d6":"## XG Boosting"}}