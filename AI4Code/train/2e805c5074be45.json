{"cell_type":{"211b4425":"code","4c0b6f91":"code","13b2b50f":"code","ed5dcddf":"code","1bf39b11":"code","dd759dc2":"code","e9c8905d":"code","21ff25fa":"code","193bd078":"code","22a4eee8":"code","7809a3c7":"code","1f41aeb4":"code","50f67f8c":"code","83c7b7a1":"code","b441a2be":"code","452fef8e":"code","95378135":"code","88739171":"code","3f8dc70b":"code","695afdbc":"code","f740d0b3":"code","d2e056b6":"code","fc4be91a":"code","b3f54e9c":"code","831a656e":"code","226a3a59":"code","23a42889":"code","554651b7":"code","20b6484f":"code","9cef6a16":"code","320f76e0":"code","dc732598":"code","0d1b3297":"code","9c39d14c":"code","ba1c5502":"code","3d0652b5":"code","fdd9af4f":"code","bcd899c7":"code","df81c4b1":"code","a58cdf2d":"code","72a30a9d":"code","b417ce7d":"code","c2f3c5cc":"code","d4eac546":"code","f1f77d59":"code","d3d1135d":"code","8c68dd50":"code","c992e8e1":"markdown","7b7d522c":"markdown","8b610d81":"markdown","32d44dbd":"markdown","29db34f4":"markdown","d7da7917":"markdown","49db6f58":"markdown","2087943a":"markdown","bc98d970":"markdown","ba13d337":"markdown","525246c5":"markdown","561c1719":"markdown","900cf1f5":"markdown","7e051353":"markdown","7d4b4684":"markdown","8684c03b":"markdown","dfe49693":"markdown","bfe7ddd9":"markdown","eab07c29":"markdown","aaca9f0b":"markdown","4c128908":"markdown","0d566d3e":"markdown","7edeff8d":"markdown","c1f07a44":"markdown","b8e0742e":"markdown","178e5532":"markdown","f8aac467":"markdown","d4de5dd3":"markdown","bc9d25dd":"markdown","5ef1a70f":"markdown","e25dfeb1":"markdown","7c0911ea":"markdown","e19b4626":"markdown","821d17b9":"markdown"},"source":{"211b4425":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4c0b6f91":"### import packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, ShuffleSplit, learning_curve, GridSearchCV, KFold\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.metrics import roc_curve, accuracy_score, confusion_matrix, classification_report, roc_auc_score, make_scorer, precision_recall_curve, average_precision_score \nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.ensemble import RandomForestClassifier, IsolationForest, VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n%matplotlib inline\nplt.style.use('ggplot')","13b2b50f":"def read_data(tp = \"Train\", N = 1542865627584):\n    target = pd.read_csv(\"\/kaggle\/input\/healthcare-provider-fraud-detection-analysis\/{}-{}.csv\".format(tp.title(), N))\n    pt = pd.read_csv(\"\/kaggle\/input\/healthcare-provider-fraud-detection-analysis\/{}_Beneficiarydata-{}.csv\".format(tp.title(), N))\n    in_pt = pd.read_csv(\"\/kaggle\/input\/healthcare-provider-fraud-detection-analysis\/{}_Inpatientdata-{}.csv\".format(tp.title(), N))\n    out_pt = pd.read_csv(\"\/kaggle\/input\/healthcare-provider-fraud-detection-analysis\/{}_Outpatientdata-{}.csv\".format(tp.title(), N))\n    return (in_pt, out_pt, pt, target)","ed5dcddf":"### Load Train data\nin_pt, out_pt, asl, target = read_data()","1bf39b11":"asl = asl.replace({'ChronicCond_Alzheimer': 2, 'ChronicCond_Heartfailure': 2, 'ChronicCond_KidneyDisease': 2,\n                           'ChronicCond_Cancer': 2, 'ChronicCond_ObstrPulmonary': 2, 'ChronicCond_Depression': 2, \n                           'ChronicCond_Diabetes': 2, 'ChronicCond_IschemicHeart': 2, 'ChronicCond_Osteoporasis': 2, \n                           'ChronicCond_rheumatoidarthritis': 2, 'ChronicCond_stroke': 2, 'Gender': 2 }, 0)\nasl = asl.replace({'RenalDiseaseIndicator': 'Y'}, 1).astype({'RenalDiseaseIndicator': 'int64'})","dd759dc2":"print(asl.shape)\nasl.head()","e9c8905d":"print(target.shape)\ntarget.head()","21ff25fa":"plt.title(\"Potential Fraud Test distribution\")\ntarget.groupby( [\"PotentialFraud\"] ).Provider.count().plot(kind = \"bar\", figsize = (10,6))\nplt.xlabel('Status')\nplt.ylabel('Count')\nplt.show()","193bd078":"print(in_pt.shape)\nin_pt.head()","22a4eee8":"print(out_pt.shape)\nout_pt.head()","7809a3c7":"asl['WhetherDead']= 0\nasl.loc[asl.DOD.notna(),'WhetherDead'] = 1","1f41aeb4":"target[\"target\"] = np.where(target.PotentialFraud == \"Yes\", 1, 0) ","50f67f8c":"MediCare = pd.merge(in_pt, out_pt, left_on = [ x for x in out_pt.columns if x in in_pt.columns], right_on = [ x for x in out_pt.columns if x in in_pt.columns], how = 'outer')\nMediCare.shape","83c7b7a1":"data = pd.merge(MediCare, asl,left_on='BeneID',right_on='BeneID',how='inner')\ndata.shape","b441a2be":"### Check Physicians columns for stange records and value length.\ndef len_check(data , l):\n    S = dict()\n    for i in data.columns:\n         S[i] = [x for x in data.loc[ np.any(data[[i]].notnull().to_numpy(), axis = 1)][i].unique() if (len(str(x)) < l | len(str(x)) > l ) ]\n    \n    print(S)\n\nlen_check(data[['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician']], len('PHY388358'))  ","452fef8e":"def uniq(a):\n    return np.array([len(set([i for i in x[~pd.isnull(x)]])) for x in a.values])","95378135":"### Create new variable and drop 'AttendingPhysician', 'OperatingPhysician', 'OtherPhysician'\ndata['NumPhysicians'] = uniq(data[['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician']]) \ndata = data.drop(['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician'], axis = 1)","88739171":"ClmProcedure_vars = ['ClmProcedureCode_{}'.format(x) for x in range(1,7)]\n### Create new variable \ndata['NumProc'] = data[ClmProcedure_vars].notnull().to_numpy().sum(axis = 1)","3f8dc70b":"keep = ['BeneID', 'ClaimID', 'ClmAdmitDiagnosisCode', 'NumProc' ] + ClmProcedure_vars\n### Checking if procedures is unique\nprint(data[keep].loc[data['NumProc'] != uniq( data[ClmProcedure_vars])])\n\ndata = data.drop(ClmProcedure_vars, axis = 1)","695afdbc":"ClmDiagnosisCode_vars =['ClmAdmitDiagnosisCode'] + ['ClmDiagnosisCode_{}'.format(x) for x in range(1, 11)]\n\n### Create new variable \ndata['NumClaims'] = data[ClmDiagnosisCode_vars].notnull().to_numpy().sum(axis = 1)","f740d0b3":"keep = ['BeneID', 'ClaimID', 'ClmAdmitDiagnosisCode', 'NumClaims'] + ClmDiagnosisCode_vars\n\n### Create new variable \ndata['NumClaims'] = data[ClmDiagnosisCode_vars].notnull().to_numpy().sum(axis = 1)\n\nprint(data[keep].loc[data['NumClaims'] != uniq( data[ClmDiagnosisCode_vars])].head())\n### if checking result of unique claims is not missing, we are going to add number of unique claims.","d2e056b6":"data['NumUniqueClaims'] = uniq(data[ClmDiagnosisCode_vars])\n\ndata['ExtraClm'] = data['NumClaims'] - data['NumUniqueClaims']\n\ndata = data.drop(ClmDiagnosisCode_vars, axis = 1)\ndata = data.drop(['NumClaims'], axis = 1)","fc4be91a":"### \ndata['AdmissionDt'] = pd.to_datetime(data['AdmissionDt'] , format = '%Y-%m-%d')\ndata['DischargeDt'] = pd.to_datetime(data['DischargeDt'],format = '%Y-%m-%d')\n\ndata['ClaimStartDt'] = pd.to_datetime(data['ClaimStartDt'] , format = '%Y-%m-%d')\ndata['ClaimEndDt'] = pd.to_datetime(data['ClaimEndDt'],format = '%Y-%m-%d')\n\ndata['DOB'] = pd.to_datetime(data['DOB'] , format = '%Y-%m-%d')\ndata['DOD'] = pd.to_datetime(data['DOD'],format = '%Y-%m-%d')\n\n### Number of hospitalization days\ndata['AdmissionDays'] = ((data['DischargeDt'] - data['AdmissionDt']).dt.days) + 1\n### Number of claim days \ndata['ClaimDays'] = ((data['ClaimEndDt'] - data['ClaimStartDt']).dt.days) + 1\n\ndata['Age'] = round(((data['ClaimStartDt'] - data['DOB']).dt.days + 1)\/365.25)","b3f54e9c":"data['Hospt'] = np.where(data.DiagnosisGroupCode.notnull(), 1, 0)\ndata = data.drop(['DiagnosisGroupCode'], axis = 1)","831a656e":"### Check if there were any actions after death. \ndata['DeadActions'] = np.where(np.any(np.array([ data[x] > data['DOD'] for x in ['AdmissionDt', 'DischargeDt', 'ClaimStartDt', 'ClaimEndDt']]), axis = 0), 1, 0)\n\nprint(data.loc[data['DeadActions'] > 0])\n\n### If there is no actions after death date, we will drop this variable. \ndata = data.drop(['AdmissionDt', 'DeadActions', 'DischargeDt', 'ClaimStartDt', 'ClaimEndDt', 'DOD', 'DOB'], axis = 1)","226a3a59":"data.describe(exclude = ['object'])","23a42889":"data.shape","554651b7":"data.isnull().sum()","20b6484f":"## Fill missing results using 0\ndata = data.fillna(0).copy()\ndata.columns","9cef6a16":"### Sum all results\ndf1 = data.groupby(['Provider'], as_index = False)[['InscClaimAmtReimbursed', 'DeductibleAmtPaid', 'RenalDiseaseIndicator', \n                                                  'ChronicCond_Alzheimer', 'ChronicCond_Heartfailure',\n                                                  'ChronicCond_KidneyDisease', 'ChronicCond_Cancer', \n                                                  'ChronicCond_ObstrPulmonary', 'ChronicCond_Depression', \n                                                  'ChronicCond_Diabetes', 'ChronicCond_IschemicHeart', \n                                                  'ChronicCond_Osteoporasis', 'ChronicCond_rheumatoidarthritis',\n                                                  'ChronicCond_stroke', 'WhetherDead', 'NumPhysicians', \n                                                  'NumProc','NumUniqueClaims', 'ExtraClm', 'AdmissionDays',\n                                                  'ClaimDays', 'Hospt']].sum()\n### Count number of records\ndf2 = data[['BeneID', 'ClaimID']].groupby(data['Provider']).nunique().reset_index()\n### Calculate mean\ndf3 = data.groupby(['Provider'], as_index = False)[['NoOfMonths_PartACov', 'NoOfMonths_PartBCov',\n                                                    'IPAnnualReimbursementAmt', 'IPAnnualDeductibleAmt',\n                                                    'OPAnnualReimbursementAmt', 'OPAnnualDeductibleAmt', 'Age']].mean()\n### Combine all together\ndf = df2.merge(df1, on='Provider', how='left').merge(df3, on='Provider', how='left')\nprint(df.shape, target.shape)","320f76e0":"df1 = df.merge(target, on='Provider', how='left').drop(['Provider', 'target'], axis = 1)\ndf2 = df.merge(target, on='Provider', how='left').drop(['Provider', 'PotentialFraud'], axis = 1)\nprint(df.shape, target.shape)","dc732598":"g = sns.pairplot(df1, hue = 'PotentialFraud', markers=\"+\")\ng.fig.suptitle('Plot pairwise relationships in a dataset')\nplt.show()","0d1b3297":"plt.figure(figsize=(20, 20))\nplt.title('Correlation heatmap')\nsns.heatmap(df2.corr())\nplt.show()","9c39d14c":"countFraud = target.target.value_counts()\nprint('No:', countFraud[0])\nprint('Yes:', countFraud[1])\nprint('Proportion:', round(countFraud[1] \/ countFraud[0], 2))\n### We should keep in mind that we are using unbalanced data","ba1c5502":"### Only Train dataset is labeled that why we split it to two sets train and validation\nX_train, X_val, y_train, y_val = train_test_split(df.drop(['Provider'], axis = 1), target.target.to_numpy(), test_size=0.25, random_state=1)\n\ncols = X_train.columns\n\nX_train = StandardScaler().fit_transform(X_train)\nX_val = StandardScaler().fit_transform(X_val)\n\nprint(\"Train obs: {}; Features Number: {}\".format(X_train.shape[0], X_train.shape[1]))\nprint(\"Validation obs: {};\".format(X_val.shape[0]))","3d0652b5":"## write Master Learn class which we are going to use for our analysis\nclass MasterL:\n    \n    def __init__(self, model, #### model is a method which we are going to use for detecting FRAUDS. For example: sklearn.svm\n                 X= X_train, y= y_train, test= X_val, ### data\n                 **kvars  #### additional key parameters for model\n                ):\n        self.clf = model( **kvars)\n        self.methodname = model.__name__\n        self.X_train = X\n        self.y_train = y\n        self.X_test = test\n        self.fit(self.X_train, self.y_train)\n        self.predicted = self.predict(test)\n        \n    def fit (self, X, y):\n        self.clf.fit(X, y)\n    \n    def predict(self, x):\n        return self.clf.predict(x)\n       \n    def get_score(self, y = y_val, roc = True, params = False):\n        accuracy = accuracy_score(self.predicted, y)\n        if params:\n            print(self.clf.get_params())\n        print(self.methodname+ \" metrics:\\n\")\n        print(\" Accuracy Score: %.2f%%\" % (accuracy * 100.0))\n        print(\" Confusion matrix:\", \"\\n\",confusion_matrix(y_true=y, y_pred=self.predicted))\n        print( 'Classification report:\\n', classification_report(y, self.predicted))\n        if roc:\n            print(\" ROC Score: %.2f%%\" % (roc_auc_score(y, self.clf.predict_proba(self.X_test)[:,1])))\n        \n    def plot_curves(self, y = y_val):   \n        plt.figure(figsize=(17, 5))\n        plt.subplot(131)\n        # Plot the recall precision tradeoff        \n        self.plot_pr_curve(y)\n        plt.subplot(132)        \n        self.plot_lern_curve(accuracy_score)     \n        plt.subplot(133)\n        self.plot_lern_curve(roc_auc_score)\n        plt.show()\n        \n    def plot_pr_curve(self, y = y_val):\n        \n        plt.subplot(122)\n        # Calculate average precision and the PR curve\n        average_precision = average_precision_score(y, self.predicted)\n\n        # Obtain precision and recall \n        precision, recall, _ = precision_recall_curve(y, self.clf.predict_proba(self.X_test)[:,1])\n        \n        plt.step(recall, precision, where='post')\n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.ylim([0.0, 1.05])\n        plt.xlim([0.0, 1.05])\n        plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format( average_precision))\n    \n    def plot_lern_curve(self, metrics):\n        plt.title(self.methodname + \" Learning Curves\")\n        plt.xlabel(\"Training examples\")\n        plt.ylabel(\"{}\".format(' '.join(metrics.__name__.split('_')).title()))\n        \n        train_sizes, train_scores, test_scores = learning_curve(self.clf, self.X_train, self.y_train, n_jobs=-1, \n                                                                cv = ShuffleSplit(n_splits=5, test_size=.25 , random_state = 5), \n                                                                train_sizes=np.linspace(0.5, 1.0, 10), scoring = make_scorer(metrics))\n        train_scores_mean = np.mean(train_scores, axis=1) \n        test_scores_mean = np.mean(test_scores, axis=1) \n        #plt.grid()\n\n        plt.plot(train_sizes,  train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n        plt.plot(train_sizes,  test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n        \n        plt.legend(loc=\"best\")\n    \n    def plot_roc_curve(self, y = y_val, models = None, fig = None):\n        fig = plt.figure(figsize=(15, 7))\n        ax = fig.add_subplot(121)\n        \n        self.roc_curves(ax, y, models)\n        \n        ax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n        \n        ax.set_xlabel('False Positive Rate')\n        ax.set_ylabel('True Positive Rate')\n        plt.title('Receiver Operating Characteristic (ROC) Curve')\n        \n        plt.legend(loc=\"best\")\n        \n        #if fig != None:\n            #plt.savefig( fig, bbox_inches = 'tight')\n       \n    def roc_curves(self, p, y, M):\n        if M == None:\n            fpr, tpr, thresholds = roc_curve(y, self.clf.predict_proba(self.X_test)[:,1] )\n            p.plot(fpr, tpr,  label=self.methodname )\n        else:\n            fpr, tpr, thresholds = roc_curve(y, self.clf.predict_proba(self.X_test)[:,1] )\n            p.plot(fpr, tpr,  label=self.methodname )\n            for i in M:\n                fpr, tpr, thresholds = roc_curve(y, i.clf.predict_proba(i.X_test)[:,1] )\n                p.plot(fpr, tpr,  label=i.methodname )\n\n#### Function for serching best parameters which is fiting the model and shows best results for specified method.               \ndef grid(method, parameters):\n    \n    grid_1 = GridSearchCV(method, parameters, scoring = make_scorer(accuracy_score), cv=5, n_jobs = -1)\n    grid_2 = GridSearchCV(method, parameters, scoring = make_scorer(roc_auc_score), cv=5, n_jobs = -1)\n    \n    grid_1.fit(X_train, y_train)\n    print('Best parameters using accuracy score:')\n    print(grid_1.best_params_)\n\n    grid_2.fit(X_train, y_train)\n    print('Best parameters usin ROC accuracy score:')\n    print(grid_2.best_params_)","fdd9af4f":"### Logistic regression \n### Balanced Weight and Scaled data\nML1 = MasterL(LogisticRegression, \n              penalty= 'l1',\n              solver= 'liblinear', class_weight='balanced', random_state = 5 , C = 0.001)\n# Get your performance metrics\nML1.get_score()","bcd899c7":"ML1.plot_roc_curve()\nML1.plot_pr_curve()","df81c4b1":"# SVM(scaled data)\nML2 = MasterL(SVC, \n              gamma = 'auto', probability = True, random_state= 5, class_weight= 'balanced', C=1 )\n\n# Get your performance metrics\nML2.get_score()","a58cdf2d":"ML2.plot_roc_curve(models = [ML1])\nML2.plot_pr_curve()","72a30a9d":"### Random Forest Clasifier\n# Continue fitting the model and obtain predictions\n\nML3 = MasterL(RandomForestClassifier, \n              n_estimators = 60, n_jobs = -1, random_state = 5, class_weight = 'balanced_subsample', \n              min_samples_split = 0.25\n             )\n \n# Get your performance metrics\nML3.get_score() ","b417ce7d":"ML3.plot_roc_curve(models = [ML1, ML2])\nML3.plot_pr_curve()","c2f3c5cc":"features = ML3.clf.feature_importances_\nFeatures_score = pd.DataFrame(np.array([cols, features]).T, columns = [\"VarName\", \"Importamce\"]).sort_values(by=[\"Importamce\"], ascending=False)\n\nFeatures_score.head()","d4eac546":"### Generate ensemble\nML4 = MasterL(VotingClassifier, \n              estimators=[ ('lr', ML1.clf), (\"rf\", ML3.clf)], voting='soft', n_jobs = -1\n             )\n \n# Get your performance metrics\nML4.get_score()","f1f77d59":"ML4.methodname = \"log-reg + RandomForestCl\"\nML4.plot_roc_curve(models = [ML1, ML2, ML3])\nML4.plot_pr_curve()","d3d1135d":"### Multy Layer Perceptron\nML5 = MasterL( MLPClassifier, \n              activation = 'logistic',\n              hidden_layer_sizes = (1, 3),random_state = 5, max_iter= 1000 )\n# Get your performance metrics \nML5.get_score()","8c68dd50":"ML5.plot_roc_curve(models = [ML1, ML2, ML3, ML4])\nML5.plot_pr_curve()","c992e8e1":"### Top highest scoring Random Forest Features\n","7b7d522c":"## Explore datasets ","8b610d81":"- 3. Combine Inpatient and Outpatient datasets ","32d44dbd":"- 9. Hospitalization flag 'Hospt'","29db34f4":"## 5. Multi-layer Perceptron classifier(MLP)","d7da7917":"## Group by provider each column","49db6f58":"- 4. Add Patients information","2087943a":"* ### Plot validation results","bc98d970":"### Plot validation results","ba13d337":"- 2. Adding Target numeric variable. ","525246c5":"- 5. Create a new variable \"NumPhysicians\" with number of physians(from 0 to 3): 'AttendingPhysician' not missing + 'OperatingPhysician' not missing +  'OtherPhysician' not missing","561c1719":"### Search best parametes**\n```\nkf = KFold(n_splits=5)\nest = np.linspace(10, 200, 39)\n\nRandFo = dict() \ndef kind_GridSearchCV(i):\n    acScore = list()\n    rocScore = list()    \n    for train_index, test_index in kf.split(X_train):\n        Xtrain, Xtest = X_train[train_index], X_train[test_index]\n        ytrain, ytest = y_train[train_index], y_train[test_index]\n        RandFo[i] = RandomForestClassifier(n_estimators =  i, n_jobs = -1, random_state = 5, \n                                           class_weight = 'balanced_subsample', min_samples_split = 0.25 )\n        RandFo[i].fit(Xtrain, ytrain)\n        acScore.append(accuracy_score(RandFo[i].predict(X_val), y_val))\n        rocScore.append(roc_auc_score(y_val, RandFo[i].predict_proba(X_val)[:,1]))\n        \n    return  [ i, np.mean(acScore), np.mean(rocScore) ]\n\nscores = list()\nfor i in est:\n    scores.append(kind_GridSearchCV(int(i)))\n\nscoresD = pd.DataFrame(scores, columns = ['N_est', 'Accuracy', \"ROC accuracy\"])\n\nprint(scoresD.sort_values(by=['Accuracy'], ascending=False).iloc[0])\nprint(scoresD.sort_values(by=[\"ROC accuracy\"], ascending=False).iloc[0])\n```","900cf1f5":"# Machine Learning for fraud detection ","7e051353":"## Splitting the data","7d4b4684":"### Grid Seach best Parametes for MLP\n```\nx = np.array(list(map(int, np.linspace(1, 31, 31))))\n\nparameters = { \n        'hidden_layer_sizes' : list(zip(np.tile(x, len(x)), np.repeat(x, len(x))))\n}\n\ngrid(MLPClassifier ( activation = 'logistic', random_state = 5, max_iter= 1000), parameters)\n```","8684c03b":"### Plot validation results","dfe49693":"![MediCare](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcT7EqGpB-UkZNUi3pcVWVX1nsaXAHzZpM5z7E37Ps71VHD9nFYK)","bfe7ddd9":"## Checking and impute missing records","eab07c29":"### Plot validation results","aaca9f0b":"- 1. Add Flag column 'WhetherDead' using DOD values to tell whether beneficiary is dead on not","4c128908":"### Plot validation results","0d566d3e":"## 4. Ensemble method clasifier (log-reg + Random Forest)","7edeff8d":"## Descriptive stats","c1f07a44":"- 7. Count number of claims, extra reported claims and unique.","b8e0742e":"## 2. Support Vector Machines(SVM)","178e5532":"# [HEALTHCARE PROVIDER FRAUD DETECTION ANALYSIS](https:\/\/www.kaggle.com\/rohitrox\/healthcare-provider-fraud-detection-analysis)","f8aac467":"## Adding features and combine datasets","d4de5dd3":"## 3. Random Forest Clasifier","bc9d25dd":"### Grid Seach best Parametes for Log-reg L1 regularization\n```\nparameters = { \n        'C' : np.linspace(0.001, 1.0, 1000)\n}\n\ngrid(LogisticRegression(penalty = 'l1', solver= 'liblinear', class_weight='balanced', random_state = 5), parameters)\n```","5ef1a70f":"## 1. Logistic regression","e25dfeb1":"- 8. Convert Dates and calculate days for Claim and for Admission. Calculate patient age at time of claim. ","7c0911ea":"### Grid Seach best Parametes for SVM\n```\nparameters = { \n        #'gamma' : np.linspace(0.01, .1, 5),\n        'C' : np.linspace(0.1, 1.0, 10)\n}\n\ngrid(SVC( gamma = 'auto', probability = True, random_state= 5, class_weight= 'balanced'), parameters)\n```","e19b4626":"### Grid Seach best Parametes for Log-reg L2 regularization\n```pyhon\nparameters = { \n        'C' : np.linspace(0.001, 1.0, 1000)\n}\n\ngrid(LogisticRegression(solver= 'liblinear', class_weight='balanced', random_state = 5), parameters)\n```","821d17b9":"- 6. Count number of procedures 'NumProc' for each claim."}}