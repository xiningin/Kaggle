{"cell_type":{"a1e26da3":"code","134de486":"code","4f3bdf8a":"code","3fde98f7":"code","b3538eba":"code","d31a2057":"code","99917e01":"code","e70a1b75":"code","d597f3ec":"code","0f0368e3":"code","ea4175cb":"code","94212b5d":"code","37632f7e":"code","ebd33f51":"code","05eb29db":"code","4b349398":"code","efa99bb0":"code","0a4b0ef0":"code","c4f6dd6a":"code","3cf0bbcc":"code","4e13bf16":"code","f23de659":"code","53893b6b":"code","aa676440":"code","6e0653f4":"code","e54122b8":"code","4a37b671":"code","47aeadd3":"code","11f6e5f3":"code","b82280ce":"code","3e02419d":"code","4d4e8a73":"code","0fe3017d":"code","8f6089a9":"code","450d0a6a":"code","3cec0285":"code","3012bb26":"code","823aab51":"code","4210544b":"code","1a651114":"code","7234e9a4":"code","78131f83":"code","6f59f192":"code","c5a49131":"code","65ab6bbd":"code","05ddec2f":"code","f143de0d":"code","d42b8da8":"code","a971e569":"code","1fb5ae9c":"code","844923ac":"code","fe4acaec":"code","35a7792d":"code","042126cf":"code","26778de7":"code","fff87216":"code","282e4e44":"code","38448c1b":"code","dda6c8bf":"code","7b093890":"code","0c09645f":"code","dae06357":"code","a6bd0e86":"code","3fe28a92":"code","383aadbf":"code","30fe2e6a":"code","3b40a692":"code","232d2139":"code","2563fb1b":"code","449d5aeb":"code","a959e4bc":"code","54a19479":"code","28bd9ea1":"code","89d52bd2":"code","90040043":"code","404189a8":"code","153500cd":"code","f6b028c4":"code","6a8da514":"code","fde18295":"code","915edd62":"code","4e62a275":"code","36e35226":"code","85a16e9c":"code","cb4a1ebf":"code","8446c3d2":"code","e93282d5":"code","888116c8":"code","9882b37b":"code","110c6b8d":"code","92b0d471":"markdown","3ada0e6b":"markdown","5c94a68e":"markdown","76956406":"markdown","91641795":"markdown","28f55e79":"markdown","114a4bb0":"markdown","71d13567":"markdown","ebc0147f":"markdown","ea7d4901":"markdown","6f7f58d6":"markdown","ae1b7ff1":"markdown","480f2d07":"markdown","8aa79c17":"markdown","cecd31ad":"markdown","72757ed0":"markdown","5df9d3ef":"markdown","de1f91d6":"markdown","06fda669":"markdown","424a3493":"markdown","db7e8047":"markdown","fe85a5c3":"markdown","93ca88d5":"markdown"},"source":{"a1e26da3":"import seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn import metrics\nfrom scipy.stats import skew\nfrom scipy import stats\nfrom collections import Counter\nwarnings.filterwarnings('ignore')\n%matplotlib inline","134de486":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.stats import norm\nfrom scipy import stats\nfrom scipy.stats import skew\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/house-prices-advanced-regression-techniques\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# Any results you write to the current directory are saved as output.","4f3bdf8a":"train=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n# Check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n# Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n# Now drop the 'Id' column since it's unnecessary for the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n# Check data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","3fde98f7":"train=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain.head()","b3538eba":"train.describe()","d31a2057":"###importing necesary libraries...\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","99917e01":"null_columns=train.columns[train.isnull().any()]\ntrain[null_columns].isnull().sum()","e70a1b75":"train[null_columns].head()","d597f3ec":"null_columnst=test.columns[test.isnull().any()]\ntest[null_columnst].isnull().sum()","0f0368e3":"test[null_columns].head()","ea4175cb":"# Checking Categorical Data\ntrain.select_dtypes(include=['object']).columns","94212b5d":"# Checking Categorical Data\ntest.select_dtypes(include=['object']).columns","37632f7e":"# Checking Numerical Data\ntrain.select_dtypes(include=['int64','float64']).columns","ebd33f51":"cat = len(train.select_dtypes(include=['object']).columns)\nnum = len(train.select_dtypes(include=['int64','float64']).columns)\nprint('Total Features: ', cat, 'categorical', '+',\n      num, 'numerical', '=', cat+num, 'features')","05eb29db":"# Checking Numerical Data\ntest.select_dtypes(include=['int64','float64']).columns","4b349398":"cat = len(test.select_dtypes(include=['object']).columns)\nnum = len(test.select_dtypes(include=['int64','float64']).columns)\nprint('Total Features: ', cat, 'categorical', '+',\n      num, 'numerical', '=', cat+num, 'features')","efa99bb0":"# Correlation Matrix Heatmap\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","0a4b0ef0":"# Top 10 Heatmap\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","c4f6dd6a":"most_corr = pd.DataFrame(cols)\nmost_corr.columns = ['Most Correlated Features']\nmost_corr","3cf0bbcc":"# Overall Quality vs Sale Price\nvar = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","4e13bf16":"# Living Area vs Sale Price\nsns.jointplot(x=train['GrLivArea'], y=train['SalePrice'], kind='reg')","f23de659":"# Removing outliers manually (Two points in the bottom right)\ntrain = train.drop(train[(train['GrLivArea']>4000) \n                         & (train['SalePrice']<300000)].index).reset_index(drop=True)","53893b6b":"# Living Area vs Sale Price\nsns.jointplot(x=train['GrLivArea'], y=train['SalePrice'], kind='reg')","aa676440":"# Garage Cars Area vs Sale Price\nsns.boxplot(x=train['GarageCars'], y=train['SalePrice'])","6e0653f4":"# Removing outliers manually (More than 4-cars, less than $300k)\ntrain = train.drop(train[(train['GarageCars']>3) \n                         & (train['SalePrice']<300000)].index).reset_index(drop=True)","e54122b8":"# Garage Area vs Sale Price\nsns.boxplot(x=train['GarageCars'], y=train['SalePrice'])","4a37b671":"# Garage Area vs Sale Price\nsns.jointplot(x=train['GarageArea'], y=train['SalePrice'], kind='reg')","47aeadd3":"# Removing outliers manually (More than 1000 sqft, less than $300k)\ntrain = train.drop(train[(train['GarageArea']>1000) \n                         & (train['SalePrice']<300000)].index).reset_index(drop=True)","11f6e5f3":"# Garage Area vs Sale Price\nsns.jointplot(x=train['GarageArea'], y=train['SalePrice'], kind='reg')","b82280ce":"# Basement Area vs Sale Price\nsns.jointplot(x=train['TotalBsmtSF'], y=train['SalePrice'], kind='reg')","3e02419d":"# First Floor Area vs Sale Price\nsns.jointplot(x=train['1stFlrSF'], y=train['SalePrice'], kind='reg')","4d4e8a73":"# Total Rooms vs Sale Price\nsns.boxplot(x=train['TotRmsAbvGrd'], y=train['SalePrice'])","0fe3017d":"# Total Rooms vs Sale Price\nvar = 'YearBuilt'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","8f6089a9":"training_null = pd.isnull(train).sum()\ntesting_null = pd.isnull(test).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Train\", \"Test\"])","450d0a6a":"null_many = null[null.sum(axis=1) > 200]  #a lot of missing values\nnull_few = null[(null.sum(axis=1) > 0) & (null.sum(axis=1) < 200)]  #not as much missing values","3cec0285":"null_many","3012bb26":"null_few","823aab51":"null_has_meaning = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]","4210544b":"for i in null_has_meaning:\n    train[i].fillna(\"None\", inplace=True)\n    test[i].fillna(\"None\", inplace=True)","1a651114":"null_columns=train.columns[train.isnull().any()]\ntrain[null_columns].isnull().sum()","7234e9a4":"import numpy as np\nfrom sklearn.impute import SimpleImputer\n\nSimpleImputer = SimpleImputer(strategy=\"median\")","78131f83":"training_null = pd.isnull(train).sum()\ntesting_null = pd.isnull(test).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Train\", \"Test\"])","6f59f192":"null_many = null[null.sum(axis=1) > 200]  #a lot of missing values\nnull_few = null[(null.sum(axis=1) > 0) & (null.sum(axis=1) < 200)]  #few missing values","c5a49131":"null_many","65ab6bbd":"train.drop(\"LotFrontage\", axis=1, inplace=True)\ntest.drop(\"LotFrontage\", axis=1, inplace=True)","05ddec2f":"null_columns=train.columns[train.isnull().any()]\ntrain[null_columns].isnull().sum()","f143de0d":"null_few","d42b8da8":"train[\"GarageYrBlt\"].fillna(train[\"GarageYrBlt\"].median(), inplace=True)\ntest[\"GarageYrBlt\"].fillna(test[\"GarageYrBlt\"].median(), inplace=True)\ntrain[\"MasVnrArea\"].fillna(train[\"MasVnrArea\"].median(), inplace=True)\ntest[\"MasVnrArea\"].fillna(test[\"MasVnrArea\"].median(), inplace=True)\ntrain[\"MasVnrType\"].fillna(\"None\", inplace=True)\ntest[\"MasVnrType\"].fillna(\"None\", inplace=True)","a971e569":"types_train = train.dtypes #type of each feature in data: int, float, object\nnum_train = types_train[(types_train == int) | (types_train == float)] #numerical values are either type int or float\ncat_train = types_train[types_train == object] #categorical values are type object\n\n#we do the same for the test set\ntypes_test = test.dtypes\nnum_test = types_test[(types_test == int) | (types_test == float)]\ncat_test = types_test[types_test == object]","1fb5ae9c":"#we should convert num_train and num_test to a list to make it easier to work with\nnumerical_values_train = list(num_train.index)\nnumerical_values_test = list(num_test.index)","844923ac":"print(numerical_values_train)","fe4acaec":"fill_num = []\n\nfor i in numerical_values_train:\n    if i in list(null_few.index):\n        fill_num.append(i)","35a7792d":"print(fill_num)","042126cf":"for i in fill_num:\n    train[i].fillna(train[i].median(), inplace=True)\n    test[i].fillna(test[i].median(), inplace=True)","26778de7":"categorical_values_train = list(cat_train.index)\ncategorical_values_test = list(cat_test.index)","fff87216":"print(categorical_values_train)","282e4e44":"fill_cat = []\n\nfor i in categorical_values_train:\n    if i in list(null_few.index):\n        fill_cat.append(i)","38448c1b":"print(fill_cat)","dda6c8bf":"def most_common_term(lst):\n    lst = list(lst)\n    return max(set(lst), key=lst.count)\n#most_common_term finds the most common term in a series\n\nmost_common = [\"Electrical\", \"Exterior1st\", \"Exterior2nd\", \"Functional\", \"KitchenQual\", \"MSZoning\", \"SaleType\", \"Utilities\", \"MasVnrType\"]\n\ncounter = 0\nfor i in fill_cat:\n    most_common[counter] = most_common_term(train[i])\n    counter += 1","7b093890":"most_common_dictionary = {fill_cat[0]: [most_common[0]], fill_cat[1]: [most_common[1]], fill_cat[2]: [most_common[2]], fill_cat[3]: [most_common[3]],\n                          fill_cat[4]: [most_common[4]], fill_cat[5]: [most_common[5]], fill_cat[6]: [most_common[6]], fill_cat[7]: [most_common[7]],\n                          fill_cat[8]: [most_common[8]]}\nmost_common_dictionary","0c09645f":"counter = 0\nfor i in fill_cat:  \n    train[i].fillna(most_common[counter], inplace=True)\n    test[i].fillna(most_common[counter], inplace=True)\n    counter += 1","dae06357":"training_null = pd.isnull(train).sum()\ntesting_null = pd.isnull(test).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])\nnull[null.sum(axis=1) > 0]","a6bd0e86":"sns.distplot(train[\"SalePrice\"])","3fe28a92":"sns.distplot(np.log(train[\"SalePrice\"]))","383aadbf":"train[\"TransformedPrice\"] = np.log(train[\"SalePrice\"])","30fe2e6a":"categorical_values_train = list(cat_train.index)\ncategorical_values_test = list(cat_test.index)","3b40a692":"print(categorical_values_train)","232d2139":"train.head()","2563fb1b":"for i in categorical_values_train:\n    feature_set = set(train[i])\n    for j in feature_set:\n        feature_list = list(feature_set)\n        train.loc[train[i] == j, i] = feature_list.index(j)\n\nfor i in categorical_values_test:\n    feature_set2 = set(test[i])\n    for j in feature_set2:\n        feature_list2 = list(feature_set2)\n        test.loc[test[i] == j, i] = feature_list2.index(j)","449d5aeb":"train.head()","a959e4bc":"test.head()","54a19479":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge","28bd9ea1":"X_training = train.drop([\"Id\", \"SalePrice\", \"TransformedPrice\"], axis=1).values\ny_training = train[\"TransformedPrice\"].values\nX_test = test.drop(\"Id\", axis=1).values","89d52bd2":"from sklearn.model_selection import train_test_split #to create validation data set\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_training, y_training, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets","90040043":"linreg = LinearRegression()\nlinreg.fit(X_train, y_train)\nlin_pred = linreg.predict(X_valid)\nr2_lin = r2_score(y_valid, lin_pred)\nrmse_lin = np.sqrt(mean_squared_error(y_valid, lin_pred))\nprint(\"R^2 Score: \" + str(r2_lin))\nprint(\"RMSE Score: \" + str(rmse_lin))","404189a8":"scores_lin = cross_val_score(linreg, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_lin)))","153500cd":"dtr = DecisionTreeRegressor()\ndtr.fit(X_train, y_train)\ndtr_pred = dtr.predict(X_valid)\nr2_dtr = r2_score(y_valid, dtr_pred)\nrmse_dtr = np.sqrt(mean_squared_error(y_valid, dtr_pred))\nprint(\"R^2 Score: \" + str(r2_dtr))\nprint(\"RMSE Score: \" + str(rmse_dtr))","f6b028c4":"scores_dtr = cross_val_score(dtr, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_dtr)))","6a8da514":"rf = RandomForestRegressor()\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_valid)\nr2_rf = r2_score(y_valid, rf_pred)\nrmse_rf = np.sqrt(mean_squared_error(y_valid, rf_pred))\nprint(\"R^2 Score: \" + str(r2_rf))\nprint(\"RMSE Score: \" + str(rmse_rf))","fde18295":"scores_rf = cross_val_score(rf, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_rf)))","915edd62":"lasso = Lasso()\nlasso.fit(X_train, y_train)\nlasso_pred = lasso.predict(X_valid)\nr2_lasso = r2_score(y_valid, lasso_pred)\nrmse_lasso = np.sqrt(mean_squared_error(y_valid, lasso_pred))\nprint(\"R^2 Score: \" + str(r2_lasso))\nprint(\"RMSE Score: \" + str(rmse_lasso))","4e62a275":"scores_lasso = cross_val_score(lasso, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_lasso)))","36e35226":"ridge = Ridge()\nridge.fit(X_train, y_train)\nridge_pred = ridge.predict(X_valid)\nr2_ridge = r2_score(y_valid, ridge_pred)\nrmse_ridge = np.sqrt(mean_squared_error(y_valid, ridge_pred))\nprint(\"R^2 Score: \" + str(r2_ridge))\nprint(\"RMSE Score: \" + str(rmse_ridge))","85a16e9c":"scores_ridge = cross_val_score(ridge, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_ridge)))","cb4a1ebf":"model_performances = pd.DataFrame({\n    \"Model\" : [\"Linear Regression\", \"Decision Tree Regressor\", \"Random Forest Regressor\",\"Ridge\", \"Lasso\"],\n    \"R Squared\" : [str(r2_lin)[0:5], str(r2_dtr)[0:5], str(r2_rf)[0:5], str(r2_ridge)[0:5], str(r2_lasso)[0:5]],\n    \"RMSE\" : [str(rmse_lin)[0:8], str(rmse_dtr)[0:8], str(rmse_rf)[0:8], str(rmse_ridge)[0:8], str(rmse_lasso)[0:8]]\n})\nmodel_performances.round(4)","8446c3d2":"print(\"Sorted by R Squared:\")\nmodel_performances.sort_values(by=\"R Squared\", ascending=False)","e93282d5":"print(\"Sorted by RMSE:\")\nmodel_performances.sort_values(by=\"RMSE\", ascending=True)","888116c8":"linreg.fit(X_training, y_training)","9882b37b":"submission_predictions = np.exp(linreg.predict(X_test))","110c6b8d":"submission = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": submission_predictions\n    })\n\nsubmission.to_csv(\"saleprice_3_group6.csv\", index=False)\nprint(submission.shape)","92b0d471":"These are all the categorical features in our data","3ada0e6b":"**Numerical Imputing**","5c94a68e":"**2.Correlation data**","76956406":"**Ridge**","91641795":"**Random Forest Regressor Model**","28f55e79":"**6.Valuation Models**","114a4bb0":"**Predict House Price**","71d13567":"**Imputing \"Real\" Null Values**","ebc0147f":"**Defining Training\/Test Sets**","ea7d4901":"These are all the numerical features in our data.","6f7f58d6":"**3.Imputing Null values and places where Null means something**","ae1b7ff1":"**Splitting into Validation**\n\nIt is always good to split our training data again into validation sets. This will help us evaluate our model performance as well as avoid overfitting our model.","480f2d07":"These are the categorical features in the data that have missing values in them. We'll impute with the most common term below.","8aa79c17":"**7.Submission**","cecd31ad":"Now, the features with a lot of missing values have been taken care of! Let's move on to the features with fewer missing values.","72757ed0":"**4.Feature Engineering**","5df9d3ef":"**Linear Regression Model**","de1f91d6":"**Categorical Imputing**","06fda669":"**5.Creating, Training, Evaluating, Validating, and Testing ML Models**","424a3493":"**1. Importing Packages**","db7e8047":"Great! It seems like we have changed all the categorical strings into a representative number. We are ready to build our models!","fe85a5c3":"**Lasso Model**","93ca88d5":"**Decision Tree Regressor Model**"}}