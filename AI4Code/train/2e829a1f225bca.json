{"cell_type":{"3833b599":"code","836e3824":"code","eaa4a6c3":"code","46f0a7e8":"code","30fe9c0b":"code","8f00f296":"code","46243ed2":"code","34e5b2fc":"code","83755783":"code","44894413":"code","20ad267b":"code","8b1a9148":"code","f3a28c8a":"code","9adb479b":"code","ba7a7c11":"code","3f63b18e":"code","27cf7e3c":"code","e4593a37":"code","a2505de7":"code","9c944045":"code","e4e04c9c":"code","48e3adf3":"code","31d90e81":"code","3a2678da":"code","b1cae6db":"code","240f944e":"code","e1fe6f9d":"code","06d8301f":"code","356cbd3f":"code","85092cb4":"code","3bc2c9f5":"code","4888246f":"code","b67e85d8":"code","4ee5ea55":"code","512d2d84":"code","4c11f359":"code","9126d160":"code","050d82ba":"code","1307664e":"code","25459295":"code","1e461f72":"code","4708491d":"code","13f35899":"code","e1640799":"code","c01945cc":"code","92504cea":"code","1aa388e9":"code","47050fe1":"code","4405c45f":"code","556cf094":"code","ac81d050":"code","8d2344ee":"code","8e9a9490":"code","4da884f7":"code","50f40505":"code","7c9a4f83":"code","ec83dc12":"code","c7cb2df5":"code","f40142e1":"code","e4b48870":"code","ff6ef30c":"code","6b0d1d6e":"code","e5656608":"code","a8152d09":"code","bb006e1d":"code","bb45983d":"code","492be341":"code","6d6b1203":"code","6a7a30b0":"code","99209248":"code","a1c4e0f9":"code","3cdb2ac0":"code","c2a21175":"code","a6d78091":"code","a867fc76":"code","b11d654c":"code","7f909cce":"code","a8edcc4d":"code","29f21fea":"code","c375d1d3":"code","6cc31a03":"code","c6f67423":"code","b48262ad":"code","761ab864":"code","3bff8e26":"code","c549b0d7":"code","569d9e36":"code","2cca3bf5":"code","ce83dc94":"code","a0c22a35":"code","013b8b8b":"code","e12f5d8e":"code","229f6bc3":"code","c1e421ae":"code","063f926d":"code","c71d3f3e":"code","833ee4d1":"code","5c84d865":"code","075ef44f":"code","9c0a9082":"code","53350744":"code","aeca892f":"code","358d179e":"code","9673b350":"code","3e4bac35":"code","2cc01a82":"code","03a00d84":"code","7ae63da9":"code","4b69f58c":"code","6762e8c5":"code","6447b3ef":"code","d41cf1f6":"code","9b7b01d8":"code","dcb83b0f":"code","9cc01097":"code","6084cb91":"code","c42bd96e":"code","1b0e9bc2":"code","4d906470":"code","37bd6434":"code","43cb26a8":"code","42cf2563":"code","c9da12c9":"code","8de4840f":"code","34d6c999":"code","c9793a80":"code","a707c5f3":"code","9338c9f8":"code","9685d1e5":"code","4fd7cc69":"code","4357806c":"code","efb09bda":"code","c2928fde":"code","81e97bb8":"code","ca648359":"code","7ba500ed":"code","0ce8fcad":"code","66254939":"code","8847d135":"code","b90ff510":"code","d41e5db5":"code","336a2d97":"code","b37f8bf2":"code","6454abff":"code","b119dbed":"markdown","8a66f35c":"markdown","71e17d3d":"markdown","21889e18":"markdown","1c07aa28":"markdown","f6f2d910":"markdown","4b1e484b":"markdown","f774358a":"markdown","9b65de30":"markdown","36d042cb":"markdown","a7213003":"markdown","609eefa4":"markdown","2a0dd027":"markdown","07adb116":"markdown","71f51ce6":"markdown","425597a9":"markdown","32fbfa37":"markdown","955b8d4c":"markdown","b767f4c7":"markdown","c4044241":"markdown","e978e60d":"markdown","ac31afb2":"markdown","67641e5d":"markdown","90be6666":"markdown","4c0d8fdc":"markdown","edc2eeb4":"markdown","cd8d5eba":"markdown","0ccc38e6":"markdown","edc377f5":"markdown","11af2d30":"markdown","b805b1e8":"markdown","89982877":"markdown","1e84760c":"markdown","8fde61c4":"markdown","560800ec":"markdown","a09f6074":"markdown","38a305ee":"markdown","75610f97":"markdown","9526d630":"markdown","85a34fe9":"markdown","4e7fba12":"markdown","9f511b2d":"markdown","f08f9a2c":"markdown","3ce623e3":"markdown","aedf1205":"markdown","fcfe4544":"markdown","5d1e8f12":"markdown","6d93908d":"markdown","e84c89e1":"markdown","75cff275":"markdown","59beebbf":"markdown","d9f39ab9":"markdown","7cc071de":"markdown","450ee00a":"markdown","fab9c145":"markdown","f8937624":"markdown","e6b7c05a":"markdown","d8f2c916":"markdown","cdfa201c":"markdown","c77ab371":"markdown","cac64410":"markdown","00a5f02f":"markdown","3c3ec940":"markdown","657c3730":"markdown","8cff2f73":"markdown","8e8cb716":"markdown","a7d7e51d":"markdown","9592d1d8":"markdown","b31fd325":"markdown","6549867e":"markdown","081b66ab":"markdown","cb12471a":"markdown","7201c741":"markdown","d3289cb1":"markdown","944ff0db":"markdown","0e1fa5ab":"markdown","1157fb5a":"markdown","5f6f674b":"markdown","6718d447":"markdown","f7d17cd3":"markdown","cd802226":"markdown","392f6b77":"markdown","8ea27056":"markdown","c82987e2":"markdown","aa6479bf":"markdown","072e4465":"markdown","606b1b56":"markdown","21ac3a02":"markdown","c5a7e2b4":"markdown","81a0c84c":"markdown","101dd747":"markdown","23c00aff":"markdown","42439304":"markdown","875ffe57":"markdown","20a23036":"markdown","66af07bc":"markdown","31052c21":"markdown","21c3492e":"markdown","51b18834":"markdown","c6be4d3d":"markdown","7f2a79ad":"markdown","6203cd8c":"markdown","b36696f9":"markdown","2b5ae221":"markdown","ca35fc45":"markdown","6cfe9a26":"markdown","2644f454":"markdown","0c524093":"markdown","2eec56ce":"markdown","4db76730":"markdown","76c06cf6":"markdown","25548d90":"markdown","30bbc88b":"markdown","50e98ee8":"markdown","2fee8891":"markdown","ff6a08c5":"markdown","6e49aa93":"markdown","6d725c5a":"markdown","d6cc598c":"markdown","e097092d":"markdown","71367e28":"markdown","3181c128":"markdown","347765bd":"markdown","4f6e8626":"markdown","a0ba0aa5":"markdown","96fe32cf":"markdown","f84d5cba":"markdown","1e970f53":"markdown","98e34a58":"markdown","047654a1":"markdown","f5653b40":"markdown","6017512c":"markdown","584356ca":"markdown","fbf04c8d":"markdown","8ded39e0":"markdown","7a0ea8df":"markdown","50cf0617":"markdown","3502f620":"markdown","d59ccb1b":"markdown","686fa47e":"markdown","662f2fd1":"markdown","f8f42462":"markdown","a5b7bca8":"markdown","b5746ade":"markdown","59692c15":"markdown","a96e2f9d":"markdown","f97956f7":"markdown","c204f9c1":"markdown"},"source":{"3833b599":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy import stats\nsns.set(style=\"darkgrid\", color_codes=True)\nfrom mpl_toolkits.mplot3d import Axes3D \nfrom scipy.stats import skew, norm, probplot, boxcox","836e3824":"##Imported the test and train datasets\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n##Saved the IDs to the side\n\nTrain_id = train['Id']\nTest_id = test['Id']\n\n#Dropping the 'id' column as they are not predictive\n\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","eaa4a6c3":"def dfdetails(df, pred=None): \n    types = df.dtypes\n    counts = df.apply(lambda x: x.count())\n    nulls = df.apply(lambda x: x.isnull().sum())\n    distincts = df.apply(lambda x: x.unique().shape[0])\n    missing_ration = (df.isnull().sum()\/ df.shape[0]) * 100\n    skewness = df.skew()\n    kurtosis = df.kurt() \n    \n    if pred is None:\n        cols = ['types', 'counts', 'distincts', 'nulls', 'missing ration', 'skewness', 'kurtosis']\n        df2 = pd.concat([types, counts, distincts, nulls, missing_ration, skewness, kurtosis], axis = 1)\n\n    else:\n        corr = df.corr()[pred]\n        df2 = pd.concat([types, counts, distincts, nulls, missing_ration, skewness, kurtosis, corr], axis = 1, sort=False)\n        corr_col = 'corr '  + pred\n        cols = ['types', 'counts', 'distincts', 'nulls', 'missing_ration', 'skewness', 'kurtosis', corr_col ]\n    \n    df2.columns = cols\n    \n    return df2","46f0a7e8":"details_train = dfdetails(train, 'SalePrice')\ndisplay(details_train.sort_values(by='corr SalePrice', ascending=False))","30fe9c0b":"fig = plt.figure(figsize=(20,7))\n\n##Plotting the distribution of Sales price by using a kernel density plot\nfig1 = fig.add_subplot(121)\nsns.distplot(train.SalePrice, fit=norm)\nfig1.set_title('Sales Price distribution', loc='center')\nfig1.set_xlabel('Sales Price')\nfig1.set_ylabel('Frequency')\n\n##Plotting the probability distribution of Sales price by using a quantile-quantile plot to see how and where the data deviates\n##from the normal distribution\nfig2 = fig.add_subplot(122)\nres = probplot(train.SalePrice, plot=fig2)\nfig2.set_title('Sales Price probability plot', loc='center')","8f00f296":"plt.figure(figsize=(20, 15))\nsns.boxplot(x='OverallQual', y='SalePrice', data=train[['SalePrice', 'OverallQual']])","46243ed2":"plt.figure(figsize=(20, 15))\nsns.regplot(x='GrLivArea', y='SalePrice', data=train[['SalePrice', 'GrLivArea']]) ","34e5b2fc":"plt.figure(figsize=(20, 15))\nsns.regplot(x='GarageCars', y='SalePrice', data=train[['SalePrice', 'GarageCars']])","83755783":"plt.figure(figsize=(20, 15))\nsns.regplot(x='GarageArea', y='SalePrice', data=train[['SalePrice', 'GarageArea']])","44894413":"plt.figure(figsize=(20, 15))\nsns.regplot(x='TotalBsmtSF', y='SalePrice', data=train[['SalePrice', 'TotalBsmtSF']])","20ad267b":"display(details_train.sort_values(by='nulls', ascending=False))","8b1a9148":"details_test = dfdetails(test)\ndisplay(details_test.sort_values(by='nulls', ascending=False))","f3a28c8a":"#The test dataset lacks a sales price column, so using this we will be able to combine the datasets\ntest['SalePrice'] = 0\n\nall_data = pd.concat((train, test)).reset_index(drop=True)\n\ndetails_all = dfdetails(all_data)\ndisplay(details_all[details_all['nulls'] > 0].sort_values(by='nulls', ascending=False))","9adb479b":"all_data.loc[(all_data.PoolArea==0) & (all_data.PoolQC.isnull()), ['PoolQC']] = 'NA'","ba7a7c11":"##Checked if all nulls are gone\nall_data.PoolQC.isnull().sum()","3f63b18e":"all_data[(all_data['PoolArea'] > 0) & (all_data['PoolQC'].isnull())][['OverallQual', 'PoolQC']]","27cf7e3c":"poolind = all_data[(all_data['PoolArea'] > 0) & (all_data['PoolQC'].isnull())][['OverallQual', 'PoolQC']].index","e4593a37":"all_data[(all_data['PoolArea'] > 0) & (all_data['PoolQC'].notnull())][['OverallQual', 'PoolQC']]","a2505de7":"# Overall quality seems to be a good indicator of pool quality, so took overall quality and divided it by two for the three\n#houses where a pool was present but no pool QC was recorded. Additionally, numbers have been put into a categorical column, \n##so need to convert them to the categories to which they correspond. This is what the apply function at the end of the statement\n##does.\n\nall_data.loc[(all_data['PoolArea']>0) & (all_data['PoolQC'].isnull()), 'PoolQC'] =\\\n        (all_data.loc[(all_data['PoolArea']>0) & (all_data['PoolQC'].isnull()), 'OverallQual']\/2).round().astype(int).\\\napply({0: 'NA', 1: 'Po', 2: 'Fa', 3: 'TA', 4: 'Gd', 5: 'Ex'}.get)","9c944045":"all_data.iloc[poolind][['PoolQC']]","e4e04c9c":"all_data[(all_data['MiscVal'] > 0) & (all_data['MiscFeature'].isnull())][['MiscFeature','MiscVal']]","48e3adf3":"all_data[['MiscVal', 'MiscFeature']].groupby(['MiscFeature']).median()","31d90e81":"all_data[all_data['MiscFeature'].notnull()].sort_values(by = 'MiscVal', ascending = False)[['MiscVal', 'MiscFeature']].head()","3a2678da":"##The most expensive miscellaneous feature is a second garage, so its very likely that our missing feature is a second garage.\nall_data.loc[(all_data['MiscFeature'].isnull()) & (all_data['MiscVal']>0), ['MiscFeature']] = 'Gar2'","b1cae6db":"#The rest of the nulls correspond to a lack of a miscellaneous feature so they were filled with NA\nall_data['MiscFeature'] = all_data['MiscFeature'].fillna('NA')","240f944e":"##Nulls most probably indicate a lack of an alley\nall_data['Alley'] = all_data['Alley'].fillna('NA')","e1fe6f9d":"##Nulls most probably indicate a lack of an alley\nall_data['Fence'] = all_data['Fence'].fillna('NA')","06d8301f":"all_data[all_data['FireplaceQu'].isnull()]['Fireplaces'].sum()","356cbd3f":"all_data.loc[(all_data['Fireplaces']==0) & (all_data['FireplaceQu'].isnull()), ['FireplaceQu']] = 'NA'","85092cb4":"sns.jointplot(x = 'LotFrontage', y = 'SalePrice', data=train, kind = 'reg')","3bc2c9f5":"plt.figure(figsize=(25,8))\nsns.boxplot(x='Neighborhood', y='LotFrontage', data=train[['Neighborhood', 'LotFrontage']])","4888246f":"all_data['LotFrontage'] = all_data.groupby('Neighborhood').LotFrontage.transform(lambda x: x.fillna(x.median()))\n","b67e85d8":"#By making a list of the garage related features, we can filter to see whether the data clean-up is working\ngrg = ['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond']\n\n##Saw from the dfdetails output that garage type had two less missing values than the rest of the garage related featues.\n##Wanted to investigate what information these entirs had.\nall_data[all_data['GarageYrBlt'].isnull()][grg].sort_values(by = 'GarageType', ascending = False)","4ee5ea55":"##Are there terms where Garagetype is null but the area is more than 0\nall_data[(all_data['GarageType'].isnull()) & (all_data['GarageArea'] > 0)]","512d2d84":"##No rows fit the above criteria, so filled the nulls with NA indicating a gargage is not present at the house in question\nall_data['GarageType'] = all_data['GarageType'].fillna('NA')","4c11f359":"##Let's fix the entries with garage type detached \nall_data[(all_data['GarageType'] == 'Detchd') & (all_data['GarageYrBlt'].isnull())][grg]","9126d160":"##To check if the NAs are filled afterwards, will need the indexes of these two rows\nmissgrg = all_data[(all_data['GarageType'] == 'Detchd') & (all_data['GarageYrBlt'].isnull())].index","050d82ba":"all_data[all_data['GarageType']=='Detchd']['GarageArea'].median()","1307664e":"all_data[(all_data['GarageType'] == 'Detchd') & (all_data['GarageYrBlt'].isnull())][['GarageType', 'GarageArea']]","25459295":"all_data.loc[(all_data['GarageType']=='Detchd') & (all_data['GarageArea'].isnull()), ['GarageArea']] = 399.5","1e461f72":"all_data[all_data['GarageType']=='Detchd']['GarageCars'].median()","4708491d":"all_data.loc[(all_data['GarageType']=='Detchd') & (all_data['GarageCars'].isnull()), ['GarageCars']] = 2.0","13f35899":"all_data[all_data['GarageType']=='Detchd']['GarageYrBlt'].median()","e1640799":"all_data.loc[(all_data['GarageType']=='Detchd') & (all_data['GarageYrBlt'].isnull()), ['GarageYrBlt']] = 1962.0","c01945cc":"all_data[all_data['GarageType']=='Detchd']['GarageFinish'].mode()","92504cea":"all_data.loc[(all_data['GarageType']=='Detchd') & (all_data['GarageFinish'].isnull()), ['GarageFinish']] = 'Unf'","1aa388e9":"all_data[all_data['GarageType']=='Detchd']['GarageQual'].mode()","47050fe1":"all_data.loc[(all_data['GarageType']=='Detchd') & (all_data['GarageQual'].isnull()), ['GarageQual']] = 'TA'","4405c45f":"all_data[all_data['GarageType']=='Detchd']['GarageCond'].mode()","556cf094":"all_data.loc[(all_data['GarageType']=='Detchd') & (all_data['GarageCond'].isnull()), ['GarageCond']] = 'TA'","ac81d050":"all_data.iloc[missgrg][grg]","8d2344ee":"all_data['GarageArea'] = all_data['GarageArea'].fillna(0)\nall_data['GarageCars'] = all_data['GarageCars'].fillna(0)\nall_data['GarageYrBlt'] = all_data['GarageYrBlt'].fillna(0)\nall_data['GarageFinish'] = all_data['GarageFinish'].fillna('NA')\nall_data['GarageQual'] = all_data['GarageQual'].fillna('NA')\nall_data['GarageCond'] = all_data['GarageCond'].fillna('NA')","8e9a9490":"##So we can keep track of the basement related features\nbsmt = ['BsmtExposure', 'BsmtFinType2', 'BsmtQual', 'BsmtCond', 'BsmtFinType1','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF' ,'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']\n\n##Basement exposure and conditions have the most missing values, so will hopefully be able to view all the basement related nulls\n##by filtering on nulls in those columns\nall_data[(all_data['BsmtExposure'].isnull()) | (all_data['BsmtCond'].isnull())][bsmt]","4da884f7":"all_data[all_data['TotalBsmtSF']>0]['BsmtExposure'].value_counts()","50f40505":"all_data.loc[(all_data['BsmtExposure'].isnull()) & (all_data['TotalBsmtSF'] > 0), ['BsmtExposure']] = 'No'","7c9a4f83":"all_data[all_data['TotalBsmtSF']>0]['BsmtQual'].value_counts()","ec83dc12":"all_data.loc[(all_data['BsmtQual'].isnull()) & (all_data['TotalBsmtSF'] > 0), ['BsmtQual']] = 'TA'","c7cb2df5":"all_data[all_data['TotalBsmtSF']>0]['BsmtCond'].value_counts()","f40142e1":"all_data.loc[(all_data['BsmtCond'].isnull()) & (all_data['TotalBsmtSF'] > 0), ['BsmtCond']] = 'TA'","e4b48870":"all_data[(all_data['BsmtFinSF2']>0) & (all_data['BsmtFinType2'].isnull())][bsmt]","ff6ef30c":"##Since BsmtUnfSF is more than 0, change BsmtFinType2 to unfinished\nall_data.loc[(all_data['BsmtFinType2'].isnull()) & (all_data['BsmtFinSF2'] > 0), ['BsmtFinType2']] = 'Unf'","6b0d1d6e":"#The rest of the data nulls correspond to a lack of a basement in general, a second basement or a feature in the basement\nbsmt_nulls = {'BsmtExposure': 'NA', 'BsmtFinType2': 'NA', 'BsmtQual': 'NA', 'BsmtCond': 'NA', 'BsmtFinType1': 'NA',\n              'BsmtFinSF1': 0, 'BsmtFinSF2': 0, 'BsmtUnfSF': 0 ,'TotalBsmtSF': 0, 'BsmtFullBath': 0, 'BsmtHalfBath': 0}\nall_data = all_data.fillna(value=bsmt_nulls)","e5656608":"all_data[(all_data['MasVnrType'].isnull()) & (all_data['MasVnrArea'] > 0)]['MasVnrArea'].count()","a8152d09":"all_data['MasVnrType'].value_counts()","bb006e1d":"# All MasVnrType nulls with an MasVnrArea greater than 0 were updated to the mode, BrkFace\nall_data.loc[(all_data['MasVnrType'].isnull()) & (all_data['MasVnrArea']>0), ['MasVnrType']] = 'BrkFace'","bb45983d":"# Filling 0 and None for records where both MasVnrArea and MasVnrType are nulls\nall_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)\nall_data['MasVnrType'] = all_data['MasVnrType'].fillna('None')","492be341":"all_data[(all_data['MasVnrType'] == 'None') & (all_data['MasVnrArea'] > 0)]['MasVnrType'].count()","6d6b1203":"##Since there are houses with veneer type none but have an area of more than 0, will change the type to the mode\nall_data.loc[(all_data['MasVnrType']=='None') & (all_data['MasVnrArea']>0), ['MasVnrType']] = 'BrkFace'","6a7a30b0":"all_data[(all_data['MasVnrType'] != 'None') & (all_data['MasVnrArea'] == 0)]['MasVnrType'].count()","99209248":"all_data[(all_data['MasVnrArea'] > 0)]['MasVnrArea'].median()","a1c4e0f9":" all_data.loc[(all_data['MasVnrType']!='None') & (all_data['MasVnrArea'] == 0), ['MasVnrArea']] = 202.5","3cdb2ac0":"##MSZoning can't be NA since it denotes the general zoning classification of the sale\nall_data['MSZoning'].value_counts()","c2a21175":"all_data['MSZoning'] = all_data['MSZoning'].fillna('RL')","a6d78091":"all_data['Utilities'].value_counts()","a867fc76":"all_data.drop('Utilities', axis=1, inplace=True)","b11d654c":"all_data['Functional'].value_counts()","7f909cce":"all_data['Functional'] = all_data['Functional'].fillna('Typ')\n","a8edcc4d":"all_data['Exterior2nd'].value_counts()","29f21fea":"all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna('VinylSd')","c375d1d3":"all_data['Exterior1st'].value_counts()","6cc31a03":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna('VinylSd')","c6f67423":"all_data[all_data['KitchenQual'].isnull()]","b48262ad":"all_data[(all_data['KitchenAbvGr'] > 0 ) & (all_data['KitchenQual'].isnull())]","761ab864":"##Since there is a kitchen but its quality is missing, will fill with the mode.\nall_data['KitchenQual'].value_counts()","3bff8e26":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna('TA')","c549b0d7":"all_data['Electrical'].value_counts()","569d9e36":"all_data['Electrical'] = all_data['Electrical'].fillna('SBrkr')","2cca3bf5":"all_data['SaleType'].value_counts()","ce83dc94":"all_data['SaleType'] = all_data['SaleType'].fillna('WD')","a0c22a35":"print(\"Data nulls:\", all_data.isnull().sum().sum())\n##Still some nulls","013b8b8b":"sorted(all_data['GarageYrBlt'].unique())","e12f5d8e":"all_data['GarageYrBlt'].max()","229f6bc3":"all_data.loc[all_data.GarageYrBlt==2207.0, 'GarageYrBlt'] = 2007.0\n","c1e421ae":"def ordinal_conv(df):\n    \n    #Dictionaries with all the conversions for the ordinals from words to numbers\n    \n    Func = dict(Typ=7,Min1=6,Min2=5 ,Mod= 4, Maj1=3, Maj2=2, Sev=1, Sal=0) \n    BaseFinish = dict(GLQ=6,ALQ=5 , BLQ= 4, Rec=3, LwQ=2, Unf=1, NA=0) \n    Qual = dict(Ex=5, Gd= 4, TA=3, Fa=2, Po=1, NA = 0) \n    Fence = dict(GdPrv=4, MnPrv= 3, GdWo=2, MnWw=1, NA=0)\n    BasementExp = dict(Gd=4, Av= 3, Mn=2, No=1, NA=0)\n    LotShp = dict(Reg=4, IR1= 3, IR2=2, IR3=1)\n    GargFin = dict(Fin = 3, RFn=2, Unf=1, NA=0)\n    Slope =  dict(Gtl=3, Mod=2, Sev=1) \n    Paved = dict(Y=3, P=2, N=1)\n    CentAir = dict(Y=1, N=0)\n    Strt = dict(Pave=1, Grvl=0)\n\n    # Functional: Home functionality\n    df['Functional'] = df['Functional'].apply(Func.get)\n    \n    \n    # BsmtFinType1: Rating of basement finished area\n     \n    df['BsmtFinType1'] = df['BsmtFinType1'].apply(BaseFinish.get)\n    \n    # BsmtFinType2: Rating of basement finished area (if multiple types)\n    \n    df['BsmtFinType2'] = df['BsmtFinType2'].apply(BaseFinish.get)\n    \n    \n    \n    \n    \n    # ExterQual: Evaluates the quality of the material on the exterior\n    \n    df['ExterQual'] = df['ExterQual'].apply(Qual.get)\n    \n    # ExterCond: Evaluates the present condition of the material on the exterior\n    \n    df['ExterCond'] = df['ExterCond'].apply(Qual.get)\n    \n    #HeatingQC: Heating quality and condition\n\n    df['HeatingQC'] = df['HeatingQC'].apply(Qual.get)\n\n    # KitchenQual: Kitchen quality\n    \n    df['KitchenQual'] = df['KitchenQual'].apply(Qual.get)\n\n    # FireplaceQu: Fireplace quality\n\n    df['FireplaceQu'] = df['FireplaceQu'].apply(Qual.get)\n    \n    # GarageCond: Garage Conditionals\n    \n    df['GarageCond'] = df['GarageCond'].apply(Qual.get)\n    \n    # BsmtQual: Evaluates the height of the basement\n    \n    df['BsmtQual'] = df['BsmtQual'].apply(Qual.get)\n    \n    # BsmtCond: Evaluates the general condition of the basement\n    \n    df['BsmtCond'] = df['BsmtCond'].apply(Qual.get)\n    \n    # GarageQual: Garage quality\n    \n    df['GarageQual'] = df['GarageQual'].apply(Qual.get)\n    \n    # PoolQC: Pool quality\n    \n    df['PoolQC'] = df['PoolQC'].apply(Qual.get)\n    \n    \n    # LotShape: General shape of property\n    \n    df['LotShape'] = df['LotShape'].apply(LotShp.get)\n    \n    \n    # Fence: Fence quality\n\n    \n    df['Fence'] = df['Fence'].apply(Fence.get)\n    \n    \n    \n    \n    \n    # BsmtExposure: Refers to walkout or garden level walls\n    \n    df['BsmtExposure'] = df['BsmtExposure'].apply(BasementExp.get)\n    \n    \n    \n    \n    \n    # LandSlope: Slope of property\n  \n    df['LandSlope'] = df['LandSlope'].apply(Slope.get)\n    \n    # GarageFinish: Interior finish of the garage\n    \n    df['GarageFinish'] = df['GarageFinish'].apply(GargFin.get)\n    \n    #PavedDrive: Paved driveway\n    \n    df['PavedDrive'] = df['PavedDrive'].apply(Paved.get)\n    \n        \n    \n\n    #CentralAir: Central air conditioning\n    \n    \n    df['CentralAir'] = df['CentralAir'].apply(CentAir.get)\n    \n\n    \n    #Street: Type of road access to property\n\n    \n    df['Street'] = df['Street'].apply(Strt.get)\n    \nordinal_conv(all_data)","063f926d":"# Correct Categorical from int to str types\nall_data['MSSubClass'] = all_data['MSSubClass'].astype('str')\nall_data['MoSold'] = all_data['MoSold'].astype('str')","c71d3f3e":"def one_hot_encode_processing(df, drop_zero = 'yes'):\n    categorical_cols = df.select_dtypes(include=['object']).columns\n\n    print(len(categorical_cols), \"categorical columns\")\n    print(categorical_cols)\n    # Remove special charactres and with spaces. \n    for col in categorical_cols:\n        df[col] = df[col].str.replace('\\W', '').str.replace(' ', '_') #.str.lower()\n\n    dummies = pd.get_dummies(df[categorical_cols], columns = categorical_cols).columns\n    df = pd.get_dummies(df, columns = categorical_cols)\n\n    ##This part of the function removes dummies that correspond to categories present in the train set but not in the test set\n    ##and vice versa, these columns can decrease the accuracy of the model downstream. If the user has a test set that will\n    ##increase in size later and wishes to keep these columns, they may select no.\n    \n    if drop_zero == 'yes':\n        # Find Dummies with all test observatiosn are equal to 0\n        ZeroTest = df[dummies][train.shape[0]:].sum()==0\n        df.drop(dummies[ZeroTest], axis=1, inplace=True)\n        print('Dummies in test dataset with all observatios equal to 0:',len(dummies[ZeroTest]),'of \\n',dummies[ZeroTest],'\\n')\n        dummies = dummies.drop(dummies[ZeroTest])\n\n        # Find dummies with all training observatiosn are equal to 0\n        ZeroTrain = df[dummies][:train.shape[0]].sum()==0\n        df.drop(dummies[ZeroTrain], axis=1, inplace=True)\n        print('Dummies in training dataset with all observatios equal to 0:',len(dummies[ZeroTrain]),'of \\n',dummies[ZeroTrain],'\\n')\n        dummies = dummies.drop(dummies[ZeroTrain])\n\n        del ZeroTest\n        del ZeroTrain\n    elif drop_zero == 'no':\n        df = df\n        dummies = dummies\n    else:\n        raise ValueError('drop_zero can only take the options yes and no')\n    \n    return df, dummies","833ee4d1":"all_data, dummies = one_hot_encode_processing(all_data)","5c84d865":"def data_prep(df, transy = 'no'):\n    \n    X = df.drop('SalePrice', axis=1)\n    y = df[df['SalePrice']>0]['SalePrice']\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    X_standardize = pd.DataFrame(X_scaled,columns=X.columns)\n    test_ind = all_data[all_data['SalePrice'] == 0].index.tolist()\n    train_ind = all_data[all_data['SalePrice'] > 0].index.tolist()\n    if transy == 'no':\n        y = y\n    elif transy == 'yes':\n        y = np.log1p(y)\n    return X_standardize.iloc[train_ind], X_standardize.iloc[test_ind], y","075ef44f":"X_train, X_test, y_train = data_prep(all_data)","9c0a9082":"ridge = Ridge()\nridge.fit(X_train, y_train)\ntest_ridge = ridge.predict(X_test)","53350744":"lasso = Lasso()\nlasso.fit(X_train, y_train)\ntest_lasso = lasso.predict(X_test)","aeca892f":"rfr = RandomForestRegressor(random_state = 2)\nrfr.fit(X_train, y_train)\ntest_rfr = rfr.predict(X_test)","358d179e":"def get_sub(arr, name):\n    sub = np.column_stack((Test_id, arr))\n    sub_2 = pd.DataFrame(sub)\n    sub_2.columns = ['Id','SalePrice']\n    sub_2.Id = sub_2.Id.astype(int)\n    return sub_2.to_csv(name, index = False)","9673b350":"#get_sub(test_ridge, 'test_ridge.csv')\n#get_sub(test_lasso, 'test_lasso.csv')\n#get_sub(test_rfr, 'test_rfr.csv')","3e4bac35":"train2= train.copy()\ntrain2['SalePrice']=np.log1p(train2['SalePrice'])\n\n\n##Lets see how the data is distributed after log transformation\n\n\nfig = plt.figure(figsize=(20,7))\n\n##Plotting the distribution of Sales price by using a kernel density plot\nfig1 = fig.add_subplot(121)\nsns.distplot(train2.SalePrice, fit=norm)\nfig1.set_title('Sales Price distribution', loc='center')\nfig1.set_xlabel('Sales Price')\nfig1.set_ylabel('Frequency')\n\n##Plotting the probability distribution of Sales price by using a quantile-quantile plot to see how and where the data deviates\n##from the normal distribution\nfig2 = fig.add_subplot(122)\nres = probplot(train2.SalePrice, plot=fig2)\nfig2.set_title('Sales Price probability plot', loc='center')","2cc01a82":"#Excluded the OHE columns that have a uint8 format\ndetails_all_noOHE = dfdetails(all_data[all_data.select_dtypes(exclude=['uint8']).columns.tolist()])\ndisplay(details_all_noOHE.sort_values(by='skewness', ascending=False))","03a00d84":"trans_cols = ['LotArea',\n'MasVnrArea',\n'BsmtFinSF1',\n'BsmtFinSF2',\n'BsmtUnfSF',\n'TotalBsmtSF',\n'1stFlrSF',\n'2ndFlrSF',\n'LowQualFinSF',\n'GrLivArea',\n'GarageArea',\n'WoodDeckSF',\n'OpenPorchSF',\n'EnclosedPorch',\n'3SsnPorch',\n'ScreenPorch',\n'PoolArea']\n\nall_data[trans_cols] = all_data[trans_cols].apply(lambda x : np.log1p(x))","7ae63da9":"X_train, X_test, y_train = data_prep(all_data, transy = 'yes')","4b69f58c":"def get_pred():\n    ridge.fit(X_train, y_train)\n    test_ridge = ridge.predict(X_test)\n    \n    lasso.fit(X_train, y_train)\n    test_lasso = lasso.predict(X_test)\n    \n    rfr.fit(X_train, y_train)\n    test_rfr = rfr.predict(X_test)\n\n    return np.expm1(test_ridge), np.expm1(test_lasso), np.expm1(test_rfr)","6762e8c5":"test_ridge_exp, test_lasso_exp, test_rfr_exp = get_pred()","6447b3ef":"#get_sub(test_ridge_exp, 'test_ridge_log.csv')\n#get_sub(test_lasso_exp, 'test_lasso_log.csv')\n#get_sub(test_rfr_exp, 'test_rfr_log.csv')","d41cf1f6":"ridge = Ridge(random_state = 2)\nrfr = RandomForestRegressor(random_state = 2)\nlasso = Lasso(random_state = 2)","9b7b01d8":"param_grid_ridge = dict(alpha = [0.001, 0.1, 0.5, 1, 5, 10,15,20], fit_intercept = [True, False], max_iter=[100,110,120,130,140])","dcb83b0f":"param_grid_lasso = dict(alpha = [0.0001, 0.001, 0.05, 0.01, 0.5, 1], max_iter=[100,110,120,130,140])","9cc01097":"param_grid_rfr = dict(max_depth = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], max_features = [0.1,0.3, 0.5,0.7,0.9,\"log2\",\"sqrt\"])","6084cb91":"def get_best_params(iterations = 30):\n    pg = {\"lasso\": param_grid_lasso, \"ridge\": param_grid_ridge, \"rfr\" : param_grid_rfr}\n    md = {\"lasso\" : lasso, \"ridge\" : ridge, \"rfr\" : rfr}\n    b = pd.DataFrame()\n    #This loops through the dictionary that contains the three regressors.\n    for key,val in pg.items():\n        random = RandomizedSearchCV(estimator = md[key], param_distributions = val, cv = 5, n_iter = iterations, random_state=2)\n        random_result = random.fit(X_train, y_train)\n        a = pd.DataFrame([{'model': key, 'BestScore' : random_result.best_score_, 'BestParameters' : random_result.best_params_}])\n        b = b.append(a)\n    return b","c42bd96e":"#this is commented out as several warnings appear for lasso and random forests, so just saved the output to csv, the results\n#are provided below.\n#get_best_params(100).to_csv('HT_log.csv')","1b0e9bc2":"ridge = Ridge(alpha = 20, fit_intercept=True, max_iter = 100, random_state = 2)\nlasso = Lasso(alpha = 0.01, max_iter = 100, random_state = 2)\nrfr = RandomForestRegressor(max_depth = 10, max_features = 0.7, random_state = 2)","4d906470":"test_ridge_exp, test_lasso_exp, test_rfr_exp = get_pred()","37bd6434":"#get_sub(test_ridge_exp, 'test_ridge_log.csv')\n#get_sub(test_lasso_exp, 'test_lasso_log.csv')\n#get_sub(test_rfr_exp, 'test_rfr_log.csv')","43cb26a8":"#top 10 coefficients for lasso\ncoeff_lasso = pd.DataFrame(lasso.coef_, X_train.columns, columns=['Coefficient'])\ncoeff_lasso = coeff_lasso.sort_values(by = 'Coefficient', ascending= False)\nlasso_list = coeff_lasso.index[0:10].tolist()","42cf2563":"#top 10 coefficients for ridge\ncoeff_ridge = pd.DataFrame(ridge.coef_, X_train.columns, columns=['Coefficient'])\ncoeff_ridge = coeff_ridge.sort_values(by = 'Coefficient', ascending= False)\nridge_list = coeff_ridge.index[:10].tolist()","c9da12c9":"#top 10 features for ridge\nfeat_imp = sorted(zip(map(lambda x: x, rfr.feature_importances_), X_train.columns), reverse=True)\nval,feat = zip(*feat_imp[:10])\nrfr_list = list(feat)","8de4840f":"#Combined the three lists of features to obtain a non-redundant list of features that have the largest effect on sale price.\n#Also examined which features were picked by how many regressors as informative. \nall_features = pd.DataFrame(lasso_list + ridge_list + rfr_list)\nfeat_count = all_features[0].value_counts()\nprint(feat_count)","34d6c999":"##Obtained a list the most informative features and subsetted all_data to only include these features\ninf_feat = feat_count.index.tolist()\ninf_feat.append('SalePrice')\nall_data_FS = all_data[inf_feat]","c9793a80":"X_train, X_test, y_train = data_prep(all_data_FS, transy = 'yes')","a707c5f3":"ridge = Ridge(random_state = 2)\nrfr = RandomForestRegressor(random_state = 2)\nlasso = Lasso(random_state = 2)","9338c9f8":"#get_best_params(100).to_csv('HT_FS.csv')","9685d1e5":"ridge = Ridge(alpha = 20, fit_intercept=True, max_iter = 100, random_state = 2)\nlasso = Lasso(alpha = 0.001, max_iter = 100, random_state = 2)\nrfr = RandomForestRegressor(max_depth = 10, max_features = 0.9, random_state = 2)","4fd7cc69":"test_ridge_exp, test_lasso_exp, test_rfr_exp = get_pred()","4357806c":"#get_sub(test_ridge_exp, 'test_ridge_fs.csv')\n#get_sub(test_lasso_exp, 'test_lasso_fs.csv')\n#get_sub(test_rfr_exp, 'test_rfr_fs.csv')","efb09bda":"all_data_FS['TotalPoints'] =  (all_data_FS.ExterQual + all_data_FS.KitchenQual + all_data_FS.BsmtQual    + all_data_FS.OverallCond ) + all_data_FS.OverallQual**2 ","c2928fde":"all_data_FS['Total_area'] = (all_data_FS.GrLivArea + all_data_FS['1stFlrSF'] + all_data_FS.LotArea + all_data_FS.TotalBsmtSF + all_data_FS.BsmtFinSF1)","81e97bb8":"Feat_eng = all_data_FS.drop(['ExterQual', 'KitchenQual', 'BsmtQual','OverallCond', 'OverallQual', 'GrLivArea', '1stFlrSF', 'LotArea', 'TotalBsmtSF', 'BsmtFinSF1'], axis = 1)","ca648359":"X_train, X_test, y_train = data_prep(Feat_eng, transy = 'yes')","7ba500ed":"ridge = Ridge(random_state = 2)\nrfr = RandomForestRegressor(random_state = 2)\nlasso = Lasso(random_state = 2)","0ce8fcad":"#get_best_params(100).to_csv('HT_FE.csv')","66254939":"ridge = Ridge(alpha = 10, fit_intercept=True, max_iter = 100, random_state = 2)\nlasso = Lasso(alpha = 0.001, max_iter = 100, random_state = 2)\nrfr = RandomForestRegressor(max_depth = 8, max_features = 0.7, random_state = 2)","8847d135":"test_ridge_exp, test_lasso_exp, test_rfr_exp = get_pred()","b90ff510":"#get_sub(test_ridge_exp, 'test_ridge_fe.csv')\n#get_sub(test_lasso_exp, 'test_lasso_fe.csv')\n#get_sub(test_rfr_exp, 'test_rfr_fe.csv')","d41e5db5":"X_train, X_test, y_train = data_prep(all_data, transy = 'yes')","336a2d97":"ridge = Ridge(alpha = 20, fit_intercept=True, max_iter = 100, random_state = 2)\nlasso = Lasso(alpha = 0.01, max_iter = 100, random_state = 2)\nrfr = RandomForestRegressor(max_depth = 10, max_features = 0.7, random_state = 2)","b37f8bf2":"test_ridge_exp, test_lasso_exp, test_rfr_exp = get_pred()","6454abff":"ens = pd.DataFrame(np.column_stack((test_ridge_exp, test_lasso_exp, test_rfr_exp)))\nens['SalePrice'] = ens.mean(axis=1)\nget_sub(np.asarray(ens['SalePrice']), 'test_ens_log_HT.csv')","b119dbed":"## Basement Nulls","8a66f35c":"### 6. One Hot Encoding (Getting Dummies) ","71e17d3d":"What did we discover?\nSo to start off with, we saw that using feature engineering after dropping many of the features led to a worse score (Go figure). Our higest score (0.12781) came about thanks to Hyperparameter tuning and logging the sale price column. \n\nAnd surprise surprise the best predictors for the price of a house are both the quality of the house and the size of the house. Who would have thought? ","21889e18":"Here we are just checking the top five variables that are correlated with Sale Price, no transformations here. ","1c07aa28":"### Best parameters, Best score (R^2), model","f6f2d910":"### Lot Frontage ","4b1e484b":"For a random forest regressor, important factors to consider is how many splits should the tree make before stopping. This will be determined by setting the max depth. Everytime a split is made by the regressor, needs to consider number of features when looking for the best split. Eg, 0.1 indicates that a random subset of 10% of the features will be available to consider for the best split. While log2 is the log2 of the number of features whole sqrt is the square root of the number of features.","f774358a":"### Sale Price ","9b65de30":"### Best parameters, Best score (R^2), model","36d042cb":"We wanted to see which non-one hot encoded columns also showed skewness","a7213003":"### Fence","609eefa4":"-More regressors","2a0dd027":"#### Handling the missing categorical data.","07adb116":"### Electrical","71f51ce6":"We obtained the highest score using this approach (0.12781), indicating that transforming the data, tuning the parameters and doing an ensemble of models was able to produce the best prediction. When feature selection is attempted again, these three steps will be used in conjuction to obtain the best model and score.","425597a9":"The score once again got worse, ridge: 0.18694, lasso: 0.18770, rfr: 0.26208. This may indicate that the feature engineering was not successful in reducing colinearity. Feature selection will need to be redone and optimised at a later stage.  ","32fbfa37":"-Engineer better features","955b8d4c":"## Lasso Regression ","b767f4c7":"## 8.1 Log transformation","c4044241":"## 2.1 Overview of Data ","e978e60d":"## Fixing strange results","ac31afb2":"### First exterior covering of the house ","67641e5d":"What to do with the missing values of Lot Frontage? Hmm let's check it out first. Keep in mind that it has a correlation of 0.35 with Sale Price and 15% of its data is missing. ","90be6666":"### Alley","4c0d8fdc":"With all our nulls sorted out, we need to get all the categorical variables into numerical form. If we neglect to do this (Leaving the categorical data in its raw form), some of the data will be hidden from our model and as a result reducing the accuracy of our model.  \n\nIn the 'all_data' dataset,we have the following categorical variables:\n\nLandSlope,ExterQual,HeatingQC,KitchenQual,FireplaceQu,GarageCond,PavedDrive,LotShape,BsmtQual,BsmtCond,GarageQual,PoolQC,\nBsmtExposure,BsmtFinType1,BsmtFinType2,CentralAir,GarageFinish,Functional,Street,Fence\n\nMost of the variables have a rating system (e.g. 5 rating system for the Basement Quality) from Poor(1) to Excellent(5).\nAlso, we filled nulls with zero if the variable doesn't apply on a certain house e.g. if a house does not have a basement the Basement Quality wil equal zero.\n\nOnce again, due to the good logical flow used in the kernel by Marcelo Marques to convert ordinal data to numerical data, we made use of his categorical to numeric conversion system. We made adjustments to his appraoch where we disagreed and replaced  it with our own version.","edc2eeb4":"# 11. References (Basically what and who helped us get this far)\n\n### EDA: \n[https:\/\/www.kaggle.com\/dgawlik\/house-prices-eda](http:\/\/) - **Dominik Gawlink**\n\nhttps:\/\/www.kaggle.com\/mgmarques\/houses-prices-complete-solution - **Marcelo Marques**\n### Data Cleaning:\n\nhttps:\/\/www.kaggle.com\/dgawlik\/house-prices-eda - **Dominik Gawlink**\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2015\/11\/easy-methods-deal-categorical-variables-predictive-modeling\/ - Categorical to Numerical\n\n### Regression: \n\nhttps:\/\/towardsdatascience.com\/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b - Ridge and Lasso \n\nhttps:\/\/www.statisticshowto.datasciencecentral.com\/ridge-regression\/ - Ridge Regression\n\nhttps:\/\/www.statisticshowto.datasciencecentral.com\/lasso-regression\/ - Lasso Regression\n\nhttps:\/\/www.sciencedirect.com\/topics\/biochemistry-genetics-and-molecular-biology\/random-forest - Random Forest\n\n\n### Other: \n","cd8d5eba":"\"{'max_features': 0.9, 'max_depth': 10}\",0.8602679672653393,rfr","0ccc38e6":"### Functional","edc377f5":"### 5. Mapping ordinals ","11af2d30":"That last year seems peculiar","b805b1e8":"### Fireplaces","89982877":"## Feature Selection ","1e84760c":"#### Remaining garage nulls","8fde61c4":"Group by GarageType and fill missing value with median where GarageType=='Detchd'","560800ec":"The main features that stuck out in our selection had to do with the square footage and quality of parts of the house. Perhaps creating new features with these features may be a manner in which the model is improved.","a09f6074":"This amazing function comes courtesy of Marcelo Marques (see references), the function summarises all the information we would like the see from our dataset. This includes information for each column or feature as it is called in machine learning such the datatype, the number of entries in a column, how many values are missing, the number of unique terms, the proportion of the missing data to the data present, the skewness of the data (how it deviates from the normal distribution and identify columns with outliers), the kurtosis (for identifying columns with many outliers) as well as the correlation of the feature with sales price if the user specifies this option.","38a305ee":"### 2. Preparations","75610f97":"While variables like basement quality make sense to be classified as poor,excellent, etc and thereby giving them the associated values e.g 1 for poor and 5 for excellent,some categorical variables like 'Exterior1st' need to be converted into dummy variables. This is to avoid thinking that a house with one type of exterior covering (Cinder Block) is better than one with another type (Brick Face). In other words, converting categorical data with no inherent order into a format the machine learning algorithm will understand. This entails creating a new column for each category related to a feature and encoding the data as 1 (presence of the feature) and 0 (lack of a feature).","9526d630":"Similar story with the Test data. ","85a34fe9":"The above code (dfdetails)  takes all the data in our training set and returns the kurtosis, skewness and the correlation of each attribute vs the dependent variable. \n\nWe used this code in order to determine the distribution of the data and the number of outliers(but due to time constraints we were not able to fix this problem).","4e7fba12":"### SaleType","9f511b2d":"There are 1460 instances of training data and 1459 of test data. Total number of variables equals 81 (80 for test), of which 36 are quantitative, 43 categorical + Id and SalePrice (not in test).Courtesy of Dominik Gawlink\n\nQuantitative: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold\n\nQualitative: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities,","f08f9a2c":"# 7. Regression (First run)","3ce623e3":"Combining both datasets allows us to fix problems with nulls at the same time, instead of having to do a repeat process on the other dataset. Also, because there are values in the categorical data that are in the train dataset, but not in the test data set, we might run into problems  with the model. Now that we have glimpsed our data, let's delve even deeper into our data and try cleaning it in the next section. ","aedf1205":"The following section will deal with what we did with the missing values. ","fcfe4544":"# 8. Optimization ","5d1e8f12":"\"{'max_features': 0.7, 'max_depth': 8}\",0.8130245204248053,rfr","6d93908d":"Much better. The graphs look like your matric dance outfit, well fit! From now on, we will be setting the transy condition in the data_prep function to 'yes'","e84c89e1":"\"{'max_features': 0.7, 'max_depth': 10}\",0.8648001754712819,rfr","75cff275":"Having seen that the majority of the nulls correspond to a lack of a basement or basement feature, we first wanted to fill the nulls for basements that are indeed present but some information is missing. Filled these categorical variables with the mode in these cases","59beebbf":"We imputed those missing values with median Frontage of the Neighborhood","d9f39ab9":"It seems that these houses do have a masonry veneer covering but the area was incorrectly recorded, so will replace the area of 0 with the median of the masonry veneer area","7cc071de":"Okay so we got our best score of 0.23. So everything should be hunky dory. However, we wanted to see if we could still improve on our score. That's where optimizing comes in","450ee00a":"LotFrontage is the feet of street connected to the property,so we assumed that houses in the same area(Neighborhood) have the same LotFrontage","fab9c145":"\"{'max_iter': 100, 'alpha': 0.01}\",0.8961561765139406,lasso","f8937624":"Group by GarageType and fill missing value with mode","e6b7c05a":"### 11. References (Basically what and who helped us get this far)","d8f2c916":"Following hyperparameter tuning the results improved for all models. The score were as follows: Ridge: 0.13390, LASSO: 0.13109 and RFR: 0.15472. The scores improved again, lets try improve the score even more with some feature selection.","cdfa201c":"## MSZoning nulls","c77ab371":"In this section we look at first the dependent variable and the variables that are highly correlated with it, then we check what to do with the missing values. \n\nAs mentioned earlier, Overall Quality has the highest correlation (0.79) with the dependent variable. Makes intuitive sense. The lower the quality of the house, the lower the sale price. Let's take a further look at some of the variables. ","cac64410":"# 5. Mapping ordinals ","00a5f02f":"# 10. What could have been done better?\n\nThe work on this kernel will continue, aspects that can be addressed in future iterations include:","3c3ec940":"### 1. Introduction","657c3730":"As shown by the plots above, we can see that the Sale Price is now normally distributed after log transformation. Therefore we will be using giving transy the condition yes. ","8cff2f73":"## Highly correlated variables","8e8cb716":"### 3. Exploring the Data (There's a word for this..)","a7d7e51d":"\"{'max_iter': 100, 'fit_intercept': True, 'alpha': 20}\",0.8798241988380094,ridge","9592d1d8":"Following log transformation of features that showed skewness, the results improved for both ridge and rfr, while no improvement was seen from LASSO. The score were as follows: Ridge: 0.13458, LASSO: 0.41901 and RFR: 0.15250. However, lasso didn't improve at all, perhaps changing the parameters will help.","b31fd325":"So when you one hot encode, you get a column of 1 and 0, but sometimes you find that the training set has 1 and 0 values\nwhile the test set only has 0, or vice versa. This will mess with the model, so the columns need to be removed and that is \nwhat the drop_zero parameter does. By default, it is set to yes, but if the user chooses, they can set it to no and those columns will be retained. You can check this for yourself by taking the test and train dataset and doing a value_counts on the \ncolumns that were listed below. \n\nThis function is adapted from code by Marcelo Marquez, he performs these two steps separately. We tried to re-code this but the found that the code already did what was required but to avoid having to run the code separately, combined the code into a function that allows the user to chose how to process categorical variables.","6549867e":"## Averaging the predictions","081b66ab":"## Ridge Regression","cb12471a":"-Look for collinearity","7201c741":"What does the following function do? It loops through the three regressors, and for each one it performs a randomised search using a random selection of the specified values for the parameters of interest for a user specified number of iterations. Once the iteractions have been completed, parameters that resulted in the model that produced the best R2 are put into a dataframe and provided to the user. \n","d3289cb1":"#### Handling the missing numerical data.","944ff0db":"### Importing the necesarry files\n\nMost of  the libraries we needed in order to get ourselves started. \n\nNumpy was used for data processing (helping us with those dang arrays) and pandas was used read data, amongst other things. \n\nPlyplot  and seaborn are imported for data visualisation and plotting. \n\nStandardScaler is used to transform the data such that its distribution will have a mean value 0 and standard deviation of 1 (I.e Normalization) \n\nRidge, Lasso and Random Forest are all regressions. (Will go into detail later) \n\nRandomizedSearchCV is used later for hyperparameter tuning at a fraction of the time compared to the brute force approach taken by GridSearchCV  \n","0e1fa5ab":"Since utilities indicates that the majority of houses has all public utilities (AllPub) and a single case has NoSeWa, we decided this feature is not helpful, so we are going to drop it. ","1157fb5a":"### Miscellaneous features","5f6f674b":"Now that we have cleaned our data as far as possible, now we will try to run our model using different regressions. For X we drop the Sale Price and for Y we keep only the Sale price. Since we know from the EDA we did above that SalePrice is positively skewed, we have added the option to log transform the dependent variable if needed ","6718d447":"### Best parameters, Best score (R^2), model","f7d17cd3":"# 3. Exploring the Data (There's a word for this..)","cd802226":"Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of muticollinearity or when you want to automate certain parts of model selection, like variable selection\/parameter elimination.The acronym \u201cLASSO\u201d stands for Least Absolute   Shrinkage and Selection Operator","392f6b77":"Fireplaces is equal to zero, but FireplaceQu null, so its safe to impute  with NA","8ea27056":"Random forest (Breiman, 2001) is an ensemble approach that can be used to perform both classification and regression tasks. The main principle behind ensemble methods is that a group of weak learners can be joined to form a strong learner. That is, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone","c82987e2":"### 4. Missing Values","aa6479bf":"## Hyperparameterisation for feature selection ","072e4465":"\"{'max_iter': 100, 'alpha': 0.001}\",0.7799027798773486,lasso","606b1b56":"All the remaing terms indicate lack of a garage, so filled the numeric features with 0 and the categorical with NA","21ac3a02":"### Loading the data ","c5a7e2b4":"\"{'max_iter': 100, 'alpha': 0.001}\",0.8798841901089682,lasso","81a0c84c":"The whole dataset was used to train these models on default settings. These The scores are as follows: Ridge: 0.23392, LASSO: 0.41901 and RFR: 0.23758. Ridge gives a good score, but it could be better. Need to look at fixing up data that is not normally distributed and has outliers. According to https:\/\/codeburst.io\/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa, the data is moderately skewed its skewness is between -1 and -0.5(negatively skewed) or between 0.5 and 1(positively skewed). The data is highly skewed if the skewness is less than -1(negatively skewed) or greater than 1(positively skewed). ","101dd747":"### PoolQC","23c00aff":"The main trend that sticks out is that the size of the house and the state that it is in has a strong effect on sales price. Additional features such as the number of cars the garage can fit as well as the neighbourhood the house is found in has an effect on sales prices.","42439304":"### Overall Quality (0.79)","875ffe57":"This is an intro to the work we had done on for the House Prices: Advanced Regression Techniques Challenge. Just to give a brif overview of what to expect in this notebook. We looked at the data we had, cleaned and analysed the data then ran multiple types of regression to end up with the score we had. Each section will go into much more depth of how we came about with our results. Hope you enjoy :) ","20a23036":"## Utilities Nulls","66af07bc":"### 7. Regression (First run)","31052c21":"##  Masonry Veneer Nulls","21c3492e":"# Internet of Kings Notebook","51b18834":"The score got worse: Ridge: 0.13538, Lasso: 0.13524, RFR: 0.23758. This drop in the score may be that some of the selected features may be displaying colinearity. Perhaps engineering some new features out of the existing features may help us improve the models and reduce the score.","c6be4d3d":"### Second exterior covering of the house ","7f2a79ad":"For ridge alpha, max iterations and fit_intercept were the parameters of interest. \nRegularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Therefore higher alphas lead to higher penalties and vice versa, so lower alphas result in the coefficients being closer to a standard linear regression. We also wanted to test whether calculating the intercept for this model improves the score, as the underlying assumption is that the data is expected to already be centered. This dataset is rather small, but we assume that we will use these models again for other datasets, and very large datasets will need many iteractions to get a good model, so optimising the number of iterations is vital.","6203cd8c":"-Backward selection","b36696f9":"The size of the house and the property on which it stands is a strong predictor of sales prices as many features related to size were represented in the list of important features. These were all put together to get the total area of the house. ","2b5ae221":" # 1. Introduction","ca35fc45":"For lasso when the alpha is 0, the regression produces the same coefficients as a linear regression. When alpha is very very large, all coefficients are zero. So tried to chose lower coefficients rather than large ones so not too many coefficients are set to zero. Similar to the situation above, we wanted to optimise the maximum number of interation that the regressor needs to go through to give the best model.","6cfe9a26":"-Remove outliers using kurtosis score","2644f454":"Now that we have looked at the data at a high-level, time to check if any of our data is missing. As you can see from the above table, the variable with the most missing values is Pool Quality (99% of the data is missing), followed by Misc. Features. Most of the data that is missing is as a result of some houses not having the feature present, for example a pool. Thus the data was not filled in. ","0c524093":"We know from the EDA we did that SalePrice is right skewed, so we decided to log transform the dependent variable to see if it will better fit a normal distribution that is favourable for linear regressors such as lasso and ridge since they assume the data is normally distributed. \n\nWe use the numpy fuction log1p which applies log(1+x) to all elements of the column. ","2eec56ce":"## Total Basement Area Square Footage (0.61)","4db76730":"\"{'max_iter': 100, 'fit_intercept': True, 'alpha': 20}\",0.8784364996900141,ridge","76c06cf6":"We found the null processing that Marcelo Marques performed was very logical, so adapt his strategy in this section. We cleaned nulls by going from the features with the most nulls to the least.","25548d90":"Ridge regression is a way to create a model when the number of predictor variables in a set exceeds the number of \nobservations, or when a data set has multicollinearity (correlations between predictor variables)","30bbc88b":"# 4. Missing Values","50e98ee8":"## Feature engineering","2fee8891":"\"{'max_iter': 100, 'fit_intercept': True, 'alpha': 10}\",0.7796066098631426,ridge","ff6a08c5":"As mentioned before, Overall Quality of a house is the most correlated variable with Sale Price. The box plot above shows the quartile range, median, lowest and highest value and any outliers. Next we will look at the Garage variables.   ","6e49aa93":"First we check if the Sale Price is normally distributed or not as we assume the dataset is normally distributed. The following function (obtained from Marcelo, link is in the references section) shows the outliers, skewness and kurtosis. ","6d725c5a":"Chosing the correct features can help improve the models performance. All three models can provide valuable information that will allow us to select the best features. When calling coefficients for ridge and lasso, and subsequently sorting by size, we can see that coefficients that are penalised the least. Random forests regressor has a feature importances attribute, and the higher the score, the more important the attribute, so sorting by size will also produce features that contribute to the sales price. Taking the top 10 features for each model would provide us with a non-redunant list of features that these models highlight as contributing highly to estimating the sales price. By keeping the features on this list, we believe the model will improve as well as our score.","d6cc598c":"### 9. Conclusion ","e097092d":"Now that our data is ready, the question now is what models do we use? For this project we picked Ridge, Lasso and Random Forest. But what do they do and what is the difference? Well let's look at each one individually. ","71367e28":"# Hyperparameter tuning","3181c128":"From the table above, one can see that the Overall Quality of a house is the highest correlated variable to Sale Price. We also see that the sub-sections of Garage are correlated highly with Sale Price, which intuitively makes sense. The more garage space or garage area available, one expects the house to be more pricey. ","347765bd":"With all our data cleaned up,  we can finally start running some regression models","4f6e8626":"## Table of contents:","a0ba0aa5":"# 2. Preparations","96fe32cf":"Are there any houses that have fireplaces but the quality is zero?","f84d5cba":"### Kitchen Quality","1e970f53":"## Garage Area (0.62)","98e34a58":"### 8. Optimization","047654a1":"The size of the dataset changed, so will need to retune the parameters to have an optimised model.","f5653b40":"# 9. Conclusion ","6017512c":"### 10. What could have been done better?","584356ca":"If the pool area is 0, then that must mean that the house does not have a pool, so pool QC will be NA.","fbf04c8d":"## Garage Cars (0.64)\n","8ded39e0":"### Garage Nulls","7a0ea8df":"https:\/\/www.investopedia.com\/terms\/k\/kurtosis.asp - Simple explanation of kurtosis","50cf0617":"Now to have a quick check if the data makes sense in this category","3502f620":"Running models on default settings will only get you that far, changing the parameters that the regressor uses uses can improve its performance.\n\nChosing the parameters manually is possible, but you may need to run through many iterations to find the best model. So automating a process where random combinations of parameters are assigned to the regressor several times and then obtaining the paramters that produced the best result is desirable. \n\nThis section focusses on tuning the parameters using RandomizedSearchCV to find the best parameters for our three regressors. First we call our regressors again with default settings.","d59ccb1b":"The main columns that showed a higher than 0.5 skewness were related to square footage of various parts of the house. These were log transformed to better fit a normal distribution","686fa47e":"As shown by the above graphs, the distribution of Sale price is positively skewed (The tail is to the right of the bump so to speak). Meaning if we were to run a normal linear regression our output would be incorrect. I.E Our predictions would'nt be that great. \n\nThe probability plot shows that the distribution is not normally distributed. The plotted points bend up and to the left of the normal line, indicating that the tail is to the right.","662f2fd1":"## Random Forest","f8f42462":"The best score we received was for all the data that has been log transformed and parameters optimised. \nAn average of all three predictions was taken as each model was detecting associations between the features and the sales price but errors in the model may be limiting how good the model could be. By averaging the predictions we hope to average the errors to 0 and keep only the important insight that the model detected. This may improve the prediction as we assume that the errors between the three models are poorly correlated, a condition required for the errors to average out. ","a5b7bca8":"After these two new features were engineered, the individual features were dropped","b5746ade":"Interesting to see the result here. ","59692c15":"Since overall quality takes the state the whole house is in, it was used as the backbone of this feature. Every other quality and condition metrics were added to it to get the total points that the house receives.  ","a96e2f9d":"# 6. One Hot Encoding (Getting Dummies) ","f97956f7":"Since the size of the dataset has changed again, this mean some more parameter tuning. ","c204f9c1":"### Ground Living Area (0.71)"}}