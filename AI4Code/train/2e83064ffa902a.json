{"cell_type":{"757d685d":"code","b5ea5b9b":"code","e59ea43f":"code","4e419839":"code","5f88688f":"code","4eb3c949":"code","00541986":"code","ff72b9a8":"code","2a097db8":"code","f82ff519":"code","fe6e5b5c":"markdown","b590053d":"markdown","2aa003b7":"markdown","2120084a":"markdown","5cc2b69b":"markdown","bcd49870":"markdown"},"source":{"757d685d":"# install gluonts package\n!pip install gluonts\n!pip install sentence_transformers","b5ea5b9b":"# load and clean data\nimport pandas as pd\nimport numpy as np\ntrain_all = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-1\/train.csv\")\n\ndef preprocess(\n    frame: pd.DataFrame,\n    log_transform: bool = False\n):\n    \n    # set index\n    new_frame = frame.set_index('Date')\n\n    # fill 'NaN' Province\/State values with Country\/Region values\n    new_frame['Province\/State'] = new_frame['Province\/State'].fillna(new_frame['Country\/Region'])\n    \n    # convert target values to log scale\n    if log_transform:\n        new_frame[['ConfirmedCases', 'Fatalities']] = np.log1p(\n            new_frame[['ConfirmedCases', 'Fatalities']].values\n    )\n    \n    return new_frame\n\ndef split(\n    df: pd.DataFrame, \n    date: str = '2020-03-12'\n):\n    train = df.loc[train_all.index < date] \n    test = df.loc[train_all.index >= date]\n    return train, test\n\ntrain_all = preprocess(train_all)\ntrain, test = split(train_all)","e59ea43f":"# plot confirmed cases and fatalities in train\nimport matplotlib.pyplot as plt\nfrom gluonts.dataset.util import to_pandas\nfrom gluonts.dataset.common import ListDataset\n\ncum_train = train.groupby('Date').sum()\ncum_test = test.groupby('Date').sum()\n\ndef plot_observations(\n    target: str = 'ConfirmedCases'\n):\n    fig = plt.figure(figsize=(15, 6.1), facecolor=\"white\",  edgecolor='k')\n\n    train_ds = ListDataset(\n        [{\"start\": cum_train.index[0], \"target\": cum_train[target].values}],\n        freq = \"D\",\n    )\n    test_ds = ListDataset(\n        [{\"start\": cum_test.index[0], \"target\": cum_test[target].values}],\n        freq = \"D\",\n    )\n    \n    for tr, te in zip(train_ds, test_ds):\n        tr = to_pandas(tr)\n        te = to_pandas(te)\n        tr.plot(linewidth=2, label = f'train {target}')\n        tr[-1:].append(te).plot(linewidth=2, label = f'test {target}')\n    \n    plt.axvline(cum_train.index[-1], color='purple') # end of train dataset\n    plt.title(f'Cumulative global number of {target}', fontsize=16)\n    plt.legend(fontsize=16)\n    plt.grid(which=\"both\")\n    plt.show()\n    \nplot_observations('ConfirmedCases')\nplot_observations('Fatalities')","4e419839":"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')\ncountry_embeddings = model.encode(list(train['Country\/Region'].unique()))\nembedding_dim = len(country_embeddings[0])\nembed_df = pd.DataFrame(np.concatenate([np.array(list(train['Country\/Region'].unique())).reshape(-1,1),country_embeddings],axis=1))\nembed_df.columns=['Country\/Region']+list(range(embedding_dim))","5f88688f":"from sklearn.preprocessing import OrdinalEncoder\n\n#pop_df = pd.read_csv('..\/input\/population-by-country-2020\/population_by_country_2020.csv')\n\ncountry_map = {\n    \"Congo (Brazzaville)\": \"Congo\",\n    \"Congo (Kinshasa)\": \"Congo\",\n    \"Cote d'Ivoire\": \"C\u00f4te d'Ivoire\",\n    \"Czechia\": \"Czech Republic (Czechia)\",\n    \"Gambia, The\": \"Gambia\",\n    \"Guernsey\": \"Channel Islands\",\n    \"Jersey\": \"Channel Islands\",\n    \"Korea, South\": \"South Korea\",\n    \"Republic of the Congo\": \"DR Congo\",\n    \"Reunion\": \"R\u00e9union\",\n    \"Saint Vincent and the Grenadines\": \"St. Vincent & Grenadines\",\n    \"Taiwan*\": \"Taiwan\",\n    \"The Bahamas\": \"Bahamas\",\n    \"The Gambia\": \"Gambia\",\n    \"US\": \"United States\"\n}\n\ndef clean_pop_data(\n    pop_df: pd.DataFrame\n):\n    \"\"\" bootstrapped from https:\/\/www.kaggle.com\/saga21\/covid-global-forecast-sir-model-ml-regressions\"\"\"\n        \n    # remove the % character from Urban Pop, World Share, and Yearly Change values \n    pop_df['Yearly Change'] = pop_df['Yearly Change'].str.rstrip('%')\n    pop_df['Urban Pop %'] = pop_df['Urban Pop %'].str.rstrip('%')\n    pop_df['World Share'] = pop_df['World Share'].str.rstrip('%')\n    \n    # Replace Urban Pop, Fertility Rate, and Med Age \"N.A\" by their respective modes\n    pop_df.loc[pop_df['Urban Pop %']=='N.A.', 'Urban Pop %'] = pop_df.loc[pop_df['Urban Pop %']!='N.A.', 'Urban Pop %'].mode()[0]\n    pop_df.loc[pop_df['Med. Age']=='N.A.', 'Med. Age'] = pop_df.loc[pop_df['Med. Age']!='N.A.', 'Med. Age'].mode()[0]    \n    pop_df.loc[pop_df['Fert. Rate']=='N.A.', 'Fert. Rate'] = pop_df.loc[pop_df['Fert. Rate']!='N.A.', 'Fert. Rate'].mode()[0]    \n    \n    # replace empty migration rows with 0s\n    pop_df['Migrants (net)'] =  pop_df['Migrants (net)'].fillna(0)\n    \n    # cast to types\n    pop_df = pop_df.astype({\n        \"Population (2020)\": int,\n        \"Yearly Change\": float,\n        \"Net Change\": int,\n        \"Density (P\/Km\u00b2)\": int,\n        \"Land Area (Km\u00b2)\": int,\n        \"Migrants (net)\": float,\n        \"Fert. Rate\": float,\n        \"Med. Age\": int,\n        \"Urban Pop %\": int,\n        \"World Share\": float,\n    })   \n\n    return pop_df\n\ndef join(\n    df: pd.DataFrame,\n    pop_df: pd.DataFrame\n):\n    \n    # add join column with country mapping\n    df['join_col'] = [\n        val if val not in country_map.keys() \n        else country_map[val] \n        for val in df['Country\/Region'].values\n    ]\n    \n    # join, delete merge columns\n    new_df = df.reset_index().merge(\n        pop_df,\n        left_on = 'join_col',\n        right_on = 'Country (or dependency)',\n        how = 'left'\n    ).set_index('Date')\n    new_df = new_df.drop(columns=['join_col', 'Country (or dependency)'])\n    \n    # replace columns that weren't matched in join with mean\n    new_df = new_df.fillna(new_df.mean())\n    \n    return new_df\n\ndef join_with_embeddings(\n    df: pd.DataFrame,\n    embed_df: pd.DataFrame\n):\n    \n    # join, delete merge columns\n    new_df = df.reset_index().merge(\n        embed_df,\n        left_on = 'Country\/Region',\n        right_on = 'Country\/Region',\n        how = 'left'\n    ).set_index('Date')\n    \n    # make sure no NaN in dataframe\n    assert new_df.isnull().sum().sum()==0\n    return new_df\n\ndef encode(\n    df: pd.DataFrame\n):\n    \"\"\" encode 'Province\/State' and 'Country\/Region' categorical variables as numerical ordinals\"\"\"\n    \n    enc = OrdinalEncoder()\n    df[['Province\/State', 'Country\/Region']] = enc.fit_transform(\n        df[['Province\/State', 'Country\/Region']].values\n    )\n    return df, enc\n\n# pop_df = clean_pop_data(pop_df)\njoin_df = join_with_embeddings(train_all, embed_df)\n\nall_df, enc = encode(join_df)\ntrain_df, test_df = split(all_df)\n_, val_df = split(all_df, date = '2020-02-28')\n","4eb3c949":"from gluonts.dataset.common import ListDataset\nfrom gluonts.dataset.field_names import FieldName\nimport typing\n\nREAL_VARS = [\n    'Population (2020)', \n    'Yearly Change', \n    'Net Change', \n    'Density (P\/Km\u00b2)', \n    'Land Area (Km\u00b2)',\n    'Migrants (net)',\n    'Fert. Rate',\n    'Med. Age',\n    'Urban Pop %',\n    'World Share'\n]\n\nEMBED_VARS = list(range(embedding_dim))\n\ndef build_dataset(\n    frame: pd.DataFrame,\n    target: str = 'Fatalities',\n    cat_vars: typing.List[str] = ['Province\/State', 'Country\/Region'],\n    real_vars: typing.List[str] = EMBED_VARS\n):\n    return ListDataset(\n        [\n            {\n                FieldName.START: df.index[0], \n                FieldName.TARGET: df[target].values,\n                FieldName.FEAT_STATIC_CAT: df[cat_vars].values[0],\n                FieldName.FEAT_STATIC_REAL: df[real_vars].values[0]\n            }\n            for g, df in frame.groupby(by=['Province\/State', 'Country\/Region'])\n        ],\n        freq = \"D\",\n    )\n\ntraining_data_fatalities = build_dataset(train_df)\ntraining_data_cases = build_dataset(train_df, target = 'ConfirmedCases')\ntraining_data_fatalities_all = build_dataset(all_df)\ntraining_data_cases_all = build_dataset(all_df, target = 'ConfirmedCases')\nval_data_fatalities = build_dataset(val_df)\nval_data_cases = build_dataset(val_df, target = 'ConfirmedCases')","00541986":"from gluonts.model.deepar import DeepAREstimator\nfrom gluonts.trainer import Trainer\nfrom gluonts.distribution import NegativeBinomialOutput\nimport mxnet as mx\nimport numpy as np\n\n# set random seeds for reproducibility\nmx.random.seed(42)\nnp.random.seed(42)\n\ndef fit(\n    training_data: ListDataset,\n    validation_data: ListDataset = None,\n    use_real_vars: bool = False,\n    pred_length: int = 14,\n    epochs: int = 10,\n    weight_decay: float = 5e-5,\n):\n    estimator = DeepAREstimator(\n        freq=\"D\", \n        prediction_length=pred_length,\n        use_feat_static_real = use_real_vars,\n        use_feat_static_cat = True,\n        cardinality = [train['Province\/State'].nunique(), train['Country\/Region'].nunique()],\n        distr_output=NegativeBinomialOutput(),\n        trainer=Trainer(\n            epochs=epochs,\n            learning_rate=0.005, \n            batch_size=64,\n            weight_decay=weight_decay,\n            learning_rate_decay_factor=0.1,\n            patience=15,\n        ),\n    )\n    predictor = estimator.train(training_data = training_data, validation_data = validation_data)\n    \n    return predictor\n\npredictor_fatalities = fit(training_data_fatalities)\npredictor_cases = fit(training_data_cases, epochs=20)\npredictor_fatalities_all = fit(training_data_fatalities_all, pred_length = 30, epochs=20)\npredictor_cases_all = fit(training_data_cases_all, pred_length = 30, epochs=40)","ff72b9a8":"from gluonts.dataset.util import to_pandas\nimport matplotlib.pyplot as plt\nfrom typing import List\n\ndef plot_forecast(\n    predictor,\n    location: List[str] = ['Italy', 'Italy'],\n    target: str = 'Fatalities',\n    cat_vars: typing.List[str] = ['Province\/State', 'Country\/Region'],\n    real_vars: typing.List[str] = EMBED_VARS,\n    log_preds: bool = False,\n    fontsize: int = 16\n):\n    fig = plt.figure(figsize=(15, 6.1), facecolor=\"white\",  edgecolor='k')\n\n    # plot train observations, true observations from public test set, and forecasts\n    location_tr = enc.transform(np.array(location).reshape(1,-1))\n    tr_df = train_df[np.all((train_df[['Province\/State', 'Country\/Region']].values == location_tr), axis=1)]\n    train_obs = ListDataset(\n        [{\n            FieldName.START: tr_df.index[0], \n            FieldName.TARGET: tr_df[target].values,\n            FieldName.FEAT_STATIC_CAT: tr_df[cat_vars].values[0],\n            FieldName.FEAT_STATIC_REAL: tr_df[real_vars].values[0]\n        }],\n        freq = \"D\",\n    )\n    te_df = test_df[np.all((test_df[['Province\/State', 'Country\/Region']].values == location_tr), axis=1)]\n    test_gt = ListDataset(\n        [{\"start\": te_df.index[0], \"target\": te_df[target].values}],\n        freq = \"D\",\n    )\n    for train_series, gt, forecast in zip(train_obs, test_gt, predictor.predict(train_obs)):\n        \n        train_series = to_pandas(train_series)\n        gt = to_pandas(gt)\n        \n        if log_preds:\n            train_series = np.expm1(train_series)\n            gt = np.expm1(gt)\n            forecast.samples = np.expm1(forecast.samples)\n        \n        train_series.plot(linewidth=2, label = 'train series')\n        gt.plot(linewidth=2, label = 'test ground truth')\n        forecast.plot(color='g', prediction_intervals=[50.0, 90.0])\n        \n    plt.title(f'Cumulative number of {target} in {location[0]}', fontsize=fontsize)\n    plt.legend(fontsize = fontsize)\n    plt.grid(which='both')\n    plt.show()\n    \nplot_forecast(predictor_fatalities, ['Italy', 'Italy'])\nplot_forecast(predictor_fatalities, ['California', 'US'])\nplot_forecast(predictor_fatalities, ['Korea, South', 'Korea, South'])\nplot_forecast(predictor_fatalities, ['Hubei', 'China'])\n\n","2a097db8":"from gluonts.evaluation.backtest import make_evaluation_predictions\nfrom gluonts.evaluation import Evaluator\nimport json\n\nall_data = build_dataset(all_df)\n\n# evaluate fatalities predictor\nforecast_iterable, ts_iterable = make_evaluation_predictions(\n    dataset=all_data,\n    predictor=predictor_fatalities,\n    num_samples=100\n)\nevaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\nagg_metrics, item_metrics = evaluator(ts_iterable, forecast_iterable, num_series=len(all_data))\nprint('Fatalities Predictor Metrics: ')\nprint(json.dumps(agg_metrics, indent=4))\n\n# evaluate confirmed cases predictor\nforecast_iterable, ts_iterable = make_evaluation_predictions(\n    dataset=all_data,\n    predictor=predictor_cases,\n    num_samples=100\n)\nagg_metrics, item_metrics = evaluator(ts_iterable, forecast_iterable, num_series=len(all_data))\nprint('Confirmed Cases Predictor Metrics: ')\nprint(json.dumps(agg_metrics, indent=4))","f82ff519":"# generate submission csv\n\n# aggregate fatalities\nfatalities = []\nfor public_forecast, private_forecast in zip(\n    predictor_fatalities.predict(training_data_fatalities),\n    predictor_fatalities_all.predict(training_data_fatalities_all)\n):\n    # offset by 1 because last training date is March 24, want to start predicting at March 26\n    fatalities.append(np.concatenate((public_forecast.median, private_forecast.median[1:])))\n\n# aggregate cases\ncases = []\nfor public_forecast, private_forecast in zip(\n    predictor_cases.predict(training_data_cases),\n    predictor_cases_all.predict(training_data_cases_all)\n):\n    # offset by 1 because last training date is March 24, want to start predicting at March 26\n    cases.append(np.concatenate((public_forecast.median, private_forecast.median[1:])))\n\n# load test csv \nsub_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-1\/test.csv\")\n\n# fill 'NaN' Province\/State values with Country\/Region values\nsub_df['Province\/State'] = sub_df['Province\/State'].fillna(sub_df['Country\/Region'])\n\n# get forecast ids\nids = []\nfor _, df in sub_df.groupby(by=['Province\/State', 'Country\/Region']):\n    ids.append(df['ForecastId'].values)\n\n# create submission df\nsubmission = pd.DataFrame(\n    list(zip(\n        np.array(ids).flatten(),\n        np.array(cases).flatten(),\n        np.array(fatalities).flatten()\n    )), \n    columns = ['ForecastId', 'ConfirmedCases', 'Fatalities']\n)\nsubmission.to_csv('submission.csv', index=False)","fe6e5b5c":"# Plot predictions from fit model parameters","b590053d":"# Fit DeepAR Model Estimates\n\nThe DeepAR model was proposed by David Salinas, Valentin Flunkert, and Jan Gasthaus in \"DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks\" (https:\/\/arxiv.org\/abs\/1704.04110). The approach trains an autoregressive RNN to produces time-variant parameters of a specified distribution on a large collection of related time series. The learned distribution can then be used to produce probabilistic forecasts. Here we use the authors' *GluonTS* implementation (https:\/\/gluon-ts.mxnet.io\/index.html).\n\nWe believe the probabilistic nature of the DeepAR forecasts is a feature that differentiates our approach from others we have seen so far. Specifically, the ability to provide both confidence intervals and point estimates allows one to better understand the range of possible trajectories, from the worst-case scenario, to the best-case scenario, to the expected scenario. \n\n*TODO:*\n> 1. Experiment with adding learned Box Cox transformation before learning Negative Binomial paramaters?","2aa003b7":"# Data Preprocessing\n\n>1. fill 'NaN' Province\/State values with Country\/Region values\n>2. (optionally) apply log transformation to target values","2120084a":"# Data Exploration","5cc2b69b":"# Data Augmentation\n\n>1. Categorical feature for 'Province\/State'\n2. Categorical feature for 'Country\/Region'\n3. (Attempted, not used) Augment with 2020 country level population data from https:\/\/www.kaggle.com\/tanuprabhu\/population-by-country-2020.  This approach was inspired by this previous submission to the competition: https:\/\/www.kaggle.com\/saga21\/covid-global-forecast-sir-model-ml-regressions:\n  1. Population\n  2. % Population Increase 2019-2020\n  3. Net Population Increase 2019-2020\n  4. Population Density (P\/Km^2)\n  5. Land Area (Km^2)\n  6. Net Migration\n  7. Fertility Rate\n  8. Median Age\n  9. % Urban Population\n  10. % of World Population\n4. (Attempted, not used) Use NLP Embeddings learned from BERT with the package [sentence-transformers](https:\/\/github.com\/UKPLab\/sentence-transformers), to act as data augmentation\n  \n**Current Status**: To our surprise, we actually found that the forecasts produced by the DeepAR model were worse with these country-level statistics as additional features. Adding these covariates seemed to bias all forecasts toward 0. We  hypothesize that this is because the distribution of cases\/fatalities by province\/state for a single country (which has a one-to-many mapping to provinces\/states) is right skewed. Thus the model is rewarded for learning the low volume series and the learned parameters and resulting forecasts reflect this. Thus, in the forecasts we have submitted, these country-level statistics have not been included as additional features (see `use_real_vars` argument in `fit()` function defined in this notebook). \n\n\n*TODO*:\n>1. Experiment with other covariates (state level population data, healthcare infrastructure dataset)","bcd49870":"# Calculate metrics on public test"}}