{"cell_type":{"066430d8":"code","3781efff":"code","b857ec23":"code","e425a360":"code","8305c387":"code","2a9c5ee6":"code","fe085fd4":"code","a88b13a3":"code","7d7a9205":"code","8bb6ad39":"code","8b6b4ee3":"code","68846b32":"code","3e4e82d7":"code","d6515f2e":"markdown","4b6ee1ec":"markdown","93a5286c":"markdown"},"source":{"066430d8":"# import the necessary libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# machine learning\nfrom sklearn.preprocessing import StandardScaler\n\nimport sklearn.linear_model as skl_lm\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score\nfrom sklearn.model_selection import train_test_split\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# initialize some package settings\nsns.set(style=\"whitegrid\", color_codes=True, font_scale=1.3)\n\n%matplotlib inline","3781efff":"# read in the data and check the first 5 rows\ndf = pd.read_csv('..\/input\/data.csv', index_col=0)\ndf.head()","b857ec23":"# remove the 'Unnamed: 32' column\ndf = df.drop('Unnamed: 32', axis=1)","e425a360":"# Convert the 'target' variable to numeric\ndf['target'] = df['diagnosis'].apply(lambda x : 1 if x == 'M' else 0)  # Convert to numeric\ndf = df.drop('diagnosis',axis=1)","8305c387":"# import packages\nimport pandas as pd\nimport numpy as np\nimport pandas.core.algorithms as algos\nfrom pandas import Series\nimport scipy.stats.stats as stats\nimport re\nimport traceback\nimport string\n\nmax_bin = 20\nforce_bin = 3\n\n# define a binning function\ndef mono_bin(Y, X, n = max_bin):\n    \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]\n    r = 0\n    while np.abs(r) < 1:\n        try:\n            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n            d2 = d1.groupby('Bucket', as_index=True)\n            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n            n = n - 1 \n        except Exception as e:\n            n = n - 1\n\n    if len(d2) == 1:\n        n = force_bin         \n        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n        if len(np.unique(bins)) == 2:\n            bins = np.insert(bins, 0, 1)\n            bins[1] = bins[1]-(bins[1]\/2)\n        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n        d2 = d1.groupby('Bucket', as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"MIN_VALUE\"] = d2.min().X\n    d3[\"MAX_VALUE\"] = d2.max().X\n    d3[\"COUNT\"] = d2.count().Y\n    d3[\"EVENT\"] = d2.sum().Y\n    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n    d3=d3.reset_index(drop=True)\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT\/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT\/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT\/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT\/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    \n    return(d3)\n\ndef char_bin(Y, X):\n        \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]    \n    df2 = notmiss.groupby('X',as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"COUNT\"] = df2.count().Y\n    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n    d3[\"EVENT\"] = df2.sum().Y\n    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT\/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT\/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT\/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT\/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    d3 = d3.reset_index(drop=True)\n    \n    return(d3)\n\ndef data_vars(df1, target):\n    \n    stack = traceback.extract_stack()\n    filename, lineno, function_name, code = stack[-2]\n    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n    \n    x = df1.dtypes.index\n    count = -1\n    \n    for i in x:\n        if i.upper() not in (final.upper()):\n            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n                conv = mono_bin(target, df1[i])\n                conv[\"VAR_NAME\"] = i\n                count = count + 1\n            else:\n                conv = char_bin(target, df1[i])\n                conv[\"VAR_NAME\"] = i            \n                count = count + 1\n                \n            if count == 0:\n                iv_df = conv\n            else:\n                iv_df = iv_df.append(conv,ignore_index=True)\n    \n    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n    iv = iv.reset_index()\n    return(iv_df,iv) ","2a9c5ee6":"# Weight of Evidence encode the data\nfinal_iv, IV = data_vars(df, df.target)","fe085fd4":"# Take a look at the variable names and their respective Information Value\nIV.sort_values('IV')","a88b13a3":"transform_vars_list = df.columns.difference(['target'])\ntransform_prefix = '' # leave this value blank if you need to replace the original column values","7d7a9205":"for var in transform_vars_list:\n    small_df = final_iv[final_iv['VAR_NAME'] == var]\n    transform_dict = dict(zip(small_df.MAX_VALUE,small_df.WOE))\n    replace_cmd = ''\n    replace_cmd1 = ''\n    for i in sorted(transform_dict.items()):\n        replace_cmd = replace_cmd + str(i[1]) + str(' if x <= ') + str(i[0]) + ' else '\n        replace_cmd1 = replace_cmd1 + str(i[1]) + str(' if x == \"') + str(i[0]) + '\" else '\n    replace_cmd = replace_cmd + '0'\n    replace_cmd1 = replace_cmd1 + '0'\n    if replace_cmd != '0':\n        try:\n            df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd))\n        except:\n            df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd1))","8bb6ad39":"# Split the data into training and testing sets\nX = df\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=40)","8b6b4ee3":"import statsmodels as sm\nimport pandas as pd\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\n\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, class_weight='balanced')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log","68846b32":"y_pred = model_log.predict(X_test)","3e4e82d7":"from sklearn.metrics import classification_report, confusion_matrix , accuracy_score\nprint(confusion_matrix(y_test,y_pred))  \nprint(classification_report(y_test,y_pred)) \nprint(\"The accuracy score is\" + \" \"+ str(accuracy_score(y_test, y_pred)))","d6515f2e":"The weight of evidence encoding below is drawn from [https:\/\/github.com\/Sundar0989\/WOE-and-IV\/blob\/master\/WOE_IV.ipynb].","4b6ee1ec":"The goal of this notebook is to apply weight of evidence encoding to a data set and run a logistic regression model. This combination, which for all intents and purposes amounts to a risk score card, often results in a solid baseline model for classification problems without too much effort.\n\nRead [https:\/\/medium.com\/@sundarstyles89\/weight-of-evidence-and-information-value-using-python-6f05072e83eb] if you'd like to learn more.","93a5286c":"## Weight of Evidence Encoding & Logistic Regression"}}