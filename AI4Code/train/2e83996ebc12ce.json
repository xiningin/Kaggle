{"cell_type":{"2aab1e71":"code","bca781ca":"code","daf00a0a":"code","49a71963":"code","32a7f05c":"code","8f91d840":"code","6e857442":"code","330f5493":"code","cfeba17f":"code","bbcafe6e":"code","c531a7a8":"code","c9eaae1b":"code","1c333db3":"code","646fe329":"code","bb7847ae":"code","b3141209":"code","254c394e":"code","8035c788":"code","77eecdc0":"code","57bdef37":"code","0e4d1cb7":"code","524dd3cd":"code","a8206a9e":"code","2abbc7a7":"code","24d558df":"code","a9eab4a4":"code","f424bd8d":"code","43849174":"code","d5e38665":"code","4db4f39d":"code","4f8f9f59":"code","33d8c707":"code","ca5417ed":"code","c1dfd10a":"code","a2315735":"code","ad35a596":"markdown","6d187253":"markdown","12fae8e5":"markdown","7aa4da74":"markdown","43e47ff2":"markdown","f4c8e5f8":"markdown","14b9a4dd":"markdown","baedbb7a":"markdown","7e1260d7":"markdown","712cdb82":"markdown","4b9cf943":"markdown","c28c94cc":"markdown","138c8a8e":"markdown","e21b8236":"markdown","f5a97951":"markdown"},"source":{"2aab1e71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime\nfrom pathlib import Path\n\nfrom sklearn.metrics import roc_auc_score\n\nimport numpy as np\nimport os\nimport cv2\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# IMPORT KERAS LIBRARY\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, GlobalAveragePooling2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend as K\nimport tensorflow as tf\n\n%load_ext autoreload\n%autoreload","bca781ca":"model_path='.'\npath='..\/input\/chexpert\/chexp'\ntrain_folder=f'{path}train'\ntest_folder=f'{path}test'\ntrain_lbl=f'{path}train_labels.csv'","daf00a0a":"chestxrays_root = Path(path)\ndata_path = chestxrays_root","49a71963":"!ls '..\/input'","32a7f05c":"full_train_df = pd.read_csv(data_path\/'CheXpert-v1.0-small\/train.csv')\nfull_valid_df = pd.read_csv(data_path\/'CheXpert-v1.0-small\/valid.csv')","8f91d840":"chexnet_targets = ['No Finding',\n       'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n       'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n       'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n       'Support Devices']\n\nchexpert_targets = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']","6e857442":"full_train_df.head()","330f5493":"u_one_features = ['Atelectasis', 'Edema']\nu_zero_features = ['Cardiomegaly', 'Consolidation', 'Pleural Effusion']","cfeba17f":"def feature_string(row):\n    feature_list = []\n    for feature in u_one_features:\n        if row[feature] in [-1,1]:\n            feature_list.append(feature)\n            \n    for feature in u_zero_features:\n        if row[feature] == 1:\n            feature_list.append(feature)\n            \n    return ';'.join(feature_list)\n            \n     ","bbcafe6e":"full_train_df['train_valid'] = False\nfull_valid_df['train_valid'] = True","c531a7a8":"full_train_df['patient'] = full_train_df.Path.str.split('\/',3,True)[2]\nfull_train_df  ['study'] = full_train_df.Path.str.split('\/',4,True)[3]\n\nfull_valid_df['patient'] = full_valid_df.Path.str.split('\/',3,True)[2]\nfull_valid_df  ['study'] = full_valid_df.Path.str.split('\/',4,True)[3]","c9eaae1b":"full_df = pd.concat([full_train_df, full_valid_df])\nfull_df.head()","1c333db3":"full_df['feature_string'] = full_df.apply(feature_string,axis = 1).fillna('')\nfull_df['feature_string'] =full_df['feature_string'] .apply(lambda x:x.split(\";\"))\nfull_df.head()","646fe329":"#get the first 5 whale images\npaths =  full_df.Path[:5]\nlabels = full_df.feature_string[:5]\n\nfig, m_axs = plt.subplots(1, len(labels), figsize = (20, 10))\n#show the images and label them\nfor ii, c_ax in enumerate(m_axs):\n    c_ax.imshow(cv2.imread(os.path.join(data_path,paths[ii])))\n    c_ax.set_title(labels[ii])","bb7847ae":"from collections import Counter\n\nlabels_count = Counter(label for chexpert_targets in full_df['feature_string'] for label in chexpert_targets)\n#plt.bar(chexpert_targets, labels_count.values(), align='center', alpha=0.5)\n#plt.show\nx_pos = np.arange(len(labels_count.values()))\n#Plot the data:\nmy_colors = 'rgbkymc'\nlbls = list.copy(chexpert_targets)\nlbls.insert(0,'')\nplt.bar(x_pos, labels_count.values(), align='center', alpha=0.5 , color=my_colors)\nplt.xticks(x_pos, lbls, rotation='vertical')\n","b3141209":"sample_perc = 0.00\ntrain_only_df = full_df[~full_df.train_valid]\nvalid_only_df = full_df[full_df.train_valid]\nunique_patients = train_only_df.patient.unique()\nmask = np.random.rand(len(unique_patients)) <= sample_perc\nsample_patients = unique_patients[mask]\n\ndev_df = train_only_df[full_train_df.patient.isin(sample_patients)]\ntrain_df = train_only_df[~full_train_df.patient.isin(sample_patients)]\n\nprint(valid_only_df.Path.size)\nprint(train_df.Path.size)","254c394e":"datagen=image.ImageDataGenerator(rescale=1.\/255, \n                                 featurewise_center=True,\n                                 featurewise_std_normalization=True,\n                                 rotation_range=5,\n                                 width_shift_range=0.2,\n                                 height_shift_range=0.2,\n                                 horizontal_flip=True,\n                                 validation_split = 0.1)\ntest_datagen=image.ImageDataGenerator(rescale=1.\/255)","8035c788":"def generate_datasets(image_size = 224):\n\n    train_generator=datagen.flow_from_dataframe(dataframe=train_df, directory=data_path, \n                                                x_col=\"Path\", y_col=\"feature_string\", has_ext=True, seed = 42, #classes = chexpert_targets,\n                                                class_mode=\"categorical\", target_size=(image_size,image_size), batch_size=32, subset = \"training\")\n\n    validation_generator = datagen.flow_from_dataframe(dataframe=train_df, directory=data_path, \n                                                       x_col=\"Path\", y_col=\"feature_string\", has_ext=True, seed = 42, #classes = chexpert_targets,\n                                                       class_mode=\"categorical\", target_size=(image_size,image_size), batch_size=32, subset = \"validation\")\n\n    test_generator = test_datagen.flow_from_dataframe(dataframe=valid_only_df, directory=data_path, \n                                                      target_size=(image_size,image_size),class_mode='categorical',\n                                                      batch_size=1, shuffle=False, #classes = chexpert_targets,\n                                                      x_col=\"Path\", y_col=\"feature_string\")\n    \n    return [train_generator,validation_generator,test_generator]","77eecdc0":"from keras.callbacks import *\n\nclass CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https:\/\/arxiv.org\/abs\/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w\/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi\/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.01, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1\/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary\/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations\/(2*self.step_size))\n        x = np.abs(self.clr_iterations\/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())","57bdef37":"def auc(y_true, y_pred):\n    auc = tf.metrics.auc(y_true, y_pred)[1]\n    K.get_session().run(tf.local_variables_initializer())\n    return auc","0e4d1cb7":"from keras.layers import Conv2D\nfrom keras.optimizers import Adam\ndef build_model(image_size = 224):\n    base_model = InceptionResNetV2(include_top= False, input_shape=(image_size,image_size,3), weights='imagenet')\n    \n    # IRNV2 is already trained. We want to preserve such weights and train our custom layers.\n    for layer in base_model.layers:\n        layer.trainable = True\n\n    # this is the model we will train\n    \n    ########################### CUSTOM \n    \n    x = base_model.output\n    x = Conv2D(500, kernel_size = (3,3), activation='relu', input_shape = (image_size,image_size,3))(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    x = GlobalAveragePooling2D(input_shape=(1024,1,1))(x)\n    \n    # Add a fully connected layer .\n    # x = Flatten()(x)\n    \n    x = Dense(700, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    \n    predictions = Dense(6, activation='sigmoid')(x)\n    ############### END CUSTOM\n    \n    model = Model(inputs=base_model.input, outputs=predictions)\n    \n    model.load_weights('..\/input\/chexpert-inceptionresv2-clean\/weights.hdf5')\n\n    adamOptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=10**-8, decay=0.0, amsgrad=False)\n    # compile the model (should be done *after* setting layers to non-trainable)\n    model.compile(optimizer=adamOptimizer, loss='categorical_crossentropy', metrics=['accuracy', auc])\n    return model","524dd3cd":"def train_model(clr_triangular, model , datasets, epochs=1, image_size = 224):\n    \n    checkpointer = ModelCheckpoint(filepath='weights.hdf5', \n                                   verbose=1, save_best_only=True)\n    \n    train_generator,validation_generator,test_generator = datasets\n    \n    STEP_SIZE_TRAIN=train_generator.n\/\/train_generator.batch_size\n    STEP_SIZE_VALID=validation_generator.n\/\/validation_generator.batch_size\n    print(STEP_SIZE_TRAIN)\n    print(STEP_SIZE_VALID)\n\n    return model.fit_generator(generator=train_generator,\n                        steps_per_epoch=STEP_SIZE_TRAIN,\n                        validation_data=validation_generator,\n                        validation_steps=STEP_SIZE_VALID,\n                        epochs=epochs, callbacks = [clr_triangular, checkpointer])","a8206a9e":"image_size_input = 224\nmodel = build_model(image_size = image_size_input)","2abbc7a7":"#from keras.utils import plot_model\n#plot_model(model, to_file='model.png')","24d558df":"datasets = generate_datasets(image_size = image_size_input)\ntrain_generator,validation_generator,test_generator = datasets","a9eab4a4":"clr_triangular = CyclicLR(mode='exp_range')\nhistory = train_model(clr_triangular, model , datasets, epochs=3, image_size = image_size_input)","f424bd8d":"clr_triangular.history","43849174":"print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\nprint(\"Momentum Range : \", min(clr_triangular.history['momentum']), max(clr_triangular.history['momentum']))","d5e38665":"plt.xlabel('Training Iterations')\nplt.ylabel('Learning Rate')\nplt.title(\"CLR\")\nplt.plot(clr_triangular.history['lr'])\nplt.show()","4db4f39d":"plt.xlabel('Training Iterations')\nplt.ylabel('Momentum')\nplt.title(\"CLR\")\nplt.plot(clr_triangular.history['momentum'])\nplt.show()","4f8f9f59":"history.history","33d8c707":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","ca5417ed":"from sklearn.preprocessing import MultiLabelBinarizer\ntest = pd.Series(test_generator.labels)\nmlb = MultiLabelBinarizer()\ny_labels = mlb.fit_transform(test)","c1dfd10a":"test_generator.reset()\ny_pred_keras = model.predict_generator(test_generator,verbose = 1,steps=test_generator.n)","a2315735":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\n\nfor ii in range(1, y_pred_keras.shape[1]):\n    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_labels[:,ii], y_pred_keras[:,ii])\n    auc_keras = auc(fpr_keras, tpr_keras)\n    plt.plot(fpr_keras, tpr_keras, label=chexpert_targets[ii-1] + '(area = {:.3f})'.format(auc_keras))\n    \nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()\n    \n\n","ad35a596":"Choose better LR","6d187253":"### Add target features string","12fae8e5":"Define the metrics","7aa4da74":"# Learning Curves","43e47ff2":"# Show some images","f4c8e5f8":"## Set up the dataframe for training: \n* Separate the validation and the training samples\n* Take a unique sample per patient\n* Get a samle of 5% of the dataset","14b9a4dd":"# Build the model","baedbb7a":"### Load configuration with local path and url for dataset","7e1260d7":"### Load Data and the targets ","712cdb82":"This is to get the number of study and the number of patient for each entry of the dataset","4b9cf943":"### Create patient and study columns","c28c94cc":"Callback!!!","138c8a8e":"Create a feature string containing all the classes and omiting the unknowns","e21b8236":"### Uncertainty Approaches\n\nThe CheXpert paper outlines several different approaches to mapping using the uncertainty labels in the data:\n\n- Ignoring - essentially removing from the calculation in the loss function\n- Binary mapping - sending uncertain values to either 0 or 1\n- Prevalence mapping - use the rate of prevelance of the feature as it's target value\n- Self-training - consider the uncertain values as unlabeled\n- 3-Class Classification - retain a separate value for uncertain and try to predict it as a class in its own right\n\n\nThe paper gives the results of different experiments with the above approaches and indicates the most accurate approach for each feature.\n    \n|Approach\/Feature|Atelectasis|Cardiomegaly|Consolidation|Edema|PleuralEffusion|\n|-----------|-----------|-----------|-----------|-----------|-----------|\n|`U-Ignore`|0.818(0.759,0.877)|0.828(0.769,0.888)|0.938(0.905,0.970)|0.934(0.893,0.975)|0.928(0.894,0.962)|\n|`U-Zeros`|0.811(0.751,0.872)|0.840(0.783,0.897)|0.932(0.898,0.966)|0.929(0.888,0.970)|0.931(0.897,0.965)|\n|`U-Ones`|**0.858(0.806,0.910)**|0.832(0.773,0.890)|0.899(0.854,0.944)|0.941(0.903,0.980)|0.934(0.901,0.967)|\n|`U-Mean`|0.821(0.762,0.879)|0.832(0.771,0.892)|0.937(0.905,0.969)|0.939(0.902,0.975)|0.930(0.896,0.965)|\n|`U-SelfTrained`|0.833(0.776,0.890)|0.831(0.770,0.891)|0.939(0.908,0.971)|0.935(0.896,0.974)|0.932(0.899,0.966)|\n|`U-MultiClass`|0.821(0.763,0.879)|**0.854(0.800,0.909)**|0.937(0.905,0.969)|0.928(0.887,0.968)|0.936(0.904,0.967)|\n\nThe binary mapping approaches (U-Ones and U-Zeros) are easiest to implement and so to begin with we take the best option between U-Ones and U-Zeros for each feature\n\n- Atelectasis `U-Ones`\n- Cardiomegaly `U-Zeros`\n- Consolidation `U-Zeros`\n- Edema `U-Ones`\n- Pleural Effusion `U-Zeros`","f5a97951":"# Set up data generation"}}