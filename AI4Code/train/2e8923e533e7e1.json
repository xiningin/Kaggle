{"cell_type":{"9f8f34b1":"code","1e29ea19":"code","dedfc707":"code","153df593":"code","9710a92c":"code","a2fb1735":"code","36519880":"code","d862871c":"code","c5dd946b":"code","ac71e276":"code","4de6ccde":"code","ee14b80f":"code","14b79cca":"code","859d80bb":"code","d489745c":"code","9331a546":"code","d09a2ae1":"code","6165271f":"code","7b5267f9":"code","288af6d8":"code","7c71dce9":"code","cea397c3":"code","f06b78f0":"code","182eaec2":"code","54de0cee":"markdown","1c6033a4":"markdown","c1b393b5":"markdown","162dd543":"markdown","e1bab343":"markdown","2df3c728":"markdown","1edd0813":"markdown","bebf8881":"markdown"},"source":{"9f8f34b1":"import os\nimport pandas as pd\nimport keras \nimport cv2\nfrom tensorflow.keras.preprocessing.image import img_to_array, array_to_img\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.layers import Dense, Input, Reshape, UpSampling2D\nfrom tensorflow.keras.layers import  MaxPooling2D, Conv2D, Conv2DTranspose\nfrom tensorflow.keras.layers import SeparableConv2D, BatchNormalization\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, AveragePooling2D\nfrom tensorflow.keras.layers import Activation, Add, Concatenate\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.models import Model\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1e29ea19":"class_dict = pd.read_csv('\/kaggle\/input\/semantic-drone-dataset\/class_dict_seg.csv')\nclass_dict","dedfc707":"def show_images(paths, count):\n    original_dir, mask_dir, label_dir = paths\n    original_paths = [os.path.join(original_dir, filename) \n    for filename in sorted(os.listdir(original_dir))][:count]\n    mask_paths = [os.path.join(mask_dir, filename) \n    for filename in sorted(os.listdir(mask_dir))][:count]\n    label_paths = [os.path.join(label_dir, filename) \n    for filename in sorted(os.listdir(label_dir))][:count]\n    \n    title = 'original '\n    title += 'mask ' \n    title += 'label'\n    unique_types = 3\n    image_height = 256\n    image_width = 256\n    channels = 3\n    \n    images = np.zeros(shape = (count, \n                               image_height, \n                               image_width * unique_types, \n                               channels))\n    \n    for iteration in range(count):\n        # path for each class of image\n        current_original_path = original_paths[iteration]\n        current_mask_path = mask_paths[iteration]\n        current_label_path = label_paths[iteration]\n        \n        original = load_img(current_original_path).resize((image_height, image_width))\n        original = img_to_array(original).reshape((image_height, image_width, 3))\n        \n        mask = load_img(current_mask_path).resize((image_height, image_width))\n        mask = img_to_array(mask).reshape((image_height, image_width, 3))\n\n        label = load_img(current_label_path).resize((image_height, image_width))\n        label = img_to_array(label).reshape((image_height, image_width, 3))\n        \n        h_stack = np.hstack([original, mask, label])\n        images[iteration] = h_stack\n    \n    images = np.concatenate(images) \/ 255\n    plt.figure(figsize = (20,20))\n    plt.title(title)\n    plt.axis('off')\n    plt.imshow(images)\n\nimages_path = '\/kaggle\/input\/semantic-drone-dataset\/dataset\/semantic_drone_dataset\/original_images'\nmasks_path = '\/kaggle\/input\/semantic-drone-dataset\/RGB_color_image_masks\/RGB_color_image_masks'\nlabels_path = '\/kaggle\/input\/semantic-drone-dataset\/dataset\/semantic_drone_dataset\/label_images_semantic'\n\npaths = [images_path, masks_path, labels_path]\ncount = 10\nshow_images(paths = paths, count = count)","153df593":"def sep_bn_act(inputs, filters, index, use_maxpool):\n    block = Activation('relu')(inputs)\n    block = SeparableConv2D(filters = filters, \n                            kernel_size = 3, \n                            padding ='same', \n                            name = f'block{index}_sepconv_1', )(block)\n    block = BatchNormalization(name = f'block{index}_sepconv1_BN')(block)\n    block = Activation('relu', name = f'block{index}_sepconv1_act')(block)\n    \n    block = SeparableConv2D(filters = filters, \n                            kernel_size = 3, \n                            padding ='same', \n                            name = f'block{index}_sepconv_2')(block)\n    \n    block = BatchNormalization(name = f'block{index}_sepconv2_BN')(block)\n    \n#     if use_maxpool:\n#         block = MaxPooling2D(pool_size = 2)(block)\n#     else:\n    block = Activation('relu')(block)\n        \n        \n    return block\n\n\ndef residual(inputs, filters, index, strides, padding = 'same'):\n    res_conv = Conv2D(filters = filters, \n                      kernel_size = 3, \n                      strides = strides,\n                      padding = padding,\n                      name = f'residual_conv_{index}')(inputs)\n    res_conv = BatchNormalization(name = f'residual_BN_{index}')(res_conv)\n    \n    return res_conv","9710a92c":"def light_xception(inputs, mode = 'layers'):\n\n    '''\n    this function create like-xception model\n    with 5 millions params instead 22 millons \n    in original\n\n    Arguments:\n        inputs(tensor): outputs of previuos layers or input data\n        mode(str): parameter of this model\n    \n    Returns: \n        light-weight xception model\n    '''\n    strides = 1\n    image_height = inputs.shape[1]\n    image_width = inputs.shape[2] \n    width_condition = np.sqrt(image_width) == int(np.sqrt(image_width))\n    height_condition = np.sqrt(image_height) == int(np.sqrt(image_height))\n    assertion = 'both sides of image must be 2 **n and greater than 2 **3'\n    # assert width_condition, assertion\n    # assert height_condition, assertion\n    \n    index = 1\n    \n    #       input block           \n    ################################################################\n    if mode == 'model':\n        inputs = Input(shape = input_shape, name = 'input_layer')\n    \n    # block 1\n    ################################################################\n    filters = 64\n    block_0 = sep_bn_act(inputs = inputs, \n                         filters = filters, \n                         index = index, \n                         use_maxpool = False)\n    \n    ################################################################\n    \n    index +=1\n    filters = 128\n    \n    #separable\n    block_1 = sep_bn_act(inputs = block_0, \n                         filters = filters, \n                         index = index, \n                         use_maxpool = True)\n    \n    #    residual block      \n    res_conv_1 = residual(inputs = block_0, \n                          filters = filters,\n                          index=index, \n                          strides = strides)\n    \n    #    add block    \n    add_1 = Add(name = f'add_{index}')([block_1, res_conv_1])\n    \n    \n    #################################################################\n    \n    index +=1\n    filters = 256\n    \n    #separable\n    block_2 = sep_bn_act(inputs = add_1, \n                         filters = filters, \n                         index = index, \n                         use_maxpool = True)\n    \n    #    residual block      \n    res_conv_2 = residual(inputs = add_1, \n                          filters = filters, \n                          index=index, \n                          strides = strides)\n    #    add block    \n    add_2 = Add(name = f'add_{index}')([block_2, res_conv_2])\n    \n    #################################################################\n    \n    index +=1\n    filters = 512\n    \n    #separable\n    block_3 = sep_bn_act(inputs = add_1, \n                         filters = filters, \n                         index = index, \n                         use_maxpool = True)\n    \n    #    residual block      \n    res_conv_3 = residual(inputs = add_1,\n                          filters = filters, \n                          index=index, \n                          strides = strides)\n    #    add block    \n    add_3 = Add(name = f'add_{index}')([block_3, res_conv_3])\n    \n    #################################################################\n    \n    index +=1\n    filters = 512\n    \n    #separable\n    block_4 = sep_bn_act(inputs = add_3, \n                         filters = filters, \n                         index = index, \n                         use_maxpool = False)\n    index +=1\n    block_4 = sep_bn_act(inputs = block_4, \n                         filters = filters, \n                         index = index, \n                         use_maxpool = False)\n    \n    #    add block    \n    add_4 = Add(name = f'add_{index}')([block_4, add_3])\n    \n    #################################################################\n    \n\n    index +=1\n    filters = 512\n    \n    #separable\n    block_5 = sep_bn_act(inputs = add_4, \n                         filters = filters, \n                         index = index, \n                         use_maxpool = True)\n    #    residual block      \n    res_conv_5 = residual(inputs = add_4, \n                          filters = filters, \n                          index=index, \n                          strides = strides)\n    #    add block    \n    add_5 = Add(name = f'add_{index}')([block_5, res_conv_5])\n    \n    ##################################################################\n    \n    index +=1\n    filters = 128\n    \n    #separable\n    outputs = sep_bn_act(inputs = add_5, \n                         filters = filters, \n                         index = index, \n                         use_maxpool = False)\n    \n    result = Model(inputs = inputs, outputs = outputs)\n    \n    if mode == 'layers':\n        result = outputs\n    \n    return result\n\ninput_shape = (128, 128, 3)\n#fake data for building the model\nexample_data = np.zeros(shape = (32,) + input_shape)\n\nlight_xception_model = light_xception(inputs = example_data, mode = 'model')","a2fb1735":"xception_model = Xception()","36519880":"light_count = light_xception_model.count_params()\nfull_count = xception_model.count_params()\nprint('light: ', light_count)\nprint('full :       ', full_count)\nprint('difference:     ', round(full_count \/ light_count, 1))","d862871c":"def PPM(inputs):\n    '''\n    PPM - Pyramyd Pooling Module\n    this function just create PP module\n    '''\n    \n    # parameters\n    filters = 128\n    kernel_size = (1, 1)\n    interpolation = 'bilinear'\n    \n    #base\n    base = light_xception(inputs = inputs, mode = 'layers')\n    \n    #red\n    red = GlobalAveragePooling2D(name = 'red_glob_av_pooling')(base)\n    red = Reshape(target_shape = (1, 1,128))(red)\n    red = Conv2D(filters = filters, \n                 kernel_size = kernel_size, \n                 padding = 'same', name = 'red_1_1x1')(red)\n    red = BatchNormalization(name = 'red_BN_1')(red)\n    red = Conv2D(filters = filters * 2,\n                 kernel_size = kernel_size, \n                 padding = 'same', name = 'red_2_1x1')(red)\n    red = BatchNormalization(name = 'red_BN_2')(red)\n    red = Conv2D(filters = filters, \n                 kernel_size = kernel_size, \n                 padding = 'same', name = 'red_3_1x1')(red)\n    red = BatchNormalization(name = 'red_BN_3')(red)\n    red = UpSampling2D(size = 128, \n                       interpolation = interpolation, \n                       name = 'red_upsampling')(red)\n    \n    #blue\n    blue = AveragePooling2D(pool_size = 2, \n                            name = 'blue_av_pooling')(base)\n    blue = Conv2D(filters = filters, \n                  kernel_size = kernel_size, \n                  padding = 'same', \n                  name = 'blue_1_1x1')(blue)\n    blue = BatchNormalization(name = 'blue_BN_1')(blue)\n    blue = Conv2D(filters = filters * 2, \n                  kernel_size = kernel_size, \n                  padding = 'same', name = 'blue_2_1x1')(blue)\n    blue = BatchNormalization(name = 'blue_BN_2')(blue)\n    blue = Conv2D(filters = filters, \n                  kernel_size = kernel_size, \n                  padding = 'same', name = 'blue_3_1x1')(blue)\n    blue = BatchNormalization(name = 'blue_BN_3')(blue)\n    blue = UpSampling2D(size = 2, interpolation = interpolation, \n                        name = 'blue_upsampling')(blue)\n    \n    #green\n    green = AveragePooling2D(pool_size = 4, \n                             name = 'green_av_pooliing')(base)\n    green = Conv2D(filters = filters, \n                   kernel_size = kernel_size, \n                   padding = 'same', name = 'green_1_1x1')(green)\n    green = BatchNormalization(name = 'green_BN_1')(green)\n    green = Conv2D(filters = filters * 2, \n                   kernel_size = \n                   kernel_size, padding = 'same', \n                   name = 'green_2_1x1')(green)\n    green = BatchNormalization(name = 'green_BN_2')(green)\n    green = Conv2D(filters = filters, \n                   kernel_size = kernel_size, \n                   padding = 'same', name = 'green_3_1x1')(green)\n    green = BatchNormalization(name = 'green_BN_3')(green)\n    green = UpSampling2D(size = 4, name = 'green_upsampling')(green)\n    \n    #yellow\n    yellow = AveragePooling2D(pool_size = 8, \n                              name = 'yellow_av_pooling')(base)\n    yellow = Conv2D(filters = filters, \n                    kernel_size = kernel_size, padding = 'same', \n                    name = 'yellow_1_1x1')(yellow)\n    yellow = BatchNormalization(name = 'yellow_BN_1')(yellow)\n    yellow = Conv2D(filters = filters * 2, \n                    kernel_size = kernel_size, \n                    padding = 'same', name = 'yellow_2_1x1')(yellow)\n    yellow = BatchNormalization(name = 'yellow_BN_2')(yellow)\n    yellow = Conv2D(filters = filters, \n                    kernel_size = kernel_size, \n                    padding = 'same', name = 'yellow_3_1x1')(yellow)\n    yellow = BatchNormalization(name = 'yellow_BN_3')(yellow)\n    yellow = UpSampling2D(size = 8, name = 'yellow_upsampling')(yellow)\n    \n    result = Concatenate(\n        name = 'concatenate_layer')([base, red, blue, green, yellow])\n    \n    return result\n    \n\n    \ndef PSPnet_last_conv(inputs, classes):\n    ppm_outputs = PPM(inputs)\n    \n    x = Conv2D(filters = classes, \n               kernel_size = 3, \n               padding = 'same', \n               name = 'last_module_conv')(ppm_outputs)\n    x = BatchNormalization(name = 'last_module_BN')(x)\n    x = Activation('softmax')(x)\n    \n    return x\n\n\n\ndef build_PSPnet():\n    inputs = Input(shape = input_shape, name = 'input')\n    outputs = PSPnet_last_conv(inputs, classes = 24)\n\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.metrics import MeanIoU\n\n    pspnet = Model(inputs = inputs, outputs = outputs, name = 'PSPnet')\n    pspnet.compile(loss = 'categorical_crossentropy', \n                   metrics = ['accuracy', MeanIoU(num_classes = 24)], \n                   optimizer = Adam(lr = 0.01))\n    return pspnet\n\npspnet = build_PSPnet()","c5dd946b":"print('number of parameters in pspnet: ', pspnet.count_params())","ac71e276":"plot_model(pspnet, show_shapes = True, to_file = 'PSPnet.png')","4de6ccde":"num_classes = len(class_dict)\nimage_src = '\/kaggle\/input\/semantic-drone-dataset\/RGB_color_image_masks\/RGB_color_image_masks\/005.png'\n\nmask_example = load_img(image_src)\nmask_example = mask_example.resize((256, 256))\nmask_example = img_to_array(mask_example).astype('uint8')\nseparate_mask = np.zeros(shape = (1, 256, 256, num_classes))\n\nplt.figure(figsize = (30, 30))\nplt.subplot(5,5,1)\nplt.axis('off')\nplt.title('original')\nplt.imshow(mask_example)\n\nfor iteration in range(num_classes):\n    current_feature_name = list(class_dict.iloc[iteration])[0]\n    colors = tuple(class_dict.iloc[iteration][1:].map(int))\n    separate_mask_class = cv2.inRange(mask_example, colors, colors)\n    separate_mask[0, :, :, iteration] = separate_mask_class \/ 255\n    \n    \n    \n    plt.subplot(5, 5, iteration + 2)\n    plt.axis('off')\n    plt.title(current_feature_name)\n    plt.imshow(separate_mask_class)\n","ee14b80f":"def separate_image(mask_src, num_classes):\n    '''\n    this function takes original mask and convert it into \n    24 channels tensor, each channel consist binary image \n    \n    Arguments:\n        mask_src(string): path to original mask\n        num_classes(int): number of unique classes in mask\n    '''\n    \n    channels = 3\n    image_size = 128\n    current_mask = load_img(mask_src)\n    current_mask = img_to_array(current_mask).astype('uint8')\n    separated_mask = np.zeros(shape = (image_size, image_size, num_classes))\n    \n    for iteration in range(num_classes):\n        current_feature_name = list(class_dict.iloc[iteration])[0]\n        colors = tuple(class_dict.iloc[iteration][1:].map(int))\n        separated_mask_class = cv2.inRange(current_mask, colors, colors)\n        separated_mask[:, :, iteration] = separated_mask_class\n        \n    return separated_mask\n\nnum_classes = len(class_dict)\nimage_src = '\/kaggle\/input\/semantic-drone-dataset\/RGB_color_image_masks\/RGB_color_image_masks\/000.png'","14b79cca":"def reduce_dim(source_dir, target_size):\n    '''\n    this function takes images from one \n    directory reduce their size to target_size\n    and write them into target directory\n\n    Arguments:\n        source_dir(str): path to directory with images\n        target_size(tuple): image will be resized to this size\n\n    returns:\n        target_dir(str): directory with resized images\n    '''\n    working_directory = '\/kaggle\/working'\n    dir_name = source_dir.split('\/')[-1]\n    target_dir = os.path.join(working_directory, dir_name)\n    print(target_dir)\n    \n    if os.path.isdir(target_dir) == False:\n        os.mkdir(target_dir)\n        \n    original_filenames = [os.path.join(source_dir, filename) for filename in sorted(os.listdir(source_dir))]\n    target_filenames = [os.path.join(target_dir, filename) for filename in sorted(os.listdir(source_dir))]\n    \n    for index in range(len(original_filenames)):\n        current_orig_filename = original_filenames[index]\n        current_target_filename = target_filenames[index]\n        \n        image = cv2.imread(current_orig_filename)\n        image = cv2.resize(image, target_size)\n\n        cv2.imwrite(img = image, filename = current_target_filename)\n    \n    return target_dir\n\nsource_dir = '\/kaggle\/input\/semantic-drone-dataset\/dataset\/semantic_drone_dataset\/original_images'\nmasks_dir = '\/kaggle\/input\/semantic-drone-dataset\/RGB_color_image_masks\/RGB_color_image_masks'\ntarget_size = (128, 128)\n\nimages_dir = reduce_dim(source_dir = source_dir, target_size = target_size)\nmasks_dir = reduce_dim(source_dir = masks_dir, target_size = target_size)\n","859d80bb":"# if u already reduced dim\nimages_dir = '\/kaggle\/working\/original_images'\nmasks_dir = '\/kaggle\/working\/RGB_color_image_masks'","d489745c":"len(os.listdir('\/content\/original_images'))","9331a546":"def make_batch(batch_size, masks_path, images_path):\n    \n    '''\n    this function convert images from directory to tensors. \n    images to (img_size, img_size, 3) masks to (img_size, img_size, 24)\n\n    Arguments:\n        batch_size(int): number of images in tensor\n        masks_path(str): path to directory with masks\n        images_path(str): path to directory with images\n    Returns:\n        two tensors: with masks and with images\n    '''\n\n    condition = batch_size <= len(os.listdir(masks_path))\n    assertion = 'batch size have not be greater'\n    assertion += ' than all lenth of all directory'\n    assert condition, assertion \n    images_paths = [os.path.join(images_path, filename) for filename \n                    in sorted(os.listdir(images_path))[:batch_size]]\n    \n    masks_paths = [os.path.join(masks_path, filename) for filename \n                    in sorted(os.listdir(masks_path))[:batch_size]]\n    image_size = 128\n    channels = 3\n    num_classes = 24\n    \n  \n    train_images_paths = images_paths\n    train_masks_paths = masks_paths\n    \n    train_images = np.zeros(shape = (batch_size, \n                                     image_size, \n                                     image_size, \n                                     channels))\n    train_masks = np.zeros(shape = (batch_size, \n                                    image_size, \n                                    image_size, \n                                    num_classes))\n    \n    for index in range(batch_size):\n        # print(f\"{'-' * 5}train_set:_{index + 1}{'-' * 5}\")\n        current_image_path = train_images_paths[index]\n        current_mask_path = train_masks_paths[index]\n        image = img_to_array(load_img(current_image_path)) \/ 255\n        image = image.astype('float32') \n        mask = separate_image(current_mask_path, num_classes) \/ 255\n        mask = mask.astype('float32') \n        \n        train_images[index] = image\n        train_masks[index] = mask\n   \n   \n        \n    data = [train_images.astype(np.float32), \n            train_masks.astype(np.float32)]\n    \n    return data","d09a2ae1":"train_images, train_masks = make_batch(batch_size = 100, \n                                       masks_path = masks_dir, \n                                       images_path = images_dir)","6165271f":"pspnet = build_PSPnet()\n\ndef plot_masks(img, model, iteration = -1):\n    '''\n    this function shows predicted masks for image\n\n    Arguments:\n        img(tensor): image for prediction\n        model(tf.model): neural network\n        iteration(int): current iteration of training\n    '''\n    img_for_pred = img.reshape((1, ) + img.shape)\n    prediction = pspnet.predict(img_for_pred)\n    mask = prediction.reshape(128, 128, 24)\n\n    epoch = (iteration + 1) * 10\n    num_classes = len(class_dict)\n    plt.figure(figsize = (10, 10))\n    plt.subplot(5,5,1)\n    plt.axis('off')\n    plt.imshow(img)\n    for iteration in range(num_classes):\n        plt.subplot(5,5,iteration + 2)\n        plt.axis('off')\n        plt.imshow(mask[:,:,iteration])\n    \n    plt.savefig(fname = f'prediction_{epoch}.png')\n\nplot_masks(train_images[0], model = pspnet)","7b5267f9":"train_images.dtype","288af6d8":"!nvidia-smi","7c71dce9":"def train_model(iterations, weight_path = None):\n    '''\n    training function\n    \n    iterations:\n        iterations(int): number of iterations, \n        each consist 10 epochs of training\n\n        weights_path(str): path to weights for model\n        if this path contain already builded weights\n        model will load it \n    returns:\n        trained model\n    '''\n    pspnet = build_PSPnet()\n    if os.path.isfile(weights_path):\n        pspnet.load_weights(weights_path)\n    callbacks = [ReduceLROnPlateau(monitor = 'loss',\n                                factor = 0.2, \n                                min_delta = 0.00001,\n                                patience = 4),\n                ModelCheckpoint(filepath = weight_path,\n                                monitor = 'loss',\n                                save_best_only = True)]\n\n    test_image = train_images[5]\n\n    for iteration in range(iterations):\n        pspnet.fit(x = train_images, \n                y = train_masks,\n                batch_size = 8,\n                epochs = 10,\n                verbose = 1,\n                callbacks = callbacks)\n\n        plot_masks(img = test_image, \n                   model = pspnet, \n                   iteration = iteration)\n\nweights_path = '\/kaggle\/working\/pspnet_best_weights.h5'\ntrain_model(iterations = 2, weights_path = weights_path )","cea397c3":"def cvt_masks_to_mask(masks):\n    '''\n    this function convert input tensor \n    with shape (image_size, image_size, num_classes)\n    to (image_size, image_size, 3)\n\n    Arguments:\n        masks(tensor): tensor with shape == (128, 128, num_classes)\n    '''\n    image_shape = (128, 128, 3)\n    result = np.zeros(shape = image_shape)\n    for iteration in range(num_classes):\n        current_result = np.zeros(shape = image_shape)\n        current_mask = masks[:,:,iteration]\n        current_feature_name = list(class_dict.iloc[iteration])[0]\n        colors = tuple(class_dict.iloc[iteration][1:].map(int))\n\n        current_result[:,:,0] = current_mask\n        current_result[:,:,1] = current_mask\n        current_result[:,:,2] = current_mask \n\n        current_result *= colors\n\n        result += current_result\n    \n    return result\n        ","f06b78f0":"def show_pred_24masks():\n    '''\n    this function shows us mask for each class\n    '''\n    pspnet.load_weights('\/kaggle\/working\/pspnet_best_weights.h5')\n    test_image = train_images[5]\n    plot_masks(img = test_image, \n               model = pspnet, \n               iteration = iteration)\n    img_for_pred = test_image.reshape((1, ) + test_image.shape)\n    prediction = pspnet.predict(img_for_pred)\n    masks = prediction.reshape(128, 128, 24)\n    mask = cvt_masks_to_mask(masks) \/ 255\n\n    return mask\nmask = show_pred_24masks()","182eaec2":"def show_preds(num_examples = 5):\n    '''\n    this function shows us predicted masks\n    from pspnet model\n\n    Arguments:\n        num_examples(int): number examples of predictions\n    '''\n    pspnet.load_weights('\/kaggle\/working\/pspnet_best_weights.h5')\n\n    \n    rgb_path = '\/kaggle\/working\/RGB_color_image_masks'\n    masks_paths = [os.path.join(rgb_path, filename) \n    for filename in sorted(os.listdir(rgb_path))[1:]]\n\n    space = ' ' * 5\n    suptitle = f'|{space}original {space}|{space}truth_mask{space}|'\n    suptitle += f' truth_segmentated{space}|'\n    suptitle += f'{space}pred_mask{space}| pred_segmentated |'\n    plt.figure(figsize=(16, 16))\n    print(suptitle)\n\n    for index in range(num_examples):\n        test_image = train_images[index + 1]\n        img_for_pred = test_image.reshape((1, ) + test_image.shape)\n        prediction = pspnet.predict(img_for_pred)\n\n        ground_truth_mask = img_to_array(\n            load_img(masks_paths[index])) \/ 255\n\n        masks = prediction.reshape(128, 128, 24)\n        mask = cvt_masks_to_mask(masks) \/ 255\n        segmentated_image = (mask + test_image) \/ 2\n        truth_segmentated_img = (ground_truth_mask + test_image) \/ 2\n        hstack = np.hstack([test_image, ground_truth_mask, \n                            truth_segmentated_img, \n                            mask, segmentated_image])\n        \n        plt.subplot(num_examples, 1, index + 1)\n        plt.axis('off')\n        plt.imshow(hstack)\n\nhstack = show_preds()","54de0cee":"as an input we have 6000 * 4000 images and masks, I reduced its resolution to 128*128 after I seperated masks by classes thus I got tensor with shape (128, 128, 24) \nand then I have two tensors: images and masks\n\nI built model it consist light-weight architecture like Xception as backbone and PPM module, as last conv block we have Conv2D with kernel-size (3,3), BN and last layer - activation function softmax. The architecture may look cumbersome, but in fact there are only 5 million parameters, for example original Xception has 22 millions\n\nI was really excited when I looked at results first time.I didn't have access to a GPU and the training part took 4 hours on the CPU.\n\nU don't need to train model for using this model for ur aims, I already shared weight, u only have to use: pspnet.load_weights(weights_path)","1c6033a4":"![image.png](attachment:fded2742-3167-4ac5-aec7-3ecaf6bbb1c0.png)","c1b393b5":"# let's separate masks by classes","162dd543":"# train model","e1bab343":"# what all this data mean","2df3c728":"# overview","1edd0813":"![image.png](attachment:34016ca6-ca51-419e-9b26-ebfc9cbbf940.png)","bebf8881":"# model architecture"}}