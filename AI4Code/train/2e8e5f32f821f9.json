{"cell_type":{"554efd83":"code","e7317620":"code","bdc81d43":"code","6067de7a":"code","da4330fd":"code","ed5a7619":"code","6bdab6ac":"code","96ce991a":"code","47778f19":"code","7a1e8734":"code","bc126c1f":"code","21597145":"code","578f7cb7":"code","84ac9a8b":"code","5668e18d":"code","2a27ea9b":"code","7a4bafc9":"code","9a2123a0":"code","96b8aca8":"code","e5778fcb":"code","06743ecb":"code","c355b3ee":"code","7ab1f851":"markdown","951dc6a7":"markdown","d8b4ee15":"markdown","345433b9":"markdown","56a36a86":"markdown","503c3ecc":"markdown","3965ba86":"markdown","8dc26431":"markdown","de723111":"markdown","34cbdc5c":"markdown","0d3ed5fb":"markdown"},"source":{"554efd83":"import os\nfrom datetime import datetime\n\n# Preprocessing and Plotting Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Tools for training model\nimport tensorflow as tf\nfrom tensorflow.keras.applications import vgg16\n\n#Tools for evaluation of model\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport scipy as sp\n\n# Get dataset from GCS\nfrom kaggle_datasets import KaggleDatasets                    # Uncomment to run on kaggle","e7317620":"os.environ","bdc81d43":"try:\n    tpu_address = os.environ['TPU_NAME']                                       # For running notebook on Kaggle notebook\n    #tpu_address = 'grpc:\/\/' + os.environ['COLAB_TPU_ADDR']                     # For running notebook on Colab\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_address)\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)                      # Training of model takes place wrt this strategy.\n\n    print('Running on TPU: ', tpu.cluster_spec().as_dict()['worker'])\n    print('Number of accelerators: ', strategy.num_replicas_in_sync)\n\nexcept ValueError:\n    print('TPU failed to initialize.')","6067de7a":"GCS_PATH = 'gs:\/\/kds-84d72844fcdf3e93f635a51c229600d46e76d474201c6ba89581d043'\nprint(GCS_PATH)","da4330fd":"!gsutil ls $GCS_PATH                    # Looking at sub-dirs and files in GCS PATH","ed5a7619":"train_path = GCS_PATH + '\/Fish_Dataset\/Fish_Dataset\/*\/*\/*'","6bdab6ac":"# Define constants\nIMG_SIZE = [224, 224]\nBATCH_SIZE = 128\nVAL_SPLIT = 0.2\nBUFFER_SIZE = 1024\nCLASSES = ['Black Sea Sprat', 'Gilt-Head Bream', 'Hourse Mackerel', 'Red Mullet', 'Red Sea Bream', 'Sea Bass', 'Shrimp', 'Striped Red Mullet', 'Trout']","96ce991a":"# Training And Validation Data\ndef get_class(file_path):\n    return tf.strings.split(file_path, os.path.sep)[-2]\n\n\ndef get_label(file_path):\n    img_class = get_class(file_path)\n    all_classes = ['Black Sea Sprat', 'Gilt-Head Bream', 'Hourse Mackerel', 'Red Mullet', 'Red Sea Bream', 'Sea Bass', 'Shrimp', 'Striped Red Mullet', 'Trout']\n    label = tf.convert_to_tensor(list(map((lambda x: int(x == img_class)), all_classes)))\n    return label\n\ndef process_image(file_path):\n    label = get_label(file_path)\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img)\n    # img = tf.keras.applications.vgg16.preprocess_input(img)\n    img = tf.image.resize(img, IMG_SIZE)\n    img = tf.cast(img, tf.float64) \/ 255.0\n    return img, label\n\n\ndef load_dataset(file_path):\n    image_dataset = tf.data.Dataset.list_files(file_path, shuffle = True)\n    split = int(len(image_dataset) * VAL_SPLIT)\n\n    valid_dataset = image_dataset.take(split)\n    train_dataset = image_dataset.skip(split)\n\n    train_dataset = train_dataset.filter(lambda x: tf.strings.split(get_class(x), ' ')[-1] != 'GT')\n    train_dataset = train_dataset.map(process_image)\n\n    valid_dataset = valid_dataset.filter(lambda x: tf.strings.split(get_class(x), ' ')[-1] != 'GT')\n    valid_dataset = valid_dataset.map(process_image)\n    return train_dataset, valid_dataset\n\n\ndef get_batched_dataset(dataset):\n    dataset = dataset.shuffle(BUFFER_SIZE)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder = False)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)                                # Fetch next batch of data from drive while training current batch\n    dataset = dataset.cache()                                                   # Keep loaded batches in memory during the training.\n    return dataset\n\n\ndef get_train_val_data(file_path):\n    dataset = load_dataset(file_path)\n    train = get_batched_dataset(dataset[0])\n    val = get_batched_dataset(dataset[1])\n    return train, val","47778f19":"train_data, valid_data = get_train_val_data(train_path)","7a1e8734":"images = None\nlabels = None\nfor img, label in train_data.take(1):                           # Returns a tuple of 1 batch of images and their labels.\n    images = img\n    labels = label","bc126c1f":"labels.shape","21597145":"images.shape","578f7cb7":"fig, axes = plt.subplots(4, 4, figsize = (20, 20))\naxarr = axes.flat\n\nfor i, axis in enumerate(axarr):\n  axis.imshow(images.numpy()[i, :, :, :])\n  axis.set_title(CLASSES[np.argmax(labels.numpy()[i])])\n  axis.axis('off')\n\nplt.subplots_adjust(\n    left=0.1,\n    bottom = 0.1,\n    right=0.9,\n    top = 0.9,\n    wspace=0.2,\n    hspace = 0.2    \n)","84ac9a8b":"# Define model\ndef MyModel():\n    transfer_model = tf.keras.applications.vgg16.VGG16(include_top=False, input_shape=(224, 224, 3))\n\n    base_model = tf.keras.models.Model(\n        inputs = transfer_model.input,\n        outputs = transfer_model.output\n    )\n    for layer in base_model.layers[:-3]:\n        layer.trainable = False\n\n    model = tf.keras.Sequential(layers = [layer for layer in base_model.layers])\n\n    model.add(tf.keras.layers.GlobalAveragePooling2D())\n    model.add(tf.keras.layers.Dense(9, activation = 'softmax', name = 'classifier'))\n\n    return model","5668e18d":"with strategy.scope():                          # Use all 8 cores of the TPU for training\n\n    model = MyModel()\n\n    loss_object = tf.keras.losses.CategoricalCrossentropy(\n        reduction = tf.keras.losses.Reduction.NONE\n    )\n\n    def compute_loss(labels, predictions):       \n        per_example_loss = loss_object(labels, predictions)\n        return tf.nn.compute_average_loss(per_example_loss, global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync)\n\n    test_loss = tf.keras.metrics.Mean(name = 'test_loss')\n\n    train_accuracy = tf.keras.metrics.CategoricalAccuracy(name = 'train_accuracy')\n    test_accuracy = tf.keras.metrics.CategoricalAccuracy(name = 'test_accuracy')\n\n    lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate = 0.001,\n        decay_steps = 50,\n        decay_rate = 0.95\n    )\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n\n    @tf.function                                                                # Converts the function in graph mode for faster execution\n    def distributed_training_step(datasets_inputs):\n        per_replica_losses = strategy.run(train_steps, args = (datasets_inputs, ))\n        print(per_replica_losses)\n        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis = None)\n\n    @tf.function\n    def distributed_test_step(datasets_inputs):\n        strategy.run(test_steps, args = (datasets_inputs, ))\n\n    def train_steps(inputs):\n        images, labels = inputs\n\n        # Shape of images here -> [128, 224, 224, 3]  with 128 being batch size.\n        # Shape of Labels here -> (128, 9) representing 9 classes.\n\n        with tf.GradientTape() as tape:                                         # To keep in check gradients of weights and biases(kernel)\n            predictions = model(images)\n            loss = compute_loss(labels, predictions)\n\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))    # Applying changes in weights and biases.\n\n        train_accuracy.update_state(labels, predictions)\n\n        return loss\n\n    def test_steps(inputs):\n        images, labels = inputs\n\n        predictions = model(images)\n        loss = loss_object(labels, predictions)\n\n        test_loss.update_state(loss)\n        test_accuracy.update_state(labels, predictions)","2a27ea9b":"model.summary()","7a4bafc9":"EPOCHS = 10\nwith strategy.scope():\n    for epoch in range(EPOCHS):\n        epoch_start = datetime.now()\n        total_loss = 0.0\n        num_batches = 0\n\n        # Training Loop\n        for x in train_data: \n            total_loss += distributed_training_step(x)\n            num_batches += 1\n\n        train_loss = total_loss \/ num_batches\n\n        # Testing Loop\n        for x in valid_data:\n            distributed_test_step(x)\n\n        epoch_end = datetime.now()\n        \n        template = (\"Epoch {}, Loss: {:.2f}, Accuracy: {:.2f}, Test Loss: {:.2f}, Test Accuracy: {:.2f}, \\t Elapsed Time: {}\")\n\n        print(template.format(\n            epoch + 1,\n            train_loss,\n            train_accuracy.result() * 100,\n            test_loss.result() \/ strategy.num_replicas_in_sync,\n            test_accuracy.result() * 100,\n            (epoch_end - epoch_start).seconds\n        ))\n\n        test_loss.reset_states()\n        train_accuracy.reset_states()\n        test_accuracy.reset_states()","9a2123a0":"# model.save('Transfer_model-VGG16.h5')","96b8aca8":"gap_weights = model.layers[-1].get_weights()[0]","e5778fcb":"# A new model is created via subclassing to obtain the features extracted in final convolutional layer and the results as well.\n# In our model, 3rd last layer is the final layer containing the image after applying filters.\n\nfeature_extractor = tf.keras.Model(\n    inputs = model.input,\n    outputs = (model.layers[-3].output, model.layers[-1].output)\n)","06743ecb":"features, results = feature_extractor(images)\nfeatures.shape\n# Features contain outputs of 128(BATCH SIZE) images from the final convolutional layer.\n# This should give a output of shape (128, 7, 7, 512) as observed in summary.","c355b3ee":"def show_cam(image_index):\n    plt.figure(figsize = (8, 8))\n    features_for_img = features[image_index, :, :, :]\n    prediction = np.argmax(results[image_index])\n\n    class_activation_weights = gap_weights[:, prediction]\n    class_activation_features = sp.ndimage.zoom(features_for_img, (224\/7, 224\/7, 1), order = 2)\n    cam_output = np.dot(class_activation_features, class_activation_weights)\n\n    print(\"Predicted Class = \" + str(CLASSES[prediction]) + ', Probability = ' + str(results[image_index][prediction]))\n    plt.imshow(images[image_index])\n\n    if results[image_index][prediction] > 0.95:\n        cmap_str = \"PiYG_r\"\n    else:\n        cmap_str = \"Reds\"\n    \n    plt.imshow(cam_output, cmap = cmap_str, alpha = 0.7)\n    plt.show()\n\ndesired_class = 3                               # Randomly choosing a class\ncount = 0\nfor i in range(0, 128):                         # Batch size    \n    if np.argmax(results[i]) == desired_class:\n        show_cam(i)\n        count += 1\n        if count == 4:\n            break\n","7ab1f851":"# Class Activation Mapping","951dc6a7":"# Importing Required Libraries","d8b4ee15":"# Obsevations From Class Activation Map\n    -> The pixels with higher intensity of magenta colour had higher impact in classifying the image.\n    -> We can observe that the eyes of the fishes were the center of attention","345433b9":"# Visualization Of Fishes","56a36a86":"# Loading Data as Tensorflow Dataset","503c3ecc":"## Show Class Activation Map Steps For 1 image:-\n    -> We Obtain 512x(7, 7) images from features in which each filter contains different features of different fishes\n    -> These are converted to 512x(224, 224) images to make it comparable with original image.\n    -> We get a dot product of these filters with the corresponding class activation weights.\n    -> As we know, only a few filters of those 64 would be having suitable features for classifying an image to a particular class. And these filters have higher class activation weights.\n    ->Now, all these filters are combined to get a final class activation map.\n    -> The original image, and class activation map are superimposed to get a final output.","3965ba86":"## Save Model For Future Use and Evaluation","8dc26431":"# Model Training","de723111":"### TPUs on Kaggle deny to load dataset on local device, rather accept data stored in Google Cloud Storage.\n### GCS PATH of a public dataset on Kaggle can be obtained by running the code below in Kaggle Notebook opened using a dataset.\n\n GCS_PATH = KaggleDatasets().get_gcs_path()","34cbdc5c":"# Observations\n    -> Distributed Training is way faster in training the model.\n    -> A similar model takes around 3 to 4 hours to train without distributed training.\n\n    -> The first epoch took around 1 hour which was very high compared to upcoming epochs\n    -> This was due to time taken while importing the batches of training set.\n    -> Subsequent epochs would take the same time as first epoch, if \"Prefetch\" and \"Cache\" is not implemented in training dataset.","0d3ed5fb":"# Setting Environment For Distributed Training On Kaggle TPU"}}