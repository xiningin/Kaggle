{"cell_type":{"847caeef":"code","6f41893f":"code","f1c9add8":"code","3864cad8":"code","5dde9bc4":"code","f01f4d2d":"code","ce941282":"code","9924e677":"code","8cd737f5":"code","83213314":"code","681cad37":"code","c0a2eaa1":"code","9fe02e1c":"code","723f51c4":"code","d6f62744":"code","95a5ef38":"code","f0378aa2":"code","395172bb":"code","787605cc":"code","988ca6da":"markdown"},"source":{"847caeef":"import pandas as pd\nimport numpy as np\nimport gresearch_crypto\nimport xgboost as xgb\nimport traceback\nimport datetime\n\n\nTRAIN_CSV = '\/kaggle\/input\/g-research-crypto-forecasting\/train.csv'\nASSET_DETAILS_CSV = '\/kaggle\/input\/g-research-crypto-forecasting\/asset_details.csv'\n\ndf = pd.read_csv(TRAIN_CSV)\ndf_asset_details = pd.read_csv(ASSET_DETAILS_CSV).sort_values(\"Asset_ID\")\n\nasset_to_weight = df_asset_details.Weight.values\ndf[\"Weight\"] = df[\"Asset_ID\"].apply(lambda x: asset_to_weight[x])","6f41893f":"def clean(df):\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df.dropna(how=\"any\", inplace=True)\n\ndef test_train_split(df):\n    X_train = df[df['timestamp'] <= 1623542400].drop('Target', axis=1)\n    y_train = df[df['timestamp'] <= 1623542400].Target\n    X_test = df[df['timestamp'] > 1623542400].iloc[:-1].drop('Target', axis=1)\n    y_test = df[df['timestamp'] > 1623542400].iloc[:-1].Target\n    return X_train, y_train, X_test, y_test\n\nclean(df)\nX_train, y_train, X_test, y_test = test_train_split(df)","f1c9add8":"# NOT USED, I'll use it later though <3\nfrom pandas import DataFrame\nfrom pandas import concat\n \ndef time_lag(data, n_in=1, n_out=1, dropnan=True, interpolate = False):\n    \"\"\"\n    Frame a time series as a supervised learning dataset.\n    Arguments:\n        data: Sequence of observations as a list or NumPy array.\n        n_in: Number of lag observations as input (X).\n        n_out: Number of observations as output (y).\n        dropnan: Boolean whether or not to drop rows with NaN values.\n    Returns:\n        Pandas DataFrame of series framed for supervised learning.\n    \"\"\"\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # put it all together\n    agg = concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    if interpolate:\n        agg.fillna(method='bfill', inplace=True)\n    return agg\n ","3864cad8":"def process(df):\n    df.loc[df[\"VWAP\"] <= 0, \"VWAP\"] = df.loc[df[\"VWAP\"] <= 0, \"Close\"]\n    df[\"Unity\"] = 1\n    df[\"Open\"] = np.log(df[\"Open\"])\n    df[\"High\"] = np.log(df[\"High\"])\n    df[\"Low\"] = np.log(df[\"Low\"])\n    df[\"Close\"] = np.log(df[\"Close\"])\n    df[\"VWAP\"] = np.log(df[\"VWAP\"])\n    #df['log_ret'] = np.log(df.Close\/df.Open).fillna(0)\n    \n    #df['weird_feature'] = -(df['log_ret'] - (df['Weight'] * df['log_ret']).sum() \/ df['Weight'].sum())\n    \n    #norm_cols = ['Open','VWAP']\n    #ref = \"Close\"\n    #for col in norm_cols:\n    #    df[\"norm_\" + col] = df[col] \/ df[ref]\n    \n    #return df\n    #return pd.concat([df, time_lag(df[[\"VWAP\", \"Volume\", \"Open\", \"Close\"]], n_in=1, n_out=0, dropnan=False, interpolate=True)], axis=1)\n\n#X_train = \nprocess(X_train)\n#X_test = \nprocess(X_test)","5dde9bc4":"X_train.head()","f01f4d2d":"def f1(rows):\n    return (rows[\"Weight\"]*rows[\"Close\"]).sum()\/rows[\"Weight\"].sum()\ndef f2(rows):\n    return (rows[\"Weight\"]*rows[\"Open\"]).sum()\/rows[\"Weight\"].sum()\n\nX_test[\"mClose\"] = X_test.timestamp.map(X_test.groupby(\"timestamp\").apply(f1))\nX_test[\"mOpen\"] = X_test.timestamp.map(X_test.groupby(\"timestamp\").apply(f2))","ce941282":"X_train[\"mClose\"] = X_train.timestamp.map(X_train.groupby(\"timestamp\").apply(f1))\nX_train[\"mOpen\"] = X_train.timestamp.map(X_train.groupby(\"timestamp\").apply(f2))","9924e677":"#(X_test[\"mClose\"] - X_test[\"mOpen\"]).head(30)","8cd737f5":"# NOT USED\n# Two new features from the competition tutorial\ndef upper_shadow(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\n\ndef lower_shadow(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']\n\ndef generate_features(df, lag = 1, shuffle = False):\n    df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].copy()\n    \n    #df_feat.fillna(-999,inplace=True)\n    if lag > 0:\n        df_feat = series_to_supervised(df_feat, n_in = lag)\n    df_feat['Upper_Shadow'] = upper_shadow(df)\n    df_feat['Lower_Shadow'] = lower_shadow(df)\n    if shuffle is True:\n        df_feat = df_feat.sample(frac=1)\n    return df_feat","83213314":"X_train.columns","681cad37":"from sklearn.preprocessing import StandardScaler\n\n\nclass BestModel:\n    def __init__(self):\n        self.beta = None\n        self.scaler = StandardScaler()\n    \n    def fit(self, X_train, y_train):\n        #self.scaler.fit(X_train)\n        #X = self.scaler.transform(X_train)\n        X = X_train.values\n        k = 0.5\n        mat = X.T@X\n        mat += k*np.identity(len(mat))\n        self.beta = np.linalg.inv(mat)@X.T@y_train.values\n        \n    def predict(self, X_test):\n        #X = self.scaler.transform(X_test)\n        X = X_test.values\n        return X@self.beta","c0a2eaa1":"features = [\"Count\", \"Open\", \"Close\", \"High\", \"Low\", \"Volume\", \"VWAP\", \"Unity\", \"mClose\", \"mOpen\"]#, 'weird_feature']\nprint(X_train.columns)","9fe02e1c":"models = [BestModel() for _ in range(len(df_asset_details))]\n\ny_pred = pd.Series(data=np.full_like(y_test.values, np.nan), index=y_test.index)\nfor asset_ID, model in enumerate(models):\n    X_asset_train = X_train[X_train.Asset_ID == asset_ID]\n    y_asset_train = y_train[X_train.Asset_ID == asset_ID]\n    X_asset_test = X_test[X_test.Asset_ID == asset_ID]\n    \n    model.fit(X_asset_train[features], y_asset_train)\n    y_pred[X_test.Asset_ID == asset_ID] = model.predict(X_asset_test[features])\n    print(f\"Trained model for asset {asset_ID}\")","723f51c4":"def corr(a, b, w):\n    cov = lambda x, y: np.sum(w * (x - np.average(x, weights=w)) * (y - np.average(y, weights=w))) \/ np.sum(w)\n    return cov(a, b) \/ np.sqrt(cov(a, a) * cov(b, b))\n\nalt_weight = np.ones_like(y_pred)\nR = corr(y_pred, y_test.values, X_test.Weight)\nprint(f\"{R:.5f}\")\n# 0.01641 shenanigans\n# 0.01523 remove two\n#0.01464 all\n# 0.01519 - log\n#0.01784587806310679\n\n#0.01992 - 0.01\n#0.02151 - 0.5\n# 0.02205 - 1.0\n# 0.02235 - 2.0\n#0.02081 - 0.2","d6f62744":"from scipy.stats import pearsonr\nprint(pearsonr(y_pred, y_test))","95a5ef38":"for i in range(len(models[0].beta)):\n    print(f\"{features[i]}: {models[0].beta[i]\/np.sum(np.abs(models[0].beta)):.8f}\")","f0378aa2":"import matplotlib.pyplot as plt\n\nplt.plot(np.abs(models[0].beta))\nplt.ylim(bottom=0)","395172bb":"env = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\nfor i, (df_test, df_pred) in enumerate(iter_test):\n    df_pred['Target'] = np.nan\n    \n    df_test[\"Weight\"] = df_test[\"Asset_ID\"].apply(lambda x: asset_to_weight[x])\n    process(df_test)\n    df_test[\"mClose\"] = (df_test['Weight'] * df_test['Close']).sum() \/ df_test['Weight'].sum()\n    df_test[\"mOpen\"] = (df_test['Weight'] * df_test['Open']).sum() \/ df_test['Weight'].sum()\n    \n    for asset_ID, model in enumerate(models):\n        X_asset_test = df_test[df_test.Asset_ID == asset_ID]\n        df_pred.loc[df_test.Asset_ID == asset_ID, 'Target'] = model.predict(X_asset_test[features])\n    df_pred['Target'] = df_pred['Target'].interpolate('nearest')\n    env.predict(df_pred)","787605cc":"print(\"oh yes!\")","988ca6da":"# Predict & submit\n\nReferences: [Detailed API Introduction](https:\/\/www.kaggle.com\/sohier\/detailed-api-introduction)\n\nSomething that helped me understand this iterator was adding a pdb checkpoint inside of the for loop:\n\n```python\nimport pdb; pdb.set_trace()\n```\n\nSee [Python Debugging With Pdb](https:\/\/realpython.com\/python-debugging-pdb\/) if you want to use it and you don't know how to.\n"}}