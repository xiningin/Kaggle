{"cell_type":{"db20c2c2":"code","701a784e":"code","bf18e1ff":"code","8deb98b5":"code","a961f8b1":"code","794f845d":"code","28ecd250":"code","b0549165":"code","fdf32d1e":"code","338d9540":"code","1a2f1b43":"code","d00f4bda":"code","358a481c":"code","210542ac":"code","f2549cca":"code","f1f7936f":"code","c1762513":"code","5e6b0323":"code","972e9ca8":"code","e5597474":"code","e422ce8a":"markdown","a465674d":"markdown","aa51949d":"markdown","55134f27":"markdown","ad02840d":"markdown","e185ebaf":"markdown","31049a84":"markdown","9915ad93":"markdown","f6b2d1ce":"markdown"},"source":{"db20c2c2":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","701a784e":"# load into dataframe and look at the first few rows \ndata = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","bf18e1ff":"# print dataframe information, particularly data type and count, if there are any missing entries which turns out to be none\ndata.info()","8deb98b5":"# Check the target incidence\nprint(data.Class.value_counts(normalize=True).round(3))\n\n# Plot it out \ndata.Class.value_counts().sort_index().plot(kind='bar')\nplt.title(\"Fraud class histogram\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\");","a961f8b1":"# Segregate features and labels into separate variables, drop Loan ID (index 0) \nX = data.iloc[:, 0:30].values\ny = data.iloc[:, 30].values","794f845d":"# split data into train and test sets with sklearn\nfrom sklearn.model_selection import train_test_split\n\n# split into train\/test sets with same class ratio\ntrainX, testX, trainy, testy = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n\n# summarize dataset\nprint('Dataset: Class0=%d, Class1=%d' % (len(y[y==0]), len(y[y==1])))\nprint('Train: Class0=%d, Class1=%d' % (len(trainy[trainy==0]), len(trainy[trainy==1])))\nprint('Test: Class0=%d, Class1=%d' % (len(testy[testy==0]), len(testy[testy==1])))","28ecd250":"# data normalization with sklearn\nfrom sklearn.preprocessing import MinMaxScaler\n\n# instantiate MinMaxScaler and use it to rescale X_train and test \nscaler = MinMaxScaler(feature_range=(0, 1))\ntrainX_norm = scaler.fit_transform(trainX)\ntestX_norm = scaler.fit_transform(testX)","b0549165":"trainX_norm[1]","fdf32d1e":"# Load libraries for modeling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","338d9540":"# plot no skill and model roc curves\ndef plot_roc_curve(test_y, naive_probs, model_probs):\n\t# plot naive skill roc curve\n\tfpr, tpr, _ = roc_curve(test_y, naive_probs)\n\tplt.plot(fpr, tpr, linestyle='--', label='No Skill')\n\t# plot model roc curve\n\tfpr, tpr, _ = roc_curve(test_y, model_probs)\n\tplt.plot(fpr, tpr, marker='.', label='Logistic')\n\t# axis labels\n\tplt.xlabel('False Positive Rate')\n\tplt.ylabel('True Positive Rate')\n\t# show the legend\n\tplt.legend()\n\t# show the plot\n\tplt.show()","1a2f1b43":"# no skill model, stratified random class predictions\nmodel = DummyClassifier(strategy='stratified')\nmodel.fit(trainX_norm, trainy)\n\n# predict probabilities for each class of the label y \nyhat = model.predict_proba(testX_norm)\n\n# retrieve just the probabilities for the positive class\nnaive_probs = yhat[:, 1]\n\n# calculate roc auc\nroc_auc = roc_auc_score(testy, naive_probs)\nprint('No Skill ROC AUC %.3f' % roc_auc)","d00f4bda":"# train and fit a skilled model with Logistic Regression\nmodel = LogisticRegression(solver='lbfgs')\nmodel.fit(trainX_norm, trainy)\n\nyhat = model.predict_proba(testX_norm)\nmodel_probs = yhat[:, 1]\n\n# calculate roc auc\nroc_auc = roc_auc_score(testy, model_probs)\nprint('Logistic ROC AUC %.3f' % roc_auc)\n\n# plot roc curves\nplot_roc_curve(testy, naive_probs, model_probs)","358a481c":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc","210542ac":"def plot_pr_curve(test_y, model_probs):\n\t# calculate the no skill line as the proportion of the positive class\n\tno_skill = len(test_y[test_y==1]) \/ len(test_y)\n\t# plot the no skill precision-recall curve\n\tplt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n\t# plot model precision-recall curve\n\tprecision, recall, _ = precision_recall_curve(testy, model_probs)\n\tplt.plot(recall, precision, marker='.', label='Logistic')\n\t# axis labels\n\tplt.xlabel('Recall')\n\tplt.ylabel('Precision')\n\t# show the legend\n\tplt.legend()\n\t# show the plot\n\tplt.show()","f2549cca":"# calculate the precision-recall auc\nprecision, recall, _ = precision_recall_curve(testy, naive_probs)\nauc_score = auc(recall, precision)\nprint('No Skill PR AUC: %.3f' % auc_score)","f1f7936f":"# calculate the precision-recall auc\nprecision, recall, _ = precision_recall_curve(testy, model_probs)\nauc_score = auc(recall, precision)\nprint('Logistic PR AUC using normalized data: %.3f' % auc_score)\n\n# plot precision-recall curves\nplot_pr_curve(testy, model_probs)","c1762513":"# check the distribution of the first 6 features\n\nfrom scipy.stats import norm\n\nf, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15, 6))\n\nv1_fraud_dist = data['V1'].loc[data['Class'] == 1].values\nsns.distplot(v1_fraud_dist,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('V1 Distribution', fontsize=14)\n\nv2_fraud_dist = data['V2'].loc[data['Class'] == 1].values\nsns.distplot(v2_fraud_dist,ax=ax2, fit=norm, color='#FB8861')\nax2.set_title('V2 Distribution', fontsize=14)\n\nv3_fraud_dist = data['V3'].loc[data['Class'] == 1].values\nsns.distplot(v3_fraud_dist,ax=ax3, fit=norm, color='#FB8861')\nax3.set_title('V3 Distribution', fontsize=14)\n\nv4_fraud_dist = data['V4'].loc[data['Class'] == 1].values\nsns.distplot(v4_fraud_dist,ax=ax4, fit=norm, color='#FB8861')\nax4.set_title('V4 Distribution', fontsize=14)\n\nv5_fraud_dist = data['V5'].loc[data['Class'] == 1].values\nsns.distplot(v5_fraud_dist,ax=ax5, fit=norm, color='#FB8861')\nax5.set_title('V5 Distribution', fontsize=14)\n\nv6_fraud_dist = data['V6'].loc[data['Class'] == 1].values\nsns.distplot(v6_fraud_dist,ax=ax6, fit=norm, color='#FB8861')\nax6.set_title('V6 Distribution', fontsize=14)\n\nplt.show()","5e6b0323":"# check if there are outliers in the features \n\nlist = [1,2,3,4,5,6]\n\nfor i in list:\n    ax = sns.boxplot(x=data.iloc[i])\n    plt.show();","972e9ca8":"# data standardization with sklearn\nfrom sklearn.preprocessing import StandardScaler\n\n# Instantiate a StandardScaler \nscale = StandardScaler()\ntrainX_stand = scale.fit_transform(trainX)\ntestX_stand = scale.fit_transform(testX)","e5597474":"# train and fit a skilled model with Logistic Regression, this time using trainX_stand\nmodel1 = LogisticRegression(solver='lbfgs')\nmodel1.fit(trainX_stand, trainy)\n\nyhat1 = model1.predict_proba(testX_stand)\nmodel1_probs = yhat1[:, 1]\n\n# calculate the precision-recall auc\nprecision, recall, _ = precision_recall_curve(testy, model1_probs)\nauc_score = auc(recall, precision)\nprint('Logistic PR AUC using standardized data: %.3f' % auc_score)\n\n# plot precision-recall curves\nplot_pr_curve(testy, model1_probs)","e422ce8a":"Indeed, this is a severely imbalanced data set with 0.002% being the positive class! ","a465674d":"At this point, we have achieved a decent score. However, for comparison, I want to see the results using standardization instead of normalization on the data. For knowledge:\n\n**Normalization** shifts and rescales the values so that they end up ranging between 0 and 1. It is also known as Min-Max scaling. \n\n**Standardization** centers all the values around the mean with a unit standard deviation i.e. mean of the attribute becomes 0 and the final distribution has a unit standard deviation.\n\nAccording to this very helpful [website](https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/feature-scaling-machine-learning-normalization-standardization\/), normalization is good to use when the distribution of the data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like KNN and Neural Networks. On the other hand, standardization is good to use in cases where the data follows a Gaussian distribution. Also, unlike normalization, standardization does not have a bounding range. So, if there are outliers (there are in this data set), they will not be affected by standardization.\n\nTherefore, I did a quick and dirty check of the distribution of the first 6 features: it seems to be roughly normally distributed based on their histograms below. In this case, I would expect standardization to work better...","aa51949d":"**Precision-Recall Curves and AUC**\n\nWe can look at the common alternative as recommended by the author: **precision-recall curve and AUC**.\n\n**Precision** is a metric that quantifies the number of correct positive predictions made.\n**Recall** is a metric that quantifies the number of correct positive predictions made out of all positive predictions that could have been made.\n\nY-axis: Precision = TruePositives \/ (TruePositives + FalsePositives)\n\nX-axis: Recall = TruePositives \/ (TruePositives + FalseNegatives)\n\nA precision-recall curve is a plot of the precision and the recall for different probability thresholds - by focusing on the minority class, it is an effective diagnostic for imbalanced binary classification models. A model with perfect skill is depicted as a point at a coordinate of (1,1).\n\nSimilarly, a PR AUC summarizes the curve with a range of threshold values as a single score.\n\n\n","55134f27":"# Conclusion\n* It is important to investigate the target class incidence before proceeding with any modeling due to the potential skew and bias that real-life data may have \n* Data preprocessing is also a critical step to make sure that our features contribute equally to the result, not just because they have a high absolute number e.g. the Amount feature in our data set \n* Try both standardization and normalization for data scaling - the choice depends on the feature distribution as well as presence of outliers \n* For severely imbalanced classification problems, use Precision-Recall curve instead of ROC curve, which predicts both the majority and minority class, falsely giving us a high \n* **Our final best model of logistic regression using standardized features resulted in a PR AUC of 0.812**\n\nNext Possible Steps: \n* Remove outliers \n* Other modeling algorithms \n\nThanks for checking out my attempt - I welcome any suggestions and feedback for improvement. ","ad02840d":"We will use Logistic Regression model as the model of choice, because the predicted probabilities are well-calibrated, as opposed to other ML models that are not developed using a probabilistic model. For example for SVM, one needs to calibrate the probabilities before using it. ","e185ebaf":"# **1. Load and Explore Data**","31049a84":"# **2. Preprocess the Data**","9915ad93":"# **3. Train and Predict**\n\nWhile the data set has recommended precision-recall curve for assessment, I wanted to compare the difference and practice ROC as well. **ROC curve** is a plot that summarizes the performance of a binary classification model on the positive class.\n\nX-axis: FalsePositiveRate = FalsePositives \/ (FalsePositives + TrueNegatives)\n\nY-axis: TruePositiveRate = TruePositives \/ (TruePositives + False Negatives)\n\n\nROC curve tells us the *fraction of correct predictions for the positive class* (y-axis) versus the *fraction of errors for the negative class* (x-axis). Thus the best possible classifier that achieves perfect skill is the top-left of the plot (coordinate 0,1).\n\nBut it is difficult to compare classifier simply by looking at curves, hence we turn to **AUC ROC (area under the curve)**. It can be interpreted as the probability that the scores given by a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. The score is a value between 0.0 and 1.0 for a perfect classifier. This is the most commonly used metric for comparing binary classification models for imbalanced problems.","f6b2d1ce":"This seems like a great model, when actually it is just predicting correctly the negative class because it is the majority of the data set, and doesn't prove its true predictive ability. For an imbalanced classification with a severe skew and few minority classes, ROC AUC is misleading. "}}