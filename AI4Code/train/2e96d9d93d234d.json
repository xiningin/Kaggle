{"cell_type":{"f8a7b0d1":"code","4da33e9f":"code","a21ec131":"code","ae112b19":"code","d17ed41f":"code","629e48eb":"code","c7bc7c31":"code","9b1296ed":"code","e022fc7d":"code","82fe59f8":"code","23a4ee9f":"code","1a911815":"code","c3e710c7":"code","88eaf29b":"code","56b77116":"code","eae4873e":"code","8becba1e":"code","b491d283":"code","1fb93c23":"code","cdadd96e":"code","7de56066":"code","b5d1aeb2":"code","d7a202b8":"code","2b69364c":"code","7760a413":"code","6a13baaa":"code","b1ae5915":"code","36fa80e4":"code","ee438553":"code","bb833103":"code","9e1e1c63":"code","0aa4ce57":"code","cb11caf0":"code","977797e4":"code","c48ad3aa":"code","0d5b7cfe":"code","e286316c":"code","d5181e3d":"code","a81e73ba":"markdown","33689210":"markdown","3135eee5":"markdown","ae62df21":"markdown","a936df4e":"markdown","7ffa0bae":"markdown","09d8ff5e":"markdown","e0c46cbb":"markdown","ee05a984":"markdown","ce9815d5":"markdown","3cdf2419":"markdown","9baf521c":"markdown","c6414147":"markdown","b616045d":"markdown"},"source":{"f8a7b0d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport cv2\nimport os\nimport random\nimport torch\nfrom torch.utils.data import Dataset, DataLoader  \nfrom albumentations import RandomCrop, HorizontalFlip, CenterCrop, Compose, Normalize\nfrom albumentations.pytorch.transforms import ToTensor\nfrom sklearn.metrics import accuracy_score\n\nsns.set_style('darkgrid')\nplt.style.use('fivethirtyeight')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nseed = 69\nrandom.seed(seed)\nos.environ[\"PYTHONHASHSEED\"] = str(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\n\n\nprint(os.listdir('..\/input\/stanford-dogs-dataset'))\n\n# Any results you write to the current directory are saved as output.","4da33e9f":"annot_path = '..\/input\/stanford-dogs-dataset\/annotations\/Annotation'\nimage_path = '..\/input\/stanford-dogs-dataset\/images\/Images'","a21ec131":"print('Length of annotations :', len(os.listdir(annot_path)))\nprint('Length of image classes :', len(os.listdir(image_path)))\nif len(os.listdir(annot_path)) == len(os.listdir(image_path)):\n    print('Number of unique annotations matches the number of classes')\nelse:\n    print(\"Number of unique annotations doesn't the number of classes\")\n","ae112b19":"valid = []\n\nfor element in os.listdir(image_path):\n    breed = element.split('-')[1]\n    images = len(os.listdir(os.path.join(image_path,element)))\n    valid.append((breed, images))","d17ed41f":"df = pd.DataFrame(valid, columns = ['Breeds', 'Number of images'])\nprint('Total number of images :', df['Number of images'].sum())","629e48eb":"df.head()","c7bc7c31":"plt.figure(figsize=(15,8))\nplt.title('Number of images of different breeds in descending order')\nsns.barplot(x = 'Number of images', y = 'Breeds', data = df.sort_values('Number of images', ascending = False).head(30))","9b1296ed":"# We select only the top 5 breeds having most number of images for classification\n\nsubset = df.sort_values('Number of images', ascending = False)['Breeds'][:5].values.tolist()","e022fc7d":"subset","82fe59f8":"valid = []\n\nfor element in os.listdir(image_path):\n    if element.split('-')[1] in subset:\n        for img_id in os.listdir(os.path.join(image_path, element)):\n            path = os.path.join(element, img_id)\n            label = element.split('-')[1]\n            valid.append((path, label))\n        \ndf = pd.DataFrame(valid, columns = ['Path', 'Label'])\nprint('Shape of dataframe :', df.shape)","23a4ee9f":"# Dataset with the suffix path and label\n\ndf.head()","1a911815":"normalise = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])","c3e710c7":"def aug1():\n    return Compose([RandomCrop(height = 224, width = 224, p = 1.0), HorizontalFlip(p = 0.5),  ToTensor(normalize = normalise)], p = 1)","88eaf29b":"# Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p = 1, max_pixel_value = 1.0)","56b77116":"def aug2():\n    return Compose([ToTensor(normalize = normalise)], p = 1)","eae4873e":"train_aug = aug1()\nvalid_aug = aug2()","8becba1e":"def PCAColorAug(image, category = 'Tensor'):\n    if type(image) == torch.Tensor:\n        image = image.numpy()\n        image = np.moveaxis(image, 0, 2)\n    \n    \n    img_reshaped = image.reshape(-1, 3).astype('float32')\n    mean, std = np.mean(img_reshaped, 0), np.std(img_reshaped, 0)\n    img_rescaled = (img_reshaped - mean)\/std\n    cov_matrix = np.cov(img_rescaled, rowvar = False) # Covariant matrix of reshaped image.  Output is 3*3 matrix\n    eigen_val, eigen_vec = np.linalg.eig(cov_matrix) # Compute Eigen Values and Eigen Vectors of the covariant matrix. eigen_vec is 3*3 matrix with eigen vectors as column. \n    alphas = np.random.normal(loc = 0, scale = 0.1, size = 3)\n    vec1 = alphas*eigen_val\n    valid = np.dot(eigen_vec, vec1) # Matrix multiplication\n    pca_aug_norm_image = img_rescaled + valid\n    pca_aug_image = pca_aug_norm_image*std + mean\n    aug_image = np.maximum(np.minimum(pca_aug_image, 255), 0).astype('uint8')\n    if category == 'Tensor':\n        return torch.from_numpy(aug_image.reshape(3,256,256))\n    else:\n        return aug_image.reshape(256,256,3)\n    ","b491d283":"labels = pd.get_dummies(df['Label'])","1fb93c23":"import torchvision.transforms as transforms\n\nclass StanfordDogs(Dataset):\n    def __init__(self, transform1, transform2, X, Y, objective = 'train'):\n        self.X = X\n        self.Y = Y\n        self.train_transform = transform1\n        self.valid_transform = transform2\n        self.objective = objective\n        \n    def __getitem__(self, idx):\n        path = self.X['Path'][idx]\n        label = self.Y.iloc[idx, :].values\n        img = cv2.imread(os.path.join(image_path, path))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Shortest side of image is scaled to 256 pixels and the other side is scaled so as to maintain aspect ratio\n        \n        h, w, _ = img.shape\n        \n        if h <= w:\n            aspect_ratio = w\/h\n            dim = (256, int(256*aspect_ratio))\n            img = cv2.resize(img, dim)\n        else:\n            aspect_ratio = h\/w\n            dim = (int(256*aspect_ratio), 256)\n            img = cv2.resize(img, dim)\n\n           \n        img = CenterCrop(height = 256, width = 256, p = 1)(image = img)['image']\n        \n        if self.objective == 'train':\n            random = np.random.uniform(size = 1)\n            if random < 0.5:                            # PCA Augmentation carried out only 50 percent of time\n                img = PCAColorAug(img, category = 'numpy')\n                \n            augmented = self.train_transform(image = img)\n            img = augmented['image']\n            \n            return img, label\n        \n        elif ((self.objective == 'validation') |  (self.objective == 'test')):\n            img = cv2.resize(img, (224, 224))\n            augmented = self.valid_transform(image = img)\n            img = augmented['image']  \n            \n            return img, label\n        \n    \n    def __len__(self):\n        return len(self.X)\n        \n    ","cdadd96e":"from sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(df, labels, test_size = 0.25, random_state = 5, stratify = df['Label'])\ntrain_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size = 0.25, random_state = 5, stratify = train_X['Label'])","7de56066":"train_X.reset_index(drop = True, inplace = True)\nval_X.reset_index(drop = True, inplace = True)\ntest_X.reset_index(drop = True, inplace = True)\n\ntrain_y.reset_index(drop = True, inplace = True)\nval_y.reset_index(drop = True, inplace = True)\ntest_y.reset_index(drop = True, inplace = True)","b5d1aeb2":"image = cv2.imread(image_path + '\/n02105162-malinois\/n02105162_6625.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage1 = cv2.resize(image, (256,256))\naug_image = PCAColorAug(image1, category = 'numpy')\ncropped_img = CenterCrop(height = 256, width = 256, p = 1)(image = image)['image']\nimage2 = cv2.resize(cropped_img, (224,224))\nimage3 = train_aug(image = cropped_img)['image']\nimage3 = image3.numpy()\nimage3 = np.moveaxis(image3, 0, 2)\nimage3 = cv2.resize(image3, (224,224))\n\nplt.figure(figsize = (15, 10))\nplt.subplot(1,4,1)\nplt.imshow(image1)\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1,4,2)\nplt.imshow(aug_image)\nplt.title('PCA Color Augmentation')\nplt.axis('off')\n\nplt.subplot(1,4,3)\nplt.imshow(image2)\nplt.title('Centered Cropped Image')\nplt.axis('off')\n\nplt.subplot(1,4,4)\nplt.imshow(image3)\nplt.title('Training Augmentations')\nplt.axis('off')","d7a202b8":"BATCH_SIZE = 128\n\ndef loader(data_X, data_Y, batch_size = BATCH_SIZE, obj = 'train'):\n    data = StanfordDogs(train_aug, valid_aug, X = data_X, Y = data_Y, objective = obj)\n    loader = DataLoader(data, batch_size = BATCH_SIZE, shuffle = True)\n    \n    return loader","2b69364c":"train_loader = loader(train_X, train_y, batch_size = BATCH_SIZE)\nval_loader = loader(val_X, val_y, batch_size = BATCH_SIZE, obj = 'validation')","7760a413":"fig, ax = plt.subplots(3,3, figsize = (15,15))\n\nfor batch in train_loader:\n    img, label = batch\n    img = img.numpy()\n    img = np.moveaxis(img, 1, 3)\n    for i in range(9):\n        ax[i\/\/3, i%3].imshow(img[i, :, :, :])\n        ax[i\/\/3, i%3].set_title(subset[torch.max(label, 1)[1][i]])\n        ax[i\/\/3, i%3].axis('off')\n    break","6a13baaa":"fig, ax = plt.subplots(3,3, figsize = (15,15))\n\nfor batch in val_loader:\n    img, label = batch\n    img = img.numpy()\n    img = np.moveaxis(img, 1, 3)\n    for i in range(9):\n        ax[i\/\/3, i%3].imshow(img[i, :, :, :])\n        ax[i\/\/3, i%3].set_title(subset[torch.max(label, 1)[1][i]])\n        ax[i\/\/3, i%3].axis('off')\n    break","b1ae5915":"import torchvision.models as models\nimport torch\nimport torch.nn as nn\n\nalexnet = models.alexnet(pretrained = False)\nalexnet.load_state_dict(torch.load('..\/input\/alexnet\/alexnet.pth'))","36fa80e4":"alexnet.classifier[6] = nn.Linear(4096, 5) # 5 classes ","ee438553":"alexnet","bb833103":"from torch.optim import SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\noptimizer = SGD(alexnet.classifier.parameters(), lr = 0.01, momentum = 0.9, weight_decay = 0.0005) # We train only on classifier head. We keep the weights in feature head as it is.\nscheduler = ReduceLROnPlateau(optimizer, patience = 4, factor = 0.1, mode = 'min')\ncriterion = nn.CrossEntropyLoss()","9e1e1c63":"device = torch.device('cuda')","0aa4ce57":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']","cb11caf0":"epochs = 20\nlosses_df = {'train_loss' : [], 'val_loss' : [], 'val_acc' : []}\nnum_batches = len(train_X)\/\/BATCH_SIZE\nval_num_batches = len(val_X)\/\/BATCH_SIZE\nbest_loss = np.inf\n\n\nalexnet = alexnet.to(device)\n\nfor epoch in range(epochs):\n    print('Epoch :', epoch)\n    scheduler.step(epoch)\n    print('Learning rate :', get_lr(optimizer))\n    print('Training for epoch {}'.format(epoch))\n    val_outputs = []\n    loss = 0\n    vloss = 0\n    \n    alexnet.train()\n    for i, batch in enumerate(train_loader):\n        optimizer.zero_grad()\n        img, label = batch\n        img, label = img.to(device, dtype = torch.float), label.to(device, dtype = torch.long)\n        output = alexnet(img)\n        batch_loss = criterion(output, torch.max(label, 1)[1])\n        batch_loss.backward()\n        optimizer.step()\n        loss +=batch_loss.item()\n    \n    train_loss = loss\/num_batches\n    print('Training loss for epoch {} is {:.4f}'.format(epoch, train_loss))\n    losses_df['train_loss'].append(train_loss)\n    print('Validation for epoch {}'.format(epoch))\n    with torch.no_grad():\n        alexnet.eval()\n    \n        for i, batch in enumerate(val_loader):\n            img, label = batch\n            img, label = img.to(device, dtype = torch.float), label.to(device, dtype = torch.long)\n            output = alexnet(img)\n            batch_loss = criterion(output, torch.max(label, 1)[1])\n            output = output.detach().cpu().numpy()\n            label = label.detach().cpu().numpy()\n            val_outputs.extend(np.argmax(output, 1))\n            vloss += batch_loss.item()\n            \n    val_loss = vloss\/val_num_batches\n    print('Validation loss for epoch {} is {:.4f}'.format(epoch, val_loss))\n    losses_df['val_loss'].append(val_loss)\n    val_labels = list(np.argmax(np.array(val_y), 1))\n    acc = accuracy_score(val_outputs, val_labels)\n    print('Accuracy for epoch {} is {:.4f}'.format(epoch, acc))\n    losses_df['val_acc'].append(acc)\n    \n    if val_loss <= best_loss:\n        print('Validation loss has reduced from {:.4f} to {:.4f}'.format(best_loss, val_loss))\n        print('Saving model')\n        best_loss = val_loss\n        torch.save(alexnet.state_dict(), 'alexnet_finetuning.pth')","977797e4":"plt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nx_range = np.arange(20)\nplt.plot(x_range, losses_df['train_loss'], label = 'Train loss')\nplt.plot(x_range, losses_df['val_loss'], label = 'validation loss')\nplt.title('Losses')\nplt.legend()\n\nplt.subplot(1,2,2)\nx_range = np.arange(20)\nplt.plot(x_range, losses_df['val_acc'])\nplt.title('Validation Accuracy')\nplt.legend()\n","c48ad3aa":"test_loader = loader(test_X, test_y, batch_size = BATCH_SIZE, obj = 'test')","0d5b7cfe":"test_preds = []\n\nalexnet.load_state_dict(torch.load('alexnet_finetuning.pth'))\nwith torch.no_grad():\n    alexnet.eval()\n    \n    for i, batch in enumerate(test_loader):\n        img, label = batch\n        img, label = img.to(device, dtype = torch.float), label.to(device, dtype = torch.long)\n        output = alexnet(img)\n        output = output.detach().cpu().numpy()\n        test_preds.extend(np.argmax(output, 1))","e286316c":"labels = list(np.argmax(np.array(test_y), 1))","d5181e3d":"from sklearn.metrics import accuracy_score\n\nscore = accuracy_score(test_preds, labels)\nprint('Accuracy on test dataset : {:.2%}'.format(score))","a81e73ba":"After 10 epochs, both training as well as validation loss have stopped decreasing. Increasing the number of epochs will surely lead to overfitting. Validation loss would increase while training loss might decrease further. ","33689210":"This augmentation was used in the AlexNet paper. Following is the excerpt from AlexNet paper. \nFor more information refer to Alexnet Paper : https:\/\/papers.nips.cc\/paper\/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf","3135eee5":"## Data Loading and Preparation","ae62df21":"In this kernel, I have tried to implement the [AlexNet paper](http:\/\/papers.nips.cc\/paper\/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf). Originally, the model was trained on ImageNet dataset. I shall use Stanford Dogs dataset. The number of images in this dataset are drastically less as compared to ImageNet daataset. I shal be using the pretrained weights of AlexNet. The training process is implemented as mentioned in the paper with the mentioned augmentations as well. ","a936df4e":"## Visualizing Validation Images","7ffa0bae":"Accuracy is very less as compared to the state-of-the-art models. One of the reasons is the low number of images of each class. There are only close to 120 images of each class. These are very less to train and the model is likely to overfit. But we are achieving similar accuracy on validation as well as test set.","09d8ff5e":"To each RGB image pixel I xy =\nR\nG\nB T\n[I xy\n, I xy\n, I xy\n] we add the following quantity:\n[p 1 , p 2 , p 3 ][\u03b1 1 \u03bb 1 , \u03b1 2 \u03bb 2 , \u03b1 3 \u03bb 3 ] T\nwhere p i and \u03bb i are ith eigenvector and eigenvalue of the 3 \u00d7 3 covariance matrix of RGB pixel\nvalues, respectively, and \u03b1 i is the aforementioned random variable","e0c46cbb":"## DataSet","ee05a984":"## Model Training and Evaluation","ce9815d5":"## Visualizing Train Images","3cdf2419":"## Importing Modules","9baf521c":"## PCA Color Augmentation","c6414147":"## Loading Pretrained AlexNet Model","b616045d":"## Model performance on Test Set"}}