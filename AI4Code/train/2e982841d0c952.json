{"cell_type":{"e5097081":"code","a7c01cee":"code","42b4da95":"code","d64a46d2":"markdown","d565d916":"markdown","d5a34dbf":"markdown","f8c037a7":"markdown","84cd309b":"markdown","89bf2c0a":"markdown","49225f9a":"markdown","2c2c8f53":"markdown","36849625":"markdown","f76fbe25":"markdown"},"source":{"e5097081":"from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, f1_score, classification_report\ny_true = [0, 0, 1, 1, 1, 1]\ny_pred1 = [1, 1, 1, 1, 1, 1]\ny_pred2 = [0, 1, 1, 1, 1, 0]\n\ndef eval_preds(y_true, y_pred):\n    print('True:',y_true)\n    print('Pred:',y_pred)\n    print('--------------------------')\n    print('accuracy:', accuracy_score(y_true, y_pred))\n    print('balcanced_accuracy:', balanced_accuracy_score(y_true, y_pred))\n    print('roc_auc:', roc_auc_score(y_true, y_pred))\n    print('f1:', f1_score(y_true, y_pred))\n    print(classification_report(y_true, y_pred))","a7c01cee":"eval_preds(y_true, y_pred1)","42b4da95":"eval_preds(y_true, y_pred2)","d64a46d2":"# Setup","d565d916":"The true data set contains six entries. Two entries of class 1 and four entries of class 2:\n\n[0, 0, 1, 1, 1, 1]\n\nWe run two test rounds with two false entries each: \n1. Simulating a total overfitting to class 1: \n[1, 1, 1, 1, 1, 1]\n2. Simulating one missmatch per class:\n[0, 1, 1, 1, 1, 0]\n","d5a34dbf":"# Conclussion","f8c037a7":"Both predictions contain two mismatches. \nIn the first test the minority class contains only mismatches (0% match). The predictions of the majority classes are all right (100 % match).\nIn the second test each class contains one missmatch (50 % match for Class 0 and 75 % match for Class 1).\n\nAlthough both predictions have different characteristics **accuracy** has the same value (0.666). It ignors that class 0 is predicted totaly wrong in the first test.\n**balanced_accuracy** averages the accuracy per class. It figuers out that on the second prediction both classes are predicted partialy right. **roc_auc** shows the same results. **f1** states that the first test with the correct classified class 1 is more accurate then the second test.","84cd309b":"# Test","89bf2c0a":"The notebook contains some simple experiment to evaluate what metric to use on inbalanced classification. The question came up when trying Adversarial Validation on an imbalanced data set.\nAdversiaral Validation can be used to measure how similar a test and train set are and can furthermore be used to build a test-set-like validation set from train.\n\nThis notebooks does some basic tests on some metrics that can be used. Help my figuere out which one is the best (or the best in certain situations).\n\nHere are some references where I first stumbled upon Adverserial Validation (AV):\n* http:\/\/fastml.com\/adversarial-validation-part-one\/\n* https:\/\/www.kaggle.com\/tunguz\/adversarial-santander\n\nHere are public kernels relevant to this competition applying AV:\n* https:\/\/www.kaggle.com\/joatom\/a-test-like-validation-set\n* https:\/\/www.kaggle.com\/lukeimurfather\/adversarial-validation-train-vs-test-an-update","49225f9a":"What is our opinion of metrics on imbalanced data? Which one do you prefere? Please leave a comment if I got things wrong.","2c2c8f53":"# Intro","36849625":"## 2. Simulating one missmatch per class","f76fbe25":"## 1. Simulating overfitting to majority class"}}