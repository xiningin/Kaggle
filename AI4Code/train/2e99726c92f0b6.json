{"cell_type":{"fc65b07d":"code","1be1f404":"code","8bc444c1":"code","f07b251f":"code","daea50b8":"code","c30e709a":"code","cebba0a3":"code","259ad23f":"code","2a4ad733":"code","ca716c2f":"code","4df4ccb4":"code","39237018":"code","e6f3888d":"code","aa44149f":"code","3e733726":"code","0a6cde0b":"code","39ba7af4":"code","425f5b38":"code","a410026d":"code","56447e94":"code","3eb1087e":"markdown","6851da3d":"markdown","6cd292c6":"markdown","dbc7b297":"markdown","10105ace":"markdown","f4b54644":"markdown","aa2c908d":"markdown","3deda44c":"markdown","d28b3c6c":"markdown","b96c64c6":"markdown","69b8c1b2":"markdown"},"source":{"fc65b07d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1be1f404":"data=pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndata.head()","8bc444c1":"x_data=data.drop([\"Outcome\"],axis=1)\ny=data.Outcome.values\n\nx_data.head()","f07b251f":"x_data.tail()","daea50b8":"x=(x_data-np.min(x_data))\/((np.max(x_data)-np.min(x_data))).values\nx.head()","c30e709a":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)","cebba0a3":"print(\"x_train: {}\".format(x_train.shape))\nprint(\"x_test: {}\".format(x_test.shape))\nprint(\"y_train: {}\".format(y_train.shape))\nprint(\"y_test: {}\".format(y_test.shape))","259ad23f":"x_train=x_train.T\nx_test=x_test.T\ny_train=y_train.T\ny_test=y_test.T","2a4ad733":"print(\"x_train: {}\".format(x_train.shape))\nprint(\"x_test: {}\".format(x_test.shape))\nprint(\"y_train: {}\".format(y_train.shape))\nprint(\"y_test: {}\".format(y_test.shape))","ca716c2f":"def initialize_weights_and_bias(dimension):\n    w=np.full((dimension,1),0.01) # (dimension,1) matrisinin ba\u015flang\u0131\u00e7 weightlerini 0.01 yapt\u0131k.\n    b=0.0\n    \n    return w,b","4df4ccb4":"def sigmoid(z):\n    y_head=1\/(1+np.exp(-z)) # Sigmoid fonksiyonunun form\u00fcl\u00fc -> 1\/(1+exp(-z))\n    \n    return y_head","39237018":"sigmoid(0) # Fonksiyona bir de\u011fer vererek denedik.","e6f3888d":"def forward_and_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z=np.dot(w.T,x_train)+b # w.T dememizin sebebi matris \u00e7arp\u0131m\u0131 yapabilmemiz i\u00e7in uygun hale getirmek. w -> (diemnsion,1)  w.T -> (1,dimension)\n    y_head=sigmoid(z)\n    \n    # loss ve cost functions\n    loss = -y_train*np.log(y_head) - (1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1] \n    \n    # backward propagation\n    derivative_weight=(np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # t\u00fcrev ald\u0131k\n    derivative_bias=np.sum(y_head-y_train)\/x_train.shape[1] # t\u00fcrev ald\u0131k\n    \n    gradients={\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","aa44149f":"def update(w,b,x_train,y_train,learning_rate,iteration):\n    cost_list=[]\n    cost_list2=[]\n    index=[]\n    \n    for i in range(iteration):\n        \n        cost,gradients=forward_and_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost) # costlar\u0131 tutuyorum. \u00c7\u00fcnk\u00fc ileride grafik \u00e7izdirirken gerekli olacak.\n        \n        # weight ve bias g\u00fcncelleme\n        \n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        \n        if i%10==0: # buradaki 10'u biz belirledik. 10 ad\u0131mda bir costu g\u00f6stermesini istedi\u011fim i\u00e7in\n            cost_list2.append(cost)\n            index.append(i)\n        \n    parameters={\"weight\":w,\n                \"bias\":b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation=\"vertical\")\n    plt.xlabel(\"Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    return parameters,gradients,cost_list","3e733726":"def prediction(w,b,x_test):\n    z=sigmoid(np.dot(w.T,x_test)+b)\n    \n    y_prediction=np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_prediction[0,i]=0\n        else:\n            y_prediction[0,i]=1\n            \n    return y_prediction","0a6cde0b":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,iteration):\n    \n    dimension=x_train.shape[0]\n    w,b = initialize_weights_and_bias(dimension)\n    \n    parameters,gradients,cost_list=update(w,b,x_train,y_train,learning_rate,iteration)\n    \n    y_prediction_test=prediction(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n    # accuracy bizim do\u011fruluk oran\u0131m\u0131zd\u0131r.\n    \n    print(\"test accuracy: {} %\".format(100-np.mean(np.abs(y_prediction_test-y_test))*100))","39ba7af4":"logistic_regression(x_train,y_train,x_test,y_test,learning_rate=0.01,iteration=200)","425f5b38":"logistic_regression(x_train,y_train,x_test,y_test,learning_rate=2,iteration=200)","a410026d":"logistic_regression(x_train,y_train,x_test,y_test,learning_rate=2,iteration=400)","56447e94":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression(random_state = 42,max_iter= 150)\nlr.fit(x_train.T,y_train.T)\n\nprint(\"Train Accuracy: {}\".format(lr.score(x_train.T,y_train.T)))\nprint(\"Test Accuracy: {}\".format(lr.score(x_test.T,y_test.T)))","3eb1087e":"# Normalization","6851da3d":"# Logistic Regression with Sklearn","6cd292c6":"# Train,Test Split","dbc7b297":"Train veri setimizi ve test veri setimizi belirliyoruz. Bunu yaparken sklearn k\u00fct\u00fcphanesinden yararlan\u0131yoruz.","10105ace":"Transpozunu ald\u0131k.","f4b54644":"# Forward Propagation","aa2c908d":"Prediction -> Test verilerimize g\u00f6re prediction yap\u0131yoruz.","3deda44c":"# Initializing Parameters","d28b3c6c":"# Update","b96c64c6":"normalization -> (x - min(x)) \/ (max(x)-min(x))","69b8c1b2":"Bu k\u0131s\u0131m bizim weight ve biaslar\u0131m\u0131z\u0131 g\u00fcncelledi\u011fimiz k\u0131s\u0131m."}}