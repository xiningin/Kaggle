{"cell_type":{"a3e6df4a":"code","2ddc2a46":"code","dd90cfcc":"code","d71981ec":"code","2462a50a":"code","28697c39":"code","e72584d5":"code","a6e3fff3":"code","c0737c0a":"code","fe262408":"code","6b6f49cb":"code","99f88176":"code","a611c03c":"code","003e3957":"code","3a050611":"code","1bb9b590":"code","f75aee1c":"code","42279c86":"code","3007f35c":"code","ebf6be41":"code","c1b20eb7":"code","ab78392b":"code","05551188":"code","11ab5586":"code","17bd1602":"code","3bba2ed7":"code","fab55adb":"code","0ac1495f":"code","1f032819":"code","f552abad":"code","56739c73":"code","796e9ae3":"code","83154b51":"code","923a1ab6":"code","0fe64883":"code","e962039f":"code","fff4fd37":"code","9ed96929":"code","db37a6de":"code","be7c56fb":"code","0fa60bf0":"code","30f3f17f":"code","f7d32a54":"code","7afabf31":"code","1053a564":"code","2ffcac15":"code","7e0e1322":"code","1a505b32":"code","c62f4ac3":"code","d6839ca9":"code","f98ef436":"code","6f43d2d7":"code","d961c377":"code","153409a5":"code","cd5f95fc":"code","b3953433":"code","3211c0f8":"code","27bfba74":"code","4ed040f7":"code","5df912bf":"code","985bdb81":"code","c826a6a3":"code","8b8e1917":"code","6e24ca0a":"code","06157586":"code","a56d2ba5":"code","b8488011":"code","d8827628":"code","f6f2acca":"code","8101a71c":"code","cfa397eb":"code","dcf693e4":"code","ed255290":"code","14154d1c":"code","4d4c61a0":"code","24d5a939":"code","96e75072":"code","9b0f156b":"code","370b45e9":"code","29d68d09":"code","2d2582ee":"code","db3b6c91":"markdown","29a6d2b4":"markdown","6922913c":"markdown","5824f830":"markdown","e730f541":"markdown","2d766576":"markdown","a7ac700d":"markdown","ba4b32a4":"markdown","25622ca1":"markdown","8c22e8d6":"markdown","4a061fad":"markdown","631f2a2e":"markdown","6df44d48":"markdown","81dd0cae":"markdown","1ca472f7":"markdown","1b20bcd1":"markdown","05a08d23":"markdown","4e20ecbd":"markdown","60de40e4":"markdown","a65915be":"markdown","b7633971":"markdown","6d7c5310":"markdown","0de4c87b":"markdown","f3b15395":"markdown","f53501b9":"markdown","1105cb35":"markdown","b8390c9b":"markdown","e0073b72":"markdown","9c98076a":"markdown","8402c822":"markdown","e148f42a":"markdown","daba6f74":"markdown","29814e21":"markdown","839f3271":"markdown","d997ecb9":"markdown","16ec24df":"markdown","92a3e8a6":"markdown","82c7c30e":"markdown","90f603e0":"markdown","9c536364":"markdown","7b201a72":"markdown","616821d0":"markdown","d796a9d1":"markdown","3da19c05":"markdown","f191e652":"markdown","72c0fb06":"markdown","34c0866f":"markdown","246ad13c":"markdown","2e282430":"markdown","72466759":"markdown","43a182c5":"markdown","c1c7080a":"markdown","65768dda":"markdown","800c41d6":"markdown","27f5313c":"markdown"},"source":{"a3e6df4a":"%load_ext autoreload\n%autoreload 2\nimport os\n\n%matplotlib inline","2ddc2a46":"import numpy as np\nimport pandas as pd\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom scipy.stats import norm, skew\n\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nimport category_encoders as ce\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mean_squared_log_error\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox\n\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')","dd90cfcc":"!ls ..\/input","d71981ec":"PATH = \"..\/input\/house-prices-advanced-regression-techniques\/\"","2462a50a":"df_train=pd.read_csv(f'{PATH}train.csv')#, index_col='Id')\ndf_test=pd.read_csv(f'{PATH}test.csv')#, index_col='Id')","28697c39":"# for the purpose of evaluation of current competition we transform target value\ndf_train.SalePrice = np.log1p(df_train.SalePrice)","e72584d5":"print('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}\\n'.format(df_train['SalePrice'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))\n#print(df_train.columns)\n#print(df_test.columns)","a6e3fff3":"#print(df_train.info())\n#df_train.sample(3)\n#print(df_test.info())\n#df_test.sample(3)","c0737c0a":"fig, ax = plt.subplots()\nax.scatter(x = df_train['GrLivArea'], y = df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","fe262408":"# Deleting outliers\ndf_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(df_train['GrLivArea'], df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","6b6f49cb":"#remember where to divide train and test\nntrain = df_train.shape[0]\nntest = df_test.shape[0]\n\n#Save the 'Id' column\ntrain_ID = df_train['Id']\ntest_ID = df_test['Id']","99f88176":"def concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set on axis 0\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\ndf_all.shape","a611c03c":"#Dividing Target column (Y)\ny_train_full = df_train.SalePrice.values\ndf_all.drop(['SalePrice'], axis=1, inplace=True)\ndf_all.drop('Id',axis=1,inplace=True)","003e3957":"y_train_full","3a050611":"def mark_missing (df):\n    for col in df.columns:\n        if df_all[col].isnull().sum()>0:\n            df_all[col+'_missed']=df_all[col].isnull()","1bb9b590":"mark_missing(df_all)","f75aee1c":"df_all.shape","42279c86":"def display_missing(df):\n    for col in df.columns:\n        print(col, df[col].isnull().sum())\n    print('\\n')\n    \nfor df in dfs:\n    print(format(df.name))\n    display_missing(df)\n    \n    \n    \n#Check remaining missing values if any \ndef display_only_missing(df):\n    all_data_na = (df.isnull().sum() \/ len(df)) * 100\n    all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n    missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n    print(missing_data)","3007f35c":"display_only_missing(df_all)","ebf6be41":"# fill NA values (not missed) with None - based on data description -  - for non-Numerical (object) Columns\nfor col in ('Alley','MasVnrType','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n            'BsmtFinType2','FireplaceQu','GarageType', 'GarageFinish', 'GarageQual', \n            'GarageCond','PoolQC','Fence','MiscFeature'):\n    df_all[col] = df_all[col].fillna('None')","c1b20eb7":"display_only_missing(df_all)","ab78392b":"#fill NA numerical value with '0' - based on data description of correspondent Object columns - for Numerical Columns\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath','MasVnrArea'):\n    df_all[col] = df_all[col].fillna(0)","05551188":"display_only_missing(df_all)","11ab5586":"# Fill missing value in corresponding columns with most frequent value in column\nfor col in ('Utilities','Functional','SaleType','KitchenQual','Exterior2nd','Exterior1st','Electrical'):\n    df_all[col].fillna(df_all[col].mode()[0], inplace=True)\n    \n# Functional : data description says NA means typical\n# BTW we just used df_all.Functional.mode() = use most frequent value (as 'Typ' is most frequent value)\n#df_all[\"Functional\"] = df_all[\"Functional\"].fillna(\"Typ\")","17bd1602":"display_only_missing(df_all)","3bba2ed7":"df_all.MSZoning.isnull().sum()","fab55adb":"df_all[\"MSZoning\"] = df_all[\"MSZoning\"].fillna(\"None\")","0ac1495f":"display_only_missing(df_all)","1f032819":"#### Iteration 2 - replacing by machine learning","f552abad":"df_all['LotFrontage'].isnull().sum()","56739c73":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ndf_all[\"LotFrontage\"] = df_all.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","796e9ae3":"df_all['LotFrontage'].isnull().sum()","83154b51":"display_only_missing(df_all)","923a1ab6":"df_all.info()","0fe64883":"# Function Splitting Train - Validation\ndef quick_get_dumm(df):\n    X_train_full=df.iloc[:ntrain] # Full Train set\n\n    # Creating train and validation sets\n    X_train, X_valid, y_train, y_valid = train_test_split(pd.get_dummies(X_train_full), y_train_full, random_state=42)\n    return X_train, X_valid, y_train, y_valid","e962039f":"X_train, X_valid, y_train, y_valid = quick_get_dumm(df_all)","fff4fd37":"X_train.shape, X_valid.shape, y_train.shape, y_valid.shape","9ed96929":"# Defining evaluation functions\ndef rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m,X_train=X_train, X_valid=X_valid, y_train=y_train, y_valid=y_valid):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","db37a6de":"m_rf = RandomForestRegressor(n_estimators=160, min_samples_leaf=1, max_features=0.5, n_jobs=-1, oob_score=True, random_state=42)\nm_rf.fit(X_train, y_train)\nprint_score(m_rf)","be7c56fb":"def elastic_score(X,y):\n    elastic = ElasticNet(random_state=1)\n    param = {'l1_ratio' : [0],\n             'alpha' : [0.017]}\n    elastic = GridSearchCV(elastic, param, cv=5, scoring='neg_mean_squared_error')\n    elastic.fit(X,y)\n    print('Elastic:', np.sqrt(elastic.best_score_*-1))\n    return elastic","0fa60bf0":"elastic_score(X_train, y_train)","30f3f17f":"m_xgb = XGBRegressor(n_estimators=160, learning_rate=0.05, random_state=42)\n# using early_stop to find out where validation scores don't improve\n#m_xgb.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=False)\nm_xgb.fit(X_train, y_train)\nprint_score(m_xgb)","f7d32a54":"# We created function to return NA values of feature\/column back in place, \n# based on _missed column, we created to state what values was missed in original dataset\n\n# returning original NA values back\ndef return_original_na(df, feature):\n    df[feature].loc[df.index[df[feature+'_missed'] == True].tolist()]=np.nan\n    return df[feature]","7afabf31":"#Returning original NA values of MSZoning back in place\ndf_all['LotFrontage']=return_original_na(df_all, 'LotFrontage')","1053a564":"df_all['LotFrontage'].isnull().sum()","2ffcac15":"display_only_missing(df_all)","7e0e1322":"def filling_na_with_predictions(df, feature):\n    \"\"\"\n    df - DataFrame without target column y. Train+Test DataFrame (df_all)\n    feature - feature (column), containing real NA values we will fill\n\n    Assumption:\n    All other columns do not have NA values. In case of having we have to impute with some Statistical method (Median, etc)\n    We do not do it inside this function\n    \"\"\"\n\n    flag_object=0\n    \n    if df[feature].isnull().sum()>0:\n        ## Store Indexes of rows with NA values (we can just call \"_missed\" column with True values, to check those indexes as well)\n        ## Creating index based on NA values present in column\n        na_rows_idxs=df[df[feature].isnull()].index \n            ## Creating index based on NA values being present in original DF column\n            #na_rows_idxs=df.index[df[feature+'_missed'] == True].tolist()\n\n        ## For fitting and predictiong - convert DF to dummies DF, ready for ML\n        #df=pd.get_dummies(df)\n        ## If feature object we cant just dummy all, we shouldn't dummy feature column\n        df=pd.concat([ pd.Series(df[feature]), pd.get_dummies(df.drop([feature], axis=1)) ], axis=1)\n\n\n        ## Splitting DF to Feature_Train_X, Feature_Train_y, Feature_Predict_X:\n        ## Feature_Train_X = DF without NA values in \"feature_with_NA\"column\n        ## Feature_Train_y = target values that we have. All values in \"feature_with_NA\" except NA values\n        ## Feature_Predict_X = DF of correcponding to NA values in \"feature_with_NA\" without target vales (basically because they is equal to NA)\n        Feature_Train_X=df.drop(df[df[feature].isnull()].index).drop([feature], axis=1)\n        Feature_Train_y=df[feature].drop(df[df[feature].isnull()].index).values\n        Feature_Predict_X=df[df[feature].isnull()].drop([feature], axis=1)\n\n        ## If feature is NOT Numerical\n        ## Label encoding of y values in case it is not numerical\n        if is_string_dtype(df[feature]) or is_categorical_dtype(df[feature]):\n            flag_object=1\n            from sklearn.preprocessing import LabelEncoder\n            le = LabelEncoder()\n            le.fit(Feature_Train_y)\n            Feature_Train_y=le.transform(Feature_Train_y)\n             \n        ## Making predictions, what might be in NA fields based on Train DF\n        #m_xgb = XGBRegressor(n_estimators=160, learning_rate=0.05)\n        #m_xgb.fit(Feature_Train_X, Feature_Train_y)\n        elastic = ElasticNet(random_state=1)\n        param = {'l1_ratio' : [0],\n             'alpha' : [0.017]}\n        elastic = GridSearchCV(elastic, param, cv=5, scoring='neg_mean_squared_error')\n        elastic.fit(Feature_Train_X,Feature_Train_y)\n    \n        ## Creating (Predicting) values to impute NA\n        #fillna_values=m_xgb.predict(Feature_Predict_X)\n        fillna_values=elastic.predict(Feature_Predict_X)\n\n        ## If feature is NOT Numerical\n        ## Return Encoded values back to Object\/Category if feature NOT numerical\n        if flag_object==1:\n            fillna_values=le.inverse_transform(np.around(fillna_values).astype(int))\n        \n        ## Replacing NA values with predicted Series of values\n        df[feature]=df[feature].fillna(pd.Series(index=na_rows_idxs,data=fillna_values))\n\n        ## Returning feature column without NA values    \n        return df[feature]\n    else:\n        print ('There were no NA values')","1a505b32":"# Datafremas to predict missed LotFrontage in train test. We add SalePrice to exploit all data that we have, hence to have more accuracy.\n# As next step we will use concatenated dataframe Train+Test but without SalePrice, as we don't have SalePrice column for test, hence can't use this info to restore missed values of LotFrontage in Test dataset\ndf_tmp_train=df_all.iloc[:ntrain] # Full Train set\ndf_tmp_train['SalePrice']=y_train_full\ndf_tmp_test=df_all.iloc[ntrain:] # Test set","c62f4ac3":"# Replacing missing LotFrontage values in Train dataset\ndf_tmp_train['LotFrontage']=filling_na_with_predictions(df_tmp_train, \"LotFrontage\")\n# Replacing missing LotFrontage values in Test dataset\ndf_tmp_test['LotFrontage']=filling_na_with_predictions(df_tmp_test, \"LotFrontage\")","d6839ca9":"df_tmp_train.drop(['SalePrice'], axis=1, inplace=True)\ndf_all = concat_df(df_tmp_train, df_tmp_test)","f98ef436":"df_all['LotFrontage'].isnull().sum()","6f43d2d7":"def evaluate(df):\n    # Split dataset for train-validation\n    X_train, X_valid, y_train, y_valid = quick_get_dumm(df)\n    \n    #ElasticNet\n    elastic_score(X_train, y_train)\n\n    #XGBoost\n    m_xgb.fit(X_train, y_train)\n    print('XGBoost')\n    print_score(m_xgb,X_train, X_valid, y_train, y_valid)\n\n    # Random Forest\n    m_rf.fit(X_train, y_train)\n    print('Random Forest')\n    print_score(m_rf,X_train, X_valid, y_train, y_valid)","d961c377":"evaluate(df_all)","153409a5":"#Returning original NA values of MSZoning back in place\ndf_all['MSZoning']=return_original_na(df_all, 'MSZoning')","cd5f95fc":"display_only_missing(df_all)","b3953433":"df_all[df_all['MSZoning'].isnull()].index","3211c0f8":"df_all['MSZoning']=filling_na_with_predictions(df_all, 'MSZoning')","27bfba74":"df_all['MSZoning'].loc[df_all.index[df_all['MSZoning'+'_missed'] == True].tolist()]","4ed040f7":"evaluate(df_all)","5df912bf":"##### Dealing with Missing values we replaced with most common - now replacing them with ML predictions","985bdb81":"for col in ('Utilities','Functional','SaleType','KitchenQual','Exterior2nd','Exterior1st','Electrical'):\n    print ('Filling with most common:\\n',df_all[col].loc[df_all.index[df_all[col+'_missed'] == True].tolist()])\n    df_all[col]=return_original_na(df_all, col)\n    df_all[col]=filling_na_with_predictions(df_all, col)\n    print ('Filling with predictions:\\n',df_all[col].loc[df_all.index[df_all[col+'_missed'] == True].tolist()])","c826a6a3":"evaluate(df_all)","8b8e1917":"# Saving Train Dataset after cleaning\ndf_train_save=df_all.iloc[:ntrain]\ndf_train_save['SalePrice']=y_train_full\n\n# Saving Test Dataset after cleaning\ndf_test_save=df_all.iloc[ntrain:] # Test set\n#df_test_save['Id']=test_ID","6e24ca0a":"df_test_save.head()","06157586":"df_train_save.to_csv('train_clean.csv', index=False)\ndf_test_save.to_csv('test_clean.csv', index=False)","a56d2ba5":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nskewness = df_all.select_dtypes(include=numerics).apply(lambda x: skew(x))\nskew_index = skewness[abs(skewness) >= 0.85].index\nskewness[skew_index].sort_values(ascending=False)","b8488011":"'''BoxCox Transform'''\nlam = 0.15\n\nfor column in skew_index:\n    df_all[column] = boxcox1p(df_all[column], lam)\n","d8827628":"# Evaluation after working with skewed data\nevaluate(df_all)","f6f2acca":"df_all=pd.get_dummies(df_all)","8101a71c":"df_all.shape","cfa397eb":"\"\"\"Dividing working DataFrame back to Train and Test\"\"\"\n# split Validational\/Test set from Training set after Categorical Value Engeneering\n#def original_train_test(df_all):\nX_test=df_all.iloc[ntrain:] # Test set\nX_train_full=df_all.iloc[:ntrain] # Train set\nX_train, X_valid, y_train, y_valid = train_test_split(pd.get_dummies(X_train_full), y_train_full)","dcf693e4":"# Saving all features for future comparison.\nall_features = df_all.keys()\n# Removing features.\ndf_all = df_all.drop(df_all.loc[:,(df_all==0).sum()>=(df_all.shape[0]*0.984)],axis=1)\ndf_all = df_all.drop(df_all.loc[:,(df_all==1).sum()>=(df_all.shape[0]*0.984)],axis=1) \n# Getting and printing the remaining features.\nremain_features = df_all.keys()\nremov_features = [st for st in all_features if st not in remain_features]\nprint(len(remov_features), 'features were removed:', remov_features)","ed255290":"# Evaluation after dropping not important features\nevaluate(df_all)","14154d1c":"from sklearn import preprocessing\n\nscaler = preprocessing.RobustScaler()\ndf_all = pd.DataFrame(scaler.fit_transform(df_all))","4d4c61a0":"# Evaluation after Normalization\nevaluate(df_all)","24d5a939":"\"\"\"Dividing working DataFrame back to Train and Test\"\"\"\n# split Validational\/Test set from Training set after Categorical Value Engeneering\n#def original_train_test(df_all):\nX_test=df_all.iloc[ntrain:] # Test set\nX_train_full=df_all.iloc[:ntrain] # Train set\nX_train, X_valid, y_train, y_valid = train_test_split(pd.get_dummies(X_train_full), y_train_full)","96e75072":"# ElasticNet\nprint('ElasticNet')\ndef elastic_score(X,y):\n    elastic = ElasticNet(random_state=1)\n    param = {'l1_ratio' : [0],\n             'alpha' : [0.017]}\n    elastic = GridSearchCV(elastic, param, cv=5, scoring='neg_mean_squared_error')\n    elastic.fit(X,y)\n    print('ElasticNet:', np.sqrt(elastic.best_score_*-1))\n    return elastic\nelastic_score(X_train, y_train)\n\n# XGBoost\nprint('XGBoost')\nm_xgb = XGBRegressor(n_estimators=160, learning_rate=0.05)\n# using early_stop to find out where validation scores don't improve\n#m_xgb.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=False)\nm_xgb.fit(X_train, y_train)\nprint_score(m_xgb,X_train, X_valid, y_train, y_valid)\n\n\n# Random Forest\nprint('Random Forest')\nm_rf = RandomForestRegressor(n_estimators=160, min_samples_leaf=1, max_features=0.5, n_jobs=-1, oob_score=True)\nm_rf.fit(X_train, y_train)\nprint_score(m_rf,X_train, X_valid, y_train, y_valid)\n","9b0f156b":"def cv_train():\n    elastic = ElasticNet(random_state=1)\n    param = {'l1_ratio' : [0],\n             'alpha' : [0.017]}\n    elastic = GridSearchCV(elastic, param, cv=5, scoring='neg_mean_squared_error')\n    elastic.fit(X_train_full, y_train_full)\n    print('Elastic:', np.sqrt(elastic.best_score_*-1))\n    return elastic\nelastic = cv_train()","370b45e9":"y_pred=np.expm1(elastic.predict(X_test)); y_pred","29d68d09":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = y_pred\nsub.to_csv('submission_31Aug19.csv',index=False)","2d2582ee":"sub.head()","db3b6c91":"\"\"\"\"\nFeatures that have too low variance can negatively impact the model, so we need to remove them by the number of repetitive equal values. In this case, we used a threshold of 99.2% (not 0 or 1 values). Therefore, if any feature has more than \u02dc99% reps of 1 or 0 it will be excluded. When doing this,\n\"\"\"","29a6d2b4":"## Dropping low variance features","6922913c":"##### Dealing with missing values left","5824f830":"### Y (target value) to Log, as stated at Kaggle Evaluation page","e730f541":"##### Once again dealing with missed LotFrontage feature","2d766576":"### Replace Missing","a7ac700d":"As we can see we had all 'RL' values for MSZoning column, but ML algorithms proposed to change it a little bit. Let's check score","ba4b32a4":"##### Replace NA missing values by most often in column (only for columns with 2 and less NA values, where do not make sense to invest hugely into Analysis)","25622ca1":"# Dealing with Outliers","8c22e8d6":"#Normalization, the Sigmoid, Log, Cube Root and the Hyperbolic Tangent. \n#It all depends on what one is trying to accomplish.","4a061fad":"## Dealining with Scewed data","631f2a2e":"**So let us start with Step 1. This simple Step 1, required us almost no brain activity, gave us score of Top 20% = 0.11819**\nIt is much better than Random Forest, XGBoost, LightGBM and other modeling techniques. We evaluated some of them in this kernel as well","6df44d48":"## DataFrame concatination and Y separation","81dd0cae":"Great! We are ready to Train real model","1ca472f7":"## Saving DataFrame for next Steps","1b20bcd1":"After this step we drastically improved scores, especially ElasticNet","05a08d23":"* ### Experimenting with ElasticNet","4e20ecbd":"As we can see - nothing in scores changed, so it was unnecessary step, possibly because these last features weren't important for models","60de40e4":"**This is Step 1** - simple notebook on \"**House Prices: Advanced Regression Techniques**\"\n\nIf you like this work - upvotes and feedback are highly appreciated :)\n\n\nMain purpose is learning and evaluating how different data cleaning, feature encoding, feature creation, skew, normalization, external feature engineering, affects different modeling and stacking techniques and how different modeling techniques affect performance of predictions.\n\n\n**We will explore this competition in a few steps. **\n\nEach next step we decided just to use CSV data from the previous step:\n\n**Step 1 (current) - Data cleaning** - simple data cleaning + Scew + Normalization + ElasticNet. Without any feature engineering, external feature creation, and any advanced modeling techniques. One important thing we have done at this step is using ML to impute missing values rather then just use our logical thinking.\n\n**Step 2 - Basic Feature Engineering** - basic feature engineering + Scew + Normalization + ElasticNet. At this step, we will test different encoding techniques for our data (ordinal, one-hot, label, binary, ...). We will not create any new features. And we will measure heavily effect of each change on different models. \n\n**Step 3 - Advanced Feature Engineering** - at this step, we will create and encode new features, use feature importance to drop not important features, etc. And we will measure heavily effect of each change on different models.\n\n**Step 1.2, 2.2, 3.2** - same as corresponding 1, 2, 3 but instead of simple ElasticNet model we will use StackNet for prediction and submission.","a65915be":"##### Dealing with MSZoning","b7633971":"# Submission","6d7c5310":"# Pre-Evaluation - benchmarking","0de4c87b":"### Replace non-missing but \"NA\", \"None\", etc values by Data description","f3b15395":"## Evaluation","f53501b9":"### Replacing real missing values","1105cb35":"##### Replace NA in Numerical columns, based on information from description","b8390c9b":"# Initial Setup and Data Load","e0073b72":"## Predictions for submission","9c98076a":"# Conclusins before Step 2","8402c822":"We also have REAL missing values, that we can't just replace based on description that if missed - use 'None' or 0. Hence, we will work here","e148f42a":"## Dummies","daba6f74":"We will try ML techniques to predict all real missing values. We'll see how it will improve accuracy","29814e21":"#### Iteration 1 - replacing by logic and deduction of human","839f3271":"## Making Training, Validation, Test Dataset","d997ecb9":"# Dealing with missing values based on machine learning predictions","16ec24df":"Great! As we can see in all 3 models scores improved using ML algorithms to replace missing values","92a3e8a6":"In MSZoning we have 4 missing values. \nWe can replace them either by most common in column, or I have decided just with 'None' object values","82c7c30e":"As next Step (2) we will work on basic Feature Engineering (heavily on data encoding techniques).","90f603e0":"##### Dealing with LotFrontage","9c536364":"### Create columns to mark originally missed values","7b201a72":"### XGBoost","616821d0":"## Pred ML Evaluation","d796a9d1":"### Experimenting with Random Forest","3da19c05":"##### Seems no missed values\nMissing Values = DONE","f191e652":"##### Replace NA in Object columns, based on information from description","72c0fb06":"# Preparing clean data for ML","34c0866f":"**As we can see with very simple modeling of ElasticNet and even without any FeatureEngineering we still can achieve score 0.11819 that is about Top 20%**","246ad13c":"\"\"\"\nAlley: Type of alley access to property\n       NA \tNo alley access\nMasVnrType: Masonry veneer type\n       None\tNone\nBsmtQual: Evaluates the height of the basement\n       NA\tNo Basement\nBsmtCond: Evaluates the general condition of the basement\n       NA\tNo Basement\nBsmtExposure: Refers to walkout or garden level walls\n       No\tNo Exposure\n       NA\tNo Basement\nBsmtFinType1: Rating of basement finished area\n       NA\tNo Basement\nBsmtFinType2: Rating of basement finished area (if multiple types)\n       NA\tNo Basement\nCentralAir: Central air conditioning\n       N\tNo\nFireplaceQu: Fireplace quality\n       NA\tNo Fireplace\nGarageType: Garage location\n       NA\tNo Garage\nGarageFinish: Interior finish of the garage\n       NA\tNo Garage\nGarageQual: Garage quality\n       NA\tNo Garage\nGarageCond: Garage condition\n       NA\tNo Garage\nPavedDrive: Paved driveway\n       N\tDirt\/Gravel\nPoolQC: Pool quality\n       NA\tNo Pool\nFence: Fence quality\n       NA\tNo Fence\nMiscFeature: Miscellaneous feature not covered in other categories\n       NA\tNone\n\"\"\"","2e282430":"As ML algorithms for prediction we will use:\n\nStep 1 - simple ElasticNet\n\nStep 1.1 - same data, StackNet\n\nStep 2 - basic feature engineering, ML Elastic Net\n\nStep 2.2 - basic feature engineering, ML StackNet\n\nStep 3 - advanced feature engineering, ML Elastic Net\n\nStep 3.2 - advanced feature engineering, ML StackNet","72466759":"\"\"\"\"\nMSZoning: Identifies the general zoning classification of the sale.\n       A\tAgriculture\n       C\tCommercial\n       FV\tFloating Village Residential\n       I\tIndustrial\n       RH\tResidential High Density\n       RL\tResidential Low Density\n       RP\tResidential Low Density Park \n       RM\tResidential Medium Density\n\"\"\"","43a182c5":"# Machine Learning","c1c7080a":"We will deal with next features\nUtilities         0.068517\nFunctional        0.068517\nSaleType          0.034258\nKitchenQual       0.034258\nExterior2nd       0.034258\nExterior1st       0.034258\nElectrical        0.034258","65768dda":"## Normalization","800c41d6":"##### Once again dealing with missed MSZoning feature","27f5313c":"# Dealing with Missing Values (logically)"}}