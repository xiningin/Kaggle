{"cell_type":{"ac100465":"code","1d561ccc":"code","5b1853cb":"code","637e4c8c":"code","f3d294fe":"code","32ee90b7":"code","25ee5020":"code","17832d51":"code","c8e646d5":"code","c1cd0a42":"code","5b5f871e":"code","81d7d3ee":"code","021146d5":"code","7d14d7d4":"code","b4fce352":"code","2630a60d":"code","e803658f":"code","d94815d9":"code","4d1736da":"code","bb4ec1c4":"markdown","f0bbc55b":"markdown","39c99510":"markdown","bf335ac7":"markdown","e8064b4a":"markdown","8feb70d9":"markdown"},"source":{"ac100465":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot","1d561ccc":"df = pd.read_csv('..\/input\/fake-news\/train.csv')\ntest = pd.read_csv('..\/input\/fake-news\/test.csv')","5b1853cb":"df.head()","637e4c8c":"#filling NULL values with empty string\ndf=df.fillna('')\ntest=test.fillna('')","f3d294fe":"# We will be only using title and author name for prediction\n# Creating new coolumn total concatenating title and author\ndf['total'] = df['title']+' '+df['author']\ntest['total']=test['title']+' '+test['author']","32ee90b7":"X = df.drop('label',axis=1)\ny=df['label']\nprint(X.shape)\nprint(y.shape)","25ee5020":"#Choosing vocabulary size to be 5000 and copying data to msg for further cleaning\nvoc_size = 5000\nmsg = X.copy()\nmsg_test = test.copy()","17832d51":"#Downloading stopwords \n#Stopwords are the words in any language which does not add much meaning to a sentence.\n#They can safely be ignored without sacrificing the meaning of the sentence.\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')","c8e646d5":"#We will be using Stemming here\n#Stemming map words to their root forms\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []","c1cd0a42":"#Applying stemming and some preprocessing\nfor i in range(len(msg)):\n  review = re.sub('[^a-zA-Z]',' ',msg['total'][i])\n  review = review.lower()\n  review = review.split()\n  review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n  review = ' '.join(review)\n  corpus.append(review)","5b5f871e":"#Applying stemming and some preprocessing for test data\ncorpus_test = []\nfor i in range(len(msg_test)):\n  review = re.sub('[^a-zA-Z]',' ',msg_test['total'][i])\n  review = review.lower()\n  review = review.split()\n  review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n  review = ' '.join(review)\n  corpus_test.append(review)","81d7d3ee":"# Converting to one hot representation\nonehot_rep = [one_hot(words,voc_size)for words in corpus]\nonehot_rep_test = [one_hot(words,voc_size)for words in corpus_test]","021146d5":"#Padding Sentences to make them of same size\nembedded_docs = pad_sequences(onehot_rep,padding='pre',maxlen=25)\nembedded_docs_test = pad_sequences(onehot_rep_test,padding='pre',maxlen=25)","7d14d7d4":"#We have used embedding layers with LSTM\nmodel = Sequential()\nmodel.add(Embedding(voc_size,40,input_length=25))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","b4fce352":"#Converting into numpy array\nX_final = np.array(embedded_docs)\ny_final = np.array(y)\ntest_final = np.array(embedded_docs_test)\nX_final.shape,y_final.shape,test_final.shape","2630a60d":"#training model\nmodel.fit(X_final,y_final,epochs=20,batch_size=64)","e803658f":"y_pred = model.predict_classes(test_final)","d94815d9":"final_sub = pd.DataFrame()\nfinal_sub['id']=test['id']\nfinal_sub['label'] = y_pred\nfinal_sub.to_csv('final_sub.csv',index=False)","4d1736da":"final_sub.head()","bb4ec1c4":"### Data preprocessing and cleaning","f0bbc55b":"## Creating Submission file ","39c99510":"### Importing dataset","bf335ac7":"# Introduction\n* The notebook gives an introduction to NLP.\n* The notebook also introduce us to some preprocessing techniques required for text data\n* We will be working on the famous fake news dataset.","e8064b4a":"# Conclusion\n* LSTM with embedding layer works great\n* The model gives more than 99% accuracy on test data.\n* Furthermore we can also try vectoriztion or bi-directional LSTM.\n\n![](https:\/\/st3.depositphotos.com\/1998651\/13850\/v\/600\/depositphotos_138506364-stock-illustration-cup-of-coffee-with-have.jpg)","8feb70d9":"# Creating and training model"}}