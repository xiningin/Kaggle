{"cell_type":{"29761d99":"code","ce7574cb":"code","70d4d30a":"code","43842281":"code","fa003d8b":"code","cc0b1464":"code","488afe9f":"code","a04b15a8":"code","c2657027":"code","cddd456b":"code","62e093e4":"code","b32a90a2":"code","9950ca6d":"code","4b3aca8b":"code","b4ab2046":"code","3d360636":"code","4a79fe2f":"code","7791a8fa":"code","3a959664":"code","41e56da6":"code","11a4caa9":"code","fff76e43":"code","442f5a31":"code","7fd59ba1":"code","ca7e17ab":"code","ca85cb79":"code","dee4ba44":"code","b4d5d5b9":"code","998c0890":"code","05b761cb":"code","7f5a7f1a":"code","e63e2794":"code","7c90c04f":"code","6f361863":"code","67727ea8":"code","e7f7338e":"code","ae3dfbc3":"code","66e97718":"code","fa7b278c":"code","991704ef":"code","4af00a07":"code","8c9e0b6d":"code","1e911bf3":"code","a99da0b2":"code","5b81ae1b":"code","b4d23563":"code","9c181ad7":"code","e956621c":"code","60cb2c6e":"code","197cfc3a":"code","1240ca6c":"code","a6a002ac":"code","0d5aa747":"code","be415604":"code","8e9a1c2e":"code","34296ded":"code","4b3fbac4":"code","e5652234":"code","f6285dd7":"code","e1ff18da":"code","677035b0":"code","03340158":"code","bc40a9a2":"code","00279355":"code","c5c05fa8":"code","6c27f004":"code","1cf7b7bc":"markdown","6d03c5c5":"markdown","113d2e68":"markdown","6001b8ad":"markdown","f8d3575f":"markdown","19f2ce20":"markdown","94a439bf":"markdown","c5430058":"markdown","cdeed95a":"markdown","9398738b":"markdown","72719270":"markdown","b073b7fb":"markdown","264dde3a":"markdown","3be1ec8b":"markdown","77b11a37":"markdown","2cdfea09":"markdown","59a63670":"markdown","7b9c4cd4":"markdown","03993fe4":"markdown","0a9960b5":"markdown","c9d275b2":"markdown","8660de06":"markdown","15929017":"markdown","d55eb2c7":"markdown","b3543a81":"markdown","c26ec252":"markdown","c73b5553":"markdown","5b2e8ad9":"markdown","c249d594":"markdown","2d7de139":"markdown","58f4b797":"markdown","7d9a6c91":"markdown","de710636":"markdown","bb45b63f":"markdown"},"source":{"29761d99":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce7574cb":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","70d4d30a":"train_car = pd.read_csv('..\/input\/week1-car-acceptability\/car_acc_train.csv')\ntrain_car.dropna(inplace=True)\ntest_car = pd.read_csv('..\/input\/it2034ch1502-car-acceptability-prediction\/test.csv')\ndev_car = pd.read_csv('..\/input\/week1-car-acceptability\/car_acc_dev_v2.csv')","43842281":"train_car.head()","fa003d8b":"len(train_car), len(test_car), len(dev_car)","cc0b1464":"train_temp = train_car[['buying_price', 'maintenance_price', 'number_of_doors',\n       'carry_capacity', 'trunk_size', 'safety']]\ntest_temp = test_car[['buying_price', 'maintenance_price', 'number_of_doors',\n       'carry_capacity', 'trunk_size', 'safety']]\nconcat_df = pd.concat((train_temp, test_temp)).drop_duplicates()\nlen(train_temp), len(test_temp), len(concat_df)","488afe9f":"train_car.isnull().sum()","a04b15a8":"dev_car.isnull().sum()","c2657027":"test_car.isnull().sum()","cddd456b":"train_car.describe()","62e093e4":"buying_price = pd.crosstab(train_car['buying_price'], train_car['acceptability'])\nmaintenance_price = pd.crosstab(train_car['maintenance_price'], train_car['acceptability'])\nnumber_of_doors = pd.crosstab(train_car['number_of_doors'], train_car['acceptability'])\ncarry_capacity = pd.crosstab(train_car['carry_capacity'], train_car['acceptability'])\ntrunk_size = pd.crosstab(train_car['trunk_size'], train_car['acceptability'])\nsafety = pd.crosstab(train_car['safety'], train_car['acceptability'])","b32a90a2":"buying_price","9950ca6d":"f, ax = plt.subplots(figsize=(9, 9))\nstacked = buying_price.stack().reset_index().rename(columns={0:'value'})\nsns.barplot(x=stacked['buying_price'], y=stacked['value'], hue=stacked['acceptability'])","4b3aca8b":"maintenance_price","b4ab2046":"f, ax = plt.subplots(figsize=(9, 9))\nstacked = maintenance_price.stack().reset_index().rename(columns={0:'value'})\nsns.barplot(x=stacked['maintenance_price'], y=stacked['value'], hue=stacked['acceptability'])","3d360636":"number_of_doors","4a79fe2f":"f, ax = plt.subplots(figsize=(9, 9))\nstacked = number_of_doors.stack().reset_index().rename(columns={0:'value'})\nsns.barplot(x=stacked['number_of_doors'], y=stacked['value'], hue=stacked['acceptability'])","7791a8fa":"carry_capacity","3a959664":"f, ax = plt.subplots(figsize=(9, 9))\nstacked = carry_capacity.stack().reset_index().rename(columns={0:'value'})\nsns.barplot(x=stacked['carry_capacity'], y=stacked['value'], hue=stacked['acceptability'])","41e56da6":"trunk_size","11a4caa9":"f, ax = plt.subplots(figsize=(9, 9))\nstacked = trunk_size.stack().reset_index().rename(columns={0:'value'})\nsns.barplot(x=stacked['trunk_size'], y=stacked['value'], hue=stacked['acceptability'])","fff76e43":"safety","442f5a31":"f, ax = plt.subplots(figsize=(9, 9))\nstacked = safety.stack().reset_index().rename(columns={0:'value'})\nsns.barplot(x=stacked['safety'], y=stacked['value'], hue=stacked['acceptability'])","7fd59ba1":"label_mapper = {'unacc': 0, 'acc': 1, 'good': 2, 'vgood': 3}\nlabel_demapper = {0: 'unacc', 1: 'acc', 2: 'good', 3: 'vgood'}","ca7e17ab":"train_car['acceptability'].unique()","ca85cb79":"def build_data(df):\n    X = df[['buying_price', 'maintenance_price', 'number_of_doors',\n       'carry_capacity', 'trunk_size', 'safety']]\n    X = pd.get_dummies(X)\n    if 'acceptability' in df.columns:\n        y = df['acceptability'].map(label_mapper)\n        return X, y\n    return X","dee4ba44":"X_train, y_train = build_data(train_car)\nX_dev, y_dev = build_data(dev_car)","b4d5d5b9":"from sklearn.model_selection import GridSearchCV, PredefinedSplit\n\ndef find_best_params(params, cls_model, X_train, y_train, X_test, y_test):\n    X = pd.concat((X_train, X_test))\n    y = pd.concat((y_train, y_test))\n    \n    train_index = [-1] * len(X_train)\n    test_index = [0] * len(X_test)\n    \n    split_index = train_index + test_index\n    split = PredefinedSplit(test_fold=split_index)\n\n    grid_model = GridSearchCV(estimator=cls_model,\n                              param_grid=params,\n                              cv=split,\n                              scoring='accuracy',\n                              verbose=20,\n                              n_jobs=-1,\n                            refit=False)\n    grid_results = grid_model.fit(X, y)\n    print('Best Score %.4f' % grid_model.best_score_)\n    print('Best params : ')\n    print(grid_model.best_params_)\n\n    return grid_model, grid_results","998c0890":"from sklearn import svm\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import RandomizedSearchCV","05b761cb":"svc_params = {'C': [0.05, 0.1, 1, 10, 20, 50, 100],\n             'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\nsvm_model = svm.SVC(kernel='rbf', random_state=2021)\n_, svc_results = find_best_params(svc_params, svm_model, X_train, y_train, X_dev, y_dev)","7f5a7f1a":"best_svm_cls = svm.SVC(kernel='poly', C=10, random_state=2021, probability=True)\nbest_svm_cls.fit(X_train, y_train)\nsvc_predicts = best_svm_cls.predict(X_dev)\nprint(classification_report(y_dev, svc_predicts, digits=4))","e63e2794":"import xgboost as xgb\nfrom xgboost import plot_importance\nfrom xgboost import XGBClassifier","7c90c04f":"params = {'max_depth': [3, 4, 5, 6, 7, 8],\n          'learning_rate': [0.005, 0.01, 0.05, 0.1],\n          'n_estimators': [500, 600, 700, 800, 900, 1000, 1200, 1300, 1400, 1500]}\n\nxgb_cls = xgb.XGBClassifier(seed=20)\n_, xgb_results = find_best_params(params, xgb_cls, X_train, y_train, X_dev, y_dev)","6f361863":"best_xgb =  xgb.XGBClassifier(n_estimators=1200,\n                              max_depth=5,\n                              learning_rate=0.1,\n                              seed=20)\nbest_xgb.fit(X_train, y_train)\ny_pred = best_xgb.predict(X_dev)\nprint(classification_report(y_dev, y_pred, digits=4))","67727ea8":"from lightgbm import LGBMClassifier\n\nlight_gbm_params = {'n_estimators': [1000, 1200, 1300, 1400, 1500, 2000, 2200, 2500, 2800, 3000, 3500, 4000],\n                    'boosting_type': ['gbdt', 'dart', 'goss', 'rf'],\n                    'num_leaves': [6, 8, 10, 15, 20],\n                    'learning_rate': [0.005, 0.01, 0.05, 0.1]}\n\nlightgbm = LGBMClassifier(objective='multiclass',\n                          num_leaves=6,\n                          max_bin=200,\n                          verbose=-1,\n                          random_state=42)\n\nlightgbm_best_model, lightgbm_results = find_best_params(light_gbm_params, lightgbm, X_train, y_train, X_dev, y_dev)","e7f7338e":"best_lgbm = LGBMClassifier(objective='multiclass',\n                          num_leaves=15,\n                          max_bin=2800,\n#                           learning_rate=0.05,\n                          boosting_type='gbdt',\n                          verbose=-1,\n                          random_state=42)\nbest_lgbm.fit(X_train, y_train)\ny_pred = best_lgbm.predict(X_dev)\nprint(classification_report(y_dev, y_pred, digits=4))","ae3dfbc3":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest_cls = RandomForestClassifier(random_state=2021)\n\n\nrandom_forest_params = {'n_estimators': [1200, 1300, 1500, 1700, 2000, 2200, 2400, 2500, 2700, 3000, 3200],\n                        'criterion': ['gini', 'entropy'],\n                        'max_features': ['auto', 'sqrt', 'log2'],\n                        'class_weight': ['balanced', 'balanced_subsample']}\n_, random_forest_results = find_best_params(random_forest_params, random_forest_cls, X_train, y_train, X_dev, y_dev)","66e97718":"best_rfc = RandomForestClassifier(random_state=2021, class_weight='balanced', criterion='gini',max_features='auto', n_estimators= 2000)\nbest_rfc.fit(X_train, y_train)\nbest_rfc.predict(X_dev)\nprint(classification_report(y_dev, y_pred, digits=4))","fa7b278c":"from sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boost_params = {'n_estimators': [800, 1000, 1100, 1300, 1500, 1600, 2300, 2500],\n                         'learning_rate': [0.005, 0.01, 0.05, 0.1],\n                         'max_depth': [3,4, 5],\n                         'max_features': ['auto', 'sqrt', 'log2']}\ngradient_boost_model = GradientBoostingClassifier(random_state=2021)\n_, gradient_boost_results = find_best_params(gradient_boost_params, gradient_boost_model, X_train, y_train, X_dev, y_dev)","991704ef":"best_gdb = GradientBoostingClassifier(n_estimators=1300, \n                                    learning_rate=0.05,\n                                    max_depth=5,\n                                    max_features='auto',\n                                    random_state=2021)\nbest_gdb.fit(X_train, y_train)\ny_pred = best_gdb.predict(X_dev)\nprint(classification_report(y_dev, y_pred, digits=4))","4af00a07":"from sklearn.ensemble import StackingClassifier, VotingClassifier\n\ndef voting_ensemble(X_dev, lgbm, xgb, svc, gdb, rfc):\n    estimator_list = [('svc', svc),\n                      ('xgb', xgb),\n                      ('lgbm', lgbm),\n                      ('rfc', rfc),\n                      ('gdb', gdb)]\n\n    voting_ensemble = VotingClassifier(estimator_list,\n                                       voting='soft',\n                                       weights=[0.15, 0.15, 0.4, 0.1, 0.15],\n                                       n_jobs=-1)\n    voting_ensemble.fit(X_train, y_train)\n    voting_pred = voting_ensemble.predict(X_dev)\n    return voting_pred\n\ny_pred = voting_ensemble(X_dev, best_lgbm, best_xgb, best_svm_cls, best_gdb, best_rfc)\nprint(classification_report(y_dev, y_pred, digits=4))","8c9e0b6d":"def ensemble_models(X_dev, lgbm, xgb, svc, gdb, rfc):\n    svc_out = svc.predict_proba(X_dev)\n    xgb_out = xgb.predict_proba(X_dev)\n    lgbm_out = lgbm.predict_proba(X_dev)\n    gdb_out = gdb.predict_proba(X_dev)\n    rfc_out = rfc.predict_proba(X_dev)\n\n    ensemble_out = 0.4 * lgbm_out + 0.15 * xgb_out + 0.15 * svc_out + 0.15 * gdb_out + 0.1 * rfc_out\n    ensemble_predicts = np.argmax(ensemble_out, axis=1)\n\n    return ensemble_predicts\n\ny_pred = ensemble_models(X_dev, best_lgbm, best_xgb, best_svm_cls, best_gdb, best_rfc)\nprint(classification_report(y_dev, y_pred, digits=4))","1e911bf3":"X_test = build_data(test_car)","a99da0b2":"!pip install pyspark","5b81ae1b":"from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline","b4d23563":"df_train =spark.read.csv(\"..\/input\/week1-car-acceptability\/car_acc_train.csv\", header=True)\ndf_train = df_train.dropna()\ndf_test = spark.read.csv(\"..\/input\/it2034ch1502-car-acceptability-prediction\/test.csv\", header=True)","9c181ad7":"df_train.show(5)","e956621c":"indexers = [StringIndexer(inputCol=column, outputCol=column + \"_category\").fit(df_train) for column in\n            df_train.columns[1:-1]]\npipeline = Pipeline(stages=indexers)\ntransformer = pipeline.fit(df_train)\n\ntrain = transformer.transform(df_train)\ntest = transformer.transform(df_test)\n\nlabel_indexer = StringIndexer(inputCol='acceptability', outputCol='acceptability_category').fit(df_train)\ntrain = label_indexer.transform(train)\n\n# transform to feature\nfeature_assembler = VectorAssembler(inputCols=train.columns[8:14], outputCol=\"features\")\ntrain = feature_assembler.transform(train)\ntest_feature_assembler = VectorAssembler(inputCols=test.columns[7:13], outputCol=\"features\")\ntest = test_feature_assembler.transform(test)","60cb2c6e":"from sklearn.metrics import classification_report\nfrom sklearn import svm\n\nbest_svm_cls = svm.SVC(kernel='poly', C=10, random_state=2021, probability=True)\nbest_svm_cls.fit(X_train, y_train)\nsvc_predicts = best_svm_cls.predict(X_test)","197cfc3a":"svc_df = pd.DataFrame({'car_id': test_car['car_id'], 'acceptability': svc_predicts})\nsvc_df['acceptability'] = svc_df['acceptability'].map(label_demapper)","1240ca6c":"from pyspark.ml.classification import RandomForestClassifier","a6a002ac":"rdf = RandomForestClassifier(labelCol=\"acceptability_category\", featuresCol=\"features\", maxDepth=10, seed=465, numTrees=2000, impurity='gini')\nrdf_model = rdf.fit(train)\nrdf_predict = rdf_model.transform(test)","0d5aa747":"rdf_predict.select('car_id', 'prediction').show(5)","be415604":"rdf_predict['prediction'].unique()","8e9a1c2e":"rdf_predict = rdf_predict.toPandas()\nrdf_predict.rename({'prediction': 'acceptability'}, inplace=True, axis=1)\nlb_demapper = {0.0: 'unacc', 1.0: 'acc', 2.0: 'good', 3.0: 'vgood'}\nrdf_predict['acceptability'] = rdf_predict['acceptability'].map(lb_demapper)","34296ded":"rdf_predict.head()","4b3fbac4":"rdf_predict = rdf_predict[['car_id', 'acceptability']]","e5652234":"best_gdb = GradientBoostingClassifier(n_estimators=1300, \n                                    learning_rate=0.05,\n                                    max_depth=5,\n                                    max_features='auto',\n                                    random_state=2021)\nbest_gdb.fit(X_train, y_train)\nprint()\ngdb_pred = best_gdb.predict(X_test)","f6285dd7":"gdb_predict = pd.DataFrame({'car_id': test_car['car_id'], 'acceptability': gdb_pred})\ngdb_predict['acceptability'] = gdb_predict['acceptability'].map(label_demapper)","e1ff18da":"best_xgb =  xgb.XGBClassifier(n_estimators=1200,\n                              max_depth=5,\n                              learning_rate=0.1,\n                              seed=20)\nbest_xgb.fit(X_train, y_train)\nxgb_pred = best_xgb.predict(X_test)","677035b0":"xgb_predict = pd.DataFrame({'car_id': test_car['car_id'], 'acceptability': xgb_pred})\nxgb_predict['acceptability'] = xgb_predict['acceptability'].map(label_demapper)","03340158":"xgb_predict.head(), gdb_predict.head(), rdf_predict.head(), svc_df.head()","bc40a9a2":"len(svc_df)","00279355":"from collections import Counter","c5c05fa8":"def voting_classifer(xgb_predict, gdb_predict, rdf_predict, svc_df):\n    voting_output = pd.concat(\n        (xgb_predict, gdb_predict['acceptability'], rdf_predict['acceptability'], svc_df['acceptability']), axis=1)\n    data = []\n    for i, row in voting_output.iterrows():\n        counter = Counter()\n        counter = counter.update(row['acceptability'])\n        voting_label = max(counter, key=counter.get)\n        data.append((row['car_id'], voting_label))\n    data = pd.DataFrame(data, columns=['car_id', 'acceptability'])\n    return data","6c27f004":"# data = voting_classifer(xgb_predict, gdb_predict, rdf_predict, svc_df)\n# data.to_csv('submit.csv', index=False)\n\ny_pred = ensemble_models(X_dev, best_lgbm, best_xgb, best_svm_cls, best_gdb, best_rfc)\ntest_label = [label_demapper[w] for w in y_pred]\n\nsubmit_df = pd.DataFrame({'car_id': test_car['car_id'], 'acceptability': test_label})\nsubmit_df.to_csv('submit.csv', index=False)\n\n","1cf7b7bc":"### Safety","6d03c5c5":"### Maintenance Price","113d2e68":"## Gradient boosting classifier","6001b8ad":"### T\u0103ng c\u01b0\u1eddng d\u1eef li\u1ec7u","f8d3575f":"# Exploratory Data Analysis (EDA)","19f2ce20":"### Voting ensemble model","94a439bf":"#### Random search for finding best parameters","c5430058":"### SVM - linear\/non-linear","cdeed95a":"- t\u00ecm dataset c\u00f3 t\u00ednh ch\u1ea5t t\u01b0\u01a1ng \u0111\u01b0\u01a1ng\n- Extract t\u1ea5t c\u1ea3 c\u00e1c c\u00e2u trong t\u1eadp test ch\u01b0a \u0111\u01b0\u1ee3c g\u00e1n nh\u00e3n v\u00e0 \u0111\u1eb7t l\u00e0m t\u1eadp dev\n- Notebook: https:\/\/www.kaggle.com\/ppprabbit\/big-data-week1-data-augment","9398738b":"# Building model","72719270":"# Build pyspark model","b073b7fb":"## Gradient boosting model","264dde3a":"### Null count","3be1ec8b":"### Feature tranformation","77b11a37":"### SVC model","2cdfea09":"### Voting model ","59a63670":"## Get File for prediction","7b9c4cd4":"### Data preparation","03993fe4":"### Carry capacity","0a9960b5":"## Check feature interaction","c9d275b2":"### Buying price","8660de06":"## Random forest classifier","15929017":"## average output probs ensemble","d55eb2c7":"## XGBoost model","b3543a81":"## Danh s\u00e1ch nh\u00f3m\n\n+ Nguy\u1ec5n Xu\u00e2n V\u0129nh Ph\u00fa\n+ \u0110\u1ed7 Nh\u1eadt Kha\n+ Tr\u1ea7n Cao Kh\u00e1nh Ng\u1ecdc\n+ Ng\u00f4 Quang B\u1ea3o","c26ec252":"## Random Forest model","c73b5553":"## LightGBM ","5b2e8ad9":"### Trunk size","c249d594":"## XGBoost","2d7de139":"### Number of doors","58f4b797":"###  Ki\u1ec3m tra d\u1eef li\u1ec7u t\u1eadp train, test c\u00f3 tr\u00f9ng nhau kh\u00f4ng","7d9a6c91":"## Build models","de710636":"## Feature binarizing","bb45b63f":"## H\u01b0\u1edbng ti\u1ebfp c\u1eadn\n+ x\u00e2y d\u1ef1ng v\u00e0 t\u1ed1i \u01b0u tham s\u1ed1 tr\u00ean th\u01b0 vi\u1ec7n sklearn, v\u00e0 d\u00f9ng c\u00e1c tham s\u1ed1 thay \u0111\u1ec3 kh\u1edfi t\u1ea1o m\u00f4 h\u00ecnh Pyspark\n+ Output cu\u1ed1i c\u00f9ng c\u1ee7a m\u00f4 h\u00ecnh \u0111\u01b0\u1ee3c t\u00ednh b\u1eb1ng ch\u1ed3ng c\u00e1c model l\u00ean v\u1edbi nhau (ensemble)"}}