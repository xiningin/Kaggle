{"cell_type":{"7372b110":"code","6b9343d9":"code","94183fff":"code","6687e59a":"markdown","e7f9ec8e":"markdown","6ec15edc":"markdown","01da52b4":"markdown"},"source":{"7372b110":"import random\nimport numpy as np\nfrom keras import metrics, optimizers, Sequential\nfrom keras.layers import Dense\nfrom keras.losses import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n\ndef cube(x):\n    return x ** 3 + random.random()\n\n\nx = np.linspace(1.0, 5.0, num=1000).reshape((1000, 1))\ncube_fun = np.vectorize(cube)\ny = cube_fun(x)\nBATCH_SIZE = 128\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42,shuffle=True)\n\n\ndef alexnet_model():\n    alexnet = Sequential([\n        Dense(10, activation='relu', input_dim=1),\n        Dense(10),\n        Dense(1)])\n    return alexnet\n\n\ndef train_model(model):\n    model.compile(optimizer=optimizers.SGD(), loss=mean_squared_error)\n    model.fit(X_train, y_train,\n              batch_size=BATCH_SIZE,\n              epochs=100,\n              verbose=1,\n              validation_data=(X_test, y_test), callbacks=[])\n\n\nmodel = alexnet_model()\ntrain_model(model)\n","6b9343d9":"def train_model(model):\n    model.compile(optimizer=optimizers.RMSprop(), loss=mean_squared_error)\n    model.fit(X_train, y_train,\n              batch_size=BATCH_SIZE,\n              epochs=100,\n              verbose=1,\n              validation_data=(X_test, y_test), callbacks=[])\nmodel = alexnet_model()\ntrain_model(model)","94183fff":"def train_model(model):\n    model.compile(optimizer=optimizers.SGD(lr=0.00001), loss=mean_squared_error)\n    model.fit(X_train, y_train,\n              batch_size=BATCH_SIZE,\n              epochs=100,\n              verbose=1,\n              validation_data=(X_test, y_test), callbacks=[])\nmodel = alexnet_model()\ntrain_model(model)","6687e59a":"After changing the learning rate. We will get the right result.\nHowever, it is too dull to tune the learning rate to an appropriate value. So, in the neural network training, remember a rule-\"never to use the SGD optimizer, although SGD is easy to understand\".","e7f9ec8e":"Why did this error occur? After read some materials, I find that the adaptive learning rate is much helpful in this problem.\nIn this simple problem, if we use a large learning rate, the model parameters will change dramatically, which make a considerable loss value. If the loss value is too high, we will get a NAN error.\nSo, after knowing that, let us try a model with a small fixed learning rate.","6ec15edc":"The main idea of this passage is we should **never to use the SGD optimizer**.\nI used to love using the SGD optimizer until a problem occurred as below.\nI had built a simple model to fit the cube function, but I got a nan loss. ","01da52b4":"But when I use the RMSProp loss function, it will get the right result."}}