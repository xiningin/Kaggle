{"cell_type":{"08da6f7d":"code","4af46d9b":"code","1b291f6b":"code","b559cf75":"code","ee9cc29f":"code","5ee916c6":"code","b1d8e0b6":"code","caa8bcda":"code","2d55b0ef":"code","47ea9f05":"code","daf23c23":"code","71264a13":"code","3ea4ce83":"code","eeaa96da":"code","b407d34c":"code","fb087cd2":"code","1dd4d867":"code","5d9d1cb4":"code","cb78bca8":"code","360573e3":"code","713ec20c":"code","3452be79":"code","63d51fa5":"code","7927521c":"code","16e1d24c":"code","0903ce09":"code","72aa23b8":"code","45b703ce":"code","8d165ed2":"code","63a9732a":"markdown","591e0153":"markdown","46f12820":"markdown","f9be55be":"markdown","ddda8d88":"markdown","579be38f":"markdown","5ac38225":"markdown","36679d52":"markdown","3b301e3d":"markdown","6a30bf83":"markdown","e9b27a94":"markdown","a943d243":"markdown","c245bdba":"markdown","2f7aa626":"markdown","2866b1a9":"markdown","35595fb6":"markdown","466b1a46":"markdown","a38ae400":"markdown","160db62c":"markdown","966c1591":"markdown","22da6451":"markdown","8ea60829":"markdown","d8c616e1":"markdown","37cb47b2":"markdown","1490f24f":"markdown"},"source":{"08da6f7d":"import pandas as pd\nimport numpy as np\nimport re\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.get_backend()\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn import decomposition, ensemble\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import  precision_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer  # for bag of words\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import normalize\nimport operator\nfrom functools import reduce\n\n","4af46d9b":"metadata = pd.read_csv('..\/input\/gender-classifier-DFE-791531.csv', encoding='latin1')\n\nprint(metadata.info())\n","1b291f6b":"data_row = pd.read_csv('..\/input\/gender-classifier-DFE-791531.csv',usecols= [1,3,5,6,8,10,11,13,17,18,19,21],encoding='latin1')\ndisplay(data_row.head(10))","b559cf75":"\ndata = data_row.where((data_row['gender:confidence'] > 0.9) & (data_row['gender'] != 'unknown'))\n\n\n","ee9cc29f":"\nprint(\"Zmieniam gender: {'male':0, 'female':1, 'brand':2}\")\nprint(data.gender.head(5))\ndata['gender_label'] = data.gender.map({'male':0, 'female':1, 'brand':2})\nprint(data.gender_label.head(5))","5ee916c6":"#------------------------Kolory-------------------------\n\ndef hexToRGB(color):\n    if color == '0':\n        return 255,255,255\n    if len(color)<5:\n        return None, None, None\n    try:\n        r=int(color[0:2],16)\n        g=int(color[2:4],16)\n        b=int(color[4:6],16)\n    except (RuntimeError, TypeError, NameError, ValueError):\n        return None, None, None\n    else:\n        return r,g,b\n    \n    \n\nprint(\"Zmeniam kolory z postaci #RRGGBB w wersji hex na (rr,gg,bb) w wersji dec\")\nprint(data[\"link_color\"].head(5))\ndata[\"rl\"] = data[\"link_color\"].apply(lambda x: hexToRGB(str(x))[0])\ndata[\"gl\"] = data[\"link_color\"].apply(lambda x: hexToRGB(str(x))[1])\ndata[\"bl\"] = data[\"link_color\"].apply(lambda x: hexToRGB(str(x))[2])\ndata[\"rs\"] = data[\"sidebar_color\"].apply(lambda x: hexToRGB(str(x))[0])\ndata[\"gs\"] = data[\"sidebar_color\"].apply(lambda x: hexToRGB(str(x))[1])\ndata[\"bs\"] = data[\"sidebar_color\"].apply(lambda x: hexToRGB(str(x))[2])\nprint(data[['rl', 'gl','bl']].head(5))\ndata.dropna(inplace=True,axis=0)\n\n\n\nmale_top_sidebar_color = data[data['gender'] == 'male']['sidebar_color'].value_counts().head(10)\nmale_top_sidebar_color_idx = male_top_sidebar_color.index\nmale_top_color = male_top_sidebar_color_idx.values\n\nmale_top_color[1] = '000000'\nprint (male_top_color)\nl = lambda x: '#'+x\n\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \"#F5ABB5\"})\nplot3 = sns.barplot (x = male_top_sidebar_color, y = male_top_color, palette=list(map(l, male_top_color)))\nfig3 = plot3.get_figure()\nfig3.savefig(\"male_colours.jpg\")\n","b1d8e0b6":"female_top_sidebar_color = data[data['gender'] == 'female']['sidebar_color'].value_counts().head(10)\nfemale_top_sidebar_color_idx = female_top_sidebar_color.index\nfemale_top_color = female_top_sidebar_color_idx.values\n\nfemale_top_color[2] = '000000'\nprint (female_top_color)\nl = lambda x: '#'+x\n\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \"#F5ABB5\"})\nplot4 =sns.barplot (x = female_top_sidebar_color, y = female_top_color, palette=list(map(l, female_top_color)))\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \"#FFFFFF\"})\nfig4 = plot4.get_figure()\nfig4.savefig(\"female_colours.jpg\")","caa8bcda":"\ndef train_model(classifier, feature_vector_train, label, feature_vector_valid,valid_y, is_neural_net=False):\n    \"\"\"Model training\n    Keyword arguments:\n    classifier -- Klasyfikator posiadajacy metody fit,predict, predict_proba\n    feature_vector_train -- X_train\n    label -- Y_train\n    feature_vector_valid -- x_val\n    valid_y -- y_val\n    Returns:\n    accuracy -- dok\u0142adno\u015b\u0107 dopasowania,\n    predykcja_prawdopodobienstwa -- prawdopodobie\u0144stwo dopasowania dla ka\u017cdej klasy,\n    predykcja -- numerklasy najbardziej prawdopodobnej,\n    przetrenowany klasyfikator.\"\"\"\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    #only for neural networks\n    if is_neural_net:\n        classifier.fit(feature_vector_train, label,epochs = 2)\n        predictions = predictions.argmax(axis=-1)\n\n    return metrics.accuracy_score(predictions, valid_y),  classifier.predict_proba(feature_vector_valid), precision_score(y_test, classifier.predict(feature_vector_valid),average=None), classifier\n\n\n\n\n","2d55b0ef":"test_percentage = 0.15\nX_train, X_test, y_train, y_test = train_test_split(data[['rl','gl','bl','rs','gs','bs']], data['gender_label'], test_size=test_percentage, random_state=20)","47ea9f05":"\nprint(\"Szukam najlepszej liczby perceptronow miedzy 10 a 19 za wzorem:\")\nhidden_min = 4\nhidden_layer_size = 20\nprint(\"(no of inputs + no of outputs)^0.5 + (1 to 10)\")\nmlp_accuracy = [0]*(hidden_min+hidden_layer_size)\nfor i in range(hidden_min,hidden_min+hidden_layer_size):\n    accuracy, Color_predictions, color_precision, color_classifier = train_model(MLPClassifier(hidden_layer_sizes=(i), early_stopping=True, random_state = np.random.RandomState(47)),\n                                                               X_train, y_train, X_test, y_test)\n    mlp_accuracy[i] = accuracy\n    print(\"N iter= \", color_classifier.n_iter_ )\n\nfig5, ax = plt.subplots()\nax.plot(range(hidden_min,hidden_min+hidden_layer_size), mlp_accuracy[hidden_min:hidden_layer_size+hidden_layer_size], 'o')\nplt.ylabel('accuracy')\nplt.xlabel('n_perceptrons')\nplt.title('MLPClassifier accuracy')\nplt.show()  \n\nfig5.savefig(\"mlpclassifier_accuracy.jpg\")\n\naccuracy, Color_predictions,color_precision, color_classifier = train_model(MLPClassifier(hidden_layer_sizes=(np.argmax(mlp_accuracy)), early_stopping=True, random_state = np.random.RandomState(47)), X_train, y_train, X_test,y_test)\nprint(\"MLPClassifier(hidden_layer_sizes={0}) Color accuracy: {1}  \".format(np.argmax(mlp_accuracy),accuracy))","daf23c23":"hidden_layer_size = 20\nprint(\"Szukam najlepszej liczby perceptronow w dwu warstwach miedzy 10 a \", hidden_layer_size, \" za wzorem:\")\n\nprint(\"(no of inputs + no of outputs)^0.5 + (1 to \", hidden_layer_size,\")\")\nmlp_accuracy = [0]*(hidden_layer_size+hidden_layer_size)\nfor i in range(hidden_layer_size,hidden_layer_size+hidden_layer_size):\n    accuracy, Color_predictions, color_precision, color_classifier = train_model(MLPClassifier(hidden_layer_sizes=(i,i), early_stopping=True, learning_rate='invscaling', random_state = np.random.RandomState(47)),\n                                                               X_train, y_train, X_test, y_test)\n    mlp_accuracy[i] = accuracy\n    print(\"N iter= \", color_classifier.n_iter_ )\n\nfig5, ax = plt.subplots()\nax.plot(range(hidden_layer_size,hidden_layer_size+hidden_layer_size), mlp_accuracy[hidden_layer_size:hidden_layer_size+hidden_layer_size], 'o')\nplt.ylabel('accuracy')\nplt.xlabel('n_perceptrons')\nplt.title('MLPClassifier accuracy')\nplt.show()  \n\nfig5.savefig(\"mlpclassifier_accuracy_2_layers.jpg\")\n\naccuracy, Color_predictions,color_precision, color_classifier = train_model(MLPClassifier(hidden_layer_sizes=(np.argmax(mlp_accuracy),np.argmax(mlp_accuracy)), early_stopping=True,learning_rate='invscaling', random_state = np.random.RandomState(47)), X_train, y_train, X_test,y_test)\nprint(\"MLPClassifier(hidden_layer_sizes={0}) Color accuracy: {1}  \".format(np.argmax(mlp_accuracy),accuracy))","71264a13":"max_neighbours = 40\nprint(\"Szukam najlepszej liczby s\u0105siad\u00f3w miedzy 1 a \", max_neighbours)\n\nneighbours_accuracy = [0]*max_neighbours\nfor i in range(1,max_neighbours):\n    accuracy, Color_predictions, color_precision, color_classifier = train_model(KNeighborsClassifier(i), X_train, y_train, X_test,y_test)\n    neighbours_accuracy[i] = accuracy\n    \nfig6, ax = plt.subplots()\nax.plot(range(1,max_neighbours), neighbours_accuracy[1:max_neighbours], 'o')\nplt.ylabel('accuracy')\nplt.xlabel('n_neighbours')\nplt.title('KNeighborsClassifier accuracy')\nplt.show() \naccuracy, Color_predictions, color_precision, color_classifier = train_model(KNeighborsClassifier(np.argmax(neighbours_accuracy)), X_train, y_train, X_test,y_test)\nprint(\"KNeighborsClassifier({0}) Color accuracy:  {1} \".format(np.argmax(neighbours_accuracy), accuracy))\nfig5.savefig(\"KNeighborsClassifier_accuracy.jpg\")\n","3ea4ce83":"#----------------------------- Other Features --------------------------------\n\n# other_features = data [[\"_golden\" , \"_trusted_judgments\" , \"fav_number\", \"retweet_count\" , \"tweet_count\" , \"profile_yn:confidence\"]]\nX_train, X_test, y_train, y_test = train_test_split(normalize(data[[\"_golden\" , \"_trusted_judgments\" , \"fav_number\", \"retweet_count\" , \"tweet_count\" , \"profile_yn:confidence\"]], axis=0), data['gender_label'], test_size=test_percentage, random_state=20)\n\n\nprint(\"Szukam najlpeszej liczby perceptronow miedzy 10 a 19 za wzorem:\")\nhidden_layer_size = 10\nprint(\"(no of inputs + no of outputs)^0.5 + (1 to 10)\")\nmlp_accuracy = [0]*(hidden_layer_size+9)\nfor i in range(hidden_layer_size,hidden_layer_size+9):\n    accuracy, Other_Features, other_precision, other_classifier = train_model(\n        MLPClassifier(hidden_layer_sizes=(i), max_iter=200, early_stopping = True, random_state = np.random.RandomState(47)), X_train, y_train, X_test, y_test)\n    mlp_accuracy[i] = accuracy\n\naccuracy, Other_Features, other_precision, other_classifier = train_model(MLPClassifier(hidden_layer_sizes=(np.argmax(mlp_accuracy)),max_iter=200, early_stopping = True, random_state = np.random.RandomState(47)), X_train, y_train, X_test,y_test)\nprint(\"MLPClassifier(hidden_layer_sizes={0}) Other_Features accuracy: {1}  \".format(np.argmax(mlp_accuracy),accuracy))\n\n","eeaa96da":"def cleaning(s):\n    s = str(s)\n    s = s.lower()\n    s = s.replace(\"'s\",' is')\n    s = s.replace(\"'re\",' are')\n    s = s.replace(\"'ve\",' have')\n    s = re.sub('\\s\\W',' ',s) #whitespace characters\n    s = re.sub('\\W,\\s',' ',s)\n    s = re.sub(r'[^\\w]', ' ', s)\n    s = re.sub(\"\\d+\", \"\", s)\n    s = re.sub('\\s+',' ',s)\n    s = re.sub('[!@#$_]', '', s)\n    s = s.replace(\"\u00f9\",\"\")\n    s = s.replace(\"\u00f9\", \"\")\n    s = s.replace(\"\u00fb\", \"\")\n    s = s.replace(\"\u00e2\u00f9\", \"\")\n    s = s.replace(\"\u00fc\", \"\")\n    s = s.replace(\"\u00e5\", \"\")\n    s = s.replace(\"\u00e2\", \"\")\n    s = s.replace(\"\u00e4\", \"\")\n    s = s.replace(\"co\",\"\")\n    s = s.replace(\"https\",\"\")\n    s = s.replace(\",\",\"\")\n    s = s.replace(\"[\\w*\",\" \")\n    return s\n\n\n\nprint(\"Przykladowy wpis: \")\nprint(data.text.get(64))\nprint(\"\\n\")\ndata['Tweets'] = [cleaning(s) for s in data['text']]\ndata['Description'] = [cleaning(s) for s in data['description']]\n\nprint(\"Przykladowy wpis poczyszczeniu ze znakow specjalnych: \")\nprint(data.Tweets.get(64))\n\nprint(\"\\n\")\n\ndata.dropna(inplace=True,axis=0) # pozbywam sie wierszy bez danych NaN","b407d34c":"def stop_words(data, column_name,splitted=False):\n    #funcion deleting stop words \n    \n    stop = set(stopwords.words('english'))\n    notInStopSet = lambda word: word not in stop\n    if splitted == False:\n        data[column_name] = data[column_name].str.lower().str.split()\n\n    data[column_name] = data[column_name].apply(lambda x: [item for item in x if notInStopSet(item)])\n\n    if splitted == False:\n        data[column_name] = data[column_name].apply(lambda x: ' '.join(x))\n        \ndef stemming(data, column_name,splitted=False):\n    #funcion stemming \n    sno = nltk.stem.SnowballStemmer('english')\n    stop = set(stopwords.words('english'))\n    notInStopSet = lambda word: word not in stop\n    if splitted == False:\n        data[column_name] = data[column_name].str.lower().str.split()\n\n    data[column_name] = data[column_name].apply(lambda x: [sno.stem(item) for item in x if notInStopSet(item)])\n\n    if splitted == False:\n        data[column_name] = data[column_name].apply(lambda x: ' '.join(x))\n\n        \nstop_words(data, 'Tweets')\nstop_words(data, 'Description')\n\nprint(\"Przykladowy wpis  usunieciu stop words: \")\nprint(data.Tweets.get(64))\nprint(\"\\n\")        \n        \nstemming(data, 'Tweets')\nstemming(data, 'Description')\n\nprint(\"Przykladowy wpis po Stemmingu \")\nprint(data.Tweets.get(64))\nprint(\"\\n\")","fb087cd2":"Male = data[data['gender_label'] == 0]\nFemale = data[data['gender_label'] == 1]\nBrand = data[data['gender_label'] == 2]\nMale_Words = pd.Series(' '.join(Male['Tweets'].astype(str)).lower().split(\" \")).value_counts()[:20]\nFemale_Words = pd.Series(' '.join(Female['Tweets'].astype(str)).lower().split(\" \")).value_counts()[:20]\nBrand_words = pd.Series(' '.join(Brand['Tweets'].astype(str)).lower().split(\" \")).value_counts()[:20]\nplot0 = Female_Words.plot(kind='bar',stacked=True, colormap='OrRd', title='Most used words by female users')\nfig0 = plot0.get_figure()\nfig0.savefig(\"female_words.jpg\")","1dd4d867":"plot1 = Male_Words.plot(kind='bar',stacked=True, colormap='plasma', title='Most used words by male users' )\nfig1 = plot1.get_figure()\nfig1.savefig(\"male_words.jpg\")\n","5d9d1cb4":"plot2 = Brand_words.plot(kind='bar',stacked=True, colormap='Paired', title='Most used words by brands')\nfig2 = plot2.get_figure()\nfig2.savefig(\"brand_words.jpg\")","cb78bca8":"#------------------------TEXT ANALYSIS------------------------------------\n\n\ny_all = data.gender_label\nX_Tweets = data.Tweets\nX_Description = data.Description\n\n\nX_Tweets_train, X_Tweets_test, y_train, y_test = train_test_split(X_Tweets, y_all, test_size=test_percentage, random_state=20)\nX_Description_train, X_Description_test, y_train, y_test = train_test_split(X_Description, y_all, test_size=test_percentage, random_state=20)\n# print(y_test)Usredniona 64%\n\n#Unigramy\n\n\ndef vektoriser(X_train, X_test,ngram = False):\n    if ngram == False:\n        vect= CountVectorizer(analyzer='word', token_pattern=r'[0-9a-zA-Z]{2,}',min_df=2)\n    else:\n        vect = CountVectorizer(analyzer='word', token_pattern=r'[0-9a-zA-Z]{2,}', ngram_range=(1, 2),min_df=2)\n    vect.fit(X_train)\n    return vect.transform(X_train), vect.transform(X_test), vect\n\n# print(\"Typ danych = \" , type(X_Tweets_test))\nX_Tweets_train_dtm, X_Tweets_test_dtm, tweets_vect = vektoriser(X_Tweets_train, X_Tweets_test)\nprint(len(tweets_vect.vocabulary_))\n\nX_Description_train_dtm, X_Description_test_dtm, description_vect = vektoriser(X_Description_train,X_Description_test)\n\n\n\n\n\n\n","360573e3":"\n\naccuracy, Tweets_predictions, tweet_precision, tweet_classifier = train_model(MultinomialNB(), X_Tweets_train_dtm, y_train, X_Tweets_test_dtm, y_test)\nprint(\"NB, Tweets WordLevel:  \", accuracy)\n\n\n\naccuracy, Description_predictions, description_precision,description_classifier = train_model(MultinomialNB(), X_Description_train_dtm, y_train, X_Description_test_dtm,y_test)\nprint(\"NB, Description WordLevel:  \", accuracy)\n\nprobs = np.transpose(description_classifier.feature_log_prob_)\nwords = list(description_vect.vocabulary_.keys())\n# print(words)\nprobs = description_classifier.predict_proba(description_vect.transform(words))\nnot_log_probabs = [[i for i in x] for x in probs]\n# print(not_log_probabs)\n\nall_gender_words = {x[1]: np.argmax(x[0])  for x in zip(probs,words) if max(x[0]) > 0.7}\nMost_masculine_words = {key:value for key, value in all_gender_words.items() if value == 0}\nprint(\"Meskie slowa\", Most_masculine_words.keys())\nMost_masculine_words = {key:value for key, value in all_gender_words.items() if value == 1}\n\nprint('Kobiece slowa',  Most_masculine_words.keys())\nMost_masculine_words = {key:value for key, value in all_gender_words.items() if value == 2}\n\nprint('Brandowe slowa', Most_masculine_words.keys())\n\nimport sys\nfrom termcolor import colored, cprint\n\n\n\n","713ec20c":"def print_colorful_tweet(Tweet):\n     \n    if type(Tweet) == str:\n#         print('size == 1')\n        input_frase = Tweet.split()\n    else:\n#         print('size != 1')\n        input_frase = Tweet\n        \n    colors = {0:'\\033[34m', 1:'\\033[95m', 2:'\\033[32m'} # blue, pink, green\n    frase = str()\n    if not input_frase:\n        return \"\"\n    for word in input_frase:\n        if word in all_gender_words:\n            gender = all_gender_words.get(word)\n#             print(gender)\n            color = colors.get(gender)\n#             print(color)\n            frase += colors.get(gender) + word + '\\x1b[0m'+ \" \" \n        else:\n            frase += word + \" \"\n            pass\n    print(frase)\n\nprint_colorful_tweet(data.Description.get(64))\nprint_colorful_tweet(data.Description.get(75))\nprint_colorful_tweet(data.Description.get(94))\nprint_colorful_tweet(data.Description.get(67))\nprint_colorful_tweet(data.Description.get(86))\nprint_colorful_tweet(data.Description.get(84))","3452be79":"#Bigramy----------------------------------------------------------------------------------------------------------------------\n\nX_Tweets_train_dtm, X_Tweets_test_dtm, tweets_vect = vektoriser(X_Tweets_train, X_Tweets_test,ngram=True)\nX_Description_train_dtm, X_Description_test_dtm, description_vect = vektoriser(X_Description_train,X_Description_test,ngram=True)\n\n\naccuracy, Tweets_predictions, tweet_precision, tweet_classifier = train_model(MultinomialNB(), X_Tweets_train_dtm, y_train, X_Tweets_test_dtm,y_test)\nprint(\"NB, Tweets N-Gram Vectors:\", accuracy)\n# print(tweet_classifier.predict_proba((tweets_vect.transform(X_Tweets))[1]))\n\n\n\naccuracy, Description_predictions,description_precision,description_classifier  = train_model(MultinomialNB(), X_Description_train_dtm, y_train, X_Description_test_dtm,y_test)\nprint(\"NB, Description N-Gram Vectors:\", accuracy)","63d51fa5":"X_Tweets_train, X_Tweets_test, y_train, y_test = train_test_split(X_Tweets, y_all, test_size=test_percentage, random_state=20)\n\nencoder = preprocessing.LabelEncoder()\ntrain_y = encoder.fit_transform(y_train)\nvalid_y = encoder.fit_transform(y_test)\n\nxtrain_count =  X_Tweets_train_dtm\nxvalid_count =  X_Tweets_test_dtm\n\n\ntfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n\n\n\ntfidf_vect.fit(X_Tweets_train)\n\nxtrain_tfidf =  tfidf_vect.transform(X_Tweets_train)\nxvalid_tfidf =  tfidf_vect.transform(X_Tweets_test)\n\n\n# print(tfidf_vect.vocabulary_)\n\n# ngram level tf-idf\ntfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\ntfidf_vect_ngram.fit(X_Tweets_train)\nxtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_Tweets_train)\nxvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_Tweets_test)\n\n# characters level tf-idf\ntfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\ntfidf_vect_ngram_chars.fit(X_Tweets_train)\nxtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_Tweets_train)\nxvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_Tweets_test)\n\n\n\n# Naive Bayes on Word Level TF IDF Vectors\naccuracy = train_model(MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)[0]\nprint(\"NB, Tweets WordLevel TF-IDF: \", accuracy)\n\n# Naive Bayes on Ngram Level TF IDF Vectors\naccuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)[0]\nprint(\"NB, Tweets N-Gram Vectors: \", accuracy)\n\n# Naive Bayes on Character Level TF IDF Vectors\naccuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)[0]\nprint(\"NB, Tweets CharLevel Vectors: \", accuracy)\n\n# Linear Classifier on Count Vectors\naccuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count, valid_y)[0]\nprint(\"LR, Tweets Count Vectors: \", accuracy)\n\n# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)[0]\nprint(\"LR, Tweets WordLevel TF-IDF: \", accuracy)\n\n# Linear Classifier on Ngram Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)[0]\nprint(\"LR, Tweets N-Gram Vectors: \", accuracy)\n\n# Linear Classifier on Character Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)[0]\nprint(\"LR, Tweets CharLevel Vectors: \", accuracy)\n\n","7927521c":"# RF on Count Vectors\naccuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count, valid_y)\nprint(\"RF, Count Vectors: \", accuracy)\n\n# RF on Word Level TF IDF Vectors\naccuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\nprint(\"RF, WordLevel TF-IDF: \", accuracy)","16e1d24c":"def getPrediction(Probabs):\n    return [ np.argmax(x) for x in Probabs]\n\n\n\ntotal_Predictions = (tweet_precision*Tweets_predictions + description_precision*Description_predictions + color_precision*Color_predictions + other_precision*Other_Features)\ny_pred_class = getPrediction(total_Predictions)\n\n\nprint(\"Weighted mean bigrams: \" , metrics.accuracy_score(y_test, y_pred_class))","0903ce09":"total_Predictions = (tweet_precision*Tweets_predictions + description_precision*Description_predictions + color_precision*Color_predictions)\ny_pred_class = getPrediction(total_Predictions)\nprint(\"Weighted mean bigrams: \" , metrics.accuracy_score(y_test, y_pred_class))","72aa23b8":"\ndef myEnseblePredict(estimators, data, voting=\"Fuzzy voting\"):\n    \"\"\"Modular Neural Network Voting\n          zrodlo: Voting  Schemes For Cooperative  Neural  Network  Classifiers\n          https:\/\/pdfs.semanticscholar.org\/721c\/f173c00fff7af5edca9bd8976407591f647c.pdf\n          \n    Keyword arguments:\n    estimators -- lista par (Klasyfikator, kolumny) \n    data -- dane do predykcji\/klasyfikacji\n    voting -- rodzaj g\u0142owsowania\n    \n    Returns:\n    \n    result -- klasyfikacja\n    \"\"\"\n    \n    def prod(iterable):\n        \"\"\"iloczyn element\u00f3w listy\"\"\"\n        return reduce(operator.mul, iterable, 1)\n    \n    def plurality(row):\n        \"\"\"wyostrzanie listy [0.2, 0.6, 0.2] -> [0, 1, 0]\"\"\"\n        maxIdx = np.argmax(row)\n        result = [0 for x in row]\n        result[maxIdx] = 1\n        return result\n\n    def borda(row):\n        \"\"\"zliczanie g\u0142os\u00f3w typu border [0.1, 0.6, 0.3] -> [0, 2, 1]\"\"\"\n        count = len(row)\n        row_copy = row\n        maxIdx = [0]*count\n        result = [0 for x in row]\n        for  i in range(count-1):\n            maxIdx[i] = np.argmax(row_copy)\n            row_copy[maxIdx[i]] = 0\n        for i in range(count-1):\n            result[maxIdx[i]] = count - 1 - i\n        return result\n\n    votes = [[0,0,0] for x in data.index]\n    for el in estimators:\n        estimator = el[0]\n        columns = el[1]\n        dane = data.loc[:, data.columns.isin(columns)]\n        if len(el) == 3:\n            # print(\"Typ do wektoryzacji:\", type(data))\n            dane = el[2].transform(dane.T.squeeze())\n            # print(\"Dane po wektoryzacji:\" , dane)\n\n        temp = estimator.predict_proba(dane)\n        # print(temp)\n        if voting == \"plurality\":\n            temp = [plurality(x) for x in temp]\n            # print(temp)\n        if voting == \"Borda _count_voting\":\n            temp = [borda(x) for x in temp]\n        if voting == \"Fuzzy voting\":\n            pass\n\n        if voting == \"Nash voting\":\n            votes = [list(map(prod, zip(*t))) for t in zip(votes, temp)]\n        else:\n            votes = [list(map(sum, zip(*t))) for t in zip(votes, temp)]\n\n    # print(\"Final votes:\" , votes)\n    result = [np.argmax(vote) for vote in votes]\n    return result\n","45b703ce":"X_train, X_test, y_train, y_test = train_test_split(data, data['gender_label'], test_size=test_percentage, random_state=20)","8d165ed2":"\n\n\n\n\nplurality_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']), ( other_classifier,[\"_golden\" , \"_trusted_judgments\" , \"fav_number\", \"retweet_count\" , \"tweet_count\" , \"profile_yn:confidence\"]), ( tweet_classifier, [\"Tweets\"], tweets_vect), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='plurality')\nprint(\"Podejscie pluralistyczne: \" , metrics.accuracy_score(y_test, plurality_votes))\n\nborda_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']), ( other_classifier,[\"_golden\" , \"_trusted_judgments\" , \"fav_number\", \"retweet_count\" , \"tweet_count\" , \"profile_yn:confidence\"]), ( tweet_classifier, [\"Tweets\"], tweets_vect), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='Borda _count_voting')\nprint(\"Podejscie Borda count voting: \" , metrics.accuracy_score(y_test, borda_votes))\n\nborda_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']), ( other_classifier,[\"_golden\" , \"_trusted_judgments\" , \"fav_number\", \"retweet_count\" , \"tweet_count\" , \"profile_yn:confidence\"]), ( tweet_classifier, [\"Tweets\"], tweets_vect), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='Fuzzy voting')\nprint(\"Podejscie Fuzzy voting voting: \" , metrics.accuracy_score(y_test, borda_votes))\n\nborda_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']), ( other_classifier,[\"_golden\" , \"_trusted_judgments\" , \"fav_number\", \"retweet_count\" , \"tweet_count\" , \"profile_yn:confidence\"]), ( tweet_classifier, [\"Tweets\"], tweets_vect), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='Nash voting')\nprint(\"Podejscie Nash voting voting: \" , metrics.accuracy_score(y_test, borda_votes))\n\n\nprint(\"Rezygnujemy z Other_Features:\")\n\nplurality_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']),  ( tweet_classifier, [\"Tweets\"], tweets_vect), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='plurality')\nprint(\"Podejscie pluralistyczne: \" , metrics.accuracy_score(y_test, plurality_votes))\n\nborda_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']), ( tweet_classifier, [\"Tweets\"], tweets_vect), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='Borda _count_voting')\nprint(\"Podejscie Borda count voting: \" , metrics.accuracy_score(y_test, borda_votes))\n\n\n\nborda_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='Nash voting')\nprint(\"Podejscie Nash voting voting: \" , metrics.accuracy_score(y_test, borda_votes))\n\nborda_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']), ( tweet_classifier, [\"Tweets\"], tweets_vect), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='Fuzzy voting')\nprint(\"Podejscie Fuzzy voting voting: \" , metrics.accuracy_score(y_test, borda_votes))\n\n\n\nresults = confusion_matrix(y_test, y_pred_class)\nprint('Confusion Matrix :')\nprint(results)\n# print ('Accuracy Score :',accuracy_score(y_test, y_pred_class))\nprint('Report : ')\nprint(classification_report(y_test, y_pred_class))\n\n\n\nnull_accuracy = y_test.value_counts().head(1) \/ len(y_test)\nprint('Null accuracy:', null_accuracy)\n\n# Manual calculation of null accuracy by always predicting the majority class\nprint('Manual null accuracy:',(1172 \/ (1172 + 1084 + 849)))\n\n\n","63a9732a":"**\u0141adowanie danych**\n\nWszystkie dane z podsumowaniem.","591e0153":"**Analiza tekstu**","46f12820":"*Naiwna metoda Bayesa*\n\n   | Text         |        \t      Tag|\n       |--|--|\n    |\u201cA great game\u201d            \t|Male|\n    |\u201cThe election was over\u201d       | Female|\n    |\u201cVery clean match\u201d           \t|Male|\n    |\u201cA clean but forgettable game\u201d \t|Male|\n    |\u201cIt was a close election\u201d \t  | Female|\n    \nChemy policzy\u0107 prawdopodobie\u0144stwo, \u017ce s\u0142owa: \"A very close game\" wypowiedzia\u0142 m\u0119\u017cczyzna\n$$P(Male |\\; A\\;very\\; close\\; game) = \\frac{P( A\\;very\\; close\\; game |\\; Male) \\cdot P(Male)}{P(A\\;very\\; close\\; game )}$$\n\nZale\u017cy nam jednak jedynie aby por\u00f3wna\u0107 prawdopodobie\u0144stwo, \u017ce powiedzia\u0142 to m\u0119\u017cczyzna z prawdopodobie\u0144stwoem, \u017c\u0119 zrobi\u0142a to kobieta lub wpis nie pochodzi\u0142 od cz\u0142owieka. Pomijamy zatem mianownik.\n\n\nLiczymy jedynie: $$P( A\\;very\\; close\\; game |\\; Male) \\cdot P(Male)$$\n\n\nTeraz wyja\u015bni si\u0119, dlaczego metoda jest \"naiwna\", ot\u00f3\u017c przyjmuje si\u0119, \u017ce:\n$$P( A\\;very\\; close\\; game |\\; Male) = P(A |\\; Male) \\cdot P(very |\\; Male) \\cdot P(close |\\; Male) \\cdot P(game |\\; Male)$$\n\n    \n    \n    \n    \n    ","f9be55be":"Ogl\u0105damy dane:","ddda8d88":"**Analiza pozosta\u0142ych features**","579be38f":"2.  *Plurality \nvoting. *\n\n\nIt \nis \nthe \nmost  common \nvoting \nscheme. \nEach  voter  votes \nfor \none \nalternative, \nand the \nalternative  with \nthe \nlargest \nnumber \nof \nvotes  wins \n[9]. \nThe \nadvantage \nof \nthis \nscheme, \nfrom \nthe \nNN \nperspective,  is \nthat \nit \nonly \nuses \nthe \nhighest \noutput \nvalue, \nwhich \nis \nthe \nmost probable \noutput \nto \nbe true. \nHowever, \nit \ndoes \nnot \nconsider \nthe outputs\u2019 \npreferences. \n\n\n\nPrzyk\u0142ad:\n\nSie\u0107 A                    [0.2, **0.6**, 0.1]\n\nSie\u0107 B                    [0.1, **0.5**, 0.4]\n\nSie\u0107 C                    [**0.6**, 0.3, 0.1]\n\n\nObliczamy: \n         [0, 1, 0] +  [0, 1, 0] + [1, 0, 0] = [1, **2**, 0] \n \n        Wynik = [0 , 1 , 0]\n        \n 3. *Borda \ncount \nvoting. *\n\n\nFor \nm \nalternatives, \nassign \n \n\n1 \npoints \nfor \nthe \nalternative  ranked \nfirst, \n \n2 \npoints \nfor \nthe \nsecond, \n \nand \nso \non. \n\n\n\nThe \nalternative \nranked \nlast \nreceives \nzero \npoints. \nThe \npoints \ngiven \nto \nthe \ndifferent \nalternatives \n(modules) \nare summed \nup \nand \nthe \nhighest \nis \nthe \nwinner.\n\nPrzyk\u0142ad:\n\nSie\u0107 A                    [0.2, **0.6**, 0.1]\n\nSie\u0107 B                    [0.1, **0.5**, 0.4]\n\nSie\u0107 C                    [**0.6**, 0.3, 0.1]\n\n\nObliczamy: \n         [1, 2 0] +  [0, 2, 1] + [2, 1, 0] = [3, **5**, 1] \n \n        Wynik = [0 , 1 , 0]\n        \n4. *Fuzzy voting.* \n \nEach  voter  assigns \na \nnumber \nbetween zero \nand \none \nfor each \ncandidate. \nCompare \nthe summation \nof \nthe \nvotes\u2019 values for \nall \ncandidates. \nThe \nhigher \nis \nthe \nwinner.   And \nsince \nthe \nmodules\u2019 \noutputs \nare \nnormally \nbetween \nthe \nzero \nand \none \nvalues, \nthe \nvalue \nwill \nrepresent \nthe true \nvoting bid.         \n\nPrzyk\u0142ad:\n\nSie\u0107 A                    [0.2, **0.6**, 0.1]\n\nSie\u0107 B                    [0.1, **0.5**, 0.4]\n\nSie\u0107 C                    [**0.6**, 0.3, 0.1]\n\n\nObliczamy: \n          [0.2, **0.6**, 0.1] + [0.1, **0.5**, 0.4]+  [**0.6**, 0.3, 0.1]= [0.9, **1.4**, 0.6] \n \n        Wynik = [0 , 1 , 0]\n        \n5. *Nash \nvoting.* \nIt \nis similar \nto \nthe \nFuzzy \nvoting \nbut \ncompares \nthe \nproduct \nof \nthe \nvotes\u2019 values \nfor \nall candidates. \nThe \nhigher \nis  the \nwinner. \n\n\nPrzyk\u0142ad:\n\nSie\u0107 A                    [0.2, **0.6**, 0.1]\n\nSie\u0107 B                    [0.1, **0.5**, 0.4]\n\nSie\u0107 C                    [**0.6**, 0.3, 0.1]\n\n\nObliczamy: \n          [0.2, **0.6**, 0.1] .* [0.1, **0.5**, 0.4].*  [**0.6**, 0.3, 0.1]= [0.012, **0.09**, 0.004] \n \n        Wynik = [0 , 1 , 0]\n        ","5ac38225":"\n\nB\u0119dziemy g\u0142\u00f3wnie korzysta\u0107 z *naiwnej metody Bayesa.*\nW skr\u00f3cie:\n\nZliczamy ile jest r\u00f3\u017cnych s\u0142\u00f3w:\n\nPrzyk\u0142ad:\n\n    \u201cA great game\u201d            \n\n    \u201cA clean but forgettable game\u201d \n\nPo wektoryzacji unigramowej:\n\n\n                                       A   great  game  clean  but  forgettable  \n    \u201cA great game\u201d                    [1    1     1      0     0       0     ]\n\n    \u201cA clean but forgettable game\u201d    [1    0     1      1     1       1     ]\n\n\n","36679d52":"Model Perceptronowy Multi-layer Perceptron classifier.\n\nJedna warstwa HIDDEN.\n\nPr\u00f3bujemy r\u00f3\u017cnej liczby perceptron\u00f3w za wzorem:\n\n    (no of inputs + no of outputs)^0.5 + (1 to 20)","3b301e3d":"Podzia\u0142 na dane testowe i ucz\u0105ce.\n\n15% danych do testow\n85% do uczenia","6a30bf83":"Powtarzam zabaw\u0119 dla metody Nearest Neighbours,\n\nposzukuj\u0119 najlepszej liczby s\u0105siad\u00f3w mi\u0119dzy 1 a 20.","e9b27a94":"Dzielimy dane na testowe i ucz\u0105ce.","a943d243":"**Wybieramy cechy do analizy**\n\nWybieramy do analizy kolumny:\n\n|Kolumna | Opis|\n|--|--|\n|  _golden   |                       czy klient ma konto gold|\n| _trusted_judgments  |           number of trusted judgments|\n|  gender:\t        |                        one of male, female, or brand (for non-human profiles)|\n|  gender_confidence\t  |      a float representing confidence in the provided gender|\n|  profile_yn:confidence |      confidence in the existence\/non-existence of the profile|\n|retweet: \t      |                number of times the user has retweeted (or possibly, been retweeted)| \n|  text:    \t    |                    text of a random one of the user's tweets|\n|  tweet_count:       |      \t number of tweets that the user has posted|\n|  description: \t  |           the user's profile description|\n|  fav number:       |       \t number of tweets the user has favorited |\n| link_color| |\n| sidebar_color| | \n","c245bdba":"Zmieniamy etykiety na liczby:\n\n'male'    $ \\Rightarrow$ 0, \n\n'female'    $ \\Rightarrow$ 1, \n\nbrand'    $ \\Rightarrow$ 2","2f7aa626":"**Czyszczenie danych**\n\nUsuwamy elementy o nieznanym gender i gender:confidence wiekszym niz 90%.\n","2866b1a9":"**Modularne Sieci Neuronowe - Modular neural network**\n\n*\u201cVoting\u201d, *\n\n\nto najpopularniejsza strategia dla multi-module cooperation. \nOutput ka\u017cdego modu\u0142u, wskazuj\u0105cy na konkretn\u0105 klas\u0119 traktowany jest jak jeden g\u0142os. Decyzja podejmowana jest zgodnie z przyj\u0119t\u0105 strategi\u0105 g\u0142osowania -  \u201cvoting strategy.\u201d \n\n\n\u017br\u00f3d\u0142o: https:\/\/pdfs.semanticscholar.org\/721c\/f173c00fff7af5edca9bd8976407591f647c.pdf\n\n*1.  \u015arednia wa\u017cona:*\n\n\nPrzyk\u0142ad:\n\nSie\u0107 A o dok\u0142adno\u015bci 50%   [0.2, 0.6, 0.1]\nSie\u0107 B o dok\u0142adno\u015bci 90%   [0.1, 0.5, 0.4]\n\nObliczamy: \n        50% * [0.2, 0.6, 0.1] + 90%  *  [0.1, 0.5, 0.4] =  [0.1, 0.3, 0.05] +  [0.09, 0.45, 0.36]\n \n        Wynik = [0.19,0.75,0.41]","35595fb6":"Pr\u00f3bujemy pomin\u0105\u0107 najmniej efektywn\u0105 sie\u0107 - Other Features:","466b1a46":"Pewne ulepszenie do modelu mo\u017cna wprowadzi\u0107 stosuj\u0105 n-gramy:\n\n* unigram (1-gram)\n\n|A| clean |but| forgettable| game  |\n|--|--|--|--|--|\n\n* bigram (2-gram)\n\n|A clean | clean but| but  forgettable| forgettable game  |\n|--|--|--|--|--|\n\n* trigram (3-gram)\n\n|A clean but | clean but  forgettable| but forgettable game  |\n|--|--|--|\n\n.\n\n","a38ae400":"Co mo\u017cna jeszcze zrobi\u0107 (where to go next)\n\n*Lepsza miara dok\u0142adno\u015bci modelu - accuracy measure\n\nhttp:\/\/regulatorygenomics.upf.edu\/courses\/Master_AGB\/2_ClassificationAlgorithms\/Lecture_Accuracy.pdf\n\nstrony: 28-32\n\n* Rozdzielic ceche gender na human, sex\nMoznaby zobaczy\u0107, jak zachodzi odr\u00f3\u017cnanie cz\u0142owieka od firmy, kobiety od m\u0119\u017cczyzny je\u015bli wiadomo, \u017ce jest to cz\u0142owiek","160db62c":"Definuj\u0119 funkcj\u0119 train_model, kt\u00f3ra b\u0119dzie za mnie robi\u0107 uczenie.","966c1591":"*TF-IDF*  *Term frequency - Inverse document frequency\n*\n\n\n\n\n   | Text         |        \t      Tag|\n       |--|--|\n    |\u201cA great game\u201d            \t|Male|\n    |\u201cThe election was over\u201d       | Female|\n    |\u201cVery **clean** match\u201d           \t|Male|\n    |\u201cA **clean** but forgettable game\u201d \t|Male|\n    |\u201cIt was a close election\u201d \t  | Female|\n    \n    \n\nObliczanie Term frequency\n$$TF(clean, \\;A\\; clean\\; but\\; forgettable\\; game) =  20\\%$$\nObliczanie Inverse document frequency\n$$ IDF(clean) = \\ln{\\frac{wszystkich\\; wpis\u00f3w}{wpis\u00f3w\\; zawieraj\u0105cych\\; s\u0142owo\\; \"clean\"}} =\\ln(\\frac{5}{2}) $$","22da6451":"\n**Wst\u0119p**\n\nAnalizujemy dane dotycz\u0105ce u\u017cytkownik\u00f3w Twittera.\n\n*Co uda\u0142o si\u0119 zrobi\u0107?*\n\n\n1.     Popracowa\u0107 z r\u00f3\u017cnymi klasyfikatorami i przygotowa\u0107 sobie metod\u0119 do sprawnej analizy: \n         \n             train_model (classifier, X_train, y_train,X_test, y_test):\n\nGdzie:\n\n     class classifier:\n\n          def fit\n              ...\n\n          def predict\n              ...\n\n          def  predict_proba\n               ...\n\n\nPrzyklady:\n\n* MLP Multi-layer Perceptron classifier\n* KNeighborsClassifier\n* Random Forests\n\n\n2.  Przeprowadzi\u0107 analiz\u0119 tekstu\n\n * cleaning\n * Stemming\n * stopwords\n * wektoryzacja (unigramy, n-gramy)\n * MultinomialNB()\n * TF-IDF  Term frequency - Inverse document frequency\n\n3.  Modularne Sieci Neuronowe - Modular neural network\n\n\n*      voting strategy\n*   myEnseblePredict(estimators, data, voting=\"Fuzzy voting\")\n\n\n4. Ocena modelu\n\n\n\n*   Null accuracy\n\n\n\n\n\n\n\n","8ea60829":"**Stemming**\n\n\nPrzeksztacamy odmienione s\u0142owa na formy podstawowe:\n\n\nam, are, is $\\Rightarrow$ be\n\n\ncar, cars, car's, cars' $\\Rightarrow$ car ","d8c616e1":"**Random Forests**\n\n![obraz.png](https:\/\/blog.citizennet.com\/hs-fs\/hubfs\/Imported_Blog_Media\/RF.jpg?t=1536614411555&width=2356&name=RF.jpg)","37cb47b2":"**Kolory**\n\n\n\nB\u0119dziemy dokonywa\u0107 predykcji gender na podstawie dwu kolor\u00f3w:\n\n    koloru panelu bocznego\n\n    koloru linku\n\nRozpoczynamy od przekszta\u0142cenia koloru w wersji #RRGGBB  na  (r,g,b)","1490f24f":"**\u0141adowanie bibliotek**"}}