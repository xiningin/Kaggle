{"cell_type":{"a15b6fee":"code","6d335689":"code","15592075":"code","a17750ae":"code","39383c42":"code","94c4c769":"code","37695a41":"code","234768b6":"code","d4b748f4":"code","41b1537e":"code","cb29ef26":"code","4abbd0aa":"code","fdcd1c47":"code","1edf2027":"code","263bc66b":"code","24174e90":"code","b08f1f55":"code","f8fd89b2":"code","5672a624":"code","d6f99dd6":"code","e2ab99e7":"code","61271964":"code","632a98d9":"code","df25a305":"code","d251aeed":"code","b19cc28d":"code","8e84c1b8":"code","1b1ddbad":"code","583f9e5b":"code","52e15c12":"code","041b2acf":"code","26514fc4":"code","6430481c":"code","08f3e753":"code","e555d583":"code","8d75c60f":"code","0806c424":"code","eecc55bd":"code","53a1ea8a":"code","64e30360":"code","3f87000b":"code","f077300d":"code","ec507f19":"code","43404081":"code","c23a0cdb":"markdown","06c16ea2":"markdown","45029793":"markdown","337e40c5":"markdown","6cb49b52":"markdown","215b2bbc":"markdown","cc1c6ae6":"markdown","f18085f1":"markdown","2bc46a09":"markdown","cd3cf0ed":"markdown","63a23052":"markdown","fee35d03":"markdown","a75cf414":"markdown","9b256722":"markdown","edee9c22":"markdown","ad580bca":"markdown","c675b365":"markdown","ef44878c":"markdown","0a19b7b4":"markdown","837b4c54":"markdown","f9335d11":"markdown","6d12eef8":"markdown","af51b6c7":"markdown","62db8a18":"markdown","4e35daa9":"markdown","44a826ce":"markdown","d60f44d3":"markdown","7181a630":"markdown","c125202e":"markdown","7c5c923c":"markdown","a58716e4":"markdown","5c626411":"markdown","4a8a8c10":"markdown","00428378":"markdown","5158bfed":"markdown","3be9fbe9":"markdown","c08f6190":"markdown","ae2c0476":"markdown","8cd67a23":"markdown","7a45e284":"markdown","99c8a2db":"markdown","f46f43af":"markdown","23df507c":"markdown","fba8a9d0":"markdown","0790933d":"markdown","6cc2b4a2":"markdown","3d7c38ca":"markdown","a8ff4b2e":"markdown","b7a6899b":"markdown","192976d0":"markdown","564162bb":"markdown","598fe25c":"markdown","2ac7d6ab":"markdown"},"source":{"a15b6fee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6d335689":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\nimport operator\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nimport re\nimport nltk\nimport pandas as pd\nfrom collections import Counter\nfrom itertools import chain\nimport tensorflow as tf\nfrom keras.optimizers import Nadam\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.metrics import confusion_matrix, classification_report\n","15592075":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","a17750ae":"train.head()","39383c42":"def getNullColDict(train,columns):\n    null_count = []\n    for column in columns:\n        null_count.append((train[column].isnull().sum())*100\/(train.shape[0]))\n    \n    dict_percentage = dict(zip(columns,null_count))\n    return dict_percentage\n\ncolumns = list(train.columns[1:])\ndict_percentage = getNullColDict(train,columns)   \ndict_percentage = dict(sorted(dict_percentage.items(), key=lambda x: x[1]))","94c4c769":"def barplot(X,Xlabel,Y,Ylabel,title,size=(10,10)):\n    plt.figure(figsize=size)\n    sns.set(style=\"whitegrid\")\n    plt.title(title,fontsize=14,fontweight=\"bold\")\n    plt.xlabel(Xlabel,fontsize=14,fontweight=\"bold\")\n    plt.ylabel(Ylabel,fontsize=14,fontweight=\"bold\")\n    ax = sns.barplot(x=X, y=Y,palette=\"rocket\")\n    plt.show()\nbarplot(list(dict_percentage.keys()),'Feature',list(dict_percentage.values()),'Percentage', 'Missing Feature Values Percentage',size=(8,8))","37695a41":"labels = ['Real Disaster', 'Not Disaster']\namount = []\namount.append(train.loc[train.target == 1].shape[0]\/train.shape[0]*100)\namount.append(train.loc[train.target == 0].shape[0]\/train.shape[0]*100)\nbarplot(labels,'Target',amount,'Percentage', 'Target Classes Percentage')","234768b6":"plt.figure(figsize=(20,15))\ndf = train.keyword.value_counts().to_frame()\nbarplot(list(df.keyword)[0:20],'Kewords',list(df.index)[0:20],'Count', 'Top 10 Most Frequent Keywords',(20,10))","d4b748f4":"def getDisasterDict(df,train):\n    positive = []\n    negative = []\n    for key in list(df.index[0:20]):\n        positive.append(train.loc[(train.keyword == key) & (train.target == 1)].shape[0])\n        negative.append(train.loc[(train.keyword == key) & (train.target == 0)].shape[0])\n    dict_top10 = {'type': list(df.index[0:20]), 'positives': positive, 'negatives': negative}\n    return dict_top10\n\ndict_top10 = getDisasterDict(df,train)","41b1537e":"barWidth = 0.25\n\nbars1 = dict_top10['positives']\nbars2 = dict_top10['negatives']\n \nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\n\nplt.figure(figsize=(25,15))\nplt.bar(r1, bars1, color='#7f6d5f', width=barWidth, edgecolor='white', label='Real Disasters')\nplt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='Not Disasters')\n \nplt.title(\"Top 10 Disaster Ocurrances - Real or Not\", fontsize=14, fontweight='bold')\nplt.xlabel('Disaster Keyword', fontweight='bold')\nplt.ylabel('Count', fontweight='bold')\nplt.xticks([r + barWidth for r in range(len(bars1))], list(dict_top10['type']))\n \nplt.legend()\nplt.show()","cb29ef26":"def getNumberWords (row):\n    return len(row.text.split())\n\ndef getNumberUnique(row):\n    return len(set(str(row).split()))\n\ndef getMeanLength(row):\n    words = row.split()\n    return sum(len(word) for word in words) \/ len(words)\n\nfrom nltk.corpus import stopwords    \nstop_words = set(stopwords.words('english'))\ntrain['stopwords'] = train['text'].str.split().apply(lambda x: len(set(x) & stop_words))\ntrain.head()","4abbd0aa":"f, axes = plt.subplots(2, 2,figsize=(20,10))\ntrue_disaster = train[train.target == 1]['text'].str.len()\nfalse_disaster = train[train.target == 0]['text'].str.len()\n#plt.figure(figsize=(20,10))\n#\"Character Number in Tweets and Disaster Veracity\"\naxes[0][0].set_title(\"Character Number in Tweets and Disaster Veracity\",fontsize=14,fontweight='bold')\naxes[0][0].set_xlabel('Tweet Length',fontsize=14, fontweight='bold')\naxes[0][0].set_ylabel('Probability',fontsize=14, fontweight='bold')\nsns.distplot( true_disaster , color=\"blue\", label='True Disaster',ax=axes[0][0])\nsns.distplot( false_disaster , color=\"red\", label=\"Not Disaster\",ax=axes[0][0])\naxes[0][0].legend()\n\ntrain['number_words_real'] = train[train.target == 1].apply(lambda row: getNumberWords(row), axis=1)\ntrain['number_words_fake'] = train[train.target == 0].apply(lambda row: getNumberWords(row), axis=1)\n\naxes[0][1].set_title(\"Tweet Number of Words and Disaster Veracity\", fontsize=14,fontweight='bold')\naxes[0][1].set_xlabel('Number of Words',fontsize=14,fontweight='bold')\naxes[0][1].set_ylabel('Probability', fontsize=14,fontweight='bold')\nsns.distplot( list(train['number_words_real']) , color=\"blue\", label='True Disaster',ax=axes[0][1])\nsns.distplot( list(train['number_words_fake']) , color=\"red\", label=\"Not Disaster\",ax=axes[0][1])\naxes[0][1].legend()\n\n\ntrain['unique_words'] = train.text.apply(lambda row: getNumberUnique(row))\n\naxes[1][0].set_title(\"Number of Unique Words Distribution Plot\", fontsize=14,fontweight='bold')\naxes[1][0].set_xlabel('Number of unique words', fontsize=14,fontweight='bold')\naxes[1][0].set_ylabel('Probability', fontsize=14,fontweight='bold')\nsns.distplot( list(train[train.target == 1].unique_words) , color=\"blue\", label='True Disaster',ax=axes[1][0])\nsns.distplot( list(train[train.target == 0].unique_words) , color=\"red\", label=\"Not Disaster\",ax=axes[1][0])\naxes[1][0].legend()\n\n\naxes[1][1].set_title(\"Number of Stp Words Distribution Plot\", fontsize=14,fontweight='bold')\naxes[1][1].set_xlabel('Number of stop words', fontsize=14,fontweight='bold')\naxes[1][1].set_ylabel('Probability', fontsize=14,fontweight='bold')\nsns.distplot( list(train[train.target == 1].stopwords) , color=\"blue\", label='True Disaster',ax=axes[1][1])\nsns.distplot( list(train[train.target == 0].stopwords) , color=\"red\", label=\"Not Disaster\",ax=axes[1][1])\naxes[1][1].legend()\nf.subplots_adjust(hspace=0.3)\nplt.show()","fdcd1c47":"import re\ndef elementCount(row,string):\n    if string == '#':\n        return len(re.findall(r\"#(\\w+)\", row))\n    else:\n        return len(re.findall(r\"@(\\w+)\", row))\n\ntrain['hashtag_count'] = train.text.apply(lambda row: elementCount(row,'#'))\ntrain['at_count'] = train.text.apply(lambda row: elementCount(row,'@'))","1edf2027":"train.describe()","263bc66b":"train[train.target == 1].hashtag_count","24174e90":"print(\"Correlation between target and hashtag_count \", train.target.corr(train.hashtag_count))\nprint(\"Correlation between target and at_count \", train.target.corr(train.at_count))","b08f1f55":"hashtags_real_disasters = []\nhashtags_fake_disasters = []\nfor row in range(train.shape[0]):\n    if row < train[train.target == 1].shape[0]:\n        hashtags_real_disasters.extend(re.findall(r\"#(\\w+)\", train[train.target == 1].reset_index().text[row]))\n    if row < train[train.target == 0].shape[0]:\n        hashtags_fake_disasters.extend(re.findall(r\"#(\\w+)\", train[train.target == 0].reset_index().text[row]))\n        \nhashtags_real_disasters = [x.lower() for x in hashtags_real_disasters]\nhashtags_fake_disasters = [x.lower() for x in hashtags_fake_disasters]\n\nhashtags_real_disasters = dict(Counter(hashtags_real_disasters))\nhashtags_real_disasters = dict(sorted(hashtags_real_disasters.items(), key=operator.itemgetter(1),reverse=True))\nhashtags_fake_disasters = dict(Counter(hashtags_fake_disasters))\nhashtags_fake_disasters = dict(sorted(hashtags_fake_disasters.items(), key=operator.itemgetter(1),reverse=True))\n","f8fd89b2":"f, axes = plt.subplots(1, 2,figsize=(20,15))\nsns.set(style=\"whitegrid\")\naxes[0].set_title('Top 20 Keywords - Real Disasters',fontsize=14,fontweight=\"bold\")\naxes[0].set_xlabel('Words',fontsize=14,fontweight=\"bold\")\naxes[0].set_ylabel('Ocrruance',fontsize=14,fontweight=\"bold\")\nsns.barplot(list(hashtags_real_disasters.values())[0:20], list(hashtags_real_disasters.keys())[0:20],ax=axes[0],palette='rocket')\n\naxes[1].set_title('Top 20 Keywords - Not Disasters',fontsize=14,fontweight=\"bold\")\naxes[1].set_xlabel('Words',fontsize=14,fontweight=\"bold\")\naxes[1].set_ylabel('Occurance',fontsize=14,fontweight=\"bold\")\nsns.barplot(list(hashtags_fake_disasters.values())[0:20], list(hashtags_fake_disasters.keys())[0:20],ax=axes[1],palette='rocket')\n\nplt.show()\n\n","5672a624":"def contractions(tweet):\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"don\u00e5\u00abt\", \"do not\", tweet)   \n    \n    return tweet","d6f99dd6":"\n\ndef removeNonEnglish(row):\n    words = set(nltk.corpus.words.words())\n    return \" \".join(w for w in nltk.wordpunct_tokenize(sent) \\\n             if w.lower() in words or not w.isalpha())\n\ndef convertToLowerCase(row):\n    return row.lower()\n\ndef removeNumbers(row):\n    return re.sub('[0-9]+', '', row)\n\ndef removeURL(row):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',row)\n\ndef removeHTML(row):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',row)\n\ndef removeEmoji(row):\n    return row.encode('ascii', 'ignore').decode('ascii')\n\ndef removeSymbols(row):\n    return re.sub(r'[^\\w]', ' ', row)\n\ndef removeUnderscore(row):\n    return row.replace(\"_\",\"\")\n\ndef removeStopWords(row):\n    filtered_sentence = []\n    stop_words = set(stopwords.words('english')) \n    word_tokens = word_tokenize(row) \n    return ' '.join([word for word in word_tokens if word not in stop_words])\n\ndef removeSpecialChar(row):\n    punctuations = '@#!_?+&*[]-%.:\/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        row = row.replace(p, f' {p} ')\n    return row\n\n\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"yr\" : 'year',\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n\ndef convert_abbrev(row):\n    return abbreviations[row.lower()] if row.lower() in abbreviations.keys() else row\n\n","e2ab99e7":"\ndef cleanAll(train):\n    train['text'] = train.text.apply(lambda row: convertToLowerCase(row))\n    train['text'] = train.text.apply(lambda row: contractions(row))\n    train['text'] = train.text.apply(lambda row: convert_abbrev(row))\n    train['text'] = train.text.apply(lambda row: removeURL(row))\n    train['text'] = train.text.apply(lambda row: removeNumbers(row))\n    train['text'] = train.text.apply(lambda row: removeSymbols(row))\n    train['text'] = train.text.apply(lambda row: removeHTML(row))\n    train['text'] = train.text.apply(lambda row: removeEmoji(row))\n    train['text'] = train.text.apply(lambda row: convertToLowerCase(row))\n    train['text'] = train.text.apply(lambda row: removeUnderscore(row))\n    train['text'] = train.text.apply(lambda row: removeStopWords(row))\n    train['text'] = train.text.apply(lambda row: removeSpecialChar(row))\n    #train['text'] = train.text.apply(lambda row: removeNonEnglish(row))\n    return train\n\ntrain = cleanAll(train)\ntest = cleanAll(test)","61271964":"from collections import Counter\nfrom collections import OrderedDict\nfrom operator import itemgetter    \n\nreal_disasters = list(train[train.target == 1].text.str.cat(sep=' ').lower().split())\nnot_disasters = list(train[train.target == 0].text.str.cat(sep=' ').lower().split())\ncounts_real = dict(Counter(real_disasters))\ncounts_not_real= dict(Counter(not_disasters))\nreal_disasters_occurance = dict(OrderedDict(sorted(counts_real.items(), key = itemgetter(1), reverse = True)))\nnot_disasters_occurance = dict(OrderedDict(sorted(counts_not_real.items(), key = itemgetter(1), reverse = True)))","632a98d9":"f, axes = plt.subplots(1, 2,figsize=(20,15))\nsns.set(style=\"whitegrid\")\naxes[0].set_title('Real Disasters - Top 20 Words',fontsize=14,fontweight=\"bold\")\naxes[0].set_xlabel('Top Words',fontsize=14,fontweight=\"bold\")\naxes[0].set_ylabel('Ocrruance',fontsize=14,fontweight=\"bold\")\nsns.barplot(list(real_disasters_occurance.values())[0:20], list(real_disasters_occurance.keys())[0:20],ax=axes[0],palette='rocket')\n\naxes[1].set_title('Not Disasters - Top 20 Words',fontsize=14,fontweight=\"bold\")\naxes[1].set_xlabel('Top Words',fontsize=14,fontweight=\"bold\")\naxes[1].set_ylabel('Occurance',fontsize=14,fontweight=\"bold\")\nsns.barplot(list(not_disasters_occurance.values())[0:20], list(not_disasters_occurance.keys())[0:20],ax=axes[1],palette='rocket')\n\nplt.show()\n","df25a305":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\ndef generateWordCloud(df):\n    text =df\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000,\n        background_color = 'white',\n        stopwords = STOPWORDS).generate(str(text))\n    fig = plt.figure(\n        figsize = (10, 6),\n        facecolor = 'k',\n        edgecolor = 'k')\n    plt.imshow(wordcloud, interpolation = 'bilinear')\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    plt.show()","d251aeed":"generateWordCloud(list(real_disasters_occurance.keys())[0:100])","b19cc28d":"generateWordCloud(list(not_disasters_occurance.keys())[0:100])","8e84c1b8":"def find_ngrams(input_list, n):\n    return list(zip(*[input_list[i:] for i in range(n)]))\n\ndef convertTuple(tup): \n    str =  ' '.join(tup) \n    return str\n\ntrain['bigrams'] = train['text'].map(lambda x: find_ngrams(x.split(\" \"), 2))\nbigrams_real = train[train.target == 1].bigrams.tolist()\nbigrams_real = list(chain(*bigrams_real))\nbigrams_real = [(x.lower(), y.lower()) for x,y in bigrams_real]\n\nbigrams_fake = train[train.target == 0].bigrams.tolist()\nbigrams_fake = list(chain(*bigrams_fake))\nbigrams_fake = [(x.lower(), y.lower()) for x,y in bigrams_fake]\n\nbigram_counts_real = Counter(bigrams_real)\nbigram_real_dict = dict(bigram_counts_real.most_common(20))\n\nbigram_counts_fake = Counter(bigrams_fake)\nbigram_fake_dict = dict(bigram_counts_fake.most_common(20))","1b1ddbad":"bigrams_real = []\nbigrams_fake = []\nfor bigram in range(len(bigram_real_dict)):\n    bigrams_real.append(convertTuple(list(bigram_real_dict.keys())[bigram]))\n    bigrams_fake.append(convertTuple(list(bigram_fake_dict.keys())[bigram]))\n\n\nf, axes = plt.subplots(1, 2,figsize=(25,15))\nsns.set(style=\"whitegrid\")\naxes[0].set_title('Most Common Bigrams - Real Disasters',fontsize=14,fontweight=\"bold\")\naxes[0].set_xlabel('Bigrams',fontsize=14,fontweight=\"bold\")\naxes[0].set_ylabel('Ocrruance',fontsize=14,fontweight=\"bold\")\nsns.barplot(list(bigram_real_dict.values()),bigrams_real,ax=axes[0],palette='rocket')\n\naxes[1].set_title('Most Common Bigrams - Not Disasters',fontsize=14,fontweight=\"bold\")\naxes[1].set_xlabel('Bigrams',fontsize=14,fontweight=\"bold\")\naxes[1].set_ylabel('Occurance',fontsize=14,fontweight=\"bold\")\nsns.barplot(list(bigram_fake_dict.values()),bigrams_fake,ax=axes[1],palette='rocket')\n\nplt.show()    \n","583f9e5b":"def find_ngrams(input_list, n):\n    return list(zip(*[input_list[i:] for i in range(n)]))\n\ndef convertTuple(tup): \n    str =  ' '.join(tup) \n    return str\n\ntrain['trigrams'] = train['text'].map(lambda x: find_ngrams(x.split(\" \"), 3))\ntrigrams_real = train[train.target == 1].trigrams.tolist()\ntrigrams_real = list(chain(*trigrams_real))\ntrigrams_real = [(x.lower(), y.lower(), z.lower()) for x,y,z in trigrams_real]\n\ntrigrams_fake = train[train.target == 0].trigrams.tolist()\ntrigrams_fake = list(chain(*trigrams_fake))\ntrigrams_fake = [(x.lower(), y.lower(), z.lower()) for x,y,z in trigrams_fake]\n\ntrigram_counts_real = Counter(trigrams_real)\ntrigram_real_dict = dict(trigram_counts_real.most_common(20))\n\ntrigram_counts_fake = Counter(trigrams_fake)\ntrigram_fake_dict = dict(trigram_counts_fake.most_common(20))\n\ntrigrams_real = []\ntrigrams_fake = []\nfor bigram in range(len(bigram_real_dict)):\n    trigrams_real.append(convertTuple(list(trigram_real_dict.keys())[bigram]))\n    trigrams_fake.append(convertTuple(list(trigram_fake_dict.keys())[bigram]))\n\n\nf, axes = plt.subplots(1, 2,figsize=(25,15))\nsns.set(style=\"whitegrid\")\naxes[0].set_title('Most Common Trigrams - Real Disasters',fontsize=14,fontweight=\"bold\")\naxes[0].set_xlabel('Bigrams',fontsize=14,fontweight=\"bold\")\naxes[0].set_ylabel('Ocrruance',fontsize=14,fontweight=\"bold\")\nsns.barplot(list(trigram_real_dict.values()),trigrams_real,ax=axes[0],palette='rocket')\n\naxes[1].set_title('Most Common Trigrams - Not Disasters',fontsize=14,fontweight=\"bold\")\naxes[1].set_xlabel('Bigrams',fontsize=14,fontweight=\"bold\")\naxes[1].set_ylabel('Occurance',fontsize=14,fontweight=\"bold\")\nsns.barplot(list(trigram_fake_dict.values()),trigrams_fake,ax=axes[1],palette='rocket')\n\nplt.show()    ","52e15c12":"from nltk.stem import WordNetLemmatizer     \nlemmatizer = WordNetLemmatizer()\n#lemmatizer.lemmatize()\n\ndef getLemma(row):\n    text = row\n    text = text.lower().split(\" \")\n    lemma = [lemmatizer.lemmatize(each) for each in text]\n    return lemma\n\ntrain['text'] = train.text.apply(lambda row: getLemma(row))\ntrain.head()","041b2acf":"train['text'] = train.text.apply(lambda row: \" \".join(row))","26514fc4":"def tokenize(train,test):\n    \n    train_ = train.text\n    test_ = test.text\n    full = train_.append(test_)\n    tokenizer = Tokenizer() \n    tokenizer.fit_on_texts(full)\n    sequences = tokenizer.texts_to_sequences(full)\n\n    word_index = tokenizer.word_index\n    both_datasets = pad_sequences(sequences)\n    train_data = both_datasets[:len(train)]\n    test_data = both_datasets[len(train):]\n    labels = train['target']\n\n    return train_data, labels, word_index, test_data\n\n\n\nX_train, y_train, word_index, X_test = tokenize(train,test)\n\nX_test.shape","6430481c":"embeddings_index = {}\nwith open('..\/input\/glove-data\/glove.6B.200d.txt') as f:\n    for line in (f):\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\nf.close()\nembedding_dim = 200\nprint('Found %s word vectors in the GloVe library' % len(embeddings_index))","08f3e753":"embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n","e555d583":"\nembedding_dim = 200\nmax_length = 31\ntrunc_type='post'\n\ntrain_sentences = train.text.tolist()\ntrain_labels = train.target\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences,maxlen=max_length, truncating=trunc_type)\n\n\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Embedding(len(word_index)+1, embedding_dim, input_length=max_length))\nmodel.add(tf.keras.layers.GlobalAveragePooling1D())\nmodel.add(tf.keras.layers.Dense(14, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='Nadam',metrics=['accuracy'])\nmodel.summary()\nnum_epochs = 10\nhistory = model.fit(train_padded, train_labels, epochs=num_epochs,validation_split=0.2)","8d75c60f":"f, axes = plt.subplots(1, 2,figsize=(25,10))\naxes[0].set_title('Progress During Training - Accuracy',fontweight='bold',fontsize=14)\naxes[0].set_xlabel('Epochs',fontweight='bold',fontsize=14)\naxes[0].set_ylabel('Accuracy',fontweight='bold',fontsize=14)\naxes[0].plot(range(1,len(history.history['accuracy'])+1),history.history['accuracy'],label='Training Set')\naxes[0].plot(range(1,len(history.history['accuracy'])+1),history.history['val_accuracy'],label='Test Set')\naxes[0].legend()\n\naxes[1].set_title('Progress During Training - Accuracy',fontweight='bold',fontsize=14)\naxes[1].set_xlabel('Epochs',fontweight='bold',fontsize=14)\naxes[1].set_ylabel('Loss',fontweight='bold',fontsize=14)\naxes[1].plot(range(1,len(history.history['accuracy'])+1),history.history['loss'],label='Training Set')\naxes[1].plot(range(1,len(history.history['accuracy'])+1),history.history['val_loss'],label='Test Set')\naxes[1].legend()\nplt.show()","0806c424":"train_size = (0.1*X_train.shape[0])\nx_train = X_train[:int(train_size)]\ny_train_ = y_train[:int(train_size)]\nx_validation = X_train[int(train_size):]\ny_validation = y_train[int(train_size):]\nx_validation.shape","eecc55bd":"def create_model():\n\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Embedding(19633, 200, weights=[embedding_matrix], input_length=31,trainable=False))\n    model.add(tf.keras.layers.Dropout(0.3))\n    model.add(tf.keras.layers.GlobalAveragePooling1D())\n    model.add(tf.keras.layers.Dense(14, activation='relu'))\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n    model.compile(loss = 'binary_crossentropy', optimizer = 'Nadam', metrics = ['accuracy'])\n    return model\n\ndef LSTM():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Embedding(len(word_index)+1, 200, weights=[embedding_matrix], input_length=31,trainable=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200, dropout=0.2, recurrent_dropout=0.2)))\n    model.add(tf.keras.layers.Dense(14, activation='relu'))\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n    model.compile(loss = 'binary_crossentropy', optimizer = 'Nadam', metrics = ['accuracy'])\n    return model","53a1ea8a":"model2 = create_model()\nmodel2.summary()\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\nhistory2 = model2.fit(X_train, y_train, validation_data=(x_validation,y_validation),\n         epochs = 30, batch_size = 32, verbose = 1, shuffle = True)","64e30360":"f, axes = plt.subplots(1, 2,figsize=(25,10))\naxes[0].set_title('Progress During Training - Accuracy',fontweight='bold',fontsize=14)\naxes[0].set_xlabel('Epochs',fontweight='bold',fontsize=14)\naxes[0].set_ylabel('Accuracy',fontweight='bold',fontsize=14)\naxes[0].plot(range(1,len(history2.history['accuracy'])+1),history2.history['accuracy'],label='Training Set')\naxes[0].plot(range(1,len(history2.history['accuracy'])+1),history2.history['val_accuracy'],label='Test Set')\naxes[0].legend()\n\naxes[1].set_title('Progress During Training - Accuracy',fontweight='bold',fontsize=14)\naxes[1].set_xlabel('Epochs',fontweight='bold',fontsize=14)\naxes[1].set_ylabel('Loss',fontweight='bold',fontsize=14)\naxes[1].plot(range(1,len(history2.history['accuracy'])+1),history2.history['loss'],label='Training Set')\naxes[1].plot(range(1,len(history2.history['accuracy'])+1),history2.history['val_loss'],label='Test Set')\naxes[1].legend()\nplt.show()","3f87000b":"\ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title,fontweight='bold',fontsize=14)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=0)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label',fontweight='bold',fontsize=14)\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass),fontweight='bold',fontsize=14)\n    plt.show()\n    \n","f077300d":"pred = model2.predict(X_train)\nY_pred_classes = np.around(pred.transpose()[0])\nY_true = np.array(y_train)\ncm = confusion_matrix(Y_pred_classes,Y_true)     \nplot_confusion_matrix(cm,cmap='Blues',normalize=True,target_names=['Disaster','Not Disaster'],title='Confusion Matrix - Training Set')\n","ec507f19":"pred = model2.predict(x_validation)\nY_pred_classes = np.around(pred.transpose()[0])\nY_true = np.array(y_validation)\ncm = confusion_matrix(Y_pred_classes,Y_true)     \nplot_confusion_matrix(cm,normalize=True,target_names=['Disaster','Not Disaster'],title='Confusion Matrix')\n","43404081":"test_id = test.id\nsubmission1 = pd.DataFrame()\nsubmission1['id'] = test_id\nsubmission1['target'] = np.around(model2.predict(X_test).transpose()[0]).astype(int)\nsubmission1\nsubmission1.head(10)\nsubmission1.to_csv('submission.csv', index=False)","c23a0cdb":"### Null Features\nWhich features have most missing values?**","06c16ea2":"### Baseline Model","45029793":"From the figure above we can see that that both classes are well balanced for training. ","337e40c5":"### Number of Hashtags and At Sign\n\nWe might find that the use of hashtags and at signs could help to distinguish the classes. Let's feature engineer them and analyze the results. ","6cb49b52":"The main objective initially proposed was reached. We have come up with hypothesis and some of them showed to be true. We have explored the characteristics of the disaster\/not disaster labeld tweets and we have captured some of the trends behind them. We have developed a baseline model that performed quite well on training set, but showed an overfitting behavior on the test set. Using some transfer learning techniques and Glove word embedding, the more elaborated model performed way better. However, some enlightenment\/further implementations could be pointed out:\n* The second model performed better, especially taking into consideration overfitting, due to previously trained embedding layer and dropout layers that were added. It performed way better because Glove does the word embeddings by aggregating global word-word co-occurrence matrix from a corpus, and it is much more optmized than training an embedding layer from random weights.\n* For further enhancements we could propose some comparative testing between lemmatization and stemming techniques. They have different purposes and could lead to different results than what we obtained using just lemmatization. \n* We have analyzed features that were engineered, and some of them, by the distribution plots, seem to be useful to help the model training. They could be concatenated with the X_train matrix and tested to see the possible results. \n* The neural network architecture, after the pre-trained layer, was build from scratch and generic ideas of nlp architectures. Since LSTM and some RNN are very common on NLP context, we could test some of them and analyze the results. We have built one simple LSTM model but there wasn't much of time to test and vary the hyperparameters. \n* Some notebooks point out that some tweets on the training set were mislabeled. Further investigation\/correction could be done.\n* For feature selection, some of the words presented on the training set could be tested with chi-square hypothesis test to see wether the features (words) one-hot-encoded contribute to the prediction of the labels.\n","215b2bbc":"# Dependencies","cc1c6ae6":"Now we define an initial model to address the problem. The layers are: \n\n* Embedding: transforms the input data to a space of 200 dimensions, with a vocabulary related to the number of words that occur on our training set. The input_length parameter is set as 31, which is the value of the mean number of words on the dataset plus the std.\n* GlobalAveragePooling: gives a sparse representation of the Embedding layer output. Minimizes overfitting by reducing the total number of parameters in the model and extracts features from the input.\n* Dense: consists in a linear operation between the weights, in which all the inputs are connected to the ouput. The aim is to take the inputs from the feature analysis and apply weights to classify the initial input. The last one takes the operations between the previous layer activations multiplied by the weights and, by activating that signal with a sigmoid, converts it into a probability of belonging to a binary class.","f18085f1":"# Submission","2bc46a09":"As can be seen, most of the missing values are related to the location column. We plan to investigate if the location have a significant impact on the target prediction, but for now, it doesn't seem to be a good feature to rely on. ","cd3cf0ed":"#### Not Disasters","63a23052":"We now search for the words that we are going to use during training and form a matrix of weights related to those words. ","fee35d03":"#### Most Frequent Bigrams","a75cf414":"### Word Clouds\nWord Clouds are a visual representation of the frequency importance of each word on some text. We are going to build a sorted dictionary with the most frequent words for both classes (disasters and not disasters) and visualize their word clouds.\n","9b256722":"### More Elaborated Model \nNow we are using Transfer Learning. The weights loaded previously from the Glove model are used on the Embedding layer, and this layer is not trained (trainable = False). We also experimentally added a dropout layer to prevent overfitting. The LSTM model was tested, but the results were not good as the baseline upgraded model. Further discussion include proposing LSTM as an alternative. ","edee9c22":"# Data Cleaning\nBefore we move on exploring the text, we need to remove\/convert:\n1. Convert all strings to lower case.\n2. Remove URLs and HTMLs\n3. Remove emojis\n4. Remove symbols (this includes # and @)\n5. Remove stopwords \n6. Correct mispelling\n7. Convert slangs and contractions (thanks to [Up-to-date list of Slangs for Text Preprocessing](https:\/\/www.kaggle.com\/nmaguette\/up-to-date-list-of-slangs-for-text-preprocessing) and  [NLP with Disaster Tweets - EDA, Cleaning and BERT](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert) for providing a list of them)","ad580bca":"# Word Embeddings\nA word embedding is a learned representation for text where words that have the same meaning have a similar representation. So, if we can apply this representation to our dataset, theoretically we can provide to our model a much more significant data. ","c675b365":"From the plots above, the model  seems much more stable. Overfitting was reduced, and there might be a trending of improvement if the model is trained for more epochs. However, it seems to be stuck around 82% of accuracy on the test set. ","ef44878c":"# Most Frequent Keywords\nThe 'keyword' column may give a particular insight about the tweet content. Let's see which of them occur more on the dataset. ","0a19b7b4":"# Introduction\n\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster, it could be metaphorically. This is clear to a human right away, especially with the visual aid. But it\u2019s less clear to a machine. The challenge here is to develop a machine learning model to predict wether a tweet is related to a disaster or not.","837b4c54":"### Glove vs Embedding Layer\n\n\n\nEmbeddings are methods for learning vector representations of one-hot-encoded categorical data. GloVe it's one of these techniques, and it is based on matrix factorization techniques on the word-context matrix. It first constructs a large matrix of co-occurrence information, basically counting how frequently we see one word in some \u201ccontext\u201d in a large corpus. What embeddings do, is they simply learn to map the one-hot encoded categorical variables to vectors of floating point numbers of smaller dimensionality then the input vectors. <br>\n\nThe Keras Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset. It can be trained from scratch, or you can use pre-trained models, such as Glove, and use transfer-learning, and this can be advantageosu, since the pretrained embeddings were trained on large volumes of text.\n\nWe are going to test training the embedding layer jointly with the whole model and by using transfer learning and the loaded wheigts from Glove. ","f9335d11":"# Exploratory Data Analysis\n\n#### Initial Hypothesis (What makes a disaster tweet?)\n1. Do hashtags and at count correlate with the target variable?\n2. Are there some keywords that are common on disasters?\n3. Do the number of characters on sentences help to identify a disaster? What about the number of words?\n4. Do punctuation correlate to the target? \n\n\nNote that some data cleaning will be done while the exploration is made. Some might help subsequent analysis and some, if done previously, could harm specific analysis. ","6d12eef8":"From these plots we might infer:\n* The most probable number of characters in a tweet is aroung 135 for both classes (disasters and not disasters). They are very similar and might not provide our model with relevant information\n* The number of words that happen the most is 17 for real disasters. The distribution of not disasters is close to that, but there are some differences on the curves that might help the model to distingish the classes\n* The number of unique words distribution plots seems to show more differences between the classes. Whilst the not disaster distribution plot looks like a normal distribution shape, the true disaster curve has a different shape. Due to these differences, this feature could be selected in the future to help the model making decisions. \n","af51b6c7":"As can be seen above, the performance is similar on train and test sets. For both of them, the 'Disaster' class has most of the false positives. So our model is mislabeling around 20% of the real disasters. From the true positives of the 'Not Disaster' class, we can see that our model has a good classification percentage at identifying tweets not related to disasters. The model almost doesn't vary from training at correctly predicting disasters, which is good (shows that overfitting was adressed).","62db8a18":"As one can see, the number of hashtags and at signs on average are less than one, with a high standard deviation value. Let's analyze if","4e35daa9":"# Dataset Description\n\n### Features\n\n1. id - a unique identifier for each tweet\n2. text - the text of the tweet\n3. location - the location the tweet was sent from (may be blank)\n4. keyword - a particular keyword from the tweet (may be blank)\n5. target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\n### Files\n1. train.csv - the training set\n2. test.csv - the test set\n3. sample_submission.csv - a sample submission file in the correct format","44a826ce":"### Analyzing Features\nA probability distribution is a function that describes the likelihood of obtaining the possible values that a random variable can assume. So it might give some insights about the behavior of some features. ","d60f44d3":"### Top Words - After Cleaning","7181a630":"From these correlation results, it seems that the counting variables in respect to hashtags and at signs are not relevant to the problem. ","c125202e":"Now we have much more relevant context from each label, rather than just isolated words, especially for disaster labeled texts. For not disaster tweets we can see that the semantics seem to be related to other topics explicitilly not related to disasters. ","7c5c923c":"### Number of Characters in a Tweet\nLet's analyze if there are significant differences between train and test distribution plots for both classes. ","a58716e4":"### Training and Test Sets","5c626411":"#### Most Frequent Trigrams","4a8a8c10":"# Loading Data","00428378":"Thanks to [Plot a Confusion Matrix](https:\/\/www.kaggle.com\/grfiv4\/plot-a-confusion-matrix) for the excellent function provided.","5158bfed":"### Most Frequent Hashtags","3be9fbe9":"# References \n#### [1] https:\/\/blog.insightdatascience.com\/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e<br>\n#### [2] https:\/\/labs.bawi.io\/deep-learning-word2vec-and-embedding-3b00ff571cc1<br>\n#### [3] https:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/<br>\n#### [4] https:\/\/nlp.stanford.edu\/projects\/glove\/<br>\n#### [5] https:\/\/stats.stackexchange.com\/questions\/335793\/what-is-difference-between-keras-embedding-layer-and-word2vec<br>\n#### [6] https:\/\/towardsdatascience.com\/deep-learning-4-embedding-layers-f9a02d55ac12<br>\n#### [7] https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa<br>\n#### [8] https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert<br>\n#### [9] https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove<br>\n#### [10] https:\/\/www.pyohio.org\/2018\/schedule\/presentation\/38\/<br>\n#### [11] https:\/\/www.youtube.com\/watch?v=xvqsFTUsOmc<br>","c08f6190":"# Conclusions","ae2c0476":"# Real or Not? NLP with Disaster Tweets\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/nlp1-cover.jpg\">\n\n\n\n","8cd67a23":"So, there are a lot of words that semantically seem to be related to disasters, but we have taken the whole column, and not the specific target being equal to one. Let's see how many of these most frequent words are associated to real disasters and how many don't. ","7a45e284":"As we can see, there are some frequent words that could denote real and not real disasters, such as 'earthquake', 'hiroshima' and others. However, there are some of them present on both labels, such as 'news', 'hot', 'best' that don't help to make good decisions about the label. These should be removed from the dataset. Furthermore, some of these keywords could be used as features to the model. ","99c8a2db":"# Lemmatization\nLemmatization is the process of grouping together the inflected forms of a word so they can be analysed as a single item. One exemple can be seen below:\n\n<img src=\"https:\/\/searchingforbole.files.wordpress.com\/2018\/01\/re-learning-english-multiple1.png?w=624\">\n","f46f43af":"### Author: Arthur Dimitri <br>\n### arthur.dimitri@ee.ufcg.edu.br","23df507c":"### Evaluating The Model - Confusion Matrix","fba8a9d0":"### Data Tokenization\n\nHere we call a function to tokenize the data. To save time, we apply the tokenization to train and test data merged, and after that we return the separated sets. ","0790933d":"### N-Grams\n\n\"In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech.\"(Wikipedia). So they could be useful to understand the most important combination of words on a text and thus help to understand how ideas were structured.","6cc2b4a2":"As evidenced above, although some words seem to be attached to real disasters, in most cases they are not. Some of them, such as 'fatalities', 'collided', 'outbreak', 'evacuate', 'collision and 'famine' tend to occur more in real disaster examples. For further feature engineering we should pay attention to them, since they might to help to classify the tweet. ","3d7c38ca":"To achieve that, we are going to use nltk package at each tweet. ","a8ff4b2e":"### Target Variable Count","b7a6899b":"As infered from these plots, the model seems to overfit considerably. Loss value seems to be stuck and so the accuracy, which seems not to improve even for more epochs of training (tending to drop).","192976d0":"We have chosen Nadam as the optmizer due to its good performance on multiple previous tests. Since it is a binary classification problem, we have set the loss function as binary crossentropy.","564162bb":"Apllying all of these cleaning functions to both train and test sets:","598fe25c":"From the glove model we are going to use the file related to the 200-dimensional version of the embedding.","2ac7d6ab":"As we can see, the stop words and irrelevant characters and phrases were remove from the dataset. There are some frequent words that occur in both cases, such as amp, like, people, etc. These should be removed from the dataset, since they could muddle the model. "}}