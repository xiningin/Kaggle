{"cell_type":{"b1702431":"code","76ccc23d":"code","84fa694f":"code","589677a7":"code","adcb5208":"code","dcea2512":"code","dd3b7e1f":"code","5ef49323":"code","67aeef7f":"code","36e0cd68":"code","c6fb5d2b":"code","29ff3056":"code","da478b00":"code","b2c10b35":"code","e202e9cc":"code","a7ec83c5":"code","b1027cbb":"code","e48fdc73":"code","3604334c":"markdown","646b2588":"markdown","76b94526":"markdown"},"source":{"b1702431":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","76ccc23d":"# Import Libraries\n\n# Basic Libraries\nimport numpy as np\nimport pandas as pd\n# Visualization Libraries\nfrom matplotlib import pyplot as plt\nimport matplotlib.colors as colors\nimport seaborn as sns\nimport itertools\nfrom scipy.stats import norm\nimport scipy.stats\n\n# Classification and Regression Algorithm Libraries\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import confusion_matrix\n\n\n%matplotlib inline\nsns.set()\n\n# Importing the housing dataset\ndf = pd.read_csv('\/kaggle\/input\/house-sales-prediction-and-classification\/house_data.csv')\n\n# Identify Schema and datatypes\ndf.info()","84fa694f":"# Data Analysis\ndf.head(10)\ndf.tail(10)","589677a7":"# Field by Field Distinct Values\nprint(\"Number of Bedrooms avaialble:\")\ndf.bedrooms.unique()\nprint(\"Number of Bathrooms avaialble:\")\ndf.bathrooms.unique()\nprint(\"Number of FLoors avaialble:\")\ndf.floors.unique()\nprint(\"Types of Views avaialble:\")\ndf.view.unique()\nprint(\"Condition of Houses avaialble:\")\ndf.condition.unique()\nprint(\"Grade of Houses avaialble:\")\ndf.grade.unique()\nprint(\"Houses are built in the year range:\")\ndf.yr_built.unique()\nprint(\"Houses are renovated in the year range:\")\ndf.yr_renovated.unique()","adcb5208":"# Number of houses avaialble avaiable by TYpe\n\ndf[\"bedrooms\"].value_counts()\ndf[\"bathrooms\"].value_counts()\ndf[\"floors\"].value_counts()\ndf[\"view\"].value_counts()\ndf[\"condition\"].value_counts()\ndf[\"grade\"].value_counts()\ndf[\"yr_built\"].value_counts()\ndf[\"yr_renovated\"].value_counts() ","dcea2512":"# Field by Field Min and Max Distinct Values\n\nprint(\"Min Bedroom avaiable:\")\nmin(df['bedrooms'])\nprint(\"Max Bedroom avaiable:\")\nmax(df['bedrooms'])\nprint(\"Min Price for house:\")\nmin(df['price'])\nprint(\"Max price avaiable:\")\nmax(df['price'])\nprint(\"Min Number of floors available:\")\nmin(df['floors'])\nprint(\"Min Number of floors available::\")\nmax(df['floors'])\nprint(\"Oldest house built:\")\nmin(df['yr_built'])\nprint(\"Newest  house built:\")\nmax(df['yr_built'])","dd3b7e1f":"# Data Pre Procecssing and Cleansing\n\n# Remove Null Values\ndf = df.dropna()\n\n# Remove Duplicates\ndf.drop_duplicates(inplace = True)\n\n# Remove Houses which have bedrooms greater than 10 for plotting purpose\ndf1 = df.query('bedrooms <= 10')\n\ndf1 = df1.drop(['id','date'],axis=1)\n\n# Generate New Column Rating based on House Grading\ndf1['Rating'] = ['Good' if x < 7 else 'Excellent' for x in df1['grade']] \n\ndf1.info()","5ef49323":"# Data Visualization\n\n# Seabron Visualization using pairplot\nsns.pairplot(df1[['price','bedrooms','bathrooms','floors','view','condition','grade','yr_built','yr_renovated','sqft_lot']])\n\n# Heat map showing co-relationship between variables\n\nplt.figure(figsize=(15,10))\ncolumns =['price','bedrooms','bathrooms','sqft_living','floors','grade','yr_built','condition','sqft_lot']\nsns.heatmap(df1[columns].corr(),annot=True)","67aeef7f":"# Linear regression between Price and Sqft\n\n\n# Rating(Good vs Excellent) and their price, # of Bedrooms and their price\nfig, ax = plt.subplots(1,2, figsize=(14,8))\nsns.boxplot(x = 'Rating',y='price', data = df1, ax=ax[0])\nax[0].set_title('Rating vs price')\nsns.boxplot(x = 'bedrooms',y='price', data = df1, ax=ax[1])\nax[1].set_title('Number of Bedrooms')","36e0cd68":"# Indivudal variable relationship with Price\n\n# Linear regression between sqft and price\nsns.jointplot(x='sqft_lot',y='price',data=df1,kind='reg',size=4)\n\n# Linear regression between Year Built and price\nsns.jointplot(x='yr_built',y='price',data=df1,kind='reg',size=4)\n\n# Linear regression between Year renovated and price\nsns.jointplot(x='yr_renovated',y='price',data=df1,kind='reg',size=4)\n\n# Linear regression between Number of Bedrooms and price\nsns.jointplot(x='bedrooms',y='price',data=df1,kind='reg',size=4)\n\n# Linear regression between Number of Bathrooms and price\nsns.jointplot(x='bathrooms',y='price',data=df1,kind='reg',size=4)\n\n# Linear regression between condition and price\nsns.jointplot(x='condition',y='price',data=df1,kind='reg',size=4)\n\n# Linear regression between grade and price\nsns.jointplot(x='grade',y='price',data=df1,kind='reg',size=4)","c6fb5d2b":"# Building our model using different regression models\n\n# Data Preparation for fitting data into model\ndf1.info()\ndf1.head(10)\n\ndf2 = df1.drop(['Rating'],axis=1)\n\ndf2['bathrooms'] = df2['bathrooms'].apply(np.int64)\ndf2['price'] = df2['price'].apply(np.int64)\n\ndf2.info()\n\n# X(Independent variables) and y(target variables) \nX = df2.iloc[:,1:].values\ny = df2.iloc[:,0].values\n\n#Splitting the data into train,test data \nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)","29ff3056":"# Multiple Linear Regression Model\n\nmlr = LinearRegression()\nmlr.fit(X_train,y_train)\nmlr_score = mlr.score(X_test,y_test)\npred_mlr = mlr.predict(X_test)\nexpl_mlr = explained_variance_score(pred_mlr,y_test)\n\n\n# Decision Tree Regressional Model\n\ntr_regressor = DecisionTreeRegressor(random_state=0)\ntr_regressor.fit(X_train,y_train)\ntr_regressor.score(X_test,y_test)\npred_tr = tr_regressor.predict(X_test)\ndecision_score=tr_regressor.score(X_test,y_test)\nexpl_tr = explained_variance_score(pred_tr,y_test)\n\n# Random Forest Regressional Model\n\nrf_regressor = RandomForestRegressor(n_estimators=28,random_state=0)\nrf_regressor.fit(X_train,y_train)\nrf_regressor.score(X_test,y_test)\nrf_pred =rf_regressor.predict(X_test)\nrf_score=rf_regressor.score(X_test,y_test)\nexpl_rf = explained_variance_score(rf_pred,y_test)","da478b00":"# Calculate Model Score for all Regressional Models\n\nprint(\"Multiple Linear Regression Model Score is \",round(mlr.score(X_test,y_test)*100))\nprint(\"Decision tree  Regression Model Score is \",round(tr_regressor.score(X_test,y_test)*100))\nprint(\"Random Forest Regression Model Score is \",round(rf_regressor.score(X_test,y_test)*100))\n\n#Let's have a tabular pandas data frame, for a clear comparison\n\nmodels_score =pd.DataFrame({'Model':['Multiple Linear Regression','Decision Tree','Random forest Regression'],\n                            'Score':[mlr_score,decision_score,rf_score],\n                            'Explained Variance Score':[expl_mlr,expl_tr,expl_rf]\n                           })\nmodels_score.sort_values(by='Score',ascending=False)","b2c10b35":"# Apply Na\u00efve Bayes Classifier to classify Rating of houses with the decision boundaries\n\ndef predict_NB_gaussian_class(X,mu_list,std_list,pi_list): \n    #Returns the class for which the Gaussian Naive Bayes objective function has greatest value\n    scores_list = []\n    classes = len(mu_list)\n    \n    for p in range(classes):\n        score = (norm.pdf(x = X[0], loc = mu_list[p][0][0], scale = std_list[p][0][0] )  \n                * norm.pdf(x = X[1], loc = mu_list[p][0][1], scale = std_list[p][0][1] ) \n                * pi_list[p])\n        scores_list.append(score)\n             \n    return np.argmax(scores_list)\n\ndef predict_Bayes_class(X,mu_list,sigma_list): \n    #Returns the predicted class from an optimal bayes classifier - distributions must be known\n    scores_list = []\n    classes = len(mu_list)\n    \n    for p in range(classes):\n        score = scipy.stats.multivariate_normal.pdf(X, mean=mu_list[p], cov=sigma_list[p])\n        scores_list.append(score)\n             \n    return np.argmax(scores_list)\n\n#Estimating the parameters\nmu_list = np.split(df1.groupby('Rating').mean().values,[1,2])\nstd_list = np.split(df1.groupby('Rating').std().values,[1,2], axis = 0)\n\npi_list = df1.iloc[:,2].value_counts().values \/ len(df1)\n\n\n# Our 2-dimensional distribution will be over variables X and Y\nN = 100\nX = np.linspace(4, 8, N)\nY = np.linspace(1.5, 5, N)\nX, Y = np.meshgrid(X, Y)\n\ncolor_list = ['b','r']\n\nmy_norm = colors.Normalize(vmin=-1.,vmax=1.)\n\ng = sns.FacetGrid(df1, hue=\"Rating\", size=10, palette = 'colorblind') .map(plt.scatter, \"yr_built\", \"bedrooms\",)  .add_legend()\nmy_ax = g.ax\n","e202e9cc":"# Apply logistic regression to classify Rating  with the decision boundaries\n\ndf1.info()\n\n#split dataset in features and target variable\nfeature_cols = ['bedrooms','yr_built','floors']\nX = df1[feature_cols] # Features\ny = df1.condition # Target variable\n\n# split X and y into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)\n\n# import the class\nfrom sklearn.linear_model import LogisticRegression\n# instantiate the model (using the default parameters)\nlogreg = LogisticRegression()\n# fit the model with data\nlogreg.fit(X_train,y_train)\n\ny_pred=logreg.predict(X_test)\n\n# import the metrics class\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix\n\n# import required modules\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","a7ec83c5":"import sklearn.linear_model as skl_lm\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\n\nregr = skl_lm.LogisticRegression()\nregr.fit(X_train, y_train)\n\ndef plot_confusion_matrix(cm, classes, n_neighbors, title='Confusion matrix (Normalized)',\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Normalized confusion matrix: KNN-{}'.format(n_neighbors))\n    plt.colorbar()\n    plt.xticks(np.arange(2), classes)\n    plt.yticks(np.arange(2), classes)\n    plt.tight_layout()\n    plt.xlabel('True label',rotation='horizontal', ha='right')\n    plt.ylabel('Predicted label')\n    plt.show()\n\npred = regr.predict(X_test)\ncm_df = pd.DataFrame(confusion_matrix(y_test, pred).T, index=regr.classes_,columns=regr.classes_)\ncm_df.index.name = 'Predicted'\ncm_df.columns.name = 'True'\nprint(cm_df)\nprint(classification_report(y_test, pred))\n","b1027cbb":"# Data Preparation for Random Forest Plot\n\ndf1.info()\ndf1.head(10)\n\ndf3 = df1[['bedrooms','yr_built','Rating']]\n\ndf3['Rating'] = ['1' if x == 'Excellent' else '0' for x in df3['Rating']] \n\ndf3.info()\n\ndf3.head(10)\n\n# Select Year bedrooms, Year built \nX1 = df3.iloc[:, [0, 1]].values\n\n# select Rating\ny1 = df3.iloc[:, 2].values","e48fdc73":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size = 0.25, random_state = 0)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Random Forest Classification (Training set)')\nplt.xlabel('No of BedRooms')\nplt.ylabel('Year Built')\nplt.legend()\nplt.show()","3604334c":"Create Random Forest Classsifier Plot","646b2588":"Apply logistic regression to classify Rating  with the decision boundaries","76b94526":"Now, Let's apply Na\u00efve Bayes Classifier to classify Rating of houses with the decision boundaries"}}