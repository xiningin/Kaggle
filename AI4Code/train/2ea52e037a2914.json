{"cell_type":{"644153e1":"code","954099e4":"code","94e20cf0":"code","370dce7b":"code","94143ecc":"code","aafd3a06":"code","d00d387c":"code","40a4de18":"code","f1069e47":"code","0cd8985c":"code","ee84357e":"code","c1660879":"code","80f3fd33":"code","6eea642f":"code","b9553ec1":"code","a4428e71":"code","f019f603":"code","106e57d3":"code","eba08c14":"code","abd22d3c":"code","cd70ed93":"markdown","a71ebb5d":"markdown","444f5d6b":"markdown","976ad0e5":"markdown","2b42fd0a":"markdown","c140d0a0":"markdown","144281b2":"markdown","227e40de":"markdown","59208865":"markdown","3c48eb4f":"markdown","1740c002":"markdown"},"source":{"644153e1":"import os\nimport math\nimport numpy as np \nimport pandas as pd \n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import make_column_transformer\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import base\n\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split, learning_curve, validation_curve\nfrom sklearn.metrics import roc_auc_score, make_scorer\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import metrics\n\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom category_encoders import TargetEncoder\n\n\n\nkf = StratifiedKFold(5, random_state=0, shuffle=True)\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","954099e4":"train = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\", index_col='id')\ntest =  pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\", index_col='id')\nsubmission = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/sample_submission.csv\", index_col='id')","94e20cf0":"def chi_2_and_cramers_v(contingency_df):\n    \"\"\"\n    perform chi-square and cramer's V\n        H0: feature and target are independent\n        H1: feature and target are not independent\n    \"\"\"\n    from scipy import stats\n    chi2, p, dof, expected = stats.chi2_contingency(contingency_df)\n    n = contingency_df.sum().sum()\n    r, k = contingency_df.shape\n    cramers_v = np.sqrt(chi2 \/ (n * min((r-1), (k-1))))\n    return chi2, p, cramers_v\n\n\ndef describe_data(train_df, test_df, confidence_value=0.95):\n    # train_df = train_df.drop(\"id\", axis=1)\n    # test_df = test_df.drop(\"id\", axis=1)\n    feat_property_list = []\n    columns = [col for col in train_df.columns if not col == 'target']\n    for col in columns:\n        feat_description = {}\n        contingency_df = pd.crosstab(train_df[col], train_df['target'], margins=False)\n        chi2, p, cramers_v = chi_2_and_cramers_v(contingency_df)\n                \n        feat_description['feature'] = col\n        #feat_description['chi_2'] = chi2\n        feat_description['chi_2_p_value'] = p\n        if p < (1- confidence_value):\n            feat_description['feat-dependence'] = \"dependent\"\n        else:\n            feat_description['feat-dependence'] = \"independent\"\n            \n        \n\n        feat_description['cramers_v'] = cramers_v\n                \n        feat_description['train_cardinality'] = train_df[col].nunique()\n        feat_description['test_cardinality'] = test_df[col].nunique()\n        feat_description['in_train_not_in_test'] = len(set(train_df[col]) - set(test_df[col]))\n        feat_description['in_test_not_in_train'] = len(set(test_df[col]) - set(train_df[col]))\n        # feat_description['cardinality_diff(test-train)'] = feat_description['test_cardinality'] - feat_description['train_cardinality']\n        feat_description['biggest-cat(train)%'] = 100 * train[col].value_counts(normalize = True, dropna = False).values[0]\n        feat_description['dtype'] = train_df[col].dtype\n\n        feat_property_list.append(feat_description)\n        \n    return pd.DataFrame(feat_property_list).round(3)\n\n        \ndata_description = describe_data(train, test)\ndata_description.sort_values(\"cramers_v\")      ","370dce7b":"def plt_count_and_target(train_df, features, sortby='count', h=20, w=20, pad=8, tm_max_range=100, max_features=15):\n    import seaborn as sns\n    feat_num = len(features)\n    m = np.ceil((feat_num)\/2.)\n    fig, axes = plt.subplots(int(m), 2, figsize=(w, h+(feat_num\/\/2)*pad))\n    fig.tight_layout(pad=pad)\n    try:\n        axes = axes.ravel()\n    except AttributeError:\n        axes = [axes]\n    for feat in features:\n        if train_df[feat].dtype != 'object':\n            train_df[feat] = train_df[feat].astype(str)\n    for idx, feat in enumerate(features):\n        if idx < feat_num:\n            ax1 = axes[idx]\n            ax2 = ax1.twinx()\n            dt1 = train.groupby(feat).agg({'target':'count'})\n            dt2 = pd.crosstab(train[feat], train['target'], margins=True).sort_values('All', ascending=False).drop(index=['All'])\n            dt2['target_mean'] = (dt2[1]*100)\/dt2['All']\n            if sortby == 'count':\n                master_data = pd.merge(dt1, dt2, left_index=True, right_index=True).reset_index().sort_values('All', ascending=False)\n            if sortby == 'target_mean':\n                master_data = pd.merge(dt1, dt2, left_index=True, right_index=True).reset_index().sort_values('target_mean', ascending=False)\n\n            master_data[feat] = master_data[feat].astype(str)\n            if master_data.shape[0] > (max_features + 1):\n                master_data = master_data.iloc[:max_features]\n            sns.barplot(master_data[feat].tolist(), master_data['target'].tolist(), ax=ax1)\n            ax2.plot(master_data[feat].tolist(), master_data['target_mean'].tolist(), '-ro')\n            ax2.axhline((train.target.sum()*100)\/train.shape[0], color='r', label='overall_target_mean')\n            ax1.set_xlabel('feature')\n            ax1.set_ylabel('count', )\n            ax2.set_ylabel('target mean(%)',)\n            ax1.set_title(f\"{features[idx]}\")\n            ax1.xaxis.set_tick_params(rotation=90)\n            # step_ = (int(master_data.target_mean.max() + 5) - int(master_data.target_mean.min()-5))\/\/10\n            #ax2.yaxis.set_ticks(np.arange(int(master_data.target_mean.min()-5), int(master_data.target_mean.max() + 5), step_))\n            ax2.set_ylim(max(0, int(master_data.target_mean.min()-10)), int(master_data.target_mean.max() + 3),)\n\n            plt.legend()\n    plt.show()","94143ecc":"plt_count_and_target(train, features=[col for col in train.columns if col.startswith(\"bin\")], sortby='target_mean', w=18, h=10, pad=4, tm_max_range=55)","aafd3a06":"plt_count_and_target(train, features=[col for col in train.columns if col.startswith(\"nom\")], sortby='target_mean', w=15, h=5, pad=3, tm_max_range=105, max_features=100)","d00d387c":"plt_count_and_target(train, features=[col for col in train.columns if col.startswith(\"ord\")], sortby='target_mean', w=18, h=10, pad=4, tm_max_range=55, max_features=256)","40a4de18":"plt_count_and_target(train, features=['day', 'month'], sortby='target_mean', w=15, h=0, pad=4.5, tm_max_range=105)","f1069e47":"X = train.drop(\"target\", axis=1)\ny = train['target']","0cd8985c":"def pipeline_baseline(**kwargs):\n    \"\"\"This function return data encoding pipeline\"\"\"\n    # Logistic Regression parameters\n    ohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\"),\n            [f for f in X.columns])\n\n    pipe = make_pipeline(make_column_transformer(ohe1,remainder='drop'), LogisticRegression(**kwargs))\n    return pipe\n\npipe = pipeline_baseline()\n\nlr_params = {'logisticregression__penalty': ['l2'], \n             'logisticregression__solver': ['lbfgs'], \n             'logisticregression__C': np.random.uniform(0.1, 0.13, 20), \n             'logisticregression__max_iter':[1000],\n             'logisticregression__fit_intercept': [True, False],\n              'logisticregression__class_weight': [None, 'balanced'],\n            }\ngs = RandomizedSearchCV(pipe, param_distributions=lr_params, cv=kf, \n                        verbose=10, scoring='roc_auc', \n                        n_jobs=-1, n_iter=3, random_state=0)\nmodel = gs.fit(X, y)\n\nprint(f\"bast score={model.best_score_} and best params={model.best_params_}\")","ee84357e":"def plot_roc(model, X, y, label=\"\", title=''):\n    y_pred_proba = model.predict_proba(X)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y,  y_pred_proba)\n    auc = metrics.roc_auc_score(y, y_pred_proba)\n    plt.plot(fpr,tpr,label=label +str(round(auc, 5)))\n    plt.legend(loc=4)\n    plt.title(title)\n    plt.show()","c1660879":"plot_roc(model, X, y, label=\"Baseline ROC_AUC=\", title=\"Baseline Model Training ROC\")","80f3fd33":"def plot_learning_curve(train_sizes, train_scores, test_scores, title, alpha=0.1):\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.plot(train_sizes, train_mean, label='train score', color='blue', marker='o')\n    plt.fill_between(train_sizes, train_mean + train_std,\n                     train_mean - train_std, color='blue', alpha=alpha)\n    plt.plot(train_sizes, test_mean, label='val score', color='red', marker='o')\n\n    plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, color='red', alpha=alpha)\n    plt.title(title)\n    plt.xlabel('Number of training points')\n    plt.ylabel('roc_auc')\n    plt.grid(ls='--')\n    plt.legend(loc='best')\n    plt.ylim(0.5, 1)\n    plt.show()\n\n\ndef plot_validation_curve(param_range, train_scores, test_scores, title, alpha=0.1):\n    param_range = [x for x in param_range] \n    sort_idx = np.argsort(param_range)\n    param_range=np.array(param_range)[sort_idx]\n    train_mean = np.mean(train_scores, axis=1)[sort_idx]\n    train_std = np.std(train_scores, axis=1)[sort_idx]\n    test_mean = np.mean(test_scores, axis=1)[sort_idx]\n    test_std = np.std(test_scores, axis=1)[sort_idx]\n    plt.plot(param_range, train_mean, label='train score', color='blue', marker='o')\n    plt.fill_between(param_range, train_mean + train_std,\n                 train_mean - train_std, color='blue', alpha=alpha)\n    plt.plot(param_range, test_mean, label='val score', color='red', marker='o')\n    plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, color='red', alpha=alpha)\n    plt.xscale(\"log\")\n    plt.title(title)\n    plt.grid(ls='--')\n    plt.xlabel('Weight of class 2')\n    plt.ylabel('Average values and standard deviation for ROC_AUC')\n    plt.legend(loc='best')\n    plt.show()\n","6eea642f":"plt.figure(figsize=(9, 6))\ntrain_sizes1, train_scores1, test_scores1 = learning_curve(\n    estimator=model.best_estimator_, X=X, y=y,\n    train_sizes=np.arange(0.1, 1.1, 0.1), cv=kf, scoring='roc_auc', n_jobs=- 1)\n\nplot_learning_curve(train_sizes1, train_scores1, test_scores1, title='Learning curve for Baseline Model')","b9553ec1":"param_range1 = [math.pow(10, i) for i in range(-4, 4)]\ntrain_scores, test_scores = validation_curve(\n    estimator=model.best_estimator_, X=X, y=y, param_name=\"logisticregression__C\", param_range=param_range1,\n    cv=kf, scoring=\"roc_auc\", n_jobs=-1, verbose=True)\n\nplot_validation_curve(param_range1, train_scores, test_scores, title=\"Validation Basline Model:Param=C\", alpha=0.1)","a4428e71":"def pipeline_target_encode_nom(**kwargs):\n    \"\"\"This function return data encoding pipeline\"\"\"\n    # Logistic Regression parameters\n    ohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\"),\n            ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', \n             # 'ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4',\n            ])\n    \n    te_nom_5 = (TargetEncoder(), ['nom_5',])\n    te_nom_6 = (TargetEncoder(),['nom_6',])\n    te_nom_7 = (TargetEncoder(), ['nom_7',])\n    te_nom_8 = (TargetEncoder(),['nom_8',])\n    te_nom_9 = (TargetEncoder(),['nom_9',])\n    \n    #std = (StandardScaler(), ['nom_5', 'nom_6','nom_7','nom_8','nom_9','ord_5',])\n    te_ord_0 = (TargetEncoder(), ['ord_0',])\n    te_ord_1 = (TargetEncoder(), ['ord_1',])\n    te_ord_2 = (TargetEncoder(), ['ord_2',])\n    te_ord_3 = (TargetEncoder(), ['ord_3',])\n    te_ord_4 = (TargetEncoder(), ['ord_4',])\n    te_ord_5 = (TargetEncoder(), ['ord_5',])\n   \n    pipe = make_pipeline(make_column_transformer(ohe1, te_nom_5, te_nom_6, te_nom_7, te_nom_8, te_nom_9, \n                                                 te_ord_0, te_ord_1, te_ord_2, te_ord_3, te_ord_4, te_ord_5, remainder='drop'), StandardScaler(with_mean=False), \n                         LogisticRegression(**kwargs))\n    return pipe\n\nparams = {'columntransformer__targetencoder-1__smoothing': [i\/20. for i in range(20)],\n          'columntransformer__targetencoder-2__smoothing': [i\/20. for i in range(20)],\n          'columntransformer__targetencoder-3__smoothing': [i\/20. for i in range(20)],\n          'columntransformer__targetencoder-4__smoothing': [i\/20. for i in range(20)],\n          'columntransformer__targetencoder-5__smoothing': [i\/20. for i in range(20)],\n          'columntransformer__targetencoder-1__min_samples_leaf': [5, 10, 15],\n          'columntransformer__targetencoder-6__smoothing': [i\/20. for i in range(20)],\n    'logisticregression__penalty': ['l2'], \n     'logisticregression__solver': ['lbfgs'], \n     'logisticregression__C': np.random.uniform(0.1, 0.13, 20), \n     'logisticregression__max_iter':[1000],\n     'logisticregression__fit_intercept': [True],\n      'logisticregression__class_weight': [None, 'balanced'],\n            }\npipe = pipeline_target_encode_nom()\n\ngs = RandomizedSearchCV(pipe, param_distributions=params, cv=kf, \n                        verbose=10, scoring='roc_auc', \n                        n_jobs=-1, n_iter=5, random_state=0)\nmodel = gs.fit(X, y)\n\nprint(f\"bast score={model.best_score_} and best params={model.best_params_}\")","f019f603":"pipe.get_params().keys()","106e57d3":"plt.figure(figsize=(9, 6))\ntrain_sizes1, train_scores1, test_scores1 = learning_curve(\n    estimator=model.best_estimator_, X=X, y=y,\n    train_sizes=np.arange(0.1, 1.1, 0.1), cv=kf, scoring='roc_auc', n_jobs=- 1)\n\nplot_learning_curve(train_sizes1, train_scores1, test_scores1, title='Learning curve for Target Encode Model')","eba08c14":"param_range1 = [i\/20. for i in range(20)]\ntrain_scores, test_scores = validation_curve(\n    estimator=model.best_estimator_, X=X, y=y, param_name=\"columntransformer__targetencoder-5__smoothing\", param_range=param_range1,\n    cv=kf, scoring=\"roc_auc\", n_jobs=-1, verbose=True)\n\nplot_validation_curve(param_range1, train_scores, test_scores, title=\"Validation Basline Model:param=nom_9_smoothing\", alpha=0.1)","abd22d3c":"param_range1 = [i\/5. for i in range(5)]\ntrain_scores, test_scores = validation_curve(\n    estimator=model.best_estimator_, X=X, y=y, param_name=\"columntransformer__targetencoder-4__smoothing\", param_range=param_range1,\n    cv=kf, scoring=\"roc_auc\", n_jobs=-1, verbose=True)\n\nplot_validation_curve(param_range1, train_scores, test_scores, title=\"Validation Basline Model:param=nom_8_smoothing\", alpha=0.1)","cd70ed93":"### Day and Month","a71ebb5d":"### Learning Curve of The new model","444f5d6b":"### Nominal Features","976ad0e5":"### Ordinal Features","2b42fd0a":"### Validation Curve for Baseline Model","c140d0a0":"### Baseline Model","144281b2":"### Learning Curve for Baseline Model","227e40de":"### Learning Curve","59208865":"### validation curve","3c48eb4f":"### Binary Features","1740c002":"### first model with target encoding"}}