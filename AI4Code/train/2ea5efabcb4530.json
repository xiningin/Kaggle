{"cell_type":{"ac93522c":"code","fbc491a1":"code","93a73e77":"code","ff7d7b02":"code","1f12ab7c":"code","4e76a22e":"code","f4fbeac2":"code","9d46800a":"code","1a1f9591":"code","59f9aa05":"code","041501fe":"code","3935900f":"code","de65bf3e":"code","e9bbb593":"code","f3c665a7":"code","099b8ba7":"code","79e080cd":"code","35c78045":"code","d94136df":"code","c3eb097b":"code","7f23f712":"code","2484fa5b":"code","e16bfbe0":"code","2f2d2b5d":"code","d6f9cb70":"code","a5c1bab9":"code","55c5018b":"code","33e4867c":"code","4935c679":"code","b35cab3d":"code","6d87b88b":"code","79ef8a1b":"code","fe2e9d3b":"code","0480b4c6":"code","f985bba0":"code","1c6c4309":"code","278c3b52":"code","ec9ebdcc":"code","4f0518b0":"code","765edd76":"code","94289808":"code","a43a50cd":"code","1e2de1b3":"code","59845a51":"code","8edc981a":"code","7af4f72e":"code","06f9a95f":"code","fcb0eafd":"code","7d4b9888":"code","dea32332":"code","9915ea8b":"code","e1089485":"code","1cc5a199":"code","e37c2c63":"code","0b14a8fc":"code","050e9da6":"code","cbd571fa":"code","1d45b835":"code","28bb8528":"code","db41fabf":"code","842932e1":"code","5029304a":"code","57b437aa":"code","0a2f206e":"code","523cc887":"code","4fdf0368":"code","a917a17f":"code","4d8a3178":"code","38d3dbf4":"code","849cb1bb":"code","c6fb723d":"code","39b5ab15":"code","bdd5d600":"code","8ad76d9e":"code","7db0b044":"code","55f8d9f6":"code","5d2f0b67":"code","8cb31ba9":"code","1f969ceb":"code","e8ca3f97":"code","3974cdd0":"code","62566df5":"code","47eca3f6":"code","a68194a5":"code","ace5ed6c":"code","d18bde3a":"code","31b353cd":"code","8e7c10b1":"code","8c7f4315":"code","bb328cfe":"code","90396806":"code","9573bb38":"code","14b3f630":"code","54301d08":"code","a20d4155":"code","88ae794a":"code","88bd6431":"code","1892cb67":"code","ee938143":"code","cf22c6f5":"code","5659c54b":"code","71701ea0":"code","2cf8c106":"code","c6e72d9c":"code","acb49a22":"code","1813c6bc":"code","44f30813":"code","5116e089":"code","4d57c547":"code","31409881":"code","cad44590":"code","a7caa7b7":"code","29bb5283":"code","ace35567":"code","cf56839a":"code","3c4ad49f":"code","35d4c3ef":"code","b5ec1b51":"code","e80af020":"code","ff96e4cf":"code","98006da5":"code","478f5da1":"code","de4feed3":"code","5b005c9e":"code","5d92e05c":"code","4aaa29c6":"code","cd71f6c3":"code","13171677":"code","9f91868d":"code","4cc138b3":"code","2f76d866":"code","e229167b":"code","ce7271e3":"code","fdfbb40a":"code","36266851":"code","f4f675eb":"code","db9f9196":"code","ab7fccd5":"code","b317c3a6":"code","6620768e":"code","9c070160":"code","69fc1b13":"code","c9986ac9":"code","432cc4bb":"code","2d7fa2b5":"code","b6e03514":"code","ec190a8d":"code","2986c5b6":"code","7db7d9ca":"code","4bb152d1":"code","08e8ab90":"code","28e94080":"code","02995f2a":"code","053181a9":"code","27343187":"code","a518d941":"code","8553833b":"code","08619875":"code","b2914742":"code","f6ff8242":"code","53bf6015":"code","f0bbb2b0":"code","230ddfdb":"code","c885c195":"code","fcb15b79":"code","e01593ae":"code","56d91944":"code","d2aa6c4c":"code","a43534a9":"code","56aeadf0":"code","217be5bc":"code","912abf17":"code","9d3b3984":"code","3a83d533":"code","9acfd843":"code","3f4fdb56":"code","74162922":"code","6d102796":"code","c985da51":"code","9b8fbce6":"code","8e498812":"code","8a84c15a":"code","8e892dff":"code","d479fe7f":"code","7d5431f9":"code","7c0dda8a":"code","a6dc9b80":"code","38367392":"code","a1bf6685":"code","50ecabb8":"code","bce3a937":"code","2ba3b1c1":"code","97318e8f":"code","bb3f012f":"code","d32abd6f":"code","3369ae61":"code","07f33b8a":"code","7b278f41":"markdown","1696e7db":"markdown","546381b9":"markdown","fbcdb363":"markdown","2f8bf6be":"markdown","10a0069e":"markdown","4205816a":"markdown","f9e2c50a":"markdown","754f8169":"markdown","2e429bd5":"markdown","4f66893b":"markdown","e5e0137b":"markdown","7a8065b4":"markdown","92dbc993":"markdown","185e0c66":"markdown","0a330749":"markdown","7a8d409f":"markdown","30b85abf":"markdown","bcf96810":"markdown","314e069a":"markdown","c3bfcf90":"markdown","89ac1ccf":"markdown","99ec626a":"markdown","020e226c":"markdown","19005225":"markdown","22710347":"markdown","b02c7f8e":"markdown","3ccf856b":"markdown","dde6bd2b":"markdown","4a3691a9":"markdown","cc15bd6a":"markdown","7fc5478d":"markdown","a4be4731":"markdown","af32525d":"markdown","bd1b876a":"markdown","34d3a04c":"markdown","4e9ed65c":"markdown","7863351d":"markdown","30da7b91":"markdown","c61ac470":"markdown","d994741b":"markdown","a3afd146":"markdown","76aeed3c":"markdown","9b84426b":"markdown","dbe709bc":"markdown","24df7a70":"markdown","0191a75d":"markdown","d7f0f9b5":"markdown","34c5b697":"markdown","b56dca92":"markdown","a2573bd6":"markdown","c083900a":"markdown","5c61359e":"markdown","f94d263c":"markdown","a57829bf":"markdown","3527d05f":"markdown","12982924":"markdown","9ed24cda":"markdown","cf6dd5f1":"markdown","f993324a":"markdown","4d4c8204":"markdown","dccf3cd8":"markdown","7e5a1855":"markdown","bdceb2ec":"markdown","8f6817f4":"markdown","59a2ffa6":"markdown","9a9209eb":"markdown","43360126":"markdown","60128e1f":"markdown","df08825c":"markdown","99e29c4a":"markdown","fd5d9f03":"markdown","46e6c014":"markdown","6467dc3d":"markdown","75a4fbb2":"markdown","4e10f091":"markdown","23585f97":"markdown","4b312f0d":"markdown","9ed49920":"markdown","a08a7237":"markdown","c4111a04":"markdown","06867ef9":"markdown","1f08955a":"markdown","356748c2":"markdown","23aa6b57":"markdown","8c1b7ec6":"markdown","bb186817":"markdown","cb4b7355":"markdown","8b07c8ef":"markdown","1fbde27f":"markdown","d81c401f":"markdown","97bfa273":"markdown","4cf1351c":"markdown","6f6e4f03":"markdown","a2a18bc0":"markdown","663713e5":"markdown","3e5382ff":"markdown","6b5e6b34":"markdown"},"source":{"ac93522c":"#importing the libraries that we use\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling as pp","fbc491a1":"#importing the dataset\ndataset = pd.read_csv('..\/input\/imdb-5000-movie-dataset\/movie_metadata.csv')\ndataset.head()","93a73e77":"dataset.shape","ff7d7b02":"dataset.columns","1f12ab7c":"dataset.profile_report()","4e76a22e":"dataset.drop_duplicates(inplace = True)\ndataset.shape","f4fbeac2":"numerical_cols = [col for col in dataset.columns if dataset[col].dtype != 'object']\ncategorical_cols = [col for col in dataset.columns if dataset[col].dtype == 'object']","9d46800a":"categorical_cols, numerical_cols","1a1f9591":"dataset[numerical_cols].describe()","59f9aa05":"dataset[categorical_cols].describe()","041501fe":"dataset.isnull().sum()","3935900f":"dataset.color.unique()","de65bf3e":"color_mode = dataset['color'].mode().iloc[0]\ndataset.color.fillna(color_mode, inplace = True)\ndataset.color.isnull().sum()","e9bbb593":"dataset.director_name.nunique(), dataset.director_name.isnull().sum()","f3c665a7":"dataset = dataset.dropna(axis = 0, subset = ['director_name'] )","099b8ba7":"dataset.num_critic_for_reviews.min(), dataset.num_critic_for_reviews.max(), dataset.num_critic_for_reviews.median()","79e080cd":"num_critic_for_reviews_median = dataset['num_critic_for_reviews'].median()\ndataset.num_critic_for_reviews.fillna(num_critic_for_reviews_median, inplace = True)\ndataset.num_critic_for_reviews.isnull().sum()","35c78045":"dataset.duration.min(), dataset.duration.max(), dataset.duration.median()","d94136df":"duration_median = dataset.duration.median()\ndataset.duration.fillna(duration_median, inplace = True)\ndataset.duration.isnull().sum()","c3eb097b":"dataset.director_facebook_likes.min(), dataset.director_facebook_likes.max(), dataset.director_facebook_likes.median(),dataset.director_facebook_likes.mean()","7f23f712":"director_facebook_likes_mean = dataset.director_facebook_likes.mean()\ndataset.director_facebook_likes.fillna(director_facebook_likes_mean, inplace = True)\ndataset.director_facebook_likes.isnull().sum()","2484fa5b":"dataset.actor_3_facebook_likes.min(), dataset.actor_3_facebook_likes.max(), dataset.actor_3_facebook_likes.median(),dataset.actor_3_facebook_likes.mean()","e16bfbe0":"actor_3_facebook_likes_mean = dataset.actor_3_facebook_likes.mean()\ndataset.actor_3_facebook_likes.fillna(actor_3_facebook_likes_mean, inplace = True)\ndataset.actor_3_facebook_likes.isnull().sum()","2f2d2b5d":"dataset = dataset.dropna(axis = 0, subset = ['actor_2_name'])\ndataset.actor_2_name.isnull().sum()","d6f9cb70":"dataset.actor_1_facebook_likes.min(), dataset.actor_1_facebook_likes.max(), dataset.actor_1_facebook_likes.median(),dataset.actor_1_facebook_likes.mean()","a5c1bab9":"actor_1_facebook_likes_mean = dataset.actor_1_facebook_likes.mean()\ndataset.actor_1_facebook_likes.fillna(actor_1_facebook_likes_mean, inplace = True)\ndataset.actor_1_facebook_likes.isnull().sum()","55c5018b":"dataset.gross.describe()","33e4867c":"dataset.gross.isnull().sum()","4935c679":"dataset = dataset.dropna(axis = 0, subset = ['gross'])\ndataset.gross.isnull().sum()","b35cab3d":"dataset.shape","6d87b88b":"dataset.isnull().sum()","79ef8a1b":"dataset = dataset.dropna(axis = 0, subset = ['budget'])\ndataset.budget.isnull().sum()","fe2e9d3b":"dataset.isnull().sum()","0480b4c6":"dataset.shape","f985bba0":"dataset = dataset.dropna(axis = 0, subset = ['actor_3_name'])\ndataset.actor_3_name.isnull().sum()","1c6c4309":"facenumber_in_poster_median = dataset.facenumber_in_poster.median()\ndataset.facenumber_in_poster.fillna(facenumber_in_poster_median, inplace = True)\ndataset.facenumber_in_poster.isnull().sum()","278c3b52":"dataset.plot_keywords.unique()","ec9ebdcc":"dataset.language.unique()","4f0518b0":"dataset.language.value_counts()","765edd76":"language_mode = dataset.language.mode().iloc[0]\ndataset.language.fillna(language_mode, inplace = True)\ndataset.language.isnull().sum()","94289808":"dataset = dataset.dropna(axis = 0, subset = ['plot_keywords'])\ndataset.plot_keywords.isnull().sum()","a43a50cd":"dataset.content_rating.unique()","1e2de1b3":"dataset.content_rating.fillna('Not Rated', inplace = True)","59845a51":"dataset.aspect_ratio.unique()","8edc981a":"aspect_ratio_mode = dataset.aspect_ratio.mode().iloc[0]\ndataset.aspect_ratio.fillna(aspect_ratio_mode, inplace = True)                                                    ","7af4f72e":"dataset.isnull().sum()","06f9a95f":"dataset.reset_index(inplace = True, drop = True)","fcb0eafd":"dataset.profile_report()","7d4b9888":"numerical_cols, categorical_cols","dea32332":"dataset.color.unique(), dataset.color.nunique()","9915ea8b":"dataset['color'] = dataset.color.map({'Color' : 1 , ' Black and White' : 0})","e1089485":"dataset.director_name.unique(), dataset.director_name.nunique()","1cc5a199":"director_name_value_counts = dataset.director_name.value_counts()","e37c2c63":"director_name_value_counts  = pd.DataFrame(director_name_value_counts).reset_index().rename(columns = {'index': 'director_name', 'director_name':'director_name_value_counts'})","0b14a8fc":"dataset = pd.merge(dataset, director_name_value_counts,left_on = 'director_name', right_on = 'director_name', how = 'left')","050e9da6":"dataset = dataset.drop(columns = 'director_name')","cbd571fa":"dataset.actor_2_name.unique(), dataset.actor_2_name.nunique()","1d45b835":"actor_2_name_value_counts = dataset.actor_2_name.value_counts()","28bb8528":"actor_2_name_value_counts  = pd.DataFrame(actor_2_name_value_counts).reset_index().rename(columns = {'index': 'actor_2_name', 'actor_2_name':'actor_2_name_value_counts'})","db41fabf":"dataset = pd.merge(dataset, actor_2_name_value_counts,left_on = 'actor_2_name', right_on = 'actor_2_name', how = 'left')","842932e1":"dataset = dataset.drop(columns = 'actor_2_name')","5029304a":"dataset.genres.unique(), dataset.genres.nunique()","57b437aa":"dataset['main_genre'] = dataset.genres.str.split('|').str[0]","0a2f206e":"dataset.main_genre.unique(), dataset.main_genre.nunique()","523cc887":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndataset['main_genre'] = le.fit_transform(dataset.main_genre)","4fdf0368":"genres_value_counts = dataset.genres.value_counts()","a917a17f":"genres_value_counts  = pd.DataFrame(genres_value_counts).reset_index().rename(columns = {'index' : 'genres', 'genres' : 'genres_value_counts'})","4d8a3178":"dataset = pd.merge(dataset, genres_value_counts,left_on = 'genres', right_on = 'genres', how = 'left')","38d3dbf4":"dataset = dataset.drop(columns = 'genres')","849cb1bb":"dataset.actor_1_name.unique(), dataset.actor_1_name.nunique()","c6fb723d":"actor_1_name_value_counts = dataset.actor_1_name.value_counts()","39b5ab15":"actor_1_name_value_counts = pd.DataFrame(actor_1_name_value_counts).reset_index().rename(columns = {'index' : 'actor_1_name', 'actor_1_name' : 'actor_1_name_value_counts'})","bdd5d600":"dataset = pd.merge(dataset, actor_1_name_value_counts,left_on = 'actor_1_name', right_on = 'actor_1_name', how = 'left')","8ad76d9e":"dataset = dataset.drop(columns = 'actor_1_name')","7db0b044":"dataset.movie_title.unique(), dataset.movie_title.nunique()","55f8d9f6":"dataset = dataset.drop(columns = 'movie_title')","5d2f0b67":"dataset.actor_3_name.unique(), dataset.actor_3_name.nunique()","8cb31ba9":"actor_3_name_value_counts = dataset.actor_3_name.value_counts()","1f969ceb":"actor_3_name_value_counts = pd.DataFrame(actor_3_name_value_counts).reset_index().rename(columns = {'index' : 'actor_3_name', 'actor_3_name' : 'actor_3_name_value_counts'})","e8ca3f97":"dataset= pd.merge(dataset, actor_3_name_value_counts,left_on = 'actor_3_name', right_on = 'actor_3_name', how = 'left')","3974cdd0":"dataset = dataset.drop(columns = 'actor_3_name')","62566df5":"dataset.plot_keywords.unique(), dataset.plot_keywords.nunique()","47eca3f6":"dataset['main_plot_keyword'] = dataset.plot_keywords.str.split('|').str[0]","a68194a5":"dataset = dataset.drop(columns = 'plot_keywords')","ace5ed6c":"dataset.main_plot_keyword.unique(), dataset.main_plot_keyword.nunique()","d18bde3a":"main_plot_keyword_value_counts = dataset.main_plot_keyword.value_counts()","31b353cd":"main_plot_keyword_value_counts = pd.DataFrame(main_plot_keyword_value_counts).reset_index().rename(columns = {'index' : 'main_plot_keyword', 'main_plot_keyword' : 'main_plot_keyword_value_counts'})","8e7c10b1":"dataset = pd.merge(dataset, main_plot_keyword_value_counts, left_on = 'main_plot_keyword', right_on = 'main_plot_keyword', how = 'left')","8c7f4315":"dataset = dataset.drop(columns = 'main_plot_keyword')","bb328cfe":"dataset.movie_imdb_link.unique(), dataset.movie_imdb_link.nunique()","90396806":"dataset = dataset.drop(columns = 'movie_imdb_link')","9573bb38":"dataset.language.unique(), dataset.language.nunique()","14b3f630":"from sklearn.preprocessing import LabelEncoder\nle1 = LabelEncoder()\ndataset['language'] = le1.fit_transform(dataset.language)","54301d08":"dataset.country.unique(), dataset.country.nunique()","a20d4155":"from sklearn.preprocessing import LabelEncoder\nle2 = LabelEncoder()\ndataset['country'] = le2.fit_transform(dataset.country)","88ae794a":"dataset.content_rating.unique(),dataset.content_rating.nunique()","88bd6431":"from sklearn.preprocessing import LabelEncoder\nle3 = LabelEncoder()\ndataset['content_rating'] = le3.fit_transform(dataset.content_rating)","1892cb67":"dataset.head().T","ee938143":"dataset.profile_report()","cf22c6f5":"datasetR = dataset.copy() #lets keep our original dataset for reference. Here datasetR is for Regression model\ndatasetC = dataset.copy() #Here datasetC is for classification model","5659c54b":"from sklearn.model_selection import train_test_split\ny = datasetR.pop('imdb_score')\nX = datasetR\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 42)","71701ea0":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","2cf8c106":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train.values), columns=X_train.columns, index=X_train.index)","c6e72d9c":"X_test = pd.DataFrame(scaler.transform(X_test.values), columns = X_train.columns, index = X_test.index)","acb49a22":"X_train.shape","1813c6bc":"#removing variables with high colinearity\ndef correlation(dataset, threshold):\n    col_corr = set() # Set of all the names of deleted columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n                colname = corr_matrix.columns[i] # getting the name of column\n                col_corr.add(colname)\n                if colname in dataset.columns:\n                    del dataset[colname] # deleting the column from the dataset\ncorrelation(X_train,0.90)","44f30813":"X_train.shape","5116e089":"#importing the required libraries\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","4d57c547":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)            # running RFE\nrfe = rfe.fit(X_train, y_train)","31409881":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","cad44590":"col_rfe = X_train.columns[rfe.support_]\ncol_rfe","a7caa7b7":"X_train.columns[~rfe.support_]","29bb5283":"#Creating a X_train dataframe with rfe varianles\nX_train_rfe = X_train[col_rfe]","ace35567":"# Adding a constant variable for using the stats model\nimport statsmodels.api as sm\nX_train_rfe_constant = sm.add_constant(X_train_rfe)","cf56839a":"lm = sm.OLS(y_train,X_train_rfe_constant).fit()   # Running the linear model","3c4ad49f":"#Let's see the summary of our linear model\nprint(lm.summary())","35d4c3ef":"X_test_rfe = X_test[col_rfe]\nX_test_rfe_constant = sm.add_constant(X_test_rfe)","b5ec1b51":"y_pred_linear = lm.predict(X_test_rfe_constant)","e80af020":"y_pred_linear.values","ff96e4cf":"y_pred_linear.min(), y_pred_linear.max()","98006da5":"from sklearn.metrics import mean_squared_error","478f5da1":"mean_squared_error(y_pred_linear, y_test)","de4feed3":"from sklearn.svm import SVR\nsvr_rbf = SVR(kernel='rbf', gamma=0.1)\nsvr_lin = SVR(kernel='linear', gamma='auto')\nsvr_poly = SVR(kernel='poly', gamma='auto', degree=3)","5b005c9e":"svr_rbf.fit(X_train_rfe, y_train)\ny_pred_svm_rbf = svr_rbf.predict(X_test_rfe)","5d92e05c":"y_pred_svm_rbf","4aaa29c6":"y_pred_svm_rbf.min(), y_pred_svm_rbf.max()","cd71f6c3":"mean_squared_error(y_pred_svm_rbf, y_test)","13171677":"svr_lin.fit(X_train_rfe, y_train)\ny_pred_svm_lin = svr_lin.predict(X_test_rfe)","9f91868d":"y_pred_svm_lin","4cc138b3":"y_pred_svm_lin.min(), y_pred_svm_lin.max()","2f76d866":"mean_squared_error(y_pred_svm_lin, y_test)","e229167b":"svr_poly.fit(X_train_rfe, y_train)\ny_pred_svm_poly = svr_poly.predict(X_test_rfe)","ce7271e3":"y_pred_svm_poly","fdfbb40a":"y_pred_svm_poly.min(), y_pred_svm_poly.max()","36266851":"mean_squared_error(y_pred_svm_poly, y_test)","f4f675eb":"from sklearn import ensemble\nn_trees=200\ngradientboost = ensemble.GradientBoostingRegressor(loss='ls',learning_rate=0.03,n_estimators=n_trees,max_depth=4)\ngradientboost.fit(X_train_rfe,y_train)","db9f9196":"y_pred_gb=gradientboost.predict(X_test_rfe)\nerror=gradientboost.loss_(y_test,y_pred_gb) ##Loss function== Mean square error\nprint(\"MSE:%.3f\" % error)","ab7fccd5":"mean_squared_error(y_pred_gb, y_test)","b317c3a6":"y_pred_gb.min(), y_pred_gb.max()","6620768e":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'loss' : ['ls'],\n    'max_depth' : [3, 4, 5],\n    'learning_rate' : [0.01, 0.001],\n    'n_estimators': [100, 200, 500]\n}\n# Create a based model\ngb = ensemble.GradientBoostingRegressor()\n# Instantiate the grid search model\ngrid_search_gb = GridSearchCV(estimator = gb, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","9c070160":"grid_search_gb.fit(X_train_rfe, y_train)\ngrid_search_gb.best_params_","69fc1b13":"grid_search_gb_pred = grid_search_gb.predict(X_test_rfe)","c9986ac9":"mean_squared_error(y_test.values, grid_search_gb_pred)","432cc4bb":"from sklearn.ensemble import RandomForestRegressor\nrf_regressor = RandomForestRegressor(n_estimators = 500)\nrf_regressor.fit(X_train_rfe, y_train)\nrf_pred = rf_regressor.predict(X_test_rfe)","2d7fa2b5":"mean_squared_error(rf_pred, y_test)","b6e03514":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [90, 100],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100, 500, 1000]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search_rf = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","ec190a8d":"grid_search_rf.fit(X_train_rfe, y_train)\ngrid_search_rf.best_params_","2986c5b6":"y_grid_pred_rf = grid_search_rf.predict(X_test_rfe)","7db7d9ca":"mean_squared_error(y_grid_pred_rf, y_test.values)","4bb152d1":"import xgboost as xgb\nxg_model = xgb.XGBRegressor(n_estimators = 500)\nxg_model.fit(X_train_rfe, y_train)","08e8ab90":"results = xg_model.predict(X_test_rfe)","28e94080":"mean_squared_error(results, y_test.values)","02995f2a":"xg_model.score(X_train_rfe, y_train)","053181a9":"from sklearn.metrics import r2_score\nr2_score(y_test, results)","27343187":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [3, 4],\n    'learning_rate' : [0.1, 0.01, 0.05],\n    'n_estimators' : [100, 500, 1000]\n}\n# Create a based model\nmodel_xgb= xgb.XGBRegressor()\n# Instantiate the grid search model\ngrid_search_xgb = GridSearchCV(estimator = model_xgb, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","a518d941":"grid_search_xgb.fit(X_train_rfe, y_train)\ngrid_search_xgb.best_params_","8553833b":"y_pred_xgb = grid_search_xgb.predict(X_test_rfe)","08619875":"mean_squared_error(y_test.values, y_pred_xgb)","b2914742":"feature_importance = grid_search_xgb.best_estimator_.feature_importances_\nsorted_importance = np.argsort(feature_importance)\npos = np.arange(len(sorted_importance))\nplt.figure(figsize=(12,5))\nplt.barh(pos, feature_importance[sorted_importance],align='center')\nplt.yticks(pos, X_train_rfe.columns[sorted_importance],fontsize=15)\nplt.title('Feature Importance ',fontsize=18)\nplt.show()","f6ff8242":"datasetC.head()","53bf6015":"y_train_classification = y_train.copy()","f0bbb2b0":"y_train_classification = pd.cut(y_train_classification, bins=[1, 3, 6, float('Inf')], labels=['Flop Movie', 'Average Movie', 'Hit Movie'])","230ddfdb":"y_test_classification = y_test.copy()","c885c195":"y_test_classification = pd.cut(y_test_classification, bins=[1, 3, 6, float('Inf')], labels=['Flop Movie', 'Average Movie', 'Hit Movie'])","fcb15b79":"X_train_rfe_classification = X_train_rfe.copy()\nX_test_rfe_classification = X_test_rfe.copy()","e01593ae":"from sklearn.linear_model import LogisticRegression\nlogit_model = LogisticRegression(solver = 'saga', random_state = 0)\nlogit_model.fit(X_train_rfe_classification, y_train_classification)","56d91944":"y_logit_pred = logit_model.predict(X_test_rfe_classification)","d2aa6c4c":"y_logit_pred","a43534a9":"from sklearn import metrics\ncount_misclassified = (y_test_classification != y_logit_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_logit_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_logit_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_logit_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_logit_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","56aeadf0":"from sklearn.svm import SVC\nsvc_linear_model = SVC(kernel='linear', C=100, gamma= 'scale', decision_function_shape='ovo', random_state = 42)","217be5bc":"svc_linear_model.fit(X_train_rfe_classification, y_train_classification)\ny_svc_linear_pred = svc_linear_model.predict(X_test_rfe_classification)","912abf17":"y_svc_linear_pred","9d3b3984":"from sklearn import metrics\ncount_misclassified = (y_test_classification != y_svc_linear_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_svc_linear_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_svc_linear_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_svc_linear_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_svc_linear_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","3a83d533":"from sklearn.svm import SVC\nsvc_poly_model = SVC(kernel='poly', C=100, gamma= 'scale', degree = 3, decision_function_shape='ovo', random_state = 42)","9acfd843":"svc_poly_model.fit(X_train_rfe_classification, y_train_classification)\ny_svc_poly_pred = svc_poly_model.predict(X_test_rfe_classification)","3f4fdb56":"y_svc_poly_pred","74162922":"from sklearn import metrics\ncount_misclassified = (y_test_classification != y_svc_poly_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_svc_poly_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_svc_poly_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_svc_poly_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_svc_poly_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","6d102796":"from sklearn.svm import SVC\nsvc_rbf_model = SVC(kernel='rbf', C=100, gamma= 'scale', decision_function_shape='ovo', random_state = 42)","c985da51":"svc_rbf_model.fit(X_train_rfe_classification, y_train_classification)\ny_svc_rbf_pred = svc_rbf_model.predict(X_test_rfe_classification)","9b8fbce6":"y_svc_rbf_pred","8e498812":"from sklearn import metrics\ncount_misclassified = (y_test_classification != y_svc_rbf_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_svc_rbf_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_svc_rbf_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_svc_rbf_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_svc_rbf_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","8a84c15a":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [90, 100],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100, 500, 1000],\n    'random_state' :[0]\n}\n# Create a based model\nrf_model_classification = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search_rf_model_classificaiton = GridSearchCV(estimator = rf_model_classification, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","8e892dff":"grid_search_rf_model_classificaiton.fit(X_train_rfe_classification, y_train_classification)","d479fe7f":"y_rf_classification_pred = grid_search_rf_model_classificaiton.predict(X_test_rfe_classification)","7d5431f9":"y_rf_classification_pred","7c0dda8a":"from sklearn import metrics\ncount_misclassified = (y_test_classification != y_rf_classification_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_rf_classification_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_rf_classification_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_rf_classification_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_rf_classification_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","a6dc9b80":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [10, 50, 90],\n    'max_features': [3],\n    'min_samples_leaf': [3],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100, 500],\n    'learning_rate' : [0.1, 0.2],\n    'random_state' : [0]\n}\n# Create a based model\ngbc_model_classification = GradientBoostingClassifier()\n# Instantiate the grid search model\ngrid_search_gbc_model_classificaiton = GridSearchCV(estimator = gbc_model_classification, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","38367392":"grid_search_gbc_model_classificaiton.fit(X_train_rfe_classification, y_train_classification)","a1bf6685":"y_gbc_model_pred = grid_search_gbc_model_classificaiton.predict(X_test_rfe_classification)","50ecabb8":"y_gbc_model_pred","bce3a937":"from sklearn import metrics\ncount_misclassified = (y_test_classification != y_gbc_model_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_gbc_model_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_gbc_model_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_gbc_model_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_gbc_model_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","2ba3b1c1":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\n     'objective' : ['multi:softmax', 'multi:softprob'],\n     'n_estimators': [100, 500, 1000],\n     'random_state': [0]\n}\n# Create a based model\nxgb_model_classification = XGBClassifier()\n# Instantiate the grid search model\ngrid_search_xgb_model_classificaiton = GridSearchCV(estimator = xgb_model_classification, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","97318e8f":"grid_search_xgb_model_classificaiton.fit(X_train_rfe_classification, y_train_classification)","bb3f012f":"y_xgb_classification_pred = grid_search_xgb_model_classificaiton.predict(X_test_rfe_classification)","d32abd6f":"y_xgb_classification_pred","3369ae61":"from sklearn import metrics\ncount_misclassified = (y_test_classification != y_xgb_classification_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_xgb_classification_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_xgb_classification_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_xgb_classification_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_xgb_classification_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","07f33b8a":"feature_importance = grid_search_gbc_model_classificaiton.best_estimator_.feature_importances_\nsorted_importance = np.argsort(feature_importance)\npos = np.arange(len(sorted_importance))\nplt.figure(figsize=(12,5))\nplt.barh(pos, feature_importance[sorted_importance],align='center')\nplt.yticks(pos, X_train_rfe.columns[sorted_importance],fontsize=15)\nplt.title('Feature Importance ',fontsize=18)\nplt.show()","7b278f41":"To Build a classification Model I would like to reuse the preprocessed data from the Regression Model.\nHowever I am going to replace the target variable and create a new target variable for our classification Model.","1696e7db":"As we see the extracted main Plot keyword also consists of high cardinality but is stable. we can replace it with the value counts","546381b9":"<a id='emc'><\/a>","fbcdb363":"<a id='xgbc'><\/a>","2f8bf6be":"# Statistical Approach to for predicting IMDB","10a0069e":"<a id='svmr'><\/a>","4205816a":"## 4.1 Logistic Regression","f9e2c50a":"## 1.1 Background","754f8169":"### 2.3.2 Profile Report after missing value treatment ","2e429bd5":"# 4. Building a Classification Model","4f66893b":"A commercial success movie not only entertains audience, but also enables film companies to gain tremendous profit. A lot of factors such as good directors, experienced actors are considerable for creating good movies. However, famous directors and actors can always bring an expected box-office income but cannot guarantee a highly rated imdb score.","e5e0137b":"### 2.3.4 Profile Report after data cleaning ","7a8065b4":"## 3.6 Ensemble Models","92dbc993":"The column genres has huge amount of values unique values. Let us divide this feature in to 2 different features with main_genre and the genres","185e0c66":"|imdb_score | Classify |\n| --- | ---|\n|1-3 | Flop Movie|\n|3-6 | Average Movie|\n|6-10 | Hit Movie|","0a330749":"Based on the massive movie information, it would be interesting to understand what are the important factors that make a movie more successful than others. So, we would like to analyze what kind of movies are more successful, in other words, get higher IMDB score. \n\nIn this notebook we are going to build two different kind of models, Regression and Classification. Under each kind of model we are going to start from a basic model to advanced model and also a description of why we choose advanced one. \n\nUnder Regression we are goint to fit Regression line to our data and find the continous target variable imdb_score.\n\nUnder Classification we are  going to fit the Classification Model to our data and the Classify the imdb_score in to three categories. ","7a8d409f":"### 2.3.3 Converting Categoricals to Numericals to feed our model","30b85abf":"Language variable has only 38 unique values and is consistent. So, we just do label encoding.","bcf96810":"Lets convert both the columns in to the numbericals. The main_genre and the genres","314e069a":"## 3.5 Support Vector Machines with Linear, Polynomial, RBF Kernels","c3bfcf90":"As we look in to the profile report we are now having warnings of about the skewness and the zeros. This will be wiped off after doing a scaling operation after dealing with spiltting the dataset. All the unwanted variables will also be removed during the Feature elimination","89ac1ccf":"<a id='datacleaning'><\/a>","99ec626a":"<a id='rmb'><\/a>","020e226c":"Support Vector Classifier also basically does binary classification. In order to achieve the multi classification, we need to use the decision_function_shape as 'ovo'. The original one-vs-one (\u2018ovo\u2019) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) \/ 2)","19005225":"# 3. Regression Model Building","22710347":"## 3.4 Simple Linear Regression","b02c7f8e":"<a id='irr'><\/a>","3ccf856b":"As we see that the Gradient Boost with Hyper Parameter seems to give us the best Results. This is because the nature of Ensemble models tend to being overfitted. However we consider the final model for our classification as Gradient Boosting Classifier.","dde6bd2b":"|imdb_score | Classify |\n| --- | ---|\n1-3 | Flop Movie\n3-6 | Average Movie\n6-10 | Hit Movie","4a3691a9":"<a id='slr'><\/a>","cc15bd6a":"## 3.7 XGBoost with Hyperparameter tuning","7fc5478d":"|Variable Name |\tDescription|\n| --- | --- |\n|movie_title\t | Title of the Movie|\n|duration\t| Duration in minutes|\n|director_name\t| Name of the Director of the Movie|\n|director_facebook_likes |\tNumber of likes of the Director on his Facebook Page|\n|actor_1_name |\tPrimary actor starring in the movie|\n|actor_1_facebook_likes |\tNumber of likes of the Actor_1 on his\/her Facebook Page|\n|actor_2_name |\tOther actor starring in the movie|\n|actor_2_facebook_likes\t| Number of likes of the Actor_2 on his\/her Facebook Page|\n|actor_3_name |\tOther actor starring in the movie|\n|actor_3_facebook_likes |\tNumber of likes of the Actor_3 on his\/her Facebook Page|\n|num_user_for_reviews |\tNumber of users who gave a review|\n|num_critic_for_reviews |\tNumber of critical reviews on imdb|\n|num_voted_users | \tNumber of people who voted for the movie|\n|cast_total_facebook_likes |\tTotal number of facebook likes of the entire cast of the movie|\n|movie_facebook_likes |\tNumber of Facebook likes in the movie page|\n|plot_keywords |\tKeywords describing the movie plot|\n|facenumber_in_poster |\tNumber of the actor who featured in the movie poster|\n|color |\tFilm colorization. \u2018Black and White\u2019 or \u2018Color\u2019|\n|genres |\tFilm categorization like \u2018Animation\u2019, \u2018Comedy\u2019, \u2018Romance\u2019, \u2018Horror\u2019, \u2018Sci-Fi\u2019, \u2018Action\u2019, \u2018Family\u2019|\n|title_year |\tThe year in which the movie is released (1916:2016)|\n|language |\tEnglish, Arabic, Chinese, French, German, Danish, Italian, Japanese etc|\n|country |\tCountry where the movie is produced|\n|content_rating |\tContent rating of the movie|\n|aspect_ratio |\tAspect ratio the movie was made in|\n|movie_imdb_link |\tIMDB link of the movie|\n|gross |\tGross earnings of the movie in Dollars|\n|budget |\tBudget of the movie in Dollars|\n|imdb_score |\tIMDB Score of the movie on IMDB|","a4be4731":"1. [Introduction](#introduction)<br>\n    1.1 [Background](#background)<br>\n    1.2 [Data Description](#datadescription)<br>\n    1.3 [Problem Statement](#problemstatement)<br>\n2. [Data Exploration](#dataexploration)<br>\n    2.1 [Data Loading](#dataloading)<br>\n    2.2 [Data Profile](#dataprofile)<br>\n    2.3 [Data Cleaning](#datacleaning)<br>\n3. [Regression Model Building](#rmb)<br>\n    3.1 [Splitting the Dataset](#std)<br>\n    3.2 [Scaling to avoid Euclidean Distance problem](#s)<br>\n    3.3 [Feature Elimination](#fe)<br>\n    3.4 [Simple Linear Regression](#slr)<br>\n    3.5 [Support Vector Machines with Linear, Polynomial and RBF Kernels](#svmr)<br>\n    3.6 [Ensemble Models](#em)<br>\n     3.6.1 [Gradient Boosting with Hyperparameter Tuning](#gbr)<br>\n     3.6.2 [Random Forest with Hyperparameter Tuning](#rbr)<br>\n    3.7 [XGBoost with Hyperparameter Tuning](#xgbr)<br>\n    3.8 [Interpreting Results of a Regresison Model](#irr)<br>\n4. [Building a Classificaiton Model](#bc)<br>\n    4.1 [Logistic Regression](#lr)<br>\n    4.2 [Support Vector machines with Linear, Polynomial adn RBF Kernels](#svmc)<br>\n    4.3 [Ensemble Models](#emc)<br>\n     4.3.1 [Random Forest with Hyperparameter Tuning](#rfc)<br>\n     4.3.2 [Gradient Boosting with Hyperparameter Tuning](#gbc)<br>\n    4.4 [XGBoost with Hyperparameter Tuning](#xgbc)<br>\n    4.5 [Interpreting Results of Classification Model](#ircm)\n5. [Conclusion](#conclusion)<br>","af32525d":"# 2. Data Exploration","bd1b876a":"<a id='conclusion'><\/a>","34d3a04c":"<a id='em'><\/a>","4e9ed65c":"<a id='bc'><\/a>","7863351d":"So as we see there are only 2 different categorical variables available in the color variable. We can just map color to 1 and 0 to black and white","30da7b91":"## 2.3 Data Cleaning","c61ac470":"### 2.3.1 Missing Value Treatment","d994741b":"<a id='fe'><\/a>","a3afd146":"## 2.1 Data Loading","76aeed3c":"<a id='dataloading'><\/a>","9b84426b":"The dataset is from Kaggle website. It contains 28 variables for 5043 movies, spanning across 100 years in 66 countries. There are 2399 unique director names, and thousands of actors\/actresses. \u201cimdb_score\u201d is the response variable while the other 27 variables are possible predictors.","dbe709bc":"Considering Gradient Boosting classifier as the final model with 83 % accuracy","24df7a70":"This variable movie_imdb_link is however unique the whole. So considering it will not help out prediciting variable we drop it off.","0191a75d":"We dont want our model to feed with all the variables which might mot help in prediction. We do remove variables having High Collinearity and use only variables useful for our model by doing the Recursive Feature Elimination.","d7f0f9b5":"## 4.4 XG Boost Classifier with Hyper Parameter Tuning ","34c5b697":"<a id='datadescription'><\/a>","b56dca92":"Data Cleaning is a most important part of building a model. Here we do the standard preprocessing steps of the Data cleaning to make sure our model is not feeded crap.","a2573bd6":"After looking in to all the metrics almost we have seen that XGBRegressor with \"{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 500}\" these parameters has given the best results with mean squared error of 0.404. The Feature Importance given by this model is shown above.","c083900a":"<a id='s'><\/a>","5c61359e":"Building our model. As we are having many number of features, out of which there will be only some useful. Lets do some feature selection for our Regression model.","f94d263c":"The variable actor_1_name is also having high cardinaity, hence we decide to change it in to the number of counts","a57829bf":"Looking in to the variable, we can see has a high cardinality which is unstable and we can delete such variable and mainly, we need to extract the main_plot_keywords of all in it.","3527d05f":"<a id='lr'><\/a>","12982924":"Lets tweek in to the hyperparameter tuning of the RandomForestRegressor to find the best parameters of the model","9ed24cda":"<a id='ircm'><\/a>","cf6dd5f1":"## 2.2 Data Profile","f993324a":"<a id='introduction'><\/a>","4d4c8204":"<a id='gbc'><\/a>","dccf3cd8":"## 3.8 Interpreting Results of Regression Model","7e5a1855":"After Looking in to the feature importance of the best models in the Regression and Classification Model we see that both the models have given almost the same amount of importance to the respective features, considering XGBosot Regressor and Gradient Boost Classiifier. The results of all Regression and Classification Models are as follows:\n\n|Regression Model|Mean_squared_error|\n| --- | --- |\n|Simple Linear Regression |0.70|\n|SVRegressor Linear|0.72|\n|SVRegressor Polynomial|0.93|\n|SVRegressor RBF|0.68|\n|Gradient Boost|0.43|\n|Random Forest|0.45|\n|XGBoost|0.40|\n\n|Classification  Model|MisClassifications|Accuracy|Precision|Recall|F1-Score|\n| --- | --- | --- | --- | --- | --- |\n| Logistic Regression | 190 | 0.75 | 0.47 | 0.40 | 0.41 |\n| SVC Linear | 181 | 0.76 | 0.47 | 0.45 | 0.46 |\n| SVC Polynomial | 143 | 0.81 | 0.52 | 0.50 | 0.51 |\n| SVC RBF | 146 | 0.81 | 0.51 | 0.50 | 0.50 |\n| Random Forest | 130 | 0.83 | 0.54 | 0.50 | 0.51 |\n| Gradient Boosting | 127 | 0.83 | 0.54 | 0.51 | 0.52 |\n| XGBoost | 139 | 0.82 | 0.52 | 0.51 | 0.51 |\n","bdceb2ec":"## 1.3 Problem Statement","8f6817f4":"As we see out of 3816 records, we have 3749 unique records which in not helpful for us for making predictions. So we drop the column from our dataframe","59a2ffa6":"### 4.3.1 Random Forest Classifier with Hyper Parameter tuning ","9a9209eb":"<a id='dataprofile'><\/a>","43360126":"Content rating has only 12 unique variables and can be done label encoding","60128e1f":"# 5. Conclusion","df08825c":"<a id='dataexploration'><\/a>","99e29c4a":"<a id='bakground'><\/a>","fd5d9f03":"<a id='gbr'><\/a>","46e6c014":"Dealing with Null Data amount we have lost 25% of the given data. Let's deal with converting the Data in to numericals to feed our model. ","6467dc3d":"## 4.3 Ensemble Models","75a4fbb2":"Let us deal with the categorical_cols first by converting them in to numericals.","4e10f091":"## 1.2 Data Description","23585f97":"# 1 Introduction","4b312f0d":"## 3.1 Splitting the Dataset","9ed49920":"<a id='asvmc'><\/a>","a08a7237":"<a id='std'><\/a>","c4111a04":"<a id='xgbr'><\/a>","06867ef9":"This variable also has high cadinality. So changing it in to the value counts variable.","1f08955a":"We do scaling after we aplit the dataset as we donot want to make our training set metrics to fit the test set.","356748c2":"## 4.2 Support Vector Classifier with Linear, Polynomial, RBF","23aa6b57":"<a id='problemstatement'><\/a>","8c1b7ec6":"## 3.2 Scaling to avoid Euclidean Distance Problem","bb186817":"We have created the target variable and now we will re use the independent variables form the Regression Model.","cb4b7355":"Country variable has only 47 unique values and is consistent. So, we just do label encoding.","8b07c8ef":"## 3.3 Feature Elimination ","1fbde27f":"<a id='rbr'><\/a>","d81c401f":"After looking in to the stats, We observe that the r2 score is low of about 0.37 aafter having all consistent variables and the regression line is not fitting the data correctly. So we have to go for much advanced curved model such as support vector machine and ensemble algorithms to make our model to fit the data correctly.","97bfa273":"### 3.6.1 Gradient Boosting with Hyper Parameter Tuning","4cf1351c":"Logistic Regresion is a linear algorithm does basically a binary classification. In order to use the Logistic Regression for Multiclass Classification we need to use the parameter solver as 'saga'. There are also other parameters for solver to do multiclass classification, I used saga as it also does L2 regularisation.","6f6e4f03":"# 4.5 Interpreting Results of Classfication Model","a2a18bc0":"Considering XG Boost as a final model with very less error rate.","663713e5":"### 4.3.2 Gradient Boost Classifier with Hyper Parameter Tuning ","3e5382ff":"<a id='rfc'><\/a>","6b5e6b34":"### 3.6.2 Random Forest with Hyper Parameter Tuning "}}