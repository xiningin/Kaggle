{"cell_type":{"ab4e3b28":"code","f05e57d1":"code","40b659e9":"code","6297072f":"code","fb1e7869":"code","507fd8de":"code","ec92ba2f":"code","6ae59eed":"code","d1810745":"code","5577b9ed":"code","4eae6898":"code","bf4cfc52":"code","74ec09ea":"code","c34718e3":"code","b42b8ad2":"code","0668a750":"code","e9b5935f":"code","4b74e62b":"code","93f9b1bb":"code","29800605":"code","18f24aea":"code","429504e4":"code","94d988b0":"code","d1afc87f":"code","1d16ab89":"code","cd801e58":"code","312dcf4e":"code","07da42db":"code","f5c23d4d":"code","718f6d58":"code","7a177320":"code","a37fc81e":"code","aa870258":"code","3ec7f41d":"code","1c47607f":"code","7d7f54d6":"code","0f08345b":"code","fecb802d":"code","c90a6b0e":"code","c6915bb6":"code","f2347b6f":"code","cfcca3be":"code","dc5b34b1":"code","72b84563":"code","663d5b1e":"code","a7dc44bd":"code","c884911d":"code","7a01161a":"code","d25f09a2":"code","49ffa54e":"code","04e6b409":"code","b167a022":"code","90fe695e":"code","47d72ca3":"code","59d5f2b6":"code","f39eb6c5":"code","f766b938":"code","896ef2bf":"code","c6d42153":"code","15f5f6a8":"code","9db04454":"code","148e1318":"code","297df536":"code","816a578e":"code","74ab24f2":"code","431639ee":"code","9b28caf5":"code","b74eff2d":"code","2383e47b":"code","e7c143ce":"code","bc9d53b8":"code","f520c80e":"code","9970ca0d":"code","7a23401f":"code","ec770881":"code","d4321f58":"code","bf7705e9":"code","21587a69":"code","16981e45":"code","226d3443":"code","0e097ddc":"code","3b8696ce":"code","ac7d07a5":"code","a94de8a5":"code","5c89a54e":"code","ddc40333":"code","f711613b":"code","e351bf9b":"code","6c74898d":"code","5c5f7e40":"code","61c4bd12":"code","157d44e2":"code","67e8a086":"code","d684df4f":"code","372638e6":"code","145023a7":"code","6d131563":"code","bf148660":"code","62ddc023":"code","64646901":"code","098fa0df":"code","7680b793":"code","10de9bae":"code","695f9dfd":"code","ff889df3":"code","c26f7004":"code","b5cdf760":"code","57c139c4":"code","14c24337":"code","d6fdd904":"code","c54f1e11":"code","b14df8c3":"code","f5e5cb94":"code","758842cd":"code","80728354":"code","51fd21ad":"code","a6550b1c":"code","d35d10bc":"code","ceade4a4":"code","1941d772":"code","57949535":"code","6c660317":"code","b4ec2a57":"code","d9d7a67b":"code","4854313a":"code","9dd35144":"code","ac840ace":"code","3ab96e85":"code","2dc42326":"code","6e44211a":"code","42b4c98f":"code","23f19a0e":"code","a4b1a646":"code","09917e8c":"code","cbbd30c3":"code","3a35e24c":"code","40495e59":"code","e97f752e":"code","c278374f":"code","7a544ce0":"code","8f5aacd8":"code","c2f0dcba":"markdown","75e96e1e":"markdown","025341c3":"markdown","75ad0052":"markdown","bf7ed2f0":"markdown","8dca5e5f":"markdown","9eee29ce":"markdown","bf8637d7":"markdown","e3f9e9e2":"markdown","63acc351":"markdown","af24e8c7":"markdown","97b893bd":"markdown","3c52c5a0":"markdown","dcb71658":"markdown","a0424a4f":"markdown","1e5f9620":"markdown","eba2b411":"markdown","0792cb4d":"markdown","6aa13351":"markdown","77262774":"markdown","21d6d8cb":"markdown","113f37c8":"markdown","efe332a7":"markdown","e52d7b61":"markdown","c83f32ea":"markdown","8ed5be8b":"markdown","768af090":"markdown","6b841e17":"markdown","91a9b4b8":"markdown","cd2ba607":"markdown","9398cc19":"markdown","4b9167ab":"markdown","37fb7297":"markdown","1299f99f":"markdown","5a0c1b1b":"markdown","40069cdf":"markdown","804907e2":"markdown","8cfa93f7":"markdown","6c297fb8":"markdown","98a7f3a8":"markdown","6bf837df":"markdown","02ff3077":"markdown","b5636d44":"markdown","46326f13":"markdown","20b1cca8":"markdown","e7f93c64":"markdown","cbd830d8":"markdown","21c2db6a":"markdown","657ad098":"markdown","175df14b":"markdown","50e58e26":"markdown","1bdbc27e":"markdown","8889a276":"markdown","a51486af":"markdown","d9d7469e":"markdown","85cc77f7":"markdown","296e92b2":"markdown","3eec8e88":"markdown","049a5b71":"markdown","0cfcd95c":"markdown","7cce8b44":"markdown","b5dc3b7c":"markdown","f0faca73":"markdown","980294e6":"markdown","72c7c224":"markdown","1852bf2c":"markdown","591421c8":"markdown","14e32ee8":"markdown","4c642827":"markdown","755273f7":"markdown","fef433ba":"markdown","c4a17c38":"markdown","bbb46365":"markdown","92359685":"markdown","c897c4fe":"markdown","4920de9d":"markdown","ae3c4011":"markdown","2c8f2a77":"markdown","be5df6d0":"markdown","aec83fb5":"markdown","0c409548":"markdown","09aaa475":"markdown","7d2e7352":"markdown","8dc6e7a7":"markdown","f186c5dd":"markdown","3665e59e":"markdown","626ccc2a":"markdown","93b79428":"markdown","b049afa1":"markdown","b85d1c08":"markdown","37f621c5":"markdown","a0134b68":"markdown","69e5853d":"markdown","80bda292":"markdown","0038ffb5":"markdown","d6e7899d":"markdown","36cdf6d3":"markdown"},"source":{"ab4e3b28":"# They are for data manipulation\/ \uae30\ubcf8 \ub370\uc774\ud130 \uc815\ub9ac \ubc0f \ucc98\ub9ac\nimport pandas as pd\nimport numpy as np\n\n# For Visualization \/ \uc2dc\uac01\ud654\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\nimport missingno\n\n# For preprocessing and ML algorithms \/ \uc804\ucc98\ub9ac \ubc0f \uba38\uc2e0 \ub7ec\ub2dd \uc54c\uace0\ub9ac\uc998\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Tunning and Evaluation \/ \ubaa8\ub378 \ud29c\ub2dd \ubc0f \ud3c9\uac00\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import model_selection\n\n# Ignore warnings \/ \uacbd\uace0 \uc81c\uac70 (Pandas often makes warnings)\nimport sys\nimport warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","f05e57d1":"# This may be harder than expected for a firsttimer, but if the data was not delivered correctly from the copy, press \"+ Add Data\", load \"Titanic Data\" from \"Competition Data\", and click each file to check the path address.\n# \uc774 \uac83\uc774 \ucc98\uc74c\ud558\ub294 \uc0ac\ub78c\uc5d0\uac8c \uc608\uc0c1\ubcf4\ub2e4 \uc5b4\ub824\uc6b8 \uc218 \uc788\ub294\ub370 \ubcf5\uc0ac\ud55c \uac83\uc5d0\uc11c \ub370\uc774\ud130\uac00 \uc804\ub2ec\uc774 \uc798 \uc548 \ub418\uc5c8\ub2e4\uba74 \"+Add Data\" \ub204\ub974\uc2dc\uace0 'Competition Data'\uc5d0\uc11c \"Titanic Data\" \ubd88\ub7ec\uc628 \ud6c4 \ud30c\uc77c\uc744 \ucc0d\uc5b4\uc11c \uacbd\ub85c \uc8fc\uc18c \ud655\uc778\ud574\uc57c \ud568 \ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\n\n# Now csv files, test and train, have become data frames.    ","40b659e9":"train.head()","6297072f":"train.tail()","fb1e7869":"train.describe(include='all')","507fd8de":"train.dtypes","ec92ba2f":"train.info()","6ae59eed":"train.columns","d1810745":"train.columns[3], train.columns[3:5]","5577b9ed":"train[5:20]","4eae6898":"train.shape","bf4cfc52":"# \ubcd1\ud569 \uc900\ube44\nntrain = train.shape[0]\nntest = test.shape[0]\n\n# \uc544\ub798\ub294 \ub530\ub85c \uc798 \ubaa8\uc154 \ub461\ub2c8\ub2e4.\ny_train = train['Survived'].values\npassId = test['PassengerId']\n\n# \ubcd1\ud568 \ud30c\uc77c \ub9cc\ub4e4\uae30\ndata = pd.concat((train, test))\n\n# \ub370\uc774\ud130 \ud589\uacfc \uc5f4\uc758 \ud06c\uae30\ub294\nprint(\"data size is: {}\".format(data.shape))","74ec09ea":"train['Survived'].value_counts()","c34718e3":"missingno.matrix(data, figsize = (15,8))","b42b8ad2":"data.isnull().sum() #\ube44\uc5b4 \uc788\ub294 \uac12\ub4e4\uc744 \uccb4\ud06c\ud574 \ubcf8\ub2e4.","0668a750":"data.Age.isnull().any()","e9b5935f":"data.columns","4b74e62b":"train.head()","93f9b1bb":"# Co-relation \ub9e4\ud2b8\ub9ad\uc2a4\ncorr = data.corr()\n# \ub9c8\uc2a4\ud06c \uc14b\uc5c5\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# \uadf8\ub798\ud504 \uc14b\uc5c5\nplt.figure(figsize=(14, 8))\n# \uadf8\ub798\ud504 \ud0c0\uc774\ud2c0\nplt.title('Overall Correlation of Titanic Features', fontsize=18)\n#  Co-relation \ub9e4\ud2b8\ub9ad\uc2a4 \ub7f0\uce6d\nsns.heatmap(corr, mask=mask, annot=False,cmap='RdYlGn', linewidths=0.2, annot_kws={'size':20})\nplt.show()","29800605":"fig = plt.figure(figsize=(10,2))\nsns.countplot(y='Survived', data=train)\nprint(train.Survived.value_counts())","18f24aea":"f,ax=plt.subplots(1, 2, figsize=(15, 6))\ntrain['Survived'].value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[0], shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=train, ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","429504e4":"def piecount(col):\n    f, ax = plt.subplots(1, 2, figsize=(15, 6))\n    train[col].value_counts().plot.pie(explode=[0.1 for i in range(train[col].nunique())], autopct='%1.1f%%', ax=ax[0], shadow=True)\n    ax[0].set_title(col)\n    ax[0].set_ylabel('')\n    sns.countplot(col, data=train, ax=ax[1])\n    ax[1].set_title(col)\n    plt.show()\n\npiecount('Survived')","94d988b0":"piecount(\"Pclass\")","d1afc87f":"train.groupby(['Pclass','Survived'])['Survived'].count()","1d16ab89":"pd.crosstab(train.Pclass, train.Survived, margins=True).style.background_gradient(cmap='summer_r')","cd801e58":"f, ax = plt.subplots(1, 2, figsize=(12, 6))\ntrain[['Pclass','Survived']].groupby(['Pclass']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived per Pcalss')\nsns.countplot('Pclass', hue='Survived', data=train, ax=ax[1])\nax[1].set_title('Pcalss Survived vs Not Survived')\nplt.show()","312dcf4e":"piecount(\"Pclass\")","07da42db":"data.Name.value_counts()","f5c23d4d":"temp = data.copy()\ntemp['Initial'] = 0\ntemp['Initial'] = data.Name.str.extract('([A-Za-z0-9]+)\\.')","718f6d58":"temp['Initial'].value_counts()","7a177320":"pd.crosstab(temp.Initial, temp.Sex).T.style.background_gradient(cmap='summer_r')","a37fc81e":"def survpct(col):\n    return temp.groupby(col)['Survived'].mean()\n\nsurvpct('Initial')","aa870258":"temp['LastName'] = data.Name.str.extract('([A-Za-z]+)')","3ec7f41d":"pd.crosstab(temp.LastName, temp.Survived).T.style.background_gradient(cmap='summer_r')","1c47607f":"temp.loc[temp['Initial'] == 'Dona']","7d7f54d6":"temp.loc[temp['Initial'] == 'Dona', 'Initial'] = 'Mrs'","0f08345b":"pd.crosstab(temp.Initial, temp.Survived).T.style.background_gradient(cmap='summer_r')","fecb802d":"temp['NumName'] = temp['LastName'].factorize()[0]","c90a6b0e":"pd.crosstab(temp.NumName, temp.Survived).T.style.background_gradient(cmap='summer_r')","c6915bb6":"temp.loc[temp['LastName'] == 'Ali']","f2347b6f":"train[['Sex','Survived']].groupby(['Sex']).mean()","cfcca3be":"def bag(col, target, title, title1):\n    f,ax=plt.subplots(1,2,figsize=(12,5))\n    train.groupby([col])[target].mean().plot(kind='bar', ax=ax[0])\n    ax[0].set_title(title)\n    sns.countplot(col, hue=target, data=train, ax=ax[1])\n    ax[1].set_title(title1)\n    plt.show()\n\nbag('Sex','Survived','Survived per Sex','Sex Survived vs Not Survived')","dc5b34b1":"pd.crosstab([train.Sex, train.Survived],train.Pclass,margins=True).style.background_gradient(cmap='summer_r')","72b84563":"print('Oldest Passenger was', data['Age'].max(), 'Years')\nprint('Youngest Passenger was', data['Age'].min(), 'Years')\nprint('Average Age on the ship was', int(data['Age'].mean()), 'Years')","663d5b1e":"sns.swarmplot(x=train['Survived'], y=train['Age'])\nplt.xlabel(\"Survived\")\nplt.ylabel(\"Age\")\nplt.show()","a7dc44bd":"f, ax = plt.subplots(1,2,figsize=(18,8))\nsns.violinplot(\"Pclass\", \"Age\", hue=\"Survived\", data=train, split=True, ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0, 110, 10))\nsns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=train, split=True, ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0, 110, 10))\nplt.show()","c884911d":"temp.groupby('Initial').agg({'Age': ['mean', 'count']}) #\uc774\ub2c8\uc15c \ubcc4 \ud3c9\uade0 \uc5f0\ub839 \uccb4\ud06c","7a01161a":"# Assining NaN Age items with mean value of Initials\ntemp = temp.reset_index(drop=True)\n\ntemp['Age'] = temp.groupby('Initial')['Age'].apply(lambda x: x.fillna(x.mean()))\n\ntemp[31:50]","d25f09a2":"temp['Initial'].replace(['Capt', 'Col', 'Countess', 'Don', 'Dona' , 'Dr', 'Jonkheer', 'Lady', 'Major', 'Master',  'Miss'  ,'Mlle', 'Mme', 'Mr', 'Mrs', 'Ms', 'Rev', 'Sir'], ['Sacrificed', 'Respected', 'Nobles', 'Mr', 'Mrs', 'Respected', 'Mr', 'Nobles', 'Respected', 'Kids', 'Miss', 'Nobles', 'Nobles', 'Mr', 'Mrs', 'Nobles', 'Sacrificed', 'Nobles'],inplace=True)\ntemp['Initial'].replace(['Kids', 'Miss', 'Mr', 'Mrs', 'Nobles', 'Respected', 'Sacrificed'], [4, 4, 2, 5, 6, 3, 1], inplace=True)","49ffa54e":"temp['Age_Range'] = pd.qcut(temp['Age'], 10)","04e6b409":"survpct('Age_Range')","b167a022":"temp['Agroup'] = 0\n\ntemp.loc[temp['Age'] < 1.0, 'Agroup'] = 1\ntemp.loc[(temp['Age'] >=1.0) & (temp['Age'] <= 3.0), 'Agroup'] = 2\ntemp.loc[(temp['Age'] > 3.0) & (temp['Age'] < 11.0), 'Agroup'] = 7\ntemp.loc[(temp['Age'] >= 11.0) & (temp['Age'] < 15.0), 'Agroup'] = 13\ntemp.loc[(temp['Age'] >= 15.0) & (temp['Age'] < 18.0), 'Agroup'] = 16\ntemp.loc[(temp['Age'] >= 18.0) & (temp['Age'] <=  20.0), 'Agroup'] = 18\ntemp.loc[(temp['Age'] > 20.0) & (temp['Age'] <= 22.0), 'Agroup'] = 21\ntemp.loc[(temp['Age'] > 22.0) & (temp['Age'] <= 26.0), 'Agroup'] = 24\ntemp.loc[(temp['Age'] > 26.0) & (temp['Age'] <= 30.0), 'Agroup'] = 28\ntemp.loc[(temp['Age'] > 30.0) & (temp['Age'] <= 32.0), 'Agroup'] = 31\ntemp.loc[(temp['Age'] > 32.0) & (temp['Age'] <= 34.0), 'Agroup'] = 33\ntemp.loc[(temp['Age'] > 34.0) & (temp['Age'] <= 38.0), 'Agroup'] = 36\ntemp.loc[(temp['Age'] > 38.0) & (temp['Age'] <= 52.0), 'Agroup'] = 45\ntemp.loc[(temp['Age'] > 52.0) & (temp['Age'] <= 75.0), 'Agroup'] = 60\ntemp.loc[temp['Age'] > 75.0, 'Agroup'] = 78\n","90fe695e":"temp.head()","47d72ca3":"temp['Gclass']=0\ntemp.loc[((temp['Sex'] == 'male') & (temp['Pclass'] == 1)), 'Gclass'] = 1\ntemp.loc[((temp['Sex'] == 'male') & (temp['Pclass'] == 2)), 'Gclass'] = 2\ntemp.loc[((temp['Sex'] == 'male') & (temp['Pclass'] == 3)), 'Gclass'] = 2\ntemp.loc[((temp['Sex'] == 'female') & (temp['Pclass'] == 1)), 'Gclass'] = 3\ntemp.loc[((temp['Sex'] == 'female') & (temp['Pclass'] == 2)), 'Gclass'] = 4\ntemp.loc[((temp['Sex'] == 'female') & (temp['Pclass'] == 3)), 'Gclass'] = 5\ntemp.loc[(temp['Age'] < 1), 'Gclass'] = 6","59d5f2b6":"survpct('Gclass')","f39eb6c5":"# Fill in missing Fare value by overall Fare mean\ntemp['Fare'].fillna(temp['Fare'].mean(), inplace=True)\n\n# Setting coin flip (e.g. random chance of surviving)\ndefault_survival_chance = 0.5\ntemp['Family_Survival'] = default_survival_chance\n\n# Grouping data by last name and fare - looking for families\nfor grp, grp_df in temp[['Survived','Name', 'LastName', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['LastName', 'Fare']):\n    \n    # If not equal to 1, a family is found \n    # Then work out survival chance depending on whether or not that family member survived\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                temp.loc[temp['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin == 0.0):\n                temp.loc[temp['PassengerId'] == passID, 'Family_Survival'] = 0\n\n# Print the headline\nprint(\"Number of passengers with family survival information:\", \n      temp.loc[temp['Family_Survival']!=0.5].shape[0])","f766b938":"# If not equal to 1, a group member is found\n# Then work out survival chance depending on whether or not that group member survived\nfor _, grp_df in temp.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    temp.loc[temp['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    temp.loc[temp['PassengerId'] == passID, 'Family_Survival'] = 0\n\n# Print the headline\nprint(\"Number of passenger with family\/group survival information: \" \n      +str(temp[temp['Family_Survival']!=0.5].shape[0]))","896ef2bf":"bag('Parch', 'Survived', 'Survived per Parch', 'Parch Survived vs Not Survived')","c6d42153":"pd.crosstab([temp.Family_Survival, temp.Survived], temp.Pclass, margins=True).style.background_gradient(cmap='summer_r')","15f5f6a8":"# Creating two features of relatives and not alone\ntemp['Family Size'] = temp['SibSp'] + temp['Parch']","9db04454":"survpct('Family Size')","148e1318":"temp.Ticket.head()","297df536":"temp.Ticket.isnull().any()","816a578e":"temp['Initick'] = temp.Ticket.str.extract('^([A-Za-z]+)')\n\ntemp = temp.reset_index(drop=True)  # to avoid `ValueError: cannot reindex from a duplicate axis`\n\ntemp.loc[temp.Initick.isnull(), 'Initick'] = temp['Ticket']\n\ntemp.head()","74ab24f2":"temp['NumTicket'] = temp['Initick'].factorize()[0]","431639ee":"temp.head(n=15)","9b28caf5":"temp.groupby('NumTicket')['Survived'].mean().to_frame().plot(kind='hist')\nplt.title('Distribution of survival rate for different tickets');","b74eff2d":"print('Highest Fare was:', temp['Fare'].max())\nprint('Lowest Fare was:', temp['Fare'].min())\nprint('Average Fare was:', temp['Fare'].mean())","2383e47b":"f,ax=plt.subplots(1, 3, figsize=(20, 6))\nsns.distplot(train[train['Pclass'] == 1].Fare,ax=ax[0])\nax[0].set_title('Fares in Pclass 1')\nsns.distplot(train[train['Pclass'] == 2].Fare,ax=ax[1])\nax[1].set_title('Fares in Pclass 2')\nsns.distplot(train[train['Pclass'] == 3].Fare,ax=ax[2])\nax[2].set_title('Fares in Pclass 3')\nplt.show()","e7c143ce":"def groupmean(a,b):\n    return temp.groupby([a])[b].mean().to_frame().style.background_gradient(cmap='summer_r')\n\ntemp['Fare_Range'] = pd.qcut(train['Fare'], 10)\ngroupmean('Fare_Range', 'Fare')","bc9d53b8":"temp['Fgroup'] = 0\n\ntemp.loc[temp['Fare'] <= 0,'Fgroup'] = 0\ntemp.loc[(temp['Fare'] > 0) & (temp['Fare'] <= 7.125), 'Fgroup'] = 1\ntemp.loc[(temp['Fare'] > 7.125) & (temp['Fare'] <= 7.9), 'Fgroup'] = 2\ntemp.loc[(temp['Fare'] > 7.9) & (temp['Fare'] <= 8.03), 'Fgroup'] = 3\ntemp.loc[(temp['Fare'] > 8.03) & (temp['Fare'] < 10.5), 'Fgroup'] = 4\ntemp.loc[(temp['Fare'] >= 10.5) & (temp['Fare'] < 23.0), 'Fgroup'] = 5\ntemp.loc[(temp['Fare'] >= 23.0) & (temp['Fare'] <= 27.8), 'Fgroup'] = 6\ntemp.loc[(temp['Fare'] > 27.8) & (temp['Fare'] <= 51.0), 'Fgroup'] = 7\ntemp.loc[(temp['Fare'] > 51.0) & (temp['Fare'] <= 73.5), 'Fgroup'] = 8\ntemp.loc[temp['Fare'] > 73.5, 'Fgroup'] = 9\n\ntemp.head()","f520c80e":"temp.Cabin.value_counts().head(10)","9970ca0d":"temp.Cabin.isnull().sum()","7a23401f":"temp['Inicab'] = 0\ntemp['Inicab'] = temp['Cabin'].str.extract('^([A-Za-z]+)')\ntemp.loc[((temp['Cabin'].isnull()) & (temp['Pclass'].values == 1)), 'Inicab'] = 'X'\ntemp.loc[((temp['Cabin'].isnull()) & (temp['Pclass'].values == 2)), 'Inicab'] = 'Y'\ntemp.loc[((temp['Cabin'].isnull()) & (temp['Pclass'].values == 3)), 'Inicab'] = 'Z'\n    \ntemp.head()","ec770881":"temp['Inicab'] = temp['Inicab'].factorize()[0]\n    \ntemp[11:20]","d4321f58":"pd.crosstab([temp.Embarked, temp.Pclass], [temp.Sex, temp.Survived], margins=True).style.background_gradient(cmap='summer_r')","bf7705e9":"sns.factorplot('Embarked', 'Survived', data=temp)\nfig = plt.gcf()\nfig.set_size_inches(5, 3)\nplt.show()","21587a69":"f,ax=plt.subplots(2,2,figsize=(20,15))\nsns.countplot('Embarked', data=temp, ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked', hue='Sex', data=temp, ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked', hue='Survived', data=temp, ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked', hue='Pclass', data=temp, ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","16981e45":"temp.loc[(temp.Embarked.isnull())]","226d3443":"temp.loc[(temp.Ticket == '113572')]","0e097ddc":"temp.sort_values(['Ticket'], ascending = True)[55:70]","3b8696ce":"temp.loc[(temp.Embarked.isnull()), 'Embarked'] = 'S'","ac7d07a5":"temp.loc[(temp.Embarked.isnull())]","a94de8a5":"temp['Embarked'] = temp['Embarked'].factorize()[0]\n    \ntemp[11:20]","5c89a54e":"temp['Priority'] = 0\ntemp.loc[(temp['Initial'] == 6), 'Priority'] = 1\ntemp.loc[(temp['Gclass'] == 3), 'Priority'] = 2\ntemp.loc[(temp['Gclass'] == 6), 'Priority'] = 3\ntemp.loc[(temp['Pclass'] == 1) & (temp['Age'] <= 17), 'Priority'] = 4\ntemp.loc[(temp['Pclass'] == 2) & (temp['Age'] <= 17), 'Priority'] = 5\ntemp.loc[(temp['Pclass'] == 2) & (temp['Sex'] == 2), 'Priority'] = 6\ntemp.loc[(temp['Fgroup'] == 9), 'Priority'] = 7\n\nsurvpct('Priority')","ddc40333":"temp['FH'] = 0\ntemp.loc[(temp['Gclass'] == 1), 'FH'] = 0\ntemp.loc[(temp['Gclass'] == 2), 'FH'] = 0\ntemp.loc[(temp['Gclass'] == 3), 'FH'] = 1\ntemp.loc[(temp['Gclass'] == 4) & (temp['Family Size'] == 2), 'FH'] = 2\ntemp.loc[(temp['Gclass'] == 4) & (temp['Family Size'] == 3), 'FH'] = 3\ntemp.loc[(temp['Gclass'] == 4) & (temp['Family Size'] == 4), 'FH'] = 4\ntemp.loc[(temp['Gclass'] == 4) & (temp['Family Size'] == 1) & (temp['Pclass'] == 1), 'FH'] = 5\ntemp.loc[(temp['Gclass'] == 4) & (temp['Family Size'] == 1) & (temp['Pclass'] == 2), 'FH'] = 6\ntemp.loc[(temp['Gclass'] == 4) & (temp['Fgroup'] == 3), 'FH'] = 7\ntemp.loc[(temp['Gclass'] == 4) & (temp['Fgroup'] >= 5), 'FH'] = 8\n\nsurvpct('FH')","f711613b":"temp['MH'] = 0\ntemp.loc[(temp['Sex'] == 2), 'MH'] = 0\ntemp.loc[(temp['Gclass'] == 1), 'MH'] = 1\ntemp.loc[(temp['Gclass'] == 1) & (temp['Family Size'] == 2), 'MH'] = 2\ntemp.loc[(temp['Gclass'] == 1) & (temp['Family Size'] == 3), 'MH'] = 3\ntemp.loc[(temp['Gclass'] == 1) & (temp['Family Size'] == 4), 'MH'] = 4\ntemp.loc[(temp['Gclass'] == 1) & (temp['Family Size'] == 1) & (temp['Pclass'] == 1), 'MH'] = 5\ntemp.loc[(temp['Gclass'] == 1) & (temp['Family Size'] == 1) & (temp['Pclass'] == 2), 'MH'] = 6\ntemp.loc[(temp['Gclass'] == 1) & (temp['Fgroup'] == 3), 'MH'] = 7\ntemp.loc[(temp['Gclass'] == 1) & (temp['Fgroup'] >= 5), 'MH'] = 8\n\nsurvpct('MH')","e351bf9b":"temp['FL'] = 0\ntemp.loc[(temp['Gclass'] != 5), 'FL'] = 0\ntemp.loc[(temp['Gclass'] == 5) & (temp['Fgroup'] < 5), 'FL'] = 1\ntemp.loc[(temp['Gclass'] == 5) & (temp['Fgroup'] != 3), 'FL'] = 2\ntemp.loc[(temp['Gclass'] == 5) & (temp['FH'] == 1), 'FL'] = 3\ntemp.loc[(temp['Gclass'] == 5) & (temp['Family Size'] < 2), 'FL'] = 4\ntemp.loc[(temp['Gclass'] == 5) & (temp['Family Size'] > 4), 'FL'] = 5\ntemp.loc[(temp['Gclass'] == 5) & (temp['Family Size'] == 1) & (temp['Pclass'] == 3), 'FL'] = 6","6c74898d":"survpct('FL')","5c5f7e40":"temp['ML'] = 0\ntemp.loc[(temp['Gclass'] == 2) & (temp['Fgroup'] < 5), 'ML'] = 1\ntemp.loc[(temp['Gclass'] == 2) & (temp['Fgroup'] != 3), 'ML'] = 2\ntemp.loc[(temp['Gclass'] == 2) & (temp['MH'] <7), 'ML'] = 3\ntemp.loc[(temp['Gclass'] == 2) & (temp['Family Size'] < 2), 'ML'] = 4\ntemp.loc[(temp['Gclass'] == 2) & (temp['Family Size'] > 4), 'ML'] = 5\ntemp.loc[(temp['Gclass'] == 2) & (temp['Family Size'] == 1) & (temp['Pclass'] == 3), 'ML'] = 6\ntemp.loc[(temp['Gclass'] == 3) & (temp['Fgroup'] < 5), 'ML'] = 1\ntemp.loc[(temp['Gclass'] == 3) & (temp['Fgroup'] != 3), 'ML'] = 2\ntemp.loc[(temp['Gclass'] == 3) & (temp['MH'] <7), 'ML'] = 3\ntemp.loc[(temp['Gclass'] == 3) & (temp['Family Size'] < 2), 'ML'] = 4\ntemp.loc[(temp['Gclass'] == 3) & (temp['Family Size'] > 4), 'ML'] = 5\ntemp.loc[(temp['Gclass'] == 3) & (temp['Family Size'] == 1) & (temp['Pclass'] == 3), 'ML'] = 6\n\nsurvpct('ML')","61c4bd12":"from random import randint\ntemp['RAND'] = [ randint(1,149)  for k in temp.index]\n\ntemp.head(10)","157d44e2":"temp['Score1']= 0\ntemp.loc[(temp['Priority'] > 0), 'Score1'] = 1\ntemp.loc[(temp['RAND'] > 116), 'Score1'] = 1\n\ntemp.Score1.value_counts()","67e8a086":"from random import randint\ntemp['rand'] = [ randint(1,9)  for k in temp.index]\n\ntemp.head(10)","d684df4f":"temp['Score2']= 0\ntemp.loc[(temp['Priority'] > 0), 'Score2'] = 1\ntemp.loc[((temp['Family Size'] > 1) & (temp['Family Size'] < 5)), 'Score2'] = 1\ntemp.loc[(temp['rand'] > 7), 'Score2'] = 1\n\ntemp.Score2.value_counts()","372638e6":"missingno.matrix(temp, figsize = (15,8))","145023a7":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder","6d131563":"dfl = pd.DataFrame() # for label encoding","bf148660":"good_columns = ['Priority', 'Gclass', 'Agroup', 'Family_Survival', 'Initial','NumName', 'Initick', 'FL', 'ML', 'FH', 'MH', 'Fgroup', 'Family Size','Embarked', 'Score1', 'Score2']\ndfl[good_columns] = temp[good_columns]","62ddc023":"dfl.head()","64646901":"dfh = dfl.copy()","098fa0df":"dfl_enc = dfl.apply(LabelEncoder().fit_transform)\n                          \ndfl_enc.head(20)","7680b793":"one_hot_cols = dfh.columns.tolist()\ndfh_enc = pd.get_dummies(dfh, columns=one_hot_cols)\n\ndfh_enc.head()","10de9bae":"train = dfh_enc[:ntrain]\ntest = dfh_enc[ntrain:]","695f9dfd":"X_test = test\nX_train = train","ff889df3":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","c26f7004":"ran = RandomForestClassifier(random_state=1)\nknn = KNeighborsClassifier()\nlog = LogisticRegression()\nxgb = XGBClassifier()\ngbc = GradientBoostingClassifier()\nsvc = SVC(probability=True)\next = ExtraTreesClassifier()\nada = AdaBoostClassifier()\ngnb = GaussianNB()\ngpc = GaussianProcessClassifier()\nbag = BaggingClassifier()\n\n# Prepare lists\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nmodel_names = ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier']\nscores = {}\n\n# Sequentially fit and cross validate all models\nfor ind, mod in enumerate(models):\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores[model_names[ind]] = acc","b5cdf760":"# \uacb0\uacfc \ud14c\uc774\ube14\uc744 \ub9cc\ub4ed\ub2c8\ub2e4.\nresults = pd.DataFrame(scores).T\nresults['mean'] = results.mean(1)\n\nresult_df = results.sort_values(by='mean', ascending=False)#.reset_index()\nresult_df.head(11)","57c139c4":"\nresult_df = result_df.drop(['mean'], axis=1)\nsns.boxplot(data=result_df.T, orient='h')\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)');","14c24337":"# \uc911\uc694\ub3c4\ub97c \ubcf4\ub294 \ud568\uc218\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.\ndef importance_plotting(data, xlabel, ylabel, title, n=20):\n    sns.set(style=\"whitegrid\")\n    ax = data.tail(n).plot(kind='barh')\n    \n    ax.set(title=title, xlabel=xlabel, ylabel=ylabel)\n    ax.xaxis.grid(False)\n    ax.yaxis.grid(True)\n    plt.show()","d6fdd904":"# \ub370\uc774\ud130 \ud504\ub808\uc784\uc5d0 \ud56d\ubaa9 \uc911\uc694\ub3c4\ub97c \ub123\uc2b5\ub2c8\ub2e4.\nfi = {'Features':train.columns.tolist(), 'Importance':xgb.feature_importances_}\nimportance = pd.DataFrame(fi, index=fi['Features']).sort_values('Importance', ascending=True)","c54f1e11":"# \uadf8\ub798\ud504 \uc81c\ubaa9\ntitle = 'Top 20 most important features in predicting survival on the Titanic: XGB'\n\n# \uadf8\ub798\ud504 \uadf8\ub9ac\uae30\nimportance_plotting(importance, 'Importance', 'Features', title, 20)","b14df8c3":"# \uc911\uc694\ub3c4\ub97c \ub370\uc774\ud130\ud504\ub808\uc784\uc5d0 \ub123\uc2b5\ub2c8\ub2e4. Logistic regression\uc5d0\uc11c\ub294 \uc911\uc694\ub3c4\ubcf4\ub2e4 coefficients\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \n# \uc544\ub798\ub294 Features\ub77c\ub294 \uc5f4\uc5d0 \ud2b8\ub808\uc778\uc758 \uc5f4\ub4e4\uc758 \uc774\ub984\uc744 \ub9ac\uc2a4\ud2b8\ub85c \ub9cc\ub4e4\uc5b4\uc11c \ub123\uace0 Importance\uc5d0\ub294 Logistic regression\uc5d0\ub294 coefficient\ub97c \ubc14\uafb8\uc5b4 \ub123\uc5b4\ub77c\ub294 \ub118\ud30c\uc774 \uba85\ub839\uc785\ub2c8\ub2e4.(\uc989 \uac00\ub85c\ub97c \uc138\ub85c\ub85c)\nfi = {'Features':train.columns.tolist(), 'Importance':np.transpose(log.coef_[0])}\nimportance = pd.DataFrame(fi, index=fi['Features']).sort_values('Importance', ascending=True)\n# \uadf8\ub798\ud504 \ud0c0\uc774\ud2c0\ntitle = 'Top 20 important features in predicting survival on the Titanic: Logistic Regression'\n\n# \uadf8\ub798\ud504 \uadf8\ub9ac\uae30\nimportance_plotting(importance, 'Importance', 'Features', title, 20)","f5e5cb94":"# 5\uac00\uc9c0 \ubaa8\ub378\uc5d0 \ub300\ud55c \ud56d\ubaa9 \uc911\uc694\ub3c4 \uc5bb\uae30\ngbc_imp = pd.DataFrame({'Feature':train.columns, 'gbc importance':gbc.feature_importances_})\nxgb_imp = pd.DataFrame({'Feature':train.columns, 'xgb importance':xgb.feature_importances_})\nran_imp = pd.DataFrame({'Feature':train.columns, 'ran importance':ran.feature_importances_})\next_imp = pd.DataFrame({'Feature':train.columns, 'ext importance':ext.feature_importances_})\nada_imp = pd.DataFrame({'Feature':train.columns, 'ada importance':ada.feature_importances_})\n\n# \uc774\ub97c \ud558\ub098\uc758 \ub370\uc774\ud130\ud504\ub808\uc784\uc73c\ub85c\nimportances = gbc_imp.merge(xgb_imp, on='Feature').merge(ran_imp, on='Feature').merge(ext_imp, on='Feature').merge(ada_imp, on='Feature')\n\n# \ud56d\ubaa9\ub2f9 \ud3c9\uade0 \uc911\uc694\ub3c4\nimportances['Average'] = importances.mean(axis=1)\n\n# \ub7ad\ud0b9 \uc815\ud558\uae30\nimportances = importances.sort_values(by='Average', ascending=False).reset_index(drop=True)\n","758842cd":"# \uc911\uc694\ub3c4\ub97c \ub2e4\uc2dc \ub370\uc774\ud130 \ud504\ub808\uc784\uc5d0 \ub123\uae30\nfi = {'Features':importances['Feature'], 'Importance':importances['Average']}\nimportance = pd.DataFrame(fi).set_index('Features').sort_values('Importance', ascending=True)\n\n# \uadf8\ub798\ud504 \ud0c0\uc774\ud2c0\ntitle = 'Top 20 important features in predicting survival on the Titanic: 5 model average'\n\n# \uadf8\ub798\ud504 \ubcf4\uae30\nimportance_plotting(importance, 'Importance', 'Features', title, 20)","80728354":"importance1 = importance[-120:]\n\nimportance1[111:120]","51fd21ad":"# \uc601\uc591\uac00 \uc788\ub294 120\uac1c\ub9cc \ub123\uae30\nmylist = list(importance1.index)","a6550b1c":"train1 = pd.DataFrame()\ntest1 = pd.DataFrame()\n\nfor i in mylist:\n    train1[i] = train[i]\n    test1[i]= test[i]\n    \ntrain1.head()\n","d35d10bc":"train = train1\ntest = test1\n\n# \ubaa8\ub378\uc758 \ubcc0\uc218\ub97c \ub2e4\uc2dc \uc815\uc758\ud558\uace0\nX_train = train\nX_test = test\n\n# \ubc14\uafc9\ub2c8\ub2e4.\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","ceade4a4":"ran = RandomForestClassifier(random_state=1)\nknn = KNeighborsClassifier()\nlog = LogisticRegression()\nxgb = XGBClassifier(random_state=1)\ngbc = GradientBoostingClassifier(random_state=1)\nsvc = SVC(probability=True)\next = ExtraTreesClassifier(random_state=1)\nada = AdaBoostClassifier(random_state=1)\ngnb = GaussianNB()\ngpc = GaussianProcessClassifier()\nbag = BaggingClassifier(random_state=1)\n\n# Prepare lists\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nmodel_names = ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier']\nscores2 = {}\n\n# Sequentially fit and cross validate all models\nfor ind, mod in enumerate(models):\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores2[model_names[ind]] = acc","1941d772":"# \uacb0\uacfc \ud14c\uc774\ube14\uc744 \ub9cc\ub4ed\ub2c8\ub2e4.\nresults = pd.DataFrame(scores2).T\nresults['mean'] = results.mean(1)\n\nresult_df = results.sort_values(by='mean', ascending=False)#.reset_index()\nresult_df.head(11)\nresult_df = result_df.drop(['mean'], axis=1)\nsns.boxplot(data=result_df.T, orient='h')\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)');","57949535":"# \ud30c\ub77c\ubbf8\ud130 \uc11c\uce58\nCs = [0.01, 0.1, 1, 5, 10, 15, 20, 50]\ngammas = [0.001, 0.01, 0.1]\n\n# \ud30c\ub77c\ubbf8\ud130 \uadf8\ub9ac\ub4dc \uc14b\ud305\nhyperparams = {'C': Cs, 'gamma' : gammas}\n\n# \uad50\ucc28\uac80\uc99d\ngd=GridSearchCV(estimator = SVC(probability=True), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\n# \ubaa8\ub378 fiting \ubc0f \uacb0\uacfc\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","6c660317":"learning_rate = [0.01, 0.05, 0.1, 0.2, 0.5]\nn_estimators = [100, 1000, 2000]\nmax_depth = [3, 5, 10, 15]\n\nhyperparams = {'learning_rate': learning_rate, 'n_estimators': n_estimators}\n\ngd=GridSearchCV(estimator = GradientBoostingClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","b4ec2a57":"penalty = ['l1', 'l2']\nC = np.logspace(0, 4, 10)\n\nhyperparams = {'penalty': penalty, 'C': C}\n\ngd=GridSearchCV(estimator = LogisticRegression(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","d9d7a67b":"learning_rate = [0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\nn_estimators = [10, 50, 100, 250, 500, 1000]\n\nhyperparams = {'learning_rate': learning_rate, 'n_estimators': n_estimators}\n\ngd=GridSearchCV(estimator = XGBClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","4854313a":"max_depth = [3, 4, 5, 6, 7, 8, 9, 10]\nmin_child_weight = [1, 2, 3, 4, 5, 6]\n\nhyperparams = {'max_depth': max_depth, 'min_child_weight': min_child_weight}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.2, n_estimators=10), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","9dd35144":"gamma = [i*0.1 for i in range(0,5)]\n\nhyperparams = {'gamma': gamma}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.2, n_estimators=10, max_depth=6, \n                                          min_child_weight=1), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","ac840ace":"subsample = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\ncolsample_bytree = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n    \nhyperparams = {'subsample': subsample, 'colsample_bytree': colsample_bytree}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.2, n_estimators=10, max_depth=6, \n                                          min_child_weight=1, gamma=0), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","3ab96e85":"\nreg_alpha = [1e-5, 1e-2, 0.1, 1, 100]\n    \nhyperparams = {'reg_alpha': reg_alpha}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.2, n_estimators=10, max_depth=6, \n                                          min_child_weight=1, gamma=0, subsample=1, colsample_bytree=1),\n                                         param_grid = hyperparams, verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","2dc42326":"n_restarts_optimizer = [0, 1, 2, 3]\nmax_iter_predict = [1, 2, 5, 10, 20, 35, 50, 100]\nwarm_start = [True, False]\n\nhyperparams = {'n_restarts_optimizer': n_restarts_optimizer, 'max_iter_predict': max_iter_predict, 'warm_start': warm_start}\n\ngd=GridSearchCV(estimator = GaussianProcessClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","6e44211a":"n_estimators = [10, 100, 200, 500]\nlearning_rate = [0.001, 0.01, 0.1, 0.5, 1, 1.5, 2]\n\nhyperparams = {'n_estimators': n_estimators, 'learning_rate': learning_rate}\n\ngd=GridSearchCV(estimator = AdaBoostClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","42b4c98f":"n_neighbors = [1, 2, 3, 4, 5]\nalgorithm = ['auto']\nweights = ['uniform', 'distance']\nleaf_size = [1, 2, 3, 4, 5, 10]\n\nhyperparams = {'algorithm': algorithm, 'weights': weights, 'leaf_size': leaf_size, \n               'n_neighbors': n_neighbors}\n\ngd=GridSearchCV(estimator = KNeighborsClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","23f19a0e":"n_estimators = [10, 50, 100, 200]\nmax_depth = [3, None]\nmax_features = [0.1, 0.2, 0.5, 0.8]\nmin_samples_split = [2, 6]\nmin_samples_leaf = [2, 6]\n\nhyperparams = {'n_estimators': n_estimators, 'max_depth': max_depth, 'max_features': max_features,\n               'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n\ngd=GridSearchCV(estimator = RandomForestClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","a4b1a646":"n_estimators = [10, 25, 50, 75, 100]\nmax_depth = [3, None]\nmax_features = [0.1, 0.2, 0.5, 0.8]\nmin_samples_split = [2, 10]\nmin_samples_leaf = [2, 10]\n\nhyperparams = {'n_estimators': n_estimators, 'max_depth': max_depth, 'max_features': max_features,\n               'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n\ngd=GridSearchCV(estimator = ExtraTreesClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","09917e8c":"n_estimators = [10, 50, 75, 100, 200]\nmax_samples = [0.1, 0.2, 0.5, 0.8, 1.0]\nmax_features = [0.1, 0.2, 0.5, 0.8, 1.0]\n\nhyperparams = {'n_estimators': n_estimators, 'max_samples': max_samples, 'max_features': max_features}\n\ngd=GridSearchCV(estimator = BaggingClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","cbbd30c3":"\n# \ud29c\ub2dd \ubaa8\ub378 \uc2dc\uc791\n# sample\uc744 split\ud558\ub294 \uac83\uc740 \uc804\uccb4\ub370\uc774\ud130 80%\ub97c \ud2b8\ub808\uc778\uc14b\uc5d0 20%\ub294 \ud14c\uc2a4\ud2b8\uc14b\uc5d0 \uc90c  \nran = RandomForestClassifier(max_depth=3, max_features=0.8, min_samples_leaf=6, min_samples_split=2, n_estimators=200, random_state=1)\n\nknn = KNeighborsClassifier(leaf_size=1, n_neighbors=2, weights='uniform')\n\nlog = LogisticRegression(C=1.0, penalty='l2')\n\nxgb = XGBClassifier(learning_rate=0.05, n_estimators=250, max_depth=3, \n                                          min_child_weight=6, gamma=0.4, subsample=1, colsample_bytree=0.65, reg_alpha=1e-05)\n\ngbc = GradientBoostingClassifier(learning_rate=0.05, max_depth=3, n_estimators=100)\n\nsvc = SVC(probability=True, gamma=0.001, C=10)\n\next = ExtraTreesClassifier(max_depth=3, max_features=0.08, min_samples_leaf=6, min_samples_split=2, n_estimators=200, random_state=1)\n\nada = AdaBoostClassifier(learning_rate=1, n_estimators=200, random_state=1)\n\ngpc = GaussianProcessClassifier(max_iter_predict=1, n_restarts_optimizer=0, warm_start=True)\n\nbag = BaggingClassifier(max_features=0.2, max_samples=0.2, n_estimators=100, random_state=1)\n\n# \ub9ac\uc2a4\ud2b8\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nmodel_names = ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier']\nscores3 = {}\n\n# Sequentially fit and cross validate all models\nfor ind, mod in enumerate(models):\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores3[model_names[ind]] = acc","3a35e24c":"results = pd.DataFrame(scores).T\nresults['mean'] = results.mean(1)\nresult_df = results.sort_values(by='mean', ascending=False)\nresult_df.head(11)\n\n\nresult_df = result_df.drop(['mean'], axis=1)\nsns.boxplot(data=result_df.T, orient='h')\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)');","40495e59":"#\ud29c\ub2dd\ud55c \ud30c\ub77c\ubbf8\ud130\ub85c \ud558\ub4dc\ubcf4\ud305\ngrid_hard = VotingClassifier(estimators = [('Random Forest', ran), \n                                           ('Logistic Regression', log),\n                                           ('XGBoost', xgb),\n                                           ('Gradient Boosting', gbc),\n                                           ('Extra Trees', ext),\n                                           ('AdaBoost', ada),\n                                           ('Gaussian Process', gpc),\n                                           ('SVC', svc),\n                                           ('K Nearest Neighbour', knn),\n                                           ('Bagging Classifier', bag)], voting = 'hard')\n\ngrid_hard_cv = model_selection.cross_validate(grid_hard, X_train, y_train, cv=10)\ngrid_hard.fit(X_train, y_train)\n\nprint(\"Hard voting on test set score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean() * 100))","e97f752e":"grid_soft = VotingClassifier(estimators = [('Random Forest', ran), \n                                           ('Logistic Regression', log),\n                                           ('XGBoost', xgb),\n                                           ('Gradient Boosting', gbc),\n                                           ('Extra Trees', ext),\n                                           ('AdaBoost', ada),\n                                           ('Gaussian Process', gpc),\n                                           ('SVC', svc),\n                                           ('K Nearest Neighbour', knn),\n                                           ('Bagging Classifier', bag)], voting = 'soft')\n\ngrid_soft_cv = model_selection.cross_validate(grid_soft, X_train, y_train, cv=10)\ngrid_soft.fit(X_train, y_train)\n\nprint(\"Soft voting on test set score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean() * 100))","c278374f":"# Final predictions\n\npredictions = grid_hard.predict(X_test)\n\nsubmission = pd.concat([pd.DataFrame(passId), pd.DataFrame(predictions)], axis = 'columns')\n\nsubmission.columns = [\"PassengerId\", \"Survived\"]\nsubmission.to_csv('titanic_submission305.csv', header = True, index = False)","7a544ce0":"# Final predictions2\n\npredictions = grid_soft.predict(X_test)\n\nsubmission = pd.concat([pd.DataFrame(passId), pd.DataFrame(predictions)], axis = 'columns')\n\nsubmission.columns = [\"PassengerId\", \"Survived\"] \nsubmission.to_csv('titanic_submission306.csv', header = True, index = False)","8f5aacd8":"# And we finally make a submission \uadf8\ub9ac\uace0 \uc81c\ucd9c \ud569\ub2c8\ub2e4.\n# Please make sure you \"commit\" (It take a few minutes) \/ commit\ubc84\ud134\uc744 \ub204\ub974\uc2dc\ub294 \uac83\uc744 \uc78a\uc9c0 \ub9c8\uc138\uc694 (\uba87 \ubd84 \uac78\ub9bd\ub2c8\ub2e4)\n# And then you will see the submission file on the top right hand side at Data>Output>Kaggle\/working \/ \uadf8\ub7fc \uc6b0\uce21 \uc0c1\ub2e8 \ub370\uc774\ud130 \uc544\uc6c3\ud48b\uc5d0\uc11c \uc81c\ucd9c\uc6a9 \uacb0\uacfc\ubb3c\uc774 \ub098\uc62c \uac83\uc785\ub2c8\ub2e4.","c2f0dcba":"* test \uc5d0 \uc788\ub294 Dona\uc758 \ub098\uc774\ub97c \ubcf4\uace0 \uc5b4\ub514\uc5d0 \ub123\uc744\uc9c0 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n* Ms. \ub294 \ud604\ub300\ucc98\ub7fc Miss + Mrs\ub97c \ud569\uce5c \ub9d0\uc774 \uc544\ub2c8\ub77c \ub2f9\uc2dc\uc5d0\ub294 \uadc0\uc871\ubbf8\ub9dd\uc778\uc744 \uc758\ubbf8\ud558\ub294 \uac83\uc774 \uc5c8\uc2b5\ub2c8\ub2e4. Mlle\ub098 Mme\ub4f1\ub3c4 \ub9c8\ub4dc\ubaa8\uc544\uc824\uacfc \ub9c8\ub2f4\uc758 \uc904\uc778\ub9d0\uc77c \uacbd\uc6b0\uc77c \uac83\uc785\ub2c8\ub2e4. \uadc0\uc871 \uc5ec\uc131\ub4e4\ub85c \ubcf4\uc544\uc57c\uaca0\uc8e0.\n \n* Let's look at Dona's age and decide to where to classify her.\n  \n* Ms. did not mean Miss + Mrs combined like these days, at the time it meant widow. Also Mlle, & Mme may be short for Mademoiselle and Madame. I should see them as women in a higher class.","75e96e1e":"# Making ML models \/ \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378 \ub9cc\ub4e4\uae30","025341c3":"* \uc704\uc5d0 \ub9cc\ub4e0 \ud568\uc218\ub97c \ud55c \ubc88 \uc368 \uba39\uc5b4 \ubcfc\uae4c\uc694?\n* Let's use a function we made above.","75ad0052":"* Now we are ready to move on.\n* \uc774\uc81c \ub2e4\uc74c \ub2e8\uacc4\ub85c \uac11\ub2c8\ub2e4.","bf7ed2f0":"#### \"Ticket\"\ubd84\uc11d\n* Ticket\uc758 \ud615\ud0dc\ub97c \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n* Let's look at the numbers of the ticket.","8dca5e5f":"We can see that 342 people survived and 549 didn't survive","9eee29ce":"* \"Pclass\" \ubd84\uc11d\n\n* Pclass\ub294 \uac12\uc774 \uc22b\uc790\uc774\ub098 \uc11c\uc5f4\uc774 \uc815\ud574\uc9c4 Ordinal Feature\uc774\ub2e4.\n* Pclass is an \"Ordinal Feature\" whose values are numerical but sequenced.\n* Key:1 = 1st, 2= 2nd, 3 = 3rd\n* \n* \uac01 \ud074\ub798\uc2a4 \ub2f9 \uc0dd\uc874\uc790\ub97c \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n* * Let's look at the survivors for each class.","bf8637d7":"### \"Fare\" \ubd84\uc11d","e3f9e9e2":"#### \uad00\ucc30 :\n\n1) Pclass\uc5d0 \ub530\ub77c \uc5b4\ub9b0\uc774 \uc218\uac00 \uc99d\uac00\ud558\uace0 10 \uc138 \ubbf8\ub9cc\uc758 \uc5b4\ub9b0\uc774 (\uc989, \uc5b4\ub9b0\uc774)\uc758 \uc0dd\uc874\uc728\uc740 Pclass\uc5d0 \uc0c1\uad00\uc5c6\uc774 \uc591\ud638\ud574 \ubcf4\uc785\ub2c8\ub2e4.\n\n2) Pclass1\uc5d0\uc11c 20-50\uc138\uc758 Passeneger\uc758 \uc0dd\uc874 \uac00\ub2a5\uc131\uc740 \ub192\uace0 \uc5ec\uc131\uc5d0\uac8c\ub294 \ub354 \uc88b\uc2b5\ub2c8\ub2e4.\n\n3) \ub0a8\uc131\uc758 \uacbd\uc6b0 \uc0dd\uc874 \ud655\ub960\uc740 \ub098\uc774\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uac10\uc18c\ud569\ub2c8\ub2e4.\n\n#### observation :\n\n1) The number of children increases with Pclass, and the survival rate for children under 10 years old (i.e. children) looks good regardless of Pclass.\n\n2) Passeneger, 20-50 years old, in Pclass1 has a higher chance of survival and is better for women.\n\n3) In men, the probability of survival decreases with age.\n\n\uc6b0\uc120 age\uc758 \ube48\uce78 \ubd80\ud130 \ud574\uacb0 \ud569\ub2c8\ub2e4.\n\nFirst, resolve the issue of null values of age.\n\n\uc55e\uc5d0\uc11c \uc0b4\ud3b4\ubcf8 \uac83\ucc98\ub7fc Age \ud56d\ubaa9\uc5d0\ub294 177 null \uac12\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c NaN \uac12\uc744 \ub300\uccb4\ud558\uae30 \uc704\ud574 \ub370\uc774\ud130 \uc9d1\ud569\uc758 \ud3c9\uade0 \uc218\uba85\uc744 \uc9c0\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uadf8\ub7ec\ub098 \ubb38\uc81c\ub294 \ud3c9\uade0 \uc5f0\ub839\uc774 29 \uc138\ub97c 4\uc138 \uc544\uc774\uc5d0\uac8c \ud560\ub2f9 \ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4. \uc2b9\uac1d\uc774 \uc5b4\ub5a4 \uc5f0\ub839\ub300\uc5d0 \uc788\ub294\uc9c0 \uc54c \uc218\uc788\ub294 \ubc29\ubc95\uc774 \uc788\uc744\uae4c\uc694? \uc774\ub984\uc5d0\uc11c \ud78c\ud2b8\ub97c \ucc3e\uc544 \ubd05\ub2c8\ub2e4.\n\nAs we saw earlier, the Age item has a 177 null values. To replace these NaN values, you can specify the average age of the dataset.\n\nBut the problem is that the average age is 29 years old can not be assigned this to a kid. Is there any other way ? Look for some hints in their names.","63acc351":"### XGB Step 3.\n","af24e8c7":"* Fare\ub97c \uadf8\ub8f9\ud654 \uc2dc\ud0b5\ub2c8\ub2e4. Fgroup\uc774\ub77c\uace0 \uc774\ub984 \uc9d3\uaca0\uc2b5\ub2c8\ub2e4.\n* Group Fare with the name of Fgroup","97b893bd":"#### \"Embarked\" \ubd84\uc11d","3c52c5a0":"### Basic operations with dataframes","dcb71658":"This function works for features with any number of categories","a0424a4f":"* Names of columns \/ \uc5f4 \uc774\ub984\uc744 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","1e5f9620":"This plot shows missing values in each column and in each row. Note that in the lower half of `Survived` column all the values are missing. This is because these rows are from test data.","eba2b411":"* Last name \uc740 \uc804\ubd80 \uc22b\uc790\ub85c \ubc14\uafc9\ub2c8\ub2e4.\n* Let's change the Last Names to numbers.","0792cb4d":"### Adaboost.","6aa13351":"* \ub3c4\ubb34\uc9c0 \uac10\uc774 \uc548 \uc7a1\ud788\ub294 \ubc30\uc5f4\uc785\ub2c8\ub2e4.\n* \ube48\uce78\uc774 \uc5c6\ub294\uc9c0 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n \n* It is an array that may not make any sense.\n* Let's see if there are any blanks.","77262774":"* 1\ub4f1\uae09 \uac1d\uc2e4\uc758 \uc0ac\ub78c\ub4e4\uc740 \uc0dd\uc874\uc790\uac00 \ub354 \ub9ce\uace0, 2\ub4f1\uae09\uc740 \uc0dd\uc874\uc790\uc5d0 \ube44\ud574 \uc0ac\ub9dd\uc790\uac00 \uc870\uae08 \ub354 \ub9ce\uc73c\ub098, 3\ub4f1\uae09\uc740 \uc0ac\ub9dd\uc790\uac00 3\ubc30 \uc774\uc0c1 \ub9ce\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n* The 1st class cabin has more survivors, the 2nd class has a few more deaths than the survivors, but the 3rd class has three times more deaths.","21d6d8cb":"* Age\ub294 \uadf8\ub8f9\ud654 \uc2dc\ud0a4\uba74 \uc88b\uc73c\ub098 \ud559\uc2b5\uc744 \uc704\ud574\uc11c \uadf8\ub0e5 \ub193\uc544\ub450\uace0, \uadf8\ub8f9\ud654 \uc5f0\uc2b5\uc740 Fare\ub85c \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n* Age should be grouped, but we are focusing on practices not on competition itself, we will just let it be as is and group \"fare\" later on. ","113f37c8":"#### Family or Alone?\n","efe332a7":"* \ubcf4\uc2dc\ub2e4\uc2dc\ud53c \uac19\uc740 Last name\uc5d0 \uac19\uc740 \ubc88\ud638\uac00 \uc4f0\uc5ec\uc84c\ub2e4.\n* As you see NumnName is the same as their Last Names are the same.\n* The part at the end: [0] means you are only taking the labels, throwing away the uniques that map back to your input.\n* \ub05d\uc5d0 [0]\uc740 \ub77c\ubca8\ub9cc \ubcf4\uace0 \ubc88\ud638\ub97c \ubd99\uc774\ub294 \uac83\uc73c\ub85c \uc815\ub9d0 unique\ud55c \uac83\uc774\ub780 \uac83\uc740 \uc548 \ubcf8\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n\n* We have turned 'Initial' & 'NumName' representing \"Name\" and \"Last Name\" into numbers. Let's move on.\n* \uc790 \uc774\uc81c \uc774\ub984\uc744 \uc758\ubbf8\ud558\ub294 \uc911\uc694\ud55c \uc694\uc18c \ub450 \uac1c\ub97c \uc22b\uc790\ub85c \ubc14\uafb8\uc5c8\uc73c\ub2c8 \ub2e4\uc74c\uc73c\ub85c \uac11\ub2c8\ub2e4.","e52d7b61":"* %\ub294 3\ub4f1\uce78\uc774 \ubc18\uc774 \ub118\uc73c\ub098 \uc704\uc758 \uadf8\ub798\ud504\uc5d0\uc11c \uc0dd\uc874\uc790\ub294 1\ub4f1\uc11d\uc774 \uac00\uc7a5 \ub9ce\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n* \uac01 \ud074\ub798\uc2a4 \ub2f9 \uc0dd\uc874\ub960\uc744 \ubcfc\uae4c\uc694?","c83f32ea":"* \ub098\uc774\ub85c \ucd94\uce21\ud574\uc11c Mrs.\ub85c \ub123\uc2b5\ub2c8\ub2e4.\n* Let me guess she is Mrs. based on her age.","8ed5be8b":"* \uc548\uc804\uc744 \uc704\ud574 \uce74\ud53c\ub97c \ud558\ub098 \ub9cc\ub4e4\uc5b4\uc11c \uc0c8\ub85c\uc6b4 \ud56d\ubaa9\uc744 \ub9cc\ub4e4\uc5b4 \ubd05\ub2c8\ub2e4.\n* Just in case, let's make a copy of the df and make new features on it.","768af090":"## Voting (Hard\/Soft)","6b841e17":"### \ubaa8\ub378 \uc7ac \ud2b8\ub808\uc774\ub2dd","91a9b4b8":"`info()` is an advanced version of `dtypes` which shows not only types, but also the number of non-null values.","cd2ba607":"* \uc774\uc81c \uc6b0\ub9ac\ub294 Initial\uc5d0\uc11c Mr.\ub4f1\uc758 \ud638\uce6d\uc744 \ubf51\uc544\ub0b4\uc5c8\uace0, \uc131\uc744 \ubf51\uc544\ub0b4\uc5c8\uc2b5\ub2c8\ub2e4.\n* Now we have extracted salutations in 'Initial' and last names in 'LastName'\n\n* \uba38\uc2e0\uc774 \uc54c\ud30c\ubcb3\ubcf4\ub2e4\ub294 \uc22b\uc790\ub97c \uc88b\uc544 \ud558\ubbc0\ub85c \uc22b\uc790\ub85c \ubc14\uafc9\ub2c8\ub2e4.\n* Since our machine prefers numbers over alphabets, we change them to numbers.\n\n* \uc544, \uadf8\ub7ec\uae30 \uc804\uc5d0 Dona\ub97c \ucc98\ub9ac\ud574\uc57c\uc9c0\uc694.\n* But we have to take care of Dona first.","9398cc19":"* \ud2f0\ucf13\uc5d0\uc11c \uc601\ubb38\uc788\ub294 \uac83\uacfc \uc22b\uc790\ub9cc \uc788\ub294 \uac83\uc744 \ub530\ubd05\ub2c8\ub2e4.\n* See the tickets with letters or numbers only ","4b9167ab":"* \"Surived\" \ubd84\uc11d\n\n\ud55c \uc5f4\uc529 \uac80\ud1a0\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\nWe will look at the columns one by one\n\nSurvived - Key: (0 - Not Survived, 1- Survived)\n\nSurvived\ub294 \uc218\uc790\ub85c \uac12\uc744 \uc8fc\uc9c0\ub9cc Categorical Variable\uc778 \uc148\uc785\ub2c8\ub2e4.\n\n\uc8fd\ub358\uc9c0 \uc0b4\ub358\uc9c0 \ub458 \uc911 \ud558\ub098\uc758 \uac12\uc744 \uc90d\ub2c8\ub2e4.\n\ncountplot\uc744 \uadf8\ub824 \ubd05\ub2c8\ub2e4.\n\n\uc0ac\uc774\uc988\ub294 \uac00\ub85c 10\uc778\uce58 \uc138\ub85c 2\uc778\uce58\n\n\uc0dd\uc874 \uc5ec\ubd80 0\uacfc 1\uc758 \uc22b\uc790\ub97c \uc138\uc5b4 \ubcf8 \ud6c4 \uadf8\ub9bc\uc744 \uadf8\ub9ac\ub3c4\ub85d \uba85\ub839\uc744 \ud558\ub294 \uac83\uc785\ub2c8\ub2e4.\n\npyplot(plt)\uc758 figure\ub77c\ub294 \uba54\uc18c\ub4dc\ub97c \uc368\uc11c \uadf8\ub9bc\ud310\uc758 \ud06c\uae30\ub97c \uc815\ud558\uace0, seaborn\uc758 \uce74\uc6b4\ud2b8\ud50c\ub86f\uc744 \uadf8\ub9ac\ub77c\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n\"Survived\" gives a number, but it's a categorical variable.\n\nEither Survived or Not Survived\n\nDraw a countplot.\n\nSize is 10 inches wide by 2 inches long\n\nSurvival counts\n\nUse pyplot (plt) figure method to size the paint and draw a seaborn count plot.","37fb7297":"* \ud30c\uc77c \uac01 \uc5f4\uc758 \uc0c1\uad00 \uad00\uacc4\ub97c \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\nLet's take a look at the correlation of each column in the file.\n\nCo-relation \ub9e4\ud2b8\ub9ad\uc2a4\ub294 seaborn\uc5d0\uc11c \ubcc0\uc218 \uac04 \uc0c1\uad00 \uacc4\uc218\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4. \ud45c\uc758 \uac01 \uc140\uc740 \ub450 \ubcc0\uc218 \uac04\uc758 \uc0c1\uad00 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc0c1\uad00 \ub9e4\ud2b8\ub9ad\uc2a4\ub294 \uace0\uae09 \ubd84\uc11d\uc5d0 \ub300\ud55c \uc785\ub825 \ubc0f \uace0\uae09 \ubd84\uc11d\uc5d0 \ub300\ud55c \uc9c4\ub2e8\uc73c\ub85c \ub370\uc774\ud130\ub97c \uc694\uc57d\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \ucc38\uace0: https:\/\/seaborn.pydata.org\/examples\/many_pairwise_correlations.html\n\nCo-relation matrix is a table showing the correlation coefficient between variables in seaborn. Each cell in the table shows a correlation between two variables. Correlation matrices are used to summarize data as input to advanced analysis and as diagnostics for advanced analysis. Note: https:\/\/seaborn.pydata.org\/examples\/many_pairwise_correlations.html\n\n\uc544\ub798 \ub9c8\uc2a4\ud06c \uc14b\uc5c5\uc740 0\ub85c \ud589\ub82c\uc744 \uc0c1\uad00 \ud589\ub82c\uacfc \uac19\uc740 \ubaa8\uc591\uc73c\ub85c \ub9cc\ub4e0 \ud6c4 \uc5ec\uae30\uc5d0 \ubd88\ub9ac\uc548 \uac12\uc744 \ub123\uace0 \uc774\ub97c \ub2e4\uc2dc True\ub9cc \ub9cc\ub4ed\ub2c8\ub2e4.\n\nThe mask setup below makes a matrix that looks like a correlation matrix with zeros, then puts a Boolean value into it and makes it true.\n\ntriu \ub294 \uc6b0\uce21 \uc0c1\ub2e8 \uc0bc\uac01\ud589\ub82c\uc744 \uc758\ubbf8\n\ntriu means upper right triangle\n\nannot= True\ub294 \uac01 \uc140\uc5d0 \uc22b\uc790\ub97c \ud45c\uc2dc\ud558\ub77c\ub294 \uac83\uc774\uace0, False\ub294 \ud558\uc9c0 \ub9d0\ub77c\ub294 \uac83\uc774\uad6c\uc694\n\nannot = True means to display a number in each cell, False means oppposite. https:\/\/seaborn.pydata.org\/generated\/seaborn.heatmap.html\n\n\uc774\uc5b4\uc11c \uc774\ub97c heatmap\uc73c\ub85c \ub7f0\uce6d\ud569\ub2c8\ub2e4.\n\nThen launch it as a heatmap.","1299f99f":"### Random Forest.","5a0c1b1b":"`describe()` shows basic statistics for columns. By default it shows information only about continuous features. You can see information about all features by setting `include='all'`","40069cdf":"### Deciding final features","804907e2":"### Extra Trees","8cfa93f7":"### KNN","6c297fb8":"### XGBoost Step 1.","98a7f3a8":"### XGB Step 5","6bf837df":"* Survival rates per Embarked ports\n* \uc2b9\uc120 \uc7a5\uc18c \ubcc4\ub85c \uc0dd\uc874 \ud655\ub960","02ff3077":"### Bagging Classifier","b5636d44":"You can slice the list of columns like a usual python list object","46326f13":"### Age \ubd84\uc11d\n \n* Age\ub294 Continuous\ud55c \uac12\uc785\ub2c8\ub2e4.\n* \ube48\uce78\uc774 \ub9ce\uc544\uc11c \ube48\uce78\ucc98\ub9ac\uac00 \uacb0\uc815\uc801\uc778 \uc5ed\ud560\uc744 \ud560 \uac83 \uac19\uc2b5\ub2c8\ub2e4.\n \n* Age\uc758 \ucd5c\ub300, \ucd5c\uc18c, \uc911\uac04\uc744 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n \n* Age is a continuous value.\n \n* Because there are a lot of blanks, blank processing seems to play a decisive role.\n \n* Let's take a look at the maximum, minimum, and middle of Age.","20b1cca8":"### \uc81c\ucd9c","e7f93c64":"* \uc774\uc81c Initial\uc744 \uc880 \uc815\ub9ac\ud569\ub2c8\ub2e4.","cbd830d8":"0 and below -> 0\n\n7.125 and below-> 5.0\n\n7.9 and below-> 7.5\n\n8.03 or less-> 8.0\n\nLess than 10.5-> 9.5\n\nLess than 23-> 16.0\n\n27.8 and below-> 25.5\n\n51 and below-> 38\n\n73.5 and below-> 62\n\nOver 73.5-> 100","21c2db6a":"* \uc790, \uc774\uc81c \uba38\uc2e0 \ub7ec\ub2dd \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4 \ubcf4\uc9c0\uc694.\n* \uc6b0\uc120 \uc778\ucf54\ub529\ud55c \ud30c\uc77c\uc744 train\uacfc test\ub85c \uc544\uae4c \uad6c\ubd84\ud574 \ub193\uc740 \ud589\uc73c\ub85c \ucabc\uac2d\ub2c8\ub2e4\n\n* Now let's create a machine learning model.\n* First, split the encoded file into the lines that were previously separated by train and test.","657ad098":"`tail()` shows last five rows","175df14b":"* Let's make two new dataframes, one for label encoding, the other for one-hot-encoding\n* \ub450\uac1c\uc758 \uc0c8\ub85c\uc6b4 \ub370\uc774\ud130 \ud504\ub808\uc784\uc744 \ub9cc\ub4ed\ub2c8\ub2e4. \ud558\ub098\ub294 \ub808\uc774\ube14 \uc778\ucf54\ub529 \ub2e4\ub978 \ud558\ub098\ub294 \uc6d0\ud56b \uc778\ucf54\ub529 (\ub458\uc774 \uaf2d \ud544\uc694\ud55c \uac83\uc774 \uc544\ub2c8\ub77c \uc5f0\uc2b5\uc774\ub098 \ub450 \uac00\uc9c0 \ubc29 \ubc95 \ubaa8\ub450 \uc0ac\uc6a9\ud574\ubd04)","50e58e26":"### XGB Step 2.","1bdbc27e":"### Gaussian Process","8889a276":"# Titanic Challenge","a51486af":"* \ubc30\uc5d0 \uc788\ub358 \ub0a8\uc790\uc758 \uc218\ub294 \uc5ec\uc790\uc758 \uc218\ubcf4\ub2e4 \ud6e8\uc52c \ub9ce\uc2b5\ub2c8\ub2e4. \uc5ec\uc804\ud788 \uc0dd\uc874 \uc5ec\uc131 \uc218\ub294 \ub0a8\uc131 \uc218\uc758 \uac70\uc758 \ub450 \ubc30\uc785\ub2c8\ub2e4. \uc120\ubc15 \uc5ec\uc131\uc758 \uc0dd\uc874\uc728\uc740 \uc57d 75 % \uc778 \ubc18\uba74 \ub0a8\uc131\uc758 \uc0dd\uc874\uc728\uc740 \uc57d 18-19 %\uc785\ub2c8\ub2e4.\n* The number of men on the ship is much more than the number of women. Still, the number of surviving women is almost twice that of men. The survival rate of ship women is about 75%, while the survival rate of men is about 18-19%.\n* \uc774 \uac83\uc740 \ub0a8\uc131\/\uc5ec\uc131\uc744 1,2\ub85c \ub098\ub204\uba74 \ub420 \uac83 \uac19\uc740 \ubed4\ud574 \ubcf4\uc774\ub294 \uac83\uc774\uc9c0\ub9cc \uc880 \ub354 \uc0c8\ubd84\ud654\ud558\uba74 \uc88b\uc544 \ubcf4\uc785\ub2c8\ub2e4.\n* \uc608\ub97c \ub4e4\uc5b4 \uc544\uae30\ub4e4\uc740 \uc544\uae30\uc774\uc9c0, \ub0a8\uc790\uc778\uc9c0 \uc5ec\uc790\uc778\uc9c0 \uad6c\uba85\ubcf4\ud2b8 \ud0dc\uc6b8 \ub54c \uc548 \ubb3c\uc5b4 \ubcfc \uac83\uc774\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\n* \uc624\ud788\ub824 (\ubd88\ud589\ud558\uac8c\ub3c4) \uadc0\uc871 \uc544\uae30\uc778\uc9c0 \uc11c\ubbfc\uc758 \uc544\uae30\uc778\uc9c0\ub294 \ud589\uacfc\ubd88\ud589\uc744 \uac00\ub97c \uc218 \uc788\uc2b5\ub2c8\ub2e4 \u3160\u3160\n* \uc0dd\uc874 Pclass\ubcc4\ub85c \uc131\ubcc4\uc744 \ubd05\ub2c8\ub2e4.\n* This seems obvious to divide males \/ females by 1,2, but it looks good if you break it down a bit.\n* For example, babies are just babies and would not be matter whether it is boy or girl.\n* (Unfortunately) whether you are a baby in a higher social class or one from a humble family, it might have mattered.\n* View gender by survival Pclass.","d9d7469e":"* \uc0dd\uc874\ub960\ub85c \ubd05\ub2c8\ub2e4.\n* by survival percentage","85cc77f7":"### Gradient Boosting Classifier\n* learning_rate\ub294 \uac01 \ud2b8\ub9ac\uc758 \uae30\uc5ec\ub97c \uc904\uc774\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.\n* n_estimator\ub294 \uac01 \uacbd\uc6b0\uc758 \ud2b8\ub9ac \uc22b\uc790\uc785\ub2c8\ub2e4.","296e92b2":"* \ub300\ud45c\uc801\uc778 \uc778\ucf54\ub529\uc5d0 Label Encoding\uc774 \uc788\ub294\ub370 \uc774\ub294 \uac01 \ud56d\ubaa9\uc758 \uac12\uc744 \uc11c\uc5f4\ud654 \uc2dc\ucf1c \uc8fc~\uc6b1 \uc904\uc138\uc6b4 \uac83\uc774\ub77c \uc0dd\uac01\ud558\uc2dc\uba74 \ub429\ub2c8\ub2e4.\n* \uadf8 \uc678\uc5d0 \uc790\uc8fc\uc4f0\ub294 One Hot Encoding \uac19\uc740 \uacbd\uc6b0 \uc5f4 \ub0b4\uc5d0\uc11c\uc758 \ud56d\ubaa9\uc744 \ub098\ub204\uc5b4\uc11c (\uc5f4\uc774 \uc8fc~\uc6b1 \ub298\uc5b4\ub098\uba70) \uc774\ub97c 0\uc774\ub0d0 1\uc774\ub0d0\ub85c \uad6c\ubd84\ud574 \ub193\uc740 \uac83\uc785\ub2c8\ub2e4.\n* A typical encoding is Label Encoding, which can be considered to be a very good order of the values of each item.\n* In addition, the one-of-a-kind One Hot Encoding is divided into 0 or 1 by dividing the items in the column (they increase in number).","3eec8e88":"### 6.1 Turning string to numbers\n\n* Already done above","049a5b71":"* \uadf8\ub9ac\uace0 Initial \ubcc4 \ud3c9\uade0 \uc5f0\ub839\uc744 \ubcf4\uace0 Age\uc5d0 \uc801\uc6a9 \uc2dc\ud0a4\ub294 \uac83\uc774 \uc88b\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2e4.\n* And it is better to see the average age by Initial and apply it to Age.","0cfcd95c":"* \ub2e4\uc2dc \ub9d0\uc500 \ub4dc\ub824\uc11c \ub808\uc774\ube14 \uc778\ucf54\ub529\uc740 \uc904\uc744 \uc138\uc6cc\uc11c \ubc88\ud638\ub97c \ubd80\uc5ec\ud558\ub294 \uac83\uc774\uace0, \uc6d0\ud56b\uc778\ucf54\ub529\uc740 \uae34\uac00 \uc544\ub2cc\uac00 \ub450 \uac00\uc9c0\uc785\ub2c8\ub2e4.\n* label encoding assign ordinal numbers while one hot encoding gives 1 or 0\n\n* For example 16 year old is number 5 and 17 year old is number 6 ..that's the way it is for the label encoding\n* \uc608\ub97c\ub4e4\uc5b4 \ub098\uc774\ubcc4\ub85c \uc904\uc744 \uc138\uc6cc \ub108\ub294 5\ubc88, \ub108\ub294 6\ubc88\uc774\ub7f0\uc2dd\uc774 \ub808\uc774\ube14 \uc778\ucf54\ub529\uc774\uace0\n\n* If you are 16 yes or no , 17 yes or no...that's one hot encoding\n* 16\uc0b4\uc774\uc57c? 1, 16\uc0b4 \uc544\ub0d0 0 ..\uadf8 \ub2e4\uc74c 17\uc0b4\uc774\uc57c? 1 17\uc0b4 \uc544\ub0d0 0 ..\uc774\ub7f0 \uc2dd\uc73c\ub77c \uc5f4\uc758 \uc218\uac00 \ubb34\uc9c0\ud558\uac8c \ub298\uc5b4\ub0a9\ub2c8\ub2e4.","7cce8b44":"* This one can be in top 3% if you try a few times\n* After figuring out that I have classified many who did not have a good chance to survive but actully survived as \"Not Survived\", I have added a random feature to make some of them become 0 to 1, and then I could enter the range inside top 3% \n","b5dc3b7c":"* \uac00\uc7a5 \ube44\uc2b7\ud55c \ubc88\ud638\ub97c \ucc3e\uc544 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n* Let's find the similar numbers.","f0faca73":"* \uc704\uc758 \uac83\uc744 \uc544\ub798\uc640 \uac19\uc774 \ud568\uc218\ub85c \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4. (\ubb3c\ub860 \uc790\uc8fc \uc4f0\uc774\uc9c0\ub294 \uc54a\uaca0\uc9c0\ub9cc \uc5f0\uc2b5\uc774\ub2c8)\n* Let's make the above as a function. (it's not going to be used often, but it's a practice for making a function)","980294e6":"* \uc774\ub984\uc740 \uc5b8\ub73b \ubcf4\uc544\uc11c \uac10\uc774 \uc548 \uc635\ub2c8\ub2e4. \uc911\uac04\uc5d0 \uc788\ub294 Mr. \uac19\uc740 \ud638\uce6d\uc744 \ubcfc\uae4c\uc694.\n* ['Initial']\uc774\ub780 \uc5f4\uc744 \uc0c8\ub85c \ub9cc\ub4e4\uc5b4\uc11c \uc5ec\uae30\uc5d0 Name\uc5d0\uc11c \ucd94\ucd9c\ud55c Regular Expression\uc744 \ub123\uc2b5\ub2c8\ub2e4.\n* \uc544\ub798\uc5d0\uc11c str.extract('([A-Za-z]+).')\ubd80\ubd84\uc740 str\uc5d0\uc11c \ub300\ubb38\uc790 A~Z, \uc18c\ubb38\uc790 a~z \uc911\uc5d0 . \uba85\ub839\uc744 \ud1b5\ud574 .\uc73c\ub85c \ub05d\ub098\ub294 \ubd80\ubd84\uc744 \ucd94\ucd9c\ud574 \ub0b4\ub294 \uac83\uc785\ub2c8\ub2e4.\n* ('^([A-Za-z]+)')\uc73c\ub85c \ud558\uba74 \ucc98\uc74c\uc5d0 \ub098\uc624\ub294 \ubb38\uc790 \ub369\uc5b4\ub9ac\uac00 \ub420 \uac83\uc774\uace0 +\ub97c \ube7c\uba74 \uccab \uc2a4\ud3a0\ub9c1 \ud55c\uce90\ub9ad\ud130\ub9cc \ucd94\ucd9c\ud569\ub2c8\ub2e4.\n \n* The name doesn't seem to have any meaning at first glance.\n \n* Create a new column called ['Initial'] and put the regular expressions extracted from Name.\n* The str.extract ('([A-Za-z] +) .') Part of the lower part of the str is the . Is to extract the part that ends with.\n* ('^ ([A-Za-z] +)') will be the first chunk of characters, and minus + will extract only the first spelled character.","72c7c224":"1) \ud3ec\ud2b8 C\uc758 \uc0dd\uc874 \uac00\ub2a5\uc131\uc740 0.55 \uc815\ub3c4\uc774\uba70 S\ub294 \uac00\uc7a5 \ub0ae\uc2b5\ub2c8\ub2e4.S\uc5d0\uc11c \ud0d1\uc2b9 \ucd5c\ub300. \ub300\ub2e4\uc218\ub294 Pclass3\n\n2) C\uc758 \uc2b9\uac1d\ub4e4\uc740 \ub9ce\uc740 \ube44\uc728\uc774 \uc0b4\uc544\ub0a8\uc558\uc2b5\ub2c8\ub2e4. \uadf8 \uc774\uc720\ub294 Pclass1 \ubc0f Pclass2 \uc2b9\uac1d\uc774 \ub9ce\uc544\uc11c \uc77c \uac83\uc785\ub2c8\ub2e4\n\n3) Embark S\ub294 \ub300\ubd80\ubd84\uc758 \ubd80\uc790\ub4e4\uc774 \ud0d1\uc2b9\ud55c \ud56d\uad6c\uc9c0\ub9cc \uc0dd\uc874 \uac00\ub2a5\uc131\uc740 \ub0ae\uc2b5\ub2c8\ub2e4. Pclass3\uc758 \uc2b9\uac1d\ub3c4 \ub9ce\uc558\uc2b5\ub2c8\ub2e4.\n\n4) \ud3ec\ud2b8 Q\ub294 \uc2b9\uac1d\uc758 \uac70\uc758 95 %\uac00 Pclass3\n\nobservation :\n\n1) Maximum boarding in S. The majority is Pclass3\n\n2) Passengers of C survived a large proportion. The reason would be due to the large number of Pclass1 and Pclass2 passengers\n\n3) Embark S is the port where most rich people board, but it is unlikely to survive. Lots of passengers in Pclass3.\n\n4) Port Q has almost 95% of passengers Pclass3\n\n* \ube48\uce78\uc774 \ub450\uac1c \uc788\ub294\ub370 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n* Let us see 2 null values","1852bf2c":"### \"Cabin\" \ubd84\uc11d\n* cabin \uc758 \uc704\uce58\uc5d0 \ub530\ub77c \ub2ec\ub77c\uc9c0\ub294 \uac83\uc774 \uc788\ub294\uc9c0 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n* Let's see if there is anything that depends on the location of the cabin.","591421c8":"### Priority - (1) Nobles (2) Women in Pclass 1 (3) Babies under 1 (4) Kids under 17 in Pclass 1 & 2  (5) Women in Pclass 2 (6) Higher Fare","14e32ee8":"## 6. Feature Engineering","4c642827":"`dtypes` shows types of all columns","755273f7":"* \uc55e \ub4a4\ub85c \ubaa8\ub450 S\uc774\uace0 Pclass\ub3c4 \ubaa8\ub450 1\uc778 \uac83\uc73c\ub85c \ubd10\uc11c S\uc77c \uac00\ub2a5\uc131\uc774 \ud07d\ub2c8\ub2e4.\n* It is most likely that it is S because both front and back are S and Pclass is all 1.","fef433ba":"* Let's combine Pclass & Sex and make one coulmn called Gclass = Gender class","c4a17c38":"## \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\n\n### SVC\n* Scikit-Learn\uc5d0\uc11c\ub294 3\uac00\uc9c0 \ubaa8\ud615 \ucd5c\uc801\ud654 \ub3c4\uad6c\ub97c \uc9c0\uc6d0\ud558\ub294\ub370 validation_curve\/ GridSearchCV\/ ParameterGrid\uc774\ub2e4\n* fit \uba54\uc18c\ub4dc\ub97c \ud638\ucd9c\ud558\uba74 grid search\uac00 \uc790\ub3d9\uc73c\ub85c \uc5ec\ub7ec\uac1c\uc758 \ub0b4\ubd80 \ubaa8\ud615\uc744 \uc0dd\uc131\ud558\uace0 \uc774\ub97c \ubaa8\ub450 \uc2e4\ud589\uc2dc\ucf1c\uc11c \ucd5c\uc801 \ud30c\ub77c\ubbf8\ud130\ub97c \ucc3e\ub294\ub2e4.\n\n* bestscore\ub294 \ucd5c\uace0 \uc810\uc218\uc774\uace0 best estimator\ub294 \ucd5c\uace0 \uc810\uc218\ub97c \ub0b8 \ud30c\ub77c\ubbf8\ud130\ub97c \uac00\uc9c4 \ubaa8\ud615\n* c\uac12\uacfc gamma\uac12\uc740 10\uc758 \ubc30\uc218\ub85c \uc77c\ubc18\uc801\uc73c\ub85c \ud55c\ub2e4.\n* \uac10\ub9c8 \ub9e4\uac1c \ubcc0\uc218\ub294 \ub2e8\uc77c \ud559\uc2b5 \uc608\uc81c\uc758 \uc601\ud5a5\uc774 \ub3c4\ub2ec\ud558\ub294 \uc815\ub3c4\ub97c \uc815\uc758\ud558\uba70 \ub0ae\uc740 \uac12\uc740 'far'\ub97c, \ub192\uc740 \uac12\uc740 'close'\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uac10\ub9c8 \ub9e4\uac1c \ubcc0\uc218\ub294 \uc11c\ud3ec\ud2b8 \ubca1\ud130\ub85c \ubaa8\ub378\uc5d0 \uc758\ud574 \uc120\ud0dd\ub41c \uc0d8\ud50c\uc758 \uc601\ud5a5 \ubc18\uacbd\uc758 \uc5ed\uc73c\ub85c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n* C \ub9e4\uac1c \ubcc0\uc218\ub294 \uc758\uc0ac \uacb0\uc815 \ud45c\uba74\uc758 \ub2e8\uc21c\uc131\uc5d0 \ub300\ud55c \ud6c8\ub828 \uc608\uc81c\uc758 \uc624 \ubd84\ub958\ub97c \uc81c\uac70\ud569\ub2c8\ub2e4. C\uac00 \ub0ae\uc744\uc218\ub85d \uacb0\uc815 \ud45c\uba74\uc774 \ub9e4\ub044\ub7fd\uace0 \ub192\uc740 C\ub294 \ubaa8\ub378\uc774 \ub354 \ub9ce\uc740 \uc0d8\ud50c\uc744 \uc11c\ud3ec\ud2b8 \ubca1\ud130\ub85c \uc790\uc720\ub86d\uac8c \uc120\ud0dd\ud560 \uc218 \uc788\ub3c4\ub85d\ud558\uc5ec \ubaa8\ub4e0 \ud559\uc2b5 \uc608\uc81c\ub97c \uc62c\ubc14\ub974\uac8c \ubd84\ub958\ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c\ud569\ub2c8\ub2e4.\n* Verbose\ub294 \ubd88\ub9ac\uc548 \uac12\uc73c\ub85c True\ub85c \ub123\uc73c\uba74 \uaf2c\uce58 \uaf2c\uce58 \ub2e4 \uc54c\ub824\uc8fc\ub294\ub370, \ub300\uc2e0 \uc2dc\uac04\uc774 \uc880 \ub354 \uc624\ub798 \uac78\ub9bd\ub2c8\ub2e4.\n* cv =5\ub294 5 fold\ub85c \uad50\ucc28 \uac80\uc99d\ud55c\ub2e4\ub294 \ub73b\uc785\ub2c8\ub2e4.","bbb46365":"### Logistic Regression\n* Penalty - L1 \uc744 \uc0ac\uc6a9\ud558\ub294 \ud68c\uadc0 \ubaa8\ub378\uc744 Lasso Regression\uc774\ub77c\uace0\ud558\uace0 L2\ub97c \uc0ac\uc6a9\ud558\ub294 \ubaa8\ub378\uc744 Ridge Regression\uc774\ub77c\uace0\ud569\ub2c8\ub2e4. \uc774 \ub458\uc758 \uc8fc\uc694 \ucc28\uc774\uc810\uc740 \ud398\ub110\ud2f0\uc785\ub2c8\ub2e4. \ub9bf\uc9c0 \ud68c\uadc0\ub294 \uc190\uc2e4 \ud568\uc218\uc5d0 \ud398\ub110\ud2f0 \ud56d\uc73c\ub85c \uacc4\uc218\uc758 \"\uc81c\uacf1 \ud06c\uae30\"\ub97c \ucd94\uac00\ud569\ub2c8\ub2e4. L2-norm\uc774 \uc624\ucc28\ub97c \uc81c\uacf1\ud558\uae30 \ub54c\ubb38\uc5d0 (\uc624\ub958> 1 \uc778 \uacbd\uc6b0 \ub85c\ud2b8\uac00 \uc99d\uac00 \ud568) \ubaa8\ub378\uc740 L1-norm\ubcf4\ub2e4 \ud6e8\uc52c \ud070 \uc624\ucc28 (e vs e ^ 2)\ub97c \ubcf4\uac8c\ub418\ubbc0\ub85c \ud6e8\uc52c \ub354 \ubbfc\uac10\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c \uc624\ub958\ub97c \ucd5c\uc18c\ud654\ud558\uae30 \uc704\ud574 \ubaa8\ub378\uc744 \uc870\uc815\ud574\uc90d\ub2c8\ub2e4.\n* C\ub294 estimator \uc785\ub2c8\ub2e4. logspace 1\ucc28\uc6d0 10\uac1c \ubc30\uc5f4\ub85c 0\uc5d0\uc11c 4\uae4c\uc9c0\ub97c estimator\ub85c \ub193\uc740 \uac83\uc785\ub2c8\ub2e4.","92359685":"## 1. Import libraries & data  \ub370\uc774\ud130\uc900\ube44 \ubc0f \ubaa8\ub4c8 \uc784\ud3ec\ud2b8","c897c4fe":"\uc0c8\ub85c\uc6b4 Data Frame\uc744 \ub9cc\ub4ed\ub2c8\ub2e4.","4920de9d":"You can select several rows of dataframe by indexing","ae3c4011":"## 4. Features \ud56d\ubaa9\n\n#### \ud56d\ubaa9\uc758 \uc885\ub958 There are many types of features.\n* \ubc94\uc8fc\ud615 \ud56d\ubaa9 (Categorical Features)\n\n\ubc94\uc8fc\ud615 \ubcc0\uc218\ub294 \ub458 \uc774\uc0c1\uc758 \uacb0\uacfc \uc694\uc18c\uac00 \uc788\ub294 \ubcc0\uc218\uc774\uba70 \ud574\ub2f9 \uae30\ub2a5\uc758 \uac01 \uac12\uc744 \ubc94\uc8fc\ubcc4\ub85c \ubd84\ub958 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \uc131\ubcc4\uc740 \ub450 \uac00\uc9c0 \ubc94\uc8fc (\ub0a8\uc131\uacfc \uc5ec\uc131)\uc758 \ubc94\uc8fc \ud615 \ubcc0\uc218\uc785\ub2c8\ub2e4. \uc774\uc0b0\ud615 \ubcc0\uc218(discrete variable) = \ubc94\uc8fc\ud615 \ubcc0\uc218 (categorical variable) \uc758 \ud558\ub098\ub85c \uba85\ubaa9 \ubcc0\uc218 norminal variable \ub77c\uace0\ub3c4\ud569\ub2c8\ub2e4.\n\nCategorical variables are variables that have more than one result element, and each value of that function can be classified by category. For example, gender is a categorical variable in two categories (male and female). Discrete variable = It is one kind of categorical variable, and is also known as nominal variable .\n\n* \ub370\uc774\ud130 \uc14b\uc5d0\uc11c \uba85\ubaa9 \ud56d\ubaa9 : Sex, Embark \uc774\uba70 \uc6b0\ub9ac\ub294 Name, Ticket \ub4f1\uc744 \uc774\ub85c \ubcc0\ud658\ud574\uc57c \ud560 \uac83 \uac19\uc2b5\ub2c8\ub2e4. The nominal items in the data set are: Sex, Embark and we have to convert Name, Ticket, etc to numbers.\n\n* Ordinal Variable :\n\n\uc21c\uc704 \ubcc0\uc218\ub294 \ubc94\uc8fc \ud615\uc758 \ud558\ub098\uc9c0\ub9cc \uadf8 \ucc28\uc774\uc810\uc740 \uac12 \uc0ac\uc774\uc758 \uc0c1\ub300 \uc21c\uc11c(=\uc11c\uc5f4) \ub610\ub294 \uc815\ub82c\uc774 \uac00\ub2a5\ud558\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\nOrdinal variables are one of the categorical types, but the difference is the relative order (= sequence) or sorting between the values.\n\n\ub370\uc774\ud130 \uc14b\uc5d0\uc11c \uc21c\uc704 \ud56d\ubaa9 : PClass \uc774\uba70 \uc6b0\ub9ac\ub294 Cabin\uc744 \uc774 \ubc94\uc8fc\ub85c \ubcc0\ud658\ud574\uc11c \uc0ac\uc6a9\ud574\uc57c \ud560 \uac83 \uac19\uc2b5\ub2c8\ub2e4.\n\nOrdinal variables in the data set: PClass and we think we should convert Cabin to this category.\n\n* \uc5f0\uc18d\ud615 \ud56d\ubaa9 (Continuous Features):\n\n\uc11c\ub85c \uc5f0\uc18d\ub41c \uac12\uc744 \uac00\uc9c4 \ubcc0\uc218\ub97c \uac00\uc9c4 \ud56d\ubaa9\uc774\uba70 \uc5ec\uae30\uc5d0\uc11c \uc6b0\ub9ac\ub294 \uc5f0\ub839\uc744 \ub300\ud45c\uc801\uc778 \uac83\uc73c\ub85c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\nThis is an item with variables with continuous values, age is one good sample\n\nAge, SipSp, Parch, Fare\ub294 interval variable\ub85c \ub9cc\ub4e4\uc5b4 \uc774\uc5d0 \uc801\uc6a9\ud574\uc57c \ud560 \uac83 \uac19\uc2b5\ub2c8\ub2e4.\n\nAge, SipSp, Parch, and Fare should be catgorized to the interval variable.\n\n* Feature Information\n\n          Variable        Definition                Key\n\n          survival          Survival                    0 = No, 1 = Yes\n\n          pclass          Ticket class                1 = 1st, 2 = 2nd, 3 = 3rd\n\n          sex              Sex    \n\n          Age              Age in years    \n\n          sibsp              # of siblings \/ spouses aboard the Titanic    \n\n          parch              # of parents \/ children aboard the Titanic    \n\n          ticket          Ticket number    \n\n          fare              Passenger fare    \n\n          cabin              Cabin number    \n\n          embarked          Port of Embarkation         C = Cherbourg, Q = Queenstown, S = Southampton","2c8f2a77":"## 2. \ud30c\uc77c \ubd84\ub9ac \ubc0f \ubcd1\ud569 (File Separations and Merges) \n\n* Secure the shape of ntrain and ntest. (Preparation for later splitting from the merged one)\n* y_train is a known result.\n* Separate the passenger ID of the test as it will be added to the final result later.\n* Merge train and test to create a file called data. When you convert a string to a number, or group numbers by interval, it is easier for us to put them together to do it all at once.\n* ntrain\uacfc ntest\uc758 shape\uc744 \ud655\ubcf4\ud574\ub193\uc2b5\ub2c8\ub2e4. (\ubcd1\ud569 \ud55c \uac83\uc744 \ub098\uc911\uc5d0 \ub2e4\uc2dc \uac08\ub77c \ub193\uae30 \uc704\ud55c \uc900\ube44)\n* y_train\uc740 \uc54c\ub824\uc9c4 \uacb0\uacfc \uac12\uc774\ub2c8 \ub530\ub85c \ubaa8\uc154 \ub193\uace0\n* \ud14c\uc2a4\ud2b8\uc758 \uc2b9\uac1d \uc544\uc774\ub514\ub294 \ub098\uc911\uc5d0 \ucd5c\uc885 \uacb0\uacfc\uc5d0 \ub123\uc744 \uac83\uc774\uae30 \ub54c\ubb38\uc5d0 \ub530\ub85c \ub5bc\uc5b4 \ub193\uc2b5\ub2c8\ub2e4.\n* train\uacfc test\ub97c \ubcd1\ud569\ud558\uc5ec data \ub780 \ud30c\uc77c\uc744 \ub9cc\ub4ed\ub2c8\ub2e4. \ubb38\uc790\ub85c \ub41c \uac83\uc744 \uc22b\uc790\ub85c \ubc14\uafbc\ub2e4\ub4e0\uac00.\uc22b\uc790\ub97c \uc778\ud130\ubc1c \ubcc4\ub85c \uadf8\ub8f9\ud654 \ud55c\ub2e4\ub4e0\uac00 \ud560 \ub54c \ud55c\uaebc\ubc88\uc5d0 \ud558\uae30 \uc704\ud574 \ud569\ud574 \ub193\uc2b5\ub2c8\ub2e4.","be5df6d0":"* \ub450 \uc0ac\ub78c\uc758 \ud2f0\ucf13 \ubc88\ud638\uac00 \uac19\uc2b5\ub2c8\ub2e4.\n* \ud639\uc2dc \uac19\uc740 \ud2f0\ucf13 \ubc88\ud638\uac00 \uc788\ub294 \ub2e4\ub978 \uc0ac\ub78c\uc774 \uc788\ub294\uc9c0 \ubd05\ub2c8\ub2e4.\n \n* Two people have the same ticket number.\n* See if anyone else has the same ticket number.","aec83fb5":"## \ub9c8\uc9c0\ub9c9 \ubaa8\ub378 \uc608\uce21","0c409548":"`columns` shows the list of all columns in dataframe","09aaa475":"* \ube48\uce78\uc774 \ubb34\ucc99 \ub9ce\uc2b5\ub2c8\ub2e4.\n* Lots of null values\n* Cabin\uc5d0 \ube44\uc5b4 \uc788\ub294 \uac83\uc774 \ub9ce\uc544 \uc774\ub97c \ub2e4\ub978 \ubd84\ub958\ub85c \uc77c\ub2e8 \uc7a1\uace0 \uae30\uc874 \uac83\uc740 \uc774\ub2c8\uc15c\ub85c \ubd84\ub958\ud569\ub2c8\ub2e4.\n* \ube48 \uac83\uc740 X\ub85c \uad6c\ubd84\ud558\ub824\ub294\ub370 \uc774 \ub610\ud55c 1,2,3 Pclass\uc640 \uc5f0\ub3d9\ub420 \uac83 \uac19\uc73c\ub2c8 \ube44\uc5b4\uc788\uace0 1\ub4f1\uae09\uc740 X, 2\ub4f1\uae09\uc740 Y, 3\ub4f1\uae09\uc740 Z\ub85c \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n \n* There are so many empty bins in the cabin,\n* Existing ones are classified as initials.\n* I want to classify the nulls as X, but this is also reclassified by Pclasses, so there will be X, Y and Z","7d2e7352":"## 5. \ub370\uc774\ud130 \ud0d0\uad6c  Exploratory Data Analysis\n\n* train\ud30c\uc77c \uc21c\uc11c\ub300\ub85c \ub370\uc774\ud130 \ud30c\uc77c\uc758 \uc5f4\ub4e4\uc744 \ubd05\ub2c8\ub2e4. \n* Let's view the columns of the data file in the order in the train file.\n\n![](https:\/\/1.bp.blogspot.com\/-rBTabaGeOTo\/XicYRmv9s7I\/AAAAAAAAKts\/WQDUpGJbv20xbAO8vfnOkqtbCHyme3zNQCLcBGAsYHQ\/s640\/grey%2Barea.png)","8dc6e7a7":"### \"Sex\" \ubd84\uc11d\n\n* \ud568\uc218\ub97c \ub9cc\ub4e4\uc5b4\uc11c train\ud30c\uc77c\uc744 \ubcf4\uc9c0\uc694\n* Let's see the graph Survived \/ Sex","f186c5dd":"* \ubd88\ud589\ud788\ub3c4 \uc0ac\ub9dd\uc790\uac00 \ud6e8\uc52c \ub9ce\uc544 \ubcf4\uc785\ub2c8\ub2e4.\n* \uc804\uccb4 \uc0ac\ub9dd\uc790 \ube44\uc728\uc744 \uc880 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n \n* Unfortunately, there are more deaths than \"survived\".\n \n* Let's take a look at the overall rate.\n* \ud30c\uc774\uadf8\ub798\ud504\ub791 \uce74\uc6b4\ud2b8 \ud50c\ub86f\uc744 \uc11c\ube0c\ud50c\ub86f\uc73c\ub85c \uadf8\ub9bd\ub2c8\ub2e4.\n* \ud589\uc740 \ud558\ub098 \uc5f4\uc740 2\uac1c\uc758 \uc11c\ube0c \ud50c\ub86f\uc785\ub2c8\ub2e4. \uc0ac\uc774\uc988\ub294 \uac00\ub85c 15\uc778\uce58 \uc138\ub85c 6\uc778\uce58\n* 'Survived'\uc758 \uac12\uc744 \uce74\uc6b4\ud2b8\ud574\uc11c \ud30c\uc774\ud50c\ub86f\uc744 \ub9cc\ub4ed\ub2c8\ub2e4.\n* explode\ub294 \ud3ed\ubc1c\ud558\ub294 \uac83\uc774\ub2c8\uae4c 1\uc774\uba74 \ud280\uc5b4 \ub098\uac00\ub294 \uac83\uc778\ub370 0\uc744 \uc8fc\uba74 \ubd84\ub9ac\ub9cc \ub418\uace0 \ub3cc\ucd9c\uc740 \ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc774\uc5b4\uc11c 0, 1\uc778 \uac83\uc740 \uccab \ubc88\uc9f8 \uac83\uc740 \uc544\ub2c8\uace0 \ub450\ubc88 \uc9f8 \uac83\uc740 \ubd84\ub9ac\ub41c\ub2e4\ub294 \uc758\ubbf8\ub85c \uc0dd\uac01\ud558\uc2dc\uba74 \ub429\ub2c8\ub2e4.\n* autopercent\ub294 1.1\uc774 \ud45c\ud604\ud558\ub294 \ubd80\ubd84\uc740 \uc18c\uc218\uc810 \ud55c \uc790\ub9ac\uae4c\uc9c0 \ubcf4\uc5ec \uc8fc\ub77c\ub294 \uc758\ubbf8\uc785\ub2c8\ub2e4. \ub4a4\uc5d0 \uc810 \uc774\ud558\uac00 4\uba74 \ub458 \ub2e4 \uc18c\uc218\uc810 4\uc790\ub9ac\uc218 \uae4c\uc9c0 \ubcf4\uc5ec \uc90d\ub2c8\ub2e4.\n* ax[0]\uc740 \uccab\ubc88\uc9f8 \uce78\uc785\ub2c8\ub2e4.\n* set_title \uba54\uc18c\ub4dc\ub294 \uc11c\ube0c \ud50c\ub86f\uc758 \uc81c\ubaa9\uc744 \ubcf4\uc5ec \uc90d\ub2c8\ub2e4.\n \n* Draw a pie chart and count plot in the subplot parts.\n \n* Two rows and one column. Size : 15 inches wide by 6 inches high\n* Create a pieplot by counting the value of 'Survived'.\n* Explode means \"protrusion\", so if it's 1, it's popping out. 0 and 1 means that the second one is poping out.\n* Autopercent means 1.1 shows up to one decimal place. If there is 4 after the dot, both show up to four decimal places.\n* ax [0] is the first cell.\n* The set_title method displays the title of the subplot.","3665e59e":"### 6.2 Adding Features\n\n* Some done above\n\n* \uadf8\ub798\ub3c4 \uba87 \uac1c \ub9cc\ub4e4\uc5b4 \ubcfc\uae4c\uc694? But why don't we try making some more\n\n* Priority - (1) Nobles (2) Women in Pclass 1 (3) Babies under 1 (4) Kids under 17 in Pclass 1 & 2  (5) Women in Pclass 3\n* FH - Female Higher Survival Group\n* MH - Male Higher Survival Group\n* FL - Female Lower Surival Group\n* ML - Male Lower Survival Group\n\n* And I will keep making new ones until all the data are either over 80% or le","626ccc2a":"### \"Name\" \ubd84\uc11d\n* \uc774\ub984\uc740 \uac70\uc758 \ubaa8\ub450 \ub2e4\ub97c \uac00\ub2a5\uc131\uc774 \ud07d\ub2c8\ub2e4. Family Name, First Name, Middle Name and even Dr. Capt, master and so on \ubaa8\ub450 \uac10\uc548\ud558\uba74...\n* \ubd84\ub958\ub97c \ud55c \ubc88 \ud574 \ubd05\ub2c8\ub2e4.\n* \ub9ac\uc2a4\ud2b8\ub97c \ud55c \ubc88 \uc8fc \uc6b1 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n* The names are most likely different...all of them . Family Name, First Name, Middle Name and even Dr. Given Capt, Master and so on ...\n* Try to classify them.\n* Let's look at the list once.","93b79428":"### CSV to DF\n\nimport modules -> import data -> turning data into data frame (pandas) \n\n* \uc774\uc5b4\uc11c \uc774\ub97c \ub370\uc774\ud130\ud504\ub808\uc784\uc5d0 \uc784\ud3ec\ud2b8\ud558\uc5ec \ub370\uc774\ud130\uc14b\uc774 \ud310\ub2e4\uc2a4 \ub370\uc774\ud130\ud504\ub808\uc784\uc5d0 \uc784\ud3ec\ud2b8 \ub418\ub3c4\ub85d \ud569\ub2c8\ub2e4.\n* Import the data into the dataframe so the dataset is converted as a pandas dataframe.","b049afa1":"* \uc0ac\ud68c\ub294 \ubd88\uacf5\ud3c9 \ud588\uc73c\ub098 \ucd5c\uc18c\ud55c \ub0a8\uc790\ub4e4\uc758 \uc2e0\uc0ac\ub3c4\ub294 \uc788\uc5c8\ub2e4\uace0 \ud560 \uc218 \uc788\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2e4.\n* The society was unfair but gentlemenship worked there.","b85d1c08":"We can see that several titles are common, but most of them are rare.","37f621c5":"\uc774\ub97c \uc131\ubcc4\ub85c \ubd05\ub2c8\ub2e4.\nView it by gender.","a0134b68":"`head()` shows first five rows","69e5853d":"`shape` shows number of rows and number of columns","80bda292":"### XGB Step 4","0038ffb5":"* Pclass1\uc758 \uc2b9\uac1d \uc694\uae08\uc5d0\ub294 \ud070 \ubd84\ud3ec\uac00\uc788\ub294 \uac83\uc73c\ub85c \ubcf4\uc774\uba70 \ubd88\uc5f0\uc18d \uac12\uc73c\ub85c \ubcc0\ud658 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n* The passenger fare for Pclass1 seems to have a large distribution and can be converted to discrete values.\n\n* Fare\ub97c \uadf8\ub8f9\uc73c\ub85c \ub098\ub204\uc5b4 \ub193\uaca0\uc2b5\ub2c8\ub2e4.\n* qcut\uc744 \ud65c\uc6a9\ud558\uba74 \uc6d0\ud558\ub294 \uc870\uac01\uc73c\ub85c \ub370\uc774\ud130\ub97c \ub098\ub204\uc5b4 \uc90d\ub2c8\ub2e4.\n \n* I will divide the Fare into groups.\n* Use qcut to divide the data into the desired pieces.","d6e7899d":"## 3. Check the files \/ \ud30c\uc77c\uc744 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","36cdf6d3":"* \uc0dd\uc874 \uc22b\uc790\ub85c \ubd05\ub2c8\ub2e4.\n* by number"}}