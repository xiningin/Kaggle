{"cell_type":{"6b4cf73b":"code","2089c96c":"code","d4971b53":"code","dd381fc6":"code","15e58c5c":"code","bcc2d6c1":"code","2d6f2158":"code","4a1d150b":"code","019859e2":"code","bca6ec66":"code","981b6db3":"code","4adc5ae5":"code","a04b9b3e":"code","3ebfdc49":"code","76a1ffa0":"code","a1cfef4f":"code","adaedd70":"markdown","5e58e520":"markdown","8fd651f8":"markdown","82fa7c83":"markdown","eea8b412":"markdown","9312b215":"markdown","b1badd2d":"markdown","4292f5ca":"markdown","1471f61d":"markdown","cb055c92":"markdown"},"source":{"6b4cf73b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt \n\nimport tensorflow as tf\nprint(tf.__version__)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n        \n#df = pd.read_csv('kaggle\/input\/timeseries_cell.csv')        \n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2089c96c":"\ndf = pd.read_csv('..\/input\/timeseries-cellcsv\/timeseries_cell.csv') ","d4971b53":"time_steps = []\nobservations = []\ncounter = 0\n\ndf_i = df.iloc[::4, :]\n   \nfor index, row in df_i.iterrows(): \n    observations.append(float(row['nr_people']))\n    time_steps.append(int(counter))\n    counter+=1","dd381fc6":"def plot_the_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time span\")\n    plt.ylabel(\"Observations\")\n    plt.grid(True)","15e58c5c":"series = np.array(observations)\ntime = np.array(time_steps)\nplt.figure(figsize=(20, 6))\nplot_the_series(time, series)","bcc2d6c1":"split_time = 2100\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\nwindow_size = 30\nbatch_size = 32\nshuffle_buffer_size = 1000\n","2d6f2158":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)\n\ndef model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","4a1d150b":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\nwindow_size = 60\nbatch_size = 256\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\nprint(train_set)\nprint(x_train.shape)","019859e2":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=64, kernel_size=5,strides=1, padding=\"causal\",\n                      activation=\"relu\", input_shape=[window_size, 1]),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 30)\n])\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch \/ 20))\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\nhistory = model.fit(train_set, epochs=150, callbacks=[lr_schedule])","bca6ec66":"plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-8, 1e-1, 0, 200], figsize = (15, 5))","981b6db3":"from keras.utils.vis_utils import plot_model\n\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True, rankdir=\"LR\")","4adc5ae5":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\nwindow_size = 168\nbatch_size = 328\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\ntf.keras.backend.clear_session()\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=64, kernel_size=5,strides=1, padding=\"causal\",\n                      activation=\"relu\", input_shape=[window_size, 1]),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 30)\n])\n\noptimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\nhistory = model.fit(train_set, epochs=150)","a04b9b3e":"plt.plot(history.history['loss'], label='')\nplt.ylabel('MAE value')\nplt.xlabel('No. epoch')\nplt.legend(loc=\"upper left\")\nplt.show()","3ebfdc49":"forecast = model_forecast(model, series[..., np.newaxis], window_size)\nforecast = forecast[split_time - window_size:-1, -1, 0]","76a1ffa0":"tf.keras.metrics.mean_absolute_error(x_valid, forecast).numpy()","a1cfef4f":"plt.figure(figsize=(20, 6))\nplot_the_series(time_valid, x_valid)\nplot_the_series(time_valid, forecast)","adaedd70":"### Prepare moving window and train-test dataset\n","5e58e520":"### Load timeseries dataset","8fd651f8":"### Visualize forecastg vs test data\n","82fa7c83":"### Visualize time series to forecast","eea8b412":"### visualize loss with learning rate 1e-5","9312b215":"### Visualize network","b1badd2d":"### Tune the learning rate","4292f5ca":"### Compute the forecasting error  (rmse) root mean square error","1471f61d":"### Prepare data for neural network\n","cb055c92":"### Train with the right learning rate, window and batch size"}}