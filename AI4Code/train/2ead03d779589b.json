{"cell_type":{"9ed07331":"code","7365f4fa":"code","6baa5579":"code","a3ac7776":"code","ed88e533":"code","a402005e":"code","b4a7f952":"code","71131288":"code","b98e629e":"code","94efd92d":"code","01f4f47f":"code","df718338":"code","86f850ec":"code","a55b6078":"markdown","e974c869":"markdown"},"source":{"9ed07331":"!pip install pytorch-tabnet","7365f4fa":"import os\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport time\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")","6baa5579":"pd.options.display.max_rows = None\npd.options.display.max_columns = None","a3ac7776":"def splitData(df: \"pd.DataFrame\", FEATS: \"List\"):\n    \"\"\"Split the dataframe into train and test\n    \n    Args:\n        df: preprocessed dataframe\n        FEATS: feature list\n        \n    Returns:\n        X_train, y_train, X_test, y_test\n    \"\"\"\n    \n    train, test = train_test_split(df, test_size = .3, random_state = 42)\n    \n    return train[FEATS], train[\"Survived\"], test[FEATS], test[\"Survived\"]","ed88e533":"def prepareInputs(df: \"pd.DataFrame\"):\n    \"\"\"Preprocess\n    \n    Args:\n        df: raw dataframe\n    \n    Return:\n        df: processed dataframe\n    \"\"\"\n    \n    # 1: Inpute missing values\n    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n    df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].mean())\n    \n    # 2: One hot encoding\n    df[\"Cabin_category\"] = [str(cabin_category)[0] for cabin_category in df[\"Cabin\"]]\n    \n    categorical_cols = [\n        \"Pclass\", \"Sex\", \"SibSp\",\n        \"Parch\", \"Embarked\", \n        \"Cabin_category\"\n                        ]\n    \n    for col in categorical_cols:\n        dummies = pd.get_dummies(df[col], \n                                 drop_first = True,\n                                 prefix = col\n                                )\n        df = pd.concat([df, dummies], 1)\n        \n    df = df.drop(categorical_cols, 1)\n    \n    df[\"Ticket_A\/5\"] = np.where(df[\"Age\"] == \"A\/5\", 1, 0)\n    df[\"Ticket_C.A\"] = np.where(df[\"Age\"] == \"C.A.\", 1, 0)\n    df[\"Ticket_SC\/PARIS\"] = np.where(df[\"Age\"] == \"SC\/PARIS\", 1, 0)\n    df = df.drop([\"Cabin\", \"Ticket\"], 1)\n    \n    return df","a402005e":"# Standardise the data sets\ndef standardiseNumericalFeats(df):\n    \"\"\"Standardise the numerical features\n    \n    Returns:\n        Standardised dataframe\n    \"\"\"\n\n    numerical_cols = [\n        \"Age\", \"Fare\"\n    ]\n\n    for col in numerical_cols:\n        scaler = StandardScaler()\n\n        df[col] = scaler.fit_transform(df[[col]])\n        \n    return df","b4a7f952":"def tabNetPretrain(X_train):\n    \"\"\"Pretrain TabNet model\n    \n    Return:\n        TabNet pretrainer obj\n    \"\"\"\n    tabnet_params = dict(n_d=8, n_a=8, n_steps=3, gamma=1.3,\n                             n_independent=2, n_shared=2,\n                             seed=42, lambda_sparse=1e-3,\n                             optimizer_fn=torch.optim.Adam,\n                             optimizer_params=dict(lr=2e-2,\n                                                   weight_decay=1e-5\n                                                  ),\n                             mask_type=\"entmax\",\n                             scheduler_params=dict(max_lr=0.05,\n                                                   steps_per_epoch=int(X_train.shape[0] \/ 256),\n                                                   epochs=200,\n                                                   is_batch_level=True\n                                                  ),\n                             scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n                             verbose=10\n                        )\n\n    pretrainer = TabNetPretrainer(**tabnet_params)\n\n    pretrainer.fit(\n        X_train=X_train.to_numpy(),\n        eval_set=[X_train.to_numpy()],\n        max_epochs = 100,\n        patience = 15, \n        batch_size = 256, \n        virtual_batch_size = 128,\n        num_workers = 1, \n        drop_last = True)\n    \n    return pretrainer","71131288":"def trainTabNetModel(X_train, y_train, X_test, y_test, pretrainer):\n    \"\"\"Train TabNet model\n    \n    Args:\n        pretrainer: pretrained model. If not using this, use None\n        \n    Return:\n        TabNet model obj\n    \"\"\"\n    \n    tabNet_model = TabNetClassifier(\n                                   n_d=8,\n                                   n_a=8,\n                                   n_steps=4,\n                                   gamma=1.3,\n                                   n_independent=4,\n                                   n_shared=5,\n                                   seed=42,\n                                   optimizer_fn = torch.optim.Adam,\n                                   scheduler_params = {\"milestones\": [150,250,300,350,400,450],'gamma':0.2},\n                                   scheduler_fn=torch.optim.lr_scheduler.MultiStepLR\n                                  )\n\n    tabNet_model.fit(\n        X_train = X_train.to_numpy(),\n        y_train = y_train.to_numpy(),\n        eval_set=[(X_train.to_numpy(), y_train.to_numpy()),\n                  (X_test.to_numpy(), y_test.to_numpy())],\n        eval_metric=['accuracy'],\n        max_epochs = 100,\n        batch_size = 256,\n        patience = 15,\n        from_unsupervised = pretrainer\n        )\n    \n    return tabNet_model","b98e629e":"# Make predictions\ndef makePredictions(X_test, tabNet_model) -> \"pd.DataFrame\":\n    \"\"\"Make predictions\n    \n    Return:\n        Predictions\n    \"\"\"\n    \n    return tabNet_model.predict_proba(X_test.to_numpy())[:,1]","94efd92d":"# Evaluation\ndef evaluate(y_test, y_tabNet_pred) -> None:\n    \"\"\"Evaluate the predictions\n    \n    Process:\n        Print accuracy score\n    \"\"\"\n    \n    print(\"The accuracy score of TabNet model is \" +\n          str(round(accuracy_score(y_test, np.where(y_tabNet_pred > .42, 1, 0)), 4))\n         )","01f4f47f":"FEATS = [\n        \"Pclass\", \"Sex\", \"Age\",\n        \"SibSp\", \"Parch\", \"Ticket\",\n        \"Fare\", \"Cabin\", \"Embarked\"\n    ]\n\nprint(\"Reading the data\")\ndf = pd.read_csv(\"..\/input\/tabular-playground-series-apr-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-apr-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv\")\n\nprint(\"Preprocessing the data\")\nX_train, y_train, X_validation, y_validation = splitData(df, FEATS)\nX_train, X_validation = prepareInputs(X_train), prepareInputs(X_validation)\nX_train, X_validation = standardiseNumericalFeats(X_train), standardiseNumericalFeats(X_validation)\n\nprint(\"The ratio of lapse class in training set is \" +\n      str(round(y_train.sum()\/len(y_train) * 100, 2)) +\n      \"%\"\n     )\n\nprint(\"The ratio of lapse class in validation set is \" +\n      str(round(y_validation.sum()\/len(y_validation) * 100, 2)) +\n      \"%\"\n     )\n\nprint(\"Pretrain TabNet model\")\npretrainer = tabNetPretrain(X_train)\n\nprint(\"Training TabNet model\")\ntabNet_model = trainTabNetModel(X_train, y_train, X_validation, y_validation, pretrainer)\n\nprint(\"Making validation predictions\")\ny_tabNet_pred = makePredictions(X_validation, tabNet_model)\n\nprint(\"Evaluation of the model\")\nevaluate(y_validation, y_tabNet_pred)\n\nprint(\"Making predictions\")\ntest = prepareInputs(test[FEATS])\ntest = standardiseNumericalFeats(test)\n\nsubmission[\"Survived\"] = makePredictions(test, tabNet_model)\nsubmission[\"Survived\"] = np.where(submission[\"Survived\"] > .42, 1, 0)\n\nsubmission.to_csv(\"submission.csv\", index = False)","df718338":"# TabNet model\nimportance_tabNet = pd.DataFrame(tabNet_model.feature_importances_,index=X_train.columns).sort_values(0, ascending = False)\nimportance_tabNet.columns = [\"importance\"]\nimportance_tabNet","86f850ec":"plt.hist(y_tabNet_pred, bins = 100)\nplt.title(\"Prediction distribution of pretrained TabNet\")\nplt.show()","a55b6078":"## Feature importance","e974c869":"## Prediction distribution"}}