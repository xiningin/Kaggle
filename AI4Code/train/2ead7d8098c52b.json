{"cell_type":{"1129245a":"code","65ed6e4e":"code","5ae36de6":"code","9f2678ff":"code","c0569fbc":"code","1e8cd8bf":"code","4888e28b":"code","aeb73fc4":"code","aede1e34":"code","b6b92624":"code","00106faa":"code","61a3ea30":"code","917d36e1":"code","20db0d82":"code","99fa3188":"code","84d07a88":"code","9ae043b1":"code","0b755247":"code","bcc40150":"code","f84a62c0":"code","9f687481":"code","9d265ce0":"code","1a9af0a4":"code","b7fbbd28":"code","a238a4b9":"code","fb92f364":"code","e6b5c8c2":"code","b8c6b363":"code","d4396f9b":"code","e8ba2617":"code","7bd38f3b":"code","9ff9f1a0":"code","7244f5f1":"code","2ad00339":"code","70f77674":"code","c6a995f9":"code","bf04c0f3":"code","f4d58b24":"code","a9d5185e":"code","fc575ce9":"code","f1a65780":"code","9c517287":"code","f88b792a":"code","e82b2ddd":"code","e113ea58":"code","c8960859":"code","155b59c8":"code","91f72fef":"code","c311e983":"code","9385e4bb":"code","232dfc42":"code","d908df5d":"code","3b1dd5f4":"code","0ef9f511":"code","d71e067f":"code","ea9229c4":"code","a486ee24":"code","179aaf2c":"code","2bf678a0":"code","2abaefa2":"code","db50af29":"code","bce5a238":"code","7aa056ca":"code","a49e9141":"code","a7eda6d8":"code","77ce2ea4":"code","a99523ff":"code","97d21c1a":"code","abc53902":"code","b96c2564":"code","251a5ab7":"code","4386d189":"code","0cc18b38":"code","0ad07e77":"code","d8ba6c81":"code","a44fef38":"code","882c2673":"code","e44cceb4":"code","196a5cc1":"code","bf16b5a3":"code","82c6bd7e":"code","daa42427":"markdown","f1e330dd":"markdown","9d461286":"markdown","e170c70f":"markdown","a4d6896d":"markdown","5a802518":"markdown","a6f38c68":"markdown","a4f8e4bd":"markdown","8fd2195c":"markdown","093578c2":"markdown","f5dd7c03":"markdown","7f236bc4":"markdown","d59243e0":"markdown","48a1e948":"markdown","07801bc2":"markdown","aec4d1e1":"markdown","cb3c5fe9":"markdown"},"source":{"1129245a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\n\nimport os\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","65ed6e4e":"# reading the dataset\nhouse = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","5ae36de6":"# head\nhouse.head()","9f2678ff":"house.shape","c0569fbc":"test.shape","1e8cd8bf":"# summary of the dataset: 1460 rows, 81 columns\nhouse.info()","4888e28b":"house.describe()      #other atributes of the dataframe","aeb73fc4":"# all numeric (float and int) variables in the dataset\nhouse_numeric = house.select_dtypes(include=['float64', 'int64'])\nhouse_numeric.head()","aede1e34":"house_numeric.info()","b6b92624":"# dropping the columns we want to treat as categorical variables\nhouse_numeric = house_numeric.drop(['MSSubClass', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', \n                                    'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', \n                                   'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', \n                                   'MoSold', 'YrSold'], axis=1)\nhouse_numeric.head()","00106faa":"house_numeric.describe(percentiles=[.25, .5, .75, .90, .95, .99])","61a3ea30":"# outlier treatment\nplt.boxplot(house['PoolArea'])\nQ1 = house['PoolArea'].quantile(0.1)\nQ3 = house['PoolArea'].quantile(0.9)\nIQR = Q3 - Q1\nhouse = house[(house['PoolArea'] >= Q1 - 1.5*IQR) & \n                      (house['PoolArea'] <= Q3 + 1.5*IQR)]\nhouse.shape","917d36e1":"# outlier treatment\nplt.boxplot(house['MiscVal'])\nQ1 = house['MiscVal'].quantile(0.1)\nQ3 = house['MiscVal'].quantile(0.9)\nIQR = Q3 - Q1\nhouse = house[(house['MiscVal'] >= Q1 - 1.5*IQR) & \n                      (house['MiscVal'] <= Q3 + 1.5*IQR)]\nhouse.shape","20db0d82":"# outlier treatment\nplt.boxplot(house['ScreenPorch'])\nQ1 = house['ScreenPorch'].quantile(0.1)\nQ3 = house['ScreenPorch'].quantile(0.9)\nIQR = Q3 - Q1\nhouse = house[(house['ScreenPorch'] >= Q1 - 1.5*IQR) & \n                      (house['ScreenPorch'] <= Q3 + 1.5*IQR)]\nhouse.shape","99fa3188":"# outlier treatment\nplt.boxplot(house['LotArea'])\nQ1 = house['LotArea'].quantile(0.1)\nQ3 = house['LotArea'].quantile(0.9)\nIQR = Q3 - Q1\nhouse = house[(house['LotArea'] >= Q1 - 1.5*IQR) & \n                      (house['LotArea'] <= Q3 + 1.5*IQR)]\nhouse.shape","84d07a88":"# outlier treatment\nplt.boxplot(house['MasVnrArea'])\nQ1 = house['MasVnrArea'].quantile(0.1)\nQ3 = house['MasVnrArea'].quantile(0.9)\nIQR = Q3 - Q1\nhouse = house[(house['MasVnrArea'] >= Q1 - 1.5*IQR) & \n                      (house['MasVnrArea'] <= Q3 + 1.5*IQR)]\nhouse.shape","9ae043b1":"# outlier treatment\nplt.boxplot(house['SalePrice'])\nQ1 = house['SalePrice'].quantile(0.1)\nQ3 = house['SalePrice'].quantile(0.9)\nIQR = Q3 - Q1\nhouse = house[(house['SalePrice'] >= Q1 - 1.5*IQR) & \n                      (house['SalePrice'] <= Q3 + 1.5*IQR)]\nhouse.shape","0b755247":"# correlation matrix\ncor = house_numeric.corr()\ncor","bcc40150":"# plotting correlations on a heatmap\n\n# figure size\nplt.figure(figsize=(18,10))\n\n# heatmap\nsns.heatmap(cor, annot=True)\nplt.show()\n","f84a62c0":"# variable formats\nhouse.info()","9f687481":"house.isnull().sum()  #checking the number of null values in the dataset","9d265ce0":"# Checking the percentage of missing values\nround(100*(house.isnull().sum()\/len(house.index)), 2)","1a9af0a4":"house.shape","b7fbbd28":"house = pd.concat((house,test))","a238a4b9":"#NA in Alley column means No Alley, so we will replace NA by it.\nhouse['Alley'].fillna('No Alley', inplace=True)","fb92f364":"house['MasVnrType'].fillna('None', inplace=True) ","e6b5c8c2":"#NA in FireplaceQu column means No Fireplace, so we will replace NA by it.\nhouse['FireplaceQu'].fillna('No Fireplace', inplace=True)","b8c6b363":"#NA in PoolQC column means No Pool, so we will replace NA by it.\nhouse['PoolQC'].fillna('No Pool', inplace=True) ","d4396f9b":"#NA in Fence column means No Fence, so we will replace NA by it.\nhouse['Fence'].fillna('No Fence', inplace=True) ","e8ba2617":"house['MasVnrArea'].fillna(0, inplace=True) ","7bd38f3b":"house['LotFrontage'].fillna(0, inplace=True) ","9ff9f1a0":"#NA in GarageType, GarageFinish, GarageQual, GarageCond columns mean No Garage, so we will replace NA by it.\n\nhouse['GarageType'].fillna('No Garage', inplace=True) \nhouse['GarageFinish'].fillna('No Garage', inplace=True) \nhouse['GarageQual'].fillna('No Garage', inplace=True) \nhouse['GarageCond'].fillna('No Garage', inplace=True) ","7244f5f1":"# MiscFeature column has almost 99% null values so we will drop it\nhouse= house.drop('MiscFeature', axis=1)","2ad00339":"house.isnull().sum()","70f77674":"#converting year to number of years\nhouse['YearBuilt'] = 2019 - house['YearBuilt']\nhouse['YearRemodAdd'] = 2019 - house['YearRemodAdd']\nhouse['GarageYrBlt'] = 2019 - house['GarageYrBlt']\nhouse['YrSold'] = 2019 - house['YrSold']","c6a995f9":"#converting from int type to object to treat the variables as categorical variables\nhouse['MSSubClass'] = house['MSSubClass'].astype('object')\nhouse['OverallQual'] = house['OverallQual'].astype('object')\nhouse['OverallCond'] = house['OverallCond'].astype('object')\nhouse['BsmtFullBath'] = house['BsmtFullBath'].astype('object')\nhouse['BsmtHalfBath'] = house['BsmtHalfBath'].astype('object')\nhouse['FullBath'] = house['FullBath'].astype('object')\nhouse['HalfBath'] = house['HalfBath'].astype('object')\nhouse['BedroomAbvGr'] = house['BedroomAbvGr'].astype('object')\nhouse['KitchenAbvGr'] = house['KitchenAbvGr'].astype('object')\nhouse['TotRmsAbvGrd'] = house['TotRmsAbvGrd'].astype('object')\nhouse['Fireplaces'] = house['Fireplaces'].astype('object')\nhouse['GarageCars'] = house['GarageCars'].astype('object')","bf04c0f3":"house.shape","f4d58b24":"final = house","a9d5185e":"# List of variables to map\n\nvarlist1 =  ['Street']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({'Pave': 1, \"Grvl\": 0})\n\n# Applying the function to the Lead list\nfinal[varlist1] = final[varlist1].apply(binary_map)","fc575ce9":"# List of variables to map\n\nvarlist2 =  ['Utilities']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({'AllPub': 1, \"NoSeWa\": 0})\n\n# Applying the function to the Lead list\nfinal[varlist2] = final[varlist2].apply(binary_map)","f1a65780":"# List of variables to map\n\nvarlist3 =  ['CentralAir']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({'Y': 1, \"N\": 0})\n\n# Applying the function to the Lead list\nfinal[varlist3] = final[varlist3].apply(binary_map)","9c517287":"# split into X and y\nX = final.drop([ 'Id'], axis=1)\n","f88b792a":"# creating dummy variables for categorical variables\n\n# subset all categorical variables\nhouse_categorical = X.select_dtypes(include=['object'])\nhouse_categorical.head()\n","e82b2ddd":"# convert into dummies\nhouse_dummies = pd.get_dummies(house_categorical, drop_first=True)\nhouse_dummies.head()","e113ea58":"# drop categorical variables \nfinal = final.drop(list(house_categorical.columns), axis=1)","c8960859":"# concat dummy variables with X\nfinal = pd.concat([final, house_dummies], axis=1)","155b59c8":"final.shape","91f72fef":"test = final.tail(1459)","c311e983":"test.shape","9385e4bb":"X = final.head(1253)\ny = np.log(X.SalePrice)\nX = X.drop(\"SalePrice\",1) # take out the target variable","232dfc42":"test = test.fillna(test.interpolate())\n","d908df5d":"X = X.fillna(X.interpolate())","3b1dd5f4":"test = test.drop(\"SalePrice\",1) # take out the target variable","0ef9f511":"# scaling the features\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)","d71e067f":"# scaling the features\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(test)","ea9229c4":"# split into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.7,\n                                                    test_size = 0.3, random_state=100)","a486ee24":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\n\nridge = Ridge()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train) ","179aaf2c":"#checking the value of optimum number of parameters\nprint(model_cv.best_params_)\nprint(model_cv.best_score_)","2bf678a0":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results = cv_results[cv_results['param_alpha']<=1000]\ncv_results","2abaefa2":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\nplt.figure(figsize=(16,5))\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper right')\nplt.show()","db50af29":"alpha = 4\nridge = Ridge(alpha=alpha)\n\nridge.fit(X_train, y_train)\nridge.coef_","bce5a238":"#lets predict the R-squared value of test and train data\ny_train_pred = ridge.predict(X_train)\nprint(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))","7aa056ca":"lasso = Lasso()\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","a49e9141":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","a7eda6d8":"#lets find out the R-squared value of the lasso model\nmodel_cv1 = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'r2', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv1.fit(X_train, y_train) ","77ce2ea4":"# cv results\ncv_results1 = pd.DataFrame(model_cv1.cv_results_)\ncv_results1","a99523ff":"# plotting cv results\nplt.figure(figsize=(16,4))\n\nplt.plot(cv_results1[\"param_alpha\"], cv_results1[\"mean_test_score\"])\nplt.plot(cv_results1[\"param_alpha\"], cv_results1[\"mean_train_score\"])\nplt.xlabel('number of features')\nplt.ylabel('r-squared')\nplt.title(\"Optimal Number of Features\")\nplt.legend(['test score', 'train score'], loc='upper right')","97d21c1a":"#checking the value of optimum number of parameters\nprint(model_cv.best_params_)\nprint(model_cv.best_score_)","abc53902":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.figure(figsize=(16,5))\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper right')\nplt.show()","b96c2564":"alpha = 0.0001\n\nlasso = Lasso(alpha=alpha)\n        \nlasso.fit(X_train, y_train) ","251a5ab7":"#lets predict the R-squared value of test and train data\ny_train_pred = lasso.predict(X_train)\nprint(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))\n","4386d189":"alpha = 0.0001\n\nlasso = Lasso(alpha=alpha)\n        \nlasso.fit(X_train, y_train) ","0cc18b38":"#lets predict the R-squared value of test and train data\ny_test_pred = lasso.predict(X_test)\nprint(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))\n","0ad07e77":"from sklearn.metrics import mean_squared_error\nprint ('RMSE is: \\n', mean_squared_error(y_test, y_test_pred))","d8ba6c81":"alpha = 0.0001\n\nlasso = Lasso(alpha=alpha)\n\nlasso.fit(X_train,y_train)\npreds = lasso.predict(test)\nfinal_predictions = np.exp(preds)","a44fef38":"test.index = test.index + 1461","882c2673":"submission = pd.DataFrame({'Id': test.index ,'SalePrice': final_predictions })\n","e44cceb4":"submission.to_csv(\"submission.csv\",index=False)","196a5cc1":"alpha = 4\n\nridge = Ridge(alpha=alpha)\n\nridge.fit(X_train,y_train)\npreds1 = ridge.predict(test)\nfinal_predictions1 = np.exp(preds1)","bf16b5a3":"submission1 = pd.DataFrame({'Id': test.index ,'SalePrice': final_predictions1 })\n","82c6bd7e":"submission1.to_csv(\"submission1.csv\",index=False)","daa42427":"## Assignment- Advanced Regression\n\n\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual value and flip them at a higher price. For the same purpose, the company has collected a data set from house sales in Australia. The data is provided in the csv file below.\n\n \n\nThe company is looking at prospective properties to buy to enter the market.\n\nYou are required to build a regression model using regularization, so as to predict the actual value of the prospective properties and decide whether to invest in them or not.\n\n \n\nThe company wants to know:\n\nWhich variables are significant in predicting the price of a house\n\nHow well those variables describe the price of a house","f1e330dd":"### from the above graph and the best_param_ score we got optimum lambda to be 4","9d461286":"## 4. Model Building and Evaluation","e170c70f":"## 1. Data Understanding and Exploration\n\nLet's first have a look at the dataset and understand the size, attribute names etc.","a4d6896d":"## Ridge Regression","5a802518":"## Checking the Correlation between the variables","a6f38c68":"Here, although some variables are numeric (int), we'd rather treat them as categorical since they have discrete values.","a4f8e4bd":"## Lasso","8fd2195c":"## 2. Data Cleaning\n\nLet's now conduct some data cleaning steps. \n\nWe've seen that there are some missing values in the dataset. We've also seen that variables are in the correct format, except some variables with distinct values, which should rather be categorical variables (so that dummy variable are created for the categories).","093578c2":"### from the above graph and the best_param_ score we got optimum lambda to be 0.0001","f5dd7c03":"#### Data Exploration","7f236bc4":"## 3. Data Preparation \n\n\n#### Data Preparation\n\nLet's now prepare the data and build the model.","d59243e0":"## Dummy Variables","48a1e948":"## Null value treatment\nInstead of dropping the null values which will result in a data loss, we will impute the null values according to the domain understanding and the data dictionary provided with the data.","07801bc2":"## Ridge and Lasso Regression\n\nLet's now try predicting car prices, a dataset used in simple linear regression, to perform ridge and lasso regression.","aec4d1e1":"predictions using ridge","cb3c5fe9":"### Outlier Treatment"}}