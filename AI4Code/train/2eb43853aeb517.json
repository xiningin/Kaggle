{"cell_type":{"3effa0d4":"code","2e89f6d7":"code","b366b6a2":"code","93c20c9b":"code","d321f013":"code","5923c426":"code","677053d3":"code","dc5da910":"code","74237723":"code","7720cda5":"code","74d6c1c0":"code","a1e7693b":"code","4c8ab6b8":"code","7590cb93":"code","9fa526cf":"markdown","e0156ed0":"markdown","88e3f7f7":"markdown","c8de6315":"markdown","5f622f97":"markdown","ee1f1efd":"markdown","aa5dc391":"markdown","ca879aa0":"markdown","f8008204":"markdown","e3aadfa4":"markdown","ebabee7f":"markdown","a49ddffc":"markdown","12f9fff8":"markdown","b4a9f321":"markdown","799a89c0":"markdown","9160dd7f":"markdown","9dfef460":"markdown"},"source":{"3effa0d4":"# To prevent the annoyning Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","2e89f6d7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')\ncmap = sns.cm.mako_r\n\n%matplotlib inline\n\nimport re\nfrom nltk import word_tokenize, corpus\nfrom nltk.stem import PorterStemmer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nenglish_words = set(corpus.words.words())\n\nfrom IPython.core.display import HTML\nHTML(\"\"\"<style> \n            .output_png { display: table-cell; text-align: center; vertical-align: middle; } \n     <\/style> \"\"\")","b366b6a2":"reddit = pd.read_csv('..\/input\/reddit-wallstreetsbets-posts\/reddit_wsb.csv')\nreddit.drop(columns=['id', 'url', 'created'], inplace=True)","93c20c9b":"reddit.head()","d321f013":"reddit.info()","5923c426":"reddit['timestamp'] = pd.to_datetime(reddit['timestamp'])\n\nday_of_the_week = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\ndays_order = list(day_of_the_week.values())\n\nreddit['Weekday'] = reddit['timestamp'].apply(lambda x : day_of_the_week[x.weekday()])\n\nxs = reddit['Weekday'].value_counts().index\nys = reddit['Weekday'].value_counts().values\n\nplt.figure(figsize=(14,6))\n\nsns.barplot(x=xs, y=ys, order=days_order)\n\nplt.title(\"No. of Posts vs Day of the Week\", fontsize=15)\n\nplt.xlabel(\"Days\", fontsize=15)\nplt.ylabel(\"No. of Posts\", fontsize=15)\n\nplt.show()","677053d3":"reddit_title = reddit['title'].dropna()\nreddit_body = reddit['body'].dropna()\n\n\ndef clean_text_date(text):\n    text = text.lower()\n\n    # Replacing Handlers with Empty String\n    text = re.sub('@[^\\s]+','',text)\n\n    # Replacing URLs with Empty String\n    text = re.sub(r\"http\\S+\", \"\",text)\n\n    # Remove all the special characters\n    text = ' '.join(re.findall(r'\\w+', text))\n\n    # Replacing Single Characters with Empty String\n    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n\n    # Removing Extra Spaces\n    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n    \n    return text\n\n    \n# Text Preprocessing\nreddit_title = reddit_title.apply(lambda x : clean_text_date(x))\nreddit_body = reddit_body.apply(lambda x : clean_text_date(x))","dc5da910":"title_length = [len(word_tokenize(text)) for text in reddit_title]\nbody_length = [len(word_tokenize(text)) for text in reddit_body]\n\nfig, (axis1, axis2) = plt.subplots(1,2, figsize=(16,6))\n\nsns.histplot(title_length, bins=50, kde=True, ax=axis1)\nsns.histplot(body_length, bins=40, kde=True, ax=axis2)\n\naxis1.set_xlabel(\"Length of Title\")\naxis2.set_xlabel(\"Length of Body\")\n\nplt.show()","74237723":"word_tokens = [word_tokenize(text) for text in reddit_title]\n\nword_cloud_string = \"\"\n\nfor word_list in word_tokens:\n    for word in word_list:\n        if word.lower() in english_words:\n            word_cloud_string += word + \" \"\n        \n# Updating some of the Words into Stopwords \ndescription_stopwords = set(STOPWORDS)\n\nmy_word_cloud = WordCloud(background_color='white',stopwords=description_stopwords).generate(word_cloud_string)\nplt.figure(figsize=(10,20))\nplt.imshow(my_word_cloud, interpolation='bilinear')\nplt.title(\"Word Cloud for Post Title\", fontsize=20)\nplt.axis('off')\nplt.show()","7720cda5":"word_tokens = [word_tokenize(text) for text in reddit_body]\n\nword_cloud_string = \"\"\n\nfor word_list in word_tokens:\n    for word in word_list:\n        if word.lower() in english_words:\n            word_cloud_string += word + \" \"\n        \n# Updating some of the Words into Stopwords \ndescription_stopwords = set(STOPWORDS)\n\nmy_word_cloud = WordCloud(background_color='white',stopwords=description_stopwords).generate(word_cloud_string)\nplt.figure(figsize=(10,20))\nplt.imshow(my_word_cloud, interpolation='bilinear')\nplt.title(\"Word Cloud for Post Body\", fontsize=20)\nplt.axis('off')\nplt.show()","74d6c1c0":"def get_sentiment(sia, text):\n    if sia.polarity_scores(text)[\"compound\"] > 0:\n        return \"Positive\"\n    elif sia.polarity_scores(text)[\"compound\"] < 0:\n        return \"Negative\"\n    else:\n        return \"Neutral\"\n\nsia = SentimentIntensityAnalyzer()    \n    \nreddit_title_df = reddit_title.to_frame(name='Title')\nreddit_title_df['Sentiment'] = reddit_title_df['Title'].apply(lambda x : get_sentiment(sia, x))\n\nreddit_body_df = reddit_body.to_frame(name='Body')\nreddit_body_df['Sentiment'] = reddit_body_df['Body'].apply(lambda x : get_sentiment(sia, x))","a1e7693b":"fig, (axis1, axis2) = plt.subplots(1,2, figsize=(12,5))\n\norder = ['Positive', 'Neutral', 'Negative']\n\nxs = reddit_title_df['Sentiment'].value_counts().index\nys = reddit_title_df['Sentiment'].value_counts().values\nsns.barplot(x=xs, y=ys, order=order, ax=axis1)\n\nxs = reddit_body_df['Sentiment'].value_counts().index\nys = reddit_body_df['Sentiment'].value_counts().values\nsns.barplot(x=xs, y=ys, order=order, ax=axis2)\n\naxis1.set_title(\"For Title\")\naxis2.set_title(\"For Body\")\n\nplt.show()","4c8ab6b8":"fig, (axis1, axis2, axis3) = plt.subplots(3, 1, figsize=(12,18))\n\naxes = [axis1, axis2, axis3]\nsentiments = ['Positive', 'Neutral', 'Negative']\n\nfor i in range(3):\n    word_tokens = [word_tokenize(text) for text in reddit_title_df[reddit_title_df['Sentiment'] == sentiments[i]]['Title']]\n    \n    word_cloud_string = \"\"\n    \n    for word_list in word_tokens:\n        for word in word_list:\n            if word.lower() in english_words:\n                word_cloud_string += word + \" \"\n    \n    description_stopwords = set(STOPWORDS)\n\n    my_word_cloud = WordCloud(background_color='white',stopwords=description_stopwords).generate(word_cloud_string)\n    axes[i].imshow(my_word_cloud, interpolation='bilinear')\n    axes[i].set_title(f\"Word Cloud for Post Title with {sentiments[i]} Sentiment\", fontsize=20)\n    axes[i].axis('off')\n\n\nplt.show()","7590cb93":"fig, (axis1, axis2, axis3) = plt.subplots(3, 1, figsize=(12,18))\n\naxes = [axis1, axis2, axis3]\nsentiments = ['Positive', 'Neutral', 'Negative']\n\nfor i in range(3):\n    word_tokens = [word_tokenize(text) for text in reddit_body_df[reddit_body_df['Sentiment'] == sentiments[i]]['Body']]\n    \n    word_cloud_string = \"\"\n    \n    for word_list in word_tokens:\n        for word in word_list:\n            if word.lower() in english_words:\n                word_cloud_string += word + \" \"\n    \n    description_stopwords = set(STOPWORDS)\n\n    my_word_cloud = WordCloud(background_color='white',stopwords=description_stopwords).generate(word_cloud_string)\n    axes[i].imshow(my_word_cloud, interpolation='bilinear')\n    axes[i].set_title(f\"Word Cloud for Post Body with {sentiments[i]} Sentiment\", fontsize=20)\n    axes[i].axis('off')\n\n\nplt.show()","9fa526cf":"**Importing the necessary Libraries**","e0156ed0":"1. Title","88e3f7f7":"**Importing the Data using pandas read_csv() and we will drop the columns id, url and created as I am not gonna be using these columns for analysis**","c8de6315":"1. Calling get_sentiment() on each Title and Body\n2. Creating a new Column called 'Sentiment' and storing the respective sentiment\n3. Using the Sentiment column to plot various graphs.","5f622f97":"---\n\n# Thank You","ee1f1efd":"# Reddit WallStreetBets Posts","aa5dc391":"---\n2. Body","ca879aa0":"---\n**Sentiment Analysis Using SentimentIntensityAnalyzer from nltk.sentiment**","f8008204":"---\n**WordCloud for Title and Body of the Post**","e3aadfa4":"**Plotting a Bar Graph for Sentiment Counts**","ebabee7f":"---\n**Plotting a Histograms to see the Length(No. of words) Distribution of Title and Body**\n\n1. We use the word_tokenize from nltk on each Title and Body to get a list of Lengths.\n2. Then using the Seaborn histplot we plot a histogram. ","a49ddffc":"---\n**Which Day of the week we have most Post**\n\n1. We are converting the 'timestamp' to a datetime object\n2. Using the weekday() from the datetime object we get the Day of the Week\n3. We will plot a barplot using the Seaborn and set the order of days accordingly","12f9fff8":"From the above Barplot we can clearly see that the there were huge number of posts on Friday.","b4a9f321":"---\n**Focusing more on the 'title' and 'body' column of the data**\n\n1. We will preprocess the Text Data in the Title and Body using the clean_text_date().\n2. The function will remove Handlers, URLs, Special Characters, Single Characters and Extra Spaces","799a89c0":"---\n**WordCloud for Different Sentiment**","9160dd7f":"From the above information on the the DataFrame we can clearly see that we only have NaN in the case of body and also around half the cells in the Body Column are NaN.","9dfef460":"**Calling head() and info() in the DataFrame**"}}