{"cell_type":{"4a03fc24":"code","ffd9579e":"code","d35e4bdf":"code","b2037b5a":"code","315bf2ea":"code","21c59eae":"code","2e04feb8":"code","bcefd841":"code","051327f4":"code","392f5cc3":"code","3c8d4c2c":"code","20a7a93d":"code","a2f7cd2f":"code","4dd2a6ff":"code","479441d0":"code","a813544f":"code","5cb0d258":"code","0be862c7":"code","2bec23e4":"code","22a83372":"code","9104df1d":"code","16b9feed":"code","e096c242":"code","dfa9f857":"code","57ed74ea":"code","e004ab11":"code","c0bb3d99":"code","e39a53af":"code","b5524e7a":"code","48b0eb46":"code","463c2099":"code","8a775c77":"code","badf026a":"code","f3fc3a06":"code","8255487c":"code","ef162114":"code","bca44684":"code","2009826d":"code","03857fef":"code","e2dad40a":"code","f316d3ae":"code","5cf5ce8d":"code","a75e064a":"code","32eab59a":"code","b6f6017f":"code","e240bc74":"code","2d9cc9a3":"code","de2440df":"code","ed0c0013":"code","cf8237d7":"code","837e5f07":"code","f66f416b":"code","40ede8dd":"code","a22ccfee":"code","81fcb203":"code","be961ecd":"code","14906057":"code","7b03112e":"code","d3ad4cd6":"code","38cb708d":"markdown","46ec3ca0":"markdown","7fa0b41c":"markdown","d3998bdd":"markdown","4d2a945d":"markdown","715f07f7":"markdown","d963de20":"markdown","ddb8e6bb":"markdown","430e6f5c":"markdown","0e44ce82":"markdown","c5305d74":"markdown","4e66be38":"markdown","10f82704":"markdown","80a01f1b":"markdown","49850604":"markdown","4bfe126c":"markdown","e60e31a3":"markdown","e4ed5796":"markdown","311fd548":"markdown","08b02d0b":"markdown","cb745427":"markdown","d8d7ead1":"markdown","d4cf9d8b":"markdown","4b1a5e28":"markdown","84796385":"markdown","6087e2b9":"markdown","9b3e5a38":"markdown","0d0b55a3":"markdown","c9a756af":"markdown","ca2b22f2":"markdown","ecbad8c5":"markdown","19a0bf75":"markdown","f123e108":"markdown","2db2b1ee":"markdown","a068ca8a":"markdown"},"source":{"4a03fc24":"from nltk.corpus import stopwords\nimport string, re\nfrom collections import Counter\nimport wordcloud\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport os\n\nreview_dataset_path=\"\/kaggle\/input\/movie-review\/movie_reviews\/movie_reviews\"\nprint(os.listdir(review_dataset_path))","ffd9579e":"#Positive and negative reviews folder paths \npos_review_folder_path=review_dataset_path+\"\/\"+\"pos\"\nneg_review_folder_path=review_dataset_path+\"\/\"+\"neg\"","d35e4bdf":"#Positive and negative file names\npos_review_file_names=os.listdir(pos_review_folder_path)\nneg_review_file_names=os.listdir(neg_review_folder_path)","b2037b5a":"def load_text_from_textfile(path):\n    file=open(path,\"r\")\n    review=file.read()\n    file.close()\n    \n    return review\n\ndef load_review_from_textfile(path):\n    return load_text_from_textfile(path)\n","315bf2ea":"def get_data_target(folder_path, file_names, review_type):\n    data=list()\n    target =list()\n    for file_name in file_names:\n        full_path = folder_path + \"\/\" + file_name\n        review =load_review_from_textfile(path=full_path)\n        data.append(review)\n        target.append(review_type)\n    return data, target","21c59eae":"pos_data, pos_target=get_data_target(folder_path=pos_review_folder_path,\n               file_names=pos_review_file_names,\n               review_type=\"positive\")\nprint(\"Positive data ve target builded...\")\nprint(\"positive data length:\",len(pos_data))\nprint(\"positive target length:\",len(pos_target))","2e04feb8":"neg_data, neg_target = get_data_target(folder_path = neg_review_folder_path,\n                                      file_names= neg_review_file_names,\n                                      review_type=\"negative\")\nprint(\"Negative data ve target builded..\")\nprint(\"negative data length :\",len(neg_data))\nprint(\"negative target length :\",len(neg_target))","bcefd841":"data = pos_data + neg_data\ntarget_ = pos_target + neg_target\nprint(\"Positive and Negative sets concatenated\")\nprint(\"data length :\",len(data))\nprint(\"target length :\",len(target_))","051327f4":"le = LabelEncoder()\nle.fit(target_)\ntarget = le.transform(target_)\nprint(\"Target labels transformed to number...\")","392f5cc3":"print(le.inverse_transform([0,0,0,1,1,1]))","3c8d4c2c":"X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, stratify=target, random_state=24)\nprint(\"Dataset splited into train and test parts...\")\nprint(\"train data length  :\",len(X_train))\nprint(\"train target length:\",len(y_train))\nprint()\nprint(\"test data length  :\",len(X_test))\nprint(\"test target length:\",len(y_test))","20a7a93d":"import seaborn as sns\nfig, axarr = plt.subplots(nrows=1, ncols=2, figsize=(8,4),sharey=True)\naxarr[0].set_title(\"Number of samples in train\")\nsns.countplot(x=y_train, ax=axarr[0])\naxarr[1].set_title(\"Number of samples in test\")\nsns.countplot(x=y_test, ax=axarr[1])\nplt.show()","a2f7cd2f":"class MakeString:\n    def process(self, text):\n        return str(text)","4dd2a6ff":"class ReplaceBy:\n    def __init__(self, replace_by):\n        #replace_by is a tuple contains pairs of replace and by characters.\n        self.replace_by = replace_by\n    def process(self, text):\n        for replace, by in replace_by:\n            text = text.replace(replace, by)\n        return text","479441d0":"class LowerText:\n    def process(self, text):\n        return text.lower()","a813544f":"class ReduceTextLength:\n    def __init__(self, limited_text_length):\n        self.limited_text_length = limited_text_length\n    def process(self, text):\n        return text[:self.limited_text_length]","5cb0d258":"class VectorizeText:\n    def __init__(self):\n        pass\n    def process(self, text):\n        return text.split()","0be862c7":"class FilterPunctuation:\n    def __init__(self):\n        print(\"Punctuation Filter created...\")\n    def process(self, words_vector):\n        reg_exp_filter_rule=re.compile(\"[%s]\"%re.escape(string.punctuation))\n        words_vector=[reg_exp_filter_rule.sub(\"\", word) for word in words_vector]\n        return words_vector\n","2bec23e4":"class FilterNonalpha:\n    def __init__(self):\n        print(\"Nonalpha Filter created...\")\n    def process(self, words_vector):\n        words_vector=[word for word in words_vector if word.isalpha()]\n        return words_vector","22a83372":"class FilterStopWord:\n    def __init__(self, language):\n        self.language=language\n        print(\"Stopwords Filter created...\")\n    def process(self, words_vector):\n        stop_words=set(stopwords.words(self.language))\n        words_vector=[word for word in words_vector if not word in stop_words]\n        return words_vector","9104df1d":"class FilterShortWord:\n    def __init__(self, min_length):\n        self.min_length=min_length\n        print(\"Short Words Filter created...\")\n    def process(self, words_vector):\n        words_vector=[word for word in words_vector if len(word)>=self.min_length]\n        return words_vector        ","16b9feed":"class TextProcessor:\n    def __init__(self, processor_list):\n        self.processor_list = processor_list\n    def process(self, text):\n        for processor in self.processor_list:\n            text = processor.process(text)\n        return text","e096c242":"text_len = np.vectorize(len)\n\ntext_lengths = text_len(X_train)","dfa9f857":"mean_review_length =int(text_lengths.mean())\nprint(\"Mean length of reviews   :\",mean_review_length)    \nprint(\"Minimum length of reviews:\",text_lengths.min())\nprint(\"Maximum length of reviews:\",text_lengths.max())","57ed74ea":"sns.distplot(a=text_lengths)","e004ab11":"makeString = MakeString()\n\nreplace_by = [(\".\",\" \"), (\"?\",\" \"), (\",\",\" \"), (\"!\",\" \"),(\":\",\" \"),(\";\",\" \")]\nreplaceBy =ReplaceBy(replace_by=replace_by)\n\nlowerText = LowerText()\n\nFACTOR=8\nreduceTextLength = ReduceTextLength(limited_text_length=mean_review_length*FACTOR)\n\nvectorizeText = VectorizeText()\nfilterPunctuation = FilterPunctuation()\nfilterNonalpha = FilterNonalpha()\nfilterStopWord = FilterStopWord(language = \"english\")\n\nmin_length = 2\nfilterShortWord = FilterShortWord(min_length=min_length)\nprocessor_list_1 = [makeString,\n                      replaceBy,\n                      lowerText,\n                      reduceTextLength,\n                      vectorizeText,\n                      filterPunctuation,\n                      filterNonalpha,\n                      filterStopWord,\n                      filterShortWord]","c0bb3d99":"textProcessor1 = TextProcessor(processor_list=processor_list_1)","e39a53af":"random_number=np.random.randint(0,len(X_train))\nprint(\"Original Review:\\n\",X_train[random_number][:500])\nprint(\"=\"*100)\nprint(\"Processed Review:\\n\",textProcessor1.process(text=X_train[random_number][:500]))","b5524e7a":"class VocabularyHelper:\n    def __init__(self, textProcessor):\n        self.textProcessor=textProcessor\n        self.vocabulary=Counter()\n    def update(self, text):\n        words_vector=self.textProcessor.process(text=text)\n        self.vocabulary.update(words_vector)\n    def get_vocabulary(self):\n        return self.vocabulary","48b0eb46":" vocabularyHelper=VocabularyHelper(textProcessor=textProcessor1)\nprint(\"VocabularyHelper created...\")","463c2099":"for text in X_train:\n    vocabularyHelper.update(text)\nvocabulary = vocabularyHelper.get_vocabulary()\nprint(\"Vocabulary filled...\")","8a775c77":"print(\"Length of vocabulary:\",len(vocabulary))\nn=10\nprint(\"{} most frequented words in vocabulary:{}\".format(n, vocabulary.most_common(n)))","badf026a":"print(\"{} least frequented words in vocabulary:{}\".format(n, vocabulary.most_common()[:-n-1:-1]))","f3fc3a06":"vocabulary_list = \" \".join([key for key, _ in vocabulary.most_common()])\nplt.figure(figsize=(15, 35))\nwordcloud_image = wordcloud.WordCloud(width = 1000, height = 1000, \n                background_color ='white', \n                #stopwords = stopwords, \n                min_font_size = 10).generate(vocabulary_list)\n\n\nplt.xticks([])\nplt.yticks([])\nplt.imshow(wordcloud_image)","8255487c":"min_occurence=2\nvocabulary = Counter({key:value for key, value in vocabulary.items() if value>min_occurence})","ef162114":"print(\"{} least frequented words in vocabulary:{}\".format(n, vocabulary.most_common()[:-n-1:-1]))","bca44684":"print(\"Length of vocabulary after removing words occurenced less than {} times:{}\".format(min_occurence, len(vocabulary)))","2009826d":"class FilterNotInVocabulary:\n    def __init__(self, vocabulary):\n        self.vocabulary = vocabulary\n    def process(self, words_vector):\n        words_vector = [word for word in words_vector if word in self.vocabulary]\n        return words_vector","03857fef":"class JoinWithSpace:\n    def __init__(self):\n        pass\n    def process(self, words_vector):\n        return \" \".join(words_vector)","e2dad40a":"filterNotInVocabulary = FilterNotInVocabulary(vocabulary = vocabulary)\njoinWithSpace = JoinWithSpace()\nprocessor_list_2 = [makeString,\n                    replaceBy,\n                    lowerText,\n                    reduceTextLength,\n                    vectorizeText,\n                    filterPunctuation,\n                    filterNonalpha,\n                    filterStopWord,\n                    filterShortWord,\n                    filterNotInVocabulary,\n                    joinWithSpace\n                   ]\ntextProcessor2=TextProcessor(processor_list = processor_list_2)","f316d3ae":"review = X_train[np.random.randint(0,len(X_train))]\nprint(\"Original Text:\\n\",review[:500])\nprocessed_review = textProcessor2.process(review[:500])\nprint(\"=\"*100)\nprint(\"Processed Text:\\n\",processed_review)","5cf5ce8d":"X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, stratify=target, random_state=24)\nprint(\"Dataset splited into train and test parts...\")\nprint(\"train data length  :\",len(X_train))\nprint(\"train target length:\",len(y_train))\nprint()\nprint(\"test data length  :\",len(X_test))\nprint(\"test target length:\",len(y_test))","a75e064a":"def  process_text(texts, textProcessor):\n    processed_texts=list()\n    for text in texts:\n        processed_text = textProcessor.process(text)\n        processed_texts.append(processed_text)\n    return processed_texts","32eab59a":"X_train_processed = process_text(texts=X_train, textProcessor=textProcessor2)\nprint(\"X_train processed...\")","b6f6017f":"X_test_processed = process_text(texts=X_test, textProcessor=textProcessor2)\nprint(\"X_test processed...\")","e240bc74":"from keras.preprocessing.text import Tokenizer\ndef create_and_train_tokenizer(texts):\n    tokenizer=Tokenizer()\n    tokenizer.fit_on_texts(texts)\n    return tokenizer","2d9cc9a3":"from keras.preprocessing.sequence import pad_sequences\ndef encode_reviews(tokenizer, max_length, docs):\n    encoded=tokenizer.texts_to_sequences(docs)\n    \n    padded=pad_sequences(encoded, maxlen=max_length, padding=\"post\")\n    \n    return padded","de2440df":"tokenizer=create_and_train_tokenizer(texts = X_train)\nvocab_size=len(tokenizer.word_index) + 1\nprint(\"Vocabulary size:\", vocab_size)","ed0c0013":"max_length=max([len(row.split()) for row in X_train])\nprint(\"Maximum length:\",max_length)","cf8237d7":"X_train_encoded=encode_reviews(tokenizer, max_length, X_train_processed)\nprint(\"X_train encoded...\")","837e5f07":"X_test_encoded = encode_reviews(tokenizer, max_length, X_test_processed)\nprint(\"X_test encoded...\")","f66f416b":"from keras import layers, models\ndef create_embedding_model(vocab_size, max_length):\n    model=models.Sequential()\n    model.add(layers.Embedding(vocab_size, 100, input_length=max_length))\n    model.add(layers.Conv1D(32, 8, activation=\"relu\"))\n    model.add(layers.MaxPooling1D(2))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(10, activation=\"relu\"))\n    model.add(layers.Dense(1,  activation=\"sigmoid\"))   \n    return model","40ede8dd":"embedding_model = create_embedding_model(vocab_size=vocab_size, max_length=max_length)\nembedding_model.summary()","a22ccfee":"embedding_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","81fcb203":"from keras.callbacks import EarlyStopping\nearlyStopping = EarlyStopping(monitor=\"val_accuracy\", patience=1)\nmodelHistory = embedding_model.fit(X_train_encoded, \n                                   y_train, \n                                   validation_data=(X_test_encoded, y_test),\n                                   epochs=10, \n                                   callbacks=[earlyStopping])\nprint(\"Model trained...\")","be961ecd":"_, acc = embedding_model.evaluate(X_train_encoded, y_train, verbose=0)\nprint(\"Train accuracy:{:.2f}\".format(acc*100))","14906057":"_,acc= embedding_model.evaluate(X_test_encoded, y_test, verbose=0)\nprint(\"Test accuracy:{:.2f}\".format(acc*100))","7b03112e":"embedding_model.save(\"embedding_model.h5\")\nprint(os.listdir(\"\/kaggle\/working\"))","d3ad4cd6":"loaded_model=models.load_model(\"\/kaggle\/working\/embedding_model.h5\")\nprint(\"Saved model loaded...\")","38cb708d":"[Go to Content Menu](#0.)\n\n# <a class=\"anchor\" id=\"3.\">3. Loading The Dataset<\/a>","46ec3ca0":"[Go to Content Menu](#0.)\n\n## <a class=\"anchor\" id=\"6.2.\">6.2. Plot The Model <\/a>","7fa0b41c":"[Go to Content Menu](#0.)\n\n## <a class=\"anchor\" id=\"5.3.\">5.3. Train Keras Tokenizer on Train Dataset <\/a>","d3998bdd":"[Go to Content Menu](#0.)\n\n### <a class=\"anchor\" id=\"4.2.4.\">4.2.4. Filter Stop Words<\/a>","4d2a945d":"[Go to Contents](#0.)","715f07f7":"[Go to Content Menu](#0.)\n\n## <a class=\"anchor\" id=\"6.1.\">6.1. Create Keras Model: Embedding + Conv1D <\/a>","d963de20":"[Go to Content Menu](#0.)\n\n# <a class=\"anchor\" id=\"6.\">6. Using Keras Model <\/a>","ddb8e6bb":"[Go to Content Menu](#0.)\n\n## <a class=\"anchor\" id=\"5.2.\">5.2. Process Train and Test Datasets<\/a>","430e6f5c":"[Go to Content Menu](#0.)\n\n### <a class=\"anchor\" id=\"4.2.2.\">4.2.2. Filter Punctuation<\/a>","0e44ce82":"[Go to Content Menu](#0.)\n\n### <a class=\"anchor\" id=\"4.4.2.\">4.4.2 Filter Not In Vocabulary<\/a>","c5305d74":"[Go to Content Menu](#0.)\n\n### <a class=\"anchor\" id=\"4.4.1.\">4.4.1 Reduce Vocabulary Length<\/a>","4e66be38":"[Go to Content Menu](#0.)\n\n### <a class=\"anchor\" id=\"4.2.3.\">4.2.3. Filter Nonalpha<\/a>","10f82704":"[Go to Content Menu](#0.)\n\n### <a class=\"anchor\" id=\"4.1.3.\">4.1.3. Lower Text<\/a>","80a01f1b":"[Go to Content Menu](#0.)\n\n# <a class=\"anchor\" id=\"4.\">4. Processing Reviews<\/a>\n...","49850604":"[Go to Content Menu](#0.)\n\n### <a class=\"anchor\" id=\"4.2.5.\">4.2.5. Filter Short Words<\/a>","4bfe126c":"[Go to Content Menu](#0.)\n\n### <a class=\"anchor\" id=\"4.4.3.\">4.4.3. Join With Space<\/a>","e60e31a3":"# <a class=\"anchor\" id=0.>Contents<\/a>\n* [1. Introduction](#1.)\n* [2. About Dataset](#2.)\n* [3. Loading Dataset](#3.)\n* [4. Processing Reviews](#4.)\n* * [4.1. Text Processing](#4.1.)\n* * * [4.1.1. Make String](#4.1.1.)\n* * * [4.1.2. Replace By](#4.1.2.)\n* * * [4.1.3. Lower String](#4.1.3.)\n* * * [4.1.4. Reduce Text Length](#4.1.4.)\n* * [4.2. Word Vector Processing](#4.2.)\n* * * [4.2.1 Vectorize Text](#4.2.1.)\n* * * [4.2.2 Filter Punctuation](#4.2.2.)\n* * * [4.2.3 Filter Nonalpha](#4.2.3.)\n* * * [4.2.4 Filter Stop Words](#4.2.4.)\n* * * [4.2.5 Filter Short Words](#4.2.5.)\n* * [4.3. Text Processor](#4.3.)\n* * [4.4. Create Vocabulary](#4.4.)\n* * * [4.4.1. Reduce Vocabulary Length](#4.4.1.)\n* * * [4.4.2. Filter Not In Vocabulary](#4.4.2.)\n* * * [4.4.3. Join With Space](#4.4.3.)\n* [5. Building Dataset](#5.)\n* * [5.1. Spliting Dataset into Train and Test Parts](#5.1.)\n* * [5.2. Process Train and Test Dataset](#5.2.)\n* * [5.3. Train Keras Tokenizer on Train Dataset](#5.3.)\n* * [5.4. Transform Train Dataset with Trained Tokenizer](#5.4.)\n* * [5.5. Transform Test Dataset with Trained Tokenizer](#5.5.)\n* [6. Using Keras Model](#6.)\n* * [6.1. Create Keras Model: Embedding + Conv1D](#6.1.)\n* * [6.2. Plot The Model](#6.2.)\n* * [6.3. Compile The Model](#6.3.)\n* * [6.4. Train The Model](#6.4.)\n* * [6.5. Evaluate The Model](#6.5.)\n* * [6.6. Save The Model](#6.6.)\n* [7. Make Prediction with Loaded Model](#7.) -- will be added soon\n* * [7.1. Load The Model](#7.1.)\n* * [7.2. Make Prediction](#7.2.)","e4ed5796":"[Go to Content Menu](#0.)\n\n### <a class=\"anchor\" id=\"4.2.1.\">4.2.1. Vectorize Text<\/a>","311fd548":"[Go to Content Menu](#0.)\n\n# <a class=\"anchor\" id=\"5.\">5. Building Dataset<\/a>","08b02d0b":"[Go to Content Menu](#0.)\n\n## <a class=\"anchor\" id=\"6.3.\">6.3. Compile The  Model <\/a>","cb745427":"[Go to Content Menu](#0.)\n\n### <a class=\"anchor\" id=\"4.1.1.\">4.1.1. Make String<\/a>","d8d7ead1":"[Go to Content Menu](#0.)\n\n## <a class=\"anchor\" id=\"6.4.\">6.4. Train The Model <\/a>","d4cf9d8b":"[Go to Content Menu](#0.)\n\n### <a class=\"anchor\" id=\"4.1.2.\">4.1.2. Replace By<\/a>","4b1a5e28":"# <a class=\"anchor\" id=\"1.\">1. Introduction<\/a>\n\nNatural Language Processing covers studies aimed at the automatic evaluation of texts and speeches. Natural language processing has been around for more than 50 years and has achieved success that can turn into commercial value through deep learning models.\n\nNLP aims at engineering-oriented and experimental use of statistical methods and makes inferences from speech and texts which means language data by using statistical methods. NLP can take the language data as input and also produce the output of the language data as well as the estimation output.\n\nDeep learning is a subfield of machine learning and the name given to the multi-layered architectures of artificial neural studies that attempt to model the workings of the human brain. The advantages of deep learning models for NLP studies can be simply divided into two; better performance and requiring less linguistics knowledge.\n\nText Classification, which defines the analysis of emotions from text data such as tweets, comments, articles, reviews, and the separation of emails from spam and non-spam, is an important field of study of natural language processing. Deep learning methods have come to the forefront in this field with significant achievements in the shortening data sets used for text classification.\n\nIn brief, the work done in this Kernel is as follows;\n* Movie comments in text files are loaded and data set is created\n* The data set is divided into training and test sets.\n* Processors are defined for processing data sets\n* Using the processor, first the training dataset and then the test data set were processed\n* Embedding + Conv1D deep learning model with Keras\n* Compiling, training, evaluation and saving of the model.\n* The saved model is re-loaded and used for classification on new reviews.\n\nIn order to understand, reuse and extend the code, text processor are defined and used with Object Oriented Programming approach.","84796385":"[Go to Content Menu](#0.)\n\n### <a class=\"anchor\" id=\"4.1.4.\">4.1.4. Reduce Text Length<\/a>","6087e2b9":"[Go to Content Menu](#0.)\n\n## <a class=\"anchor\" id=\"4.3.\">4.3. Text Processor<\/a>","9b3e5a38":"[Go to Content Menu](#0.)\n\n## <a class=\"anchor\" id=\"4.2.\">4.2. Word Vector Processing<\/a>","0d0b55a3":"[Go to Content Menu](#0.)\n\n## <a class=\"anchor\" id=\"6.5.\">6.5. Evaluate The  Model <\/a>","c9a756af":"[Go to Content Menu](#0.)\n\n## <a class=\"anchor\" id=\"4.4.\">4.4. Create Vocabulary<\/a>","ca2b22f2":"[Go to Content Menu](#0.)\n\n## <a class=\"anchor\" id=\"5.5.\">5.5. Transform Test Dataset with Trained Tokenizer <\/a>","ecbad8c5":"[Go to Content Menu](#0.)\n\n## <a class=\"anchor\" id=\"4.1.\">4.1. Text Processing<\/a>","19a0bf75":"[Go to Content Menu](#0.)\n\n## <a class=\"anchor\" id=\"5.4.\">5.4. Transform Trained Dataset with Trained Tokenizer <\/a>","f123e108":"[Go to Content Menu](#0.)\n\n## <a class=\"anchor\" id=\"6.6.\">6.6. Save The  Model <\/a>","2db2b1ee":"[Go to Content Menu](#0.)\n\n# <a class=\"anchor\" id=\"2.\">2. About Dataset<\/a>","a068ca8a":"[Go to Content Menu](#0.)\n\n## <a class=\"anchor\" id=\"5.1.\">5.1. Spliting Dataset into Train and Test Parts<\/a>"}}