{"cell_type":{"19e41abd":"code","a1a5a019":"code","1344ef79":"code","6754b776":"code","d7996b47":"code","5bd9fd98":"code","27b85525":"code","765b22ac":"code","e357d12f":"code","cca12223":"code","eed08a04":"code","59db6fb7":"code","3be409d6":"code","364f5d07":"code","297f1ccd":"code","72f8edc6":"code","8d779c38":"code","9b43ddb0":"markdown","ad707372":"markdown","76c61ed6":"markdown","482704aa":"markdown","6dd0282d":"markdown","70ae5da4":"markdown","3aff242a":"markdown"},"source":{"19e41abd":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns',0)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport gc\nfrom tqdm import tqdm_notebook\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom scipy.optimize import differential_evolution\n\nseed = 42","a1a5a019":"def parse_month(df,cols,newcol=False):\n    for col in cols:\n        if newcol:\n            cn = col+'_parsed' \n        else: cn = col\n        df[cn] = pd.to_datetime(df[col],format='%Y%m')\n    return df\n","1344ef79":"file_path = '..\/input\/interbank-internacional-2019\/'\ninitr = parse_month(pd.read_csv(file_path+'ib_base_inicial_train\/ib_base_inicial_train.csv'),['codmes'],newcol=True)\ninite = parse_month(pd.read_csv(file_path+'ib_base_inicial_test\/ib_base_inicial_test.csv'),['codmes'],newcol=True)\nbdigit = pd.read_csv(file_path+'ib_base_digital\/ib_base_digital.csv',parse_dates=['codday'])\nbcamp = parse_month(pd.read_csv(file_path+'ib_base_campanias\/ib_base_campanias.csv'),['codmes'],newcol=True)\nbsunat = pd.read_csv(file_path+'ib_base_sunat\/ib_base_sunat.csv')\nbreniec = pd.read_csv(file_path+'ib_base_reniec\/ib_base_reniec.csv')\nbasercc = parse_month(pd.read_csv(file_path+'ib_base_rcc\/ib_base_rcc.csv'),['codmes'],newcol=True)\nbvehic = pd.read_csv(file_path+'ib_base_vehicular\/ib_base_vehicular.csv')\nsubmit = pd.read_csv(file_path+'sampleSubmission\/sampleSubmission.csv')","6754b776":"inifull = pd.concat([initr,inite],ignore_index=True,sort=False)\ninifull.shape","d7996b47":"def get_sunat_features(df, train, sunat):\n    agg = {'codtarget' : ['count', 'mean'],\n           'margen' : ['sum', 'mean', 'median', 'std', 'min', 'max']}\n    # Inicializar las columnas\n    for c in agg.keys():\n        for a in agg[c]:\n            sunat[c+'_'+a] = np.nan\n    # Calculo\n    for g in tqdm_notebook(sunat['activ_econo'].unique()):\n        for c in agg.keys():\n            encs = train[train['id_persona'].isin(sunat[sunat['activ_econo']==g]['id_persona'].values)][c].agg(agg[c])\n            for i,e in zip(encs.index,encs):\n                sunat[c+'_'+i] = np.where(sunat['activ_econo']==g,e,bsunat[c+'_'+i])\n\n    agg2 = {x:['mean'] for x in bsunat.columns[3:]}\n    agg2['meses_alta'] = ['mean','max']\n    sunatp = sunat.groupby('id_persona').agg(agg2).reset_index()\n    sunatp.columns = ['id_persona']+[a+'_'+b for a,b in sunatp.columns[1:]]\n\n    return df.merge(sunatp,on='id_persona',how='left')\n\ndef get_reniec_features(df,reniec):\n    reniec['soc_var1'] = (reniec['soc_var1']*53).astype(int) # Valor real?\n    return df.merge(reniec,on='id_persona',how='left')\n\ndef get_vehicular_features_single(mrc,km,old,vehic):\n    q1 = vehic.query('marca==\"{}\" and veh_var1=={}'.format(mrc,km))\n    q2 = vehic.query('veh_var1=={}'.format(km))\n    return (old > q1['veh_var2']).mean(), (old > q2['veh_var2']).mean()\n\ndef get_vehicular_features(df,vehic):\n    \n    # No es la forma mas r\u00e1pida, pero hace el trabajo\n    usg, usgf = [],[]\n    vehic['veh_var1'] = vehic['veh_var1'].fillna(-999)\n    for mrc,km,old in zip(vehic['marca'],vehic['veh_var1'],vehic['veh_var2']):\n        a,b = get_vehicular_features_single(mrc,km,old,vehic)\n        usg.append(a)\n        usgf.append(b)\n    vehic['rel_use'] = usg\n    vehic['rel_use_full'] = usgf\n    \n    vehic['marca_count'] = np.nan\n    for m in tqdm_notebook(vehic['marca'].unique()):\n        vehic['marca_count'] = np.where(vehic['marca']==m,sum(vehic['marca']==m),vehic['marca_count'])\n        \n    vehic['veh_var1'] = vehic['veh_var1'].replace(-999,np.nan)\n    \n    agg = {'id_persona' : ['count'],\n           'veh_var1' : ['mean'],\n           'veh_var2' : ['mean'],\n           'rel_use' : ['median'],\n           'rel_use_full' : ['mean'],\n           'marca_count': ['mean']}\n    \n    vehicp = vehic.groupby('id_persona').agg(agg).reset_index()\n    vehicp.columns = ['id_persona']+[a+'_'+b for a,b in vehicp.columns[1:]]\n\n    return df.merge(vehicp,on='id_persona',how='left')\n\ndef get_campaign_features(df, camp):\n    camp['canal_asignado'] = camp['canal_asignado'].str.lower().replace({\n        'red_de_tiendas':'red de tiendas','bolsa(ex-e)':'bolsa','bolsa(ex)':'bolsa','bolsa(ex-p)':'bolsa','gt(ex)':'gt','tlv cd':'tlv','tlv bd':'tlv',\n        'bolsa(e)':'bolsa','ebp (lima)':'ebp','bolsa(ex-t)':'bolsa','bolsa(t)':'bolsa','express-abp':'abp','ebp_lima':'ebp','ebp lima':'ebp',\n        'bolsa(p)':'bolsa','cav':'otros','tlv(ex)':'tlv','ebp provincia':'ebp','ebp venta':'ebp','tlv segmentos':'tlv','gt_cav':'gt','express abp':'abp',\n        'cartera':'carteras','cartera_pro':'carteras','s1p':'otros','somos_uno_provincia':'otros','ac_prov':'otros','er':'otros'})\n    \n    camp['producto'] = camp['producto'].str.lower().replace({\n        'combos tc+cuenta+app':'combos','combos tc + pa':'combos','combos tc+pa':'combos','retenci\u00f3n':'retencion tc',\n        'cuenta millonaria supertasa':'cuenta millonaria','membres\u00eda':'membresia','seguro vida retorno':'seguro salud','seguro dental':'seguro salud',\n        'seguro oncosalud':'seguro salud','plazo':'dep\u00f3sito a plazo','dep\u00f3sito a plazo jubilacion':'dep\u00f3sito a plazo','cartera abp':'otros',\n        'dep\u00f3sito a plazo renovacion':'dep\u00f3sito a plazo','combos cuenta+app':'combos','d\u00e9posito a plazo renovacion':'dep\u00f3sito a plazo','televentas':'otros',\n        'combos cuenta sueldo + app':'combos','hipotecario':'cr\u00e9dito hipotecario','seguro renta hospitalaria':'seguro salud','extracash ataque':'extracash',\n        'upgrade':'incremento linea','upgrade tc':'incremento linea'})\n    \n    cc = []\n    for mes in tqdm_notebook(df['codmes'].unique()):\n        partial = df[df['codmes']==mes][['codmes','id_persona']]\n        q = camp[(camp['id_persona'].isin(partial['id_persona'].unique())) & (camp['codmes']<mes)]\n        q1 = q.groupby(['id_persona','producto'])['id_persona'].count().unstack().reset_index().fillna(0)\n        q2 = q.groupby(['id_persona','canal_asignado'])['id_persona'].count().unstack().reset_index().fillna(0)\n        cc.append(partial.merge(q1,on='id_persona',how='left').merge(q2,on='id_persona',how='left'))\n    cc = pd.concat(cc,sort=False)\n    \n    return df.merge(cc,on=['codmes','id_persona'],how='left')\n\ndef replace_low_classes(lst,thresh,string):\n    c2r = lst.value_counts()[lst.value_counts()<thresh].index\n    return lst.replace({x:string for x in c2r})\n\ndef get_rcc_features(df,rcc):\n    rcc['producto'] = rcc['producto'].replace({'TARJETA_EMP SIN DEFINIR':'TARJETA_EMP','TARJETA_EMP EFECTIVO':'TARJETA_EMP',\n                             'TARJETA_EMP OTROS CONCEPTOS':'TARJETA_EMP','TARJETA_EMP COMPRA':'TARJETA_EMP',\n                             'CREDITO CASTIGADOS SIENDO AMORTIZADOS':'CREDITO CASTIGOS'})\n    rcc['producto'] = replace_low_classes(rcc['producto'],thresh=500,string='OTROS').values\n    \n    dd = []\n    for mes in tqdm_notebook(df['codmes'].unique()):\n        partial = df[df['codmes']==mes][['codmes','id_persona']]\n        q = rcc[(rcc['id_persona'].isin(partial['id_persona'].unique())) & (rcc['codmes']<mes)]\n        q1 = q.groupby(['id_persona','producto'])['mto_saldo'].sum().unstack().add_prefix('sum_saldo_prod_').reset_index().fillna(0)\n        q2 = q.groupby(['id_persona','cod_banco'])['mto_saldo'].sum().unstack().add_prefix('sum_saldo_bco_').reset_index().fillna(0)\n        q3 = q.groupby(['id_persona','producto'])['rango_mora'].mean().unstack().add_prefix('mean_mora_prod_').reset_index().fillna(0)\n        q4 = q.groupby(['id_persona','cod_banco'])['rango_mora'].mean().unstack().add_prefix('mean_mora_bcp_').reset_index().fillna(0)\n        dd.append(partial.merge(q1,on='id_persona',how='left').merge(q2,on='id_persona',how='left').merge(q3,on='id_persona',how='left').merge(q4,on='id_persona',how='left'))\n    dd = pd.concat(dd,sort=False)\n    \n    return df.merge(dd,on=['codmes','id_persona'],how='left')\n\ndef get_digital_features(df, digit):\n    ee = []\n    for mes in tqdm_notebook(df['codmes_parsed'].unique()):\n        partial = df[df['codmes_parsed']==mes][['codmes_parsed','id_persona']]\n        q = digit[(digit['id_persona'].isin(partial['id_persona'].unique())) & (digit['codday']<mes)]\n        q1 = q.groupby(['id_persona']).sum().add_suffix('_sum').reset_index().fillna(0)\n        q2 = q.groupby(['id_persona']).count().add_suffix('_count').reset_index().fillna(0)\n        ee.append(partial.merge(q1,on='id_persona',how='left').merge(q2,on='id_persona',how='left'))\n    ee = pd.concat(ee,sort=False)\n    \n    return df.merge(ee,on=['codmes_parsed','id_persona'],how='left')","5bd9fd98":"# Obteniendo los features\nprint('(1\/6) Getting SUNAT features...')\ninifull = get_sunat_features(inifull, initr, bsunat)\nprint('(2\/6) Getting RENIEC features...')\ninifull = get_reniec_features(inifull, breniec)\nprint('(3\/6) Getting vehicular features, this may take a while...')\ninifull = get_vehicular_features(inifull, bvehic)\nprint('(4\/6) Getting campaign features...')\ninifull = get_campaign_features(inifull, bcamp)\nprint('(5\/6) Getting RCC features...')\ninifull = get_rcc_features(inifull, basercc)\nprint('(6\/6) Getting digital channel features')\ninifull = get_digital_features(inifull, bdigit)\n","27b85525":"Xa = inifull[inifull['prediction_id'].isna()].drop(columns=['codmes','id_persona','codtarget','margen','codmes_parsed','prediction_id']).reset_index(drop=True)\nya = (inifull[inifull['prediction_id'].isna()]['margen']!=-5).astype(int).reset_index(drop=True)\nassert Xa.shape[0]==ya.shape[0]\nmrgn = inifull[inifull['prediction_id'].isna()]['margen'].values\nX_val = inifull[inifull['prediction_id'].notna()][Xa.columns].reset_index(drop=True)","765b22ac":"kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\n\npredictions=pd.DataFrame()\npreds_oof = np.zeros((Xa.shape[0],))\n\nfor i,(a,b) in enumerate(kf.split(Xa,ya)):\n    X_tr, y_train = Xa.iloc[a,:].values, ya[a]\n    X_te, y_test =Xa.iloc[b,:].values, ya[b]\n    \n    train_data=lgb.Dataset(X_tr, y_train, weight = np.abs(mrgn[a]))\n    test_data=lgb.Dataset(X_te, y_test, reference=train_data, weight = np.abs(mrgn[b]))\n    \n    print('---------- Training fold N\u00ba {} ----------'.format(i+1))\n    \n    params = {'num_leaves': 10, 'min_data_in_leaf': 1000, 'objective': 'binary', 'learning_rate': 0.02, 'boosting': 'gbdt',\n         'random_state': seed, 'metric': 'auc', 'verbosity': -1,}\n    \n    model1 = lgb.train(params,train_data,num_boost_round=10000,valid_sets = [train_data, test_data],verbose_eval=500,early_stopping_rounds = 500)\n    predictions[str(i)] = model1.predict(X_val.values,num_iterations=model1.best_iteration)\n    preds_oof[b] = model1.predict(X_te,num_iterations=model1.best_iteration)\n    \n    gc.collect()\n    \npreds_oof_clf = preds_oof\npreds_clf = predictions","e357d12f":"Xa = inifull[inifull['prediction_id'].isna()].drop(columns=['codmes','id_persona','codtarget','codmes_parsed','prediction_id']).reset_index(drop=True)\nya = inifull[inifull['prediction_id'].isna()]['margen'].reset_index(drop=True)\nassert Xa.shape[0]==ya.shape[0]\nX_val = inifull[inifull['prediction_id'].notna()][Xa.columns].reset_index(drop=True)\ndel X_val['margen']","cca12223":"kf = KFold(n_splits=5,shuffle=True,random_state=seed)\n\npredictions=pd.DataFrame()\npreds_oof = np.zeros((Xa.shape[0],))\n\nfor i,(a,b) in enumerate(kf.split(Xa,ya)):\n    \n    X_tr=Xa.iloc[a,:].query('margen!=-5')\n    X_te=Xa.iloc[b,:].query('margen!=-5')\n    y_train=ya[X_tr.index]\n    y_test=ya[X_te.index]\n    \n    del X_tr['margen'], X_te['margen']\n    \n    train_data=lgb.Dataset(X_tr.values, y_train)\n    test_data=lgb.Dataset(X_te.values, y_test, reference=train_data)\n    \n    print('---------- Training fold N\u00ba {} ----------'.format(i+1))\n    \n    params = {'num_leaves': 15,  'min_data_in_leaf': 600, 'objective': 'regression', 'max_depth': -1, 'learning_rate': 0.005, 'boosting': 'gbdt',\n         'random_state': seed, 'metric': ['l1','l2'], 'verbosity': -1,}\n    \n    model1 = lgb.train(params,train_data,num_boost_round=10000,valid_sets = [test_data], verbose_eval=500,early_stopping_rounds = 500)\n    predictions[str(i)] = model1.predict(X_val[X_tr.columns].values,num_iterations=model1.best_iteration)\n    preds_oof[b] = model1.predict(Xa.iloc[b,:].drop(columns=['margen']).values,num_iterations=model1.best_iteration)\n    \n    gc.collect()\n    \npreds_oof_reg = preds_oof\npreds_reg = predictions","eed08a04":"Xa = pd.DataFrame({'cals':preds_oof_clf,'regs':preds_oof_reg})\nya = (inifull[inifull['prediction_id'].isna()]['margen']>0).astype(int).reset_index(drop=True)\nassert Xa.shape[0]==ya.shape[0]\nmrgn = inifull[inifull['prediction_id'].isna()]['margen'].values\nX_val = pd.DataFrame({'cals':preds_clf.mean(axis=1),'regs':preds_reg.median(axis=1)})","59db6fb7":"kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\n\npredictions=pd.DataFrame()\npreds_oof = np.zeros((Xa.shape[0],))\n\nimportances_lgb=pd.DataFrame()\nimportances_lgb['Features']=Xa.columns\nfor i,(a,b) in enumerate(kf.split(Xa,ya)):\n    X_tr, y_train = Xa.iloc[a,:].values, ya[a]\n    X_te, y_test = Xa.iloc[b,:].values, ya[b]\n    \n    train_data=lgb.Dataset(X_tr, y_train, weight = np.abs(mrgn[a]))\n    test_data=lgb.Dataset(X_te, y_test, reference=train_data, weight = np.abs(mrgn[b]))\n    \n    print('---------- Training fold N\u00ba {} ----------'.format(i+1))\n    \n    params = {'num_leaves': 10, 'min_data_in_leaf': 50, 'objective': 'binary', 'learning_rate': 0.005, 'boosting': 'gbdt', 'random_state': seed,\n          'metric': 'auc', 'verbosity': -1,}\n    \n    model1 = lgb.train(params,train_data,num_boost_round=10000,valid_sets = [train_data, test_data], verbose_eval=500,early_stopping_rounds = 500)\n    predictions[str(i)] = model1.predict(X_val.values,num_iterations=model1.best_iteration)\n    preds_oof[b] = model1.predict(X_te,num_iterations=model1.best_iteration)\n    \n    print('Threshold Optimization:')\n    res = pd.DataFrame({'real':ya[b],'probs':model1.predict(X_te,num_iterations=model1.best_iteration),'margen':mrgn[b]})\n    optimization = differential_evolution(lambda c: -((res.probs > c[0]) * res.margen \/ res.margen.sum()).sum(), [(0, 1)])\n    \n    print(' - Best threshold: ',optimization['x'][0])\n    print(' - Margin of fold: ', sum((preds_oof[b]>optimization['x'][0])*mrgn[b])\/sum(mrgn[b]))\n    gc.collect()","3be409d6":"from scipy.optimize import differential_evolution\n\nres = pd.DataFrame({'real':ya,'probs':preds_oof,'margen':mrgn})\noptimization = differential_evolution(lambda c: -((res.probs > c[0]) * res.margen \/ res.margen.sum()).sum(), [(0, 1)])\noptimization","364f5d07":"inite['class'] = (predictions.mean(axis=1)>optimization['x'][0]).astype(int)\nsubmit = submit[['prediction_id']].merge(inite[['prediction_id','class']],on='prediction_id',how='left')","297f1ccd":"aa = submit['prediction_id'].str.split('_',expand=True).astype(int)\ncorrect = 1-aa.sort_values([0,1],ascending=[True,False])[0].duplicated().sort_index()\nsubmit['class'] = np.where(correct==0,0,submit['class'])","72f8edc6":"plt.plot(X_val[inite['class']==1]['cals'],X_val[inite['class']==1]['regs'],'.')\nplt.plot(X_val[inite['class']==0]['cals'],X_val[inite['class']==0]['regs'],'.')\nplt.xlabel('Probabilidad de aceptar')\nplt.ylabel('Estimaci\u00f3n del Margen')","8d779c38":"submit.to_csv('submission.csv',index=None,encoding='utf-8')","9b43ddb0":"Si es que vuelven a llamar a una persona, es porque previamente ha rechazado el producto (codtarget=0 y margen=-5). Esto era un leak en la data, con el que el score final se incremente entre 0.04 y 0.06.","ad707372":"## Creaci\u00f3n de variables","76c61ed6":"## La \"magia\"","482704aa":"## Inferencia final","6dd0282d":"## Carga de datos","70ae5da4":"## Modelo 2: Estimaci\u00f3n del margen","3aff242a":"## Modelo 1: Clasificaci\u00f3n"}}