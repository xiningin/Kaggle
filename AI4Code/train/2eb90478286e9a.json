{"cell_type":{"3c5f9d26":"code","08ee8b14":"code","952eab74":"code","99edf6a8":"code","0b7d2eec":"code","ecbbf772":"code","8893ccfa":"code","307c7bf6":"code","dfe3b9b4":"code","64c0d479":"code","f349ae05":"code","5cf17dc2":"code","6584e626":"code","65b1f4ae":"code","50533f80":"code","8a7f266a":"code","2abe1177":"code","1de54dba":"code","29909f15":"code","fe86b320":"code","2c6593a3":"code","ac9349d0":"code","22ab11b8":"code","342ea201":"code","f16d76ba":"code","f8d89369":"code","c31ae9ba":"code","f18d5cf4":"code","4a66bd2e":"markdown"},"source":{"3c5f9d26":"#imports necessarios\nimport numpy as np\nimport pandas as pd\nimport string, os \nimport keras\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.core import Activation, Dense, Dropout\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.layers.core import Dense, Activation\nimport keras.utils as kutils\nimport pickle\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nimport string\n","08ee8b14":"#loading datasets\ndf_ep_VI = pd.read_table(\"..\/input\/SW_EpisodeVI.txt\",header=0, escapechar='\\\\',delim_whitespace=True,)\ndf_ep_IV = pd.read_table('..\/input\/SW_EpisodeIV.txt',header=0, escapechar='\\\\',delim_whitespace=True)\ndf_ep_V = pd.read_table(\"..\/input\/SW_EpisodeV.txt\",header=0, escapechar='\\\\',delim_whitespace=True)","952eab74":"#checking if all have same structure\ndf_ep_VI.columns","99edf6a8":"df_ep_V.columns","0b7d2eec":"df_ep_IV.columns","ecbbf772":"#checking sizes\nprint(df_ep_VI.shape)\nprint(df_ep_V.shape)\nprint(df_ep_IV.shape)\n","8893ccfa":"#join the datasets, to work with only one.\ndf = pd.concat([df_ep_IV, df_ep_V, df_ep_VI])","307c7bf6":"#checking size\ndf.shape","dfe3b9b4":"#getting the lines \nall_head_lines = []\nfor filename in df:\n    all_head_lines.extend(list(df.dialogue.values))\n    break\n","64c0d479":"len(all_head_lines)","f349ae05":"all_head_lines[:5]","5cf17dc2":"#creating the corpus\ndef lower_text(txt):\n    txt = \"\".join(b for b in txt if b not in string.punctuation).lower()\n    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n    return txt \n\ncorpus = [lower_text(line) for line in all_head_lines]\n","6584e626":"len(corpus)","65b1f4ae":"corpus[:5]","50533f80":"#creating sents\nsents = [[wd.lower() for wd in word_tokenize(sent) if not wd in string.punctuation]  for sent in all_head_lines]\nx = []\ny = []\nprint(sents[:10])","8a7f266a":"for sent in sents:\n    for i in range(1, len(sent)):\n        x.append(sent[:i])\n        y.append(sent[i])","2abe1177":"print(x[:5])\nprint(y[:5])\n","1de54dba":"#train,test split\ntext = [i for sent in x for i in sent]\ntext += [i for i in y]\n#unknow\ntext.append('UNK') \nwords = list(set(text))      \nword_indexes = {word: index for index, word in enumerate(words)}      \nmax_features = len(word_indexes)\n\nx = [[word_indexes[i] for i in sent] for sent in x]\ny = [word_indexes[i] for i in y]","29909f15":"print(x[:5])\nprint(y[:5])","fe86b320":"y = kutils.to_categorical(y, num_classes=max_features)\nmaxlen = max([len(sent) for sent in x])\nprint(maxlen)","2c6593a3":"x = pad_sequences(x, maxlen=maxlen)\nx = pad_sequences(x, maxlen=maxlen)\n\nfor ya in y:\n    for i in range(len(ya)):\n        if ya[i] != 0:\n            print(i)","ac9349d0":"embedding_size = 10\nmodel = Sequential()  \n# Add Input Embedding Layer\nmodel.add(Embedding(max_features, embedding_size, input_length=maxlen))\n# Add Hidden Layer 1 - LSTM Layer\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.1))\n    \n# Add Output Layer\nmodel.add(Dense(max_features, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","22ab11b8":"model.summary()","342ea201":"model.fit(x, y, epochs=30, verbose=5)","f16d76ba":"print(\"Saving model...\")\nmodel.save('shak-nlg.h5')\n\nwith open('shak-nlg-dict.pkl', 'wb') as handle:\n    pickle.dump(word_indexes, handle)\n\nwith open('shak-nlg-maxlen.pkl', 'wb') as handle:\n    pickle.dump(maxlen, handle)\nprint(\"Model Saved!\")","f8d89369":"model = keras.models.load_model('shak-nlg.h5')\nmaxlen = pickle.load(open('shak-nlg-maxlen.pkl', 'rb'))\nword_indexes = pickle.load(open('shak-nlg-dict.pkl', 'rb'))","c31ae9ba":"#igual da aula\nsample_seed = input()\nsample_seed_vect = np.array([[word_indexes[c] if c in word_indexes.keys() else word_indexes['UNK'] for c in word_tokenize(sample_seed)]])\n\nprint(sample_seed_vect)\nsample_seed_vect = pad_sequences(sample_seed_vect, maxlen=maxlen)\nprint(sample_seed_vect)\n\npredicted = model.predict_classes(sample_seed_vect, verbose=0)\nprint(predicted)\n\ndef get_word_by_index(index, word_indexes):\n    for w, i in word_indexes.items():\n        if index == i:\n            return w\n        \n    return None\n\nfor p in predicted:    \n    print(get_word_by_index(p, word_indexes))","f18d5cf4":"sample_seed = input()\nsample_seed_vect = [word_indexes[c] if c in word_indexes.keys() else word_indexes['UNK']  for c in word_tokenize(sample_seed)]\n\nprint(sample_seed_vect)\npredicted = []\n\nwhile len(sample_seed_vect) < 100:\n    predicted = model.predict_classes(pad_sequences([sample_seed_vect], maxlen=maxlen, padding='pre'), verbose=0)\n    sample_seed_vect.extend(predicted)\n\nres = []\n\nfor p in sample_seed_vect:    \n   res.append(get_word_by_index(p, word_indexes)) \n\nprint(' '.join (res))","4a66bd2e":"**Trabalho em Grupo**\n\nLink Edmodo: https:\/\/new.edmodo.com\/turnin\/assignment:42942429:145606093\n\n**Problema**\n\n> Conversa com personagens do Star Wars: Crie um chatbot que, dado um texto que voc\u00ea digitou, gere uma resposta no estilo \"Star Wars\". A resposta deve conter 100 palavras.\n\nPara criar o modelo de gera\u00e7\u00e3o de texto, voc\u00ea pode utilizar o dataset https:\/\/www.kaggle.com\/xvivancos\/star-wars-movie-scripts\n\n\nNome dos integrantes:\n\n* Aluno 1: Aline Cristini - 183132\n* Aluno 2: Camila Rodrigues - 183143\n* Aluno 3: Rafael Gimenes Leite - 101634\n"}}