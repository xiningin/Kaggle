{"cell_type":{"6b46c26c":"code","6c2c7e24":"code","cd46df49":"code","180164e5":"markdown","2dc9a1ea":"markdown","a9850e09":"markdown"},"source":{"6b46c26c":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)","6c2c7e24":"# Data Loading Code Runs At This Point\nimport pandas as pd\n    \n# Load data\nmelbourne_file_path = '..\/input\/melbourne-housing-snapshot\/melb_data.csv'\nmelbourne_data = pd.read_csv(melbourne_file_path) \n# Filter rows with missing values\nfiltered_melbourne_data = melbourne_data.dropna(axis=0)\n# Choose target and features\ny = filtered_melbourne_data.Price\nmelbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n                        'YearBuilt', 'Lattitude', 'Longtitude']\nX = filtered_melbourne_data[melbourne_features]\n\nfrom sklearn.model_selection import train_test_split\n\n# split data into training and validation data, for both features and target\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)","cd46df49":"# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","180164e5":"\nOf the options listed, 500 is the optimal number of leaves.\n\n---\n\n# Conclusion\n\nHere's the takeaway: Models can suffer from either:\n- **Overfitting:** capturing spurious patterns that won't recur in the future, leading to less accurate predictions, or \n- **Underfitting:** failing to capture relevant patterns, again leading to less accurate predictions. \n\nWe use **validation** data, which isn't used in model training, to measure a candidate model's accuracy. This lets us try many candidate models and keep the best one. \n\n\n---\n# Your Turn\n\nTry **[optimizing the model you've previously built](https:\/\/www.kaggle.com\/kernels\/fork\/1259126)**.\n\n\n---\n**[Course Home Page](https:\/\/www.kaggle.com\/learn\/machine-learning)**\n\n","2dc9a1ea":"We can use a for-loop to compare the accuracy of models built with different values for *max_leaf_nodes.*","a9850e09":"The data is loaded into **train_X**, **val_X**, **train_y** and **val_y** using the code you've already seen (and which you've already written)."}}