{"cell_type":{"aa9afab6":"code","d3c4773a":"code","2dc4de61":"code","2d91bdfa":"code","38652d46":"code","c52a7778":"code","78bff910":"code","2b639667":"code","91735920":"code","631d68be":"code","16898cdb":"code","e7c7eec1":"code","91c51197":"code","30f6506a":"code","02157e3c":"code","ace9ca71":"code","8669a0d4":"code","a6bb6605":"code","174e5015":"code","834141db":"code","a1a9e566":"code","c9723ea3":"code","6e1cf0fe":"code","a6495b09":"code","1bfbc662":"code","9965bd55":"code","ab544286":"code","e6b994dc":"code","92ebc46b":"code","ba7dcb22":"code","63c411b8":"code","dc7ed9fe":"code","b368d29a":"code","c88ba857":"code","1c98c339":"code","d1c640e6":"code","f06162c6":"code","75aee29e":"code","56e7f707":"code","5607e99d":"markdown","3f4dd889":"markdown","99770c4f":"markdown","1d633a29":"markdown","249a35f1":"markdown","b9fee70a":"markdown","9672d4a5":"markdown","ccdf230a":"markdown","73943008":"markdown"},"source":{"aa9afab6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d3c4773a":"#Utils\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report, roc_auc_score, make_scorer, accuracy_score, roc_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC \nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cluster import KMeans\nfrom kmodes.kmodes import KModes\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform as sp_uniform\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.metrics.pairwise import pairwise_kernels\nfrom sklearn.metrics.pairwise  import cosine_similarity\nfrom sklearn.metrics.pairwise import chi2_kernel\nfrom catboost import CatBoostClassifier\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\nfrom sklearn.manifold import TSNE\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport category_encoders as ce","2dc4de61":"pd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","2d91bdfa":"train= pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv', sep=',')\ntest= pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv', sep=',')","38652d46":"train = train.set_index('id')\ntest = test.set_index('id')\n","c52a7778":"sub_sample = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv', sep=',')\nsub_sample = sub_sample.set_index('id')","78bff910":"fig = px.histogram(train, x=\"target\",\n                   width=600, \n                   height=400,\n                   histnorm='percent',\n                   template=\"simple_white\"\n                   )\nfig.update_layout(title=\"Target Description\", \n                  font_family=\"San Serif\",\n                  titlefont={'size': 20},\n                  showlegend=True,\n                  legend=dict(\n                      orientation=\"v\", \n                      y=1, \n                      yanchor=\"top\", \n                      x=1.0, \n                      xanchor=\"right\"\n                  )                \n                 ).update_xaxes(categoryorder='total descending')#\nfig.update_traces( \n                  marker_line_width=1.5, opacity=0.99)\nfig.show()","2b639667":"def calc_loss(class_perc, num):\n    \n    lst=[]\n    \n    for i,z in enumerate(class_perc):\n        lst = lst+[i for x in range(int(z*(num+1)))]\n        \n    preds=[]\n    \n    for i in range(num):\n        preds+=[class_perc]\n    \n    return (log_loss(lst,preds))","91735920":"train['target'].value_counts(normalize=True)","631d68be":"train['target'].count()","16898cdb":"calc_loss([0.57497, 0.21420, 0.12593, 0.08490], 200000)","e7c7eec1":"#example of how could be the distribution in classes according to the LB\ncalc_loss([0.60, 0.20, 0.10, 0.10], 200000)","91c51197":"#target Map\ndict1 = dict(zip(list(train.target.unique()),range(4)))\ndict1","30f6506a":"train['target']= train['target'].replace(dict1)","02157e3c":"corr = train.corr()\nfig = go.Figure(data= go.Heatmap(z=corr,\n                                 x=corr.index.values,\n                                 y=corr.columns.values,\n                                 zmin=-0.05,\n                                 zmax=0.05\n                                 )\n                )\nfig.update_layout(title_text='<b>Correlation Matrix<b>',\n                  title_x=0.5,\n                  titlefont={'size': 24},\n                  width=900, height=800,\n                  xaxis_showgrid=False,\n                  yaxis_showgrid=False,\n                  yaxis_autorange='reversed', \n                  paper_bgcolor=None,\n                  )\nfig.show()","ace9ca71":"cols = train.columns[:-1]\ntarget = train['target']\ntrain = train[cols]","8669a0d4":"df = pd.concat([train[cols], test[cols]], axis=0)","a6bb6605":"unique_df = pd.DataFrame(df.nunique()).reset_index()\nunique_df.columns=['features','count']\n\nfig1 = px.bar(unique_df, y='count', x=cols)\n\nfig1.update_layout(title='Feature cardinality in train+test set',\n                  xaxis_title='features',\n                  yaxis_title='# unique values',\n                  titlefont={'size': 28, 'family':'Serif'},\n                  template='simple_white',\n                  showlegend=True,\n                  width=900, height=500)\nfig1.show()","174e5015":"(train==0).mean().plot(kind='bar', figsize=(15, 5), title='# zeros on total')","834141db":"df.skew().plot(kind='bar', figsize=(15, 5), title='# Skew')\nplt.ylim(0,15)","a1a9e566":"df2=df\ndf2[df2<0]=0\ndf2 = np.log(df2+1)\ndf2.skew().plot(kind='bar', figsize=(15, 5), title='# skew after log+1 trasformation')\nplt.ylim(0,15)","c9723ea3":"df_skew = pd.DataFrame(df2.skew()).reset_index()\ndf_skew.columns=['features','skew']","6e1cf0fe":"df_skew[df_skew['skew']<1.1]","a6495b09":"#first attempt is to consider as numeric features the one with skewness <1.5 and as categorical all the other\nn=1.1\nlow =list(df_skew[df_skew['skew']<=n]['features'])\nhigh = list(df_skew[df_skew['skew']>n]['features'])","1bfbc662":"clf = RandomForestClassifier(n_estimators=200, max_features='sqrt')\nclf = clf.fit(train, target)\nfeatures = pd.DataFrame()\nfeatures['feature'] = train.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n\nfeatures.plot(kind='barh', figsize=(25, 25))","9965bd55":"#training set log+1 trasformation\ntrain[train<0]=0\ntest[test<0]=0\nfor l in low:\n    train[l] = np.log(train[l]+1)\n    test[l] = np.log(test[l]+1)","ab544286":"train[high] = train[high].astype('category')\ntest[high] = test[high].astype('category')","e6b994dc":"import optuna","92ebc46b":"def objective(trial , data = train , target = target):\n    train_x , test_x , train_y , test_y = train_test_split(data , target , \\\n            test_size = 0.028059109276941666 , random_state = 2)\n\n    params = {'iterations':20000,\n              'depth': trial.suggest_int(\"depth\", 3, 80),\n              'l2_leaf_reg': trial.suggest_float(\"l2_leaf_reg\", 0.0001, 25, log=True),\n              'bagging_temperature': trial.suggest_float(\"bagging_temperature\", 0, 100),\n              'auto_class_weights':trial.suggest_categorical('auto_class_weights', [None,'Balanced','SqrtBalanced']),\n              'grow_policy': 'Lossguide',\n              'loss_function':trial.suggest_categorical(\"loss_function\", ['MultiClassOneVsAll', 'MultiClass']),\n              'bootstrap_type':trial.suggest_categorical(\"bootstrap_type\", ['Poisson']),\n              'use_best_model':True,\n              'task_type':'GPU', \n              'cat_features':high,\n              'learning_rate': trial.suggest_uniform('learning_rate' , 1e-5 , 1.0),\n              'max_bin': trial.suggest_int('max_bin', 5, 700),\n              'verbose':False,\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 600),\n              'subsample': trial.suggest_uniform('subsample' , 1e-5 , 1.0),\n              'max_ctr_complexity': trial.suggest_int('max_ctr_complexity', 2, 15),\n             }\n    model = CatBoostClassifier(**params)\n    model.fit(train_x , train_y , eval_set = [(test_x , test_y)] , \\\n             verbose = False)\n    preds =  model.predict_proba(test_x)\n    lgl = log_loss(test_y , preds)\n    return lgl","ba7dcb22":"#study = optuna.create_study(direction = 'minimize' , study_name = 'cb')\n#study.optimize(objective , n_trials = 100)\n#print('numbers of the finished trials:' , len(study.trials))\n#print('the best params:' , study.best_trial.params)\n#print('the best value:' , study.best_value)","63c411b8":"iso = IsolationForest(n_estimators=500,contamination=0.003, random_state=1)\nyhat = iso.fit_predict(train)","dc7ed9fe":"train['anomaly']=yhat\ntrain_clean=train.loc[train['anomaly']!=-1]\ntrain_clean = train_clean.drop(columns=['anomaly'])\ntrain_clean.shape","b368d29a":"train_index=list(train_clean.index)\ntarget_clean = target.iloc[train_index]\ntarget_clean.shape","c88ba857":"param_cb ={\n    'depth': 3, \n    'l2_leaf_reg': 4.287566030099442, \n    'bagging_temperature': 27.174417642203863, \n    'auto_class_weights': None, \n    'loss_function': 'MultiClassOneVsAll',\n    'eval_metric': 'MultiClassOneVsAll',\n    'grow_policy': 'Lossguide',\n    'bootstrap_type': 'Poisson',\n    'cat_features': high,\n    'iterations':10000,\n    'max_bin': 484, \n    'min_data_in_leaf': 414,\n    'task_type':'GPU',\n    'subsample': 0.13534551086578891,\n    'max_ctr_complexity':10\n}","1c98c339":"preds = pd.DataFrame(index=test.index)\noof_preds = np.zeros(train_clean.shape[0])\nkf = StratifiedKFold(n_splits = 7, random_state = 22 , shuffle = True)\nroc = []\n\nn = 0\nfor trn_idx , val_idx in kf.split(train_clean , target_clean):\n    train_x = train_clean.iloc[trn_idx]\n    train_y = target_clean.iloc[trn_idx]\n    val_x = train_clean.iloc[val_idx]\n    val_y = target_clean.iloc[val_idx]\n    \n    \n    model = CatBoostClassifier(**param_cb, random_seed=1)\n    model.fit(train_x , train_y , eval_set = [(val_x , val_y)] , verbose = False)\n    preds = pd.concat([preds, pd.DataFrame(model.predict_proba(test),index=test.index)], axis=1)\n    oof_preds = model.predict_proba(train_clean)\n    roc.append(log_loss(val_y ,np.clip(model.predict_proba(val_x),0.025,0.975)))\n    \n    print(n+1 , roc[n])\n    \n    n+=1","d1c640e6":"dict1","f06162c6":"sub_sample['Class_1']=np.clip(preds[1].mean(axis=1), 0.025,0.975)\nsub_sample['Class_2']=np.clip(preds[0].mean(axis=1),0.025,0.975)\nsub_sample['Class_3']=np.clip(preds[3].mean(axis=1), 0.025,0.975)\nsub_sample['Class_4']=np.clip(preds[2].mean(axis=1), 0.025,0.975)","75aee29e":"sub_sample = sub_sample.reset_index()\nsub_sample.head()","56e7f707":"sub_sample.to_csv('submission.csv',index=False)","5607e99d":"### Use isolation Forest to clean the training set","3f4dd889":"### A basic methodology for approacing TPG May competition","99770c4f":"### Estimation model Log-loss vs Random Guessing","1d633a29":"The rule of thumb seems to be: If the skewness is between -0.5 and 0.5, the data are fairly symmetrical. If the skewness is between -1 and \u2013 0.5 or between 0.5 and 1, the data are moderately skewed. If the skewness is less than -1 or greater than 1, the data are highly skewed.","249a35f1":"### Load Data","b9fee70a":"using np.clip function to keep probabilities between 0.025\u20130.975. In this case,there won't be a big growth of the log loss funcion in case of wrong predictions.","9672d4a5":"### Using Optuna with Catboost","ccdf230a":"rewrite a function I've read in an article some time ago..","73943008":"### Train the Model"}}