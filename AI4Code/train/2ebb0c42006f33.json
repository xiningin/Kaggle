{"cell_type":{"38711548":"code","146bd747":"code","cb096bc0":"code","70d17864":"code","b44bbaf8":"code","4ffa4c4e":"code","3dfe7c12":"code","ccd9ebbe":"code","8b7b7eb6":"code","c1c4da2f":"code","322eecf6":"code","3a7f7ad8":"code","68c96349":"code","6bfff0ce":"code","3aaf36db":"code","0a218116":"code","dcf9dea0":"code","4ddfc6e7":"code","af8e5452":"code","d1f5490a":"code","cc9fd5ff":"code","0020510e":"code","3fbc7e31":"code","f596b147":"markdown","3de6501f":"markdown","948d9ce6":"markdown","3d1d4e81":"markdown","de95fea6":"markdown","4172a749":"markdown","d075da80":"markdown","5e39014d":"markdown","0a80de01":"markdown","f8521b4d":"markdown"},"source":{"38711548":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup\nimport pickle\nimport os, re\nprint(os.listdir(\"..\/input\"))\nfrom nltk.corpus import stopwords\n\n# Any results you write to the current directory are saved as output.","146bd747":"train = pd.read_csv(\"..\/input\/labeledTrainData.tsv\", header = 0, delimiter = '\\t')\ntest = pd.read_csv(\"..\/input\/testData.tsv\", header = 0, delimiter = '\\t')","cb096bc0":"test[\"sentiment\"] = test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\ny_test = test[\"sentiment\"]","70d17864":"def html_to_text(review):\n    \"\"\"Return extracted text string from provided HTML string.\"\"\"\n    review_text = BeautifulSoup(review, \"lxml\").get_text()\n    if len(review_text) == 0:\n        review_text = review\n    review_text = re.sub(r\"\\<.*\\>\", \"\", review_text)\n    try:\n        review_text = review_text.encode('ascii', 'ignore').decode('ascii')#ignore \\xc3 etc.\n    except UnicodeDecodeError:\n        review_text = review_text.decode(\"ascii\", \"ignore\")\n    return review_text\n\n\ndef letters_only(text):\n    \"\"\"Return input string with only letters (no punctuation, no numbers).\"\"\"\n    # It is probably worth experimenting with milder prepreocessing (eg just removing punctuation)\n    return re.sub(\"[^a-zA-Z]\", \" \", text)\n\ndef rnn_tokenizer_review_preprocess(review):\n    \"\"\"Preprocessing used before fitting\/transforming RNN tokenizer - Html->text, remove punctuation\/#s, lowercase.\"\"\"\n    return letters_only(html_to_text(review)).lower()","b44bbaf8":"def get_train_val_data(reviews_to_features_fn=None, df = train):\n    \"\"\"Extracts features (using reviews_to_features_fn), splits into train\/test data, and returns\n    x_train, y_train, x_test, y_test.  If no feature extraction function is provided, x_train\/x_test will\n    simply consist of a Series of all the reviews.\n    \"\"\"\n#     df = pd.read_csv('labeledTrainData.tsv', header=0, quotechar='\"', sep='\\t')\n    SEED = 1000\n    # Shuffle data frame rows\n    np.random.seed(SEED)\n    df = df.iloc[np.random.permutation(len(df))]\n\n    if reviews_to_features_fn:\n        feature_rows = df[\"review\"].map(reviews_to_features_fn)\n        if type(feature_rows[0]) == np.ndarray:\n            num_instances = len(feature_rows)\n            num_features = len(feature_rows[0])\n            x = np.concatenate(feature_rows.values).reshape((num_instances, num_features))\n        else:\n            x = feature_rows\n    else:\n        x = df[\"review\"]\n\n    y = df[\"sentiment\"]\n\n    # Split 80\/20\n    test_start_index = int(df.shape[0] * .8)\n    x_train = x[0:test_start_index]\n    y_train = y[0:test_start_index]\n    x_val = x[test_start_index:]\n    y_val = y[test_start_index:]\n\n    return x_train, y_train, x_val, y_val","4ffa4c4e":"x_train, y_train, x_val, y_val = get_train_val_data(rnn_tokenizer_review_preprocess)\nx_test = test[\"review\"].map(rnn_tokenizer_review_preprocess)\ny_test = test[\"sentiment\"]","3dfe7c12":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","ccd9ebbe":"np.random.seed(1000)\nnum_most_freq_words_to_include = 5000\nMAX_REVIEW_LENGTH_FOR_KERAS_RNN = 500\nembedding_vector_length = 32","8b7b7eb6":"# train_review_list = [s.encode('ascii') for s in x_train.tolist()]\n# val_review_list = [s.encode('ascii') for s in x_val.tolist()]\n# all_review_list = train_review_list + val_review_list\ntrain_review_list = x_train.tolist()\nval_review_list = x_val.tolist()\ntest_review_list = x_test.tolist()\nall_review_list = x_train.tolist() + x_val.tolist()","c1c4da2f":"tokenizer = Tokenizer(num_words=num_most_freq_words_to_include)\ntokenizer.fit_on_texts(all_review_list)","322eecf6":"train_reviews_tokenized = tokenizer.texts_to_sequences(train_review_list)\nx_train = pad_sequences(train_reviews_tokenized, maxlen=MAX_REVIEW_LENGTH_FOR_KERAS_RNN)\nval_review_tokenized = tokenizer.texts_to_sequences(val_review_list)\nx_val = pad_sequences(val_review_tokenized, maxlen=MAX_REVIEW_LENGTH_FOR_KERAS_RNN)\ntest_review_tokenized = tokenizer.texts_to_sequences(test_review_list)\nx_test = pad_sequences(test_review_tokenized, maxlen=MAX_REVIEW_LENGTH_FOR_KERAS_RNN)","3a7f7ad8":"from keras.layers import Input, Embedding, Dropout, Conv1D, MaxPool1D, GRU, LSTM, Dense\nfrom keras.models import Model","68c96349":"def rnn_model(use_cnn = True, use_lstm = False):\n    input_sequences = Input(shape = (MAX_REVIEW_LENGTH_FOR_KERAS_RNN,))\n    initial_dropout = 0.2\n    embedding_layer = Embedding(input_dim = num_most_freq_words_to_include, \n                                output_dim = embedding_vector_length,\n                                input_length = MAX_REVIEW_LENGTH_FOR_KERAS_RNN)\n    X = embedding_layer(input_sequences)\n    X = Dropout(0.2)(X)\n    if use_cnn:\n        X = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(X)\n        X = MaxPool1D(pool_size=2)(X)\n        \n    # Add GRU layers\n    dropout_W = 0.0\n    dropout_U = 0.0\n    \n    if use_lstm:\n        X = LSTM(100, dropout = dropout_W, recurrent_dropout = dropout_U)(X)\n    else:\n        X = GRU(100, dropout=dropout_W, recurrent_dropout=dropout_U)(X)\n    X = Dropout(0.2)(X)\n    outputs= Dense(1, activation='sigmoid')(X)\n    model = Model(inputs = input_sequences, outputs = outputs)\n    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    return model","6bfff0ce":"gru_model = rnn_model(use_lstm=False)","3aaf36db":"gru_model.summary()","0a218116":"gru_model.fit(x_train, y_train, batch_size=64, epochs=3, validation_data=[x_val, y_val])","dcf9dea0":"y_test_pred_gru = gru_model.predict(x_test)","4ddfc6e7":"lstm_model = rnn_model(use_lstm=True)\nlstm_model.summary()","af8e5452":"lstm_model.fit(x_train, y_train, batch_size = 64, epochs = 3, validation_data=[x_val, y_val])","d1f5490a":"y_test_pred_lstm = lstm_model.predict(x_test)","cc9fd5ff":"from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\nimport matplotlib.pyplot as plt\n%matplotlib inline","0020510e":"print(\"The AUC socre for GRU model is : %.4f.\" %roc_auc_score(y_test, y_test_pred_gru))\nprint(\"The AUC socre for LSTM model is : %.4f.\" %roc_auc_score(y_test, y_test_pred_lstm))","3fbc7e31":"y_pred_list = [y_test_pred_gru, y_test_pred_lstm]\nlabel_list = [\"GRU\", \"LSTM\"]\npred_label = zip(y_pred_list, label_list)\nfor y_pred, lbl in pred_label:\n    fpr, tpr, _ = roc_curve(y_test, y_pred)\n    plt.plot(fpr, tpr, label = lbl)\n\nplt.xlabel(\"True Postive Rate\")\nplt.ylabel(\"False Positive Rate\")\nplt.title(\"ROC Curve for RNN Models\")\nplt.legend()\nplt.show()","f596b147":"## Architecture the RNN Model\n\n","3de6501f":"## Propcessed the data","948d9ce6":"Overall, GRU model performs slightly well than LSTM, but the difference is very small.","3d1d4e81":"Define some pre-processing functions","de95fea6":"Extract out the labels from the test data, due to the data leakage issue as pointed in the post https:\/\/www.kaggle.com\/c\/word2vec-nlp-tutorial\/discussion\/27022 . The label from the test data will be used for evaluation purpose.","4172a749":"The key idea of this kernel is mainly from https:\/\/github.com\/bluelight773\/Kaggle_IMDB_Bags_of_Popcorn\/blob\/master\/imdb.py. The purpose is to demo the effectiveness of sequence models in semantic anlaysis and sentiment classification.","d075da80":"### Evaluate the Model Performance on Test Data","5e39014d":"## Generate the text sequence for RNN model","0a80de01":"## LSTM Model","f8521b4d":"## GRU Model"}}