{"cell_type":{"e0956f6a":"code","3d723172":"code","e4ec1de1":"code","e1393b6f":"code","16f92630":"code","d49f1724":"code","1fec1bd4":"code","59e2b409":"code","2492e364":"code","7df769a4":"code","d29b220a":"code","e2f73b27":"code","8cb9c01c":"code","9827c80b":"code","fcc5047b":"code","39943f77":"code","1a6d7765":"code","54170199":"markdown","eed4d7a0":"markdown","dc31fc35":"markdown","a760ab2d":"markdown","d06ad193":"markdown","62adc8a2":"markdown","cc3759b9":"markdown","774883bb":"markdown","ccff336e":"markdown","e8c5b302":"markdown","edf665de":"markdown","ebc1f89b":"markdown","30b1ecb6":"markdown","2eba5c39":"markdown","e4bc7f12":"markdown","143d2730":"markdown","dce4a762":"markdown","83931458":"markdown","d8f35527":"markdown","7934ae61":"markdown","3932cad5":"markdown","504f21bd":"markdown","76623d02":"markdown","70fe4bcb":"markdown"},"source":{"e0956f6a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","3d723172":"#Load data as dataframe\ndata=pd.read_csv(\"..\/input\/creditcard.csv\")\ndata.head()","e4ec1de1":"# min max scaling except on time and class columns\nX=data.drop([\"Time\",\"Class\"],axis=1)\nX= (X- X.min()) \/ (X.max() - X.min())","e1393b6f":"X.head()","16f92630":"for j in list(data):\n    for i in range(2):\n        sns.kdeplot(data[data.Class==i][j])\n    plt.show()","d49f1724":"# using only some of the features that look distinct for anomaly and normal class. \nkeep=['V1','V3','V4','V7','V9','V10','V11','V12','V14','V16','V17','V18','V19']\nX=X[keep]","1fec1bd4":"X['Class']=data['Class']","59e2b409":"anomalies=X[X['Class']==1]\nnormal=X[X['Class']==0]","2492e364":"# data_train, data_cv, data_test\n\ndata_train=normal.iloc[:int(len(normal)*0.6)]\ndata_cv=normal.iloc[int(len(normal)*0.6):int(len(normal)*0.8)]\ndata_test=normal.iloc[int(len(normal)*0.8):]\n\ndata_cv=data_cv.append(anomalies.iloc[:int(len(anomalies)*0.5)])\ndata_test=data_test.append(anomalies.iloc[int(len(anomalies)*0.5):])\n\nx_train=data_train.drop(labels='Class',axis=1)\ny_train=data_train['Class']\nx_cv=data_cv.drop(labels='Class',axis=1)\ny_cv=data_cv['Class']\nx_test=data_test.drop(labels='Class',axis=1)\ny_test=data_test['Class']","7df769a4":"from scipy.stats import multivariate_normal\n# function to calculate parameters, mu and sigma \ndef estimateGaussian(dataset):\n    mu = np.mean(dataset, axis=0)\n    sigma = np.cov(dataset.T)\n    return mu, sigma\n\n# function to calculate probability density\ndef multivariateGaussian(dataset,mu,sigma):\n    p = multivariate_normal(mean=mu, cov=sigma)\n    return p.pdf(dataset)","d29b220a":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score , average_precision_score\nfrom sklearn.metrics import precision_score, precision_recall_curve\nfrom sklearn.metrics import confusion_matrix","e2f73b27":"# mu,sigma of normal transactions to be used\nmu,sigma=estimateGaussian(x_train)\n\np_train=multivariateGaussian(x_train,mu,sigma)\np_cv=multivariateGaussian(x_cv,mu,sigma)\np_test=multivariateGaussian(x_test,mu,sigma)","8cb9c01c":"test_cases=np.sort(np.unique(p_train))[:1000]\n#epsilon=1.0527717316e-70 \nd={}\nd2={}\nfor epsilon in test_cases:\n    y_pred_cv=p_cv<epsilon\n    d[epsilon]=f1_score(y_cv,y_pred_cv.astype(int))\nepsilon=max(d.items(),key=lambda x: x[1])[0]\n\n#refining epsilon value\nfor i in range(100):\n    epsilon=epsilon*0.9\n    y_pred_cv=p_cv<epsilon\n    d2[epsilon]=f1_score(y_cv,y_pred_cv.astype(int))\nepsilon=max(d2.items(),key=lambda x: x[1])[0]\nprint(f\"Optimum epsilon value : {epsilon}\")","9827c80b":"print(\"f1 score,recall,precision on test set :\")\nf1_score(y_cv,y_pred_cv.astype(int)),recall_score(y_cv,y_pred_cv.astype(int)),precision_score(y_cv,y_pred_cv.astype(int))","fcc5047b":"confusion_matrix(y_cv,y_pred_cv.astype(int))","39943f77":"y_pred_test=p_test<epsilon\nprint(\"f1 score,recall,precision on test set :\")\nf1_score(y_test,y_pred_test.astype(int)),recall_score(y_test,y_pred_test.astype(int)),precision_score(y_test,y_pred_test.astype(int))","1a6d7765":"confusion_matrix(y_test,y_pred_test.astype(int))","54170199":"a=(pd.Series(y_pred)+data['Class']).value_counts()\na #-1 is fp ,1 is tp , 2 is fn , 0 is tn ","eed4d7a0":"# I'll apply few algorithms below :","dc31fc35":"# Importing libraries","a760ab2d":"print(f\"Accuracy is {(a[0]+a[1])*100\/(a[2]+a[-1]+a[0]+a[1])} %\")\nprint(f\"Recall is {a[1]*100\/(a[-1]+a[1]),a[0]*100\/(a[0]+a[-1])} %\")\nprint(f\"Precision is {a[1]*100\/(a[2]+a[1]),a[0]*100\/(a[0]+a[2])} %\")","d06ad193":"# Features to be kept(after analysing plots)","62adc8a2":"# Multivariate Gaussian Distribution model","cc3759b9":"# kde plots of features (for feature selection)","774883bb":"# functions","ccff336e":"Drawing kde plots of all the features for both classes 0 and 1,to find out whether those features really impact in causing anomaly and if they don't,we exclude them before building models.","e8c5b302":"y_pred","edf665de":"# Data division","ebc1f89b":"from sklearn.neighbors import LocalOutlierFactor\nclf = LocalOutlierFactor(n_neighbors=30,contamination=0.002)\ny_pred = clf.fit_predict(X)","30b1ecb6":"LOF and Isolation forest didn't give good results","2eba5c39":"# model","e4bc7f12":"# Isolation Forest","143d2730":"Dividing data into 3 parts :\n1st - 60% of normal\n2nd- 20% of normal , 50% of anomalous (cv)\n3rd- 20% of normal , 50% of anomalous (test)","dce4a762":"print(f\"Predicted\\n{pd.Series(y_pred).value_counts()}\\n\")\nprint(f\"Actual\\n{data['Class'].value_counts()}\")","83931458":"# Normalise data","d8f35527":"# LOF algorithm (unsupervised )","7934ae61":"from sklearn.ensemble import IsolationForest\nmodel = IsolationForest(random_state=42, n_jobs=4, bootstrap=True, n_estimators=50,contamination=0.002)\nmodel.fit(X)\ny_pred=model.predict(X)","3932cad5":"# finding the optimum epsilon value (on cv set)","504f21bd":"# Load data","76623d02":"a=(pd.Series(y_pred)+data['Class']).value_counts()\nprint(f\"Accuracy is {(a[0]+a[1])*100\/(a[2]+a[-1]+a[0]+a[1])} %\")\nprint(f\"Recall is {a[1]*100\/(a[-1]+a[1]),a[0]*100\/(a[0]+a[-1])} %\")\nprint(f\"Precision is {a[1]*100\/(a[2]+a[1]),a[0]*100\/(a[0]+a[2])} %\")","70fe4bcb":"# predicting on test set"}}