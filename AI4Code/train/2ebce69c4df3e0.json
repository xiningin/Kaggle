{"cell_type":{"2994a6c4":"code","8649aa21":"code","2f737b16":"code","efe5c394":"code","6208fa9e":"code","a56c3fb5":"code","209c2fde":"code","78de1f51":"code","fe05c1c3":"code","929264cf":"code","1aed33c5":"code","309458be":"code","dc4b8d23":"code","833fa218":"code","1d4dffb3":"code","e21af4fb":"code","50e005b8":"code","01cbe53b":"code","b9a59ee5":"code","a2c62cdd":"code","3acbcfbf":"code","edf66b85":"code","6dd16980":"code","f33be748":"code","5b4ba088":"code","c69067a6":"code","27569be9":"code","a98df74f":"code","6a2f2fa3":"markdown"},"source":{"2994a6c4":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nfrom sklearn.metrics import accuracy_score\nimport json\nimport os\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import r2_score\n\nimport torch.utils.data\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 500)","8649aa21":"os.mkdir('save')","2f737b16":"train_csv = pd.read_csv(\"..\/input\/train.csv\", keep_default_na=False)\ntest_csv = pd.read_csv(\"..\/input\/test.csv\", keep_default_na=False)\n\n# train = train[0:30]\nprint(train_csv.columns)","efe5c394":"def preprocess_data(dataset):\n    dataset['LotFrontage'] = pd.to_numeric(dataset[\"LotFrontage\"].str.strip().replace(\"NA\", \"\"))\n    dataset['MasVnrArea'] = pd.to_numeric(dataset[\"MasVnrArea\"].str.strip().replace(\"NA\", \"\"))\n    dataset['GarageYrBlt'] = pd.to_numeric(dataset[\"GarageYrBlt\"].str.strip().replace(\"NA\", \"\"))\n    \n    dataset['LotFrontage'].fillna(dataset['LotFrontage'].mean(), inplace=True)\n    dataset['MasVnrArea'].fillna(dataset['MasVnrArea'].mean(), inplace=True)\n    dataset['GarageYrBlt'].fillna(dataset['GarageYrBlt'].mean(), inplace=True)\n    \n    dataset = dataset.drop(['Id'],axis=1)\n    return dataset","6208fa9e":"train = preprocess_data(train_csv)\ntest = preprocess_data(test_csv)\n\ndisplay(train.head())","a56c3fb5":"# train.dtypes","209c2fde":"def combinelabelencode(cols, traindataset, testdataset):\n    finallist = []\n    for c in cols:\n        finallist = np.concatenate((finallist, traindataset[c].values, testdataset[c].values), axis=None)\n    \n    finallist = np.unique(finallist)\n    print(list(finallist))\n    conditionlbl = LabelEncoder()\n    conditionlbl.fit(list(finallist))\n    for c in cols:\n        traindataset[c] = conditionlbl.transform(list(traindataset[c].values))\n        testdataset[c] = conditionlbl.transform(list(testdataset[c].values))\n    return traindataset, testdataset","78de1f51":"cols = ('MSZoning','Street','Alley','LotShape','LandContour','Utilities','LotConfig', 'LandSlope', 'Neighborhood',\n        'RoofStyle', 'RoofMatl', 'SaleType', 'SaleCondition', 'MiscFeature', 'Fence', 'PoolQC', 'PavedDrive',\n        'Functional', 'Electrical', 'Heating', 'Foundation', 'GarageType', 'BldgType', 'HouseStyle', 'CentralAir')\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(np.concatenate((train[c].values, test[c].values), axis=None))) \n    train[c] = lbl.transform(list(train[c].values))\n    test[c] = lbl.transform(list(test[c].values))\n\ntrain, test = combinelabelencode(('Condition1', 'Condition2'), train, test)\ntrain, test = combinelabelencode(('BsmtQual', 'BsmtCond', 'BsmtExposure', 'KitchenQual', 'HeatingQC'), train, test)\ntrain, test = combinelabelencode(('BsmtFinType1', 'BsmtFinType2', 'GarageFinish'), train, test)\ntrain, test = combinelabelencode(('GarageQual', 'GarageCond', 'FireplaceQu', 'ExterQual', 'ExterCond'), train, test)\ntrain, test = combinelabelencode(('Exterior1st', 'Exterior2nd', 'MasVnrType'), train, test)\n","fe05c1c3":"display(train.head())","929264cf":"cat = len(train.select_dtypes(include=['object']).columns)\nnum = len(train.select_dtypes(include=['int64','float64']).columns)\nprint('Total Features: ', cat, 'categorical', '+', num, 'numerical', '=', cat+num, 'features')\nprint(train.select_dtypes(include=['object']).columns)","1aed33c5":"k = 52\nmin_val_corr = 0.4\ncorrmat = train.corr()\nser_corr = corrmat.nlargest(k, 'SalePrice')['SalePrice']\ncols = ser_corr.index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=0.75)\nf, ax = plt.subplots(figsize=(35, 35))\nsns.heatmap(cm, cbar=True, annot=True, square=True, vmax=.8, fmt='.2f', yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","309458be":"cols_abv_corr_limit = list(ser_corr[ser_corr.values > min_val_corr].index)\ncols_bel_corr_limit = list(ser_corr[ser_corr.values <= min_val_corr].index)\nprint(cols_abv_corr_limit)\nprint(cols_bel_corr_limit)","dc4b8d23":"trimed_train = pd.DataFrame(columns=cols_abv_corr_limit)\ntrimed_test = pd.DataFrame(columns=cols_abv_corr_limit)\nfor c in cols_abv_corr_limit:\n    trimed_train[c] = train[c]\n    try:\n        trimed_test[c] = test[c]\n    except:\n        print(\"column not found\")\n\ntrimed_test = trimed_test.drop('SalePrice',axis = 1)\n\nfor c in trimed_test.select_dtypes(include=['object']).columns:\n    trimed_test[c] = pd.to_numeric(trimed_test[c].str.strip().replace(\"NA\", \"\"))\n    trimed_test[c].fillna(trimed_test[c].median(), inplace=True)\n# trimed_train = pd.concat([trimed_train, trimed_train], ignore_index=True)\n\ncol_train = list(cols_abv_corr_limit)\ncol_train_bis = list(cols_abv_corr_limit)\n\ncol_train_bis.remove('SalePrice')\nprint(len(col_train))\nprint(trimed_train.shape)\n\n","833fa218":"display(trimed_test.head())","1d4dffb3":"mat_train = np.matrix(trimed_train)\nmat_test  = np.matrix(trimed_test)\n\nmat_new = np.matrix(trimed_train.drop('SalePrice',axis = 1))\nmat_y = np.array(trimed_train.SalePrice).reshape((1460,1))\n\nprepro_y = MinMaxScaler()\nprepro_y.fit(mat_y)\n\nprepro = MinMaxScaler()\nprepro.fit(mat_train)\n\nprepro_test = MinMaxScaler()\nprepro_test.fit(mat_new)\n\n# trimed_test.to_csv(\"output_final_3.csv\")\ntrain_set = pd.DataFrame(prepro.transform(trimed_train),columns = col_train)\n\n# test = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_bis)\n\ntest_set  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_bis)\n\ndisplay(train_set.head())","e21af4fb":"COLUMNS = col_train\nFEATURES = col_train_bis\nLABEL = \"SalePrice\"\n\n# Training set and Prediction set with the features to predict\ntraining_set = train_set[COLUMNS]\nprediction_set = training_set.SalePrice\n\n# print(prediction_set)\n\nX_train, X_val, y_train, y_val = train_test_split(training_set[FEATURES] , prediction_set, test_size=0.4)\n\ntrain_set_tensor = torch.utils.data.TensorDataset(torch.FloatTensor(X_train.values), torch.FloatTensor(y_train.values))\nval_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_val.values), torch.FloatTensor(y_val.values))\n\nbatch_size = 8\ntrain_loader = torch.utils.data.DataLoader(train_set_tensor,batch_size=batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_set,batch_size=batch_size)\n\n# test_loader = torch.utils.data.DataLoader(torch.FloatTensor(test.values),batch_size=batch_size)\n# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)","50e005b8":"# Hyperparameters\nbatch_no = len(X_train) \/\/ batch_size  #batches\ncols=X_train.shape[1] #Number of columns in input matrix\nn_output=1\n\n# Sequence Length\nsequence_length = 6  # of words in a sequence 892110\n# Batch Size\n# batch_size = 128\n# train_loader = batch_data(int_text, sequence_length, batch_size)\n# Number of Epochs\nnum_epochs = 1000\n# Learning Rate\nlearning_rate = 0.002\n# Model parameters\n# Input size\ninput_size = cols\n# Output size\noutput_size = 1\n# Embedding Dimension\nembedding_dim = 128\n# Hidden Dimension\nhidden_dim = 256\n# Number of RNN Layers\nn_layers = 2\n\n# Show stats for every n number of batches\nshow_every_n_batches = 50","01cbe53b":"import torch.nn as nn\n\nclass LSTMClassifier(nn.Module):\n    \"\"\"\n    This is the simple RNN model we will be using to perform Sentiment Analysis.\n    \"\"\"\n\n    def __init__(self, embedding_dim, hidden_dim, input_size, n_layers, output_size, dropout=0.5):\n        \"\"\"\n        Initialize the model by settingg up the various layers.\n        \"\"\"\n        super(LSTMClassifier, self).__init__()\n\n        # self.embedding = nn.Embedding(input_size, embedding_dim)\n        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n        self.dense = nn.Linear(in_features=hidden_dim, out_features=output_size)\n        self.sig = nn.Sigmoid()        \n        # self.word_dict = None\n        \n        # self.fc1 = nn.Linear(input_size, hidden_dim * 2)\n        # self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n        # self.fc3 = nn.Linear(hidden_dim, output_size)\n        # self.dropout = nn.Dropout(p=0.25)\n        self.init_weights()\n        \n    def init_weights(self):\n        initrange = 0.08\n        # self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.lstm.weight_ih_l0.data.uniform_(-initrange, initrange)\n        self.lstm.weight_hh_l0.data.uniform_(-initrange, initrange)\n        \n        self.lstm.bias_ih_l0.data.zero_()\n        self.lstm.bias_hh_l0.data.zero_()\n        \n        # self.fc.bias.data.zero_()\n        self.dense.bias.data.fill_(0)\n        # self.fc.weight.data.uniform_(-initrange, initrange)\n        self.dense.weight.data.normal_(0.0, (1.0 \/ np.sqrt(self.dense.in_features)))\n        \n    def forward(self, x):\n        \"\"\"\n        Perform a forward pass of our model on some input.\n        \"\"\"\n        batch_size = x.size(0)\n        # print(x.shape)\n        # x = x.permute(14, 32)\n        # x = F.relu(self.fc1(x))\n        # x = self.dropout(x)\n        # x = F.relu(self.fc2(x))\n        # x = self.dropout(x)\n        # out = self.fc3(x)\n        # x = x.permute(2, 0, 1)\n        # print(batch_size)\n        # print(x.shape)\n        # x = x.t()\n        # embeds = self.embedding(x)\n        lstm_out, _ = self.lstm(x)\n        # avg_pool_l = torch.mean(lstm_out.permute(1, 0, 2), 1)\n        # max_pool_l, _ = torch.max(lstm_out.permute(1, 0, 2), 1)\n        # print(avg_pool_l)\n        # x = torch.cat((avg_pool_l, max_pool_l), 1)\n        # print(x.shape)\n        out = self.dense(lstm_out)\n        # out = self.sig(out.squeeze())\n        # print(out)\n        # out = out[lengths - 1, range(len(lengths))]\n        return out","b9a59ee5":"from torch.autograd import Variable\n\ndef forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden_dim, clip=9):\n\n    if(train_on_gpu):\n        inputs, labels = inputs.cuda(), labels.cuda()\n\n    hidden = {}\n    # hidden = tuple([each.data for each in hidden_dim])\n    \n    rnn.zero_grad()\n    optimizer.zero_grad()\n    \n    try:\n        # get the output from the model\n        # output, hidden = rnn(inputs, hidden)\n        output = rnn(inputs.unsqueeze(0))\n        output = output.squeeze()\n        # print(output.shape)\n    except RuntimeError:\n        raise\n    # print(labels)\n    loss = criterion(output, labels)\n    loss.backward()\n    \n    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs \/ LSTMs.\n    nn.utils.clip_grad_norm_(rnn.parameters(),  clip)\n   \n    optimizer.step()\n\n    return loss.item(), hidden","a2c62cdd":"def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n    batch_losses = []\n    val_batch_losses = []\n    valid_loss_min = np.Inf\n    \n    rnn.train()\n    \n    previousLoss = np.Inf\n    minLoss = np.Inf\n\n    print(\"Training for %d epoch(s)...\" % n_epochs)\n    for epoch_i in range(1, n_epochs + 1):\n        \n        # initialize hidden state\n        # hidden = rnn.init_hidden(batch_size)\n        hidden = {}\n        # print(\"epoch \",epoch_i)\n        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n            # batch_last = batch_i\n            # n_batches = len(train_loader.dataset) \/\/ batch_size\n            \n            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden, clip=5)          \n            # record loss\n            batch_losses.append(loss)\n            \n        for batch_i, (inputs, labels) in enumerate(val_loader, 1):\n            # batch_last = batch_i\n            # n_batches = len(val_loader.dataset) \/\/ batch_size\n            # if(batch_i > n_batches):\n                # break\n            \n            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden, clip=5)          \n            # record loss\n            val_batch_losses.append(loss)\n\n        # printing loss stats\n        if epoch_i%show_every_n_batches == 0:\n            average_loss = np.average(batch_losses)\n            val_average_loss = np.average(val_batch_losses)\n            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch_i, average_loss, val_average_loss))\n\n            ## TODO: save the model if validation loss has decreased\n            # save model if validation loss has decreased\n            if val_average_loss <= valid_loss_min:\n                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n                valid_loss_min,\n                val_average_loss))\n                with open('.\/save\/trained_rnn_new', 'wb') as pickle_file:\n                    # print(pickle_file)\n                    torch.save(rnn, pickle_file)\n                valid_loss_min = val_average_loss\n\n            batch_losses = []\n            val_batch_losses = []\n            \n    return rnn","3acbcfbf":"train_on_gpu = torch.cuda.is_available()\nif not train_on_gpu:\n    print('No GPU found. Please use a GPU to train your neural network.')","edf66b85":"# create model and move to gpu if available\n# rnn = RNN(input_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.25)\n# rnn.apply(weight_init)\nrnn = LSTMClassifier(embedding_dim, hidden_dim, input_size, n_layers, output_size)\n# rnn = torch.load(\".\/save\/trained_rnn_new\")\n\nif train_on_gpu:\n    rnn.cuda()\n\ndecay_rate = learning_rate \/ num_epochs\n\n# print(decay_rate)\n# defining loss and optimization functions for training\noptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n# optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate, momentum=0.9, weight_decay=decay_rate)\n\n# criterion = nn.CrossEntropyLoss()\ncriterion = nn.MSELoss(size_average=False)\n# rnn = helper.load_model('.\/save\/trained_rnn_new')\n\n# training the model\ntrained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n\n# saving the trained model\n# helper.save_model('.\/save\/trained_rnn', trained_rnn)\nprint('Model Trained and Saved')","6dd16980":"def predict(model, inputs):\n\n    if(train_on_gpu):\n        inputs = inputs.cuda()\n    \n    try:\n        output = model(inputs.unsqueeze(0))\n        output = output.squeeze()\n    except RuntimeError:\n        raise\n    \n    # prediction = np.array(output).argmax(0)\n    # p = F.softmax(output, dim=1).data\n    # p = F.sigmoid(output)\n    # p = F.logsigmoid(output)\n    p = output.cpu().detach().numpy().flatten()\n    # print(p)\n    # prediction = np.argmax(p)\n    # print(prediction)\n    return p","f33be748":"model_rnn = torch.load(\"save\/trained_rnn_new\")\nmodel_rnn.eval()\n\nX = Variable(torch.FloatTensor(X_train.values)) \npred = predict(model_rnn, X)\nprint(pred[:30])\n# pred= result\nprint(y_train.values[:30])\nr2_score(y_train.values, pred)\n\n# probs = probs[:, 1]\n# calculate AUC\n# auc = roc_auc_score(y_train, pred)\n# print('AUC: %.3f' % auc)\n# calculate roc curve\n# fpr, tpr, thresholds = roc_curve(y_train, pred)\n# plot no skill\n# pyplot.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\n# pyplot.plot(fpr, tpr, marker='.')\n# show the plot\n# pyplot.show()\n","5b4ba088":"display(test_set.head())","c69067a6":"test_X = Variable(torch.FloatTensor(test_set.values))\nprint(test_X)\ntest_pred = predict(model_rnn, test_X)\nprint(test_pred)\nprint(len(test_pred))\n# print(np.array(test_p).reshape(9614,1))","27569be9":"predictions = pd.DataFrame(test_csv[\"Id\"], columns = [\"Id\"])\npredictions[\"SalePrice\"] = prepro_y.inverse_transform(np.array(test_pred).reshape(1459,1))\n# predictions = pd.DataFrame(np.array(test_pred).reshape(8037,1), columns = [\"FORECLOSURE\"])\n# predictions[\"FORECLOSURE\"] = predictions[\"FORECLOSURE\"]\n# predictions['SalePrice'] = predictions['SalePrice']\n# predictions['FORECLOSURE'] = predictions['FORECLOSURE'].apply(lambda x: 0 if x < 0.01 else 1)\n# predictions['FORECLOSURE'] = predictions['FORECLOSURE'].apply(lambda x: 1 if x > 0 else x)\n# predictions = predictions.round(2)\n# predictions[\"Id\"] = test_csv[\"Id\"]\ndisplay(predictions.head())","a98df74f":"predictions.to_csv(\"submission.csv\")","6a2f2fa3":"House Prices regression using Deep Learning - beginner"}}