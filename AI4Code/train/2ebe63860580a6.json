{"cell_type":{"9f821155":"code","59804f1d":"code","69de75e6":"code","edb7552f":"code","df165909":"code","70ff2615":"code","2c2cb7e5":"code","c30df6eb":"code","06e4e5ec":"code","c8cde25d":"code","00e95f06":"code","501a7d83":"code","2e85b757":"code","370a76bd":"code","e50b4f68":"code","9a9daba8":"code","232925d5":"markdown","16036c7c":"markdown","6fc66afc":"markdown","f9fba9b0":"markdown","b00e4896":"markdown","a88ad97f":"markdown"},"source":{"9f821155":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","59804f1d":"data = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","69de75e6":"data","edb7552f":"data.info()","df165909":"def onehot_encode(df, column):\n    df = df.copy()\n    dummies = pd.get_dummies(df[column], prefix=column)\n    df = pd.concat([df, dummies], axis=1)\n    df = df.drop(column, axis=1)\n    return df","70ff2615":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    # Drop id column\n    df = df.drop('id', axis=1)\n    \n    # Binary encoding\n    df['ever_married'] = df['ever_married'].replace({'No': 0, 'Yes': 1})\n    df['Residence_type'] = df['Residence_type'].replace({'Rural': 0, 'Urban': 1})\n    \n    # One-hot encoding\n    for column in ['gender', 'work_type', 'smoking_status']:\n        df = onehot_encode(df, column=column)\n    \n    # Split df into X and y\n    y = df['stroke']\n    X = df.drop('stroke', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    \n    # KNN imputation of missing values\n    imputer = KNNImputer()\n    imputer.fit(X_train)\n    X_train = pd.DataFrame(imputer.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(imputer.transform(X_test), index=X_test.index, columns=X_test.columns)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n    \n    return X_train, X_test, y_train, y_test","2c2cb7e5":"X_train, X_test, y_train, y_test = preprocess_inputs(data)","c30df6eb":"X_train","06e4e5ec":"y_train","c8cde25d":"models = {\n    \"                   Logistic Regression\": LogisticRegression(),\n    \"                   K-Nearest Neighbors\": KNeighborsClassifier(),\n    \"                         Decision Tree\": DecisionTreeClassifier(),\n    \"Support Vector Machine (Linear Kernel)\": LinearSVC(),\n    \"   Support Vector Machine (RBF Kernel)\": SVC(),\n    \"                        Neural Network\": MLPClassifier(),\n    \"                         Random Forest\": RandomForestClassifier(),\n    \"                     Gradient Boosting\": GradientBoostingClassifier(),\n    \"                               XGBoost\": XGBClassifier(eval_metric='mlogloss'),\n    \"                              LightGBM\": LGBMClassifier(),\n    \"                              CatBoost\": CatBoostClassifier(verbose=0)\n}\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    print(name + \" trained.\")","00e95f06":"y_train.value_counts()","501a7d83":"print(\"Model Performance\\n-----------------\")\nfor name, model in models.items():\n    y_pred = model.predict(X_test)\n    print(\n        \"\\n\" + name + \" Accuracy: {:.3f}%\\n\\t\\t\\t\\t       F1-Score: {:.5f}\"\\\n        .format(accuracy_score(y_test, y_pred) * 100, f1_score(y_test, y_pred))\n    )","2e85b757":"oversampled_data = pd.concat([X_train, y_train], axis=1).copy()\n\nnum_samples = y_train.value_counts()[0] - y_train.value_counts()[1]\nnew_samples = oversampled_data.query(\"stroke == 1\").sample(num_samples, replace=True, random_state=1)\n\noversampled_data = pd.concat([oversampled_data, new_samples], axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)\n\ny_train_oversampled = oversampled_data['stroke']\nX_train_oversampled = oversampled_data.drop('stroke', axis=1)","370a76bd":"models = {\n    \"                   Logistic Regression\": LogisticRegression(),\n    \"                   K-Nearest Neighbors\": KNeighborsClassifier(),\n    \"                         Decision Tree\": DecisionTreeClassifier(),\n    \"Support Vector Machine (Linear Kernel)\": LinearSVC(),\n    \"   Support Vector Machine (RBF Kernel)\": SVC(),\n    \"                        Neural Network\": MLPClassifier(),\n    \"                         Random Forest\": RandomForestClassifier(),\n    \"                     Gradient Boosting\": GradientBoostingClassifier(),\n    \"                               XGBoost\": XGBClassifier(eval_metric='mlogloss'),\n    \"                              LightGBM\": LGBMClassifier(),\n    \"                              CatBoost\": CatBoostClassifier(verbose=0)\n}\n\nfor name, model in models.items():\n    model.fit(X_train_oversampled, y_train_oversampled)\n    print(name + \" trained.\")","e50b4f68":"y_train_oversampled.value_counts()","9a9daba8":"print(\"Model Performance\\n-----------------\")\nfor name, model in models.items():\n    y_pred = model.predict(X_test)\n    print(\n        \"\\n\" + name + \" Accuracy: {:.3f}%\\n\\t\\t\\t\\t       F1-Score: {:.5f}\"\\\n        .format(accuracy_score(y_test, y_pred) * 100, f1_score(y_test, y_pred))\n    )","232925d5":"# Task for Today  \n\n***\n\n## Patient Stroke Prediction  \n\nGiven *medical patient data*, let's try to predict if a given patient will have a **stroke**.\n\nWe will use a variety of classification models to make our predictions.","16036c7c":"# Handling Class Imbalance With Oversampling","6fc66afc":"# Getting Started","f9fba9b0":"# Preprocessing","b00e4896":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/ZY5SK-A_myc","a88ad97f":"# Training"}}