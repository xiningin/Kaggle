{"cell_type":{"28f913c6":"code","27dcfd24":"code","70cc5026":"code","49df6e12":"code","c2f2e2d5":"code","eef923be":"code","bd2f3926":"code","5a30f4ae":"code","9e397496":"code","236281d5":"code","0c7689ca":"code","93ccd1e1":"code","d430edbc":"code","e74c90be":"code","277310e7":"code","1aa077b5":"code","908c3dd4":"code","8a54112b":"code","3a940e80":"code","53c3cc4e":"code","374dcbe8":"code","f9d4d332":"code","87a5fa64":"code","5f025464":"code","eb21b62d":"code","8863031f":"code","a96d2273":"code","868e857b":"code","6c2e2135":"code","8136100e":"code","e2b0441f":"code","c9de34aa":"code","378fd926":"code","c0fe4170":"code","9cf63f6d":"code","6d872178":"code","81986a7a":"code","6a822303":"code","a9a912b4":"code","ee8c0ff6":"code","c530a7e1":"code","9a08b8ae":"code","ccedac58":"code","273fa435":"code","578f0913":"code","aed3b3ac":"code","5d0dae55":"code","2b25157f":"code","5f94eaba":"code","5779aed1":"code","f18a7259":"code","1fbd8bdf":"code","e5f89339":"code","81e038fe":"code","6db3f98f":"code","b8a0239f":"code","0013532c":"code","e19871b3":"code","f9365855":"code","aa47323f":"code","94d08930":"markdown","ffbfe286":"markdown","86741966":"markdown","e768ac10":"markdown","d326a7fb":"markdown","2b21cb03":"markdown","5ea6cda8":"markdown","4f5ae79c":"markdown","41a30ea0":"markdown","9ea7901e":"markdown","0aa510bd":"markdown","453c3040":"markdown","9dcf9640":"markdown","3467fcbf":"markdown","0aaddc39":"markdown","f1ba04ca":"markdown","07462e7d":"markdown","20816e14":"markdown","3d0037a3":"markdown","79dcc07c":"markdown","92302e9f":"markdown","47259d27":"markdown","217f7374":"markdown","04460131":"markdown","fc006ede":"markdown","3662c65a":"markdown","c314ba60":"markdown","4ba5475f":"markdown","a46c2591":"markdown","f9b99864":"markdown","762daf5b":"markdown","02cc4a73":"markdown","44e10b3c":"markdown"},"source":{"28f913c6":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom scipy import stats\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info","27dcfd24":"items = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ncats = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntrain = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\n# set index to ID to avoid droping it later\ntest  = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')","70cc5026":"train.head()","49df6e12":"def knowningData(df, limit=5): #seting the function with df, \n    print(f\"Dataset Shape: {df.shape}\")\n    print('Unique values per column: ')\n    print(df.nunique())\n    print(\"################\")\n    print(\"\")    \n    for column in df.columns: #initializing the loop\n        print(\"Column Name: \", column )\n        entropy = round(stats.entropy(df[column].value_counts(), base=2),2)\n        # scipy.stats.entropy \u60c5\u5831\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3000\u57fa\u5e95\uff12\n        print(\"entropy \", entropy, \n              \" | Total nulls: \", (round(df[column].isnull().sum() \/ len(df[column]) * 100,3)),\n              \" | Total unique values: \", df.nunique()[column], #print the data and % of nulls\n              \" | Missing: \", df[column].isna().sum(),\n              \"\\nMin: \", df[column].min(),\n              \" | Max: \", df[column].max())\n        print(\"Top 5 most frequent values: \")\n        print(round(df[column].value_counts()[:limit] \/ df[column].value_counts().sum() * 100,2))\n        # value_counts()\u306f\u3001\u30e6\u30cb\u30fc\u30af\u306a\u8981\u7d20\u306e\u5024\u304cindex\u3001\u305d\u306e\u51fa\u73fe\u500b\u6570\u304cdata\u3068\u306a\u308bpandas.Series\u3092\u8fd4\u3059\u3002\n        # \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u51fa\u73fe\u56de\u6570\u304c\u591a\u3044\u3082\u306e\u304b\u3089\u9806\u306b\u30bd\u30fc\u30c8\u3055\u308c\u308b\uff08\u964d\u9806\uff09\u3002\n        # normalize=True\u3067\u5272\u5408\u304c\u51fa\u305b\u308b\u304c\u4f7f\u3063\u3066\u3044\u306a\u3044\u3002\n        print(\"\")\n        print(\"####################################\")","c2f2e2d5":"knowningData(train)","eef923be":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)","bd2f3926":"train = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1001]","5a30f4ae":"# check Knowning Data\ntrain[train.item_price<0]","9e397496":"# Substitute the median of the same year \/ month \/ shop_id \/ item_id to median\n# \u540c\u3058\u5e74\u6708\/shop_id\/item_id\u306e\u4e2d\u592e\u5024\u3092 median \u306b\u4ee3\u5165\nmedian = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\ntrain.loc[train.item_price<0, 'item_price'] = median","236281d5":"# look all shops data\nshops","0c7689ca":"# unify shop_id 0 to shop_id 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# unify shop_id 1 to shop_id 58\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# unify shop_id 10 to shop_id 11\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","93ccd1e1":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\n# shop_name\u306e\u5148\u982d\u3092\u62bd\u51fa\u3057\u3066shop\u306b\u65b0\u305f\u306a\u5217[city(\u90fd\u5e02)]\u3092\u8ffd\u52a0\u3057\u307e\u3059\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\n# city\u306e\u5148\u982d\u306b[!]\u304c\u30b4\u30df(\u30bf\u30a4\u30dd\u3089\u3057\u3044)\u3068\u3057\u3066\u5165\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u306e\u3067\u524a\u9664\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\n# Create a city_code feature using LabelEncoder.\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]","d430edbc":"shops.head()","e74c90be":"cats.head()","277310e7":"# Create a type_code and subtype_code feature using LabelEncoder.\ncats['split'] = cats['item_category_name'].str.split('-')\ncats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n# if subtype is nan then type\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]","1aa077b5":"cats.head()","908c3dd4":"items.head()","8a54112b":"items.drop(['item_name'], axis=1, inplace=True)","3a940e80":"knowningData(test)","53c3cc4e":"print(\"The number of new item_id compared to the train is\",\\\n      len(list(set(test.item_id) - set(test.item_id).intersection(set(train.item_id)))))","374dcbe8":"ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    # \u76f4\u7a4d\uff08\u30c7\u30ab\u30eb\u30c8\u7a4d\uff09\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\n# type cast\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\ntime.time() - ts","f9d4d332":"matrix.head()","87a5fa64":"group = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\n# change last column name as sum within date_block_num\ngroup.columns = ['item_cnt_month']\n# reset index as normal dataframe\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)  # fill NA\/NaN values with 0\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))\nmatrix.head()","5f025464":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","eb21b62d":"# set 34 as November 2015\ntest['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)\n\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month\nmatrix.head()","8863031f":"matrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\n\nmatrix.head()","a96d2273":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","868e857b":"ts = time.time()\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')\ntime.time() - ts","6c2e2135":"matrix.head()","8136100e":"# mean of item_cnt_month for each date_block_num\n\nts = time.time()\ngroup = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)\n\ntime.time() - ts","e2b0441f":"# mean of item_cnt_month for each date_block_num and item_id\n\nts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","c9de34aa":"# mean of item_cnt_month for each date_block_num and shop_id\n\nts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","378fd926":"# mean of item_cnt_month for each date_block_num and item_category_id\n\nts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","c0fe4170":"# mean of item_cnt_month for each date_block_num, shop_id and item_category_id\n\nts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","9cf63f6d":"# mean of item_cnt_month for each date_block_num, shop_id and type_code\n\nts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_type_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'type_code'], how='left')\nmatrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_type_avg_item_cnt')\nmatrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","6d872178":"# mean of item_cnt_month for each date_block_num, shop_id and subtype_code\n\nts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","81986a7a":"# mean of item_cnt_month for each date_block_num and city_code\n\nts = time.time()\ngroup = matrix.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'city_code'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","6a822303":"# mean of item_cnt_month for each date_block_num, item_id and city_code\n\nts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","a9a912b4":"# mean of item_cnt_month for each date_block_num and type_code\n\nts = time.time()\ngroup = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'type_code'], how='left')\nmatrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\nmatrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","ee8c0ff6":"# mean of item_cnt_month for each date_block_num and subtype_code\n\nts = time.time()\ngroup = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_subtype_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","c530a7e1":"ts = time.time()\ngroup = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) \/ matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\n# https:\/\/stackoverflow.com\/questions\/31828240\/first-non-null-value-per-row-from-a-list-of-pandas-columns\/31828559\n# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n# Invalid dtype for backfill_2d [float16]\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)\n\ntime.time() - ts","9a08b8ae":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\ntime.time() - ts","ccedac58":"# save file to prevent for timeout\n#matrix.to_csv(\"matrix1.csv\")","273fa435":"# load file to prevent for timeout\n#matrix = pd.read_csv('..\/input\/matrix1\/matrix1.csv')\n#matrix.head()","578f0913":"matrix['month'] = matrix['date_block_num'] % 12","aed3b3ac":"days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix['days'] = matrix['month'].map(days).astype(np.int8)","5d0dae55":"ts = time.time()\ncache = {}\nmatrix['item_shop_last_sale'] = -1\nmatrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = str(row.item_id)+' '+str(row.shop_id)\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n        cache[key] = row.date_block_num         \ntime.time() - ts","2b25157f":"ts = time.time()\ncache = {}\nmatrix['item_last_sale'] = -1\nmatrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = row.item_id\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        if row.date_block_num>last_date_block_num:\n            matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n            cache[key] = row.date_block_num         \ntime.time() - ts","5f94eaba":"ts = time.time()\nmatrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\ntime.time() - ts","5779aed1":"ts = time.time()\nmatrix = matrix[matrix.date_block_num > 11]\ntime.time() - ts","f18a7259":"ts = time.time()\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)\ntime.time() - ts","1fbd8bdf":"matrix.columns","e5f89339":"matrix.to_pickle('data.pkl')\ndel matrix\ndel cache\ndel group\ndel items\ndel shops\ndel cats\ndel train\n# leave test for submission\ngc.collect();","81e038fe":"data = pd.read_pickle('..\/input\/datapkl\/data.pkl')\ndata.head()","6db3f98f":"data = data[[\n    'date_block_num',\n    'shop_id',\n    'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code',\n    'subtype_code',\n    'item_cnt_month_lag_1',\n    'item_cnt_month_lag_2',\n    'item_cnt_month_lag_3',\n    'item_cnt_month_lag_6',\n    'item_cnt_month_lag_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_2',\n    'date_item_avg_item_cnt_lag_3',\n    'date_item_avg_item_cnt_lag_6',\n    'date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1',\n    'date_shop_avg_item_cnt_lag_2',\n    'date_shop_avg_item_cnt_lag_3',\n    'date_shop_avg_item_cnt_lag_6',\n    'date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    #'date_shop_type_avg_item_cnt_lag_1',\n    #'date_shop_subtype_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    #'date_type_avg_item_cnt_lag_1',\n    #'date_subtype_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month',\n    'days',\n    'item_shop_last_sale',\n    'item_last_sale',\n    'item_shop_first_sale',\n    'item_first_sale',\n]]","b8a0239f":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","0013532c":"del data\n#gc.collect();","e19871b3":"ts = time.time()\n\n# Parameters are the result of Hyperparameter tuning.\nmodel = XGBRegressor(\n    max_depth=8, # \u6728\u306e\u6df1\u3055\u306e\u4e0a\u9650\n    n_estimators=1000, # \u52fe\u914d\u30d6\u30fc\u30b9\u30c8\u3055\u308c\u305f\u30c4\u30ea\u30fc\u306e\u6570\u3002\u30d6\u30fc\u30b9\u30c6\u30a3\u30f3\u30b0\u30e9\u30a6\u30f3\u30c9\u306e\u6570\u306b\u76f8\u5f53\u3002\n    min_child_weight=300, # \u8449\u306e\u91cd\u307f\u306e\u4e0b\u9650\n    colsample_bytree=0.8, # \u5404\u30b9\u30c6\u30c3\u30d7\u306e\u6c7a\u5b9a\u6a5f\n    subsample=0.8, # \u5404\u30b9\u30c6\u30c3\u30d7\u306e\u6c7a\u5b9a\u6728\u306e\u69cb\u7bc9\u306b\u7528\u3044\u308b\u30c7\u30fc\u30bf\u306e\u5272\u5408\n    eta=0.3,    # learning_rate\n    seed=42) # random seed \u3092\u8a2d\u5b9a\u3057\u305f\u6642\u70b9\u3092\u30b9\u30bf\u30fc\u30c8\u3068\u3057\u3066\u3001\u540c\u3058\u64ec\u4f3c\u4e71\u6570\u5217\u304c\u751f\u6210\u3055\u308c\u308b\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\ntime.time() - ts","f9365855":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","aa47323f":"plot_features(model, (10,14))","94d08930":"Price trend for the last six months.<br>\nThis is difference between the average after lag_number months and the overall average.","ffbfe286":"There are items with strange prices and sales. After detailed exploration I decided to remove items with price > 100000 and sales > 1001 (1000 is ok).<br>\nThis is done not to correct inaccurate data but to increase the accuracy of the prediction.","86741966":"## Final preparations\uff08\u6700\u7d42\u6e96\u5099\uff09\nBecause of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set).","e768ac10":"## Test set\nTo use time tricks append test pairs to the matrix.","d326a7fb":"# Part 1, perfect features\uff08\u7279\u5fb4\u91cf\u306e\u5b8c\u5099\uff09","2b21cb03":"# Part 2, xgboost","5ea6cda8":"## Data Cleaning\uff08\u30af\u30ea\u30fc\u30f3\u30c7\u30fc\u30bf\uff09","4f5ae79c":"## Monthly sales","41a30ea0":"Test set is a product of some shops and some items within 34 month. There are 5100 items * 42 shops = 214200 pairs. 363 items are new compared to the train. Hence, for the most of the items in the test set target value should be zero. \nIn the other hand train set contains only pairs which were sold or returned in the past. Tha main idea is to calculate monthly sales and <b>extend it with zero sales<\/b> for each unique pair within the month. This way train data will be similar to test data.","9ea7901e":"Last month shop revenue trend\n","0aa510bd":"Aggregate train set by shop\/item pairs to calculate target aggreagates, then <b>clip(0,20)<\/b> target value. This way train target will be similar to the test predictions.\n\n<i>I use floats instead of ints for item_cnt_month to avoid downcasting it after concatination with the test set later. If it would be int16, after concatination with NaN values it becomes int64, but foat16 becomes float16 even with NaNs.<\/i>","453c3040":"Months since the last sale for each shop\/item pair and for item only. I use programing approach.\n\n<i>Create HashTable with key equals to {shop_id,item_id} and value equals to date_block_num. Iterate data from the top. Foreach row if {row.shop_id,row.item_id} is not present in the table, then add it to the table and set its value to row.date_block_num. if HashTable contains key, then calculate the difference beteween cached value and row.date_block_num.<\/i>","9dcf9640":"Several shops are duplicates of each other (according to its name). Fix train and test set.","3467fcbf":"\nSimilarly, it is possible that the same item_name is registered with a different item_id, but this idea is suspended.<br>\n\u540c\u69d8\u306b\u3001\u540c\u3058item_name\u3092\u5225item_id\u3067\u767b\u9332\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u3082\u8003\u3048\u3089\u308c\u308b\u304c\u3001\u3053\u306e\u30a2\u30a4\u30c7\u30a2\u306f\u4fdd\u7559\u3059\u308b\u3002","0aaddc39":"## Shops\/Cats\/Items preprocessing\uff08\u7279\u5fb4\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0\uff09\n The immediate goal is to merge items, shops and tests into the train.<br>\n As preprocessing, encode categorical variables into numeric.\n\nObservations:\n* Each shop_name starts with the city name.\n* Each category contains type and subtype in its name.","f1ba04ca":"# The remaining Challenge\uff08\u6b8b\u8ab2\u984c\uff09\n* Data Leakage\n* Eensumble\n* ","07462e7d":"Select perfect features","20816e14":"## Trend features\uff08\u30c8\u30ec\u30f3\u30c9\uff09","3d0037a3":"No useful function found for item_name. Therefore, delete item_name.","79dcc07c":"This data indicates that item_category_name is named type_subtype.<br> \nSo let's add the type \/ subtype column from the cats item_category_name.","92302e9f":"matrix.info()","47259d27":"Validation strategy is 34 month for the test set, 33 month for the validation set and 13-33 months for the train.","217f7374":"## Check and Remove Outliers \uff08\u5916\u308c\u5024\u306e\u78ba\u8a8d\u3068\u9664\u5916\uff09","04460131":"Number of days in a month. There are no leap years.","fc006ede":"# Predict Future Sales\n\nThis notebook is simpified version of the final project in the [How to Win a Data Science Competition: Learn from Top Kagglers](https:\/\/www.coursera.org\/learn\/competitive-data-science) course.\n\nThis kernel is created with reference to [Feature engineering, xgboost](https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost).\n\nThis notebook also explains in Japanese for Japanese.<br>\n\u65e5\u672c\u4eba\u5411\u3051\u306e\u65e5\u672c\u8a9e\u3067\u306e\u8aac\u660e\u3082\u3057\u3066\u3044\u307e\u3059\u3002\n\n#### Pipline\uff08\u624b\u9806\uff09\n* load data\n* heal data and remove outliers\n* work with shops\/items\/cats objects and features\n* create matrix as product of item\/shop pairs within each month in the train set\n* get monthly sales for each item\/shop pair in the train set and merge it to the matrix\n* clip item_cnt_month by (0,20)\n* append test to the matrix, fill 34 month nans with zeros\n* merge shops\/items\/cats to the matrix\n* add target lag features\n* add mean encoded features\n* add price trend features\n* add month\n* add days\n* add months since last sale\/months since first sale features\n* cut first year and drop columns which can not be calculated for the test set\n* select best features\n* set validation strategy 34 test, 33 validation, less than 33 train\n* fit the model, predict and clip targets for the test set","3662c65a":"## Knowning Data\uff08\u30c7\u30fc\u30bf\u3092\u77e5\u308b\uff09","c314ba60":"## Merging Shops\/Items\/Cats features\uff08\u7279\u5fb4\u91cf\u306e\u7d50\u5408\uff09","4ba5475f":"Months since the first sale for each shop\/item pair and for item only.","a46c2591":"Producing lags brings a lot of nulls.","f9b99864":"There is one item with price below zero. Fill it with median.","762daf5b":"## Special features","02cc4a73":"## Target lags\uff08\u30e9\u30b0\u8ffd\u52a0\uff09\n\n\nExpected to contribute to time-series prediction, and add the value after 1,2,3,6,12 months to the record.<br>\n\u6642\u7cfb\u5217\u4e88\u6e2c\u306b\u5bc4\u4e0e\u3059\u308b\u3068\u671f\u5f85\u3057\u30011,2,3,6,12\u6708\u5f8c\u306e\u5024\u3092\u30ec\u30b3\u30fc\u30c9\u306b\u8ffd\u52a0\u3059\u308b\u3002","44e10b3c":"## Mean encoded features\uff08\u5e73\u5747\u91cf\uff09"}}