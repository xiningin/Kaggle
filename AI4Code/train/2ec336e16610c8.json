{"cell_type":{"7e5d95c7":"code","51b7d984":"code","48a732ff":"code","9dc6e4d2":"code","9c9fddff":"code","3113ecf9":"code","48cc8f36":"code","11b51710":"code","a6e4956f":"code","c82d9dd2":"code","c74feffc":"code","37f7320c":"code","d4f57019":"code","b30e8a05":"code","c09a9597":"code","508f562d":"code","4860f641":"code","89b97aa2":"code","181fd4ea":"code","18f7cae3":"code","74a1e424":"code","4ed2938f":"code","bdcd5cd3":"code","8e6b9fe6":"code","8ec3dc2e":"code","da85663c":"code","1b1a0707":"markdown","870e8c0d":"markdown","1955c257":"markdown","1a41d9ec":"markdown","bbf3aece":"markdown","d37ec9ea":"markdown","2f9dc9ac":"markdown","2fa99d82":"markdown","476cf8f2":"markdown","54a6302e":"markdown"},"source":{"7e5d95c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","51b7d984":"!pip install xgboost graphviz lightgbm scikit-learn xgboost lightgbm --upgrade --quiet","48a732ff":"# Import Data\ntrain_df=pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\nsubmission_df=pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\nprint('The train dataset contains {} rows and {} columns.'.format( len(train_df.index), len(train_df.columns)))\nprint('The test dataset contains {} rows and {} columns.'.format( len(test_df.index), len(test_df.columns)))","9dc6e4d2":"input_cols =list(train_df.columns)[1:-1]\ntarget_cols='target'\ninputs_df = train_df[input_cols].copy()\ntargets = train_df[target_cols].copy()\ntest_df = test_df[input_cols].copy()\ntest_df","9c9fddff":"from sklearn.metrics import mean_squared_error\n# Extracting Numeric and Categorical cols from data\nnumeric_cols=inputs_df.select_dtypes(exclude=['object']).columns.tolist()\ncategorical_cols = inputs_df.select_dtypes('object').columns.tolist()","3113ecf9":"%%time\nfrom sklearn.preprocessing import OrdinalEncoder,StandardScaler\n# Encode Categorical Columns\nordinal_encoder = OrdinalEncoder()\ninputs_df[categorical_cols] = ordinal_encoder.fit_transform(inputs_df[categorical_cols])\ntest_df[categorical_cols] = ordinal_encoder.transform(test_df[categorical_cols])\n    \n# standarization numerical cols\nscaler = StandardScaler()\ninputs_df[numeric_cols] = scaler.fit_transform(inputs_df[numeric_cols])\ntest_df[numeric_cols] = scaler.transform(test_df[numeric_cols])","48cc8f36":"train_df","11b51710":"%%time\n# Train Model\nfrom xgboost import XGBRegressor\nmodel = XGBRegressor(random_state=42, n_jobs=-1,tree_method='gpu_hist', n_estimators=20, max_depth=4)\nmodel.fit(inputs_df, targets)","a6e4956f":"preds = model.predict(test_df)","c82d9dd2":"from sklearn.metrics import mean_squared_error\n\ndef rmse(a, b):\n    return mean_squared_error(a, b, squared=False)","c74feffc":"import matplotlib.pyplot as plt\n%matplotlib inline\n# Plot target columns\nplt.hist(train_df.target.sample(10000));","37f7320c":"import matplotlib.pyplot as plt\nfrom xgboost import plot_tree\nfrom matplotlib.pylab import rcParams\n%matplotlib inline\n\nrcParams['figure.figsize'] = 30,30","d4f57019":"plot_tree(model, rankdir='LR', num_trees=19);","b30e8a05":"#Notice how the trees only compute residuals, and not the actual target value.\ntrees = model.get_booster().get_dump()\nlen(trees)","c09a9597":"# We can also visualize the tree as text.\nprint(trees[0])","508f562d":"importance_df = pd.DataFrame({\n    'feature': inputs_df.columns,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)","4860f641":"importance_df.head(10)","89b97aa2":"import seaborn as sns\n# Plot Top 10 Features based upon  feature importance score.\nplt.figure(figsize=(10,6))\nplt.title('Feature Importance')\nsns.barplot(data=importance_df.head(10), x='importance', y='feature');","181fd4ea":"from sklearn.model_selection import KFold","18f7cae3":"def train_and_evaluate(X_train, train_targets, X_val, val_targets, **params):\n    model = XGBRegressor(random_state=42, n_jobs=-1,tree_method='gpu_hist', **params)\n    model.fit(X_train, train_targets)\n    train_rmse = rmse(model.predict(X_train), train_targets)\n    val_rmse = rmse(model.predict(X_val), val_targets)\n    return model, train_rmse, val_rmse","74a1e424":"import numpy as np\ndef test_params_kfold(n_splits,inputs,**params):\n    train_rmses, val_rmses, models = [], [], []\n    kfold = KFold(n_splits)\n    for train_idxs, val_idxs in kfold.split(inputs_df):\n        X_train, train_targets = inputs_df.iloc[train_idxs], targets.iloc[train_idxs]\n        X_val, val_targets = inputs_df.iloc[val_idxs], targets.iloc[val_idxs]\n        #print(X_val.shape)\n        model, train_rmse, val_rmse = train_and_evaluate(X_train, train_targets, X_val, val_targets, **params)\n        models.append(model)\n        train_rmses.append(train_rmse)\n        val_rmses.append(val_rmse)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(np.mean(train_rmses), np.mean(val_rmses)))\n    print(X_val.shape)\n    return np.mean([model.predict(inputs) for model in models], axis=0)","4ed2938f":"%%time\ntest_params_kfold(18,test_df,learning_rate=0.3,n_estimators=163,max_depth=5,colsample_bytree=0.4)","bdcd5cd3":"test_preds = test_params_kfold(18,test_df,learning_rate=0.3,n_estimators=163,max_depth=5,colsample_bytree=0.4)\ntest_preds","8e6b9fe6":"submission_df['target'] = test_preds\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df","8ec3dc2e":"#test_params( n_estimators=20,max_depth=2)#0.7307046350713617","da85663c":"#%%time\n#test_params(n_estimators=400,learning_rate=0.6, max_depth=10, subsample=0.9,colsample_bytree=0.7)#0.8673598723673045","1b1a0707":"* Let's define a helper function `train_and_evaluate` which trains a model the given parameters and returns the trained model, training error and validation error.","870e8c0d":"import numpy as np\n\ndef predict_avg(models, inputs):\n    return np.mean([model.predict(inputs) for model in models], axis=0)","1955c257":"preds = predict_avg(models, test_df)","1a41d9ec":"![image.png](attachment:02e1d086-38bb-4512-ae52-e2c706814afd.png)","bbf3aece":"* Here's a helper function to test hyperparameters with K-fold cross validation.","d37ec9ea":"models = []\n\nfor train_idxs, val_idxs in KFold(n_splits=5).split(inputs_df):\n    X_train, train_targets = inputs_df.iloc[train_idxs], targets.iloc[train_idxs]\n    X_val, val_targets = inputs_df.iloc[val_idxs], targets.iloc[val_idxs]\n    model, train_rmse, val_rmse = train_and_evaluate(X_train, \n                                                     train_targets, \n                                                     X_val, \n                                                     val_targets, \n                                                     max_depth=4, \n                                                     n_estimators=20)\n    models.append(model)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse,val_rmse))","2f9dc9ac":"# Feature importance\nJust like decision trees and random forests, XGBoost also provides a feature importance score for each column in the input","2fa99d82":"# Visualization\nWe can visualize individual trees using `plot_tree` (note: this requires the `graphviz` library to be installed).","476cf8f2":"# K Fold Cross Validation\n![image.png](attachment:b577da01-cafa-4243-af09-a6c9bf4eae1e.png)","54a6302e":"# Gradient Boosting\nWe're now ready to train our gradient boosting machine (GBM) model. Here's how a GBM model works:\n\nThe average value of the target column and uses as an initial prediction every input.\nThe residuals (difference) of the predictions with the targets are computed.\nA decision tree of limited depth is trained to predict just the residuals for each input.\nPredictions from the decision tree are scaled using a parameter called the learning rate (this prevents overfitting)\nScaled predictions fro the tree are added to the previous predictions to obtain the new and improved predictions.\nSteps 2 to 5 are repeated to create new decision trees, each of which is trained to predict just the residuals from the previous prediction.\nThe term \"gradient\" refers to the fact that each decision tree is trained with the purpose of reducing the loss from the previous iteration (similar to gradient descent). The term \"boosting\" refers the general technique of training new models to improve the results of an existing model.\nHere's a visual representation of gradient boosting:\n\n![image.png](attachment:d47a9595-a376-444d-b45a-3ab6e85858c1.png)"}}