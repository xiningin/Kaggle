{"cell_type":{"d4376b96":"code","df3f52ae":"code","003030a4":"code","a8236150":"code","9b54b24d":"code","7128d8a3":"code","c63be792":"code","5ef2a7fa":"code","febf5162":"code","68c54273":"code","2b86a276":"code","5f98254f":"code","39aa6f75":"code","6b0454fe":"code","83d1c76c":"code","f9ffd871":"code","9790b688":"code","e661dcb9":"code","9dbc9ad3":"code","7062c66c":"code","2a6bd322":"code","726a20e1":"code","38d041ee":"code","1d5598b9":"code","9c51425d":"code","4bbd6bfa":"code","9074c004":"code","25f4358e":"code","f94b2f39":"code","02a868bd":"code","0eedbe87":"code","876e94f3":"code","ba48fc14":"code","24bed472":"code","cd0a3dac":"code","5146832b":"code","497ee610":"code","4f612306":"code","1b78162d":"code","49684ff1":"code","2088321b":"code","267a9861":"code","20e9f10c":"code","dc995962":"code","e44780b5":"code","543c9298":"code","aeea73d0":"code","dfc3d85e":"code","3ef9ec65":"code","8193bca4":"code","2341ba21":"code","5cfbdd4b":"code","d73a78e0":"code","f637e916":"code","2a282034":"markdown","b5de1f98":"markdown","d5240c1e":"markdown","7af11eca":"markdown","f74afc3c":"markdown","41433d26":"markdown","e07e2b21":"markdown","c6a42664":"markdown","109914d4":"markdown","72a07c29":"markdown","b96b1e7d":"markdown","bd3458ee":"markdown"},"source":{"d4376b96":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nsc = StandardScaler()\nmx = MinMaxScaler()\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df3f52ae":"train_data = pd.read_csv(\"..\/input\/house-price-prediction-challenge\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/house-price-prediction-challenge\/test.csv\")\ntrain_data.head()","003030a4":"train_data.describe().T","a8236150":"train_data.info()","9b54b24d":"obj_cols = list(train_data.columns[train_data.dtypes == 'object'])\nobj_cols","7128d8a3":"import plotly.express as px\nfig = px.histogram(train_data['TARGET(PRICE_IN_LACS)'], x = 'TARGET(PRICE_IN_LACS)')\nfig.show()","c63be792":"fig1 = px.histogram(train_data['SQUARE_FT'], x = 'SQUARE_FT')\nfig1.show()","5ef2a7fa":"# The below function cleans the dataset and label encodes the categorical columns\ndef data_cleaning(data):\n    for i in range(len(data)):\n        str1 = data['ADDRESS'][i].split(\",\")[-1]\n        data['ADDRESS'][i] = str1\n    encoder = LabelEncoder()\n    #for col in obj_cols:\n        #data[col] = pd.get_dummies(data[col],drop_first=True)\n    for col in obj_cols:\n        data[col] = encoder.fit_transform(data[col].astype(str))\n        \n    data.drop(\"BHK_OR_RK\",axis = 1, inplace = True) # dropping the location co-ordinates as the city name already does the job  \n    data.drop(\"POSTED_BY\", axis = 1, inplace = True)\n    data.drop(\"UNDER_CONSTRUCTION\",axis = 1,inplace = True)\n    data.drop(['LONGITUDE','LATITUDE'],axis = 1,inplace = True)\n    if 'TARGET(PRICE_IN_LACS)' in data.columns:\n        data = data[data['TARGET(PRICE_IN_LACS)']<=99] #dropping houses with prices >5crs as they affect the distribution of data\n        data = data[data['SQUARE_FT']<=2900]\n    data[\"SQUARE_FT\"] = data[\"SQUARE_FT\"].astype(int)\n    return data\ndata_train = data_cleaning(train_data)\ndata_test = data_cleaning(test_data)\ndata_test.head(10)","febf5162":"\nfig1 = px.histogram(data_train['SQUARE_FT'], x = 'SQUARE_FT')\nfig1.show()","68c54273":"\nfig1 = px.histogram(data_train['TARGET(PRICE_IN_LACS)'], x = 'TARGET(PRICE_IN_LACS)')\nfig1.show()","2b86a276":"#splitting into inputs and targets\n\ninput_data =data_train.iloc[:,:-1]\ntarget_data = data_train.iloc[:,-1]\n#input_data.head()","5f98254f":"input_data.head()","39aa6f75":"sns.pairplot(data_train)","6b0454fe":"sc.fit(input_data)\nscaled_inputs = sc.transform(input_data)","83d1c76c":"#mx.fit(input_data)\n#scaled_inputs = mx.transform(input_data)  #StandardScaler gives better results\n","f9ffd871":"# Train test split method for splitting the inputs into train and test data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression(normalize=True) #normalizing turned down the rmse even more\ntrain_inputs,test_inputs,train_targets,test_targets = train_test_split(scaled_inputs,target_data)\n","9790b688":"test_targets.head()","e661dcb9":"lr.fit(train_inputs,train_targets)\npreds = lr.predict(test_inputs)","9dbc9ad3":"d = pd.DataFrame()\nd['preds']=preds\nd['real_price'] = np.array(test_targets)\nd['%_error'] = np.absolute((preds-np.array(test_targets))\/np.array(test_targets))*100\nd","7062c66c":"d.describe().T","2a6bd322":"from sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(test_targets, preds))\nprint(\"RMSE: %f\" % (rmse))","726a20e1":"from sklearn.linear_model import ElasticNet\nelr = ElasticNet(l1_ratio = 0.85,normalize = False,selection = 'random') #l1 ratio of 0.85 gives the best results\nelr.fit(train_inputs,train_targets)\npredse = elr.predict(test_inputs)","38d041ee":"es = pd.DataFrame()\nes['preds']=predse\nes['real_price'] = np.array(test_targets)\nes['%_error'] = np.absolute((predse-np.array(test_targets))\/np.array(test_targets))*100\nes","1d5598b9":"\nes.describe().T","9c51425d":"rsme = np.sqrt(mean_squared_error(test_targets,predse))\nprint(\"RSME: {}\".format(rsme))","4bbd6bfa":"# 2 xgboost regressor\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error","9074c004":"xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)","25f4358e":"xg_reg.fit(train_inputs,train_targets)\n\npreds2 = xg_reg.predict(test_inputs)","f94b2f39":"\ne = pd.DataFrame()\ne['preds2']=preds2\ne['real_price'] = np.array(test_targets)\ne['%_error'] = np.absolute((preds2-np.array(test_targets))\/np.array(test_targets))*100\ne","02a868bd":"e.describe().T","0eedbe87":"rmse = np.sqrt(mean_squared_error(test_targets, preds2))\nprint(\"RMSE: %f\" % (rmse))","876e94f3":"import tensorflow as tf\nimport tensorflow as tf\nfrom tensorflow.keras.layers.experimental import preprocessing\nimport numpy as np","ba48fc14":"data_test1 = data_test.copy()\nsc.fit(data_test1)\ndata_test1 = sc.transform(data_test1)","24bed472":"#for using neural netowrks you have to save the data either in  npz format or tensorslices\nnp.savez('data_train', inputs=train_inputs, targets=train_targets)\nnp.savez('data_validation', inputs=test_inputs, targets=test_targets)\nnp.savez('data_test', inputs= data_test1)\n#np.savez('data_test', inputs=test_inputs, targets=test_targets)","cd0a3dac":"npz = np.load('data_train.npz')\ntrain_inputs_tf = npz['inputs'].astype(np.float)\n# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\ntrain_targets_tf = npz['targets'].astype(np.int8)\n\n# we load the validation data in the temporary variable\nnpz = np.load('data_validation.npz')\n# we can load the inputs and the targets in the same line\nvalidation_inputs_tf, validation_targets_tf = npz['inputs'].astype(np.float), npz['targets'].astype(np.int8)\n\nnpz = np.load('data_test.npz')\ntest_inputs_tf = npz['inputs'].astype(np.float)\n","5146832b":"#building the model\n\nmodel = tf.keras.Sequential([tf.keras.layers.Dense(32, activation='relu'),\n                             tf.keras.layers.Dense(32, activation='relu'),\n                             tf.keras.layers.Dense(1)\n                             ])\nmodel.compile(loss='mse',\n              optimizer='RMSprop',\n              metrics=['mse'])","497ee610":"early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n\nmodel.fit(\n          train_inputs_tf, # train inputs\n          train_targets_tf,\n          epochs = 15,\n          callbacks=[early_stopping], # early stopping\n          validation_data=(validation_inputs_tf, validation_targets_tf),\n          verbose = 1\n\n          )","4f612306":"preds_tf = model.predict(validation_inputs_tf)","1b78162d":"preds_tf = preds_tf.reshape(validation_inputs_tf.shape[0],)","49684ff1":"preds_tf.shape","2088321b":"tft = pd.DataFrame()\ntft['preds_tf']=preds_tf\ntft['real_price'] = np.array(test_targets)\ntft['%_error'] = np.absolute((preds_tf-np.array(test_targets))\/np.array(test_targets))*100\ntft","267a9861":"tft.sort_values(by=['%_error'], inplace=True,ascending = False)\n","20e9f10c":"tft.head(15)","dc995962":"tft.describe().T","e44780b5":"rmse = np.sqrt(mean_squared_error(test_targets, preds_tf))\nprint(\"RMSE: %f\" % (rmse))","543c9298":"test_preds = model.predict(test_inputs_tf)","aeea73d0":"test_preds","dfc3d85e":"answers = test_preds.reshape(68720,)","3ef9ec65":"answers_r = np.round(answers,1) \nanswers_r\n","8193bca4":"sample_submission= pd.read_csv('..\/input\/house-price-prediction-challenge\/sample_submission.csv')","2341ba21":"submission = sample_submission.copy()","5cfbdd4b":"submission['TARGET(PRICE_IN_LACS)'] = answers_r","d73a78e0":"submission.head(10)","f637e916":"#submission.to_csv('submission.csv')","2a282034":"## we see that there are a lot of outliers which is impacting the graph i.e prices of houses greater than 5k lacs or 50 crores  and square foot values so we remove them","b5de1f98":"## *Now from the charts u can see that the dataset is more evenly distributed with considerably less outliers that negatively performance*","d5240c1e":"### We'll try different types of learning:-\n1. Linear regression\n2. ElasticNet regression\n3. Xgb boost\n4. Neural Networks","7af11eca":"## *3. Using the xgboost regressor*","f74afc3c":"# **EDA on the cleaned dataset**","41433d26":"## 1. Plotting graphs of continuous variables and getting rid of outliers\n## 2. Getting the city name from address\n## 3. Converting BHK or RK into categorical variables\n## 4. Cleaning the dataset by imputing the missing values and fixing obj type datatypes into float or int","e07e2b21":"# Preprocessing the train and test datasets","c6a42664":"## *2. ElasticNet regression*","109914d4":"### *4. Using Deep neural networks*","72a07c29":"# **Since the tensorflow model had the lowest RSME and mean error I choose the neural network model for test predictions**","b96b1e7d":"## *1.Multiple linear regression*","bd3458ee":"### As you can see the linear regression model is underperforming so we'll try other methods"}}