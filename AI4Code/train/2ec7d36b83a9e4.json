{"cell_type":{"a1e194bd":"code","c42c27c6":"code","33540349":"code","55233316":"code","6e6b2101":"code","b6cd0b21":"code","bd5fded3":"code","3140f1be":"code","dca468d3":"code","9c781e09":"code","ea013161":"code","eaa41b3d":"code","9fe216bd":"code","8e926789":"code","7ac4978e":"code","b2874b3b":"code","89edc7f0":"code","d8fd9f5d":"code","e76fa519":"code","bcfb726c":"code","5757694b":"code","fe1376ef":"code","9720dfb8":"code","9b959b46":"code","eb1095ef":"code","2c4d668d":"code","fe4e6c58":"code","f777be70":"code","ff9a9e7a":"code","db429208":"code","3a23e6aa":"code","74dd1a6a":"code","cf4590cc":"code","915424c5":"code","8a3a1094":"code","0f02798a":"code","4e73e423":"code","729608f3":"code","76c23627":"code","7fe1dbcf":"code","4d686f1f":"code","dd1dd716":"code","0a205829":"code","af810778":"code","2ea82062":"code","af5679e3":"code","568be5df":"code","51f69117":"code","5e7d3145":"code","e932be61":"code","93744745":"code","a0feb879":"code","0929db35":"code","c55251e6":"code","a55a40c8":"code","7de29fa0":"code","20096bce":"code","bd678617":"code","0b2186d6":"code","e9c59bb6":"code","7663b6ce":"code","1410aac8":"code","87237b6c":"code","c1482aa7":"code","ec1ff59b":"code","fe18a841":"code","729fe91c":"code","e05fc330":"code","93f2000f":"code","38dbbbea":"code","acd58962":"code","df9be9e2":"code","4626a21b":"code","279ab993":"code","cc582558":"code","217c27af":"code","8a8866f0":"code","12a1a1a6":"code","e0c8a1a3":"markdown","8105c4e8":"markdown","d8465beb":"markdown","273f9398":"markdown","460fbc92":"markdown","27678b91":"markdown","f0954225":"markdown","12badb32":"markdown","c5897b99":"markdown","fa04c3dd":"markdown","dddce8d7":"markdown","a9515663":"markdown","2471ca71":"markdown","7025f3c1":"markdown","020979ef":"markdown","10dd5972":"markdown","c7b8b79e":"markdown","3569c9d8":"markdown","eb31e7b0":"markdown","729f7704":"markdown","f80831d1":"markdown","815cce5f":"markdown","bbbd772e":"markdown","27875f0b":"markdown","50dd6c2d":"markdown","71babfa4":"markdown","ce7ce1cd":"markdown","2eb96b36":"markdown","676f3a84":"markdown","d8de3f64":"markdown","b66cc129":"markdown","f265fbd8":"markdown","209b26b1":"markdown","3da1bea2":"markdown","093c862b":"markdown","48ddb0fc":"markdown","104ec06b":"markdown","5dd55a90":"markdown","5fb784b8":"markdown"},"source":{"a1e194bd":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV","c42c27c6":"#import train and test CSV files\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","33540349":"# copy data in order to avoid any change in the original:\n\ntrain = train_data.copy()\ntest = test_data.copy()","55233316":"#take a look at the training data's head and tail\nhead = train.head()\nsample= train.sample(5)\ntail = train.tail()\ndata_row = pd.concat([head,sample,tail], axis =0, ignore_index =True)\ndata_row","6e6b2101":"## get simple statistics on this dataset\ntrain.describe(include=\"all\").T","b6cd0b21":"#get information about the dataset\ntrain.info","bd5fded3":"#get a list of the features within the dataset\nprint(train.columns)","3140f1be":"#check for any other unusable values\ntrain.isnull().any()","dca468d3":"print(pd.isnull(train).sum())","9c781e09":"!pip install missingno","ea013161":"import missingno as msno","eaa41b3d":"msno.bar(train);","9fe216bd":"# Classes of some categorical variables","8e926789":"def bar_plot(variable):\n    \"\"\"\n    input: Variable ex:Sex\n    output: barplot&value count\n    \"\"\"\n    #get feature\n    var=train[variable]\n    #Count of Categorical variable(value\/sample) \n    varValue=var.value_counts()\n    \n    #visualize\n    plt.figure(figsize=(9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable,varValue))","7ac4978e":"category1=[\"Survived\", \"Sex\", \"Pclass\", \"Embarked\", \"SibSp\", \"Parch\"]\nfor c in category1:\n    bar_plot(c)","b2874b3b":"train['Ticket'].value_counts()","89edc7f0":"train['Cabin'].value_counts()","d8fd9f5d":"# Classes of some numerical variables","e76fa519":"def plot_hist(variable):\n    plt.figure(figsize=(9,3))\n    plt.hist(train[variable], bins=50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\" {} distribution with hist\".format(variable))\n    plt.show()","bcfb726c":"numericVar=[\"Fare\", \"Age\", \"PassengerId\"]\nfor n in numericVar:\n    plot_hist(n)","5757694b":"#draw a bar plot of survival by Sex\n\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train)\n#Print percentages of females vs. males that survive\nprint(\"Percentage of females who survived:\", train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of males who survived  :\", train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)\n","fe1376ef":"#draw a bar plot of survival by Pclass\n\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train)\n#print percentage of people by Pclass that survived\nprint(\"Percentage of Pclass = 1 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Pclass = 2 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Pclass = 3 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","9720dfb8":"#draw a bar plot for SibSp vs. survival\n\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train)\n\n#I won't be printing individual percent values for all of these.\n\nprint(\"Percentage of SibSp = 0 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 0].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 1 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 2 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 3 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 3].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 4 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 4].value_counts(normalize = True)[1]*100)\n","9b959b46":"#draw a bar plot for Parch vs. survival\n\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train)\nplt.show()","eb1095ef":"#sort the ages into logical categories\n\ntrain[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())\ntest[\"Age\"] = test[\"Age\"].fillna(test[\"Age\"].median())\nbins = [-1, 0, 5, 12, 18, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of AgeGroup vs. survival\n\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train)\nplt.show()","2c4d668d":"#Create CabinBool variable which states if someone has a Cabin data or not:\n\ntrain[\"CabinBool\"] = (train[\"Cabin\"].notnull().astype('int'))\ntest[\"CabinBool\"] = (test[\"Cabin\"].notnull().astype('int'))\n\n#calculate percentages of CabinBool vs. survived\n\nprint(\"Percentage of CabinBool = 1 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of CabinBool = 0 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 0].value_counts(normalize = True)[1]*100)\n\n#draw a bar plot of CabinBool vs. survival\n\nsns.barplot(x=\"CabinBool\", y=\"Survived\", data=train)\nplt.show()\n","fe4e6c58":"test.describe()","f777be70":"test.head()","ff9a9e7a":"train.head()","db429208":"# We can drop the Ticket, Name and Cabin for the test and train data\n\ntest = test.drop(['Ticket'], axis = 1)\ntest = test.drop(['Cabin'], axis = 1)\ntest = test.drop(['Name'], axis = 1)\ntest.head()\n\n ","3a23e6aa":"train = train.drop(['Ticket'], axis = 1)\ntrain = train.drop(['Cabin'], axis = 1)\ntrain = train.drop(['Name'], axis = 1)\ntrain.head()","74dd1a6a":"train.isnull().sum()","cf4590cc":"test.isnull().sum()","915424c5":"train.describe().T\n","8a3a1094":"test.describe().T","0f02798a":"# It looks like there is a problem in Fare max data. Visualize with boxplot.\n\nsns.boxplot(x = train['Fare']);","4e73e423":"Q1 = train['Fare'].quantile(0.25)\nQ3 = train['Fare'].quantile(0.75)\nIQR = Q3 - Q1\n\nlower_limit = Q1- 1.5*IQR\nlower_limit\n\nupper_limit = Q3 + 1.5*IQR\nupper_limit","729608f3":"# observations with Fare data higher than the upper limit:\n\ntrain['Fare'] > (upper_limit)","76c23627":"train.sort_values(\"Fare\", ascending=False).head()","7fe1dbcf":"# In boxplot, there are too many data higher than upper limit; we can not change all. Just repress the highest value -512- \n\ntrain['Fare'] = train['Fare'].replace(512.3292, 300)","4d686f1f":"train.sort_values(\"Fare\", ascending=False).head()","dd1dd716":"test.sort_values(\"Fare\", ascending=False)","0a205829":"test['Fare'] = test['Fare'].replace(512.3292, 300)","af810778":"test.sort_values(\"Fare\", ascending=False)","2ea82062":"train.isnull().sum()","af5679e3":"train['Embarked'].value_counts()","568be5df":"train[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")","51f69117":"train.isnull().sum()","5e7d3145":"test.isnull().sum()","e932be61":"test[test[\"Fare\"].isnull()]","93744745":"test[[\"Pclass\",\"Fare\"]].groupby(\"Pclass\").mean()","a0feb879":"test[\"Fare\"] = test[\"Fare\"].fillna(12)","0929db35":"test.isnull().sum()","c55251e6":"train.head(5)","a55a40c8":"#map each Sex value to a numerical value\n\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntrain.head()","7de29fa0":"# Convert Sex values into 1-0:\n\nfrom sklearn import preprocessing\nlbe = preprocessing.LabelEncoder()\ntest[\"Sex\"] = lbe.fit_transform(test[\"Sex\"])\ntest.head()","20096bce":"#map each Embarked value to a numerical value\n\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)\n\ntrain.head()","bd678617":"train.head()","0b2186d6":"# Map each Age value to a numerical value:\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3,  'Young Adult': 4, 'Adult': 5, 'Senior': 6}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)","e9c59bb6":"test.head()","7663b6ce":"#dropping the Age feature for now, might change:\ntrain = train.drop(['Age'], axis = 1)\ntest = test.drop(['Age'], axis = 1)","1410aac8":"# Map Fare values into groups of numerical values:\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])","87237b6c":"# Drop Fare values:\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)","c1482aa7":"train.head()","ec1ff59b":"train.head()","fe18a841":"train[\"FamilySize\"] = train_data[\"SibSp\"] + train_data[\"Parch\"] + 1","729fe91c":"test[\"FamilySize\"] = test_data[\"SibSp\"] + test_data[\"Parch\"] + 1","e05fc330":"train","93f2000f":"test","38dbbbea":"#Spliting the train data\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\npredictors = train.drop(['Survived','PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_test, y_train, y_test = train_test_split(predictors, target, test_size = 0.25, random_state = 0)\n\nx_train.shape\nx_test.shape","acd58962":"#Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_test)\nacc_logreg = round(accuracy_score(y_pred, y_test) * 100, 1)\nprint(acc_logreg)\n\n","df9be9e2":"#Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_test)\nacc_randomforest = round(accuracy_score(y_pred, y_test) * 100, 1)\nprint(acc_randomforest)\n","4626a21b":"#Gradient Boosting Classifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_test)\nacc_gbk = round(accuracy_score(y_pred, y_test)*100, 1)\nprint(acc_gbk)","279ab993":"#Model Tuning\n\nxgb_params = {\n        'n_estimators': [200, 500],\n        'subsample': [0.6, 1.0],\n        'max_depth': [2,5,8],\n        'learning_rate': [0.1,0.01,0.02],\n        \"min_samples_split\": [2,5,10]}\n\nxgb = GradientBoostingClassifier()\nxgb_cv_model = GridSearchCV(xgb, xgb_params, cv = 10, n_jobs = -1, verbose = 2)\nxgb_cv_model.fit(x_train, y_train)\nxgb_cv_model.best_params_\n\nxgb = GradientBoostingClassifier(learning_rate = xgb_cv_model.best_params_[\"learning_rate\"], \n                    max_depth = xgb_cv_model.best_params_[\"max_depth\"],\n                    min_samples_split = xgb_cv_model.best_params_[\"min_samples_split\"],\n                    n_estimators = xgb_cv_model.best_params_[\"n_estimators\"],\n                    subsample = xgb_cv_model.best_params_[\"subsample\"])\nxgb_tuned =  xgb.fit(x_train,y_train)\ny_pred = xgb_tuned.predict(x_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 1)\nprint(acc_gbk)","cc582558":"test","217c27af":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = xgb_tuned.predict(test.drop('PassengerId', axis=1))","8a8866f0":"#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","12a1a1a6":"output.head()","e0c8a1a3":"##### Family Size","8105c4e8":"Parch Feature\n","d8465beb":"#### Embarked Feature","273f9398":"I think the idea here is that people with recorded cabin numbers are of higher socioeconomic class, and thus more likely to survive. \n\n","460fbc92":"#### Sex Feature","27678b91":"### Data Visualization","f0954225":"We'll convert catagorical values to the numerical values","12badb32":"##### To clean data to account for missing values and unnecessary information!\n\n##### Test Data\n##### See how the test data looks!\n\n","c5897b99":"Babies are more likely to survive than any other age group.\n\n","fa04c3dd":"As predicted, people with higher socioeconomic class had a higher rate of survival. (62.9% vs. 47.3% vs. 24.2%)\n","dddce8d7":"#### -Drop The Feature\n","a9515663":"## 3. Data preparation\n### Cleaning Data","2471ca71":"#### FareBand","7025f3c1":"## 6. Deployment","020979ef":"#### AgeGroup","10dd5972":"In general, it's clear that people with more siblings or spouses aboard were less likely to survive.\n\nHowever, contrary to expectations, people with no siblings or spouses were less to likely to survive than those with one or two. (34.5% vs 53.4% vs. 46.4%)\n","c7b8b79e":"People with less than four parents or children aboard are more likely to survive than those with four or more.\n\nAgain, people traveling alone are less likely to survive than those with 1-3 parents or children.","3569c9d8":"Some Observations:\n\nThe total passengers in  train data, 891 .\nThe Age, cabin and Embarked feature have missing values. I think the Age feature is pretty important to survival, so we should probably attempt to fill these gaps.\nThe Cabin feature have so much  missing values. I hink it would be hard to fill in the missing values. We can drop all these variable from the dataset.\nThe Embarked feature have only two values. We can fill it from the mod value of these variable","eb31e7b0":"### -Variable Transformation\n","729f7704":"## 4.Modeling","f80831d1":"As predicted, females have a much higher chance of survival than males. The Sex feature is essential in our predictions.","815cce5f":"Cabin Feature\n","bbbd772e":"We'll fill in the missing values in the test data.","27875f0b":" SibSp Feature","50dd6c2d":"## 1. Business understanding\n### Titanic survival prediction using with CRISP-DM:\n### Use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.","71babfa4":"# INTRODUCTION\n### 1. Business understanding\n### 2. Data understanding\n### 3. Data preparation\n### 4. Modeling\n### 5. Evaluation\n### 6. Deployment","ce7ce1cd":"Pclass Feature","2eb96b36":"We'll fill in the missing values in the train data.","676f3a84":"Age Feature\n","d8de3f64":"Some Predictions:\n\nSex: Females are more likely to survive.\n\nPclass: People of higher socioeconomic class are more likely to survive.\n\nSibSp\/Parch: People traveling alone are more likely to survive.\n\nAge: Young children are more likely to survive.\n\n","b66cc129":" ### Read in our training and testing data using pd.read_csv","f265fbd8":"## 2. Data understanding","209b26b1":"### First of all,import several Python libraries such as numpy, pandas, matplotlib and seaborn.","3da1bea2":"### -Missing Value Treatment\n","093c862b":"##### Train Data\n##### See how the train data looks!","48ddb0fc":"We can see that except for the above mentioned missing values, not NaN values exist.","104ec06b":"## 5. Evaluation","5dd55a90":"### -Feature Engineering\n","5fb784b8":"People with a recorded Cabin number are, in fact, more likely to survive. (66.6% vs 29.9%)"}}