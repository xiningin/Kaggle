{"cell_type":{"1de5a18b":"code","372b5678":"code","f47907b0":"code","6bc9b1fe":"code","08fa676e":"code","f90c66b1":"code","9362748a":"code","b9a21ba6":"code","d9d18789":"code","958a509b":"code","6be3af44":"code","993d43a2":"code","21a9d30a":"code","19d8a91e":"code","5f5351c7":"code","f4ffc640":"code","e54b5620":"code","277a23fd":"code","f8b3fec2":"code","50acbd5c":"code","08a18d37":"code","b58a365b":"code","e246762a":"markdown","b1f07284":"markdown","4a43fb82":"markdown","dcdaf683":"markdown","dfc3b3c0":"markdown","95171135":"markdown","1c6f1262":"markdown","d5bd95e2":"markdown","be6d381e":"markdown","fdc897b8":"markdown","499a928f":"markdown","796fbefa":"markdown","5ea5b4bc":"markdown","3873c3f6":"markdown"},"source":{"1de5a18b":"import os\nimport random\nfrom time import time\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm import tqdm\nimport albumentations as albu","372b5678":"def make_reproducible(seed=0):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","f47907b0":"make_reproducible()","6bc9b1fe":"config = {\n    'arch': 1,  # \ubaa8\ub378 \uad6c\uc870 (1,2,3,4 \uc911 \ud558\ub098)\n    'aug': True,  # augmentation \uc720\ubb34\n    'optimizer': 'momentum',  # optimizer ('momentum', 'adam' \uc911 \ud558\ub098)\n    'lr': 0.1,  # learning rate\n    'weight_decay': 1e-4,  # l2 norm\n    'batch_size': 1024,  # batch size\n    'min_lr': 1e-5,  # learning rate \uac10\uc18c\uc2dc\ud0ac \ub54c \ucd5c\uc18c\uac12\n    'epochs': 300,\n#     'rp_patience': 30,  # validation metric\uc774 30 epoch \ub3d9\uc548 \uac10\uc18c\ud558\uc9c0 \uc54a\uc73c\uba74 learning rate 0.3\ubc30\ud568\n#     'es_patience': 70,  # validation metric\uc774 70 epoch \ub3d9\uc548 \uac10\uc18c\ud558\uc9c0 \uc54a\uc73c\uba74 early stop\n    'amp': False,  # mixed precision \uc720\ubb34\n    'num_workers': 8,  # dataloader \uba40\ud2f0\ud504\ub85c\uc138\uc2f1 \uac1c\uc218\n}","08fa676e":"train_data = pd.read_csv('..\/input\/bitamin-deep-learning-contest\/train_df.csv')\ntest_data = pd.read_csv('..\/input\/bitamin-deep-learning-contest\/test_df.csv')\n\nx_data = (train_data.drop('label',axis = 1).values.reshape(-1, 28, 28, 1)\/255).astype(np.float32)\ny_data = train_data['label'].values\n\nx_test = (test_data.drop('Unnamed: 0', axis=1).values.reshape(-1, 28, 28, 1)\/255).astype(np.float32)\n\nfolds = list(StratifiedKFold(5, shuffle=True, random_state=0).split(x_data, y_data))\ntrn_idx = folds[0][0]\nval_idx = folds[0][1]\nx_train, x_val, y_train, y_val = x_data[trn_idx], x_data[val_idx], y_data[trn_idx], y_data[val_idx]","f90c66b1":"print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)","9362748a":"class MNISTDataset(nn.Module):\n    \n    def __init__(self, x, y=None, transforms=None):\n        self.x = x\n        self.y = y\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, idx):\n        x = self.x[idx]\n        if self.transforms is not None:\n            x = self.transforms(image=x)['image']\n        x = np.transpose(x, (2, 0, 1))  # h, w, c -> c, h, w\n        if self.y is None:\n            return x\n        else:\n            return x, self.y[idx]","b9a21ba6":"if config['aug']:\n    train_transforms = albu.Compose([\n        albu.RandomBrightnessContrast(brightness_limit=(-0.2, 0.5), contrast_limit=0.5, p=1),\n#         albu.OneOf([\n#             albu.Blur(blur_limit=2, p=1),\n#             albu.GaussianBlur(blur_limit=3, p=1)\n#         ], p=0.5),\n        albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.15, rotate_limit=25, border_mode=0, value=0, p=0.7),\n#         albu.CoarseDropout(max_holes=5, max_height=3, max_width=3, p=1)\n    ])\nelse:\n    train_transforms = None","d9d18789":"train_dataset = MNISTDataset(x_train, y_train, transforms=train_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, config['batch_size'], shuffle=True, num_workers=config['num_workers'])\nval_dataset = MNISTDataset(x_val, y_val, transforms=None)\nval_loader = torch.utils.data.DataLoader(val_dataset, config['batch_size'], shuffle=False, num_workers=config['num_workers'])\nprint(f'# Train Samples: {len(train_dataset)} | # Val Samples: {len(val_dataset)}')","958a509b":"class SimpleBlock(nn.Module):\n    \n    def __init__(self, in_c, out_c):\n        super(SimpleBlock, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_c, out_c, 3, 1, padding=1, bias=False),\n            nn.BatchNorm2d(out_c),\n            nn.ReLU()\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(out_c, out_c, 3, 1, padding=1, bias=False),\n            nn.BatchNorm2d(out_c),\n            nn.ReLU()\n        )\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x_ = x\n        x = self.conv2(x)\n        x = x+x_\n        return x","6be3af44":"class SELayer(nn.Module):\n    \n    def __init__(self, channel, reduction=4):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel\/\/reduction),\n            nn.ReLU(),\n            nn.Linear(channel\/\/reduction, channel),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n    \n    \nclass ECALayer(nn.Module):\n    \n    \"\"\"Constructs a ECA module.\n    Args:\n        channel: Number of channels of the input feature map\n        k_size: Adaptive selection of kernel size\n        \n    https:\/\/github.com\/BangguWu\/ECANet\/blob\/master\/models\/eca_module.py\n    \"\"\"\n    def __init__(self, channel, k_size=3):\n        super(ECALayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) \/\/ 2, bias=False) \n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # x: input features with shape [b, c, h, w]\n        b, c, h, w = x.size()\n\n        # feature descriptor on the global spatial information\n        y = self.avg_pool(x)\n\n        # Two different branches of ECA module\n        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n\n        # Multi-scale information fusion\n        y = self.sigmoid(y)\n\n        return x * y.expand_as(x)\n    \n    \nclass Separable(nn.Module):\n    \n    def __init__(self, in_c, out_c, se=False):\n        super(Separable, self).__init__()\n        \n        hidden_c = in_c*2\n        self.expand = nn.Sequential(\n            nn.Conv2d(in_c, hidden_c, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(hidden_c),\n            nn.ReLU()\n        )\n        \n        depthwise = [\n            nn.Conv2d(hidden_c, hidden_c, 3, 1, 1, groups=hidden_c, bias=False),\n            nn.BatchNorm2d(hidden_c)\n        ]\n        if se == 'eca': \n            depthwise.append(ECALayer(hidden_c))\n        elif se == 'se':\n            depthwise.append(SELayer(hidden_c))\n        depthwise.append(nn.ReLU())\n        self.depthwise = nn.Sequential(*depthwise)\n\n        self.pointwise = nn.Sequential(\n            nn.Conv2d(hidden_c, out_c, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(out_c),\n        )\n        \n    def forward(self, x):\n        x = self.expand(x)\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n        return x\n    \nclass MobileBlock(nn.Module):\n    \n    def __init__(self, in_c, out_c, se=False):\n        super(MobileBlock, self).__init__()\n        self.conv1 = Separable(in_c, out_c, se=se)\n        self.conv2 = Separable(out_c, out_c, se=se)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x_ = x\n        x = self.conv2(x)\n        x = x+x_\n        return x\n    \n\nclass Net(nn.Module):\n    def __init__(self, arch=1):\n        super(Net, self).__init__()\n        \n        if arch == 1:\n            self.block1 = SimpleBlock(1, 32)\n            self.block2 = SimpleBlock(32, 64)\n            self.block3 = SimpleBlock(64, 128)\n            self.head = nn.Linear(128, 10)\n        elif arch == 2:\n            self.block1 = nn.Sequential(\n                nn.Conv2d(1, 24, 3, 1, 1, bias=False),\n                nn.BatchNorm2d(24),\n                nn.ReLU(),\n                MobileBlock(24, 48)\n            )\n            self.block2 = MobileBlock(48, 96)\n            self.block3 = MobileBlock(96, 192)\n            self.head = nn.Linear(192, 10)\n        elif arch == 3:\n            self.block1 = nn.Sequential(\n                nn.Conv2d(1, 24, 3, 1, 1, bias=False),\n                nn.BatchNorm2d(24),\n                nn.ReLU(),\n                MobileBlock(24, 48, se='eca')\n            )\n            self.block2 = MobileBlock(48, 96, se='eca')\n            self.block3 = MobileBlock(96, 192, se='eca')\n            self.head = nn.Linear(192, 10)\n        elif arch == 4:\n            self.block1 = nn.Sequential(\n                nn.Conv2d(1, 24, 3, 1, 1, bias=False),\n                nn.BatchNorm2d(24),\n                nn.ReLU(),\n                MobileBlock(24, 48, se='se')\n            )\n            self.block2 = MobileBlock(48, 96, se='se')\n            self.block3 = MobileBlock(96, 192, se='se')\n            self.head = nn.Linear(192, 10)\n            \n        self.pool1 = nn.MaxPool2d(2)\n        self.pool2 = nn.MaxPool2d(2)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.pool1(x)\n        x = self.block2(x)\n        x = self.pool2(x)\n        x = self.block3(x)\n        x = x.mean(dim=(2,3))\n        x = self.head(x)\n        return x","993d43a2":"model = Net(config['arch']).cuda()\nprint(f'# Parameters: {sum(p.numel() for p in model.parameters())}')","21a9d30a":"criterion = nn.CrossEntropyLoss()","19d8a91e":"if config['optimizer'] == 'momentum':\n    optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'], momentum=0.9)\nelif config['optimizer'] == 'adam':\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])","5f5351c7":"scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=config['epochs'])","f4ffc640":"t0 = time()\n\n# mixed precision\nscaler = torch.cuda.amp.GradScaler(enabled=config['amp']) \n# validation metric \ucd08\uae30\ud654\nbest_val_metric = np.inf\n# learning curve\nlearning_curve = pd.DataFrame(columns=['trn_loss', 'trn_metric', 'val_loss', 'val_metric'])\n\n# loop\nfor epoch in range(config['epochs']):\n    \n    # \ud604\uc7ac learning rates \ucd9c\ub825\n    current_lrs = [x[\"lr\"] for x in optimizer.param_groups]\n    print(f'EPOCH: {epoch} | LRs: {set(current_lrs)}')\n    \n    # train\n    model.train()\n    train_loss = 0.0\n    train_metric = 0.0\n    for inputs, targets in train_loader:\n        inputs, targets = inputs.cuda(), targets.cuda()\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast(enabled=config['amp']):\n            logits = model(inputs) \n            loss = criterion(logits, targets)\n        \n        # mixed precision\n        scaler.scale(loss).backward() \n        scaler.step(optimizer)  \n        scaler.update() \n        \n        optimizer.step()\n        scheduler.step()\n        \n        train_loss += loss.item() * len(inputs) \/ len(train_dataset)\n        train_metric -= (logits.detach().cpu().numpy().argmax(axis=1)==targets.detach().cpu().numpy()).mean() * len(inputs) \/ len(train_dataset)\n            \n    str_train_loss = str(train_loss)[:9]\n    str_train_metric = str(train_metric)[:9]\n    print(f'(train) LOSS: {str_train_loss} | METRIC: {str_train_metric}')\n\n    # validate\n    model.eval()\n    val_loss = 0.0\n    val_metric = 0.0\n    val_preds = []\n    val_targets = []\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.cuda(), targets.cuda()\n            logits = model(inputs) \n            val_preds += logits.cpu().tolist()\n            val_targets += targets.cpu().tolist()\n            val_loss += loss.item() * len(inputs) \/ len(val_dataset)\n    \n    val_preds, val_targets = np.array(val_preds), np.array(val_targets)\n    val_metric = -(val_preds.argmax(axis=1)==val_targets).mean() # negative accuracy\n    str_val_metric = str(val_metric)[:9]\n    str_val_loss = str(val_loss)[:9]\n    print(f'(valid) LOSS: {str_val_loss} | METRIC: {str_val_metric}')\n    \n    # learning curve \uc5c5\ub370\uc774\ud2b8\n    learning_curve.loc[epoch] = train_loss, train_metric, val_loss, val_metric\n\n    if val_metric <= best_val_metric:\n        # val metric\uc774 \uac1c\uc120\ub410\uc73c\uba74 info \uc800\uc7a5\n        best_info = {\n            'epoch': epoch,\n            'model': deepcopy(model.state_dict()),\n            'amp_scaler': deepcopy(scaler.state_dict()),\n            'optimizer': deepcopy(optimizer.state_dict()),\n            'scheduler': deepcopy(scheduler.state_dict()),\n            'learning_rates': current_lrs,\n            'train_loss': train_loss,\n            'train_metric': train_metric,\n            'val_loss': val_loss,\n            'val_metric': val_metric,\n            'val_preds': val_preds,\n            'config': config\n        }\n        best_val_metric = val_metric\n\nruntime = int(time()-t0)\nprint(f'Best Val Score: {best_val_metric}')\nprint(f'Runtime: {runtime}')\n\n# learning curve \uadf8\ub9ac\uae30\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\nfig.suptitle('Learning Curves')\naxes[0].plot(learning_curve['trn_loss'], label='trn_loss')\naxes[0].plot(learning_curve['val_loss'], label='val_loss')\naxes[0].set_ylim(0.0, 0.01)\naxes[0].legend()\naxes[1].plot(-learning_curve['trn_metric'], label='trn_metric')\naxes[1].plot(-learning_curve['val_metric'], label='val_metric')\naxes[1].set_ylim(0.99, 1.0)\naxes[1].legend()\nplt.tight_layout()\nplt.show()","e54b5620":"model.load_state_dict(best_info['model'])","277a23fd":"# \ud559\uc2b5\ub41c fold0 \ubaa8\ub378\ub85c oof \uc608\uce21\noof_preds = np.zeros(len(x_data))\ntmp_val_preds = []\nwith torch.no_grad():\n    for inputs, targets in val_loader:\n        tmp_val_preds.append(model(inputs.cuda()).cpu().numpy().argmax(axis=1))\noof_preds[folds[0][1]] = np.concatenate(tmp_val_preds)","f8b3fec2":"test_dataset = MNISTDataset(x_test, transforms=None)\ntest_loader = torch.utils.data.DataLoader(test_dataset, config['batch_size'], shuffle=False, num_workers=config['num_workers'])","50acbd5c":"# \ud559\uc2b5\ub41c fold0 \ubaa8\ub378\ub85c test \uc608\uce21\ntmp_test_preds = []\nwith torch.no_grad():\n    for inputs in test_loader:\n        tmp_test_preds.append(model(inputs.cuda()).cpu().numpy())\ntest_preds = np.concatenate(tmp_test_preds)\/len(folds)","08a18d37":"# fold1~4 \ubaa8\ub378\ub4e4 \ud559\uc2b5 \ubc0f \uc608\uce21\nfor trn_idx, val_idx in folds[1:]:\n    \n    x_train, x_val, y_train, y_val = x_data[trn_idx], x_data[val_idx], y_data[trn_idx], y_data[val_idx]\n    train_dataset = MNISTDataset(x_train, y_train, transforms=train_transforms)\n    train_loader = torch.utils.data.DataLoader(train_dataset, config['batch_size'], shuffle=True, num_workers=config['num_workers'])\n    val_dataset = MNISTDataset(x_val, y_val, transforms=None)\n    val_loader = torch.utils.data.DataLoader(val_dataset, config['batch_size'], shuffle=False, num_workers=config['num_workers'])\n    print(f'# Train Samples: {len(train_dataset)} | # Val Samples: {len(val_dataset)}')  \n    \n    model = Net(config['arch']).cuda()\n    print(f'# Parameters: {sum(p.numel() for p in model.parameters())}')\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    if config['optimizer'] == 'momentum':\n        optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'], momentum=0.9)\n    elif config['optimizer'] == 'adam':\n        optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n        \n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=config['epochs'])    \n    # train\n    t0 = time()\n\n    # mixed precision\n    scaler = torch.cuda.amp.GradScaler(enabled=config['amp']) \n\n    # validation metric \ucd08\uae30\ud654\n    best_val_metric = np.inf\n    # learning curve\n    learning_curve = pd.DataFrame(columns=['trn_loss', 'trn_metric', 'val_loss', 'val_metric'])\n\n    # loop\n    for epoch in range(20):\n\n        # \ud604\uc7ac learning rates \ucd9c\ub825\n        current_lrs = [x[\"lr\"] for x in optimizer.param_groups]\n        print(f'EPOCH: {epoch} | LRs: {set(current_lrs)}')\n\n        # train\n        model.train()\n        train_loss = 0.0\n        train_metric = 0.0\n        for inputs, targets in train_loader:\n            inputs, targets = inputs.cuda(), targets.cuda()\n            optimizer.zero_grad()\n            with torch.cuda.amp.autocast(enabled=config['amp']):\n                logits = model(inputs) \n                loss = criterion(logits, targets)\n\n            # mixed precision\n            scaler.scale(loss).backward() \n            scaler.step(optimizer)  \n            scaler.update() \n\n            optimizer.step()\n            scheduler.step()\n\n            train_loss += loss.item() * len(inputs) \/ len(train_dataset)\n            train_metric -= (logits.detach().cpu().numpy().argmax(axis=1)==targets.detach().cpu().numpy()).mean() * len(inputs) \/ len(train_dataset)\n\n        str_train_loss = str(train_loss)[:9]\n        str_train_metric = str(train_metric)[:9]\n        print(f'(train) LOSS: {str_train_loss} | METRIC: {str_train_metric}')\n\n        # validate\n        model.eval()\n        val_loss = 0.0\n        val_metric = 0.0\n        val_preds = []\n        val_targets = []\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                inputs, targets = inputs.cuda(), targets.cuda()\n                logits = model(inputs) \n                val_preds += logits.cpu().tolist()\n                val_targets += targets.cpu().tolist()\n                val_loss += loss.item() * len(inputs) \/ len(val_dataset)\n\n        val_preds, val_targets = np.array(val_preds), np.array(val_targets)\n        val_metric = -(val_preds.argmax(axis=1)==val_targets).mean() # negative accuracy\n        str_val_metric = str(val_metric)[:9]\n        str_val_loss = str(val_loss)[:9]\n        print(f'(valid) LOSS: {str_val_loss} | METRIC: {str_val_metric}')\n\n        # learning curve \uc5c5\ub370\uc774\ud2b8\n        learning_curve.loc[epoch] = train_loss, train_metric, val_loss, val_metric\n\n\n        if val_metric <= best_val_metric:\n            # val metric\uc774 \uac1c\uc120\ub410\uc73c\uba74 info \uc800\uc7a5\n            best_info = {\n                'epoch': epoch,\n                'model': deepcopy(model.state_dict()),\n                'amp_scaler': deepcopy(scaler.state_dict()),\n                'optimizer': deepcopy(optimizer.state_dict()),\n                'scheduler': deepcopy(scheduler.state_dict()),\n                'learning_rates': current_lrs,\n                'train_loss': train_loss,\n                'train_metric': train_metric,\n                'val_loss': val_loss,\n                'val_metric': val_metric,\n                'val_preds': val_preds,\n                'config': config\n            }\n            best_val_metric = val_metric\n\n    runtime = int(time()-t0)\n    print(f'Best Val Score: {best_val_metric}')\n    print(f'Runtime: {runtime}')\n\n    # learning curve \uadf8\ub9ac\uae30\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\n    fig.suptitle('Learning Curves')\n    axes[0].plot(learning_curve['trn_loss'], label='trn_loss')\n    axes[0].plot(learning_curve['val_loss'], label='val_loss')\n    axes[0].set_ylim(0.0, 0.01)\n    axes[0].legend()\n    axes[1].plot(-learning_curve['trn_metric'], label='trn_metric')\n    axes[1].plot(-learning_curve['val_metric'], label='val_metric')\n    axes[1].set_ylim(0.99, 1.0)\n    axes[1].legend()\n    plt.tight_layout()\n    plt.show()\n    \n    # oof, test \uc608\uce21\n    model.load_state_dict(best_info['model'])\n    tmp_test_preds = []\n    tmp_val_preds = []\n    with torch.no_grad():\n        for inputs in test_loader:\n            tmp_test_preds.append(model(inputs.cuda()).cpu().numpy())\n        for inputs, targets in val_loader:\n            tmp_val_preds.append(model(inputs.cuda()).cpu().numpy().argmax(axis=1))\n    oof_preds[val_idx] = np.concatenate(tmp_val_preds)\n    test_preds += np.concatenate(tmp_test_preds)\/len(folds)","b58a365b":"print('OOF ACC:', (oof_preds==y_data).mean())","e246762a":"## \ub370\uc774\ud130 \ub85c\ub4dc \ubc0f Fold \ub098\ub204\uae30\n\ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\uc744 \uc704\ud574\uc11c Fold0\ub9cc \ud65c\uc6a9\ud558\uace0, \uac00\uc7a5 \uc88b\uc740 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \ucc3e\uc73c\uba74 5Fold\ub85c \ud559\uc2b5\uc2dc\ucf1c \ud3c9\uade0\uc744 \ub0b4\uc5b4 \uc81c\ucd9c\ud569\ub2c8\ub2e4.","b1f07284":"## \u115fOptimizer","4a43fb82":"## Seed \uace0\uc815","dcdaf683":"## Hyperparameter\n\ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \uad00\ub9ac\ud558\uae30 \uc704\ud574 Config\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.","dfc3b3c0":"## Train","95171135":"\ubcf8 \ub300\ud68c\ub294 \uc81c\uac00 \ud22c\ube45\uc2a4\ub77c\ub294 \ub3d9\uc544\ub9ac\uc5d0 \uc788\uc744 \ub54c \ub9cc\ub4e4\uc5b4\ub454 \uac83\uc785\ub2c8\ub2e4.\n\n\uc194\ub8e8\uc158 \ucf54\ub4dc\ub294 \uae40\uc724\uc218\ub2d8\uc758 [\ub9c1\ud06c](https:\/\/github.com\/tobigs-datamarket\/tobigs-14th\/blob\/master\/7wk_%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%8B%AC%ED%99%94\/%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%8B%AC%ED%99%94_14%EA%B8%B0%20%EA%B9%80%EC%9C%A4%EC%88%98.ipynb)\ub97c \ucc38\uc870\ud558\uc600\uc74c\uc744 \ubbf8\ub9ac \ub9d0\uc500\ub4dc\ub9bd\ub2c8\ub2e4.","1c6f1262":"OneCycleLR scheduler\uc744 \uc4f0\uba74, \uae30\ubcf8 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc0c1\ud0dc\uc5d0\uc11c\ub294 max_lr\uc758 1\/25\uc758 lr\uc5d0\uc11c \uc2dc\uc791\ud558\uc5ec \ucd1d Epoch\uc758 1\/4\ub9cc\ud07c max_lr\uae4c\uc9c0 lr\uc774 \uc99d\uac00\ud558\ub2e4\uac00 \uadf8 \uc774\ud6c4\ub294 max_lr\uc758 1\/10000\uae4c\uc9c0 lr\uc774 \uac10\uc18c\ud569\ub2c8\ub2e4. \uc99d\uac00\ud558\uac70\ub098 \uac10\uc18c\ud560 \ub54c\ub294 cosine\ud568\uc218\uc758 \ubaa8\uc591\uc744 \ub530\ub985\ub2c8\ub2e4.\n\nhttps:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.lr_scheduler.OneCycleLR\n\nhttps:\/\/arxiv.org\/abs\/1708.07120","d5bd95e2":"## Learning Rate Scheduler","be6d381e":"## Load Library","fdc897b8":"## Prediction","499a928f":"## \ub370\uc774\ud130 \ub85c\ub354 \uc815\uc758\n\ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc624\uae30 \uc704\ud55c \ub370\uc774\ud130\uc14b \ud074\ub798\uc2a4\ub97c \uc815\uc758\ud569\ub2c8\ub2e4. \uc5b4\ub5a4 \ud615\ud0dc\ub85c \ubd88\ub7ec\uc62c\uc9c0 \uc815\uc758\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","796fbefa":"\ubaa8\ub378\uc758 \ud559\uc2b5\uc744 \uadf8\ub300\ub85c \uc7ac\ud604\ud558\uae30 \uc704\ud574 Seed\ub97c \uace0\uc815\ud569\ub2c8\ub2e4.","5ea5b4bc":"## \ubaa8\ub378\n\n\ub124 \uc885\ub958\uc758 architecture\uc744 \uace0\ub824\ud558\uc600\uc2b5\ub2c8\ub2e4. arch 4\ub97c \uc81c\uc678\ud558\uace0\ub294 \ud30c\ub77c\ubbf8\ud130 \uac1c\uc218\ub97c \ube44\uc2b7\ud558\uac8c \ub9de\ucdb0\uc11c \ub3d9\uc77c\uc120\uc0c1\uc5d0\uc11c \ube44\uad50\ud558\uc600\uc2b5\ub2c8\ub2e4.\n### arch 1\n[residual connection](https:\/\/arxiv.org\/abs\/1512.03385)\uc774 \uc788\ub294 \uae30\ubcf8\uc801\uc778 \uad6c\uc870\n* Parameters: 288170\n\n### arch 2\n[mobilenetv2](https:\/\/arxiv.org\/abs\/1801.04381) \uae30\ubc18: inverted residual + depthwise seperable\n* Parameters: 282754\n\n### arch 3\n[ecanet](https:\/\/arxiv.org\/abs\/1910.03151) \uae30\ubc18: mobilenetv2 + channel-wise attention (eca)\n* Parameters: 282772\n\n### arch 4\n[mobilenetv3](https:\/\/arxiv.org\/abs\/1905.02244) \uae30\ubc18: mobilenetv2 + channel-wise attention (squeeze-excitation)\n* Parameters: 404974","3873c3f6":"## \uc190\uc2e4 \ud568\uc218\nMulti Class \ubd84\ub958 \ubb38\uc81c\uc774\ubbc0\ub85c CrossEntropy \ud568\uc218\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. Multi label \ubb38\uc81c\ub77c\uba74 SoftMarginLoss\ub97c \uc0ac\uc6a9\ud558\uba74 \ub429\ub2c8\ub2e4. "}}