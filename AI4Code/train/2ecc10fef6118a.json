{"cell_type":{"0b599754":"code","f9244df1":"code","2c03d511":"code","b035fe4e":"code","47116267":"code","fb4e94b4":"code","1d484bbe":"code","6782f6d9":"code","38380965":"code","d2748fc1":"code","45ffd631":"code","2e2d8a4f":"code","e7a6a658":"code","48b3bbbb":"code","3dc6291f":"code","ca2da6ad":"code","274154b6":"code","293835f5":"code","5479f206":"code","062c0b31":"code","49f9567e":"code","a4972113":"code","79b30beb":"code","6150fb48":"code","fea90bb3":"code","b6be8908":"code","7e8b6cd4":"code","70ea4fb4":"code","4d927cac":"code","f83acb8b":"code","da513782":"code","593942d2":"code","ecfc880d":"code","685d80ef":"code","056f8b02":"code","9a3eb5b8":"code","c4611296":"code","2fb9cde4":"code","ed4635c4":"code","097519aa":"code","d47348bc":"code","c2ef1a7e":"code","41c509ab":"code","785cb4a0":"code","eb7ccb61":"code","ace4041f":"code","049beae6":"code","36d21dca":"code","ee70c7a4":"code","0f7552bb":"code","70fa1a6f":"code","d9c18ce8":"code","372982b7":"code","91be57a4":"code","4b39d4cd":"code","694ed182":"code","0f969a5d":"code","1f40f219":"code","feb73165":"code","10003cca":"code","6a45b474":"code","38ee2a23":"code","347a3d6e":"code","46de1c70":"code","19b7c7ed":"code","83c6d34a":"code","7639830f":"code","9a5ab0aa":"code","97fc8807":"code","a17521e4":"code","cafd092b":"code","78eb7e3c":"code","0c2e436d":"code","38fb5d1b":"code","9d488359":"code","7a13d00e":"code","7c331087":"code","e4eca3a2":"code","a43e4e69":"code","b38194cf":"code","31478b04":"code","755c8c43":"code","c12ce0bb":"code","774e2c6e":"code","2ae777a6":"code","6a576e2f":"code","435b48b6":"code","2e45e7a7":"code","06700e57":"code","a2520a02":"code","82d9cb31":"code","b9f5c585":"code","f1b4e50e":"code","9ba48f16":"code","c5bba27a":"code","88204a50":"code","17595194":"code","b1657867":"code","45dff99d":"code","dafaeb26":"code","152f8d2b":"code","cc12cabe":"code","d652a0e9":"code","36f8f85f":"code","446ea007":"code","6f11b5a6":"code","58f1087e":"code","cdf61950":"code","8538212d":"code","462e97c4":"code","72771ca2":"code","a66bf914":"code","fb89815e":"code","3cf64e4a":"code","fea3fc2e":"code","b9c618b5":"code","828694dc":"code","16767704":"code","45afd9a8":"code","032be895":"code","f15e46ac":"code","e07084f7":"code","45981575":"code","db5384dc":"code","bd7d0ca1":"code","091a9035":"code","62d6c289":"code","c33af488":"code","cde52c52":"code","1209aea7":"code","3fa27786":"code","521d0234":"code","29a31683":"code","c09d863a":"code","2cb248af":"code","43e83fe9":"code","95f7c8cd":"code","211eb022":"code","1096bdfa":"code","0cecaef7":"code","95ddc93e":"code","89a1bcfd":"code","611f4803":"code","87d31d85":"code","1f89ee1a":"code","d127f1cd":"code","36bf8671":"code","8df91f10":"code","2d7eab9d":"code","e8beaf50":"code","b620d777":"code","df2cd0c8":"code","21d624d5":"code","8285d6eb":"code","98184330":"code","748663f8":"code","575ddcee":"code","a8ba6d42":"code","25beafd2":"code","076551ef":"code","3701f9b1":"code","7937c222":"code","0ac1a13a":"code","2e0c10a7":"code","d4c8398a":"code","4563fdfd":"code","fce9ea14":"code","c3f5d3ce":"code","801835af":"code","0d5af119":"code","d3c885b3":"code","df0222aa":"code","a1cd2202":"code","91410357":"code","49f52886":"code","a9396665":"code","c8f8750c":"code","988f9d67":"code","98c93ae4":"code","aacfaf5e":"code","44ed27ec":"code","bd8874d7":"code","d9cb2e3d":"code","6172a6d3":"code","6523d179":"code","0da44ab3":"code","8b7e371b":"code","59e17d7b":"code","46cef0b5":"code","eb341f2a":"code","e3be5305":"code","093c75e8":"code","c4457f3b":"markdown","6fea9fb4":"markdown","482efe47":"markdown","81b1d77c":"markdown","9044273b":"markdown","9cc3d31b":"markdown","51a8adde":"markdown","e7176250":"markdown","1252294a":"markdown","c690bc27":"markdown","0f99a91e":"markdown","9f1a13b7":"markdown","99b842b3":"markdown","f7fe4cb2":"markdown","650aaf3b":"markdown","56242088":"markdown","61312d1d":"markdown","9b67227f":"markdown","a0260eb8":"markdown","088a195b":"markdown","39de3ce8":"markdown","822707a3":"markdown","77d8e610":"markdown","76708660":"markdown","7facc751":"markdown","e3ef8613":"markdown","4efbbff3":"markdown","9de0a699":"markdown","695a449c":"markdown","9c0943ed":"markdown","a8c9db67":"markdown","fc1682e6":"markdown","8ca37aaf":"markdown","4af82920":"markdown","dd0b7857":"markdown","5958230a":"markdown","4001a369":"markdown","c6694ab9":"markdown","2eca8b0e":"markdown","5957b023":"markdown","c2a82cda":"markdown","c1a4c041":"markdown","c68ee386":"markdown","9611a7ef":"markdown","59fa04ca":"markdown","47dcd265":"markdown","d350caef":"markdown","c81a1f25":"markdown","bb1de26d":"markdown","3685d200":"markdown","4e2f21a0":"markdown","b3e410c8":"markdown","524ce385":"markdown","6ee7cb1d":"markdown","3cfc7eb4":"markdown","d1c3f48f":"markdown","023566ff":"markdown","716d6f03":"markdown","eeaa84f2":"markdown","bea48fea":"markdown","4318db9b":"markdown","eb54c307":"markdown","56fe0d26":"markdown","45b0721c":"markdown","a41ce7dd":"markdown","aa1d251c":"markdown","50d1165e":"markdown","e3d64e22":"markdown","aec7816d":"markdown"},"source":{"0b599754":"### Download required packages\n\n# import nltk\n# nltk.download('gutenberg')\n# nltk.download('genesis')","f9244df1":"##### Imporing Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n## Importing Textblob package\nfrom textblob import TextBlob\n\n# Importing CountVectorizer for sparse matrix\/ngrams frequencies\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n## Import datetime\nimport datetime as dt\n\n\nimport nltk.compat\nimport itertools\n\n\n","2c03d511":"#### Checking on encoding","b035fe4e":"import chardet","47116267":"##### Read the data file\nfilepath = \"..\/input\/ebi-finance-and-qlikview-incidents-data\/Incident_2017_18_Final.csv\"\n\n\n## Checking the encoding factor\nwith open(filepath,\"rb\") as mydata:\n    result = chardet.detect(mydata.read(1000000))","fb4e94b4":"result","1d484bbe":"##### Read the data file\nfilepath = \"..\/input\/ebi-finance-and-qlikview-incidents-data\/Incident_2017_18_Final.csv\"\ntrain_incidents = pd.read_csv(filepath,encoding=\"Windows-1252\")\n","6782f6d9":"train_incidents[\"short_description_nwords\"] = train_incidents[\"short_description\"].apply(lambda x: len(str(x).split(\" \")))","38380965":"\ntrain_incidents[[\"short_description\",\"short_description_nwords\"]].sort_values(by = \"short_description_nwords\",ascending = True).head()","d2748fc1":"\n\ntrain_incidents[[\"short_description\",\"short_description_nwords\"]].sort_values(by = \"short_description_nwords\",ascending = False).head()","45ffd631":"train_incidents[\"short_description_nchars\"] = train_incidents[\"short_description\"].str.len()","2e2d8a4f":"\ntrain_incidents[[\"short_description\",\"short_description_nchars\"]].sort_values(by = \"short_description_nchars\",ascending = False).head()","e7a6a658":"\ntrain_incidents[[\"short_description\",\"short_description_nchars\"]].sort_values(by = \"short_description_nchars\",ascending = True).head()","48b3bbbb":"#sum of words\/total words","3dc6291f":"def ave_word_len(sentence):\n    words  = sentence.split(\" \")\n    return ((sum((len(word) for word in words))\/len(words)))\n\ntrain_incidents[\"short_description_avg_word_len\"] = train_incidents[\"short_description\"].apply(ave_word_len)","ca2da6ad":"train_incidents[[\"short_description\",\"short_description_avg_word_len\"]].sort_values(by = \"short_description_avg_word_len\",ascending = True).head()","274154b6":"## Importing stop words from nltk.corpus\nfrom nltk.corpus import stopwords","293835f5":"stop = stopwords.words(\"english\")","5479f206":"train_incidents[\"short_description_nstopwords\"] = train_incidents[\"short_description\"].apply(lambda word: len([x for x in word.split(\" \") if x in stop]))\ntrain_incidents[[\"short_description\",\"short_description_nstopwords\"]].sort_values(by = \"short_description_nstopwords\",ascending = False).head()","062c0b31":"train_incidents[\"short_description_ndigits\"] = train_incidents[\"short_description\"].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n\ntrain_incidents[[\"short_description\",\"short_description_ndigits\"]].sort_values(by = \"short_description_ndigits\",ascending = False).head()","49f9567e":"train_incidents[\"short_description_nupper\"] = train_incidents[\"short_description\"].apply((lambda word: len([x for x in word.split() if x.isupper()])))\ntrain_incidents[[\"short_description\",\"short_description_nupper\"]].sort_values(by = \"short_description_nupper\",ascending = False).head()","a4972113":"train_incidents[\"short_description\"] = train_incidents[\"short_description\"].apply(lambda x: x.lower())\ntrain_incidents[\"short_description\"].head()","79b30beb":"train_incidents[\"short_description\"] = train_incidents[\"short_description\"].str.replace(\"qlik view\",\"qlikview\")\ntrain_incidents[\"short_description\"] = train_incidents[\"short_description\"].str.replace(\"qv\",\"qlikview\")\ntrain_incidents[\"short_description\"] = train_incidents[\"short_description\"].str.replace(\"wrongly\",\"wrong\")\n","6150fb48":"train_incidents[\"short_description\"] = train_incidents[\"short_description\"].str.replace(\"[^\\w\\s]\",\"\")\ntrain_incidents[\"short_description\"].tail()","fea90bb3":"train_incidents[\"short_description\"] = train_incidents[\"short_description\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))","b6be8908":"from textblob import Word","7e8b6cd4":"train_incidents[\"short_description\"] = train_incidents[\"short_description\"].apply(lambda x: \" \".join([Word(myword).lemmatize() for myword in x.split()])  )","70ea4fb4":"train_incidents[\"short_description\"].head(5)","4d927cac":"### Most frequent words in short description\nShort_description_most_freq_words = pd.Series(\" \".join(train_incidents[\"short_description\"]).split()).value_counts()\nShort_description_most_freq_words.head(20)","f83acb8b":"### Least frequent words in short description\nshort_description_least_freq_words =  pd.Series(\" \".join(train_incidents[\"short_description\"]).split()).value_counts().sort_values(ascending = True)\nshort_description_least_freq_words.head(10)","da513782":"## Correction for top 10 sentences\n##  train_incidents[\"short_description\"] = train_incidents[\"short_description\"].apply(lambda x: str(TextBlob(x).correct()))","593942d2":"TextBlob(train_incidents[\"short_description\"][1]).words","ecfc880d":"train_incidents[\"short_description\"][1]","685d80ef":"train_incidents[\"short_description_tokens\"] =  train_incidents[\"short_description\"].apply(lambda x: TextBlob(x).words)","056f8b02":"train_incidents[\"short_description_tokens\"].head(10)","9a3eb5b8":"from nltk import word_tokenize,sent_tokenize","c4611296":"train_incidents[\"short_description\"].apply(lambda x: word_tokenize(x))","2fb9cde4":"from nltk.stem import PorterStemmer\n\nst = PorterStemmer()\n\ntrain_incidents[\"short_description\"][:5].apply(lambda words: \" \".join([st.stem(word) for word in words.split()]))\n","ed4635c4":"train_incidents[\"sys_created_on\"] = (pd.to_datetime(train_incidents[\"sys_created_on\"],format='%d\/%m\/%Y %H:%M'))\ntrain_incidents[\"sys_updated_on\"] = (pd.to_datetime(train_incidents[\"sys_updated_on\"],format='%d\/%m\/%Y %H:%M'))\ntrain_incidents[\"opened_at\"] = (pd.to_datetime(train_incidents[\"opened_at\"],format='%d\/%m\/%Y %H:%M'))\ntrain_incidents[\"resolved_at\"] = (pd.to_datetime(train_incidents[\"resolved_at\"],format='%d\/%m\/%Y %H:%M'))\n","097519aa":"### Extracting dates from datetime object\ntrain_incidents[\"opened_at_date\"] = train_incidents[\"opened_at\"].dt.date","d47348bc":"## Creating Category GROUPBY Object\nincidents_category = train_incidents.groupby(\"category\")\n## Creating sub Category GROUPBY Object\nincidents_incident_subcategory = train_incidents.groupby(\"incident_subcategory\")\n## Creating priority GROUPBY Object\nincidents_priority= train_incidents.groupby(\"priority\")\n## Creating priority GROUPBY Object\nincidents_urgency= train_incidents.groupby(\"urgency\")\n## Creating re-open GROUPBY Object\nincidents_reopen_count= train_incidents.groupby(\"reopen_count\")\n## Creating made_sla GROUPBY Object\nincidents_made_sla= train_incidents.groupby(\"made_sla\")\n## Creating incident type GROUPBY Object\nincidents_type= train_incidents.groupby(\"incident_type\")\n\n\n## Creating impact GROUPBY Object\nincidents_impact= train_incidents.groupby(\"impact\")\n\n## Creating Escalations GROUPBY Object\nincidents_escalation= train_incidents.groupby(\"escalation\")\n\n## Creating E2E resolution met Object\nincidents_e2e_resolution_met= train_incidents.groupby(\"e2e_resolution_met\")\n\n## Creating location Object\nincidents_location = train_incidents.groupby(\"current_location\")\n\n## Creating location Object\nincidents_country = train_incidents.groupby(\"country\")\n\n## Creating contact type Object\nincidents_contact_type = train_incidents.groupby(\"contact_type\")\n\n## Creating affected user Object\nincidents_affected_user = train_incidents.groupby(\"affected_user\")\n\n## Creating assigned group Object\nincidents_assignment_group = train_incidents.groupby(\"assignment_group\")\n\n\n\n\n\n\n\n\n","c2ef1a7e":"### Analyzing top 20 frequent words\n\n\nsd_freq_plot = Short_description_most_freq_words.head(20).sort_values(ascending = True).plot(kind=\"barh\",title = \"Top 20 Frequent Number Of Words\")\n\nplt.style.use(\"ggplot\")\nsd_freq_plot.set_xlabel(\"Frequency\")\nsd_freq_plot.set_ylabel(\"Terms\")\n\ntotals = []\nfor i in sd_freq_plot.patches:\n    totals.append(i.get_width())\n\nfor i in sd_freq_plot.patches:\n    sd_freq_plot.text(i.get_width()+.3,i.get_y()+0.1,str(i.get_width()),fontsize = 8,color= 'black')\n    \n","41c509ab":"### Lets generate bigrams and store it in a bi_grams variable\n### train_incidents[\"bi_grams\"] = train_incidents[\"short_description\"].apply(lambda x: TextBlob(x).ngrams(2))\n### train_incidents[\"bi_grams\"].head()\n","785cb4a0":"### Lets generate trigrams and store it in a tri_grams variable\n### train_incidents[\"tri_grams\"] = train_incidents[\"short_description\"].apply(lambda x: TextBlob(x).ngrams(3))\n### train_incidents[\"tri_grams\"].head()","eb7ccb61":"bigrams = TextBlob(\" \".join(train_incidents[\"short_description\"])).ngrams(2)\n#bigrams = pd.Series(bigrams).apply(lambda x: list(x))","ace4041f":"word_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(train_incidents[\"short_description\"])\nfrequencies = sum(sparse_matrix).toarray()[0]\nbi_grams_df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n    ","049beae6":"bi_grams_df.sort_values(by = \"frequency\",ascending=False).head(20)\n\n#grams_df[grams_df.index.str.contains(\"reconciliation\")]","36d21dca":"### Analyzing top 20 frequent BI Gram words\n\nplt.style.use(\"ggplot\")\nplt.xlabel(\"Frequency\",)\nplt.ylabel(\"Terms\")\ntop20_bigrams = bi_grams_df[\"frequency\"].sort_values(ascending = False).head(20)\n\ntop20_bigrams.head(20).sort_values(ascending = True).plot(kind=\"barh\",title = \"Top 20 Frequent Bi Grams\")\n","ee70c7a4":"train_incidents_word_issue = train_incidents[train_incidents[\"short_description\"].str.contains(\"issue\")]\n\ntrain_incidents_word_issue[\"short_description\"] = train_incidents_word_issue[\"short_description\"].str.replace(\"issue qlikview\",\"qliview issue\")\ntrain_incidents_word_issue[\"short_description\"] = train_incidents_word_issue[\"short_description\"].str.replace(\"issue data\",\"data issue\")\ntrain_incidents_word_issue[\"short_description\"] = train_incidents_word_issue[\"short_description\"].str.replace(\"issue mrh\",\"mrh issue\")\ntrain_incidents_word_issue[\"short_description\"] = train_incidents_word_issue[\"short_description\"].str.replace(\"issue dashboard\",\"dashboard issue\")\ntrain_incidents_word_issue[\"short_description\"] = train_incidents_word_issue[\"short_description\"].str.replace(\"issue access\",\"access issue\")\ntrain_incidents_word_issue[\"short_description\"] = train_incidents_word_issue[\"short_description\"].str.replace(\"issue query\",\"query issue\")\ntrain_incidents_word_issue[\"short_description\"] = train_incidents_word_issue[\"short_description\"].str.replace(\"issue ebi\",\"ebi issue\")\ntrain_incidents_word_issue[\"short_description\"] = train_incidents_word_issue[\"short_description\"].str.replace(\"issue report\",\"report issue\")\ntrain_incidents_word_issue[\"short_description\"] = train_incidents_word_issue[\"short_description\"].str.replace(\"issue hr\",\"hr issue\")\ntrain_incidents_word_issue[\"short_description\"] = train_incidents_word_issue[\"short_description\"].str.replace(\"issue mrh2\",\"mrh2 issue\")\ntrain_incidents_word_issue[\"short_description\"] = train_incidents_word_issue[\"short_description\"].str.replace(\"issue master\",\"master issue\")\ntrain_incidents_word_issue[\"short_description\"] = train_incidents_word_issue[\"short_description\"].str.replace(\"issue file\",\"file issue\")\n\n","0f7552bb":"word_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(train_incidents_word_issue[\"short_description\"])\nfrequencies = sum(sparse_matrix).toarray()[0]\nbi_grams_issue_df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n    ","70fa1a6f":"bi_grams_issue_df[bi_grams_issue_df.index.str.contains(\"issue\")].sort_values(by = \"frequency\",ascending=False).head(10)\n","d9c18ce8":"### Analyzing top 20 frequent BI Gram words- word containing issue\n\nplt.style.use(\"ggplot\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Terms\")\nplt.title(\"Top 10 Frequent Bi Grams contains word \"\"issue\"\"\")\ntop20_bigrams_issue = bi_grams_issue_df[\"frequency\"].sort_values(ascending = False)\n\ntop20_bigrams_issue_plot = top20_bigrams_issue[top20_bigrams_issue.index.str.contains(\"issue\")].head(10).sort_values(ascending = True).plot(kind=\"barh\")\n\ntotals = []\nfor i in top20_bigrams_issue_plot.patches:\n    totals.append(i.get_width())\n\nfor i in top20_bigrams_issue_plot.patches:\n    top20_bigrams_issue_plot.text(i.get_width()+.3,i.get_y()+0.1,str(i.get_width()),fontsize = 10,color= 'black')\n","372982b7":"train_incidents_word_issue.head(1)","91be57a4":"train_incidents_word_issue[\"data_issue_count\"] = \"\"\n","4b39d4cd":"\nword_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\nfor each in (train_incidents_word_issue[\"short_description\"].index):\n    text_issue_list = [train_incidents_word_issue[\"short_description\"][each]]\n    sparse_matrix = word_vectorizer.fit_transform(text_issue_list)\n    frequencies = sum(sparse_matrix).toarray()[0]\n    bi_grams_issue_df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n    train_incidents_word_issue[\"data_issue_count\"][each] = bi_grams_issue_df[bi_grams_issue_df.index.str.contains(\"^data issue$\")][\"frequency\"].sum()\n    ","694ed182":"### Occurance of Word \"data issue\" in \"short description\" VS categories\nissue_category_groupby = train_incidents_word_issue.groupby(by=\"category\")\nissue_category_groupby[\"data_issue_count\"].sum().sort_values(ascending = False)","0f969a5d":"### Word \"data issue\" in \"short description\" VS sub categories\nissue_subcategory_groupby = train_incidents_word_issue.groupby(by=\"incident_subcategory\")\nissue_subcategory_groupby[\"data_issue_count\"].sum().sort_values(ascending = False)","1f40f219":"### Word \"data issue\" in \"short description\" VS impact\nissue_impact_groupby = train_incidents_word_issue.groupby(by=\"impact\")\nissue_impact_plot= issue_impact_groupby[\"data_issue_count\"].sum()\nissue_impact_plot","feb73165":"### Word \"data issue\" in \"short description\" VS source\nissue_assignment_groupby = train_incidents_word_issue.groupby(by=\"assignment_group\")\nissue_assignment_freq= issue_assignment_groupby[\"data_issue_count\"].sum()\nissue_assignment_freq","10003cca":"### Word \"data issue\" in \"short description\" VS location\nissue_location_groupby = train_incidents_word_issue.groupby(by=\"current_location\")\nissue_location_freq= issue_location_groupby[\"data_issue_count\"].sum()\nissue_location_freq","6a45b474":"### Word \"data issue\" in \"short description\" VS Contact type\nissue_contact_type_groupby = train_incidents_word_issue.groupby(by=\"contact_type\")\nissue_contact_type_freq= issue_contact_type_groupby[\"data_issue_count\"].sum()\nissue_contact_type_freq","38ee2a23":"### Word \"data issue\" in \"short description\" VS Syngenta location\nissue_syn_loc_groupby = train_incidents_word_issue.groupby(by=\"syngenta_location\")\nissue_syn_loc_freq= issue_syn_loc_groupby[\"data_issue_count\"].sum()\nissue_syn_loc_freq.sort_values(ascending=False)","347a3d6e":"train_incidents_word_issue[\"top10_issue_count\"] = \"\"\n\nword_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\nfor each in (train_incidents_word_issue[\"short_description\"].index):\n    text_issue_list = [train_incidents_word_issue[\"short_description\"][each]]\n    sparse_matrix = word_vectorizer.fit_transform(text_issue_list)\n    frequencies = sum(sparse_matrix).toarray()[0]\n    bi_grams_top_10_issue_df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n    train_incidents_word_issue[\"top10_issue_count\"][each] = bi_grams_top_10_issue_df[bi_grams_top_10_issue_df.index.str.contains(\"^data issue$\")][\"frequency\"].sum() + bi_grams_top_10_issue_df[bi_grams_top_10_issue_df.index.str.contains(\"^dashboard issue$\")][\"frequency\"].sum() + bi_grams_top_10_issue_df[bi_grams_top_10_issue_df.index.str.contains(\"^query issue$\")][\"frequency\"].sum() + bi_grams_top_10_issue_df[bi_grams_top_10_issue_df.index.str.contains(\"^mapping issue$\")][\"frequency\"].sum() + bi_grams_top_10_issue_df[bi_grams_top_10_issue_df.index.str.contains(\"^access issue$\")][\"frequency\"].sum() + bi_grams_top_10_issue_df[bi_grams_top_10_issue_df.index.str.contains(\"^mrh issue$\")][\"frequency\"].sum() + bi_grams_top_10_issue_df[bi_grams_top_10_issue_df.index.str.contains(\"^file issue$\")][\"frequency\"].sum() + bi_grams_top_10_issue_df[bi_grams_top_10_issue_df.index.str.contains(\"^ebi issue$\")][\"frequency\"].sum() + bi_grams_top_10_issue_df[bi_grams_top_10_issue_df.index.str.contains(\"^report issue$\")][\"frequency\"].sum() + bi_grams_top_10_issue_df[bi_grams_top_10_issue_df.index.str.contains(\"^qlikview issue$\")][\"frequency\"].sum()\n    ","46de1c70":"issue_impact_groupby[\"top10_issue_count\"].sum()","19b7c7ed":"plt.style.use(\"ggplot\")\nplt.title(\"Top 10 issues VS tickets Priority\")\nplt.ylabel(\"Top 10 Issues\")\nplt.xlabel(\"Tickets Priority\")\nissue_priority_groupby = train_incidents_word_issue.groupby(\"priority\")\nissue_priority_groupby[\"top10_issue_count\"].sum().plot(kind = \"bar\")\n","83c6d34a":"train_incidents_word_issue[\"opened_at_date\"].head(2)\n","7639830f":"train_incidents_word_issue[\"opened_at_year\"] = train_incidents_word_issue[\"opened_at\"].dt.year","9a5ab0aa":"# Top 10 data issues VS ticket Opened Year Analysis\nplt.style.use(\"ggplot\")\nplt.title(\"Top 10 data issues VS ticket Opened Year Analysis\")\nplt.ylabel(\"Top 10 Issues\")\nissue_opened_at_year_grpby = train_incidents_word_issue.groupby(\"opened_at_year\")\nissue_opened_at_year_grpby[\"top10_issue_count\"].sum().plot(kind =\"bar\")\nplt.xlabel(\"Tickets Opened at Year\")\n","97fc8807":"word_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(train_incidents[\"short_description\"])\n#sparse_matrix = word_vectorizer.fit_transform(train_incidents[\"short_description_tokens\"])\nfrequencies = sum(sparse_matrix).toarray()[0]\ntri_grams_df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n\n\n","a17521e4":"tri_grams_df.sort_values(by = \"frequency\",ascending=False).head(20)\n\n##grams_df[grams_df.index.str.contains(\"reconciliation\")]","cafd092b":"### Analyzing top 20 frequent Tri Gram words\n\nplt.style.use(\"ggplot\")\nplt.xlabel(\"Terms\",)\nplt.ylabel(\"Frequency\")\ntrigrams_short_description = tri_grams_df[\"frequency\"].sort_values(ascending = False)\ntop20_trigrams = tri_grams_df[\"frequency\"].sort_values(ascending = False).head(20)\n\ntop5_trigrams_plot =  top20_trigrams.head(5).sort_values(ascending = False).plot(kind=\"bar\",title = \"Top 5 Frequent Tri Grams\")\ntop5_trigrams_plot\nplt.xticks(rotation=75)\n\n","78eb7e3c":"#### Find N grams for category - incident\n\nword_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(incidents_category.get_group(\"Incident\")[\"short_description\"])\nfrequencies_Incident_cate = sum(sparse_matrix).toarray()[0]\ngrams_df_incident_cate = pd.DataFrame(frequencies_Incident_cate, index=word_vectorizer.get_feature_names(), columns=['Incident_category_frequency'])","0c2e436d":"grams_df_incident_cate.sort_values(by = \"Incident_category_frequency\",ascending= False).head(10)","38fb5d1b":"#### Find N grams for category - Request\n\nword_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(incidents_category.get_group(\"Request\")[\"short_description\"])\nfrequencies_Request_cate = sum(sparse_matrix).toarray()[0]\ngrams_df_Request_cate = pd.DataFrame(frequencies_Request_cate, index=word_vectorizer.get_feature_names(), columns=['Request_category_frequency'])","9d488359":"grams_df_Request_cate.sort_values(by = \"Request_category_frequency\",ascending= False)","7a13d00e":"plt.style.use(\"ggplot\")\nplt.xlabel(\"Frequency\",)\nplt.ylabel(\"Bi Grams\")\ngrams_Request_cate = grams_df_Request_cate[\"Request_category_frequency\"].sort_values(ascending = False).head(20)\ngrams_Request_cate.sort_values(ascending = True).plot(kind=\"barh\",title = \"Request Category - Top 20 Frequent Number Of Words\")","7c331087":"plt.style.use(\"ggplot\")\nplt.xlabel(\"Frequency\",)\nplt.ylabel(\"Bi Grams\")\ngrams_df_incident_cate = grams_df_incident_cate[\"Incident_category_frequency\"].sort_values(ascending = False).head(20)\ngrams_df_incident_cate.sort_values(ascending = True).plot(kind=\"barh\",title = \"Incident Category - Top 20 Frequent Number Of Words\")\n","e4eca3a2":"train_incidents[\"short_desc_report_count\"]  = train_incidents[\"short_description_tokens\"].apply(lambda x: list(x).count(\"report\"))","a43e4e69":"incidents_category[\"short_desc_report_count\"].sum()","b38194cf":"train_incidents[\"short_desc_authorization_count\"] = train_incidents[\"short_description_tokens\"].apply(lambda x: list(x).count(\"authorization\"))\n","31478b04":"incidents_category[\"short_desc_authorization_count\"].sum()","755c8c43":"incidents_escalation[\"short_desc_authorization_count\"].sum()","c12ce0bb":"incidents_type[\"short_desc_authorization_count\"].sum()","774e2c6e":"train_incidents_sorted_opened_at_df  = train_incidents.sort_values(by = \"opened_at\")","2ae777a6":"train_incidents_sorted_opened_at_df.shape","6a576e2f":"\n#nltk.download('inaugural')\n#nltk.download('nps_chat')\n#nltk.download('webtext')\n#nltk.download('treebank')\n","435b48b6":"short_desc_tokens_series = train_incidents[\"short_description_tokens\"].apply(lambda x: list(x))\nshort_desc_tokens_series = short_desc_tokens_series.tolist()\nshort_desc_tokens_series\n\n#short_desc_tokens_list = list(itertools.chain.from_iterable(short_desc_tokens_series))   \n#short_desc_tokens_list","2e45e7a7":"\nshort_desc_tokens_series = train_incidents[\"short_description_tokens\"].apply(lambda x: list(x))\nshort_desc_tokens_series = short_desc_tokens_series.tolist()\nshort_desc_tokens_series\n\nshort_desc_tokens_list = list(itertools.chain.from_iterable(short_desc_tokens_series))   \nshort_desc_tokens_list\n\nplt.figure(figsize=(16,5))\n## Make the list as NLTK object\nshort_desc_tokens_list = nltk.Text(short_desc_tokens_list)\n\ntopics = ['authorization', 'access', 'reload',\"qlikview\",\"mismatch\",\"reconciliation\",\"ebi\",\"mapping\",\"report\"]\nshort_desc_tokens_list.dispersion_plot(topics)\n","06700e57":"###\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# datetime parser\nfilepath2 = \"..\/input\/it-incidents-tokens-vs-date-fields-data\/short_description_token_vs_open_at_dates.csv\"\n\nwith open(filepath2,'rb') as filep:\n    result2 = chardet.detect(filep.read(1000000))\n    result2\n\nresult2\n\nsd_token_timeseries = pd.read_csv(filepath2,encoding=\"Windows-1252\")\n\nsd_token_timeseries[\"Opened_at\"] = (pd.to_datetime(sd_token_timeseries[\"Opened_at\"],format = '%d\/%m\/%Y'))\n## Delete duplicates value of all the rows\nsd_token_timeseries = sd_token_timeseries.drop_duplicates()\n\nsd_token_timeseries.head\n\n","a2520a02":"### Filter the tokens data containing selected tokens to analyze with open dates\n#selected_tokens_mask = sd_token_timeseries[\"Short_desc_tokens\"].str.contains(\"access|reload\",regex = True)\n\n#sd_token_timeseries[\"Short_desc_selected_tokens\"] = np.where(selected_tokens_mask,sd_token_timeseries[\"Short_desc_tokens\"])\n\nsd_token_timeseries[\"Short_desc_selected_tokens\"] = sd_token_timeseries[\"Short_desc_tokens\"].str.extract(\"(\"+'authorization|reload|mismatch|reconciliation|access|qlikview|ebi|query|report|mapping'+\")\",expand = False)\n","82d9cb31":"sd_token_timeseries_updated = sd_token_timeseries.dropna()\nsd_token_timeseries_updated.drop_duplicates(subset=[\"Opened_at\",\"Short_desc_selected_tokens\"],keep=\"first\")","b9f5c585":"# set size of figure\nplt.figure(figsize=(16,10))\n\n# use horizontal stripplot with x marker size of 5\nsns.stripplot(y='Short_desc_selected_tokens',x='Opened_at', data=sd_token_timeseries_updated,\n orient='h', marker='^', color='navy', size=4)\n# rotate x tick labels\nplt.xticks(rotation=50,size= 15)\nplt.yticks(size= 15)\n# remover borders of plot\nplt.style.use(\"ggplot\")\nplt.tight_layout()\nplt.title(\"Tokens VS tickets Open Date - Time Series Analysis\",size= 30)\nplt.ylabel(\"Issues\",size = 20)\nplt.xlabel(\"Tickets Opened at\",size = 20)\nplt.show()\n","f1b4e50e":"# set size of figure\nplt.figure(figsize=(16,10))\n\n\n# use horizontal stripplot with x marker size of 5\nsns.stripplot(y='incident_subcategory',x='opened_at', data=train_incidents,\n orient='h', marker='X', color='navy', size=4)\n# rotate x tick labels\nplt.xticks(rotation=50)\n# remover borders of plot\nplt.style.use(\"ggplot\")\nplt.tight_layout()\nplt.title(\"Incident Sub-Category VS tickets Open Date - Time Series Analysis\")\nplt.ylabel(\"Sub category\")\nplt.xlabel(\"Tickets Opened at\")\nplt.show()\n","9ba48f16":"# set size of figure\n\nplt.figure(figsize=(16,10))\n\n# use horizontal stripplot with x marker size of 5\nsns.stripplot(y='priority',x='opened_at', data=train_incidents,\n orient='h', marker='X', color='navy', size=4)\n# rotate x tick labels\nplt.xticks(rotation=50)\n# remover borders of plot\nplt.style.use(\"ggplot\")\nplt.tight_layout()\nplt.title(\"Incident Priority VS tickets Open Date - Time Series Analysis\")\nplt.ylabel(\"Priority\")\nplt.xlabel(\"Tickets Opened at\")\nplt.show()\n","c5bba27a":"# set size of figure\n\nplt.figure(figsize=(16,10))\n\n# use horizontal stripplot with x marker size of 5\nsns.stripplot(y='close_code',x='opened_at', data=train_incidents,\n orient='h', marker='X', color='navy', size=4)\n# rotate x tick labels\nplt.xticks(rotation=50)\n# remover borders of plot\nplt.style.use(\"ggplot\")\nplt.tight_layout()\nplt.title(\"Ticket closure status VS tickets Open Date - Time Series Analysis\")\nplt.ylabel(\"Tickets Closure Status\")\nplt.xlabel(\"Tickets Opened at\")\nplt.show()\n","88204a50":"train_incidents[\"selected_bi_grams_text\"] = train_incidents[\"short_description\"].str.extract(\"(\"+'net sale|ebi report|mrh report|sale report|qlikview dashboard|sale broadcast|tp tool|daily sale|mrh query|demand review|access request|review dashboard|data issue|edwh ebi|edwh report|complaint valid|sale data|fr qlikview|3rd party|incorrect data'+\")\",expand = False)\ntrain_incidents[\"selected_bi_grams_text\"].head()\ntrain_incidents[\"selected_bi_grams_text\"] = train_incidents[\"selected_bi_grams_text\"].apply(lambda x: str(x))\n#train_incidents[\"selected_bi_grams_text\"].str.replace(np.nan,\"\",regex= True)\ntrain_incidents.selected_bi_grams_text.fillna(\"\",inplace=True)","17595194":"train_incidents[\"top_20_bi_grams_list\"] = \" \"\n","b1657867":" \nword_vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",stop_words=None,ngram_range=(2,2), analyzer='word')\nfor each in (train_incidents[\"selected_bi_grams_text\"].index):\n    if (train_incidents[\"selected_bi_grams_text\"][each] != 'nan'):\n        text_list = [train_incidents[\"selected_bi_grams_text\"][each]]\n        sparse_matrix = word_vectorizer.fit_transform(text_list)\n        df11 = pd.DataFrame(word_vectorizer.get_feature_names(), columns=['bi_grams'])\n        train_incidents[\"top_20_bi_grams_list\"][each] = list(df11.bi_grams)\n","45dff99d":"## Lets check the format of the data\ntrain_incidents[\"top_20_bi_grams_list\"].head(1)","dafaeb26":"bi_grams_date_df =  train_incidents[[\"opened_at_date\",\"top_20_bi_grams_list\"]]\n#bi_grams_date_df[\"top20_bi_grams_list\"].apply(lambda x: str(x))\nbi_grams_date_df[\"top_20_bi_grams_list\"] = bi_grams_date_df[\"top_20_bi_grams_list\"].apply(lambda x: \"\".join(x))\n#bi_grams_date_df[bi_grams_date_df.isnull]\nbi_grams_date_df = bi_grams_date_df[bi_grams_date_df[\"top_20_bi_grams_list\"] != \" \"]","152f8d2b":"\n# set size of figure\nplt.figure(figsize=(16,10))\n\n# use horizontal stripplot with x marker size of 5\nsns.stripplot(y='top_20_bi_grams_list',x='opened_at_date', data=bi_grams_date_df,\n orient='h', marker='^', color='navy', size=4)\n# rotate x tick labels\nplt.xticks(rotation=50,size= 15)\nplt.yticks(size= 15)\n# remover borders of plot\nplt.style.use(\"ggplot\")\nplt.tight_layout()\nplt.title(\"Bi Grams VS Tickets Open Date - Time Series Analysis\",size= 30)\nplt.ylabel(\"Bi Grams\",size = 20)\nplt.xlabel(\"Tickets Opened at\",size = 20)\nplt.show()\n\n### Its observed majority of the bigrams piled up during 2018 compare to 2017 and few terms like demand review,FR Qlikview are not seen in 2017 at all","cc12cabe":"train_incidents[\"selected_tri_grams_text\"] = train_incidents[\"short_description\"].str.extract(\"(\"+'net sale broadcast|demand review dashboard|global edwh ebi|hr master file|3rd party net|daily sale report|party net sale|manual file upload|ea field crop|file upload qlikview|crop manual file|apac ea field|field crop manual|qlikview demand review|global qlikview demand|qlik demand dashboard|demand dashboard refresh|incident apac ea|related qlik demand|wd activity related'+\")\",expand = False)\ntrain_incidents[\"selected_tri_grams_text\"].head()\ntrain_incidents[\"selected_tri_grams_text\"] = train_incidents[\"selected_tri_grams_text\"].apply(lambda x: str(x))\ntrain_incidents.selected_tri_grams_text.fillna(\"\",inplace=True)","d652a0e9":"train_incidents[\"top_20_tri_grams_list\"] = \" \"\n","36f8f85f":" \nword_vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",stop_words=None,ngram_range=(3,3), analyzer='word')\nfor each in (train_incidents[\"selected_tri_grams_text\"].index):\n    if (train_incidents[\"selected_tri_grams_text\"][each] != 'nan'):\n        text_list = [train_incidents[\"selected_tri_grams_text\"][each]]\n        sparse_matrix = word_vectorizer.fit_transform(text_list)\n        df12 = pd.DataFrame(word_vectorizer.get_feature_names(), columns=['tri_grams'])\n        train_incidents[\"top_20_tri_grams_list\"][each] = list(df12.tri_grams)\n","446ea007":"## Lets check the format of the data\ntrain_incidents[\"top_20_tri_grams_list\"].tail(5)","6f11b5a6":"tri_grams_date_df =  train_incidents[[\"opened_at_date\",\"top_20_tri_grams_list\"]]\ntri_grams_date_df[\"top_20_tri_grams_list\"] = tri_grams_date_df[\"top_20_tri_grams_list\"].apply(lambda x: \"\".join(x))\ntri_grams_date_df = tri_grams_date_df[tri_grams_date_df[\"top_20_tri_grams_list\"] != \" \"]","58f1087e":"\n# set size of figure\nplt.figure(figsize=(16,10))\n\n# use horizontal stripplot with x marker size of 5\nsns.stripplot(y='top_20_tri_grams_list',x='opened_at_date', data=tri_grams_date_df,\n orient='h', marker='^', color='navy', size=4)\n# rotate x tick labels\nplt.xticks(rotation=50,size= 15)\nplt.yticks(size= 15)\n# remover borders of plot\nplt.style.use(\"ggplot\")\nplt.tight_layout()\nplt.title(\"Tri Grams VS Tickets Open Date - Time Series Analysis\",size= 30)\nplt.ylabel(\"Tri Grams\",size = 20)\nplt.xlabel(\"Tickets Opened at\",size = 20)\nplt.show()\n\n","cdf61950":"train_incidents[\"bi_grams_contains_issue\"] = train_incidents[\"short_description\"].str.extract(\"(\"+'data issue|dashboard issue|mapping issue|query issue|access issue|mrh issue|file issue|ebi issue|report issue|mrh2 issue'+\")\",expand = False)\ntrain_incidents[\"bi_grams_contains_issue\"].head()\ntrain_incidents[\"bi_grams_contains_issue\"] = train_incidents[\"bi_grams_contains_issue\"].apply(lambda x: str(x))\ntrain_incidents.bi_grams_contains_issue.fillna(\"\",inplace=True)","8538212d":"train_incidents[\"top_10_bi_grams_issue_list\"] = \" \"\n","462e97c4":" \nword_vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",stop_words=None,ngram_range=(2,2), analyzer='word')\nfor each in (train_incidents[\"bi_grams_contains_issue\"].index):\n    if (train_incidents[\"bi_grams_contains_issue\"][each] != 'nan'):\n        text_list = [train_incidents[\"bi_grams_contains_issue\"][each]]\n        sparse_matrix = word_vectorizer.fit_transform(text_list)\n        df13 = pd.DataFrame(word_vectorizer.get_feature_names(), columns=['bi_grams_contains_issues'])\n        train_incidents[\"top_10_bi_grams_issue_list\"][each] = list(df13.bi_grams_contains_issues)\n","72771ca2":"## Lets check the format of the data\ntrain_incidents[\"top_10_bi_grams_issue_list\"][2274]","a66bf914":"bi_grams_issue_date_df =  train_incidents[[\"opened_at_date\",\"top_10_bi_grams_issue_list\"]]\nbi_grams_issue_date_df[\"top_10_bi_grams_issue_list\"] = bi_grams_issue_date_df[\"top_10_bi_grams_issue_list\"].apply(lambda x: \"\".join(x))\nbi_grams_issue_date_df = bi_grams_issue_date_df[bi_grams_issue_date_df[\"top_10_bi_grams_issue_list\"] != \" \"]\n","fb89815e":"\n# set size of figure\nplt.figure(figsize=(16,10))\n\n# use horizontal stripplot with x marker size of 5\nsns.stripplot(y='top_10_bi_grams_issue_list',x='opened_at_date', data=bi_grams_issue_date_df,\n orient='h', marker='^', color='navy', size=4)\n# rotate x tick labels\nplt.xticks(rotation=50,size= 15)\nplt.yticks(size= 15)\n# remover borders of plot\nplt.style.use(\"ggplot\")\nplt.tight_layout()\nplt.title(\"Bi Grams contains word 'Issue' VS Tickets Open Date - Time Series Analysis\",size= 30)\nplt.ylabel(\"Issue Bi Grams\",size = 20)\nplt.xlabel(\"Tickets Opened at\",size = 20)\nplt.show()\n\n### Its observed data issues count were low in 2018 compared to 2017.However,report and mrh2 issue are occured more in 2018","3cf64e4a":"train_incidents[\"sentiments\"] = train_incidents[\"short_description\"].apply(lambda x: TextBlob(x).sentiment[0])\n","fea3fc2e":"train_incidents[[\"short_description\",\"sentiments\",\"short_description_tokens\"]].sort_values(by = \"sentiments\",ascending = True)","b9c618b5":"incidents_impact[\"sentiments\"].sum()","828694dc":"incidents_impact[\"sentiments\"].sum().plot(kind= \"bar\")\nplt.title(\"Sentiment Polarity VS Incident Impact\")\nplt.xlabel(\"Impact\")\nplt.ylabel(\"Polarity\")\nplt.xticks(rotation = \"0.5\")\n","16767704":"incidents_category[\"sentiments\"].sum().sort_values(ascending = True)","45afd9a8":"incidents_incident_subcategory[\"sentiments\"].sum().sort_values(ascending = True)","032be895":"incidents_assignment_group[\"sentiments\"].sum()","f15e46ac":"incidents_assignment_group[\"sentiments\"].sum().plot(kind =\"bar\",color= [\"pink\",\"brown\"])\nplt.title(\"Application Wise Sentiment Polarity Analysis\")\nplt.xlabel(\"Application\")\nplt.ylabel(\"Polarity\")\nplt.xticks(rotation = \"0.5\")\n","e07084f7":"## Defining sentiment type based on polarity","45981575":"def sentiment_type(value):\n    if value >= 0.5:\n        return \"Positive\"\n    elif value <= -0.5:\n        return \"Negitive\"\n    else:\n        return \"Neutral\"","db5384dc":"train_incidents[\"sentiment_types\"] = train_incidents[\"sentiments\"].apply(sentiment_type)","bd7d0ca1":"train_incidents[\"sentiment_types\"].value_counts()","091a9035":"train_incidents[\"sentiment_types\"].value_counts().plot(kind = \"bar\",color = [\"blue\",\"red\",\"green\"])\nplt.title(\"Sentiment types classification frequency\")\nplt.xlabel(\"Sentiment Types\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation = \"0.5\")\n","62d6c289":"#### Sentiment types Vs incident category analysis ","c33af488":"incident_cate_senti_type_grpby = train_incidents.groupby([\"category\",\"sentiment_types\"])","cde52c52":"df_ramdom = incident_cate_senti_type_grpby[\"number\"].count().to_frame(name = \"count\")\n\ndf_ramdom.unstack(1)","1209aea7":"# Importing SentimentIntensityAnalyzer  --- Method 2 -- in which we can calculate sentiment polarity\n# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n# sia = SentimentIntensityAnalyzer()","3fa27786":"## Function to hold positive sentiments and its words\ndef sentiment_type_words_fun_positive(words):\n    mysentilist = []\n    for word in words.split(\" \"):\n        if (TextBlob(word).sentiment[0]) >= 0.5:\n            mysentilist.append(word)\n    return mysentilist    \n    ","521d0234":"## Function to hold negitive sentiments and its words\ndef sentiment_type_words_fun_negitive(words):\n    mysentilist = []\n    for word in words.split(\" \"):\n        if (TextBlob(word).sentiment[0]) <= -0.5:\n            mysentilist.append(word)\n    return mysentilist    \n    ","29a31683":"## Function to hold neutral sentiments and its words\ndef sentiment_type_words_fun_neutral(words):\n    mysentilist = []\n    for word in words.split(\" \"):\n        if ((TextBlob(word).sentiment[0]) > -0.5 and (TextBlob(word).sentiment[0]) < 0.5):\n            mysentilist.append(word)\n    return mysentilist    \n    ","c09d863a":"### List of all postive sentimental words found in text \"short description\"\ntrain_incidents[\"sentiment_types_postive_words\"] = train_incidents[\"short_description\"].apply(sentiment_type_words_fun_positive)\n","2cb248af":"postive_senti_df = pd.DataFrame(train_incidents[\"sentiment_types_postive_words\"].apply(lambda x: \"\".join(x)).value_counts())\npostive_senti_df[postive_senti_df[\"sentiment_types_postive_words\"] != 2271].plot(kind = \"bar\",color = \"green\")\nplt.title(\"Most observed Positive words\")\nplt.xlabel(\"Positive Words\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation = \"0.5\")\n","43e83fe9":"### List of all negitive sentimental words found in text \"short description\"\ntrain_incidents[\"sentiment_types_negitive_words\"] = train_incidents[\"short_description\"].apply(sentiment_type_words_fun_negitive)\n","95f7c8cd":"negitive_df = pd.DataFrame((train_incidents[\"sentiment_types_negitive_words\"].apply(lambda x: \"\".join(x))).value_counts())\nnegitive_df[negitive_df[\"sentiment_types_negitive_words\"] != 2199].head(5).plot(kind = \"bar\")\nplt.title(\"Most observed negitive words\")\nplt.xlabel(\"Negitive Words\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation = \"0.5\")\n","211eb022":"mask1 = (train_incidents[\"sentiment_types\"] == \"Negitive\")\n\nnegitive_sentiment_data_df = train_incidents[[\"short_description\",\"sentiments\",\"sentiment_types_negitive_words\"]][mask1].sort_values(by = \"sentiments\",ascending = True)\n","1096bdfa":"## Collecting top 5 negitive words into a list\ntop5_negitive_words_list = negitive_df.index[1:6]\ntop5_negitive_words_list = list(top5_negitive_words_list)\ntop5_negitive_words_list\n","0cecaef7":"negitive_sentiment_data_df.head()","95ddc93e":"word_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(negitive_sentiment_data_df[\"short_description\"])\nfrequencies = sum(sparse_matrix).toarray()[0]\nbi_grams_negitive_df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n","89a1bcfd":"#### Bi grams of Negitive Sentiment Words\nbi_grams_negitive_df = bi_grams_negitive_df[bi_grams_negitive_df.index.str.contains(\"wrong|unable|failed|bad|inconvenient\")].sort_values(by = \"frequency\",ascending= False).head(20)\nbi_grams_negitive_df","611f4803":"### Analyzing top 20 frequent Negitive BI Gram words\n\nplt.style.use(\"ggplot\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Negitive Bi Grams\")\ntop20_negitive_bigrams = bi_grams_negitive_df[\"frequency\"].sort_values(ascending = False).head(20)\n\ntop20_negitive_bigrams.head(20).sort_values(ascending = True).plot(kind=\"barh\",title = \"Top 20 Frequent Negitive Bi Grams\")","87d31d85":"####### Negitive Sentiment Trigrams","1f89ee1a":"word_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(negitive_sentiment_data_df[\"short_description\"])\nfrequencies = sum(sparse_matrix).toarray()[0]\ntri_grams_negitive_df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n\n\n#### tri grams of Negitive Sentiment Words\ntri_grams_negitive_df = tri_grams_negitive_df[tri_grams_negitive_df.index.str.contains(\"wrong|unable|failed|bad|inconvenient\")].sort_values(by = \"frequency\",ascending= False).head(20)\ntri_grams_negitive_df","d127f1cd":"### Analyzing top 20 frequent Negitive TRI Gram words\n\nplt.style.use(\"ggplot\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Negitive Tri Grams\")\ntop20_negitive_trigrams = tri_grams_negitive_df[\"frequency\"].sort_values(ascending = False).head(20)\n\ntop20_negitive_trigrams.head(20).sort_values(ascending = True).plot(kind=\"barh\",title = \"Top 20 Frequent Negitive Tri Grams\")\n","36bf8671":"## SInce Tri grams above seems like have common grams revolving aroung top negitive words, \n## so higher grams would make sense to check any patterns","8df91f10":"word_vectorizer = CountVectorizer(ngram_range=(4,4), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(negitive_sentiment_data_df[\"short_description\"])\nfrequencies = sum(sparse_matrix).toarray()[0]\nnegitive_sentences_df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n\n\n#### tri grams of Negitive Sentiment Words\nnegitive_sentences_df = negitive_sentences_df[negitive_sentences_df.index.str.contains(\"wrong|unable|failed|bad|inconvenient\")].sort_values(by = \"frequency\",ascending= False).head(20)\nnegitive_sentences_df","2d7eab9d":"### Analyzing top 20 frequent Negitive TRI Gram words\n\nplt.style.use(\"ggplot\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Negitive Tri Grams\")\ntop20_negitive_trigrams = tri_grams_negitive_df[\"frequency\"].sort_values(ascending = False).head(20)\n\ntop20_negitive_trigrams.head(20).sort_values(ascending = True).plot(kind=\"barh\",title = \"Top 20 Frequent Negitive Tri Grams\")\n","e8beaf50":"mask1 = (train_incidents[\"sentiment_types\"] == \"Positive\")\n\npositive_sentiment_data_df = train_incidents[[\"short_description\",\"sentiments\",\"sentiment_types_postive_words\"]][mask1].sort_values(by = \"sentiments\",ascending = True)\n","b620d777":"positive_sentiment_data_df.head()","df2cd0c8":"word_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(positive_sentiment_data_df[\"short_description\"])\nfrequencies = sum(sparse_matrix).toarray()[0]\nbi_grams_positive_df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n","21d624d5":"#### Bi grams of positive Sentiment Words\nbi_grams_positive_df = bi_grams_positive_df[bi_grams_positive_df.index.str.contains(\"able|latest|good|successful|many|best|sure\")].sort_values(by = \"frequency\",ascending= False).head(20)\nbi_grams_positive_df","8285d6eb":"### Analyzing top 20 frequent Negitive BI Gram words\n\nplt.style.use(\"ggplot\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Positive Bi Grams\")\ntop20_positive_bigrams = bi_grams_positive_df[\"frequency\"].sort_values(ascending = False).head(20)\n\ntop20_positive_bigrams.head(20).sort_values(ascending = True).plot(kind=\"barh\",title = \"Top 20 Frequent Positive Bi Grams\")","98184330":"####### Negitive Sentiment Trigrams","748663f8":"word_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(positive_sentiment_data_df[\"short_description\"])\nfrequencies = sum(sparse_matrix).toarray()[0]\ntri_grams_positive_df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n\n\n''#### tri grams of Positive Sentiment Words\ntri_grams_positive_df = tri_grams_positive_df[tri_grams_positive_df.index.str.contains(\"able|latest|good|successful|many|best|sure\")].sort_values(by = \"frequency\",ascending= False).head(20)\ntri_grams_positive_df","575ddcee":"### Analyzing top 20 frequent Positive TRI Gram words\n\nplt.style.use(\"ggplot\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Positive Tri Grams\")\ntop20_positive_trigrams = tri_grams_positive_df[\"frequency\"].sort_values(ascending = False).head(20)\n\ntop20_positive_trigrams.head(20).sort_values(ascending = True).plot(kind=\"barh\",title = \"Top 20 Frequent Positive Tri Grams\")\n","a8ba6d42":"# Lets analyze the correlation between top 20 bi\/tri grams and the sentiment data","25beafd2":"train_incidents[\"top_20_bi_grams_list\"] = train_incidents[\"top_20_bi_grams_list\"].apply(lambda x: \"\".join(x))\ntop20_bigrams_sentiment_sum = train_incidents[[\"top_20_bi_grams_list\",\"sentiments\"]]\ntop20_bigrams_sentiment_grpby = top20_bigrams_sentiment_sum.groupby(by = \"top_20_bi_grams_list\")\n","076551ef":"## Sentiments VS Bi grams\ntop20_bigrams_sentiment_series = (top20_bigrams_sentiment_grpby[\"sentiments\"].sum()).sort_values(na_position= \"last\")\ntop20_bigrams_sentiment_series","3701f9b1":"top20_bigrams_sentiment_df = pd.DataFrame(top20_bigrams_sentiment_series.values,top20_bigrams_sentiment_series.index)\ntop20_bigrams_sentiment_df = top20_bigrams_sentiment_df[top20_bigrams_sentiment_df.index != \" \"]\ntop20_bigrams_sentiment_df","7937c222":"\n#plt.title(\"Top 20 Bi Grams VS Sentiment Correlation\")\ntop20_bigrams_sentiment_df.sort_values(by = 0,ascending = True).plot(kind=\"bar\",title = \"Top 20 Bi Grams VS Sentiment Plot\")\nplt.style.use(\"ggplot\")\nplt.xlabel(\"Bi Grams\")\nplt.ylabel(\"Sentiment Polarity\")\nplt.show()","0ac1a13a":"train_incidents[\"top_20_tri_grams_list\"] = train_incidents[\"top_20_tri_grams_list\"].apply(lambda x: \"\".join(x))\ntop20_trigrams_sentiment_sum = train_incidents[[\"top_20_tri_grams_list\",\"sentiments\"]]\ntop20_trigrams_sentiment_sum_grpby = top20_trigrams_sentiment_sum.groupby(by = \"top_20_tri_grams_list\")\n","2e0c10a7":"## Sentiments VS Tri grams\ntop20_trigrams_sentiment_series = (top20_trigrams_sentiment_sum_grpby[\"sentiments\"].sum()).sort_values(na_position= \"last\")\ntop20_trigrams_sentiment_series\n","d4c8398a":"top20_trigrams_sentiment_df = pd.DataFrame(top20_trigrams_sentiment_series.values,top20_trigrams_sentiment_series.index)\ntop20_trigrams_sentiment_df = top20_trigrams_sentiment_df[top20_trigrams_sentiment_df.index != \" \"]\ntop20_trigrams_sentiment_df","4563fdfd":"#plt.title(\"Top 20 Bi Grams VS Sentiment Correlation\")\ntop20_trigrams_sentiment_df.sort_values(by = 0,ascending = True).plot(kind=\"bar\",title = \"Top 20 Tri Grams VS Sentiment Plot\")\nplt.style.use(\"ggplot\")\nplt.xlabel(\"Tri Grams\")\nplt.ylabel(\"Sentiment Polarity\")\nplt.show()","fce9ea14":"#incidents_category VS sentiments\n\nincidents_category[\"sentiments\"].sum().sort_values()\n","c3f5d3ce":"## incidents_incident_subcategory vs sentiments\nincidents_incident_subcategory[\"sentiments\"].sum().sort_values()","801835af":"##incidents_priority vs sentiments\nincidents_priority[\"sentiments\"].sum().sort_values()","0d5af119":"#incidents_urgency vs sentiments\nincidents_urgency[\"sentiments\"].sum().sort_values()","d3c885b3":"#incidents_made_sla vs sentiments\nincidents_made_sla[\"sentiments\"].sum().sort_values()","df0222aa":"#incidents_type vs sentiments\nincidents_type[\"sentiments\"].sum().sort_values()","a1cd2202":"#incidents_impact vs sentiments\nincidents_impact[\"sentiments\"].sum().sort_values()","91410357":"#incidents_escalation vs sentiments\nincidents_escalation[\"sentiments\"].sum().sort_values()","49f52886":"#incidents_e2e_resolution_met vs sentiments\nincidents_e2e_resolution_met[\"sentiments\"].sum().sort_values()","a9396665":"#incidents_location vs sentiments\nincidents_location[\"sentiments\"].sum().sort_values()","c8f8750c":"#incidents_country vs sentiments\ncountry_sentiments_series = incidents_country[\"sentiments\"].sum().sort_values()\ncountry_sentiments_series\n##create a dataframe of country series\ncountry_sentiments_df = pd.DataFrame(country_sentiments_series.values,country_sentiments_series.index)\ncountry_sentiments_df","988f9d67":"#plt.title(\"Top 20 Bi Grams VS Sentiment Correlation\")\ncountry_sentiments_df.sort_values(by = 0,ascending = True).plot(kind=\"bar\",title = \"Country VS Sentiment Plot\")\nplt.style.use(\"ggplot\")\nplt.xlabel(\"Countries\")\nplt.ylabel(\"Sentiment Polarity\")\nplt.show()","98c93ae4":"#incidents_contact_type vs sentiments\nincidents_contact_type[\"sentiments\"].sum().sort_values()","aacfaf5e":"#incidents_affected_user vs sentiments\nincidents_affected_user[\"sentiments\"].sum().sort_values()","44ed27ec":"#incidents_assignment_group vs sentiments\nincidents_assignment_group[\"sentiments\"].sum().sort_values()","bd8874d7":"close_code_grpby = train_incidents.groupby(\"close_code\")\nclose_code_grpby[\"sentiments\"].sum().sort_values()","d9cb2e3d":"resolved_by_grpby = train_incidents.groupby(\"resolved_by\")\nresolved_by_grpby[\"sentiments\"].sum().sort_values()","6172a6d3":"from scipy.stats import linregress\n\n### Lets check the correlation between infosys_e2e_resolution_duration and sentiments \nlinregress(train_incidents[\"infosys_e2e_resolution_duration\"],train_incidents[\"sentiments\"])\n# Correlation coefficient - 0.07(on positive side but very minimal correlation, hence can rule out the correlation\n# between these 2 variables) \n","6523d179":"\n### Lets check the correlation between infosys_e2e_response_duration and sentiments \nlinregress(train_incidents[\"infosys_e2e_response_duration\"],train_incidents[\"sentiments\"])","0da44ab3":"\n### Lets check the correlation between e2e_response_duration and sentiments \nlinregress(train_incidents[\"e2e_response_duration\"],train_incidents[\"sentiments\"])","8b7e371b":"\n### Lets check the correlation between e2e_resolution_duration and sentiments \nlinregress(train_incidents[\"e2e_resolution_duration\"],train_incidents[\"sentiments\"])\n","59e17d7b":"### Extract numeric cols without null data\ntrain_incidents.dtypes == \"int64\" \nint_float_cols = train_incidents.dtypes[(train_incidents.isna().sum() != 2301) & ((train_incidents.dtypes == \"int64\") | (train_incidents.dtypes == \"float64\"))].index\ntrain_incidents[int_float_cols].head(5)\n\n## Selecting only required numeric cols for correlation plot\n\nnumeric_cols_selected = train_incidents[[\"reopen_count\",\"reassignment_count\",\"infosys_e2e_response_duration\",\"infosys_e2e_resolution_duration\",\"followup_counter\",\"e2e_response_duration\",\"e2e_resolution_duration\",\"calendar_duration\",\"business_duration\",\"sentiments\"]]\n","46cef0b5":"pd.plotting.scatter_matrix(numeric_cols_selected,alpha = 0.8,figsize=(25, 25))\nplt.tight_layout()\n","eb341f2a":"train_incidents[\"issue_turnaround_time_hours\"] = (train_incidents[\"resolved_at\"] - train_incidents[\"opened_at\"]).astype('timedelta64[h]')\n## Impute missing values with mean \ntrain_incidents[\"issue_turnaround_time_hours\"][train_incidents[\"issue_turnaround_time_hours\"].isna() == True] = train_incidents[\"issue_turnaround_time_hours\"].mean(skipna= True)\n\ntrain_incidents[\"issue_turnaround_time_days\"] = (train_incidents[\"resolved_at\"].dt.date - train_incidents[\"opened_at\"].dt.date)\ntrain_incidents[\"issue_turnaround_time_days\"] = pd.to_numeric(train_incidents[\"issue_turnaround_time_days\"].dt.days)\n## Impute missing values with mean \ntrain_incidents[\"issue_turnaround_time_days\"][train_incidents[\"issue_turnaround_time_days\"].isna() == True] = train_incidents[\"issue_turnaround_time_days\"].mean(skipna= True)\n\ntrain_incidents[\"issue_turnaround_time_days\"].head(2)\n","e3be5305":"### Lets check the correlation between turenaround time and sentiments \nlinregress(train_incidents[\"issue_turnaround_time_days\"],train_incidents[\"sentiments\"])\n","093c75e8":"##  Incidents with high turnaround times have more negative sentiment polarity score. \n##  As we you can see in above plot, majority of turnaround times have sentiment polarity scores < -0.10.\n\ntrain_incidents.plot.scatter(\"issue_turnaround_time_days\",\"sentiments\")\nplt.show()","c4457f3b":"## Tri grams- time series analysis","6fea9fb4":"Previously, we just removed commonly occurring words in a general sense. We can also remove commonly occurring words from our text data First, let\u2019s check the 10 most frequently occurring words in our text data then take call to remove or retain.","482efe47":"#### Handling Date fields\n","81b1d77c":"### 2. Basic Pre-processing\n","9044273b":"\n\n###### Tickets Opened at VS Priority -- DIspersion Analysis","9cc3d31b":"###### 2.5 Common word  -- Currently we are not removing as the frequent words are useful to perform further analysis","51a8adde":"#                                         Incidents Text Analytics   ","e7176250":"###### 2.4  Lemmatization\n\n\nLemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. It makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore, we usually prefer using lemmatization over stemming.","1252294a":"###  Time Series Analysis on Bi grams and Tri Grams","c690bc27":"* ##### 3.1. N-Grams","0f99a91e":"## sentences involving top 20 negitive words","9f1a13b7":"Note: The basic intuition behind this is that generally, the negative sentiments contain a lesser amount of words than the positive ones.","99b842b3":"## Check the correlation plot between turnaround times and sentiments","f7fe4cb2":"###### 2.7 Stemming","650aaf3b":"### Finding Correlations between numeric variables","56242088":"#### Lets  check for bigrams \/ trigrams with the postive words and undiscovered patterns","61312d1d":"##### 1.3 Average word length","9b67227f":"## Lets check the patterns between sentiments created and other attributes","a0260eb8":"# Sentiment Analysis","088a195b":"spelling correction is a useful pre-processing step because this also will help us in reducing multiple copies of words.\nFor example, \u201cAnalytics\u201d and \u201canalytcs\u201d will be treated as different words even if they are used in the same sense.","39de3ce8":"###### 1.4 Number of stop words","822707a3":"\n\n### Check the overall turnaround times and check the sentiment correlation with the turnaround times","77d8e610":"##### What are the most frequent words combined with word \"Issue\"","76708660":"###### 2.1 Lower case","7facc751":"### \"Short description\" token VS \"Opened at\" time series analysis","e3ef8613":"##  3. Extract Featuring  - Advanced Text Analytics","4efbbff3":"The first pre-processing step which we will do is transform our tweets into lower case. This avoids having multiple copies of the same words. For example, while calculating the word count, \u2018Analytics\u2019 and \u2018analytics\u2019 will be taken as different words.","9de0a699":"##### 1.5 Number of numerics","695a449c":"\n###### 2.6 Tokenization","9c0943ed":"#### Finding out the patterns between word \"data issues\" with other attributes:","a8c9db67":"Applying Tokenization to short description text:","fc1682e6":"###### Bi Grams Data Visualization-- words containing issue","8ca37aaf":"###### 2.6 Spelling Correction\n","4af82920":"###### 2.2 Removing Punctuation","dd0b7857":"\n\n##### Method 1","5958230a":"#### Lets analyze the time series of all the incidents contains word 'issue'. So, we will use the extracted text contains word issues and see the time series pattern.","4001a369":"###### Converging similar words eg: Qlik to Qlikview","c6694ab9":"##### This is the plot of a word vs the offset of the word in the text corpus.\nThe y-axis represents the word. Each word has a strip representing entire text in terms of offset, and a mark on the strip indicates the occurrence of the word at that offset, a strip is an x-axis. \n\nSo if you observe the plot below the terms \"ebi\",\"reload\" occur more often at the 2nd half the incidents and words like \"authorization\" and some words have somewhat uniform distribution in the middle. \n","2eca8b0e":"###### 2.3 Removal of Stop Words","5957b023":"Tokenization refers to dividing the text into a sequence of words or sentences. In our example, we have used the textblob library to first transform INCIDENT texts into a blob and then converted them into a series of words.","c2a82cda":"\n\n###### Tickets Opened at VS incident_sub category -- DIspersion Analysis","c1a4c041":"#### Tri Grams Data Visualization","c68ee386":"#### Plotting bi grams of Categories \"Request\" vs \"Incidents\"","9611a7ef":"### Tri-Gram Frequency for Text - \"Short Description\"","59fa04ca":"least avg number of word len","47dcd265":"### 1. Basic Feature Extraction\n","d350caef":"5 texts with least number of charecters","c81a1f25":"### Bi-Gram Frequency for Text - \"Short Description\"","bb1de26d":"#### Occurances of Term \"Report\" in Incidents text variable \"short description\" Vs variable \"Category\" Analysis","3685d200":"##### 1.1 Number of words","4e2f21a0":"Stemming refers to the removal of suffices, like \u201cing\u201d, \u201cly\u201d, \u201cs\u201d, etc. by a simple rule-based approach. For this purpose, we will use PorterStemmer from the NLTK library.","b3e410c8":"#### Occurances of Term \"Authorization\" in Incidents text variable \"short description\" Vs variable \"Escalation\" Analysis","524ce385":"##### 1.2 Number of characters","6ee7cb1d":"###### Method 2","3cfc7eb4":"### Creating group by objects","d1c3f48f":"##### 1.6 Number of upper case words","023566ff":"#### Lets  check for bigrams \/ trigrams with the negative words and undiscovered patterns","716d6f03":"###### Tickets Opened at VS Close status -- DIspersion Analysis","eeaa84f2":"##### Extracting sentiment words and analyzing those words with other categorical variables","bea48fea":"Top 5 texts with more number of charecters","4318db9b":"Converting string date objects to timestamps- Required to extract dates and time","eb54c307":"Top 5 texts with least number of words","56fe0d26":"The next step is to remove punctuation, as it doesn\u2019t add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the training data.","45b0721c":"Top 5 texts with more number of words","a41ce7dd":"##### [TextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.]","aa1d251c":"### Correlation matrix","50d1165e":"## Lexical dispersion plot","e3d64e22":"#### Occurances of Term \"Authorization\" in Incidents text variable \"short description\" Vs variable \"Category\" Analysis","aec7816d":"#### Bi Grams Data Visualization"}}