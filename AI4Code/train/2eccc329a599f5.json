{"cell_type":{"25e0c9f3":"code","6831c774":"code","f82a4ae1":"code","b71e95d9":"code","c551ce09":"code","bdf72d84":"code","decd896e":"code","d97e78d1":"code","783fc2aa":"code","da1fac23":"code","023af0bc":"code","5c66bf2c":"code","4b35e454":"code","2144bf5b":"code","6b4ac961":"code","51232232":"code","dd5335a4":"code","6a434868":"code","0d975916":"code","acc1c728":"code","d10a0b42":"code","ba984c48":"code","c0003ba8":"code","0795ef3c":"code","7a0910a9":"code","19d1d78f":"code","68f69aaf":"code","1d42c21f":"code","64011e75":"code","d4effc0b":"code","cd3a6b35":"code","50b640ab":"code","a2a2b75f":"code","0736df38":"code","f1a2637c":"code","3ea4433a":"code","b3d1c7f9":"code","a40ddb54":"code","a6abd9df":"code","53212f3b":"code","217b3303":"code","0606b081":"code","42a58515":"code","89003167":"code","4b5f1fae":"code","88e5899f":"code","defe69ce":"code","ff659109":"code","497aedfd":"code","38918e6d":"code","12bc979a":"code","e4f651ec":"code","4e897b6f":"markdown","83acc550":"markdown","e520db4b":"markdown","ee0efda1":"markdown","f8b5edfb":"markdown","744f1869":"markdown","2bf20c3f":"markdown","0f5a970f":"markdown","0070d15a":"markdown","10be1784":"markdown","e16e6148":"markdown","ce2fee11":"markdown","727885de":"markdown","faa972c7":"markdown","43074592":"markdown","ae9ef0a8":"markdown","83d34241":"markdown","c2eaae70":"markdown","1bea1545":"markdown","198a2539":"markdown","17264f9f":"markdown","083608a5":"markdown","0779e73d":"markdown","88b439a9":"markdown","1b782939":"markdown","1dff1989":"markdown","1addd937":"markdown","c1515043":"markdown","ea940f78":"markdown","f2fbab00":"markdown","d6d69ce5":"markdown","27d337ef":"markdown","3b1f049d":"markdown","12297f79":"markdown"},"source":{"25e0c9f3":"import warnings\nwarnings.filterwarnings('ignore')","6831c774":"import os\nimport pandas as pd\nimport numpy as np\nnp.random.seed(0)\n\n# filename = os.listdir(\"input\")[0]\npath = os.path.join(\"..\/input\", \"sentiment140\/training.1600000.processed.noemoticon.csv\")\ndf = pd.read_csv(path, encoding='ISO-8859-1', names = ['target','ids','date','flag','user','text'])","f82a4ae1":"df.head()","b71e95d9":"df = df[['target', 'text']]\ndf.head()","c551ce09":"df.isnull().sum()","bdf72d84":"df.target.unique()","decd896e":"import seaborn as sns\nsns.countplot(df.target)","d97e78d1":"df['target'] = df['target'].replace(4,1)","783fc2aa":"# splitting categories\ndf_positive = df[df['target'] == 1]\ndf_negative = df[df['target'] == 0]\n# sampling\ndf_positive = df_positive.iloc[:16000]\ndf_negative = df_negative.iloc[:16000]\n# concatenation\ndf = pd.concat([df_positive, df_negative])\nlen(df)","da1fac23":"import re, string\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nstop_words = set(stopwords.words('english'))\n\ndef clean_text(text):\n    # miniscule\n    text = text.lower()\n    \n    # removing usernames\n    text = re.sub('@[^\\s]+', '', text)\n\n    # removing links\n    text = re.sub('((https?:\/\/[^\\s]+)|(www\\.[^\\s]+))','',text)\n    \n    # removing punctuation\n    text = text.translate(str.maketrans('','',string.punctuation))\n    \n    # tokenization\n    tokens = word_tokenize(text)\n   \n    # suppression of stop words and lemmatization\n    wordLemm = WordNetLemmatizer()\n    final_words = []\n    for token in tokens:\n      # suppression of stop words\n      if token not in stop_words and len(token) > 1:\n        lemm = wordLemm.lemmatize(token)\n        final_words.append(lemm)\n    return ' '.join(final_words)","023af0bc":"df['cleaned_text'] = df['text'].apply(lambda x: clean_text(x))\ndf.head()","5c66bf2c":"df.isnull().sum()","4b35e454":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud","2144bf5b":"plt.figure(figsize = (15,15)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.target == 1].cleaned_text))\nplt.imshow(wc , interpolation = 'bilinear')\nplt.title('Tweets positifs')","6b4ac961":"plt.figure(figsize = (15,15)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.target == 0].cleaned_text))\nplt.imshow(wc , interpolation = 'bilinear')\nplt.title('Tweets n\u00e9gatifs')","51232232":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(df['cleaned_text'].values, df['target'].values, test_size=0.2, random_state=1)\nprint(\"x_train : \", x_train.shape)\nprint(\"y_train : \", y_train.shape)\nprint(\"x_test : \", x_test.shape)\nprint(\"y_test : \", y_test.shape)","dd5335a4":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer=CountVectorizer()\n\ncountVec_train=count_vectorizer.fit_transform(x_train) \ncountVec_test=count_vectorizer.transform(x_test)\nprint(\"count_train : \", countVec_train.shape)\nprint(\"count_test : \", countVec_test.shape)","6a434868":"from sklearn.ensemble import RandomForestClassifier\nnp.random.seed(0)\n\nrf_clf_countVec = RandomForestClassifier()\nrf_clf_countVec = rf_clf_countVec.fit(countVec_train, y_train)","0d975916":"from sklearn.metrics import accuracy_score\ny_pred = rf_clf_countVec.predict(countVec_test)\n\nscore=accuracy_score(y_test,y_pred)\n\nprint(f'Accuracy: {round(score*100,2)}%')\n# Confusion matrix\npd.crosstab(y_test,y_pred, rownames=['True'], colnames=['Predicted'])","acc1c728":"from sklearn.linear_model import LogisticRegression\nnp.random.seed(0)\n\nlr_clf_countVec = LogisticRegression(max_iter=100, solver='liblinear')\nlr_clf_countVec = lr_clf_countVec.fit(countVec_train, y_train)","d10a0b42":"from sklearn.metrics import accuracy_score\ny_pred = lr_clf_countVec.predict(countVec_test)\n\nscore=accuracy_score(y_test,y_pred)\n\nprint(f'Accuracy: {round(score*100,2)}%')\n# Confusion matrix\npd.crosstab(y_test,y_pred, rownames=['True'], colnames=['Predicted'])","ba984c48":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer=TfidfVectorizer()\n\ntfidf_train=tfidf_vectorizer.fit_transform(x_train) \ntfidf_test=tfidf_vectorizer.transform(x_test)\nprint(\"tfidf_train : \", tfidf_train.shape)                                                                                                                                                                                                                                                    \nprint(\"tfidf_test : \", tfidf_test.shape)","c0003ba8":"from sklearn.ensemble import RandomForestClassifier\nnp.random.seed(0)\n\nrf_clf_tfidf = RandomForestClassifier()\nrf_clf_tfidf = rf_clf_tfidf.fit(tfidf_train, y_train)","0795ef3c":"from sklearn.metrics import accuracy_score\ny_pred = rf_clf_tfidf.predict(tfidf_test)\n\nscore=accuracy_score(y_test,y_pred)\n\nprint(f'Accuracy: {round(score*100,2)}%')\n# Confusion matrix\npd.crosstab(y_test,y_pred, rownames=['True'], colnames=['Predicted'])","7a0910a9":"from sklearn.linear_model import LogisticRegression\nnp.random.seed(0)\n\nlr_clf_tfidf = LogisticRegression(max_iter=100, solver='liblinear')\nlr_clf_tfidf = lr_clf_tfidf.fit(tfidf_train, y_train)","19d1d78f":"from sklearn.metrics import accuracy_score\ny_pred = lr_clf_tfidf.predict(tfidf_test)\n\nscore=accuracy_score(y_test,y_pred)\n\nprint(f'Accuracy: {round(score*100,2)}%')\n# Confusion matrix\npd.crosstab(y_test,y_pred, rownames=['True'], colnames=['Predicted'])","68f69aaf":"! pip install --upgrade gensim\nimport gensim\nprint(gensim.__version__)","1d42c21f":"from gensim.models import Word2Vec\n\nclass MyCorpus:\n    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n\n    def __iter__(self):\n        \n        for sentence in df['cleaned_text'].values:\n            yield sentence.split()\n\nsentences = MyCorpus()\n# I eliminate tokens whose frequency is less than 10\ntrained_w2v = Word2Vec(sentences=sentences, min_count=10, vector_size=100)\nprint(trained_w2v)","64011e75":"from sklearn.decomposition import IncrementalPCA\nfrom sklearn.manifold import TSNE\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nnp.random.seed(0)\n\ndef visualize_embeddings(wv):\n    num_dimensions = 2\n\n    vectors = np.asarray(wv.vectors)\n    labels = np.asarray(wv.index_to_key)\n\n    tsne = TSNE(n_components=num_dimensions, random_state=0)\n    vectors = tsne.fit_transform(vectors)\n\n    x_vals = [v[0] for v in vectors]\n    y_vals = [v[1] for v in vectors]\n    \n    random.seed(0)\n\n    plt.figure(figsize=(12, 12))\n    plt.scatter(x_vals, y_vals)\n\n    indices = list(range(len(labels)))\n    selected_indices = random.sample(indices, 25)\n    for i in selected_indices:\n        plt.annotate(labels[i], (x_vals[i], y_vals[i]))","d4effc0b":"visualize_embeddings(trained_w2v.wv)","cd3a6b35":"def w2v_of_sentences(sentences, wv):\n    vectors = np.zeros((len(sentences), wv.vector_size))\n    for i, sentence in enumerate(sentences):\n        sum_vector = np.zeros((1, wv.vector_size))\n        tokens = sentence.split()\n        for token in tokens:\n            try:\n              sum_vector += wv[token]\n            except:\n              pass\n        if len(tokens) > 0:\n          vectors[i] = sum_vector \/ len(tokens)\n        else:\n          vectors[i] = sum_vector\n    return vectors","50b640ab":"w2v_train=w2v_of_sentences(x_train, trained_w2v.wv)\nw2v_test=w2v_of_sentences(x_test, trained_w2v.wv)\nprint(\"w2v_train : \", w2v_train.shape)\nprint(\"w2v_test : \", w2v_test.shape)","a2a2b75f":"from sklearn.ensemble import RandomForestClassifier\nnp.random.seed(0)\nrf_clf_trained_w2v = RandomForestClassifier()\nrf_clf_trained_w2v = rf_clf_trained_w2v.fit(w2v_train, y_train)","0736df38":"from sklearn.metrics import accuracy_score\ny_pred = rf_clf_trained_w2v.predict(w2v_test)\n\nscore = accuracy_score(y_test,y_pred)\n\nprint(f'Accuracy: {round(score*100,2)}%')\n# Confusion matrix\npd.crosstab(y_test,y_pred, rownames=['True'], colnames=['Predicted'])","f1a2637c":"from sklearn.linear_model import LogisticRegression\nnp.random.seed(0)\nlr_clf_trained_w2v = LogisticRegression(max_iter=100, solver='liblinear')\nlr_clf_trained_w2v = lr_clf_trained_w2v.fit(w2v_train, y_train)","3ea4433a":"from sklearn.metrics import accuracy_score\ny_pred = lr_clf_trained_w2v.predict(w2v_test)\n\nscore = accuracy_score(y_test,y_pred)\n\nprint(f'Accuracy: {round(score*100,2)}%')\n# Confusion matrix\npd.crosstab(y_test,y_pred, rownames=['True'], colnames=['Predicted'])","b3d1c7f9":"!wget http:\/\/nlp.stanford.edu\/data\/glove.twitter.27B.zip\n!unzip glove.twitter.27B.zip","a40ddb54":"pretrained_glove_w2v = gensim.models.KeyedVectors.load_word2vec_format('glove.twitter.27B.50d.txt', binary=False, no_header=True)","a6abd9df":"# vectorization of tweets\npretrained_glove_w2v_train=w2v_of_sentences(x_train, pretrained_glove_w2v)\npretrained_glove_w2v_test=w2v_of_sentences(x_test, pretrained_glove_w2v)\nprint(\"w2v_train : \", pretrained_glove_w2v_train.shape)\nprint(\"w2v_test : \", pretrained_glove_w2v_test.shape)","53212f3b":"from sklearn.ensemble import RandomForestClassifier\nnp.random.seed(0)\nrf_clf_pretrained_glove_w2v = RandomForestClassifier()\nrf_clf_pretrained_glove_w2v = rf_clf_pretrained_glove_w2v.fit(pretrained_glove_w2v_train, y_train)","217b3303":"from sklearn.metrics import accuracy_score\ny_pred = rf_clf_pretrained_glove_w2v.predict(pretrained_glove_w2v_test)\n\nscore = accuracy_score(y_test,y_pred)\n\nprint(f'Accuracy: {round(score*100,2)}%')\n# Confusion matrix\npd.crosstab(y_test,y_pred, rownames=['True'], colnames=['Predicted'])","0606b081":"from sklearn.linear_model import LogisticRegression\nnp.random.seed(0)\nlr_clf_pretrained_glove_w2v = LogisticRegression(max_iter=100, solver='liblinear')\nlr_clf_pretrained_glove_w2v = lr_clf_pretrained_glove_w2v.fit(pretrained_glove_w2v_train, y_train)","42a58515":"from sklearn.metrics import accuracy_score\ny_pred = lr_clf_pretrained_glove_w2v.predict(pretrained_glove_w2v_test)\n\nscore = accuracy_score(y_test,y_pred)\n\nprint(f'Accuracy: {round(score*100,2)}%')\n# Confusion matrix\npd.crosstab(y_test,y_pred, rownames=['True'], colnames=['Predicted'])","89003167":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Activation, Embedding, GRU\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.initializers import glorot_uniform","4b5f1fae":"list_len = [len(tweet.split()) for tweet in df.cleaned_text]\nmaxLen = max(list_len)\nprint('maxLen:', maxLen)","88e5899f":"def sentences_to_indices(X, w2v, max_len):\n    \n    m = X.shape[0]  # number of tweets\n    \n    X_indices = np.zeros((m, max_len))\n    \n    for i in range(m):\n        \n        sentence_words = X[i].split()\n\n        for j, w in enumerate(sentence_words):\n          try:\n            X_indices[i, j] =  w2v.get_index(w)\n          except:\n            try:\n              X_indices[i, j] =  w2v.get_index('unk')\n            except:\n              pass\n    \n    return X_indices","defe69ce":"# Transform the tweets tokens to indices\nx_train_indices = sentences_to_indices(x_train, pretrained_glove_w2v, maxLen)\nx_test_indices = sentences_to_indices(x_test, pretrained_glove_w2v, max_len = maxLen)","ff659109":"def gensim_to_keras_embedding(gensim_w2v, train_embeddings=False):\n    \"\"\"Get a Keras 'Embedding' layer with weights set from Word2Vec model's learned word embeddings.\n\n    Parameters\n    ----------\n    train_embeddings : bool\n        If False, the returned weights are frozen and stopped from being updated.\n        If True, the weights can \/ will be further updated in Keras.\n\n    Returns\n    -------\n    `keras.layers.Embedding`\n        Embedding layer, to be used as input to deeper network layers.\n\n    \"\"\"\n    keyed_vectors = gensim_w2v  # structure holding the result of training\n    weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array    \n    index_to_key = keyed_vectors.index_to_key  # which row in `weights` corresponds to which word?\n\n    layer = Embedding(\n        input_dim=weights.shape[0],\n        output_dim=weights.shape[1],\n        weights=[weights],\n        trainable=train_embeddings,\n    )\n    return layer","497aedfd":"def My_RNN(input_shape, wv):\n\n    sentence_indices = Input(shape=input_shape, dtype=np.int32)\n    \n    embedding_layer =  gensim_to_keras_embedding(wv, train_embeddings = False)  # embeddings need to be fixed as I don't want to train them\n    \n    embeddings = embedding_layer(sentence_indices)   \n    \n    X = LSTM(128, return_sequences=True)(embeddings)\n    # X = Dropout(0.5)(X)\n    X = LSTM(128)(X)\n    # X = Dropout(0.5)(X)\n    X = Dense(1, activation='sigmoid')(X)\n    \n    # Create Model instance which converts sentence_indices into X.\n    model = tf.keras.models.Model(sentence_indices, X)\n    \n    return model\n\nmodel = My_RNN((maxLen,), pretrained_glove_w2v)\nmodel.summary()","38918e6d":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","12bc979a":"history = model.fit(\n    x = x_train_indices,\n    y = y_train,\n    epochs = 10,\n    batch_size = 64,\n    shuffle=True,\n    verbose=2)\n\n    #validation_data = (x_test_indices, y_test),","e4f651ec":"loss, acc = model.evaluate(x_test_indices, y_test)\n\nprint(f'Test Accuracy: {round(acc*100,2)}%')","4e897b6f":"### 4.2 Classification using Logistic Regression","83acc550":"### 3.2 Classification using Logistic Regression","e520db4b":"**Conclusion :** The best score on the test set that I got is 74%. All the approaches I have tested have almost the same performance. \n\nMuch more thorough cleaning should improve the results.","ee0efda1":"There is no **neutral tweet**.","f8b5edfb":"\n\n## 6. Vectorization with pre-trained word embeddings (GloVe)\n\nThere are models already trained on tweets which are available through this link https:\/\/nlp.stanford.edu\/data\/glove.twitter.27B.zip. These models have been trained on a vocabulary of 1.2 million tokens.\n\nAfter having decompressed the archive, I will test the model whose vectors have a size of 50.\n\nI load the model with Gensim's **load_word2vec_format** function.","744f1869":"So I went from **1.6 million** tweets to **32,000** tweets.","2bf20c3f":"### 2.1 Cleaning\nRemoving usernames and links.\n\nRemoving punctuation\n\nRemoving stop words","0f5a970f":"### 6.2 Classification using Random Forest","0070d15a":"### 5.3 Sentences embeddings\nThe technique for using a Word2Vec on a sentence is to **average the vectors** of the words contained in the sentence.","10be1784":"I check if any values are missing.","e16e6148":"### 6.3 Classification using Logistic Regression","ce2fee11":"## 1. The dataset\nThis is the sentiment140 dataset.\nIt contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 2 = neutral, 4 = positive) and they can be used to detect sentiment .\nIt contains the following 6 fields:\n\ntarget: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n\nids: The id of the tweet ( 2087)\n\ndate: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n\nflag: The query (lyx). If there is no query, then this value is NO_QUERY.\n\nuser: the user that tweeted (robotickilldozr)\n\ntext: the text of the tweet (Lyx is cool)","727885de":"### 5.2 Viewing Word Embeddings","faa972c7":"# Twitter Sentiment Analysis - Differents vectorization techniques, Random Forest, RNN","43074592":"### 2.2 Wordcloud by class (positive \/ negative)","ae9ef0a8":"### 5.4 Classification using Random Forest","83d34241":"### 3.1 Classification with Random Forest","c2eaae70":"## 4. Vectorization with Tfidf (Term Frequency - Inverse Document) and classification","1bea1545":"Since the database is large, I'm going to divide the size of each category of tweets by **50**.","198a2539":"I will only keep the 2 columns that interest us.","17264f9f":"### 2.3 Splitting dataset in train and test","083608a5":"### 5.5 Classification using Logistic Regression","0779e73d":"## 3. Vectorization with CountVectorizer and classification","88b439a9":"J'ai travaill\u00e9 avec la version **4.1.2** de Gensim.","1b782939":"\nBelow I found a function to match the model loaded with Gensim to an Embedding layer from Keras. I found that here https:\/\/github.com\/RaRe-Technologies\/gensim\/wiki\/Using-Gensim-Embeddings-with-Keras-and-Tensorflow .","1dff1989":"### 4.1 Classification with RandomForest","1addd937":"## 7. Classification using RNN\nIn this part, I will use the **pretrained word embeddings** of the previous part.","c1515043":"There are 800,000 positive tweets and 800,000 negative tweets.\n\nI will replace the **'4'** of **'positive'** with **'1'** for clarity.","ea940f78":"### 6.1 Loading word embeddings with Gensim ","f2fbab00":"**The RNN architecture**","d6d69ce5":"### 5.1 Training of Word2Vec on this corpus","27d337ef":"To suit the Keras logic, I must convert sentences to lists of indices.\n\nMoreover, the input data must have the same size, so I need to find the largest tweet size (after cleaning) and adjust the others according to that.","3b1f049d":"## 5. Vectorization with Word2Vec trained with Gensim on this corpus\n\n\nI named the model **trained_w2v**.","12297f79":"## 2. Data cleaning and processing"}}