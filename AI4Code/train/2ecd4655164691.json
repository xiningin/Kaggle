{"cell_type":{"f18e7eef":"code","87b7a481":"code","f7fd4886":"code","b10f7bcc":"code","f67d2250":"code","765c99ed":"code","568067cf":"code","fbd32df9":"code","aa95beac":"code","3f17c812":"code","ce3cf017":"code","c81a373e":"code","08a9186e":"code","465154af":"code","d388d1f8":"code","1ab9f29c":"code","ee39b190":"code","c509801e":"code","0a4dcc22":"code","e34d40d4":"code","c75629dd":"code","6a8ab69d":"code","aeb39d10":"markdown","2eca10ea":"markdown","44c79b05":"markdown","bf17ad7e":"markdown","fdaa194f":"markdown","3f143ead":"markdown","717666f4":"markdown","10c9b728":"markdown","37dd962d":"markdown","0228aeed":"markdown","216605f7":"markdown","aa6bf3eb":"markdown","50c7e140":"markdown","8ff673aa":"markdown"},"source":{"f18e7eef":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.manifold import TSNE\nimport matplotlib.colors as mcolors\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import Phrases, phrases, ldamodel, CoherenceModel\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport spacy\nimport gensim.corpora as corpora\nfrom pprint import pprint\nimport pyLDAvis\nimport pyLDAvis.gensim \nfrom collections import Counter","87b7a481":"data_df = pd.read_csv('\/kaggle\/input\/spooky-author-identification\/train.zip')\ndata_df.head()","f7fd4886":"# shape of the dataset\ndata_df.shape","b10f7bcc":"# value count for eavh author\nsns.countplot(data_df['author'])","f67d2250":"# null value\ndata_df.isnull().sum()","765c99ed":"# drop id column\ndata_df = data_df.drop(columns = ['id'], axis=1)\ndata_df.head()","568067cf":"data_df['Number_of_words'] = data_df['text'].apply(lambda x:len(str(x).split()))\ndata_df","fbd32df9":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\nplt.figure(figsize=(12,6))\nsns.distplot(data_df['Number_of_words'],kde = False,color=\"red\",bins=200)\nplt.title(\"Frequency distribution of number of words for each text extracted\", size=20)","aa95beac":"cloud=WordCloud(colormap=\"winter\",width=600,height=400).generate(str(data_df[\"text\"]))\nfig=plt.figure(figsize=(13,18))\nplt.axis(\"off\")\nplt.imshow(cloud,interpolation='bilinear')","3f17c812":"data_df['text_processed'] = data_df['text'].map(lambda x: re.sub('[,\\.!?]','',x))\ndata_df['text_processed'] = data_df['text_processed'].map(lambda x:x.lower())\nprint(data_df['text_processed'].head())","ce3cf017":"# remove all characters, number or characters\ndef cleanText(input_string):\n    modified_string = re.sub('[^A-Za-z0-9]+', ' ', input_string)\n    return(modified_string)\ndata_df['text_processed'] = data_df.text_processed.apply(cleanText)\ndata_df['text_processed'][150]","c81a373e":"# remove stopwords\nstopWords = stopwords.words('english')\ndef removeStopWords(stopWords, rvw_txt):\n    newtxt = ' '.join([word for word in rvw_txt.split() if word not in stopWords])\n    return newtxt\ndata_df['text_processed'] = [removeStopWords(stopWords,x) for x in data_df['text_processed']]","08a9186e":"# join the different text together\nlongText = ','.join(list(data_df['text_processed'].values))\n# generate the word cloud\nwordcloud = WordCloud(background_color=\"black\",\n                      max_words= 600,\n                      contour_width = 10,\n                      contour_color = \"steelblue\",\n                     collocations=False).generate(longText)\n# visualize the word cloud\nfig = plt.figure(1, figsize = (12, 12))\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()\n","465154af":"fig = plt.figure(1, figsize = (20,10))\n# split() returns list of all the words in the string\nsplit_it = longText.split()\n# Pass the split_it list to instance of Counter class.\nCounter = Counter(split_it)\n#print(Counter)\n# most_common() produces k frequently encountered\n# input values and their respective counts.\nmost_occur = Counter.most_common(30)\nx_df = pd.DataFrame(most_occur, columns=(\"words\",\"count\"))\nsns.barplot(x = 'words', y = 'count', data = x_df)","d388d1f8":"nltk.download(\"punkt\")\n# word_tokenize \ndata_df[\"tokenized\"] = data_df[\"text_processed\"].apply(lambda x: nltk.word_tokenize(x))\ndata_df[\"tokenized\"] = data_df[\"tokenized\"].apply(lambda words: [word for word in words if word.isalnum()])\ndata_df\n","1ab9f29c":"from nltk.stem import WordNetLemmatizer \nnltk.download('wordnet')\ndef word_lemmatizer(text):\n  lem_text = [WordNetLemmatizer().lemmatize(i,pos='v') for i in text]\n  return lem_text\ndata_df[\"lemmatized\"] = data_df[\"tokenized\"].apply(lambda x: word_lemmatizer(x))\ndata_df[\"lemmatize_joined\"] = data_df[\"lemmatized\"].apply(lambda x: ' '.join(x))\npd.set_option('display.max_colwidth', 100)\ndata_df.head()","ee39b190":"plt.style.use('ggplot')\nplt.figure(figsize=(14,6))\nfreq=pd.Series(\" \".join(data_df[\"lemmatize_joined\"]).split()).value_counts()[:30]\nfreq.plot(kind=\"bar\", color = \"orangered\")\nplt.title(\"30 most frequent words\",size=20)","c509801e":"tokens = data_df[\"lemmatize_joined\"].apply(lambda x: nltk.word_tokenize(x))\ntokens","0a4dcc22":"import gensim \nfrom gensim.models import Word2Vec \nw2v_model = Word2Vec(tokens,\n                     min_count=20,\n                     window=10,\n                     size=250,\n                     alpha=0.03, \n                     min_alpha=0.0007,\n                     workers = 4,\n                     seed = 42)\n","e34d40d4":"dictionary = corpora.Dictionary(data_df[\"lemmatized\"])\ndoc_term_matrix = [dictionary.doc2bow(rev) for rev in data_df[\"lemmatized\"]]\n\nLDA = gensim.models.ldamodel.LdaModel\n\n# Build LDA model\nlda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=3, random_state=100,\n                chunksize=200, passes=100)\nlda_model.print_topics()","c75629dd":"# COHERENCE SCORE\ncoherence_model_lda = CoherenceModel(model=lda_model,\ntexts= data_df[\"lemmatized\"], dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)\n\n# # Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(doc_term_matrix))  ","6a8ab69d":"pyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)\npyLDAvis.display(vis)","aeb39d10":"**Vectorization using Word2Vec**","2eca10ea":"## Preparing data for Topic Modelling","44c79b05":"Now it's time to clean the text","bf17ad7e":"## Import Libraries","fdaa194f":"Now let's see 30 most frequent words","3f143ead":"## Visulization","717666f4":"## Model","10c9b728":"## Data Cleaning ","37dd962d":"We don't required id column so we will going to drop it","0228aeed":"Clean the text by lowering all words,removing special characters, numbers and stopwords","216605f7":"## WordCloud","aa6bf3eb":"The input will be in the form of document-term matrix, and we will convert that using the below piece of code.","50c7e140":"First of all we will do tokenization then will do lemmatization","8ff673aa":"### What is Topic Modeling\nIn machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is."}}