{"cell_type":{"9038c03d":"code","3358bc49":"code","9a208eff":"code","95d45ec1":"code","d211b9ad":"code","9dcbad02":"code","fd0c66af":"code","6f64fae6":"code","4df5e395":"code","9a226abb":"code","816d6632":"code","8a6f0c4a":"code","7060ac56":"code","285d71ff":"code","cdaa3cc8":"code","edef5209":"code","ecbd83b1":"code","3240410f":"code","9bf0e044":"code","1e91ab3d":"code","eafe0965":"code","4a9297d7":"code","7df65255":"code","37bbbb53":"code","914d6847":"code","8c832fac":"code","218cd7ac":"code","9a30ebda":"code","7afdbf61":"code","aa5f3678":"code","54d47cfb":"code","5bfe0b53":"code","eb548537":"code","20417d1f":"code","a96f5d5f":"markdown","03b6a9b1":"markdown","9c26cb80":"markdown","4ad27ea1":"markdown","dfbb8d6b":"markdown","1a20775b":"markdown","01e51407":"markdown","1ba41065":"markdown","5a2e4cd3":"markdown","fcc7724f":"markdown","c88f1d24":"markdown","7d24a8f0":"markdown","b7d44007":"markdown"},"source":{"9038c03d":"internet_on = True","3358bc49":"if internet_on==True:\n    !pip install fairseq fastBPE ","9a208eff":"if internet_on==True:\n    !ls \/root\/.cache\/pip\/wheels\/df\/60\/ff\/1764bce64cccd9d2c06ba19e5f6f4108ad29e2d48e1068c684","95d45ec1":"if internet_on==True:\n    !ls \/root\/.cache\/pip\/wheels\/fb\/85\/9b\/286072121774d5b8b0253ab66271b558069189cbe795bc6084","d211b9ad":"if internet_on==True:\n    !mv \/root\/.cache\/pip\/wheels\/df\/60\/ff\/1764bce64cccd9d2c06ba19e5f6f4108ad29e2d48e1068c684\/* \/kaggle\/working\n    !mv \/root\/.cache\/pip\/wheels\/fb\/85\/9b\/286072121774d5b8b0253ab66271b558069189cbe795bc6084\/* \/kaggle\/working","9dcbad02":"if internet_on==False:\n    !pip install ..\/input\/fairseq-and-fastbpe\/sacrebleu-1.4.9-py3-none-any.whl ","fd0c66af":"# These files were saved in version 1 of this notebook when internet_on was True\nif internet_on==False:\n    !pip install ..\/input\/v1-fairseq-fastbpe\/fastBPE-0.1.0-cp36-cp36m-linux_x86_64.whl\n    !pip install ..\/input\/v1-fairseq-fastbpe\/fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl","6f64fae6":"# Install TensorFlow 2.2.0 inplace of 2.0.0\n!pip install -U tensorflow==2.2.0","4df5e395":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","9a226abb":"import tensorflow.compat.v1.logging as tf_logging\ntf_logging.set_verbosity(tf_logging.INFO)\n\nimport os\n\ntry:\n    TPU_WORKER = os.environ[\"TPU_NAME\"]\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f\"Running on TPU: {tpu.cluster_spec().as_dict()['worker']}\")\n    print(f\"TPU_WORKER: {TPU_WORKER}\")\nexcept ValueError: \n    tpu = None\n    gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelif len(gpus) > 1: # multiple GPUs on the VM\n    strategy = tf.distribute.MirroredStrategy(gpus)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nREPLICAS_OR_WORKERS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS_OR_WORKERS}')\n\nAUTO = tf.data.experimental.AUTOTUNE","816d6632":"#From: https:\/\/www.kaggle.com\/christofhenkel\/setup-tokenizer\nfrom types import SimpleNamespace\nfrom fairseq.data.encoders.fastbpe import fastBPE\nfrom fairseq.data import Dictionary\n\nclass BERTweetTokenizer():\n    \n    def __init__(self,pretrained_path = 'pretrained_models\/BERTweet_base_transformers\/'):\n        \n        self.bpe = fastBPE(SimpleNamespace(bpe_codes= pretrained_path + \"bpe.codes\"))\n        self.vocab = Dictionary()\n        self.vocab.add_from_file(pretrained_path + \"dict.txt\")\n        self.cls_token_id = 0\n        self.pad_token_id = 1\n        self.sep_token_id = 2\n        self.pad_token = '<pad>'\n        self.cls_token = '<s>'\n        self.sep_token = '<\/s>'\n        \n    def bpe_encode(self,text):\n        return self.bpe.encode(text)\n    \n    def encode(self,text,add_special_tokens=False):\n        subwords = self.bpe.encode(text)\n        input_ids = self.vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n        return input_ids\n    \n    def tokenize(self,text):\n        return self.bpe_encode(text).split()\n    \n    def convert_tokens_to_ids(self,tokens):\n        input_ids = self.vocab.encode_line(' '.join(tokens), append_eos=False, add_if_not_exist=False).long().tolist()\n        return input_ids\n    \n    #from: https:\/\/www.kaggle.com\/nandhuelan\/bertweet-first-look\n    def decode_id(self,id):\n        return self.vocab.string(id, bpe_symbol = '@@')\n    \n    def decode_id_nospace(self,id):\n        return self.vocab.string(id, bpe_symbol = '@@ ')","8a6f0c4a":"tokenizer = BERTweetTokenizer('\/kaggle\/input\/bertweet-base-transformers\/')","7060ac56":"tokenizer.encode('positive')","285d71ff":"tokenizer.encode('negative')","cdaa3cc8":"tokenizer.encode('neutral')","edef5209":"tokenizer.decode_id([14058])","ecbd83b1":"def read_train():\n    train=pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\n    train['text']=train['text'].astype(str)\n    train['selected_text']=train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\n    test['text']=test['text'].astype(str)\n    return test\n\ndef read_submission():\n    test=pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\n    return test\n    \ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()","3240410f":"train_df.sentiment.value_counts(dropna=False)","9bf0e044":"def jaccard(str1, str2): \n    a = set(str(str1).lower().split()) \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","1e91ab3d":"MAX_LEN = 96\nBATCH_SIZE = 16 * REPLICAS_OR_WORKERS # originally 8\nPATH = '..\/input\/bertweet-base-transformers\/'\nsentiment_id = {'positive': 1809, 'negative': 3392, 'neutral': 14058}","eafe0965":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train_df.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n    \n    # ID_OFFSETS\n    # From: https:\/\/www.kaggle.com\/nandhuelan\/bertweet-first-look (comments)\n    offsets = []; idx=0\n    for t in enc:\n        w = tokenizer.decode_id([t])\n        if text1[text1.find(w,idx)-1] == \" \":\n            idx+=1\n            offsets.append((idx,idx+len(w)))\n            idx += len(w)\n        else:\n            offsets.append((idx,idx+len(w)))\n            idx += len(w)\n\n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n    if len(enc)<92:\n        input_ids[k,:len(enc)+5] = [0] + enc + [2,2] + [s_tok] + [2]\n        attention_mask[k,:len(enc)+5] = 1\n        if len(toks)>0:\n            start_tokens[k,toks[0]+1] = 1\n            end_tokens[k,toks[-1]+1] = 1        \n    if len(enc)>91:\n        input_ids[k,:96] = [0] + enc[:91] + [2,2] + [s_tok] + [2]\n        attention_mask[k,:96] = 1        \n        if len(toks)>0:\n            start_tokens[k,toks[0]+1] = 1\n            end_tokens[k,96-1] = 1","4a9297d7":"ct = test_df.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test_df.shape[0]):        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_df.loc[k,'sentiment']]    \n    if len(enc)<92:\n        input_ids_t[k,:len(enc)+5] = [0] + enc + [2,2] + [s_tok] + [2]\n        attention_mask_t[k,:len(enc)+5] = 1\n    if len(enc)>91:\n        input_ids_t[k,:96] = [0] + enc[:91] + [2,2] + [s_tok] + [2]\n        attention_mask_t[k,:96] = 1  ","7df65255":"all=[]\ncount=0\nfor k in range(train_df.shape[0]):    \n    a = np.argmax(start_tokens[k,])\n    b = np.argmax(end_tokens[k,])\n    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)\n    st = tokenizer.decode_id_nospace(enc[a-1:b])\n    st = st.replace('<unk>','')\n    all.append(jaccard(st,train_df.loc[k,'selected_text']))\nprint('>>>> Jaccard =',np.mean(all))","37bbbb53":"improve_jacc_review = False","914d6847":"if improve_jacc_review == True:\n    all=[]\n    count=0\n    for k in range(train_df.shape[0]):    \n        a = np.argmax(start_tokens[k,])\n        b = np.argmax(end_tokens[k,])\n        text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode_id_nospace(enc[a-1:b])\n        st = st.replace('<unk>','')\n        if jaccard(st,train_df.loc[k,'selected_text'])<.3:\n            print(k)\n            print(st)\n            print(train_df.loc[k,'selected_text'])\n            print()\n        all.append(jaccard(st,train_df.loc[k,'selected_text']))\n    print('>>>> Jaccard =',np.mean(all))","8c832fac":"def scheduler(epoch):\n    return 5e-5 * 0.2**epoch","218cd7ac":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'model.bin',config=config,from_pt=True)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n\n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x[0])\n    x1 = tf.keras.layers.BatchNormalization()(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x[0])\n    x2 = tf.keras.layers.BatchNormalization()(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n    \n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)    \n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return model","9a30ebda":"#5-fold CV\nfolds = 5 # n_splits","7afdbf61":"#This will loop over more than one seed and average all folds from all seeds together\nn_seeds = 1","aa5f3678":"#Set equal to False if you already have model trained and just want to generate predictions. You'll need to save the model weights to input>pre-trained-model.\ntrainModel=True","54d47cfb":"if trainModel==True:\n\n    for x in range(n_seeds): \n        \n        jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n        oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n        oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n        preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n        preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\n        skf = StratifiedKFold(n_splits=folds,shuffle=True,random_state=777+x)\n        for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n            \n            print('#'*25)\n            print('### FOLD (n_splits) %i\/%i'%(fold+1, folds))\n            print('#'*25)\n\n            K.clear_session()\n            with strategy.scope():\n                model = build_model()\n\n            reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n            \n            sv = tf.keras.callbacks.ModelCheckpoint(\n                '%s-roberta-%i-%x.h5'%(VER,fold,x), monitor='val_loss', verbose=1, save_best_only=True,\n                save_weights_only=True, mode='auto', save_freq='epoch')\n\n            hist = model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n                epochs=3, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[sv, reduce_lr],\n                validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n                [start_tokens[idxV,], end_tokens[idxV,]]))\n\n            print('Loading model...')\n            model.load_weights('%s-roberta-%i-%x.h5'%(VER,fold,x))\n\n            print('Predicting OOF...')\n            oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n\n            print('Predicting Test...')\n            preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n            preds_start += preds[0]\/(folds*n_seeds)\n            preds_end += preds[1]\/(folds*n_seeds)\n\n            # DISPLAY FOLD JACCARD\n            all = []\n            for k in idxV:\n                a = np.argmax(oof_start[k,])\n                b = np.argmax(oof_end[k,])\n                if a>b: \n                    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n                    enc = tokenizer.encode(text1)                   \n                    st = tokenizer.decode_id_nospace(enc[a-1:a+3])\n                else:\n                    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n                    enc = tokenizer.encode(text1)\n                    st = tokenizer.decode_id_nospace(enc[a-1:b])\n                st = st.replace('<unk>','')\n                all.append(jaccard(st,train_df.loc[k,'selected_text']))\n            jac.append(np.mean(all))\n            print('>>>> FOLD %i\/%i Jaccard ='%(fold+1, folds),np.mean(all))\n            print()\n        \n        print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))","5bfe0b53":"if trainModel==False:\n        \n    DISPLAY=1\n    \n    for x in range(n_seeds): \n        \n        jac = []\n    \n        oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n        oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n    \n        preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n        preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n        \n        print('#'*70)\n        print('### SEED %x'%(x+1))\n        print('#'*70)\n\n        skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777+x)\n        for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):  \n            \n            print('#'*25)\n            print('### MODEL %i'%(fold+1))\n            print('#'*25)\n\n            K.clear_session()\n            model = build_model()\n            model.load_weights('..\/input\/bertweet-files\/v0-roberta-%i-%x.h5'%(fold,x))\n\n            print('Predicting Test...')\n            preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n            preds_start += preds[0]\/(n_splits*n_seeds)\n            preds_end += preds[1]\/(n_splits*n_seeds)\n            \n            print('Predicting OOF...')\n            oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n            \n            # DISPLAY FOLD JACCARD\n            all = []; pos = []; neg = []; nue = []\n            for k in idxV:\n                a = np.argmax(oof_start[k,])\n                b = np.argmax(oof_end[k,])\n                if a>b: \n                    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n                    enc = tokenizer.encode(text1)\n                    st = tokenizer.decode_id_nospace(enc[a-1:a+3])\n                else:\n                    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n                    enc = tokenizer.encode(text1)\n                    st = tokenizer.decode_id_nospace(enc[a-1:b])\n                st = st.replace('<unk>','')                                              \n                if train_df.loc[k,'sentiment']=='positive':\n                    pos.append(jaccard(st,train_df.loc[k,'selected_text']))\n                if train_df.loc[k,'sentiment']=='negative':\n                    neg.append(jaccard(st,train_df.loc[k,'selected_text']))\n                if train_df.loc[k,'sentiment']=='neutral':\n                    st = text1\n                    nue.append(jaccard(st,train_df.loc[k,'selected_text']))\n                all.append(jaccard(st,train_df.loc[k,'selected_text']))  \n            jac.append(np.mean(all))\n            print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n            print('>>>> FOLD %i Neutral Jaccard ='%(fold+1),np.mean(nue))\n            print('>>>> FOLD %i Positive Jaccard ='%(fold+1),np.mean(pos))\n            print('>>>> FOLD %i Negative Jaccard ='%(fold+1),np.mean(neg))                 \n            print()\n            \n        print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))","eb548537":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode_id_nospace(enc[a-1:a+3])\n    else:\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode_id_nospace(enc[a-1:b])\n    st = st.replace('<unk>','')\n    if test_df.loc[k,'sentiment']=='neutral':\n        st = text1\n    all.append(st)","20417d1f":"test_df['selected_text'] = all\ntest_df[['textID','selected_text']].to_csv('submission.csv',index=False)","a96f5d5f":"This is mostly adjusting and combining code from the following:\n\nhttps:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705 (for majority of script)\n\nhttps:\/\/www.kaggle.com\/al0kharba\/tensorflow-roberta-0-712 (for inference and CNN head, switched from dropout to batch normalization)\n\nhttps:\/\/www.kaggle.com\/christofhenkel\/setup-tokenizer (for tokenizer)\n\nhttps:\/\/www.kaggle.com\/nandhuelan\/bertweet-first-look (for offsets and decoding)","03b6a9b1":"# TensorFlow BERTweet","9c26cb80":"Note: I could not figure out how to get sacrebleu, but found it loaded to Kaggle already","4ad27ea1":"# Data preproccesing","dfbb8d6b":"# Inference","1a20775b":"Changes from V2: Changed CNN head and added LR schedule\n\nChanges from V3: Adjusted epochs and batch size to get fold 4 training to learn (last iteration was stuck and produced a 0.65 jaccard)\n\nChanges from V6: Adjusted post-processing for unk tokens. Added jaccard scores for sentiments. Set neutral equal to text instead of relying on model (jaccard is higher in CV when we do this).","01e51407":"# Test predictions","1ba41065":"# Train","5a2e4cd3":"How good does our process work?","fcc7724f":"See tips on how to install offline: https:\/\/www.kaggle.com\/c\/severstal-steel-defect-detection\/discussion\/113195","c88f1d24":"This notebook uses BERTweet from:\n\nhttps:\/\/github.com\/VinAIResearch\/BERTweet\n\nhttps:\/\/arxiv.org\/abs\/2005.10200\n\n(see discussion here: https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/152861)","7d24a8f0":"# Load  data and libraries","b7d44007":"# Model"}}