{"cell_type":{"c7737bec":"code","74dd422f":"markdown","40fde677":"markdown"},"source":{"c7737bec":"import os\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils import data\n\nnp.random.seed(1234)\ntorch.manual_seed(1234)\n\nTRAIN_SIZE = 0.8\nNUM_PTS = 971\nCROP_SIZE = 128\nSUBMISSION_HEADER = \"file_name,Point_M0_X,Point_M0_Y,Point_M1_X,Point_M1_Y,Point_M2_X,Point_M2_Y,Point_M3_X,Point_M3_Y,Point_M4_X,Point_M4_Y,Point_M5_X,Point_M5_Y,Point_M6_X,Point_M6_Y,Point_M7_X,Point_M7_Y,Point_M8_X,Point_M8_Y,Point_M9_X,Point_M9_Y,Point_M10_X,Point_M10_Y,Point_M11_X,Point_M11_Y,Point_M12_X,Point_M12_Y,Point_M13_X,Point_M13_Y,Point_M14_X,Point_M14_Y,Point_M15_X,Point_M15_Y,Point_M16_X,Point_M16_Y,Point_M17_X,Point_M17_Y,Point_M18_X,Point_M18_Y,Point_M19_X,Point_M19_Y,Point_M20_X,Point_M20_Y,Point_M21_X,Point_M21_Y,Point_M22_X,Point_M22_Y,Point_M23_X,Point_M23_Y,Point_M24_X,Point_M24_Y,Point_M25_X,Point_M25_Y,Point_M26_X,Point_M26_Y,Point_M27_X,Point_M27_Y,Point_M28_X,Point_M28_Y,Point_M29_X,Point_M29_Y\\n\"\n\nLEN_DF = 393930\nCHUNK_SIZE = 50000\n\n\nclass ScaleMinSideToSize(object):\n    def __init__(self, size=(CROP_SIZE, CROP_SIZE), elem_name='image'):\n        self.size = torch.tensor(size, dtype=torch.float)\n        self.elem_name = elem_name\n\n    def __call__(self, sample):\n        h, w, _ = sample[self.elem_name].shape\n        if h > w:\n            f = self.size[0] \/ w\n        else:\n            f = self.size[1] \/ h\n\n        sample[self.elem_name] = cv2.resize(sample[self.elem_name], None, fx=f, fy=f, interpolation=cv2.INTER_AREA)\n        sample[\"scale_coef\"] = f\n\n        if 'landmarks' in sample:\n            landmarks = sample['landmarks'].reshape(-1, 2).float()\n            landmarks = landmarks * f\n            sample['landmarks'] = landmarks.reshape(-1)\n\n        return sample\n\n\nclass CropCenter(object):\n    def __init__(self, size=128, elem_name='image'):\n        self.size = size\n        self.elem_name = elem_name\n\n    def __call__(self, sample):\n        img = sample[self.elem_name]\n        h, w, _ = img.shape\n        margin_h = (h - self.size) \/\/ 2\n        margin_w = (w - self.size) \/\/ 2\n        sample[self.elem_name] = img[margin_h:margin_h + self.size, margin_w:margin_w + self.size]\n        sample[\"crop_margin_x\"] = margin_w\n        sample[\"crop_margin_y\"] = margin_h\n\n        if 'landmarks' in sample:\n            landmarks = sample['landmarks'].reshape(-1, 2)\n            landmarks -= torch.tensor((margin_w, margin_h), dtype=landmarks.dtype)[None, :]\n            sample['landmarks'] = landmarks.reshape(-1)\n\n        return sample\n\n\nclass TransformByKeys(object):\n    def __init__(self, transform, names):\n        self.transform = transform\n        self.names = set(names)\n\n    def __call__(self, sample):\n        for name in self.names:\n            if name in sample:\n                sample[name] = self.transform(sample[name])\n\n        return sample\n\n\nclass ThousandLandmarksDataset(data.Dataset):\n\n    def __init__(self, root, transforms, split=\"train\"):\n        super(ThousandLandmarksDataset, self).__init__()\n        self.root = root\n        landmark_file_name = os.path.join(root, 'landmarks.csv') if split is not \"test\" \\\n            else os.path.join(root, \"test_points.csv\")\n        images_root = os.path.join(root, \"images\")\n\n        # 393930 the number of rows in the training dataset\n        # change to a smaller number if you need to learn from a piece of data\n        n_rows = LEN_DF if split is not \"test\" else None\n        print(f\"Cook {split} data from csv...\")\n\n        df_chunk = pd.read_csv(landmark_file_name, nrows=n_rows, chunksize=CHUNK_SIZE, delimiter='\\t', )\n        self.landmarks = []\n        self.image_names = []\n\n        print(f\"Chunk...\", end=' ')\n        for i, chunk in enumerate(df_chunk):\n            split_idxs = {\"train\": range(0, int(TRAIN_SIZE * len(chunk))),\n                          \"val\": range(int(TRAIN_SIZE * len(chunk)), len(chunk)),\n                          \"test\": range(len(chunk))}\n            idxs = split_idxs[split]\n\n            print(f'{i}...', end=' ')\n            if split in (\"train\", \"val\"):\n                for row in chunk._values[idxs]:\n                    self.image_names.append(os.path.join(images_root, row[0]))\n                    self.landmarks.append(row[1:].astype('int32').reshape((len(row) \/\/ 2, 2)))\n            elif split == 'test':\n                for row in chunk._values[idxs]:\n                    self.image_names.append(os.path.join(images_root, row[0]))\n                self.landmarks = None\n            else:\n                raise NotImplementedError(split)\n        print(f'finish')\n\n        print('Convert to tensor...', end=' ')\n        if split in (\"train\", \"val\"):\n            self.landmarks = torch.as_tensor(self.landmarks)\n        elif split == 'test':\n            self.landmarks = None\n        else:\n            raise NotImplementedError(split)\n\n        self.transforms = transforms\n        print('finish')\n\n    def __getitem__(self, idx):\n        sample = {}\n        if self.landmarks is not None:\n            landmarks = self.landmarks[idx]\n            sample[\"landmarks\"] = landmarks\n\n        image = cv2.imread(self.image_names[idx])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        sample[\"image\"] = image\n\n        if self.transforms is not None:\n            sample = self.transforms(sample)\n\n        return sample\n\n    def __len__(self):\n        return len(self.image_names)\n\n\ndef restore_landmarks(landmarks, f, margins):\n    dx, dy = margins\n    landmarks[:, 0] += dx\n    landmarks[:, 1] += dy\n    landmarks \/= f\n    return landmarks\n\n\ndef restore_landmarks_batch(landmarks, fs, margins_x, margins_y):\n    landmarks[:, :, 0] += margins_x[:, None]\n    landmarks[:, :, 1] += margins_y[:, None]\n    landmarks \/= fs[:, None, None]\n    return landmarks\n\n\ndef create_submission(path_to_data, test_predictions, path_to_submission_file):\n    test_dir = os.path.join(path_to_data, \"test\")\n\n    output_file = path_to_submission_file\n    wf = open(output_file, 'w')\n    wf.write(SUBMISSION_HEADER)\n\n    mapping_path = os.path.join(test_dir, 'test_points.csv')\n    mapping = pd.read_csv(mapping_path, delimiter='\\t')\n\n    for i, row in mapping.iterrows():\n        file_name = row[0]\n        point_index_list = np.array(eval(row[1]))\n        points_for_image = test_predictions[i]\n        needed_points = points_for_image[point_index_list].astype(np.int)\n        wf.write(file_name + ',' + ','.join(map(str, needed_points.reshape(2 * len(point_index_list)))) + '\\n')\n","74dd422f":"\u0412 \u0431\u0435\u0439\u0437\u043b\u0430\u0439\u043d\u0435 \u0432 \u043a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0442\u043e\u0440\u0435 \u043a\u043b\u0430\u0441\u0441\u0430 ThousandLandmarksDataset \u043c\u044b \u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u0432\u0435\u0441\u044c csv-\u0444\u0430\u0439\u043b \u0432 pandas-dataframe:\n```\ndf = pd.read_csv(landmark_file_name, delimiter='\\t'),\n```\n\u0430 \u0437\u0430\u0442\u0435\u043c \u0435\u0449\u0435 \u0440\u0430\u0437 \u043f\u0435\u0440\u0435\u043a\u043b\u0430\u0434\u044b\u0432\u0430\u0435\u043c \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0435 \u0442\u043e\u0447\u043a\u0438 \u0438 \u043f\u0443\u0442\u0438 \u043a \u0444\u0430\u0439\u043b\u0430\u043c \u0432 \u0441\u043f\u0438\u0441\u043a\u0438:\n```\nself.image_names.append(os.path.join(images_root, row[0]))\nself.landmarks.append(row[1:].astype('int32').reshape((len(row) \/\/ 2, 2)))\n```\n\u041d\u0430 \u043c\u043e\u0435\u0439 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u043e\u0439 \u043c\u0430\u0448\u0438\u043d\u0435 \u0432\u0441\u0435\u0433\u043e 16Gb \u043e\u043f\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u043f\u0430\u043c\u044f\u0442\u0438, \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043d\u0435 \u0445\u0432\u0430\u0442\u0430\u043b\u043e \u0434\u043b\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u044d\u0442\u0438\u0445 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0439 \u043d\u0430 \u0432\u0441\u0435\u043c \u043e\u0431\u044a\u0435\u043c\u0435 \u0434\u0430\u043d\u043d\u044b\u0445.\n\n\u042f \u043f\u0435\u0440\u0435\u043f\u0438\u0441\u0430\u043b \u043a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0442\u043e\u0440 \u043a\u043b\u0430\u0441\u0441\u0430 ThousandLandmarksDataset \u0442\u0430\u043a, \u0447\u0442\u043e\u0431\u044b \u0434\u0430\u043d\u043d\u044b\u0435 \u043c\u043e\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u0447\u0438\u0442\u0430\u0442\u044c \u0447\u0430\u0441\u0442\u044f\u043c\u0438. \u042d\u0442\u043e \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u043b\u043e \u0432 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u044d\u0442\u0430\u043f\u043e\u0432 \u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0442\u0430\u0431\u043b\u0438\u0446\u0443 \u0438 \u043d\u0430 \u043a\u0430\u0436\u0434\u043e\u043c \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u0441\u043f\u0438\u0441\u043a\u0438. \n\n\u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0432 \u043a\u043e\u0434\u0435 \u043d\u0435\u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435, \u043d\u043e \u044d\u0442\u043e\u0442 \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u043c\u0435\u043d\u044c\u0448\u0435 \u043e\u043f\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u043f\u0430\u043c\u044f\u0442\u0438. \u0422\u0430\u043a \u043d\u0430 16Gb \u0443 \u043c\u0435\u043d\u044f \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432\u0441\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b. \n","40fde677":"# \u041a\u0430\u043a \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043d\u0430 \u0432\u0441\u0435\u043c \u043e\u0431\u044a\u0435\u043c\u0435 \u0434\u0430\u043d\u043d\u044b\u0445, \u0435\u0441\u043b\u0438 \u0443 \u0432\u0430\u0441 \u043c\u0430\u043b\u043e \u043e\u043f\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u043f\u0430\u043c\u044f\u0442\u0438."}}