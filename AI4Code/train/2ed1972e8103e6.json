{"cell_type":{"6c612907":"code","fe8edc7f":"code","f001c71d":"code","4bdd0af1":"code","e794f1ee":"code","257c2f91":"code","aff13222":"code","3cd8ed49":"code","10f521d2":"code","bc7e3b17":"code","2623cedb":"code","358c2ac3":"code","64340d3a":"code","1324d83d":"code","c0a9e6bf":"code","f09d53fc":"code","12b73171":"code","24d1eb2a":"code","396d660b":"code","de34eaae":"code","0a630c79":"code","3b044199":"code","d02ffa53":"code","5e9a1d4c":"code","823f668d":"code","a65327a4":"code","bb22baeb":"code","b9b20ff4":"code","5302b1c1":"markdown","ab7dcd5e":"markdown","39973023":"markdown","8550d24a":"markdown","5661b050":"markdown","4b3dd9ca":"markdown"},"source":{"6c612907":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fe8edc7f":"import numpy as np\nimport scipy as sp\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score","f001c71d":"data = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","4bdd0af1":"data.head()","e794f1ee":"# Check data types\ndata.dtypes","257c2f91":"# Drop duplicate rows\nduplicates = data[data.duplicated()]\nprint('no of duplicate rows:',duplicates.shape)\n# there are no duplicate rows, so we don't have to remove anything","aff13222":"# Drop missing\/null values\nprint('Number of null values')\nprint(data.isnull().sum())\n# there are no null values","3cd8ed49":"data = data.dropna()\ndata.count()","10f521d2":"# sns.boxplot(x=data['age'])\nsns.boxplot(x=data['creatinine_phosphokinase'])","bc7e3b17":"sns.boxplot(x=data['ejection_fraction'])","2623cedb":"sns.boxplot(x=data['platelets'])","358c2ac3":"sns.boxplot(x=data['serum_creatinine'])\n","64340d3a":"sns.boxplot(x=data['serum_sodium'])\n","1324d83d":"sns.boxplot(x=data['time'])","c0a9e6bf":"Q1 = data.quantile(0.25)\nQ3 = data.quantile(0.75)\nIQR = Q3-Q1\n\n# df = (data <(Q1 - 1.5*IQR)) | (data > (Q3 + 1.5*IQR))\nQ1 - 1.5*IQR","f09d53fc":"# For each of the features we visualized bloxplot above, we will take value that lies within Q1-1.5IQR and Q3+1.5IQR (this will eliminate any outliers)\n# creatinine_phosphokinase\ndata = data[data['creatinine_phosphokinase']>= Q1['creatinine_phosphokinase'] - 1.5*IQR['creatinine_phosphokinase']]\ndata = data[data['creatinine_phosphokinase']<= Q3['creatinine_phosphokinase'] + 1.5*IQR['creatinine_phosphokinase']]\n\n# ejection_fraction\ndata = data[data['ejection_fraction']>= Q1['ejection_fraction'] - 1.5*IQR['ejection_fraction']]\ndata = data[data['ejection_fraction']<= Q3['ejection_fraction'] + 1.5*IQR['ejection_fraction']]\n\n# platelets\ndata = data[data['platelets']>= Q1['platelets'] - 1.5*IQR['platelets']]\ndata = data[data['platelets']<= Q3['platelets'] + 1.5*IQR['platelets']]\n\n# serum_creatinine\ndata = data[data['serum_creatinine']>= Q1['serum_creatinine'] - 1.5*IQR['serum_creatinine']]\ndata = data[data['serum_creatinine']<= Q3['serum_creatinine'] + 1.5*IQR['serum_creatinine']]\n\n# serum_sodium\ndata = data[data['serum_sodium']>= Q1['serum_sodium'] - 1.5*IQR['serum_sodium']]\ndata = data[data['serum_sodium']<= Q3['serum_sodium'] + 1.5*IQR['serum_sodium']]\n\n# time\ndata = data[data['time']>= Q1['time'] - 1.5*IQR['time']]\ndata = data[data['time']<= Q3['time'] + 1.5*IQR['time']]","12b73171":"# after eliminating outliers we get this shape\ndata.shape","24d1eb2a":"# find correlation between variables\n# Finding the relations between the variables.\nplt.figure(figsize=(20,10))\n\nsns.heatmap(data.corr(),cmap='BrBG',annot=True)","396d660b":"# Create train and test set\ny = data['DEATH_EVENT']\nx = data.iloc[:,0:-1]\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)","de34eaae":"# scale data\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","0a630c79":"# LOGISTIC REGRESSION\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train, y_train)\nlog_predictions = logmodel.predict(X_test)\nprint(classification_report(y_test,log_predictions))\nprint('Accuracy LR:',accuracy_score(y_test,log_predictions))","3b044199":"# KNN\n# use elbow method to find the best K \nfrom sklearn.neighbors import KNeighborsClassifier\nerror_rate = []\nfor i in range(1,150):\n    print('runnin: ',i)\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test)) \n    print('KNN',accuracy_score(y_test,pred_i))","d02ffa53":"plt.figure(figsize=(10,6))\nplt.plot(range(1,150), error_rate, color='blue', linestyle = 'dashed', marker='o', markerfacecolor='red',markersize=10)\nplt.title('Error Rate vs K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nprint('When K is between around 4 to 11 the error rate is the lowest, so we will pick value 4 as the best K')","5e9a1d4c":"knn = KNeighborsClassifier(n_neighbors=4)\nknn.fit(X_train, y_train)\nknn_pred = knn.predict(X_test)\nprint(classification_report(y_test,knn_pred))\nprint('Accuracy KNN:',accuracy_score(y_test,knn_pred))","823f668d":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=300)\nrfc.fit(X_train, y_train)\nrfc_pred = rfc.predict(X_test)\nprint(classification_report(y_test,rfc_pred))\nprint('Accuracy RFC',accuracy_score(y_test,rfc_pred))","a65327a4":"# SVM\nfrom sklearn.svm import SVC\nsvm = SVC()\nsvm.fit(X_train, y_train)\nsvm_pred = svm.predict(X_test)\nprint(classification_report(y_test,svm_pred))\nprint('SVM',accuracy_score(y_test,svm_pred))","bb22baeb":"#grid search\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'C':[0.1,1,10,100,1000], 'gamma':[1,0.1,0.01,0.001,0.0001]}\ngrid = GridSearchCV(SVC(probability=True),param_grid, verbose = 3)\ngrid.fit(X_train,y_train)","b9b20ff4":"# Use the best gamma and c value for svm after grid search\nfrom sklearn.svm import SVC\nsvm = SVC(probability=True)\nsvm.fit(X_train, y_train)\nsvm_pred = svm.predict(X_test)\nprint(classification_report(y_test,svm_pred))\nprint('SVM',accuracy_score(y_test,svm_pred))","5302b1c1":"We will use IQR method to find the list of outliers. ie. to remove values that falls outside 1.5IQR below Q1 and 1.5IQR above Q3","ab7dcd5e":"We will first do an exploratory data analysis of the heart failure clinical dataset","39973023":"Now that we have checked for null\/missing values and duplicate rows, \nwe will detect outliers and remove them so that the model is not affected by extreme high or low values","8550d24a":"1. Death_Event mainly depends on serum creatinine and age\n2. non of the features are highly correlated to one another","5661b050":"Out of all the models, I will think of the Random Forest model as the best among them because in this situation, other than having highest accuracy, what is most important is to accurately detect Death Events. Which means if a person died, is the model actually classifying that event as DEATH_EVENT= False. So I will give importance to the recall metrics. Among all the models tested, Random Forest model has the highest recall value and therefore it is the best among all these models.","4b3dd9ca":"Now we will create the Machine learning models\n"}}