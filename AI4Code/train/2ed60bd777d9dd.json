{"cell_type":{"00e6d653":"code","feeedfbb":"code","33fac9d7":"code","0f62831d":"code","acd3cd2f":"code","243b7be8":"code","c56639eb":"code","8040f0a0":"code","904b7068":"code","9845c375":"code","effd7aee":"code","aa848148":"code","ab6a0b28":"code","dfb0cbee":"code","4d605e5b":"code","d6506374":"code","9af4d8d8":"code","4a38cb12":"code","a2568cdc":"code","18e45223":"code","21d54b34":"code","4ff22ce5":"code","0d18d613":"code","2f27cd85":"code","25535010":"code","ab2f03ba":"code","07fb1f51":"markdown","19e244b9":"markdown","03bcbc7f":"markdown","865c7084":"markdown","44d5a768":"markdown","b9dd6010":"markdown","79e089fd":"markdown","a87483bb":"markdown","9eee8034":"markdown"},"source":{"00e6d653":"%ls ..\/input","feeedfbb":"import sys\npackage_dir_a = \"..\/input\/huggingfacepytorchpretrainedbert\/pytorch-pretrained-bert-master\/pytorch-pretrained-BERT-master\"\nsys.path.insert(0, package_dir_a)","33fac9d7":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nimport warnings\nimport numpy as np\nimport gc\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\n\n# BERT \nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\n\n# GPT2\nfrom pytorch_pretrained_bert.modeling_gpt2 import GPT2PreTrainedModel, GPT2Model, GPT2Config\nfrom pytorch_pretrained_bert import GPT2Tokenizer\nfrom pytorch_pretrained_bert import OpenAIAdam\n\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')","0f62831d":"import pdb","acd3cd2f":"MODELS_PATH = '..\/input\/submission-toxicity-classification'","243b7be8":"BERT_MODEL_PATH = '..\/input\/submission-toxicity-classification\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12\/'","c56639eb":"# bert uncased\n!mkdir ..\/input\/bert_uncased\n!cp ..\/input\/submission-toxicity-classification\/bert-models\/bert-models\/bert_config.json  ..\/input\/bert_uncased\/bert_config.json\n!cp ..\/input\/submission-toxicity-classification\/bert-models\/bert-models\/pytorch_model.bin ..\/input\/bert_uncased\/pytorch_model.bin","8040f0a0":"test_df = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype(str) ","904b7068":"#test_df = test_df.iloc[:100]","9845c375":"def bert_get_preds(model, x_test):\n    test_preds = np.zeros((len(x_test)))\n    test = torch.utils.data.TensorDataset(torch.tensor(x_test, dtype=torch.long))\n    test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n    tk0 = tqdm(test_loader)\n    for i, (x_batch,) in enumerate(tk0):\n        pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n        test_preds[i*32:(i+1)*32] = pred[:, 0].detach().cpu().squeeze().numpy()\n    test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n    return test_pred","effd7aee":"def gpt2_get_preds(model, x_test):\n    test_preds = np.zeros((len(x_test)))\n    test = torch.utils.data.TensorDataset(torch.tensor(x_test, dtype=torch.long))\n    test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n    tk0 = tqdm(test_loader)\n    for i, (x_batch,) in enumerate(tk0):\n        pred = model(x_batch.to(device))\n        test_preds[i*32:(i+1)*32] = pred[:, 0].detach().cpu().squeeze().numpy()\n    test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n    return test_pred","aa848148":"def bert_convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)","ab6a0b28":"def gpt2_convert_lines(example, max_seq_length, tokenizer):\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a) > max_seq_length:\n            tokens_a = tokens_a[-max_seq_length:]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids(tokens_a) + [0]*(max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)","dfb0cbee":"class BertHead(nn.Module):\n    \n    def __init__(self, config, hidden_units=256, num_aux_targets=6):\n        super(BertHead, self).__init__()\n        self.hidden_units = hidden_units \n        self.num_aux_targets = num_aux_targets\n        self.config = config\n        \n        self.bert_model = BertForSequenceClassification.from_pretrained(config, cache_dir=None, num_labels=hidden_units)\n        param_optimizer = list(self.bert_model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        \n        self.fc_out = nn.Linear(768, 1)\n        self.fc_aux_out = nn.Linear(768, num_aux_targets)\n        \n        self.fc_dp = nn.Dropout(p=0.4)\n        \n        self.optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n            {'params': [p for p in self.fc_out.parameters()], 'weight_decay': 0.0}, \n            {'params': [p for p in self.fc_aux_out.parameters()], 'weight_decay': 0.0}\n        ]\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        _, pooled_output = self.bert_model.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.bert_model.dropout(pooled_output)\n        logits = self.fc_dp(pooled_output)\n        out = self.fc_out(logits)\n        out_aux = self.fc_aux_out(logits)\n        return torch.cat([out, out_aux], 1) ","4d605e5b":"bert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None, do_lower_case=True)","d6506374":"bert_base_models = ['bert_epoch_2_lb_093967.bin', 'bert_epoch_2_lb_093916.bin', 'bert_epoch_1_lb_093970.bin']","9af4d8d8":"x_test = bert_convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), 220, bert_tokenizer)","4a38cb12":"bert_base_preds = np.zeros((x_test.shape[0], len(bert_base_models)))\n\nfor i, bert_model in enumerate(bert_base_models):\n    model = BertHead('..\/input\/bert_uncased')\n    model.load_state_dict(torch.load(os.path.join(MODELS_PATH, bert_model)))\n    model.to(device)\n    for param in model.parameters():\n        param.requires_grad = False\n    model.eval()\n    preds = bert_get_preds(model, x_test)\n    bert_base_preds[:,i] = preds\n    \n    del model\n    gc.collect()","a2568cdc":"class GPT2ClassificationHeadModel(GPT2PreTrainedModel):\n\n    def __init__(self, config, clf_dropout=0.4, num_aux_targets=6):\n        super(GPT2ClassificationHeadModel, self).__init__(config)\n        self.transformer = GPT2Model(config)\n        self.dropout = nn.Dropout(clf_dropout)\n        \n        self.fc_out = nn.Linear(config.n_embd * 2, 1)\n        self.fc_aux_out = nn.Linear(config.n_embd * 2, num_aux_targets)\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):\n        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n        avg_pool = torch.mean(hidden_states, 1)\n        max_pool, _ = torch.max(hidden_states, 1)\n        logits = torch.cat((avg_pool, max_pool), 1)\n        logits = self.dropout(logits)\n        \n        out = self.fc_out(logits)\n        out_aux = self.fc_aux_out(logits)\n        \n        return torch.cat([out, out_aux], 1) ","18e45223":"gpt2_tokenizer = GPT2Tokenizer.from_pretrained('..\/input\/submission-toxicity-classification\/gpt2-models\/')","21d54b34":"gpt2_base_models = ['gpt2_epoch_1_lb_093912.bin', 'gpt2_epoch_1_lb_093904.bin', 'gpt2_k_epoch_2_lb_093902.bin']","4ff22ce5":"x_test = gpt2_convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), 220, gpt2_tokenizer)","0d18d613":"gpt2_base_preds = np.zeros((x_test.shape[0], len(gpt2_base_models)))\n\nfor i, gpt2_model in enumerate(gpt2_base_models):\n    model = GPT2ClassificationHeadModel.from_pretrained('..\/input\/submission-toxicity-classification\/gpt2-models\/')\n    model.load_state_dict(torch.load(os.path.join(MODELS_PATH, gpt2_model)))\n    model.to(device)\n    for param in model.parameters():\n        param.requires_grad = False\n    model.eval()\n    preds = gpt2_get_preds(model, x_test)\n    gpt2_base_preds[:,i] = preds\n    \n    del model\n    gc.collect()","2f27cd85":"### sample average for now\ntest_pred = ( np.average(bert_base_preds, axis=1) + np.average(gpt2_base_preds, axis=1) ) \/ 2","25535010":"submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': test_pred\n})\nsubmission.to_csv('submission.csv', index=False)","ab2f03ba":"submission.head()","07fb1f51":"### Base","19e244b9":"### Base","03bcbc7f":"## GPT2","865c7084":"## Setup","44d5a768":"> ## BERT","b9dd6010":"# Submission: BERT + GPT2","79e089fd":"## Helpers","a87483bb":"## Data","9eee8034":"## Submission"}}