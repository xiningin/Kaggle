{"cell_type":{"55800785":"code","c42dbdd9":"code","950c68db":"code","5ce298b2":"code","d83797bb":"code","08ab52a8":"code","983ce820":"code","ea482a53":"code","6192d0df":"code","85cac22f":"code","c6590acb":"code","51c7892a":"code","446a92dc":"code","6e346077":"code","cd577ba3":"code","dd5e6067":"code","a8b33e0f":"code","917e3d60":"markdown"},"source":{"55800785":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport zipfile\nimport pandas as pd\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization","c42dbdd9":"# Execute this only in colab after loading the 'data_v2.zip' in the workspace\n\nwith zipfile.ZipFile('..\/input\/hw2-ycbs-273-intro-to-prac-ml\/data_v2.zip', 'r') as zip_ref:\n    zip_ref.extractall('data_v2')","950c68db":"# Loading the dataset from the 'train' directory\n\nbatch_size = 512\nseed = 1337 # Keep the seed same for both 'train' & 'validation' to avoid overlap\n\ntrain_ds = keras.preprocessing.text_dataset_from_directory(\n    \"..\/input\/hw2-ycbs-273-intro-to-prac-ml\/train\", \n    batch_size=batch_size,\n    label_mode='int',\n    validation_split=0.2,\n    subset='training',\n    seed=seed)\n\nval_ds = keras.preprocessing.text_dataset_from_directory(\n    \"..\/input\/hw2-ycbs-273-intro-to-prac-ml\/train\",\n    batch_size=batch_size,\n    label_mode='int',\n    validation_split=0.2,\n    subset='validation',\n    seed=seed)\n\ntext_only_train_ds = train_ds.map(lambda x, y: x)","5ce298b2":"# Create a TextVectorization instance using 2-grams and 'count' mode\n# Note 'text_vectorization' can also be used a keras layer\n# We will use this during the prediction on test data\n\n# max_length = 50\nmax_tokens = 20000\ntext_vectorization = TextVectorization(\n    ngrams=2,\n    output_mode=\"count\",\n    max_tokens=max_tokens,\n)\n\n# Fit it on the train dataset\ntext_vectorization.adapt(text_only_train_ds)\n\n# Map the vocabulary on the 'train' and 'validation' sets\n\ncount_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\ncount_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n","d83797bb":"# Printing few samples of the raw data\n\nfor text_batch, label_batch in train_ds.take(1):\n  for i in range(5):\n    print(\"News: \", text_batch.numpy()[i])\n    print(\"Label:\", label_batch.numpy()[i])","08ab52a8":"# Retrieve a batch (of 512 news and labels) from the dataset and printing 1 sample\n\ntext_batch, label_batch = next(iter(train_ds))\nfirst_news, first_label = text_batch[0], label_batch[0]\nprint(\"News\", first_news)\nprint(\"Label\", first_label)","983ce820":"# Helper function for using 'text_vectorization'\n\ndef count_vectorize_text(text, label):\n  text = tf.expand_dims(text, -1)\n  return text_vectorization(text), label","ea482a53":"# Printing out vectorized text data using 'text_vectorization' layer\n\nprint(\"'count' vectorized question:\",\n      count_vectorize_text(first_news, first_label)[0])","6192d0df":"inputs = keras.Input(shape=(max_tokens,))\nx = layers.Dense(256, activation=\"relu\")(inputs)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(4, activation=\"softmax\")(x)\nmodel = keras.Model(inputs, outputs)\n\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\nmodel.summary()","85cac22f":"callbacks = [\n    keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n                                  patience=2),\n    keras.callbacks.ModelCheckpoint(\"bow_2grams_1.keras\",\n                                    save_best_only=True)\n]","c6590acb":"# Train the model and use validation ds for early stopping and model saving\n\nhistory_bow_2grams_1 = model.fit(count_train_ds,validation_data = count_val_ds, epochs=10, callbacks=callbacks)\nmodel = keras.models.load_model(\"bow_2grams_1.keras\")\nprint(f\"Test acc: {model.evaluate(count_val_ds)[1]:.3f}\")","51c7892a":"history_dict = history_bow_2grams_1.history\nloss_values = history_dict[\"loss\"]\nval_loss_values = history_dict[\"val_loss\"]\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","446a92dc":"# Using the trained model to make prediction on unseen (test) data\n# Here we use the 'adapted' text_vectorization layer and include it as part of a prediction_model\n\nprediction_model = tf.keras.Sequential(\n    [text_vectorization, model])\n\nprediction_model.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    optimizer='adam',\n    metrics=['accuracy'])\n\n# Test it with `val_ds`, which yields raw strings\nloss, accuracy = prediction_model.evaluate(val_ds)\nprint(\"Accuracy: {:2.2%}\".format(accuracy))","6e346077":"# Read the test data in the form of a dataframe\n\ndf_test_data = pd.read_csv('..\/input\/hw2-ycbs-273-intro-to-prac-ml\/data_test_df.csv')\ninputs = df_test_data['data']","cd577ba3":"# Make sure you use the 'prediction_model' and not the trained 'model' alone\n# If you use the 'model' object, you will run int error as the data is still in the 'text' format and needs vectorization\n\npredicted_scores = prediction_model.predict(inputs)\npredicted_scores[0:5]","dd5e6067":"# populating the dataframe to make a submission on Kaggle\n\ndf_predictions = pd.DataFrame(predicted_scores, columns=['solution_' + str(i+1) for i in range(4)])\ndf_predictions.index.rename('Id', inplace=True)\n\ndf_predictions.head(30)","a8b33e0f":"# If using colab, then download this and submit on Kaggle\n\ndf_predictions.to_csv('df_predictions.csv')","917e3d60":"## BoW (2-grams) + 1 FF layer + count vectorization"}}