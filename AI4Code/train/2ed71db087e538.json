{"cell_type":{"562f1787":"code","8ad2f2ed":"markdown","9a51c6c5":"markdown"},"source":{"562f1787":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\ntrain_dataset = dsets.FashionMNIST(root='.\/data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.FashionMNIST(root='.\/data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters \/ (len(train_dataset) \/ batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\n\nclass RNNModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(RNNModel, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n\n        # Number of hidden layers\n        self.layer_dim = layer_dim\n\n        # Building your RNN\n        # batch_first=True causes input\/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='tanh')\n\n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Initialize hidden state with zeros\n        #######################\n        #  USE GPU FOR MODEL  #\n        #######################\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n\n        # One time step\n        # We need to detach the hidden state to prevent exploding\/vanishing gradients\n        # This is part of truncated backpropagation through time (BPTT)\n        out, hn = self.rnn(x, h0.detach())\n\n        # Index hidden state of last time step\n        # out.size() --> 100, 28, 100\n        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --> 100, 10\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28\nhidden_dim = 100\nlayer_dim = 2  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\noutput_dim = 10\n\nmodel = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n\n#######################\n#  USE GPU FOR MODEL  #\n#######################\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\n\n# Number of steps to unroll\nseq_dim = 28  \n\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as tensors with gradient accumulation abilities\n        #######################\n        #  USE GPU FOR MODEL  #\n        #######################\n        images = images.view(-1, seq_dim, input_dim).requires_grad_().to(device)\n        labels = labels.to(device)\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output\/logits\n        # outputs.size() --> 100, 10\n        outputs = model(images)\n\n        # Calculate Loss: softmax --> cross entropy loss\n        loss = criterion(outputs, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                #######################\n                #  USE GPU FOR MODEL  #\n                #######################\n                images = images.view(-1, seq_dim, input_dim).to(device)\n\n                # Forward pass only to get logits\/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                #######################\n                #  USE GPU FOR MODEL  #\n                #######################\n                if torch.cuda.is_available():\n                    correct += (predicted.cpu() == labels.cpu()).sum()\n                else:\n                    correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct \/ total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))# This Python 3 environment comes with many helpful analytics libraries installed\n","8ad2f2ed":"### Purpose: Building a Recurrent Neural Network with PyTorch & Tanh - Activation Function (GPU)\n\n2 Hidden Layers\nUnroll 28 time steps\nEach step input size: 28 x 1\nTotal per unroll: 28 x 28\nFeedforward Neural Network inpt size: 28 x 28\n\n2 Hidden layer\nTanh Activation Function","9a51c6c5":"![image.png](attachment:5bd907a4-8404-4976-97fc-cc7b4eef0878.png)![image.png](attachment:77459801-c5e1-473d-b6a6-6c5eb67aaea6.png)"}}