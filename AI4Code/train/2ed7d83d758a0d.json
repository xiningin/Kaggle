{"cell_type":{"07f89fdc":"code","e04ab437":"code","b7d0483a":"code","518db01a":"code","72797acd":"code","30066927":"code","78b66076":"code","28980817":"code","28192cec":"code","d4eb76c9":"code","ef41daab":"code","c29c9910":"code","2d05729b":"code","645c5a46":"code","03e96acd":"code","b2bc1522":"code","2093313e":"code","cbcfd41e":"code","d4738f6c":"code","ec0db0c8":"code","5983065b":"code","01e1f46b":"code","d96f66f9":"code","636b5f20":"code","6be9f3ca":"code","54090473":"code","e1a51468":"code","b281b2f2":"code","287fa684":"code","6d28907b":"code","f1bfdcca":"code","11059253":"code","26942b48":"code","9198fc43":"code","733bbc03":"code","6934e395":"markdown","8f7466d6":"markdown","fab9a15b":"markdown","2859ce37":"markdown","bd98bfde":"markdown","ff82c478":"markdown","df4cf72a":"markdown","8a694a58":"markdown","b86767f6":"markdown","199d7bea":"markdown","79a2bbc7":"markdown","c26298aa":"markdown"},"source":{"07f89fdc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd\nfrom bayes_opt import BayesianOptimization\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split,GridSearchCV,StratifiedKFold\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder\nfrom sklearn.metrics import roc_auc_score,mean_absolute_error,mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\n\n\nfrom sklearn.model_selection import cross_val_score,KFold\n\n\n# data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e04ab437":"df_train = pd.read_csv('..\/input\/wecrec2020\/Train_data.csv')\ndf_test = pd.read_csv('..\/input\/wecrec2020\/Test_data.csv')","b7d0483a":"print(len(df_train))\nprint(len(df_test))","518db01a":"df_test","72797acd":"df_train","30066927":"df_train.describe()","78b66076":"df_test.describe()","28980817":"test_index=df_test['Unnamed: 0']\nprint(test_index)\n\n","28192cec":"df_train.info()","d4eb76c9":"plt.figure(figsize=(25,30))\n\n# Add title\nplt.title(\"analysis\")\n\n# Heatmap \nsns.heatmap(df_train.corr(),  annot=True)","ef41daab":"plt.figure(figsize=(25,30))\n\n# Add title\nplt.title(\"analysis\")\n\n# Heatmap \nsns.heatmap(df_test.corr(),  annot=True)\ndf_test.corr()","c29c9910":"df_test.info()","2d05729b":"count=0\ncounnt=0\nfor i in df_test['F4']:\n    if i==1:\n        count+=1\nfor i in df_train['F4']:\n    if i==1:\n        counnt+=1\nprint(count,counnt)\n","645c5a46":"df_train.drop(['F1', 'F2'], axis = 1, inplace = True)\n","03e96acd":"train_X = df_train.loc[:, 'F3':'F17']\ntrain_y = df_train.loc[:, 'O\/P']\n\nX_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.2, random_state=43)","b2bc1522":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, train_X, train_y,\n                                    scoring=\"neg_mean_squared_error\",\n                                    cv=kfolds))\n    return rmse","2093313e":"# space = {\n\n\n#         'n_estimators':hp.choice('n_estimators', np.arange(400, 1000, 10, dtype=int)),\n\n    \n#         'gamma': hp.uniform ('gamma', 1,15),\n\n#         'subsample':hp.quniform('subsample', 0.5, 0.9, 0.01),\n\n#         'eta':hp.quniform('eta', 0.05, 0.5, 0.01),\n\n#         'objective':'reg:squarederror',\n\n\n#         'eval_metric': 'rmse',\n\n#     }","cbcfd41e":"\n\n\n\n# def score(params):\n\n#     model = XGBRegressor(**params)\n\n#     model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)],\n\n#               verbose=False, early_stopping_rounds=10,eval_metric='rmse')\n\n#     Y_pred = model.predict(X_test)\n\n#     score = np.sqrt(mean_squared_error(y_test, Y_pred)) \n# #     score=np.sqrt(-cross_val_score(model,train_X,train_y,cv=kfolds,scoring='neg_mean_squared_error'))\n\n#     print(score)\n\n#     return {'loss': score, 'status': STATUS_OK}    \n\n# def optimize(trials, space):\n\n#     best = fmin(score, space, algo=tpe.suggest, max_evals=100)\n\n#     return best\n\n\n# trials = Trials()\n\n# best_params = optimize(trials, space)\n\n\n# # Return the best parameters\n\n# space_eval(space, best_params)\n","d4738f6c":"# rf = RandomForestRegressor(n_estimators=500)\nxgbr=XGBRegressor( gamma= 11.2,eta=0.07, n_estimators= 440,subsample=0.7)\n# xgbb=XGBRegressor()","ec0db0c8":"score = cv_rmse(xgbr)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), )","5983065b":"# space = {\n\n\n#         'n_estimators':hp.choice('n_estimators', np.arange(400, 1200, 10, dtype=int)),\n\n    \n#         'min_child_weight': hp.uniform ('min_child_weight', 1,30),\n    \n#         'max_depth':hp.choice('max_depth', np.arange(5, 13, 1, dtype=int)),\n\n#         'subsample':hp.quniform('subsample', 0.3, 0.9, 0.01),\n\n#         'learning_rate':hp.quniform('learning_rate', 0.05, 0.5, 0.01),\n        \n#         'bagging_fraction': hp.uniform('bagging_fraction',0.5, 1),\n\n#         'eval_metric': 'rmse',\n\n#     }","01e1f46b":"# def score(params):\n\n#     model = LGBMRegressor(**params)\n\n#     model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)],\n\n#               verbose=False, early_stopping_rounds=10,eval_metric='rmse')\n\n#     Y_pred = model.predict(X_test)\n\n#     score = np.sqrt(mean_squared_error(y_test, Y_pred)) \n# #     score=np.sqrt(-cross_val_score(model,train_X,train_y,cv=kfolds,scoring='neg_mean_squared_error'))\n\n#     print(score)\n\n#     return {'loss': score, 'status': STATUS_OK}    \n\n# def optimize(trials, space):\n\n#     best = fmin(score, space, algo=tpe.suggest, max_evals=100)\n\n#     return best\n\n\n# trials = Trials()\n\n# best_params = optimize(trials, space)\n\n\n# # Return the best parameters\n\n# space_eval(space, best_params)","d96f66f9":"lgbm=LGBMRegressor(bagging_fraction=0.77,eval_metric='rmse',learning_rate=0.07,max_depth=12,min_child_weight=13.87,n_estimators=920,subsample=0.73)","636b5f20":"score = cv_rmse(lgbm)\nprint(\"lgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), )","6be9f3ca":"# space = {\n\n\n#         'n_estimators':hp.choice('n_estimators', np.arange(400, 1000, 10, dtype=int)),\n    \n#         'max_depth':hp.choice('max_depth', np.arange(5, 13, 1, dtype=int)),\n\n#         'subsample':hp.quniform('subsample', 0.3, 0.9, 0.01),\n\n#         'learning_rate':hp.quniform('learning_rate', 0.05, 0.5, 0.01),\n\n#     }","54090473":"# def score(params):\n\n#     model = GradientBoostingRegressor(**params)\n\n#     model.fit(X_train, y_train)\n\n#     Y_pred = model.predict(X_test)\n\n#     score = np.sqrt(mean_squared_error(y_test, Y_pred)) \n# #     score=np.sqrt(-cross_val_score(model,train_X,train_y,cv=kfolds,scoring='neg_mean_squared_error'))\n\n#     print(score)\n\n#     return {'loss': score, 'status': STATUS_OK}    \n\n# def optimize(trials, space):\n\n#     best = fmin(score, space, algo=tpe.suggest, max_evals=100)\n\n#     return best\n\n\n# trials = Trials()\n\n# best_params = optimize(trials, space)\n\n\n# # Return the best parameters\n\n# space_eval(space, best_params)","e1a51468":"gbm=GradientBoostingRegressor(learning_rate=0.05,max_depth=8,n_estimators=710,subsample=0.49)","b281b2f2":"# score = cv_rmse(gbm)\n# print(\"gbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), )","287fa684":"stack_gen = StackingCVRegressor(regressors=(gbm, xgbr, lgbm),\n                                meta_regressor=xgbr,\n                                use_features_in_secondary=True)\n","6d28907b":"stack_gen_model = stack_gen.fit(np.array(train_X), np.array(train_y))\nxgb_model=xgbr.fit(train_X, train_y)\ngbr_model=gbm.fit(train_X, train_y)\nlgbm_model=lgbm.fit(train_X, train_y)","f1bfdcca":"\ndef blend_models_predict(X=train_X):\n    return ((0.175 * gbr_model.predict(X)) + (0.175 * xgb_model.predict(X)) + (0.25 * lgbm_model.predict(X)) + (0.4 * stack_gen_model.predict(np.array(X))))\n","11059253":"print('RMSLE score on train data:')\nprint(rmsle(train_y, blend_models_predict(train_X)))","26942b48":"df_test = df_test.loc[:, 'F3':'F17']\n\n# pred = xgb.predict(df_test)","9198fc43":"result = pd.DataFrame()\n\nresult['Id'] = test_index\n\nresult['PredictedValue'] = pd.DataFrame(blend_models_predict(df_test))\n\n","733bbc03":"result.to_csv(\"output.csv\", index=False)","6934e395":"# 3.GBR Tuning","8f7466d6":"# stacking","fab9a15b":"# Tuning Hayperparameters of following estimators using hyperopt :","2859ce37":"# 2.Lightgbm tuning","bd98bfde":"# I have used three estimators namely xgboost,lightgbm and gbm and then stacked and blended them for better performance","ff82c478":"# Util functions for calculating score","df4cf72a":"# 1.Tuning xgboost","8a694a58":"## Reading input data","b86767f6":"## Predicting output for test set and Packing it into output file","199d7bea":"# EDA","79a2bbc7":"# Dropping unnecessary Columns","c26298aa":"## Separating the input and output fields"}}