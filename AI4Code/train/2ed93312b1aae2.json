{"cell_type":{"a542b62b":"code","f5bc2740":"code","355d1be2":"code","0c61320b":"code","343bd1fc":"code","23305b42":"code","8c801824":"code","801db6fc":"code","be51cf70":"code","81a18607":"code","3423a583":"code","ad4aa698":"code","58f1b6a9":"code","84c6ff8b":"code","42978f22":"code","9aeaa9c0":"code","212f6f61":"code","c338c145":"code","bfe99cb5":"code","f51c6bae":"code","b54a8a09":"code","3f9295a5":"code","946df43a":"code","7f77f2c3":"code","350e34c4":"code","19029c25":"code","812a34c3":"code","9a72d3db":"code","375055f2":"code","86cccd3a":"code","6a79851f":"code","d75b48a5":"code","6443889a":"code","fed1c840":"code","3c591369":"code","14d63db2":"code","36bf9e0c":"code","e24a42a8":"code","bde18b37":"code","9b221d1b":"code","ed44808d":"code","9825230b":"code","6c2d37ed":"code","6adb0c87":"code","3e0685c5":"code","b744eee4":"code","9c026a64":"markdown","d4c8ccb5":"markdown","c31320ae":"markdown","8e47098c":"markdown","7c34b12f":"markdown","7e08a388":"markdown","44a6cb87":"markdown","246b2606":"markdown","91f60091":"markdown","629ff641":"markdown","01062b35":"markdown","8966af6b":"markdown","9b9117c9":"markdown","a17d18ee":"markdown","f52ca37a":"markdown","51f85546":"markdown","5b83fda1":"markdown","1372870d":"markdown","1e24fffb":"markdown","6f220759":"markdown","1fb80ca7":"markdown","d69dc404":"markdown","2c2af9bd":"markdown","25653a09":"markdown","7301a1e1":"markdown","f6eb298c":"markdown","2f693cba":"markdown","0d14bb9e":"markdown","563311ed":"markdown","23e4dd69":"markdown","8707573d":"markdown","8acf60db":"markdown","b6413def":"markdown","06cfd9b2":"markdown","7a97e724":"markdown","8bb9406b":"markdown","ed561b64":"markdown","b29eac61":"markdown","9c9ec1b9":"markdown","cec4b65e":"markdown","55392e35":"markdown","74941815":"markdown","2a70c890":"markdown","ac133dc5":"markdown","f78c6e49":"markdown","438432cc":"markdown"},"source":{"a542b62b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f5bc2740":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n\n# To ignore unwanted warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n\n# Reading Data from CSV to DF - Jupyter\n# train = pd.read_csv('data\/train.csv')\n# test = pd.read_csv('data\/test.csv')\n# submission = pd.read_csv('data\/gender_submission.csv')","355d1be2":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\n\n# train data \nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='RdBu_r')\nax[0].set_title('Train data')\n\n# test data\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='RdBu_r')\nax[1].set_title('Test data');","0c61320b":"train.isnull().sum()","343bd1fc":"test.isnull().sum()","23305b42":"#print number of females vs. males that survive\nprint(\"females who survived:\", train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts())\n\nprint(\" males who survived:\", train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts())","8c801824":"# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nlabels = 'Male', 'Female'\nsizes = [109, 233]\nexplode = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","801db6fc":"for x in [1,2,3]:\n    train.Survived[train.Pclass == x].plot(kind=\"kde\")\nplt.title(\"class wrt Survived\")\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.legend((\"1st\",\"2nd\",\"3rd\"))\n\nplt.show","be51cf70":"sns.barplot(x=\"Parch\", y=\"Survived\", data=train,palette=\"Blues\")\nplt.title('Parch Distribution by survived', fontsize=14)","81a18607":"fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(12, 12))\nsns.set_palette(sns.color_palette(('RdBu_r')))\nsns.barplot(train[ 'Pclass'], train['Survived'], ax=ax[0,0])\nsns.barplot(train['Sex'], train[ 'Survived'], ax=ax[0,1]) \nsns.distplot(train['Age'],bins=24,ax=ax[1,0])\nsns.distplot(train['Fare'],bins=24, ax=ax[1,1])\nax[0,0].set_title('The number of survived by Pclass', fontsize=10)\nax[0,1].set_title('The number of survived by Sex', fontsize=10)\nax[1,0].set_title('The number of survived by Age', fontsize=10)\nax[1,1].set_title('The number of survived by Fare', fontsize=10)\nplt.show()","3423a583":"train.Embarked.value_counts()","ad4aa698":"test.Embarked.value_counts()","58f1b6a9":"y = train['Survived']\ntrain.drop('Survived', axis=1, inplace=True)\ntrain.head()","84c6ff8b":"test.set_index('PassengerId',inplace=True)\ntrain.set_index('PassengerId',inplace=True)","42978f22":"test.head()","9aeaa9c0":"data_merge = pd.merge(train ,test,how='outer',left_index=False, right_index=False)","212f6f61":"data_merge.head()","c338c145":"# Split the name column to two different columns (last_name & title)\ndata_merge['last_name'] = data_merge['Name'].apply(lambda x: x.split(',')[0])\ndata_merge['title'] = data_merge['Name'].apply(lambda x: x.split('.')[0].split(',')[1])\n\n# We will also add parch and sibsp to get total of family memebers \ndata_merge['family_size'] = data_merge['SibSp'] + data_merge['Parch'] + 1","bfe99cb5":"data_merge['Embarked'] = data_merge['Embarked'].fillna('S')","f51c6bae":"data_merge['Fare'] = data_merge['Fare'].fillna(data_merge.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0])","b54a8a09":"data_merge['Age'] = data_merge.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","3f9295a5":"bins = [0, 5, 13, 19, 30, 60, 80]\nlabels = ['Baby', 'Child', 'Teenager', 'Young Adult', 'Adult', 'Senior']\ndata_merge['AgeGroup'] = pd.cut(data_merge[\"Age\"], bins, labels = labels)","946df43a":"data_merge.drop('Age', axis = 1, inplace=True)","7f77f2c3":"data_merge.drop('Name', axis=1, inplace=True)","350e34c4":"data_merge.drop('SibSp', axis = 1, inplace=True)\ndata_merge.drop('Parch', axis = 1, inplace=True)","19029c25":"data_merge['Cabin'] = data_merge['Cabin'].fillna('M')","812a34c3":"ax = plt.axes()\nsns.heatmap(data_merge.isnull(), yticklabels=False, ax = ax, cbar=False, cmap='RdBu_r')\nax.set_title('Merga DF');","9a72d3db":"data_merge['Cabin'] = data_merge['Cabin'].apply(lambda x: 'ABC' if x[0] in 'ABCT' else 'DE' if x[0] in 'DE' else 'FG' if x[0] in 'FG' else 'M')","375055f2":"data_merge","86cccd3a":"# DF used for plotting only\ndf_plot_train = data_merge[:891].copy()\n# Seperating our target in a different DF\ndf_plot_train['Survived'] = y\ndf_plot_train","6a79851f":"df_plot_train['title'] = df_plot_train['title'].apply(lambda x: x.strip(' '))\ndf_plot_train['title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms', inplace=True)\ndf_plot_train['title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy', inplace=True)","d75b48a5":"cat_features = ['title', 'Pclass', 'family_size','Cabin']\n\nfig, axs = plt.subplots(ncols=2, nrows=2, figsize=(20, 20))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, feature in enumerate(cat_features, 1):    \n    plt.subplot(2, 2, i)\n    sns.countplot(x=feature, hue='Survived', data=df_plot_train)\n    \n    plt.xlabel('{}'.format(feature), size=20)\n    plt.ylabel('Passenger Count', size=20)    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(['UnSurvived', 'Survived'], loc='upper center', prop={'size': 18})\n    plt.title('Count of Survival in {}'.format(feature), size=20, y=1.05)\n\nplt.show()","6443889a":"# Pie chart\nlabels = 'Baby', 'Child', 'Teenager', 'Young Adult', 'Adult', 'Senior'\nsizes = [44,27,93,392,313,22]\nexplode = (0, 0,0,0,0,0)  \n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=20)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.tight_layout()\nplt.show()","fed1c840":"data_merge.drop('Ticket', axis = 1, inplace=True)\ndata_merge.drop('last_name', axis = 1, inplace=True)","3c591369":"# Changing Sex from male\/female to 0\/1\ndata_merge['Sex'] = data_merge['Sex'].map({'male':0, 'female':1})","14d63db2":"data_merge_dummy = pd.get_dummies(data_merge, drop_first=True)","36bf9e0c":"train_cleaned = data_merge_dummy[:891]\ntest_cleaned = data_merge_dummy[891:]","e24a42a8":"train_cleaned.tail()","bde18b37":"test_cleaned.head()","9b221d1b":"X=train_cleaned","ed44808d":"# Need to do some imports for modeling \nfrom IPython.display import Image  \nfrom io import StringIO\nfrom sklearn.tree import export_graphviz\nimport pydot # please install this package if you don't have it.\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\nfrom sklearn import svm\nsvm_m=svm.SVC()","9825230b":"# Fit Decision Trees\ndt = DecisionTreeClassifier(max_depth = 6,random_state=8)\ndt.fit(X,y)\n# Getting model score\nprint('dt: ', dt.score(X,y))\n# Print results \ns = cross_val_score(dt, X, y, cv=7 )\nprint(\"{} {} Score:\\t{:0.3} \u00b1 {:0.3}\".format(\"Decision Tree\", \"Train\", s.mean().round(3), s.std().round(3)))\nprint()\n\n\n# Fit Bagging + Decision Trees\ndt_bag = BaggingClassifier(max_features =16, n_estimators = 100,random_state=8)\ndt_bag.fit(X,y)\n# Getting model score\nprint('dt_bag: ', dt_bag.score(X,y))\n# Print results \ns = cross_val_score(dt_bag, X, y, cv=7)\nprint(\"{} {} Score:\\t{:0.3} \u00b1 {:0.3}\".format(\"Decision Tree\", \"Train\", s.mean().round(3), s.std().round(3)))\nprint()\n\n\n# Fit Random Forest\nrf = RandomForestClassifier(random_state=8)\nrf.fit(X,y)\n# Getting model score\nprint('rf: ', rf.score(X,y))\n# Print results \ns = cross_val_score(rf, X, y, cv=7)\nprint(\"{} {} Score:\\t{:0.3} \u00b1 {:0.3}\".format(\"Decision Tree\", \"Train\", s.mean().round(3), s.std().round(3)))\nprint()\n\n\n# Fit Extra Trees\ndt_et = ExtraTreesClassifier(random_state=8)\ndt_et.fit(X,y)\n# Getting model score\nprint('dt_et: ', dt_et.score(X,y))\n# Print results \ns = cross_val_score(dt_et, X, y, cv=7)\nprint(\"{} {} Score:\\t{:0.3} \u00b1 {:0.3}\".format(\"Decision Tree\", \"Train\", s.mean().round(3), s.std().round(3)))\n\n# Fit SVM\nsvm_m.fit(X,y)\n# Getting model score\nprint('svm_m: ', svm_m.score(X,y))\n# Print results \ns = cross_val_score(svm_m, X, y, cv=7)\nprint(\"{} {} Score:\\t{:0.3} \u00b1 {:0.3}\".format(\"svm_m\", \"Train\", s.mean().round(3), s.std().round(3)))\nprint()","6c2d37ed":"bag_pred = dt_bag.predict(test_cleaned)\nbag_pred","6adb0c87":"submission['Survived'] = bag_pred\n# Save CSV to path\nsubmission.to_csv('submission_dt_bag.csv', index=False)\n# Jupyter save to c\n# submission.to_csv('data\/submission_dt_bag_best_r, index=False)","3e0685c5":"dt_best = dt_bag.base_estimator_\ndt_best.fit(X,y)","b744eee4":"# Need to do some imports\nfrom IPython.display import Image  \nfrom io import StringIO\nfrom sklearn.tree import export_graphviz\nimport pydot # please install this package if you don't have it.\n\ndot_data = StringIO('\/data\/tree.dot')  \nexport_graphviz(decision_tree = dt_best,\n                 out_file = dot_data,\n                 feature_names = X.columns, \n                 filled=True, \n                 rounded=True) \n\ngraph = pydot.graph_from_dot_data(dot_data.getvalue())  \nImage(graph[0].create_png()) ","9c026a64":"- As we can see, the merge DF does not have a missing data.","d4c8ccb5":"- Seperating features into X ","c31320ae":"## Now let's start looking at missing data","8e47098c":"We noticed that both dataframes contain missing data.\n- The train DF have missing data in Age,Cabin and Embarked features.\n- The test DF have missing data in Age,Cabin and Fare features.","7c34b12f":"- Filling Embarked columns with the mode of the column","7e08a388":"The titanic happened more than a 100 years ago, yet its data still gets used to determine factors of survivel. It is unfortunate that many of the critical data that might have been helpful in increasing accurcy was lost. Cabin information based on the titanic blue print mentioned that cabin A - G have an increasing distance to the stair case leading to the top of the ship. This would have greatly impacted the survival rate of each cabin. Also some more info such as which members in the data were part of the crew and which were passengers might have added to the accuracy of our model since many of the crew were on the ship helping everyone off board to avoid the tragedy. We can see that almost 75 % of women survived the tragedy while only about 15 % of men did. From the men, most of them were childern. Between Class, Sex and Age, we were able to get an acceptiable prediction based on the data that was provied. \nI hope this was insightful and Thank you for reading\n\n- Group 1 DSI7 ","44a6cb87":"- z=  1 cx =  5 max =  17 n_e=  104\n- dt_bag:  0.9281705948372615\n- Decision Tree Train Score:\t0.845 \u00b1 0.038\n","246b2606":"- We noticed in the plot that the percentage of adults and young adults people is the highest percentage of passengers in Titanic.","91f60091":"- Seperating data to train and test again","629ff641":"- Cabin data is missing 77.6 % of its data, it does not make sense to fill it with the mode ofe the column \n- We decided that since this is a large group, could be characterized a separate cabin group","01062b35":"- A quick look at the survival infor based on Sex","8966af6b":"- Dropping unnecessary columns after creating features from them ","9b9117c9":"# Survival based on class","a17d18ee":"## here we want to see the null values","f52ca37a":"- Merging train and test data in preperation for cleaning","51f85546":"- We can see that the 2nd class is almost devivded 50\/50 between survivng and not. While the 1st class have the higest survival rate and 3rd have the lowest","5b83fda1":"We noticed on plots the following:\n- The highest number of survivors was in 'Miss\/Mrs\/Ms'  group. Then, men in the 'Mr' are the highest number from another group of men.\n- The number of first Pclass survivors is higher than the second and third Pclass.\n- We labeled the family size with 1 as single, then (2,3) as a small family, (4,5,6) as a medium family and (7,8,11) as a big family. And the plot showed that the highest number of survivors was single people.\n- The Highest number of survivors was in M Cabin.","1372870d":"# Modeling Section","1e24fffb":"- From the ticket column and last_name column, we were able to see that some families or groups shared a similiar ticket and all had the same fair \n- While they might have had different cabins, they still were all close to each other and that might have impacted their survival rate \n- Since cabin data are missing so much cabin info, we think the columns will be of no use to us","6f220759":"# General plots to get more info ","1fb80ca7":"- Plotting a heatmap to check missing data after cleaning ","d69dc404":"### we chang the title so it can be esay to read ","2c2af9bd":"- Creating 6 different age groups to be apple to characterize them ","25653a09":"- If the above section did not work, please see the picture in github ","7301a1e1":"## as we can see we alot of missing  data in Age , Cabin and Embarked","f6eb298c":"# Many passengers had families with them","2f693cba":"- We will try to group families using last name to get an idea of families that survived this tragic incident ","0d14bb9e":"Team Members: \n    - Rawan Almalki \n    - Rayan Raad\n    - Ghalib Tawfiq","563311ed":"- We can see the Decision Tree Bagging Classifier is the current best model \n- We will generate predicitions based on the best model","23e4dd69":"- Filling fare column with the average based on class, parch and sibsp","8707573d":"The titanic is one of the most famous ships to ever sail the oceans. The ship was sailing through the atlantic ocean when it hit an iceberg and sunk on April 15, 1912. It is considered to be one of the famouse tragedies in history as approximately 68 % of people on it died, icluding passengers and crew members. Today we will explore some of the factories that might have attributed to the survival of the people that were on board.","8acf60db":"# Conclusion","b6413def":"# DSI7 - Group 1","06cfd9b2":"# Creating Dummies after cleaning data","7a97e724":"- Approximatly 78 % of cabin data is missing which might greatly affect our predictions of survival","8bb9406b":"## We remove Survived column form Train so we can merge","ed561b64":"- Since cabin A,B & C are exclusive for class one, and FG are for class 2 we can group them as four seperate categories \n- ABC for class 1\n- DE, D shared between all groups and \n- FG are shared between class 2 and 3","b29eac61":"- Store predicitions in the submissions survive column and save it as a csv","9c9ec1b9":" - As we can see,the percent of female survivors is higher than male.","cec4b65e":"As we can see, the  plots shows:\n- The number of first Pclass survivors is higher than the second and third Pclass.\n- The number of female survivors is higher than male.\n- The plot showed us a peak between the ages of 0 and 5 for infants and children. Also, showed a peak in the number of survivors between the ages of 20 and 40 years.\n- The Fare showed a positively skewed.\n\nAs we can see above how many Survived by (sex , age , fare  and pclass)","55392e35":"## The Dataframe Description\n|Feature|Type|Dataset|Description|\n|---|---|---|---|\n|PassengerId|int64|Train-Test|Passenger ID|\n|Name|object|Train-Test|Name|\n|survival|int64|Train-Test|Survival (0 = No, 1 = Yes)|\n|pclass|int64|Train-Test|Ticket class(1 = 1st, 2 = 2nd, 3 = 3rd)|\n|sex|object|Train-Test|Sex|\n|Age|float64|Train-Test|Age in years|\n|sibsp|int64|Train-Test|# of siblings \/ spouses aboard the Titanic|\n|parch|int64|Train-Test|# of parents \/ children aboard the Titanic|\n|ticket|object|Train-Test|Ticket number|\n|fare|float64|Train-Test|Passenger fare|\n|cabin|object|Train-Test|Cabin number|\n|embarked|object|Train-Test|Port of Embarkation(C = Cherbourg, Q = Queenstown, S = Southampton)|","74941815":"- We will start by plotting a heatmap of missing values for both datasets","2a70c890":"## Here we import the libarey and the Dataframe","ac133dc5":"# Visual Model of our Decision Tree","f78c6e49":"- Filling the age column with the median based on sex and class","438432cc":"- The plot show us the number of total passengers that have parents or children. So, we noticed that families have chance to survive more than the single."}}