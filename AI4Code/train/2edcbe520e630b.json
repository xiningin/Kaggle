{"cell_type":{"97404a43":"code","540c3656":"code","962e4fd2":"code","deaa7f29":"code","472a4858":"code","79a0f95d":"code","ba5b858a":"code","da01b70b":"code","1e32a31a":"code","5736c0e4":"code","ddfa55a9":"code","1c66f8fd":"code","7543a4e4":"code","e92891a8":"code","135c806c":"code","bf140c2f":"code","df4a02f8":"code","4a44b89e":"code","85e606df":"code","98dfb75b":"code","a75b30e7":"code","9d23f03f":"code","f2c11417":"code","81b540aa":"code","5e2013d7":"code","f21ad683":"code","89a84f0e":"code","02912866":"code","99a4cae5":"code","5a9432e6":"code","a44429ba":"code","baeaf913":"code","c77415e5":"code","be27ccd2":"code","8c7b99bc":"code","d75b222f":"code","67a8c9c6":"code","d689e8c1":"code","48fdf284":"code","ec6e71f8":"code","a48ee549":"code","05652c70":"code","668b99b0":"code","c6b3f0c6":"code","0e0766c4":"code","fe702212":"code","eecbe94c":"code","421440c4":"code","9976a611":"code","a3e07c5a":"code","02688207":"code","18446721":"code","49f368c5":"code","8d3414ea":"code","26a9e772":"code","d67016c9":"code","1122e9f6":"code","25de69fa":"code","120f8138":"code","0c39b3fd":"markdown","8fa912ed":"markdown","7968fe48":"markdown","347bd1ca":"markdown","dd9934d8":"markdown","26bbf322":"markdown","fcf6229c":"markdown","e60dc2f5":"markdown","bc4eaadf":"markdown","17c8ab57":"markdown","7f5ff44d":"markdown","3803bdd6":"markdown","421acf2c":"markdown","6d9beafd":"markdown","a1420791":"markdown","3d272347":"markdown","90e19e1d":"markdown","82966b2f":"markdown","16e9fd0e":"markdown","233fbded":"markdown","cef6d56d":"markdown","f8dabc71":"markdown","243de7b3":"markdown","1ad471a5":"markdown","dd1c6eea":"markdown","931e4c1f":"markdown","49d5dfbe":"markdown","501c49db":"markdown","f646d86d":"markdown","1fc2238b":"markdown","5d301f56":"markdown","467fd293":"markdown","58a2cb86":"markdown","41acdce2":"markdown","16a6a047":"markdown","5e47b8e3":"markdown","ac2167cf":"markdown","4cf27116":"markdown"},"source":{"97404a43":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport numpy as np \nimport pandas as pd \nwarnings.filterwarnings('ignore')\nimport os\n\nimport plotly\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score,ConfusionMatrixDisplay,precision_score,recall_score,f1_score,classification_report,roc_curve,plot_roc_curve,auc,precision_recall_curve,plot_precision_recall_curve,average_precision_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder","540c3656":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","962e4fd2":"data=pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","deaa7f29":"display(data.shape)\ndisplay(data.head())","472a4858":"data.info()","79a0f95d":"data['hypertension'].unique()","ba5b858a":"data['heart_disease'].unique()","da01b70b":"data.isna().sum()\n### There are few missing values in bmi column","1e32a31a":"data[data['bmi'].notna()]['bmi'].median()","5736c0e4":"data['bmi'].fillna(value=data[data['bmi'].notna()]['bmi'].median(),inplace=True)","ddfa55a9":"data.describe()","1c66f8fd":"data[['age','avg_glucose_level','bmi']]","7543a4e4":"ex.box(data_frame=data,y=['age','avg_glucose_level','bmi'],template='ggplot2',title='Boxplot')","e92891a8":"data.groupby('gender')['id'].count().reset_index().rename({'id':'count'},axis=1)","135c806c":"from plotly.subplots import make_subplots\nfig=go.Figure()\nfig.add_trace(go.Bar(\n    x=data.groupby('gender')['id'].count().reset_index().rename({'id':'count'},axis=1)['gender'],\n    y=data.groupby('gender')['id'].count().reset_index().rename({'id':'count'},axis=1)['count'],\n    name='Gender Count',\n    marker_color='orange',\n    text=data.groupby('gender')['id'].count().reset_index().rename({'id':'count'},axis=1)['count'],\n    textposition='inside',\n    yaxis='y1'\n))\nfig.update_layout(\n    title=\"Gender Wise distribution\",\n    xaxis_title=\"Gender\",\n    yaxis_title=\"Counts\",\n    template='ggplot2',\n    font=dict(\n        size=20,\n        color=\"Black\",  \n    ),\n    xaxis=dict(showgrid=False),\n    yaxis=dict(showgrid=False),\n    plot_bgcolor='white',\n)\nfig.show()","bf140c2f":"data.groupby('hypertension')['id'].count().reset_index().rename({'id':'count'},axis=1)","df4a02f8":"from plotly.subplots import make_subplots\nfig=go.Figure()\nfig.add_trace(go.Bar(\n    x=data.groupby('hypertension')['id'].count().reset_index().rename({'id':'count'},axis=1)['hypertension'],\n    y=data.groupby('hypertension')['id'].count().reset_index().rename({'id':'count'},axis=1)['count'],\n    name='Hypertension Count',\n    marker_color='maroon',\n    text=data.groupby('hypertension')['id'].count().reset_index().rename({'id':'count'},axis=1)['count'],\n    textposition='inside',\n    yaxis='y1'\n))\nfig.update_layout(\n    title=\"Hypertension Wise distribution\",\n    xaxis_title=\"Hypertension\",\n    yaxis_title=\"Counts\",\n    template='ggplot2',\n    font=dict(\n        size=20,\n        color=\"Black\",  \n    ),\n    xaxis=dict(showgrid=False),\n    yaxis=dict(showgrid=False),\n    plot_bgcolor='white',\n)\nfig.show()","4a44b89e":"data.groupby('heart_disease')['id'].count().reset_index().rename({'id':'count'},axis=1)","85e606df":"from plotly.subplots import make_subplots\nfig=go.Figure()\nfig.add_trace(go.Bar(\n    x=data.groupby('heart_disease')['id'].count().reset_index().rename({'id':'count'},axis=1)['heart_disease'],\n    y=data.groupby('heart_disease')['id'].count().reset_index().rename({'id':'count'},axis=1)['count'],\n    name='Hypertension Count',\n    marker_color='pink',\n    text=data.groupby('heart_disease')['id'].count().reset_index().rename({'id':'count'},axis=1)['count'],\n    textposition='inside',\n    yaxis='y1'\n))\nfig.update_layout(\n    title=\"Heart Disease Wise distribution\",\n    xaxis_title=\"Heart Disease\",\n    yaxis_title=\"Counts\",\n    template='ggplot2',\n    font=dict(\n        size=20,\n        color=\"Black\",  \n    ),\n    xaxis=dict(showgrid=False),\n    yaxis=dict(showgrid=False),\n    plot_bgcolor='white',\n)\nfig.show()","98dfb75b":"from plotly.subplots import make_subplots\nfig=go.Figure()\nfig.add_trace(go.Bar(\n    x=data.groupby('ever_married')['id'].count().reset_index().rename({'id':'count'},axis=1)['ever_married'],\n    y=data.groupby('ever_married')['id'].count().reset_index().rename({'id':'count'},axis=1)['count'],\n    name='Hypertension Count',\n    marker_color='lightgreen',\n    text=data.groupby('ever_married')['id'].count().reset_index().rename({'id':'count'},axis=1)['count'],\n    textposition='inside',\n    yaxis='y1'\n))\nfig.update_layout(\n    title=\"Ever Married Wise distribution\",\n    xaxis_title=\"Ever Married\",\n    yaxis_title=\"Counts\",\n    template='ggplot2',\n    font=dict(\n        size=20,\n        color=\"Black\",  \n    ),\n    xaxis=dict(showgrid=False),\n    yaxis=dict(showgrid=False),\n    plot_bgcolor='white',\n)\nfig.show()","a75b30e7":"# Create two additional DataFrames to traces\ndf1 = data[data[\"stroke\"] == 1]\ndf2 = data[data[\"stroke\"] == 0]\n\ntrace1 = go.Bar(x=df1.groupby('work_type')['id'].count().reset_index().rename({'id':'count'},axis=1)['work_type'], \n                y=df1.groupby('work_type')['id'].count().reset_index().rename({'id':'count'},axis=1)['count'], \n                name=\"Stroke0\")\ntrace2 = go.Bar(x=df2.groupby('work_type')['id'].count().reset_index().rename({'id':'count'},axis=1)['work_type'], \n                y=df2.groupby('work_type')['id'].count().reset_index().rename({'id':'count'},axis=1)['count'], \n                name=\"Stroke1\")\n# Fill out  data with our traces\nd = [trace1, trace2]\n# Create layout and specify title, legend and so on\nlayout = go.Layout(title=\"Work Type Wise distribution\",\n                   xaxis=dict(title=\"Work Type\"),\n                   yaxis=dict(title=\"Counts\"),\n                   legend=dict(x=1.0, y=0.5),\n                   # Here annotations need to create legend title\n                   annotations=[\n                                dict(\n                                    xref=\"paper\",\n                                    yref=\"paper\",\n                                    text=\"Stroke\",\n                                    showarrow=False\n                                )],\n                   barmode=\"group\",\n                   template='ggplot2')\n# Create figure with all prepared data for plot\nfig = go.Figure(data=d, layout=layout)\nfig.show()","9d23f03f":"data.info()","f2c11417":"# Create two additional DataFrames to traces\ndf1 = data[data[\"stroke\"] == 1]\ndf2 = data[data[\"stroke\"] == 0]\n\n# Create two traces, first \"Medium\" and second \"High\"\ntrace1 = go.Bar(x=df1.groupby('Residence_type')['id'].count().reset_index().rename({'id':'count'},axis=1)['Residence_type'], \n                y=df1.groupby('Residence_type')['id'].count().reset_index().rename({'id':'count'},axis=1)['count'], \n                name=\"Stroke0\")\ntrace2 = go.Bar(x=df2.groupby('Residence_type')['id'].count().reset_index().rename({'id':'count'},axis=1)['Residence_type'], \n                y=df2.groupby('Residence_type')['id'].count().reset_index().rename({'id':'count'},axis=1)['count'], \n                name=\"Stroke1\")\n# Fill out  data with our traces\nd = [trace1, trace2]\n# Create layout and specify title, legend and so on\nlayout = go.Layout(title=\"Residence Types Wise distribution\",\n                   xaxis=dict(title=\"Residence types\"),\n                   yaxis=dict(title=\"Counts\"),\n                   legend=dict(x=1.0, y=0.5),\n                   # Here annotations need to create legend title\n                   annotations=[\n                                dict(\n                                    xref=\"paper\",\n                                    yref=\"paper\",\n                                    x=1.1,\n                                    y=0.6,\n                                    text=\"Stroke\",\n                                    showarrow=False\n                                )],\n                   barmode=\"group\",\n                   template='ggplot2')\n# Create figure with all prepared data for plot\nfig = go.Figure(data=d, layout=layout)\nfig.show()","81b540aa":"# Create two additional DataFrames to traces\ndf1 = data[data[\"stroke\"] == 1]\ndf2 = data[data[\"stroke\"] == 0]\n\ntrace1 = go.Bar(x=df1.groupby('smoking_status')['id'].count().reset_index().rename({'id':'count'},axis=1)['smoking_status'], \n                y=df1.groupby('smoking_status')['id'].count().reset_index().rename({'id':'count'},axis=1)['count'], \n                name=\"Stroke0\")\ntrace2 = go.Bar(x=df2.groupby('smoking_status')['id'].count().reset_index().rename({'id':'count'},axis=1)['smoking_status'], \n                y=df2.groupby('smoking_status')['id'].count().reset_index().rename({'id':'count'},axis=1)['count'], \n                name=\"Stroke1\")\n# Fill out  data with our traces\nd = [trace1, trace2]\n# Create layout and specify title, legend and so on\nlayout = go.Layout(title=\"Smoking Status Wise distribution\",\n                   xaxis=dict(title=\"Smoking Status\"),\n                   yaxis=dict(title=\"Counts\"),\n                   legend=dict(x=1.0, y=0.5),\n                   # Here annotations need to create legend title\n                   annotations=[\n                                dict(\n                                    xref=\"paper\",\n                                    yref=\"paper\",\n                                    x=1.09,\n                                    y=0.6,\n                                    text='Stroke',\n                                    showarrow=False\n                                )],\n                   barmode=\"group\",\n                   template='ggplot2')\n# Create figure with all prepared data for plot\nfig = go.Figure(data=d, layout=layout)\nfig.show()","5e2013d7":"data.head()","f21ad683":"fig = plt.figure(figsize=(10,10))\nsns.pairplot(data[['gender','age','hypertension','heart_disease','avg_glucose_level','bmi','stroke']],hue='stroke',kind='kde')\nplt.show()","89a84f0e":"data.head()","02912866":"### Generate Label encoders\nle = LabelEncoder()\ndata['gender'] = le.fit_transform(data['gender'])\ndata['ever_married'] = le.fit_transform(data['ever_married'])\ndata['work_type'] = le.fit_transform(data['work_type'])\ndata['Residence_type'] = le.fit_transform(data['Residence_type'])\ndata['smoking_status'] = le.fit_transform(data['smoking_status'])","99a4cae5":"X = data.iloc[:,1:-1]\nY = data.iloc[:,-1]\n\nprint('X Shape', X.shape)\nprint('Y Shape',Y.shape)","5a9432e6":"Y.unique()","a44429ba":"### one hot encoding columns gender,work type and smoking status\nX['gender']=X['gender'].astype(object)\nX['work_type']=X['work_type'].astype(object)\nX['smoking_status']=X['smoking_status'].astype(object)\nX=pd.concat([X,pd.get_dummies(X[['gender','work_type','smoking_status']])],axis=1).drop(['gender','work_type','smoking_status'],axis=1)","baeaf913":"X.shape","c77415e5":"X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=123)\n\nprint('Number transations x_train df',X_train.shape)\nprint('Number transations x_test df',X_test.shape)\nprint('Number transations y_train df',y_train.shape)\nprint('Number transations y_test df',y_test.shape)","be27ccd2":"X.columns","8c7b99bc":"### Define independent variable\npredictors = ['age', 'hypertension', 'heart_disease', 'ever_married',\n       'Residence_type', 'avg_glucose_level', 'bmi', 'gender_0', 'gender_1',\n       'gender_2', 'work_type_0', 'work_type_1', 'work_type_2', 'work_type_3',\n       'work_type_4', 'smoking_status_0', 'smoking_status_1',\n       'smoking_status_2', 'smoking_status_3']","d75b222f":"Y.value_counts()","67a8c9c6":"## Define default model with 1000 estimators and pass these params to the CV method of XGB to get the optimal n_estimators.\n## Pass this optimal n_estimator to the fit method of Xgb on train data\nxgb1 = xgb.XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n nthread=4,\n seed=27,\n scale_pos_weight=19.5)","d689e8c1":"### Define a xgb_cv function to fit on data and find the optimal number of iteration keeping other parameters fixed\n### Function takes input = xgb object with default params , train data ,train y data \ndef modelfit(alg, dtrainX, dtrainY,predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(dtrainX[predictors].values, label=dtrainY)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n                          metrics={'auc'},early_stopping_rounds=early_stopping_rounds)\n    return cvresult ## return dataframe for the iteration till the optimal iteration is reached","48fdf284":"### Object return the optimal number of trees to grow\nn_est=modelfit(xgb1, X_train, y_train,predictors)","ec6e71f8":"### check the returned dataframe\nn_est.shape[0]### 12 iterations","a48ee549":"### Now set the optimal n_estimators \nxgb1.set_params(n_estimators=n_est.shape[0])","05652c70":"#Fit the algorithm on the data\nxgb1.fit(X_train[predictors], y_train)\n\n#Predict training set:\ndtrain_predictions = xgb1.predict(X_train[predictors])\n\n#Print model report:\nprint(\"\\nModel Report Train\")\nprint(\"Accuracy score : %.4g\" % accuracy_score(y_train, dtrain_predictions))\nprint(\"precision_score  : %.4g\" % precision_score(y_train, dtrain_predictions))\nprint(\"recall score : %.4g\" % recall_score(y_train, dtrain_predictions))\nprint(\"F1 score : %.4g\" % f1_score(y_train, dtrain_predictions))\nprint(\"Auc score : %.4g\" % roc_auc_score(y_train, dtrain_predictions))\nprint(\"classification report :{}\".format(classification_report(y_train, dtrain_predictions)))","668b99b0":"### Use grid search by keepin n_estimators from above = 12 and tune max_depth and gamma ,min_child_weight\n## Define the grid\n\nparam_test1 = {\n    'max_depth':np.arange(3,10,2),\n    'min_child_weight':np.arange(1,6,2),\n    'gamma':[i\/10.0 for i in range(0,5)]\n}\n\n### Base estimator with Default values and n_estimators=12\ngsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=12, max_depth=5,\n                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n                                                      objective= 'binary:logistic', nthread=4, seed=27,scale_pos_weight=19.5), \n                                    param_grid = param_test1,scoring='f1',\n                                    n_jobs=4,\n                                    cv=5)","c6b3f0c6":"gsearch1.fit(X_train[predictors],y_train.values)\ngsearch1.best_params_, gsearch1.best_score_","0e0766c4":"### This round try the param value found from above with adjacent values for max_depth,min_child_weight and gamma\nparam_test2 = {\n 'max_depth':[8,9,10],\n 'gamma':[0.1,0.2,0.3]\n}\ngsearch2 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=12, max_depth=5,\n                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n                                                      objective= 'binary:logistic', nthread=4,seed=27,scale_pos_weight=19.5), \n                        param_grid = param_test2, \n                        scoring='f1',\n                        n_jobs=4, cv=5)\n","fe702212":"gsearch2.fit(X_train[predictors],y_train)\ngsearch2.best_params_, gsearch1.best_score_","eecbe94c":"### Fit the Xgb with these parameters and get the optimal n_estimators \n## Pass this optimal n_estimator to the fit method of Xgb on train data\nxgb2 = xgb.XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=8,\n min_child_weight=1,\n gamma=0.2,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n seed=27,\n scale_pos_weight=19.5)","421440c4":"### Object return the optimal number of trees to grow\nn_est_1=modelfit(xgb2, X_train, y_train,predictors)","9976a611":"n_est_1.shape[0]","a3e07c5a":"### Now set the optimal n_estimators \nxgb2.set_params(n_estimators=n_est_1.shape[0])","02688207":"#Fit the algorithm on the data\nxgb2.fit(X_train[predictors], y_train)\n\n#Predict training set:\ndtrain_predictions = xgb2.predict(X_train[predictors])\n\n#Print model report:\nprint(\"\\nModel Report Train\")\nprint(\"Accuracy score : %.4g\" % accuracy_score(y_train, dtrain_predictions))\nprint(\"precision_score  : %.4g\" % precision_score(y_train, dtrain_predictions))\nprint(\"recall score : %.4g\" % recall_score(y_train, dtrain_predictions))\nprint(\"F1 score : %.4g\" % f1_score(y_train, dtrain_predictions))\nprint(\"Auc score : %.4g\" % roc_auc_score(y_train, dtrain_predictions))\nprint(\"classification report :{}\".format(classification_report(y_train, dtrain_predictions)))","18446721":"param_test3 = {\n    'reg_alpha':[0.5, 1, 5, 10, 50],### regularization L1\n    'reg_lambda':[5e-4, 1e-3, 5e-3] ### regularization L2\n}\ngsearch3 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=29, max_depth=8,\n                                                      min_child_weight=1, gamma=0.2, subsample=0.8, \n                                                      colsample_bytree=0.8,\n                                                      objective= 'binary:logistic', nthread=4,seed=27,scale_pos_weight=19.5), \n\n                        param_grid = param_test3, \n                        scoring='f1',\n                        n_jobs=4, cv=5)\n\ngsearch3.fit(X_train[predictors],y_train)\ngsearch3.best_params_, gsearch3.best_score_","49f368c5":"### Fit the Xgb with these parameters and get the optimal n_estimators \n## Pass this optimal n_estimator to the fit method of Xgb on train data\nxgb3 = xgb.XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=8,\n min_child_weight=1,\n gamma=0.2,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,reg_alpha=10,reg_lambda=0.0005,\n seed=27,scale_pos_weight=19.5)","8d3414ea":"### Object return the optimal number of trees to grow\nn_est_2=modelfit(xgb3, X_train, y_train,predictors)","26a9e772":"n_est_2.shape[0]","d67016c9":"### Now set the optimal n_estimators \nxgb3.set_params(n_estimators=n_est_2.shape[0])","1122e9f6":"#Fit the algorithm on the data\nxgb3.fit(X_train[predictors], y_train)\n\n#Predict training set:\ndtrain_predictions = xgb3.predict(X_train[predictors])\n\n#Print model report:\nprint(\"\\nModel Report Train\")\nprint(\"Accuracy score : %.4g\" % accuracy_score(y_train, dtrain_predictions))\nprint(\"precision_score  : %.4g\" % precision_score(y_train, dtrain_predictions))\nprint(\"recall score : %.4g\" % recall_score(y_train, dtrain_predictions))\nprint(\"F1 score : %.4g\" % f1_score(y_train, dtrain_predictions))\nprint(\"Auc score : %.4g\" % roc_auc_score(y_train, dtrain_predictions))\nprint(\"classification report :{}\".format(classification_report(y_train, dtrain_predictions)))","25de69fa":"xgb2","120f8138":"#Fit the algorithm on the data\nxgb2.fit(X_train[predictors], y_train)\n\n#Predict test set:\ndtest_predictions = xgb2.predict(X_test[predictors])\n\n#Print model report:\nprint(\"\\nModel Report Test\")\nprint(\"Accuracy score : %.4g\" % accuracy_score(y_test, dtest_predictions))\nprint(\"precision_score  : %.4g\" % precision_score(y_test, dtest_predictions))\nprint(\"recall score : %.4g\" % recall_score(y_test, dtest_predictions))\nprint(\"F1 score : %.4g\" % f1_score(y_test, dtest_predictions))\nprint(\"Auc score : %.4g\" % roc_auc_score(y_test, dtest_predictions))\nprint(\"classification report :{}\".format(classification_report(y_test, dtest_predictions)))","0c39b3fd":"<a id='3.1.3'><\/a>\n### Round 3","8fa912ed":"<a id='3.1.1'><\/a>\n### Round 1","7968fe48":"#### Hypertension and heart_disease seems to have binary values.","347bd1ca":"### <p style=\"background-color:pink; font-family:newtimeroman; font-size:120%; text-align:center;color:white;border-radius: 15px\">Context<\/p>\n\nAccording to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\nThis dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n\n<b>Data information:<\/b>\n\n1. id: unique identifier\n2. gender: \"Male\", \"Female\" or \"Other\"\n3. age: age of the patient\n4. hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n5. heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n6. ever_married: \"No\" or \"Yes\"\n7. work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n8. Residence_type: \"Rural\" or \"Urban\"\n9. avg_glucose_level: average glucose level in blood\n10. bmi: body mass index\n11. smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n12. stroke: 1 if the patient had a stroke or 0 if not\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient","dd9934d8":"### Data info.","26bbf322":"### <p style=\"color:darkblue\">Ever married<\/p>","fcf6229c":"### Insight Round 3\n* No improvement , set back the default params.","e60dc2f5":"### <p style=\"background-color:pink; font-family:newtimeroman; font-size:120%; text-align:center;color:white;border-radius: 15px\">Table of contents<\/p>\n\n* [1. Loading Data \ud83d\udc8e](#1)\n* [2. EDA \ud83d\udcca](#2)\n* [3. Models \u2699\ufe0f](#3)\n    * [3.1 Xgboost \ud83d\udee0](#3.1)\n        * [3.1.1 Xgboost Round 1 \ud83d\udee0](#3.1.1)\n        * [3.1.2 Xgboost Round 2 \ud83d\udee0](#3.1.2)\n        * [3.1.3 Xgboost Round 3 \ud83d\udee0](#3.1.3)\n        * [3.1.4 Xgboost Round 4 \ud83d\udee0](#3.1.4)\n        * [3.1.5 Xgboost Round 5 \ud83d\udee0](#3.1.5)\n        * [3.1.6 Xgboost Round 6 \ud83d\udee0](#3.1.6)\n* [4. Take away notes \u2699\ufe0f](#4)","bc4eaadf":"### <p style=\"color:darkblue\">Hypertension <\/p>","17c8ab57":"<center>\n<img src=\"https:\/\/images.medicinenet.com\/images\/slideshow\/visual-guide-to-stroke-s2-diagram-of-a-stroke.jpg\">\n<\/center>","7f5ff44d":"### Run CV to get the number of iteration basis early stopping rounds","3803bdd6":"### Insight Round 4\n* Improvement over the benchmark score of F1 with new value to 0.61.\n* Set depth=8,gamma=0.2,n_estimators=29 and tune regularization params like l1 and l2.","421acf2c":"### <p style=\"color:darkblue\">Work type<\/p>","6d9beafd":"### Set the number of iteration fixed and fit the model","a1420791":"### Choose xgb2 model with F1 score 0.6","3d272347":"#### <i><p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:left;color:black;border-radius: 15px\">&nbsp;&nbsp;&nbsp;&nbsp;Insights from EDA<\/p><\/i>\n\n* Age,hypertension ,heart disease doesnt seem to separate stroke. from the above pair plot having overlapping peaks across stork=1 and 0 signifies these variables are not strong enough to separate\/explaing the stroke.\n* The distribution of residence type across the stroke is not significant.\n* There is a little of variation in smoking status for stroke =1.\n* A lot of extreme values are observed in avg glucode level.\n","90e19e1d":"\n<a id='2'><\/a>\n### <p style=\"background-color:maroon; font-family:arial; font-size:160%; text-align:center; border-radius: 15px;color:white\">Lets do some EDA<\/p>","82966b2f":"### <p style=\"color:darkblue\">Residence type<\/p>","16e9fd0e":"### Check the shape","233fbded":"<a id='3.1.6'><\/a>\n### Round 6","cef6d56d":"#### <i><p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:left;color:black;border-radius: 15px\">&nbsp;&nbsp;&nbsp;&nbsp;Pair Plots<\/p><\/i>","f8dabc71":"#### <i><p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:left;color:black;border-radius: 15px\">&nbsp;&nbsp;&nbsp;&nbsp;Count plots<\/p><\/i>","243de7b3":"### \u2b07\ufe0f Importing Libraries","1ad471a5":"### <p style=\"color:darkblue\">Smoking Status<\/p>","dd1c6eea":"### Replace nulls in BMI with median values","931e4c1f":"### <p style=\"color:darkblue\">Gender <\/p>","49d5dfbe":"### <p style=\"color:darkblue\"> Heart disease<\/p>","501c49db":"<a id='3.1.2'><\/a>\n### Round 2 : Fine tuning model complexity using depth,min_child_weights,gamma","f646d86d":"<a id='3.1.5'><\/a>\n### Round 5","1fc2238b":"#### <i><p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:left;color:black;border-radius: 15px\">&nbsp;&nbsp;&nbsp;&nbsp;Lets plot the box plot for numerical columns<\/p><\/i>","5d301f56":"\n<a id='4'><\/a>\n### <p style=\"background-color:maroon; font-family:arial; font-size:160%; text-align:center; border-radius: 15px;color:white\">Take away notes<\/p>\n\n* Model can further be improved with using min_sample_weight and colsample_bytree and fine tuning them.\n* Stratified sampling while spliting into train test.\n* Trying other algorithm likes LGBM ,catboost.","467fd293":"### Insight Round 2\n* Lower F1 score as comapred to Round 1 model.\n* Tune depth and gamma a bit more and others to default at n_estimators=12","58a2cb86":"### Insights from Round 1\n\n* scale_pos_weight uplifts the F1 score to 0.33.\n* Recall for 1's is close to 92%.\n* Overall accuracy is 81%.","41acdce2":"<a id='3'><\/a>\n### <p style=\"background-color:maroon; font-family:arial; font-size:160%; text-align:center; border-radius: 15px;color:white\">Models<\/p>","16a6a047":"<a id='3.1'><\/a>\n### <p style=\"color:darkblue\">Xgboost<\/p>","5e47b8e3":"#### <i><p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:left;color:black;border-radius: 15px\">&nbsp;&nbsp;&nbsp;&nbsp;Lets check missing values first<\/p><\/i>","ac2167cf":"<a id='1'><\/a>\n### <p style=\"background-color:maroon; font-family:arial; font-size:160%; text-align:center; border-radius: 15px;color:white\">Loading dataset<\/p>","4cf27116":"<a id='3.1.4'><\/a>\n### Round 4"}}