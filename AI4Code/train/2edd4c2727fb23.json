{"cell_type":{"4baf6033":"code","ebe04f93":"code","d431d38d":"code","98c7bc7f":"code","293c5d67":"code","8372165e":"code","61d7020d":"code","9f023d4d":"code","44ea01e9":"code","4e523aeb":"code","baafe1d7":"code","f2e3703b":"code","b4314466":"code","51f1b491":"code","86630ae0":"code","180ecc09":"code","d146d6da":"code","9197e495":"code","3e708c1a":"code","318481f3":"code","52ae64fb":"code","565d5a9c":"code","8f9fbcc9":"code","8e0a5a25":"markdown","f43a232a":"markdown","9fd60747":"markdown","e6ba9a5f":"markdown","eecc5677":"markdown","dfd6cdb9":"markdown"},"source":{"4baf6033":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ebe04f93":"train_data = pd.read_csv(\"..\/input\/Kannada-MNIST\/train.csv\")\ntrain_data.shape","d431d38d":"import matplotlib.pyplot as plt\n\nshow_exmpl = train_data.values[:8, :-1]\nplt.figure(1, figsize=(14, 7))\nfor i in range(8):\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(show_exmpl[i].reshape((28, 28)), cmap='gray')","98c7bc7f":"X_train_test = train_data.values[:, 1:]\ny_train_test = train_data.label.values\n\nX_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, test_size = 0.02, random_state=42) \n\nprint('Train shapes: ', X_train.shape, y_train.shape)\nprint('Test shapes: ', X_test.shape, y_test.shape)","293c5d67":"print(np.min(X_train), np.max(X_train))\nX_train_max = np.max(X_train)\nX_train = X_train \/ (0.5 * X_train_max) - 1\nprint(np.min(X_train), np.max(X_train))\n\nprint(np.min(X_test), np.max(X_test))\nX_test = X_test \/ (0.5 * X_train_max) - 1 \nprint(np.min(X_test), np.max(X_test))","8372165e":"from keras.layers import *\nfrom keras.models import Sequential\nfrom keras.optimizers import *\nfrom keras import regularizers\nfrom keras.utils import plot_model, model_to_dot\nfrom IPython.display import SVG","61d7020d":"model = Sequential()\nl2_reg_conv2d = 0\nl2_reg_dense = 0.01\nactivation_type = 'relu'\n\nmodel.add(Conv2D(64, kernel_size=3, activation=activation_type, input_shape=(28, 28, 1), padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=3, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=5, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(128, kernel_size=3, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, kernel_size=3, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, kernel_size=5, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(256, kernel_size=3, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(256, kernel_size=3, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(256, kernel_size=5, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation=activation_type, kernel_regularizer=regularizers.l2(l2_reg_dense)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(128, activation=activation_type, kernel_regularizer=regularizers.l2(l2_reg_dense)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(optimizer = SGD(lr=0.01),\n              loss = 'sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()","9f023d4d":"# model_SGD = Sequential()\n# l2_reg_conv2d = 0\n# l2_reg_dense = 0.01\n\n# model_SGD.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1), padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Conv2D(64, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(MaxPooling2D(pool_size=(3, 3)))\n# model_SGD.add(Dropout(0.4))\n\n# model_SGD.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Conv2D(128, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(MaxPooling2D(pool_size=(3, 3)))\n# model_SGD.add(Dropout(0.3))\n\n# model_SGD.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Conv2D(256, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(MaxPooling2D(pool_size=(3, 3)))\n# model_SGD.add(Dropout(0.2))\n\n# model_SGD.add(Flatten())\n# model_SGD.add(Dense(256, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Dense(128, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Dense(10, activation='softmax'))\n\n# model_SGD.compile(optimizer = SGD(lr=0.01),\n#       loss = 'sparse_categorical_crossentropy',\n#       metrics=['accuracy'])\n\n# model_SGD.summary()","44ea01e9":"# model_SGD_mom = Sequential()\n# l2_reg_conv2d = 0\n# l2_reg_dense = 0.01\n\n# model_SGD_mom.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1), padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Conv2D(64, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(MaxPooling2D(pool_size=(3, 3)))\n# model_SGD_mom.add(Dropout(0.4))\n\n# model_SGD_mom.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Conv2D(128, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(MaxPooling2D(pool_size=(3, 3)))\n# model_SGD_mom.add(Dropout(0.3))\n\n# model_SGD_mom.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Conv2D(256, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(MaxPooling2D(pool_size=(3, 3)))\n# model_SGD_mom.add(Dropout(0.2))\n\n# model_SGD_mom.add(Flatten())\n# model_SGD_mom.add(Dense(256, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Dense(128, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Dense(10, activation='softmax'))\n\n# model_SGD_mom.compile(optimizer = SGD(lr=0.01, momentum=0.9),\n#       loss = 'sparse_categorical_crossentropy',\n#       metrics=['accuracy'])\n\n# model_SGD_mom.summary()","4e523aeb":"# model_Adadelta = Sequential()\n# l2_reg_conv2d = 0\n# l2_reg_dense = 0.01\n\n# model_Adadelta.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1), padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Conv2D(64, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(MaxPooling2D(pool_size=(3, 3)))\n# model_Adadelta.add(Dropout(0.4))\n\n# model_Adadelta.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Conv2D(128, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(MaxPooling2D(pool_size=(3, 3)))\n# model_Adadelta.add(Dropout(0.3))\n\n# model_Adadelta.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Conv2D(256, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(MaxPooling2D(pool_size=(3, 3)))\n# model_Adadelta.add(Dropout(0.2))\n\n# model_Adadelta.add(Flatten())\n# model_Adadelta.add(Dense(256, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Dense(128, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Dense(10, activation='softmax'))\n\n# model_Adadelta.compile(optimizer = Adadelta(learning_rate=1.0),\n#       loss = 'sparse_categorical_crossentropy',\n#       metrics=['accuracy'])\n\n# model_Adadelta.summary()","baafe1d7":"# model_Adam = Sequential()\n# l2_reg_conv2d = 0\n# l2_reg_dense = 0.01\n\n# model_Adam.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1), padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Conv2D(64, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(MaxPooling2D(pool_size=(3, 3)))\n# model_Adam.add(Dropout(0.4))\n\n# model_Adam.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Conv2D(128, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(MaxPooling2D(pool_size=(3, 3)))\n# model_Adam.add(Dropout(0.3))\n\n# model_Adam.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Conv2D(256, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(MaxPooling2D(pool_size=(3, 3)))\n# model_Adam.add(Dropout(0.2))\n\n# model_Adam.add(Flatten())\n# model_Adam.add(Dense(256, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Dense(128, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Dense(10, activation='softmax'))\n\n# model_Adam.compile(optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),\n#       loss = 'sparse_categorical_crossentropy',\n#       metrics=['accuracy'])\n\n# model_Adam.summary()","f2e3703b":"# from keras.utils import plot_model\n# plot_model(model, to_file='model.png')","b4314466":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping,  ReduceLROnPlateau\n\ndatagen = ImageDataGenerator(\n    rotation_range = 20,\n    width_shift_range = 0.3,\n    height_shift_range = 0.3,\n    shear_range = 0.2,\n    zoom_range = 0.3,\n    horizontal_flip = False)","51f1b491":"epochs = 75\nbatch_size = 128\n\nX_train = X_train.reshape(X_train.shape[0],28,28,1)\nX_test = X_test.reshape(X_test.shape[0],28,28,1)\n\ntrain_story = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n                    epochs = epochs, \n                    steps_per_epoch = 100,\n                    validation_data = (X_test, y_test), \n                    callbacks=[\n                      ModelCheckpoint('\/kaggle\/working\/best_kannada_model.h5', save_best_only=True),\n                      CSVLogger('\/kaggle\/working\/learning_log_RMSprop_without_BN.csv'),\n#                       ReduceLROnPlateau(monitor='val_loss', patience=200, verbose=1, factor=0.2),\n                      ],\n                    verbose=1)","86630ae0":"import matplotlib.pyplot as plt \n\nlog_batch_norm = np.array(pd.read_csv(\"\/kaggle\/working\/learning_log_RMSprop_with_BN.csv\")['val_accuracy'])\nlog_no_batch_norm = np.array(pd.read_csv(\"\/kaggle\/working\/learning_log_RMSprop_without_BN.csv\")['val_accuracy'])\n\nplt.figure(figsize=(20,10))\nplt.plot(range(1, 11), log_batch_norm, label='with BatchNorm')\nplt.plot(range(1, 11), log_no_batch_norm, label='without BatchNorm')\nplt.legend()\nplt.show()","180ecc09":"import matplotlib.pyplot as plt \n\nlog_softmax = np.array(pd.read_csv(\"\/kaggle\/working\/learning_log_RMSprop_softmax.csv\")['val_accuracy'])\nlog_elu = np.array(pd.read_csv(\"\/kaggle\/working\/learning_log_RMSprop_elu.csv\")['val_accuracy'])\nlog_relu = np.array(pd.read_csv(\"\/kaggle\/working\/learning_log_RMSprop_relu.csv\")['val_accuracy'])\nlog_tanh = np.array(pd.read_csv(\"\/kaggle\/working\/learning_log_RMSprop_tanh.csv\")['val_accuracy'])\nlog_sigmoid = np.array(pd.read_csv(\"\/kaggle\/working\/learning_log_RMSprop_sigmoid.csv\")['val_accuracy'])\nlog_exponential = np.array(pd.read_csv(\"\/kaggle\/working\/learning_log_RMSprop_exponential.csv\")['val_accuracy'])\n\nplt.figure(figsize=(20,10))\nplt.plot(range(1, 11), log_softmax, label='softmax')\nplt.plot(range(1, 11), log_elu, label='elu')\nplt.plot(range(1, 11), log_relu, label='relu')\nplt.plot(range(1, 11), log_tanh, label='tanh')\nplt.plot(range(1, 11), log_sigmoid, label='sigmoid')\nplt.plot(range(1, 11), log_exponential, label='exponential')\nplt.legend()\nplt.show()","d146d6da":"# OPTIMIZATORS EXP DO NOT RUN\n\n# epochs = 10\n# batch_size = 128\n\n# X_train = X_train.reshape(X_train.shape[0],28,28,1)\n# X_test = X_test.reshape(X_test.shape[0],28,28,1)\n\n# train_story_RMSprop = model_RMSprop.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n#                     epochs = epochs, \n#                     steps_per_epoch = 100,\n#                     validation_data = (X_test, y_test), \n#                     callbacks=[\n#                       ModelCheckpoint('\/kaggle\/working\/best_kannada_model.h5', save_best_only=True),\n#                       CSVLogger('\/kaggle\/working\/learning_log_RMSprop.csv'),\n# #                       ReduceLROnPlateau(monitor='val_loss', patience=200, verbose=1, factor=0.2),\n#                       ],\n#                     verbose=1)\n\n# train_story_SGD_mom = model_SGD_mom.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n#                     epochs = epochs, \n#                     steps_per_epoch = 100,\n#                     validation_data = (X_test, y_test), \n#                     callbacks=[\n#                       ModelCheckpoint('\/kaggle\/working\/best_kannada_model.h5', save_best_only=True),\n#                       CSVLogger('\/kaggle\/working\/learning_log_SGD_mom.csv'),\n#                       ],\n#                     verbose=1)\n\n# train_story_SGD = model_SGD.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n#                     epochs = epochs, \n#                     steps_per_epoch = 100,\n#                     validation_data = (X_test, y_test), \n#                     callbacks=[\n#                       ModelCheckpoint('\/kaggle\/working\/best_kannada_model.h5', save_best_only=True),\n#                       CSVLogger('\/kaggle\/working\/learning_log_SGD.csv'),\n#                       ],\n#                     verbose=1)\n\n# train_story_Adam = model_Adam.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n#                     epochs = epochs, \n#                     steps_per_epoch = 100,\n#                     validation_data = (X_test, y_test), \n#                     callbacks=[\n#                       ModelCheckpoint('\/kaggle\/working\/best_kannada_model.h5', save_best_only=True),\n#                       CSVLogger('\/kaggle\/working\/learning_log_Adam.csv'),\n#                       ],\n#                     verbose=1)\n\n# train_story_Adadelta = model_Adadelta.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n#                     epochs = epochs, \n#                     steps_per_epoch = 100,\n#                     validation_data = (X_test, y_test), \n#                     callbacks=[\n#                       ModelCheckpoint('\/kaggle\/working\/best_kannada_model.h5', save_best_only=True),\n#                       CSVLogger('\/kaggle\/working\/learning_log_Adadelta.csv'),\n#                       ],\n#                     verbose=1)","9197e495":"log_SGD = np.array(pd.read_csv(\"\/kaggle\/working\/learning_log_SGD.csv\")['val_accuracy'])\nlog_SGD_mom = np.array(pd.read_csv(\"\/kaggle\/working\/learning_log_SGD_mom.csv\")['val_accuracy'])\nlog_Adam = np.array(pd.read_csv(\"\/kaggle\/working\/learning_log_Adam.csv\")['val_accuracy'])\nlog_Adadelta = np.array(pd.read_csv(\"\/kaggle\/working\/learning_log_Adadelta.csv\")['val_accuracy'])\nlog_RMSprop = np.array(pd.read_csv(\"\/kaggle\/working\/learning_log_RMSprop.csv\")['val_accuracy'])","3e708c1a":"import matplotlib.pyplot as plt \n\nplt.figure(figsize=(20,10))\nplt.plot(range(1, 11), log_SGD, label='SGD')\nplt.plot(range(1, 11), log_SGD_mom, label='SGD_mom')\nplt.plot(range(1, 11), log_Adam, label='Adam')\nplt.plot(range(1, 11), log_Adadelta, label='Adadelta')\nplt.plot(range(1, 11), log_RMSprop, label='RMSprop')\nplt.legend()\nplt.show()","318481f3":"test_csv = pd.read_csv(\"..\/input\/Kannada-MNIST\/test.csv\")\nX_val = np.array(test_csv.drop(\"id\",axis=1), dtype=np.float32)\nX_val.shape","52ae64fb":"X_val_max = np.max(X_val)\nX_val = X_val \/ (0.5 * X_val_max) - 1\nX_val = np.reshape(X_val, (-1,28,28,1))\n\nprint(X_val.shape, np.min(X_val), np.max(X_val))","565d5a9c":"from keras.models import load_model\n\nbest_model = load_model('\/kaggle\/working\/best_kannada_model.h5')\nY_val = best_model.predict(X_val)\nY_val = np.argmax(Y_val, axis = 1)","8f9fbcc9":"submission = pd.read_csv(\"..\/input\/Kannada-MNIST\/sample_submission.csv\")\nsubmission['label'] = Y_val\nsubmission.to_csv(\"submission.csv\",index=False)","8e0a5a25":"### Load train data","f43a232a":"### Divide to train and validate data","9fd60747":"### CNN with RMSprop Optimizer","e6ba9a5f":"### Submission","eecc5677":"### Visualise train data","dfd6cdb9":"### Data normalization"}}