{"cell_type":{"7086ee3c":"code","cd4c9321":"code","aeead4ac":"code","4b403d9d":"code","53fb2987":"code","e201cecc":"code","e5c713f8":"code","f6b47ba4":"code","5932a693":"code","aae39f6a":"code","08cf0a0b":"code","0cc3eced":"code","046f25af":"code","1c33908d":"code","110e03e1":"code","90899354":"code","40c5b9f3":"code","4b421a6a":"code","7d83fc37":"code","daf4180e":"code","44cabc0e":"code","dfc23a19":"code","21e8ab59":"markdown","cb4ca2ba":"markdown","30c09c03":"markdown","568d4228":"markdown","329b706b":"markdown","61e607e3":"markdown","622aadf3":"markdown","f4535a89":"markdown","790ca631":"markdown","c00e51eb":"markdown","01548b8b":"markdown","d539505f":"markdown","c112bff8":"markdown"},"source":{"7086ee3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport mpl_toolkits.mplot3d.axes3d as p3\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# try:\n#      from rich import print\n# except ImportError as e:\n#     !pip install rich\n#     from rich import print\n    \n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cd4c9321":"nci=pd.read_csv(\"\/kaggle\/input\/nci-data\/d_unsupervised_nci_unique_prep.csv\")","aeead4ac":"print(nci.shape)\nprint(nci.columns)","4b403d9d":"#plot the percent of missing values for columns and rows\nthreshhold=0.3\nto_be_deleted_rows=[]\nto_be_deleted_cols=[]\n\nfor dire,dire_vs in [(1,nci.columns),(0,nci.index)]:\n    NaN_percent=[]\n    max_NaN_percent=(-10,\"\")\n    for c in dire_vs.to_list():\n        d,cr=(nci.iloc[c,:],\"rows\")  if dire==0  else (nci[c],\"columns\")\n        col_=(sum(d.isna())\/len(d),c)\n        NaN_percent.append(col_[0])\n        max_NaN_percent=max([col_, max_NaN_percent])\n        if col_[0]>threshhold: to_be_deleted_rows.append(c) if dire==0  else to_be_deleted_cols.append(c)\n    print(\"the largest percent of NaN's for %s is %.3f which is %s\"%(cr,max_NaN_percent[0],max_NaN_percent[1]))\n    fig=plt.figure(figsize=(10,4))\n    _=plt.plot(NaN_percent,\"-*\")\n    _=plt.title(\"the percentage of NaN's for %s\"%cr)\n    _=plt.ylabel(\"%\")\n    _=plt.xlabel(\"%s index\"%cr)    \n\nprint(\"the rows to be discarded:{}\".format(to_be_deleted_rows))    \nprint(\"the cols to be discarded:{}\".format(to_be_deleted_cols)) \n\n#delete the rows\/cols having percentage of missing values higher than threshold\nnci.drop(index=to_be_deleted_rows,inplace=True) #delete rows\nnci.drop(columns=to_be_deleted_cols,inplace=True) #delete cols","53fb2987":"nci.head(10)","e201cecc":"#drop the rows with Weight=0\nnci.drop(nci[nci[\"Weight\"]==0].index,axis=0,inplace=True)","e5c713f8":"#drop the chemical structure symbol column:\ntry:\n    nci.drop(columns=\"smiles\",inplace=True)\nexcept:\n    pass","f6b47ba4":"# nci.info()\n# check the data with non-float dtype\nnci[nci.columns[nci.dtypes.apply(lambda x: not x in [\"float64\",\"int64\"])]].dtypes","5932a693":"def isfloat(x):\n      if x.startswith(\"-\"):\n            x= x[1:]\n      return \"\".join(x.split(\".\")).isnumeric()\n\nA=nci[\"PEOE_PC-\"].astype(str).apply(lambda x: isfloat(x))\n\n# list the special values that are not numeric:\nprint(\"These are values of PEOE_PC- that are not numeric:\")\nnci.loc[A[A==False].index,\"PEOE_PC-\"]","aae39f6a":"print(nci.loc[[1957,2209],\"PEOE_PC-\"])\nprint(nci.loc[[1957,2209],\"PEOE_PC-\"].astype(str))\nprint(nci.loc[[1957,2209],\"PEOE_PC-\"].astype(str).apply(lambda x: isfloat(x)))\n# nci.loc[[1957,2209],\"PEOE_PC-\"].astype(str)","08cf0a0b":"#drop the line with \"PEOE_PC-\"=\"#NAME?\"\nss=nci.loc[nci[\"PEOE_PC-\"]!=\"#NAME?\",\"PEOE_PC-\"].astype(\"float\")\nnci=nci.loc[ss.index,:]\nprint(\"shape after droping the line with PEOE_PC- = #Name:{}\".format(nci.shape))\n#change datatye for \"PEOE_PC-\"\nnci.loc[ss.index,\"PEOE_PC-\"]=ss.values\n\n#drop too large negative values for \"PEOE_PC-\":\nnci=nci[nci[\"PEOE_PC-\"]>=-10]   \nprint(\"shape after droping large negative values:{}\".format(nci.shape))\n#list all non-float columns to check now \"PEOE_PC-\" column has float dtype \nprint(\"datatypes for columns that are not floattype:\")\nnci.loc[:, nci.columns[nci.dtypes!=\"float64\"] ].dtypes","0cc3eced":"#fill missing values with 0 and record the corresponding positions\nif nci.isna().sum().sum()>0:\n    nan_df=nci.isna()\n    nci.replace(np.nan, 0,inplace=True)\ndisplay(nci.head())\nlen(nci)","046f25af":"print(nci.columns)","1c33908d":"UL_features=nci.loc[:,'CCRF-CEM':'T-47D'].columns\nprint(UL_features)","110e03e1":"\ndata1=nci[UL_features].copy()\nCorrMatrix=data1.corr();\n## reordering features according to the magnitude of the corr matrix element\nfor i in range(CorrMatrix.shape[0]):  \n     CorrMatrix.iat[i, i]= 0 \n\nnew_c=[]\n#Sort the Corr matrix according to absolute values and order the features accordingly\nfor k,v in abs(CorrMatrix).idxmax()[abs(CorrMatrix).max().sort_values(ascending=False).index].items():\n   if not k in new_c:\n       new_c.append(k)\n   if not v in new_c:\n       new_c.append(v)\n\nprint(pd.Index(new_c))\nprint(len(new_c))\n\n\n#use the reordered feature list:\ndata1=data1.reindex(columns=new_c)\nCorrMatrix=data1.corr();","90899354":"f, ax = plt.subplots(figsize=(11, 15))\n#crop a submatrix to plot:\nrow_start=0; col_start=0; size_matrix=10;\nsub_corrmat_to_plot=CorrMatrix.iloc[row_start:(row_start+size_matrix),col_start:(col_start+size_matrix)]\nmask = np.zeros_like(sub_corrmat_to_plot, dtype=np.bool)\n# mask[np.triu_indices_from(mask)]= True\n\n\nheatmap = sns.heatmap(sub_corrmat_to_plot,\n                      mask = mask,\n                      square = True,\n                      linewidths = .5,\n                      cmap = 'coolwarm',\n                      cbar_kws = {'shrink': .4,\n                                'ticks' : [-1, -.5, 0, 0.5, 1]},\n                      ax=ax,\n                      vmin = -1,\n                      vmax = 1,\n                      annot = True,\n                      annot_kws = {'size': 12})\n\n#add the column names as labels\nax.set_yticklabels(sub_corrmat_to_plot.columns, rotation = 0)\nax.set_xticklabels(sub_corrmat_to_plot.columns)\nax.xaxis.set_label_position('top') \n\n# f, ax = plt.subplots(figsize=(11, 15))\n\ngrids = sns.PairGrid(sub_corrmat_to_plot)\ngrids.map_diag(sns.histplot)\ngrids.map_upper(sns.kdeplot)\ngrids.map_lower(plt.scatter)\n\n# sns.set_style({'xtick.bottom': False}, {'ytick.left': True})","40c5b9f3":"from sklearn.decomposition import PCA\nfrom sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler()\n# scaler = preprocessing.RobustScaler()\n# scaler = preprocessing.MinMaxScaler()\n\ndata_scaled = scaler.fit_transform(data1) \ndata_scaled_df=pd.DataFrame(data_scaled,columns=data1.columns)\n\ndata_to_use_in_PCA=data_scaled_df\n\nn_components=15\npca = PCA(n_components=n_components)\nprincipalComponents = pca.fit_transform(data_to_use_in_PCA)\nx_pca_transformed = pd.DataFrame(data = principalComponents)\nx_pca_transformed.head()\nprint(\"explained variance ratio:\",pca.explained_variance_ratio_)\nprint(\"sum of explained variance ratio\",sum(pca.explained_variance_ratio_))\nplt.bar(range(1,n_components+1),pca.explained_variance_ratio_)\nplt.xlabel(\"nth component\")\nplt.ylabel(\"explained variance ratio\")","4b421a6a":"from sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\n# kmeans = KMeans(n_clusters=20, random_state=0).fit(data1)\n\nK=range(1,15)\ninertia={}\ndistortions=[]\n\n\nfor k in K:\n  kmeans = KMeans(n_clusters=k).fit(data1)\n  inertia[k] = kmeans.inertia_\n  distortions.append(sum(np.min(cdist(data1, kmeans.cluster_centers_,\n                                        'euclidean'), axis=1)) \/ data1.shape[0])\n\nfig, ax1 = plt.subplots()\nax1.plot(K,list(inertia.values()),\"bx-\")\nplt.ylabel(\"Inertia\")\nplt.arrow(4,0.85*1e6,-2,0, color='blue',linestyle='dashed',head_width=0.02*1e6, head_length=0.3)\nax2 = ax1.twinx() \nax2.plot(K,distortions,\"r*-\")\nplt.arrow(10,6.6,3,0,color='red',linestyle='dashed',head_width=0.05, head_length=0.3)\nplt.xlabel(\"k\")\nplt.ylabel('Distortions')\n_=plt.title('The Elbow Method using Inertia: unscaled data')","7d83fc37":"#using scaled data data_scaled_df to repeat the Kmeans calculation\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\n# kmeans = KMeans(n_clusters=20, random_state=0).fit(data1)\n\nK=range(1,15)\ninertia={}\ndistortions=[]\n\ntry:\n    data_scaled_df.drop(columns=\"cluster\",inplace=True)\nexcept:\n    pass\n\nfor k in K:\n  kmeans = KMeans(n_clusters=k).fit(data_scaled_df)\n  inertia[k] = kmeans.inertia_\n  distortions.append(sum(np.min(cdist(data_scaled_df, kmeans.cluster_centers_,\n                                        'euclidean'), axis=1)) \/ data_scaled_df.shape[0])\n\nfig, ax1 = plt.subplots()\nax1.plot(K,list(inertia.values()),\"bx-\")\nplt.ylabel(\"Inertia\")\nplt.arrow(4,1e6,-2,0, color='blue',linestyle='dashed',head_width=0.02*1e6, head_length=0.3)\nax2 = ax1.twinx() \nax2.plot(K,distortions,\"r*-\")\nplt.arrow(10,7.3,3,0,color='red',linestyle='dashed',head_width=0.05, head_length=0.3)\nplt.xlabel(\"k\")\nplt.ylabel('Distortions')\n_=plt.title('The Elbow Method using Inertia: scaled data')","daf4180e":"## define a function to find two planes in the feature space for which the projections \n## of inter-cluster distances were as large as possible\ndef projectMax3dim(centerPositions):\n   m=[(0,0,0),(0,0,0)] \n   for k1 in range(1,len(centerPositions)):\n        for k2 in range(k1):\n             A1=centerPositions[k1]-centerPositions[k2]\n             for i in range(1,len(A1)):\n                  for j in range(i):\n                      m.append((np.linalg.norm(np.array([A1[i],A1[j]])),i,j))\n                      m.sort(reverse=True)\n                      m.pop()\n   return m #return two sub-planes","44cabc0e":"# 2-classes clustering:\ntry:\n    data1.drop(columns=\"cluster\",inplace=True)\nexcept:\n    pass\ntry:\n    data_scaled_df.drop(columns=\"cluster\",inplace=True)\nexcept:\n    pass\n\n\ndata_to_use=data1.copy()  #choose whether to use the rescaled x_pca_df or the original data1\n\nkmeans = KMeans(n_clusters=2).fit(data_to_use)\n\ncenterPositions=kmeans.cluster_centers_\ndata_to_use[\"cluster\"]=pd.Categorical(kmeans.labels_)\n\nm=projectMax3dim(centerPositions)\n\nprint(m)\n\nFeatureNames=data_to_use.columns\nm=list(set(m[0][1:]+m[1][1:])) #two planes usually define 3-subdimenions in feature space. \n\n\nfig = px.scatter_3d(data_to_use, x=FeatureNames[m[0]], y=FeatureNames[m[1]], z=FeatureNames[m[2]],\n                color=\"cluster\",size_max=2)\nfig.show()\nif len(m)>3:\n  fig1 = px.scatter_3d(data_to_use, x=FeatureNames[m[1]], y=FeatureNames[m[2]], z=FeatureNames[m[3]],\n                color=\"cluster\",size_max=0.2)\n  fig1.show()\nfig2 = px.scatter(data_to_use, x=FeatureNames[m[0]], y=FeatureNames[m[1]],\n                color=\"cluster\",symbol='cluster',size_max=2)\nfig2.show()\nfig3 = px.scatter(data_to_use, x=FeatureNames[m[0]], y=FeatureNames[m[2]],\n                color=\"cluster\",symbol='cluster',size_max=2)\nfig3.show()\nfig4 = px.scatter(data_to_use, x=FeatureNames[m[1]], y=FeatureNames[m[2]],\n                color=\"cluster\",symbol='cluster',size_max=2)\n# fig4.add_trace(go.Scatter(x=[centerPositions[0][m[1]],centerPositions[1][m[1]]],\n#                           y=[centerPositions[0][m[2]],centerPositions[1][m[2]]],mode='markers',name=\"center\"))\nfig4.show()","dfc23a19":"def prepare_pca(n_components, data, kmeans_labels):\n    names = ['P1', 'P2', 'P3']\n    matrix = PCA(n_components=n_components).fit_transform(data)\n    df_matrix = pd.DataFrame(matrix)\n    df_matrix.rename({i:names[i] for i in range(n_components)}, axis=1, inplace=True)\n    df_matrix['cluster'] = pd.Categorical(kmeans_labels)\n    return df_matrix\n\npca_df2 = prepare_pca(3,data_to_use,kmeans.labels_)\n# f=sns.scatterplot(x=pca_df.x, y=pca_df.y, hue=pca_df.labels, \n#                 palette=\"Set2\")\nfig = px.scatter_3d(pca_df2, x=pca_df2.P1, y=pca_df2.P2, z=pca_df2.P3,\n                color=\"cluster\",symbol='cluster',size_max=2)\nfig.show()\n\n# pca_df2 = prepare_pca(3, data1 ,kmeans.labels_)\n# f=sns.scatterplot(x=pca_df.x, y=pca_df.y, hue=pca_df.labels, \n#                 palette=\"Set2\")\nfig1 = px.scatter(pca_df2, x=pca_df2.P1, y=pca_df2.P2, \n                color=\"cluster\",symbol=\"cluster\",size_max=2)\nfig1.show()\nfig2 = px.scatter(pca_df2, x=pca_df2.P1, y=pca_df2.P3, \n                color=\"cluster\",symbol=\"cluster\",size_max=2)\nfig2.show()\nfig3 = px.scatter(pca_df2, x=pca_df2.P2, y=pca_df2.P3, \n                color=\"cluster\",symbol=\"cluster\",size_max=2)\nfig3.show()","21e8ab59":"Now let's find the 2-classes Kmeans clustering and plot the labeled data in the \n3-dimensions(found by function projectMax3dim) in the feature space. ","cb4ca2ba":"# PCA","30c09c03":"## plot correlation","568d4228":"The elbow method calculations indicate we should choose the cluster number to be 2.","329b706b":"## Reorder features according to correlation","61e607e3":"# 2-classes clustering","622aadf3":"#  Kmeans","f4535a89":"It's interesting to note that the first three entries of the above list seem have numeric type, yet 'astype(str)' would make it into scientific notation and become a non-numeric string. Explicit comparison was made in the following.","790ca631":"# Data import and cleaning","c00e51eb":"# Unsupervised Learning(UL): 'CCRF-CEM':'T-47D' as features","01548b8b":"# Discussion\nAt first look, this result is quite surprising. Let's recall some basic concepts. What PCA does is to find the best axes(through linear transformation in the feature space) so that the data projected onto these axes would have as much variance as possible.  On the other hand, what Kmeans tries to do is to separate data points into different classes so that the total intra-class variance would be minimized. If the number of classes is fixed, this would be roughly equivalent to maximize the inter-class variance(recall the [total variance theorem](https:\/\/en.wikipedia.org\/wiki\/Law_of_total_variance)). So, the PCA axis would be great choices on which class centers separates from each other as far as possible.\n\n The above heuristic argument was supported by a [mathematical proof](http:\/\/ranger.uta.edu\/~chqding\/papers\/KmeansPCA1.pdf) with some mathematical rigor. In this paper, it was shown that principal components calculated by PCA(used for dimension reduction) are aculally the continuous solution to the underlying discrete cluster membership indicator vectors in Kmeans(used for data clustering). This interesting finding and some of its subtlety have attracted lots of attention of [discussion](https:\/\/stats.stackexchange.com\/questions\/183236\/what-is-the-relation-between-k-means-clustering-and-pca). \n\nOne interesting point is that the 2-class separation in the P1-P2 plane is better(less \"mislabeled\" points across the boundary between the two classes) using the original dataset data1 than using the rescaled dataset data_scaled_df(not shown here, one can simply change the line \"data_to_use=data1.copy()\" two cells above and rerun to to see the difference). At first sight this is a little contra-intuitive. Yet it is in agreement with the elbow curve calculations shown above, where for original dataset, a clearer \"elbow\" was seen for the 2-class Kmeans clustering. This fact can be understood by the following way. The \"original\" dataset carries the \"natural difference\" between different features, yet the rescaling makes them uniformly take a zero mean and one variance, which somehow washes away those differences and hurt the 2-class separability of the samples. Another interesting point supporting this argument is the behavior of the total explained variance ratio(by 15 principle components) calculated in the PCA section, i.e., it decreased from around 50% for the unscaled dataset to 44% for the scaled one. \n\n\nAnother point we would mention is that our ploting scheme is somehow complemetary yet roughly equivalent to to other's work, e.g.,  [example](https:\/\/medium.com\/@dmitriy.kavyazin\/principal-component-analysis-and-k-means-clustering-to-visualize-a-high-dimensional-dataset-577b2a7a5fe2). We do Kmeans and label data points first and plot them in the principle component space. More often, people do Kmeans directly to the PCA transformed data in which feature columns are the principle components. \n\nLastly, here is an open problem. Though we are able to effectively separate the two classes in P1-P2 and P1-P3 planes,which are principle components planes calculated by PCA, how can we distinguish different classes practically?  This is an open problem. One way to answer this question is to plot corr matrix for samples for each class, separately, and to discern some quailtative differences for different classes.  The second way might be to use this unsupervised clustering method to a labeled dataset and see how those labels agree with our prediction up to class ordering.  ","d539505f":"Now plot the labeled data in the subspace spanned by the first three principal components","c112bff8":"It's amazing to note that in these **Kmeans-PCA-plots**, P1-P2 and P1-P3 plots have nearly perfect boundaries between two classes. "}}