{"cell_type":{"6ac51eaf":"code","cb1876c2":"code","7368dea4":"code","145d68d5":"code","12d2f2bb":"markdown","02587234":"markdown","c8738160":"markdown","99a02293":"markdown"},"source":{"6ac51eaf":"import numpy as np \nimport matplotlib.pyplot as plt \n\ndef estimate_coef(x, y): \n    n = np.size(x) \n    m_x, m_y = np.mean(x), np.mean(y) \n \n    SS_xy = np.sum(y*x) - n*m_y*m_x \n    SS_xx = np.sum(x*x) - n*m_x*m_x \n\n    theta_1 = SS_xy \/ SS_xx \n    theta_0 = m_y - theta_1*m_x \n\n    return(theta_0, theta_1) \n\ndef plot_regression_line(x, y, theta): \n\n    plt.scatter(x, y, color = \"b\",marker = \"o\", s = 30) \n    y_pred = theta[0] + theta[1]*x \n\n    plt.plot(x, y_pred, color = \"r\") \n\n    plt.xlabel('x') \n    plt.ylabel('y') \n    plt.show() \n\n\nx = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) \ny = np.array([11 ,13, 12, 15, 17, 18, 18, 19, 20, 22]) \n\ntheta = estimate_coef(x, y) \nprint(\"Estimated coefficients:\\ntheta_0 = {} \\ntheta_1 = {}\".format(theta[0], theta[1])) \n\nplot_regression_line(x, y, theta) \n\nprint(round(theta[0]+ theta[1]*11,4))","cb1876c2":"import numpy as np;\nfrom sklearn.linear_model import LinearRegression;\n\n\nx = np.array([[0], [1],[2], [3], [4], [5], [6], [7], [8], [9]]) \ny = np.array([[11], [13], [12], [15], [17], [18], [18], [19], [20], [22]]) \n\n\nLR=LinearRegression()\nLR.fit(x,y)\nb=LR.predict(np.array([[11]]))\n\nprint(round(b[0][0],4))","7368dea4":"import numpy as np;\nfrom matplotlib import pyplot as plt;\n\ndef cost(z,theta,y):\n    m,n=z.shape;\n    htheta = z.dot(theta.transpose())\n    cost = ((htheta - y)**2).sum()\/(2.0 * m);\n    return cost;\n\ndef gradient_descent(z,theta,alpha,y,itr):\n    cost_arr=[]\n    m,n=z.shape;\n    count=0;\n    htheta = z.dot(theta.transpose())\n    while count<itr:\n        htheta = z.dot(theta.transpose())\n        a=(alpha\/m)\n        temp0=theta[0,0]-a*(htheta-y).sum();\n        \n        temp1=theta[0,1]-a*((htheta-y)*(z[::,1:])).sum();\n        theta[0,0]=temp0;\n        theta[0,1]=temp1;\n        cost_arr.append(float(cost(z,theta,y)));\n        \n        count+=1;\n     \n    cost_log = np.array(cost_arr);\n    \n    plt.plot(np.linspace(0, itr, itr, endpoint=True), cost_log)\n    plt.xlabel(\"No. of iterations\")\n    plt.ylabel(\"Error Function value\")\n    plt.show()\n    \n    return theta;\n\nx = np.array([[0], [1],[2], [3], [4], [5], [6], [7], [8], [9]]) \ny = np.array([[11], [13], [12], [15], [17], [18], [18], [19], [20], [22]]) \n\nm,n=x.shape;\n\nz=np.ones((m,n+1),dtype=int);\n\nz[::,1:]=x;\n\n\ntheta=np.array([[21,2]],dtype=float)\n\ntheta_minimised=gradient_descent(z,theta,0.01,y,10000)\nnew_x=np.array([1,11])\n\n\npredicted_y=new_x.dot(theta_minimised.transpose())\n\nprint(round(predicted_y[0],4));","145d68d5":"import numpy as np;\n\nx= np.array([[0], [1],[2], [3], [4], [5], [6], [7], [8], [9]]) \ny= np.array([[11], [13], [12], [15], [17], [18], [18], [19], [20], [22]]) \nm,n=x.shape;\n\nz=np.ones((m,n+1),dtype=int);\n\nz[:,1:]=x;\n\nmat=np.matmul(z.transpose(),z);\nmatinv=np.linalg.inv(mat)\nval=np.matmul(matinv,z.transpose())\ntheta=np.matmul(val,y)\n\nnew_x=np.array([1,11]);\npredicted_y=new_x.dot(theta);\n\nprint(round(predicted_y[0],4));","12d2f2bb":"# Program 2\n## Linear Regression using scikit-learn ","02587234":"# Program 4\n\n## Linear Regression using Pseudo Inverse Method","c8738160":"# Program 1\n\n## Linear Regression Using Python\n\n","99a02293":"# Program 3\n\n## Linear Regression using Gradient Descent"}}