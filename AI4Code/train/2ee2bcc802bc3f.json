{"cell_type":{"251bb2ce":"code","8f1d1776":"code","ea4afa2b":"code","d0277c2e":"code","7c7edfab":"code","9f196338":"code","96c12f31":"code","7f844dc6":"code","fbbe472b":"code","3bf4f02b":"code","8b18e061":"code","b9c19d36":"code","3252beea":"code","a84b791b":"code","9c081a14":"code","17779fc7":"code","16237ce1":"code","39deab94":"code","5ea9c501":"code","d8d58c35":"code","78f7eed5":"code","4036e08e":"code","44c2fe8d":"code","daa8c009":"code","3f3176e0":"code","006b4559":"code","c62a894f":"code","2fb04d03":"code","f4c31c6e":"code","fb5b9f13":"code","bd2ee391":"code","95596f1b":"code","c41bfe7e":"code","05949be2":"code","eccd4f3d":"code","dfdd560b":"code","4a834cef":"code","62f0681b":"code","15b68fe6":"code","f855792f":"code","5eb71e5d":"code","6edb8be4":"code","ab3794fc":"code","7b0d7dcb":"code","3428ad40":"code","55e66d7d":"code","ae922a0d":"code","93d70ea8":"code","df6ac33f":"code","fed9766c":"code","d9a438ee":"markdown","8baa4678":"markdown","f8c5011b":"markdown","7878b6fa":"markdown","daf76411":"markdown","6a9299ee":"markdown","bc03c699":"markdown","aebb4b52":"markdown","e0ec7e03":"markdown","9ca4fde3":"markdown","d70ebadb":"markdown","1ad1befa":"markdown","a2a4d556":"markdown","f6a60fea":"markdown","8fac4bfd":"markdown","dd21cebc":"markdown","4b393a0d":"markdown","ee2d78bc":"markdown","9078c032":"markdown","5b5335cb":"markdown","c8fc0195":"markdown","1c917b60":"markdown","cadb7b9a":"markdown","2ab0ff94":"markdown","166646fb":"markdown","90d5846b":"markdown","c5c4b6b9":"markdown","554c478a":"markdown","bee5ea18":"markdown","cb889809":"markdown","f954ea10":"markdown","6ef81a0f":"markdown","7bbf9102":"markdown","71393c96":"markdown","3d755f2e":"markdown","ea6a3212":"markdown","8413fe5d":"markdown"},"source":{"251bb2ce":"import numpy as np \nimport pandas as pd \nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import randint\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.pipeline import Pipeline\nfrom copy import deepcopy\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn.svm import LinearSVC\nfrom sklearn import preprocessing\nfrom sklearn.svm import LinearSVC\nimport pandas as pd\nfrom scipy import stats\nfrom statsmodels.stats import weightstats as stests\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8f1d1776":"train= pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","ea4afa2b":"train.head()","d0277c2e":"print(\"The number of traning examples(data points) = %i \" % train.shape[0])\nprint(\"The number of features = %i \" % train.shape[1])","7c7edfab":"train.info()","9f196338":"train.describe()","96c12f31":"train.isnull().sum()","7f844dc6":"print(\"% of Cabin null values: \", (train['Cabin'].isnull().sum()\/train.shape[0])*100)\nprint(\"% of Age null values: \",(train['Age'].isnull().sum()\/train.shape[0])*100)","fbbe472b":"train= train.drop(['Cabin'], axis=1)","3bf4f02b":"train['Age'].fillna((train['Age'].mean()), inplace=True)","8b18e061":"duplicate = train[train.duplicated()]\nduplicate.size","b9c19d36":"train.drop('PassengerId', inplace=True, axis=1)","3252beea":"train['Age'].value_counts(bins=6, sort=False)","a84b791b":"train['Age'].fillna((train['Age'].mean()), inplace=True) # fill the columns that do with nan values with mean age number \n\nbins = [0, 13, 26,40,53,66,80]\nnames = ['<13', '13-26', '26-40', '40-53', '53-66','66-80']\n\ntrain['AgeCategory'] = pd.cut(train['Age'], bins, labels=names)\n\nNumberedAgeCategories = {'<13':0 , '13-26':1, '26-40':2, '40-53':3, '53-66':4,'66-80':5 }\ntrain['AgeCategory']=train['AgeCategory'].map(NumberedAgeCategories)  \ntrain['AgeCategory']=pd.to_numeric(train['AgeCategory'])\ntrain.head()","9c081a14":"train['FamilySize']= train['SibSp']+train['Parch']+1","17779fc7":"# let's make boxplots to visualise outliers in the continuous variables \n# Age and Fare\n \nplt.figure(figsize=(15,6))\nplt.subplot(1, 2, 1)\nfig = train.boxplot(column='Age')\nfig.set_title('')\nfig.set_ylabel('Age')\n \nplt.subplot(1, 2, 2)\nfig = train.boxplot(column='Fare')\nfig.set_title('')\nfig.set_ylabel('Fare')","16237ce1":"# Explore Age distribution \ng = sns.displot(train[\"Age\"], color=\"b\", label=\"Skewness : %.2f\"%(train[\"Age\"].skew()))\n","39deab94":"Upper_boundary = train.Age.mean() + 3* train.Age.std()\nLower_boundary = train.Age.mean() - 3* train.Age.std()\nprint('Age outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_boundary, upperboundary=Upper_boundary))","5ea9c501":"outliers = train[train['Age']>68]['Age']\noutliers","d8d58c35":"# Explore Fare distribution \ng = sns.displot(train[\"Fare\"], color=\"b\", label=\"Skewness : %.2f\"%(train[\"Fare\"].skew()))","78f7eed5":"train[\"Fare\"] = train[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","4036e08e":"g = sns.displot(train[\"Fare\"], color=\"b\", label=\"Skewness : %.2f\"%(train[\"Fare\"].skew()))","44c2fe8d":"train.columns[train.dtypes==object]","daa8c009":"cleanup_nums = { \"Embarked\": {\"S\": 0, \"C\": 1, \"Q\": 2 },\"Sex\": {\"male\": 0, \"female\": 1}}","3f3176e0":"train.replace(cleanup_nums, inplace=True)\ntrain.head()","006b4559":"train['Ticket'].unique().size","c62a894f":"train['Name']","2fb04d03":"train['Family_name']=train['Name'].str.split(', ').str[0]\n\ntrain['Title']=train['Name'].str.split(', ').str[1].str.split('.').str[0]","f4c31c6e":"train['Title'].unique()","fb5b9f13":"train['Title'] =train['Title'].replace(['Ms','Mlle'], 'Miss')\ntrain['Title'] = train['Title'].replace(['Mme','Dona','the Countess','Lady'], 'Mrs')\ntrain['Title'] =train['Title'].replace(['Rev','Mlle','Jonkheer','Dr','Capt','Don','Col','Major','Sir'], 'Mr')","bd2ee391":"cleanup_nums = { \"Title\": {\"Mr\": 0, \"Mrs\": 1, \"Miss\": 2, \"Master\": 3 } }\ntrain.replace(cleanup_nums, inplace=True)\n","95596f1b":"train.corr()\n# here is a table that shows the correlation between each feature with the rest of the features.","c41bfe7e":"corr = train.corr()\nf, ax = plt.subplots(figsize=(20, 8))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr,linewidths=.5, annot= True)","05949be2":"train.corr()['Survived'] \n# we will hone on the correlation values between the target feature since that's what we are investigating. ","eccd4f3d":"all_corr=train.corr()['Survived'].abs() >= .2\nhighly_correlated =all_corr[all_corr==True].index\n# Only the features that have a correlation value of 0.2 or above with the target will be selected. \nhighly_correlated= highly_correlated.tolist() \nhighly_correlated.remove('Survived')\nprint('The number of features removed out of', all_corr.size, 'is', all_corr.size- len(highly_correlated),', leaving',len(highly_correlated),'selected features.')\nhighly_correlated","dfdd560b":"sns.pairplot(data=train[['Pclass', 'Sex', 'Fare', 'Title', 'Survived']],\n             hue=\"Survived\", dropna=True)","4a834cef":"#label encoder can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.\nlabel_encoder = LabelEncoder()\nfor col in train.columns[train.dtypes == \"object\"]:\n    train[col] = label_encoder.fit_transform(train[col].astype('str'))\n\nfor col in test.columns[test.dtypes == \"object\"]:\n    test[col] = label_encoder.fit_transform(test[col].astype('str'))\n\n# drop rows with null values    \ntrain.dropna(inplace=True)\n\nX = train.drop('Survived', axis=1)\n\n# create our response variable\ny = train['Survived']","62f0681b":"# keep only the best 4 features according to p-values of ANOVA test\nk_best = SelectKBest(f_classif, k=4)","15b68fe6":"y= train['Survived']\nX= train.drop('Survived',axis='columns')\n# fit the data and then tranform it.\nk_best.fit_transform(X, y)\nk_best","f855792f":"# get the p values of columns\nk_best.pvalues_\n# make a dataframe of features and p-values\n# sort that dataframe by p-value\np_values = pd.DataFrame({'column': X.columns, 'p_value': k_best.pvalues_}).sort_values('p_value')\n# show the top 4 features\np_values.head(4)","5eb71e5d":"import random\nmale = train[train['Sex'] == 1]\nfemale = train[train['Sex'] == 0]\n\n## empty list for storing mean sample\nm_mean_samples = []\nf_mean_samples = []\n\nfor i in range(50):\n    m_mean_samples.append(np.mean(random.sample(list(male['Survived']),50,)))\n    f_mean_samples.append(np.mean(random.sample(list(female['Survived']),50,)))","6edb8be4":"ttest,pval = stats.ttest_rel(m_mean_samples, f_mean_samples)\nprint(float(pval))\nif pval<0.05:\n    print(\"reject null hypothesis\")\nelse:\n    print(\"accept null hypothesis\")","ab3794fc":"ztest ,pval1 = stests.ztest(m_mean_samples,f_mean_samples, value=0,alternative='two-sided')\nif pval1<0.05:\n    print(\"reject null hypothesis\")\nelse:\n    print(\"accept null hypothesis\")","7b0d7dcb":"# Plot\nsns.barplot(x=train['Sex'],y=train['Survived']);","3428ad40":"\ngrps = pd.unique(train.Pclass.values)\np1 = train[(train['Pclass']==1)]\np2 = train[(train['Pclass']==2)]\np3 = train[(train['Pclass']==3)]\n\np1_mean_samples=[]\np2_mean_samples=[]\np3_mean_samples=[]\n\nfor i in range(50):\n    p1_mean_samples.append(np.mean(random.sample(list(p1['Survived']),50,)))\n    p2_mean_samples.append(np.mean(random.sample(list(p2['Survived']),50,)))\n    p3_mean_samples.append(np.mean(random.sample(list(p3['Survived']),50,)))\n                          \nF, p = stats.f_oneway(p1_mean_samples, p2_mean_samples, p3_mean_samples)\nprint(\"p-value for significance is: \", p)\nif p<0.05:\n    print(\"reject null hypothesis\")\nelse:\n    print(\"accept null hypothesis\")","55e66d7d":"# Plot\nsns.barplot(x=train['Pclass'],y=train['Survived']);","ae922a0d":"headers = train.columns ","93d70ea8":"train.head()\n","df6ac33f":"# Feature Scaling\n## We will be using standardscaler to transform\nfrom sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\n\n## transforming \"train_x\"\ntrain = std_scale.fit_transform(train)","fed9766c":"train=pd.DataFrame(train, columns=headers)\ntrain.head()","d9a438ee":"# Scaling","8baa4678":"For now, we will just remove the columns that are clearly not affecting the target column, 'Survived' , which in this case is 'PassengerID'","f8c5011b":"# Hypothesis testing","7878b6fa":"If we choose 6 classes, we would be going with intervals of 13. ","daf76411":"# **Checking for null values:**","6a9299ee":"# Pearson's correlation","bc03c699":"# Remove irrelevant columns","aebb4b52":"Theoretically, 25 to 30% is the maximum missing values are allowed, beyond which we might want to drop the variable from analysis. ","e0ec7e03":"# Examining the dataset: ","9ca4fde3":"11 features not including the passenger_ID\n","d70ebadb":"It is a step of data Pre-Processing which is applied to independent variables to normalize the data within a particular range. It also helps in speeding up the calculations in an algorithm. Scaling is important in the algorithms such as support vector machines (SVM) and k-nearest neighbors (KNN) where distance between the data points is important.\n","1ad1befa":"![standardization-vs-normalization.png](attachment:14ec3b3b-e840-413c-adec-ba84d41346be.png)","a2a4d556":"these values seem reasonable for the 'Age' feature, so we will leave them as they are. ","f6a60fea":"We still have columns that are not of numerical type so we need to change them before scaling\/ running it on a model.","8fac4bfd":"We want to compare more than two groups at the same time, hence z-tests will not be suitable in this case. The analysis of variance or ANOVA is a statistical inference test that lets you compare multiple groups at the same time.\n<br>\nANNOVA: Analysis of variance is a collection of statistical models and their associated estimation procedures used to analyze the differences among group means in a sample","dd21cebc":"# Predicting Family Size ","4b393a0d":"# Preparing data for the model ","ee2d78bc":"We can use two-sampled ttests or two-sampled z-tests for this case, but since we chose our sample size to be greater than 30, z-tests would be preferrable.","9078c032":"**Null Hypothesis( H0 )**: There is no difference in the survival rate among passenger of different Pclasses. <br>\n**Alternative Hypothesis( HA ):** There is a difference in the survival rate among passengers of different Pclasses.","5b5335cb":"# Checking for duplicates","c8fc0195":"Add the number of siblings\/spouses to the parch (parch is also part of the family).","1c917b60":"![download.png](attachment:40a151b2-6de2-4d69-bbb4-b9f3d93773e2.png)","cadb7b9a":"# Feature Selection","2ab0ff94":"**Null Hypothesis( H0 ):** There is no difference in the survival rate between passengers females and males. <br>\n**Alternative Hypothesis( HA )**: There is a difference in the survival rate between passengers females and males.","166646fb":"About 60% of the people travelling in the first class survived, while only around 25% of the people travelling in the third class survived. Accordingly, this plot suggests that the class in which people travel affects the chances of survival.","90d5846b":"replace age null values with mean values","c5c4b6b9":"This graph illustrates the difference in survival rate among males and females:","554c478a":"# Binning","bee5ea18":"# Possible questions: \n- Is there a significant difference in the survival rate among people of different Pclasses? \n- Is there a significant difference in the survival rate between males and females? ","cb889809":"We use repeated sampling from the same units. The test measures whether the average score differs significantly across samples. If we observe a large p-value, for example greater than 0.05 then we cannot reject the null hypothesis of identical survival rate. If the p-value is smaller than the threshold then we reject the null hypothesis of equal averages.","f954ea10":"We can divide the age column into categories.","6ef81a0f":"Extracting information from 'Name' column:","7bbf9102":"We can note that >25% of Cabin Data is null, hence it might be reasonable to just drop it completely. ","71393c96":"# Outliers","3d755f2e":"As we can see, Fare distribution is very skewed. This can lead to overweighing very high values in the model, even if it is scaled.\n\nIn this case, it is better to transform it with the log function to reduce this skew.","ea6a3212":"Age has a normal distribution while the fare feature has skewed distribution.","8413fe5d":"![1_yR54MSI1jjnf2QeGtt57PA.png](attachment:cd81c2f7-6d07-4342-8d0f-82a077ee7f04.png)"}}