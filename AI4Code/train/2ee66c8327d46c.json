{"cell_type":{"095a78f0":"code","f825d797":"code","a8921515":"code","13b108d6":"code","b3775e05":"code","ce043887":"code","9f84e110":"code","b400d4e8":"code","0ea3dc1d":"code","2fce2979":"code","b39e127a":"code","5a047084":"code","c1061c0c":"code","4fa007eb":"code","26317a64":"code","f1cd0786":"code","10b0d48e":"code","33b7510e":"code","bcdfe6ce":"code","d6b90493":"code","ef138f4d":"code","631cab4c":"code","6a62f7e5":"code","9949578e":"code","5e30f3f2":"code","72d3f33f":"code","42195943":"code","ff2f0c7b":"code","32f90e94":"code","82027af6":"code","6e7e70ab":"code","7fd47a36":"code","ba36b8e1":"code","70cc34a1":"code","fb35c4d8":"code","68494f71":"code","147b61bd":"code","fc968d18":"code","231c2897":"code","cb920e86":"code","1908adef":"code","ebec11ae":"markdown","e58b4911":"markdown","148002f0":"markdown","932d1bc8":"markdown","d1a8050e":"markdown","6ae5dc75":"markdown","f86cf852":"markdown"},"source":{"095a78f0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimagefiles=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        imagefiles.append(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f825d797":"import tensorflow as tf\nimport tensorflow_hub as hub\nprint(tf.__version__)\nprint(hub.__version__)\n\nprint(\"GPU Available\" if tf.config.list_physical_devices('GPU') else \"Not Available\")","a8921515":"imagefiles[:5]","13b108d6":"from IPython.display import Image\nImage(imagefiles[0])","b3775e05":"labels=[]\nfor i in imagefiles:\n    if '\/other\/' in i:\n        labels.append(0);\n    else:\n        labels.append(1);","ce043887":"labels[:5]","9f84e110":"df=pd.DataFrame({\n    'Image':imagefiles,\n    'label':labels\n})\ndf.head()","b400d4e8":"df.info()","0ea3dc1d":"df['label'].value_counts().plot.bar()","2fce2979":"df['label'].value_counts()","b39e127a":"len(df)","5a047084":"df=df.sample(frac=1).reset_index(drop=True)\ndf.head()","c1061c0c":"X=np.array(df['Image'])\ny=np.array(df['label'])","4fa007eb":"len(X),len(y),type(X),type(y)","26317a64":"from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2,random_state=42)","f1cd0786":"X_train[:5]","10b0d48e":"y_train[:5]","33b7510e":"from matplotlib.pyplot import imread\nimage=imread(imagefiles[42])\nimage.shape","bcdfe6ce":"image","d6b90493":"image.min(),image.max()","ef138f4d":"IMG_SIZE=224\n\ndef process_image(image_path,img_size=IMG_SIZE):\n    '''\n    Takes an image file path and turns an image into a tensor\n    '''\n    image=tf.io.read_file(image_path)\n    image=tf.image.decode_jpeg(image,channels=3)\n    image=tf.image.convert_image_dtype(image,tf.float32)\n    image=tf.image.resize(image,size=[img_size,img_size])\n    \n    return image","631cab4c":"process_image(imagefiles[42])","6a62f7e5":"def get_image_label(image_path,label):\n    '''\n    Takes an image file path name and label and processes the image\n    '''\n    return process_image(image_path),label","9949578e":"process_image(X[42]),tf.constant(y[42])","5e30f3f2":"BATCH_SIZE=32\n\ndef create_data_batches(X,y=None,batch_size=BATCH_SIZE,valid_data=False):\n    '''\n    Creates batches of data of image X and label y pairs,\n    Shuffles in training set but not else\n    '''\n    \n    if valid_data:\n        print('Creating Validation data batches...')\n        data=tf.data.Dataset.from_tensor_slices((tf.constant(X),\n                                                tf.constant(y)))\n        data_batch=data.map(get_image_label).batch(BATCH_SIZE)\n        \n        return data_batch\n    else:\n        print('Creating training data batches...')\n        data=tf.data.Dataset.from_tensor_slices((tf.constant(X),\n                                                tf.constant(y)))\n        data=data.shuffle(buffer_size=len(X))\n        data_batch=data.map(get_image_label).batch(BATCH_SIZE)\n    return data_batch","72d3f33f":"train_data=create_data_batches(X_train,y_train)\nval_data=create_data_batches(X_val,y_val,valid_data=True)","42195943":"train_data.element_spec","ff2f0c7b":"val_data.element_spec","32f90e94":"INPUT_SHAPE=[None,224,224,3]\n\nOUTPUT_SHAPE=1\n\nMODEL_URL='https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v2_130_224\/classification\/4'","82027af6":"def create_model(imput_shape=INPUT_SHAPE,output_shape=OUTPUT_SHAPE,model=MODEL_URL):\n    model=tf.keras.Sequential([\n        hub.KerasLayer(MODEL_URL),\n        tf.keras.layers.Dense(units=OUTPUT_SHAPE,\n                             activation='sigmoid')\n    ])\n    \n    model.compile(\n        loss=tf.keras.losses.BinaryCrossentropy(),\n        optimizer=tf.keras.optimizers.Adam(),\n        metrics=['accuracy']\n    )\n    \n    model.build(INPUT_SHAPE)\n    \n    return model","6e7e70ab":"model=create_model()\nmodel.summary()","7fd47a36":"early_stopping=tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n                                               patience=3)","ba36b8e1":"NUM_EPOCHS=50","70cc34a1":"def train_model():\n    model=create_model()\n    model.fit(\n        x=train_data,\n        epochs=NUM_EPOCHS,\n        validation_data=val_data,\n        validation_freq=1,\n        callbacks=[early_stopping]\n    )\n    return model","fb35c4d8":"model=train_model()","68494f71":"predictions=model.predict(val_data,verbose=1)\npredictions[:5]","147b61bd":"len(predictions)","fc968d18":"def unbatchify(data):\n    images=[]\n    labels=[]\n    \n    for image,label in data.unbatch().as_numpy_iterator():\n        images.append(image)\n        labels.append(label)\n    return images,labels\n\nval_images,val_labels=unbatchify(val_data)\nval_images[0],val_labels[0]","231c2897":"import matplotlib.pyplot as plt","cb920e86":"def plot_pred(predictions,labels,images,n):\n    plt.imshow(images[n])\n    plt.xticks([])\n    plt.yticks([])\n    \n    pred_label=1 if predictions[n]>0.5 else 0\n    true_label=labels[n]\n    pred_prob=np.squeeze(predictions[n])\n    \n    pred_label= 'Dandelion' if pred_label==1 else 'Other'\n    true_label= 'Dandelion' if true_label==1 else 'Other'\n    \n    if pred_label==true_label:\n        color='green'\n    else:\n        color='red'\n    \n    plt.title(f'{pred_label} {(pred_prob*100):.2f}% {true_label}',color=color)","1908adef":"plot_pred(predictions,val_labels,val_images,n=130)","ebec11ae":"### Early Stopping","e58b4911":"## Predictions on Validation Data","148002f0":"## Turning data into Batches","932d1bc8":"# End to End Dandelion Detector\n\n## Problem\n\nGiven grass images identify if its dandelion or not\n\n## Data\n\nTaken from Kaggle,Contains images in 2 folders\n* dandelion - Does Have dandelion images\n* other - Does not have dandelion\n\n## Evaluation \n\nCross Entropy loss is found on the test data\n\n## Features\n\n* We are dealing with images so its best to use deep learning\/transfer learning\n* Binary Classification","d1a8050e":"## Creating Callbacks","6ae5dc75":"## Building a Model","f86cf852":"## Model and training is done and results are shown on the validation data(not used in training)"}}