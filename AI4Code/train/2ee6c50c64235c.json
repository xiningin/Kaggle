{"cell_type":{"0cf0f499":"code","85837ea0":"code","9f280de9":"code","dda6e76e":"code","dde4693e":"code","21954b44":"code","1a80f5dc":"code","e281c4e1":"code","6092fea8":"code","2d15b0bd":"code","31f7b3d4":"code","0483dff9":"code","8f215088":"code","ac3e01ae":"code","765aeffd":"code","3ec7634d":"code","b1cd76df":"code","7af18729":"code","37c8b617":"code","7c3ff8f5":"code","049527cc":"code","01881687":"markdown","9e943382":"markdown","696eb150":"markdown","0a536461":"markdown","a69704a3":"markdown","e009fbca":"markdown","f8f0128c":"markdown","c69b7c18":"markdown","6a389cf4":"markdown","c61779a7":"markdown","ae4e9b25":"markdown","1f25188d":"markdown"},"source":{"0cf0f499":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime as dt\nimport seaborn as sns\nfrom scipy.stats import mstats\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.stats.stattools import durbin_watson, jarque_bera\nfrom statsmodels.stats.diagnostic import het_breuschpagan\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set_style('whitegrid')","85837ea0":"sales = pd.read_csv('..\/input\/retail-analysis-with-walmart-sales-data\/WALMART_SALES_DATA.csv')","9f280de9":"sales.info()","dda6e76e":"sales.head()","dde4693e":"plt.figure(figsize = (20,7))\nbarplot = sns.barplot(x = 'Store',\n           y = 'Weekly_Sales',\n           data = sales,\n           estimator = np.sum,\n           ci = None,\n           order = sales.groupby('Store').agg('sum').reset_index().sort_values(by = 'Weekly_Sales', ascending = False)['Store']).set_title('Total Sales By Store')\nplt.ylabel('Sales (millions)')\nplt.show()","21954b44":"std_sales = sales.groupby('Store').agg('std')['Weekly_Sales'].reset_index()\nstd_sales.rename(columns = {'Weekly_Sales':'Sales Standev'}, inplace = True)\nmean_sales = sales.groupby('Store').agg('mean')['Weekly_Sales'].reset_index()\nmean_sales.rename(columns = {'Weekly_Sales': 'Mean Sales'}, inplace = True)\n#Calculating coefficient of variation.\nstd_sales['CV%'] = (std_sales['Sales Standev'] \/ mean_sales['Mean Sales'])*100\nstd_sales['CV%'] = round(std_sales['CV%'], 2)\nstd_sales.sort_values(by = 'Sales Standev',ascending = False).head()","1a80f5dc":"# Q3 starts from July 1st to September 30th.\nsales['Date'] = pd.to_datetime(sales['Date'], dayfirst = True)\nQ3_2012 = sales[(sales['Date'] >= '2012-07-01') & (sales['Date'] <= '2012-09-30')]\nsorted_Q3 = Q3_2012.sort_values(by = ['Store','Date'])\n\n#Growth rate formula (Ending Value - Starting Value) \/ Starting Value x 100\nstart = sorted_Q3[sorted_Q3['Date'] == sorted_Q3['Date'].min()].reset_index()[['Store','Weekly_Sales']]\nstart.rename(columns = {'Weekly_Sales':'start_value'}, inplace = True)\nend = sorted_Q3[sorted_Q3['Date'] == sorted_Q3['Date'].max()].reset_index()[['Store','Weekly_Sales']]\nend.rename(columns = {'Weekly_Sales':'end_value'}, inplace = True)\n\n# Top 5\ngrowth = start.merge(end, on = 'Store')\ngrowth['Growth%'] = round(((growth['end_value'] - growth['start_value'])\/growth['start_value'])*100,2)\ngrowth.sort_values(by = 'Growth%', ascending = False).head()","e281c4e1":"super_bowl = sales[sales['Date'].isin(['2010-02-12','2011-02-11','2012-02-10'])]\nlabour_day = sales[sales['Date'].isin(['2010-09-10','2011-09-09','2012-09-07'])]\nthanksgiving = sales[sales['Date'].isin(['2010-11-26','2011-11-25','2012-11-23'])]\nchristmas = sales[sales['Date'].isin(['2010-12-31','2011-12-30','2012-12-28'])]\nno_holiday = sales[sales['Holiday_Flag'] == 0]\n\ny = [super_bowl['Weekly_Sales'].mean(),\n    labour_day['Weekly_Sales'].mean(),\n    thanksgiving['Weekly_Sales'].mean(),\n    christmas['Weekly_Sales'].mean(),\n    no_holiday['Weekly_Sales'].mean()]\nx = ['Super Bowl',\n    'Labour Day',\n    'Thanksgiving',\n    'Christmas',\n    'No Holiday']\n\nplt.figure(figsize = (20,7))\nbarplot = sns.barplot(x = x,\n           y = y,\n           ci = None,\n            palette = 'pastel')\n\nbarplot.bar_label(barplot.containers[0])\nplt.show()","6092fea8":"sales1 = sales.copy()\nsales1['year'] = sales1['Date'].dt.year\nsales1['month'] = sales1['Date'].dt.month\nsales1['year_month'] = list(zip(sales1['month'],sales1['year']))\n\ndef semester(row):\n    if row in [(2,2010),(3,2010),(4,2010),(5,2010),(6,2010)]:\n        return 1\n    elif row in [(7,2010),(8,2010),(9,2010),(10,2010),(11,2010),(12,2010)]:\n        return 2\n    elif row in [(2,2011),(3,2011),(4,2011),(5,2011),(6,2011)]:\n        return 3\n    elif row in [(7,2011),(8,2011),(9,2011),(10,2011),(11,2011),(12,2011)]:\n        return 4\n    elif row in [(2,2012),(3,2012),(4,2012),(5,2012),(6,2012)]:\n        return 5\n    else:\n        return 6\n    \nsales1['semester'] = sales1['year_month'].apply(lambda x: semester(x))\n   \nfig, ax = plt.subplots(1,2, figsize = (20,6))\n\nsns.lineplot(x = 'month', y = 'Weekly_Sales',\n             hue = 'year',\n            data = sales1,\n            ci = None,\n            estimator = np.sum,\n             palette = 'pastel',\n            ax = ax[0]).set_title('Monthly Sales By Year')\nsns.lineplot(x = 'semester', y = 'Weekly_Sales',\n            data = sales1,\n            ci = None,\n            estimator = np.sum,\n            ax = ax[1]).set_title('Sales By Semester')\n\nplt.show()","2d15b0bd":"#converting each date to number of days since the 1st day reported on this dataset.\nstore1 = sales[sales['Store'] == 1].sort_values(by = 'Date', ascending = True)\n\ndef date_to_days(df):\n    days = []\n    for i in df:\n        convert = (i - df[0]).days\n        days.append(convert)\n    days[0] = 1\n    return days\nstore1['days'] = date_to_days(store1['Date'])","31f7b3d4":"fig, ax = plt.subplots(2,3, figsize = (20,7))\n\ncolumns = list(store1.drop(['Store','Holiday_Flag','Date'], axis = 1).columns)\nfor i, col in enumerate(store1[columns]):\n    sns.boxplot(x = col,\n                data = store1,\n                palette = 'pastel',\n                   ax = ax[i\/\/3, i%3])\n\nplt.show()","0483dff9":"fig, ax = plt.subplots(2,3, figsize = (20,15))\n\nfor i, col in enumerate(store1[columns]):\n    sns.histplot(x = col,\n                   data = store1,\n                   ax = ax[i\/\/3, i%3])\n\nplt.show()","8f215088":"fig, ax = plt.subplots(2,3, figsize = (20,15))\nreg_columns = columns = list(store1.drop(['Store','Weekly_Sales','Holiday_Flag','Date'], axis = 1).columns)\nfor i, col in enumerate(store1[reg_columns]):\n    sns.regplot(x = store1[col], y = store1['Weekly_Sales'],\n                ci = None,\n                robust = True,\n                line_kws = {'color':'red','label':'ROBUST'},\n                ax = ax[i\/\/3, i%3])\n    sns.regplot(x = store1[col], y = store1['Weekly_Sales'],\n                ci = None,\n                line_kws = {'color':'black', 'label':'OLS'},\n                ax = ax[i\/\/3, i%3])\n    ax[i\/\/3,i%3].legend()\n\nax[1,2].set_visible(False)\n\nplt.show()","ac3e01ae":"x = store1[store1.drop(['Weekly_Sales','Date','Store'], axis = 1).columns]\ny = store1['Weekly_Sales']\nx_constant = sm.add_constant(x)\n\nlm = sm.OLS(y, x_constant).fit()\n\nfig, ax = plt.subplots(1,3, figsize = (20,5))\nsns.histplot(lm.resid, ax = ax[0]).set_title('Residual Histogram')\nsm.qqplot(lm.resid,line = 'r', ax = ax[1])\nsns.residplot(lm.fittedvalues, lm.resid, ax = ax[2]).set_title('Residuals VS Predicted')\nplt.show()","765aeffd":"dw = durbin_watson(lm.resid)\n_,jbpval,_,_ =  jarque_bera(lm.resid)\n_,hppval,_,_ = het_breuschpagan(lm.resid, lm.model.exog)\n\nif dw > 1.5:\n    print('No autocorrelation.')\nelse:\n    print('Autoccorelation is present.')\nif jbpval < 0.05 and round(np.mean(lm.resid)) == 0:\n    print('Residuals are not completely normal, but mean of residuals is approximately zero.')\nelse:\n    print('Residuals are nornmal.')\nif hppval < 0.05:\n    print('Heteroskedasticity')\nelse:\n    print('Homoskedasticity')","3ec7634d":"vif_data = pd.DataFrame()\nvif_data[\"feature\"] = x.columns\n  \n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(x.values, i)\n                          for i in range(len(x.columns))]\n\n#VIF over 10 is problematic\nvif_data","b1cd76df":"# Centering technique to reduce multicollinearity.\ncols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment','days']\n\nfor col in cols:\n    x_constant[col] = x_constant[col] - np.mean(x_constant[col])","7af18729":"vif_data = pd.DataFrame()\nvif_data[\"feature\"] = x_constant.columns\n  \n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(x_constant.values, i)\n                          for i in range(len(x_constant.columns))]\n\n#VIF over 10 is problematic\nvif_data[1:]","37c8b617":"rlm_constant = x_constant.drop(['CPI','days'], axis = 1)\nrlm = sm.RLM(y,rlm_constant).fit()","7c3ff8f5":"# Diagnostic tests on robust linear model.\ndw = durbin_watson(rlm.resid)\n_,jbpval,_,_ =  jarque_bera(rlm.resid)\n_,hppval,_,_ = het_breuschpagan(rlm.resid, rlm.model.exog)\n\nif dw > 1.5:\n    print('No autocorrelation.')\nelse:\n    print('Autoccorelation is present.')\nif jbpval < 0.05 and round(np.mean(rlm.resid)) == 0:\n    print('Residuals are not completely normal, but mean of residuals is approximately zero.')\nelse:\n    print('Residuals are nornmal.')\nif hppval < 0.05:\n    print('Heteroskedasticity')\nelse:\n    print('Homoskedasticity')","049527cc":"rlm.summary()","01881687":"- Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n- Labour Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n- Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n- Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13","9e943382":"Based on this initial model, the assumption of no multicollinearity and homoskedasticity were not satisified. Residuals were deemed to be non-normal, but the mean of the residuals is approximately zero. Centering technique was applied to the indpendent variables to reduce multicollinearity. Multicollinearities were heavily reduced, but two independent variables `CPI` and `days` still had high VIF. Both variables will be dropped. Robust regression will be used over OLS due to outliers in the dependent variable and heteroskedasticity present.","696eb150":"# Some holidays have a negative impact on sales. Find out holidays which have higher sales than the mean sales in non-holiday season for all stores together","0a536461":"Despite the outliers in the dependent variable `Weekly_Sales`, the relationship with the independent variables seem to be fairly linear. OLS will be affected by the outliers so a comparison was made with robust method.","a69704a3":"2012 seems to be the year where it increased sales based on the underperforming months in 2010 and 2011. No data was provided for December for 2012. Weekly sales shoot up from November to December. Third and fifth semester underperformed. There seems to be a trend where sales increase every other semester.","e009fbca":"Independent variables do not particularly have any outliers. However, the dependent variable `Weekly_Sales` has many.","f8f0128c":"# Which store has maximum standard deviation i.e., the sales vary a lot. Also, find out the coefficient of mean to standard deviation","c69b7c18":"# For Store 1 \u2013 Build prediction models to forecast demand (Linear Regression \u2013 Utilize variables like date and restructure dates as 1 for 5 Feb 2010 (starting from the earliest date in order). Hypothesize if CPI, unemployment, and fuel price have any impact on sales.) Change dates into days by creating new variable.\n","6a389cf4":"The model shows that `Fuel_Price` is siginificant in explaining weekly sales, but `Unemployment` is not. `CPI` was dropped from the model because it is highly correlated with other variables. ","c61779a7":"#  Which store\/s has good quarterly growth rate in Q3\u20192012","ae4e9b25":"# Which store has maximum sales?","1f25188d":"# Provide a monthly and semester view of sales in units and give insights"}}