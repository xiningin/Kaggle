{"cell_type":{"0616e864":"code","c6b26705":"code","d7646020":"code","a3f619eb":"code","2e9e5f01":"code","bd206f7b":"code","f4772c4c":"code","86359c28":"code","5126b266":"code","afed8815":"code","4a155c10":"code","a7cc34ed":"code","b97ca90c":"code","df87465c":"code","eb4fcf65":"code","54bbc410":"code","d51dbbac":"code","4857cc74":"code","61c28f78":"markdown"},"source":{"0616e864":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6b26705":"data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndata_test = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","d7646020":"data.head()","a3f619eb":"pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\").head()","2e9e5f01":"data.columns","bd206f7b":"data.columns.shape","f4772c4c":"with pd.option_context('display.max_columns', None):\n    display(data.describe())","86359c28":"data.columns[data.isna().any()].tolist()","5126b266":"data_test.columns[data_test.isna().any()].tolist()","afed8815":"data.columns[data.isna().all()].tolist()","4a155c10":"drop_cols_train = data.columns[data.isna().any()].tolist()\ndrop_cols_test = data_test.columns[data_test.isna().any()].tolist()\ndrop_cols = list(set(drop_cols_train + drop_cols_test))\ndrop_cols","a7cc34ed":"data.select_dtypes(include='object').columns","b97ca90c":"for col in data.select_dtypes(include='object'):\n    unique_data = data[col].unique()\n    print(col)\n    print(unique_data)\n    print('count: '+ str(len(unique_data)))\n    print()","df87465c":"data = data.drop(columns=drop_cols)\ntarget = data['SalePrice']\nfeatures = data.drop('SalePrice', axis=1)\nnumerical = list(features.select_dtypes(exclude='object').columns)\n# normalize numerical data\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nfeatures[numerical] = scaler.fit_transform(features[numerical])\n# impute the numerical data\nfrom sklearn.impute import SimpleImputer\n\nsimple_imputer = SimpleImputer()\nfeatures[numerical] = simple_imputer.fit_transform(features[numerical])\nnon_numerical = list(features.select_dtypes(include='object').columns)\n\nfrom sklearn.preprocessing import LabelEncoder\nfor nn in non_numerical:\n    features[nn] = LabelEncoder().fit_transform(features[nn])","eb4fcf65":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(features, \n                                                    target, \n                                                    test_size = 0.2, \n                                                    random_state = 0)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test.shape[0]))","54bbc410":"from sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n\n# model = DecisionTreeRegressor(random_state=0)\n# model = MLPRegressor(hidden_layer_sizes=(256, 128,64,32,),random_state=1, max_iter=10000)\nmodel = GradientBoostingRegressor(random_state=42)\n# model = RandomForestRegressor(max_depth=5, random_state=30)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nnp.sqrt(mean_squared_log_error(y_test, y_pred))","d51dbbac":"features = data_test.drop(columns=drop_cols)\nnumerical = list(features.select_dtypes(exclude='object').columns)\n# normalize numerical data\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nfeatures[numerical] = scaler.fit_transform(features[numerical])\n# impute the numerical data\nfrom sklearn.impute import SimpleImputer\n\nsimple_imputer = SimpleImputer()\nfeatures[numerical] = simple_imputer.fit_transform(features[numerical])\nnon_numerical = list(features.select_dtypes(include='object').columns)\n\nfrom sklearn.preprocessing import LabelEncoder\nfor nn in non_numerical:\n    features[nn] = LabelEncoder().fit_transform(features[nn])\n    \ny_pred = model.predict(features)","4857cc74":"pred = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\npred['SalePrice'] = y_pred\npred.to_csv(\"\/kaggle\/working\/pred_1.csv\", index=False)","61c28f78":"with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print(data.dtypes)"}}