{"cell_type":{"33e484ea":"code","0e294875":"code","4888cf56":"code","9e8578ed":"code","47245b17":"code","2b72d1a0":"code","4dbff26b":"code","e924a16a":"code","e7468c30":"code","81f8cb4e":"code","af5a32c0":"code","14c76f18":"code","245f7b4c":"code","f63ccb1c":"code","5e4d0004":"code","0ebe1983":"markdown","c16ce19f":"markdown","64e44c6f":"markdown","045b7a14":"markdown","e8f2d5b3":"markdown","9a7704ff":"markdown","cb104261":"markdown","ae6cba5e":"markdown","b6971414":"markdown","a0c8f3a8":"markdown"},"source":{"33e484ea":"import numpy as np\nimport pandas as pd\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\n%matplotlib inline","0e294875":"%%time\ndata = pd.read_csv('..\/input\/santander-value-prediction-challenge\/train.csv')\ntarget = np.log1p(data['target'])\ndata.drop(['ID', 'target'], axis=1, inplace=True)","4888cf56":"%%time\nleak = pd.read_csv('..\/input\/breaking-lb-fresh-start-with-lag-selection\/train_leak.csv')\ndata['leak'] = leak['compiled_leak'].values\ndata['log_leak'] = np.log1p(leak['compiled_leak'].values)","9e8578ed":"%%time\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred) ** .5\n\nreg = XGBRegressor(n_estimators=1000)\nfolds = KFold(4, True, 134259)\nfold_idx = [(trn_, val_) for trn_, val_ in folds.split(data)]\nscores = []\n\nnb_values = data.nunique(dropna=False)\nnb_zeros = (data == 0).astype(np.uint8).sum(axis=0)\n\nfeatures = [f for f in data.columns if f not in ['log_leak', 'leak', 'target', 'ID']]\nfor _f in features:\n    score = 0\n    for trn_, val_ in fold_idx:\n        reg.fit(\n            data[['log_leak', _f]].iloc[trn_], target.iloc[trn_],\n            eval_set=[(data[['log_leak', _f]].iloc[val_], target.iloc[val_])],\n            eval_metric='rmse',\n            early_stopping_rounds=50,\n            verbose=False\n        )\n        score += rmse(target.iloc[val_], reg.predict(data[['log_leak', _f]].iloc[val_], ntree_limit=reg.best_ntree_limit)) \/ folds.n_splits\n    scores.append((_f, score))","47245b17":"report = pd.DataFrame(scores, columns=['feature', 'rmse']).set_index('feature')\nreport['nb_zeros'] = nb_zeros\nreport['nunique'] = nb_values\nreport.sort_values(by='rmse', ascending=True, inplace=True)","2b72d1a0":"plt.figure(figsize=(10, 7))\nplt.xlabel('Number of zeros in the feature', fontsize=14)\nplt.ylabel('Feature RMSE (on np.log1p)', fontsize=14)\nplt.title('Feature score vs number of zeros', fontsize=16, fontweight='bold', color='#ae3453')\nplt.scatter(report['nb_zeros'], report['rmse'])","4dbff26b":"fig, ax = plt.subplots(figsize=(10, 7))\nplt.xlabel('Number of unique values in the feature', fontsize=14)\nplt.ylabel('Feature RMSE (on np.log1p)', fontsize=14)\nax.set_title('Feature score vs number of unique values', fontsize=16, fontweight='bold', color='#ae3453')\nscatter = ax.scatter(report['nunique'], report['rmse'])","e924a16a":"from bokeh.plotting import figure, show, output_file, output_notebook, ColumnDataSource\n\nreport.sort_values('rmse', ascending=False, inplace=True)\n\nradii = 1000 * (report['rmse'].max() - report['rmse']).values\n\nsource = ColumnDataSource(data=dict(\n    x=report['nunique'].tolist(),\n    y=report['nb_zeros'].tolist(),\n    desc=report.index.tolist(),\n    radius=radii,\n    fill_color=[\n       \"#%02x%02x%02x\" % (int(r), 100, 150) for r in 255 * ((report['rmse'].max() - report['rmse']) \/ (report['rmse'].max() - report['rmse'].min())).values\n    ],\n    rmse=report['rmse'].tolist()\n))\n\nTOOLTIPS = [\n    (\"rmse\", \"@rmse\"),\n    (\"(nunique, nb_zeros)\", \"(@x, @y)\"),\n    (\"feature\", \"@desc\"),\n]\nTOOLS = \"hover, crosshair, pan, wheel_zoom, zoom_in, zoom_out, box_zoom, undo, redo, reset, tap, save, box_select, poly_select, lasso_select\"\n\np = figure(plot_width=600, plot_height=600, tooltips=TOOLTIPS, tools=TOOLS,\n           title=\"Number of unique values vs Number of zeros\")\np.xaxis.axis_label = 'Number of unique values in feature'\np.yaxis.axis_label = 'Number of zeros in feature'\np.xaxis.axis_label_text_font_style ='bold'\np.yaxis.axis_label_text_font_style ='bold'\np.title.text_color = '#ae3453'\np.title.text_font_size = '16pt'\np.scatter(\n    'x', 'y', source=source,\n    radius='radius',\n    fill_color='fill_color',\n    line_color=None,\n    fill_alpha=0.8\n)\n\noutput_notebook()\n\nshow(p)  # open a browser","e7468c30":"report.to_csv('feature_report.csv', index=True)","81f8cb4e":"good_features = report.loc[report['rmse'] <= 0.7925].index\nrmses = report.loc[report['rmse'] <= 0.7925, 'rmse'].values\ngood_features","af5a32c0":"test = pd.read_csv('..\/input\/santander-value-prediction-challenge\/test.csv')","14c76f18":"for i, f in enumerate(good_features):\n    plt.subplots(figsize=(10, 3))\n    plt.title('Feature %s RMSE %.3f train\/test distributions' % (f, rmses[i]), fontsize=16, fontweight='bold', color='#ae3453')\n    hists = plt.hist(np.log1p(data[f].replace(0, np.nan).dropna().values), alpha=.7, label='train', \n             bins=50, density=True,  histtype='bar')\n    plt.hist(np.log1p(test[f].replace(0, np.nan).dropna().values), alpha=.5, label='test', \n             bins=hists[1], density=True, histtype='bar')\n    plt.legend()","245f7b4c":"%%time\ntst_leak = pd.read_csv('..\/input\/breaking-lb-fresh-start-with-lag-selection\/test_leak.csv')\ntest['leak'] = tst_leak['compiled_leak']\ntest['log_leak'] = np.log1p(tst_leak['compiled_leak'])","f63ccb1c":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=1)\n\n# Use all features for stats\nfeatures = [f for f in data if f not in ['ID', 'leak', 'log_leak', 'target']]\ndata.replace(0, np.nan, inplace=True)\ndata['log_of_mean'] = np.log1p(data[features].replace(0, np.nan).mean(axis=1))\ndata['mean_of_log'] = np.log1p(data[features]).replace(0, np.nan).mean(axis=1)\ndata['log_of_median'] = np.log1p(data[features].replace(0, np.nan).median(axis=1))\ndata['nb_nans'] = data[features].isnull().sum(axis=1)\ndata['the_sum'] = np.log1p(data[features].sum(axis=1))\ndata['the_std'] = data[features].std(axis=1)\ndata['the_kur'] = data[features].kurtosis(axis=1)\n\ntest.replace(0, np.nan, inplace=True)\ntest['log_of_mean'] = np.log1p(test[features].replace(0, np.nan).mean(axis=1))\ntest['mean_of_log'] = np.log1p(test[features]).replace(0, np.nan).mean(axis=1)\ntest['log_of_median'] = np.log1p(test[features].replace(0, np.nan).median(axis=1))\ntest['nb_nans'] = test[features].isnull().sum(axis=1)\ntest['the_sum'] = np.log1p(test[features].sum(axis=1))\ntest['the_std'] = test[features].std(axis=1)\ntest['the_kur'] = test[features].kurtosis(axis=1)\n\n# Only use good features, log leak and stats for training\nfeatures = good_features.tolist()\nfeatures = features + ['log_leak', 'log_of_mean', 'mean_of_log', 'log_of_median', 'nb_nans', 'the_sum', 'the_std', 'the_kur']\ndtrain = lgb.Dataset(data=data[features], \n                     label=target, free_raw_data=False)\ntest['target'] = 0\n\ndtrain.construct()\noof_preds = np.zeros(data.shape[0])\n\nfor trn_idx, val_idx in folds.split(data):\n    lgb_params = {\n        'objective': 'regression',\n        'num_leaves': 58,\n        'subsample': 0.6143,\n        'colsample_bytree': 0.6453,\n        'min_split_gain': np.power(10, -2.5988),\n        'reg_alpha': np.power(10, -2.2887),\n        'reg_lambda': np.power(10, 1.7570),\n        'min_child_weight': np.power(10, -0.1477),\n        'verbose': -1,\n        'seed': 3,\n        'boosting_type': 'gbdt',\n        'max_depth': -1,\n        'learning_rate': 0.05,\n        'metric': 'l2',\n    }\n\n    clf = lgb.train(\n        params=lgb_params,\n        train_set=dtrain.subset(trn_idx),\n        valid_sets=dtrain.subset(val_idx),\n        num_boost_round=10000, \n        early_stopping_rounds=100,\n        verbose_eval=0\n    )\n\n    oof_preds[val_idx] = clf.predict(dtrain.data.iloc[val_idx])\n    test['target'] += clf.predict(test[features]) \/ folds.n_splits\n    print(mean_squared_error(target.iloc[val_idx], \n                             oof_preds[val_idx]) ** .5)\n\ndata['predictions'] = oof_preds\ndata.loc[data['leak'].notnull(), 'predictions'] = np.log1p(data.loc[data['leak'].notnull(), 'leak'])\nprint('OOF SCORE : %9.6f' \n      % (mean_squared_error(target, oof_preds) ** .5))\nprint('OOF SCORE with LEAK : %9.6f' \n      % (mean_squared_error(target, data['predictions']) ** .5))\n","5e4d0004":"test['target'] = np.expm1(test['target'])\ntest.loc[test['leak'].notnull(), 'target'] = test.loc[test['leak'].notnull(), 'leak']\ntest[['ID', 'target']].to_csv('leaky_submission.csv', index=False, float_format='%.2f')","0ebe1983":"### Add leak to test","c16ce19f":"### Train lightgbm","64e44c6f":"### Add train leak","045b7a14":"### Create dataframe","e8f2d5b3":"Due to leaks found in the past week, I wondered how it would modify the simple XGB scoring method demonstrated in this notebook.\n\nFor this purpose I use the results found in : https:\/\/www.kaggle.com\/johnfarrell\/breaking-lb-fresh-start-with-lag-selection\/output\n","9a7704ff":"### Display distributions of test and train for selected features","cb104261":"### Feature Scoring using XGBoost with the leak feature","ae6cba5e":"### Plot a few diagrams","b6971414":"### Select some features (threshold is not optimized)","a0c8f3a8":"### Save submission"}}