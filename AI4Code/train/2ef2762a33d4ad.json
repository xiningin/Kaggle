{"cell_type":{"a605363a":"code","9f8a2218":"code","e0f436b8":"code","8b496043":"code","61009a56":"code","7382f91d":"code","cab92fad":"code","d25f9ec1":"code","8d88c1d6":"code","d23e479a":"code","e6075558":"code","143e4204":"code","ce080f07":"code","4c8d618c":"code","ce30582a":"code","74c9dadd":"code","fcebe71b":"code","7a172571":"code","96e8e4a0":"code","b8aac85d":"code","6f21d346":"code","c1def7cb":"code","d89d1c1c":"code","af9473d2":"code","59113964":"code","ad8c4791":"code","6c67dbae":"code","3a8f8c0d":"code","3418837f":"code","0860871d":"code","bbfeb576":"code","19074648":"code","732f2999":"code","6c28b666":"code","980e325c":"code","59f439a9":"code","2c1854a8":"code","7f13e7cf":"code","72dc5b8b":"code","41233f10":"code","6bf5edef":"code","2f2bfe84":"code","19d65622":"code","c9dd8b76":"code","5dfea6e3":"code","26836e28":"code","add8ddc6":"code","9c69690b":"code","94bcce58":"code","ce4b6458":"markdown","2a1a1647":"markdown","bd2a03ff":"markdown","41b83326":"markdown","6e8bd422":"markdown","786b4909":"markdown","08f6f13e":"markdown"},"source":{"a605363a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","9f8a2218":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","e0f436b8":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","8b496043":"df = pd.concat((df_train,df_test),axis=0)","61009a56":"df.info()","7382f91d":"df.head()","cab92fad":"sns.scatterplot(x='Electrical',y='SalePrice',data=df)","d25f9ec1":"total = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\nmissing_data.head(30)","8d88c1d6":"drop_features = list(missing_data.index[missing_data.Percent >= .00136])","d23e479a":"drop_features.remove('SalePrice')","e6075558":"# drop columns with too many NaNs\ndf_cleaned = df.drop(columns=drop_features)","143e4204":"df_cleaned.Utilities.value_counts()","ce080f07":"missing_rows_idx = df_cleaned.drop('SalePrice',axis=1).isnull().any(axis=1)","4c8d618c":"# if there is no Bsmt values then most like there is no basement at all\ndf_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)] = df_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)].fillna({\"BsmtFinSF1\":0,\"BsmtFinSF2\":0,\"BsmtFullBath\":0,\"BsmtHalfBath\":0,\"BsmtUnfSF\":0,\"TotalBsmtSF\":0})\n# Basically all the Utilities are AllPub\ndf_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)] = df_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)].fillna({\"Utilities\":\"AllPub\"})\n# Garage most likely to be zero if missing in report...\ndf_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)] = df_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)].fillna({\"GarageArea\":0,\"GarageCars\":0})\n# Houses in the neighborhood Sawyer saletype are probably WD\ndf_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)] = df_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)].fillna({\"SaleType\":\"WD\"})\n# Just go with typical functional\ndf_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)] = df_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)].fillna({\"Functional\":\"Typ\"})\n# When the ExterCond and ExterQual are both TA, these are most likely values for that neighboord Edwards\ndf_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)] = df_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)].fillna({\"Exterior1st\":\"Wd Sdng\",\"Exterior2nd\":\"Wd Sdng\"})\n# Just go with typical \ndf_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)] = df_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)].fillna({\"KitchenQual\":\"TA\"})","ce30582a":"# Houses in the neighborhood Sawyer are probably WD...\ndf_cleaned[df_cleaned.loc[:,\"Neighborhood\"] == \"Sawyer\"].SaleType.value_counts()","74c9dadd":"df_cleaned.KitchenQual.value_counts()","fcebe71b":"df_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)]","7a172571":"df_cleaned.loc[np.logical_and(np.logical_and(df_cleaned.ExterCond==\"TA\",df_cleaned.ExterQual==\"TA\"),df_cleaned.Neighborhood==\"Edwards\")].Exterior1st.value_counts()\ndf_cleaned.loc[np.logical_and(df_cleaned.Exterior1st==\"Wd Sdng\",np.logical_and(np.logical_and(df_cleaned.ExterCond==\"TA\",df_cleaned.ExterQual==\"TA\"),df_cleaned.Neighborhood==\"Edwards\"))].Exterior2nd.value_counts()","96e8e4a0":"df_cleaned.loc[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)]","b8aac85d":"df_cleaned[np.logical_and(df_cleaned.Id>1460,missing_rows_idx)].info()","6f21d346":"# drop the few rows with NaNs\ndf_cleaned = df_cleaned.dropna(subset=[col for col in df_cleaned.columns if col != 'SalePrice'],how='any')","c1def7cb":"df_cleaned = pd.get_dummies(df_cleaned)","d89d1c1c":"# too many plots to do pairwise, so just do individual plts one at a time\nfor name in df.columns:\n    plt.figure\n    sns.scatterplot(x=name,y='SalePrice',data=df)\n    plt.xticks(rotation=90)\n    plt.show()","af9473d2":"plt.figure(figsize=(15,10));\nsns.heatmap(data=df_train.corr(),vmin=-1,vmax=1,linewidths=.3,cmap='jet',square=True);","59113964":"# check if pearson correlation is above .8\nplt.figure(figsize=(15,10));\nsns.heatmap(data=abs(df_train.corr())>.50,vmin=0,vmax=1,linewidths=.3,cmap='YlGnBu',square=True);","ad8c4791":"df_cleaned['TotalFlrSF'] = df_cleaned['1stFlrSF'] + df_cleaned['2ndFlrSF'] + df_cleaned['TotalBsmtSF']\ndf_cleaned['Total_Bathrooms'] = (df_cleaned['FullBath'] + (0.5*df_cleaned['HalfBath']) + \n                               df_cleaned['BsmtFullBath'] + (0.5*df_cleaned['BsmtHalfBath']))\n\ndf_cleaned['Total_porch_sf'] = (df_cleaned['OpenPorchSF'] + df_cleaned['3SsnPorch'] +\n                              df_cleaned['EnclosedPorch'] + df_cleaned['ScreenPorch'] +\n                             df_cleaned['WoodDeckSF'])","6c67dbae":"X_train_cleaned = df_cleaned[df_cleaned.Id <= 1460]\nX_test_cleaned = df_cleaned[df_cleaned.Id > 1460]","3a8f8c0d":"X_test_Ids = X_test_cleaned.pop('Id')\nX_test_cleaned.pop('SalePrice');\nX_train_Ids = X_train_cleaned.pop('Id')","3418837f":"y_train_cleaned = X_train_cleaned.pop('SalePrice')","0860871d":"from scipy import stats\nX_trained_cleaned = X_train_cleaned[(np.abs(stats.zscore(X_train_cleaned)) < 10).all(axis=1)]","bbfeb576":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_regression\nkbest = SelectKBest(mutual_info_regression,k=int(np.floor(len(X_train_cleaned.columns)\/4))).fit(X_train_cleaned,y_train_cleaned)\nkbest_idx = kbest.get_support(indices=True)","19074648":"X_train = X_train_cleaned.iloc[:,kbest_idx]\nX_test = X_test_cleaned.iloc[:,kbest_idx]","732f2999":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)","6c28b666":"X_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","980e325c":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor \n\nfrom sklearn.metrics import r2_score","59f439a9":"rff = RandomForestRegressor()","2c1854a8":"params = {\n    \"n_estimators\":[10,100,500,1000,3000],\n    \"max_depth\":[10,50,100,None],\n    \"min_samples_split\":[2,5,10],\n    \"max_features\":[\"auto\",\"sqrt\",\"log2\",None]\n}\ngs = RandomizedSearchCV(rff,param_distributions=params,n_iter=100,scoring='neg_mean_squared_error')","7f13e7cf":"gs.fit(X_train_scaled,y_train_cleaned)","72dc5b8b":"# Utility function to report best scores\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","41233f10":"report(gs.cv_results_)","6bf5edef":"y_pred_train = gs.best_estimator_.predict(X_train_scaled)\nsns.scatterplot(x=y_pred_train,y=y_train_cleaned)\nprint(\"R2: \",r2_score(y_train_cleaned,y_pred_train))","2f2bfe84":"from sklearn.ensemble import GradientBoostingRegressor","19d65622":"gbb = GradientBoostingRegressor()","c9dd8b76":"params = {\n    \"loss\":['ls','lad','huber','quantile'],\n    \"learning_rate\":[.1,.01,.005,.0005],\n    \"n_estimators\":[100,500,3000],\n    \"min_samples_split\":[2,5,10,20],\n    \"max_features\":[\"auto\",\"sqrt\",\"log2\",None]\n}\n\ngs2 = RandomizedSearchCV(gbb,param_distributions=params,n_iter=100,scoring='neg_mean_squared_error')\ngs2.fit(X_train_scaled,y_train_cleaned)","5dfea6e3":"report(gs2.cv_results_)","26836e28":"y_pred_train = gs2.best_estimator_.predict(X_train_scaled)\nsns.scatterplot(x=y_pred_train,y=y_train_cleaned)\nprint(\"R2: \",r2_score(y_train_cleaned,y_pred_train))","add8ddc6":"from sklearn.linear_model import Lasso\nparams = {\n    \"alpha\":[.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n}\nlasso = Lasso()\ngs3 = RandomizedSearchCV(lasso,param_distributions=params,n_iter=100,scoring='neg_mean_squared_error',cv=5)\ngs3.fit(X_train_scaled,y_train_cleaned)","9c69690b":"y_pred_train = gs3.best_estimator_.predict(X_train_scaled)\nsns.scatterplot(x=y_pred_train,y=y_train_cleaned)\nprint(\"R2: \",r2_score(y_train_cleaned,y_pred_train))","94bcce58":"y_pred =.2*gs.best_estimator_.predict(X_test_scaled) + .6*gs2.best_estimator_.predict(X_test_scaled) + .2 * gs3.best_estimator_.predict(X_test_scaled)\nsubmission = pd.DataFrame({\"Id\":X_test_Ids,\"SalePrice\":y_pred})\nsubmission.to_csv(\"submission1\",index=False)","ce4b6458":"## Select K best features","2a1a1647":"## Feature Engineering","bd2a03ff":"## Data Transformation and Preparation","41b83326":"#### Clean the testdata to have all 1459 rows to submit data","6e8bd422":"## Data Analysis","786b4909":"## Get best Model","08f6f13e":"## Split into train and test"}}