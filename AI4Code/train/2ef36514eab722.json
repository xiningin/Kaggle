{"cell_type":{"6a15d485":"code","19a2a64a":"code","a8f3fdd1":"code","af37efa0":"code","b9e8ac32":"code","0b3030bf":"code","fc1b40e1":"code","ef300f6a":"markdown","a46265b3":"markdown","466fdcea":"markdown"},"source":{"6a15d485":"import os\n\nimport albumentations as A\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt","19a2a64a":"KEYPOINT_COLOR = (0, 255, 0)\n\ndef vis_keypoints(image, keypoints, ax, color=KEYPOINT_COLOR, diameter=1):\n    image = image.copy()\n    for (x, y) in keypoints:\n        cv2.circle(image, (int(x), int(y)), diameter, (0, 255, 0), -1)\n    ax.imshow(image)","a8f3fdd1":"PATH = \"..\/input\/made-cv-2021-contest-01-facial-landmarks\/contest01_data\/train\/\"\n\ndf = pd.read_csv(os.path.join(PATH, \"landmarks.csv\"), sep='\\t', nrows=100)\nlandmarks = df.iloc[:, 1:].values.reshape((len(df), 971, 2))\nimage_names = df.iloc[:, 0].apply(lambda x: os.path.join(PATH, \"images\", x)).values","af37efa0":"idx = 0\n\nimage = cv2.imread(image_names[idx])\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nkeypoints = landmarks[idx]","b9e8ac32":"class FaceResize(A.Resize):\n    def apply_to_keypoints(self, keypoints, **params):\n        keypoints_new = keypoints.copy()\n        height = params[\"rows\"]\n        width = params[\"cols\"]\n        scale_x = self.width \/ width\n        scale_y = self.height \/ height\n        keypoints_new[:, 0] = keypoints[:, 0] * scale_x\n        keypoints_new[:, 1] = keypoints[:, 1] * scale_y\n        return keypoints_new    \n\nclass FaceShiftScaleRotate(A.ShiftScaleRotate):\n    def apply_to_keypoints(self, keypoints, **params):\n        keypoints_new = keypoints.copy()\n\n        height, width = params[\"rows\"], params[\"cols\"]\n        center = (width \/ 2, height \/ 2)\n        matrix = cv2.getRotationMatrix2D(center, params[\"angle\"], params[\"scale\"])\n        matrix[0, 2] += params[\"dx\"] * width\n        matrix[1, 2] += params[\"dy\"] * height\n\n        keypoints_new = cv2.transform(keypoints_new[None, :, :], matrix).squeeze()\n        return keypoints_new\n    \nclass FaceSmallestMaxSize(A.SmallestMaxSize):\n    def apply_to_keypoints(self, keypoints, **params):\n        keypoints_new = keypoints.copy()\n\n        height, width = params[\"rows\"], params[\"cols\"]\n        scale = self.max_size \/ min([height, width])\n        keypoints_new[:, 0] = keypoints[:, 0] * scale\n        keypoints_new[:, 1] = keypoints[:, 1] * scale\n\n        return keypoints_new\n    \nclass FaceCenterCrop(A.CenterCrop):\n    def apply_to_keypoints(self, keypoints, **params):\n        keypoints_new = keypoints.copy()\n\n        crop_height, crop_width = self.height, self.width\n        height, width = params[\"rows\"], params[\"cols\"]\n            \n        y1 = (height - crop_height) \/\/ 2\n        x1 = (width - crop_width) \/\/ 2\n        \n        keypoints_new[:, 0] = keypoints[:, 0] - x1\n        keypoints_new[:, 1] = keypoints[:, 1] - y1\n\n        return keypoints_new\n    \n# Source: https:\/\/www.kaggle.com\/sergemsu\/landmark-preserve-horizontal-flip\nclass FaceHorizontalFlip(A.HorizontalFlip):\n    def apply_to_keypoints(self, keypoints, **params):\n        keypoints_new = keypoints.copy()\n        keypoints_new[:, 0] = (params['cols'] - 1) - keypoints_new[:, 0]\n        lm = keypoints_new\n\n        nm = np.zeros_like(lm)\n\n        nm[:64,:]     = lm[64:128,:]     # [  0, 63]  -> [ 64, 127]:  i --> i + 64\n        nm[64:128,:]  = lm[:64,:]        # [ 64, 127] -> [  0, 63]:   i --> i - 64\n        nm[128:273,:] = lm[272:127:-1,:] # [128, 272] -> [128, 272]:  i --> 400 - i\n        nm[273:337,:] = lm[337:401,:]    # [273, 336] -> [337, 400]:  i --> i + 64\n        nm[337:401,:] = lm[273:337,:]    # [337, 400] -> [273, 336]:  i --> i - 64\n        nm[401:464,:] = lm[464:527,:]    # [401, 463] -> [464, 526]:  i --> i + 64\n        nm[464:527,:] = lm[401:464,:]    # [464, 526] -> [401, 463]:  i --> i - 64\n        nm[527:587,:] = lm[527:587,:]    # [527, 586] -> [527, 586]:  i --> i\n        nm[587:714,:] = lm[714:841,:]    # [587, 713] -> [714, 840]:  i --> i + 127\n        nm[714:841,:] = lm[587:714,:]    # [714, 840] -> [587, 713]:  i --> i - 127\n        nm[841:873,:] = lm[872:840:-1,:] # [841, 872] -> [841, 872]:  i --> 1713 - i\n        nm[873:905,:] = lm[904:872:-1,:] # [873, 904] -> [873, 904]:  i --> 1777 - i\n        nm[905:937,:] = lm[936:904:-1,:] # [905, 936] -> [905, 936]:  i --> 1841 - i\n        nm[937:969,:] = lm[968:936:-1,:] # [937, 968] -> [937, 968]:  i --> 1905 - i\n        nm[969:971,:] = lm[970:968:-1,:] # [969, 970] -> [969, 970]:  i --> 1939 - i\n\n        return nm","0b3030bf":"CROP_SIZE = 128\ntransforms = {\n    'Original': FaceResize(image.shape[0], image.shape[1], always_apply=True),\n    'FaceResize': FaceResize(CROP_SIZE, CROP_SIZE, always_apply=True),\n    'FaceShiftScaleRotate': FaceShiftScaleRotate(border_mode=cv2.BORDER_CONSTANT, always_apply=True),\n    'FaceSmallestMaxSize': FaceSmallestMaxSize(CROP_SIZE \/\/ 2, always_apply=True),\n    'FaceCenterCrop': FaceCenterCrop(CROP_SIZE, CROP_SIZE, always_apply=True),\n    'FaceHorizontalFlip': FaceHorizontalFlip(always_apply=True),\n}","fc1b40e1":"n_rows = 2\nn_cols = 3\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 10))\n\nfor i, (name, transform) in enumerate(transforms.items()):\n    ax = axes[i \/\/ n_cols][i % n_cols]\n    transformed = transform(image=image, keypoints=keypoints)\n    vis_keypoints(transformed['image'], transformed['keypoints'], ax=ax)\n    ax.set_title(name)","ef300f6a":"\u041f\u0440\u043e\u0431\u043e\u0432\u0430\u043b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 albumentations, \u043d\u043e \u0438\u0445 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0442\u043e\u0447\u0435\u043a \u0443 \u043c\u0435\u043d\u044f \u0440\u0430\u0431\u043e\u0442\u0430\u043b\u0430 \u0434\u043e\u043b\u0433\u043e. \u0422\u0430\u043a \u0447\u0442\u043e \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u043e\u0432\u0430\u043b \u043d\u0443\u0436\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u0441\u0442\u0430\u043b\u043e \u043f\u043e\u0448\u0443\u0441\u0442\u0440\u0435\u0435.  \n\u0414\u043e \u043a\u0443\u0447\u0438 \u0434\u043e\u0431\u0430\u0432\u0438\u043b \u0435\u0449\u0435 \u0432\u043e\u0442 \u044d\u0442\u043e\u0442 \u043a\u043e\u0434 \u0433\u043e\u0440\u0438\u0437\u043e\u043d\u0442\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0444\u043b\u0438\u043f\u0430: https:\/\/www.kaggle.com\/sergemsu\/landmark-preserve-horizontal-flip","a46265b3":"## Augmentations","466fdcea":"# Albumentations faster geometric transforms"}}