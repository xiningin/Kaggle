{"cell_type":{"0b4c2c7d":"code","c44f1220":"code","e17a414a":"code","98a96d8a":"code","3cd792be":"code","8c79312a":"code","9c3d409c":"code","cfb30735":"code","04674f47":"code","b1c7ca0d":"code","7e84c609":"code","c6d5a41c":"code","aaa91987":"code","820281de":"code","b295a729":"code","94630b19":"code","22e55201":"code","637bb949":"code","71e54d3a":"code","6b20ba27":"code","21c2ed5f":"code","c3c1052a":"code","06c7c7b8":"code","b40511f2":"code","b943c375":"code","d47c3350":"code","ad362ef7":"code","5d915293":"code","833e7845":"markdown","2825b6eb":"markdown","ba49f946":"markdown","e2258c3f":"markdown","04d92fe0":"markdown","f1acea91":"markdown","eb4d164a":"markdown","48e4ce1f":"markdown","031eb201":"markdown","da3def06":"markdown","48e2e64c":"markdown","4b26b7a1":"markdown","176a8ad6":"markdown","b1cb8b2f":"markdown","7918b8d4":"markdown","cc1a66ef":"markdown","7a281ff0":"markdown","59013899":"markdown","f5ffb7a2":"markdown","a282619f":"markdown","c90e2218":"markdown","30446a02":"markdown","52b041aa":"markdown","00d0dd20":"markdown","34efda2c":"markdown","8cc0e82a":"markdown","b30aa9b9":"markdown","08655a3e":"markdown"},"source":{"0b4c2c7d":"import os\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.resnet_v2 import ResNet152V2\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.applications.vgg16 import VGG16\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","c44f1220":"InceptionV3_pre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n                                include_top = False)\n\n# uncomment the follwing code to use presaved weights\n#pre_trained_model.load_weights(local_weights_file)\nfor layer in InceptionV3_pre_trained_model.layers:\n    layer.trainable = False","e17a414a":"ResNet152V2_pre_trained_model = ResNet152V2(input_shape = (150, 150, 3), \n                                include_top = False)\n# uncomment the follwing code to use presaved weights\n#pre_trained_model.load_weights(local_weights_file)\nfor layer in ResNet152V2_pre_trained_model.layers:\n    layer.trainable = False","98a96d8a":"VGG19_pre_trained_model = VGG19(input_shape = (150, 150, 3), \n                                include_top = False)\n# uncomment the follwing code to use presaved weights\n#pre_trained_model.load_weights(local_weights_file)\nfor layer in VGG19_pre_trained_model.layers:\n    layer.trainable = False","3cd792be":"VGG16_pre_trained_model = VGG16(input_shape = (150, 150, 3), \n                                include_top = False)\n# uncomment the follwing code to use presaved weights\n#pre_trained_model.load_weights(local_weights_file)\nfor layer in VGG16_pre_trained_model.layers:\n    layer.trainable = False","8c79312a":"InceptionV3_pre_trained_model.summary()\nlast_layer_InceptionV3 = InceptionV3_pre_trained_model.get_layer('mixed7')\nprint('InceptionV3 last layer output shape: ', last_layer_InceptionV3.output_shape)\nInceptionV3_last_output = last_layer_InceptionV3.output","9c3d409c":"ResNet152V2_pre_trained_model.summary()\nlast_layer_ResNet152V2 = ResNet152V2_pre_trained_model.get_layer('conv4_block36_1_relu')\nprint('ResNet152V2 last layer output shape: ', last_layer_ResNet152V2.output_shape)\nResNet152V2_last_output = last_layer_ResNet152V2.output","cfb30735":"VGG19_pre_trained_model.summary()\nlast_layer_VGG19 = VGG19_pre_trained_model.get_layer('block5_conv4')\nprint('VGG19 last layer output shape: ', last_layer_VGG19.output_shape)\nVGG19_last_output = last_layer_VGG19.output","04674f47":"VGG16_pre_trained_model.summary()\nlast_layer_VGG16 = VGG16_pre_trained_model.get_layer('block5_conv3')\nprint('VGG16 last layer output shape: ', last_layer_VGG16.output_shape)\nVGG16_last_output = last_layer_VGG16.output","b1c7ca0d":"from tensorflow.keras.optimizers import Adam\n#########################################################\nnumber_of_used_breed = 5\n#########################################################\n# number_of_used_breed is used here in Kaggle to reduce number of classes to be able to run all CNN's without timeout failing\n# it can be increased to 120 to have all data if there is a suffecient machine","7e84c609":"# Flatten the output layer to 1 dimension\nx_InceptionV3 = layers.Flatten()(InceptionV3_last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx_InceptionV3 = layers.Dense(512, activation='relu')(x_InceptionV3)\n# Add a dropout rate of 0.2\nx_InceptionV3 = layers.Dropout(0.2)(x_InceptionV3)                  \n# Add a final sigmoid layer for classification\nx_InceptionV3 = layers.Dense  (number_of_used_breed, activation='softmax')(x_InceptionV3)           \nInceptionV3_model = Model( InceptionV3_pre_trained_model.input, x_InceptionV3) \nInceptionV3_model.compile(optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False), \n              loss = 'categorical_crossentropy', \n              metrics = ['acc'])","c6d5a41c":"# Flatten the output layer to 1 dimension\nx_ResNet152V2 = layers.Flatten()(ResNet152V2_last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx_ResNet152V2 = layers.Dense(512, activation='relu')(x_ResNet152V2)\n# Add a dropout rate of 0.2\nx_ResNet152V2 = layers.Dropout(0.2)(x_ResNet152V2)                  \n# Add a final sigmoid layer for classification\nx_ResNet152V2 = layers.Dense  (number_of_used_breed, activation='softmax')(x_ResNet152V2)           \nResNet152V2_model = Model( ResNet152V2_pre_trained_model.input, x_ResNet152V2) \nResNet152V2_model.compile(optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False), \n              loss = 'categorical_crossentropy', \n              metrics = ['acc'])","aaa91987":"# Flatten the output layer to 1 dimension\nx_VGG19 = layers.Flatten()(VGG19_last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx_VGG19 = layers.Dense(1024, activation='relu')(x_VGG19)\n# Add a dropout rate of 0.2\nx_VGG19 = layers.Dropout(0.2)(x_VGG19)                  \n# Add a final sigmoid layer for classification\nx_VGG19 = layers.Dense  (number_of_used_breed, activation='softmax')(x_VGG19)           \nVGG19_model = Model( VGG19_pre_trained_model.input, x_VGG19) \nVGG19_model.compile(optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False), \n              loss = 'categorical_crossentropy', \n              metrics = ['acc'])","820281de":"# Flatten the output layer to 1 dimension\nx_VGG16 = layers.Flatten()(VGG16_last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx_VGG16 = layers.Dense(1024, activation='relu')(x_VGG16)\n# Add a dropout rate of 0.2\nx_VGG16 = layers.Dropout(0.2)(x_VGG16)                  \n# Add a final sigmoid layer for classification\nx_VGG16 = layers.Dense  (number_of_used_breed, activation='softmax')(x_VGG16)           \nVGG16_model = Model( VGG16_pre_trained_model.input, x_VGG16) \nVGG16_model.compile(optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False), \n              loss = 'categorical_crossentropy', \n              metrics = ['acc'])","b295a729":"class myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('acc')>0.90):\n            print('\/nTraining will stop as we have reached 90% of acc')\n            self.model.stop_training=True\n        \ncallback=myCallback()","94630b19":"###################################\n####### Import dataset here #######\n###################################","22e55201":"breed_list = os.listdir(\"..\/input\/stanford-dogs-dataset\/images\/Images\/\")\n\nnum_classes = len(breed_list)\nprint(\"{} breeds\".format(num_classes))\n\nn_total_images = 0\nfor breed in breed_list:\n    n_total_images += len(os.listdir(\"..\/input\/stanford-dogs-dataset\/images\/Images\/{}\".format(breed)))\nprint(\"{} images\".format(n_total_images))","637bb949":"from PIL import Image\nbreed_in_use = 1\n\nos.makedirs('data',exist_ok= True)\nos.makedirs('data\/Training',exist_ok= True)\nos.makedirs('data\/Validation',exist_ok= True)\nfor breed in breed_list:\n    os.makedirs('data\/Training\/' + breed,exist_ok= True)\n    os.makedirs('data\/Validation\/' + breed,exist_ok= True)\n    if breed_in_use == number_of_used_breed:\n        break\n    breed_in_use = breed_in_use+1\nprint('Created {} folders to store Training images of the different breeds.'.format(len(os.listdir('data\/Training'))))\nprint('Created {} folders to store Validation images of the different breeds.'.format(len(os.listdir('data\/Validation'))))\n\nvalidation_to_training_ratio = .1\nbreed_in_use = 1\nfor breed in os.listdir('data\/Training'):\n    cpt = sum([len(files) for r, d, files in os.walk('..\/input\/stanford-dogs-dataset\/images\/Images\/{}\/'.format(breed))])\n    validation = (int)(cpt * validation_to_training_ratio)\n    index = 0\n    for file in os.listdir('..\/input\/stanford-dogs-dataset\/annotations\/Annotation\/{}'.format(breed)):\n        img = Image.open('..\/input\/stanford-dogs-dataset\/images\/Images\/{}\/{}.jpg'.format(breed, file))\n        img = img.convert('RGB')        \n        if index<validation:\n            img.save('data\/Validation\/' + breed + '\/' + file + '.jpg')\n        else:\n            img.save('data\/Training\/' + breed + '\/' + file + '.jpg')\n        index = index +1\n    if breed_in_use == number_of_used_breed:\n        break    \n    breed_in_use = breed_in_use+1","71e54d3a":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ntrain_dir = 'data\/Training'\nvalidation_dir = 'data\/Validation'\n# Add our data-augmentation parameters to ImageDataGenerator\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255.,\n                                   rotation_range = 40,\n                                   width_shift_range = 0.2,\n                                   height_shift_range = 0.2,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True,\n                                   fill_mode='nearest')\n\n# Note that the validation data should not be augmented!\ntest_datagen = ImageDataGenerator( rescale = 1.0\/255. )\n\n# Flow training images in batches of 50 using train_datagen generator\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    batch_size = 50,\n                                                    class_mode = 'categorical',\n                                                    target_size = (150, 150))     \n\n# Flow validation images in batches of 10 using test_datagen generator\nvalidation_generator =  test_datagen.flow_from_directory( validation_dir,\n                                                          batch_size  = 10,\n                                                          class_mode  = 'categorical',\n                                                          target_size = (150, 150))\n","6b20ba27":"history_InceptionV3 = InceptionV3_model.fit_generator(\n            train_generator,\n            validation_data = validation_generator,\n            steps_per_epoch = 16,\n            epochs = 20,\n            validation_steps = 9,\n            verbose = 2,\n            callbacks=[callback]\n)","21c2ed5f":"acc = history_InceptionV3.history['acc']\nval_acc = history_InceptionV3.history['val_acc']\nloss = history_InceptionV3.history['loss']\nval_loss = history_InceptionV3.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\nplt.show()","c3c1052a":"#history_VGG19 = VGG19_model.fit_generator(\n#            train_generator,\n#            validation_data = validation_generator,\n#            steps_per_epoch = 16,\n#            epochs = 20,\n##            validation_steps = 9,\n#            verbose = 1,\n#            callbacks=[callback]\n#)","06c7c7b8":"#acc = history_VGG19.history['acc']\n#val_acc = history_VGG19.history['val_acc']\n#loss = history_VGG19.history['loss']\n#val_loss = history_VGG19.history['val_loss']\n\n#epochs = range(len(acc))\n\n#plt.plot(epochs, acc, 'r', label='Training accuracy')\n#plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n#plt.title('Training and validation accuracy')\n#plt.legend(loc=0)\n#plt.figure()\n\n#plt.show()","b40511f2":"#history_VGG16 = VGG16_model.fit_generator(\n#            train_generator,\n##            validation_data = validation_generator,\n#            steps_per_epoch = 16,\n#            epochs = 20,\n#            validation_steps = 9,\n##            verbose = 1,\n#            callbacks=[callback]\n#)","b943c375":"#acc = history_VGG16.history['acc']\n#val_acc = history_VGG16.history['val_acc']\n#loss = history_VGG16.history['loss']\n#val_loss = history_VGG16.history['val_loss']\n\n#epochs = range(len(acc))\n\n#plt.plot(epochs, acc, 'r', label='Training accuracy')\n#plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n#plt.title('Training and validation accuracy')\n#plt.legend(loc=0)\n#plt.figure()\n\n#plt.show()","d47c3350":"#history_ResNet152V2 = ResNet152V2_model.fit_generator(\n#            train_generator,\n#            validation_data = validation_generator,\n#            steps_per_epoch = 16,\n#            epochs = 20,\n#            validation_steps = 9,\n#            verbose = 1,\n#            callbacks=[callback]\n#)","ad362ef7":"#acc = history_ResNet152V2.history['acc']\n#val_acc = history_ResNet152V2.history['val_acc']\n#loss = history_ResNet152V2.history['loss']\n#val_loss = history_ResNet152V2.history['val_loss']\n#\n#epochs = range(len(acc))\n#\n#plt.plot(epochs, acc, 'r', label='Training accuracy')\n#plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n#plt.title('Training and validation accuracy')\n#plt.legend(loc=0)\n#plt.figure()\n#\n#plt.show()","5d915293":"import shutil\nshutil.rmtree('data', ignore_errors=False, onerror=None)","833e7845":"**This is just a quick comparasion between pre built and trained CNN's **\n\n\nThere is a Lot of enhancment that can be Implement even on Data like cropping, and Lable checking ...etc\nas well as one the networks it selfs like , number of freezed and trained layers, number of added layers, number of neurons on each added layers, the activation functions, the optimizers, and much more.\n\nthis is just how to import and use these CNN's (**Demonstrating Transfer Learning only**)","2825b6eb":"other network has been comminted because of an issue in kaggle committing\n\nJUST UNCOMMENT THE REST NETWORK TO TEST THEM\n","ba49f946":"Creat list of all dogs breed that we have","e2258c3f":"Add the flatten layer,Dense layer and softmax layer one the top of pretrained VGG16 network\n\nDropout layer as added for regularaization","04d92fe0":"fit the data into the model using fit_generator to the InceptionV3 compiled network","f1acea91":"fit the data into the model using fit_generator to the VGG16 compiled network","eb4d164a":"view InceptionV3 network summary and select the layers that you want to use as last layer to build on it","48e4ce1f":"Add the flatten layer,Dense layer and softmax layer one the top of pretrained ResNet152V2 network\n\nDropout layer as added for regularaization","031eb201":"**Import needed library and frameworks**","da3def06":"create new directories for training and validation datasets and copy the image data there","48e2e64c":"Add the flatten layer,Dense layer and softmax layer one the top of pretrained InceptionV3 network\n\nDropout layer as added for regularaization","4b26b7a1":"plot the results of Training accuracy and Validation accuracy ","176a8ad6":"view VGG16 network summary and select the layers that you want to use as last layer to build on it","b1cb8b2f":"fit the data into the model using fit_generator to the ResNet152V2 compiled network","7918b8d4":"view VGG19 network summary and select the layers that you want to use as last layer to build on it","cc1a66ef":"plot the results of Training accuracy and Validation accuracy ","7a281ff0":"Call the VGG19 Structured Network without flatten and softmax layers","59013899":"create data augmentation data flow to add more data based on the image data that we have","f5ffb7a2":"fit the data into the model using fit_generator to the VGG19 compiled network","a282619f":"plot the results of Training accuracy and Validation accuracy ","c90e2218":"Add the flatten layer,Dense layer and softmax layer one the top of pretrained VGG19 network\n\nDropout layer as added for regularaization","30446a02":"Call the ResNet152V2 Structured Network without flatten and softmax layers","52b041aa":"Call the VGG16 Structured Network without flatten and softmax layers","00d0dd20":"Import tnsorflow Adam Optimizor, this is one of the research area as importing and using different optimizor would help","34efda2c":"view ResNet152V2 network summary and select the layers that you want to use as last layer to build on it","8cc0e82a":"plot the results of Training accuracy and Validation accuracy ","b30aa9b9":"Call the InceptionV3 Structured Network without flatten and softmax layers","08655a3e":"**the following callback class have been added to stop training at specific accuracy amount**"}}