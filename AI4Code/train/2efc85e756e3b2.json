{"cell_type":{"8beb453a":"code","99c421d3":"code","cc7bad70":"code","1c206700":"code","5211faa6":"code","d533e56a":"code","3e593f54":"code","e18551e7":"code","d27550ce":"code","db38f084":"code","e90c4b8d":"code","78ff84b2":"code","4728c339":"code","01ab9611":"code","cb3724db":"code","096f5cb5":"code","3ad03e5c":"code","33a8e438":"code","b0e05d06":"markdown","943a9290":"markdown","86645da5":"markdown","a3476f14":"markdown"},"source":{"8beb453a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nfrom sklearn.preprocessing import LabelEncoder\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","99c421d3":"def reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings    \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return props, NAlist","cc7bad70":"def createTargetFiles(sales_train_val, start_range, end_range):  \n    cat_val_set = [f\"d_{cat}\" for cat in range(start_range, end_range+1)]\n    val_demand_set = np.zeros([len(sales_train_val), len(cat_val_set)]).astype('uint8')\n    val_demand_set = pd.DataFrame(val_demand_set)\n    val_demand_set.columns = cat_val_set\n    \n    sales_train_basic_feat = sales_train_val.loc[:,['id']]\n    val_set = pd.concat([sales_train_basic_feat, val_demand_set], axis=1)\n        \n    return val_set","1c206700":"def createTrainTestSet(train_set, start_range, end_range, calenderSet, priceSet):\n    test_set = createTargetFiles(train_set, start_range, end_range)\n    \n    fullSet = train_set.merge(test_set, how='left', on='id')\n    \n    dropD = [f\"d_{val}\" for val in range(1, 1069-31)]\n    fullSet.drop(dropD, axis=1, inplace=True)\n    \n    fullSet = pd.melt(fullSet,\n                    id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                    var_name = 'day', value_name = 'demand')\n\n    fullSet.columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd', 'demand']\n    \n    calenderSet = calenderSet.drop(['year', 'date', 'wday'], axis = 1)\n    \n    fullSet = fullSet.merge(calenderSet, how='left', on='d')\n    \n    fullSet['weekday'] = [1 if (day=='Saturday' or day=='Sunday') else '0' for day in fullSet['weekday']] \n    \n    fullSet = fullSet.merge(sp, how='left', on=['store_id', 'item_id', 'wm_yr_wk'])\n    fullSet.drop(['item_id','wm_yr_wk'], axis=1, inplace=True)\n    \n    avg_sellPrice_val = fullSet.groupby(by=['dept_id', 'store_id']).aggregate({'sell_price': 'mean'})\n    avg_sellPrice_val.reset_index(drop=False, inplace=True)\n    \n    fullSet = fullSet.merge(avg_sellPrice_val, how='left', on=['dept_id', 'store_id'])\n\n    gc.collect()\n    del avg_sellPrice_val, calenderSet\n    \n    fullSet['sell_price_x'].fillna(fullSet['sell_price_y'], inplace=True)\n    fullSet.drop('sell_price_y', axis=1, inplace=True)\n\n    fullSet,_ = reduce_mem_usage(fullSet)\n    \n    cat_list = ['dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', \n           'event_type_2']\n    labelEncoder = [LabelEncoder() for i in range(len(cat_list))]\n    \n    for x in range(len(cat_list)):\n        labelEncoder[x].fit(fullSet[cat_list[x]].astype(str))\n        \n    for x in range(len(cat_list)):\n        print(x)\n        fullSet[cat_list[x]] = labelEncoder[x].transform(fullSet[cat_list[x]].astype(str))\n        gc.collect()    \n        fullSet,_ = reduce_mem_usage(fullSet)\n        \n    fullSet['lag_t28'] = fullSet.groupby(['id'])['demand'].shift(28)\n    fullSet['lag_t30'] = fullSet.groupby(['id'])['demand'].shift(30)\n    fullSet['lag_t31'] = fullSet.groupby(['id'])['demand'].shift(31)\n    fullSet['rolling_mean30'] = fullSet.groupby(['id'])['demand'].shift(1).rolling(30).mean()\n    fullSet['rolling_mean31'] = fullSet.groupby(['id'])['demand'].shift(1).rolling(31).mean()\n    fullSet['rolling_std30'] = fullSet.groupby(['id'])['demand'].shift(1).rolling(30).std()\n    fullSet['rolling_std31'] = fullSet.groupby(['id'])['demand'].shift(1).rolling(31).std()\n    \n    fullSet = fullSet[fullSet['lag_t31'].notnull()]\n    \n    fullSet,_ = reduce_mem_usage(fullSet)\n    \n    return fullSet","5211faa6":"def createTrainTestSet_Experiment(train_set, start_range, end_range, calenderSet, priceSet):\n    test_set = createTargetFiles(train_set, start_range, end_range)\n        \n    fullSet = train_set.merge(test_set, how='left', on='id')\n    \n    dropD = [f\"d_{val}\" for val in range(1, 1069-30)]\n    fullSet.drop(dropD, axis=1, inplace=True)\n    \n    fullSet = pd.melt(fullSet,\n                    id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                    var_name = 'day', value_name = 'demand')\n\n    fullSet.columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd', 'demand']\n    \n    calenderSet = calenderSet.drop(['year', 'date', 'wday'], axis = 1)\n    \n    fullSet = fullSet.merge(calenderSet, how='left', on='d')\n    \n    fullSet['weekday'] = [1 if (day=='Saturday' or day=='Sunday') else '0' for day in fullSet['weekday']] \n    \n    fullSet = fullSet.merge(priceSet, how='left', on=['store_id', 'item_id', 'wm_yr_wk'])\n    fullSet.drop(['item_id','wm_yr_wk', 'state_id', 'store_id'], axis=1, inplace=True)\n    \n    gc.collect()\n    del calenderSet, priceSet\n    \n    fullSet.sell_price = fullSet.sell_price.fillna(0)\n\n    fullSet,_ = reduce_mem_usage(fullSet)\n    \n    cat_list = ['dept_id', 'cat_id', 'event_name_1', 'event_type_1', 'event_name_2', \n               'event_type_2', 'snap_CA', 'snap_TX']\n                \n    labelEncoder = [LabelEncoder() for i in range(len(cat_list))]\n    \n    for x in range(len(cat_list)):\n        labelEncoder[x].fit(fullSet[cat_list[x]].astype(str))\n        \n    for x in range(len(cat_list)):\n        print(x)\n        fullSet[cat_list[x]] = labelEncoder[x].transform(fullSet[cat_list[x]].astype(str))\n        gc.collect()    \n        fullSet,_ = reduce_mem_usage(fullSet)\n    \n    fullSet.drop(['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'month','snap_WI'], axis=1, inplace=True)\n    \n    #Use Lag and Rolling Features to enhanced the richness of the data\n    \n    fullSet['lag_t25'] = fullSet.groupby(['id'])['demand'].shift(25)\n    fullSet['lag_t30'] = fullSet.groupby(['id'])['demand'].shift(30)\n    fullSet['lag_t7'] = fullSet.groupby(['id'])['demand'].shift(7)\n    fullSet['lag_t1'] = fullSet.groupby(['id'])['demand'].shift(1)\n    fullSet['lag_t2'] = fullSet.groupby(['id'])['demand'].shift(2)\n    fullSet['rolling_mean7'] = fullSet.groupby(['id'])['demand'].shift(1).rolling(7).mean()\n    fullSet['rolling_std7'] = fullSet.groupby(['id'])['demand'].shift(1).rolling(7).std()\n    fullSet['rolling_mean2'] = fullSet.groupby(['id'])['demand'].shift(1).rolling(2).mean()\n    fullSet['rolling_std2'] = fullSet.groupby(['id'])['demand'].shift(1).rolling(2).std()\n    fullSet['rolling_mean_t30'] = fullSet.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n    fullSet['rolling_std_t30'] = fullSet.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n    fullSet['rolling_skew_t30'] = fullSet.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).skew())\n    fullSet['rolling_kurt_t30'] = fullSet.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).kurt())\n    \n    fullSet['rolling_pricemean7'] = fullSet.groupby(['id'])['sell_price'].shift(1).rolling(7).mean()\n    fullSet['pricelag_t1'] = fullSet.groupby(['id'])['sell_price'].shift(1)\n    \n    fullSet['demand_price_mean7'] = fullSet['rolling_mean7']\/fullSet['rolling_pricemean7']\n    fullSet['demand_price_mean2'] = fullSet['rolling_mean2']\/fullSet.groupby(['id'])['sell_price'].shift(1).rolling(2).mean()\n\n    fullSet = fullSet[fullSet['lag_t30'].notnull()]\n    \n    fullSet,_ = reduce_mem_usage(fullSet)\n    \n    return fullSet","d533e56a":"train_val = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ntrain_val, _ = reduce_mem_usage(train_val)","3e593f54":"cal = pd.read_csv('..\/input\/m5-forecasting-accuracy\/calendar.csv')\ncal, _ = reduce_mem_usage(cal)","e18551e7":"sp = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sell_prices.csv')","d27550ce":"validationSet = createTrainTestSet_Experiment(train_val, 1914, 1941, cal, sp)","db38f084":"validationSet.to_csv('validation_set.csv', index=False)","e90c4b8d":"sp = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nsp,_ = reduce_mem_usage(sp)","78ff84b2":"validationSet.head()","4728c339":"del validationSet","01ab9611":"train_eval = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\ntrain_eval, _ = reduce_mem_usage(train_eval)","cb3724db":"evaluationSet = createTrainTestSet_Experiment(train_eval, 1942, 1969, cal, sp)","096f5cb5":"evaluationSet.to_csv('evaluation_set.csv', index=False)","3ad03e5c":"corr = evaluationSet.drop(['id', 'd'], axis=1).corr()","33a8e438":"mask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(corr, mask=mask, cmap='BrBG', vmin=-1, vmax=1, annot=True)","b0e05d06":"# Create a Validation Set","943a9290":"# Create a Train Set and Test Set","86645da5":"# Create the Evaluation Set","a3476f14":"# Check the Correlation Matrix Between Features and Target"}}