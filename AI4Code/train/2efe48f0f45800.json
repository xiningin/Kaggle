{"cell_type":{"3bd10dc6":"code","1d3e5836":"code","6059e7e5":"code","a7b7a82d":"code","ec7fa73c":"code","5ba3b2d2":"code","73bf66df":"code","877c441a":"code","1c49a52a":"code","fe190859":"code","ab5684e4":"code","69e761fb":"markdown","a6843a45":"markdown","f5b22763":"markdown","86155150":"markdown","9469b26b":"markdown","27852410":"markdown","d70c7f8d":"markdown","fa9714e5":"markdown","6773fa76":"markdown"},"source":{"3bd10dc6":"! conda install -y -c conda-forge hvplot==0.5.2 bokeh==1.4.0 imbalanced-learn","1d3e5836":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nfrom toolz.curried import pipe, map, partial\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import stats\nimport hvplot.pandas\nimport holoviews as hv\nimport matplotlib.pyplot as plt\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer, PolynomialFeatures\nfrom sklearn.decomposition import PCA, FactorAnalysis\nfrom sklearn.pipeline import Pipeline\nfrom statsmodels.genmod.families import links\nimport statsmodels.api as sm \n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nhv.extension('bokeh')","6059e7e5":"data = pd.read_excel('\/kaggle\/input\/covid19\/dataset.xlsx').dropna(1, how='all')\n\n((data\n .isna().mean().reset_index()\n .rename(columns={\"index\": 'Measurement', 0: '% Missing'})\n .hvplot.table()) + \n(data\n .isna().apply(np.logical_not).sum().reset_index()\n .rename(columns={\"index\": 'Measurement', 0: 'Number Not NaN'})\n .hvplot.table())).cols(2)","a7b7a82d":"excluded = ['Patient addmited to regular ward (1=yes, 0=no)',\n             'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n             'Patient addmited to intensive care unit (1=yes, 0=no)']\n\nfeatures = (data\n            .select_dtypes(np.number)\n            .drop(columns=excluded))\n\nisna = features.isna().sum()\n\nfeatures = features.loc[:, isna < isna.median()] # we get rid of the worst offending features with the worst coverage\n\npower = PowerTransformer()\nscaler = StandardScaler()\nimputer = IterativeImputer(sample_posterior=True)\n\nX = (pipe(features,\n          power.fit_transform, # bayesian ridge will assume data is normally distributed\n          scaler.fit_transform, # we scale cause we are using regularization\n          imputer.fit_transform,\n          scaler.inverse_transform, # invert the transforms to original space\n          power.inverse_transform,\n          partial(pd.DataFrame, columns=features.columns, index=features.index))\n     .replace({np.inf: np.nan, -np.inf:np.nan})\n     .astype(np.float)\n     .join(data\n           .loc[:, excluded]\n           .assign(negative = (data\n                               .loc[:,'SARS-Cov-2 exam result']\n                               .replace({'negative':1, 'positive':0})))\n           .assign(positive = lambda df: 1 - df.sum(0))\n           .idxmax(1)\n           .to_frame('care'))\n     .replace({'nan', np.nan})\n     .dropna())\n\nmissing_weights_rows = (X.join(features\n                          .isna()\n                          .apply(np.logical_not)\n                          .mean(1)\n                          .to_frame('missing'), how='left')\n                   .missing)\n\nmissing_weights_cols = (features\n                        .iloc[X.index, :]\n                        .loc[:, X.drop(columns=['care']).columns]\n                       .isna()\n                       .apply(np.logical_not)\n                       .mean(0))\n\nX_weighted = (X\n                .drop(columns=['care'])\n                .pipe(lambda df: pd.DataFrame(StandardScaler().fit_transform(df)))# standard scale\n                .apply(lambda c: c * missing_weights_rows.to_numpy()) # doubly-weight the data according to its missingness\n                .apply(lambda c: c * missing_weights_cols.to_numpy(), 1))\n\nX_weighted.columns = X.drop(columns=['care']).columns\n\npipeline = Pipeline([('pca', PCA(2))])\nZ_proj = (pipeline\n     .fit_transform(X_weighted))\n\ncomponents = ([f'Components {i} ({round(v*100)}%)' for i, v in enumerate(pipeline.named_steps['pca'].explained_variance_ratio_)] \n              if hasattr(pipeline.named_steps['pca'], 'explained_variance_ratio_') \n              else ['Component 1', 'Component 2'])\n\n(pd.DataFrame(Z_proj, columns=components)\n .assign(care=X.care)\n .dropna()\n .hvplot.scatter(x=components[0], y=components[1],\n                 color='care', title='Not NaN Weighted Principle Components of Patient Outcomes',\n                 width=1000, height=400))","ec7fa73c":"care_level = X.care.replace({v: k for k, v in enumerate(['negative',\n                                                         'Patient addmited to regular ward (1=yes, 0=no)',\n                                                         'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n                                                         'Patient addmited to intensive care unit (1=yes, 0=no)'])})\ncare_level.hvplot.hist(title='Distribution of Care')","5ba3b2d2":"fa = FactorAnalysis(4)\nZ = fa.fit_transform(X_weighted)\n\n(pd.DataFrame(fa.components_,\n              columns=X_weighted.columns)\n .assign(Factors = [f'Factor {i+1}' for i in range(fa.components_.shape[0])])\n .melt(id_vars = 'Factors', var_name='Variable', value_name='Loading')\n .hvplot.heatmap(x='Variable', y='Factors', C='Loading', \n                  height=600, width=800, colorbar=True, cmap='spectral_r')\n .opts(xrotation=90, title='Factor Loadings'))","73bf66df":"Z_orth = Z\n\nendog = care_level\nexog = sm.add_constant(pd.DataFrame(Z_orth, columns = [f'Factor {i+1}' for i in range(Z_orth.shape[1])], index = endog.index))\nglm_model = sm.GLM(endog,\n                     exog,\n                     family=sm.families.NegativeBinomial(link = links.Power(0.25)))\n\nresults = glm_model.fit()\nresults.summary()","877c441a":"fig, ax = plt.subplots()\n\nax.scatter(endog, results.resid_pearson)\nax.hlines(0, 0, 1)\nax.set_xlim(0, 1)\nax.set_title('Residual Dependence Plot')\nax.set_ylabel('Pearson Residuals')\nax.set_xlabel('Fitted values')","1c49a52a":"fig, ax = plt.subplots()\n\nresid = results.resid_deviance.copy()\nresid_std = stats.zscore(resid)\nax.hist(resid_std, bins=25)\nax.set_title('Histogram of standardized deviance residuals');","fe190859":"sm.graphics.plot_partregress_grid(results)","ab5684e4":"(pd.DataFrame(np.dot(results\n                     .params\n                     .drop('const')\n                     .to_frame('coef')\n                     .T, fa.components_) + fa.mean_,\n              columns=X.drop(columns=['care']).columns,\n              index =['Coefficients'])\n.T.hvplot.bar()\n.opts(xrotation=90, width=800, height=600, title='Reprojected GLM Factor Weights'))","69e761fb":"I opted to reproject the coefficient of my model into the original feature space. Here the magnitudes of the coefficients may be more interpretable for clinicians.  ","a6843a45":"\nThe main challenge in dealing with this data is in handling the number of missing data points.  Doctors seldom issue every test to every patient, meaning there are many every single patient; we only observe a small fraction from the universe of possible tests. One thing to bear in mind is that the number of tests issued may be correlated with the severity of the patient's case, as doctors may hold the patient in a high level of care and might see greater value in tracking more metrics on the patient when deciding on what treatments or interventions to take.  \n\n","f5b22763":"The application of Factor Analysis can be useful as a framework by which to model latent characteristics in one data. Traditionally, these factors in Exploratory Factor Analysis may be given names by domain experts characterizing their properties. I will leave this task to the medical professionals among you. ","86155150":"Looking at the diagnostic plots to this model, we see little evidence of our errors being correlated with our response but should note a number of obvious outliers in the data. ","9469b26b":"Given the distribution of the data, I opted to use the Negative Binomial Distribution as the conditional distribution of my Generalized Linear Model. I experimented with a number of appropriate link functions but in the end, opted to use the Power Transform with an exponent of 0.25.  ","27852410":"In this notebook, I investigate an anonymized dataset of patients seen at the Hospital Israelita Albert Einstein, at S\u00e3o Paulo, Brazil, who had samples collected to perform the SARS-CoV-2 RT-PCR and additional laboratory tests during a visit to the hospital.  \n\nThis is a fascinating dataset, as unlike many public COVID-19 datasets, it contains patient-level data with detailed test results. Another feature of this dataset is the fact that all the data contained in this sample are from the sample hospital in a narrow time-window, meaning we are unlikely to observe effects which relate to varying regional infections rates or changing hospital conditions.  ","d70c7f8d":"After discarding fifty-per cent of the features with the most missing value, I chose to then impute missing data points using an Iterature Imputer which samples from the posterior of a Bayesian Ridge Regression Model. In order to apply this imputation method, I took the approach to normalize the data using Yeo-Johnson Power Transformer, due to the assumption of the Linear Model on our conditional distribution, and Standard Scale the data, due to the application of regularization through our weight prior. These are both reversible transformations, and so our data remained at its original scale after imputation. Following imputation, I looked to apply Principle Component Analysis (PCA) to analyze the variance in our data. While I did investigate Kernel Principle Component Methods, I found this branch of inquiry less informative.  Before applying PCA, I opted to standard scale the data again and weighted the column- and row-spaces by the number of missing data points. I visualize the first two Principle Components against patient outcomes in the data, and the variance explained ratios of the principle components.  ","fa9714e5":"Looking at our partial plot, a number of our factors do appear correlated with patient outcomes, which may motivate one to investigate other latent variables or possibly non-linear transformation of the data to better capture these relationships. ","6773fa76":"The next approach I took was to apply Generalized Linear Models and Factor Analysis to our data.  The response variable I aimed to model was the level of patient care, coded as follows:  \n0. negative  \n1. Patient admitted to regular ward (1=yes, 0=no)  \n2. Patient admitted to semi-intensive unit (1=yes, 0=no)  \n3. Patient admitted to intensive care unit (1=yes, 0=no)  "}}