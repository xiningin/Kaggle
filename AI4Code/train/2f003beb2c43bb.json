{"cell_type":{"39ef5bf7":"code","61a8558f":"code","da501798":"code","3f97d0f6":"code","811eec6f":"code","ee8bfb82":"code","900493e6":"code","ab6a61a9":"code","d8b22f36":"code","84c73db1":"code","66aceb42":"code","7874879a":"code","be9b6386":"code","1e7d4090":"code","7e28925e":"code","bd2c77be":"code","f9ce2964":"code","081687a5":"code","5184e4a3":"code","4c07a5b8":"code","dfffd2ae":"code","d39cbffc":"code","72b5e5d1":"markdown","ffa8246a":"markdown","c3ae9af3":"markdown","b1f06735":"markdown","6e4b4c68":"markdown","857d4998":"markdown","27307b24":"markdown","249b39f7":"markdown","336caab3":"markdown"},"source":{"39ef5bf7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","61a8558f":"!pip install -U torch==1.5 torchvision==0.6 -f https:\/\/download.pytorch.org\/whl\/cu101\/torch_stable.html \n!pip install cython pyyaml==5.1\n!pip install -U 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'\n!gcc --version\n\nimport torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())","da501798":"!pip install detectron2==0.1.3 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu101\/index.html","3f97d0f6":"import os, sys\n\n!git clone https:\/\/github.com\/aim-uofa\/AdelaiDet.git\nos.chdir('AdelaiDet')","811eec6f":"DATA_DIR = '\/kaggle\/input'\nROOT_DIR = '\/kaggle\/working'\n\nsys.path.append(os.path.join(ROOT_DIR, 'AdelaiDet')) ","ee8bfb82":"!python setup.py build develop","900493e6":"!wget -O tt_attn_R_50.pth https:\/\/cloudstor.aarnet.edu.au\/plus\/s\/t2EFYGxNpKPUqhc\/download\n!ls -lh tt_attn_R_50.pth","ab6a61a9":"import cv2\nimport glob\nimport matplotlib.pyplot as plt\n\ndef process(filename):\n    plt.figure(figsize=(25,15))\n    plt.imshow(filename)","d8b22f36":"z = glob.glob(\"\/kaggle\/input\/shopee-product-detection-student\/train\/train\/train\/00\/*.jpg\")[:100]\nimages = [cv2.imread(file) for file in z]\nprint(len(images))\n    \ni = 0\nfor file in images:\n    process(file)\n    i += 1\n    if i > 4: break","84c73db1":"os.chdir(\"demo\")\n!pwd","66aceb42":"import argparse\nimport multiprocessing as mp\nimport os\nimport time\nimport tqdm\n\nfrom detectron2.data.detection_utils import read_image\nfrom detectron2.utils.logger import setup_logger\n\nfrom predictor import VisualizationDemo\nfrom adet.config import get_cfg\n\nfrom tqdm.auto import tqdm, trange\nfrom tqdm import tqdm_notebook","7874879a":"logger = setup_logger()","be9b6386":"cfg = get_cfg()\ncfg.merge_from_file(\"..\/configs\/BAText\/TotalText\/attn_R_50.yaml\")\ncfg.merge_from_list([\"MODEL.WEIGHTS\", \"..\/tt_attn_R_50.pth\"])","1e7d4090":"confidence = 0.5\ncfg.MODEL.RETINANET.SCORE_THRESH_TEST = confidence\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = confidence\ncfg.MODEL.FCOS.INFERENCE_TH_TEST = confidence\ncfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = confidence\ncfg.freeze()","7e28925e":"demo = VisualizationDemo(cfg)","bd2c77be":"inmages = glob.glob(\"\/kaggle\/input\/shopee-product-detection-student\/train\/train\/train\/**\/*.jpg\")","f9ce2964":"def decode_recognition(rec):\n    CTLABELS = [' ','!','\"','#','$','%','&','\\'','(',')','*','+',',','-','.','\/','0','1','2','3','4','5','6','7','8','9',':',';','<','=','>','?','@','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','[','\\\\',']','^','_','`','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','{','|','}','~']\n\n    s = ''\n    for c in rec:\n        c = int(c)\n        if c < 95:\n            s += CTLABELS[c]\n        elif c == 95:\n            s += u'\u53e3'\n    return s","081687a5":"p = []\nfor path in tqdm(inmages):\n    # use PIL, to be consistent with evaluation\n    img = read_image(path, format=\"BGR\")\n    start_time = time.time()\n    predictions, visualized_output = demo.run_on_image(img)\n    tqdm.write(\n        \"{}: detected {} instances in {:.2f}s\".format(\n            path, len(predictions[\"instances\"]), time.time() - start_time\n        )\n    )\n    p.append([decode_recognition(p) for p in predictions[\"instances\"].recs])","5184e4a3":"anott = pd.DataFrame({'path': inmages, 'annot': p})","4c07a5b8":"anott.to_csv(\"..\/..\/annot_train.csv\", index=False)","dfffd2ae":"flatten = lambda l: [item for sublist in l for item in sublist]\n\nc = pd.Series(flatten(anott.annot.values))","d39cbffc":"c.apply(lambda v: v.lower()).value_counts()[:60]","72b5e5d1":"Sanity check; plot some images","ffa8246a":"Run the inference. This will take 40 minutes to 1 hour for this dataset which is way way faster compared with other OCR method.","c3ae9af3":"Set min. confidence to 0.5. Adjust as needed","b1f06735":"## Extract Text","6e4b4c68":"Change directory to the demo dir as we need to import from a file in that directory('predictor.py')","857d4998":"Download pretrained weights","27307b24":"Install dependencies. I recommend you use exactly these versions.","249b39f7":"The predictions are encoded, the function below performs the decoding.","336caab3":"The variable `inmages` represents the files we want to perform OCR on."}}