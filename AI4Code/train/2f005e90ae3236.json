{"cell_type":{"b4b980de":"code","4faa1071":"code","74997b3e":"code","bbd73a86":"code","4488dc89":"code","277f57fc":"code","d3871eaa":"code","d7af0284":"code","bcf93ea8":"code","772b9c94":"code","c949dff2":"code","8f62ff77":"code","2417cea6":"code","7540975d":"code","c60e7e8a":"code","30c729bf":"code","58d539a4":"code","8ba21809":"code","c0e7c947":"code","bfadec9f":"code","0f3c2e03":"code","75e89a77":"code","22aab14a":"code","734b2658":"code","2da6d576":"code","a962b2df":"code","344abf52":"code","cc8b0ca0":"markdown","fffcc03b":"markdown","6c074f1c":"markdown","baed96f6":"markdown","ca172710":"markdown","de98659d":"markdown","8cf0b9d2":"markdown","5c09c6b4":"markdown","1d943dd9":"markdown","a73077c0":"markdown","9fb6e3ea":"markdown","4a7400ad":"markdown","e83f65af":"markdown","a5415db2":"markdown","25c0fc59":"markdown","e2a3b26b":"markdown"},"source":{"b4b980de":"import re\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom sklearn.model_selection import train_test_split\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n    \nprint(tf.__version__)","4faa1071":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path(\"covid19-radiography-database\")\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [180, 180]","74997b3e":"filenames = tf.io.gfile.glob(str(GCS_PATH + '\/COVID-19 Radiography Database\/COVID-19\/*'))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH + '\/COVID-19 Radiography Database\/NORMAL\/*')))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH + '\/COVID-19 Radiography Database\/Viral Pneumonia\/*')))\n\nrandom.seed(1337)\ntf.random.set_seed(1337)\nrandom.shuffle(filenames)","bbd73a86":"train_filenames, test_filenames = train_test_split(filenames, test_size=0.1)\ntrain_filenames, val_filenames = train_test_split(train_filenames, test_size=0.1)","4488dc89":"COUNT_NORMAL = len([filename for filename in train_filenames if \"NORMAL\" in filename])\nprint(\"Normal images count in training set: \" + str(COUNT_NORMAL))\n\nCOUNT_COVID = len([filename for filename in train_filenames if \"\/COVID-19\/\" in filename])\nprint(\"COVID-19 images count in training set: \" + str(COUNT_COVID))\n\nCOUNT_PNEUMONIA = len([filename for filename in train_filenames if \"Viral\" in filename])\nprint(\"Pneumonia images count in training set: \" + str(COUNT_PNEUMONIA))","277f57fc":"train_list_ds = tf.data.Dataset.from_tensor_slices(train_filenames)\nval_list_ds = tf.data.Dataset.from_tensor_slices(val_filenames)\ntest_list_ds = tf.data.Dataset.from_tensor_slices(test_filenames)","d3871eaa":"TRAIN_IMG_COUNT = tf.data.experimental.cardinality(train_list_ds).numpy()\nprint(\"Training images count: \" + str(TRAIN_IMG_COUNT))\n\nVAL_IMG_COUNT = tf.data.experimental.cardinality(val_list_ds).numpy()\nprint(\"Validating images count: \" + str(VAL_IMG_COUNT))","d7af0284":"CLASSES = ['NORMAL', 'COVID-19', 'Viral Pneumonia']","bcf93ea8":"def get_label(file_path):\n    # convert the path to a list of path components\n    parts = tf.strings.split(file_path, os.path.sep)\n    # The second to last is the class-directory\n    return parts[-2] == CLASSES","772b9c94":"def decode_img(img):\n  # convert the compressed string to a 3D uint8 tensor\n  img = tf.image.decode_png(img, channels=3)\n  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n  img = tf.image.convert_image_dtype(img, tf.float32)\n  # resize the image to the desired size.\n  return tf.image.resize(img, IMAGE_SIZE)","c949dff2":"def process_path(file_path):\n    label = get_label(file_path)\n    # load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return img, label","8f62ff77":"train_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\nval_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\ntest_ds = test_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)","2417cea6":"def prepare_for_training(ds, cache=True):\n    # This is a small dataset, only load it once, and keep it in memory.\n    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n    # fit in memory.\n    if cache:\n        if isinstance(cache, str):\n            ds = ds.cache(cache)\n        else:\n            ds = ds.cache()\n\n    ds = ds.shuffle(buffer_size=1000)\n    ds = ds.batch(BATCH_SIZE)\n\n    if cache:\n        ds = ds.prefetch(buffer_size=AUTOTUNE)\n\n    return ds","7540975d":"train_ds = prepare_for_training(train_ds)\nval_ds = prepare_for_training(val_ds)\ntest_ds = prepare_for_training(test_ds, False)","c60e7e8a":"def show_batch(image_batch, label_batch):\n    plt.figure(figsize=(10,10))\n    for n in range(25):\n        ax = plt.subplot(5,5,n+1)\n        plt.imshow(image_batch[n])\n        plt.title(CLASSES[np.argmax(label_batch[n])])\n        plt.axis(\"off\")","30c729bf":"image_batch, label_batch = next(iter(train_ds))\nshow_batch(image_batch.numpy(), label_batch.numpy())","58d539a4":"early_stopping_cb = keras.callbacks.EarlyStopping(patience=5,\n                                                  restore_best_weights=True)","8ba21809":"with strategy.scope():\n    reconstructed_model = keras.models.load_model(\"..\/input\/test-model\/xray_model.h5\")\n    reconstructed_model.pop()\n    reconstructed_model.add(keras.layers.Dense(3, activation='softmax'))\n    \n    METRICS = [\n        'accuracy',\n        keras.metrics.Precision(name=\"precision\"),\n        keras.metrics.Recall(name=\"recall\")\n    ]\n    \n    reconstructed_model.compile(\n        optimizer=\"adam\",\n        loss=\"categorical_crossentropy\",\n        metrics=METRICS,\n    )","c0e7c947":"history = reconstructed_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=20,\n    callbacks=[early_stopping_cb]\n)","bfadec9f":"reconstructed_model.evaluate(test_ds, return_dict=True)","0f3c2e03":"reconstructed_model.summary()","75e89a77":"# last convolution block of the model\nreconstructed_model.layers[7].layers","22aab14a":"def get_img_array(img_path, size=IMAGE_SIZE):\n    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n    # `array` is a float32 NumPy array\n    array = keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size (1, 180, 180, 3)\n    array = np.expand_dims(array, axis=0) \/ 255.0\n    return array","734b2658":"def make_gradcam_heatmap(img_array, model):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer\n    last_conv_layer = model.layers[7]\n    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n    \n    # Mark the classifying layers\n    classifier_layers = model.layers[-5:]\n\n    # Second, we create a model that maps the activations of the last conv\n    # layer to the final class predictions\n    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n    x = classifier_input\n    for classifier_layer in classifier_layers:\n        x = classifier_layer(x)\n    classifier_model = keras.Model(classifier_input, x)\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        # Compute activations of the last conv layer and make the tape watch it\n        last_conv_layer_output = last_conv_layer_model(img_array)\n        tape.watch(last_conv_layer_output)\n        # Compute class predictions\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = tf.argmax(preds[0])\n        top_class_channel = preds[:, top_pred_index]\n\n    # This is the gradient of the top predicted class with regard to\n    # the output feature map of the last conv layer\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n    pooled_grads = pooled_grads.numpy()\n    for i in range(pooled_grads.shape[-1]):\n        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n    # The channel-wise mean of the resulting feature map\n    # is our heatmap of class activation\n    heatmap = np.mean(last_conv_layer_output, axis=-1)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = np.maximum(heatmap, 0) \/ np.max(heatmap)\n    return heatmap","2da6d576":"def superimposed_cam(file_path):\n    # Prepare image\n    img_array = get_img_array(file_path)\n\n    # Generate class activation heatmap\n    heatmap = make_gradcam_heatmap(\n        img_array, reconstructed_model\n    )\n\n    # Rescale the original image\n    img = img_array * 255\n\n    # We rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # We use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # We use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # We create an image with RGB colorized heatmap\n    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * 0.4 + img\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img[0])\n    \n    return superimposed_img, CLASSES[np.argmax(reconstructed_model.predict(img_array))]","a962b2df":"covid_filenames = tf.io.gfile.glob('..\/input\/covid19-radiography-database\/COVID-19 Radiography Database\/COVID-19\/*')\npneumonia_filenames = tf.io.gfile.glob('..\/input\/covid19-radiography-database\/COVID-19 Radiography Database\/Viral Pneumonia\/*')","344abf52":"plt.figure(figsize=(20,20))\nfor n in range(10):\n    ax = plt.subplot(5,5,n+1)\n    img, pred = superimposed_cam(covid_filenames[n])\n    plt.imshow(img)\n    plt.title(pred)\n    plt.axis(\"off\")\nfor n in range(15, 25):\n    ax = plt.subplot(5,5,n+1)\n    img, pred = superimposed_cam(pneumonia_filenames[n])\n    plt.imshow(img)\n    plt.title(pred)\n    plt.axis(\"off\")","cc8b0ca0":"# Visualize class activation mapping\n\nLet's compare what the model uses to classify COVID-19 X-rays versus Pneumonia X-rays.","fffcc03b":"The following functions will help us format our dataset into the necessary (image, label) tuple for easy training. We also one-hot encode our labels (i.e. [1 0 0] means NORMAL).","6c074f1c":"# Make grad-CAM heatmap\n\nLet's define our grad-CAM function. We need to identify our last convolution layer. For our model, this would be the convolution layer within our `sequential_3` model in our summary above. Since we only need the outputs of the layer and not the actual layer itself, we can specify `sequential_3`, our 7th layer in oour model, as the last convolution layer.\n\nWe also need to specifiy our classifying layers. The flatten layer and the layers that follow it does the classifying for us so we'll label `layers[-5:]` as our classifying layers.","baed96f6":"# Visualize our images","ca172710":"# Load the images","de98659d":"We need to get the output of the last convolution layer. Because I used Sequential instead of Functional API for my pneumonia model in my other notebook, the blocks that I used show us as nested Sequential models instead of as individual layers. That's not a problem because we can look at the structures of internal models as well.","8cf0b9d2":"# Evaluate our model","5c09c6b4":"# Introduction and Set-up\n\nGradient-weighted class activation mapping is a great way to better understand what's happening in your CNN. It helps visualize what the important parts of your image are for classification.\n\nIn this notebook, we're going to be using Grad-CAM to see what parts of the image are important for the CNN when classifying between COVID-19 and pneumonia X-rays. From an initial analysis, it may  seem that COVID-19 and pneumonia images will be relatively similar, given than many cases of COVID-19 cause pnuemonia. The Grad-CAM visualizations may help pinpoint what differentiates these classes of images. \n\nBefore we run our notebook, make sure to change the accelerator to TPU for quick training.","1d943dd9":"Divide the set into training, validation, and testing sets.","a73077c0":"Our original model from the pneumonia notebook was used as a binary classifier, but our dataset has three classes. Therefore, before we train our model, we have to pop our top level classifying layer and replace it with a 3-node dense layer. Thankfully, Keras API makes this change simple.","9fb6e3ea":"Define the function to get a NumPy repressentation of our image.","4a7400ad":"# Define superimposing function\n\nLet's superimpose the heatmap on the original image to visualize what the CNN marks as important and is used to classify the image.","e83f65af":"We correctly classify all of our testing images as well! Even though we had a very limited number of images, we could build a great model by loading in a pre-trained model.","a5415db2":"The top two rows are COVID-19 images and the botton two rows are Pneumonia images. Parts of the image that are redder on the rainbow spectrum are the more \"important\" parts of the image, as defined by the CNN, and purple parts of the image are less important.\n\nFor this set of 10 COVID-19 images, it seems that our model focuses on one lung more than the other. This may be biologically significant, or it may be an artificial artifact of the model. One of the hardest aspects of computational biology is maneuvering the differences between biological vs computational significance. Having collaborative discussions with experts in both fields will help answer some of these questions. For the Pnueumonia images, at least for the 10 images shown here, it seems that images are seen in a more holistic manner than the COVID-19 images by the model. However, we've only displayed 20 images here. Looking at the rest of the images will give us a clearer picture. ","25c0fc59":"# Load in our saved model\n\nWe want to load in the model from the [pneumonia notebook](https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays). Keras has an easy API to just load in previously trained models. The weights of the model will be preserved as well. First, let's define an early stopping callback.","e2a3b26b":"# Grad-CAM Setup\n\nCheck out Keras IO [Grad-CAM Code Example](https:\/\/keras.io\/examples\/vision\/grad_cam\/) for the original source code.\n\nTo start, let's first look at the structure of our model."}}