{"cell_type":{"881bf81d":"code","e52d3760":"code","670417cc":"code","340cb4ec":"code","6c7f9ee8":"code","9efba55f":"code","a2509061":"code","4c0ab655":"code","0bb0a892":"code","e746a6c4":"code","c5e0066f":"code","934706b0":"code","ecca1e00":"markdown","70ebc9c3":"markdown","08dafcf3":"markdown","48e2bbae":"markdown"},"source":{"881bf81d":"## creating a method to generate input\ndef create_input_data(number_of_rows, number_of_columns = 3):\n    \"\"\"\n\n        Method returns a 3D data equal to number of rows.\n        in this method we wil return a range of colors\n        so each row will have random values for 0-255\n        for red green blue channel\n\n    \"\"\"\n\n\n    return np.random.randint(0,255, (number_of_rows, number_of_columns))","e52d3760":"\n## our distance measure method. we will be using this to calculate distance between random point in dataset and the neuron.\n\n## We will also be using this to identify the weights around the Best Matching Unit, which will undergo reweighing\n\ndef calculate_distance(a, b):\n    \"\"\"\n    this method would calculate the euclidean distance between\n    points a and b.\n\n    essentially this will serve as a distance measure between our\n    node and weight neuron.\n\n    \"\"\"\n    ## sqrt(np.sum(a-b))\n    return np.linalg.norm(a - b)","670417cc":"def scale_input_data(x):\n    \"\"\"\n    \n    normalized input to the map to a range of 0-1.\n    SOMs work best under scaled inputs.\n    Using scikit learn MinMaXScaler. else we could have\n    done manually as well.\n\n    \"\"\"\n    return MinMaxScaler(feature_range = (0,1)).fit_transform(x)\n","340cb4ec":"\n## this particular method will calculate distance between a randomly selected X value from data set\n## and the neurons in the weight matrix.This will identify the lowest weight neuron(BEST MATCHING UNIT) and return the index of the weight matrix\n\n## the intuition behind this is: lowest weight is the most resembling the dataset selected\ndef compare_node_with_weight_matrix(x, w):\n\n    _units = {}\n    \n    for i, j in enumerate(w):\n        count = 0\n        \n        for neuron in j:\n\n            _units[(i,count)] = calculate_distance( x, neuron )\n            count += 1\n\n    ## sorting the dictionary according to value of each tuple\n    ## where each tuple represents the co ordinates\n    return sorted(_units, key = lambda x: _units[x])[0]\n\n","6c7f9ee8":"## this is our neighborhood function. this will be used to remeasure our weights. the intuition behind this is to slowly reduce\n## the sphere of influence so that with more number of iteration, we remeasure more precise number of \n## neurons around the best matching unit\n\ndef calculate_degree_of_influence(radius, distance):\n    \"\"\"\n\n    :param radius: the decayed value of radius\n    :param distance: the distance between current weight cell and the best matching unit\n\n    \"\"\"\n    denom = 2 * (radius)**2\n    if denom > np.finfo(np.float).eps: ## this prevents divide by zero issue. in case number is close to 0, we simply return 0\n        return np.exp(- distance\/denom) \n    else:\n        return 0","9efba55f":"def calculate_learning_rate(learning_rate, iteration_max, iteration, tau, final_learning_rate):\n    \"\"\"\n        learning rate will start decaying and reach a limit after iteration\n        reaches iteration_max value\n\n        if learning rate needs to be decayed, we need to pass not only the\n        iteration max as a non zero value, we would also need to pass\n        the range of learning rate\n        eg: learning_rate = (0.1 , 0.02) \n\n        the way it will work is\n\n        if iteration< itration_max\n            new_learning_rate = learning_rate[0] * exp(-iteration\/tau)\n        else\n            new_learning_rate = learning_rate[1]\n\n    \"\"\"       \n    \n    if iteration_max > 0 and final_learning_rate:\n        \n        if iteration < iteration_max:\n            return learning_rate * np.exp(-iteration\/tau)\n    \n        else:\n\n            return final_learning_rate\n\n    else:\n        return learning_rate\n","a2509061":"def decay_radius(radius, time_decay, iteration):\n    \"\"\"\n        time decay operation on the initial radius value given\n\n    \"\"\"\n    return radius * np.exp(- iteration\/time_decay )\n","4c0ab655":"\n## we will use this method to visualize the final color grid\n\ndef plot_in_self_organized_map(net):\n    fig = plot.figure()\n    # setup axes\n    ax = fig.add_subplot(111, aspect='equal')\n    ax.set_xlim((0, net.shape[0]+1))\n    ax.set_ylim((0, net.shape[1]+1))\n    ax.set_title('Self-Organising Map after iterations')\n\n    # plot the rectangles\n    for x in range(1, net.shape[0] + 1):\n        for y in range(1, net.shape[1] + 1):\n            ax.add_patch(patches.Rectangle((x-0.5, y-0.5), 1, 1,\n                        facecolor=net[x-1,y-1,:],\n                        edgecolor='none'))\n    plot.show()\n","0bb0a892":"def update_weights(x, _best_matching_index, weight_matrix, \\\n                    learning_rate, tau, sigma, iteration, \\\n                        iteration_max, final_learning_rate):\n    \"\"\"\n\n        how are the weights updated.\n        based on sigma parameter, we select the sphere of influence\n        w = w + learning_rate*neighborhood_function(sigma)*(x_random-w)\n\n    \"\"\"\n\n    _best_matching_unit = weight_matrix[_best_matching_index[0]][_best_matching_index[1]]\n    radius = decay_radius(sigma, tau, iteration)\n\n    for i, j in enumerate(weight_matrix):\n        count = 0\n        for neuron in j:\n\n            distance = calculate_distance( np.reshape(_best_matching_index,2), np.reshape((i,count),2) )\n\n            if distance <= radius**2: ## within sphere of influence\n                \n                degree_of_influence = calculate_degree_of_influence(radius, distance)\n                \n                learning_rate = calculate_learning_rate(learning_rate, iteration_max, \\\n                                    iteration, tau, final_learning_rate)\n                \n                new_weight = neuron + (learning_rate * degree_of_influence * (x - neuron))\n\n                # neuron = new_weight\n\n                weight_matrix[i][count] = new_weight\n            \n            count+= 1\n","e746a6c4":"\n## our goal will be to visualize the above 3channel 3d color as a \n## 2D color map, with similar colors grouping together closely.\n## so reds closes to reds, greens closer to green\n## we will use a self organizing map to act as a dimensionality reduction\n## technique. once reduced the network would be plotted on matplotlib\n## to check if we have actually reduced the dimensions\n\ndef self_organize_map(x, size_of_grid, epoch, learning_rate, tau, \\\n                        sigma, final_learning_rate = 0.02, iteration_max = 0):\n    \"\"\"\n        runs a SOM on the given input data.\n\n        :param x: the scaled input to the network\n\n        :param size_of_grid: the size of the weight matrix. expecting a tuple (k,p)\n                             remember the weight matrix is k*p*m dimension\n                             where each k*p cell is associated with a vector\n                             of length m\n\n        :param epoch: int value depicting total number of iterations to be executed\n\n        :param learning_rate: learning rate. for now we will use the same learning rate\n                              across all iterations. however generally a decaying learning \n                              rate performs better\n\n        :param tau: tau works as a decaying parameter. we will use the same tau parameter to\n                    decay the neighbor hood radius as well as learning rate if applicable\n\n        :param sigma: the sigma value will be used to depict the starting radius\n                      after every iteration we will decay the radius using the tau parameter\n\n        :param final_learning_rate: if learning rate needs to be decayed this is the value we will\n                                take after iteration count exceeds the iteration_max\n\n        :param iteration_max: this parameter will be used to dictate from which particular \n                            iteration the learning rate will stop decaying\n    \"\"\"\n    \n    dimensionality_of_input = x.shape[1]\n    \n    ## random weight matrix initialized.\n    weight_matrix = np.random.rand(size_of_grid[0],size_of_grid[1],dimensionality_of_input)\n\n    # sigma = max(weight_matrix[0], weight_matrix[1]) \/ 2\n    for _iteration in range(epoch):\n\n        ## select a random x input\n        x_random = x[random.randint(0, x.shape[0]-1)]\n\n        ## compare values of weight matrix with the above selected x_random\n        ## we will select the closest neuron - best matching unit BMU\n        _best_matching_index = compare_node_with_weight_matrix(x_random,weight_matrix)\n\n        ## once our BMU is selected. we will update the weights around it\n        ## based on the neighborhood function as sigma parameter(As radius)\n        ## and tau parameter( as decaying option)\n\n        update_weights(x_random,_best_matching_index, \\\n                            weight_matrix,learning_rate, \\\n                                tau, sigma, _iteration, iteration_max, final_learning_rate)\n\n    ## our weight_matrix is the network which has learnt the behavior and\n    ## correlations between the values. we will plot this grid on a matplot\n    ## for visualization aid\n\n    return weight_matrix    \n\n\n","c5e0066f":"\ndef execute():\n\n    x = scale_input_data(create_input_data(100)) ## we create a 100 row, 3d color map and scale it\n\n    epoch = 2000\n    weight_shape = (10,10)\n    learning_rate = 0.5\n    tau = 1000\n    sigma = 5 ## initial radius will be half way of the weight matrix dimension (5x,5y) \/2 = 2.5\n\n    weight_matrix = self_organize_map(x, weight_shape,epoch,learning_rate,tau,sigma, iteration_max= 200)\n\n    plot_in_self_organized_map(weight_matrix)\n    \n    \n","934706b0":"import numpy as np \nfrom matplotlib import pyplot as plot \nfrom sklearn.preprocessing import MinMaxScaler\nimport random\nfrom matplotlib import patches as patches\n\nexecute()","ecca1e00":"Self organizing map is a wonderful way to figure out patterns within data set provided. It creates a network (dimensions given by user, usually is 2D for easier visualization) where similar datasets cluster together.\n\n\nThe way SOM works is given below:\n\n1. Your input is a dataset of the dimension n * m. eg n rows with each row having m number of columns\n\n\n2. You create a random weight matrix of any dimensions ,say (p, j) . Each cell in the weight matrix had m number of values. so essentially you create a random weight matrix of (p,j,m).\n\n\n3. normalize your data set within values of 0 - 1. In this example we will be using sklearn.preprocessing.MinMaxScaler\n\n\n4. At this time we will provide a couple of hyperparameters, as given in the below steps\n\n\n5. Provide a value for number of iterations eg 400\/2000,etc\n\n\n6. Provide a learning rate. You may give a learning rate that remains constant through out the training phase, or you may chose to have a decaying learning rate( this is generally the case, and also what we will be using in our example below). IN case you provide a decaying learning rate, you will have to provide a range of learning rate. eg a starting learning rate and a lower learning range .\n\n\n7. If you are using decaying learning rate technique, we will also be giving a iteration count, till which we would be decaying our learning rate\n\n\n8. We will also be giving a starting radius, which will decide our sphere of influence. More on this later.\n\n\n9. Our last parameter would be tau parameter, which is a decaying degree. This will define how much our radius or learning rate will decay\n\n\n10. We will run the following number of iteration times:\n\n      -select a random x from our dataset\n      \n      -for each weight in the weight matrix, calculate the distance(could be euclidean,eg)\n      \n      -select the weight matrix with the least distance from x. Lets say this weight is W, known as BEST MATCHING UNIT\n      \n      -based on the radius parameter, calculate the neurons around W which falls within this circumference\n      \n      -based on the learning rate, recalculate the weights within the sphere of influence\n      \n      -recalculation will be : w = w + learning_rate*neighborhood_function(radius)*(x_random-w)\n      \n      -neighborhood function will be  exp(-distance\/2 * squared(radius), where distance is the distance between W and this particular weight\n      \n \n11. At the end of predecided number of iterations, we will have a network of weights. The weights have been dragged, recalculated to resemble similarities between original dataset. leading to clustering of patterns together \n\n\nour learning rate decaying will work as:\n\nif iteration< itration_max\n\n    new_learning_rate = starting_learning_rate * exp(-iteration\/tau)\n    \nelse\n\n    new_learning_rate = end_learning_rate\n            \n            \nour radius will decay as given:\n\n\nradius = radius * exp(-iteration\/tau)\n\n\n\nMore details can be found in the following resources:\n\nhttps:\/\/en.wikipedia.org\/wiki\/Self-organizing_map\n\nhttps:\/\/medium.com\/@navdeepsingh_2336\/self-organizing-maps-for-machine-learning-algorithms-ad256a395fc5\n","70ebc9c3":"# Self Organizing Map: Unsupervised\/Dimensionality Reduction\n\n### Custom python implementation of SOM \n#### Tested with the usual suspect: color map reduced to 2D grid","08dafcf3":"### As we can see above, we had an input of 100 rows of RGB values. SOM reduced the dimensions to 2D grid, and at the same time , colors of similar nature have clustered together to form a pattern \n\n\nP.S the above SOM algorithm can be used to feed in any data in order to find patterns. Go ahead, feed it MNIST data !","48e2bbae":"### Lets proceed to creating some of the utility function that will be used before we write the SOM algorithm\n\n#### significant amount of information has been provided in the doc strings. HOpe you find them useful!"}}