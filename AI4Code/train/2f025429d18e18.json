{"cell_type":{"c0e0e7b0":"code","9ee055de":"code","2b286947":"code","7eafecb9":"code","58bde0d7":"code","78bd480b":"code","35448988":"code","4fd4aab7":"code","279d7853":"code","bb73eae3":"code","7879ce0a":"code","8a767777":"code","39e1d551":"code","da476f70":"code","2422a4c5":"code","4e9ad638":"code","68af1947":"code","b098dc88":"code","e0fc485b":"code","d8a4c53d":"code","b7b0b7ea":"code","e96a1e8a":"code","0ada48b5":"code","f81b3b70":"code","d94997f2":"code","ca6dceb5":"markdown","095bcc24":"markdown"},"source":{"c0e0e7b0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ee055de":"# importing packages\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport tensorflow as tf\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2b286947":"# Load data\ndf = pd.read_csv('..\/input\/imdb-dataset-for-text-classification\/IMDB_Data.csv')","7eafecb9":"# Shape of the data\ndf.shape","58bde0d7":"# Printing 10 sample records from the data randomly\ndf.sample(10,random_state=1)","78bd480b":"# To Remove HTML tags \ndf.review = df.review.str.replace('<[^<]+?>', '')","35448988":"# Printing the total positive & Nrgative Reviews\ndf.sentiment.value_counts()","4fd4aab7":"# Encoding the target variable\ndf['sentiment'] = df['sentiment'].map({'negative':0,'positive':1})","279d7853":"df.head()","bb73eae3":"# Splitting the target and response variables\nx = df['review']\ny = df['sentiment']","7879ce0a":"# Creating word cloud for positive reviews\ncomments = df['review'].loc[df['sentiment']==1].values\nwordcloud = WordCloud(\n    width = 640,\n    height = 640,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(comments))\nfig = plt.figure(\n    figsize = (12, 8),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","8a767777":"# Creating word cloud for negative reviews\ncomments = df['review'].loc[df['sentiment']==0].values\nwordcloud = WordCloud(\n    width = 640,\n    height = 640,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(comments))\nfig = plt.figure(\n    figsize = (12, 8),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","39e1d551":"max_features = 20000\n\n# Providing the max length of text\nmax_text_length = 400","da476f70":"# Preparing the tokenizer\nx_tokenizer = text.Tokenizer(max_features)\nx_tokenizer.fit_on_texts(list(x))","2422a4c5":"# Save tokenizer for future use\nwith open('imdb_tokenizer.pkl', 'wb') as f:\n    pickle.dump(x_tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)","4e9ad638":"# integer encode the documents\nx_tokenized = x_tokenizer.texts_to_sequences(x)\n\n# padding the text to a max length of 400 words\nx_train_val= sequence.pad_sequences(x_tokenized, maxlen=max_text_length)","68af1947":"embedding_dim =100\nembeddings_index = dict()\n\n# loading 100 Dimensional GloVe embedding file\nf = open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt', encoding=\"utf8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:],dtype='float32')\n    embeddings_index[word]= coefs\nf.close()\n\nprint(f'Found {len(embeddings_index)} word vectors')","b098dc88":"# preparing embedding matrix\nembedding_matrix= np.zeros((max_features,embedding_dim))\nfor word, index in x_tokenizer.word_index.items():\n    if index>max_features-1:\n        break\n    else:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index]= embedding_vector","e0fc485b":"# Model Building\nfilters= 250\nkernel_size=3\nhidden_dims= 250\nmodel = Sequential()\nmodel.add(Embedding(max_features,\n                    embedding_dim,\n                    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n                    trainable=False))\nmodel.add(Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu'))\nmodel.add(MaxPooling1D())\nmodel.add(Conv1D(filters,\n                 5,\n                 padding='valid',\n                 activation='relu'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(hidden_dims, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","d8a4c53d":"# Compiling the model\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","b7b0b7ea":"# Splitting the data into train and validation set\nx_train,x_val,y_train,y_val = train_test_split(x_train_val,y,test_size=0.35, random_state=1)","e96a1e8a":"# Fitting and training the model\nbatch_size= 32\nepochs = 5\nhist = model.fit(x_train,y_train,\n                    batch_size= batch_size,\n                    epochs=epochs,\n                    validation_data= (x_val,y_val)\n                    )","0ada48b5":"# Plot accuracy scores\nplt.plot(hist.history['accuracy'], label='train')\nplt.plot(hist.history['val_accuracy'], label='validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training loss vs. Epochs')\nplt.legend()\nplt.show()","f81b3b70":"# Plot loss\nplt.plot(hist.history['loss'], label='train')\nplt.plot(hist.history['val_loss'], label='validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training loss vs. Epochs')\nplt.legend()\nplt.show()","d94997f2":"# Saving the model\nmodel.save('imdb_cnn.h5')","ca6dceb5":"The pre-Trained GloVe embeddings package can be downloaded here- http:\/\/nlp.stanford.edu\/data\/glove.6B.zip","095bcc24":"**Hello Kagglers**,\n\n> Happy to see you here !\n\n> > **Text classification:-**\n is the process of categorizing the text into a group of words. By using NLP, text classification can automatically analyze text and then assign a set of predefined tags or categories based on its context. NLP is used for sentiment analysis, topic detection, and language detection. \n\n> > In this notebook,  we will do the following:\n> > > 1. EDA\n> > > 2. Tokenization\n> > > 3. Removing Stopwords\n> > > 4. Positive & Negative sentiment analysis\n> > > 5. Wordcloud\n> > > 6. Word2vec of text corpus\n\n> If there is any code snippet in the notebook, you need better explanation about, Please let me know in comments.\n\n**Rise & Shine!**"}}