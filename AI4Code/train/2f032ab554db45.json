{"cell_type":{"12cd14ac":"code","cb5bdf44":"code","e2a602ec":"code","d5649a34":"code","11d0e28e":"code","36713d56":"code","e84ec419":"code","8b1f5859":"code","64e3ab3f":"code","598fa02b":"code","3694b361":"code","8cbe4eda":"code","7e5f1c63":"code","7041f754":"code","8aa82e6d":"code","e81cbc3c":"code","bc282141":"markdown","58ff30b4":"markdown"},"source":{"12cd14ac":"!pip install jax jaxlib dm-haiku","cb5bdf44":"!pip install gym","e2a602ec":"import haiku as hk\n\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jrandom\n\nfrom jax import grad, value_and_grad, vmap\n\nimport random\n\nrandom.seed(0)\nrng = jrandom.PRNGKey(0)","d5649a34":"def actor(x):\n    model = hk.Sequential([hk.nets.MLP([128, 128, 2])])\n    return model(x)\n\nactor = hk.transform(actor)\n\nactor_params = actor.init(rng, jrandom.normal(rng, (4,)))","11d0e28e":"def critic(x):\n    model = hk.Sequential([hk.nets.MLP([256, 256, 1])])\n    return model(x)\n\ncritic = hk.transform(critic)\n\ncritic_params = critic.init(rng, jrandom.normal(rng, (4,)))","36713d56":"def actor_eval(actor_params, rng, x):\n    result = actor.apply(actor_params, rng, x)\n    action_probs = jax.nn.softmax(result)\n    return action_probs","e84ec419":"def select_action(action_probs):\n    return random.choices(range(len(action_probs)), weights=action_probs)[0]","8b1f5859":"def critic_eval(critic_params, rng, x):\n    return critic.apply(critic_params, rng, x)[0]","64e3ab3f":"from jax import tree_util\n\ndef add_trees(tree1, tree2):\n    return tree_util.tree_multimap(lambda t1, t2: t1 + t2,\n                                   tree1,\n                                   tree2)\n\ndef scale_tree(x: float, tree):\n        return tree_util.tree_map(lambda t: x * t, tree)","598fa02b":"actor_step_size = 1e-5\ncritic_step_size = 3e-4 # This should be greater than actor_step_size.\nconsecutive_solutions_required = 3 # How many times should we have to solve the task in a row to stop training?\nmax_episodes = 2000 # How many episodes should we go through before giving up?","3694b361":"def grad_log_prob(actor_params, rng, x, action):\n    def get_log_prob(actor_params):\n        action_probs = actor_eval(actor_params, rng, x)\n        return jnp.log(action_probs[action])\n    return grad(get_log_prob)(actor_params)","8cbe4eda":"import gym\n\nenv = gym.make(\"CartPole-v1\")\nobs = env.reset()\n\n# This accumulator speeds up the parameter update computation.\ntotal_reward = 0\n# This is a list of tuples containing (state, action, reward).\nepisode_SARs = []\n\n# Used for plotting.\ntotal_rewards = []\n\nconsecutive_solutions = 0\nepisode = 1\nwhile True:\n    action = select_action(actor_eval(actor_params, rng, obs))\n    prev_obs = obs\n    obs, reward, done, info = env.step(action)\n    \n    # Update episode training data.\n    total_reward += reward\n    episode_SARs.append((prev_obs, action, reward))\n    \n    # Compute TD error.\n    def differentiable_critic_estimate(critic_params):\n        return critic_eval(critic_params, rng, obs)\n    \n    critic_estimate, grad_critic_estimate = value_and_grad(differentiable_critic_estimate)(critic_params)\n    td_error = reward + critic_eval(critic_params, rng, obs) * (1 - done) - critic_estimate\n    \n    # Update critic parameters.\n    critic_params = add_trees(critic_params,\n                              scale_tree(td_error * critic_step_size,\n                                         grad_critic_estimate))\n    \n    # Update actor parameters.\n    actor_params = add_trees(actor_params,\n                             scale_tree(td_error * actor_step_size,\n                                        grad_log_prob(actor_params, rng, obs, action)))\n    \n    if done:\n        total_rewards.append(total_reward)\n        print(f\"Episode {episode} reward achieved: {total_reward}.\")\n        \n        if total_reward >= env.spec.reward_threshold:\n            consecutive_solutions += 1\n            if consecutive_solutions == consecutive_solutions_required:\n                # Training complete.\n                print(\"Task solved!\")\n                break\n        else:\n            consecutive_solutions = 0\n            \n        if episode == max_episodes:\n            # Task was not solved, training ending.\n            print(f\"Maximum episode {episode} reached, task not solved.\")\n            break\n        \n        # Reset the state to initial conditions.\n        obs = env.reset()\n        episode_SARs = []\n        episode += 1\n        total_reward = 0","7e5f1c63":"from matplotlib import pyplot as plt\n\nplt.plot(total_rewards)\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Total reward\")\nplt.show()","7041f754":"!apt-get install python-opengl -y\n!pip install pyvirtualdisplay","8aa82e6d":"from pyvirtualdisplay import Display\nimport os\n\ndisplay = Display(visible=0, size=(1400, 900))\ndisplay.start()\nos.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display._obj._screen)","e81cbc3c":"from matplotlib import animation, rc\n\nfig = plt.figure()\n\nframe = []\n\ntotal_reward = 0\nobs = env.reset()\n\nwhile True:\n    action = select_action(actor_eval(actor_params, rng, obs))\n    observation, reward, done, info = env.step(action)\n    \n    total_reward += reward\n    \n    img = plt.imshow(env.render(\"rgb_array\"))\n    frame.append([img])\n    if done:\n        break\n\nanim = animation.ArtistAnimation(fig, frame, interval=100, repeat_delay=1000, blit=True)\nrc(\"animation\", html=\"jshtml\")\n\nprint(f\"Final reward: {total_reward}.\")\n\nanim","bc282141":"![Sutton & Barto excerpt](https:\/\/i.stack.imgur.com\/h5t1V.png)","58ff30b4":"# CartPole-1AC\n*A one-step actor-critic agent.*\n\nThis is an implementation of the \"One-Step Actor-Critic (episodic), for estimating $\\pi_{\\vec\\theta} \\approx \\pi_*$\" algorithm described in `Chapter 13: Policy Gradient Methods` of Sutton & Barto's reinforcement learning text. \n\nThe agent does not perform return discounting.\n\nAt time of writing, there is little commentary online about this particular algorithm beyond a couple of [Reddit comments](https:\/\/www.reddit.com\/r\/reinforcementlearning\/comments\/gtqj30\/actorcritic_implementation_not_learning\/fse7nbo?utm_source=share&utm_medium=web2x&context=3) discussing that it doesn't learn a good policy for even the simplest environments. I wanted to reproduce this agent since it is a distilled form of the actor-critic agents found in the literature. We see in the final result that the agent unfortunately does not learn a satisfactory policy for CartPole-v1. **You are welcome to copy this notebook and play with hyperparameters in efforts to get it to learn a good policy. Please inform me what worked for you.**"}}