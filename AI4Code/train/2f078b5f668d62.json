{"cell_type":{"10eafa8e":"code","3caab1fd":"code","f04344c9":"code","f67492af":"code","f8da8a3d":"code","6c7f4476":"code","bd0ed1d1":"code","ff4fd008":"code","c25ca483":"code","53c99785":"code","933b86b4":"code","4b753b26":"code","4612b51f":"code","43c4e978":"code","57d970bf":"code","bec5290b":"code","1f9c7f9e":"code","89c260ea":"code","226101d8":"code","aff5968d":"code","b3d90da8":"code","26a7eebf":"code","cb559bf2":"code","580f5420":"code","bdeff454":"code","faa5ee75":"code","77378f35":"code","a55d9373":"code","1ed412e9":"code","77ffecac":"code","1e9847ce":"code","d84fa987":"code","22a94042":"code","bf5054dd":"code","8b0fcd37":"code","fa5b734f":"code","694becc6":"code","62f92ee4":"code","72fd62b4":"code","d8a773ce":"code","991520db":"code","4b70baef":"code","57761b56":"code","593798ea":"code","29768a93":"code","52efb0e7":"code","7590ebb2":"code","61d12e0f":"code","dc6e6e46":"code","8a27674a":"code","6c8be4f9":"code","73dfca4a":"code","7e13af96":"code","d5c92ba3":"code","361e48e7":"code","129b2abf":"code","445956cc":"code","bdeb2236":"code","37db67ca":"code","d05c77d1":"code","a322d3ca":"code","adfd4cd8":"code","c97c1321":"code","3a3979d1":"code","d97cb36c":"code","f2df8aa8":"code","b7e3f1e5":"code","c60d1236":"code","b520ed41":"code","341b8df5":"code","80df0e25":"code","caf231c6":"code","70d9d72a":"code","b37e21d0":"code","5f6f8981":"code","59caae9d":"code","33a49d43":"code","fdd0e9a3":"code","698d32f1":"code","198ac620":"code","cbc85466":"code","ec2e73c0":"code","6a1ddeaa":"code","e5f7de47":"code","0bf06b3d":"code","0dd70bf5":"code","3f540424":"code","949be030":"code","cf5f36df":"code","76512c67":"code","1e2688df":"code","04914e17":"code","f239670a":"code","a33af8f5":"code","cbcb3154":"code","3efbab3a":"code","43d828ee":"markdown","ab9d8c43":"markdown","daa36d12":"markdown","1eceba04":"markdown","cdd41b66":"markdown","c23dfece":"markdown","b4071cf7":"markdown","3afc66de":"markdown","35d70211":"markdown","14db7036":"markdown","b7bf1700":"markdown","5280d85a":"markdown","92f58750":"markdown","1ad141e8":"markdown","d7986341":"markdown","4d054f20":"markdown","9de56a0e":"markdown","351e3d6a":"markdown","89da7a82":"markdown","82d1f04d":"markdown","0a5f039f":"markdown","ec1843d0":"markdown","d3280dc0":"markdown","878d4360":"markdown","d5980f17":"markdown","e8c73109":"markdown","91e12181":"markdown","8cb960f5":"markdown","ad66df9d":"markdown","9a881363":"markdown","ef62a9e9":"markdown","6ac37c26":"markdown"},"source":{"10eafa8e":"Some minor changes","3caab1fd":"# Warning Libraries :\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Scientific and Data Manipulation Libraries :\nimport pandas as pd\nimport numpy as np\nimport math\nimport gc\nimport os\n\n# ML Libraries :\nfrom sklearn.preprocessing            import LabelEncoder, OneHotEncoder \nfrom sklearn.preprocessing            import StandardScaler, MinMaxScaler, Normalizer, RobustScaler, MaxAbsScaler\nfrom sklearn.model_selection          import KFold, StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.tree                     import DecisionTreeClassifier\nfrom sklearn.ensemble                 import VotingClassifier, RandomForestClassifier\nfrom sklearn.metrics                  import f1_score, confusion_matrix, classification_report\n\n                    \n# Data Visualization Libraries :\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px","f04344c9":"rs=1331 ##random_state","f67492af":"train= pd.read_csv('..\/input\/hranalysis\/train.csv')\ntest= pd.read_csv('..\/input\/hranalysis\/test.csv')","f8da8a3d":"test.head()","6c7f4476":"train.head()","bd0ed1d1":"print(train.columns)\nprint(\"*\"*100)\nprint(test.columns)","ff4fd008":"print(\"Train data shape\",train.shape)\nprint(\"Test data shape\",test.shape)","c25ca483":"train.info()","53c99785":"train.describe(include='all')","933b86b4":"test.info()","4b753b26":"test.describe(include='all')","4612b51f":"train.isna().sum()","43c4e978":"#Using missingno to visualize null values in train data\nimport missingno as msno\nmsno.bar(train, color = '#6389df', figsize = (10,8))  \n","57d970bf":"test.isna().sum()","bec5290b":"#Using missingno to visualize null values in test data\nmsno.bar(test, color = '#6389df', figsize = (10,8))  \n","1f9c7f9e":"corr=train.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr,square=True,annot=True)","89c260ea":"train.dtypes","226101d8":"# Let\u2019s plot the distribution of each feature\ndef plot_distribution(dataset, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width,height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(float(dataset.shape[1]) \/ cols)\n    for i, column in enumerate(dataset.columns):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.set_title(column)\n        if dataset.dtypes[column] == np.object:\n            g = sns.countplot(y=column, data=dataset)\n            substrings = [s.get_text()[:18] for s in g.get_yticklabels()]\n            g.set(yticklabels=substrings)\n            plt.xticks(rotation=25)\n        else:\n            g = sns.distplot(dataset[column])\n            plt.xticks(rotation=25)\n    \nplot_distribution(train,cols=2, width=30, height=60, hspace=0.45, wspace=0.5)","aff5968d":"\ndef plot_bivariate_bar(dataset, hue, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n    dataset = dataset.select_dtypes(include=[np.object])\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width,height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(float(dataset.shape[1]) \/ cols)\n    for i, column in enumerate(dataset.columns):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.set_title(column)\n        if dataset.dtypes[column] == np.object:\n            g = sns.countplot(y=column, hue=hue, data=dataset)\n            substrings = [s.get_text()[:10] for s in g.get_yticklabels()]\n            g.set(yticklabels=substrings)\n            \n            \n            \nplot_bivariate_bar(train, hue=train.is_promoted, cols=1, width=10, height=35, hspace=0.4, wspace=0.5)","b3d90da8":"#unique value in education feature\ntrain.education.value_counts()","26a7eebf":"#plotting a pie chart\nsize = [36669,14925,805]\nlabel=[\"Bachelor's\",\"Master's & above\",'Below Secondary']\ncolor=['#6389df','#1f2b6c','#a3ccf4']\nexplode = [0.1, 0.2 , 0.3]\nplt.figure(figsize=(8,8))\nplt.pie(size,labels=label,colors=color,explode=explode,shadow=True,autopct=\"%.1f%%\")\nplt.title(\"Pie Chart of the Employees Degrees\", fontsize = 20)\nplt.axis('off')\nplt.legend(title='Education Degrees')\nplt.show()","cb559bf2":"#unique value in gender feature\ntrain.gender.value_counts()","580f5420":"#plotting a pie chart\nsize = [38496,16312]\nlabel=[\"Male\",\"Female\"]\ncolor=['#6389df','#1f2b6c']\nexplode = [0.1, 0.2 ]\nplt.figure(figsize=(8,8))\nplt.pie(size,labels=label,colors=color,explode=explode,shadow=True,autopct=\"%.1f%%\")\nplt.title(\"Pie Chart of the GenderGap\", fontsize = 20)\nplt.axis('off')\nplt.legend(title='Gender')\nplt.show()","bdeff454":"plt.subplots(figsize=(15,5))\nsns.countplot(x = 'education', data = train, hue = 'gender', palette = 'Paired')\nplt.title('Showing Degree & Gender ratio', fontsize = 20)\nplt.show()","faa5ee75":"train['recruitment_channel'].value_counts()","77378f35":"size=[30446,23220,1142]\nlabel=[\"Other\",\"Sourcing\",'Referred']\ncolor=['#6389df','#1f2b6c','#a3ccf4']\nexplode=[.05,.05,.05]\nplt.figure(figsize=(8,8))\nplt.pie(size,labels=label,colors=color, startangle=90,shadow=True,autopct=\"%.2f%%\",pctdistance=.85)\n\ncenter_circle=plt.Circle((0,0),.7,fc='white')\nfig=plt.gcf()\nfig.gca().add_artist(center_circle)\n\nplt.title('A Pie Chart Representing Recruitment_Channel', fontsize = 30)\nplt.axis('off')\nplt.legend()\nplt.show()\n","a55d9373":"#### Check most popular department\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nstopword = set(STOPWORDS)\n\nwordcloud = WordCloud(stopwords = stopword).generate(str(train['department']))\n\nplt.rcParams['figure.figsize'] = (15, 8)\nprint(wordcloud)\nplt.imshow(wordcloud)\nplt.title('Most Popular Departments', fontsize = 30)\nplt.axis('off')\nplt.show()","1ed412e9":"plt.figure(figsize=(15,5))\nsns.distplot(train['age'],color='#6389df')\nplt.title('Distribution of Age of Employees', fontsize = 30)\nplt.grid(axis='both')\nplt.show()","77ffecac":"#pie chart for the KPIs_met\ntrain['KPIs_met >80%'].value_counts()","1e9847ce":"size = [35517, 19291]\nlabels = \"Not Met KPI > 80%\", \"Met KPI > 80%\"\ncolor=['#6389df','#a3ccf4']\nexplode = [0, 0.1]\nplt.figure(figsize=(8,8))\nplt.pie(size, labels = labels, colors = color, explode = explode, shadow = True, autopct = \"%.2f%%\")\nplt.title('A Pie Chart Representing Gap in Employees in terms of KPI', fontsize = 30)\nplt.axis('off')\nplt.legend()\nplt.show()","d84fa987":"train['awards_won?'].value_counts()","22a94042":"size = [53538,1270]\nlabels = \"Awards Won\", \"NO Awards Won\"\ncolor=['#6389df','#a3ccf4']\nexplode = [0, 0.1]\n\nmy_circle = plt.Circle((0, 0), 0.7, color = 'white')\n\nplt.figure(figsize=(8,8))\nplt.pie(size, labels = labels, colors = color, explode = explode, shadow = True, autopct = \"%.2f%%\")\nplt.title('A Pie Chart Representing Gap in Employees in terms of KPI', fontsize = 30)\np = plt.gcf()\np.gca().add_artist(my_circle)\nplt.legend()\nplt.show()","bf5054dd":"size = [50140, 4668]\nlabels = \"NOT Promoted \", \"Promoted \"\ncolor=['#a3ccf4','#6389df']\nexplode = [0, 0.1]\n\n#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.rcParams['figure.figsize'] = (8, 8)\nplt.pie(size, labels = labels, colors = color, explode = explode, shadow =False, autopct = \"%.2f%%\",startangle=180)\nplt.title('Showing a Percentage of employees who Promoted ' , fontsize = 20)\nplt.axis('off')\nplt.legend()\nplt.show()","8b0fcd37":"## BIVARIATE FEATURE ANALYSIS","fa5b734f":"train.columns","694becc6":"a=['department', 'region', 'education', 'gender', 'no_of_trainings', 'age', 'previous_year_rating',\n       'length_of_service', 'KPIs_met >80%', 'awards_won?']\nfor i in a:\n  data = pd.crosstab(train[i],train['is_promoted'])\n  data.div(data.sum(1).astype('float'), axis = 0).plot(kind = 'bar', stacked = True, figsize = (15, 5), color = ['#a3ccf4','#6389df'])\n\nplt.legend()\nplt.show()","62f92ee4":"import plotly.express as px\nfig = px.parallel_categories(train[['department','education','gender','previous_year_rating','KPIs_met >80%',\n                                    'recruitment_channel',\n                                   'is_promoted']], \n                             color=\"is_promoted\", \n                             color_continuous_scale=px.colors.sequential.Aggrnyl  )\nfig.show()","72fd62b4":"#  Removes Data Duplicates while Retaining the First one - Similar to SQL DISTINCT :\ndef remove_duplicate(data):\n    \n    print(\"BEFORE REMOVING DUPLICATES - No. of Rows = \",data.shape[0])\n    data.drop_duplicates(keep=\"first\", inplace=True) \n    print(\"AFTER REMOVING DUPLICATES  - No. of Rows = \",data.shape[0])\n    return \"Checked Duplicates\"\n# Remove Duplicates from \"train\" data :\nremove_duplicate(train)\n# No Duplicates at all !!!","d8a773ce":"##missing value function which return an dataframe with total null values and percentage\ndef missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data(train)","991520db":"missing_data(test)","4b70baef":"train.previous_year_rating=train.previous_year_rating.fillna(0)\ntest.previous_year_rating=test.previous_year_rating.fillna(0)","57761b56":"train['Fresher']=train['previous_year_rating'].apply(lambda x: 'Fresher' if x==0 else 'Experienced')\ntest['Fresher']=test['previous_year_rating'].apply(lambda x: 'Fresher' if x==0 else 'Experienced')","593798ea":"train['education']=train['education'].ffill(axis=0)\ntrain['education']=train['education'].bfill(axis=0)\n\ntest['education']=test['education'].ffill(axis=0)\ntest['education']=test['education'].bfill(axis=0)","29768a93":"display(missing_data(train))\ndisplay(missing_data(test))\n","52efb0e7":"#BINNING THE AGE FEATURE IN 20-29 , 29-39 , 39-49 \nsns.distplot(train['age'])\n\ntrain['age'] = pd.cut( x=train['age'], bins=[20, 29, 39, 49], labels=['20', '30', '40'] )\ntest['age']  = pd.cut( x=test['age'], bins=[20, 29, 39, 49],  labels=['20', '30', '40'] )","7590ebb2":"train.age.value_counts(dropna=False)","61d12e0f":"train.drop(['employee_id'],axis=1,inplace=True)\ntest_d=test","dc6e6e46":"test_d.drop(['employee_id'],axis=1,inplace=True)","8a27674a":"def data_encoding( encoding_strategy , encoding_data , encoding_columns ):\n    \n    if encoding_strategy == \"LabelEncoding\":\n        print(\"IF LabelEncoding\")\n        Encoder = LabelEncoder()\n        for column in encoding_columns :\n            print(\"column\",column )\n            encoding_data[ column ] = Encoder.fit_transform(tuple(encoding_data[ column ]))\n        \n    elif encoding_strategy == \"OneHotEncoding\":\n        print(\"ELIF OneHotEncoding\")\n        encoding_data = pd.get_dummies(encoding_data)\n        \n    dtypes_list =['float64','float32','int64','int32']\n    encoding_data.astype( dtypes_list[0] ).dtypes\n    \n    return encoding_data","6c8be4f9":"encoding_columns  = [ \"region\", \"age\",\"department\", \"education\", \"gender\", \"recruitment_channel\" ]\nencoding_strategy = [ \"LabelEncoding\", \"OneHotEncoding\"]\n\ntrain_encode = data_encoding( encoding_strategy[1] , train , encoding_columns )\ntest_encode =  data_encoding( encoding_strategy[1] , test  , encoding_columns )","73dfca4a":"test_encode.head()","7e13af96":"train_encode.head()","d5c92ba3":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, RobustScaler, MaxAbsScaler\ndef data_scaling( scaling_strategy , scaling_data , scaling_columns ):\n    \n    if    scaling_strategy ==\"RobustScaler\" :\n        scaling_data[scaling_columns] = RobustScaler().fit_transform(scaling_data[scaling_columns])\n        \n    elif  scaling_strategy ==\"StandardScaler\" :\n        scaling_data[scaling_columns] = StandardScaler().fit_transform(scaling_data[scaling_columns])\n        \n    elif  scaling_strategy ==\"MinMaxScaler\" :\n        scaling_data[scaling_columns] = MinMaxScaler().fit_transform(scaling_data[scaling_columns])\n        \n    elif  scaling_strategy ==\"MaxAbsScaler\" :\n        scaling_data[scaling_columns] = MaxAbsScaler().fit_transform(scaling_data[scaling_columns])\n        \n    else :  # If any other scaling send by mistake still perform Robust Scalar\n        scaling_data[scaling_columns] = RobustScaler().fit_transform(scaling_data[scaling_columns])\n    \n    return scaling_data","361e48e7":"scaling_st=[\"RobustScaler\" ,\"StandardScaler\",\"MinMaxScaler\",\"MaxAbsScaler\"]\n\ntrain_scale=data_scaling(scaling_st[0],train_encode,train_encode.columns)\ntest_scale=data_scaling(scaling_st[0],test_encode,test_encode.columns)","129b2abf":"X=train_scale.drop(['is_promoted'],axis=1)\nY=train.is_promoted","445956cc":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y)","bdeb2236":"Y_train.value_counts(normalize=True)*100","37db67ca":"def oversample(X,Y):\n    over_sample = SMOTETomek(random_state=rs)\n    X_over,Y_over = over_sample.fit_resample(X,Y)\n    return X_over,Y_over","d05c77d1":"X_train_os,Y_train_os=oversample(X_train,Y_train)","a322d3ca":"clf = RandomForestClassifier(n_estimators=100, random_state=0)\nclf.fit(X_train, Y_train)\n\n### FEATURE SCORES \nfeature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\nprint(feature_scores)\n\n### PLOT TO VISUALIZE\nsns.barplot(x=feature_scores, y=feature_scores.index)\n# Add labels to the graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\n# Add title to the graph\nplt.title(\"Visualizing Important Features\")\n# Visualize the graph\nplt.show()\n","adfd4cd8":"from sklearn.ensemble          import RandomForestClassifier\nfrom sklearn.tree              import DecisionTreeClassifier\n\nfrom sklearn.metrics           import accuracy_score\nfrom sklearn.metrics           import classification_report\nfrom sklearn.metrics           import confusion_matrix\n\nfrom sklearn.model_selection   import RandomizedSearchCV\nfrom sklearn.model_selection   import KFold,cross_val_score\n","c97c1321":"## PASSING THE TRAIN DATA IN THE CROSS VALIDATION\nkf=KFold(n_splits=5,random_state=rs,shuffle=True)\ncnt = 1\n# split()  method generate indices to split data into training and test set.\nfor train_index, test_index in kf.split(X_train_os, Y_train_os):\n    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n    cnt+=1","3a3979d1":"score = cross_val_score(RandomForestClassifier(random_state= rs), X_train_os, Y_train_os, cv= kf, scoring=\"accuracy\")\nprint(f'Scores for each fold are: {score}')\nprint(f'Average score: {\"{:.2f}\".format(score.mean())}')","d97cb36c":"rfc=RandomForestClassifier(random_state=rs)\nrfc.fit(X_train_os,Y_train_os)\ny_pred_rf=rfc.predict(X_train_os)\nprint(accuracy_score(y_pred_rf,Y_train_os))","f2df8aa8":"cm = confusion_matrix(Y_train_os, y_pred_rf)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu',fmt=\".1f\")","b7e3f1e5":"y_pred_test=rfc.predict(X_test)\nprint(accuracy_score(y_pred_test,Y_test))","c60d1236":"cm = confusion_matrix(Y_test, y_pred_test)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu',fmt=\".1f\")","b520ed41":"print(classification_report(Y_test,y_pred_test))","341b8df5":"#Randomized Search CV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","80df0e25":"random_grid={'n_estimators': n_estimators,\n            'max_features': max_features,\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'min_samples_leaf': min_samples_leaf}","caf231c6":"rf=RandomForestClassifier()","70d9d72a":"rf_random = RandomizedSearchCV(estimator = rf,param_distributions = random_grid,\n                                scoring='f1',\n                                n_iter = 10, cv = 5,\n                                verbose=2, random_state=rs,\n                                n_jobs = 1)\nrf_random.fit(X_train_os,Y_train_os)","b37e21d0":"rf_random.best_params_","5f6f8981":"rfc= RandomForestClassifier(random_state=rs,\n                            n_estimators=1100,\n                            min_samples_split=5,\n                            max_features='auto',\n                            min_samples_leaf= 1,\n                            max_depth=20,oob_score=True)\nrfc.fit(X_train_os,Y_train_os)","59caae9d":"y_pred_rf_ht=rfc.predict(X_train_os)\nprint(accuracy_score(y_pred_rf_ht,Y_train_os))\ncm = confusion_matrix(Y_train_os, y_pred_rf_ht)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu',fmt=\".1f\")","33a49d43":"y_pred_test_ht=rfc.predict(X_test)\nprint(accuracy_score(y_pred_test_ht,Y_test))","fdd0e9a3":"cm = confusion_matrix(Y_test, y_pred_test_ht)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu',fmt=\".1f\")","698d32f1":"dtc=DecisionTreeClassifier(random_state=rs)\nscore = cross_val_score(dtc, X_train_os, Y_train_os, cv= kf, scoring=\"accuracy\")\nprint(f'Scores for each fold are: {score}')\nprint(f'Average score: {\"{:.2f}\".format(score.mean())}')","198ac620":"def hyperparameter_tuning(X,Y,rf):\n#Randomized Search CV\n# Number of features to consider at every split\n    max_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\n    max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\n    min_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\n    min_samples_leaf = [1, 2, 5, 10]\n\n    random_grid={\n            'max_features': max_features,\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'min_samples_leaf': min_samples_leaf}\n\n    rf_random = RandomizedSearchCV(estimator = rf,param_distributions = random_grid,\n                                scoring='f1', \n                                n_iter = 10, cv = 5,\n                                verbose=0, random_state=rs,\n                                n_jobs = 1)\n    rf_random.fit(X,Y)\n    return rf_random.best_params_","cbc85466":"param_dt=hyperparameter_tuning(X_train_os,Y_train_os,dtc)","ec2e73c0":"## Printing the best parameters obtained after randomizesearch CV or hyperparameter tuning\nprint(param_dt)","6a1ddeaa":"dtc=DecisionTreeClassifier(random_state=rs,\n                           min_samples_split=10,\n                           min_samples_leaf=2,\n                           max_features='auto',\n                           max_depth=25)\ndtc.fit(X_train_os,Y_train_os)\ny_pred_train_dc_ht=dtc.predict(X_train_os)\ny_pred_test_dt_ht=dtc.predict(X_test)\nprint('Test Accuracy',accuracy_score(y_pred_test_dt_ht,Y_test))\nprint('Train Accuracy',accuracy_score(y_pred_train_dc_ht,Y_train_os))","e5f7de47":"# Boosting Algorithms :\nfrom xgboost                          import XGBClassifier\nfrom catboost                         import CatBoostClassifier\nfrom lightgbm                         import LGBMClassifier\n\nfrom scipy.stats                      import randint","0bf06b3d":"mod= CatBoostClassifier(random_state=rs)\n\npar={'max_depth':[5,10,None],\n              'n_estimators':[200,300,400,500,600],'learning_rate':[0.1,0.01,0.001]}\ndef hyperparameter_tuning(mod,param_d,p,q):\n    rdmsearch=  RandomizedSearchCV(mod, param_distributions=param_d,n_jobs=-1,cv=9,scoring='roc_auc')\n    rdmsearch.fit(p,q)\n    ht_params = rdmsearch.best_params_\n    ht_score = rdmsearch.best_score_\n    return ht_params, ht_score\n\n\nrf_parameters, rf_ht_score = hyperparameter_tuning(mod, par,  X_train_os,Y_train_os)\n","0dd70bf5":"print(rf_parameters, rf_ht_score)","3f540424":"mod=XGBClassifier(random_state=rs)\nparam_tuning = {\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5, 7, 10], \n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [100, 200, 500]\n                }\nrf_parameters_xgb, rf_ht_score_xgb = hyperparameter_tuning(mod, param_tuning,  X_train_os,Y_train_os)","949be030":"print(rf_parameters_xgb, rf_ht_score_xgb)","cf5f36df":"lgb = LGBMClassifier()\nlgb.fit(X_train_os,Y_train_os)\n\nlgb_pred = lgb.predict(X_test)\n\nprint(\"Training Accuracy :\", lgb.score(X_train_os, Y_train_os))","76512c67":"# Create a Dictionary (Key->Value Pairs) for \"ML Model Name\"-> \"ML Model Functions with Hyper-Parameters\" :\n\nClassifiers = {'0.XGBoost' : XGBClassifier(learning_rate =0.1, \n                                           n_estimators=394, \n                                           max_depth=10, \n                                           subsample = 0.50, \n                                           verbosity = 0,\n                                           scale_pos_weight = 2.5,\n                                           updater =\"grow_histmaker\",\n                                           base_score  = 0.2,\n                                          min_child_weight=1),\n                            \n               '1.CatBoost' : CatBoostClassifier(learning_rate=0.1, \n                                                 n_estimators=300, \n                                                 subsample=0.085, \n                                                 max_depth=10, \n                                                 scale_pos_weight=2.5),\n               \n               '2.LightGBM' : LGBMClassifier(subsample_freq = 2, \n                                             objective =\"binary\",\n                                             importance_type = \"gain\",\n                                             verbosity = -1, \n                                             max_bin = 60,\n                                             num_leaves = 300,\n                                             boosting_type = 'dart',\n                                             learning_rate=0.10, \n                                             n_estimators=494,\n                                             max_depth=10, \n                                             scale_pos_weight=2.5)\n }\n\nprint( list(Classifiers.keys()) )\nprint(\"--#--\"*25)\nprint( list(Classifiers.values()) )","1e2688df":"from sklearn.ensemble import VotingClassifier\nvoting_model = VotingClassifier(estimators=[\n                                              ('XGBoost_Best', list(Classifiers.values())[0]), \n                                              ('CatBoost_Best', list(Classifiers.values())[1]),\n                                              ('LightGBM_Best', list(Classifiers.values())[2]),\n                                             ], \n                                              voting='soft',weights=[5,5,5.2])\n\nvoting_model.fit(X_train_os,Y_train_os) \n\npredictions_of_voting = voting_model.predict_proba( test_encode )[::,1]","04914e17":"predictions_of_voting\n","f239670a":"y_pred_class = [int(round(value)) for value in predictions_of_voting]","a33af8f5":"### Final ensembel model after hyperparameter tuning ","cbcb3154":"# Data Visualization Libraries :\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px","3efbab3a":"Classifiers = {'0.XGBoost' : XGBClassifier(learning_rate =0.1, \n                                           n_estimators=394, \n                                           max_depth=5,\n                                           subsample = 0.70, \n                                           verbosity = 0,\n                                           scale_pos_weight = 2.5,\n                                           updater =\"grow_histmaker\",\n                                           base_score  = 0.2),\n               \n            \n               '1.CatBoost' : CatBoostClassifier(learning_rate=0.15, \n                                                 n_estimators=300, \n                                                 max_depth=5, \n                                                 scale_pos_weight=2.5,\n                                                verbose=False),\n               \n               '2.LightGBM' : LGBMClassifier(learning_rate=0.15, \n                                             n_estimators=494,\n                                             subsample_freq = 2, \n                                             objective =\"binary\",\n                                             importance_type = \"gain\",\n                                             verbosity = -1, \n                                             max_bin = 60,\n                                             num_leaves = 300,\n                                             boosting_type = 'dart',                                            \n                                             max_depth=5, \n                                             scale_pos_weight=2.5)\n                }\n\nprint( list(Classifiers.keys()) )\n\nclf1 = list(Classifiers.values())[0]\nclf2 =list(Classifiers.values())[1]\nclf3 = list(Classifiers.values())[2]\nX = np.array([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\ny = np.array([1, 1, 2, 2])\n\neclf = VotingClassifier(estimators=[('xgboost', clf1), \n                                    ('catboost', clf2), \n                                    ('lgbm', clf3)],\n                        voting='soft',\n                        weights=[5, 5, 5.2])\n\n# predict class probabilities for all classifiers\nprobas = [c.fit(X, y).predict_proba(X) for c in (clf1, clf2, clf3, eclf)]\n\n# get class probabilities for the first sample in the dataset\nclass1_1 = [pr[0, 0] for pr in probas]\nclass2_1 = [pr[0, 1] for pr in probas]\n\n# plotting\n\nN = 4  # number of groups\nind = np.arange(N)  # group positions\nwidth = 0.35  # bar width\n\nfig, ax = plt.subplots()\n\n# bars for classifier 1-3\np1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width,\n            color='green', edgecolor='k')\np2 = ax.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width,\n            color='lightgreen', edgecolor='k')\n\n# bars for VotingClassifier\np3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width,\n            color='blue', edgecolor='k')\np4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width,\n            color='steelblue', edgecolor='k')\n\n# plot annotations\nplt.axvline(2.8, color='k', linestyle='dashed')\nax.set_xticks(ind + width)\nax.set_xticklabels(['XGBoost\\nweight 5',\n                    'CatBoost\\nweight 5',\n                    'LightGBM\\nweight 5.2',\n                    'VotingClassifier\\n(average probabilities)'],\n                   rotation=40,\n                   ha='right')\nplt.ylim([0, 1])\nplt.title('Class probabilities for sample 1 by different classifiers')\nplt.legend([p1[0], p2[0]], ['is_promoted=No', 'is_promoted=Yes'], loc='upper right')\nplt.tight_layout()\nplt.show()","43d828ee":"### Hyperparameter Tuning \n- CatBoostClassifier\n","ab9d8c43":"# <font size=\"+2\" color=red ><b> <center><u>Using CAT BOOST XGBOOST and LGBM <\/u><\/center><\/b><\/font><br><a id=\"top\"><\/a>","daa36d12":"![](https:\/\/blog.walkme.com\/wp-content\/uploads\/2019\/07\/2.jpg)","1eceba04":"## 3 DATA CLEANING ","cdd41b66":"* `ffill is used to forward fill the missing values in the dataset - https:\/\/www.geeksforgeeks.org\/python-pandas-dataframe-ffill\/`\n\n* `bfill is used to backward fill the missing values in the dataset - https:\/\/www.geeksforgeeks.org\/python-pandas-dataframe-bfill\/`","c23dfece":"**Used Voting classifier** -A Voting Classifier is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output.\n\n **Soft Voting** -In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier.\n","b4071cf7":"## Missing value Analysis ","3afc66de":"# Loading the Data","35d70211":"## Encoding \n`Converting the categorical features into binary or numerical counterparts`","14db7036":"### CHECKING DUPLICATES AND REMOVAL","b7bf1700":"- {'n_estimators': 1100,\n- 'min_samples_split': 5,\n- 'min_samples_leaf': 1,\n- 'max_features': 'auto',\n- 'max_depth': 20}","5280d85a":"# If you like this kernel please upvote and make this kernel reach more people","92f58750":"## FEATURE SCALING","1ad141e8":"## BIVARIATE ANALYSIS","d7986341":"### Checking Missing Value","4d054f20":"# Boosting algorithms","9de56a0e":"## FEATURE ENGINEERING","351e3d6a":"###  Cleary we can see data is unbalanced with only 8% of 1s in it so we have to use over Sampling on the data so as to make it balanced\n### using Smote Over Sampling Method ","89da7a82":"## OVERSAMPLING\n### Checking of the data is unbalanced or balanced ","82d1f04d":"- {'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 25}","0a5f039f":"### Hyperparameter Tuning \n- RANDOM FOREST CLASSIFIER\n","ec1843d0":"#### Here i have used Multiple Algoriths starting from \n#### 1.Randomforest \n#### 2.Decision tree\n#### 3.CatBoost\n#### 4.XG Boost \n#### 5.LGBM ","d3280dc0":"## Test train split","878d4360":"## UNIVARIATE ANALYSIS ","d5980f17":"# <font size=\"+3\" color=Blue ><b> <center><u>HR Analysis, Prediction and Visualization and Ensemble Model <\/u><\/center><\/b><\/font><br><a id=\"top\"><\/a>","e8c73109":"## Model Building ","91e12181":"### Hyperparameter Tuning \n- Decision Tree CLASSIFIER\n","8cb960f5":"### FEATURE SELECTION","ad66df9d":"## 1.RANDOMFOREST CLASSIFIER","9a881363":"## 2. DECISION TREE\n","ef62a9e9":"### Correlation between features through Heatmap","6ac37c26":"## Split target variable and predictors"}}