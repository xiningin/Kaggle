{"cell_type":{"7e701204":"code","0dec492d":"code","4a887220":"code","ac5a082c":"code","54b71229":"code","b919a26e":"code","e386ed50":"code","73d93ddb":"code","dde8744e":"code","912b05e7":"code","8e0d8155":"code","9230f68b":"code","cecb3c31":"code","f7c96e85":"code","c948ee46":"code","8116acef":"code","ca76e173":"code","4518c3c6":"code","ae4bb04d":"code","9ac8b955":"code","82b7f1b1":"code","f0b86f53":"code","f63109ad":"code","30257fb3":"code","6889843a":"code","5042b6e3":"code","51000a93":"code","a9e00e4c":"code","14924eb6":"code","55045e3a":"code","f0049ee7":"code","38128ee8":"code","3b4605e5":"code","7f895df5":"code","7ca88aa6":"code","1be936b2":"code","85fed66f":"markdown","e21db51e":"markdown","edf99838":"markdown","c4a8afe2":"markdown","e19cee49":"markdown","71393398":"markdown","81540b82":"markdown","9c3955bf":"markdown","1d78a952":"markdown","1d4b7595":"markdown","1cc5d7c2":"markdown","18d9a317":"markdown","523e765d":"markdown","169ac98f":"markdown","816922a1":"markdown","53c77898":"markdown","d8c68f09":"markdown","e2b3f16a":"markdown","538383c6":"markdown","7240be10":"markdown","6cc352bd":"markdown","af55c104":"markdown","4483d4ab":"markdown","f345230a":"markdown","7d40e8f7":"markdown"},"source":{"7e701204":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import plot_tree\nfrom sklearn.feature_extraction.text import TfidfVectorizer","0dec492d":"#Read data\ndata = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',\n                   encoding='ISO-8859-1', \n                   usecols=['v1', 'v2'])\ndata.rename(columns={'v1':'labels', 'v2':'content'}, inplace=True)\n\nprint(\"data shape: \", data.shape)\ndisplay(data.head())","4a887220":"#distribution of the labels to check for imbalance\nsns.countplot(data.labels);\ndata['labels'].value_counts(normalize=True)","ac5a082c":"#Example for a ham content\nham_indices = data[data.labels == 'ham'].index\nrandom_ham = np.random.choice(ham_indices)\n\ndata.iloc[random_ham, :].content","54b71229":"#Example for a spam content\nspam_indices = data[data.labels == 'spam'].index\nrandom_ham = np.random.choice(spam_indices)\n\ndata.iloc[random_ham, :].content","b919a26e":"f = plt.figure(figsize=(8,12))\nwordcloud = WordCloud(max_words=100).\\\n            generate(' '.join(data.loc[data['labels'] == 'ham', 'content'].to_list()))\nplt.imshow(wordcloud)\nplt.title('ham words');","e386ed50":"f = plt.figure(figsize=(8,12))\nwordcloud = WordCloud(max_words=100).\\\n            generate(' '.join(data.loc[data['labels'] == 'spam', 'content'].to_list()))\nplt.imshow(wordcloud)\nplt.title('spam words');","73d93ddb":"from collections import Counter\n\nspam_text = ' '.join(data.iloc[spam_indices]['content'])\nspam_list = spam_text.lower().split()\n\ncnt = Counter()\n\nfor word in spam_list:\n    if word not in stopwords.words('english'):\n        cnt[word] += 1\n        \ncnt.most_common(30)","dde8744e":"ham_text = ' '.join(data.iloc[ham_indices]['content'])\nham_list = ham_text.lower().split()\n\ncnt = Counter()\n\nfor word in ham_list:\n    if word not in stopwords.words('english'):\n        cnt[word] += 1\n        \ncnt.most_common(30)","912b05e7":"word_count = data['content'].apply(lambda s: len(s.split()))\nword_count.plot.hist(bins=100);","8e0d8155":"word_count[(word_count < word_count.quantile(0.98)) & (word_count > word_count.quantile(0.02))].plot.hist(bins=100);","9230f68b":"data = data[(word_count < word_count.quantile(0.98)) & (word_count > word_count.quantile(0.02))]","cecb3c31":"print(data.shape)","f7c96e85":"cv = TfidfVectorizer(max_features=5000, stop_words='english')\nsparse_mat = cv.fit_transform(data['content'])","c948ee46":"X = sparse_mat.toarray()\nprint(X.shape)","8116acef":"#print some of the features\nprint(cv.get_feature_names()[1000:1005])","ca76e173":"#split data\nfrom sklearn.model_selection import train_test_split\ny = data['labels']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)","4518c3c6":"print(X_train.shape)\nprint(y_train.shape)\nprint(y_test.shape)\nprint(X_test.shape)","ae4bb04d":"#fitting the model\nada_model = AdaBoostClassifier( n_estimators=100)\nada_model.fit(X_train, y_train)\n\nada_preds = ada_model.predict(X_test)","9ac8b955":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n#printing metrics\nprint(\"Accuracy score: \", accuracy_score(y_test, ada_preds))\nprint(\"Confusion matrix: \\n\",confusion_matrix(y_test, ada_preds))\nprint(\"Classification report: \\n\",classification_report(y_test, ada_preds))","82b7f1b1":"feat_importance_index = np.argsort(ada_model.feature_importances_)[-10:]\n\nprint(\"10 top important words: \\n\", pd.Series(cv.get_feature_names())[feat_importance_index])","f0b86f53":"top_3 = np.argsort(ada_model.estimator_weights_)[-3:]","f63109ad":"plot_tree(ada_model.estimators_[top_3[0]], class_names=['ham', 'spam'], proportion=True,\n               rounded=True, filled=True, feature_names=cv.get_feature_names());","30257fb3":"plot_tree(ada_model.estimators_[top_3[1]], class_names=['ham', 'spam'], proportion=True,\n               rounded=True, filled=True, feature_names=cv.get_feature_names());","6889843a":"plot_tree(ada_model.estimators_[top_3[2]], class_names=['ham', 'spam'], proportion=True,\n               rounded=True, filled=True, feature_names=cv.get_feature_names());","5042b6e3":"#Fitting the model\nmodel_rf = RandomForestClassifier(n_estimators=100)\nmodel_rf = model_rf.fit(X_train, y_train)\n#Predicting\ny_rf_pred = model_rf.predict(X_test)","51000a93":"#print metrics\nprint('accuracy_score: ', accuracy_score(y_test, y_rf_pred))\nprint('confusion matrix: \\n', confusion_matrix(y_test, y_rf_pred))\nprint('report: \\n', classification_report(y_test, y_rf_pred))","a9e00e4c":"feat_importance_index = np.argsort(model_rf.feature_importances_)[-10:]\n\nprint(\"10 top important words: \\n\", pd.Series(cv.get_feature_names())[feat_importance_index])","14924eb6":"f = plt.figure(figsize=(35,15))\nplot_tree(model_rf.estimators_[0], class_names=['ham', 'spam'], proportion=True,\n               rounded=True, filled=True, max_depth=5, \n               feature_names=cv.get_feature_names());","55045e3a":"from sklearn.pipeline import Pipeline","f0049ee7":"X_train, X_test, y_train, y_test = train_test_split(data['content'], data['labels'], stratify=data['labels'])","38128ee8":"pipeline = Pipeline(steps=[\n                ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n                ('model', AdaBoostClassifier(n_estimators=100))])\n_ = pipeline.fit(X_train, y_train)\n\npreds = pipeline.predict(X_test)\nprint(classification_report(y_test, preds))","3b4605e5":"pipeline = Pipeline(steps=[\n                ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n                ('model', RandomForestClassifier(n_estimators=100))])\n_ = pipeline.fit(X_train, y_train)\n\npreds = pipeline.predict(X_test)\nprint(classification_report(y_test, preds))","7f895df5":"from sklearn.decomposition import TruncatedSVD\n\npipeline = Pipeline(steps=[\n                ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n                ('dim_reduction', TruncatedSVD(n_components=100)),\n                ('model', RandomForestClassifier(n_estimators=100))])\n_ = pipeline.fit(X_train, y_train)\n\npreds = pipeline.predict(X_test)\nprint(classification_report(y_test, preds))","7ca88aa6":"from imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import RandomOverSampler","1be936b2":"pipeline = make_pipeline(\n                    TfidfVectorizer(max_features=5000, stop_words='english'),\n                    RandomOverSampler(),\n                    RandomForestClassifier(n_estimators=100))\n\n_ = pipeline.fit(X_train, y_train)\n\npreds = pipeline.predict(X_test)\nprint(classification_report(y_test, preds))\nprint(confusion_matrix(y_test, preds))","85fed66f":"**top 10 important word\/features in our Random Forest model**","e21db51e":"**TFIDF**\n\nA method that will help us convert text data to numerical data. This method assigns weights for each document term, taking into consideration the frequency of a term in a document and the frequency of a term across all documents.","edf99838":"**Plotting top 3 tree stumps(weak classifiers)**","c4a8afe2":"### Preprocessing","e19cee49":"Removing too small sentences or too large ones","71393398":"We can see that we improved the recall for our minority class(spam) and the f1-score.\n\n**accruacy score is not the right metric when we have imbalance in the data**","81540b82":"validation","9c3955bf":"Removed a bit over 200 sentences\n","1d78a952":"Part 2- \n\nover-sampling is simply a process of repeating some samples of the minority class and balance the number of samples between classes in the dataset.","1d4b7595":"**Top 30 words for ham**","1cc5d7c2":"We have an imbalanced dataset.","18d9a317":"**top 10 important word\/features in our Adaboost model**","523e765d":"**Plotting the first decision tree in the RF model.**","169ac98f":"**I will only continue with Random Forest.** \n\n1) I will to use TruncatedSVD to reduce the dimensions. TSVD is good for sparse data whereas PCA is good for dense data.\n\n2) I will try oversampling methods since our data is imbalanced. These methods may improve the f1-score and the recall. I will use SMOTE and RandomOverSampler","816922a1":"Random Forest","53c77898":"**top 30 words for spam**","d8c68f09":"### Boosting\n\nA procedure that combines the outputs of many \"weak classifiers(or tree stumps) to produce a powerful committee. ","e2b3f16a":"## **Context**\n\nThe SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.","538383c6":"**Wordcount plots**","7240be10":"**splitting data**","6cc352bd":"### Bagging","af55c104":"Part 1 - ","4483d4ab":"### Pipelines\n\nThe advanced way","f345230a":"### EDA","7d40e8f7":"Adaboost"}}