{"cell_type":{"d600d6a4":"code","d7dc32ed":"code","04851d8d":"code","7ea526f6":"code","ef944be2":"code","eb8e792b":"code","3d6fa041":"code","949d1f67":"code","5bdec165":"code","1109ade1":"code","54850e5a":"code","2227f1d4":"code","21c9ceaf":"code","0ae8efa6":"code","6a45e9b4":"code","0e1d43fb":"code","4bc01958":"code","9b680fe7":"code","b8984188":"code","1324420d":"code","226c5248":"code","9c064073":"code","0e61ebe4":"code","fe9d2a60":"code","8b1564d3":"code","f4f00603":"code","2d970f10":"code","9e79b498":"code","aac70501":"code","1a8a0202":"code","bb2203ed":"code","ddf362b1":"code","e8631dd3":"code","67825a36":"code","0cde4f33":"code","09091d73":"code","a2b8be3c":"code","aa0f852b":"code","ab6ff4d2":"code","6bc82cf5":"code","10861ff1":"code","69e28be7":"code","50d46faa":"markdown","feeec256":"markdown","1f87c24a":"markdown","5de69774":"markdown","dacee973":"markdown","553e692a":"markdown","3197fa88":"markdown","3ef1b421":"markdown","cfb32677":"markdown","ffff8a47":"markdown","00ba7e07":"markdown","dc374597":"markdown","674c463d":"markdown","7cb3ce65":"markdown","1ed5347b":"markdown","07cbd6e5":"markdown","dd676122":"markdown","b2101b64":"markdown","c39cf780":"markdown","1be094b3":"markdown","7805a876":"markdown","f3f6d42b":"markdown","ebc3998c":"markdown"},"source":{"d600d6a4":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"] = (8, 8)\nplt.rcParams[\"figure.dpi\"] = 125\nplt.rcParams[\"font.size\"] = 14\nplt.rcParams['font.family'] = ['sans-serif']\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans']\nplt.style.use('ggplot')\nsns.set_style(\"whitegrid\", {'axes.grid': False})\nplt.rcParams['image.cmap'] = 'gray' # grayscale looks better\nfrom itertools import cycle\nprop_cycle = plt.rcParams['axes.prop_cycle']\ncolors = prop_cycle.by_key()['color']","d7dc32ed":"# tests help notebooks stay managable\nimport doctest\nimport copy\nimport functools\n\ndef autotest(func):\n    globs = copy.copy(globals())\n    globs.update({func.__name__: func})\n    doctest.run_docstring_examples(\n        func, globs, verbose=True, name=func.__name__)\n    return func","04851d8d":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport os\nfrom skimage.io import imread as imread\nfrom skimage.util import montage\nmontage_rgb = lambda x, **kwargs: np.stack([montage(x[..., i], **kwargs) for i in range(x.shape[-1])], -1)\nfrom itertools import product\nfrom tqdm import tqdm_notebook\nfrom IPython.display import clear_output","7ea526f6":"from keras.layers.core import Layer\nfrom keras import layers, models\nimport tensorflow as tf\n\nclass Dense2DSpatialTransformer(Layer):\n    \"\"\"\n    The layer takes an input image and an offset field and generates a transformed version of the image\n    >>> lay_in = layers.Input((5, 5, 1))\n    >>> lay_shift = layers.Input((5, 5, 2))\n    >>> dst = Dense2DSpatialTransformer()\n    >>> dst_lay = dst([lay_in, lay_shift])\n    >>> dst_mod = models.Model(inputs=[lay_in, lay_shift], outputs=[dst_lay])\n    >>> shift_img = np.full((1, 5, 5, 2), -1)\n    >>> test_img = np.pad(np.eye(2), [(1, 2), (2, 1)]).astype(float).reshape((1, 5, 5, 1))\n    >>> test_img[0, :, :, 0].astype(int)\n    array([[0, 0, 0, 0, 0],\n           [0, 0, 1, 0, 0],\n           [0, 0, 0, 1, 0],\n           [0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0]])\n    >>> dst_mod.predict([test_img, shift_img])[0, :, :, 0].astype(int)\n    array([[0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0],\n           [0, 0, 0, 1, 0],\n           [0, 0, 0, 0, 1],\n           [0, 0, 0, 0, 0]])\n    >>> shift_img[0, :, :, 1]*=-1\n    >>> dst_mod.predict([test_img, shift_img])[0, :, :, 0].astype(int)\n    array([[0, 0, 0, 1, 0],\n           [0, 0, 0, 0, 1],\n           [0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0]])\n    >>> dst_mod.predict([2-test_img, shift_img])[0, :, :, 0].astype(int)\n    array([[0, 2, 2, 1, 2],\n           [0, 2, 2, 2, 1],\n           [0, 2, 2, 2, 2],\n           [0, 2, 2, 2, 2],\n           [0, 0, 0, 0, 0]])\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super(Dense2DSpatialTransformer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        if len(input_shape) > 3:\n            raise Exception('Spatial Transformer must be called on a list of length 2 or 3. '\n                            'First argument is the image, second is the offset field.')\n\n        if len(input_shape[1]) != 4 or input_shape[1][3] != 2:\n            raise Exception('Offset field must be one 4D tensor with 2 channels. '\n                            'Got: ' + str(input_shape[1]))\n\n        self.built = True\n\n    def call(self, inputs):\n        return self._transform(inputs[0], inputs[1][:, :, :, 0],\n                               inputs[1][:, :, :, 1])\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0]\n\n    def _transform(self, I, dx, dy):\n\n        batch_size = tf.shape(dx)[0]\n        height = tf.shape(dx)[1]\n        width = tf.shape(dx)[2]\n\n        # Convert dx and dy to absolute locations\n        x_mesh, y_mesh = self._meshgrid(height, width)\n        x_mesh = tf.expand_dims(x_mesh, 0)\n        y_mesh = tf.expand_dims(y_mesh, 0)\n\n        x_mesh = tf.tile(x_mesh, [batch_size, 1, 1])\n        y_mesh = tf.tile(y_mesh, [batch_size, 1, 1])\n        x_new = dx + x_mesh\n        y_new = dy + y_mesh\n\n        return self._interpolate(I, x_new, y_new)\n\n    def _repeat(self, x, n_repeats):\n        rep = tf.transpose(\n            tf.expand_dims(tf.ones(shape=tf.stack([n_repeats, ])), 1), [1, 0])\n        rep = tf.cast(rep, dtype='int32')\n        x = tf.matmul(tf.reshape(x, (-1, 1)), rep)\n        return tf.reshape(x, [-1])\n\n    def _meshgrid(self, height, width):\n        x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])),\n                        tf.transpose(tf.expand_dims(tf.linspace(0.0,\n                                                                tf.cast(width, tf.float32)-1.0, width), 1), [1, 0]))\n        y_t = tf.matmul(tf.expand_dims(tf.linspace(0.0,\n                                                   tf.cast(height, tf.float32)-1.0, height), 1),\n                        tf.ones(shape=tf.stack([1, width])))\n\n        return x_t, y_t\n\n    def _interpolate(self, im, x, y):\n\n        im = tf.pad(im, [[0, 0], [1, 1], [1, 1], [0, 0]], \"CONSTANT\")\n\n        num_batch = tf.shape(im)[0]\n        height = tf.shape(im)[1]\n        width = tf.shape(im)[2]\n        channels = tf.shape(im)[3]\n\n        out_height = tf.shape(x)[1]\n        out_width = tf.shape(x)[2]\n\n        x = tf.reshape(x, [-1])\n        y = tf.reshape(y, [-1])\n\n        x = tf.cast(x, 'float32')+1\n        y = tf.cast(y, 'float32')+1\n\n        max_x = tf.cast(width - 1, 'int32')\n        max_y = tf.cast(height - 1, 'int32')\n\n        x0 = tf.cast(tf.floor(x), 'int32')\n        x1 = x0 + 1\n        y0 = tf.cast(tf.floor(y), 'int32')\n        y1 = y0 + 1\n\n        x0 = tf.clip_by_value(x0, 0, max_x)\n        x1 = tf.clip_by_value(x1, 0, max_x)\n        y0 = tf.clip_by_value(y0, 0, max_y)\n        y1 = tf.clip_by_value(y1, 0, max_y)\n        \n        dim3 = 1\n        dim2 = width\n        dim1 = width*height\n        base = self._repeat(tf.range(num_batch)*dim1,\n                            out_height*out_width)\n\n        base_y0 = base + y0*dim2\n        base_y1 = base + y1*dim2\n\n        idx_a = base_y0 + x0*dim3 \n        idx_b = base_y1 + x0*dim3\n        idx_c = base_y0 + x1*dim3\n        idx_d = base_y1 + x1*dim3\n        idx_e = base_y0 + x0*dim3\n        idx_f = base_y1 + x0*dim3\n        idx_g = base_y0 + x1*dim3\n        idx_h = base_y1 + x1*dim3\n\n        # use indices to lookup pixels in the flat image and restore\n        # channels dim\n        im_flat = tf.reshape(im, tf.stack([-1, channels]))\n        im_flat = tf.cast(im_flat, 'float32')\n\n        Ia = tf.gather(im_flat, idx_a)\n        Ib = tf.gather(im_flat, idx_b)\n        Ic = tf.gather(im_flat, idx_c)\n        Id = tf.gather(im_flat, idx_d)\n        Ie = tf.gather(im_flat, idx_e)\n        If = tf.gather(im_flat, idx_f)\n        Ig = tf.gather(im_flat, idx_g)\n        Ih = tf.gather(im_flat, idx_h)\n\n        # and finally calculate interpolated values\n        x1_f = tf.cast(x1, 'float32')\n        y1_f = tf.cast(y1, 'float32')\n\n        dx = x1_f - x\n        dy = y1_f - y\n\n        wa = tf.expand_dims((dx * dy), 1)\n        wb = tf.expand_dims((dx * (1-dy)), 1)\n        wc = tf.expand_dims(((1-dx) * dy), 1)\n        wd = tf.expand_dims(((1-dx) * (1-dy)), 1)\n        we = tf.expand_dims((dx * dy), 1)\n        wf = tf.expand_dims((dx * (1-dy)), 1)\n        wg = tf.expand_dims(((1-dx) * dy), 1)\n        wh = tf.expand_dims(((1-dx) * (1-dy)), 1)\n\n        output = tf.add_n([wa*Ia, wb*Ib, wc*Ic, wd*Id,\n                           we*Ie, wf*If, wg*Ig, wh*Ih])\n        output = tf.reshape(output, tf.stack(\n            [-1, out_height, out_width, channels]))\/2.0\n        return output\nclear_output()\nautotest(Dense2DSpatialTransformer) # can also be run seperately","ef944be2":"from keras.datasets import cifar10\ntrain_ds, test_ds = cifar10.load_data()\nX_train, y_train = train_ds\nX_test, y_test = test_ds\nX_train = X_train\/255.0\ny_train = y_train[:, 0]\nX_test = X_test\/255.0\ny_test = y_test[:, 0]\nclear_output()\nprint(X_train.shape, y_train.shape)","eb8e792b":"img_idx = np.random.choice(range(X_train.shape[0]))\nplt.imshow(X_train[img_idx])","3d6fa041":"@autotest\ndef cut_jigsaw(\n    in_image, # type: np.ndarray\n    x_wid, # type: int\n    y_wid,# type: int\n    gap=False,\n    jitter=False,\n    jitter_dim=None, # type: Optional[int]\n):\n    # type: (...) -> List[np.ndarray]\n    \"\"\"Cuts the image into little pieces\n    :param in_image: the image to cut-apart\n    :param x_wid: the size of the piece in x\n    :param y_wid: the size of the piece in y\n    :param gap: if there is a gap between tiles\n    :param jitter: if the positions should be moved around\n    :param jitter_dim: amount to jitter (default is x_wid or y_wid\/2)\n    :return : a 4D array with tiles x x_wid x y_wid * d\n    Examples\n    >>> test_image = np.arange(20).reshape((4, 5)).astype(int)\n    >>> test_image\n    array([[ 0,  1,  2,  3,  4],\n           [ 5,  6,  7,  8,  9],\n           [10, 11, 12, 13, 14],\n           [15, 16, 17, 18, 19]])\n    >>> cut_jigsaw(test_image, 2, 2, False, False)\n    array([[[ 0,  1],\n            [ 5,  6]],\n    <BLANKLINE>\n           [[ 2,  3],\n            [ 7,  8]],\n    <BLANKLINE>\n           [[10, 11],\n            [15, 16]],\n    <BLANKLINE>\n           [[12, 13],\n            [17, 18]]])\n    >>> cut_jigsaw(test_image, 2, 2, True, False)\n    array([[[ 0,  1],\n            [ 5,  6]],\n    <BLANKLINE>\n           [[ 3,  4],\n            [ 8,  9]],\n    <BLANKLINE>\n           [[10, 11],\n            [15, 16]],\n    <BLANKLINE>\n           [[13, 14],\n            [18, 19]]])\n    >>> np.random.seed(0)\n    >>> cut_jigsaw(test_image, 2, 2, True, True, 1)\n    array([[[ 1,  2],\n            [ 6,  7]],\n    <BLANKLINE>\n           [[ 7,  8],\n            [12, 13]],\n    <BLANKLINE>\n           [[ 5,  6],\n            [10, 11]],\n    <BLANKLINE>\n           [[ 7,  8],\n            [12, 13]]])\n    \"\"\"\n    if len(in_image.shape)==2:\n        in_image = np.expand_dims(in_image, -1)\n        expand = True\n    else:\n        expand = False\n    x_size, y_size, d_size = in_image.shape\n    out_tiles = []\n    x_chunks = x_size\/\/x_wid\n    y_chunks = y_size\/\/y_wid\n    out_tiles = np.zeros((x_chunks*y_chunks, x_wid, y_wid, d_size), dtype=in_image.dtype)\n    if gap:\n        # we calculate the maximum gap and \n        x_gap = x_size-x_chunks*x_wid\n        y_gap = y_size-y_chunks*y_wid\n    else:\n        x_gap, y_gap = 0, 0\n    x_jitter = x_wid\/\/2 if jitter_dim is None else jitter_dim\n    y_jitter = y_wid\/\/2 if jitter_dim is None else jitter_dim\n    for idx, (i, j) in enumerate(product(range(x_chunks), range(y_chunks))):\n        x_start = i*x_wid+min(x_gap, i)\n        y_start = j*y_wid+min(y_gap, j)\n        if jitter:\n            x_range = max(x_start-x_jitter, 0), min(x_start+x_jitter+1, x_size-x_wid)\n            y_range = max(y_start-y_jitter, 0), min(y_start+y_jitter+1, y_size-y_wid)\n            \n            x_start = np.random.choice(range(*x_range)) if x_range[1]>x_range[0] else x_start\n            y_start = np.random.choice(range(*y_range)) if y_range[1]>y_range[0] else y_start\n            \n        out_tiles[idx, :, :, :] = in_image[x_start:x_start+x_wid, y_start:y_start+y_wid, :]\n    \n    return out_tiles[:, :, :, 0] if expand else out_tiles\n                ","949d1f67":"@autotest\ndef jigsaw_to_image(\n    in_tiles, # type: np.ndarray\n    out_x, # type: int\n    out_y, # type: int\n    gap=False\n):\n    # type: (...) -> np.ndarray\n    \"\"\"Reassembles little pieces into an image\n    :param in_tiles: the tiles to reassemble\n    :param out_x: the size of the image in x (default is calculated automatically)\n    :param out_y: the size of the image in y\n    :param gap: if there is a gap between tiles\n    :return : an image from the tiles\n    Examples\n    >>> test_image = np.arange(20).reshape((4, 5)).astype(int)\n    >>> test_image\n    array([[ 0,  1,  2,  3,  4],\n           [ 5,  6,  7,  8,  9],\n           [10, 11, 12, 13, 14],\n           [15, 16, 17, 18, 19]])\n    >>> js_pieces = cut_jigsaw(test_image, 2, 2, False, False)\n    >>> jigsaw_to_image(js_pieces, 4, 5)\n    array([[ 0,  1,  2,  3,  0],\n           [ 5,  6,  7,  8,  0],\n           [10, 11, 12, 13,  0],\n           [15, 16, 17, 18,  0]])\n    >>> js_gap_pieces = cut_jigsaw(test_image, 2, 2, True, False)\n    >>> jigsaw_to_image(js_gap_pieces, 4, 5, True)\n    array([[ 0,  1,  0,  3,  4],\n           [ 5,  6,  0,  8,  9],\n           [10, 11,  0, 13, 14],\n           [15, 16,  0, 18, 19]])\n    >>> np.random.seed(0)\n    >>> js_gap_pieces = cut_jigsaw(test_image, 2, 2, False, True)\n    >>> jigsaw_to_image(js_gap_pieces, 4, 5, False)\n    array([[ 1,  2,  6,  7,  0],\n           [ 6,  7, 11, 12,  0],\n           [ 6,  7,  7,  8,  0],\n           [11, 12, 12, 13,  0]])\n    \"\"\"\n    if len(in_tiles.shape)==3:\n        in_tiles = np.expand_dims(in_tiles, -1)\n        expand = True\n    else:\n        expand = False\n    tile_count, x_wid, y_wid, d_size = in_tiles.shape\n    x_chunks = out_x\/\/x_wid\n    y_chunks = out_y\/\/y_wid\n    out_image = np.zeros((out_x, out_y, d_size), dtype=in_tiles.dtype)\n    \n    if gap:\n        x_gap = out_x-x_chunks*x_wid\n        y_gap = out_y-y_chunks*y_wid\n    else:\n        x_gap, y_gap = 0, 0\n        \n    for idx, (i, j) in enumerate(product(range(x_chunks), range(y_chunks))):\n        x_start = i*x_wid+min(x_gap, i)\n        y_start = j*y_wid+min(y_gap, j)\n        out_image[x_start:x_start+x_wid, y_start:y_start+y_wid] = in_tiles[idx, :, :]\n    \n    return out_image[:, :, 0] if expand else out_image\n    \n    \n    ","5bdec165":"TILE_X = 10\nTILE_Y = 10\nJITTER_SIZE = 3\nTRAIN_TILE_COUNT = 2**15\nVALID_TILE_COUNT = 2**11\nKEEP_RANDOM_PERM = 150\nLATENT_SIZE = 24\nBIG_LATENT_SIZE = 64","1109ade1":"fig, m_axs = plt.subplots(4, 11, figsize=(50, 10))\nfor img_idx, c_axs in enumerate(m_axs, 10):\n    c_axs[0].imshow(X_train[img_idx, :, :])\n    c_axs[0].set_title('Input')\n    out_tiles = cut_jigsaw(X_train[img_idx, :, :], TILE_X, TILE_Y, gap=False) \n    for k, c_ax in zip(range(out_tiles.shape[0]), c_axs[1:]):\n        c_ax.imshow(out_tiles[k, :, :])\n    recon_img = jigsaw_to_image(out_tiles, X_train.shape[1], X_train.shape[2])\n    c_axs[-1].imshow(recon_img[:, :])\n    c_axs[-1].set_title('Reconstruction')","54850e5a":"from itertools import permutations\nall_perm = np.array(list(permutations(range(out_tiles.shape[0]), out_tiles.shape[0])))\nprint('Permutation count:' , len(all_perm))\n\nnp.random.seed(2019)\n# first one is always unmessed up\nkeep_perm = all_perm[0:1, :].tolist()+all_perm[np.random.choice(range(1, len(all_perm)), KEEP_RANDOM_PERM-1), :].tolist()","2227f1d4":"fig, m_axs = plt.subplots(5, 5, figsize=(15, 25))\nfor i, c_axs in enumerate(m_axs.T):\n    out_tiles = cut_jigsaw(X_train[7], TILE_X, TILE_Y, gap=False, jitter=i>0, jitter_dim=JITTER_SIZE) \n    for j, (c_ax, c_perm) in enumerate(zip(c_axs, keep_perm)): \n        scrambled_tiles = out_tiles[c_perm]\n        recon_img = jigsaw_to_image(scrambled_tiles, X_train.shape[1], X_train.shape[2])\n        c_ax.imshow(recon_img)\n        c_ax.set_title('Permutation:#{}\\nJitter:{}'.format(j, i>0))\n        c_ax.axis('off')","21c9ceaf":"out_tiles = cut_jigsaw(X_train[8, :, :], TILE_X, TILE_Y, gap=False) \n\ndef _generate_batch(in_idx, is_valid=False):\n    np.random.seed(in_idx)\n    if is_valid:\n        img_ds = X_test\n    else:\n        img_ds = X_train\n    img_idx = np.random.choice(range(img_ds.shape[0]))\n    out_tiles = cut_jigsaw(img_ds[img_idx, :, :], TILE_X, TILE_Y, gap=True, jitter=JITTER_SIZE>0, jitter_dim=JITTER_SIZE) \n    perm_idx = np.random.choice(range(len(keep_perm)))\n    c_perm = keep_perm[perm_idx]\n    return out_tiles[c_perm], perm_idx, img_ds[img_idx, :, :]\n\ndef make_tile_group(tile_count, is_valid=False):\n    c_tiles = np.zeros((tile_count,)+out_tiles.shape, dtype='float32')\n    c_perms = np.zeros((tile_count,), dtype='int')\n    c_recon_image = np.zeros((tile_count,)+X_train.shape[1:], dtype='float32')\n    for i in tqdm_notebook(range(tile_count)):\n        # should be parallelized\n        c_tiles[i], c_perms[i], c_recon_image[i] = _generate_batch(i, is_valid=is_valid)\n    return c_tiles, c_perms, c_recon_image\ntrain_tiles, train_perms, train_recons = make_tile_group(TRAIN_TILE_COUNT)\nvalid_tiles, valid_perms, valid_recons = make_tile_group(VALID_TILE_COUNT, is_valid=True)","0ae8efa6":"from keras import models, layers\ntile_encoder = models.Sequential(name='TileEncoder')\n# we use None to make the model more usuable later\ntile_encoder.add(layers.BatchNormalization(input_shape=(None, None)+(train_tiles.shape[-1],)))\ntile_encoder.add(layers.Conv2D(8, (3,3), padding='same', activation='linear'))\ntile_encoder.add(layers.BatchNormalization())\ntile_encoder.add(layers.MaxPool2D(2,2))\ntile_encoder.add(layers.LeakyReLU(0.1))\ntile_encoder.add(layers.Conv2D(16, (3,3), padding='same', activation='linear'))\ntile_encoder.add(layers.BatchNormalization())\ntile_encoder.add(layers.MaxPool2D(2,2))\ntile_encoder.add(layers.LeakyReLU(0.1))\ntile_encoder.add(layers.Conv2D(32, (2,2), padding='valid', activation='linear'))\ntile_encoder.add(layers.BatchNormalization())\ntile_encoder.add(layers.LeakyReLU(0.1))\ntile_encoder.add(layers.Conv2D(LATENT_SIZE, (1,1), activation='linear'))\ntile_encoder.add(layers.BatchNormalization())\ntile_encoder.add(layers.LeakyReLU(0.1))\nclear_output() # some annoying loading\/warnings come up","6a45e9b4":"tile_encoder.summary()","0e1d43fb":"print('Model Input Shape:', train_tiles.shape[2:], \n      '-> Model Output Shape:', tile_encoder.predict(np.zeros((1,)+train_tiles.shape[2:])).shape[1:])","4bc01958":"from keras.utils.vis_utils import model_to_dot\nfrom IPython.display import Image","9b680fe7":"# here we build a mini-model to reconstruct the image the output of the RNN\nfull_X, full_Y, full_D = X_train.shape[1:]\nx_pad = full_X-TILE_X\ny_pad = full_Y-TILE_Y\n\ntile_in = layers.Input((TILE_X, TILE_Y, X_train.shape[-1]), name='TileInput')\nlstm_features_in = layers.Input((2*LATENT_SIZE,), name='BiLSTMOutputs')\n\npad_layer = layers.ZeroPadding2D(padding=((x_pad\/\/2, x_pad-x_pad\/\/2), (y_pad\/\/2, y_pad-y_pad\/\/2)), name='PadTileUpToImage')(tile_in)\n\nnext_features = layers.Dense(LATENT_SIZE, name='StepFeatures')(layers.Dropout(0.5)(lstm_features_in))\npos_xy = layers.Dense(2, activation='tanh', name='TileToXY')(next_features)\npos_xy = layers.Lambda(lambda x: x*x_pad\/2.0, name='ScaleXY_{}'.format(x_pad\/\/2))(pos_xy)\n\ndens_st = Dense2DSpatialTransformer()\n\npos_xy_field = layers.Reshape((1, 1, 2))(pos_xy)\npos_xy_full = layers.UpSampling2D((full_X, full_Y))(pos_xy_field)\nrecon_image = dens_st([pad_layer, pos_xy_full])\ntile_slide_model = models.Model(inputs=[tile_in, lstm_features_in], \n                                outputs=[recon_image], \n                                name='SlideTile')\nImage(model_to_dot(tile_slide_model, show_shapes=True).create_png())","b8984188":"big_in = layers.Input(train_tiles.shape[1:], name='All_Tile_Input')\nimage_x = []\nfeat_vec_1d = []\nfor k in range(train_tiles.shape[1]):\n    lay_x = layers.Lambda(lambda x: x[:, k], name='Select_{}_Tile'.format(k))(big_in)\n    image_x += [lay_x]\n    feat_x = tile_encoder(lay_x)\n    feat_x = layers.GlobalAvgPool2D()(feat_x)\n    feat_vec_1d += [layers.Reshape((1, LATENT_SIZE))(feat_x)]\n    \nfeat_cat = layers.concatenate(feat_vec_1d, axis=1, name='CombineFeatures')\n\n# first try and reconstruct the image\n# use fresh features for the tile_slide results\nlstm_1_output = layers.Bidirectional(layers.CuDNNLSTM(LATENT_SIZE, return_sequences=True))(feat_cat)\nlstm_2_output = layers.Bidirectional(layers.CuDNNLSTM(LATENT_SIZE, return_sequences=True))(lstm_1_output)\n\n# use the tile slide model on each tile\nbig_image = []\nfor k, c_img in enumerate(image_x):\n    c_feat = layers.Lambda(lambda x: x[:, k], name='Select_{}_Feature'.format(k))(lstm_2_output)\n    big_image.append(tile_slide_model([c_img, c_feat]))\n\nrecon_image = layers.add(big_image, name='ReconImage')\n\n# now learn the scrambling permutation\nfeat_cat_flat = layers.Bidirectional(layers.CuDNNLSTM(LATENT_SIZE, return_sequences=False))(lstm_2_output)\nfeat_dr = layers.Dropout(0.25)(feat_cat_flat)\nfeat_latent = layers.Dense(LATENT_SIZE)(feat_dr)\nfeat_latent_dr = layers.Dropout(0.25)(feat_latent)\nout_pred = layers.Dense(KEEP_RANDOM_PERM, activation='softmax', name='PermutationCategory')(feat_latent_dr)\nbig_model = models.Model(inputs=[big_in], outputs=[out_pred, recon_image])\nbig_model.compile(optimizer='adam', \n                  loss={'PermutationCategory': 'sparse_categorical_crossentropy', 'ReconImage': 'mse'}, \n                  metrics={\n                      'PermutationCategory': ['sparse_categorical_accuracy', \n                                              'sparse_top_k_categorical_accuracy'],\n                      'ReconImage': ['mae', \n                                     'binary_accuracy']}\n                 )","1324420d":"dot_model = model_to_dot(big_model, show_shapes=True)\ndot_model.set_rankdir('LR')\nImage(dot_model.create_png())","226c5248":"reversed_keep_perm = [[c_dict[j] for j in range(out_tiles.shape[0])]\n                      for c_dict in [{j: i for i, j in enumerate(c_perm)}\n                                     for c_perm in keep_perm]]\nfor i in range(3):\n    print('forward', keep_perm[i], 'reversed', reversed_keep_perm[i])","9c064073":"def show_model_output(image_count=4, perm_count=3): \n    fig, m_axs = plt.subplots(image_count, perm_count+3, figsize=(3*(perm_count+2), 3*image_count))\n    [c_ax.axis('off') for c_ax in m_axs.flatten()]\n    for img_idx, c_axs in enumerate(m_axs):\n        img_idx = np.random.choice(range(X_train.shape[0]))\n        perm_idx = np.random.choice(range(len(keep_perm)))\n        c_axs[0].imshow(X_train[img_idx])\n        \n        c_axs[0].set_title('Input')\n        # generate tiles\n        out_tiles = cut_jigsaw(X_train[img_idx, :, :], TILE_X, TILE_Y, gap=True, jitter=JITTER_SIZE>0, jitter_dim=JITTER_SIZE)\n        # scramble tiles\n        \n        c_perm = keep_perm[perm_idx]\n        scr_tiles = out_tiles[c_perm]\n        recon_img = jigsaw_to_image(scr_tiles, X_train.shape[1], X_train.shape[2])\n        # show scrambled input\n        c_axs[1].imshow(recon_img)\n        c_axs[1].set_title('As Input: #{}'.format(perm_idx))\n        # get model prediction\n        out_pred, out_recon = big_model.predict(np.expand_dims(scr_tiles, 0))\n        out_pred = out_pred[0]\n        \n        for c_ax, k_idx in zip(c_axs[2:-1], np.argsort(-1*out_pred)):\n            pred_rev_perm = reversed_keep_perm[k_idx]\n            recon_img = jigsaw_to_image(scr_tiles[pred_rev_perm], X_train.shape[1], X_train.shape[2])\n            c_ax.imshow(recon_img)\n            c_ax.set_title('Pred: #{} ({:2.2%})'.format(k_idx, out_pred[k_idx]))\n        \n        c_axs[-1].imshow(out_recon[0])\n        c_axs[-1].set_title('Recon with Tiles')\nshow_model_output()","0e61ebe4":"fit_results = big_model.fit(train_tiles, {'PermutationCategory': train_perms, 'ReconImage': train_recons}, \n                            validation_data=(valid_tiles, {'PermutationCategory': valid_perms, 'ReconImage': valid_recons}),\n                            batch_size=128,\n                            epochs=50,\n                           verbose=True)\nclear_output()","fe9d2a60":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize = (20, 10))\nax1.semilogy(fit_results.history['loss'], label='Training')\nax1.semilogy(fit_results.history['val_loss'], label='Validation')\nax1.legend()\nax1.set_title('Loss')\nax2.plot(fit_results.history['PermutationCategory_sparse_categorical_accuracy'], label='Training')\nax2.plot(fit_results.history['val_PermutationCategory_sparse_categorical_accuracy'], label='Validation')\nax2.legend()\nax2.set_title('Accuracy')\nax2.set_ylim(0, 1)\n\nax3.semilogy(fit_results.history['ReconImage_mean_absolute_error'], label='Training')\nax3.semilogy(fit_results.history['val_ReconImage_mean_absolute_error'], label='Validation')\nax3.legend()\nax3.set_title('Image Reconstruction')\n      \nax4.plot(fit_results.history['ReconImage_binary_accuracy'], label='Training')\nax4.plot(fit_results.history['val_ReconImage_binary_accuracy'], label='Validation')\nax4.legend()\nax4.set_ylim(0, 1)\nax4.set_title('Reconstruction\\nAccuracy')","8b1564d3":"show_model_output(image_count=10, perm_count=2)","f4f00603":"conv_weight_dict = {(idx, k.name): k.get_weights() for idx, k in enumerate(tile_encoder.layers) if isinstance(k, layers.Conv2D)}\nprint(conv_weight_dict.keys())\nfig, m_axs = plt.subplots(2, 2, figsize=(20, 20))\nfor c_ax, ((idx, lay_name), [W, b]) in zip(m_axs.flatten(), conv_weight_dict.items()):\n    c_ax.set_title('{} #{}\\n{}'.format(lay_name, idx, W.shape))\n    flat_W = W.reshape((W.shape[0], W.shape[1], -1)).swapaxes(0, 2).swapaxes(1,2)\n    if flat_W.shape[1]>1 or flat_W.shape[2]>1:\n        pad_W = np.pad(flat_W, [(0, 0), (1, 1), (1,1)], mode='constant', constant_values=np.NAN)\n        pad_W = montage(pad_W, fill=np.NAN, grid_shape=(W.shape[2], W.shape[3]))\n    else:\n        pad_W = W[0, 0]\n    c_ax.imshow(pad_W, vmin=-1, vmax=1, cmap='RdBu')\n    ","2d970f10":"rgb_conv_W, _ = list(conv_weight_dict.values())[0]\nrgb_conv_W = rgb_conv_W.swapaxes(0, 3).swapaxes(1, 3).swapaxes(1, 2)\noffset_val = np.percentile(np.abs(rgb_conv_W), 99)\nprint(rgb_conv_W.shape, offset_val)\nrgb_conv_W = np.clip((rgb_conv_W+offset_val)\/(2*offset_val), 0, 1)\npad_W = montage_rgb(rgb_conv_W, fill=np.NAN, padding_width=1)\nplt.imshow(pad_W)","9e79b498":"gp_outputs = []\nfor k in tile_encoder.layers:\n    if isinstance(k, layers.LeakyReLU):\n        c_output = k.get_output_at(0)\n        c_smooth = layers.AvgPool2D((2, 2))(c_output)\n        c_gp = layers.GlobalMaxPool2D(name='GP_{}'.format(k.name))(c_smooth)\n        gp_outputs += [c_gp]\nactivation_tile_encoder = models.Model(inputs = tile_encoder.inputs, \n                                       outputs = gp_outputs)\nactivation_maps = dict(zip(activation_tile_encoder.output_names, activation_tile_encoder.predict(X_train, batch_size=128, verbose=True)))\n\nfor k, v in activation_maps.items():\n    print(k, v.shape)","aac70501":"keep_top_n = 5\nfig, m_axs = plt.subplots(1, len(activation_maps), figsize=(20, 20))\nfor c_ax, (k, v) in zip(m_axs.T, activation_maps.items()):\n    c_ax.set_title(k)\n    active_rows = []\n    for i in range(v.shape[1]):\n        top_idx = np.argsort(-np.abs(v[:, i]))[:keep_top_n]\n        active_rows += [X_train[top_idx, :, :]]\n    c_ax.imshow(montage_rgb(np.concatenate(active_rows, 0), grid_shape=(v.shape[1], keep_top_n), padding_width=1))\n    c_ax.axis('off')","1a8a0202":"print(X_train[0].shape, '->', tile_encoder.predict(X_train[0:1]).shape)","bb2203ed":"img_in = layers.Input(X_train.shape[1:])\nfull_feat_mat = tile_encoder(img_in)\ngap_out = layers.GlobalAvgPool2D()(full_feat_mat)\nimage_encoder = models.Model(inputs=[img_in], outputs=[gap_out], name='EncodeImage')\nimage_encoder.summary()","ddf362b1":"X_features = image_encoder.predict(X_train, batch_size=128)","e8631dd3":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, perplexity=40, verbose=2, n_iter=250, early_exaggeration=1)\nX_tsne = tsne.fit_transform(X_features)\nclear_output()","67825a36":"fig, ax1 = plt.subplots(1, 1, figsize=(20, 20))\nfor k in np.unique(y_train):\n    ax1.plot(X_tsne[y_train==k, 0], X_tsne[y_train==k, 1], '.', label='{}'.format(k))\nax1.legend()","0cde4f33":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC # too slow\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer","09091d73":"def train_and_show_model(in_model, train_size=X_train.shape[0]\/\/2, random_state=2019, show_figure=True):\n    train_imgs, test_imgs, train_feat, test_feat, train_y, test_y = train_test_split(\n        X_train,\n        X_features, \n        y_train, \n        random_state=random_state, \n        test_size=X_train.shape[0]-train_size,\n        stratify=y_train\n    )\n    # fit pixel model\n    baseline_model = make_pipeline(Normalizer(), in_model)\n    baseline_model.fit(train_imgs.reshape((train_imgs.shape[0], -1)), train_y)\n    baseline_pred_y = baseline_model.predict(test_imgs.reshape((test_imgs.shape[0], -1)))\n    \n    # fit feature model\n    feat_model = make_pipeline(Normalizer(), in_model)\n    feat_model.fit(train_feat, train_y)\n    pred_y = feat_model.predict(test_feat)\n    if show_figure:\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n\n        sns.heatmap(confusion_matrix(test_y, baseline_pred_y), annot=True, fmt='d', ax=ax1)\n        ax1.set_title('Pixel Accuracy: {:2.1%}'.format(accuracy_score(test_y, baseline_pred_y)))\n\n        sns.heatmap(confusion_matrix(test_y, pred_y), annot=True, fmt='d', ax=ax2)\n        ax2.set_title('Feature Accuracy: {:2.1%}'.format(accuracy_score(test_y, pred_y)))\n    return {\n        'train_size': train_imgs.shape[0],\n        'random_state': random_state,\n        'pixel_accuracy': accuracy_score(test_y, baseline_pred_y),\n        'feature_accuracy': accuracy_score(test_y, pred_y)\n    }","a2b8be3c":"train_and_show_model(LogisticRegression(solver='lbfgs', multi_class='auto'))","aa0f852b":"train_and_show_model(RandomForestClassifier(n_estimators=100))","ab6ff4d2":"import gc; gc.enable(); gc.collect() # memory gets very tight\nmodel_fn = lambda : RandomForestClassifier(n_estimators=25, random_state=2019, n_jobs=1)\nparm_seq = product(np.logspace(1, 3, 10).astype(int), # sample size\n                          np.arange(3))\n\nmap_fn = lambda args: train_and_show_model(model_fn(),\n                                     train_size=args[0], \n                                     random_state=args[1],\n                                    show_figure=False)\nUSE_DASK = False\n\nif USE_DASK:\n    import dask\n    from dask import bag\n    import dask.diagnostics as diag\n    from multiprocessing.pool import ThreadPool\n    \n    parm_sweep = bag.\\\n        from_sequence(parm_seq).\\\n        map(map_fn)\n    print(parm_sweep)\n    with diag.ProgressBar(), dask.config.set(pool = ThreadPool(2)):\n        parm_df = pd.DataFrame(parm_sweep.compute())\nelse:\n    from tqdm import tqdm_notebook\n    parm_df = pd.DataFrame([map_fn(args) for args in tqdm_notebook(list(parm_seq))])","6bc82cf5":"\n# clean-up the output\nnice_parm_df = pd.melt(parm_df, id_vars=['train_size', 'random_state']).\\\n    rename(columns={'value': 'Test Accuracy', 'variable': 'input'})\nnice_parm_df['Test Accuracy']*=100\nnice_parm_df['input'] = nice_parm_df['input'].map(lambda x: x.split('_accuracy')[0])\nnice_parm_df['Samples Per Class'] = nice_parm_df['train_size']\/np.unique(y_train).shape[0]\nnice_parm_df.head(3) ","10861ff1":"sns.catplot(x='Samples Per Class', \n            y='Test Accuracy', \n            hue='input', \n            kind='point',\n            alpha=0.5,\n            ci='sd',\n            data=nice_parm_df)","69e28be7":"sns.catplot(x='Samples Per Class', \n            y='Test Accuracy', \n            hue='input', \n            kind='swarm',\n            data=nice_parm_df)","50d46faa":"# Jigsaw on MNIST Images\nHere we start the actual code. We have some predefined constants below for the size of various layers and tiles. These should be optimized to be well suited for the problem at hand","feeec256":"## Show a slightly more complicated model","1f87c24a":"### Activated Neurons\nHere we show each intermediate layer (panel) with each neuron\/depth-channel (row) and the top-n images for activating that pattern (columns). Each row should more or less represent the kinds of images that particular neuron is sensitive too.","5de69774":"## Dense Spatial Transformer\nHere we define the dense spatial transformer layer that takes an input image $(batch, x, y, d)$ and a offset image $(batch, x, y, 2)$ and stretches the input image according to the offset image. We have a few basic examples below but the technique is quite versatile and fully-differentiable which means we can learn the offset map (as we do below with the tile-slider model)","dacee973":"# Jigsaw Code\nHere we write the jigsaw code to break the image up into a bunch of little pieces (```cut_jigsaw```) and reassemble the pieces back into an image (```jigsaw_to_image```). The methods right now are very simple, but are fairly easy to read and include a few basic test-cases. The code is implemented in a very intuitive, but inefficient manner. This should certainly be optimized before use on real problems.","553e692a":"### Show combinations\nHere we can show combinations along with various instances of jitter noise to see how much that affects the reconstruction","3197fa88":"## Show Output\nIn order to show the model output we need to descramble the image with the given scrambling code. Given that a scrambling is a mapping from $i\\rightarrow j$ we need to make a reverse mapping for each combination","3ef1b421":"# Data Preparation\nIn order to train a model we need to pre-compute a whole bunch of data to train models with","cfb32677":"# Fitting Models\nHere we try to fit models with the features created by the tile-encoder. I try logistic regression as the baseline and then see if random forest or SVMs are able to do any better.","ffff8a47":"# Overview\nExperiment with the ideas from [Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles](https:\/\/link.springer.com\/chapter\/10.1007\/978-3-319-46466-4_5) by Mehdi Noroozi and Paolo Favaro. \n\nThe core idea is to take an image, break it up into square tiles, scramble the tiles and have a model learn to unscramble them $\\rightarrow$ \"solving a jigsaw puzzle\". The features the model learns on the individual tiles should then be useful for describing the important information in the image.\n\nFor this approach we try a more auto-encoder style meaning we try to produce the original image as well as the scramble permutation. Instead of generating the original image from features as some of the other models we, we attempt to use a dense Spatial Transformer to move the original tiles into the correct position and map the loss from the image reconstructed in that manner.\n\n## Overview\n- Create and test the jigsaw puzzle code (and the reconstruction)\n- Setup scrambling\n- Build a training and validation dataset\n- Train the model to unscramble\n- Evaluate performance and visualize what the model has done\n- Use the features to try and classify digits","00ba7e07":"# Scramble Combinations\nWe have $9!$ different possible permutations, but that is too many and is probably not a great problem to solve (since it is under-constained, there are alot of permutations where it would be hard to know what exactly is being matched to what.","dc374597":"## Logistic Regression","674c463d":"# Did we learn useful intermediate representations?\nSo we have a nice pretrained model that seems to have figured out how to solve the jigsaw puzzle (sometimes). Can we do anything with it?\n- Use the model to calculate features on all of the images in MNIST\n- See if the feature space has anything meaningful","7cb3ce65":"## Comparison to Pixel-based Methods\nWe compare every model to what the performance would have been had it been trained directly on the raw pixel data.","1ed5347b":"# Model Building\n## Encoder Model\nWe first build the tile encoder model to come up with a feature representation of the tiles","07cbd6e5":"### Run lots of models\nRunning multiple models with multiple parameters is very time consuming so we use dask to run them in parallel.","dd676122":"## To be continued\n...","b2101b64":"## Increasing Training Size\nHere we try and evaluate how well the model works as we increase the sample count. We provide the `test_size` as the sample count (as an integer) and the `random_state` to allow multiple runs to be done to make sure there aren't strong sample specific dependencies.","c39cf780":"## Look at the filters\nWe can examine the filters and try to see what the model was doing?","1be094b3":"## Big Jigsaw Permutation Model\nHere we reuse (shared-weights) the tile-encoder to process a number of tiles and predict which permutation is most likely.\n\nThe second head of the model tries to predict how to move each tile in order to get the best ouput image","7805a876":"## Find the most activating imaging channels\nWe can run all of the images through the model and record all of the intermediate points","f3f6d42b":"## Calculate TSNE\nWe can see if the TSNE space seperates the digits well (we will quantify this later)","ebc3998c":"## Convert into an Image Model\nWe throw in global average pooling to turn the output of the `tile_encoder` into a single feature-vector. We can then use this feature vector as a basis classifying image."}}