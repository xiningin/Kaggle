{"cell_type":{"7a212832":"code","9054cc35":"code","66b06258":"code","3e501554":"code","7f044dc3":"code","3609c3eb":"code","c4927d49":"code","da11a033":"code","78b55d63":"code","0846c799":"code","18dea265":"code","96a14652":"code","eb2592a2":"code","36850b8f":"code","6dc2082d":"code","16dfacb4":"code","c3f2fee1":"code","ffd30157":"code","53e53fb4":"markdown"},"source":{"7a212832":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9054cc35":"data = pd.read_csv(\"..\/input\/regression-kernel\/column_2C_weka.csv\")","66b06258":"data.tail()","3e501554":"data.loc[:,'class'] = [1 if each == \"Abnormal\" else 0 for each in data.loc[:,'class']]\ny = data.loc[:,'class']\nx_data = data.drop([\"class\"],axis=1)","7f044dc3":"x = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","3609c3eb":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=1)","c4927d49":"# knn model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 25) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(25,knn.score(x_test,y_test)))","da11a033":"# find k value\nscore_list = []\nfor each in range(1,150):\n knn2 = KNeighborsClassifier(n_neighbors = each)\n knn2.fit(x_train,y_train)\n score_list.append(knn2.score(x_test,y_test))","78b55d63":"plt.plot(range(1,150),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","0846c799":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\n# %% test\nprint(\"print accuracy of svm algo: \",svm.score(x_test,y_test))","18dea265":"# %% Naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n# %% test\nprint(\"print accuracy of naive bayes algo: \",nb.score(x_test,y_test))","96a14652":"#%%\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"score: \", dt.score(x_test,y_test))","eb2592a2":"#%% random forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrf.fit(x_train,y_train)\nprint(\"random forest algo result: \",rf.score(x_test,y_test))","36850b8f":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrf.fit(x_train,y_train)\nprint(\"random forest algo result: \",rf.score(x_test,y_test))\ny_pred = rf.predict(x_test)\ny_true = y_test\n#%% confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n# %% cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","6dc2082d":"data2 = data.loc[:,['degree_spondylolisthesis','pelvic_radius']]\nfrom sklearn.cluster import KMeans\nwcss = []\nfor k in range(1,15):\n kmeans = KMeans(n_clusters=k)\n kmeans.fit(data2)\n wcss.append(kmeans.inertia_)\nplt.plot(range(1,15),wcss)\nplt.xlabel(\"number of k (cluster) value\")\nplt.ylabel(\"wcss\")\nplt.show()","16dfacb4":"kmeans2 = KMeans(n_clusters=4)\nclusters = kmeans2.fit_predict(data)\nlabels = clusters\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = labels) #c=color\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()","c3f2fee1":"# %% dendogram\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nmerg = linkage(data2,method=\"ward\")\ndendrogram(merg,leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()","ffd30157":"# %% HC\nfrom sklearn.cluster import AgglomerativeClustering\nhiyerartical_cluster = AgglomerativeClustering(n_clusters = 3,affinity= \"euclidean\",linkage = \"ward\")\ncluster = hiyerartical_cluster.fit_predict(data)\nlabels2 = cluster\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = labels) #c=color\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()","53e53fb4":"%confusion matrix = (21+60)\/93 = 0.8709677419354839\n%Best Method is random forest (accuracy = 0.87)"}}