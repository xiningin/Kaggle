{"cell_type":{"5a4fd6c6":"code","88ff2125":"code","34b50ae2":"code","4ed40b59":"code","590c3f43":"code","8947328d":"code","63c39c56":"code","109136c0":"code","16e70d7f":"code","16776812":"code","678ba701":"code","0d4de19c":"code","0ae0e96a":"code","373d31aa":"code","a47d42dd":"code","fa4a2eec":"code","79336c41":"code","0779b827":"code","9e21cb29":"code","c87ae5df":"code","b3438396":"code","b9fa633a":"code","7ef05e3b":"code","d47b0a3d":"code","5241e391":"code","49a16351":"code","f525a7a0":"code","70775a7b":"code","efd427ad":"code","4062082a":"code","231e5b3e":"code","8a4954f6":"code","c0a29bcc":"code","0142f734":"code","351bd7c6":"code","d978c178":"code","d6cfa303":"code","4a9cfb31":"code","4aa3f221":"code","0d610f31":"code","8f22949b":"code","cdf9b188":"code","0670cc0c":"code","5dbd0d06":"code","f3b237dc":"code","24d65cba":"code","a34ae6c7":"code","a4d9e318":"code","a711d41b":"code","88132b9c":"code","77007233":"code","f554386d":"code","335eea47":"code","fa30e5d8":"code","c1843f0f":"code","791ab3a8":"code","90782fa7":"code","88a69a50":"code","497df55a":"code","13fc8b0b":"code","de07fed6":"code","865e70b2":"code","1df3f2ec":"code","58abba2e":"code","87ccb150":"code","42d90e09":"code","1eb98e9a":"code","0082f17b":"code","05e73116":"code","5e7b4cd1":"code","77d00043":"code","4d1ad70a":"code","c522b979":"code","ef3645ca":"code","fc60fd69":"code","c0759a4a":"code","6495d1b1":"code","de04caa8":"code","d1cd8801":"code","df2f6e0b":"code","760ede3e":"code","98e1edb9":"markdown","daf88893":"markdown","d3fe3738":"markdown","aa522a2b":"markdown","30e0e1b6":"markdown","561375e6":"markdown","2dea32ae":"markdown","c5d4ad9b":"markdown","2155c3de":"markdown","b49235b3":"markdown","36112c0d":"markdown","11f6f7ef":"markdown","950cc09e":"markdown","f8c9c68f":"markdown","510d57b1":"markdown","1bd11d85":"markdown","9e5e5b6d":"markdown","a2158794":"markdown","1f9af27e":"markdown","01f42a40":"markdown","9b24b2f4":"markdown","dc47dbeb":"markdown","714bbf30":"markdown","92a60292":"markdown","3dc2e642":"markdown","74e02312":"markdown","d2573e8b":"markdown"},"source":{"5a4fd6c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n#Import libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nfrom matplotlib import style\nimport math\nimport seaborn as sns \nimport missingno as msno\nfrom datetime import datetime\nimport statsmodels.formula.api as sm\n\n#import the necessary modelling algos.\n\n#classifiaction.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV,LassoCV\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification\n\n\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","88ff2125":"train_data=pd.read_csv(r'..\/input\/bike-sharing-demand\/train.csv')\ntest_data=pd.read_csv(r'..\/input\/bike-sharing-demand\/test.csv')\ndf=train_data.copy()\ntest_df=test_data.copy()\ndf.head()","34b50ae2":"df.columns.unique()","4ed40b59":"df.info()","590c3f43":"df.describe() ","8947328d":"df.isnull().values.any()\n\ndf.isnull().sum()  # implies no null values and hence no imputation needed ::).","63c39c56":"# let us consider season.\ndf.season.value_counts()","109136c0":"sns.barplot(x='season', y='count', data=df)","16e70d7f":"sns.factorplot(x='season',data=df,kind='count',size=5,aspect=1.5)","16776812":"#holiday\ndf.holiday.value_counts()\nsns.barplot(x='holiday', y='count', data=df)\nsns.factorplot(x='holiday',data=df,kind='count',size=5,aspect=1) # majority of data is for non holiday days.","678ba701":"#workingday\ndf.workingday.value_counts()\nsns.barplot(x='workingday', y='count', data=df)\nsns.factorplot(x='workingday',data=df,kind='count',size=5,aspect=1) # majority of data is for working days.","0d4de19c":"#weather\ndf.workingday.value_counts()\nsns.barplot(x='weather', y='count', data=df)\nsns.factorplot(x='weather',data=df,kind='count',size=5,aspect=1) # majority of data is for weather.","0ae0e96a":"df.describe()","373d31aa":"sns.boxplot(data=df[['temp',\n       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count']])\nfig=plt.gcf()\nfig.set_size_inches(10,10)","a47d42dd":"dfWithoutOutliers = df[np.abs(df[\"count\"]-df[\"count\"].mean())<=(3*df[\"count\"].std())] \ndisplay(\"Shape Of the dataframe before Ouliers: \",df.shape)\ndisplay(\"Shape Of the dataframe after Ouliers: \",dfWithoutOutliers.shape)\ndf =dfWithoutOutliers","fa4a2eec":"# can also be visulaized using histograms for all the continuous variables.\ndf.temp.unique()\nfig,axes=plt.subplots(2,2)\naxes[0,0].hist(x=\"temp\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[0,0].set_title(\"Variation of temp\")\naxes[0,1].hist(x=\"atemp\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[0,1].set_title(\"Variation of atemp\")\naxes[1,0].hist(x=\"windspeed\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[1,0].set_title(\"Variation of windspeed\")\naxes[1,1].hist(x=\"humidity\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[1,1].set_title(\"Variation of humidity\")\nfig.set_size_inches(10,10)","79336c41":"#corelation matrix.\ncor_mat= df[:].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)","0779b827":"# # seperating season as per values. this is bcoz this will enhance features.\nseason=pd.get_dummies(df['season'],prefix='season')\ndf=pd.concat([df,season],axis=1)\ndf.head()\nseason=pd.get_dummies(test_df['season'],prefix='season')\ntest_df=pd.concat([test_df,season],axis=1)\ntest_df.head()","9e21cb29":"# # # same for weather. this is bcoz this will enhance features.\nweather=pd.get_dummies(df['weather'],prefix='weather')\ndf=pd.concat([df,weather],axis=1)\ndf.head()\nweather=pd.get_dummies(test_df['weather'],prefix='weather')\ntest_df=pd.concat([test_df,weather],axis=1)\ntest_df.head()","c87ae5df":"# # # now can drop weather and season.\ndf.drop(['season','weather'],inplace=True,axis=1)\ndf.head()\ntest_df.drop(['season','weather'],inplace=True,axis=1)\ntest_df.head()\n\n\n# # # also I dont prefer both registered and casual but for ow just let them both.","b3438396":"df[\"hour\"] = [t.hour for t in pd.DatetimeIndex(df.datetime)]\ndf[\"day\"] = [t.dayofweek for t in pd.DatetimeIndex(df.datetime)]\ndf[\"month\"] = [t.month for t in pd.DatetimeIndex(df.datetime)]\ndf['year'] = [t.year for t in pd.DatetimeIndex(df.datetime)]\ndf['year'] = df['year'].map({2011:0, 2012:1})\ndf.head()","b9fa633a":"test_df[\"hour\"] = [t.hour for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df[\"day\"] = [t.dayofweek for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df[\"month\"] = [t.month for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df['year'] = [t.year for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df['year'] = test_df['year'].map({2011:0, 2012:1})\ntest_df.head()","7ef05e3b":"# now can drop datetime column.\ndf.drop('datetime',axis=1,inplace=True)\ndf.head()","d47b0a3d":"cor_mat= df[:].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)","5241e391":"df.drop(['casual','registered'],axis=1,inplace=True)\ndf.head()","49a16351":"sns.factorplot(x=\"hour\",y=\"count\",data=df,kind='bar',size=5,aspect=1.5)\n# note that time of day affects wheteher people take bike or not. like night time lesser bikes used and using for office commute\n\nsns.factorplot(x=\"day\",y='count',kind='bar',data=df,size=5,aspect=1)\n# note that day has lesser affects wheteher people take bike or not. \n\nsns.factorplot(x=\"month\",y=\"count\",data=df,kind='bar',size=5,aspect=1.5)\n# note that month affects season and that effects wheteher people take bike or not. like climate conditions rainy,hazy etc... .\n\nsns.factorplot(x=\"year\",y=\"count\",data=df,kind='bar',size=5,aspect=1.5)\n# 0 for 2011 and 1 for 2012. Hence demand has increased over the years.","f525a7a0":"# for temp using scatter plot as random values \nplt.scatter(x=\"temp\",y=\"count\",data=df,color='green')\n\n# note that this way this is hard to visualze. \n# a better way is to convert the 'temp' variable into intervals or so called bins and then treat it like a discrete variable\n\ndf_temp=df.copy()\ndf_temp.temp.describe()\ndf_temp['temp_bin']=np.floor(df_temp['temp'])\/\/5\ndf_temp['temp_bin'].unique()\n# now we can visualize as follows\nsns.factorplot(x=\"temp_bin\",y=\"count\",data=df_temp,kind='bar')\n#now the demand is highest for bins 6 and 7 which is about tempearure 30-35(bin 6) and 35-40 (bin 7).","70775a7b":"df.head()","efd427ad":"df.columns.to_series().groupby(df.dtypes).groups","4062082a":"df.dtypes","231e5b3e":"X, y = df.iloc[:, :], df['count']\nX = X.drop('count',axis=1)\nX.head()","8a4954f6":"#Normalize the train set\n#def norm_func(i):\n    #x = (i-i.min())\t\/ (i.max()-i.min())\n    #return (x)\nfrom sklearn.preprocessing import StandardScaler\nscl= StandardScaler()\n#X = scl.fit_transform(X)\n#y = scl.fit_transform(y)\n\nfrom sklearn.model_selection import  train_test_split\n#x_train,x_test,y_train,y_test=train_test_split(df.drop('count',axis=1),df['count'],test_size=0.25,random_state=42)\n#x_train,x_test,y_train,y_test=train_test_split(df.drop('count',axis=1),df['count'],test_size=0.2,random_state=0)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nX_train = scl.fit_transform(X_train)\nX_test = scl.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","c0a29bcc":"alphas = 10**np.linspace(10,-2,100)*0.5\nalphas","0142f734":"coefs = []\n#from sklearn.linear_model import Ridge\nfor a in alphas:\n    model = Ridge(alpha=a, normalize=True)\n    model.fit(X_train,y_train)\n    coefs.append(model.coef_)\n    score = model.score(X_train,y_train)\n    pred2 = model.predict(X_test)\n    mse = mean_squared_error(y_test, pred2) \n    print(\"Alpha:{0:.6f}, R2:{1:.3f}, MSE:{2:.2f}, RMSE:{3:.2f}\".format(a, score, mse, np.sqrt(mse)))\n","351bd7c6":"np.shape(coefs)","d978c178":"ax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale('log')\nplt.axis('tight')\nplt.xlabel('alpha')\nplt.ylabel('weights')","d6cfa303":"#Alpha:0.015269, R2:0.396, MSE:19938.35, RMSE:141.20\nridge_mod=Ridge(alpha=0.015269, normalize=True).fit(X_train,y_train)\npred2 = ridge_mod.predict(X_test)\nscore = model.score(X_test,y_test)\nRmse = mean_squared_error(y_test,pred2)\nprint(\"R2:{0:.3f}, MSE:{1:.2f}, RMSE:{2:.2f}\"\n   .format(score, Rmse,np.sqrt(mse))) \nprint(pd.Series(ridge_mod.coef_, index = X.columns)) # Print coefficients\nprint(mean_squared_error(y_test, pred2))          # Calculate the test MSE\n","4a9cfb31":"#Alpha:0.015269, R2:0.396, MSE:19938.35, RMSE:141.20\nridge_cv=RidgeCV(alphas=alphas,scoring = 'neg_mean_squared_error', normalize = True, store_cv_values=True)\nridge_cv_mod = ridge_cv.fit(X_train,y_train)\nprint(ridge_cv_mod.alpha_)\n#0.01\n#print(np.mean(ridge_mod.cv_values_, axis=0))\n\n\nprint(ridge_mod.intercept_)\nprint(ridge_mod.coef_)\n\nprint(ridge_cv_mod.intercept_)\nprint(ridge_cv_mod.coef_)","4aa3f221":"sns.regplot(y_test,pred2)\nplt.title('Residual Analysis - Ridge_Regression')\nplt.xlabel('Observed')\nplt.ylabel('Residual')\nplt.show()","0d610f31":"from sklearn import metrics\nprint(\"MAE:\", metrics.mean_absolute_error(y_test,pred2))\nprint('MSE:', metrics.mean_squared_error(y_test, pred2))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred2)))","8f22949b":"# grid search hyperparameters for ridge regression\nfrom numpy import arange\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import Ridge","cdf9b188":"# define model\nmodel = Ridge()\n# define model evaluation method\ncv = RepeatedKFold(n_splits=10, random_state=1)\n# define grid\ngrid = dict()\ngrid['alpha'] = arange(0, 1, 0.01)\n# define search\nsearch = GridSearchCV(model, grid, scoring='neg_mean_squared_error', cv=cv)\n# perform the search\nresults = search.fit(X_train, y_train)\n# summarize\nprint('MSE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)\n","0670cc0c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nX_train = scl.fit_transform(X_train)\nX_test = scl.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","5dbd0d06":"coefs = []\n#from sklearn.linear_model import Ridge\nfor a in alphas:\n    Lmodel = Lasso(alpha=a, normalize=True)\n    Lmodel.fit(X_train,y_train)\n    coefs.append(Lmodel.coef_)\n    Lscore = Lmodel.score(X_train,y_train)\n    Lpred2 = Lmodel.predict(X_test)\n    mse = mean_squared_error(y_test, Lpred2) \n    print(\"Alpha:{0:.6f}, R2:{1:.3f}, MSE:{2:.2f}, RMSE:{3:.2f}\".format(a, Lscore, mse, np.sqrt(mse)))\n\n","f3b237dc":"from sklearn.linear_model import LassoCV\nfrom sklearn.datasets import make_regression\n\n#Alpha:0.005000, R2:0.396, MSE:19959.38, RMSE:141.28\nlasso_cv=LassoCV(cv=20)\nlasso_cv_mod = lasso_cv.fit(X_train,y_train)\nprint(lasso_cv_mod.alpha_)\n\n","24d65cba":"print(Lmodel.intercept_)\nprint(Lmodel.coef_)","a34ae6c7":"np.shape(coefs)\n\n","a4d9e318":"#Alpha:0.005000, R2:0.400, MSE:19959.38, RMSE:141.28\n\nax = plt.gca()\nax.plot(alphas*2, coefs)\nax.set_xscale('log')\nplt.axis('tight')\nplt.xlabel('alpha')\nplt.ylabel('weights')","a711d41b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nX_train = scl.fit_transform(X_train)\nX_test = scl.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","88132b9c":"# evaluate an lasso regression model on the dataset\nfrom numpy import mean\nfrom numpy import std\nfrom numpy import absolute\nfrom sklearn.model_selection import cross_val_score\n\n#Alpha:0.005000, R2:0.396, MSE:19959.38, RMSE:141.28\nLmodel = Lasso(alpha=0.005000, normalize=True)\nLmodel.fit(X_train,y_train)\ncoefs.append(Lmodel.coef_)\nLscore = Lmodel.score(X_train,y_train)\nLpred2 = Lmodel.predict(X_test)\nmse = mean_squared_error(y_test, Lpred2) \n\n\n# define model evaluation method\nLcv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define grid\ngrid = dict()\ngrid['alpha'] = arange(0, 1, 0.01)\n# define search\nsearch = GridSearchCV(Lmodel, grid, scoring='neg_mean_absolute_error', cv=Lcv, n_jobs=-1)\n# evaluate model\nscores = cross_val_score(Lmodel, X_train, y_train, scoring='neg_mean_absolute_error', cv=Lcv, n_jobs=-1)\n\n# perform the search\nresults = search.fit(X_train, y_train)\n# summarize\nprint('MAE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)\n\n# force scores to be positive\nscores = absolute(scores)\nprint('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))","77007233":"sns.regplot(y_test,Lpred2)\nplt.title('Residual Analysis - Lasso Regression')\nplt.xlabel('Observed')\nplt.ylabel('Residual')\nplt.show()","f554386d":"print(\"MAE:\", metrics.mean_absolute_error(y_test,Lpred2))\nprint('MSE:', metrics.mean_squared_error(y_test, Lpred2))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, Lpred2)))","335eea47":"# 10 fold CV\nfrom sklearn.linear_model import LassoCV\n## define model evaluation method\ncv = RepeatedKFold(n_splits=10, random_state=1)\n# define model\nmodel = LassoCV(alphas=arange(0, 1, 0.01), cv=cv)\n# fit model\nmodel.fit(X_train, y_train)\n# summarize chosen configuration\nprint('alpha: %f' % model.alpha_)","fa30e5d8":"from sklearn.tree import DecisionTreeRegressor\nreg4 = DecisionTreeRegressor()\nreg4.fit(X_train,y_train)\nreg4.score(X_train,y_train)","c1843f0f":"Dec_Tree = reg4.predict(X_test)\nDec_Tree","791ab3a8":"sns.regplot(y_test,Dec_Tree)\nplt.title('Residual Analysis - Decision Tree Regression')\nplt.xlabel('Observed')\nplt.ylabel('Residual')\nplt.show()","90782fa7":"print(\"MAE:\", metrics.mean_absolute_error(y_test,Dec_Tree ))\nprint('MSE:', metrics.mean_squared_error(y_test, Dec_Tree))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, Dec_Tree)))","88a69a50":"# 10 Fold\nparameters = {'max_depth':range(3,20)}\nclf = GridSearchCV(reg4, parameters,scoring='neg_mean_squared_error', cv=10)\nclf.fit(X_train, y_train)\ntree_model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_)","497df55a":"# Pruning the Tree\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Minimum observations at the internal node approach\nregtree2 = DecisionTreeRegressor(min_samples_split = 3)\nregtree2.fit(X_train, y_train)","13fc8b0b":"# Prediction\ntest_pred2 = regtree2.predict(X_test)\ntrain_pred2 = regtree2.predict(X_train)","de07fed6":"# Error on test dataset\nmean_squared_error(y_test, test_pred2)\nr2_score(y_test, test_pred2)","865e70b2":"# Error on train dataset\nmean_squared_error(y_train, train_pred2)\nr2_score(y_train, train_pred2)","1df3f2ec":"## Minimum observations at the leaf node approach\nregtree3 = DecisionTreeRegressor(min_samples_leaf = 3)\nregtree3.fit(X_train, y_train)\n\n# Prediction\ntest_pred3 = regtree3.predict(X_test)\ntrain_pred3 = regtree3.predict(X_train)\n\n# measure of error on test dataset\nmean_squared_error(y_test, test_pred3)\nr2_score(y_test, test_pred3)\n\n# measure of error on train dataset\nmean_squared_error(y_train, train_pred3)\nr2_score(y_train, train_pred3)","58abba2e":"# 10 Fold DT pruning with leaf node apporoach\nparameters = {'max_depth':range(3,20)}\nclf = GridSearchCV(regtree3, parameters,scoring='neg_mean_squared_error' ,cv=10)\nclf.fit(X_train, y_train)\ntree_model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_)","87ccb150":"# 10 Fold DT pruning internal node approach\nparameters = {'max_depth':range(3,20)}\nclf = GridSearchCV(regtree2, parameters,scoring='neg_mean_squared_error', cv=10)\nclf.fit(X_train, y_train)\ntree_model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_)","42d90e09":"from sklearn.ensemble import RandomForestRegressor\nforest = RandomForestRegressor(n_estimators = 400, criterion='mse',random_state=1, n_jobs=-1)\nforest.fit(X_train, y_train)\ny_train_pred = forest.predict(X_train)\ny_test_pred = forest.predict(X_test)","1eb98e9a":"from sklearn.metrics import mean_squared_error, r2_score\n#Root_Mean_Square_Log_Error(RMSE) is accuracy criteria for this problem\nprint('RMSLE train: %.3f' % np.sqrt(mean_squared_error(np.log(y_train + 1), np.log(y_train_pred + 1))))\nprint('RMSLE test: %.3f' % np.sqrt(mean_squared_error(np.log(y_test + 1), np.log(y_test_pred + 1))))\nprint('R2 train: %.3f' % r2_score(y_train, y_train_pred))\nprint('R2 test: %.3f' % r2_score(y_test, y_test_pred))","0082f17b":"#model = RandomForestClassifier()\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean\nfrom numpy import std\n# evaluate the model\nmodel = RandomForestRegressor()\n# evaluate the model\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(forest, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise')\n# report performance\nprint('MSE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","05e73116":"sns.regplot(y_test,y_test_pred)\nplt.title('Residual Analysis - Random Forest Regression')\nplt.xlabel('Observed')\nplt.ylabel('Residual')\nplt.show()","5e7b4cd1":"import xgboost as xgb\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.ensemble import GradientBoostingRegressor","77d00043":"#model1 = GradientBoostingRegressor()\n\nmodel1 = GradientBoostingRegressor(n_estimators=250, learning_rate=0.1, max_depth=7,subsample=0.9, random_state=42,loss='ls', verbose=2).fit(X_train, y_train)","4d1ad70a":"# fit the model on the whole dataset\nmodel1.fit(X_train, y_train)","c522b979":"# define the evaluation procedure\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate the model\nn_scores = cross_val_score(model1, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# report performance\nprint('MSE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n","ef3645ca":"df.describe()","fc60fd69":"#x_train,x_test,y_train,y_test=train_test_split(df.drop('count',axis=1),df['count'],test_size=0.20,random_state=0)\nX_train,X_test,y_train,y_test=train_test_split(df.drop('count',axis=1),df['count'],test_size=0.25,random_state=42)","c0759a4a":"models=[RandomForestRegressor(),AdaBoostRegressor(),BaggingRegressor(),KNeighborsRegressor()]\nmodel_names=['RandomForestRegressor','AdaBoostRegressor','BaggingRegressor','KNeighborsRegressor']\nrmsle=[]\nmse=[]\nd={}\nfor model in range (len(models)):\n    clf=models[model]\n    clf.fit(X_train,y_train)\n    test_pred=clf.predict(X_test)\n    rmsle.append(np.sqrt(mean_squared_log_error(test_pred,y_test)))\n    mse.append(mean_squared_error(test_pred,y_test))\nd={'Modelling Algo':model_names,'RMSLE':rmsle,'MSE':mse}   \nd\n    ","6495d1b1":"rmsle_frame=pd.DataFrame(d)\nrmsle_frame","de04caa8":"sns.factorplot(x='Modelling Algo',y='RMSLE',data=rmsle_frame,kind='bar',size=5,aspect=2)\nsns.factorplot(x='Modelling Algo',y='MSE',data=rmsle_frame,kind='bar',size=5,aspect=2)","d1cd8801":"#for random forest regresion.\nno_of_test=[1000]\nparams_dict={'n_estimators':no_of_test,'n_jobs':[-1],'max_features':[\"auto\",'sqrt','log2']}\nclf_rf=GridSearchCV(estimator=RandomForestRegressor(),param_grid=params_dict,scoring='neg_mean_squared_log_error')\nclf_rf.fit(X_train,y_train)\npred=clf_rf.predict(X_test)\nprint((np.sqrt(mean_squared_log_error(pred,y_test))))","df2f6e0b":"clf_rf.best_params_","760ede3e":"pred=clf_rf.predict(test_df.drop('datetime',axis=1))\nd={'datetime':test_data['datetime'],'count':pred}\nans=pd.DataFrame(d)\nans.to_csv('submission.csv',index=False) # saving to a csv file for predictions.","98e1edb9":"RANDOM FORETS REGRESSOR GIVES THE LEAST RMSLE. HENCE WE USE IT TO MAKE PREDICTIONS","daf88893":"# **Cross Validation Lasso**","d3fe3738":"# **A SHORT DESCRIPTION OF THE FEATURES**\n\ndatetime - hourly date + timestamp\n\nseason - 1 = spring, 2 = summer, 3 = fall, 4 = winter\n\nholiday - whether the day is considered a holiday\n\nworkingday - whether the day is neither a weekend nor holiday\n\nweather -\n\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\ntemp - temperature in Celsius\n\natemp - \"feels like\" temperature in Celsius\n\nhumidity - relative humidity\n\nwindspeed - wind speed\n\ncasual - number of non-registered user rentals initiated\n\nregistered - number of registered user rentals initiated\n\ncount - number of total rentals","aa522a2b":"# **C. Ridge Regression**","30e0e1b6":"# **H. Optimal Model**","561375e6":"# **# Data Exploration**","2dea32ae":"The best result given by Random Forest across A-H.","c5d4ad9b":"# **E1. Regression Tree**","2155c3de":"# Find Correlation for NEW FEATURES","b49235b3":"# **G. Gradient Boosting**","36112c0d":"*The 10-Fold Cross-Validation Error 2*","11f6f7ef":"*COUNT VARIATION WITH DIFFERENT FEATURES*","950cc09e":"*ALL THE VARIABLES OR FEATURES ARE NUMERIC AND THE TARGET VARIABLE THAT WE HAVE TO PREDICT IS THE count VARIABLE. HENCE THIS IS A TYPICAL EXAMPLE OF A **REGRESSION PROBLEM** AS THE count VARIABLE IS CONTINUOUS VARIED.*","f8c9c68f":"# **D. Lasso Regression**","510d57b1":"FEATURE ENGINEERING AND GET SOME NEW FEATURES AND DROP SOME USELESS OR LESS RELEVANT FEATURES","1bd11d85":"no missing value.","9e5e5b6d":"INFERENCES FROM THE ABOVE HEATMAP--\nself realtion i.e. of a feature to itself is equal to 1 as expected.\n\ntemp and atemp are highly related as expected.\n\nhumidity is inversely related to count as expected as the weather is humid people will not like to travel on a bike.\n\nalso note that casual and working day are highly inversely related as you would expect.\n\nAlso note that count and holiday are highly inversely related as you would expect.\n\nAlso note that temp(or atemp) highly effects the count.\n\nAlso note that weather and count are highly inversely related. This is bcoz for uour data as weather increases from (1 to 4) implies that weather is getting more worse and so lesser people will rent bikes.\n\nregistered\/casual and count are highly related which indicates that most of the bikes that are rented are registered.\n\nsimilarly we can draw some more inferences like weather and humidity and so on... .","a2158794":"# **E2. Decision Tree with Pruning**","1f9af27e":"# **A. Data Preprocessing**\ncheck Null values","01f42a40":"# **F. Random Forest**","9b24b2f4":"# **Compute the MSE for the Test Data & Compare with the CV Error**","dc47dbeb":"Random Forest with 10 fold cross validation was giving the least error and high accuracy across all the models from A-H\nSimilar apporach within train and validation can be apply to the given test data","714bbf30":"# **Identify Unique Columns**","92a60292":"# **B. Data Wrangling **","3dc2e642":"# Applying Machine Learning Models","74e02312":"# **DATA MODELLING**","d2573e8b":"# Read Training and Test Data"}}