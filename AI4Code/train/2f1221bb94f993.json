{"cell_type":{"7acf4bfa":"code","2def6382":"code","37ff8f82":"code","85a616d5":"code","53e32fdb":"code","94e259d8":"code","16336e79":"code","95fa0bbe":"code","ec91a1e6":"code","f5bfe8bc":"code","8db9ea83":"code","f28c3cb1":"code","577761d1":"code","c1949ce7":"code","8016c4f2":"code","bc2e664e":"code","c9cd7033":"code","b5557504":"code","f597fae0":"code","5a820dc0":"code","d448aab0":"code","8213dbfc":"code","51261886":"markdown","18f29db3":"markdown","2ab88f26":"markdown","8739745f":"markdown","3aefea39":"markdown","94f8f185":"markdown","eafa7908":"markdown","7398712c":"markdown","624b012c":"markdown","aa254b6b":"markdown","19e54f80":"markdown","7d6bb507":"markdown","555e5354":"markdown","19460d8e":"markdown","6b4cc3c2":"markdown","3b01ade9":"markdown","13884f24":"markdown"},"source":{"7acf4bfa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2def6382":"data = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndef display_all(data):\n    with pd.option_context(\"display.max_row\", 1000, \"display.max_columns\", 1000):\n        display(data)\n        \ndisplay_all(data.head())","37ff8f82":"data.Class.value_counts()","85a616d5":"df1 = data.copy()","53e32fdb":"y = df1.Class\nX = df1.drop(\"Class\", axis = 1)","94e259d8":"print(y.head())\nprint(X.head())","16336e79":"from sklearn.model_selection import train_test_split\nfrom sklearn.dummy import DummyClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 27)\ndummy_clf = DummyClassifier(strategy = \"most_frequent\")\ndummy_clf.fit(X, y)\ndummy_clf.predict(X)\ndummy_clf.score(X, y)","95fa0bbe":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nloj_model = LogisticRegression(solver = \"liblinear\").fit(X_train, y_train)\nloj_pred = loj_model.predict(X_test)\naccuracy_score(y_test, loj_pred)","ec91a1e6":"from sklearn.metrics import f1_score,recall_score\nprint(f\"f1 score: {f1_score(y_test, loj_pred)}\")\nprint(f\"recall_score: {recall_score(y_test, loj_pred)}\")","f5bfe8bc":"df2 = df1.copy()","8db9ea83":"y = df2.Class\nX = df2.drop(\"Class\", axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)","f28c3cb1":"from sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(n_estimators = 10).fit(X_train, y_train)\nrf_pred = rf_model.predict(X_test)\naccuracy_score(y_test, rf_pred)","577761d1":"print(f\"f1 score: {f1_score(y_test, rf_pred)}\")\nprint(f\"recall_score: {recall_score(y_test,rf_pred)}\")","c1949ce7":"from sklearn.utils import resample\n\ny = data.Class\nX = data.drop('Class', axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n\n\nX = pd.concat([X_train, y_train], axis=1)\n\n\nnot_fraud = X[X.Class==0]\nfraud = X[X.Class==1]\n\n\nfraud_upsampled = resample(fraud,\n                          replace=True,\n                          n_samples=len(not_fraud),\n                          random_state=27)\n\nupsampled = pd.concat([not_fraud, fraud_upsampled])\n\n\nupsampled.Class.value_counts()","8016c4f2":"y_train = upsampled.Class\nX_train = upsampled.drop('Class', axis=1)\n\nupsampled = LogisticRegression(solver = \"liblinear\").fit(X_train, y_train)\n\nupsampled_pred = upsampled.predict(X_test)\naccuracy_score(y_test, upsampled_pred)","bc2e664e":"print(f\"f1 score: {f1_score(y_test, upsampled_pred)}\")\nprint(f\"recall_score: {recall_score(y_test,upsampled_pred)}\")","c9cd7033":"not_fraud_downsampled = resample(not_fraud,\n                                replace = False, \n                                n_samples = len(fraud),\n                                random_state = 27)\n\ndownsampled = pd.concat([not_fraud_downsampled, fraud])\n\ndownsampled.Class.value_counts()","b5557504":"y_train = downsampled.Class\nX_train = downsampled.drop('Class', axis=1)\n\nundersampled = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n\nundersampled_pred = undersampled.predict(X_test)\n\naccuracy_score(y_test, undersampled_pred)","f597fae0":"print(f\"f1 score: {f1_score(y_test, undersampled_pred)}\")\nprint(f\"recall_score: {recall_score(y_test,undersampled_pred)}\")","5a820dc0":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=27)\nX_train, y_train = sm.fit_sample(X_train, y_train)","d448aab0":"smote = LogisticRegression(solver = \"liblinear\").fit(X_train, y_train)\nsmote_pred = smote.predict(X_test)\naccuracy_score(y_test, smote_pred)","8213dbfc":"print(f\"f1 score: {f1_score(y_test, smote_pred)}\")\nprint(f\"recall_score: {recall_score(y_test,smote_pred)}\")","51261886":"# 3. Resampling Techniques \u2014 Oversample minority class\n*Our next method begins our resampling techniques.\nOversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don\u2019t have a ton of data to work with.\nWe will use the resampling module from Scikit-Learn to randomly replicate samples from the minority class.*","18f29db3":"**Our recall score increased, but F1 is much lower than with either our baseline logistic regression or random forest from above. Let\u2019s see if undersampling might perform better here.**","2ab88f26":"# 1. Change the performance metric\n*As we saw above, accuracy is not the best metric to use when evaluating imbalanced datasets as it can be very misleading. Metrics that can provide better insight include:\n\n* **Confusion Matrix:** a table showing correct predictions and types of incorrect predictions.\n* **Precision:** the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier\u2019s exactness. Low precision indicates a high number of false positives.\n* **Recall:** the number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier\u2019s completeness. Low recall indicates a high number of false negatives.\n* **F1: Score:** the weighted average of precision and recall.","8739745f":"**These scores don\u2019t look quite so impressive. Let\u2019s see what other methods we might try to improve our new metrics.**","3aefea39":"**After resampling we have an equal ratio of data points for each class! Let\u2019s try our logistic regression again with the balanced training data.**","94f8f185":"# Important Note\n*Always split into test and train sets BEFORE trying oversampling techniques! Oversampling before splitting the data can allow the exact same observations to be present in both the test and train sets. This can allow our model to simply memorize specific data points and cause overfitting and poor generalization to the test data.*","eafa7908":"**While our accuracy score is slightly lower, both F1 and recall have increased as compared to logistic regression! It appears that for this specific problem, random forest may be a better choice of model.**","7398712c":"# 2. Change the algorithm\n*While in every machine learning problem, it\u2019s a good rule of thumb to try a variety of algorithms, it can be especially beneficial with imbalanced datasets. Decision trees frequently perform well on imbalanced data. They work by learning a hierarchy of if\/else questions and this can force both classes to be addressed.*","624b012c":"# 4. Resampling techniques \u2014 Undersample majority class\n*Undersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback is that we are removing information that may be valuable. This could lead to underfitting and poor generalization to the test set.\nWe will again use the resampling module from Scikit-Learn to randomly remove samples from the majority class.*","aa254b6b":"# The Problem with Accuracy\n*Here we can use the DummyClassifier to always predict \u201cnot fraud\u201d just to show how misleading accuracy can be.*","19e54f80":"**Again, we have an equal ratio of fraud to not fraud data points, but in this case a much smaller quantity of data to train the model on. Let\u2019s again apply our logistic regression.**","7d6bb507":"**Let\u2019s see what happens when we apply these F1 and recall scores to our logistic regression from above.**","555e5354":"# 5. Generate synthetic samples\n**A technique similar to upsampling is to create synthetic samples. Here we will use imblearn\u2019s SMOTE or Synthetic Minority Oversampling Technique. SMOTE uses a nearest neighbors algorithm to generate new and synthetic data we can use for training our model.\nAgain, it\u2019s important to generate the new samples only in the training set to ensure our model generalizes well to unseen data.**","19460d8e":"**Our F1 score is increased and recall is similar to the upsampled model above and for our data here outperforms undersampling.**","6b4cc3c2":"# The Problem with Imbalanced Classes\n\n*Most machine learning algorithms work best when the number of samples in each class are about equal. This is because most algorithms are designed to maximize accuracy and reduce error.*","3b01ade9":"**We got an accuracy score of 99.8% \u2014 And without even training a model! Let\u2019s compare this to logistic regression, an actual trained classifier.**","13884f24":"# Dealing with Imbalanced Data\n\n*Imbalanced classes are a common problem in machine learning classification where there are a disproportionate ratio of observations in each class. Class imbalance can be found in many different areas including medical diagnosis, spam filtering, and fraud detection.*"}}