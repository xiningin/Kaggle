{"cell_type":{"a3f972d5":"code","b86436ee":"code","938339dc":"code","2b71d15d":"code","f6a0bd2f":"code","528d721e":"code","17849f4d":"code","9596dcdf":"markdown"},"source":{"a3f972d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b86436ee":"def permute_predict(y):\n    _y = y.copy()\n    _c1 = _y < 0.00001\n    _c2 = _y > 0.99999\n    _y[_c1] = _y[_c1].max() - _y[_c1] + _y[_c1].min()\n    _y[_c2] = _y[_c2].max() - _y[_c2] + _y[_c2].min()\n    return _y","938339dc":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.mixture import GaussianMixture as GMM\nfrom sklearn.mixture import BayesianGaussianMixture as BGM\nfrom tqdm import tqdm_notebook\nfrom sklearn.covariance import GraphicalLasso\n\nimport warnings\nwarnings.filterwarnings('ignore')","2b71d15d":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain.head()","f6a0bd2f":"cols = [c for c in train.columns if c not in ['id', 'target']]\ncols.remove('wheezy-copper-turtle-magic')","528d721e":"oof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\nfor i in tqdm_notebook(range(512)):\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = VarianceThreshold(2.3).fit_transform(data[cols])\n    train3 = data2[:train2.shape[0]]; test3 = data2[train2.shape[0]:]\n    \n    for c in range(train3.shape[1]):\n        low_=np.quantile(train3[:,c] , 0.001)\n        up_=np.quantile(train3[:,c], 0.999)\n        train3[:,c]=np.clip(train3[:,c],low_, up_ )\n        test3[:,c]=np.clip(test3[:,c],low_, up_ )\n        \n#     train3 = ((train3) \/ data[:train2.shape[0]].std(axis=1)[:, np.newaxis])\n#     test3 = ((test3) \/ data[train2.shape[0]:].std(axis=1)[:, np.newaxis])\n    \n    skf = StratifiedKFold(n_splits=11, random_state=42)\n    for train_index, test_index in skf.split(train2, train2['target']):\n        gmm=GMM(n_components=5, random_state=42, covariance_type='full')\n        gmm.fit(np.vstack([train3[train_index], test3]))\n        gmm_1_train=gmm.predict_proba(train3[train_index])\n        gmm_1_val=gmm.predict_proba(train3[test_index])\n        gmm_1_test=gmm.predict_proba(test3)\n\n        gmm=GMM(n_components=4, random_state=42, covariance_type='full')\n        gmm.fit(np.vstack([train3[train_index], test3]))\n        gmm_2_train=gmm.predict_proba(train3[train_index])\n        gmm_2_val=gmm.predict_proba(train3[test_index])\n        gmm_2_test=gmm.predict_proba(test3)\n\n        gmm=GMM(n_components=6, random_state=42, covariance_type='full')\n        gmm.fit(np.vstack([train3[train_index], test3]))\n        gmm_3_train=gmm.predict_proba(train3[train_index])\n        gmm_3_val=gmm.predict_proba(train3[test_index])\n        gmm_3_test=gmm.predict_proba(test3)\n\n\n\n        bgm=BGM(n_components=5, random_state=42)\n        bgm.fit(np.vstack([train3[train_index], test3]))\n        bgm_1_train=bgm.predict_proba(train3[train_index])\n        bgm_1_val=bgm.predict_proba(train3[test_index])\n        bgm_1_test=bgm.predict_proba(test3)\n\n        bgm=BGM(n_components=4, random_state=42)\n        bgm.fit(np.vstack([train3[train_index], test3]))\n        bgm_2_train=bgm.predict_proba(train3[train_index])\n        bgm_2_val=bgm.predict_proba(train3[test_index])\n        bgm_2_test=bgm.predict_proba(test3)\n\n        bgm=BGM(n_components=6, random_state=42)\n        bgm.fit(np.vstack([train3[train_index], test3]))\n        bgm_3_train=bgm.predict_proba(train3[train_index])\n        bgm_3_val=bgm.predict_proba(train3[test_index])\n        bgm_3_test=bgm.predict_proba(test3)\n    \n        _train = np.hstack((train3[train_index],\n                            gmm_1_train, gmm_2_train, gmm_3_train,\n                            bgm_1_train, bgm_2_train, bgm_3_train))\n        _val = np.hstack((train3[test_index],\n                            gmm_1_val, gmm_2_val, gmm_3_val,\n                            bgm_1_val, bgm_2_val, bgm_3_val))\n        _test = np.hstack((test3,\n                            gmm_1_test, gmm_2_test, gmm_3_test,\n                            bgm_1_test, bgm_2_test, bgm_3_test))\n        clf = QuadraticDiscriminantAnalysis(reg_param=0.04, tol=0.01) #0.04 bst - 0.9734+\n        clf.fit(_train,train2.loc[train_index]['target'])\n        \n        oof[idx1[test_index]] = clf.predict_proba(_val)[:,1]\n        preds[idx2] += clf.predict_proba(_test)[:,1] \/ skf.n_splits\n    print(i, roc_auc_score(train2['target'], oof[idx1]))\nprint(roc_auc_score(train['target'], oof))","17849f4d":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = permute_predict(preds)\nsub.to_csv('submission.csv',index=False)\n\nimport matplotlib.pyplot as plt\nplt.hist(preds,bins=100)\nplt.title('Final Test.csv predictions')\nplt.show()","9596dcdf":"### IDEA\n    If you know about make clf module from sklearn. You see that make clf returns Gaussians.\n    This fact can help you to choose very good unsupervised algo to improve your score -> Gaussian and Bayessian Mixture Model. \n    GMM and BMM give us probability of belonging n clusters with gaussian distribution."}}