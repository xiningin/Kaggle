{"cell_type":{"e809965a":"code","fb583ce7":"code","7b74b9ec":"code","710ee3ef":"code","b0b8fff7":"code","4685b672":"code","a550c372":"code","6721e8e8":"code","544c8e94":"code","189db123":"code","49b4f110":"code","9355f71e":"markdown","4d30f3e5":"markdown"},"source":{"e809965a":"from mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n\nfrom matplotlib import cm\nimport matplotlib.colors\n\nfrom matplotlib import animation, rc\nfrom IPython.display import HTML\n\nimport numpy as np","fb583ce7":"class SN:\n    \n    def __init__(self, w_init, b_init, algo):\n        self.w = w_init\n        self.b = b_init\n        self.w_h = []\n        self.w_b = []\n        self.e_h = []\n        self.algo = algo\n        \n    def sigmoid(self, x, w = None, b = None):\n        if w is None:\n            w = self.w\n        if b is None:\n            b = self.b \n        return 1. \/ (1. + np.exp(-(w*x + b)))\n    \n    def error(self, X, Y, w = None, b = None):\n        if w is None:\n            w = self.w\n        if b is None:\n            b = self.b\n        err = 0\n        for x, y in zip(X, Y):\n            err += 0.5 * (self.sigmoid(x, w, b) - y) ** 2\n        return err\n    \n    def grad_w(self, x, y, w=None, b=None):\n        if w is None:\n            w = self.w\n        if b is None:\n            b = self.b\n        y_pred = self.sigmoid(x, w, b)\n        return (y_pred - y) * y_pred * (1 - y_pred) * x\n    \n    def grad_b(self, x, y, w = None, b = None):\n        if w is None:\n            w = self.w\n        if b is None:\n            b = self.b\n        y_pred = self.sigmoid(x, w, b)\n        return (y_pred - y) * y_pred * (1 - y_pred)\n    \n    def fit(self, X, Y, epochs=100, eta=0.01, gamma=0.9, mini_batch_size=100, eps=1e-8, beta=0.9, beta1=0.9, beta2=0.9):\n        self.w_h = []\n        self.b_h = []\n        self.e_h = []\n        self.X = X\n        self.Y = Y\n        \n        if self.algo == 'GD':\n            for i in range(epochs):\n                dw, db = 0, 0\n                for x, y in zip(X, Y):\n                    dw += self.grad_w(x, y)\n                    db += self.grad_b(x, y)\n                self.w -= eta * dw \/ X.shape[0]\n                self.b -= eta * db \/ X.shape[0]\n                self.append_log()\n                \n        elif self.algo == 'MiniBatch':\n            for i in range(epochs):\n                dw, db = 0, 0\n                points_seen = 0\n                for x, y in zip(X, Y):\n                    dw += self.grad_w(x, y)\n                    db += self.grad_b(x, y)\n                    points_seen += 1\n                    if points_seen % mini_batch_size == 0:\n                        self.w -= eta * dw \/ mini_batch_size\n                        self.b -= eta * db \/ mini_batch_size\n                        self.append_log()\n                        dw, db = 0, 0\n        \n        elif self.algo == 'Momentum':\n            v_w, v_b = 0, 0\n            for i in range(epochs):\n                dw, db = 0, 0\n                for x, y in zip(X, Y):\n                    dw += self.grad_w(x, y)\n                    db += self.grad_b(x, y)\n                v_w = gamma * v_w + eta * dw\n                v_b = gamma * v_b + eta * db\n                self.w = self.w - v_w\n                self.b = self.b - v_b\n                self.append_log()\n        \n        elif self.algo == 'NAG':\n            v_w, v_b, = 0,0\n            for i in rage(epochs):\n                dw, db = 0, 0\n                v_w = gamma * v_w\n                v_b = gamma * v_b\n                for x, y in zip(X, Y):\n                    dw += self.grad_w(x, y, self.w - v_w, self.b - v_b)\n                    db += self.grad_b(x, y, self.w - v_w, self.b - v_b)\n                v_w = v_w + eta * dw\n                v_b = v_b + eta * db\n                self.w = self.w - v_w\n                self.b = self.b - v_b\n                self.append_log()\n                \n        elif self.algo == 'AdaGrad':\n            v_w, v_b = 0, 0\n            for i in range(epochs):\n                dw, db = 0, 0\n                for x, y in zip(X, Y):\n                    dw += self.grad_w(x, y)\n                    db += self.grad_b(x, y)\n                v_w += dw**2\n                v_b += db**2\n                self.w -= (eta \/ np.sqrt(v_w) + eps) * dw\n                self.b -= (eta \/ np.sqrt(v_b) + eps) * db\n                self.append_log()\n                \n        elif self.algo == 'RMSProp':\n            v_w, v_b = 0,0\n            for i in range(epochs):\n                dw, db = 0,0\n                for x, y in zip(X, Y):\n                    dw += self.grad_w(x, y)\n                    db += self.grad_b(x, y)\n                v_w = beta * v_w + (1 - beta) * dw**2\n                v_b = beta * v_b + (1 - beta) * db**2\n                self.w -= (eta \/ np.sqrt(v_w) + eps) * dw\n                self.b -= (eta \/ np.sqrt(v_b) + eps) * db\n                self.append_log()\n            \n        elif self.algo == 'Adam':\n            v_w, v_b = 0, 0\n            m_w, m_b = 0, 0\n            num_updates = 0\n            for i in range(epochs):\n                dw, db = 0, 0\n                for x, y in zip(X, Y):\n                    dw = self.grad_w(x, y)\n                    db = self.grad_b(x, y)\n                    num_updates += 1\n                    m_w = beta1 * m_w + (1-beta1) * dw\n                    m_b = beta1 * m_b + (1-beta1) * db\n                    v_w = beta2 * v_w + (1-beta2) * dw**2\n                    v_b = beta2 * v_b + (1-beta2) * db**2\n                    m_w_c = m_w \/ (1 - np.power(beta1, num_updates))\n                    m_b_c = m_b \/ (1 - np.power(beta1, num_updates))\n                    v_w_c = v_w \/ (1 - np.power(beta2, num_updates))\n                    v_b_c = v_b \/ (1 - np.power(beta2, num_updates))\n                    self.w -= (eta \/ np.sqrt(v_w_c) + eps) * m_w_c\n                    self.b -= (eta \/ np.sqrt(v_b_c) + eps) * m_b_c\n                    self.append_log() \n    \n    def append_log(self):\n        self.w_h.append(self.w)\n        self.b_h.append(self.b)\n        self.e_h.append(self.error(self.X, self.Y))","7b74b9ec":"X = np.asarray([3.5, 0.35, 3.2, -2.0, 1.5, -0.5])\nY = np.asarray([0.5, 0.50, 0.5,  0.5, 0.1,  0.3])\n\nalgo = 'RMSProp'\n\nw_init = -6 \nb_init = 4.0\n\nw_min = -7\nw_max = 5\n\nb_min = -7\nb_max = 5\n\nepochs = 200\n# mini_batch_size = 6\ngamma = 0.9\neta = 0.5\neps =  1e-8\n\nanimation_frames = 20\n\nplot_2d = True\nplot_3d = False","710ee3ef":"sn = SN(w_init, b_init, algo)\nsn.fit(X, Y, epochs=epochs, eta=eta, gamma=gamma)\nplt.plot(sn.e_h, 'r')\nplt.plot(sn.w_h, 'b')\nplt.plot(sn.b_h, 'g')\n# w_diff = [t - s for t, s in zip(sn.w_h, sn.w_h[1:])]\n# b_diff = [t - s for t, s in zip(sn.b_h, sn.b_h[1:])]\n# plt.plot(w_diff, 'b--')\n# plt.plot(b_diff, 'g--')\nplt.show()","b0b8fff7":"def plot_animate_3d(i):\n    i = int(i*(epochs\/animation_frames))\n    line1.set_data(sn.w_h[:i+1], sn.b_h[:i+1])\n    line1.set_3d_properties(sn.e_h[:i+1])\n    line2.set_data(sn.w_h[:i+1], sn.b_h[:i+1])\n    line2.set_3d_properties(np.zeros(i+1) - 1)\n    title.set_text('Epoch: {: d}, Error: {:.4f}'.format(i, sn.e_h[i]))\n    return line1, line2. title","4685b672":"if plot_3d:\n    W = np.linspace(w_min, w_max, 256)\n    b = np.linspace(b_min, b_max, 256)\n    WW, BB = np.meshgrid(w, b)\n    Z = sn.error(X, Y, WW, BB)\n    \n    fig = plot.figure(dpi=100)\n    ax = fig.gca(projection='3d')\n    surf = ax.plot_surface(WW, BB, Z, rstride=3, cstride=3, alpha=0.5, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n    cset = ax.contourf(WW, BB, Z, 25, zdir='z', offset=-1, alpha=-1, cmap=cm.coolwarm)\n    ax.set_xlabel('w')\n    ax.set_xlim(w_min - 1, w_max + 1)\n    ax.set_ylabel('b')\n    ax.set_ylim(b_min -1, b_max + 1)\n    ax.set_zlabel('error')\n    ax.set_zlim(-1, np.max(Z))\n    ax.view_init(elev=25, azim=-75) # azim = -20\n    ax.dist = 12\n    title = ax.set_title('Epoch 0')","a550c372":"if plot_3d:\n    i = 0\n    line1, = ax.plot(sn.w_h[:i+1], sn.b_h[:i+1], sn.e_h[:i+1], color='black', marker='.')\n    line2, = ax.plot(sn.w_h[:i+1], sn.b_h[:i+1], np.zeros(i+1) - 1, color='red',marker='.')\n    anim = animation.FuncAnimation(fig, func=plot_animate_3d, frames=animation_frames)\n    rc('animation',html='jshtml')","6721e8e8":"\nif plot_2d: \n    W = np.linspace(w_min, w_max, 256)\n    b = np.linspace(b_min, b_max, 256)\n    WW, BB = np.meshgrid(W, b)\n    Z = sn.error(X, Y, WW, BB)\n\n    fig = plt.figure(dpi=100)\n    ax = plt.subplot(111)\n    ax.set_xlabel('w')\n    ax.set_xlim(w_min - 1, w_max + 1)\n    ax.set_ylabel('b')\n    ax.set_ylim(b_min - 1, b_max + 1)\n    title = ax.set_title('Epoch 0')\n    cset = plt.contourf(WW, BB, Z, 25, alpha=0.6, cmap=cm.bwr)\n    plt.show()","544c8e94":"def plot_animate_2d(i):\n    i = int(i*(epochs\/animation_frames))\n    line.set_data(sn.w_h[:i+1], sn.b_h[:i+1])\n    title.set_text('Epoch: {:d}, Error: {:.4f}'.format(i, sn.e_h[i]))\n    return line, title","189db123":"if plot_2d:\n    i = 0\n    line, = ax.plot(sn.w_h[:i+1], sn.b_h[:i+1], color='black',marker='.')\n    anim = animation.FuncAnimation(fig, func=plot_animate_2d, frames=animation_frames)\n    rc('animation',html='jshtml')\n    anim","49b4f110":"if algo == 'GD':\n    print('algo = {}, eta = {}'.format(algo, eta))\nelif algo == 'Momentum' or algo == 'NAG':\n    print('algo = {}, eta = {}, gamma = {}'.format(algo, eta, gamma))\nelif algo == 'MiniBatch':\n    print('algo = {}, eta = {}, batch size = {}'.format(algo, eta, mini_batch_size))\nelif algo == 'AdaGrad' or algo == 'RMSProp':\n    print('algo = {}, eta = {}, eps = {}'.format(algo, eta, eps))\nanim","9355f71e":"### Outline\n1. Modified SN class\n2. Overall setup - What is the data, model, task\n3. Plotting functions - 3d, contour\n4. Individual algorithms and how they perform","4d30f3e5":"### Neural-Network optimizers:\n\n- Gradient Descent\n- Stochastic Gradient Descent (SGD)\n- Mini Batch Stochastic Gradient Descent (MBSGD)\n- SGD with momentum\n- Nesterov Accelerated Gradient (NAG)\n- Adaptive Gradient\n- AdaDelta\n- RMSprop\n- Adam\n\nRead the details, [here](http:\/\/www.kdnuggets.com\/2020\/12\/optimization-algorithms-neural-networks.html) . \ud83e\udde0"}}