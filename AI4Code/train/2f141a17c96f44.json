{"cell_type":{"2dd59a4b":"code","f6a89697":"code","ffb632e1":"code","29360892":"code","925841bb":"code","1460d48d":"code","305c870d":"code","4e2accb3":"code","a76ec9cc":"code","eeae6c85":"code","ef19a691":"code","87fe2e90":"code","15c38e91":"code","a0ab40c8":"code","5f50b791":"code","261de6fa":"code","4a45ed3c":"code","883df329":"code","3b91cc04":"code","bca9b215":"code","46cfc134":"code","77ada1bb":"code","1ecacf36":"code","3cfbf0f5":"code","bc5a5207":"code","d1825ee7":"code","fe97829d":"code","77585b8c":"code","5dbfb43f":"code","a6f18657":"code","226923a9":"code","96f276bd":"code","7fcce1f6":"code","931dcb5c":"code","ac4471ca":"code","0bc2b502":"code","23394b09":"code","8a6423d4":"code","21568246":"code","79baf0f2":"code","6a585751":"code","24010cc9":"code","0eb3519e":"code","a4012fc1":"code","c1b5ae1a":"code","65799935":"code","3e6b8eb0":"code","39d461be":"code","ca6d9e02":"code","116b4b65":"code","373af443":"code","0d20f784":"code","53cf2944":"code","7d9ad833":"code","15d7baed":"code","9af6587c":"code","527b6911":"code","5cde5ef9":"code","0d4e37a9":"code","48c3407e":"code","ddba268c":"code","1efa5ac8":"code","e9dcebf9":"code","f39501be":"code","eb400351":"code","ef599133":"code","3e08a8e3":"code","35455099":"code","70463fe7":"code","097ad76d":"code","39a5a7b8":"code","1c196463":"code","89124d63":"code","65581b2f":"code","d66b94d9":"code","d9016062":"markdown","c01e8bc8":"markdown","6d2eaacc":"markdown","2966df80":"markdown","01c82c5d":"markdown","63383eec":"markdown","5e677f2e":"markdown","c17461b5":"markdown","39125d1f":"markdown","b1cf3801":"markdown","51deee2f":"markdown","9e4775e8":"markdown","ceb033ac":"markdown","a32eaaa3":"markdown","318229c4":"markdown","8bebc8a0":"markdown","f543ac9e":"markdown","d59b1264":"markdown","8ddf5dad":"markdown","b3f93a95":"markdown","94f8b681":"markdown","ac8cf13c":"markdown","48774028":"markdown","03326480":"markdown","4f38472a":"markdown","37aa7352":"markdown","1f4ba191":"markdown","22ecc162":"markdown","d3353a4f":"markdown","a6aceff1":"markdown","cd8c63b2":"markdown","90fc696f":"markdown","66be97df":"markdown","9268b821":"markdown","21732c2b":"markdown","70abb399":"markdown","ac355364":"markdown","7c94a283":"markdown","652f5b4d":"markdown","786c532f":"markdown","70693649":"markdown","05cbc8d1":"markdown","2bb632af":"markdown","7ebbfe63":"markdown","a877ec76":"markdown","06774466":"markdown","7fb17af3":"markdown","b690824f":"markdown","70ec43ac":"markdown","ac2b7462":"markdown","0a655204":"markdown","54b9af90":"markdown"},"source":{"2dd59a4b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","f6a89697":"def show_double_hist(x_1, x_0, n_bins=11, title='', label_1='Class 1', \n                          label_0='Class 0', density=0):\n    '''\n    Receives two probability distributions and represents them\n    on the same graph\n    '''\n    bins = n_bins\n    plt.hist(x_1, bins, density = density, alpha=0.5, label=label_1, color='red')    \n    plt.hist(x_0, bins, density = density, alpha=0.5, label=label_0, color='green')\n    plt.title(title)\n    plt.legend(loc='best') \n\ndef hist_pos_neg_feat(x, y, density=0, nbins=11, targets=(0,1)):\n    '''\n    Represent the variables in x divided into two distributions\n    depending on its 'y' values\n    '''\n    fig_tot = len(x.columns)\n    fig_tot_fila = 4.; fig_tamanio = 4.\n    num_filas = int( np.ceil(fig_tot\/fig_tot_fila) )\n    plt.figure( figsize=( fig_tot_fila*fig_tamanio+2, num_filas*fig_tamanio+2 ) )\n    target_neg, target_pos = targets\n    for i, feat in enumerate(x.columns):\n        plt.subplot(num_filas, fig_tot_fila, i+1);\n        plt.title('%s' % feat)\n        idx_pos = y == target_pos\n        idx_neg= y == target_neg\n        show_double_hist(x[feat][idx_pos].values, x[feat][idx_neg].values, nbins, \n                   density = density, title=('%s' % feat))","ffb632e1":"XY = pd.read_csv('..\/input\/online-shoppers-intention\/online_shoppers_intention.csv')\n\nprint('- Number of rows in the dataset is: {}'.format(XY.shape[0]))\nprint('- Number of columns in the dataset is: {}'.format(XY.shape[1]))\nprint('- Names of the independent variables are: {}'.format(list(XY.columns)))","29360892":"XY.head(10)","925841bb":"XY.info()","1460d48d":"XY.describe()","305c870d":"XY.describe(include = ['object','bool'])","4e2accb3":"XY.isnull().sum()","a76ec9cc":"XY[XY.isnull().any(axis=1)]","eeae6c85":"# Drop of observations with null values\n\nXY = XY.dropna()","ef19a691":"XY['Administrative_Duration'].describe()","87fe2e90":"XY[XY['Administrative_Duration']==-1].head(10)","15c38e91":"# Drop of observations with negative time values\nXY = XY.drop(XY[XY['Administrative_Duration']==-1].index)","a0ab40c8":"plt.figure(figsize=(10,10))\nlabels =  ['No buy', 'buy']\nplt.pie(XY['Revenue'].value_counts(),autopct='%1.1f',textprops={'fontsize': 20},explode =(0.1,0),\n       colors=['#1f80b8','#e5f5e0'],labels=labels,shadow=True)\nplt.title('Distribution of Variable Revenue in the dataset', fontsize = 18)\nplt.ylabel('')\nplt.show()","5f50b791":"matriz_correlacion = XY.corr(method='pearson')\nplt.figure(figsize=(16, 14))\nsns.set(font_scale=1.3)\ncorr_heatmap = round(matriz_correlacion, 2)\nheatmap = sns.heatmap(corr_heatmap, vmin=-1, vmax=1, annot=True, cmap='GnBu')\nheatmap.set_title('Pearson\u00b4s Correlation Matrix', fontdict={'fontsize':20}, pad=16);\nplt.show()","261de6fa":"# Correlated variables in relation to the variable 'Revenue'\n\ncorrelaciones_target = matriz_correlacion.values[ -1, : -1]\nindices_inversos =  abs(correlaciones_target[ : ]).argsort()[ : : -1]\ndiccionario = {}\nfor nombre, correlacion in zip( XY.columns[indices_inversos], \n                               list(correlaciones_target[indices_inversos])):diccionario[nombre] = correlacion\npd.DataFrame.from_dict(diccionario, orient='index', columns=['Correlate with Revenue'])","4a45ed3c":"# Consolidation of Operating Systems with low label observation in #4\n\nXY['OperatingSystems'] = XY['OperatingSystems'].replace([4,5,6,7,8],4)","883df329":"plt.figure(figsize=(10,10))\nlabels =['2', '1', '3', '4']\nplt.pie(XY['OperatingSystems'].value_counts(),autopct='%1.1f',textprops={'fontsize': 20}, \n        labels=labels, shadow=True)\nplt.title('Variable OperatingSystem Distribution', fontsize = 18)\nplt.ylabel('')\nplt.show()","3b91cc04":"XY = XY.drop(['OperatingSystems'],axis=1)","bca9b215":"plt.figure(figsize=(10,5))\nsns.countplot(XY['Browser'])\nplt.title('Variable Browsers Distribution', fontsize=15)\nplt.show()","46cfc134":"XY = XY.drop(['Browser'],axis=1)","77ada1bb":"XY = XY.drop(['TrafficType'],axis=1)","1ecacf36":"mes = XY['Month'].value_counts()\nplt.figure(figsize=(10,5))\nsns.countplot(XY['Month'], order = mes.index)\nplt.title('Observations by Month', fontsize = 15)\nxval = -.41\nplt.ylim(0,4000)\n\nfor index, value in mes.items():\n    plt.text(x = xval, y = value + 50, s = str(value))\n    xval += 1.03","3cfbf0f5":"XY.head()","bc5a5207":"# Numerical variables for analysis\n\ncolumnas_numericas=['Administrative', 'Administrative_Duration', 'Informational',\n                     'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',\n                     'BounceRates', 'ExitRates', 'PageValues']","d1825ee7":"# Normalized of numerical independent variables for visualization\n\nX_normal = (XY[columnas_numericas]-XY[columnas_numericas].mean())\/XY[columnas_numericas].std()\n\n# Boxplots of each variables\nplt.figure(figsize=(15,7))\nax = sns.boxplot(data=X_normal)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.rc('ytick', labelsize=16)\nplt.rc('xtick', labelsize=14)\nplt.title('Variables independent', fontsize=20)\nplt.ylabel('normalized value', fontsize=18)\n_ = plt.xlabel('variable name', fontsize=18)","fe97829d":"# Variables to apply outlier treatment.\n\ncolumnas_numericas=['Administrative', 'Administrative_Duration', 'ProductRelated',\n                    'ProductRelated_Duration', 'BounceRates', 'ExitRates']","77585b8c":"for cols in columnas_numericas:\n    IQR = np.percentile(XY[cols],75) - np.percentile(XY[cols],25)\n    \n    limite_superior = np.percentile(XY[cols],75) + 1.5*IQR\n    limite_inferior = np.percentile(XY[cols],25) - 1.5*IQR\n    \n    XY[cols] = np.where(XY[cols] > limite_superior,limite_superior,XY[cols])\n    XY[cols] = np.where(XY[cols] < limite_inferior,limite_inferior,XY[cols])","5dbfb43f":" # Normalized of numerical independent variables for visualization\n\nX_normal = (XY[columnas_numericas]-XY[columnas_numericas].mean())\/XY[columnas_numericas].std()\n\n# Boxplots de variables\nplt.figure(figsize=(15,7))\nax = sns.boxplot(data=X_normal)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.rc('ytick', labelsize=16)\nplt.rc('xtick', labelsize=14)\nplt.title('Variables independent', fontsize=20)\nplt.ylabel('normalized value', fontsize=18)\n_ = plt.xlabel('variable name', fontsize=18)","a6f18657":"# Coding with Label Encoder\nle = LabelEncoder()\nXY.Month= le.fit_transform(XY.Month.values)","226923a9":"XY.Region= le.fit_transform(XY.Region.values)","96f276bd":"XY.loc[XY['VisitorType'] != 'Returning_Visitor', 'VisitorType'] = 0\nXY.loc[XY['VisitorType'] == 'Returning_Visitor', 'VisitorType'] = 1\nXY['VisitorType'] = XY['VisitorType'].astype(int)","7fcce1f6":"XY.loc[XY.Weekend == False, 'Weekend'] = 0\nXY.loc[XY.Weekend == True, 'Weekend'] = 1\nXY.Weekend = XY.Weekend.astype(int)","931dcb5c":"XY.loc[XY.Revenue == True, 'Revenue'] = 1\nXY.loc[XY.Revenue == False, 'Revenue'] = 0\nXY.Revenue = XY.Revenue.astype(int)","ac4471ca":"X = XY.drop('Revenue', axis=1)\nY = XY['Revenue']","0bc2b502":"plt.figure(figsize=(14,16))\nn = 0\nfor i, column in enumerate(X.columns[:9]):\n    n+=1\n    plt.subplot(3, 3, n)\n    sns.distplot(X[column], bins=30)\n    \nplt.show()","23394b09":"hist_pos_neg_feat(X,Y)","8a6423d4":"obj_escalar = StandardScaler()\nX_estandard = obj_escalar.fit_transform(X)","21568246":"X_train, X_test, y_train, y_test = train_test_split(X_estandard, Y, \n                                                    test_size=0.2, random_state=0, stratify=Y)","79baf0f2":"# class_weight = 'balanced' is used in order to compesate the unbalaced dataset\n\nmodelo = LogisticRegression(class_weight = 'balanced')\n\nparametros = {\"C\": [1, 5,10,30, 40, 50,60,70,80,90,100,120,130,140,150], \n             \"solver\":['newton-cg', 'lbfgs']}","6a585751":"modelo_gs1 = GridSearchCV(modelo, param_grid=parametros,\n                         cv = 5, scoring='roc_auc')\nmodelo_gs1.fit(X_train, y_train)","24010cc9":"# Best parameters obtained\n\nprint(modelo_gs1.best_params_, \"\\nROC AUC: {}\".format(round(modelo_gs1.best_score_,2)))","0eb3519e":"reg_log =  LogisticRegression(C=modelo_gs1.best_params_['C'],solver=modelo_gs1.best_params_['solver'])","a4012fc1":"reg_log.fit(X_train, y_train)","c1b5ae1a":"y_test_pred_prob = reg_log.predict_proba(X_test)\ny_test_pred_prob_pos = y_test_pred_prob[np.where(y_test == 1)[0]]\ny_test_pred_prob_neg = y_test_pred_prob[np.where(y_test == 0)[0]]","65799935":"# ROC Curve\n\npreds = y_test_pred_prob[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize=(10,7))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","3e6b8eb0":"plt.figure(figsize=(8,6))\nshow_double_hist(y_test_pred_prob_pos[:, 1], y_test_pred_prob_neg[:, 1], n_bins=30, density=0)","39d461be":"umbral = 0.17\ny_umbralizadas = 1*(y_test_pred_prob[:, 1] > umbral)","ca6d9e02":"# Confusion Matrix\n\ncm =  metrics.confusion_matrix(y_test, y_umbralizadas)\nAccuracy =round(metrics.accuracy_score(y_test, y_umbralizadas),2)\nSensitividad = round(metrics.recall_score(y_test, y_umbralizadas),2)\nPrecision = round(metrics.precision_score(y_test, y_umbralizadas),2)\nf1 = round(metrics.f1_score(y_test, y_umbralizadas),2)\nroc = round(metrics.roc_auc_score(y_test, y_umbralizadas),2)\n\nprint(\"Confusion Matrix\\n\", cm) ","116b4b65":"ml_comp = {'Model' : [], 'Class 1' : [],'type I Error' : [], 'type II Error' : [], 'Threshold' : [],\n           'Accuracy' : [],'Recall' : [], 'Presition' : [], 'F1_score' : [], 'ROC_AUC_Score': []}\nml_comp = pd.DataFrame(ml_comp)\n\n\nml_comp.loc[0,:] = ['Logistic. R', cm[1,1], cm[0,1], cm[1,0], umbral, Accuracy,\n                    Sensitividad, Precision, f1, roc]\nml_comp","373af443":"modelo = MLPClassifier()\nparametros = {'solver': ['lbfgs'], \n              'max_iter': [200, 300, 500, 800], \n              'alpha': [0.001, 0.0001, 0.00001], \n              'hidden_layer_sizes':np.arange(10, 15), \n              'random_state':[0]}","0d20f784":"modelo_gs2 = GridSearchCV(modelo, param_grid=parametros, cv = 5, \n                         scoring='roc_auc', n_jobs=-1, verbose=10)\nmodelo_gs2.fit(X_train, y_train)","53cf2944":"print(modelo_gs2.best_params_, \"\\nROC AUC: {}\".format(round(modelo_gs2.best_score_,2)))","7d9ad833":"mejor_modelo3 = MLPClassifier(**modelo_gs2.best_params_, verbose=10)","15d7baed":"mejor_modelo3.fit(X_train, y_train)","9af6587c":"y_test_pred_prob = mejor_modelo3.predict_proba(X_test) \ny_test_pred_prob_pos = y_test_pred_prob[np.where(y_test == 1)[0]]\ny_test_pred_prob_neg = y_test_pred_prob[np.where(y_test == 0)[0]]","527b6911":"# ROC Curve\n\npreds = y_test_pred_prob[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize=(10,7))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","5cde5ef9":"plt.figure(figsize=(8,6))\nshow_double_hist(y_test_pred_prob_pos[:, 1], y_test_pred_prob_neg[:, 1], n_bins=30, density=0)","0d4e37a9":"umbral = 0.19\ny_umbralizadas = 1*(y_test_pred_prob[:, 1] > umbral)","48c3407e":"# Confusion Matrix\n\ncm =  metrics.confusion_matrix(y_test, y_umbralizadas)\nAccuracy =round(metrics.accuracy_score(y_test, y_umbralizadas),2)\nSensitividad = round(metrics.recall_score(y_test, y_umbralizadas),2)\nPrecision = round(metrics.precision_score(y_test, y_umbralizadas),2)\nf1 = round(metrics.f1_score(y_test, y_umbralizadas),2)\nroc = round(metrics.roc_auc_score(y_test, y_umbralizadas),2)\n\nprint(\"Confusion Matrix\\n\", cm) ","ddba268c":"ml_comp.loc[1,:] = ['Neural Net', cm[1,1], cm[0,1], cm[1,0], umbral, Accuracy,\n                    Sensitividad, Precision, f1, roc]\nml_comp","1efa5ac8":"modelo = XGBClassifier()","e9dcebf9":"!nvidia-smi","f39501be":"parametros ={'max_depth': range(1, 6),\n             'n_estimators': range(35,  40),\n             'learning_rate': [0.1, 0.01],\n             'tree_method': ['gpu_hist'],\n             }","eb400351":"modelo_gs3 = GridSearchCV(estimator=modelo, param_grid=parametros,\n                           scoring = 'roc_auc', n_jobs = -1,cv = 5,\n                           verbose=True)","ef599133":"modelo_gs3.fit(X_train,y_train)","3e08a8e3":"print(modelo_gs3.best_params_, \"\\nROC AUC: {}\".format(round(modelo_gs3.best_score_,2)))","35455099":"mejor_modelo3 = XGBClassifier(**modelo_gs3.best_params_)","70463fe7":"mejor_modelo3.fit(X_train, y_train)","097ad76d":"y_test_pred_prob = mejor_modelo3.predict_proba(X_test) \ny_test_pred_prob_pos = y_test_pred_prob[np.where(y_test == 1)[0]]\ny_test_pred_prob_neg = y_test_pred_prob[np.where(y_test == 0)[0]]","39a5a7b8":"# ROC Curve\n\npreds = y_test_pred_prob[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize=(10,7))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","1c196463":"plt.figure(figsize=(8,6))\nshow_double_hist(y_test_pred_prob_pos[:, 1], y_test_pred_prob_neg[:, 1], n_bins=30, density=0)","89124d63":"umbral = 0.19\ny_umbralizadas = 1*(y_test_pred_prob[:, 1] > umbral)","65581b2f":"# Confution Matrix\n\ncm =  metrics.confusion_matrix(y_test, y_umbralizadas)\nAccuracy =round(metrics.accuracy_score(y_test, y_umbralizadas),2)\nSensitividad = round(metrics.recall_score(y_test, y_umbralizadas),2)\nPrecision = round(metrics.precision_score(y_test, y_umbralizadas),2)\nf1 = round(metrics.f1_score(y_test, y_umbralizadas),2)\nroc = round(metrics.roc_auc_score(y_test, y_umbralizadas),2)\n\nprint(\"Confution Matrix\\n\", cm) ","d66b94d9":"ml_comp.loc[2,:] = ['XGBoost', cm[1,1], cm[0,1], cm[1,0], umbral, Accuracy,\n                    Sensitividad, Precision, f1, roc]\nml_comp","d9016062":"#### Variable **Month:**","c01e8bc8":"The significant bias to the left of the continuous independent variables is observed as a result of the measurement of low browsing time.","6d2eaacc":"# Functions definitions: ","2966df80":"### Better parameters:","01c82c5d":"The Operating Systems variable is labeled by number. The low-use Operating Systems have been consolidated in the label '4', whose totality just represents 4.8% of the dataset. Most users use the '2' operating system.\n\nOperating systems can identify users of a specific type of computer (Windows users, Mac users, Linux users). This variable is captured by Google Analytics for technical analysis, it is not related to the possibility of buying or not buying, so leaving it could unfairly skew the data to classify by Operating System.\n\nLikewise, it is observed in the Correlation matrix that there is only a 2% correlation with the Revenue variable, therefore this column is discarded from our classifier model.\n","63383eec":"# Visualizations and correlations","5e677f2e":"### Boxplots: ","c17461b5":"### Description of the problem:\n\nDue to the change in customer consumption habits, a retail company is widely promoting the online sales service. The company wants to run a machine learning model to rank customers based on the likelihood of generating income when shopping on the web.\n\nThe goal is to perform a series of specific actions for the customers who are most likely to make purchases on the web.\n\nFor this, the company has been collecting data with its Google Analytics tool. It has session data, each of them from a different client in a period of one year.\n\nThe data for this exercise comes from the University of California Irvine. More details in the following url: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+Shoppers+Purchasing+Intention+Dataset","39125d1f":"### Splitting in features X and target Y ","b1cf3801":"## Outliers Analysis.","51deee2f":"## Conclusions:\n### \n\nWhen you have an unbalanced dataset, it pretty common you get a high precision value in the Majority class and a low Sensitivity in the Minority class.\n\nIn these cases, the Accuracy is less relevant, on the other hand, it is more important pay attention to the Sensitivity and even more to the F1 Score, since it takes into account the incorrect predictions of class 1 (minority).\n\nThis is the typical case with this dataset, where the low Sensitivity (Recall) values in the minority class (Class 1), forces us to reduce the classification threshold in the detriment of precision (increase in Type I Error), to reduce the False positives.\n\nAnd it is the case that we are looking for, since what the Retail company is looking to detect the maximum number of customers most likely to make purchases on the web, in order to take specific actions. For this, he cares little about false positives, but ignoring the probability of purchase when there really is one, that would be more dangerous.\n\nThe best performing model was the ** XGBoost **.\n\nIt was the one with best detected Class 1 for the selected threshold.\n\nThe performance of the featured Models was good, but not exceptional, so there is plenty of room for improvement. An option to improve the models would be to deal with the imbalance of the dataset beyond stratifying it in the train-test datasplit, such as using oversampling and subsampling techniques, (SMOTE) thus reducing the imbalance and the models can predict with greater accuracy.","9e4775e8":"# Importing Required Libraries:","ceb033ac":"The train_test_split library from the Sci-kit library is used, with the **stratified** mode activated in order to correct the unbalance problem of the Revenue variable. \"80-20 train_test split\" is used","a32eaaa3":"### Threshold:","318229c4":"Another way to check the imbalance of the dataset between users who buy (Class 1) and those who do not buy (Class 0).\n\nThere are no variables that discriminate or separate the two classes.","8bebc8a0":"# Exploratory data analysis:","f543ac9e":"As in the last models, we obtain the best parameters found and fit a model with those parameters:","d59b1264":"We can see the number of observations in which the customer ended up not buying is much greater than the number of observations in which the customer ended up completing a transaction. This makes sense, as most purchases generally end without a successful purchase.\n####\n##### Bias problem:\nBecause the data for this variable are so skewed in the direction of the category \"No purchase was made\" (False), there is a great imbalance in the distribution of these data, therefore, it should be stratified when selecting the data used for training \/ testing so that the proportion or relationship of training \/ test labels is even.\n","8ddf5dad":"#### Variable  **OperatingSystem:**","b3f93a95":"This variable will not be taken into account because traffic sources are not useful to determine if a user will make a purchase. It generally helps website owners measure traffic sources and helps determine where they should spend on advertising, for example. It is a technical variable that does not influence the purchase decision.","94f8b681":"### Optimizing 'C' and 'Solver' with GridSearch ","ac8cf13c":"The model is trained with the training data.","48774028":"Since the columns *Administrative_Duration, Informational_Duration, ProductRelated_Duration* indicate the time spent in certain type of page, a value of -1 is a value that does not make any sense and since they represent around 0.12% of the dataset, then we can eliminate them without repercussions to the dataset .","03326480":"## Data standardization: ","4f38472a":"All the variables are concentrated in the lower part of the graph, mainly due to the fact that they indicate times that are spent within the pages of the shopping website, therefore when an observation or potential customer takes more time on any page, it is shown as a possible **\"outlier\"**, although it may not necessarily be considered an outlier.\n\nThis is a good indicator of the need to standardize the data before training our ML model.\n\nWe observed: **Informational, informational_duration** and **PageValues**, do not have any distribution at all and if we eliminated their outliers, there will be no values left in them. We will apply treatment of \"outliers\", except in these 3 characteristics.\n","37aa7352":"With this optimized model, *test* values are predicted to see how it behaves with data that the model have not seen before.","1f4ba191":"#### Target Variable **Revenue:**","22ecc162":"The *Browser* variable behaves similarly to the *OperatingSystem* variable.\n\nA large majority of users use browser 2, and a smaller number of users use browser 1. All other browsers represent a small subsection of users online. The use of a certain browser does not condition a purchase, which is confirmed with a correlation of just 1% with respect to the * Revenue * variable.\n\nThis variable will not be used as it does not provide relevant information to our model.","d3353a4f":"___\n<h1><center> Online Shopper's Intention<\/center><\/h1>\n<h1><center> A Machine Learning Exercise<\/center><\/h1>\n\n___\n\n\n## Customer segmentation of a retail company\n","a6aceff1":"### Predicciones:","cd8c63b2":"### Histogram: ","90fc696f":"## Logistic regression model (classification).\n","66be97df":"### Correlation Matrix:","9268b821":"This figure represents in green the probabilities assigned by the model to data values are 0s (the closer the green distribution is to 0, the better) and in red the probabilities assigned to data values are 1s (the closer the distribution is to 1 red the better).","21732c2b":"### train and test Split: ","70abb399":"### Relations with the variable Revenue.","ac355364":"The probabilities returned by the model are continuous values between 0 and 1. To change it to 0s and 1s, it is necessary to use a cutoff threshold. Anything greater than the threshold will be prediction = 1, and anything less than the threshold will be prediction = 0.","7c94a283":"### Predictions:","652f5b4d":"### Dataset :\n\nThe dataset consists of feature vectors belonging to 12,330 sessions. The dataset was formed so that each session would belong to a different user in a 1-year period to avoid any tendency to a specific campaign, special day, user profile, or period.\n\n#### Attribute :\n\n- **Revenue** => class whether it can make a revenue or not.\n- **Administrative, Administrative Duration, Informational, Informational Duration, Product Related and Product Related Duration** => represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories.\n- **Bounce Rate** => percentage of visitors who enter the site from that page and then leave (\"bounce\") without triggering any other requests to the analytics server during that session.\n- **Exit Rate** => the percentage that were the last in the session.\n- **Page Value** => feature represents the average value for a web page that a user visited before completing an e-commerce transaction.\n- **Special Day** => indicates the closeness of the site visiting time to a specific special day (e.g. Mother\u2019s Day, Valentine's Day) in which the sessions are more likely to be finalized with transaction. For example, for Valentina\u2019s day, this value takes a nonzero value between February 2 and February 12, zero,before and after this date unless it is close to another special day, and its maximum value of 1 on February 8.\n- **Operating system,browser, region, traffic type** => Different types of operating systems, browser, region and traffic type used to visit the website.\n- **Visitor type** => Whether the customer is a returning or new visitor.\n- **Weekend** => A Boolean value indicating whether the date of the visit is weekend.\n- **Month** => Month of the year.","786c532f":"### Predictions Histogram:","70693649":"January and April are missing in 'Month' column. Visually, several months have many samples (May, November) and a couple have very few samples (February, June).","05cbc8d1":"# Reading the data set:","2bb632af":"## Classification Model with Neural Network","7ebbfe63":"### Threshold:","a877ec76":"#### Variable **TrafficType:**","06774466":"#### Analyzing the model with the best parameter ","7fb17af3":"In this step we are left with the best parameters obtained in the previous step:","b690824f":"#### Variable **Browser:**","70ec43ac":"## Threshold :","ac2b7462":"## Classification Model with XGBoost:","0a655204":"## Change Categorical variables to numeric:","54b9af90":"The dataset contains 14 null values, they represent a little bit more than 0.1% of the total observations, they can be eliminated without repercussions to the dataset."}}