{"cell_type":{"9420beae":"code","ab18c62b":"code","6e0a3d44":"code","a98a0aa5":"code","0c6c0455":"code","a51808b6":"code","f41b1873":"code","d1beb6a6":"code","0c8a36f5":"code","d27838b4":"code","8d152784":"code","42a1137d":"code","af1626b9":"code","7b7d7d4e":"code","3514e441":"code","59e1dfa6":"code","99ac0416":"code","7cd61df4":"code","8c29fe49":"code","57ee15dc":"code","990623e4":"code","559fef9a":"code","1752c8dd":"code","ca2b513b":"code","6ff79418":"code","93a79dc0":"markdown","9bf587ef":"markdown","9725cc5a":"markdown","3069a072":"markdown","bad82fd3":"markdown","45845b5a":"markdown","d8263b9d":"markdown","71decab5":"markdown","8799da3d":"markdown","8ac71ee9":"markdown","9cc46ec4":"markdown","f1ccf7e0":"markdown","46f1d26c":"markdown","cb1ec98e":"markdown","24b8c3d1":"markdown","f6c51ff1":"markdown","5e064143":"markdown","2e72e28e":"markdown","7b818c4b":"markdown","cedab922":"markdown","dab54033":"markdown","4061ff72":"markdown","98de7cd4":"markdown","2f53105f":"markdown","3ee7117c":"markdown","2a8e212a":"markdown","857ed52c":"markdown","7284ce46":"markdown","cc995b3a":"markdown"},"source":{"9420beae":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport warnings\nwarnings.filterwarnings('ignore')","ab18c62b":"train_data=pd.read_csv('..\/input\/titanic\/train.csv')\ntrain_data.head()","6e0a3d44":"#No. of samples and features\/atrributes\ntrain_data.shape","a98a0aa5":"test_data=pd.read_csv('..\/input\/titanic\/test.csv')\ntest_data.head()","0c6c0455":"test_data.shape","a51808b6":"plt.figure(figsize=(8,6))\nd=sb.countplot(x=train_data['Survived'])","f41b1873":"plt.figure(figsize=(8,6))\nd=sb.barplot(x=train_data['Sex'],y=train_data['Survived']).set_title('Survival rate by Sex')","d1beb6a6":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","0c8a36f5":"plt.figure(figsize=(8,6))\nd=sb.barplot(x=train_data['Pclass'],y=train_data['Survived']).set_title('Survival rate by Pclass')","d27838b4":"d1=sb.displot(train_data['Age'],kde=True)\nd2=sb.displot(train_data['Fare'],kde=True)","8d152784":"fig,axes=plt.subplots(1,2,figsize=(10,6))\nd1=sb.barplot(x=train_data['SibSp'],y=train_data['Survived'],ax=axes[0]).set_title('Survival rate by Siblings & Spouse')\nd2=sb.barplot(x=train_data['Parch'],y=train_data['Survived'],ax=axes[1]).set_title('Survival rate by Parents & Children')","42a1137d":"train_data.isnull().sum()","af1626b9":"test_data.isnull().sum()","7b7d7d4e":"train_data['Embarked']=train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])","3514e441":"train_data['Age']=train_data['Age'].fillna(train_data['Age'].mean())\ntest_data['Age']=test_data['Age'].fillna(test_data['Age'].mean())","59e1dfa6":"test_data['Fare']=test_data['Fare'].fillna(test_data['Fare'].median())","99ac0416":"train_data['FamilySize']=train_data['SibSp']+train_data['Parch']+1\ntrain_data['FamilySize']=pd.cut(train_data['FamilySize'], [0,1,4,7], labels=['Singleton', 'Small', 'Big'])\nd=sb.barplot(x=train_data['FamilySize'], y=train_data['Survived']).set_ylabel('Survival rate')","7cd61df4":"test_data['FamilySize']=test_data['SibSp']+test_data['Parch']+1\ntest_data['FamilySize']=pd.cut(test_data['FamilySize'], [0,1,4,7], labels=['Singleton', 'Small', 'Big'])","8c29fe49":"train_data['Cabin']=train_data['Cabin'].fillna(0)\ntrain_data['Has_Cabin'] = train_data[\"Cabin\"].apply(lambda x: 1 if x != 0 else 0)\ntest_data['Cabin']=test_data['Cabin'].fillna(0)\ntest_data['Has_Cabin'] = test_data[\"Cabin\"].apply(lambda x: 1 if x != 0 else 0)","57ee15dc":"from sklearn.preprocessing import StandardScaler\n\ncols=['Age','Fare']\nscaler=StandardScaler()\ntrain_data[cols]=scaler.fit_transform(train_data[cols])\ntrain_data[cols]","990623e4":"test_data[cols]=scaler.fit_transform(test_data[cols])\ntest_data[cols]","559fef9a":"d1=sb.displot(train_data['Age'],kde=True)\nd2=sb.displot(train_data['Fare'],kde=True)","1752c8dd":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score,GridSearchCV,KFold\n\ny=train_data['Survived']#Label\nfeatures=['Pclass','Sex','Age','Fare','FamilySize','Embarked','Has_Cabin']\nX=pd.get_dummies(train_data[features])\nX_test=pd.get_dummies(test_data[features])\n\nmodel=[]\nmodel.append(('LR',LogisticRegression(solver='liblinear')))\nmodel.append(('RF',RandomForestClassifier()))\nmodel.append(('SVM',SVC()))\nfor i,j in model:\n    cv_res=cross_val_score(j,X,y,cv=10,scoring='accuracy')\n    print(\"%s : %f\" %(i,cv_res.mean()*100))\n    ","ca2b513b":"clf=SVC()\nkf=KFold(n_splits=5)\nparam_grid={\n     'C': [0.1,1,10,50,100],\n    'gamma': [0.0001,0.001,0.1,1,10],\n    'kernel':['linear','rbf']\n}\ngrid=GridSearchCV(estimator=clf,param_grid=param_grid,cv=kf)\ngres=grid.fit(X,y)\nprint(\"Best\",gres.best_score_)\nprint(\"params\",gres.best_params_)","6ff79418":"model=SVC(C=1,gamma=0.1,kernel='rbf',random_state=42)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","93a79dc0":"#### Look's like the SVM performs better, so lets choose this as our classifier. Before that let's try tuning some of its hyperparameters, these are parameters of an algorithm that need to be adjusted to find the perfect balance between bias and variance of a model which can even slightly increase a model's performance. SVM's main hyperparameters are gamma and C ,more info can be found [here](https:\/\/towardsdatascience.com\/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167).\n#### As for the method to tune the parameters we shall go for the [GridSearch](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html?highlight=gridsearch#sklearn.model_selection.GridSearchCV) algorithm provided by sklearn that basically performs cross-validation on a parameter grid and compares results to give us the best parameters.This is actually a very time taking process , but since our training set is small and we won't select a large parameter space GridSearch should suffice.","9bf587ef":"#### Similarly lets look at our test data.","9725cc5a":"# **Feature Engineering and Data preprocessing**\n#### Lets first check for missing values in the train and test sets and deal with them before moving to feature engineering.","3069a072":"#### Looking at the plots above,passengers who are alone or have more siblings are less likely to survive.The Parch survival rate is somewhat confusing ( the scenario when parch is 5). We shall look more into this when we perform feature engineering.","bad82fd3":"# **Modeling and Prediction**\n#### Let's compare some classifiers to select a classifier and further tune it's parameters to get a better classifier. We use [cross-validation](http:\/\/https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html) to compare accuracies of three models: Random Forest, Logistic Regression and Support Vector Machine. We have not considered features like PassengerID,Ticket,Name for training as they dont provide useful information.","45845b5a":"### **Cabin**\n#### The Cabin feature has a lot of missing values in both the sets, so we can assume those values denote that passenger don't have a private cabin(?), anyways lets create a new feature 'Has_Cabin' and keep it since we are unsure on how this is going to be useful to us.","d8263b9d":"#### The age and fare features could be standardized to help increase performance,we shall do that soon.","71decab5":"### **Age and Fare**\n#### Let's use sklearn's StandardScaler to standardize the age and fare features.","8799da3d":"#### We got 77.5% - 78.4% = ~ 1% increase in accuracy after doing some basic data preprocessing and EDA!!","8ac71ee9":"#### We can see that most of the passengers didnt survive, lets take a more closer look on survival rates among male and female passengers.","9cc46ec4":"### **Pclass**","f1ccf7e0":"### **Age**\n#### We could fill the missing ages with the mean of each set.","46f1d26c":"#### We see that first class and second class passengers had a higher survival rate,this feature could be useful to us.","cb1ec98e":"# **Exploratory Data Analysis (Contribution)**","24b8c3d1":"### **Fare**\n#### We can fill the single missing fare value in test set with the median, since its effective against outliers.","f6c51ff1":"### **SibSP and Parch**\n#### We combine sibsp and parch to create a new feature 'FamilySize' to better see the survival rate and it's also clean data.","5e064143":"#### We got a accuracy of 77.5%, now lets try exploring the data some more and perform some feature engineering to tidy up the data and increase performance.","2e72e28e":"# **Load Data**\n#### Let's first import some common modules and have a look at our data.","7b818c4b":"#### From the above plot we can safely assume that females have a higher chance of surviving than males. Lets build a basic model on some features and see how it performs.The features I am considering are Pclass,Sex,Sibsp,Parch and I am training a random forest model.","cedab922":"#### We can see that in both the training set and test set the Cabin column has some 'NaN' values we might find more such values in other columns ,we shall look into that later.","dab54033":"#### Now we are done with some basic feature engineering, lets move on to the next step and start building our classifier!!.","4061ff72":"### **SibSp and Parch**","98de7cd4":"### **Embarked**\n#### The missing port values in the train set can be filled with the most common port","2f53105f":"### **Age and Fare**","3ee7117c":"# **Baseline model**\n#### We begin by looking at some simple patterns and see how well it can predict survival.First lets take a look at our label.","2a8e212a":"#### Look's like there is only a 0.11% increase,still lets use it and make predictions","857ed52c":"#### SibSp and Parch indicates the number of siblings\/spouses and parents\/children of a passenger that are aboard the titanic.Embarked denotes the port of embarkation C = Cherbourg, Q = Queenstown, S = Southampton.Survival is our target\/label ,where 0 means death and 1 means survived.","7284ce46":"![image.png](attachment:a748068a-b290-4067-a225-3a94a22f65dc.png)","cc995b3a":"![image.png](attachment:b350c895-32d4-45e0-a01d-bdb56c78aeac.png)"}}