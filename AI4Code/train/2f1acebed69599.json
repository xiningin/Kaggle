{"cell_type":{"34a77c7a":"code","cd813503":"code","836cee11":"code","d2abb9c8":"code","5b6669b5":"code","6314725d":"code","c02cbffd":"code","290a0627":"code","49044f63":"code","958138e7":"code","544b8714":"code","3445bce0":"code","911b6213":"code","986f7b33":"code","428c0998":"code","e8b0a0e5":"code","2886a98e":"code","900fd68f":"code","b6a40e12":"code","56a97aed":"code","05f5872b":"code","25d9fe62":"code","2db1ba21":"code","787673d6":"code","54eee11b":"code","1934d3f1":"code","dae58638":"code","de98aacc":"code","e65e7939":"code","d530de28":"code","ff54cb26":"code","0fa3c6c8":"code","0d41b011":"markdown","97185ada":"markdown","df9459ed":"markdown","8548cf8d":"markdown","f1477c5f":"markdown","ac7be206":"markdown","09b69d1e":"markdown","8c3b0fb3":"markdown","9fda4e66":"markdown","eed0ad49":"markdown","b5e64db1":"markdown","1e81f430":"markdown","6e2ffbef":"markdown"},"source":{"34a77c7a":"# if you keras is not using tensorflow as backend set \"KERAS_BACKEND=tensorflow\" use this command\nfrom keras.utils import np_utils \nfrom keras.datasets import mnist \nimport seaborn as sns\nfrom keras.initializers import RandomNormal\nfrom keras.layers.normalization import BatchNormalization\nprint(\"DONE\")","cd813503":"%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\n# https:\/\/gist.github.com\/greydanus\/f6eee59eaf1d90fcb3b534a25362cea4\n# https:\/\/stackoverflow.com\/a\/14434334\n# this function is used to update the plots for each epoch and error\ndef plt_dynamic(fig,x, vy, ty, ax, colors=['b']):\n    \n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    plt.legend()\n    plt.grid()\n    fig.canvas.draw()","836cee11":"def printLossPlot(modelName):\n    score = modelName.evaluate(X_test, Y_test, verbose=0) \n    print('Test score:', score[0]) \n    print('Test accuracy:', score[1])\n#     fig = plt.figure()\n    fig,ax = plt.subplots(1,1)\n    ax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n    # list of epoch numbers\n    x = list(range(1,nb_epoch+1))\n    vy = history.history['val_loss']\n    ty = history.history['loss']\n    plt_dynamic(fig,x, vy, ty, ax)","d2abb9c8":"# the data, shuffled and split between train and test sets \n(X_train, y_train), (X_test, y_test) = mnist.load_data()","5b6669b5":"print(\"Number of training examples :\", X_train.shape[0], \"and each image is of shape (%d, %d)\"%(X_train.shape[1], X_train.shape[2]))\nprint(\"Number of training examples :\", X_test.shape[0], \"and each image is of shape (%d, %d)\"%(X_test.shape[1], X_test.shape[2]))","6314725d":"X_train.shape[1]*X_train.shape[2]","c02cbffd":"# if you observe the input shape its 2 dimensional vector\n# for each image we have a (28*28) vector\n# we will convert the (28*28) vector into single dimensional vector of 1 * 784 \n\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]) \nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]) ","290a0627":"# after converting the input images from 3d to 2d vectors\n\nprint(\"Number of training examples :\", X_train.shape[0], \"and each image is of shape (%d)\"%(X_train.shape[1]))\nprint(\"Number of training examples :\", X_test.shape[0], \"and each image is of shape (%d)\"%(X_test.shape[1]))","49044f63":"# An example data point\nprint(X_train[0])","958138e7":"# if we observe the above matrix each cell is having a value between 0-255\n# before we move to apply machine learning algorithms lets try to normalize the data\n# X => (X - Xmin)\/(Xmax-Xmin) = X\/255\n\nX_train = X_train\/255\nX_test = X_test\/255","544b8714":"# example data point after normlizing\nprint(X_train[0])","3445bce0":"# here we are having a class number for each image\nprint(\"Class label of first image :\", y_train[0])\n\n# lets convert this into a 10 dimensional vector\n# ex: consider an image is 5 convert it into 5 => [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n# this conversion needed for MLPs \n\nY_train = np_utils.to_categorical(y_train, 10) \nY_test = np_utils.to_categorical(y_test, 10)\n\nprint(\"After converting the output into a vector : \",Y_train[0])","911b6213":"from keras.models import Sequential \nfrom keras.layers import Dense, Activation ","986f7b33":"# some model parameters\noutput_dim = 10\ninput_dim = X_train.shape[1]\nbatch_size = 128 \nnb_epoch = 20","428c0998":"# https:\/\/stackoverflow.com\/questions\/34716454\/where-do-i-call-the-batchnormalization-function-in-keras\n\nfrom keras.layers import Dropout\nfrom keras.layers.normalization import BatchNormalization\n\nmodel_2_d_bn_relu_rn_adam = Sequential()\n\nmodel_2_d_bn_relu_rn_adam.add(Dense(784, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_2_d_bn_relu_rn_adam.add(BatchNormalization())\nmodel_2_d_bn_relu_rn_adam.add(Dropout(0.5))\n\nmodel_2_d_bn_relu_rn_adam.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_2_d_bn_relu_rn_adam.add(BatchNormalization())\nmodel_2_d_bn_relu_rn_adam.add(Dropout(0.5))\n\nmodel_2_d_bn_relu_rn_adam.add(Dense(output_dim, activation='softmax'))\n\nmodel_2_d_bn_relu_rn_adam.summary()\n\nmodel_2_d_bn_relu_rn_adam.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_2_d_bn_relu_rn_adam.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","e8b0a0e5":"printLossPlot(model_2_d_bn_relu_rn_adam)","2886a98e":"# https:\/\/stackoverflow.com\/questions\/34716454\/where-do-i-call-the-batchnormalization-function-in-keras\nfrom keras.initializers import he_uniform\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Dropout\n\nmodel_2_d_bn_relu_heUni_adam = Sequential()\n\nmodel_2_d_bn_relu_heUni_adam.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=he_uniform(seed=None)))\nmodel_2_d_bn_relu_heUni_adam.add(BatchNormalization())\nmodel_2_d_bn_relu_heUni_adam.add(Dropout(0.5))\n\nmodel_2_d_bn_relu_heUni_adam.add(Dense(256, activation='relu', kernel_initializer=he_uniform(seed=None)) )\nmodel_2_d_bn_relu_heUni_adam.add(BatchNormalization())\nmodel_2_d_bn_relu_heUni_adam.add(Dropout(0.5))\n\nmodel_2_d_bn_relu_heUni_adam.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_2_d_bn_relu_heUni_adam.summary()\n\nmodel_2_d_bn_relu_heUni_adam.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_2_d_bn_relu_heUni_adam.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","900fd68f":"printLossPlot(model_2_d_bn_relu_heUni_adam)","b6a40e12":"# https:\/\/stackoverflow.com\/questions\/34716454\/where-do-i-call-the-batchnormalization-function-in-keras\nfrom keras.initializers import he_normal\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Dropout\n\nmodel_2_d_bn_relu_heNor_adam = Sequential()\n\nmodel_2_d_bn_relu_heNor_adam.add(Dense(600, activation='relu', input_shape=(input_dim,), kernel_initializer=he_normal(seed=None)))\nmodel_2_d_bn_relu_heNor_adam.add(BatchNormalization())\nmodel_2_d_bn_relu_heNor_adam.add(Dropout(0.5))\n\nmodel_2_d_bn_relu_heNor_adam.add(Dense(256, activation='relu', kernel_initializer=he_normal(seed=None)) )\nmodel_2_d_bn_relu_heNor_adam.add(BatchNormalization())\nmodel_2_d_bn_relu_heNor_adam.add(Dropout(0.5))\n\nmodel_2_d_bn_relu_heNor_adam.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_2_d_bn_relu_heNor_adam.summary()\n\nmodel_2_d_bn_relu_heNor_adam.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_2_d_bn_relu_heNor_adam.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","56a97aed":"printLossPlot(model_2_d_bn_relu_heNor_adam)","05f5872b":"# https:\/\/stackoverflow.com\/questions\/34716454\/where-do-i-call-the-batchnormalization-function-in-keras\n\nfrom keras.layers import Dropout\n\nmodel_3_d_bn_relu_rn_adam = Sequential()\n\nmodel_3_d_bn_relu_rn_adam.add(Dense(784, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_3_d_bn_relu_rn_adam.add(BatchNormalization())\nmodel_3_d_bn_relu_rn_adam.add(Dropout(0.5))\n\nmodel_3_d_bn_relu_rn_adam.add(Dense(512, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_3_d_bn_relu_rn_adam.add(BatchNormalization())\nmodel_3_d_bn_relu_rn_adam.add(Dropout(0.5))\n\nmodel_3_d_bn_relu_rn_adam.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_3_d_bn_relu_rn_adam.add(BatchNormalization())\nmodel_3_d_bn_relu_rn_adam.add(Dropout(0.5))\n\nmodel_3_d_bn_relu_rn_adam.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_3_d_bn_relu_rn_adam.summary()\n\nmodel_3_d_bn_relu_rn_adam.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_3_d_bn_relu_rn_adam.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","25d9fe62":"printLossPlot(model_3_d_bn_relu_rn_adam)","2db1ba21":"# https:\/\/stackoverflow.com\/questions\/34716454\/where-do-i-call-the-batchnormalization-function-in-keras\n\nfrom keras.layers import Dropout\n\nmodel_3_d_bn_relu_heUni_adam = Sequential(name=\"MLP\")\n\nmodel_3_d_bn_relu_heUni_adam.add(Dense(500, activation='relu', input_shape=(input_dim,), kernel_initializer=he_uniform(seed=None)))\nmodel_3_d_bn_relu_heUni_adam.add(BatchNormalization())\nmodel_3_d_bn_relu_heUni_adam.add(Dropout(0.5))\n\nmodel_3_d_bn_relu_heUni_adam.add(Dense(256, activation='relu', kernel_initializer=he_uniform(seed=None)) )\nmodel_3_d_bn_relu_heUni_adam.add(BatchNormalization())\nmodel_3_d_bn_relu_heUni_adam.add(Dropout(0.5))\n\nmodel_3_d_bn_relu_heUni_adam.add(Dense(128, activation='relu', kernel_initializer=he_uniform(seed=None)) )\nmodel_3_d_bn_relu_heUni_adam.add(BatchNormalization())\nmodel_3_d_bn_relu_heUni_adam.add(Dropout(0.5))\n\nmodel_3_d_bn_relu_heUni_adam.add(Dense(output_dim, activation='softmax'))\n\nfor i, layer in enumerate(model_3_d_bn_relu_heUni_adam.layers):\n    layer.name = 'layer_' + str(i)\nmodel_3_d_bn_relu_heUni_adam.summary()\n\nmodel_3_d_bn_relu_heUni_adam.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_3_d_bn_relu_heUni_adam.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","787673d6":"printLossPlot(model_3_d_bn_relu_heUni_adam)","54eee11b":"# https:\/\/stackoverflow.com\/questions\/34716454\/where-do-i-call-the-batchnormalization-function-in-keras\n\nfrom keras.layers import Dropout\n\nmodel_3_d_bn_relu_heNor_adam = Sequential(name=\"MLP\")\n\nmodel_3_d_bn_relu_heNor_adam.add(Dense(128, activation='relu', input_shape=(input_dim,), kernel_initializer=he_normal(seed=None)))\nmodel_3_d_bn_relu_heNor_adam.add(BatchNormalization())\nmodel_3_d_bn_relu_heNor_adam.add(Dropout(0.5))\n\nmodel_3_d_bn_relu_heNor_adam.add(Dense(64, activation='relu', kernel_initializer=he_normal(seed=None)) )\nmodel_3_d_bn_relu_heNor_adam.add(BatchNormalization())\nmodel_3_d_bn_relu_heNor_adam.add(Dropout(0.5))\n\nmodel_3_d_bn_relu_heNor_adam.add(Dense(32, activation='relu', kernel_initializer=he_normal(seed=None)) )\nmodel_3_d_bn_relu_heNor_adam.add(BatchNormalization())\nmodel_3_d_bn_relu_heNor_adam.add(Dropout(0.5))\n\nmodel_3_d_bn_relu_heNor_adam.add(Dense(output_dim, activation='softmax'))\n\nfor i, layer in enumerate(model_3_d_bn_relu_heNor_adam.layers):\n    layer.name = 'layer_' + str(i)\nmodel_3_d_bn_relu_heNor_adam.summary()\n\nmodel_3_d_bn_relu_heNor_adam.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_3_d_bn_relu_heNor_adam.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","1934d3f1":"printLossPlot(model_3_d_bn_relu_heNor_adam)","dae58638":"# https:\/\/stackoverflow.com\/questions\/34716454\/where-do-i-call-the-batchnormalization-function-in-keras\n\nfrom keras.layers import Dropout\n\nmodel_5_d_bn_relu_rn_adam = Sequential()\n\nmodel_5_d_bn_relu_rn_adam.add(Dense(784, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_d_bn_relu_rn_adam.add(BatchNormalization())\nmodel_5_d_bn_relu_rn_adam.add(Dropout(0.5))\n\nmodel_5_d_bn_relu_rn_adam.add(Dense(512, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_5_d_bn_relu_rn_adam.add(BatchNormalization())\nmodel_5_d_bn_relu_rn_adam.add(Dropout(0.5))\n\nmodel_5_d_bn_relu_rn_adam.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_5_d_bn_relu_rn_adam.add(BatchNormalization())\nmodel_5_d_bn_relu_rn_adam.add(Dropout(0.5))\n\nmodel_5_d_bn_relu_rn_adam.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_5_d_bn_relu_rn_adam.add(BatchNormalization())\nmodel_5_d_bn_relu_rn_adam.add(Dropout(0.5))\n\nmodel_5_d_bn_relu_rn_adam.add(Dense(64, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_5_d_bn_relu_rn_adam.add(BatchNormalization())\nmodel_5_d_bn_relu_rn_adam.add(Dropout(0.5))\n\nmodel_5_d_bn_relu_rn_adam.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_5_d_bn_relu_rn_adam.summary()\n\nmodel_5_d_bn_relu_rn_adam.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_5_d_bn_relu_rn_adam.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","de98aacc":"printLossPlot(model_5_d_bn_relu_rn_adam)","e65e7939":"# https:\/\/stackoverflow.com\/questions\/34716454\/where-do-i-call-the-batchnormalization-function-in-keras\n\nfrom keras.layers import Dropout\n\nmodel_5_d_bn_relu_heUni_adam = Sequential()\n\nmodel_5_d_bn_relu_heUni_adam.add(Dense(784, activation='relu', input_shape=(input_dim,), kernel_initializer=he_uniform(seed=None)))\nmodel_5_d_bn_relu_heUni_adam.add(BatchNormalization())\nmodel_5_d_bn_relu_heUni_adam.add(Dropout(0.5))\n\nmodel_5_d_bn_relu_heUni_adam.add(Dense(600, activation='relu', kernel_initializer=he_uniform(seed=None)) )\nmodel_5_d_bn_relu_heUni_adam.add(BatchNormalization())\nmodel_5_d_bn_relu_heUni_adam.add(Dropout(0.5))\n\nmodel_5_d_bn_relu_heUni_adam.add(Dense(512, activation='relu', kernel_initializer=he_uniform(seed=None)) )\nmodel_5_d_bn_relu_heUni_adam.add(BatchNormalization())\nmodel_5_d_bn_relu_heUni_adam.add(Dropout(0.5))\n\nmodel_5_d_bn_relu_heUni_adam.add(Dense(128, activation='relu', kernel_initializer=he_uniform(seed=None)) )\nmodel_5_d_bn_relu_heUni_adam.add(BatchNormalization())\nmodel_5_d_bn_relu_heUni_adam.add(Dropout(0.5))\n\nmodel_5_d_bn_relu_heUni_adam.add(Dense(64, activation='relu', kernel_initializer=he_uniform(seed=None)) )\nmodel_5_d_bn_relu_heUni_adam.add(BatchNormalization())\nmodel_5_d_bn_relu_heUni_adam.add(Dropout(0.5))\n\nmodel_5_d_bn_relu_heUni_adam.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_5_d_bn_relu_heUni_adam.summary()\n\nmodel_5_d_bn_relu_heUni_adam.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_5_d_bn_relu_heUni_adam.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","d530de28":"printLossPlot(model_5_d_bn_relu_heUni_adam)","ff54cb26":"# https:\/\/stackoverflow.com\/questions\/34716454\/where-do-i-call-the-batchnormalization-function-in-keras\n\nfrom keras.layers import Dropout\n\nmodel_5_d_bn_relu_heNor_adam = Sequential()\n\nmodel_5_d_bn_relu_heNor_adam.add(Dense(600, activation='relu', input_shape=(input_dim,), kernel_initializer=he_normal(seed=None)))\nmodel_5_d_bn_relu_heNor_adam.add(BatchNormalization())\nmodel_5_d_bn_relu_heNor_adam.add(Dropout(0.5))\n\nmodel_5_d_bn_relu_heNor_adam.add(Dense(512, activation='relu', kernel_initializer=he_normal(seed=None)) )\nmodel_5_d_bn_relu_heNor_adam.add(BatchNormalization())\nmodel_5_d_bn_relu_heNor_adam.add(Dropout(0.5))\n\nmodel_5_d_bn_relu_heNor_adam.add(Dense(256, activation='relu', kernel_initializer=he_normal(seed=None)) )\nmodel_5_d_bn_relu_heNor_adam.add(BatchNormalization())\nmodel_5_d_bn_relu_heNor_adam.add(Dropout(0.5))\n\nmodel_5_d_bn_relu_heNor_adam.add(Dense(128, activation='relu', kernel_initializer=he_normal(seed=None)) )\nmodel_5_d_bn_relu_heNor_adam.add(BatchNormalization())\nmodel_5_d_bn_relu_heNor_adam.add(Dropout(0.5))\n\nmodel_5_d_bn_relu_heNor_adam.add(Dense(64, activation='relu', kernel_initializer=he_normal(seed=None)) )\nmodel_5_d_bn_relu_heNor_adam.add(BatchNormalization())\nmodel_5_d_bn_relu_heNor_adam.add(Dropout(0.5))\n\nmodel_5_d_bn_relu_heNor_adam.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_5_d_bn_relu_heNor_adam.summary()\n\nmodel_5_d_bn_relu_heNor_adam.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_5_d_bn_relu_heNor_adam.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","0fa3c6c8":"printLossPlot(model_5_d_bn_relu_heNor_adam)","0d41b011":"**OBSERVATION**\n1. From the above table we can see that the Model with 2 Layers with Input as 600-256 and \"he_normal\" Initializer gave the highest accuracy with **0.9852** \n<br> then followed by the Model with 5 Layers with Input as 600-512-256-128-64 and \"he_normal\" Initializer gave the second highest accuracy with **0.9845**\n1. The Model with 3 Layers with Input as 600-256 and \"he_normal\" Initializer gave the lowest accuracy with **0.7722**","97185ada":"<h2> 3.1 Layer: MLP + Dropout + Batch Normalization+ ReLu (Random Normal) + Adam Optimizer <\/h2>","df9459ed":"<h2> 5.1 Layer: MLP + Dropout + Batch Normalization+ ReLu (RandomNormal) + Adam Optimizer <\/h2>","8548cf8d":"## MLPs on MNIST using Keras","f1477c5f":"<h2> 3.3 Layer: MLP + Dropout + Batch Normalization+ ReLu (He_Normal) + Adam Optimizer <\/h2>","ac7be206":"<h2> 2.2 Layer: MLP + Dropout + Batch Normalization+ ReLu(He_Uniform) + Adam Optimizer <\/h2>","09b69d1e":"<h2> 5.2 Layer: MLP + Dropout + Batch Normalization+ ReLu (He_Uniform) + Adam Optimizer <\/h2>","8c3b0fb3":"## SUMMARY","9fda4e66":"<h2> 2.3 Layer: MLP + Dropout + Batch Normalization+ ReLu(He_Normal) + Adam Optimizer <\/h2>","eed0ad49":"<h2> 2.1 Layer: MLP + Dropout + Batch Normalization+ ReLu (RandomNormal) + Adam Optimizer <\/h2>","b5e64db1":"<h2> 3.2 Layer: MLP + Dropout + Batch Normalization+ ReLu (He_Uniform) + Adam Optimizer <\/h2>","1e81f430":"![assignemntArchi.JPG](attachment:assignemntArchi.JPG)","6e2ffbef":"<h2> 5.3 Layer: MLP + Dropout + Batch Normalization+ ReLu (He_Normal) + Adam Optimizer <\/h2>"}}