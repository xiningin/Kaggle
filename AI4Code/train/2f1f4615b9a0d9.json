{"cell_type":{"65e8faa3":"code","cd453073":"code","86a0ea3e":"code","84fe5f67":"code","c0601bed":"code","2564516a":"code","726c3378":"code","be40d63a":"code","ccb946e3":"code","95361c6f":"code","c3f0b34b":"code","54c7c74d":"code","6230b09d":"code","e6180b82":"code","a3d5ebe4":"code","b06f1d1e":"markdown","d5510726":"markdown","67f89ffb":"markdown","c79acab4":"markdown","54caf492":"markdown","026c92c8":"markdown","babf9a66":"markdown","28f33069":"markdown","07a0342b":"markdown","aa96c195":"markdown","4e317168":"markdown","100893c5":"markdown","a56c4637":"markdown","7c295b52":"markdown"},"source":{"65e8faa3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\nimport math\nfrom scipy.stats import norm, skew, pearsonr\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\n\n# Any results you write to the current directory are saved as output.","cd453073":"def scoreData(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators = 100, random_state =1)\n    model.fit(X_train,y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid,preds)","86a0ea3e":"## Load in Data\ntrain_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')\n\n## check to make sure target field is fully populated\nprint(\"There are \"+ str(train_data.SalePrice.isnull().sum()) + \" entries missing for the target column SalePrice.\")\n\n## Store ID field then drop the ID field. We drop the ID field because \n## it is not useful in predicting \"SalePrice\". Set \"y_XXX\" as the target\/dependent column(\"SalePrice\")\ntrain_ID = train_data[\"Id\"]\ntest_ID = test_data[\"Id\"]\ntrain_data = train_data.drop(\"Id\",axis = 1)\ntest_data = test_data.drop(\"Id\", axis = 1)\nX_train = train_data\nX_test = test_data\ny_train = train_data[\"SalePrice\"]\nX_train = X_train.drop(\"SalePrice\",axis = 1)\n\n\n## validate that there are not extra fields within train data that are not present within test data ()\nunique_col_check = len(set(list(X_train.columns)+list(X_test.columns)))\nprint(\"There are \" + str(unique_col_check) + \" unique columns between the two data sets\")\nprint(\"There are \" + str(len(X_train.columns)) + \" within the training set.\")\nprint(\"There are \" + str(len(X_test.columns)) + \" within the test set.\")","84fe5f67":"## By running \"y_train.describe()\" we can tell that this data is skewed and might need to be normalized\n\n## Try different transformations to reducde skewness\nprint(\"Initial\")\nprint(\"Skewness: %.2f\" % y_train.skew())\nprint(\"Kurtosis: %.2f\" % y_train.kurtosis())\ny_train_sqrt = np.sqrt(y_train)\nprint(\"Sqrt\")\nprint(\"Skewness: %.2f\" % y_train_sqrt.skew())\nprint(\"Kurtosis: %.2f\" % y_train_sqrt.kurtosis())\ny_train_cbrt = np.cbrt(y_train)\nprint(\"Cbrt\")\nprint(\"Skewness: %.2f\" % y_train_cbrt.skew())\nprint(\"Kurtosis: %.2f\" % y_train_cbrt.kurtosis())\ny_train_log = np.log(y_train)\nprint(\"Log\")\nprint(\"Skewness: %.2f\" % y_train_log.skew())\nprint(\"Kurtosis: %.2f\" % y_train_log.kurtosis())\n## Log is most effective in reducing skewness to normal distribution levels\n\n\nfig = plt.figure(figsize=(14,7))\nplt.subplot(2,2,1)\nsns.distplot(y_train, fit = norm)\nplt.subplot(2,2,2)\nres = scipy.stats.probplot(y_train, plot = plt)\n\nplt.subplot(2,2,3)\nsns.distplot(y_train_log, fit = norm)\nplt.subplot(2,2,4)\nres = scipy.stats.probplot(y_train_log, plot = plt)\n\n","c0601bed":"All_data = pd.concat([X_train,X_test])\nprint(All_data.shape)\nempty_count = All_data.isnull().sum().sort_values(ascending = False)\nempty_percent = empty_count\/len(All_data)\nempty_keep = [key for key in empty_percent.keys() if empty_percent[key] < .3] \nAll_data = All_data[empty_keep]\nprint(All_data.shape)\n\n##num_cols = [name for name in All_data.columns if All_data[name].dtype in ['int64','float64']]\n##print(len(num_cols))\n##cat_cols = [name for name in All_data.columns if All_data[name].dtype not in ['int64','float64']]\n##print(len(cat_cols))\n","2564516a":"All_data[\"Age\"] = All_data[\"YrSold\"] - All_data[\"YearBuilt\"]\nAll_data[\"MSSubClass\"] = All_data[\"MSSubClass\"].apply(str)\nAll_data[\"RemodAge\"] = All_data[\"YrSold\"] - All_data[\"YearRemodAdd\"] ## Change to was there remod yes=1 no=0 \nAll_data[\"GarageYrBlt\"] = All_data[\"GarageYrBlt\"].apply(str)\nAll_data[\"MoSold\"] = All_data[\"MoSold\"].apply(str)\nAll_data[\"YearBuilt\"] = All_data[\"YearBuilt\"].apply(str)\nAll_data[\"YearRemodAdd\"] = All_data[\"YearRemodAdd\"].apply(str)\nAll_data[\"YrSold\"] = All_data[\"YrSold\"].apply(str)\nAll_data[\"TotalSF\"] = All_data[\"TotalBsmtSF\"] - All_data[\"BsmtUnfSF\"] + All_data[\"1stFlrSF\"] + All_data[\"2ndFlrSF\"]\nAll_data = All_data.drop([\"TotalBsmtSF\",\"BsmtUnfSF\",\"1stFlrSF\",\"2ndFlrSF\"],axis = 1 )\nAll_data[\"TotalBath\"] = All_data[\"BsmtFullBath\"] + .5*All_data[\"BsmtHalfBath\"] + All_data[\"FullBath\"] + .5*All_data[\"HalfBath\"]\nAll_data = All_data.drop([\"BsmtFullBath\",\"BsmtHalfBath\",\"FullBath\",\"HalfBath\"],axis = 1 )\n#All_data[\"RemodYN\"] = All_data[\"YearRemodAdd\"].apply(lambda x: 'Y' if x == All_data[\"YearBuilt\"] else \"N\")   \nAll_data[\"RemodYN\"] = \"Y\"\n\nAll_data.loc[All_data.YearRemodAdd == All_data.YearBuilt, 'RemodYN'] = 'N'  \n\n## Check how many columns are numerical versus categorical\nprint(All_data.shape)\nnum_cols = [name for name in All_data.columns if All_data[name].dtype in ['int64','float64']]\nprint(len(num_cols))\ncat_cols = [name for name in All_data.columns if All_data[name].dtype not in ['int64','float64']]\nprint(len(cat_cols))\n\n","726c3378":"All_data_cat = All_data[cat_cols]\nAll_data_num = All_data[num_cols]\n\ncat_count = All_data_cat.isnull().sum().sort_values(ascending = False)\nnum_count = All_data_num.isnull().sum().sort_values(ascending = False)\nprint(cat_count)\nprint(\" *********************************************************  \")\nprint(num_count)","be40d63a":"## Categorical attributes to be replaced with \"None\"\ncat_none_replace = [\"GarageCond\",\"GarageQual\",\"GarageFinish\",\"GarageType\",\"BsmtCond\",\"BsmtExposure\",\"BsmtQual\",\n                    \"BsmtFinType2\",\"BsmtFinType1\",\"MasVnrType\"]\n\n## Categorical attributes to be replaced with their mode set values\ncat_mode_replace = [\"MSZoning\", \"Utilities\", \"Functional\", \"Exterior2nd\", \"Exterior1st\", \"SaleType\",\n                    \"Electrical\", \"KitchenQual\"]\n## Numerical attributes to be replaced with their mean set values\nnum_mean_replace = [\"TotalBath\", \"BsmtFinSF2\", \"BsmtFinSF1\", \"GarageArea\", \"GarageCars\", \"TotalSF\",\"LotFrontage\"]\n\n## Replace MasVnrArea with 0 \nAll_data[\"MasVnrArea\"] = All_data[\"MasVnrArea\"].fillna(0)\n\n## Replace Categoricals with \"None\"\nfor cat in cat_none_replace:\n    All_data[cat] = All_data[cat].fillna(\"None\")\n    \n## Replace Categoricals with mode\nfor cat in cat_mode_replace:\n    All_data[cat] = All_data[cat].fillna(All_data[cat].mode()[0])\n    \n## Replace Numericals with median\nfor num in num_mean_replace:\n    All_data[num] = All_data[num].fillna(All_data[num].median())\n\nAll_data_cat = All_data[cat_cols]\nAll_data_num = All_data[num_cols]\n\n\n## Check to make sure there is no more missing data\ncat_count = All_data_cat.isnull().sum().sort_values(ascending = False)\nnum_count = All_data_num.isnull().sum().sort_values(ascending = False)\nprint(cat_count)\nprint(\"   \")\nprint(num_count)\n\n","ccb946e3":"final_train = All_data[:len(train_data)]\nfinal_test = All_data[len(train_data):]\n\nnum_cols = [name for name in final_train.columns if final_train[name].dtype in ['int64','float64']]\nprint(len(num_cols))\n\n\nfig = plt.figure(figsize=(15,75))\n\nlow_corr_drop = []\n\ni = 1\nfor num_cols in num_cols:\n    ax = fig.add_subplot(14,3,i)\n    sns.regplot(final_train[num_cols],y_train)\n    corr = scipy.stats.pearsonr(final_train[num_cols],y_train)\n    ax.set_title(\"r = \" + \"{0:.2f}\".format(corr[0]) + \"   p = \" + \"{0:.2f}\".format(corr[1]))\n    if abs(corr[0]) <.35 or corr[1] > .05:\n        low_corr_drop += [num_cols]\n    i+=1\nprint(low_corr_drop)\n\nfinal_train.drop(low_corr_drop, axis = 1, inplace= True)\nfinal_test.drop(low_corr_drop, axis = 1, inplace= True)\nnum_cols = [name for name in final_train.columns if final_train[name].dtype in ['int64','float64']]\n\nprint(len(final_train.columns))","95361c6f":"plt.figure(figsize=(7,7))\n\n\n# calculate the correlation matrix\ncorr = final_train[num_cols].corr()\n\n# plot the heatmap\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns,annot=True)\n","c3f0b34b":"\ncat_cols = [name for name in final_train.columns if final_train[name].dtype not in ['int64','float64']]\nprint(len(cat_cols))\n\nfig = plt.figure(figsize=(15,75))\n\ni = 1\nfor c in cat_cols:\n    ax = fig.add_subplot(15,3,i)\n    sns.stripplot(x=final_train[c],y=train_data[\"SalePrice\"])\n    i+=1\n","54c7c74d":"dispersion_drop = []\ndispersion = []\nfor cols in cat_cols:\n    total = final_train[cols].value_counts()\n    dispersion += [total\/1460]\n\nscore = 0\ntotal_score = []\nfor cat in dispersion:\n    score = 0\n    total = len(cat)\n    expected = 1 \/ total\n  \n    for j in range(len(cat)):\n        if (cat[j]\/expected) < .05:\n            score +=1\n        \n    if score\/total > .499:\n        total_score += [cat.name] \nprint(total_score)\nfinal_train.drop(total_score, axis = 1, inplace= True)\nfinal_test.drop(total_score, axis = 1, inplace= True)    \n\nprint(final_train.shape)\n\ncat_cols = [name for name in final_train.columns if final_train[name].dtype not in ['int64','float64']]\nprint(len(cat_cols))\n\nfig = plt.figure(figsize=(15,75))\n\ni = 1\nfor c in cat_cols:\n    ax = fig.add_subplot(15,3,i)\n    sns.stripplot(x=final_train[c],y=train_data[\"SalePrice\"])\n    i+=1","6230b09d":"## Figure out unique entries per attribute\n\nOne_hot_unique = [[cat, len(final_train[cat].unique())] for cat in cat_cols if len(final_train[cat].unique()) >16]\nprint(One_hot_unique)\n\nfor i,x in One_hot_unique:\n    final_train = final_train.drop(i, axis = 1)\n    final_test = final_test.drop(i, axis = 1)\n\nnum_cols = [name for name in final_train.columns if All_data[name].dtype in ['int64','float64']]\nprint(len(num_cols))\ncat_cols = [name for name in final_train.columns if All_data[name].dtype not in ['int64','float64']]\nprint(len(cat_cols))\n\n#final_train = All_data[:len(train_data)]\n#final_test = All_data[len(train_data):]\n## Create encoder\nOH_encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\nOH_train = pd.DataFrame(OH_encoder.fit_transform(final_train[cat_cols]))\nOH_test = pd.DataFrame(OH_encoder.transform(final_test[cat_cols]))\n\nOH_train.index = final_train.index\nOH_test.index = final_test.index\n\nnum_final_train = final_train.drop(cat_cols, axis = 1)\nnum_final_test = final_test.drop(cat_cols, axis = 1)\n\ncombined_final_train = pd.concat([num_final_train,OH_train],axis = 1)\ncombined_final_test = pd.concat([num_final_test,OH_test],axis = 1)\n## Consider replacing neighborhood with avg sale price for each neighborhood\n\n## Consider bucketing (for example YrBuilt 1900-1909 =1,1910-1919 =2 etc.)","e6180b82":"X_train, X_valid, y_train, y_valid = train_test_split(combined_final_train,y_train,train_size = 0.8, \n                                                      test_size = 0.2, random_state = 1)\nscoreData(X_train, X_valid, y_train, y_valid)","a3d5ebe4":"y_train = train_data[\"SalePrice\"]\nmodel = RandomForestRegressor(n_estimators = 100, random_state =1)\nmodel.fit(combined_final_train,y_train)\nfinal_preds = model.predict(combined_final_test)\n\nfinal_preds = pd.DataFrame(final_preds)\nfinal_preds.columns = [\"SalePrice\"]\n\n## sub = pd.concat([test_ID,final_preds])\nsample_submission.iloc[:,1] = final_preds\nprint(sample_submission)\n\nsample_submission.to_csv(\"submission.csv\", index=False)","b06f1d1e":"Break up data to be send to scoreData function","d5510726":"Some categories have labels with extreemly few data points. I believe this reduces the suitabilty to retain these columns to fit our model. Below I created a process that tries to quantify a way to remove data that is not evenly dispersed amongst its different labels. This method removed 8 columns of data. A list of the removed columns is printed below as part of the output.","67f89ffb":"One Hot Encode categoricals","c79acab4":"Create dataset scoring function to be used later. This function will be fed with the training data that has been broken into four parts:\n\nX_train - A portion of the training data that was broke off to be used for fitting the model. This data contains the independent variables used to try and estimate the target\/dependent variable \"SalePrice\"\n\ny_train - A portion of the training data that was broke off to be used for fitting the model. This data contains the dependent variable \"SalePrice\" used to help fit the model.\n\nX_valid - The remaining portion of the training data containing the independent variables. This data will be fed to the fitted model to generate predictions for \"SalePrice\".\n\ny_valid - The remaining portion of the training data contraining the dependent variable \"SalePrice\". This data will be compared to the predict values generated with \"X_valid\" to score the accuracy of our model.\n","54caf492":"There is no missing data in the \"SalePrice\" column, however it is still important to check for normality. We check for normality because a lot of models are built under the assumption that the data they are fed is normal. ","026c92c8":"Before we can graph the data and run correlation we must fill-in\/impute missing values. Because we impute categorical data differently from numerical data we must break the data up into a numerical subset and a categorical subset. We will list how many entries are missing from each column to get an idea of how to impute. ","babf9a66":"It is most important to check for skewness in determining whether data is normal. Skewness is 0 for perfectly normal data and should be observed as a straigh line when graphed as a probablity plot. I used different methods to make \"SalePrice\" conform to normality. Taking the natural log was the most effect in reducing both kurtosis and skewness. We can take ln(x) instead of ln(x+1) because \"SalePrice\" is non zero. \n\n\n\n\n\n\nWe must remove columns where too much data is missing because it will make imputing methods bias our data. So next we concatenate test and train data and then remove attributes where over 30% of data is missing. By checking the shape of data, we can see that 5 columns were removed in this process.","28f33069":"Break the data back into training and testing data. We can only create graphs and run correlations on the training data because the testing data does not contain the \"SalePrice\" column. In this section of code we will remove all columns that either have a low correlation (below .35) or a high p-value (above .05). Printed out are the titles of the columns that were removed due to the critera explained above as well as graphs with their corresponding correlation (r) and significance (p) values.\n\n\n\nIn some of the graphs we see a lot of data this contained along the y-axis. This seems to me like it would negatively anchor the line of best fit. Is there a better way to determine correlations? It is acceptable to somehow ignore this data that is clustered along the y-axis?","07a0342b":"GarageCond       159\nGarageQual       159\nGarageFinish     159\nGarageType       157\nBsmtCond          82\nBsmtExposure      82\nBsmtQual          81\nBsmtFinType2      80\nBsmtFinType1      79\nMasVnrType        24\n\nAll of the above categorical data will be set to \"None\". This data is currently missing because most of these homes do not have either a Basement, Garage or Vaneer. The remaining categorical data with missing values will be replaced with their mode.\n\n\n\nFor numerical values:\n\nLotFrontage      486\nMasVnrArea        23\nTotalBath          2\nBsmtFinSF2         1\nBsmtFinSF1         1\nGarageArea         1\nGarageCars         1\nTotalSF            1\n\nAll values will be replaced with the median of their set except for MasVnrArea which will be set to 0. We set this to 0 because the missing total is close to the missing total for the categorical data which told us that the house does not have vaneer area. \n\n\nThere are a lot of values missing for LotFrontage, I am curious if anyone has any good ideas on how to better impute this data. Would it be possible to run linear regression between LotFrontage and SalePrice and use this relationship to back-fill LotFrontage using the given SalePrice? What are the implications of such a strategy?","aa96c195":"The highest correlation between two categories is \"GarageArea\" and \"GarageCars\" at 0.88. It might make sense to remove the category that is less strongly correlated with \"SalePrice\"\n\nI expected the correlation between \"Age\" and \"RemodAge\" to be the highest because it is calcualted in a similar fashion however it is 0.59. This result is what led me to believe that many of the houses have been remodeled (if the houses were never remodeled then \"RemodAge\" == \"YearBuilt\") And create the \"RemodYN\" feature.\n\nOverall, I believe it is okay to keep all the above categories to train the model.\n\n\n\nNext we will graph the categorical data.\n","4e317168":"Hello,\n\nThis is the first problem I am attempting on Kaggle. I would appreciate a lot of constructive critisism:\n\n* Things I've done wrong\n* Things I could do to improve results\n* Steps missed or steps I've done in the wrong order\n* Things I could do more efficiently \n* Statistical theory to be aware of\n\n\nThe steps I plan on taking:\n\n1.) Load in the data\n\n2.) Verify the target column \"SalePrice\" is fully populated. Check for normality.\n\n3.) Remove columns where there are many data entries missing.\n\n4.) Break up the data into categorical and numerical subsets.\n\n5.) Verify that the independent columns are not missing data or subject to leakage. Impute data for missing values or delete the columns if too much data is missing. Engineer attributes to be of more value (e.g. create a new numerical attribute \"age\" from multiple attributes)\n\n6.) Analyze the numerical data graphically on an attribute by attribute basis by looking at correlations and p-values between the different variables and the target \"SalePrice\"\n\n7.) Decide whether some attributes\/columns need to be eliminated (low correlation value or high p-value), decide whether some rows need to be removed from the training data (outliers), and choose the methodology for doing so.\n\n8.) Graph categoircal data. Decide what data might be of use.\n\n9.) Engineer data so that it can be better formated numerically (e.g. bucketing categorical data with a large amount of unqiue entries)\n\n10.) Hot_Encode categorical data into numerical data. Check correlations of the new numerical data for suitability (high correlation values). Remove columns that do not add much explanatory value. \n\n11.) Create model, fit training data, and calcuate predictions. Score predictions based on mean avereage error\n\n12.) Run cross validation to ensure results are similar to those calcuated in step 10\n\n13.) Take the model fit with the training data and use it to predict \"SalePrice\" for the test data.\n\n","100893c5":"Next we have created a map of the cross-correlations between the independent variables. If the cross-correlations are close to 1.0 then we have multicollinearity which is generally bad. Independent variables should not just be independent of the dependent variable but also independent of each other. However we are not worried about determining the attribution of each independent variable to the dependent variable but we are strictly concerned with the accuracy of our prediction which should not be affected by multicollinearity. Still, if two variables are both close to 1.0, it might tell us that the data is coming from the same source and decribing a similar feature. It would not make sense to keep two \"copies\" data explaining the same thing. In the event that two variables are strongly cross-correlated, we should remove the one that has a weaker correlation with the dependent variable. \n","a56c4637":"Create Submission","7c295b52":"Apply changes:\n*  Create \"age\" as (\"YrSold\" - \"YearBuilt\")  \u221a---------------------------------------------How old the house is\n*  Change \"MSSubClass\" to string and make categorical  \u221a----------------------------These int value are categorical classifiers as described in \"data_description.txt\"\n*  Create \"RemodAge\" as (\"YrSold\" - \"YearRemodAdd\") \u221a--------------------------------How many years since the house has been remodeled\n*  Change \"GarageYrBlt\" to string and make categorical \u221a-----------------------------Vintage data is categorical\n*  Change \"MoSold\" to string and make Categorical \u221a-----------------------------------Months of the year are categorical\n*  Change \"YearBuilt\" to string and make Categorical \u221a--------------------------------Vintage data is categorical\n*  Change \"YrSold\" to string and make Categorical \u221a-----------------------------------Vintage data is categorical\n*  Create \"TotalSF\" as (\"TotalBsmtSF\" - \"BsmtUnfSF\") + \"1stFlrSF\" + \"2ndFlrSF\" \u221a------------\"TotalBsmtSF\" - \"BsmtUnfSF\" represents the amount of finished sqft in the basement, when added to first floor and second floor sqft gives total sqft. This will bring together columns that measure the same thing and will allow us to consolidate the total number of columns\n*  Remove \"BsmtFinSF1\" , \"BsmtFinSF2\" , \"1stFlrSF\" , \"2ndFlrSF\" , \"BsmtUnfSF\" \u221a-------------Consolidation from step above\n*  Create \"TotalBath\" as \"BsmtFullBath\" + .5* \"BsmtHalfBath\" + \"FullBath\" + .5* \"HalfBath\" \u221a-----Similar to creating a total sqft column.\n*  Remove \"BsmtFullBath\" , \"BsmtHalfBath\" , \"FullBath\" , \"HalfBath\" \u221a---------------More consolidation\n*  Create \"RemodYN\" \u221a---------------------------------------------------------------------A new categorical column to signify whether or not a house has been remodeled\n\n"}}