{"cell_type":{"3090859c":"code","a969dcdd":"code","4f42eb25":"code","82bc4d3d":"code","ac26cb29":"code","ec31034b":"code","14190229":"code","1587d314":"code","04c7e233":"code","84a22fdb":"code","92f853cd":"code","dc2dcc5e":"code","58c3c86a":"code","e4d04fc8":"code","6c070e2a":"code","71c04dba":"code","2ba2c454":"code","e88a2630":"code","5f61b9fd":"code","0c1b8acb":"code","4f0d8305":"code","89cd52fa":"code","f27e73af":"code","f0378614":"code","590f61f6":"code","fe6ce362":"code","be6e3cd6":"code","66cd7aee":"code","9c76d9ba":"code","92d58868":"code","305477f8":"code","9f834b93":"code","99fea05a":"code","3d4e395c":"code","3d613be9":"code","7188fd11":"code","a5b872ad":"code","a5bfe883":"code","ea6f1f8a":"code","2b373e3c":"code","1649cd4a":"code","014d0340":"code","7e2c57c7":"code","7b140e1c":"code","4310ef30":"code","852b0957":"code","815af679":"code","6e118d1c":"code","e39790f4":"code","d81f2124":"code","905359de":"code","c50c836c":"code","045d8164":"markdown","ad625959":"markdown","ce2f9922":"markdown","85b0c489":"markdown","20e76fde":"markdown","9336a04f":"markdown","584fcfe2":"markdown","59ac13f8":"markdown","1c5da44a":"markdown","255d01bf":"markdown","d6819e57":"markdown","101b1832":"markdown","331609d3":"markdown","fa7209fd":"markdown","84b78772":"markdown","eb0c3923":"markdown","901100f6":"markdown","a925b85b":"markdown","210a80de":"markdown","d5e81c00":"markdown","bd22bcd8":"markdown","bf718f26":"markdown","af7c016e":"markdown","87355628":"markdown","d84f2035":"markdown","8f0c8931":"markdown","5cdf3cd1":"markdown","94274119":"markdown","a58b5cb9":"markdown","55f69a2e":"markdown","b902b265":"markdown","c9d95733":"markdown","ba602faa":"markdown","0b0938dc":"markdown","aecc08bf":"markdown","00299924":"markdown","ab35b221":"markdown","bff4aa12":"markdown","41f3e5bb":"markdown","27c7aff4":"markdown","1b715fb4":"markdown","3624c6d5":"markdown","7ce4bd76":"markdown","2883acaa":"markdown","23a6e9ad":"markdown","ae3c34ab":"markdown"},"source":{"3090859c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a969dcdd":"import itertools\nimport pandas as pd\nimport scipy.stats as stats\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import feature_selection\n\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom statsmodels.graphics.gofplots import qqplot\n\nsns.set()","4f42eb25":"df = pd.read_csv(\"..\/input\/car-mpg\/mpg_raw.csv\")\ndf.head()","82bc4d3d":"# so now the data is in rectangular form with 398 entries each having 9 distinct properties\ndf.shape","ac26cb29":"# let's list all the columns\ncolumns = list(df.columns)\ncolumns","ec31034b":"# we now describe the properties of this dataframe like column datatype etc.\ndf.info()","14190229":"cats = list(df.select_dtypes(include=['object']).columns)\nnums = list(df.select_dtypes(exclude=['object']).columns)\nprint(f'categorical variables:  {cats}')\nprint(f'numerical variables:  {nums}')","1587d314":"# let's inspect how many unique values are there in each column.\ndf.nunique(axis=0)","04c7e233":"# cylinders and model_year also seems to be categorical so lets update the lists\ncats.extend(['cylinders', 'model_year'])\nnums.remove('cylinders')\nnums.remove('model_year')\n\nprint(f'categorical variables:  {cats}')\nprint(f'numerical variables:  {nums}')","84a22fdb":"# check for `nans` in each column\ndf.isna().sum()","92f853cd":"# let's print these 6 `nan` containing rows \ndf[df.isnull().any(axis=1)]","dc2dcc5e":"# nan rows proportion in data\n6 \/ len(df)","58c3c86a":"# for now remove all nan rows as they are just 1.5%\ndf = df[~df.isnull().any(axis=1)]\ndf.reset_index(inplace=True)\ndf.drop('index', inplace=True, axis=1)\ndf.shape","e4d04fc8":"# find total duplicate entries and drop them if any\nprint(f'total duplicate rows: {df.duplicated().sum()}')\n\n# drop duplicate rows if any\ndf = df[~df.duplicated()]\ndf.shape","6c070e2a":"# remove extra spaces if any\nfor col in ['origin', 'name']:\n    df[col] = df[col].apply(lambda x: ' '.join(x.split()))","71c04dba":"df['mpg_level'] = df['mpg'].apply(lambda x: 'low' if x<17 else 'high' if x>29 else 'medium')\ncats.append('mpg_level')\nprint(f'categorical variables:  {cats}')","2ba2c454":"# before we move ahead it's a good practice to group all variables together having same type.\ndf = pd.concat((df[cats], df[nums]), axis=1)\ndf.head()","e88a2630":"ALPHA = 0.05","5f61b9fd":"# Contingency Table (aka frequency table)\npd.crosstab(df.origin, df.model_year)","0c1b8acb":"observed_values = pd.crosstab(df.origin, df.mpg_level).values\nobserved_values","4f0d8305":"# help(stats.chi2_contingency)\nchi2, p, dof, expected_values = stats.chi2_contingency(observed_values)\nchi2, p, dof, expected_values","89cd52fa":"if p <= ALPHA:\n    print(f'Rejected H0 under significance level {ALPHA} `origin` & `model_year` are dependent.')\nelse:\n    print(f'Fail to reject H0 due to lack of evidence under significance level {ALPHA} `origin` & `model_year` are independent.')","f27e73af":"# help(stats.fisher_exact)\n# stats.fisher_exact doesn't support contingency table more than 2x2","f0378614":"df_cat_label =  pd.concat([df.loc[:, ['origin', 'mpg_level']].apply(lambda x: LabelEncoder().fit_transform(x)),\n                           df.loc[: , 'cylinders': 'model_year']], axis=1)\n\ndf_cat_label.head()","590f61f6":"chi2_res = feature_selection.chi2(df_cat_label, df.mpg_level)\ndf_chi2 = pd.DataFrame({\n    'attr1': 'mpg_level',\n    'attr2': df_cat_label.columns,\n    'chi2': chi2_res[0],\n    'p': chi2_res[1],\n    'alpha': ALPHA\n})\ndf_chi2['H0'] = df_chi2.p.apply(lambda x: 'rejected' if x <= ALPHA else 'fail to reject')\ndf_chi2['relation'] = df_chi2.H0.apply(lambda x: 'dependent' if x=='rejected' else 'independent')\ndf_chi2","fe6ce362":"nums","be6e3cd6":"fig = pyplot.figure(1, (10, 4))\n\nax = pyplot.subplot(1,2,1)\nsns.distplot(np.log2(df.mpg))\npyplot.tight_layout()\n\nax = pyplot.subplot(1,2,2)\nsns.distplot(np.log2(df.weight))\npyplot.tight_layout()\n\npyplot.show()","66cd7aee":"# quantile-quantile plots on original data\nfig = pyplot.figure(1, (18,8))\n\nfor i,num in enumerate(nums):\n    ax = pyplot.subplot(2,3,i+1)\n    qqplot(df[num], line= 's', ax=ax)\n    ax.set_title(f'qqplot - {num}')\n    pyplot.tight_layout()","9c76d9ba":"# let's contruct a function\ndef shapiro_wilk_test(df: pd.DataFrame, cols: list, alpha=0.05):\n    # test the null hypothesis for columns given in `cols` of the dataframe `df` under significance level `alpha`.\n    for col in cols:\n        _,p = stats.shapiro(df[col])\n        if p <= alpha:\n            print(f'''\\nRejected H0 under significance level {alpha}\\n{col} doesn't seems to be normally distributed''')\n        else:\n            print(f'''\\nFail to reject H0 due to lack of evidence under significance level {alpha}\\n{col} seem to be normally distributed''')","92d58868":"shapiro_wilk_test(df, nums)","305477f8":"_, p = stats.shapiro(df.acceleration)\np","9f834b93":"from sklearn.preprocessing import PowerTransformer\n\ndf_tfnum = pd.DataFrame(PowerTransformer().fit_transform(df[nums]), columns=nums)\ndf_tfnum.head()","99fea05a":"fig = pyplot.figure(1, (18,8))\n\nfor i,num in enumerate(['mpg', 'displacement', 'horsepower', 'acceleration']):\n    ax = pyplot.subplot(2,3,i+1)\n    sns.distplot(df_tfnum[num])\n    ax.set_xlabel(f'transformed {num}')\n    pyplot.tight_layout()","3d4e395c":"# quantile-quantile plots on transformed data\nfig = pyplot.figure(1, (18,8))\n\nfor i,num in enumerate(['mpg', 'displacement', 'horsepower', 'acceleration']):\n    ax = pyplot.subplot(2,3,i+1)\n    qqplot(df_tfnum[num], line='s', ax=ax)\n    ax.set_title(f'qqplot - transformed {num}')\n    pyplot.tight_layout()","3d613be9":"shapiro_wilk_test(df_tfnum, ['mpg', 'displacement', 'horsepower', 'acceleration'])","7188fd11":"_, p = stats.shapiro(df_tfnum.acceleration)\np","a5b872ad":"for num in nums:\n    if num == 'mpg':\n        continue\n    \n    corr, p = stats.spearmanr(df.mpg, df[num])\n\n    print(f'\\n* `mpg` & `{num}`\\n')\n    print(f'corr: {round(corr, 4)} \\t p: {p}')\n\n    if p <= ALPHA:\n        print(f'Rejected H0 under significance level {ALPHA}, mpg & {num} are correlated')\n    else:\n        print(f'''Fail to reject H0 due to lack of evidence under significance level {ALPHA}, \n              mpg & {num} are not correlated''')","a5bfe883":"def test_correlation(x1, x2, method='spearman', alpha=0.05):\n    # this function returns correlation, p-value and H0 for `x1` & `x2`\n    \n    ALLOWED_METHODS = ['pearson', 'spearman', 'kendall']\n    if method not in ALLOWED_METHODS:\n        raise ValueError(f'allowed methods are {ALLOWED_METHODS}')\n        \n    if method=='pearson':\n        corr, p = stats.pearsonr(x1,x2)\n    elif method=='spearman':\n        corr, p = stats.spearmanr(x1,x2)\n    else:\n        corr, p = stats.kendalltau(x1,x2)\n    \n    h0 = (\n    'rejected'\n    if p<=ALPHA else\n    'fail to reject')\n    \n    return corr, p, h0","ea6f1f8a":"df_corr = pd.DataFrame(columns=['attr1', 'attr2', 'corr', 'p', 'H0'])\n\nfor combo in itertools.combinations(nums, r=2):\n    corr, p, h0 = test_correlation(df[combo[0]], df[combo[1]])\n    df_corr = df_corr.append({'attr1':combo[0], 'attr2':combo[1],\n                              'corr':round(corr, 5), 'p':p, 'H0':h0}, ignore_index=True)\n    \ndf_corr","2b373e3c":"shapiro_wilk_test(df[df.origin=='japan'], ['acceleration'])","1649cd4a":"shapiro_wilk_test(df[df.origin=='usa'], ['acceleration'])","014d0340":"# because the variance is not same for the two distributions hence equal_var=False\n_, p = stats.ttest_ind(df[df.origin=='japan'].acceleration, df[df.origin=='usa'].acceleration, equal_var=False)\n\nif p <= ALPHA:\n    print(f'Rejected H0 under {ALPHA*100}% significance, Different distributions.')\nelse:\n    print(f'Fail to Reject H0 under {ALPHA*100}% significance, Same distributions.')","7e2c57c7":"_, p = stats.f_oneway(df[df.origin=='japan'].acceleration, df[df.origin=='usa'].acceleration, df[df.origin=='europe'].acceleration)\n\nif p <= ALPHA:\n    print(f'Rejected H0 under {ALPHA*100}% significance, Different distributions.')\nelse:\n    print(f'Fail to Reject H0 under {ALPHA*100}% significance, Same distributions.')","7b140e1c":"shapiro_wilk_test(df[df.origin=='japan'], ['horsepower'])","4310ef30":"shapiro_wilk_test(df[df.origin=='europe'], ['horsepower'])","852b0957":"shapiro_wilk_test(df[df.origin=='usa'], ['horsepower'])","815af679":"_, p = stats.kruskal(df[df.origin=='japan'].horsepower, df[df.origin=='usa'].horsepower, df[df.origin=='europe'].horsepower)\n\nif p <= ALPHA:\n    print(f'Rejected H0 under {ALPHA*100}% significance, Different distributions.')\nelse:\n    print(f'Fail to Reject H0 under {ALPHA*100}% significance, Same distributions.')","6e118d1c":"_, p = stats.mannwhitneyu(df[df.mpg_level=='high'].acceleration, df[df.mpg_level=='medium'].acceleration)\n\nif p <= ALPHA:\n    print(f'Rejected H0 under {ALPHA*100}% significance, Different distributions.')\nelse:\n    print(f'Fail to Reject H0 under {ALPHA*100}% significance, Same distributions.')","e39790f4":"acc_gb_year = df.groupby('model_year')['mpg']\n\nacc_yr = []\nfor yr in df.model_year.unique():\n    acc_yr.append(list(acc_gb_year.get_group(yr)))","d81f2124":"_, p = stats.kruskal(*acc_yr)\n\nif p <= ALPHA:\n    print(f'Rejected H0 under {ALPHA*100}% significance, Different distributions.')\nelse:\n    print(f'Fail to Reject H0 under {ALPHA*100}% significance, Same distributions.')","905359de":"# help(feature_selection.f_classif)\nresult_f = feature_selection.f_classif(df.loc[:, 'mpg': 'acceleration'], df.cylinders)\nanova_test_cat = pd.DataFrame({\n    'cat-attr': 'cylinders',\n    'cont-attr': df.loc[:, 'mpg': 'acceleration'].columns,\n    'f': result_f[0],\n    'p': result_f[1],\n    'alpha': ALPHA\n})\nanova_test_cat['H0'] = anova_test_cat.p.apply(lambda x: 'rejected' if x <= ALPHA else 'fail to reject')\nanova_test_cat['relation'] = anova_test_cat.H0.apply(lambda x: 'dependent' if x=='rejected' else 'independent')\nanova_test_cat","c50c836c":"result_f = feature_selection.f_classif(df_cat_label[['origin', 'cylinders', 'model_year']], df.mpg)\nanova_test_cat = pd.DataFrame({\n    'cont-attr': 'mpg',\n    'cat-attr': ['origin', 'cylinders', 'model_year'],\n    'f': result_f[0],\n    'p': result_f[1],\n    'alpha': ALPHA\n})\nanova_test_cat['H0'] = anova_test_cat.p.apply(lambda x: 'rejected' if x <= ALPHA else 'fail to reject')\nanova_test_cat['relation'] = anova_test_cat.H0.apply(lambda x: 'dependent' if x=='rejected' else 'independent')\nanova_test_cat","045d8164":"But in tranformed data all relations are purely linear.\n![image.png](attachment:image.png)\n\n**Note:** Almost everyone is **homoscedastic** but acceleration is bit more **hetro** seeming.","ad625959":"    Test whether acceleration has same distribution for samples with mpg_level high & medium.","ce2f9922":"### Statistical Normality Tests\n\nWe will do hypothesis testing for the normality of numerical attributes using the `shapiro wilk test`.<br>\n\n&emsp;&emsp;&emsp;$H_{0}$: Data is drawn from normal distribution. &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $\\alpha=0.05$","85b0c489":"Ooops, we expected acceleration to be normally distributed but are test rejected it, let's check the p-value.","20e76fde":"Correlation of pairs (mpg, acceleration), (displacement, acceleration) and (weight, acceleration) is moderate whereas remaining all pairs has very high correlation between them.","9336a04f":"Indeed on normalizing the data the likelihood of observing `acceleration` assuming it's normal is very high as compared to earlier.\n\n    So `acceleration` is normally distributed both visually and statistically.","584fcfe2":"## Tests for independence between two categorical variables\n\n### Pearson's Chi-square test\nThe Chi-square statistic is a **non-parametric statistic** tool designed to analyze group differences when the dependent variable is measured at a nominal level(ordinal data can also be used). It is commonly used to compare observed data with data we would expect to obtain according to a specific hypothesis.\n\n$$\\large \\chi ^{2} = \\sum \\frac{(O-E)^{2}}{E}$$\n\nWhere,<br>\n&emsp;&emsp;O : Observed (the actual count of cases in each cell of the table)<br>\n&emsp;&emsp;E : Expected value (calculated below)<br>\n&emsp;&emsp;$\\chi ^{2}$ : The cell Chi-square value\n\n**Assumptions**<br>\n&emsp;&emsp;&emsp;**1.** The test becomes invalid if any of the expected values are below 5<br>\n&emsp;&emsp;&emsp;**2.** The p value calculated is not exact but approximate and converges to exact on increasing data(so not good for small sample size)<br>\n&emsp;&emsp;&emsp;**3.** The number of observations must be 20+<br>\n\nSo, if the expected cell counts are small, it is better to use an exact test as the chi-squared test is no longer a good approximation in such cases. To overcome this we will be using fisher exact test.\n\n### Fisher\u2019s exact test\nFisher\u2019s exact test is used to determine whether there is a significant association between two categorical variables in a contingency table. Fisher\u2019s exact test is an alternative to Pearson\u2019s chi-squared test for independence. While actually valid for all sample sizes, Fisher\u2019s exact test is practically applied when sample sizes are small.\nA general recommendation is to use Fisher\u2019s exact test- instead of the chi-squared test - whenever more than 20 % of cells in a contingency table have expected frequencies < 5.","59ac13f8":"A **log-normal distribution** is a distribution of a random variable whose logarithm is normally distributed. Thus, if the random variable X is log-normally distributed, then Y = ln(X) has a normal distribution.\n\n    We will check whether mpg and weight are log-normal or not.","1c5da44a":"    As there are very few unique values for cylinders and model_year, so it's safe to make them categorical instead of numeric.","255d01bf":"### Statistical Hypothesis Tests\nThe idea of Statistical Hypothesis Tests is very simple and straight forward. We first assumes something about the data like two samples has same mean etc. And then we find the likelihood of observing the given data assuming this assumption as true. If the likelihood is close to zero then we **reject the assumption** and if the likelihood has value greater than some threshold(set by us) then we **fail to reject the assumption**.\n\nIn statistics lingo the assumption is called **Hypothesis**, the likelihood we get is called **p-value**, the threshold we set is of two types either **level of significance** or **critical value** and the test we use is called **Statistical Hypothesis Tests**.\n\nSo if the likelihood we get is very close to zero then that mean **assuming this hypothesis to be true the likelihood of observing\/occurring this data is very less** so that suggests there is something wrong with our assumption. So in the example to means of 2 samples, if the resulted p-value is very close to zero than we can say that **assuming the two samples having the same mean the data we have in hand is very less likely to be generated hence there is something wrong with our assumption and hence we reject it.**\n\n    Note all this is probabilistic and we can do mistakes sometimes and there are known name for those mistakes namely False Positive and False Negative.\n\n### Hypothesis\n\nThere are two type of hypothesis namely-<br><br>\n**Null Hypothesis, $H_{0}$ -** A null hypothesis, proposes that no significant difference exists in a set of given observations.<br>\n**Alternate Hypothesis, $H_{1}$ -** An alternate hypothesis, proposes that there is a significant difference exists in a set of given observations.\n\nFor the purpose of these tests in general,\n\n&emsp;&emsp;&emsp;$H_{0}$: Variable A and Variable B are independent<br>\n&emsp;&emsp;&emsp;$H_{1}$: Variable A and Variable B are not independent.\n\n**Note:** $H_{0}$ and $H_{1}$ are complement of each other.\n\n\n### p-value\nIt's the probability of data given the assumption in a statistical test.\n\n$$\\large Pr(data\\ | \\ assumption)$$\n\nThe **statistical significance** of any finding is done by intrerpreting the p-values. P-value tells us that whether are findings are due to same real change or they are just random fluctuations. \n\n* p-value \u2264 $\\alpha$: significant result, reject null hypothesis.\n* p-value > $\\alpha$: not significant result, fail to reject the null hypothesis.\n\nA p-value can be calculated from a test statistic by retrieving the probability from the test statistics cumulative density function (CDF).\n\nSome tests return a test statistic value from a specific data distribution that can be interpreted in the context of critical values. A **critical value** is a value from the distribution of the test statistic after which point the result is significant\nand the null hypothesis can be rejected.\n* Test Statistic < Critical Value: not significant result, fail to reject null hypothesis.\n* Test Statistic \u2265 Critical Value: significant result, reject null hypothesis.\n\n**Note:** The most common value for significance level used throughout the data science and ML is 5% i.e., $\\alpha=0.05$ and we will be using this same value throughout this notebook.\n\nI recommend **Statistical Methods for ML by Jason Brownlee** if you want to go in-depth.\n\n[refer this decision tree](https:\/\/qphs.fs.quoracdn.net\/main-qimg-0138e6d464b3ff6320971528e4e72c04)","d6819e57":"### Data Description\n\nThe data we are using for EDA is the [auto mpg](https:\/\/archive.ics.uci.edu\/ml\/datasets\/auto+mpg) dataset taken from UCI repository.\n\nInformation regarding data<br>\n&emsp;&emsp;&emsp;&emsp;**Title:** Auto-Mpg Data<br>\n&emsp;&emsp;&emsp;&emsp;**Number of Instances:** 398<br>\n&emsp;&emsp;&emsp;&emsp;**Number of Attributes:** 9 including the class attribute<br>\n&emsp;&emsp;&emsp;&emsp;**Attribute Information:**\n    1. mpg:           continuous\n    2. cylinders:     multi-valued discrete\n    3. displacement:  continuous\n    4. horsepower:    continuous\n    5. weight:        continuous\n    6. acceleration:  continuous\n    7. model year:    multi-valued discrete\n    8. origin:        multi-valued discrete\n    9. car name:      string (unique for each instance)\n    \n    All the attributes are self-explanatory.\n\nThis data is not complex and is good for analysis as it has a nice blend of both categorical and numerical attributes.\n\n<sup>[data source](https:\/\/archive.ics.uci.edu\/ml\/datasets\/auto+mpg)<sup>","101b1832":"Un-normalized data can sometimes lead to some wrong insights also. For eg,\n\nIn [**Part1-EDA**](https:\/\/www.kaggle.com\/gauravsharma99\/eda-on-mpg-data\/) we ploted the relationship and from these plots it's seems that although all the attributes have a monotonic relation with `mpg` but the relations are not seeming exactly linear.\n\n![image.png](attachment:image.png)","331609d3":"    So all of them are not normally distributed so we will apply non-parametric test.\n    \n&emsp;&emsp;&emsp;$H_{0}$: Sample distributions are equal for horsepower across region. &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $\\alpha=0.05$","fa7209fd":"We will now test whether two samples has the same mean or not. For this we have two types of significance tests for two different conditions.\n\n### Parametric Statistical Significance Tests\n1. **Student\u2019s t-test** - It tests whether the two independent normal distributed samples has the same mean or not.\n2. **Analysis of Variance Test (ANOVA)** - It tests whether the two or more independent normal distributed samples has the same mean or not.\n\nANOVA is same as t-test but for more than 2 variables. So either we can apply t-test pair-wise of apply ANOVA once. Also ANOVA only tells whether all samples are same or not, it doesn't quantify which samples differ or by how much.\n\n### Non-Parametric Statistical Significance Tests\n1. **Mann-Whitney U Test** - Non-parameetric equivalent of Student's t-test.\n2. **Kruskal-Wallis H** - Non-parameetric equivalent of ANOVA (it's for median).\n\nWe will apply appropriate test depending on the sample, i.e., if samples are normally distributed then parametric tests otherwise non-parametric tests.","84b78772":"    So both are normally distributed so we can apply parametric test.\n    \n&emsp;&emsp;&emsp;$H_{0}$: `acceleration of japan` and `acceleration of usa` has same sample mean. &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $\\alpha=0.05$","eb0c3923":"### Statistical Tests for Numerical Attributes","901100f6":"### Visual Normality Checks\n\nFrom these distributions generated in [**Part1-EDA**](https:\/\/www.kaggle.com\/gauravsharma99\/eda-on-mpg-data\/) we can clearly see that `acceleration` is gaussian, `mpg` and `weight` are right-skewed or maybe log-normal.\n\n![image.png](attachment:image.png)","a925b85b":"This is a 3 part series in which I will walk through a dataset analysing it and then at the end do predictive modelling. I highly recommend to follow this series in the order given below but you can also jump to any part by cliking on the heading links in blue.\n\n[**Part 1, Exploratory Data Analysis(EDA):**](https:\/\/www.kaggle.com\/gauravsharma99\/eda-on-mpg-data\/)<br>\nThis part consists of summary statistics of data but the major focus will be on EDA where we extract meaning\/information from data using plots and report important insights about data. This part is more about **data analysis** and **business intelligence(BI)**.\n\n**Part 2, Statistical Analysis:**<br>\nIn this part we will do many statistical hypothesis testing, apply estimation statistics and interpret the results we get. We will also validate this with the findings from part one. We will apply both parametric and non-parametric tests. We will report all the important insights we get in this part. This part is all about **data science** requires statistical background.\n\n[**Part 3, Predictive Modelling:**](https:\/\/github.com\/greatsharma\/MPG\/tree\/master\/Modelling)<br>\nIn this part we will predict some response using predictors. This part is all about **machine learning**.\n\n<span style=\"color:red\">**If you like these notebooks then please upvote and also share with others.**<span>","210a80de":"### Relation between Categorical and Continous attributes","d5e81c00":"Before moving we should first have a good understanding of various terms used in statistics. Otherwise as we move we will surely lost while interpreting the results.\n\n* **Population:** The entire data or entire possible observations.\n* **Sample:** A subset of observations taken from population. As the sample size increases sample will represent the population more closely(Law of Large Numbers).\n* **Parameters:** It's the property of population which we are interested in and never know the exact value unless we do analysis on entire population(which is never the case) eg. mean($\\mu$).\n* **Estimates:** It's sample idea\/value about the population parameters. The entire goal of statistics is to make these sample estimates as close as population parameters eg, average($\\bar{x}$) is the best possible sample estimate of $\\mu$.\n* **Descriptive Statistics:** It's for summarizing data.\n* **Inferential Statistics:** It's for drawing conclusions about the population from samples eg., estimating population mean($\\mu$) using sample average($\\bar{x}$).\n* **Parametric Statistics:** Statistical methods where we assume a distribution of the data such as Gaussian.<br>\n* **Non-Parametric Statistics:** Statistical methods where we do not assume any distribution of the data ie., distribution free.\n* **Statistical Hypothesis Tests:** Methods that quantify the likelihood of observing the result given an assumption or expectation about the result. We will talk on this more later.\n* **Estimation Statistics:** Methods that quantify the uncertainty of a result using confidence intervals.","bd22bcd8":"### Normality Test\nI divided normality test into two parts -\n\n### 1. Visual Normality Checks\nWe will visually check for normality using -\n1. Histogram\n2. Quantile-Quantile plot\n\n### 2. Statistical Normality Tests\n\nThere are three statistical tests for checking the normality of data -\n1. Shapiro-Wilk Test (only for gaussian distribution)\n2. D\u2019Agostino\u2019s K 2 Test (only for gaussian distribution)\n3. Anderson-Darling Test (for many other distributions as well)\n\nWe will use `Shapiro-Wilk Test` but you can try other as well.","bf718f26":"The coming 2 cells are already explained in-depth in [**Part1-EDA**](https:\/\/www.kaggle.com\/gauravsharma99\/eda-on-mpg-data\/) so please refer that if you feel uncomfortable.","af7c016e":"We now make two distinct list for categorical and numerical column names as the analysis differ for both the types. For that we introspect the datatypes of each column and if it is of type `object` then it's categorical and else numerical.\n\nWe will use these two lists heavily throughout the analysis.","87355628":"    So all the H0 are rejected under the significance level of 5%. Accept `acceleration` all the other correlations are very high and  this is also evident from our previous plots.\n    \nWe now create a dataframe for the correlation b\/w every pair.","d84f2035":"So we rejected it under the significance level of 5% but if it was 2.5% then we would have failed to reject the null-hypothesis. We won't change the p-value now otherwise it will be **p-hacking**. One possible reason for the rejection of $H_{0}$ is maybe our data is not scaled and I think scaling it helps.","8f0c8931":"<span style=\"color:red\">**If you already visited **<span> [**Part1-EDA**](https:\/\/www.kaggle.com\/gauravsharma99\/eda-on-mpg-data\/)<span>** then you can directly jump to **<span>[this cell](#stats_analysis_beginning).","5cdf3cd1":"We will now apply `power transform` to **make the data more gaussian like**. And after that check for normality on the transformed data.\n\n**Power Transform**: It transforms data featurewise to make it more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired just like here.\n","94274119":"**Note**: on removing `nans` from the dataframe the power transformer is making the entire weight column 0. I am unable to find the reason for this and asked the sklearn community and update the notebook once I figured this out. If you detect the reason for that then please comment.\n\nFor now we will leave `weight`.","a58b5cb9":"So we are done for now. We did a lot and can extend it further but you get the idea. In the next part we will do some [**predictive modelling**](https:\/\/github.com\/greatsharma\/MPG\/tree\/master\/Modelling).\n\n<span style=\"color:red\">**If you like this notebook then please upvote and also share with others.**<span>","55f69a2e":"## Tests for correlation between two continous variables\n\n### Covariance\n\n$$\\large Cov(x,y) = \\frac{\\sum (x_{_i}-\\bar{x})(y_{_i}-\\bar{y})}{n-1}$$\n\nThe use of the mean in the calculation suggests the need for each data sample to have a Gaussian or Gaussian-like distribution hence its **parametric statistic**. Also it's hard to interpret because it can take any value.\n\n## Linear Association (Pearson's Correlation)\n\n$$\\large Corr_{_p}(x,y) = \\frac{Cov(x,y)}{\\sigma _{x}\\sigma _{y}}$$\n\nThe Pearson correlation coefficient is just a **normalized covariance** between the two variables to give an interpretable score such that $Corr_{p}(x,y)\\in [-1,1]$. It can be used to summarize the strength of the linear relationship between two data samples. The use of mean and standard deviation in the calculation suggests the need for the two data samples to have a Gaussian or Gaussian-like distribution hence it's a **parametric statistic**.\n\nAs a statistical hypothesis test, the method assumes that the samples are uncorrelated (fail to reject H0).\n\n**Assumptions of pearson correlation**:<br>\n&emsp;&emsp;&emsp;**1.** Both variables should have a Gaussian or Gaussian-like distribution.<br>\n&emsp;&emsp;&emsp;**2.** Relationship between the variables should be linear.<br>\n&emsp;&emsp;&emsp;**3.** Homoscedasticity i.e., a sequence of random variables where all its random variables have the same finite variance.<br>\n\nAlso Pearson is quite sensitive to outliers.<br>\n\nTwo variables may be related by a nonlinear relationship, such that the relationship is stronger or weaker across the distribution of the variables. In these cases, even when variables have a strong association, Pearson\u2019s correlation would be low. Further, the two variables being considered may have a non-Gaussian distribution. To properly identify association between variables with non-linear relationships, we can use rank-based correlation approaches.\n\n## Ordinal Association (Rank correlation)\nRank correlation refers to methods that quantify the association between variables using the ordinal relationship between the values rather than the specific values. In this we first sort data in ascending order, then assign integer rank to them and then use it to find the correlation b\/w variables. Because no distribution for the values is assumed, rank correlation methods are referred to as distribution-free correlation or nonparametric correlation. \n\nFour types of rank correlation methods are as follows:\n\n### 1. Spearman's Rank Correlation\n\n$$\\large Corr_{_s}(x,y) = \\frac{Cov(rank(x), rank(y))}{\\sigma _{rank(x)}\\sigma _{rank(y)}}$$\n\nSpearman's Correlation is a **non-parametric rank correlation** and is also interpretable because $Corr_{s}(x,y)\\in [-1,1]$. In this instead of calculating the coefficient using covariance and standard deviations on the samples themselves, these statistics are calculated by converting the raw data into rank data hence non-parametric. **This is a common approach used in non-parametric statistics.**\n\nAs a statistical hypothesis test, the method assumes that the samples are uncorrelated (fail to reject H0).\n\n### 2. Kendall\u2019s Rank Correlation\nThe intuition for the test is that it calculates a normalized score for the number of matching or concordant rankings between the two samples. As such, the test is also referred to as Kendall\u2019s concordance test.\n\nAs a statistical hypothesis test, the method assumes that the samples are uncorrelated (fail to reject H0).\n\n### 3. Goodman and Kruskal\u2019s Rank Correlation\n### 4. Somers\u2019 Rank Correlation\n\n**Types of Correlation**:<br>\n&emsp;&emsp;&emsp;**Positive:** both variables change in the same direction.<br>\n&emsp;&emsp;&emsp;**Neutral:** no relationship in the change of the variables.<br>\n&emsp;&emsp;&emsp;**Negative:** variables change in opposite directions.","b902b265":"### References\nThe following material has been used as source and guidance-\n* Data source:  https:\/\/archive.ics.uci.edu\/ml\/datasets\/auto+mpg\n* Statistical Methods for ML by Jason Brownlee. Most of the definations are taken from this awesome book.\n* https:\/\/cyfar.org\/what-data-analysis","c9d95733":"The statistical test only tells the likelihood of an effect. It doesn't tell us the size of the effect. The results of an experiment could be significant, but the effect so small that it has little consequence or the result could be insignificant, but the effect is large.\n\n**Effect size:** It is the size or magnitude of an effect or result as it would be expected to occur in a population. Unlike **significance tests** which just tells how likely is the effect, **effect size** actually tells the value of the effect occurred. So it gives us more power.\n\n**We will find the effect test for the relation of `mpg` with other numercal features**. i.e., we will be getting an absolute value instead of likelihood which quantify how much correlation is there b\/w `mpg` and other numerical features.","ba602faa":"This is **part 2** ie., Statistical Analysis. We won't stretch this part too long and do following things in sequential manner.\n\n1. **Preprocess the data**, exact same as [**Part1-EDA**](https:\/\/www.kaggle.com\/gauravsharma99\/eda-on-mpg-data\/).\n2. **Tests for independence between two categorical attributes**\n3. **Normality Test for numeric attributes**\n4. **Correlation between numeric attributes**\n5. **Parametric and Non-Parametric test for samples**\n\nI make use of **hypothesis-testing** heavily throughout the notebook, so it is also a good to go notebook for those who are looking for how to apply hypothesis-testing in data science and machine learning.\n\nAll the references used for this notebook is mentioned at the end.","0b0938dc":"But all the above association tests not only gives the effect size but also the p-value. So can look at both the values. We will be using `spearman` but you can use any other accept `pearson` because all numerical variables doesn't satisfies the pearson's assumptions.<br>\n\nSo for all correlation test b\/w mpg and other attribute our null hypothesis will be,\n\n&emsp;&emsp;&emsp;$H_{0}$: `mpg` and `other attribute` are not correlated. &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $\\alpha=0.05$","aecc08bf":"So after applying log transformation we find that weight is not log-normal but mpg visually **looks like log-normal**.\n\n    Let's check for normality using quantile-quantile plots.","00299924":"<a id='stats_analysis_beginning'><\/a>\n## Statistical Analysis","ab35b221":"So `chi2` assumption failed for every pair but it's not that we can't apply, we can but the results are not reliable. But the contingency table of `origin` and `model_year` is still good to try most values are >= 5.<br>\n\n&emsp;&emsp;&emsp;$H_{0}$: origin are model_year are independent. &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $\\alpha=0.05$","bff4aa12":"    Sklearn also has chi2 test available let's use it to test dependency of all categorical attributes with `mpg_level`","41f3e5bb":"    So horsepower consists of total of 6 nan rows comprising of around 1.5% of data. As this fraction is very low so it's safe to drop these nan rows for now.\n\n<sup>Note: If the nan-row proportion is large enough then we won't drop it but instead impute missing values.<sup>","27c7aff4":"    Let's test whether acceleration in `japan` and `usa` has the same mean.\n    \nFirst we check whether acceleration of both japan and usa are normally distributed or not and then apply the applicable tests.","1b715fb4":"    Test for mpg distribution across the years.","3624c6d5":"We will first import the data into a pandas dataframe and inspect it's properties.","7ce4bd76":"    so both histplot & qqplot of `acceleration` indicates that it is indeed close to gaussian.","2883acaa":"    Let's test whether horsepower across all the regions has the same distribution or not.","23a6e9ad":"Power transforms does two things, first it scaled the data i.e., now data is centered at 0 and also made the distribution more gaussian-like simultaneously preserving the original structure. It does so by applying transformations like square-root, log etc.\n\n`acceleration` is still gaussian, skewness is removed from `mpg` & `weight` making mpg gaussian-like. Also the distribution for `displacement` is improved now it's bimodal which respects the observation.\n\nOne thing you have noticed that after applying power transform distribution of `mpg` & `weight` is quite similar to what we get on applying log transform. Infact the power transform of sklearn indeed applies log transforms, refer [this](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#preprocessing-transformer).\n\n    Let's check for normality using quantile-quantile plots.","ae3c34ab":"The coming few cells invloves **cleaning the data**, this includes dealing with missing values, duplicate data if any and then align the data. This I already covered in part1. So you can skip to [this](#stats_analysis_beginning) cell if you already visited part1."}}