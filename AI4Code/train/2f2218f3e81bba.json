{"cell_type":{"926c0684":"code","66c71f1b":"code","e1227ea6":"code","b5c4929b":"code","223c771e":"code","164a0954":"code","b47eea40":"code","43c02fb2":"code","e4912721":"code","d742e57c":"code","15405c71":"code","10626f95":"code","98c5470e":"code","9f9c756e":"code","231c2413":"code","d61a2b5e":"code","94c5d99e":"code","81ed40cd":"code","867eff71":"code","e70818bb":"code","657a6d9b":"code","44effc79":"code","ff1d5964":"code","2f2b9abc":"code","7cb298d2":"code","717f3e9b":"code","8296f0de":"code","d5c4a8d5":"code","9f38a111":"code","b51a0216":"code","3b9e2c4b":"code","6e405a59":"code","32f301d0":"code","aef0a751":"code","0eedab9e":"code","efdaba8a":"code","7ef2cf84":"code","b5968de8":"code","f7e9f5ea":"code","70128000":"code","075bbc20":"code","27a7af32":"code","4ac068e5":"code","2901b92a":"code","2c17c7ad":"code","da4a7a7b":"code","bd946cbc":"code","a35351b1":"code","adb19102":"code","d4c1da09":"code","2600d174":"code","17f51f0f":"code","799161f4":"code","eb056f22":"code","45a3f82f":"code","53cb80dc":"code","3bfb2994":"code","d021e38c":"code","d96c1a8e":"code","99a68ea4":"code","1ca27bb7":"markdown","0e21da75":"markdown","c768ce2c":"markdown","321e70ab":"markdown","1d67db69":"markdown","42da62ee":"markdown","f764b5bd":"markdown","541ce39b":"markdown","6e9ff407":"markdown","10b9312e":"markdown","36a1a9b4":"markdown","86395b22":"markdown","4e4ae84c":"markdown","aacd4f99":"markdown","a562adb8":"markdown","c5d02ab4":"markdown","6c4dbb3d":"markdown","9aa82107":"markdown","a0526eaa":"markdown","09e8242e":"markdown","333d3534":"markdown","718f1e15":"markdown","7136bc78":"markdown","9d00e886":"markdown","fbc7b0a4":"markdown","0d1349e3":"markdown","d609f6e4":"markdown","7e04cffc":"markdown","af5274e5":"markdown","96b1d23a":"markdown","f2c0d28f":"markdown","dd616671":"markdown","2a8a4a0f":"markdown","1c6b2e1f":"markdown","3007a919":"markdown","4006bb08":"markdown","42712153":"markdown","c84eb046":"markdown","fcfec7c3":"markdown","cb13267d":"markdown","397fc27f":"markdown","b94c3134":"markdown"},"source":{"926c0684":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66c71f1b":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n# Simply works with Numpy, Matplotlib, Pandas, Sympy etc. \n# SciPy provides numerical integral routines and differential equations interpreters, algorithms to root out equations, standard continuous\/differentiated probability distributions, and various statistical tools.\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\n# Limiting floats output to 3 decimal points","e1227ea6":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","b5c4929b":"train.head()","223c771e":"test.head()","164a0954":"train.info()","b47eea40":"test.info()","43c02fb2":"train.shape","e4912721":"test.shape","d742e57c":"#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","15405c71":"print(train.shape, test.shape)","10626f95":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","98c5470e":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","9f9c756e":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","231c2413":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","d61a2b5e":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)","94c5d99e":"all_data.shape","81ed40cd":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]","867eff71":"all_data_na","e70818bb":"missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","657a6d9b":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='45')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","44effc79":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.8, square=True)","ff1d5964":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n    \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n    \nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n\n# df.mode() : The value that occurs most frequently\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\nall_data = all_data.drop(['Utilities'], axis=1)\n\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","2f2b9abc":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","7cb298d2":"all_data['MSSubClass'].value_counts()","717f3e9b":"all_data['OverallCond'].value_counts()","8296f0de":"all_data['YrSold'].value_counts()","d5c4a8d5":"all_data['MoSold'].value_counts()","9f38a111":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","b51a0216":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","3b9e2c4b":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","6e405a59":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","32f301d0":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","aef0a751":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","0eedab9e":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","efdaba8a":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","7ef2cf84":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","b5968de8":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","f7e9f5ea":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","70128000":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","075bbc20":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","27a7af32":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","4ac068e5":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","2901b92a":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","2c17c7ad":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","da4a7a7b":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","bd946cbc":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a35351b1":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","adb19102":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","d4c1da09":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","2600d174":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","17f51f0f":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","799161f4":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","eb056f22":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","45a3f82f":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","53cb80dc":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","3bfb2994":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","d021e38c":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","d96c1a8e":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","99a68ea4":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","1ca27bb7":"# **Exploratory data analysis**","0e21da75":"**Log-transformation of the target variable**","c768ce2c":"**Box Cox Transformation of (highly) skewed features**\n\n**Box-Cox Transformation** : Originally called Power Transformation, it is also called Box-Cox Transformation, introduced by boxes and cockles. The main purpose of box-cockx transformation is to bring data closer to normal distribution or stabilize the distribution of data, which can be useful in preprocessing data before using analysis methods that assume normality or analysis methods that require normality.\n\nWe use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x .\n\nNote that setting  \u03bb=0  is equivalent to log1p used above for the target variable.","321e70ab":"# **Stacking models**\n\n**Simplest Stacking approach : Averaging base models**\n\nWe begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse (inheritance)\n\n**Averaged base models class**","1d67db69":"**Missing Data**","42da62ee":"* **Gradient Boosting Regression**\n\n**Reference** : https:\/\/3months.tistory.com\/368","f764b5bd":"**More features engeneering**","541ce39b":"* **Elastic Net Regression**","6e9ff407":"**Label Encoding some categorical variables that may contain information in their ordering set**\n* Reference : https:\/\/2-chae.github.io\/category\/1.ai\/30","10b9312e":"# **Data Correlation**","36a1a9b4":"# **Modelling**\n\n**Reference**\n* pipeline : https:\/\/rk1993.tistory.com\/entry\/Python-sklearnpipeline-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8Pipeline%EC%9D%B4%EB%9E%80\n* sklearn.base : https:\/\/suuntree.tistory.com\/307","86395b22":"Is there any remaining missing value ?","4e4ae84c":"**Stacking averaged Models Class**","aacd4f99":"It remains no missing value.","a562adb8":"* **Kernel Ridge Regression**","c5d02ab4":"* **LightGBM**","6c4dbb3d":"The target variable is skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.\n\nReferences\n* https:\/\/mizykk.tistory.com\/44\n* https:\/\/m.blog.naver.com\/PostView.nhn?blogId=s2ak74&logNo=220616766539&proxyReferer=https:%2F%2Fwww.google.com%2F\n* https:\/\/hong-yp-ml-records.tistory.com\/28","9aa82107":"# **Data Processing**\n\n* SalePrice\n* GrlivArea : Above grade (ground) living area square feet","a0526eaa":"Transforming some numerical variables that are really categorical","09e8242e":"# **Target Variable**\n**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first.","333d3534":"# **Ensembling StackedRegressor, XGBoost and LightGBM**\nWe add XGBoost and LightGBM to the StackedRegressor defined previously.\n\nWe first define a rmsle evaluation function","718f1e15":"**Stacking Averaged models Score**\n\nTo make the two approaches comparable (by using the same number of models) , we just average Enet KRR and Gboost, then we add lasso as meta-model.","7136bc78":"**Define a cross validation strategy**\n\nWe use the cross_val_score function of Sklearn. However this function has not a shuffle attribut, we add then one line of code, in order to shuffle the dataset prior to cross-validation","9d00e886":"**Less simple Stacking : Adding a Meta-model**\n\nIn this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model.\n\nThe procedure, for the training part, may be described as follows:\n\n  1. Split the total training set into two disjoint sets (here train and .holdout )\n\n  2. Train several base models on the first part (train)\n\n  3. Test these base models on the second part (holdout)\n\n  4. Use the predictions from 3) (called out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.\n\nThe first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration, we train every base model on 4 folds and predict on the remaining fold (holdout fold).\n\nSo, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as new feature to train our meta-model in the step 4.\n\nFor the prediction part , We average the predictions of all base models on the test data and used them as meta-features on which, the final prediction is done with the meta-model.\n\n![image.png](attachment:image.png)\n\n(Image taken from Faron)\n\n**Reference** : https:\/\/www.kdnuggets.com\/2017\/02\/stacking-models-imropved-predictions.html","fbc7b0a4":"**Imputing missing values**\n\nWe impute them by proceeding sequentially through features with missing values\n\n* **PoolQC** : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.\n* **MiscFeature** : data description says NA means \"no misc feature\"\n* **Alley** : data description says NA means \"no alley access\"\n* **Fence** : data description says NA means \"no fence\"\n* **FireplaceQu** : data description says NA means \"no fireplace\"\n* **LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.\n* **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None\n* **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n* **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement\n* **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2** : For all these categorical basement-related features, NaN means that there is no basement.\n* **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.\n* **MSZoning (The general zoning classification)** : 'RL' is by far the most common value. So we can fill in missing values with 'RL'\n* **Utilities** : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, **this feature won't help in predictive modelling**. We can then safely remove it.\n* **Functional** : data description says NA means typical\n* **Electrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n* **KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\n* **Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n* **SaleType** : Fill in again with most frequent which is \"WD\"\n* **MSSubClass** : Na most likely means No building class. We can replace missing values with None","0d1349e3":"# **Base models**\n\n* LASSO Regression: This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline\n\n**References**\n1. https:\/\/wikidocs.net\/89704\n2. https:\/\/hwiyong.tistory.com\/93","d609f6e4":"**Adding one more important feature**\n\nSince area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house","7e04cffc":"* **XGBoost**","af5274e5":"**Base models scores**","96b1d23a":"# **Import necessary librairies**","f2c0d28f":"# **Final Training and Prediction**","dd616671":"* **XGBoost**","2a8a4a0f":"You can find two outliers(values that deviate significantly from the normal category) in the lower right corner. This should be eliminated because it can cause distortion in the analysis results.\n\n\n\n***Note :***\n\nOutliers removal is note always safe. We decided to delete these two as they are very huge and really bad ( extremely large areas for very low prices).\n\nThere are probably others outliers in the training data. However, removing all them may affect badly our models if ever there were also outliers in the test data. That's why , instead of removing them all, we will just manage to make some of our models robust on them. You can refer to the modelling part of this notebook for that.\n","1c6b2e1f":"**Averaged base models score**","3007a919":"**Skewed features**","4006bb08":"# **Collecting Data**","42712153":"* **LightGBM**\n\n**Reference** : https:\/\/nurilee.com\/lightgbm-definition-parameter-tuning\/","c84eb046":"* **StackedRegressor**","fcfec7c3":"* **Ensemble prediction**","cb13267d":"# **Features engineering**\n\nLet's first concatenate the train and test data in the same dataframe","397fc27f":"# **Reference**\n* https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard","b94c3134":"**Getting dummy categorical features**"}}