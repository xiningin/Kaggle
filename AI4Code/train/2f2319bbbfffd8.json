{"cell_type":{"1bd65bf2":"code","37340c3a":"code","3331331c":"code","b365fd54":"code","fbf859db":"code","7ae8adaf":"code","c19576d4":"code","a2b3b197":"code","9664d208":"code","ea1f0dbc":"code","c92eb93a":"code","79ff24a3":"code","84f12dea":"code","063c1a45":"code","18fdafb9":"code","c0f00966":"code","cab8d9c6":"code","7dc65c0a":"code","d1d85e78":"code","eab304a9":"code","98b0a72e":"code","bbd7aeba":"code","f3efe327":"code","b2d80bed":"code","8af97943":"code","a24498ec":"code","13b3f692":"code","7a181931":"code","32e994f7":"code","36defb62":"code","4514b997":"code","65bcab50":"code","4fcac232":"code","8fc55982":"markdown","097329f9":"markdown","f4f51117":"markdown","4a094c5e":"markdown","40ecf1ab":"markdown","65aa8024":"markdown","2a537302":"markdown","b79e12ae":"markdown","c1520118":"markdown","99e3421e":"markdown","848479dc":"markdown","2459c2a7":"markdown","d1702aca":"markdown","1aa7d4aa":"markdown","3c005487":"markdown","d72f6af1":"markdown","4cab528d":"markdown","6a71c89a":"markdown"},"source":{"1bd65bf2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","37340c3a":"import matplotlib.pyplot as plt\nimport seaborn as sns","3331331c":"\nplt.style.use('ggplot')","b365fd54":"x = np.arange(-5.0, 5.0, 0.1)\nplt.figure(figsize=(20,10))\n##You can adjust the slope and intercept to verify the changes in the graph\ny = 1*(x**3) + 1*(x**2) + 1*x + 3\ny_noise = 20 * np.random.normal(size=x.size)\nydata = y + y_noise\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Independent Variable')\nplt.show()","fbf859db":"x = np.arange(-5.0, 5.0, 0.1)\nplt.figure(figsize=(20,10))\ny = np.power(x,2)\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Independent Variable')\nplt.show()","7ae8adaf":"plt.figure(figsize=(20,15))\nX = np.arange(-5.0, 5.0, 0.1)\nY= np.exp(X)\nplt.plot(X,Y)\nplt.ylabel('Dependent Variable')\nplt.xlabel('Independent Variable')\nplt.show()","c19576d4":"plt.figure(figsize=(20,10))\nX = np.arange(-5.0, 5.0, 0.1)\n\nY = np.log(X)\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Independent Variable')\nplt.show()","a2b3b197":"plt.figure(figsize=(20,10))\nX = np.arange(-5.0, 5.0, 0.1)\n\n\nY = 1-4\/(1+np.power(3, X-2))\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Independent Variable')\nplt.show()","9664d208":"df=pd.read_csv(\"..\/input\/chinagdp\/china_gdp.csv\")\ndf.head()","ea1f0dbc":"plt.figure(figsize=(20,10))\nplt.plot(df[\"Year\"],df[\"Value\"],'r^')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.title(\"China's GDP Development Throughout Years\")","c92eb93a":"x_data, y_data = (df[\"Year\"].values, df[\"Value\"].values)\nx_data","79ff24a3":"# Lets normalize our data\nxdata =x_data\/max(x_data)\nydata =y_data\/max(y_data)","84f12dea":"def sigmoid(x, Beta_1, Beta_2):\n     y = 1 \/ (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y","063c1a45":"from scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, xdata, ydata)\n#print the final parameters\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n","18fdafb9":"\nx = np.linspace(1960, 2015, 55)\nx = x\/max(x)\nplt.figure(figsize=(8,5))\ny = sigmoid(x, *popt)\nplt.plot(xdata, ydata, 'ro', label='data')\nplt.plot(x,y, linewidth=3.0, label='fit')\nplt.legend(loc='best')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()","c0f00966":"#we normalize data\nX=df[\"Year\"]\/max(df[\"Year\"])\ny=df[\"Value\"]\/max(df[\"Value\"])\nX","cab8d9c6":"from sklearn.model_selection import train_test_split\n","7dc65c0a":"X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.25)","d1d85e78":"X_train.shape","eab304a9":"X_test.shape","98b0a72e":"y_train.shape","bbd7aeba":"y_test.shape","f3efe327":"from sklearn.svm import SVR\n#We will choose Regression version of Support Vector Machines","b2d80bed":"model=SVR()","8af97943":"model.fit(X_train.values.reshape(-1, 1), y_train.values.reshape(-1, 1))","a24498ec":"predictions=model.predict(X_test.values.reshape(-1, 1))","13b3f692":"predictions","7a181931":"y_test=y_test.values.reshape(-1,1)\ny_test","32e994f7":"from sklearn.metrics import mean_absolute_error, mean_squared_error","36defb62":"pred=pd.DataFrame(y_test,predictions)\npred","4514b997":"print(mean_absolute_error(y_test, predictions))\n#The error are very low","65bcab50":"print(mean_squared_error(y_test,predictions))","4fcac232":"\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,8))\nfig.suptitle('Horizontally stacked subplots')\nax1.plot(y_test,\"o\")\nax2.plot(predictions,\"o\")\n","8fc55982":"Non-linear regressions are a relationship between independent variables $x$ and a dependent variable $y$ which result in a non-linear function modeled data. Essentially any relationship that is not linear can be termed as non-linear, and is usually represented by the polynomial of $k$ degrees (maximum power of $x$). \n\n$$ \\ y = a x^3 + b x^2 + c x + d \\ $$\n\nNon-linear functions can have elements like exponentials, logarithms, fractions, and others. For example: $$ y = \\log(x)$$\n\nOr even, more complicated such as :\n$$ y = \\log(a x^3 + b x^2 + c x + d)$$\n","097329f9":"# 3. Model Selection and Implemeting Algorithm","f4f51117":"Sometimes, the trend of data is not really linear, and looks curvy. In this case we can use Polynomial regression methods. In fact, many different regressions exist that can be used to fit whatever the dataset looks like, such as quadratic, cubic, and so on, and it can go on and on to infinite degrees.\n\nIn essence, we can call all of these, polynomial regression, where the relationship between the\u00a0independent variable\u00a0x\u00a0and the\u00a0dependent variable\u00a0y\u00a0is modeled as an\u00a0nth degree\u00a0polynomial\u00a0in\u00a0x. Lets say you want to have a polynomial regression (let's make 2 degree polynomial):\n\n$$y = b + \\theta_1  x + \\theta_2 x^2$$\n\nNow, the question is: how we can fit our data on this equation while we have only x values? \nWell, we can create a few additional features: 1, $x$, and $x^2$.\n\n**PolynomialFeatures()** function in Scikit-learn library, drives a new feature sets from the original feature set. That is, a matrix will be generated consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. \n","4a094c5e":"<font color=\"blue\" >\n    As we can see from the plot above and error reports, SVR performs very good although we have verty few data for training set","40ecf1ab":"From an initial look at the plot, we determine that either the exponential or logistic function could be a good approximation, since it has the property of starting with a slow growth, increasing growth in the middle, and then decreasing again at the end:","65aa8024":"Sigmoid(Logistic):\n$$ Y = a + \\frac{b}{1+ c^{(X-d)}}$$\n\u200b\n","2a537302":"The response $y$ is a results of applying logarithmic map from input $x$'s to output variable $y$. It is one of the simplest form of **log()**: i.e. $$ y = \\log(x)$$\n\nWe can also consider that instead of $x$, we can use $X$, which can be polynomial representation of the $x$'s. In general form it would be written as  \n\\begin{equation}\ny = \\log(X)\n\\end{equation}","b79e12ae":"# 2. Other Non_Linear Regressions:","c1520118":"<font color=\"blue\" >\n    3.1. Visualization of the Data:","99e3421e":"<font color=\"blue\" >\n    1.2. Quadratic Function:","848479dc":"#### How we find the best parameters for our fit line?\n\nwe can use **curve_fit** which uses non-linear least squares to fit our sigmoid function, to data. Optimal values for the parameters so that the sum of the squared residuals of sigmoid(xdata, *popt) - ydata is minimized.\n\npopt are our optimized parameters.","2459c2a7":"<font color=\"blue\" >\n    2.1. Exponential Function:","d1702aca":"<font color=\"blue\" >\n    1.1. Qubic Function:","1aa7d4aa":"<font color=\"blue\" >\n    3.1. Splitting Data as Train and Test Set and Implementing Model:\n    \n    \n    We can also sklearn algorithms on this data set and SVR fit better for non-linear regressions","3c005487":"<font color=\"blue\" >\n    2.2. Logarithmic Function:","d72f6af1":"# 1. Polynomial Regression","4cab528d":"<font color=\"blue\" >\n    2.3. Sigmoid(Logistic) Function:","6a71c89a":"An exponential function with base c is defined by $$ Y = a + b c^X$$ where b \u22600, c > 0 , c \u22601, and x is any real number. The base, c, is constant and the exponent, x, is a variable. \n"}}