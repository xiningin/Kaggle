{"cell_type":{"e9da0602":"code","3871e44d":"code","881799db":"code","0c24fcf9":"code","5149419a":"code","971a0619":"code","a863fedf":"code","1ed69765":"code","7247184b":"code","0672be87":"code","faeadbed":"code","6733e105":"code","3953eb0e":"code","697e0c08":"code","fe2a4ab3":"code","1b3c68cb":"code","3e58da4e":"code","54d0f415":"code","eccec4d6":"markdown","16ba96f2":"markdown","8f0c4760":"markdown","e43c88bd":"markdown","cdda45ed":"markdown","2103ecda":"markdown","ad5f48b1":"markdown","ea0e81f1":"markdown","39cb2274":"markdown","7cada45e":"markdown","9546d1b4":"markdown"},"source":{"e9da0602":"!pip --quiet install ..\/input\/treelite\/treelite-0.93-py3-none-manylinux2010_x86_64.whl","3871e44d":"!pip --quiet install ..\/input\/treelite\/treelite_runtime-0.93-py3-none-manylinux2010_x86_64.whl","881799db":"import numpy as np\nimport pandas as pd\n\nimport os, sys\nimport gc\nimport math\nimport random\nimport pathlib\nfrom tqdm import tqdm\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn import linear_model\nfrom sklearn import metrics\nimport operator\nimport xgboost as xgb\nimport lightgbm as lgb\nimport optuna\nfrom tqdm import tqdm_notebook as tqdm\n\n# treelite\nimport treelite\nimport treelite_runtime \n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib_venn import venn2\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('fivethirtyeight')\npd.options.display.max_columns = None\n\nimport warnings\nwarnings.filterwarnings('ignore')","0c24fcf9":"SEED = 20201225 # Merry Christmas!\nNFOLD = 4\n# INPUT_DIR = '..\/input\/jane-street-market-prediction\/'\nINPUT_DIR = '..\/input\/janestreet-save-as-feather\/'\nTRADING_THRESHOLD = 0.502 # 0 ~ 1: The smaller, the more aggressive\nDATE_BEGIN = 0 # 0 ~ 499: set 0 for model training using the complete data ","5149419a":"os.listdir(INPUT_DIR)","971a0619":"%%time\n\n# load data blitz fast!\ndef load_data(input_dir=INPUT_DIR):\n    train = pd.read_feather(pathlib.Path(input_dir + 'train.feather'))\n    features = pd.read_feather(pathlib.Path(input_dir + 'features.feather'))\n    example_test = pd.read_feather(pathlib.Path(input_dir + 'example_test.feather'))\n    ss = pd.read_feather(pathlib.Path(input_dir + 'example_sample_submission.feather'))\n    return train, features, example_test, ss\n\ntrain, features, example_test, ss = load_data(INPUT_DIR)","a863fedf":"# delete irrelevant files to save memory\ndel features, example_test, ss\ngc.collect()","1ed69765":"# remove weight = 0 for saving memory \noriginal_size = train.shape[0]\ntrain = train.query('weight > 0').reset_index(drop=True)\n\n# use data later than DATE_BEGIN\ntrain = train.query(f'date >= {DATE_BEGIN}')\n\nprint('Train size reduced from {:,} to {:,}.'.format(original_size, train.shape[0]))","7247184b":"# target\ntrain['action'] = train['resp'] * train['weight']\ntrain['action'] = 1 * (train['action'] > 0)","0672be87":"# features to use\nfeats = [f for f in train.columns.values.tolist() if f.startswith('feature')]\nprint('There are {:,} features.'.format(len(feats)))","faeadbed":"import random\nfrom collections import Counter, defaultdict\nfrom sklearn import model_selection\n\n# ---- GroupKFold ----\nclass GroupKFold(object):\n    \"\"\"\n    GroupKFold with random shuffle with a sklearn-like structure\n    \"\"\"\n\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X, y, group):\n        kf = model_selection.KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n        unique_ids = X[group].unique()\n        for fold, (tr_group_idx, va_group_idx) in enumerate(kf.split(unique_ids)):\n            # split group\n            tr_group, va_group = unique_ids[tr_group_idx], unique_ids[va_group_idx]\n            train_idx = np.where(X[group].isin(tr_group))[0]\n            val_idx = np.where(X[group].isin(va_group))[0]\n            yield train_idx, val_idx\n\n# ---- StratifiedGroupKFold ----\nclass StratifiedGroupKFold(object):\n    \"\"\"\n    StratifiedGroupKFold with random shuffle with a sklearn-like structure\n    \"\"\"\n\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X, y, group):\n        labels_num = np.max(y) + 1\n        y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n        y_distr = Counter()\n        groups = X[group].values\n        for label, g in zip(y, groups):\n            y_counts_per_group[g][label] += 1\n            y_distr[label] += 1\n\n        y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n        groups_per_fold = defaultdict(set)\n\n        def eval_y_counts_per_fold(y_counts, fold):\n            y_counts_per_fold[fold] += y_counts\n            std_per_label = []\n            for label in range(labels_num):\n                label_std = np.std([y_counts_per_fold[i][label] \/ y_distr[label] for i in range(self.n_splits)])\n                std_per_label.append(label_std)\n            y_counts_per_fold[fold] -= y_counts\n            return np.mean(std_per_label)\n        \n        groups_and_y_counts = list(y_counts_per_group.items())\n        random.Random(self.random_state).shuffle(groups_and_y_counts)\n\n        for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n            best_fold = None\n            min_eval = None\n            for i in range(self.n_splits):\n                fold_eval = eval_y_counts_per_fold(y_counts, i)\n                if min_eval is None or fold_eval < min_eval:\n                    min_eval = fold_eval\n                    best_fold = i\n            y_counts_per_fold[best_fold] += y_counts\n            groups_per_fold[best_fold].add(g)\n\n        all_groups = set(groups)\n        for i in range(self.n_splits):\n            train_groups = all_groups - groups_per_fold[i]\n            test_groups = groups_per_fold[i]\n\n            train_idx = [i for i, g in enumerate(groups) if g in train_groups]\n            test_idx = [i for i, g in enumerate(groups) if g in test_groups]\n\n            yield train_idx, test_idx","6733e105":"# from https:\/\/www.kaggle.com\/gogo827jz\/jane-street-super-fast-utility-score-function\/notebook\nfrom numba import njit\n\n@njit(fastmath = True)\ndef utility_score_numba(date, weight, resp, action):\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ len(Pi))\n    u = min(max(t, 0), 6) * np.sum(Pi)\n    return u","3953eb0e":"params = {\n    'num_leaves': 256,\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'max_depth': 7,\n    'min_child_weight': 4,\n    'feature_fraction': 0.7,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 4,\n    'min_child_samples': 16,\n    'lambda_l1': 1,\n    'lambda_l2': 1,\n}\n\nth = 0.5\ncv = GroupKFold(n_splits=NFOLD, shuffle=True, random_state=SEED)\ngroup = 'date'\ntarget = 'action'\noof = np.zeros(train.shape[0])\nmodels = []\nfor fold, (train_idx, val_idx) in tqdm(enumerate(cv.split(train, train[target], group))):\n    # train test split\n    x_train, x_val = train[feats].iloc[train_idx], train[feats].iloc[val_idx]\n    y_train, y_val = train[target].iloc[train_idx], train[target].iloc[val_idx]\n\n    # model fitting\n    lgb_train = lgb.Dataset(x_train, y_train)\n    lgb_eval = lgb.Dataset(x_val, y_val)\n    \n    model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_eval], \n                      early_stopping_rounds=40, verbose_eval=1000)\n    oof[val_idx] = model.predict(x_val)\n    models.append(model)\n    \n    # save model\n    model.save_model(f'model_fold{fold}.txt')\n    \n    # score\n    date = train['date'].iloc[val_idx].values\n    weight = train['weight'].iloc[val_idx].values\n    resp = train['resp'].iloc[val_idx].values\n    action = 1 * (oof[val_idx] > th)\n    score = utility_score_numba(date, weight, resp, action)\n    print(f'FOLD {fold}: score = {score}')","697e0c08":"# save oof\nnp.save('oof', oof)","fe2a4ab3":"# score\ndate = train['date'].values\nweight = train['weight'].values\nresp = train['resp'].values\naction = 1 * (oof > th)\nscore = utility_score_numba(date, weight, resp, action)\nprint('CV score = {}'.format(score))","1b3c68cb":"lgb.plot_importance(model, importance_type=\"gain\", figsize=(7, 40))","3e58da4e":"predictors = []\nfor fold in range(len(models)):\n    # load LGB with Treelite\n    model = treelite.Model.load(f'model_fold{fold}.txt', model_format='lightgbm')\n    \n    # generate shared library\n    toolchain = 'gcc'\n    model.export_lib(toolchain=toolchain, libpath=f'.\/mymodel{fold}.so',\n                     params={'parallel_comp': 32}, verbose=True)# predictor from treelite\n\n    # predictors\n    predictor = treelite_runtime.Predictor(f'.\/mymodel{fold}.so', verbose=True)\n    predictors.append(predictor)","54d0f415":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n    \nfor (test_df, pred_df) in tqdm(iter_test):\n    if test_df['weight'].item() > 0:\n        # inference with treelite\n        batch = treelite_runtime.Batch.from_npy2d(test_df[feats].values)\n        pred = np.mean([predictor.predict(batch) for predictor in predictors], axis=0)\n        pred_df.action = (pred > TRADING_THRESHOLD).astype('int')\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","eccec4d6":"# Config\nSome configuration setups.","16ba96f2":"# Preprocess","8f0c4760":"# Load data\nI have already saved the training data in the feather-format in [my another notebook](https:\/\/www.kaggle.com\/code1110\/janestreet-save-as-feather?scriptVersionId=47635784). Loading csv takes time but loading feather is really light:)","e43c88bd":"All done!","cdda45ed":"# Fit with GroupKFold\nI use GroupKFold with shuffle = True.","2103ecda":"# Install Treelite","ad5f48b1":"# Feature importance\nLet's see feature importance given by the model.","ea0e81f1":"<center><h2>Jane Street Market Prediction | LGB with GroupKFold | katsu1110 <\/h2><\/center><hr>\n\n![](https:\/\/optuna.org\/assets\/img\/optuna-logo@2x.png)\n\nThis is my another attempt to get a good LGB model. I start over with a simple model;)\n\nAs a bonus, I save the tuned model in the [Treelite](https:\/\/treelite.readthedocs.io\/en\/latest\/) format to accelerate the inference speed.\n\nThis notebook loads feathered-data from [my another notebook](https:\/\/www.kaggle.com\/code1110\/janestreet-save-as-feather?scriptVersionId=47635784) such that we don't have to spend our time on waiting long for loading csv files.\n\nIn this notebook we treat the task as a binary classification.","39cb2274":"# Submit\nLet's use Treelite for faster inference.","7cada45e":"# CV score","9546d1b4":"# Treelite\nI believe Treelite is must in this competition, to avoid the sumission error due to the long inference time."}}