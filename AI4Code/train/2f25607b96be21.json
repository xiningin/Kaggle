{"cell_type":{"16f26f4b":"code","f6f6e37d":"code","0276de6d":"code","80c8797f":"code","cfe1159b":"code","c615f911":"code","e22fc70d":"code","8d8447b8":"code","22527bfd":"code","b40893b1":"code","8b15f2a3":"code","8104b988":"code","901b05d3":"code","649500b5":"code","41fdf390":"code","081db78c":"markdown","64d99e0f":"markdown","5eda8197":"markdown","0cb6ae75":"markdown","fb3b05ca":"markdown","3e882697":"markdown","3555e919":"markdown","f6cd73f8":"markdown","f839d430":"markdown","23731e46":"markdown","1cfb3a9f":"markdown","4515559a":"markdown","1fa7d7c7":"markdown","33624d05":"markdown","9692fa6b":"markdown","ce6143dc":"markdown"},"source":{"16f26f4b":"import numpy as np \nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom numpy import isnan\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n#from xgboost.sklearn import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV","f6f6e37d":"## TARGET_COL = \"diabetes_mellitus\"\ndf = pd.read_csv(\"\/kaggle\/input\/widsdatathon2021\/TrainingWiDS2021.csv\")\nprint(df.shape)\ntest = pd.read_csv(\"\/kaggle\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\")\nprint(test.shape)","0276de6d":"temp = test[\"encounter_id\"]\n#Remove duplicate rows\ndf.drop_duplicates()\nprint(df.shape)\n'''\n#Remove columns which are not needed\ndf.drop(df.columns[[0,1,2,8,9,11,12,14,15,18,19,20,28,29,43,49,50,57,58,61,62,67,68,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,119,120,149,150]], axis = 1, inplace = True) \ntest.drop(test.columns[[0,1,2,8,9,11,12,14,15,18,19,20,28,29,43,49,50,57,58,61,62,67,68,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,119,120,149,150]], axis = 1, inplace = True)\nprint(df.shape)\nprint(test.shape)\n'''","80c8797f":"#Find categorical columns\ncols = df.columns\nnum_cols = df._get_numeric_data().columns\nlist(set(cols) - set(num_cols))\n\n#one hot encoding for categorical columns\ndf = pd.get_dummies(df, columns=[\"ethnicity\",\"icu_type\",\"gender\",\"icu_admit_source\", \"icu_stay_type\", \"hospital_admit_source\"])\ntest = pd.get_dummies(test, columns=[\"ethnicity\",\"icu_type\",\"gender\",\"icu_admit_source\", \"icu_stay_type\", \"hospital_admit_source\"])\ntest['hospital_admit_source_ICU'] = 0\ntest['hospital_admit_source_Other'] = 0\ntest['hospital_admit_source_PACU'] = 0\ntest['hospital_admit_source_Acute Care\/Floor'] = 0\ntest['hospital_admit_source_Observation'] = 0\nprint(df.shape)\nprint(test.shape)\n","cfe1159b":"## Using SimpleInputer with strategy 'mean'\n#imputer = SimpleImputer(strategy='mean') #roc_auc: 0.824\n#imputer = SimpleImputer(strategy='median') #roc_auc: 0.820\n#imputer = SimpleImputer(strategy='most_frequent') #roc_auc: 0.827\nimputer = SimpleImputer(strategy='constant') #roc_auc: 0.822\n\n# fit on the dataset\ntransformed_values = imputer.fit_transform(df)\ntransformed_values_test = imputer.fit_transform(test)\ncols = df.columns\ntest_cols = test.columns\n\n# print total missing\n#print('Missing: %d' % sum(isnan(transformed_values).flatten()))\n\n# convert array into dataframe \ndf = pd.DataFrame(transformed_values, columns = cols)\ntest = pd.DataFrame(transformed_values_test, columns = test_cols)\nprint(test.shape)","c615f911":"'''\n#Normalization for training data set\nscaler = MinMaxScaler()\nscaler.fit(df)\nnormalized = scaler.transform(df)\nnormalized_df = pd.DataFrame(data = normalized, columns = cols)\nnormalized_df.shape\n\n#Normalization for test data set\nscaler = MinMaxScaler()\nscaler.fit(test)\nnormalized = scaler.transform(test)\ntest = pd.DataFrame(data = normalized, columns = test_cols)\nprint(test.shape)\n'''","e22fc70d":"'''\n# define dataset\ndf_bkp = normalized_df\ntrain_Y = normalized_df['diabetes_mellitus']\ntrain_X = normalized_df.drop(columns=['diabetes_mellitus'])\ntrain_x, validation_x, train_y, validation_y = train_test_split(train_X, train_Y,test_size=0.20)\ntrain_x.shape, validation_x.shape, train_y.shape, validation_y.shape\n'''\n\ntrain_Y = df['diabetes_mellitus']\ntrain_X = df.drop(columns=['diabetes_mellitus'])\ntest = test[train_X.columns]\ntrain_x, validation_x, train_y, validation_y = train_test_split(train_X, train_Y, test_size=0.20)\ntrain_x.shape, validation_x.shape, train_y.shape, validation_y.shape","8d8447b8":"# define the model --- XGBClassifier\nmodel_XGB = XGBClassifier(use_label_encoder=False, eval_metric=\"auc\")\nmodel_XGB.fit(train_x, train_y)\nprint(\"XGBClassifier : On validation set, the ROC AUC score is \",roc_auc_score(validation_y, model_XGB.predict_proba(validation_x)[:,1]))\n\n# get feature importances\nfeature_imp=pd.DataFrame()\nfeature_imp['columns']=train_X.columns\nfeature_imp['importances'] = model_XGB.feature_importances_\nfeature_imp.sort_values(by='importances',ascending=False,inplace=True)\nnonzero = feature_imp['columns'].head(192)\n'''\ncount = 0\nfor index, row in feature_imp.iterrows():\n    if row['importances'] > 0:\n        count+=1\n        \nprint(count)\n'''","22527bfd":"\n#Using features with non-zero importance\ntrain_X = train_X[nonzero]\ndf_top61 = train_X.copy(deep=True)\ndf_top61[\"diabetes_mellitus\"] = train_Y.copy(deep=True)\ntest = test[nonzero]\ntrain_x, validation_x, train_y, validation_y = train_test_split(train_X, train_Y,test_size=0.20)\ntrain_x.shape, validation_x.shape, train_y.shape, validation_y.shape\n\n#model_XGB.fit(train_X, train_Y)\n#print(\"XGBClassifier : On validation set, the ROC AUC score is \",roc_auc_score(validation_y, model_XGB.predict_proba(validation_x)[:,1]))","b40893b1":"def modelfit(alg, dtrain, predictors, train_y, useTrainCV=True, cv_folds=5, early_stopping_rounds=90):\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=train_y.values)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], train_y, eval_metric='auc')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n            \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"Accuracy : %.4g\" % accuracy_score(train_y.values, dtrain_predictions))\n    print (\"AUC Score (Train): %f\" % roc_auc_score(train_y, dtrain_predprob))","8b15f2a3":"predictors = [x for x in train_X.columns]\nxgb1 = XGBClassifier( #Accuracy - 0.8537, AUC - 0.8906\n learning_rate =0.1,\n n_estimators=1200,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27,\n use_label_encoder=False)\n#modelfit(xgb1, train_X, predictors, train_Y)","8104b988":"param_test1 = {\n 'min_child_weight': [1,2,3]\n}\nparam_test2 = {\n 'gamma':[i\/10.0 for i in range(0,5)]\n}\nparam_test3 = {\n 'subsample':[i\/10.0 for i in range(6,10)],\n 'colsample_bytree':[i\/10.0 for i in range(6,10)]\n}\nparam_test4 = {\n 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n}\nparam_test5 = {\n 'reg_alpha':[0, 0.00001, 0.00005, 0.0001]\n}\n#{'colsample_bytree': 0.8, 'subsample': 0.6},0.8410413276440941)\n'''\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=3,\n min_child_weight=2, gamma=0, subsample=0.6, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n param_grid = param_test5, scoring='roc_auc',n_jobs=-1,iid=False, cv=5, refit='AUC', verbose=2)\ngsearch1.fit(train_X,train_Y)\ngsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_\n'''","901b05d3":"#Tuned max_depth to 3 and min_child_weight to 2(giving best_score_ = 0.84293, Accuracy - 0.8452, AUC - 0.8777)\nxgb2 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=3,\n min_child_weight=2,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27,\n use_label_encoder=False)\n#modelfit(xgb2, train_X, predictors)\n\n#Tuned colsample_bytree': 0.8, 'subsample': 0.6, 0.8410413276440941) ## Giving best score on submission, Accuracy - 0.8454, AUC - 8736\nxgb3 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1500,\n max_depth=3,\n min_child_weight=2,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27,\n reg_alpha=0.00001,\n use_label_encoder=False)\nmodelfit(xgb3, train_X, predictors, train_Y)","649500b5":"pseudo_labels = xgb3.predict(test)\naugmented_test = test.copy(deep=True)\naugmented_test[\"diabetes_mellitus\"] = pseudo_labels\naugmented_test.shape\ntrain_new = df_top61.append(augmented_test, ignore_index=True)\n#train_new = df.append(augmented_test, ignore_index=True)\ntrain_new.shape\ntrain_newY = train_new[\"diabetes_mellitus\"]\ntrain_newX = train_new.drop(columns=['diabetes_mellitus'])\ntrain_newX.shape, train_newY.shape\n#train_x, validation_x, train_y, validation_y = train_test_split(train_newX, train_newY,test_size=0.30)\n#train_x.shape, validation_x.shape, train_y.shape, validation_y.shape\npredictors = [x for x in train_newX.columns]\nmodelfit(xgb3, train_newX, predictors, train_newY)\ntrain_newX.shape, train_newY.shape","41fdf390":"test[\"diabetes_mellitus\"] = xgb3.predict_proba(test)[:,1]\n\ntest[\"encounter_id\"] = temp\ntest[[\"encounter_id\",\"diabetes_mellitus\"]].to_csv(\"submission.csv\",index=False)","081db78c":"**Installation**","64d99e0f":"# Submission","5eda8197":"# Data Pre-Processing","0cb6ae75":"**Normalization**","fb3b05ca":"**Identify Important Features**","3e882697":"# Load the Data","3555e919":"**Handling Categorical Columns**","f6cd73f8":"# Pseudo Labelling","f839d430":"**Hyper-parameter Tuning**","23731e46":"**Removing Low Importance Data**","1cfb3a9f":"# XGBClassifier","4515559a":"*Here we are using Diabetes Mellitus data from WiDS Datathon 2021 to determine best suited XGBoost Classifier model*","1fa7d7c7":"**Import Libraries**","33624d05":"**Using Non-Zero Importance Features**","9692fa6b":"**Handling N\/A Values using Imputer**","ce6143dc":"# Defining Training Data"}}