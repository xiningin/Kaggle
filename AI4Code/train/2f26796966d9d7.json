{"cell_type":{"89cee35a":"code","d9ac7736":"code","479f5f81":"code","091dd174":"code","8c80e673":"code","14183647":"code","d668775a":"code","eed94535":"code","0ab34cb8":"code","1ecf62e9":"markdown","5cc8b3a9":"markdown","07c03ab8":"markdown","a618a122":"markdown"},"source":{"89cee35a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\nimport re\nimport scipy\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.models import Word2Vec, KeyedVectors, FastText\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Flatten, SimpleRNN, RNN,GRU, SpatialDropout1D, Dropout\nfrom keras.preprocessing.sequence import pad_sequences\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d9ac7736":"data = pd.read_csv('..\/input\/Text_Similarity_Dataset.csv')\ndata.shape #(4023,3)\n\nX = data.iloc[:,1:].values\ns1 = X[:,0]\ns2 = X[:,1]\n","479f5f81":"tokens1 = []\ntokens2 = []\ntokens1 = [word_tokenize(str(sentence)) for sentence in s1]\ntokens2 = [word_tokenize(str(sentence)) for sentence in s2]\n\nrm1 = []\nfor w in tokens1:\n    sm = re.sub('[^A-Za-z]',' ', str(w))\n    x = re.split(\"\\s\", sm)\n    rm1.append(x)\n    \nrm2 = []\nfor w in tokens2:\n    sm2 = re.sub('[^A-Za-z]',' ',str(w))\n    x2 = re.split(\"\/s\",sm2)\n    rm2.append(x2)\n","091dd174":"#Removing whitespaces    \nfor sent in rm1:\n    while '' in sent:\n        sent.remove('')\n\nfor sent in rm2:\n    while '' in sent:\n        sent.remove('')\n        \n# Lowercasing\nlow1 = []\nfor i in rm1:\n    i = [x.lower() for x in i]\n    low1.append(i)\n\nlow2 = []\nfor i in rm2:\n    i = [x.lower() for x in i]\n    low2.append(i)\n","8c80e673":"#Lemmatozation\nlemma1 = []\nwnl = WordNetLemmatizer()\nfor sent in low1:\n    tokens = [wnl.lemmatize(w) for w in sent]\n    lemma1.append(tokens)\n    \nlemma2 = []\nfor sent in low2:\n    tok = [wnl.lemmatize(se) for se in sent]\n    lemma2.append(tok)\n    \n# Removing Stopwords\nfilter_words1 = []\nStopwords = set(stopwords.words('english'))\n\nfor sent in lemma1:\n    tokens = [w for w in sent if w not in Stopwords]\n    filter_words1.append(tokens)\n    \nfilter_words2 = []\nfor sent in lemma2:\n    tokens2 = [w for w in sent if w not in Stopwords]\n    filter_words2.append(tokens2)\n","14183647":"model = FastText('https:\/\/s3.amazonaws.com\/dl4j-distribution\/GoogleNews-vectors-negative300.bin.gz')\nword_vectors = model.wv\nvocabulary = word_vectors.vocab.items()\nsimilarity = word_vectors.similarity('woman', 'man')\n\ndef cosine_similarity_bw_two_words(word1,word2):\n    return word_vectors.similarity(word1,word2)\n","d668775a":"result = []\nfor sent1, sent2 in zip(filter_words1,filter_words2):\n    vector1 = np.mean([model[word] for word in sent1], axis = 0)\n    vector2 = np.mean([model[word] for word in sent2], axis = 0)\n    cosine = scipy.spatial.distance.cosine(vector1, vector2)\n    result.append((1-cosine)*100)\n    \ndata['Result[%]'] = result","eed94535":"data","0ab34cb8":"data.sort_values([\"Result[%]\"], axis=0, \n                 ascending=False) ","1ecf62e9":"The data contains a pair of paragraphs. These text paragraphs are randomly sampled from a raw dataset. Each pair of the sentence may or may not be semantically similar. The dataset considered for this project does not contain any labels. Given below is the solution to the unsupervised machine learning problem","5cc8b3a9":"## Cosine Similarity\nCosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. The cosine of 0\u00b0 is 1, and it is less than 1 for any angle in the interval (0, \u03c0] radians.\n","07c03ab8":"# Preprocessing of Data","a618a122":"# FastText\nFastText is an extension to Word2Vec proposed by Facebook in 2016. Instead of feeding individual words into the Neural Network, FastText breaks words into several n-grams (sub-words). For instance, the tri-grams for the word apple is app, ppl, and ple (ignoring the starting and ending of boundaries of words). The word embedding vector for apple will be the sum of all these n-grams. After training the Neural Network, we will have word embeddings for all the n-grams given the training dataset. Rare words can now be properly represented since it is highly likely that some of their n-grams also appears in other words.\n"}}