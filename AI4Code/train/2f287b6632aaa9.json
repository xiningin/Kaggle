{"cell_type":{"513e9976":"code","6a5de609":"code","f3106bd4":"code","1e425b91":"code","e6f435fb":"code","29784e27":"code","0b4dc143":"code","93aca5a9":"code","22ca61dc":"code","ef5e6074":"code","01d26104":"code","5e0ce93a":"code","dff7618f":"markdown","5b0ba194":"markdown","fb1c0894":"markdown","1b510233":"markdown","1fa607b6":"markdown","816e52f0":"markdown","9a471d98":"markdown","912f5263":"markdown","276b6fa4":"markdown","2f9886e0":"markdown","643d9a08":"markdown","fd39c9ae":"markdown","20b1d276":"markdown","0203658b":"markdown","e7f2c8be":"markdown","d9a24038":"markdown","1cab8081":"markdown","d74f2de0":"markdown","c1fbffca":"markdown","a465b188":"markdown","d9c28ab1":"markdown","84120606":"markdown"},"source":{"513e9976":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(context=\"notebook\", palette=\"Spectral\", style = 'darkgrid' ,font_scale = 1.5, color_codes=True)\nprint(os.listdir(\"..\/input\"))","6a5de609":"# Importing the dataset\ndataset = pd.read_csv('..\/input\/Mall_Customers.csv',index_col='CustomerID')","f3106bd4":"dataset.head()","1e425b91":"dataset.info()","e6f435fb":"dataset.describe()","29784e27":"dataset.isnull().sum()","0b4dc143":"dataset.drop_duplicates(inplace=True)","93aca5a9":"# using only Spending_Score and income variable for easy visualisation\nX = dataset.iloc[:, [2, 3]].values","22ca61dc":"# Using the elbow method to find the optimal number of clusters\nfrom sklearn.cluster import KMeans\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(X)\n    # inertia method returns wcss for that model\n    wcss.append(kmeans.inertia_)","ef5e6074":"plt.figure(figsize=(10,5))\nsns.lineplot(range(1, 11), wcss,marker='o',color='red')\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()\n","01d26104":"# Fitting K-Means to the dataset\nkmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)\ny_kmeans = kmeans.fit_predict(X)","5e0ce93a":"# Visualising the clusters\nplt.figure(figsize=(15,7))\nsns.scatterplot(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], color = 'yellow', label = 'Cluster 1',s=50)\nsns.scatterplot(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], color = 'blue', label = 'Cluster 2',s=50)\nsns.scatterplot(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], color = 'green', label = 'Cluster 3',s=50)\nsns.scatterplot(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], color = 'grey', label = 'Cluster 4',s=50)\nsns.scatterplot(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], color = 'orange', label = 'Cluster 5',s=50)\nsns.scatterplot(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color = 'red', \n                label = 'Centroids',s=300,marker=',')\nplt.grid(False)\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()","dff7618f":"Big Thanks to:\n\n* https:\/\/www.datascience.com\/blog\/k-means-clustering\n* https:\/\/www.superdatascience.com\n* https:\/\/www.udemy.com\n","5b0ba194":"## Step Summary\n### To summarise the steps we can say :\n![](https:\/\/i.imgur.com\/3jTk7Y0.png)","fb1c0894":"<a id=\"5\"><\/a>\n## 5. Centroid Random Initialisation Trap\n\nThrough these images let's see how two different random initialisations can cause a totally different outcome.\n\n### Init 1\n\n\n\n![](https:\/\/i.imgur.com\/zsC9z0z.png)\n\n\n\n\n### Init 2\n\n\n\n![](https:\/\/i.imgur.com\/kU5BX6j.png)","1b510233":"So we saw that even with clear distinction possible visually, wrong randomisation can produce wrong results.\nThere have been researches carried out and one of the most famous ways to initialise centroids is KMeans++.\nThe best thing is that the whole algorithm remains the same but the only difference is that we provide an argument to SKlearn to use KMeans++ for initialisation. There are many papers explaining the KMeans++ but the explanation is beyond this notebook for now. :)","1fa607b6":"<a id=\"3\"><\/a>\n## 3. Working\n\nLet's now discuss the working of KMeans algorithm. The aim is to break the explanation down in the simplest way possible. \n\n\n#### It begins with choosing the number of K clusters. The K signifies the number of clusters that the algorithm would find in the dataset. Now choosing the right K is very important. Sometimes the K is clearly visible from the dataset when visualized. However most of the times this is not the case and in a short time we'll see about how to choose the right K value.\n\n\n\n![](https:\/\/i.imgur.com\/RBK4dtA.png)\n","816e52f0":"![](https:\/\/cdn-images-1.medium.com\/max\/1200\/1*x7P7gqjo8k2_bj2rTQWAfg.jpeg)","9a471d98":"<a id=\"7\"><\/a>\n## 7. Visualisation","912f5263":"<a id=\"6\"><\/a>\n## 6. Implementation","276b6fa4":"#### The second step is to allocate K random points as centroids. These K points could be points from the dataset or outside. There's one thing to note however. The random initialisation of centroids can sometimes cause random initialisation trap which we would see in this section soon.\n\n![](https:\/\/i.imgur.com\/LfI2qfl.png)","2f9886e0":"### *Table of content*\n\n[1. What does KMeans do?](#1)\n\n\n[2. Applications](#2)\n\n\n[3. Working](#3)\n\n\n[4. Choosing the right K](#4)\n\n\n\n[5. Centroid Random Initialisation Trap](#5)\n\n\n\n[6. Implemention](#6)\n\n\n\n[7. Visualisation](#7)","643d9a08":"#### The fifth step is to reassign points like we did in step 3. If reassignment takes place then we need to go back to step four. If no reassignment takes place then we can say that our model has converged and its ready.\n\n\n\n\n![](https:\/\/i.imgur.com\/aRaGcKB.png)","fd39c9ae":"![](http:\/\/cdn-images-1.medium.com\/max\/2400\/1*PRC6tdXpTekQ6X7qdUDehg.jpeg)","20b1d276":"No Nans found! Great","0203658b":"#### The fourth step is to calculate the centroid of the individual clusters and place the old centroid there.\n\n\n\n\n![](https:\/\/i.imgur.com\/FyIeKuA.png)","e7f2c8be":"The Elbow Method is then used to choose the best K value. In the depiction below we can see that after 3 there's no significant decrease in WCSS so 3 is the best here. Therefore there's an elbow shape that forms and it is usually a good idea to pick the number where this elbow is formed. There would be many times when the graph wouldn't be this intuitive but with practice it becomes easier.\n\n![](https:\/\/i.imgur.com\/gi9p7V5.png)","d9a24038":"## Super Data Science and Udemy! Big Thanks! \n\n![](https:\/\/preview.redd.it\/o4mshdf4hui01.jpg?width=750&auto=webp&s=2d647d5d30a1f1b7507411929ff077e1df967e00)","1cab8081":"<a id=\"1\"><\/a>\n## 1. What does KMeans do?\n\nK-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity. The results of the K-means clustering algorithm are:\n\n* The centroids of the K clusters, which can be used to label new data\n* Labels for the training data (each data point is assigned to a single cluster)","d74f2de0":"![](https:\/\/i.imgur.com\/rwkQNbv.png)","c1fbffca":"#### In the third step the dataset points would be allocated to the centroid which is closest to them.\n\n\n\n![](https:\/\/i.imgur.com\/9I5JH3m.png)\n","a465b188":"![](https:\/\/imgur.com\/a\/wVDJPuZ)","d9c28ab1":"<a id=\"2\"><\/a>\n## 2. Applications\nThe K-means clustering algorithm is used to find groups which have not been explicitly labeled in the data. This can be used to confirm business assumptions about what types of groups exist or to identify unknown groups in complex data sets. Once the algorithm has been run and the groups are defined, any new data can be easily assigned to the correct group.\n\nThis is a versatile algorithm that can be used for any type of grouping. Some examples of use cases are:\n\n* Behavioral segmentation:\n* * Segment by purchase history\n* * Segment by activities on application, website, or platform\n* * Define personas based on interests\n* * Create profiles based on activity monitoring\n* Inventory categorization:\n* * Group inventory by sales activity\n* * Group inventory by manufacturing metrics\n* Sorting sensor measurements:\n* * Detect activity types in motion sensors\n* * Group images\n* * Separate audio\n* * Identify groups in health monitoring\n* Detecting bots or anomalies:\n* * Separate valid activity groups from bots","84120606":"<a id=\"4\"><\/a>\n## 4. Choosing the right K\n\nThe way to evaluate the choice of K is made using a parameter known as WCSS. WCSS stands for **Within Cluster Sum of Squares**.\nIt should be low. Here's the formula representation for example when K = 3\n\nSummation Distance(p,c) is the sum of distance of points in a cluster from the centroid.\n\n\n![](https:\/\/i.imgur.com\/5W63xul.png)"}}