{"cell_type":{"f8e6b4bb":"code","ad95a5b3":"code","f0ecb4a3":"code","8549f2b3":"code","e71c0a9d":"code","a978a493":"code","139d43fc":"code","16101386":"code","36bccdde":"code","3ddd31e4":"code","1f1b6d38":"code","0ecd5029":"code","6a2615de":"code","700b790f":"code","a80933ca":"code","bc270d78":"code","7433c169":"code","f83c2c7e":"code","a676b317":"code","92874382":"code","177698fa":"code","91db4055":"code","3f8832d0":"code","9c1b1f8f":"code","7313ab4f":"code","6628ff42":"code","3eadd4a4":"code","45416ee7":"code","cf6f4711":"code","26029b04":"code","1b0282dd":"code","bfc7a982":"code","80ee78fc":"code","3bc763c3":"code","0b99c898":"code","07467857":"code","475ecda5":"code","2b298797":"code","3873a6ea":"code","502bfd56":"code","4cff32f7":"code","987c05f5":"code","f9d85283":"code","dee170d0":"code","6e119f53":"code","453a7564":"code","09c2ca5c":"code","d183ff1f":"code","1e2623ed":"code","84818146":"code","52267eb2":"code","2a2c3cf4":"code","c4b32887":"code","c5dfd17b":"code","740a7f54":"code","f4217587":"code","31b14223":"code","9fa20b8b":"code","af969794":"code","d1e98362":"code","3197d608":"code","8b5b6aa5":"code","bbb24f6d":"code","fe1050e4":"code","86a3fac8":"markdown","8b39a8d9":"markdown","0e89c258":"markdown","fcc15410":"markdown","00dcb152":"markdown","2f4408f9":"markdown","01fc809d":"markdown","3ccb5c73":"markdown","165582bb":"markdown","9f75e166":"markdown","4b173e14":"markdown","64eb132d":"markdown","27e8fd76":"markdown","2d5eeb1c":"markdown","8d86fdf6":"markdown","fd2d0f58":"markdown","34e62041":"markdown","6c322450":"markdown","7fdf8a5c":"markdown","c1018589":"markdown","705e1ed9":"markdown","52a78d24":"markdown","8d1011af":"markdown","51b1c6ab":"markdown","1a491cce":"markdown","cfe395a1":"markdown","15a7a30e":"markdown","645f98de":"markdown","43f70142":"markdown","0392e66e":"markdown","e7884421":"markdown","12e6e339":"markdown","62351cce":"markdown","52ceb7b6":"markdown","6b845a56":"markdown","6c666906":"markdown","059c0685":"markdown","bfa18fe7":"markdown","987da72d":"markdown","4fc291c7":"markdown","3d30c8bb":"markdown","75b27adc":"markdown","c55caaec":"markdown","bdfcfd6d":"markdown","ee4cb987":"markdown","966add38":"markdown","348766dc":"markdown","2121444a":"markdown","7d9d02e0":"markdown","bdf748c8":"markdown"},"source":{"f8e6b4bb":"! pip install imblearn","ad95a5b3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f0ecb4a3":"from imblearn.over_sampling import SMOTE","8549f2b3":"df=pd.read_csv('..\/input\/fetal-health-classification\/fetal_health.csv')","e71c0a9d":"df.head()","a978a493":"df.shape","139d43fc":"df.isnull().sum().sum()","16101386":"df.info()","36bccdde":"df.fetal_health.value_counts()","3ddd31e4":"sns.countplot(x='fetal_health',data=df)","1f1b6d38":"df2=df.copy(deep=True)\npie1=pd.DataFrame(df2['fetal_health'].replace(1.0,'Normal').replace(2.0,'Suspect').replace(3.0,'Pathological').value_counts())\npie1.reset_index(inplace=True)\npie1.plot(kind='pie', title='Pie chart of fetal health',y = 'fetal_health', \n          autopct='%1.1f%%', shadow=False, labels=pie1['index'], legend = False, fontsize=14, figsize=(12,12))","0ecd5029":"df.iloc[:,:-1].hist(figsize=[20,25], layout=[7,3])","6a2615de":"print('Number of unique values in light_decelerations feature:',len(df.light_decelerations.unique()))\nprint('Number of unique values in prolongued_decelerations feature:',len(df.prolongued_decelerations.unique()))\nprint('Number of unique values in severe_decelerations feature:',len(df.severe_decelerations.unique()))\nprint('Number of unique values in histogram_number_of_zeroes feature:',len(df.histogram_number_of_zeroes.unique()))\nprint('Number of unique values in histogram_tendency feature:',len(df.histogram_tendency.unique()))","700b790f":"df.iloc[:,:-1].describe().T","a80933ca":"df[['fetal_movement', 'histogram_number_of_zeroes', 'histogram_variance', 'light_decelerations',\n   'mean_value_of_long_term_variability','mean_value_of_short_term_variability','percentage_of_time_with_abnormal_long_term_variability',\n   'prolongued_decelerations','severe_decelerations']].describe().T","bc270d78":"plt.figure(figsize=(25,35))\ni=1\nfor feat in df.iloc[:,:-1].columns:\n    plt.subplot(7,3,i)\n    sns.boxplot(x='fetal_health',y=feat,data=df)\n    i+=1","7433c169":"plt.figure(figsize=(5, 12))\nheatmap = sns.heatmap(df.corr()[['fetal_health']].sort_values(by='fetal_health', ascending=True), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features Correlating with Fetal Health', fontdict={'fontsize':18}, pad=22)\nheatmap.set_ylim([0,22])","f83c2c7e":"features = df.iloc[:,:-1]\nlabel=df['fetal_health']","a676b317":"features.shape, label.shape","92874382":"from sklearn.preprocessing import PolynomialFeatures","177698fa":"pf = PolynomialFeatures(degree=2, include_bias=False)\ndf3 = pf.fit_transform(features)","91db4055":"df3.shape","3f8832d0":"target_feature_names = ['x'.join(['{}^{}'.format(pair[0],pair[1]) for pair in tuple if pair[1]!=0]) for tuple in [zip(features.columns,p) for p in pf.powers_]]\noutput_df = pd.DataFrame(df3, columns = target_feature_names)","9c1b1f8f":"output_df.head()","7313ab4f":"sm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(output_df, label)","6628ff42":"X_res.shape","3eadd4a4":"y_res.value_counts()","45416ee7":"from sklearn.model_selection import train_test_split","cf6f4711":"X_train, X_test, y_train, y_test = train_test_split(X_res, y_res , random_state=42, test_size = 0.3)","26029b04":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import label_binarize","1b0282dd":"from sklearn.preprocessing import StandardScaler\ns = StandardScaler()","bfc7a982":"X_train_s = s.fit_transform(X_train)\nX_test_s = s.transform(X_test)","80ee78fc":"from sklearn.linear_model import LogisticRegressionCV\n\nlr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2', solver='liblinear').fit(X_train_s, y_train)","3bc763c3":"pd.DataFrame(lr_l2.coef_.T, columns=[0,1,2])","0b99c898":"y_pred_lr=lr_l2.predict(X_test_s).T\npd.DataFrame(y_pred_lr,columns=['Class predicted']).head(10)","07467857":"pd.DataFrame(lr_l2.predict_proba(X_test_s),columns=['1','2','3']).head(10)","475ecda5":"print(classification_report(y_test,y_pred_lr))","2b298797":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nsvm_model=SVC(kernel='rbf', decision_function_shape='ovr', probability=True)\ntuned_parameters = {'gamma': [0.01,0.1,1,10],'C':[0.01,0.1,1,10]}\n\nmodel_svm = GridSearchCV(svm_model, tuned_parameters,cv=4,scoring='accuracy')\nmodel_svm.fit(X_train_s, y_train)","3873a6ea":"print(model_svm.best_estimator_)","502bfd56":"print(accuracy_score(model_svm.predict(X_test_s),y_test))","4cff32f7":"svc= SVC(kernel='rbf',C=10,gamma=0.01,decision_function_shape='ovr',probability=True)\nsvc.fit(X_train_s,y_train)\ny_pred_svm=svc.predict(X_test_s)","987c05f5":"y_pred_svm=svc.predict(X_test_s).T\npd.DataFrame(y_pred_svm,columns=['Class predicted']).head(10)","f9d85283":"pd.DataFrame(svc.predict_proba(X_test_s),columns=['1','2','3']).head(10)","dee170d0":"print(classification_report(y_test,y_pred_svm))","6e119f53":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)","453a7564":"from sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(oob_score=True,\n                            random_state=42,\n                            warm_start=True,\n                            n_jobs=-1)\noob_list = list()\nfor n_trees in [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]:\n    RF.set_params(n_estimators=n_trees)\n    RF.fit(X_train_s, y_train)\n    oob_error = 1 - RF.oob_score_\n    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n\nrf_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')\nrf_oob_df","09c2ca5c":"sns.set_context('talk')\nsns.set_style('white')\n\nax = rf_oob_df.plot(legend=False, marker='o', figsize=(14, 7), linewidth=5)\nax.set(ylabel='out-of-bag error');","d183ff1f":"RF_300 = RandomForestClassifier(n_estimators=300\n          ,oob_score=True \n          ,random_state=42\n          ,n_jobs=-1)\n\nRF_300.fit(X_train_s,y_train)\noob_error300 = 1 - RF_300.oob_score_\noob_error300","1e2623ed":"y_pred_rf=RF_300.predict(X_test_s)","84818146":"y_pred_rf=RF_300.predict(X_test_s).T\npd.DataFrame(y_pred_rf,columns=['Class predicted']).head(10)","52267eb2":"pd.DataFrame(RF_300.predict_proba(X_test_s),columns=['1','2','3']).head(10)","2a2c3cf4":"print(classification_report(y_test,y_pred_rf))","c4b32887":"from sklearn.ensemble import VotingClassifier\n\n# The combined model: logistic regression, SVC and Random Forest\nestimators = [('LR_L2', lr_l2), ('SVM', svc), ('RF', RF_300)]\n\nVC = VotingClassifier(estimators, voting='soft')\nVC = VC.fit(X_train_s, y_train)","c5dfd17b":"y_pred_VC = VC.predict(X_test_s)\nprint(classification_report(y_test, y_pred_VC))","740a7f54":"metrics = list()\nmodels = ['Logistic Regression', 'Support Vector Classifier', 'Random Forest', 'Voting Classifier']\npredictions=[y_pred_lr, y_pred_svm, y_pred_rf, y_pred_VC]\n\nfor lab,i in zip(models, predictions):\n    precision, recall, fscore, _ = score(y_test, i, average='weighted')\n    accuracy = accuracy_score(y_test, i)\n    auc = roc_auc_score(label_binarize(y_test, classes=[1,2,3]),\n                        label_binarize(i, classes=[1,2,3]),\n                        average='weighted')\n    metrics.append(pd.Series({'precision':precision, 'recall':recall,\n                              'fscore':fscore, 'accuracy':accuracy,\n                              'auc':auc}, name=lab))\n    \nmetrics = pd.concat(metrics, axis=1)","f4217587":"metrics","31b14223":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","9fa20b8b":"disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred_rf), display_labels=RF_300.classes_)\ndisp.plot(cmap='Blues')","af969794":"from sklearn.metrics import roc_curve, precision_recall_curve\nsns.set_context('talk')","d1e98362":"y_prob = RF_300.predict_proba(X_test_s)","3197d608":"y_test_b=label_binarize(y_test, classes=[1,2,3])","8b5b6aa5":"from itertools import cycle\nfrom sklearn.metrics import auc\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\nn_class = 3\nlw = 2\nplt.figure(figsize=(10,8))\n\nfor i in range(n_class):\n    fpr[i], tpr[i], _ = roc_curve(y_test_b[:, i], y_prob[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    \ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(n_class), colors):\n    plt.plot(fpr[i], tpr[i], color=color,lw=lw,label='ROC curve of class {0} (area = {1:0.4f})'\n             ''.format(i, roc_auc[i]))\n    \nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Fetal Health Classification')\nplt.legend(loc='best')\nplt.show()","bbb24f6d":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nlines = []\nlabels = []\nplt.figure(figsize=(10,8))\n\nfor i in range(n_class):\n    precision[i], recall[i], _ = precision_recall_curve(y_test_b[:, i],y_prob[:, i])\n    average_precision[i] = average_precision_score(y_test_b[:, i], y_prob[:, i])\n\nfor i, color in zip(range(n_class), colors):\n    plt.plot(recall[i], precision[i], color=color, lw=2, label='Precision-recall for class {0} (area = {1:0.4f})'\n             ''.format(i, average_precision[i]))\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Fetal Health Classification')\nplt.legend(loc='lower left')\nplt.show()","fe1050e4":"feat=pd.DataFrame(RF_300.feature_importances_,index=X_res.columns, columns=['Importance']).sort_values(by='Importance',ascending=False).head(15)\nax=feat.plot(kind='bar', figsize=(16,6))\nax.set(ylabel='Feature Importance')\nax.set(xlabel='Features')","86a3fac8":"Plotting a pie chart with the apropriate name of the classes:","8b39a8d9":"## Logistic Regression","0e89c258":"## Support Vector Classifier","fcc15410":"# Feature Engineering ","00dcb152":"## Plottings of the best model","2f4408f9":"## Random Forest ","01fc809d":"Printing the probabilities that the instances belong to each one of the classes for SVC:","3ccb5c73":"As we see above all features even the label are numerical, therefore processing of categorical variables is something that will not be done in this project, we will only explore the distribution and meaning of each numerical. ","165582bb":"We can see our model has a good performance, but let's compare this with the next models:","9f75e166":"Until now Random Forest has the best performance, but finally if we combine the three models built using Voting Classifier we can think the performance could increase or at least reduce in variance:   ","4b173e14":"$ Features= 2n + \\sum \\limits _{j=1} ^{n-1} i $  \n\n$ Features= 2n + \\frac{n(n-1)}{2}$","64eb132d":"SVC performed a bit better than Logistic Regression.","27e8fd76":"The following is a summary of the 9 most skewed features:","2d5eeb1c":"Statistical summary of features using 'describe table':","8d86fdf6":"As we know in tree-based models is not needed to scale the features nor encoding, but these are already engineered and in order to compare the performance of models under certain context the random forest will be trained in the same way as the prior models.","fd2d0f58":"The model has misclassified some instances which belong to class 1 and 2, this is why precision and recall for these classes had values of 97%, but overall it has classified correctly the huge majority.","34e62041":"## Voting Classifier ","6c322450":"Printing the probabilities that the instances belong to each one of the classes for Logistic regression:","7fdf8a5c":"We can see our label as a continuous variable because as the number increases in magnitude is more likely that the fetal would have a health problem, therefore we could correlate this with the features and interpret a positive pearson correlation as a feature with direct proportion to a health problem.  \nNow based on this assumption let's make a heat map showing the pearson correlation of each feature to the label:","c1018589":"Looking carefully to every histogram we could say that at least 8 features are extremely skewed and contain a significant amount of outliers, giving us the idea that these could be scaled using the technique 'robust scaling', but all of these values are correct and confirmed by the publisher of the dataset.  \nAlso we can see that the features had already been processed because some of them were created by binning or encoding categorical ordinal variables, such as: Light_decelerations, prolongued_decelerations, severe_decelerations, histogram_number_of_zeroes and histogram_tendency, which contain a specific number of possible values. Even the label was encoded too in this process this is why we had numbers instead of the apropriate name of the classes.  \nAbout all others which were not mentioned above correspond to numerical continuous features, some of these are already standardized whereas others not yet. In order to assure a flawless performance of the classifiers models we will scale every feature by standardization.","705e1ed9":"Despite the fact that no features has a strong correlation with the label, we can have an idea of how each one impacts the outcome.","52a78d24":"As all correlations computed were a bit low we could create polynomial features to obtain relationships between them which will expand the information given to the predictive model. Considering a second degree function any of the following formulas will give us the total number of features in our dataset omitting the bias component:","8d1011af":"The effect of balancing the classes in our label (oversampling) was evident at avoiding a tendency towards predicting more the class 0 and also allowed us to compare much better the classification in the confusion matrix by having approximately the same proportion of them. Another highlight is that as it corresponds to a tree-based model we could have simply used features without encoding nor scaling which makes the building of this much easier and fast, whereas polynomial transformation had a significant and worthy effect and as we will see in the plot of the feature importances below the two biggest predictors were created in this process, this is a clear evidence which supports the use of polynomials in the training of models.","51b1c6ab":"This model had the same performance setting voting as soft and hard, and is similar to the SVC. Finally in order to compare the error metrics of every model let's summarize in a table all of them.  \nNote: As our label is multiclass the average of the following metrics was computed: Precision, recall , f1-score and Area under the curve.","1a491cce":"Following we can see the 5 features mentioned and how many unique values each one contain:","cfe395a1":"Finally, although the label was finally balanced it is always recommended that our original dataset have a vast amount of records for each class implying a good representation of the population making our prediction more accurate and \u201creliable\u201d, if we look at the original dataset and count the number of records of the less frequent class which surprisingly corresponds to \u2018pathological\u2019 is only 176!, clearly could be better and more representative to have at least 1000 of them, making us balance the label by undersampling which results better than extrapolate records from existing ones.","15a7a30e":"Checking if there are some null values in the entire dataframe:","645f98de":"# Modeling","43f70142":"Let's see the distribution of the label:","0392e66e":"We can see when the number of trees is around 300 the model has the lowest error, thus we will build a new model with this characteristics:","e7884421":"Some of these box plots look like the IQR is almost null, but this is because there are a few unique values in each feature, which is product of binning and encoding ordinal categorical variables.","12e6e339":"Classification report showing all measures of precision, recall, f1 score and accuracy:","62351cce":"The last step of feature engineering is the oversampling process, because we have an unbalanced label the prediction will tend to have a bias towards the most frequent class, which clearly is not good, so SMOTE will be used to have the same number of instances per class.","52ceb7b6":"Print the best estimators 'hyperparameters found by grid search':","6b845a56":"The following will keep the name of each column, which is crucial when we wanted to see see the importance and impact of each one in the prediction:","6c666906":"Plotting ROC Curve and Precision-Recall Curve:","059c0685":"Let's see the histogram of each feature:","bfa18fe7":"Before building the different models let's declare some error metrics in order to compare the performace of each one:","987da72d":"The following models will be built and compared using their corresponding error measurements:  \n1. Logistic Regression with Ridge 'L2' regularization.\n2. SVC with RBF kernel.\n3. Random Forest with the best number of trees.\n4. Voting Classifier combining the three models.","4fc291c7":"Confirming non null values in each feature and their corresponding data type:","3d30c8bb":"We can see below the 3 sets of coefficients generated by the model.","75b27adc":"The number of trees will be selected by computing the 'out of bag error' of models with number of trees from 15 until 400, plotting their corresponding error and warm_start will be set to True to just add more trees to the existing ones reducing execution time.","c55caaec":"Firstly, grid search will be used to find the best hyperparameters for the model, as we have multiclass in our label the decision_function_shape is set to 'ovr' which stands for 'one versus rest' and probability to True in order to obtain the probability that every instance belongs to each class.","bdfcfd6d":"# Main Objectives\nThe scope of this project is to build several machine learning algorithms which can predict and classify the health of the fetus with the best accuracy possible. This can be broken down into the following milestones:  \n1. Data Cleaning, Exploration and Feature Engineering.\n2. Modeling.\n3. Selection of best model.  \n\nThe best model built could benefit the medical personnel in the task of automating the diagnosis of fetus and maternal health given the information gathered by the exam saving time, budget, also help in the search of the most impactful metrics or those most correlated to any pathology and finally in the aim to early detect diseases in both patients.","ee4cb987":"We can see above that all models had outstanding performances, even the accuracy of the worst is almost 96% which is not far than 97.5% of the best one, however as we are dealing with a medical environment and the health of patients is the most important the recommended model is Random Forest due to its highest metrics, relatively fast training and easy interpretability. From here we will compute all metrics and plots related to our chosen model.  \nLet's plot the confusion matrix:","966add38":"Building a new model with those hyperparameters:","348766dc":"Printing the class predicted for each instance and then the probabilities:","2121444a":"Let's see the box plot of each feature related to the label:","7d9d02e0":"This process will increase more than 10 times our features, specifically to 252 which at the same time will increase complexity and inaccuracy by curse of dimensionality, but let's evaluate the performance of models with and without these extra features.","bdf748c8":"Printing the class predicted for each instance:"}}