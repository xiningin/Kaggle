{"cell_type":{"82d65305":"code","3c094fdb":"code","2a1b532e":"code","f782b321":"code","80dd24f6":"code","be3803c1":"code","59199e66":"code","534e91b7":"code","15b9a290":"code","f5687724":"code","cafd7116":"code","c16414a2":"code","9867fc39":"code","0be883d1":"code","4e2fa4f8":"code","a4b021b6":"code","fa1f54fc":"code","ba244686":"code","e25a2718":"code","87f9ee4d":"markdown","fe419216":"markdown","55405111":"markdown","1eae597f":"markdown","e3c6902e":"markdown","9cba8364":"markdown","56b47fdf":"markdown","7287991e":"markdown","3063c57a":"markdown","b4823537":"markdown","f92e4890":"markdown","56c6758a":"markdown","8638c759":"markdown","e2fa5c0f":"markdown","4e591c2b":"markdown","23330963":"markdown","683994cd":"markdown","604fc98a":"markdown","3f0bc80f":"markdown","03930930":"markdown","08049522":"markdown","90744007":"markdown","f303ac39":"markdown"},"source":{"82d65305":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/eval.csv\")","3c094fdb":"train_data.head()","2a1b532e":"train_data.shape","f782b321":"print(\"Columns:\")\ntrain_data.dtypes","80dd24f6":"train_data.no_descriptors.value_counts()","be3803c1":"print(\"ESRB ratings of games that had \\\"No content descriptors\\\" = 0:\")\nprint(train_data[train_data.no_descriptors == 0].esrb_rating.value_counts())\n\nprint(\"\\nESRB ratings of games that had \\\"No content descriptors\\\" = 1:\")\nprint(train_data[train_data.no_descriptors == 1].esrb_rating.value_counts())","59199e66":"sns.displot(train_data.alcohol_reference)\nsns.displot(train_data.animated_blood)\nsns.displot(train_data.blood)\nsns.displot(train_data.blood_and_gore)\nsns.displot(train_data.cartoon_violence)\nsns.displot(train_data.crude_humor)\nsns.displot(train_data.drug_reference)\nsns.displot(train_data.fantasy_violence)\nsns.displot(train_data.intense_violence)\nsns.displot(train_data.language)\nsns.displot(train_data.lyrics)\nsns.displot(train_data.mature_humor)\nsns.displot(train_data.mild_blood)\nsns.displot(train_data.mild_cartoon_violence)\nsns.displot(train_data.mild_fantasy_violence)\nsns.displot(train_data.mild_language)\nsns.displot(train_data.mild_lyrics)\nsns.displot(train_data.mild_suggestive_themes)\nsns.displot(train_data.mild_violence)\nsns.displot(train_data.no_descriptors)\nsns.displot(train_data.nudity)\nsns.displot(train_data.partial_nudity)\nsns.displot(train_data.sexual_content)\nsns.displot(train_data.sexual_themes)\nsns.displot(train_data.simulated_gambling)\nsns.displot(train_data.strong_janguage)\nsns.displot(train_data.strong_sexual_content)\nsns.displot(train_data.use_of_alcohol)\nsns.displot(train_data.use_of_drugs_and_alcohol)\nsns.displot(train_data.violence)","534e91b7":"# Training set\nprint(\"Training Set\\n=========================================\")\nNARows = train_data.isna().sum().sum()\nprint(\"Total number of rows (across all columns) that have NaN value = \" + str(NARows))\n\nNAAllRows = train_data.isna().sum()\nprint(\"Num of rows in columns that have NaN value:\")\nprint(NAAllRows)\n\n# Testing set\nprint(\"\\n\\nTest Set\\n=========================================\")\nNARows = test_data.isna().sum().sum()\nprint(\"Total number of rows (across all columns) that have NaN value = \" + str(NARows))\n\nNAAllRows = test_data.isna().sum()\nprint(\"Num of rows in columns that have NaN value:\")\nprint(NAAllRows)","15b9a290":"# Dropping features\ntrain_data.drop(columns=['title', 'console'], inplace=True)","f5687724":"# Imports\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb","cafd7116":"# Subset feature and target columns\nx = train_data[train_data.columns.difference(['id', 'esrb_rating'])]\ny = train_data.esrb_rating\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y)","c16414a2":"x_train.shape","9867fc39":"logreg_param_grid = {'penalty': ['l2'],\n                 'warm_start': [True],\n                 'tol': [0.0001, 0.001, 0.1],\n                 'C': [1, 3, 6, 10],\n                 'fit_intercept': [True, False],\n                 'solver': ['saga']}\nlogreg_gridsearch = GridSearchCV(estimator=LogisticRegression(), param_grid=logreg_param_grid, scoring='accuracy', cv=10)\nlogreg_gridsearch.fit(x_train, y_train)\n\n# Extract best hyperparameters\nlogreg_best_hyperparams = logreg_gridsearch.best_params_\nprint('Best hyperparameters: ', logreg_best_hyperparams)\n\n# Extract best CV score from\nlogreg_best_CV_score = logreg_gridsearch.best_score_\nprint('\\nBest CV accuracy: ', logreg_best_CV_score)\n\n# Extract best model\nlogreg_best_model = logreg_gridsearch.best_estimator_\n\n# Obtain distribution of validation scores\nlogreg_fold_metrics = []\nlogreg_cv_results = logreg_gridsearch.cv_results_\nfor split_iter in range(10):\n    # Iterate through the columns that contain each split's scores\n    current_split = 'split' + str(split_iter) + '_test_score'\n    \n    # For each split, out of all the combinations of parameters it tried, obtain its highest test score\n    metric = logreg_cv_results[current_split].max()\n    \n    logreg_fold_metrics.append(metric)\n\nprint(pd.DataFrame(logreg_fold_metrics).describe())","0be883d1":"svc_param_grid = {'C': [1.5, 1.6, 1.65, 1.7, 1.75],\n                 'kernel': ['rbf'],\n                 'gamma': ['scale'],\n                 'tol': [0.0001, 0.01, 0.1, 0.5, 1]}\nsvc_gridsearch = GridSearchCV(estimator=SVC(), param_grid=svc_param_grid, scoring='accuracy', cv=10)\nsvc_gridsearch.fit(x_train, y_train)\n\n# Extract best hyperparameters\nsvc_best_hyperparams = svc_gridsearch.best_params_\nprint('Best hyperparameters: ', svc_best_hyperparams)\n\n# Extract best CV score from\nsvc_best_CV_score = svc_gridsearch.best_score_\nprint('\\nBest CV accuracy: ', svc_best_CV_score)\n\n# Extract best model\nsvc_best_model = svc_gridsearch.best_estimator_\n\n# Obtain distribution of validation scores\nsvc_fold_metrics = []\nsvc_cv_results = svc_gridsearch.cv_results_\nfor split_iter in range(10):\n    # Iterate through the columns that contain each split's scores\n    current_split = 'split' + str(split_iter) + '_test_score'\n    \n    # For each split, out of all the combinations of parameters it tried, obtain its highest test score\n    metric = svc_cv_results[current_split].max()\n    \n    svc_fold_metrics.append(metric)\n\nprint(pd.DataFrame(svc_fold_metrics).describe())","4e2fa4f8":"dt_param_grid = {'max_depth': [3, 4, 5, 6],\n                 'min_samples_leaf': [0.01, 0.03, 0.05, 0.08],\n                 'min_samples_split': [2, 4, 8],\n                 'max_features': [0.2, 0.5, 0.8]}\ndt_gridsearch = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=dt_param_grid, scoring='accuracy', cv=10)\ndt_gridsearch.fit(x_train, y_train)\n\n# Extract best hyperparameters\ndt_best_hyperparams = dt_gridsearch.best_params_\nprint('Best hyperparameters: ', dt_best_hyperparams)\n\n# Extract best CV score from\ndt_best_CV_score = dt_gridsearch.best_score_\nprint('\\nBest CV accuracy: ', dt_best_CV_score)\n\n# Extract best model\ndt_best_model = dt_gridsearch.best_estimator_\n\n# Obtain distribution of validation scores\ndt_fold_metrics = []\ndt_cv_results = dt_gridsearch.cv_results_\nfor split_iter in range(10):\n    # Iterate through the columns that contain each split's scores\n    current_split = 'split' + str(split_iter) + '_test_score'\n    \n    # For each split, out of all the combinations of parameters it tried, obtain its highest test score\n    metric = dt_cv_results[current_split].max()\n    \n    dt_fold_metrics.append(metric)\n\nprint(pd.DataFrame(dt_fold_metrics).describe())","a4b021b6":"rf_param_grid = {'n_estimators': [250],\n                 'max_depth': [15, 20],\n                 'min_samples_leaf': [0.005, 0.01, 0.015],\n                 'min_samples_split': [8],\n                 'max_features': [0.4],\n                 'warm_start': [True]}\nrf_gridsearch = GridSearchCV(estimator=RandomForestClassifier(), param_grid=rf_param_grid, scoring='accuracy', cv=10)\nrf_gridsearch.fit(x_train, y_train)\n\n# Extract best hyperparameters\nrf_best_hyperparams = rf_gridsearch.best_params_\nprint('Best hyperparameters: ', rf_best_hyperparams)\n\n# Extract best CV score from\nrf_best_CV_score = rf_gridsearch.best_score_\nprint('\\nBest CV accuracy: ', rf_best_CV_score)\n\n# Extract best model\nrf_best_model = rf_gridsearch.best_estimator_\n\n# Obtain distribution of validation scores\nrf_fold_metrics = []\nrf_cv_results = rf_gridsearch.cv_results_\nfor split_iter in range(10):\n    # Iterate through the columns that contain each split's scores\n    current_split = 'split' + str(split_iter) + '_test_score'\n    \n    # For each split, out of all the combinations of parameters it tried, obtain its highest test score\n    metric = rf_cv_results[current_split].max()\n    \n    rf_fold_metrics.append(metric)\n\nprint(pd.DataFrame(rf_fold_metrics).describe())","fa1f54fc":"knn_param_grid = {'n_neighbors': [3, 5, 10, 20, 40, 80],\n                  'weights': ['uniform', 'distance'],\n                  'p': [1, 2],\n                  'algorithm': ['ball_tree', 'kd_tree', 'brute']}\nknn_gridsearch = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=knn_param_grid, scoring='accuracy', cv=10)\nknn_gridsearch.fit(x_train, y_train)\n\n# Extract best hyperparameters\nknn_best_hyperparams = knn_gridsearch.best_params_\nprint('Best hyperparameters: ', knn_best_hyperparams)\n\n# Extract best CV score from\nknn_best_CV_score = knn_gridsearch.best_score_\nprint('\\nBest CV accuracy: ', knn_best_CV_score)\n\n# Extract best model\nknn_best_model = knn_gridsearch.best_estimator_\n\n# Obtain distribution of validation scores\nknn_fold_metrics = []\nknn_cv_results = knn_gridsearch.cv_results_\nfor split_iter in range(10):\n    # Iterate through the columns that contain each split's scores\n    current_split = 'split' + str(split_iter) + '_test_score'\n    \n    # For each split, out of all the combinations of parameters it tried, obtain its highest test score\n    metric = knn_cv_results[current_split].max()\n    \n    knn_fold_metrics.append(metric)\n\nprint(pd.DataFrame(knn_fold_metrics).describe())","ba244686":"# Do not grade this, this will take forever to run\n'''\nxgb_param_grid = {'n_estimators': [50, 100, 150],\n                  'max_depth': [4],\n                  'learning_rate': [0.4, 0.55, 0.7],\n                  'objective': ['binary:logistic'],\n                  'booster': ['gbtree'],\n                 }\nxgb_gridsearch = GridSearchCV(estimator=xgb.XGBClassifier(), param_grid=xgb_param_grid, scoring='accuracy', cv=10)\nxgb_gridsearch.fit(x_train, y_train)\n\n# Extract best hyperparameters\nxgb_best_hyperparams = xgb_gridsearch.best_params_\nprint('Best hyperparameters: ', xgb_best_hyperparams)\n\n# Extract best CV score from\nxgb_best_CV_score = xgb_gridsearch.best_score_\nprint('\\nBest CV accuracy: ', xgb_best_CV_score)\n\n# Extract best model\nxgb_best_model = xgb_gridsearch.best_estimator_\n\n# Obtain distribution of validation scores\nxgb_fold_metrics = []\nxgb_cv_results = xgb_gridsearch.cv_results_\nfor split_iter in range(10):\n    # Iterate through the columns that contain each split's scores\n    current_split = 'split' + str(split_iter) + '_test_score'\n    \n    # For each split, out of all the combinations of parameters it tried, obtain its highest test score\n    metric = xgb_cv_results[current_split].max()\n    \n    xgb_fold_metrics.append(metric)\n\nprint(pd.DataFrame(xgb_fold_metrics).describe())\n\ntable = pd.DataFrame(xgb_gridsearch.cv_results_)                    \ncv_results.to_csv('xgb_gridsearchcv.csv', index=False)\n'''","e25a2718":"# Take out id column from test_data\ntesting_columns = test_data.columns.difference(['id', 'console'])\n\nfinal_predictions = svc_best_model.predict(test_data[testing_columns])\n\noutput = pd.DataFrame({'id': test_data.id, 'esrb_rating': final_predictions})\nprint(output)\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","87f9ee4d":"# Model Construction","fe419216":"# Feature Engineering","55405111":"Not sure how this can be done as it is all binary feature sets...","1eae597f":"## SVC","e3c6902e":"Analyzing the distributions of the binary features.","9cba8364":"Well, at least the data isn't all 1 or all 0. Let's check the ratings for each row depending on its value of `no_descriptors`.","56b47fdf":"## K-Nearest Neighbors","7287991e":"### Imports","3063c57a":"## Decision Tree","b4823537":"Hmm, what is `no_descriptors`? In the data dictionary, it is just referred to as \"No content descriptors\". Let's analyze the column.","f92e4890":"## Random Forest","56c6758a":"### Checking for Missing Values","8638c759":"# Exploratory Data Analysis","e2fa5c0f":"### Splitting training set","4e591c2b":"## Logistic Regression","23330963":"Right off the bat, we can get rid of the `title` feature since it will not help our model predict the ESRB rating. The `console` feature is also irrelevant to the ESRB rating of a game.","683994cd":"# Generating Submission File","604fc98a":"Awesome, there are no missing values in the dataset!","3f0bc80f":"# Pre-Req Code","03930930":"Out of all the models, SVC produced the best testing accuracy. So I will be using it for the competition submission.","08049522":"## (Extra) XGBoost","90744007":"### Checking for Outliers","f303ac39":"Well, that's only a little helpful. I could interpret this as the `no_descriptors` column having a favorable correlation toward a game having an `E` ESRB rating."}}