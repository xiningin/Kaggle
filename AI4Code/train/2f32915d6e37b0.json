{"cell_type":{"4fe82496":"code","f6c50013":"code","5f587ce3":"code","2e73d980":"code","217aa381":"code","39667a02":"code","91dedc3b":"code","49d2159a":"code","6509c952":"code","db0dd26a":"code","a94f3d99":"code","55dd9128":"code","51d1495a":"code","63876c5c":"code","28f1c297":"code","65852318":"code","c75e44ed":"code","86542361":"code","fd57ac39":"code","269c0319":"code","95334b28":"code","7ff564d1":"code","3b56e50e":"code","3841e3ff":"code","b77c0af5":"code","7bea9e7e":"code","286b1bea":"code","34dcf05c":"code","1d9ea9cb":"code","b94edfe6":"code","a68d62eb":"code","c99635dc":"code","95b1d873":"code","c40ae211":"code","4e4c6bc7":"code","b37eec1e":"code","57c9c379":"code","670804aa":"code","6a97305e":"code","de9e2f19":"code","8950a92b":"code","b7325c4b":"code","620b2c43":"code","679c3c02":"code","166f4d16":"code","f4c4b339":"code","c887db31":"code","8ff79bd9":"code","72e70166":"code","b644918a":"code","7f25b109":"code","9ef9b675":"code","0e606719":"code","bee96edf":"code","c83a3be7":"code","fba1bc0c":"code","25218d4d":"code","d083a9eb":"code","13ccfb95":"code","5e7872c0":"code","f4479bb8":"code","f04f6f2c":"code","a62a68ff":"markdown","b2845daf":"markdown","94a7f73b":"markdown","38b881d1":"markdown","dad5711c":"markdown","ea49ea1c":"markdown","d9e46ed9":"markdown","f4b63bcd":"markdown","76ae50d4":"markdown","9f89fed6":"markdown","641793df":"markdown","0b95d597":"markdown","47e8c34d":"markdown","0f129a93":"markdown","1ab29b24":"markdown","64183ee8":"markdown","ca72f17b":"markdown","980bb657":"markdown","14461e63":"markdown","511e0733":"markdown","1d7872e8":"markdown","6e446ff9":"markdown","2c0f59e0":"markdown","badfde72":"markdown","e2e26cce":"markdown","ea391662":"markdown","d12a0481":"markdown","95089a05":"markdown","1e45a4b0":"markdown","6a7368d3":"markdown","ab0d512c":"markdown","0af190b3":"markdown","8ad08049":"markdown","590b1d90":"markdown","731a4ad8":"markdown","d4db61e1":"markdown","90035661":"markdown","95292944":"markdown","a945d799":"markdown","0a020d7d":"markdown","b56c7919":"markdown"},"source":{"4fe82496":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f6c50013":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","5f587ce3":"df=pd.read_csv(\"\/kaggle\/input\/tweet_data.csv\",encoding='ISO-8859-1')","2e73d980":"df","217aa381":"col=[\"profileimage\",\"tweet_location\",\"user_timezone\",\"sidebar_color\",\"tweet_coord\",\"link_color\",\"fav_number\",\"tweet_id\",\"_last_judgment_at\",\"created\",\"tweet_created\"]\ndf.drop(col,axis=1,inplace=True)","39667a02":"df\n## Sanity Check ##","91dedc3b":"df.drop_duplicates(inplace=True)","49d2159a":"df.isnull().sum()","6509c952":"c=[\"gender\",\"gender:confidence\"]\ndf.dropna(subset=c,how=\"any\",inplace=True)","db0dd26a":"df","a94f3d99":"df.isnull().sum()","55dd9128":"df[\"text\"].fillna(\"\", inplace=True)","51d1495a":"df[\"description\"].fillna(\"\",inplace=True)","63876c5c":"o=list(df[\"text\"])\nimport re\n\nl=[]\nk=[]\nfor s in df[\"text\"] :\n    a=re.sub(r\"http:\/\/t.co\/[a-zA-Z0-9]*\",\" \",str(s))\n    b=re.sub(r\"https:\/\/t.co\/[a-zA-Z0-9]*\",\" \",str(s))\n    \n    l.append(a)\n    k.append(b)\n    \ndf.replace(inplace=True, to_replace=o, value=l)\no=list(df[\"text\"])\ndf.replace(inplace=True, to_replace=o, value=k)\n    \ndf[\"text\"].replace(regex=True, inplace=True, to_replace=r'[,!.; -@!%^&*)(]', value=' ')\n    ","28f1c297":"o=list(df[\"description\"])\nimport re\n\nl=[]\nk=[]\nfor s in df[\"description\"] :\n    s=re.sub(r\"http:\/\/t.co\/[a-zA-Z0-9]*\",\" \",str(s))\n    s=re.sub(r\"https:\/\/t.co\/[a-zA-Z0-9]*\",\" \",str(s))\n    \n    l.append(s)\n    k.append(s)\n    \ndf.replace(inplace=True, to_replace=o, value=l)\ndf.replace(inplace=True, to_replace=o, value=k)\n\ndf[\"description\"].replace(regex=True, inplace=True, to_replace=r'[,!.; -@!%^&*)(]', value=' ')","65852318":"df.head(10)","c75e44ed":"df.shape","86542361":"df.describe()","fd57ac39":"df.info()","269c0319":"df.columns","95334b28":"df.gender.nunique()","7ff564d1":"df.gender.unique()","3b56e50e":"df._golden.unique()","3841e3ff":"df._unit_state.unique()","b77c0af5":"num_col=df.select_dtypes(include=np.number).columns\nprint(\"Numerical Columns :\\n\",num_col)\ncat_col=df.select_dtypes(exclude=np.number).columns\nprint(\"Categorical Columns :\\n\",cat_col)","7bea9e7e":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ndf['gender']=le.fit_transform(df['gender'])#Converts brand=0,female=1,male=2,unknown=3\ndf['_golden']=le.fit_transform(df['_golden'])#Converts true as 1 and false as 0\ndf['_unit_state']=le.fit_transform(df['_unit_state'])#converts finalized as 0 and golden as 1","286b1bea":"df.gender.unique()","34dcf05c":"df._golden.unique()","1d9ea9cb":"df._unit_state.unique()","b94edfe6":"df.head(10)","a68d62eb":"df.tail()","c99635dc":"df.gender.plot(kind='hist')\n","95b1d873":"fig=plt.figure(figsize=(10,5))\nplt.bar(df.gender,df.tweet_count,color='maroon',width=0.4)\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Tweet count\")\nplt.title(\"Tweet count based on gender\")\nplt.show()","c40ae211":"sns.heatmap(df.corr(),annot=True,fmt='.1g',cbar=False)","4e4c6bc7":"matrix=np.triu(df.corr())\nsns.heatmap(df.corr(),annot=True,mask=matrix)","b37eec1e":"from sklearn.model_selection import train_test_split","57c9c379":"df.info()","670804aa":"def normalize_text(s):\n    # just in case\n    s = str(s)\n    s = s.lower()\n    \n    # remove punctuation that is not word-internal (e.g., hyphens, apostrophes)\n    s = re.sub('\\s\\W',' ',s)\n    s = re.sub('\\W\\s',' ',s)\n    \n    # make sure we didn't introduce any double spaces\n    s = re.sub('\\s+',' ',s)\n    \n    return s\ndf['text_norm'] = [normalize_text(s) for s in df['text']]\ndf['description_norm'] = [normalize_text(s) for s in df['description']]","6a97305e":"df['all_features'] = df['text_norm'].str.cat(df['description_norm'], sep=' ')\ndf_confident = df[df['gender:confidence']==1] ## Choosing only the one's with confidence\ndf_confident.shape #Now we have approx 14000 entries.","de9e2f19":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nx = vectorizer.fit_transform(df_confident['text_norm'])\nencoder = LabelEncoder()\ny = encoder.fit_transform(df_confident['gender'])","8950a92b":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom catboost import CatBoostClassifier\n#nb = CatBoostClassifier(silent = True)\n\n#### Multi-Nomial Naive Bayes was found to give considerably good peroformance with this data ####\n\nnb = MultinomialNB(alpha = 0.6,fit_prior = True)\nnb.fit(x_train, y_train)\n\nprint(nb.score(x_test, y_test))","b7325c4b":"### Just to illustrate how the text data looks like ###\ndf_just_text = pd.DataFrame(x)\ndf_just_text","620b2c43":"X=df[['_unit_id','_golden','_unit_state','_trusted_judgments','gender:confidence','profile_yn:confidence','retweet_count','tweet_count']]","679c3c02":"X.info()","166f4d16":"df.corr()","f4c4b339":"Y=df[['gender']]","c887db31":"df_conf = df[df['gender:confidence']==1]\ndf_conf.shape","8ff79bd9":"X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size = 0.2)","72e70166":"import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process,model_selection\nimport xgboost\nfrom xgboost import XGBClassifier\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    #gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    #svm.SVC(probability=True),\n    #svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost\n    XGBClassifier()    \n    ]\n\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\nMLA_predict = Y['gender']\nrow_index = 0\nX1 = X.copy()\nfor alg in MLA:\n    #print(row_index)\n    X = X1\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    print('Examining ',MLA_name)\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    cv_results = model_selection.cross_validate(alg, X, Y, cv  = cv_split)\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3\n    alg.fit(X, Y)\n    MLA_predict[MLA_name] = alg.predict(X)\n    row_index+=1\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare","b644918a":"#### Taking 4 Ensembles ###\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process,model_selection\nimport xgboost\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    XGBClassifier(),\n    CatBoostClassifier(verbose = False)    ## Just to see how it does! ##\n    ]\n\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\nMLA_predict = Y['gender']\nrow_index = 0\nX1 = X.copy()\nfor alg in MLA:\n    X = X1\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    print(MLA_name)\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    cv_results = model_selection.cross_validate(alg, X, Y, cv  = cv_split)\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3\n    alg.fit(X, Y)\n    MLA_predict[MLA_name] = alg.predict(X)\n    row_index+=1\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare","7f25b109":"#barplot using https:\/\/seaborn.pydata.org\/generated\/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https:\/\/matplotlib.org\/api\/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","9ef9b675":"from sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_auc_score","0e606719":"#a=XGBClassifier(num_rounds = 150,min_split_leaf = 10,max_depth = 3,random_state=100)\n#a=GradientBoostingClassifier(num_rounds = 150,min_split_leaf = 10,max_depth = 3,random_state=100)\n#a=CatBoostClassifier(num_rounds = 150,min_split_leaf = 10,max_depth = 3,random_state=100)\n#a=AdaBoostClassifier(num_rounds = 150,min_split_leaf = 10,max_depth = 3,random_state=100)\n\na = MultinomialNB()","bee96edf":"a.fit(X_train,Y_train)","c83a3be7":"y_pred=a.predict(X_test)","fba1bc0c":"score=accuracy_score(Y_test,y_pred)\nscore*100","25218d4d":"import tensorflow as tf","d083a9eb":"model=tf.keras.Sequential([\n    tf.keras.layers.Dense(units=8,input_dim=X_train.shape[1],activation='relu'),\n     tf.keras.layers.LeakyReLU(0.3),\n    tf.keras.layers.Dense(units=1,activation='sigmoid')\n])","13ccfb95":"model.compile(loss = 'mean_squared_error',optimizer = 'adam',metrics = ['accuracy'])","5e7872c0":"model.fit(X_train,Y_train,epochs=5)","f4479bb8":"y_pred=model.predict(X_test)","f04f6f2c":"score=accuracy_score(Y_test,y_pred)\nscore*100","a62a68ff":"<a id=\"regexp\"><\/a>\n# Reg-exp based text preprocessing","b2845daf":"## Exploratory Data Analysis and Sample ML Models on the Twitter Dataset","94a7f73b":"Although the accuracies are lesser than the models that use text data (which is kind of obvious), Seems like GBClassifier and XGBoost does the best on this data!","38b881d1":"Accuracy's between 54 - 59 % can be obtained with suitable tuning","dad5711c":"<a id=\"notext\"><\/a>\n## Gender Analysis without Text","ea49ea1c":"### Finding the number of unique values of columns","d9e46ed9":"<a id=\"nulls\"><\/a>\n## Nulls Checking and removal","f4b63bcd":"I have used the following code in several of my notebooks, and i suggest the reader to use this as a template for any other ensembling tasks to save your time! I have provided a boiler plate, Naive (non-optimized) ensemble of 20+ algos here.","76ae50d4":"### Analysis using important features without text","9f89fed6":"###  Importing the required modules ","641793df":"### Removing duplicates from the dataframe","0b95d597":"<a id=\"text\"><\/a>\n## Gender Analysis with just Text","47e8c34d":"The dataset is obtained from https:\/\/www.kaggle.com\/darkknight98\/twitter-data . The objective is to determine which gender commits more typos on twitter, based on approx. 20000 user tweets and other tweet\/user related meta-data.","0f129a93":"## So we have seen several implementations for analysis on twitter data with different algorithms using text and without the text (using just meta-data)too. Hope it was helpful!","1ab29b24":"<a id=\"under\"><\/a>\n## Understanding the Data","64183ee8":"fig=plt.figure(figsize=(10,5))\nplt.bar(df.gender,df.tweet_count,color='maroon',width=0.4)\nplt.show()","ca72f17b":"### Removing the special characters and hyperlinks in text column using Regular Expressions","980bb657":"### Replacing null values in description column with empty string","14461e63":"### Converting Categorical data to Numerical data","511e0733":"<a id=\"encoding\"><\/a>\n## Encoding the Cat-Features","1d7872e8":"P.S: Also find the regexp based vectorization of text, and preprocessing of tweet-text so as to make it a useful feature","6e446ff9":"### Read on to see boiler-plate implementations of different ML algorithms applied on the dataset. I've provided implementations using just the text and without using text. Models tested include an ensemble of simple classifiers, logistic regression,Random Forest, SVC and MultinomialNB along with a basic Neural network.","2c0f59e0":"### Checking null values that still exist in the entire dataframe","badfde72":"### Dropping null values in gender and gender:confidence columns as they are of no significance with gender being the dependant variable","e2e26cce":"### Replacing null values in text column with empty string","ea391662":"### Heads up: The two cells below take long to run to completion","d12a0481":"### Removing the special characters and hyperlinks in description column using regular Expressions","95089a05":"The following section examines predictive modelling of typos even without using the 'Text' of the tweet. Let's see how linear models and GBDTs perform in this scenario.","1e45a4b0":"<a id=\"nn\"><\/a>\n## Just trying out a simple NN for completeness","6a7368d3":"### Dropping the columns which are of less significance.","ab0d512c":"### Checking for null values across the columns","0af190b3":"<a id=\"ensembling\"><\/a>\n## Ensembling","8ad08049":"### Loading the dataset with suitable encoding","590b1d90":"Well, 2 Layered-NN ? Not that great I guess!, But there's still scope for improving it. But I'll stop here as i feel i have covered enough for you to take it from here.","731a4ad8":"###  Seaborn heatmap for a correlation matrix","d4db61e1":"<a id=\"dv\"><\/a>\n## Data Visualization","90035661":"### This is where we vectorize the tweets such that they are now a candidate for a feature","95292944":"# Brief Contents\n\n\n* [Understanding the data](#under)\n* [Nulls Checking & Removal](#nulls)\n* [Regexp based text preprocessing](#regexp)\n* [Encoding Cat-Features](#encoding)\n* [Data Visualizations](#dv)\n* [Gender Analysis with Just Text](#text)\n* [Gender Analysis without Text](#notext)\n    - [Ensembling](#ensembling)\n    - [Simple NN](#nn)","a945d799":"### Histogram of gender ","0a020d7d":"Uncomment and run whichever you feel like!","b56c7919":"### Bar plot against gender and tweet count"}}