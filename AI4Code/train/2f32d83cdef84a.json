{"cell_type":{"2be802ea":"code","0c9fbedf":"code","6d4ef279":"code","ee5ca34e":"code","52ab27ac":"code","130cedcc":"code","0fec7cb1":"code","2e5d06f7":"code","003ca0ff":"code","1f662ead":"code","c2a13629":"code","990f511c":"code","c9385518":"code","234da777":"code","23648126":"code","3a073161":"code","8fccafdc":"code","94517b22":"code","f06446a6":"code","143f9f7b":"code","d61eea5b":"code","21bbd804":"code","51aa634e":"code","b065e104":"code","bb8a09be":"markdown","16e6542c":"markdown","7b86040c":"markdown","f9632178":"markdown","07eb9b40":"markdown","a31891ab":"markdown","2ac811e0":"markdown","50c2a2ca":"markdown","f962095a":"markdown","84a53e5d":"markdown","72fd0ed2":"markdown","9bf7c0c9":"markdown","3038eb0d":"markdown","d40ee7be":"markdown","268d952d":"markdown","fc6303c2":"markdown","8f52e84f":"markdown","5cd21b68":"markdown","f19577b2":"markdown","3cff7742":"markdown","5e72c133":"markdown","7fefd149":"markdown","1a9f6243":"markdown"},"source":{"2be802ea":"import os\nimport re\nimport urllib\nfrom tarfile import open as open_tar\nimport shutil\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom bs4 import BeautifulSoup\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom scipy.sparse import csr_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nimport email\nimport email.policy\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.tree import DecisionTreeClassifier","0c9fbedf":"download_url = \"https:\/\/spamassassin.apache.org\/old\/publiccorpus\/\"\ndownload_dir = os.path.join(\"datasets\", \"downloads\")\nham = os.path.join(\"datasets\", \"ham\")\nspam = os.path.join(\"datasets\", \"spam\")\nos.makedirs(download_dir, exist_ok=True)\nos.makedirs(ham, exist_ok=True)\nos.makedirs(spam, exist_ok=True)\nfile_names = {\n    \"20021010_easy_ham.tar.bz2\": \"ham\",\n    \"20021010_hard_ham.tar.bz2\": \"ham\",\n    \"20021010_spam.tar.bz2\": \"spam\",\n    \"20030228_easy_ham.tar.bz2\": \"ham\",\n    \"20030228_easy_ham_2.tar.bz2\": \"ham\",\n    \"20030228_hard_ham.tar.bz2\": \"ham\",\n    \"20030228_spam.tar.bz2\": \"spam\",\n    \"20030228_spam_2.tar.bz2\": \"spam\",\n    \"20050311_spam_2.tar.bz2\": \"spam\"\n}\n#download the files and unzip them\nfor key, value in file_names.items():\n    file_name = os.path.join(download_dir, key)\n    urllib.request.urlretrieve(download_url + key, file_name)\n    with open_tar(file_name) as tar:\n        tar.extractall(path=download_dir)","6d4ef279":"# delete the zipped files\nfor key, value in file_names.items():\n    os.remove(os.path.join(download_dir, key))","ee5ca34e":"#move the unzipped emails into appropriate folders\nfor folder in os.listdir(download_dir):\n    if \"spam\" in folder:\n        target = \"spam\"\n    elif \"ham\" in folder:\n        target = \"ham\"\n    for file in os.listdir(os.path.join(download_dir, folder)):\n        old_name = os.path.join(download_dir, folder, file)\n        new_name = os.path.join(\"datasets\", target, file)\n        os.replace(old_name, new_name)","52ab27ac":"# delete the downloads folder and all subfolders\nshutil.rmtree(download_dir)","130cedcc":"# now I read the emails into separate lists\nham_filenames = [name for name in os.listdir(ham) if len(name) > 20]\nspam_filenames = [name for name in os.listdir(spam) if len(name) > 20]\n\n# now read the emails into arrays\ndef load_email(is_spam, filename):\n    directory = spam if is_spam else ham\n    with open(os.path.join(directory, filename), \"rb\") as f:\n        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n\nham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\nspam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]","0fec7cb1":"# now delete the emails\nshutil.rmtree(\"datasets\")","2e5d06f7":"# now that we have the emails, split the data into test and train sets\nX = np.array(ham_emails + spam_emails, dtype=object)\ny = np.array([0] * len(ham_emails) + [1] * len(spam_emails))","003ca0ff":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=8)","1f662ead":"# now we need a function to convert the email into a string text\ndef email_to_text(email):\n    content = \"\"\n    # go through the email looking for the body\n    for part in email.walk():\n        content_type = part.get_content_type()\n        # if we reach the body, which is either text or html, extract it\n        if content_type in (\"text\/plain\", \"text\/html\"):\n            try:\n                # the get_content() method extracts the body\n                content = part.get_content()\n            except:\n                # sometimes parse errors might arise\n                content = str(part.get_payload())\n            # if the content extracted is html, convert it to text\n            if content_type == \"text\/html\":\n                soup = BeautifulSoup(content)\n                content = soup.getText(content)\n            return content.strip()\n    return content","c2a13629":"print(email_to_text(X_train[1]))","990f511c":"class EmailToWordCount(BaseEstimator, TransformerMixin):\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        # create en amepty array which will be returned\n        X_transformed = []\n        # go through each email in X, converting each to a counter\n        for email in X:\n            # extract the text from the email using the function that we created\n            text = email_to_text(email) or \"\"\n            # convert to lower case\n            text = text.lower()\n            # replace urls by URL\n            text = re.sub(r'(https?:\/\/\\S+)', \"URL\", text)\n            # replace numbers by NUMBER\n            text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n            # replace emails by the word \"EMAIL\"\n            text = re.sub('\\S*@\\S*\\s?', 'EMAIL', text)\n            # remove punctuation\n            text = re.sub(r'[\\W+]', ' ', text)\n            # remove non alphabet characters\n            text = re.sub('[^a-zA-Z]', ' ', text)\n            # now that we have the body in the form that we want, we count the\n            # number of times each word appears. This is quickly accomplished by\n            # creating a counter and pssing to it the list of words in the email.\n            word_count = Counter(text.split())\n            # we add the counter to the array that will be returned\n            X_transformed.append(word_count)\n        return np.array(X_transformed)","c9385518":"X_try = X[0:2]\nX_try_count = EmailToWordCount().fit_transform(X_try)\nX_try_count","234da777":"class WordCountToMatrix(BaseEstimator, TransformerMixin):\n\n    def __init__(self, vocabulary_size=1000):\n        # set the size of the number of words to include in the most common words list\n        self.vocabulary_size = vocabulary_size\n\n    def fit(self, X, y=None):\n        # create a counter to count the frequency of all words in all emails\n        total_count = Counter()\n        # loop through the counter of each email\n        for word_count in X:\n            # get each word and its count in each email\n            for word, count in word_count.items():\n                # add the count of the word in this email to the overall count for this word\n                total_count[word] += count\n        # now that we have the count for all words that apeared in all emails, get the \n        # most common words using the vocabulary size\n        most_common = total_count.most_common()[:self.vocabulary_size]\n        # save the most common words in a dictionary with an index number that will be used\n        # to indicate the number of the column associated with this word in the final matrix\n        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n        return self\n\n    def transform(self, X, y=None):\n        # we will use three arrays that will save the row number, column number, and the\n        # count. The row represents the email. The column represent the word. The data \n        # represents the count of this word in this particular email\n        rows = []\n        cols = []\n        data = []\n        # go through the matrix that contains the counters for each email\n        for row, word_count in enumerate(X):\n            # for each counter, get the word and the count\n            for word, count in word_count.items():\n                # the row is the email\n                rows.append(row)\n                # the column is the word. If the word is in the most common list then\n                # include the count in the apropriate column. Otherwise include the count\n                # in column 0 which has been reserved for the \"uncategorized\" words, i.e.\n                # all words that are not included in the most common list\n                cols.append(self.vocabulary_.get(word, 0))\n                # the data is the count\n                data.append(count)\n        # we now have three matrices where one matrix records the row number, another \n        # records the column number, and a third contains the entry in that particular row\n        # and column\n        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size+1))","23648126":"word_counter_to_matrix = WordCountToMatrix(vocabulary_size=10)\nX_try_matrix = word_counter_to_matrix.fit_transform(X_try_count)\nX_try_matrix.toarray()","3a073161":"# convert the X_train set which includes the emails to a set of counters for each email\nX_train_transformed_to_counters = EmailToWordCount().fit_transform(X_train)\n# now convert the individual counters for each email to a single matrix\n# use the default vocabulary size which is 1000\nword_counter_to_matrix = WordCountToMatrix()\nX_train_transformed = word_counter_to_matrix.fit_transform(X_train_transformed_to_counters)","8fccafdc":"lr_clf = LogisticRegression(max_iter=10000)\nscore = cross_val_score(lr_clf, X_train_transformed, y_train, cv=3)\nlr_accuracy = score.mean()\nscore = cross_val_score(lr_clf, X_train_transformed, y_train, cv=3, scoring=\"f1\")\nlr_f1 = score.mean()","94517b22":"knn_clf = KNeighborsClassifier()\nn_neighbors = range(2, 10)\nparameters = dict(n_neighbors=n_neighbors)\nknn_grid = GridSearchCV(knn_clf, parameters, cv=3)\nbest_model = knn_grid.fit(X_train_transformed, y_train)\nbest_parameters = best_model.best_params_\nknn_clf = KNeighborsClassifier(n_neighbors=best_parameters[\"n_neighbors\"])\nknn_clf.fit(X_train_transformed, y_train)\nknn_yhat = knn_clf.predict(X_train_transformed)\nknn_accuracy = accuracy_score(y_train, knn_yhat)\nknn_f1 = f1_score(y_train, knn_yhat)","f06446a6":"from sklearn.tree import DecisionTreeClassifier\ntree_clf = DecisionTreeClassifier()\nmax_depth = list(range(1, 50, 5))\nmin_samples_split = list(range(2, 20, 2))\nmax_features = np.linspace(0.1, 0.9, 5)\nparameters = dict(max_depth=max_depth, min_samples_split=min_samples_split, max_features=max_features)\ntree_grid = GridSearchCV(tree_clf, parameters, cv=3)\nbest_model = tree_grid.fit(X_train_transformed, y_train)\nbest_parameters = best_model.best_params_\ntree_clf = DecisionTreeClassifier(max_depth=best_parameters[\"max_depth\"],\n                                  min_samples_split=best_parameters[\"min_samples_split\"],\n                                  max_features=best_parameters[\"max_features\"])\ntree_clf.fit(X_train_transformed, y_train)\ntree_yhat = tree_clf.predict(X_train_transformed)\ntree_accuracy = accuracy_score(y_train, tree_yhat)\ntree_f1 = f1_score(y_train, tree_yhat)","143f9f7b":"models = [\"logistic\", \"knn\", \"tree\"]\naccuracy = [lr_accuracy, knn_accuracy, tree_accuracy]\nf1 = [lr_f1, knn_f1, tree_f1]\n# plot\nplt.plot(models, accuracy, label=\"accuracy\")\nplt.plot(models, f1, label=\"f1 score\")\nplt.legend()","d61eea5b":"# first transform the X_test set\n# convert the X_test set which includes the emails to a set of counters for each email\nX_test_transformed_to_counters = EmailToWordCount().fit_transform(X_test)\n# now convert the individual counters for each email to a single matrix\n# use the default vocabulary size which is 1000\nword_counter_to_matrix = WordCountToMatrix()\nX_test_transformed = word_counter_to_matrix.fit_transform(X_test_transformed_to_counters)","21bbd804":"tree_yhat = tree_clf.predict(X_test_transformed)\ntree_accuracy = accuracy_score(y_test, tree_yhat)\ntree_f1 = f1_score(y_test, tree_yhat)\nprint(\"Accuracy: \", tree_accuracy)\nprint(\"F1 score: \", tree_f1)","51aa634e":"knn_yhat = knn_clf.predict(X_test_transformed)\nknn_accuracy = accuracy_score(y_test, knn_yhat)\nknn_f1 = f1_score(y_test, knn_yhat)\nprint(\"KNN\")\nprint(\"Accuracy: \", knn_accuracy)\nprint(\"F1 score: \", knn_f1)","b065e104":"lr_clf = LogisticRegression(max_iter=10000)\nlr_clf.fit(X_train_transformed, y_train)\nlog_yhat = lr_clf.predict(X_test_transformed)\nlog_accuracy = accuracy_score(y_test, log_yhat)\nlog_f1 = f1_score(y_test, log_yhat)\nprint(\"Logistic\")\nprint(\"Accuracy: \", log_accuracy)\nprint(\"F1 score: \", log_f1)","bb8a09be":"Now that the emails are downloaded, unzipped, and stored in the appropriate folders, we need to read them. First we store the names of the emails in two arrays, one for spam and one for ham. Then we use these names to read the emails into python arrays. A function is created to help read the emails using the email package:","16e6542c":"First we download the emails from the course. Each zip file contaisn either spam or non-spam (ham) emails:","7b86040c":"Everything looks fine. We have two counters in the returned array where each lists the words and the number of times that the words appeared.\n\nWe now need another transformer that transforms this matrix into another matrix that contains the words as columns and the individual emails as rows. This way, the matrix will contain information about the count of each word in each email. To do this, the transformer will first create a list of the most frequent word. This is done because we do not want to include all words as columns. Instead, we just want the most frequent, or important, words. Once we have a list of the most common words, we can create the matrix that includes that count of each of these words in each of the emails:","f9632178":"Now that the files have been unzipped, we can delete the zipped files to save memory:","07eb9b40":"Everything seems fine. \n\nIn order to categorise emails as spam or ham, we use a method where we count the number of times that words appear in each email. This way, we use the number of times that each word appears to predict whether the email is spam or not. This means that we need to take the email and convert it to a counter which contains each word and how many times it appears in the email. We accomplish this using a transformer. This will help streamline our work. In the transformer, we take the array containing the emails, convert each email to text, and then use a counter to count the number of words. We are not interested in individual URLs which might be in the body, or individual numbers. Therefore, we replace any URL with the letters \"URL\" and we replace any number with the word \"NUMBERS\". This way we keep track of the number of URLs and numbers and in each email.","a31891ab":"Let us try the transformer:","2ac811e0":"Let us try this transformer out:","50c2a2ca":"It seems that the KNN model performs the best when predicting the test set.","f962095a":"## Decision tree","84a53e5d":"We now predict whether the emails in the test sets are spam or ham. We use the decision tree model that was last fit since it had the best parameters:","72fd0ed2":"We can now delete the downloads dirctory and the subfolders:","9bf7c0c9":"Let us try out this function on an email:","3038eb0d":"Now tha tthe emails are read, we can delete the actual emails to save space:","d40ee7be":"## KNN","268d952d":"It seems that the dcision tree is doing the best job in terms of accuracy and the f1 score. Let us see the result when we predict the values of the test set:","fc6303c2":"## Compare","8f52e84f":"Now that we have the X and y matrices, we can split our data set into train and test sets:","5cd21b68":"We now move the emails into one of two folders, spam or ham:","f19577b2":"We now have a matrix where the first column represents words that were not included in the most common list, anf each of the other columns represents one of the words in the most common list. The rows represent the individual emails. Finally, the data represents the count of this particular word in this particular email.\n\nLet us now run machine learning algorithms on the tran data sets. First, transform the X_train set:","3cff7742":"We notice a huge drop in both accuracy and f1 score. The model seems to have been overfitting the data. Let us see the results of the knn and logistic models when we try to predict the test data set:","5e72c133":"X_train contains the actualy email, with the header and all. We need to extract the body from each email since this is the information that we will be using to categorise emails as spam or ham. The following function takes the email and returns the body as a string. Some emails contain multiple parts and this makes our job more complex. This means we have to walk, or iterate through the email, looking for the body. Once the body is found we need to return in. Sometimes the body of the email is plain text. This is good. However, some times the body is html. In this case, we need to convert html to text. This is done using BeatutifulSoup:","7fefd149":"Now that we have transformed the data, we can run the machine learning algorithms.\n\n## Logistic regression","1a9f6243":"We now have two arrays that store the emails, one for spam and one for ham. We can now create our dataset by combining these two arrays to form the X matrix and the y matrix:"}}