{"cell_type":{"503434d9":"code","b861a497":"code","be0b6f47":"code","f36c14bf":"code","491c4585":"code","ea8649c1":"code","592093ce":"code","09017718":"code","e263ebcf":"code","17016255":"code","8ef02134":"code","4dfb5460":"code","5427dc4f":"code","a790e333":"code","d007ce62":"code","61d0d8d5":"code","1967144d":"code","b5537c99":"code","7a053cd0":"code","6ca1a831":"code","9fb0c8a1":"code","86a6e1d1":"code","6d6c4670":"code","4df1d07a":"code","fd9f34a3":"code","528ce780":"code","c5df4925":"code","4d37f6e7":"code","8a05a5f0":"code","8d8390a0":"code","045d46cc":"code","62e29bcc":"code","ec83be63":"code","2c664684":"code","65071dea":"code","1b6326a2":"code","1362df0d":"code","38718918":"code","4eee8242":"code","37f5061e":"code","2a18c152":"code","3979c798":"code","e5fe1a30":"code","89f36643":"code","d38d2518":"code","10b278ba":"code","4077ed51":"code","290acd9e":"code","0e1daf7c":"code","e99e3316":"code","eecaba3e":"code","a872f559":"code","e8204123":"code","04f59908":"code","eec81cf3":"code","dd8eeb9e":"code","0db6fdc2":"code","95965b07":"code","32ce9c3c":"code","f7d8d968":"code","a3bd006a":"code","d3695035":"code","ed297427":"code","cb67db3f":"code","dead57ea":"markdown","1685962f":"markdown","6ec8bf06":"markdown","1b8da2f2":"markdown","0399b1df":"markdown","6bff8cb0":"markdown","4abb356b":"markdown","eae07048":"markdown","595f1065":"markdown","36a9ba26":"markdown","8714c12d":"markdown","8e847067":"markdown","d010a779":"markdown","622a149d":"markdown","68d555a1":"markdown"},"source":{"503434d9":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib\nimport os\n\nimport statistics as stat\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model, metrics\nfrom sklearn.linear_model import Ridge,Lasso\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\n\n#hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nmatplotlib.rc('figure', max_open_warning = 0)","b861a497":"housing_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv',encoding='ISO-8859-1')\nhousing_df.head()","be0b6f47":"housing_df.columns","f36c14bf":"housing_df.shape","491c4585":"housing_df.info()","ea8649c1":"# all numeric (float and int) variables in the dataset\nnum_housing_df = housing_df.select_dtypes(include=['float64', 'int64'])\n\n# all Categoric variables in the dataset\ncat_housing_df = housing_df.select_dtypes(exclude=['float64', 'int64'])","592093ce":"print('Missing value Columns with Percentage Pre-Imputation')\nprint('****************************************************')\nfor column in num_housing_df.columns.values:\n    if num_housing_df[column].isnull().values.sum() != 0:\n        missing_percentage=num_housing_df[column].isnull().values.sum()\/len(num_housing_df)\n        print(column, missing_percentage*100)","09017718":"num_housing_df[['LotFrontage','MasVnrArea','GarageYrBlt']].describe()","e263ebcf":"num_housing_df['LotFrontage'].fillna(num_housing_df['LotFrontage'].mean(),inplace=True) # Numerical Data \nnum_housing_df['MasVnrArea'].fillna(num_housing_df['MasVnrArea'].mean(),inplace=True)   # Numerical Data\nnum_housing_df['GarageYrBlt'].fillna(num_housing_df['GarageYrBlt'].mean(),inplace=True) # Numerical Data","17016255":"print('Missing value Columns with Percentage Post-Imputation')\nprint('****************************************************')\nfor column in num_housing_df.columns.values:\n    if num_housing_df[column].isnull().values.sum() != 0:\n        missing_percentage=num_housing_df[column].isnull().values.sum()\/len(num_housing_df)\n        print(column, missing_percentage*100)","8ef02134":"num_housing_df.columns","4dfb5460":"''' Performing Box Plot to see the variation in features and understand the outliers. \n    Also if Neccessary Transformations are required to remove skweness From Data.''' \nfor values in num_housing_df:  \n    plt.figure(figsize=(6,6))\n    sns.boxplot(x=values,data=num_housing_df)\nplt.show()","5427dc4f":"num_housing_df['SalePrice']=np.log(num_housing_df['SalePrice']+1)","a790e333":"plt.figure(figsize=(6,6))\nsns.distplot(num_housing_df['SalePrice'])\nplt.show()","d007ce62":"# Performing a Scatter Plot to Find the Relation of Numerical Data Type \n# having any impact or relation #with Sale Price.\n\nfor features in num_housing_df.columns.values:\n    plt.figure(figsize=(5,4))\n    plt.scatter(num_housing_df[features],num_housing_df['SalePrice'], alpha = 0.3)\n    plt.title(\"SalePrice vs \"+str(features))\n    plt.xlabel(str(features))\n    plt.ylabel('SalePrice')\nplt.show()","61d0d8d5":"num_housing_df['As_of_Date']=num_housing_df['YrSold']-num_housing_df['YearBuilt']","1967144d":"plt.figure(figsize = (20, 12))\nsns.heatmap(num_housing_df.corr(), annot = True, cmap=\"YlGnBu\",fmt='.1g')\nplt.show()","b5537c99":"num_housing_df.drop(columns=['Id','GarageArea','GarageYrBlt','YearRemodAdd','1stFlrSF','TotRmsAbvGrd','YearBuilt','YrSold','MoSold','PoolArea','LowQualFinSF','MSSubClass','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','MiscVal'],inplace=True),\n     \nplt.figure(figsize = (20, 12))\nsns.heatmap(num_housing_df.corr(), annot = True, cmap=\"YlGnBu\",fmt='.1g')\nplt.show()","7a053cd0":"print('Missing value Columns with Percentage Pre-Imputation')\nprint('****************************************************')\nfor column in cat_housing_df.columns.values:\n    if cat_housing_df[column].isnull().values.sum() != 0:\n        missing_percentage=cat_housing_df[column].isnull().values.sum()\/len(cat_housing_df)\n        print(column, missing_percentage*100)","6ca1a831":"# Remove the Columns with Majority NaN Values in it which Include\ncat_housing_df.drop(columns=['Alley','PoolQC','Fence','MiscFeature'],inplace=True)\ncat_housing_df['FireplaceQu'].fillna('No Fireplace',inplace=True)                  # Categorical Data\ncat_housing_df['MasVnrType'].fillna(stat.mode(cat_housing_df['MasVnrType']),inplace=True)   # Categorical Data\ncat_housing_df['Electrical'].fillna(stat.mode(cat_housing_df['Electrical']),inplace=True)   # Categorical Data\ncat_housing_df['BsmtQual'].fillna('No Basement',inplace=True)                      # Categorical Data     \ncat_housing_df['BsmtCond'].fillna('No Basement',inplace=True)                      # Categorical Data\ncat_housing_df['BsmtExposure'].fillna('No Basement',inplace=True)                  # Categorical Data\ncat_housing_df['BsmtFinType1'].fillna('No Basement',inplace=True)                  # Categorical Data\ncat_housing_df['BsmtFinType2'].fillna('No Basement',inplace=True)                  # Categorical Data\ncat_housing_df['GarageType'].fillna('No Garage',inplace=True)                      # Categorical Data\ncat_housing_df['GarageFinish'].fillna('No Garage',inplace=True)                    # Categorical Data \ncat_housing_df['GarageQual'].fillna('No Garage',inplace=True)                      # Categorical Data\ncat_housing_df['GarageCond'].fillna('No Garage',inplace=True)                      # Categorical Data","9fb0c8a1":"print('missing value Columns with Percentage After Imputation')\nprint('******************************************************')\nfor column in cat_housing_df.columns.values:\n    if cat_housing_df[column].isnull().values.sum() != 0:\n        missing_percentage=cat_housing_df[column].isnull().values.sum()\/len(cat_housing_df)\n        print(column, missing_percentage*100)","86a6e1d1":"for value in cat_housing_df.columns.values:\n    sns.boxplot(x=cat_housing_df[value],y=num_housing_df['SalePrice'])\n    plt.show()","6d6c4670":"for value in cat_housing_df.columns.values:\n    print('Unique_Values in '+ value +'Feature :',cat_housing_df[value].unique())\n    print(len(cat_housing_df[value].unique()))\n    for category in cat_housing_df[value].unique():\n        print('Percentage of '+category+ ':', (len(cat_housing_df[cat_housing_df[value]==category])\/len(cat_housing_df[value]))*100)\n    print('******************************')","4df1d07a":"cat_housing_df.columns.values","fd9f34a3":"# Removing Categorical Data due to Dominance of a Category over others (threshold ~90%)\ncat_housing_df.drop(columns=['Street','Utilities','Condition2','RoofMatl','Heating','Functional','PavedDrive','GarageCond','Electrical','LandSlope'],inplace=True)","528ce780":"print('Count of Numerical Attributes in Housing Sales Dataset : ',len(num_housing_df.columns.values))\nprint('********************************************************')\nprint(num_housing_df.columns.values)\nprint('********************************************************')\nprint('Count of Categorical Attributes in Housing Sales Dataset : ',len(cat_housing_df.columns.values))\nprint('********************************************************')\nprint(cat_housing_df.columns.values)","c5df4925":"#Get the dummy variables for the categorical feature and store it in a new Dataframe\ncat_housing_df_Dummies = pd.get_dummies(cat_housing_df,drop_first = True)\ncat_housing_df_Dummies.shape","4d37f6e7":"hosuing_sales_df=pd.concat([num_housing_df,cat_housing_df_Dummies],axis=1)\nprint('Total No of Attributes\/Features for Feature Engineering',len(hosuing_sales_df.columns.values))","8a05a5f0":"hosuing_sales_df.head()","8d8390a0":"X= hosuing_sales_df.drop('SalePrice',axis=1)\ny= hosuing_sales_df['SalePrice']","045d46cc":"X.shape","62e29bcc":"y.shape","ec83be63":"# We specify this so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\nX_train, X_test,y_train,y_test = train_test_split(X,y, train_size = 0.7, test_size = 0.3, random_state = 100)","2c664684":"print('\" Shape of Post Train-Test Split \" X_train : {0}, X_test : {1}, y_train : {2}, y_test : {3}'.format(X_train.shape, X_test.shape,y_train.shape,y_test.shape))","65071dea":"scaler = StandardScaler()\n\nnumeric_headers=list(num_housing_df.columns.values)\nnumeric_headers.remove('SalePrice')\n\nX_train[numeric_headers] = scaler.fit_transform(X_train[numeric_headers])\nX_train.head()","1b6326a2":"y_train.head()","1362df0d":"X_test[numeric_headers] = scaler.fit_transform(X_test[numeric_headers])\nX_test.head()","38718918":"y_test.head()","4eee8242":"import statsmodels.api as sm\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import r2_score","37f5061e":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 100)            \nrfe = rfe.fit(X_train, y_train)","2a18c152":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","3979c798":"col_sup = X_train.columns[rfe.support_]\ncol_sup","e5fe1a30":"X_train_rfe = X_train[col_sup]","89f36643":"X_train_rfe.shape","d38d2518":"X_test_rfe = X_test[col_sup]","10b278ba":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\nridge = Ridge()\n\n# cross validation :\n\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train_rfe, y_train) ","4077ed51":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results = cv_results[cv_results['param_alpha']<=200]\ncv_results[['param_alpha','params','mean_test_score','mean_train_score']]","290acd9e":"# plotting mean test and train scores with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n# plotting\nplt.figure(figsize=(10,8))\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","0e1daf7c":"alpha = 5.0\nridge = Ridge(alpha=alpha)\nridge.fit(X_train_rfe, y_train)\ny_pred_ridge_train=ridge.predict(X_train_rfe)\nprint('Train R2 Square : ',round(r2_score(y_train,y_pred_ridge_train),2))\ny_pred_ridge_test=ridge.predict(X_test_rfe)\nprint('Test R2 Square : ',round(r2_score(y_test,y_pred_ridge_test),2))","e99e3316":"sns.distplot((y_train-y_pred_ridge_train))","eecaba3e":"#Ridge model parameters\nmodel_parameters = list(ridge.coef_)\nmodel_parameters.insert(0, ridge.intercept_)\nmodel_parameters = [round(x, 2) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nvar_coeff = list(zip(cols, model_parameters))\nvar_coeff = [x for x in var_coeff if abs(x[-1] != 0)]\nvar_coeff","a872f559":"len(var_coeff)","e8204123":"df = pd.DataFrame.from_records(var_coeff, columns =['Features', 'B-Coeff'])\ndf['B-Coeff']=df['B-Coeff'].abs()\ndf=df.sort_values(by=['B-Coeff'],ascending=False)\n#df['B-Coeff']=df['B-Coeff'].abs()\ndf[1:19].plot(x='Features',y='B-Coeff',kind='bar',figsize=(20,10))\nplt.show()","04f59908":"print('Top 5 Predictor Variables using Ridge :',df[1:6].values)","eec81cf3":"lasso = Lasso()\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train_rfe, y_train) ","dd8eeb9e":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results[['param_alpha','params','mean_test_score','mean_train_score']]","0db6fdc2":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.figure(figsize=(10,8))\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","95965b07":"alpha = 0.0001\nlasso = Lasso(alpha=alpha)\nlasso.fit(X_train_rfe, y_train)\ny_pred_lasso_train=lasso.predict(X_train_rfe)\nprint('Train R2 Square : ',round(r2_score(y_train,y_pred_lasso_train),2))\ny_pred_lasso_test=lasso.predict(X_test_rfe)\nprint('Test R2 Square : ',round(r2_score(y_test,y_pred_lasso_test),2))\n#lasso.coef_","32ce9c3c":"sns.distplot((y_train-y_pred_lasso_train))","f7d8d968":"#lasso model parameters\nmodel_parameters = list(lasso.coef_)\nmodel_parameters.insert(0, lasso.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nvar_coeff = list(zip(cols, model_parameters))\nvar_coeff = [x for x in var_coeff if abs(x[-1] != 0)]\nvar_coeff","a3bd006a":"len(var_coeff)","d3695035":"df = pd.DataFrame.from_records(var_coeff, columns =['Features', 'B-Coeff'])\ndf['B-Coeff']=df['B-Coeff'].abs()\ndf=df.sort_values(by=['B-Coeff'],ascending=False)\ndf[1:11].plot(x='Features',y='B-Coeff',kind='bar',figsize=(20,10))\nplt.show()","ed297427":"print('Top 5 Predictor Variables using Lasso :',df[1:6].values)","cb67db3f":"print('r2_score in train dataset:')\nprint('r2_score for ridge:', round(r2_score(y_train, y_pred_ridge_train), 2))\nprint('r2_score for lasso:', round(r2_score(y_train, y_pred_lasso_train), 2))\n\nprint('r2_score in test dataset:')\nprint('r2_score for ridge:', round(r2_score(y_test, y_pred_ridge_test), 2))\nprint('r2_score for lasso:', round(r2_score(y_test, y_pred_lasso_test), 2))","dead57ea":"## Lasso Regularization - L1","1685962f":"### Step 1: Reading and Understanding the Data\n\nLet us first import Libraries for our Analysis","6ec8bf06":"### Author : Srikant Sahoo","1b8da2f2":"### Step 4 : Splitting the Data into Training and Testing Sets\n\nAs you know, the first basic step for regression is performing a train-test split.","0399b1df":"### Observations :\n---------------------\n    1. Id is not a neccessary Attribute for Modeling as it is a unique Data Point.\n    2. The Target Variable\/Response Variable is Right Skewed Hence we will apply log Tranformation for it Follow a Normal   Distribution.\n    3. As of Age of the House when Sold can be Derived Metric where that can be Year Built - Year Sold and thus we can remove two numeric variables and go ahead with just 1.","6bff8cb0":"### Insights :\n--------------\nWe see Above Variables having values Missing \n\nHence First is to Remove the Columns with Majority NaN Values in it which Include :\n    \n    Alley ,  PoolQC , Fence , MiscFeature\n\nNote : * As Per Information From Data Dictionary *\n    \n     FireplaceQu Column has 47 Percent Data Missing which is a Misinterpretation and can be imputed with \n     No Fireplace in Place of NA\n     \n     * GarageFinish can be imputed with No Garage in place of NA\n     \n     * GarageType can be imputed with No Garage in place of NA\n     \n     * GarageQual can be imputed with No Garage in place of NA\n     \n     * GarageCond can be imputed with No Garage in place of NA\n\nRest all Values will Be Imputed with there \"Mode\" Values As it a Categorical Data","4abb356b":"### Initial Observations of Numerical Features :\n---------------------\n\nWith A Threshold of ~ 50 % Corelation with Target Variable (SalePrice), Folowing Features show Highest Corellation:\n\n##### OverallQual ,YearBuilt ,YearRemodAdd ,TotalBsmtSF ,1stFlrSF ,GrLivArea ,FullBath ,GarageCars, GarageArea\n\nWe Also see (TotalBsmtSF & 1stFlrSF) , (GarageCars & GarageArea) , (YearBuilt & YearRemodAdd) and (YearBuilt & As_of_Age) are Highly Corelated that would lead to Multi-Colinearity i.e. Adding Redundant Data to the Model\n\n#### Post Removing Similar Features:\n\nOverallQual , TotalBsmtSF , GrLivArea , GarageCars , As_of_Age , TotRmsAbvGrd\n\nId Column is an Isignifcant Column and only acts as an Identifer. Hence Droping it As well.\n\nRemoving Columns that after Studying the Scatter Plot and Heat Mat Did not Show mUch Co relation with Target Variable.\n\n['YrSold','MoSold','PoolArea','LowQualFinSF','MSSubClass','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','MiscVal']","eae07048":"## Ridge Regression - L2","595f1065":"## Step 5 : Outcomes in Terms of Business Prospects \n\n#### Top 8 Predictors variables that are significant in predicting the price of a house based on Ridge are :\n\n    1.  Age of The House (As of Age) : Derived From Year Sold and Year Built\n    2.  BsmtFinSF2\n    3.  Neighborhood\n    4.  BsmtFullBath\n    5.  MSZoning\n    6.  WoodDeckSF\n    7.  GrLivArea\n    8.  LotShape\n    9.  KitchenAbvGr\n    10. Exterior1st\n\n#### Top 8 Predictors variables that are significant in predicting the price of a house based on Lasso are :\n\n    1.  GrLivArea\n    2.  BsmtFullBath\n    3.  2ndFlrSF\n    4.  BsmtHalfBath\n    5.  Exterior1st\n    6.  BsmtFinSF2\n    7.  MSZoning\n    8.  Neighborhood\n    9.  Age of The House (As of Age) : Derived From Year Sold and Year Built\n    10. KitchenAbvGr","36a9ba26":"### Step 2: Visualising the Data (EDA - Explorartory Data Analysis)\n\nLet's now spend some time doing what is arguably the most important step - **understanding the data**.\n\n- If there is some obvious multicollinearity going on, this is the first place to catch it\n- Here's where you'll also identify if some predictors directly have a strong association with the outcome variable\n\nWe'll visualise our data using `matplotlib` and `seaborn`.","8714c12d":"### Step 3 : Introduce Dummmy Variables based on Data for categorical data - Dummy Encoding ","8e847067":"#### Using RFE as there many Independent Variables - Feature Engineering","d010a779":"### ( Negative Mean Absolute Error - Scoring Method )","622a149d":"#### Imputing Numerical Missing Values","68d555a1":"### Imputing Categorical Missing Values"}}