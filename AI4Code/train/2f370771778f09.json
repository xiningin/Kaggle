{"cell_type":{"a6b4cc3f":"code","1dd1fb7e":"code","6bc30d6a":"code","94f7d170":"code","89c3309c":"code","2c252e9b":"code","554085ca":"code","e6752d70":"code","228beb0a":"code","175cd192":"code","05fc0df5":"markdown","777fe2d5":"markdown","2d8ea8b6":"markdown","060ea83a":"markdown","b97750de":"markdown","ac0879b3":"markdown","bb101bfb":"markdown","b99b743e":"markdown","56c18166":"markdown","0303d1f4":"markdown","02af7f46":"markdown","b7cdf6cc":"markdown"},"source":{"a6b4cc3f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nsns.set_theme(style='white', font_scale=1.5)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","1dd1fb7e":"def read_training_test_data(path_str=\"..\/input\/digit-recognizer\/\", file='train'):\n    import pandas as pd\n    import xarray as xr\n    import numpy as np\n    from pathlib import Path\n    path = Path(path_str)\n    if file == 'train':\n        raw = pd.read_csv(path\/'train.csv')\n        training_labels = raw['label'].to_frame('Label')\n        train = raw.drop('label', axis=1)\n        samples = train.shape[0]\n        pixels = int(np.sqrt(train.shape[1]))\n        train = xr.DataArray(train.values.reshape(\n            (samples, pixels, pixels)), dims=['ImageId', 'y', 'x'])\n        return train, training_labels\n    elif file == 'test':\n        test = pd.read_csv(path\/'test.csv')\n        samples = test.shape[0]\n        pixels = int(np.sqrt(test.shape[1]))\n        test = xr.DataArray(test.values.reshape(\n            (samples, pixels, pixels)), dims=['ImageId', 'y', 'x'])\n        return test\n    elif file == 'submission':\n        filename = 'sample_submission.csv'\n        df = pd.read_csv(path\/filename)\n        return df\n    \ntrain, training_labels = read_training_test_data(file='train')\ntest = read_training_test_data(file='test')\nsubmission_df = read_training_test_data(file='submission')","6bc30d6a":"# one liner plot function using Xarray superb API:\ntrain.isel(ImageId=slice(0, 16)).plot.imshow(col_wrap=4, col='ImageId', cmap='gray', origin='upper')\n# compare the labels by eye:\ntraining_labels.loc[0:16].T","94f7d170":"print('NaNs in train set: {}'.format(train.isnull().sum().item()))\nprint('NaNs in test set: {}'.format(test.isnull().sum().item()))","89c3309c":"sns.catplot(data=training_labels, x='Label', kind='count', aspect=2)","2c252e9b":"def stack_pixels_and_scale(DataArray, kind='std'):\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    # stack 2 dimensions (x,y) to one, i.e., flatten array:\n    X = DataArray.stack(xy=['y', 'x'])\n    # two options to scale, either by std or simply divide by 255, i.e., -->[0,1]\n    if kind == 'std':\n        X = X.astype('float32')\n        X = scaler.fit_transform(X)\n    else:\n        X = X.astype('float32') \/ 255\n    return X\n\nX = stack_pixels_and_scale(train)\nX_test = stack_pixels_and_scale(test)\ny = training_labels['Label']","554085ca":"class ML_Classifier_Switcher(object):\n\n    def pick_model(self, model_name, params={}):\n        \"\"\"Dispatch method\"\"\"\n        # one can define a param_grid attribute for each model and optimize using e.g., gridsearch\n        self.param_grid = None\n        self.params = params\n        method_name = str(model_name)\n        # Get the method from 'self'. Default to a lambda.\n        method = getattr(self, method_name, lambda: \"Invalid ML Model\")\n        return method()\n\n    def SVM(self):\n        from sklearn.svm import SVC\n        import numpy as np\n        # an example for a param_grid:\n        self.param_grid = {'kernel': ['rbf', 'sigmoid', 'linear'],\n                           'C': np.logspace(-2, 2, 10),\n                           'gamma': np.logspace(-5, 1, 14)} \n\n        return SVC(**self.params)\n\n    def MLP(self):\n        import numpy as np\n        from sklearn.neural_network import MLPClassifier\n        return MLPClassifier(random_state=42, **self.params)\n\n    def RF(self):\n        from sklearn.ensemble import RandomForestClassifier\n        return RandomForestClassifier(random_state=42, n_jobs=-1, **self.params)\n\n    def KNN(self):\n        from sklearn.neighbors import KNeighborsClassifier\n        return KNeighborsClassifier(**self.params, n_jobs=-1)","e6752d70":"def model_validate(X, y, model_name='RF', val_size=0.2):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    import time\n    print('Fitting {} model.'.format(model_name))\n    switcher = ML_Classifier_Switcher()\n    model = switcher.pick_model(model_name)\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, stratify=y, random_state=42)\n    start = time.time()\n    model.fit(X_train, y_train)\n    acc = model.score(X_val, y_val)\n    end = time.time()\n    print('Model {} scored {:.2f} accuracy in {:.2f} minutes.'.format(model_name, acc, (end-start)\/60))\n    return acc, model","228beb0a":"model_names = ['KNN', 'RF', 'SVM', 'MLP']\nmodels = []\nfor model_name in model_names:\n    _, fitted_model = model_validate(X,y, model_name=model_name)\n    models.append(fitted_model)\n","175cd192":"def predict_on_test(X_test, model, submission_df, target='Label'):\n    preds_test = model.predict(X_test)\n    submission_df.loc[:, target] = [int(x) for x in preds_test]\n    submission_df.to_csv('submission.csv', index=False)\n    return\npredict_on_test(X_test, models[-1], submission_df)","05fc0df5":"Now, we run all five models to get the best one (without hyper-parameters tuning):","777fe2d5":"First we define a ML model switcher class and a model validate function:","2d8ea8b6":"<a id=\"preprocessing\"><\/a>\n## 2. Preprocessing: pixel normalization","060ea83a":"Let's check if any NaNs are in the data (should not be):","b97750de":"<a id=\"modelling\"><\/a>\n## 3. Modelling: 4 different models","ac0879b3":"Now, let's plot the training labels distribution:","bb101bfb":"OK, so MLP (NN) model was the best with 0.97, and it took 43 seconds per fit. The fastest was RF with 0.96 with 9.6 seconds per fit. SVM was the slowest because it was not parallelized, KNN was the worst. Obviously we can further optimize by hyper parameters tuning, but we'll leave it for now.\nLet's classify the test dataset and submit:","b99b743e":"Let's visualize some of the training set and their labels:","56c18166":"Since we're dealing here with image files, it is best to read them into Numpy arrays. I like to use Xarray which is basically labeled Numpy arrays with a nice interface to Seaborn and Matplotlib. Let's define a function that reads the datasets:","0303d1f4":"<a id=\"motivation\"><\/a>\n## 0. Motivation\nMost of the Digit Recognizer notebooks are solving the classification with CNNs, however i'd like to explore some more classic ML models and time them along with their accuracy.\nSo, we'll try: K nearest neighbors, Support vector machine, Random forest, and a simple Multi layered preceptron.","02af7f46":"Convert to 'float32' dtype, flatten array and scale pixels value to [0,1] for the train and test arrays:","b7cdf6cc":"<a id=\"load\"><\/a>\n## 1. Load datasets, missing values and simple EDA"}}