{"cell_type":{"d50e2f65":"code","1630ee6e":"code","aa612788":"code","06de4ba0":"code","9e52ff3f":"code","9f82e053":"code","556d6d20":"code","7d92089d":"code","de795b6f":"code","44279d27":"code","711b6c4d":"code","15a42a8b":"code","8af2bd62":"code","83d4045c":"code","2af662d2":"code","d38c03a4":"code","9a3b9df7":"code","65d904f9":"markdown","ef7604f8":"markdown","d8cc4b7a":"markdown","2bcdee64":"markdown","68077759":"markdown","0851bcb7":"markdown","4148506e":"markdown","a3577688":"markdown","9e264e40":"markdown","ff9b4c71":"markdown","f3832e50":"markdown","4710896c":"markdown","0bf15f55":"markdown","99a37b62":"markdown","3e96ac22":"markdown","37794f09":"markdown","da289236":"markdown","3a480dce":"markdown","103b1461":"markdown","7ec121f3":"markdown","4c69c0cc":"markdown","abe7722d":"markdown"},"source":{"d50e2f65":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\npd.set_option('precision', 3)\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\n\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nmpl.rcParams['axes.labelsize'] = 14\nmpl.rcParams['axes.titlesize'] = 15\nmpl.rcParams['xtick.labelsize'] = 12\nmpl.rcParams['ytick.labelsize'] = 12\nmpl.rcParams['legend.fontsize'] = 12\n\nprint ('Libraries Loaded!')","1630ee6e":"train_df = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\ntest_df = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')\n\nprint ('Dataframes loaded successfully!\\n')\nprint ('The train set contains {} rows and {} columns'.format(train_df.shape[0], train_df.shape[1]))\nprint (' The test set contains {} rows and {} columns'.format(test_df.shape[0], test_df.shape[1]))","aa612788":"common_cols = train_df.columns & test_df.columns\nprint (\"Column not in common: '{}'\".format(np.setdiff1d(train_df.columns, common_cols)[0]))","06de4ba0":"train_df.head()","9e52ff3f":"train_df.drop(['Id'], axis = 1, inplace = True)\nprint (\"'Id' dropped!\")","9f82e053":"missing_counts = train_df.isnull().sum().sort_values(ascending = False)\nmissing_percent = (train_df.isnull().sum()*100\/train_df.shape[0]).sort_values(ascending = False)\n\nmissing_df = pd.concat([missing_counts, missing_percent], axis = 1, keys = ['Counts', '%'])\n\ndisplay(missing_df.head(20).style.background_gradient(cmap = 'Reds', axis = 0))","556d6d20":"numeric_atts = train_df.select_dtypes(exclude = ['object'])\ncat_atts = train_df.select_dtypes(include = ['object'])\n\nprint ('    Number of Numeric columns: ', len(numeric_atts.columns))\nprint ('Number of Categorical columns: ', len(cat_atts.columns))","7d92089d":"numeric_atts.hist(figsize = (12, 28), layout = (10, 4), bins = 20, \n                  color = 'lightsteelblue', edgecolor = 'firebrick', linewidth = 1.5)\nplt.tight_layout();","de795b6f":"def low_variance(df, thd = 0.95):\n    \n    columns = []\n    for column in df.columns:\n        \n        values = df[column].value_counts(normalize = True) # normalize = True --> relative frequencies of the unique values\n        if (values > thd).sum() > 0:                       # sum > 0 means that there is one value greater than the threshold\n            columns.append(column)\n    \n    return columns\n\n\nthd = 0.95\ncolumns_low_var = low_variance(numeric_atts, thd)\n\nprint ('Numeric attributes with mostly the same value (> {}%): '.format(thd))\nprint (*columns_low_var, sep = ',\\n')","44279d27":"discrete_n_atts = ['OverallQual', 'OverallCond', 'BsmtFullBath', 'FullBath', 'HalfBath', \n                   'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'MoSold', 'YrSold']\ncontinuous_n_atts = np.setdiff1d(numeric_atts.columns, discrete_n_atts)\n\nfig = plt.figure(figsize = (18, 27))\nflierprops = dict(marker = 'o', markerfacecolor = 'r', markeredgecolor = 'k', markersize = 9)\n\nfor index, column in enumerate(continuous_n_atts):\n    \n    plt.subplot(7, 4, index + 1)\n    sns.boxplot(x = column, data = train_df, color = 'mediumseagreen', flierprops = flierprops)\n    plt.xticks(rotation = 90)\n    \nfig.tight_layout()","711b6c4d":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4))\n\nsns.distplot(train_df['SalePrice'], color = 'darkslateblue', ax = ax1)\nsns.boxplot(train_df['SalePrice'], color = 'slateblue', ax = ax2)\nax2.set_xticks([0, 200000, 400000, 600000])\n\n#skewness and kurtosis\nprint('Skewness: {}'.format( np.round(train_df['SalePrice'].skew(), 2) ))\nprint('Kurtosis: {}'.format( np.round(train_df['SalePrice'].kurt(), 2) ))","15a42a8b":"train_df['SalePrice-log'] = np.log1p(train_df['SalePrice'])\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4))\n\nsns.distplot(train_df['SalePrice-log'], color = 'darkslateblue', ax = ax1)\nsns.boxplot(train_df['SalePrice-log'], color = 'darkslateblue', ax = ax2)\n\n#skewness and kurtosis\nprint('Skewness: {}'.format( np.round(train_df['SalePrice-log'].skew(), 2) ))\nprint('Kurtosis: {}'.format( np.round(train_df['SalePrice-log'].kurt(), 2) ))","8af2bd62":"fig = plt.figure(figsize = (18, 37))\n\nfor index, column in enumerate(cat_atts):\n    \n    plt.subplot(11, 4, index + 1)\n    sns.countplot(x = column, data = train_df, palette = 'Set2')\n    plt.xticks(rotation = 90)\n    \nfig.tight_layout();","83d4045c":"thd = 0.95\ncolumns_low_var = low_variance(cat_atts, thd)\n\nprint ('Categorical attributes with mostly the same value (> {}%): '.format(thd))\nprint (*columns_low_var, sep = ',\\n')","2af662d2":"correlations = numeric_atts.select_dtypes(exclude = ['object']).corr()\ncorrelations = correlations[['SalePrice']].sort_values(by = ['SalePrice'], ascending = False)\ncorrelations.style.background_gradient(cmap = 'Blues', axis = 0)","d38c03a4":"fig = plt.figure(figsize = (20, 30))\n\nfor index, column in enumerate(numeric_atts.columns):\n    \n    plt.subplot(8, 5, index+1)\n    sns.regplot(x = column, y = 'SalePrice', data = train_df, ci = None,\n                color = 'steelblue', line_kws = {'color': 'firebrick'})\n\nfig.tight_layout();","9a3b9df7":"plt.figure(figsize = (15, 13))\n\ncorr_matrix = numeric_atts.corr()\nsns.heatmap(corr_matrix, mask = corr_matrix < 0.75, linewidth = 1, cmap = 'Reds');","65d904f9":"# A Quick Look at our Data\n\nThe aim of this section is to familiarize ourselves with the data. We will focus entirely on the **training set** and forget about the test set completely (which ensures we won't add *data snooping* bias). \n\nFirstly, we can take a quick glance at the top five rows of the training set using the `head()` method:","ef7604f8":"# Exploratory Data Analysis \ud83d\udcca\n\n## Numeric Data\n\nBy calling the `hist()` method we can plot a histogram for each numeric attribute:","d8cc4b7a":"- Most categotical features are nominal,\n- Again, some of them have mostly one value\/category (see 'Street' as an example).","2bcdee64":"It would be really tedious to go through the meaning of each column individually. If you want to know more, please read the [data_description.txt](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/data?select=data_description.txt).\n\nWe can drop 'Id' since it is unique to each instance\/observation and doesn't provide any useful information:","68077759":"## Data Types\n\nThere are two types of variables:\n\n1) **Numeric** variables that can be expressed on a numeric scale. There are two basic categories of numeric variables:\n\n- Continuous: can take any value within a range\/interval (e.g. a person's height), and\n- Discrete: can only take certain integer values (e.g. the number of students in a class).\n\n2) **Categorical** variables that can take only a fixed set of values that correspond to a set of possible categories (e.g. gender, country names, etc.). Special cases of categorical variables are:\n\n- Binary: only two categories exist (e.g. male\/female, 1\/0, etc.), and\n- Ordinal: the categories have an order associated with them (e.g. low\/medium\/high). The opposite of ordinal are nominal categories.\n\nDifferent visualization techniques apply to each type, so it's useful to isolate numeric and categorical attributes as separate dataframes:","0851bcb7":"Our target variable deviates from the symmetrical bell curve we would expect from a normal distribution. Specifically, it is right-skewed (similar terms: right-tailed or skewed to the right) since the right tail is longer.\n\nThis could be a problem since many ML algorithms don't do well with data that are not normally distributed. We can correct for this by performing a **log-transformation** of the target variable:","4148506e":"Let's now plot a **boxplot** for every continuous attribute:","a3577688":"# Conclusions\n\nOur kernel came to an end! We can conclude that:\n\n- Our training set has a total of **80 attributes + the target variable** ('SalePrice'),\n- Many attributes contain **missing values**. A closer look reveals that a NaN value may not always indicate a missing value. For **imputation**, we should consult the description of each attribute to select the best strategy,\n- Numeric attributes have **different scales**,\n- Many attributes (both numeric and categorical) display **low variance**,\n- Many attributes contain **outliers**,\n- Bivariate analysis reveals which attributes display a **higher correlation** with 'SalePrice'. It also reveals four sets of highly **multicollinear attributes**.\n\n<br>\n\nA seperate kernel on processing and Machine Learning models will be out soon! \ud83d\ude09\n\n---\n\nFeel free to ask me anything in the comment section. I would also like to hear <font size = 3 color = \"mediumblue\"><b> suggestions <\/b><\/font> for improving my analysis!\n\nPlease <font size = 3 color = \"firebrick\"><b> upvote <\/b><\/font> if you liked this notebook! Thank you!\ud83d\ude09\ud83e\uddd0","9e264e40":"<br>\n\nFinally, we can use plot a **correlation matrix** which can help us identify **multicollinear features**.\n\nWe read in [Investopedia](https:\/\/www.investopedia.com\/terms\/m\/multicollinearity.asp) that '*multicollinearity is the occurrence of high intercorrelations among two or more independent variables in a multiple regression model*'.\n\nThe same source explaing why we should avoid it: '*Multicollinearity can lead to skewed or misleading results when a researcher or analyst attempts to determine how well each independent variable can be used most effectively to predict or understand the dependent variable in a statistical model.*'\n\nIn the following figure, I have used a mask so that only correlations > 0.75 are highlighted:","ff9b4c71":"There are **four sets** of highly correlated features:\n\n- 'GarageArea' and 'GarageCars', just as we expected,\n- 'GarageYrBlt' and 'YearBuilt',\n- 'TotRmsAbvGrd' and 'GrLivArea', and\n- '1stFlrSF' and 'TotalBsmtSF'.\n\nWe should remove one feature from each set during the preprocessing state.\n","f3832e50":"19 attributes contain missing values. Our first instinct would be to discard the first five attributes ('PoolQC' to 'FireplaceQu') since more than 40% is missing. However, if we take a closer look at the description, a NaN value in these columns indicates the absence of what is been described. As an example, NaN in 'PoolQC' simply means that there is no pool.\n\nTherefore, we should **not remove** these values when building our models but instead construct a strategy for **imputation**. For more details, take a look at my other kernel (which will be out soon!)","4710896c":"We won't go into details about each feature, but in general we can observe that:\n\n- Some feautures, such as 'LowQualFinSF' and 'PoolArea', display **low variance** (or variability), which means that they are close to being constant. Therefore, they do not provide any information to a ML model for learning the patterns in the data and they **should be removed**,\n- Different attributes have **different scales** and we need to take care of that at the processing stage, \n- Most attributes are **continuous** (e.g. 'LotFrontage'), but there are some **discrete** ones ('FullBath', 'YrSold', etc.), and \n- The target variable ('SalePrice') **deviates** from the symmetrical bell curve of the **normal distribution**. We will explore this in greater detail later.\n\n<br>\n\nWe can write a function to isolate features with **low variance**:","0bf15f55":"The difference in the number of columns\/attributes is due to the fact that the **training set** is **labeled**, i.e. it contains a column for the price of each house. This set should be used to build our machine learning models, while the test set should be used to see how well our model performs on unseen\/unlabeled data.","99a37b62":"<font size = +3 color = \"#1E1E1E\"><center><b> Housing Prices Competition: Clear and Concise Exploratory Data Analysis \ud83c\udfd8\ufe0f\ud83d\udcb0\ud83d\udcca <\/b><\/center><\/font>\n\n<img src=\"https:\/\/images.unsplash.com\/photo-1464082354059-27db6ce50048?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1350&q=80\" width = 500>\n<center>Photo by Toa Heftiba (Unsplash)<\/center>\n\n# Introduction\n\nHello, readers! Understanding data is one of the most important steps in any Machine Learning process. **Exploratory Data Analysis** (**EDA**) does exacly that!\n\nIn this notebook, we will explore the dataset for the [Housing Prices Competition](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course) on Kaggle. The dataset, compiled by Dean De Cock, contains information about (almost) every aspect of residential homes in Ames, Iowa. The **aim** of our analysis is to:\n\n- Understand which features are important in determining price, and\n- Form a strategy for processing our data before feeding them to a Machine Learning algorithm.\n\nProcessing the data and building Machine Learning models will be the topic of another notebook. Here, we will only study EDA. I have included text to explain my reasoning\/workflow and make this kernel as <font size=+0 color=\"darkviolet\"><b>beginner friendly<\/b><\/font> as possible. I have only assumed you are familiar with different visualization techniques (e.g. boxplots) and key statistical concepts (e.g. normal distribution etc).\n\n<br>\n\nPlease consider <font size=+0 color=\"red\"><b>upvoting<\/b><\/font> if you found it useful! \ud83e\uddd0\n    \n<br>\n\n**Table of Contents**\n\n1. [Introduction](#Introduction)\n2. [Libraries](#Libraries)\n3. [Getting the Data](#Getting-the-Data)\n4. [A Quick Look at our Data](#A-Quick-Look-at-our-Data)\n5. [Exploratory Data Analysis \ud83d\udcca](#Exploratory-Data-Analysis-\ud83d\udcca)\n6. [Conclusions](#Conclusions)","3e96ac22":"Our target variable is highly (positively) correlated with:\n\n- Overall material and finish quality ('OverallQual'),\n- Above grade (ground) living area square feet ('GrLivArea'),\n- Size of garage in car capacity ('GarageCars'),\n- Size of garage in square feet ('GarageArea'), and\n- First Floor square feet ('1stFlrSF')\n\nIt's not a surprise that people pay more for bigger houses and houses with garage space. Note that 'GarageCars' and 'GarageArea' convey similar information, therefore we need to check for **multicollinearity**.\n\n<br>\n\n'SalePrice' is not affected (-0.10 < correlation < 0.10) from:\n\n- 'MoSold': Month Sold,\n- 'YrSold': Year Sold, \n- 'MSSubClass': The building class,\n- . . .\n\n<br>\n\nAdditionally, we can visually extract information about correlations by using Seaborn's `regplot()`. This could also help us detect any non-linear relationships.","37794f09":"# Libraries\n\nWe start by importing the necessary libraries and setting some parameters for the whole notebook (such as parameters for the plots, etc.). We will mainly use:\n\n- Pandas for handling and analysing data, and\n- Seaborn and Matplotlib for data visualization.","da289236":"# Getting the Data\n\nThe data has already been split into a training set ('train.csv') and a test set ('test.csv'). We can use the `read_csv()` method to load them as Pandas dataframes:","3a480dce":"- Most of the attributes suffer from **outliers**, and\n- For some attributes, the min, max and the interquantile range 'collapse' into a line. That's because these attributes consist of mostly one value (for example, see '3SsnPorch' and 'PoolArea').\n\n### SalePrice\n\nLet's have a look at the target variable in greater detail:","103b1461":"## Categorical Data\n","7ec121f3":"## Missing Values\n\nNotice that some columns contain **missing values** ('NaN' i.e. 'Not a Number'). We can see both the count and the percentage of missing values for each column:","4c69c0cc":"We can visually observe that our target variable appears more normally distributed now. Both the skewness (measure of the asymmetry) and kurtosis (measure of the tailedness) of the distribution decreased after the transformation.\n\nNote that we should also take care of skewness present in the predictor variables. This will be done in the preprocessing stage (not shown in this kernel).","abe7722d":"## Bivariate Analysis\n\nSo far, we examined each feature individually. Bivariate analysis refers to the analysis of bivariate data with the goal of determining if a relationship between two features exists.\n\nBy using the `corr()` method, we can calculate how much each feature (linearly-)correlates with 'SalePrice':"}}