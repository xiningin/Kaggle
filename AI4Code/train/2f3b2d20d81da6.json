{"cell_type":{"ebca3204":"code","1cbedf59":"code","3586e9e2":"code","9e147a30":"code","aedee2e8":"code","d181815a":"code","a893bfeb":"code","f6d3f06d":"code","9a25c577":"code","52f1efc7":"code","49446567":"code","605b296a":"code","8059acda":"code","188cde1b":"code","a544a6d6":"code","14af16b4":"code","1e6484bb":"code","8d6932d9":"code","245a336c":"code","9f9fb49e":"code","14fc4272":"code","3b54be23":"code","f633ac3f":"code","a7a94a20":"code","79e51d07":"code","1a4a713a":"code","124ba325":"code","3dfd162a":"code","a214a5f1":"code","29d55549":"code","d2c33f85":"code","a02c20b9":"code","56f001bf":"code","14c72259":"code","bedd4432":"code","5234bbd9":"code","aff014d2":"code","7d37847c":"code","8f1fd7ff":"code","230a7fe4":"code","b42b41fc":"code","fccb47c9":"code","570d41c7":"markdown","aa1e7f63":"markdown","6cb8df0f":"markdown","8a44c717":"markdown","8c46c3b7":"markdown","6e8a582c":"markdown","e9e61ab2":"markdown","88003ea8":"markdown","d9f78e50":"markdown","ed36bf31":"markdown","d0eebc35":"markdown","c8291d99":"markdown","875eb732":"markdown"},"source":{"ebca3204":"### Necessary imports\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import fbeta_score\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import cross_val_score\nimport shap\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","1cbedf59":"df_credit = pd.read_csv(\"..\/input\/german-credit-data-with-risk\/german_credit_data.csv\", index_col=0)\n\n### below renaming is just to make the data consistent with the one on my local\ndf_credit.rename(columns = {'Checking account': 'Credit History', 'Sex': 'Gender'}, inplace=True)\n\ny = df_credit['Risk']\nX = df_credit.drop(columns = ['Risk'])\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1)\n\ndf_train = pd.concat([X_train, y_train], axis = 1)\ndf_test = pd.concat([X_test, y_test], axis = 1)\n\nprint(df_train.shape, df_test.shape)","3586e9e2":"df_train.head()","9e147a30":"df_train.loc[:, 'Credit History'].hist(alpha=0.5, label='Train', density=True)    \ndf_test.loc[:, 'Credit History'].hist(alpha=0.5, label='Test', density=True)\nplt.xlabel('Credit History')\nplt.legend()","aedee2e8":"df_train.loc[:, 'Age'].hist(alpha=0.5, label='Train', density=True)    \ndf_test.loc[:, 'Age'].hist(alpha=0.5, label='Test', density=True)  \nplt.xlabel('Age')\nplt.legend()","d181815a":"df_train.loc[:, 'Job'].hist(alpha=0.5, label='Train', density=True)    \ndf_test.loc[:, 'Job'].hist(alpha=0.5, label='Test', density=True)\nplt.xlabel('Job')\nplt.legend()","a893bfeb":"df_train.loc[:, 'Duration'].hist(alpha=0.5, label='Train', density=True)    \ndf_test.loc[:, 'Duration'].hist(alpha=0.5, label='Test', density=True)\nplt.xlabel('Duration')\nplt.legend()","f6d3f06d":"df_train.loc[:, 'Credit amount'].hist(alpha=0.5, label='Train', density=True)    \ndf_test.loc[:, 'Credit amount'].hist(alpha=0.5, label='Test', density=True)\nplt.xlabel('Credit amount')\nplt.legend()","9a25c577":"df_train.loc[:, 'Saving accounts'].hist(alpha=0.5, label='Train', density=True)    \ndf_test.loc[:, 'Saving accounts'].hist(alpha=0.5, label='Test', density=True)\nplt.xlabel('Saving accounts')\nplt.legend()","52f1efc7":"from scipy.stats import ks_2samp\nks_2samp(df_train['Age'], df_test['Age'])\nks_2samp(df_train['Credit amount'], df_test['Credit amount'])","49446567":"df_train.head()","605b296a":"df_train.nunique()","8059acda":"df_train.Risk.value_counts() ### Good = 1 (credit worthy), Bad = 0 (not worthy)","188cde1b":"df_train.Risk.value_counts()","a544a6d6":"df_train_summ = df_train.describe()\ndf_train_summ","14af16b4":"plt.hist(df_train['Credit amount'])\nplt.title('Credit amount distribution')","1e6484bb":"### the credit amount is highly skewed distribution, lets analyse the extreme values beyond 3 sigma\ndef extreme_count(sig_factor, feat):\n    sig_cutoff = df_train_summ[feat]['mean'] + sig_factor*df_train_summ[feat]['std'] \n    sig_count = len(df_train[df_train[feat] > sig_cutoff])\n    print(\"instances of {} greater than {} sigma ({} cutoff) are {}\".format(feat, sig_factor, sig_cutoff, sig_count))\n    return\n\nextreme_count(3, feat = 'Credit amount')","8d6932d9":"plt.hist(df_train['Age'])\nplt.title('Age distribution')\nextreme_count(3, feat = 'Age')","245a336c":"plt.hist(df_train['Duration'])\nplt.title('Duration distribution')\nextreme_count(3, feat = 'Duration')","9f9fb49e":"''' Even if there are certain instances where the above features are beyond 3sigma of their mean value, they dont appear to be \noutliers, as its legible to have certain certain loans with high credit value, or loan duration is longer, or older population \nis seeking loan. Hence, not eliminating these rows'''","14fc4272":"df_train.isnull().sum()","3b54be23":"### NaN is a valid field here implying no saving account\n\ndf_train['Saving accounts'].value_counts()\ndf_train['Saving accounts'].unique()\n\n### So, replacing NaN with 'no account'\ndf_train.loc[df_train['Saving accounts'].isnull(), 'Saving accounts'] = 'no account'\ndf_train.loc[df_train['Credit History'].isnull(), 'Credit History'] = 'no history'\n\n### Replaced in df\ndf_train['Saving accounts'].value_counts()\ndf_train['Saving accounts'].unique()\n\n### No NaNs anymore\ndf_train.isnull().sum()","f633ac3f":"df_train.dtypes","a7a94a20":"df_train.nunique()","79e51d07":"df_dtypes = pd.DataFrame((df_credit.dtypes == 'object'), columns = ['obj_type'])\nobj_list = df_dtypes[(df_dtypes.obj_type == True)].index\nprint(\"Features for label encoding:\", obj_list)","1a4a713a":"df_train[obj_list].head()","124ba325":"def le_col(df, col):\n    le = LabelEncoder()\n    le.fit(df[col])\n    df[col] = le.transform(df[col])\n    return df, le\n\ndf_train, le_gender = le_col(df_train, 'Gender')\ndf_train, le_housing = le_col(df_train, 'Housing')\ndf_train, le_sa = le_col(df_train, 'Saving accounts')\ndf_train, le_purpose = le_col(df_train, 'Purpose')\ndf_train, le_ch = le_col(df_train, 'Credit History')\ndf_train, le_risk = le_col(df_train, 'Risk')\n\ndf_train[obj_list].head()","3dfd162a":"df_train.Risk.value_counts()","a214a5f1":"df_train.corr()","29d55549":"plt.hist([df_train.loc[df_train['Risk'] == 0, 'Credit History'].values, df_train.loc[df_train['Risk'] == 1, 'Credit History'].values], alpha=0.5, label=['Bad Risk', 'Good Risk'])\nplt.legend(loc='upper right')","d2c33f85":"df_train[df_train['Risk'] == 0]['Credit History'].value_counts()","a02c20b9":"df_train[df_train['Risk'] == 1]['Credit History'].value_counts()","56f001bf":"plt.hist([df_train.loc[df_train['Risk'] == 0, 'Age'].values, df_train.loc[df_train['Risk'] == 1, 'Age'].values], alpha=0.5, label=['Bad Risk', 'Good Risk'])\nplt.legend(loc='upper right')","14c72259":"df_train.Age[df_train.Age <= 30] = 0\ndf_train.Age[(df_train.Age > 30) & (df_train.Age < 45)] = 1\ndf_train.Age[(df_train.Age >= 45)] = 2","bedd4432":"df_train[df_train['Risk'] == 0]['Age'].value_counts()\ndf_train[df_train['Risk'] == 1]['Age'].value_counts()","5234bbd9":"y_train = df_train['Risk']\nX_train = df_train.drop(columns = ['Risk'])","aff014d2":"df_test.isnull().sum()\n### So, replacing NaN with 'no account' and 'no history'\ndf_test.loc[df_test['Saving accounts'].isnull(), 'Saving accounts'] = 'no account'\ndf_test.loc[df_test['Credit History'].isnull(), 'Credit History'] = 'no history'\ndf_test.isnull().sum()","7d37847c":"df_test['Gender'] = le_gender.transform(df_test['Gender'])\ndf_test['Housing'] = le_housing.transform(df_test['Housing'])\ndf_test['Saving accounts'] = le_sa.transform(df_test['Saving accounts'])\ndf_test['Purpose'] = le_purpose.transform(df_test['Purpose'])\ndf_test['Credit History'] = le_ch.transform(df_test['Credit History'])\n\ndf_test['Risk'] = le_risk.transform(df_test['Risk'])","8f1fd7ff":"df_test.head()","230a7fe4":"y_test = df_test['Risk']\nX_test = df_test.drop(columns = ['Risk'])","b42b41fc":"import numpy as np\nfrom sklearn.metrics import fbeta_score, make_scorer\nftwo_scorer = make_scorer(fbeta_score, beta=1\/5)","fccb47c9":"### Assuming, it is bad to classify a customer as good when they are bad i.e. objective is to reduce FP, we want better precision \n### Hence, applying beta = 1\/5 and selecting fbeta_score as evaluation metric\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nfbeta_score(y_test, y_pred, beta=1\/5)","570d41c7":"### Checking data types and categorical states of features for encoding","aa1e7f63":"### Modelling","6cb8df0f":"### Label Encoding","8a44c717":"### A beginner's guide to model the German credit Risk data","8c46c3b7":"### EDA 2 : Are young people more credit worthy?","6e8a582c":"### Distribution of Risk variable","e9e61ab2":"### Finding Missing values, checking if they are legitimate and applying apt transformation","88003ea8":"### Feature Selection for label and one hot encoding","d9f78e50":"Test data prep","ed36bf31":"### EDA 1 : More credit history is equivalent to credit worthiness\n\nConclusion: As the credit history increases, the good risk increases proportionately i..e credit worthiness improves sharply","d0eebc35":"X_train, y_train prep","c8291d99":"### Preliminary data analysis","875eb732":"Train and Test Distribution comparison "}}