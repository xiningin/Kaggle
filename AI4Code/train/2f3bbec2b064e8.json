{"cell_type":{"b5ac9553":"code","c565d441":"code","9a755975":"code","5a800f85":"code","4d47cc7f":"code","4dc9b105":"code","98d5046b":"code","65f78e36":"code","e245b614":"code","58c0e877":"code","75d92c11":"code","a165b268":"code","adc3e11a":"code","332506d0":"code","abf6e1a1":"code","59b1aa4b":"code","1f683114":"code","c34b76ef":"code","0e8f6100":"code","b93953b7":"code","34fbd75b":"code","04e991de":"code","ea1e0327":"code","11ce4407":"code","4fc38e48":"code","ff69fd77":"code","cb60a48f":"code","f24e15e6":"code","d20d01ca":"code","690c8db5":"code","a619ac65":"code","dc5a746b":"code","8919f1e9":"code","0c201af3":"code","45c187e8":"code","06e30e2b":"code","c1ebbd46":"code","7717865f":"code","1698eead":"code","4f6ec838":"code","82c4baa3":"code","5fa12025":"code","d2a0cd59":"code","4fefe22c":"code","a51199e6":"code","333c49f9":"code","2979bbea":"code","24700bc8":"code","0b681d27":"code","449750aa":"code","742cf2bf":"code","520f785a":"code","ce51a9d0":"code","764799c3":"code","6919914f":"code","9c41f987":"code","45f6c4ef":"code","5683c809":"code","99f822db":"code","7a9a17eb":"code","eabb7ca9":"code","2f81eb95":"code","4b3ad802":"code","41e128ef":"code","143b8a89":"code","99cebcd4":"code","43577711":"code","060b6697":"code","cfe22780":"code","ee9c375d":"code","9d73f8ce":"code","fd08dee0":"code","18004dc4":"code","929674a8":"code","8d65622f":"code","c85f9ca5":"code","2a8191d0":"code","f3dbfd97":"code","1f5169ad":"code","8d425e09":"code","8b57781b":"code","041f0ac0":"code","fed0dd04":"code","e8d488bb":"code","c95ef00b":"code","879c59a8":"code","a5899b93":"code","a14606b2":"code","287aa2d2":"code","2ee09c34":"code","a2b505ae":"code","f866bd36":"code","97af1b54":"code","02a109f1":"code","3be590a3":"code","bdda06c0":"code","2bb5773e":"markdown","8167091b":"markdown","cba11219":"markdown","4bb70e32":"markdown","0fbdb845":"markdown","1bf754c2":"markdown","7fb8c2f3":"markdown","7c5b6adc":"markdown","4d14188b":"markdown","4fe56466":"markdown","4c843f9d":"markdown","eb98cdef":"markdown","ee832a5d":"markdown","5598210b":"markdown","756b1a47":"markdown","da798d2d":"markdown","6962e04f":"markdown","18a9581e":"markdown","3b74c1ae":"markdown","09cb5f03":"markdown","2c36f1a6":"markdown","3dfcc841":"markdown","35f775df":"markdown","b3b3c677":"markdown","1f8498f1":"markdown","990b5a6f":"markdown","6570b1dc":"markdown","c82e0db0":"markdown","19d11018":"markdown","48d98cf8":"markdown","a3338e93":"markdown","8ec20869":"markdown","09fbac47":"markdown","51f938eb":"markdown","45d0dde3":"markdown","7a270a67":"markdown","db481613":"markdown","f90593fb":"markdown","ea3b5664":"markdown","27e7eafb":"markdown","90917b60":"markdown","4e4e0345":"markdown","980f2d75":"markdown","7a5b92f3":"markdown","0315c560":"markdown","88069e0f":"markdown","fb3939b5":"markdown","e0a71398":"markdown","d87d2c71":"markdown","0d118d0c":"markdown","3726e0eb":"markdown","7e6a70b1":"markdown","dab3e630":"markdown","182308e6":"markdown","a82b5175":"markdown","b9d5f86d":"markdown","a0aaa931":"markdown","18db4d98":"markdown","2c2289f0":"markdown","39c5dbe9":"markdown","f3b4cc48":"markdown","bd05a90f":"markdown","3717c0c6":"markdown","72de35ee":"markdown","d843747d":"markdown","2789f4f0":"markdown","d71b2d98":"markdown","a8a9bb4f":"markdown","35ea8885":"markdown","2f549e35":"markdown","0d619f32":"markdown","2741daf8":"markdown","1ad27bf4":"markdown","b6c9a033":"markdown","197cde01":"markdown","1836e99e":"markdown","edd3cb0f":"markdown","0064867c":"markdown"},"source":{"b5ac9553":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\n# plotly library\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n#warnings.filterwarnings(\"ignore\")\n#warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n#warnings.filterwarnings(action='once')\n\nfrom sklearn.utils.testing import ignore_warnings\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","c565d441":"def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn","9a755975":"def get_best_score(model):\n    \n    print(model.best_score_)    \n    print(model.best_params_)\n    print(model.best_estimator_)\n    \n    return model.best_score_\n\n\ndef plot_feature_importances(model, columns):\n    nr_f = 10\n    imp = pd.Series(data = model.best_estimator_.feature_importances_, \n                    index=columns).sort_values(ascending=False)\n    plt.figure(figsize=(7,5))\n    plt.title(\"Feature importance\")\n    ax = sns.barplot(y=imp.index[:nr_f], x=imp.values[:nr_f], orient='h')","5a800f85":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","4d47cc7f":"df_train.head()","4dc9b105":"df_test.head()","98d5046b":"df_train.info()","65f78e36":"df_test.info()","e245b614":"fig, ax = plt.subplots(figsize=(9,5))\nsns.heatmap(df_train.isnull(), cbar=False, cmap=\"YlGnBu_r\")\nplt.show()","58c0e877":"fig, ax = plt.subplots(figsize=(9,5))\nsns.heatmap(df_test.isnull(), cbar=False, cmap=\"YlGnBu_r\")\nplt.show()","75d92c11":"cols = ['Survived', 'Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']","a165b268":"nr_rows = 2\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        \n        i = r*nr_cols+c       \n        ax = axs[r][c]\n        sns.countplot(df_train[cols[i]], hue=df_train[\"Survived\"], ax=ax)\n        ax.set_title(cols[i], fontsize=14, fontweight='bold')\n        ax.legend(title=\"survived\", loc='upper center') \n        \nplt.tight_layout()   ","adc3e11a":"bins = np.arange(0, 80, 5)\ng = sns.FacetGrid(df_train, row='Sex', col='Pclass', hue='Survived', margin_titles=True, size=3, aspect=1.1)\ng.map(sns.distplot, 'Age', kde=False, bins=bins, hist_kws=dict(alpha=0.6))\ng.add_legend()  \nplt.show()  ","332506d0":"df_train['Fare'].max()","abf6e1a1":"bins = np.arange(0, 550, 50)\ng = sns.FacetGrid(df_train, row='Sex', col='Pclass', hue='Survived', margin_titles=True, size=3, aspect=1.1)\ng.map(sns.distplot, 'Fare', kde=False, bins=bins, hist_kws=dict(alpha=0.6))\ng.add_legend()  \nplt.show()  ","59b1aa4b":"sns.barplot(x='Pclass', y='Survived', data=df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Pclass\")\nplt.show()","1f683114":"sns.barplot(x='Sex', y='Survived', hue='Pclass', data=df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Pclass and Sex\")\nplt.show()","c34b76ef":"sns.barplot(x='Embarked', y='Survived', data=df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Embarked Port\")\nplt.show()","0e8f6100":"sns.barplot(x='Embarked', y='Survived', hue='Pclass', data=df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Embarked Port\")\nplt.show()","b93953b7":"sns.countplot(x='Embarked', hue='Pclass', data=df_train)\nplt.title(\"Count of Passengers as function of Embarked Port\")\nplt.show()","34fbd75b":"sns.boxplot(x='Embarked', y='Age', data=df_train)\nplt.title(\"Age distribution as function of Embarked Port\")\nplt.show()","04e991de":"sns.boxplot(x='Embarked', y='Fare', data=df_train)\nplt.title(\"Fare distribution as function of Embarked Port\")\nplt.show()","ea1e0327":"cm_surv = [\"darkgrey\" , \"lightgreen\"]","11ce4407":"fig, ax = plt.subplots(figsize=(13,7))\nsns.swarmplot(x='Pclass', y='Age', hue='Survived', split=True, data=df_train , palette=cm_surv, size=7, ax=ax)\nplt.title('Survivals for Age and Pclass ')\nplt.show()","4fc38e48":"fig, ax = plt.subplots(figsize=(13,7))\nsns.violinplot(x=\"Pclass\", y=\"Age\", hue='Survived', data=df_train, split=True, bw=0.05 , palette=cm_surv, ax=ax)\nplt.title('Survivals for Age and Pclass ')\nplt.show()","ff69fd77":"g = sns.factorplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", col=\"Sex\", data=df_train, kind=\"swarm\", split=True, palette=cm_surv, size=7, aspect=.9, s=7)","cb60a48f":"g = sns.factorplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", col=\"Sex\", data=df_train, kind=\"violin\", split=True, bw=0.05, palette=cm_surv, size=7, aspect=.9, s=7)","f24e15e6":"for df in [df_train, df_test] :\n    \n    df['FamilySize'] = df['SibSp'] + df['Parch'] +1\n    \n    df['Alone']=0\n    df.loc[(df.FamilySize==1),'Alone'] = 1\n    \n    df['NameLen'] = df.Name.apply(lambda x : len(x)) \n    df['NameLenBin']=np.nan\n    for i in range(20,0,-1):\n        df.loc[ df['NameLen'] <= i*5, 'NameLenBin'] = i\n    \n    \n    df['Title']=0\n    df['Title']=df.Name.str.extract(r'([A-Za-z]+)\\.') #lets extract the Salutations\n    df['Title'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n                    ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)","d20d01ca":"print(df_train[['NameLen' , 'NameLenBin']].head(10))","690c8db5":"grps_namelenbin_survrate = df_train.groupby(['NameLenBin'])['Survived'].mean().to_frame()\ngrps_namelenbin_survrate","a619ac65":"plt.subplots(figsize=(10,6))\nsns.barplot(x='NameLenBin' , y='Survived' , data = df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of NameLenBin\")\nplt.show()","dc5a746b":"fig, ax = plt.subplots(figsize=(9,7))\nsns.violinplot(x=\"NameLenBin\", y=\"Pclass\", data=df_train, hue='Survived', split=True, \n               orient=\"h\", bw=0.2 , palette=cm_surv, ax=ax)\nplt.show()","8919f1e9":"g = sns.factorplot(x=\"NameLenBin\", y=\"Survived\", col=\"Sex\", data=df_train, kind=\"bar\", size=5, aspect=1.2)","0c201af3":"grps_title_survrate = df_train.groupby(['Title'])['Survived'].mean().to_frame()\ngrps_title_survrate","45c187e8":"plt.subplots(figsize=(10,6))\nsns.barplot(x='Title' , y='Survived' , data = df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Title\")\nplt.show()","06e30e2b":"pd.crosstab(df_train.FamilySize,df_train.Survived).apply(lambda r: r\/r.sum(), axis=1).style.background_gradient(cmap='summer_r')","c1ebbd46":"plt.subplots(figsize=(10,6))\nsns.barplot(x='FamilySize' , y='Survived' , data = df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of FamilySize\")\nplt.show()","7717865f":"for df in [df_train, df_test]:\n\n    # Title\n    df['Title'] = df['Title'].fillna(df['Title'].mode().iloc[0])\n\n    # Age: use Title to fill missing values\n    df.loc[(df.Age.isnull())&(df.Title=='Mr'),'Age']= df.Age[df.Title==\"Mr\"].mean()\n    df.loc[(df.Age.isnull())&(df.Title=='Mrs'),'Age']= df.Age[df.Title==\"Mrs\"].mean()\n    df.loc[(df.Age.isnull())&(df.Title=='Master'),'Age']= df.Age[df.Title==\"Master\"].mean()\n    df.loc[(df.Age.isnull())&(df.Title=='Miss'),'Age']= df.Age[df.Title==\"Miss\"].mean()\n    df.loc[(df.Age.isnull())&(df.Title=='Other'),'Age']= df.Age[df.Title==\"Other\"].mean()\n    df = df.drop('Name', axis=1)\n\n\n","1698eead":"# Embarked\ndf_train['Embarked'] = df_train['Embarked'].fillna(df_train['Embarked'].mode().iloc[0])\ndf_test['Embarked'] = df_test['Embarked'].fillna(df_test['Embarked'].mode().iloc[0])\n\n# Fare\ndf_train['Fare'] = df_train['Fare'].fillna(df_train['Fare'].mean())\ndf_test['Fare'] = df_test['Fare'].fillna(df_test['Fare'].mean())","4f6ec838":"for df in [df_train, df_test]:\n    \n    df['Age_bin']=np.nan\n    for i in range(8,0,-1):\n        df.loc[ df['Age'] <= i*10, 'Age_bin'] = i\n        \n    df['Fare_bin']=np.nan\n    for i in range(12,0,-1):\n        df.loc[ df['Fare'] <= i*50, 'Fare_bin'] = i        \n    \n    # convert Title to numerical\n    df['Title'] = df['Title'].map( {'Other':0, 'Mr': 1, 'Master':2, 'Miss': 3, 'Mrs': 4 } )\n    # fill na with maximum frequency mode\n    df['Title'] = df['Title'].fillna(df['Title'].mode().iloc[0])\n    df['Title'] = df['Title'].astype(int)        ","82c4baa3":"df_train_ml = df_train.copy()\ndf_test_ml = df_test.copy()\n\npassenger_id = df_test_ml['PassengerId']","5fa12025":"df_train_ml.info()","d2a0cd59":"df_test_ml.info()","4fefe22c":"df_train_ml = pd.get_dummies(df_train_ml, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\ndf_test_ml = pd.get_dummies(df_test_ml, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\n\ndf_train_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age', 'Fare_bin'],axis=1,inplace=True)\ndf_test_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age', 'Fare_bin'],axis=1,inplace=True)\n\n#df_train_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age_bin', 'Fare_bin'],axis=1,inplace=True)\n#df_test_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age_bin', 'Fare_bin'],axis=1,inplace=True)\n","a51199e6":"df_train_ml.dropna(inplace=True)","333c49f9":"for df in [df_train_ml, df_test_ml]:\n    df.drop(['NameLen'], axis=1, inplace=True)\n\n    df.drop(['SibSp'], axis=1, inplace=True)\n    df.drop(['Parch'], axis=1, inplace=True)\n    df.drop(['Alone'], axis=1, inplace=True)","2979bbea":"df_train_ml.head()","24700bc8":"df_test_ml.fillna(df_test_ml.mean(), inplace=True)\ndf_test_ml.head()","0b681d27":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# for df_train_ml\nscaler.fit(df_train_ml.drop(['Survived'],axis=1))\nscaled_features = scaler.transform(df_train_ml.drop(['Survived'],axis=1))\ndf_train_ml_sc = pd.DataFrame(scaled_features) # columns=df_train_ml.columns[1::])\n\n# for df_test_ml\ndf_test_ml.fillna(df_test_ml.mean(), inplace=True)\n#scaler.fit(df_test_ml)\nscaled_features = scaler.transform(df_test_ml)\ndf_test_ml_sc = pd.DataFrame(scaled_features) # , columns=df_test_ml.columns)","449750aa":"df_train_ml_sc.head()","742cf2bf":"df_test_ml_sc.head()","520f785a":"df_train_ml.head()","ce51a9d0":"X = df_train_ml.drop('Survived', axis=1)\ny = df_train_ml['Survived']\nX_test = df_test_ml\n\nX_sc = df_train_ml_sc\ny_sc = df_train_ml['Survived']\nX_test_sc = df_test_ml_sc","764799c3":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import tree\n\n\nfrom sklearn.metrics import accuracy_score\n","6919914f":"from sklearn.model_selection import cross_val_score","9c41f987":"svc = SVC(gamma = 0.01, C = 100)\nscores_svc = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\nprint(scores_svc)\nprint(scores_svc.mean())","45f6c4ef":"svc = SVC(gamma = 0.01, C = 100)\nscores_svc_sc = cross_val_score(svc, X_sc, y_sc, cv=10, scoring='accuracy')\nprint(scores_svc_sc)\nprint(scores_svc_sc.mean())","5683c809":"rfc = RandomForestClassifier(max_depth=5, max_features=6)\nscores_rfc = cross_val_score(rfc, X, y, cv=10, scoring='accuracy')\nprint(scores_rfc)\nprint(scores_rfc.mean())","99f822db":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import uniform","7a9a17eb":"model = SVC()\nparam_grid = {'C':uniform(0.1, 5000), 'gamma':uniform(0.0001, 1) }\nrand_SVC = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=100)\nrand_SVC.fit(X_sc,y_sc)\nscore_rand_SVC = get_best_score(rand_SVC)","eabb7ca9":"param_grid = {'C': [0.1,10, 100, 1000,5000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}\nsvc_grid = GridSearchCV(SVC(), param_grid, cv=10, refit=True, verbose=1)\nsvc_grid.fit(X_sc,y_sc)\nsc_svc = get_best_score(svc_grid)","2f81eb95":"pred_all_svc = svc_grid.predict(X_test_sc)\n\nsub_svc = pd.DataFrame()\nsub_svc['PassengerId'] = df_test['PassengerId']\nsub_svc['Survived'] = pred_all_svc\nsub_svc.to_csv('svc.csv',index=False)","4b3ad802":"knn = KNeighborsClassifier()\nleaf_range = list(range(3, 15, 1))\nk_range = list(range(1, 15, 1))\nweight_options = ['uniform', 'distance']\nparam_grid = dict(leaf_size=leaf_range, n_neighbors=k_range, weights=weight_options)\nprint(param_grid)\n\nknn_grid = GridSearchCV(knn, param_grid, cv=10, verbose=1, scoring='accuracy')\nknn_grid.fit(X, y)\n\nsc_knn = get_best_score(knn_grid)","41e128ef":"pred_all_knn = knn_grid.predict(X_test)\n\nsub_knn = pd.DataFrame()\nsub_knn['PassengerId'] = df_test['PassengerId']\nsub_knn['Survived'] = pred_all_knn\nsub_knn.to_csv('knn.csv',index=False)","143b8a89":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\n\nparam_grid = {'min_samples_split': [4,7,10,12]}\ndtree_grid = GridSearchCV(dtree, param_grid, cv=10, refit=True, verbose=1)\ndtree_grid.fit(X_sc,y_sc)\n\nprint(dtree_grid.best_score_)\nprint(dtree_grid.best_params_)\nprint(dtree_grid.best_estimator_)","99cebcd4":"pred_all_dtree = dtree_grid.predict(X_test_sc)\n\nsub_dtree = pd.DataFrame()\nsub_dtree['PassengerId'] = df_test['PassengerId']\nsub_dtree['Survived'] = pred_all_dtree\nsub_dtree.to_csv('dtree.csv',index=False)","43577711":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\n\nparam_grid = {'max_depth': [3, 5, 6, 7, 8], 'max_features': [6,7,8,9,10],  \n              'min_samples_split': [5, 6, 7, 8]}\n\nrf_grid = GridSearchCV(rfc, param_grid, cv=10, refit=True, verbose=1)\nrf_grid.fit(X_sc,y_sc)\nsc_rf = get_best_score(rf_grid)","060b6697":"plot_feature_importances(rf_grid, X.columns)","cfe22780":"pred_all_rf = rf_grid.predict(X_test_sc)\n\nsub_rf = pd.DataFrame()\nsub_rf['PassengerId'] = df_test['PassengerId']\nsub_rf['Survived'] = pred_all_rf\nsub_rf.to_csv('rf.csv',index=False)","ee9c375d":"from sklearn.ensemble import ExtraTreesClassifier\nextr = ExtraTreesClassifier()\n\nparam_grid = {'max_depth': [6,7,8,9], 'max_features': [7,8,9,10],  \n              'n_estimators': [50, 100, 200]}\n\nextr_grid = GridSearchCV(extr, param_grid, cv=10, refit=True, verbose=1)\nextr_grid.fit(X_sc,y_sc)\nsc_extr = get_best_score(extr_grid)","9d73f8ce":"pred_all_extr = extr_grid.predict(X_test_sc)\n\nsub_extr = pd.DataFrame()\nsub_extr['PassengerId'] = df_test['PassengerId']\nsub_extr['Survived'] = pred_all_extr\nsub_extr.to_csv('extr.csv',index=False)","fd08dee0":"from sklearn.ensemble import GradientBoostingClassifier\ngbdt = GradientBoostingClassifier()\n\nparam_grid = {'n_estimators': [50, 100], \n              'min_samples_split': [3, 4, 5, 6, 7],\n              'max_depth': [3, 4, 5, 6]}\ngbdt_grid = GridSearchCV(gbdt, param_grid, cv=10, refit=True, verbose=1)\ngbdt_grid.fit(X_sc,y_sc)\nsc_gbdt = get_best_score(gbdt_grid)","18004dc4":"plot_feature_importances(gbdt_grid, X.columns)","929674a8":"pred_all_gbdt = gbdt_grid.predict(X_test_sc)\n\nsub_gbdt = pd.DataFrame()\nsub_gbdt['PassengerId'] = df_test['PassengerId']\nsub_gbdt['Survived'] = pred_all_gbdt\nsub_gbdt.to_csv('gbdt.csv',index=False)","8d65622f":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nparam_grid = {'max_depth': [5,6,7,8], 'gamma': [1, 2, 4], 'learning_rate': [0.1, 0.2, 0.3, 0.5]}\n\nwith ignore_warnings(category=DeprecationWarning):\n    xgb_grid = GridSearchCV(xgb, param_grid, cv=10, refit=True, verbose=1)\n    xgb_grid.fit(X_sc,y_sc)\n    sc_xgb = get_best_score(xgb_grid)","c85f9ca5":"plot_feature_importances(xgb_grid, X.columns)","2a8191d0":"with ignore_warnings(category=DeprecationWarning):\n    pred_all_xgb = xgb_grid.predict(X_test_sc)\n\nsub_xgb = pd.DataFrame()\nsub_xgb['PassengerId'] = df_test['PassengerId']\nsub_xgb['Survived'] = pred_all_xgb\nsub_xgb.to_csv('xgb.csv',index=False)","f3dbfd97":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier()\n\nparam_grid = {'n_estimators': [30, 50, 100], 'learning_rate': [0.08, 0.1, 0.2]}\nada_grid = GridSearchCV(ada, param_grid, cv=10, refit=True, verbose=1)\nada_grid.fit(X_sc,y_sc)\nsc_ada = get_best_score(ada_grid)\n\npred_all_ada = ada_grid.predict(X_test_sc)","1f5169ad":"sub_ada = pd.DataFrame()\nsub_ada['PassengerId'] = df_test['PassengerId']\nsub_ada['Survived'] = pred_all_ada\nsub_ada.to_csv('ada.csv',index=False)","8d425e09":"from catboost import CatBoostClassifier\ncat=CatBoostClassifier()\n\nparam_grid = {'iterations': [100, 150], 'learning_rate': [0.3, 0.4, 0.5], 'loss_function' : ['Logloss']}\n\ncat_grid = GridSearchCV(cat, param_grid, cv=10, refit=True, verbose=1)\ncat_grid.fit(X_sc,y_sc, verbose=False)\nsc_cat = get_best_score(cat_grid)\n\npred_all_cat = cat_grid.predict(X_test_sc)","8b57781b":"sub_cat = pd.DataFrame()\nsub_cat['PassengerId'] = df_test['PassengerId']\nsub_cat['Survived'] = pred_all_cat\nsub_cat['Survived'] = sub_cat['Survived'].astype(int)\nsub_cat.to_csv('cat.csv',index=False)","041f0ac0":"import lightgbm as lgb\nlgbm = lgb.LGBMClassifier(silent=False)\nparam_grid = {\"max_depth\": [8,10,15], \"learning_rate\" : [0.008,0.01,0.012], \n              \"num_leaves\": [80,100,120], \"n_estimators\": [200,250]  }\nlgbm_grid = GridSearchCV(lgbm, param_grid, cv=10, refit=True, verbose=1)\nlgbm_grid.fit(X_sc,y_sc, verbose=True)\nsc_lgbm = get_best_score(lgbm_grid)\n\npred_all_lgbm = lgbm_grid.predict(X_test_sc)","fed0dd04":"sub_lgbm = pd.DataFrame()\nsub_lgbm['PassengerId'] = df_test['PassengerId']\nsub_lgbm['Survived'] = pred_all_lgbm\nsub_lgbm.to_csv('lgbm.csv',index=False)","e8d488bb":"from sklearn.ensemble import VotingClassifier","c95ef00b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\n\neclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n\nparams = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200],}\n\nwith ignore_warnings(category=DeprecationWarning):\n    votingclf_grid = GridSearchCV(estimator=eclf, param_grid=params, cv=10)\n    votingclf_grid.fit(X_sc,y_sc)\n    sc_vot1 = get_best_score(votingclf_grid)","879c59a8":"clf4 = GradientBoostingClassifier()\nclf5 = SVC()\nclf6 = RandomForestClassifier()\n\neclf_2 = VotingClassifier(estimators=[('gbdt', clf4), \n                                      ('svc', clf5), \n                                      ('rf', clf6)], voting='soft')\n\nparams = {'gbdt__n_estimators': [50], 'gbdt__min_samples_split': [3],\n          'svc__C': [10, 100] , 'svc__gamma': [0.1,0.01,0.001] , 'svc__kernel': ['rbf'] , 'svc__probability': [True],  \n          'rf__max_depth': [7], 'rf__max_features': [2,3], 'rf__min_samples_split': [3] } \n\nwith ignore_warnings(category=DeprecationWarning):\n    votingclf_grid_2 = GridSearchCV(estimator=eclf_2, param_grid=params, cv=10)\n    votingclf_grid_2.fit(X_sc,y_sc)\n    sc_vot2_cv = get_best_score(votingclf_grid_2)","a5899b93":"with ignore_warnings(category=DeprecationWarning):    \n    pred_all_vot2 = votingclf_grid_2.predict(X_test_sc)\n\nsub_vot2 = pd.DataFrame()\nsub_vot2['PassengerId'] = df_test['PassengerId']\nsub_vot2['Survived'] = pred_all_vot2\nsub_vot2.to_csv('vot2.csv',index=False)","a14606b2":"from mlxtend.classifier import StackingClassifier","287aa2d2":"# Initializing models\nclf1 = xgb_grid.best_estimator_\nclf2 = gbdt_grid.best_estimator_\nclf3 = rf_grid.best_estimator_\nclf4 = svc_grid.best_estimator_\n\nlr = LogisticRegression()\nst_clf = StackingClassifier(classifiers=[clf1, clf1, clf2, clf3, clf4], meta_classifier=lr)\n\nparams = {'meta-logisticregression__C': [0.1,1.0,5.0,10.0] ,\n          #'use_probas': [True] ,\n          #'average_probas': [True] ,\n          'use_features_in_secondary' : [True, False]\n         }\n\nwith ignore_warnings(category=DeprecationWarning):\n    st_clf_grid = GridSearchCV(estimator=st_clf, param_grid=params, cv=5, refit=True)\n    st_clf_grid.fit(X_sc, y_sc)\n    sc_st_clf = get_best_score(st_clf_grid)\n    pred_all_stack = st_clf_grid.predict(X_test_sc)","2ee09c34":"list_scores = [sc_knn, sc_rf, sc_extr, sc_svc, sc_gbdt, sc_xgb, \n               sc_ada, sc_cat, sc_lgbm, sc_vot2_cv, sc_st_clf]\n\nlist_classifiers = ['KNN','RF','EXTR','SVC','GBDT','XGB',\n                    'ADA','CAT','LGBM','VOT2','STACK']","a2b505ae":"score_subm_svc   = 0.80861\nscore_subm_vot2  = 0.78947\nscore_subm_ada   = 0.78468\nscore_subm_lgbm  = 0.78468\nscore_subm_rf    = 0.77990\nscore_subm_xgb   = 0.77033\nscore_subm_dtree = 0.76076\nscore_subm_extr  = 0.76076\nscore_subm_gbdt  = 0.74641\nscore_subm_cat   = 0.74162\nscore_subm_knn   = 0.69856\n\nscore_subm_st_clf = 0.7   # TODO","f866bd36":"subm_scores = [score_subm_knn, score_subm_rf, score_subm_extr, score_subm_svc, \n               score_subm_gbdt, score_subm_xgb, score_subm_ada, score_subm_cat, \n               score_subm_lgbm, score_subm_vot2, score_subm_st_clf]","97af1b54":"trace1 = go.Scatter(x = list_classifiers, y = list_scores,\n                   name=\"Validation\", text = list_classifiers)\ntrace2 = go.Scatter(x = list_classifiers, y = subm_scores,\n                   name=\"Submission\", text = list_classifiers)\n\ndata = [trace1, trace2]\n\nlayout = dict(title = \"Validation and Submission Scores\", \n              xaxis=dict(ticklen=10, zeroline= False),\n              yaxis=dict(title = \"Accuracy\", side='left', ticklen=10,),                                  \n              legend=dict(orientation=\"v\", x=1.05, y=1.0),\n              autosize=False, width=750, height=500,\n              )\n\nfig = dict(data = data, layout = layout)\niplot(fig)","02a109f1":"predictions = {'KNN': pred_all_knn, 'RF': pred_all_rf, 'EXTR': pred_all_extr, \n               'SVC': pred_all_svc, 'GBDT': pred_all_gbdt, 'XGB': pred_all_xgb, \n               'ADA': pred_all_ada, 'CAT': pred_all_cat, 'LGBM': pred_all_lgbm, \n               'VOT2': pred_all_vot2, 'STACK': pred_all_stack}\ndf_predictions = pd.DataFrame(data=predictions) \ndf_predictions.corr()","3be590a3":"plt.figure(figsize=(9, 9))\nsns.set(font_scale=1.25)\nsns.heatmap(df_predictions.corr(), linewidths=1.5, annot=True, square=True, \n                fmt='.2f', annot_kws={'size': 10}, \n                yticklabels=df_predictions.columns , xticklabels=df_predictions.columns\n            )\nplt.yticks(rotation=0)\nplt.show()","bdda06c0":"imp_rf = pd.Series(data = rf_grid.best_estimator_.feature_importances_, \n                    index=X.columns).sort_values(ascending=False)","2bb5773e":"### Seaborn heatmaps  \nmissing data in df_train and df_test","8167091b":"**submission for knn**","cba11219":"### lightgbm LGBM","4bb70e32":"# Part 1: Exploratory Data Analysis","0fbdb845":"# Part 3: Optimization of Classifier parameters, Boosting, Voting and Stacking","1bf754c2":"### KNN","7fb8c2f3":"### Bining for Age and Fare, convert Title to numerical","7c5b6adc":"**TODO:**  \nplot for validation and test scores  \ncompare feature importances  \ncomplete documentation","4d14188b":"**submission for svc**","4fe56466":"**RandomizedSearchCV  and GridSearchCV apply k fold cross validation on a chosen set of parameters**\n**and then find the parameters that give the best performance.**  \nFor GridSearchCV, all possible combinations of the specified parameter values are tried out, resulting in a parameter grid.  \nFor RandomizedSearchCV, a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.","4c843f9d":"### scores from GridSearchCV","eb98cdef":"## Hyperparameter tuning with RandomizedSearchCV and GridSearchCV","ee832a5d":"**Increase of survival rate with length of name most important for male passengers**","5598210b":"This violinplot shows exactly the same info like the swarmplot before.","756b1a47":"### New Feature: NameLenBin","da798d2d":"### New Feature: Title","6962e04f":"**submission for ExtraTreesClassifier**","18a9581e":"### Seaborn Countplots  \nfor all categorical columns","3b74c1ae":"Passengers embarked in \"S\" had the lowest survival rate, those embarked in \"C\" the highest.  \nAgain, with hue we see the survival rate as function of Embarked and Pclass.","09cb5f03":"Of the 891 passengers in df_test, less than 350 survive.  \nMuch more women survive than men.  \nAlso, the chance to survive is much higher in Pclass 1 and 2 than in Class 3.  \nSurvival rate for passengers travelling with SibSp or Parch is higher than for those travelling alone.  \nPassengers embarked in C and Q are more likely to survie than those embarked in S.","2c36f1a6":"Best chances to survive for male passengers was in Pclass 1 or being below 5 years old.  \nLowest survival rate for female passengers was in Pclass 3 and being older than 40.  \nMost passengers were male, in Pclass 3 and between 15-35 years old.","3dfcc841":"### ExtraTreesClassifier","35f775df":"**submission for random forest**","b3b3c677":"But survival rate alone is not good beacuse its uncertainty depends on the number of samples.  \nWe also need to consider the total number (count) of passengers that embarked.","1f8498f1":"## Data Wrangling","990b5a6f":"**Looks like there is very strong correlation of Survival rate and Name length**","6570b1dc":"### Seaborn Distplots \n**Distribution of Age as function of Pclass, Sex and Survived**","c82e0db0":"# Titanic Survival: Seaborn and Ensembles\n**My second Titanic kernel**\n\n**[Part 0: Imports, Functions](#Part-0:-Imports,-Functions)** \n\n**[Part 1: Exploratory Data Analysis](#Part-1:-Exploratory-Data-Analysis)** \n\n* Seaborn [heatmaps](#Seaborn-heatmaps) : missing data in df_train and df_test\n* Seaborn [countplots](#Seaborn-Countplots) : Number of (Non-)Survivors as function of features\n* Seaborn [distplots](#Seaborn-Distplots) : Distribution of Age and Fare as function of Pclass, Sex and Survived  \n* [Bar and Box plots](#Bar-and-Box-plots) for categorical features : Pclass and Embarked\n* Seaborn [violin and swarm plots](#Swarm-and-Violin-plots) : Survivals as function of Age, Pclass and Sex\n\n**[Part 2: Data Wrangling and Feature Engineering](#Part-2:-Data-Wrangling-and-Feature-Engineering)**  \n\n* [Feature Engineering](#Feature-Engineering): include new features to improve the performance of the classifiers and to fill missing values:  \nFamily size, Alone, Name length, Title\n* [Data Wrangling](#Data-Wrangling): fill NaN, convert categorical to numerical, [Standard Scaler](#Standard-Scaler), create X, y and X_test for Part 3\n\n\n**[Part 3: Optimization of Classifier parameters, Boosting, Voting and Stacking](#Part-3:-Optimization-of-Classifier-parameters,-Boosting,-Voting-and-Stacking)**  \n\n* Review: [k fold cross validation](#Review:-k-fold-cross-validation) for SVC and Random Forest: \n * SVC, features not scaled \n * SVC, features scaled \n * Random Forest Classifier, RFC, features not scaled \n* Hyperparameter tuning with GridSearchCV and RandomizedSearchCV for: \n * Support Vector Machine Classifier, [SVC](#SVC-:-RandomizedSearchCV) \n * K Nearest Neighbor, [KNN](#KNN)\n * [Decision Tree](#Decision-Tree)\n * [Random Forest Classifier](#Random-Forest), RFC\n\n* study Ensemble models like Boosting, Stacking and Voting:  \n * [ExtraTreesClassifier](#ExtraTreesClassifier)\n * Gradient Boost Decision Tree - [GBDT](#Gradient-Boost-Decision-Tree-GBDT)\n * eXtreme Gradient Boosting - [XGBoost](#eXtreme-Gradient-Boosting---XGBoost)   \n * Adaptive Boosting - [AdaBoost](#Ada-Boost)\n * [CatBoost](#CatBoost)\n * lightgbm [LGBM](#lightgbm-LGBM)\n * Voting: [VotingClassifier 1](#First-Voting), [VotingClassifier 2](#Second-Voting)  \n * Stacking : [StackingClassifier](#StackingClassifier)  \n* Compare Classifier performance based on the validation score : [comparison plot](#Comparison-plot-for-best-models)\n* Correlation of prediction results : [correlation matrix](#Correlation-of-prediction-results)","19d11018":"**submission for GradientBoostingClassifier**","48d98cf8":"**Disribution of Fare as function of Pclass, Sex and Survived**","a3338e93":"**loading the data**","8ec20869":"### Standard Scaler","09fbac47":"### eXtreme Gradient Boosting - XGBoost","51f938eb":"### Decision Tree","45d0dde3":"### StackingClassifier","7a270a67":"**Boxplot**","db481613":"### New Feature: Family size","f90593fb":"**Fill NaN with mean or mode**","ea3b5664":"### Gradient Boost Decision Tree GBDT \n","27e7eafb":"### **Conclusion**","90917b60":"### Review: k fold cross validation  \njust a short review of this technique that we already studied in the first kernel","4e4e0345":"**double-check for missing values**","980f2d75":"### RFC, features not scaled  ","7a5b92f3":"* With this kernel we studied EDA with **Seaborn** including some unusual plots like violin and swarm.   \n* Based on the EDA we filled missing values according to related features and developed new features (**Feature Engineering**) to improve model performance.  \n* In Part 3 we learned basics of applying **ensemble models** for classification like **Boosting**, **Stacking** and **Voting**.   \n* For this we applied libraries like: **sklearn, mlxtend, lightgbm, catboost, xgboost**","0315c560":"Here, the high survival rate for kids in Pclass 2 is easily observed.  \nAlso, it becomes more obvious that for passengers older than 40 the best chance to survive is in Pclass 1,  \nand smallest chance in Pclass 3   ","88069e0f":"**In the following we apply GridSearchCV and RandomizedSearchCV for these Classification models:**  \n**KNN, Decision Tree, Random Forest, SVC**","fb3939b5":"### submission scores","e0a71398":"Highest survival rate (>0.9) for women in Pclass 1 or 2.  \nLowest survival rate (<0.2) for men in Pclass 3.","d87d2c71":"**convert categorical to numerical : get_dummies**","0d118d0c":"**submission for decision tree**","3726e0eb":"### SVC, features scaled  ","7e6a70b1":"### First Voting  \nfor the first voting ensemble I use three simple models (LR, RF, GNB)","dab3e630":"### Correlation of prediction results","182308e6":"As we know from the first Titanic kernel, survival rate decreses with Pclass.  \nThe hue parameter lets us see the difference in survival rate for male and female. ","a82b5175":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6e\/St%C3%B6wer_Titanic.jpg\" width=\"520\">","b9d5f86d":"**This is my second notebook for the Titanic classification competition.**\n\nIf you are new to Machine Learning, have a look at  **[my first Titanic notebook](https:\/\/www.kaggle.com\/dejavu23\/titanic-survival-for-beginners-eda-to-ml)** where  I studied the  \nbasics of EDA with Pandas and Matplotlib and how to do Classification with the scikit-learn library.  ","a0aaa931":"**some useful functions**","18db4d98":"**Chance to survive increases with length of name for all Passenger classes**","2c2289f0":"## Feature Engineering\n**New Features: 'FamilySize'  ,  'Alone' , 'NameLen' and 'Title'**","39c5dbe9":"### Bar and Box plots","f3b4cc48":"### SVC : RandomizedSearchCV","bd05a90f":"### SVC : GridSearchCV","3717c0c6":"### Comparison plot for best models","72de35ee":"### Random Forest","d843747d":"Default mode for seaborn barplots is to plot the mean value for the category.  \nAlso, the standard deviation is indicated.  \nSo, if we choose Survived as y-value, we get a plot of the survival rate as function   \nof the categories present in the feature chosen as x-value.","2789f4f0":"Looks like the feature Cabin has lots of missing data, also some data for Age and Embarked is missing.  \nLets plot the seaborn heatmap of the isnull matrix for the train and test data","d71b2d98":"### CatBoost\nlibrary for gradient boosting on decision trees with categorical features support","a8a9bb4f":"### Ada Boost  ","35ea8885":"### VotingClassifier","2f549e35":"### Second Voting\n\nfor the 2nd voting ensemble I use the three models (together with the optimal parameters found by GridSearchCV)  \nthat had the best test score based on the cross validations above ","0d619f32":"TODO : compare feature importances","2741daf8":"### Swarm and Violin plots\nAlthough the following swarm and violin plots show the same data like the countplots or distplots before,  \nthey can reveal ceratin details that disappear in other plots. However, it takes more time to study these plots in detail.","1ad27bf4":"### SVC, features not scaled  \nSupport Vector Machine Classifier","b6c9a033":"Mean fare for Passengers embarked in \"C\" was higher.","197cde01":"Passengers embarked in \"C\" had largest proportion of Pclass 1 tickets.  \nAlmost all Passengers embarked in \"Q\" had Pclass 3 tickets.  \nFor every class, the largest count of Passengers  embarked in \"S\".","1836e99e":"**References**  \nWhile this notebook contains some work work based on my ideas, it is also a collection of approaches  \nand techniques from these kaggle notebooks:","edd3cb0f":"# Part 2: Data Wrangling and Feature Engineering","0064867c":"# Part 0: Imports, Functions"}}