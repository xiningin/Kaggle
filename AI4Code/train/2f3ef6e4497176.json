{"cell_type":{"7ef44e10":"code","199abf6a":"code","9e345529":"code","ca0ff05d":"code","e804c73c":"code","e3f72115":"code","92ea9f49":"code","7b9d5542":"code","c047bfe6":"code","18e0ee1c":"code","607c686b":"code","eeb748f5":"code","e43f5fce":"code","9f26bbd8":"code","79a052d0":"code","5b085de1":"markdown","04f575e4":"markdown","7f1c54a9":"markdown","35e31259":"markdown","44c905ff":"markdown"},"source":{"7ef44e10":"import numpy as np\nimport matplotlib.pyplot as plt \nimport matplotlib.colors\nimport pandas as pd \nimport seaborn as sns\nimport imageio\nimport warnings\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.datasets import make_blobs\nfrom IPython.display import HTML\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error, log_loss\nfrom tqdm import tqdm_notebook","199abf6a":"my_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"red\",\"yellow\",\"green\"])","9e345529":"np.random.seed(0)","ca0ff05d":"data, labels = make_blobs(n_samples=1000, centers=4, n_features=2, random_state=0)\nprint(data.shape, labels.shape)","e804c73c":"plt.scatter(data[:,0], data[:,1], c=labels, cmap=my_cmap)\nplt.show()","e3f72115":"labels_orig = labels\nlabels = np.mod(labels_orig, 2)","92ea9f49":"X_train, X_val, Y_train, Y_val = train_test_split(data, labels_orig, stratify=labels_orig, random_state=0)\nprint(X_train.shape, X_val.shape, labels_orig.shape)","7b9d5542":"enc = OneHotEncoder()\n# 0 -> (1, 0, 0, 0), 1 -> (0, 1, 0, 0), 2 -> (0, 0, 1, 0), 3 -> (0, 0, 0, 1)\ny_OH_train = enc.fit_transform(np.expand_dims(Y_train,1)).toarray()\ny_OH_val = enc.fit_transform(np.expand_dims(Y_val,1)).toarray()\nprint(y_OH_train.shape, y_OH_val.shape)","c047bfe6":"class FFNetwork:\n  \n  def __init__(self, init_method = 'random', activation_function = 'sigmoid', leaky_slope = 0.1):\n        \n    self.params={}\n    self.params_h = []\n    self.num_layers=2\n    self.layer_sizes = [2, 2, 4]\n    self.activation_function = activation_function\n    self.leaky_slope = leaky_slope\n    \n    np.random.seed(0)\n    \n    if init_method == \"random\":\n      for i in range(1,self.num_layers+1):\n        self.params[\"W\"+str(i)] = np.random.randn(self.layer_sizes[i-1],self.layer_sizes[i])\n        self.params[\"B\"+str(i)] = np.random.randn(1,self.layer_sizes[i])\n        \n    elif init_method == \"he\":\n      for i in range(1,self.num_layers+1):\n        self.params[\"W\"+str(i)] = np.random.randn(self.layer_sizes[i-1],self.layer_sizes[i])*np.sqrt(2\/self.layer_sizes[i-1])\n        self.params[\"B\"+str(i)] = np.random.randn(1,self.layer_sizes[i])\n        \n    elif init_method == \"xavier\":\n      for i in range(1,self.num_layers+1):\n        self.params[\"W\"+str(i)]=np.random.randn(self.layer_sizes[i-1],self.layer_sizes[i])*np.sqrt(1\/self.layer_sizes[i-1])\n        self.params[\"B\"+str(i)]=np.random.randn(1,self.layer_sizes[i])\n        \n    elif init_method == \"zeros\":\n      for i in range(1,self.num_layers+1):\n        self.params[\"W\"+str(i)]=np.zeros((self.layer_sizes[i-1],self.layer_sizes[i]))\n        self.params[\"B\"+str(i)]=np.zeros((1,self.layer_sizes[i]))\n    \n    self.gradients={}\n    self.update_params={}\n    self.prev_update_params={}\n    for i in range(1,self.num_layers+1):\n      self.update_params[\"v_w\"+str(i)]=0\n      self.update_params[\"v_b\"+str(i)]=0\n      self.update_params[\"m_b\"+str(i)]=0\n      self.update_params[\"m_w\"+str(i)]=0\n      self.prev_update_params[\"v_w\"+str(i)]=0\n      self.prev_update_params[\"v_b\"+str(i)]=0\n  \n  def forward_activation(self, X): \n    if self.activation_function == \"sigmoid\":\n      return 1.0\/(1.0 + np.exp(-X))\n    elif self.activation_function == \"tanh\":\n      return np.tanh(X)\n    elif self.activation_function == \"relu\":\n      return np.maximum(0,X)\n    elif self.activation_function == \"leaky_relu\":\n      return np.maximum(self.leaky_slope*X,X)\n      \n  def grad_activation(self, X):\n    if self.activation_function == \"sigmoid\":\n      return X*(1-X) \n    elif self.activation_function == \"tanh\":\n      return (1-np.square(X))\n    elif self.activation_function == \"relu\":\n      return 1.0*(X>0)\n    elif self.activation_function == \"leaky_relu\":\n      d=np.zeros_like(X)\n      d[X<=0]=self.leaky_slope\n      d[X>0]=1\n      return d\n\n  def softmax(self, X):\n    exps = np.exp(X)\n    return exps \/ np.sum(exps, axis=1).reshape(-1,1)\n  \n  def forward_pass(self, X, params = None):\n    if params is None:\n        params = self.params\n    self.A1 = np.matmul(X, params[\"W1\"]) + params[\"B1\"] # (N, 2) * (2, 2) -> (N, 2)\n    self.H1 = self.forward_activation(self.A1) # (N, 2)\n    self.A2 = np.matmul(self.H1, params[\"W2\"]) + params[\"B2\"] # (N, 2) * (2, 4) -> (N, 4)\n    self.H2 = self.softmax(self.A2) # (N, 4)\n    return self.H2\n  \n  def grad(self, X, Y, params = None):\n    if params is None:\n      params = self.params \n      \n    self.forward_pass(X, params)\n    m = X.shape[0]\n    self.gradients[\"dA2\"] = self.H2 - Y # (N, 4) - (N, 4) -> (N, 4)\n    self.gradients[\"dW2\"] = np.matmul(self.H1.T, self.gradients[\"dA2\"]) # (2, N) * (N, 4) -> (2, 4)\n    self.gradients[\"dB2\"] = np.sum(self.gradients[\"dA2\"], axis=0).reshape(1, -1) # (N, 4) -> (1, 4)\n    self.gradients[\"dH1\"] = np.matmul(self.gradients[\"dA2\"], params[\"W2\"].T) # (N, 4) * (4, 2) -> (N, 2)\n    self.gradients[\"dA1\"] = np.multiply(self.gradients[\"dH1\"], self.grad_activation(self.H1)) # (N, 2) .* (N, 2) -> (N, 2)\n    self.gradients[\"dW1\"] = np.matmul(X.T, self.gradients[\"dA1\"]) # (2, N) * (N, 2) -> (2, 2)\n    self.gradients[\"dB1\"] = np.sum(self.gradients[\"dA1\"], axis=0).reshape(1, -1) # (N, 2) -> (1, 2)\n    \n  def fit(self, X, Y, epochs=1, algo= \"GD\", display_loss=False, \n          eta=1, mini_batch_size=100, eps=1e-8,  \n          beta=0.9, beta1=0.9, beta2=0.9, gamma=0.9 ):\n      \n    if display_loss:\n      loss = {}\n      Y_pred = self.predict(X)\n      loss[0] = log_loss(np.argmax(Y, axis=1), Y_pred)\n\n    for num_epoch in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n      m = X.shape[0]\n      \n      if algo == \"GD\":\n        self.grad(X, Y)\n        for i in range(1,self.num_layers+1):\n          self.params[\"W\"+str(i)] -= eta * (self.gradients[\"dW\"+str(i)]\/m)\n          self.params[\"B\"+str(i)] -= eta * (self.gradients[\"dB\"+str(i)]\/m)\n          \n      elif algo == \"MiniBatch\":\n        for k in range(0,m,mini_batch_size):\n          self.grad(X[k:k+mini_batch_size], Y[k:k+mini_batch_size])\n          for i in range(1,self.num_layers+1):\n            self.params[\"W\"+str(i)] -= eta * (self.gradients[\"dW\"+str(i)]\/mini_batch_size)\n            self.params[\"B\"+str(i)] -= eta * (self.gradients[\"dB\"+str(i)]\/mini_batch_size)\n            \n      elif algo == \"Momentum\":\n        self.grad(X, Y)\n        for i in range(1,self.num_layers+1):\n          self.update_params[\"v_w\"+str(i)] = gamma *self.update_params[\"v_w\"+str(i)] + eta * (self.gradients[\"dW\"+str(i)]\/m)\n          self.update_params[\"v_b\"+str(i)] = gamma *self.update_params[\"v_b\"+str(i)] + eta * (self.gradients[\"dB\"+str(i)]\/m)\n          self.params[\"W\"+str(i)] -= self.update_params[\"v_w\"+str(i)]\n          self.params[\"B\"+str(i)] -= self.update_params[\"v_b\"+str(i)]  \n          \n      elif algo == \"NAG\":\n        temp_params = {}\n        for i in range(1,self.num_layers+1):\n          self.update_params[\"v_w\"+str(i)]=gamma*self.prev_update_params[\"v_w\"+str(i)]\n          self.update_params[\"v_b\"+str(i)]=gamma*self.prev_update_params[\"v_b\"+str(i)]\n          temp_params[\"W\"+str(i)]=self.params[\"W\"+str(i)]-self.update_params[\"v_w\"+str(i)]\n          temp_params[\"B\"+str(i)]=self.params[\"B\"+str(i)]-self.update_params[\"v_b\"+str(i)]\n        self.grad(X,Y,temp_params)\n        for i in range(1,self.num_layers+1):\n          self.update_params[\"v_w\"+str(i)] = gamma *self.update_params[\"v_w\"+str(i)] + eta * (self.gradients[\"dW\"+str(i)]\/m)\n          self.update_params[\"v_b\"+str(i)] = gamma *self.update_params[\"v_b\"+str(i)] + eta * (self.gradients[\"dB\"+str(i)]\/m)\n          self.params[\"W\"+str(i)] -= eta * (self.update_params[\"v_w\"+str(i)])\n          self.params[\"B\"+str(i)] -= eta * (self.update_params[\"v_b\"+str(i)]) \n        self.prev_update_params=self.update_params \n        \n      elif algo == \"AdaGrad\":\n        self.grad(X, Y)\n        for i in range(1,self.num_layers+1):\n          self.update_params[\"v_w\"+str(i)] += (self.gradients[\"dW\"+str(i)]\/m)**2\n          self.update_params[\"v_b\"+str(i)] += (self.gradients[\"dB\"+str(i)]\/m)**2\n          self.params[\"W\"+str(i)] -= (eta\/(np.sqrt(self.update_params[\"v_w\"+str(i)])+eps)) * (self.gradients[\"dW\"+str(i)]\/m)\n          self.params[\"B\"+str(i)] -= (eta\/(np.sqrt(self.update_params[\"v_b\"+str(i)])+eps)) * (self.gradients[\"dB\"+str(i)]\/m)\n      \n      elif algo == \"RMSProp\":\n        self.grad(X, Y)\n        for i in range(1,self.num_layers+1):\n          self.update_params[\"v_w\"+str(i)] = beta*self.update_params[\"v_w\"+str(i)] +(1-beta)*((self.gradients[\"dW\"+str(i)]\/m)**2)\n          self.update_params[\"v_b\"+str(i)] = beta*self.update_params[\"v_b\"+str(i)] +(1-beta)*((self.gradients[\"dB\"+str(i)]\/m)**2)\n          self.params[\"W\"+str(i)] -= (eta\/(np.sqrt(self.update_params[\"v_w\"+str(i)]+eps)))*(self.gradients[\"dW\"+str(i)]\/m)\n          self.params[\"B\"+str(i)] -= (eta\/(np.sqrt(self.update_params[\"v_b\"+str(i)]+eps)))*(self.gradients[\"dB\"+str(i)]\/m)\n      \n      elif algo == \"Adam\":\n        self.grad(X, Y)\n        num_updates=0\n        for i in range(1,self.num_layers+1):\n          num_updates+=1\n          self.update_params[\"m_w\"+str(i)]=beta1*self.update_params[\"m_w\"+str(i)]+(1-beta1)*(self.gradients[\"dW\"+str(i)]\/m)\n          self.update_params[\"v_w\"+str(i)]=beta2*self.update_params[\"v_w\"+str(i)]+(1-beta2)*((self.gradients[\"dW\"+str(i)]\/m)**2)\n          m_w_hat=self.update_params[\"m_w\"+str(i)]\/(1-np.power(beta1,num_updates))\n          v_w_hat=self.update_params[\"v_w\"+str(i)]\/(1-np.power(beta2,num_updates))\n          self.params[\"W\"+str(i)] -=(eta\/np.sqrt(v_w_hat+eps))*m_w_hat\n          \n          self.update_params[\"m_b\"+str(i)]=beta1*self.update_params[\"m_b\"+str(i)]+(1-beta1)*(self.gradients[\"dB\"+str(i)]\/m)\n          self.update_params[\"v_b\"+str(i)]=beta2*self.update_params[\"v_b\"+str(i)]+(1-beta2)*((self.gradients[\"dB\"+str(i)]\/m)**2)\n          m_b_hat=self.update_params[\"m_b\"+str(i)]\/(1-np.power(beta1,num_updates))\n          v_b_hat=self.update_params[\"v_b\"+str(i)]\/(1-np.power(beta2,num_updates))\n          self.params[\"B\"+str(i)] -=(eta\/np.sqrt(v_b_hat+eps))*m_b_hat\n          \n      if display_loss:\n        Y_pred = self.predict(X)\n        loss[num_epoch+1] = log_loss(np.argmax(Y, axis=1), Y_pred)\n        self.params_h.append(np.concatenate((self.params['W1'].ravel(), self.params['W2'].ravel(), self.params['B1'].ravel(), self.params['B2'].ravel())))\n    \n    if display_loss:\n      plt.plot(np.fromiter(loss.values(), dtype = float), '-o', markersize=5)\n      plt.xlabel('Epochs')\n      plt.ylabel('Log Loss')\n      plt.show()\n      \n  \n  def predict(self, X):\n    Y_pred = self.forward_pass(X)\n    return np.array(Y_pred).squeeze()","18e0ee1c":"def post_process(scatter_plot=False, gradient_plot=True, plot_scale=0.1):    \n    Y_pred_train = model.predict(X_train)\n    Y_pred_train = np.argmax(Y_pred_train,1)\n    Y_pred_val = model.predict(X_val)\n    Y_pred_val = np.argmax(Y_pred_val,1)\n    accuracy_train = accuracy_score(Y_pred_train, Y_train)\n    accuracy_val = accuracy_score(Y_pred_val, Y_val)\n    print(\"Training accuracy\", round(accuracy_train, 4))\n    print(\"Validation accuracy\", round(accuracy_val, 4))    \n    \n    if scatter_plot:\n      plt.scatter(X_train[:,0], X_train[:,1], c=Y_pred_train, cmap=my_cmap, s=15*(np.abs(np.sign(Y_pred_train-Y_train))+.1))\n      plt.show()\n      \n    if gradient_plot:\n      h = np.asarray(model.params_h)\n      h_diff = (h[0:-1, :] - h[1:, :])\n      for i in range(18):\n        plt.subplot(6, 3, i+1)\n        plt.plot(h_diff[:, i], '-')\n        plt.ylim((-plot_scale, plot_scale))\n        plt.yticks([])\n        plt.xticks([])\n      plt.show()","607c686b":"model = FFNetwork(init_method='xavier', activation_function='sigmoid')\nmodel.fit(X_train, y_OH_train, epochs=10, eta=1, algo=\"GD\", display_loss=True)\npost_process()","eeb748f5":"for init_method in ['zeros', 'random', 'xavier', 'he']:\n  for activation_function in ['sigmoid']:\n    print(init_method, activation_function)\n    model = FFNetwork(init_method=init_method, activation_function=activation_function)\n    model.fit(X_train, y_OH_train, epochs=50, eta=1, algo=\"GD\", display_loss=True)\n    post_process(plot_scale=0.05)\n    print('\\n--\\n')","e43f5fce":"for init_method in ['zeros', 'random', 'xavier', 'he']:\n  for activation_function in ['tanh']:\n    print(init_method, activation_function)\n    model = FFNetwork(init_method=init_method, activation_function=activation_function)\n    model.fit(X_train, y_OH_train, epochs=100, eta=0.5, algo=\"NAG\", display_loss=True)\n    post_process(plot_scale=0.05)\n    print('\\n--\\n')","9f26bbd8":"for init_method in ['zeros', 'random', 'xavier', 'he']:\n  for activation_function in ['relu']:\n    print(init_method, activation_function)\n    model = FFNetwork(init_method=init_method, activation_function=activation_function)\n    model.fit(X_train, y_OH_train, epochs=50, eta=0.25, algo=\"GD\", display_loss=True)\n    post_process(plot_scale=0.05)\n    print('\\n--\\n')","79a052d0":"for init_method in ['zeros', 'random', 'xavier', 'he']:\n  for activation_function in ['leaky_relu']:\n    print(init_method, activation_function)\n    model = FFNetwork(init_method=init_method, activation_function=activation_function, leaky_slope=0.1)\n    model.fit(X_train, y_OH_train, epochs=50, eta=0.5, algo=\"GD\", display_loss=True)\n    post_process(plot_scale=0.05)\n    print('\\n--\\n')","5b085de1":"### Generate Data","04f575e4":"### Multi Class Classification","7f1c54a9":"### Setup","35e31259":"Read more [here](https:\/\/towardsdatascience.com\/implementing-different-activation-functions-and-weight-initialization-methods-using-python-c78643b9f20f)","44c905ff":"### FF Class"}}