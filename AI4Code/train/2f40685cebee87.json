{"cell_type":{"4fff066b":"code","be887d74":"code","8b2803d5":"code","1aca723f":"code","48fef787":"code","0dc0dd8e":"code","99314807":"code","343d2112":"code","d4f6fad5":"code","a7ca4afa":"code","c6d5ac97":"code","029988ec":"markdown","2930f4b6":"markdown","d583c363":"markdown","a59ffd1a":"markdown","2a9dbe57":"markdown","57197536":"markdown","cd1fafe6":"markdown","78906e0b":"markdown","73c308ec":"markdown","9cedc5cf":"markdown","30ae6701":"markdown","3480392b":"markdown"},"source":{"4fff066b":"from tensorflow import keras\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping","be887d74":"(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()","8b2803d5":"train_images = mnist_train_images.reshape(60000, 784)\ntest_images = mnist_test_images.reshape(10000, 784)\ntrain_images = train_images.astype('float32')\ntest_images = test_images.astype('float32')\ntrain_images \/= 255\ntest_images \/= 255","1aca723f":"train_labels = keras.utils.to_categorical(mnist_train_labels, 10)\ntest_labels = keras.utils.to_categorical(mnist_test_labels, 10)","48fef787":"import matplotlib.pyplot as plt\n\ndef display_sample(num):\n    #Print the one-hot array of this sample's label \n    print(train_labels[num])  \n    #Print the label converted back to a number\n    label = train_labels[num].argmax(axis=0)\n    #Reshape the 768 values to a 28x28 image\n    image = train_images[num].reshape([28,28])\n    plt.title('Sample: %d  Label: %d' % (num, label))\n    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n    plt.show()\n    \ndisplay_sample(143)","0dc0dd8e":"model = Sequential([\n    Dense(512,activation='relu',input_shape=(784,)),\n    Dropout(0.2),\n    Dense(512,activation='relu'),\n    Dropout(0.2),\n    Dense(10, activation='softmax'),\n])","99314807":"model.summary()","343d2112":"model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","d4f6fad5":"early_stopping = EarlyStopping(\n    patience=20,\n    min_delta=0.0001,\n    restore_best_weights=True,\n)\n\nhistory = model.fit(train_images, train_labels,\n                    batch_size=250,\n                    epochs=3000,\n                    verbose=2,\n                    shuffle=True,\n                    validation_data=(test_images, test_labels),\n                    callbacks = [early_stopping]\n                   )","a7ca4afa":"score = model.evaluate(test_images, test_labels, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","c6d5ac97":"for x in range(1000):\n    test_image = test_images[x,:].reshape(1,784)\n    predicted_cat = model.predict(test_image).argmax()\n    label = test_labels[x].argmax()\n    if (predicted_cat != label):\n        plt.title('Prediction: %d Label: %d' % (predicted_cat, label))\n        plt.imshow(test_image.reshape([28,28]), cmap=plt.get_cmap('gray_r'))\n        plt.show()","029988ec":"I need to explicitly convert the data into the format Keras \/ TensorFlow expects. I divide the image data by 255 in order to normalize it into 0-1 range, after converting it into floating point values.","2930f4b6":"I can even get a nice description of the resulting model:","d583c363":"Let's take a peek at one of the training images just to make sure it looks OK:","a59ffd1a":"Let's visualize the ones it got wrong. As this model is much better, I'll have to search deeper to find mistakes to look at.","2a9dbe57":"You can see most of the ones it's having trouble with, are images a human would have trouble with as well!\n\nYou can improve on the results! Does running more epochs help considerably? How about trying different optimizers?\n\nYou can also take advantage of Keras's ease of use to try different topologies quickly. Keras includes a MNIST example, where they add an additional layer, and use Dropout at each step to prevent overfitting, like this:\n\nTry adapting that to our code above and see if it makes a difference or not.","57197536":"Now I'll convert the 0-9 labels into \"one-hot\" format. Think of one_hot as a binary representation of the label data - that is, which number each handwriting sample was intended to represent. Mathematically one_hot represents a dimension for every possible label value. Every dimension is set to the value 0, except for the \"correct\" one which is set to 1. For example, the label vector representing the number 1 would be [0, 1, 0, 0, 0, 0, 0, 0, 0, 0] (remember I start counting at 0.) It's just a format that's optimized for how the labels are applied during training.\n\nSo the training label data is a tensor of shape [60,000, 10] - 60,000 test images each associated with 10 binary values that indicate whether or not the image represents a given number from 0-9.","cd1fafe6":"# Introducing Keras\n\nBe sure to be using tensorflow 1.9 or newer!\n\nKeras is a higher-level API within TensorFlow that makes things a lot easier. Not only is it easier to use, it's easier to tune.\n\nLet's set up the same deep neural network I set up with TensorFlow to learn from the MNIST data set.\n\nFirst I'll import all the stuff I need, which will initialize Keras as a side effect:","78906e0b":"Here's where things get exciting. All that [code](https:\/\/www.kaggle.com\/alirezanematolahy\/handwriting-recognition-with-keras) I wrote in Tensorflow creating placeholders, variables, and defining a bunch of linear algebra for each layer in our neural network? None of that is necessary with Keras!\n\nI can set up the same layers like this. The input layer of 784 features feeds into a ReLU layer of 512 nodes, Dropout layer with 20 persent rate which then goes into another ReLU layer of 512 nodes and again a Dropout layer with 20 persent rate which then goes into 10 nodes with softmax applied :","73c308ec":"Training our model is also just one line of code with Keras. Here I'll do 3000 epochs with a batch size of 250. Keras is slower, and if I'm not running on top of a GPU-accelerated Tensorflow this can take a fair amount of time.","9cedc5cf":"I've outperformed our Tensorflow version considerably!","30ae6701":"I'll load up the MNIST data set. Again, there are 60K training samples and 10K test samples.****","3480392b":"Setting up our optimizer and loss function is just as simple. I will use the Adam optimizer here. Other choices include Adagrad, SGD, RMSProb, Adamax, and Nadam."}}