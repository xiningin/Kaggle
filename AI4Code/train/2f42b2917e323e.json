{"cell_type":{"ea0e5554":"code","188ddd58":"code","3286a288":"code","d639fe73":"code","63df2bf3":"code","54b31944":"code","0f126834":"code","00301c1f":"code","a033f823":"code","3b44cd63":"code","f32efd9c":"code","a1e94791":"code","b20b14ce":"code","899d4958":"code","c2062c91":"code","013148e8":"code","e987049a":"code","2012ab4a":"code","eea622f0":"code","bceb6f3b":"code","f96c6116":"code","83b2afbb":"code","9c677aef":"code","bd0ec304":"code","93f0ec6e":"code","3fe2f054":"code","e31b431d":"code","377d16f6":"code","0c7d2c36":"code","0a21f9b5":"markdown","b59ef8a4":"markdown","6f8f46c5":"markdown","283109ac":"markdown","0aaf7c65":"markdown","7b8a0c66":"markdown","c8049611":"markdown","26e7c646":"markdown","1eaddc92":"markdown","c8bbab1e":"markdown","a70503a9":"markdown","b5d49356":"markdown","1ab0c06a":"markdown","7d6a1f5d":"markdown","4fb9979c":"markdown","bb4854dc":"markdown","48e40cd0":"markdown","66c58ef4":"markdown","462c2e56":"markdown","c1d8e4d9":"markdown","f837b5dd":"markdown","cffcea35":"markdown","8745c8e5":"markdown","a1f8b9b1":"markdown","697cd5c4":"markdown","015811a1":"markdown","4db94b62":"markdown","8b0014dc":"markdown"},"source":{"ea0e5554":"import numpy as np \nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.preprocessing import MinMaxScaler\nfrom imblearn.over_sampling import SMOTE \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\nwarnings.filterwarnings(\"ignore\")","188ddd58":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3286a288":"diabetes = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndiabetes.head()","d639fe73":"# Checking Number of Rows and columns\nprint(f\"Number of rows in dataframe are : {diabetes.shape[0]} \\nNumber of columns in dataframe are : {diabetes.shape[1]} \\n\")","63df2bf3":"# Checking information of data\ndiabetes.info()","54b31944":"# Checking Statistics of he data\ndiabetes.describe()","0f126834":"diabetes[diabetes.duplicated()]","00301c1f":"# Checking unique values\ndiabetes.nunique()","a033f823":"# Checking Null values\ndiabetes.isnull().sum()","3b44cd63":"print(\"Number of samples for Outcome 0 are : \",len(diabetes[diabetes['Outcome']==0]))\nprint(\"Number of samples for Outcome 1 are : \",len(diabetes[diabetes['Outcome']==1]))","f32efd9c":"# Add all column names to a list except for the target variable\ncolumns=diabetes.columns\ncolumns=list(columns)\ncolumns.pop()\nprint(\"Column names except for the target column are :\",columns)\n\n#Graphs to be plotted with these colors\ncolours=['b','c','g','k','m','r','y','b']\nprint()\nprint('Colors for the graphs are :',colours)","a1e94791":"sns.set(rc={'figure.figsize':(15,17)})\nsns.set_style(style='white')\nfor i in range(len(columns)):\n    \n    plt.subplot(4,2,i+1)\n    sns.distplot(diabetes[columns[i]], hist=True, rug=True, color=colours[i])","b20b14ce":"sns.set(rc={'figure.figsize':(15,17)})\ncolors_list = ['#78C850', '#F08030']\nj=1\nsns.set_style(style='white')\n\nfor i in (columns):\n    plt.subplot(4,2,j)\n\n    sns.violinplot(x=\"Outcome\", y=i,data=diabetes, kind=\"violin\", split=True, height=4, aspect=.7,palette=colors_list)\n       \n    sns.swarmplot(x='Outcome', y=i,data=diabetes, color=\"k\", alpha=0.8)\n    \n\n    j=j+1","899d4958":"sns.set(rc={'figure.figsize':(20,100)})\nj=1\n\nsns.set_style(style='white')\nfor i in range(len(columns)):\n    for k in range(i,len(columns)):\n        try:\n            if i==k:\n                continue\n            plt.subplot(18,2,j)\n            sns.scatterplot(x=diabetes[columns[i]],y=diabetes[columns[k]],hue=\"Outcome\",data=diabetes)\n            j=j+1\n        except:\n            break","c2062c91":"sns.set(rc={'figure.figsize':(15,15)})\nj=1\nsns.set_style(style='white')\n\nfor i in range(len(columns)):\n    plt.subplot(4,2,j)\n    sns.stripplot(x='Outcome', y=columns[i] , data=diabetes)\n    j=j+1","013148e8":"sns.set(rc={'figure.figsize':(15,100)})\nsns.set_style(style='white')\n\nsns.pairplot(diabetes, hue='Outcome')","e987049a":"# HeatMap\n\nplt.figure(figsize=(20,20))\nsns.light_palette(\"seagreen\", as_cmap=True)\nsns.heatmap(diabetes.corr(), annot=True)","2012ab4a":"plt.figure(figsize=(6,8))\nsns.set_style(style='white')\nsns.countplot(diabetes['Outcome'])","eea622f0":"with sns.axes_style(\"white\"):\n    sns.set_palette(\"BuGn_r\")\n    g2 = sns.jointplot(\"Insulin\", \"BMI\", data=diabetes,\n                kind=\"kde\", space=0)","bceb6f3b":"# Scaling those columns which have values greater than 1\n\nscaleIt = MinMaxScaler()\ncolumns_to_be_scaled = [c for c in diabetes.columns if diabetes[c].max() > 1]\nprint(\"The columns which are to be scaled are :\",columns_to_be_scaled)\n\nscaled_columns = scaleIt.fit_transform(diabetes[columns_to_be_scaled])\nscaled_columns = pd.DataFrame(scaled_columns, columns=columns_to_be_scaled)\nscaled_columns['Outcome'] = diabetes['Outcome'] \n\n\n\n#copying the scaled DataFrame to original DataFrame\n\ndiabetes=scaled_columns\ndiabetes","f96c6116":"x=diabetes.iloc[:,:-1]\ny=diabetes.iloc[:,-1:]\nx.head(5),y.head(5)","83b2afbb":"x_train, x_test, y_train, y_test = train_test_split(x,y , test_size = 0.2, random_state = 42)","9c677aef":"print(\"Percentage of Positive Values in training data before Smote :\",y_train.value_counts(normalize=True)[1]\/(y_train.value_counts(normalize=True)[0]+y_train.value_counts(normalize=True)[1])*100,\"%\")\nprint(\"Percentage of Negative Values in training data before Smote :\",y_train.value_counts(normalize=True)[0]\/(y_train.value_counts(normalize=True)[0]+y_train.value_counts(normalize=True)[1])*100,\"%\")\n\nprint()\nprint('Shape of x before applying SMOTE :', x_train.shape)\n\n\nsmote = SMOTE()\nx_train,y_train = smote.fit_resample(x_train,y_train)\n\nprint('Shape of x after applying SMOTE : ', x_train.shape)\nprint()\n\nprint(\"Percentage of Positive Values in training data after Smote :\",y_train.value_counts(normalize=True)[1]\/(y_train.value_counts(normalize=True)[0]+y_train.value_counts(normalize=True)[1])*100,\"%\")\nprint(\"Percentage of Negative Values in training data after Smote :\",y_train.value_counts(normalize=True)[0]\/(y_train.value_counts(normalize=True)[0]+y_train.value_counts(normalize=True)[1])*100,\"%\")","bd0ec304":"model = LogisticRegression()\nmodel.fit(x_train, y_train)\npredicted=model.predict(x_test)\nconf = confusion_matrix(y_test, predicted)\nprint (\"Confusion Matrix : \\n\", conf)\nprint()\nprint (\"The accuracy of Logistic Regression is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"Precision score for Logistic Regression is :\",precision_score(y_test, predicted,)*100, \"%\")\nprint()\nprint(\"Recall score for Logistic Regression is :\",recall_score(y_test, predicted,)*100, \"%\")","93f0ec6e":"model = GaussianNB()\nmodel.fit(x_train, y_train)\n  \npredicted = model.predict(x_test)\n  \nconf = confusion_matrix(y_test, predicted)\nprint (\"Confusion Matrix : \\n\", conf)\nprint()\nprint (\"The accuracy of Gaussian Naive Bayes is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"Precision score for Gaussian Naive Bayes is :\",precision_score(y_test, predicted,)*100, \"%\")\nprint()\nprint(\"Recall score for Gaussian Naive Bayes is :\",recall_score(y_test, predicted,)*100, \"%\")","3fe2f054":"model = BernoulliNB()\nmodel.fit(x_train, y_train)\n  \npredicted = model.predict(x_test)\nconf = confusion_matrix(y_test, predicted)\nprint (\"Confusion Matrix : \\n\", conf)\nprint()\nprint (\"The accuracy of Bernoulli Naive Bayes is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"Precision score for Bernoulli Naive Bayes is :\",precision_score(y_test, predicted,)*100, \"%\")\nprint()\nprint(\"Recall score for Bernoulli Naive Bayes is :\",recall_score(y_test, predicted,)*100, \"%\")","e31b431d":"model = SVC()\nmodel.fit(x_train, y_train)\npredicted = model.predict(x_test)\n\nconf = confusion_matrix(y_test, predicted)\nprint (\"Confusion Matrix : \\n\", conf)\nprint()\nprint (\"The accuracy of SVM is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"Precision score for SVM is :\",precision_score(y_test, predicted,)*100, \"%\")\nprint()\nprint(\"Recall score for SVM is :\",recall_score(y_test, predicted,)*100, \"%\")","377d16f6":"model = KNeighborsClassifier(n_neighbors = 1)  \nmodel.fit(x_train, y_train)\npredicted = model.predict(x_test)\n\nprint()\nprint (\"The accuracy of KNN is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"Precision score for KNN is :\",precision_score(y_test, predicted,)*100, \"%\")\nprint()\nprint(\"Recall score for KNN is :\",recall_score(y_test, predicted,)*100, \"%\")","0c7d2c36":"model = xgb.XGBClassifier(use_label_encoder=False)\nmodel.fit(x_train, y_train)\npredicted = model.predict(x_test)\n\n\nprint()\nprint (\"The accuracy of XGBoost is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"Precision score for XGBoost is :\",precision_score(y_test, predicted,)*100, \"%\")\nprint()\nprint(\"Recall score for XGBoost is :\",recall_score(y_test, predicted,)*100, \"%\")","0a21f9b5":"# Gaussian NavieBayes","b59ef8a4":"* Classes are imbalenced","6f8f46c5":"# Plotting The Pair Plot","283109ac":"# Importing Modules","0aaf7c65":"# Checking Duplicates","7b8a0c66":"# Logestic Regression","c8049611":"# Visualising various features","26e7c646":"* \"Zero\" duplicates in the data sest","1eaddc92":"# Feature Engineering","c8bbab1e":"# Using SMOTE To Handle Class Imbalance","a70503a9":"# Building Models","b5d49356":"# Train and Test","1ab0c06a":"# Importing DataSet","7d6a1f5d":"# Checking Class Distribution","4fb9979c":"# Conclusion\n* Class Imbalance was found.\n* Gaussian Naive bayes is performing the best in context of precision and recall","bb4854dc":"## About Data\n* Pregnancies - Number of times pregnant\n* Glucose - Plasma glucose concentration\n* BloodPressure - Diastolic blood pressure (mm Hg)\n* SkinThickness - Triceps skin fold thickness (mm)\n* Insulin - 2-Hour serum insulin (mu U\/ml)\n* BMI - Body mass index\n* DiabetesPedigreeFunction - Diabetes pedigree function\n* Age - Age (years)\n* Outcome - Class variable (0 or 1) - (Target variable)","48e40cd0":"# ScatterPlot Of All Attributes Against Each Other","66c58ef4":"* None of the graphs here are following a normal distribution.","462c2e56":"# Berounili NavieBayes","c1d8e4d9":"# Loading Files","f837b5dd":"#  Strip Plot Distribution Of Attributes Vs Outcome","cffcea35":"# X Gradient Boosting","8745c8e5":"#  ViolinPlot For Outcome Vs. Other Attributes","a1f8b9b1":"# Suport Vector Meachine","697cd5c4":"# KNearest Neighbour","015811a1":"# KDE plot","4db94b62":"# Seperating X and Y","8b0014dc":"# Distribution Of Target Variable"}}