{"cell_type":{"dbfba20f":"code","92bd6846":"code","14128836":"code","816d3b06":"code","cee65420":"code","abd1a576":"code","abaa76be":"code","3eaa8916":"code","6fdeb7b0":"code","3350532d":"code","78cc6017":"markdown","8ae826a2":"markdown","15e842f8":"markdown","dc841099":"markdown","f6ce5a1c":"markdown","7427de4c":"markdown","f926dab5":"markdown","6cd19c2e":"markdown","81781e1c":"markdown","0fbce2d9":"markdown","74ace0b6":"markdown"},"source":{"dbfba20f":"!pip install -q -U tensorflow-text\n!pip install -q -U tf-models-official","92bd6846":"import os\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom official.nlp import optimization  # to create AdamW optmizer\n\nimport matplotlib.pyplot as plt\n\ntf.get_logger().setLevel('ERROR')\n\nprint('TensorFlow:', tf.__version__)\n\nstrategy = tf.distribute.MirroredStrategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","14128836":"batch_size = 16\nepochs = 6\ninit_lr = 3e-5","816d3b06":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ny = train_ds['target']\n\nx_len = len(train_ds['text'])\n\ntrain_ds[\"keyword\"] = train_ds[\"keyword\"] + \"; \"\ntrain_ds[\"location\"] = train_ds[\"location\"] + \"; \"\n\ntrain_ds[\"keyword\"] = train_ds[\"keyword\"].fillna(\"\")\ntrain_ds[\"location\"] = train_ds[\"location\"].fillna(\"\")\n\ntrain_ds['text'] = train_ds[\"keyword\"] + train_ds[\"location\"] + train_ds['text']\n\ntrain_len = int(0.8 * x_len)\nval_len = int(0.2 * x_len)\n\n\nfull_df = tf.data.Dataset.from_tensor_slices((train_ds['text'],y)).batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\ntrain_df = full_df.take(int(train_len\/batch_size)).shuffle(buffer_size=int(train_len\/batch_size))\nval_df = full_df.skip(int(train_len\/batch_size)).shuffle(buffer_size=int(val_len\/batch_size))","cee65420":"bert_model_name = 'talking-heads_base' \n\nmap_name_to_handle = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/3',\n    'bert_en_uncased_L-24_H-1024_A-16':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/3',\n    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_wwm_uncased_L-24_H-1024_A-16\/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_L-12_H-768_A-12\/3',\n    'bert_en_cased_L-24_H-1024_A-16':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_L-24_H-1024_A-16\/3',\n    'bert_en_wwm_cased_L-24_H-1024_A-16':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_wwm_cased_L-24_H-1024_A-16\/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_multi_cased_L-12_H-768_A-12\/3',\n    'small_bert\/bert_en_uncased_L-2_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-2_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-2_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-2_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-4_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-4_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-4_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-4_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-6_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-6_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-6_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-6_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-8_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-8_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-8_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-8_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-10_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-10_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-10_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-10_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-12_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-12_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-12_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-768_A-12\/1',\n    'albert_en_base':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_base\/2',\n    'albert_en_large':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_large\/2',\n    'albert_en_xlarge':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_xlarge\/2',\n    'albert_en_xxlarge':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_xxlarge\/2',\n    'electra_small':\n        'https:\/\/tfhub.dev\/google\/electra_small\/2',\n    'electra_base':\n        'https:\/\/tfhub.dev\/google\/electra_base\/2',\n    'experts_pubmed':\n        'https:\/\/tfhub.dev\/google\/experts\/bert\/pubmed\/2',\n    'experts_wiki_books':\n        'https:\/\/tfhub.dev\/google\/experts\/bert\/wiki_books\/2',\n    'talking-heads_base':\n        'https:\/\/tfhub.dev\/tensorflow\/talkheads_ggelu_bert_en_base\/1',\n    'talking-heads_large':\n        'https:\/\/tfhub.dev\/tensorflow\/talkheads_ggelu_bert_en_large\/1',\n}\n\nmap_model_to_preprocess = {\n    'bert_en_uncased_L-24_H-1024_A-16':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'bert_en_wwm_cased_L-24_H-1024_A-16':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_preprocess\/3',\n    'bert_en_cased_L-24_H-1024_A-16':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_preprocess\/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_preprocess\/3',\n    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-2_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-2_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-2_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-2_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-4_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-4_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-4_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-4_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-6_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-6_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-6_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-6_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-8_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-8_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-8_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-8_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-10_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-10_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-10_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-10_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-12_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-12_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-12_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_multi_cased_preprocess\/3',\n    'albert_en_base':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_preprocess\/3',\n    'albert_en_large':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_preprocess\/3',\n    'albert_en_xlarge':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_preprocess\/3',\n    'albert_en_xxlarge':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_preprocess\/3',\n    'electra_small':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'electra_base':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'experts_pubmed':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'experts_wiki_books':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'talking-heads_base':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'talking-heads_large':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n}\n\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\ntfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocessing model auto-selected: {tfhub_handle_preprocess}')","abd1a576":"with strategy.scope():\n    bert_preprocess_model = hub.load(tfhub_handle_preprocess)\n\n    test_inp = [train_ds['text'].iloc[0]]\n    print(test_inp)\n    print(bert_preprocess_model.tokenize(test_inp))","abaa76be":"def build_bert():\n    bert_preprocess_model = hub.load(tfhub_handle_preprocess)\n    \n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dropout(0.1)(net)\n    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n    return tf.keras.Model(text_input, net)","3eaa8916":"with strategy.scope():\n    bert = build_bert()\n\n    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    metrics = [tf.metrics.BinaryAccuracy()]\n\n    steps_per_epoch = train_len\n    num_train_steps = steps_per_epoch * epochs\n    num_warmup_steps = int(0.2*num_train_steps)\n\n    optimizer = optimization.create_optimizer(init_lr=init_lr,\n                                              num_train_steps=num_train_steps,\n                                              num_warmup_steps=num_warmup_steps,\n                                              optimizer_type='adamw')\n\n    bert.compile(optimizer=optimizer,\n                            loss=loss,\n                            metrics=metrics)\n\nbert.summary()\n\nhistory = bert.fit(x=train_df,\n                  validation_data=val_df,\n                  epochs=epochs)","6fdeb7b0":"init_lr = 3e-5\nepochs = 3\n\nwith strategy.scope():\n    bert = build_bert()\n\n    steps_per_epoch = train_len + val_len\n    num_train_steps = steps_per_epoch * epochs\n    num_warmup_steps = int(0.2*num_train_steps)\n\n\n    optimizer = optimization.create_optimizer(init_lr=init_lr,\n                                              num_train_steps=num_train_steps,\n                                              num_warmup_steps=num_warmup_steps,\n                                              optimizer_type='adamw')\n\n    bert.compile(optimizer=optimizer,\n                            loss=loss,\n                            metrics=metrics)\n\n    bert.summary()\n\nhistory = bert.fit(x=full_df,epochs=epochs)","3350532d":"test_ds = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n\ntest_ds[\"keyword\"] = test_ds[\"keyword\"] + \"; \"\ntest_ds[\"location\"] = test_ds[\"location\"] + \"; \"\n\ntest_ds[\"keyword\"] = test_ds[\"keyword\"].fillna(\"\")\ntest_ds[\"location\"] = test_ds[\"location\"].fillna(\"\")\n\ntest_ds['text'] = test_ds[\"keyword\"] + test_ds[\"location\"] + test_ds['text']\n\ntest_pred = bert.predict(test_ds['text'])\n\ntest_id = test_ds['id']\n\nwith open(\".\/bert_talking_heads_predicitions.csv\",\"w+\") as f:\n    f.write(\"id,target\\n\")\n    \n    for i,val in enumerate(test_pred):\n        f.write(\"%d,%d\\n\" % (test_id[i],round(tf.sigmoid(val).numpy()[0])))","78cc6017":"### Lets see what a pre-processed sequence looks like\nBasically this transforms word, or sub-word units into integer tokens, which are typically associated with an embedded vector within the model.","8ae826a2":"## Import Tweet Dataset\nTo squeeze all the information we have from the tweet dataset, if the tweet has a location and keyword, that is concatenated at the beginning of the sequence with a semi-colon.\n\nWe split the data into 80% train, and 20% validation. Typically one would partition the training set in k-folds for very reliable results, it is often the case with these larger SOTA models that this is no longer computationally feasible.\n\nWe then apply various BERT models and fine-tune them.","15e842f8":"### Test set prediction","dc841099":"### Less common installs for BERT pre-trained model","f6ce5a1c":"### Preamble","7427de4c":"## BERT Model Selection\nLuckily the people at tensorflow make it as easy as changing a key in a dictionary to select a SOTA pre-trained model","f926dab5":"## Build and run BERT","6cd19c2e":"### Dev Performances\n * **bert_multi_cased_L-12_H-768_A-12**\n   * val_loss: 0.4034 - val_binary_accuracy: 0.8239\n   * 16 batch, 1e-5 LR, 3\/4 epochs\n * **bert_en_uncased_L-24_H-1024_A-16**\n   * val_loss: 0.4089 - val_binary_accuracy: 0.8311\n   * 16 batch, 1e-5 LR, 3\/4 epochs\n * **bert_en_uncased_L-12_H-768_A-12**\n   * val_loss: 0.4176 - val_binary_accuracy: 0.8297\n   * 32 batch, 3-e5 LR, 4\/4 epochs\n * **bert_en_cased_L-24_H-1024_A-16**\n   * val_loss: 0.3840 - val_binary_accuracy: 0.8350\n   * 16 batch, 1e-5 LR, 3\/4 epochs\n * **electra_base**\n   * val_loss: 0.4147 - val_binary_accuracy: 0.8232\n   * 16 batch, 3e-5 LR, 2\/4 epochs\n * **albert_en_xxlarge**\n   * val_loss: 0.4116 - val_binary_accuracy: 0.8393\n   * 8 batch, 3e-5 LR, 3\/4 epochs\n * **albert_en large**\n   * val_loss: 0.4517 - val_binary_accuracy: 0.8258\n   * 16 batch, 2e-5 LR, 3\/4 epochs\n * **talking_heads_large**\n   * val_loss: 0.4345 - val_binary_accuracy: 0.8420\n   * 8 batch, 2e-5 LR, 2\/4 epochs\n\n * **talking_heads_base**\n   * val_loss: 0.3835 - val_binary_accuracy: 0.8369\n   * 32 batch, 2e-5 LR, 4\/4 epochs\n   * val_loss: 0.3723 - val_binary_accuracy: 0.8382\n   * 32 batch, 3e-5 LR, 4\/4 epochs\n   * val_loss: 0.3844 - val_binary_accuracy: 0.8343\n   * 32 batch, 2e-5 LR, 6\/6 epochs","81781e1c":"## BERT Disaster Tweet Classification\nThis notebook illustrates the ease with which modern APIs (in this case Tensorflow) enable the use of SOTA models for down-stream NLP tasks.","0fbce2d9":"### Train model on entire training set","74ace0b6":"### Hyperparameters and global variables"}}