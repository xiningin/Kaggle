{"cell_type":{"1fef15a2":"code","8f5bca80":"code","e9db926d":"code","f7b8eb42":"code","ff4c27bb":"code","c36947e2":"code","034c4ba2":"code","1292e637":"code","0a33be21":"code","15e2b4e4":"code","88855818":"code","f4c69ea5":"code","997ffb5b":"code","626a7d6a":"code","55949602":"code","a42174fc":"code","b1d8497e":"code","43ae5d7e":"code","70b0dd9c":"code","48f1d142":"code","e7131423":"code","893bab1c":"code","af5246a2":"code","d4430744":"code","8ba9a359":"code","d77942ab":"code","ed4cde5c":"code","3911e0b8":"code","0f29df43":"code","b953a3b7":"code","e60ce66d":"code","576e35a0":"code","c494053b":"code","da1ada9f":"code","46bf73f9":"code","bfc695d0":"code","3b8465ce":"code","e2c12854":"code","9ce179d8":"markdown","4e479090":"markdown","726f486c":"markdown","d61a4044":"markdown","df8d5a4e":"markdown","fd8f2c9b":"markdown","9589e880":"markdown","9466d723":"markdown","6e139c93":"markdown","00eb15f0":"markdown","845205e0":"markdown","6fef2f2d":"markdown","d1ebf149":"markdown","0ea1cdad":"markdown","14c7f705":"markdown","dd258409":"markdown","8eedce04":"markdown","1f562980":"markdown","d01fefa3":"markdown","4dfea31e":"markdown","2c999cdf":"markdown","f8d487b5":"markdown","c9141e52":"markdown","ac686461":"markdown","4ad5179b":"markdown"},"source":{"1fef15a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8f5bca80":"!pip3 install mglearn","e9db926d":"df_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","f7b8eb42":"import re\ndef count_twitters_user(df):\n    twitter_username_re = re.compile(r'@([A-Za-z0-9_]+)')\n    count = 0\n    list_ = []\n    for text in df['text']:\n        users_in_twitter = re.findall(twitter_username_re, text)\n        for user in users_in_twitter:\n            list_.append(user)\n        count += len(users_in_twitter)\n    return len(set(list_)), set(list_)\n\n\ndef count_twitters_hashtags(df):\n    twitter_hashtag_re = re.compile(r'#([A-Za-z0-9_]+)')\n    count = 0\n    list_ = []\n    for text in df['text']:\n        hashtags = re.findall(twitter_hashtag_re, text)\n        for tags in hashtags:\n            list_.append(tags)\n        count += len(hashtags)\n    return len(set(list_)), set(list_)\n\nreal = df_train[df_train['target']==1]\nfake = df_train[df_train['target']==0]\n\ncount_real, users_real = count_twitters_user(real)\ncount_fake, users_fake = count_twitters_user(fake)\n\n\ncount_tags_real, tags_real = count_twitters_hashtags(real)\ncount_tags_fake, tags_fake = count_twitters_hashtags(fake)","ff4c27bb":"import plotly.express as px\nimport plotly.graph_objects as go\nfig = go.Figure()\nfig.add_trace(go.Pie(\n    values=list(df_train['target'].value_counts()),\n    labels=['Real', 'Fake'],\n   # marker_color='lightsalmon'\n))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'Distribution',\n})\n\nfig.show()","c36947e2":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    y=[count_tags_real, count_tags_fake],\n    x=['Real', 'Fake'],\n    marker_color='lightsalmon'\n))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'unique hashtags mentions in twitters',\n})\nfig.show()","034c4ba2":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    y=[count_real, count_fake],\n    x=['Real', 'Fake'],\n    marker_color='lightsalmon'\n))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'unique \"@users\" mentions in twitters',\n})\nfig.show()","1292e637":"users_only_fake = list(users_fake - users_real)\nprint('there are', len(users_only_fake), 'mentionate only fake twitter news')\nusers_only_fake = list(tags_fake - tags_real)\nprint('there are', len(users_only_fake), 'mentionate only fake twitter news')\nusers_only_fake = list(users_real - users_fake)\nprint('there are', len(users_only_fake), 'mentionate only fake twitter real')\nusers_only_fake = list(tags_real - tags_fake)\nprint('there are', len(users_only_fake), 'mentionate only fake twitter real')","0a33be21":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef get_top_n_words(corpus, n=None, stop=None):\n    vec = CountVectorizer(stop_words=stop).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","15e2b4e4":"from plotly.subplots import make_subplots\n\nfig = make_subplots(\n    rows=2, cols=2,\n    specs=[[{}, {}],\n           [{\"colspan\": 2}, None]],\n    subplot_titles=(\"Real twitters\",\"Fake twitters\", \"All twitters\"))\n\ndata = get_top_n_words(df_train['text'], 25)\nnew_list_words = [ seq[0] for seq in data ]\nnew_list_values = [ seq[1] for seq in data ]\n\ndata_real = get_top_n_words(real['text'], 25)\nnew_list_words_real = [ seq[0] for seq in data_real ]\nnew_list_values_real = [ seq[1] for seq in data_real ]\n\ndata_fake = get_top_n_words(fake['text'], 25)\nnew_list_words_fake = [ seq[0] for seq in data_fake ]\nnew_list_values_fake = [ seq[1] for seq in data_fake ]\n\n\nfig.add_trace(go.Bar(x=new_list_words_real, y=new_list_values_real),\n                 row=1, col=1)\n\nfig.add_trace(go.Bar(x=new_list_words_fake, y=new_list_values_fake),\n                 row=1, col=2)\nfig.add_trace(go.Bar(x=new_list_words, y=new_list_values),\n                 row=2, col=1)\n\nfig.update_layout(showlegend=False, title_text=\"Specs with Subplot Title\")\nfig.show()","88855818":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\nimport mglearn\nvec = CountVectorizer()\n\nX = vec.fit_transform(df_train['text'])\n\ncomponents = 6\n\nlda=LDA(n_components=components, n_jobs=-1, random_state=42)\nlda_dtf=lda.fit_transform(X)\nsorting=np.argsort(lda.components_)[:,::-1]\nfeatures=np.array(vec.get_feature_names())\nmglearn.tools.print_topics(topics=range(components), feature_names=features,sorting=sorting, topics_per_chunk=components, n_words=15)\n\ntopic = []\nfor n in range(lda_dtf.shape[0]):\n    topic_most_pr = lda_dtf[n].argmax()\n    topic.append(topic_most_pr)\ndf_train['topic'] = topic","f4c69ea5":"import nltk\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords = stopwords + [\"http\", \"https\", \"co\"]","997ffb5b":"real = df_train[df_train['target']==1]\nfake = df_train[df_train['target']==0]\nfig = make_subplots(\n    rows=2, cols=2,\n    specs=[[{}, {}],\n           [{\"colspan\": 2}, None]],\n    subplot_titles=(\"Real twitters\",\"Fake twitters\", \"All twitters\"))\n\ndata = get_top_n_words(df_train['text'], 25, stopwords)\nnew_list_words = [ seq[0] for seq in data ]\nnew_list_values = [ seq[1] for seq in data ]\n\ndata_real = get_top_n_words(real['text'], 25, stopwords)\nnew_list_words_real = [ seq[0] for seq in data_real ]\nnew_list_values_real = [ seq[1] for seq in data_real ]\n\ndata_fake = get_top_n_words(fake['text'], 25, stopwords)\nnew_list_words_fake = [ seq[0] for seq in data_fake ]\nnew_list_values_fake = [ seq[1] for seq in data_fake ]\n\n\nfig.add_trace(go.Bar(x=new_list_words_real, y=new_list_values_real),\n                 row=1, col=1)\n\nfig.add_trace(go.Bar(x=new_list_words_fake, y=new_list_values_fake),\n                 row=1, col=2)\nfig.add_trace(go.Bar(x=new_list_words, y=new_list_values),\n                 row=2, col=1)\n\nfig.update_layout(showlegend=False, title_text=\"Specs with Subplot Title\")\nfig.show()","626a7d6a":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import chi2\n\nvectorizer = CountVectorizer(stop_words = stopwords)\n\nX = vectorizer.fit_transform(df_train['text'])\n\nchi2score = chi2(X,df_train['target'])[0]","55949602":"wscores = dict(zip(vectorizer.get_feature_names(), chi2score))","a42174fc":"dict_ = {k: v for k, v in sorted(wscores.items(), key=lambda item: item[1], reverse=True)}\nkeys = list(dict_.keys())\nvalues = list(dict_.values())\nfig = px.bar(x=list(keys[0:50]), y=list(values[0:50]))\nfig.show()","b1d8497e":"import xgboost\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nvec = TfidfVectorizer(stop_words=stopwords)\nX= vec.fit_transform(df_train['text'])\ny = df_train['target']\nxgboost_params = {'n_estimators' :[25,50,100, None],\n                   'learning_rate': [0.0001, 0.001, 0.01, 0.1],\n                  'gamma':[0.5, 0.1, 1, 10],\n                  'max_depth':[5, 10, 15, None]}\n\nxgb = xgboost.XGBClassifier(random_state=42)\nclf_xgb = GridSearchCV(xgb, xgboost_params, cv=5,n_jobs= 4, verbose = 1)\nclf_xgb.fit(X, y)\nprint(clf_xgb.best_estimator_)\nprint(clf_xgb.best_score_)\n\n","43ae5d7e":"import lightgbm as lgb\nlightgbm_params ={'learning_rate':[0.0001, 0.001, 0.003, 0.01, 0.1],\n                  'n_estimators':[10,20, 50, 100, None],\n                 'max_depth':[4, 6, 10, 15, 20, 50, None]}\ngbm = lgb.LGBMClassifier(random_state = 42)\nclf_gbm = GridSearchCV(gbm, lightgbm_params, cv=5,n_jobs= 4, verbose = 1)\nclf_gbm.fit(X, y)\nprint(clf_gbm.best_estimator_)\nprint(clf_gbm.best_score_)","70b0dd9c":"from sklearn.svm import LinearSVC\n\nsvr_params = {'C':[0.0001, 0.001,0.01, 0.1, 1 , 10, 100]}\nsvr = LinearSVC(random_state=42)\nclf_svr = GridSearchCV(svr, svr_params, cv=5, n_jobs=4, verbose=1)\nclf_svr.fit(X, y)\nprint(clf_svr.best_estimator_)\nprint(clf_svr.best_score_)\n","48f1d142":"from sklearn.ensemble import RandomForestClassifier\nlightran_params ={'n_estimators':[10,20, 50, 100, None],\n                 'max_depth':[4, 6, 10, 15, 20, 50, None]}\nrandom = RandomForestClassifier(random_state = 42)\nclf_random = GridSearchCV(random, lightran_params, cv=5,n_jobs= 4, verbose = 1)\nclf_random.fit(X, y)\nprint(clf_random.best_estimator_)\nprint(clf_random.best_score_)","e7131423":"from sklearn.linear_model import LogisticRegression\nlf_params ={'C':[0.0001, 0.001,0.01, 0.1, 1 , 10, 100],\n           'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\nlf = LogisticRegression(random_state=42)\nclf_lf = GridSearchCV(lf, lf_params, cv=5,n_jobs= 4, verbose = 1)\nclf_lf.fit(X, y)\n\nprint(clf_lf.best_estimator_)\nprint(clf_lf.best_score_)","893bab1c":"from sklearn.ensemble import ExtraTreesClassifier\nlightree_params ={'n_estimators':[10,20, 50, 100, None],\n                  'max_depth':[4, 6, 10, 15, 20, 50, None]}\n\ntree = ExtraTreesClassifier(random_state=42, n_jobs=4)\nclf_tree = GridSearchCV(tree, lightree_params, cv=5, n_jobs= 4, verbose = 1)\nclf_tree.fit(X, y)\nprint(clf_tree.best_estimator_)\nprint(clf_tree.best_score_)","af5246a2":"import xgboost\nfrom sklearn.model_selection import GridSearchCV\n\nvec = TfidfVectorizer()\nX= vec.fit_transform(df_train['text'])\ny = df_train['target']\nxgboost_params = {'n_estimators' :[25,50,100, None],\n                   'learning_rate': [0.0001, 0.001, 0.01, 0.1],\n                  'gamma':[0.5, 0.1, 1, 10],\n                  'max_depth':[5, 10, 15, None]}\n\nxgb = xgboost.XGBClassifier(random_state=42, n_jobs=4)\nclf_xgb = GridSearchCV(xgb, xgboost_params, cv=5,n_jobs= 4, verbose = 1)\nclf_xgb.fit(X, y)\nprint(clf_xgb.best_estimator_)\nprint(clf_xgb.best_score_)\n\n","d4430744":"import lightgbm as lgb\nlightgbm_params ={'learning_rate':[0.0001, 0.001, 0.003, 0.01, 0.1],\n                  'n_estimators':[10,20, 50, 100, None],\n                 'max_depth':[4, 6, 10, 15, 20, 50, None]}\ngbm = lgb.LGBMClassifier(random_state = 42, n_jobs=4)\nclf_gbm = GridSearchCV(gbm, lightgbm_params, cv=5,n_jobs= 4, verbose = 1)\nclf_gbm.fit(X, y)\nprint(clf_gbm.best_estimator_)\nprint(clf_gbm.best_score_)","8ba9a359":"from sklearn.svm import LinearSVC\n\nsvr_params = {'C':[0.0001, 0.001,0.01, 0.1, 1 , 10, 100]}\nsvr = LinearSVC(random_state=42)\nclf_svr = GridSearchCV(svr, svr_params, cv=5, n_jobs=4, verbose=1)\nclf_svr.fit(X, y)\nprint(clf_svr.best_estimator_)\nprint(clf_svr.best_score_)\n","d77942ab":"from sklearn.ensemble import RandomForestClassifier\nlightran_params ={'n_estimators':[10,20, 50, 100, None],\n                 'max_depth':[4, 6, 10, 15, 20, 50, None]}\nrandom = RandomForestClassifier(random_state = 42, n_jobs=42)\nclf_random = GridSearchCV(random, lightran_params, cv=5,n_jobs= 4, verbose = 1)\nclf_random.fit(X, y)\nprint(clf_random.best_estimator_)\nprint(clf_random.best_score_)","ed4cde5c":"from sklearn.ensemble import ExtraTreesClassifier\nlightree_params ={'n_estimators':[10,20, 50, 100, None],\n                  'max_depth':[4, 6, 10, 15, 20, 50, None]}\n\ntree = ExtraTreesClassifier(random_state=42, n_jobs=42)\nclf_tree = GridSearchCV(tree, lightree_params, cv=5, n_jobs= 4, verbose = 1)\nclf_tree.fit(X, y)\nprint(clf_tree.best_estimator_)\nprint(clf_tree.best_score_)","3911e0b8":"from sklearn.linear_model import LogisticRegression\nlf_params ={'C':[0.0001, 0.001,0.01, 0.1, 1 , 10, 100],\n           'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\nlf = LogisticRegression(random_state=42)\nclf_lf = GridSearchCV(lf, lf_params, cv=5,n_jobs= 4, verbose = 1)\nclf_lf.fit(X, y)\n\nprint(clf_lf.best_estimator_)\nprint(clf_lf.best_score_)","0f29df43":"from sklearn.ensemble import StackingClassifier\nestimators = [\n    ('svc', LinearSVC(C=0.1, random_state=42)),\n    ('extra', ExtraTreesClassifier(random_state=42, n_jobs=4)),\n    ('random',RandomForestClassifier(random_state=42, n_jobs=4)),\n    ('lgb', lgb.LGBMClassifier(max_depth=50, n_estimators=50, random_state=42)),\n    ('xgb', xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=10, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=5,\n              min_child_weight=1,\n              n_estimators=50, n_jobs=0, num_parallel_tree=1, random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None))\n]\nclf_stack = StackingClassifier(\n    estimators=estimators, final_estimator=LogisticRegression(C=1, random_state=42, solver='newton-cg')\n)","b953a3b7":"from sklearn.ensemble import StackingClassifier\nestimators = [\n    ('lr',LogisticRegression(C=1, random_state=42, solver='newton-cg')),\n    ('extra', ExtraTreesClassifier(random_state=42, n_jobs=4)),\n    ('random',RandomForestClassifier(random_state=42, n_jobs=4)),\n    ('lgb', lgb.LGBMClassifier(max_depth=50, n_estimators=50, random_state=42, n_jobs=4)),\n]\nclf_stack1 = StackingClassifier(\n    estimators=estimators, final_estimator=LinearSVC(C=0.1, random_state=42)\n)\n","e60ce66d":"from sklearn.ensemble import StackingClassifier\nestimators = [\n    ('lr',LogisticRegression(C=1, random_state=42, solver='newton-cg')),\n    ('extra', ExtraTreesClassifier(random_state=42, n_jobs=4)),\n    ('random',RandomForestClassifier(random_state=42, n_jobs=4)),\n    ('lgb', lgb.LGBMClassifier(max_depth=50, n_estimators=50, random_state=42, n_jobs=4)),\n    ('xgb', xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=10, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=5,\n              min_child_weight=1,\n              n_estimators=50, n_jobs=4, num_parallel_tree=1, random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None))\n]\nclf_stack2 = StackingClassifier(\n    estimators=estimators, final_estimator=LinearSVC(C=0.1, random_state=42)\n)\n","576e35a0":"from sklearn.ensemble import StackingClassifier\nestimators = [\n    ('lr',LogisticRegression(C=1, random_state=42, solver='newton-cg')),\n    ('extra', ExtraTreesClassifier(random_state=42, n_jobs=4)),\n    ('lgb', lgb.LGBMClassifier(max_depth=50, n_estimators=50, random_state=42, n_jobs=4)),\n    ('xgb', xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=10, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=5,\n              min_child_weight=1,\n              n_estimators=50, n_jobs=4, num_parallel_tree=1, random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)),\n    ('svc',LinearSVC(C=0.1, random_state=42))\n]\nclf_stack3 = StackingClassifier(\n    estimators=estimators, final_estimator=RandomForestClassifier(random_state=42, n_jobs=4)\n)","c494053b":"from tqdm import tqdm\nscores = {}\n\nlg = LogisticRegression(C=1, random_state=42, solver='newton-cg')\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(lg, X, df_train['target'], cv=a)\n    list_.append(score.mean())\nscores['Logistic Regression'] = list_\n\nlinear = LinearSVC(C=0.1, random_state=42)\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(linear, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['Linear SVC'] = list_\n\nrandom = RandomForestClassifier(random_state=42, n_jobs=4)\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(random, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['Random Forest'] = list_\n\nextr = ExtraTreesClassifier(random_state=42,  n_jobs=4)\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(extr, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['Extra Tree'] = list_\n    \nlgbm = lgb.LGBMClassifier(max_depth=50, n_estimators=50, random_state=42,  n_jobs=4)\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(lgbm, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['LGBM'] = list_","da1ada9f":"xgb = xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=10, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=5,\n              min_child_weight=1,\n              n_estimators=50, n_jobs=4, num_parallel_tree=1, random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(xgb, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['XGBoost'] = list_","46bf73f9":"list_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(clf_stack, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['Stack1'] = list_\n\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(clf_stack1, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['Stack2'] = list_\n    \nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(clf_stack2, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['Stack3'] = list_\n\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(clf_stack3, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['Stack4'] = list_","bfc695d0":"import plotly.graph_objects as go\ndef plot_scores(scores):\n    fig = go.Figure()\n    for key, values in zip(scores.keys(), scores.values()):\n        fig.add_trace(go.Scatter(y=values, x=[2,4,6,8],\n                        mode='lines',\n                        name='scores '+str(key)))\n    fig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'Results CV',\n    })\n    fig.show()\nplot_scores(scores)","3b8465ce":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nstds = []\nfor value in scores.values():\n    std = np.std(np.array(value), axis=0)\n    stds.append(std)\ndf_to_scatter = pd.DataFrame([])\ndf_to_scatter['scores'] = stds\ndf_to_scatter['models'] = scores.keys()\n\nfig = px.scatter(df_to_scatter, x=\"models\", y=\"scores\", color=\"models\",\n                 size='scores')\nfig.show()","e2c12854":"X_test = vec.transform(df_test['text'])\nsub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nmodel = clf_stack1.fit(X, df_train['target'])\npredict = clf_stack1.predict(X_test)\nsub['target'] = predict\nsub.to_csv('submission.csv', index=False)","9ce179d8":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">&nbsp;Summary:<\/h1>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\" aria-controls=\"profile\">1. Introduction<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2\" role=\"tab\" aria-controls=\"messages\">2. EDA<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"messages\">3. Models<span class=\"badge badge-primary badge-pill\">3<\/span><\/a>\n     <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"messages\">4. Evaluate Models<span class=\"badge badge-primary badge-pill\">4<\/span><\/a>\n<\/div>","4e479090":"## ExtraTreesClassifier","726f486c":"## LinearSVC","d61a4044":"* the choose model was stack1 because when CV is 2 is the model with best result, and between stack has low stds in results","df8d5a4e":"# ExtraTreesClassifier","fd8f2c9b":"* the data is relatively balanced.\n* observe the values and presence of # among twitter in an attempt to observe some pattern for identification and users\n","9589e880":"## LGBMClassifier","9466d723":"## RandomForestClassifier","6e139c93":"* even with topic modeling there are only stopwords, they will be removed for better visualization of relevant words from the data\n","00eb15f0":"* Based on the information above the hashtags and users bring information regarding real and fake twitters, so in preprocessing the data is interested in maintaining the presence of these elements.\n","845205e0":"<a id=\"2\"><\/a> <br>\n<font size=\"+3\" color=\"black\"><b>3 - EDA<\/b><\/font><br><a id=\"2\"><\/a>\n<br> ","6fef2f2d":"## XGBClassifier","d1ebf149":"## LinearSVC","0ea1cdad":"* The frequent words are similar in fake and real twitters.\n\n","14c7f705":"<a id=\"3\"><\/a> <br>\n<font size=\"+3\" color=\"black\"><b>3 - Model<\/b><\/font><br><a id=\"3\"><\/a>\n<br> \n\n*  So now i will test the model classification ignoring the use of stopwords and the words http, https and co, training some models using hyper parameterization to get better results and in the end use stack to combine the result in the perspective of having a better result\n","dd258409":"* with the removal of stopwords words appearing related to disasters.\n* let's start modeling, performing two experiments, one using the stopwords removal and the other keeping the original data.\n\n","8eedce04":"* So know try to identify best model","1f562980":"## LogisticRegression","d01fefa3":"<a id=\"1\"><\/a> <br>\n<font size=\"+3\" color=\"black\"><b>1 - Introduction<\/b><\/font><br><a id=\"1\"><\/a>\n<br> \n\n* first I will observe the distribution between the real and fake twitters\n* And let's see how the usage ratio is between real and fake twitter, in an attempt to identify patterns\n","4dfea31e":"## RandomForestClassifier","2c999cdf":"## XGBClassifier","f8d487b5":"## LGBMClassifier","c9141e52":"## LogisticRegression","ac686461":"<a id=\"4\"><\/a> <br>\n<font size=\"+3\" color=\"black\"><b>4 - Evaluate Models<\/b><\/font><br><a id=\"4\"><\/a>\n<br> ","4ad5179b":"* the most frequent words seem to be just stopwords and \"http\", \"https\" and \"co\", so let's remove all those words and see which are the most relevant words using chi2"}}