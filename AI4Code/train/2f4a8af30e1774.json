{"cell_type":{"315c1b3a":"code","775ad248":"code","58d4421e":"code","ff105d69":"code","34a16a0e":"code","7309ff98":"code","08cf6df4":"code","a311b2e5":"code","5ba8e150":"code","5b531975":"code","ede888cd":"code","fbfd4781":"code","49bf62dd":"code","deb93031":"markdown","774b369c":"markdown","9fc2f978":"markdown","c41141b5":"markdown","e03cc1df":"markdown","435cddb6":"markdown","b2d0c877":"markdown","f9a598dc":"markdown","f37999f1":"markdown","0af42356":"markdown","bbd3747b":"markdown","1dac7a73":"markdown"},"source":{"315c1b3a":"# Import Modules\nimport pandas as pd\nimport numpy as np\nimport gc\nimport random\nimport lightgbm as lgbm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\"))","775ad248":"# Import modules specific for Bayesian Optimization\nfrom bayes_opt import BayesianOptimization\nfrom bayes_opt.logger import JSONLogger\nfrom bayes_opt.event import Events","58d4421e":"# Specify some constants\nseed = 4249\nfolds = 5\nnumber_of_rows = 1000000","ff105d69":"# Select Features\nfeatures = ['AVProductStatesIdentifier',\n            'AVProductsInstalled', \n            'Census_ProcessorModelIdentifier',\n            'Census_TotalPhysicalRAM',\n            'Census_PrimaryDiskTotalCapacity',\n            'EngineVersion',\n            'Census_SystemVolumeTotalCapacity',\n            'Census_InternalPrimaryDiagonalDisplaySizeInInches',\n            'Census_OSBuildRevision',\n            'AppVersion',\n            'Census_OEMNameIdentifier',\n            'Census_InternalPrimaryDisplayResolutionVertical',\n            'Census_ProcessorCoreCount',\n            'Census_OEMModelIdentifier',\n            'CountryIdentifier',\n            'LocaleEnglishNameIdentifier',\n            'GeoNameIdentifier',\n            'Census_InternalPrimaryDisplayResolutionHorizontal',\n            'IeVerIdentifier',\n            'HasDetections']","34a16a0e":"# Load Data with selected features\nX = pd.read_csv('..\/input\/train.csv', usecols = features, nrows = number_of_rows)","7309ff98":"# Labels\nY = X['HasDetections']\n\n# Remove Labels from Dataframe\nX.drop(['HasDetections'], axis = 1, inplace = True)","08cf6df4":"# Factorize Some Columns\nX['EngineVersion'] = pd.to_numeric(pd.factorize(X['EngineVersion'])[0])\nX['AppVersion'] = pd.to_numeric(pd.factorize(X['AppVersion'])[0])","a311b2e5":"# Final Data Shapes\nprint(X.shape)\nprint(Y.shape)","5ba8e150":"# Create LightGBM Dataset\nlgbm_dataset = lgbm.Dataset(data = X, label = Y)","5b531975":"# Specify LightGBM Cross Validation function\ndef lgbm_cv_evaluator(learning_rate, num_leaves, feature_fraction, bagging_fraction, max_depth):\n    # Setup Parameters\n    params = {  'objective':            'binary',\n                'boosting':             'gbdt',\n                'num_iterations':       1250, \n                'early_stopping_round': 100, \n                'metric':               'auc',\n                'verbose':              -1\n            }\n    params['learning_rate'] =       learning_rate\n    params['num_leaves'] =          int(round(num_leaves))\n    params['feature_fraction'] =    feature_fraction\n    params['bagging_fraction'] =    bagging_fraction\n    params['max_depth'] =           int(round(max_depth))\n        \n    # Run LightGBM Cross Validation\n    result = lgbm.cv(params, lgbm_dataset, nfold = folds, seed = seed, \n                     stratified = True, verbose_eval = -1, metrics = ['auc']) \n    \n    # Return AUC\n    return max(result['auc-mean'])","ede888cd":"def display_progress(event, instance):\n    iter = len(instance.res) - 1\n    print('Iteration: {} - AUC: {} - {}'.format(iter, instance.res[iter].get('target'), instance.res[iter].get('params')))","fbfd4781":"def bayesian_parameter_optimization(init_rounds = 1, opt_rounds = 1):    \n    \n    # Initialize Bayesian Optimization\n    optimizer = BayesianOptimization(f = lgbm_cv_evaluator, \n                                    pbounds = { 'learning_rate':        (0.02, 0.06),\n                                                'num_leaves':           (20, 100),\n                                                'feature_fraction':     (0.25, 0.75),\n                                                'bagging_fraction':     (0.75, 0.95),\n                                                'max_depth':            (8, 15) },\n                                    random_state = seed, \n                                    verbose = 2)\n    \n    # Subscribe Logging to file for each Optimization Step\n    logger = JSONLogger(path = 'parameter_output.json')\n    optimizer.subscribe(Events.OPTMIZATION_STEP, logger)\n    \n    # Subscribe the custom display_progress function for each Optimization Step\n    optimizer.subscribe(Events.OPTMIZATION_STEP, \" \", display_progress)\n\n    # Perform Bayesian Optimization. \n    # Modify acq, kappa and xi to change the behaviour of Bayesian Optimization itself.\n    optimizer.maximize(init_points = init_rounds, n_iter = opt_rounds, acq = \"ei\", kappa = 2, xi = 0.1)\n    \n    # Return Found Best Parameter values and Target\n    return optimizer.max","49bf62dd":"# Configure and Perform Bayesian Optimization \nmax_params = bayesian_parameter_optimization(init_rounds = 15, opt_rounds = 15)\n\nprint('================= Results')\nprint('Found Max AUC: {} with the following Parameters: '.format(max_params.get('target')))\nprint(max_params.get('params'))","deb93031":"I hope you enjoyed this notebook and that you can use it for your own benefit.\n\nPlease let me know if you have any questions\/remarks\/improvements. Those are allways welcome.","774b369c":"Next we create a function to display a custom progress status for each round of Bayesian Optimization","9fc2f978":"I specify a function to run LightGBM Cross Validation with the specified parameters. After running for a maximum of 1250 iterations the function will return the achieved AUC.\n\nThe specified parameters are:\n* learning_rate\n* num_leaves\n* feature_fraction\n* bagging_fraction\n* max_depth","c41141b5":"Load the train dataframe","e03cc1df":"For the features I just choose a couple of them. I'am still working on my own feature selection and engineering ;-)","435cddb6":"Assign the labels to Y and drop the label column from the train dataframe.","b2d0c877":"Finally we will trigger the optimization process and show the found optimal results. Note that the results from all rounds will be logged to the .json file in the output. In the Kaggle webpage it will show only 1 round..if you download the file you will see the information for all rounds.","f9a598dc":"Let's import the modules needed for Bayesian optimization","f37999f1":"The script will run LightGBM 5 folds Cross Validation and will only load the first 1000000 rows of the train set. ","0af42356":"In this competition feature selection and feature engineering are 2 very important steps in achieving a good model and hopefully high score. Another important task is choosing the parameters for your tool\/model of choice wisely. There are many ways to choose or search those parameters.\n\nIn this notebook I will setup a basic solution to use Bayesian optimization to search for an optimal set of parameters for LightGBM. It should be no problem to modify this code and use it for XGBoost for example.\n\nSome points to mention upfront. Because of the time needed I specified only 15 initialization rounds and 15 optimization rounds .. however the more rounds the better. I also limited the number of rows used and the maximum iterations for LightGBM. These could also be increased to get better results.\n\nFor more background information visit the github site for Bayesian Optimization package used [https:\/\/github.com\/fmfn\/BayesianOptimization](https:\/\/github.com\/fmfn\/BayesianOptimization)","bbd3747b":"The following function initializes the BayesianOptimization package with the function to use and the different ranges for the parameters. For each parameter a lower and upper bound is specified.\nAlso we subscribe to each Optimization Step a logger to log all results to json file and the function to show the progress.","1dac7a73":"2 columns are factorized. The remainder of the columns are used as-is."}}