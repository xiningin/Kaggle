{"cell_type":{"9a87c4e2":"code","28f357e7":"markdown"},"source":{"9a87c4e2":"import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\n\ndef Create_Dataset():\n\n    X, y = datasets.make_regression(n_samples=1000, n_features=4, noise=10, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    return X_train, X_test, y_train, y_test\n\n\ndef Linear_Regression_with_Ordinary_Least_Squares(X, y):\n\n    X = np.c_[np.ones(X.shape[0]), X]  # add bias\n    beta_hat = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))\n    print(\"\\nLinear Regression with Ordinary Least Squares\")\n    print(\"Coefficients: {}\".format(beta_hat[1:]), \"\\nIntercept: {}\".format(beta_hat[0]))\n\n\ndef Linear_Regression_with_Gradient_Descent(X_train, X_test, y_train, y_test):\n\n    class Multiple_Linear_Regression:\n\n        def __init__(self, learning_rate = 0.001):\n\n            self.learning_rate = learning_rate\n            self.weight = None\n            self.bias = None\n\n        def fit(self, X, y):\n\n            self.weight = np.random.randn(X.shape[1])\n            self.bias = np.random.randn()\n\n            for i in range(10000):\n\n                hypothesis = np.dot(X, self.weight) + self.bias\n\n                derivative_weight = np.mean(X.T * (hypothesis - y), axis = 1)\n                derivative_bias = np.mean(hypothesis - y)\n\n                self.weight -= self.learning_rate * derivative_weight\n                self.bias -= self.learning_rate * derivative_bias\n\n        def predict(self, X):\n\n            return np.dot(X, self.weight) + self.bias\n\n        def R_Squared(self, y_prediction, y_test):\n\n            SSE = np.sum((y_test - y_prediction) ** 2)\n            y_avg = np.sum(y_test) \/ len(y_test)\n            SST = np.sum((y_prediction - y_avg) ** 2)\n            RSquared = 1 - (SSE \/ SST)\n            return RSquared\n\n    lr = Multiple_Linear_Regression(0.01)\n    lr.fit(X_train, y_train)\n    y_prediction = lr.predict(X_test)\n    print(\"\\nLinear Regression with Gradient Descent\")\n    print(\"Coefficients: {}\".format(lr.weight), \"\\nIntercept: {}\".format(lr.bias))\n    print(\"R Squared Value: {}\".format(lr.R_Squared(y_prediction, y_test)))\n\n\ndef Linear_Regression_with_Sklearn(X_train, X_test, y_train, y_test):\n\n    lr = LinearRegression()\n    lr.fit(X_train, y_train)\n    y_prediction = lr.predict(X_test)\n    print(\"\\nLinear Regression with Sklearn\")\n    print(\"Coefficients: {}\".format(lr.coef_), \"\\nIntercept: {}\".format(lr.intercept_))\n    print(\"R Squared Value: {}\".format(r2_score(y_test, y_prediction)))\n\n\nif __name__ == \"__main__\":\n\n    X_train, X_test, y_train, y_test = Create_Dataset()\n    Linear_Regression_with_Ordinary_Least_Squares(X_train, y_train)\n    Linear_Regression_with_Gradient_Descent(X_train, X_test, y_train, y_test)\n    Linear_Regression_with_Sklearn(X_train, X_test, y_train, y_test)\n\n\n","28f357e7":"**MULTIPLE LINEAR REGRESSION**\n\nWhen implementing multiple linear regression, I have created a dataset of size 1000 with four features and separated them as train and test data. I have used the methods of **ordinary least squares** and **gradient descent** along with Sklearn. \n> "}}