{"cell_type":{"85a0195e":"code","a64389fc":"code","6ce157e3":"code","790a40e4":"code","de8feab7":"code","27936bfb":"code","d2e9731a":"code","dd4b5a69":"code","a302b788":"code","c18ce61e":"code","1bf165fa":"code","53256494":"code","3995feac":"code","297eb79b":"code","28958e14":"code","374644d2":"code","9c7899bc":"code","ef7cf43f":"code","06848b9e":"code","afdf0d96":"code","336370eb":"code","a318ad9b":"code","459b90d7":"code","67684135":"code","b0c0f762":"code","05b20af6":"code","026a1e1f":"code","f8bd1e0a":"code","6deedc17":"code","2b8b2cdc":"code","850440ad":"code","19450c6c":"code","a766686a":"code","9886edb5":"code","6fb06c36":"code","7862a331":"code","e9169ef2":"code","bf3315c9":"code","0848ee1c":"code","50abe2ae":"code","54d584a6":"code","beb91235":"markdown","f0d73bb3":"markdown","b758b802":"markdown","7da705ef":"markdown","2d47ee0a":"markdown","330b4721":"markdown","c2c1bf14":"markdown","2462e5b4":"markdown","01c94d22":"markdown","cb09b26b":"markdown","08c8ec5d":"markdown","c7b8ab71":"markdown","45317ebe":"markdown","65daf562":"markdown","3c5aaa66":"markdown","0af0bafb":"markdown","9d1f80c7":"markdown","2e037a57":"markdown","33a42545":"markdown","a8e06468":"markdown","d714ea8f":"markdown","cb7a7587":"markdown","24ba179a":"markdown","c4816a2a":"markdown","6a649a7d":"markdown","6ba9f872":"markdown","0b90a51c":"markdown","ccaed28e":"markdown"},"source":{"85a0195e":"# salt and pepper\nimport pandas as pd\nimport numpy as np\n\n# plotting\nimport matplotlib.pylab as plt \nimport plotly.graph_objs as go\nimport seaborn as sns\n\n\n# sklearn\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report, roc_curve, auc, roc_auc_score\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier # Decision Tree\nfrom sklearn.svm import SVC # Support Vector Machine\nfrom sklearn.ensemble import RandomForestClassifier # Random Forest Classifier\n\n# widgets\nimport ipywidgets as widgets\n\n# warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# pandas options\npd.set_option('display.max_columns', None)\n#pd.set_option('display.max_rows', None)","a64389fc":"def rebinnable_interactive_histogram(series, initial_bin_width=10):\n    figure_widget = go.FigureWidget(\n        data=[go.Histogram(x=series, xbins={\"size\": initial_bin_width})]\n    )\n\n    bin_slider = widgets.FloatSlider(\n        value=initial_bin_width,\n        min=1,\n        max=30,\n        step=1,\n        description=\"Bin width:\",\n        readout_format=\".0f\",  # display as integer\n    )\n\n    histogram_object = figure_widget.data[0]\n\n    def set_bin_size(change):\n        histogram_object.xbins = {\"size\": change[\"new\"]}\n\n    bin_slider.observe(set_bin_size, names=\"value\")\n\n    output_widget = widgets.VBox([figure_widget, bin_slider])\n    return output_widget","6ce157e3":"df = pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')\ndf.head(2)","790a40e4":"df.describe()","de8feab7":"# chest pain\ndf['chest_pain_from_heart'] = np.where(df.ChestPainType.isin(['NAP', 'ASY']), 'N', 'Y')\n\n# resting blood pressure\nconditions = [\n    df['RestingBP'] < 120,\n    (df['RestingBP'] >= 120) & (df['RestingBP'] < 140),\n    df['RestingBP'] >= 140\n]\n\nchoices = ['optimal', 'normal', 'hypertension']\n\ndf['bP_ranges'] = np.select(conditions, choices, default=np.nan)\n\n# cholesterol\nconditions = [\n    df['Cholesterol'] < 200,\n    (df['Cholesterol'] >= 200) & (df['Cholesterol'] < 240),\n    df['Cholesterol'] >= 240\n]\n\nchoices = ['optimal', 'borderline', 'high']\n\ndf['cholesterol_ranges'] = np.select(conditions, choices, default=np.nan)\n\n# fasting blood sugar\ndf.rename(columns={'FastingBS': 'is_diabetic'}, inplace=True)\n\n# maximum heart rate achieved\ndf['optimal_max_hr'] = 220 - df['Age']\ndf['hr_ranges'] = np.where(df.optimal_max_hr>df.MaxHR, 'normal', 'high')\n\n# oldpeak ST segment\ndf['op_ranges'] = np.where(df.Oldpeak<0.5, 'normal', 'pathological')\n\n# ST slope\ndf['st_ranges'] = np.where(df.ST_Slope=='Up', 'normal', 'pathological')","27936bfb":"df.head()","d2e9731a":"cols = ['Age',\n 'Sex',\n 'chest_pain_from_heart',\n 'ExerciseAngina',\n 'RestingECG',\n 'bP_ranges',\n 'cholesterol_ranges',\n 'hr_ranges',\n 'op_ranges',\n 'st_ranges']\n\nX = df[cols]\ny = df['HeartDisease']","dd4b5a69":"y.value_counts().plot(kind='bar')\nplt.title('Patient had a heart disease Y vs N')\nplt.legend()\nplt.show()","a302b788":"cat_df = X.select_dtypes(include=[\"object\"])   \ncolumns = cat_df.columns.to_list()\n\nplt.figure(figsize=(12,12))\n\nfor i,column in enumerate(columns):\n    i += 1\n    plt.subplot(3,4,i)\n    cat_df[column].value_counts().plot(kind='bar')\n    plt.tight_layout()\n    plt.title(column)","c18ce61e":"# select columns with numeric values\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnum_df = X.select_dtypes(include=numerics)\ncolumns = num_df.columns.to_list()\n\nplt.figure(figsize=(8,8))\n\nfor i,column in enumerate(columns):\n    i += 1\n    plt.subplot(2,4,i)\n    num_df[column].plot.kde()\n    plt.title(column)\n    plt.tight_layout()\n    plt.ticklabel_format(style='plain', axis='x', scilimits=(0,0))\n    #plt.legend()","1bf165fa":"print(\"Age range: \", X.Age.min(), '-', X.Age.max())\nbins = [25, 50, 60, 80]\nX['age_binned'] = pd.cut(X['Age'], bins)","53256494":"# OrdinalEncoder \n\n# resting ECG\necg_encoder = OrdinalEncoder(categories=[['Normal','ST', 'LVH']])\nX[['ecg_encoded']] = ecg_encoder.fit_transform(X[['RestingECG']])\n\n# blood pressure\nbp_encoder = OrdinalEncoder(categories=[['optimal','normal', 'hypertension']])\nX[['bp_encoded']] = bp_encoder.fit_transform(X[['bP_ranges']])\n\n# cholesterol\nchol_encoder = OrdinalEncoder(categories=[['optimal','borderline', 'high']])\nX[['chol_encoded']] = chol_encoder.fit_transform(X[['cholesterol_ranges']])\n\n# heart rate\nhr_encoder = OrdinalEncoder(categories=[['normal', 'high']])\nX[['hr_encoded']] = hr_encoder.fit_transform(X[['hr_ranges']])\n\n# oldpeak\nop_encoder = OrdinalEncoder(categories=[['normal', 'pathological']])\nX[['op_encoded']] = op_encoder.fit_transform(X[['op_ranges']])\n\n# ST slope\nst_encoder = OrdinalEncoder(categories=[['normal', 'pathological']])\nX[['st_encoded']] = st_encoder.fit_transform(X[['st_ranges']])\n","3995feac":"# Get dummies\nX_dummy = pd.get_dummies(X[['Sex', 'age_binned', 'chest_pain_from_heart', 'ExerciseAngina']]).reset_index(drop=True)\nX_dummy.head(2)","297eb79b":"cols = [\n 'ecg_encoded',\n 'bp_encoded',\n 'chol_encoded',\n 'hr_encoded',\n 'op_encoded',\n 'st_encoded'\n]\nX = X[cols].reset_index(drop=True)\n\nX = pd.concat([X, X_dummy], axis=1)\nX.head(2)","28958e14":"attributes = X.columns.to_list()","374644d2":"# train test split\n\nX_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, stratify=y)","9c7899bc":"# MinMax scaler \nscaler = MinMaxScaler()\n\nX_scaled = round(pd.DataFrame(scaler.fit_transform(X.values), columns=X.columns.to_list()), 2)\n\nX_train_scaled = round(pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns.to_list()), 2)\nX_test_scaled = round(pd.DataFrame(scaler.fit_transform(X_test), columns=X.columns.to_list()), 2)\n\n","ef7cf43f":"dtc = DecisionTreeClassifier(criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1)","06848b9e":"cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)","afdf0d96":"X_scaled = round(pd.DataFrame(scaler.fit_transform(X.values), columns=X.columns.to_list()), 2)\n\nparam_list = {'max_depth': [None] + list(np.arange(2, 20)),\n              'min_samples_split': [2, 5, 10, 20, 30, 50, 100],\n              'min_samples_leaf': [1, 5, 10, 20, 30, 50, 100],\n             }\n\nrandom_search_dt = RandomizedSearchCV(dtc, param_distributions=param_list, n_iter=100, cv = cv, scoring = 'accuracy')\nrandom_search_dt.fit(X_scaled.values, y) # remember that y = df['HeartDisease']\nbest_score = random_search_dt.best_score_\nbest_params = random_search_dt.best_params_\nprint(\"Best score: {}, Best parameters: {}\".format(best_score, best_params))","336370eb":"# Build the model with the best parameters\nclf_dt = random_search_dt.best_estimator_","a318ad9b":"clf_dt = clf_dt.fit(X_train_scaled, y_train)\ny_pred = clf_dt.predict(X_test_scaled)\ny_pred_tr = clf_dt.predict(X_train_scaled)","459b90d7":"feat_importances = pd.Series(clf_dt.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')","67684135":"fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=2000)\ntree.plot_tree(clf_dt,\n                feature_names=attributes,\n                class_names=['Heart Attack' if x == 1 else 'No Heart Attack' for x in clf_dt.classes_],  \n                filled=True,\n                max_depth=3)\nfig.show()\n\nfig.savefig('heart_failure_decision_tree.png')","b0c0f762":"print('Train Accuracy %s' % round(accuracy_score(y_train, y_pred_tr),2))\nprint('Train F1-score %s' % f1_score(y_train, y_pred_tr, average=None))\nprint(classification_report(y_train, y_pred_tr))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_train, y_pred_tr))","05b20af6":"print('Test Accuracy %s' % round(accuracy_score(y_test, y_pred),2))\nprint('Test F1-score %s' % f1_score(y_test, y_pred, average=None))\nprint(classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))","026a1e1f":"# Compute false positive rate, true positive rate\nfpr, tpr, _ = roc_curve(y_test, y_pred)\n\n# Compute AUC (Area Under Curve)\nroc_auc = auc(fpr, tpr) \nprint(\"AUC: {}\".format(roc_auc))\n\n\nplt.figure(figsize=(8, 5))\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc))\n    \nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=20)\nplt.ylabel('True Positive Rate', fontsize=20) \nplt.tick_params(axis='both', which='major', labelsize=22)\nplt.legend(loc=\"lower right\", fontsize=14, frameon=False)\nplt.show()","f8bd1e0a":"svc = SVC(kernel = 'rbf', class_weight = 'balanced')","6deedc17":"cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\nparam_list = {'gamma': np.logspace(-4,0,100, base=10),\n              'C': np.logspace(0,3,100, base=10)\n             }\n\nrandom_search_svm = RandomizedSearchCV(svc, param_distributions=param_list, n_iter=100, cv = cv, scoring = 'accuracy')\nrandom_search_svm.fit(X, y)\nbest_score = random_search_svm.best_score_\nbest_params = random_search_svm.best_params_\nprint(\"Best score: {}, Best parameters: {}\".format(best_score, best_params))","2b8b2cdc":"clf_svm = random_search_svm.best_estimator_","850440ad":"clf_svm = clf_svm.fit(X_train, y_train)\ny_pred = clf_svm.predict(X_test)\ny_pred_tr = clf_svm.predict(X_train)","19450c6c":"print('Train Accuracy %s' % round(accuracy_score(y_train, y_pred_tr),2))\nprint('Train F1-score %s' % f1_score(y_train, y_pred_tr, average=None))\nprint(classification_report(y_train, y_pred_tr))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_train, y_pred_tr))","a766686a":"print('Test Accuracy %s' % round(accuracy_score(y_test, y_pred),2))\nprint('Test F1-score %s' % f1_score(y_test, y_pred, average=None))\nprint(classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))","9886edb5":"# Compute false positive rate, true positive rate\nfpr, tpr, _ = roc_curve(y_test, y_pred)\n\n# Compute AUC (Area Under Curve)\nroc_auc = auc(fpr, tpr) \nprint(\"AUC: {}\".format(roc_auc))\n\nplt.figure(figsize=(8, 5))\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc))\n    \nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=20)\nplt.ylabel('True Positive Rate', fontsize=20) \nplt.tick_params(axis='both', which='major', labelsize=22)\nplt.legend(loc=\"lower right\", fontsize=14, frameon=False)\nplt.show()","6fb06c36":"rf = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, \n                             min_samples_split=2, min_samples_leaf=1, class_weight=None)","7862a331":"cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\nparam_list = {'max_depth': [None] + list(np.arange(2, 20)),\n              'min_samples_split': [2, 5, 10, 20, 30, 50, 100],\n              'min_samples_leaf': [1, 5, 10, 20, 30, 50, 100],\n             }\n\nrandom_search_rf = RandomizedSearchCV(rf, param_distributions=param_list, n_iter=100, cv = cv, scoring = 'accuracy')\nrandom_search_rf.fit(X, y)\nbest_score = random_search_rf.best_score_\nbest_params = random_search_rf.best_params_\n\nprint(\"Best score: {}, Best parameters: {}\".format(best_score, best_params))","e9169ef2":"clf_rf = random_search_rf.best_estimator_","bf3315c9":"clf_rf = clf_rf.fit(X_train, y_train)\ny_pred = clf_rf.predict(X_test)\ny_pred_tr = clf_rf.predict(X_train)","0848ee1c":"print('Train Accuracy %s' % round(accuracy_score(y_train, y_pred_tr),2))\nprint('Train F1-score %s' % f1_score(y_train, y_pred_tr, average=None))\nprint(classification_report(y_train, y_pred_tr))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_train, y_pred_tr))","50abe2ae":"print('Test Accuracy %s' % round(accuracy_score(y_test, y_pred),2))\nprint('Test F1-score %s' % f1_score(y_test, y_pred, average=None))\nprint(classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))","54d584a6":"# Compute false positive rate, true positive rate\nfpr, tpr, _ = roc_curve(y_test, y_pred)\n\n# Compute AUC (Area Under Curve)\nroc_auc = auc(fpr, tpr) \nprint(\"AUC: {}\".format(roc_auc))\n\nplt.figure(figsize=(8, 5))\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc))\n    \nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=20)\nplt.ylabel('True Positive Rate', fontsize=20) \nplt.tick_params(axis='both', which='major', labelsize=22)\nplt.legend(loc=\"lower right\", fontsize=14, frameon=False)\nplt.show()","beb91235":"#### Performance on the test set","f0d73bb3":"#### I will again apply Cross-validation and the Hyperparameter Tuning (see above for documentation)","b758b802":"# Support Vector Machine\nRadial Basis Function is a commonly used kernel in SVC:\n$$K(x,x') = exp\\left(-\\frac{\\parallel x-x'\\parallel^2}{2\\gamma^2}\\right)$$\nwhere $\\parallel x-x'\\parallel^2$ is the squared Euclidean distance between two data points \nx and x', while ***C*** and ***gamma*** are two fundamental parameters for the SVC classifier using an RBF kernel.\n- **gamma:**\n\ngamma is a parameter of the RBF kernel and can be thought of as the \u2018spread\u2019 of the kernel and therefore the decision region. When gamma is low, the \u2018curve\u2019 of the decision boundary is very low and thus the decision region is very broad. When gamma is high, the \u2018curve\u2019 of the decision boundary is high, which creates islands of decision-boundaries around data points. We will see this very clearly below.\n\n- **C:**\n\nC is a parameter of the SVC learner and is the penalty for misclassifying a data point. When C is small, the classifier is okay with misclassified data points (high bias, low variance). When C is large, the classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias, high variance).","7da705ef":"## Functions","2d47ee0a":"## Preprocessing\n\nI want to bin most of the numerical features, but first I need to research what are the value ranges for each attribute, so I'll get to know what are the good and the risky values (e.g. high cholesterol, low RestingBP, etc..)\n\n- Chest pain: I'll distinguish two classes only, the ones who actually had a chest pain due to a heart attack and the ones whose was due to a different cause. Here is the [reference](https:\/\/www.harringtonhospital.org\/typical-and-atypical-angina-what-to-look-for\/)\n- Resting blood pressure: here is the [reference for the ranges](https:\/\/en.wikipedia.org\/wiki\/Blood_pressure#Classification,_normal_and_abnormal_values)\n- Cholesterol: here is the [reference for the ranges](https:\/\/www.medicalnewstoday.com\/articles\/315900#recommended-levels)\n- Fasting blood sugar: here is the [reference](https:\/\/www.mayoclinic.org\/diseases-conditions\/diabetes\/diagnosis-treatment\/drc-20371451#:~:text=Fasting%20blood%20sugar%20test.&text=A%20fasting%20blood%20sugar%20level,separate%20tests%2C%20you%20have%20diabetes.)\n- Maximum heart rate achieved: here is [how to calculate a person's optimal maximum heart rate](https:\/\/www.cdc.gov\/physicalactivity\/basics\/measuring\/heartrate.htm#:~:text=You%20can%20estimate%20your%20maximum,beats%20per%20minute%20(bpm))\n- Oldpeak ST depression: here is the [reference](https:\/\/ecgwaves.com\/st-segment-normal-abnormal-depression-elevation-causes\/#:~:text=ST%20segment%20depression%20is%20measured,or%20more%20is%20considered%20pathological)\n- ST slope: Here is the [reference](https:\/\/litfl.com\/st-segment-ecg-library\/)","330b4721":"#### I will again apply Cross-validation and the Hyperparameter Tuning (see above for documentation)","c2c1bf14":"#### Test set","2462e5b4":"Now that I have assigned at the values of all the features, I can plot them and see how the population is divided between the different classes.","01c94d22":"# Predictive models\n## We'll be analyzing the following models:\n- Decision Tree Classifier\n- Support Vector Machine\n- Random Forest Classifier\n\n# Decision Tree Classifier","cb09b26b":"### Perform classifications and analyze the performances","08c8ec5d":"Next steps are:\n- apply OrdinalEncoder to all those features that show an ordinal ordering (e.g. cholesterol_ranges)\n- get dummies from the remaining categorical features \n- apply MinMaxScaler to all the features\n\nNote: Before applying any scaling transformations it is very important to **split your data into training and test set**. If you start scaling before, your training (and test) data might end up scaled around a mean value that is not actually the mean of the train or test data, and go past the whole reason why you\u2019re scaling in the first place.","c7b8ab71":"### Thank you! If you like my work, please upvote it! It keeps me motivated\n### Other kernels:\n- https:\/\/www.kaggle.com\/simomatt\/preprocessing-clustering-pca-3d-visualization\/notebook","45317ebe":"### Visualize the Decision Tree","65daf562":"#### Test set","3c5aaa66":"# Context\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n\n## Attribute Information\n- Age: age of the patient [years]\n- Sex: sex of the patient [M: Male, F: Female]\n- ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n- RestingBP: resting blood pressure [mm Hg]\n- Cholesterol: serum cholesterol [mg\/dl]\n- FastingBS: fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n- RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions - and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n- MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n- ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n- Oldpeak: oldpeak = ST [Numeric value measured in depression]\n- ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n- HeartDisease: output class [1: heart disease, 0: Normal]\n## Source\nThis dataset was created by combining different datasets already available independently but not combined before. In this dataset, 5 heart datasets are combined over 11 common features which makes it the largest heart disease dataset available so far for research purposes. The five datasets used for its curation are:\n\n- Cleveland: 303 observations\n- Hungarian: 294 observations\n- Switzerland: 123 observations\n- Long Beach VA: 200 observations\n- Stalog (Heart) Data Set: 270 observations\n\nTotal: 1190 observations\n\nDuplicated: 272 observations\n\nFinal dataset: 918 observations","0af0bafb":"#### Training set","9d1f80c7":"### Cross-validation:\nLearning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test. [source](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html)\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/grid_search_workflow.png\" alt=\"image info\" style=\"width: 500px;\"\/>\n\n\n*StratifiedKFold* provides train\/test indices to split data in train\/test sets.\nThis cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.\n\n","2e037a57":"I will now bin the age values","33a42545":"### Analyze the performances\n#### Performance on the training set","a8e06468":"# Random Forest\n\nA random forest is a machine learning technique that\u2019s used to solve regression and classification problems. It utilizes ensemble learning, which is a technique that combines many classifiers to provide solutions to complex problems.\n\nA random forest algorithm consists of many decision trees. The \u2018forest\u2019 generated by the random forest algorithm is trained through bagging (or bootstrap aggregating). Bagging is an ensemble meta-algorithm that improves the accuracy of machine learning algorithms.\n\nFor more info, take a look [here](https:\/\/www.section.io\/engineering-education\/introduction-to-random-forest-in-machine-learning\/)","d714ea8f":"### ROC curve\nThe **ROC curve** (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n\n- True Positive Rate\n- False Positive Rate\n\n**AUC** stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve.\n\nYou can find more info about ROC and AUC [here](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc)","cb7a7587":"### Perform classifications and analyze the performances","24ba179a":"### ROC curve","c4816a2a":"# Preprocessing, Feature Engineering and Visualization ","6a649a7d":"### Perform classification and visualize features importance","6ba9f872":"### ROC curve","0b90a51c":"### Hyperparameter optimization\nFind more info [here](https:\/\/www.jeremyjordan.me\/hyperparameter-tuning\/) on hyperparameter optimization\/tuning.","ccaed28e":"#### Training set"}}