{"cell_type":{"844d1d96":"code","643b14df":"code","4cc27e7a":"code","9f93f752":"code","2f36c10f":"code","fc04276a":"code","0d22875a":"code","5c0d2082":"code","58ce70f0":"code","2e94f6bf":"code","93c42eff":"code","57928f63":"code","543cd387":"code","1765a331":"markdown","7a7bf9bd":"markdown","7a8ec6c3":"markdown","65e945e7":"markdown","ca309471":"markdown","b2fdbc1f":"markdown","2ba58523":"markdown","aa21ea36":"markdown","42ed2aca":"markdown","d6e0e1f8":"markdown","d427563a":"markdown","0d99b2aa":"markdown","e27a7a98":"markdown","11da6a1c":"markdown"},"source":{"844d1d96":"from copy import deepcopy\nfrom datetime import datetime as dt\nfrom itertools import product\nimport math\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import RidgeCV, Ridge, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tqdm import tqdm\nimport warnings\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline","643b14df":"def reload(df=\"train.csv\", dropped_columns=['Id']): # set dropped_columns = [] if want to keep Id column\n    data_path = \"..\/input\/house-prices-advanced-regression-techniques\/\"\n    if df == \"all_data\":\n        train = pd.read_csv(data_path + \"train.csv\")\n        test = pd.read_csv(data_path + \"test.csv\")\n        data = train.drop(columns=\"SalePrice\").append(test)\n    else:\n        data = pd.read_csv(data_path + df)\n    print(df + \" loaded successfully!\")\n    return data.reset_index(drop=True).drop(columns=dropped_columns)\n\ntrain = reload()\ntest = reload(\"test.csv\", dropped_columns=[])\nall_data = reload(\"all_data\")","4cc27e7a":"print(\"train shape: {}\".format(train.shape))\nprint(\"test shape: {}\\n\\n\".format(test.shape))\n\nfor df, name in zip([train, test], [\"train\", \"test\"]):\n    print(\"There are {} features with Nulls in the {} set.\".format(df.isna().any().sum(), name))\n\nnull_counts = all_data.isna().sum()\nprint(\"\\nFeatures with Nulls:\")\nprint(*list(null_counts[null_counts > 0].index), sep=\", \")\n\nprint(\"\\n\" + \"=\"*20 +\"\\nNull ratio in tran set:\\n\")\nnull_cols = train.isna().sum() \/ len(train)\nprint(null_cols[null_cols > 0].sort_values())","9f93f752":"garage = ['GarageType', 'GarageYrBlt', 'GarageFinish'\n          , 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond']\nindex_nonerror = all_data[garage].isin([0, np.nan]).sum(axis=1) >= len(garage)\nprint(\"a) Nulls that most likely represent missing properties (non-erroneous)\")\nall_data.loc[index_nonerror, garage].head()","2f36c10f":"indx_nonerror = all_data[garage].isin([0, np.nan]).sum(axis=1)\nindx_nonerror = (indx_nonerror < len(garage)) & (indx_nonerror > 0)\nprint(\"b) Nulls that are actually missing data (erroneous)\")\nall_data.loc[indx_nonerror, garage].head()","fc04276a":"class RelationImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, related, threshold=None, strategy=\"mean\", fill_value=None\n                 , missing_num=np.nan, missing_obj=np.nan\n                 , final_num=0, final_obj=\"missing_value\"):\n        self.related = related if isinstance(related[0], list) else [related]\n        self.threshold = threshold\n        if strategy == 'constant':\n            self.fill_value = fill_value\n        elif strategy not in ['mean', 'median', 'most_frequent']:\n            raise Exception(\"Wrong strategy type! --{}\".format(strategy))\n        self.strategy = strategy\n        self.missings = [missing_num, missing_obj, final_num, final_obj]\n        self.missing_num, self.missing_obj, self.final_num, self.final_obj =\\\n            self.missings            \n            \n    def fit(self, X, y=None):\n        if self.strategy == 'constant':\n            self.statistics_ = pd.Series(data=self.fill_value, index=X.columns)\n        else:\n            # object columns always have strategy=\"most_frequent\"\n            self.statistics_ = X[X.select_dtypes(object).columns].mode().T[0]\n            num_cols = X.select_dtypes(\"number\").columns\n            if self.strategy ==  \"most_frequent\":\n                self.statistics_ = self.statistics_.append(X[num_cols].mode().T[0])\n            elif self.strategy in [\"mean\", \"median\"]:\n                strat = eval(\"X[num_cols].{}()\".format(self.strategy))\n                self.statistics_ = self.statistics_.append(strat)\n        return self\n    \n    def _indexes(self, X, cols):\n        \"\"\"\n            Return indexes of \"actual\" nulls.\n        \"\"\"\n        if not self.threshold:\n            l = len(cols)\n        elif (self.threshold > 0) & (self.threshold < 1):\n            l = round(len(cols)*self.threshold)\n        else:\n            l = self.null_threshold\n        missing = X[cols].isin(self.missings).sum(axis=1)\n        index = (missing > 0) & (missing < l)\n        return index\n        \n    def transform(self, X, y=None):\n        X = X.copy()\n        obj_cols = list(X.select_dtypes(object).columns)\n        num_cols = list(X.select_dtypes(\"number\").columns)\n        obj_len = len(obj_cols)\n        num_len = len(num_cols)\n        to_replace = pd.Series(\n            data=[self.missing_obj]*obj_len + [self.missing_num]*num_len\n            , index=obj_cols + num_cols)\n        final_value = pd.Series(\n            data=[self.final_obj]*obj_len + [self.final_num]*num_len\n            , index=obj_cols + num_cols)\n        \n        for cols in self.related:\n            index = self._indexes(X, cols)\n            X.loc[index, cols] = X.loc[index, cols].replace(\n                to_replace=to_replace, value=self.statistics_[cols])\n            X[cols] = X[cols].replace(to_replace=to_replace, value=final_value)\n        \n        flat_related = [c for sublist in self.related for c in sublist]\n        remaining = [c for c in X.columns if c not in flat_related]\n        X[remaining] = X[remaining].replace(to_replace=to_replace\n                                            , value=self.statistics_[remaining])\n        return X","0d22875a":"# Here is an example\ntest_df = pd.DataFrame({'c1':[np.nan,2,0,0,5,5,7,33]\n                        , 'c2':['a','b',np.nan,'d','a',np.nan,'g',np.nan]\n                        , 'c3':[np.nan,13,np.nan,64,-999,np.nan,713, np.nan]\n                        , 'c4':[99,100,np.nan,102,103,0,105,33]\n                       })\nprint(\"Original dataframe:\")\ntest_df","5c0d2082":"test_related = [['c1','c2'],['c3']]\ntest_RI = RelationImputer(test_related)\ntest_RI.fit(test_df)\nprint(\"Impute value of each column:\\n{}\\n\".format(test_RI.statistics_.sort_index()))\nprint(\"Transformed dataframe:\")\ntest_RI.transform(test_df)","58ce70f0":"RELATED = [[\"Alley\"]\n           , [\"Condition1\", \"Condition2\"]\n           , [\"Exterior2nd\"]\n           , [\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\"\n              , \"BsmtFinSF1\", \"BsmtUnfSF\", \"TotalBsmtSF\"\n              , \"BsmtFullBath\", \"BsmtHalfBath\"] # Basement\n           , [\"BsmtFinType2\", \"BsmtFinSF2\"] # Basement 2\n           , [\"Heating\", \"HeatingQC\"] # Heating\n           , [\"KitchenQual\", \"KitchenAbvGr\"] # Kitchen\n           , [\"FireplaceQu\", \"Fireplaces\"] # Fireplaces\n           , [\"GarageType\", \"GarageYrBlt\", \"GarageFinish\" # Garage\n              , \"GarageCars\", \"GarageArea\", \"GarageQual\", \"GarageCond\"]\n           , [\"MasVnrType\", \"MasVnrArea\"] # Masonry Veneer\n           , [\"PoolArea\", \"PoolQC\"] # Pool\n           , [\"Fence\"] # Fence\n           , [\"MiscFeature\", \"MiscVal\"] # Other miscellaneous\n            ]\nRI = RelationImputer(RELATED, strategy=\"constant\", fill_value=np.nan)\ntrue_nulls = RI.fit_transform(train)\ntrue_nulls = true_nulls.isna().sum(axis=0) \/ len(train)\ntrue_nulls[true_nulls > 0]","2e94f6bf":"# Pre-process data before imputation\nclass PreImputation(BaseEstimator, TransformerMixin):\n    def __init__(self, old_new, numeric_to_object):\n        self.numeric_to_object = numeric_to_object\n        self.old_new = old_new\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X = X.copy()\n        sold_time = X[[\"MoSold\", \"YrSold\"]].astype(str).agg(\"\/\".join, axis=1)\n        sold_time = pd.to_datetime(sold_time)\n        for old in self.old_new:\n            X[self.old_new[old]] = (sold_time - pd.to_datetime(X[old], format=\"%Y\"))\\\n                \/ pd.Timedelta(days=365.25)\n        X[self.numeric_to_object] = X[self.numeric_to_object].astype(str)\n        X.drop(columns=list(self.old_new.keys()), inplace=True)\n        return X\n\ndef change_nullables(cols, map_dict):\n    return [c if c not in map_dict.keys() else map_dict[c] for c in cols]\nDATE_TO_YRS = {\"YearBuilt\":\"Age\"\n               , \"YearRemodAdd\":\"YrSinceRemod\"\n               , \"GarageYrBlt\": \"YrSinceGarageBlt\"}\nRELATED = list(map(lambda x: change_nullables(x, DATE_TO_YRS), RELATED))\n\nNUM_TO_OBJ = [\"MSSubClass\", \"MoSold\", \"YrSold\"]\n# CAT_COLS = list(all_data.select_dtypes(object).columns) + NUM_TO_OBJ\nCAT_COLS=['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour',\n       'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1',\n       'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n       'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond',\n       'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n       'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n       'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType',\n       'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC',\n       'Fence', 'MiscFeature', 'MoSold', 'YrSold', 'SaleType',\n       'SaleCondition']\nNUM_COLS = [col for col in all_data.columns if col not in CAT_COLS]\nNUM_COLS = [col \n            if col not in DATE_TO_YRS.keys() \n            else DATE_TO_YRS[col] \n            for col in NUM_COLS]\nIMPUTE_STRAT = \"most_frequent\"\nFINAL_NUM = 0\nFINAL_OBJ = \"None\"\nL1_RATIO = 0.5\nALPHAS = [0.1, 0.3, 1, 3, 10]","93c42eff":"# estimators\nPreI = PreImputation(DATE_TO_YRS, NUM_TO_OBJ)\nRidge = RidgeCV(alphas=ALPHAS, normalize=True, store_cv_values=True)\nEnet = ElasticNetCV(l1_ratio=L1_RATIO\n                    , alphas=ALPHAS\n                    , normalize=True\n                    , max_iter=5000\n                    , random_state=713\n                   )\nGBR = GradientBoostingRegressor(n_estimators=500\n                                , min_samples_split=5\n                                , min_samples_leaf=2\n                                , max_depth=2\n                               )\nestimators = {\"Ridge\": Ridge\n              , \"Elastic Net\": Enet\n              , \"Gradient Boosting\": GBR\n             }\n\n# Simple-imputing\ncat_SI_OHE = Pipeline(\n    [(\"cat_Imputation\", SimpleImputer(strategy=\"most_frequent\"))\n     , (\"cat_OHE\", OneHotEncoder(handle_unknown=\"ignore\"))]\n    )\nnum_SI_OHE = Pipeline(\n    [(\"num_Imputation\", SimpleImputer(strategy=IMPUTE_STRAT))]\n    )\nSI_imp_OHE = ColumnTransformer(\n    [(\"cat_SI_OHE\", deepcopy(cat_SI_OHE), CAT_COLS)\n     , (\"num_SI_OHE\", deepcopy(num_SI_OHE), NUM_COLS)]\n    , remainder=\"passthrough\"\n    )\nSI_transform = [(\"PreImputation\", deepcopy(PreI))\n                , (\"Imputation_OHE\", deepcopy(SI_imp_OHE))\n               ]\n\n# Relation-imputing\nRI = RelationImputer(RELATED, strategy=IMPUTE_STRAT\n                     , final_num=FINAL_NUM, final_obj=FINAL_OBJ)\nRI_OHE = ColumnTransformer(\n    [('OHE', OneHotEncoder(handle_unknown=\"ignore\"), CAT_COLS)]\n    , remainder=\"passthrough\"\n    )\nRI_transform = [(\"PreImputation\", deepcopy(PreI))\n                , (\"Imputation\", deepcopy(RI))\n                , (\"OHE\", deepcopy(RI_OHE))\n               ]\n\n# Mixed-imputing\n# Here we have something that is kind of in-between\n# the two logics above - SimpleImputer and RelationImputer\n#\n# All columns in RELATED list will be imputed with strategy=\"constant\"\n# while the rest is imputed with strategy=\"mean\" (numerical) and\n# \"most_frequent\" (categorical)\n\nRELATED_flat = [c for cs in RELATED for c in cs]\nNUM_const, CAT_const = [\n    [c for c in COLS if c in RELATED_flat] \n    for COLS in [NUM_COLS, CAT_COLS]\n]\nNUM_imp, CAT_imp = [\n    [c for c in COLS if c not in RELATED_flat] \n    for COLS in [NUM_COLS, CAT_COLS]\n]\nMI_NUM_const = SimpleImputer(strategy=\"constant\", fill_value=0)\nMI_NUM_imp = SimpleImputer(strategy=\"mean\")\nMI_CAT_const = Pipeline(\n    [(\"MI_CAT_const\", SimpleImputer(strategy=\"constant\", fill_value=\"missing_value\"))\n     , (\"MI_CAT_OHE\", OneHotEncoder(handle_unknown=\"ignore\"))]\n)\nMI_CAT_imp = Pipeline(\n    [(\"MI_CAT_imp\", SimpleImputer(strategy=\"most_frequent\"))\n     , (\"MI_CAT_OHE\", OneHotEncoder(handle_unknown=\"ignore\"))]\n)\nMI_imp_OHE = ColumnTransformer(\n    [(\"NUM_const\", deepcopy(MI_NUM_const), NUM_const)\n     , (\"NUM_imp\", deepcopy(MI_NUM_imp), NUM_imp)\n     , (\"CAT_const\", deepcopy(MI_CAT_const), CAT_const)\n     , (\"CAT_imp\", deepcopy(MI_CAT_imp), CAT_imp)]\n    , remainder=\"passthrough\"\n)\nMI_transform = [(\"PreImputation\", deepcopy(PreI))\n                , (\"Imputation_OHE\", deepcopy(MI_imp_OHE))\n               ]\n\ntransformers = {\"SimpleImputer\": SI_transform\n                , \"RelationImputer\": RI_transform\n                , \"Mixed\": MI_transform\n               }","57928f63":"KFOLDS = 3\ndef RMSE(y_true, y_pred):\n    return math.sqrt(mean_squared_error(y_true, y_pred))\nrmse_scorer = make_scorer(RMSE)\nX = train.drop(columns=\"SalePrice\").copy()\ny = train.SalePrice.copy()\ny = np.log1p(y)\n\ni = 0\noutput = pd.DataFrame({\"Transformer\": []\n                       , \"Estimator\": []\n                       , \"Mean score\": []\n                       , \"Std score\": []\n                      })\nfor e, t in tqdm(list(product(estimators, transformers))):\n    trans = deepcopy(transformers[t])\n    trans.append((\"Estimator\", estimators[e]))\n    pipe = Pipeline(trans)\n    scores = cross_val_score(\n        pipe, X=X, y=y, scoring=rmse_scorer, cv=KFOLDS, n_jobs=-1\n    )\n    output.loc[i] = [t, e, np.mean(scores), np.std(scores)]\n    i += 1\noutput","543cd387":"selected_pipeline = Pipeline(\n    RI_transform + [(\"GBR\", deepcopy(GBR))]\n)\nsubmission = pd.DataFrame()\nsubmission[\"Id\"] = test[\"Id\"]\nselected_pipeline.fit(X, y)\nsubmission[\"SalePrice\"]= selected_pipeline.predict(test.drop(columns=\"Id\"))\nsubmission[\"SalePrice\"] = np.expm1(submission.SalePrice)\nsubmission.to_csv(\"Submission.csv\", index=False)\nsubmission.head()","1765a331":"As we can see, only *LotFrontage* seems to have a lot of actual missing data and possibly should be dropped during feature selection (though it should be noted that for features with significant portion being `Null` that we see earlier such as *PoolQC*, most of their values will be the same - \"missing_value\" in this case - so eventually they may get dropped anyway). ","7a7bf9bd":"# Package & Data Loading <a id='Loading'><\/a>","7a8ec6c3":"The imputer performs as expected!\n## 3. Actual Missing Values <a id='ActualMissing_RelationImputer'><\/a>\nHere I use `RelationImputer` to see how many actual `Null`s are there in the train set (by setting impute strategy `strategy`=\"constant\" and imputed value `fill_value`=`np.nan` so all `Null`s stay the same).","65e945e7":"For this dataset, being able to tell which `Null` represents property-missing and which one is value-missing will much benefit imputation process. Here, one may wonder why we don't use Scikit-Learn's [IterativeImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html). The reason is that `IterativeImputer` doesn't support imputing categorical (classification) and numerical features (regression) at the same time. There is already a [request](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17087) for such ability, but as of now (scikit-learn 0.23.1) it has not been available (in the same thread someone suggests [this package](https:\/\/pypi.org\/project\/autoimpute\/) which is said to be able to perform mixed-dtype imputation, but I personally have not had a chance to test it).<br>\n<br>\nHaving said that, I decided to take a (slightly) further step by making an imputer that works like `SimpleImputer` but takes into account features' \"nullability\".\n# II. RelationImputer <a id='RelationImputer'><\/a>\nIn the following section, I made an imputer called `RelationImputer` that attempts to impute \"erroneous\" (representing missing values) and \"non-erroneous\" (implying the lack of a property) `Null`s separately. The logic is described in section 1 below. You can skip this part and go to section 2 where I provide an example of how the imputer works.\n## 1. Logic <a id='Logic_RelationImputer'><\/a>\nIn `RelationImputer`, the features need to be manually separated into two types:\n- Nullable: features whose `Null`s **may** not be erroneous and, instead, due to the lack of a property (*GarageType*, *TotalBsmtSF*, etc.).\n- Non-nullable: features whose `Null`s **always** represents missing values (*HouseStyle*, *LandContour*, etc.).<br>\n<br>\n(these names are of course unofficial so feel free to drop in any suggestion you have)","ca309471":"Erroneous `Null`s are imputed similarly to Non-nullable while non-erroneous `Null`s will be labeled differently.","b2fdbc1f":"Most will choose to remove *FireplaceQu*, *Fence*, *Alley*, *MiscFeature* and *PoolQC* because they all have large amount of missing values. For the rest, one may be quick to point out that their `Null`s actually imply that a sample doesn't have a particular feature (e.g. `Null` *GarageType* means no basement). Therefore, we can use [SimpleImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html) and replace the `Null`s with a constant (e.g. \"missing_value\" for categorical `Null`s and 0 for numerical ones).<br>\n<br>\nHowever, that method doesn't apply to all cases. For example, we know that all properties must have some degree of flatness, and therefore `Null` *LandContour* is actually a missing value. Even for the case of *GarageType*, if all other garage-related features (*GarageYrBlt*, *GarageFinish*, *GarageCars*, etc.) are not missing, a `Null` *GarageType* is also likely to be erroneous. Let's take a look at the examples below.","2ba58523":"# III. Testing <a id='Testing'><\/a>\nIn this section, I do a quick comparison test between different `SimpleImputer`-like imputing ways. The three imputing way I'm testing are:\n- Simple-imputing (with `SimpleImputer`): all `Null`s are replaced by their features' statistics (mean, median, mode).\n- Relation-imputing (with `RelationImputer`): logic as mentioned above.\n- Mixed (with `SimpleImputer`): This method is kind of a mixed between the two logics above. Specifically, `Null`s in selected columns (that are possible to have \"non-error\" NULLs\") will be replaced with some constants, while the rest will be SimpleImpute-ed.","aa21ea36":"### Some preprocessing:\nBased on [data description](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data), *MSSubClass* is numerical but actually categorical (without any information regarding order of its values), so it will be handled using one-hot encoding. In addition, time-related features will be converted their respective numerical time-interval values as follows:\n- *YearBuilt*: convert to *Age* (*MoSold*\/*YrSold* - *YearBuilt*)\n- *YearRemodAdd*: convert to *YrSinceRemod* (*MoSold*\/*YrSold* - *YearRemodAdd*)\n- *GarageYrBlt*: convert to *YrSinceGarageBlt* (*MoSold*\/*YrSold* - *GarageYrBlt*)\n- *MoSold*, *YrSold*: convert to categorical features.","42ed2aca":"## 2. Example <a id='Example_RelationImputer'><\/a>\nHere I provide an example for how `RelationImputer` works.<br>\n<br>\nBelow is an example dataset. Columns *c1*, *c2* and *c3* are Nullable and *c4* is Non-nullable. In addition, *c1* and *c2* are related (i.e. they are all about a property of the samples).","d6e0e1f8":"In this notebook, I focus on imputing the `Null`s in the dataset. Different from most of other publicly available notebooks which either ignore features\/samples that contain `Null`s or use [SimpleImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html), I took a little further step and examined the relationship between features.\n<br><br>\n### Table of Content\n...<a href='#Loading'>Package & Data Loading<\/a><br>\n...<a href='#QuickLook'>I. A Quick Look<\/a><br>\n...<a href='#RelationImputer'>II. RelationImputer<\/a><br>\n......<a href='#Logic_RelationImputer'>1. Logic<\/a><br>\n......<a href='#Example_RelationImputer'>2. Example<\/a><br>\n......<a href='#ActualMissing_RelationImputer'>3. Actual Missing Values<\/a><br>\n...<a href='#Testing'>III. Testing<\/a><br>","d427563a":"Based on the logic of `RelationImputer`:\n- `Null` *c1* of sample 1, and `Null` *c2* of sample 5 and sample 7 are actual missing values and should be imputed.\n- `Null` *c2* of sample 2 is non-erroneous and should be labeled differently (in this case it's \"missing_value\" for categorical and 0 for numerical feature). \n- All `Null`s in *c4* should be missing values and therefore imputed.<br>\n<br>\nBelow is the actual imputed result.","0d99b2aa":"# I. A Quick Look <a id='QuickLook'><\/a>\nLet's first take a glance at the data.","e27a7a98":"\nActually there is not much difference between the three methods. In my next notebook, we will observe if `RelationImputer`makes a bigger difference when features are more carefully selected.","11da6a1c":"`RelationImputer` imputes a Non-nullable feature in a way that is mostly similar that of `SimpleImputer`. On the other hand, for a Nullable feature, `RelationImputer` will consider other Nullables that are related to the feature (thus the name) to determine if the feature's `Null` is erroneous or not. Specifically:\n- If the number of `Null`s among those related Nullables is less than a specified threshold (which is by default equal to the number of the features themselves) then the `Null` being examined is an actually erroneous and missing data point.\n- Otherwise, the `Null` is considered non-erroneous"}}