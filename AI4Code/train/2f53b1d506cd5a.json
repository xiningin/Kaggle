{"cell_type":{"6b7b4615":"code","5e1da516":"code","f6c757f3":"code","03c8e252":"code","66fdb137":"code","3f43c408":"code","2c2309b8":"code","742e82d5":"code","c2bc6c89":"code","a40333fb":"code","75117a2d":"code","aefd493a":"code","b3cc79ac":"code","2c9841e7":"code","29223cb0":"code","49c09dd7":"code","052bc2db":"code","cca1a8f3":"code","444ab3ee":"code","7888e1cb":"code","129f1bd6":"code","c6231b91":"markdown","3f6d69d8":"markdown","07c1b5f0":"markdown","1835a9ff":"markdown","b3cfe7f6":"markdown","24bf2fe1":"markdown","5c2fa0ee":"markdown","3074256c":"markdown","86e21d18":"markdown","7f19e039":"markdown","7bc7e8be":"markdown","71c992d0":"markdown","38ebcf3e":"markdown","d5cd7cb3":"markdown","601ed20d":"markdown","ef0b4324":"markdown"},"source":{"6b7b4615":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nsns.set()\n%matplotlib inline\nfrom pylab import rcParams\n\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\n\n\n\n\n","5e1da516":"print(df.shape)\ndf.head()","f6c757f3":"df.info()","03c8e252":"df.describe()","66fdb137":"df.columns","3f43c408":"fraud_cases=len(df[df['Class']==1])\nprint('number of fraud cases:=',fraud_cases)\nNonfraud_cases=len(df[df['Class']==0])\nprint('number of Nonfraud cases:=',Nonfraud_cases)","2c2309b8":"print('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')","742e82d5":"## Get the Fraud and the normal dataset \n\nfraud = df[df['Class']==1]\n\nnormal = df[df['Class']==0]\n\nLABELS=['Normal','Fraud']","c2bc6c89":"count_classes=pd.value_counts(df['Class'],sort=True)\ncount_classes.plot(kind='bar',color='red')\nplt.title('Transaction Class Distribution')\nplt.xticks(range(2),LABELS)\nplt.xlabel('class')\nplt.ylabel('frequency')","a40333fb":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 50\nax1.hist(fraud.Amount, bins = bins)\nax1.set_title('Fraud')\nax2.hist(normal.Amount, bins = bins)\nax2.set_title('Normal')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show()","75117a2d":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Time of transaction vs Amount by class')\nax1.scatter(fraud.Time, fraud.Amount)\nax1.set_title('Fraud')\nax2.scatter(normal.Time, normal.Amount)\nax2.set_title('Normal')\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","aefd493a":"x=df.drop('Class',axis=1)\ny=df['Class']","b3cc79ac":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.2)\n","2c9841e7":"from sklearn.linear_model import LogisticRegression\nlog=LogisticRegression()\nlog.fit(x_train,y_train)\ny_pred=log.predict(x_test)\ny_pred","29223cb0":"from sklearn.metrics import accuracy_score, confusion_matrix\naccuracy_score(y_test,y_pred)","49c09dd7":"#Random UnderSampling\nnormal_under=normal.sample(fraud_cases)\nunder_sample=pd.concat([normal_under,fraud],axis=0)\nprint('random under sampling:')\nprint(under_sample.Class.value_counts())\nunder_sample.Class.value_counts().plot(kind='bar',title='Count')","052bc2db":"#Random OverSampling\nfraud_over=fraud.sample(Nonfraud_cases,replace=True)\nover_sample=pd.concat([fraud_over,normal],axis=0)\nprint('random under sampling:')\nprint(over_sample.Class.value_counts())\nover_sample.Class.value_counts().plot(kind='bar',title='Count')","cca1a8f3":"over_sample.head()\nX=over_sample.drop('Class',axis=1)\nY=over_sample['Class']\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=.2)","444ab3ee":"from sklearn.linear_model import LogisticRegression\nlog=LogisticRegression()\nlog.fit(x_train,y_train)\ny_pred_log=log.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score \nfrom sklearn.metrics import f1_score\n\nprint('Accuracy:',accuracy_score(y_test,y_pred_log))\nprint('Precision:',precision_score(y_test,y_pred_log))\nprint('Recall:',recall_score(y_test,y_pred_log))\nprint('F1 score:',f1_score(y_test,y_pred_log))","7888e1cb":"conf_mat_log=confusion_matrix(y_test, y_pred_log)\nprint('Confusion Matrix:\\n',conf_mat_log)\n\nlabels=['Class 0','Class 1']\nfig=plt.figure()\nax=fig.add_subplot(111)\ncax=ax.matshow(conf_mat_log,cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","129f1bd6":"from sklearn.naive_bayes import GaussianNB\nnv = GaussianNB()\nnv.fit(x_train,y_train)\ny_pred_nv=nv.predict(x_test)\nprint('Accuracy:',accuracy_score(y_test,y_pred_nv))","c6231b91":"# LOGISTIC REGRESSION ON ORIGINAL DATA(Imbalanced Data)","3f6d69d8":"# Fraud Case and Non Fraud cases","07c1b5f0":"Do fraudulent transactions occur more often during certain time frame ? Let us find out with a visual representation.","1835a9ff":"We need to analyze more amount of information from the transaction data\nHow different are the amount of money used in different transaction classes?","b3cfe7f6":"# Performing Sampling Technique to balance the data\n \nRandom Oversampling: Randomly duplicate examples in the minority class. Random Undersampling: Randomly delete examples in the majority class.","24bf2fe1":"**Notice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!\n**","5c2fa0ee":"# Motive","3074256c":"# Challenges Involved","86e21d18":"# LOGISTIC REGRESSION","7f19e039":"Credit card fraud detection is:\n* One of the most explored domains of fraud detection\n* Relies on the automatic analysis of recorded transactions\n\n\nThe main challenges in credit card fraud detection are:\n* Huge size of data\n* Imbalanced dataset\n* Availability of data","7bc7e8be":"\n\nLet's check data unbalance with respect with target value, i.e. Class.\n","71c992d0":"* Identify fraudulent credit card transactions.\n* to find new methods for fraud detection and to increase the accuracy of results.\n* to detect the credit card fraud in the dataset obtained from ULB by applying Logistic regression, Decision tree to evaluate their   Accuracy, sensitivity, specificity, precision using different models and compare and collate them to state the best possible model   to solve the credit card fraud detection problem.","38ebcf3e":"# Naive Bayes Classifier","d5cd7cb3":"* The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n* This dataset presents transactions that occurred in two days,where we have 492 frauds out of 284,807 transactions.\n* It contains only numeric input variables which are the result of a PCA transformation.\n\n","601ed20d":"# Dataset","ef0b4324":"In this kernel we will use various predictive models to see how accurate they are in detecting whether a transaction is a normal payment or a fraud."}}