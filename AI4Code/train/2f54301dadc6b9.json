{"cell_type":{"6f882121":"code","d9437e3f":"code","95341818":"code","4ea24101":"code","faac8e26":"code","ea5d95a8":"code","40efd4cd":"code","01d9a539":"code","b57fb1be":"code","c607d12c":"code","1072a7af":"code","dbfbc038":"code","fb4bbb1d":"code","af330757":"code","8b093291":"code","91363d8b":"code","1251c3c7":"code","7536f0f8":"code","7ce2e90b":"code","05881c25":"code","51dde232":"code","bf4977a1":"code","38c3b380":"code","17090028":"code","55c54ca4":"code","26e015ec":"code","5b177397":"code","3d3ed994":"code","8a10e244":"code","3f808c43":"code","3c9c9ee1":"code","ff6b264b":"code","6c645ed5":"code","0ffe8252":"code","e45af495":"code","f8b81a52":"code","b648b68f":"code","82670f46":"code","7c4a96c7":"code","151e1cf1":"code","445a134d":"code","1905d39c":"code","4c85ec00":"code","812a5d96":"code","19e5a5e9":"code","3d0cb8c1":"code","d992937e":"code","6c9f4c51":"code","626f3d5b":"code","dd6f6055":"code","754b7870":"code","a7c61725":"code","5b6b9733":"code","dd40bd4d":"code","c421864c":"code","387fcce8":"code","7b625912":"code","e6774ce4":"code","49ccfc33":"code","85f68bf6":"code","84e1e2cc":"code","e0de17d6":"code","675129e7":"code","5ff05be8":"code","80ec246a":"code","e466c6fc":"code","23835708":"code","c8a7ad05":"code","92fd24e2":"code","26f4e0bc":"code","59d08b22":"code","007602ee":"code","0f5eaaa3":"code","2490add1":"code","a1d7756a":"code","bbf3f5de":"code","09838a35":"code","cd89d9d6":"code","cd007d99":"code","6e5d3729":"code","f407066a":"code","217df268":"code","3063c4cc":"code","a2be1165":"code","d07be38d":"code","13e04b5c":"code","1afc29ed":"code","2b915fd2":"markdown","5710803e":"markdown","3d3510e0":"markdown","ad778a0b":"markdown","6af92119":"markdown","a33216e7":"markdown","192c8992":"markdown","b19e284d":"markdown","cd417877":"markdown","7260fa4d":"markdown","0b3a686c":"markdown","b20c9f1b":"markdown","2de49a9a":"markdown","294c9ce5":"markdown","40ca0dc2":"markdown","cba24c44":"markdown","1593da3b":"markdown","98cd3b86":"markdown","1e0948ca":"markdown","693ae12c":"markdown","b6fb313a":"markdown","8d1b3435":"markdown","00ad12ce":"markdown","0afde8ab":"markdown","9fd2a836":"markdown","d96b3cc4":"markdown","65658df0":"markdown","b519186c":"markdown","ee57b3ad":"markdown","fc82aea4":"markdown","282abf28":"markdown","5d43e265":"markdown","a7444169":"markdown","273c3d85":"markdown","7786ff89":"markdown","a66eeff7":"markdown","e856d4ab":"markdown","f10bcbda":"markdown","a001b12f":"markdown","98069f28":"markdown","f720abef":"markdown","74c788b7":"markdown","3a2671f2":"markdown","4d3fd26f":"markdown"},"source":{"6f882121":"import numpy as np\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn import decomposition\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nnp.set_printoptions(suppress=True)","d9437e3f":"categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\nremove = ('headers', 'footers', 'quotes')\n# fix for SSL handshake error: http:\/\/thomas-cokelaer.info\/blog\/2016\/01\/python-certificate-verified-failed\/\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\n# Captain Obvious reminds us to enable Internet ;)\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\nnewsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)","95341818":"newsgroups_train.filenames.shape, newsgroups_train.target.shape","4ea24101":"print(\"\\n\".join(newsgroups_train.data[:3]))","faac8e26":"np.array(newsgroups_train.target_names)[newsgroups_train.target[:3]]","ea5d95a8":"newsgroups_train.target[:10]","40efd4cd":"num_topics, num_top_words = 6, 8","01d9a539":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","b57fb1be":"vectorizer = CountVectorizer(stop_words='english')\nvectors = vectorizer.fit_transform(newsgroups_train.data).todense() # (documents, vocab)\nvectors.shape # vectors.nnz \/ vectors.shape[0], row_means.shape","c607d12c":"print(len(newsgroups_train.data), vectors.shape)","1072a7af":"vocab = np.array(vectorizer.get_feature_names())\nvocab.shape","dbfbc038":"vocab[7000:7020]","fb4bbb1d":"%time U, s, Vh = linalg.svd(vectors, full_matrices=False) # %time is line magic","af330757":"print(U.shape, s.shape, Vh.shape)","8b093291":"#Exercise: confirm that U, s, Vh is a decomposition of the var Vectors\nreconstructed_vectors = U @ np.diag(s) @ Vh\nnp.linalg.norm(reconstructed_vectors - vectors)\nnp.allclose(reconstructed_vectors, vectors)","91363d8b":"#Exercise: Confirm that U, Vh are orthonormal\nnp.allclose(U.T @ U, np.eye(U.shape[0]))\nnp.allclose(Vh @ Vh.T, np.eye(Vh.shape[0]))","1251c3c7":"plt.plot(s);","7536f0f8":"plt.plot(s[:10])","7ce2e90b":"num_top_words=8\n\ndef show_topics(a):\n    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n    topic_words = ([top_words(t) for t in a])\n    return [' '.join(t) for t in topic_words]","05881c25":"show_topics(Vh[:10])","51dde232":"m,n=vectors.shape\nd=5  # num topics","bf4977a1":"# pegs CPU usage to almost 200%\nclf = decomposition.NMF(n_components=d, random_state=1)\n\nW1 = clf.fit_transform(vectors)\nH1 = clf.components_","38c3b380":"show_topics(H1)","17090028":"vectorizer_tfidf = TfidfVectorizer(stop_words='english')\nvectors_tfidf = vectorizer_tfidf.fit_transform(newsgroups_train.data) # (documents, vocab)","55c54ca4":"W1 = clf.fit_transform(vectors_tfidf)\nH1 = clf.components_","26e015ec":"show_topics(H1)","5b177397":"plt.plot(clf.components_[0])","3d3ed994":"clf.reconstruction_err_","8a10e244":"lam=1e3\nlr=1e-2\nm, n = vectors_tfidf.shape","3f808c43":"\n# pegs CPU usage to almost 200%\nW1 = clf.fit_transform(vectors)\nH1 = clf.components_","3c9c9ee1":"show_topics(H1)","ff6b264b":"mu = 1e-6\ndef grads(M, W, H):\n    R = W@H-M\n    return R@H.T + penalty(W, mu)*lam, W.T@R + penalty(H, mu)*lam # dW, dH","6c645ed5":"def penalty(M, mu):\n    return np.where(M>=mu,0, np.min(M - mu, 0))","0ffe8252":"def upd(M, W, H, lr):\n    dW,dH = grads(M,W,H)\n    W -= lr*dW; H -= lr*dH","e45af495":"def report(M,W,H): \n    print(np.linalg.norm(M-W@H), W.min(), H.min(), (W<0).sum(), (H<0).sum())","f8b81a52":"W = np.abs(np.random.normal(scale=0.01, size=(m,d)))\nH = np.abs(np.random.normal(scale=0.01, size=(d,n)))","b648b68f":"report(vectors_tfidf, W, H)","82670f46":"upd(vectors_tfidf,W,H,lr)","7c4a96c7":"report(vectors_tfidf, W, H)","151e1cf1":"# pegs CPU usage to ~ 150%+\nfor i in range(50): \n    upd(vectors_tfidf,W,H,lr)\n    if i % 10 == 0: report(vectors_tfidf,W,H)","445a134d":"show_topics(H)","1905d39c":"# old notes (w\/o GPU)\n# See https:\/\/github.com\/pytorch\/pytorch\/issues\/1668\n#if torch.cuda.available(): # AttributeError: module 'torch.cuda' has no attribute 'available'\n#    import torch.cuda as t\n#else:\n#    import torch as t\nimport torch\nimport torch.cuda as tc\nfrom torch.autograd import Variable","4c85ec00":"def V(M): return Variable(M, requires_grad=True)","812a5d96":"v=vectors_tfidf.todense()","19e5a5e9":"#t_vectors = torch.Tensor(v.astype(np.float32))#.cuda()\nt_vectors = torch.Tensor(v.astype(np.float32)).cuda()","3d0cb8c1":"mu = 1e-5","d992937e":"def grads_t(M, W, H):\n    R = W.mm(H)-M\n    return (R.mm(H.t()) + penalty_t(W, mu)*lam, \n        W.t().mm(R) + penalty_t(H, mu)*lam) # dW, dH\n\ndef penalty_t(M, mu):\n    return (M<mu).type(tc.FloatTensor)*torch.clamp(M - mu, max=0.)\n\ndef upd_t(M, W, H, lr):\n    dW,dH = grads_t(M,W,H)\n    W.sub_(lr*dW); H.sub_(lr*dH)\n\ndef report_t(M,W,H): \n    print((M-W.mm(H)).norm(2), W.min(), H.min(), (W<0).sum(), (H<0).sum())","6c9f4c51":"# old\n# See AssertionError: Torch not compiled with CUDA enabled or import torch vs torch.cuda as t (see imports above)\n#t_W = tc.FloatTensor(m,d)\nt_W = tc.FloatTensor(m,d)\nt_H = tc.FloatTensor(d,n)\nt_W.normal_(std=0.01).abs_(); \nt_H.normal_(std=0.01).abs_();\n\n","626f3d5b":"# old\n#d=6; lam=100; lr=0.05\n# checking torch version for next cell\n#print(torch.__version__) # 1.0.0.dev20181205\n# downgraded torch with: 'conda install pytorch=0.4.0 cuda92 -c pytorch'\n# See https:\/\/forums.fast.ai\/t\/runtime-error-cannot-initialize-aten-cuda-library\/20378\/5\nd=6; lam=100; lr=0.05","dd6f6055":"# old\n#%%time # I let this run for 42 hours before shutting down the kernel\n\n# First time this cell runs we get an error:\n#AssertionError: \n#Found no NVIDIA driver on your system. Please check that you\n#have an NVIDIA GPU and installed a driver from\n#http:\/\/www.nvidia.com\/Download\/index.aspx\nfor i in range(1000): \n    upd_t(t_vectors,t_W,t_H,lr)\n    if i % 100 == 0: \n        report_t(t_vectors,t_W,t_H)\n        lr *= 0.9","754b7870":"show_topics(t_H.cpu().numpy())","a7c61725":"plt.plot(t_H.cpu().numpy()[0])","5b6b9733":"t_W.mm(t_H).max()","dd40bd4d":"t_vectors.max()","c421864c":"x = Variable(torch.ones(2, 2), requires_grad=True)\nprint(x)","387fcce8":"print(x.data)\n\n","7b625912":"print(x.grad)\n\n","e6774ce4":"y = x + 2\nprint(y)","49ccfc33":"z = y * y * 3\nout = z.sum()\nprint(z, out)","85f68bf6":"out.backward()\nprint(x.grad)\n\n","84e1e2cc":"lam=1e6\n\n","e0de17d6":"# 0ld\n# Got that 'AssertionError: Found no NVIDIA driver on your system.' once sometimes twice, then it worked?!?\n# This is taking forever as was the for loop between cells 51 and 52 ... lack of GPU issue? Pleanty of RAM and swap available.\n# Skipping this after a few weeks\npW = Variable(tc.FloatTensor(m,d), requires_grad=True)\npH = Variable(tc.FloatTensor(d,n), requires_grad=True)\npW.data.normal_(std=0.01).abs_()\npH.data.normal_(std=0.01).abs_();","675129e7":"def report():\n    W,H = pW.data, pH.data\n    print((M-pW.mm(pH)).norm(2).data[0], W.min(), H.min(), (W<0).sum(), (H<0).sum())\n\ndef penalty(A):\n    return torch.pow((A<0).type(tc.FloatTensor)*torch.clamp(A, max=0.), 2)\n\ndef penalize(): return penalty(pW).mean() + penalty(pH).mean()\n\ndef loss(): return (M-pW.mm(pH)).norm(2) + penalize()*lam","5ff05be8":"M = Variable(t_vectors).cuda()","80ec246a":"opt = torch.optim.Adam([pW,pH], lr=1e-3, betas=(0.9,0.9))\nlr = 0.05\nreport()","e466c6fc":"for i in range(1000): \n    opt.zero_grad()\n    l = loss()\n    l.backward()\n    opt.step()\n    if i % 100 == 99: \n        report()\n        lr *= 0.9 # learning rate annealling","23835708":"h = pH.data.cpu().numpy()\nshow_topics(h)","c8a7ad05":"plt.plot(h[0]);","92fd24e2":"vectors.shape","26f4e0bc":"%time U, s, Vh = linalg.svd(vectors, full_matrices=False)","59d08b22":"print(U.shape, s.shape, Vh.shape)","007602ee":"%time u, s, v = decomposition.randomized_svd(vectors, 5)","0f5eaaa3":"%time u, s, v = decomposition.randomized_svd(vectors, 5)","2490add1":"u.shape, s.shape, v.shape","a1d7756a":"show_topics(v)","bbf3f5de":"from scipy import linalg","09838a35":"# computes an orthonormal matrix whose range approximates the range of A\n# power_iteration_normalizer can be safe_sparse_dot (fast but unstable), LU (in between), or QR (slow but most accurate)\ndef randomized_range_finder(A, size, n_iter=5):\n    Q = np.random.normal(size=(A.shape[1], size))\n    \n    for i in range(n_iter):\n        Q, _ = linalg.lu(A @ Q, permute_l=True)\n        Q, _ = linalg.lu(A.T @ Q, permute_l=True)\n        \n    Q, _ = linalg.qr(A @ Q, mode='economic')\n    return Q","cd89d9d6":"def randomized_svd(M, n_components, n_oversamples=10, n_iter=4):\n    \n    n_random = n_components + n_oversamples # see How should we choose r? cell below for n_oversamples rationale\n    \n    Q = randomized_range_finder(M, n_random, n_iter)\n    \n    # project M to the (k + p) dimensional space using the basis vectors\n    B = Q.T @ M\n    \n    # compute the SVD on the thin matrix: (k + p) wide\n    Uhat, s, V = linalg.svd(B, full_matrices=False)\n    del B\n    U = Q @ Uhat\n    \n    return U[:, :n_components], s[:n_components], V[:n_components, :]","cd007d99":"\n# pegs CPU usage to ~150%\nu, s, v = randomized_svd(vectors, 5)","6e5d3729":"%time u, s, v = randomized_svd(vectors, 5)","f407066a":"u.shape, s.shape, v.shape","217df268":"show_topics(v)","3063c4cc":"#Exercise: Write a loop to calculate the error of your decomposition as you vary the # of topics\n# pegs CPU usage between 100% to 200%\nstep = 20\nn = 20\nerror = np.zeros(n)\n\nfor i in range(n):\n    U, s, V = randomized_svd(vectors, i * step)\n    reconstructed = U @ np.diag(s) @ V\n    error[i] = np.linalg.norm(vectors - reconstructed)","a2be1165":"plt.plot(range(0,n*step,step),error)","d07be38d":"%time u, s, v = decomposition.randomized_svd(vectors, 5)","13e04b5c":"%time u, s, v = decomposition.randomized_svd(vectors.todense(), 5)","1afc29ed":"type(vectors)","2b915fd2":"Write a loop to calculate the error of your decomposition as you vary the # of topics. Plot the result\n\n\n**Answer**","5710803e":"Well, *that* worked anyway. :)","3d3510e0":"## Singular Value Decomposition (SVD)\n\n\"SVD is not nearly as famous as it should be.\" - Gilbert Strang\n\nWe would clearly expect that the words that appear most frequently in one topic would appear less frequently in the other - otherwise that word wouldn't make a good choice to separate out the two topics. Therefore, we expect the topics to be **orthogonal**.\n\nThe SVD algorithm factorizes a matrix into one matrix with **orthogonal columns** and one with **orthogonal rows** (along with a diagonal matrix, which contains the **relative importance** of each factor).\n\n<img src=\"https:\/\/github.com\/fastai\/numerical-linear-algebra\/blob\/master\/nbs\/images\/svd_fb.png\" style=\"width: 80%\"\/>\n (source: [Facebook Research: Fast Randomized SVD](https:\/\/research.fb.com\/fast-randomized-svd\/))\n\nSVD is an **exact decomposition**, since the matrices it creates are big enough to fully cover the original matrix. SVD is extremely widely used in linear algebra, and specifically in data science, including:\n\n*    semantic analysis\n*    collaborative filtering\/recommendations ([winning entry for Netflix Prize](https:\/\/datajobs.com\/data-science-repo\/Recommender-Systems-%5BNetflix%5D.pdf))\n*    calculate Moore-Penrose pseudoinverse\n*    data compression\n*    principal component analysis (will be covered later in course)\n","ad778a0b":"We get topics that match the kinds of clusters we would expect! This is despite the fact that this is an **unsupervised algorithm** - which is to say, we never actually told the algorithm how our documents are grouped.\n\nWe will return to SVD in **much more detail** later. For now, the important takeaway is that we have a tool that allows us to exactly factor a matrix into orthogonal columns and orthogonal rows.\n\n\n## Non-negative Matrix Factorization (NMF)\n\n**Motivation**\n\n<img src=\"https:\/\/github.com\/fastai\/numerical-linear-algebra\/blob\/master\/nbs\/images\/face_pca.png\" style=\"width: 80%\"\/>\n(source: [NMF Tutorial](https:\/\/perso.telecom-paristech.fr\/essid\/teach\/NMF_tutorial_ICME-2014.pdf))\n\nA more interpretable approach:\n\n<img src=\"https:\/\/github.com\/fastai\/numerical-linear-algebra\/blob\/master\/nbs\/images\/face_outputs.png\" style=\"width: 80%\"\/>\n(source: [NMF Tutorial](https:\/\/perso.telecom-paristech.fr\/essid\/teach\/NMF_tutorial_ICME-2014.pdf))\n\n**Idea**\n\nRather than constraining our factors to be orthogonal, another idea would to constrain them to be *non-negative*. NMF is a factorization of a non-negative data set V:\nV = WH\ninto non-negative matrices W,H. Often positive factors will be **more easily interpretable** (and this is the reason behind NMF's popularity). \n\n<img src=\"https:\/\/github.com\/fastai\/numerical-linear-algebra\/blob\/master\/nbs\/images\/face_nmf.png\" style=\"width: 80%\"\/>\n(source: [NMF Tutorial](https:\/\/perso.telecom-paristech.fr\/essid\/teach\/NMF_tutorial_ICME-2014.pdf))\n\nNonnegative matrix factorization (NMF) is a non-exact factorization that factors into one skinny positive matrix and one short positive matrix. NMF is NP-hard and non-unique. There are a number of variations on it, created by adding different constraints. \n\n**Applications of NMF**\n\n*    [Face Decompositions](https:\/\/scikit-learn.org\/stable\/auto_examples\/decomposition\/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py)\n*    [Collaborative Filtering, eg movie recommendations](http:\/\/www.quuxlabs.com\/blog\/2010\/09\/matrix-factorization-a-simple-tutorial-and-implementation-in-python\/)\n*    [Audio source separation](https:\/\/pdfs.semanticscholar.org\/cc88\/0b24791349df39c5d9b8c352911a0417df34.pdf)\n*    [Chemistry](https:\/\/ieeexplore.ieee.org\/document\/1532909)\n*    [Bioinformatics](https:\/\/bmcbioinformatics.biomedcentral.com\/articles\/10.1186\/s12859-015-0485-4) and [Gene Expression](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC2623306\/)\n*    Topic Modeling (our problem!)\n\n<img src=\"https:\/\/github.com\/fastai\/numerical-linear-algebra\/blob\/master\/nbs\/images\/nmf_doc.png\" style=\"width: 80%\"\/>\n(source: [NMF Tutorial](https:\/\/perso.telecom-paristech.fr\/essid\/teach\/NMF_tutorial_ICME-2014.pdf))\n\n**More Reading:**\n\n* [The Why and How of Nonnegative Matrix Factorization](https:\/\/arxiv.org\/pdf\/1401.5226.pdf)\n\n### NMF from sklearn\n\nFirst, we will use [scikit-learn's implementation of NMF:](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.NMF.html)","6af92119":"HUH? Should have returned:\nVariable containing:\n 0  0\n 0  0\n[torch.FloatTensor of size 2x2]\n\n","a33216e7":"Maybe someday I will try to troubleshoot this before I retire. \/s\n\nShould have returned:\n\n43.628597259521484 -0.022899555042386055 -0.26526615023612976 692 82579\n43.62860107421875 -0.021287493407726288 -0.2440912425518036 617 77552\n43.628597259521484 -0.020111067220568657 -0.22828206419944763 576 77726\n43.628604888916016 -0.01912039890885353 -0.21654289960861206 553 84411\n43.62861251831055 -0.018248897045850754 -0.20736189186573029 544 75546\n43.62862014770508 -0.01753264293074608 -0.19999365508556366 491 78949\n43.62862777709961 -0.016773322597146034 -0.194113627076149 513 83822\n43.628639221191406 -0.01622121036052704 -0.18905577063560486 485 74101\n43.62863540649414 -0.01574397087097168 -0.18498440086841583 478 85987\n43.628639221191406 -0.015293922275304794 -0.18137598037719727 487 74023","192c8992":"**How should we choose r?**\n\nSuppose our matrix has 100 columns, and we want 5 columns in U and V. To be safe, we should project our matrix onto an orthogonal basis with a few more rows and columns than 5 (let's use 15). At the end, we will just grab the first 5 columns of U and V\n\nSo even although our projection was only approximate, by making it a bit bigger than we need, we can make up for the loss of accuracy (since we're only taking a subset later).\n","b19e284d":"The method randomized_range_finder finds an orthonormal matrix whose range approximates the range of A (step 1 in our algorithm above). To do so, we use the LU and QR factorizations, both of which we will be covering in depth later.\n\nI am using the [scikit-learn.extmath.randomized_svd source](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/14031f65d144e3966113d3daec836e443c6d7a5b\/sklearn\/utils\/extmath.py) code as a guide.\n","cd417877":"**Stochastic Gradient Descent (SGD)**\n\n**Stochastic gradient descent** is an incredibly useful optimization method (it is also the heart of deep learning, where it is used for backpropagation).\n\nFor standard gradient descent, we evaluate the loss using **all** of our data which can be really slow. In stochastic gradient descent, we evaluate our loss function on just a sample of our data (sometimes called a mini-batch). We would get different loss values on different samples of the data, so this is *why it is stochastic*. It turns out that this is still an effective way to optimize, and it's much more efficient!\n\nWe can see how this works in this [excel spreadsheet](https:\/\/github.com\/fastai\/numerical-linear-algebra\/blob\/master\/nbs\/graddesc.xlsm) (originally from the [fast.ai deep learning course](https:\/\/github.com\/fastai\/courses)).\n\nResources:\n\n*    [SGD Lecture from Andrew Ng's Coursera ML course](https:\/\/www.coursera.org\/lecture\/machine-learning\/stochastic-gradient-descent-DoRHJ)\n*    [fast.ai wiki page on SGD](http:\/\/wiki.fast.ai\/index.php\/Stochastic_Gradient_Descent_(SGD))\n*    [Gradient Descent For Machine Learning](https:\/\/machinelearningmastery.com\/gradient-descent-for-machine-learning\/) (Jason Brownlee- Machine Learning Mastery)\n*    [An overview of gradient descent optimization algorithms](http:\/\/ruder.io\/optimizing-gradient-descent\/)\n\n**Applying SGD to NMF**\n\nGoal: Decompose V(m\u00d7n) into\nV \u2248 WH\nwhere W(m\u00d7d) and H(d\u00d7n), W,H>=0, and we've minimized the Frobenius norm of V\u2212WH.\n\n**Approach:** We will pick random positive W & H, and then use SGD to optimize.\n\n**To use SGD, we need to know the gradient of the loss function.**\n\n**Sources:**\n\n*    Optimality and gradients of NMF: http:\/\/users.wfu.edu\/plemmons\/papers\/chu_ple.pdf\n*    Projected gradients: https:\/\/www.csie.ntu.edu.tw\/~cjlin\/papers\/pgradnmf.pdf","7260fa4d":"Fortunately, there is a faster way:","0b3a686c":"Still frustrating; expected '0.9188119769096375'\n\n","b20c9f1b":"**So how do we find Q (in step 1)?**\n\nTo estimate the range of A, we can just take a bunch of random vectors wi, evaluate the subspace formed by Awi. We can form a matrix W with the wi as it's columns. Now, we take the QR decomposition of AW=QR, then the columns of Q form an orthonormal basis for AW, which is the range of A.\n\nSince the matrix AW of the product has far more rows than columns and therefore, approximately, orthonormal columns. This is simple probability - with lots of rows, and few columns, it's unlikely that the columns are linearly dependent.\n","2de49a9a":"My guess is that the code has changed somewhere. After all, this is almost 2 years old. \/s\n\nShould have returned:\n43.66044616699219 -0.0002547535696066916 -0.00046720390673726797 319 8633\n\nHow to apply SGD, using autograd:\n\n","294c9ce5":"Next, scikit learn has a method that will extract all the word counts for us.","40ca0dc2":"Let's look at some of the data. Can you guess which category these messages are in?","cba24c44":"GPU \/ PyTorch issue?\n\nShould have returned:\nVariable containing:\n 1  1\n 1  1\n[torch.FloatTensor of size 2x2]","1593da3b":"## Additional Resources\n\n\n*    Data source (404): Newsgroups are discussion groups on Usenet, which was popular in the 80s and 90s before the web really took off. This dataset includes 18,000 newsgroups posts with 20 topics.\n*    [Chris Manning's book chapter](https:\/\/nlp.stanford.edu\/IR-book\/pdf\/18lsi.pdf) on matrix factorization and LSI\n*    Scikit learn [truncated SVD LSI details](https:\/\/scikit-learn.org\/stable\/modules\/decomposition.html#lsa)\n\n### Other Tutorials\n\n*    Scikit-Learn: [Out-of-core classification of text documents:](https:\/\/scikit-learn.org\/stable\/auto_examples\/applications\/plot_out_of_core_classification.html) uses [Reuters-21578](https:\/\/archive.ics.uci.edu\/ml\/datasets\/reuters-21578+text+categorization+collection) dataset (Reuters articles labeled with ~100 categories), HashingVectorizer\n*    Text Analysis with Topic Models for the Humanities and Social Sciences (404): uses British and French Literature dataset (404) of Jane Austen, Charlotte Bronte, Victor Hugo, and more\n\n\n### Set up data\n\nScikit Learn comes with a number of built-in datasets, as well as loading utilities to load several standard external datasets. This is a [great resource](https:\/\/scikit-learn.org\/stable\/datasets\/), and the datasets include Boston housing prices, face images, patches of forest, diabetes, breast cancer, and more. We will be using the newsgroups dataset.\n\nNewsgroups are discussion groups on Usenet, which was popular in the 80s and 90s before the web really took off. This dataset includes 18,000 newsgroups posts with 20 topics.\n\n","98cd3b86":"HUH? Didn't expect this","1e0948ca":"https:\/\/github.com\/scipy\/scipy\/issues\/7403\nMaybe a numpy issue?","693ae12c":"**Topics**\n\nWhat can we say about the singular values s?\n","b6fb313a":"### PyTorch: autograd\n\nAbove, we used our knowledge of what the gradient of the loss function was to do SGD from scratch in PyTorch. However, PyTorch has an automatic differentiation package, [autograd](https:\/\/pytorch.org\/docs\/stable\/autograd.html) which we could use instead. This is really useful, in that we can use autograd on problems where we don't know what the derivative is.\n\nThe approach we use below is very general, and would work for almost any optimization problem.\n\nIn PyTorch, Variables have the same API as tensors, but Variables remember the operations used on to create them. This lets us take derivatives.\nPyTorch Autograd Introduction\n\nExample taken from this tutorial in the official documentation.\n\n**PyTorch Autograd Introduction**\n\nExample taken from [this tutorial](https:\/\/pytorch.org\/tutorials\/beginner\/former_torchies\/autograd_tutorial.html) in the official documentation.","8d1b3435":"I got:\n\nCPU times: user 4min 23s, sys: 2.84 s, total: 4min 26s\n\nWall time: 1min 10s\n\n*Much faster* than on my box!","00ad12ce":"\n\nThe runtime complexity for SVD is O(min(m^2n, mn^2))\n\n**Question**: How can we speed things up? (without new breakthroughs in SVD research)\n\n**Idea**: Let's use a smaller matrix (with smaller n)!\n\nInstead of calculating the SVD on our full matrix A which is m x n, let's use B = AQ, which is just m x r and r<<n\n\nWe haven't found a better general SVD method, we are just using the method we have on a smaller matrix.\n","0afde8ab":"### Truncated SVD\n\nWe saved a lot of time when we calculated NMF by only calculating the subset of columns we were interested in. Is there a way to get this benefit with SVD? Yes there is! It's called truncated SVD. We are just interested in the vectors corresponding to the **largest** singular values.\n\n<img src=\"https:\/\/github.com\/fastai\/numerical-linear-algebra\/blob\/master\/nbs\/images\/svd_fb.png\" style=\"width: 80%\"\/>\n(source: [Facebook Research: Fast Randomized SVD](https:\/\/research.fb.com\/fast-randomized-svd\/))\n\n\n**Shortcomings of classical algorithms for decomposition:**\n\n*    Matrices are \"stupendously big\"\n*    Data are often **missing or inaccurate**. Why spend extra computational resources when imprecision of input limits precision of the output?\n*    **Data transfer** now plays a major role in time of algorithms. Techniques the require fewer passes over the data may be substantially faster, even if they require more flops (flops = floating point operations).\n*    Important to take advantage of **GPUs**.\n\n(source: [Halko](https:\/\/arxiv.org\/abs\/0909.4061))\n\n**Advantages of randomized algorithms:**\n\n*    inherently stable\n*    performance guarantees do not depend on subtle spectral properties\n*    needed matrix-vector products can be done in parallel\n\n(source: [Halko](https:\/\/arxiv.org\/abs\/0909.4061))\n\n### Randomized SVD\n\nReminder: full SVD is **slow**. This is the calculation we did above using Scipy's Linalg SVD:","9fd2a836":"This is my attempt at porting [fast.ai](https:\/\/www.fast.ai\/)'s Computational Linear Algebra for Coders course to Kaggle.\nI tried running this [repo](https:\/\/github.com\/fastai\/numerical-linear-algebra) on my Ubuntu box, but with only 8GB ram it was taking forever. Hopefully running this in Kaggle with 16GB ram will make life a bit easier. Also need a GPU for this lesson or else you will experience great pain and frustration working with PyTorch; at least I did.\n\nThis is the second lesson in this course.\n\nBTW, have tried six times by Sunday to get the images to display without any luck. Any suggestions will be greatly appreciated. :)","d96b3cc4":"The target attribute is the integer index of the category.","65658df0":"Confirm that U, V are orthonormal\n\n**Answer**","b519186c":"hint: definition of *perijove* is the point in the orbit of a satellite of Jupiter nearest the planet's center ","ee57b3ad":"*That* worked.","fc82aea4":"**The QR Decomposition**\n\nWe will be learning about the QR decomposition in depth later on. For now, you just need to know that A=QR, where Q consists of orthonormal columns, and R is upper triangular. Trefethen says that the QR decomposition is the most important idea in numerical linear algebra! We will definitely be returning to it.\n","282abf28":"## Motivation\n\nConsider the most extreme case - reconstructing the matrix using an outer product of two vectors. Clearly, in most cases we won't be able to reconstruct the matrix exactly. But if we had one vector with the relative frequency of each vocabulary word out of the total word count, and one with the average number of words per document, then that outer product would be as close as we can get.\n\nNow consider increasing that matrices to two columns and two rows. The optimal decomposition would now be to cluster the documents into two groups, each of which has as different a distribution of words as possible to each other, but as similar as possible amongst the documents in the cluster. We will call those two groups \"topics\". And we would cluster the words into two groups, based on those which most frequently appear in each of the topics.\n\n### In today's class\n\nWe'll take a dataset of documents in several different categories, and find topics (consisting of groups of words) for them. Knowing the actual categories helps us evaluate if the topics we find make sense.\n\nWe will try this with two different matrix factorizations: **Singular Value Decomposition (SVD) and Non-negative Matrix Factorization (NMF)**","5d43e265":"Here are some results from [Facebook Research](https:\/\/research.fb.com\/fast-randomized-svd\/):\n\n<img src=\"https:\/\/github.com\/fastai\/numerical-linear-algebra\/blob\/master\/nbs\/images\/randomizedSVDbenchmarks.png\" style=\"width: 80%\"\/>\n\n**Johnson-Lindenstrauss Lemma**: (from [wikipedia](https:\/\/en.wikipedia.org\/wiki\/Johnson%E2%80%93Lindenstrauss_lemma)) a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved.\n\nIt is desirable to be able to reduce dimensionality of data in a way that preserves relevant structure. The Johnson\u2013Lindenstrauss lemma is a classic result of this type.\n\n### Implementing our own Randomized SVD\n","a7444169":"Further Resources:\n\n*    [a whole course on randomized algorithms](https:\/\/www.cs.ubc.ca\/~nickhar\/W12\/)\n\n","273c3d85":"Not even close, but the y-axis marks are off by around 10% so maybe?\n\n\n### Comparing Approaches\n\n**Scikit-Learn's NMF**\n\n*    Fast\n*    No parameter tuning\n*    Relies on decades of academic research, took experts a long time to implement\n\n<img src=\"https:\/\/github.com\/fastai\/numerical-linear-algebra\/blob\/master\/nbs\/images\/nimfa.png\" style=\"width: 80%\"\/>\nsource: [Python Nimfa Documentation](http:\/\/nimfa.biolab.si\/)\n\n**Using PyTorch and SGD**\n\n*    Took us an hour to implement, didn't have to be NMF experts\n*    Parameters were fiddly\n*    Not as fast (tried in numpy and was so slow we had to switch to PyTorch)","7786ff89":"### TF-IDF\n\n[Topic Frequency-Inverse Document Frequency](http:\/\/www.tfidf.com\/) (TF-IDF) is a way to normalize term counts by taking into account how often they appear in a document, how long the document is, and how commmon\/rare the term is.\n\nTF = (# occurrences of term t in document) \/ (# of words in documents)\n\nIDF = log(# of documents \/ # documents with term t in it)\n","a66eeff7":"Confirm this is a decomposition of the input.\n\n**Answer**","e856d4ab":"**Using Autograd for NMF**","f10bcbda":"### More Details\n\nHere is a process to calculate a truncated SVD, described in [Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions](https:\/\/arxiv.org\/pdf\/0909.4061.pdf) and [summarized in this blog post](https:\/\/research.fb.com\/fast-randomized-svd\/):\n\n1. Compute an approximation to the range of A. That is, we want Q with r orthonormal columns such that\nA\u2248QQ^TA\n\n\n2. Construct B=Q^TA, which is small (r\u00d7n)\n\n3. Compute the SVD of B by standard methods (fast since B is smaller than A), B=S\u03a3V^T\n\n4. Since\nA\u2248QQ^TA=Q(S\u03a3V^T)\n\nif we set U=QS, then we have a low rank approximation A\u2248U\u03a3V^T.\n","a001b12f":"## End","98069f28":"And here's our randomized SVD method:\n","f720abef":"expected '0.43389660120010376'","74c788b7":"This is painfully slow to train! Lots of parameter fiddling and still slow to train (or explodes).\n\n### PyTorch\n\n[PyTorch](https:\/\/pytorch.org\/) is a Python framework for tensors and dynamic neural networks with GPU acceleration. Many of the core contributors work on Facebook's AI team. In many ways, it is similar to Numpy, only with the increased parallelization of using a GPU.\n\nFrom the [PyTorch documentation](https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/tensor_tutorial.html):\n\n<img src=\"https:\/\/github.com\/fastai\/numerical-linear-algebra\/blob\/master\/nbs\/images\/what_is_pytorch.png\" style=\"width: 80%\"\/>\n\n**Further learning**: If you are curious to learn what *dynamic* neural networks are, you may want to watch [this talk](https:\/\/www.youtube.com\/watch?v=Z15cBAuY7Sc) by Soumith Chintala, Facebook AI researcher and core PyTorch contributor.\n\nIf you want to learn more PyTorch, you can try this [tutorial](https:\/\/pytorch.org\/tutorials\/beginner\/deep_learning_60min_blitz.html) or this [learning by examples](https:\/\/pytorch.org\/tutorials\/beginner\/pytorch_with_examples.html).\n\n**Note about GPUs**: If you are not using a GPU, you will need to remove the .cuda() from the methods below. GPU usage is not required for this course, but I thought it would be of interest to some of you. To learn how to create an AWS instance with a GPU, you can watch the fast.ai setup lesson (404).\n","3a2671f2":"### NMF in summary\n\nBenefits: Fast and easy to use!\n\nDownsides: took years of research and expertise to create\n\nNotes:\n\n*    For NMF, matrix needs to be at least as tall as it is wide, or we get an error with fit_transform\n*    Can use df_min in CountVectorizer to only look at words that were in at least k of the split texts\n\n### NMF from scratch in numpy, using SGD\n\n**Gradient Descent**\n\nThe key idea of standard gradient descent:\n\n1.    Randomly choose some weights to start\n2.    Loop:\n*        Use weights to calculate a prediction\n*        Calculate the derivative of the loss\n*        Update the weights\n3.    Repeat step 2 lots of times. Eventually we end up with some decent weights.\n\n**Key:** We want to decrease our loss and the derivative tells us the direction of **steepest descent**.\n\nNote that loss, error, and cost are all terms used to describe the same thing.\n\nLet's take a look at the [Gradient Descent Intro notebook](https:\/\/github.com\/fastai\/numerical-linear-algebra\/blob\/master\/nbs\/gradient-descent-intro.ipynb) (originally from the [fast.ai deep learning course](https:\/\/github.com\/fastai\/courses)).\n\n","4d3fd26f":"# 2. Topic Modeling with NMF and SVD\n\nTopic modeling is a great way to get started with matrix factorizations. We start with a **term-document matrix:**\n\n<img src=\"https:\/\/github.com\/fastai\/numerical-linear-algebra\/blob\/master\/nbs\/images\/document_term.png\" style=\"width: 80%\"\/>\n(source: [Introduction to Information Retrieval](http:\/\/player.slideplayer.com\/15\/4528582\/))\n\nWe can decompose this into one tall thin matrix times one wide short matrix (possibly with a diagonal matrix in between).\n\nNotice that this representation does not take into account word order or sentence structure. It's an example of a **bag of words** approach."}}