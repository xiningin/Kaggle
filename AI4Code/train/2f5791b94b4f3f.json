{"cell_type":{"a1700168":"code","72bdd0aa":"code","e56f1c69":"code","da601448":"code","61dfee2c":"code","b983fc9a":"code","6d7f42c0":"code","97ce80a5":"code","1079784a":"code","34a28788":"code","42207fd0":"code","6e2bf021":"code","fde72459":"code","21ccfb2e":"code","2c487fd1":"code","b5fbdb72":"code","ca7f2cc9":"code","bb1d8b2d":"code","b044feb1":"code","f9a2eefe":"code","541e80f7":"code","16bc53e3":"code","e2ac6049":"code","8472d613":"code","dd87b720":"code","5807a82f":"code","de47053a":"code","ecd3eb71":"code","cc13fe25":"code","b7ca317a":"code","b6885b39":"markdown","f39a642f":"markdown","4a788bc5":"markdown"},"source":{"a1700168":"import os\nimport csv\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","72bdd0aa":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","e56f1c69":"files = glob(\"..\/input\/gmb-v220\/gmb-2.2.0\/data\/*\/*\/*.tags\")\n\nlen(files)","da601448":"files[:2]","61dfee2c":"ner_tags = Counter()\niob_tags = Counter()\n\n\ndef strip_ner_subcat(tag):\n    # NER tag from {cat}-{subcat}\n    # we omly want first part\n    return tag.split(\"-\")[0]","b983fc9a":"!mkdir ner","6d7f42c0":"## take a sequence of tags and convert them into IOB format\n\ndef iob_format(ners):\n    \n    iob_tokens = []\n    for i, token in enumerate(ners):\n        if token != 'O': # other\n            if i==0:\n                token = \"B-\"+token # start of sentence tag\n            elif ners[i-1]==token:\n                token = \"I-\"+token # continues\n            else:\n                token = \"B-\"+token\n        iob_tokens.append(token)\n        iob_tags[token] += 1\n    return iob_tokens","97ce80a5":"total_sentences = 0\noutfiles = []\n\nfor i, file in enumerate(files):\n    with open(file, 'rb') as content:\n        data = content.read().decode('utf-8').strip()\n        sentences = data.split(\"\\n\\n\")\n#         print(i, file, len(sentences))\n        total_sentences += len(sentences)\n        \n        with open(\".\/ner\/\"+str(i)+\"-\"+os.path.basename(file), 'w') as outfile:\n            outfiles.append(\".\/ner\/\"+str(i)+\"-\"+os.path.basename(file))\n            writer = csv.writer(outfile)\n            for sentence in sentences:\n                toks = sentence.split(\"\\n\")\n                words, pos, ner = [], [], []\n                \n                for tok in toks:\n                    t = tok.split(\"\\t\")\n                    words.append(t[0])\n                    pos.append(t[1])\n                    ner_tags[t[3]] += 1\n                    ner.append(strip_ner_subcat(t[3]))\n                writer.writerow([\" \".join(words),\n                                \" \".join(iob_format(ner)),\n                                \" \".join(pos)])","1079784a":"print(\"Total number of sentences:\", total_sentences)","34a28788":"files = glob(\".\/ner\/*.tags\")\n\ndata_pd = pd.concat([pd.read_csv(f, header=None, names=['text', 'label', 'pos']) for f in files], ignore_index=True)\n\ndata_pd.info()","42207fd0":"data_pd.head()","6e2bf021":"text_tok = Tokenizer(filters='[\\\\]^\\t\\n', lower=False, split=' ', oov_token='<OOV>')\npos_tok = Tokenizer(filters='\\t\\n', lower=False, split=' ', oov_token='<OOV>')\nner_tok = Tokenizer(filters='\\t\\n', lower=False, split=' ', oov_token='<OOV>')\n\ntext_tok.fit_on_texts(data_pd['text'])\npos_tok.fit_on_texts(data_pd['pos'])\nner_tok.fit_on_texts(data_pd['label'])","fde72459":"text_config = text_tok.get_config()\nner_config = ner_tok.get_config()\nprint(ner_config)","21ccfb2e":"text_vocab = eval(text_config['index_word'])\nner_vocab = eval(ner_config['index_word'])\n\nprint(\"Unique words in vocab:\", len(text_vocab))\nprint(\"Unique NER tags in vocab:\", len(ner_vocab))","2c487fd1":"x_tok = text_tok.texts_to_sequences(data_pd['text'])\ny_tok = ner_tok.texts_to_sequences(data_pd['label'])","b5fbdb72":"# pad sequences to a max length\n\nmax_len = 50\n\nx_pad = pad_sequences(x_tok, padding='post', maxlen=max_len)\ny_pad = pad_sequences(y_tok, padding='post', maxlen=max_len)\n\nprint(x_pad.shape, y_pad.shape)","ca7f2cc9":"num_classes = len(ner_vocab) + 1\nY = keras.utils.to_categorical(y_pad, num_classes=num_classes)\n\nY.shape","bb1d8b2d":"vocab_size = len(text_vocab) + 1\nembedding_dims = 64\nrnn_units = 100\n\nBATCH_SIZE = 128\n\nmodel = Sequential([\n    Embedding(vocab_size, embedding_dims, mask_zero=True, input_length=50),\n    Bidirectional(LSTM(rnn_units, return_sequences=True, dropout=0.2, kernel_initializer=\"he_normal\")),\n    TimeDistributed(Dense(rnn_units, \"relu\")),\n    Dense(num_classes, 'softmax')\n])\n\nmodel.summary()","b044feb1":"model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nhistory = model.fit(x_pad, Y, batch_size=BATCH_SIZE, epochs=15, validation_split=0.2)","f9a2eefe":"from tensorflow.keras.layers import Layer\nfrom tensorflow.keras import backend as K\nimport tensorflow_addons as tfa","541e80f7":"class CRFLayer(Layer):\n    \"\"\"\n    Computes the log likelihood during training\n    Performs Viterbi decoding during predictions\n    \"\"\"\n    def __init__(self, label_size, mask_id=0, \n                 trans_params=None, name='crf', \n                 **kwargs):\n        super(CRFLayer, self).__init__(name=name, **kwargs)\n        self.label_size = label_size\n        self.mask_id = mask_id\n        self.transition_params = None\n        \n        if trans_params is None: # not reloading pretrained params\n            self.transition_params = tf.Variable(tf.random.uniform(shape=(label_size, label_size)), trainable=False)\n        else:\n            self.transition_params = trans_params\n            \n    def call(self, inputs, seq_lengths, training=None):\n        if training is None:\n            training = K.learning_phase()\n            \n        # during training, this layer just returns the logits\n        if training:\n            return inputs\n        \n        # viterbi decode logic to proper\n        # results at inference\n        _, max_seq_len, _ = inputs.shape\n        seqlens = seq_lengths\n        paths = []\n        for logit, text_len in zip(inputs, seqlens):\n            viterbi_path, _ = tfa.text.viterbi_decode(logit[:text_len], self.transition_params)\n            paths.append(self.pad_viterbi(viterbi_path, max_seq_len))\n        \n        return tf.convert_to_tensor(paths)\n    \n    def pad_viterbi(self, viterbi, max_seq_len):\n        if len(viterbi) < max_seq_len:\n            viterbi = viterbi + [self.mask_id] * (max_seq_len - len(viterbi))\n        return viterbi\n    \n    def loss(self, y_true, y_pred):\n        y_pred = tf.convert_to_tensor(y_pred)\n        y_true = tf.cast(self.get_proper_labels(y_true), y_pred.dtype)\n        \n        seq_lengths = self.get_seq_lengths(y_true)\n        log_likelihoods, self.transition_params = tfa.text.crf_log_likelihood(y_pred, y_true, seq_lengths)\n        \n        # save transition params\n        self.transition_params = tf.Variable(self.transition_params, trainable=False)\n        # calc loss\n        loss = - tf.reduce_mean(log_likelihoods)\n        return loss\n    \n    def get_proper_labels(self, y_true):\n        shape = y_true.shape\n        if len(shape)>2:\n            return tf.argmax(y_true, -1, output_type=tf.int32)\n        return y_true\n    \n    def get_seq_lengths(self, matrix):\n        # matrix is of shape (batch_size, max_seq_len)\n        mask = tf.not_equal(matrix, self.mask_id)\n        seq_lengths = tf.math.reduce_sum(tf.cast(mask, dtype=tf.int32), axis=-1)\n        \n        return seq_lengths","16bc53e3":"class NerModel(tf.keras.Model):\n    def __init__(self, hidden_num, vocab_size, \n                 label_size, embedding_size, \n                 name='BilstmCrfModel', **kwargs):\n        super(NerModel, self).__init__(name=name, **kwargs)\n        self.vocab_size = vocab_size\n        self.label_size = label_size\n        self.embedding = Embedding(vocab_size, embedding_size, mask_zero=True, name=\"embedding\")\n        self.biLSTM = Bidirectional(LSTM(hidden_num, return_sequences=True), name='bilstm')\n        self.dense = TimeDistributed(Dense(label_size), name='dense')\n        self.crf = CRFLayer(self.label_size, name='crf')\n        \n    def call(self, text, labels=None, training=None):\n        seq_lengths = tf.math.reduce_sum(\n        tf.cast(tf.math.not_equal(text,0), dtype=tf.int32), axis=-1)\n        \n        if training is None:\n            training = K.learning_phase()\n            \n        inputs = self.embedding(text)\n        bilstm = self.biLSTM(inputs)\n        logits = self.dense(bilstm)\n        outputs = self.crf(logits, seq_lengths, training)\n        \n        return outputs","e2ac6049":"vocab_size = len(text_vocab) + 1\nembedding_dims = 64\nrnn_units = 100\nbatch_size = 90\n\nnum_classes = len(ner_vocab) + 1\n\nblc_model = NerModel(rnn_units, vocab_size, num_classes, embedding_dims, dynamic=True)\noptimizer = keras.optimizers.Adam(learning_rate=1e-3)","8472d613":"total_sentences = 62010\ntest_size = round(total_sentences \/ batch_size * 0.2)\n\nX_train = x_pad[batch_size*test_size:]\nY_train = Y[batch_size*test_size:]\n\nX_test = x_pad[:batch_size*test_size]\nY_test = Y[:batch_size*test_size]\n\nY_train_int = tf.cast(Y_train, dtype=tf.int32)\nY_test_int = tf.cast(Y_test, dtype=tf.int32)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train_int))\ntrain_dataset = train_dataset.batch(batch_size, drop_remainder=True)","dd87b720":"## training custom model\n\nloss_metric = keras.metrics.Mean()\n\nepochs = 5\n\n# iterate over epochs\nfor epoch in range(epochs):\n    print(f'Start of epoch {epoch+1},')\n    \n    # iterate over batches of the dataset\n    for step, (text_batch, labels_batch) in enumerate(train_dataset):\n        labels_max = tf.argmax(labels_batch, -1, output_type=tf.int32)\n        \n        with tf.GradientTape() as tape:\n            logits = blc_model(text_batch, training=True)\n            loss = blc_model.crf.loss(labels_max, logits)\n            grads = tape.gradient(loss, blc_model.trainable_weights)\n            optimizer.apply_gradients(zip(grads, blc_model.trainable_weights))\n            \n            loss_metric(loss)\n            \n            if step % 50 ==0:\n                print(f\"step {step+1}: mean loss = {loss_metric.result()}\")","5807a82f":"blc_model.summary()","de47053a":"test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test_int))\ntest_dataset = test_dataset.batch(batch_size, drop_remainder=True)","ecd3eb71":"out = blc_model.predict(test_dataset.take(1))\n\nprint(\"Ground Truth:\", ner_tok.sequences_to_texts([tf.argmax(Y_test[3],-1).numpy()]))\nprint(\"\\nPrediction:\", ner_tok.sequences_to_texts([out[3]]))","cc13fe25":"# custom accuracy\n\ndef np_precision(pred, true):\n    # expect numpy arrays\n    assert pred.shape==true.shape\n    assert len(pred.shape)==2\n    mask_pred = np.ma.masked_equal(pred, 0)\n    mask_true = np.ma.masked_equal(true, 0)\n    acc = np.equal(mask_pred, mask_true)\n    \n    return np.mean(acc.compressed().astype('int'))","b7ca317a":"np_precision(out, tf.argmax(Y_test[:batch_size], -1).numpy())","b6885b39":"## Loading Data","f39a642f":"## Normalizing and Vectorizing Data","4a788bc5":"## Implementing custom CRF layer, loss, and model"}}