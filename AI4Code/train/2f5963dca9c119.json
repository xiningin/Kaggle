{"cell_type":{"5be77643":"code","17ae9f02":"code","9c9c65e6":"code","313dc40f":"code","ab181fa2":"code","19fd7e5b":"code","8be930de":"code","a5ac7414":"code","553824f8":"code","16096f6a":"code","7a11a0f6":"code","0e4402d5":"code","c48a0dc8":"code","23a4b85f":"code","493cbb75":"code","2fda86d5":"code","96277c8e":"markdown","d6c45504":"markdown","b0a62a99":"markdown","e47793d2":"markdown","c7437a0c":"markdown","d32ce624":"markdown","dc9b3a67":"markdown","9fd60ebb":"markdown","32576e38":"markdown","a53be3b1":"markdown","4cc54594":"markdown","1d0e1043":"markdown","205d7e8b":"markdown"},"source":{"5be77643":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ntest=pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ntest_shape= test.shape\n\ntrain=pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\ntrain_shape= train.shape\n\n","17ae9f02":"test.head()","9c9c65e6":"train.head()","313dc40f":"Sex_pivot= train.pivot_table(index=\"Sex\", values=\"Survived\")\nSex_pivot.plot.bar()\nplt.show()","ab181fa2":"Pclass_pivot= train.pivot_table(index=\"Pclass\", values=\"Survived\")\nPclass_pivot.plot.bar()\nplt.show()","19fd7e5b":"print(train[\"Age\"].describe())","8be930de":"survived= train[train[\"Survived\"]==1]\ndied=train[train[\"Survived\"]==0]\nsurvived[\"Age\"].plot.hist(alpha=0.5,color=\"green\", bins=50)\n\ndied[\"Age\"].plot.hist(alpha=0.5,color=\"red\", bins=50)\nplt.legend([\"Survived\",\"Died\"])\nplt.show()","a5ac7414":"def process_age(df,cut_points,label_names):\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n    return df\ncut_points=[-1,0,5,12,18,35,60,100]\nlabel_names= [\"Missing\",\"Infant\",\"Child\",\"Teenager\",\"Young Adult\",\"Adult\",\"Senior\"]\ntrain= process_age(train,cut_points,label_names)\ntest= process_age(test,cut_points,label_names)\n\npivot=train.pivot_table(index=\"Age_categories\", values=\"Survived\")\npivot.plot.bar()\nplt.show()","553824f8":"train[\"Pclass\"].value_counts()","16096f6a":"def create_dummies(df,column_name):\n    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n    df = pd.concat([df,dummies],axis=1)\n    return df\n\ntrain = create_dummies(train,\"Pclass\")\ntest = create_dummies(test,\"Pclass\")\n\ntrain = create_dummies(train,\"Sex\")\ntest = create_dummies(test,\"Sex\")\n\ntrain = create_dummies(train,\"Age_categories\")\ntest = create_dummies(test,\"Age_categories\")","7a11a0f6":"columns = ['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',\n       'Age_categories_Missing','Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Teenager',\n       'Age_categories_Young Adult', 'Age_categories_Adult',\n       'Age_categories_Senior']\n\nfrom sklearn.linear_model import LogisticRegression\nlr= LogisticRegression()\nlr.fit(train[columns], train['Survived'])","0e4402d5":"holdout = test #  from now on we will refer to this dataframe as the holdout data\nfrom sklearn.model_selection import train_test_split\n\n\ncolumns = ['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',\n       'Age_categories_Missing','Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Teenager',\n       'Age_categories_Young Adult', 'Age_categories_Adult',\n       'Age_categories_Senior']\nall_X=train[columns]\nall_y=train['Survived']\n\ntrain_X,test_X,train_y,test_y= train_test_split(all_X,all_y, test_size=0.2, random_state=0)\n\n","c48a0dc8":"from sklearn.metrics import accuracy_score\nlr= LogisticRegression()\nlr.fit(train_X,train_y)\npredictions=lr.predict(test_X)\naccuracy = accuracy_score(test_y, predictions)\nprint(accuracy)","23a4b85f":"from sklearn.model_selection import cross_val_score\nimport numpy as np\nlr= LogisticRegression()\nscores=cross_val_score(lr, all_X, all_y, cv=10)\naccuracy=np.mean(scores)\nprint(scores)\nprint(accuracy)","493cbb75":"columns = ['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',\n       'Age_categories_Missing','Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Teenager',\n       'Age_categories_Young Adult', 'Age_categories_Adult',\n       'Age_categories_Senior']\nlr= LogisticRegression()\nlr.fit(all_X,all_y)\nholdout_predictions=lr.predict(holdout[columns])","2fda86d5":"holdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": holdout_predictions}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv(\"submission.csv\", index=False)\n","96277c8e":"We have trained the  machine learning model. The next step is to find out how accurate our model is, and to do that, I have to make some predictions.\n\n\nThe convention in machine learning is to call these two parts train and test. This can become confusing, since I already have our test dataframe that I will eventually use to make predictions to submit. To avoid confusion, from here on, I am going to call this Kaggle 'test' data holdout data, which is the technical name given to this type of data used for final predictions.\n\n\nThe scikit-learn library has a handy `model_selection.train_test_split()` function that we can use to split our data. `train_test_split()` accepts two parameters, X and y, which contain all the data we want to train and test on, and returns four objects: `train_X`, `train_y`, `test_X`, `test_y`.\n\n\n`test_size`, which lets us control what proportions our data are split into, and random_state. The `train_test_split()` function randomizes observations before dividing them, and setting a random seed means that our results will be reproducible, which is important if you are collaborating, or need to produce consistent results each time","d6c45504":"Now that our data has been prepared, we are ready to train our first model. The first model we will use is called Logistic Regression, which is often the first model you will train when performing classification.\n\nWe will be using the scikit-learn library as it has many tools that make performing machine learning easier.\n\n\nEach model in scikit-learn is implemented as a separate class and the first step is to identify the class we want to create an instance of. In this case, we  use the LogisticRegression class.","b0a62a99":"The model has an accuracy score of 81.0% when tested against our 20% test set.","e47793d2":"From the results of the k-fold validation, you can see that the accuracy number varies with each fold - ranging between 76.4% (0.764) and 87.6% (0.876). This demonstrates why cross validation is important.","c7437a0c":"From observation, the  average accuracy score was 80.2%, which is not far from the 81.0% we got from our simple train\/test split\n\n\nI am now ready to use the model we have built to train our final model and then make predictions on our unseen holdout data, or what Kaggle calls the 'test' data set.","d32ce624":"I have successfully made the prediction. The `submission.csv` can be found in this [link](https:\/\/github.com\/makozi\/Titanic-Machine-Learning-from-Disaster).","dc9b3a67":"\n\n\n# Titanic: Machine Learning from Disaster\n\n\n### Predict survival on the Titanic and get familiar with ML basics","9fd60ebb":"### Conclusion\n\n\nThere are many things that can be done to  improve the accuracy of our model. Here are some that we will cover in the next two missions of this course:\n\n#### Improving the features:\n- Feature Engineering: Create new features from the existing data.\n- Feature Selection: Select the most relevant features to reduce noise and overfitting.\n\n#### Improving the model:\n- Model Selection: Try a variety of models to improve performance.\n- Hyperparameter Optimization: Optimize the settings within each particular machine learning model.","32576e38":"Now we have fit our model, we can use the `LogisticRegression.predict()` method to make predictions.\n\n\nThe `predict()` method takes a single parameter X, a two dimensional array of features for the observations I wish to predict. X must have the exact same features as the array we used to fit our model. The method returns single dimensional array of predictions.\n\n\nMeasuring the Accuracy:","a53be3b1":"While the class of each passenger certainly has some sort of ordered relationship, the relationship between each class is not the same as the relationship between the numbers 1, 2, and 3. For instance, class 2 isn't \"worth\" double what class 1 is, and class 3 isn't \"worth\" triple what class 1 is.\n\nIn order to remove this relationship, we can create dummy columns for each unique value in Pclass.\n\n\nThe code below creates a function to create the dummy columns for the Pclass column and add it back to the original dataframe. It then applies that function the train and test dataframes.","4cc54594":"Missing  -1 to 0\nInfant 0 to 5\nChild 5 to 12\nTeenage 12 to 18\nYoung Adult 18 to 35\nAdult 35 to 60\nSenior 60 to 100","1d0e1043":"To  better understand the real performance of our model, we can use a technique called cross validation to train and test our model on different splits of our data, and then average the accuracy scores.\n\nThe most common form of cross validation, and the one we will be using, is called k-fold cross validation. 'Fold' refers to each different iteration that we train our model on, and 'k' just refers to the number of folds. \n\nI will use scikit-learn's model_selection.cross_val_score() function to automate the process. The basic syntax for cross_val_score() is:\n\n`cross_val_score(estimator, X, y, cv=None)`\n\n- estimator is a scikit-learn estimator object, like the LogisticRegression() objects we have been creating.\n- X is all features from our data set.\n- y is the target variables.\n- cv specifies the number of folds.\n\n\nThe function returns a numpy ndarray of the accuracy scores of each fold.","205d7e8b":"Using Titanic Data Set from kaggle (https:\/\/www.kaggle.com\/c\/titanic\/data)"}}