{"cell_type":{"3a31b8ab":"code","5996551f":"code","d2306398":"code","5858b318":"code","c7708768":"code","17bc9b89":"code","1eedec30":"code","a4ef25f4":"code","0bc4b4dc":"code","1f4e49f5":"code","c056f667":"code","bc5c5495":"code","8cd5fea2":"code","cf292cab":"code","739c4a5f":"code","219541d8":"code","977677fa":"code","b27212d5":"code","ba595f69":"code","c82929d6":"code","536ad920":"code","637e626c":"code","9b0e68a0":"code","9bc7b594":"code","de020a1a":"code","c8cf65dd":"code","56e70d5d":"code","49cc82cc":"code","d947ea05":"code","102f8fe5":"code","fcbb741b":"code","f0334b44":"code","c1fa006b":"code","0e568db0":"code","5874bc1f":"code","a51f94b4":"code","90b6d878":"code","785c1ece":"code","e14bdfb5":"code","98e6a588":"code","46cd4507":"markdown","8a3e9865":"markdown","2d408288":"markdown","4504c91f":"markdown","1c7f30b2":"markdown","71e154eb":"markdown","a613772d":"markdown","0bf38295":"markdown","ab2afb63":"markdown","e227c56b":"markdown","934adc59":"markdown","433d46dd":"markdown","e5aefeca":"markdown","29d440a2":"markdown","5cf9e287":"markdown","8f118d1a":"markdown","e8a6ef27":"markdown","be9922c8":"markdown","0f43111a":"markdown","42034296":"markdown","4ab64cec":"markdown","860f3173":"markdown","38ece13a":"markdown","c2c89c3b":"markdown","6bba04b0":"markdown","857d1a74":"markdown","d58ecfa5":"markdown","ed8efd68":"markdown","6e7589f0":"markdown","ceea06c5":"markdown","eb2ccd22":"markdown","d8fb663d":"markdown","27d19ac6":"markdown","76ccd77e":"markdown","32eda813":"markdown","fbaf6b94":"markdown","ac89db58":"markdown","d3ddafd1":"markdown"},"source":{"3a31b8ab":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\"))","5996551f":"os.listdir('.\/')","d2306398":"os.listdir('..\/')","5858b318":"os.listdir('..\/input\/')","c7708768":"# Suppressing all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib\nmatplotlib.rc('figure', figsize = (20, 8))\nmatplotlib.rc('font', size = 14)\nmatplotlib.rc('axes.spines', top = False, right = False)\nmatplotlib.rc('axes', grid = False)\nmatplotlib.rc('axes', facecolor = 'white')","17bc9b89":"# Execute this in your kernel to view the first n (here-4) lines of the json file.\n! head -n 4 ..\/input\/modcloth_final_data.json","1eedec30":"mc_df = pd.read_json('..\/input\/modcloth_final_data.json', lines=True)\nmc_df.head()","a4ef25f4":"len(mc_df)","0bc4b4dc":"mc_df.columns","1f4e49f5":"mc_df.columns = ['bra_size', 'bust', 'category', 'cup_size', 'fit', 'height', 'hips',\n       'item_id', 'length', 'quality', 'review_summary', 'review_text',\n       'shoe_size', 'shoe_width', 'size', 'user_id', 'user_name', 'waist']","c056f667":"mc_df.info()","bc5c5495":"missing_data = pd.DataFrame({'total_missing': mc_df.isnull().sum(), 'perc_missing': (mc_df.isnull().sum()\/82790)*100})\nmissing_data","8cd5fea2":"mc_df.describe()","cf292cab":"num_cols = ['bra_size','hips','quality','shoe_size','size','waist']\nplt.figure(figsize=(18,9))\nmc_df[num_cols].boxplot()\nplt.title(\"Numerical variables in Modcloth dataset\", fontsize=20)\nplt.show()","739c4a5f":"mc_df[mc_df.shoe_size == 38]","219541d8":"mc_df.at[37313,'shoe_size'] = None","977677fa":"mc_df.sort_values(by=['bra_size'], ascending=False).head(10)","b27212d5":"plt.figure(figsize=(18,8))\nplt.xlabel(\"bra_size\", fontsize=18)\nplt.ylabel(\"size\", fontsize=18)\nplt.suptitle(\"Joint distribution of bra_size vs size\", fontsize= 20)\nplt.plot(mc_df.bra_size, mc_df['size'], 'bo', alpha=0.2)\nplt.show()","ba595f69":"def plot_dist(col, ax):\n    mc_df[col][mc_df[col].notnull()].value_counts().plot('bar', facecolor='y', ax=ax)\n    ax.set_xlabel('{}'.format(col), fontsize=20)\n    ax.set_title(\"{} on Modcloth\".format(col), fontsize= 18)\n    return ax\n\nf, ax = plt.subplots(3,3, figsize = (22,15))\nf.tight_layout(h_pad=9, w_pad=2, rect=[0, 0.03, 1, 0.93])\ncols = ['bra_size','bust', 'category', 'cup_size', 'fit', 'height', 'hips', 'length', 'quality']\nk = 0\nfor i in range(3):\n    for j in range(3):\n        plot_dist(cols[k], ax[i][j])\n        k += 1\n__ = plt.suptitle(\"Initial Distributions of features\", fontsize= 25)","c82929d6":"mc_df.bra_size = mc_df.bra_size.fillna('Unknown')\nmc_df.bra_size = mc_df.bra_size.astype('category').cat.as_ordered()\nmc_df.at[37313,'bust'] = '38'\nmc_df.bust = mc_df.bust.fillna(0).astype(int)\nmc_df.category = mc_df.category.astype('category')","536ad920":"mc_df[mc_df.cup_size.isnull()].sample(20)","637e626c":"mc_df.cup_size.fillna('Unknown', inplace=True)\nmc_df.cup_size = mc_df.cup_size.astype('category').cat.as_ordered()\n\nmc_df.fit = mc_df.fit.astype('category')","9b0e68a0":"def get_cms(x):\n    if type(x) == type(1.0):\n        return\n    #print(x)\n    try: \n        return (int(x[0])*30.48) + (int(x[4:-2])*2.54)\n    except:\n        return (int(x[0])*30.48)\nmc_df.height = mc_df.height.apply(get_cms)","9bc7b594":"mc_df[mc_df.height.isnull()].head(20)\n# Do look at the output to be able to better understand the inferences!","de020a1a":"print(mc_df[((mc_df.bra_size != 'Unknown') | (mc_df.cup_size != 'Unknown')) & (mc_df.height.isnull()) & (mc_df.hips.isnull()) &\n     (mc_df.shoe_size.isnull()) & (mc_df.shoe_width.isnull()) & (mc_df.waist.isnull())].head(3))\nprint(mc_df[(mc_df.bra_size == 'Unknown') & (mc_df.cup_size == 'Unknown') & (mc_df.height.isnull()) & (mc_df.hips.isnull()) &\n     ((mc_df.shoe_size.notnull()) | (mc_df.shoe_width.notnull())) & (mc_df.waist.isnull())].head(3))\nprint(mc_df[(mc_df.bra_size == 'Unknown') & (mc_df.cup_size == 'Unknown') & (mc_df.height.isnull()) & ((mc_df.hips.notnull()) | (mc_df.waist.notnull())) &\n     (mc_df.shoe_size.isnull()) & (mc_df.shoe_width.isnull())].head(3))","c8cf65dd":"lingerie_cond = (((mc_df.bra_size != 'Unknown') | (mc_df.cup_size != 'Unknown')) & (mc_df.height.isnull()) & (mc_df.hips.isnull()) &\n     (mc_df.shoe_size.isnull()) & (mc_df.shoe_width.isnull()) & (mc_df.waist.isnull()))\nshoe_cond = ((mc_df.bra_size == 'Unknown') & (mc_df.cup_size == 'Unknown') & (mc_df.height.isnull()) & (mc_df.hips.isnull()) &\n     ((mc_df.shoe_size.notnull()) | (mc_df.shoe_width.notnull())) & (mc_df.waist.isnull()))\ndress_cond = ((mc_df.bra_size == 'Unknown') & (mc_df.cup_size == 'Unknown') & (mc_df.height.isnull()) & ((mc_df.hips.notnull()) | (mc_df.waist.notnull())) &\n     (mc_df.shoe_size.isnull()) & (mc_df.shoe_width.isnull()))\n#print(len(mc_df[lingerie_cond]))   # To check if these items add up in the final column we are adding.\n#print(len(mc_df[shoe_cond]))\n#print(len(mc_df[dress_cond]))\nmc_df['first_time_user'] = (lingerie_cond | shoe_cond | dress_cond)\nprint(\"Column added!\")\nprint(\"Total transactions by first time users who bought bra, shoes, or a dress: \" + str(sum(mc_df.first_time_user)))\nprint(\"Total first time users: \" + str(len(mc_df[(lingerie_cond | shoe_cond | dress_cond)].user_id.unique())))","56e70d5d":"# Handling hips column\nmc_df.hips = mc_df.hips.fillna(-1.0)\nbins = [-5,0,31,37,40,44,75]\nlabels = ['Unknown','XS','S','M', 'L','XL']\nmc_df.hips = pd.cut(mc_df.hips, bins, labels=labels)\n\n# Handling length column\nmissing_rows = mc_df[mc_df.length.isnull()].index\nmc_df.drop(missing_rows, axis = 0, inplace=True)\n\n# Handling quality\nmissing_rows = mc_df[mc_df.quality.isnull()].index\nmc_df.drop(missing_rows, axis = 0, inplace=True)\nmc_df.quality = mc_df.quality.astype('category').cat.as_ordered()","49cc82cc":"from pandas.api.types import CategoricalDtype\nshoe_widths_type = CategoricalDtype(categories=['Unknown','narrow','average','wide'], ordered=True)\n\nmc_df.review_summary = mc_df.review_summary.fillna('Unknown')\nmc_df.review_text = mc_df.review_text.fillna('Unkown')\nmc_df.shoe_size = mc_df.shoe_size.fillna('Unknown')\nmc_df.shoe_size = mc_df.shoe_size.astype('category').cat.as_ordered()\nmc_df.shoe_width = mc_df.shoe_width.fillna('Unknown')\nmc_df.shoe_width = mc_df.shoe_width.astype(shoe_widths_type)\nmc_df.drop(['waist', 'bust', 'user_name'], axis=1, inplace=True)\nmissing_rows = mc_df[mc_df.height.isnull()].index\nmc_df.drop(missing_rows, axis = 0, inplace=True)","d947ea05":"mc_df.info()","102f8fe5":"def plot_dist(col, ax):\n    if col != 'height':\n        mc_df[col].value_counts().plot('bar', facecolor='y', ax=ax)\n    else:\n        mc_df[col].plot('density', ax=ax, bw_method = 0.15, color='y')\n        ax.set_xlim(130,200)\n        ax.set_ylim(0, 0.07)\n    ax.set_xlabel('{}'.format(col), fontsize=18)\n    ax.set_title(\"{} on Modcloth\".format(col), fontsize= 18)\n    return ax\n\nf, ax = plt.subplots(2,4, figsize = (22,15))\nf.tight_layout(h_pad=9, w_pad=2, rect=[0, 0.03, 1, 0.93])\ncols = ['bra_size','category', 'cup_size', 'fit', 'height', 'hips', 'length', 'quality']\nk = 0\nfor i in range(2):\n    for j in range(4):\n        plot_dist(cols[k], ax[i][j])\n        k += 1\n__ = plt.suptitle(\"Final Distributions of different features\", fontsize= 23)","fcbb741b":"def plot_barh(df,col, cmap = None, stacked=False, norm = None):\n    df.plot(kind='barh', colormap=cmap, stacked=stacked)\n    fig = plt.gcf()\n    fig.set_size_inches(24,12)\n    plt.title(\"Category vs {}-feedback -  Modcloth {}\".format(col, '(Normalized)' if norm else ''), fontsize= 20)\n    plt.ylabel('Category', fontsize = 18)\n    plot = plt.xlabel('Frequency', fontsize=18)\n    \ndef norm_counts(t):\n    norms = np.linalg.norm(t.fillna(0), axis=1)\n    t_norm = t[0:0]\n    for row, euc in zip(t.iterrows(), norms):\n        t_norm.loc[row[0]] = list(map(lambda x: x\/euc, list(row[1])))\n    return t_norm","f0334b44":"mc_df.category.value_counts()","c1fa006b":"g_by_category = mc_df.groupby('category')\ncat_fit = g_by_category['fit'].value_counts()\ncat_fit = cat_fit.unstack()\ncat_fit_norm = norm_counts(cat_fit)\ncat_fit_norm.drop(['fit'], axis=1, inplace=True)\nplot_barh(cat_fit, 'fit')","0e568db0":"plot_barh(cat_fit_norm, 'fit', norm=1, cmap='Set3')","5874bc1f":"cat_len = g_by_category['length'].value_counts()\ncat_len = cat_len.unstack()\nplot_barh(cat_len, 'length', 'Set3')","a51f94b4":"cat_len_norm = norm_counts(cat_len)\ncat_len_norm.drop(['just right'], axis = 1, inplace=True)\nplot_barh(cat_len_norm, 'length', cmap='Set3', norm=1)","90b6d878":"cat_quality = g_by_category['quality'].value_counts()\ncat_quality = cat_quality.unstack()\nplot_barh(cat_quality, 'quality', 'Set3', stacked=1)","785c1ece":"cat_quality_norm = norm_counts(cat_quality)\ncat_quality_norm.drop([5.0], axis = 1, inplace=True)\nplot_barh(cat_quality_norm, 'quality', 'Set3', stacked=1, norm=1)","e14bdfb5":"# Users who bought so many items\nitems_bought = []\ntotal_users = []\nfor i in range(min(mc_df.user_id.value_counts()), max(mc_df.user_id.value_counts())+1):\n    all_users = sum(mc_df.user_id.value_counts() == i)\n    if all_users != 0:\n        total_users.append(all_users)\n        items_bought.append(i)\nplt.xlabel(\"Number of items bought\", fontsize = 18)\nplt.ylabel(\"Number of users\", fontsize = 18)\nplt.title(\"Distribution of items bought by users on Modcloth\")\n__ = sns.barplot(x=items_bought, y=total_users, color='y')\nfig = plt.gcf()\nfig.set_size_inches(20,10)","98e6a588":"fig = plt.gcf()\nfig.set_size_inches(20,10)\n__ = sns.violinplot(x='shoe_size', y='height',data=mc_df, size = 20)","46cd4507":"We can extend our observations on missing data and the datatypes here:\n* Out of 18 columns, only 6 columns have complete data.\n* Quite a lot of data seems to be missing in bust, shoe width, shoe size and waist.\n* We might want to especially look at the items which **have** shoe size and shoe width available- these could possibly be shoes!\n* Alot of the columns have strings (object datatype), which needs to be parsed into the category datatype (aids in efficient memory consumption as well). \n* *Waist* column surprisingly has a lot of NULL values- considering most of the data from Modcloth comes from the 3 categories of 'dresses, tops and bottoms'.\n\n### Looking at the percentage of missing values per column","8a3e9865":">We can't see any significant deviation from usual behavior for bra-size, infact for all other numerical variables as well- we can expect the 'apparent' outliers, from the boxplot, to behave similarly. Now, we 'll head to preprocessing the dataset for suitable visualizations.\n\n<a id=\"8\"><\/a>\n# Data Cleaning & Pre-processing\nLet's handle the variables and change the dtype to the appropriate type for each column. We define a function first for creating the distribution plot of different variables. Here, is the initial distribution of features.\n\n**Note: The final distribution plots are [below](#dist_plots).**\n\n<a id=\"9\"><\/a>\n### Initial Distribution of features","2d408288":"- **Category vs Quality**","4504c91f":"<a id=\"2\"><\/a>\n#  [Modcloth](http:\/\/modcloth.com) Dataset\n<a id=\"3\"><\/a>\n## Importing data using Pandas\nTaking a look at the first few lines of the modcloth data's json file using the inbuilt OS bash command-head. ","1c7f30b2":"* **height**- We need to parse the height column as currently it is a string object, of the form - Xft. Yin. It will make sense to convert height to cms. We also take a look at the rows where the height data is missing.","71e154eb":"Using the pd.read_json() function the json file is brought into a pandas DataFrame, with the *lines* parameter as *True*- because every new object is separated by a new line. ","a613772d":"<a id=\"7\"><\/a>\n## Joint Distribution of bra_size vs size\nWe can visualize the distribution of bra_size vs size (bivariate) to have an understanding about the values.","0bf38295":"* Best length-fitting ('just right') belongs to tops, new, dresses and bottoms! (Also due to predominance of these categories in our total transactions- **they make up almost 92% of our transactions!**)\n* All transactions share a similar order of reasons for return (in the order of importance), which is kind of intuitive as well:\n    * slightly long\n    * slightly short\n    * very long\n    * very short","ab2afb63":">We can't see anything glaring from the rows where this data is missing, however, as per the curator of the dataset- \"***Note that these datasets are highly sparse, with most products and customers having only a single transaction.***\" It does point to that maybe these customers have not bought lingerie from modcloth yet and so modcloth does not have that data. So, it makes sense to fill these null values as 'Unknown'. From the prevalence of the values like dd\/e, ddd\/f, and dddd\/g, we can assume these to be legit cup_sizes, also confirmed by [**this**](https:\/\/www.herroom.com\/full-figure-bra-cup-sizing,905,30.html) article, where some brands change the cup size dd to e, ddd to f etc. We can directly convert this to *category* dtype.","e227c56b":"> This filtering gives us interesting observations here:\n> 1. Some customers have given bra_size, cup_size data, whereas all other measurements are empty- possible first-time purchase at Modcloth for lingerie!\n> 2. Some customers have given shoe_size and all other measurements are empty- possible first-time purchase at Modcloth for shoes!\n>     \n> It leads us to saying that there are some first-time buyers in the dataset, also talked about by the authors of the data in [1]- about the sparsity of the data due to 1 transactions! Also, as we have no data about the height of these customers, it only makes sense to leave the missing values in the column as it is and **possibly remove these rows for future statistical modeling.** We have removed the corresponding rows.\n>\n>\n><a id=\"11\"><\/a>\n> # Feature Engineering \n> ## Creating a new feature of first_time_user\n> \n> Building on our observations above, it makes sense to identify the transactions which belong to first time users. We use the following logic to identify such trxns:\n> * If bra_size\/cup_size have a value and height, hips, shoe_size, shoe_width and waist do not- it is a first time buyer of lingerie.\n> * If shoe_size\/shoe_width have a value and bra_size, cup_size, height, hips, and waist do not- it is a first time buyer of shoes.\n> * If hips\/waist have a value and bra_size, cup_size, height, shoe_size, and shoe_width do not- it is a first time buyer of a dress\/tops.\n> \n> Below we will verify the above logic, with samples, before we create the new feature.\n> \n> **1. Looking at the few rows where either bra_size or cup_size exists, but no other measurements are available.**\n>\n>**2. Looking at the few rows where either shoe_size or shoe_width exists, but no other measurements are available.**\n>\n>**3. Looking at the few rows where either hips or waist exists, but no other measurements are available.**","934adc59":"* Here also we can assert our previous observation that all the categories share similar share of ratings.\n* To nitpick- *new , sale & tops* seem to have a higher share than normal of bad ratings (1.0 & 2.0) in terms of quality.","433d46dd":"<a id=\"15\"><\/a>\n### 2. Total Number of Users vs Total Number of items bought\nVisualizing the total number of users who bought *x* number of items, where we affirm the author's [[1]](#references) statement that the data is very sparse with a major chunk (38.45%) of the users who bought only 1 item from the website during the time this data was collected. ","e5aefeca":"*Aditya Agrawal*\n\nI intend to make this as a simple-to-follow, yet thorough Exploratory Data Analysis task using Python, seaborn and matplotlib. It is my first kernel and feedback would be greatly appreciated. Also, feel free to upvote if you like the work!\n\n**Update V29:**\n1. Ordered categories added.\n2. Violin plot- shoe-size vs height.\n\n### Table of Contents: <a id=\"1\"><\/a>\n1. [About the dataset](#1)\n2. [Importing Data- Modcloth](#3)\n3. [EDA & Preprocessing](#4)\n    1. [Boxplot of Numerical Variables](#5)\n    2. [Handling Outliers](#6)\n    3. [Joint Distribution of bra_size vs size](#7)\n4. [Data Cleaning & Preprocessing](#8)\n    1. [Initial Distribution of Features](#9)\n    2. [Step-by-step feature processing](#10)\n        1. [Feature Engineering - new feature added](#11)\n5. [EDA via Visualizations](#12)\n    1. [Distribution of features](#13)\n    2. [Categories vs. Fit\/Length\/Quality](#14)\n    3. [Users vs Items bought](#15)\n    4. [Height vs Shoe-size](#16)\n6. [References](#references)\n7. [Assumptions](#assumptions)\n\n<a id=\"1\"><\/a>\n### About the dataset\n\n>This dataset contains self-reported clothing-fit feedback from customers as well as other side information like reviews, ratings, product categories, catalog sizes, customers\u2019 measurements (etc.) from 2 websites:\n>1. [Modcloth](http:\/\/modcloth.com)\n>2. [Renttherunway](http:\/\/renttherunway.com)\n>\n>[[1]](#references) ModCloth sells women\u2019s vintage clothing and accessories, from which the curator of the dataset collected data from three categories: dresses, tops, and bottoms. RentTheRunWay is a unique platform that allows women to rent clothes for various occasions; they collected data from several categories. \n>\n>**Note:** In both datasets, fit feedback belongs to one of three classes: \u2018Small,\u2019 \u2018Fit,\u2019 and \u2018Large.\u2019 And also, some [assumptions](#assumptions) have been made about the features in the dataset.","29d440a2":"Some more important observations here, before we dive into performing the pre-processing tasks onto our data:\n* Bra_size, hips might not need to be a float- category dtype?\n* Most of the shoe sizes are around 5-9, but the maximum shoe size is 38! (It is surprising as the website uses UK shoe sizing.)\n* Size has a minimum of 0 and maximum Size matches the maximum shoe size.\n\nLet's visualize the numerical quantities in our dataset as boxplots, to have a better sense of the outliers.\n\n<a id=\"5\"><\/a>\n## Boxplot of numerical variables","5cf9e287":"<a id='references'><\/a>\n# References:\n    [1] Rishabh Misra, Mengting Wan, Julian McAuley Decomposing Fit Semantics for Product Size Recommendation in Metric Spaces. RecSys, 2018.\n    \n<a id='assumptions'><\/a>\n# Assumptions:\nThe data source has been assumed as following on the Modcloth dataset: \n* item_id- from item.\n* waist- from user input.\n* size - from item.\n* quality- from user input.\n* cup size- from user input.\n* hips- from user input.\n* bra size- from user input.\n* category- from item.\n* bust- from user input.\n* height- from user input\n* user_name- from user input\n* length - from user input\n* fit- from user input\n* user_id- from user.\n\n\n*Aditya Agrawal*","8f118d1a":"* **review_summary\/ review_text**- The NA values are there because these reviews are simply not provided by customers. Let's just fill those as 'Unknown'.\n* **shoe_size** -  Roughly 66.3% of the shoe_size data is missing. We will change the shoe_size into *category* dtype and fill the NA values as 'Unknown'.\n* **shoe_width** - Roughly 77.5% of the shoe_width data is missing. We will fill the NA values as 'Unknown' \n* **waist**- Waist column has the highest number of missing values - 96.5%! We will drop this column.\n* **bust**- 85.6% missing values and highly correlated to bra_size. Remove.\n* **user_name**- user_name itself is not needed with the user_id given. Remove.\n\nTo convert shoe_width to an ordered category type we have to import CategoricalDType and supply the order of the categories.","e8a6ef27":"<a id=\"4\"><\/a>\n# EDA - Exploratory Data Analysis\n\nWe can already make few observations here, by looking at the head of the data:\n1. There are missing values across the dataframe, which need to be handled.\n2. Cup-size contains multiple preferences- which will need handling, if we wish to define cup sizes as 'category' datatype.\n3. Height column needs to be parsed for extracting the height in a numerical quantity, it looks like a string (object) right now.\n4. Not so important, but some columns could do with some renaming- for removing spaces.\n\nFirstly, we handle the naming of columns for ease-of-access in pandas.","be9922c8":"* **hips**-\nHips column has a lot of missing values ~ 32.28%! We know this data would possibly be missing because Modcloth never got this data from the user most probably. We cannot remove such a significant chunk of the data, so we need another way of handling this feature. We will bin the data- on the basis of quartiles.\n* **length**- There are only 35 missing rows in length, we'll take a look at these. We saw that most probably the customers did not leave behind the feedback or the data was corrupted in these rows. However, we should be able to impute these values using review related fields (if they are filled!). Or we could also simply choose to remove these rows. For the sake of this analysis, we will remove these rows.\n* **quality**- There are only 68 missing rows in quality, we'll took a look at these. Similarly to length, the customers did not leave behind the feedback or the data was corrupted in these rows. We will remove these rows and convert the dtype to an ordinal variable (ordered categorical).","0f43111a":"We can see here a \"linear correlation between foot size and height\". This observation was also seen [here](https:\/\/www.statcrunch.com\/5.0\/viewreport.php?reportid=35115).","42034296":"<a id=\"14\"><\/a>\n## 2. Categories vs. Fit\/Length\/Quality\nHere, we will visualize how the items of different categories fared in terms of - fit, length, and quality. This will tell Modcloth which categories need more attention! \n\nI have plotted 2 distributions in categories here:\n\n**1. Unnormalized**- viewing the frequency counts directly- for comparison across categories. We also include the best fit, length, or quality measure in this plot.\n\n**2. Normalized** -  viewing the distribution for the category after normalizing the counts, amongst the category itself- it will help us compare what are major reason for return amongst the category itself. We exclude the best sizing & quality measures, so as to focus on the pre-dominant reasons of return per category (if any).","4ab64cec":"- **Category vs. Fit**","860f3173":"<a id=\"10\"><\/a>\n### Step-by-step features processing:\n* **bra_size:** Although it looks numerical, it only ranges from 28 to 48, with most of the sizing lying around 34-38. It makes sense to convert this to *categorical* dtype. We'll fill the NA values into an 'Unknown' category. We can see above that most of the buyers have a bra-sizing of 34 or 36.\n* **bust**- We can see by looking at the values which are not null, that bust should be an integer dtype. We also need to handle a special case where bust is given as - '37-39'. We'll replace the entry of '37-39' with the mean, i.e.- 38, for analysis purposes. Now we can safely convert the dtype to int. However, considering that **roughly 86% of the bust data is missing**, eventually it was decided to remove this feature.\n* **category**- none missing; change to dtype *category*.\n* **cup size**- Change the dtype to *category* for this column. This col has around 7% missing values. Taking a look at the rows where this value is missing might hint us towards how to handle these missing values.","38ece13a":"* A **major chunk of the users (~40%) have only bought 1 item from Modcloth** during the time this data was collected. Although we found only 903 out of those were first time users (no previous data existed of these customers). This explains and reaffirms the dataset curator's statement about sparsity of the data.\n* Most users bought 1, 2, or 3 products from Modcloth out of the ~80,000 transactions in this dataset.","c2c89c3b":"Here, we can see that amongst the categories themselves:\n* Wedding, tops, & outerwear categories usually have more returns due to large sizing.\n* New, sale, & bottoms usually have frequent returns due to small sized buys.\n* Dresses has similar return reasons, in terms of fit.","6bba04b0":"- **Category vs Length**","857d1a74":"### Statistical description of numerical variables","d58ecfa5":"### Thank you so much reading this EDA and feel to reach out to me\/comment below for questions. Please share any constructive feedback you have and if you like the work- please upvote!\n\n#### Looking forward to more Kaggling! Cheers. ","ed8efd68":"<a id=\"16\"><\/a>\n## 3. Height vs shoe_size -  Modcloth customers\nIt would be interesting to see if there exists a linear relation between the height of a person and their shoe-size, i.e.- it will mean shoe-size increases with increase in height!","6e7589f0":"* **bra_size**:\nWe can take a look at the top 10 bra-sizes (we can see that boxplot shows 2 values as outliers, as per the IQR- Inter-Quartile Range).","ceea06c5":">We can see that the entry seems to be legit, except for the shoe size- it could be wrongly entered by the customer or simple noise. We'll enter this as null value for now.","eb2ccd22":"The normalized plot, focusing on the problems allows us to dig deeper into length-wise reasons of return per category:\n* Customers tend to make 'slightly long' purchases in *bottoms, new, sale, & tops* categories.\n* 'slightly short' returns take place mostly in *dresses and wedding * categories.","d8fb663d":"<a id=\"6\"><\/a>\n## Handling Outliers\n\n* **shoe_size**:\nWe can clearly see that the single maximum value of shoe size (38) is an outlier and we should ideally remove that row or handle that outlier value. Let's take a look at that entry in our data.","27d19ac6":">Now we add a new column to the original data- *first_time_user*, which is a bool feature which indicates if a user, of a transaction, is a first-time user or not. This is based on the grounds that Modcloth has no previous information about the person, infact it is possible that the new user did multiple transactions in the first time!","76ccd77e":"* Almost the same share of people have rated the categories of *tops, new, dresses, & bottoms* as 5, 4, & 3.\n* All the trends in terms of share of ratings seems to be constant across categories.","32eda813":"* **fit**- Change the dtype to *category* for this column. We can see that a vast majority of customers gave a good 'fit' feedback for the items on Modcloth!","fbaf6b94":"<a id=\"12\"><\/a>\n# EDA via visualizations\n\n<a id=\"13\"><\/a>\n<a id='dist_plots'><\/a>\n## 1. Distribution of different features over Modcloth dataset","ac89db58":"Observations:\n* Best-fit response (*fit*) has been highest for *new, dresses, and tops* categories. \n* Overall maximum bad fit-feedback has belonged mostly to 2 categories- *new and tops*! *Dresses and bottoms* categories follow. \n* *Weddings, outerwear, and sale* are not prominent in our visualization- mostly due to the lack of transactions in these categories.","d3ddafd1":"We can see that now there are no more missing values! We can move onto visualizing and gaining more insight about the data."}}