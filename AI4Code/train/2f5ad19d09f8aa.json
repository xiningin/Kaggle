{"cell_type":{"dfb6ba90":"code","ec0922fc":"code","8fdaf581":"code","94bafec1":"code","2d40c1e2":"code","4d823e26":"code","8eaf28df":"code","b1ae4844":"code","9dcd7f20":"code","02345a0a":"code","ecc80ea5":"code","9f53f079":"code","26453796":"code","66a72444":"code","87780ee1":"code","fc60028d":"code","bd18ad04":"code","4fadb54e":"code","c044385a":"code","14ec697e":"code","7871660a":"code","eb298803":"code","e2fa7d55":"code","9759cc4c":"code","c3a6502f":"code","2f8317d7":"code","734f255d":"code","bbc5007d":"code","1f9e3ca2":"code","1849cca7":"code","68658335":"code","90e60e3a":"code","faa715df":"code","9ed3791c":"code","e74c7288":"code","269d2788":"code","4bd9514c":"code","c6230455":"code","6e7ebe06":"code","cf1ff6ae":"code","88ee601a":"code","a0a26224":"code","f7359161":"code","acc91dab":"code","78b4f3be":"code","1258502a":"code","05f067b2":"code","dfca2cb4":"code","a83629e9":"code","cd0059e8":"code","6bbdbf8d":"code","df22b0c4":"code","509a9126":"code","4ec0122e":"code","1afd6209":"code","3be96736":"code","64cbbac9":"code","a2bd5a83":"code","93af139f":"code","d83a8151":"code","07b4fc98":"code","c6a54602":"code","1a3e5a6f":"markdown","457b96c0":"markdown","4ea89158":"markdown","2f8fd755":"markdown","4c9ee1dc":"markdown","602aaff2":"markdown","bc9c9df6":"markdown","3841c58e":"markdown","afe3c0f5":"markdown","82543dd7":"markdown","d3b7fb0a":"markdown","d8a77a54":"markdown","a93a907d":"markdown","46942de1":"markdown","e341ca29":"markdown","081895f7":"markdown","6484cdb7":"markdown","1a3ea4ed":"markdown","8267dd4e":"markdown","e09a846e":"markdown","148ceb5a":"markdown","f3dece73":"markdown","f6978258":"markdown","191a4263":"markdown","6e8d9088":"markdown","9332bb8c":"markdown","df4e9108":"markdown","8f7aebb6":"markdown","990f20b1":"markdown","e23ca0b5":"markdown","e2b4355b":"markdown"},"source":{"dfb6ba90":"%%time\n\n## Scalability limitations with ANN\nim_size=1024* 1024*3\nim_size\n(im_size*512 + 512) + (512*100 + 100) + (100 + 1)","ec0922fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(os.listdir('\/kaggle\/input\/'))\n\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8fdaf581":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')","94bafec1":"# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns","2d40c1e2":"#model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder","4d823e26":"#preprocess.\nfrom keras.preprocessing.image import ImageDataGenerator","8eaf28df":"#dl libraraies\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.utils import to_categorical","b1ae4844":"# specifically for cnn\nfrom keras.layers import Dropout, Flatten,Activation\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization","9dcd7f20":"# #configure\n# # sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \n# style.use('fivethirtyeight')\n# sns.set(style='whitegrid',color_codes=True)\n\n\nimport tensorflow as tf\nimport random as rn\n\n# specifically for manipulating zipped images and getting numpy arrays of pixel values of images.\nimport cv2                  \nimport numpy as np  \nfrom tqdm import tqdm\nimport os                   \nfrom random import shuffle  \nfrom zipfile import ZipFile\nfrom PIL import Image","02345a0a":"X=[]\nZ=[]\nIMG_SIZE=150\nFLOWER_DAISY_DIR='..\/input\/flowers-recognition\/flowers\/daisy'\nFLOWER_SUNFLOWER_DIR='..\/input\/flowers-recognition\/flowers\/sunflower'\nFLOWER_TULIP_DIR='..\/input\/flowers-recognition\/flowers\/tulip'\n#FLOWER_DANDI_DIR='..\/input\/flowers-recognition\/flowers\/dandelion'\nFLOWER_ROSE_DIR='..\/input\/flowers-recognition\/flowers\/rose'","ecc80ea5":"def assign_label(img,flower_type):\n    return flower_type","9f53f079":"def make_train_data(flower_type,DIR):\n    for img in tqdm(os.listdir(DIR)):\n        label=assign_label(img,flower_type)\n        path = os.path.join(DIR,img)\n        img = cv2.imread(path,cv2.IMREAD_COLOR)\n        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n        \n        X.append(np.array(img))\n        Z.append(str(label))  ","26453796":"make_train_data('Daisy',FLOWER_DAISY_DIR)\nprint(len(X))","66a72444":"make_train_data('Sunflower',FLOWER_SUNFLOWER_DIR)\nprint(len(X))","87780ee1":"make_train_data('Tulip',FLOWER_TULIP_DIR)\nprint(len(X))","fc60028d":"# make_train_data('Dandelion',FLOWER_DANDI_DIR)\n# print(len(X))","bd18ad04":"make_train_data('Rose',FLOWER_ROSE_DIR)\nprint(len(X))","4fadb54e":"print(len(X))\nprint(len(Z))\nprint(set(Z))","c044385a":"fig,ax=plt.subplots(5,3)\nfig.set_size_inches(15,15)\nfor i in range(5):\n    for j in range (3):\n        l=rn.randint(0,len(Z))\n        ax[i,j].imshow(X[l])\n        ax[i,j].set_title('Flower: '+Z[l])\n        \nplt.tight_layout();       ","14ec697e":"print(\"Number of Pixles in each image :\",150*150*3)","7871660a":"Z[:10]","eb298803":"le=LabelEncoder()\nY=le.fit_transform(Z)\nY=to_categorical(Y,4)\nprint(Y.shape)","e2fa7d55":"Y","9759cc4c":"print(type(X))\nprint(len(X))\nprint(X[1].shape)","c3a6502f":"X_NEW=np.array(X)\n# Image Standardization [Scale 0-1]\nX_NEW=X_NEW\/255","2f8317d7":"X_train,X_test,y_train,y_test=train_test_split(X_NEW,Y,test_size=0.20,random_state=42)\n#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)","734f255d":"np.shape(X_train),np.shape(y_train),np.shape(X_test),np.shape(y_test)\n#np.shape(X_val),np.shape(y_val)","bbc5007d":"np.random.seed(42)\nrn.seed(42)\ntf.set_random_seed(42)","1f9e3ca2":"9*64*32 + 64","1849cca7":"# # modelling starts using a CNN.\n\n\n# Initialising the CNN classifier\nmodel = Sequential()\n\n# Add a Convolution layer with 32 kernels of 5X5 shape with activation function ReLU\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),activation ='relu', input_shape = (150,150,3)))\n\n# Add a Max Pooling layer of size 2X2\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n# Add another Convolution layer with 64 kernels of 3X3 shape with activation function ReLU\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n\n# Adding another pooling layer\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n \n\n# Add another Convolution layer with 96 kernels of 3X3 shape with activation function ReLU\nmodel.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n\n# Adding another pooling layer\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\n# Flattening the layer before fully connected layers\nmodel.add(Flatten())\n\n# Adding a fully connected layer with 512 neurons\nmodel.add(Dense(512,activation='relu'))\n\n# The final output layer with 5 neuron to predict the categorical classifcation\nmodel.add(Dense(4, activation = \"softmax\"))\n\nmodel.summary()","68658335":"model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])","90e60e3a":"((y_train.shape[0])*.85)\/256","faa715df":"batchSize=256\nep=16\nmodel.fit(X_train,y_train,batch_size=batchSize,epochs=ep,validation_split=.15)","9ed3791c":"model.evaluate(X_test,y_test)","e74c7288":"plt.plot(range(len(model.history.history['acc'])),model.history.history['acc'])\nplt.plot(range(len(model.history.history['loss'])),model.history.history['loss'])\nplt.title('Model- Accuracy Vs Loss')\nplt.xlabel('Epochs')\nplt.legend(['Accuracy', 'Loss'])\nplt.show()","269d2788":"plt.plot(model.history.history['loss'])\nplt.plot(model.history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'Valid'])\nplt.show()","4bd9514c":"plt.plot(model.history.history['acc'])\nplt.plot(model.history.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'Valid'])\nplt.show()","c6230455":"# getting predictions on val set.\npred=model.predict(X_test)\nprint(\"Predicted Probabilities -\\n\",pred[:4])\npred_digits=np.argmax(pred,axis=1)\nprint(\"\\nPredicted Class [Highest Prob] -\",pred_digits[:4])","6e7ebe06":"# now storing some properly as well as misclassified indexes'.\ni=0\nprop_class=[]\nmis_class=[]\n\nfor i in range(len(y_test)):\n    if(np.argmax(y_test[i])==pred_digits[i]):\n        prop_class.append(i)\n    if(len(prop_class)==8):\n        break\n\ni=0\nfor i in range(len(y_test)):\n    if(not np.argmax(y_test[i])==pred_digits[i]):\n        mis_class.append(i)\n    if(len(mis_class)==8):\n        break","cf1ff6ae":"warnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\ncount=0\nfig,ax=plt.subplots(4,2)\nfig.set_size_inches(15,15)\nfor i in range (4):\n    for j in range (2):\n        ax[i,j].imshow(X_test[prop_class[count]])\n        ax[i,j].set_title(\"Predicted Flower : \"+str(le.inverse_transform([pred_digits[prop_class[count]]]))+\"\\n\"+\"Actual Flower : \"+str(le.inverse_transform(np.argmax([y_test[prop_class[count]]]))))\n        plt.tight_layout()\n        count+=1","88ee601a":"warnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\ncount=0\nfig,ax=plt.subplots(4,2)\nfig.set_size_inches(15,15)\nfor i in range (4):\n    for j in range (2):\n        ax[i,j].imshow(X_test[mis_class[count]])\n        ax[i,j].set_title(\"Predicted Flower : \"+str(le.inverse_transform([pred_digits[mis_class[count]]]))+\"\\n\"+\"Actual Flower : \"+str(le.inverse_transform(np.argmax([y_test[mis_class[count]]]))))\n        plt.tight_layout()\n        count+=1","a0a26224":"from IPython.display import Image\nImage(r'\/kaggle\/input\/into-to-cnn\/data_aug_img.jpg', width=600)","f7359161":"# ?ImageDataGenerator","acc91dab":"##Storing the Augumented images just for only one image.\n\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\ndatagen = ImageDataGenerator(\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest')\n\nimg = load_img('\/kaggle\/input\/into-to-cnn\/data_aug_img.jpg')  \nx = img_to_array(img)  \nx = x.reshape((1,) + x.shape)  \n\nprint(x.shape)\n# the .flow() command below generates batches of randomly transformed images\n# and saves the results to the `preview\/` directory\ni = 0\nfor batch in datagen.flow(x, batch_size=1,save_to_dir='\/kaggle\/working', save_prefix='vehichle', save_format='jpeg'):\n    i += 1\n    if i > 19:\n        break  # otherwise the generator would loop indefinitely","78b4f3be":"# # modelling starts using a CNN.\n\n\n# Initialising the CNN classifier\nmodel = Sequential()\n\n# Add a Convolution layer with 32 kernels of 5X5 shape with activation function ReLU\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),activation ='relu', input_shape = (150,150,3)))\n\n# Add a Max Pooling layer of size 2X2\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n# Add another Convolution layer with 64 kernels of 3X3 shape with activation function ReLU\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n\n# Adding another pooling layer\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n \n\n# Add another Convolution layer with 96 kernels of 3X3 shape with activation function ReLU\nmodel.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n\n# Adding another pooling layer\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\n# Flattening the layer before fully connected layers\nmodel.add(Flatten())\n\n# Adding a fully connected layer with 512 neurons\nmodel.add(Dense(512,activation='relu'))\n\n# The final output layer with 5 neuron to predict the categorical classifcation\nmodel.add(Dense(4, activation = \"softmax\"))\n\nmodel.summary()","1258502a":"model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])","05f067b2":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n#datagen.fit(X_train)\ndtAug=datagen.flow(X_train,y_train)","dfca2cb4":"History = model.fit_generator(dtAug,epochs = ep,verbose = 1,use_multiprocessing=True, steps_per_epoch=X_train.shape[0] \/\/ batchSize,validation_data=(X_test,y_test))","a83629e9":"model.evaluate(X_test,y_test)","cd0059e8":"plt.plot(History.history['loss'])\nplt.plot(History.history['acc'])\nplt.title('Model Loss Vs Model Accuracy')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['Loss', 'Accuracy'])\nplt.show()","6bbdbf8d":"#Transfer Learning specific modules\nfrom keras.applications.vgg16 import VGG16","df22b0c4":"base_model=VGG16(include_top=False, weights=None,input_shape=(150,150,3), pooling='avg')","509a9126":"import os\nprint(os.listdir('\/kaggle\/input\/trans-learn-weights\/'))","4ec0122e":"weights_path='..\/input\/trans-learn-weights\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'","1afd6209":"base_model.load_weights(weights_path)","3be96736":"base_model.summary()","64cbbac9":"model=Sequential()\nmodel.add(base_model)\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(4,activation='softmax'))\n\nmodel.summary()","a2bd5a83":"base_model.trainable=False # setting the VGG model to be untrainable.","93af139f":"model.compile(optimizer=Adam(lr=1e-4),loss='categorical_crossentropy',metrics=['accuracy'])","d83a8151":"History = model.fit_generator(datagen.flow(X_train,y_train, batch_size=batchSize),\n                              epochs = ep,use_multiprocessing=True,\n                              verbose = 1, steps_per_epoch=X_train.shape[0] \/\/ batchSize)","07b4fc98":"model.evaluate(X_test,y_test)","c6a54602":"plt.plot(History.history['loss'])\nplt.plot(History.history['acc'])\nplt.title('Model Loss Vs Model Accuracy')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['Loss', 'Accuracy'])\nplt.show()","1a3e5a6f":"![image.png](attachment:image.png)","457b96c0":"## C. Fitting on the Training set and making predcitons on the Validation set","4ea89158":"[ **5 ) Visualizing Predictons on the Validation Set**](#content5)","2f8fd755":"[ **4 ) Evaluating the Model Performance**](#content4)","4c9ee1dc":"# <center> Transfer Learning [Bonus - 2]\n\n* **A Comprehensive Guide to Transfer Learning -** https:\/\/www.kaggle.com\/rajmehra03\/a-comprehensive-guide-to-transfer-learning\n\n* **CNN Architectures : VGG, ResNet, Inception + TL -** https:\/\/www.kaggle.com\/shivamb\/cnn-architectures-vgg-resnet-inception-tl\n    \n* **VGG16 \u2013 Convolutional Network for Classification and Detection -** https:\/\/neurohive.io\/en\/popular-networks\/vgg16\/","602aaff2":"# Reference Blogs\n\n* **TensorFlow Payground -** http:\/\/playground.tensorflow.org\/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.68420&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\n\n* **Softmax Ativation Function -** https:\/\/stats.stackexchange.com\/questions\/233658\/softmax-vs-sigmoid-function-in-logistic-classifier\n\n\n### DL\/CNN Tutorials \n\n* **Getting Started with Neural Networks -** https:\/\/courses.analyticsvidhya.com\/courses\/take\/getting-started-with-neural-networks\/quizzes\/11776978-quiz-sequential-modeling\n* **Convolutional Neural Networks (CNN) from Scratch -** https:\/\/courses.analyticsvidhya.com\/courses\/take\/convolutional-neural-networks-cnn-from-scratch\/texts\/10846922-transforming-the-data\n\n* **Practical Deep Learning for Coders -** https:\/\/course19.fast.ai\/\n\n* **Making neural nets uncool again-** https:\/\/www.fast.ai\/\n\n\n### Interactive CNN\n\n* **Interactice Convolution Neural Network on MNIST -** https:\/\/cs.stanford.edu\/people\/karpathy\/convnetjs\/demo\/mnist.html\n\n* **Interactive CNN with CIFAR-10 dataset -** https:\/\/cs.stanford.edu\/people\/karpathy\/convnetjs\/demo\/cifar10.html\n\n* **Toy 2D data set classification using CNN -** https:\/\/cs.stanford.edu\/people\/karpathy\/convnetjs\/demo\/classify2d.html\n\n### Blogs to follow for Computer Vision\n\n* **Computer Vision for Dummies -** https:\/\/www.visiondummy.com\/\n\n* **Learn OpenCV -** https:\/\/www.learnopencv.com\/\n\n* **Tombone's CV blog -** https:\/\/www.computervisionblog.com\/\n\n* **Andrez Karpathy Blog -** http:\/\/karpathy.github.io\/\n\n* **AI Shack Blog -** https:\/\/aishack.in\/\n\n* **Computer Vision Talks -** https:\/\/computer-vision-talks.com\/","bc9c9df6":"## B. Visualize some Random Images","3841c58e":"[ **2 ) Preparing the Data**](#content2)","afe3c0f5":"### CORRECTLY CLASSIFIED FLOWER IMAGES","82543dd7":"# VGG16\n\nVGG16 was publised in 2014 and is one of the simplest (among the other cnn architectures used in Imagenet competition). It's Key Characteristics are:\n\n* The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes\n*  This network contains total __16 layers__ in which weights and bias parameters are learnt.\n*  A total of 13 convolutional layers are stacked one after the other and 3 dense layers for classification.\n* This network is a pretty large network and it has about __138 million__ (approx) parameters.\n* The number of filters in the convolution layers follow an increasing pattern (similar to decoder architecture of autoencoder).\n* The informative features are obtained by max pooling layers applied at different steps in the architecture.\n* The dense layers comprises of 4096, 4096, and 1000 nodes each.\n*  The cons of this architecture are that it is slow to train and produces the model with very large size.\n\n**The VGG16 architecture is given below:**","d3b7fb0a":"## E. Setting the Random Seeds","d8a77a54":"### Difference Between CPU and GPU by NVIDIA - https:\/\/www.youtube.com\/watch?v=-P28LKWTzrI","a93a907d":"![vgg_16.png](attachment:6a7d785f-a52c-47eb-91cb-a647dab57da6.png)","46942de1":"> ## B. Compiling the Keras Model & Summary","e341ca29":"<a id=\"content2\"><\/a>\n# 2. Data Preparation","081895f7":"## A. Making the functions to get the training and validation set from the Images","6484cdb7":"## A. Building the CNN Model","1a3ea4ed":"<a id=\"content5\"><\/a>\n## E. Visualizing Predictons on the Validation Set","8267dd4e":"<a id=\"content3\"><\/a>\n# 3. Modelling","e09a846e":"## C. Label Encoding the Y array (i.e. Daisy->0, Rose->1 etc...) & then One Hot Encoding","148ceb5a":"# <center> Flower Recognition Through CNN Keras","f3dece73":"1. <a id=\"content4\"><\/a>\n## D. Evaluating the Model Performance","f6978258":"<a id=\"content1\"><\/a>\n# 1. Importing Required Libraries","191a4263":"[ **3 ) Modelling**](#content3)","6e8d9088":"---","9332bb8c":"[ **1 ) Importing Various Modules**](#content1)","df4e9108":"## CONTENTS ::","8f7aebb6":"# Pre - Requisites\n\n* Logisitic Regression - https:\/\/www.analyticsvidhya.com\/blog\/2015\/10\/basics-logistic-regression\/\n* Overfitting and Underfittting - https:\/\/www.analyticsvidhya.com\/blog\/2020\/02\/underfitting-overfitting-best-fitting-machine-learning\/\n* Deep Learning \n* Forward Propogation \n* Gradient Decscent \n* Back Propogation  ","990f20b1":"# <center> Data Augmentation to Prevent Overfitting [Bonus -1]\n\n1. How to Configure Image Data Augmentation in Keras - https:\/\/machinelearningmastery.com\/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks\/","e23ca0b5":"## D. Splitting into Training and Test Sets","e2b4355b":"### MISCLASSIFIED IMAGES OF FLOWERS"}}