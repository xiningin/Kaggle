{"cell_type":{"5f4f2cbd":"code","73ed2908":"code","5ec1e4f7":"code","b7068a9f":"code","344f24b3":"code","639f1937":"code","496de13a":"code","7bb8b62e":"code","68cc071b":"code","eafcddeb":"code","4002f288":"code","4e7994aa":"code","15539ba5":"code","efec9c71":"code","1a92834c":"code","40740938":"code","95436223":"code","e7978924":"code","26300c21":"code","35b9696c":"code","5bccff7b":"code","32b925c3":"code","2ddb819f":"code","32c60dcb":"code","16268619":"code","1ebf0cf6":"code","b79319ee":"code","9c23e8a5":"code","7644f0f9":"code","d38c537a":"code","1059fea8":"code","4daef875":"code","22581350":"code","b40cb83b":"code","6f66bd63":"code","d71d57fd":"code","6e56afae":"code","6e0ea61b":"code","18904e1b":"code","be2fc1e5":"code","ca720890":"code","a28cfe20":"code","f00367fb":"code","92092c71":"code","0cd55601":"code","7a56b0c6":"code","83fbce61":"code","a278364f":"code","8a451f42":"code","5295ff78":"code","2d05e491":"code","52a9f7bc":"code","6ea048ce":"code","68604a80":"code","555cf10c":"code","58dc8d7c":"code","a7f157be":"code","069c26d3":"code","e087b497":"code","f8a1923d":"code","24eb96b8":"code","9dae0fc6":"code","fbcf78e4":"code","77822680":"code","8250375c":"code","425435f2":"code","add2f9a8":"code","5e6d31e5":"code","76846488":"code","cd9f3c28":"code","353fa39d":"code","9099e915":"code","3c11ab03":"code","dd8116cf":"code","90c1b24f":"code","db243d37":"code","ff61144d":"code","55d58a26":"code","a77f86d7":"code","2045d6ea":"code","6505a021":"code","9d356059":"code","278117cb":"code","08b61b72":"code","a64f7153":"code","7992e195":"code","09ed5ff0":"code","7eae7811":"code","09590577":"code","075e9f05":"code","4338f171":"code","51d70f05":"code","b0358356":"code","0bda8d10":"code","0199d707":"markdown","b21eea6f":"markdown","e126cda4":"markdown","afda2c2a":"markdown","92d39d82":"markdown","80095d6e":"markdown","41b5c4da":"markdown","4f090450":"markdown","5f2aa29c":"markdown","d65c5f3a":"markdown","c1295914":"markdown","f6cf3545":"markdown","748a7899":"markdown","44350c0f":"markdown","030eafde":"markdown","347a8b11":"markdown"},"source":{"5f4f2cbd":"import pandas as pd #Analysis \nimport matplotlib.pyplot as plt #Visulization\nimport seaborn as sns #Visulization\nimport numpy as np #Analysis \nfrom scipy.stats import norm #Analysis \nfrom sklearn.preprocessing import StandardScaler #Analysis \nfrom scipy import stats #Analysis \nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport gc\nfrom textblob import TextBlob\nfrom nltk import word_tokenize, pos_tag, ne_chunk\nimport os\nimport string\ncolor = sns.color_palette()\nimport spacy\nimport re\nfrom nltk.tokenize import RegexpTokenizer\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\nimport os\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom textblob import TextBlob, Word\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import defaultdict\nfrom nltk import FreqDist\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\nimport copy\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sn","73ed2908":"df_train = pd.read_csv(\"..\/input\/kuc-hackathon-winter-2018\/drugsComTrain_raw.csv\", parse_dates=[\"date\"])\ndf_test = pd.read_csv(\"..\/input\/kuc-hackathon-winter-2018\/drugsComTest_raw.csv\", parse_dates=[\"date\"])","5ec1e4f7":"print(\"Train shape :\" ,df_train.shape)\nprint(\"Test shape :\", df_test.shape)","b7068a9f":"df_all = pd.concat([df_train,df_test])\ndf_all.info()","344f24b3":"df_all = df_all.dropna(axis=0)","639f1937":"print(\"unique values count of all : \" ,len(set(df_all['uniqueID'].values)))\nprint(\"length of all : \" ,df_all.shape[0])","496de13a":"df_all=df_all.drop(['uniqueID'], axis=1)\ndf_all=df_all.drop(['date'], axis=1)","7bb8b62e":"condition_dn = df_all.groupby(['condition'])['drugName'].nunique().sort_values(ascending=False)\ncondition_dn[0:20].plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\nplt.xlabel(\"\", fontsize = 20)\nplt.ylabel(\"\", fontsize = 20)\nplt.title(\"Top20 : The number of drugs per condition.\", fontsize = 20)","68cc071b":"print('data before:')\nprint(df_all.shape)\ndf_all=df_all[~df_all.condition.str.contains(\"<\/span>\", na=False)]\ndf_all=df_all[df_all['condition']!='Not Listed \/ Othe']\nprint('data after:')\nprint(df_all.shape)","eafcddeb":"condition_dn = df_all.groupby(['condition'])['drugName'].nunique().sort_values(ascending=False)\ncondition_dn[0:20].plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\nplt.xlabel(\"\", fontsize = 20)\nplt.ylabel(\"\", fontsize = 20)\nplt.title(\"Top20 : The number of drugs per condition.\", fontsize = 20)","4002f288":"# condition_dn = df_all.groupby(['condition'])['drugName'].nunique().sort_values(ascending=False)\n\n# condition_dn[condition_dn.shape[0]-20:condition_dn.shape[0]].plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\n# plt.xlabel(\"\", fontsize = 20)\n# plt.ylabel(\"\", fontsize = 20)\n# plt.title(\"Bottom20 : The number of drugs per condition.\", fontsize = 20)","4e7994aa":"# condition_dn_all = df_all.groupby(['condition'])['drugName'].nunique().sort_values(ascending=False)\n# condition_dn_df_all=pd.DataFrame(condition_dn_all).reset_index()\n# condition_dn_df_one_to_one_all = condition_dn_df_all[condition_dn_df_all['drugName']==1].reset_index()\n# condition_dn_df_40_all = condition_dn_df_all[condition_dn_df_all['drugName']<=10].reset_index()\n# condition_dn_df_other_all = condition_dn_df_all[condition_dn_df_all['drugName']>10].reset_index()","15539ba5":"# all_list = set(df_all.index)\n# condition_list = []\n# for i,j in enumerate(df_all['condition']):\n#     for c in list(condition_dn_df_40_all['condition']):\n#         if j == c:\n#             condition_list.append(i)\n            \n# new_idx = all_list.difference(set(condition_list))\n# df_all = df_all.iloc[list(new_idx)].reset_index()\n# del df_all['index']","efec9c71":"drug_using_count=df_all.groupby('condition')['drugName'].count().sort_values(ascending=False)\ndrug_using_count[0:10].plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\nplt.xlabel(\"\", fontsize = 20)\nplt.ylabel(\"\", fontsize = 20)\nplt.title(\"Top10 : The number of reviews per condition.\", fontsize = 20)","1a92834c":"rating = df_all['rating'].value_counts().reset_index(name='count').sort_values(by=['index'],ascending=True)\nrating\nrating.plot(kind=\"bar\",x='index',y='count', figsize = (14,6), fontsize = 10,color=\"green\", legend=None)\nplt.xlabel(\"\", fontsize = 20)\nplt.ylabel(\"\", fontsize = 20)\nplt.xticks(rotation=0)\nplt.title(\"Number of Reviews for Different Ratings\", fontsize = 20)","40740938":"df_all=df_all[df_all['condition'].isin(['Acne','Pain','Anxiety'])]","95436223":"medicineList=set(df_all['drugName'])\nmedicineList=[med.lower() for med in medicineList]\nmedicineList_copy=copy.deepcopy(medicineList)\nfor i in medicineList_copy:\n    if(len(i.split(' '))>1):\n        for j in i.split(' '):\n            if len(j)>2:\n                medicineList.append(j)\nmedicineList.append('propanolol')","e7978924":"# drug_using_count=df_all.groupby('drugName')['condition'].count().sort_values(ascending=False)\n# print(drug_using_count)\n# drug_using_count_df=pd.DataFrame(drug_using_count).reset_index()\n# drug_using_count_df_100_all = drug_using_count_df[drug_using_count_df['condition']<=1500].reset_index()\n# drug_using_count_df_100_all","26300c21":"# all_list = set(df_all.index)\n# condition_list = []\n# for i,j in enumerate(df_all['drugName']):\n#     for c in list(drug_using_count_df_100_all['drugName']):\n#         if j == c:\n#             condition_list.append(i)\n            \n# new_idx = all_list.difference(set(condition_list))\n# df_all = df_all.iloc[list(new_idx)].reset_index()\n# del df_all['index']","35b9696c":"rating = df_all['rating'].value_counts().reset_index(name='count').sort_values(by=['index'],ascending=True)\nrating\nrating.plot(kind=\"bar\",x='index',y='count', figsize = (14,6), fontsize = 10,color=\"green\", legend=None)\nplt.xlabel(\"\", fontsize = 20)\nplt.ylabel(\"\", fontsize = 20)\nplt.xticks(rotation=0)\nplt.title(\"Number of Reviews for Different Ratings\", fontsize = 20)","5bccff7b":"import spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndef get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    #tag = nltk.pos_tag([word])[0][1][0].upper()\n    doc = nlp(word)\n    tag=doc[0].tag_[0]    \n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n    if tag==\"R\":\n        return tag_dict.get(tag, wordnet.ADJ)\n    elif tag==\"J\":\n        return tag_dict.get(tag, wordnet.ADJ)\n    elif tag==\"V\":\n        return tag_dict.get(tag, wordnet.VERB)\n    else:\n        return tag_dict.get(tag, wordnet.NOUN)\n\n\n# 1. Init Lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# # 2. Lemmatize Single Word with the appropriate POS tag\n# word = 'feet'\n# print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n\n# 3. Lemmatize a Sentence with the appropriate POS tag\n# sentence = \"The striped aren't been hanging on their feet for best\"\n# print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])","32b925c3":"def lemmatize_with_postag(sentence):\n    sent = TextBlob(sentence)\n    tag_dict = {\"J\": 'a', \n                \"N\": 'n', \n                \"V\": 'v', \n                \"R\": 'r'}\n    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n    return lemmatized_list\n#lemmatize_with_postag(\"The striped 123 aren't been hanging on their feet for best\")","2ddb819f":"# from nltk.collocations import *\n# bigram_measures = nltk.collocations.BigramAssocMeasures()\n# trigram_measures = nltk.collocations.TrigramAssocMeasures()\n# text=\"Pristiq 12-23 1324 12mg is awesome so far.Don&#039;t reject it bcs it is nearly identical to Effexor. Effexor caused extreme sweating.I would stand in front of a forceful fan while dressing, and still be drenched!My doc told me I might not have that with pristiq and I don&#039;t! I am just over 6 weeks, and it took a full 4 weeks to kick in.The first couple of days I felt great, calm and happy, and got hopeful, but then slid back into the dark place.But I kept going and at 4 wks I suddenly one day woke up and started planning fun outings for my teen, after months of sofa lounging.  100mg, F, 56, 135lbs.Tried prozac (allergic), paxil (weight gain), effex (sweating), lexapro (out of dark place, but flat).I hear it&#039;s hell to stop, but I&#039;m not going to stop. Why wld I?\"\n# tokens = pre_process(text)\n# finder = BigramCollocationFinder.from_words(tokens)\n# scored = finder.score_ngrams(bigram_measures.raw_freq)\n# scored\n#sorted(bigram for bigram, score in scored)  # doctest: +NORMALIZE_WHITESPACE","32c60dcb":"word_map={'weight gain':'gain weight','depression anxiety':'anxiety depression','start':'begin',\n          'lose lb':'lose weight','feel like':'feel','sever pain':'severe pain','remarkable':\"good\",\n          'seem help':'help','work well':'help','awful':'bad','childhood':'child','previously':'previous',\n          'affect':'effect','suffers':'suffer','well work':'work well','pain severe':'severe pain',\n          'miserable':'bad','terrible':'bad','great':'good','prescribed':'prescribe','doc':'doctor',\n          'felt':'feel','tremendously':'lot','could not':'not','get severe':'get bad','found':'find',\n          'quickly':'fast','rise':'increase','med':'medication','badly':'bad','bear':'bearable',\n          'battling':'battle','medicationicine':'medication','drug':'medication',\n          'cautiously':'cautious','burning':'burn','begin':'start','bad':'bad','well':'good','medicationication':'medication',\n          'absorbs':'absorb','absorbed':'absorb','long term':'long time','issue':'problem','medicine':'medication'}","16268619":"nlp = spacy.load(\"en_core_web_sm\",disable=['ner', 'parser'])\ntokenizer = RegexpTokenizer(r'\\w+')\nnltk.download('stopwords')\nporter=nltk.PorterStemmer()\nadditional_dic=['soooo','month','year','day','week','say','etc','usually','bcs','one','two','much','something','anything','doctortor','039','ive','absolutely','actually',\"also\",\"always\",\"almost\",\"already\",\"still\",\"would\",'ago']\nadditional_dic_step=[\"story\",\"news\",\"old\",\"home\",\"due\",\"thats\",\"try\",\"ing\",\"seem\",\"haha\",\"age\",\"could\",\"even\",\"usually\",\"since\",\"another\",\"really\",\"theyre\",\"nearly\",\"finally\",\"mg\",\"hour\",\"week\",\"month\",\"year\",\"years\",\"day\",\"yr\",\"able\",\"although\",\"able\",\"around\",\"...\"]\nstoplist = stopwords.words('english')+additional_dic\nstoplist+=additional_dic_step\nnot_stop = [\"aren't\",\"couldn't\",\"didn't\",\"doesn't\",\"don't\",\"hadn't\",\"hasn't\",\"haven't\",\"isn't\",\"mightn't\",\"mustn't\",\"needn't\",\"no\",\"nor\",\"not\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"wouldn't\"]\nto_be_change=[\"n't\",\"no\",\"nor\",\"didnt\",\"havent\",\"cant\",\"shouldnot\",\"wasnt\",\"wasnot\",\"werent\",\"dont\",\"werenot\",\"wouldnt\",\"wouldnot\",\"couldnt\"]\nfor i in not_stop:\n    stoplist.remove(i)\n\ndef pre_process(text,n_gram=1):\n    text=text.replace('&#039;', '')\n    text=text.replace('&quot;', '')\n    text=text.replace('&amp;', '')\n    text=text.lower()\n    for med in medicineList:\n        text=text.replace(med,\"\")\n    for i in word_map:\n        text=text.replace(i,word_map[i])\n    text=re.sub(\"\\d+[A-Za-z]{2}\",\"\",text)\n    text=re.sub(\"\\d+[-\\\/]\\d+\",\"\",text)\n    text=re.sub(\"\\d+[x]\",\"\",text)\n    text=re.sub(r'\\d+', '', text)\n    #tokens=word_tokenize(text)\n    tokens=tokenizer.tokenize(text)\n    tokens=[t for t in tokens if t not in stoplist and t not in string.punctuation]    \n    #tokens=[nltk.WordNetLemmatizer().lemmatize(t,pos='n' or 'v' or 's' or 'r' or 'a') for t in tokens]\n    #tokens=[t for t in tokens if t not in string.digits]\n    tokens=['not' if t in to_be_change else t for t in tokens]\n    #tokens=[nltk.WordNetLemmatizer().lemmatize(t,pos='n' or 'v' or 'r' or 'a') for t in tokens]\n    #tokens=[porter.stem(t) for t in tokens]  \n    #tokens=[lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in tokens]\n    tokens=[t for t in tokens if len(t)>=3]\n    text=' '.join(tokens)\n    doc = nlp(text)\n    tokens = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n    #result=[\" \".join(tokens)]\n    #ngrams = zip(*[tokens[i:] for i in range(n_gram)])\n    return tokens","1ebf0cf6":"punctuations = string.punctuation\ndef pre_process_2(text):\n    text=text.replace('&#039;', '')\n    text=text.replace('&quot;', '')\n    text=text.replace('&amp;', '')\n    text=text.lower()\n    for med in medicineList:\n        text=text.replace(med,\"\")\n    for i in word_map:\n        text=text.replace(i,word_map[i])\n    text=re.sub(\"\\d+[A-Za-z]{2}\",\"\",text)\n    text=re.sub(\"\\d+[-\\\/]\\d+\",\"\",text)\n    text=re.sub(\"\\d+[x]\",\"\",text)\n    text=re.sub(r'\\d+', '', text)\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens=tokenizer.tokenize(text)\n    text=\" \".join(tokens)\n    doc = nlp(text)\n    tokens = [token.lemma_.strip() for token in doc if token.lemma_ != '-PRON-']\n    tokens=[t for t in tokens if t not in stoplist and t not in punctuations]\n    tokens=['not' if t in to_be_change else t for t in tokens]\n    tokens=[t for t in tokens if len(t)>=3]\n    return tokens\n# text=\"Pristiq 12-23 1324 12mg wouldn't is awesome so far.Don&#039;t reject it bcs it is nearly identical to Effexor. Effexor caused extreme sweating.I would stand in front of a forceful fan while dressing, and still be drenched!My doc told me I might not have that with pristiq and I don&#039;t! I am just over 6 weeks, and it took a full 4 weeks to kick in.The first couple of days I felt great, calm and happy, and got hopeful, but then slid back into the dark place.But I kept going and at 4 wks I suddenly one day woke up and started planning fun outings for my teen, after months of sofa lounging.  100mg, F, 56, 135lbs.Tried prozac (allergic), paxil (weight gain), effex (sweating), lexapro (out of dark place, but flat).I hear it&#039;s hell to stop, but I&#039;m not going to stop. Why wld I?\"\n# pre_process_2(text)","b79319ee":"# doc = nlp('days')\n# tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n# tokens","9c23e8a5":"# text=\"Pristiq 12-23 1324 wouldnt 12mg is awesome so far.Don&#039;t went it bcs it is nearly identical to Effexor. Effexor caused extreme sweating.I would stand in front of a forceful fan while dressing, and still be drenched!My doc told me I might not have that with pristiq and I don&#039;t! I am just over 6 weeks, and it took a full 4 weeks to kick in.The first couple of days I felt great, calm and happy, and got hopeful, but then slid back into the dark place.But I kept going and at 4 wks I suddenly one day woke up and started planning fun outings for my teen, after months of sofa lounging.  100mg, F, 56, 135lbs.Tried prozac (allergic), paxil (weight gain), effex (sweating), lexapro (out of dark place, but flat).I hear it&#039;s hell to stop, but I&#039;m not going to stop. Why wld I?\"\n# pre_process(text)","7644f0f9":"def pre_ngram(tokens,n_gram=2):\n    ngrams = zip(*[tokens[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]","d38c537a":"df_all.shape","1059fea8":"toks=df_all['review'].apply(pre_process)\ntoks_2gram=toks.apply(pre_ngram,n_gram=2)\ntoks_3gram=toks.apply(pre_ngram,n_gram=3)","4daef875":"#https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc kernel \n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='white',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  ","22581350":"# plot_wordcloud(df_all[\"review\"], title=\"Word Cloud of review\")\ntext_tokens=[c for l in toks for c in l]\ntext_tokens_frequency=FreqDist(text_tokens)\ntext=\" \".join(text_tokens)\nplot_wordcloud(text, title=\"Word Cloud of review\")","b40cb83b":"df_all_9_10 = df_all[df_all[\"rating\"]>8]\ndf_all_4_8 = df_all[(df_all[\"rating\"]>=4) & (df_all[\"rating\"]<9)]\ndf_all_1_3 = df_all[df_all[\"rating\"]<4]","6f66bd63":"toks_9_10=df_all_9_10['review'].apply(pre_process)\ntoks_4_8=df_all_4_8['review'].apply(pre_process)\ntoks_1_3=df_all_1_3['review'].apply(pre_process)","d71d57fd":"text_tokens_9_10=[c for l in toks_9_10 for c in l]\ntext_tokens_4_8=[c for l in toks_4_8 for c in l]\ntext_tokens_1_3=[c for l in toks_1_3 for c in l]\ntext_tokens_frequency_9_10=FreqDist(text_tokens_9_10)\ntext_tokens_frequency_4_8=FreqDist(text_tokens_4_8)\ntext_tokens_frequency_1_3=FreqDist(text_tokens_1_3)","6e56afae":"toks_1_3_toCombine=' '.join(text_tokens_1_3)\ntoks_4_8_toCombine=' '.join(text_tokens_4_8)\ntoks_9_10_toCombine=' '.join(text_tokens_9_10)","6e0ea61b":"from nltk.collocations import *\nbigram_measures = nltk.collocations.BigramAssocMeasures()\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\n#text=\"Pristiq 12-23 1324 12mg is awesome so far.Don&#039;t reject it bcs it is nearly identical to Effexor. Effexor caused extreme sweating.I would stand in front of a forceful fan while dressing, and still be drenched!My doc told me I might not have that with pristiq and I don&#039;t! I am just over 6 weeks, and it took a full 4 weeks to kick in.The first couple of days I felt great, calm and happy, and got hopeful, but then slid back into the dark place.But I kept going and at 4 wks I suddenly one day woke up and started planning fun outings for my teen, after months of sofa lounging.  100mg, F, 56, 135lbs.Tried prozac (allergic), paxil (weight gain), effex (sweating), lexapro (out of dark place, but flat).I hear it&#039;s hell to stop, but I&#039;m not going to stop. Why wld I?\"\ntokens = pre_process(toks_1_3_toCombine)\nfinder = BigramCollocationFinder.from_words(tokens)\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nscored\n#sorted(bigram for bigram, score in scored)  # doctest: +NORMALIZE_WHITESPACE","18904e1b":"toks_2gram_1_3=toks_1_3.apply(pre_ngram,n_gram=2)\ntoks_2gram_4_8=toks_4_8.apply(pre_ngram,n_gram=2)\ntoks_2gram_9_10=toks_9_10.apply(pre_ngram,n_gram=2)","be2fc1e5":"text_tokens_2gram_1_3=[c for l in toks_2gram_1_3 for c in l]\ntext_tokens_2gram_4_8=[c for l in toks_2gram_4_8 for c in l]\ntext_tokens_2gram_9_10=[c for l in toks_2gram_9_10 for c in l]\ntext_tokens_frequency_2gram_9_10=FreqDist(text_tokens_2gram_9_10)\ntext_tokens_frequency_2gram_4_8=FreqDist(text_tokens_2gram_4_8)\ntext_tokens_frequency_2gram_1_3=FreqDist(text_tokens_2gram_1_3)","ca720890":"toks_3gram_1_3=toks_1_3.apply(pre_ngram,n_gram=3)\ntoks_3gram_4_8=toks_4_8.apply(pre_ngram,n_gram=3)\ntoks_3gram_9_10=toks_9_10.apply(pre_ngram,n_gram=3)","a28cfe20":"text_tokens_3gram_1_3=[c for l in toks_3gram_1_3 for c in l]\ntext_tokens_3gram_4_8=[c for l in toks_3gram_4_8 for c in l]\ntext_tokens_3gram_9_10=[c for l in toks_3gram_9_10 for c in l]\ntext_tokens_frequency_3gram_9_10=FreqDist(text_tokens_3gram_9_10)\ntext_tokens_frequency_3gram_4_8=FreqDist(text_tokens_3gram_4_8)\ntext_tokens_frequency_3gram_1_3=FreqDist(text_tokens_3gram_1_3)","f00367fb":"## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace","92092c71":"fd_sorted = pd.DataFrame(sorted(text_tokens_frequency.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace = horizontal_bar_chart(fd_sorted.head(50), '#82C8F3')\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=1, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of rating 1 to 10\"\n                                          ])\nfig.append_trace(trace, 1, 1)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')","0cd55601":"text_9_10=\" \".join(text_tokens_9_10)\nplot_wordcloud(text, title=\"Word Cloud of review: Positive Sentiment\")\ntext_4_8=\" \".join(text_tokens_4_8)\nplot_wordcloud(text, title=\"Word Cloud of review: Neutral Sentiment\")\ntext_1_3=\" \".join(text_tokens_1_3)\nplot_wordcloud(text, title=\"Word Cloud of review: Negative Sentiment\")","7a56b0c6":"fd_sorted1 = pd.DataFrame(sorted(text_tokens_frequency_1_3.items(), key=lambda x: x[1])[::-1])\nfd_sorted1.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted1.head(15), '#82C8F3')\n\nfd_sorted2 = pd.DataFrame(sorted(text_tokens_frequency_4_8.items(), key=lambda x: x[1])[::-1])\nfd_sorted2.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted2.head(15), '#82C8F3')\n\nfd_sorted3 = pd.DataFrame(sorted(text_tokens_frequency_9_10.items(), key=lambda x: x[1])[::-1])\nfd_sorted3.columns = [\"word\", \"wordcount\"]\ntrace3 = horizontal_bar_chart(fd_sorted3.head(15), '#82C8F3')","83fbce61":"fig = tools.make_subplots(rows=1, cols=3, vertical_spacing=0.03,horizontal_spacing=0.08,\n                          subplot_titles=[\"Frequent words of rating 1 to 3\",\"Frequent words of rating 4 to 8\",\"Frequent words of rating 9 to 10\"\n                                          ])\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig['layout'].update(height=800, width=1300, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\",font=dict(size=16))\npy.iplot(fig, filename='word-plots')\n#['and','the','039','for','was','have','had','but','not','with','this','been','about']","a278364f":"fd_sorted1_2gram = pd.DataFrame(sorted(text_tokens_frequency_2gram_1_3.items(), key=lambda x: x[1])[::-1])\nfd_sorted1_2gram.columns = [\"word\", \"wordcount\"]\ntrace1_2gram = horizontal_bar_chart(fd_sorted1_2gram.head(15), '#82C8F3')\n\nfd_sorted2_2gram = pd.DataFrame(sorted(text_tokens_frequency_2gram_4_8.items(), key=lambda x: x[1])[::-1])\nfd_sorted2_2gram.columns = [\"word\", \"wordcount\"]\ntrace2_2gram = horizontal_bar_chart(fd_sorted2_2gram.head(15), '#82C8F3')\n\nfd_sorted3_2gram = pd.DataFrame(sorted(text_tokens_frequency_2gram_9_10.items(), key=lambda x: x[1])[::-1])\nfd_sorted3_2gram.columns = [\"word\", \"wordcount\"]\ntrace3_2gram = horizontal_bar_chart(fd_sorted3_2gram.head(15), '#82C8F3')","8a451f42":"fig = tools.make_subplots(rows=1, cols=3, vertical_spacing=0.03,horizontal_spacing=0.18,\n                          subplot_titles=[\"Frequent words of rating 1 to 3\",\"Frequent words of rating 4 to 8\",\"Frequent words of rating 9 to 10\"\n                                          ])\nfig.append_trace(trace1_2gram, 1, 1)\nfig.append_trace(trace2_2gram, 1, 2)\nfig.append_trace(trace3_2gram, 1, 3)\nfig['layout'].update(height=800, width=1300, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\",font=dict(size=16))\npy.iplot(fig, filename='word-plots')\n#['and','the','039','for','was','have','had','but','not','with','this','been','about']","5295ff78":"fd_sorted1_3gram = pd.DataFrame(sorted(text_tokens_frequency_3gram_1_3.items(), key=lambda x: x[1])[::-1])\nfd_sorted1_3gram.columns = [\"word\", \"wordcount\"]\ntrace1_3gram = horizontal_bar_chart(fd_sorted1_3gram.head(15), '#82C8F3')\n\nfd_sorted2_3gram = pd.DataFrame(sorted(text_tokens_frequency_3gram_4_8.items(), key=lambda x: x[1])[::-1])\nfd_sorted2_3gram.columns = [\"word\", \"wordcount\"]\ntrace2_3gram = horizontal_bar_chart(fd_sorted2_3gram.head(15), '#82C8F3')\n\nfd_sorted3_3gram = pd.DataFrame(sorted(text_tokens_frequency_3gram_9_10.items(), key=lambda x: x[1])[::-1])\nfd_sorted3_3gram.columns = [\"word\", \"wordcount\"]\ntrace3_3gram = horizontal_bar_chart(fd_sorted3_3gram.head(15), '#82C8F3')","2d05e491":"fig = tools.make_subplots(rows=1, cols=3, vertical_spacing=0.03,horizontal_spacing=0.18,\n                          subplot_titles=[\"Frequent words of rating 1 to 3\",\"Frequent words of rating 4 to 8\",\"Frequent words of rating 9 to 10\"\n                                          ])\nfig.append_trace(trace1_3gram, 1, 1)\nfig.append_trace(trace2_3gram, 1, 2)\nfig.append_trace(trace3_3gram, 1, 3)\nfig['layout'].update(height=800, width=1300, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\",font=dict(size=16))\npy.iplot(fig, filename='word-plots')","52a9f7bc":"\" \".join(toks.values[0])","6ea048ce":"doc = nlp(\" \".join(toks.values[1]))\nfrom spacy.symbols import nsubj, VERB\nfrom spacy import displacy\ndisplacy.render(doc, style='dep')","68604a80":"#!pip install sense2vec==1.0.0a0","555cf10c":"# import sense2vec\n# #from sense2vec import Sense2VecComponent\n# s2v = sense2vec.load('..\/input\/reddit-vectors-for-sense2vec-spacy\/reddit_vectors-1.1.0\/reddit_vectors-1.1.0\/')\n# s2v = sense2vec.Sense2VecComponent('..\/input\/reddit-vectors-for-sense2vec-spacy\/reddit_vectors-1.1.0\/reddit_vectors-1.1.0\/')\n# spacy_tok.add_pipe(s2v)\n# doc = spacy_tok(u\"dessert.\")\n# freq = doc[0]._.s2v_freq\n# vector = doc[0]._.s2v_vec\n# most_similar = doc[0]._.s2v_most_similar(5)\n# most_similar,freq","58dc8d7c":"# index_list=[]\n# for idx, val in enumerate(toks.values):\n#     if len(val)==0 or val is None:\n#         index_list.append(idx)\n# print(index_list)\n# from textblob import TextBlob\n# from nltk import word_tokenize, pos_tag, ne_chunk\n# for text in toks.values[0:10]:\n#     result = TextBlob(\" \".join(text))\n#         #print(result.tags)\n#         #{<JJ><NN>?<NN>}\n#         #{<NN><NN>?<JJ>}\n#         #{<VBP><JJS>}\n#     reg_exp=r\"\"\"\n#         NP: {<JJ><NN>?<NN>}\n#             {<NN><NN>?<JJ>}\n#     \"\"\"\n#     rp = nltk.RegexpParser(reg_exp)\n#     result = rp.parse(result.tags)\n#     print(result)\n# print([i for i in result.subtrees()])\n# empty_list = []\n# filter_list=['try','range','story','box','weekend']\n# for subtree in result.subtrees():\n#     if subtree.label() == 'NP':\n#         subtree = str(subtree.flatten())\n#         tree = nltk.Tree.fromstring(subtree, read_leaf=lambda x: x.split(\"\/\")[0])   \n#         result=[checkIfMatch(element,tree.leaves()) for element in filter_list]\n#         if True not in result:\n#             if(TextBlob(\" \".join(tree.leaves())).sentences[0].sentiment.polarity<0):\n#                  empty_list.append(tree.leaves())            \n#         print(tree.leaves())\n# empty_list\n# text='severe muscle stiffness'\n# result = TextBlob(text)\n# result.sentences[0].sentiment.polarity","a7f157be":"def checkIfMatch(elem,match):\n    if elem in match:\n        return True;\n    else :\n        return False;\n\n\nreg_exp=r\"\"\"\n    NP: {<JJ><NN>?<NN>}\n        {<NN><NN>?<JJ>}\n        {<VBP><JJS>}\n\"\"\"\nrp = nltk.RegexpParser(reg_exp)\nphase_list = []\nphase_df_array=[]\nfilter_list=['try','range','story','box','weekend','thing','school','bit','time','football','way','count','side','prescribe','turkey']\nfor text in toks.values:\n    if text is not None and len(text)>0:\n        phase_df_array_sub=[]\n        result=rp.parse(TextBlob(\" \".join(text)).tags)    \n        for subtree in result.subtrees():\n            if subtree.label() == 'NP':\n                subtree = str(subtree.flatten())\n                tree = nltk.Tree.fromstring(subtree, read_leaf=lambda x: x.split(\"\/\")[0])   \n                result=[checkIfMatch(element,tree.leaves()) for element in filter_list]\n                if True not in result:\n                    if(TextBlob(\" \".join(tree.leaves())).sentences[0].sentiment.polarity<0):\n                        phase_list.append(' '.join(tree.leaves()))   \n                        phase_df_array_sub.append(' '.join(tree.leaves()))   \n        phase_df_array.append(phase_df_array_sub)\n    else:\n        phase_df_array.append([])","069c26d3":"df_all['sentiment'] = df_all[\"rating\"].apply(lambda x: 0 if x <4 else ( 2 if x>8 else 1))\ndf_all['cleanText']=[\" \".join(t) for t in toks]\ndf_all['side_effect']=phase_df_array","e087b497":"import multiprocessing\nfrom time import time\nfrom gensim.models import Word2Vec\ncores = multiprocessing.cpu_count()\nw2v_model = Word2Vec(min_count=10,\n                     window=4,\n                     size=300,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=cores-1)\nt = time()\n\nw2v_model.build_vocab(toks, progress_per=10000)\n\nprint('Time to build vocab: {} mins'.format(round((time() - t) \/ 60, 2)))\nt = time()\n\nw2v_model.train(toks, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to train the model: {} mins'.format(round((time() - t) \/ 60, 2)))\nw2v_model.init_sims(replace=True)\n","f8a1923d":"len(w2v_model.wv.vocab)","24eb96b8":"w2v_model.wv.most_similar(positive=[\"anxiety\"])","9dae0fc6":"w2v_model.wv.most_similar(positive=[\"face\"])","fbcf78e4":"import numpy as np\n\nclass DocSim(object):\n    def __init__(self, w2v_model , stopwords=[]):\n        self.w2v_model = w2v_model\n        self.stopwords = stopwords\n\n    def vectorize(self, doc):\n        \"\"\"Identify the vector values for each word in the given document\"\"\"\n        doc = doc.lower()\n        words = [w for w in doc.split(\" \") if w not in self.stopwords]\n        word_vecs = []\n        for word in words:\n            try:\n                vec = self.w2v_model[word]\n                word_vecs.append(vec)\n            except KeyError:\n                # Ignore, if the word doesn't exist in the vocabulary\n                pass\n\n        # Assuming that document vector is the mean of all the word vectors\n        # PS: There are other & better ways to do it.\n        vector = np.mean(word_vecs, axis=0)\n        return vector\n\n\n    def _cosine_sim(self, vecA, vecB):\n        \"\"\"Find the cosine similarity distance between two vectors.\"\"\"\n        csim = np.dot(vecA, vecB) \/ (np.linalg.norm(vecA) * np.linalg.norm(vecB))\n        if np.isnan(np.sum(csim)):\n            return 0\n        return csim\n\n    def calculate_similarity(self, source_doc, target_docs=[], threshold=0):\n        \"\"\"Calculates & returns similarity scores between given source document & all\n        the target documents.\"\"\"\n        if isinstance(target_docs, str):\n            target_docs = [target_docs]\n\n        source_vec = self.vectorize(source_doc)\n        results = []\n        for doc in target_docs:\n            target_vec = self.vectorize(doc)\n            sim_score = self._cosine_sim(source_vec, target_vec)\n            if sim_score > threshold:\n                results.append({\n                    'score' : sim_score,\n                    'doc' : doc\n                })\n            # Sort results by score in desc order\n            results.sort(key=lambda k : k['score'] , reverse=True)\n\n        return results","77822680":"#document similarity\n# from gensim.models.keyedvectors import KeyedVectors\n# model_path = '.\/data\/GoogleNews-vectors-negative300.bin'\n# w2v_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\nds = DocSim(w2v_model)\nsource_doc = 'how to delete an invoice'\ntarget_docs = ['delete a invoice', 'how do i remove an invoice', 'purge an invoice']\n\n# This will return 3 target docs with similarity score\nsim_scores = ds.calculate_similarity(df_all['cleanText'].values[0],df_all['cleanText'].values[6])\n\nprint(sim_scores)","8250375c":"#df_all.groupby('drugName')['side_effect'].count()\n# df_valid=df_all[df_all.astype(str)['side_effect'] != '[]']\n# df_drug_count_all=df_all.groupby('drugName')['side_effect'].count().reset_index(name='count')\n# df_drug_count_valid=df_valid.groupby('drugName')['side_effect'].count().reset_index(name='count')\n# df_drug_count_bind=pd.merge(df_drug_count_all,df_drug_count_valid,how='left',on=['drugName'])\n# df_drug_count_bind['percentage']=df_drug_count_bind['count_y']\/df_drug_count_bind['count_x']\n# df_drug_count_bind.percentage.fillna(value=0, inplace=True)\n# df_drug_count_bind","425435f2":"# DrugNameList=set(df_all['drugName'].values)\n# DrugNameList\n# df_all_Pain=df_all[df_all['condition']=='Pain']\n# df_all_Insomnia=df_all[df_all['condition']=='Insomnia']\n# df_all_Anxiety=df_all[df_all['condition']=='Anxiety']\npd.set_option('display.max_rows', 5000)","add2f9a8":"def sideEffectAnalysis(condition):\n    df_all_Pain=df_all[df_all['condition']==condition]\n    df_valid=df_all_Pain[df_all_Pain.astype(str)['side_effect'] != '[]']\n    df_drug_count_all=df_all_Pain.groupby('drugName')['side_effect'].count().reset_index(name='count')\n    df_drug_count_valid=df_valid.groupby('drugName')['side_effect'].count().reset_index(name='count')\n    df_drug_count_bind=pd.merge(df_drug_count_all,df_drug_count_valid,how='left',on=['drugName'])\n    df_drug_count_bind['percentage']=df_drug_count_bind['count_y']\/df_drug_count_bind['count_x']\n    df_drug_count_bind.percentage.fillna(value=0, inplace=True)\n    df_drug_count_bind=df_drug_count_bind.sort_values('percentage',ascending=False)\n    ax=df_drug_count_bind[0:20].plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"#31b0c1\",x='drugName',y='percentage', legend=None)\n    plt.xlabel(\"\", fontsize = 20)\n    plt.ylabel(\"\", fontsize = 20)\n    vals = ax.get_yticks()\n    ax.set_yticklabels(['{:,.2%}'.format(x) for x in vals])\n    plt.title(\"Percentage of Side Effect Occurrence for \"+condition, fontsize = 20)\n    \n    df_all_Pain_group=df_all_Pain.groupby('drugName')['side_effect'].apply(sum).reset_index(name='side_effect')\n    df_all_Pain_noSide=df_all_Pain_group[df_all_Pain_group.astype(str)['side_effect'] == '[]']['drugName']\n    print('----------Medicine with no side effects mentioned: '+condition+'----------')\n    print(df_all_Pain_noSide.values)\n    side_effect_list_pain=df_all_Pain_group[df_all_Pain_group.astype(str)['side_effect'] != '[]']['side_effect'].values\n    #print(side_effect_list_pain)\n    side_effect_list_str_pain=[c for l in side_effect_list_pain for c in l]\n    print('----------Top Frequent Side Effect for: '+condition+'----------')\n    side_effect_list_str_pain_frequency=FreqDist(side_effect_list_str_pain).most_common(20)\n    print(side_effect_list_str_pain_frequency)\n    side_effect_list_str_pain_text=\" \".join(side_effect_list_str_pain)\n    plot_wordcloud(side_effect_list_str_pain_text, title=\"Word Cloud of side effect: \"+condition)\n    df_pain_with_side_effect=df_all_Pain_group[df_all_Pain_group.astype(str)['side_effect'] != '[]']\n    df_pain_with_side_effect['count']=df_pain_with_side_effect['side_effect'].str.len()\n    df_pain_with_side_effect=df_pain_with_side_effect.sort_values('count',ascending=False)\n    df_pain_with_side_effect[0:20].plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"#31b0c1\",x='drugName',y='count')\n    plt.xlabel(\"\", fontsize = 20)\n    plt.ylabel(\"\", fontsize = 20)\n    plt.title(\"Count of side effect reviews for \"+condition, fontsize = 20)","5e6d31e5":"sideEffectAnalysis('Pain')","76846488":"sideEffectAnalysis('Acne')","cd9f3c28":"sideEffectAnalysis('Anxiety')","353fa39d":"import multiprocessing\nfrom time import time\nfrom gensim.models import Word2Vec\nfrom nltk.cluster import KMeansClusterer\nimport nltk\ndef w2v_clustering(condition,cluster):\n    NUM_CLUSTERS=cluster\n    df_all_cluster=df_all[df_all['condition']==condition]\n    df_all_cluster_group=df_all_cluster.groupby('drugName')['side_effect'].apply(sum).reset_index(name='side_effect')\n    side_effect_list=df_all_cluster_group[df_all_cluster_group.astype(str)['side_effect'] != '[]']['side_effect'].values\n\n    cores = multiprocessing.cpu_count()\n    w2v_model = Word2Vec(min_count=5,\n                         window=4,\n                         size=300,\n                         sample=6e-5, \n                         alpha=0.03, \n                         min_alpha=0.0007, \n                         negative=20,\n                         workers=cores-1)\n    t = time()\n\n    w2v_model.build_vocab(side_effect_list, progress_per=10000)\n\n    print('Time to build vocab: {} mins'.format(round((time() - t) \/ 60, 2)))\n    t = time()\n\n    w2v_model.train(side_effect_list, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n\n    print('Time to train the model: {} mins'.format(round((time() - t) \/ 60, 2)))\n    w2v_model.init_sims(replace=True)\n#     print(w2v_model.wv.vocab)\n    kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n    assigned_clusters = kclusterer.cluster(w2v_model[w2v_model.wv.vocab], assign_clusters=True)\n#     print (assigned_clusters)\n#     print('As:', assigned_clusters)\n#     print('Means:', kclusterer.means())\n    vocabulary=[i for i in w2v_model.wv.vocab.keys()]\n    data = {'Vocabulary':vocabulary, 'Cluster':assigned_clusters} \n    data_cluster=pd.DataFrame(data)\n    df=data_cluster.groupby('Cluster')['Vocabulary'].count()\n    index_list=['type'+str(i+1) for i in range(NUM_CLUSTERS)]\n    df = pd.DataFrame({'count': df.values},\n                            index=index_list)\n    df['count'].plot.pie( figsize=(5, 5))\n    for i in range(NUM_CLUSTERS):\n        print('Cluster'+str(i+1)+':')\n        print(data_cluster[data_cluster['Cluster']==i]['Vocabulary'].values.tolist())","9099e915":"#w2v_model.wv.most_similar(positive=[\"dry mouth\"])","3c11ab03":"w2v_clustering('Pain',3)\n","dd8116cf":"w2v_clustering('Anxiety',3)\n","90c1b24f":"w2v_clustering('Anxiety',2)","db243d37":"w2v_clustering('Acne',3)","ff61144d":"from sklearn.decomposition import TruncatedSVD\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\ndef print_terms(cm, num,svd,vectorizer):\n    original_space_centroids = svd.inverse_transform(cm.cluster_centers_)\n    order_centroids = original_space_centroids.argsort()[:, ::-1]\n    terms = vectorizer.get_feature_names()\n    for i in range(num):\n        print(\"Cluster %d:\" % i, end='')\n        for ind in order_centroids[i, :50]:\n            print(' %s' % terms[ind], end='')\n        print()\ndef cluster_side_effect(condition,n_cluster=3):\n    index_list=['type'+str(i+1) for i in range(n_cluster)]\n    df_all_cluster=df_all[df_all['condition']==condition]\n    df_all_cluster_group=df_all_cluster.groupby('drugName')['side_effect'].apply(sum).reset_index(name='side_effect')\n    side_effect_list=df_all_cluster_group[df_all_cluster_group.astype(str)['side_effect'] != '[]']['side_effect'].values\n    #print(side_effect_list)\n    side_effect_list_str=[c for l in side_effect_list for c in l]\n    #print(side_effect_list)\n    side_effect_list_str=set(side_effect_list_str)\n    #print(side_effect_list)\n    #print(side_effect_list_str)\n    vectorizer=TfidfVectorizer(use_idf=True)\n    X=vectorizer.fit_transform(side_effect_list_str)\n    svd = TruncatedSVD(n_components=500,n_iter=200)\n    svd.fit(X)\n    print('total explained: ')\n    print(svd.explained_variance_ratio_.sum()) \n    X_transformed=svd.fit_transform(X)\n    km=KMeans(n_clusters=n_cluster,init='k-means++',max_iter=5000,n_init=1).fit(X_transformed)\n    print_terms(km,n_cluster,svd,vectorizer)\n    df = pd.DataFrame({'data': km.labels_}).reset_index()\n    df=df.groupby('data')['index'].count()\n    df = pd.DataFrame({'count': df.values},\n                        index=index_list)\n    print(df['count'])\n    df['count'].plot.pie( figsize=(5, 5))","55d58a26":"# def cluster_drug(condition,n_cluster=3):\n#     index_list=['type'+str(i+1) for i in range(n_cluster)]\n#     df_all_cluster=df_all[df_all['condition']==condition]\n#     df_all_cluster_group=df_all_cluster.groupby('drugName')['cleanText'].apply(sum).reset_index(name='cleanText')\n#     side_effect_list=df_all_cluster_group['cleanText'].values\n#     #print(side_effect_list)\n#     side_effect_list_str=side_effect_list\n# #     print(side_effect_list_str)\n#     vectorizer=TfidfVectorizer(use_idf=True)\n#     X=vectorizer.fit_transform(side_effect_list_str)\n#     svd = TruncatedSVD(n_components=200,n_iter=100)\n#     svd.fit(X)\n#     print('total explained: ')\n#     print(svd.explained_variance_ratio_.sum()) \n#     X_transformed=svd.fit_transform(X)\n#     km=KMeans(n_clusters=n_cluster,init='k-means++',max_iter=5000,n_init=1).fit(X_transformed)\n#     print_terms(km,n_cluster,svd,vectorizer)\n#     df = pd.DataFrame({'data': km.labels_}).reset_index()\n#     df=df.groupby('data')['index'].count()\n#     df = pd.DataFrame({'count': df.values},\n#                         index=index_list)\n#     print(df['count'])\n#     df['count'].plot.pie( figsize=(5, 5))","a77f86d7":"#     df_all_cluster=df_all[df_all['condition']=='Pain']\n#     df_all_cluster_group=df_all_cluster.groupby('drugName')['side_effect'].apply(sum).reset_index(name='side_effect')\n#     side_effect_list=df_all_cluster_group[df_all_cluster_group.astype(str)['side_effect'] != '[]']['side_effect'].values\n#     #print(side_effect_list)\n#     side_effect_list_str=[c for l in side_effect_list for c in l]\n#     #print(side_effect_list_str)\n#     vectorizer=TfidfVectorizer(use_idf=True)\n#     X=vectorizer.fit_transform(side_effect_list_str)\n#     svd = TruncatedSVD(n_components=500,n_iter=100)\n#     svd.fit(X)\n#     print('total explained: ')\n#     print(svd.explained_variance_ratio_.sum()) \n#     X_transformed=svd.fit_transform(X)\n#     km=KMeans(n_clusters=2,init='k-means++',max_iter=5000,n_init=1).fit(X_transformed)\n#     print_terms(km,2)\n    \n#     df = pd.DataFrame({'data': km.labels_}).reset_index()\n#     df=df.groupby('data')['index'].count()\n#     df = pd.DataFrame({'count': df.values},\n#                     index=['Type1','Type2'])\n#     df['count'].plot.pie( figsize=(5, 5))\n# #     df_all_cluster_count=df_all_cluster.groupby('cluster_type')['drugName'].count().sort_values(ascending=False)","2045d6ea":"#cluster_drug('Pain',3)","6505a021":"#cluster_drug('Anxiety',3)","9d356059":"#cluster_drug('Acne',3)","278117cb":"cluster_side_effect('Pain',3)\n","08b61b72":"cluster_side_effect('Anxiety',3)","a64f7153":"cluster_side_effect('Acne',3)","7992e195":"from sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom sklearn.utils import resample\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom scipy import sparse\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Bidirectional, LSTM, BatchNormalization, Dropout\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nimport keras\nfrom scipy import sparse\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport random\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Bidirectional, LSTM, BatchNormalization, Dropout\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport random\nfrom numpy import array\nfrom numpy import vstack\nfrom keras import optimizers","09ed5ff0":"type1 = df_all[df_all.sentiment==0]\ntype2 = df_all[df_all.sentiment==1]\ntype3 = df_all[df_all.sentiment==2]\ntype1_upsampled = resample(type1,\n                          replace=True, # sample with replacement\n                          n_samples=len(type3), # match number in majority class\n                          random_state=27) # reproducible results\ntype2_upsampled = resample(type2,\n                          replace=True, # sample with replacement\n                          n_samples=len(type3), # match number in majority class\n                          random_state=27) # reproducible results\ntype3_upsampled = resample(type3,\n                          replace=True, # sample with replacement\n                          n_samples=len(type3), # match number in majority class\n                          random_state=27) # reproducible results\nupsampled = pd.concat([type1_upsampled, type2_upsampled,type3_upsampled])\ntype1_upsampled.shape,type2_upsampled.shape,type3_upsampled.shape\n\nvectorizer = CountVectorizer(analyzer = 'word', \n                             tokenizer = None,\n                             preprocessor = None, \n                             stop_words = None, \n                             min_df = 2, \n                             ngram_range=(2,2),\n                             max_features = 10000\n                            )\ntransformer=TfidfTransformer()\npipeline = Pipeline([\n    ('vect', vectorizer)\n])\npipeline_1 = Pipeline([\n    ('vect', vectorizer),\n    ('trans', transformer)\n])\n%time features = pipeline.fit_transform(upsampled['cleanText'])\nnum_feats =  to_categorical(upsampled['sentiment'])\n\ndata = sparse.hstack((features, num_feats))\ndf_train, df_test = train_test_split(data, test_size=0.2, random_state=42) ","7eae7811":"# 1. Dataset\n# y_train = df_train['sentiment']\n# y_test = df_test['sentiment']\n\ny_train = df_train[:,10000:10003]\ny_test = df_test[:,10000:10003]\nx_train = df_train[:,0:10000]\nx_test = df_test[:,0:10000]\nsolution = y_test.copy()\n\n# 2. Model Structure\nmodel = keras.models.Sequential()\n\nmodel.add(keras.layers.Dense(400, input_shape=(10000,)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Activation('relu'))\nmodel.add(keras.layers.Dropout(0.5))\n\nmodel.add(keras.layers.Dense(200))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Activation('relu'))\nmodel.add(keras.layers.Dropout(0.5))\n\n# model.add(keras.layers.Dense(100, activation='softmax'))\nmodel.add(keras.layers.Dense(3, activation='softmax'))\n\n#sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)#10\nadam=optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)#8\n# 3. Model compile\nmodel.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n# 4. Train model\nhist = model.fit(x=x_train, y=y_train, epochs=20,batch_size=512,validation_data=(x_test,y_test))\n#hist = model.fit(x=train_data_features_1, y=y_train, epochs=4,batch_size=512)\n# 5. Traing process\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfig, loss_ax = plt.subplots()\n\nacc_ax = loss_ax.twinx()\n\nloss_ax.set_ylim([0.0, 2.0])\nacc_ax.set_ylim([0.0, 2.0])\n\nloss_ax.plot(hist.history['loss'], 'y', label='train loss')\nacc_ax.plot(hist.history['val_loss'], 'b', label='validation loss')\n\nloss_ax.set_xlabel('epoch')\nloss_ax.set_ylabel('loss')\nacc_ax.set_ylabel('val_loss')\n\nloss_ax.legend(loc='upper left')\nacc_ax.legend(loc='lower left')\n\nplt.show()\n\n# 6. Evaluation\nloss_and_metrics = model.evaluate(x_test, y_test, batch_size=512)\nprint('loss_and_metrics : ' + str(loss_and_metrics))\n\nsub_preds_deep = model.predict(x_test,batch_size=512)\nresultList=[]\nfor result in sub_preds_deep:\n    resultList.append(np.argmax(result)) \ncm=confusion_matrix(y_pred=resultList, y_true=np.argmax(y_test,axis=1))\nplt.figure(figsize = (10,7))\ndf_cm = pd.DataFrame(cm, index = [\"Negative\",\"Neutral\",\"Positive\"],\n                  columns =  [\"Negative\",\"Neutral\",\"Positive\"])\nsn.heatmap(df_cm, annot=True,fmt='g',cmap=\"Blues\")","09590577":"vectorizer = CountVectorizer(analyzer = 'word', \n                             tokenizer = None,\n                             preprocessor = None, \n                             stop_words = None, \n                             min_df = 2, \n                             ngram_range=(2,2),\n                             max_features = 10000\n                            )\ntransformer=TfidfTransformer()\n# pipeline = Pipeline([\n#     ('vect', vectorizer)\n# ])\npipeline = Pipeline([\n    ('vect', vectorizer),\n    ('trans', transformer)\n])\n#%time features = pipeline.fit_transform(df_all['cleanText'])\n%time features = pipeline.fit_transform(df_all['cleanText'])\ndf_all['feature']=list(features.toarray())\n# num_feats =  df_all['sentiment']\n# data = sparse.hstack((features, num_feats))\ndf_train, df_test = train_test_split(df_all, test_size=0.2, random_state=42) \ntype1 = df_train[df_train.sentiment==0]\ntype2 = df_train[df_train.sentiment==1]\ntype3 = df_train[df_train.sentiment==2]\ntype1_upsampled = resample(type1,\n                          replace=True, # sample with replacement\n                          n_samples=len(type3), # match number in majority class\n                          random_state=27) # reproducible results\ntype2_upsampled = resample(type2,\n                          replace=True, # sample with replacement\n                          n_samples=len(type3), # match number in majority class\n                          random_state=27) # reproducible results\ntype3_upsampled = resample(type3,\n                          replace=True, # sample with replacement\n                          n_samples=len(type3), # match number in majority class\n                          random_state=27) # reproducible results\nupsampled = pd.concat([type1_upsampled, type2_upsampled,type3_upsampled])\ntype1 = df_test[df_test.sentiment==0]\ntype2 = df_test[df_test.sentiment==1]\ntype3 = df_test[df_test.sentiment==2]\ntype1_upsampled = resample(type1,\n                          replace=True, # sample with replacement\n                          n_samples=len(type3), # match number in majority class\n                          random_state=27) # reproducible results\ntype2_upsampled = resample(type2,\n                          replace=True, # sample with replacement\n                          n_samples=len(type3), # match number in majority class\n                          random_state=27) # reproducible results\ntype3_upsampled = resample(type3,\n                          replace=True, # sample with replacement\n                          n_samples=len(type3), # match number in majority class\n                          random_state=27) # reproducible results\nupsampled_validate = pd.concat([type1_upsampled, type2_upsampled,type3_upsampled])","075e9f05":"# 1. Dataset\n# y_train = df_train['sentiment']\n# y_test = df_test['sentiment']\n\ny_train = to_categorical(upsampled['sentiment'])\ny_test = to_categorical(df_test['sentiment'])\nx_train =vstack(upsampled['feature'])\nx_test = vstack(df_test['feature'])\ny_validate = to_categorical(upsampled_validate['sentiment'])\nx_validate =vstack(upsampled_validate['feature'])\nsolution = y_test.copy()\n\n# 2. Model Structure\nmodel = keras.models.Sequential()\n\nmodel.add(keras.layers.Dense(300, input_shape=(10000,)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Activation('relu'))\nmodel.add(keras.layers.Dropout(0.5))\n\nmodel.add(keras.layers.Dense(200))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Activation('relu'))\nmodel.add(keras.layers.Dropout(0.5))\n\n# model.add(keras.layers.Dense(100, activation='softmax'))\nmodel.add(keras.layers.Dense(3, activation='softmax'))\n\nsgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)#10\nadam=optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)#8\n# 3. Model compile\nmodel.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy'])","4338f171":"# vectorizer.get_feature_names()","51d70f05":"# from tensorflow.python.keras.models import Sequential\n# from tensorflow.python.keras.layers import Dense, Bidirectional, LSTM, BatchNormalization, Dropout\n# from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n# import numpy as np\n# import keras\n# from keras.models import Sequential\n# from keras.layers import Dense\n# import random\n\n# # 1. Dataset\n# # y_train = df_train['sentiment']\n# # y_test = df_test['sentiment']\n\n# y_train = df_train[:,10000:10003]\n# y_test = df_test[:,10000:10003]\n# # x_train = df_train[:,0:10000]\n# x_test = df_test[:,0:10000]\n# solution = y_test.copy()\n\n# # 2. Model Structure\n# model = keras.models.Sequential()\n\n# model.add(keras.layers.Dense(400, input_shape=(10000,)))\n# model.add(keras.layers.BatchNormalization())\n# model.add(keras.layers.Activation('relu'))\n# model.add(keras.layers.Dropout(0.5))\n\n# model.add(keras.layers.Dense(200))\n# model.add(keras.layers.BatchNormalization())\n# model.add(keras.layers.Activation('relu'))\n# model.add(keras.layers.Dropout(0.5))\n\n# # model.add(keras.layers.Dense(100, activation='softmax'))\n# model.add(keras.layers.Dense(3, activation='softmax'))\n# from keras import optimizers\n# #sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)#10\n# #adam=optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)#8\n# # 3. Model compile\n# model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy'])","b0358356":"print(model.summary())","0bda8d10":"# 4. Train model\nhist = model.fit(x=x_train, y=y_train, epochs=15,batch_size=512,validation_data=(x_test,y_test))\n#hist = model.fit(x=train_data_features_1, y=y_train, epochs=4,batch_size=512)\n# 5. Traing process\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfig, loss_ax = plt.subplots()\n\nacc_ax = loss_ax.twinx()\n\nloss_ax.set_ylim([0.0, 1.0])\nacc_ax.set_ylim([0.0, 1.0])\n\nloss_ax.plot(hist.history['categorical_accuracy'], 'y', label='train accuracy')\nacc_ax.plot(hist.history['val_categorical_accuracy'], 'b', label='validation accuracy')\n\nloss_ax.set_xlabel('epoch')\nloss_ax.set_ylabel('accuracy')\nacc_ax.set_ylabel('val_accuracy')\n\nloss_ax.legend(loc='upper left')\nacc_ax.legend(loc='lower left')\n\nplt.show()\n\n# 6. Evaluation\nloss_and_metrics = model.evaluate(x_test, y_test, batch_size=512)\nprint('loss_and_metrics : ' + str(loss_and_metrics))\n\nsub_preds_deep = model.predict(x_test,batch_size=512)\nresultList=[]\nfor result in sub_preds_deep:\n    resultList.append(np.argmax(result)) \ncm=confusion_matrix(y_pred=resultList, y_true=np.argmax(y_test,axis=1))\nplt.figure(figsize = (10,7))\ndf_cm = pd.DataFrame(cm, index = [\"Negative\",\"Neutral\",\"Positive\"],\n                  columns =  [\"Negative\",\"Neutral\",\"Positive\"])\nsn.heatmap(df_cm, annot=True,fmt='g',cmap=\"Blues\")","0199d707":"*When we investigate on different conditions, we can find some obvious mistakes for conditions:3<span> uses found..., not useful and need to be deleted;meanwhile for not listed and others, should be deleted as well*","b21eea6f":"Clustering","e126cda4":"*Can see missing data for column condition(testing and training data both), need to delete*","afda2c2a":"*Select 3 conditions to study*","92d39d82":"Data Importing and Visualisation","80095d6e":"**Modelling**","41b5c4da":"unigram","4f090450":"Bi-gram","5f2aa29c":"*uniqueID are unique among all data rows, can ignore, so we remove column uniqueID*","d65c5f3a":"Overall","c1295914":"**Side Effect Analysis**","f6cf3545":"Tri-gram","748a7899":"Type1:","44350c0f":"**Data Exploration**","030eafde":"*word cloud*","347a8b11":"Type2:"}}