{"cell_type":{"cb7e78ea":"code","18afdcd4":"code","2c4b0bca":"code","e53a5d10":"code","0eccd9dd":"code","3bc5f008":"code","8dd698fd":"code","a030ece6":"code","93e2c90b":"code","40664183":"code","00fc5de0":"code","9ea3d551":"code","17f48419":"code","8260c389":"code","591acf00":"code","f511ad6f":"code","0b6c3cab":"code","90dad3ad":"code","f4117bf9":"code","fa3d0d7c":"code","bf5ffae4":"code","628e989b":"code","ae849224":"code","6ddf723c":"code","828e2a63":"code","13200ffc":"code","6f0f979f":"code","dd94c04d":"code","ba59b84c":"code","aa87796a":"code","ea178f1c":"code","ef68140f":"code","db6804e6":"code","cca99a8b":"code","4d578bcb":"code","f1c38cea":"code","d4f58b36":"code","b5b363d3":"code","b6b91b4b":"code","6a54fff7":"code","85a7f614":"code","eca4b709":"code","68bac5f2":"code","dabd0a74":"code","fd261a97":"code","e0954680":"code","20a49202":"code","aa693ff7":"code","238096a6":"code","b90df1b6":"code","6acae0d8":"code","c0f99725":"code","a43a1d41":"code","5d87287b":"code","30c2f961":"code","85321fc8":"code","1b62a22c":"code","55f77a09":"code","13c5b4b1":"code","3303d28a":"code","6f58b78f":"code","55ec11c1":"code","ef1083f3":"code","40524673":"code","556b1648":"code","7d49c84f":"code","e694cd64":"code","88eab57a":"code","8344bf09":"code","032b673f":"code","97979dca":"code","7c33d1da":"code","c83d97e7":"code","e69b543c":"code","4c48087b":"code","d115f9d2":"code","0af13227":"code","23ef9c26":"code","c2d032b6":"code","42d899ce":"code","19869caa":"code","4eb81fe0":"markdown","9efd33dc":"markdown","86e25932":"markdown","07aca1f2":"markdown","cef60025":"markdown","6c4e3881":"markdown","893f56f9":"markdown","d3dd19c8":"markdown","93130865":"markdown"},"source":{"cb7e78ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","18afdcd4":"import catboost\nprint('catboost version:', catboost.__version__)\nfrom catboost import CatBoostClassifier \n#!pip install --upgrade scikit-learn\n#!pip install --upgrade scikit-learn\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import learning_curve\nfrom pprint import pprint\nfrom time import time\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport  nbconvert\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nimport math as mat\nfrom sklearn import metrics\nfrom sklearn.datasets import make_classification \nimport random\nfrom sklearn.metrics import roc_curve  \nfrom sklearn.metrics import roc_auc_score  \nimport random as rd\nfrom sklearn import linear_model\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_score\nimport matplotlib.pyplot as plt\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom sklearn.metrics import confusion_matrix\nimport  nbconvert\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import precision_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import RadiusNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import datasets\nfrom pandas.api.types import CategoricalDtype\nfrom sklearn.neighbors import RadiusNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.datasets import make_classification\nimport pyarrow.parquet as pq\nimport os\nimport pyarrow as pa\nimport math\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import precision_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.datasets import make_hastie_10_2\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nfrom sklearn.metrics import recall_score\nfrom scipy.cluster import hierarchy as sch\nfrom scipy.spatial import distance_matrix \npd.set_option('max_columns',1000)\nfrom sklearn import model_selection   \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier \nfrom xgboost import XGBClassifier\nimport catboost\n\n\nfrom sklearn import ensemble\nfrom sklearn import model_selection\n\n","2c4b0bca":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_train.head(6)","e53a5d10":"df_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf_test.head(6)","0eccd9dd":"df_test.shape","3bc5f008":"df_ids = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ndf_ids.head(6)","8dd698fd":"df_ids.shape","a030ece6":"df_train.head(6)","93e2c90b":"df_train.shape","40664183":"df_train.isnull().sum()","00fc5de0":"for col in df_train.columns.tolist():\n    w = df_train[col].value_counts()\n    print(w)","9ea3d551":"plt.figure(figsize=(9,9))\nsns.catplot(x=\"Survived\", kind=\"count\", palette=\"ch:.25\", data=df_train)\nplt.show()","17f48419":"print(df_train.Survived.value_counts(normalize=True)*100)\nprint('0 no survived, 1 survived')","8260c389":"plt.figure(figsize=(6,6))\nsns.catplot(x=\"Survived\", y=\"Age\", data=df_train)","591acf00":"plt.figure(figsize=(9,9))\nsns.catplot(x=\"Pclass\", kind=\"count\", palette=\"ch:.25\", data=df_train)\n#plt.xticks(rotation=45)\nplt.show()","f511ad6f":"plt.figure(figsize=(6,6))\nsns.catplot(x=\"Cabin\", y=\"Fare\", data=df_train)","0b6c3cab":"plt.figure(figsize=(9,9))\nsns.catplot(x=\"Cabin\", kind=\"count\", palette=\"ch:.25\", data=df_train)\n#plt.xticks(rotation=45)\nplt.show()","90dad3ad":"df_train.Cabin.value_counts(dropna=False)","f4117bf9":"sns.catplot(y=\"Pclass\", hue=\"Survived\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",data=df_train)","fa3d0d7c":"df_train.Age.hist()","bf5ffae4":"df_train.info()","628e989b":"sub_df = df_train[['PassengerId','Pclass','Sex','Age','SibSp','Parch',\n                  'Fare','Cabin','Embarked','Survived']]\n\nsub_df","ae849224":"sub_df['Age'].fillna(sub_df['Age'].median(), inplace=True)\nsub_df","6ddf723c":"sub_df.Cabin.value_counts()","828e2a63":"sub_df['Cabin'] = sub_df['Cabin'].replace(np.nan,'nodata')\nsub_df","13200ffc":"sub_df['Embarked'] = sub_df['Embarked'].replace(np.nan,'nodata')\nsub_df","6f0f979f":"sub_df.set_index('PassengerId', inplace=True)","dd94c04d":"sub_df.head()","ba59b84c":"sub_df.dtypes","aa87796a":"tab = sub_df.dtypes== object\ntab","ea178f1c":"catevars = [i for i in tab.index if tab[i] == True]\ncatevars","ef68140f":"# weights class\nprint(df_train.Survived.value_counts(normalize=True))","db6804e6":"target = sub_df[['Survived']]\ntarget","cca99a8b":"X = sub_df[[i for i in sub_df.columns.tolist() if i!= 'Survived']]\nX","4d578bcb":"X.isnull().sum()","f1c38cea":"X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2, \n                                                    stratify = target, random_state=1)","d4f58b36":"                                                  \nparams = {'iterations':10000,\n        'learning_rate':0.01,\n        'cat_features':catevars,\n        'depth':9,\n        'eval_metric':'Recall',\n        'verbose':200,\n        'od_type':\"Iter\", # overfit detector\n        'od_wait':800, # most recent best iteration to wait before stopping\n        'random_seed': 1,\n        'leaf_estimation_method':'Newton',\n        'score_function':'NewtonL2',\n         'task_type':'GPU',\n          'l2_leaf_reg':0.075\n          }\n\ncat_model2 = CatBoostClassifier(**params, class_weights=[0.383838,0.616162] )\ncat_model2.fit(X_train, y_train,   \n          eval_set=(X_test, y_test), \n          use_best_model=True, # True if we don't want to save trees created after iteration with the best validation score\n          plot=True  \n         );","b5b363d3":"pred_cat2 = cat_model2.predict(X_test)\npred_cat2\n\nprint(classification_report(y_test,pred_cat2))","b6b91b4b":"df_test","6a54fff7":"sub_df_test = df_test[['PassengerId','Pclass','Sex','Age','SibSp','Parch',\n                  'Fare','Cabin','Embarked']]\n\nsub_df_test","85a7f614":"sub_df_test['Cabin'] = sub_df_test['Cabin'].replace(np.nan,'nodata')\nsub_df_test","eca4b709":"sub_df_test['Embarked'] = sub_df_test['Embarked'].replace(np.nan,'nodata')\nsub_df_test","68bac5f2":"sub_df_test.set_index('PassengerId', inplace=True)","dabd0a74":"df_prediccion =sub_df_test.copy()","fd261a97":"df_prediccion","e0954680":"pred_cat_test = cat_model2.predict(df_prediccion)\npred_cat_test","20a49202":"df_ids['pred']=pred_cat_test\ndf_ids","aa693ff7":"print(classification_report(df_ids['Survived'],df_ids['pred']))","238096a6":"df_fin = df_ids[['PassengerId']]\ndf_fin","b90df1b6":"df_fin['Survived'] = pred_cat_test\ndf_fin","6acae0d8":"#df_fin.to_csv('result_titanic.csv',index=False)","c0f99725":"### Getting a better model ","a43a1d41":"df_ids = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ndf_ids.head(6)","5d87287b":"X2 = X.copy()\nX2","30c2f961":"X.dtypes","85321fc8":"X2.drop(['Cabin'],axis=1, inplace=True)\nX2","1b62a22c":"from sklearn.preprocessing import OrdinalEncoder\n\nemb = list(X2.Embarked.unique())\n\nencoder = OrdinalEncoder(categories=[emb])\n\n# Ajustamos el codificador con la variable education y la transformamos\nencoder.fit(X2[[\"Embarked\"]])\nX2[\"Embarked\"] = encoder.transform(X2[[\"Embarked\"]])\nX2.head(2)","55f77a09":"X2['Sex'] =[ 1 if i =='male' else 0 for i in X2.Sex]\nX2","13c5b4b1":"X_train, X_test, y_train, y_test = train_test_split(X2, target, test_size=0.2, \n                                                    stratify = target, random_state=1)","3303d28a":"X_train","6f58b78f":"                                               \nparams = {'iterations':10000,\n        'learning_rate':0.095,\n        'depth':12,\n        'eval_metric':'Recall',\n        'verbose':200,\n        'od_type':\"Iter\", # overfit detector\n        'od_wait':800, # most recent best iteration to wait before stopping\n        'random_seed': 1,\n        'leaf_estimation_method':'Newton',\n        'score_function':'NewtonL2',\n         'task_type':'GPU',\n          'l2_leaf_reg':2\n          }\n\ncat_model3 = CatBoostClassifier(**params, class_weights=[0.383838,0.616162] )\ncat_model3.fit(X_train, y_train,   \n          eval_set=(X_test, y_test), \n          use_best_model=True, # True if we don't want to save trees created after iteration with the best validation score\n          plot=True  \n         );","55ec11c1":"pred_cat3 = cat_model3.predict(X_test)\npred_cat3\n\nprint(classification_report(y_test,pred_cat3))","ef1083f3":"# !pip install lightgbm\nfrom lightgbm import LGBMClassifier\n\nclf_lgbm = LGBMClassifier(objective='binary', random_state=16, \n                          class_weight ='balanced', learning_rate =0.085,max_depth=15,n_estimator=50000, n_jobs=-1,\n                         num_leaves =70, reg_lambda =0.5,is_unbalance=True)","40524673":"%%time\nahuste_lgbm = clf_lgbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='binary')","556b1648":"print(classification_report(y_test, ahuste_lgbm.predict(X_test)))","7d49c84f":"eclf = VotingClassifier(estimators=[('cat', cat_model2), ('lgnm', ahuste_lgbm)], voting='soft')\neclf.fit(X_train, y_train)\n","e694cd64":"y_val_pred = eclf.predict(X_test)\nprint(classification_report(y_test, y_val_pred))","88eab57a":"df_test2 = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_test2","8344bf09":"sub_df_test2 = df_test2[['PassengerId','Pclass','Sex','Age','SibSp','Parch',\n                  'Fare','Embarked']]\n\nsub_df_test2","032b673f":"sub_df_test2['Embarked'] = sub_df_test2['Embarked'].replace(np.nan,'nodata')\nsub_df_test2","97979dca":"sub_df_test2['Age'].fillna(sub_df_test2['Age'].median(), inplace=True)\nsub_df_test2","7c33d1da":"sub_df_test2.set_index('PassengerId', inplace=True)","c83d97e7":"sub_df_test2['Sex'] =[ 1 if i =='male' else 0 for i in sub_df_test2.Sex]\nsub_df_test2","e69b543c":"from sklearn.preprocessing import OrdinalEncoder\n\nemb = list(sub_df_test2.Embarked.unique())\n\nencoder = OrdinalEncoder(categories=[emb])\n\n# Ajustamos el codificador con la variable education y la transformamos\nencoder.fit(sub_df_test2[[\"Embarked\"]])\nsub_df_test2[\"Embarked\"] = encoder.transform(sub_df_test2[[\"Embarked\"]])\nsub_df_test2.head(2)","4c48087b":"pred_cat_test3 = cat_model3.predict(sub_df_test2)\npred_cat_test3","d115f9d2":"df_ids['pred2']=pred_cat_test3\ndf_ids","0af13227":"print(classification_report(df_ids['Survived'],df_ids['pred2']))","23ef9c26":"df_ids.drop(['Survived'], axis=1, inplace=True)","c2d032b6":"df_ids.rename(columns={'pred2':'Survived'}, inplace=True)","42d899ce":"df_ids","19869caa":"df_ids.to_csv('result_titanic2.csv',index=False)","4eb81fe0":"### FIRST EDA TITANIC","9efd33dc":"# data to explore\n\n","86e25932":"### MODELO LIGHTGBM...","07aca1f2":"## prediccion en test...","cef60025":"### SEGUNDA PREDICCION...","6c4e3881":"## MODELO 2..","893f56f9":"### PREDICCION VERSION 2...","d3dd19c8":"### DATA TYPES...","93130865":"### train set test set..."}}