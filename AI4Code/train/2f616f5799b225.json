{"cell_type":{"98500c34":"code","f8b9bc62":"code","031f50f4":"code","e3811b71":"code","facd9825":"code","3bfa92de":"code","b7448a42":"code","d308d1a2":"code","a007941b":"code","3e518ca2":"code","a5d35f75":"code","5e529fb8":"code","80fd48db":"code","10d84db0":"code","cc54ec2f":"code","d262b7b9":"code","3c7b1b5d":"code","f8af8a7e":"code","5f2c912f":"code","7ae50aa4":"code","ce4e23b4":"code","08652756":"code","ef64e665":"code","f22cab88":"code","e2ee8d3b":"markdown","8b388fb8":"markdown","19be162b":"markdown","50ebe9dd":"markdown","4a50578f":"markdown","1f291d6c":"markdown","d12227fc":"markdown","a2436d49":"markdown"},"source":{"98500c34":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom os import path\nfrom PIL import Image\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\n\n# Importing TextBlob\nfrom textblob import TextBlob","f8b9bc62":"# Read csv\ntweets = pd.read_csv(\"..\/input\/pfizer-vaccine-tweets\/vaccination_tweets.csv\")\n\ntweets.head()","031f50f4":"tweets.dtypes","e3811b71":"tweets.info()","facd9825":"tweets.describe()","3bfa92de":"# Let's see the length of the tweets\nseq_length = [len(i) for i in tweets['text']]\n\npd.Series(seq_length).hist(bins = 25)","b7448a42":"# A bit of cleaning\n\n# remove special characters from text column\ntweets.text = tweets.text.str.replace('[#,@,&]', '')\n#Remove twitter handlers\ntweets.text = tweets.text.str.replace('@[^\\s]+','')\n#Remove digits\ntweets.text = tweets.text.str.replace(' \\d+ ','')\n# remove multiple spaces with single space\ntweets.text = tweets.text.str.replace(\"http\\S+\", \"\")\n# remove multiple spaces with single space\ntweets.text = tweets.text.str.replace('\\s+', ' ')\n#remove all single characters\ntweets.text = tweets.text.str.replace(r'\\s+[a-zA-Z]\\s+', '')","d308d1a2":"# Get stopwords\n# Define nltk stopwords in english\nstop_words = stopwords.words('english')\nstop_words.extend(['ha', 'wa', '-'])\n\n# Get a string of tweets \ntweet_text = \",\".join(review.lower() for review in tweets.text if 'covid' not in review)\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_font_size=50, \n                      max_words=100, \n                      stopwords=stop_words,\n                      scale=5,\n                      background_color=\"white\").generate(tweet_text)\n\n# Display the generated image:\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Most repeated words in tweets',fontsize=15)\nplt.show()","a007941b":"# lemmatize text column by using a lemmatize function\ndef lemmatize_text(text):\n    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text.lower())]\n\n\n# Initialize the Lemmatizer and Whitespace Tokenizer\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\n# Lemmatize words\ntweets['lemmatized'] = tweets.text.apply(lemmatize_text)\ntweets['lemmatized'] = tweets['lemmatized'].apply(lambda x: [word for word in x if word not in stop_words])\n\n# use explode to expand the lists into separate rows\nwf_tweets = tweets.lemmatized.explode().to_frame().reset_index(drop=True)\n\n# plot dfe\nsns.countplot(x='lemmatized', data=wf_tweets, order=wf_tweets.lemmatized.value_counts().iloc[:10].index)\nplt.xlabel('Most common used words')\nplt.ylabel('Frequency [%]')\nplt.xticks(rotation=70)\n","3e518ca2":"MostUsedTweets = tweets.hashtags.value_counts().sort_values(ascending=False)[:5]\ncolors = ['lightcoral', 'lightskyblue', 'yellowgreen', 'grey', 'orange']\nexplode = (0.1, 0.2, 0.1, 0.1, 0.1) \n\n# Wedge properties \nwp = { 'linewidth' : 0.5, 'edgecolor' : \"red\" }\n\n# Creating autocpt arguments \ndef func(pct, allvalues): \n    absolute = int(pct \/ 100.*np.sum(allvalues)) \n    return \"{:.1f}%\\n({:d} g)\".format(pct, absolute) \n  \n# Creating the plot \nfig, ax = plt.subplots(figsize =(10, 7)) \nwedges, texts, autotexts = ax.pie(MostUsedTweets,  \n                                  autopct = lambda pct: func(pct, MostUsedTweets), \n                                  explode = explode,  \n                                  labels = MostUsedTweets.keys(), \n                                  shadow = True, \n                                  colors = colors, \n                                  startangle = 90, \n                                  wedgeprops = wp, \n                                  textprops = dict(color =\"black\")) \n  \n# Adding legend \nax.legend(wedges, MostUsedTweets.keys(), \n          title =\"Most used tweets\", \n          loc =\"center left\", \n          bbox_to_anchor =(1, 0, 0.5, 1)) \n\n\nplt.setp(autotexts, size=9, weight=\"bold\") \nax.set_title(\"Most used tweets\") \nplt.axis('equal')\nplt.show()","a5d35f75":"\ncmap = cm.get_cmap('Spectral') \n\ncountries=tweets['source'].value_counts().sort_values(ascending=False)[:5].plot(\n    kind = 'bar', \n    cmap=cmap, \n    edgecolor='None')","5e529fb8":"# Get countries which post more tweets\ncmap = cm.get_cmap('Spectral') \n\ncountries=tweets['user_location'].value_counts().sort_values(ascending=False)[:5].plot(\n    kind = 'barh', \n    cmap=cmap, \n    edgecolor='None')","80fd48db":"import geopandas\ncities = geopandas.read_file(geopandas.datasets.get_path('naturalearth_cities'))\n\nax = cities.plot()\n\nfor x, y, label in zip(cities.geometry.x, cities.geometry.y, cities.name):\n    ax.annotate(label, xy=(x, y), xytext=(3, 3), textcoords=\"offset points\")","10d84db0":"tweets['polarity'] = tweets.text.apply(lambda x: TextBlob(x).polarity)\ntweets['subjectivity'] = tweets.text.apply(lambda x: TextBlob(x).subjectivity)\n\ntweets.head()","cc54ec2f":"tweets['sentiment'] = np.where(tweets.polarity > 0, 'positive', \n                                 np.where(tweets.polarity < 0, 'negative', 'neutral'))\ntweets.head()","d262b7b9":"# Shows the top 5 tweets with highest polarity scores\ntweets.nlargest(5,'polarity')['text']","3c7b1b5d":"# Shows the top 5 tweets with highest polarity and subjectivity scores\ntweets.nlargest(5, ['polarity', 'subjectivity'])['text']","f8af8a7e":"# Shows the top 5 tweets with lowest polarity scores\ntweets.nsmallest(5,'polarity')['text']","5f2c912f":"# Shows the top 5 tweets with lowest polarity and subjectivity scores\ntweets.nsmallest(5, ['polarity', 'subjectivity'])['text']","7ae50aa4":"tweets['sentiment'].value_counts()","ce4e23b4":"from sklearn.preprocessing import LabelBinarizer\n\n# LabelBinarize sentiment column and merge with tweets DF\nlb = LabelBinarizer()\n\nsentbinarized = lb.fit_transform(tweets['sentiment']).tolist()\n#lb.classes_ # Classes of the LabelBinarizer\n\ndfbinarized = pd.DataFrame(sentbinarized, columns=lb.classes_)\ntweets[dfbinarized.columns] = dfbinarized\ntweets.head()","08652756":"plt.figure(figsize=(15,5))\nplt.title('Distribution Of Sentiments Across Our Tweets',fontsize=12,fontweight='bold')\nsns.kdeplot(tweets['polarity'], label='Polarity', lw=2.5)\nsns.kdeplot(tweets['subjectivity'], label='Subjectivity', lw=2.5)\nplt.xlabel('Polarity|subjetivity Value', fontsize=10)\nplt.ylabel('Density', fontsize=10)\n# Display the generated image:\n\nplt.legend()\nplt.show()","ef64e665":"plt.figure(figsize=(15,5))\nplt.title('CDF Of Sentiments Across Our Tweets',fontsize=12, fontweight='bold')\nsns.kdeplot(tweets['polarity'],cumulative=True, label='Polarity',lw=2.5)\nsns.kdeplot(tweets['subjectivity'],cumulative=True, label='Subjectivity',lw=2.5)\nplt.xlabel('Polarity Value', fontsize=10)\nplt.ylabel('Density', fontsize=10)\nplt.legend()\nplt.show()","f22cab88":"# Sorting and feature engineering dates\ntweets = tweets.sort_values(by='date')\ntweets=tweets.copy()\ntweets['date'] = pd.to_datetime(tweets['date']).dt.date\n\ntweets['year']         = pd.DatetimeIndex(tweets['date']).year\ntweets['month']        = pd.DatetimeIndex(tweets['date']).month\ntweets['day']          = pd.DatetimeIndex(tweets['date']).day\ntweets['day_of_year']  = pd.DatetimeIndex(tweets['date']).dayofyear\n","e2ee8d3b":"# Check top 5 most used hashtags","8b388fb8":"# Word frequency lemmatized","19be162b":"# Sentiment analysis with TextBlob\n\n[TextBlob](https:\/\/textblob.readthedocs.io\/en\/dev\/) is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.","50ebe9dd":"# Top 10 tweet posts countries ","4a50578f":"# EDA of Sentiment analysis result","1f291d6c":"# WordCloud","d12227fc":"In TextBlob, based on the polarity and subjectivity, you determine whether it is a positive text or negative or neutral. For TextBlob, if  polarity is > 0, it is considered positive, if polarity < 0 is considered negative and if polarity == 0 is considered as neutral.","a2436d49":"# Tweets source"}}