{"cell_type":{"70c26f81":"code","8c1c89e2":"code","ce9fecf8":"code","c068a32e":"code","31e68feb":"code","02cfdc07":"code","9b73e7d1":"code","63e3cbfe":"code","53733ae6":"code","975de28b":"code","07f09b01":"code","16bb58b1":"code","149959ab":"code","65835fd2":"code","f2681f06":"code","e8fe8050":"code","5c12ded5":"code","e9dd2c6a":"code","54b3bb06":"code","29c3c6b1":"code","4700a8ee":"code","16675214":"code","7da64735":"code","fef1502a":"code","7617a628":"code","847e4123":"code","bb228058":"code","8d405e55":"code","3d709be9":"markdown","d17d3c7a":"markdown","a6fbfd83":"markdown"},"source":{"70c26f81":"# Import Libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchtext.datasets import Multi30k #German to English dataset\nfrom torchtext.data import Field, BucketIterator\nimport numpy as np\nimport spacy\nimport random\nfrom torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\nimport torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\nimport sys","8c1c89e2":"!python -m spacy download de","ce9fecf8":"# Loading Tokeniser in german and English\nspacy_ger = spacy.load('de')\nspacy_eng = spacy.load('en')","c068a32e":"# Tokenization of German Language\ndef tokenize_ger(text):\n    return [tok.text for tok in spacy_ger.tokenizer(text)]","31e68feb":"# Tokenization of English Language\n\ndef tokenize_eng(text):\n    return [tok.text for tok in spacy_eng.tokenizer(text)]","02cfdc07":"# Applyling Tokenization , lowercase and special Tokens for preprocessing\ngerman = Field(tokenize = tokenize_ger,lower = True,init_token = '<sos>',eos_token = '<eos>')","9b73e7d1":"english = Field(tokenize = tokenize_eng,lower = True,init_token = '<sos>',eos_token = '<eos>')","63e3cbfe":"# Dwonloading Dataset and storing them\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(\".de\", \".en\"), fields=(german, english)\n)","53733ae6":"# Creating vocabulary in each language\ngerman.build_vocab(train_data,max_size = 10000,min_freq = 2)\nenglish.build_vocab(train_data,max_size = 10000,min_freq = 2)\n","975de28b":"\n# Defining the Encoder part of the model\nclass Encoder(nn.Module):\n    \n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        # x shape: (seq_length, N) where N is batch size\n\n        embedding = self.dropout(self.embedding(x))\n        # embedding shape: (seq_length, N, embedding_size)\n\n        outputs, (hidden, cell) = self.rnn(embedding)\n        # outputs shape: (seq_length, N, hidden_size)\n\n        return hidden, cell","07f09b01":"# Defining the Decoder part\n\nclass Decoder(nn.Module):\n    def __init__(\n        self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden, cell):\n        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n        # is 1 here because we are sending in a single word and not a sentence\n        x = x.unsqueeze(0)\n\n        embedding = self.dropout(self.embedding(x))\n        # embedding shape: (1, N, embedding_size)\n\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        # outputs shape: (1, N, hidden_size)\n\n        predictions = self.fc(outputs)\n\n        # predictions shape: (1, N, length_target_vocabulary) to send it to\n        # loss function we want it to be (N, length_target_vocabulary) so we're\n        # just gonna remove the first dim\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell","16bb58b1":"# Defining the complete model\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = source.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(english.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n\n        hidden, cell = self.encoder(source)\n\n        # Grab the first input to the Decoder which will be <SOS> token\n        x = target[0]\n\n        for t in range(1, target_len):\n            # Use previous hidden, cell as context from encoder at start\n            output, hidden, cell = self.decoder(x, hidden, cell)\n\n            # Store next output prediction\n            outputs[t] = output\n\n            # Get the best word the Decoder predicted (index in the vocabulary)\n            best_guess = output.argmax(1)\n\n            # With probability of teacher_force_ratio we take the actual next word\n            # otherwise we take the word that the Decoder predicted it to be.\n            # Teacher Forcing is used so that the model gets used to seeing\n            # similar inputs at training and testing time, if teacher forcing is 1\n            # then inputs at test time might be completely different than what the\n            # network is used to. This was a long comment.\n            x = target[t] if random.random() < teacher_force_ratio else best_guess\n\n        return outputs","149959ab":"# Hyperparameters\nnum_epochs = 20\nlearning_rate = 0.001\nbatch_size = 64\n","65835fd2":"# Model hyperparameters\nload_model = False\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\ninput_size_encoder = len(german.vocab)\ninput_size_decoder = len(english.vocab)\noutput_size = len(english.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\n\nhidden_size = 1024\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n","f2681f06":"# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f'runs\/Loss_plot')\nstep = 0","e8fe8050":"train_iterator, validation_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n     batch_size = batch_size, sort_within_batch = True, \n     sort_key = lambda x:len(x.src),\n     device = device)","5c12ded5":"encoder_net = Encoder(input_size_encoder, \n                      encoder_embedding_size,\n                      hidden_size,num_layers, \n                      enc_dropout).to(device)\n\n\ndecoder_net = Decoder(input_size_decoder, \n                      decoder_embedding_size,\n                      hidden_size,output_size,num_layers, \n                      dec_dropout).to(device)","e9dd2c6a":"model = Seq2Seq(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","54b3bb06":"pad_idx = english.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n\n","29c3c6b1":"def translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]","4700a8ee":"def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n","16675214":"def load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])","7da64735":"if load_model:\n    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n","fef1502a":"sentence = \"Cristiano Ronaldo ist ein gro\u00dfartiger Fu\u00dfballspieler mit erstaunlichen F\u00e4higkeiten und Talenten.\"\n","7617a628":"for epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} \/ {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, german, english, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        # Get input and targets and get to cuda\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n        # way that we have output_words * batch_size that we want to send in into\n        # our cost function, so we need to do some reshapin. While we're at it\n        # Let's also remove the start token while we're at it\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1","847e4123":"def bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)","bb228058":"import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\nimport sys","8d405e55":"\nscore = bleu(test_data[1:100], model, german, english, device)\nprint(f\"Bleu score {score*100:.2f}\")","3d709be9":"Hey Guys , I have made a Tutorial for Seq2Seq Machine Translation from scratch. Hope you like it. Upvote :)","d17d3c7a":"![image.png](attachment:image.png)","a6fbfd83":"## Preprocessing of Text"}}