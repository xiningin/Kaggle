{"cell_type":{"1fc3cab1":"code","de4d4919":"code","0b21803f":"code","054f0a54":"code","bdf2c65b":"code","5b4eb8c1":"code","c4ca50fa":"code","a9a0ef33":"code","9d228f6b":"code","98c6b380":"code","4f47629c":"code","344358a4":"code","f03c2ed2":"code","81534005":"code","56af318c":"code","f4b1cb27":"code","a9e396e7":"code","0e29ba5d":"markdown"},"source":{"1fc3cab1":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport os\n\nbase_dir = \"..\/input\"\n\nprint(os.listdir(base_dir))","de4d4919":"X_train = pd.read_csv(f'{base_dir}\/X_train.csv')\ny_train = pd.read_csv(f'{base_dir}\/y_train.csv')","0b21803f":"USE_CUDA = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n#device = torch.device(\"cpu\")\ndevice","054f0a54":"series_id_to_surface = {}\nsurface_to_series_id = {}\n\nfor row in y_train.iterrows():\n    surface_to_series_id.setdefault(row[1].surface, []).append(row[1].series_id)\n    series_id_to_surface[row[1].series_id] = row[1].surface\nall_surfaces = list(surface_to_series_id.keys())\n\nn_features = X_train.drop(['row_id', 'series_id', 'measurement_number'], axis=1).shape[1]\nn_surfaces = len(all_surfaces)\nn_hidden = 128\nn_examples = len(X_train['series_id'].unique())\nn_sequence = len(X_train['measurement_number'].unique())","bdf2c65b":"import random\nfrom sklearn.model_selection import train_test_split\n\ntrain_val_split = int(n_examples * 0.9)\ndataset_IDs = {}\ndataset_IDs['train'], dataset_IDs['eval'] = train_test_split(X_train['series_id'].unique(), test_size=0.05)","5b4eb8c1":"print(f'Training size: {len(dataset_IDs[\"train\"])}')\nprint(f'Validation size: {len(dataset_IDs[\"eval\"])}')","c4ca50fa":"surface_to_surface_tensor = {}\nseries_id_to_tensor = {}\n\nfor i in range(len(all_surfaces)):\n    surface_to_surface_tensor[all_surfaces[i]] = torch.tensor([i], dtype=torch.long).to(device)\n\ndef tensor_from_series_id(series_id, df):\n    series = df[df['series_id'] == series_id].drop(['row_id', 'series_id', 'measurement_number'], axis=1)\n    series_tensor = torch.from_numpy(series.values).view(series.shape[0], 1, -1).float().to(device)\n    return series_tensor\n    \nfor series_id in X_train['series_id'].unique():\n    series_id_to_tensor[series_id] = tensor_from_series_id(series_id, X_train)\n    \ndef random_training_example(phase):\n    series_id = random.choice(dataset_IDs[phase])\n    surface = series_id_to_surface[series_id]\n    surface_tensor = surface_to_surface_tensor[surface]\n    series_tensor = series_id_to_tensor[series_id]\n    return series_id, surface, series_tensor, surface_tensor","a9a0ef33":"dataset_targets = {}\ndataset_targets['train'] = [series_id_to_surface[series_id] for series_id in dataset_IDs['train']]\ndataset_targets['eval'] = [series_id_to_surface[series_id] for series_id in dataset_IDs['eval']]","9d228f6b":"class LSTMSurface(nn.Module):\n    def __init__(self, series_dim, hidden_dim, surface_dim):\n        super(LSTMSurface, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.lstm = nn.LSTM(series_dim, hidden_dim, num_layers=2, dropout=0.2)\n        self.hidden2surface = nn.Linear(hidden_dim, surface_dim)\n\n    def forward(self, series):\n        lstm_out, _ = self.lstm(series)\n        surface_space = self.hidden2surface(lstm_out)\n        surface_scores = F.log_softmax(surface_space, dim=2)\n        return surface_scores[-1]","98c6b380":"def surface_from_output(output):\n    top_n, top_i = output.topk(1)\n    surface_i = top_i[0].item()\n    return all_surfaces[surface_i], surface_i","4f47629c":"model = LSTMSurface(n_features, n_hidden, n_surfaces).to(device)\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters())","344358a4":"%matplotlib inline\nfrom IPython import display\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport statistics\nimport copy\n\nn_epochs = 200000\nsave_loss_every = 10\nprint_every = 1000\nplot_every = 5000\nlosses = {'train': [], 'eval': []}\naccs = {'train': [], 'eval': []}\n\nall_losses = {'train': [], 'eval': []}\n\nbest_model_wts = copy.deepcopy(model.state_dict())\nbest_acc = 0.0\n        \nfig = plt.figure()\n\nfor epoch in range(1, n_epochs + 1):\n    for phase in ['train', 'eval']:\n        optimizer.zero_grad()\n\n        if phase == 'train':\n            model.train()\n        else:\n            model.eval()\n        \n        _, _, inputs, targets = random_training_example(phase)\n        \n        with torch.set_grad_enabled(phase == 'train'):\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            _, preds = torch.max(outputs, 1)\n\n            if phase == 'train':\n                loss.backward()\n                optimizer.step()\n        \n        if epoch % save_loss_every == 0:\n            running_loss = loss.item() * inputs.size(1)\n            running_corrects = torch.sum(preds == targets.data)\n            epoch_loss = running_loss \/ len(targets)\n            epoch_acc = running_corrects.double() \/ len(targets)\n            losses[phase].append(epoch_loss)\n            accs[phase].append(epoch_acc.item())\n        \n        if phase == 'eval' and epoch % print_every == 0:\n            training_loss = statistics.mean(losses['train'])\n            validation_loss = statistics.mean(losses['eval'])\n            all_losses['train'].append(training_loss)\n            all_losses['eval'].append(validation_loss)\n            training_acc = statistics.mean(accs['train'])\n            validation_acc = statistics.mean(accs['eval'])\n            if validation_acc > best_acc:\n                best_acc = validation_acc\n                print(f'Best accuracy so far! Acc = {best_acc}')\n                best_model_wts = copy.deepcopy(model.state_dict())\n            print(f'{epoch} {epoch \/ n_epochs:.2%} \/ Train Loss: {training_loss:.4}, Validation Loss: {validation_loss:.4} \/ Train Accuracy: {training_acc:.2%}, Validation Accuracy: {validation_acc:.2%}')\n            losses = {'train': [], 'eval': []}\n            accs = {'train': [], 'eval': []}\n        \n        if phase == 'eval' and epoch % plot_every == 0:\n            plt.plot(all_losses['train'], label='Train loss')\n            plt.plot(all_losses['eval'], label='Validation loss')\n            display.display(plt.gcf())\n            display.clear_output(wait=True)","f03c2ed2":"import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nplt.figure()\nplt.plot(all_losses['train'], label='Train loss')\nplt.plot(all_losses['eval'], label='Validation loss')\nplt.legend()","81534005":"import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nmodel.load_state_dict(best_model_wts)\n\nconfusion = torch.zeros(n_surfaces, n_surfaces)\nn_confusion = 1000\n\ndef evaluate(series_tensor):\n    with torch.no_grad():\n        scores = model(series_tensor)\n        return scores\n\nall_guesses = []\nhits = 0\nfor i in range(n_confusion):\n    series_id, surface, series_tensor, surface_tensor = random_training_example('eval')\n    output = evaluate(series_tensor)\n    guess, guess_i = surface_from_output(output)\n    all_guesses.append(guess)\n    surface_i = all_surfaces.index(surface)\n    confusion[surface_i][guess_i] += 1\n    if surface_i == guess_i:\n        hits += 1\n\naccuracy = hits \/ n_confusion\n\nfor i in range(n_surfaces):\n    confusion[i] = confusion[i] \/ confusion[i].sum()\n\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(confusion.numpy())\nfig.colorbar(cax)\n\nax.set_xticklabels([''] + all_surfaces, rotation=90)\nax.set_yticklabels([''] + all_surfaces)\n\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\nplt.show()\nprint(pd.Series(all_guesses).value_counts())\nprint(f'Validation accuracy = {accuracy:.2%}')","56af318c":"X_test = pd.read_csv(f'{base_dir}\/X_test.csv')\npredictions = []\nfor series_id in X_test['series_id'].unique():\n    series_tensor = tensor_from_series_id(series_id, X_test)\n    output = evaluate(series_tensor)\n    guess, guess_i = surface_from_output(output)\n    predictions.append((series_id, guess))\nsubmission = pd.DataFrame(predictions, columns=['series_id', 'surface'])","f4b1cb27":"print(submission.to_csv(index=False))","a9e396e7":"submission.to_csv('lstm_submission_final.csv', index=False)","0e29ba5d":"**Prepare the training data**"}}