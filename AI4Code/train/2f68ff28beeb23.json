{"cell_type":{"2a30eb7d":"code","96a77e35":"code","5289e154":"code","43ddb185":"code","f58abfc9":"code","306e7f47":"code","39e0cf01":"code","784cebb3":"code","5bc4379e":"code","9568c53e":"code","5ecfb806":"code","08adbf49":"code","30c69b1d":"code","793e7ec7":"code","c110578d":"code","0fe68201":"code","d3648fe0":"code","6d53bd84":"code","954515df":"code","c5118723":"code","cbf86858":"code","747d00b7":"code","b4c1f956":"code","15ad0351":"code","c0b1a1fb":"code","80fc583f":"code","defcf014":"code","3e15e716":"code","6017827d":"code","5f622183":"code","6dba802d":"code","9227d49d":"code","4d9344cb":"code","e92d633c":"code","cb77f6a8":"code","03fbff24":"code","0db01ce2":"code","6721f135":"code","64e71b2f":"code","cb38c82b":"code","af79bdb3":"code","69c5287d":"code","52c057ad":"code","9878db94":"code","8fcf408d":"code","80bba692":"code","5bf3b580":"code","b5f30157":"code","141550f2":"code","72425854":"code","4e45d752":"code","e34833ba":"code","6f1dd73e":"code","d7a70933":"code","1af0a2bf":"code","7cadca95":"code","680c01be":"code","840ccd9b":"code","1b0ec85d":"code","41b93e06":"code","c6b65037":"code","9be42751":"code","ebde7649":"code","e6d66727":"markdown","26ce7468":"markdown","025819d9":"markdown","760f381b":"markdown","d17fcabb":"markdown","1cc22f3d":"markdown","68f71328":"markdown","200db742":"markdown","732b2279":"markdown","cbb4f84d":"markdown","e4095b15":"markdown","94f4f135":"markdown","4c1ef5e1":"markdown","625fb7e2":"markdown","16e5d658":"markdown","6332cc6d":"markdown","1a46783e":"markdown","170beade":"markdown","cdac7207":"markdown","6d892b56":"markdown"},"source":{"2a30eb7d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","96a77e35":"!conda install -y gdcm -c conda-forge","5289e154":"import pydicom\nimport math\nimport PIL\nfrom PIL import Image\nimport numpy as np\nfrom keras import layers\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\nimport scipy\nimport tensorflow as tf\nfrom tqdm import tqdm\n%matplotlib inline","43ddb185":"from scipy.stats import shapiro\nfrom scipy import stats \nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\nimport gc","f58abfc9":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","306e7f47":"BATCH_SIZE = 30\nTRAIN_VAL_RATIO = 0.35\nEPOCHS_M1 = 200\nEPOCHS_M2 = 400\nLR = 0.005\nimSize = 224","39e0cf01":"train_df = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/train.csv')\ntest_df = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv')\nsub_df = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/sample_submission.csv')\nprint(train_df.shape)\nprint(test_df.shape)\ntrain_df.head()","784cebb3":"sub_df['Patient'] = sub_df['Patient_Week'].apply(lambda x: x.split(\"_\", 1)[0])\nsub_df['Weeks'] = sub_df['Patient_Week'].apply(lambda x: x.split(\"_\", 1)[1])\nsub_df.head()","5bc4379e":"plt.hist(train_df['FVC'])","9568c53e":"# https:\/\/machinelearningmastery.com\/a-gentle-introduction-to-normality-tests-in-python\/\n# normality test\nstat, p = shapiro(train_df['FVC'])\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.01\nif p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\nelse:\n    print('Sample does not look Gaussian (reject H0)')","5ecfb806":"# transform training data & save lambda value \n_, fitted_lambda = stats.boxcox(train_df['FVC']) \nprint(\"solved lambda: \", fitted_lambda)","08adbf49":"def BoxCoxTransform(x, lmbda):\n    part1 = x**lmbda\n    part2 = part1-1\n    result = part2\/lmbda\n    return result\n\ndef ReverseBoxCoxTranform(x, lmbda):\n    x = np.where(x<0,0,x)\n    part1 = x*lmbda + 1\n    result = part1**(1\/lmbda)\n    return result","30c69b1d":"fitted_data = BoxCoxTransform(train_df['FVC'], lmbda=fitted_lambda)\nplt.hist(fitted_data)","793e7ec7":"# https:\/\/machinelearningmastery.com\/a-gentle-introduction-to-normality-tests-in-python\/\n# normality test\nstat, p = shapiro(fitted_data)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.01\nif p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\nelse:\n    print('Sample does not look Gaussian (reject H0)')","c110578d":"# show that we can reverse transformed values back to original values\nplt.hist(ReverseBoxCoxTranform(fitted_data, lmbda=fitted_lambda))","0fe68201":"def preprocess_image(image_path, desired_size=imSize):\n    im = pydicom.dcmread(image_path).pixel_array\n    im = Image.fromarray(im, mode=\"L\")\n    im = im.resize((desired_size,desired_size)) \n    im = np.array(im).flatten().astype(np.uint8)\n    return im","d3648fe0":"def process_patient_images(patient_path, imSize=imSize):\n    image_filenames = os.listdir(patient_path)\n    final_array = np.zeros((imSize*imSize), dtype=np.uint8)\n    total_images = len(image_filenames)\n    for image_filename in image_filenames:\n        image_path = patient_path + \"\/\" + image_filename\n        image_arr = preprocess_image(image_path, desired_size=imSize)\n        final_array += image_arr\n    final_array = final_array \/ total_images\n    return final_array","6d53bd84":"# get the number of training images from the target\\id dataset\nN = train_df.shape[0]\n# create an empty matrix for storing the images\nx_train = np.empty((N, imSize*imSize), dtype=np.uint8)\n# loop through the images from the images ids from the target\\id dataset\n# then grab the cooresponding image from disk, pre-process, and store in matrix in memory\nfor i, Patient in enumerate(tqdm(train_df['Patient'])):\n    x_train[i, :] = process_patient_images(\n        f'..\/input\/osic-pulmonary-fibrosis-progression\/train\/{Patient}'\n    )","954515df":"# get the number of training images from the target\\id dataset\nN = test_df.shape[0]\n# create an empty matrix for storing the images\nx_test = np.empty((N, imSize*imSize), dtype=np.uint8)\n# loop through the images from the images ids from the target\\id dataset\n# then grab the cooresponding image from disk, pre-process, and store in matrix in memory\nfor i, Patient in enumerate(tqdm(test_df['Patient'])):\n    x_test[i, :] = process_patient_images(\n        f'..\/input\/osic-pulmonary-fibrosis-progression\/train\/{Patient}'\n    )","c5118723":"# get the number of training images from the target\\id dataset\nN = sub_df.shape[0]\n# create an empty matrix for storing the images\nx_sub = np.empty((N, imSize*imSize), dtype=np.uint8)\n# loop through the images from the images ids from the target\\id dataset\n# then grab the cooresponding image from disk, pre-process, and store in matrix in memory\nfor i, Patient in enumerate(tqdm(sub_df['Patient'])):\n    x_sub[i, :] = process_patient_images(\n        f'..\/input\/osic-pulmonary-fibrosis-progression\/train\/{Patient}'\n    )","cbf86858":"# one hot encoding\ntrain_df['Sex_Male'] = train_df['Sex'].apply(lambda x: 1 if str(x)=='Male' else 0)\ntrain_df['SmokingStatusEx'] = train_df['SmokingStatus'].apply(lambda x: 1 if str(x)=='Ex-smoker' else 0)\ntest_df['Sex_Male'] = test_df['Sex'].apply(lambda x: 1 if str(x)=='Male' else 0)\ntest_df['SmokingStatusEx'] = test_df['SmokingStatus'].apply(lambda x: 1 if str(x)=='Ex-smoker' else 0)","747d00b7":"train_df.head()","b4c1f956":"# patient profile\npatient_profile = train_df.groupby(\"Patient\", as_index=False) \\\n                          .agg({'Percent':'mean', 'Sex_Male':'max', 'SmokingStatusEx':'max', 'Age':'mean', 'Weeks':'min'})\npatient_profile = patient_profile.rename(columns={'Percent':'AvgPercent','Age':'AvgAge','Weeks':'BaseWeek'})","15ad0351":"patient_profile.head()","c0b1a1fb":"# merge profile and create more features\ntrain_df = train_df.merge(patient_profile[[\"Patient\",\"AvgPercent\",\"AvgAge\",\"BaseWeek\"]], on=\"Patient\", how='left')\ntrain_df['RelativeWeek'] = train_df['Weeks'].apply(lambda x: int(x)) - train_df['BaseWeek'].apply(lambda x: int(x))\ntest_df = test_df.merge(patient_profile[[\"Patient\",\"AvgPercent\",\"AvgAge\",\"BaseWeek\"]], on=\"Patient\", how='left')\ntest_df['RelativeWeek'] = test_df['Weeks'].apply(lambda x: int(x)) - test_df['BaseWeek'].apply(lambda x: int(x))\nsub_df = sub_df.merge(patient_profile, on=\"Patient\", how='left')\nsub_df['RelativeWeek'] = sub_df['Weeks'].apply(lambda x: int(x)) - sub_df['BaseWeek'].apply(lambda x: int(x))","80fc583f":"train_df.head()","defcf014":"test_df.head()","3e15e716":"sub_df.head()","6017827d":"# final dataframes and tabular features\n# train\nx_train_tabular_features = train_df[['Weeks','AvgPercent','AvgAge','Sex_Male','SmokingStatusEx','BaseWeek','RelativeWeek']].values\n# test\nx_test_tabular_features = test_df[['Weeks','AvgPercent','AvgAge','Sex_Male','SmokingStatusEx','BaseWeek','RelativeWeek']].values\n# submission\nx_sub_tabular_features = sub_df[['Weeks','AvgPercent','AvgAge','Sex_Male','SmokingStatusEx','BaseWeek','RelativeWeek']].values","5f622183":"# dimensionality reduction\npca = PCA(n_components=100)\npca.fit(x_train)","6dba802d":"x_train = pca.transform(x_train)\nx_test = pca.transform(x_test)\nx_sub = pca.transform(x_sub)","9227d49d":"# merge\nx_train_full = np.concatenate((x_train_tabular_features, x_train), axis=1)\nx_test_full = np.concatenate((x_test_tabular_features, x_test), axis=1)\nx_sub_full = np.concatenate((x_sub_tabular_features, x_sub), axis=1)","4d9344cb":"scaler = StandardScaler()\nscaler.fit(x_train_full)\nx_train_full = scaler.transform(x_train_full)\nx_test_full = scaler.transform(x_test_full)\nx_sub_full = scaler.transform(x_sub_full)","e92d633c":"# squared features for some model flexability\nx_train_full2 = np.square(x_train_full)\nx_test_full2 = np.square(x_test_full)\nx_sub_full2 = np.square(x_sub_full)","cb77f6a8":"# merge\nx_train_full = np.concatenate((x_train_full, x_train_full2), axis=1)\nx_test_full = np.concatenate((x_test_full, x_test_full2), axis=1)\nx_sub_full = np.concatenate((x_sub_full, x_sub_full2), axis=1)","03fbff24":"print(x_train_full.shape)\nprint(x_test_full.shape)\nprint(x_sub_full.shape)","0db01ce2":"# view data\nx_train_full[:2,:]","6721f135":"# This section is commented out, because it doesn't prove useful based on the way the holdout set is designed\n\n# save the data to disk so we can save as a Kaggle Dataset\n# and skip data preprocessing in other scripts\n# filename = 'osic_processed_train_data_v1.pkl'\n# pickle.dump(x_train_full, open(filename, 'wb'))\n# filename = 'osic_processed_test_data_v1.pkl'\n# pickle.dump(x_test_full, open(filename, 'wb'))\n# filename = 'osic_processed_sub_data_v1.pkl'\n# pickle.dump(x_sub_full, open(filename, 'wb'))","64e71b2f":"x_train_fvc, x_val_fvc, y_train_fvc, y_val_fvc = train_test_split(\n    x_train_full, BoxCoxTransform(train_df['FVC'], lmbda=fitted_lambda),\n    test_size=TRAIN_VAL_RATIO, \n    random_state=2020\n)","cb38c82b":"with strategy.scope():\n    # define structure\n    xin = tf.keras.layers.Input(shape=(x_train_full.shape[1], ))\n    xout = tf.keras.layers.Dense(1, activation='linear')(xin)\n    # put it together\n    model1 = tf.keras.Model(inputs=xin, outputs=xout)\n    # compile\n    opt = tf.optimizers.RMSprop(LR)\n    model1.compile(optimizer=opt, loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n# print summary\nmodel1.summary()","af79bdb3":"### define callbacks\nearlystopper = EarlyStopping(\n    monitor='val_mean_squared_error', \n    patience=30,\n    verbose=1,\n    mode='min'\n)\n\nlrreducer = ReduceLROnPlateau(\n    monitor='val_mean_squared_error',\n    factor=.5,\n    patience=10,\n    verbose=1,\n    min_lr=1e-9\n)","69c5287d":"print(\"Fit model on training data\")\nhistory = model1.fit(\n    x_train_fvc,\n    y_train_fvc,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS_M1,\n    validation_data=(x_val_fvc, y_val_fvc),\n    callbacks=[earlystopper,lrreducer]\n)","52c057ad":"history_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()\nhistory_df[['mean_squared_error', 'val_mean_squared_error']].plot()","9878db94":"train_df['FVC_pred1'] = ReverseBoxCoxTranform(model1.predict(x_train_full), lmbda=fitted_lambda)","8fcf408d":"train_df.head()","80bba692":"stats.describe(model1.predict(x_train_full))","5bf3b580":"%%time\n\nbest_confidence = np.zeros(len(train_df))\nBestRandSearchScore = -1000000000\nscorehist = []\nprint(\"running random search...\")\nfor i in range(10000):\n    trial_confidence = np.random.randint(70, 1000, size=len(train_df))\n    train_df['Confidence'] = trial_confidence\n    train_df['sigma_clipped'] = train_df['Confidence'].apply(lambda x: max(x, 70))\n    train_df['diff'] = abs(train_df['FVC'] - train_df['FVC_pred1'])\n    train_df['delta'] = train_df['diff'].apply(lambda x: min(x, 1000))\n    train_df['score'] = -math.sqrt(2)*train_df['delta']\/train_df['sigma_clipped'] - np.log(math.sqrt(2)*train_df['sigma_clipped'])\n    score = train_df['score'].mean()\n    if score>BestRandSearchScore:\n        BestRandSearchScore = score\n        best_confidence = trial_confidence\n        print(\"best confidence values found in round {} with best score of {}\".format(i,score))\n    scorehist.append(BestRandSearchScore)","b5f30157":"plt.plot(scorehist)\nplt.ylabel('best score')\nplt.xlabel('round')\nplt.show()","141550f2":"def md_learning_rate(val):\n    if val == 0:\n        return 10\n    elif val == 1:\n        return 8\n    else:\n        return 5\/np.log(val)","72425854":"%%time\n\nRounds = 100\nbest_md_confidence = np.zeros(len(train_df))\nBestManualDescentScore = -1000000000\nrowScore = -1000000000\ntrain_df['Confidence'] = best_confidence\ntrain_df['diff'] = abs(train_df['FVC'] - train_df['FVC_pred1']) # don't need to compute this every time\ntrain_df['delta'] = train_df['diff'].apply(lambda x: min(x, 1000)) # don't need to compute this every time\nfor j in range(Rounds):\n    for i in range(len(train_df)):\n        originalValue = train_df['Confidence'].iloc[i]\n        # try moving value up\n        train_df['Confidence'].iloc[i] = originalValue + md_learning_rate(j)\n        train_df['sigma_clipped'] = train_df['Confidence'].apply(lambda x: max(x, 70))\n        train_df['score'] = -math.sqrt(2)*train_df['delta']\/train_df['sigma_clipped'] - np.log(math.sqrt(2)*train_df['sigma_clipped'])\n        scoreup = train_df['score'].mean()\n        # try moving value down\n        train_df['Confidence'].iloc[i] = originalValue - md_learning_rate(j)\n        train_df['sigma_clipped'] = train_df['Confidence'].apply(lambda x: max(x, 70))\n        train_df['score'] = -math.sqrt(2)*train_df['delta']\/train_df['sigma_clipped'] - np.log(math.sqrt(2)*train_df['sigma_clipped'])\n        scoredown = train_df['score'].mean()\n        if scoreup>scoredown:\n            train_df['Confidence'].iloc[i] = originalValue + md_learning_rate(j)\n            rowScore = scoreup\n        else:\n            train_df['Confidence'].iloc[i] = originalValue - md_learning_rate(j)\n            rowScore = scoredown\n    if rowScore>BestManualDescentScore:\n        BestManualDescentScore = rowScore\n        best_md_confidence = train_df['Confidence'].to_numpy()\n        if j % 10 == 0:\n            print(\"best confidence values found in round {} with best score of {}\".format(j,BestManualDescentScore))","4e45d752":"if BestManualDescentScore>BestRandSearchScore:\n    best_confidence = best_md_confidence\n    print(\"some manual descent improved confidence values\")","e34833ba":"# x_train, x_val, y_train1, y_val1, y_train2, y_val2 = train_test_split(\n#     x_train_full, \n#     best_confidence,\n#     BoxCoxTransform(train_df['FVC'], lmbda=fitted_lambda), \n#     test_size=TRAIN_VAL_RATIO, \n#     random_state=2020\n# )\n\nx_train, x_val, y_train, y_val = train_test_split(\n    x_train_full, \n    best_confidence,\n    test_size=TRAIN_VAL_RATIO, \n    random_state=2020\n)","6f1dd73e":"# def Laplace_Log_Likelihood(y_true, y_pred):\n#     # get predictions\n#     y_pred1 = tf.cast(y_pred[:,0], dtype=tf.float32) # confidence\n#     y_pred2 = tf.cast(y_pred[:,1], dtype=tf.float32) # fvc\n#     # reverse box cox\n#     tfz = tf.cast(tf.constant([0]), dtype=tf.float32) \n#     y_pred2 = tf.where(y_pred2<tfz,tfz,y_pred1)\n#     lbda = tf.cast(tf.constant([0.376401998544658]), dtype=tf.float32) \n#     tf1 = tf.cast(tf.constant([1]), dtype=tf.float32)\n#     p1 = tf.math.add(tf.math.multiply(y_pred2,lbda), tf1)\n#     y_pred2 = tf.pow(p1,tf.math.divide(tf1,lbda)) # fvc reverse box cox\n#     # laplace log likelihood                \n#     threshold = tf.cast(tf.constant([70]), dtype=tf.float32) \n#     sig_clip = tf.math.maximum(y_pred1, threshold)\n#     threshold = tf.cast(tf.constant([1000]), dtype=tf.float32) \n#     delta = tf.math.minimum(tf.math.abs(tf.math.subtract(y_true,y_pred2)),threshold)\n#     sqrt2 = tf.cast(tf.constant([1.4142135623730951]), dtype=tf.float32) \n#     numerator = tf.math.multiply(sqrt2,delta)\n#     part1 = tf.math.divide(numerator,sig_clip)\n#     innerlog = tf.math.multiply(sqrt2,sig_clip)\n#     metric = tf.math.subtract(-part1,tf.math.log(innerlog))\n#     return tf.math.reduce_mean(metric)","d7a70933":"with strategy.scope():\n    # define structure\n    xin = tf.keras.layers.Input(shape=(x_train_full.shape[1], ))\n    xout = tf.keras.layers.Dense(1, activation='linear')(xin)\n    # put it together\n    model2 = tf.keras.Model(inputs=xin, outputs=xout)\n    # compile\n    opt = tf.optimizers.RMSprop(LR)\n    model2.compile(optimizer=opt, loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n# print summary\nmodel2.summary()","1af0a2bf":"### define callbacks\nearlystopper = EarlyStopping(\n    monitor='val_mean_squared_error', \n    patience=3,\n    verbose=1,\n    mode='min'\n)\n\nlrreducer = ReduceLROnPlateau(\n    monitor='val_mean_squared_error',\n    factor=.5,\n    patience=2,\n    verbose=1,\n    min_lr=1e-9\n)","7cadca95":"print(\"Fit model on training data\")\nhistory = model2.fit(\n    x_train,\n    y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS_M2,\n    validation_data=(x_val, y_val),\n    callbacks=[earlystopper,lrreducer]\n)","680c01be":"model2.save('model.h5')","840ccd9b":"history_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()\nhistory_df[['mean_squared_error', 'val_mean_squared_error']].plot()\n# history_df[['Laplace_Log_Likelihood', 'val_Laplace_Log_Likelihood']].plot()","1b0ec85d":"model2.predict(x_test_full)","41b93e06":"fvc = model1.predict(x_train_full)\nconf = model2.predict(x_train_full)[:,0]\nfvc = ReverseBoxCoxTranform(fvc, lmbda=fitted_lambda)\ntrain_df['FVC_pred1'] = fvc\ntrain_df['Confidence'] = conf\ntrain_df['sigma_clipped'] = train_df['Confidence'].apply(lambda x: max(x, 70))\ntrain_df['diff'] = abs(train_df['FVC'] - train_df['FVC_pred1'])\ntrain_df['delta'] = train_df['diff'].apply(lambda x: min(x, 1000))\ntrain_df['score'] = -math.sqrt(2)*train_df['delta']\/train_df['sigma_clipped'] - np.log(math.sqrt(2)*train_df['sigma_clipped'])\nscore = train_df['score'].mean()\nprint(\"train score: \", score)","c6b65037":"train_df.head()","9be42751":"fvc = model1.predict(x_test_full)\nconf = model2.predict(x_test_full)\nfvc = ReverseBoxCoxTranform(fvc, lmbda=fitted_lambda)\ntest_df['FVC_pred1'] = fvc\ntest_df['Confidence'] = conf\ntest_df['sigma_clipped'] = test_df['Confidence'].apply(lambda x: max(x, 70))\ntest_df['diff'] = abs(test_df['FVC'] - test_df['FVC_pred1'])\ntest_df['delta'] = test_df['diff'].apply(lambda x: min(x, 1000))\ntest_df['score'] = -math.sqrt(2)*test_df['delta']\/test_df['sigma_clipped'] - np.log(math.sqrt(2)*test_df['sigma_clipped'])\nscore = test_df['score'].mean()\nprint(\"test score: \", score)","ebde7649":"test_df.head()","e6d66727":"# Imports & Settings ","26ce7468":"### Constants","025819d9":"### process tabular features\n\nWhen I went to make my own submission, I had some issues here.  First, pickling the data for later use was a major fail.  Also, for reasons I still don't understand, Kaggle doesn't like the way I merge my data for the submission file.  For merging features to the submission file, I would recommend checking out https:\/\/www.kaggle.com\/titericz\/tabular-simple-eda-linear-model and using their 'traintest' approach.  After pulling my hair out for a couple days, I found this to work.  Please, review this work for ideas, but don't use it as base code for a submission (or you'll go crazy).","760f381b":"# Train Score","d17fcabb":"# Feature Preprocessing","1cc22f3d":"### process test images","68f71328":"# Test Score","200db742":"# Random Search Confidence\n\nTo solve for confidence, we treat each row value as a hyperparameter and try some random search.  The values that produce the best score can be used to build a Confidence model.  I was able to use this in an actual submission.  \n\nhttps:\/\/docs.scipy.org\/doc\/numpy-1.13.0\/reference\/generated\/numpy.random.randint.html\n\nhttps:\/\/www.kaggle.com\/yasufuminakama\/osic-lgb-baseline","732b2279":"# Target Preprocessing","cbb4f84d":"# OSIC Image Data Prep and Baseline Regression Model\n\n - This is really just meant to be a **demonstration**, and for various reasons (the runtime of image data prep, other ways the data is prepped and merged, using internet, etc.) this can't be used to make a submission.  If you're looking for a base script that is somewhat similar and allows for a submission, I would check out (https:\/\/www.kaggle.com\/titericz\/tabular-simple-eda-linear-model).\n - Some other general thoughts and findings:\n     - I commented out the pickle data idea since the true holdout set is hidden, so we **can't pickle our submission data**.  I learned this the hard way.\n     - For whatever reason, Kaggle doesn't like the way I merge some features to the submission file, so again I would check out (https:\/\/www.kaggle.com\/titericz\/tabular-simple-eda-linear-model) for base code for merging data for making submission.\n     - First we model FVC, then we treat training confidence like hyperparameters and do random search. Next, we do a crude implementation of gradient descent without the gradient (so instead of a gradient telling us which direction to go, we try increasing and decreasing the value, see which does best, and take the better value).  Note that we also utilized hyperopt at one time, but this code was removed since it took too long to run (and it didn't really prove itself overly valuable in this case).  Finally we build a model for the confidence (because random search on new data is not efficient, stable, or flexible).  \n     - In our model for confidence, we comment out code for Laplace Log Likelihood.  The tensorflow code works, but TF itself doesn't appear to fit both models at the same time.  I assume this is because both outputs are compared to confidence instead of 1 to fvc and 1 to confidence (but I don't know for sure).  In short, I couldn't get this to work the way I wanted, but I included the code if anyone wants to take a look.  \n     - Finally we use the FVC model and Confidence model, to make our final predictions.\n     - Again, this notebook is mainly a showcase of ideas, so take some of the code with a grain of salt.\n\n\n### Other References\n\n - Feature Engineering ideas:  https:\/\/www.kaggle.com\/yasufuminakama\/osic-lgb-baseline?select=submission.csv\n\n### Acknowledgements\n\nhttps:\/\/www.kaggle.com\/yasufuminakama\n\nhttps:\/\/www.kaggle.com\/titericz","e4095b15":"# Model FVC","94f4f135":"### Configs","4c1ef5e1":"# Get Tabular Data","625fb7e2":"### helper functions","16e5d658":"# Manual Descent\n\nThis is a crude implementation of gradient descent without the gradients (so instead of a gradient telling us which direction to go, we try increasing and decreasing the value, we see which direction give us a better score, and we take the value that gives us the better score).  Random Search is faster, but if we try to implement some kind of adaptive 'learning rate' (i.e. making big changes in the beginning and decreasing the changes as we go) this approach moves a bit faster.  I was able to use this in an actual submission.  ","6332cc6d":"### Transformed Target Distribution","1a46783e":"### process training images","170beade":"# Model Confidence\n\nHere we want to model confidence.  I tried adding Laplace Log Likelihood as an additional metric, but eventually removed it due to issues.  The metric works, but the model doesn't really solve for fvc and confidence at the same time, making the metric useless.  In hindsight I think I needed to change the way I was defining my model output, but I just dropped the idea for the time being.  I included the code in case anyone wanted to take a look.    ","cdac7207":"### process submission images","6d892b56":"### we add some squared features for some model flexability\n\nSince we're building a regression model here, we have 2 options to account for flexability:\n - Include polynomial features\n - Bin our continuous variables and 1-hot encode the bins (leaving 1 group out)\n\nFor simplicity, and since our data is small and we don't want to overfit, I've just included 2nd order features."}}